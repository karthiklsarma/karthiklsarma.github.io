"V1","V2","V3","V4"
"0.0674967307062827","0.0339422116651065","  5293","<p>My name is Tuhin.
I came up with a couple of questions when I was doing an
analysis in R.</p>

<p>I did a logistic regression analysis in R and tried to check
how good the model fits the data.</p>

<p>But, I got stuck as I could not get the pseudo R square value
for the model which could give me some idea about the variation
explained by the model.</p>

<p>Could you please guide me on how to achieve this value (pseudo
R square for Logistic regression analysis).
It would also be helpful if you could show me a way to get the
Hosmer Lemeshow statistic for the model as well. I found out a
user defined function to do it, but if there is a quicker way
possible, it would be really helpful.</p>

<p>I would be very grateful if you can provide me the answers to
my queries.</p>

<p>Eagerly waiting for your response.</p>

<p>Regards</p>
"
"0.0551108498454793","0.083141099321054","  8661","<p>I'm trying to undertake a logistic regression analysis in <code>R</code>. I have attended courses covering this material using STATA. I am finding it very difficult to replicate functionality in <code>R</code>. Is it mature in this area? There seems to be little documentation or guidance available. Producing odds ratio output seems to require installing <code>epicalc</code> and/or <code>epitools</code> and/or others, none of which I can get to work, are outdated or lack documentation. I've used <code>glm</code> to do the logistic regression. Any suggestions would be welcome.  </p>

<p>I'd better make this a real question. How do I run a logistic regression and produce odds rations in <code>R</code>?  </p>

<p>Here's what I've done for a univariate analysis:  </p>

<p><code>x = glm(Outcome ~ Age, family=binomial(link=""logit""))</code>  </p>

<p>And for multivariate:  </p>

<p><code>y = glm(Outcome ~ Age + B + C, family=binomial(link=""logit""))</code>  </p>

<p>I've then looked at <code>x</code>, <code>y</code>, <code>summary(x)</code> and <code>summary(y)</code>.  </p>

<p>Is <code>x$coefficients</code> of any value?</p>
"
"0.126274820515127","0.108857251714449"," 10986","<p>I am trying to find some help with something that is called an ""Adjusted Analysis"" (or also Covariate Adjusted Logistic Regression);  a typical response has been that I might just want multivariable logistic regression, but this is not quite what I am looking for. The trouble I have is with what exactly an ""adjusted"" analysis is.</p>

<p>As an example, I have at my disposal a software suite that performs this type of adjusted analysis. We have some genes and various clinical variables from patients; what the method seems to do is adjust the p-values of the genes. But I can't figure out why, or how.  So I am trying to move outside of this software suite to truly understand what the underlying mathematics of this statistical technique is. </p>

<p>When I've posted this question in <a href=""http://stackoverflow.com/questions/6061305/using-r-for-covariate-adjusted-logistic-regression"">other places</a> the response has been that I should just take more courses in statistics. So while acknowledging my short comings, I would like to please ask if anyone can point me in a somewhat correct direction. I have been trying to find resources to help however I think I am not posing my question correctly enough.  As an aside I have a background in computer science and  more recently I am branching into biostatistics and I don't like using black box software so I would eventually like to re-implement this technique in R.</p>

<p>Thank you for any help that can be offered. Please let me know if there is a way I can pose my question clearer.</p>
"
"0.137777124613698","0.138568498868423"," 11107","<p>I need to do a logistic regression using R on my data. My response variable (<code>y</code>) is survival at weaning (<code>surv=0</code>; did not <code>surv=1</code>) and I have several independent variables which are binary and categoricals in nature.</p>

<p>I am following some examples on this website <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/dae/logit.htm</a> and trying to run some models.</p>

<p>Running the model: </p>

<pre><code>&gt; mysurv2 &lt;- glm(surv~as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
                 as.factor(pmtone), family=binomial(link=""logit""), data=ap)
&gt; summary(mysurv2)

Call:
glm(formula = surv ~ as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
    as.factor(pmtone), family = binomial(link = ""logit""), data = ap)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.2837  -0.5121  -0.5121  -0.5058   2.0590  

Coefficients:
                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -0.01135    0.23613  -0.048  0.96166    
as.factor(PTEM)2   -0.74642    0.24482  -3.049  0.00230 ** 
as.factor(PTEM)3   -1.95401    0.23259  -8.401  &lt; 2e-16 ***
as.factor(pshiv)2  -0.02638    0.06784  -0.389  0.69738    
as.factor(presp)2   0.74549    0.10532   7.079 1.46e-12 ***
as.factor(presp)3   0.66793    0.66540   1.004  0.31547    
as.factor(pmtone)2  0.54699    0.09678   5.652 1.58e-08 ***
as.factor(pmtone)3  1.82337    0.75409   2.418  0.01561 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 7892.6  on 8791  degrees of freedom
Residual deviance: 7252.8  on 8784  degrees of freedom
  (341 observations deleted due to missingness)
AIC: 7268.8

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Adding the <code>na.action=na.pass</code> at the end of the model gave me an error message. I thought that this would take care NA's in my independent variables.</p>

<pre><code>&gt; mysurv &lt;- glm(surv~as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
                as.factor(pmtone), family=binomial(link=""logit""), data=ap, 
                na.action=na.pass)
Error: NA/NaN/Inf in foreign function call (arg 1)
</code></pre>

<p>Since this is my first time to venture into logistic regression, I am wondering whether there is any package in R that would be more suitable?</p>

<p>I am also tryng to understand the regression coefficients. The independent variables used in the model are:</p>

<ol>
<li><p>rectal temperature: </p>

<ul>
<li><code>(PTEM)1</code> = newborns with rectal temp. below 35.4 0C</li>
<li><code>(PTEM)2</code> = newborns with rectal temp. between 35.4 to 36.9 0C</li>
<li><code>(PTEM)3</code> = newborns with rectal temp. above 37.0 0C</li>
</ul></li>
<li><p>shivering:</p>

<ul>
<li><code>(pshiv)1</code> = newborns that were not shivering</li>
<li><code>(pshiv)2</code> = newborns that were shivering</li>
</ul></li>
<li><p>respiration:</p>

<ul>
<li><code>(presp)1</code> = newborns with normal respiration</li>
<li><code>(presp)2</code> = newborns with slight respiration problem</li>
<li><code>(presp)3</code> = newborns with poor respiration</li>
</ul></li>
<li><p>muscle tone:</p>

<ul>
<li><code>(pmtone)1</code> = newborns with normal muscle tone</li>
<li><code>(pmtone)2</code> = newborns with moderate muscle tone</li>
<li><code>(pmtone)1</code> = newborns with poor muscle tone</li>
</ul></li>
</ol>

<p>Looking at the coefficients, I got the following:</p>

<pre><code>                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -0.01135    0.23613  -0.048  0.96166    
as.factor(PTEM)2   -0.74642    0.24482  -3.049  0.00230 ** 
as.factor(PTEM)3   -1.95401    0.23259  -8.401  &lt; 2e-16 ***
as.factor(pshiv)2  -0.02638    0.06784  -0.389  0.69738    
as.factor(presp)2   0.74549    0.10532   7.079 1.46e-12 ***
as.factor(presp)3   0.66793    0.66540   1.004  0.31547    
as.factor(pmtone)2  0.54699    0.09678   5.652 1.58e-08 ***
as.factor(pmtone)3  1.82337    0.75409   2.418  0.01561 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>In my other analysis, I found that newborns:  </p>

<p>a) with higher rectal temperature<br>
b) do not shiver<br>
c) good respiration and<br>
d) good muscle tone at birth were more likely to survive.  </p>

<p>I am a bit confused with the coefficients I am getting above. I am wondering whether whether I am not interpreting the results correctly or is it something else?</p>
"
"0.126274820515127","0.108857251714449"," 11679","<p>I have a nested-case control study that I have been using for analysis. At the end of my work I have deduced a set of variables that I use later to to classify new cases. One example of a simple classifier I am using is a naive Bayes, which will output simply a probability. </p>

<p>So here is my question:</p>

<p>Could I make my probabilities reflect the real world? In my specific example, the condition that I am testing for has a prevalence of 33% in my study, but a it has a population prevalence of only 10%.  Bayes factors have been suggested to me as a way to achieve this, however I am little unsure how to set up the problem. </p>

<p>As an example I have seen a Bayes factor as a logit between the true vs. study prevalence of the outcome. The classifier however was a logistic regression, and in that case the Bayes factor was just added to the linear predictors. I think the example there was very specific, and perhaps an inappropriate method for probabilities of a naive Bayes. Instead what I did was add the logit Bayes factor to the logged probabilities, but I am also not convinced this is right either. I also think a simpler solution would be to use Bayes theorem directly, but there I am not sure how to represented my study vs.population prevalences. The method below isn't quite right, but gets at what I want:</p>

<pre><code>        p_final = classier_posterior*(population_prev)/(study_prev)
</code></pre>

<p>I should contextualize that I use the probabilities to establish a threshold for classification down stream.</p>
"
"0.165332549536438","0.166282198642108"," 13172","<p>I would like to use a binary logistic regression model in the context of streaming data (multidimensional time series) in order to predict the value of the dependent variable of the data (i.e. row) that just arrived, given the past observations. As far as I know, logistic regression is traditionally used for postmortem analysis, where each dependent variable has already been set (either by inspection, or by the nature of the study). </p>

<p>What happens in the case of time series though,  where we want to make prediction (on the fly) about the dependent variable in terms of historical data (for example in a time window of the last $t$ seconds) and, of course, the previous estimates of the dependent variable?</p>

<p>And if you see the above system over time, how it should be constructed in order for the regression to work? Do we have to train it first by labeling, let's say, the first 50 rows of our data (i.e. setting the dependent variable to 0 or 1) and then use the current estimate of vector ${\beta}$ to estimate the new probability of the dependent variable being 0 or 1 for the data that just arrived (i.e. the new row that was just added to the system)?</p>

<p>To make my problem more clear, I am trying to build a system that parses a dataset row by row and tries to make prediction of a binary outcome (dependent variable) , given the knowledge (observation or estimation) of all the previous dependent or explanatory variables that have arrived in a fixed time window. My system is in Rerl and uses R for the inference. </p>
"
"0.134993461412565","0.135768846660426"," 15623","<p>I am completely new to R, just downloaded and installed it today. I am familiar with SAS and Stata; I am using R because I have found out that in survey regression analysis, R is capable of using data that have stratum with one PSU. However, I cannot figure out how to write the code at all.</p>

<p>Here is what I have done so far: read a Stata dataset and save the .RData file. I have also put in the MASS, pscl, and survey (for svyglm) packages.</p>

<p>Here's what I need to do:
1) I am using survey data, so I have a ""weight"" variable, a ""strata"" variable, and a ""PSU"" variable. I need to incorporate those; I know how to use svyset in Stata, but no idea in R.
2) I have stratum with singleton PSUs. I need to use an option called survey.lonely.psu I believe, and I have no idea where to even begin with that. This is the reason why I am using R instead of Stata as I do not want to collapse stratum or delete observations.
3) The types of regression models I have to run: survey negative binomial, survey zero-inflated negative binomial (need to also determine the predictors of zeros), survey logistic, and survey OLS regression.
4)I also really can't make much sense in R of how to write the model in R code. In Stata, I can simply write the model as:</p>

<p>svy: nbreg dependent_var independent_var1 independent_var2 independent_var3</p>

<p>I can't figure out how to do that at all in R.</p>

<p>Any and all help will be greatly appreciated.</p>
"
"0.143903513382412","0.159203084517278"," 17480","<p>I've created a few Cox regression models and I would like to see how well these models perform and I thought that perhaps a ROC-curve or a c-statistic might be useful similar to this articles use:</p>

<p><a href=""http://onlinelibrary.wiley.com/doi/10.1002/bjs.6930/abstract"" rel=""nofollow"">J. N. Armitage och J. H. van der Meulen, â€Identifying coâ€morbidity in surgical patients using administrative data with the Royal College of Surgeons Charlson Scoreâ€, British Journal of Surgery, vol. 97, num. 5, ss. 772-781, Maj 2010.</a></p>

<p>Armitage used logistic regression but I wonder if it's possible to use a model from the survival package, the <a href=""http://cran.r-project.org/web/packages/survivalROC/index.html"" rel=""nofollow"">survivalROC</a> gives a hint of this being possible but I can't figure out how to get that to work with a regular Cox regression. </p>

<p>I would be grateful if someone would show me how to do a ROC-analysis on this example:</p>

<pre><code>library(survival)
data(veteran)

attach(veteran)
surv &lt;- Surv(time, status)
fit &lt;- coxph(surv ~ trt + age + prior, data=veteran)
summary(fit)
</code></pre>

<p>If possible I would appreciate both the raw c-statics output and a nice graph</p>

<p>Thanks!</p>

<h2>Update</h2>

<p>Thank you very much for answers. @Dwin: I would just like to be sure that I've understood it right before selecting your answer. </p>

<p>The calculation as I understand it according to DWin's suggestion:</p>

<pre><code>library(survival)
library(rms)
data(veteran)

fit.cph &lt;- cph(surv ~ trt + age + prior, data=veteran, x=TRUE, y=TRUE, surv=TRUE)

# Summary fails!?
#summary(fit.cph)

# Get the Dxy
v &lt;- validate(fit.cph, dxy=TRUE, B=100)
# Is this the correct value?
Dxy = v[rownames(v)==""Dxy"", colnames(v)==""index.corrected""]

# The c-statistic according to the Dxy=2(c-0.5)
Dxy/2+0.5
</code></pre>

<p>I'm unfamiliar with the validate function and bootstrapping but after looking at prof. Frank Harrel's answer <a href=""http://r.789695.n4.nabble.com/Interpreting-the-example-given-by-Prof-Frank-Harrell-in-Design-validate-cph-tt3316820.html#a3324516"" rel=""nofollow"">here on R-help</a> I figured that it's probably the way to get the Dxy. The help for validate states:</p>

<blockquote>
  <p>... Somers' Dxy rank correlation to be computed at each resample (this
  takes a bit longer than the likelihood based statistics). The values
  corresponting to the row Dxy are equal to 2 * (C - 0.5) where C is the
  C-index or concordance probability.</p>
</blockquote>

<p>I guess I'm mostly confused by the columns. I figured that the corrected value is the one I should use but I haven't really understood the validate output:</p>

<pre><code>      index.orig training    test optimism index.corrected   n
Dxy      -0.0137  -0.0715 -0.0071  -0.0644          0.0507 100
R2        0.0079   0.0278  0.0037   0.0242         -0.0162 100
Slope     1.0000   1.0000  0.2939   0.7061          0.2939 100
...
</code></pre>

<p>In the <a href=""http://r.789695.n4.nabble.com/Interpreting-the-example-given-by-Prof-Frank-Harrell-in-Design-validate-cph-tt3316820.html#a3324516"" rel=""nofollow"">R-help question</a> I've understood that I should have ""surv=TRUE"" in the cph if I have strata but I'm uncertain on what the purpose of the ""u=60"" parameter in the validate function is. I would be grateful if you could help me understand these and check that I haven't made any mistakes.</p>
"
"0.0853773614587515","0.0858677581482184"," 25839","<p>First off, I'll say I am a biologist and new to the statistics side of things so excuse my ignorance</p>

<p>I have a data set that consists of a binary outcome and then a bunch of trinary explanatory variables that looks something like this:</p>

<pre><code>head()
 Category block21_hap1 block21_hap2 block21_hap3 block21_check
1        1            1            1            0             2
2        1            2            0            0             2
3        1            1            0            1             2
4        1            1            0            1             2
5        1            1            1            0             2
6        1            1            1            0             2
</code></pre>

<p>A quick summary of the data</p>

<pre><code>summary()
Category block21_hap1 block21_hap2 block21_hap3 block21_check
 1:718    0:293        0:777        0:1026       2:1467       
 0:749    1:709        1:577        1: 390                    
          2:465        2:113        2:  51  
</code></pre>

<p>and another summary grouped by outcome levels</p>

<pre><code>by(hap.ped.final, hap.ped.final$Category, summary)
hap.ped.final$Category: 1
 block21_hap1 block21_hap2 block21_hap3 block21_check
 0:146        0:374        0:518        2:718        
 1:336        1:286        1:174                     
 2:236        2: 58        2: 26                     
---------------------------------------------------------------------------- 
hap.ped.final$Category: 0
 block21_hap1 block21_hap2 block21_hap3 block21_check
 0:147        0:403        0:508        2:749        
 1:373        1:291        1:216                     
 2:229        2: 55        2: 25          
</code></pre>

<p>So I am trying to run logistic regression on this data. When I do this:</p>

<pre><code>fit = glm(Category~ block21_hap1 + block21_hap2 + block21_hap3, data = hap.ped.final ,family = ""binomial"")
summary(fit)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.301  -1.177   1.059   1.177   1.200  

Coefficients: (1 not defined because of singularities)
                             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)                 -0.039221   0.280110  -0.140    0.889
hap.ped.final$block21_hap11  0.123555   0.183087   0.675    0.500
hap.ped.final$block21_hap12  0.009111   0.295069   0.031    0.975
hap.ped.final$block21_hap21 -0.084334   0.183087  -0.461    0.645
hap.ped.final$block21_hap22 -0.013889   0.337468  -0.041    0.967
hap.ped.final$block21_hap31  0.201113   0.183087   1.098    0.272
hap.ped.final$block21_hap32        NA         NA      NA       NA

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2033  on 1466  degrees of freedom
Residual deviance: 2028  on 1461  degrees of freedom
AIC: 2040

Number of Fisher Scoring iterations: 3
</code></pre>

<p>So I don't really know what a singularity is or what's going wrong here that is throwing up NA's as a result of my analysis. Is it my data, or what I'm doing to it.
I tried googling the warning (or whatever you might call it) and I got some pages talking about collinearity and multilinearity, which I do not understand at all. 
Again, sorry for lack of knowledge here. I wish I had done more maths in undergrad. </p>
"
"0.131392986922243","0.143159999128503"," 26831","<p>Still on running logistic regression models and would like to ask a few questions around it.</p>

<p><strong>Question 1</strong>:
Is there a simple way of getting the p-values of each independent factor in a logistic regression model. For example, I am running this model:</p>

<pre><code>mymod3 &lt;- as.formula(surv~as.factor(tdate)+as.factor(sline)+as.factor(pgrp)
                                          +as.factor(weight5)+as.factor(backfat5)
                                          +as.factor(srect2)+as.factor(bcs)
                                          +as.factor(agit)+as.factor(uscore)
                                          +as.factor(loco)+as.factor(teat2)
                                          +as.factor(uscoref)+as.factor(colos)
                                          +as.factor(tb5)+as.factor(nerve)
                                          +as.factor(feed5)+as.factor(fos)
                                          +as.factor(gest3)+as.factor(int3)
                                          +as.factor(psex)+as.factor(bwt5)
                                          +as.factor(presp2)+as.factor(mtone2)
                                          +as.factor(pscolor)+as.factor(pmstain)
                                          +as.factor(pshiv)+as.factor(ppscore)
                                          +as.factor(pincis)+as.factor(prectem5)
                                          +as.factor(pcon12)+as.factor(crum5)
                                          +as.factor(pindx5))

sofNoMis3 &lt;- apf[which(complete.cases(apf[,all.vars(mymod3)])),]
FulMod3 &lt;- glm(mymod3,family=binomial(link=""logit""),data=sofNoMis3)
summary(FulMod3)
</code></pre>

<p>I am using this to look at the significant level of each factor:</p>

<pre><code>anova(FulMod3,test=""Chisq"")
</code></pre>

<p>and got this:</p>

<pre><code>Analysis of Deviance Table

Model: binomial, link: logit

Response: surv

Terms added sequentially (first to last)


                    Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                                 7791     7096.2              
as.factor(tdate)    15    50.71      7776     7045.4 9.215e-06 ***
as.factor(sline)     1    13.90      7775     7031.5 0.0001924 ***
as.factor(pgrp)      3     8.83      7772     7022.7 0.0316335 *  
as.factor(weight5)   4     7.18      7768     7015.5 0.1268943    
as.factor(backfat5)  4     3.86      7764     7011.7 0.4258074    
as.factor(srect2)    1     0.15      7763     7011.5 0.6987832    
as.factor(bcs)       3     6.46      7760     7005.1 0.0910745 .  
as.factor(agit)      2    13.44      7758     6991.6 0.0012075 ** 
as.factor(uscore)    2     2.16      7756     6989.5 0.3401845    
as.factor(loco)      2     1.58      7754     6987.9 0.4530983    
as.factor(teat2)     2    25.45      7752     6962.4 2.980e-06 ***
as.factor(uscoref)   2     0.48      7750     6962.0 0.7861675    
as.factor(colos)     1     1.06      7749     6960.9 0.3034592    
as.factor(tb5)       4    49.22      7745     6911.7 5.265e-10 ***
as.factor(nerve)     2     0.99      7743     6910.7 0.6105452    
as.factor(feed5)     4    11.79      7739     6898.9 0.0190170 *  
as.factor(fos)       1    47.10      7738     6851.8 6.732e-12 ***
as.factor(gest3)     2    22.60      7736     6829.2 1.235e-05 ***
as.factor(int3)      2     6.61      7734     6822.6 0.0367298 *  
as.factor(psex)      1     9.50      7733     6813.1 0.0020493 ** 
as.factor(bwt5)      4   348.42      7729     6464.7 &lt; 2.2e-16 ***
as.factor(presp2)    1   106.23      7728     6358.4 &lt; 2.2e-16 ***
as.factor(mtone2)    1    34.13      7727     6324.3 5.146e-09 ***
as.factor(pscolor)   1    12.57      7726     6311.7 0.0003928 ***
as.factor(pmstain)   1     0.30      7725     6311.4 0.5845095    
as.factor(pshiv)     1    32.29      7724     6279.2 1.328e-08 ***
as.factor(ppscore)   1    16.71      7723     6262.4 4.351e-05 ***
as.factor(pincis)    1     0.02      7722     6262.4 0.8892848    
as.factor(prectem5)  4   126.06      7718     6136.4 &lt; 2.2e-16 ***
as.factor(pcon12)    1    17.88      7717     6118.5 2.350e-05 ***
as.factor(crum5)     4    15.25      7713     6103.2 0.0042137 ** 
as.factor(pindx5)    4    25.58      7709     6077.6 3.838e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>but it does not always agree with the final model after applying backward elimination:</p>

<p>Example: </p>

<p>these three factors were not significant above but they still appeared in the final model below</p>

<pre><code>as.factor(weight5)   4     7.18      7768     7015.5 0.1268943    
as.factor(backfat5)  4     3.86      7764     7011.7 0.4258074    
as.factor(srect2)    1     0.15      7763     7011.5 0.6987832    

step(FulMod3,direction=""backward"",trace=FALSE)
</code></pre>

<p>which gives:</p>

<pre><code>Call:  glm(formula = surv ~ as.factor(tdate) + as.factor(pgrp) + as.factor(weight5) + 
    as.factor(backfat5) + as.factor(srect2) + as.factor(agit) + 
    as.factor(uscore) + as.factor(teat2) + as.factor(uscoref) + 
    as.factor(fos) + as.factor(gest3) + as.factor(int3) + as.factor(psex) + 
    as.factor(bwt5) + as.factor(presp2) + as.factor(mtone2) + 
    as.factor(pscolor) + as.factor(pshiv) + as.factor(ppscore) + 
    as.factor(prectem5) + as.factor(pcon12) + as.factor(pindx5), 
    family = binomial(link = ""logit""), data = sofNoMis3)

Coefficients:
               (Intercept)  as.factor(tdate)2009-09-11  as.factor(tdate)2009-09-15  as.factor(tdate)2009-09-18  as.factor(tdate)2009-09-22  
                   1.34799                     0.18414                    -0.19490                    -0.15552                    -0.16822  
as.factor(tdate)2009-09-25  as.factor(tdate)2009-09-29  as.factor(tdate)2010-01-26  as.factor(tdate)2010-01-29  as.factor(tdate)2010-02-02  
                   0.60046                     0.80784                    -1.03442                    -1.30562                    -1.01486  
as.factor(tdate)2010-02-05  as.factor(tdate)2010-02-09  as.factor(tdate)2010-02-12  as.factor(tdate)2010-02-16  as.factor(tdate)2010-02-19  
                  -1.04438                    -0.89311                    -1.06260                    -0.79833                    -1.09651  
as.factor(tdate)2010-02-23            as.factor(pgrp)2            as.factor(pgrp)3            as.factor(pgrp)4         as.factor(weight5)2  
                  -0.55411                     0.12659                    -0.04727                     0.21817                    -0.22592  
       as.factor(weight5)3         as.factor(weight5)4         as.factor(weight5)5        as.factor(backfat5)2        as.factor(backfat5)3  
                  -0.10143                    -0.31562                    -0.37656                    -0.19883                    -0.01188  
      as.factor(backfat5)4        as.factor(backfat5)5          as.factor(srect2)2            as.factor(agit)2            as.factor(agit)3  
                   0.08293                    -0.17116                    -0.18201                    -0.49145                    -0.36659  
        as.factor(uscore)2          as.factor(uscore)3           as.factor(teat2)2           as.factor(teat2)3         as.factor(uscoref)2  
                  -0.12265                     0.15334                     0.16575                     0.21520                     0.24166  
       as.factor(uscoref)3             as.factor(fos)2           as.factor(gest3)2           as.factor(gest3)3            as.factor(int3)2  
                  -0.24363                    -0.29506                     0.09747                     0.81894                    -0.25595  
          as.factor(int3)3            as.factor(psex)2            as.factor(bwt5)2            as.factor(bwt5)3            as.factor(bwt5)4  
                  -1.21086                     0.20025                     0.30753                     0.29614                     0.56753  
          as.factor(bwt5)5          as.factor(presp2)2          as.factor(mtone2)2         as.factor(pscolor)2           as.factor(pshiv)2  
                   0.86479                    -0.29270                    -0.40912                    -0.72782                    -0.33848  
       as.factor(ppscore)2        as.factor(prectem5)2        as.factor(prectem5)3        as.factor(prectem5)4        as.factor(prectem5)5  
                  -0.25958                     0.73842                     0.77476                     0.92158                     0.96269  
        as.factor(pcon12)2          as.factor(pindx5)2          as.factor(pindx5)3          as.factor(pindx5)4          as.factor(pindx5)5  
                   0.38119                     0.43199                     0.44496                     0.73458                     0.59771  

Degrees of Freedom: 7791 Total (i.e. Null);  7732 Residual
Null Deviance:      7096 
Residual Deviance: 6102         AIC: 6222
</code></pre>

<p><strong>Question 2</strong>:</p>

<p>I would like to calculate the standard errors of the odds ratio of each factor level </p>

<pre><code>exp(NewMod3$coefficients)  #Odds ratios
</code></pre>

<p><strong>Question 3:</strong></p>

<p>Lastly, to tell whether the levels of each factor are significantly different or not </p>

<pre><code>               (Intercept) as.factor(tdate)2009-09-11 as.factor(tdate)2009-09-15 as.factor(tdate)2009-09-18 as.factor(tdate)2009-09-22 
                 3.8496863                  1.2021883                  0.8229141                  0.8559688                  0.8451676 
as.factor(tdate)2009-09-25 as.factor(tdate)2009-09-29 as.factor(tdate)2010-01-26 as.factor(tdate)2010-01-29 as.factor(tdate)2010-02-02 
                 1.8229563                  2.2430525                  0.3554327                  0.2710041                  0.3624544 
as.factor(tdate)2010-02-05 as.factor(tdate)2010-02-09 as.factor(tdate)2010-02-12 as.factor(tdate)2010-02-16 as.factor(tdate)2010-02-19 
                 0.3519109                  0.4093819                  0.3455567                  0.4500787                  0.3340336 
as.factor(tdate)2010-02-23           as.factor(pgrp)2           as.factor(pgrp)3           as.factor(pgrp)4        as.factor(weight5)2 
                 0.5745817                  1.1349500                  0.9538339                  1.2437928                  0.7977835 
       as.factor(weight5)3        as.factor(weight5)4        as.factor(weight5)5       as.factor(backfat5)2       as.factor(backfat5)3 
                 0.9035410                  0.7293337                  0.6862173                  0.8196866                  0.9881871 
      as.factor(backfat5)4       as.factor(backfat5)5         as.factor(srect2)2           as.factor(agit)2           as.factor(agit)3 
                 1.0864697                  0.8426844                  0.8335940                  0.6117399                  0.6930936 
        as.factor(uscore)2         as.factor(uscore)3          as.factor(teat2)2          as.factor(teat2)3        as.factor(uscoref)2 
                 0.8845715                  1.1657233                  1.1802836                  1.2401126                  1.2733576 
       as.factor(uscoref)3            as.factor(fos)2          as.factor(gest3)2          as.factor(gest3)3           as.factor(int3)2 
                 0.7837753                  0.7444886                  1.1023798                  2.2681046                  0.7741829 
          as.factor(int3)3           as.factor(psex)2           as.factor(bwt5)2           as.factor(bwt5)3           as.factor(bwt5)4 
                 0.2979401                  1.2217088                  1.3600609                  1.3446543                  1.7639063 
          as.factor(bwt5)5         as.factor(presp2)2         as.factor(mtone2)2        as.factor(pscolor)2          as.factor(pshiv)2 
                 2.3745019                  0.7462454                  0.6642372                  0.4829602                  0.7128545 
       as.factor(ppscore)2       as.factor(prectem5)2       as.factor(prectem5)3       as.factor(prectem5)4       as.factor(prectem5)5 
                 0.7713779                  2.0926314                  2.1700692                  2.5132469                  2.6187261 
        as.factor(pcon12)2         as.factor(pindx5)2         as.factor(pindx5)3         as.factor(pindx5)4         as.factor(pindx5)5 
                 1.4640265                  1.5403203                  1.5604231                  2.0845978                  1.8179532 
</code></pre>

<p>Example:</p>

<p>I would like to have a table like this:</p>

<pre><code>Factor           levels  Odds ratio

Parity group      (1)    1.00Â±standard error   a
                   2     1.50Â±standard errror  b
                  3-4    1.17Â±standard error   c
                   &gt;5    1.19Â±standard error   c
</code></pre>

<p>I would really appreciate your help on these 3 areas.</p>

<p>Baz</p>
"
"0.106721701823439","0.107334697685273"," 27400","<p>I'm reading A. Agresti (2007), <em><a href=""http://rads.stackoverflow.com/amzn/click/0471226181"">An Introduction to Categorical Data Analysis</a></em>, 2nd. edition, and am not sure if I understand this paragraph (p.106, 4.2.1) correctly (although it should be easy):</p>

<blockquote>
  <p>In Table 3.1 on snoring and heart disease in the previous chapter, 254
  subjects reported snoring every night, of whom 30 had heart disease.
  If the data file has grouped binary data, a line in the data file
  reports these data as 30 cases of heart disease out of a sample size
  of 254. If the data file has ungrouped binary data, each line in the
  data file refers to a separate subject, so 30 lines contain a 1 for
  heart disease and 224 lines contain a 0 for heart disease. The ML
  estimates and SE values are the same for either type of data file.</p>
</blockquote>

<p>Transforming a set of ungrouped data (1 dependent, 1 independent) would take more then ""a line"" to include all the information!? </p>

<p>In the following example a (unrealistic!) simple data set is created and a logistic regression model is build. </p>

<p>How would grouped data actually look like (variable tab?)? How could the same model be build using grouped data? </p>

<pre><code>&gt; dat = data.frame(y=c(0,1,0,1,0), x=c(1,1,0,0,0))
&gt; dat
  y x
1 0 1
2 1 1
3 0 0
4 1 0
5 0 0
&gt; tab=table(dat)
&gt; tab
   x
y   0 1
  0 2 1
  1 1 1
&gt; mod1=glm(y~x, data=dat, family=binomial())
</code></pre>
"
"0.134993461412565","0.135768846660426"," 34930","<p>I have a large set of data for 37 different clinical units (all oncology) in their respective 37 hospitals. There are two specific outcome variables that I need to analyse:</p>

<p>First, drug usage for specific drugs types and classes (aggregated drugs) that are expressed as a rate â€“  DDD (Defined Daily Doses) per 100 patient days. There are individual patient drug use figures for this set.</p>

<p>Question1: Which regression approach should I take? From what I can gather I can use a Poisson regression model. IF there is overdispersion in the outcome I could resort to a negative binomial model.</p>

<p>Second: I have antibiotic resistance data that is expressed as proportion in the range 0 â€“ 1.These are not available as individual patient data points but aggregated to each of the 37 hospitals.</p>

<p>Question 2: Again, which approach? From what I have read I can use a logistic regression model. I have been advised by another statistician to initiall use a logit model and then use a probit model and compare goodness of fit for each model.</p>

<p>Does this sound like a reasonable approach? Is there a specific text that you could direct me to in order to upgrade my basic regression modelling skills. I will be using R for the analysis.</p>

<p>Thanks in advance.</p>
"
"0.116907766928076","0.117579270250443"," 35269","<p>I'm using R to test some distribution families to my data.
I've done KS, AD tests and determined the loglike.</p>

<p>For one of the data the indications given by KS and AD do not agree with the ones given by the loglike:</p>

<pre><code>Table: p-values
Test    Normal  Log-normal  Gamma   Logistic    Weibull Gumbel
KS      0,16    0,00        0,00    0,26        0,00    0,49
AD      0,17    0,00        0,00    0,27        0,00    -

Measure Normal  Log-normal  Gamma   Logistic    Weibull Gumbel
loglike 282,86  279,54      308,96  284,41      304,00  291,55
</code></pre>

<p>I've read that KS gives more emphasis to the middle part of the distributions and AD to the tails. On the loglike one maximizes the probability of a model fitting the data.
The graphical analysis says to me that Log-normal, Gamma and Weibull only fit the data well at the left tail whereas the other distributions fit data quite well all over the domain....
So why does these three dist. have a larger loglike than the others that seem to fit better the data? Thanks</p>
"
"0.178977734963755","0.192006144294928"," 35940","<p>This question is in response to an answer given by @Greg Snow in regards to a <a href=""http://stats.stackexchange.com/questions/35918/logistic-regression-the-standard-deviation-used-in-glmpower"">question</a> I asked concerning power analysis with logistic regression and SAS <code>Proc GLMPOWER</code>.</p>

<p>If I am designing an experiment and will analze the results in a factorial logistic regression, how can I use <a href=""http://stats.stackexchange.com/questions/22406/power-analysis-for-ordinal-logistic-regression/22410#22410"">simulation</a> ( and <a href=""http://sas-and-r.blogspot.com/2009/06/example-72-simulate-data-from-logistic.html"">here</a> ) to conduct a power analysis?</p>

<p>Here is a simple example where there are two variables, the first takes on three possible values {0.03, 0.06, 0.09} and the second is a dummy indicator {0,1}. For each we estimate the response rate for each combination (# of responders / number of people marketed to). Further, we wish to have 3 times as many of the first combination of factors as the others (which can be considered equal) because this first combination is our tried and true version. This is a setup like given in the SAS course mentioned in the linked question.</p>

<p><img src=""http://i.stack.imgur.com/4LIvh.jpg"" alt=""enter image description here""></p>

<p>The model that will be used to analyze the results will be a logistic regression, with main effects and interaction (response is 0 or 1). </p>

<pre><code>mod &lt;- glm(response ~ Var1 + Var2 + I(Var1*Var2))
</code></pre>

<p>How can I simulate a data set to use with this model to conduct a power analysis?</p>

<p>When I run this through SAS <code>Proc GLMPOWER</code> (using <code>STDDEV =0.05486016</code>
 which corresponds to <code>sqrt(p(1-p))</code> where p is the weighted average of the shown response rates):  </p>

<pre class=""lang-sas prettyprint-override""><code>data exemplar;
  input Var1 $ Var2 $ response weight;
  datalines;
    3 0 0.0025  3
    3 1 0.00395 1
    6 0 0.003   1
    6 1 0.0042  1
    9 0 0.0035  1
    9 1 0.002   1;
run;

proc glmpower data=exemplar;
  weight weight;
  class Var1 Var2;
  model response = Var1 | Var2;
  power
    power=0.8
    ntotal=.
    stddev=0.05486016;
run;
</code></pre>

<p>Note: <code>GLMPOWER</code> only will use class (nominal) variables so 3, 6, 9 above are treated as characters and could have been low, mid and high or any other three strings. When the real analysis is conducted, Var1 will be used a numeric (and we will include a polynomial term Var1*Var1) to account for any curvature.</p>

<p>The output from SAS is </p>

<p><img src=""http://i.stack.imgur.com/T44tM.png"" alt=""enter image description here""></p>

<p>So we see that we need 762,112 as our sample size (Var2 main effect is the hardest to estimate) with power equal to 0.80 and alpha equal to 0.05. We would allocate these so that 3 times as many were the baseline combination (i.e. 0.375 * 762112) and the remainder just fall equally into the other 5 combinations.</p>
"
"0.0826662747682189","0.083141099321054"," 37714","<p>I would like to make a prediction for a (new) subject to have a certain outcome given the historical data and the model:</p>

<pre><code>glm(outcome ~ age + treatment + history, family=binomial, ...) 
</code></pre>

<p>however in the historical data that will be fitted by the model, I have some sort of repeated measurements on some of the subjects (and I don't know if repeated measures is the appropriate term to be used here, hence using lmer etc is doubtful); example:<br></p>

<pre><code>subject_ID    age    treatment    history    outcome
S_1           33      T_1         H_1        0
S_2           27      T_2         H_2        1
S_2           27      T_3         H_2        1
S_3           56      T_1         H_11       0
etc...
</code></pre>

<p>In this example subject_2 (S_2) has two rows because he had simultaneously two different treatments at the same time. could a logistic regression still be used or should cases like subject_2 be removed from the analysis?</p>
"
"0.248157138284588","0.258496182752659"," 43040","<p>I need some guidance related to regression model verification using validation data. 
I am new to R-tool &amp; statistics and trying my best to learn. I did search on internet too but I couldn't get a final answer to my questions. 
Actually I have a lot of questions, I may try my best to explain the problems:
I am experimenting with network packets and R-tool.
I have captured some packets from a network using a custom made packet sniffer in java. The sniffer will capture some packets and save the information of packet header like: tcp window size, tcp sequence numbers, date-time, ip header length, ip time to live etc... in a csv file.</p>

<p>Also the sniffer will add category number to each csv file so that we can know which packet belongs to which category. I created 9 different categories saved in 9 different csv files.
Now I extracted 1000 observation from each of the csv files and created a data set named ""alldata"".</p>

<p>Then I created training data set and validation data set from ""alldata"" data set.</p>

<p>Now I want to perform linear regression, logistic regression, decision tree analysis, cluster analysis etc on this ""alldata"" data set.</p>

<p>So my plan was to use training data set to create models and then later use validation data set to verify my models. </p>

<p>Category will be my target variable in any case. I want to predict the category from other independent variables.</p>

<ol>
<li><p>My first confusion is that after I created scatter plot of category with other independent variables and I don't see any linear relationship between them. Moreover I even don't know what relation exists between category and independent variables. From scatter plots it seems to me that there is no specific relation between category and other independent variables(except date_time it is bit linear to category). Am I doing the correct interpretation ?
Here are some of the plots:
<a href=""http://imageshack.us/photo/my-images/211/tcpdport.png/"" rel=""nofollow"">plot 1</a>
<a href=""http://imageshack.us/photo/my-images/547/tcpchksum.png/"" rel=""nofollow"">plot 2</a></p></li>
<li><p>I think doing linear regression won't make any sense now after having a look at scatter plots. Is this correct assumption?</p></li>
<li><p>Although I tried to do make some regression models with training data set, but the R-square values for all the models is quite low (for example like 0.00019, 0.0035, 0.018 etc. ) 
So can I assume that these models are not good due to very low r-square vales?</p></li>
<li><p>As logistic regression is used when we have target variables having only two values 1 or 0, or some probabilities between 0.0 - 1.0.
This means performing logistic regression is not possible for this type of data set.
Is my assumption true?</p></li>
<li><p>My main question was how to verify a model created with training data set by using validation data set?
Please let me know the commands and the procedure.
Please let me know if I am doing this in wrong way or if you can suggest me a better way to do this whole work. I think if someone could please clear my doubts then I may ask further more questions.</p></li>
</ol>

<p>If you don't understand my problem we can discuss in more detail
I look forward for your replies.
Thank you!</p>

<hr>

<p>@Wayne</p>

<p>Hello thanks for the reply, but the thing is for each category I have almost same range of values of independent variables like(tcpheader, ipttl, iplen). For example iptype is only having two values 6 and 17. So most of the categories are having iptype value of 6 &amp; 17.
So it is also same is for tcpheader, tcp sequence number, tcp acknowledgement number etc. I don't think there is any way to distinguish a particular packet based on these independent variables. Only the independent variable that can be helpful is time.
But when I created a model with time it had good r-squared value but the regression line equation doesn't predict category with any value of date_time.
I don't understand this behaviour.</p>

<p>Thanks.</p>
"
"0.150927278118246","0.136614766661756"," 46789","<p>I collected data to find whether the presence or absence of vision, sound, and touch during a task affected the successful completion of that task. However, there were no samples collected where all three senses were absent. So the dependent variable is boolean success but I have a question about how to model the independent variables in a logistic regression.</p>

<p>My initial analysis used a single categorical variable with seven levels representing each combination of senses (seven because there were no cases where all three senses were absent).</p>

<pre><code>summary( glmer( Success ~ Condition + ( 1 | Participant ), family=binomial, data=trials))
</code></pre>

<p>When I tried to build a model with the Vision, Sound, and Touch as separate variables, the analysis fails. <a href=""http://stat.ethz.ch/pipermail/r-sig-mixed-models/2010q4/004552.html"" rel=""nofollow"" title=""[R-sig-ME] Structural zeros in lme4"">I believe this is because I have empty cells when including the vision*sound*touch interaction</a> because we did not collect results where all senses were absent.</p>

<pre><code>summary( glmer( Success ~ Vision + Sound + Touch + Vision*Sound + Vision*Touch + 
                Sound*Touch + Vision*Sound*Touch + ( 1 | Participant ), 
                family=binomial, data=trials))
</code></pre>

<p>I followed the suggestion linked above to use the <code>interaction</code> function to drop the unused factor (all three senses absent). However, this seems to create a variable that looks like my original single categorical variable.</p>

<pre><code>senses &lt;- interaction( trials$Vision, trials$Sound, trials$Touch, drop=TRUE )
summary( glmer( Success ~ senses + ( 1 | Participant ), family=binomial, data=trials))
</code></pre>

<p>As I try to refine this analysis, is there a way to model the senses as separate variables to make the interaction between these variables clearer? That is, to appropriately model the contribution of vision in the <code>vision</code>, <code>vision*sound</code>, <code>vision*touch</code> and <code>vision*sound*touch</code> conditions. From the initial analysis, the <code>vision*sound*touch</code> interaction is the most interesting.</p>
"
"0.134993461412565","0.135768846660426"," 56055","<p>I have data from a randomized survey experiment in which each respondent was assigned to one of 4 groups, one of which can be considered a ""control"" or ""no treatment"" group. The key question asked in the survey was a binary one: i.e. each respondent was faced with a choice between two products given some stimulus based on the assigned group. Of course, there are several other questions to be controlled for (demographics, pre-existing preferences, etc.).</p>

<p>I want to know what effect, if any, being in a particular group had on the respondent's choice for that key question, controlling for the other factors. Since my response variable is categorical I can't use ANOVA (at least R doesn't appear willing to let me have a non-numeric response variable). I have tried to do a logistic regression but it seems like the structure of my data means that this would result in the respondents in each group being compared to the rest of the respondents which seems like it would be incorrect.</p>

<p>My data resembles the following in structure:</p>

<pre><code>| Id | Group | Product Chosen | ... (other variables)
| 1  |     1 | A              | ...
| 2  |     4 | B              | ...
| 3  |     3 | B              | ...
| 4  |     2 | B              | ...
| 5  |     1 | A              | ...
| 5  |     2 | B              | ...
| 5  |     4 | A              | ...
| 5  |     3 | B              | ...
</code></pre>

<p>etc.</p>

<p>In case it is relevant, I have been using R for my analysis.</p>

<p><strong>Update:</strong> Just so it's clear, my working hypothesis is that respondents in non-control groups were more likely to choose product A than B (and less importantly, but similarly, that respondents in group 2 were more likely than those in group 3, and those in group 3 were more likely than those in group 4).</p>
"
"0.0826662747682189","0.083141099321054"," 56608","<p>I'm doing some clinical database research and in an effort to lessen the burden on our statistical staff, I started to look for different software solutions to get the analyses I need, and that's where BIDS (business intelligence development studio).  BIDS allows queries to be run against a SQL Server and store the results of those queries in a table or view.  That table or view is then consumed by BIDS and logistic and linear regressions as while as CART analyses can be done on them.<br>
BIDS Version</p>

<p>The x axis that is cut off on the lift chart is 'overall population %'
<img src=""http://i.stack.imgur.com/LbpXf.jpg"" alt=""enter image description here""> </p>

<p>A mining accuracy chart of the CART
<img src=""http://i.stack.imgur.com/Vm5Te.jpg"" alt=""enter image description here""></p>

<p>I'm not quite sure how this works in other stats packages, but in BIDS you define a certain percentage of your data set to train the model and the rest of the set is compared against that model and the lift chart shows the improvement in identifying the outcome you desire vs. a guess.  I'm only vaguely familiar with CART analyses in the first place, and don't know the first thing about R, but this same analysis was done in R with very similar results.  The red portion of what looks like a health meter in a video game corresponds nearly identical to the analysis done in R.  However, there are no p values in the BIDS version.  Consider the node Max Total Poly Pharm >=7 and &lt; 13.
BIDS shows me (not present in the picture)
value         cases    probability
not present   2133     91.89</p>

<p>Is there any way to ascertain a p value from that.  And does R use any of it's cases as training data for the model?</p>
"
"0.0954547919806695","0.0960030721474639"," 59311","<p>I'm working on a behavoural scorecard modelling exercise, and many of the decisions taken to date have been based on the experience of a consulting credit analyst (whose experience software-wise is SAS) as I am primarily in BI. So far I have:</p>

<ul>
<li>a linux pc with 32gb of ram and an i7 processor</li>
<li>an observation window  </li>
<li>~90 potential characteristics </li>
<li>a binary outcome</li>
</ul>

<p>In <strong>R</strong>, I have</p>

<ol>
<li>loaded the dataset (225k obs of 88 vars, 1 outcome)</li>
<li>split the dataset up based on the recommendations/examples in the package <strong>caret</strong> i.e. predictors and outcomes split up (150k obs in training sample)</li>
<li>removed any variables showing a high degree of correlation (caret::findCorrelation)</li>
<li>cut all continuous variables into categorical intervals </li>
<li>reduced the number of variables based on near zero variance, missing values, and low information value (IV) (150k obs of 48 vars)</li>
<li>tried bestglm::bestglm, caret::train (with glm and glmnet), FWDselect::selection, FWDselect::qselection but eventually had to interrupt each of these due to not completing after 4 hours of 100% CPU usage</li>
<li>used FactoMineR:MCA to perform a multiple correspondence analysis (on predictors only)</li>
</ol>

<p>What I would like to do is have a selection of logistic regression models for say 4, 8, 12, and 16 variables that are the most predictive models at each point.  I'm not sure if I'm going in the correct direction here with MCA as I've mainly been simply trying to find something that works in a timely fashion for reducing my variables further or going directly to variable selection steps.</p>

<p>I would appreciate any advice on how to do any of step 6 better, whether 7 makes sense and what step 8 should be. </p>

<p>Thanks,</p>

<p>Steph</p>

<p>PS Design decisions up to 6&amp;7 can't be revised so please, no telling me off for them! </p>
"
"0.0674967307062827","0.0678844233302131"," 62180","<p>I want to do some regression analysis that constrains the coefficients to vary smoothly as a function of their sequence.
It is similar to the ""Phoneme Recognition"" example in the part 5 ""Basis Expansions and Regularization"" of <em><a href=""http://www.stanford.edu/~hastie/local.ftp/Springer/OLD//ESLII_print4.pdf"" rel=""nofollow"">The Elements of Statistical Learning</a></em> (Hastie, Tibshirani &amp; Friedman, 2008).</p>

<p>in the book it says:</p>

<blockquote>
  <p>The smooth red curve was obtained through a very simple use of natural cubic splines. We can represent the coefficient function as an expansion of splines $\beta(f)=\sum_{m=1}^M h_m(f)\theta_m$. In practice this means that $\beta=\mathbf{H}\theta$ where, $\mathbf{H}$ is a $p Ã— M$ basis matrix of natural cubic splines, defined on the set of frequencies. Here we used $M = 12$ basis functions, with knots uniformly placed over the integers 1, 2, . . . , 256 representing the frequencies. Since $x^T\beta=x^T\mathbf{H}\theta$, we can simply replace the input features $x$ by their filtered versions $x^* = \mathbf{H}^T x$, and fit $\theta$ by linear logistic regression on the $x^*$. The red curve is thus $\hat\beta(f) = h(f)^T\hat \theta$.</p>
</blockquote>

<p>But I am not sure about how to create the basis matrix $\mathbf{H}$. When using ns() function in R, how to set the knots?</p>

<p>Any hint will be appreciated.</p>
"
"0.184847409828413","0.173515206485542"," 62208","<p>I've tried to simulate data for a power analysis of a logistic regression. The results of the power analysis look reasonable: power=90% for a sample of 6000 persons. But I feel that the analysis lacks something. So, my question is: when generating the data should I include something about how the variables are correlated, or their covariance, other than just defining their linear relationship as I have done in the example below, and if so where do I write that into the code?</p>

<p>I know other questions look like this but I'm not confident that they answer this question.</p>

<pre><code>library(plyr) # functions
## Define Function
simfunktion &lt;- function() {
   # Number in each sample
  antal &lt;- 6000
  beta0 &lt;- log(0.16) # logit in reference group
  beta1 &lt;- log(1.1)  # logit given smoking
  beta2 &lt;- log(1.1)  # logit given SNP(genevariation)
  beta3 &lt;- log(1.2)  # logit for interactioncoefficient for SNP*rygning
   ## Smoking variable, with probabilities defined according to empirical studies.
  smoking  &lt;- sample(x = 0:2, size = antal, replace = TRUE, prob = c(40,25,40))
   ## SNP variables with probabilities defined according to empirical studies
  SNP      &lt;- sample(x = 0:2, size = antal, replace = TRUE, prob = c(40,40,20))
   ## calculated probabilites given the model:
  pi.x     &lt;- exp( beta0 + beta1*smoking + beta2*SNP + beta3*smoking*SNP) / 
              ( 1 + exp(beta0 + beta1*smoking + beta2*SNP + beta3*smoking*SNP) )
   ## binoial events given the probabilities:
  sim.y    &lt;- rbinom( n = antal, size = 1, prob = pi.x)  
  sim.data &lt;- data.frame(sim.y, smoking, SNP)
   #################### p-value of the interaction is extracted:
   ## the model is run:
  glm1     &lt;- glm( data = sim.data, formula = sim.y ~ smoking + SNP + smoking:SNP, 
                  family=binomial )
   ## p-value of the interactionterm is extracted:
  summary(    glm( data = sim.data, formula = sim.y ~ smoking + SNP + smoking:SNP, 
                  family=binomial ))$coef[4,4]
}
pvalue     &lt;- as.vector(replicate( 100 , simfunktion()))
mean(pvalue &lt; 0.05)
</code></pre>
"
"0.0477273959903348","0.0480015360737319"," 62483","<p>I've got a conditional logistic regression setup using <code>clogit</code> in <code>R</code> like this: </p>

<pre><code>m&lt;-clogit(PHENO==2 ~ x + as.factor(COVAR[,1]) + strata(COVAR[,2]) )
</code></pre>

<p>I wanted to try doing GLMM analysis in <code>R</code>. I'm a little confused on the syntax for the <code>lme4</code> package in <code>R</code>. <code>COVAR[,2]</code> is the matching variable in my data. Can someone explain to me the difference between some of these statements:</p>

<pre><code>m &lt;-  lmer(PHENO==2 ~ x + as.factor(COVAR[,1]) + (x|COVAR[,2]) )
m &lt;-  lmer(PHENO==2 ~ x + as.factor(COVAR[,1]) + (1|COVAR[,2]) )
</code></pre>

<p>Which one is more appropriate? </p>
"
"0.0826662747682189","0.083141099321054"," 65174","<p>I am doing a logistic regression analysis using the glm command in R. It is to identify causes of valve narrowing beyond a certain threshold; 0=no narrowing, 1=narrowed. One of my variables is the size of a medical device that is implanted (range 25-36mm). Sometimes the device isn't implanted and I've left this as a blank field, but of course this is interpreted as a missing field. Not implanting the device seems to have a significant effect using Chi-sq analysis, and the size of the device has a significant effect using a t-test. How do I get around this in a linear regression model?</p>

<p>To make it more complicated I actually have two different makes of the device: ""C"" and ""D"" with sizes 25-36mm, another device without a size ""S"" and then no device ""N"". Can it all be entered together or is it best to analyze separately outside of regression?</p>

<p>What effect does the ""missingness"" have on various other variables that are in the analysis?</p>

<p>Please &amp; thankyou</p>
"
"0.134993461412565","0.135768846660426"," 65859","<p>Recently, I have read an article which name is â€œ<a href=""http://www.ncbi.nlm.nih.gov/pubmed/9618776"" rel=""nofollow""><em>Feed forward neural networks for the analysis of censored survival data: A partial logistic regression approach</em></a>â€.</p>

<p>Without a math background, I catch only a little about the ANN applying the analysis of censored survival data. </p>

<p>In the training group, each subject is replicated for all the intervals in which the subjects is observed and coupled with the event; why is this done? Especially, in the testing group, indicator each subjects are replicated into full number of time interval of observed with all event indicator as zero?</p>

<p>I know this process can take into account the censoring data.</p>

<p>In addition, are the output of neural networks are conditional probabilities of failure? My problem is how to produce the survival curve using the output data. Does anybody know the R code or MATLAB code to perform this whole process? Or give me some suggestion to find answers! The following R code is my try on this method but I can't go on it, for I don't know draw the survival curve depending on the output conditional failure probabilities!</p>

<pre><code>dat&lt;-read.csv(""traininglj.csv"",header=T)
tt &lt;- dat$time &lt;- as.numeric(cut(dat$TTR,c(0,6,12,18,24,30,36,42,48,54,200)))
dat2 &lt;- dat[rep(1:nrow(dat), tt), ]
time2 &lt;- NULL
for (i in 1:length(tt)) time2 &lt;- c(time2, 1:tt[i])
dat2$time &lt;- time2
    dat2$Recurrence &lt;- 0
dat2$Recurrence[cumsum(tt)] &lt;- dat$Recurrence
write.csv(dat2,file=""result.csv"")
mydat &lt;- apply(dat2[,13:23],2,function(x)(x-min(x,na.rm=TRUE))/
    (range(x,na.rm=TRUE)[2]-range(x,na.rm=TRUE)[1]))
training &lt;- cbind(dat2[,1:12],mydat,dat2[24])
library(nnet)
library(lattice)
attach(training)
dat.net &lt;- nnet(Recurrence ~ time+ALT+ALB+PLT+INR+age+MELD+logAFP+Diameter
        +sex+number+Gstage+HBsAg+AN+MVI, 
    data = training, 
    size = 12, 
    decay=0.025,
    maxit = 1000,
    entropy=TRUE,
    trace=TRUE)
</code></pre>
"
"0.294308192330579","0.303588370359458"," 67873","<p><strong>TLDR</strong>: How can I perform inference for the between group differences in a possibly logistic growth with time in the presence of outliers, unequal measurement times and frequency, bounded measurements and possible random effects on individual and per study level?</p>

<p>I am attempting to analyse a dataset where measurements for individuals were made at different time points. Measurements start low at time 0 and follow (very roughly) a logistic growth pattern with time. I am trying to establish if there are differences between two groups of individuals. The analysis is complicated by the following factors:</p>

<ul>
<li>The effect of time is non-linear, so either a non-linear logistic regression (biologically plausible, but not particularly well fitting) or a non-parametric regression seem appropriate</li>
<li>There are massive outliers, so regression using the sum of squared residuals seems off the table. Quantile regression seems appropriate.</li>
<li>Random effects may be appropriate on a per individual and per study level. Mixed effects models seems appropriate.</li>
<li>Measurement times, number of available measurements and end of monitoring differ between individuals. Survival analysis techniques seem appropriate. Possibly also applying weights equal to 1 / number of observations for individual.</li>
<li>Measurements are bounded below at 0 and while there is no obvious boundary above, arbitrarily high measurements seem biologically implausible. However, quite a few individuals have some measurements of zero (partly due to the measurement accuracy of the device).</li>
<li>A few models I tried so far failed to fit, usually with an unhelpful error related to the numerical procedure. This leads me to believe that I will need a reasonably robust method able to deal with this somewhat ugly dataset.</li>
<li>Finally, I want to produce inference of the form ""group 1 has faster growth than group 2"" or ""group 1 has a higher asymptotic level than group 2"".</li>
</ul>

<p>What I have tried so far (all in R) - I was aware that most of the below are not particularly appropriate for the dataset, but I wanted to see which models could actually be fitted without numerical errors:</p>

<ul>
<li>Non-parametric regression using crs in the crs package. Nicely produces a curve reasonably close to logistic growth for most of the time period with some strange behavious toward the end of the monitoring period (where there are fewer observations). Using individuals as fixed effects reveals some outliers. Using the variable of interest as fixed effects shows some difference. However, I am not sure if there is any way to assess fits and do inference on a model this complex.</li>
<li>Non-linear mixed effects regression using nlme in package nlme and SSlogis. Gradually building up the model with update() works reasonably well. Getting too complex with the fixed effects or the random effects leads to convergence failure. Since there is no reason to believe that residuals are normally distributed, it seems a bad idea to pursue this further. Edit: I have recently become aware that it is possible to specify autocorrelated residuals in nlme. However, at the moment it seems I cannot even get fixed weights to work. Advice on the correct syntax is welcome.</li>
<li>Non-linear mixed effects regression using nlmer in package LME4 and a custom likelihood for the logistic growth model. Works fairly well, but standard errors on the fixed effects get massive, probably due to the outliers. I also have the slight suspicion that some of the models fail to fit without error, as I sometimes get tiny random effects (about 10^10 smaller than with slightly simpler models). Since there is no reason to believe that residuals are normally distributed, it seems a bad idea to pursue this further.</li>
<li>Non-linear quantile regression using nlrq in package quantreg and SSlogis. Fits reliably and quickly, but percentile lines intersect. This means that an area containing 90% of the data is not fully contained in an area containing 95% of the data.</li>
<li>Non-parametric quantile regression using the LMS method with package VGAM. Even trivial models failed with obscure errors using this dataset. I believe the number of zeros in the dataset and / or the large range of the data while also getting close to zero may be an issue.</li>
<li>To complete this list, I should probably also mention the lqmm package for Linear Quantile Mixed Models, which I have not used yet. While the package cannot use non-linear models as far as I know, transforming the time variable may produce something reasonably close.</li>
</ul>

<p>I would appreciate feedback if these or any other method might be used to produce reasonably robust inference in this scenario. Maybe regression is not needed at all and another, possibly simpler method is sufficient. I'd be happy to provide an example dataset, if required, but think this question might also be of interest beyond the current dataset.</p>
"
"0.116907766928076","0.117579270250443"," 68388","<p>I have a great interest in learning new methods(at least to me) of variable selection in regards to binary logistic regression when I am working with over 500 potential predictor variables and have the duty of selecting 8 to 15 variables to build a parsimonious predictive model without using the notorious stepwise techniques. </p>

<p>With that being said, I was wondering if anyone has any experience using <code>proc factor</code> for binary logistic regression variable selection? I assume my factors will correlate, and thus I will use <code>promax</code> rotation, but with the results of the Exploratory Factor Analysis (EFA), I will simply retain the variable within each factor that has the highest loading on its own factor (latent variables models would confuse the hell out of the end-user of 99.999% of my models!) for further variable reduction through another technique such as <code>randomForest</code> until the number of variables is small enough to build a model that has fewer than 15 variables in it. </p>

<p>Does anyone have any thoughts in regards to this process? Any suggested readings or input would be greatly appreciated. Thanks!</p>
"
"0.126274820515127","0.127000127000191"," 68553","<p>I am trying to run a logistic regression analysis in R using the speedglm package. 
The data is CNVs (a type of genetic variant), whether that CNV occurred in a case or control and wether genes in a pathway is hit by the CNV or not (Pathway.hit), and how many genes were hit by the CNV that were not in the pathway (Pathway.out).
I run two models with and without the Pathway.hit covariate and compare to see if a pathway is preferentially hit by cases vs controls.  </p>

<p>the models and comparison of said are as follows:</p>

<pre><code>fit1 = speedglm(status~size+Pathway.out, data=cnvs, family=binomial('logit'))
fit2 = speedglm(status~size+Pathway.out+Pathway.hit, data=cnvs,family=binomial('logit'))
P.anova = 1-pchisq(abs(fit1$deviance - fit2$deviance), abs(fit1$df - fit2$df))
</code></pre>

<p>It seems to work okay for most data I throw at it, but in a few cases I get the error:</p>

<pre><code>Error in solve.default(XTX, XTz, tol = tol.solve) : 
  system is computationally singular: reciprocal condition number = 1.87978e-16
</code></pre>

<p>After some googling around I think I found what's causing the problem:</p>

<pre><code>by(cnvs$Pathway.hit, cnvs$status, summary)
cnvs$status: 1 (controls)
        0     1 
    13333     0 
    ------------------------------------ 
    cnvs$status: 2 (cases)
    0     1 
10258     2 
</code></pre>

<p>So here there no observations in controls and only 2 in cases. </p>

<p>If I use with normal glm method however, then it does not throw an error (but that of course doesn't necessarily mean the results will be meaningful). The reason I am using the speedglm package is that I have approximately 16,000 of these analyses to run, and using the base glm function for all 16,000 takes about 20 hours, where as I think speedglm can reduce it down to 8 or so.</p>

<p>So my question is, should I ignore those analyses which throw an error and list the results as NA as there were too few observations, or when speed glm fails should I retry with normal glm? In the above example there are 2 observations of the covariate in cases, but 0 in controls. Might this not be interesting? Would the analysis also fail if there were 0 in controls and 20 in cases - that would certainly be interesting would it not?</p>

<p>Thanks for the help in advance,
Cheers,
Davy</p>
"
"0.106721701823439","0.107334697685273"," 69088","<p>For those of you familiar with <a href=""http://en.wikipedia.org/wiki/Exploratory_factor_analysis"" rel=""nofollow"">Exploratory Factor Analysis</a> (EFA) and <a href=""http://en.wikipedia.org/wiki/Random_forest"" rel=""nofollow"">Random Forest</a> (RF), I have recently had an idea of combining these two methods to reduce the number of potential predictor variables for use in a parsimonious binary logistic regression model. For the purposes of this post, assume large <em>n</em> (200k or more) and 1000 potential predictor variables.</p>

<p>To employ this idea, the first step would be to perform an EFA with all potential predictor variables using <code>proc varclus</code>. Additionally, using <code>randomForest</code> to rank all potential predictor variables by <code>IncNodePurity</code> (<a href=""http://en.wikipedia.org/wiki/Gini_coefficient"" rel=""nofollow"">Gini Index</a>). </p>

<p>After these two methods are independently used, I propose retaining the variable with the largest <code>IncNodePurity</code> (from RF) within each factor (from EFA).</p>

<p>Does anyone have any thoughts/concerns with this methodology (or lack thereof) for feature selection? I am aware that this ""picking and choosing"" of methods may be complete garbage, but I had this random thought and wanted to share. Thanks!</p>
"
"0.234002811045853","0.244760769122387"," 71414","<p>I'm trying to use a zero-inflated gamma model (or a gamma 'hurdle' model). The model is a mixture of logistic regression and generalized linear modeling. I can do this analysis in two steps: 1) do a logistic regression against presence/absence data and then 2) Use a generalized linear model with a gamma distribution on the positive values. I'm trying to set the model up where, if y = 0, then E(y) = p, but if y > 0, then E(y) is gamma distributed.</p>

<p>I'm trying to set this up in BUGS/JAGS, because I've seen these models <a href=""http://stats.stackexchange.com/questions/15967/how-can-i-set-up-a-zero-inflated-poisson-in-jags"">worked before for poisson-distributions</a>. I tried to adapt the code from that link into a gamma distribution, but I can't quite seem to get it to work. I know the code won't work because my data have zeroes, and the likelihood function can't be evaluated with zeroes. However, even when restricted to positive values, I get errors for invalid parent values. Even so, I'm not even sure the model is specified correctly.</p>

<p>Here is the model:</p>

<pre><code># For the ones trick
C &lt;- 10000

# for every observation
for(i in 1:N){
    # log-likelihood of the observation from the gamma likelihood
    LogPos[i] &lt;- (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - N*shape[i]*log(scale[i]) - N*loggam(scale[i])
    #likelihood
    Lpos[i] &lt;- exp(LogPos[i])

    # redefine the shape and rate parameters as a function of the mean and sd
    shape[i] &lt;- pow(mu[i], 2) / pow(sd, 2)
    scale[i] &lt;- mu[i] / pow(sd, 2)

    # mu is a function of MTD: use the inverse link
    #mu[i] &lt;- 1/eta[i]
    mu[i] &lt;- beta0 + beta1*MTD[i]


    # zero-inflated part, where w[i] is the probability of being zero
    w[i] &lt;- exp(zeta[i]) / (1 + exp(zeta[i]))
    zeta[i] &lt;- gamma0 + gamma1*MPD[i]

    # ones trick
    p[i] &lt;- Lpos[i] / C
    ones[i] ~ dbern(p[i])

    # Full likelihood
    Lik[i] &lt;- Lpos[i]*(1 - w[i]) + equals(y[i], 0)*w[i]
  } 

# PRIORS
beta0 ~ dnorm(0, 0.001)
beta1 ~ dnorm(0, 0.001)

gamma0 ~ dnorm(0, 0.001)
gamma1 ~ dnorm(0, 0.001)

sd ~ dunif(0, 100)
</code></pre>

<p>Has anyone set a model up like this or have any advice on how to set it up correctly?</p>

<p><strong>UPDATE</strong></p>

<p>I've tried a new set of code that's similar, but slight different. I still have not gotten it to work</p>

<pre><code>model{

  # For the ones trick
  C &lt;- 10000

  # for every observation
  for(i in 1:N){

    # make a dummy variable that is 0 if y is &lt; 0.0001 and 1 if y &gt; 0.0001. This is essentially a presence
    # absence dummy variable
    z[i] &lt;- step(y[i] - 0.0001)

    # define the logistic regression model, where w is the probability of occurance.
    # use the logistic transformation exp(z)/(1 + exp(z)), where z is a linear function
    w[i] &lt;- exp(zeta[i]) / (1 + exp(zeta[i]))
    zeta[i] &lt;- gamma0 + gamma1*MPD[i]

    # define the gamma regression model for the mean. use the log link the ensure positive, non-zero mu
    mu[i] &lt;- exp(eta[i])
    eta[i] &lt;- beta0 + beta1*MTD[i]

    # redefine the mu and sd of the continuous part into the shape and scale parameters
    shape[i] &lt;- pow(mu[i], 2) / pow(sd, 2)
    scale[i] &lt;- mu[i] / pow(sd, 2)

    # for readability, define the log-likelihood of the gamma here
    logGamma[i] &lt;- (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - shape[i]*log(scale[i]) - loggam(scale[i])

    # define the total likelihood, where the likelihood is (1 - w) if y &lt; 0.0001 (z = 0) or
    # the likelihood is w * gammalik if y &gt;= 0.0001 (z = 1). So if z = 1, then the first bit must be
    # 0 and the second bit 1. Use 1 - z, which is 0 if y &gt; 0.0001 and 1 if y &lt; 0.0001
    logLik[i] &lt;- (1 - z[i]) * log(1 - w[i]) + z[i] * ( log(w[i]) + logGamma[i] )

    # Use the ones trick
    p[i] &lt;- logLik[i] / C
    ones[i] ~ dbern(p[i])
  } 

  # PRIORS
  beta0 ~ dnorm(0, 0.001)
  beta1 ~ dnorm(0, 0.001)

  gamma0 ~ dnorm(0, 0.001)
  gamma1 ~ dnorm(0, 0.001)

  sd ~ dgamma(1, 2)

}
</code></pre>

<p><strong>UPDATE 2</strong></p>

<p>I've gotten it to run by deleting the definition of logGamma[i] and putting it directly into the likelihood function, which now reads:</p>

<pre><code>logLik[i] &lt;- (1 - z[i]) * log(1 - w[i]) + z[i] * ( log(w[i]) + (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - shape[i]*log(scale[i]) - loggam(scale[i]) )
</code></pre>

<p>The problem was that JAGS was trying to evaluate the log likelihood at all observations first, resulting in NA's for the 0's. In the new way, it only evaluates it if z = 1 (I think). I'm still having trouble getting the parameter estimates to line up. For example, the gamma's are almost identical to a logistic regression of the same form (hooray!). But the betas are pretty far off from a gamma GLM of the positive values. I don't know if this is normal or not, but I suspect there are still problems with my model specification.</p>
"
"0.0954547919806695","0.0480015360737319"," 72516","<p>I am examining how English ivy affects the occurrence of a salamander species under cover objects (e.g., logs). Soil moisture is assumed to be the major factor that affect their occurrence. </p>

<p>My hypothesized pathway: The presence/absence of salamanders under cover objects is either a direct consequence of changes in ivy-induced abioitc environment (i.e., drier soil) or an indirect result of changes in prey community that resulted from altered abiotic factors. But, there are multiple factors, other than English ivy, that affect soil moisture.</p>

<p><img src=""http://i.stack.imgur.com/k65Ag.jpg"" alt=""enter image description here""></p>

<p>My questions are:</p>

<ol>
<li><p>I think that a path analysis is most suitable for testing my causal mechanisms. But, given a small sample size (n = 71), is a path analysis appropriate?</p></li>
<li><p>Another potential problem for a path analysis is that the effects of English ivy on soil moisture seem to depend on the other factors (e.g., the number of overstory trees), as shown below. Are there any way to account for such patterns in a path analysis?</p>

<p><img src=""http://i.stack.imgur.com/ArgZm.jpg"" alt=""The relationship between soil moisture and English ivy cover on cover objects (&quot;the number of overstory trees&quot; for the left graph) for different levels of the surrounding overstory trees (&quot;English ivy cover on cover objects&quot; for the left graph""></p></li>
<li><p>Are there any other analyses suitable for testing my hypothesized relationships? I am considering multiple (linear and logistic) regressions, but again my sample size is small <strong>AND</strong> regressions do not reflect my hypothesized causal relationships accurately.</p></li>
</ol>

<p>I am using R, so any recommended code would be greatly helpful (I am a relatively new R user, though). </p>
"
"0.143182187971004","0.144004608221196"," 73191","<p>For ordinary linear regression with Gaussian noise, it is easy to interpret the significance of a variable.  This is consistent with a partial F test.  The square of the t-test for the second variable equals to the partial F-test statistic, and their p-values are the same.</p>

<p>I wrote simple R codes to confirm this.</p>

<p>Is there something like this for logistic regression?  I thought/hoped that the likelihood ratio test would correspond to this, but no.  What should I do if the variable and the likelihood ratio test (of adding that particular variable) do not have the same (in)significant effect?</p>

<p>I appreciate your time and help,</p>

<pre><code>rm(list=ls(all=TRUE)) 
n = 100   ;       x1 = runif(n,-4,4)   ;       x2 = runif(n,6,10)
y = 3*x1 + 8*x2 + rnorm(n,2,4)
l1 = lm(y~x1)  ;  l2 = lm(y~x1+x2)  ;  a = anova(l1,l2)

summary(l1)$coeff
             Estimate Std. Error   t value     Pr(&gt;|t|)
(Intercept) 66.093853  1.0123131 65.289929 1.385202e-82
x1           3.199212  0.4292828  7.452458 3.664499e-11

summary(l2)$coeff
            Estimate Std. Error    t value     Pr(&gt;|t|)
(Intercept) 2.767750  2.7871368  0.9930441 3.231592e-01
x1          2.870897  0.1707022 16.8181610 1.648852e-30
x2          7.871545  0.3428392 22.9598753 5.370614e-41

(summary(l2)$coeff[3,3])^2
527.1559
&gt;     a 
    Analysis of Variance Table

Model 1: y ~ x1
Model 2: y ~ x1 + x2
  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    
1     98 9899.1                                  
2     97 1538.4  1    8360.6 527.16 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

&gt;     a$F ; a$Pr
   [1]       NA 527.1559
[1]           NA 5.370614e-41
&gt; 
&gt; 
&gt; 
&gt; rm(list=ls(all=TRUE)) 
&gt; n = 100
&gt; x1 = runif(n,-4,4)
&gt; x2 = runif(n,6,10)
&gt; 
&gt; y = rbinom(n,1,1/(1+exp(-3*x1 - 2*x2 + 20)))
&gt; 
&gt; l1 = glm(y~x1,family=binomial)
&gt; l2 = glm(y~x1+x2,family=binomial)
&gt; 
&gt; a = anova(l1,l2)
&gt; 
&gt; summary(l1)$coeff
                 Estimate Std. Error   z value     Pr(&gt;|z|)
    (Intercept) -2.988069   0.812041 -3.679702 2.335068e-04
    x1           2.115333   0.498431  4.243984 2.195858e-05
    &gt; summary(l2)$coeff
              Estimate Std. Error   z value     Pr(&gt;|z|)
(Intercept) -17.215960  5.5710699 -3.090243 0.0019999276
x1            3.048657  0.8618367  3.537395 0.0004040949
x2            1.675323  0.5976386  2.803238 0.0050592272
&gt; 
&gt; (summary(l2)$coeff[3,3])^2
    [1] 7.858145
    &gt; 
    &gt; l1$deviance -  l2$deviance
    [1] 13.65371
    &gt; pchisq(l1$deviance -  l2$deviance,df=1)
[1] 0.9997802
&gt; 
&gt; a
Analysis of Deviance Table

Model 1: y ~ x1
Model 2: y ~ x1 + x2
  Resid. Df Resid. Dev Df Deviance
1        98     45.534            
2        97     31.880  1   13.654
&gt; a$F
    NULL
    &gt; a$Pr
    NULL
</code></pre>
"
"0.0826662747682189","0.083141099321054"," 74549","<p>This is from the book <em>The statistical sleuth--A course in methods of Data analysis</em> Chapter 20, Exercise 12(c)-(e). I am using logistic regression to predict carrier with possible predictors <code>CK</code> and <code>H</code>. Here is my solution:</p>

<pre><code>Carrier &lt;- c(0,0,0,0,0,1,1,1,1,1)  
CK      &lt;- c(52,20,28,30,40,167,104,30,65,440)  
H       &lt;- c(83.5,77,86.5,104,83,89,81,108,87,107)  
logCK   &lt;- log(CK)  
fit4    &lt;- glm(Carrier~logCK+H, family=""binomial"", control=list(maxit=100))  
Warning message:  
glm.fit: fitted probabilities numerically 0 or 1 occurred   
summary(fit4)
## 
## Call:
## glm(formula = Carrier ~ logCK + H, family = ""binomial"", control = list(maxit = 100))
## 
## Deviance Residuals: 
##        Min          1Q      Median          3Q         Max  
## -1.480e-05  -2.110e-08   0.000e+00   2.110e-08   1.376e-05  
##
## Coefficients:  
##              Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)   -2292.8  4130902.8  -0.001        1  
## logCK           315.6   589675.2   0.001        1  
## H                11.5    21279.6   0.001        1
</code></pre>

<p>This results appear to be weird, because it seems that all coefficients are not significant.  Also the next question is to do a drop-in-deviance test for this full model and the reduced model that neither of <code>logCK</code> and <code>H</code> is useful predictor. I get:  </p>

<pre><code>fit5 &lt;- glm(Carrier~1, family=""binomial"")  
1-pchisq(deviance(fit5)-deviance(fit4), df.residual(fit5)-df.residual(fit4))  
## [1] 0.0009765625
</code></pre>

<p>So the p-value indicates that at least one of <code>logCK</code> and <code>H</code> is useful. Then I'm stuck at the next question, it asks me to calculate odds ratio for a woman with (CK, H)=(300,100) over one with (CK, H)=(80, 85).  </p>

<p>But how can I get a meaningful result with all coefficients in this model ranging so wildly? Is there anything wrong with the way I did this logistic regression? Are there any remedial measures?  </p>
"
"0.143182187971004","0.144004608221196"," 76490","<h2>Background</h2>

<p>A laboratory wants to evaluate whether a certain form of <a href=""http://en.wikipedia.org/wiki/Polyacrylamide_gel_electrophoresis"" rel=""nofollow"">gel electrophoresis</a> is suited as a classification method for the quality of a certain substance. Several gels were loaded, each with a clean sample of the substance and with a sample that contains impurities. In addition, a molecular marker was also loaded which serves as a reference. The following picture illustrates the setup (the picture doesn't show the actual experiment, I have taken it from Wikipedia for illustration):</p>

<p><img src=""http://i.stack.imgur.com/vC53q.png"" alt=""Example of a gel electrophoresis""></p>

<p>Two parameters were measured for each gel and each lane:</p>

<ol>
<li>The <strong>molecular weight</strong> (that is how ""high up"" a compound wandered during the electrophoresis)</li>
<li>The <strong>relative quantity.</strong> The total quantity of each lane is normalized to 1 and the density of each band is measured which results in the relative quantity of each band.</li>
</ol>

<p>A scatterplot of the relative quantity vs. molecular weight is then produced which could look something like this (it's artificial data):</p>

<p><img src=""http://i.stack.imgur.com/ndzh9.png"" alt=""Example scatterplot""></p>

<p>This graphic can be read as follows: Both the ""good"" (blue points) and ""impure"" (red points) substance exhibit two bands, one at around a molecular weight of 120 and one at around 165. The bands of the ""impure"" substance at a molecular weight around 120 are considerably less dense than the ""good"" substance and can be well distinguished.</p>

<hr>

<h2>Goal</h2>

<p>The goal is to determine two boxed (see graphic below) which determine a ""good"" substance. These boxes will then be used for classification of the substance in the future into ""good"" and ""impure"". If a substance exhibits lanes that fall within the boxes it is classified as ""good"" and else as ""impure"".</p>

<p>These decision-rules should be <em>simple</em> to apply for someone in the laboratory. That's why it should be boxes instead of curved decision boundaries.</p>

<p>False-negatives (i.e. classify a sample as ""impure"" when it's really ""good"") are considered worse than false-positives. That is, an emphasis should be placed on the <a href=""http://en.wikipedia.org/wiki/Sensitivity_and_specificity#Sensitivity"" rel=""nofollow"">sensitivity</a>, rather than on the <a href=""http://en.wikipedia.org/wiki/Sensitivity_and_specificity#Specificity"" rel=""nofollow"">specificity</a>.</p>

<p><img src=""http://i.stack.imgur.com/vhzEW.png"" alt=""Example decision boxed""></p>

<hr>

<h2>Question</h2>

<p>I'm am no expert in machine learning. I know, however, that there are quite a few machine learning algorithms/techniques that could be helpful: $k$-nearest neighbors (e.g. <code>knn</code> in <code>R</code>), classification trees (e.g. <code>rpart</code> or <code>ctree</code>), support vector machines (<code>ksvm</code>), logistic regression, boosting and bagging methods and many more.</p>

<p>One problem of many of those algorithms is that they don't provide a simple ruleset or linear boundaries. In addition, the <strong>sample size</strong> is around <strong>70.</strong></p>

<p>My questions are:</p>

<ul>
<li>Has anyone an idea of how to proceed here?</li>
<li>Does it make sense to split the dataset into training- and test-set?</li>
<li>What proportion of the data should the training set be (I thought around a 60/40-split).</li>
<li>What, in general, is the workflow for such an analysis? Something like: Splitting dataset -> fit algorithm on the training set -> predict outcome for the test set?</li>
<li>How to avoid overfitting (i.e. boxes that are too small)?</li>
<li>What is a good statistic to assess the predictive performance in this case? AUC? Accurary? Positive predictive value? <a href=""http://en.wikipedia.org/wiki/Matthews_correlation_coefficient"" rel=""nofollow"">Matthews correlation coefficient</a>?</li>
</ul>

<p>Assume that I'm familiar with <code>R</code> and the <code>caret</code> package. Thank you very much for you time and help.</p>

<hr>

<h2>Example data</h2>

<p>Here is an example dataset.</p>

<pre><code>structure(list(mol.wt = c(125.145401455869, 118.210252208676, 
165.048583787746, 126.003687476776, 170.149347112565, 127.761533014759, 
155.523172614798, 120.094514977175, 161.234986765321, 168.471542655269, 
156.522990530521, 154.377948321209, 165.365756398877, 167.965538771316, 
116.132241687833, 115.143539160903, 156.696830822196, 162.578494491556, 
136.830624758899, 123.886594633942, 124.247484227948, 126.257226352824, 
160.684010454816, 166.618872115047, 126.599387146887, 165.690375912529, 
159.786861142652, 114.520735974329, 125.753594471656, 157.551537154148, 
157.320636890647, 171.5759136115, 158.580005438661, 125.647463565197, 
130.404710783509, 127.128218318572, 162.144126888907, 161.804616951055, 
167.917268243627, 168.582197247178), rel.qtd = c(57.68339235957, 
54.0514508510085, 25.0703901938793, 37.6933881305906, 36.6853653723001, 
53.6650555524679, 52.268438087776, 52.8621831466857, 43.1242291166037, 
46.6771236380788, 38.0328239221277, 40.0454611708371, 44.6406366176158, 
40.8238699987682, 51.9464749018547, 54.0302533272953, 37.9792331383524, 
48.3853988095525, 38.2093977349102, 42.2636098418388, 42.9876895407144, 
40.8018728193786, 40.1097096927465, 38.7432550253867, 39.2633283608111, 
43.4673723102812, 53.3740718733815, 49.1067921475768, 52.3002598744634, 
44.9847844953241, 44.3014423068017, 44.0191971364465, 47.0805245356855, 
55.0124134796556, 57.9938440244052, 62.8314454977068, 45.8093815891894, 
43.2300677500964, 39.4801550161538, 51.6253515591173), quality = structure(c(2L, 
2L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 1L), .Label = c(""bad"", ""good""), class = ""factor"")), .Names = c(""mol.wt"", 
""rel.qtd"", ""quality""), row.names = c(10L, 14L, 47L, 16L, 57L, 
54L, 45L, 12L, 43L, 67L, 25L, 21L, 1L, 55L, 20L, 22L, 37L, 15L, 
8L, 38L, 46L, 64L, 51L, 65L, 52L, 61L, 63L, 32L, 50L, 27L, 19L, 
69L, 23L, 42L, 6L, 48L, 11L, 13L, 5L, 71L), class = ""data.frame"")
</code></pre>
"
"0.0477273959903348","0.0480015360737319"," 78360","<p>I need your help with a Statistical Learning homework in R.
I have to perform classification over this dataset: <a href=""http://archive.ics.uci.edu/ml/machine-learning-databases/mammographic-masses/"" rel=""nofollow"">mammographic masses</a> predicting Severity (0=""not severe"",1 = ""severe) using these predictors:</p>

<ul>
<li>Age (quantitative)</li>
<li>Margin (qualitative)</li>
<li>Shape (qualitative)</li>
</ul>

<p>Everything is fine and understandable when I use logistic regression, but I don't know if it's possible to run QDA (or linear discriminant analysis either), since two of the variables are qualitative.</p>
"
"0.09742313910673","0.0783861801669621"," 83908","<p>I am working in program R. I am modeling the incidence of flight in a seabird in relation to distance to the nearest ship (potential disturbance, range = 0 to 74 km from the bird). 1= flight during observation, 0 = no flight. The bird does fly with some unknown probability when no ships are present or really ""far"" away. I am trying to find this really far distance and associated probability of flight using binary logistic regression.</p>

<p>Model = Flight ~ ship distance. Other variables were explored but fell out with stepwise selection.</p>

<p>During exploratory analysis I truncated the data down only looking at smaller distances from the ship (20, 15, 10 km). These models are highly significant and predict that as the ship gets closer the probability of flight increases. However when I include all the data (out to 74 km) the intercept is significant (and predicts the true % of observed flight events) but the slope term is non-significant. </p>

<p>Can I use a weighting scheme to give more weight to observations when the ship was closer?</p>

<p>Thanks.</p>

<p>Edit: I am working through the suggestions made by @Scortchi and @Underminer. Here is a plot of a loess smooth on the observed data to better help visualize the pattern. </p>

<p><img src=""http://i.stack.imgur.com/ZabQh.jpg"" alt=""Loess smooth of probability of flight as a function of distance to nearest ship""></p>

<p>The distance to the ship data does not discriminate between approaching ships and departing ships it is just a straight line measure to the nearest ship. The dip in the probability of flight at 8.5 I believe can be attributed to ""unaffected"" birds that did not fly as the ship passed by them. So as the ship departs and gets further from the observation site we were more like to be observing birds that for whatever reason did not fly when the ship passed and are less likely to fly for ""naturally occurring"" reason. As additional birds fill back into the observation area the ""baseline"" flushing rate is resumed and birds start to fly at ""normal"" probabilities. </p>
"
"0.0954547919806695","0.0960030721474639"," 87956","<p>I have a repeated-measures experiment where the dependent variable is a percentage, and I have multiple factors as independent variables. I'd like to use <code>glmer</code> from the R package <code>lme4</code> to treat it as a logistic regression problem (by specifying <code>family=binomial</code>) since it seems to accommodate this setup directly.</p>

<p>My data looks like this:</p>

<pre><code> &gt; head(data.xvsy)
   foldnum      featureset noisered pooldur dpoolmode       auc
 1       0         mfcc-ms      nr0       1      mean 0.6760438
 2       1         mfcc-ms      nr0       1      mean 0.6739482
 3       0    melspec-maxp    nr075       1       max 0.8141421
 4       1    melspec-maxp    nr075       1       max 0.7822994
 5       0 chrmpeak-tpor1d    nr075       1       max 0.6547476
 6       1 chrmpeak-tpor1d    nr075       1       max 0.6699825
</code></pre>

<p>and here's the R command that I was hoping would be appropriate:</p>

<pre><code> glmer(auc~1+featureset*noisered*pooldur*dpoolmode+(1|foldnum), data.xvsy, family=binomial)
</code></pre>

<p>The problem with this is that the command complains about my dependent variable not being integers:</p>

<pre><code>In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>and the analysis of this (pilot) data gives weird answers as a result.</p>

<p>I understand why the <code>binomial</code> family expects integers (yes-no counts), but it seems it should be OK to regress percentage data directly. How to do this?</p>
"
"0.108235560441538","0.108857251714449"," 88796","<p>I am exploring the probability of flight in a seabird (1=flight, 0=no flight) using binomial logistic regression. My predictors are distance to a disturbance (continuous), hour of the day (continuous), site (factor), season (factor), sea state (dichotomous), and group size (dichotomous). I have explored the use of piecewise regression in relation to the distance to a disturbance as this variable spans a large range (out to 74 km) and there is no way that this is affecting flight at the largest distance. </p>

<p>When the model was fit with just reference to distance to a disturbance within the R program 'segmented' it points to a break in the data at 3.9 km. The slope up to this distance is negative and statistically significant while the slope estimate for distances further than 3.9 km is estimated to be 0 and non-significant.</p>

<p>I would like to now sequentially add in additional terms to the model to see if there is any reduction in the deviance when the additional terms are added. Can a term be added just to the section before or after the break? I cannot seem to find any information in the literature regarding this </p>

<p>My questions is can I do this? Or do I need to split the data into two chunks, before and after the breakpoint and explore additional terms this way.</p>

<p>Also the motivation to do this analysis is more to find and identify the breakpoint. Instead of adding in terms after I assess the breakpoint should I explore the breakpoint within a the model including all the terms? Would this find the break in the data in relation to the other terms or does the algorithm completely ignore the other terms in the model when searching for a break in the distance to disturbance variable.</p>

<p>Thanks, </p>
"
"0.150927278118246","0.151794185179729"," 89692","<p>My data has 3 major inputs: <code>BLDDAY</code> (a factor), <code>BLDMNT</code> (a factor), and <code>D_BLD_SER</code> (days as an integer variable).  The output is whether input variable has any impact on failure.  My model is: <code>model = glm(FAILED~BLDDAY+BLDMNT+D_BLD_SER, family=""binomial"", data=data_list)</code>.  (I used <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">UCLA's statistics help site's guide to logistic regression in R</a> to build this model.)  </p>

<p>Output: </p>

<pre><code>Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.3282  -0.9123  -0.8128   1.4056   2.1053  

Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)     -0.7672583  0.1317132  -5.825 5.70e-09 ***
BLDDAYMonday    -0.1545646  0.0839380  -1.841  0.06556 .  
BLDDAYSaturday  -0.1257976  0.2028259  -0.620  0.53511    
BLDDAYSunday    -0.1183008  0.1868713  -0.633  0.52669    
BLDDAYThursday  -0.2007452  0.0772653  -2.598  0.00937 ** 
BLDDAYTuesday    0.0480453  0.0758603   0.633  0.52651    
BLDDAYWednesday -0.0358585  0.0760027  -0.472  0.63707    
BLDMNTAug        0.3009445  0.1405545   2.141  0.03226 *  
BLDMNTDec        0.5562170  0.1338467   4.156 3.24e-05 ***
BLDMNTFeb        0.3334978  0.2133475   1.563  0.11801    
BLDMNTJan        0.4076504  0.2277978   1.790  0.07353 .  
BLDMNTJul        0.1306585  0.1415302   0.923  0.35591    
BLDMNTJun       -0.0357361  0.1428105  -0.250  0.80241    
BLDMNTMar        0.4570491  0.1949815   2.344  0.01907 *  
BLDMNTMay       -0.2292620  0.1614577  -1.420  0.15562    
BLDMNTNov        0.3060012  0.1334034   2.294  0.02180 *  
BLDMNTOct        0.2390501  0.1341877   1.781  0.07484 .  
BLDMNTSep        0.2481405  0.1384901   1.792  0.07317 .  
D_BLD_SER       -0.0020960  0.0003367  -6.225 4.82e-10 ***

(Dispersion parameter for binomial family taken to be 1)
    Null deviance: 10288  on 8182  degrees of freedom
Residual deviance: 10154  on 8164  degrees of freedom
AIC: 10192
Number of Fisher Scoring iterations: 4
</code></pre>

<p>The ANOVA table is the following:</p>

<pre><code>anova(model, test=""Chisq"")
Analysis of Deviance Table
Model: binomial, link: logit
Response: FAILED
Terms added sequentially (first to last)

          Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                       8182      10288              
BLDDAY     6   20.392      8176      10268  0.002357 ** 
BLDMNT    11   70.662      8165      10197 9.142e-11 ***
D_BLD_SER  1   43.797      8164      10154 3.642e-11 ***
</code></pre>

<p>My questions are:</p>

<ol>
<li><p>Although the p-values for all three components are less than 0.05, which can be considered as significant, the deviance reduced due to each component is less than 1% of the total deviance. <strong>Normally the interpretation of output like this is input parameter affects output and it's better to use this parameter then using noting.</strong> But does it really make sense of taking this parameter as significant input?</p></li>
<li><p>The p-values for <code>BLDDAY</code> and <code>BLDMNT</code> given by <code>anova()</code> is the overall p-value,  which is significant, but <code>summary()</code> gives detailed impact of each factor level. If I consider the p-values for each factor overall <code>BLDDAY</code> is significant but individually only <code>BLDDAYThursday</code> is significant. I am bit confused not as whether to consider <code>BLDDAY</code> as significant input, or Thursday only, or Thursday &amp; Friday both.</p></li>
</ol>
"
"0.118119278735995","0.135768846660426"," 91903","<p>Is the probability calculated by a logistic regression model (the one that is logit transformed) the fit of cumulative distribution function of successes of original data (ordered by the X variable)?</p>

<p><strong>EDIT:</strong> In other words - how to plot the probability distribution of the original data that you get when you fit a logistic regression model?</p>

<p>The motivation for the question was Jeff Leak's example of regression on the Raven's score in a game and whether they won or not (from Coursera's Data Analysis course). Admittedly, the problem is artificial (see @FrankHarrell's comment below). Here is his data with a mix of his and my code:</p>

<pre><code>download.file(""http://dl.dropbox.com/u/7710864/data/ravensData.rda"", 
              destfile=""ravensData.rda"", method=""internal"")
load(""ravensData.rda"")

plot(ravenWinNum~ravenScore, data=ravensData)
</code></pre>

<p><img src=""http://i.stack.imgur.com/Cr5ka.png"" alt=""enter image description here"">  </p>

<p>It doesn't seem like good material for logistic regression, but let's try anyway:</p>

<pre><code>logRegRavens &lt;- glm(ravenWinNum ~ ravenScore, data=ravensData, family=binomial)
summary(logRegRavens)
# the beta is not significant

# sort table by ravenScore (X)
rav2 = ravensData[order(ravensData$ravenScore), ]

# plot CDF
plot(sort(ravensData$ravenScore), cumsum(rav2$ravenWinNum)/sum(rav2$ravenWinNum), 
         pch=19, col=""blue"", xlab=""Score"", ylab=""Prob Ravens Win"", ylim=c(0,1), 
         xlim=c(-10,50))
# overplot fitted values (Jeff's)
points(ravensData$ravenScore, logRegRavens$fitted, pch=19, col=""red"")
# overplot regression curve
curve(1/(1+exp(-(logRegRavens$coef[1]+logRegRavens$coef[2]*x))), -10, 50, add=T)
</code></pre>

<p>If I understand logistic regression correctly, R does a pretty bad job at finding the right coefficients in this case.  </p>

<p><img src=""http://i.stack.imgur.com/Cb6o8.png"" alt=""enter image description here""></p>

<ul>
<li>blue = original data to be fitted, I believe (CDF)  </li>
<li>red = prediction from the model (fitted data = projection of original data onto regression curve)</li>
</ul>

<p><strong>SOLVED</strong><br>
 - lowess seems to be a good non-parametric estimator of the original data = what is being fitted (thanks @gung). Seeing it allows us to choose the right model, which in this case would be adding squared term to the previous model (@gung)<br>
 - Of course, the problem is pretty artificial and modelling it rather pointless in general (@FrankHarrell)<br>
 - in regular logistic regression it's not CDF, but point probabilities - first pointed out by @FrankHarrell; also my embarrassing inability to calculate CDF pointed out by @gung.</p>
"
"0.150927278118246","0.151794185179729"," 93454","<p><strong>Base Data</strong>: I have ~1,000 people marked with assessments: '1,' [good] '2,' [middle] or '3' [bad] -- these are the values I'm trying to predict for people in the future. In addition to that, I have some demographic information: gender (categorical: M / F), age (numerical: 17-80), and race (categorical: black / caucasian / latino).</p>

<p><strong>I mainly have four questions:</strong></p>

<ol>
<li><p>I was initially trying to run the dataset described above as a multiple regression analysis. But I recently learned that since my dependent is an ordered factor and not a continuous variable, I should use ordinal logistic regression for something like this. I was initially using something like <code>mod &lt;- lm(assessment ~ age + gender + race, data = dataset)</code>, can anybody point me in the right direction?</p></li>
<li><p>From there, assuming I get coefficients I feel comfortable with, I understand how to plug solely numerical values in for x1, x2, etc. -- but how would I deal with race, for example, where there are multiple responses: black / caucasian / latino? So if it tells me the caucasian coefficient is 0.289 and somebody I'm trying to predict is caucasian, how do I plug that back in since the value's not numerical?</p></li>
<li><p>I also have random values that are missing -- some for race, some for gender, etc. Do I have to do anything additional to make sure this isn't skewing anything? (I noticed when my dataset gets loaded into R-Studio, when the missing data gets loaded as <code>NA</code>, R says something like <code>(162 observations deleted due to missingness)</code> -- but if they get loaded as blanks, it does nothing.)</p></li>
<li><p>Assuming all of this works out and I have new data with gender, age, and race that I want to predict on -- is there an easier way in R to run all of that through whatever my formula with new coefficients turns out to be, rather than doing it manually? (If this question isn't appropriate here, I can take it back to the R forum.)</p></li>
</ol>
"
"0.202490192118848","0.181025128880568"," 94468","<p>I am completely out of my depth on this, and all the reading I try to do just confuses me. I'm hoping you can explain things to me in a way that makes sense. (As always seems to be the case, ""It shouldn't be this hard!"")</p>

<p>I'm trying to help a student who is looking at the effect of social systems on prevalence of diseases in various canid host species. We want to consider social system (e.g., group-living vs. solitary) as a fixed effect, and host species as a random effect nested within social system (i.e., each species only ever has one social system type).</p>

<p>My understanding is that the best way to do this would be to do a mixed-effects logistic regression. We've done this, and it works, and we were happy. Unfortunately, her advisor is insisting that she calculate the amount of variation due to social system vs. host species vs. residual. I can't figure out how to do this via mixed-effects logistic regression, and <a href=""http://stats.stackexchange.com/questions/93450/partitioning-variance-from-logistic-regression"">my previous question on this topic</a> went unanswered.</p>

<p>Her advisor suggested doing ANOVA instead, logit-transforming disease prevalence values (the fraction of each population that is infected). This presented a problem because some of the prevalence values are 0 or 1, which would result in $-\infty$ or $\infty$ once logit-transformed. Her advisor's ""solution"" was to just substitute $-5$ and $5$ for $-\infty$ or $\infty$, respectively. This feels really kludgey and makes me cringe pretty hard. But he's the one grading her, and at this point I just want to be done with this, so if he's fine with it then whatever.</p>

<p>We are using R for this analysis. The code can be downloaded <a href=""https://dl.dropboxusercontent.com/u/2225877/to_SECV/my_code.R"">here</a>, and the input data <a href=""https://dl.dropboxusercontent.com/u/2225877/to_SECV/my_data.csv"">here</a>. The data file includes data on two different pathogens (A and B), which we are analyzing separately (as shown in the code).</p>

<p>Here's the ANOVA setup we made for Pathogen B:</p>

<pre><code>mod1.lm &lt;- lm(Seroprevalence_logit ~ Social.System + Social.System/Host.Species,
              data = prev_B)
print(mod1.anova &lt;- anova(mod1.lm))
</code></pre>

<p>This leads to my first question: <strong>Is this correct and appropriate?</strong> Factors to consider:</p>

<ul>
<li>We want to have a Model II (random effect) variable nested within a Model I (fixed effect) variable.</li>
<li>Not every social system has the same number of host species nested within it.</li>
<li>Not every host species has the same number of populations examined.</li>
<li>Not every population examined had the same number of individuals (column N_indiv in mydata.csv). This is more of a weighting problem than something more fundamental, I think.</li>
</ul>

<p>My next question, and the main one of this post, is: <strong>How do I partition the variance?</strong> Here's what we were thinking:</p>

<pre><code>MS_A &lt;- mod1.anova$""Mean Sq""[1]
MS_BinA &lt;- mod1.anova$""Mean Sq""[2]
MS_resid &lt;- mod1.anova$""Mean Sq""[3]
n &lt;- length(unique(prev_A$Social.System))
r &lt;- length(unique(prev_A$Host.Species))
VC_A &lt;- (MS_A - MS_BinA)/(n*r)
VC_BinA &lt;- (MS_BinA - MS_resid)/n
VC_resid &lt;- MS_resid
</code></pre>

<p>Unfortunately, this results in sadness using the ANOVA specification I detailed above. Here are the results for Pathogen B:</p>

<ul>
<li><code>VC_A</code> (i.e., Social.System): $-1.48$</li>
<li><code>VC_BinA</code> (i.e., Host.Species): $13.8$</li>
<li><code>VC_resid</code>: $5.57$</li>
</ul>

<p>Research leads me to believe that this should result in variance component percentages of 0%, 71.3%, and 28.7%, respectively. However, this is unsatisfying for two reasons:</p>

<ul>
<li>The p-value for Social.System from the ANOVA was ~$0.025$, suggesting that it should account for at least <em>some</em> of the observed variance. (Host.Species had a p-value of ~$3*10^{-5}$.)</li>
<li>I'm concerned that a negative variance component might be a red flag for something.</li>
</ul>

<p>Please, any assistance you can render on either of these questions would be greatly appreciated. I TA'd an undergraduate course on biostatistics, so I've got some background, but I just can't seem to figure out these specific issues. Thanks in advance.</p>
"
"NaN","NaN"," 95203","<p>I need to perform stepwise binary logistic regression (The horror! The horror!) on 1.5 million observations.  This takes far too long in SAS, so I'm wondering if I can use R to process it in a multicore environment.  Apparently package gmulti (<a href=""http://www.jstatsoft.org/v34/i12/paper"" rel=""nofollow"">http://www.jstatsoft.org/v34/i12/paper</a>) will do the trick, but it's not clear to me if it will do that outside of its genetic algorithm.  That still might work for me, but I don't have a large number of variables (about 30) so it's not necessary.  As long as the results of the brute force and ga approach could be assured to be similar, then I might try it.  However, I see others have had problems getting the parallel feature to run: <a href=""https://stat.ethz.ch/pipermail/r-help/2013-April/351820.html"" rel=""nofollow"">https://stat.ethz.ch/pipermail/r-help/2013-April/351820.html</a>.  Any other suggestions 
on how to parallelize logistic regression in R?  A web search turned up a couple of papers, but not much that seemed specific to R.  And please spare me a lecture about stepwise regression-I'm very well aware of the pitfalls.  I'm replicating someone else's analysis.  I'm using a Windows 64 bit system.</p>
"
"0.165332549536438","0.166282198642108"," 95451","<p>I've been having an argument with a friend of mine, and it's very possible I'm wrong here.</p>

<p>We are performing binary logistic regression on a dataset with 10000 observations, classifying action as ""good"" or bad"".  There are two independent variables (x1, x2), and class variable (y, with values ""good"" or ""bad"").  In this dataset, we  have 7,500 observations classified as ""bad"" and 2,500 classified as ""good"".  This is because there are several different ways for a user to perform a ""bad"" action, but only one way for them to perform a ""good"" action.</p>

<p>We are doing our analysis in R using the <code>glm()</code> function.</p>

<p>We create training data by randomly sampling 7,500 observations from the dataset, and create test data from the other 2,500 observations.  we then build a model using binary logistic regression on the training data, then test it on the test data.  The accuracy of our model is 75%.</p>

<p><strong>Can we say our model is better than guessing?</strong></p>

<p>He says that this model is no better than guessing.  Even though the error rate is better than 50%, because the original data had a prevalence of ""bad"" classifiers, we would need our model to predict better than 75% in order to say it performs better than random guessing.</p>

<p>I disagree...but I can't defend my point with anything other than ""that doesn't seem right"".  Can someone shed some light on the correct interpretation, and the reason for it?</p>
"
"0.0477273959903348","0.0480015360737319"," 96006","<p>Let's say I have 10 positives out of 1000 observations. I'd like to run <code>glm</code> on the 10 positives and a sample of 10 non-positives (so a total of 20 records in the dataset going into the analysis). To maintain ratios, the weighing scheme is set to 1 for positives and 99 for non-positives, and <code>weights = c(rep(1,10),rep(99,10))</code> if the 10 positives come first in the data, followed by the 10 sampled non-positives:</p>

<pre><code> glm( is_positive ~ . , data = D_sampled , family = binomial , weights = c(rep(1,10),rep(99,10)) )
</code></pre>

<p>Is this the right way to run an oversampled logistic with 'glm' (keeping in mind that the intercept will have to be corrected)? </p>
"
"0.116907766928076","0.117579270250443"," 99254","<p>I have a large set of time series data, consisting of series from two different conditions, the averages of which are shown below.</p>

<p>I would like to fit a model to this data, to test that a) the peak value is greater in the 'conflict' condition, and b) the peak occurs earlier in this condition.</p>

<pre><code>ggplot(data, aes(x=Time, y=Variable, colour=Condition)) 
    + stat_summary(fun.data=mean_se, geom=""pointrange"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/3NQ8T.png"" alt=""enter image description here""></p>

<p>From my own research on this, I know that:</p>

<ul>
<li>Growth curve analysis is the usual way of modelling time series data like this, but I can't figure out how I would fit a polynomial for this shape of curve.
<ul>
<li>I have some experience fitting GCA models using <code>lme4</code> in R, mostly following <a href=""http://www.danmirman.org/gca"" rel=""nofollow"">Dan Mirman's tutorials</a>, but I'm still learning.</li>
</ul></li>
<li>Curves of this kind are referred to as Hubbert curves, and typically used to model oil production, as a symmetric logistic curve up and down.</li>
<li>R package <code>grofit</code> is maybe useful for analyses of this kind, although I would rather know I'm barking up the right tree before investing time in learning how to use this.</li>
</ul>

<p>Can anyone point me in the right direction here?</p>
"
"0.0954547919806695","0.0960030721474639","102689","<p>I have a problem with some analysis I need to do.</p>

<p>I have a series of regressions. Some of the predictors of these regression are categorical with multiple levels. I performed regressions, both linear and logistic, choosing a baseline for these category according to various factors.</p>

<p>The problem is that my colleagues asked not only for a confrontation of the factors to a baseline but also a pairwise confrontation. Like you it's used to do with a post-hoc test for ANOVA (they are pretty new to regressions and their benefits).</p>

<p>How should I approach this?
I thought of some solutions:</p>

<ul>
<li>Subsetting: That is subset the data to include two factors at time, and therefore repeating the regression once per every subset.</li>
<li>Splitting: Splitting the category column in a column for every factor and put 0 and 1 as levels. This approach can furthermore be conducted in two ways:
<ul>
<li>Putting all the new columns in the regression (minding that they are mutually exclusive).</li>
<li>Putting one column at time, multiplying the regressions.</li>
</ul></li>
</ul>

<p>Which approach would you suggest, minding statistical correctness and workload?</p>

<p>Especially, what's the conceptual difference between the three methods?</p>

<p>Thanks a lot!</p>
"
"0.135834550306421","0.151794185179729","102892","<p>My situation:</p>

<ul>
<li>small sample size: 116</li>
<li>binary outcome variable</li>
<li>long list of explanatory variables: 44</li>
<li>explanatory variables did not come from the top of my head; their choice was based on the literature.</li>
</ul>

<p>Statistical test chosen: logistic regression</p>

<p>I need to find the variables that best explain variations in the outcome variable (I am not interested in making predictions).</p>

<p>The problem: This question is a follow-up on the 2 questions listed below. From them, I got that performing automated stepwise regression has its downsides. Anyway, it seems that my sample size would be too small for that. It seems that my sample is also too small to enter all variables at once (using the SPSS 'Enter' method). This leaves me with my issue unresolved: how can I select a subset of variables from my original long list in order to perform multivariate logistic regression analysis?</p>

<p>UPDATE1: I am not an statistician, so I would appreciate if jargons can be reduced to the minimum. I am working with SPSS and am not familiar with other packages, so options that could be run with that software would be highly preferable.</p>

<p>UPDATE2: It seems that SPSS does not support LASSO for logistic regression. So following one of your suggestions, I am now struggling with R. I have passed through the basics, and managed to run a univariate logistic regression routine successfully using the glm code. But as I tried glmnet with the same dataset, I am receiving an error message. How could I fix it? Below is the code I used, followed by the error message:</p>

<pre><code>data1 &lt;- read.table(""C:\\\data1.csv"",header=TRUE,sep="";"",na.string=99:9999)

y &lt;- data1[,1]

x &lt;- data1[,2:45]

glmnet(x,y,family=""binomial"",alpha=1)  

**in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs,  : 
(list) object cannot be coerced to type 'double'**
</code></pre>

<p>UPDATE3: I got another error message, now related to missing values. My question concerning that matter is <a href=""http://stats.stackexchange.com/questions/104194/how-to-handle-with-missing-values-in-order-to-prepare-data-for-feature-selection"">here</a>. </p>

<ul>
<li><a href=""http://stats.stackexchange.com/questions/88482/can-univariate-linear-regression-be-used-to-identify-useful-variables-for-a-subs"">Can univariate linear regression be used to identify useful variables for a subsequent multiple logistic regression?</a></li>
<li><a href=""http://stats.stackexchange.com/questions/20836/algorithms-for-automatic-model-selection/20856#20856"">Algorithms for automatic model selection</a></li>
</ul>
"
"0.143182187971004","0.144004608221196","104194","<p>My situation:</p>

<ul>
<li>small sample size: 116 </li>
<li>binary outcome variable</li>
<li>long list of explanatory variables: 44</li>
<li>explanatory variables did not come from the top of my head; their choice was based on the literature.</li>
<li>most cases in the sample and most variables have missing values.</li>
</ul>

<p>Approach to feature selection chosen: LASSO</p>

<p>R's glmnet package won't let me run the glmnet routine, apparently due to the existence of missing values in my data set. There seems to be various methods for handling missing data, so I would like to know:</p>

<ul>
<li>Does LASSO impose any restriction in terms of the method of imputation that I can use?</li>
<li>What would be the best bet for imputation method? Ideally, I need a method that I could run on SPSS (preferably) or R.</li>
</ul>

<p>UPDATE1: It became clear from some of the answers below that I have do deal with more basic issues before considering imputation methods. I would like to add here new questions regarding that. On the the answer suggesting the coding as constant value and the creation of a new variable in order to deal with 'not applicable' values and the usage of group lasso:</p>

<ul>
<li>Would you say that if I use group LASSO, I would be able to use the approach suggested to continuous predictors also to categorical predictors? If so, I assume it would be equivalent to creating a new category - I am wary that this may introduce bias.</li>
<li>Does anyone know if R's glmnet package supports group LASSO? If not, would anyone suggest another one that does that in combination with logistic regression? Several options mentioning group LASSO can be found in CRAN repository, any suggestions of the most appropriate for my case? Maybe SGL?</li>
</ul>

<p>This is a follow-up on a previous question of mine (<a href=""http://stats.stackexchange.com/questions/102892/how-to-select-a-subset-of-variables-from-my-original-long-list-in-order-to-perfo"">How to select a subset of variables from my original long list in order to perform logistic regression analysis?</a>).</p>

<p>OBS: I am not a statistician.</p>
"
"0.137777124613698","0.138568498868423","104595","<p>I've been reading <a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments/35994#35994?newreg=4676cdfba08b48449c8c06ba4bdcb71a"">Simulation of Logistic Regression Power Analysis - Designed Experiments</a>, <a href=""http://sas-and-r.blogspot.com/2009/06/example-72-simulate-data-from-logistic.html"" rel=""nofollow"">http://sas-and-r.blogspot.com/2009/06/example-72-simulate-data-from-logistic.html</a>, and <a href=""http://stats.stackexchange.com/questions/22406/power-analysis-for-ordinal-logistic-regression?lq=1"">Power analysis for ordinal logistic regression</a> and I'm still a little lost on how to do a power analysis for my data. I want to be able to determine what N I should have if I have an interaction between a categorical variable (with 3 levels) and a continuous variable.</p>

<p><a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments/35994#35994?newreg=4676cdfba08b48449c8c06ba4bdcb71a"">Simulation of Logistic Regression Power Analysis - Designed Experiments</a> provides some information, but I can't figure out how to simulate the relationship between the categorical and continuous variables and outcome.</p>

<blockquote>
  <p>set.seed(1)<br></p>
  
  <p>repetitions = 1000<br>
  N = 10000<br>
  n = N/8<br>
  var1  = c(   .03,    .03,    .03,    .03,    .06,    .06,    .09,   .09)<br>
  var2  = c(     0,      0,      0,      1,      0,      1,      0,     1)<br>
  rates = c(0.0025, 0.0025, 0.0025, 0.00395, 0.003, 0.0042, 0.0035, 0.002)<br></p>
  
  <p>var1    = rep(var1, times=n)<br>
  var2    = rep(var2, times=n)<br>
  var12   = var1**2<br>
  var1x2  = var1 *var2<br>
  var12x2 = var12*var2<br></p>
  
  <p>significant = matrix(nrow=repetitions, ncol=7)<br></p>
  
  <p>startT = proc.time()[3]<br>
  for(i in 1:repetitions){<br>
   responses          = rbinom(n=N, size=1, prob=rates)<br>
   model              = glm(responses~var1+var2+var12+var1x2+var12x2, <br>
                            family=binomial(link=""logit""))<br>
   significant[i,1:5] = (summary(model)$coefficients[2:6,4]&lt;.05)&lt;br&gt;
&gt;      significant[i,6]   = sum(significant[i,1:5])&lt;br&gt;
&gt;      modelDev           = model$null.deviance-model$deviance<br>
   significant[i,7]   = (1-pchisq(modelDev, 5))&lt;.05<br>
  }<br>
  endT = proc.time()[3]<br>
  endT-startT<br></p>
  
  <p>sum(significant[,1])/repetitions      # pre-specified effect power for var1<br>
  [1] 0.042<br>
  sum(significant[,2])/repetitions      # pre-specified effect power for var2<br>
  [1] 0.017<br>
  sum(significant[,3])/repetitions      # pre-specified effect power for var12<br>
  [1] 0.035<br>
  sum(significant[,4])/repetitions      # pre-specified effect power for var1X2<br>
  [1] 0.019<br>
  sum(significant[,5])/repetitions      # pre-specified effect power for var12X2<br>
  [1] 0.022<br>
  sum(significant[,7])/repetitions      # power for likelihood ratio test of model<br>
  [1] 0.168<br>
  sum(significant[,6]==5)/repetitions   # all effects power<br>
  [1] 0.001<br>
  sum(significant[,6]>0)/repetitions    # any effect power<br>
  [1] 0.065<br>
  sum(significant[,4]&amp;significant[,5])/repetitions   # power for interaction terms<br>
  [1] 0.017<br></p>
</blockquote>

<p>I feel like I should be able to adapt the code from <a href=""http://stats.stackexchange.com/questions/22406/power-analysis-for-ordinal-logistic-regression?lq=1"">Power analysis for ordinal logistic regression</a> and that this would be a better, more succinct option</p>

<blockquote>
  <p>library(rms)</p>
  
  <p>tmpfun &lt;- function(n, beta0, beta1, beta2) { <br>
     x &lt;- runif(n, 0, 10) <br>
     eta1 &lt;- beta0 + beta1*x <br>
     eta2 &lt;- eta1 + beta2 <br>
     p1 &lt;- exp(eta1)/(1+exp(eta1)) <br>
     p2 &lt;- exp(eta2)/(1+exp(eta2)) <br>
     tmp &lt;- runif(n) <br>
     y &lt;- (tmp &lt; p1) + (tmp &lt; p2) <br>
     fit &lt;- lrm(y~x) <br>
     fit$stats[5] <br>
  } <br></p>
  
  <p>out &lt;- replicate(1000, tmpfun(100, -1/2, 1/4, 1/4)) <br>
  mean( out &lt; 0.05 ) <br></p>
</blockquote>

<p>but I'm not completely sure how to do so. I'm assuming tmpfun(100,-1/2, 1/4,1/4) is specifying the N and betas that you want, but how do I adjust tmpfun to another (categorical) variable and include an interaction term? Ultimately the equation should include 6 betas: the intercept, the beta for x, the beta for z1, the beta for z2, the interaction term between x and z1, and the interaction term between x and z2. </p>

<p>Finally, I can't find any reliable sources on what sorts of ""effect sizes"" I should be using as small or medium. </p>

<p>Let me know if I can provide more information!</p>
"
"0.191240737001134","0.203653269990639","106347","<p>I am somewhat familiar with various ways of testing mediation for factors in different types of regression analysis.  (I'm using R and currently working with a multilevel binary logistic regression.)  But now I have a situation in which I'd like to test whether one interaction between factors mediates another, and I'm not sure how this could be done properly.  </p>

<p>To give a simplified example of what I am interested in doing:</p>

<p>I have a multilevel binary model using student characteristics to predict pass/fail for students who are in a control versus experimental group.  </p>

<p>Let's say that the ""intervention"" appears to affect women more strongly than men, because the interaction gender*intervention is significant.  But then adding in a number of co-variates (and their interactions with the intervention),  results in a decrease in the magnitude of the coefficient and the significance of the fenale*intervention interaction, suggesting that once we control for these co-variates and their interactions with the intervention, differences between how the intervention ""affects"" men and women are no longer significantly different.  </p>

<p>I would like to be able to say something about mediation, and I understand how to test the individual factors for mediation of the gender*intervention interaction, but what if there is another interaction, such as (hours spent on childcare)*intervention, which I think may mediate the gender*intervention interaction?  Is there a way to test whether the first interaction mediates the second one?</p>

<p>EDIT:</p>

<p>As requested, here is a simple example equation which I think explains what I want to do.  For the purposes of simplicity, I am specifying this as a simple binary logistic regression model instead of a multilevel binary logistic regression model.  </p>

<p>Let's suppose there are three IV being used as factors:
intervention = whether student was in control or experimental group
gender
GPA</p>

<p>And the DV is whether the student passed or failed.  </p>

<p>And let's suppose we consider the following models (I'm just listing the factors here, without coefficients or error terms, for ease of readability):</p>

<p>M1: OUTCOME = INTERVENTION + GENDER + INTERVENTION*GENDER</p>

<p>M2: OUTCOME = INTERVENTION + GENDER + GPA + INTERVENTION*GENDER + INTERVENTION*GPA</p>

<p>Suppose in M1 that INTERVENTION*GENDER was significant, so that women benefited significantly more than men from the intervention.  </p>

<p>Then supposed that INTERVENTION*GENDER was not significant (and the female*experimental coefficient had a smaller magnitude) in M2, and we suspect that this is because INTERVENTION*GPA mediates INTERVENTION*GENDER.   </p>

<p>What I would like to know is if there is a way to test whether or not INTERVENTION*GPA mediates INTERVENTION*GENDER for these two models....</p>
"
"0.190909583961339","0.192006144294928","108315","<p>I am running multinomial logistic regression analysis on my data.  The response variable is the number of calves produced each year (0,1, or 2).  I am trying to evaluate the influence of the <em>X</em> variables on the odds of producing a calf.  My <em>X</em> variables are predation risk (WR; continuous), age of mother (age; categorical or continuous), time (wolf; categorical).</p>

<p>First, I have 4 different age classification schemes (i only show 2) - I want to know which one of the age of mother would be ""best"" to use. I could use it as continuous variable - or as categories based on biological reasoning for senescence in older moose (old ladies don't invest in reproduction as much).  So, I thought I would use a likelihood ratio test.</p>

<pre><code>library(mlogit)

modata.model1 &lt;- mlogit(no.C ~ 1 | 1, data=modata, reflevel=""1"", na.action = na.omit) 
modata.model2 &lt;- mlogit(no.C ~ 1 | age, data=modata, reflevel=""1"", na.action = na.omit) 
modata.model3 &lt;- mlogit(no.C ~ 1 | age2, data=modata, reflevel=""1"", na.action = na.omit)  
</code></pre>

<hr>

<pre><code> lrtest(modata.model2,modata.model3)

Likelihood ratio test

Model 1: no.C ~ 1 | age
Model 2: no.C ~ 1 | age2
  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    
1   4 -213.22                         
2   4 -207.57  0 11.309  &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>QUESTION: to interpret this output - there was a significant difference in the loglikelihood when we comparing the continuous age to a categorical age with 2 classes.  The loglik is smaller for model 1 and therefore it would be better to use? Or do I have that backwards? </p>

<p>Next I was going to use the Walds test to evaluate nested models.  To see if the addition of a variable was worth it.</p>

<pre><code>modata.model7 &lt;- mlogit(no.C ~ 1 | age+WR, data=modata, reflevel=""1"", na.action = na.omit) 

modata.model8 &lt;- mlogit(no.C ~ 1 | age+WR+wolf, data=modata, reflevel=""1"", na.action = na.omit) 
</code></pre>

<hr>

<pre><code>Wald test

Model 1: no.C ~ 1 | age + WR
Model 2: no.C ~ 1 | age + WR + wolf
  Res.Df Df  Chisq Pr(&gt;Chisq)
1    244                     
2    242  2 0.5828     0.7472
</code></pre>

<p>QUESTION: this tells me that there is no significant improvement when there is an additional variable of wolf added??  So, then I can use the smaller model or do I use the one with the smaller Res.DF?</p>

<p>In addition to confirming my interpretations of the results I have 2 side questions...  </p>

<p>1)to get the null model for <code>mlogit</code> library - is my <code>modata.model1</code> correct?  I want the intercept only model to compare against.</p>

<p>2) Hosmer and Lemshow suggest by getting Wald values to get significance levels for each coefficient - in mlogit, thats the same as using <code>summary(model)</code> and there they provide the t-values with p instead of needing to do an additional Walds test? (NOTE in the below model i use 2 category age class instead of continuous)</p>

<pre><code>summary(modata.model8)

Call:
mlogit(formula = no.C ~ 1 | age2 + WR + wolf, data = modata, 
    na.action = na.omit, reflevel = ""1"", method = ""nr"", print.level = 0)

Frequencies of alternatives:
    1     0     2 
0.652 0.244 0.104 

nr method
6 iterations, 0h:0m:0s 
g'(-H)^-1g = 7.4E-06 
successive function values within tolerance limits 

Coefficients :
              Estimate Std. Error t-value Pr(&gt;|t|)   
0:(intercept)  0.38730    0.40144  0.9648 0.334657   
2:(intercept) -2.40317    1.04546 -2.2987 0.021523 * 
0:age21       -1.39607    0.43989 -3.1737 0.001505 **
2:age21        0.53952    1.07275  0.5029 0.615012   
0:WR          -1.46584    0.64797 -2.2622 0.023686 * 
2:WR          -0.19214    0.60856 -0.3157 0.752206   
0:wolf1        0.56055    0.63292  0.8857 0.375797   
2:wolf1        0.42642    0.72375  0.5892 0.555744   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Log-Likelihood: -203.11
McFadden R^2:  0.053606 
Likelihood ratio test : chisq = 23.009 (p.value = 0.00079359)
</code></pre>
"
"0.135834550306421","0.151794185179729","109222","<p>I am running an analysis where I have 2500 cases and 2500 controls. The cases have disease A, and the controls do not. I am trying to see if having disease A increases the odds of various diseases. For the sake of simplicity, we can focus on one disease, call it disease B.</p>

<p>D = 1 if disease B present, 0 otherwise</p>

<p>E = 1 if disease A present, 0 otherwise</p>

<p>I am also including in the model a measure of healthcare utilization. </p>

<p>F is a positive integer proportional to an individual's utilization of healthcare.</p>

<p>I am running the logistic regression model as such in R:</p>

<pre><code>glm(D ~ E + F, family = ""binomial"") 
</code></pre>

<p>Now, this works fine. </p>

<p>However, when I try to run conditional logistic regression, it gives me an error:</p>

<pre><code>library(survival)
clogit(D ~ E + F, strata(matched.pairs))
Error in fitter(X, Y, strats, offset, init, control, weights = weights,  :
  NA/NaN/Inf in foreign function call (arg 5)
In addition: Warning message:
In fitter(X, Y, strats, offset, init, control, weights = weights,  :
  Ran out of iterations and did not converge
</code></pre>

<p>I have tried different strata, including dividing the individuals into quantile bins based on F. It does not seem to change anything. (note: pairs are matched on age, gender, race, and F)</p>

<p>This occurs only when I run it on a larger sample size. I ran this same analysis on a sample size of 200 (100 cases and 100 controls) and it worked fine. When I use a sample size of 5000, I get the above error. </p>

<p>I also made sure that at least 10 cases and 10 controls had the disease in question (disease B, for this example). </p>

<p>I am not sure why logistic regression runs fine when conditional logistic regression does not. Can anyone offer me any advice?</p>

<p>Thank you all in advance.</p>
"
"NaN","NaN","110570","<p>For my survey data analysis, I ran an Ordinal Logistic regression using the 'polr' function.
The summary of the regression is as follows:</p>

<p><img src=""http://i.stack.imgur.com/csKGq.png"" alt=""enter image description here""></p>

<p>My question is:</p>

<ol>
<li>Do I need to standardize my  beta values?</li>
<li>If so, is lm.beta the right approach (as per my understanding, it only works for linear models)? And if not, could you please provide a method to do so.</li>
</ol>

<p>Thanks everyone!</p>
"
"0.0477273959903348","0.0480015360737319","111308","<p>I'm conducting a power analysis to derive the required sample size for a study - basically compared exposed / non-exposed with 30-day mortality as outcome. I'll check for crude mortality rates with chi-square, but also use logistic regression with probable confounders.</p>

<p>When I run a power analysis - power 0.8, significance level 0.05, effect size 0.15 and estimated 10 confounders I get that I'd need only n=117 which seem quite small.
comparing with chi-square - it suggest that I'd need 350.</p>

<p>I'm using R and <code>pwr</code>:</p>

<pre><code>pwr.f2.test(u=10, v=NULL, f2=0.15, sig.level=0.05, power=0.8)
pwr.chisq.test(w=0.15, N=NULL, df=1 , sig.level=0.05, power=0.8 )
</code></pre>

<p>Is this predictable or am I misusing this?</p>
"
"0.0477273959903348","0.0480015360737319","111841","<p>I ran an ordinal logistic regression in R using the polr function on a survey analysis dataset. The responses of the dependent variable range from Poor to Excellent.
The responses to the independent variables range from 1 to 5 (1 being Poor and 5 being Excellent). I obtained the following result:</p>

<p><img src=""http://i.stack.imgur.com/1ZBij.png"" alt=""enter image description here""></p>

<p>I want to measure the individual percentage contribution of my independents (R1, R2,...,R17) to the dependent variable. </p>

<p>Is there a way to do this.</p>

<p>Thanks for any help.</p>
"
"0.126274820515127","0.127000127000191","112247","<p>I'm trying to use the Match() function from the Matching package in R to do a propensity score analysis.</p>

<p>My outcome of interest is a binary variable (0/1).  My treatment is also a binary variable (0/1).  In addition, I have a number of other variables that I want to control for in this analysis.</p>

<p>First, I fit a logistic regression to define a propensity score for the treatment:</p>

<pre><code>glm1 = glm(Treatment ~ variable1 + variable2 + variable3 + ..., 
           data=dataset, family=""binomial"")
</code></pre>

<p>Then, I used the Match function to estimate the average treatment effect on the treated:</p>

<pre><code>rr1 = Match(Y = Outcome, Tr = Treatment, X = glm1$fitted)
</code></pre>

<p>Finally, I called for a summary:</p>

<pre><code>summary(rr1)
</code></pre>

<p>My question is how to interpret the output.  I get:</p>

<pre><code>Estimate... -0.349,
AI SE... 0.124,
T-stat... -2.827,
p.val... 0.005
</code></pre>

<p>What does this mean?  In particular, what is Estimate?  The documentation says it's ""The estimated average causal effect.""  But what are the units?  Can I interpret this to mean that the treatment reduced the outcome by a relative 35%?  Or by an absolute 0.35?  Or do I need to exponentiate?</p>

<p>Any help on the interpretation would be much appreciated!</p>
"
"0.126274820515127","0.127000127000191","112801","<p>I am seemingly blindly following this <a href=""http://www.cfc.umt.edu/grizzlybearrecovery/pdfs/Schwartz%20et%20al.%202006e.pdf"" rel=""nofollow"">publication</a> that has done work very similar to what I need to accomplish (page 18-21).  My analysis is a multinomial logistic regression where I have 3 possible outcomes 0, 1, or 2 offspring produced.  In the publication, they have recommended a Hosmer-Lemshow and a Persons test for goodness-of-fit.  I have only figured out how to do the Hosmer-Lemshow test and my results are not so good (i.e. P-val is 0.00002).  I have no idea how to do the Pearsons test (suggestions are appreciated).</p>

<p>The paper I am following, of course their tests are ""good"" for model fit (page 21).  But they then go onto suggest that Somers D, the Goodman-Kruskala gamma and the Kendall's tau-a all indicate that their models are a good fit.  But the paper does not report any of the values for these indices or how they calculated them. </p>

<p>I have just found a package <code>ryouready</code> that runs all of these tests.  However, I have been having difficulties finding any help explaining what the values mean, let alone knowing if I have input my variables correctly.</p>

<p>My response variable is number of offspring, most of my explanatory variables are continuous like age or risk.  Do I need to calculate the mean of each explanatory variable within each response variable (get the mean risk for 1 offspring, mean risk for 2 offspring etc...)and then compare those? It also seems that these tests are for 2x2 tables.  If I am just looking at risk, my table would be a 1x3.  However, my complete model will have 4 variables (age, risk, bp, and #offspring year before).  </p>

<p>As you can likely tell, I am in the dark here on where to start. I would appreciate suggested readings, pdf lectures or videos of lectures would even be better!    </p>

<p>EDIT/UPDATE:
I have run the tests over my counts - I have 2 time periods (before/after) and then the count of offspring in each class (0,1,2).  I do not know how to interpret the values - what is ""good"".  What should I be looking for?  Any source that explains these values would be nice to see.  </p>

<pre><code>Kendall's (and Stuart's) Tau statistics
    Tau-b: 0.143
    Tau-c: 0.130
Somers' d:
    Columns dependent: 0.151 
    Rows dependent: 0.136 
    Symmetric: 0.143 
Goodman-Kruskal Gamma: 0.312 
Warning message:
In formatC(x, digits, format = ""f"") : class of 'x' was discarded
</code></pre>
"
"0.116907766928076","0.117579270250443","113252","<p>I am so sorry, I am beginner in statistic analysis, I have project using R to analyze the correlation between dependent variables and independents variables. </p>

<p>In this case I have two dependent variables (1. Extrovert, 2. Introvert). 
And the independent variables i have the data from (Call Log-> how long they call everyday, how many they call everyday, SMS log-> how length text in SMS body every day, how many they sent/received sms for each day).</p>

<p>I am so confused how I can do it, please anyone can give me some good references about it. 
I also have some questions such as : </p>

<ol>
<li>I use the different type of variables, independent variables (data type : numeric) but dependent variable (data type is categorical), so it is possible to apply logistic regression and Pearson? </li>
<li>Or any someone will give me some advice the better solution such as another methods for solving this problem. </li>
</ol>

<p>The example of data from dput()</p>

<pre><code>structure(list(sumcallin = c(462L, 998L, 335L, 179L, 34L, 0L, 
0L, 0L, 0L, 0L), caountcallin = c(7L, 5L, 8L, 5L, 1L, 1L, 0L, 
1L, 1L, 1L), sumcallout = c(1068L, 81L, 519L, 393L, 342L, 0L, 
583L, 1902L, 358L, 1017L), countcallout = c(15L, 3L, 10L, 5L, 
6L, 0L, 3L, 3L, 3L, 3L), sumreceived = c(322L, 75L, 20L, 35L, 
8L, 35L, 135L, 103L, 471L, 173L), countreceived = c(15L, 4L, 
2L, 3L, 1L, 2L, 7L, 3L, 18L, 5L), sumsent = c(171L, 31L, 25L, 
23L, 8L, 55L, 87L, 9L, 400L, 258L), countsent = c(10L, 4L, 1L, 
3L, 1L, 3L, 4L, 1L, 13L, 8L), personality = structure(c(2L, 2L, 
2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L), .Label = c(""extro"", ""intro""), class = ""factor"")), .Names = c(""sumcallin"", 
""caountcallin"", ""sumcallout"", ""countcallout"", ""sumreceived"", 
""countreceived"", ""sumsent"", ""countsent"", ""personality""), row.names = c(1L, 
2L, 3L, 4L, 5L, 37L, 38L, 39L, 40L, 41L), class = ""data.frame"")
</code></pre>

<p>Thank you for your help.</p>
"
"0.165332549536438","0.152425348755266","114728","<p>I'm trying to run a QAP logistic regression to predict the odds of a tie in a social network (represented as a binary adjacency matrix) given two independent variables (also binary matrices) but am getting opposite results depending on whether I run the analysis in R or UCINET.</p>

<p>All three matrices are rectangular (30 x 75). The 30 rows are people I've interviewed and the 75 columns are the entire population (including the 30 interviewees). All matrices include the person IDs as row and column names.</p>

<p>Running the analysis in R (see code at the bottom of the question), I get the following output:</p>

<pre><code>            Estimate  Exp(b)       Pr(&lt;=b) Pr(&gt;=b) Pr(&gt;=|b|)
(intercept) -5.298525  0.004998961 0.0001  0.9999  0.0001   
indep1       1.797138  6.032358591 0.9693  0.0307  0.2393   
indep2       3.194162 24.389724184 1.0000  0.0000  0.0030   
</code></pre>

<p>But after exporting the variables to .csv files and re-running it in UCINET, I get:</p>

<pre><code>                  1       2       3       4       5       6       7       8       9 
               Coef OddsRat     Sig      SD     Avg     Min     Max   P(ge)   P(le) 
            ------- ------- ------- ------- ------- ------- ------- ------- ------- 
1 Intercept  -3.931   0.020   0.000   0.540  -2.785  -3.931  -1.783       1   0.000 
2    indep1   2.391  10.925   0.003   1.508  -0.239  -5.089  15.437   0.003   0.998 
3    indep2   1.458   4.296   0.000   0.504  -0.026 -15.991   1.458   0.000       1 
</code></pre>

<p>Any ideas why this might be happening?</p>

<p>In case it's important, the (QAP) correlation coefficient between the two independent variables is 0.382</p>

<p>I've only included the 30 interviewees in the matrix rows because they are the only people from whom there might be a tie. The networks and QAP regressions are directed.</p>

<p>Incidentally, if I run the QAP logit using full 75x75 adjacency matrices (all people in the columns also appear as rows), I get the same output in both programs.</p>

<p>I also have a related question... A colleague suggested I could run the analysis using the 75x75 matrices but replace the rows of people I haven't interviewed with NAs. This gives me the same results in R and UCINET. Does this seem like a sensible approach, rather than using rectangular matrices?</p>

<p>Thanks!</p>

<p>Reproducible example in R:</p>

<pre><code>library(sna)

# row and column labels
rowIDs &lt;- c(""1"",  ""2"",  ""3"",  ""5"",  ""9"",  ""16"", ""18"", ""19"", ""26"", ""27"", ""34"", ""35"", ""36"", ""40"", ""46"", ""49"", ""60"", ""64"", ""70"", ""72"", ""73"", ""74"", ""75"", ""76"", ""77"", ""82"", ""85"", ""86"", ""97"", ""100"")
colIDs &lt;- c(""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", ""12"", ""13"", ""14"", ""15"", ""16"", ""17"", ""18"", ""19"", ""20"", ""21"", ""23"", ""26"", ""27"", ""34"", ""35"", ""36"", ""38"", ""40"", ""41"", ""43"", ""45"", ""46"", ""47"", ""49"", ""51"", ""52"", ""53"", ""57"", ""59"", ""60"", ""61"", ""62"", ""63"", ""64"", ""65"", ""66"", ""67"", ""68"", ""70"", ""72"", ""73"", ""74"", ""75"", ""76"", ""77"", ""78"", ""79"", ""81"", ""82"", ""83"", ""84"", ""85"", ""86"", ""87"", ""88"", ""89"", ""91"", ""92"", ""93"", ""94"", ""95"", ""96"", ""97"", ""100"", ""101"")

# create matrices
adj.dep &lt;- as.matrix( read.csv(header=F, text=
               ""0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0""))

adj.indep1 &lt;- as.matrix( read.csv(header=F, text=
               ""0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1""))

adj.indep2 &lt;- as.matrix( read.csv(header=F, text=
               ""0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0
                1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0
                0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1""))

# assign row/column names
rownames(adj.dep) &lt;- rowIDs
colnames(adj.dep) &lt;- colIDs
rownames(adj.indep1) &lt;- rowIDs
colnames(adj.indep1) &lt;- colIDs
rownames(adj.indep2) &lt;- rowIDs
colnames(adj.indep2) &lt;- colIDs

# set up independent variables
g.indeps &lt;- array(dim=c(2, nrow(adj.indep1), ncol(adj.indep1)))
g.indeps[1,,] &lt;- adj.indep1
g.indeps[2,,] &lt;- adj.indep2

# run the analysis
# (warning, this command takes a bit of time to run with 10,000 reps)
nl &lt;- netlogit(adj.dep, g.indeps, reps=10000, nullhyp=""qap"")
# print output
summary(nl)
</code></pre>
"
"0.143903513382412","0.144730076833889","115219","<p><strong><em>Imagine the situation:</em></strong> Mythical Seafolk use holes in the seabed as their burrows. Each hole has two parameters - diameter and depth. <strong>Majority of holes are unoccupied</strong> due to their surplus (n = 235). Occupied holes (n = 15) are (generally) expected to be much deeper and with larger diameter than random.</p>

<pre><code># generate data
set.seed(1234)
x &lt;- runif(250, min=0, max=10)
y &lt;- runif(250, min=0, max=10)
rbPal   &lt;- colorRampPalette(c(""deepskyblue"",""darkblue""))
my.data &lt;- data.frame(x_coor = x, y_coor = y,
                      diameter = c(abs(rnorm(15)+2.5), abs(rnorm(235))),
                      depth = c(abs(rnorm(15)+2.5), abs(rnorm(235))),
                      usage = rep(c(1,0), times = c(15, 235)))
my.data$col &lt;- rbPal(10)[as.numeric(cut(my.data$depth,breaks = 10))]

# look at the situation
# occupied holes are marked by red circles
plot(my.data$x, my.data$y, cex = my.data$diameter, col = my.data$col, pch = 20)
grid(5, 5, lwd = 0.75, lty = 2, col = ""grey"") 
points(my.data$x[1:25], my.data$y[1:25], pch = 1, cex = 2,
       col = ""red"", lwd = 1.75)
</code></pre>

<p><img src=""http://i.stack.imgur.com/8O1Xg.png"" alt=""enter image description here""></p>

<p>Although this analysis seem to be rather straightforward, <strong>I have some doubts about comparing unequal samples (15 vs 235)</strong>. Such problem do not occur in other <em>Seafolk burrow selection studies</em> because <strong>mapping of seabed is very expensive and time demanding</strong> and when researchers find 10 used holes, they continue to map the seabed only till they get find additonal 10 nonused holes (which are most probably located in close vicinity of used holes). Due to this
data collection approach the sample sizes of compared groups are usually almost equal.
<strong>However, we were able to find ALL holes in seabed - which can be now considered as a ""handicap"".</strong></p>

<p><strong><em>We do not want to do random sampling of 15 holes from 235 to obtain equal sizes (15 vs 15). Why to throw away such large amount of data!? Is there any solution/approach to this situation which is able to fully embrace such unique dataset?</em></strong></p>

<p><em>I have in mind two possible ways (please comment or add others):</em></p>

<p><strong>1.</strong> Take the sample of unoccupied holes (n = 235) and randomly resample them to (for example 50) smaller groups (all with n = 15). Subsequently, each of these reduced groups will be compared with sample of occupied holes by binomial logistic regression. By this I will obtain 50 logistic curves which will
be ""averaged"" into just one curve - final model.</p>

<p><strong>2.</strong> Take the sample of unoccupied holes (n = 235) and randomly resample them to (for example 50) smaller groups (all with n = 15). Subsequently, from each of these reduced groups (and from sample of used holes also) we gain (mean, sd, sample size). Thus, we will have six numbers for each comparison and we calculate effect size - standardized mean difference (SMD). Effect sizes will be inserted into meta-analytic model and tested if there is a heterogeneity present among effect sizes (<em>to see if the ""randomnes"" of creating unused holes sample has the significant impact on test result</em>).</p>
"
"0.0954547919806695","0.0960030721474639","116850","<p>I have a dataset consisting of repeated measures data of graded toxicity scores (0-4) in a large number of patients being treated with a anti-cancer drug. We would like to identify predictors for developing dose limiting toxicity (e.g. score 3 or 4).</p>

<p>Initially the dataset was analyzed using a cumulative link mixed model to identify such predictors (clmm in R package ordinal).</p>

<p>However, there is also informative dropout occuring in the dataset (MNAR), e.g. the risk of dropping out is related to the severity of toxicity. As this therefore may bias parameter estimates, it seems a joint model that includes a survival model for dropout would be best. </p>

<p>My questions are:</p>

<ol>
<li><p>Is a joint model for dropout and outcome the only suitable option to account for dropout?  Or would there be other ways as well ? The only ugly alternative would be to analyze only the first part of the dataset, but I would rather like to use ALL the available data.</p></li>
<li><p>Is there an R package available that would readily allow such an analysis. E.g. longitudinal mixed-effect modelling of ordered categorical data, together with a dropout survival model. Or, alternatively, a mixed effect model logistic regression model with dropout (e.g. in this case we would reduce the dataset to having a dose limiting toxicity yes/no) ? The JM and joinR packages appear to be based on continuous data for the repeated measures.</p></li>
</ol>
"
"0.202490192118848","0.192339199435604","118141","<p>I'm trying to fit a logistic curve to cumulative data, derived from satellite imagery. Previously, I have point observation data which were either 0s or 1s. Os being 'forest' and 1s being 'non-forest'. These point observations existed for multiple images/dates. So I had one csv file with 'observation date' in once column and 'state' in another. Weights were also included, but these aren't relevant here. </p>

<p>This was the code I used:</p>

<pre><code># Read in data
data=read.csv(""C:\\Users\\Owner\\Desktop\\BV\\Deforestation_Analysis\\Ambaro_Ambanja\\Analysis\\CDM_Table_Final.csv"")

head(data)
attach(data)
Time&lt;-strptime(Observation_Date, ""%m/%d/%Y"")
Time2&lt;-as.Date(Time)
Time2
# Set project start date to zero
Zero&lt;-as.Date(""2015/7/1"")
temp.time&lt;-as.numeric(Zero-Time2)
T.Time&lt;-temp.time*(-1)
T.Time
data2&lt;-cbind(data,T.Time)

# Fit the model and summarise
model.glm=glm(formula=State~T.Time,data=data2,weights=Weight,family=binomial(link=logit))
summary(model.glm)
</code></pre>

<p>Now, rather than having multiple 0s and 1s for each date, I have simply number of non-forest pixels (and no weight). Here is the data I have:</p>

<p><img src=""http://i.stack.imgur.com/oFFIe.jpg"" alt=""enter image description here""></p>

<p>The 'State' field is literally the portion of non-forest pixels (18.6% of pixels were non-forest in 2014)</p>

<p>I tried runnning the following adaption of the original script:</p>

<pre><code># Read in data
data=read.csv(""C:\\Users\\Leah\\Desktop\\BV\\AAB\\Geospatial\\Deforestation_Analysis\\Analysis\\For_R.csv"")

head(data)
attach(data)
Time&lt;-strptime(Observation_Date, ""%d/%m/%Y"")
Time2&lt;-as.Date(Time)
Time2
# Set project start date to zero
Zero&lt;-as.Date(""2015/7/1"")
temp.time&lt;-as.numeric(Zero-Time2)
T.Time&lt;-temp.time*(-1)
T.Time
data2&lt;-cbind(data,T.Time)

# Fit the model and summarise
model.glm=glm(formula=State~T.Time,data=data2,family=binomial(link=logit))
summary(model.glm)
</code></pre>

<p>I fully expected it to fail, because the data is no longer binomial. But while it did throw up a warning ('In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!'), the coefficients it spat out formed a curve which looked perfect. But I wasn't overly confident in this hash.</p>

<p>I've read that you can still use the binomial glm family if you feed R with a table containing successes (non-forest) and failures. So I came up with this adapted data:</p>

<p><img src=""http://i.stack.imgur.com/CMSd3.jpg"" alt=""enter image description here""></p>

<p>and the following adapted script:</p>

<pre><code># Read in data
data=read.csv(""C:\\Users\\Leah\\Desktop\\BV\\AAB\\Geospatial\\Deforestation_Analysis\\Analysis\\For_R_Final.csv"")

head(data)
attach(data)
Time&lt;-strptime(Observation_Date, ""%d/%m/%Y"")
Time2&lt;-as.Date(Time)
Time2
# Set project start date to zero
Zero&lt;-as.Date(""2015/7/1"")
temp.time&lt;-as.numeric(Zero-Time2)
T.Time&lt;-temp.time*(-1)
T.Time
data2&lt;-cbind(data,T.Time)

# Fit the model and summarise
model.glm=glm(formula=cbind(Deforested, Total-Deforested)~T.Time,data=data2,family=binomial(link=logit))
summary(model.glm)
</code></pre>

<p>It ran fine with no errors, but the trend it generated doesn't fit the data anywhere near as well as the hashed version:</p>

<p><img src=""http://i.stack.imgur.com/FXL6I.jpg"" alt=""Blue line is hased version; Grey line is adapted script; Red points are the data I&#39;m using to fit the model""></p>

<p>The blue line is hased version; grey line is adapted script; red points are the data I'm using to fit the model.</p>

<p>Why does the adapted version fit the point worse than the hashed version? Is R so clever that it just uses my fractional values how I want it to in the glm(family=binomial)?</p>

<p>Any advice greatly appreciated! Am not happy with how I got the blue trend and this work is very important to our study.</p>

<p>THANK YOU!!</p>
"
"0.305604446055268","0.299863218602654","118215","<p>This is my first post on StackExchange, but I have been using it as a resource for quite a while, I will do my best to use the appropriate format and make the appropriate edits. Also, this is a multi-part question. I wasn't sure if I should split the question into several different posts or just one. Since the questions are all from one section in the same text I thought it would be more relevant to post as one question.</p>

<p>I am researching habitat use of a large mammal species for a Master's Thesis. The goal of this project is to provide forest managers (who are most likely not statisticians) with a practical framework to assess the quality of habitat on the lands they manage in regard to this species. This animal is relatively elusive, a habitat specialist, and usually located in remote areas. Relatively few studies have been carried out regarding the distribution of the species, especially seasonally.  Several animals were fitted with GPS collars for a period of one year. One hundred locations (50 summer and 50 winter) were randomly selected from each animal's GPS collar data. In addition, 50 points were randomly generated within each animal's home range to serve as ""available"" or ""pseudo-absence"" locations. The locations from the GPS collars are coded a 1 and the randomly selected available locations are coded as 0.</p>

<p>For each location, several habitat variables were sampled in the field (tree diameters, horizontal cover, coarse woody debris, etc) and several were sampled remotely through GIS (elevation, distance to road, ruggedness, etc). The variables are mostly continuous except for 1 categorical variable that has 7 levels.</p>

<p>My goal is to use regression modelling to build resource selection functions (RSF) to model the relative probability of use of resource units. I would like to build a seasonal (winter and summer) RSF for the population of animals (design type I) as well as each individual animal (design type III).</p>

<p>I am using R to perform the statistical analysis.</p>

<p>The <strong>primary text</strong> I have been using isâ€¦</p>

<ul>
<li>""Hosmer, D. W., Lemeshow, S., &amp; Sturdivant, R. X. 2013. Applied Logistic Regression. Wiley, Chicester"".</li>
</ul>

<p>The majority of the examples in Hosmer et al. use STATA, <strong>I have also been using the following 2 texts for reference with R</strong>.</p>

<ul>
<li>""Crawley, M. J. 2005. Statistics : an introduction using R. J. Wiley,
Chichester, West Sussex, England.""</li>
<li>""Plant, R. E. 2012. Spatial Data Analysis in Ecology and Agriculture 
Using R. CRC Press, London, GBR.""</li>
</ul>

<p>I am currently following the steps in <strong>Chapter 4 of Hosmer et al. for the ""Purposeful Selection of Covariates""</strong> and have a few questions about the process. I have outlined the first few steps in the text below to aid in my questions.</p>

<ol>
<li>Step 1: A univariable analysis of each independent variable (I used a
univariable logistic regression). Any variable whose univariable test
has a p-value of less than 0.25 should be included in the first
multivariable model.</li>
<li>Step 2: Fit a multivariable model containing all covariates
identified for inclusion at step 1 and to assess the importance of
each covariate using the p-value of its Wald statistic. Variables
that do not contribute at traditional levels of significance should
be eliminated and a new model fit. The newer, smaller model should be
compared to the old, larger model using the partial likelihood ratio
test.</li>
<li>Step 3: Compare the values of the estimated coefficients in the
smaller model to their respective values from the large model. Any
variable whose coefficient has changed markedly in magnitude should
be added back into the model as it is important in the sense of
providing a needed adjustment of the effect of the variables that
remain in the model. Cycle through steps 2 and 3 until it appears that all of the important variables are included in the model and those excluded are clinically and/or statistically unimportant. Hosmer et al. use the ""<em>delta-beta-hat-percent</em>""
as a measure of the change in magnitude of the coefficients. They
suggest a significant change as a <em>delta-beta-hat-percent</em> of >20%. Hosmer et al. define the <em>delta-beta-hat-percent</em> as 
$\Delta\hat{\beta}\%=100\frac{\hat{\theta}_{1}-\hat{\beta}_{1}}{\hat{\beta}_{1}}$.
Where $\hat{\theta}_{1}$ is the coefficient from the smaller model and $\hat{\beta}_{1}$ is the coefficient from the larger model.</li>
<li>Step 4: Add each variable not selected in Step 1 to the model
obtained at the end of step 3, one at a time, and check its
significance either by the Wald statistic p-value or the partial
likelihood ratio test if it is a categorical variable with more than
2 levels. This step is vital for identifying variables that, by
themselves, are not significantly related to the outcome but make an
important contribution in the presence of other variables. We refer
to the model at the end of Step 4 as the <em>preliminary main effects
model</em>.</li>
<li>Steps 5-7: I have not progressed to this point so I will leave these
steps out for now, or save them for a different question.</li>
</ol>

<p><strong>My questions:</strong> </p>

<ol>
<li>In step 2, what would be appropriate as a traditional level of
significance, a p-value of &lt;0.05 something larger like &lt;.25?</li>
<li>In step 2 again, I want to make sure the R code I have been using for the partial likelihood test is correct and I want to make sure I am interpreting the results correctly. Here is what I have been doing&hellip;<code>anova(smallmodel,largemodel,test='Chisq')</code> If the p-value is significant (&lt;0.05) I add the variable back to the model, if it is insignificant I proceed with deletion?</li>
<li>In step 3, I have a question regarding the <em>delta-beta-hat-percent</em> and when it is appropriate to add an excluded variable back to the model. For example, I exclude one variable from the model and it changes the $\Delta\hat{\beta}\%$ for a different variable by >20%. However, the variable with the >20% change in $\Delta\hat{\beta}\%$ seems to be insignificant and looks as if it will be excluded from the model in the next few cycles of Steps 2 and 3. How can I make a determination if both variables should be included or excluded from the model? Because I am proceeding by excluding 1 variable at a time by deleting the least significant variables first, I am hesitant to exclude a variable out of order.</li>
<li><p>Finally, I want to make sure the code I am using to calculate $\Delta\hat{\beta}\%$ is correct. I have been using the following code. If there is a package that will do this for me or a more simple way of doing it I am open to suggestions.  </p>

<p><code>100*((smallmodel$coef[2]-largemodel$coef[2])/largemodel$coef[2])</code></p></li>
</ol>
"
"NaN","NaN","122508","<p>I am trying to replicate a multilevel logistic analysis which uses a dyadic time series data set and R. As for my part, I am using the same data set but Stata.</p>

<p>The original syntax has the following specifications:</p>

<p><code>lmer(depvar ~ indepvar_n (1|country_a)+(1|country_b)+(1|year), data=d, family='binomial'))</code></p>

<p>My Stata syntax currently looks like this:</p>

<p><code>xtmelogit depvar indepvars || country_a:country_b</code></p>

<p>Obviously, somehow I still have to add the <strong>year</strong> variable, but I have troubles in doing so. When excluding years from both equations I produce identical results. However, this is pointless since it is a different analysis.</p>

<p>In which way do I have to modify the Stata syntax to produce the same results which are computed by the R command? I cannot simply add :year after country_b since this is not possible. What am I missing?</p>
"
"0.0826662747682189","0.083141099321054","123172","<p>I am making a table from results of an analysis using generalised linear model which involves detecting association of a categorical predictor variable over multiple outcome variables. Of those multiple outcome variables, few are binary where I display the odds ratio for each category of the predictor (as we do in logistic regression); while few are continuous outcome variables, in which case I can display the beta estimate for each category of the predictor. My question is will it be ok if exponentiate the beta value  and express it as odds ratios. Can I do that?</p>
"
"0.143182187971004","0.144004608221196","124512","<p>I have been developing a logistic regression model based on retrospective data from a national trauma database of head injury in the UK. The key outcome is 30 day mortality (denoted as ""Survive"" measure). Other measures across the whole database with published evidence of significant effect on outcome in previous studies include:</p>

<pre><code>Year - Year of procedure = 1994-2013
Age - Age of patient = 16.0-101.5
ISS - Injury Severity Score = 1-75
GCS - Glasgow Coma Scale = 3-15
Sex - Gender of patient = Male or Female
inctoCran - Time from head injury to craniotomy in minutes = 0-2880 (After 2880 minutes is defined as a separate diagnosis)
</code></pre>

<p>There are two additional variables where the data is only available from 2004:</p>

<pre><code>Pupil reactivity (1 is brisk, 2 is sluggish and 3 is unreactive)
Pupil size (continuous variable in mm - 1-10)
</code></pre>

<p>Based on literature these are significant predictors of outcome. However, out of a series of 2140, I am missing 936. Secondly, the measure is not missing at random, having only been collected in recent years.</p>

<p>My questions are the following in order to address the year range 1994-2013:</p>

<p>1) My data is heavily skewed to later years; how can I adapt the logistic regression to reduce the effect of this when assessing the effect of the year of procedure on outcome?</p>

<p>2) Can I exclude pupil reactivity since it was not collected before 2004 in performing this analysis even if it is a strong predictor?</p>

<p>3) If I should include pupil reactivity, can a multivariate regression be built with the variables above with which to perform imputation to create data for 1994-2003 given 43% of the data is missing?</p>

<p>4) If not possible, could imputation be performed based on data since 2009 where ~15% is missing?</p>

<p>I perform all statistical analyses exclusively with R and would be grateful if you could add known packages/formulae to execute your suggestions.</p>
"
"0.150927278118246","0.151794185179729","125130","<p>I realize that a similar question to this has been asked, but it was not ultimately resolved. I have tried the suggestions posted to that question <a href=""https://stackoverflow.com/questions/23347467/is-there-any-way-to-fit-a-glm-so-that-all-levels-are-included-i-e-no-refer"">here</a>, but have had no success. I am using the following code:       </p>

<pre><code>allinfa4.exp = glm(survive ~ year + julianvisit + class + sitedist + roaddist
+ ngwdist, family = binomial(logexp(alldata$expos)), data=alldata)
summary(allinfa4.exp)

 Call:
glm(formula = survive ~ year + julianvisit + class + sitedist + 
roaddist + ngwdist, family = binomial(logexp(alldata$expos)), 
data = alldata)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.6435   0.3477   0.4164   0.4960   0.9488  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  4.458e+00  7.117e-01   6.265 3.74e-10 ***
year2013     3.680e-01  1.862e-01   1.976  0.04819 *  
year2014     2.136e-02  1.802e-01   0.119  0.90564    
julianvisit -5.714e-03  3.890e-03  -1.469  0.14192    
classb       2.863e-02  2.194e-01   0.131  0.89615    
classc      -2.394e-01  2.277e-01  -1.051  0.29304    
classd      -1.868e-01  2.479e-01  -0.754  0.45109    
classe      -4.500e-01  2.076e-01  -2.167  0.03021 *  
classf      -5.728e-01  2.005e-01  -2.858  0.00427 ** 
classg      -8.495e-01  3.554e-01  -2.390  0.01684 *  
classh      -1.858e-01  2.224e-01  -0.835  0.40351    
classi      -3.196e-01  4.417e-01  -0.724  0.46932    
sitedist    -2.607e-04  5.043e-04  -0.517  0.60520    
roaddist     6.768e-05  4.311e-04   0.157  0.87525    
ngwdist     -5.751e-05  9.456e-05  -0.608  0.54306
</code></pre>

<p>The main thing to note here is that I have two categorical variables, <code>year</code> and <code>class</code>, and R has combined the first level of each (2012 and class a) into a reference level intercept term. Not only do I need to know the intercept term for these levels individually, but I also need to know the base intercept terms itself (beta0), just as SAS produces. </p>

<p>I have tried changing the contrasts and deviation coding to accomplish this, but although doing so allows me to extract different levels, it changes the way they are calculated and still does not produce beta0. I've also tried adding +0 and -1, but this also does not provide what I need. Is what I'm trying to do simply impossible in R? It may seem like a strange request, but beta0 is necessary to convert the results of logistic exposure (special kind of logistic regression for nest-survival data) to daily survival rates. Any help would be hugely appreciated. Thanks!</p>

<p>Here is an example of SAS output I want to emulate (taken from a similar analysis done by my lab mate) :</p>

<pre><code>Parameter Estimates
Parameter   Estimate    Standard Error  DF  t Value Pr &gt; |t|              
beta0       7.8404      2.8479          19  2.75    0.0127  
NT         -3.8786      1.8831          19  -2.06   0.0534  
bgdensity  -0.1127      0.1614          19  -0.70   0.4935  
nwh         1.3466      1.4625          19  0.92    0.3687      
NRD        -2.6981      1.9496          19  -1.38   0.1824      
NAGW       -0.4898      2.2518          19  -0.22   0.8301      
</code></pre>
"
"0.106721701823439","0.0858677581482184","125465","<p>I'm searching for a built-in function in R for calculating the required sample size (given certain power, alpha,..) of a mixed ANOVA (2 between, 1 within variable, 2x2x2 design). Does this function exist? I didn't find it in the following packages:</p>

<p>â€¢pwr is the oldest power-analysis library; some introductory info can be found on Quick-R</p>

<p>â€¢PoweR: Computation of power and level tables for hypothesis tests</p>

<p>â€¢Power2Stage: Power and Sample size distribution of 2-stage BE studies via simulations</p>

<p>â€¢powerAnalysis: Power analysis in experimental design</p>

<p>â€¢powerGWASinteraction: Power Calculations for Interactions for GWAS</p>

<p>â€¢powerMediation: Power/Sample size calculation for mediation analysis, simple linear 
regression, logistic regression, or longitudinal study</p>

<p>â€¢powerpkg: Power analyses for the affected sib pair and the TDT design</p>

<p>â€¢powerSurvEpi: Power and sample size calculation for survival analysis of epidemiological studies</p>

<p>â€¢PowerTOST: Power and Sample size based on two one-sided t-tests (TOST) for (bio)equivalence studies</p>

<p>â€¢longpower: Power and sample size for linear model of longitudinal data</p>

<p>Thanks!</p>
"
"0.123999412152328","0.152425348755266","129739","<p>I have been developing a logistic regression model based on retrospective data from a national trauma database of head injury in the UK. The key outcome is 30 day mortality (denoted as <code>Outcome30</code> measure). Other measures across the whole database with published evidence of significant effect on outcome in previous studies include:</p>

<pre><code>Yeardecimal - Date of procedure = 1994.0-2013.99
inctoCran - Time from head injury to craniotomy in minutes = 0-2880 (After 2880 minutes is defined as a separate diagnosis)
ISS - Injury Severity Score = 1-75
Age - Age of patient = 16.0-101.5
GCS - Glasgow Coma Scale = 3-15
Sex - Gender of patient = Male or Female
rcteyemi - Pupil reactivity (1 = neither, 2 = one, 3 = both)
neuroFirst2 - Location of admission (Neurosurgical unit or not)
Other - other traums (0 - No, 1 - Yes)
othopYN - Other operation required
LOS - Length of stay in days
LOSCC - Length of stay in critical care in days 
</code></pre>

<p>When I conduct univariate analysis of the variables, I have conducted a logistic regression for each continuous variable. I am unable to model Yeardecimal however, with the following result:</p>

<pre><code>&gt; rcs.ASDH&lt;-lrm(formula = Survive ~ Yeardecimal, data = ASDH_Paper1.1)
singular information matrix in lrm.fit (rank= 1 ).  Offending variable(s):
Yeardecimal 
Error in lrm(formula = Survive ~ Yeardecimal, data = ASDH_Paper1.1) : 
  Unable to fit model using â€œlrm.fitâ€
</code></pre>

<p>However, the restricted cubic spline works:</p>

<pre><code>&gt; rcs.ASDH&lt;-lrm(formula = Survive ~ rcs(Yeardecimal), data = ASDH_Paper1.1)
&gt; 
&gt; rcs.ASDH

Logistic Regression Model

lrm(formula = Survive ~ rcs(Yeardecimal), data = ASDH_Paper1.1)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          5998    LR chi2     106.61    R2       0.027    C       0.578    
 0           1281    d.f.             4    g        0.319    Dxy     0.155    
 1           4717    Pr(&gt; chi2) &lt;0.0001    gr       1.376    gamma   0.160    
max |deriv| 2e-08                          gp       0.057    tau-a   0.052    
                                           Brier    0.165                     

               Coef     S.E.    Wald Z Pr(&gt;|Z|)
Intercept      -68.3035 45.8473 -1.49  0.1363  
Yeardecimal      0.0345  0.0229  1.51  0.1321  
Yeardecimal'     0.1071  0.0482  2.22  0.0262  
Yeardecimal''   -2.0008  0.6340 -3.16  0.0016  
Yeardecimal'''  11.3582  4.0002  2.84  0.0045  
</code></pre>

<p>Could anyone explain why this is? I am nervous about using a mode complicated model if I am unable to model with a simpler approach.</p>

<p>I am currently using restricted cubic splines to model Age, ISS and Yeardecimal. Would anyone recommend any alternative approach?</p>
"
"0.220934912588279","0.274051519321612","129761","<p>These multiple imputation results relate to data I have previously described and shown here - <a href=""http://stats.stackexchange.com/questions/129739/skewed-distributions-for-logistic-regression"">Skewed Distributions for Logistic Regression</a></p>

<p>Three variables I am using have missing data. Their names, descriptions and % missing are shown below.</p>

<pre><code>inctoCran - Time from head injury to craniotomy in minutes = 0-2880 (After 2880 minutes is defined as a separate diagnosis) - 58% missing
GCS - Glasgow Coma Scale = 3-15 - 37% missing
rcteyemi - Pupil reactivity (1 = neither, 2 = one, 3 = both) - 56% missing
</code></pre>

<p>I have been using mutliple imputation to model the missing data above following advice in a previous post here - <a href=""http://stats.stackexchange.com/questions/127134/describing-results-from-logistic-regression-with-restricted-cubic-splines-using"">Describing Results from Logistic Regression with Restricted Cubic Splines Using rms in R</a></p>

<p>Given this is a longitudinal analysis, a key variable of importance is the year of the treatment so we can investigate how our patient management has improved. The variable in question, <code>Yeardecimal</code> is highly significant in univariate analysis:</p>

<pre><code>&gt; rcs.ASDH&lt;-lrm(formula = Survive ~ Yeardecimalc, data = ASDH_Paper1.1)
&gt; 
&gt; rcs.ASDH

Logistic Regression Model

lrm(formula = Survive ~ Yeardecimalc, data = ASDH_Paper1.1)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          5998    LR chi2      91.47    R2       0.023    C       0.572    
 0           1281    d.f.             1    g        0.309    Dxy     0.143    
 1           4717    Pr(&gt; chi2) &lt;0.0001    gr       1.362    gamma   0.146    
max |deriv| 3e-12                          gp       0.054    tau-a   0.048    
                                           Brier    0.165                     

             Coef   S.E.   Wald Z Pr(&gt;|Z|)
Intercept    0.8696 0.0530 16.42  &lt;0.0001 
Yeardecimalc 0.0551 0.0057  9.70  &lt;0.0001 
</code></pre>

<p>To deal with missingness, I used <code>aregImpute</code> and <code>fit.mult.impute</code> to conduct multiple imputation prior to multivariate logisic regression. When including Yeardecimal, the results were as follows:</p>

<pre><code>&gt; a &lt;- aregImpute(~ I(Outcome30) + Age + GCS + I(Other) + ISS + inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + LOS + Yeardecimalc, nk=4, data = ASDH_Paper1.1, n.impute=10)
Iteration 13 
&gt; 
&gt; a

Multiple Imputation using Bootstrap and PMM

aregImpute(formula = ~I(Outcome30) + Age + GCS + I(Other) + ISS + 
    inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + 
    LOS + Yeardecimalc, data = ASDH_Paper1.1, n.impute = 10, 
    nk = 4)

n: 5998     p: 12   Imputations: 10     nk: 4 

Number of NAs:
   Outcome30          Age          GCS        Other          ISS    inctoCran     rcteyemi   neuroFirst      neuroYN 
           0            0         2242            0            0         3500         3376            0            0 
   Mechanism          LOS Yeardecimalc 
           0            0            0 

             type d.f.
Outcome30       c    1
Age             s    3
GCS             s    3
Other           c    1
ISS             s    3
inctoCran       s    3
rcteyemi        l    1
neuroFirst      l    1
neuroYN         l    1
Mechanism       c    4
LOS             s    3
Yeardecimalc    s    3

Transformation of Target Variables Forced to be Linear

R-squares for Predicting Non-Missing Values for Each Variable
Using Last Imputations of Predictors
      GCS inctoCran  rcteyemi 
    0.421     0.181     0.358 

&gt; rcs.ASDH &lt;- fit.mult.impute(Survive ~ rcs(Age) + GCS + Mechanism + rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + Sex + Other + rcteyemi,lrm,a,data=ASDH_Paper1.1)

&gt; rcs.ASDH

Logistic Regression Model

fit.mult.impute(formula = Survive ~ rcs(Age) + GCS + Mechanism + 
    rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + 
    Sex + Other + rcteyemi, fitter = lrm, xtrans = a, data = ASDH_Paper1.1)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          5998    LR chi2    1609.98    R2       0.365    C       0.836    
 0           1281    d.f.            25    g        1.584    Dxy     0.672    
 1           4717    Pr(&gt; chi2) &lt;0.0001    gr       4.875    gamma   0.674    
max |deriv| 0.001                          gp       0.222    tau-a   0.226    
                                           Brier    0.121                     

                              Coef    S.E.    Wald Z Pr(&gt;|Z|)
Intercept                     21.3339 67.4400  0.32  0.7517  
Age                           -0.0088  0.0132 -0.67  0.5052  
Age'                          -0.0294  0.0643 -0.46  0.6471  
Age''                         -0.0134  0.2479 -0.05  0.9570  
Age'''                         0.2588  0.3534  0.73  0.4639  
GCS                            0.1100  0.0145  7.61  &lt;0.0001 
Mechanism=Fall &gt; 2m           -0.0651  0.1162 -0.56  0.5754  
Mechanism=Other                0.2285  0.1338  1.71  0.0876  
Mechanism=RTC                  0.0449  0.1332  0.34  0.7360  
Mechanism=Shooting / Stabbing  2.1150  1.1142  1.90  0.0577  
ISS                           -0.1069  0.0318 -3.36  0.0008  
ISS'                          -0.0359  0.1306 -0.27  0.7835  
ISS''                          1.8296  1.9259  0.95  0.3421  
neuroFirst                    -0.3483  0.0973 -3.58  0.0003  
inctoCrand                     0.0001  0.0053  0.02  0.9872  
inctoCrand'                   -0.0745  0.3060 -0.24  0.8077  
inctoCrand''                   0.1696  0.5901  0.29  0.7738  
inctoCrand'''                 -0.1167  0.3150 -0.37  0.7110  
inctoCranYN                   -0.2814  0.6165 -0.46  0.6480  
Yeardecimalc                  -0.0101  0.0337 -0.30  0.7641  
Yeardecimalc'                  0.0386  0.0651  0.59  0.5536  
Yeardecimalc''                -0.7417  0.8210 -0.90  0.3663  
Yeardecimalc'''                7.0367  4.9344  1.43  0.1539  
Sex=Male                       0.0668  0.0891  0.75  0.4534  
Other=1                        0.3238  0.1611  2.01  0.0445  
rcteyemi                       1.1589  0.1050 11.04  &lt;0.0001 


&gt; anova(rcs.ASDH)
                Wald Statistics          Response: Survive 

 Factor          Chi-Square d.f. P     
 Age              83.07      4   &lt;.0001
  Nonlinear        5.97      3   0.1131
 GCS              57.89      1   &lt;.0001
 Mechanism         8.14      4   0.0867
 ISS              77.31      3   &lt;.0001
  Nonlinear       35.04      2   &lt;.0001
 neuroFirst       12.81      1   0.0003
 inctoCrand        2.32      4   0.6777
  Nonlinear        2.29      3   0.5149
 inctoCranYN       0.21      1   0.6480
 Yeardecimalc      4.19      4   0.3807
  Nonlinear        3.77      3   0.2874
 Sex               0.56      1   0.4534
 Other             4.04      1   0.0445
 rcteyemi        121.80      1   &lt;.0001
 TOTAL NONLINEAR  47.27     11   &lt;.0001
 TOTAL           679.09     25   &lt;.0001
&gt; 
</code></pre>

<p>Yeardecimal is no longer significant. However, if I exclude Yeardecimal from aregImpute only, I have the alternative result below:</p>

<pre><code>&gt; a &lt;- aregImpute(~ I(Outcome30) + Age + GCS + I(Other) + ISS + inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + LOS, nk=4, data = ASDH_Paper1.1, n.impute=10)
Iteration 13 
&gt; 
&gt; a

Multiple Imputation using Bootstrap and PMM

aregImpute(formula = ~I(Outcome30) + Age + GCS + I(Other) + ISS + 
    inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + 
    LOS, data = ASDH_Paper1.1, n.impute = 10, nk = 4)

n: 5998     p: 11   Imputations: 10     nk: 4 

Number of NAs:
 Outcome30        Age        GCS      Other        ISS  inctoCran   rcteyemi neuroFirst    neuroYN  Mechanism        LOS 
         0          0       2242          0          0       3500       3376          0          0          0          0 

           type d.f.
Outcome30     c    1
Age           s    3
GCS           s    3
Other         c    1
ISS           s    3
inctoCran     s    3
rcteyemi      l    1
neuroFirst    l    1
neuroYN       l    1
Mechanism     c    4
LOS           s    3

Transformation of Target Variables Forced to be Linear

R-squares for Predicting Non-Missing Values for Each Variable
Using Last Imputations of Predictors
      GCS inctoCran  rcteyemi 
    0.407     0.194     0.320 
&gt; 

&gt; rcs.ASDH &lt;- fit.mult.impute(Survive ~ rcs(Age) + GCS + Mechanism + rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + Sex + Other + rcteyemi,lrm,a,data=ASDH_Paper1.1)
&gt; rcs.ASDH

Logistic Regression Model

fit.mult.impute(formula = Survive ~ rcs(Age) + GCS + Mechanism + 
    rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + 
    Sex + Other + rcteyemi, fitter = lrm, xtrans = a, data = ASDH_Paper1.1)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          5998    LR chi2    1607.92    R2       0.364    C       0.834    
 0           1281    d.f.            25    g        1.578    Dxy     0.667    
 1           4717    Pr(&gt; chi2) &lt;0.0001    gr       4.846    gamma   0.669    
max |deriv| 0.003                          gp       0.221    tau-a   0.224    
                                           Brier    0.120                     

                              Coef     S.E.    Wald Z Pr(&gt;|Z|)
Intercept                     -55.6574 58.3464 -0.95  0.3401  
Age                            -0.0084  0.0128 -0.66  0.5105  
Age'                           -0.0335  0.0612 -0.55  0.5838  
Age''                           0.0050  0.2365  0.02  0.9830  
Age'''                          0.2321  0.3387  0.69  0.4930  
GCS                             0.1099  0.0124  8.88  &lt;0.0001 
Mechanism=Fall &gt; 2m            -0.0631  0.1138 -0.55  0.5793  
Mechanism=Other                 0.2354  0.1381  1.70  0.0883  
Mechanism=RTC                   0.0315  0.1319  0.24  0.8114  
Mechanism=Shooting / Stabbing   1.9297  1.0930  1.77  0.0775  
ISS                            -0.1012  0.0335 -3.02  0.0025  
ISS'                           -0.0599  0.1366 -0.44  0.6613  
ISS''                           2.1581  2.0120  1.07  0.2834  
neuroFirst                     -0.3753  0.0888 -4.23  &lt;0.0001 
inctoCrand                     -0.0007  0.0054 -0.13  0.9002  
inctoCrand'                    -0.0496  0.3116 -0.16  0.8734  
inctoCrand''                    0.1316  0.6021  0.22  0.8270  
inctoCrand'''                  -0.1078  0.3224 -0.33  0.7381  
inctoCranYN                    -0.1697  0.6172 -0.27  0.7834  
Yeardecimalc                    0.0281  0.0291  0.96  0.3349  
Yeardecimalc'                   0.0682  0.0600  1.14  0.2553  
Yeardecimalc''                 -1.4037  0.7685 -1.83  0.0678  
Yeardecimalc'''                10.2513  4.8156  2.13  0.0333  
Sex=Male                        0.0595  0.0890  0.67  0.5037  
Other=1                         0.3579  0.1641  2.18  0.0292  
rcteyemi                        1.1862  0.0799 14.85  &lt;0.0001 


&gt; anova(rcs.ASDH)
                Wald Statistics          Response: Survive 

 Factor          Chi-Square d.f. P     
 Age              78.39      4   &lt;.0001
  Nonlinear        6.23      3   0.1011
 GCS              78.86      1   &lt;.0001
 Mechanism         7.53      4   0.1104
 ISS              76.46      3   &lt;.0001
  Nonlinear       31.16      2   &lt;.0001
 neuroFirst       17.87      1   &lt;.0001
 inctoCrand        3.22      4   0.5214
  Nonlinear        3.19      3   0.3630
 inctoCranYN       0.08      1   0.7834
 Yeardecimalc     44.83      4   &lt;.0001
  Nonlinear        4.67      3   0.1979
 Sex               0.45      1   0.5037
 Other             4.76      1   0.0292
 rcteyemi        220.51      1   &lt;.0001
 TOTAL NONLINEAR  45.39     11   &lt;.0001
 TOTAL           715.22     25   &lt;.0001
&gt; 
</code></pre>

<p>Can anyone help me understand why the statistical results for Yeardecimal are so starkly different?</p>
"
"0.134993461412565","0.135768846660426","131331","<p>This is the first time I am posting a question, so please excuse any etiquette violations and poorly worded questions!</p>

<p>I am working on the analysis for a chapter of my thesis. I am examining the behavioural response of an animal to a visual stimulus, and trying to determine which of eight explanatory variables (and their two-way interactions) affect this response. I recorded the response on an ordinal scale of 0 (no response), 1 (attention to but no avoidance of stimulus) or 2 (escape response to stimulus). I am leaning towards collapsing categories and using logistic regression where a 1 is an escape response, and 0 is anything else because logistic regression seems much easier to interpret. </p>

<p>I have 794 observations. I am including observer and location (because field sites differed) as random effects, although I am unsure this is a good approach. </p>

<p>I am having trouble with model selection. I ran all possible subsets using the dredge function in packing 'MuMIn'. I thought I was avoiding data dredging by </p>

<ul>
<li>including main effects which were selected because I thought they would have an effect (rather than all conceivable variables)</li>
<li>including only the two-way interactions of interest (R will not run if the global model includes all possible two-way interactions because of the huge number of terms/models)</li>
</ul>

<p>I've come to realise that the second point may be problematic because it leads to an unbalanced model set as in Burnham and Anderson (2002). </p>

<blockquote>
  <p>Page 169: When assessing the relative importance of variables using sums of the AIC    weights, it is important to achieve a balance in the number of models that contain each variable j.</p>
</blockquote>

<p>My questions are</p>

<ol>
<li><p>Is it possible to have a balanced model set without it being considered data dredging? If so, how? </p></li>
<li><p>Is my approach at all reasonable? If not, are there other avenues I should explore? I started with Hosmer&amp;Lemeshow purposeful forward selection, as advocated by my supervisor, but I had some issues with this which I can elaborate on if necessary. </p></li>
</ol>
"
"0.0674967307062827","0.0678844233302131","132787","<p>How do you calculate marginal effects of parameters of logit model in R uging package {glm}?</p>

<p>Are following codes correct?</p>

<pre><code>#### preparation ####
# dependent variable
yseed &lt;- rnorm(100)
y &lt;- ifelse(yseed &gt; 0, 1, 0)

# independent variables
x1 &lt;- rnorm(100, mean=100, sd=20)
x2 &lt;- rnorm(100, mean=50, sd=20)
X &lt;- cbind(1, x1, x2)ã€€
Xmean &lt;- apply(X, 2, mean)

#### analysis ####
# logit model
res &lt;- glm(y ~ x1 + x2, family=binomial(link=""logit""))
summary(res)

# Marginal effects (ME) calculation
LAMBDA &lt;- function(x) { 1 / (1 + exp(-x))}ã€€# cdf of standard logistic distribution

# ME of (intercept)
ME_1  &lt;- coef[1] * LAMBDA(Xmean %*% coef(res)) * (1 - LAMBDA(Xmean %*% coef(res)))
# ME of x1   
ME_x1 &lt;- coef[2] * LAMBDA(Xmean %*% coef(res)) * (1 - LAMBDA(Xmean %*% coef(res))) 
# ME of x2
ME_x2 &lt;- coef[3] * LAMBDA(Xmean %*% coef(res)) * (1 - LAMBDA(Xmean %*% coef(res))) 
</code></pre>
"
"0.126274820515127","0.127000127000191","133248","<p>Iâ€™m analyzing student performance data. In my dataset, each row corresponds to a student and each column contains several performance metrics (continuous) and the student type (categorical, 4 types). The student type was computed in another analysis, using Expectation-Maximization, based on how students were graded over time. My sample is small, with 50 students.</p>

<p>I want to understand what characterizes each student type, regarding the performance features I have. I want to understand things like â€œthe more grade they have the more likely is to belong to a particular clusterâ€ and so on, if they are present at all in my data.</p>

<p>My first question is:
1) I believe that what I need is Multinomial Logistic Regression. Am I right or is there a better way to achieve this?</p>

<p>If yes, Iâ€™ve been exploring Multinomial Logistic Regression in R, using the multinom of the nnet package, but I need help with the following:</p>

<p>2) Understanding if the model is any good. So far I have the percentage of correctly classified instances, but I know this is not a very good goodness of fit measure. </p>

<p>3) How to assess how good each individual predictor is. I know how to look for the exponentiated B, but I donâ€™t know how to assess its significance. I read that using the t-distribution to compute the p-value here is usually a mistake. I found a <a href=""http://stats.stackexchange.com/questions/63222/getting-p-values-for-multinorm-in-r-nnet-package"">similar post here</a>, but a clear answer was not provided.</p>

<p>Thank you in advance for any answer, suggestion or comment.</p>
"
"0.196785094903831","0.186273320869547","133571","<p>I know there are already lots of questions around this topic (especially <a href=""http://stats.stackexchange.com/questions/16390/when-to-use-generalized-estimating-equations-vs-mixed-effects-models/16415#16415"">this one</a> and <a href=""http://stats.stackexchange.com/questions/24689/interpreting-coefficients-of-ordinal-logistic-regression-when-there-is-clusterin/29701#29701"">this one</a>) but I haven't really seen anything that directly helps me (It will be obvious I'm not a great statistician, but I'll do my best to explain). </p>

<p>I am running an ordinal regression in R (<code>clm</code> and <code>clmm</code>). My response variable is a rating between 0 and 4. I have two types of explanatory variables: individual and scenario variables [let's say <code>IVs</code> and <code>SVs</code>]. </p>

<p>Six different scenario variables (all dummies with at most 4 different values) represent potential collaboration scenarios that get rated by the respondent (between 0 and 4) creating the response variable. (Research design is a conjoint analysis; there are a total of 192 different scenarios possible)</p>

<p>On top of that I have a variety of individual characteristics about the respondent (age, gender, work experience, networking skills, ...) all derived from a survey.</p>

<p>Every respondent rates between 3 and 16 different scenarios (average 8.1); every scenario is rated by at least 8 respondents. Every respondent and every scenario have a unique identifier (called <code>IVid</code> and <code>SVid</code>). So they are non nested within each other.</p>

<p>Thus the basic regression looks like this:</p>

<pre><code>clm.base &lt;- clm(rating ~ SVs + IVs, data = dt) 
</code></pre>

<p>The hypothesis I am trying to test is that there are specific individual characteristics, that will influence the rating of the scenarios, independent of the actual content of the scenarios. Basically, some people are more or less favourable to all types of collaboration scenarios. </p>

<p>Now a reviewer of my paper asks me to include individual fixed effects (which in management [my field] basically means dummies for each individual). My assumption originally was that this would result in all individual variables being dropped. This is exactly what happens when I use another model (package <code>lfe</code>)</p>

<pre><code>felm.complete &lt;- felm(rating ~ SVs + IVs | SVid + IVid | 0 | IVid, data = dt) 
</code></pre>

<p>In this regression basically all my variables are perfectly collinear as expected.
However, when I approximate this in the ordinal package, there is no perfect collinearity. I presume this is related that <code>clmm</code> adds so-called 'random effects'. The regression takes a couple of minutes to run but eventually returns results</p>

<pre><code>clmm.complete &lt;- clmm(rating ~ SVs + IVs + (1|SVid) + (1|IVid), data = dt)
</code></pre>

<p>Now, the results here are pretty useless:</p>

<ul>
<li>All but one of my IVs are insignificant</li>
</ul>

<p>I am trying to understand what exactly happens when adding the <code>(1|IVid)</code> term in the <code>clmm</code> model. If it basically adds something like an individual dummy than the fact almost everything is now insignificant is no surprise. The coefficients of the <code>IVid</code> dummies would capture the effect I am looking for (some people rate all scenarios higher or lower, regardless of scenario content) most accurately.</p>

<p>Now I wonder whether this interpretation is correct or whether the results I got from running the simple <code>clm</code> regression are just not reliable? </p>

<p>Concretely, I'd like to find out:</p>

<ul>
<li>What happens when adding a random effect to <code>clmm</code></li>
<li>A laymen explanation of how the Laplace approximation works</li>
<li>How to group errors around individuals when running <code>clm</code></li>
<li>Is it possible to extract the coefficients of these random effects <code>(1|id)</code> for as far as there is such a thing?</li>
</ul>
"
"0.116907766928076","0.117579270250443","134335","<p>I am using <a href=""http://cran.r-project.org/web/packages/twang/vignettes/mnps.pdf"" rel=""nofollow"">mnsp</a> function to estimate propensity scores for multiple treatments. Then, I generate weights using the survey package, but I cannot use svyglm to estimate my treatment effects because my outcome is not binary or numeric. </p>

<p>My outcome variable has 3 categories, therefore I want to estimate relative risk ratios by running a multinomial logistic model. However, survey package does not allow multinomial logit. It seems like mlogit function allows weights. Would it be fine to just plug in the weights I derived from the get.weights function? </p>

<p>I am a novice in R, any recommendations are welcome. Here is my R code:</p>

<pre><code>DAT &lt;-read.delim(""DAT.txt"", header=TRUE)
library(twang)
set.seed(1)
mnps.DAT &lt;- mnps(city_ber ~ age + unemplyd + student + moedum + faedum,
                data = TIES, estimand = ""ATE"", verbose = FALSE,
               stop.method = c(""es.mean"", ""ks.mean""),
                n.trees = 10000)
#Diagnostics
plot(mnps.DAT, plots = 1)
plot(mnps.DAT, plots = 2)
plot(mnps.DAT, plots = 3, pairwiseMax = FALSE, figureRows = 3)
means.table(mnps.DAT, stop.method = ""es.mean"", digits = 3)

#Generating Weights
require(survey)
DAT$w &lt;- get.weights(mnps.DAT, stop.method = ""es.mean"")
    design.mnps &lt;- svydesign(ids=~1, weights=~w, data=DAT)
    summary(DAT$w)

#Outcome analysis?
</code></pre>
"
"NaN","NaN","134885","<p>Since a feedforward NN with a logistic function as activation function is not linear, does it make sense to reduce variables first with principal components or discriminant analysis?</p>

<p>Because shouldn't be done this before training the NN as with logistic regression?</p>
"
"0.134993461412565","0.135768846660426","135792","<p>I'm interested in building a set of candidate models in R for an analysis using logistic regression. Once I build the set of candidate models and evaluate their fit to the data using AICc (<code>aicc = dredge(results, eval=TRUE, rank=""AICc"")</code>), I would like to use k-fold cross fold validation to evaluate the predictability of the final model chosen from the analysis. I have a few questions associated to k-fold cross validation: </p>

<ol>
<li><p>I assume you use your entire data set for initially building your candidate set of models. For example, say I have 20,000 data values, wouldn't I first build my candidate set of models based on the entire 20,000 data values? Then do use AIC to rank the models and select the most parsimonious model?</p></li>
<li><p>After you select the final model (or model averaged model), would you then conduct a k-fold cross validation to evaluate the predictability of the model? </p></li>
<li><p>What is the easiest way to code a k-fold cross-validation in R? </p></li>
<li><p>Does the k-fold cross validation code break up your entire data set (e.g., 20,000 data values) into training and validation sets automatically? Or do you have to subset the data manually? </p></li>
</ol>
"
"0.0477273959903348","0.0480015360737319","137148","<p>I am looking for advice on what package and syntax to use in R for modelling a logistic multilevel analysis with variables as follows:</p>

<ul>
<li>Dependent variable (binary) = 0 or 1</li>
<li>Independent variable 1 (fixed effect, 2 levels) = A or B</li>
<li>Independent variable 2 (10 levels, cluster-randomized based on number of subjects, 6 nested within A and 4 nested within B) = A1, A2, A3, A4, A5, A6 &amp; B1, B2, B3, B4</li>
<li>Units of measure (rows) = individual subjects</li>
</ul>

<p>I am interested in extracting an odds ratio with 95% CI for the fixed effect. Any advice or resources are much appreciated, thanks!</p>
"
"0.208299433189372","0.18854629074174","138230","<p>I want to perform propensity score matching of observational data of an Intensive Care Unit in order to find out wheather hydroxyethyl starch is better or worse than colloids in terms of renal replacement therapy (RRT), Akute Kidney Injury (AKI) and mortality. </p>

<p>I use the MatchIt package in R (King et al. 2007 - <a href=""http://gking.harvard.edu/matchit"" rel=""nofollow"">http://gking.harvard.edu/matchit</a>). This package is quite well documented. But there are some things that I dont understand.
First I matched on sociodemographic covariates (as this seems standard protocol with matching): Gender, weight, height and age.
Nearest neighbor matching seems to have worked:</p>

<p>NN matching</p>

<pre><code>m.out.nn

Call: 
matchit(formula = treat1 ~ Geschlecht + Gewicht.kg + Groesse.cm + 
    Alter.bei.ITS.Aufn + BMI, data = hes.vs.kristall.clean, method = ""nearest"")

Sample sizes:
          Control Treated
All           359    3944
Matched       359     359
Unmatched       0    3585
Discarded       0       0

Treatment status treat1 is HES = yes , Colloids otherwise btw. I did a numerical balance check and balance actually WORSENED after matching. Overall as well as some of the covariates drastically:
Call:
matchit(formula = treat1 ~ Geschlecht + Gewicht.kg + Groesse.cm + 
    Alter.bei.ITS.Aufn + BMI, data = hes.vs.kristall.clean, method = ""nearest"")

Summary of balance for all data:
                   Means Treated Means Control SD Control Mean Diff eQQ Med eQQ Mean  eQQ Max
distance                  0.9168        0.9145     0.0131    0.0022  0.0018   0.0023   0.0388
Geschlecht                0.0041        0.0000     0.0000    0.0041  0.0000   0.0056   1.0000
Geschlechtm               0.6463        0.6100     0.4884    0.0363  0.0000   0.0362   1.0000
Geschlechtw               0.3496        0.3900     0.4884   -0.0403  0.0000   0.0390   1.0000
Gewicht.kg               79.1349       77.8930    18.2092    1.2419  2.0000   1.8462  30.0000
Groesse.cm              169.9184      169.9861    11.9693   -0.0677  0.0000   0.7604  30.0000
Alter.bei.ITS.Aufn       64.5950       63.4808    14.4918    1.1142  0.8000   1.2916   6.2000
BMI                      28.4858       27.8005    15.1559    0.6853  0.7080   2.1550 347.8520


Summary of balance for matched data:
                   Means Treated Means Control SD Control Mean Diff eQQ Med eQQ Mean  eQQ Max
distance                  0.9357        0.9145     0.0131    0.0212  0.0172   0.0212   0.0687
Geschlecht                0.0446        0.0000     0.0000    0.0446  0.0000   0.0446   1.0000
Geschlechtm               0.9053        0.6100     0.4884    0.2953  0.0000   0.2953   1.0000
Geschlechtw               0.0501        0.3900     0.4884   -0.3398  0.0000   0.3398   1.0000
Gewicht.kg               98.1744       77.8930    18.2092   20.2813 17.0000  20.2813  62.0000
Groesse.cm              164.7103      169.9861    11.9693   -5.2758  2.0000   5.4540  77.0000
Alter.bei.ITS.Aufn       72.5702       63.4808    14.4918    9.0894  7.0000   9.0894  26.5000
BMI                      44.1753       27.8005    15.1559   16.3748  6.7500  16.3748 258.9020

Percent Balance Improvement:
                   Mean Diff.   eQQ Med  eQQ Mean   eQQ Max
distance            -852.5593 -839.8420 -808.2029  -77.0723
Geschlecht          -998.6072    0.0000 -700.0000    0.0000
Geschlechtm         -714.0668    0.0000 -715.3846    0.0000
Geschlechtw         -742.6908    0.0000 -771.4286    0.0000
Gewicht.kg         -1533.1522 -750.0000 -998.5214 -106.6667
Groesse.cm         -7691.0845      -Inf -617.2161 -156.6667
Alter.bei.ITS.Aufn  -715.7611 -775.0000 -603.7093 -327.4194
BMI                -2289.4307 -853.3898 -659.8482   25.5712

Sample sizes:
          Control Treated
All           359    3944
Matched       359     359
Unmatched       0    3585
Discarded       0       0
</code></pre>

<p>How can this be possible?</p>

<p>I also did genetic matching (Sekhon 2011 - <a href=""http://sekhon.berkeley.edu/matching/"" rel=""nofollow"">http://sekhon.berkeley.edu/matching/</a>). This is a fancy algorithm that automatically optimizes covariate balance. There covariate balance has indeed improved (as it should have):</p>

<pre><code>Genetic matching
load(file=""m.out.genetic.RData"")
Numerical Balance Check 
summary(m.out.genetic)

Call:
matchit(formula = treat1 ~ Geschlecht + Gewicht.kg + Groesse.cm + 
    Alter.bei.ITS.Aufn, data = hes.vs.kristall.clean, method = ""genetic"")

Summary of balance for all data:
                   Means Treated Means Control SD Control Mean Diff eQQ Med eQQ Mean eQQ Max
distance                  0.9167        0.9147     0.0126    0.0021  0.0019   0.0022  0.0374
Geschlecht                0.0041        0.0000     0.0000    0.0041  0.0000   0.0056  1.0000
Geschlechtm               0.6463        0.6100     0.4884    0.0363  0.0000   0.0362  1.0000
Geschlechtw               0.3496        0.3900     0.4884   -0.0403  0.0000   0.0390  1.0000
Gewicht.kg               79.1349       77.8930    18.2092    1.2419  2.0000   1.8462 30.0000
Groesse.cm              169.9184      169.9861    11.9693   -0.0677  0.0000   0.7604 30.0000
Alter.bei.ITS.Aufn       64.5950       63.4808    14.4918    1.1142  0.8000   1.2916  6.2000


Summary of balance for matched data:
                   Means Treated Means Control SD Control Mean Diff eQQ Med eQQ Mean eQQ Max
distance                  0.9167        0.9164     0.0105    0.0003  0.0018   0.0021  0.0374
Geschlecht                0.0041        0.0000     0.0000    0.0041  0.0000   0.0056  1.0000
Geschlechtm               0.6463        0.6481     0.4782   -0.0018  0.0000   0.0364  1.0000
Geschlechtw               0.3496        0.3519     0.4782   -0.0023  0.0000   0.0392  1.0000
Gewicht.kg               79.1349       79.0556    15.9832    0.0793  2.0000   1.7801 30.0000
Groesse.cm              169.9184      170.0479    10.6992   -0.1296  0.0000   0.7703 30.0000
Alter.bei.ITS.Aufn       64.5950       64.7378    13.3160   -0.1428  0.8000   1.2440  6.2000

Percent Balance Improvement:
                   Mean Diff. eQQ Med eQQ Mean eQQ Max
distance              83.7418  3.8423   2.7923       0
Geschlecht             0.0000  0.0000  -0.5602       0
Geschlechtm           95.1066  0.0000  -0.5602       0
Geschlechtw           94.3414  0.0000  -0.5602       0
Gewicht.kg            93.6115  0.0000   3.5817       0
Groesse.cm           -91.3359  0.0000  -1.2969       0
Alter.bei.ITS.Aufn    87.1817  0.0000   3.6903       0

Sample sizes:
          Control Treated
All           359    3944
Matched       357    3944
Unmatched       2       0
Discarded       0       0
</code></pre>

<p>I also checked balance graphically and it did improve (despite being good pre-matching).</p>

<p>Now my questions are:</p>

<ol>
<li><p>Can I use the nearest neighbor matched data? How could I change this so that balance does improve? What kind of distance metric does Nearest neighbor matching use (by default) (Euclidean ?). Because with Euclidean the non-Boolean covariates (Gender) could be made more important than they are.</p></li>
<li><p>How can I perform analysis after matching? - How can I get the Average Treatment Effect (ATE) and the Average Treatment Effect on the Treated (ATET) in terms of HES for AKI, RRT and mortaility and does that make sense for these response variables (AKI, RRT and mortaility)? Or should I get the odds ratio for Akute Kidney Injury, renal replacement therapy and mortaility from the matched observational data? How do I get these values?
I know that MatchIt recommends using Zelig to get these values but that didn't seem to work with my data. 
Can I use logistic regression with the matched data to get the odds ratio of HES vs. Cristalloids of AKI, RRT and mortality ?</p></li>
</ol>
"
"0.0674967307062827","0.0678844233302131","138923","<p>I'm working on a dataset where I have dates as the main unit of analysis. I'm trying to see if two events are related; that is, if the first event happens, will the second event happen within a month of the first event? </p>

<p>I have the dates for both, but I'm kind of at a loss for how to proceed. I thought I should be using logistic regression because the variables (outcomes) are binary: either the second event happened within a month of the first event, or it didn't. Should I? How do I make this month-long envelope after the first event? Am I totally on the wrong track?  </p>
"
"0.165823880633098","0.179605302026775","139528","<p>When modelling continuous proportions (e.g. proportional vegetation cover at survey quadrats, or proportion of time engaged in an activity), logistic regression is considered inappropriate (e.g. <a href=""http://www.esajournals.org/doi/full/10.1890/10-0340.1"" rel=""nofollow"">Warton &amp; Hui (2011) The arcsine is asinine: the analysis of proportions in ecology</a>). Rather, OLS regression after logit-transforming the proportions, or perhaps beta regression, are more appropriate.</p>

<p>Under what conditions do the coefficient estimates of logit-linear regression and logistic regression differ when using R's <code>lm</code> and <code>glm</code>?</p>

<p>Take the following simulated dataset, where we can assume that <code>p</code> are our raw data (i.e. continuous proportions, rather than representing ${n_{successes}\over n_{trials}}$):</p>

<pre><code>set.seed(1)
x &lt;- rnorm(1000)
a &lt;- runif(1)
b &lt;- runif(1)
logit.p &lt;- a + b*x + rnorm(1000, 0, 0.2)
p &lt;- plogis(logit.p)

plot(p ~ x, ylim=c(0, 1))
</code></pre>

<p><img src=""http://i.stack.imgur.com/AzWOX.png"" alt=""enter image description here""></p>

<p>Fitting a logit-linear model, we obtain:</p>

<pre><code>summary(lm(logit.p ~ x))
## 
## Call:
## lm(formula = logit.p ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.64702 -0.13747 -0.00345  0.15077  0.73148 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.868148   0.006579   131.9   &lt;2e-16 ***
## x           0.967129   0.006360   152.1   &lt;2e-16 ***
## ---
## Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
## 
## Residual standard error: 0.208 on 998 degrees of freedom
## Multiple R-squared:  0.9586, Adjusted R-squared:  0.9586 
## F-statistic: 2.312e+04 on 1 and 998 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>Logistic regression yields:</p>

<pre><code>summary(glm(p ~ x, family=binomial))
## 
## Call:
## glm(formula = p ~ x, family = binomial)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -0.32099  -0.05475   0.00066   0.05948   0.36307  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  0.86242    0.07684   11.22   &lt;2e-16 ***
## x            0.96128    0.08395   11.45   &lt;2e-16 ***
## ---
## Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 176.1082  on 999  degrees of freedom
## Residual deviance:   7.9899  on 998  degrees of freedom
## AIC: 701.71
## 
## Number of Fisher Scoring iterations: 5
## 
## Warning message:
## In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>Will the logistic regression coefficient estimates always be unbiased with respect to the logit-linear model's estimates?</p>
"
"0.0826662747682189","0.083141099321054","139653","<p>Original post on stackoverflow:
<a href=""http://stackoverflow.com/questions/28773153/how-to-do-regression-model-selection-if-dummy-variables-are-involved"">http://stackoverflow.com/questions/28773153/how-to-do-regression-model-selection-if-dummy-variables-are-involved</a></p>

<p>I am trying to do a logistic regression analysis in R with two continuous explanatory variables and six other explanatory categorical variables, and find a good regression model to do predictions. When I do step-wise model selections, there are always some levels of certain categorical variables identified as insignificant. I am just wondering how should I deal with this situation. Should I simply drop these levels, or I should force the program to keep all levels of the categorical variables and try to drop the relatively insignificant variables?</p>

<p>Thanks a lot!</p>
"
"0.165823880633098","0.179605302026775","140761","<p>I have a question regarding the use of propensity score in a survival analysis with use of mutliple imputation to handle missing data. The question is of theoretical nature and may well apply to other situations.</p>

<p>I have a data set of <em>n</em> individuals. The aim is to estimate the effect of a treatment on a binary outcome (death). The analysis is based on propensity score; the propensity score is derived by means of logistic regression, which includes 30 predictors variables. Effect estimation is carried out by means of Cox regression (which uses the propensity score in various ways [stratification, covariate adjustments etc]). There are a large number of patients, and on average 2â€“7% missing for each variable (of which there are 30 included in the prop. score).</p>

<p>Thus, I have a large data set with a substantial amount of missing data (at least in terms of complete cases) which is why I use multiple imputation - 5 complete data sets are imputed. Now the question is what to do with the muliply imputed data sets; which one of the strategies below should I prefer?</p>

<p><strong>1.</strong> Calculate one average propensity score for each individual using the 5 separate data sets. That way, each individual will have one propensity score, which is the average from the n complete data sets. Then do the Cox regression..</p>

<p><strong>2.</strong> Analyze each separate multiply imputed data set (with Cox regression), and then pool the 5 hazard ratio estimates to one hazard ratio.</p>

<p>The second method appears to be used more often, but is it better/worse?</p>

<p>Any thoughts about this?</p>
"
"0.0853773614587515","0.0858677581482184","140929","<p>I wish to carry out logistic regression analysis using Firth's method, as implemented in R logistf package, to analyse SNP case/control data, for rare variants, whilst controlling for stratification using PCA eigenvectors as covariates. I wish to obtain p-values for each SNP (additive model).</p>

<p>Previously I have performed logistic regression analysis using PLINK:</p>

<pre><code>plink  --bfile snpdata --logistic --ci 0.95 --covar plink2_pca.eigenvec --covar-number 1-2 --out snpout
</code></pre>

<p>I would like to perform similar analysis, but wish to handle quasi-complete separation of the rare variants in my data sets.</p>

<p>I have followed a SNP analysis example provided with logistf and been able to obtain P values:</p>

<p>A very small sample of the snpdata (cases: case 1, control 0; SNP additive allele counts for minor allele: 0, 1, 2):</p>

<pre><code>           PC01         PC02 case exm226_A exm401_A exm4584_A exm146_A
1  -0.003092320 -0.002737810    1            0       0       0       0
2   0.015637300  0.008232330    1            0       0       0       0
3   0.006746730  0.008704400    1            0       1       0       1
4   0.001438270  0.000875751    0            0       0       0       0
5  -0.004161490  0.011407500    0            0       0       2       0

for(i in 1:ncol(snpdata)) snpdata[,i] &lt;-as.factor(snpdata[,i])
snpdata &lt;- snpdata[sapply(snpdata,function(x) length(levels(x))&gt;=2)] 
fitsnp &lt;- logistf(data=snpdata, formula=case~1, pl=FALSE)
add1(fitsnp)
</code></pre>

<p>But I am not clear on how to pass the eigenvectors in as covariates, or whether I can used the eigenvector values as is, or need to convert to these as factors?   </p>

<pre><code>fitsnp &lt;- logistf(data=snpdata,formula=case~(1+PC01+PC02), pl=FALSE)
</code></pre>

<p>I'm not sure if I am on the right track here and can't find a sufficiently similar example on-line to follow.</p>

<p>I would appreciate any assistance, or explanation if I am going completely wrong here.</p>

<p>Thanks in advance.</p>
"
"0.208038895960218","0.19822153725485","140972","<p>Iâ€™m using a maximal logistic regression model to analyze some data. I would like to keep using this technique if possible, just include more data in the model. The main data Iâ€™m looking at is counts of a particular behavior over items in a sequence, and I would like my analysis to also include data from a post-experiment questionnaire (8 items, 1-9 Likert scored). Hereâ€™s some info about my data:</p>

<pre><code>'data.frame':
Pair          : Factor w/ 36 levels ""1"",""2"",""3"",""4"",..: 1 1 1 1 1 1 1 1 1 1 ...
SpeakerID     : Factor w/ 72 levels ""10A"",""10B"",""11A"",..: 21 22 21 22 22 21 22 21 21 22 ...
Speaker       : Factor w/ 2 levels ""A"",""B"": 1 2 1 2 2 1 2 1 1 2 ...
Condition1     : Factor w/ 4 levels ""ANTI"",""CONTROL"",..: 1 1 1 1 1 1 1 1 1 1 ...
..- attr(*, ""contrasts"")= num [1:4, 1:3] -0.333 1 -0.333 -0.333 0.25 ...
.. ..- attr(*, ""dimnames"")=List of 2
.. .. .. : chr  ""ANTI"" ""CONTROL"" ""IN"" ""OUT""
.. .. .. : NULL
Condition2         : Factor w/ 3 levels ""0"",""90"",""180"": 2 3 1 1 2 1 1 2 2 3 ...
..- attr(*, ""contrasts"")= num [1:3, 1:2] 0 -0.5 0.5 -0.5 0.25 0.25
.. ..- attr(*, ""dimnames"")=List of 2
.. .. ..$ : chr  ""0"" ""90"" ""180""
    .. .. ..$ : NULL
Item         : Factor w/ 16 levels ""MAP1"",""MAP10"",..: 1 9 10 11 12 13 14 15 16 2 ...
Foo       : num  0.847 1.099 1.946 -1.099 -0.452 ...
wtsFoo          : num  0.952 0.889 2.286 0.889 0.468 ...
Close      : num  -1.798 0.202 -1.798 0.202 0.202 ...
Similar    : num  0.505 0.505 0.505 0.505 0.505 ...
Like       : num  -0.833 0.167 -0.833 0.167 0.167 ...
Task1Hard   : num  -0.89 4.11 -0.89 4.11 4.11 ...
Task2Hard: num  -1.02 2.98 -1.02 2.98 2.98 ...
</code></pre>

<p>My analysis is based on this guide to empirical logit analyses:
<a href=""http://talklab.psy.gla.ac.uk/tvw/elogit-wt.html"" rel=""nofollow"">http://talklab.psy.gla.ac.uk/tvw/elogit-wt.html</a>
So far, so good. In my regression model, Iâ€™m testing the fixed effects of Condition1 (4 levels) and Condition2 (3 levels) on Foo (the behavior, expressed as a proportion converted into empirical logit form, see link for how and why). Pair, Pair:Subject (Subject nested within Pair) and Item are included as random effects. Condition1 is between-subjects/pairs and Condition2 is within-subjects. Hereâ€™s the model Iâ€™m using in R:</p>

<pre><code>model &lt;- lmer(Foo ~ Condition1*Condition2 + (1+Condition1 | Pair) 
+ (1+Condition1 | Pair:Subject) + (1+Condition2 | Item), weights=1/wtsFoo, data)
</code></pre>

<p>This all works fine, but hereâ€™s where it gets fun. Where should the questionnaire data go? </p>

<p>Bad idea #1: Each participant has one score for each questionnaire item, so each questionnaire item type should be included as a fixed effect, so that Foo can be predicted by any of the variables discovered in the post-experiment questionnaire (things like social closeness and task difficulty). This is a terrible idea because the questionnaire items are NOT independent variables from Condition1 and Condition2, and if I include them as fixed effects it will introduce a mess of multicollinearity and will just be flat-out wrong.</p>

<p>Bad idea #2: Analyzing the questionnaire data separately. Not such a bad idea, just one that my committee doesnâ€™t like. </p>

<p>Less bad ideas: please suggest a model that allows me to observe the effects of Condition1 and Condition2 on questionnaire items (Close, Similar, Like, Task1Hard, Task2Hard) AND allows me to observe the effects of questionnaire items on Foo. Failing that, explain to me why the only good thing to do is analyze the questionnaire separately from the observation data.</p>

<p>I've read around on Stackexchange and I haven't seen this particular problem covered, although some answers come close to looking useful, I don't yet have the R or stats chops to make them work for me. If I've missed something obvious, please clue me.</p>
"
"0.185209501085959","0.197915403423893","141844","<p>I tried to plot the results of an ordered logistic regression analysis by calculating the probabilities of endorsing every answer category of the dependent variable (6-point Likert scale, ranging from ""1"" to ""6""). However, I've received strange probabilities when I calculated the probabilities based on this formula: $\rm{Pr}(y_i \le k|X_i) = \rm{logit}^{-1}(X_i\beta)$.</p>

<p>Below you see how exactly I tried to calculate the probabilities and plot the results of the ordered logistic regression model (<code>m2</code>) that I fitted using the <code>polr</code> function (<code>MASS</code> package). The probabilities (<code>probLALR</code>) that I calculated and used to plot an ""expected mean score"" are puzzling as the expcected mean score in the plot increases along the RIV.st continuum while the coefficient for <code>RIV.st</code> is negative (-0.1636). I would have expected that the expected mean score decreases due to the negative main effect of <code>RIV.st</code> and the irrelevance of the interaction terms for the low admiration and low rivalry condition (LALR) of the current 2 by 2 design (first factor = <code>f.adm</code>; second factor = <code>f.riv</code>; dummy coding 0 and 1).</p>

<p>Any idea of how to make sense of the found pattern? Is this the right way to calculate the probabilities? The way I used the intercepts in the formula to calculate the probabilities might be problematic (cf., <a href=""https://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression"">Negative coefficient in ordered logistic regression</a>).</p>

<pre><code>m2 &lt;- polr(short.f ~ 1 + f.adm*f.riv + f.adm*RIV.st + f.riv*RIV.st, data=sampleNS)

# f.adm  = dummy (first factor of 2 by 2 design);
# f.riv  = dummy (second factor of 2 by 2 design);
# RIV.st = continuous predictor (standardized)
summary(m2)
Coefficients:
                Value Std. Error t value
f.adm1         1.0203    0.14959  6.8203
f.riv1        -0.8611    0.14535 -5.9240
RIV.st        -0.1636    0.09398 -1.7403
f.adm1:f.riv1 -1.2793    0.20759 -6.1625
f.adm1:RIV.st  0.0390    0.10584  0.3685
f.riv1:RIV.st  0.6989    0.10759  6.4953

Intercepts:
    Value    Std. Error t value 
1|2  -2.6563   0.1389   -19.1278
2|3  -1.2139   0.1136   -10.6898
3|4  -0.3598   0.1069    -3.3660
4|5   0.9861   0.1121     8.7967
5|6   3.1997   0.1720    18.6008
</code></pre>

<p>Here you see how I tried to calculate the probabilities (<code>probLALR</code>) for 1 of the 4 conditions of the 2 by 2 design:</p>

<pre><code>inv.logit  &lt;- function(x){ return(exp(x)/(1+exp(x))) }
Pred       &lt;- seq(-3, 3, by=0.01)
b = c(-2.6563,-1.2139,-0.3598,0.9861,3.1997) # intercepts of model m2
a = c(1.0203,-0.8611,-0.1636,-1.2793,0.0390,0.6989) # coefficients of m2
probLALR   &lt;- data.frame(matrix(NA,601,5))
for (k in 1:5){ 
    probLALR[,k] &lt;- inv.logit(b[k] + a[1]*0 + a[2]*0 + 
                               a[3]*Pred  + a[4]*0*0 + 
                               a[5]*Pred*0 + a[6]*Pred*0)
}

plot(Pred,probLALR[,1],type=""l"",ylim=c(0,1))   # p(1)
lines(Pred,probLALR[,2],col=""red"")             # p(1 or 2)
lines(Pred,probLALR[,3],col=""green"")           # P(1 or 2 or 3)
lines(Pred,probLALR[,4],col=""orange"")          # P(1 or 2 or 3 or 4)
lines(Pred,probLALR[,5],col=""orange"")          # P(1 or 2 or 3 or 4 or 5)

# option response functions:

orc = matrix(NA,601,6)
orc[,6] = 1-probLALR[,5]        # prob of 6
orc[,5]= probLALR[,5]-probLALR[,4]  # prob of 5
orc[,4]= probLALR[,4]-probLALR[,3]  # prob of 4
orc[,3]= probLALR[,3]-probLALR[,2]  # prob of 3
orc[,2]= probLALR[,2]-probLALR[,1]  # prob of 2
orc[,1]= probLALR[,1]           # prob of 1


plot(Pred,orc[,1],type=""l"",ylim=c(0,1))   # p(1)
lines(Pred,orc[,2],col=""red"")             # p(2)
lines(Pred,orc[,3],col=""green"")           # P(3)
lines(Pred,orc[,4],col=""orange"")          # P(4)
lines(Pred,orc[,5],col=""purple"")          # P(5)
lines(Pred,orc[,6],col=""purple"")          # P(6)

# mean score

mean = orc[,1]*1+orc[,2]*2+orc[,3]*3+orc[,4]*4+orc[,5]*5+orc[,6]*6
plot(Pred,mean,type=""l"",xlab=""RIV.st"",ylab=""expected mean score"",ylim=c(1,6))  
</code></pre>
"
"NaN","NaN","144520","<p>I would like to conduct nominal logistic regression analysis using <code>molt</code>in R, but don't know how to enter the data below into R. Any advice?</p>

<p><img src=""http://i.stack.imgur.com/sAnc4.png"" alt=""enter image description here""></p>
"
"0.106721701823439","0.107334697685273","152868","<p>I have an administrative database with hospital readmissions (binomial: yes/no) and a couple of predictors. I've fitted a multilevel model with the function <code>glmer</code> from the package <code>lme4</code> to estimate the effect of these predictors on readmissions. 
The model has two levels: <code>hospital</code> and <code>patient</code>.
When I calculate the predicted probabilities (the chance of a readmission), and afterwards calculate the readmission ratio's for each hospital (by dividing the observed readmissions by the predicted readmissions), all my ratio's are around 1 which can't be correct.</p>

<p>Before I've calculated the predicted probabilities with a normal logistic regression, which gives more plausible ratio's (from 0,64 to 1,5)</p>

<p>This is my code to calculate predicted probabilities for the multilevel model:</p>

<pre><code>database$predprob &lt;- fitted(model1)
</code></pre>

<p>I've also tried this one, but it gives exactly the same predicted probabilities:</p>

<pre><code>database$predprob &lt;- predict(model1, newdata = database, type = ""response"", na.action = na.omit)
</code></pre>

<p>Does anybody know how to calculate predicted probabilities for a multilevel analysis? I suppose there must be another way to calculate it as my calculated ratios (observed/predicted) are all around 1.</p>
"
"0.106721701823439","0.107334697685273","153510","<p>I am trying to fit a regularized logistic regression to my data using glmnet. Using $\alpha=1$ I get a LASSO-regression, which is what I want. My problem is though that I don't know how the intercept is fitted. In glmnet one has the option to put <code>Intercept=TRUE</code> or <code>Intercept=FALSE</code>. As far as I understand <code>FALSE</code> sets my intercept to 0. When <code>TRUE</code>, I understood that the intercept was fitted as the mean of the $y$-values. Since my data is balanced binary data with values 0 and 1, $\bar{y}=0.5$, but my analysis gives me the value -2.6. </p>

<p>I read <a href=""http://stats.stackexchange.com/questions/13617/how-is-the-intercept-computed-in-glmnet"">How is the intercept computed in GLMnet? </a> but I don't understand it, so I hope someone will give some details. Also, in the link's article there is a likelihood function (13) and (14) on page 8 and I don't understand why it has $1/N$ in front.  </p>
"
"0.179991281883421","0.192339199435604","154782","<p>I'm attempting logistic regression in R for a survey for 613 students. I'm looking to see if there is an association between my <strong>Dependent Variable</strong> (called 'BinaryShelter', coded as 0 or 1, signifying whether students took shelter during a tornado warning) and my <strong>5 independent/predictor variables</strong>. My categorical IV's have anywhere from 3 to 11 distinct levels/categories within them. The other two IV's are binary coded as 0 or 1. The first 10 surveys and R output are given below: </p>

<pre><code>    Survey  KSCat   WSCat   PlanHome    PlanWork    KLNKVulCat  BinaryShelter
    1       J       B       1           1           A           1
    2       A       B       1           0           NA          1
    3       B       B       1           1           C           1
    4       B       D       1           1           A           0
    5       B       D       1           1           A           1
    6       G       E       1           1           A           0
    7       A       A       1           1           B           1
    8       C       F       NA          1           C           0
    9       B       B       1           1           A           1
    10      C       B       0           0           NA          1



Call:
glm(formula = BinaryShelter ~ KSCat + WSCat + PlanHome + PlanWork + 
KLNKVulCat, family = binomial(""logit""), data = mydata)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.0583  -1.3564   0.7654   0.8475   1.6161  

Coefficients:
              Estimate   St. Error  z val   Pr(&gt;|z|)  
(Intercept)    0.98471    0.43416   2.268   0.0233 *
KSCatB        -0.63288    0.34599  -1.829   0.0674 .
KSCatC        -0.14549    0.27880  -0.522   0.6018  
KSCatD         0.59855    1.12845   0.530   0.5958  
KSCatE        15.02995 1028.08167   0.015   0.9883  
KSCatF         0.61015    0.68399   0.892   0.3724  
KSCatG        -1.60723    1.54174  -1.042   0.2972  
KSCatH        -1.57777    1.26621  -1.246   0.2127  
KSCatI        -2.06763    1.18469  -1.745   0.0809 .
KSCatJ        -0.23560    0.65723  -0.358   0.7200  
WSCatB        -0.30231    0.28752  -1.051   0.2931  
WSCatC        -0.49467    1.26400  -0.391   0.6955  
WSCatD         0.52501    0.71082   0.739   0.4601  
WSCatE        -0.32153    0.63091  -0.510   0.6103  
WSCatF        -0.51699    0.74680  -0.692   0.4888  
WSCatG        -0.64820    0.39537  -1.639   0.1011  
WSCatH        -0.05866    0.89820  -0.065   0.9479  
WSCatI       -17.07156 1455.39758  -0.012   0.9906  
WSCatJ       -16.31078  662.38939  -0.025   0.9804  
PlanHome       0.27095    0.28121   0.964   0.3353  
PlanWork       0.24983    0.24190   1.033   0.3017  
KLNKVulCatB    0.17280    0.42353   0.408   0.6833  
KLNKVulCatC   -0.12551    0.24777  -0.507   0.6125  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 534.16  on 432  degrees of freedom
Residual deviance: 502.31  on 410  degrees of freedom
  (180 observations deleted due to missingness)
AIC: 548.31

Number of Fisher Scoring iterations: 14

&gt; Anova(ShelterYorN, Test = ""LR"")
Analysis of Deviance Table (Type II tests)

Response: BinaryShelter
          LR Chisq Df Pr(&gt;Chisq)
KSCat       13.3351  9     0.1480
WSCat       14.3789  9     0.1095
PlanHome     0.9160  1     0.3385
PlanWork     1.0583  1     0.3036
KLNKVulCat   0.7145  2     0.6996
</code></pre>

<p>My questions are:</p>

<p><strong>1)</strong> Does a very large St. Deviation (like the one for KSCatE) indicate that I should not use that level of that categorical IV if I want the model to fit the data better? The ones that had such large St. Deviations were from small groups. Should I not include data from very small groups? For instance if only 2 or 3 people picked category 'E' for KSCat, should I exclude that data?</p>

<p><strong>2)</strong> When using factors for my categorical data, or when adding in more than one IV, sometimes my beta coefficients flip signs. Does this mean I should test for interaction and then try to conduct some form of a PCA or jump straight to doing a PCA?</p>

<p>These next questions may be better asked on stack overflow, but I figured I'd give it a shot here:</p>

<p><strong>3)</strong> I do not want a particular level of the categorical variables to be the reference level. I know that R automatically picks the reference level (A if letters, and the first one if numbers). As in the answer to this question (<a href=""http://stats.stackexchange.com/questions/60817/significance-of-categorical-predictor-in-logistic-regression"">Significance of categorical predictor in logistic regression</a>), I tried fitting the model without an intercept by adding - 1 to the formula to see all coefficients directly. But when I do this, the results only show the 'A' level of the first variable and none of the others. For example, I can see results for 'KSCatA' but not 'WSCatA' or 'KLNKVulCatA'. </p>

<p><strong>4)</strong> How does R handle missing observations for logistic regression? For example survey #10 was missing the 'KLNKVulCat' Variable, but not any of the other IV's. Would R or any other statistical languages not use any of the information for this particular person, or just that particular variable?</p>

<p>Any help is greatly appreciated, thank you.</p>
"
"0.106721701823439","0.107334697685273","154986","<p>We want to do the logistic regression analysis to consider the effect of Age, CD4 on drug resistance mutations. The code that we wrote is:</p>

<pre><code>logist.summary(glm(DRM ~ Age, data = Database, family = binomial),""wald"")
</code></pre>

<p>The results are: </p>

<pre><code>            log.OR OR lower.CI upper.CI p.value
(Intercept)  -0.31 0.74     0.05     9.95  0.8169
Age          -0.07 0.93     0.86     1.00  0.0525
</code></pre>

<p>However, we want to do the test like, we will consider whether, 20 years old differences between the subjects, what the results is? Is it relative to DRMs? We wrote:</p>

<pre><code>logist.summary(glm(DRM ~ I(Age+20), data = Database, family = binomial),""wald"")
</code></pre>

<p>Results:</p>

<pre><code>            log.OR   OR lower.CI upper.CI p.value
(Intercept)   1.17 3.22     0.05   190.62  0.5742
I(Age + 20)  -0.07 0.93     0.86     1.00  0.0525
</code></pre>

<p>I want to ask:</p>

<ul>
<li>Is the code we wrote correct?</li>
<li>Can you help me explain what is meaning of these table?</li>
<li>Why it is the same results for the Age and Age+20? But differences in the Intercept? What does intercept meaning in this case?</li>
</ul>
"
"0.202490192118848","0.203653269990639","156465","<p>The following multilevel logistic model with
one explanatory variable at level 1 (individual level) and
one explanatory variable at level 2 (group level) : </p>

<p>$$\text{logit}(p_{ij})=\pi_{0j}+\pi_{1j}x_{ij}\ldots (1)$$
$$\pi_{0j}=\gamma_{00}+\gamma_{01}z_j+u_{0j}\ldots (2)$$
$$\pi_{1j}=\gamma_{10}+\gamma_{11}z_j+u_{1j}\ldots (3)$$</p>

<p>where , the group-level residuals $u_{0j}$ and $u_{1j}$ are assumed to have a multivariate normal distribution with expectation zero . The variance of the residual errors  $u_{0j}$ is specified as $\sigma^2_0$ , and the variance of the residual errors  $u_{1j}$ is specified as $\sigma^2_1$ .</p>

<p>I want to estimate the parameter of the model and I like to use  <code>R</code> command <code>glmmPQL</code> . </p>

<p>Substituting  equation (2) and (3) in equation (1) yields ,</p>

<p>$$\text{logit}(p_{ij})=\gamma_{00}+\gamma_{10}x_{ij}+\gamma_{01}z_j+\gamma_{11}x_{ij}z_j+u_{0j}+u_{1j}x_{ij}\ldots (4)$$</p>

<p>There are 30 groups$(j=1,...,30)$ and 5 individual in each group .</p>

<p>R code  :</p>

<pre><code>   #Simulating data from multilevel logistic distribution 
   library(mvtnorm)
   set.seed(1234)

   J &lt;- 30             ## number of groups
   n_j &lt;- rep(5,J)     ## number of individuals in jth group
   N &lt;- sum(n_j)

   g_00 &lt;- -1
   g_01 &lt;- 0.3
   g_10 &lt;- 0.3
   g_11 &lt;- 0.3

   s2_0 &lt;- 0.13  ##variance corresponding to specific ICC
   s2_1 &lt;- 1     ##variance standardized to 1
   s01  &lt;- 0     ##covariance assumed zero

   z &lt;- rnorm(J)
   x &lt;- rnorm(N)

   #Generate (u_0j,u_1j) from a bivariate normal .
   mu &lt;- c(0,0)
  sig &lt;- matrix(c(s2_0,s01,s01,s2_1),ncol=2)
  u &lt;- rmvnorm(J,mean=mu,sigma=sig,method=""chol"")

  pi_0 &lt;- g_00 +g_01*z + as.vector(u[,1])
  pi_1 &lt;- g_10 + g_11*z + as.vector(u[,2])
  eta &lt;- rep(pi_0,n_j)+rep(pi_1,n_j)*x
  p &lt;- exp(eta)/(1+exp(eta))

  y &lt;- rbinom(N,1,p)
</code></pre>

<p>Now the parameter estimation .</p>

<pre><code>  #### estimating parameters 
  library(MASS)
  library(nlme)

  sim_data_mat &lt;- matrix(c(y,x,rep(z,n_j),rep(1:30,n_j)),ncol=4)
  sim_data &lt;- data.frame(sim_data_mat)
  colnames(sim_data) &lt;- c(""Y"",""X"",""Z"",""cluster"")
  summary(glmmPQL(Y~X*Z,random=~1|cluster,family=binomial,data=sim_data,,niter=200))
</code></pre>

<h3>OUTPUT :</h3>

<pre><code>      iteration 1
      Linear mixed-effects model fit by maximum likelihood
      Data: sim_data 

      Random effects:
      Formula: ~1 | cluster
              (Intercept)  Residual
      StdDev: 0.0001541031 0.9982503

      Variance function:
      Structure: fixed weights
      Formula: ~invwt 
      Fixed effects: Y ~ X * Z 
                      Value Std.Error  DF   t-value p-value
      (Intercept) -0.8968692 0.2018882 118 -4.442404  0.0000
      X            0.5803201 0.2216070 118  2.618691  0.0100
      Z            0.2535626 0.2258860  28  1.122525  0.2712
      X:Z          0.3375088 0.2691334 118  1.254057  0.2123
      Correlation: 
           (Intr) X      Z     
      X   -0.072              
      Z    0.315  0.157       
      X:Z  0.095  0.489  0.269

      Number of Observations: 150
      Number of Groups: 30 
</code></pre>

<ul>
<li><p>Why does it take only $1$ iteration while I mentioned to take $200$ iterations inside the function <code>glmmPQL</code> by the argument <code>niter=200</code> ?</p></li>
<li><p>Also p-value of group-level variable $(Z)$ and cross-level interaction $(X:Z)$ shows they are not significant . Still why in this <a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1955447/"" rel=""nofollow"">article</a>, they keep the group-level variable $(Z)$ and cross-level interaction $(X:Z)$ for further analysis ?</p></li>
<li><p>Also How are the degrees of freedom <code>DF</code> being calculated ?</p></li>
<li><p>It doesn't match with the relative bias of the various estimates of <a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1955447/table/T1/"" rel=""nofollow"">the table</a> .  I tried to calculate the relative bias as :</p>

<pre><code> #Estimated Fixed Effect parameters :

 hat_g_00 &lt;- -0.8968692 #overall intercept
 hat_g_10 &lt;- 0.5803201  # X
 hat_g_01 &lt;-0.2535626   # Z
 hat_g_11 &lt;-0.3375088   #X*Z

fixed &lt;-c(g_00,g_10,g_01,g_11)
hat_fixed &lt;-c(hat_g_00,hat_g_10,hat_g_01,hat_g_11)


#Estimated Random Effect parameters :

hat_s_0 &lt;-0.0001541031  ##Estimated Standard deviation of random intercept 
hat_s_1 &lt;-  0.9982503 

std  &lt;- c(sqrt(0.13),1) 
hat_std  &lt;- c(0.0001541031,0.9982503) 

##Relative bias of Fixed Effect :
rel_bias_fixed &lt;- ((hat_fixed-fixed)/fixed)*100
[1] -10.31308  93.44003 -15.47913  12.50293

##Relative bias of Random Effect :
rel_bias_Random &lt;- ((hat_std-std)/std)*100
[1] -99.95726  -0.17497
</code></pre></li>
<li>Why doesn't the relative bias match with the table ?</li>
</ul>
"
"0.116907766928076","0.117579270250443","159469","<p>I am relatively new to R. The short version of the data looks ike this:   </p>

<pre><code>sNumber  blockNo running TrialNo    wordTar   wordTar1   Freq Len code code2
1        1       1       5           spouse    violent   5011   6    1     2
1        1       1       5          violent     spouse  17873   7    2     1
1        1       1       5           spouse    aviator   5011   6    1     1
1        1       1       5          aviator       wife    515   7    1     1
1        1       1       5             wife    aviator  87205   4    1     1
1        1       1       5          aviator     spouse    515   7    1     1
1        1       1       9        stability    usually  12642   9    1     3
1        1       1       9          usually   requires  60074   7    3     4
1        1       1       9         requires     client  25949   8    4     1
1        1       1       9           client   requires  16964   6    1     4
2        2       1       5            grimy      cloth    757   5    2     1
2        2       1       5            cloth       eats   8693   5    1     4
2        2       1       5             eats    whitens   3494   4    4     4
2        2       1       5          whitens      woman     18   7    4     1
2        2       1       5            woman    penguin 162541   5    1     1
2        2       1       9              pie   customer   8909   3    1     1
2        2       1       9         customer  sometimes  13399   8    1     3
2        2       1       9        sometimes reimburses  96341   9    3     4
2        2       1       9       reimburses  sometimes     65  10    4     3
2        2       1       9        sometimes   gangster  96341   9    3     1
</code></pre>

<p>I have a code for ordinal regression analysis for one participant for one trial (eye-tracking data - eyeData) that looks like this:</p>

<pre><code>#------------set the path and import the library-----------------
setwd(""/AscTask-3/Data"")
library(ordinal)

#-------------read the data----------------
read.delim(file.choose(), header=TRUE) -&gt; eyeData

#-------------extract 1 trial from one participant---------------
ss &lt;- subset(eyeData, sNumber == 1 &amp; runningTrialNo == 5) # extract the 5th trial from the 1st participant

#-------------delete duplicates = refixations-----------------
ss.s &lt;- ss[!duplicated(ss$wordTar), ] 

#-------------change the raw frequencies to log freq--------------
ss.s$lFreq &lt;- log(ss.s$Freq)

#-------------add a new column with sequential numbers as a factor ------------------
ss.s$rankF &lt;- as.factor(seq(nrow(ss.s))) 

#------------ estimate an ordered logistic regression model - fit ordered    logit model----------
m &lt;- clm(rankF~lFreq*Len, data=ss.s, link='probit')
summary(m)

#---------------get confidence intervals (CI)------------------
(ci &lt;- confint(m)) 

#----------odd ratios (OR)--------------
exp(coef(m))
</code></pre>

<p>The eyeData file is a huge massive of data consisting of 91832 observations with 11 variables. In total there are 41 participants with 78 trials each. In my code I extract data from one trial from each participant to run the anaysis. However, it takes a long time to run the analysis manually for all trials for all participants. Could you, please, help me to create a loop that will read in all 78 trials from all 41 participants and save the output of statistics (I want to save <strong>summary(m), ci, and coef(m)</strong>) in one file.</p>
"
"0.165332549536438","0.166282198642108","160638","<h1>General question</h1>

<p>When I perform a logistic regression using lrm and specify weights for the observations, I get the following warning message:</p>

<blockquote>
  <p>Warning message:
  In lrm(Tag ~ DLL, weights = W, data = tagdata, x = TRUE, y = TRUE) :
    currently weights are ignored in model validation and bootstrapping lrm fits</p>
</blockquote>

<p>My interpretation is that everything that the rms package will tell me regarding goodness-of-fit, notably using the residuals.lrm tool, is wrong. Is this correct?</p>

<h1>Specific example</h1>

<p>To be more specific, I have working example. All the code and output can be found in this <a href=""https://github.com/jwimberley/crossvalidated-posts/tree/master/lrm_gof"" rel=""nofollow"">GitHub repository</a>. I have two CSV tables of data, <a href=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/toystudy.csv"" rel=""nofollow"">toystudy.csv</a> and <a href=""https://github.com/jwimberley/crossvalidated-posts/raw/master/lrm_gof/realstudy.csv"" rel=""nofollow"">realstudy.csv</a>. There are three columns in each:</p>

<ol>
<li>The binomial response $y$ (0 or 1) [called Tag in code]</li>
<li>The predictor $x$ [called DLL in code]</li>
<li>The weight for the observation [called W in code]</li>
</ol>

<p>The former is simulated data, where all the weights are unity and where a logistic regression $log(\pi) = \theta_0 + \theta_1 x$ should fit the data perfectly. The latter is real data from my analysis, where the validity of this simple model is in question. The real data has weighted observations. (Some of the weights are negative, but there is a well-defined reason for this). The analysis code in contained completely in <a href=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/regressionTest.R"" rel=""nofollow"">regressionTest.R</a>; the meat of the code is</p>

<pre><code>library(rms)
fit &lt;- lrm(Tag ~ DLL, weights = W, data = tagdata, x=TRUE, y=TRUE)
residuals(fit,""gof"")
</code></pre>

<p>Here are the results for the two tables of data.</p>

<h3>Case 1: Toy data</h3>

<p>The goodness-of-fit claimed by lrm (which is something called the le Cessie-van Houwelingen-Copas-Hosmer test, I understand?) is very good:</p>

<p><img src=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/toy/residuals.png"" alt=""enter image description here""></p>

<p>This is confirmed by grouping the data into 20 quantiles of the predictor and overlaying the predicted success rate over the average actual success rate:</p>

<p><img src=""http://i.stack.imgur.com/hOEFs.png"" alt=""enter image description here""></p>

<h3>Case 2: Real data</h3>

<p>In this case, the goodness-of-fit reported by lrm is horrendous:</p>

<p><img src=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/real/residuals.png"" alt=""enter image description here""></p>

<p>However, I don't think it should be that bad. Again grouping the data into quantiles, and taking into account the weights when computing the average values in each bin:</p>

<p><img src=""http://i.stack.imgur.com/mgzhc.png"" alt=""enter image description here""></p>

<p>Comparing the prediction to the observed values and their standard errors, I don't think this is that bad (the error bars here depend on how the standard error on a weighted mean is computed, so they might not be 100% right, but should at least be close). On the other hand, if I produce the same plot while ignoring the weights:</p>

<p><img src=""http://i.stack.imgur.com/dId9F.png"" alt=""enter image description here""></p>

<p>I can definitely imagine this fit being as poor as the goodness-of-fit test says.</p>

<h2>Conclusion</h2>

<p>So, is residuals.rm simply ignoring the weights when it calculates its goodness-of-fit statistic? And if so, is there any R package that will do this correctly?</p>
"
"0.143182187971004","0.144004608221196","162463","<p>I am doing some data analysis on a fairly large health data set of patients with diagnoses and the respective procedures received for each event. I was asked to run a multinomial logistic regression on my data.</p>

<p>The dataset has around 4,000 columns of attributes, of which around 3,000 are unique diagnoses. The diagnosis variables take on the value of 1 if the patient had that diagnosis and 0 if he or she did not.  The remaining approximately 1,000 variables pertain to unique procedures, which also take on the value of 1 if the patient has received it, and 0 if he or she did not.</p>

<p>The dataset contains information on approximately 30,000 patients. I, admittedly naively, ran a the multinom function in the multinom package in R on all 4,000 variables, with the dependent variable being the very last procedure the patient has received (marked as ""Final procedure"" in the matrix), but R isn't able to complete the computation. </p>

<p>I would like some overall advice in perhaps a different package I could use for running regressions on large data sets (cannot use bigmemory however because this is on windows) or even perhaps reformatting my data. </p>

<p>Initially, my data set had around 50 columns, because the maximum number of diagnoses and procedures a patient had was 25 diagnoses and 25 procedures, so each column was marked as ""Diagnosis X"" and ""Procedure x,"" with the corresponding element being the actual diagnosis/procedure identifier. For all the patients who did I have all 25 diagnoses/procedures (so most of them), the values in the data frame would just be NA. Now I am wondering if I could perhaps resort to using this data frame instead and have a nicer, smaller matrix to work with? The only real reason I reformatted my data set into the much larger matrix was because my grad student asked me to do so, but maybe this isn't the way to go.</p>
"
"0.0954547919806695","0.0960030721474639","162599","<p>This question is in response to an answer given by @gung in regards to this <a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments"">question</a></p>

<p>I am also wanting to use simulation to conduct a power analysis on a multiple logistic regression. To keep it simple <strong>I want to do a post-hoc power analysis to determine the power associated with my regression</strong>. Lets take an <code>alpha=0.05</code></p>

<p>Lets say we have 1000 samples of data. Our dataset can be assumed as:</p>

<pre><code>set.seed(123)
N &lt;- 1000
var1 &lt;- runif(N, min=0, max=0.5)
var2 &lt;- runif(N, min=0.3, max=0.7)
var3 &lt;- rbinom(n=N, size=1, prob = 0.15)
output &lt;- rbinom(n=N, size=1, prob = 0.05)

df &lt;- data.frame(var1, var2, var3, output)
</code></pre>

<p>And a simple logistic model we are using is</p>

<pre><code>model &lt;- glm(output~var1+var2+var3, 
             data=df,
             family = binomial()
</code></pre>

<p>Now where this question differs from the example, we have our binary output (<code>output</code>) and not a continuous rate. </p>

<p>From what I have read, when unsure of the effect size, select a medium rate.</p>
"
"0.0954547919806695","0.0960030721474639","163819","<p>I am running a multinomial logistic regression model (with 3 possible outcomes) in R. I am trying to find the best way to assess the predictive power/accuracy of the model, and the best thing I've come up with is using a ROC curve.</p>

<p>For multi-class ROC analysis, I know that there is the one vs. one comparison or the one vs. all comparison. For the one vs. one comparison, would I need three separate ROC curves for each possible combination of outcome comparisons? If so, do I need to make a third model for comparing the two outcomes that were initially being compared to the baseline outcome?</p>

<p>For one vs. all comparison is the threshold for a r ""random model"" now 33% instead of 50%?</p>

<p>And finally, is there a better way to go about doing this/visualizing it?</p>

<p>EDIT: I know the pROC package has a multi class.roc function, but I don't totally get what it does.</p>
"
"0.0477273959903348","0.0480015360737319","163986","<p>I effectively want to model the probability of a player winning his service point (a point in which he is the server) based on the values of explanatory variables (namely court surface and opponent world ranking)</p>

<p>Can this be done using a binary response logistic regression?</p>

<p>Consider the fact that I can view my response variable as number of successes out of a total number of trials (for which I have the data). Will it work considering I have both categorical and numerical explanatory variables?</p>

<p>Any feedback on why this will/won't work or how I can make it work would be hugely appreciated! I am doing the analysis in R, so pointers on functions or packages would also be welcome! </p>
"
"0.143903513382412","0.159203084517278","166584","<p>I am conducting a regression in order to predict a tennis player's service point win % i.e. the percentage of points he wins when he is the server.
Model 1 If my DV data lies in the range 0.3-0.9, does it make sense to use a logistic regression? If using logistic I would endeavor to build a model with serve win % as my DV and my IV's as:</p>

<p>+average serve win % of last n matches (maybe n=5 or 10) to account for form </p>

<p>+surface </p>

<p>+player ranking </p>

<p>+opposition ranking</p>

<p>..... Would this be a good model to use? Preliminary logistic regressions just involving serve win % regressed on surface + player ranking + opponent ranking ... are showing some strange results so im losing faith in logistic for this data.</p>

<p>An alternative I'm considering is to use raw variables in a linear regression type model with interactions.... Along the lines of Aiken &amp; West 1991
My dependent variable will be number of service points won in match, and my independent variables will be:</p>

<p>+no. service points played in match +the surface the match played on </p>

<p>+the player's ranking points +the opponents ranking points</p>

<p>+an interaction between player and opponent ranking points </p>

<p>+an interaction between surface and no. points played </p>

<p>+average service points won in last n matches</p>

<p>+average % of service points won in last m matches</p>

<p>Do either of these models stand out as smart or appropriate ways to model this data? For context, for each player I have between 100-350 matches worth of data. I would love to hear what you guys think, or if you have any other suggestions on how to predict serve win % using the stated variables I would really appreciate it. I'm conducting this analysis in R so any code/package suggestions would also be great </p>
"
"0.0674967307062827","0.0678844233302131","172958","<p>I am working on a school enrollment admission project to see how high school students react to scholarship in admission. The purpose is to redesign the scholarship level.</p>

<p>The original policy is 3 levels(0,2000,4000,6000) and used as training data. 
The other attributes are like GPA, ACT/SAT, gender,etc.. Y={enrolled, not enrolled}</p>

<p>What I did is manually expand the levels to (0,1000,2000,...,6000) for this year as testing data. And I used logistic regression and regression tree(LOTUS). </p>

<p>Ideally the probability will increase as the scholarship increases and it will give a sigmoid or S-curve, but not all the plots shown this. I think the reason is there are no data in the training set has the new levels.</p>

<p>I tried conjoint analysis but I don't know what does it mean.</p>

<p>what methods should I use or do I miss something here? </p>
"
"0.106721701823439","0.107334697685273","174518","<p>consider the following data set:</p>

<pre><code>a &lt;- c(1, 2, 3, 1, 4, 1968, 2, 1)
b &lt;- c(2, 1, 2, 4, 3, 1984, 2, 0)
c &lt;- c(3, 3, 4, 2, 1, 1945, 1, 0)
d &lt;- c(4, 1, 4, 3, 2, 1975, 3, 1)
df &lt;- data.frame(rbind(a,b,c,d))
names(df) &lt;- c(""ID"", ""OptionW"", ""OptionX"", ""OptionY"", ""OptionZ"", ""yearofBirth"", ""education"", ""sex"")


ID OptionW OptionX OptionY OptionZ yearofBirth education sex
1       2       3       1       4        1968         2   1
2       1       2       4       3        1984         2   0
3       3       4       2       1        1945         1   0
4       1       4       3       2        1975         3   1
</code></pre>

<p>Two hundred people where asked to rank Options W to Z from 1 to 4 in their effectiveness to lower crime rates in their community. Their age, highest academic degree and sex are annotated as well.
I want to find out:</p>

<ul>
<li>which options are preferred by the majority of citizens?</li>
<li>are there significant differences in what men or women, old or young, well or less well educated citizens believe?</li>
<li>how likely is the ranking order going to change if the person is older/younger, has had more or less formal education and is male or female? </li>
</ul>

<p>I read that a multinomial logistic regression might be the way to go, but I find it hard to adapt the examples I find to my data set. Often they allow for only one option to be chosen, making each choice (W, X Y Z) a level of one variable (Options). But in my case I have several variables (OptionW, OptionX, OptionY, OptionZ) where the ranking placement appears to be the level (1,2,3,..10). Or am I looking at it the wrong way?</p>

<p>Which function from what package would be suitable? And are there other methods available apart from mlr?</p>

<p>I use R mostly for spatial analysis and am not very fluent in statistics. Hopefully you can help me here.</p>
"
"0.0826662747682189","0.083141099321054","176388","<p>The following two question outline how one can plot the results from a survival analysis using R. <a href=""http://stackoverflow.com/questions/9151591/how-to-plot-the-survival-curve-generated-by-survreg-package-survival-of-r"">Q1</a> and <a href=""http://stackoverflow.com/questions/16236939/plot-survival-and-hazard-function-of-survreg-using-curve"">Q2</a></p>

<p>But both of the examples assume, or more directly specify a weibull distribution fitted to the survival model.</p>

<p>Refering to <code>survreg()</code> which is within the <code>survival</code> package in R, the following are possible fitting assumptions:</p>

<ul>
<li>weibull </li>
<li>exponential</li>
<li>gaussian </li>
<li>logistic </li>
<li>lognormal </li>
<li>loglogistic</li>
</ul>

<p>So if we take the example from the <code>survreg()</code> help.</p>

<pre><code>library(survival)

data(ovarian)

head(ovarian)

survival.weibull &lt;- survreg(Surv(time, status) ~ ph.ecog + age + strata(sex), 
                          dist='weibull', lung)

survival.logn &lt;- survreg(Surv(time, status) ~ ph.ecog + age + strata(sex), 
                            dist='lognormal', lung)

survival.logl &lt;- survreg(Surv(time, status) ~ ph.ecog + age + strata(sex), 
                         dist='loglogistic', lung)
</code></pre>

<p>How can one verify the best appropriate distribution to fit. From the summary statistic we get the <code>Loglik(model)</code> value. Is this the best indicator? Or is there a graphical method to visualise the best fit - I was suggested a QQ-plot may be of help.</p>

<p>Thanks in advance for an advice </p>
"
"0.165823880633098","0.166776351882005","176586","<p>This question stems from <a href=""http://stats.stackexchange.com/questions/175853/what-type-of-hypothesis-test-for-multivariate-testing-website"">another I asked last week</a>, where the person answering stated </p>

<blockquote>
  <p>""Finally, and this is very, very important: please don't just run the
  code I've provided, and consider your job complete. If you don't
  actually read up and understand some of how these analyses work, all
  of this information will be less than useless.""</p>
</blockquote>

<p>This is my intention, to really understand what is going on as well as how to interpret.</p>

<p>Context is website testing. Show people a different landing page, change the design and look of each page with a goal of getting more people to purchase online (""success"").</p>

<p>Here is my data:</p>

<pre><code>variant successes   failures
Original    757 49114
Date    553 41794
Cranberry   494 41495
Apple   546 41835
</code></pre>

<p>My script and output are below. I think I understand how to interpret it but just wanted to make sure. My questions:</p>

<ol>
<li>The first thing I want to do is check if there is a difference between the variance overall, or if it's just ebbs n flows. With a p-value of 8.55e-05 translates to 0.0000855 (right?) then yes, there is a meaningful variance between the groups. Is that a correct statement?</li>
<li>Since I'm comparing each group to the original (It's really a case of ""which test can beat the original), then it looks like only first Vs. 4th (Original Vs. Apple) is the only real difference statistically because the p-value is 0.0098. Is this a correct statement?</li>
<li>In my contrast function I have assumed data are read int he order they appear in test2. Is this correct?</li>
<li>Reading more about logistic regression it seems to be used to measure the impact of incrementing a predictor up or down a unit (resulting in the log unit increase or decrease). But in the context of measuring a web page variant performance in this way, why is logistic regression an appropriate method of determining whether or not the variants are different? Put another way, I'm hypothesis testing rather than predicting the impact of each variant, since an observation can only be one variant, not a combination of 1 or more predictors (they can only ever see one of the test pages, not 2 or more test pages).</li>
<li>I edited my data to include only visits from one state, just to experiment and play around. The output I got in this instance was a p-value of 0.001721 in the anova of m whereas the p-values for contrast where between 0.2 -0.3 (reject). If the script says overall there is a variance but at an individual test level there is not, how would I interpret that? I can provide the output if desired.</li>
</ol>

<p>Here is my script &amp; output:</p>

<pre><code>&gt; test2 &lt;- read.csv(""test2.csv"")
&gt; 
&gt; m &lt;- glm(cbind(successes, failures) ~ variant, family=binomial, data=test2)
&gt; anova(m, test='Chisq') # Tests if there's a difference between the variants
Analysis of Deviance Table

Model: binomial, link: logit

Response: cbind(successes, failures)

Terms added sequentially (first to last)


        Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)    
NULL                        3     21.435             
variant  3   21.435         0      0.000 8.55e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; 
&gt; library(lsmeans)
&gt; #lsmeans(m, pairwise ~ variant) # Compares every variant to every other one
&gt; 
&gt; m.comparisons = lsmeans(m, specs = pairwise ~ variant)
&gt; contrast(m.comparisons,
+          list(
+            first.vs.second = c(1,-1,0,0),
+            first.vs.third =  c(1,0,-1,0),
+            first.vs.fourth = c(1,0,0,-1)
+            ), adjust=""tukey"")
 contrast           estimate         SE df    z.ratio p.value
 first.vs.second  0.09192309 0.06248035 NA  1.4712319  0.3667
 first.vs.third  -0.01371955 0.06072602 NA -0.2259254  0.9943
 first.vs.fourth -0.16633346 0.05653998 NA -2.9418735  0.0098

P value adjustment: sidak method for 3 tests 
</code></pre>
"
"0.0826662747682189","0.083141099321054","177673","<p>I am conducting survival analysis research and I am trying to decide which distribution (Weibull, lognormal and loglogistic) best fits my data.</p>

<p>One can ""Fit of univariate distributions to non-censored data"" using the <code>fitdist()</code> from the <code>fitdistrplus</code> package in R.</p>

<p>referring to the examples from the help page, we can fit hypothetical distributions by the following:</p>

<pre><code>fitW &lt;- fitdist(serving, ""weibull"")
fitg &lt;- fitdist(serving, ""gamma"")
fitln &lt;- fitdist(serving, ""lnorm"") 
</code></pre>

<p>etc.</p>

<p>But one cannot fit an assumed log-logistic distribution with the package.</p>

<p>From research, I have found <a href=""https://cran.r-project.org/web/packages/fitdistrplus/vignettes/paper2JSS.pdf"" rel=""nofollow"">this paper</a> (see pg 5-6) which attempts to implement a secondary package, <code>actuar</code> which has the capability to fit a loglogis distribution.</p>

<p>Here we find the code:</p>

<pre><code>fendo.ll &lt;- fitdist(ATV, ""llogis"", start = list(shape = 1, scale = 500))
</code></pre>

<p>which then states at the bottom of the page</p>

<p>It can thus help to find correct initial values for the distribution parameters in non trivial cases, by iterative calls if necessary (see the reference manual for examples (<a href=""https://cran.r-project.org/web/packages/fitdistrplus/fitdistrplus.pdf"" rel=""nofollow"">Delignette-Muller et al., 2014</a>))</p>

<p>So can one explain where these values come from?</p>
"
"0.106721701823439","0.107334697685273","180191","<p>We can apply the Hosmer-Lemeshow goodness of fit to logistic regression modelling and to test if an underlying assumption is not applicable.</p>

<p>This <a href=""https://www.youtube.com/watch?v=MYW8gA1EQCQ"" rel=""nofollow"">link</a> shows a video of the application to a standard <code>glm()</code> model</p>

<p>This <a href=""http://stats.stackexchange.com/questions/132652/how-to-determine-which-distribution-fits-my-data-best-r"">detailed question</a>, outlines various simulation-based tests one can run to assess underlying distributions.</p>

<p><strong>But I want to apply the Hosmer-Lemeshow goodness of fit to survival analysis with assumed underlying data distributions</strong>.</p>

<p>Much literature points one towards a cox proportional hazards model, but from what I understand, a cox ph model does not assume an underlying distribution of data.
Therefore lets take some random data from the <code>survreg()</code> function of the <code>survival</code> package</p>

<pre><code>library(survival)

data(ovarian)

head(ovarian)

s &lt;- Surv(ovarian$futime, ovarian$fustat)
sWei &lt;- survreg(s ~ age,dist='weibull',data=ovarian)
</code></pre>

<p>How can we applying a H+L G.O.F statistic test?
I had hoped to follow this <a href=""http://thestatsgeek.com/2014/02/16/the-hosmer-lemeshow-goodness-of-fit-test-for-logistic-regression/"" rel=""nofollow"">link</a>, however the <code>survreg()</code> does not allow a <code>fitted()</code> function. Thus this does not work</p>

<pre><code>library(ResourceSelection)
hl &lt;- hoslem.test(sWei$y, fitted(sWei), g=10))
</code></pre>
"
"0.134993461412565","0.135768846660426","182286","<p>I am doing a regression analysis for an ordinal response variable with 5 explanatory variables. I will be using the <code>polr()</code> or <code>lrm()</code> functions to do the ordinal logistic regression. For my non-ordinal response variables (e.g., count and binary data), I have been using glmulti for model selection, but this doesn't seem to be compatible with the <code>polr()</code> and <code>lrm()</code> R functions. I've also tried <code>stepAIC()</code>, <code>step()</code> and <code>leap()</code> functions without any luck. The summary of the <code>polr()</code> regression shows an AIC score.</p>

<pre><code>&gt; model1 &lt;- polr(x ~ Age + Gender + StudentType + StudentYear + RacialGroup,
+ data = question8a, Hess =TRUE)
&gt; summary(model1)
Call:
polr(formula = x ~ Age + Gender + StudentType + 
    StudentYear + RacialGroup, data = question8a, Hess = TRUE)

Coefficients:
                                   Value Std. Error  t value
Age                             -0.16691    0.04925 -3.38872
GenderWoman                      0.05514    0.24655  0.22366
StudentTypeUndergraduatestudent -1.36414    0.50748 -2.68807
StudentYear2ndyear              -0.02042    0.29600 -0.06899
StudentYear3rdyear              -0.05997    0.38253 -0.15676
StudentYear4+years               0.89921    0.66430  1.35363
StudentYear4thyear               0.25324    0.42433  0.59680
RacialGroupNon-Indigenous       -2.13460    0.42163 -5.06268

Intercepts:
    Value   Std. Error t value
1|2 -9.9335  1.5283    -6.4999
2|3 -8.3051  1.4752    -5.6298
3|4 -7.2498  1.4567    -4.9770
4|5 -4.8720  1.4240    -3.4214

Residual Deviance: 657.086 
AIC: 681.086 
</code></pre>

<p>I tried to follow this suggestion: <a href=""http://statistics.unl.edu/faculty/bilder/categorical/Chapter5/glmultiFORpolr.R"" rel=""nofollow"">http://statistics.unl.edu/faculty/bilder/categorical/Chapter5/glmultiFORpolr.R</a>, but wasn't able to get it to work. </p>

<p>Has anyone been able to get this to work? Or do I need to compare the 2^5 = 32 model AIC scores by hand? </p>
"
"0.0853773614587515","0.107334697685273","182656","<p>I have data in longitudinal or clustered format (please see the example below). My response variable is dichotomous. I want to examine which factors explains why a subject in the dataset gets Y=1. In the example below I show only one predictor â€“ X.</p>

<p>Since I have a dichotomous response variable, I am thinking of logistic regression. However, the longitudinal format violates the distributional assumption of ML-theory. <strong>So my question is which logistic model would be appropriate here?</strong> And if possible, which R-package would be relevant (if not covered by the standard stats)? </p>

<p>All subjects are countries and observed (let's say) from 1990-1994. A country can get more than 1 Y per year, from different Z's.  I have been thinking of logistic panel models. Although I am not sure which specific model would be appropriate (assuming that panel models are more appropriate group of models). Perhaps random effects as each observation is not of the same nature (a country can get Y=1 from different groups, see variable Z). The Z variable is not a part of analysis though. Grateful for all suggestions!</p>

<pre><code>COUNTRY      YEAR  Y           X        Z

    A        1990  0           0        K
    A        1991  1           0        K
    A        1992  0           0        K
    A        1993  1           0        L
    B        1994  0           1        L
    B        1990  0           1        L
    B        1991  0           1        L
    B        1992  1           1        L
    C        1990  1           0        K
    C        1991  1           0        K
    C        1992  0           0        L
    C        1993  0           1        K
    C        1994  0           1        L
    D        1990  0           1        L
    D        1991  0           0        K
    D        1992  0           0        K
    D        1993  0           1        K
    D        1994  0           1        K
</code></pre>
"
"0.0477273959903348","0.0480015360737319","184391","<p>I am attempting to perform a piecewise/segmented logistic regression on survey data using  <a href=""http://www.asdfree.com/2015/11/statistically-significant-trends-with.html"" rel=""nofollow"">this tutorial</a> as my basis. I have data for the period 2006 to 2013, however 2012 is missing.</p>

<p>The analysis proceeds as expected until the point in step 8 where I add the segmented variable with one breakpoint (the final line of code in the example below).</p>

<pre><code>library(segmented)
df &lt;- data.frame(yr=c(2006:2011,2013),
             mean= c(0.11290830, 0.12814364, 0.11149552, 0.12071058, 0.11776731, 0.10363014, 0.09888132),
             wgt = c(602.2272, 546.2958, 594.1818, 756.0167, 579.1533, 481.9694, 654.3281))
o &lt;- lm( log( mean ) ~ yr , weights = wgt , data = df )
os &lt;- segmented( o , ~yr)
</code></pre>

<p>At this point I get the error message:</p>

<blockquote>
  <p>""Error in segmented.lm(o, ~yr) : only 1 datum in an interval: breakpoint(s) at the boundary or too close each other""</p>
</blockquote>

<p>From my reading, in particular <a href=""http://r.789695.n4.nabble.com/Estimating-and-predicting-using-quot-segmented-quot-Package-td4682541.html"" rel=""nofollow"">here</a>, this is because the breakpoint falls at 2007, thus leaving 2006 on it's own and unable to have a slope calculated for it. I understand that this is likely because I have so few data points.</p>

<p>Does anyone have any tips for getting around this or another package / technique that would be more appropriate? The second link suggests using additional dummy data but I'm a bit wary of this approach.</p>
"
"0.106721701823439","0.107334697685273","185495","<p>I am developing some stochastic simulations in which I have four explanatory variables that I named <code>Birth Rate</code>, <code>Inactivation Rate</code>, <code>Deletion Rate</code> and <code>Model</code>. They are all continuous data. I have several responses. One of these responses is a categorical value that can take 5 categories that I just labelled 1 to 5. I called this response an outcome.</p>

<p>I am having lots of trouble figuring out an analysis that I could use to understand the influence of my explanatory variables (all continuous data) on my response (categorical data). For example, I am interested in understanding how the outcome is influenced by <code>Birth Rate</code>, <code>Inactivation Rate</code>, <code>Deletion Rate</code> and <code>Model</code>. </p>

<p>I thought about a multinomial logistic regression, but I am not quite sure whether I can actually apply it to my data.</p>

<hr>

<p><em>Edit</em>: I have followed <a href=""https://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;ved=0ahUKEwj30O-9u8zJAhXIqg4KHWlwDjQQFggsMAI&amp;url=https%3A%2F%2Fstatsthewayilikeit.files.wordpress.com%2F2015%2F05%2Fmultinomial-logistic-regression.docx&amp;usg=AFQjCNHj4IdMOAdjfavqcd-Q8HG66vYoag&amp;sig2=ScEoSVJGu1dDvMyIiAyljQ&amp;cad=rja"" rel=""nofollow"">this Word document</a> to do a multinomial logistic regression. An example of one of my plots after performing the regression is reproduced below (the x-axis is in log10 for the <code>Birth Rate</code>):  </p>

<p><a href=""http://i.stack.imgur.com/jjITQ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/jjITQ.png"" alt=""enter image description here""></a></p>

<p>My code in R:</p>

<pre><code>all.m        &lt;- multinom(code_Final_Info ~ Model+Birth_Rate+Inact_Rate+Del_Rate1+Time, 
                         data=all.mod)
PredProb     &lt;- cbind(preds, predict(all.m, newdata=preds, type='probs', se=TRUE))
PredProbMelt &lt;- melt(PredProb, value.name=""Probability"", 
                     id.vars=c(""Model"",""Birth_Rate"",""Inact_Rate"",""Del_Rate1"",""Time""))

(p &lt;- ggplot(PredProbMelt, aes(x=Del_Rate1, y=Probability, colour=Time)) + 
                           geom_line() +
                           facet_grid(variable ~., scales=""free"") + 
                           theme_bw()) 
</code></pre>

<p>My data.frame looks like:  </p>

<pre><code>&gt; str(all.mod)
'data.frame':   900000 obs. of  6 variables:
 $ Model          : num  0.98 0.98 0.98 0.98 0.98 0.98 0.98 0.98 0.98 0.98 ...
 $ Birth_Rate     : num  -4.05 -4.05 -4.05 -4.05 -4.05 ...
 $ Inact_Rate     : num  -3.14 -3.14 -3.14 -3.14 -3.14 ...
 $ Del_Rate1      : num  -4.26 -4.26 -4.26 -4.26 -4.26 ...
 $ code_Final_Info: Factor w/ 5 levels ""1"",""2"",""3"",""4"",..: 3 3 3 3 3 3 3 3 3 3 ...
 $ Time           : Factor w/ 3 levels ""0T250"",""1T500"",..: 1 1 1 1 1 1 1 1 1 1 ...
</code></pre>
"
"0.126274820515127","0.127000127000191","185757","<p>I'm doing a meta-analysis which involves some multiple logistic regression.  As it's a meta-analysis, I'm compiling data from various studies, which therefore differ slightly in their methodology.  One particular aspect of methodology is likely to affect the response variable, and I therefore decided to include it as a random effect in my models. </p>

<p>However, now I'm being asked to provide more information about the likely effect of this predictor.  Is it significant?  How strong is the effect?  The only way I can think to do so is to treat it as a fixed effect and see if adding/removing it improves the model or not, using F tests to compare models (they are quasi-binomial models).  However, I had a feeling that treating variables as both random and fixed variables was wrong - it should be one of the other.  I've never quite got a grip on the difference between the two, and would appreciate any advice.
Thanks</p>

<p>edit: Here's a bit more info about my study.  In my meta-analysis I'm comparing the results of other studies to my own study.  The response in the model is the similarity in the data.  Regarding this particular aspect of methodology, I used ""method A"". If all the other studies used ""method A"" it would be fine, but some used ""method A"", some ""method B"", some ""method C"" and some ""method D"". Each different method will introduce bias, but may do so to a different degree.  Hope that makes sense.</p>
"
"0.0954547919806695","0.0960030721474639","187468","<p>I have a question about analyzing a dataset that I'm currently working with. Each row of the dataset represents an individual songbird, and its reproductive success over the course of a breeding season. Reproductive success was recorded as a score or rank that was based on breeding activities that we observed for each bird. Scores were recorded as follows:</p>

<pre><code>1 - unpaired
2 - paired
3 - successfully raised 1 brood of fledglings
4 - successfully raised 2 broods
5 - successfully raised 3 broods
</code></pre>

<p>In my analysis, these scores will be the response variables, and several environmental covariates will be used as predictor variables. Typically I see that ranked data is analyzed using ordinal logistic regression, but would it also be reasonable to model this data using a poisson glm/glmm? I have experience with poisson glm's, and this distribution is often used in my field (wildlife ecology) for count data or to model age structure. This reproductive index is not commonly used, so there are not many examples I've come across that attempted to do a similar analysis.</p>

<p>Thanks!
Jay</p>
"
"0.158293864720653","0.159203084517278","189903","<p>I am conducting logistic regression analysis: The data includes 107 observations, dependent variable is a binary one, there is about 5 covariates which are both continuous, binary and multi-categorical variables. I want to use some cut_off points to predict the outcome.</p>

<p>So basicly, I select one cut_off point (based on the requirement that the sensitivity >70% and specificity > 70%). Then I devide my data into train set (85% data points) and test set (15% data points).</p>

<p>I fit the model with 5 covariates on the train set, and use the model to predict the outcome on the test set. I use the glm() function to fit the model, and glm.predict() function to predict on test sets. </p>

<p>Since there is missing data, I create 40 imputed data sets using MICE package in R. The procedure above is repeated over 40 imputed data sets. For each data set, I obtain the mis-classification errors.</p>

<p>So, to get the overall mis-classification errors, I averaged over 40 mis-classification error rates.</p>

<p>My question is: </p>

<p>How to assess the variability of this overall mis-classification errors?</p>

<p>As I am thinking that we can not use the usual formula to calculate the variance for this number, as the mis-classification errors over different imputed data sets might be correlated to each other.</p>

<p>Does any one have a suggestion or reference to do this?
(I am using R).
Thank you for any inputs.</p>
"
"0.320792265925364","0.315913296343955","191810","<h2>Study design</h2>

<p>504 individuals were all sampled 2 times. Once before and once after a celebration.</p>

<p>The goal is to investigate if this event (Celebration) as well as working with animals (sheepdog) have an influence on the probability that an individual gets infected by a parasite. (out of 1008 observations only 22 are found to be infected)</p>

<p><strong>Variables</strong></p>

<ul>
<li>dependent variable = ""T_hydat"" (infected or not) </li>
</ul>

<p>(most predictiv variables are categorical) </p>

<ul>
<li>""Celebration"" (yes/no)</li>
<li>""sex"" (m/f)</li>
<li>""RelAge"" (5 levels)</li>
<li>""SheepDog"" (yes/no)</li>
<li>""Area"" (geographical area = 4 levels)</li>
<li>""InfectionPeriodT_hydat"" (continuous --> Nr Days after deworming"")</li>
<li>""Urbanisation (3 levels)</li>
</ul>

<h2>Question 1:</h2>

<p>1) Should I include Individual-ID (""ID"") as a random Effekt as I sampled each Ind. 2 times? (Pseudoreplication?) </p>

<pre><code>mod_fail &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|ID), family = binomial)

Warnmeldungen:
1: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,  :
  failure to converge in 10000 evaluations
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 1.10808 (tol = 0.001, component 10)
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p>--> unfortunately this model fails to converge (is it a problem that ID = 504 levels with only 2 observations per level?) 
Convergence is achieved with glmmPQL() but after droping some unsignficant preditiv variables the model fails to converge again ? What is the Problem here? Could geeglm() be a solution?</p>

<p>In an other attempt I run the model only with ""Area"" (4 levels) as random effect (my expectation is that Ind. in the same geogr. Area are suffering from the same parasite pressure etc.) and received the follwoing p-Values. </p>

<h2>My model in R:</h2>

<pre><code>mod_converges &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|Area), family = binomial)
</code></pre>

<h2>mod_converges output:</h2>

<p>summary(mod_converges)</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + sex + SheepDog + RelAge + Urbanisation +  
    InfectionPeriodT_hydat + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   203.0    262.0    -89.5    179.0      996 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.461 -0.146 -0.088 -0.060 31.174 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.314    0.561   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)            -6.81086    1.96027   -3.47  0.00051 ***
Celebration1            1.36304    0.57049    2.39  0.01688 *  
sexm                   -0.18064    0.49073   -0.37  0.71279    
SheepDog1               2.02983    0.51232    3.96  7.4e-05 ***
RelAge2                 0.34815    1.18557    0.29  0.76902    
RelAge3                 0.86344    1.05729    0.82  0.41412    
RelAge4                -0.54501    1.43815   -0.38  0.70471    
RelAge5                 0.85741    1.25895    0.68  0.49584    
UrbanisationU           0.17939    0.78669    0.23  0.81962    
UrbanisationV           0.01237    0.59374    0.02  0.98338    
InfectionPeriodT_hydat  0.00324    0.01159    0.28  0.77985    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<h2>glmmPQL-Model output below (added after the first reply by usÎµr11852)</h2>

<p>This model converges with ""Sample_ID"" as a random effect, however, as statet by 
<strong>usÎµr11852</strong> the varaince of the random effect is quiet high  4.095497^2 = 16.8. And the std.error of Area5 is far to high (complete separation).
Can I just remove Datapoints from Area5 to overcome this Problem? </p>

<pre><code>#     T_hydat
#  Area   0   1
#     1 226   4
#     2 203   3
#     4 389  15
#     5 168   0  ## here is the problematic cell

Linear mixed-effects model fit by maximum likelihood
 Data: dat 
  AIC BIC logLik
   NA  NA     NA

Random effects:
 Formula: ~1 | Sample_ID
        (Intercept)  Residual
StdDev:    4.095497 0.1588054

Variance function:
 Structure: fixed weights
 Formula: ~invwt 
Fixed effects: T_hydat ~ Celebration + sex + SheepDog + YoungOld + Urbanisation +      InfectionPeriodT_hydat + Area 
                            Value Std.Error  DF    t-value p-value
(Intercept)            -20.271630     1.888 502 -10.735869  0.0000
Celebration1             5.245428     0.285 502  18.381586  0.0000
sexm                    -0.102451     0.877 495  -0.116865  0.9070
SheepDog1                3.356856     0.879 495   3.817931  0.0002
YoungOldyoung            0.694322     1.050 495   0.661017  0.5089
UrbanisationU            0.660842     1.374 495   0.480990  0.6307
UrbanisationV            0.494653     1.050 495   0.470915  0.6379
InfectionPeriodT_hydat   0.059830     0.007 502   8.587736  0.0000
Area2                   -1.187005     1.273 495  -0.932576  0.3515
Area4                   -0.700612     0.973 495  -0.720133  0.4718
Area5                  -23.436977 28791.059 495  -0.000814  0.9994
 Correlation: 
                       (Intr) Clbrt1 sexm   ShpDg1 YngOld UrbnsU UrbnsV InfPT_ Area2  Area4 
Celebration1           -0.467                                                               
sexm                   -0.355  0.018                                                        
SheepDog1              -0.427  0.079  0.066                                                 
YoungOldyoung          -0.483  0.017  0.134  0.045                                          
UrbanisationU          -0.273  0.005 -0.058  0.317 -0.035                                   
UrbanisationV          -0.393  0.001 -0.138  0.417 -0.087  0.586                            
InfectionPeriodT_hydat -0.517  0.804  0.022  0.088  0.016  0.007  0.003                     
Area2                  -0.044 -0.035 -0.044 -0.268 -0.070 -0.315 -0.232 -0.042              
Area4                  -0.213 -0.116 -0.049 -0.186 -0.023 -0.119  0.031 -0.148  0.561       
Area5                   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000

Standardized Within-Group Residuals:
          Min            Q1           Med            Q3           Max 
-14.208465914  -0.093224405  -0.022551663  -0.004948562  14.733133744 

Number of Observations: 1008
Number of Groups: 504 
</code></pre>

<p><strong>Output from logistf (Firth's penalized-likelihood logistic regression)</strong></p>

<pre><code>logistf(formula = T_hydat ~ Celebration + sex + SheepDog + YoungOld + 
    Urbanisation + InfectionPeriodT_hydat + Area, data = dat, 
    family = binomial)

Model fitted by Penalized ML
Confidence intervals and p-values by Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood 

                               coef   se(coef)  lower 0.95  upper 0.95       Chisq
(Intercept)            -5.252164846 1.52982941 -8.75175093 -2.24379091 12.84909207
Celebration1            1.136833737 0.49697927  0.14999782  2.27716500  5.17197661
sexm                   -0.200450540 0.44458464 -1.09803320  0.77892986  0.17662930
SheepDog1               2.059166246 0.47197694  1.10933774  3.12225212 18.92002321
YoungOldyoung           0.412641416 0.56705186 -0.66182554  1.77541644  0.50507269
UrbanisationU           0.565030324 0.70697218 -0.98974390  1.97489240  0.56236485
UrbanisationV           0.265401035 0.50810444 -0.75429596  1.33772658  0.25619218
InfectionPeriodT_hydat -0.003590666 0.01071497 -0.02530179  0.02075254  0.09198425
Area2                  -0.634761551 0.74958750 -2.27274031  0.90086554  0.66405078
Area4                   0.359032194 0.57158464 -0.76903324  1.63297249  0.37094569
Area5                  -2.456953373 1.44578029 -7.36654837 -0.13140806  4.37267766
                                  p
(Intercept)            3.376430e-04
Celebration1           2.295408e-02
sexm                   6.742861e-01
SheepDog1              1.363144e-05
YoungOldyoung          4.772797e-01
UrbanisationU          4.533090e-01
UrbanisationV          6.127483e-01
InfectionPeriodT_hydat 7.616696e-01
Area2                  4.151335e-01
Area4                  5.424892e-01
Area5                  3.651956e-02

Likelihood ratio test=36.56853 on 10 df, p=6.718946e-05, n=1008
Wald test = 32.34071 on 10 df, p = 0.0003512978
</code></pre>

<p>** glmer Model (Edited 28th Jan 2016)**</p>

<p>Output from glmer2var: Mixed effect model with the 2 most ""important"" variables (""Celebration"" = the factor I am interested in and ""SheepDog"" which was found to have a significant influence on infection when data before and after the celebration were analysed separately.) The few number of positives make it impossible to fit a model with more than two explanatory variables (see commet EdM). </p>

<p>There seems to be a strong effect of ""Celebration"" that probably cancels out the effect of ""SheepDog"" found in previous analysis.</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Sample_ID)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   113.0    132.6    -52.5    105.0     1004 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.5709 -0.0022 -0.0001  0.0000 10.3491 

Random effects:
 Groups    Name        Variance Std.Dev.
 Sample_ID (Intercept) 377.1    19.42   
Number of obs: 1008, groups:  Sample_ID, 504

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -19.896      4.525  -4.397  1.1e-05 ***
Celebration1    7.626      2.932   2.601  0.00929 ** 
SheepDog1       1.885      2.099   0.898  0.36919    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.908       
SheepDog1   -0.297 -0.023
</code></pre>

<h2>Question 2:</h2>

<p>2) Can I use drop1() to get the final model and use the p-Values from summary(mod_converges) for interpretation? Does my output tell me if it makes sense to include the random effect (""Area"") ?</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   190.8    210.4    -91.4    182.8     1004 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.369 -0.135 -0.096 -0.071 17.438 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.359    0.599   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)    -5.912      0.698   -8.47  &lt; 2e-16 ***
Celebration1    1.287      0.512    2.51    0.012 *  
SheepDog1       2.014      0.484    4.16  3.2e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.580       
SheepDog1   -0.504  0.027
</code></pre>

<p>I know there are quite a few questions but I would really appreciate some advice from experienced people. Thanks!</p>
"
"0.0954547919806695","0.0960030721474639","196586","<p>I would like to do a gene x environment interaction analysis in a matched (1-1) case control samples. I referred all related previous publications and in most of the papers authors used either STATA or SAS. I got few references for performing conditional logisitic regression in R, for example using survival (clogit) package. But I couldn't find any reference for adding interaction terms in conditional logistic models in R. Can someone help me with references for interaction analysis using conditional logistic regression in R?</p>
"
"0.0901963003679479","0.108857251714449","198925","<p>Although there has been some detailed discussions about power analysis on this website (for example <a href=""http://stats.stackexchange.com/questions/22406/power-analysis-for-ordinal-logistic-regression/22410#22410"">here</a> and <a href=""http://stackoverflow.com/questions/27234696/how-do-you-conduct-a-power-analysis-for-logistic-regression-in-r"">here</a>), the answer provided to this question has  outlines the steps to simulating a power analysis, <a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments"">here</a>.</p>

<p>Say we take some data (data was linked to a <a href=""http://stats.stackexchange.com/questions/59829/which-bootstrapped-regression-model-should-i-choose"">bootstrapping question</a>)</p>

<p>We create a regression that will predict <code>admit</code> based on the two continous variables <code>gpa</code> and <code>gre</code></p>

<ul>
<li>Now we have a <code>n=400</code>. </li>
<li>We can then elect our power level, <code>alpha = 0.5</code></li>
<li>The effect size you would like to detect, e.g., odds ratios  (we obtain this from our regression)</li>
</ul>

<p>So in following the detailed method provided by @gung <a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments"">here</a>, I want to run the simulation. Here is the code I have adjusted, but my output is not correct. Can someone outline what I have not understood</p>

<pre><code>mydata &lt;- read.csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
head(mydata)

set.seed(1234)

my.mod &lt;- glm(admit ~ gre + gpa , data = mydata, family = ""binomial"")


repetitions &lt;- length(mydata$admit)

gre &lt;- mydata$gre
    gpa &lt;- mydata$gpa


significant = matrix(nrow=repetitions, ncol=4)

for(i in 1:repetitions){
  responses          = mydata$admit
      #responses          = rbinom(n=N, size=1, prob=mydata$admit)      # we can interchange this comment
  model              = glm(responses ~ gre + gpa, family = binomial(link=""logit""))
  significant[i,1:2] = (summary(model)$coefficients[2:3,4]&lt;.05)
      significant[i,3]   = sum(significant[i,1:2])
      modelDev           = model$null.deviance-model$deviance
  significant[i,4]   = (1-pchisq(modelDev, 2))&lt;.05
}



sum(significant[,1])/repetitions      # pre-specified effect power for gre

sum(significant[,2])/repetitions      # pre-specified effect power for gpa

sum(significant[,4])/repetitions  # power for likelihood ratio test of model

sum(significant[,3]==2)/repetitions   # all effects power

sum(significant[,3]&gt;0)/repetitions    # any effect power
</code></pre>
"
"0.135834550306421","0.151794185179729","204145","<p><strong>Background and Problem</strong></p>

<p>I have a question concerning a meta-analysis combining effects from between- and within-subject designs using log-odds ratios (OR) as the metric of interest. I am familiar with conducting meta-analyses and will be undertaking my calculations in R (using the <code>metafor</code> and <code>lme4</code> packages). To provide greater context, the studies in question ask research subjects to make a binary decision with respect to a personal preference across one of two conditions. In some cases, each participant is assigned to a single condition (making only a single binary response); in others, each subject takes part in both conditions (making two binary responses). For now, presume I have the raw data in all cases. The issue I face is how best to calculate an OR that is comparable across design and whether I should take the correlation between conditions into account for the within-subject designs.</p>

<p><strong>My Current Approach</strong></p>

<p>I presently use logistic regression to estimate the OR for between-subject designs. The slope represents the OR and the sampling variance can be calculated by squaring the SE of the slope coefficient. Using this approach produces estimates comparable to equations reported in common texts such The Handbook of Research Synthesis and Meta-Analysis, 2nd Edition (p. 243). I then extend this approach to use a multilevel logistic regression model including a random intercept by subject to estimate the OR for within-subject designs while account for the dependency between conditions. The OR and sampling variance are otherwise calculated in the same fashion. </p>

<p><strong>My Questions:</strong></p>

<p>With this in mind, I would like to ask:</p>

<ol>
<li>Is it reasonable to meta-analytically aggregate OR calculated using standard and multilevel logistic regression?</li>
<li>Would it be better to use standard logistic regression for both designs (ignoring the correlation between conditions for the within-subject designs)?</li>
</ol>
"
"0.257162801153867","0.258639911528153","206042","<p>I implement <code>n</code> permutations into a regression analysis, to test the model for stability. Thus I obtain <code>n</code> odds ratios (ORs) and <code>n</code> associated 95% CI intervals. </p>

<p>Each permutation represents a matched-pair study. We pair similar <code>case</code>'s with <code>control</code>'s and then run a conditional logistic regression to obtain a measure of association between the outcome of interest and exposure variable (treatment status).</p>

<p>Taking the following example I have implemented into a <code>R</code> script.
In short what I have done is:</p>

<ol>
<li>Take a portion of the a given population</li>
<li>We assign at dummy variable to the population (1/0) to indicate treatment status</li>
<li>based on a set of parameters we pair those with treatment status==1 to equivalent treatment status==0</li>
<li>we define an outcome of interest that we wish to measure if treatment had an effect on the outcome</li>
<li>We conduct a logistic regression to determine the ORs associated with treatment status</li>
<li>we repeat this n time, each time obtaining an ORs and associated 95% confidence interval</li>
</ol>

<p><strong>But what I am not sure, is how I can report on the spread of my data. I generate a different odds ratio and 95% CI for each permutation.</strong></p>

<p>Taking the following hypothetical example, we run a simulation 100 times. It only takes a minute to simulate.</p>

<p>We take an worked exampled from the <a href=""https://cran.r-project.org/web/packages/Matching/Matching.pdf"" rel=""nofollow"">Matching package</a> in R. </p>

<pre><code>set.seed(123)    
# preamble, prepare the data for the simulation
    #1.
    library(Matching)
    library(survival)
    #2.
    require(doParallel)
    cl&lt;-makeCluster(2)
    registerDoParallel(cl)
    #3.
    clusterEvalQ(cl,library(Matching))
    clusterEvalQ(cl,library(survival))

    m &lt;- 100


    Result = foreach(i=1:m,.combine=cbind) %do%{

      # attach the data
      data(lalonde)

      # we want to assess if treatment is associated with greater odds for the outcome of interest
      # lets create our hypothetical outcome of interest
      lalonde$success &lt;- with(lalonde, ifelse(re78 &gt; 8125, 1, 0))

      # lets take a portion of the original population, say only 395
      n &lt;- sample(1:445,420, replace = F)
      n &lt;- sort(n, decreasing = F)
      lalonde &lt;- lalonde[n,]
      head(lalonde$age)

      # taking from the example from GenMatch (in Matching package)
      #The covariates we want to match on
      # but we only include some of the original variables (we come back to the others later)
      X = cbind(lalonde$age, lalonde$educ, lalonde$black, lalonde$hisp, 
                lalonde$married, lalonde$nodegr)

      #The covariates we want to obtain balance on
      BalanceMat &lt;- X

      # creat our matrix
      genout &lt;- GenMatch(Tr=lalonde$treat, X=X, BalanceMatrix=BalanceMat, 
                         pop.size=16, max.generations=10, wait.generations=1)


      # match our collisions on a 1-1 basis
      mout &lt;- Match(Y=NULL, Tr=lalonde$treat, X=X, Weight.matrix=genout, ties = F, replace = F)
      summary(mout)

      # here we create our case and control populations
      treat &lt;- lalonde[mout$index.treat,]
          control &lt;- lalonde[mout$index.control,]

      # and we want to apply a unique identifier for each pair
      # we call this command during the regression
      treat$Pair_ID &lt;- c(1:length(treat$age))
      control$Pair_ID &lt;- treat$Pair_ID 

      # finally we combine the data
      matched &lt;- rbind(treat, control)

      # now we run a conditional logitic regression on the paired data to determine the Odds Ratio associated with treatment
      # we account for the difference in pairs by the strata() command
      # we account for some of the original matching parameters that we removed from the matching process
      model_1 &lt;- clogit(success ~ treat + strata(Pair_ID) + re74, matched, method = 'efron')
      summary(model_1)

      OR_M1 &lt;- exp(model_1$coefficients[1])
      CI_U1 &lt;- exp(confint(model_1))[1,2]
      CI_L1 &lt;- exp(confint(model_1))[1,1]

      Result &lt;- rbind(OR_M1, CI_U1, CI_L1)

    }
</code></pre>

<p>To summarise the script:</p>

<ul>
<li>we take 420 people from the original population (of 445)</li>
<li>we define the outcome of interest is. That is if the person had <code>re78 &gt; 8125</code> yes or no</li>
<li>for each treat==1, we find an equivalent treat==0 based on age, educ,  black, hisp, married, nodegr. We only want exact 1-1 matching</li>
<li>we assign an unique indicator variable for each pair 1,2,3.....x</li>
<li>We then develop a regression model to determine the OR for our outcome of interest (<code>re78 &gt; 8125</code>) associated with the treatment status (=1 relative to =0). </li>
<li>we save the ORs and 95%CI</li>
</ul>

<p>We can then plot the ORs and shade the 95%CI</p>

<pre><code>plot(Result[1,], ylim=c(0,2.5))
polygon(c(1:m,m:1), c(Result[3,],Result[2,]),col=adjustcolor(""grey"", alpha=0.4), border = NA)
</code></pre>

<p><strong>But how can I summarise the several ORs I obtained, the spread of it and/or an associated confidence level?</strong></p>

<p><strong>EDIT</strong>
Am I able to assess my study as if it was a meta-analysis. If so, one could implement the solution proposed by @Bernd Weiss <a href=""http://stats.stackexchange.com/questions/9483/how-to-calculate-confidence-intervals-for-pooled-odd-ratios-in-meta-analysis?rq=1"">here</a>?</p>

<p>For this we need to obtain the natural log of the ORs and the std. err.?</p>

<p>We update the last part of the command to:</p>

<pre><code>.......    
model_1 &lt;- clogit(success ~ treat + strata(Pair_ID) + re74, matched, method = 'efron')
      summary(model_1)

  OR_M1 &lt;- exp(model_1$coefficients[1])

  l_OR_T2 &lt;- model_1$coefficients[1]
  s_e &lt;- coef(summary(model_1))[1,3]

  CI_U1 &lt;- exp(confint(model_1))[1,2]
  CI_L1 &lt;- exp(confint(model_1))[1,1]

  Result &lt;- rbind(OR_M1, l_OR_T2, s_e, CI_U1, CI_L1)
</code></pre>

<p>Using we can then call upon the <code>metagen()</code>, command</p>

<pre><code>library(meta)
or.fem &lt;- metagen(as.numeric(Result[2,]), as.numeric(Result[3,]), sm = ""OR"")
</code></pre>

<p>Where <code>as.numeric(Result[2,])</code> is the log(OR) and <code>as.numeric(Result[3,])</code> is the std. err. Thus we obtain a 95% CI ...... But have we introduced a bias in the CI by the imputations. We see our 95% range is significant (greater than 1), however for each permutation, we only get a lower 95% CI > 1 </p>

<pre><code>sum(as.numeric(Result[5,])&gt;1.00)
</code></pre>

<p>times. Therefore I think the large <code>n</code> and thus <code>degrees of freedom</code> in the meta-analysis are giving us a significant result </p>
"
"0.09742313910673","0.117579270250443","210646","<p>I am replicating an analysis that models tree mortality data. Data are structured such that forest sites are revisted at some random interval, which is recorded. It is then determined if a tree lived or died over that random interval, generating 0 1 mortality data (if a tree dies, it gets a 1 in the dependent variable). The interval between initial and final observation varies continuously, from 5-15 years. This is relevant, as the more time that passes, the more likely a tree will die. </p>

<p>Here are some pseudo data for R:</p>

<pre><code>mort &lt;- c(0,1,0,0,0,0,0,1,1,1,1,0,1,0,0,0,0,0,0,0,1,0)
interval &lt;- runif(length(mort), 5, 15)
pollution &lt;- rnorm(length(mort), 25,5)
data&lt;- data.frame(mort, interval, pollution)
</code></pre>

<p>I am trying to replicate an analysis which uses a logistic regression model for binary mortality data using the the logit transformation. Authors then model how pollution affects tree mortality rates. In the manuscript the authors write, ""because recensus is not annual, we relate annual mortality probability, <code>pi</code>, of tree <code>i</code> to the observed binomial data on whether that tree lived or died <code>Mi</code> via a Bernoulli likelihood,</p>

<p><a href=""http://i.stack.imgur.com/7i7jA.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7i7jA.png"" alt=""enter image description here""></a></p>

<p>where <code>ti</code> is the time interval between successive censuses.""</p>

<p>My question: How would I implement this using the <code>glm</code> function, or something analagous, in R? Note: I understand modeling this as a hazard function would also be appropriate, but it is not what I am interested in.</p>
"
"0.143182187971004","0.128004096196618","210900","<p>I have one outcome/dependent variable that can be ordinal or nominal and 3 independent variables. I have learned so far how to perform ordinal and multinomial logistic regression in SPSS between a single independent variable and the outcome variable.</p>

<p>However, I am more interested in examining the effect of the combinations of independent variables on the outcome variable. I could not find an answer online and I am new to regressions.</p>

<p>For example, the first independent variable X1 levels are friends and public, and he second independent variable X2 levels are location and time. So, I want to examine the effect on the outcome var. (what is the coefficient or estimate) in the following four cases that as a mix of X1 and X2 level:</p>

<ul>
<li>X1= friends and X2= location</li>
<li>X1= friends and X2= time</li>
<li>X1= public and X2= location </li>
<li>X1= public and X2= time</li>
</ul>

<p>I would really appreciate it if you have any thought of to perform such nested analysis whether ordinal or multinomial regression. The only way I thought of is to have the different combinations of X1 and X2 merged in one column and have another column for the outcome variable and run the analysis in SPSS but I am not sure if this the right way.
If there is no way to do that is SPSS, I would be happy to know to do it in R.</p>

<p>Many thanks.</p>
"
"0.172083573487526","0.173071999614875","213531","<p>I am trying to analyse my data using bam. And I would greatly appreciate your advice as to the appropriate analyses.</p>

<p>The experimental design is: 
There are two groups of participants, ""CAT"" and ""PA"" coded in the factor ""group.""
Within each group, there are two conditions, ""Label"" and ""Ideo"" coded in the factor ""cnd.""
The prediction is that performance of participants in the Label condition would be higher compared to the Ideo condition, and that this difference would be greater in the CAT group compared to the PA group. So, my hypothesis forces me to test for an interaction of group by condition. The dependent variable is accuracy (so I need a logistic model) which greatly depends on time (coded as ""ctrial""). ""Sbj"" codes participants that will be treated as random effects. </p>

<p>My understanding of gamms is that If I had two isotropic continuous variables I could use s(x1,x2) and If I had two non-isotropic variables I could use te(x1,x2) to model interaction. But this is not the case, because my variables of interest are factors.</p>

<p>I believe that the standard approach is that I should create a variable with four levels (group by condition)</p>

<pre><code>data$igc &lt;- as.factor(paste(as.character(data$group),as.character(data$cnd),sep=""."")) 
</code></pre>

<p>and use an additive model such as:</p>

<pre><code>bam(acc~ 1 + igc + s(ctrial, by=igc) + s(sbj, bs = ""re""), data=data, family=binomial)
</code></pre>

<p>I could then inspect the plots and find out if CAT. label - CAT.ideo is greater compared to PA.label- PA.ideo.</p>

<p>But do I really need the 4-level variable?
I mean, is my 4-level model the most parsimonious one?</p>

<p>If there is no differences between the two groups, wouldn't a model such as 
    bam(acc~ 1 + cnd + s(ctrial, by=cnd) + s(sbj, bs =""re""), data=data, family=binomial)
be more appropriate?</p>

<p>[I believe that this question is not the same as the one posted here: <a href=""http://stats.stackexchange.com/questions/32730/how-to-include-an-interaction-term-in-gam"">How to include an interaction term in GAM?</a>, as my question is related to model comparison] </p>

<p>My best guess is that I should do the following: </p>

<pre><code>Analysis of Deviance Table

Model 1: acc ~ 1 + igc + s(ctrial) + s(sbj, bs = ""re"")
Model 2: acc ~ 1 + group + s(ctrial, by = group) + s(sbj, bs = ""re"")
Model 3: acc ~ 1 + cnd + s(ctrial, by = cnd) + s(sbj, bs = ""re"")
Model 4: acc ~ 1 + igc + s(ctrial, by = igc) + s(sbj, bs = ""re"")    
  Resid. Df Resid. Dev     Df Deviance  Pr(&gt;Chi)    
1     18350     8495.0                              
2     18347     8497.3 2.1307   -2.262              
3     18346     8474.8 1.3922   22.419 4.861e-06 ***
4     18338     8456.5 7.7730   18.327   0.01667 *  
</code></pre>

<p>Is my understanding correct?
And is this the correct way to ""justify"" the use of a 4-level variable?</p>

<p>Thank you in advance for your time,</p>

<p>Fotis</p>
"
"0.116907766928076","0.117579270250443","214882","<p>I am currently performing a retrospective study that is comparing a surgical procedure vs a modified version of the same procedure. There is obvious selection bias because of the selection criteria necessary to perform the modified procedure. I was wondering how I would control for these 3 variables (all are simple T/F requirements)? Should I just perform a logistic regression for each dependent variable we are investigating and hope none of them reach significance? Or is there a statistical test that automatically adjusts for these 3 selected covariates?</p>

<p>Initially I did not perform any statistical tests between these groups for this reason, but if we were to prove that these variables are not confounding then I could simply perform the appropriate two sample test, correct?</p>

<p>My Data:</p>

<ul>
<li>Independent Variable (2 groups) = Procedure 1, Procedure 2</li>
<li>We also have multiple dependent variables we want to compare between the two groups: Numerical and Logical (e.g. Length of Stay,
or Re-operation within 30 days, etc.)</li>
<li>But I have 3 Logical variables that I am afraid are confounding.</li>
</ul>

<p>PS. I'm using R to carry out this analysis and any reference to R functions would be a plus. </p>
"
"0.143182187971004","0.128004096196618","215447","<p>I have one outcome/dependent variable that can be ordinal or nominal and 3 independent variables. I have learned so far how to perform ordinal and multinomial logistic regression in SPSS between a single independent variable and the outcome variable.</p>

<p>However, I am more interested in examining the effect of the combinations of independent variables on the outcome variable. I could not find an answer online and I am new to regressions.</p>

<p>For example, the first independent variable X1 levels are friends and public, and the second independent variable X2 levels are location and time. So, I want to examine the effect on the outcome var. (what is the coefficient or estimate) in the following four cases that as a mix of X1 and X2 level:</p>

<pre><code>X1= friends and X2= location
X1= friends and X2= time
X1= public and X2= location
X1= public and X2= time
</code></pre>

<p>I would really appreciate it if you have any thought of to perform such nested analysis whether ordinal or multinomial regression. </p>

<p>The only way I thought of is to have the different combinations of X1 and X2 merged in one column and have another column for the outcome variable and run the analysis in SPSS but I am not sure if this the right way. If there is no way to do that is SPSS, I would be happy to know to do it in R.</p>

<p>Many thanks in advance.</p>
"
"0.0674967307062827","0.0678844233302131","215560","<p>I have three data sets that, when joined, have O(320) independent variables for a classification problem.  </p>

<p>Principal component analysis (PCA) seems out of the question because the data is mostly factors, not continuous.</p>

<p>I'm at a loss as to how to proceed.  </p>

<p>How do experienced analysts go about winnowing a large data set with hundreds of columns to something manageable?  How do you decide between variables?  What calculations can you go on to supplement your gut and experience?  How do you avoid throwing away significant variables?</p>

<p>A large number of columns might not be a problem for R, given enough CPU and RAM, but coming up with a cogent story should include identifying what is truly significant.  How to accomplish that?</p>

<p>Should I just toss all of it into a logistic regression and see what happens, without any forethought?</p>

<p>More detail in response to comments:</p>

<ol>
<li>Classification. </li>
<li>Many more observations than columns. </li>
<li>Yes, big oh notation meaning approximately. </li>
<li>Linear model at first. Also interested in boosted models in addition to logistic regression. </li>
</ol>
"
"0.106721701823439","0.107334697685273","218477","<p>I have a database which contains 100k records. It includes 2 continuous and 6 categorical variables. The output is categorical as well and it can take one of 8 different values (e.g. 1, 2, 3...8). My goal is:</p>

<ol>
<li><p>Investigate which of the variables are the most significant ones to determine my output.</p></li>
<li><p>After the analysis, to be able to calculate the probability for any of those outputs to happen if I only know what are the values for (e.g.) two variables? For example, to have some coefficients for every possible categorical value in order to calculate the probability...</p></li>
</ol>

<p>I tried this with the logistic regression but somehow I have big deviation from manually calculated probability (e.g. when I use the number of positive outputs and the total number of the records contained within my database). Anybody has a better idea how I could analyse this? Sth better than logistic regression?
Thanks in advance!</p>
"
"0.158293864720653","0.159203084517278","218738","<p>I want to build a linear regression model where I predict a mean of a group of participants (how they rate something on average). Predictors should be </p>

<ol>
<li>age (continuous)</li>
<li>origin (deviation coded, each level compared to grand mean, levels=1,2,3,4)</li>
<li>education (Helmert coded, each level compared to subsequent ones, haven't decided on number of levels yet)</li>
<li>gender/sex (dummy coded, 0/1)</li>
</ol>

<p>Following questions:</p>

<p><strong>1.</strong> In R, I use the following code for the coding, for example for 4) sex:</p>

<pre><code>    data$sex &lt;- factor(data$sex, labels=c(""1"",""2"")) 
    contr.treatment(2) 
    contrasts(data$sex) = contr.treatment(2)  
</code></pre>

<p>That gives me the right (dummy) coding, for the other 2 and 3 a little differently. Can I use run this kind of code for each predictor (except age) and then throw all predictors into a model like this:</p>

<pre><code>    model &lt;- lm(Mean ~ age +  sex + educ..., data)
</code></pre>

<p>It seems wrong because: what is the common intercept going to be with these different coding systems? It's different for each coding system.
But then, how am I going to enter these different predictors into a model?</p>

<p><strong>2.</strong> Can I leave age in there as it is, unchanged, continuous?</p>

<p><strong>3. Quite a different question:</strong> This is my <em>participant analysis</em>. For the <em>item analysis</em>, I used a logistic regression based on medians instead of means. That's because I did four rating surveys with Likert scales. 4 surveys - 4 participant groups - each group rated the items on <strong>one</strong> property only, such that each item was rated on 4 properties by different people.</p>

<p>Given this, is it okay if I use linear models and means in this analysis now? And can I even build my model as I suggested above?</p>

<p><strong>Many thanks</strong> for any input! I've been trying some things, but confusion isn't fading yet...</p>
"
"0.150927278118246","0.136614766661756","219304","<p>I am examining social interaction data in individuals within two groups. Each social encounter has been coded to one of 4 categories, and these encounters are nested within individual, whom are nested within groups. The number of social encounters per individual is variable and my groups are unequal sample sizes. </p>

<p>I want to examine whether the proportion of social encounters in different categories significantly differ as a function of group. I previously examined a different DV in this data that was continuous, not categorical, and used a multilevel model in R (nlme package) to do so (data nested within individuals within groups). I have done some looking online and as far as I can tell, R should be able to run a multilevel model with categorical dependent variable as well. (i.e., <a href=""http://www.upa.pdx.edu/IOA/newsom/mlrclass/ho_binary.pdf"" rel=""nofollow"">http://www.upa.pdx.edu/IOA/newsom/mlrclass/ho_binary.pdf</a>). However, I am not sure how to implement this and I think that the naming online is inconsistent (some sources referring to this analysis as MLM with categorical variable, others calling it a multinomial logistic regression). </p>

<p>Is it possible to modify my current R script for continuous DV so that it analyzes for a categorical DV instead? Or do I need a different script? Thank you in advance for any help.</p>
"
"0.0826662747682189","0.0554273995473693","219390","<p><a href=""http://i.stack.imgur.com/fUmBg.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fUmBg.png"" alt=""enter image description here""></a></p>

<p>This is a graph of revenues for different products with the Y-axis showing normalized revenues (mean of 3 and SD of 1) and X-axis is weeks. I need do a regression analysis of sorts on this data and am unsure how to find a curve/function in R that fits this data. </p>

<p>The data points can be interpreted as being: Week 0 of product release yield normalized revenues between 2.25 to 3.25, etc.</p>

<p>Any help regarding what kind of statistical analysis I can use to create a regression model (linear and logistic wouldn't work clearly) with the end goal being to do predictive analysis (ie. if a new product is released, what normalized revenues would it yield in the first 6 weeks)</p>

<p>Thanks</p>
"
"0.158293864720653","0.144730076833889","221046","<p>I have a small data set of cases from an outbreak. For each case I have collected data for a few variables (age, HIV status, hospitalisation, country of infection etc. ). There are only 28 cases and so I imagine that it won't be possible to do any meaningful inferential statistical analysis. I would however like to present the data in as scientific manner as possible. </p>

<p>I would like to, for example, tabulate the data showing the difference between the cases that are HIV positive and HIV negative. I've included some R script and the results of what I've managed to do so far: </p>

<pre><code>data &lt;- read.csv(""Shigella.csv"")
library(dplyr)
library(epiR)
data$Hospitalised &lt;- factor(data$Hospitalised, levels = c(""Yes"", ""No""), labels = c(""Hospitalised"", ""Not Hospitalised""))
data$HIV &lt;- factor(data$HIV, levels = c(""Positive"", ""Negative""))
ttab &lt;- table(data$Hospitalised, data$HIV)
ttab


                   Positive Negative
  Hospitalised            2        1
  Not Hospitalised       13       11


Odds_Ratio &lt;- epi.2by2(ttab, method = ""case.control"", conf.level = 0.95)
Odds_Ratio

&gt; Odds_Ratio
             Outcome +    Outcome -      Total        Prevalence *        Odds
Exposed +            2            1          3                66.7        2.00
Exposed -           13           11         24                54.2        1.18
Total               15           12         27                55.6        1.25
Point estimates and 95 % CIs:
-------------------------------------------------------------------
Odds ratio (W)                               1.69 (0.13, 21.27)
Attrib prevalence *                          12.50 (-44.45, 69.45)
Attrib prevalence in population *            1.39 (-25.97, 28.75)
Attrib fraction (est) in exposed  (%)        39.79 (-1206.69, 99.08)
Attrib fraction (est) in population (%)      5.45 (-22.83, 27.23)
-------------------------------------------------------------------
 X2 test statistic: 0.169 p-value: 0.681
 Wald confidence limits
 * Outcomes per 100 population units
</code></pre>

<p>From the above, my interpreting is that the odds of a hospitalised case being HIV postive are 1.69 that of a non-hospitalised case being HIV positive. The 95% CI is (0.13, 21.27). </p>

<p>I am not sure that the p value is however. I see a p value of 0.681 but that seems to be for the X2 value. </p>

<p>Here is what I'm stuck with: 
1) am I using the right statistical test (or should I be doing logistic regression or something else)? 
2) how do I get a p value for the odds ratio (or is the p value provided above actually for the odds ratio)? </p>

<p>With appreciation! </p>

<p>Greg</p>
"
"0.179991281883421","0.203653269990639","221510","<p>I'm new to logistic regression analysis, and was unable to find an answer elsewhere in Cross Validated or Stack Overflow. </p>

<p>Consider a standard logistic regression analysis of a binary outcome (admission to college) based on continuous covariates gre score and high school gpa, and ordinal categorical rank prestige of the undergraduate institution (data from the nice UCLA stats dept. logistic regression in R tutorial: <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/dae/logit.htm</a>)</p>

<pre><code>&gt; admissions.data &lt;- read.csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
&gt; admissions.data$rank &lt;- as.factor(admissions.data$rank)
&gt; summary(admissions.data)
     admit             gre             gpa        rank
 Min.   :0.0000   Min.   :220.0   Min.   :2.260   1: 61
 1st Qu.:0.0000   1st Qu.:520.0   1st Qu.:3.130   2:151
 Median :0.0000   Median :580.0   Median :3.395   3:121
 Mean   :0.3175   Mean   :587.7   Mean   :3.390   4: 67
 3rd Qu.:1.0000   3rd Qu.:660.0   3rd Qu.:3.670
 Max.   :1.0000   Max.   :800.0   Max.   :4.000

&gt; fit1 &lt;- glm(admit ~ gre + gpa + rank, data = admissions.data, family=""binomial"")
&gt; summary(fit1)

Call:
glm(formula = admit ~ gre + gpa + rank, family = ""binomial"",
    data = admissions.data)

Deviance Residuals:
    Min       1Q   Median       3Q      Max
-1.6268  -0.8662  -0.6388   1.1490   2.0790

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -3.989979   1.139951  -3.500 0.000465 ***
gre          0.002264   0.001094   2.070 0.038465 *
gpa          0.804038   0.331819   2.423 0.015388 *
rank2       -0.675443   0.316490  -2.134 0.032829 *
rank3       -1.340204   0.345306  -3.881 0.000104 ***
rank4       -1.551464   0.417832  -3.713 0.000205 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 499.98  on 399  degrees of freedom
Residual deviance: 458.52  on 394  degrees of freedom
AIC: 470.52

Number of Fisher Scoring iterations: 4

# Odds Ratios
&gt; exp(coef(fit1))
(Intercept)         gre         gpa       rank2       rank3       rank4
  0.0185001   1.0022670   2.2345448   0.5089310   0.2617923   0.2119375

# 95% confidence intervals
&gt; exp(confint(fit1))
Waiting for profiling to be done...
                  2.5 %    97.5 %
(Intercept) 0.001889165 0.1665354
gre         1.000137602 1.0044457
gpa         1.173858216 4.3238349
rank2       0.272289674 0.9448343
rank3       0.131641717 0.5115181
rank4       0.090715546 0.4706961
</code></pre>

<p>My questions are:</p>

<p>1) In R, is there a straight-forward way to determine ORs with 95% CIs for specific values of the covariates? E.g., based on this model, what are the odds of college acceptance for students applying to a rank 2 schools with a gpa of 3 and a gre score of 750, compared with a student applying to a rank 3 school with the same gpa and gre score? I could calculate ORs by hand given the model coefficient estimates and these specific covariate values, but am unsure how to correctly propagate SEs to calculate 95% CIs.</p>

<p>2) Would this particular example be considered a case-control study design, and therefore odds ratios could be estimated, but not predictions? (See: <a href=""http://stats.stackexchange.com/questions/69561/case-control-study-and-logistic-regression"">Case-control study and Logistic regression</a>)</p>
"
"0.106721701823439","0.107334697685273","222426","<p>I would like to analyze a Randomized Response variable as the final response variable in a Structural Equation Model (SEM), with <code>R</code>. However, I found no example about this. To the best of my knowledge <code>R</code> packages enable users to fit multivariate logistic regressions only. 
However, I have seen that Randomized Response can be analyzed with SEM in Mplus (Hox, J., &amp; Lensvelt-Mulders, G. (2004). <a href=""http://www.tandfonline.com/doi/abs/10.1207/s15328007sem1104_6?journalCode=hsem20"" rel=""nofollow"">Randomized response analysis in Mplus</a>. Structural equation modeling, 11(4), 615-620.). </p>

<p>Given that <code>R</code> has good packages for latent variable analysis, like <code>lavaan</code>, could you tell me how could I model a randomized response with SEM in <code>R</code>?</p>
"
"0.126274820515127","0.127000127000191","224947","<p>What are some of the best practices and steps to building models for prediction and or inferences? </p>

<p>What have been taught to me during my classes was the steps outlined in Chapter 4 of Hosmer et al. for the ""Purposeful Selection of Covariates"". The method to screen a large dataset with many potential predictors is to use a algorithmic approach such as Stepwise, best subset regression, etc. Then verify the model after the fact for potential collinearity, confounders, etc.  However, I have read much criticism on this site in regards to those said steps and methods.</p>

<p>For example - if I was provided a dataset with ~100 potential predictors, what would be the best practice to selecting those said predictors for inclusion or exclusion of the model for prediction/inference ? </p>

<p>According to Hosmer et al., the steps would be to perform univariate analysis to screen for all of those potential predictors (p &lt; .25), then move to inclusion of those said predictors to a multivariate model. Take a stepwise approach to removing insignificant predictors, then add back and verify the significance of each non significant predictor. </p>

<p>However - the more I've read on this site the more confused I've gotten about what is considered best practices, and I've come to question more and more of what was taught during my classes.</p>

<p>Once again just to reiterate - </p>

<ol>
<li><p>What would be the best practices for building a model for obtaining unbiased measure of association for each individual predictors?</p></li>
<li><p>What would be the best practices for building a model strictly for prediction?</p></li>
</ol>

<p>I'm still learning much about the world of data science and appreciate any help that is provided!</p>

<p>""Hosmer, D. W., Lemeshow, S., &amp; Sturdivant, R. X. 2013. Applied Logistic Regression. Wiley, Chicester"".</p>
"
"0.126274820515127","0.127000127000191","228493","<p>The literature on Survival Analysis is mainly from the Medical science where tipically the researcher want to evaluate the effect of a treatment to that of another one. So far, all the example I read and studied thus contain one or more categorical variable (with at least 2 levels) and possibly some continuous variable as a covariate. Anyway the main interest is on a categorical variable (e.g. treatment).</p>

<p>Is it possible and correct to run a non parametric Cox model (or alternatively a parametric one) using only one or more continuous variables? In particular without categorizing the continuous var into 2 or more groups? </p>

<p>Something like a logistic regression. </p>

<p>To give you a more practical example, I'm trying to model the survival of say bush in a field depending on the number of cows in the same field. </p>

<p>I'm pretty sure it can be done but the lack of examples leave me in the doubt.</p>

<p>If possible how can one use the predict function for example to predict the survival when the predictor has a specified value? like survival of my plant when 10 cows are in the field...</p>

<p>any help is welcome!</p>
"
"0.126274820515127","0.127000127000191","229336","<p>I have a data set looking into whether a farm experienced a livestock disease or not in the year 2011 and 2012  and if several factors could be predictors for the livestock disease.</p>

<p>The independent variables were also collected for both years though some variables did not change e.g Thistles remained the same for both years.</p>

<p>I am looking for an appropriate method that will allow statistical comparison between the two years rather than treating analysis as two separate sets of analyses (i.e not to treating 2011 and 2012 as two separate data set)</p>

<p>Whilst trying to do the analysis I have created dependent variable as farm having the disease or not between year 2011 and 2012(Orf.Yes.No2011.2012)against the dependent variables using logistic regression:</p>

<p>I'm just wondering whether I doing the right thing or what could be the best statistical approach which will allow for statistical comparison between the two years? Any help will be very much appreciated</p>

<pre><code>Here is the R output and sample of dataset:





 &gt; mod=glm(Orf.Yes.No2011.2012~F2011+ F2012+as.factor(Breed)+ 
                                  D2011+D2012,family=binomial, data=orf)
      summary(mod)

    Call:
    glm(formula = Orf.Yes.No2011.2012 ~ F2011 + F2012 + as.factor(Breed) + 
        D2011 + D2012, family = binomial, data = orf)

    Deviance Residuals: 
       Min      1Q  Median      3Q     Max  
    -1.862  -1.293   1.023   1.065   1.318  

    Coefficients:
                        Estimate Std. Error z value Pr(&gt;|z|)  
    (Intercept)        0.3917290  0.1626769   2.408    0.016 *
    F2011              0.0003269  0.0002782   1.175    0.240  
    F2012             -0.0003596  0.0002786  -1.291    0.197  
    as.factor(Breed)2  0.0558285  0.1489246   0.375    0.708  
    D2011             -0.0311978  0.0272068  -1.147    0.252  
    D2012              0.0226963  0.0274981   0.825    0.409  
    Small sample data set:

    F2011   F2012   Breed   Orf.Yes.No2011  Orf.Yes.No2012  Orf.Yes.No2011.2012
    155     150     1       0               0               0
    740     760     2       0               1               1
    1000    850     1       0               0               0
    1630    1520    1       1               1               1
    0       460     1       0               0               0
    1300    1335    1       0               1               1
    450     450     1       0               0               0
    390     730     1       1               0               1
    390     380     2       0               0               0
    600     600     2       0               0               0
</code></pre>
"
"0.158293864720653","0.159203084517278","229884","<p>I have a cancer classification problem (type A vs type B) on radiological images from which i have generated 756 texture-based predictive features (wavelet transform followed by texture analysis, i.e., features described by Haralick, Amasadun etc) and 8 semantic features based on subjective assessment by expert radiologist. This is entirely for research and publication to show that these predictive features may be useful in this particular problem. I do not intend to deploy the model for practitioners. </p>

<p>I have 107 cases. 60% cases are type A and 40% type B (in keeping with their natural proportions in population). I have done several iterations of model development with varying results. One particular method is giving me an 80% 80% classification accuracy but I am suspicious that my method is not going to stand critical analysis. I am going to outline my method and a few alternatives. I will be grateful if someone can pick if it is flawed. I have used R for this:</p>

<p>Step 1: Split into 71 training and 36 test cases.<br>
Step 2: remove correlated features from training dataset (766 -> 240) using findcorrelation function in R (caret package)<br>
Step 3: rank training data features using Gini index (Corelearn package)<br>
Step 4: Train multivariate logistic regression models on top 10 ranked features using subsets of sizes 3 , 4, 5 ,and 6 in all possible combination (<sup>10</sup>C<sub>3</sub>=252, <sup>10</sup>C<sub>4</sub>=504, <sup>10</sup>C<sub>5</sub>=630). So <strong>total 1386 multivariate logistic regression models were trained</strong> using 10-fold cross-validation and tested on test dataset.<br>
Step 5: Of these I selected a model which gave the best combination of training and test dataset accuracy, i.e., 3 feature model with 80% 80% accuracy.<p></p>

<p>Somehow running 1300 permutations seems quite dodgy to me and seems to have introduced some false discovery. Just want to confirm if this is a valid ML technique or whether I should skip step 4 and only train on top 5 ranked features without running and permutations.</p>

<p>Thanks. <p> PS I experiemented a bit with naive bayes and random forests but get rubbish test set accuracy so dropped them</p>

<p>====================</p>

<h1>UPDATE</h1>

<p>Following discussion with SO members, i have changed the model drastically and thus moved more recent questions regarding model optimisation into a new post <a href=""http://stats.stackexchange.com/questions/232829/lasso-regularised-classification-highly-variable-choice-of-lambda-min-on-repeate"">LASSO regularised classification highly variable choice of lambda.min on repeated cv</a></p>
"
"0.202490192118848","0.203653269990639","231066","<p>*EDIT: I ran test again with data set provided and realized that the cause of problem is definitely rank deficiency, because estimated values of parameters in nonlinear regression showed non existing p values and there was no way to create confidence intervals with this data. </p>

<h2>Thank you all for reading and help! This question is closed.</h2>

<p>I researched seed germination. I took 75 seed replicates and put them in  different ecological parameters (like temperature) and took data about sprouts in different time intervals. </p>

<p>Reading statistical science papers about this topic, I found that I should analyze my data in a time-to-event model (dose response curve), where I can use log-logistic regression or nonlinear regression (Ritz et al., 2013 -<a href=""http://dx.doi.org/10.1016/j.eja.2012.10.003"" rel=""nofollow"">http://dx.doi.org/10.1016/j.eja.2012.10.003</a>). </p>

<p>Two models (nonlinear and log-logistic) lead to quantitatively very similar fitted germination curves, i.e., similar parameter estimates, but qualitatively different statements about the precision of estimates. Nonlinear regression model yields an overly precise estimate of the proportion of seeds that germinated during the experiment, so the precision reported by the nonlinear regression is too high.</p>

<p>Similarly, the 95% confidence intervals of the fitted curves also demonstrate the dramatic difference in precision of the two models: Accurate prediction of germination percentages is not warranted by the data unless very low percentages are of interest.</p>

<p>Because of that I choose log-logistic regression as a model. First few data sets; treatments analyzed in R using analysis of Dose-Response Curves (drc package) went smooth, and I was able to plot and get final graph. Such data, which was successfully analyzed, contained treatments where max seeds germination was for example 50% of total seed number.</p>

<p>Example:</p>

<p><a href=""http://i.stack.imgur.com/yUqOu.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/yUqOu.jpg"" alt=""Example of successful data analysis""></a></p>

<p>The problems arose when I entered the log-logistic model with treatment where all the seeds germinated in a short amount of time (meaning the treatment for this set of seeds is most adequate for their successful sprouting). For example, 100% of seeds germinated in only 5 days, so there are only two or three time intervals and a large number of sprouted seeds. The R program here reported  convergence error:</p>

<pre><code>Error in optim(startVec, opfct, hessian = TRUE, method = optMethod, control = list(maxit = maxIt,  : 
non-finite value supplied by optim
Error in drmOpt(opfct, opdfct1, startVecSc, optMethod, constrained, warnVal,  : 
Convergence failed 
</code></pre>

<p>Since I'm still a student in biology I have a very basic knowledge in statistics, so I tried to solve the problem with literature. </p>

<p>At first I thought that convergence failed because of perfect or complete separation, but through longer research it seems that the problem lies in rank deficiency. </p>

<p>When I analyzed the same data with nonlinear regression I've managed to fit curve and plot a graph without a problem.  </p>

<p>So, is there a way to make log-logistic model work even though I have obviously small data in cases of 100% germination? Should I switch to nonlinear regression  even though the reported precision would be too high. </p>
"
"0.197089480383364","0.187209229629581","232829","<p>I am using CT scans to classify lung cancer into one of two types (Adenocarcinoma vs. Squamous carcinoma; we can abbreviate them A &amp; B). I am applying LASSO penalized logistic regression to a data set containing 756 CT-derived texture features and 12 radiologist identified categorical (yes/no) features. The latter are based on literature for relevance whereas the former are computer generated with no prior proof of being useful. I have 107 cases so my final dataframe (df) is 107 x 768 dimensional:</p>

<p>i)Texture features (mathematical quantities n=756) are continuous variables scaled and centered. Their names are stored in list <code>â€˜texVarsâ€™</code></p>

<p>ii)Semantic features(Qualitative features subjective assessed by experienced radiologist, n=12). These are usually categorical binary inputs of yes / no type. Their names are stored in list <code>â€˜semVarsâ€™</code>.</p>

<p>Following comments from community on my original (very different) model <a href=""http://stats.stackexchange.com/questions/229884/is-my-high-dimensional-data-logistic-regression-workflow-correct"">Is my high dimensional data logistic regression workflow correct?</a>, I performed my LR development in three steps:</p>

<p>1)Feature selection: I used principle components analysis to reduce texture feature-space from 756 to 30. I kept 4 most relevant (from literature) semantic features. This gave me 34 final features. I used the following command:</p>

<pre><code>trans = preProcess(df[,texVars], method=c(""BoxCox"", ""center"",   ""scale"", ""pca""),thresh=.95)  # only column-names matching â€˜texVarsâ€™ are included.
neodf2 &lt;- predict(trans,df[,texVars]).
neodf.sem &lt;- neodf2[,c(""Tumour"",""AirBronchogram"", ""Cavity"", ""GroundglassComponent"",""Shape"")]  # this DF is 107 x 4 dimensional, containing only 4 semantic features (most relevant from prior knowledge).
neodf.tex &lt;- neodf2[,c(""Tumour"",setdiff(names(neodf2),names(neodf.sem)))] # this only has the 30 PCA vectors (labelled PC1 â€“ PC30).
</code></pre>

<p>2) Model development (LASSO) and penalty term tuning (10fold cross-validation) using cv.glmnet command  Deviance was used as determinant of model quality. Using this method, I developed a model incorporating only semantic features, a second model incorporating only texture features, and a third model incorporating both semantic and texture features. Here are the commands:</p>

<pre><code>#Converting to model.matrix for glmnet 
xall &lt;- model.matrix(Tumour~.,neodf2)[,-1]
xtex &lt;- model.matrix(Tumour~.,neodf.tex)[,-1]
xsem &lt;- model.matrix(Tumour~.,neodf.sem)[,-1]
y &lt;- neodf$Tumour
require(glmnet)
grid &lt;- 10^seq(10,-2,length=100)

lasso.all &lt;- cv.glmnet(xall,y,alpha=1,lambda=grid, family=""binomial"", type.measure=""deviance"") 
lasso.tex &lt;- cv.glmnet(xtex,y,alpha=1,lambda=grid, family=""binomial"", type.measure=""deviance"")
lasso.sem &lt;- cv.glmnet(xsem,y,alpha=1,lambda=grid, family=""binomial"", type.measure=""deviance"")
</code></pre>

<p>3) Testing model classification accuracy on entire dataset. The following is the backbone of  bootstrap to generate 95% confidence intervals of predictive accuracy:</p>

<pre><code>pred &lt;- predict(lasso.all, newx = xall, s = ""lambda.min"", ""class"")
tabl &lt;- table(pred,y)
sum(diag(prop.table(tabl)))
</code></pre>

<p>4) As an alternative means to assess model performance than classification accuracy, I used ROC area under curve on entire dataset and compared AUROC curves from different models using DeLong's method (pROC package)</p>

<p>The results are interesting</p>

<pre><code> =================================================
</code></pre>

<p>LR MODEL BASED ON SEMANTIC FEATURES ALONE:
    lasso.sem$lambda.min
     0.01</p>

<p>Plot cv lambda vs. binomial devance <a href=""http://i.stack.imgur.com/6Modw.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6Modw.png"" alt=""cv lambda vs binomia deviance""></a></p>

<pre><code>             Feature          Odds Ratio
1                (Intercept)  0.1292604
2      AirBronchogramPresent  0.1145378
3              CavityPresent 35.4350358
4 GroundglassComponentAbsent  4.3657928
5                 ShapeOvoid  2.4752881

AUC: .84


=================================================    
</code></pre>

<p>LR MODEL BASED ON TEXTURE FEATURES ALONE:</p>

<pre><code>lasso.tex$lambda.min
1e+10   
</code></pre>

<p>Plot  cv lambda vs binomial deviance (texture alone). Note how the 95% CI's are all overlapping! <a href=""http://i.stack.imgur.com/1Mk7M.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1Mk7M.png"" alt="" cv lambda vs binomial deviance ""></a></p>

<pre><code>   Feature OddsRatio
1 (Intercept) 0.6461538



============================================================
</code></pre>

<p>LR MODEL BASED ON TEXTURE + SEMANTIC FEATURES:</p>

<pre><code>lasso.all$lambda.min
0.05 
</code></pre>

<p>Plot  cv lambda vs binomial deviance <a href=""http://i.stack.imgur.com/p1AHX.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/p1AHX.png"" alt=""cv lambda vs binomial deviance""></a></p>

<pre><code>                          Feature   OddsRatio
1                (Intercept)        0.3136489
2                       PC23        0.9404430
3                       PC27        0.8564001
4      AirBronchogramPresent        0.2691959
5              CavityPresent        6.7422427
6 GroundglassComponentAbsent        2.0514275
7                 ShapeOvoid        1.5974378

 AUC : .88
</code></pre>

<p>Plot showing loglambda vs coefficients. The dashed vertical line shows the cross-validated optimum lambda:<a href=""http://i.stack.imgur.com/d6FaO.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/d6FaO.jpg"" alt=""enter image description here""></a></p>

<p>Having rejected the texture only model, which only contains intercept, i was left with two models - semantic and combined texture+semantic. I created ROC curves for both and compared them using DeLong's method:</p>

<pre><code>pred.sem&lt;- predict(lasso.sem, newx = xsem, s = ""lambda.min"")
pred.all&lt;- predict(lasso.all, newx = xall, s = ""lambda.min"")

roc.sem&lt;- roc(y,as.numeric(pred.sem), ci=TRUE, boot.n=1000, ci.alpha=0.95,stratified=FALSE)    

roc.all&lt;- roc(y,as.numeric(pred.all), ci=TRUE, boot.n=1000, ci.alpha=0.95,stratified=FALSE)
</code></pre>

<p>Outputs of ROC analysis are:</p>

<pre><code>data:  roc.sem and roc.all
Z = -2.1212, p-value = 0.0339
alternative hypothesis: true difference in AUC is not equal to 0
sample estimates:
AUC of roc1 AUC of roc2 
  0.8369963   0.8809524 
</code></pre>

<p>Showing that combined model ROC curve is significantly better despite the modest improvement in AUC (83% vs 88%).
Questions are:</p>

<p>a) is my methodology airtight from a publication point of view now? Apologies in advance for any gross errors in my presentation of this problem.</p>

<p>b) what is the formal inference that texture model is intercept only.</p>

<p>c) if texture model is useless, how do its variables become useful once added to semantic features and yield a higher overall accuracy in the combined result? perhaps that means the effect of texture features alone is too small to be detected in this small dataset but becomes apparent when combined with a stronger predictor (i.e., semantic features). </p>

<p>Any further comments are welcome.</p>
"
"0.0674967307062827","0.0678844233302131","233802","<p>I've been given a data set containing 155 training examples and 108 features.I removed the features with more than 79 NA values and brought then them down to 99. I trained using the first 140 examples and used the rest for testing the prediction.The target variable is a discrete value i.e 0,1,2,3,4 though not restricted to those. I tried using multinomial logistic regression and random forest and got poor accuracy. Is there an algorithm that performs well on small data sets? Decision trees and regression don't seem to be working well for this. I am using R for the data analysis</p>
"
"0.126274820515127","0.108857251714449","234537","<p>I've split my data set into a training and test set. I've performed a principal component analysis on the training set and have used the first 3 principal components to generate a logistic regression model for my response.</p>

<p>I now want to use this model to make predictions for my test data set and check if this is true. </p>

<p>I've been trying to use the predict function but obviously the model uses the principal components of the training set as the predictors whereas my test set just has all the original predictors so obviously they're not compatible.</p>

<p><strong>How do I go about 'projecting' my test data onto the principal components I've already generated so I can use my model to make predictions?</strong></p>

<p>Ideally I'd like to do this without using any external packages (it's for university). I am working in R.</p>
"
"0.150927278118246","0.151794185179729","234541","<p>I am currently doing an analysis for my Master Thesis and encountered some results I cannot explain.</p>

<p>In my paper, I am trying to explore factors that decide whether people joined a local energy initiative or not. Since I have a lot of different variables, my instructor suggested a model building approach. Concretely, I am adding sets of predictors to my logistic regression and only keep those that are significant in the model, before adding the next set. To assess model fit, I was told to use classification tables.</p>

<p>My problem now is the following:</p>

<p>I start with a set of dummies to control for participants coming from different neighbourhoods. This basic model classifies 56% of cases correctly. Now I add the second set of predictors and some of them are significant, so I keep those in the model. If I now use the classification table again, my classification got worse. Even worse than chance! (48%).</p>

<p>How can I find significant predictors but my model gets worse than chance?</p>

<p>EDIT FOR ADDITIONAL INFO:</p>

<p>My Dataset consits of 636 cases. 318 are partakers of the initiative, 318 are not partakers. The sets of variables I use are structured as follows:</p>

<p>1) ""Control"": People come from 30 different neighbourhoods, so I added 29 dummy variables to control for differences due to neighbourhood membership (not the best approach, I know, but IÂ´m just following orders on this one)</p>

<p>2) Individual predictors: 15 demographic and psychological variables</p>

<p>3) Assessment of group predictors: 8 variables that measure how individuals perceive the group of potential partakers</p>

<p>I used the classification tables on the same data that I used for building the model, unfortunately I only have this one dataset and IÂ´m trying to figure out which predictors are most promising for future (causational) research.</p>
"
