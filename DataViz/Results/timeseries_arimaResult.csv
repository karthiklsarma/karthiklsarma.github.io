"V1","V2","V3","V4"
"0.0490880693673816","0.0641500299099584","9512","<p>I have a time series data of 30 years and found that ARIMA(0,1,1) has best model among others. I have used the simulate.Arima (forecast package) function to simulate the series into the future.</p>

<pre><code>library(forecast)

series &lt;- ts(seq(25,55), start=c(1976,1))

arima_s &lt;- Arima(series, c(0,1,1))

simulate(arima_s, nsim=50, future=TRUE)
</code></pre>

<p>Later on, i have found the updated value of first forecasted year (i.e. series[31] &lt;- 65). Now i want to simulate the series with this updated value. I am wondering how to do this in R.</p>
"
"0.0490880693673816","0.0641500299099584","8750","<p>If I have an arima object like <code>a</code>:</p>

<pre><code>set.seed(100)
x1 &lt;- cumsum(runif(100))
x2 &lt;- c(rnorm(25, 20), rep(0, 75))
x3 &lt;- x1 + x2

dummy = c(rep(1, 25), rep(0, 75))

a &lt;- arima(x3, order=c(0, 1, 0), xreg=dummy)
print(a)
</code></pre>

<p>.</p>

<pre><code>Series: x3 
ARIMA(0,1,0)                    

Call: arima(x = x3, order = c(0, 1, 0), xreg = dummy) 

Coefficients:
        dummy
      17.7665
s.e.   1.1434

sigma^2 estimated as 1.307:  log likelihood = -153.74
AIC = 311.48   AICc = 311.6   BIC = 316.67
</code></pre>

<p>How do calculate the R squared of this regression?</p>
"
"NaN","NaN","7956","<p>Starting out with arima models in R, I do not understand why fitted.values (of an AR(2) process for example) are not part of the output like they are in regressions. Did I miss them when running <code>str(result)</code> or did I get something completely wrong? </p>
"
"0.148006097976858","0.116051770637132","10037","<p>let v to be forecasted value for periods 1 through T and $v_{t}$ be its forecasted value at time $t$. We express $v_{t}$  as the sum of two terms, its mean at time $t$,  and its deviation from the mean at time $t$, $\epsilon_{t}$. In other words, $$ v_{t}= \overline{v_{t}} + \epsilon_{t} $$ 
The $\overline{v_{t}}$ are chosen based on the arguments. The $\epsilon_{t}$  term is assumed to be a normally distributed random variable with mean zero and standard deviation $\sigma (Îµ_{t})=0.234$.</p>

<p>The moving average formation of order q is choosen, MA(q) where q is the number of lagged terms in the moving average. We use the following moving average specification:
$$\epsilon_{t} = \sum^{q}_{i=0}{\alpha_{i} \mu_{t-i}} $$</p>

<p>where $\mu_{t-i}$ are independently distributed standard normal random variables. To ensure that the standard deviation of $Îµt$ is equal to its pre-specified value, we set the
$$\alpha_{i}= \frac{\sigma(\epsilon_{t})}{\sqrt{q+1}}$$
Note that $\epsilon t$ depends on $q+1$ random terms.</p>

<p>the R-code that i have used for above model</p>

<pre><code>q=31
iter=10000
for(i in 1:31){alpha_i[i] &lt;- 0.234/(sqrt(31+1))}
err_pk &lt;- array(0,c(iter,11))
for(i in 1:iter){err_pk[i,] &lt;- arima.sim(list(order=c(0,0,31), ma=alpha_i), n=11, innov=rnorm(iter,0,1))}
ffe &lt;- array(0,11)
for(i in 1:11){ffe[i] &lt;- median(err_pk[,i])}`
plot.ts(ffe)
</code></pre>

<p>I am wondering that, $\alpha$ is changing through time?</p>

<p>and also i never get the same output as in the figure 2 <a href=""http://www.nature.com/nature/journal/v412/n6846/extref/412543a0_S1.htm"" rel=""nofollow"">The end of world Population growth Nature, v412, 543</a></p>

<p>the parameter for figure in the paper are:</p>

<p>Note: MA(30), (31 terms), $\sigma(\epsilon_{t})=0.234$, 31 initial values of $\mu=0$, 10,000 simulation</p>

<p>Am I missing any thing?</p>
"
"0.0981761387347632","0.0962250448649376","10110","<p>100 periods have been collected from a 3 dimensional periodic signal. The wavelength slightly varies. The noise of the wavelength follows Gaussian distribution with zero mean. A good estimate of the wavelength is known, that is not an issue here. The noise of the amplitude may not be Gaussian and may be contaminated with outliers.</p>

<p><strong>How can I compute a single period that approximates 'best' all of the collected 100 periods?</strong></p>

<p>I have no idea how time-series models work. Are they prepared for varying wavelengths? Can they handle non-smooth true signals? If a time-series model is fitted, can I compute a 'best estimate' for a single period? How?</p>

<p>A related question is <a href=""http://stackoverflow.com/q/2572444/341970"">this</a>. Speed is not an issue in my case. Processing is done off-line, after all periods have been collected.</p>

<p><strong>Origin of the problem:</strong> I am measuring acceleration during human steps at 200 Hz. After that I am trying to double integrate the data to get the vertical displacement of the center of gravity. Of course the noise introduces a HUGE error when you integrate twice. I would like to exploit periodicity to reduce this noise. Here is a crude graph of the actual data (y: acceleration in g, x: time in second) of 6 steps corresponding to 3 periods (1 left and 1 right step is a period):
<img src=""http://i.stack.imgur.com/44q8d.png"" alt=""Human steps""></p>

<p>My interest is now purely theoretical, as <a href=""http://jap.physiology.org/content/39/1/174.abstract"" rel=""nofollow"">http://jap.physiology.org/content/39/1/174.abstract</a> gives a pretty good recipe what to do. It does not address periodicity.</p>

<p><strong>Note:</strong> I have asked <a href=""http://stackoverflow.com/q/5702974/341970"">this question on stackoverflow</a> but it seems to be off-topic there.</p>
"
"NaN","NaN","53429","<p>I'm wondering if someone might be able to help me locate an appropriate model for the following two time-series (the cyan and blue one, the reds are rolling means).</p>

<p><img src=""http://i.stack.imgur.com/LN9RP.png"" alt=""[The time-series in question](http://i.imgur.com/pmXerft.png)""></p>

<p>I'm looking more for a general direction based on the attributes of the data rather than a ""use this"" answer. Otherwise I'll learn less. Though all assistance is welcome. </p>
"
"0.0981761387347632","0.0962250448649376","52617","<p>I have time series data for a set of cities that goes back for about 10 years. I also have the data at the state level for almost 30 years. There was an event that occurred about 20 years ago, that is captured in the longer, state level data, but not the city data, that I would like to investigate at the city level. </p>

<p>What I think might be useful is to create some kind of ARIMA model that regresses the state data as an exogenous variable. If I were to do this, how do I use the model to backfill the city data such that it ends at the same point as where the actual city data starts? Is there already a canonical method of doing something like this? Thanks for any help you can give (literature references, R libraries, etc.)</p>
"
"0.213969933705931","0.27962349760262","14742","<p>I am fitting an ARIMA model on a daily time series.
Data are collected daily from 02-01-2010 to 30-07-2011 and are about newspaper sales.
Since a weekly pattern in sales can be found (the daily average amount of copies sold is usually the same from Monday to Friday, then increases on Saturday and Sunday), I am trying to capture this ""seasonality"".
Given the sales data ""data"", I create the time series as follows:</p>

<pre><code>salests&lt;-ts(data,start=c(2010,1),frequency=365)
</code></pre>

<p>and then I use the auto.arima(.) function to select the best ARIMA model via AIC criterion. The result is always a non-seasonal ARIMA model, but if I try some SARIMAs model with the following syntax as example:</p>

<pre><code>sarima1&lt;-arima(salests, order = c(2,1,2), seasonal = list(order = c(1, 0, 1), period = 7))
</code></pre>

<p>I can obtain better results.
Is there anything wrongs in the ts command / arima specification? The weekly pattern is very strong so I would not expect so many difficulties in capturing it. 
Any help would be very useful.
Thank you,
Giulia Deppieri</p>

<p>Update:</p>

<p>I have already changed some arguments. More precisely, the procedure selects ARIMA(4,1,3) as the best model when I set <code>D=7</code>, but AIC and the others good of fit indexes and forecasts as well) do not improve at all. I guess there's some mistakes due to confusion between seasonality and periodicity..?! </p>

<p>Auto.arima call used and output obtained:</p>

<pre><code>modArima&lt;-auto.arima(salests,D=7,max.P = 5, max.Q = 5)



 ARIMA(2,1,2) with drift         : 1e+20
 ARIMA(0,1,0) with drift         : 5265.543
 ARIMA(1,1,0) with drift         : 5182.772
 ARIMA(0,1,1) with drift         : 1e+20
 ARIMA(2,1,0) with drift         : 5137.279
 ARIMA(2,1,1) with drift         : 1e+20
 ARIMA(3,1,1) with drift         : 1e+20
 ARIMA(2,1,0)                    : 5135.382
 ARIMA(1,1,0)                    : 5180.817
 ARIMA(3,1,0)                    : 5117.714
 ARIMA(3,1,1)                    : 1e+20
 ARIMA(4,1,1)                    : 5045.236
 ARIMA(4,1,1) with drift         : 5040.53
 ARIMA(5,1,1) with drift         : 1e+20
 ARIMA(4,1,0) with drift         : 5112.614
 ARIMA(4,1,2) with drift         : 4953.417
 ARIMA(5,1,3) with drift         : 1e+20
 ARIMA(4,1,2)                    : 4960.516
 ARIMA(3,1,2) with drift         : 1e+20
 ARIMA(5,1,2) with drift         : 1e+20
 ARIMA(4,1,3) with drift         : 4868.669
 ARIMA(5,1,4) with drift         : 1e+20
 ARIMA(4,1,3)                    : 4870.92
 ARIMA(3,1,3) with drift         : 1e+20
 ARIMA(4,1,4) with drift         : 4874.095

 Best model: ARIMA(4,1,3) with drift        
</code></pre>

<p>So I assume the arima function should be used as:</p>

<pre><code>bestOrder &lt;- cbind(modArima$arma[1],modArima$arma[5],modArima$arma[2])
sarima1&lt;-arima(salests, order = c(4,1,3))
</code></pre>

<p>with no seasonal component parameters and period specifications.
Data and exploratory analysis show that the same weekly pattern can be approximatively considered for each week, with the only exception of August 2010 (when a consistent increase in sales is registered). Unfortunately I have no expertise in timeseries modeling at all, in fact I am trying this approach in order to find an alternative solution to other parametric e non-parametric models I have tried to fit for these problematic data.
I have also many dependent numeric variables but they have shown low power in explaining the response variable: undoubtedly, the most difficult part to model is the time component. Moreover, the construction of dummy variables to represent months and weekdays turned out not to be a robust solution.  </p>
"
"0.0490880693673816","0.0641500299099584","53051","<p>I was working on ARIMA in R and I am trying not to use library <code>forecast</code> as much as possible. I have a code for finding the best ARIMA model, but it is showing some warning messages.</p>

<p>Here is my function:</p>

<pre><code>best.aic&lt;-1e8
n&lt;-length(x.ts)
for(p in 0:maxord[1]) for(d in 0:maxord[2]) for(q in 0:maxord[3])
{   
        fit&lt;-arima(x.ts,order=c(p,d,q))
    fit.aic&lt;--2*fit$loglik+(log(n)+1)*length(fit$coef)
    if(fit.aic&lt;best.aic)
    {
        best.aic&lt;-fit.aic
        best.fit&lt;-fit
        best.model&lt;-c(p,d,q)
    }
}
list(best.aic,best.fit,best.model)
</code></pre>

<p>It gives the same warning 4 times:</p>

<pre><code>In arima(x.ts, order = c(p, d, q)) :
possible convergence problem: optim gave code=1
</code></pre>

<p>Also, I will appreciate any help concerning simulation of ARIMA by hand, not by <code>arima.sim</code> function. Thank you.</p>
"
"0.0694210134500623","0.0907218423253029","55168","<p>Hi all I'm trying to do one step ahead forecast. Lets say I have 1000 data and fit an ARIMA model with it and then I do a forecast for one period ahead. When I get more data I would like to forecast another step using the new data without having to reestimate all coefficients and so on...</p>

<p>This is my code but for some reason it's very slow for a bigger dataset and am not too sure that is doing what I want:</p>

<pre><code>set.seed(1234)
y=ts(log(35+10*rnorm(1000)))
set.seed(4567)
new.data=ts(log(35+10*rnorm(10)))

library(forecast)
model = auto.arima(y)

onestep.for=forecast(model,h=1)
for (i in 1:10) {
  data=c()
  data=c(y,new.data[1:i])
  newfit=Arima(data, model=model)
  forec=forecast(newfit,h=1)
  onestep.for=c(onestep.for,forec)
}
</code></pre>
"
"0.0981761387347632","0.128300059819917","184425","<p>I've sampled 100 variables from a Gauss distribution with mean 0 and standard deviation 1.</p>

<pre><code>&gt; set.seed(1)
&gt; wn=rnorm(100)
</code></pre>

<p>Then I've fitted an AR(1) model with the arima command and sent the results to the wnF variable</p>

<pre><code>&gt; wnF=arima(wn, order=c(1,0,0))
</code></pre>

<p>Finally I've requested the estimated coefficients</p>

<pre><code>&gt; wnF$coef
         ar1    intercept 
-0.003655755  0.108935363 
</code></pre>

<p>Now I want to replicate the computation R. I exported the data to an Excel file (<a href=""https://www.dropbox.com/s/6c8ukcbtxe9gqp1/DataTester.xlsx?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/6c8ukcbtxe9gqp1/DataTester.xlsx?dl=0</a>) and computed the following model:</p>

<p>$$
x{_t}-\mu = \psi_1x_{t-1}+\omega_t
$$</p>

<p>I've replaced $\mu$ and $\psi_1$ with the <strong>intercept</strong> and <strong>ar1</strong> coefficients reported by the <code>wnF$coef</code> command. I've also replaced $\omega_t$ with zero, since I've sampled the data from a zero-mean population.</p>

<p>Finally I've compared the residuals from the R computed model (<code>wnF$residuals</code>) with the residuals I've computed in the Excel file and I've noticed that they differ about $\delta&lt;0.0005$ in absolute value.</p>

<p>I know that 0.0005 is not much, but when dealing with such small values it may not be negligible.</p>

<p>I also find strange that there is no difference up the fourth decimal place.</p>

<p>Can you please help me finding the origin of the difference?</p>
"
"0.0490880693673816","0.0641500299099584","225995","<p>I'm new to time series modeling and am trying to do seasonal ARIMA modeling here. I have figured out the p,d,q values but im not sure how to select the period  in the below formula. There seem to be troughs in the data during summer months and winter holidays, what does it suggest the value of period should be, what is the concept.</p>

<pre><code>arima(time_series,c(2,1,4),seasonal=list(order=c(2,1,4),period=&lt;??&gt;))
</code></pre>

<p>My data looks like this</p>

<p><a href=""http://i.stack.imgur.com/7yxe2.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7yxe2.png"" alt=""![Viewership Data""></a></p>

<p><strong>EDIT</strong>
 : I have 365 data points for a year</p>
"
"0.155230105141267","0.142002144538376","225578","<p>I have the following time series dataset (dependent | independent) :</p>

<pre><code>Sales | Income,Inflation, Interest Rates etc
</code></pre>

<p>All of this is dynamic data pertaining to each of 24 months (month:0 to month:24). For 25th month onward I have no data for the independent variables (Income,Inflation, Interest Rates etc), yet I want to be able to predict sales for month:25 +.</p>

<p>I have been trying to figure out models which I can used to implement this scenario including Dynamic Regression and ARMAX/ARIMAX models. However, it seems that to be able to predict sales for the 25th month, i need data for dependent variables (Income,Inflation, Interest Rates etc) for the month (25). </p>

<p>Can I create a model using lagged values of the dependent and independent variables, used together in a regression model? I'm not sure if that makes sense.</p>

<p>This is my first time series model and im not sure if i am on the right track. Please advise.</p>
"
"0.147264208102145","0.171066746426556","120806","<p>I'm using R(3.1.1), and ARIMA models for forecasting. 
I would like to know <strong>what should be the ""frequency"" parameter, which is assigned in the <code>ts()</code> function</strong>, if im using time series data which is:</p>

<ol>
<li>separated by <strong>minutes</strong> and is spread over 180 days (1440 minutes/day) </li>
<li>separated by <strong>seconds</strong> and is spread over 180 days (86,400 seconds/day).</li>
</ol>

<p>If I recall right the definition, a ""frequency"" in ts in R, is the number of observations per ""season"". </p>

<h2>Question part 1:</h2>

<p>then, what is the ""season"" in my case?</p>

<p>If the season is ""day"", then is the ""frequency"" for minutes = 1440 and 86,400 for seconds?</p>

<h2>Question part 2:</h2>

<p><strong>Could the ""frequency""  also depend on what I am trying to achieve/forecast?</strong>
for example, in my case, I'd like to have a very short-term forecast. 
One-step ahead of 10minutes each time. 
<strong>Would it then be possible to consider the season as an hour instead of a day?</strong>
In that case frequency= 60 for minutes, frequency = 3600 for seconds?</p>

<p>I've tried for example to use frequency = 60 for the minute data and got better results compared to frequency = 1440 (used <code>fourier</code> see link below by Hyndman)
<a href=""http://robjhyndman.com/hyndsight/forecasting-weekly-data/"">http://robjhyndman.com/hyndsight/forecasting-weekly-data/</a> </p>

<p>(The comparison was made by using MAPE for the measure of forecast accuracy)</p>

<p><strong>In case the results are complete arbitrary, and the frequency cannot be changed. 
What would be actually the interpretation of using freq = 60 on my data?</strong> </p>

<p>I also think it's worth mentioning that my data contains seasonality at every hour and every two hours (by observing the raw data and the Autocorrelation function)</p>

<p>Thanks!</p>
"
"NaN","NaN","55937","<p>I have fitted an ARIMA(1,1,2) to time series <code>TS1</code> as below:</p>

<pre><code>arima112 &lt;- arima(TS1, c(1,1,2))
</code></pre>

<p>Now I want to use the coefficients of AR and MA that I got from <code>arima112</code> to forecast another time series (<code>TS2</code>). How can I apply the <code>arima112</code> model on <code>TS2</code>?</p>
"
"0.0694210134500623","0.0907218423253029","160305","<p>I have several different time series with monthly values for 8 years, where I fit an ARIMA model.</p>

<p>And the purpose is to forecast the next year <em>and</em> indicate possible outliers in a fancy way. </p>

<p>Is the simplest way of doing this just to check the graph and see which values are outside the confidence intervals of the suggested forecast by the ARIMA model?</p>

<p>For e.g (random pic from google)</p>

<p><img src=""http://i.stack.imgur.com/0omXk.png"" alt=""enter image description here""></p>

<p>Or can one, in say <strong>R</strong>, use some more fancy tools that would tell you the outliers aswell as the ARIMA forecasts? </p>
"
"0","0.0907218423253029","203105","<p>My apologies in advance for asking what I suspect is a dumb question. I have looked around and I can't figure this out.</p>

<p>I've done an auto.arima model in r.</p>

<pre><code>revenue = ts(arima$both.markets, frequency = 7)
    media=ts (arima$all_media, frequency = 7)
xreg &lt;- cbind(media=model.matrix(~as.factor(media)))
xreg &lt;- xreg[,-1]
modArima &lt;- auto.arima(revenue, xreg=xreg)
</code></pre>

<p>The output includes this:</p>

<pre><code>                ma1     ma2     sar1    (media)1    (media)2    (media)3    media)4 (media)5    (media)6    (media)7    (media)8    (media)9    (media)11   (media)13   (media)17   (media)18   (media)20   (media)23   (media)39
Coefficients:   -0.4081 -0.5391 0.6145  -8345.84    20129.82    1809.952    -14906.92   -42454.82   1885.815    -101350.54  12055.98    56197.28    -49130.22   128427.87   45600.38    -28911.02   46118.11    -95280.16   62833.34
s.e.            0.0791  0.0778  0.0718  11672.8     13384.69    21822.541   34298.06    23533.55    30755.171   35534.13    57394.97    44116.15    55263.58    55887.56    56920.24    60269.17    40252.73    49884.66    63023.11
</code></pre>

<p>Are the various <code>media</code> outputs lags of the media variable? If not, how can I include lags in the model?</p>
"
"0.0694210134500623","0.0907218423253029","203142","<p>my professor says that if we see that there are NA values for the AR or MA terms(either the estimated values or the estimated se) in the R output for the arima models fitted using the arima() function, it is suggesting that our model is over fit. Is he right?    </p>

<p>I figured out that if I allow the function for a slightly higher number of iterations allowed, we can get get rid of those NAs in estimates and be able to obtain the a model.    </p>

<p>So it appears to me that it can be just a simple problem with the optimization rather than blame the model to be too complex and overfit.  </p>
"
"0.0694210134500623","0.0907218423253029","179378","<p>I was thinking, is it possible to implement a quarterly forecast for one year ahead such that its sum over year equals some constant number?</p>

<p>This problem may arise if we have, for example, some external forecast over next year, and we need to produce a quarterly forecast that is consistent with the yearly one.</p>

<p>Theoretically, I can write down an ML-maximization problem, and write then some stack of code. But is there maybe some existing solutions?</p>

<p>Thanks!</p>
"
"0.0850230301897704","0.0740740740740741","124349","<p>I'm new to TS modeling, but have some experience in classic classification modeling. In classification I can train one model and use it for some time while some indices are stable (e.g. PSI).</p>

<p>Assuming I want to forecast daily demand on bread. One day (31 DEC 2013) I've trained ARIMA model and made a forecast for tomorrow (1 JAN 2014). Is there any way to use this model tomorrow (1 JAN 2014) without retrain it on new data (demand before 1 JAN 2014 and demand on 1 JAN 2014)?</p>

<p>The reason to ask: in case of huge amount of goods it requires a lot of time to retrain all models everyday. Retrain it once a month (like in classification usage pattern) will be better.</p>
"
"0.0694210134500623","0.0907218423253029","124351","<p>I'm trying to explain in detail step by step what my code does and I am stuck at explaining what the coefficients are in an Arima model and where they are from/what relevance they have.</p>

<p>Could someone please explain to me what <code>ma1</code>, <code>sar1</code>, <code>sar2</code>, <code>ar1</code>, <code>ar2</code>, <code>ar3</code>, <code>sma1</code> are and if possible show with a formula where they may appear in the equation for an ARIMA process? (The equation is not high priority as long as they are explained well)</p>

<p>Here is an ARIMA model found using <code>?Arima</code> just you can see what i mean by the coefficients</p>

<pre><code>fit &lt;- Arima(WWWusage,order=c(3,1,0))
</code></pre>
"
"0.0490880693673816","0.0641500299099584","57123","<p>I need to estimate parameters of an AR model which is in the form of AR(1,11) it means that coefficients of AR orders from order 2 until order 10 are zero. How can I estimate these two parameters in <code>R</code> since <code>arima</code> function only accepts p as the order of the AR component. Note that this model has a different structure than Seasonal AR. </p>

<p>Thanks</p>
"
"0.129874823886379","0.121232161242218","203477","<p>I would like a test to compare the trends of multiple time series.</p>

<p>I have already saw others solutions, like fitting a same arima model and compare both with a f-test but I want to try another one.</p>

<p>Now, I try to determine if there is a differencing parameter, hoping that the differencing parameter could give me an estimation of the presence of a general trend.</p>

<p>I try to do an ANOVA on the lag, looking at the time series.</p>

<p>A friend told me that this was non-sense. And a simulation shows me that it doesn't work, indeed. My graph don't have a general trend. Why the lag don't show a sense on a possible trend and could an ANOVA be correct, here?</p>

<p>An exemple of code:</p>

<pre><code>library(forecast)
library(ggplot2)

#### test if two arima ts have the same differencing or not:

set.seed(1)
dif.0 &lt;- arima.sim(list(order = c(0,1,0)), 100, rand.gen = rnorm, n.start = 50)
lag.0 &lt;- diff(dif.0)
dif.1 &lt;- arima.sim(list(order = c(0,1,0)), 100, rand.gen = rnorm, n.start = 50)
lag.1 &lt;- diff(dif.1)
dif.2 &lt;- arima.sim(list(order = c(0,0,0)), 101, rand.gen = rnorm, n.start = 50)
lag.2 &lt;- diff(dif.2)

ts &lt;- rbind(data.frame(ts = as.numeric(dif.0), type = ""t0"", t = 1:101)
    , data.frame(ts = as.numeric(dif.1), type = ""t1"", t = 1:101)
    , data.frame(ts = as.numeric(dif.2), type = ""t2"", t = 1:101))

lag &lt;- rbind(data.frame(ts = as.numeric(lag.0), type = ""t0"", t = 1:100)
        , data.frame(ts = as.numeric(lag.1), type = ""t1"", t = 1:100)
        , data.frame(ts = as.numeric(lag.2), type = ""t2"", t = 1:100))

ggplot(ts, aes(x = t, y = ts, color = type, group = type)) +
  geom_line()
</code></pre>

<p><a href=""http://i.stack.imgur.com/ZNllk.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZNllk.png"" alt=""enter image description here""></a>
    ggplot(lag, aes(x = t, y = ts, color = type, group = type)) +
      geom_point()</p>

<pre><code># t2 is white noise, t0 and t1 have a global trend.

# differencing time series:

summary(aov(ts~type, lag[lag$type %in% c(""t0"", ""t1""), ]))
    summary(aov(ts~type, lag[lag$type %in% c(""t0"", ""t2""), ]))
</code></pre>
"
"0.0850230301897704","0.111111111111111","57119","<p>I'm dealing with a time series data and I'm trying to construct a time series model for this particular dataset.  I'm new to R and tried using the the <code>auto.arima</code> function under the forecast package:</p>

<pre><code>fit &lt;- auto.arima(tsdata, xreg=cbind(CSS2$Month,CSS2$DayID,CSS2$Year), 
                  stepwise=FALSE, approximation=FALSE)
summary(fit)
resid(fit)
acf(resid(fit))
</code></pre>

<p>However, I noticed some problems in the ACF plot and the PACF plot from the resulting model.  Both of those appear to show some trends or seasonality.
The data that I'm dealing with do have some strong seasonal patterns (on a weekly basis and on a monthly basis), and I thought that this would be captured using <code>auto.arima</code>.  I do have the most recent version of the forecast package.
I should mention that I have daily data for 3 years, so there are a total of 1095 observations.</p>

<p>Any suggestions would be much appreciated, thank you!</p>

<p>ACF Plot PACF Plot</p>
"
"NaN","NaN","209730","<p>Could someone please explain the differences between the 3 fitting methods, method = c(""CSS-ML"", ""ML"", ""CSS""), in Arima?  If I run the code below I get an error message, but if I specify method=""ML"" in Arima it runs fine.  So I was curious what the difference was between the 3 fitting methods.</p>

<p>Code with error:</p>

<pre><code>library(""fpp"")

tsTrain &lt;- window(hsales,end=1989.99)

pvar&lt;-1:10
dvar&lt;-1:2
qvar&lt;-1:7

OrderGrid&lt;-expand.grid(pvar,dvar,qvar)

n &lt;- function(a,b,c) {Arima(tsTrain, order=c(a,b,c))}
ModFit &lt;- do.call(Vectorize(n, SIMPLIFY=FALSE), unname(OrderGrid))
</code></pre>

<p>Fixed Code:</p>

<pre><code>library(""fpp"")

tsTrain &lt;- window(hsales,end=1989.99)

pvar&lt;-1:10
dvar&lt;-1:2
qvar&lt;-1:7

OrderGrid&lt;-expand.grid(pvar,dvar,qvar)

n &lt;- function(a,b,c) {Arima(tsTrain, order=c(a,b,c),method=""ML"")}
ModFit &lt;- do.call(Vectorize(n, SIMPLIFY=FALSE), unname(OrderGrid))
</code></pre>
"
"0.0850230301897704","0.111111111111111","201669","<p>I have read that auto.arima choses the model with the best AIC.</p>

<p>I am looking to create a model that best neutralises the autocorrelation, as it will be used for prewhitening.</p>

<p>Can I use auto.arima for that, or should I use a different function/is this at all possible in R? I am looking to find the best model that removes the acf.</p>
"
"0.157432060586516","0.171448166624547","123723","<p>Can anyone tell me the formula behind the <code>forecast</code> function in R? Preferably in the form easily understood by mathematicians (e.g  x_t, Î¸ etc)</p>

<p>Here is my code in case it helps</p>

<pre><code>suppressMessages(library(lmtest))
suppressMessages(library(car))
suppressMessages(library(tseries))
suppressMessages(library(forecast))
suppressMessages(library(TTR))
suppressMessages(library(geoR))
suppressMessages(library(MASS))
suppressMessages(library(gtools))
#-------------------------------------------------------------------------------
Model &lt;- ""choosing ARIMA""
Series.title &lt;- ""EMEA GAM&lt;250K""
#-------------------------------------------------------------------------------
Input.data &lt;- matrix(c(""08Q1"",""08Q2"",""08Q3"",""08Q4"",""09Q1"",""09Q2"",""09Q3"",""09Q4"",""10Q1"",""10Q2"",""10Q3"",""10Q4"",""11Q1"",""11Q2"",""11Q3"",""11Q4"",""12Q1"",""12Q2"",""12Q3"",""12Q4"",""13Q1"",""13Q2"",""13Q3"",""13Q4"",""14Q1"",""14Q2"",""14Q3"",5403.675741,6773.504993,7231.117289,7835.55156,5236.709983,5526.619467,6555.781711,11464.72728,7210.068674,7501.610403,8670.903486,10872.93518,8209.022658,8153.393088,10196.44775,13244.50201,8356.732878,10188.44157,10601.32205,12617.82102,11786.52641,10044.98676,11006.0051,15101.9456,10992.27282,11421.18922,10731.31198),ncol=2,byrow=FALSE)

#-------------------------------------------------------------------------------
# The frequency of the data. 1/4 for QUARTERLY, 1/12 for MONTHLY

Frequency &lt;- 1/4

#-------------------------------------------------------------------------------
# How many quarters/months to forecast

Forecast.horizon &lt;- 4

#-------------------------------------------------------------------------------
# The first date in the series. Use c(8, 1) to denote 2008 q1

Start.date &lt;- c(8, 1)

#-------------------------------------------------------------------------------
# The dates of the forecasts

Forecast.dates &lt;- c(""14Q4"", ""15Q1"", ""15Q2"", ""15Q3"")

#-------------------------------------------------------------------------------
# Selects the data column from Input.data

Data.col &lt;- as.numeric(Input.data[, 2])

#-------------------------------------------------------------------------------
# Turns the Data.col into a time-series

Data.col.ts &lt;- ts(Data.col, deltat=Frequency, start = Start.date)

#-------------------------------------------------------------------------------
# A character vector of the dates from Input.data

Dates.col &lt;- as.character(Input.data[,1])

#------- Transform ------------------------------------------------------------------------
# Starts the testing to see if the data should be logged

transform.method &lt;- round(BoxCox.lambda(Data.col.ts, method = ""loglik""), 5)

log.values &lt;- seq(0, 0.24999, by = 0.00001)
sqrt.values &lt;- seq(0.25, 0.74999, by = 0.00001)

which.transform.log &lt;- transform.method %in% log.values
which.transform.sqrt &lt;- transform.method %in% sqrt.values

if (which.transform.log == ""TRUE""){
  as.log &lt;- ""log""
  Data.new &lt;- log(Data.col.ts)
} else {
  if (which.transform.sqrt == ""TRUE""){
    as.log &lt;- ""sqrt""
    Data.new &lt;- sqrt(Data.col.ts)
  } else {
    as.log &lt;- ""no""
    Data.new &lt;- Data.col.ts
  }
}

#----- Find best ARIMA model ---------------------------------------------------

a &lt;- permutations(n = 3, r = 6, v = c(0:2), repeats.allowed = TRUE)
a &lt;- a[ifelse((a[, 1] + a[, 4] &gt; 2 | a[, 2] + a[, 5] &gt; 2 | a[, 3] + a[, 6] &gt; 2),
              FALSE, TRUE), ]

Arimafit &lt;- matrix(0,
                   ncol  = length(Data.new),
                   nrow  = length(a[, 1]),
                   byrow = TRUE)

totb &lt;- matrix(0, ncol = 1, nrow = length(a[, 1]))
arimaerror &lt;- matrix(0, ncol = length(Data.new), nrow = 1)

for (i in 1:length(a[, 1])){
  ArimaData.new &lt;- try(Arima(Data.new,
                             order    = a[i, c(1:3)],
                             seasonal = list(order = a[i, c(4:6)]),
                             method   = ""ML""),
                       silent = TRUE)

  if (is(ArimaData.new, ""try-error"")){
    ArimaData.new &lt;- arimaerror
  } else {
    ArimaData.new &lt;- ArimaData.new
  }

  arimafitted &lt;- try(fitted(ArimaData.new), silent = TRUE)

  if (is(arimafitted, ""try-error"")){
    fitarima &lt;- arimaerror
  } else {
    fitarima &lt;- arimafitted
  }

  if (as.log == ""log""){
    Arimafit[i, ] &lt;- c(exp(fitarima))
    Datanew &lt;- c(exp(Data.new))
  } else {
    if (as.log == ""sqrt""){
      Arimafit[i, ] &lt;- c((fitarima)^2)
      Datanew &lt;- c((Data.new)^2)
    } else {
      Arimafit[i, ] &lt;- c(fitarima)
      Datanew &lt;- c(Data.new)
    }
  }

  data &lt;- c(Datanew)

  arima.fits &lt;- c(Arimafit[i, ])

  fullres &lt;- data - arima.fits

  v &lt;- acf(fullres, plot = FALSE)

  w &lt;- pacf(fullres, plot = FALSE)

  if (v$acf[2]&gt;0.4|v$acf[2]&lt;(-0.4)|v$acf[3]&gt;0.4|v$acf[3]&lt;(-0.4)|v$acf[4]&gt;0.4|v$acf[4]&lt;(-0.4)|v$acf[5]&gt;0.4|v$acf[5]&lt;(-0.4)|v$acf[6]&gt;0.4|v$acf[6]&lt;(-0.4)|v$acf[7]&gt;0.4|v$acf[7]&lt;(-0.4)|w$acf[1]&gt;0.4|w$acf[1]&lt;(-0.4)|w$acf[2]&gt;0.4|w$acf[2]&lt;(-0.4)|w$acf[3]&gt;0.4|w$acf[3]&lt;(-0.4)|w$acf[4]&gt;0.4|w$acf[4]&lt;(-0.4)|w$acf[5]&gt;0.4|w$acf[5]&lt;(-0.4)|w$acf[6]&gt;0.4|w$acf[6]&lt;(-0.4)){
    totb[i] &lt;- ""n""
  } else {
    totb[i] &lt;- sum(abs(w$acf[1:4]))
  }

  j &lt;- match(min(totb), totb)

  order.arima &lt;- a[j, c(1:3)]

  order.seasonal.arima &lt;- a[j, c(4:6)]
}

#----- ARIMA -------------------------------------------------------------------
# Fits an ARIMA model with the orders set
Arima.Data.new &lt;- Arima(Data.new,
                        order    = order.arima,
                        seasonal = list(order=order.seasonal.arima),
                        method   = ""ML"")

#-------------------------------------------------------------------------------
# Forecasts from the ARIMA model

suppressWarnings(forecast.Data.new &lt;- forecast(Arima.Data.new,
                                               h        = ifelse(frequency(Arima.Data.new) &gt; 1, 2 * frequency(Arima.Data.new), 10),
                                               simulate = TRUE,
                                               fan      = TRUE))
</code></pre>
"
"0.191446782789517","0.206038366654562","124388","<p>I have a code which tests each possible order of ARIMA and selects the best model by choosing the one with the absolute minimum sum of lags from the PACF graph. The code then proceeds to add weight to recent errors and runs an optimization on the parameters to get the minimum mean absolute error.</p>

<p>The code runs fine and gives excellent results (e.g 0.2% MAPE etc) however once the parameters have been optimized the ACF and PACf graphs show lags outside the threshold.</p>

<p>I would like to add into my code a loop which does the following:</p>

<p>if any of the first 4 lags of the ACF or PACF graphs of the residuals found from the optimized ARIMA model are outside the threshold (2/sqrt(n)) then the optimization is re-run but doesn't allow those parameters to be selected/those parameters are skipped in the optimization process.</p>

<p>Here is my code:</p>

<pre><code>suppressMessages(library(lmtest))
suppressMessages(library(car))
suppressMessages(library(tseries))
suppressMessages(library(forecast))
suppressMessages(library(TTR))
suppressMessages(library(geoR))
suppressMessages(library(MASS))
suppressMessages(gtools))
#-------------------------------------------------------------------------------
Data.col&lt;-c(5403.676,6773.505, 7231.117, 7835.552, 5236.710, 5526.619, 6555.782,11464.727, 7210.069, 7501.610, 8670.903,10872.935, 8209.023, 8153.393,10196.448,13244.502, 8356.733,10188.442,10601.322,12617.821, 11786.526,10044.987,11006.005,15101.946,10992.273,11421.189,10731.312)
#-------------------------------------------------------------------------------
# Turns the Data.col into a time-series

Data.col.ts &lt;- ts(Data.col, deltat=(1/4), start = c(8,1))

#-------------------------------------------------------------------------------
# Starts the testing to see if the data should be logged

trans&lt;- BoxCox.lambda(Data.col, method = ""loglik"")
categ&lt;-as.character( c(cut(trans,c(0,0.25,0.75,Inf),right=FALSE)) )
Data.new&lt;-switch(categ,
                 ""1""=log(Data.col.ts),
                 ""2""=sqrt(Data.col.ts),
                 ""3""=Data.col.ts
)

#----- Weighting ---------------------------------------------------------------
fweight &lt;- function(x){
  PatX &lt;- 0.5+x 
  return(PatX)
}

#Split the integral to several intervals, and pick the weights accordingly

integvals &lt;- rep(0, length.out = length(Data.new))
for (i in 1:length(Data.new)){
  integi &lt;- integrate(fweight, lower = (i-1)/length(Data.new), upper= i/length(Data.new))
  integvals[i] &lt;- 2*integi$value
}

#----- Find best ARIMA model ---------------------------------------------------

a &lt;- permutations(n = 3, r = 6, v = c(0:2), repeats.allowed = TRUE)
a &lt;- a[ifelse((a[, 1] + a[, 4] &gt; 2 | a[, 2] + a[, 5] &gt; 2 | a[, 3] + a[, 6] &gt; 2),
              FALSE, TRUE), ]

Arimafit &lt;- matrix(0,
                   ncol  = length(Data.new),
                   nrow  = length(a[, 1]),
                   byrow = TRUE)

totb &lt;- matrix(0, ncol = 1, nrow = length(a[, 1]))
arimaerror &lt;- matrix(0, ncol = length(Data.new), nrow = 1)

for (i in 1:length(a[, 1])){
  ArimaData.new &lt;- try(Arima(Data.new,
                             order    = a[i, c(1:3)],
                             seasonal = list(order = a[i, c(4:6)]),
                             method   = ""ML""),
                       silent = TRUE)

  if (is(ArimaData.new, ""try-error"")){
    ArimaData.new &lt;- arimaerror
  } else {
    ArimaData.new &lt;- ArimaData.new
  }

  arimafitted &lt;- try(fitted(ArimaData.new), silent = TRUE)

  if (is(arimafitted, ""try-error"")){
    fitarima &lt;- arimaerror
  } else {
    fitarima &lt;- arimafitted
  }

  if (categ==""1""){
    Arimafit[i, ] &lt;- c(exp(fitarima))
    Datanew &lt;- c(exp(Data.new))
  } else {
    if (categ==""2""){
      Arimafit[i, ] &lt;- c((fitarima)^2)
      Datanew &lt;- c((Data.new)^2)
    } else {
      Arimafit[i, ] &lt;- c(fitarima)
      Datanew &lt;- c(Data.new)
    }
  }

  data &lt;- c(Datanew)

  arima.fits &lt;- c(Arimafit[i, ])

  fullres &lt;- data - arima.fits

  v &lt;- acf(fullres, plot = FALSE)

  w &lt;- pacf(fullres, plot = FALSE)

  if (v$acf[2]&gt;(2/sqrt(length(Data.col)))|v$acf[2]&lt;(-(2/sqrt(length(Data.col))))|v$acf[3]&gt;(2/sqrt(length(Data.col)))|v$acf[3]&lt;(-(2/sqrt(length(Data.col))))|v$acf[4]&gt;(2/sqrt(length(Data.col)))|v$acf[4]&lt;(-(2/sqrt(length(Data.col))))|v$acf[5]&gt;(2/sqrt(length(Data.col)))|v$acf[5]&lt;(-(2/sqrt(length(Data.col))))|v$acf[6]&gt;(2/sqrt(length(Data.col)))|v$acf[6]&lt;(-(2/sqrt(length(Data.col))))|v$acf[7]&gt;(2/sqrt(length(Data.col)))|v$acf[7]&lt;(-(2/sqrt(length(Data.col))))|w$acf[1]&gt;(2/sqrt(length(Data.col)))|w$acf[1]&lt;(-(2/sqrt(length(Data.col))))|w$acf[2]&gt;(2/sqrt(length(Data.col)))|w$acf[2]&lt;(-(2/sqrt(length(Data.col))))|w$acf[3]&gt;(2/sqrt(length(Data.col)))|w$acf[3]&lt;(-(2/sqrt(length(Data.col))))|w$acf[4]&gt;(2/sqrt(length(Data.col)))|w$acf[4]&lt;(-(2/sqrt(length(Data.col))))|w$acf[5]&gt;(2/sqrt(length(Data.col)))|w$acf[5]&lt;(-(2/sqrt(length(Data.col))))|w$acf[6]&gt;(2/sqrt(length(Data.col)))|w$acf[6]&lt;(-(2/sqrt(length(Data.col))))){
    totb[i] &lt;- ""n""
  } else {
    totb[i] &lt;- sum(abs(w$acf[1:4]))
  }

  j &lt;- match(min(totb), totb)

  order.arima &lt;- a[j, c(1:3)]

  order.seasonal.arima &lt;- a[j, c(4:6)]
}

#----- ARIMA -------------------------------------------------------------------
# Fits an ARIMA model with the orders set
stAW &lt;- Arima(Data.new, order= order.arima, seasonal=list(order=order.seasonal.arima), method=""ML"")
parSW &lt;- stAW$coef
    WMAEOPT &lt;- function(parSW)
    {
      ArimaW &lt;- Arima(Data.new, order = order.arima, seasonal=list(order=order.seasonal.arima), 
                      include.drift=FALSE, method = ""ML"", fixed = c(parSW))
      errAR &lt;- c(abs(resid(ArimaW)))
      WMAE &lt;- t(errAR) %*% integvals 
      return(WMAE)
    }
    OPTWMAE &lt;- optim(parSW, WMAEOPT, method=""SANN"", set.seed(2), control = list(fnscale= 1, maxit = 5000))
    # Alternatively, set  method=""Nelder-Mead"" or method=""L-BFGS-B"" 
    parS3 &lt;- OPTWMAE$par
Arima.Data.new &lt;- Arima(Data.new, order = order.arima, seasonal=list(order=order.seasonal.arima), 
                        include.drift=FALSE, method = ""ML"", fixed = c(parS3))
</code></pre>

<p>Before the parameters are optimized it gives a graph like this:
<img src=""http://i.stack.imgur.com/cjY1x.png"" alt=""enter image description here""></p>

<p>After the parameters are optimized it gives a graph like this:
<img src=""http://i.stack.imgur.com/6HKXl.png"" alt=""enter image description here""></p>

<p>I want to stop this happening in the second picture. Is this possible to do using <code>optim</code>?</p>
"
"0.0490880693673816","0.0641500299099584","204440","<p>I'm using the <code>auto.arima</code> function in R's <code>forecast</code> package to build an ARIMA model with external regressors. I have a non-seasonal monthly stationary time-series dataset as shown below:</p>

<pre><code>&gt; dim(tsdata)
[1] 95  4
&gt; head(tsdata)
                    y         x1         x2          x3
2007-02-01  0.0532113 -0.7547812 -1.1156320  1.15193457
2007-03-01 -0.4461565  0.5104070  1.2489777 -1.19172591
2007-04-01 -1.4087036  2.0866994  0.2835917  0.15941672
2007-05-01 -0.4960451 -1.9455242 -2.6847517 -0.06603252
2007-06-01  0.8025322 -2.9295067 -0.6049654  0.34332637
2007-07-01 -0.8053754 -0.2385492 -1.7850528 -1.29843072
</code></pre>

<p>I can use <code>auto.arima(tsdata[,1], xreg=tsdata[,2:4])</code> to fit a model with <code>x1</code>, <code>x2</code>, and <code>x3</code> as regressors. My question is, is there a way to model the interaction between external regressions?</p>
"
"0.0736321040510724","0.0962250448649376","20929","<p>I want to make forecast on my data by running an arimax model.
The data is like:</p>

<pre><code>Value1, Flag1, Flag2, ................., FlagN    
Value2, Flag1, Flag2, ................., FlagN    
Value3, Flag1, Flag2, ................., FlagN    
Value4, Flag1, Flag2, ................., FlagN    
Value5, Flag1, Flag2, ................., FlagN    
....    
ValueM, Flag1, Flag2, ................., FlagN
</code></pre>

<p>So, when I want to make a new forecast, I will provide flag values to the model, then it can give me the forecast value?</p>

<p>How can I prepare the input data?
What is the proper R function and proper way of calling?</p>
"
"0.0850230301897704","0.111111111111111","122803","<p>I would like to conduct a forecast based on a multiple time series ARIMA-model with multiple exogeneous variables. Since I am not that skillfull with regards to neither statistics nor R I want to keep is as simple as possible (Trend forecast for 3 months is sufficient).</p>

<p>I have 1 dependent time series and 3-5 predictor time series, all monthly data, no gaps, same time ""horizon"".</p>

<p>I encountered the auto.arima function and asked myself if this would be a suitable solution for my problem. I have different commodity prices and prices of products made from them. All raw-data are non-stationary but via first-order differencing they all become stationary data. ADF, KPSS indicate this. (This means that I have tested for integration, right?).</p>

<p>My question now is: How do I apply this with the auto.arima function AND is ARIMA the right approach anyways? Some ppl already adviced me to use VAR, but is it possible with ARIMA too?</p>

<p>The following table is my data. Actually the data-set goes up til 105 observations, but the first 50 will do. Trend as well as seasonality are obviously of interest here.</p>

<p><img src=""http://i.stack.imgur.com/8lnMa.png"" alt=""enter image description here""></p>

<p>Thanks for any advices and help!
Georg</p>
"
"0.184080260127681","0.192450089729875","186164","<p>I am new to time series and am trying to fit some time series data.</p>

<p>I understand the general concept of ARIMA model. However, as I read more textbooks and articles from Rob Hyndman, I realized I could put some regressors using the <code>xreg</code> argument for the functions <code>auto.arima</code> or <code>arima</code> in R to get an ARMAX model. Therefore, I wonder if it is still necessary to include seasonality in <code>ts(...,frequency)</code> as everything can be specified as dummy variable within the <code>xreg</code> matrix and a more complicated seasonality structure (e.g. monthly seasonality) can be specified. </p>

<p>In addition, what would be a good way to check the accuracy of the forecast? I am fitting multiple time series data with a hierarchical structure. Using <code>auto.arima</code>, I am able to select the best model and validate the model by looking at the residuals (check whether they are white noise). However, is there a way to even improve on the model if the prediction is still far from the actual data?</p>

<p>To sum up, </p>

<ol>
<li>Is the <code>frequency</code> argument in <code>ts</code> function really necessary?  Can I just specify everything in the <code>xreg</code> matrix?</li>
<li>What would be a normal routine to improve on model after selecting the appropriate ARIMA model with the lowest AIC?</li>
</ol>

<p>Updates (Dec 17):</p>

<p>I am now able to fit an ARIMA model with SARIMA error by specifying <code>xreg</code> argument and <code>seasonal=F</code>. One issue that I have with that is, my <code>xreg</code> matrix is not invertible (I assumed) and its not due to the presence of intercept term. Thus <code>auto.arima()</code> only fit a <code>c(0,0,0)</code> model.</p>

<p>I then tried using <code>Arima()</code> to manually select model and it outputted the following error</p>

<pre><code>Error in optim(init[mask], armafn, method = optim.method, hessian = TRUE,  : 
non-finite value supplied by optim
</code></pre>

<p>I check the <code>xreg</code> matrix and it turns out column 48 (Day) and column 52 (2015) is causing the issue. Could you check if there's something wrong with my <a href=""https://drive.google.com/file/d/0B-b9YsAB5mpnam1oN0hYcFRwLXM/view?usp=sharing"" rel=""nofollow"">matrix structure</a> ? </p>

<p>If you think this additional updates should be asked in stack overflow or additional question, I will move it.</p>
"
"0.0981761387347632","0.128300059819917","121745","<p>I have trouble understanding the output of function <code>arima()</code> in <code>R</code>. </p>

<p>Reading the help file and other sources has not helped much...</p>

<p>Consider an AR(1) model as an example. As I understand it,</p>

<pre><code>m1=arima(x, order=c(1,0,0), include.mean=TRUE)
</code></pre>

<p>estimates a model of the form</p>

<p>$$(x_t-\mu_x)=\varphi_0+\varphi_1(x_{t-1}-\mu_x)+\varepsilon_t$$</p>

<p>where $\mu_x$ is the mean of $x$. Since the population mean is not known, its sample counterpart $\frac{1}{T}\sum_{t=1}^{T} x_t$ is used.</p>

<p>However, I am <strong>unable to reconstruct</strong> $x$ from the fitted values and the residuals of the model, thus I must have misunderstood something. Here is what I do step-by-step:</p>

<pre><code>T=1000; set.seed(1); x=rnorm(T, sd=10)         # generate a random variable x
m1=arima(x, order=c(1,0,0), include.mean=TRUE) # fit an AR(1) model for x
</code></pre>

<p>Obtain the $\varphi_0+\varphi_1(x_{t-1}-\mu_x)$:</p>

<pre><code>fitted=(x-mean(x))*m1$coef[1] + 1*m1$coef[2]   # define fitted values from the AR(1) model
fitted=c(NA,fitted[-T])        # adjust so that fitted[t] corresponds to x[t] for any t
</code></pre>

<p>Obtain the $\varepsilon_t$:</p>

<pre><code>resids=resid(m1)               # residuals from the AR(1) model
resids=c(NA,resids[-1])        # adjust so that resids[t] corresponds to x[t] for any t
</code></pre>

<p>Obtain the $(x_t-\mu_x)$:</p>

<pre><code>true=x-mean(x)
</code></pre>

<p>Plot the $(x_t-\mu_x)$ in black and the $\varphi_0+\varphi_1(x_{t-1}-\mu_x)+\varepsilon_t$ in red:</p>

<pre><code>plot( true[1:20], type=""l"" )
lines( (fitted+resids)[1:20], col=""red"" )
</code></pre>

<p>The two are <strong>not equal!</strong> (Although close, but likely more than purely due to an approximation error.)</p>

<p><strong>Edit:</strong> Perhaps I should use $\frac{\varphi_0}{1-\varphi_1}$ instead of $\mu_x$, i.e. <code>m1$coef[2]/(1-m1$coef[1])</code> instead of <code>mean(x)</code>? I have tried that, too, and it did not help.</p>

<p><strong>Have I misinterpreted the model definition</strong> (see how I define <code>true</code>, <code>fitted</code> and <code>resids</code>)<strong>?</strong></p>

<hr>

<p>By the way, I have no problem with the case of no constant term: <code>arima(..., include.mean=FALSE)</code>:</p>

<pre><code>T=1000; set.seed(1); x=rnorm(T, sd=10)
m1=arima(x, order=c(1,0,0), include.mean=FALSE)
fitted=x[-T]*m1$coef[1]
resids=resid(m1)[-1]
plot(x[(1+1):(20+1)],type=""l"")
lines((fitted+resids)[1:20],col=""red"")
x[(1+1):(20+1)]==(fitted+resids)[1:20]
</code></pre>
"
"0.10976425998969","0.143443827637312","41446","<p>I'm following an   undergraduate course on timeseries using OxMetrics and wanted to reproduce som results in R</p>

<p>Estimating an ARMA(3,3) model:</p>

<pre><code>arima(temp,c(3,0,3))
</code></pre>

<p>using stats package and also TSA package results in following error:</p>

<pre><code>Error in arima(temp, c(3, 0, 3)) : non-stationary AR part from CSS
</code></pre>

<p>Tried to use ML-method:</p>

<pre><code>arima(temp,c(3,0,3),method=""ML"")
</code></pre>

<p>no error so I get estimates, but not the estimates found in OxMetrics.</p>

<p>Then I tried installing the package FitARMA and used:</p>

<pre><code>FitARMA(temp,c(3,0,3),TRUE,FALSE)
</code></pre>

<p>Which results in the same estimates for the coefficients as when estimated in OxMetrics using exact MaxLikelyhood except for the constantterm. </p>

<p>Then I tried changing fourth argument of FitARMA from FALSE to TRUE:</p>

<pre><code>FitARMA(temp,c(3,0,3),TRUE,TRUE)
</code></pre>

<p>Estimating mean by max.likelyhood and then getting the same estimate for the constantterm but now the estimates for the coefficients change to approximately the same as when using arima in R.</p>

<p>I've already consulted four books on R and I can't really figure out why this is happening. Both OxMetrics and R use BFGS for one. Being quite new to timeseries I'm quite lost...</p>
"
"0.260328800437734","0.283505757266572","122270","<p>This is a long post but it is <em>not</em> conceptually difficult. Please bear with me.</p>

<p>I am trying to model the <strong>seasonality</strong> of production volume of an agricultural commodity. I do not care about the structure of the model (as long as it produces sensible output) nor about explaining seasonality; my goal is to adjust the series for seasonality before proceeding to forecasting. </p>

<p>I know the seasonality is due to supply (weather is a key determinant) as well as demand (higher demand around Christmas and Easter, etc.). </p>

<p>My <strong>data is weekly</strong>, and I am following the approach by prof. Hyndman of modelling the time series as an <strong>ARMA process with Fourier terms</strong> to account for seasonality (see <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">here</a>). I also include Christmas and Easter dummies together with Fourier terms. </p>

<p>You might think I do not need a Christmas dummy since Christmas is always the 25th of December and thus Fourier terms would account for it, unlike for Easter which jumps back and forth in March-April from year to year. However, Christmas sometimes occur in week 51 and sometimes in week 52 (at least in my calendar), and Fourier terms will not account for that; hence a Christmas dummy is actually relevant.</p>

<p>Actually, I would like to include up to four dummies for Christmas (from two weeks before Christmas to one week after Christmas) as the seasonal effect is not concentrated on just the Christmas week but also spread out a little in time. The same is with Easter.</p>

<p>Thus I end up with $4+4=8$ dummies, up to 25 pairs of Fourier terms and an unknown ARMA order, which could be as high as (5,5), perhaps. This gives me an <strong>awfully large pool of models to choose from</strong>. Thus I have trouble replicating the approach used in the source above because of <strong>limited computational resources</strong>. Recall that in the original source there were no dummies, so the model pool was $2^{4+4}=256$ times smaller than mine and it was possible to try out a reasonably large proportion of all possible models, and then choose one of them by AIC. </p>

<p>Even worse, I would like to repeat my exercise multiple times (100 times) using a rolling window on the data I have. Thus the <strong>computational requirements</strong> for my exercise <strong>become exorbitant</strong>. I am looking for a solution, and hope someone could help me out.</p>

<p>Here are a few ideas I have:</p>

<ol>
<li><p>Drop the ARMA terms all together. </p>

<ul>
<li>This will make the estimates of coefficients on Fourier terms and dummies biased and inconsistent, thus the seasonal component (the Fourier terms times their coefficients plus dummies times their coefficients) might be rubbish. However, if ARMA patterns are pretty weak, maybe the bias and inconsistency of the coefficient estimates will not be too bad?</li>
</ul></li>
<li><p>Estimate only some models from the pool of all possible models. E.g. select the ARMA order by <code>auto.arima()</code> function in <code>forecast</code> package in <code>R</code>: <code>auto.arima()</code> does not consider all possible ARMA orders but only non-restricted ones (e.g. an AR(2) model with the AR(1) coefficient restricted to zero is not considered) and takes some other shortcuts. Also, I could force all dummies to be present without questioning whether all of them are really relevant. </p>

<ul>
<li>This indeed reduces the model pool considerably (by a factor of $2^{4+4}=256$ only due to dummies), but it comes at a cost of likely inclusion of irrelevant variables. Not sure how bad that could be. </li>
</ul></li>
<li><p>Use a different estimation method: conditional sum of squares (CSS) instead of maximum likelihood (ML) when possible, something else? </p>

<ul>
<li>Unfortunately, CSS in place of ML does not seem to reduce the computational time enough in practice (I tested a few examples).</li>
</ul></li>
<li><p>Do the estimation in two (or more) steps: select the ARMA order independently of the number of Fourier terms and inclusion/exclusion of dummies, then select the number of Fourier terms, then select the number of dummies, or something similar. </p>

<ul>
<li>This would reduce the model pool to the following: the number of all possible ARMA models (e.g. $2^{5+5}$) plus 26 (0 pairs of Fourier terms, 1 pair of Fourier terms, ..., 25 pairs of Fourier terms) plus $2^{4+4}$ (all combinations of dummies). This particular division into stages certainly does not make sense, I admit. But perhaps there exists a smart way to divide model selection into steps? I guess we need mutual orthogonality of regressors to be able to consider their inclusion/exclusion from the model one by one, or something similar. I need some insight here...</li>
</ul></li>
<li><p>Do something along the lines of Cochrane-Orcutt correction: <strong>(1)</strong> estimate a model without any ARMA terms (thus a simple OLS regression of production volume on Fourier terms and dummies); <strong>(2)</strong> examine the ARMA patterns in residuals and determine the relevant ARMA order; <strong>(3)</strong> estimate the corresponding ARMA model with Fourier terms and dummies as exogenous regressors. <br>(I know Cochrane-Orcutt does not consider ARMA(p,q) but just AR(1), but I thought maybe I could generalize it? Not sure, perhaps I am making a mistake here?)</p>

<ul>
<li>This would reduce the model pool to be estimated by a factor of $2^{5+5}$ if we would otherwise consider ARMA(5,5) and all sub-models nested in it, which is very nice. However, Cochrane-Orcutt correction is not as good as getting the model order correct in the first place. (I lack a theoretical argument there, but I suspect you do not always end up with the correct ARMA order if you use Cochrane-Orcutt or something similar.)</li>
</ul></li>
</ol>

<p>I would appreaciate any comments on the ideas above and, even more, your own ideas on how to save computational time.</p>

<p><strong>Thank you!</strong></p>

<p>P.S. Here is the data (2007 week 1 to 2012 week 52): <br><code>6809  8281  8553  8179  8257  7426  8098  8584  8942  9049  9449 10099 10544  6351  8667  9395  9062  8602 10500  8723  9919  9120 10068  9937  9998  9379  9432  8751 10250  7690  6857  8199  8299  9204 11755  9861 10193 10452  9769 10289 10399 10181 10419 10324 11347 11692 12571 13049 13712 15381 13180  3704  5833  9909  8979  8883  9599  9034  9617  9710  9878 10872 12246  7122  8071  9656  9157  9729  8814  8704 10724  9294 10176 10394  9633 10872 10353  9400  9692  9244  8921  7698  8191  8494  8681  9420  8947  9287 10043  9953 10136 10235 10027 10137 11187 10383 11321 12023 11824 13752 13907 14658 14634  5353  5603 11607  9512  9537  9775  8558  9227  9146  9673 10225 10794 10646 10545 12375  7742  9269 11134  9588 10890 10061 10244 11275  9428 10301 11039 10779 10580  9215  9538  9717  9966 10678 10821 10898 11458 11090 12080 10904 12990 12325 12257 12246 12351 12408 11732 12702 14181 14963 15027 16657 17766  8265  7552 11067 10808 10891 10967 10826 10929 10891  9293 10902 11640 11921 13320  7751  9929 11632 11073 10955 11785 10533 10629 11535 12513 11780 12639 12280 12245 11817 11275 10242 10625  9884 10182 11048 12120 12697 13132 12597 13516 13536 13279 13935 13539 14335 13748 13988 14497 15255 15346 17626 17716 10713  7752  9309 11133 11853 11570 10320 10817 10982 10585 10941 12801 12165 11899 10941 12729 13763  8130  9888 11889 13036 10413 12609 10992 13201 11833 13159 12880 12160 13277 12758 10897 11197 12169 12639 13788 13819 14804 14613 14992 16079 16648 16376 16207 15906 15301 16194 17327 18890 18710 19953 22369 16499  9721 11785 12528 13994 12990 13226 12883 13613 14382 15829 15525 16395 17175 18334  9286 15530 16510 15370 13019 16526 14425 16542 15328 16993 17607 16734 15826 16555 16064 14412 14355 15238 15064 15317 15533 16542 16474 17354 18359 18459 17312 17499 18298 17301 17795 17867 18453 19372 19394 21897 23674 23219  7356</code></p>
"
"0.0347105067250312","0.0907218423253029","81632","<p>i want to use an ARIMA model in R for predicting an electrical load on a minutely basis. By examining the ACF I figured out which model could suit. The ACF has shown that the value one day ahead has a periodic autocorrelation. Therefore I'd like to implement a seasonal difference with a lag of 1440 (min/day).</p>

<p>Thus, I found this page (<a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/longseasonality/</a>) describing how to deal with long seasonal periods in R.</p>

<p>However, by applying that method, I experienced the following problem in R:</p>

<pre><code>&gt;Arima(x,order=c(2,0,2),xreg=fourier1(1:length(x),4,1440))

Error in stats::arima(x = x, order = order, seasonal = seasonal, xreg = xreg,  : 
lengths of 'x' and 'xreg' do not match
</code></pre>

<p>x is the dataset as a zoo-Object with the following structure (it's just an example, I do not have access to the real structure at the moment; main difference: much more data!):</p>

<pre><code>&gt; str(x)
â€˜zooâ€™ series from 2010-01-01 00:00:00 to 2010-01-01 00:06:00
Data: num [1:7] 1 2 3 4 5 6 7
Index:  chr [1:7] ""2010-01-01 00:00:00"" ""2010-01-01 00:01:00"" ...
</code></pre>

<p>Since the number of rows in xreg should be exactly the same as in x, they are apparently not.</p>

<p>Does anyone has any suggestions about or experiecend this?</p>

<p>I'll appreciate any hints!</p>

<p>Marc</p>
"
"0.0694210134500623","0.0907218423253029","205967","<p>Suppose I have a website which has some baseline hourly traffic. I also run TV advertising intermittently which drives up my web traffic. I want to determine how much effect my TV advertising is having in terms of driving up web traffic.</p>

<p>If I fit an ARMAX model with hourly TV advertising spend or impressions as exogenous variables, is it valid to claim that the AR terms represent the ""baseline traffic"" while the regression terms represent the traffic that should be attributed to TV advertising?</p>

<p>Here is some example code of what I'm trying to do:</p>

<pre><code>library(forecast)

xmat &lt;- as.matrix(cbind(data[,c(""AdSpend"",""Impressions"")]))
xvar &lt;- data$WebSessions

fit &lt;- Arima(x=xvar, xreg=xmat, order=c(12,0,0), include.constant=FALSE)

reg_terms &lt;- fit$coef[""AdSpend""] * data$AdSpend + fit$coef[""Impressions""] * data$Impressions
AR_terms &lt;- fitted(fit) - reg_terms
</code></pre>

<p>I can then create a stacked area chart using AR_terms (the baseline hourly web traffic) and reg_terms (the TV attributed hourly traffic).</p>

<p><a href=""http://i.stack.imgur.com/PjaLr.png""><img src=""http://i.stack.imgur.com/PjaLr.png"" alt=""enter image description here""></a></p>

<p>Is this a valid approach?</p>

<p>Thanks for the help.</p>
"
"NaN","NaN","187132","<p>I am working on time series data and have both conditional mean and conditional variance in the process. My strategy has so far been to fit a GARCH on the residuals of a fitted ARMA model. </p>

<p>But then I stumbled upon some questions (<a href=""http://stats.stackexchange.com/a/143521/97107"">here</a> is a link to one of them), where it stated that you should do the simultaneous one since it's the most effective and otherwise you'll get inconsistent parameter estimates. This was made by @RichardHardy and seems to be really sound advice. </p>

<p>The question I have is if anyone can confirm this and more preferably post a reference to it. I've been searching for some confirmation for hours with no prevail.</p>
"
"0.269006427118992","0.31752644813856","151657","<p>I am running X-13 SEATS on r for monthly data in six years of observations and I think I got a (sufficiently) reasonable fit for the ARIMA model, but the output also shows me that my original series does not have significant seasonality, as it follows:</p>

<pre><code> Call:
seas(x = data_r[, 1], transform.function = ""log"", regression.aictest = NULL, 
    outlier = NULL, arima.model = ""(0 1 1)(1 1 0)"")

Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
AR-Seasonal-12     -0.6194     0.1110  -5.581 2.39e-08 ***
MA-Nonseasonal-01   0.6220     0.1093   5.690 1.27e-08 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

SEATS adj.  ARIMA: (0 1 1)(1 1 0)  Obs.: 60  Transform: log
AICc: 773.4, BIC: 778.4  QS (no seasonality in final):    0  
Box-Ljung (no autocorr.): 20.04   Shapiro (normality): 0.9754
    &gt; 
                qs p-val
    qsori        0     1
    qsorievadj   0     1
    qsrsd        0     1
    qssadj       0     1
    qssadjevadj  0     1
    qsirr        0     1
    qsirrevadj   0     1
</code></pre>

<p>(Still, there is also the fact that the irregular component seems to dominate the SI ratio for some specific months in some years. So maybe there is some dummy variable in the pre-adjustment that I am missing (right?)) </p>

<p>But when I run a regression on Stata for yearly and monthly dummies on the original series -- assuming the seasonality is deterministic --, I cannot reject with an F test that they are all equal to zero. What does this show me? That my ARIMA fit is not correct?</p>

<p>Also, if someone could point me out the difference in interpretation that you should have when running a regression on seasonal dummies and deseasonalizing data with a X-13 SEATS, it would be also very helpful. Maybe that is what I am missing here.</p>

<p>Edit: is it by any chance a common practice, in some particular situations (when you are deseasonalizing a set of series), still deseasonalize a given series even if that series does not show significant seasonality?</p>

<p>Edit2: Adding the results of the automatic adjustment:</p>

<pre><code>Coefficients:
                   Estimate Std. Error z value Pr(&gt;|z|)    
Constant            59.1761    38.0551   1.555  0.11994    
Easter[15]        -903.6151   341.1891  -2.648  0.00809 ** 
MA-Nonseasonal-01    0.4974     0.1138   4.370 1.24e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

SEATS adj.  ARIMA: (0 1 1)  Obs.: 60  Transform: none
AICc: 925.6, BIC: 933.2  QS (no seasonality in final):    0  
Box-Ljung (no autocorr.):  21.9   Shapiro (normality): 0.9498 *

            qs p-val
qsori        0     1
qsorievadj   0     1
qsrsd        0     1
qssadj       0     1
qssadjevadj  0     1
qsirr        0     1
qsirrevadj   0     1 
</code></pre>

<p>I also, I get the following error for the monthplot function with the automatic adjustment: </p>

<pre><code>Error in `[.default`(x$data, , ""seasonal"") : subscript out of bounds
</code></pre>

<p>Following this result from the automatic adjustment, the use of the dummy for easter, with the original specification, does not change that much the first output:</p>

<pre><code>Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
Easter[15]        -0.08307    0.02690  -3.088  0.00202 ** 
AR-Seasonal-12    -0.63353    0.10816  -5.858  4.7e-09 ***
MA-Nonseasonal-01  0.50391    0.12075   4.173  3.0e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

SEATS adj.  ARIMA: (0 1 1)(1 1 0)  Obs.: 60  Transform: log
AICc: 767.9, BIC: 774.3  QS (no seasonality in final):    0  
Box-Ljung (no autocorr.): 29.37   Shapiro (normality): 0.9721  
            qs p-val
qsori        0     1
qsorievadj   0     1
qsrsd        0     1
qssadj       0     1
qssadjevadj  0     1
qsirr        0     1
qsirrevadj   0     1
</code></pre>

<p>Most recent observation: Now I Think I am fairly sure that there is no significant seasonality in this series, but I would be thankful if someone could show me other problems that I might not be considering. Still, I would like a possible canonical/scholarly answer on why I can reject the null hypothesis for the whole set of seasonal dummies being zero (though I had a small result for the F test with my data, ~4, but I still reject the null) and still get a reasonable ARIMA fit with which I cannot reject no seasonality in my original data. Does that have something to do with the difference of the adjustment with ARIMA models and deterministic seasonality? An intuitive answer on this difference would be of some help.</p>
"
"0.0694210134500623","0.0907218423253029","187250","<p>I want to simulate a time series in R, following an ARMA(1,0) model in the form $Y_t = Y_{t-1} + \epsilon_t$, shocking it at time 20.
In a few words, I therefore have to input $\epsilon_{20} = 30$ (the shock magnitude).</p>

<p>Now, I am using the <code>arima.sim</code> function as it is the one I'm familiar with for simulating a time series, but I am not sure on how to implement a shock into it.</p>

<p>Let's start with a standard simulation, based on 250 observations:</p>

<pre><code>shocksim &lt;- arima.sim(n=250, list(ar = c(0.5)))
</code></pre>

<p>How can I input the shock in such a simulation?</p>
"
"0.138842026900125","0.181443684650606","187304","<p><code>auto.arima</code> returns two different models weather I define my time series with <code>frequency=1</code> (default value) or <code>frequency=7</code> which is the seasonality period for my data (weekly data).</p>

<p>Below the forecasted values when <code>frequency</code> parameter is set to 1.</p>

<p><a href=""http://i.stack.imgur.com/B3uAo.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/B3uAo.png"" alt=""Forecast results with time series frequency set to 1""></a></p>

<p>And now the forecasted values when <code>frequency</code> parameter is set to 7 (which is the seasonality of the data).</p>

<p><a href=""http://i.stack.imgur.com/3bhOR.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/3bhOR.png"" alt=""Forecast results with time series frequency set to 7""></a></p>

<p>From the help page (<code>?ts</code>) I can see that the <code>frequency</code> parameter sets <b>""the number of observations per unit of time""</b>. </p>

<p>I donâ€™t quite understand what does ""unit of time"" refers to. Since my time series consists of daily data, my first guess would be that â€œthe number of observations per unit of timeâ€ would be 1 since I collect one observation every day. On the other hand, when I define parameter <code>frequency=7</code>, I get better forecasts from <code>auto.arima</code>.</p>

<p>My question is:</p>

<ol>
<li>What is the true meanning of the <code>frequency</code> parameter? Is it the seasonality period?</li>
</ol>
"
"0.12024072240843","0.157134840263677","205897","<p>I modeled a univariate time series in R using the <code>Arima</code> command. One can obtain fitted values for the original series using this command by applying the function <code>fitted</code> to the model. However, I noticed that the fitted data has the same dimension as the original data. Hence, a fitted value for the first value in the time series was computed even though there is no past data. I checked wheter it is the mean of the series or the intercept of the model but that isn't the solution. What are possible approaches to get a fitted value here?</p>

<p>The code below is a reproducible example using the <code>Lynx</code> data set.</p>

<pre><code>&gt; library(xts)
&gt; library(forecast)
&gt; 
&gt; data(""lynx"")
&gt; 
&gt; Y      &lt;- as.xts(log10(lynx))
&gt; model  &lt;- Arima(Y, order= c(1, 0, 0))  #Fit AR(1) model
&gt; fit    &lt;- fitted(model)
&gt; length(fit) == length(Y)  
[1] TRUE
</code></pre>
"
"NaN","NaN","145168","<p>I have a set of monthly data and detected seasonality. The ACF and PACF is shown below. How can I set c=(p,d,q) for non-seasonal part and c=(P,D,Q) for seasonal part based on the figures.
<img src=""http://i.stack.imgur.com/mb4ut.png"" alt=""enter image description here""></p>
"
"0.0490880693673816","0.0641500299099584","181533","<p>I have a time series (quarterly data) that I will use to predict the upcoming 4 quarters.The total number of observations is 20 quarters, thus, I need to predict quarter 21 -> 24.
First I took the diff(data) to have a stationary data and I want to fit AR(1). I am using the following in R:</p>

<p><code>arima(diff(data), order=c(1,0,0))</code> and I obtained: ar1 (- 0.2441) and Intercept (1.2004) </p>

<p>Is the following correct?</p>

<pre><code>âˆ†y(t+1) = 1.2004 - 0.2441*âˆ†y(t)

âˆ†y(t+2) = 1.2004 - 0.2441*âˆ†y(t+1)
</code></pre>

<p>If I want to predict y(t+1), do I find âˆ†y(t+1) and then</p>

<pre><code>y(t+1) = y(t) + âˆ†y(t+1)

y(t+2) = y(t) + âˆ†y(t+1)+ âˆ†y(t+2)
</code></pre>

<p>and so on... until y(t+4)</p>

<p>Is this analogy correct? Do you know how can I get the predicted value in R without doing it manually? How can I tell R that first I need to predict the delta and then the original value.</p>
"
"0.0981761387347632","0.128300059819917","25780","<p>I am really new with R and time series. But, I have understood most concept. Part where I am (very) confused is the <code>xreg=</code> argument in <code>arima()</code> from <a href=""http://cran.r-project.org/web/packages/tseries/index.html"" rel=""nofollow"">tseries</a> package and <a href=""http://cran.r-project.org/web/packages/forecast/index.html"" rel=""nofollow"">forecast</a> package. As I have read most of the related threads on this site, I understand that <code>xreg</code> is used for exogenous data. See for example, <a href=""http://stats.stackexchange.com/questions/18375/how-to-fit-an-arimax-model-with-r"">How to fit an ARIMAX-model with R?</a>.</p>

<p>Thus, I conclude that I can fit SARIMAX or ARMAX method using <code>arima()</code>. However, I am currently reading Shumwhay's book and his web <a href=""http://www.stat.pitt.edu/stoffer/tsa2/R_time_series_quick_fix.htm"" rel=""nofollow"">tutorial</a>.</p>

<p>If you check the website (close to the bottom of the page), you can found that he said </p>

<blockquote>
  <p><code>xreg</code> in <code>arima()</code> does not fix ARMAX model.</p>
</blockquote>

<p>It is also explained in the <a href=""http://www.stat.pitt.edu/stoffer/tsa2/Rissues.htm"" rel=""nofollow"">R issues</a> on his website. In addition, he proposed fitting ARMAX model in state space model. So, the answers found on this site and the explanation in the website are completely different.</p>

<p>Can anyone explain me why there is such discrepancy? Which one is correct?</p>

<p>If Shumwhay is correct, is there any function/package that can fit ARMAX or SARIMAX model in R?</p>

<p>Sorry if it turns out I only missed some points, but I hope you can point me to the right direction.</p>
"
"0.0490880693673816","0.0641500299099584","228364","<p>On the plot black is the data and red are the fitted values obtained from <code>fitted</code> i.e. one step forecast, I am using 365 days for training and then 3000+ days for testing, I choose value of k using cross-validation on 365 data points. Following is the model I used:</p>

<pre><code>Arima(data, order=c(2,0,2),xreg=forecast::fourier(min_temp_aus,57))
</code></pre>

<p><a href=""http://i.stack.imgur.com/lFhrN.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/lFhrN.jpg"" alt=""enter image description here""></a></p>

<p>How can I improve the fit on both extremes?</p>

<p>PS: Square loss is of 17524 considering I am predicting 3000+ data points. The way I am looking at it is, if I am off by 1 with every prediction still it makes a loss of 3000. I thought it is good, but maybe I am wrong.</p>
"
"0.176989551117627","0.142336136715633","60648","<p>I am trying to forecast electricity consumption in GWh for 2 years ahead (from June 2013 ahead), using R (the forecast package). For that purpose, I tried regression with ARIMA errors. I fitted the model using the <code>auto.arima</code> function, and I used the following variables in the <code>xreg</code> argument in the <code>forecast.Arima</code> function: </p>

<p>- Heating and Cooling Degree Days,<br>
- Dummies for all 12 months and<br>
- Moving holidays dummies (Easter and Ramadan)  </p>

<p>I have several questions regarding the model:</p>

<p>1) Is it correct to use all 12 dummies for monthly seasonality, since when I tried to include 11, the function returned error. The <code>Auto.arima</code> function returned the model ARIMA(0,1,2)</p>

<p>2)The model returned the following coefficients (I won't specify all of them as there are too many coefficients):</p>

<pre><code>ma1      ma2     HDD     CDD   January  February  March     April
-0.52 -0.16      0.27    0.12  525.84   475.13    472.57    399.01
</code></pre>

<p>I am trying to determine the influence of the temperature component over electricity load. In percentages, (interpreting the coefficients just as with the usual regression) the temperature components (<code>HDD</code>+<code>CDD</code>) account for 11,3% of the electricity consumption. Isn't this too little, considering the fact that the electricity consumption is mostly influenced by the weather component? On the other hand, taking look at the dummies' coefficients, it turns out that the seasonality accounts for the greater part of the load. Why is this? Is the model completely incorrect?</p>

<p>I tried linear regression, and the temperature component accounts for 20%, but it is still a low percentage. Why is this?</p>

<p>3) I am obviously making some mistakes in the use of <code>forecast.Arima</code> or the plot function parameters since when I plot the forecasts, I get a picture of the original time series which is continued (merged) with the forecasts for the whole time series period (from 2004 until 2015). I don't know how to explain this better, I tried to paste the picture, but it seems I cannot paste pictures here.</p>
"
"0.0981761387347632","0.0962250448649376","109826","<p>I am trying to quantify the effect of a future random shocks on my seasonal ARIMA model. If I have understood the theory correctly, the easiest way is to express my seasonal ARIMA model in its ""random shock"" form, and calculate the corresponding psi weights.</p>

<p>Is there a way to do this in R? There is ARMAtoMA, but I think this only works for ARMA models, and not seasonal ARIMA models.</p>

<p>Thank you for your help.</p>

<hr>

<p>UPDATE: Apologies, I'll post the question about R onto stack overflow. It would be good to get confirmation that this is the correct method to quantify the effect of future random shocks to a seasonal ARIMA model.</p>
"
"0.0694210134500623","0.0907218423253029","60751","<p>I need to estimate specific lag ARMA model. Here is an example.</p>

<pre><code>ts1 &lt;- arima.sim(list(order = c(0, 0, 5), ma = c(0, 0.7, 0, 0, 0.1)), n = 1000)
</code></pre>

<p>The above model is $X_t = Z_t + 0.7 Z_{t-2} +0.1 Z_{t-5}$, where $\{ Z_t\}$ is white noise.</p>

<p>Is it possible to estimate this model in R? I tried <code>arima</code> function and I haven't found the option to specify the specific lag.</p>

<p>If it is not possible in R, is there any specific reason?</p>
"
"0.219528519979381","0.215165741455968","168655","<p>I have got monthly data from 1993 to 2015 and would like to do forecasting on these data. I used tsoutliers package to detect the outliers, but I do not know how do I continue to forecast with my set of data .</p>

<p>This is my code:</p>

<pre><code>product.outlier&lt;-tso(product,types=c(""AO"",""LS"",""TC""))
plot(product.outlier)
</code></pre>

<p>This is my output from tsoutliers package</p>

<pre><code>ARIMA(0,1,0)(0,0,1)[12]                    

Coefficients:
        sma1    LS46    LS51    LS61    TC133   LS181   AO183   AO184   LS185   TC186    TC193    TC200
      0.1700  0.4316  0.6166  0.5793  -0.5127  0.5422  0.5138  0.9264  3.0762  0.5688  -0.4775  -0.4386
s.e.  0.0768  0.1109  0.1105  0.1106   0.1021  0.1120  0.1119  0.1567  0.1918  0.1037   0.1033   0.1040
       LS207    AO237    TC248    AO260    AO266
      0.4228  -0.3815  -0.4082  -0.4830  -0.5183
s.e.  0.1129   0.0782   0.1030   0.0801   0.0805

sigma^2 estimated as 0.01258:  log likelihood=205.91
AIC=-375.83   AICc=-373.08   BIC=-311.19

 Outliers:
    type ind    time coefhat  tstat
1    LS  46 1996:10  0.4316  3.891
2    LS  51 1997:03  0.6166  5.579
3    LS  61 1998:01  0.5793  5.236
4    TC 133 2004:01 -0.5127 -5.019
5    LS 181 2008:01  0.5422  4.841 
6    AO 183 2008:03  0.5138  4.592
7    AO 184 2008:04  0.9264  5.911
8    LS 185 2008:05  3.0762 16.038
9    TC 186 2008:06  0.5688  5.483
10   TC 193 2009:01 -0.4775 -4.624
11   TC 200 2009:08 -0.4386 -4.217
12   LS 207 2010:03  0.4228  3.746
13   AO 237 2012:09 -0.3815 -4.877
14   TC 248 2013:08 -0.4082 -3.965
15   AO 260 2014:08 -0.4830 -6.027
16   AO 266 2015:02 -0.5183 -6.442
</code></pre>

<p><a href=""http://i.stack.imgur.com/qKI4N.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qKI4N.jpg"" alt=""This is my plot""></a></p>

<p>I have these warning messages as well.</p>

<pre><code>Warning messages:
1: In locate.outliers.iloop(resid = resid, pars = pars, cval = cval,  :
  stopped when â€˜maxitâ€™ was reached
2: In locate.outliers.iloop(resid = resid, pars = pars, cval = cval,  :
  stopped when â€˜maxitâ€™ was reached
3: In locate.outliers.oloop(y = y, fit = fit, types = types, cval = cval,  :
  stopped when â€˜maxitâ€™ was reached
4: In arima(x, order = c(1, d, 0), xreg = xreg) :
  possible convergence problem: optim gave code = 1
5: In auto.arima(x = c(5.77, 5.79, 5.79, 5.79, 5.79, 5.79, 5.78, 5.78,  :
  Unable to fit final model using maximum likelihood. AIC value approximated
</code></pre>

<p><strong>Doubts:</strong></p>

<ol>
<li>If I am not wrong, tsoutliers package will remove the outliers it detect and through the use of the dataset with outliers removed, it
will give us the best arima model suited for the data set, is it
correct?</li>
<li>The adjust series data set is being shifted down by a lot due to remove of the level shift,etc. Doesn't this mean that if the forecasting is done on the adjusted series, the output of the forecast will be very inaccurate, since the more recent data are already more than 12, while adjusted data shift it to around 7-8.</li>
<li>What does warning message 4 and 5 means? Does it mean it cannot do auto.arima using the adjusted series?</li>
<li>What does the [12] in ARIMA(0,1,0)(0,0,1)[12] mean? Is it just my frequency/periodicity of my dataset, which I set it to monthly? And does this also means that my data series is seasonal as well? </li>
<li>How do I detect seasonality in my data set? As from the visualisation of the time series plot, I cant see any obvious trend, and if I use the decompose function, it will assume that there is a seasonal trend? So do I just believe what the tsoutliers tell me, where there is seasonal trend, since there is MA of order 1?</li>
<li>How do I continue to do my forecasting with this data after identifying these outliers?</li>
<li><strong>How to incorporate these outliers to other forecasting models - Exponential Smoothing, ARIMA, Strutural Model, Random Walk, theta? I am sure I cannot remove the outliers since there are level shift, and if I only take adjusted series data, the values will be too small, so what do I do?</strong></li>
</ol>

<p><strong>Do I need to add these outliers as regressor in the auto.arima for forecasting? How does this work then?</strong></p>
"
"0.0490880693673816","0.0641500299099584","56428","<p>I have a time series data set.  I can decompose it and get the trend but I would like to put confidence ranges around the trend (past) not the forecast-ed component.  The decompose function also doesn't handle N/As very well so is there another way to define the trend with data that has N/As.  I have been trying to use Holt-Winters and ARIMA but neither seem to be able to do this.</p>"
"NaN","NaN","","<r><time-series><arima>"
"0.0694210134500623","0.0907218423253029","109594","<p>I can't seem to find much info on the following: I have a dataset <em>D</em> at time <em>t</em> which I use to fit an ARIMA model. I forecast the value of the time series at time <em>t</em> + 1. Now, when I'm in <em>t</em> + 1,  I would like to predict the value of my time series at <em>t</em> + 2 using data up until time <em>t</em> + 1. However, I don't want to refit the whole model. Is there a function in R which does this? I would have thought this is a common thing to do with time series analysis. </p>
"
"NaN","NaN","104324","<p>Is there a way to use strucchange package in R on ARIMA models?"
"NaN","NaN","I haven't been able to find any.",""
"NaN","NaN","Thanks a lot.</p>",""
"NaN","NaN","","<r><time-series><arima><arma><structural-change>"
"0.155230105141267","0.202860206483395","153204","<p>I have a time series for sales data on a weekly and monthly basis.  I tried using <code>holt.winter</code> and <code>auto.arima</code>. <code>holt.winter</code> can work only on monthly data (freq = 12 &lt; 24), and gives good results, but <code>auto.arima</code> gives very bad results on both monthly and weekly data, just a straight line in the following figures:</p>

<p><img src=""http://i.stack.imgur.com/zNZ9i.png"" alt=""arima monthly"">  </p>

<p><img src=""http://i.stack.imgur.com/b6W12.png"" alt=""enter image description here""></p>

<p>I have the following questions:  </p>

<ol>
<li>Can anyone provide some theoretical basis on why ARIMA performs poorly and why HW performs better?  </li>
<li>Also what model should I use for relatively high frequency data (weekly or daily)?  </li>
</ol>

<p>Also if someone can guide me to advanced books in that area (I have done some reading in Brockwell, 2002, <em>Introduction to Time Series</em>).</p>

<p>[Update]</p>

<p>I tried holt-winter . auto.arima . arima and got the following results</p>

<pre><code>ARIMA (1,0,1)(1,1,1) : sigma^2 estimated as 94587:  log likelihood = -266.51,  aic = 543.02

AUTO.ARIMA =&gt;ARIMA(0,0,0) with non-zero mean : sigma^2 estimated as 141005:  log likelihood=-352.67
AIC=709.33   AICc=709.6   BIC=713.07

Holt-Winter : ETS(A,A,A) 
  Smoothing parameters:
    alpha = 0.0298 
    beta  = 1e-04 
    gamma = 0.0133 
sigma:  306.1749
sigma^2 : 93743.06939001
     AIC     AICc      BIC 
767.3367 784.8851 797.2759 
</code></pre>

<p>it seems arima(1,0,1)(1,1,1) gives better AIC and log-likelihood that result of auto arima , also HW detects seasonality , is that auto.arima stucking at some local optima</p>
"
"0.162806707774543","0.212761579501408","56374","<p>as I am stepping into forecasting with ARIMA models, I am trying to understand how I can improve a forecast based on ARIMA fit with seasonality and drift. </p>

<p>My data is the following time series ( over 3 years, with clear trend upwards and visible seasonality, which seems to be not supported by autocorrelation at lags 12, 24, 36??). </p>

<pre><code>    &gt; bal2sum3years.ts
             Jan     Feb     Mar     Apr     May     Jun     Jul     Aug          
    2010 2540346 2139440 2218652 2176167 2287778 1861061 2000102 2560729 
    2011 3119573 2704986 2594432 2362869 2509506 2434504 2680088 2689888 
    2012 3619060 3204588 2800260 2973428 2737696 2744716 3043868 2867416 
             Sep     Oct     Nov     Dec
    2010 2232261 2394644 2468479 2816287
    2011 2480940 2699780 2760268 3206372
    2012 2951516 3119176 3032960 3738256
</code></pre>

<p>The model that was suggested by <code>auto.arima(bal2sum3years.ts)</code> gave me the following model:</p>

<pre><code>    Series: bal2sum3years.ts 
    ARIMA(0,0,0)(0,1,0)[12] with drift         

    Coefficients:
              drift
          31725.567
    s.e.   2651.693

    sigma^2 estimated as 2.43e+10:  log likelihood=-321.02
    AIC=646.04   AICc=646.61   BIC=648.39
</code></pre>

<p>However, the <code>acf(bal2sum3years.ts,max.lag=35)</code> does not show acf coefficients higher than 0.3. The seasonality of the data is, however, pretty obvious - spike at the beginning of every year. This is what the series looks like on the graph:
<img src=""http://i.stack.imgur.com/kQi5N.png"" alt=""Original Time Series""></p>

<p>The forecast using <code>fit=Arima(bal2sum3years.ts,seasonal=list(order=c(0,1,0),period=12),include.drift=TRUE)</code> , called by function <code>forecast(fit)</code>, results in the next 12months's means being equal to the last 12 months of the data plus constant. This can be seen by calling <code>plot(forecast(fit))</code>, </p>

<p><img src=""http://i.stack.imgur.com/GJqcG.png"" alt=""Actual and Forecasted Data""></p>

<p>I have also checked the residuals, which are not autocorrelated but have positive mean ( non zero). </p>

<p>The fit does not model the original time series precisely, in my opinion ( blue the original time series, red is the <code>fitted(fit)</code>:</p>

<p><img src=""http://i.stack.imgur.com/ux3i7.png"" alt=""Original vs fit""></p>

<p>The guestion is, is the model incorrect? Am I missing something? How can I improve the model? It seems that the model literally takes the last 12 months and adds a constant to achieve the next 12 months. </p>

<p>I am a relative beginner in time series forecasting models and statistics. </p>

<p>Thank you very much for your answers!</p>
"
"0.138842026900125","0.181443684650606","169299","<p>I use auto.arima function in R to fit a TS model to a annual data composed of electricity demand. The series is transformed w.r.t Box-Cox lambda due to the prevailing heteroscedasticity and then it is twiced differenced to eliminate the trend in the data. The first ACF/PACF plot (w/o transformation) suggest that an ARIMA model should be fitted to the model; whereas the second ACF/PACF (with transformation) plot suggests that an AR model should be fitted. However both of them depend on the same data. 
In both of the case, the auto.arima function selects the best model as ARIMA(1,2,1) which can be expected according to the first plot but not according to the second plot due to the one spike in PACF.     </p>

<p><a href=""http://i.stack.imgur.com/qoYJH.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qoYJH.jpg"" alt=""untransformed""></a>
<a href=""http://i.stack.imgur.com/FOzNv.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FOzNv.jpg"" alt=""transformed""></a></p>

<pre><code>dmnd=c(8.6,9.8,11.2,12.4,13.5,15.7,18.6,21.1,22.3,23.6,24.6,26.3,28.3,29.6,33.3,36.4,40.5,44.9,48.4,52.6,56.8,60.5,67.2,73.4,77.8,85.6,94.8,105.5,
</code></pre>

<p>114.0,118.5,128.3,126.9,132.6,141.2,150.0,160.8,174.6,190.0,198.1,194.1,210.4,230.3,242.4,246.4,257.2)</p>

<pre><code>x=ts(dmnd,frequency=1)

sdx=diff(x,differences = 2)
x1&lt;-acf(sdx,length(sdx),ylab=""Sample ACF"",main ="""")
x2&lt;-pacf(sdx,length(sdx),ylab=""Sample PACF"",main ="""")

library(FitAR)

#transformation
fit=arima(x,order=c(0,2,0))
BoxCox(fit, interval = c(-1, 1), type = ""BoxCox"")

library(forecast)
tx=BoxCox(x, -0.049)

sdx=diff(tx,differences = 2)
x1&lt;-acf(sdx,length(sdx),ylab=""Sample ACF"",main ="""")
x2&lt;-pacf(sdx,length(sdx),ylab=""Sample PACF"",main ="""")


fit=auto.arima(x,d = 2,D = 0,start.p=0, start.q=0, max.p=5, max.q=5,stationary=FALSE,seasonal=FALSE,stepwise=TRUE,trace=TRUE,approximation=FALSE,allowdrift=TRUE,ic=""aicc"",lambda=-0.049)

par(mfrow=c(1,2)) 
x1&lt;-acf(fit$residuals,length(fit$residuals),ylab=""Sample ACF"",main ="""")
x2&lt;-pacf(fit$residuals,length(fit$residuals),ylab=""Sample PACF"",main ="""")

Box.test(fit$residuals, lag = length(fit$residuals)/5, type = c(""Ljung-Box""), fitdf = length(fit$ coef))

shapiro.test(fit$residuals)

library(TSA)
x.standard=rstandard.Arima(fit)
qqnorm(x.standard,main ="""")
qqline(x.standard)
</code></pre>

<p><strong><em>Results of ARIMA(3,1,0) Model</em></strong> </p>

<p><a href=""http://i.stack.imgur.com/MmgeP.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/MmgeP.jpg"" alt=""Summary""></a></p>

<p><a href=""http://i.stack.imgur.com/ojFY1.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ojFY1.jpg"" alt=""sample ACF/PACf""></a>
<a href=""http://i.stack.imgur.com/Gh94D.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Gh94D.jpg"" alt=""Normality""></a></p>

<pre><code>Shapiro-Wilk normality test

data:  fit$residuals
W = 0.8557, p-value = 5.153e-05

Box-Ljung test

data:  fit$residuals
X-squared = 14.2044, df = 6, p-value = 0.02743
</code></pre>

<p><strong><em>Diagnostics of Residuals ARIMA(1,1,1) transformed</em></strong>
<a href=""http://i.stack.imgur.com/lQGXi.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/lQGXi.jpg"" alt=""ACF/PACF""></a>
<a href=""http://i.stack.imgur.com/twQf2.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/twQf2.jpg"" alt=""Normal""></a></p>

<p><strong><em>auto.arima result for the series without the observations ""32, 40, 41""</em></strong>
<a href=""http://i.stack.imgur.com/9XI2O.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9XI2O.jpg"" alt=""without some observation""></a></p>
"
"0.23541811049065","0.254147912069957","169468","<p>I have monthly time series data, and would like to do forecasting with detection of outliers .</p>

<p><strong>This is the sample of my data set:</strong></p>

<pre><code>       Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov   Dec
2006  7.55  7.63  7.62  7.50  7.47  7.53  7.55  7.47  7.65  7.72  7.78  7.81
2007  7.71  7.67  7.85  7.82  7.91  7.91  8.00  7.82  7.90  7.93  7.99  7.93
2008  8.46  8.48  9.03  9.43 11.58 12.19 12.23 11.98 12.26 12.31 12.13 11.99
2009 11.51 11.75 11.87 11.91 11.87 11.69 11.66 11.23 11.37 11.71 11.88 11.93
2010 11.99 11.84 12.33 12.55 12.58 12.67 12.57 12.35 12.30 12.67 12.71 12.63
2011 12.60 12.41 12.68 12.48 12.50 12.30 12.39 12.16 12.38 12.36 12.52 12.63
</code></pre>

<p>I have referred to <a href=""http://stats.stackexchange.com/questions/140163/timeseries-analysis-procedure-and-methods-using-r?lq=1"">Timeseries analysis procedure and methods using R</a>, to do a series of different model of forecasting, however it does not seems to be accurate. In additional, I am not sure how to incorporate the tsoutliers into it as well.</p>

<p>I have got another post regarding my enquiry of tsoutliers and arima modelling and procedure over <a href=""http://stats.stackexchange.com/questions/168655/how-to-interpret-and-do-forecasting-using-tsoutliers-package-and-auto-arima/168869#168869"">here</a> as well.</p>

<p>So these are my code currently, which is similar to link no.1.</p>

<p><strong>Code:</strong></p>

<pre><code>product&lt;-ts(product, start=c(1993,1),frequency=12)

#Modelling product Retail Price

#Training set
product.mod&lt;-window(product,end=c(2012,12))
#Test set
product.test&lt;-window(product,start=c(2013,1))
#Range of time of test set
period&lt;-(end(product.test)[1]-start(product.test)[1])*12 + #No of month * no. of yr
(end(product.test)[2]-start(product.test)[2]+1) #No of months
#Model using different method
#arima, expo smooth, theta, random walk, structural time series
models&lt;-list(
#arima
product.arima&lt;-forecast(auto.arima(product.mod),h=period),
#exp smoothing
product.ets&lt;-forecast(ets(product.mod),h=period),
#theta
product.tht&lt;-thetaf(product.mod,h=period),
#random walk
product.rwf&lt;-rwf(product.mod,h=period),
#Structts
product.struc&lt;-forecast(StructTS(product.mod),h=period)
)

##Compare the training set forecast with test set
par(mfrow=c(2, 3))
for (f in models){
    plot(f)
    lines(product.test,col='red')
}

##To see its accuracy on its Test set, 
#as training set would be ""accurate"" in the first place
acc.test&lt;-lapply(models, function(f){
    accuracy(f, product.test)[2,]
})
acc.test &lt;- Reduce(rbind, acc.test)
row.names(acc.test)&lt;-c(""arima"",""expsmooth"",""theta"",""randomwalk"",""struc"")
acc.test &lt;- acc.test[order(acc.test[,'MASE']),]

##Look at training set to see if there are overfitting of the forecasting
##on training set
acc.train&lt;-lapply(models, function(f){
    accuracy(f, product.test)[1,]
})
acc.train &lt;- Reduce(rbind, acc.train)
row.names(acc.train)&lt;-c(""arima"",""expsmooth"",""theta"",""randomwalk"",""struc"")
acc.train &lt;- acc.train[order(acc.train[,'MASE']),]

 ##Note that we look at MAE, MAPE or MASE value. The lower the better the fit.
</code></pre>

<p>This is the plot of my different forecasting, which doesn't seem very reliable/accurate, through the comparison of the red""test set"", and blue""forecasted"" set.
<strong>Plot of different forecast</strong>
<a href=""http://i.stack.imgur.com/WZSNq.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/WZSNq.jpg"" alt=""Different forecast""></a></p>

<p><strong>Different accuracy of the respective models of test and training set</strong></p>

<pre><code>Test set
                    ME      RMSE       MAE        MPE     MAPE      MASE      ACF1 Theil's U
theta      -0.07408833 0.2277015 0.1881167 -0.6037191 1.460549 0.2944165 0.1956893 0.8322151
expsmooth  -0.12237967 0.2681452 0.2268248 -0.9823104 1.765287 0.3549976 0.3432275 0.9847223
randomwalk  0.11965517 0.2916008 0.2362069  0.8823040 1.807434 0.3696813 0.4529428 1.0626775
arima      -0.32556886 0.3943527 0.3255689 -2.5326397 2.532640 0.5095394 0.2076844 1.4452932
struc      -0.39735804 0.4573140 0.3973580 -3.0794740 3.079474 0.6218948 0.3841505 1.6767075

Training set
                     ME      RMSE       MAE         MPE     MAPE      MASE    ACF1 Theil's U
theta      2.934494e-02 0.2101747 0.1046614  0.30793753 1.143115 0.1638029  0.2191889194        NA
randomwalk 2.953975e-02 0.2106058 0.1050209  0.31049479 1.146559 0.1643655  0.2190857676        NA
expsmooth  1.277048e-02 0.2037005 0.1078265  0.14375355 1.176651 0.1687565 -0.0007393747        NA
arima      4.001011e-05 0.2006623 0.1079862 -0.03405395 1.192417 0.1690063 -0.0091275716        NA
struc      5.011615e-03 1.0068396 0.5520857  0.18206018 5.989414 0.8640550  0.1499843508        NA
</code></pre>

<p>From the models accuracy, we can see that the most accurate model would be theta model.
I am not sure why is the forecast very inaccurate, and I think that one of the reasons would be that, I did not treat the ""outliers"" in my data set, resulting in a bad forecast for all model.</p>

<p><strong>This is my outliers plot</strong></p>

<p><strong>Outliers Plot</strong>
<a href=""http://i.stack.imgur.com/bZDQv.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bZDQv.jpg"" alt=""Outliers""></a></p>

<p><strong>tsoutliers output</strong></p>

<pre><code>ARIMA(0,1,0)(0,0,1)[12]                    

Coefficients:
        sma1    LS46    LS51    LS61    TC133   LS181   AO183   AO184   LS185   TC186    TC193    TC200
      0.1700  0.4316  0.6166  0.5793  -0.5127  0.5422  0.5138  0.9264  3.0762  0.5688  -0.4775  -0.4386
s.e.  0.0768  0.1109  0.1105  0.1106   0.1021  0.1120  0.1119  0.1567  0.1918  0.1037   0.1033   0.1040
       LS207    AO237    TC248    AO260    AO266
      0.4228  -0.3815  -0.4082  -0.4830  -0.5183
s.e.  0.1129   0.0782   0.1030   0.0801   0.0805

sigma^2 estimated as 0.01258:  log likelihood=205.91
AIC=-375.83   AICc=-373.08   BIC=-311.19

 Outliers:
    type ind    time coefhat  tstat
1    LS  46 1996:10  0.4316  3.891
2    LS  51 1997:03  0.6166  5.579
3    LS  61 1998:01  0.5793  5.236
4    TC 133 2004:01 -0.5127 -5.019
5    LS 181 2008:01  0.5422  4.841 
6    AO 183 2008:03  0.5138  4.592
7    AO 184 2008:04  0.9264  5.911
8    LS 185 2008:05  3.0762 16.038
9    TC 186 2008:06  0.5688  5.483
10   TC 193 2009:01 -0.4775 -4.624
11   TC 200 2009:08 -0.4386 -4.217
12   LS 207 2010:03  0.4228  3.746
13   AO 237 2012:09 -0.3815 -4.877
14   TC 248 2013:08 -0.4082 -3.965
15   AO 260 2014:08 -0.4830 -6.027
16   AO 266 2015:02 -0.5183 -6.442
</code></pre>

<p>I would like to know how can I further ""analyse""/forecast my data, with these relevant data set and detection of outliers, etc.
Please do help me in treatment of my outliers as well to do my forecasting as well . </p>

<p>Lastly, I would like to know how to combined the different model forecasting together, as from what @forecaster had mentioned in link no.1, combining the different model will most likely result in a better forecasting/prediction.</p>

<p><strong>EDITED</strong></p>

<p>I would like to incorporate the outliers in other models are well.</p>

<p>I have tried some codes, eg. </p>

<pre><code>forecast.ets( res$fit ,h=period,xreg=newxreg)
    Error in if (object$components[1] == ""A"" &amp; is.element(object$components[2], : argument is of length zero

forecast.StructTS(res$fit,h=period,xreg=newxreg)
Error in predict.Arima(object, n.ahead = h) : 'xreg' and 'newxreg' have different numbers of columns
</code></pre>

<p>There are some errors produced, and I am unsure about the correct code to incorporate the outliers as regressors.
Furthermore, how do I work with thetaf or rwf, as there are no forecast.theta or forecast.rwf?</p>
"
"0.133205488179172","0.193419617728553","229721","<p>I have 4 years electrical load data. I split the data into 3 years (75%) training data, 1 year for testing (25%). Also I have the temperature data for each day during the previous period. (The link to the dataset: <a href=""https://drive.google.com/open?id=0B08HdcWBksWcTUxqc1ByOW1UVEU"" rel=""nofollow"">here</a>.) </p>

<p>I want to make use of the temperature data to enhance the forecasting using argument <code>xreg</code> in <code>arima</code> function. </p>

<p>Here is my code:</p>

<pre><code>mydata1&lt;-read.csv(""1st pape/kaggle_data.csv"");
mydata&lt;-ts(mydata1[,2],start = c(2004),frequency = 365)

#split the data into trainData and test data
trainData = window(mydata, end=c(2007))
testData = window(mydata, start=c(2007))
temp&lt;-ts(mydata1[,3],start = c(2004),frequency = 365)

#split the temperature into trainData and test data
trainReg = window(temp, end=c(2007))
testReg = window(temp, start=c(2007))
</code></pre>

<p>Apply ARIMA model without using <code>xreg</code>:</p>

<pre><code>mod_arima &lt;- auto.arima(trainData, ic='aicc', stepwise=FALSE)
summary(mod_arima)
Series: trainData 
ARIMA(1,0,3) with non-zero mean 

Coefficients:
         ar1      ma1      ma2      ma3  intercept
      0.9642  -0.2098  -0.2157  -0.1693  24008.122
s.e.  0.0110   0.0322   0.0330   0.0325   1018.007

sigma^2 estimated as 9318421:  log likelihood=-10347.38
AIC=20706.75   AICc=20706.83   BIC=20736.75

Training set error measures:
                   ME     RMSE      MAE       MPE     MAPE      MASE
Training set 6.102332 3045.638 2293.946 -1.519484 9.625694 0.5151126
                    ACF1
Training set 0.004483007

plot(forecast(mod_arima)); lines(testData , col=""red"", start= c(2007,1,1)); 
legend(""topleft"", lty=1,col=c(4,2),legend=c(""forecasted data"",""real data""))

y &lt;- msts(trainData, c(7,365)) # multiseasonal ts
x &lt;- msts(trainReg, c(7,365)) # multiseasonal ts

fit &lt;- auto.arima(y, xreg=(fourier(y, K=c(3,30))))
fit_f &lt;- forecast(fit, xreg= fourier(y, K=c(3,30), 365), 365)
plot(fit_f)
</code></pre>

<p>the red line is the actual data, while the blue is the foretasted data. The left plot is appeared before using fourier function, while the right after using it. </p>

<p><a href=""http://i.stack.imgur.com/QxvKC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/QxvKC.png"" alt=""enter image description here""></a></p>

<p>Apply ARIMA model using <code>xreg</code>:</p>

<pre><code>mod_arima2 &lt;- auto.arima(trainData ,xreg = trainReg, ic='aicc', stepwise=FALSE)
summary(mod_arima2)
Series: trainData 
ARIMA(1,0,3) with non-zero mean 

Coefficients:
         ar1      ma1      ma2      ma3  intercept  trainReg
      0.9709  -0.2403  -0.2108  -0.1609  29984.188  -88.3976
s.e.  0.0094   0.0320   0.0330   0.0321   1468.108   13.1966

sigma^2 estimated as 8955023:  log likelihood=-10325.13
AIC=20664.26   AICc=20664.36   BIC=20699.26

Training set error measures:
                   ME     RMSE      MAE       MPE     MAPE      MASE
Training set 6.030471 2984.292 2267.803 -1.464553 9.529988 0.5092422
                    ACF1
Training set 0.005526977

plot(forecast(mod_arima2,xreg = testReg)); lines(testData , col=""red"", start= c(2007,1,1)); 
legend(""topleft"", lty=1,col=c(4,2), legend=c(""forecasted data"",""real data""))

l = (fourier(y, K=c(3,30)))
z = cbind(l,x)
fit2 &lt;- auto.arima(y, xreg=z)
fit_f2 &lt;- forecast(fit, xreg= z, 365)
plot(fit_f2)
</code></pre>

<p><a href=""http://i.stack.imgur.com/TgJE5.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/TgJE5.png"" alt=""enter image description here""></a>
<strong>Questions</strong>:</p>

<ol>
<li>Did I use <code>xreg</code> correctly?</li>
<li>If yes, why is the summary the same without using <code>xreg</code>?</li>
<li>Why are the forecasts far away from the real data?</li>
</ol>
"
"0.0981761387347632","0.0962250448649376","169564","<p>The <code>arimax</code> function in the <code>TSA</code> package is to my knowledge the only <code>R</code> package that will fit a transfer function for intervention models. It lacks a <a href=""http://stats.stackexchange.com/questions/34106/forecasting-with-arimax-model-including-xtransf"">predict function</a> though which is sometimes needed.</p>

<p>Is the following a work-around for this issue, leveraging the excellent <code>forecast</code> package? Will the predictive intervals be correct? In my example, the std errors are ""close"" for the components.</p>

<ol>
<li>Use the forecast package arima function to determine the pre-intervention noise series and add any outlier adjustment.</li>
<li>Fit the same model in <code>arimax</code> but add the transfer function</li>
<li>Take the fitted values for the transfer function (coefficients from <code>arimax</code>) and add them as xreg in <code>arima</code>. </li>
<li>Forecast with <code>arima</code></li>
</ol>

<blockquote>
<pre><code>library(TSA)
library(forecast)
data(airmiles)
air.m1&lt;-arimax(log(airmiles),order=c(0,0,1),
              xtransf=data.frame(I911=1*(seq(airmiles)==69)),
              transfer=list(c(1,0))
              )
</code></pre>
  
  <p>air.m1</p>
</blockquote>

<p>Output:</p>

<pre><code>Coefficients:
  ma1  intercept  I911-AR1  I911-MA0
0.5197    17.5172    0.5521   -0.4937
s.e.  0.0798     0.0165    0.2273    0.1103

sigma^2 estimated as 0.01223:  log likelihood=88.33
AIC=-168.65   AICc=-168.09   BIC=-155.02
</code></pre>

<p>This is the filter, extended out 5 more periods that the data</p>

<pre><code>tf&lt;-filter(1*(seq(1:(length(airmiles)+5))==69),filter=0.5521330,method='recursive',side=1)*(-0.4936508)
forecast.arima&lt;-Arima(log(airmiles),order=c(0,0,1),xreg=tf[1:(length(tf)-5)])
forecast.arima
</code></pre>

<p>Output:</p>

<pre><code>Coefficients:
         ma1  intercept  tf[1:(length(tf) - 5)]
      0.5197    17.5173                  1.0000
s.e.  0.0792     0.0159                  0.2183

sigma^2 estimated as 0.01223:  log likelihood=88.33
AIC=-168.65   AICc=-168.28   BIC=-157.74
</code></pre>

<p>Then to Predict</p>

<pre><code>predict(forecast.arima,n.ahead = 5, newxreg=tf[114:length(tf)])
</code></pre>
"
"NaN","NaN","88559","<p>I am trying to include a term in an AR(2) model:
$$Y_t=\left( a_0+a_1 \frac{\exp(\beta_t)}{1+\exp(\beta_t)}\right)Y_{t-1}+bY_{t-2}+\delta\epsilon_t$$</p>

<p>Can anyone please help me with this? I don't seem to be able include this in the R code.</p>

<pre><code>data=read.table(""water.txt"",header=T)
n=ncol(data)
koef=matrix(0,n-1,4) 
rownames(koef)=names(data)[-1] 
colnames(koef)=c(""a1"",""a2"",""var.a1"",""var.a2"")
for(i in 2:n) {
      ser=log(data[,i])
      ar2=arima(ser,order=c(2,0,0),xreg=data$year)
          koef[i-1,1]=ar2$coef[1]
      koef[i-1,2]=ar2$coef[2]
          koef[i-1,3]=ar2$var.coef[1,1]
      koef[i-1,4]=ar2$var.coef[2,2]
}

data:
yr  water
1986    0.01
1987    -0.63
1988    -0.14
1989    2.52
1990    1.96
1991    0.6
1992    1.82
1993    1.82
1994    0.89
1995    1.32
1996    -1.43
1997    0.75
1998    0.02
1999    0.69
2000    1.65
2001    -1
2002    0.66
2003    0.09
2004    -0.08
2005    0.4
2006    -0.87
2007    1.2
2008    1.1
2009    0.07
2010    -2.57
</code></pre>
"
"0.100200602007025","0.104756560175785","194756","<p>I've got a question regarding ARIMA modeling. 
I am having a hard time to make to model out the seasonalities of my time series. The pictures below shows my tries in modeling. 
The topic is to forecast sales of a shop. 
It shows both the real Sales Values and the fitted/forecasted values.</p>

<p><a href=""http://i.stack.imgur.com/cOnr9.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/cOnr9.png"" alt=""enter image description here""></a></p>

<p>Below you can see my avalable data. <code>Promo</code> means whether there is a promotion on a certain day in the store.
<code>Holiday1-3</code> are different kind of holidays, like Easter holidays (<code>Holiday2</code>) and Christmas holidays (<code>Holiday1</code>).
The Variables <code>PxH1</code>, <code>PxH2</code>, <code>Pxh3</code> and <code>PxS</code> are made by me and multiplicate the value of <code>Promo</code> with each holiday. A Promo and school holiday at the same time give a higher bump in Sales then just a promo or just a school holiday.
Promo usually is every 2 weeks for a whole week. That's where the 2 weekly bump comes from. The 2 weekly bump and the holiday bumps are already modeled quite nice in my opinion...</p>

<p><a href=""http://i.stack.imgur.com/Fvnri.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Fvnri.png"" alt=""enter image description here""></a></p>

<p>My question is on how to model out the yearly seasonality properly. The sales clearly stay the same from New Year to Easter, grow slowly from Easter until beginning of August, with August being the peak of sales, and then continue to fall from August to New Year.</p>

<p>If you can help me with this topic with something different than SPSS Modeler, like R, then feel free to answer in R.
If you need additional information please feel free to tell me!</p>

<p>Thank you a lot!</p>
"
"0.0694210134500623","0.0907218423253029","6958","<p>I have fitted the ARIMA models to the original time series, and the best model is ARIMA(1,1,0). Now I want to simulate the series from that model. I wrote the simple AR(1) model, but I couldn't understand how to adjust the difference within the model ARI(1,1,0). 
The following R code for AR(1) series is:</p>

<pre><code>phi= -0.7048                                 
z=rep(0,100)                                 
e=rnorm(n=100,0,0.345)                       
cons=2.1                                     
z[1]=4.1
for (i in 2:100) z[i]=cons+phi*z[i-1]+e[i]   
plot(ts(Y))                
</code></pre>

<p>How do i include the difference term ARI(1,1) in above code. 
Any one help me in this regard.</p>
"
"0.155230105141267","0.182574185835055","209874","<p>I have a model fitted with <code>auto.arima</code>, the model is ARIMA(0,1,0)x(0,1,0)[6] with seasonal period 6. The data is bi-monthly so there is an annual seasonality. There is only one regressor indicating an intervention (dummy). </p>

<p>Then I used this model to old data to see what would have happened if the intervention would have done since and earlier period, using the model and forecast from an earlier data. <strong>The thing I do not understand yet</strong> is that if I suppose the intervention only occur in one period, the series only differ in this period. Therefore, there is no persistence on the intervention.</p>

<p>As I understand, the model has ARIMA errors. The error in the intervention period should change and so there should be an effect in the next periods when using forecast to predict futures values. If the intervention occurs in only one period, <strong>why</strong> in the forecast the intervention does not affect futures predictions?</p>

<hr>

<p>EDIT:</p>

<p>The code I am using is</p>

<pre><code>model1&lt;-auto.arima(ts,xreg = X.ts)
</code></pre>

<p>Where <code>X.ts</code> is a <code>ts</code> object with <code>0</code> and a period with intervention. </p>

<p>Then I used </p>

<pre><code>model2&lt;-Arima(Xold, xreg= X.ts.old, model=model1)
</code></pre>

<p>So I used the first model on earlier data to make the following</p>

<pre><code>forecast(model2, xreg=cbin(c(0,1,1,1,1...))
</code></pre>

<p>So I am trying to show what would have been expected from an earlier period (the forecast) if the intervention would have started earlier.</p>

<p>The thing I do not understand yet is that for instance</p>

<pre><code>forecast(model2, xreg=cbin(c(0,1,1,1,0...))
forecast(model2, xreg=cbin(c(0,1,1,1,1...))
</code></pre>

<p>only differ in the periods the <code>xreg</code> differ, with no persistence of these differences. I did not expect this, <strong>why is that?</strong></p>
"
"0.155875555347912","0.185185185185185","234192","<p>I referred to <a href=""https://www.otexts.org/fpp/9/4"" rel=""nofollow"">this link</a> and I have the following questions regarding my data. Let me start by explaining the time series that I am dealing with.</p>

<p>I have <strong>daily</strong> hospital data with various <strong>departments</strong> and numerous <strong>doctors</strong> working in each department. I have several years of data and my forecast horizon is for the next 365 days. My data has weekly and annual seasonality. Moreover I intend to capture the effects of holidays and Sundays in my forecasts. As a result I have not created a hierarchical time series as suggested towards the end of the link(primarily because I am not sure whether we can pass a regressor to it and more so because I do not know how many doctors I end up predicting for in each department). </p>

<p>The reason for this is that some doctors do not have good data(short time series or sparse data). In this case I collect these doctors and aggregate them to form something I call ""OtherDocs"". Typically in <code>DeptXYZ -&gt; Doc1 , Doc2 , Doc3 , Doc4 , Doc5 and Doc6</code> I could end up creating forecasts for <code>DeptXYZ -&gt; Doc1 , Doc3 , Doc4 , Doc6 and OtherDocs</code>. If <code>OtherDocs</code> is still not predictable I generate a naive forecast. In this fashion I created <strong>base forecasts for every level in the hierarchy individually using <code>arima</code> and passing my <code>xreg</code> to it and selecting the best model on the basis of AIC</strong>.</p>

<p>Now, consider this example - </p>

<p><code>Total -&gt; DeptX and DeptY</code></p>

<p><code>DeptX -&gt; DocA and DocB</code></p>

<p><code>DeptY -&gt; Doc1 , Doc2 and Doc3</code></p>

<p>There are cases where <code>DocA</code> has a time series that starts from ""2011-03-11"" and ends on ""2016-09-07"" while <code>DocB</code> has a time series that starts from ""2011-05-17"" and ends on ""2016-09-07"". Generating the base forecasts for <code>DocA</code> and <code>DocB</code> results in the predicted values(<code>fit$mean</code>) being of a time series from ""2016-09-08"" to ""2017-09-07"". As long as the time series refers to the same dates within the Department I believe we are good to go.</p>

<p>In my attempt to reconcile the forecasts from each level I employed the forecasted proportions like so -</p>

<p>$\Largeá»¹_{DocA,365} = \frac{Å·_{DocA,365}*Å·_{DeptX,365}}{(Å·_{DocA,365}+Å·_{DocB,365})*(Å·_{DeptX,365}+Å·_{DeptY,365})}Å·_{Total,365}$</p>

<p><strong>1. Am I doing anything wrong in the above step?</strong></p>

<p><strong>2. Suppose for one moment that the topmost level forecasted values do not capture the low points of data in the case of Holidays and Sundays. Does that intuitively mean that revised forecasts for DocA might not correctly capture the same(being a proportion of $Å·_{Total,365}$)?</strong></p>

<p>Another query I have is to do with the Optimal Combination Approach -</p>

<p>$\Largeá»¹_h = S(Sâ€²S)^{-1}Sâ€²Å·_h$</p>

<p><strong>3. I am unfamiliar with this matrix notation $S'$. Is it the inverse of $S$? Could you shed some light on this? And how do you suggest I calculate the summing up matrix in my case?(Is it absolutely necessary to proceed with the exact knowledge of the number of doctors in each department?)</strong></p>
"
"0.0850230301897704","0.111111111111111","62237","<p>I am working on a data set. After using some model identification techniques, I came out with an ARIMA(0,2,1) model. </p>

<p>I used the <code>detectIO</code> function in the package <code>TSA</code> in R to detect an <em>innovative</em> outlier (IO) at the 48th observation of my original data set. </p>

<p>How do I incorporate this outlier into my model so I can use it for forecasting purposes? I don't want to use the ARIMAX model since I might not be able to make any predictions from that in R. Are there any other ways I could do this?  </p>

<p>Here are my values in order:</p>

<pre><code>VALUE &lt;- scan()
  4.6  4.5  4.4  4.5  4.4  4.6  4.7  4.6  4.7  4.7  4.7  5.0  5.0  4.9  5.1  5.0  5.4
  5.6  5.8  6.1  6.1  6.5  6.8  7.3  7.8  8.3  8.7  9.0  9.4  9.5  9.5  9.6  9.8 10.0
  9.9  9.9  9.8  9.8  9.9  9.9  9.6  9.4  9.5  9.5  9.5  9.5  9.8  9.3  9.1  9.0  8.9
  9.0  9.0  9.1  9.0  9.0  9.0  8.9  8.6  8.5  8.3  8.3  8.2  8.1  8.2  8.2  8.2  8.1
  7.8  7.9  7.8  7.8
</code></pre>

<p>That is actually my data. They are unemployment rates over a period of 6 years. There are 72 observations then . Each value is to at most one decimal place</p>
"
"0.219528519979381","0.258198889747161","172226","<p>Let's assume an analytical model predicts an epidemic trend over time, i.e. number of infections over time. We also have a computer simulation results over time to verify the performance of the model. The goal is to prove the simulation results and predicted values of the analytical model (which are both a time series) are statistically close or similar. By similarity I mean the model predicts the values close to what simulation is providing.</p>

<p><strong>Background</strong>:
Researching around this topic, I came across the following posts:</p>

<ol>
<li><p><a href=""http://stackoverflow.com/questions/13835924/similarity-of-trends-in-time-series-analysis"">http://stackoverflow.com/questions/13835924/similarity-of-trends-in-time-series-analysis</a></p></li>
<li><p><a href=""http://stats.stackexchange.com/questions/19103/how-to-statistically-compare-two-time-series"">How to statistically compare two time series?</a></p></li>
</ol>

<p>Both discussions suggest three approaches, where I am interested in two of them basically:</p>

<p>(1). Use of ARIMA; 
 (2). Use of Granger test</p>

<p>For the first suggested solution, this is what has been written there in regards to ARIMA, in (1):</p>

<blockquote>
  <p>Run ARIMA on both data sets. (The basic idea here is to see if the same set of parameters (which make up the ARIMA model) can describe both your temp time series. If you run auto.arima() in forecast (R), then it will select the parameters p,d,q for your data, a great convenience.</p>
</blockquote>

<p>I ran auto.arima on the simulation values and then ran forecast, here are the results:</p>

<pre><code>ARIMA(2,0,0) with zero mean     

Coefficients:
         ar1      ar2
      1.4848  -0.5619
s.e.  0.1876   0.1873

sigma^2 estimated as 121434:  log likelihood=-110.64
AIC=227.27   AICc=229.46   BIC=229.4
</code></pre>

<p>I ran auto.arima on predicted model values and then forecast. This is the result of the predicted model:</p>

<pre><code>ARIMA(2,0,0) with non-zero mean 

Coefficients:
         ar1      ar2  intercept
      1.5170  -0.7996  1478.8843
s.e.  0.1329   0.1412   290.4144

sigma^2 estimated as 85627:  log likelihood=-108.11
AIC=224.21   AICc=228.21   BIC=227.05
</code></pre>

<p><strong>Question 1</strong> What are the values that need to be compared to prove that the two series are similar especially the trend over time?</p>

<p>Regarding the second suggested option, I have read about it and found that Granger test is usually used to see if the values of series <em>A</em> at time <em>t</em> can predict the values of Series <em>B</em> at time <em>t+1</em>. </p>

<p><strong>Question 2</strong> Basically, in my case I want to compare the values of time series A and B at the same time, how this one is relevant to my case then?</p>

<p><strong>Question 3</strong> Is there any available method can be used to prove that the trend of two time-series over time is similar?</p>

<p>FYI. I saw another method which is using Pearson Correlation Coefficient and I could follow the reasoning there. Moreover, verifying analytical models with simulations has been widely used in the literature. see:</p>

<ol>
<li><a href=""http://users.ece.gatech.edu/~jic/tnn05.pdf"" rel=""nofollow"">Spatial-Temporal Modeling of Malware Propagation in Networks Modeling</a></li>
<li><a href=""http://cs.ucf.edu/~czou/research/emailWorm-TDSC.pdf"" rel=""nofollow"">Modeling and Simulation Study of the Propagation and Defense of Internet Email Worm</a></li>
</ol>
"
"0.12024072240843","0.157134840263677","151652","<p>I fitted a number of SARIMA models using R and chose the ARIMA(0,0,0)(3,1,0)[12] as the best fitted model to the univariate data with 180 points (periodicity=12). This model is chosen as the best model according to the criteria of lowest MAPE among other fitted 624 models.</p>

<p>The residuals of the model violates the assumption of independently distributed residuals (and same for the 2nd best, 3rd best model etc.). Actually the residuals are also non-normally distributed; however the model is fitted with the method of conditional sum of squares in order to bypass the violation of normality assumption. </p>

<p>In the data, the most of the values are close to zero and this does not allow any data transformation. </p>

<p>The data represent the evolution of coefficents of a 11th degree polynomial equation (in total 15 equations representing different years of electricity load duration curves). The purpose is to forecast the coefficients of e.g. the 16th equation and so the corresponding load duration curve.</p>

<p>Can anybody sugggest/provide any solutions to this case?</p>

<pre><code>x=c(1.887090e+04, -6.023007e+00,  1.193635e-02, -1.455856e-05,  1.064251e-08, -4.953592e-12,  1.517229e-15, -3.090332e-19,
4.137144e-23, -3.491891e-27,  1.682794e-31, -3.527046e-36,  1.904962e+04, -7.394189e+00,  1.600849e-02, -2.077511e-05,
1.585519e-08,-7.587987e-12,    2.363570e-15, -4.859251e-19,  6.534816e-23, -5.525202e-27,  2.663420e-31, -5.580438e-36,
2.009098e+04, -1.061082e+01,  2.319182e-02, -2.917768e-05,  2.171827e-08, -1.019917e-11,  3.133564e-15, -6.379905e-19,
8.520995e-23, -7.168462e-27,  3.442102e-31, -7.188143e-36,  2.067028e+04, -8.034999e+00,  1.761326e-02, -2.240562e-05,
1.680919e-08, -7.961614e-12,  2.469832e-15, -5.081494e-19,  6.861040e-23, -5.835236e-27,  2.831898e-31, -5.974519e-36,
2.233604e+04, -1.033148e+01,  2.287039e-02, -2.952031e-05,  2.255568e-08, -1.086351e-11,  3.419260e-15, -7.123005e-19,
9.720229e-23, -8.341734e-27,  4.079166e-31, -8.660882e-36,  2.392045e+04, -8.246481e+00,  1.585412e-02, -2.056180e-05,
1.636424e-08, -8.253437e-12,  2.710813e-15, -5.858824e-19,  8.245204e-23, -7.258003e-27,  3.624039e-31, -7.827743e-36,
2.636514e+04, -9.886355e+00,  1.951992e-02, -2.504930e-05,  1.963158e-08, -9.789139e-12,  3.190186e-15, -6.856046e-19,
9.606813e-23, -8.427664e-27,  4.196799e-31, -9.046539e-36,  2.866210e+04, -8.866902e+00,  1.734494e-02, -2.387617e-05,
1.957175e-08, -9.993900e-12,  3.300201e-15, -7.152619e-19,  1.008517e-22, -8.892694e-27,  4.448060e-31, -9.626143e-36,
3.002254e+04, -1.007403e+01,  2.151203e-02, -2.984675e-05,  2.427803e-08, -1.226036e-11,  3.997630e-15, -8.550747e-19,
1.190499e-22, -1.037815e-26,  5.140218e-31, -1.103334e-35,  2.929311e+04, -1.123255e+01,  2.282206e-02, -2.968240e-05,
2.323868e-08, -1.146069e-11,  3.677709e-15, -7.777557e-19,  1.073806e-22, -9.301478e-27,  4.584147e-31, -9.800725e-36,
3.306894e+04, -1.396117e+01,  2.326777e-02, -2.724425e-05,  2.023428e-08, -9.690231e-12,  3.055811e-15, -6.392630e-19,
8.763020e-23, -7.552202e-27,  3.707622e-31, -7.901994e-36,  3.491666e+04, -1.315883e+01,  2.554492e-02, -3.194439e-05,
2.437661e-08, -1.184053e-11,  3.762542e-15, -7.896499e-19,  1.082565e-22, -9.310722e-27,  4.554895e-31, -9.664092e-36,
3.775600e+04, -2.101521e+01,  4.695457e-02, -6.000206e-05,  4.510264e-08, -2.134088e-11,  6.600784e-15, -1.352465e-18,
1.817468e-22, -1.538166e-26,  7.429410e-31, -1.560507e-35,  3.699341e+04, -1.019327e+01,  1.761360e-02, -2.428662e-05,
2.084200e-08, -1.112473e-11,  3.796505e-15, -8.415154e-19,  1.204392e-22, -1.072641e-26,  5.402195e-31, -1.174885e-35,
4.009280e+04, -1.887174e+01,  3.441926e-02, -4.161190e-05,  3.152055e-08, -1.535050e-11,  4.911316e-15, -1.040003e-18,
1.440215e-22, -1.251900e-26,  6.190925e-31, -1.327693e-35)

fit=arima(x, order = c(0, 0, 0),seasonal = list(order = c(3, 1, 0), period =12),method=c(""CSS""))

par(mfrow=c(1,2)) 
x1&lt;-acf(fit$residuals,180,ylab=""Sample ACF"",main ="""",xaxt=""n"")
axis(1, at=seq(0, 15, by=2), labels = TRUE)
abline(v=(seq(0,15,1)), col=""black"", lty=""dotted"")

x2&lt;-pacf(fit$residuals,180,ylab=""Sample PACF"",main ="""",xaxt=""n"")
axis(1, at=seq(0, 15, by=2), labels = TRUE)
abline(v=(seq(0,15,by=1)), col=""black"", lty=""dotted"")
</code></pre>
"
"0.0850230301897704","0.111111111111111","6330","<p>I have previously used <a href=""http://www.forecastpro.com/"">forecast pro</a> to forecast univariate time series, but am switching my workflow over to R.  The forecast package for R contains a lot of useful functions, but one thing it doesn't do is any kind of data transformation before running auto.arima().  In some cases forecast pro decides to log transform data before doing forecasts, but I haven't yet figured out why.</p>

<p>So my question is: when should I log-transform my time series before trying ARIMA methods on it?</p>

<p>/edit: after reading your answers, I'm going to use something like this, where x is my time series:</p>

<pre><code>library(lmtest)
if ((gqtest(x~1)$p.value &lt; 0.10) {
    x&lt;-log(x)
}
</code></pre>

<p>Does this make sense?</p>
"
"0.12024072240843","0.157134840263677","6329","<p>I've been using the ets() and auto.arima() functions from the <a href=""http://robjhyndman.com/software/forecast/"">forecast package</a> to forecast a large number of univariate time series.  I've been using the following function to choose between the 2 methods, but I was wondering if CrossValidated had any better (or less naive) ideas for automatic forecasting.</p>

<pre><code>auto.ts &lt;- function(x,ic=""aic"") {
    XP=ets(x, ic=ic) 
    AR=auto.arima(x, ic=ic)

    if (get(ic,AR)&lt;get(ic,XP)) {
        model&lt;-AR
    }
    else {
        model&lt;-XP
    }
        model
}
</code></pre>

<p>/edit: What about this function?</p>

<pre><code>auto.ts &lt;- function(x,ic=""aic"",holdout=0) {
    S&lt;-start(x)[1]+(start(x)[2]-1)/frequency(x) #Convert YM vector to decimal year
    E&lt;-end(x)[1]+(end(x)[2]-1)/frequency(x)
    holdout&lt;-holdout/frequency(x) #Convert holdout in months to decimal year
    fitperiod&lt;-window(x,S,E-holdout) #Determine fit window

    if (holdout==0) {
        testperiod&lt;-fitperiod
    }
    else {
        testperiod&lt;-window(x,E-holdout+1/frequency(x),E) #Determine test window
    }

    XP=ets(fitperiod, ic=ic)
    AR=auto.arima(fitperiod, ic=ic)

    if (holdout==0) {
        AR_acc&lt;-accuracy(AR)
        XP_acc&lt;-accuracy(XP)
    }
    else {
        AR_acc&lt;-accuracy(forecast(AR,holdout*frequency(x)),testperiod)
        XP_acc&lt;-accuracy(forecast(XP,holdout*frequency(x)),testperiod)
    }

    if (AR_acc[3]&lt;XP_acc[3]) { #Use MAE
        model&lt;-AR
    }
    else {
        model&lt;-XP
    }
    model
}
</code></pre>

<p>The ""holdout"" is the number of periods you wish to use as an out of sample test.  The function then calculates a fit window and a test window based on this parameter.  Then it runs the auto.arima and ets functions on the fit window, and chooses the one with the lowest MAE in the test window.  If the holdout is equal to 0, it tests the in-sample fit.</p>

<p>Is there a way to automatically update the chosen model with the complete dataset, once it has been selected?</p>
"
"0.12024072240843","0.130945700219731","172396","<p>I'm trying to use ARIMA (R-language implementation) and forecast daily data.
The initial data set contains per-hour values for 15 days and model is built like </p>

<pre><code>myModel&lt;-arima (knownData, order=c(24, 1, 24),include.mean=FALSE,method=""CSS"")
</code></pre>

<p>Predicting next 24 values (next day) works nice - data is very close to reality (including time structure, minimum and maximum are where they really are).</p>

<p>Now I won't to predict the end of the day, based on the beginning:</p>

<pre><code>newModel&lt;-Arima(additionalData,model=myModel)
newForecast&lt;-predict(newModel, n.ahead=24)
</code></pre>

<p>Where <code>additionalData</code> contains about 12 values. The result is awful, not even close to what it should be.</p>

<p><strong>Question</strong></p>

<p>It's obvious, that I'm doing something wrong, but not sure what exactly - either model is bad, or data is used improperly, or something else.</p>

<p><strong>Values</strong></p>

<p>Daily forecast data (very close to real data structure and values):</p>

<pre><code>1.5381624  6.4080989 19.8963887 31.0804878 24.5755121 35.7171803 36.3545548 37.4275811 36.8051448 34.6626951 23.7145071 32.4005010      31.6820914 35.8199186 39.2482743 47.7286449 40.5614604 45.2695602 36.1903594 30.3068764 23.3358931 13.1883880 -0.4366872  6.1197422
</code></pre>

<p>Second forecast, after adding 13 more values:</p>

<pre><code>-10.749518 -23.843293 -21.503110  -1.098214  16.500469  34.364534  55.129484  67.012708  79.486753  73.354853  62.377744
</code></pre>

<p>Negative values, instead of relatively big positive and no minimum at the end.</p>
"
"0.176989551117627","0.195712187983995","104977","<p>I understand we should use ARIMA for modelling a non-stationary time series. Also, everything I read says ARMA should only be used for stationary time series.</p>

<p>What I'm trying to understand is, what happens in practice when misclassifying a model, and assuming <code>d = 0</code> for a time series that's non-stationary? For example: </p>

<pre><code>controlData &lt;- arima.sim(list(order = c(1,1,1), ar = .5, ma = .5), n = 44)
</code></pre>

<p>control data looks like this:</p>

<pre><code> [1]   0.0000000   0.1240838  -1.4544087  -3.1943094  -5.6205257
 [6]  -8.5636126 -10.1573548  -9.2822666 -10.0174493 -11.0105225
[11] -11.4726127 -13.8827001 -16.6040541 -19.1966633 -22.0543414
[16] -24.8542959 -25.2883155 -23.6519271 -21.8270981 -21.4351267
[21] -22.6155812 -21.9189036 -20.2064343 -18.2516852 -15.5822178
[26] -13.2248230 -13.4220158 -13.8823855 -14.6122867 -16.4143756
[31] -16.8726071 -15.8499558 -14.0805114 -11.4016515  -9.3330560
[36]  -7.5676563  -6.3691600  -6.8471371  -7.5982880  -8.9692152
[41] -10.6733419 -11.6865440 -12.2503202 -13.5314306 -13.4654890
</code></pre>

<p>Assuming I didn't know the data was <code>ARIMA(1,1,1)</code>, I might have a look at <code>pacf(controlData)</code>.</p>

<p><img src=""http://i.stack.imgur.com/IOXJf.jpg"" alt=""pacf(controlData)""></p>

<p>Then I use Dickey-Fuller to see if the data is non-stationary:</p>

<pre><code>require('tseries')
adf.test(controlData)

# Augmented Dickey-Fuller Test
#
# data:  controlData
# Dickey-Fuller = -2.4133, Lag order = 3, p-value = 0.4099
# alternative hypothesis: stationary

adf.test(controlData, k = 1)

# Augmented Dickey-Fuller Test
#
#data:  controlData
# Dickey-Fuller = -3.1469, Lag order = 1, p-value = 0.1188
# alternative hypothesis: stationary
</code></pre>

<p>So, I might assume the data is ARIMA(2,0,*) Then use <code>auto.arima(controlData)</code> to try to get a best fit?</p>

<pre><code>require('forecast')
naiveFit &lt;- auto.arima(controlData)
navifeFit
# Series: controlData 
# ARIMA(2,0,1) with non-zero mean 
# 
# Coefficients:
#          ar1      ar2     ma1  intercept
#      1.4985  -0.5637  0.6427   -11.8690
# s.e.  0.1508   0.1546  0.1912     3.2647
#
# sigma^2 estimated as 0.8936:  log likelihood=-64.01
# AIC=138.02   AICc=139.56   BIC=147.05
</code></pre>

<p>So, even though the past and future data is ARIMA(1,1,1), I might be tempted to classify it as ARIMA(2,0,1). <code>tsdata(auto.arima(controlData))</code> looks good too.</p>

<p>Here is what an informed modeler would find:</p>

<pre><code>informedFit &lt;- arima(controlData, order = c(1,1,1))
# informedFit
# Series: controlData 
# ARIMA(1,1,1)                    
#
# Coefficients:
#          ar1     ma1
#       0.4936  0.6859
# s.e.  0.1564  0.1764
#
# sigma^2 estimated as 0.9571:  log likelihood=-62.22
# AIC=130.44   AICc=131.04   BIC=135.79
</code></pre>

<p>1) Why are these information criterion better than the model selected by <code>auto.arima(controlData)</code>?</p>

<p>Now, I just graphically compare the real data, and the 2 models:</p>

<pre><code>plot(controlData)
lines(fitted(naiveFit), col = ""red"")
lines(fitted(informedFit), col = ""blue"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/sy3YR.jpg"" alt=""tsPlots""></p>

<p>2) Playing devil's advocate, what kind of consequences would I pay by using an ARIMA(2, 0, 1) as a model? What are the risks of this error? </p>

<p>3) I'm mostly concerned about any implications for multi-period forward predictions. I assume they would be less accurate? I'm just looking for some proof.</p>

<p>4) Would you suggest an alternative method for model selection? Are there any problems with my reasoning as an ""uninformed"" modeler?</p>

<p>I'm really curious what are the other consequences of this kind of misclassification. I've been looking for some sources and just couldn't find anything. All the literature I could find only touches on this subject, instead just stating the data should be stationary before performing ARMA, and if it's non-stationary, then it needs to be differenced d times.</p>

<p>Thanks!</p>
"
"0.0490880693673816","0.0641500299099584","210117","<p>I have the code below which trains ARIMA models for a range of order combinations. I'm getting the error below in the step training the ARIMA models.  The code worked just fine with the <code>hsales</code> time-series provided for Hyndman's text book in the ""fpp"" package in R. If anyone can point out the issue or suggest how to solve it, I would be grateful.</p>

<p>Code:</p>

<pre><code>library(""forecast"")
library(""tseries"")
library(""sqldf"")
library(""manipulate"")
library(""dplyr"")
library(""xts"")

tsTrain &lt;- tsTrain
tsTest &lt;- tsValidation

pvar&lt;-1:17
dvar&lt;-1:2
qvar&lt;-1:17

##Creating All Combingations

OrderGrid&lt;-expand.grid(pvar,dvar,qvar)

##Vectorize Suggestion

n &lt;- function(a,b,c) {Arima(tsTrain, order=c(a,b,c),method=""ML"")}
mod_fit &lt;- do.call(Vectorize(n, SIMPLIFY=FALSE), unname(OrderGrid))
</code></pre>

<p>Error:</p>

<pre><code>Error in optim(init[mask], armafn, method = optim.method, hessian = TRUE,  : 
  non-finite finite-difference value [3] 
</code></pre>

<p>Data:</p>

<pre><code>c(11, 14, 17, 5, 5, 5.5, 8, NA, 5.5, 6.5, 8.5, 4, 5, 9, 10, 11, 
7, 6, 7, 7, 5, 6, 9, 9, 6.5, 9, 3.5, 2, 15, 2.5, 17, 5, 5.5, 
7, 6, 3.5, 6, 9.5, 5, 7, 4, 5, 4, 9.5, 3.5, 5, 4, 4, 9, 4.5, 
6, 10, NA, 9.5, 15, 9, 5.5, 7.5, 12, 17.5, 19, 7, 14, 17, 3.5, 
6, 15, 11, 10.5, 11, 13, 9.5, 9, 7, 4, 6, 15, 5, 18, 5, 6, 19, 
19, 6, 7, 7.5, 7.5, 7, 6.5, 9, 10, 5.5, 5, 7.5, 5, 4, 10, 7, 
5, 12, 6, NA, 4, 2, 5, 7.5, 11, 13, 7, 8, 7.5, 5.5, 7.5, 15, 
7, 4.5, 9, 3, 4, 6, 17.5, 11, 7, 6, 7, 4.5, 4, 4, 5, 10, 14, 
7, 7, 4, 7.5, 11, 6, 11, 7.5, 15, 23.5, 8, 12, 5, 9, 10, 4, 9, 
6, 8.5, 7.5, 6, 5, 8, 6, 5.5, 8, 11, 10.5, 4, 6, 7, 10, 11.5, 
11.5, 3, 4, 16, 3, 2, 2, 8, 4.5, 7, 4, 8, 11, 6.5, 7.5, 17, 6, 
6.5, 9, 12, 17, 10, 5, 5, 9, 3, 8.5, 11, 4.5, 7, 16, 11, 14, 
6.5, 15, 8.5, 7, 6.5, 11, 2, 2, 13.5, 4, 2, 16, 11.5, 3.5, 9, 
16.5, 2.5, 4.5, 8.5, 5, 6, 7.5, 9.5, NA, 9.5, 8, 2.5, 4, 12, 
13, 10, 4, 6, 16, 16, 13, 8, 12, 19, 19, 5.5, 8, 6.5, NA, NA, 
NA, 15, 12, NA, 6, 11, 8, 4, 2, 3, 4, 10, 7, 5, 4.5, 4, 5, 11.5, 
12, 10.5, 4.5, 3, 4, 7, 15.5, 9.5, NA, 9.5, 12, 13.5, 10, 10, 
13, 6, 8.5, 15, 16.5, 9.5, 14, 9, 9.5, 11, 15, 14, 5.5, 6, 14, 
16, 9.5, 23, NA, 19, 12, 5, 11, 16, 8, 11, 9, 13, 6, 7, 3, 5.5, 
7.5, 19, 6.5, 5.5, 4.5, 7, 8, 7, 10, 11, 13, NA, 12, 1.5, 7, 
7, 12, 8, 6, 9, 15, 9, 3, 5, 11, 11, 8, 6, 3, 7.5, 4, 7, 7.5, 
NA, NA, NA, NA, 6.5, 2, 16.5, 7.5, 8, 8, 5, 2, 7, 4, 6.5, 4.5, 
10, 6, 4.5, 6.5, 9, 2, 6, 3.5, NA, 5, 7, 3.5, 4, 4.5, 13, 19, 
8.5, 10, 8, 13, 10, 10, 6, 13.5, 12, 11, 5.5, 6, 3.5, 9, 8, NA, 
6, 5, 8.5, 3, 12, 10, 9.5, 7, 24, 7, 9, 11.5, 5, 7, 11, 6, 5.5, 
3, 4.5, 4, 5, 5, 3, 4.5, 6, 10, 5, 4, 4, 9.5, 5, 7, 6, 3, 13, 
5.5, 5, 7.5, 3, 5, 6.5, 5, 5.5, 6, 4, 3, 5, NA, 5, 5, 6, 7, 8, 
5, 5.5, 9, 6, 8.5, 9.5, 8, 9, 6, 12, 5, 7, 5, 3.5, 4, 7.5, 7, 
5, 4, 4, NA, 7, 5.5, 6, 8.5, 6.5, 9, 3, 2, 8, 15, 6, 4, 10, 7, 
13, 14, 9.5, 9, 18, 6, 5, 4, 6, 4, 11.5, 17.5, 7, 8, 10, 4, 7, 
5, 9, 6, 5, 4, 8, 4, 2, 1.5, 3.5, 6, 5.5, 5, 4, 8, 10.5, 4, 11, 
9.5, 5, 6, 11, 21, 9.5, 11, 13.5, 7.5, 13, 10, 7, 9.5, 6, 10, 
5.5, 6.5, 12, 10, 10, 6.5, 2, 8, NA, 10, 5, 4, 4.5, 5, 7.5, 12, 
22, 5, 8.5)
</code></pre>
"
"0.0927677313474139","0.0969857289937744","6513","<p>I want to predict inter-day electricity load. My data are electricity loads for 11 months, sampled in 30 minute intervals. I also got the weather-specific data from a meteorological station (temperature, relative humidity, wind direction, wind speed, sunlight). From this, I want to predict the electricity load until the end of the day. </p>

<p>I can run my algorithm until 10:00 of the present day and after that it should give the prediction of loads in 30 minute intervals. So, it should tell the load at 10:30, 11:00, 11:30 and so on until 24:00.</p>

<p>My first attempt was to create a <strong>linear model</strong> in R.</p>

<pre><code>BP.TS &lt;- ts(Buying.power, frequency = 48)
a &lt;- data.frame(
Time, BP.TS, Weekday, Pressure, Temperature, RelHumidity, AvgWindSpeed, AvgWindDirection, MaxWindSpeed, MaxWindDirection, SunLightTime,
m, Buying.2dayago, AfterHolidayAndBPYesterday8, MovingAvgLast7DaysMidnightTemp
)
a &lt;- a[(6*48+1):nrow(a),]

start = 9716
steps.ahead = 21
par(mfrow=c(5,2))
for (i in 1:10) {
    train &lt;- a[1:(start+(i-1)*48),]
    test &lt;- a[((i-1)*48+start+1):((i-1)*48+start+steps.ahead),]
    summary(reg &lt;- lm(log(BP.TS)~., data=train, na.action=NULL))
    pred &lt;- exp(predict(reg, test))

    plot(test$BP.TS, type=""o"")
    lines(pred, col=2)
    cat(""MAE"", mean(abs(test$BP.TS - pred)), ""\n"")
}
</code></pre>

<p>This is not very succesful. Now I try to model the data with ARIMA. I used auto.arima() from the <a href=""http://cran.r-project.org/web/packages/forecast/index.html"">forecast package</a>. These are the results I got:</p>

<pre><code>&gt; auto.arima(BP.TS)
Series: BP.TS 
ARIMA(2,0,1)(1,1,2)[48]                    

Call: auto.arima(x = BP.TS) 

Coefficients:
         ar1      ar2     ma1    sar1     sma1    sma2
      1.1816  -0.2627  -0.554  0.4381  -1.2415  0.3051
s.e.  0.0356   0.0286   0.033  0.0952   0.0982  0.0863

sigma^2 estimated as 256118:  log likelihood = -118939.7
AIC = 237893.5   AICc = 237893.5   BIC = 237947
</code></pre>

<p>Now if I try something like:</p>

<pre><code>reg = arima(train$BP.TS, order=c(2,0,1), xreg=cbind(
train$Time, 
train$Weekday, 
train$Pressure, 
train$Temperature, 
train$RelHumidity, 
train$AvgWindSpeed, 
train$AvgWindDirection, 
train$MaxWindSpeed, 
train$MaxWindDirection, 
train$SunLightTime,
train$Buying.2dayago,
train$MovingAvgLastNDaysLoad,
train$X1, train$X2, train$X3, train$X4, train$X5, train$X6, train$X7, train$X8, train$X9, 
train$X11, train$X12, train$X13, train$X14, train$X15, train$X16, train$X17, train$X18, 
train$MovingAvgLast7DaysMidnightTemp
))

p &lt;- predict(reg, n.ahead=21, newxreg=cbind(
test$Time, 
test$Weekday, 
test$Pressure, 
test$Temperature, 
test$RelHumidity, 
test$AvgWindSpeed, 
test$AvgWindDirection, 
test$MaxWindSpeed, 
test$MaxWindDirection, 
test$SunLightTime,
test$Buying.2dayago,
test$MovingAvgLastNDaysLoad,
test$X1, test$X2, test$X3, test$X4, test$X5, test$X6, test$X7, test$X8, test$X9, 
test$X11, test$X12, test$X13, test$X14, test$X15, test$X16, test$X17, test$X18, 
test$MovingAvgLast7DaysMidnightTemp
))

plot(test$BP.TS, type=""o"", ylim=c(6300,8300))
par(new=T)
plot(p$pred, col=2, ylim=c(6300,8300))
cat(""MAE"", mean(p$se), ""\n"")
</code></pre>

<p>I get even worse results. Why? I ran out of ideas, so please help. If there is additional information I need to give, please ask.</p>
"
"0.170046060379541","0.222222222222222","192739","<p>I have been working with the forecast package in R a lot, recently. And my question might seem trivial (or not, maybe I'm missing something), but for the life of me I can't seem to find a way to fit an Arima model with exogenous variables (<code>xreg</code> argument) that has been computed by the <code>auto.arima</code> function to previously unseen test data.</p>

<p>So, I'm basically trying to do the following:</p>

<pre><code>library(forecast)
fit &lt;- auto.arima(trainingdata, xreg = trainingvariables)
</code></pre>

<p>...and then I would like to ""apply"" the model to new test data, for which I also have new exogenous variables available. I can see the following methods:</p>

<pre><code>fitted(fit)
</code></pre>

<p>That returns one-step in-sample forecasts, so, in effect, that's exactly what I want. Except that it's in-sample. However, I would like to calculate <strong>one-step out-of-sample forecasts</strong> (with <strong>new exogenous variables</strong> that I have available). Another method:</p>

<pre><code>forecast(fit, xreg = newvariables, h = ...)
</code></pre>

<p>That works for exactly one step, but then seems to merely forecast the trainingdata stored in the model fit. But I don't think I can use new testdata here? (So, I can't use this method for testing one-step prediction accuracy.) One more idea:</p>

<pre><code>fit2 &lt;- Arima(testdata, model = fit)
</code></pre>

<p>According to the manual, if the <code>model</code> parameter is used, ""this same model is
fitted to [testdata] without re-estimating any parameters"". Great, but I don't think I can supply any new exogenous variables, can I?</p>

<p>I really think, I must be missing something simple. Any help would be much appreciated.</p>
"
"0.0490880693673816","0.0641500299099584","193178","<p>Please consider the following code (in R)</p>

<pre><code>library(forecast)
tt&lt;-structure(c(1494.5, 1367.57, 1357.57, 1222.23, 1124.02, 1011.64, 
4575.64, 3201.87, 3050.04, 2173.38, 1967.88, 1838.55, 1666.05, 
1656.05, 1524.96, 835.96, 775.36, 592.36, 494.15, 4058.15, 2624.36, 
2448.47, 1598.47, 1398.47, 1264.14, 1165.88, 1053.67, 941.36, 
821.36, 471.36, 373.15, 259.91, 3808.91, 2262.26, 1940.39, 1011.39, 
800.81, 790.81), index = structure(c(16563L, 16565L, 16570L, 
16572L, 16577L, 16579L, 16584L, 16585L, 16586L, 16587L, 16588L, 
16589L, 16590L, 16592L, 16593L, 16599L, 16606L, 16607L, 16608L, 
16612L, 16613L, 16614L, 16617L, 16618L, 16619L, 16620L, 16621L, 
16628L, 16633L, 16635L, 16638L, 16642L, 16647L, 16648L, 16649L, 
16650L, 16651L, 16654L), class = ""Date""), class = ""zoo"")

tt2&lt;-as.ts(tt)
tt2&lt;-na.locf(tt2) #I replace the NA with the previous non-NA value
mm&lt;-auto.arima(tt2)

plot(forecast(mm, h=60))
</code></pre>

<p>The results of the auto.arima function is puzzling...
There is a clear seasonality in the data (this is the balance of an account: every month a salary is cashed in and there is a spike in the value of the series, followed by a decrease until the next salary is received). I would like to forecast a couple of cycles, but the auto.arima forecast is nothing like I expect.
Does anybody have any suggestions (also outside the auto.arima)?
Any suggestion is welcome.</p>
"
"0.0490880693673816","0.0641500299099584","8868","<p>When doing time series  research in R, I found that <code>arima</code>  provides only the coefficient values and their standard errors of fitted model. However, I also want to get the p-value of the coefficients.</p>

<p>I did not find any function that provides the significance of coef.</p>

<p>So I wish to calculate it by myself, but I don't know the degree of freedom in the t or chisq distribution of the coefficients. So my question is how to get the p-values for the coefficients of fitted arima model in R?</p>
"
"0.0490880693673816","0.0641500299099584","193550","<p>How can we decide the size or portion of the data given to get the ARIMA that has the best forecasting properties?</p>

<p>I mean, for example, we have a hourly series with over 28.000 elements.</p>

<p>Which is the criteria that tells us: do ARIMA over last 100 elements, or 250 last elements, so the ARIMA we get is better for forecasting?
I am interested in short time prediction, like for 24 hours.</p>

<p>I read everywhere but found no criterion yet.</p>
"
"0.0490880693673816","0.0641500299099584","160244","<p>I have a time series which shows an yearly spike around summer but otherwise is predictable by an AR(1) model. The tests on the data also show that the time series shows stationarity and is non-seasonal. How do I model the spike?"
"NaN","NaN","(More details about the time series here: <a href=http://stats.stackexchange.com/questions/159769/what-does-the-following-acf-curve-mean-picture-attached?noredirect=1#comment304724_159769>What does the following ACF curve mean ? (Picture attached)</a>)</p>",""
"NaN","NaN","<p><img src=http://i.stack.imgur.com/8ayfF.png alt=enter image description here></p>",""
"NaN","NaN","","<r><time-series><forecasting><arima>"
"NaN","NaN","211670","<p>I want to make predictions use ARIMA in forecast package. I find that basically the prediction is just a lag of the actuals. Is there any way that I can better fit the model or any other approach available? (I find the ARIMA parameters through function <code>auto.arima</code> in ""forecast"" package in R.)</p>

<pre><code>Model and Plot:
        fit &lt;- arima(dataf$actuals, xreg=dataf$regressor,order=c(0,1,0))
        labDates &lt;- seq(as.Date(""2016-01-01"", format = ""%Y-%m-%d""),as.Date(""2016-01-16"", format = ""%Y-%m-%d""),by = ""day"")
        plot(labDates, dataf$actuals,col=""red"",type='l')
        lines(labDates,fitted(fit),col=""blue"")
        legend('topleft',c(""Actual Number"",""Predicted Number""),pch=c(20,20,20),col=c(""red"",""blue""))

Data:
    actuals&lt;-c(26952, 38178, 36377, 45718, 40393, 43111, 39947, 40853, 38792, 41816, 46091, 41866, 35701, 52738,73834, 82813) 
    actuals_next&lt;-c(38178, 36377, 45718, 40393, 43111, 39947, 40853, 38792, 41816, 46091, 41866, 35701, 52738, 73834,82813,NA)  
    regressor&lt;-c(519020, 671049 ,501083 ,288259 ,290899 ,260817, 276166, 274859 ,279405, 286689, 234050,95562,15138,  16401,  27145,  53717)    
    dataf&lt;-as.data.frame(cbind(actuals, actuals_next, regressor))
</code></pre>
"
"0.10976425998969","0.143443827637312","65585","<p>I have a daily weather data set, which has, unsurprisingly, very strong seasonal effect.</p>

<p><img src=""http://i.stack.imgur.com/B5Zpo.jpg"" alt=""enter image description here""></p>

<p>I adapted an ARIMA model to this data set using the function auto.arima from forecast package.
To my surprise the function does not apply any seasonal operations- seasonal differencing, seasonal ar or ma components. Here is the model it estimated:</p>

<pre><code>library(forecast)
data&lt;-ts(data,frequency=365)
auto.arima(Berlin)

Series: data
ARIMA(3,0,1) with non-zero mean 

Coefficients:
         ar1      ar2     ar3      ma1  intercept
      1.7722  -0.9166  0.1412  -0.8487   283.0378
s.e.  0.0260   0.0326  0.0177   0.0214     1.7990

sigma^2 estimated as 5.56:  log likelihood=-8313.74
AIC=16639.49   AICc=16639.51   BIC=16676.7
</code></pre>

<p>And also the forecasts using this model are not really satisfying. Here is the plot of the forecast:
<img src=""http://i.stack.imgur.com/IkpIq.jpg"" alt=""enter image description here""></p>

<p>Can anyone give me a hint what is wrong here?</p>
"
"0.0490880693673816","0.0641500299099584","88722","<p>I am building a regression model of time series data in R, where my primary interest is the coefficients of the independent variables. The data exhibit strong seasonality with a trend.</p>

<p><img src=""http://i.stack.imgur.com/GYxaU.png"" alt=""Original data""></p>

<p>The model looks good, with four of the six regressors significant:
<img src=""http://i.stack.imgur.com/ZmoSd.png"" alt=""Model""></p>

<p>Here are the OLS residuals:
<img src=""http://i.stack.imgur.com/EIybo.png"" alt=""Residuals""></p>

<p>I used auto.arima to select the sARIMA structure, and it returns the model (0,1,1)(1,1,0)[12].</p>

<pre><code>fit.ar &lt;- auto.arima(at.ts, xreg = xreg1, stepwise=FALSE, approximation=FALSE)
summary(fit.ar)

Series: at.ts 
ARIMA(0,1,1)(1,1,0)[12]                    

Coefficients:
          ma1    sar1      v1       v2      v3       v4         v5
      -0.7058  0.3974  0.0342  -0.0160  0.0349  -0.0042  -113.4196
s.e.   0.1298  0.2043  0.0239   0.0567  0.0555   0.0333   117.1205

sigma^2 estimated as 3.86e+10:  log likelihood=-458.13
AIC=932.26   AICc=936.05   BIC=947.06

Training set error measures:
                   ME     RMSE      MAE       MPE     MAPE      MASE
Training set 7906.896 147920.3 103060.4 0.1590107 3.048322 0.1150526
</code></pre>

<p>My question is this: based on the parameter estimates and s.e. of the regressors, I believe that none of them are significant - is this correct, and if so, what does it imply if my goal is to interpret the relative importance of these predictors as opposed to forecasting?</p>

<p>Any other advice relative to the process of building this model is welcome and appreciated.</p>

<p>Here are the ACF and PACF for the residuals:</p>

<p><img src=""http://i.stack.imgur.com/a3Gvy.png"" alt=""ACF-PACF""></p>

<pre><code>&gt; durbinWatsonTest(mod.ols, max.lag=12)
 lag Autocorrelation D-W Statistic p-value
   1     0.120522674     1.6705144   0.106
   2     0.212723044     1.4816530   0.024
   3     0.159828108     1.5814771   0.114
   4     0.031083831     1.8352377   0.744
   5     0.081081308     1.6787808   0.418
   6    -0.024202465     1.8587561   0.954
   7    -0.008399949     1.7720761   0.944
   8     0.040751905     1.6022835   0.512
   9     0.129788310     1.4214391   0.178
  10    -0.015442379     1.6611922   0.822
  11     0.004506292     1.6133994   0.770
  12     0.376037337     0.7191359   0.000
 Alternative hypothesis: rho[lag] != 0
</code></pre>
"
"0.138842026900125","0.15876322406928","212773","<p>I have half-hourly electricity data of several homes for a duration of one month. This data is represented in <code>xts</code> time-series format. Now, I need to make half-hourly forecasts using the same data for coming day. The forecasting interval is one day (i.e., very short term forecast). I assume that electricity usage follows daywise seasonality as the number of users/occupants remain fixed. This assumption is obeyed in some homes while some other follow random electricity usage. </p>

<p>Currently, I use historical one month data to make half-hourly forecasts of coming day using <code>auto.arima</code> found in <code>forecast</code> pacakage. Using this approach I do get forecasts for next day (48 forecasted values). 48 forecasted values correspond to 48 half-hours of the day. But, I do not know </p>

<ol>
<li>How should I specify the seasonality fact, i.e., how should I mention that data is assumed seasonal day-wise. In other words, how should I mention within historical data of one month, there are 30 periods and each period consist of 48 observations. </li>
<li>Is <code>xts</code> representation suitable for this task or I need to represent this in <code>ts</code> format?</li>
</ol>

<p>Here I have attached half-hourly data of 26 days. I have removed timestamps in order to fit the data according to stackExchange limits. This data does not contain any missing readings. It contains 1248 (26 x 48) observations</p>

<pre><code>data &lt;- structure(c(1642.8, 1467.1, 165.57, 1630.99, 1618.65, 1629.29, 
1598.93, 1839.9, 1604.52, 1606.73, 1473.82, 1669.17, 1698.9, 
2111.21, 2056.41, 3671.29, 2808.01, 1336.15, 794.11, 1212.15, 
377.36, 888.54, 174.58, 218.54, 420.76, 389.58, 397.77, 395.31, 
359.11, 364.8, 376.13, 389.37, 929.5, 1702.38, 519.65, 2452.28, 
1354.45, 1842.96, 725.41, 661.11, 528.44, 733.4, 429.51, 310.47, 
279.72, 407.83, 1791.1, 1754.53, 1536.73, 1608.37, 1432.23, 1401.72, 
1582.14, 1558.75, 1536.24, 1745.59, 1375.61, 1556.71, 1671.12, 
1206.77, 1391.84, 876.23, 1617.3, 1638.99, 1833.61, 1591.42, 
1455, 183.87, 177.55, 184.36, 332.99, 352.95, 425.1, 945.67, 
342.3, 348.45, 227.18, 382.15, 268.91, 335.88, 326.94, 233.23, 
169.71, 179.51, 195.3, 207.23, 1681.9, 1493.32, 941.52, 980.36, 
924.31, 379.02, 1229.89, 1590.21, 1250.92, 1149.24, 1124.04, 
993.78, 883.98, 860.69, 934.17, 969.31, 1049.55, 1104.94, 904.3, 
1220.23, 1183.9, 891.26, 825.33, 787.77, 1060.93, 1029.1, 982.25, 
193.13, 182.65, 181.75, 167.68, 165.89, 291.02, 300.2, 418.4, 
297.66, 231.89, 305.66, 701.18, 338.5, 337.24, 332.11, 332.66, 
187.8, 179.03, 130.22, 177.73, 172.24, 173.45, 334.23, 810.53, 
359.41, 330.23, 333.29, 568.85, 2462.46, 1660.32, 1156.13, 1136.2, 
1189.07, 832.73, 181.91, 185.93, 1076.77, 672.78, 1376.71, 1020.17, 
382.18, 1160.74, 791.36, 1569.4, 817.78, 850.71, 747.21, 826.12, 
1306.46, 506.23, 140.05, 132.6, 304.19, 308.14, 406.13, 290.73, 
188.9, 165.59, 174.56, 145.97, 151.83, 142.29, 443.58, 799.9, 
279.36, 223.88, 221.03, 291.26, 374.97, 431.36, 598.98, 625.82, 
1052.02, 2036.83, 1230.03, 1429.81, 1099.34, 1646.03, 1668.56, 
1631.79, 1604.04, 2849.49, 2998.63, 2476.96, 1601.04, 1216.42, 
2004.2, 1868.51, 1961.91, 1813.35, 1500.22, 1276.94, 1369.29, 
632.43, 238.15, 488.76, 467.6, 330.27, 144.67, 153.36, 924.12, 
1348.18, 799.01, 524.3, 420.5, 264.34, 283.86, 198.95, 206.52, 
217.33, 356.16, 207.83, 197.6, 194.03, 193.01, 249.85, 271.22, 
244.25, 442.5, 660.49, 245.66, 356.13, 443.32, 336.15, 849.74, 
1709.21, 1542.09, 1315.69, 2628.6, 2261.8, 1576.42, 1776.48, 
1239.64, 1401.12, 1106.17, 1378.55, 1315.57, 1141, 1642.63, 2484.13, 
1968.94, 3059.42, 1317.32, 905.05, 484.42, 486.86, 96.79, 183.45, 
173.94, 342, 255.96, 320.54, 106.19, 147.88, 150.77, 176.17, 
344.75, 371.73, 309.46, 237.86, 187.32, 202.61, 292.5, 248.28, 
259.3, 283.67, 365.09, 230.47, 326.15, 350.92, 335.42, 419.39, 
345.31, 1093.22, 1392.87, 1298.11, 919.16, 1654.53, 1045.99, 
558.42, 437.29, 857.9, 758.34, 1220.04, 1390.62, 956.74, 909.93, 
584.67, 409.87, 387.3, 387.93, 1276.28, 871.06, 413.8, 313.2, 
199.5, 330.21, 210.9, 358.28, 352.13, 233.62, 259.18, 123.57, 
255.58, 411.78, 427.65, 318.95, 298.5, 283.23, 279.85, 200.45, 
205.97, 254.24, 307.98, 1090.53, 289.71, 215.14, 286.63, 328.55, 
288.96, 1281.19, 1354.8, 1302.05, 1254.46, 261.95, 270.09, 243.28, 
696.61, 314.27, 241.73, 245.68, 157.74, 222.55, 294.36, 185.46, 
203.49, 182.14, 246.69, 178.26, 397.5, 330.2, 212.02, 248.72, 
265.48, 249.37, 130.59, 248.97, 279.94, 319.07, 358.5, 278.98, 
251.92, 304.66, 455.05, 365.95, 340.93, 287.51, 264.82, 260.18, 
34.35, 35.11, 184.09, 247.18, 160.9, 139.27, 284.96, 296.31, 
252.3, 342.65, 353.03, 380.52, 346.19, 350.06, 218.52, 133.94, 
173.7, 128.26, 167.8, 112.77, 147.8, 129, 170.54, 89.88, 243.08, 
97.61, 190.31, 193.94, 268.17, 233.5, 205.27, 92.29, 167.43, 
168.34, 151.99, 193.84, 379.1, 318.69, 327.28, 487.39, 414.01, 
336.06, 278.02, 168.05, 155.6, 236.4, 264.94, 296.05, 326.46, 
357.43, 356.31, 340.29, 319.81, 312.79, 341.53, 317.36, 309.62, 
440.6, 285.5, 282.06, 288.99, 334.48, 196.54, 144.24, 218.55, 
173.64, 242.29, 251.78, 186.81, 184.36, 141.62, 208.91, 157.53, 
154.03, 139.44, 137.66, 256.75, 1202.05, 177.36, 177.93, 72.83, 
252.9, 231.35, 1090.39, 442.91, 363.12, 248.96, 478.75, 249.64, 
297.29, 227.28, 365.82, 879.7, 488.93, 184.79, 138.13, 151.77, 
123.18, 175.76, 251.84, 208.06, 126.68, 246.3, 307.34, 319.79, 
324.3, 379.6, 309.53, 253.17, 221.91, 228.42, 150.24, 148.59, 
118.79, 86.89, 140.51, 200.43, 212.15, 276.14, 441.81, 125.77, 
152.42, 329.28, 269.21, 177.35, 1106.29, 128.92, 96.35, 63.53, 
520.62, 940.25, 1014.34, 314.99, 390.2, 330.1, 377.04, 341.35, 
342.79, 241.79, 249.9, 391.92, 292.68, 105.02, 179.99, 118.53, 
154.17, 90.53, 206.7, 345.33, 244.75, 291.68, 820.57, 1777.84, 
1805.83, 1753.73, 1416.7, 279.2, 262.82, 1345.88, 467.98, 1136.66, 
170.02, 159.96, 1478.8, 1414.12, 1347.93, 1505.59, 1341.69, 445.53, 
277.59, 1609.61, 1476.45, 244.08, 192.57, 213.55, 439.02, 112.86, 
128.54, 376.09, 251.15, 116.27, 254.82, 302.56, 304.6, 198.5, 
240.05, 219.35, 70.3, 190.96, 211.95, 328.53, 714.62, 3176.56, 
2604.09, 191.65, 145.25, 93.93, 83.6, 81.13, 146.12, 331.75, 
250.24, 1144.53, 1616.47, 1008.7, 316.65, 311.46, 1152.99, 1504.86, 
1543.21, 1081.54, 1428.07, 1358.23, 1349.75, 190.23, 2398.92, 
2196.11, 1466.94, 2249.77, 2150.2, 2542.25, 618.03, 453.22, 880.99, 
1497.86, 440.96, 161.85, 324.88, 434.11, 316.33, 444.66, 359.14, 
277.41, 1237.28, 761.41, 183.53, 309.44, 213.48, 121.64, 346.7, 
149.86, 2060.39, 1102.13, 347.97, 600.24, 912.6, 590.77, 1805.76, 
1673.93, 1573.91, 505.74, 446.76, 1033.41, 1668.68, 1293.9, 383.81, 
1419.99, 1349.4, 711.55, 218.63, 182, 401.93, 1876, 1486.34, 
1543.11, 2313.8, 478.57, 615.19, 542.68, 971.98, 531.11, 766.21, 
489.76, 344.47, 319.86, 321.26, 311.41, 288.67, 310.67, 305.15, 
419.33, 422.84, 950.08, 2188.88, 3454.92, 1989.54, 590.33, 327.05, 
354.78, 578.41, 1583.29, 2016.66, 1481.03, 293.21, 1864.84, 399.65, 
366.76, 357.7, 2074.97, 1626.86, 1133, 1624.61, 1506.93, 628.4, 
1405.68, 217.8, 1223.11, 1356.97, 1171.72, 1182.86, 1642.11, 
2289.02, 814.39, 595.76, 542.78, 1596.41, 884.97, 235.25, 1540.68, 
781.95, 115.71, 1204.98, 718.66, 452.09, 305.58, 444.67, 356.76, 
182.54, 674.47, 153.8, 862.25, 1322.88, 323.33, 1659.64, 496.72, 
304.74, 246.6, 327.12, 239.31, 246.72, 225.72, 234.7, 324.07, 
304.27, 171.86, 97.64, 242.69, 295, 324.53, 513.81, 1100.65, 
1151.77, 231.56, 189.88, 786.3, 1164.87, 676.09, 882.82, 1496.3, 
1027.91, 872.92, 809.1, 840.31, 1302.18, 2055.87, 677.74, 934.66, 
263.91, 186.68, 248.5, 214.62, 371.54, 298, 294.52, 304.86, 1295.77, 
942.5, 305, 265.78, 255.89, 255.63, 151.54, 108.16, 116.81, 100.19, 
224.75, 84.11, 1143.89, 262.15, 784.21, 1728.29, 1506.79, 434.94, 
374.29, 265.43, 560.74, 1651.49, 1063.07, 1054.69, 1298.4, 1261.59, 
1132.75, 692.82, 660.57, 198.25, 97.81, 1258.67, 833.64, 796.35, 
868.76, 999.86, 2240.02, 885.72, 1317.52, 1267.18, 167.93, 133.22, 
364.44, 267.17, 406.13, 412.52, 1036.04, 779.34, 655.43, 1901.2, 
270.18, 266.31, 284.21, 288.66, 135.38, 176.11, 154.86, 160.21, 
146.28, 163.72, 139.75, 278.12, 253.51, 319.62, 396.39, 1662.69, 
1577.09, 1059.71, 241.64, 407.54, 290.49, 846.17, 1325.31, 1418.23, 
1432.5, 1412.6, 1015.87, 1619.88, 1426.58, 1333.32, 1963.35, 
1638.11, 1081.89, 285.56, 1084.58, 2038.77, 1022.39, 1145.92, 
513.87, 107.7, 176.19, 143.77, 374.3, 373.99, 221.76, 148.3, 
331.01, 2323.12, 1502.09, 347.17, 296.7, 306.06, 313.03, 221.69, 
295.35, 301.95, 250.92, 231.54, 140.6, 717.63, 863.5, 402.15, 
1337.78, 1575.44, 1738.49, 1675.57, 1617.66, 1365.58, 242.8, 
286.29, 712.34, 1559.59, 1600.34, 3447.88, 3432.89, 3337.23, 
1472.21, 1323.76, 1265.31, 1221.65, 1312.63, 2016.09, 2972.13, 
1451.67, 735.67, 130.13, 379.87, 162.21, 226.48, 417.18, 357.51, 
346.37, 200.89, 190.15, 276.05, 942.74, 1471.99, 1047.53, 1240.06, 
742.62, 169.34, 144.28, 220.58, 165.23, 344.8, 227.47, 254.64, 
742.37, 1809.01, 436.11, 1692.87, 1697.74, 1474.91, 1635.68, 
1664.2, 1489.85, 1316.4, 1364.07, 1510.02, 1497.89, 1709.17, 
2846.71, 2736.08, 1015.43, 1017.08, 1195.21, 751.18, 523.89, 
199.69, 2148.71, 1151.39, 1182.84, 788.91, 259.31, 146.29, 141.09, 
299.38, 349.53, 404.08, 449.9, 391.77, 251.89, 222.25, 281.04, 
565.4, 371.24, 219.75, 324.45, 227, 157.67, 212.48, 201.69, 140.84, 
220.67, 187.64, 399.79, 157.92, 275.49, 326.99, 1340.51, 1578.68, 
1599.41, 1667.74, 1129.07, 1312.55, 1393.09, 1368.69, 1130.85, 
968.71, 1130.2, 1223.1, 1124.5, 1077.09, 1052.42, 1255.31, 918.21, 
1263.93, 706.21, 3080.29, 1620.18, 1122.48, 750.06, 262.89, 110.02, 
236.83, 413, 227.53, 355, 277.51, 258.03, 368.44, 656.68, 1808.17, 
1493.41, 1272.21, 330.67, 1674.18, 719.45, 1089.68, 784.01, 275.73, 
312.69, 345.11, 761.8, 1020.11, 259.16, 345.98, 270.23, 580.15, 
1303.68, 1659.7, 1732.94, 204.9, 373.58, 373.36, 1381.52, 1437.74, 
1262.78, 1264.6, 1184.54, 1175.12, 857.09, 1428.34, 841.92, 232.47, 
223.22, 473.24, 382.7, 189.84, 1737.48, 1689.34, 378.48, 872.56, 
180.12, 363.03, 301.38, 412.57, 401.17, 387.35, 417.63, 300.61, 
376.82, 284.31, 232.31, 269.96, 188.41, 203.79, 134.88, 193.66, 
57.86, 89.71, 167.66, 60.84, 197.03, 703.66, 1638.7, 1467.03, 
347.22, 1397.23, 1511.92, 1362.25, 1397.18, 1106.19, 826.5, 1033.04, 
1039.46, 584.98, 706.35, 548.07, 373.39, 681.6, 1231.28, 288.94, 
649.36, 79.28, 209.23, 290.75, 304.12, 132.57, 91.2, 355.37, 
197.45, 343.17, 339.2, 284.65, 229, 234.73, 322.36, 323.43, 295.1, 
197.03, 308.17, 223.88, 235.08, 225.16, 172.83, 236.26, 135.17, 
394.89, 479.2, 315.34, 280.9, 282.36, 204.78, 367.24, 1712.29, 
1521, 1686.09, 960.19, 1019.12, 1062.77, 851.88, 1369.98, 689.2, 
580.5, 751.74, 547.67, 556.64, 493.85, 404.15, 428.07, 716.2, 
1442.05, 1045.81, 1497.07, 567.59, 155.07, 537.72, 446.03, 282.57, 
642.88, 409.37, 338.91, 173.89, 358.63, 195.42, 209.95, 186.44, 
152.34, 105.51, 132.26, 82.14, 122.88, 149.38, 211.42, 350.17, 
429.72, 336.4, 982.21, 1436.25, 1726.87, 1830.42, 1282.05, 1293.4, 
1121.01, 946.2, 707.1, 154.53, 767.56, 607.78, 448, 288.23, 270.32, 
223.93, 166.22, 262.45, 223.74, 159.31, 210.44, 257.94, 183.99, 
151.38, 206.11, 193.43, 388.95, 577.98, 304.64, 285.13, 256.59, 
420.26, 289.34, 356.02, 358.08, 325.22, 275.95, 164.46, 213.23, 
142.99, 221.66, 270.61, 206.56, 213.68, 254.33, 250.15, 267.99, 
403.95, 671.2, 1574.11, 396.34, 477.88, 631.08, 618.25, 1366.88, 
298.19, 287.05, 290.38, 332.44, 235.9, 229.79, 831.6, 1320.86, 
477.31, 944.84, 547.33, 411.21, 705.43, 873.49, 572.81, 585.36, 
1229.69, 701.01, 653.49, 74.81, 162.47, 179.54, 330.27, 544.51, 
332.41, 296.66, 130.66, 1055.61, 556.79, 265.43, 383.44, 398.22, 
362.66, 223.99, 130.35, 193.67, 217.68, 273.3, 247.84, 161.66, 
320.08, 322.52, 274.61, 811.44, 353.85, 323.41, 383.61, 389.5
), .Dim = c(1248L, 1L), .Dimnames = list(NULL, ""power""))
</code></pre>
"
"0.0694210134500623","0.0907218423253029","65865","<p>This is the dataset on which I am working currently, which is production data.</p>

<p>Data:</p>

<pre><code>&gt; test.ts
        Jan    Feb    Mar    Apr    May    Jun    Jul    Aug    Sep    Oct    Nov    Dec
1990                                                            0.0   10.8  180.0  418.2
1991  561.9  517.9  531.3  448.1  254.9   49.0    3.2    0.0    0.0   10.4  207.7  526.2
1992  597.2  581.5  596.4  518.4  378.3  209.9   32.1    0.0    0.0    7.9  166.7  571.7
1993  650.4  578.5  565.7  280.5   35.3    0.7    0.0    0.0    0.0   39.5  289.2  638.9
1994  643.8  533.8  410.9  159.3    0.0    0.0    0.0    0.0    0.0   38.3  322.8  684.9
1995  695.6  665.8  640.2  415.4  113.0   20.7   12.1    0.0    0.0   13.6  316.3  677.5
1996  754.5  683.4  719.6    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  678.0
1997  774.5  808.1  847.9  677.8  208.7    9.9    0.0    0.0    0.0    5.2  296.4  794.9
1998  952.0  873.1  732.0  264.6    3.9    0.0    0.0    0.0    0.0    0.0  245.8  833.0
1999  843.5  812.3  708.3  275.2   10.8    0.0    0.0    0.0    0.0    5.4  300.4  884.9
2000  949.0  898.6  892.7  474.7  130.0   19.8    0.0    0.0    0.0    8.5  367.2 1000.8
2001 1092.1  987.7  864.3  392.8   41.8    0.0    0.0    0.0    0.0    7.0  425.0  968.0
2002  983.6  925.0 1018.0  696.0  209.0   26.0    0.0    0.0    0.0    0.0   63.0  823.0
2003 1066.0  930.0  929.0 1071.0  614.0  125.0   29.0    0.0    0.0    0.5  300.0 1005.8
2004 1043.0 1051.9  863.2  279.1    8.0    0.0    0.0    0.0    0.0   67.8  597.1 1120.3
2005 1087.9 1015.9  855.7  292.9    0.8    0.0    0.0    0.0    0.0   78.3  683.3 1139.2
2006 1185.5 1162.1 1131.3  386.9   16.4    1.2    0.0    0.0    0.0    7.1  728.5 1493.0
2007 1572.9 1341.0 1652.9 1279.3  386.4   14.3    0.0    0.0    0.0    0.0  102.5 1570.1
2008 1864.7 1786.7 1523.9  422.7   48.1    0.8    0.0    0.0    0.0    0.0  192.4 1556.9
2009 1260.8  763.8  284.1    6.1    0.0    0.0    0.0    0.0    0.0    0.0   73.8 1495.6
2010 1280.8 1248.8  887.2  185.6    7.3    0.0    0.0    0.0    0.0    0.8  182.0 1524.9
2011 1461.5 1497.7 1111.5  108.6    0.0    0.0    0.0    0.0    0.0    2.9  519.3 1652.5
2012 1552.5 1563.2 1380.4  295.2    7.7    0.0    0.0    0.0    0.0    0.1  225.0 1677.6
2013 1686.2 1420.0 1691.0  795.0    0.0  
</code></pre>

<p>I used auto.arima() from forecast package.</p>

<p>Code:</p>

<pre><code>ARIMAfit &lt;- auto.arima(test.ts)
test.ar &lt;- forecast(ARIMAfit, level=70, h=12)
</code></pre>

<p>Following is the output I got</p>

<p>Output:</p>

<pre><code>&gt; test.ar
         Point Forecast       Lo 70     Hi 70
Jun 2013      -4.429870  -186.37952  177.5198
Jul 2013      -4.429870  -261.74553  252.8858
Aug 2013      -4.429870  -319.57590  310.7162
Sep 2013      -4.429870  -368.32916  359.4694
Oct 2013      -3.416802  -410.26858  403.4350
Nov 2013     296.121405  -149.56239  741.8052
Dec 2013    1505.197792  1023.80428 1986.5913
Jan 2014    1477.195886   962.56457 1991.8272
Feb 2014    1327.574562   781.72562 1873.4235
Mar 2014    1423.251183   847.87588 1998.6265
Apr 2014     550.206881   -53.25183 1153.6656
May 2014      -1.892754  -632.18482  628.3993
</code></pre>

<p>Questions:</p>

<ol>
<li><p>Why does the output shows negative value, when there has been no negative value in the historic data? The data is of production, which cannot be negative.</p></li>
<li><p>Is there any other model class which handles ""zero"" values appropriately?</p></li>
</ol>

<p>Kindly help.</p>
"
"0.10976425998969","0.143443827637312","154104","<p>I am trying to estimate parameter $d$ for ARFIMA model using different methods: Hurst, ML, fdSperio, fdGPH and the function <code>arfima</code> which selects the best fit automatically. The results shown are too large in difference, and I wonder why. </p>

<p>I attached an image of my coding:</p>

<p><img src=""http://i.stack.imgur.com/ZKz6B.png"" alt=""""></p>

<p>Especially parameter $d$ from <code>arfima</code>, the value is too extreme. Other values are having big differences as well.</p>

<p>Also, this is brief explanation on what I am doing:</p>

<ol>
<li>I am fitting ARFIMA model with my data: monthly Brent Oil Price.</li>
<li>Before that, I will need to estimate the values of $d$, and I am doing that with different methods as mentioned above.</li>
<li>After the values of $d$ are obtained, I will use <code>diffseries</code> to difference the series manually.</li>
<li>Then I will fit those differenced series into ARIMA model to compare their AICs, to find the best model.</li>
</ol>

<p>Am I going to the right way? Please correct me if I am wrong. </p>
"
"NaN","NaN","212110","<p><code>auto.arima</code> gives me that the best model is <code>arima(0,1,0)</code>. But using Arima and fitting <code>(0,1,0)(0,1,0)[6]</code> I have a better fit according to BIC, AIC, AICc. <code>auto.arima</code> is not evaluating that model</p>

<p>Here is the code</p>

<pre><code>model1&lt;-auto.arima(data.set,xreg = X1,trace = T,
                                   stepwise=F,approximation = F,
                                   parallel = F, num.cores = 2,
                   allowdrift = T,allowmean = T)
</code></pre>

<p>Why is <code>auto.arima</code> not evaluating <code>(0,1,0)(0,1,0)[6]</code>?</p>
"
"NaN","NaN","89316","<p>I am using <code>auto.arima()</code> for prediction, and getting the following warning message. I want to know if I can ignore this warning message or if I should be worried.</p>

<p><code>Warning message:</code><br>
<code>In auto.arima(forecast_data_ts) :</code><br>
<code>Unable to fit final model using maximum likelihood. AIC value approximated</code></p>
"
"0.208263040350187","0.21168429875904","89531","<p>I'm expanding a question I posed earlier because I think it was lacking detail. </p>

<p>I'm attempting to forecast daily demand for a restaurant that sells take away food, primarily to office workers on their lunch breaks. They are located in the downtown core of a major city.</p>

<p>They are only open on workdays - no holidays, no weekends. I'm familiar with models that take into account seasonality and trend - Holt-Winters triple exponential smoothing, for example. I'm also familiar with models that take into account complex seasonality and trend - the TBATS package for R, for example.</p>

<p>My problem is that I've identified 8 components that determine sales on a given day:</p>

<ol>
<li>The yearly seasonal component. Sales are lower in the summer, for example, when many office workers are on vacation.</li>
<li>The weekly component. Sales very obviously peak on Thursdays (in the absence of other effects - see below)</li>
<li>The <em>Friday-Long-Weekend-Effect</em>. The weekly sales pattern changes if the coming Friday is a holiday. Wednesday will typically have higher sales, for example.</li>
<li>The <em>Post-Friday-Long-Weekend-Effect</em>. The weekly sales pattern changes if the week before was shortened due to the Friday being a holiday.</li>
<li>The <em>Monday-Long-Weekend-Effect</em>. The weekly sales pattern changes in week $t$ if the Monday in week $t+1$ is a holiday. For example, sales are much lower on Fridays preceding Monday-Long-Weekends. Presumably people are leaving the office early and skipping lunch.</li>
<li>The <em>Post-Monday-Long-Weekend-Effect</em>. The weekly sales pattern changes if the week is shortened due to the Monday being a holiday.</li>
<li>The trend component. </li>
<li>The noise component.</li>
</ol>

<p>If holidays fell on the same date every year, then the ""long-weekend-effects"" would be captured in the yearly seasonal component. However, they don't. </p>

<p>My first thought was to include dummy variables. For example, let $X_{M+1}$ be the ""Monday-long-weekend-effect"" component, and $\beta_{M+1}$ be the associated coefficient, for a given day. Then for the Friday preceding a Monday-Long-Weekend, $X_{M+1}=1$, and for a Friday not preceding a Monday-Long-Weekend, $X=0$.</p>

<p>I'm only using three years of data, so it would be easy for me to change the $X_{M+1}$ values to 0 or 1 by hand for each year. However, I don't know how to include such dummy variables in models like those that I've mentioned.</p>

<p>Any input as to a model that can take into account the components I've mentioned would be greatly appreciated. It seems like I need to capture moving-holiday-effects, day-of-the-week effects, seasonal patterns, and trend, all in one.</p>

<p><strong>Question: Is there a model I can use that can be implemented in R and take into account the components I've listed?</strong></p>

<p><em>My background: I'm a forth year mathematics and economics student. I've also taken statistics classes, and I'm using R to perform my analysis. This is for a final report for a forth year data analysis class.</em></p>
"
"0.196352277469526","0.208487597207365","89808","<p>I am trying to model daily sales for a take out restaurant. They are only open on business days - no holidays or weekends - as their primary clients are office workers on their lunch breaks.</p>

<p>Below is what two years of the daily sales time series looks like.</p>

<p><img src=""http://i.stack.imgur.com/UhHDR.png"" alt=""enter image description here""></p>

<p>The days with zero sales, as you can see above, are the days that the restaurant was closed due to a public holiday (Easter Monday etc.). There is definitely a weekly pattern: sales tend to peak on Thursdays. Furthermore, the presence of a holiday changes the sales pattern in the surrounding weeks (the week before and the week after).</p>

<p>You might notice that there are sales spikes before or after certain holidays. An example of this: if Monday is a holiday, sales tend to be much lower on the Friday before that long weekend - presumably office workers leaving work early.</p>

<p>There are also yearly seasonal patterns. Sales are lower in the summer, for example, presumably, in part, because many office workers are taking their vacations.</p>

<p>My approach has been to use a ARIMAX model to fit the data (using R). I've followed the approach suggested by Rob Hyndman <a href=""http://stats.stackexchange.com/questions/41070/how-to-setup-xreg-argument-in-auto-arima-in-r"">here</a>. The difference is that I'm using only Mon-Fri, so my frequency is 5, and I've added dummy variables for all of the days that the restaurant is closed (holidays).</p>

<p>The model fit is not very good so far, of course. I haven't done anything to take into account the effect of a holiday on the surrounding days. Further, I'm including the holidays as days with sales equal to zero, so this must throw off the model.</p>

<p>Here is what R returns:</p>

<pre><code>ARIMA(0,1,1)(1,0,1)[5]                    

Coefficients:
     ma1     sar1    sma1      Mon       Tue       Wed      Thu     Day    Newyears   FamilyDay  GoodFriday      Easter  VictoriaDay   CanadaDay    CivicDay
  -0.804  -0.2608  0.3255  54.4530  113.8052  152.0052  -6.3025  0.0388  -1545.1973  -1604.5038  -1581.6740  -1586.8710    -1628.253  -1437.6075  -1181.0054
s.e.   0.028   0.4641  0.4529  23.2788   23.3748   23.4900  23.4367  1.5546    117.6128    113.6446    113.8825    114.5609      114.786    112.7561    114.0031
   LabourDay  Thanksgiving
  -1310.3416    -1332.8028
s.e.    113.5081      113.5179

sigma^2 estimated as 28269:  log likelihood=-3305.08
AIC=6646.16   AICc=6647.56   BIC=6722.2
</code></pre>

<p>I think I should include the month of the year as a dummy variable to capture yearly seasonal effects as well.</p>

<p>My questions:</p>

<ol>
<li>What can I do to capture ""long weekend effects""? Should I included a dummy variable for every Friday that proceeds a long weekend etc?</li>
<li>How should I deal with the holidays that the restaurant was closed for? If I remove them, then the week lengths will not be the same. If I include them, then they are outliers that throw everything off.</li>
<li>What else can I do to improve my model?</li>
</ol>

<p>Thanks very much for any input.</p>
"
"NaN","NaN","119931","<p>How can I in R fit a time series, $x_t$, with external regressors, $v_t$, and an autoregressive error? This time series model is given as follows,
$x_t = \beta v_t + \epsilon_t$ where $\epsilon_t = w_t + \sum_{i = 1}^p \gamma_i\epsilon_{t - i}$ and $w_t \sim N(0, \sigma^2)$.</p>
"
"0.0490880693673816","0.0641500299099584","230618","<p>I have a time series of around 45 data points which represent sales in 50 consecutive months. However, the first 25 months are highly influenced by hiring of new salesmen every month. The first 25 months almost show linearly increasing sales. The last 20 months, are however, more like a time series. What should be my approach for such a problem?</p>
"
"0.138842026900125","0.181443684650606","90304","<p>I perform some time series fitting with the help of the forecast and urca packages. I have a question regarding the correspondance between results coming from statistical test such as KPSS, ADF or ERS, and the results arising from automatical selection procedures such as auto.arima from the forecast package. </p>

<p>Suppose we are in a situation where the outcome of the KPSS test is that the series does not need to be differentiated. I then apply the ADF test, which shows that the series is stationnary up to a trend (at+b).</p>

<p>In that situation, I could force the auto.arima function to consider only ARIMA models that include a linear trend, but I can also explore all the models (including or not the trend) and let auto.arima choose the best one. And there are cases where the outcome is in contradiction with the test result, e.g. the chosen model does not include a trend while the test advises to include one.</p>

<p>What is the best practice in such a case ? Should the test result be prevalent to the selection of the ARIMA model, or is it best to consider all possible models and choose the best one ?</p>

<p>Thanks.</p>
"
"0.208263040350187","0.241924912867474","213192","<p>I have half-hourly electricity data of several homes for a duration of one month. Also, I have ambient temperature at same sampling rate. Now, I need to make half-hourly forecasts using historical electricity data and forecasted temperature for coming day.</p>

<p>Currently, I am facing a problem with seasonality, i.e., in some homes I observe (via visual inspection) day-wise seasonality and in some other homes no pattern is found. <a href=""http://stats.stackexchange.com/a/212797/60072"">Stephan</a> suggested to check for weekly seasonality as well, but visually I do not find any. So, I thought to try models with different forced seasonalities to find the prediction accuracy. I can think of two options:</p>

<ol>
<li>Model_1 with daily seasonality (frequency = 48 observations)</li>
<li>Model_2 with weekly and daily seasonality (48, 7*48)</li>
</ol>

<p>Keeping the above approach in mind, I am facing the following issue:</p>

<ol>
<li>For Model_1, I can use <code>auto.arima()</code> with <code>xreg</code> option to specify an extra <code>temperature</code> predictor. However, this only works for a single seasonality.</li>
<li>For Model_2, I tried to use the <code>tbats()</code> forecasting function, which models multiple seasonalities, but does not allow extra predictor variables. </li>
</ol>

<p>Is there a forecasting function which allows <em>both</em> multiple predictors and multiple seasonalities?</p>

<p>Here is the electricity data of one month. </p>

<pre><code>    data &lt;- structure(c(1642.8, 1467.1, 165.57, 1630.99, 1618.65, 1629.29, 
        1598.93, 1839.9, 1604.52, 1606.73, 1473.82, 1669.17, 1698.9, 
        2111.21, 2056.41, 3671.29, 2808.01, 1336.15, 794.11, 1212.15, 
        377.36, 888.54, 174.58, 218.54, 420.76, 389.58, 397.77, 395.31, 
        359.11, 364.8, 376.13, 389.37, 929.5, 1702.38, 519.65, 2452.28, 
        1354.45, 1842.96, 725.41, 661.11, 528.44, 733.4, 429.51, 310.47, 
        279.72, 407.83, 1791.1, 1754.53, 1536.73, 1608.37, 1432.23, 1401.72, 
        1582.14, 1558.75, 1536.24, 1745.59, 1375.61, 1556.71, 1671.12, 
        1206.77, 1391.84, 876.23, 1617.3, 1638.99, 1833.61, 1591.42, 
        1455, 183.87, 177.55, 184.36, 332.99, 352.95, 425.1, 945.67, 
        342.3, 348.45, 227.18, 382.15, 268.91, 335.88, 326.94, 233.23, 
        169.71, 179.51, 195.3, 207.23, 1681.9, 1493.32, 941.52, 980.36, 
        924.31, 379.02, 1229.89, 1590.21, 1250.92, 1149.24, 1124.04, 
        993.78, 883.98, 860.69, 934.17, 969.31, 1049.55, 1104.94, 904.3, 
        1220.23, 1183.9, 891.26, 825.33, 787.77, 1060.93, 1029.1, 982.25, 
        193.13, 182.65, 181.75, 167.68, 165.89, 291.02, 300.2, 418.4, 
        297.66, 231.89, 305.66, 701.18, 338.5, 337.24, 332.11, 332.66, 
        187.8, 179.03, 130.22, 177.73, 172.24, 173.45, 334.23, 810.53, 
        359.41, 330.23, 333.29, 568.85, 2462.46, 1660.32, 1156.13, 1136.2, 
        1189.07, 832.73, 181.91, 185.93, 1076.77, 672.78, 1376.71, 1020.17, 
        382.18, 1160.74, 791.36, 1569.4, 817.78, 850.71, 747.21, 826.12, 
        1306.46, 506.23, 140.05, 132.6, 304.19, 308.14, 406.13, 290.73, 
        188.9, 165.59, 174.56, 145.97, 151.83, 142.29, 443.58, 799.9, 
        279.36, 223.88, 221.03, 291.26, 374.97, 431.36, 598.98, 625.82, 
        1052.02, 2036.83, 1230.03, 1429.81, 1099.34, 1646.03, 1668.56, 
        1631.79, 1604.04, 2849.49, 2998.63, 2476.96, 1601.04, 1216.42, 
        2004.2, 1868.51, 1961.91, 1813.35, 1500.22, 1276.94, 1369.29, 
        632.43, 238.15, 488.76, 467.6, 330.27, 144.67, 153.36, 924.12, 
        1348.18, 799.01, 524.3, 420.5, 264.34, 283.86, 198.95, 206.52, 
        217.33, 356.16, 207.83, 197.6, 194.03, 193.01, 249.85, 271.22, 
        244.25, 442.5, 660.49, 245.66, 356.13, 443.32, 336.15, 849.74, 
        1709.21, 1542.09, 1315.69, 2628.6, 2261.8, 1576.42, 1776.48, 
        1239.64, 1401.12, 1106.17, 1378.55, 1315.57, 1141, 1642.63, 2484.13, 
        1968.94, 3059.42, 1317.32, 905.05, 484.42, 486.86, 96.79, 183.45, 
        173.94, 342, 255.96, 320.54, 106.19, 147.88, 150.77, 176.17, 
        344.75, 371.73, 309.46, 237.86, 187.32, 202.61, 292.5, 248.28, 
        259.3, 283.67, 365.09, 230.47, 326.15, 350.92, 335.42, 419.39, 
        345.31, 1093.22, 1392.87, 1298.11, 919.16, 1654.53, 1045.99, 
        558.42, 437.29, 857.9, 758.34, 1220.04, 1390.62, 956.74, 909.93, 
        584.67, 409.87, 387.3, 387.93, 1276.28, 871.06, 413.8, 313.2, 
        199.5, 330.21, 210.9, 358.28, 352.13, 233.62, 259.18, 123.57, 
        255.58, 411.78, 427.65, 318.95, 298.5, 283.23, 279.85, 200.45, 
        205.97, 254.24, 307.98, 1090.53, 289.71, 215.14, 286.63, 328.55, 
        288.96, 1281.19, 1354.8, 1302.05, 1254.46, 261.95, 270.09, 243.28, 
        696.61, 314.27, 241.73, 245.68, 157.74, 222.55, 294.36, 185.46, 
        203.49, 182.14, 246.69, 178.26, 397.5, 330.2, 212.02, 248.72, 
        265.48, 249.37, 130.59, 248.97, 279.94, 319.07, 358.5, 278.98, 
        251.92, 304.66, 455.05, 365.95, 340.93, 287.51, 264.82, 260.18, 
        34.35, 35.11, 184.09, 247.18, 160.9, 139.27, 284.96, 296.31, 
        252.3, 342.65, 353.03, 380.52, 346.19, 350.06, 218.52, 133.94, 
        173.7, 128.26, 167.8, 112.77, 147.8, 129, 170.54, 89.88, 243.08, 
        97.61, 190.31, 193.94, 268.17, 233.5, 205.27, 92.29, 167.43, 
        168.34, 151.99, 193.84, 379.1, 318.69, 327.28, 487.39, 414.01, 
        336.06, 278.02, 168.05, 155.6, 236.4, 264.94, 296.05, 326.46, 
        357.43, 356.31, 340.29, 319.81, 312.79, 341.53, 317.36, 309.62, 
        440.6, 285.5, 282.06, 288.99, 334.48, 196.54, 144.24, 218.55, 
        173.64, 242.29, 251.78, 186.81, 184.36, 141.62, 208.91, 157.53, 
        154.03, 139.44, 137.66, 256.75, 1202.05, 177.36, 177.93, 72.83, 
        252.9, 231.35, 1090.39, 442.91, 363.12, 248.96, 478.75, 249.64, 
        297.29, 227.28, 365.82, 879.7, 488.93, 184.79, 138.13, 151.77, 
        123.18, 175.76, 251.84, 208.06, 126.68, 246.3, 307.34, 319.79, 
        324.3, 379.6, 309.53, 253.17, 221.91, 228.42, 150.24, 148.59, 
        118.79, 86.89, 140.51, 200.43, 212.15, 276.14, 441.81, 125.77, 
        152.42, 329.28, 269.21, 177.35, 1106.29, 128.92, 96.35, 63.53, 
        520.62, 940.25, 1014.34, 314.99, 390.2, 330.1, 377.04, 341.35, 
        342.79, 241.79, 249.9, 391.92, 292.68, 105.02, 179.99, 118.53, 
        154.17, 90.53, 206.7, 345.33, 244.75, 291.68, 820.57, 1777.84, 
        1805.83, 1753.73, 1416.7, 279.2, 262.82, 1345.88, 467.98, 1136.66, 
        170.02, 159.96, 1478.8, 1414.12, 1347.93, 1505.59, 1341.69, 445.53, 
        277.59, 1609.61, 1476.45, 244.08, 192.57, 213.55, 439.02, 112.86, 
        128.54, 376.09, 251.15, 116.27, 254.82, 302.56, 304.6, 198.5, 
        240.05, 219.35, 70.3, 190.96, 211.95, 328.53, 714.62, 3176.56, 
        2604.09, 191.65, 145.25, 93.93, 83.6, 81.13, 146.12, 331.75, 
        250.24, 1144.53, 1616.47, 1008.7, 316.65, 311.46, 1152.99, 1504.86, 
        1543.21, 1081.54, 1428.07, 1358.23, 1349.75, 190.23, 2398.92, 
        2196.11, 1466.94, 2249.77, 2150.2, 2542.25, 618.03, 453.22, 880.99, 
        1497.86, 440.96, 161.85, 324.88, 434.11, 316.33, 444.66, 359.14, 
        277.41, 1237.28, 761.41, 183.53, 309.44, 213.48, 121.64, 346.7, 
        149.86, 2060.39, 1102.13, 347.97, 600.24, 912.6, 590.77, 1805.76, 
        1673.93, 1573.91, 505.74, 446.76, 1033.41, 1668.68, 1293.9, 383.81, 
        1419.99, 1349.4, 711.55, 218.63, 182, 401.93, 1876, 1486.34, 
        1543.11, 2313.8, 478.57, 615.19, 542.68, 971.98, 531.11, 766.21, 
        489.76, 344.47, 319.86, 321.26, 311.41, 288.67, 310.67, 305.15, 
        419.33, 422.84, 950.08, 2188.88, 3454.92, 1989.54, 590.33, 327.05, 
        354.78, 578.41, 1583.29, 2016.66, 1481.03, 293.21, 1864.84, 399.65, 
        366.76, 357.7, 2074.97, 1626.86, 1133, 1624.61, 1506.93, 628.4, 
        1405.68, 217.8, 1223.11, 1356.97, 1171.72, 1182.86, 1642.11, 
        2289.02, 814.39, 595.76, 542.78, 1596.41, 884.97, 235.25, 1540.68, 
        781.95, 115.71, 1204.98, 718.66, 452.09, 305.58, 444.67, 356.76, 
        182.54, 674.47, 153.8, 862.25, 1322.88, 323.33, 1659.64, 496.72, 
        304.74, 246.6, 327.12, 239.31, 246.72, 225.72, 234.7, 324.07, 
        304.27, 171.86, 97.64, 242.69, 295, 324.53, 513.81, 1100.65, 
        1151.77, 231.56, 189.88, 786.3, 1164.87, 676.09, 882.82, 1496.3, 
        1027.91, 872.92, 809.1, 840.31, 1302.18, 2055.87, 677.74, 934.66, 
        263.91, 186.68, 248.5, 214.62, 371.54, 298, 294.52, 304.86, 1295.77, 
        942.5, 305, 265.78, 255.89, 255.63, 151.54, 108.16, 116.81, 100.19, 
        224.75, 84.11, 1143.89, 262.15, 784.21, 1728.29, 1506.79, 434.94, 
        374.29, 265.43, 560.74, 1651.49, 1063.07, 1054.69, 1298.4, 1261.59, 
        1132.75, 692.82, 660.57, 198.25, 97.81, 1258.67, 833.64, 796.35, 
        868.76, 999.86, 2240.02, 885.72, 1317.52, 1267.18, 167.93, 133.22, 
        364.44, 267.17, 406.13, 412.52, 1036.04, 779.34, 655.43, 1901.2, 
        270.18, 266.31, 284.21, 288.66, 135.38, 176.11, 154.86, 160.21, 
        146.28, 163.72, 139.75, 278.12, 253.51, 319.62, 396.39, 1662.69, 
        1577.09, 1059.71, 241.64, 407.54, 290.49, 846.17, 1325.31, 1418.23, 
        1432.5, 1412.6, 1015.87, 1619.88, 1426.58, 1333.32, 1963.35, 
        1638.11, 1081.89, 285.56, 1084.58, 2038.77, 1022.39, 1145.92, 
        513.87, 107.7, 176.19, 143.77, 374.3, 373.99, 221.76, 148.3, 
        331.01, 2323.12, 1502.09, 347.17, 296.7, 306.06, 313.03, 221.69, 
        295.35, 301.95, 250.92, 231.54, 140.6, 717.63, 863.5, 402.15, 
        1337.78, 1575.44, 1738.49, 1675.57, 1617.66, 1365.58, 242.8, 
        286.29, 712.34, 1559.59, 1600.34, 3447.88, 3432.89, 3337.23, 
        1472.21, 1323.76, 1265.31, 1221.65, 1312.63, 2016.09, 2972.13, 
        1451.67, 735.67, 130.13, 379.87, 162.21, 226.48, 417.18, 357.51, 
        346.37, 200.89, 190.15, 276.05, 942.74, 1471.99, 1047.53, 1240.06, 
        742.62, 169.34, 144.28, 220.58, 165.23, 344.8, 227.47, 254.64, 
        742.37, 1809.01, 436.11, 1692.87, 1697.74, 1474.91, 1635.68, 
        1664.2, 1489.85, 1316.4, 1364.07, 1510.02, 1497.89, 1709.17, 
        2846.71, 2736.08, 1015.43, 1017.08, 1195.21, 751.18, 523.89, 
        199.69, 2148.71, 1151.39, 1182.84, 788.91, 259.31, 146.29, 141.09, 
        299.38, 349.53, 404.08, 449.9, 391.77, 251.89, 222.25, 281.04, 
        565.4, 371.24, 219.75, 324.45, 227, 157.67, 212.48, 201.69, 140.84, 
        220.67, 187.64, 399.79, 157.92, 275.49, 326.99, 1340.51, 1578.68, 
        1599.41, 1667.74, 1129.07, 1312.55, 1393.09, 1368.69, 1130.85, 
        968.71, 1130.2, 1223.1, 1124.5, 1077.09, 1052.42, 1255.31, 918.21, 
        1263.93, 706.21, 3080.29, 1620.18, 1122.48, 750.06, 262.89, 110.02, 
        236.83, 413, 227.53, 355, 277.51, 258.03, 368.44, 656.68, 1808.17, 
        1493.41, 1272.21, 330.67, 1674.18, 719.45, 1089.68, 784.01, 275.73, 
        312.69, 345.11, 761.8, 1020.11, 259.16, 345.98, 270.23, 580.15, 
        1303.68, 1659.7, 1732.94, 204.9, 373.58, 373.36, 1381.52, 1437.74, 
        1262.78, 1264.6, 1184.54, 1175.12, 857.09, 1428.34, 841.92, 232.47, 
        223.22, 473.24, 382.7, 189.84, 1737.48, 1689.34, 378.48, 872.56, 
        180.12, 363.03, 301.38, 412.57, 401.17, 387.35, 417.63, 300.61, 
        376.82, 284.31, 232.31, 269.96, 188.41, 203.79, 134.88, 193.66, 
        57.86, 89.71, 167.66, 60.84, 197.03, 703.66, 1638.7, 1467.03, 
        347.22, 1397.23, 1511.92, 1362.25, 1397.18, 1106.19, 826.5, 1033.04, 
        1039.46, 584.98, 706.35, 548.07, 373.39, 681.6, 1231.28, 288.94, 
        649.36, 79.28, 209.23, 290.75, 304.12, 132.57, 91.2, 355.37, 
        197.45, 343.17, 339.2, 284.65, 229, 234.73, 322.36, 323.43, 295.1, 
        197.03, 308.17, 223.88, 235.08, 225.16, 172.83, 236.26, 135.17, 
        394.89, 479.2, 315.34, 280.9, 282.36, 204.78, 367.24, 1712.29, 
        1521, 1686.09, 960.19, 1019.12, 1062.77, 851.88, 1369.98, 689.2, 
        580.5, 751.74, 547.67, 556.64, 493.85, 404.15, 428.07, 716.2, 
        1442.05, 1045.81, 1497.07, 567.59, 155.07, 537.72, 446.03, 282.57, 
        642.88, 409.37, 338.91, 173.89, 358.63, 195.42, 209.95, 186.44, 
        152.34, 105.51, 132.26, 82.14, 122.88, 149.38, 211.42, 350.17, 
        429.72, 336.4, 982.21, 1436.25, 1726.87, 1830.42, 1282.05, 1293.4, 
        1121.01, 946.2, 707.1, 154.53, 767.56, 607.78, 448, 288.23, 270.32, 
        223.93, 166.22, 262.45, 223.74, 159.31, 210.44, 257.94, 183.99, 
        151.38, 206.11, 193.43, 388.95, 577.98, 304.64, 285.13, 256.59, 
        420.26, 289.34, 356.02, 358.08, 325.22, 275.95, 164.46, 213.23, 
        142.99, 221.66, 270.61, 206.56, 213.68, 254.33, 250.15, 267.99, 
        403.95, 671.2, 1574.11, 396.34, 477.88, 631.08, 618.25, 1366.88, 
        298.19, 287.05, 290.38, 332.44, 235.9, 229.79, 831.6, 1320.86, 
        477.31, 944.84, 547.33, 411.21, 705.43, 873.49, 572.81, 585.36, 
        1229.69, 701.01, 653.49, 74.81, 162.47, 179.54, 330.27, 544.51, 
        332.41, 296.66, 130.66, 1055.61, 556.79, 265.43, 383.44, 398.22, 
        362.66, 223.99, 130.35, 193.67, 217.68, 273.3, 247.84, 161.66, 
        320.08, 322.52, 274.61, 811.44, 353.85, 323.41, 383.61, 389.5
        ), .Dim = c(1248L, 1L), .Dimnames = list(NULL, ""power""))
</code></pre>

<p>Temperature predictor values are:</p>

<pre><code>temp&lt;-structure(c(31, 31, 31, 31, 30, 29, 28, 27.5, 27, 26, 26, 26, 
26, 26, 26, 28, 29, 29, 30, 31, 32, 33, 33, 34, 34, 35, 36, 36, 
36.5, 37, 38, 38, 39, 39, 39, 38, 37, 36, 36, 35, 34, 33, 33, 
33, 32.5, 32, 32, 31, 30, 26, 24, 24, 24, 24, 24, 24, 25, 25, 
25, 25, 26, 26, 26, 27, 28.3, 29.7, 31, 32, 33, 33, 34, 34, 34, 
35, 35, 35, 36, 37, 38, 39, 39, 39, 39, 38.5, 38, 37, 36, 35, 
34, 34, 33, 32.5, 32, 30, 30, 30, 28, 28, 27, 25, 25, 24, 24, 
24, 24, 24, 25, 25, 25, 25, 25, 27, 28, 30, 31, 33, 34, 37, 37, 
38, 38, 39, 39.3, 39.7, 40, 40, 40, 40, 41, 40, 40, 38, 38, 36, 
35, 34, 33, 31, 31, 30, 31, 30, 30, 29, 28, 28, 28, 27, 27, 28, 
27, 24, 24, 24, 23, 23, 23, 24, 24, 26, 27, 28, 29.7, 31.3, 33, 
34, 35.3, 36.7, 38, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 
39, 38, 37, 36, 34, 33, 32, 32, 30, 28.5, 27, 27, 27, 27, 27, 
26, 26, 26, 25.3, 24.7, 24, 24, 24, 24, 23, 23, 24.5, 26, 28, 
29.7, 31.3, 33, 34.5, 36, 37, 38, 39, 40, 40, 40.3, 40.7, 41, 
41, 41, 41, 41, 41, 41, 39.5, 38, 37, 37, 35, 34, 34, 33, 33, 
33, 32, 31, 31, 30, 30, 30, 29, 29, 28.5, 28, 28, 28, 29, 29, 
28.5, 28, 29, 29, 30, 31, 31, 32.5, 34, 35, 37, 39.5, 42, 42, 
42, 42, 42, 43, 44, 45, 45, 44, 43, 42, 42, 41, 39, 36.5, 34, 
34, 33, 33, 33, 33, 33, 33, 32, 32, 32, 32, 31.5, 31, 30, 30, 
30, 29, 29, 28, 27, 28, 29, 30, 31, 32.5, 34, 34, 35, 37, 40, 
41, 41, 42, 42, 42.5, 43, 43, 43, 42, 42, 43, 42, 42, 41, 40, 
39, 37.5, 36, 35, 35, 34, 34, 33, 32, 32, 32, 32, 31, 31, 31, 
31, 31, 28, 28, 28, 28, 28, 28.5, 29, 30, 30.5, 31, 32, 33.5, 
35, 36, 37, 38, 39, 38, 39, 40, 41, 42, 42, 43, 42, 42, 42, 41, 
41, 40, 39, 38, 37, 36, 35, 34, 34, 33, 32, 32, 31, 30.5, 30, 
30, 30, 30, 29, 29, 29, 28, 27, 27, 27, 27, 28.5, 30, 31.5, 33, 
34, 35, 36, 37, 38, 38, 39, 40, 41, 42, 42, 42, 42, 42, 42, 42, 
42, 42, 42, 42, 40, 40, 39, 37, 37, 36.5, 36, 35, 34, 34, 33, 
33, 32, 32, 32, 32, 32, 32, 31, 31, 30, 30, 30, 30, 31, 31, 31, 
32, 33.5, 35, 35.5, 36, 37, 37, 38, 39, 40, 41, 41, 42, 42, 42, 
43, 43, 42, 41, 41, 40, 38, 36, 35, 34, 34, 35, 36, 36, 35, 35, 
33, 33, 32, 31, 31, 30, 30, 29, 29, 29, 29, 29, 31, 31.5, 32, 
31, 30, 31, 32, 33, 34, 34, 35, 36, 36, 37, 37, 38, 38, 38, 39, 
39, 39, 39, 39, 39, 39, 38, 37, 37, 37, 37, 35, 33, 31, 31, 31, 
30, 30, 30, 30, 30, 30, 30, 30, 29, 29, 28, 28, 27, 27, 26, 29, 
29, 30, 31, 32, 32, 33, 33, 33, 34, 34, 35, 35, 37, 37, 37, 37, 
37, 38, 38, 38, 38, 37, 37, 36, 35, 34.5, 34, 34, 33, 33, 32, 
32, 32, 32, 32, 31, 31, 31, 31, 31, 31, 31, 30, 30, 29.5, 29, 
29, 30, 30, 31, 32, 32, 33, 33.5, 34, 34, 34, 30.7, 27.3, 24, 
24, 24, 24.7, 25.3, 26, 27, 28, 29, 29, 29, 28, 28, 27, 27, 26, 
25, 25, 25, 24.5, 24, 24, 24, 24, 24, 24, 24, 24, 23, 23, 23, 
23, 23, 23, 24, 24, 24, 25, 25, 26, 28, 29, 30, 31, 31, 32, 33, 
34, 35, 35, 36, 37, 37, 37, 37, 37, 37, 37, 35, 34, 33, 33, 33, 
32, 31.7, 31.3, 31, 31, 31, 31, 30, 30, 30, 30, 28, 28, 27, 27, 
26, 26, 25, 25, 25, 25, 25, 26, 27, 28, 29.7, 31.3, 33, 34, 34, 
35, 36, 37, 36, 36, 35, 35, 34, 32, 32, 33, 31.3, 29.7, 28, 28, 
29, 29, 29, 29, 28, 27.5, 27, 26.5, 26, 26, 26, 26, 26, 26, 25, 
25, 25, 25, 25, 24, 24, 23, 23, 23, 24.5, 26, 27, 28, 29, 30, 
31, 32, 32, 32, 33, 34, 35, 36, 36, 36, 36, 36, 36, 36, 37, 37, 
37, 36, 36, 34, 34, 33, 32, 32, 31, 31, 30, 28, 28, 28, 28, 27, 
27, 27, 27, 27, 27, 27, 27, 26.5, 26, 26, 26, 26, 27, 28.8, 30.5, 
32.2, 34, 34, 34, 35, 36, 37, 38, 38, 38, 39, 40, 39, 39, 39, 
40, 40, 40, 39, 39, 38, 37, 35, 35, 34, 34, 34, 33, 32, 32, 32, 
32, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 33, 34, 
35, 37, 37, 37.3, 37.7, 38, 39, 40, 43, 43, 43, 44, 44, 43, 43, 
43, 43, 43, 43, 42, 41, 40, 39, 38, 37, 36, 35, 35, 34, 34, 34, 
34, 34, 33, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 34, 
35, 36, 37, 38, 38, 40, 40.5, 41, 42, 42, 42, 42, 41, 40, 40, 
39, 39, 37, 32, 27, 26, 26, 27, 28, 27, 27, 27, 26, 25, 25, 25, 
25, 25, 25, 25, 25, 25, 25, 25, 24, 23, 23, 22.5, 22, 22, 23, 
24, 25, 26.3, 27.7, 29, 30, 32, 33, 34, 36, 37, 37, 39, 40, 40.5, 
41, 42, 42, 42, 43, 43, 42, 41.5, 41, 39, 38, 35, 35, 35, 34, 
33, 33, 33, 33, 32, 32, 31, 31, 30, 30, 30, 29, 29, 28, 28, 28, 
28.5, 29, 29, 29, 30, 32, 32, 34, 35, 37, 38, 39, 40, 41, 42, 
42, 43, 44, 44, 44, 45, 45, 45, 44, 43, 43, 42, 40, 38.5, 37, 
35, 35, 34, 34, 33, 34, 33, 33, 33, 33, 32, 32, 31, 32, 31, 30, 
29, 29, 29, 29, 30, 30, 31, 32.7, 34.3, 36, 37, 38, 39, 40, 41, 
41, 42, 42, 43, 44, 44, 44, 44, 44, 43.5, 43, 43, 42, 42, 40, 
38, 38, 37, 36, 36, 36, 36, 35, 35, 34, 34, 34, 34, 32, 32, 31.5, 
31, 31, 30, 30, 30, 30, 31, 33, 34, 35, 37, 38, 39, 40, 40, 42, 
42, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 43, 42, 40, 40, 
40, 39, 38, 38, 38, 37, 37, 36, 36, 35.5, 35, 34.5, 34, 33, 33, 
32, 32, 31, 30, 30, 30, 31, 32, 33.5, 35, 36.5, 38, 38, 39, 40, 
41, 41, 41, 42, 42, 42, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 
42, 40.5, 39, 38, 38, 37, 37, 37, 36, 36, 36, 36, 36, 36, 36, 
35, 35, 34, 34, 34, 32, 32, 31, 31, 33, 34, 35, 35, 36, 38, 39, 
40, 41, 42, 42.5, 43, 43, 44, 44, 45, 45, 45, 45, 45, 45, 45, 
44, 43.5, 43, 43, 42, 41, 40, 40, 39, 39, 38, 38, 37, 36, 36, 
35, 34, 34, 34, 32, 32, 32, 32, 32, 32, 32, 34, 35, 35, 37, 38, 
38, 39, 39, 40, 41, 42, 43, 44, 44, 44, 45, 45, 46, 46, 46, 45, 
45, 45, 44, 42, 40, 38, 37, 37, 37, 37, 37, 36, 35), .Dim = c(1248L, 
1L), .Dimnames = list(NULL, ""temperature""))
</code></pre>

<p>Forecasted temperature values are:</p>

<pre><code>forecast_temp &lt;- structure(c(35, 34, 34, 33, 33, 30, 29.7, 29.3, 29, 28, 28, 27, 
27.3, 27.7, 28, 29, 30.5, 32, 33.5, 35, 36, 36, 37, 39, 39.5, 
40, 40, 41, 42, 42, 43, 43, 43, 44, 44, 44, 43, 42, 40, 39, 39, 
38, 37, 37, 36, 35, 35, 34), .Dim = c(48L, 1L), .Dimnames = list(
    NULL, ""temperature""))
</code></pre>

<h3><em>UPDATE-1</em></h3>

<p>From @Stephan's suggestion, I followed the approach mentioned by Prof. Rob at <a href=""http://robjhyndman.com/hyndsight/dailydata/"" rel=""nofollow"">link1</a>, <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">link2</a> as</p>

<pre><code>library(forecast)
tsob &lt;- ts(train_power,frequency = 48) #training electriciy data at daily frequency
tsob_weekly &lt;- fourier(ts(train_power,frequency = 7*48),K=3) #training data at weekly frequency
tempob &lt;- ts(train_temperature,frequency = 48) #Temperature, another predictor variable
fit &lt;- auto.arima(tsob,xreg=cbind(tsob_weekly,tempob),seasonal=FALSE)
tempob_forecast &lt;- ts(test_temperature,frequency = 48) #forecasted temperauture values
forecast_val &lt;- forecast(fit,xreg=tempob_forecast,h=48*5) #forecast for coming 5 days
</code></pre>

<p>As evident from the code, I have used Fourier transformation suggested at above links to show weekly seasonal affect. Up to <code>auto.arima()</code>, it works properly but at forecast function an error is thrown as:</p>

<pre><code>Error in forecast.Arima(fit, xreg = tempob_forecast, h = 48 * 5) : 
  Number of regressors does not match fitted model
</code></pre>

<p>The error is clear, i.e., I do not provide the forecasted regressor values of <code>tsob_weekly</code>, which I don't have. How should I handle this issue? Prof. Rob has used the same value of regressor for both the <code>auto.arima</code> and <code>forecast</code> functions.</p>
"
"0.0850230301897704","0.111111111111111","213201","<p>I am having basically the same issue than in <a href=""http://stats.stackexchange.com/questions/65585/auto-arima-does-not-recognize-seasonal-pattern"">this thread</a>, except one thing:</p>

<p>The difference, in my case, is that my data is measured <strong>weekly</strong> and not daily, so the argument of a too high seasonality (> 350) does not hold for my data, since the seasonality in my case is <strong>52</strong> (52 weeks in a year). </p>

<p>And yet, when I use <code>auto.arima()</code>, R returns the ARIMA model (p,d,q) = (2,1,1) and (P,D,Q) = (0,0,0), while the seasonal pattern in my data is blatant... How could you explain that R completely dismisses the seasonality in my data?</p>

<p>Since I'm still in a learning phase, I am using the data set <code>cmort</code> available in the <code>astsa</code> library, so everyone here can use the same data as me. </p>

<p>And I have done <code>cmort &lt;- ts(cmort,frequency=52)</code> to be sure that the seasonality in my data is taken account of, but it didn't change anything.</p>
"
"NaN","NaN","213414","<p>I am trying to analyze whether the intervention has an causal effect on $Y_{t}$. By ACF and PACF, it looks like the data is a random walk. I further use an ARIMAX model to examine the effect of the intervention:</p>

<p><code>cpI2 &lt;- arima(irts2$poll_p1_ipo, order=c(0,1,0), 
               xtransf=irts2$inter2, transfer=list(c(1,0)))</code></p>

<p>However, it shows a message says that:</p>

<pre><code>Error in stats:::arima(x, order = order, seasonal = seasonal, fixed = par[1:narma],  : 
  wrong length for 'fixed'
</code></pre>

<p>I am wondering which part I don't understand or do wrong.</p>

<p>Here is the data:</p>

<p><code>irts2$poll_p1_ipo &lt;- c(22.1, 22.8, 23.5, 24.3, 24.0, 24.4, 24.2, 23.7, 24.1,   23.7, 23.3, 23.6, 24.2 ,24.6, 24.7, 23.0, 22.4, 22.8 ,22.3, 22.7, 22.6, 22.6, 22.3, 22.6, 22.5, 22.0, 22.3, 21.7, 22.2, 21.6, 22.0, 22.1, 22.1, 21.2, 21.6, 20.7, 19.6, 20.3, 19.0, 18.6, 19.8, 19.5, 19.7, 19.4, 20.4, 19.9, 20.1, 20.7, 20.7, 19.6, 20.7, 20.6, 18.6, 19.2, 19.7, 19.7, 19.0, 17.6, 18.6, 18.8, 18.5, 18.9, 18.1, 18.6, 14.3, 13.8, 13.2, 12.6, 12.7, 13.1, 13.9, 16.2, 15.6, 17.7, 16.7, 16.4, 16.7, 16.5, 15.5, 16.6, 15.9, 17.3, 18.0, 17.8)</code></p>

<p><code>irts2$inter2 &lt;-  c(rep(0,64),1,rep(0,19))</code></p>
"
"0.100200602007025","0.157134840263677","194941","<p>I'm using <code>auto.arima</code> to get the best model for the <code>MASS</code> dataset <code>deaths</code>. However, <code>auto.arima</code> does not seem to give the best model by measures of AIC, AICc, or BIC. <code>auto.arima</code> code below:</p>

<pre><code>&gt; data(deaths, package='MASS')
&gt; deaths
      Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec
1974 3035 2552 2704 2554 2014 1655 1721 1524 1596 2074 2199 2512
1975 2933 2889 2938 2497 1870 1726 1607 1545 1396 1787 2076 2837
1976 2787 3891 3179 2011 1636 1580 1489 1300 1356 1653 2013 2823
1977 3102 2294 2385 2444 1748 1554 1498 1361 1346 1564 1640 2293
1978 2815 3137 2679 1969 1870 1633 1529 1366 1357 1570 1535 2491
1979 3084 2605 2573 2143 1693 1504 1461 1354 1333 1492 1781 1915

&gt; library(forecast)
&gt; auto.arima(deaths)
Series: deaths 
ARIMA(1,0,0)(2,0,0)[12] with non-zero mean 

Coefficients:
         ar1    sar1    sar2  intercept
      0.4418  0.3098  0.5078  2058.2234
s.e.  0.1345  0.0973  0.0998   175.8665

sigma^2 estimated as 79455:  log likelihood=-515.07
AIC=1040.13   AICc=1041.04   BIC=1051.51
</code></pre>

<p>If I instead fit a ARIMA(2,1,1)x(1,1,1) model, it is better by all measures: </p>

<pre><code>&gt; Arima(deaths, order=c(2,1,1), seasonal=c(1,1,1))
Series: deaths 
ARIMA(2,1,1)(1,1,1)[12]                    

Coefficients:
         ar1      ar2      ma1     sar1     sma1
      0.2729  -0.3270  -1.0000  -0.2985  -0.9999
s.e.  0.1356   0.1396   0.1305   0.1426   1.0106

sigma^2 estimated as 39753:  log likelihood=-413.08
AIC=838.17   AICc=839.79   BIC=850.64
</code></pre>
"
"0.0490880693673816","0.0641500299099584","198887","<p>I am trying to model a time series that contains a sequence of zeros. I tried fitting an ARIMA model using <code>auto.arima</code> function from the forecast package in R but the MAPE is reported as infinity (probably due to division by zero). Moreover, the <code>auto.arima</code> fits an ARIMA(0,1,0) model over the data. </p>

<p>Can you suggest any types of models that may be appropriate for such data?</p>
"
"0","0.0641500299099584","116842","<p>I have a SarimaX model with three regressor variables:</p>

<pre><code>ARIMA(1,0,0)(0,1,1)[7]                    

Coefficients:
          ar1       sma1   C1 (for xreg1)   C2 (for xreg2)   C3 (for xreg3)
      -0.0260    -0.9216          -0.0354           0.0316           0.9404
s.e.   0.0291     0.0350           0.0016           0.0017           0.0128
</code></pre>

<p>I would like to know how to use these coefficients to obtain the actual equation, like:</p>

<pre><code>y[t] = f(ar1, sma1, C1|xreg1[t], C2|xreg2[t], C3|xreg3[t])
</code></pre>

<p>I have read the following:</p>

<p><a href=""https://www.otexts.org/fpp/8/9"" rel=""nofollow"">https://www.otexts.org/fpp/8/9</a> - I'm using the forecast package in R, so I'm quite grateful for Mr. Hyndman's work,</p>

<p><a href=""http://people.duke.edu/~rnau/arimreg.htm"" rel=""nofollow"">http://people.duke.edu/~rnau/arimreg.htm</a></p>

<p>and others, and I devised some formulas, but they generated values less acurate than those from the R forecast. Somehow, my error-related terms are probably wrong.</p>

<hr>

<p><strong>EDIT</strong>: This is what I have so far:</p>

<p>$$ \ (1-ar1*B)*(1-B^7)*y_t=$$
$$ = (1-ar1*B)*(1-B^7)*(C1*xreg1_t + C2*xreg2_t+C3*xreg3_t)+ $$
$$ + e_t + sma1*e_{t-7}$$</p>

<p>I would like to know if this formula is correct, could anyone please help? Thank you.</p>
"
"0.10976425998969","0.143443827637312","160435","<p>I'm diving into arima models and was trying to repreduce the results of auto regression.</p>

<p>here is a reproducable example:</p>

<pre><code>set.seed(1)
z=arima.sim(n = 101, list(ar = c(0.8)))
</code></pre>

<p>when running ar(1) without an intercept </p>

<pre><code>&gt; ceof(arima(z, order = c(1,0,0),include.mean =FALSE))
ar1 
0.7622461
</code></pre>

<p>when comparing to a linear regression </p>

<pre><code>&gt; coef(lm(z[2:101] ~ z[1:100] + 0))
z[1:100] 
0.7586725 
</code></pre>

<p>which are very similar and can be explained by the different methods used.
However when I do this comparison with models that include an intercept, I get again similar results in the ar1 coefficient but very different measures for the intercept. while the intercept that I get in the arima model is the one that makes less sense to me.</p>

<pre><code>&gt; coef(arima(z, order = c(1,0,0)))
      ar1 intercept 
0.7274511 0.4241322 
&gt; coef(lm(z[2:101] ~ z[1:100]))
(Intercept)    z[1:100] 
  0.1578015   0.7130261 
</code></pre>

<p>Any ideas on these differencing and in what way the arima procedure is different?</p>
"
"0.0694210134500623","0.0907218423253029","163092","<p>Iâ€™m looking to build an ARIMA model in R to help me predict the number of shots a football player is going to take in a game. </p>

<p>I have last season's data to analyse to determine the optimal lags for my AR and MA parameters. I have a data frame in R, with the columns for the player name, date of match and the number of shots. </p>

<p>Unfortunately, I only have a maximum 38 data points for each player which isnâ€™t enough to build a statistically confident model. I suspect I need a way to analyse the data holistically/all-at-once to help me determine the optimal lags.</p>

<p>I donâ€™t, however, know how to do that or even if this is a statistically sound technique. </p>

<p>At the moment I am just analysing my residuals (which have come from a linear regression with independent variables such as Home/Away and Team Possession) with code such as the following:</p>

<pre><code>arima(residuals, order=c(3,0,0))
</code></pre>

<p>Is there a way to instruct R to perform this ARIMA analysis whilst looking at lots of mini-groups (where the groups are categorised by player name)?</p>

<p>Any help would be much appreciated. </p>

<p>Will </p>
"
"0.196352277469526","0.224525104684854","160675","<p>I'm interested in determining both the slope regression coefficient and plotting regression lines for autocorrelated time-series datasets of rainfall.  Specifically, I'd like to identify the best approach in R that would allow me to visualize the regression line on the original (undifferenced) time-series plot when I need to difference the data to remove stationarity (i.e, when d>0 in an arima model).</p>

<p>As a start, I'm exploring the use of auto.arima (from the forecast package) and sarima (from the astsa package) which can output regression coefficients in the presence of autocorrelation.</p>

<p>For example:</p>

<ol>
<li><p>Using auto.arima. The 'drift' of -5.009 represents the slope (see <a href=""http://robjhyndman.com/hyndsight/arima-trends/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/arima-trends/</a>) </p>

<pre><code>&gt; min.ar &lt;- auto.arima(dec.yr.mmin$min_prcp)
&gt; summary(min.ar)

Series: dec.yr.mmin$min_prcp 
ARIMA(1,1,0) with drift         

Coefficients:
          ar1    drift
      -0.5138  -5.0089
s.e.   0.2465   5.7986

sigma^2 estimated as 949:  log likelihood=-57.82
AIC=121.64   AICc=124.31   BIC=123.34

Training set error measures:
                     ME     RMSE      MAE       MPE     MAPE      MASE       ACF1
Training set -0.9479987 28.52129 23.83494 -2.484233 16.12547 0.7957998 -0.2617352
</code></pre></li>
<li><p>Using sarima to fit the model and output the slope</p>

<pre><code>  &gt; fit.min &lt;- sarima(dec.yr.mmin$min_prcp, 1,1,0,                       reg=dec.yr.mmin$decade)
initial  value 3.542448 
  iter   2 value 3.488927
  iter   3 value 3.386967
  iter   4 value 3.383464
  iter   5 value 3.382408
  iter   6 value 3.382051
  iter   7 value 3.382024
  iter   8 value 3.382020
  iter   9 value 3.381925
  iter   9 value 3.381925
  iter   9 value 3.381925
  final  value 3.381925 
  converged
  initial  value 3.400729 
  iter   2 value 3.399523
  iter   3 value 3.399490
  iter   4 value 3.399488
  iter   4 value 3.399488
  iter   4 value 3.399488
  final  value 3.399488 
  converged
</code></pre></li>
</ol>

<p>3.Output coefficients</p>

<pre><code>      &gt; fit.min$fit$coef
             ar1       xreg 
      -0.5137696 -0.5009045 
</code></pre>

<ol start=""4"">
<li><p>For comparison, this is the output from an OLS regression which may give an incorrect slope due to autocorrelation.</p>

<pre><code>  &gt; m3 &lt;- lm(dec.yr.mmin$min_prcp ~ dec.yr.mmin$decade)
  &gt; summary(m3)

  Call:
  lm(formula = dec.yr.mmin$min_prcp ~ dec.yr.mmin$decade)

  Residuals:
      Min      1Q  Median      3Q     Max 
  -45.504  -8.048   1.892  13.650  38.357 

  Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)   
  (Intercept)        1014.1570   319.9461   3.170  0.00807 **
  dec.yr.mmin$decade   -0.4222     0.1580  -2.672  0.02032 * 
  ---
  Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

  Residual standard error: 23.83 on 12 degrees of freedom
  Multiple R-squared:  0.3731,  Adjusted R-squared:  0.3209 
  F-statistic: 7.142 on 1 and 12 DF,  p-value: 0.02032
</code></pre>

<p>`</p></li>
</ol>

<p>The output from sarima identifies the slope coefficient and also intercept when d=0. When differencing is required (e.g., ARIMA (1,1,0) as output above), sarima only outputs the slope. </p>

<p>My question: when d = 1 or more, what approaches in R would allow me to add/visualize the regression line onto the original undifferenced time-series plot. Is it possible to derive the fitted values of the regression line, or derive intercept values from sarima/auto.arima or other package?</p>

<p>Many thanks in advance for your suggestions.</p>
"
"0.0490880693673816","0.0641500299099584","6967","<p>/edit: To clarify: The mtable function from the <a href=""http://cran.r-project.org/web/packages/memisc/index.html"" rel=""nofollow"">memisc</a> package does exactly what I need, but unfortunately does not work with arima models.</p>

<p>Similar to <a href=""http://stats.stackexchange.com/questions/6856/aggregating-results-from-linear-model-runs-r"">this question</a>: I have multiple Arima models, some of which I've also fit with dependent variables. I'd like an easy way to make a table/graph of the coefficients in each model, as well as summary statistics about each model.</p>

<p>Here is some example code:</p>

<pre><code>sim &lt;- arima.sim(list(order = c(1,1,0), ar = 0.7), n = 200)

ar1&lt;-arima(sim,order=c(1,1,0))
ar2&lt;-arima(sim,order=c(2,1,0))
ar3&lt;-arima(sim,order=c(3,1,0))
ar4&lt;-arima(sim,order=c(2,2,1))

#Try mtable
library(memisc)
mtable(""Model 1""=ar1,""Model 2""=ar2,""Model 3""=ar3,""Model 4""=ar4)
#&gt;Error in UseMethod(""getSummary"") : 
#  no applicable method for 'getSummary' applied to an object of class ""Arima""

#Try  apsrtable
library(apsrtable)
apsrtable(""Model 1""=ar1,""Model 2""=ar2,""Model 3""=ar3,""Model 4""=ar4)
#&gt;Error in est/x$se : non-numeric argument to binary operator
</code></pre>
"
"NaN","NaN","163682","<p>I want to use Arima model for forecasting wind speed.I plot my data."
"NaN","NaN","<a href=http://i.stack.imgur.com/TrGLz.jpg rel=nofollow><img src=http://i.stack.imgur.com/TrGLz.jpg alt=enter image description here></a></p>",""
"NaN","NaN","<p>Then i plot ACF and PACF.",""
"NaN","NaN","<a href=http://i.stack.imgur.com/RWq8H.jpg rel=nofollow><img src=http://i.stack.imgur.com/RWq8H.jpg alt=enter image description here></a></p>",""
"NaN","NaN","<p>I used ADF test and KPSS test and they said that data are stationary and doesnt need differencing but in ACF plot we see Sinusoidal trend.Is it realy stationary and doesnt need differencing?How we should know abou seasonality?",""
"NaN","NaN","TNX</p>",""
"NaN","NaN","","<r><time-series><arima><autocorrelation><augmented-dickey-fuller>"
"0.0694210134500623","0.0907218423253029","163580","<p>For testing I generated a very simple time series with a clear recurring pattern. I expected that auto.arima will generate a model, that can forecast that pattern, but Ã³bviously it doesn't. Can anyone give me some hints how I can improve the model in order to predict that pattern correctly?</p>

<pre><code>library(forecast)

ts&lt;-c(1,1,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,1,1,0,0,1,1,1,1,1)
fit &lt;- auto.arima(ts)
plot(forecast(fit,h=20))
</code></pre>

<p><a href=""http://i.stack.imgur.com/AgVha.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/AgVha.png"" alt=""enter image description here""></a>
Thanks!</p>
"
"0.129874823886379","0.169725025739105","118297","<p>I am learning arima by this site:</p>

<p><a href=""http://people.duke.edu/~rnau/411home.htm"" rel=""nofollow"">http://people.duke.edu/~rnau/411home.htm</a></p>

<p>and I want to get the same result as following notes:</p>

<p><a href=""http://people.duke.edu/~rnau/Review_of_basic_statistics_and_the_mean_model_for_forecasting--Robert_Nau.pdf"" rel=""nofollow"">http://people.duke.edu/~rnau/Review_of_basic_statistics_and_the_mean_model_for_forecasting--Robert_Nau.pdf</a></p>

<p>I was thinking that an arima with order [0, 0, 0] is the mean model, but the results is different from the notes, Here is the code:</p>

<pre><code>require(forecast);
x &lt;- c(114, 126, 123, 112, 68, 116, 50, 108, 163, 79,
      67, 98, 131, 83, 56, 109, 81, 61, 90, 92);
m &lt;- arima(x, order=c(0, 0, 0));
print(m);
print(forecast(m, 1));
print(predict(m)$se);
</code></pre>

<p>the output:</p>

<pre><code>&gt; print(m);
Series: x 
ARIMA(0,0,0) with non-zero mean 

Coefficients:
      intercept
        96.3500
s.e.     6.3124

sigma^2 estimated as 796.9:  log likelihood=-95.19
AIC=194.37   AICc=195.08   BIC=196.36

&gt; print(forecast(m, 1));
   Point Forecast    Lo 80    Hi 80   Lo 95    Hi 95
21          96.35 60.17192 132.5281 41.0204 151.6796

&gt; print(predict(m)$se);
Time Series:
Start = 21 
End = 21 
Frequency = 1 
[1] 28.2299
</code></pre>

<p>but the results in the notes are:</p>

<pre><code>SE_fcst = 29.68  (R result: 28.2299)
95% confidence intervals = 34, 158  (R result: 41, 152)
</code></pre>

<p>Where am I wrong?</p>

<p><strong>edit</strong></p>

<p>I do the simulation with random numbers, and the result is the same as the notes.</p>

<ol>
<li>make 21 normal random numbers with mu=100, sigma=30</li>
<li>calculate the error between the mean of first 20 numbers and the last number.</li>
<li>repeat 1 &amp; 2 for 100000 times</li>
</ol>

<p>Here is the python code that to do the simulation:</p>

<pre><code>import numpy as np
N = 1000000
n = 20
x = np.random.normal(100, 30, (N, n))
p = np.mean(x, axis=1)
nx = np.random.normal(100, 30, N)

err = p - nx
print (err**2).mean()**0.5

s = np.std(x, axis=1, ddof=1)
SE_mean = s / n**0.5
print (s**2 + SE_mean**2).mean()**0.5
</code></pre>

<p>the output is:</p>

<pre><code>30.7480552149 (the real standard error of forecast)
30.7375157915 (the estimated standard error of forecast by sqrt(s**2 + SE_mean**2))
</code></pre>
"
"NaN","NaN","211022","<p>I have 288 data points of the Wolf's sunspot data for the years 1700 to 1987. "
"NaN","NaN","I need to predict one step ahead forecasts for a forecast horizon of 25.",""
"NaN","NaN","I kept the last 25 data points of the time series to test against the predictions. </p>",""
"NaN","NaN","<p>Will fitting an Arima to (288-25 =) 263 data points like suggested here <a href=http://stats.stackexchange.com/questions/55168/one-step-ahead-forecast-with-new-data-collected-sequentially?rq=1>One step ahead forecast with new data collected sequentially</a> work?",""
"NaN","NaN","Or do I need to iteratively increase the size of training data by 1 and then predict the next value?</p>",""
"NaN","NaN","","<r><time-series><forecasting><arima>"
"0.0694210134500623","0.0453609211626514","119860","<p>I have data where an observation was made in 10 minute intervals for 8 weeks. 
I have around 170 variables that were measured every 10 minutes. I am trying to use multivariate time series analysis to predict what will happen in the 9th week (also in 10 minute intervals). I know that the ARIMA model is useful for these cases. But I am very new to R and statistics and I am having a little trouble starting out. Most of the information and tutorials on R that I found online are regarding single variate time series analysis and yearly/quarterly measurements, and not every 10 minute measurements, so it is difficult to apply to my problem. I was hoping to get some help on here and I would greatly appreciate any advice !</p>
"
"0.0981761387347632","0.128300059819917","68309","<p>I am using R to develop an ARIMA model to evaluate the influence of several seasonal covariates (e.g., meteorological data) upon the incidence of a seasonal disease. I have weekly data available and have set the period equal to 52 weeks. Using the <code>auto.arima</code> function the disease of interest has a form of <code>ARIMA(0,1,2)(0,0,1)</code>.</p>

<p>Using the <code>TSA</code> package, I can then evaluate the influence of each of the covariates:</p>

<pre><code>covariates &lt;- data.frame(covariate1, covariate2, covariate3)
model &lt;- arima(disease, order=c(0,1,2), seasonal=list(order=c(0,0,1), period=52), xreg=covariates)
</code></pre>

<p>However, I am worried that this approach is identifying spurious associations between the covariates, each of which has a seasonal component...  Is it more appropriate to decompose the covariate data and subtract the seasonal component before fitting the ARIMA model?</p>

<pre><code>covariate1_components &lt;- decompose(covariate1)
covariate1_adjust &lt;- covariate1 - covariate1_components$seasonal
[...]
covariates_adjust &lt;- data.frame(covariate1_adjust, covariate2_adjust, covariate3_adjust)
model2 &lt;- arima(disease, order=c(0,1,2), seasonal=list(order=c(0,0,1), period=52), xreg=covariates_adjust)
</code></pre>

<p>Any thoughts on which of the two approaches would be preferable for evaluating seasonal covariates?</p>
"
"0.0694210134500623","0.0907218423253029","28472","<p>A Regression with ARIMA errors is given by the following formula (saw on Hyndman et al, 1998):</p>

<p>$Y_t = b_0 + b_1 X_{1,t} + \dots + b_k X_{k,t} + N_t$</p>

<p>where $N_t$ is modeled as an ARIMA process.</p>

<p>If we have that the model for $N_t$ is ARIMA$(0,0,0)$, then $N_t = e_t$, and $Y_t$ is modeled by an ordinary regression.</p>

<p>Suppose the following data:</p>

<pre><code>a &lt;- structure(c(29305, 9900, 9802, 17743, 49300, 17700, 24100, 11000, 
10625, 23644, 38011, 16404, 14900, 16300, 18700, 11814, 13934, 
12124, 18097, 30026, 3600, 15700, 12300, 14600), .Tsp = c(2010.25, 
2012.16666666667, 12), class = ""ts"")
b &lt;- structure(c(1.108528016, 1.136920872, 1.100239002, 1.057191265, 
1.044200511, 1.102063834, 1.083847756, 1.068585841, 1.084879628, 
1.232979511, 1.168894672, 1.257302058, 1.264967051, 1.234793782, 
1.306452369, 1.252644047, 1.178593218, 1.124432965, 1.132878661, 
1.189926986, 1.17249669, 1.176285957, 1.176552, 1.179178082), .Tsp = 
c(2010.25, 2012.16666666667, 12), class = ""ts"")
</code></pre>

<p>If I model it using <code>auto.arima</code> function, I have:</p>

<pre><code>auto.arima(a, xreg=b)
Series: a 
ARIMA(0,0,0) with zero mean     

Coefficients:
              b
      15639.266
s.e.   1773.186

sigma^2 estimated as 101878176:  log likelihood=-255.33
AIC=514.65   AICc=515.22   BIC=517.01

lm(a~b)

Call:
lm(formula = a ~ b)

Coefficients:
(Intercept)            b  
      48638       -26143  
</code></pre>

<p>Coefficients from the models differ. Shouldn't they be the same? What am I missing?</p>
"
"0.0490880693673816","0.0641500299099584","217474","<p>Currently I'm using the ARIMA provided in R, the training series is a seasonal time series, with some values close to zero in each period, and I find that when the training series have a descending trend, then in the result of the forecast, there will be some negative values.</p>

<p>But the time series should be positive on every timestamps, is there a way to add constraints in ARIMA so as to prevent forecasting negative values?</p>

<p>If adding constraints is not possible, is there a way to transform the predicted results so as to make them all positive and still capture the tendency?</p>
"
"0.10976425998969","0.114755062109849","163922","<p>I am trying to find any evidence of warming in monthly times series data of water temperature over a 21-year period that is serially correlated. Essentially I am looking to determine a global trend, like what can be done with OLS regression with data that is from independent observations. I am at a crossroads in trying to determine whether a seasonal ARIMA model or a linear mixed model with a trend component as detailed by Crawley on page 799 of ""The R Book"" (2nd ed.) is the most appropriate method to use. I therefore explored both techniques, but got very contradicting answers!</p>

<p>ARIMA modelling gave me a seasonal ARIMA of form (2,0,2)(0,0,1)[12], indicating that no differencing is required and therefore that the series is stationary with NO trend.</p>

<p>However, the linear mixed affects modelling, comparing two models with and without a trend component using ANOVA and maximum likelihood indicated a highly significant trend (R notation):</p>

<pre><code>model2: ave ~ sin(time * 2 * pi) + cos(time * 2 * pi) + (1 | factor(yr))

model1: ave ~ index + sin(time * 2 * pi) + cos(time * 2 * pi) + (1 | factor(yr))

ANOVA(model2,model1)

      Df  AIC     BIC     logLik deviance Chisq Chi Df Pr(&gt;Chisq)   
model2 5 346.82   364.49    -168.41   336.82                           
model1 6 338.54   359.74    -163.27   326.54 10.28      1   0.001345 **
</code></pre>

<p>How can this be? What am I missing? Is it about assuming whether the trend is a parametric form (appropriate for linear mixed model) or whatever weird shape (appropriate for ARIMA)? If so how do I go about choosing which approach to adopt?</p>

<p>Thank you kindly for any advice.</p>
"
"0.12024072240843","0.157134840263677","120008","<p>I am interested in fitting an ARIMAX model using R.
As known, ARIMAX can be understood as a composition of ARIMA models and regression models with exogenous (independent) variables. I have a time series $Y_i$, and want to estimate the ARIMA and nonlinear coefficients. The nonlinear model is the following:</p>

<p>$y_i=Î²_0+Î²_1t_i+Î²_2d+Î²_3 sin(2Ï€t_i/Î²_4 )+Î²_5 (-1^{t_i})+Îµ_i$,  nonlinear 
regression with an exogenous variable.
Where 
$t_i$ =1, 2â€¦, 60
and</p>

<p>d = dummy variable with 20 0's and 40 number 1's</p>

<pre><code>d=c(rep(0,20),rep(1,40))
</code></pre>

<p>And an ARIMA model (1,1,1) for $Y_i$. Therefore, I want to estimate simultaneously the $Î²_i$ and the ARIMA coefficients in order to avoid the confusion between the exogenous coefficients and ARIMA coefficients.  I know that $arima()$ can deal with this formulation but, how do the nonlinear model can be set within function function?. It seems that the <em>xreg</em> term only deals with linear parameters.</p>
"
"0.147264208102145","0.149683403123236","164067","<p>For each day, I observe my variable, <code>y(t)</code>, for a period of 12 hours. In order to understand the data and make predictions, I want to put together these data and make a long timeseries data. Now, if I fit an <code>AR(1)</code> model to the data, or even do kalman filtering on data as explained later,it would mean that my first reading on a day depends on the last reading of the previous day, which is meaningless in my context. </p>

<p><strong>Question</strong> how can I stop the last value of previous day to influence the first observation of new day?</p>

<p>What I've done:</p>

<p>So I have put <code>NA</code> s of the same length of each observation data (12) in between any two days to preserve the seasonality. But I have trouble understanding the consequences of this:</p>

<ol>
<li>upon seeing these NA's, will <code>arima()</code> ignore the NA's, practically doing the same thing I'm trying to avoid?</li>
<li>will it try to interpolate the values, which again means constructing the same series?</li>
</ol>

<p>Now if it is represented in state-space format, the system would just update the state equation (interpolating 'NA's?). But after updating it for 12 times before reaching the next non-missing value, would the state vector be a reasonable value?</p>

<p>All the computations are done in R. </p>

<p><strong>EDIT, clarification:</strong> Assume for each day, I'm collecting data from 1 pm to 12 am. Ideally, for each day, what I want to get at is a model of type $y(t) = \phi y(t-1) + \beta y(t-12) + v(t)$. now If I have a continuous timeseries, then $y(13) = \phi y(12) + ...$ where $y(12)$ is the last value from previous day. Now, in my context, it is meaningless for data on 1pm to depend on 12 am value of last day, but it's reasonable for 3pm data to be dependent on 2 or 1pm, or on 3pm data from previous day(s). </p>

<p>Also assume that  for the first 2 hours of each day, I don't need to forecast them.</p>
"
"0.0850230301897704","0.111111111111111","68261","<p>I started evaluating and comparing some methods in forecasting. I used Price of dozen eggs in US, 1900â€“1993, in constant dollars in the R software FMA package. I held out the last 10 years for assessment of forecast. Below are the results:</p>

<p>I used auto arima method in the R software. Obviously the results are way off. Am I doing something incorrect ? Below is the forecast. It does not recognize the declining trend. </p>

<p><img src=""http://i.stack.imgur.com/KIM9O.jpg"" alt=""auto arima""></p>

<p>I also used an unobserved components model (UCM) and obtained a good forecast,  as below.</p>

<ol>
<li>Without outliers/level shifts there are very large standard errors and therefore wide confidence bands. <img src=""http://i.stack.imgur.com/dlIXM.png"" alt=""UCM without outliers level shifts""></li>
<li>After some iterative work, below is the output with outliers/level shifts (I know I'm overfitting here) but it did a pretty good job in forecasting; there are also narrow confidence bands. <img src=""http://i.stack.imgur.com/5SrYJ.png"" alt=""UCM with outliers level shifts""></li>
</ol>

<p>In looking at just this example the UCM seems to predict the hold-out sample more accurately than auto.arima.</p>

<p><strong>Why is auto.arima not providing a reasonable forecast?</strong></p>

<p><strong>Are state space models/UCMs better for forecasting long range?</strong></p>

<p><strong>Are there any benefits of using one method over other?</strong></p>
"
"0.0490880693673816","0.0641500299099584","217531","<p>I'm trying to use ARIMA process to predict the behaviour of a time series, the probleme I face is that I can't get the order of each component of ARIMA, the lag is between 0 and 1, same goes for the acf.
The time series has no tendency, and I guess no seasonality. 
<a href=""http://i.stack.imgur.com/05rtH.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/05rtH.png"" alt=""this is the plot of the pacf I found""></a> <a href=""http://i.stack.imgur.com/ufYFV.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ufYFV.png"" alt=""How the TS looks like""></a></p>

<p>My question is : Does this mean that ARIMA orders are all 0s, and if so, is the HoltWinters a good (or the best) solution for modelling this kind of TS ?</p>
"
"NaN","NaN","62810","<p>I got some users' history data and generated some sequences of real numbers. The length of each sequence is between 15 and 25. What's more, I do not know whether these sequences have patterns and the frequency is not known as well.</p>

<p>My goal is using each sequence to predict its next value, and then I use auto.arima in R to do this. However, the accuracy of the prediction is low.</p>

<p>Anyone have any good ideas to improve the accuracy?</p>

<p>One of these sequences is:</p>

<pre><code>    1.5959709882736206  
    0.7300914525985718  
    2.0011744499206543  
    3.6755871772766113  
    0.8066112399101257  
    1.3413848876953125  
    3.371157646179199  
    0.4400146007537842  
    2.637667655944824  
    2.1453769207000732  
    2.341433048248291  
    2.3429665565490723  
    1.1187453269958496  
    1.4169363975524902  
    3.328829050064087  
    4.157748699188232  
    3.9255290031433105  
    2.7843635082244873 
</code></pre>
"
"0.264347343607401","0.273984314458548","177262","<p>I have daily data points of the number of sales, but I am not looking at historic data only. My system delivers a new data point every day and in the evening I want to predict the number of sales tomorrow. The sales are typically pretty constant, but they might increase or decrease from time to time. After a change they will be constant for another reasonable amount of time. </p>

<p>Furthermore there might be seasonality which increases the sales by a certain amount for the duration of the seasonal event. I cannot be sure of it, but there might be weekly, monthly or yearly (but no other than that) seasonality, a combination of those or no seasonality at all. (Public) holidays are not considered. </p>

<p>I am using R and was looking at arima and triple exponential smoothing (Holt-Winters) models to use in order to predict the number of sales for the next day. I have to predict the number of sales for tomorrow for around 1000 different data sets: that's why I can not look at all of them by hand, plot acf or pacf or fine tune the models manually. The 1000 data sets share all the characteristics described above.</p>

<p>The Holt-Winters models are working fine until the constant value changes. This somehow messes up the predicted data. The problem with arima models is the computation time which is much higher compared with Holt-Winters. For a data set with data points of two years it takes around 2 seconds. Since the system should work with data sets that hold data for at least 10 years and I have a thousand of those this might not be acceptable, but it might be possible with a reasonable amount of parallelization. I think I will not be able to use auto.arima since it takes simply too much time.</p>

<p>I have a few questions:</p>

<ol>
<li>Is it possible to choose the same arima model that seems to work fine for one or two of my data sets, that I tested manually, for all 1000 data sets if they share the same characteristics?</li>
<li>If I look for yearly seasonality the model catches weekly and monthly seasonality as well. Does it do that in a worse/better way than pure weekly or monthly models? Should I combine different models for different seasonalities in my case?</li>
<li>Which model would you recommend using: Is arima the right way? How would you determine the parameters for the model?</li>
<li>Is there any guide or best practice I can follow for such a problem where the data set grows every day and cannot be investigated manually?</li>
<li>In general, independent of my problem: Do I get more accurate results if I predict every day using a model for yearly seasonality combined to once every year with the same model?</li>
<li>Related to 5.: How long does it take until a change in the above explained constant data affects the predictions for tomorrow in a yearly/monthly/weekly model?</li>
</ol>

<p>If you have any other hints apart from direct answers I would be very pleased as well. </p>

<p>Thank you so much for your help.</p>

<p><strong>Update</strong>:
It seems that one reason for my problems to get accurate results is the huge difference of sales between non-seasonal and seasonal days. It might happen that the number of sales is around 1 on days which are not affected by any seasonal events and bigger than 1000 on seasonally affected days.</p>
"
"0.0850230301897704","0.111111111111111","68387","<p>I asked a question earlier in the forum on auto arima click here <a href=""http://stats.stackexchange.com/questions/68261/performance-evaluation-of-auto-arima-in-r-and-ucm-on-one-dataset"">Performance evaluation of auto.arima in R and UCM on one dataset</a>. The auto.arima provided strange forecast, upon further looking at the code I did not find anything wrong in my R code see code below. This seems a very straightforward problem. If auto.arima does not fit a simple straightforward dataset, I would be very cautious in using this function to fit more complicated datasets. I would encourage using other tools/ functions and verify forecast. </p>

<pre><code>plot(eggs)

## Hold out 10 data points - 1984 thru 1993
egg_price = ts(eggs,start = 1900, end = 1983)

## Fit arima model

fit &lt;- auto.arima(egg_price)
fcast &lt;- forecast(fit,h=10)
plot(fcast)
</code></pre>
"
"0.0490880693673816","0.0641500299099584","177676","<p>I am interested in fitting a basic ARMA model to some data. The ACF and PACF graphs are as follows:</p>

<p><a href=""http://i.stack.imgur.com/uvxNF.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/uvxNF.jpg"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/Pp7FO.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Pp7FO.jpg"" alt=""enter image description here""></a></p>

<p>My first, potentially naive thought, was to fit an autoregressive model:</p>

<p>$x_t=\alpha x_{t-24}+\varepsilon_t$,</p>

<p>however, I am not sure how to do this in R. Fitting a ar model of order 24 fits all intermediate autoregressive terms: </p>

<pre><code>arima(y,order=c(24,0,0))
</code></pre>

<p>Any suggestions appreciated.</p>
"
"0.0490880693673816","0.0641500299099584","68966","<p>I am trying to manually estimate the non-seasonal components of an SARIMA (p,d,q)x(P,D,Q)[s]. I thought the estimation is going the same way like in ARIMA, but the output says somehow something different. </p>

<p>I have an autocorrelation in the acf correlogram and one significance bound at lag 1 in the pacf. That means I have an autocorrelation first order.</p>

<p>I'm confused now, why <code>auto.arima</code> is giving me the result (0,1,1)x(0,0,1)[12] instead of (1,1,0)x(0,0,1)[12]</p>

<p>Here is my code example:</p>

<pre><code>timeseries &lt;- ts(daten, start=c(1955,1), freq=12)

&gt; timeseries
      Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec
1955  1.8  1.7  1.5  1.2  1.5  1.5  1.6  1.8  1.5  1.5  1.6  1.3
1956  0.7  0.6  0.4  0.9  0.9  0.8  0.8  0.6  0.6  0.4  0.4  0.2
1957  0.2  0.1  0.6  0.8  0.3  0.4  0.5  0.7  0.8  0.9  1.0  1.3
1958  1.7  1.7  1.4  1.0  0.9  1.3  1.3  1.0  1.5  1.4  1.4  2.2
1959  1.3  1.7  1.7  2.2  2.8  2.5  2.2  2.3  1.8  1.6  1.3  1.4
1960  2.2  1.8  1.9  1.6  1.1  0.8  1.1  1.1  1.1  1.4  1.2  1.2
1961  0.9  1.2  1.3  0.9  0.7  0.8  0.8  1.2  1.0  1.0  1.4  1.0
1962  1.1  0.8  1.1  1.7  2.1  2.0  2.1  2.1  2.0  2.3  2.0  2.3
1963  1.6  1.9  1.6  1.4  1.6  1.8  1.8  1.9  2.5  2.3  2.2  2.1
1964  2.1  2.1  1.9  2.3  2.1  2.0  2.1  1.8  1.0  1.1  1.5  1.4
1965  1.8  1.9  2.0  2.0  2.0  2.0  2.0  2.0  2.7  2.7  3.3  3.1
1966  2.9  3.0  3.3  2.6  3.1  3.4  3.5  3.3  3.0  2.5  1.4  1.1
1967  0.9  1.0  0.4  0.8  0.0  0.0 -0.7 -0.1 -0.5 -0.1  0.3  0.8
1968  0.8  0.5  1.2  1.0  1.2  0.8  1.2  1.0  1.3  1.3  1.6  1.9
1969  2.0  2.2  2.3  2.7  2.4  2.4  2.6  2.5  2.9  2.9  2.8  2.3
1970  2.3  2.5  2.3  2.2  2.2  2.0  1.9  2.2  2.1  2.1  1.9  2.0
1971  1.9  1.8  1.8  1.1  1.6  1.9  1.9   NA 

diffts &lt;- diff(timeseries,12)
tsdisplay(diffts, lag.max=36)
</code></pre>

<p><img src=""http://i.stack.imgur.com/2MgzU.jpg"" alt=""enter image description here""></p>

<p>But <code>auto.arima</code> is giving me the following output:</p>

<pre><code>auto.arima(timeseries)

Series: timeseries 
ARIMA(0,1,1)(0,0,1)[12]                    

Coefficients:
          ma1     sma1
      -0.1280  -0.7260
s.e.   0.0684   0.0584

sigma^2 estimated as 0.07113:  log likelihood=-23.77
AIC=53.54   AICc=53.66   BIC=63.42
</code></pre>
"
"0.111321277616897","0.0969857289937744","121566","<p>Here is a problem that was puzzling me. Suppose I simulate the AR(2) process with constant and trend using the code below (I apologize for inefficiency and inelegance - the aim was to get job done at this point; also - it may seem strangely constructed, but it has some other purpose too for which is irrelevant here).</p>

<p>My question is - why the constant estimates are so poor? The true value is <code>70</code> but if we average 1000 regressions each over 1000 observations I get an average of <code>381.9234</code>. </p>

<p>Is it because I interpret something wrong or the did I make a mistake somwhere?</p>

<pre><code>set.key(123)

#parameter values
V=7
P=10
S=4
r1 = 50/(50+P)
r2 = V/(30+V)
mu = 10*P
l2 = 10*(S+V)
a0 = 10*V
d0 = 10*P
a1 = 0
d1 = P+V
s2 = 2*(P+V+S)

#simulate and estimate the parameters
data&lt;-NULL
data50 &lt;- NULL

for (firm in 1:1000){

  y_zero &lt;- rnorm(1, mean = mu, sd = l2)
  gamma_0 &lt;- rnorm(1, mean = a0, sd = d0)
  gamma_1 &lt;- rnorm(1, mean = a1, sd = d1)

  y_first &lt;- r1*y_zero + gamma_0 + gamma_1 + rnorm(1, mean = 0, sd = s2)
  y_second &lt;- r1*y_first - r2*(y_first - y_zero) + gamma_0 + 2*gamma_1 + rnorm(1, mean = 0, sd = s2)
  y_third &lt;- r1*y_second - r2*(y_second - y_first) + gamma_0 + 3*gamma_1 + rnorm(1, mean = 0, sd = s2)
  y_fourth &lt;- r1*y_third - r2*(y_third - y_second) + gamma_0 + 4*gamma_1 + rnorm(1, mean = 0, sd = s2)

  column &lt;- cbind(""firm"" = firm, ""t"" = 1:4, ""y"" = c(y_first, y_second, y_third, y_fourth))

  data &lt;- rbind(data, column)
  ###################################################################################
  firm50 &lt;- NULL

  y_fifth &lt;- r1*y_fourth - r2*(y_fourth - y_third) + gamma_0 + 5*gamma_1 + rnorm(1, mean = 0, sd = s2)
  y_sixth &lt;- r1*y_fifth - r2*(y_fifth - y_fourth) + gamma_0 + 6*gamma_1 + rnorm(1, mean = 0, sd = s2)

  y_previous1 &lt;- y_sixth
  y_previous2 &lt;- y_fifth

  firm50 &lt;- cbind(""firm"" = firm, ""t"" = c(5,6), ""y"" = c(y_fifth, y_sixth), ""ro1-ro2"" = c(y_fourth, y_fifth), ""ro2"" = c(y_third, y_fourth))

  for (run in 1:5000){
    time &lt;- run + 6

    the_y &lt;- r1 * y_previous1 - r2 * (y_previous1 - y_previous2) + gamma_0 + time*gamma_1 + rnorm(1, mean = 0, sd = s2)

    firm50 &lt;- rbind(firm50, cbind(""firm"" = firm, ""t"" = time, ""y"" = the_y, ""ro1-ro2"" = y_previous1, ""ro2"" = y_previous2))

    y_previous2 &lt;- y_previous1
    y_previous1 &lt;- the_y

  }
  firm50 &lt;- cbind(firm50, ""gamma0"" = gamma_0, ""gamma1"" = gamma_1)
  data50 &lt;- rbind(data50, firm50)
}

#estimate the coefficients
data &lt;- data.table(as.data.frame(data50))[t %in% c(4000:5000)]
coefs &lt;- NULL
for(i in 1:1000){
  coefs &lt;- rbind(coefs, t(coef(arima(data[firm==i, y], c(2,0,0), xreg = data[firm==i, t])))
}
</code></pre>
"
"0.0850230301897704","0.111111111111111","178014","<p>As the title states, I want to generate a time series that follows an AR(1) proces and thus has a certain overall level of autocorrelation.</p>

<p>I'm using the <code>arima.sim</code> function (which is implemented as standard in R).</p>

<p>I thought that for example the following command:</p>

<pre><code>arima.sim(model=list(Ar=-0.5),n=400)
</code></pre>

<p>would generate a time series of length 400 and an autocorrelation of -0.5.
However, I've noticed that the values you can give to the <code>Ar</code> parameter are not limited to [-1; 1]. For example, you could input <code>10 000</code>.</p>

<p>Can anyone explain to me what the <code>Ar</code> parameter actually represents? Because it apparently is not a correlation coefficient...</p>

<p>After reading on the internet it seems to me that there's not a lot of information there for people who want to simulate time series data using a model as opposed to people who want to fit data to a model...</p>
"
"0.0850230301897704","0.0740740740740741","23881","<p>I've got an ARIMA(1,1,4) model using external regressor with acceptable output but I'm not able to reproduce it outside the R. </p>

<p>this is the result for the model:</p>

<pre><code>Coefficients:
         ar1      ma1     ma2      ma3     ma4  XRegressor[1:39, ]_coeff
      0.9500  -1.0202  0.3977  -0.8283  0.6030                0.0084
s.e.  0.1106   0.1999  0.1953   0.2003  0.1526                0.0059

sigma^2 estimated as 9619542:  log likelihood=-360.56
AIC=735.11   AICc=738.84   BIC=746.57
</code></pre>

<p>The formula I'm using is as follows:</p>

<pre><code>x(t) = x(t-1)(1+ar1) - ar1*x(t-2) + XRegressor[1:39, ]_coeff*
  [xreg(t) - (1+ar1)*xreg(t-1) + ar1*xreg(t-2)] + 
  ma1*e(t-1) + ma2*e(t-2) + ma3*e(t-3) + ma4*e(t-4)
</code></pre>

<p>I'm using residuals as error term in above formula. I could get right result in one step ahead forecast and for further steps, I won't have residuals to substitute in formula. Even by deleting MA part from model, it's not working. Do I miss something here? Can I say by deleting MA part, I'm erasing residual effects?</p>

<p>Thanks a lot for your help in advance.</p>
"
"NaN","NaN","199579","<p>I would like to forecast some data. But I'm not sure whether I have implemented everything correctly. The accuracy is bad and I'm not sure whether it relates to methodology or some mistakes:</p>

<pre><code>library(caret)
library(forecast)
data(economics)
# Here I would like to use that approach and compare other models to arima
# via training/testing
timeSlices &lt;- createTimeSlices(1:nrow(economics), 
                           initialWindow = 500, horizon = 74, fixedWindow = TRUE)

trainSlices &lt;- timeSlices[[1]]
testSlices &lt;- timeSlices[[2]]


economics[trainSlices[[1]],]
economics[testSlices[[1]],]

# Here I fit the model
fit &lt;- Arima(economics[trainSlices[[1]],]$unemploy, order=c(4,1,3), seasonal = list(order = c(1, 0, 1), period = 7), lambda=2,method=""ML"")
    # Here I predict on new data
    pred &lt;- forecast(fit,h=length(economics[testSlices[[1]],]$unemploy))
# Here I extract the estimate
yHat &lt;- pred$mean
    # Here I check the accuracy
    accuracy(yHat,economics[testSlices[[1]],]$unemploy)
</code></pre>
"
"0.0490880693673816","0.0641500299099584","31550","<p>I got AIC values of all models to identify the best model using R language. As I heard, best model produce the smallest AIC value, but maximum likelihood estimation procedure optimizer should converge.</p>

<p>How can I check whether maximum likelihood estimation procedure optimizer has converged or not in R language?</p>
"
"0.10976425998969","0.114755062109849","219440","<p>I am using the excellent <code>tsoutliers</code> R package to detect outliers (additive outliers, temporary changes etc.), but the <code>cval</code> parameter in the <code>tso</code> function is providing me with inconsistent, or at least counter-intuitive results. I was under the impression that a lower value for <code>cval</code> would include more outliers (but possibly also irrelevant ones), but this doesn't always seem the case. For example, using the following data and logic, I get the following output, which is nearly exactly what I was after:</p>

<pre><code>data &lt;- c(121.54, 119.79, 119.18, 118.56, 104, 65.52, 66, 119.18,
123.42, 119.18, 118.56, 99, 61.74, 67.98, 119.18, 123.42, 120.36,
115.14, 98, 62.37, 67.98, 122.72, 121, 116.82, 117.42, 98, 83.538,
103.096, 165.332, 185.6955, 145.848, 129.162, 101, 62.37, 64.68,
115.64, 124.63, 115.64, 118.56, 102, 62.37, 67.32, 115.64, 122.21,
121.54, 114, 103, 62.37, 65.34, 118, 122.21, 119.18, 114, 99, 65.52,
65.34, 118, 122.21, 115.64, 117.42, 73.5, 40.131, 41.184, 79.4376,
95.832, 105.138, 117.42, 100, 63, 66.66, 122.72, 123.42, 116.82, 114,
98, 61.74, 64.68, 116.82, 121, 188.152, 114, 99, 61.74, 66, 122.72,
118.58, 115.64, 112.86, 101, 63.63, 66.66)

# simple tso function
volume &lt;- ts(data, start = c(2016,1,1), frequency = 7)
data.ts.outliers &lt;- tso(volume, types = c(""AO"", ""LS"", ""TC""), cval = 3.0)
data.ts.outliers
plot(data.ts.outliers)
</code></pre>

<p><a href=""http://i.stack.imgur.com/CLf7Y.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/CLf7Y.png"" alt=""Expected results, cval = 3.0""></a></p>

<p>However, using <code>cval = 2.9</code>, as well as most other values for <code>cval</code> above or below 3.0, I get the following results, which is missing some key outliers:</p>

<p><a href=""http://i.stack.imgur.com/i6dIO.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/i6dIO.png"" alt=""Missing outliers, cval = 2.9""></a></p>

<p>As I want to use this package without manual review for each timeseries, Ideally I'd be able to use a slightly lower <code>cval</code> value to ensure I am capturing most outliers, but the inconsistency that I am seeing is not allowing for this. Anything I am missing?</p>
"
"0.10976425998969","0.143443827637312","74545","<p>I have a dataset with columns that represent lagged values of predictors. To illustrate with a simple example, suppose we had car sales data for 3 years and the only predictors available were income and population for a number of car dealers, the dataset could be represented as follows,</p>

<pre><code>ID  IncLag1  PopLag1  SalesLag1  IncLag2  PopLag2 SalesLag2  IncCurrent  PopCurr  SalesCurr
a       100      1000     200        150      2000    300        500       2500         450
b       10        300      50         60       900     80         90       1000         100
</code></pre>

<p>...</p>

<pre><code>k       30        60      10        200      2000     60         80          800         ??
</code></pre>

<p>My dependent variable is SalesCurr - i.e., given a history of past sales and corresponding Income and Population values (which we can use as the train-test data), predict what the Sales will be in the current year (SalesCurr). </p>

<p>My question is as follows -- Using R or GRETL, how is it possible to create an ARIMA/TimeSeries model with the above data to predict the SalesCurrent variable. Using simple Linear Regression, one could simply have a formula such as say, <code>lm (SalesCurrent ~ ., data=mytable)</code>, but it would not be a time-series model since it does not take into account the relationship between the different variables.</p>

<p>Alternatively, I am quite familiar with Machine Learning models and wanted to get your thoughts on how such a dataset could be modeled using say, randomForest, GBM, etc. </p>

<p>Thanks in advance.</p>
"
"NaN","NaN","74639","<p>In Time Series modelsâ€™ transfer functions there is a decay parameter in the"
"NaN","NaN","formula (letâ€™s call it b). In TSA package that decay parameter is not mentioned. When I used other software before (such as SAS) I used to determine b after analyzing â€˜prewhitened seriesâ€™. But",""
"NaN","NaN","in TSA package in R there is no need to specify the decay parameter once you analyze CCF?</p>",""
"NaN","NaN","<p>If not how am I going to know when the decay starts?</p>",""
"NaN","NaN","<p>I understand CCF is used after prewhitening to determine how to",""
"NaN","NaN","filter the outputs but where b comes into the picture?</p>",""
"NaN","NaN","","<r><time-series><arima>"
"0.196692871441843","0.21168429875904","221028","<p>I have a dataset containing the prevalence rate of Malaria in Botswana, starting in 1990 and ending in 2014. My task is to verify whether these data can be used in order to make predictions on the future Malaria prevalance rate. I know that 24 data points is probably not enough, but I decided to give it a try.</p>

<pre><code>#plot data
Mal.TS &lt;- ts(Mal$Value, start=1990, end=2014, freq=1) 
plot.ts(Mal.TS)
</code></pre>

<p><a href=""http://i.stack.imgur.com/KJkE7.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/KJkE7.png"" alt=""enter image description here""></a></p>

<p>Now it would have been nice if there was a completely increasing/decreasing trend, but unfortunately the trend was first increasing and later decreasing.</p>

<pre><code>#Test for stationarity
adf.test(Mal.TS)
</code></pre>

<p><a href=""http://i.stack.imgur.com/NDrI5.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NDrI5.png"" alt=""enter image description here""></a></p>

<p>Because of the first increasing and later decreasing trend and the absence of variance as there is only one data point per year in a limited dataset, the Dickey-Fuller test suggests that the data are stationary.</p>

<pre><code>#Test AFC and PACF
acf(Mal.TS)
pacf(Mal.TS)
</code></pre>

<p><a href=""http://i.stack.imgur.com/NltIG.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NltIG.png"" alt=""enter image description here""></a>
<a href=""http://i.stack.imgur.com/6p9TP.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6p9TP.png"" alt=""enter image description here""></a></p>

<pre><code>#Do  arima
fit &lt;- arima(Mal.TS, order=c(2,0,1))
</code></pre>

<p><a href=""http://i.stack.imgur.com/04331.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/04331.png"" alt=""enter image description here""></a></p>

<pre><code>#Predict
pred &lt;- predict(fit, n.ahead = 5)
ts.plot(Mal.TS,pred$pred, lty = c(1,3))
</code></pre>

<p><a href=""http://i.stack.imgur.com/qmOgu.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qmOgu.png"" alt=""enter image description here""></a></p>

<p>I know this is a poor analysis because of the lack of datapoints. However, I would like some suggestions on how to interpret the results that I found and reasons why this cannot work. The plot shows that the Malaria Prevalence in Botswana has been decreasing over the last 10 years, so one would expect the data to suggest that in the future the prevalence will keep decreasing. Yet, the model predicts the prevalance to increase. Why is this?</p>

<p>One way to possibly address this contra-intuitive result is by adding exogeneous data using the <code>xreg</code> argument. If I could include time series data on GDP, Health care expenditures,... which probably correlate with the Malaria prevalence data and thus also show a decreasing trend, I might be able to predict the future prevalence to be decreasing. Is this correct?</p>

<p>If the task of predicting the future malaria prevalence rate using the 24 earlier data points is not possible, could you give a clear reasoning why this is the case?</p>

<p>So in short I have following questions:</p>

<ol>
<li>How come the arima model predicts the prevalance rate to increase in the future</li>
<li>Can I cause the predictions to be decreasing using exogeneous data like GDP and health care expenditures?</li>
<li>If the analysis is really hopeless, could you give a clear reasoning why this is the case?</li>
</ol>
"
"0.129874823886379","0.169725025739105","223379","<p>I'm fitting an <code>arima</code>(1,0,0) model using the <code>forecast</code> package in R on the <code>usconsumption</code> dataset. However, when I mimic the same fit using <code>lm</code>, I get different coefficients. My understanding is that they should be the same (in fact, they give the same coefficients if I model an <code>arima</code>(0,0,0) and <code>lm</code> with only the external regressor, which is related to this post: <a href=""http://stats.stackexchange.com/questions/28472/regression-with-arima0-0-0-errors-different-from-linear-regression"">Regression with ARIMA(0,0,0) errors different from linear regression</a>). </p>

<p>Is this because <code>arima</code> and <code>lm</code> use different techniques to calculate coefficients? If so, can someone explain the difference?  </p>

<p>Below is my code.</p>

<pre><code>&gt; library(forecast)
&gt; library(fpp)
&gt; 
&gt; #load data
&gt; data(""usconsumption"")
&gt; 
&gt; #create equivalent data frame from time-series
&gt; lagpad &lt;- function(x, k=1) {
+   c(rep(NA, k), x)[1 : length(x)] 
+ }
&gt; usconsumpdf &lt;- as.data.frame(usconsumption)
&gt; usconsumpdf$consumptionLag1 &lt;- lagpad(usconsumpdf$consumption)
&gt; 
&gt; #create arima model
&gt; arima(usconsumption[,1], xreg=usconsumption[,2], order=c(1,0,0))

Call:
arima(x = usconsumption[, 1], order = c(1, 0, 0), xreg = usconsumption[, 2])

Coefficients:
         ar1  intercept  usconsumption[, 2]
      0.2139     0.5867              0.2292
s.e.  0.0928     0.0755              0.0605

sigma^2 estimated as 0.3776:  log likelihood = -152.87,  aic = 313.74
&gt; 
&gt; #create lm model
&gt; lm(consumption~consumptionLag1+income, data=usconsumpdf)

Call:
lm(formula = consumption ~ consumptionLag1 + income, data = usconsumpdf)

Coefficients:
    (Intercept)  consumptionLag1           income  
         0.3779           0.2456           0.2614  
</code></pre>
"
"0.0490880693673816","0.0641500299099584","32528","<p>I have fitted ARIMA(5,1,2) model using <code>auto.arima()</code> function in R and by looking order we can say this is not a best model to forecast. If outliers exist in the data series, what is the method to fit a model to such data?</p>
"
"0.183670737350935","0.205737799949456","222914","<p>I have a time series object <code>calc_visit_ts</code>. <strong>I want to apply the best fit time series model based on the MAPE value for each model.</strong> The issue I face is that the MAPE value HOLT-WINTER multiplicative model cannot be calculated in the same way as the other models(as it gives me a different MAPE value when compared to <code>summary(visit_model_Hw_M)</code>).</p>

<pre><code>#### AUTO-ARIMA
visit_model_Arima &lt;- auto.arima(calc_visit_ts)
# summary(visit_model_Arima)

#### HOLT-WINTER ADDITIVE
visit_model_Hw_A &lt;- hw(calc_visit_ts,h=monthly_prediction,seasonal = ""additive"")
# summary(visit_model_Hw_A)

#### HOLT-WINTER MULTIPLICATIVE
visit_model_Hw_M &lt;- hw(calc_visit_ts,h=monthly_prediction,seasonal = ""multiplicative"")
# summary(visit_model_Hw_M)


#### Calculating MAPE on models for best suit
model_Mape&lt;- c( MAPE_model(visit_model_Arima)
                ,MAPE_model(visit_model_Hw_A))
                #,MAPE_model(visit_model_Hw_M))  this is not accurate

model_Mape=na.omit(model_Mape)
token&lt;-which(min(model_Mape)==model_Mape)

if(length(token)&gt;0)
{
  if(token==1)
    {visit_model&lt;-visit_model_Arima
  }else if(token==2)
    {visit_model&lt;-visit_model_Hw_A
  }else if(token==3)
    {visit_model&lt;-visit_model_Hw_M
  }else 
  {
    ##EXCEPTION HANDLING  
  }
}

summary(visit_model)
</code></pre>

<p>And here is the <strong>function I use to perform MAPE calculation</strong> on the models - </p>

<pre><code>MAPE_model &lt;- function(visit_model) {
 #CHECK FOR ZERO CONDIITION  if(visit_model$x!=0)
 mape = mean(abs(visit_model$residuals)/visit_model$x)
 return(mape)
}
</code></pre>

<p><strong>Data</strong> for time series -</p>

<pre><code>calc_visit_ts
     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
2012          35  53  65  60  64  49  63  55  59  66
2013  62  54  77  67  84  62  82  65  59  67  60  67
2014  73  75  55  76  93  96  89  76  88  65  83  82
2015  76  72  75  94  91  83  72  73  80  83  81  81
2016  97  91  90  80 101  98  

dput(calc_visit_ts)
structure(c(35, 53, 65, 60, 64, 49, 63, 55, 59, 66, 62, 54, 77, 
67, 84, 62, 82, 65, 59, 67, 60, 67, 73, 75, 55, 76, 93, 96, 89, 
76, 88, 65, 83, 82, 76, 72, 75, 94, 91, 83, 72, 73, 80, 83, 81, 
81, 97, 91, 90, 80, 101, 98), .Tsp = c(2012.16666666667, 2016.41666666667, 
12), class = ""ts"")
</code></pre>

<p>To show exactly what I mean -</p>

<p><strong>Holt-Winter Additive Plot</strong></p>

<p><a href=""http://i.stack.imgur.com/i0c0K.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/i0c0K.jpg"" alt=""Holt-Winter Additive Plot""></a></p>

<p><strong>Holt-Winter Multiplicative Plot</strong>
<a href=""http://i.stack.imgur.com/rgvFt.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/rgvFt.jpg"" alt=""Holt-Winter Multiplicative Plot""></a></p>

<p>The <strong>issue</strong> is <code>summary(visit_model_Hw_M)</code> gives <code>MAPE = 9.075097</code>
whereas, <code>MAPE_model(visit_model_Hw_M)</code> gives <code>0.001273087</code> because the multiplicative model fits the curve(data points) therefore using <code>visit_model_Hw_M$residuals</code> isn't an appropriate way to calculate the MAPE(as the function tries to fit the curve).</p>

<p>Is there a way I can fetch the MAPE value for HOLT-WINTER multiplicative from the summary itself? OR a way to correctly estimate the MAPE value for the HOLT-WINTER multiplicative model?</p>
"
"0.255069090569311","0.296296296296296","188595","<p>I have already read</p>

<p><a href=""http://stats.stackexchange.com/questions/126525/time-series-forecast-convert-differenced-forecast-back-to-before-difference-lev"">Time Series Forecast: Convert differenced forecast back to before difference level</a></p>

<p>and</p>

<p><a href=""http://stats.stackexchange.com/questions/130448/how-to-undifference-a-time-series-variable"">How to &quot;undifference&quot; a time series variable</a></p>

<p>None of these unfortunately gives any clear answer how to convert forecast done in ARIMA using differenced method(diff()) to reach at stationary series.</p>

<p>code sample.</p>

<pre><code>## read data and start from 1 jan 2014
dat&lt;-read.csv(""rev forecast 2014-23 dec 2015.csv"")
val.ts &lt;- ts(dat$Actual,start=c(2014,1,1),freq=365)
##Check how we can get stationary series
plot((diff(val.ts)))
plot(diff(diff(val.ts)))
plot(log(val.ts))
plot(log(diff(val.ts)))
plot(sqrt(val.ts))
plot(sqrt(diff(val.ts)))
##I found that double differencing. i.e.diff(diff(val.ts)) gives stationary series.

#I ran below code to get value of 3 parameters for ARIMA from auto.arima
ARIMAfit &lt;- auto.arima(diff(diff(val.ts)), approximation=FALSE,trace=FALSE, xreg=diff(diff(xreg)))
#Finally ran ARIMA
fit &lt;- Arima(diff(diff(val.ts)),order=c(5,0,2),xreg = diff(diff(xreg)))

#plot original to see fit
plot(diff(diff(val.ts)),col=""orange"")
#plot fitted
lines(fitted(fit),col=""blue"")
</code></pre>

<p>This gives me a perfect fit time series. However, how do i reconvert fitted values into their original metric from the current form it is now in? i mean from double differencing into actual number? For log i know we can do 10^fitted(fit) for square root there is similar solution, however what to do for differencing, that too double differencing?</p>

<p>Any help on this please in R? After days of rigorous exercise, i am stuck at this point.</p>

<p>Edit: Let me paste images from 3 iterations i ran to test if differencing has any impact on model fit of auto.arima function and found that it does. so auto.arima can't handle non stationary series and it requires some effort on part of analyst to convert the series to stationary.</p>

<p>Firstly, auto.arima without any differencing. Orange color is actual value, blue is fitted.</p>

<pre><code>ARIMAfit &lt;- auto.arima(val.ts, approximation=FALSE,trace=FALSE, xreg=xreg)
plot(val.ts,col=""orange"")
lines(fitted(ARIMAfit),col=""blue"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/VWVHK.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/VWVHK.png"" alt=""enter image description here""></a></p>

<p>secondly, i tried differencing</p>

<pre><code>ARIMAfit &lt;- auto.arima(diff(val.ts), approximation=FALSE,trace=FALSE, xreg=diff(xreg))
plot(diff(val.ts),col=""orange"")
lines(fitted(ARIMAfit),col=""blue"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/sTnxQ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/sTnxQ.png"" alt=""enter image description here""></a> </p>

<p>thirdly, i did differencing 2 times.</p>

<pre><code>ARIMAfit &lt;- auto.arima(diff(diff(val.ts)), approximation=FALSE,trace=FALSE, 
xreg=diff(diff(xreg)))
plot(diff(diff(val.ts)),col=""orange"")
lines(fitted(ARIMAfit),col=""blue"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/1x8ex.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1x8ex.png"" alt=""enter image description here""></a></p>

<p>A visual inspection can suggest that 3rd graph is more accurate out of all. This i am aware of. The challenge is how to reconvert this fitted value which is in the form of double differenced form into the actual metric!</p>

<p>Edit2: Why it is not so simple. Let me explain by below example.</p>

<p>Actual data with single difference and double difference.
<a href=""http://i.stack.imgur.com/hJSOF.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/hJSOF.png"" alt=""enter image description here""></a></p>

<p>Lets go back to actual data by using differences and first value of prior series.</p>

<p><a href=""http://i.stack.imgur.com/IW6js.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/IW6js.png"" alt=""enter image description here""></a></p>

<p>If i use diff(diff(val.ts)) in auto.arima as input data, i get below fitted values. However i do not have first value of first order difference of fitted value and neither i have first data point in fitted value in original metric format! This is where i am struck!</p>

<p><a href=""http://i.stack.imgur.com/llFtr.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/llFtr.png"" alt=""enter image description here""></a></p>

<p>What if i use Richard Hardy's advice and use data from actual series as reference. This gives me negative numbers. Can you imagine negative sales? And to clarify my original numbers do not have ANY negative number and it does not have any returns or cancellation data!</p>

<p><a href=""http://i.stack.imgur.com/IEKrJ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/IEKrJ.png"" alt=""enter image description here""></a></p>
"
"0.183670737350935","0.222882616611911","76761","<p><strong>Problem:</strong> I would like to extract the BIC and AICc from an arima() object in R.</p>

<p><strong>Background:</strong> The arima() function produces an output of results, which includes the estimated coefficients, standard errors, AIC, BIC, and AICc. Let's run some sample code to see what this looks like:</p>

<pre><code># Load the sunspots dataset
data(sunspots)
# Build an ARIMA(2,0,2) model and store as an object
model &lt;- arima(x=sunspots, order=c(2,0,2), method=""ML"")
# Show a summary of the model
model 
</code></pre>

<p>The output of results for the model appears like this:</p>

<pre><code>Series: sunspots 
ARIMA(2,0,2) with non-zero mean 

Coefficients:
         ar1     ar2      ma1      ma2  intercept
      0.9822  0.0004  -0.3997  -0.1135    51.2652
s.e.  0.1221  0.1196   0.1206   0.0574     8.1441

sigma^2 estimated as 247.9:  log likelihood=-11775.69
AIC=23563.39   AICc=23563.42   BIC=23599.05
</code></pre>

<p>On the bottom line, we can see values for AIC, BIC, and AICc. (Note: this is the output shown by arima() when the forecast package has been loaded, i.e. library(forecast))</p>

<p>Accessing the AIC value is quite easy. One can simply type:</p>

<pre><code>&gt; model$aic
[1] 23563.39
</code></pre>

<p>Access to the AIC value in this manner is made possible due to the fact that it's stored as one of the model's attributes. The following code and output will make this clear:</p>

<pre><code>&gt; attributes(model)
$names
 [1] ""coef""      ""sigma2""    ""var.coef""  ""mask""      ""loglik""   
 [6] ""aic""       ""arma""      ""residuals"" ""call""      ""series""   
[11] ""code""      ""n.cond""    ""model""    

$class
[1] ""Arima""
</code></pre>

<p>Notice, however, that bic and aicc are not model attributes, so the following code is no use to us:</p>

<pre><code>&gt; model$bic
NULL
&gt; model$aicc
NULL
</code></pre>

<p>The BIC and AICc values are, indeed, calculated by the arima() function, but the object that it returns does not give us direct access to their values. This is inconvenient and I've come across others who've raised the issue. Unfortunately, I've not found a solution to the problem.</p>

<p>Can anyone out there help? Which method can I use to access the BIC and AICc from the Arima class of object.</p>

<p><strong>Note:</strong> I've suggested an answer below, but would like to hear improvements and suggestions.</p>

<p>Edit (Version details as requested):</p>

<pre><code>&gt; R.Version()
$platform
[1] ""i686-pc-linux-gnu""

$arch
[1] ""i686""

$os
[1] ""linux-gnu""

$system
[1] ""i686, linux-gnu""

$status
[1] """"

$major
[1] ""3""

$minor
[1] ""0.2""

$year
[1] ""2013""

$month
[1] ""09""

$day
[1] ""25""

$`svn rev`
[1] ""63987""

$language
[1] ""R""

$version.string
[1] ""R version 3.0.2 (2013-09-25)""

$nickname
[1] ""Frisbee Sailing""
</code></pre>
"
"0.129874823886379","0.169725025739105","32634","<p>Is it better to difference a series (assuming it needs it) before using an Arima OR better to use the d parameter within Arima?</p>

<p>I was surprised how different the fitted values are depending on which route is taken with the same model and data. Or am I doing something incorrectly?</p>

<pre><code>install.packages(""forecast"")
library(forecast)

wineindT&lt;-window(wineind, start=c(1987,1), end=c(1994,8))
wineindT_diff &lt;-diff(wineindT)

#coefficients and other measures are similar
modA&lt;-Arima(wineindT,order=c(1,1,0))
summary(modA)
modB&lt;-Arima(wineindT_diff,order=c(1,0,0))
summary(modB)

#fitted values from modA
A&lt;-forecast.Arima(modA,1)$fitted

#fitted from modB, setting initial value to the first value in the original series
B&lt;-diffinv(forecast.Arima(modB,1)$fitted,xi=wineindT[1])


plot(A, col=""red"")
lines(B, col=""blue"")
</code></pre>

<p><strong>ADD:</strong></p>

<p>Please note I am differencing the series once and fitting arima (1,0,0) then I am fitting arima (1,1,0) to the original series. I am (I think) reversing the differencing on the fitted values for the arima(1,0,0) on the differenced file. </p>

<p>I am comparing the fitted values - not the predictions.</p>

<p>Here is the plot (red is arima(1,1,0) and blue is the arima (1,0,0) on the differenced series after changing back to the original scale)  :</p>

<p><img src=""http://i.stack.imgur.com/mQjAb.jpg"" alt=""enter image description here""></p>

<p><strong>Response to Dr. Hyndman's Answer:</strong></p>

<p>1) Can you illustrate in R code what I would need to do in order to get the two fitted values (and presumably forecasts) to match (allowing for small difference due to your first point in your answer) between Arima (1,1,0) and Arima(1,0,0) on the manually differenced series? I assume this has to do with the mean not being included in modA, but I am not entirely sure how to proceed.</p>

<p>2) Regarding your #3. I know I am missing the obvious, but are not $\hat{X}_t = X_{t-1} + \phi(X_{t-1}-X_{t-2}) $ and $\hat{Y}_t = \phi (X_{t-1}-X_{t-2})$ the same when $\hat{Y}_t$ is defined as $\hat{X}_t - X_{t-1}$? Are you saying I am ""undifferencing"" incorrectly?</p>
"
"0.129874823886379","0.169725025739105","221411","<p>I currently have hourly electricity demand data last for 5 years, where I used:  </p>

<pre><code>demand &lt;- msts(mydata$DEMAND,seasonal.period=c(24,182.5*24,365*24),start=2012)
</code></pre>

<p>The plot of <code>stl</code> shows the data have a clear decreasing trend and seasonality, the data is also not normally distributed. </p>

<p><a href=""http://i.stack.imgur.com/SG417.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/SG417.png"" alt=""Stl plot of original time series""></a></p>

<p>I tried:</p>

<ol>
<li>take seasonal difference, then take first difference</li>
<li>log transform</li>
<li>Box-Cox transform </li>
</ol>

<p>All of them do not work, I still have a time series with seasonality and trend. (Do you know how to deal with this?). e.g. the plot of 40 days data after seasonal and first difference: 
<a href=""http://i.stack.imgur.com/kh3YA.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/kh3YA.png"" alt=""the plot of 40 days data after seasonal and first difference:""></a></p>

<p>Then I use following code to fit the above three times series.</p>

<pre><code>fit &lt;- auto.arima(demand, seasonal=FALSE, xreg=fourier(demand, K=4))
</code></pre>

<p>I get ACF plot with clearly seasonal, and significant PACF plot until lag>200.</p>

<p>I also tried: </p>

<pre><code>fit &lt;- tbats(demand)
</code></pre>

<p>No improvement in residuals.</p>

<p>Can any one help me with this? Many thanks.</p>
"
"0.0981761387347632","0.128300059819917","16915","<p>Note that I do most of my analysis using R and Excel.</p>

<p>Let's take this data set for example. I modified it as the data itself is proprietary: the years are also different:</p>

<pre><code>1967    2,033,407
1968    2,162,275
1969    2,159,640
1970    2,312,352
1971    2,554,449
1972    2,548,425
1973    2,101,225
1974    1,951,944
1975    2,106,250
1976    1,687,625
1977    1,636,496
1978    1,494,525
1979    1,606,825
1980    1,460,937
1981    1,310,494
1982    1,319,750
1983    1,263,643
1984    1,171,656
1985    1,194,950
</code></pre>

<p>What I usually do:</p>

<ol>
<li>A linear regression</li>
<li>Some form of polynomial trending</li>
<li>Moving average and double moving average</li>
<li>Basic ARIMA using p = 1, q = 0.</li>
<li>I calculate the errors for all these as well</li>
<li>I average all the forecasts out and the error to have my final result.</li>
</ol>

<p>Note that I'm an engineer that wants to get into statistics and the ability to properly validate and calibrate my models.</p>

<h2>Question</h2>

<p>What is the correct way to forecast this to 5, 10, or even 15 future years?</p>

<p>In a way I'm looking to move beyond the plugging data into a model and believe the data. Yes, I'm aware I can look at the errors. I mainly use RMSE or MAE. But I still am not confident when it comes to just predicting data the right way.</p>

<h3>Note</h3>

<p>this is also related to <a href=""http://stats.stackexchange.com/questions/16545/how-can-i-be-confident-about-my-forecasts-and-improve-my-methodologies"">this question</a> I posted here before.</p>
"
"0.148006097976858","0.174077655955698","202319","<p>I have daily sales data for a department store for the past 850 days. I have indicators on the major holidays and the days leading up to the major holidays. The number of days before the holidays that are included was chosen by AIC. The issue I'm having is that there are outliers throughout the data that I'm not sure how to handle. Or, at least that's what I think is happening since I don't seem to get accurate forecasts. I'm using a CV to calculate the MAPE of forecasts two weeks out, using the first 450 days as the initial training set and the rest to see how well the model forecasts the data.</p>

<p>I've used tso() from the tsoutliers package and tsoutliers from the forecast package to find outliers. They both give different results.</p>

<pre><code>tsoutliers(data$Sales)

$index
[1] 230 270 271 328 635

$replacements
[1] 2222.160 2088.573 2231.577 1812.380 2138.655

train = 454
trainingdata = data$Sales[1:train]
trainingdata = ts(trainingdata,frequency = 7)
tso(trainingdata,types = c(""AO"", ""LS"", ""TC""))

Series: trainingdata 
ARIMA(2,1,1)(2,0,0)[7]                    

Coefficients:
     ar1     ar2      ma1    sar1    sar2      AO52      TC68       TC80      AO86
  0.2872  0.1331  -0.9717  0.3567  0.4607  885.2061  890.3690  -863.4296  836.8638
s.e.  0.0508  0.0480   0.0107  0.0436  0.0429  169.2521  163.4243   166.0282  169.8535
     AO111     AO121      TC229     AO259      TC270     AO328     AO416
  754.1791  691.0849  1236.8523  711.3954  1790.0292  764.9712  920.1783
s.e.  169.2042  167.7273   163.1458  167.9835   163.9663  170.0103  168.9235

sigma^2 estimated as 44080:  log likelihood=-3064.92
AIC=6152.24   AICc=6153.65   BIC=6222.21

Outliers:
type ind  time coefhat  tstat
1    AO  52  8:03   885.2  5.230
2    TC  68 10:05   890.4  5.448
3    TC  80 12:03  -863.4 -5.200
4    AO  86 13:02   836.9  4.927
5    AO 111 16:06   754.2  4.457
6    AO 121 18:02   691.1  4.120
7    TC 229 33:05  1236.9  7.581
8    AO 259 37:07   711.4  4.235
9    TC 270 39:04  1790.0 10.917
10   AO 328 47:06   765.0  4.500
11   AO 416 60:03   920.2  5.447
</code></pre>

<p>Running BoxCox on the data it recommends a transform of the data</p>

<pre><code>lambda &lt;- BoxCox.lambda(data$Sales)
trainingdata = BoxCox(trainingdata,lambda)
tso(trainingdata,types = c(""AO"", ""LS"", ""TC""))
Series: trainingdata 
ARIMA(3,1,1)(2,0,0)[7]                    

Coefficients:
     ar1     ar2      ar3      ma1    sar1    sar2      LS3    AO52     AO53    TC68
  0.3918  0.0993  -0.0587  -0.9856  0.3632  0.4144  13.5805  5.7218  -7.7957  6.3960
s.e.  0.0383  0.0418   0.0416   0.0142  0.0361  0.0341   1.3201  1.2980   1.3041  1.2763
      AO80   AO121   TC229   TC270   AO416     AO445   TC634   AO780
  -23.3707  5.5352  5.8088  7.0446  7.9304  -23.6372  5.5475  6.7194
s.e.    1.2376  1.2307  1.2594  1.2640  1.2476    1.2393  1.2598  1.2353

sigma^2 estimated as 2.332:  log likelihood=-1482.63
AIC=3003.26   AICc=3004.23   BIC=3092.34

Outliers:
type ind   time coefhat   tstat
1    LS   3   1:03  13.581  10.287
2    AO  52   8:03   5.722   4.408
3    AO  53   8:04  -7.796  -5.978
4    TC  68  10:05   6.396   5.012
5    AO  80  12:03 -23.371 -18.883
6    AO 121  18:02   5.535   4.498
7    TC 229  33:05   5.809   4.612
8    TC 270  39:04   7.045   5.573
9    AO 416  60:03   7.930   6.356
10   AO 445  64:04 -23.637 -19.073
11   TC 634  91:04   5.547   4.404
12   AO 780 112:03   6.719   5.439
</code></pre>

<p>Some of these outliers are already taken care of since they're the holidays. I'm not sure how to handle the rest of the outliers when fitting the model and in the CV.</p>

<p>What is the best way to go about taking care of the outliers? I can reset the values of the training data where it's predicted as an outliers to the recommended value if it's not a holiday for fitting the model and then still calculate the MAPE off of the original data. However, there's a LS at index 3 so I'm not sure that would make sense for that.</p>
"
"NaN","NaN","143049","<p>I have heating power data from one year (8670 observations). I also have regressors for day length and temperature (8670 observations also). </p>"
"NaN","NaN","<p>I would like to add seasonality with 24h (1 day)  168h (1 week) periods to an ARMA model. Is there an effective way to construct a this kind of seasonal matrix (with day length and temperature data appended to; so a $8670\times(167+2)$ matrix). The application where I would use this is the <code>auto.arima</code> functions xreg argument.</p>",""
"NaN","NaN","","<r><time-series><categorical-data><arima>"
"0.0490880693673816","0.0641500299099584","220742","<p>I have a time series and two models to choose from: ETS and ARIMA. 
I have used the MAE to select a model. 
But when forecasting the time series and comparing the models, I don't know which model gives the best results. How do you know that? </p>

<p>Here are the results of both models: </p>

<p><a href=""http://i.stack.imgur.com/HLSv1.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/HLSv1.png"" alt=""ETS""></a>
<a href=""http://i.stack.imgur.com/VUcqP.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/VUcqP.png"" alt=""ARIMA""></a></p>

<p>The first one is forecast of ETS model and the second one ARIMA model. </p>
"
"0.147264208102145","0.171066746426556","126196","<p>I'm developing an app in C# (WPF) that amongst other things, it makes a time-series based forecast of sales (4-5 months into the future). I'm an industrial engineer so I'm not pro in statistics nor in programming (basic knowledge of both).</p>

<p>What I'm doing right now is to aggregate my daily data into monthly data, then I test for monthly seasonality, and then either go for a <strong>Holt</strong>'s exponential smoothing or for a <strong>Holt-Winters</strong>'s one depending on the result. </p>

<p>For determining the <strong>smoothing parameters</strong> I'm using <strong>brute force</strong> (i.e. testing a lot of possible combinations) and keeping the one that would have predict the past year (backtesting) with minimum <a href=""http://en.wikipedia.org/wiki/Mean_absolute_error"" rel=""nofollow"">MAE</a>.</p>

<p>A <strong>problem</strong> arises: this method is SLOW (obviously, as always with brute force). It takes about 0,5s only trying the smoothing parameters in 0.05 intervals which doesn't give much accuracy. I need to do this with 1000+ items so it goes over 8 minutes (too much).</p>

<p>So I have a few <strong>questions</strong>:</p>

<ul>
<li>Is there any method to determine optimal smoothing parameters without testing all of them?</li>
<li>Using <em>R.NET</em> to use the forecast package of R will be faster?</li>
<li><p>If so, should I:</p>

<ul>
<li>Use daily or monthly data?</li>
<li>Make also an auto.arima? How to determine which model is better?</li>
</ul></li>
<li><p>Is my method of backtesting (make a model only with data previous to that point) valid to determine if a model is better than another?</p></li>
</ul>

<p><strong><em>EDIT:</em></strong> I have tried implementing R.NET. Time for <code>ets</code> is about 0,1s if I set which model to use and use only mae as <code>opt.crit</code> (if not, it goes up to 5s). </p>

<p>This is good enough <strong>IF</strong> I could get the same out-of-sample predictions I mention in the comment. If it's not possible then I would have to run it 12 times, adding up to 1,2s which is not fast enough.</p>

<ul>
<li>How can I do that (get predictions over the last 12 data without considering them in the model) in R?</li>
</ul>
"
"0.176989551117627","0.21350420507345","219792","<p>My objective it to manually compute one-step ahead forecast using the estimated coefficientes given by the <code>arima</code> function in R.</p>

<p>I will consider the specific model ARIMA(0,0,0)(0,1,3) with weekly seasonality (<code>period = 7</code>). The equation for this model is:</p>

<p>$$ x_{t} = x_{t-7} + \Theta_{1}e_{t-7} + \Theta_{2}e_{t-14} + \Theta_{3}e_{t-21} + e_{t} $$</p>

<p>I will start by computing the one-step ahead forecast using the <code>predict</code> function and then compare it's value with the result given from the above equation. So first I will have to compute <code>theta</code> vector and the residuals vector <code>e_t</code>.</p>

<p>My data consists of daily observations for 35 days.</p>

<pre><code>data &lt;- c(2570,4530,3990,4480,5880,3380,1340,4180,4600,4170,1980,5170,2900,940,7430,6330,7310,9210,8460,3080,1020,4400,2980,5090,7230,3670,2440,1980,2090,3380,2410,3630,3930,2450,1590)
</code></pre>

<p>I start by fitting the model:</p>

<pre><code>fit &lt;- arima(data, order=c(0,0,0), seasonal=list(order=c(0,1,3), period=7), method=""ML"")
</code></pre>

<p>Then I recover the estimated <code>theta</code> coefficients and the last 3 observed residuals. Note that the seasonality period is 7, so the last 3 residuals regarding this seasonality are as stated:</p>

<ul>
<li>Last residual is given by position <code>35 - 7 + 1 = 29</code></li>
<li>Before last residual is given by position <code>35 - 14 + 1 = 22</code></li>
<li>Before before last residual is given by position <code>35 - 28 + 1 = 15</code></li>
</ul>

<p>So that's the reason I have the funny indexes in line two of the following code:</p>

<pre><code>theta &lt;- as.vector(fit$coef)
e_t &lt;- fit$residuals[c(29,22,15)]
</code></pre>

<p>Finnaly, I also fetch the last observation (given seasonality period 7)</p>

<pre><code>z_t &lt;- data[29]
</code></pre>

<p>And when I compute the above formula:</p>

<pre><code>sum(e_t * theta) + z_t)
</code></pre>

<p>I get the value of <code>4613.141</code> which is different from </p>

<pre><code>predict(fit)$pred[1]
</code></pre>

<p>which returns the value <code>4671.607</code>.</p>

<p>Can you please explain where is my error? I've tried this procedure with several different samples and sample sizes and I never get the same forecast as the R function.</p>
"
"0.0850230301897704","0.111111111111111","59176","<p>I'm a new user on R. I'm stuck on my times series research currently with the some questions. Not sure anyone can help me.</p>

<ol>
<li><p>Dummy variable. 
I wanted to add more than 1 dummy variable in the model. But, I realized that the results unchanged after I put additional dummy variables. I have no idea why this happens. I provided my R code as below. Hopefully someone can advise me. </p>

<p>R-Code:</p>

<pre><code>xreg &lt;- cbind(CNY=model.matrix(~as.factor(daily.2yrd$CNY)),HRP=daily.2yrd$HRP,Deepa=daily.2yrd$Deepa,Xmas=daily.2yrd$Xmas)
viewData(xreg)

daily_forecast.fit &lt;- Arima(daily.2yrd$Total_Visitor,order=c(7,0,7),seasonal=c(0,1,0),lambda=round(BoxCox.lambda(daily_visitor.ts,lower=0),4), xreg=xreg)
daily_forecast.fit
</code></pre></li>
<li><p>Define dummy variable.
How can I define my dummy variable if the outlier is lower value? If I define as ""Dummy 1"", my result will skew to higher point which I think is wrong. Any comments?</p>

<p>Example:</p>

<pre><code>ID      Sales      Dummy 1
1       1800        0
2       2000        0
3       1500        0
4        500        1
5       1800        0
6       1600        0
</code></pre></li>
</ol>
"
"0.0694210134500623","0.0453609211626514","19568","<p>I have two (vehicle velocity) signals that should consist of similar ""latent"" drivers, but have different autocorrelation structures. The driver-signals are quite nasty statistically, so I'm not attempting to model them.</p>

<p>I can get quite nice results by prewhitening the signals using AR(1)-residuals, but these are very difficult to interpret in ""real world terms"" (ie. velocities). So what I'd like to do is to prewhiten one of the signals and then add the AR-model of the other signal to this, so that I'd have two signals with same autocorrelation structures.</p>

<p>It may be that there is a very simple method for doing this, but unfortunately I haven't found one, or it maybe impossible. I guess it should be sort of an inverse of the Yule-Walker method. One also that is quite close is to use arima.sim with innovations, but with the difference that I don't have innovations, but residuals.</p>
"
"0.162806707774543","0.193419617728553","77285","<p>I have two groups of time-series, each group represents one type of data. However within each group, each time series may be fitted with a different ARIMA(p,d,q) from the other time series in the same group. </p>

<p>I need to create a single model for each group (<code>Model_group1</code>, <code>Model_group2</code>). I tried the approach mentioned by Rob Hyndman in: 
<a href=""http://stats.stackexchange.com/questions/23036/estimating-same-model-over-multiple-time-series"">Estimating same model over multiple time series</a>.</p>

<p>I need to use these two models to classify any time series to one of these two groups. For each time series, I calculated the AIC of <code>Model_group1</code> and <code>Model_group2</code>, and the model with smaller AIC will mean that the time series belongs to its corresponding group. </p>

<p>I have three problems: </p>

<ol>
<li><p>I received a warning message </p>

<pre><code>Series: ts 
ARIMA(3,0,2) with non-zero mean 

Coefficients:
         ar1     ar2     ar3     ma1      ma2  intercept
      0.0714  0.1417  0.0000  0.0893  -0.0871     0.1169
s.e.     NaN  0.1381  0.0127     NaN   0.1436     0.0026

sigma^2 estimated as 0.2202:  log likelihood=-33822.63
AIC=67659.26   AICc=67659.26   BIC=67725.99
Warning message:
In sqrt(diag(x$var.coef)) : NaNs produced
</code></pre>

<p>This message was returned by only one of the group models. Does that mean that the fitted model is not correct? </p></li>
<li><p>I got two different results using </p>

<pre><code>auto.arima(ts, allowdrift=FALSE, stepwise=FALSE)
auto.arima(ts, allowdrift=FALSE, stepwise=TRUE)
</code></pre></li>
<li><p>When I tested the resulting models, the majority of the time-series were classified as <code>group_1</code>, even when I test one of the time series used to build the long time series of <code>group_2</code>. I need to mention here that the composed time series of <code>group_1</code> is quite shorter than the time series of <code>group_2</code>. Are there any expected reasons for that? </p></li>
</ol>
"
"0.0981761387347632","0.128300059819917","130152","<p>This is out of my curiosity trying to compare time series input to an ARMA model and reconstructed series after an ARMA estimate is obtained. These are the steps I am thinking:</p>

<ol>
<li><p>Construct simulation time series</p>

<pre><code>arma.sim &lt;- arima.sim(model=list(ar=c(0.9),ma=c(0.2)),n = 100)
</code></pre>

<p>estimate the model from arma.sim, assuming we know it is a (1,0,1) model</p>

<pre><code>arma.est1 &lt;- arima(arma.sim, order=c(1,0,1))
</code></pre></li>
<li><p>also say we get arma.est1 in this form, which is close to the original (0.9,0,0.2):</p>

<pre><code>Coefficients:
 ar1     ma1  intercept
 0.9115  0.0104    -0.4486
s.e.  0.0456  0.1270     1.1396

sigma^2 estimated as 1.15:  log likelihood = -149.79,  aic = 307.57
</code></pre></li>
<li><p>If I try to reconstruct another time series from <code>arma.est1</code>, how do I incorporate intercept or s.e. in <code>arima.sim</code>? Something like this doesn't seem to work well because <code>arma.sim</code> and <code>arma.rec</code> are far off:</p>

<pre><code>arma.rec &lt;- arima.sim(n=100, list(ar=c(0.9115),ma=c(0.0104)))
</code></pre></li>
</ol>

<p>Normally we use <code>predict()</code> to check the estimate. But is this a legit way to look at the estimate?</p>
"
"0.163374970262425","0.160128153805087","130256","<p>I have a number of time series with strong seasonality and I am using auto.arima() from R's Forecast package along with Fourier and dummy/explanatory variables to address the seasonality to make forecasts for each time series.  In one part of the time series there are two peaks of activity.  I am looking at previous data to see how well my model predicts out-of-sample-error.  For most of my time series, my ARIMA models do a really good job at forecasting the peaks and troughs of activity.   The models will do a good job when estimating the peaks before they happen and also if I were to update the model with recent data during the middle of the first peak.  </p>

<p>My model, however, gets wonky if the first peak was higher than expected.  In this situation, if I estimate the future using only data before the first peak, my model underestimates the first peak but it accurately forecasts the following trough and does a reasonable job at forecasting the second peak.  (See below - Red is estimated activity; Black is observed activity) </p>

<p><img src=""http://i.stack.imgur.com/SXeSp.png"" alt=""Forecast before first peak""></p>

<p>If I try updating the model with recent data during the middle of the first peak, the forecast then substantially overestimates activity during the remainder of the time series. (See below - Red is estimated activity; Black is observed activity)</p>

<p><img src=""http://i.stack.imgur.com/YvP4N.png"" alt=""Forecast made during first peak""></p>

<p>Why does an updated model do this?  And is there a way to address this issue? I know from domain knowledge that even if the first peak is higher than expected the following trough should return back down, more or less, to the originally expected level.  I have tried playing with the Fourier parameter and manually testing out different ARIMA models.  </p>
"
"0.0850230301897704","0.111111111111111","115710","<p>I have been using the forecast package in R to make forecasts based on an ARIMA model and have noticed a difference in the output of the forecast and simulate functions when calculating confidence intervals.</p>

<p>For example the 95% quantile calculated by the forecast function is about 0.5% higher than that based on 10000 applications of the simulate() function.  Also the mean of the simulated values and the point forecasts provided by the forecast functions are slightly different.</p>

<p>Which one of the functions will do the job better?  Or are the differences too small to worry about?  (The only reason I decided to try simulate was so that a distribution could be fitted to the simulated data).</p>

<p>Edit 1: Example</p>

<pre><code>library(forecast)

#Fit arima model to data
dm1 = arima(DAP, order = c(1,1,0), method = ""ML"", seasonal = list(order = c(0,1,1)))   

#Simulate 10000 times
n.mnths = 7
    n.sim = 10000
    domesticsimulator = function(i){
      simulate(dm1, nsim = n.mnths)
    }

sim.d &lt;- sapply(1:n.sim, function(x)domesticsimulator(x))
distr.d.mat&lt;-t(sim.d); distr.d.mat
distr.d&lt;-data.frame(Jun = distr.d.mat[,1],Jul = distr.d.mat[,2], Aug = distr.d.mat[,3], Sep = distr.d.mat[,4], Oct = distr.d.mat[,5], Nov = distr.d.mat[,6], Dec = distr.d.mat[,7]); distr.d

#Compare to forecast
forecast(dm1)
</code></pre>

<p>Edit 2: Data</p>

<blockquote>
  <p>dput(DAP)
  structure(c(43032450L, 41166780L, 49992700L, 47033260L, 49152352L, 
  52209516L, 55810773L, 53920973L, 44213408L, 49944935L, 47059495L, 
  49757124L, 43815481L, 45306644L, 54147227L, 53253194L, 53030873L, 
  56959142L, 59614287L, 57380873L, 47671785L, 54167489L, 51782564L, 
  52640057L, 47977657L, 47074882L, 58838975L, 54908859L, 57323876L, 
  59724061L, 62396446L, 59110633L, 50600325L, 53738093L, 52766404L, 
  52801276L, 48886043L, 47348142L, 58286011L, 55828555L, 57145193L, 
  59297121L, 60838606L, 58303233L, 49949551L, 55088986L, 53852209L, 
  53538970L, 50022168L, 47766421L, 59244232L, 57398267L, 59285571L, 
  61493934L, 63457403L, 62660179L, 52310402L, 57208618L, 55047116L, 
  53291139L, 50245100L, 50118363L, 59213077L, 55611053L, 58047400L, 
  59559171L, 61401480L, 58966473L, 47680101L, 52956023L, 47658141L, 
  50253800L, 44825056L, 43680328L, 53534891L, 52247781L, 52951246L, 
  55898027L, 59468957L, 56568180L, 48235025L, 52279405L, 48584832L, 
  49793527L, 45501620L, 42440614L, 54424077L, 52498074L, 53842422L, 
  56689853L, 59142493L, 57370748L, 50304708L, 54826050L, 51420519L, 
  51076415L, 46305000L, 43657818L, 55649428L, 52858479L, 55982234L, 
  57778699L, 60310568L, 57403835L, 50982170L, 54124363L, 51660083L, 
  51534990L, 47080840L, 46405385L, 56200391L, 53691570L, 55749349L, 
  57903293L, 59688267L, 58646304L, 50134504L, 53779646L, 51844482L, 
  51165451L, 47814031L, 45736763L, 56564538L, 53226735L, 56557964L, 
  57986530L, 59306473L, 58110953L, 50761250L, 54682312L, 50538227L, 
  54329096L, 47941907L, 45486064L, 57729464L, 54821717L, 57145762L
  ), .Tsp = c(2003, 2014.33333333333, 12), class = ""ts"")</p>
</blockquote>
"
"0.0981761387347632","0.128300059819917","115712","<p>I am running ARIMA model in R and I used auto.arima(X) function to decide appropriate model.After using this function I found that the order of my model is ARIMA(2,1,0).
The problem is I run the same ARIMA(2,1,0) model using arima(X,order=c(2,1,0)) and I got AIC as AIC=832.16. but for same model by using auto.arima(X) as AIC=805.29. I dont know why for the same model AIC is different. Please hep me to over come this problem.
Thank you in advance.</p>
"
"0.147264208102145","0.171066746426556","33862","<p>I have some models built with the <code>auto.arima</code> function from the <code>forecast</code> package. I'm modeling a variable called 'natural efluent energy' (ena), which is how much energy you can extract from some Hydrography region. There are 2 regressor variables (rainfall precipitation from period $t$ and $t-1$.)</p>

<p>Each region has it's own model - some series show positive trend, some shows negative trend, and some seems stationary. The problem is that some forecasts 'from <code>auto.arima</code>' are giving values higher/lower than usual (some forecasts give me negative values, which are not possible).</p>

<p>My original call is below:</p>

<pre><code> m1 = auto.arima(serie, xreg = regvars)
</code></pre>

<p>For the data on the link, I changed it to</p>

<pre><code> m1 = auto.arima(serie, xreg = regvars, max.P = 0, max.Q = 0, stationary = TRUE)
</code></pre>

<p>Then I get good forecasts in this case. My question is, what these parameters(<code>max.P</code>, <code>max.Q</code>) actually control, and how they relate to the trend show by my model variable?</p>

<p>Here is a link for the historic data:
<a href=""http://www.datafilehost.com/download-7718b3fc.html"" rel=""nofollow"">http://www.datafilehost.com/download-7718b3fc.html</a></p>

<p>And here a link for the forecast regressors:
<a href=""http://www.datafilehost.com/download-ca44dfa4.html"" rel=""nofollow"">http://www.datafilehost.com/download-ca44dfa4.html</a></p>

<p>And here a link of mean historic values, the forecast must fall between these values:
<a href=""http://www.datafilehost.com/download-e1e265b7.html"" rel=""nofollow"">http://www.datafilehost.com/download-e1e265b7.html</a></p>

<p>My data starts at 2001/Jun, so the serie is:</p>

<pre><code>  y = ts(dframe$ena, freq = 12, start = c(2001, 6))
</code></pre>
"
"0.10976425998969","0.143443827637312","78741","<p>In R (2.15.2) I fitted once an ARIMA(3,1,3) on a time series and once an ARMA(3,3) on the once differenced timeseries. The fitted parameters differ, which I attributed to the fitting method in ARIMA. </p>

<p>Also, fitting an ARIMA(3,0,3) on the same data as ARMA(3,3) will not result in identical parameters, no matter the fitting method I use.</p>

<p>I am interested in identifying where the difference comes from and with what parameters i can (if at all) fit the ARIMA to get the same coefficients of the fit as from the ARMA.</p>

<p>Sample code to demonstrate:</p>

<pre><code>library(tseries)
set.seed(2)
#getting a time series manually
x&lt;-c(1,2,1)
e&lt;-c(0,0.3,-0.2)
n&lt;-45
AR&lt;-c(0.5,-0.4,-0.1)
MA&lt;-c(0.4,0.3,-0.2)
for(i in 4:n){
tt&lt;-rnorm(1)
t&lt;-x[length(x)]+tt+x[i-1]*AR[1]+x[i-2]*AR[2]+x[i-3]*AR[3]+e[i-1]*MA[1]+e[i-2]*MA[2]+e[i-3]*MA[3]
x&lt;-c(x,t)
e&lt;-c(e,tt)
}
par(mfrow=c(2,1))
plot(x)
plot(diff(x,1))

#fitting different versions. What I would like to get is fit1 with ARIMA()
fit1&lt;-arma(diff(x,1,lag=1),c(3,3),include.intercept=F)
fit2&lt;-arima(x,c(3,1,3),include.mean=F)
fit3&lt;-arima(diff(x,1),c(3,0,3),include.mean=F)
fit4&lt;-arima(x,c(3,1,3),method=""CSS"",include.mean=F)
fit5&lt;-arima(diff(x,1),c(3,0,3),method=""CSS"",include.mean=F)

cbind(fit1$coe,fit2$coe,fit3$coe,fit4$coe,fit5$coe)
</code></pre>

<p>Edit: Using the conditional sum of squares comes pretty close, but is not quite there. Thanks for the hint for the fit1!</p>

<p>Edit2: I do not think this is a duplicate. Points 2 and 3 address different problems than mine, and even if I override the initialisation mentioned in point 1 by </p>

<pre><code>fit4&lt;-arima(x,c(3,1,3),method=""CSS"",include.mean=F,init=fit1$coe)
</code></pre>

<p>I still get different coefficients</p>
"
"0.162806707774543","0.193419617728553","140163","<p>I am working on a small project where we are trying to predict the prices of commodities (Oil, Aluminium, Tin, etc.) for the next 6 months. I have 12 such variables to predict and I have data from Apr, 2008 - May, 2013.</p>

<p>How should I go about prediction? I have done the following:</p>

<ul>
<li>Imported data as a Timeseries dataset </li>
<li>All variable's seasonality tends to vary with Trend, so I am going to multiplicative model. </li>
<li>I took log of the variable to convert into additive model </li>
<li>For each variable decomposed the data using STL</li>
</ul>

<p>I am planning to use Holt Winters exponential smoothing, ARIMA and neural net to forecast. I split the data as training and testing (80, 20). Planning to choose the model with less MAE, MPE, MAPE and MASE.</p>

<p>Am I doing it right?</p>

<p>Also one question I had was, before passing to ARIMA or neural net should I smooth the data? If yes, using what? The data shows both Seasonality and trend.</p>

<p>EDIT:</p>

<p>Attaching the timeseries plot and data
<img src=""http://i.stack.imgur.com/V0wes.png"" alt=""enter image description here""></p>

<pre><code>Year  &lt;- c(2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2008, 2009, 2009, 
           2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2009, 2010, 
           2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 2010, 
           2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 
           2011, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 
           2012, 2012, 2013, 2013)
Month &lt;- c(4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 
           12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 
           8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2) 
Coil  &lt;- c(44000, 44500, 42000, 45000, 42500, 41000, 39000, 35000, 34000, 
           29700, 29700, 29000, 30000, 30000, 31000, 31000, 33500, 33500, 
           33000, 31500, 34000, 35000, 35000, 36000, 38500, 38500, 35500, 
           33500, 34500, 36000, 35500, 34500, 35500, 38500, 44500, 40700, 
           40500, 39100, 39100, 39100, 38600, 39500, 39500, 38500, 39500, 
           40000, 40000, 40500, 41000, 41000, 41000, 40500, 40000, 39300, 
           39300, 39300, 39300, 39300, 39800)
coil &lt;- data.frame(Year = Year, Month = Month, Coil = Coil)
</code></pre>

<p><strong>EDIT 2:</strong>
One question, can you please tell me if my data has any seasonality or trend? And also please give me some tips on how to identify them.
<img src=""http://i.stack.imgur.com/Hg1yp.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/PdNwJ.png"" alt=""enter image description here""></p>
"
"0.0490880693673816","0.0641500299099584","34139","<p>I am using an ARIMA model to create a model for correlated errors from my regression model. I am using the <code>auto.arima</code> function from the <a href=""http://cran.r-project.org/web/packages/forecast/index.html"" rel=""nofollow"">forecast</a> package in R. I am able to get more data at some frequent interval after the regression model is created, so I get more values for the correlated errors. </p>

<p>My question is how do I update the ARIMA model with a gap in time interval between readings.</p>
"
"0.0490880693673816","0.0641500299099584","106140","<p>I have built an ARIMA(p,d,q) model, m using say, </p>

<pre><code>m &lt;- Arima(ts.data, c(p,d,q))
</code></pre>

<p>Given some starting values, I want to simulate future values based on the model m. I can manually code it but there must be a way to simulate it. In <code>arima.sim</code> there doesn't seem to be a way to specify the starting values. </p>
"
"NaN","NaN","63448","<p>I am trying to fit an ARIMA model with one exogenous variable using in R the function <code>auto.arima</code> with <code>xreg</code> option. </p>"
"NaN","NaN","<p>I am having some doubts because none of my data is stationary and they are both seasonal.  </p>",""
"NaN","NaN","<p>Do I have to apply log and  differentiate my series before applying <code>auto.arima</code>?<br>",""
"NaN","NaN","How do I treat the seasonality in the original time series and in the exogenous series?",""
"NaN","NaN","How do I interpret the results of <code>auto.arima</code> function?</p>",""
"NaN","NaN","","<r><time-series><arima>"
"0.12024072240843","0.157134840263677","34493","<p>I am using both R and SAS for the time series modeling. There is an option in SAS that I could not find so far in any packages developed in R for the time series modeling such as TSA or forecast package, at least to the best of my knowledge! To explain more, if we use the windowing environment in SAS to fit an ARIMA model with a regressor, we basically choose:  </p>

<p>Solution->Analysis->Time series Forecasting System->Develop Models<br>
Then Fit ARIMA model -> Predictors->Dynamic Regressors</p>

<p>If we ask to forecast this model, SAS says â€œThe following regressor(s) do not have any forecasting models. The system will automatically select forecasting models for these regressorsâ€. This means that we have not provided the values of the regressors over the forecasting period, and the system tries to find a model for that.</p>

<p>My questions:</p>

<ol>
<li>Is there any package in R with the same capability (explained above) as in SAS to forecast an ARIMA model?  </li>
<li>How can SAS automatically forecast the regressor(s) and based on what models?</li>
</ol>
"
"0.0981761387347632","0.128300059819917","108166","<p>I have data, which I am sure has a downward trend. I am trying to forecast this data using ARIMA and I want ARIMA to consider the trend when it is forecasting. </p>

<p>The first step in ARIMA is to determine the order of differencing for which I am using the KPSS test. The KPSS test in R has an option 'trend' and 'level'. Which one should I choose if I know that my data has a downward trend? I am confused because I dont understand what is the input ARIMA is expecting if trend has to be taken into consideration for forecasting. </p>

<p>Another question: Is there any sure shot way of knowing if the data follows a trend?</p>
"
"NaN","NaN","207987","<p>I have fit an ARIMA model to a time series with function <code>auto.arima</code> from ""forecast"" package in R. I wanted to check prediction intervals for robustness by changing the ARIMA terms. </p>

<p>Here is my R code:</p>

<pre><code>library(""forecast"", lib.loc=""~/R/win-library/3.2"")
library(""tseries"", lib.loc=""~/R/win-library/3.2"")

price = c(256, 223, 190, 170 ,140, 123, 133, 133, 125, 120, 125, 140, 166, 186, 206, 206, 206, 206, 206, 206,
       229, 263, 273, 273 ,273 ,273 ,258, 239, 233, 226, 226, 226, 249, 249, 249, 249, 249, 269, 279, 279,
       279, 279, 299, 316, 316, 316, 316, 316, 316, 316, 299, 299, 299 ,319, 319, 339 ,339, 356 ,356, 356,
       343, 343, 333 ,343 ,442 ,599, 599, 599, 599, 549, 516, 336, 336, 336, 309, 309 ,319, 565, 665, 832,
       832, 698, 632, 532, 499, 526, 526, 526, 526, 499, 466, 333 ,233, 233, 216, 200, 200, 200, 226, 239,
       279, 316, 333 ,366 ,366 ,366, 366 ,366 ,333 ,349 ,349, 349 ,359 ,359, 442 ,459 ,449 ,449, 449, 449,
       449, 449 ,449 ,459, 459 ,459, 459, 459, 446, 446, 446, 446, 459, 459, 439, 439, 439, 439, 482, 482,
       482, 482 ,516,516, 532, 532, 532 ,532 ,532 ,549, 599, 632 ,632 ,632, 632, 599 ,565 ,532, 482, 482,
       482, 482, 499 ,475 ,449, 416)

ts.plot(price)

auto.arima(price)

arima.fit&lt;-Arima(price, c(2,1,4), include.drift=TRUE)
plot(forecast.Arima(arima.fit, 60), ylim=c(-300,1300))

arima.fit&lt;-Arima(price, c(2,1,3), include.drift=TRUE)
plot(forecast.Arima(arima.fit, 60), ylim=c(-300,1300))
</code></pre>

<p>What I saw surprised me quite a bit:</p>

<p><a href=""http://i.stack.imgur.com/SHPAE.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/SHPAE.jpg"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/9pNVK.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9pNVK.jpg"" alt=""enter image description here""></a></p>

<p>Why do the prediction intervals widen in the MA(3) case and hardly so in the MA(4) case? </p>
"
"0.0850230301897704","0.111111111111111","147279","<p>I am trying to do time series analysis in R. 
I have data time series data set like this. </p>

<pre><code>    Month       Year    Value 
    December    2013    5300
    January     2014    289329.8
    February    2014    596518
    March       2014    328457
    April       2014    459600
    May         2014    391356
    June        2014    406288
    July        2014    644339
    August      2014    251238
    September   2014    386466.5
    October     2014    459792
    November    2014    641724
    December    2014    399831
    January     2015    210759
    February    2015    121690
    March       2015    280070
    April       2015    41336
</code></pre>

<p>Googling I found I can use auto.arima function to forecast the result. 
I managed to write R code to do forecast using auto.arima function </p>

<pre><code>    data &lt;- c(5300,289329.8,596518,328457,459600,391356,406288,644339,251238,386466.5,459792,641724,399831,210759,121690,280070,41336)
    data.ts &lt;- ts(data, start=c(2013, 12), end=c(2015, 4), frequency=12) 
    plot(data.ts)
    fit &lt;- auto.arima(data.ts)
    forec &lt;- forecast(fit)
    plot(forec)
</code></pre>

<p>Problem is my forecast result always remain same. </p>

<p><img src=""http://i.stack.imgur.com/SuJ6a.png"" alt=""enter image description here""></p>

<p>Could  any tell me what is going wrong. or help me to correct my forecast result. Thanks</p>
"
"0.0694210134500623","0.0907218423253029","208091","<p>I'm trying to understand how the rolling forecast example below from <a href=""http://robjhyndman.com/hyndsight/rolling-forecasts/"" rel=""nofollow"">Rob Hyndman's blog</a> works. In the final line of the <code>for</code> loop, is <code>fc</code> forecasting horizons into the future beyond the end of test?  Or is <code>fc</code> meant to be a forecasted version of test, that could be compared to check for accuracy? </p>

<p>My own goal is to create something similar that would train a model and forecast it several horizons in to the future.</p>

<p>Code:</p>

<pre><code>library(""fpp"")
library(""forecast"")

##Multi-step forecasts without re-estimation

h &lt;- 5
train &lt;- window(hsales,end=1989.99)
test &lt;- window(hsales,start=1990)
n &lt;- length(test) - h + 1
fit &lt;- auto.arima(train)
fc &lt;- ts(numeric(n), start=1990+(h-1)/12, freq=12)
for(i in 1:n)
{  
  x &lt;- window(hsales, end=1989.99 + (i-1)/12)
  refit &lt;- Arima(x, model=fit)
  fc[i] &lt;- forecast(refit, h=h)$mean[h]
}
</code></pre>
"
"0.0850230301897704","0.0740740740740741","208080","<p>I compared the <code>auto.arima</code> forecast <code>checkts</code> below  to the rolling forecast <code>fc</code> and noticed that every of the error measures is lower for <code>fc</code>.  </p>

<p>Will rolling forecasts have lower errors than a forecasted <code>auto.arima</code> model in general?<br>
Why might that happen? </p>

<p>The data to run the code below is in the ""fpp"" package. Code:</p>

<pre><code>library(""fpp"")
library(""forecast"")

##Multi-step forecasts without re-estimation

h &lt;- 5
train &lt;- window(hsales,end=1989.99)
test &lt;- window(hsales,start=1990)
n &lt;- length(test) - h + 1
fit &lt;- auto.arima(train)
fc &lt;- ts(numeric(n), start=1990+(h-1)/12, freq=12)
for(i in 1:n)
{  
  x &lt;- window(hsales, end=1989.99 + (i-1)/12)
  refit &lt;- Arima(x, model=fit)
  fc[i] &lt;- forecast(refit, h=h)$mean[h]
}


checkts&lt;-forecast(fit,h=71)

accuracy(checkts$mean,test)
	accuracy(fc,test) ##All Error measures are lower than Checkts$mean
</code></pre>
"
"0.0850230301897704","0.111111111111111","208271","<p>I simulated a MA(3) process using: </p>

<pre><code>set.seed(66)
w &lt;- rnorm(100,0,3.6)
p1 &lt;- 0.4; p2 &lt;- -0.2; p3 &lt;- 0.3;
ma3 &lt;- w[1]
ma3[2] &lt;- w[2] + p1*w[1] 
ma3[3] &lt;- w[3] + p1*w[2] + p2*w[1]
for (t in 4:100) ma3[t] &lt;- w[t] + p1*w[t-1] + p2*w[t-2] + p3*w[t-3]
</code></pre>

<p>Running auto.arima on the time series gives:</p>

<pre><code>&gt; auto.arima(ma3)                                   
Series: ma3 
ARIMA(0,0,1) with zero mean     

Coefficients:
         ma1
      0.3854
s.e.  0.1152

sigma^2 estimated as 14.41:  log likelihood=-275.39
AIC=554.77   AICc=554.89   BIC=559.98
</code></pre>

<p>However, fitting the series to a MA(3) model gives a lower AIC:</p>

<pre><code>&gt; arima(ma3, order=c(0,0,3))

Call:
arima(x = ma3, order = c(0, 0, 3))

Coefficients:
         ma1      ma2     ma3  intercept
      0.4039  -0.0836  0.5125     0.2752
s.e.  0.1158   0.0905  0.1039     0.6078

sigma^2 estimated as 11.2:  log likelihood = -264.67,  aic = 539.34
</code></pre>

<p>I'm not sure what's going on. I thought that auto.arima selected the best model based on the AIC. </p>
"
"0.0347105067250312","0.0453609211626514","208305","<p>I am trying to implement/generate a process using <code>arima.sim</code> like this: </p>

<p>$Y_t = a + b*t + \epsilon_t$, where $\epsilon_t = \phi\epsilon_{t-1}+\gamma_t$ a AR(1) process, where $\epsilon_t$ is a white noise.</p>

<p>And after this make the forecasting:</p>

<pre><code>innovs &lt;- rnorm(100,0,3)
x&lt;-1:100 #time variable
mu&lt;-10+.5*x #linear trend
y&lt;-mu+arima.sim(length(x),innov=innovs, model=list(ar=0.7),sd=3)
plot.ts(y,main=expression(Trend+AR(1)),
        ylab=expression(Y[t]),xlab='Time',
        ,col='blue',lty=1,
        bty='l' )

plot(forecast(y,h=20))
grid(col='darkgrey',lwd=2)

forecastMA1&lt;-Arima(window(y,end=y[46]),order=c(1,0,0),
                   include.drift=TRUE,include.constant=TRUE,include.mean=TRUE8,lambda=NULL)

plot(forecast(forecastMA1,h=54, level=99))
grid(col='darkgrey',lwd=2)
lines(y)
</code></pre>

<p>I hope the last graphic would bring to me the series closer to the forecast(blue) because is a trend stationary process, but it doesnt happen.</p>

<p>Is it right?
Thanks</p>
"
"0.0981761387347632","0.128300059819917","208321","<p>I am trying to forecast the median wait time each hour for a customer to get served in a call center.  I know the median wait times each hour and the number of customers who called in each hour (CustCount) in the past, but I don't know how many operators were staffed each hour to answer calls.  I imagine the call center increases staff during the busy times of day, but I don't know.  My data is also very noisy and it's hard to see any clear patterns.</p>

<p>If anyone can suggest strategy or point to a similar example I would be grateful.  I've been experimenting with Arima models with predictors.  </p>

<p>I'm really wondering how much the staffing levels matter and how they could be identified or addressed.  I was thinking maybe looking for level shifts might be an approach.</p>

<p>I have some sample data below.</p>

<p>Data:</p>

<pre><code>dput(dfE86[1:525,c(""DateTime"",""WaitTime"",""CustCount"")])
</code></pre>

<p>structure(list(DateTime = c(""2015-01-01 00:00"", ""2015-01-01 01:00"", 
""2015-01-01 02:00"", ""2015-01-01 03:00"", ""2015-01-01 04:00"", ""2015-01-01 05:00"", 
""2015-01-01 06:00"", ""2015-01-01 07:00"", ""2015-01-01 08:00"", ""2015-01-01 09:00"", 
""2015-01-01 10:00"", ""2015-01-01 11:00"", ""2015-01-01 12:00"", ""2015-01-01 13:00"", 
""2015-01-01 14:00"", ""2015-01-01 15:00"", ""2015-01-01 16:00"", ""2015-01-01 17:00"", 
""2015-01-01 18:00"", ""2015-01-01 19:00"", ""2015-01-01 20:00"", ""2015-01-01 21:00"", 
""2015-01-01 22:00"", ""2015-01-01 23:00"", ""2015-01-02 00:00"", ""2015-01-02 01:00"", 
""2015-01-02 02:00"", ""2015-01-02 03:00"", ""2015-01-02 04:00"", ""2015-01-02 05:00"", 
""2015-01-02 06:00"", ""2015-01-02 07:00"", ""2015-01-02 08:00"", ""2015-01-02 09:00"", 
""2015-01-02 10:00"", ""2015-01-02 11:00"", ""2015-01-02 12:00"", ""2015-01-02 13:00"", 
""2015-01-02 14:00"", ""2015-01-02 15:00"", ""2015-01-02 16:00"", ""2015-01-02 17:00"", 
""2015-01-02 18:00"", ""2015-01-02 19:00"", ""2015-01-02 20:00"", ""2015-01-02 21:00"", 
""2015-01-02 22:00"", ""2015-01-02 23:00"", ""2015-01-03 00:00"", ""2015-01-03 01:00"", 
""2015-01-03 02:00"", ""2015-01-03 03:00"", ""2015-01-03 04:00"", ""2015-01-03 05:00"", 
""2015-01-03 06:00"", ""2015-01-03 07:00"", ""2015-01-03 08:00"", ""2015-01-03 09:00"", 
""2015-01-03 10:00"", ""2015-01-03 11:00"", ""2015-01-03 12:00"", ""2015-01-03 13:00"", 
""2015-01-03 14:00"", ""2015-01-03 15:00"", ""2015-01-03 16:00"", ""2015-01-03 17:00"", 
""2015-01-03 18:00"", ""2015-01-03 19:00"", ""2015-01-03 20:00"", ""2015-01-03 21:00"", 
""2015-01-03 22:00"", ""2015-01-03 23:00"", ""2015-01-04 00:00"", ""2015-01-04 01:00"", 
""2015-01-04 02:00"", ""2015-01-04 03:00"", ""2015-01-04 04:00"", ""2015-01-04 05:00"", 
""2015-01-04 06:00"", ""2015-01-04 07:00"", ""2015-01-04 08:00"", ""2015-01-04 09:00"", 
""2015-01-04 10:00"", ""2015-01-04 11:00"", ""2015-01-04 12:00"", ""2015-01-04 13:00"", 
""2015-01-04 14:00"", ""2015-01-04 15:00"", ""2015-01-04 16:00"", ""2015-01-04 17:00"", 
""2015-01-04 18:00"", ""2015-01-04 19:00"", ""2015-01-04 20:00"", ""2015-01-04 21:00"", 
""2015-01-04 22:00"", ""2015-01-04 23:00"", ""2015-01-05 00:00"", ""2015-01-05 01:00"", 
""2015-01-05 02:00"", ""2015-01-05 03:00"", ""2015-01-05 04:00"", ""2015-01-05 05:00"", 
""2015-01-05 06:00"", ""2015-01-05 07:00"", ""2015-01-05 08:00"", ""2015-01-05 09:00"", 
""2015-01-05 10:00"", ""2015-01-05 11:00"", ""2015-01-05 12:00"", ""2015-01-05 13:00"", 
""2015-01-05 14:00"", ""2015-01-05 15:00"", ""2015-01-05 16:00"", ""2015-01-05 17:00"", 
""2015-01-05 18:00"", ""2015-01-05 19:00"", ""2015-01-05 20:00"", ""2015-01-05 21:00"", 
""2015-01-05 22:00"", ""2015-01-05 23:00"", ""2015-01-06 00:00"", ""2015-01-06 01:00"", 
""2015-01-06 02:00"", ""2015-01-06 03:00"", ""2015-01-06 04:00"", ""2015-01-06 05:00"", 
""2015-01-06 06:00"", ""2015-01-06 07:00"", ""2015-01-06 08:00"", ""2015-01-06 09:00"", 
""2015-01-06 10:00"", ""2015-01-06 11:00"", ""2015-01-06 12:00"", ""2015-01-06 13:00"", 
""2015-01-06 14:00"", ""2015-01-06 15:00"", ""2015-01-06 16:00"", ""2015-01-06 17:00"", 
""2015-01-06 18:00"", ""2015-01-06 19:00"", ""2015-01-06 20:00"", ""2015-01-06 21:00"", 
""2015-01-06 22:00"", ""2015-01-06 23:00"", ""2015-01-07 00:00"", ""2015-01-07 01:00"", 
""2015-01-07 02:00"", ""2015-01-07 03:00"", ""2015-01-07 04:00"", ""2015-01-07 05:00"", 
""2015-01-07 06:00"", ""2015-01-07 07:00"", ""2015-01-07 08:00"", ""2015-01-07 09:00"", 
""2015-01-07 10:00"", ""2015-01-07 11:00"", ""2015-01-07 12:00"", ""2015-01-07 13:00"", 
""2015-01-07 14:00"", ""2015-01-07 15:00"", ""2015-01-07 16:00"", ""2015-01-07 17:00"", 
""2015-01-07 18:00"", ""2015-01-07 19:00"", ""2015-01-07 20:00"", ""2015-01-07 21:00"", 
""2015-01-07 22:00"", ""2015-01-07 23:00"", ""2015-01-08 00:00"", ""2015-01-08 01:00"", 
""2015-01-08 02:00"", ""2015-01-08 03:00"", ""2015-01-08 04:00"", ""2015-01-08 05:00"", 
""2015-01-08 06:00"", ""2015-01-08 07:00"", ""2015-01-08 08:00"", ""2015-01-08 09:00"", 
""2015-01-08 10:00"", ""2015-01-08 11:00"", ""2015-01-08 12:00"", ""2015-01-08 13:00"", 
""2015-01-08 14:00"", ""2015-01-08 15:00"", ""2015-01-08 16:00"", ""2015-01-08 17:00"", 
""2015-01-08 18:00"", ""2015-01-08 19:00"", ""2015-01-08 20:00"", ""2015-01-08 21:00"", 
""2015-01-08 22:00"", ""2015-01-08 23:00"", ""2015-01-09 00:00"", ""2015-01-09 01:00"", 
""2015-01-09 02:00"", ""2015-01-09 03:00"", ""2015-01-09 04:00"", ""2015-01-09 05:00"", 
""2015-01-09 06:00"", ""2015-01-09 07:00"", ""2015-01-09 08:00"", ""2015-01-09 09:00"", 
""2015-01-09 10:00"", ""2015-01-09 11:00"", ""2015-01-09 12:00"", ""2015-01-09 13:00"", 
""2015-01-09 14:00"", ""2015-01-09 15:00"", ""2015-01-09 16:00"", ""2015-01-09 17:00"", 
""2015-01-09 18:00"", ""2015-01-09 19:00"", ""2015-01-09 20:00"", ""2015-01-09 21:00"", 
""2015-01-09 22:00"", ""2015-01-09 23:00"", ""2015-01-10 00:00"", ""2015-01-10 01:00"", 
""2015-01-10 02:00"", ""2015-01-10 03:00"", ""2015-01-10 04:00"", ""2015-01-10 05:00"", 
""2015-01-10 06:00"", ""2015-01-10 07:00"", ""2015-01-10 08:00"", ""2015-01-10 09:00"", 
""2015-01-10 10:00"", ""2015-01-10 11:00"", ""2015-01-10 12:00"", ""2015-01-10 13:00"", 
""2015-01-10 14:00"", ""2015-01-10 15:00"", ""2015-01-10 16:00"", ""2015-01-10 17:00"", 
""2015-01-10 18:00"", ""2015-01-10 19:00"", ""2015-01-10 20:00"", ""2015-01-10 21:00"", 
""2015-01-10 22:00"", ""2015-01-10 23:00"", ""2015-01-11 00:00"", ""2015-01-11 01:00"", 
""2015-01-11 02:00"", ""2015-01-11 03:00"", ""2015-01-11 04:00"", ""2015-01-11 05:00"", 
""2015-01-11 06:00"", ""2015-01-11 07:00"", ""2015-01-11 08:00"", ""2015-01-11 09:00"", 
""2015-01-11 10:00"", ""2015-01-11 11:00"", ""2015-01-11 12:00"", ""2015-01-11 13:00"", 
""2015-01-11 14:00"", ""2015-01-11 15:00"", ""2015-01-11 16:00"", ""2015-01-11 17:00"", 
""2015-01-11 18:00"", ""2015-01-11 19:00"", ""2015-01-11 20:00"", ""2015-01-11 21:00"", 
""2015-01-11 22:00"", ""2015-01-11 23:00"", ""2015-01-12 00:00"", ""2015-01-12 01:00"", 
""2015-01-12 02:00"", ""2015-01-12 03:00"", ""2015-01-12 04:00"", ""2015-01-12 05:00"", 
""2015-01-12 06:00"", ""2015-01-12 07:00"", ""2015-01-12 08:00"", ""2015-01-12 09:00"", 
""2015-01-12 10:00"", ""2015-01-12 11:00"", ""2015-01-12 12:00"", ""2015-01-12 13:00"", 
""2015-01-12 14:00"", ""2015-01-12 15:00"", ""2015-01-12 16:00"", ""2015-01-12 17:00"", 
""2015-01-12 18:00"", ""2015-01-12 19:00"", ""2015-01-12 20:00"", ""2015-01-12 21:00"", 
""2015-01-12 22:00"", ""2015-01-12 23:00"", ""2015-01-13 00:00"", ""2015-01-13 01:00"", 
""2015-01-13 02:00"", ""2015-01-13 03:00"", ""2015-01-13 04:00"", ""2015-01-13 05:00"", 
""2015-01-13 06:00"", ""2015-01-13 07:00"", ""2015-01-13 08:00"", ""2015-01-13 09:00"", 
""2015-01-13 10:00"", ""2015-01-13 11:00"", ""2015-01-13 12:00"", ""2015-01-13 13:00"", 
""2015-01-13 14:00"", ""2015-01-13 15:00"", ""2015-01-13 16:00"", ""2015-01-13 17:00"", 
""2015-01-13 18:00"", ""2015-01-13 19:00"", ""2015-01-13 20:00"", ""2015-01-13 21:00"", 
""2015-01-13 22:00"", ""2015-01-13 23:00"", ""2015-01-14 00:00"", ""2015-01-14 01:00"", 
""2015-01-14 02:00"", ""2015-01-14 03:00"", ""2015-01-14 04:00"", ""2015-01-14 05:00"", 
""2015-01-14 06:00"", ""2015-01-14 07:00"", ""2015-01-14 08:00"", ""2015-01-14 09:00"", 
""2015-01-14 10:00"", ""2015-01-14 11:00"", ""2015-01-14 12:00"", ""2015-01-14 13:00"", 
""2015-01-14 14:00"", ""2015-01-14 15:00"", ""2015-01-14 16:00"", ""2015-01-14 17:00"", 
""2015-01-14 18:00"", ""2015-01-14 19:00"", ""2015-01-14 20:00"", ""2015-01-14 21:00"", 
""2015-01-14 22:00"", ""2015-01-14 23:00"", ""2015-01-15 00:00"", ""2015-01-15 01:00"", 
""2015-01-15 02:00"", ""2015-01-15 03:00"", ""2015-01-15 04:00"", ""2015-01-15 05:00"", 
""2015-01-15 06:00"", ""2015-01-15 07:00"", ""2015-01-15 08:00"", ""2015-01-15 09:00"", 
""2015-01-15 10:00"", ""2015-01-15 11:00"", ""2015-01-15 12:00"", ""2015-01-15 13:00"", 
""2015-01-15 14:00"", ""2015-01-15 15:00"", ""2015-01-15 16:00"", ""2015-01-15 17:00"", 
""2015-01-15 18:00"", ""2015-01-15 19:00"", ""2015-01-15 20:00"", ""2015-01-15 21:00"", 
""2015-01-15 22:00"", ""2015-01-15 23:00"", ""2015-01-16 00:00"", ""2015-01-16 01:00"", 
""2015-01-16 02:00"", ""2015-01-16 03:00"", ""2015-01-16 04:00"", ""2015-01-16 05:00"", 
""2015-01-16 06:00"", ""2015-01-16 07:00"", ""2015-01-16 08:00"", ""2015-01-16 09:00"", 
""2015-01-16 10:00"", ""2015-01-16 11:00"", ""2015-01-16 12:00"", ""2015-01-16 13:00"", 
""2015-01-16 14:00"", ""2015-01-16 15:00"", ""2015-01-16 16:00"", ""2015-01-16 17:00"", 
""2015-01-16 18:00"", ""2015-01-16 19:00"", ""2015-01-16 20:00"", ""2015-01-16 21:00"", 
""2015-01-16 22:00"", ""2015-01-16 23:00"", ""2015-01-17 00:00"", ""2015-01-17 01:00"", 
""2015-01-17 02:00"", ""2015-01-17 03:00"", ""2015-01-17 04:00"", ""2015-01-17 05:00"", 
""2015-01-17 06:00"", ""2015-01-17 07:00"", ""2015-01-17 08:00"", ""2015-01-17 09:00"", 
""2015-01-17 10:00"", ""2015-01-17 11:00"", ""2015-01-17 12:00"", ""2015-01-17 13:00"", 
""2015-01-17 14:00"", ""2015-01-17 15:00"", ""2015-01-17 16:00"", ""2015-01-17 17:00"", 
""2015-01-17 18:00"", ""2015-01-17 19:00"", ""2015-01-17 20:00"", ""2015-01-17 21:00"", 
""2015-01-17 22:00"", ""2015-01-17 23:00"", ""2015-01-18 00:00"", ""2015-01-18 01:00"", 
""2015-01-18 02:00"", ""2015-01-18 03:00"", ""2015-01-18 04:00"", ""2015-01-18 05:00"", 
""2015-01-18 06:00"", ""2015-01-18 07:00"", ""2015-01-18 08:00"", ""2015-01-18 09:00"", 
""2015-01-18 10:00"", ""2015-01-18 11:00"", ""2015-01-18 12:00"", ""2015-01-18 13:00"", 
""2015-01-18 14:00"", ""2015-01-18 15:00"", ""2015-01-18 16:00"", ""2015-01-18 17:00"", 
""2015-01-18 18:00"", ""2015-01-18 19:00"", ""2015-01-18 20:00"", ""2015-01-18 21:00"", 
""2015-01-18 22:00"", ""2015-01-18 23:00"", ""2015-01-19 00:00"", ""2015-01-19 01:00"", 
""2015-01-19 02:00"", ""2015-01-19 03:00"", ""2015-01-19 04:00"", ""2015-01-19 05:00"", 
""2015-01-19 06:00"", ""2015-01-19 07:00"", ""2015-01-19 08:00"", ""2015-01-19 09:00"", 
""2015-01-19 10:00"", ""2015-01-19 11:00"", ""2015-01-19 12:00"", ""2015-01-19 13:00"", 
""2015-01-19 14:00"", ""2015-01-19 15:00"", ""2015-01-19 16:00"", ""2015-01-19 17:00"", 
""2015-01-19 18:00"", ""2015-01-19 19:00"", ""2015-01-19 20:00"", ""2015-01-19 21:00"", 
""2015-01-19 22:00"", ""2015-01-19 23:00"", ""2015-01-20 00:00"", ""2015-01-20 01:00"", 
""2015-01-20 02:00"", ""2015-01-20 03:00"", ""2015-01-20 04:00"", ""2015-01-20 05:00"", 
""2015-01-20 06:00"", ""2015-01-20 07:00"", ""2015-01-20 08:00"", ""2015-01-20 09:00"", 
""2015-01-20 10:00"", ""2015-01-20 11:00"", ""2015-01-20 12:00"", ""2015-01-20 13:00"", 
""2015-01-20 14:00"", ""2015-01-20 15:00"", ""2015-01-20 16:00"", ""2015-01-20 17:00"", 
""2015-01-20 18:00"", ""2015-01-20 19:00"", ""2015-01-20 20:00"", ""2015-01-20 21:00"", 
""2015-01-20 22:00"", ""2015-01-20 23:00"", ""2015-01-21 00:00"", ""2015-01-21 01:00"", 
""2015-01-21 02:00"", ""2015-01-21 03:00"", ""2015-01-21 04:00"", ""2015-01-21 05:00"", 
""2015-01-21 06:00"", ""2015-01-21 07:00"", ""2015-01-21 08:00"", ""2015-01-21 09:00"", 
""2015-01-21 10:00"", ""2015-01-21 11:00"", ""2015-01-21 12:00"", ""2015-01-21 13:00"", 
""2015-01-21 14:00"", ""2015-01-21 15:00"", ""2015-01-21 16:00"", ""2015-01-21 17:00"", 
""2015-01-21 18:00"", ""2015-01-21 19:00"", ""2015-01-21 20:00"", ""2015-01-21 21:00"", 
""2015-01-21 22:00"", ""2015-01-21 23:00"", ""2015-01-22 00:00"", ""2015-01-22 01:00"", 
""2015-01-22 02:00"", ""2015-01-22 03:00"", ""2015-01-22 04:00"", ""2015-01-22 05:00"", 
""2015-01-22 06:00"", ""2015-01-22 07:00"", ""2015-01-22 08:00"", ""2015-01-22 09:00"", 
""2015-01-22 10:00"", ""2015-01-22 11:00"", ""2015-01-22 12:00"", ""2015-01-22 13:00"", 
""2015-01-22 14:00"", ""2015-01-22 15:00"", ""2015-01-22 16:00"", ""2015-01-22 17:00"", 
""2015-01-22 18:00"", ""2015-01-22 19:00"", ""2015-01-22 20:00""), 
    WaitTime = c(11, 14, 17, 5, 5, 5.5, 8, NA, 5.5, 6.5, 
    8.5, 4, 5, 9, 10, 11, 7, 6, 7, 7, 5, 6, 9, 9, 6.5, 9, 3.5, 
    2, 15, 2.5, 17, 5, 5.5, 7, 6, 3.5, 6, 9.5, 5, 7, 4, 5, 4, 
    9.5, 3.5, 5, 4, 4, 9, 4.5, 6, 10, NA, 9.5, 15, 9, 5.5, 7.5, 
    12, 17.5, 19, 7, 14, 17, 3.5, 6, 15, 11, 10.5, 11, 13, 9.5, 
    9, 7, 4, 6, 15, 5, 18, 5, 6, 19, 19, 6, 7, 7.5, 7.5, 7, 6.5, 
    9, 10, 5.5, 5, 7.5, 5, 4, 10, 7, 5, 12, 6, NA, 4, 2, 5, 7.5, 
    11, 13, 7, 8, 7.5, 5.5, 7.5, 15, 7, 4.5, 9, 3, 4, 6, 17.5, 
    11, 7, 6, 7, 4.5, 4, 4, 5, 10, 14, 7, 7, 4, 7.5, 11, 6, 11, 
    7.5, 15, 23.5, 8, 12, 5, 9, 10, 4, 9, 6, 8.5, 7.5, 6, 5, 
    8, 6, 5.5, 8, 11, 10.5, 4, 6, 7, 10, 11.5, 11.5, 3, 4, 16, 
    3, 2, 2, 8, 4.5, 7, 4, 8, 11, 6.5, 7.5, 17, 6, 6.5, 9, 12, 
    17, 10, 5, 5, 9, 3, 8.5, 11, 4.5, 7, 16, 11, 14, 6.5, 15, 
    8.5, 7, 6.5, 11, 2, 2, 13.5, 4, 2, 16, 11.5, 3.5, 9, 16.5, 
    2.5, 4.5, 8.5, 5, 6, 7.5, 9.5, NA, 9.5, 8, 2.5, 4, 12, 13, 
    10, 4, 6, 16, 16, 13, 8, 12, 19, 19, 5.5, 8, 6.5, NA, NA, 
    NA, 15, 12, NA, 6, 11, 8, 4, 2, 3, 4, 10, 7, 5, 4.5, 4, 5, 
    11.5, 12, 10.5, 4.5, 3, 4, 7, 15.5, 9.5, NA, 9.5, 12, 13.5, 
    10, 10, 13, 6, 8.5, 15, 16.5, 9.5, 14, 9, 9.5, 11, 15, 14, 
    5.5, 6, 14, 16, 9.5, 23, NA, 19, 12, 5, 11, 16, 8, 11, 9, 
    13, 6, 7, 3, 5.5, 7.5, 19, 6.5, 5.5, 4.5, 7, 8, 7, 10, 11, 
    13, NA, 12, 1.5, 7, 7, 12, 8, 6, 9, 15, 9, 3, 5, 11, 11, 
    8, 6, 3, 7.5, 4, 7, 7.5, NA, NA, NA, NA, 6.5, 2, 16.5, 7.5, 
    8, 8, 5, 2, 7, 4, 6.5, 4.5, 10, 6, 4.5, 6.5, 9, 2, 6, 3.5, 
    NA, 5, 7, 3.5, 4, 4.5, 13, 19, 8.5, 10, 8, 13, 10, 10, 6, 
    13.5, 12, 11, 5.5, 6, 3.5, 9, 8, NA, 6, 5, 8.5, 3, 12, 10, 
    9.5, 7, 24, 7, 9, 11.5, 5, 7, 11, 6, 5.5, 3, 4.5, 4, 5, 5, 
    3, 4.5, 6, 10, 5, 4, 4, 9.5, 5, 7, 6, 3, 13, 5.5, 5, 7.5, 
    3, 5, 6.5, 5, 5.5, 6, 4, 3, 5, NA, 5, 5, 6, 7, 8, 5, 5.5, 
    9, 6, 8.5, 9.5, 8, 9, 6, 12, 5, 7, 5, 3.5, 4, 7.5, 7, 5, 
    4, 4, NA, 7, 5.5, 6, 8.5, 6.5, 9, 3, 2, 8, 15, 6, 4, 10, 
    7, 13, 14, 9.5, 9, 18, 6, 5, 4, 6, 4, 11.5, 17.5, 7, 8, 10, 
    4, 7, 5, 9, 6, 5, 4, 8, 4, 2, 1.5, 3.5, 6, 5.5, 5, 4, 8, 
    10.5, 4, 11, 9.5, 5, 6, 11, 21, 9.5, 11, 13.5, 7.5, 13, 10, 
    7, 9.5, 6, 10), CustCount = c(2, 6, 3, 5, 3, 2, 2, NA, 2, 6, 
    12, 11, 9, 10, 13, 9, 11, 7, 12, 8, 6, 4, 10, 6, 2, 7, 2, 
    1, 3, 2, 1, 3, 8, 7, 7, 8, 13, 13, 13, 11, 12, 4, 12, 18, 
    12, 7, 5, 4, 6, 4, 3, 3, NA, 4, 2, 8, 8, 8, 7, 3, 5, 3, 7, 
    8, 7, 7, 11, 8, 10, 3, 10, 6, 5, 5, 3, 1, 2, 1, 1, 3, 4, 
    8, 8, 5, 9, 12, 12, 11, 8, 5, 9, 10, 7, 8, 4, 6, 4, 1, 3, 
    1, 3, NA, 2, 1, 4, 10, 7, 13, 6, 9, 6, 16, 12, 11, 10, 12, 
    9, 7, 7, 7, 6, 2, 3, 1, 1, 2, 2, 3, 11, 10, 9, 8, 9, 13, 
    6, 6, 10, 9, 11, 10, 8, 7, 6, 4, 2, 3, 5, 3, 2, 4, 4, 4, 
    8, 5, 12, 8, 7, 12, 9, 12, 12, 12, 13, 12, 9, 8, 9, 10, 4, 
    7, 4, 2, 2, 4, 1, 7, 6, 6, 8, 11, 11, 5, 7, 6, 9, 12, 15, 
    9, 11, 5, 10, 5, 4, 4, 2, 3, 3, 2, 5, 4, 7, 8, 6, 6, 5, 12, 
    10, 8, 10, 10, 4, 13, 12, 6, 8, 6, 3, 1, 4, 2, NA, 4, 3, 
    2, 6, 5, 8, 10, 4, 13, 2, 13, 8, 11, 13, 8, 9, 10, 9, 5, 
    1, NA, 1, 1, 2, NA, 1, 7, 6, 10, 7, 8, 12, 12, 9, 5, 6, 8, 
    13, 13, 13, 8, 8, 1, 5, 7, 6, 2, NA, 2, 1, 2, 7, 9, 12, 12, 
    10, 10, 10, 6, 8, 2, 8, 3, 4, 5, 6, 2, 2, 1, 4, 1, NA, 3, 
    1, 3, 8, 8, 11, 11, 12, 5, 7, 14, 9, 10, 14, 11, 8, 6, 8, 
    7, 5, 4, 3, 4, 9, NA, 2, 4, 5, 8, 2, 12, 8, 15, 12, 8, 9, 
    12, 9, 9, 12, 7, 7, 8, 7, 5, 4, NA, 1, NA, NA, 4, 9, 8, 8, 
    8, 12, 13, 7, 11, 8, 14, 12, 13, 15, 8, 6, 4, 4, 5, 2, NA, 
    2, 5, 4, 5, 6, 15, 11, 10, 16, 10, 5, 5, 10, 13, 10, 9, 8, 
    7, 5, 4, 5, 6, NA, 2, 5, 4, 1, 6, 5, 8, 4, 3, 10, 11, 8, 
    12, 10, 10, 10, 12, 10, 10, 7, 5, 7, 3, 4, 3, 3, 3, 3, 8, 
    4, 8, 10, 5, 10, 10, 10, 11, 10, 11, 7, 10, 7, 6, 7, 7, 3, 
    3, NA, 3, 6, 5, 3, 3, 5, 6, 6, 13, 14, 14, 7, 13, 9, 10, 
    4, 9, 10, 8, 3, 6, 10, 5, 2, 1, NA, 3, 4, 4, 12, 12, 11, 
    12, 11, 13, 10, 9, 11, 11, 14, 10, 13, 10, 7, 11, 1, 3, 1, 
    4, 1, 2, 2, 3, 9, 6, 9, 9, 8, 9, 7, 12, 17, 13, 9, 10, 8, 
    8, 10, 2, 3, 3, 6, 2, 2, 1, 6, 8, 7, 9, 5, 11, 8, 8, 12, 
    13, 14, 10, 7, 5, 11)), .Names = c(""DateTime"", ""WaitTime"", 
""CustCount""), row.names = c(NA, 525L), class = ""data.frame"")</p>
"
"0.138842026900125","0.181443684650606","208526","<p>Assume I have a time series $ x_t $ that I want to fit using an ARIMA(1,1,0) model of the form:</p>

<blockquote>
  <p>$ \Delta x_t = \alpha \Delta x_{t-1} + w_t  $</p>
</blockquote>

<p>This could be rewritten as: </p>

<blockquote>
  <p>$ x_t - x_{t-1} = \alpha ( x_{t-1} - x_{t-2} )+ w_t  $</p>
  
  <p>$ x_t = ( 1 + \alpha)x_{t-1} - \alpha x_{t-2} + w_t  $</p>
</blockquote>

<p>The last equation describes an AR(2) model with coefficients $1+\alpha$ and $-\alpha$. I recognize that, depending on $\alpha$, this AR(2) model might be non-stationary. However, if I was taking a diff to begin with, then the series I am modeling shouldn't be stationary.</p>

<p>I know that if the model is non-stationary, a diff should be used. But how would the results differ if I used a AR(2) model vs an ARIMA(1,1,0) model? I assume (as hinted by R) that it has an issue with convergence. However, when I ask R to perform the fits, it will do both of them, and the coefficients are (mostly) consistent with my observations above. The forecasts are definitely different, though.</p>

<p>If anyone could shed some light on this, or point me to a good reference, I would appreciate it. </p>

<p>Here is the R code I used to generate both models. </p>

<pre><code>&gt; set.seed(2)
&gt; x &lt;- arima.sim(n = 1000, model=list(order=c(1,1,0), ar=c(0.3)))
&gt; plot(x)
&gt; arima(x, order=c(1,1,0))

Call:
arima(x = x, order = c(1, 1, 0))

Coefficients:
         ar1
      0.3291
s.e.  0.0298

sigma^2 estimated as 1.03:  log likelihood = -1433.91,  aic = 2871.81
&gt; arima(x, order=c(2,0,0))

Call:
arima(x = x, order = c(2, 0, 0))

Coefficients:
         ar1      ar2  intercept
      1.3290  -0.3294    50.9803
s.e.  0.0298   0.0299    35.9741

sigma^2 estimated as 1.03:  log likelihood = -1438.93,  aic = 2885.86
Warning messages:
1: In log(s2) : NaNs produced
2: In log(s2) : NaNs produced
3: In log(s2) : NaNs produced
4: In arima(x, order = c(2, 0, 0)) :
  possible convergence problem: optim gave code = 1
</code></pre>
"
"0.111321277616897","0.145478593490662","208515","<p>I'm trying to understand the steps in Rob Hyndman's Multi-step forecasts without re-estimation example below.  I'm wondering what the purpose is of </p>

<pre><code>refit &lt;- Arima(x, model=fit)
</code></pre>

<p>The model has already been determined and trained by auto.arima in the ""fit"" step.  So in the ""refit"" step are we re-training the model on a new data set?  If so, what is the point of retraining the same model on a new data set?</p>

<p>url:
<a href=""http://robjhyndman.com/hyndsight/rolling-forecasts/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/rolling-forecasts/</a></p>

<p>Code:</p>

<pre><code>library(fpp)

h &lt;- 5
train &lt;- window(hsales,end=1989.99)
test &lt;- window(hsales,start=1990)
n &lt;- length(test) - h + 1
fit &lt;- auto.arima(train)
fc &lt;- ts(numeric(n), start=1990+(h-1)/12, freq=12)
for(i in 1:n)
{  
  x &lt;- window(hsales, end=1989.99 + (i-1)/12)
  refit &lt;- Arima(x, model=fit)
  fc[i] &lt;- forecast(refit, h=h)$mean[h]
}
</code></pre>

<p>Updated Code to re-estimate coefficients:</p>

<pre><code>h &lt;- 5
train &lt;- window(hsales,end=1989.99)
test &lt;- window(hsales,start=1990)
n &lt;- length(test) - h + 1
fit &lt;- auto.arima(train)
order &lt;- arimaorder(fit)
fc &lt;- ts(numeric(n), start=1990+(h-1)/12, freq=12)
for(i in 1:n)
{  
  x &lt;- window(hsales, end=1989.99 + (i-1)/12)
  refit &lt;- Arima(x, order=order[1:3],seasonal=order[4:6])
  fc[i] &lt;- forecast(refit, h=h)$mean[h]
}
</code></pre>
"
"0.0850230301897704","0.111111111111111","209080","<p>I am trying to figure out how the first residual is calculated in an AR(1) model. It's easy to generate all of the other residuals, but I have no idea how r calculates the first one. </p>

<p>Here is an example that I am working with:</p>

<pre><code>&gt; set.seed(1)  #use 390
&gt; x &lt;- arima.sim(n = 20, model=list(order=c(1,0,0), ar=c(0.7)))
&gt; fit &lt;- arima(x, c(1,0,0), include.mean = F)
&gt; residuals &lt;- 0
&gt; residuals[2:20] &lt;- x[2:20] - fit$coef[1] * x[1:19]
    &gt; data.frame(residuals, fit$residuals)
     residuals fit.residuals
1   0.00000000    0.99077920
2   0.56625275    0.56625275
3   0.88811131    0.88811131
4   0.74271680    0.74271680
5   0.03181057    0.03181057
6  -2.02072514   -2.02072514
7   0.63642551    0.63642551
8  -0.05652348   -0.05652348
9  -0.15498384   -0.15498384
10 -1.46716431   -1.46716431
11 -0.44712965   -0.44712965
12  0.44892420    0.44892420
13  1.37226611    1.37226611
14 -0.11961349   -0.11961349
15  0.37788599    0.37788599
16 -0.06816952   -0.06816952
17 -1.38607175   -1.38607175
18 -0.39461047   -0.39461047
19 -0.37197692   -0.37197692
20 -0.03605144   -0.03605144
</code></pre>

<p>Ultimately, I would like to get a clearer understanding of how forecasts are generated for ARIMA models. But, to forecast the MA portion, I need to know the residuals for all of the observed values in the series. Not understanding how to calculate the first residual thus poses an issue. </p>

<p>Thanks. </p>
"
"0.183670737350935","0.188592983287001","209247","<p>I am trying to predict surface temperature using solar energy. I have 3650 daily averages for both variables. The plots of both are below: </p>

<p><a href=""http://i.stack.imgur.com/ywSHs.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ywSHs.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/JjFDG.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/JjFDG.png"" alt=""enter image description here""></a></p>

<p>I attempt to seasonally adjust with a periodic regression in R for both: </p>

<pre><code>stmp.model.sa &lt;- lm(stmp ~ sin((2*pi/365)*t) + cos((2*pi/365)*t))
slrd.model.sa &lt;- lm(slrd ~ sin((2*pi/365)*t) + cos((2*pi/365)*t))
</code></pre>

<p>Here are the plots of the residuals from these models: 
<a href=""http://i.stack.imgur.com/jhdLw.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/jhdLw.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/fyzuq.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fyzuq.png"" alt=""enter image description here""></a></p>

<p>As you can see, the temperature data responded well to the treatment. The solar energy data did not, as the yearly humps are still somewhat present.</p>

<p>A few questions: </p>

<ul>
<li>Is there a more effective way to remove seasonality for the solar data? </li>
<li>Is further treatment of the temperature trend recommended? Would a polynomial regression be sufficient to remove this trend?</li>
<li>What model might be recommended to predict the temperature? Linear model, ARIMA/ARIMAX models, linear regression with ARIMA errors, etc? </li>
</ul>

<p>Thanks in advance for any responses! </p>

<p>EDIT:</p>

<p>I attempted to apply a Hodrick Prescott filter (lambda = 100*365^2) with poor results. I then attempted to fit a cycle to the solar data using a 20 period moving maximum. This was done using the following code:</p>

<pre><code>seq &lt;- 11:(n-11)
ns &lt;- length(seq)
slrd.seasonal &lt;- slrd[seq]
for(i in seq){
  slrd.seasonal[i-10] &lt;- max(slrd[(i-10):(i+10)])
}
</code></pre>

<p>The moving maximum, the solar cycle, and the cycle subtracted from the original series is presented below: </p>

<p><a href=""http://i.stack.imgur.com/k1RuT.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/k1RuT.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/2QwSR.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/2QwSR.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/IMMjE.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/IMMjE.png"" alt=""enter image description here""></a></p>

<p>This did not remove the yearly cycle entirely either. Any advice?
EDIT 2: </p>

<p>I have successfully removed seasonality by fitting a 20 order polynomial to the first three years (using more years proves computationally difficult). If anyone can think of a better or more elegant way to achieve this, let me know. </p>
"
"0.0850230301897704","0.037037037037037","214382","<p>If a predictor is negatively correlated with a variable you are trying to forecast in an Arima model, will Arima pick up the negative correlation when you add the predictor in the xreg argument?  Is there anything that needs to be done to the predictor when it is added in the xreg argument in order to indicate that it is negatively correlated with the variable you are trying to predict?</p>
"
"0.0981761387347632","0.128300059819917","108925","<p>The following code shows a forecast of the next 24 hours of my electricity prices with two exogenous variables. 
My problem is, that I don't know how to build a forecast for the next 3 days or more because for example I have to take the first 24 hours into account when I want to predict the prices for the second day(25-48). And the time of my dummys and variables also have to grow in 24 hours steps. </p>

<p>I know that a loop is a solution but I  don't know how to create the loop. </p>

<p>I hope you understand my problem.  </p>

<p>My next problem is that I have to create a neural network with this data. Can someone give me a hint how to do this? </p>

<p>Thanks for your help =)</p>

<pre><code>tm1 &lt;- (25:6552)
arma.model =  auto.arima(price$Price[tm1],start.p=5,start.q=5,max.p=5,max.q=5, 
                      xreg=cbind(sol.prod$Production[tm1],wind.prod$Production[tm1],
                           price$Price[1:6528]),
                    trace=TRUE, stationary=TRUE)

arma.model 


PriceForecast = predict(object=arma.model,n.ahead=24, 
                    xreg=cbind(sol.prod$Production[tm1],wind.prod$Production[tm1],
                               price$Price[1:6528]),
                        newxreg=cbind(sol.prod$Production[6553:6576],wind.prod$Production[6553:6576],
                                  price$Price[6529:6552]))
</code></pre>
"
"0.196352277469526","0.224525104684854","58657","<p>I'm using a daily time series of sales data that contains about 2 years of daily data points. Based on some of the online-tutorials / examples I tried to identify the seasonality in the data. It seems that there is a weekly, monthly and probably a yearly periodicity / seasonality.</p>

<p>For example, there are paydays, particularly on 1st payday of the month effect that lasts for few days during the week. There are also some specific Holiday effects, clearly identifiable by noting the observations.</p>

<p>Equipped with some of these observations, I tried the following:</p>

<ol>
<li><p>ARIMA (with <code>Arima</code> and <code>auto.arima</code> from R-forecast package), using regressor (and other default values needed in the function).  The regressor I created is basically a matrix of 0/1 values:</p>

<ul>
<li>11 month (n-1) variables</li>
<li>12 holiday variables</li>
<li>Could not figure out the payday part...since it's little more complicated effect than I thought. The payday effect works differently, depending on the weekday of the 1st of month.</li>
</ul>

<p>I used 7 (i.e., weekly frequency) to model the time series. I tried the test - forecasting 7 days at a time. The results are reasonable: average accuracy for a forecast of 11 weeks comes to weekly avg RMSE to 5%.</p></li>
<li><p>TBATS model (from R-forecast package) - using multiple seasonality (7, 30.4375, 365.25) and obviously no regressor. The accuracy is surprisingly better than the ARIMA model at weekly avg RMSE 3.5% .</p>

<p>In this case, the model without ARMA errors perform slightly better. Now If I apply the coefficients for just the Holiday Effects from the ARIMA model described in #1, to the results of the TBATS model the weekly avg RMSE improves to 2.95%</p></li>
</ol>

<p>Now without having much background or knowledge on the underlying theories of these models, I'm in a dilemma whether this TBATS approach is even a valid one. Even though it's improving the RMSE significantly in the 11 weeks test, I'm wondering whether it can sustain this accuracy in the future. Or even if applying Holiday effects from ARIMA to the TBATS result is justifiable. Any thoughts from any / all the contributors will be highly appreciated. </p>

<p><a href=""https://s3.amazonaws.com/CKI-FILE-SHARE/TS+Test+Data.txt"">Link for Test Data</a></p>

<p>Note: Do ""Save Link As"", to download the file.</p>
"
"0.147264208102145","0.192450089729875","63681","<p>I have been adamantly searching the web to learn how to successfully implement a dynamic regression time series in the forecast package for R. The time series data that I am using is weekly data (frequency=52) of incoming call volume and prediction variables are mailers sent out every now and then. They are a significant predictor of the data for the week that they hit, the following week, and the week after that. I have created lagged variables and use these three as the predictors. </p>

<p>My main concern is that the arima model is not taking into account the time series frequency. When I tell it to recognize the ts with a frequency of 52 it has an error. 
I have looked at the <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">fortrain function</a> but do not understand it. I also have looked at the tbats suggested but found that those will not work with prediction variables. </p>

<p>The Zoo function recognizes 52 frequency but it is not advised to use with the <a href=""http://stackoverflow.com/questions/16050684/using-the-combination-forecastauto-arima"">forecast package</a>.</p>

<p>Here is the basic code. The problem is that the time series calwater[,5] is not recognized as such. It is imputed as a simple vector as an integer...</p>

<pre><code>#this works without taking into acount the ts
fit2 &lt;- auto.arima(calwater[6:96,5], xreg=calwater[6:96,6:8], d=0)
fccal &lt;- forecast(fit2, xreg=calwater[97:106,6:8], h=10)
fccal
plot(fccal, main=""Forecast Cal Water"", ylab=""Calls"")

#to form a ts object
calincall&lt;-ts(calwater[1:106,5],start=c(2011,23),frequency=52)

#once the ts is added to the model this dispalys
#Error in `[.default`(calincall, 2:100, 1) : incorrect number of dimensions
</code></pre>

<p>Maybe the error is because there is just a little over two years of data. </p>

<pre><code>#Time Series: Start = c(2011, 23), End = c(2013, 24),Frequency = 52 
</code></pre>

<p>I would be very grateful for any guidance in for this particular issue. I am using the forecast package and prefer to continue within the package but I am open to suggestions. </p>
"
"0.0694210134500623","0.0907218423253029","148371","<p>I'm trying to fit a sarima model on the univariate data with 180 points (periodicity=12). I use the auto.arima function in R. After fitting a model to the data, the only problem is the violation of the normality assumption. Then, I refit models after transforming the data but the residuals are still non-normal. For transformation of the data, I use both BoxCox.lambda (in forecast package) and boxcoxnc (in AID package) functions. Can anybody help me to fix this problem?</p>

<pre><code>ser=c(1.887090e+04, -6.023007e+00,  1.193635e-02, -1.455856e-05,  1.064251e-08, -4.953592e-12,  1.517229e-15, -3.090332e-19,
4.137144e-23, -3.491891e-27,  1.682794e-31, -3.527046e-36,  1.904962e+04, -7.394189e+00,  1.600849e-02, -2.077511e-05,
1.585519e-08,-7.587987e-12,    2.363570e-15, -4.859251e-19,  6.534816e-23, -5.525202e-27,  2.663420e-31, -5.580438e-36,
2.009098e+04, -1.061082e+01,  2.319182e-02, -2.917768e-05,  2.171827e-08, -1.019917e-11,  3.133564e-15, -6.379905e-19,
8.520995e-23, -7.168462e-27,  3.442102e-31, -7.188143e-36,  2.067028e+04, -8.034999e+00,  1.761326e-02, -2.240562e-05,
1.680919e-08, -7.961614e-12,  2.469832e-15, -5.081494e-19,  6.861040e-23, -5.835236e-27,  2.831898e-31, -5.974519e-36,
2.233604e+04, -1.033148e+01,  2.287039e-02, -2.952031e-05,  2.255568e-08, -1.086351e-11,  3.419260e-15, -7.123005e-19,
9.720229e-23, -8.341734e-27,  4.079166e-31, -8.660882e-36,  2.392045e+04, -8.246481e+00,  1.585412e-02, -2.056180e-05,
1.636424e-08, -8.253437e-12,  2.710813e-15, -5.858824e-19,  8.245204e-23, -7.258003e-27,  3.624039e-31, -7.827743e-36,
2.636514e+04, -9.886355e+00,  1.951992e-02, -2.504930e-05,  1.963158e-08, -9.789139e-12,  3.190186e-15, -6.856046e-19,
9.606813e-23, -8.427664e-27,  4.196799e-31, -9.046539e-36,  2.866210e+04, -8.866902e+00,  1.734494e-02, -2.387617e-05,
1.957175e-08, -9.993900e-12,  3.300201e-15, -7.152619e-19,  1.008517e-22, -8.892694e-27,  4.448060e-31, -9.626143e-36,
3.002254e+04, -1.007403e+01,  2.151203e-02, -2.984675e-05,  2.427803e-08, -1.226036e-11,  3.997630e-15, -8.550747e-19,
1.190499e-22, -1.037815e-26,  5.140218e-31, -1.103334e-35,  2.929311e+04, -1.123255e+01,  2.282206e-02, -2.968240e-05,
2.323868e-08, -1.146069e-11,  3.677709e-15, -7.777557e-19,  1.073806e-22, -9.301478e-27,  4.584147e-31, -9.800725e-36,
3.306894e+04, -1.396117e+01,  2.326777e-02, -2.724425e-05,  2.023428e-08, -9.690231e-12,  3.055811e-15, -6.392630e-19,
8.763020e-23, -7.552202e-27,  3.707622e-31, -7.901994e-36,  3.491666e+04, -1.315883e+01,  2.554492e-02, -3.194439e-05,
2.437661e-08, -1.184053e-11,  3.762542e-15, -7.896499e-19,  1.082565e-22, -9.310722e-27,  4.554895e-31, -9.664092e-36,
3.775600e+04, -2.101521e+01,  4.695457e-02, -6.000206e-05,  4.510264e-08, -2.134088e-11,  6.600784e-15, -1.352465e-18,
1.817468e-22, -1.538166e-26,  7.429410e-31, -1.560507e-35,  3.699341e+04, -1.019327e+01,  1.761360e-02, -2.428662e-05,
2.084200e-08, -1.112473e-11,  3.796505e-15, -8.415154e-19,  1.204392e-22, -1.072641e-26,  5.402195e-31, -1.174885e-35,
4.009280e+04, -1.887174e+01,  3.441926e-02, -4.161190e-05,  3.152055e-08, -1.535050e-11,  4.911316e-15, -1.040003e-18,
1.440215e-22, -1.251900e-26,  6.190925e-31, -1.327693e-35)

require(""forecast"")
fit=auto.arima(ser,d = 0,D = 1,max.p = 6, max.q = 6,max.P = 6, max.Q = 6, max.order = 25,start.p=1, start.q=1, start.P=1, start.Q=1,stationary = FALSE,
seasonal=TRUE,stepwise=TRUE,trace=TRUE,approximation=FALSE,allowdrift=FALSE,ic=""aicc"")
</code></pre>
"
"0.170046060379541","0.203703703703704","132845","<p>I have a problem in interpreting what the <code>arima</code> function in R is doing.  I have the following code:</p>

<pre><code>x &lt;- 1000*.8^(0:100)
arima(x, order = c(1,0,0), include.mean = F)
</code></pre>

<p>The resulting coefficient is ""0.9988"".  But I would think the coefficient should be exactly ""0.8"", since <code>x[t] = 0.8 * x[t-1]</code>.</p>

<p>I must be missing something that R is doing in processing the data.</p>

<p>Any help would be appreciated.</p>

<p><strong>New Info I:</strong>
If I change the function to </p>

<pre><code>arima(x, order = c(1,0,0), include.mean = F, method=""CSS"")
</code></pre>

<p>then it solves for the correct coefficient of 0.8.</p>

<p>My problem that I am generally finding that the function <code>arima</code> with <code>order = c(1,0,0)</code> is often producing different results to:</p>

<pre><code>lm(x[-1] ~ I(x[-length(x)]))
</code></pre>

<p>for all sorts of different time series that I am reviewing. </p>

<p><strong>New Info II:</strong>
Some of the comments and answers below are concerned that there is no random fluctuation in my data.  I did this to make the problem as simple as possible, but even if you add in random fluctuation, <code>arima</code> still produces the same wrong results in the default case.  To add insult to injury, <code>arima</code> will sometimes get the right answer if you change <code>method = ""CSS""</code>. This suggest that there is perhaps a computational issue with <code>arima</code> and not my misunderstanding the statistical model. Here is an extended set of two examples that highlight the problem (I ignore the intercept difference between <code>lm</code> and <code>arima</code> as these two items are not the same thing, but the coefficients should be.</p>

<pre><code>set.seed(1)
# Example 2
x &lt;- rep(1000,200)
for (i in 2:200) x[i]=x[i-1]*.8 + runif(1)*100 
plot(x,type=""l"")
arima(x, order = c(1,0,0))  #Incorrect answer:  coefficient = 0.9944
arima(x, order = c(1,0,0), method=""CSS"") # correct answer: coefficient = 0.7915
lm(x[-1] ~ I(x[-length(x)])) # correct answer: coefficient = 0.7914

# Example 3
x &lt;- rep(0,200)
for (i in 2:200) x[i]=x[i-1]*.8 + runif(1)*.2 
plot(x,type=""l"")
arima(x, order = c(1,0,0))  #Incorrect answer:  coefficient = 0.8836
arima(x, order = c(1,0,0), method=""CSS"") # correct answer: coefficient = 0.8158
lm(x[-1] ~ I(x[-length(x)])) # correct answer: coefficient = 0.8158
</code></pre>
"
"0.129874823886379","0.121232161242218","64033","<p>I have an <code>ARIMA(0,2,1)</code> model. How do i estimate the $\hat{e}_t$ component of the model. I have read a whole lot of theories that confuses me the more. Is there any practical way of estimating this $\hat{e}_t$? Does <code>R</code> offer any help to that too?</p>

<p>I know that my <code>ARIMA(0,2,1)</code> model can be written as $Y_{t} = 2Y_{t-1} - Y_{t-2} + e_{t} + \theta e_{t-1}$. I want to forecast 1-time ahead into the future. In that case, my forecast equation is given as $\hat Y_{t}(1) = 2Y_{t} -Y_{t-1} + \theta \hat e_{t}$. I know my value for $ Y_{t} =7.8$ and $ Y_{t-1} =7.8 $. I know my value of \theta as -0.6816, which i obtained from my <code>R</code> output. My problem now is, how do i determine the value for my $\hat e_{t} $ so i could find $ \hat Y_{t}(1)$? I have an <code>R</code> code that gives me all these forecasts though, but i want to know how <code>R</code> generated my first forecast and how it found the estimate for $\hat e_{t} $.</p>

<p>Thanks for looking!</p>
"
"0.0850230301897704","0.111111111111111","148820","<p>I am working on an alogorithm in R to automatize a monthly forecast calculation. I am using, among others, the forecast(method='arima') function from the forecast package to calculate forecast. It is working very well. But for some times series some forecast are quite strange.</p>

<p>Please find below the code i'm using:</p>

<p><code>train_ts&lt;- ts(values, frequency=12)
fit1 &lt;- stl(train_ts, s.window=""periodic"",t.window=24, )
arima &lt;- forecast(fit1,h=forecasthorizon,method ='arima')</code></p>

<p><code>values &lt;- c(27, 27, 7, 24, 39, 40, 24, 45, 36, 37, 31, 47, 16, 24, 6, 21, 35, 36, 21, 40, 32, 33, 27, 42, 14, 21, 5,   19, 31, 32, 19, 36, 29, 29, 24, 42, 15, 24, 21)</code></p>

<p>Here, on the graph, you will see the historical data (black), the fitted value (green) and the forecast(blue). The forecast is not in lines with the fitted value.</p>

<p><img src=""http://i.stack.imgur.com/5530d.png"" alt=""enter image description here"">
As you can see the Forecast is not in line with the history,
My question is ""does a setup for Arima to bound the forecast in line with the history exist"" ?</p>
"
"0.0981761387347632","0.128300059819917","148990","<p>I've been using <a href=""http://cran.r-project.org/web/packages/dlm/index.html"" rel=""nofollow"">DLM</a> package for modeling my timeseries in state-space format, and then use Kalman Filter to get better 2 step-ahead forecasts. </p>

<p>Even though I've read the <a href=""http://cran.r-project.org/web/packages/dlm/dlm.pdf"" rel=""nofollow"">vignette</a> and parts of <a href=""http://rads.stackoverflow.com/amzn/click/0387772375"" rel=""nofollow"">their book</a>, I'm still struggling when it comes to modeling my ARIMA process in state-space format, specifically as the dlm package understands it. My plan is to use <strong>auto.arima()</strong> to get the best arima model, then represent it as a dlm.</p>

<p>I've seen examples like <a href=""http://stats.stackexchange.com/q/66156/42176"">this</a>.</p>

<pre><code># Estimation of a state space representation of Arima(1,1,1) model and forecast:
level0 &lt;- data.s.g[1]
slope0 &lt;- mean(diff(data.s.g))
buildGap &lt;- function(u) {
trend &lt;- dlmModPoly(dV = 1e-7, dW = exp(u[1 : 2]),
                  m0 = c(level0, slope0),
                  C0 = 2 * diag(2))
gap &lt;- dlmModARMA(ar = ARtransPars(u[4]),ma=u[5], sigma2 = exp(u[3]))
return(trend + gap)}
</code></pre>

<p>and the dlmModARMA part seems clear. What is baffling me is, what to do if I want to get for example ARIMA(1,3,1); how would I represent that 3 differencing part?</p>
"
"0.202395294959361","0.233380014004668","64711","<p>I have a time series I am trying to forecast, for which I have used the seasonal ARIMA(0,0,0)(0,1,0)[12] model (=fit2). It is different from what R suggested with auto.arima (R calculated ARIMA(0,1,1)(0,1,0)[12] would be a better fit, I named it fit1). However, in the last 12 months of my time series my model (fit2) seems to be a better fit when adjusted (it was chronically biased, I have added the residual mean and the new fit seems to sit more snugly around the original time series. Here is the example of the last 12 months and MAPE for 12 most recent months for both fits:</p>

<p><img src=""http://i.stack.imgur.com/kkUOb.png"" alt=""fit1, fit2 and original data""></p>

<p>The time series looks like this:</p>

<p><img src=""http://i.stack.imgur.com/twNkT.png"" alt=""original time series""></p>

<p>So far so good. I have performed residual analysis for both models, and here is the confusion. </p>

<p>The acf(resid(fit1)) looks great, very white-noisey:</p>

<p><img src=""http://i.stack.imgur.com/gyIv3.png"" alt=""acf of fit1""></p>

<p>However, Ljung-Box test doesn't look good for , for instance, 20 lags: </p>

<pre><code>    Box.test(resid(fit1),type=""Ljung"",lag=20,fitdf=1)
</code></pre>

<p>I get the following results:</p>

<pre><code>    X-squared = 26.8511, df = 19, p-value = 0.1082
</code></pre>

<p>To my understanding, this is the confirmation that the residuals are not independent ( p-value is too big to stay with the Independence Hypothesis). </p>

<p>However, for lag 1 everything is great:</p>

<pre><code>    Box.test(resid(fit1),type=""Ljung"",lag=1,fitdf=1)
</code></pre>

<p>gives me the result: </p>

<pre><code>    X-squared = 0.3512, df = 0, p-value &lt; 2.2e-16
</code></pre>

<p>Either I am not understanding the test, or it is slightly contradicting to what I see on the acf plot. The autocorrelation is laughably low. </p>

<p>Then I checked fit2. The autocorrelation function looks like this:</p>

<p><img src=""http://i.stack.imgur.com/JZ7Sc.png"" alt=""acf fit2""></p>

<p>Despite such obvious autocorrelation at several first lags, the Ljung-Box test gave me much better results at 20 lags, than fit1:</p>

<pre><code>    Box.test(resid(fit2),type=""Ljung"",lag=20,fitdf=0)
</code></pre>

<p>results in :</p>

<pre><code>    X-squared = 147.4062, df = 20, p-value &lt; 2.2e-16
</code></pre>

<p>whereas just checking autocorrelation at lag1, also gives me the confirmation of the null-hypothesis! </p>

<pre><code>    Box.test(resid(arima2.fit),type=""Ljung"",lag=1,fitdf=0)
    X-squared = 30.8958, df = 1, p-value = 2.723e-08 
</code></pre>

<p>Am I understanding the test correctly? The p-value should be preferrably smaller than 0.05 in order to confirm the null hypothesis of residuals independence. Which fit is better to use for forecasting, fit1 or fit2? </p>

<p>Additional info: residuals of fit1 display normal distribution, those of fit2 do not.  </p>
"
"NaN","NaN","213949","<p>I have a time series data set and want to predict my data in the future. I would like to when to use a ETS or an ARIMA model? </p>"
"NaN","NaN","<p>Is is true that you can only use a ETS model when a data is non-stationair? And when a data is stationair you need to difference it first and then use an ARIMA model to make some forecasting? </p>",""
"NaN","NaN","<p>I am doing the forecasting in r. </p>",""
"NaN","NaN","","<r><time-series><arima>"
"NaN","NaN","125909","<p>I am trying to predict values using arima(0,1,1).
After doing <code>predict(mod,n.ahead=5)</code> (in <code>R</code>) am getting the same value for all the predictions: </p>

<pre><code>5947.681 5947.681 5947.681 5947.681 5947.681 
</code></pre>

<p>Is it correct?</p>
"
"0.0850230301897704","0.111111111111111","85773","<p>I am trying to fit an ARIMA model for a certain financial time series. I've used EViews for modeling, and have decided to fit a so-called reduced-form MA(3) model, where only the third lag is statistically significant.</p>

<p>Unfortunately, I have not been a to figure out how to do this in R. All I can find is how to fit a regular MA(3) model, using either the 'stats' or 'forecast' packages.</p>

<p>Can anyone please help me out? Thank you!</p>

<p>NOTE: This is a cross-post from SO. The good folks there suggested that I use the argument 'fixed' for the arima{stats} and Arima{forecast} packages, but I can't figure out how to use it just from the documentation.</p>
"
"0.0850230301897704","0.111111111111111","86248","<p>I've been attempting to forecast natural gas power demand and how it is affected by temperature and price. I'm not sure if I have done everything correctly (relatively new to R), but I do seem to get relevant data other than I can't seem to change my forecast period, nor am I sure this is an appropriate model for this data. Hopefully someone can provide me with some guidance.</p>

<p>Data: <a href=""https://www.dropbox.com/s/g9uytz3guyjrbq2/demand.csv"" rel=""nofollow"">demand.csv</a></p>

<pre><code>library(forecast)
data = read.csv(""demand.csv"")

# Create matrix of numeric predictors
xreg &lt;- cbind(weather=data$Weather,price=data$Price,m1=data$M1,
    m2=data$M2,m3=data$M3,m4=data$M4,m5=data$M5,m6=data$M6,
m7=data$M7,m8=data$M8,m9=data$M9,m10=data$M10,m11=data$M11)

# Rename columns
colnames(xreg) &lt;- c(""Weather"",""Price"",""Jan"",""Feb"",""Mar"",""Apr"",
""May"",""Jun"",""Jul"",""Aug"",""Sep"",""Oct"",""Nov"")

# Variable to be modelled
demandTS &lt;- ts(data$Demand, frequency=12)

# Find ARIMAX model
demandArima &lt;- auto.arima(demandTS, xreg=xreg)
demand.fcast &lt;- forecast(demandArima, xreg=xreg)
plot(demand.fcast)
</code></pre>

<p>Thank you for any help.</p>

<p>References:</p>

<p><a href=""http://stats.stackexchange.com/questions/41070/how-to-setup-xreg-argument-in-auto-arima-in-r"">How to setup xreg argument in auto ARIMA in R</a>
<a href=""http://stackoverflow.com/questions/10606295/from-auto-arima-to-forecast-in-r"">From auto ARIMA to forecast in R</a></p>
"
"0.273310803263835","0.207390338946085","135565","<p>I have a few questions about turing a univariate time series into a multivariate time series and optimizing the predictors. Here is the univariate data:</p>

<pre><code>index
22
26
34
33
40
39
39
45
50
58
64
78
51
60
80
80
93
100
96
108
111
119
140
164
103
112
154
135
156
170
146
156
166
176
193
204
</code></pre>

<p>My first step here was to of course create a ts object in R and visualize the data:</p>

<pre><code>tsData &lt;- ts(data = dummyData, start = c(2012,1), end = c(2014,12), frequency = 12)

     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
2012  22  26  34  33  40  39  39  45  50  58  64  78
2013  51  60  80  80  93 100  96 108 111 119 140 164
2014 103 112 154 135 156 170 146 156 166 176 193 204

plot(tsData)
</code></pre>

<p>I interpreted this plot as a deterministic time series with a trend and perhaps a bit of seasonality</p>

<p><img src=""http://i.stack.imgur.com/YD0vW.png"" alt=""enter image description here"">
Examining the acf and pacf plot confirms the trend component of the time series</p>

<p><img src=""http://i.stack.imgur.com/c8eu3.png"" alt=""enter image description here""></p>

<p>My first question has to do with creating trend &amp; seasonal variables for the time series using the decompose() function in R which yields the following plots:</p>

<p><img src=""http://i.stack.imgur.com/9xXYx.png"" alt=""enter image description here""></p>

<p>I understand that the decompose() function in R has created a list of vectors for the trend, seasonal and random components of the original time series but what am I suppose to do with them? Should I cbind() them to my univariate data and model:</p>

<pre><code>lm(index ~ trend + seasonal + random)

         index     trend    seasonal       random
Jan 2012    22        NA -23.8940972           NA
Feb 2012    26        NA -19.4357639           NA
Mar 2012    34        NA   6.8350694           NA
Apr 2012    33        NA  -7.5399306           NA
May 2012    40        NA   4.3142361           NA
Jun 2012    39        NA   9.5017361           NA
Jul 2012    39  45.20833  -6.3524306   0.14409722
Aug 2012    45  47.83333  -0.8315972  -2.00173611
Sep 2012    50  51.16667  -1.1232639  -0.04340278
Oct 2012    58  55.04167   2.2517361   0.70659722
Nov 2012    64  59.20833  11.2100694  -6.41840278
Dec 2012    78  63.95833  25.0642361 -11.02256944
Jan 2013    51  68.87500 -23.8940972   6.01909722
Feb 2013    60  73.87500 -19.4357639   5.56076389
Mar 2013    80  79.04167   6.8350694  -5.87673611
Apr 2013    80  84.12500  -7.5399306   3.41493056
May 2013    93  89.83333   4.3142361  -1.14756944
Jun 2013   100  96.58333   9.5017361  -6.08506944
Jul 2013    96 102.33333  -6.3524306   0.01909722
Aug 2013   108 106.66667  -0.8315972   2.16493056
Sep 2013   111 111.91667  -1.1232639   0.20659722
Oct 2013   119 117.29167   2.2517361  -0.54340278
Nov 2013   140 122.20833  11.2100694   6.58159722
Dec 2013   164 127.75000  25.0642361  11.18576389
Jan 2014   103 132.75000 -23.8940972  -5.85590278
Feb 2014   112 136.83333 -19.4357639  -5.39756944
Mar 2014   154 141.12500   6.8350694   6.03993056
Apr 2014   135 145.79167  -7.5399306  -3.25173611
May 2014   156 150.37500   4.3142361   1.31076389
Jun 2014   170 154.25000   9.5017361   6.24826389
Jul 2014   146        NA  -6.3524306           NA
Aug 2014   156        NA  -0.8315972           NA
Sep 2014   166        NA  -1.1232639           NA
Oct 2014   176        NA   2.2517361           NA
Nov 2014   193        NA  11.2100694           NA
Dec 2014   204        NA  25.0642361           NA
</code></pre>

<p>When I use the auto.arima function in the forecast package is this all happening under the hood? It seems to me that the auto.arima() selected a MA(1) term and a differencing term to deal with the trend? Is my interpretation correct? What is drift?</p>

<pre><code>plot(forecast(auto.arima(tsData, stepwise=FALSE)))

Forecast method: ARIMA(0,0,1)(0,1,0)[12] with drift        

Model Information:
Series: tsData 
ARIMA(0,0,1)(0,1,0)[12] with drift         

Coefficients:
         ma1   drift
      0.9622  4.5780
s.e.  0.4698  0.4352

sigma^2 estimated as 176.6:  log likelihood=-44.52
AIC=95.05   AICc=96.25   BIC=98.58

Error measures:
                    ME     RMSE      MAE        MPE     MAPE       MASE
Training set 0.2459764 7.673429 4.967187 -0.7272714 4.661455 0.08876581
                   ACF1
Training set -0.0791942
</code></pre>

<p><img src=""http://i.stack.imgur.com/3TWL6.png"" alt=""enter image description here""></p>

<p>What happens if I'm interested in expanding the model to include other time series variables such as spend_1 and spend_2? do I need to create trend and seasonal and random variables for each of these spend variables or do I just plug them into the auto.arima as external variables:</p>

<pre><code>auto.ariam(tsData, xreg=spendData, stepwise= FALSE)

spend_1 spend_2
0   0
0   0
0   0
0   0
0   0
0   209
0   0
0   0
0   239
0   0
0   553
0   216
0   0
0   161
0   449
107 0
53  0
120 81
242 0
100 80
482 0
708 81
54  240
688 0
80  0
254 108
183 84
104 191
183 84
243 167
0   108
0   0
0   191
0   191
0   167
0   0
</code></pre>

<p>Once I build this multivariate time series model how do I interpret the coefficients for spend_1 and spend_2? How do to optimize them in order to maximize the index variable where the model was something like:</p>

<pre><code>lm(index ~ spend_1 + spend_2 + trend + seasonal + random)
</code></pre>

<p>Thanks all for the advice please let me know if I can clarify anything further.</p>
"
"0.162806707774543","0.174077655955698","187870","<p>I'm working on a sales forecasting package which should be easy to use for the end user. Given a time series with historical sales data I would like to automatically select one of the three forecasts: Auto.Arima, ETS and STLF. 
The idea is to split historical data into 80% train set and 20% test (holdout) set. Then run Auto.Arima, ETS, STLF and choose the one that has best MAPE on the test set. </p>

<p>Now comes the part that is not entirely clear to me. Once I figured out that e.g. ETS gives me the best result should I now </p>

<ol>
<li>Retrain ETS on the entire set of historical data and generate
forecast using this new model? My reservation here is that after I
run ETS again it may even change the class of the algorithm as well
as the fit parameters which will render the MAPE I got on the test
set irrelevant.  </li>
<li>Just generate the forecast using the model that was trained on the
80% train set? My problem with this approach is that we are ignoring
the last 20% of data which is probably the most important
information for the forecast.</li>
<li>The third idea is to use the same model fit parameters that we got
after training the model on the 80% train set. But then use the
entire set of data for        forecasting. This seems like a
reasonable approach but I cannot figure out how to do it for ETS and
STL (For Arima we can do it by supplying the original fit as the model
parameter of the arima function)</li>
</ol>

<p>Could you please let me know what is the right way to approach this problem?</p>
"
"0.0490880693673816","0.0641500299099584","135651","<p>I've created an Arima model based on past forex closing prices using auto arima, which has generated a (0,1,0) ARIMA model.</p>

<pre><code>&gt; auto.arima(ma5)
Series: ma5 
ARIMA(0,1,0)                    

sigma^2 estimated as 5.506e-07:  log likelihood=11111.42
AIC=-22220.83   AICc=-22220.83   BIC=-22215.27
</code></pre>

<p>I next tried to plot the forecasted values, but as you can see all predictions are constant. Anyone know what I'm doing wrong?</p>

<p><img src=""http://i.stack.imgur.com/Er1k5.png"" alt=""enter image description here""></p>
"
"0.0981761387347632","0.0962250448649376","173610","<p>I create an ARIMA model for my <code>ts</code>-object. My data is available in seconds or even miliseconds. I didn't find a way to specify the time information for the start- and end-parameters when creating the <code>ts</code> object?</p>

<p>I need the exact time, because I want to extract the time information when I do the forecast based on the ARIMA model to return the exact times for my forecasted values. It would be enough to store the end-time information somewhere in my ARIMA model, so that I can use it later when I do the forecast.</p>

<p>How is this done usually with ARIMA models?</p>

<p>Thanks!</p>
"
"0.138842026900125","0.181443684650606","188597","<p>I have daily data for 3 years. This sales data is of seasonal nature as business has spikes and downfall by month. Also, sales differ by each day of the week. for example, monday in general in a month tend to have similar pattern.</p>

<p>I have used ARIMA and created a matrix of month dummy variables and day of week dummy variables and have passed that in ARIMA. however i hit the bottom when i couldn't reconvert differenced stationary number forecasts into the actual sales metric. <a href=""http://stats.stackexchange.com/questions/188595/convert-double-differenced-forecast-into-actual-value"">Posted here already</a></p>

<p>I have also tried dummy regression using sales as dependent variable and 11 month dummy variables and 6 day of week dummy variables. i abandoned this as R square was low at 48% and MAPE from the forecasted results was more than 20%</p>

<p>Edit: I have tried auto.arima as well.
My question: What technique can i use for forecasting sales for next 365 days? that will consider this month of the year and day of the week seasonality?</p>
"
"0.0981761387347632","0.128300059819917","43804","<p>I tried to fit <code>auto.arima()</code> with a <code>ts</code> data. But it is not giving the right forecast. For many it is coming as <code>arima(0,1,0)</code> model which is not good at all. Can I fit a GARCH model to the original series in this case? How do you get fitted and forecasted values of original data using <code>garch(1,1,)</code> or some other model? I tried to use code for GARCH but it is not giving the fitted and forecast of original values.</p>
"
"NaN","NaN","91706","<p>I am trying fit an ARIMA model to stock returns.</p>

<p>I have reached a decent model using the AIC criterion. </p>

<p>However, the ljung-box p value under a diagnostic plots are pretty weird. 
The null hypothesis get rejected at higher lags.
I tried modifying the parameters, but L-B p value betters only marginally, with a loss in AIC.</p>

<p>Any help how I can balance the two ?
Also any reasons why the p-value is so low for higher lags.</p>

<p>Ihave attached the diagnostic's image:</p>

<p><img src=""http://i.stack.imgur.com/AL5p5.png"" alt=""tsdiag""></p>
"
"0.0850230301897704","0.111111111111111","92935","<p>I'm making a project connected with identifying the dynamics of sales. My database concerns 26 weeks (so equally in 26 time-series observations) after launching the product.</p>

<p>This is what my database looks like: <a href=""https://imageshack.com/i/0yyh6ij"" rel=""nofollow"">https://imageshack.com/i/0yyh6ij</a> </p>

<p>I want to make forecast based on S-curve for clusters of time-series. The main aim was to compare two methods of forecasting:</p>

<ol>
<li>based on parameters of logistic curve</li>
<li>based on ARIMA</li>
</ol>

<p>However, I do not know how to compare these two methods = measure their performance.</p>

<p>That's a plot with prediction based on S-curve</p>

<p><a href=""http://imageshack.com/a/img850/6600/rzkp.jpg"" rel=""nofollow"">http://imageshack.com/a/img850/6600/rzkp.jpg</a></p>

<p>So my questions are:</p>

<ol>
<li>How to measure performance=forecast errors based on logistic curve?</li>
<li>How to compare forecasting based on logistic curve and ARIMA - what is the main difference between these two approaches if I base on one variable - units_sold_that_week?</li>
</ol>

<p>I would be grateful for any explanation.</p>
"
"0.162806707774543","0.193419617728553","189983","<p>I have daily data from last 2 years.</p>

<p>I want to do ARIMAX and the regressor component being autoregressive distributed lag of the same variable. Since it has impact, along with dummy variables to account for seasonality in the <code>xreg</code> paratemer in <code>auto.arima</code> function.</p>

<p>The challenge i am facing is predicting my predictor for future. For example, i used daily data for 2 year for model building. For forecasting into future, i also need values of lag variable, which i do not know. If i use 2 lags of daily data in the model, then in order to predict for future i will also need value of those lag variables as well. So to predict $Value$ at time $t$ i will need $Value$ at $t-1$ and $t-2$ which i have from past records. However, if i want to find value at $t+5$ then i will need to find $t+3$ and $t+4$. Not sure how to proceed in this direction. As stated earlier, i am using <code>auto.arima</code> function from <code>forecast</code> package in <code>R</code> . </p>

<p>My ultimate goal is to predict for next 365 days. What i assume to be a solution is that i predict for $t+1$ as it will require $t$ and $t-1$ as lag component which i already have. once done i can use this predicted $t+1$ component to predict for $t+2$ as i will know value of $t+1$ from previous iteration and $t$ from original values. Is it the right approach?</p>
"
"0.0850230301897704","0.111111111111111","190586","<p>I am using cross correlation to demonstrate a potential link between two time series (ext &amp; co). Both series are strongly autocorrelated, so it is difficult to assess the dependence between the two series. For a quick preliminary analysis, the cross correlation shows a clear (somehow delayed) link between the two time series, although it might spurious. <a href=""http://i.stack.imgur.com/eHUnj.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/eHUnj.jpg"" alt=""CCF""></a>. Prewhitening seems to be the best option; I will prewhiten my x variable by fitting an ARIMA process and then use the coefficients to filter my variable y. My question is if I should estimate the coefficients of the ARIMA process (for example using <code>auto.arima</code>) using my series x or by using the residuals of the OLS regression of x on y.</p>
"
"NaN","NaN","44992","<p>In the <code>arima</code> function in R, what does <code>order(1, 0, 12)</code> mean? What are the values that can be assigned to <code>p</code>, <code>d</code>, <code>q</code>, and what is the process to find those values?</p>
"
"0.10976425998969","0.0573775310549247","45921","<p>I knwo that the autocorrelation in MA(1) process varies between -.5 and +.5, if we consider d(t)=c+e(t)âˆ’Î¸â‹…e(tâˆ’1),then for positive values of Theta, autocorrelation is negative and for negative values of Theta autocorrelation is positive. Now I'm wondering if the autocorrelation of IMA(1,1) process also follows the same pattern as MA(1)? If the process is IMA(1,1) as follows d(t)=c+d(t-1)+e(t)âˆ’Î¸â‹…e(tâˆ’1),
whether for negative values autocorrelation is positive and for positive values autocorrelation is negatve? how can I calculate the range of autocorrelation for IMA(1,1) process?</p>
"
"0.0490880693673816","0.0641500299099584","191000","<p>I have a time serie, and I want a stationary process for search posible models.
One of the requirments is normality.</p>

<pre><code>shapiro.test(serie)
p-value = 0.0002322
</code></pre>

<p>How can I normalize my time serie?
(I try with Logaritimic transformation, regular differences, stationary differences, but doesn't work)</p>

<p>It's necesary normalize the serie?</p>

<p>When I look the p values for Ljung Box they are not normals. This is because the not notmality of the serie?</p>

<pre><code>shapiro.test(model1$residuals)
p-value &lt; 2.2e-16
</code></pre>

<p>EDIT:
model$residuals and p values 
<a href=""http://i.stack.imgur.com/xsqLK.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xsqLK.png"" alt=""enter image description here""></a></p>
"
"0.162806707774543","0.193419617728553","191120","<p>I am a beginner in time series analysis and I would like discuss a couple of numerical examples here implemented in R. I am reading some interesting books, but I also need some expert advice to get started.
The time series are</p>

<pre><code>ts1&lt;-structure(c(196, 196, 178, 165, 155, 138, 131, 132, 135, 146, 
160, 173, 180, 186, 180, 163, 132, 129, 134, 146, 159, 157, 161, 
179, 209, 225, 228, 196, 151, 144, 145, 157, 168, 161, 162, 176, 
205, 219, 219, 190, 147, 142, 146, 160, 175, 169, 171, 188, 220, 
235, 236, 202, 154, 146, 145, 155, 168, 158, 156, 168, 190, 202, 
204, 177, 135, 127, 125, 133, 145, 139, 143, 160, 190, 205, 200, 
160, 119, 113, 118, 129, 142, 135, 133, 142, 159, 171, 177, 164, 
135, 130, 130, 139, 152, 149, 152, 168, 195, 209, 211, 180, 138, 
134, 139, 152, 165, 158, 157, 168, 192, 207, 219, 206, 169, 164, 
161, 172, 182, 180, 182, 196, 218, 223, 229, 230, 196, 197, 200, 
209, 222, 219, 207, 210, 209, 221, 234, 224, 225, 221, 235, 216, 
224, 229, 229, 214, 230, 240, 243, 222, 189, 221, 217, 189, 197, 
194, 195, 202, 197, 224, 204, 218, 212, 191, 217, 215, 183, 186, 
191, 166, 177, 194, 180, 159, 158, 147, 166, 184, 159, 159, 187, 
194, 196, 204, 213, 236, 210, 218, 251, 227, 251, 214, 245, 209, 
215, 242, 196, 237, 212, 171, 206, 200, 204, 192, 185, 182, 194, 
242, 199, 200, 191, 172, 179, 165, 173, 198, 214, 197, 175, 227, 
197, 202, 205, 212, 216, 223, 222, 201, 217, 209, 239, 241, 251, 
225, 212, 210, 241, 223, 238, 226, 242, 228, 257, 248, 264, 229, 
223, 255, 251, 231, 254, 235, 246, 246, 243, 254, 256, 261, 254, 
247, 249, 243, 257, 228, 272), na.action = structure(c(1L, 2L, 
3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 
17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 29L, 
30L, 31L, 32L, 33L, 34L, 35L, 36L, 37L, 38L, 39L, 40L, 41L, 42L, 
43L, 44L, 45L, 46L, 47L, 48L, 49L, 50L, 51L, 52L, 53L, 54L, 55L, 
56L, 57L, 58L, 59L, 60L, 61L, 62L, 63L, 64L, 65L, 66L, 67L, 68L, 
69L, 70L, 71L, 72L, 73L, 74L, 75L, 76L, 77L, 78L, 79L, 80L, 81L, 
82L, 83L, 84L, 85L, 86L, 87L, 88L, 89L, 90L, 91L, 92L, 93L, 94L, 
95L, 96L, 97L, 98L, 99L, 100L, 101L, 102L, 103L, 104L, 105L, 
106L, 107L, 108L, 109L, 110L, 111L, 112L, 113L, 114L, 115L, 116L, 
117L, 118L, 119L, 120L, 121L, 122L, 123L, 124L, 125L, 126L, 127L, 
128L, 129L, 130L, 131L, 132L, 396L), class = ""omit""), .Tsp = c(1994, 
2015.83333333333, 12), class = ""ts"")

ts2&lt;-structure(c(3756, 3867, 3686, 3490, 3446, 3357, 3421, 3447,3321, 
3198, 3331, 3360, 3312, 3270, 3251, 3213, 2937, 3152, 3022, 2931, 
2697, 2626, 2775, 3030, 3067, 3349, 3225, 3175, 3061, 3089, 3166, 
3193, 3035, 2901, 2932, 2981, 3242, 3268, 3084, 2902, 2790, 2695, 
2756, 2649, 2627, 2643, 2554, 2638, 2783, 2660, 2618, 2383, 2319, 
2415, 2434, 2427, 2164, 2114, 2246, 2224, 2552, 2390, 2213, 2130, 
2274, 2140, 2317, 2191, 2086, 2112, 2134, 2153, 2401, 2450, 2273, 
2154, 2140, 2201, 2156, 2078, 2110, 2101, 2075, 2043, 2305, 2266, 
2227, 2134, 2002, 2008, 1945, 2110, 2045, 2017, 2106, 1913, 2068, 
2209, 2025, 2033, 1892, 1934, 1914, 1818, 1808, 
1851, 1939),na.action   = structure(c(1L, 
2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 
16L, 17L, 18L, 19L, 20L, 21L, 22L, 23L, 24L, 25L, 26L, 27L, 28L, 
29L, 30L, 31L, 32L, 33L, 34L, 35L, 36L, 37L, 38L, 39L, 40L, 41L, 
42L, 43L, 44L, 45L, 46L, 47L, 48L, 49L, 50L, 51L, 52L, 53L, 54L, 
55L, 56L, 57L, 58L, 59L, 60L, 61L, 62L, 63L, 64L, 65L, 66L, 67L, 
68L, 69L, 70L, 71L, 72L, 73L, 74L, 75L, 76L, 77L, 78L, 79L, 80L, 
81L, 82L, 83L, 84L, 85L, 86L, 87L, 88L, 89L, 90L, 91L, 92L, 93L, 
94L, 95L, 96L, 97L, 98L, 99L, 100L, 101L, 102L, 103L, 104L, 105L, 
106L, 107L, 108L, 109L, 110L, 111L, 112L, 113L, 114L, 115L, 116L, 
117L, 118L, 119L, 120L, 121L, 122L, 123L, 124L, 125L, 126L, 127L, 
128L, 129L, 130L, 131L, 132L, 133L, 134L, 135L, 136L, 137L, 138L, 
139L, 140L, 141L, 142L, 143L, 144L, 145L, 146L, 147L, 148L, 149L, 
150L, 151L, 152L, 153L, 154L, 155L, 156L, 157L, 158L, 159L, 160L, 
161L, 162L, 163L, 164L, 165L, 166L, 167L, 168L, 169L, 170L, 171L, 
172L, 173L, 174L, 175L, 176L, 177L, 178L, 179L, 180L, 181L, 182L, 
183L, 184L, 185L, 186L, 187L, 188L, 189L, 190L, 191L, 192L, 193L, 
194L, 195L, 196L, 197L, 198L, 199L, 200L, 201L, 202L, 203L, 204L, 
205L, 206L, 207L, 208L, 209L, 210L, 211L, 212L, 213L, 214L, 215L, 
216L, 217L, 218L, 219L, 220L, 221L, 222L, 223L, 224L, 225L, 226L, 
227L, 228L, 229L, 230L, 231L, 232L, 233L, 234L, 235L, 236L, 237L, 
238L, 239L, 240L, 241L, 242L, 243L, 244L, 245L, 246L, 247L, 248L, 
249L, 250L, 251L, 252L, 253L, 254L, 255L, 256L, 257L, 258L, 259L, 
260L, 261L, 262L, 263L, 264L, 265L, 266L, 267L, 268L, 269L, 270L, 
271L, 272L, 273L, 274L, 275L, 276L, 277L, 278L, 279L, 280L, 281L, 
282L, 283L, 284L, 285L, 286L, 287L, 288L, 396L),
class = ""omit""),.Tsp   = c(2007, 
2015.83333333333, 12), class = ""ts"")
</code></pre>

<p>I would prefer to avoid the use of auto.arima from the (excellent) forecast package, or at least not to use it as a black box.
I started looking at the plots of the first differences</p>

<pre><code>plot(diff(ts1))
</code></pre>

<p><a href=""http://i.stack.imgur.com/5EVBz.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5EVBz.png"" alt=""enter image description here""></a></p>

<pre><code>plot(diff(ts2))
</code></pre>

<p><a href=""http://i.stack.imgur.com/ZWNea.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZWNea.png"" alt=""enter image description here""></a></p>

<p>which should remove any trend. I also looked at the decomposition: </p>

<pre><code>plot(decompose(ts1))
</code></pre>

<p><a href=""http://i.stack.imgur.com/VpDyN.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/VpDyN.png"" alt=""enter image description here""></a></p>

<pre><code>plot(decompose(ts2))
</code></pre>

<p><a href=""http://i.stack.imgur.com/M3lkU.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/M3lkU.png"" alt=""enter image description here""></a></p>

<p>I would tend to conclude that in both cases there is a seasonality in the data. 
However, diff(ts2) appears (to me, by eye) to yield a stationary process with constant variance, whereas diff(ts1) does not seem to have a constant variance. I tried diff(diff(ts1)) and diff(log(ts2)), but I am puzzled by what I see.
If I look at  </p>

<pre><code> acf(ts1)
</code></pre>

<p><a href=""http://i.stack.imgur.com/qPbtI.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qPbtI.png"" alt=""enter image description here""></a></p>

<pre><code> acf(ts2)
</code></pre>

<p><a href=""http://i.stack.imgur.com/TlNto.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/TlNto.png"" alt=""enter image description here""></a></p>

<p>I see that in both cases the autocorrelation decays slowly and when I resort to</p>

<pre><code>acf(diff(ts1))
</code></pre>

<p><a href=""http://i.stack.imgur.com/Q62OT.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Q62OT.png"" alt=""enter image description here""></a></p>

<pre><code>acf(diff(ts2))
</code></pre>

<p><a href=""http://i.stack.imgur.com/nxycs.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/nxycs.png"" alt=""enter image description here""></a></p>

<p>I see some spikes which I do not know how to interpret.
Essentially, I am at a loss about how to link these findings with a SARIMA model.
Any suggestion on either/both time series is very appreciated!</p>
"
"0.0490880693673816","0.0641500299099584","112927","<p>Let's say I have the two following ARIMA models: </p>

<ol>
<li>ARIMA(7,1,1) (no seasonality) </li>
<li>ARIMA(6,1,1)(1,0,0)<sub>7</sub> (seasonality of period 7).</li>
</ol>

<p>Are they conceptually the same? If so, why is that when I model them using R, I get different models and different fits?</p>
"
"0.0850230301897704","0.0740740740740741","214379","<p>I'm working on an Arima model to forecast a given variable and so I'm looking in my data for variables with correlation to the variable I'm trying to predict, to add as predictors in the xreg argument.  I've found several that have correlation between 0.1 and 0.3.  I was wondering is there a way to combine predictors with lower correlation to a variable to create a predictor with higher correlation to a variable?</p>
"
"0.155875555347912","0.185185185185185","208985","<p>I'm wondering if a rolling forecast technique like the ones mentioned in Rob Hyndman's blogs, and the example below, could be used to select the order for an ARIMA model?</p>

<p>In the examples I've looked at, like the ones below, it seems like the order of the ARIMA model is already specified, or is determined once by auto.arima and then the single model is evaluated using the forloop in the rolling forecast. </p>

<p>I'm wondering how you could use the rolling forecast technique to select the order of the ARIMA model.  If anyone has a suggestion or example, that would be great.</p>

<p>Examples:
<a href=""http://robjhyndman.com/hyndsight/tscvexample/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/tscvexample/</a>
<a href=""http://robjhyndman.com/hyndsight/rolling-forecasts/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/rolling-forecasts/</a></p>

<p>Code:</p>

<pre><code>library(""fpp"")

h &lt;- 5
train &lt;- window(hsales,end=1989.99)
test &lt;- window(hsales,start=1990)
n &lt;- length(test) - h + 1
fit &lt;- auto.arima(train)
fc &lt;- ts(numeric(n), start=1990+(h-1)/12, freq=12)
for(i in 1:n)
{  
  x &lt;- window(hsales, end=1989.99 + (i-1)/12)
  refit &lt;- Arima(x, model=fit)
  fc[i] &lt;- forecast(refit, h=h)$mean[h]
}
</code></pre>

<p><strong>Update:</strong></p>

<p>Pseudo code:</p>

<pre><code>library(""fpp"")

h &lt;- 5
train &lt;- window(hsales,end=1989.99)
test &lt;- window(hsales,start=1990)
n &lt;- length(test) - h + 1

##Create models for all combinations of p 10 to 0, d 2 to 0, q 10 to 0

fit1 &lt;- Arima(train, order=c(10,2,10)
fit2 &lt;- Arima(train, order=c(9,2,10)
fit3 &lt;- Arima(train, order=c(8,2,10)
.
.
.
fit10 &lt;- Arima(train, order=c(0,2,10)
fc1 &lt;- ts(numeric(n), start=1990+(h-1)/12, freq=12)
fc2 &lt;- ts(numeric(n), start=1990+(h-1)/12, freq=12)
fc3 &lt;- ts(numeric(n), start=1990+(h-1)/12, freq=12)
.
.
.
fc10 &lt;- ts(numeric(n), start=1990+(h-1)/12, freq=12)
for(i in 1:n)
{  
  x &lt;- window(hsales, end=1989.99 + (i-1)/12)
  refit1 &lt;- Arima(x, model=fit1)
  refit2 &lt;- Arima(x, model=fit2)
  refit3 &lt;- Arima(x, model=fit3)
  .
  .
  .
  refit10 &lt;- Arima(x, model=fit10)
  fc1[i] &lt;- forecast(refit1, h=h)$mean[h]
	  fc2[i] &lt;- forecast(refit2, h=h)$mean[h]
  fc3[i] &lt;- forecast(refit3, h=h)$mean[h]
	  .
	  .
	  .
	  fc10[i] &lt;- forecast(refit10, h=h)$mean[h]
}

##Calculating mape for forecasts

Accuracy(fc1$mean,test)[,5]
	Accuracy(fc2$mean,test)[,5]
Accuracy(fc3$mean,test)[,5]
	.
	.
	.
	Accuracy(fc10$mean,test)[,5]

##Return the order of the Arima model that has the lowest mape 
</code></pre>
"
"0.208263040350187","0.257045219921692","154641","<p>This question is similar to the following <a href=""http://stats.stackexchange.com/questions/32634/difference-time-series-before-arima-or-within-arima"">question</a> in the sense I am currently doing the differencing and mean removal of the time series outside the <code>Arima</code> function in R. And I do not know how to do these steps within <code>Arima</code> function in R. The reason is that I am trying to perform the following procedure (data <code>dowj_ts</code> can be found at the bottom): </p>

<pre><code>dowj_ts_d1 &lt;- diff(dowj_ts) # differencing at lag 1 (1-B)
drift &lt;- mean(diff(dowj_ts))
dowj_ts_d1_demeaned &lt;- dowj_ts_d1 - mean(dowj_ts_d1) # mean removal
# Maximum Likelihood AR(1) for the mean-corrected differences X_t
fit &lt;- Arima(dowj_ts_d1_demeaned, order=c(1,0,0),include.mean=F, transform.pars = T)
</code></pre>

<p>Note that the <code>drift</code> is actually <code>0.1336364</code>. And <code>summary(fit)</code> gives the table below:</p>

<pre><code>Series: dowj_ts_d1_demeaned 
ARIMA(1,0,0) with zero mean     

Coefficients:
         ar1
      0.4471
s.e.  0.1051

sigma^2 estimated as 0.1455:  log likelihood=-35.16
AIC=74.32   AICc=74.48   BIC=79.01

Training set error measures:
                       ME     RMSE       MAE       MPE     MAPE      MASE
Training set -0.004721362 0.381457 0.2982851 -9.337089 209.6878 0.8477813
                    ACF1
Training set -0.04852626
</code></pre>

<p>Ultimately, I want to predict 2-step ahead forecast of <strong>the original series</strong>, and this starts to become ugly: </p>

<pre><code> tail(c(dowj_ts[1], dowj_ts[1] + cumsum(c(dowj_ts_d1_demeaned,forecast.Arima(fit,h=2)$mean) + drift)),2)
</code></pre>

<p>And currently these are all done outside the <code>Arima</code> function from the <code>forecast</code> package. I know I can do differencing within Arima like this: </p>

<pre><code> Arima(dowj_ts, order=c(1,1,0),include.drift=T,transform.pars = F)
</code></pre>

<p>This gives:</p>

<pre><code>Series: dowj_ts 
ARIMA(1,1,0) with drift         

Coefficients:
         ar1   drift
      0.4478  0.1204
s.e.  0.1059  0.0786

sigma^2 estimated as 0.1474:  log likelihood=-34.69
AIC=75.38   AICc=75.71   BIC=82.41
</code></pre>

<p>But the drift term computed by R is different from the <code>drift = 0.1336364</code> that I computed manually.</p>

<p>So <strong>my question is: how can I differenced the series and then remove the mean of the differenced series within the Arima function ?</strong></p>

<p><strong>Second question:</strong> Why is the drift term estimated by <code>Arima</code> different from the drift term I computed ? In fact, what does the <strong>mathematical model</strong> look like when <code>include.drift = T</code> ? This really confuses me. </p>

<p>Data can be found below: </p>

<pre><code>structure(c(110.94, 110.69, 110.43, 110.56, 110.75, 110.84, 110.46, 
110.56, 110.46, 110.05, 109.6, 109.31, 109.31, 109.25, 109.02, 
108.54, 108.77, 109.02, 109.44, 109.38, 109.53, 109.89, 110.56, 
110.56, 110.72, 111.23, 111.48, 111.58, 111.9, 112.19, 112.06, 
111.96, 111.68, 111.36, 111.42, 112, 112.22, 112.7, 113.15, 114.36, 
114.65, 115.06, 115.86, 116.4, 116.44, 116.88, 118.07, 118.51, 
119.28, 119.79, 119.7, 119.28, 119.66, 120.14, 120.97, 121.13, 
121.55, 121.96, 122.26, 123.79, 124.11, 124.14, 123.37, 123.02, 
122.86, 123.02, 123.11, 123.05, 123.05, 122.83, 123.18, 122.67, 
122.73, 122.86, 122.67, 122.09, 122, 121.23), .Tsp = c(1, 78, 
1), class = ""ts"")
</code></pre>
"
"0.155230105141267","0.182574185835055","72244","<p>I must be doing something very wrong here, as auto.arima in R is completely dying, but I can't see what it is.  I have the latest version of forecast and R and I think this happens on both Windows and Unix.  It works for some/most time series I tried (equities) but fails for others.  It seems to fail more often when I diff the high/low with the previous day's close as opposed to just diffing the closes as below.  Is this a bug or am I somehow giving arima bad data? (and causing it to die with a horrible error message)  I tried searching for this error but didn't come up with much.  Thanks a lot.</p>

<pre><code>library(tseries)
library(forecast)
dwa &lt;- get.hist.quote(instrument=""DWA"", start=""2010-01-01"", end=""2013-10-31"")
logreturns &lt;- diff(log(dwa$Close))*100 
fit &lt;- auto.arima(logreturns, trace=TRUE)
</code></pre>

<p>Output:</p>

<pre><code>This is forecast 4.8 

trying URL 'http://chart.yahoo.com/table.csv?s=DWA&amp;a=0&amp;b=01&amp;c=2010&amp;d=9&amp;e=31&amp;f=2013&amp;g=d&amp;q=q&amp;y=0&amp;z=DWA&amp;x=.csv'
Content type 'text/csv' length unknown
opened URL
.......... .......... .......... .......... ....
downloaded 44 Kb

time series starts 2010-01-04
time series ends   2013-10-07

 ARIMA(2,1,2) with drift         : 1e+20 *
 ARIMA(0,1,0) with drift         : 1e+20 *
 ARIMA(1,1,0) with drift         : 1e+20 *
 ARIMA(0,1,1) with drift         : 1e+20 *
 ARIMA(1,1,2) with drift         : 1e+20 *
 ARIMA(3,1,2) with drift         : 1e+20 *
 ARIMA(2,1,1) with drift         : 1e+20 *
 ARIMA(2,1,3) with drift         : 1e+20 *
 ARIMA(1,1,1) with drift         : 1e+20 *
 ARIMA(3,1,3) with drift         : 1e+20 *
 ARIMA(2,1,2)          : 1e+20 *Error in if (diffs == 1 &amp; constant) { : argument is of length zero
Calls: auto.arima -&gt; myarima
In addition: Warning messages:
1: In if (is.constant(x)) { :
  the condition has length &gt; 1 and only the first element will be used
2: In if (is.constant(x)) return(d) :
  the condition has length &gt; 1 and only the first element will be used
3: In if (is.constant(dx)) { :
  the condition has length &gt; 1 and only the first element will be used
Execution halted
</code></pre>
"
"0.0694210134500623","0.0907218423253029","214602","<p>I use R for time series analysis. I would like to evaluate decomposition algorithms. <code>decompose</code> and <code>stl</code> from ""stats"" package lead to good results but often, the residuals are not meaningless.</p>

<p>Example:</p>

<pre><code>dec &lt;- decompose(AirPassengers)
Box.test(dec$random[7:138], lag = 24, type = ""Ljung"")
&gt; p-value &lt; 2.2e-16
</code></pre>

<p>There is still a lot of autocorrelation in the residuals, the same for <code>decompose</code> with <code>type = ""multiplicative""</code> and for <code>stl</code>. If possible, I would like to extract all meaningful information from the residuals. Thus, I had a look on classical forecasting techniques:</p>

<pre><code>library(forecast)
dec &lt;- auto.arima(AirPassengers)
Box.test(dec$residuals, lag = 24, type = ""Ljung"")
&gt; p-value = 0.01356
</code></pre>

<p>Fitting a SARIMA model leads to less autocorrelation and thus, better ""information extraction"". For p > 0.05, one could argue for a Gaussian error distribution. </p>

<p>Is there a way to decompose the ARIMA fit into slowly varying components and oscillating components like with classical decomposition techniques?</p>
"
"0.155230105141267","0.162288165186716","115506","<p>Forecasting airline passengers seasonal time series using auto arima</p>

<p>Hi, I am trying to model some airline data in an attempt to provide an accurate monthly forecast for June-December this year using monthly data from January 2003 onwards.  The data is taken from: <a href=""http://www.transtats.bts.gov/Data_Elements.aspx?Data=1"" rel=""nofollow"">http://www.transtats.bts.gov/Data_Elements.aspx?Data=1</a></p>

<p>Here is the time series plot and ACF</p>

<p><a href=""http://imgur.com/EGh40pR"" rel=""nofollow""><img src=""http://i.imgur.com/EGh40pR.jpg"" title=""Hosted by imgur.com""/></a> </p>

<p><a href=""http://imgur.com/BJy78dn"" rel=""nofollow""><img src=""http://i.imgur.com/BJy78dn.jpg"" title=""Hosted by imgur.com""/></a></p>

<p>I have used auto.arima to develop two models and checked that they correspond to the autocorrelation functions.  Basically I am having trouble deciding whether to use:</p>

<ol>
<li>The following seasonal ARIMA model</li>
</ol>

<p><a href=""http://imgur.com/0k2Q8I4"" rel=""nofollow""><img src=""http://i.imgur.com/0k2Q8I4.jpg"" title=""Hosted by imgur.com""/></a></p>

<ol start=""2"">
<li><p>The following non-seasonal ARIMA model of $N_t$ after I first decomposed the model into a trend, seasonal component and random component $X_t = T_t +S_t +N_t $ using a 12-point moving average (basically did the same thing as the <code>decompose()</code> function manually)</p>

<p><a href=""http://imgur.com/r4TkpxX"" rel=""nofollow""><img src=""http://i.imgur.com/r4TkpxX.jpg"" title=""Hosted by imgur.com""/></a></p></li>
</ol>

<p>I have analysed the important properties of both models such as ensuring residuals are close to a white noise process and so on but am unsure which of the above 2 models is most suitable for forecasting purposes and why?</p>

<p>Also I am unsure how to compute forecast for the trend component vector if I use the classical decomposition model $X_t = T_t + S_t +N_t$.  Is it even possible to create forecasts using this type of model?</p>

<p>Edit:
Here is the output of <code>dput(IAP)</code> (the raw data without trend or seasonal component removed)</p>

<blockquote>
  <p>dput(IAP)
  structure(c(9726436L, 8283372L, 9538653L, 8309305L, 8801873L, 
  10347900L, 11705206L, 11799672L, 9454647L, 9608358L, 9481886L, 
  10512547L, 10252443L, 9310317L, 10976440L, 10802022L, 10971254L, 
  12159514L, 13502913L, 13203566L, 10570682L, 10772177L, 10174320L, 
  11244427L, 11387275L, 9945067L, 12479643L, 11521174L, 12164600L, 
  13140061L, 14421209L, 13703334L, 11325800L, 11107586L, 10580099L, 
  11812574L, 11724098L, 10167275L, 12707241L, 12619137L, 12610793L, 
  13690835L, 14912621L, 14171796L, 12010922L, 11517228L, 11222687L, 
  12385958L, 12072442L, 10590281L, 13246293L, 12795517L, 12978086L, 
  14170877L, 15470687L, 15120200L, 12321953L, 12381689L, 12004268L, 
  13098697L, 12767516L, 11648482L, 14194753L, 12961165L, 13602014L, 
  14413771L, 15449821L, 15327739L, 11731364L, 11921490L, 11256163L, 
  12463351L, 12075267L, 10412676L, 12508793L, 12629805L, 11806548L, 
  13199636L, 14953615L, 14844821L, 11659775L, 11905529L, 11093714L, 
  12659154L, 12393439L, 10694165L, 13279320L, 12398700L, 13380664L, 
  14406776L, 16026852L, 15317926L, 12599149L, 12874707L, 11651314L, 
  12915663L, 12668763L, 10944610L, 13473705L, 13537152L, 13935132L, 
  14814672L, 16623674L, 15753387L, 13220884L, 13185627L, 12144742L, 
  13546071L, 13206682L, 11732944L, 14387677L, 13995377L, 14291285L, 
  15582335L, 16969590L, 16621336L, 13791714L, 13397785L, 12762536L, 
  14096567L, 13766673L, 12023339L, 15177069L, 14278932L, 15306328L, 
  16232176L, 17645538L, 17517022L, 14239561L, 14209627L, 13133257L, 
  15083929L, 14589637L, 12385546L, 15486317L, 14857685L, 15615732L
  ), .Tsp = c(2003, 2014.33333333333, 12), class = ""ts"")</p>
</blockquote>

<p>Here is the output of <code>dput(IAP.res)</code> (the random component from the decomposition)</p>

<blockquote>
  <p>dput(IAP.res)
  structure(c(NA, NA, NA, NA, NA, NA, -669127.347569446, -168943.285069446, 
  225871.456597222, 271337.106597223, 711896.11076389, 284583.435763889, 
  165401.360763887, 622993.194097221, -268299.21423611, -9406.73506944434, 
  -233904.910069446, -147124.755902779, -260973.055902776, -163628.243402778, 
  -43056.7100694457, 121365.814930555, 205106.485763889, -107464.272569445, 
  247575.569097221, 279399.444097225, 309270.160763888, -166333.068402778, 
  129823.798263889, 22571.1190972265, -113455.59756944, -384199.160069444, 
  62061.8315972222, -155858.226736111, 13600.0274305546, -87564.1475694429, 
  71845.7357638887, 8145.86076388881, 47627.494097226, 442212.72326389, 
  73639.5065972234, 60882.5774305568, -135204.389236112, -437744.576736112, 
  203832.581597222, -264145.435069444, 179945.61076389, 15812.1024305553, 
  -49648.0975694434, -61460.8059027772, 89656.3690972241, 118205.931597224, 
  -84196.4517361106, 4197.78576389072, -134118.722569442, -87234.4517361117, 
  -126555.418402776, -57714.9350694417, 293250.152430556, 59462.6857638892, 
  10340.8190972245, 416646.652430557, 526459.702430556, -135041.068402776, 
  239767.631597222, 67034.9940972247, -221066.180902774, 207611.839930556, 
  -424486.00173611, -94779.3517361115, 89796.4857638886, 130285.644097223, 
  104776.152430555, 16099.8607638888, -317097.047569448, 335867.264930556, 
  -796342.285069446, -446777.464236111, -93681.7225694442, 242962.798263888, 
  -143380.293402778, 135423.439930556, 28934.7357638923, 186390.185763891, 
  116969.777430558, -113617.264236109, -39733.9225694438, -471572.526736109, 
  130389.423263891, 80446.7857638926, 298895.444097222, 38486.7982638846, 
  143712.123263886, 419260.898263889, -113385.347569445, -181233.730902779, 
  -178686.680902779, -412733.597569445, -380106.797569444, 172783.973263888, 
  220863.173263891, 11443.2440972247, 392297.319097224, -62825.8267361117, 
  176278.664930556, 139372.439930556, -174159.88923611, -111755.439236109, 
  -206233.264236111, -197431.097569445, -55065.5892361099, 48314.3065972236, 
  -6745.32673610683, 193492.494097225, 155009.569097224, 241747.214930556, 
  209670.99826389, -173438.47673611, -101510.63923611, -128948.689236113, 
  -222773.597569443, -498474.472569441, 146856.619097224, -275463.026736109, 
  386273.214930557, 213400.994097223, 171865.11076389, 464391.381597217, 
  1489.99826388643, -9918.39340277936, -362009.847569447, NA, NA, 
  NA, NA, NA, NA), .Tsp = c(2003, 2014.33333333333, 12), class = ""ts"")</p>
</blockquote>
"
"0.0694210134500623","0.0907218423253029","47729","<p>I'm currently using R to predict a time series with these instructions:</p>

<pre><code>X &lt;- ts(datas, frequency=24)
X.arima &lt;- Arima(X, order=c(2,1,0), seasonal=c(1,1,1))
pred &lt;- predict(X.arima, n.ahead=24)
plot.ts(pred$pred)
</code></pre>

<p>As you can see I've data each hour, and I chose the seasonal period of 24 (one day).</p>

<p>I would like to improve my forecasting using an additional seasonal period in order to include the seasonal component of the week (seasonal length of 7*24=168 data)</p>

<p>Is there any method for this? How do you do it?</p>

<p><strong>UPDATE:</strong>
I've read <a href=""http://robjhyndman.com/researchtips/longseasonality/"">this</a> (your) blog page, maybe can I use the external regressors to simulate a second seasonal period?</p>
"
"0.10976425998969","0.0860662965823871","47416","<p>We know that dealing with model involving MA factors is not easy to estimate, since there are past values of errors to be computed recursively. And this recursive estimation requires preliminary (initial) estimates of the parameters. For example, an ARMA(1,2)
$$z_t=\phi z_{t-1}-\theta_1 \varepsilon_{t-1}-\theta_2 \varepsilon_{t-2}+\varepsilon_t$$
To estimate the parameters, we need to compute first the values of $\varepsilon_{t-1}$ and $\varepsilon_{t-2}$, since these are not available yet. And they are computed using
$$\varepsilon_t=z_t-\phi z_{t-1}+\theta_1 \varepsilon_{t-1}+\theta_2 \varepsilon_{t-2}$$
Procedures for obtaining preliminary estimates of the parameters is available in Box and Jenkins, Time Series: Forecasting and Control. And this estimation is already available in much statistical software. My question is, ""Is there a function for obtaining a preliminary estimate of the parameters in R?""</p>

<p>I need this to obtain a preliminary estimate for my Space-Time ARIMA. Another question is, ""How does <code>arima</code> function of R compute preliminary estimates of the parameters when there are MA factors involved?""</p>

<p>Thanks in advance!</p>
"
"0.10976425998969","0.114755062109849","72294","<p>I want to remove seasonality from daily electricity demand (a time series). My understanding is there is weekly (high demand on Tue, Wed, and low demand on Sat, Sun) and annual seasonality (high demand on Winter and lower on Summer). I tried to build a model to forecast daily electricity demand in R, and plot my data as shown below:
<img src=""http://i.stack.imgur.com/pz5OS.png"" alt=""Daily electricity demand""></p>

<p>I tried to remove seasonality with the following:</p>

<pre><code>demand.xts.diff&lt;-diff(demand.xts,lag=1,difference=1)
demand.xts.diff&lt;-diff(demand.xts,lag=7,difference=1)
</code></pre>

<p>I also tried to use <code>lag=365</code> and <code>lag=366</code> (I am not sure what lag to use, due to the leap year issue), but none of them successfully removed seasonality. The ACF and PACF are shown below:</p>

<p><img src=""http://i.stack.imgur.com/OV7T4.png"" alt=""ACF"">
<img src=""http://i.stack.imgur.com/HcXKi.png"" alt=""PACF""></p>

<p>Any advice is appreciated.</p>
"
"0.230243454180072","0.218829317296592","116145","<p>I have downloaded the daily stock Adjusted Close price of one stock from sep 2011 to till date. As per my study plan, I have plotted some basic plots to understand the daily stock Adjusted closing price.</p>

<p>Here is the xyplot of the stock closing price by date and the code used to plot(My x axis not visible).</p>

<pre><code>Stock_T=stocks[which(symbol=='Stock_T'),]
xyplot(Adj.Close~Date,type='l',data=Stock_T,main='Adj.Close Price of the Stock_T')
</code></pre>

<p><img src=""http://i.stack.imgur.com/Ivlmk.png"" alt=""Timeseries plot of the raw data- Adjusted Closing price of the Stock""></p>

<p>By seeing this plot, the closing price was stable for period but had sudden huge increase in the stock price, it might had some other indicator which caused this much change in the stock price. Now my objective is to learn some ARIMA modeling concepts using this stock prices and try to do some forecasting of the stock price for few weeks. </p>

<p>As I have basic knowledge in ARIMA modeling, and I learned in the books that we should have stationary series before applying the ARIMA Model.</p>

<p>So, now I have plotted the ACF and PACF of the above raw data timeseries.</p>

<pre><code>acf(Stock_T$Adj.Close)
</code></pre>

<p><img src=""http://i.stack.imgur.com/rpy8S.png"" alt=""Raw data ACF Plot""></p>

<pre><code>pacf(Stock_T$Adj.Close)
</code></pre>

<p><img src=""http://i.stack.imgur.com/QKQge.png"" alt=""Raw data PACF plot""></p>

<p>From the above ACF and PACF plot, the series is not stationary and have huge autocorrelation (please correct me if am wrong), by differencing the series we will have stationary series (please correct me if am wrong). Here is the below plot.</p>

<pre><code>Stock_T_d1=diff(Stock_T$Adj.Close)
</code></pre>

<p><img src=""http://i.stack.imgur.com/sD9Tj.png"" alt=""First difference of the raw series""></p>

<p>Here the differencing series and its ACF AND PACF plots. ACF plot shows that there is no auto correlation and the series is stationary (please correct me if I am wrong) but I am unable to interpret the PACF plots, can someone explain it to me?  </p>

<p><img src=""http://i.stack.imgur.com/cAoVf.png"" alt=""ACF plot of Difference series""></p>

<p><img src=""http://i.stack.imgur.com/U10g8.png"" alt=""PACF plot of Difference series""></p>

<p>The above difference series shows some unequal variance in the series and so I am taking log transformation before differencing and its ACF and PACF.</p>

<pre><code>Stock_T_logd1=diff(log(Stock_T$Adj.Close))
</code></pre>

<p><img src=""http://i.stack.imgur.com/MWovq.png"" alt=""Difference Logged series ""></p>

<p><img src=""http://i.stack.imgur.com/oYd2d.png"" alt=""ACF of Difference logged Series""></p>

<p><img src=""http://i.stack.imgur.com/HQxZw.png"" alt=""PACF of Difference logged Series""></p>

<p>Now I will try to ask my questions.</p>

<ol>
<li>Should we have stationary series before we apply ARIMA?</li>
<li>Could you please explain me the ACF and PACF of the original series, and what we should do if we have this kind of series?</li>
<li>Could you please explain me the ACF and PACF of the difference series, and what will be the next step?</li>
<li>Could you please explain me the ACF and PACF of the difference logged series, and what will be the next step?</li>
<li>Should we use difference series or difference logged series?</li>
<li>What will be the ARIMA orders of this series?</li>
<li>Is there any R code to find the ARIMA order automatically of the original series?</li>
</ol>
"
"0.0981761387347632","0.128300059819917","174687","<p>I have around 10000 time series and I want to train ARIMA model using 8000 of them.</p>

<p>I wanted to use auto.arima function <a href=""http://www.inside-r.org/packages/cran/forecast/docs/auto.arima"" rel=""nofollow"">http://www.inside-r.org/packages/cran/forecast/docs/auto.arima</a>
however I am unable to find best ARIMA model for many time series. </p>

<p>Here is the code, I can always use x as my time series.  but how to train it using more time series and find best model?</p>

<pre><code>y=auto.arima(x)
plot(forecast(y,h=30))
</code></pre>

<p>Sample 
time series 1</p>

<pre><code>0.0003748,0.0003929,0.0003653,0.0003557,0.0004463,0.000349,0.0003099,0.0003395,0.0003157,0.0002871,0.0002604,0.0002422,0.0001917,0.0002117,0.0002689
</code></pre>

<p>time series 2</p>

<pre><code>0.0003977,0.0003481,0.0002413,0.0002069,0.0002127,0.0002108,0.0002003,0.0002174,0.0002098,0.0002069,0.0001955,0.0001926,0.0002108,0.0002146,0.0002079
</code></pre>
"
"0.147264208102145","0.192450089729875","174692","<p>I have around 10000 time series showing one particular metric over 5 hours. </p>

<p>I used <a href=""http://www.inside-r.org/packages/cran/forecast/docs/auto.arima"" rel=""nofollow"">auto.arima function</a> </p>

<p>In my previous question, people suggested that I have to use auto.arima for each time series, hold off some of data points and test the prediction with my hold off points.</p>

<p>I am holding off 20% of data points (if you see sample out of 40 I will hold off 8) and then let auto.arima predict. Then I can compare generated 8 values with actual 8 values.
But is there a formal way to test accuracy in ARIMA model? Is my approach correcT?</p>

<p>Is there a prebuilt function to test the accuracy of Arima.</p>

<p>Here is the code, I can always use x as my time series.</p>

<pre><code>y=auto.arima(x)
plot(forecast(y,h=30))
</code></pre>

<p>Sample 
time series 1</p>

<pre><code>0.0003748,0.0003929,0.0003653,0.0003557,0.0004463,0.000349,0.0003099,0.0003395,0.0003157,0.0002871,0.0002604,0.0002422,0.0001917,0.0002117,0.0002689
</code></pre>

<p>time series 2</p>

<pre><code>0.0003977,0.0003481,0.0002413,0.0002069,0.0002127,0.0002108,0.0002003,0.0002174,0.0002098,0.0002069,0.0001955,0.0001926,0.0002108,0.0002146,0.0002079
</code></pre>

<p>Both have 40 points. I can hold off 20% of them (8) and compare after auto.arima predicts.   But is there a simpler way I can test accuracy?</p>
"
"0.121486773537609","0.136082763487954","110589","<p>I'm using R to do some time series estimation.  I'm trying to rebuild the fitted values from an Arima model by hand to use in an Excel spreadsheet using the estimated coefficients and the input data. I can use the fitted command, but I'm trying to understand more how it works. Ex:  </p>

<pre><code>library(MASS)
library(tseries)
library(forecast)

set.seed(1)
N = ts(mvrnorm(50, mu=c(0,0), Sigma=matrix(c(1,0.56,0.56,1), ncol=2), 
       empirical=TRUE), frequency=12)
head(N)

&gt;            [,1]       [,2]
&gt;[1,] -0.05270976  0.7239571
&gt;[2,] -0.67232349 -0.6631604
&gt;[3,] -0.20193415  0.8176053
&gt;[4,] -0.54278281 -2.0458285
&gt;[5,]  1.38279994  0.9405811
&gt;[6,]  1.39979731  2.1717733

# Model: x(t) = a * x(t-1) + e(t)
fit = Arima(N[,1], order=c(1,0,0), include.constant=FALSE)

&gt; fit  
&gt;Series: N[, 1]  
&gt;ARIMA(1,0,0) with zero mean          
&gt;
&gt;Coefficients:  
&gt;         ar1  
&gt;       0.0293
&gt;s.e.   0.1400  
&gt;
&gt;sigma^2 estimated as 0.9791:  log likelihood=-70.42
&gt;AIC=144.84   AICc=145.1   BIC=148.66

# Build the fitted values: x(t)=a * x(t-1) 
pred  = fit$coef[1] * lag(fit$x, -1) 
pred1 = fitted(fit)
head(cbind(pred, pred1))   

&gt;             pred         pred1
&gt;[1,]           NA -2.255567e-05
&gt;[2,] -0.001541849 -1.541849e-03
&gt;[3,] -0.019666597 -1.966660e-02
&gt;[4,] -0.005906915 -5.906915e-03
&gt;[5,] -0.015877313 -1.587731e-02
&gt;[6,]  0.040449232  4.044923e-02 
</code></pre>

<p>In this case, <code>pred</code> and <code>pred1</code> match.  </p>

<p>However when I add in an <code>xreg</code>:  </p>

<pre><code># Model: x(t) = a*x(t-1) + b*xreg + e(t)
fit1 = Arima(N[,1], order=c(1,0,0), xreg=N[,2], include.constant=FALSE)

&gt;fit  
&gt;Series: N[, 1]  
&gt;ARIMA(1,0,0) with zero mean         
&gt;
&gt;Coefficients:  
&gt;         ar1  N[, 5]  
&gt;       0.0860  0.5606  
&gt;s.e.   0.1401  0.1155  
&gt;
&gt;sigma^2 estimated as 0.6675:  log likelihood=-60.85
&gt;AIC=127.69   AICc=128.22   BIC=133.4

# Build the fitted values: x(t) = a*x(t-1) + b*xreg 
pred2  = fit1$coef[1]*lag(fit1$x, -1) + fit1$coef[2]*fit1$xreg 
pred21 = fitted(fit1) 
head(cbind(pred2, pred21))

&gt;              pred2     pred21
&gt;[1,]         NA  0.4041670
&gt;[2,]  0.4013329 -0.4112205
&gt;[3,] -0.4296032  0.4325201
&gt;[4,]  0.4410005 -1.2037229
&gt;[5,] -1.1936161  0.5792684
&gt;[6,]  0.6462336  1.2911169
</code></pre>

<p>In this case, <code>pred2</code> and <code>pred21</code> do not match, and the only thing changed was adding an <code>xreg</code>. The only time I cannot build out the fitted values by hand is when the AR part is included. I was able to do it when only MA parts were included with the <code>xreg</code>.  I would really appreciate knowing how <code>Arima</code> treats <code>xreg</code> when generating the fitted values. </p>
"
"0.0850230301897704","0.111111111111111","223297","<p>I am trying to understand the coefficients retrieved from running <code>auto.arima</code> in R on my monthly time series of the annual change in House prices. When doing so, I obtain the following outcome:</p>

<pre><code>Series: AC.HousePrices 
ARIMA(1,1,1)(0,0,1)[12] with drift         

Coefficients:
         ar1      ma1     sma1   drift
      0.3243  -0.6592  -0.7892  -6e-04
s.e.  0.1733   0.1333   0.1161   4e-04

sigma^2 estimated as 0.0008257:  log likelihood=275.22
AIC=-540.44   AICc=-539.96   BIC=-526.07
</code></pre>

<p>To be honest I do not understand why I have two sets of parameters (p,d,q) and (P,D,Q)? The first set (1,1,1) seems to indicate that the series is first-order autoregressive model, nonstationary and with a simple exponential smoothing with drift? What are the second set of values (0,0,1)[12], is it telling me that my series looks yearly seasonal [12]?</p>
"
"0.162806707774543","0.193419617728553","138108","<p>Using the attached data that has been recently updated I am not able to obtain a statistically significant forecast. The data is extremely seasonal. The data is stored here for easy replication: </p>

<p><a href=""http://ge.tt/1uihVfA2/v/0?c"" rel=""nofollow"">http://ge.tt/1uihVfA2/v/0?c</a></p>

<pre><code># 1. Make a R timeseries out of the rawdata: specify frequency &amp; startdate
gIIP &lt;- ts(Data, frequency=12, start=c(2003,11))
print(gIIP)
plot.ts(gIIP, type=""l"", col=""blue"", ylab=""MTD Ships"", lwd=2,
        main=""Full data"")
grid()
</code></pre>

<p>Using the auto.arima function I don't need to factor a Box-Cox because the auto.arima factors that into selecting the best model. </p>

<p>Upon ""selecting the best model"" I  The best model suggested was Arima(order = c(0, 0, 1),
      seasonal = list(order = c(1, 0, 1), period = 12) with non-zero mean </p>

<pre><code># 5. Perform estimation
library(forecast)
library(zoo)
library(stats)
auto.arima(gIIP, d=NA, D=NA, max.p=12, max.q=12,
           max.P=2, max.Q=2, max.order=12, max.d=2, max.D=2,
           start.p=2, start.q=2, start.P=1, start.Q=1,
           stationary=FALSE, seasonal=TRUE,
           ic=c(""aicc"",""aic"", ""bic""), stepwise=FALSE, trace=TRUE,
           approximation=FALSE | frequency(gIIP)&gt;12), xreg=NULL,
           test=c(""kpss"",""adf"",""pp""), seasonal.test=c(""ocsb"",""ch""),
           allowdrift=TRUE, lambda=TRUE, parallel=FALSE, num.cores=4
</code></pre>

<p>)</p>

<p>then proceed to conduct accuracy diagnostics but unable to obtain any output.</p>

<pre><code>#Check standard error etc of ""fitted"" ARIMA
pos.arima &lt;- function(gIIP, order = c(0, 0, 1),
      seasonal = list(order = c(1, 0, 1), period = 12),
      xreg = NULL, include.drift=TRUE, 
      transform.pars = TRUE,
      fixed = NULL, init = NULL,
      method = c(""CSS-ML"", ""ML"", ""CSS""), 
      optim.method = ""BFGS"",
      optim.control = list(), kappa = 1e6)

acf(pos.arima) 
pacf(pos.arima)
</code></pre>

<p>The following step to conduct an ex ante (out of sample forecast) but also unable to obtain a statistically significant forecast---forecast with lowest standard error rate. I tested this by removing the last 5 observations to test the model. </p>

<pre><code># 7. Forecast Out-Of-Sample ---this used to work
fit &lt;- Arima(gIIP, order = c(0, 0, 1), seasonal = list(order = c(1, 0, 1), period = 12),
             xreg = TRUE, include.mean = TRUE, transform.pars = TRUE, 
             fixed = NULL, init = NULL, method = c(""CSS-ML"", ""ML"", ""CSS""), 
             optim.method = ""BFGS"", optim.control = list(), kappa = 1e6)
plot(forecast(fit,h=9))
print(forecast(fit,h=9))
</code></pre>

<p>Used to obtain output here. Can you please help me diagnose why there ARIMA model is not working like it once did for me? Thank you for your time.  </p>
"
"0.0850230301897704","0.111111111111111","196581","<p>Cochran et al. used an ARIMA model to investigate whether an execution had any deterrent effect on homicide. </p>

<p>""They used ARIMA modeling to control for trend, drift, and autocorrelation. However, ARIMA modeling cannot (1) control specifically for third-variable sociodemographic factors known to be associated with homicide or (2) isolate the effect of multiple independent variables of interest, such as levels of execution and the amount of news coverage that executions receive. Rather, conventional ARIMA modeling generally is capable of assessing the impact of only a single intervention factor in a time series.""</p>

<p>Baily replicated this study*, from which I've copied+pasted the above paragraph. He used multivariate time-series analysis instead of ARIMA.</p>

<p>My understanding, based on reading this <a href=""http://stats.stackexchange.com/questions/18375/how-to-fit-an-arimax-model-with-r"">answer</a>, is that xreg does what Baily suggests an ARIMA model cannot. I.e. can you not look at execution and homocide with news coverage of the event in an 'xreg'? I am asking here because Baily's paper was published in 1998, and I wonder if the problem he speaks about has since been solved.</p>

<pre><code>*Bailey, W. C. (1998). Deterrence, brutalization, and the death penalty: Another examination of Oklahoma's return to capital punishment. Criminology, 36(4), 711-734.
</code></pre>
"
"0.196692871441843","0.21168429875904","99488","<p>I am relatively new to statistics and not formally trained but have been given a complex problem to solve and need some guidance. I realise that I am out of my depth a bit here but would appreciate whatever help I can get bearing in mind that there is no budget for this and as a result it is not possible to purchase software or hire consultants.</p>

<p><strong>The Problem</strong></p>

<p>The business I work for has a large number of mobile representatives that can be dispatched to a variety of different jobs. There are ~100 different job types and each job can be broken up into 4 different final outcomes. Each of these 400 outcomes requires an allocation of man hours to complete. I have a count of how many times each one of these outcomes occurred in each hourband for the past 5 years.</p>

<p>I have been asked to forecast how many of each outcome will occur in each hourband for the 28days from the present. The resulting forecast will be used to anticipate staffing requirements on an hour-by-hour basis. As a result the forecasts for each hourband need to been fairly accurate.</p>

<p><strong>Factors</strong></p>

<p>In my data there are clearly some yearly, weekly, and daily seasonal effects. In general each outcome is more likely to occur at certain times of the day on certain days of the week and with some yearly trends.</p>

<p>Each different outcome is likely to be related to the frequency of a number of different outcomes. i.e. if <em>x</em> happens then <em>y</em> and/or <em>z</em> are likely but <em>a</em> and/or <em>b</em> are not.</p>

<p>There are a large number of environmental factors that contribute to the frequency of each outcome. These can include, but are not limited to weather, sociopolitical, financial trends, one off events.</p>

<p><strong>What I have tried</strong></p>

<p>So far I have tried using simple auto.arima, holtwinters and ets forecasts. holtwinters ended up producing a flat line (i.e. 5 and hour for the next 672 hours). ets doesnt work because the seasons are longer than 24 intervals. auto.arima produced the best results but they were still a long way off being accurate.</p>

<p>It was then suggested that I try tbats() and provide it with multiple seasonal lengths. I achieved best results by giving it seasonal lengths of 8760 (1yr) and 168 (1wk). Frustratingly, these results are within 1% when viewed as a sum of all hourbands in a 1 month block but are anything up to 300% (avg 20%) off when considering each individual hourband.</p>

<p>Both of these approaches have been applied over an individual outcome rather than considering all possible outcomes (and their correlation to each other).</p>

<p><strong>My thoughts so far</strong></p>

<p>At this stage I feel like my two options are to either to find a way to use something similar to tbats() that will look at the relationships between the multiple different outcomes as well as the seasonality and forecast based on that information.</p>

<p>or</p>

<p>Abandon that approach for a Neural Network model. My understanding (limited) is that using the Neural Network approach I may be able to 'factor' for the multitude of unknown environmental factors without having to actually identify them. I know this is lazy but my feeling for the data is that there are going to be a fair few unknown factors to identify and forecasting them may end up being a job in itself (i.e. weather conditions)</p>

<p><strong>The Question</strong> (finally)</p>

<p>What I am looking for is some guidance.</p>

<p>Considering the information above and the fact that I am pretty much limited to R, what is the best approach??</p>

<p>and</p>

<p>What are the basic steps I need to follow?</p>

<p>While I cant post my data online (due to my employers restrictions) I can send it an individual or two if someone was interested in giving us a hand to find a solution.</p>
"
"0.183670737350935","0.205737799949456","145251","<p>Sorry in advance if this is too basic of a question - I've been struggling with this data set for almost a month and feel like I'm going in circles, and the more I Google the more confused I get.</p>

<p>I have a time series of hourly activity levels (mean of 7 persons) for a period of about 2 months (1704 observations). There is obviously a strong ""seasonal"" component (freq=24) to this time series, with activity showing regular fluctuations between night and day. I am ultimately hoping to compare my activity time series to three other time series of environmental variables, to see how weather, temperature, etc affect people's activity on an hourly scale, following the methods in <a href=""http://cid.oxfordjournals.org/content/early/2012/05/21/cid.cis509.full"" rel=""nofollow"">this paper</a>. I'm not planning on doing forecasting, just wanting to know if these explanatory variables are affecting activity, and if so, how.</p>

<p>The paper linked above did their analysis in a few steps, if I understand correctly:</p>

<ol>
<li>Use stl to assess trend and seasonality.</li>
<li>Fit time series to ARIMA model.</li>
<li>Transform data into series of independent, identically distributed random variables</li>
<li>Choose best-fitting model by AIC</li>
<li>Use residuals for cross-correlating variables.</li>
</ol>

<p>Okay. Here are my questions:</p>

<ol>
<li><p>I can do step 1, but don't know how to relate that to step 2. Am I using the remainder from stl analysis for ARIMA modeling? If not, what's the point of step 1?</p></li>
<li><p>I understand how to choose some candidate models for ARIMA based on ACF, PACF, and auto.arima. But I can't get past the diagnostics. My Ljung-Box values are ALWAYS significant for ALL lags. Okay, so that means my residuals are correlated (I think). And since I want to use the residuals for cross-correlation, I assume that's bad. But no matter which models I try (I've tried maybe 6-10, is that enough?) I can't get good Ljung-Box p-values. The best fitting ARIMA so far (by AIC) is (1,0,2)x(1,1,2)24.</p></li>
</ol>

<p>Does this mean my time series doesn't fit an ARIMA model? How can I get iid residuals if I can't even get it to fit a model? Arrrghh.</p>

<p>So to be more succinct, my main question is: why do I always have these significant Ljung-Box values, and what can I do to fit a better model to get iid residuals?</p>

<p>Subsample of data (full set <a href=""https://www.dropbox.com/s/lhd9zu0x8r4o8pe/fitbit%20data.txt?dl=0"" rel=""nofollow"">here</a>):</p>

<pre><code>[1] 24 16 40 48 50 38 24  4  4  5  3  6  4  4  4  3 12 63 55 42 56 20 10 26 45 47 66 64 59
[30] 54 24  5  6  2  4  3  6 10  6  2 13 39 26 17 24 13 19 26 17 32 54 68 58 39 20  0  3  2
[59]  8  2  4  1  5 11  5 60 57 54 40 40 53 74 40 42 57 46 46 26  9  8  4  6 14  8  5  3  2
[88]  7 19 47 53 43 53 51 55 64 48 64 57 56 52 34 22  8  5  6  4  6  3  4  7  6 27 40 48 41
[117] 43 51 50 44 56 64 68 46 49 35 16  2 14  3  7  3 13  3  3  2 14 49 62 42 41 57 52 63 32
[146] 54 59 60 68 24 12  2  2  2  2  7  6  5  9 10 26 53 50 59 28 45 47 44 48 55 59 77 86 33
[175] 18 16 10  6  9  9 14  7  9  7  9 46 57 41 33 32 34 29 39 39 27 26  4 10  9  6  6  2  4
[204]  1  2  2  4  4 17 50 47 24 27 34 26 38 20  6 20 15 25  8  2  2  3  6  4  3  3  4  4  2
[233] 18 41 63 52 37 32 32 28 48 20  6 10  9  7  5 10  4  3  4  7  4  3  4 10  8 56 47 50 27
[262] 30 22 38 38 28 33 24 18 12 14  2 10  4 21  4  5  6  4  4 20 41 46 16  8 20 24 21 16 27
[291] 10  6 14  5  6  6 12  2 10  7  6  2  2  3 16 47 56 43 30 35 32 41 20 20 11 34 16  6 10
[320]  2  5 10  3 11  6  5  7  5 14 50 30 26 19 16 10  5 12 12 22 16 16 10  4  5  4  4  8 14
[349]  4  6  4  5 21 47 28 15  8 12 18 18 16 10  5  8 12  3  6  4  5 12 11  8  2  4  6 10 25
[378] 42 20 15  8 18 10 10  6 18 12  4  7  6  6  4  8 14  3 10 11  5 10  9 26 54 41 36 44  9
[407]  4  5  3  8 12 16 11 12 13 26  5 13 13  1  1  5 18  7 39 64 64 65 44 34 42 63 62 54 26
[436] 30 34 25 15  7  1  0  2  1  0  9 13 10 33 65 59 48 44 60 65 44 55 65 67 76 85 63 48  8
[465]  2  0  3  1  1  1  8 12 19 72 67 42 46 70 54 37 41 66 62 54 80 52 22  3  2  2  1  1  5
[494]  2  2  5 37 48 32 29 27 25 21  2 17  3 24  2  7  1  1  4  7  8  7  4  3  6  2  4 26 28
[523] 15  6  2  4  1 12  4  2  4 14 11  2  5  1 13 16 10  5 14  1  2  3 13 24 29 20 12  8  4
[552]  8  1 11  8 10  6  4  6  1  6  8  4  7 18 17 12  3 18 50 25 27 20 14 14  9 14 14 15  5
[581]  8  3  4  3  3 11 12 12  4 19 25  8 33 53 61 49 50 34 38 45 76 65 72 53 84 65 51 19  4
[610]  2 11  7  5  3  6  3 38 85 83 72 58 77 78 63 73 64 56 22  3 10 13 10  2  1  1  0  8  6
[639]  5  2 34 54 56 54 14  5 17 18 21  3 14 14  6  4  1  2  4 10  7  3  3  4 12 17 54 68 49
[668] 51 38 11 29 17  1  2  4  8  9  6  4  3 14  0  1 10  8  4  3  3 25 31  9  9 10  6  8  9
[697]  4 11  4  6  3  9  0  2  4  1 10 20 11  2  8  4 28 35 40 34 36 19 19 15 23 14  6  4  2
[726]  6  5  4  2  4  4  2  8 13 17  4 44 30 23 22 11  5 10 12  6  8 11  1 12 10  1  2  0  6
[755]  6  3  4  9  1  9 13 41  8  6  9 13 28  7  2  8  7  2  3  6  1  2  5  4  4  4  2  5  9
[784]  9 28 53 40 28  6  8  1  7  2 13 20  7  3  8  4  2  2  6  3  5 16  8  2 14 16 41 20 22
[813]  7  8 10 24 23 24 19 14  5  1  1  2  9  0  6  2 15  8  4  5 26 28  9  9 16 30 11 12  7
</code></pre>

<p>ACF/PACF after taking 24th difference: </p>

<p><img src=""http://i.stack.imgur.com/1SWHy.png"" alt=""ACF/PACF of time series after taking 24th difference""></p>

<p>Diagnostics of SARIMA(1,0,2)x(1,1,2)24 model (best model by AIC and as suggested by auto.arima):</p>

<p><img src=""http://i.stack.imgur.com/Tp70f.png"" alt=""enter image description here""></p>
"
"0.0490880693673816","0.0641500299099584","198662","<p>I am using the code below:</p>

<pre><code>#Training seasonal ARIMAx model on input dataset
fit&lt;-arima(visits_ts, order=c(1,0,0),seasonal=c(1,0,0),xreg=reg,method=""CSS"")

#Forecasting for future
pred&lt;-predict(fit,n.ahead=13, newxreg=nreg)
</code></pre>

<p>The code only generates out of sample forecast. However, I would also like to see the in-sample forecast for the training data set. How can I get the in-sample forecast?</p>
"
"0.10976425998969","0.143443827637312","224380","<p>I have a dataset which contains data from a sensor for every 5 minutes and am trying to predict for example 10 future values based on the first 500 values. My data looks like the following and could be downloaded <a href=""https://github.com/numenta/NAB/blob/master/data/artificialWithAnomaly/art_daily_flatmiddle.csv"" rel=""nofollow"">here</a>:</p>

<pre><code>timestamp,value
2014-04-01 00:00:00,-21.0483826823
2014-04-01 00:05:00,-20.2954768676
2014-04-01 00:10:00,-18.127229468299998
2014-04-01 00:15:00,-20.1716653997
2014-04-01 00:20:00,-21.223761612
2014-04-01 00:25:00,-19.1044911334
</code></pre>

<p><a href=""http://i.stack.imgur.com/iw6O3.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/iw6O3.png"" alt=""enter image description here""></a></p>

<p>I am taking the following steps:</p>

<pre><code># Read data from file and create time series    
myData &lt;- read.zoo(file=""filePath"", sep = "","", header = TRUE,index = 1, tz = """", format = ""%Y-%m-%d %H:%M:%S"", nrows=500)

# Fit ARIMA model to the data
fit &lt;- auto.arima(z, stepwise=FALSE, trace=TRUE, approximation=FALSE)

# Predict 10 timesteps ahead
pred &lt;- predict(fit, n.ahead = 10)
</code></pre>

<p>But when I print the prediction results they do not seem promising and model always converges to a single value:</p>

<pre><code>$pred
Time Series:
Start = 1396474800 
End = 1396477500 
Frequency = 0.00333333333333333 
 [1] 81.62789 81.62789 81.62789 81.62789 81.62789 81.62789 81.62789 81.62789 81.62789 81.62789

$se
Time Series:
Start = 1396474800 
End = 1396477500 
Frequency = 0.00333333333333333 
 [1]  7.136100  9.728122 11.762177 13.493007 15.025767 16.416032 17.697417 18.892088 20.015580 21.079276
</code></pre>

<p>And here is the summary of fit:</p>

<pre><code>&gt; summary(fit)
Series: z 
ARIMA(0,1,1)                    

Coefficients:
          ma1
      -0.0735
s.e.   0.0463

sigma^2 estimated as 50.92:  log likelihood=-1688.17
AIC=3380.34   AICc=3380.37   BIC=3388.77

Training set error measures:
                    ME     RMSE      MAE     MPE    MAPE       MASE        ACF1
Training set 0.2215984 7.121813 3.141386 1592726 1592732 0.07197436 0.001426353
</code></pre>

<p>This is my first day with R and I think I might be doing something wrong. Any help would be much appreciated.</p>

<p>Thanks</p>
"
"0.0490880693673816","0.0641500299099584","224176","<p>If my series requires a log-transformation to stabilize variability, do I apply the <code>sarima</code> function to the log-transformed series or the original series? Does the same apply to the <code>auto.arima</code> function?</p>
"
"0.12024072240843","0.157134840263677","223872","<p>Data consisting of 30 values is stored in a time series <code>time</code>.<br>
After applying ARIMA modelling on <code>time</code>, I used <code>forecast</code> function to predict future values:</p>

<pre><code>model = arima(time, order = c(3,2,1))
prediction = forecast.Arima(model,h=10)
prediction step is not working and showing error 
Error in ts(x) : object is not a matrix
</code></pre>

<p>As you see above, I am getting an error message. But if I do</p>

<pre><code>model = arima(time[1:25], order = c(3,2,1))
prediction = forecast.Arima(model,h=10)
</code></pre>

<p>it works. Why is it so?</p>

<p>When I used the <code>predict</code> function </p>

<pre><code>model = arima(time, order = c(3,2,1))
prediction=predict(model,n.ahead=10)
</code></pre>

<p>it also works.</p>

<p><strong>Which</strong> function would be better to use, <code>predict</code> or <code>forecast</code>, for ARIMA models in R, and <strong>why</strong>?</p>
"
"0.0981761387347632","0.128300059819917","161182","<p>I am trying to fit and forecast log returns of a price data using ARIMA model in R. For reproducibility, data is provided <a href=""https://docs.google.com/spreadsheets/d/1U619rL30yGcNRWxoiiIsfy-C-VOH2W2tnEivuIUOVq4/edit?usp=sharing"" rel=""nofollow"">here</a>. </p>

<p><strong>Steps Followed, Code and Results obtained</strong> </p>

<ol>
<li><p>Check for outliers (Package: <code>forecast</code>) - No outliers detected. </p>

<pre><code>outliers &lt;- tsoutliers(log.rtn)
</code></pre></li>
<li><p>Stationarity Check using ADF test (Package: fUnitRoots) - Series found to be stationary</p>

<pre><code>stationary &lt;- adfTest(log.rtn, lags = m1$order, type = c(""c""))
</code></pre></li>
<li><p>Determination of p,d,q using ACF and PACF (Package: astsa) - Based on my understanding, p = 2, d = 0, q = 2</p>

<pre><code>acf2(log.rtn, lags = 20)
</code></pre></li>
<li><p>Fitting ARIMA (Package: forecast)</p>

<pre><code>fit &lt;- auto.arima(log.rtn, stepwise=FALSE, trace=TRUE, approximation=FALSE)
</code></pre>

<p>Model obtained : ARIMA(2,0,1)</p>

<pre><code>Series: log.rtn 

  ARIMA(2,0,1) with zero mean     

Coefficients:
          ar1     ar2     ma1
      -0.5705  0.1557  0.6025
s.e.   0.1549  0.0532  0.1519

sigma^2 estimated as 0.001086:  log likelihood=775.57
AIC=-1543.14   AICc=-1543.04   BIC=-1527.29
</code></pre></li>
<li><p>Prediction (Package:forecast)</p>

<pre><code>fcast &lt;- forecast(fit, n.ahead=5)
plot(fcast)

    Point Forecast       Lo 80      Hi 80       Lo 95      Hi 95
390   1.416920e-03 -0.04080849 0.04364233 -0.06316127 0.06599511
391   8.228924e-04 -0.04142414 0.04306993 -0.06378837 0.06543416
392  -2.488236e-04 -0.04289257 0.04239493 -0.06546681 0.06496917
393   2.700663e-04 -0.04248622 0.04302635 -0.06512003 0.06566016
394  -1.928045e-04 -0.04303250 0.04264690 -0.06571047 0.06532486
395   1.520366e-04 -0.04273465 0.04303872 -0.06543749 0.06574156
396  -1.167506e-04 -0.04303183 0.04279833 -0.06574971 0.06551621
397   9.027370e-05 -0.04284167 0.04302221 -0.06556846 0.06574901
398  -6.967566e-05 -0.04301167 0.04287232 -0.06574379 0.06560444
399   5.380284e-05 -0.04289419 0.04300179 -0.06562948 0.06573708
</code></pre></li>
</ol>

<p>I am quite confused why the model is predicting so badly.</p>
"
"NaN","NaN","167434","<p>I am running an ARIMA model for my forecasts in R</p>"
"NaN","NaN","<p>My data set is 1 month's data.<br>",""
"NaN","NaN","It has 2976 observations which has a frequency of 15 min. I recieve data every 15 min.<br>",""
"NaN","NaN","There is little seasonality in the data.<br>",""
"NaN","NaN","I am trying to fit an ARIMA model using <code>auto.arima</code> for forecast of 24 tie steps.</p>",""
"NaN","NaN","<p>This error shows up: <code>AIC value approximated</code>.<br>",""
"NaN","NaN","Can one ignore this? If not then what is the way around?</p>",""
"NaN","NaN","","<r><time-series><arima>"
"0.245440346836908","0.243770113657842","162204","<p>I've got two time series (parameters of a model for males and females) and aim to identify an appropriate ARIMA model in order to make forecasts. My time series looks like:</p>

<p><img src=""http://i.stack.imgur.com/t8JkR.jpg"" alt=""enter image description here""></p>

<p>The plot and the ACF show non-stationary (the spikes of the ACF cut off very slowly). Thus, I use differencing and obtain:</p>

<p><img src=""http://i.stack.imgur.com/Zy1kC.jpg"" alt=""enter image description here""></p>

<p>This plot indicate that the series might now be stationary and the application of the kpss test and the adf test support this hypothesis.</p>

<p>Starting with the Male series, we make the following observations:</p>

<ul>
<li>The empirical autocorrelations at Lags 1,4,5,26 and 27 are significant different from zero.</li>
<li>The ACF cuts off (?), but I'm concerned about the relatively big spikes at lag 26 and 27.</li>
<li>Only the empirical partial autocorrelations at Lags 1 and 2 are significant different from zero.</li>
</ul>

<p>On ground of these observations alone, if I had to choose a pure AR or MA model for the differenced time series, I would tend to choose either an AR(2) model by arguing that:</p>

<ul>
<li>We have no significant partial autocorrelations for lag greater than 2 </li>
<li>The ACF cuts off except for the region around lag 27. (Are these few outliers alone an indicator, that a mixed ARMA model would be appropriate?)</li>
</ul>

<p>or an MA(1) model by arguing that:</p>

<ul>
<li>The PACF clearly cuts off</li>
<li>We have for lags greater 1 only 4 spikes exceeding the critical value in magnitude. This is ""only"" one more than the 3 spikes (95% out of 60) which would be allowed to lie outside the dotted area.</li>
</ul>

<p>There are no characteristica of an ARIMA(1,1,1) model and choosing orders of p and q of an ARIMA model on grounds of ACF and PACF for p+q > 2 gets difficult.</p>

<p>Using auto.arima() with the AIC criterion (Should I use AIC or AICC?) gives:</p>

<ol>
<li>ARIMA(2,1,1) with Drift; AIC=280.2783</li>
<li>ARIMA(0,1,1) with Drift; AIC=280.2784</li>
<li>ARIMA(2,1,0) with Drift; AIC=281.437</li>
</ol>

<p>All three considered models show white noise residuals:</p>

<p><img src=""http://i.stack.imgur.com/WM0By.jpg"" alt=""enter image description here""></p>

<p>My summed up questions are:</p>

<ol>
<li>Can you still describe the ACF of the time series as cutting of despite the spikes around lag 26?</li>
<li>Are these outliers an indicator that a mixed ARMA model might be more appropriate?</li>
<li>Which Information Criterion should I choose? AIC? AICC?</li>
<li>The residuals of the three models with the highest AIC do all show white noise behavior, but the difference in the AIC is only very small. Should I use the one with the fewest parameters, i.e. an ARIMA(0,1,1)?</li>
<li>Is my argumentation in general plausible?</li>
<li>Are their further possibilities to determine which model might be better or should I for example, the two with the highest AIC and perform backtests to test the plausibility of forecasts?</li>
</ol>

<p>Thanks!</p>

<p><strong>EDIT:</strong> Here is my data:</p>

<pre><code>-5.9112948202 -5.3429985122 -4.7382340534 -3.1129015623 -3.0350910288 -2.3218904871 -1.7926701792 -1.1417358384 -0.6665592055 -0.2907748318 0.2899480865 0.4637205370  0.5826312749  0.3869227286  0.6268379174  0.7439125292 0.7641139207  0.7613140511  3.0143912244 -0.7339255839  2.0109976796 0.8282394650 -2.5668367983  5.9826406394  1.9569198553  2.3860893476 2.0883339390  1.9761894580  2.2601997245  2.2464027995  2.5131158613 3.4564765529  4.2307335557  4.0298688374  3.7626317439  3.1026407174 2.1690168737  1.5617407254  2.6790460788  0.4652054768 -0.0501046517 -1.0157683791 -0.5113698054 -0.0180401353 -1.9471272198 -0.2550365250 -1.1269988523  0.5152074134  0.2362626753 -2.9978337017  1.4924705528 -1.4907767844 -0.5492041416 -0.7313021018 -0.6531515868 -0.4094159299 -0.5525401626 -0.0611454515 -0.5256272882 -1.1235247363 -1.7299848758 -1.3807763611 -1.6999054476 -4.3155973110 -4.7843298990
</code></pre>
"
"0.259904214669968","0.316227766016838","176129","<p>I've been working on some various time series forecasts and I've begun to notice a trend (pardon the pun) in my analyses. For about 5-7 datasets that I've worked with so far, it would be helpful to allow for multiple seasonal periods along with an option for holiday dummies. I've tried various methods and usually stick with <code>tbats</code> since <code>auto.arima()</code> with regressors has been giving me issues. By this point, it's probably obvious I'm working in R.</p>

<p>Before I get too far, let me give some sample data. Hopefully the following link works: <a href=""https://gist.github.com/JaredRayWolf/c8cb601dd26ec72a64d0"" rel=""nofollow"">https://gist.github.com/JaredRayWolf/c8cb601dd26ec72a64d0</a>.</p>

<p>This data yields the following time series plot:
<a href=""http://i.stack.imgur.com/FYS1x.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FYS1x.jpg"" alt=""Time Series Plot""></a>
The large dips are around Christmas and New Years, however there are also smaller dips around Thanksgiving. In the code below, I name this dataset <code>traindata</code>.</p>

<p>Now, <code>ets</code> and ""plain"" <code>auto.arima</code> don't look so hot in the long run since they are limited to only one seasonal period (I choose weekly). However for my test set that I held out they performed fairly well for the month's worth of data (with the exception of Labor Day weekend). This being said, forecasting out for a year would be ideal.</p>

<p>I next tried <code>tbats</code> with weekly and yearly seasonal periods. That results in the following forecast:
<a href=""http://i.stack.imgur.com/kcXmd.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/kcXmd.jpg"" alt=""TBATS Forecast""></a></p>

<p>Now this looks pretty good. From the naked eye it looks great at taking into account the weekly and yearly seasonal periods as well as Christmas and New Years effects (since they obviously fall on the same dates each year). It would be best if I could include the holidays (and the days around them) as dummy variables. Hence my attempts at <code>auto.arima</code> with <code>xreg</code> regressors.</p>

<p>For ARIMA with regressors, I've followed Dr. Hyndman's suggestions for the fourier function (given here: <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/longseasonality/</a>) as well as his selection of the number of fourier terms (given here: <a href=""http://robjhyndman.com/hyndsight/forecasting-weekly-data/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/forecasting-weekly-data/</a>)</p>

<p>My code is as follows:</p>

<pre><code>fourier &lt;- function(t,terms,period)
{
  n &lt;- length(t)
  X &lt;- matrix(,nrow=n,ncol=2*terms)
  for(i in 1:terms)
  {
    X[,2*i-1] &lt;- sin(2*pi*i*t/period)
    X[,2*i] &lt;- cos(2*pi*i*t/period)
  }
  colnames(X) &lt;- paste(paste(c(""S"",""C""),rep(1:terms,rep(2,terms)),sep=""""),period,sep=""_"")
  return(X)
}

fcdaysout&lt;-365
m1&lt;-7
m2&lt;-30.4375
m3&lt;-365.25

hol&lt;-cbind(traindata$CPY_HOL, traindata$DAY_BEFORE_CPY_HOL, traindata$DAY_AFTER_CPY_HOL)
hol&lt;-as.matrix(hol)

n &lt;- nrow(traindata)
bestfit &lt;- list(aicc=Inf)
bestk &lt;- 0

for(i in 1:m1)
{
    fake_xreg = cbind(fourier(1:n,i,m1), fourier(1:n,i,m3), hol)
    fit &lt;- auto.arima(traindata$ACTIVE_LOADS, xreg = fake_xreg, max.p=10, max.q=10, max.d=2, stepwise=FALSE, ic=""aicc"", allowdrift=TRUE)
    	if(fit$aicc &lt; bestfit$aicc)
    {
        bestfit &lt;- fit
        bestk &lt;- i
    }
    else
    {
    }
}

k &lt;- bestk
k
##k&lt;-3

xreg&lt;-cbind(fourier(1:n,k,m1), fourier(1:n,k,m3), hol)
xreg&lt;-as.matrix(xreg)

aacov_fit &lt;- auto.arima(traindata$ACTIVE_LOADS, xreg = xreg, max.p=10, max.q=10, max.d=2, stepwise=FALSE, ic=""aic"", allowdrift=TRUE)
summary(aacov_fit)
</code></pre>

<p>Where my issues come in is inside the for loop to determine the <code>k</code>, the number of fourier terms, that minimizes AIC. In all of my attempts at ARIMA with regressors, it always produces an error when <code>k&gt;3</code> (or <code>i&gt;3</code> if we're talking about inside my loop). The error being <code>Error in solve.default(res$hessian * n.used, A) : system is computationally singular: reciprocal condition number = 1.39139e-34</code>. Simply setting <code>k=3</code> gives some decent results for my test set but for the next year it doesn't appear to adequately catch the steep drops around the end of the year and is much smoother than imagined as evidenced in this forecast:<a href=""http://i.stack.imgur.com/rj30h.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/rj30h.jpg"" alt=""AutoArima with Covariates (k=3)""></a></p>

<p>I assume this general smoothness is due to the small number of fourier pairs. Is there an oversight in my code in that I'm just royally screwing up the procedure provided by Dr. Hyndman? Or is there a theoretical issue that I'm unknowingly running into by trying to find more than 3 pairs of fourier terms for the multiple seasons I'm attempting to account for? Is there a better way to include the multiple seasonalities and dummy variables?</p>

<p>Any help in getting these covariates into the arima model with an appropriate number of fourier terms would be appreciated. If not, I'd at least like to know whether or not what I'm attempting is possible in general with larger number of fourier pairs.</p>
"
"0.0694210134500623","0.0907218423253029","181216","<p>I am new to R and forecasting .I have data for a certain product. It contains value sales and its promotions. The data is weekly and there are about 104 data points. </p>

<p>I converted the sales into a ts object and created seasonaldummy's to capture seasonality. </p>

<pre><code>actual_val = ts(sku1$Sal , frequency = 52)
dummy_val = seasonaldummy(actual_val) 
</code></pre>

<p>The dummy's were later combined with the promo variables to create external regressors xreg_val for the model. Those promotions which were not held for this sku were removed before combining the two.</p>

<pre><code>model_value &lt;- try(auto.arima(actual_val , xreg = xreg_val ) , silent = TRUE)
</code></pre>

<p>I have received the following error</p>

<pre><code>Error in optim(init[mask], armaCSS, method = optim.method, hessian = FALSE,    
non-finite value supplied by optim
</code></pre>

<p>I could not understand where exactly I have gone wrong in this.</p>

<p>Attaching a sample of the data. Kindly help me with this</p>

<p><a href=""https://drive.google.com/file/d/0B6sOv1da0JMeb01XYW92UzRSZ0U/view?usp=sharing"" rel=""nofollow"">https://drive.google.com/file/d/0B6sOv1da0JMeb01XYW92UzRSZ0U/view?usp=sharing</a></p>
"
