"V1","V2","V3","V4"
"0.0395903791232448","0","  2728","<p>I am clustering a dataset using the pam command (from {cluster} package), and I wish to decide on the number of clusters to use.</p>

<p>I was able to implement The_Elbow_Method in R (<a href=""http://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#The_Elbow_Method"" rel=""nofollow"">see wiki</a>) for doing that.  But that doesn't provide me with any solid criteria (like AIC, <a href=""http://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set#Information_Criterion_Approach"" rel=""nofollow"">for example</a>) for decision.</p>

<p>I came by the {clValid} package which looks promising, but I wanted to know if there are <strong>any other R solutions (you know of) for choosing the number of clusters for pam?</strong></p>

<p>Here's some dummy code if someone wants to show examples:</p>

<pre><code>data(iris)
head(iris)
require(cluster)
pam(iris[,1:4], 3)
</code></pre>
"
"0.0685725481323742","0.0933859209547035","  2950","<p>I am trying to determine significant differences between groups of data using a k-nearest neighbor randomization test in R. This test basically looks for similarity amongst homogenous groups and separates them out using random clustering. In the literature, this test is called a ""K-nearest neighbor (kNN) randomization test,"" however, I'm not certain if it called by other names elsewhere.</p>

<p>For my specific data, I have isotopic ratios given for various prey items. I have already grouped together ecologically similar prey item types, and now want to see if those groups differ from one another in their isotopic signature. This is where the kNN test would come in. </p>

<p>Thanks for all of your answers - I'm new to this site, so I'll address your inquiries as applicable in the comments section.</p>
"
"0.0559892510955854","0.0381246425831512","  3048","<p>I have a matrix where a(i,j) tells me how many times individual i viewed page j. There are 27K individuals and 95K pages. I would like to have a handful of ""dimensions"" or ""aspects"" in the space of pages which would correspond to sets of pages which are often viewed together. My ultimate goal is to then be able to compute how often individual i has viewed pages that fall in dimension 1, dimension 2, etc.</p>

<p>I have read the R documentation on <a href=""http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Dimensionality_Reduction/Principal_Component_Analysis"">principal component analysis</a> and <a href=""http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Dimensionality_Reduction/Singular_Value_Decomposition"">single value decomposition</a> and have executed these commands, but I am unsure how to proceed.</p>

<p>How can I use dimensionality reduction to do this? Or is this really a clustering problem and I should instead look into clustering algorithms?</p>

<p>Many thanks for any insight
~l</p>
"
"0.0685725481323742","0.0933859209547035","  3238","<p>I have a set of time series data. Each series covers the same period, although the actual dates in each time series may not all 'line up' exactly.</p>

<p>That is to say, if the Time series were to be read into a 2D matrix, it would look something like this:</p>

<pre><code>date     T1   T2   T3 .... TN
1/1/01   100  59   42      N/A
2/1/01   120  29   N/A     42.5
3/1/01   110  N/A  12      36.82
4/1/01   N/A  59   40      61.82
5/1/01    05  99   42      23.68
...
31/12/01  100  59   42     N/A

etc 
</code></pre>

<p>I want to write an R script that will segregate the time series {T1, T2, ... TN} into 'families' where a family is defined as a set of series which ""tend to move in sympathy"" with each other. </p>

<p>For the 'clustering' part, I will need to select/define a kind of distance measure. I am not quite sure how to go about this, since I am dealing with time series, and a pair of series that may move in sympathy over one interval, may not do so in a subsequent interval.</p>

<p>I am sure there are far more experienced/clever people than me on here, so I would be grateful for any suggestions, ideas on what algorithm/heuristic to use for the distance measure and how to use that in clustering the time series. </p>

<p>My guess is that there is NOT an established robust statistic method for doing this, so I would be very interested to see how people approach/solve this problem - thinking like a statistician.</p>
"
"0.142745141943918","0.13458343602478","  3271","<p>I have seen a few queries on clustering in time series and specifically on clustering, but I don't think they answer my question. </p>

<p><strong>Background:</strong> I want to cluster genes in a time course experiment in yeast. There are four time points say: <em>t1</em>  <em>t2</em>  <em>t3</em>  and  <em>t4</em> and total number of genes <em>G</em>. I have the data in form a matrix <em>M</em> in which the columns represent the treatments (or time points)  <em>t1</em>  <em>t2</em>  <em>t3</em>  and  <em>t4</em>  and the rows represent the genes. Therefore, <em>M</em> is a Gx4 matrix. </p>

<p><strong>Problem:</strong> I want to cluster the genes which behave the same across all time points <em>t1</em>  <em>t2</em>  <em>t3</em>  and  <em>t4</em>  as well as within a particular time point <em>ti</em> , where i is in {1, 2, 3, 4} (In case we cannot do both the clusterings together, the clustering within a time point is more important than clustering across time points). In addition to this, I also want to draw a heatmap.</p>

<p><strong>My Solution:</strong> 
I use the R code below to obtain a heatmap as well as the clusters using <code>hclust</code> function in R (performs hierarchical clustering with euclidean distance)</p>

<pre><code>    row.scaled.expr &lt;- (expr.diff - rowMeans(expr.diff)) / rowSds(expr.diff)

    breaks.expr &lt;- c(quantile(row.scaled.expr[row.scaled.expr &lt; 0],
                               seq(0,1,length=10)[-9]), 0,
                               quantile(row.scaled.expr[row.scaled.expr &gt; 0],
                               seq(0,1,length=10))[-1] )


    blue.red.expr &lt;- maPalette(low = ""blue"", high = ""red"", mid = ""white"",
                     k=length(breaks.expr) - 1)

    pdf(""images/clust.pdf"",
         height=30,width=20,pointsize=20)
    ht1 &lt;- heatmap.2(row.scaled.expr, col = blue.red.expr, Colv = FALSE, key = FALSE, 
      dendrogram = ""row"", scale = ""none"", trace = ""none"",
      cex=1.5, cexRow=1, cexCol=2,
      density.info = ""none"", breaks = breaks.expr, 
      labCol = colnames(row.scaled.expr),
      labRow="""",
      lmat=rbind( c(0, 3), c(2,1), c(0,4) ), lhei=c(0.25, 4, 0.25 ),
      main=expression(""Heat Map""),
      ylab=""Genes in the Microarray"",
      xlab=""Treatments""
      )
    dev.off()
</code></pre>

<p>I recently discovered <code>hopach</code> package in <em>Bioconductor</em> which can be used to estimate the number of clusters. Previously, I was randomly assigning the number of bins for the heatmap and cutting the tree at an appropriate height to get a pre-specified number of clusters. </p>

<p><strong>Possible Problems in my solution:</strong></p>

<ol>
<li>I may be not clustering the genes within a particular treatment and clustering genes only across treatments or vice versa.</li>
<li>There may be better ways of obtaining a heatmap for the pattern I want to see (similar genes within a treatment and across treatments).</li>
<li>There may be better visualization methods which I am not aware of.</li>
</ol>

<p><strong>Note:</strong></p>

<ol>
<li><p><em>csgillespie</em> (moderator) has a more general document on his website in which he discusses all the aspects of time course analysis (including heatmaps and clustering). I would appreciate if you can point me to an articles which describe heatmaps and clustering in detail.</p></li>
<li><p>I have tried the <code>pvclust</code> package, but it complains that <em>M</em> is singular and then it crashes.</p></li>
</ol>
"
"0.0559892510955854","0.0381246425831512","  4694","<p><a href=""http://www.ambion.com/techlib/tn/95/954.html"" rel=""nofollow"">Here</a> is an example of hierarchical clustering of genes in the microarray data using the <strong>weighted pair gene method</strong> in <code>Spotfire</code>. I am not sure how to do this in <code>R</code>. In the <code>hclust</code> function, I see <code>ward"", ""single"", ""complete"", ""average"", ""mcquitty"", ""median"" or ""centroid""</code> as the methods. </p>

<p>Also, lets say I have performed hierarchical clustering and found groups of genes using <code>cuttree</code> method. I wanted to plot the expression of genes in a group across columns (which may represent treatment, time, etc.). And I want to do this for all the groups separately. In a way similar to the <a href=""http://www.bioconductor.org/packages/release/bioc/html/Mfuzz.html"" rel=""nofollow"">Mfuzz</a> package's way of showing clusters.</p>

<p>Can any one please help me?</p>

<p>TIA for any pointers.</p>
"
"0.142745141943918","0.164490866252509","  5087","<p>There are numerous procedures for functional data clustering based on orthonormal basis functions. I have a series of models built with the GAMM models, using the <code>gamm()</code> from the mgcv package in R. For fitting a long-term trend, I use a thin plate regression spline. Next to that, I introduce a CAR1 model in the random component to correct for autocorrelation. For more info, see eg the paper of Simon Wood on <a href=""http://r.789695.n4.nabble.com/attachment/2063352/0/tprs.pdf"">thin plate regression splines</a> or his <a href=""http://rads.stackoverflow.com/amzn/click/1584884746"">book on GAM models</a>.</p>

<p>Now I'm a bit puzzled in how I get the correct coefficients out of the models. And I'm even less confident that the coefficients I can extract, are the ones I should use to cluster different models. </p>

<p>A simple example, using:</p>

<pre><code>#runnable code
require(mgcv)
require(nlme)
library(RLRsim)
library(RColorBrewer)

x1 &lt;- 1:1000
x2 &lt;- runif(1000,10,500)

fx1 &lt;- -4*sin(x1/50)
fx2 &lt;- -10*(x2)^(1/4)
y &lt;- 60+ fx1 + fx2 + rnorm(1000,0,5)

test &lt;- gamm(y~s(x1)+s(x2))
# end runnable code
</code></pre>

<p>Then I can construct the original basis using smoothCon :</p>

<pre><code>#runnable code
um &lt;- smoothCon(s(x1),data=data.frame(x1=x1),
         knots=NULL,absorb.cons=FALSE)
#end runnable code
</code></pre>

<p>Now,when I look at the basis functions I can extract using </p>

<pre><code># runnable code
X &lt;- extract.lmeDesign(test$lme)$X
Z &lt;- extract.lmeDesign(test$lme)$Z

op &lt;- par(mfrow=c(2,5),mar=c(4,4,1,1))
plot(x1,X[,1],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,X[,2],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,8],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,7],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,6],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,5],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,4],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,3],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,2],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,1],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
par(op)
# end runnable code
</code></pre>

<p>they look already quite different. I can get the final coefficients used to build the smoother by</p>

<pre><code>#runnable code
Fcoef &lt;- test$lme$coef$fixed
Rcoef &lt;- unlist(test$lme$coef$random)
#end runnable code
</code></pre>

<p>but I'm far from sure these are the coefficients I look for. I fear I can't just use those coefficients as data in a clustering procedure. I would really like to know which coefficients are used to transform the basis functions from the ones I get with <code>smoothCon()</code> to the ones I extract from the lme-part of the gamm-object. And if possible, where I can find them. I've read the related articles, but somehow I fail to figure it out myself. All help is appreciated.</p>
"
"NaN","NaN","  5160","<p>I am looking for a good tutorial on clustering data in <code>R</code> using hierarchical dirichlet process (HDP) (one of the recent and popular nonparametric Bayesian methods). </p>

<p>There is <code>DPpackage</code> (IMHO, the most comprehensive of all the available ones) in <code>R</code> for nonparametric Bayesian analysis. But I am unable to understand the examples provided in <code>R News</code> or in the package reference manual well enough to code HDP.</p>

<p>Any help or pointer is appreciated.</p>

<p>A C++ implementation of HDP for topic modeling is available <a href=""http://www.cs.princeton.edu/~blei/topicmodeling.html"">here</a> (please look at the bottom for C++ code)</p>
"
"0.0395903791232448","0","  5366","<p>I need to cluster units into $k$ clusters to minimize within-group sum of squares (WSS), but I need to ensure that the clusters each contain at least $m$ units.  Any idea if any of R's clustering functions allow for clustering into $k$ clusters subject to a minimum cluster size constraint?  kmeans() does not seem to offer a size constraint option.</p>
"
"NaN","NaN","  6890","<p>I have dendrogram and a distance matrix. I wish to compute a heatmap -- without re-doing the distance matrix and clustering.</p>

<p>Is there a function in R that permits this?</p>
"
"0.131306432859723","0.14630761997153","  7175","<p>I'm experimenting with classifying data into groups. I'm quite new to this topic, and trying to understand the output of some of the analysis.</p>

<p>Using examples from <a href=""http://www.statmethods.net/advstats/cluster.html"">Quick-R</a>, several <code>R</code> packages are suggested. I have tried using two of these packages (<code>fpc</code> using the <code>kmeans</code> function,  and <code>mclust</code>). One aspect of this analysis that I do not understand is the comparison of the results.</p>

<pre><code># comparing 2 cluster solutions
library(fpc)
cluster.stats(d, fit1$cluster, fit2$cluster)
</code></pre>

<p>I've read through the relevant parts of the <code>fpc</code> <a href=""http://cran.r-project.org/web/packages/fpc/fpc.pdf"">manual</a> and am still not clear on what I should be aiming for. For example, this is the output of comparing two different clustering approaches:</p>

<pre><code>$n
[1] 521

$cluster.number
[1] 4

$cluster.size
[1] 250 119  78  74

$diameter
[1]  5.278162  9.773658 16.460074  7.328020

$average.distance
[1] 1.632656 2.106422 3.461598 2.622574

$median.distance
[1] 1.562625 1.788113 2.763217 2.463826

$separation
[1] 0.2797048 0.3754188 0.2797048 0.3557264

$average.toother
[1] 3.442575 3.929158 4.068230 4.425910

$separation.matrix
          [,1]      [,2]      [,3]      [,4]
[1,] 0.0000000 0.3754188 0.2797048 0.3557264
[2,] 0.3754188 0.0000000 0.6299734 2.9020383
[3,] 0.2797048 0.6299734 0.0000000 0.6803704
[4,] 0.3557264 2.9020383 0.6803704 0.0000000

$average.between
[1] 3.865142

$average.within
[1] 1.894740

$n.between
[1] 91610

$n.within
[1] 43850

$within.cluster.ss
[1] 1785.935

$clus.avg.silwidths
         1          2          3          4 
0.42072895 0.31672350 0.01810699 0.23728253 

$avg.silwidth
[1] 0.3106403

$g2
NULL

$g3
NULL

$pearsongamma
[1] 0.4869491

$dunn
[1] 0.01699292

$entropy
[1] 1.251134

$wb.ratio
[1] 0.4902123

$ch
[1] 178.9074

$corrected.rand
[1] 0.2046704

$vi
[1] 1.56189
</code></pre>

<hr>

<p>My primary question here is to better understand how to interpret the results of this cluster comparison.</p>

<hr>

<p>Previously, I had asked more about the effect of scaling data, and calculating a distance matrix. However that was answered clearly by mariana soffer, and I'm just reorganizing my question to emphasize that I am interested in the intrepretation of my output which is a comparison of two different clustering algorithms.</p>

<p><em>Previous part of question</em>: 
If I am doing any type of clustering, should I always scale data? For example, I am using the function <code>dist()</code> on my scaled dataset as input to the <code>cluster.stats()</code> function, however I don't fully understand what is going on. I read about <code>dist()</code> <a href=""http://stat.ethz.ch/R-manual/R-patched/library/stats/html/dist.html"">here</a> and it states that:</p>

<blockquote>
  <p>this function computes and returns the distance matrix computed by using the specified distance measure to compute the distances between the rows of a data matrix.</p>
</blockquote>
"
"0.125195771459034","0.102299150920571","  7250","<p>I'm having difficulty understanding one or two aspects of the cluster package. I'm following the example from <a href=""http://www.statmethods.net/advstats/cluster.html"">Quick-R</a> closely, but don't understand one or two aspects of the analysis. I've included the code that I am using for this particular example.</p>

<pre><code>## Libraries
library(stats)
library(fpc) 

## Data
mydata = structure(list(a = c(461.4210925, 1549.524107, 936.42856, 0, 
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 131.4349206, 0, 762.6110846, 
3837.850406), b = c(19578.64174, 2233.308842, 4714.514274, 0, 
2760.510002, 1225.392118, 3706.428246, 2693.353714, 2674.126613, 
592.7384164, 1820.976961, 1318.654162, 1075.854792, 1211.248996, 
1851.363623, 3245.540062, 1711.817955, 2127.285272, 2186.671242
), c = c(1101.899095, 3.166506463, 0, 0, 0, 1130.890295, 0, 654.5054857, 
100.9491289, 0, 0, 0, 0, 0, 789.091922, 0, 0, 0, 0), d = c(33184.53871, 
11777.47447, 15961.71874, 10951.32402, 12840.14983, 13305.26424, 
12193.16597, 14873.26461, 11129.10269, 11642.93146, 9684.238583, 
15946.48195, 11025.08607, 11686.32213, 10608.82649, 8635.844964, 
10837.96219, 10772.53223, 14844.76478), e = c(13252.50358, 2509.5037, 
1418.364947, 2217.952853, 166.92007, 3585.488983, 1776.410835, 
3445.14319, 1675.722506, 1902.396338, 945.5376228, 1205.456943, 
2048.880329, 2883.497101, 1253.020175, 1507.442736, 0, 1686.548559, 
5662.704559), f = c(44.24828759, 0, 485.9617601, 372.108855, 
0, 509.4916263, 0, 0, 0, 212.9541122, 80.62920455, 0, 0, 30.16525587, 
135.0501384, 68.38023073, 0, 21.9317122, 65.09052886), g = c(415.8909649, 
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 637.2629479, 0, 0, 
0), h = c(583.2213618, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
0, 0, 0, 0, 0), i = c(68206.47387, 18072.97762, 23516.98828, 
13541.38572, 15767.5799, 19756.52726, 17676.00505, 21666.267, 
15579.90094, 14351.02033, 12531.38237, 18470.59306, 14149.82119, 
15811.23348, 14637.35235, 13588.64291, 12549.78014, 15370.90886, 
26597.08152)), .Names = c(""a"", ""b"", ""c"", ""d"", ""e"", ""f"", ""g"", 
""h"", ""i""), row.names = c(NA, -19L), class = ""data.frame"")
</code></pre>

<p>Then I standardize the variables:</p>

<pre><code># standardize variables
mydata &lt;- scale(mydata) 

## K-means Clustering 

# Determine number of clusters
wss &lt;- (nrow(mydata)-1)*sum(apply(mydata,2,var))
for (i in 2:15) wss[i] &lt;- sum(kmeans(mydata, centers=i)$withinss)
# Q1
plot(1:15, wss, type=""b"", xlab=""Number of Clusters"",  ylab=""Within groups sum of squares"") 

# K-Means Cluster Analysis
fit &lt;- kmeans(mydata, 3) # number of values in cluster solution

# get cluster means 
aggregate(mydata,by=list(fit$cluster),FUN=mean)

# append cluster assignment
mydata &lt;- data.frame(mydata, cluster = fit$cluster)

# Cluster Plot against 1st 2 principal components - vary parameters for most readable graph
clusplot(mydata, fit$cluster, color=TRUE, shade=TRUE, labels=0, lines=0) # Q2

# Centroid Plot against 1st 2 discriminant functions
plotcluster(mydata, fit$cluster)
</code></pre>

<p>My question is, how can the plot which shows the number of clusters (marked <code>Q1</code> in my code) be related to the actual values (cluster number and variable name) ? </p>

<p>Update: I now understand that the <code>clusplot()</code> function is a bivariate plot, with PCA1 and PCA2. However, I don't understand the link between the PCA components and the cluster groups. What is the relationship between the PCA values and the clustering groups? I've read elsewhere about the link between kmeans and PCA, but I still don't understand how they can be displayed on the same bivariate graph. </p>
"
"0.197951895616224","0.204882269086533","  7903","<p>QUESTION:</p>

<p>I have binary data on exam questions (correct/incorrect). Some individuals might have had prior access to a subset of questions and their correct answers. I donâ€™t know who, how many, or which. If there were no cheating, suppose I would model the probability of a correct response for item $i$ as $logit((p_i = 1 | z)) = \beta_i + z$, where $\beta_i$ represents question difficulty and $z$ is the individualâ€™s latent ability. This is a very simple item response model that can be estimated with functions like ltmâ€™s rasch() in R. In addition to the estimates $\hat{z}_j$ (where $j$ indexes individuals) of the latent variable, I have access to separate estimates $\hat{q}_j$ of the same latent variable which were derived from another dataset in which cheating was not possible. </p>

<p>The goal is to identify individuals who likely cheated and the items they cheated on. What are some approaches you might take? In addition to the raw data, $\hat{\beta}_i$, $\hat{z}_j$, and $\hat{q}_j$ are all available, although the first two will have some bias due to cheating. Ideally, the solution would come in the form of probabilistic clustering/classification, although this is not necessary. Practical ideas are highly welcomed as are formal approaches. </p>

<p>So far, I have compared the correlation of question scores for pairs of individuals with higher vs. lower $\hat{q}_j -\hat{z}_j $ scores (where $\hat{q}_j - \hat{z}_j $ is a rough index of the probability that they cheated). For example, I sorted individuals by $\hat{q}_j - \hat{z}_j $ and then plotted the correlation of successive pairs of individualsâ€™ question scores. I also tried plotting the mean correlation of scores for individuals whose $\hat{q}_j - \hat{z}_j $ values were greater than the $n^{th}$ quantile of $\hat{q}_j - \hat{z}_j $, as a function of $n$. No obvious patterns for either approach.</p>

<hr>

<p>UPDATE:</p>

<p>I ended up combining ideas from @SheldonCooper and the helpful <a href=""http://pricetheory.uchicago.edu/levitt/Papers/JacobLevitt2003.pdf"">Freakonomics paper</a> that @whuber pointed me toward. <em>Other ideas/comments/criticisms welcome.</em></p>

<p>Let $X_{ij}$ be person $j$â€™s binary score on question $i$. Estimate the item response model $$logit(Pr(X_{ij} = 1 | z_j) = \beta_i + z_j,$$ where $\beta_i$ is the itemâ€™s easiness parameter and $z_j$ is a latent ability variable. (A more complicated model can be substituted; Iâ€™m using a 2PL in my application). As I mentioned in my original post, I have estimates $\hat{q_j } $ of the ability variable from a separate dataset $\{y_{ij}\}$ (different items, same persons) on which cheating was not possible. Specifically, $\hat{q_j} $ are empirical Bayes estimates from the same item response model as above. </p>

<p>The probability of the observed score $x_{ij}$, conditional on item easiness and person ability, can be written $$p_{ij} = Pr(X_{ij} = x_{ij} | \hat{\beta_i }, \hat{q_j }) = P_{ij}(\hat{\beta_i }, \hat{q_j })^{x_{ij}} (1 - P_{ij}(\hat{\beta_i }, \hat{q_j }))^{1-x_{ij}},$$ where $P_{ij}(\hat{\beta_i }, \hat{q_j }) = ilogit(\hat{\beta_i} + \hat{q_j})$ is the predicted probability of a correct response, and $ilogit$ is the inverse logit. Then, conditional on item and person characteristics, the joint probability that person $j$ has the observations $x_j$ is $$p_j = \prod_i p_{ij},$$ and similarly, the joint probability that item $i$ has the observations $x_i$ is $$p_i = \prod_j p_{ij}.$$ Persons with the lowest $p_j$ values are those whose observed scores are conditionally least likely -- they are possibly cheaters. Items with the lowest $p_j$ values are those which are conditionally least likely -- they are the possible leaked/shared items. This approach relies on the assumptions that the models are correct and that person $j$â€™s scores are uncorrelated conditional on person and item characteristics. A violation of the second assumption isnâ€™t problematic though, as long as the degree of correlation does not vary across persons, and the model for $p_{ij}$ could easily be improved (e.g., by adding additional person or item characteristics).</p>

<p>An additional step I tried is to take r% of the least likely persons (i.e. persons with the lowest r% of sorted p_j values), compute the mean distance between their observed scores x_j (which should be correlated for persons with low r, who are possible cheaters), and plot it for r = 0.001, 0.002, ..., 1.000. The mean distance increases for r = 0.001 to r = 0.025, reaches a maximum, and then declines slowly to a minimum at r = 1. Not exactly what I was hoping for. </p>
"
"0.104746297470869","0.0611354359454417","  8152","<p>I have a few questions regarding multiple imputation for nested data. 
Context: I have repeated measures (4 times) from a survey and these are clustered in workplaces (205 workplaces). There are about 180 items on this survey.</p>

<p>q1. Is it possible to take both the repeated measures and the workplace clustering into consideration or do i have to decide for one of the two?</p>

<p>q2. If i can only take into consideration one of the two clusterings (repeated measures vs workplace) which one would you recommend</p>

<p>q3. I have about 10000 observations and about 400 of them have missing values for the workplace. What would you recommend to do in this case? (also i should mention that the 205 workplaces are nested in 17 Organizations - For the moment i use general categories based on the organization: e.g. Organization1-Unclassified). Is there a meaningful way to actually impute these categories? </p>

<p>q4. Would you recommend to use all 180 items for imputation or the items that i intend to use in each of my models? </p>

<p>I use R for analysis and it would be greatly appreciated if you can recommend any packages for multiple imputation for clustered data.</p>

<p>Thanks in advance</p>
"
"0.0395903791232448","0.0539163866017192","  8729","<p>I'm trying to gain a better understanding of kmeans clustering and am still unclear about colinearity and scaling of data. To explore colinearity, I made a plot of all five variables that I am considering shown in the figure below, along with a correlation calculation.
<img src=""http://i.stack.imgur.com/W5MZJ.jpg"" alt=""colinearity""></p>

<p>I started off with a larger number of parameters, and excluded any that had a correlation higher than 0.6 (an assumption I made). The five I choose to include are shown in this diagram.</p>

<p>Then, I scaled the date using the <code>R</code> function <code>scale(x)</code> before applying the <code>kmeans()</code> function. However, I'm not sure whether <code>center = TRUE</code> and <code>scale = TRUE</code> should also be included as I don't understand the differences that these arguments make. (The <code>scale()</code> description is given as <code>scale(x, center = TRUE, scale = TRUE)</code>).</p>

<p>Is the process that I describe an appropriate way of identifying clusters?</p>
"
"0.0559892510955854","0.0381246425831512","  9020","<p>In SPSS, if I use the hierarchical clustering procedure, I have the ability to cluster both variables and cases using a variety of methods and distance measures.  For this task, I would like to use R to cluster my variables.</p>

<p>For context, my data come from a survey and the respondents were able to select multiple items from a block of options. In my datafile the data are coded as 0,1.  </p>

<p>I am trying to learn R so any help you can provide will be greatly appreciated.</p>

<p>Best regards,</p>

<p>Brock</p>
"
"NaN","NaN","  9074","<p>I'm wondering how to implement two-way clustering, as explained in <a href=""http://www.statsoft.com/textbook/cluster-analysis/#twotwo"" rel=""nofollow"">Statistica documentation</a> in R. Any help in this regard will be highly appreciated. Thanks</p>
"
"0.104746297470869","0.0815139145939222","  9182","<p>I have multiple variables (here: weight, horizontal diameter, price and dummy) related to different factors (here: Apple, Orange, Banana and Avocado):</p>

<pre><code>Fruit   Weight      HorDiam     Price       Dummy
Apple   60      60      5       4
Apple   50      70      8       6
Orange  80      75      7       2
Orange  72      70      9       8
Banana  40      30      3       1
Banana  45      35      4       2
Banana  80      50      8       3
Avocado 100     60      13      8
Avocado 95      70      14      6
</code></pre>

<p>I need to test if I can group some species together: are apples and oranges significantly different? ANOVA tells me if weight (or horizontal diameter, or price) is significantly different among species. Tukey test gives me if weight of one species is significantly different from weight of another one (pairwise). Clustering seems only able to group individual observations together, not species. I can't find the appropriate test (or algorithm) to tell me if, for a single variable (weight) or for all of them (weight, horDiam and price), apples can be grouped with oranges and/or with bananas. Any suggestion?</p>

<p>I created a <em>R</em> code for this example:</p>

<pre><code>### CREATE TABLE
Fruit&lt;-c(""Apple"",""Apple"",""Orange"",""Orange"",""Banana"",""Banana"",""Banana"",""Avocado"",""Avocado"")
Weight&lt;-c(60,50,80,72,40,45,85,90,95)
horDiam&lt;-c(60,70,75,70,30,35,50,60,70)
Price&lt;-c(5,8,7,9,3,4,8,13,14)
Dummy&lt;-c(4,6,2,8,1,2,3,8,6)
myData&lt;-data.frame(Fruit=Fruit, Weight=Weight, horDiam=horDiam, Price=Price, Dummy=Dummy)
rownames(myData)&lt;-c(""Apple1"",""Apple2"",""Orange1"",""Orange2"",""Banana1"",""Banana2"",""Banana3"",""Avocado1"",""Avocado2"")


### ANOVA
fit.aov&lt;-list()
summaryAOV&lt;-list()
for (i in 1:3){
  fit.aov[[i]]&lt;-aov(myData[,i+1]~myData[,1])
  summaryAOV[[i]]&lt;-summary(fit.aov[[i]])
}


### TUKEY
par(mfrow=c(1,3))
testTukey&lt;-list()
mainTukey&lt;-c(""Weight"", ""Horiz. Diameter"", ""Price"")
for (i in 1:3){
  testTukey[[i]]&lt;-TukeyHSD(fit.aov[[i]], conf.level = 0.95)
    plot(testTukey[[i]], main=mainTukey[i])
}


### CLUSTERING
plot( hclust(dist(myData), method=""ward"") )


### CLUSTERING WITH P-VALUE
fit &lt;- pvclust(t(myData[,-1]), method.hclust=""ward"", method.dist=""euclidean"")
plot(fit)
pvrect(fit, alpha=0.95)
</code></pre>
"
"0.0395903791232448","0","  9581","<p>I have a huge dataset which contains 20 columns and many rows.
I have done clustering in SAS, Knime and SPSS, but I am new to R.
I have to do clustering on my dataset. 
I have imported my data into R.</p>

<ul>
<li>What are some suggestions for getting started with cluster analysis in R?</li>
</ul>
"
"NaN","NaN","  9656","<p>am writing a simple R script to test the spectral clustering algorithm but for the eigenvalues I don't get them all positive and lambda0 is different from 0. here is my script</p>

<pre><code>Dsqrt&lt;-matrix(0,length(V(g)),length(V(g)))  # The D^-0.5 matrix
   for(i in 1:(length(V(g))-1))
         Dsqrt[i,i]&lt;-1/sqrt(degree(g,V(g)[i-1]))        

   L&lt;-graph.laplacian(g, normalized=TRUE) #Here L[i,i]=1 and L[i,j]= 1/sqrt(d[i]*d[j])
   L&lt;-Dsqrt %*% L %*% Dsqrt  
   Eigns&lt;-eigen(L)
</code></pre>

<p>And when I check <code>Eigns$values</code> sometimes I get some negative values.</p>

<p>Can any one point me my error?</p>

<p>Thanks in advance.</p>
"
"0.0791807582464896","0.0808745799025788","  9739","<p>I have a set of sea surface temperature (SST) monthly data and I want to apply some cluster methodology to detect regions with similar SST patterns. I have a set of monthly data files running from 1985 to 2009 and want to apply clustering to each month as a first step.</p>

<p>Each file contains gridded data for 358416 points where approximately 50% are land and are marked with a 99.99 value that will be NA. Data format is:</p>

<pre><code>   lon     lat   sst
-10.042  44.979  12.38
 -9.998  44.979  12.69
 -9.954  44.979  12.90
 -9.910  44.979  12.90
 -9.866  44.979  12.54
 -9.822  44.979  12.37
 -9.778  44.979  12.37
 -9.734  44.979  12.51
 -9.690  44.979  12.39
 -9.646  44.979  12.36
</code></pre>

<p>I have tried CLARA clustering method and got some apparently nice results but it also seems to me that is just smoothing (grouping) isolines. Then I am not sure this is the best clustering method to analyse spatial data.</p>

<p>Is there any other clustering method devoted to this type of datasets? Some reference would be good to start reading.</p>

<p>Thanks in advance.</p>
"
"0.158361516492979","0.188707353106017"," 10017","<p>I am trying to understand standard error ""clustering"" and how to execute in R (it is trivial in Stata). In R I have been unsuccessful using either <code>plm</code> or writing my own function. I'll use the <code>diamonds</code> data from the <code>ggplot2</code> package.</p>

<p>I can do fixed effects with either dummy variables</p>

<pre><code>&gt; library(plyr)
&gt; library(ggplot2)
&gt; library(lmtest)
&gt; library(sandwich)
&gt; # with dummies to create fixed effects
&gt; fe.lsdv &lt;- lm(price ~ carat + factor(cut) + 0, data = diamonds)
&gt; ct.lsdv &lt;- coeftest(fe.lsdv, vcov. = vcovHC)
&gt; ct.lsdv

t test of coefficients:

                      Estimate Std. Error  t value  Pr(&gt;|t|)    
carat                 7871.082     24.892  316.207 &lt; 2.2e-16 ***
factor(cut)Fair      -3875.470     51.190  -75.707 &lt; 2.2e-16 ***
factor(cut)Good      -2755.138     26.570 -103.692 &lt; 2.2e-16 ***
factor(cut)Very Good -2365.334     20.548 -115.111 &lt; 2.2e-16 ***
factor(cut)Premium   -2436.393     21.172 -115.075 &lt; 2.2e-16 ***
factor(cut)Ideal     -2074.546     16.092 -128.920 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>or by de-meaning both left- and right-hand sides (no time invariant regressors here) and correcting degrees of freedom.</p>

<pre><code>&gt; # by demeaning with degrees of freedom correction
&gt; diamonds &lt;- ddply(diamonds, .(cut), transform, price.dm = price - mean(price), carat.dm = carat  .... [TRUNCATED] 
&gt; fe.dm &lt;- lm(price.dm ~ carat.dm + 0, data = diamonds)
&gt; ct.dm &lt;- coeftest(fe.dm, vcov. = vcovHC, df = nrow(diamonds) - 1 - 5)
&gt; ct.dm

t test of coefficients:

         Estimate Std. Error t value  Pr(&gt;|t|)    
carat.dm 7871.082     24.888  316.26 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>I can't replicate these results with <code>plm</code>, because I don't have a ""time"" index (i.e., this isn't really a panel, just clusters that could have a common bias in their error terms).</p>

<pre><code>&gt; plm.temp &lt;- plm(price ~ carat, data = diamonds, index = ""cut"")
duplicate couples (time-id)
Error in pdim.default(index[[1]], index[[2]]) : 
</code></pre>

<p>I also tried to code my own covariance matrix with clustered standard error using Stata's explanation of their <code>cluster</code> option (<a href=""http://www.stata.com/support/faqs/stat/cluster.html"">explained here</a>), which is to solve $$\hat V_{cluster} = (X&#39;X)^{-1} \left( \sum_{j=1}^{n_c} u_j&#39;u_j \right) (X&#39;X)^{-1}$$ where $u_j = \sum_{cluster~j} e_i * x_i$, $n_c$ si the number of clusters, $e_i$ is the residual for the $i^{th}$ observation and $x_i$ is the row vector of predictors, including the constant (this also appears as equation (7.22) in Wooldridge's <em>Cross Section and Panel Data</em>). But the following code gives very large covariance matrices. Are these very large values given the small number of clusters I have? Given that I can't get <code>plm</code> to do clusters on one factor, I'm not sure how to benchmark my code.</p>

<pre><code>&gt; # with cluster robust se
&gt; lm.temp &lt;- lm(price ~ carat + factor(cut) + 0, data = diamonds)
&gt; 
&gt; # using the model that Stata uses
&gt; stata.clustering &lt;- function(x, clu, res) {
+     x &lt;- as.matrix(x)
+     clu &lt;- as.vector(clu)
+     res &lt;- as.vector(res)
+     fac &lt;- unique(clu)
+     num.fac &lt;- length(fac)
+     num.reg &lt;- ncol(x)
+     u &lt;- matrix(NA, nrow = num.fac, ncol = num.reg)
+     meat &lt;- matrix(NA, nrow = num.reg, ncol = num.reg)
+     
+     # outer terms (X'X)^-1
+     outer &lt;- solve(t(x) %*% x)
+ 
+     # inner term sum_j u_j'u_j where u_j = sum_i e_i * x_i
+     for (i in seq(num.fac)) {
+         index.loop &lt;- clu == fac[i]
+         res.loop &lt;- res[index.loop]
+         x.loop &lt;- x[clu == fac[i], ]
+         u[i, ] &lt;- as.vector(colSums(res.loop * x.loop))
+     }
+     inner &lt;- t(u) %*% u
+ 
+     # 
+     V &lt;- outer %*% inner %*% outer
+     return(V)
+ }
&gt; x.temp &lt;- data.frame(const = 1, diamonds[, ""carat""])
&gt; summary(lm.temp)

Call:
lm(formula = price ~ carat + factor(cut) + 0, data = diamonds)

Residuals:
     Min       1Q   Median       3Q      Max 
-17540.7   -791.6    -37.6    522.1  12721.4 

Coefficients:
                     Estimate Std. Error t value Pr(&gt;|t|)    
carat                 7871.08      13.98   563.0   &lt;2e-16 ***
factor(cut)Fair      -3875.47      40.41   -95.9   &lt;2e-16 ***
factor(cut)Good      -2755.14      24.63  -111.9   &lt;2e-16 ***
factor(cut)Very Good -2365.33      17.78  -133.0   &lt;2e-16 ***
factor(cut)Premium   -2436.39      17.92  -136.0   &lt;2e-16 ***
factor(cut)Ideal     -2074.55      14.23  -145.8   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 1511 on 53934 degrees of freedom
Multiple R-squared: 0.9272, Adjusted R-squared: 0.9272 
F-statistic: 1.145e+05 on 6 and 53934 DF,  p-value: &lt; 2.2e-16 

&gt; stata.clustering(x = x.temp, clu = diamonds$cut, res = lm.temp$residuals)
                        const diamonds....carat..
const                11352.64           -14227.44
diamonds....carat.. -14227.44            17830.22
</code></pre>

<p>Can this be done in R? It is a fairly common technique in econometrics (there's a brief tutorial in <a href=""http://sekhon.berkeley.edu/causalinf/sp2010/section/week7.pdf"">this lecture</a>), but I can't figure it out in R. Thanks!</p>
"
"NaN","NaN"," 10347","<p>I have made a heatmap based upon a regular data matrix in R, the package I use is <code>pheatmap</code>. Regular clustering of my samples is performed by the <code>distfun</code> function within the package. </p>

<p>Now I want to attach a precomputed distance matrix (generated by Unifrac) to my previously generated matrix/heatmap. Is this possible?</p>
"
"0.118771137369734","0.107832773203438"," 10411","<p>I have a few datasets of ""interactions"" between pairs of elements like so:</p>

<pre><code>element1 element2 1
element2 element3 1
element4 element5 1
...
element505535 element4 2
</code></pre>

<p>where the value in the 3rd column is the ""strength"" of interaction. Almost all of these strengths are ""1."" A strength of 1 means that this interaction was observed one time. A strength of 2 means 2x, etc. I have actually gone one step further and normalized all of my datasets by the total # of interactions observed in the dataset so that datasets interaction values can be compared.</p>

<p>There are 5-6 million interactions listed in each file and each dataset is obviously under-sampled since there are ~500k elements (making a square matrix of ~250 trillion positions).</p>

<p>I would like to cluster these datasets so that I can make statements about which types of elements tend to cluster with which other types elements. Obviously, robusticity of clustering will be a factorâ€”but this is partially ameliorated by the fact that I will make biological replicates of the data.</p>

<p>I have tried a few different ""naive"" clustering approaches just to see what I could do easily with the data. I fully realize that these are problematic ways of clustering, either because they are not robust or because they rely on the data being very undersampled, but here is what I've done:</p>

<ol>
<li><p>Clustering elements together as long as there is at least one interaction between each element in the cluster and at least one other element in the cluster. When I do this, all the elements end up in a single cluster. This was important to do because it tells me that there are no pairs of elements that are totally isolated from the rest of the group.</p></li>
<li><p>Finding ""superclusters""â€”that is, clusters where every member of the cluster interacts with every other member of the cluster (e.g. a triangle for a cluster of 3 and a box with an X in the middle for a cluster of 4, etc). This yields almost exclusively clusters with 2 and 3 elements after about 10% of the data has been analyzed (this is still running).</p></li>
</ol>

<p>I would love to be able to do some sort of hierarchical clustering using my ""interaction strength"" values as the distance measure between each pair of elements (unobserved interactions have a strength of 0). Does anyone know of a way to do HC on this sort of large, sparse dataâ€”or know of a clustering method that might be more appropriate? I've used R up until now.</p>
"
"0.0885267789745639","0.0964485644340824"," 11418","<p>this question started as ""<a href=""http://stats.stackexchange.com/questions/9739/clustering-spatial-data-in-r"">Clustering spatial data in R</a>"" and now has moved to DBSCAN question.</p>

<p>As the responses to the first question suggested I searched information about DBSCAN and read some docs about. New questions have arisen.</p>

<p>DBSCAN requires some parameters, one of them is ""distance"". As my data are three dimensional, longitude, latitude and temperature, which ""distance"" should I use? which dimension is related to that distance? I suposse it should be temperature. How do I  find such minimum distance with R?  </p>

<p>Another parameter is the minimum number of points neded to form a cluster. Is there any method to find that number? Unfortunately I haven't found. </p>

<p>Searching thorugh Google I could not find an R example for using dbscan in a dataset similar to mine, do you know any website with such kind of examples? So I can read and try to adapt to my case. </p>

<p>The last question is that my first R attempt with DBSCAN (without a proper answer to the prior questions) resulted in a memory problem. R says it can not allocate vector. I start with a 4 km spaced grid with 779191 points that ends in approximately 300000 rows x 3 columns (latitude, longitude and temperature) when removing not valid SST points. Any hint to address this memory problem. Does it depend on my computer or in DBSCAN itself?</p>

<p>Thanks for the patience to read a long and probably boring message and for your help.</p>
"
"0.0885267789745639","0.0723364233255618"," 12580","<p>I used the following example code from <strong>latticeExtra</strong> to understand two-way clustering in <em>R</em></p>

<pre><code>library(latticeExtra)
data(mtcars)
x  &lt;- t(as.matrix(scale(mtcars)))
dd.row &lt;- as.dendrogram(hclust(dist(x)))
row.ord &lt;- order.dendrogram(dd.row)

dd.col &lt;- as.dendrogram(hclust(dist(t(x))))
col.ord &lt;- order.dendrogram(dd.col)

library(lattice)

levelplot(x[row.ord, col.ord],
      aspect = ""fill"",
      scales = list(x = list(rot = 90)),
      colorkey = list(space = ""left""),
      legend =
      list(right =
           list(fun = dendrogramGrob,
                args =
                list(x = dd.col, ord = col.ord,
                     side = ""right"",
                     size = 10)),
           top =
           list(fun = dendrogramGrob,
                args =
                list(x = dd.row,
                     side = ""top"",
                     size = 10))))
</code></pre>

<p>and this is what I got</p>

<p><img src=""http://i.stack.imgur.com/zxWV1.png"" alt=""enter image description here""></p>

<p>Joining of both row and column entities make sense to me but I'm confused with different color shades of heatmap.</p>

<p><strong>Questions</strong></p>

<ul>
<li>Do the Joining of row variables also take into account the column variables and vice versa</li>
<li>What does mean the different colors in heatmap for different row variables clustering as well as for column variables clustering. Specifically focus on <strong>cyl</strong> and <strong>disp</strong> row variables.</li>
</ul>
"
"0.137145096264748","0.0933859209547035"," 13578","<p>I'm using <code>heatmap.2</code> to cluster my data, using the centroid method for clustering and the maximum method for calculating the distance matrix:</p>

<pre><code>library(""gplots"")
library(""RColorBrewer"")

test           &lt;- matrix(c(0.96, 0.07, 0.97, 0.98, 
                           0.50, 0.28, 0.29, 0.77, 
                           0.08, 0.96, 0.51, 0.51, 
                           0.14, 0.19, 0.41, 0.51), ncol=4, byrow=TRUE)
colnames(test) &lt;- c(""Exp1"",""Exp2"",""Exp3"",""Exp4"")
rownames(test) &lt;- c(""Gene1"",""Gene2"",""Gene3"", ""Gene4"")
test           &lt;- as.table(test)
mat            &lt;- data.matrix(test)

heatmap.2(mat, dendrogram=""row"", Rowv=TRUE, Colv=FALSE, 
          distfun=function(x) dist(x, method='maximum'),
          hclustfun=function(x) hclust(x, method='centroid'),
          xlab=NULL, ylab=NULL, key=TRUE, keysize=1, trace=""none"", 
          density.info=c(""none""), margins=c(6, 12), col=bluered)
</code></pre>

<p>This gives a heatmap with inversions in the cluster tree, which is inherent to the centroid method. A solution to avoid inversions is to use the Euclidean or the city-block distance, and indeed if you change maximum to Euclidean in the above example the inversions are gone (for reference see chapter 4.1.1 in <a href=""http://bonsai.hgc.jp/~mdehoon/software/cluster/manual/Hierarchical.html"" rel=""nofollow"">this link</a>).</p>

<p>Now as for my problem, when I use my actual data instead of this example table the inversions are still there when I change to Euclidean. The R code is exactly the same as in this example, only the data is different. When I use <code>cluster 3.0</code> and <code>java treeview</code> with the Euclidean and centroid method there are no inversions in my data as expected. So why does R give inversions? The theory and other software says it shouldn't.</p>

<p><em><strong>Update:</em></strong> This is an example were changing maximum to Euclidean does not fix inversions (as opposed to the above example were it did fix it)</p>

<pre><code>library(""gplots"")
library(""RColorBrewer"")

test           &lt;- matrix(c(0.96, 0.07, 0.97, 0.98, 0.99, 0.50, 
                           0.28, 0.29, 0.77, 0.78, 0.08, 0.96, 
                           0.51, 0.51, 0.55, 0.14, 0.19, 0.41, 
                           0.51, 0.40, 0.97, 0.98, 0.99, 0.50, 
                           0.28                               ), ncol=6, byrow=TRUE)
colnames(test) &lt;- c(""Exp1"", ""Exp2"", ""Exp3"", ""Exp4"", ""Exp5"", ""Exp6"")
rownames(test) &lt;- c(""Gene1"", ""Gene2"", ""Gene3"", ""Gene4"")
test           &lt;- as.table(test)
mat            &lt;- data.matrix(test)

heatmap.2(mat, dendrogram=""row"", Rowv=TRUE, Colv=FALSE, 
          distfun=function(x) dist(x, method='maximum'),
          hclustfun=function(x) hclust(x, method='centroid'),
          xlab=NULL, ylab=NULL, key=TRUE, keysize=1, trace=""none"", 
          density.info=c(""none""), margins=c(6, 12), col=bluered)
</code></pre>
"
"0.111978502191171","0.133436249041029"," 13857","<p>I have a huge matrix (individuals X features with row.names as individuals numbers) and the corresponding segment in another vector of 1D (row.names are the same as in my huge matrix and the vector represent the segments associated).
I.E. :</p>

<pre><code>row.names VAR1 VAR2 VAR3 VAR4 â€¦ VAR3000
    12     4    12    5   18      8
    58     6    13    19   3     10
</code></pre>

<p>for the huge matrix.
and:</p>

<pre><code>row.names  x
    12     4
    58     2
</code></pre>

<p>for the segment representation (where x represent the individual' segment).</p>

<p>I have no a priori model and I want to select a subset of variables (variable/feature selection) in order to predict the segment using a minimal subset of variables. I didn't use biclustering technique to detect my classes but a simple-way one. Which technique would you recommend to :</p>

<ol>
<li>select the most discriminative variables (lasso, elastic net) and why?</li>
<li>predict the segment from these variables.</li>
<li>predict multiple values in another similar matrix (same individuals, few predictors that have been selected). Is it possible in this case to use correlation matrix (or cov) to infer directly the values of predictors that are not known in the other matrix (not using the following method: predict the class, then fill the missing values with medoid values or cluster-mean values)?</li>
</ol>

<p>Thanks in advance.</p>
"
"0.0791807582464896","0.0539163866017192"," 13995","<p>I have a very large (36k items) spatial dataset of locations of commercial landuses with their corresponding square footages. I am hoping to use the <code>pam()</code> command in R (from {cluster} package) to form clusters around a set of centers determined by other methods.</p>

<p>I am trying to figure out how to weight the individual points such that large square footages have more attraction to other point than small square footages. My initial thought was to duplicate each point once per 1000 square feet, such that a 
100,000 square foot point would be duplicated 100 times. However, I've read elsewhere that the clustering algorithms are computationally intense - the package documentation suggests using <code>clara()</code> for large datasets, but this method won't allow me to specify the medoids beforehand.</p>

<p>Is there another method for weighted clustering? Am I perhaps going though this all wrong?</p>
"
"0.153332879014365","0.139211511597426"," 14051","<p>Thanks for reading my question.</p>

<p>I have several thousand data points scattered on an (x,y) grid that I am trying to cluster.  The data points are not uniformly distributed across the grid, but are concentrated in certain areas.  I am most interested in identifying the centers of the clusters as representing starting points that minimize the average (Euclidean) distance from a point to the nearest cluster center.</p>

<p>Depending on the specific model and data set, there are between 3 and 7 clusters.  The number of clusters is known beforehand in each instance, and does not need to be determined by an algorithm.  In each situation, some of the centers of the clusters are known (0 to 4 known starting points), but the rest are unknown.  The goal is to identify the centers of the unknown clusters that minimize the average distance to a center across all the data points.</p>

<p>How can I run a clustering algorithm where I can specify a certain number of cluster centers, and solve for the others?  I am using R, and have looked primarily at package mclust.  My thought was that specifying priors for mean and scale to Mclust with very small scale variances for the known centers and very large scale variances (uninformative prior) for the unknowns would be a good approach, but I am having trouble coding it.  The available examples for specifying priors in the package documentation aren't terribly helpful (to me), and might be used for a completely different purpose than what I'm trying to do.</p>

<p>My attempt to code this in R looks something like:</p>

<pre><code># create data matrix of points
x &lt;- rnorm(100, 50, 25)
y &lt;- rnorm(100, 50, 25)
my.data &lt;- cbind(x,y)

# Two known centers, rest are unknown so provide mean of x,y as default starting point
#
# known centers are (25, 10) and (90, 65), assume midpoints of grid for others
x.prior.mean &lt;- c(25, 90, 50, 50, 50)
y.prior.mean &lt;- c(10, 65, 50, 50, 50)

# Provide small scale (variance) for known centers, large scale for unknown centers (uninformative prior)
x.prior.scale &lt;- c( 0.1, 0.1, 100, 100, 100)
y.prior.scale &lt;- c( 0.1, 0.1, 100, 100, 100)

# Create a cluster model with no prior specified
my.clust.noprior &lt;- Mclust(data=my.data, G=5)

# Now add a prior for mean
my.clust.prior &lt;-   Mclust(data=my.data, G=5, prior=priorControl(mean=cbind(x=x.prior.mean, y=y.prior.mean)))

# Compare what I think are the centers of the clusters (mean of parameters).
# The centers in the prior-specified case don't seem to reflect the known centers
my.clust.noprior$parameters$mean
my.clust.prior$parameters$mean

# Commented out, but attempting the following statement that adds scale parameter yields an error:
#    Error in chol.default(priorParams$scale) : non-square matrix in 'chol'
#
# my.clust &lt;-   Mclust(data=my.data, G=5, prior=priorControl(mean=cbind(x=x.prior.mean, y=y.prior.mean), scale=cbind(x.prior.scale, y.prior.scale)))
</code></pre>

<p>Is there a way to accomplish what I'm trying to do?  I am open to using other R packages besides mclust if there's one better suited for this problem.</p>

<p>Thank you</p>
"
"0.0791807582464896","0.0808745799025788"," 14539","<p>I am using R for a simple hierarchical clustering method for finding protein sequence similarities. I already have a distance matrix computed. I am using the <code>hclust</code> method to run it (thus I dont want to use <code>dist</code> method). Could anyone tell me what format does my distance matrix need to be for <code>hclust</code> to run?</p>

<p>I used the <code>read.xls</code> method to read my Excel file. I need only the lower triangular portion of the matrix, and this is what I am sending to <code>hclust</code> to be clustered.</p>

<pre><code>d= as.matrix(mat)
hc =hclust(d, method=""single"")
</code></pre>

<p>However this is the error I get:</p>

<pre><code>Error in if (n &lt; 2) stop(""must have n &gt;= 2 objects to cluster"") : 
argument is of length zero
</code></pre>
"
"0.0559892510955854","0.0381246425831512"," 14588","<p>Please I am about to cluster some data based  which have about 15 different columns all of which are numbers(Some categorical while some are measurements) also some of my values are missing in some columns . Please can you give me pointer on how to go about it.</p>

<p>I have previously explored the clustering with weka but I am not sure about the way weka implements so I am going the R route.</p>

<p>What I know : I already know about Principal components analysis at least in theory. But is this necessary in all clustering of multiple columns . It will go a long way if anyone could provide me a link to a tutorial on this because Quick-R has for just 2 variables.</p>

<p>A sample of my dataset is listed below</p>

<pre><code>1,64,9,30,33,2,3,1,6,1,5,-3.62,-3.71,-2.73,1
2,61,4,30,33,2,3,2,7,4,4,-3.62,-3.71,-2.00,1
3,49,4,18,21,2,3,2,8,17,18,-3.68,-3.88,-2.00,1
4,40,4,10,12,2,2,2,24,20,23,-3.32,-3.42,-2.00,1
5,43,9,10,12,2,2,1,2,1,29,-3.12,-3.19,-2.73,1
6,52,9,16,19,2,3,2,35,34,35,-3.33,-3.26,-2.95,1
7,46,4,15,18,2,3,2,8,40,42,-3.59,-3.50,-2.00,1
8,40,4,10,12,2,2,2,24,20,46,-2.45,-2.69,-2.00,1
</code></pre>
"
"0.0395903791232448","0"," 15047","<p>IÂ´m using the <code>pam()</code> R function to perform clustering. As far as I know, the <code>pamk()</code> function serves as a wrapper to <code>pam()</code>, and evaluates the optimal number of clusters. However, using the same data and parameters I get different results.</p>

<p>For example, calling <code>pamk()</code> and <code>pam()</code> as follows returns 2 clusters with different medoids values:</p>

<pre><code>pk &lt;- pamk(dist, krange=2:10, criterion=""ch"", usepam=TRUE, diss=TRUE)

pk.2 &lt;- pam(dist,2,diss=TRUE)
</code></pre>

<p>How can it be?</p>

<p>Thank you,
Anat</p>
"
"0.0559892510955854","0.0381246425831512"," 15548","<p>I have a data which  contains several columns which I later reduced using a PCA algorithms to two different components. I then applied the k-means algorithms to the data.<br>
Now, how can I verify that my data  clustered  well into each group? Or how do I determine misclassification rate?</p>

<p>For instance, using R, if I check the cluster vector say k$cluster against the labels of the data I had previously before clustering  can I just draw a confusion matrix from that and assume that 1 in the clustered vector is equivalent to 1 in my labels?</p>

<pre><code>col3    col2     Col1   lables                                           
123     2.32      2.50    0           
124    2.81      3.10     1     
125    2.72      3.09     2     
126    2.92      3.03     3     
127    2.32      2.95     4     
</code></pre>

<p>Please note this is a hypothetical data; my data is way bigger than this.</p>
"
"0.0885267789745639","0.0964485644340824"," 15839","<p>I am interested in determining the optimal number of clusters calculated by the PAM clustering algorithm using the Calinski-Harabasz (CH) index. To that end, I found 2 different R functions calculating CH values for a given clustering, but which returned different results: <a href=""http://rss.acs.unt.edu/Rdoc/library/fpc/html/cluster.stats.html"" rel=""nofollow"">?cluster.stats</a> (in the <a href=""http://cran.r-project.org/web/packages/fpc/index.html"" rel=""nofollow"">fpc package</a>), and <a href=""http://rss.acs.unt.edu/Rdoc/library/clusterSim/html/index.G1.html"" rel=""nofollow"">?index.G1</a> (in the <a href=""http://cran.r-project.org/web/packages/clusterSim/index.html"" rel=""nofollow"">clusterSim package</a>).</p>

<p>First one is called via:</p>

<pre><code>pam.res &lt;- pam(dist.matrix, 2, diss=TRUE)
ch1     &lt;- cluster.stats(dist.matrix, pam.res$clustering, silhouette=TRUE)$ch
</code></pre>

<p>Second one is called via:</p>

<pre><code>ch2 &lt;- index.G1(t(dataframe), pam.res$clustering, d=dist.matrix)
</code></pre>

<p>Data may be found here: <a href=""http://www.megafileupload.com/en/file/327255/dataframe-RData.html"" rel=""nofollow"">dataframe.RData</a>, or here: <a href=""http://www.megafileupload.com/en/file/327262/dist-matrix-RData.html"" rel=""nofollow"">dist.matrix.RData</a> [dead links].</p>

<ul>
<li><p><strong>Can anybody explain the difference between these two CH index calculations to me?</strong></p>

<p>Using <code>cluster.stats()</code>, the highest CH index is obtained for 2 clusters ($\approx32$); while using <code>index.G1()</code>, the highest CH index is obtained for 3 clusters ($\approx60$, and the value for 2 clusters is totally different from the previous, $\approx54$).</p></li>
<li><p><strong>Which function is normally used to calculate the CH index?</strong></p></li>
</ul>
"
"0.111978502191171","0.0953116064578779"," 16137","<p>Are there any tools in R that could be used to optimize the allocation of customers amongst possible offers, given constraints? Can anyone give hints/examples on their use? Hope my setup makes sense...</p>

<p>Here is the problem setup:</p>

<p><strong>There are the following:</strong></p>

<ul>
<li>$N$ customers ($N$ is large)</li>
<li>$F$ offers (the offers that can be made to a customer;  $F$ is relatively small)</li>
<li>$P_{nf}$ -- the probability of acceptance of offer $f$ by customer $n$</li>
<li>$D_{nf}$ -- the expected monetary value if customer $n$ accepts offer $f$</li>
<li>$C_f$ -- the cost of offering offer $f$ to any customer</li>
<li>$E_{nf}$ -- the expected profit of offering offer $f$ to customer $n$ ($P_{nf} D_{nf} - C_{f}$)</li>
</ul>

<p><strong>Constraints:</strong></p>

<ul>
<li>Each customer can be allocated to only 1 offer (not every customer need receive anything).</li>
<li>The total number of offers made (call it $T$) between a and b. </li>
<li>The total cost $TC&lt;c$.</li>
</ul>

<p>The percentage of $T$ comprised by each offer $f$ is $\geq d$. This means that sometimes an offer has to be made at least $d$ times. There is one of these rules for each offer.</p>

<p><strong>Goal:</strong></p>

<ul>
<li>Maximize profit.</li>
</ul>

<p>Anything in R?</p>

<p><strong>EDIT:</strong></p>

<ul>
<li><p>I wonder about using something along the lines of <a href=""http://cran.r-project.org/web/packages/Rglpk/index.html"" rel=""nofollow"">http://cran.r-project.org/web/packages/Rglpk/index.html</a>
which appears to support ""large"" problems and the types of constraints I have.
Of course, ""large"" in the context appears to be much less than the millions for N I have. </p></li>
<li><p>One thought I had was to caculate the expected profit for each customer and each offer. Then, run a clustering algorithm (row = customers and columns = expected profit for each promotion) like k-means with k large (e.g. 1,000). Then assign each of the customers into a cluster and use the cluster centroid as the value of the expected profit for the optimizer.</p></li>
</ul>

<p><strong>EDIT AGAIN</strong></p>

<p>For the sake of helping others, the conclusion I came to was to indeed cluster the customers and then use a standard linear solver (I got lpSolve in R to work well). </p>

<p>The other option is to use a non linear approximation. Robert Agnew helped me tremendously on this question - using his dual formulation. See this <a href=""http://www.r-bloggers.com/marketing-optimization-using-the-nonlinear-minimization-function-nlm/"" rel=""nofollow"">post</a>.  His R script is also linked and works great - changing from equality constraints for the offer quantity to inequality constraints requires use of nlminb(). </p>
"
"0.0559892510955854","0.0381246425831512"," 16855","<p>I am conducting a simulation study, which involves several clustering methods such as model-based clustering method (MBCM). However, there is a big problem I find that MBCM runs so slowly (might be due to the EM algorithm).  I have tried my best to avoid for loop in my simulation setting but it still takes me forever. For example, I simulate 100 datasets and each data set contains 1000 subjects. This tiny simulation took me around <strong>879</strong> minutes to complete. I can't image how much time will take me to complete 1000 simulated datasets. Does anyone have a better idea to speed up the clustering analysis in R? </p>

<p>many many thanks
Tu</p>
"
"NaN","NaN"," 17105","<p>I'd like to ask a question here that I've also asked on Biostar (stackexchange) and someone there forwarded me to this website. I was wondering how I could perform a Bray Curtis similarity clustering in R in which I show the similarity percentages on an inverted Y-axis and all tree nodes ending at 100% as I've shown in a dendogram:
<img src=""http://i.stack.imgur.com/rkwcT.png"" alt=""enter image description here""></p>

<p>At the moment I create my plot in the following way (using S17 Bray Curtis dissimilarity measure, which just scales regular Bray Curtis to 0-100%):</p>

<pre><code>library(vegan)
mat = 'some matrix'
d = (1 - vegdist(mat, method=""bray"")) * 100 
h = hclust(d)
plot(h)
</code></pre>

<p>Inverting the Y-axis (with <code>ylim=c(100,80)</code>) doesn't work. How can I create a dendogram as shown above from a distance matrix? Thanks for any help / advice!</p>

<p>Original question can be found on the <a href=""http://biostar.stackexchange.com/questions/13155/r-cluster-similarity-percentages-with-inverted-y-axis-example-included"" rel=""nofollow"">Biostar website here</a></p>
"
"0.0395903791232448","0"," 18493","<p>I'm working on improving a random forest model.  I've done some clustering of the data using the <code>pvclust</code> package in R.  I use ward and euclidean distances.  My question is how do I go from the results of the clustering analysis to a better random forest.  Do I need to run multiple <code>rf</code> objects with data split along the edges defined in the clustering results or is it something else.  Any suggestions would be appreciated.  Most of my work is in R.</p>
"
"0.0791807582464896","0.0539163866017192"," 18616","<p>I am going to be using R for text analysis (mostly clustering, classification and some visualization) and was wondering what mechanisms R provides for handling high dimensional, sparse data sets. If I understand correctly, R does provide some packages (e.g., <a href=""http://cran.r-project.org/web/packages/Matrix/"">matrix library</a>) for handling large and sparse matrices - which brings me to my question. </p>

<p>Specifically, I would like to know:</p>

<ol>
<li><p>Which R libraries are most appropriate for storing and processing high dimensional sparse data? Just FYI, my data will fit into memory. </p></li>
<li><p>Do such libraries inter-operate with existing text analysis (clustering/classification) packages? Would I need to convert these sparse data structures to and from data frames if I need to text analysis? Wouldn't that add additional time overhead to the computations?  </p></li>
</ol>

<p>I am fairly new to R, so please excuse me if this sounds vague (or too general). </p>
"
"0.104746297470869","0.0611354359454417"," 18969","<p>I want to use model-based clustering to classify 1,225 time series (24 periods each). I have decomposed these time series using the fast Fourier transform and selected the harmonics that explain at least a threshold percentage of time series variance for all time series in the sample. I want to do model-based clustering on the real and imaginary parts for each transform element of a give time series because it would potentially save me from having to account for temporal autocorrelation in model based clustering across periods of a time series. I know that each complex element of the fast Fourier transform is independent from other elements, but I do not know if the imaginary and real parts of the output for a given output element are independent. I would like to know because if they were, it would allow me to maintain the default assumption of the Mclust package in R for model-based clustering that the variables analyzed have a multivariate Gaussian distribution.</p>

<p>NOTE: The input is real-valued, and I have converted from a two-sided to a one-sided spectrum by removing redundant frequency elements and multiplying the positive frequencies (other than the mean component) by two per the advice I got from another StackOverflow answer here: <a href=""http://stackoverflow.com/questions/8264530/how-do-i-calculate-amplitude-and-phase-angle-of-fft-output-from-real-valued-in"">http://stackoverflow.com/questions/8264530/how-do-i-calculate-amplitude-and-phase-angle-of-fft-output-from-real-valued-in</a></p>
"
"0.104746297470869","0.0407569572969611"," 20673","<p>We ask a set of users to independently detect and annotate all the buildings on a map.
We do not have a priori knowledge about the location or event the existence of buildings on this map.</p>

<p>I would like to aggregate/cluster their annotations in a way that only 'consensual' annotations are taking into account. That means that:</p>

<ul>
<li>I do not know a priori the number of clusters /object on the map (unsupervised approach)</li>
<li>each point in a cluster should belong to a different user (democratic clustering constraint) i.e. a cluster is formed only if the candidate points, spatially enough close, are from several users. with for instance a threshold (if 60% of the users annotate the same area, we consider it as a building)</li>
</ul>

<p>Which (modified) clustering algorithm will be appropriated for such task? (I work with R)?</p>

<p>bonus  </p>

<ul>
<li>The density of the objects on the map is changing. one map could have a dense area of buildings + sparse subareas of buildings). how to choose dynamically the distance between 2 points to consider they are enough close?</li>
</ul>

<hr>

<p>Tried algorithm:</p>

<p>A density based algorithm (DBSCAN)
but:
- it does not take into account the democratic constraint in the formation of the cluster. - there is a problem to handle the change of spatial density on a map ( dense area  of building + sparse area of building on the same map)</p>

<p>Thanks!</p>
"
"0.0559892510955854","0.0381246425831512"," 21421","<p>I have a set of about ten questions that I would like to use to create groupings from. The responses are all dichotomous (responses are in the form of 1 or 2 where 1 and 2 represent differences in preference discovered through qualitative research).  </p>

<p>The questions, were provided in the form: </p>

<pre><code>Which describes you best:
1. I prefer apples
2. I prefer pears
</code></pre>

<p>So far, I've looked at latent class analysis (challenging to interpret and not consistently reproducible), linear discriminant analysis, kmeans clustering (not consistently reproducible), and multiple correspondence analysis (MCA provided the most interpret-able results, but I'm unclear IF or how one could classify respondents using the results). </p>

<p>What would be the most reasonable clustering method to use?</p>

<p>If you want to play around with my data feel free to do so (n=799): </p>

<pre><code>dat &lt;- read.csv(""http://www.bertelsen.ca/media/stackoverflow/cluster.csv"")
</code></pre>
"
"0.0685725481323742","0.062257280636469"," 21685","<p>I am trying to get to grips with Clustering and Visualisation.</p>

<p>I have a set of data (a matrix) that I want to cluster (using R) and then visualise (HTML5 Canvas).  </p>

<p>So, I can use MDS to get the coordinates of a matrix, for example:</p>

<pre><code>cells &lt;- c(1, 1, 2, 1, 4, 3, 5, 4)
rnames &lt;- c(""A"", ""B"", ""C"", ""D"")
cnames &lt;- c(""X"", ""Y"")
x &lt;- matrix(cells, nrow=4, ncol=2, byrow=TRUE, dimnames=list(rnames, cnames))
d &lt;-dist(x)
m &lt;- cmdscale(d,eig=TRUE, k=3)
m &lt;- cmdscale(d,eig=TRUE, k=2)
print(m)
</code></pre>

<p>But, should I first cluster this data (using something like k-means) or should I cluster the output of MDS?  Or can I just cluster (and get the coordinates using some other method) and ignore MDS?</p>

<p>What is the relationship between MDS and K-means, if any?</p>

<p>I am unsure what is the best way to approach?</p>

<p>Any suggestions? </p>

<p>Thanks,</p>

<p>s</p>
"
"0.0559892510955854","0.0762492851663023"," 21718","<p>Given a data matrix like this:</p>

<pre><code>        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]
 [1,]  9.520 11.137 16.576 18.225 20.576 25.861     NA
 [2,]  9.005  9.491 11.106 16.530 18.184 20.495 25.773
 [3,]  9.437 11.050 20.393 25.711     NA     NA     NA
 [4,]  9.442 11.058 20.411 25.711     NA     NA     NA
 [5,]  9.431 11.045 20.421 25.707     NA     NA     NA
 [6,]  9.461 11.052 20.319 25.657     NA     NA     NA
 [7,]  9.245 10.819 20.253 25.628     NA     NA     NA
 [8,]  9.229 10.801 20.216 25.594     NA     NA     NA
 [9,]  9.234 10.805 20.258 25.619     NA     NA     NA
[10,]  9.241 10.814 20.264 25.626     NA     NA     NA
[11,]  9.248 10.819 20.281 25.649     NA     NA     NA
[12,]  9.231 10.800 20.219 25.567     NA     NA     NA
</code></pre>

<p>How do I get a data frame like this?:</p>

<pre><code> S     p1     p2     p3     p4     p5     p6     p7
 1     NA  9.520 11.137 16.576 18.225 20.576 25.861     
 2  9.005  9.491 11.106 16.530 18.184 20.495 25.773
 3     NA  9.437 11.050     NA     NA 20.393 25.711
 4     NA  9.442 11.058     NA     NA 20.411 25.711
 5     NA  9.431 11.045     NA     NA 20.421 25.707
 6     NA  9.461 11.052     NA     NA 20.319 25.657
 7     NA  9.245 10.819     NA     NA 20.253 25.628
 8     NA  9.229 10.801     NA     NA 20.216 25.594
 9     NA  9.234 10.805     NA     NA 20.258 25.619
10     NA  9.241 10.814     NA     NA 20.264 25.626 
11     NA  9.248 10.819     NA     NA 20.281 25.649 
12     NA  9.231 10.800     NA     NA 20.219 25.567 
</code></pre>

<p>In the matrix, each row represent the timepoints from 1 sample. The matrix is balanced with an arbitrary number of NAs as needed. In the dataframe, each column represents all the timepoints that are functionally equivalent. Timepoints in a single sample cannot be functionally equivalent. </p>

<p>I think I should be able to solve this with some form of kmeans clustering that restricts each cluster from having more than one member from each sample.  Any ideas?</p>
"
"0.0885267789745639","0.0964485644340824"," 21791","<p>I wrote a script to do some analysis and it was working fine until I tried to impliment a while loop to find the number clusters appropriate for k-means. For some reason it keeps saying that an argument is of length zero, but there shouldn't be any. I'm running this remotely, and it works fine locally.</p>

<pre><code>freq_1 &lt;- NULL
freq_alignment &lt;- NULL
for (res in point_reference) {
    point &lt;- paste(as.character(res), ""_output.txt"", sep = """")
    point_file &lt;- file(point, ""r"")
    point2 &lt;- read.table(point_file)
    point2 &lt;- as.data.frame(point2)
    k &lt;- 2
    check &lt;- 0.5
    while (check &lt; 0.75) {
            k &lt;- k + 1
            kcluster &lt;- kmeans(point2, k)
            check &lt;- kcluster$betweenss/(kcluster$tot.withins+kcluster$betweenss)
}
    config &lt;- kcluster$cluster
    frames &lt;- length(config)
    freq &lt;- as.data.frame(table(conformations))
    freq_1 &lt;- cbind(freq_1, freq)))
    freq_alignment &lt;- cbind(freq_alignment, kcluster$cluster)
    close(residue_file)
} 
</code></pre>

<p>point_reference is a list of numbers (2, 3, 4, etc.) corresponding to which file to load and the files themselves load fine. My goal is to find the k that corresponds to 75% of the total SS coming from between clusters. Only the loops is wrong...if I replace it with just clustering with k = 5, it works fine. The exact error is:</p>

<p>Error in while (check &lt; 0.75) { : argument is of length zero</p>

<p>Again, I'm doing this remotely on a cluster, and it works on my desktop R64. All files were produced on the cluster. I hope you guys can help! Thank in advance.</p>
"
"0.0969762275752854","0.0880450906325624"," 21955","<p>I am doing an unsupervised clustering analysis for a genomics project. This means that I do not know when a particular clustering analysis is good or not.</p>

<p>I am running different clustering algorithms and different 'sets of features'. What I mean with different 'sets of features' is that given a data frame, I choose different combination of columns depending on its biological importance. For instance, some variables measure things at the sequence level, while others are measuring a particular cellular process or some other feature that cannot be measured at the sequence level. I am playing around with the different outputs of these sets of features, running the algorithms with <em>all</em> the features, or ignoring some, etc .</p>

<p>What I want is to compare the different clusters of these different runs and see if some of my objects are being clustered similarly despite lacking some sets of features. Does this make sense?</p>

<p>Is there any recommendation on how can I do this?</p>
"
"0.0791807582464896","0.107832773203438"," 22073","<p>I would like to find a <em>hierarchical-clustering</em> method useful to assign a group membership into <em>k</em> groups for all individuals in my dataset. I have considered several classic ordination methods, PCA, NMDS, ""mclust"", etc., but three of my variables are categorical (<em>see data description below</em>). Further, I was wondering if it is preferable to a method that reports a posterior probability of group membership for each individual? I am using R.</p>

<p><strong>Data description</strong>: I have sampled almost 2000 individual birds (single species representing two subspecies or phenotypes) across Sweden. All individuals are adult males. Although this is one species, in middle of Sweden there is a (migratory) divide where the southern individuals presumably migrate to West Africa and north of the divide they presumably migrate to East Africa. There is a zone of overlap approximately 300 km wide at the migratory divide.</p>

<p><strong>Variables</strong>:</p>

<ul>
<li>Wing (mm) - continuous</li>
<li>Tail (mm) - continuous</li>
<li>Bill-head (mm) - continuous </li>
<li>Tarsus (mm) - continuous</li>
<li>Mass (g) - continuous</li>
<li>Colour (9 levels) - categorical</li>
<li>Stable carbon-isotopes (parts per mil) - continuous</li>
<li>Stable nitrogen-istopes (parts per mil) - continuous</li>
<li>SNP WW1 (0, 1, 2) - molecular marker, 0 and 2 are fixed and 1 is
heterozygote</li>
<li>SNP WW2 (0, 1, 2) - molecular marker, 0 and 2 are fixed and 1 is
heterozygote</li>
</ul>

<p>Description of the colour variable: (brightest yellow) S+, S, S-, M+, M (medium), M-, N+, N, N- (dullest yellow-grey)</p>
"
"0.0559892510955854","0.0381246425831512"," 24540","<p>I wonder whether it is possible to perform within R a clustering of data having mixed data variables. In other words I have a data set containing both numerical and categorical variables within and I'm finding the best way to cluster them. In SPSS I would use two - step cluster. I wonder whether in R can I find a similar techniques. Thanks in advance. I was told about poLCA package, but I'm not sure...</p>
"
"0.0395903791232448","0"," 25468","<p>I want to know how to to input a self-defined distance in R, in hierarchical clustering analysis. R implements only some default distance metrics, for example ""Euclidean"", ""Manhattan"" etc. Suppose I want to input a self-defined distance '1-cos(x-y)'. Then what should I do?</p>

<p>Writing a function is obviously a solution. But, it will be quite complicated, and also difficult to write. Please help me. I am unable to write the code.</p>
"
"0.0969762275752854","0.0880450906325624"," 26769","<p>I'm attempting to perform hierarchical agglomerative cluster analysis in R. </p>

<p>However, when I use particular clustering methods, I get reversals (upward branching) in the resulting tree, which violates the ultrametric property.</p>

<p><img src=""http://i.stack.imgur.com/WSPo2.jpg"" alt=""enter image description here""></p>

<p>The two methods are: UPGMC and WPGMC (methods=""median"" and ""centroid"" in <code>hclust</code>).  Legendre &amp; Legendre in their Numerical Ecology book suggest some reasons why this may occur (Section 8.6).  However, they provide no solutions to rectify the issue and convert the trees to ultrametric.</p>

<p>I'm curious: is this an unavoidable consequence of the data and the clustering method, or is there a way that I can produce a tree that satisfies the ultrametric property using these two methods?</p>

<p>Here is an example data set and R code to play with:</p>

<pre><code>#Generate data frame with mixed continuous and categorical trait data for 10 species
set.seed(91)
(df=data.frame(trait1=runif(10,0,10),trait2=runif(10,0,10),
               trait3=sample(letters[1:3],10,replace=T),row.names=paste(""sp"",1:10,sep="""")))

#Generate Gower dissimilarity matrix from trait data
library(cluster)
(dist.gower=daisy(df,metric=""gower""))

#Create a vector of clustering methods
tree.methods=c(""ward"",""single"",""complete"",""average"",""mcquitty"",""median"",""centroid"")  
#Build the trees using each method
trees=lapply(tree.methods,function(i) hclust(dist.gower,method=i))  
#Plot the trees
par(mfrow=c(4,2))
for(i in 1:length(trees)) {plot(trees[[i]])}
#The last two trees have reversals...cannot be converted to ultrametric!
</code></pre>
"
"0.0969762275752854","0.132067635948844"," 27132","<p>I have social network data in which an ""ego"" names a friend ""alter"". I am running a regression in R in which attributes of alter are predictors of outcomes for ego. So each observation is dyadic with variable measures for both ego and alter. </p>

<p>There are multiple observations for each ego which are accounted for by using a gee model, clustering on ego. The problem is that i have been asked to also account for multiple observations of alter, or least to demonstrate that interdependence among the multiple alters is not impacting the final results. There are multiples of the same alter in the dataset as well as multiples of the same ego. </p>

<p>The two options seem to be some kind of cross clustering and I am not sure if that is possible in R. Another option which was suggested was to run a within-group correlation of some sort on the pearson's residuals, with the groups being the alters for each observation.  I had considered some sort of ICC but the number of times any individual alter shows up in the dataset ranges from 1-7. As far as I can tell, ICCs expect that the number of measures for each group in the dataset be the same. </p>

<p>Does anyone know how to do a within group correlation which can handle groups within which there are differing numbers of measures? I have looked online and have not come across anything that seems to address this. </p>

<p>Thanks in advance for any suggestions!</p>
"
"0.0559892510955854","0.0381246425831512"," 27323","<p>I have 114 vectors with 6 boolean attributes. I saw that might be several distinct clusters in a simple visualization. K-means clustering on the transformed vectors (true = 1, false = 0) results in roughly the clusters that I had seen in the visualization.</p>

<p>However, I am not sure what the most appropriate clustering method for this kind of data is, and how to determine the confidence in those factors (the k-means results change every time due to randomization). Should I treat the data as nominal or as numerical data?</p>

<p>What would be the best way to do a cluster analysis on this kind of data in R?</p>
"
"0.0685725481323742","0.062257280636469"," 28454","<p>How do I do group wise clustering in R?</p>

<p>Hi all,</p>

<p>I have N x K data matrix, where N is the number of observations, K is the number of variables.</p>

<p>The N observations fall into M categories or groups.</p>

<p>Now I want to cluster the groups, instead of the observations, how do I do that?</p>

<p>i.e. the clustering would be at the group level...</p>

<p>Thanks a lot for your help!</p>
"
"0.158361516492979","0.202186449756447"," 28492","<p>For fun, I tried to replicate the results of <a href=""http://rpproxy.iii.com:9797/MuseSessionID=248c435aa056d82d70d390e949c628fb/MuseHost=rfs.oxfordjournals.org/MusePath/content/22/1/435.abstract"" rel=""nofollow"">Petersen (2009)</a> who deals with the correct estimation of standard errors in finance panel data sets. </p>

<p>In a nutshell, he estimates the following standard regression for a panel data set:</p>

<p>$$
Y_{it} = X_{it} \beta + \epsilon_{it}
$$ </p>

<p>where $\epsilon_{it} = \gamma_i + \eta_{it}$ and $x_{it} = \mu_{i} + \nu_{it}$. Hence, both the residual and the independent variable have a firm-specific component. Petersen goes on to show that this results in biased standard errors when applying the standard OLS. For example, he shows in table 1 of his paper that if both the residual volatility and the variable volatility are driven by 50% by a firm-specific component, the true standard errors are nearly twice as large as the ones given by OLS.</p>

<p>He shows that in a MCS and I reproduced those results in R, as you can see from the code below. Naturally, I asked myself how I would compute the correct standard errors in R and the package of choice seemed to be <code>plm</code>. However, I just don't get the correct results out of it and I don't know what I miss.</p>

<p>Here is my code:</p>

<pre><code>library(plm)
runMCS &lt;- function(runs, nrN, nrT, fracFirmX, fracFirmEps, sd_X, sd_eps, beta) {

  betas    &lt;- numeric(runs)
  se_betas &lt;- numeric(runs)
  panel_betas    &lt;- numeric(runs)
  se_panel_betas &lt;- numeric(runs)

  for (i in 1:runs) {

    #Model epsilon, X, and Y
    eps &lt;- rep(rnorm(nrN, 
                     mean=0, 
                     sd = sd_eps * sqrt(fracFirmEps)), 
               each=nrT) + 
                 rnorm(nrN * nrT, mean=0, sd = sd_eps * sqrt(1-fracFirmEps))
    X   &lt;- rep(rnorm(nrN, 
                     mean=0, 
                     sd = sd_X   * sqrt(fracFirmX)),   
               each=nrT) + 
                 rnorm(nrN * nrT, mean=0, sd = sd_X   * sqrt(1-fracFirmX))
    Y   &lt;- beta * X + eps

    #Compute regression (OLS)
    reg &lt;- summary(lm(Y ~ X))

    #Save results
    betas[i]    &lt;- reg$coef[2, 1]
    se_betas[i] &lt;- reg$coef[2, 2]

    #Try plm
    df &lt;- data.frame(Firm = rep(1:nrN, each=nrT),
                     Time = rep(1:nrT, times=nrN),
                     Y = Y,
                     X = X)
    preg &lt;- summary(plm(Y ~ X, data=df, index=c(""Firm"", ""Time""), effect=""individual"", model=""within"")) #within is fixed effects
    panel_betas[i]    &lt;- preg$coef[1, 1]
    se_panel_betas[i] &lt;- preg$coef[1, 2]
  }

  return(c(avg_beta = mean(betas), 
           true_se = sd(betas), 
           avg_se = mean(se_betas), 
           avg_clustered = mean(panel_betas),
           se_clustered = mean(se_panel_betas)))

}
MCS_50_50 &lt;- runMCS(50, 500, 10, 0.5, 0.5, 1, 2, 1)
MCS_50_50
     avg_beta       true_se        avg_se avg_clustered  se_clustered 
   1.00503955    0.06020203    0.02825567    1.00433092    0.02985546
</code></pre>

<p>Note that I only run the simulation 50 times here because the plm function slows it down considerably. So basically, it makes virtually no difference if I call <code>lm</code> or <code>plm</code>. I'm pretty confident that I set the <code>index</code> and <code>model</code> option correct after reading the vignette of the package. However, I must miss something here! Interestingly, the package also has the <code>fixef</code> function and if I call that on one run, I get something like  this:</p>

<pre><code>summary(fixef(plm(Y ~ X, data=df, index=c(""Firm"", ""Time""), effect=""individual"", model=""within"")))
1      13.60377     0.44112    30.8391 &lt; 2.2e-16 ***
2    -830.74707     0.44136 -1882.2236 &lt; 2.2e-16 ***
3    -326.96042     0.44137  -740.7840 &lt; 2.2e-16 ***
4     169.16463     0.44246   382.3287 &lt; 2.2e-16 ***
...
</code></pre>

<p>I'm not quite sure how to interpret those results, but here, I get considerably larger standard errors for each firm separately. If I would average those, I would end up with something above 0.44 which is considerably closer to the true standard errors, but still not right.</p>

<p>So, again a very long question from me, sorry for that ;-) Note that I did check answers before and I found this interesting <a href=""http://stats.stackexchange.com/questions/10017/standard-error-clustering-in-r-either-manually-or-in-plm"">link</a>. The white paper that is referred to in the answer is interestingly the same person that implemented the solution on Petersen's <a href=""http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/se_programming.htm"" rel=""nofollow"">webpage</a>. So I'm pretty sure that I could get the correct standard errors by implementing Mahmood Arai's solution. But I'm looking for an already implemented and therefore safe option and I just wonder why that plm function does not work.</p>
"
"0.0395903791232448","0"," 28620","<p>I recently read a <a href=""http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/"" rel=""nofollow"">fascinating article</a> describing methods for clustering data without assuming a fixed number of clusters.</p>

<p>The article even includes some sample code, in a mix of Ruby, Python, and R.  However, the meat of the analysis is performed using <a href=""http://scikit-learn.sourceforge.net/dev/index.html"" rel=""nofollow"">scikit-learn</a>'s <a href=""http://scikit-learn.sourceforge.net/dev/modules/mixture.html"" rel=""nofollow"">Dirichlet Process Gaussian Mixture Model</a> to actually find clusters in some sample data taken from McDonald's menu.</p>

<p>Obviously, this a a great excuse to learn some more python, but I'm lazy and would like to find a ready-made R package that can take a dataframe and return clusters, in a manner similar to the <a href=""http://stat.ethz.ch/R-manual/R-devel/library/stats/html/kmeans.html"" rel=""nofollow"">kmeans</a> function.  <a href=""http://cran.r-project.org/web/views/Cluster.html"" rel=""nofollow"">A quick search on CRAN</a> reveals the packages <a href=""http://cran.r-project.org/web/packages/dpmixsim/index.html"" rel=""nofollow"">dpmixsim</a> and <a href=""http://cran.r-project.org/web/packages/profdpm/index.html"" rel=""nofollow"">profdpm</a>.  Any suggestions for the best place to start?</p>
"
"NaN","NaN"," 29114","<p>Does anybody know if any package calculates the cubic clustering criterion (CCC) index in R to aid the selection of optimal number of clusters? </p>
"
"0.0559892510955854","0.0381246425831512"," 31565","<p>I would like to make a heatmap with row clustering based on cosine distances. I'm using R and <code>heatmap.2()</code> for making the figure. I can see that there's a <code>dist</code> parameter in <code>heatmap.2</code> but I cannot find a function to generate the cosine dissimilarity matrix. The builtin <code>dist</code> function doesn't support cosine distances, I also found a package called <code>arules</code> with a <code>dissimilarity()</code> function but it only works on binary data.</p>
"
"0.0395903791232448","0"," 31906","<p>I'm trying to do fuzzy k-means clustering on a dataset using the cmeans function (R) . The problem Im facing is that the sizes of clusters are not as I would like them to be. This is done by calculating the cluster to which the observations are ""closest"". </p>

<pre><code>cl$size
 [1]   108    31   192    51   722 18460    67  1584   419 17270
</code></pre>

<p>Here we see that for 10 clusters we have two huge clusters and a lot of very small ones. Does this imply that two clusters are optimal in any way? If I do regular K-means 10 segments look very well, with good sizes and their intepretation makes a lot of sense but I would like to try fuzzy correctly. I just started exploring this fuzzy clustering so any help and pointers are overly welcome. </p>
"
"NaN","NaN"," 32239","<p>As described in Merlo et al (<a href=""http://www.ncbi.nlm.nih.gov/pubmed/16537344"" rel=""nofollow"">J Epidem Comm Health 2006</a>), the 95% credible interval for MOR is calculated using MCMC. MOR is defined as $\exp(\sqrt{2\sigma^2}\times 0.675)$, where $\sigma$ is the level-2 variance of the random intercept $u$ from a null model of a hierarchical logistic regression.  </p>

<p>Does anyone have an idea of how to write a program for an Markov chain Monte Carlo to calculate the standard error of the  median odds ratio (MOR) using <a href=""http://cran.r-project.org/web/packages/rjags/index.html"" rel=""nofollow"">rjags</a>?<br>
My dependent variable is outcome(alive/dead) and the clustering (level2)variable is Hospital. There are 140 hospitals and would like to see variations in outcome between hospitals. Other risk factors will be included later as independent level1 variables.</p>
"
"0.0395903791232448","0.0539163866017192"," 33210","<p>I have a database where each observation is a person. They were questioned on their attitude towards the consumption of X category of product. I have being using K-means to segment this data. </p>

<p>I have noticed that people under 19 years old tend to be quite different in their responses to those over 19.  I was thinking of dividing the data in <code>&lt;19</code> and <code>&gt;=19</code> and producing two clusterings and then merging them so as to produce a single report. </p>

<p>Does this make any sense from a data mining point-of-view? Is there precedent for doing so?</p>
"
"0.0885267789745639","0.0964485644340824"," 34238","<h2>Data Structure</h2>

<pre><code>&gt; str(data)
 'data.frame':   6138 obs. of  10 variables:
 $ RT     : int  484 391 422 516 563 531 406 500 516 578 ...
 $ ASCORE : num  5.1 4 3.8 2.6 2.7 6.5 4.9 2.9 2.6 7.2 ...
 $ HSCORE : num  6 2.1 7.9 1 6.9 8.9 8.2 3.6 1.7 8.6 ...
 $ MVMNT  : Factor w/ 2 levels ""_Withd"",""Appr"": 2 2 1 1 2 1 2 1 1 2 ...
 $ STIM   : Factor w/ 123 levels "" arti"","" cele"",..: 16 23 82 42 105 4 93 9 34 25 ...
 $ DRUG   : Factor w/ 2 levels ""Inactive"",""Pharm"": 1 1 1 1 1 1 1 1 1 1 ...
 $ FULLNSS: Factor w/ 2 levels ""Fasted"",""Fed"": 2 2 2 2 2 2 2 2 2 2 ...
 $ PATIENT: Factor w/ 25 levels ""Subj01"",""Subj02"",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ SESSION: Factor w/ 4 levels ""Sess1"",""Sess2"",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ TRIAL  : Factor w/ 6138 levels ""T0001"",""T0002"",..: 1 2 3 4 5 6 7 8 9 10 ...
</code></pre>

<h2>Full Model Candidate</h2>

<pre><code>model.loaded.fit &lt;- lmer(RT ~ ASCORE*HSCORE*MVMNT*DRUG*FULLNSS
                              + (1|PATIENT) + (1|SESSION), data, REML = TRUE)
</code></pre>

<ul>
<li>Reaction times from trials are clustered within sessions, which in turn are clustered within patients</li>
<li>Each trial can be characterized by two continuous covariates of ASCORE and HSCORE (ranging between 1-9) and by a movement response (withdraw or approach)</li>
<li>Sessions are characterized by drug intake (placebo or active pharmacon) and by fullness (fasted or pre-fed)</li>
</ul>

<h2>Modeling and R Syntax?</h2>

<p>I'm trying to specify an appropriate <em>full model</em> with a loaded mean structure that can be used as a starting point in a top-down model selection strategy. </p>

<p>Specific issues:</p>

<ul>
<li>Is the syntax correctly specifying the clustering and random effects?</li>
<li>Beyond syntax, is this model appropriate for the above within-subject design?</li>
<li>Should the full model specify all interactions of fixed effects, or only the ones that I am really interested in?</li>
<li>I have not included the STIM factor in the model, which characterizes the specific stimulus type used in a trial, but which I am not interested to estimate in any way - should I specify that as a random factor given it has 123 levels and very few data points per stimulus type?</li>
</ul>
"
"0.0559892510955854","0.0381246425831512"," 34483","<pre><code>  id       sam1  sam2  sam3  sam4
 gene1       33    23    88    98
 gene2        0     0    99    95
 gene3       77   100    44    65
 gene4        0     0     0     0
 gene5      100   100   100   100
  :
  :
 gene20000   58    33    78    56
</code></pre>

<p>I have 20K genes (rows) and four samples (columns) which is given in percentage. Each gene is common to four samples. The hypothesis is, if any of gene has 100% in any of sample it is said to be present in that sample and if 0% it is absent. (For eg. gene3 is present in sample2 (sam2) and gene4 is absent in all samples.)</p>

<p>Since I have 20K genes, I would like to do clustering technique. Can I do biclustering and produce a heatmap? If so, please let me know how to do in R?</p>
"
"0.0791807582464896","0.0269581933008596"," 34647","<p>I've got a distance matrix between examples. I want to cluster them into <em>m</em> clusters with a nearest neighbor algorithm which works like this:</p>

<pre><code>1. Set i = 1 and k = 1. Assign example x_1 to cluster C_1.
2. Set i = i + 1. Find nearest neighbour of x_i
   among the patterns already assigned to clusters.
   Let d_n  denote the distance from x_i to its nearest neighbour.
   Suppose the nearest neighbour is in cluster n.
3. If d_n less than or equal to t then assign x_i to C_n where t is the 
   threshold specified by the user. Otherwise set k = k+1 and assign x_i  to a      
   new cluster C_k.
</code></pre>

<p>How could I adapt this algorithm so I could specify how many clusters I want?</p>

<p>Is anybody aware of an existing R implementation of nearest neighbour clustering?</p>
"
"0.0969762275752854","0.110056363290703"," 35047","<p>I have a data set of 40,000 individuals which I clustered using k-means. I used 30 variables, each ordinal from <code>1=minimum</code> to <code>5=maximum</code>. I reduced these 30 variables to 10 factors and ran K-means on these new variables. I kept a clustering of 12 clusters. </p>

<p>I now have 7000 new observations, I want to classify these using a discriminant function. I built one in SPSS using the 30 previous variables. I get a good clasification on the original dataset (40,000), but when I run it on the new observations, one segment doesn't show at all. 0 cases are of this type. This makes no sense to me. </p>

<p>The original clusters seem healthy and apart, I did them in R with multiple different initial center methods and ran them many many times, and stuck with the best one.  The discriminant function has no problem finding this ""disappearing"" cluster in the original dataset. </p>

<p>What could be causing this? </p>
"
"0.0969762275752854","0.110056363290703"," 38117","<p>I want to visualize the results of a clustering (produced with <code>protoclust{protoclust}</code>) by creating scater plots for each pair of variables used for classifying my data, colouring by classes and overlapping the ellipses for the 95% confidence interval for each of the classes (to inspect which elipses-classes overlap under each pair of variables).</p>

<p>I have implemented the drawing of the ellipses in two different ways and the resulting ellipses are different! (bigger ellipses for first implementation!) 
A priori they differ only in size (some diferent scaling?), as the  centers and angle of axes, seem to be the similar in both.
I guess I must be doing something wrong by using one of them (hope not with both!), or with the arguments.</p>

<p>Can anyone tell me what I am doing wrong?</p>

<p>Here the code for the two implementations; both are based on the answers to <a href=""http://stackoverflow.com/questions/2397097/how-can-a-data-ellipse-be-superimposed-on-a-ggplot2-scatterplot"">How can a data ellipse be superimposed on a ggplot2 scatterplot?</a></p>

<pre><code>### 1st implementation 
### using ellipse{ellipse}
library(ellipse)
library(ggplot2) 
library(RColorBrewer)
colorpal &lt;- brewer.pal(10, ""Paired"")

x &lt;- data$x
    y &lt;- data$y
group &lt;- data$group
df &lt;- data.frame(x=x, y=y, group=factor(group))

df_ell &lt;- data.frame() 
for(g in levels(df$group)){df_ell &lt;- rbind(df_ell, cbind(as.data.frame(with(df[df$group==g,], ellipse(cor(x, y),scale=c(sd(x),sd(y)),centre=c(mean(x),mean(y))))),group=g))} 

p1 &lt;- ggplot(data=df, aes(x=x, y=y,colour=group)) + geom_point() + 
  geom_path(data=df_ell, aes(x=x, y=y,colour=group))+scale_colour_manual(values=colorpal)

### 2nd implementation 
###using function ellipse_stat() 
###code by Josef Fruehwald available in: https://github.com/JoFrhwld/FAAV/blob/master/r/stat-ellipse.R

p2 &lt;-qplot(data=df, x=x,y=y,colour=group)+stat_ellipse(level=0.95)+scale_colour_manual(values=colorpal)
</code></pre>

<p>Here is the two plots together (left graph is <code>p1</code> implementation (<code>ellipse()</code>):</p>

<p><img src=""http://i.stack.imgur.com/Qf49A.jpg"" alt=""enter image description here""></p>

<p>The data are available here: <a href=""https://www.dropbox.com/sh/xa8xrisa4sfxyj0/l5zaGQmXJt"">https://www.dropbox.com/sh/xa8xrisa4sfxyj0/l5zaGQmXJt</a></p>
"
"0.131306432859723","0.130051217752471"," 38460","<p>I am new here - and relatively new to statistics, data mining and R. I am trying to understand why my data is not clustering correctly - or if I am reading it wrong. Shortly about the project:</p>

<p>My data points are behavior of online gaming users over time (one observation for each user). I am trying to cluster a matrix containing 5000 observations. So far I have 4 dimensions that correspond to average games/day, interval between days, score, friends - all of these happened in the first 2 months of user's online presence. So it looks approximately like:</p>

<pre><code>  avg_g_day avg_interval  score friends
1       8.5            6   6050       0
2   48.1304       1.8636  90530       0
3   70.0702       1.0714 293520       2
4        25            1   5710       4
5      3.75           10  10900       0
</code></pre>

<p>I am trying cluster this using all possible methods in R. So far I have not had success. I first scale my data using scale() and then fit the clusters. When I try kmeans (which would mean Euclidian distance, in my understanding), I get intersecting clusters upon visualization with clusplot. When I am trying to find the best k, I get that the best k is 2, which seems quite un-commonsense with 5000 points and 4 dimensions. I have tried other methods and other distance metrics, so far to no avail - I have used Mclust() and hclust. In each dimension the distribution is not uniform and I do not get warnings that best number of clusters is 1. Then why does the kmeans or pam method give me clusters that intersect on the 2 first principal components graph (clusplot)? Is there a way to separate them? </p>

<p>Since I am not sure which details could provide more insight into my problem, I will post them if questions arise. Thank you for any help. </p>
"
"0.0395903791232448","0"," 39303","<p>I have 3 covariates for 100 observations. How can I separate each of my 100 observations into groups as determined by the data. I was thinking clustering. However, apparently, I need more than 3 dimensions to do hierarchical clustering. Would some other clustering method work? How about PCA?</p>

<p>I've attached the data as R output below.</p>

<p><code>dput</code> output</p>

<pre><code>structure(c(3.87, 0.672, 0.7392, 6.471, 0.12294, 1.0857, 16.701, 
0.2754, 0.17328, 8.076, 0.12222, 1.1796, 8.625, 1.3998, 0.07233, 
3.933, 0.017484, 0.9189, 4.134, 0.7338, 2.9517, 5.091, 0.017136, 
0.6318, 6.672, 3.012, 0.08214, 15.834, 0.7968, 0.27768, 3.954, 
0.02046, 0.705, 9.465, 0.15444, 1.2702, 15.012, 0.4263, 2.262, 
21.438, 0.9291, 0.3399, 20.076, 1.023, 8.289, 5.601, 0.007992, 
0.984, 12.396, 0.4869, 2.343, 11.697, 0.4296, 0.4932, 8.247, 
0.27063, 0.3408, 3.273, 0.03954, 0.16446, 4.59, 0.0011037, 0.8937, 
11.196, 0.17538, 0.9594, 14.688, 0.13527, 0.3672, 2.8554, 0.0027594, 
0.5943, 0.26472, 0.0004233, 0.3315, 1.5633, 0.0363, 0.5232, 5.766, 
0.005901, 0.342, 10.578, 2.4477, 0.28872, 10.803, 1.0185, 1.3935, 
5.352, 1.1967, 0.5316, 4.8, 0.00672, 2.6418, 3.081, 0.15525, 
0.6873, 13.899, 0.19149, 0.4674, 11.439, 1.6521, 0.3867, 4.005, 
0.008328, 0.3675, 5.7, 0.027999, 0.3486, 13.035, 0.21639, 0.7293, 
8.706, 1.0833, 0.198, 5.871, 0.5655, 2.0367, 4.218, 0.011547, 
0.24234, 2.1603, 0.0011748, 0.4569, 5.385, 0.005091, 0.666, 9.651, 
0.4392, 1.101, 11.178, 0.4179, 0.7005, 8.151, 0.00516, 0.27696, 
6.864, 0.018753, 0.303, 3.792, 0.01449, 1.8345, 6.834, 0.03339, 
0.28896, 5.073, 0.012951, 0.5013, 3.132, 0.008892, 0.3207, 1.1841, 
5.292e-05, 0.006795, 9.432, 0.324, 0.5916, 8.55, 2.4642, 0.9576, 
3.588, 0.006912, 1.089, 6.396, 0.04818, 1.4448, 20.604, 0.363, 
0.7401, 11.712, 0.03897, 1.9491, 11.682, 1.149, 2.217, 3.882, 
0.15963, 5.916, 6.702, 0.3174, 1.6392, 7.188, 0.03582, 0.2646, 
8.853, 0.7761, 2.2446, 18.915, 0.3993, 0.002736, 9.699, 0.16638, 
0.6855, 9.423, 0.011793, 0.7986, 14.667, 2.5146, 0.28512, 5.919, 
0.06705, 0.25305, 8.184, 0.005262, 0.6492, 1.4604, 0.0138, 0.1872, 
8.604, 0.3057, 0.8052, 8.142, 0.017808, 0.9564, 2.2824, 0.000243, 
0.2565, 6.012, 0.16425, 0.3969, 12.633, 0.9408, 1.7154, 9.396, 
0.21945, 2.73, 13.479, 0.16236, 14.433, 9.612, 0.24222, 1.3275, 
12.486, 2.1543, 0.08652, 0.0003612, 5.364e-07, 0.3144, 9.942, 
1.3674, 1.326, 2.4621, 0.00019425, 0.6684, 1.6341, 0.0006165, 
0.5124, 11.796, 0.9798, 3.243, 11.73, 0.4716, 1.0248, 5.133, 
0.04527, 0.3078, 11.886, 2.6718, 1.158, 5.421, 0.06027, 1.7655, 
6.69, 0.00783, 5.907, 11.832, 0.9534, 3.228, 1.0323, 0.0016356, 
0.861, 6.774, 1.1001, 1.1811, 8.856, 0.4185, 1.3521, 11.877, 
0.2754, 2.5563, 0.0024852, 1.4796e-05, 0.6741, 6.774, 0.336, 
2.5017, 1.6425, 2.478e-05, 0.09243, 8.973, 0.25473, 0.9942, 13.245, 
0.3234, 0.6711, 10.35, 0.5148, 1.0578, 14.556, 0.774, 0.9225), .Dim = c(3L, 
100L), .Dimnames = list(c(""A"", ""B"", ""C""), c(""000162434"", ""000151547"", 
""000133688"", ""000123954"", ""000184599"", ""000122987"", ""000117559"", 
""000121528"", ""000192459"", ""000196759"", ""000172539"", ""000155583"", 
""000185889"", ""000143968"", ""000128617"", ""000185423"", ""000158324"", 
""000114797"", ""000126134"", ""000185624"", ""000123385"", ""000188299"", 
""000195142"", ""000194666"", ""000113189"", ""000182457"", ""000173324"", 
""000162459"", ""000141996"", ""000155516"", ""000148231"", ""000176159"", 
""000135131"", ""000186287"", ""000187355"", ""000199513"", ""000125251"", 
""000116237"", ""000188675"", ""000147224"", ""000198156"", ""000119366"", 
""000132841"", ""000123791"", ""000138154"", ""000149758"", ""000157127"", 
""000167763"", ""000113718"", ""000128418"", ""000148221"", ""000139836"", 
""000194814"", ""000199972"", ""000168968"", ""000198853"", ""000128498"", 
""000149484"", ""000196219"", ""000184178"", ""000144155"", ""000114251"", 
""000114264"", ""000131697"", ""000154146"", ""000163257"", ""000112289"", 
""000114416"", ""000195761"", ""000128348"", ""000144337"", ""000167126"", 
""000159175"", ""000172296"", ""000182932"", ""000198134"", ""000127718"", 
""000166651"", ""000196877"", ""000174415"", ""000131167"", ""000165476"", 
""000195958"", ""000189229"", ""000119255"", ""000165984"", ""000119118"", 
""000164273"", ""000199986"", ""000136544"", ""000124271"", ""000191248"", 
""000126459"", ""000143728"", ""000182847"", ""000162785"", ""000193387"", 
""000119516"", ""000199516"", ""000145424"")))
</code></pre>
"
"NaN","NaN"," 41268","<p>I have  two dimensional spatial (x,y - coordinates of meteo stations) data for small region (so I could neglect the shape of earth globe), for each (x,y) I have one observation of wind direction and speed I need to design metric for clustering problem of such data. It's simple to do when we consider only directions or only the speed of wind but how to combine these quantities ? I try to analyse this problem using R.</p>
"
"0.0559892510955854","0"," 41575","<p>I have a dataset of about a million companies containing their names, total employees and annual sales. I want to come up with a function that when given the company returns the 5 most similar companies in terms of their distance in total employees and annual sales.</p>

<p>I thought of doing k-means clustering on the dataset and find clusters. Then return all the companies in that cluster. The problem with this approach is that I don't know the number of clusters I should form beforehand.</p>

<p>Also, on a separate note, if I were to obtain a list of specialties (e.g. marketing, software, etc.) for each of the companies - how can I transform this qualitative value in to a number which can later help me calculate similarity.</p>
"
"0.0685725481323742","0.0311286403182345"," 44640","<p>I am trying to cluster Facebook users based on their likes.</p>

<p>I have two problems: First, since there is no dislike in Facebook all I have is having likes (1) for some items but for the rest of the items, the value is unknown and not necessarily zero (corresponding to a dislike). If use 0 for unknowns, then I think my clusters will be biased.
Any suggestion?</p>

<p>Second, supposed I assign 0 to unknown items and cluster them, using a hierarchichal clustering method using a binary measure distance such as Jaccard, Tanimoto,...</p>

<p>How can I evaluate the clustering results? The within and outside SSE is not appropriate for binary data. If I use median centers, I m afraid most of them are going to be zero as I have a sparse feature matrix. So what would be a good way to evaluate the clusters?  </p>
"
"0.0685725481323742","0.0311286403182345"," 44957","<p>A colleague and I have been clustering some data in SPSS (v19) and R (2.15), respectively. Using the same distance metric and agglomeration method, we get identical merge orders/agglomeration schedules in both programs, and the dendrograms have very similar shapes, but the actual height values are quite different. On this particular data set, the R dendrogram is about 150 units tall, but the SPSS dendrogram is only 25 units tall. This is somewhat $\ldots$ unsettling, obviously.</p>

<p>While thumbing through some manuscripts and webpages, I noticed that all but one of the SPSS-derived dendrograms were also exactly 25 units tall (example <a href=""http://cw.psypress.com/multivariate-analysis/medical-examples/chapter10/med_cluster_analysis.pdf"" rel=""nofollow"">1</a>, <a href=""http://www.cs.uu.nl/docs/vakken/arm/SPSS/spss8.pdf"" rel=""nofollow"">2</a>, <a href=""http://www.statisticshell.com/docs/cluster.pdf"" rel=""nofollow"">3</a>; we also have a pretty hefty pile of papers that are unfortunately all pay-walled, including the single counter-example).</p>

<p>The caption on the SPSS output says something about rescaling, but the documentation is oddly silent about if, how, and why SPSS might be rescaling the dendrograms.</p>

<p>Could someone please confirm that SPSS does rescales dendrograms (and rescales them onto [0,25])? For extra credit, is there a way to turn this rescaling off? It seems to cause SPSS to cut our dendrogram a few levels above the leaves.</p>
"
"0.0885267789745639","0.0964485644340824"," 46798","<p>I am looking for a clustering algorithm. My idealized dataset looks like this:<br>
<img src=""http://i.stack.imgur.com/pqEpC.png"" alt=""http://i.stack.imgur.com/bSlNU.png""></p>

<p>The clustering result should look like the Rapidminer density plot:<br>
<img src=""http://i.stack.imgur.com/uMRQ4.png"" alt=""http://i.stack.imgur.com/Sk3xK.png""> </p>

<p>Means 3 or 4 clusters should be the clustering result: One Cluster (1|1) to (5|6), one or two for the points on Z = 5 and one for the overlaying area (6|1) to (11|6).</p>

<p>Based on ""density"" I gave the <code>DBSCAN</code> R package a shot. The best result I could get was the one in the above picture. So not exactly what I expected, as the overlaying area was not recognized as a separate cluster. Any ideas which algorithm would provide the results I expect?
<img src=""http://i.stack.imgur.com/pqEpC.png"" alt=""enter image description here""></p>
"
"0.131306432859723","0.130051217752471"," 46821","<p>I am producing a script for creating bootstrap samples from the <code>cats</code> dataset (from the <code>-MASS-</code> package). </p>

<p>Following the Davidson and Hinkley textbook [1] I ran a simple linear regression and adopted a fundamental non-parametric procedure for bootstrapping from iid observations, namely <strong>pairs resampling</strong>.</p>

<p>The original sample is in the form:</p>

<pre><code>Bwt   Hwt

2.0   7.0
2.1   7.2

...

1.9    6.8
</code></pre>

<p>Through an univariate linear model we want to explain cats hearth weight through their brain weight. </p>

<p>The code is:</p>

<pre><code>library(MASS)
library(boot)


##################
#   CATS MODEL   #
##################

cats.lm &lt;- glm(Hwt ~ Bwt, data=cats)
cats.diag &lt;- glm.diag.plots(cats.lm, ret=T)


#######################
#   CASE resampling   #
#######################

cats.fit &lt;- function(data) coef(glm(data$Hwt ~ data$Bwt)) 
statistic.coef &lt;- function(data, i) cats.fit(data[i,]) 

bootl &lt;- boot(data=cats, statistic=statistic.coef, R=999)
</code></pre>

<p>Suppose now that there exists a clustering variable <code>cluster = 1, 2,..., 24</code> (for instance, each cat belongs to a given litter). For simplicity, suppose that data are balanced: we have 6 observations for each cluster. Hence, each of the 24 litters is made up of 6 cats (i.e. <code>n_cluster = 6</code> and <code>n = 144</code>).</p>

<p>It is possible to create a fake <code>cluster</code> variable through:</p>

<pre><code>q &lt;- rep(1:24, times=6)
cluster &lt;- sample(q)
c.data &lt;- cbind(cats, cluster)
</code></pre>

<p>I have two related questions:</p>

<p>How to simulate samples in accordance with the (clustered) dataset strucure? That is, <strong>how to resample at the cluster level?</strong> I would like to sample the clusters with replacement and to set the observations within each selected cluster as in the original dataset (i.e. sampling with replacenment the clusters and without replacement the observations within each cluster). </p>

<p>This is the strategy proposed by Davidson (p. 100). 
Suppose we draw <code>B = 100</code> samples. Each of them should be composed by 24 possibly recurrent clusters (e.g. <code>cluster = 3, 3, 1, 4, 12, 11, 12, 5, 6, 8, 17, 19, 10, 9, 7, 7, 16, 18, 24, 23, 11, 15, 20, 1</code>), and each cluster should contain the same 6 observations of the original dataset. How to do that in <code>R</code>? (either with or without the <code>-boot-</code> package.) Do you have alternative suggestions for proceeding?</p>

<p>The second question concerns the initial regression model. Suppose I adopt a <strong>fixed-effects model</strong>, with cluster-level intercepts. <strong>Does it change the resampling procedure</strong> adopted? </p>

<p>[1] Davidson, A. C., Hinkley, D. V. (1997). <em>Bootstrap methods and their applications</em>. Cambridge University press.</p>
"
"0.118771137369734","0.143777030937918"," 46978","<p>I am fitting a <em>Fixed-Effects</em> model, with intercepts at <code>cluster</code> level.</p>

<p>One of the most direct ways is probably to use the <code>-plm-</code> package. Another well-known possibility is to apply OLS (i.e. to adopt <code>-lm-</code>) to the <em>demeaned data</em>, where the means are taken at the clustering level.</p>

<p>This second approach is usually referred to as the <strong>within transformation</strong>. It is quite convenient from a computational standpoint, because we are still controlling unobserved heterogeneity at clustering level, but we do not need to estimate all the time-fixed intercepts.</p>

<p>I have tried both of these approaches, and I came to a strange result. In practice, the coefficient of the regressor of interest, <code>x</code>, is the same in both cases. However, its standard error (and actually all the other relevant quantities of the regression: R squared, F test, etc.) is different.</p>

<p>Please, notice that I have carefully read both the <em>R documentation</em> about <code>-plm-</code> and the <a href=""http://www.google.it/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;ved=0CD4QFjAB&amp;url=http://www.jstatsoft.org/v27/i02/paper&amp;ei=7f3mUP_0DYrXtAaD7oDADw&amp;usg=AFQjCNFu_xrsnFYsC8j8DDh9mRQnoyQ6jg&amp;bvm=bv.1355534169,d.bGE"" rel=""nofollow"">related paper of the authors</a>, where it is stated that the package apply the <em>within transformation</em> and then apply OLS, as I did...</p>

<p>The R script is:</p>

<pre><code># set seed, load packages, create fake sample

set.seed(999)
library(plyr)
library(plm)

dat &lt;- expand.grid(id=factor(1:3), cluster=factor(1:6))
dat &lt;- cbind(dat, x=runif(18), y=runif(18, 2, 5))


############################
#   FE model using -plm-   #
############################

# model fit  
fe.1 &lt;- plm(y ~ x, data=dat, index=""cluster"", model=""within"")

# estimated coefficient and standard error of x
b.1 &lt;- summary(fe.1)$coefficients[,1]
    se.1 &lt;- summary(fe.1)$coefficients[,2]


######################################
#   OLS on within-transformed data   #
######################################

# augmenting data frame with cluster-mean centered variables 
dat.2 &lt;- ddply(dat, .(cluster), transform, dem_x=x-mean(x), dem_y=y-mean(y))

# model fit
fe.2 &lt;- lm(dem_y ~ dem_x - 1, data=dat.2)

# estimated coefficient and standard error of x
b.2 &lt;- summary(fe.2)$coefficients[1,1]
    se.2 &lt;- summary(fe.2)$coefficients[1,2]


#########################
#   models comparison   #
#########################

b.1; b.2
se.1; se.2

summary(fe.1)
summary(fe.2)
</code></pre>

<p>Notice that in the second model it is necessary to manually eliminate the intercept from the model. </p>
"
"0.0559892510955854","0.0381246425831512"," 47319","<p>I have data from questionnaire from school. 35 questions are various questions (influence of friends etc.)</p>

<p>Possible answers for 35 questions are ""definitely yes"", ""mostly yes"", ""mostly no"" and ""definitely no"".</p>

<p>I did hierarchical clustering using <code>hclust</code> in R. Then I used <code>cutree</code> for cut the dendrogram.</p>

<p>How to visualize data about clusters from <code>cutree</code>? I wrote function for export information about clusters to CSV, but I want to display graphical information.</p>

<p>Thanks</p>
"
"0.0559892510955854","0"," 48745","<p>If I am looking at sports data of dozens of soccer leagues over a year:</p>

<pre><code>Dependent Variable = Goals Scored

Independent Variables:
X1 = Traditional Home Uniforms
X2 = Retro Home Uniforms
X3 = Secondary Home Uniforms 

Controls:
A whole bunch
Most importantly--Every team has a rank (1-50) from the previous year 
</code></pre>

<p>When I run my regressions clustering my errors on rank the results are fine, but when I add a dummy for each rank my output becomes skewed:</p>

<pre><code>Number of obs =    972
F( 18,   241) =       .
Prob &gt; F      =       .
R-squared     =  0.2478
</code></pre>

<p>Any ideas why my F becomes a '.'?</p>
"
"0.0969762275752854","0.0880450906325624"," 49243","<p>R's randomForest package can not handle factor with more than 32 levels. When it is given more than 32 levels, it emits an error message:</p>

<blockquote>
  <p>Can not handle categorical predictors with more than 32 categories.</p>
</blockquote>

<p>But the data I have has several factors. Some of them have 1000+ levels and some of them have 100+. It even has 'state' of united states which is 52. </p>

<p>So, here's my question.</p>

<ol>
<li><p>Why is there such limitation? randomForest refuse to run even for the simple case.</p>

<pre><code>&gt; d &lt;- data.frame(x=factor(1:50), y=1:50)
&gt; randomForest(y ~ x, data=d)
  Error in randomForest.default(m, y, ...) : 
  Can not handle categorical predictors with more than 32 categories.
</code></pre>

<p>If it is simply due to memory limitation, how can scikit learn's randomForeestRegressor run with more than 32 levels?</p></li>
<li><p>What is the best way to handle this problem? Suppose that I have X1, X2, ..., X50 independent variables and Y is dependent variable. And suppose that X1, X2 and X3 has more than 32 levels. What should I do?</p>

<p>What I'm thinking of is running clustering algorithm for each of X1, X2 and X3 where distance is defined as difference in Y. I'll run three clusterings as there are three problematic variables. And in each clustering, I wish I can find similar levels. And I'll merge them.</p>

<p>How does this sound?</p></li>
</ol>
"
"0.111978502191171","0.0571869638747267"," 49313","<p>I have a k-means clustering result with 35 clusters, there are 5000 documents that each belong to one of the 35 cluster. I would like to visualize the results of the clustering algorithm on a scatter plot (or something similar) where each document is colored based on which cluster they belong to, and their distance on the visualization is proportional to their distance in similarity (i.e. the more similar they are, the closer they appear on the visualization). Ideally, it would also be nice to see the top 10 words that belong to the clusters. I am attaching my code for the clustering algorithm, it deals with data from a database. </p>

<pre><code>myCorpus &lt;- Corpus(VectorSource(userbios$bio))
    docs &lt;- userbios$twitter_id
# convert to lower case
myCorpus &lt;- tm_map(myCorpus, tolower)
# remove punctuation
myCorpus &lt;- tm_map(myCorpus, removePunctuation)
# remove numbers
myCorpus &lt;- tm_map(myCorpus, removeNumbers)
# remove URLs
removeURL &lt;- function(x) gsub(""http[[:alnum:]]*"", """", x)
myCorpus &lt;- tm_map(myCorpus, removeURL)
# add one extra stop words:  ""via""
myStopwords &lt;- c(stopwords('english'), ""twitter"", ""tweets"", ""tweet"", ""tweeting"", ""account"")


# remove stopwords from corpus
myCorpus &lt;- tm_map(myCorpus, removeWords, myStopwords)


myTdm &lt;- TermDocumentMatrix(myCorpus, control = list(wordLengths=c(1,Inf), weighting=weightTfIdf))
# remove sparse terms
myTdm2 &lt;- removeSparseTerms(myTdm, sparse=0.90)

m2 &lt;- as.matrix(myTdm2)
#cluster terms
distMatrix &lt;- dist(scale(m2))
fit &lt;- hclust(distMatrix, method=""ward"")
# transpose the matrix to cluster documents (tweets)
 m3 &lt;- t(m2)

# k-means clustering
 k &lt;- 35
kmeansResult &lt;- kmeans(m3, k)
#cluster centers
round(kmeansResult$centers, digits=3)
    for (i in 1:k) {
      cat(paste(""cluster "", i, "": "", sep=""""))
      s &lt;- sort(kmeansResult$centers[i,], decreasing=T)
      cat(names(s)[1:15], ""\n"")
      # print the tweets of every cluster + # 
      print(docs[which(kmeansResult$cluster==i)])
}
</code></pre>
"
"0.0791807582464896","0.0539163866017192"," 49549","<p>I am attempting some variable reduction before I perform a logistic regression.  I am quite interested in using <code>Hmisc::varclus</code> in R.  However, I am having some difficulty interpreting the output.  As far as I can tell, the (tree) plot produced using <code>varclus</code> is the only built-in way to get information on the groups created by the procedure.  </p>

<p>My main question involves knowing at what level of the hierarchy to select the clusters to be used for variable reduction?  I read that of a rule-of-thumb to keep a cluster is if its rho (for Spearman's) is at least 0.30.  Would this be evaluated visually from the plot?  At what value of rho would the cut-off be made to separate the tree into <em>final</em> clusters? Again, is this to be done visually?   </p>

<p>Perhaps my internet searching skills are lacking, but I am having difficulty finding information on this procedure in general and more specifically in R. Is there a good <em>beginner's</em> article on variable clustering that I am missing that spells out the fundamentals?  Are there additional commands for <code>varclus</code> in R to help with final cluster decisions besides examining the tree visually?</p>

<pre><code># varclust example in R using mtcars data
mtcn &lt;- data.matrix(mtcars)
clust &lt;- varclus(mtcn)
clust
plot(clust)
</code></pre>
"
"0.0969762275752854","0.0440225453162812"," 49578","<p>I was trying k-medoids algorithm for clustering in R, and just removing stopwords is taking hours. After I removed stopwords, I let the algorithm run overnight and it wasn't done after 8-9 hours. I am trying to cluster 40.000 documents, what alternatives do I have? Should I wait it out? I am just afraid that it will never stop, the application will just stop responding. Is it normal for the script to take 10+ hours to run?</p>

<p>This is my code:</p>

<pre><code>myCorpus &lt;- Corpus(VectorSource(userbios$bio))
    docs &lt;- userbios$twitter_id
# convert to lower case
myCorpus &lt;- tm_map(myCorpus, tolower)
# remove punctuation
myCorpus &lt;- tm_map(myCorpus, removePunctuation)
# remove numbers
myCorpus &lt;- tm_map(myCorpus, removeNumbers)
# remove URLs
removeURL &lt;- function(x) gsub(""http[[:alnum:]]*"", """", x)
myCorpus &lt;- tm_map(myCorpus, removeURL)
# add extra stop words
myStopwords &lt;- c(""twitter"", ""tweets"", ""tweet"", ""tweeting"", ""account"")
summary(myStopwords)

# remove stopwords from corpus
myCorpus &lt;- tm_map(myCorpus, removeWords, stopwords('english'))
myCorpus &lt;- tm_map(myCorpus, removeWords, myStopwords)

# keep a copy of corpus to use later as a dictionary for stem completion
myCorpusCopy &lt;- myCorpus
# stem words
# require(rJava) # needed for stemming function 
# library(Snowball) # also needed for stemming function 
# a &lt;- tm_map(myCorpus, stemDocument, language = ""english"")

myTdm &lt;- TermDocumentMatrix(myCorpus, control = list(wordLengths=c(1,Inf), weighting=weightTf))
myTdm2 &lt;- removeSparseTerms(myTdm, sparse=0.85)
m2 &lt;- as.matrix(myTdm2)
m3 &lt;- t(m2)

library(fpc)
pamResult &lt;- pamk(m3, metric=""manhattan"")
k&lt;-pamResult$nc
    pamResult &lt;- pamResult$pamobject

 # print cluster medoids
 for (i in 1:k) {
 cat(paste(""cluster"", i, "": ""))
 cat(colnames(pamResult$medoids)[which(pamResult$medoids[i,]==1)], ""\n"")
 # print tweets in cluster i
  print(docs[pamResult$clustering==i])
 }
</code></pre>
"
"0.0685725481323742","0.062257280636469"," 50011","<p>I'm looking for some assistance in statistical analysis with R, but also some general stats advice.</p>

<p>I am analysing cardiac phenotype data by comparing 2 groups. The 2 groups are unmatched individuals, but within each group, they are clustered in family subgroups (of between 1 and ~6).</p>

<p>I want to report the difference in prevalence of a specific ECG appearance (binary - i.e. either present or absent in each individual) between the 2 groups.</p>

<p>For example:</p>

<blockquote>
  <p>Group 1 consists of 157 individuals comprised of 41 family clusters. 
  Group 2 consists of 463 individuals comprised of 163 family clusters. 
  Prevalence of x in Group 1 = 22.9% Prevalence of x in Group 2 = 24.6%. 
  Group 1 are cases and Group 2 controls (i.e. not randomized and defined by phenotype in an observational study). </p>
</blockquote>

<p>What test is most appropriate in this circumstance, and which package in R provides the easiest way to account for the clustering of relatives within families?</p>

<p>Having looked around, I have found:</p>

<ul>
<li>Ratio estimate chi-square test</li>
<li>Generalized estimating equation</li>
</ul>

<p>But I have no experience of either of these techniques, and can't find any examples of their use in R.</p>

<p>Any advice on how best to proceed?</p>

<p>EDIT: See comment below for update.
I believe the Donner (1989) chi-square correction may be the most appropriate (provided by R function donner).  Second opinions and correct use of R command appreciated. Thanks.</p>
"
"0.0559892510955854","0.0381246425831512"," 51556","<p>Does anyone know a good method to determine if clustering using kmeans is even appropriate? That is, what if your sample is actually homogenous? I know something like a mixture model (via mclust in R) will provide fit statistics for the 1:k cluster case, but it seems like all of the techniques to evaluate kmeans requires at least 2 clusters.</p>

<p>Does anyone know of a technique to compare the 1 and 2 cluster cases for kmeans?</p>
"
"0.111978502191171","0.0953116064578779"," 51856","<p>I performed and plotted a kmeans analysis in R with the following commands:</p>

<pre><code> km = kmeans(t(mat2), centers = 4)
 plotcluster(t(mat2), km$cluster)      #from library(fpc)
</code></pre>

<p>Here is the result from the plot: <img src=""http://i.stack.imgur.com/p3ds0.png"" alt=""kmeans clustering""></p>

<p>This question is related to a previous question: <a href=""http://stats.stackexchange.com/questions/51707/reading-kmeans-data-and-chart-from-r"">Previous Question</a></p>

<p>My data matrix has dimensions $291 \times 31$ (after taking the transpose by <code>t(mat2)</code>)
What I want to know, is <strong>how can I create a mapping from each row in the matrix to a 2D point in the plot?</strong> My idea is to get the $31$ dimensional coordinates for each point in the plot and then map and compute the 2D coordinates with <code>discrproj()</code>.For example, I see that I should be able to find the 2D center points of all clusters by calling <code>discrproj()</code> on the matrix given by <code>km$centers</code> (which has dimensions $4 \times 31$ and hence contains the coordinates for each cluster in $31$ dimensional space). </p>

<p>However, where is the data for the coordinates in $31$ dimensional space for every 2D point in the plot? Is this data just my $291 \times 31$ data matrix? In summary:</p>

<ol>
<li>How can I create a mapping from each row in the $291 \times 31$ data matrix to a 2D point in the plot?</li>
<li>Where/what is the data for the coordinates in $31$ dimensional space for every 2D point in the plot</li>
</ol>
"
"NaN","NaN"," 52679","<p>I've been looking for a Poisson expectation-maximization clustering implemented in R.  Does anyone know if there is one available or can point me in the right direction?</p>

<p>thanks!</p>

<p>and please let me know if there's a better way/place to ask questions.</p>
"
"0.0885267789745639","0.0964485644340824"," 54522","<p>I am working for a big company with a restrictive internet policy and Excel-addicted colleagues. I am currently working on evolution of market correlations which implies some statistics, data analysis, clustering, data visualization ... From what I have seen on the internet it's not a good idea to do it in Excel. (see here a general study: <a href=""http://stats.stackexchange.com/questions/3392/excel-as-a-statistics-workbench/3398#3398"">Excel as a statistics workbench</a>)</p>

<p>After a struggle of 2 weeks, I have finally got from IT a working version of R and some interesting packages. My market data are stocked in a .txt file, I work on it with R and create a results.txt file, then I load the results.txt file in Excel and I plot what my boss wants. </p>

<p>I admit that Excel is useful for manipulating a lot of data sets and graphs at the same place. It's the only good point compared to R for what I want to do. I think my cheap .txt solution to do calculations in R is correct and simple ... (for the anecdote things like Rexcel to connect R and Excel are forbidden where I work - don't ask why - so I have tried a macro which create a .bat to launch R and do the calculation; too complex for my colleagues)</p>

<p>But for data visualization Excel is very poor; I really miss some graphs I have in R.
Dendograms, boxplots, histograms, correlation circles, summarized correlations, and heatmaps are very interesting for me, but not available. So my question is how to get them in Excel ? (Remember the strict internet policy; I can't download any add-ins). Is there a (easy) way to plot complex things with macro or workbooks ? Do you have some sources?</p>
"
"NaN","NaN"," 55147","<p>I'm wondering if there is a good way to calculate the clustering criterion based on BIC formula, for a k-means output in R? I'm a bit confused as to how to calculate that BIC so that I can compare it with other clustering models. Currently I'm using the stats package implementation of k-means.</p>
"
"0.153332879014365","0.139211511597426"," 55232","<p>I am trying to fit a finite mixture model to a dependent variable which is bounded (practically) between -0.594 and 1 (theoretically, the latent variable is bounded between -Inf - 1). The data are also bimodal, with a large number of values at '1'. The objective of the analysis is prediction of the dependent variable.</p>

<p>My current approach has been to fit a mixture of normal distributions using the <code>flexmix</code> package in R, but I'd really like to account for the bounded nature of the data, as a recent study found this to be important (I also choose k=3 components based on this study). Using <code>flexmix</code> for truncated data appears non-trivial, as suggested <a href=""http://r.789695.n4.nabble.com/model-based-clustering-with-flexmix-td908418.html"" rel=""nofollow"">here</a>.</p>

<p>Is there an R package that will permit mixture models with bounded data? I've noticed that actually predicted values do not seem to fall outside the bounded range; i.e. predicted values are not in practice greater than 1. Is this just a fluke of my data, or is it a feature of the methods I've used? Is the bounding even a problem in this context?</p>

<p>As an alternative, I've tried transforming the data by simply taking 1-the dependent variable, thereby giving me a (zero-inflated) variable bounded by 0 and Inf which I have tried to model as a mixture of zero-inflated poisson models but I get the error:    </p>

<pre><code>Error in FLXfit(model = model, concomitant = concomitant, control = control,  
: 1 Log-    likelihood: NaN
</code></pre>

<p>Is it possible to model non-integers with the poisson family in this context? Any suggestions or thoughts would be greatly appreciated, I'm very new to mixture modelling and indeed GLMs etc.</p>

<p>Here's some simulated data: <a href=""https://dl.dropbox.com/u/65336009/mydata.csv"" rel=""nofollow"">https://dl.dropbox.com/u/65336009/mydata.csv</a></p>

<p>Here's my code:</p>

<pre><code>require(flexmix)
require(ggplot2)
mydata &lt;- data.frame(read.csv(""mydata.csv"", head=T))
attach(mydata)

#Plot of y var
summary(y)
ggplot(mydata, aes(y)) + geom_histogram(binwidth = .1)

#Simplified example of my current 'best' approach####
m1 &lt;- flexmix(y ~ x1 + x2 + x3,
              data = mydata,
              k = 3)

#Predict cluster membership
clusters &lt;- data.frame(clusters(m1, newdata = mydata))

#Predict y
a &lt;- data.frame(predict(m1, newdata = mydata))

#Select prediction based on predicted cluster membership
mydata$flexmix.norm &lt;- ifelse(clusters[,1]==1, a[,1],
                                   ifelse(clusters[,1] == 2,
                                          a[,2], a[,3]))
    print(max(mydata$flexmix.norm))

#Plot predicted values
ggplot(mydata, aes(flexmix.norm)) + geom_histogram(binwidth = .1)

#Maybe it's more natural to model as 1 - y, which is bounded (0,Inf) ####
y.d &lt;- 1 - y
ggplot(mydata, aes(y.d)) + geom_histogram(binwidth = .1)

#Error here ***
m2 &lt;- flexmix(y.d ~ x1 + x2 + x3,
              data = mydata,
              k = 3,
              model=FLXMRziglm(family=""poisson""))
rm2 = refit(m2)

#Predict cluster membership
clusters &lt;- NULL
clusters &lt;- data.frame(clusters(m2, newdata = mydata))

#Predict y (note back on original scale of y)
b &lt;- 1 - data.frame(predict(m2, newdata = mydata))

#Select prediction based on predicted cluster membership
preds$flexmix.pois &lt;- ifelse(clusters[,1]==1, b[,1],
                              ifelse(clusters[,1] == 2,
                                     b[,2], b[,3]))

ggplot(mydata, aes(flexmix.pois)) + geom_histogram(binwidth = .1)
</code></pre>

<p>Thanks</p>
"
"0.104746297470869","0.0815139145939222"," 55854","<p>I have a set of 2-D data where I want to find the centers of a specified number of centers of circles ($N$) that maximize the total number of points within a specified distance ($R$).</p>

<p>e.g. I have 10,000 data points $(X_i, Y_i)$ and I want to find the centers of $N=5$ circles that capture as many points as possible within a radius of $R=10$.  The 5 centers and radius of 10 are given beforehand, not derived from the data.</p>

<p>The presence of a data point within a circle is a binary either/or proposition.  If $R=10$, there's no difference in value to a point 11 units away vs. 100 units away, as they are both > 10.  Similarly for being within the circle, there's no extra value to being near the center vs. near the edge.  A data point is either in one of the circles or out.</p>

<p>Is there a good algorithm that can be used to solve this problem?  These seems related to  clustering techniques, but rather than minimizing the average distance, the ""distance"" function is 0 if the point is within $R$ of any of the $N$ points, and 1 otherwise.</p>

<p>My preference would be to find a way to do this in R, but any approach would be appreciated.</p>
"
"0.0395903791232448","0.0539163866017192"," 56210","<p>I'm using TraMineR to determine a certain clustering based on Optimal Matching distances:</p>

<pre><code># Define a sequence object
sequences.seq &lt;- seqdef(sequences, left=""DEL"", right=""DEL"", gaps=""DEL"", missing="""")

# Compute OM costs
costs &lt;- seqsubm(sequences.seq, method=""TRATE"")

# Create OM object
sequences.om &lt;- seqdist(sequences.seq, method=""OM"", indel=1, sm=costs, with.missing=FALSE, norm=""maxdist"")

# Use OM object to create a dendrogram
clusterward &lt;- agnes(sequences.om, diss = TRUE, method = ""ward"")
plot(clusterward, labels=colnames(sequences))
</code></pre>

<p>This gives me a plot of a dendrogram. Now I want to dummycode the cases in my dataset dependent on which cluster they fall in. How can I accomplish this?</p>
"
"0.0559892510955854","0.0762492851663023"," 56479","<p>I want to do clustering of my data in R, using kmeans or hclust (I am a new R user).</p>

<p>My data is ordinal, Likert scale, to measure the causes of cost escalation. I have 41 causes ""variables"" that scaled from 1 to 5 (1: no effect, 5: major effect). I have about 160 observations ""who rank the causes"".</p>

<p>I would like to cluster the variables (the columns) in terms of similarity of occurrence in observations, but I don't know how to start.</p>

<p>Do I have to convert the scale to percentage or z-score before clustering?</p>

<p>My data is available and shared as a <a href=""https://docs.google.com/spreadsheet/ccc?key=0AlrR2eXjV8nXdGtLdlYzVk01cE96Rzg2NzRpbEZjUFE&amp;usp=sharing"" rel=""nofollow"">Google Drive spreadsheet</a>.</p>
"
"0.104746297470869","0.0815139145939222"," 58071","<p>I have multiple <code>kmeans</code> plots that I have generated in R. Specifically I have $5$ weeks and I generate $1$ <code>kmeans</code> plot per week. I am clustering on vectors. Most vectors in the $5$ <code>kmeans</code> plots will occur in each plot. What I am interested in determining is which vectors have changed cluster membership. To make this clear suppose I have a vector identified by the word ""soccer"" then I would like to see which cluster it belongs to in week1, week2, and so forth.</p>

<p>Testing for membership change should be a simple task, for each <code>kmeans</code> clustering, each point is given an ID as to which cluster it belongs. I have 4 clusters so each week our example vector with name ""soccer"" could be tagged $1$, $2$, $3$, or $4$. The naive solution would be to just check the tag for a particular vector each week. However, this is not the case because R randomly selects the tags for each cluster. I know this is the case because each cluster represents some class of curves. You can visibly see that the kmeans algorithm has partitioned the vectors into 4 classes of curves. </p>

<p>Are there ways to make the tag IDs for each cluster stay constant? That is if the cluster tagged in week1 with ID $2$ is the linear curve, then the clusters tagged $2$ in all remaining weeks will always be the linear cluster.</p>

<p>Are there any initial conditions I can pass to <code>kmeans</code> to make this happen? I posted this question here instead of stackoverflow because I believe this questions requires more understanding of the <code>kmeans</code> algorithm.</p>
"
"0.0685725481323742","0.0933859209547035"," 58725","<p>I am experimenting with creating a distance matrix between time series for clustering and similarity searching. The main reference I am using is for the Similarity procedure in SAS (<a href=""http://support.sas.com/rnd/app/ets/papers/similarityanalysis.pdf%E2%80%8E"" rel=""nofollow"">Paper</a>). I would like to conduct the analysis in R using the <code>dtw</code> <a href=""http://dtw.r-forge.r-project.org/"" rel=""nofollow"">package</a>.</p>

<p>What I am confused about it the application of DTW to series of different lengths.</p>

<p>1) Is this possible?
2) Any hints on how to do it with the R package?</p>

<p>Regarding question #2, trying to calculate a distance matrix on a matrix of series that differ in length immediately fails:</p>

<pre><code>dist_mat&lt;-dist(appliances_t,method=""DTW"")

Error in dtw(distance.only = TRUE, ...) : 
  No warping paths exists that is allowed by costraints
</code></pre>

<blockquote>
  <p>dput(appliances_t)</p>
</blockquote>

<pre><code>structure(c(1L, 14L, 1L, 1L, 2L, 1L, 1L, 7L, 1L, 33L, 20L, 1L, 
1L, 8L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 14L, 
0L, 1L, 0L, 1L, 1L, 6L, 1L, 32L, 20L, 1L, 2L, 8L, 0L, 0L, 2L, 
1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 0L, 19L, 0L, 3L, 6L, 1L, 1L, 
7L, 1L, 42L, 27L, 1L, 3L, 10L, 0L, 1L, 3L, 1L, 3L, 1L, 1L, 1L, 
0L, 0L, 1L, 1L, 22L, 1L, 7L, 4L, 1L, 5L, 7L, 1L, 51L, 32L, 5L, 
4L, 12L, 1L, 1L, 9L, 5L, 7L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 33L, 
1L, 6L, 4L, 3L, 5L, 5L, 1L, 80L, 49L, 5L, 5L, 19L, 1L, 1L, 9L, 
5L, 7L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 28L, 1L, 7L, 8L, 3L, 5L, 
6L, 3L, 63L, 41L, 5L, 6L, 15L, 1L, 1L, 9L, 5L, 8L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 30L, 1L, 10L, 22L, 10L, 8L, 7L, 5L, 70L, 44L, 
8L, 7L, 16L, 1L, 1L, 12L, 8L, 11L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 
32L, 2L, 13L, 30L, 10L, 10L, 11L, 12L, 74L, 47L, 10L, 8L, 18L, 
2L, 1L, 16L, 10L, 14L, 1L, 1L, 1L, 1L, 2L, 1L, 5L, 23L, 4L, 6L, 
30L, 10L, 5L, 9L, 12L, 53L, 33L, 5L, 9L, 12L, 5L, 1L, 9L, 5L, 
8L, 5L, 5L, 5L, 3L, 5L, 5L, 7L, 27L, 7L, 9L, 22L, 14L, 7L, 11L, 
12L, 61L, 36L, 7L, 10L, 14L, 7L, 1L, 11L, 7L, 11L, 8L, 8L, 8L, 
6L, 7L, 8L, 14L, 27L, 14L, 13L, 38L, 19L, 10L, 7L, 15L, 61L, 
37L, 10L, 11L, 14L, 14L, 1L, 14L, 10L, 14L, 12L, 12L, 12L, 9L, 
14L, 12L, 14L, 38L, 14L, 14L, 44L, 10L, 11L, 9L, 20L, 86L, 54L, 
11L, 12L, 20L, 14L, 1L, 16L, 11L, 14L, 22L, 22L, 22L, 16L, 14L, 
22L, 14L, 27L, 14L, 16L, 42L, 13L, 13L, 12L, 11L, 61L, 39L, 13L, 
13L, 14L, 14L, 1L, 19L, 13L, 18L, 22L, 22L, 22L, 16L, 14L, 22L, 
20L, 28L, 19L, 10L, 40L, 16L, 9L, 12L, 14L, 66L, 41L, 9L, 14L, 
16L, 20L, 1L, 13L, 9L, 12L, 23L, 23L, 23L, 18L, 20L, 23L, 27L, 
16L, 27L, 10L, 60L, 19L, 8L, 11L, 20L, 39L, 23L, 8L, 15L, 9L, 
27L, 4L, 12L, 8L, 11L, 30L, 30L, 30L, 22L, 27L, 30L, 14L, 24L, 
14L, 14L, 136L, 21L, 11L, 4L, 21L, 56L, 33L, 11L, 16L, 12L, 14L, 
3L, 16L, 11L, 14L, 42L, 42L, 42L, 31L, 14L, 42L, 18L, 23L, 18L, 
16L, 206L, 14L, 14L, 3L, 25L, 54L, 33L, 14L, 17L, 12L, 18L, 2L, 
20L, 14L, 18L, 22L, 22L, 22L, 16L, 18L, 22L, 24L, 28L, 23L, 25L, 
398L, 14L, 20L, 6L, 16L, 65L, 40L, 20L, 18L, 16L, 24L, 4L, 29L, 
20L, 28L, 28L, 28L, 28L, 21L, 24L, 28L, 27L, 18L, 27L, 20L, 380L, 
19L, 16L, 5L, 16L, 40L, 25L, 16L, 19L, 10L, 27L, 3L, 25L, 16L, 
21L, 39L, 39L, 39L, 28L, 27L, 39L, 32L, 19L, 30L, 21L, 406L, 
22L, 18L, 3L, 20L, 42L, 27L, 18L, 20L, 10L, 32L, 3L, 27L, 18L, 
24L, 43L, 43L, 43L, 32L, 32L, 43L, 20L, 21L, 20L, 22L, 504L, 
33L, 19L, 7L, 27L, 49L, 31L, 19L, 21L, 12L, 20L, 2L, 28L, 19L, 
27L, 49L, 49L, 49L, 37L, 20L, 49L, 20L, 20L, 20L, 16L, 682L, 
28L, 14L, 7L, 41L, 48L, 30L, 14L, 22L, 12L, 20L, 2L, 20L, 14L, 
18L, 33L, 33L, 33L, 25L, 20L, 33L, 27L, 13L, 27L, 19L, 374L, 
30L, 14L, 8L, 32L, 29L, 19L, 14L, 23L, 6L, 27L, 3L, 21L, 14L, 
20L, 32L, 32L, 32L, 23L, 27L, 32L, 32L, 20L, 32L, 19L, 489L, 
32L, 14L, 7L, 33L, 47L, 28L, 14L, 24L, 11L, 32L, 3L, 21L, 14L, 
20L, 42L, 42L, 42L, 32L, 32L, 42L, 49L, 16L, 49L, 27L, 628L, 
23L, 21L, 13L, 36L, 35L, 21L, 21L, 25L, 8L, 49L, 4L, 32L, 21L, 
30L, 51L, 51L, 51L, 40L, 49L, 51L, 41L, 27L, 41L, 19L, 791L, 
27L, 15L, 14L, 27L, 62L, 39L, 15L, 26L, 14L, 41L, 1L, 22L, 15L, 
20L, 80L, 80L, 80L, 61L, 41L, 80L, 44L, 33L, 44L, 20L, 898L, 
27L, 16L, 16L, 29L, 77L, 48L, 16L, 27L, 18L, 44L, 1L, 25L, 16L, 
21L, 63L, 63L, 63L, 48L, 44L, 63L, 47L, 21L, 46L, 13L, 439L, 
38L, 10L, 21L, 30L, 51L, 32L, 10L, 28L, 12L, 47L, 2L, 14L, 10L, 
14L, 70L, 70L, 70L, 51L, 47L, 70L, 33L, 32L, 33L, 18L, 515L, 
27L, 14L, 22L, 43L, 75L, 47L, 14L, 29L, 18L, 33L, 2L, 20L, 14L, 
20L, 74L, 74L, 74L, 54L, 33L, 74L, 36L, 33L, 36L, 16L, 450L, 
28L, 14L, 17L, 30L, 76L, 48L, 14L, 30L, 18L, 36L, 3L, 20L, 14L, 
18L, 53L, 53L, 53L, 40L, 36L, 53L, 37L, 33L, 37L, 20L, 726L, 
16L, 16L, 14L, 32L, 78L, 48L, 16L, 31L, 19L, 37L, 3L, 24L, 16L, 
21L, 61L, 61L, 61L, 45L, 37L, 61L, 54L, 48L, 53L, 13L, 1069L, 
24L, 10L, 17L, 19L, 109L, 68L, 10L, 32L, 27L, 54L, 3L, 14L, 10L, 
14L, 61L, 61L, 61L, 45L, 54L, 61L, 39L, 51L, 37L, 13L, 889L, 
23L, 11L, 19L, 28L, 120L, 76L, 11L, 33L, 28L, 39L, 3L, 16L, 11L, 
14L, 86L, 86L, 86L, 63L, 39L, 86L, 41L, 43L, 40L, 16L, 1083L, 
28L, 13L, 22L, 28L, 97L, 62L, 13L, 34L, 22L, 41L, 3L, 19L, 13L, 
18L, 61L, 61L, 61L, 46L, 41L, 61L, 23L, 50L, 23L, 15L, 1110L, 
18L, 13L, 25L, 32L, 118L, 73L, 12L, 35L, 28L, 23L, 2L, 19L, 13L, 
17L, 66L, 66L, 66L, 49L, 23L, 66L, 33L, 80L, 33L, 10L, 803L, 
19L, 7L, 32L, 20L, 185L, 116L, 7L, 36L, 43L, 33L, 7L, 11L, 7L, 
11L, 39L, 39L, 39L, 28L, 33L, 39L, 33L, 63L, 33L, 14L, 828L, 
21L, 12L, 33L, 20L, 146L, 91L, 12L, 37L, 34L, 33L, 9L, 18L, 12L, 
16L, 56L, 56L, 56L, 42L, 33L, 56L, 40L, 68L, 40L, 11L, 1001L, 
20L, 9L, 29L, 25L, 157L, 97L, 9L, 38L, 35L, 40L, 11L, 14L, 9L, 
13L, 54L, 54L, 54L, 41L, 40L, 54L, 25L, 80L, 24L, 20L, 1281L, 
13L, 16L, 32L, 23L, 181L, 113L, 16L, 39L, 43L, 25L, 3L, 22L, 
16L, 21L, 65L, 65L, 65L, 48L, 25L, 65L, 27L, 68L, 27L, 24L, 918L, 
20L, 20L, 32L, 14L, 155L, 96L, 20L, 40L, 35L, 27L, 1L, 29L, 20L, 
27L, 40L, 40L, 40L, 29L, 27L, 40L, 31L, 85L, 30L, 16L, 974L, 
16L, 14L, 45L, 23L, 194L, 120L, 13L, 41L, 47L, 31L, 4L, 20L, 
14L, 18L, 42L, 42L, 42L, 31L, 31L, 42L, 30L, 76L, 29L, 23L, 524L, 
27L, 19L, 51L, 18L, 172L, 109L, 19L, 42L, 40L, 30L, 7L, 28L, 
19L, 27L, 49L, 49L, 49L, 36L, 30L, 49L, 19L, 74L, 19L, 23L, 486L, 
33L, 20L, 40L, 30L, 168L, 105L, 19L, 43L, 39L, 19L, 3L, 28L, 
20L, 27L, 48L, 48L, 48L, 36L, 19L, 48L, 28L, 80L, 28L, 24L, 380L, 
21L, 20L, 51L, 39L, 185L, 115L, 20L, 44L, 43L, 28L, 3L, 29L, 
20L, 28L, 29L, 29L, 29L, 21L, 28L, 29L, 21L, 131L, 21L, 33L, 
456L, 32L, 27L, 54L, 27L, 300L, 188L, 27L, 45L, 70L, 21L, 3L, 
40L, 27L, 38L, 47L, 47L, 47L, 34L, 21L, 47L, 39L, 139L, 38L, 
37L, 360L, 33L, 30L, 63L, 37L, 315L, 198L, 30L, 46L, 74L, 39L, 
3L, 47L, 30L, 41L, 35L, 35L, 35L, 27L, 39L, 35L, 48L, 197L, 48L, 
30L, 488L, 33L, 24L, 90L, 38L, 449L, 280L, 24L, 47L, 104L, 48L, 
8L, 36L, 24L, 33L, 62L, 62L, 62L, 47L, 48L, 62L, 32L, 175L, 32L, 
36L, 622L, 48L, 29L, 86L, 39L, 401L, 249L, 29L, 48L, 95L, 32L, 
3L, 44L, 29L, 40L, 77L, 77L, 77L, 58L, 32L, 77L, 47L, 147L, 47L, 
58L, 522L, 51L, 47L, 63L, 53L, 336L, 211L, 47L, 49L, 79L, 47L, 
3L, 69L, 47L, 64L, 51L, 51L, 51L, 40L, 47L, 51L, 48L, 212L, 48L, 
47L, 306L, 43L, 36L, 76L, 60L, 483L, 304L, 35L, 50L, 114L, 48L, 
3L, 54L, 36L, 50L, 75L, 75L, 75L, 56L, 48L, 75L, 48L, 175L, 48L, 
48L, 628L, 50L, 39L, 106L, 48L, 401L, 249L, 39L, 51L, 95L, 48L, 
0L, 58L, 39L, 52L, 76L, 76L, 76L, 57L, 48L, 76L, 68L, 186L, 67L, 
56L, 320L, 80L, 47L, 85L, 59L, 426L, 266L, 47L, 52L, 100L, 68L, 
1L, 68L, 47L, 63L, 78L, 78L, 78L, 59L, 68L, 78L, 76L, 144L, 75L, 
48L, 646L, 63L, 39L, 69L, 92L, 332L, 207L, 39L, 53L, 77L, 76L, 
2L, 58L, 39L, 52L, 109L, 109L, 109L, 80L, 76L, 109L, 62L, 140L, 
61L, 61L, 790L, 68L, 48L, 83L, 72L, 323L, 200L, 48L, 54L, 76L, 
62L, 3L, 72L, 48L, 66L, 120L, 120L, 120L, 91L, 62L, 120L, 73L, 
170L, 72L, 54L, 556L, 80L, 43L, 96L, 78L, 390L, 244L, 43L, 55L, 
91L, 73L, 6L, 63L, 43L, 60L, 97L, 97L, 97L, 73L, 73L, 97L, 116L, 
157L, 114L, 51L, 552L, 68L, 43L, 113L, 91L, 357L, 224L, 43L, 
56L, 84L, 116L, 7L, 63L, 43L, 58L, 118L, 118L, 118L, 88L, 116L, 
118L, 91L, 225L, 91L, 58L, 806L, 85L, 47L, 96L, 78L, 512L, 321L, 
47L, 57L, 119L, 91L, 5L, 68L, 47L, 65L, 185L, 185L, 185L, 138L, 
91L, 185L, 97L, 103L, 96L, 95L, 855L, 76L, 75L, 108L, 96L, 236L, 
148L, 75L, 58L, 56L, 97L, NA, 112L, 76L, 104L, 146L, 146L, 146L, 
109L, 97L, 146L, 113L, 128L, 112L, 97L, 1050L, 74L, 78L, 116L, 
86L, 294L, 184L, 78L, 59L, 69L, 113L, NA, 118L, 78L, 110L, 157L, 
157L, 157L, 117L, 113L, 157L, 96L, 140L, 96L, 140L, 1120L, 80L, 
112L, 123L, 83L, 321L, 199L, 112L, 60L, 75L, 96L, NA, 167L, 112L, 
155L, 181L, 181L, 181L, 137L, 96L, 181L, 120L, 146L, 119L, 126L, 
908L, 131L, 99L, 192L, 92L, 334L, 209L, 99L, 61L, 78L, 120L, 
NA, 150L, 99L, 138L, 155L, 155L, 155L, 117L, 120L, 155L, 109L, 
150L, 107L, 105L, 1068L, 139L, 84L, 373L, 150L, 343L, 214L, 83L, 
62L, 80L, 109L, NA, 126L, 84L, 117L, 194L, 194L, 194L, 146L, 
109L, 194L, 105L, 158L, 104L, 151L, 1696L, 197L, 120L, 195L, 
158L, 363L, 227L, 120L, 63L, 84L, 105L, NA, 181L, 121L, 168L, 
172L, 172L, 172L, 129L, 105L, 172L, 115L, 160L, 115L, 125L, 1658L, 
175L, 99L, 213L, 224L, 366L, 228L, 99L, 64L, 86L, 115L, NA, 150L, 
100L, 138L, 168L, 168L, 168L, 126L, 115L, 168L, 188L, 179L, 186L, 
132L, 1872L, 147L, 106L, 215L, 199L, 408L, 256L, 106L, 65L, 96L, 
188L, NA, 160L, 106L, 149L, 185L, 185L, 185L, 138L, 188L, 185L, 
198L, 148L, 195L, 103L, 2262L, 212L, 81L, 232L, 167L, 338L, 212L, 
81L, 66L, 80L, 198L, NA, 125L, 81L, 115L, 300L, 300L, 300L, 226L, 
198L, 300L, 280L, 145L, 278L, 100L, 2120L, 175L, 80L, 318L, 242L, 
332L, 209L, 80L, 67L, 138L, 280L, NA, 120L, 80L, 112L, 315L, 
315L, 315L, 236L, 280L, 315L, 249L, 151L, 248L, 122L, 2215L, 
186L, 97L, 184L, 199L, 348L, 226L, 97L, 68L, 137L, 249L, NA, 
146L, 97L, 137L, 449L, 449L, 449L, 337L, 249L, 449L, 211L, 166L, 
208L, 111L, 1756L, 144L, 91L, 195L, 213L, 402L, 434L, 91L, 69L, 
190L, 211L, NA, 134L, 91L, 124L, 401L, 401L, 401L, 299L, 211L, 
401L, 304L, 159L, 299L, 160L, 2010L, 140L, 128L, 243L, 165L, 
422L, 380L, 128L, 70L, 149L, 304L, NA, 192L, 128L, 177L, 336L, 
336L, 336L, 253L, 304L, 336L, 249L, 188L, 248L, 74L, 2278L, 170L, 
60L, 242L, 160L, 492L, 501L, 59L, 71L, 127L, 249L, NA, 89L, 60L, 
81L, 483L, 483L, 483L, 364L, 249L, 483L, 266L, 119L, 263L, 92L, 
3064L, 157L, 74L, 264L, 195L, 403L, 359L, 74L, 72L, 128L, 266L, 
NA, 110L, 74L, 102L, 401L, 401L, 401L, 300L, 266L, 401L, 207L, 
237L, 206L, 99L, 3001L, 225L, 80L, 199L, 177L, 892L, 719L, 80L, 
73L, 291L, 207L, NA, 119L, 80L, 112L, 426L, 426L, 426L, 319L, 
207L, 426L, 200L, 383L, 199L, 104L, 4429L, 103L, 83L, 173L, 256L, 
1418L, 710L, 82L, 74L, 342L, 200L, NA, 126L, 83L, 117L, 332L, 
332L, 332L, 247L, 200L, 332L, 244L, 163L, 242L, 109L, 4118L, 
128L, 86L, 207L, 118L, 1011L, 541L, 86L, 75L, 223L, 244L, NA, 
128L, 86L, 118L, 323L, 323L, 323L, 241L, 244L, 323L, 224L, 463L, 
221L, 113L, 3112L, 140L, 91L, 152L, 145L, 1268L, 436L, 91L, 76L, 
276L, 224L, NA, 137L, 91L, 125L, 390L, 390L, 390L, 293L, 224L, 
390L, 321L, 325L, 318L, 115L, 3968L, 146L, 92L, 173L, 160L, 1112L, 
286L, 92L, 77L, 278L, 321L, NA, 138L, 92L, 125L, 357L, 357L, 
357L, 268L, 321L, 357L, 148L, 579L, 145L, 127L, 3395L, 150L, 
102L, 146L, 166L, 1606L, 382L, 102L, 78L, 253L, 148L, NA, 151L, 
102L, 141L, 512L, 512L, 512L, 385L, 148L, 512L, 184L, 431L, 181L, 
106L, 3693L, 158L, 84L, 158L, 171L, 1221L, 432L, 83L, 79L, 324L, 
184L, NA, 126L, 85L, 118L, 236L, 236L, 236L, 178L, 184L, 236L, 
199L, 833L, 198L, 103L, 3038L, 160L, 82L, 145L, 179L, 814L, 210L, 
81L, 80L, 243L, 199L, NA, 125L, 82L, 115L, 294L, 294L, 294L, 
220L, 199L, 294L, 209L, 580L, 207L, 109L, 2303L, 179L, 87L, 156L, 
182L, 952L, 280L, 87L, 81L, 244L, 209L, NA, 131L, 88L, 120L, 
321L, 321L, 321L, 240L, 209L, 321L, 1063L, 654L, 212L, 118L, 
2959L, 148L, 95L, 162L, 202L, 920L, 332L, 95L, 82L, 202L, 214L, 
NA, 142L, 95L, 130L, 334L, 334L, 334L, 249L, 214L, 334L, 1535L, 
950L, 225L, 113L, 2509L, 306L, 91L, 297L, 167L, 907L, 393L, 91L, 
83L, 201L, 227L, NA, 137L, 91L, 125L, 343L, 343L, 343L, 258L, 
227L, 343L, 1705L, 1252L, 227L, 135L, 2718L, 397L, 109L, 332L, 
166L, 804L, 320L, 108L, 84L, 121L, 228L, NA, 162L, 109L, 150L, 
363L, 363L, 363L, 271L, 228L, 363L, 1203L, 1192L, 253L, 95L, 
1813L, 481L, 76L, 451L, 174L, 869L, 308L, 75L, 85L, 91L, 256L, 
NA, 113L, 76L, 104L, 366L, 366L, 366L, 275L, 256L, 366L, 1176L, 
1127L, 210L, 140L, 2103L, 441L, 111L, 425L, 188L, 872L, 447L, 
111L, 86L, 160L, 212L, NA, 167L, 111L, 154L, 408L, 408L, 408L, 
306L, 212L, 408L, 943L, 1168L, 206L, 176L, 2156L, 600L, 141L, 
346L, 179L, 980L, 478L, 141L, 87L, 169L, 209L, NA, 213L, 141L, 
197L, 338L, 338L, 338L, 256L, 209L, 338L, 685L, 1469L, 215L, 
129L, 2212L, 414L, 104L, 373L, 214L, 1195L, 764L, 104L, 88L, 
234L, 217L, NA, 157L, 104L, 143L, 332L, 332L, 332L, 248L, 217L, 
332L, 951L, 690L, 234L, 164L, 1995L, 358L, 130L, 522L, 150L, 
849L, 441L, 129L, 89L, 145L, 236L, NA, 196L, 130L, 182L, 348L, 
348L, 348L, 261L, 236L, 348L, 779L, NA, 227L, 147L, 1995L, 648L, 
118L, 398L, 222L, NA, NA, 118L, 90L, NA, 228L, NA, 178L, 118L, 
165L, 378L, 378L, 378L, 283L, 228L, 378L, 853L, NA, 266L, 167L, 
2252L, 352L, 133L, 437L, 282L, NA, NA, 135L, 91L, NA, 270L, NA, 
202L, 136L, 188L, 364L, 364L, 364L, 274L, 270L, 364L, 593L, NA, 
186L, 213L, 2142L, 415L, 160L, 346L, 208L, NA, NA, 170L, 92L, 
NA, 188L, NA, 257L, 170L, 236L, 430L, 430L, 430L, 324L, 188L, 
430L, 844L, NA, 276L, 129L, 1933L, 584L, 135L, 423L, 261L, NA, 
NA, 104L, 93L, NA, 280L, NA, 157L, 104L, 143L, 301L, 301L, 301L, 
227L, 280L, 301L, 1055L, NA, 352L, 150L, 1973L, 606L, 177L, 507L, 
236L, NA, NA, 119L, 94L, NA, 354L, NA, 180L, 119L, 167L, 447L, 
447L, 447L, 334L, 354L, 447L, 773L, NA, 258L, 159L, 1656L, 1012L, 
281L, 399L, 269L, NA, NA, 126L, 95L, NA, 261L, NA, 191L, 127L, 
175L, 567L, 567L, 567L, 424L, 261L, 567L, 722L, NA, 324L, 167L, 
1883L, 617L, 267L, 430L, 340L, NA, NA, 134L, 96L, NA, 326L, NA, 
202L, 134L, 187L, 417L, 417L, 417L, 311L, 326L, 417L, 687L, NA, 
293L, 169L, 1788L, 530L, 275L, 444L, 206L, NA, NA, 136L, 97L, 
NA, 295L, NA, 205L, 136L, 189L, 521L, 521L, 521L, 392L, 295L, 
521L, 711L, NA, 334L, 186L, 1763L, 597L, 234L, 460L, 240L, NA, 
NA, 147L, 98L, NA, 338L, NA, 222L, 147L, 205L, 473L, 473L, 473L, 
354L, 338L, 473L, 889L, NA, 422L, 198L, 1337L, 446L, 303L, 584L, 
253L, NA, NA, 166L, 99L, NA, 427L, NA, 237L, 158L, 220L, 540L, 
540L, 540L, 405L, 427L, 540L, 635L, NA, 258L, 310L, 1749L, 558L, 
396L, 600L, 269L, NA, NA, 239L, 100L, NA, 261L, NA, 373L, 247L, 
345L, 683L, 683L, 683L, 512L, 261L, 683L, 669L, NA, 298L, 359L, 
1960L, 590L, 608L, 588L, 271L, NA, NA, 372L, 101L, NA, 303L, 
NA, 431L, 288L, 398L, 416L, 416L, 416L, 311L, 303L, 416L, 712L, 
NA, 315L, 214L, 1383L, 576L, 355L, 893L, 295L, NA, NA, 256L, 
102L, NA, 319L, NA, 257L, 170L, 236L, 482L, 482L, 482L, 361L, 
319L, 482L, 727L, NA, 334L, NA, 1631L, 730L, NA, 864L, 316L, 
NA, NA, NA, 103L, NA, 336L, NA, NA, NA, NA, 509L, 509L, 509L, 
382L, 336L, 509L, 887L, NA, 339L, NA, 1513L, 736L, NA, 580L, 
496L, NA, NA, NA, 104L, NA, 340L, NA, NA, NA, NA, 539L, 539L, 
539L, 403L, 340L, 539L, 794L, NA, 369L, NA, 1687L, 647L, NA, 
632L, 572L, NA, NA, NA, 105L, NA, 264L, NA, NA, NA, NA, 546L, 
546L, 546L, 408L, 325L, 546L, 838L, NA, 392L, NA, 2145L, NA, 
NA, 712L, 340L, NA, NA, NA, 106L, NA, 357L, NA, NA, NA, NA, 594L, 
594L, 382L, 443L, 462L, 594L, 880L, NA, 616L, NA, 1522L, NA, 
NA, 700L, NA, NA, NA, NA, 107L, NA, 456L, NA, NA, NA, NA, 634L, 
634L, 424L, 476L, 447L, 634L, 1030L, NA, 712L, NA, 1578L, NA, 
NA, 703L, NA, NA, NA, NA, 108L, NA, 873L, NA, NA, NA, NA, 996L, 
996L, 497L, 746L, 518L, 996L, 842L, NA, 256L, NA, 1546L, NA, 
NA, 753L, NA, NA, NA, NA, 109L, NA, 337L, NA, NA, NA, NA, 1151L, 
1151L, 515L, 862L, 465L, 1151L, NA, NA, NA, NA, 1685L, NA, NA, 
699L, NA, NA, NA, NA, 110L, NA, NA, NA, NA, NA, NA, 685L, 685L, 
310L, 512L, NA, 685L, NA, NA, NA, NA, 1726L, NA, NA, 857L, NA, 
NA, NA, NA, 111L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, 1561L, NA, NA, 1099L, NA, NA, NA, NA, 112L, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
1411L, NA, NA, 1156L, NA, NA, NA, NA, 113L, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1714L, NA, NA, 3876L, 
NA, NA, NA, NA, 114L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, 2258L, NA, NA, 1507L, NA, NA, NA, NA, 
115L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, 1442L, NA, NA, 1524L, NA, NA, NA, NA, 116L, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
2073L, NA, NA, NA, NA, 117L, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2725L, NA, NA, NA, 
NA, 118L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, 2255L, NA, NA, NA, NA, 119L, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, 1634L, NA, NA, NA, NA, 120L, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1360L, NA, NA, 
NA, NA, 121L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, 1270L, NA, NA, NA, NA, 122L, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, 1328L, NA, NA, NA, NA, 123L, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1470L, NA, 
NA, NA, NA, 124L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, 1040L, NA, NA, NA, NA, 125L, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, 1095L, NA, NA, NA, NA, 126L, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1823L, 
NA, NA, NA, NA, 127L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, 3221L, NA, NA, NA, NA, 128L, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, 3986L, NA, NA, NA, NA, 129L, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 3632L, 
NA, NA, NA, NA, 130L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, 3760L, NA, NA, NA, NA, 131L, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, 3225L, NA, NA, NA, NA, 132L, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 3473L, 
NA, NA, NA, NA, 133L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, 3359L, NA, NA, NA, NA, 134L, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, 2032L, NA, NA, NA, NA, 135L, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 3337L, 
NA, NA, NA, NA, 136L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, 2474L, NA, NA, NA, NA, 137L, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, 1748L, NA, NA, NA, NA, 138L, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1581L, 
NA, NA, NA, NA, 139L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, 2590L, NA, NA, NA, NA, 140L, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, 2769L, NA, NA, NA, NA, 141L, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2134L, 
NA, NA, NA, NA, 142L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, 2312L, NA, NA, NA, NA, 143L, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, 1822L, NA, NA, NA, NA, 144L, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1890L, 
NA, NA, NA, NA, 145L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, 1770L, NA, NA, NA, NA, 146L, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, 1664L, NA, NA, NA, NA, 147L, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1533L, 
NA, NA, NA, NA, 148L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, 1628L, NA, NA, NA, NA, 149L, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, 1682L, NA, NA, NA, NA, 150L, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1416L, 
NA, NA, NA, NA, 151L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, 1721L, NA, NA, NA, NA, 152L, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, 1732L, NA, NA, NA, NA, 153L, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1748L, 
NA, NA, NA, NA, 154L, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, 1312L, NA, NA, NA, NA, 155L, 
NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 
NA, NA, NA, 1659L, NA, NA, NA, NA, 156L, NA, NA, NA, NA, NA, 
NA, NA, NA, NA, NA, NA, NA), .Dim = c(25L, 156L), .Dimnames = list(
    c(""units_1"", ""units_2"", ""units_3"", ""units_4"", ""units_5"", 
    ""units_6"", ""units_7"", ""units_8"", ""units_9"", ""units_10"", ""units_11"", 
    ""units_12"", ""units_13"", ""units_14"", ""units_15"", ""units_16"", 
    ""units_17"", ""units_18"", ""units_19"", ""units_20"", ""units_21"", 
    ""units_22"", ""units_23"", ""units_24"", ""units_25""), NULL))
</code></pre>
"
"0.137145096264748","0.124514561272938"," 59080","<p>I've got a set of (continuous) values from a measurement, where each object should be either positive or negative, and I know that the values of the ""negative"" objects should be approximately normally distributed.</p>

<p>I've been using a <code>k-means</code>-based algorithm in <code>R</code> to cluster the data and thus to classify the objects. Here is a typical example of the clustering result, shown as a density plot:</p>

<p><img src=""http://i.stack.imgur.com/aB0Ob.png"" alt=""enter image description here""></p>

<p>As you can see, although the algorithm does distinguish two populations, it includes a part of the positive objects into the negative cluster (the blue one). You can also see that there are some extremely negative outliers, causing the blue curve to be skewed to the right. These outliers are artefacts, resulting from the method the values are measured and calculated, and I'd like to find a cut-off to exclude them in future.</p>

<p>Now, I'd like to fit a normal distribution curve the the negative population and to define a cut-off for those artefacts (outliers) based on this fit as well as to estimate some characteristics of the distribution. I was trying to do this by extracting only those objects that fall into the blue cluster and then fitting a normal distribution on them, but obviously this doesn't work out, mainly due to the bump on the right of the blue population. So I need another classification algorithm that would a priori assume the existence of two Gauss distributions and fit them (in the best case taking into account the possibility of the existence of outliers). However, so far I haven't been able to find an algorithm that would be reliable and fast enough (I've got around 20,000-50,000 objects), although I'm sure there must be a simple way in <code>R</code> to do so.</p>

<p>So in summary, I'm looking for a classiciation algorithm implemented in <code>R</code> to fit a normal distribution to the ""negative"" objects of my measurement, ideally taking into account that outliers do exist.</p>

<p>Does anyone has a suggestion how to accomplish this?</p>
"
"0.0559892510955854","0.0762492851663023"," 59467","<p>I am trying to learn the difference between the three approaches and their applications.</p>

<p>a) As I understand,</p>

<pre><code>AIC = -LL+K 

BIC = -LL+(K*logN)/2
</code></pre>

<p>Unless I am missing something, shouldn't the K that minimizes the AIC minimize BIC as well since N is constant. </p>

<p>I looked at this <a href=""http://stats.stackexchange.com/questions/577/is-there-any-reason-to-prefer-the-aic-or-bic-over-the-other/767#767"">thread</a> but couldn't find a satisfactory answer. </p>

<p>b) According to Witten's book on Data Mining (pg 267) the definition of MDL for evaluating the quality of network is the same as BIC. Is there a difference between BIC and MDL?</p>

<p>c) What are the different approaches to compute MDL? I am looking for its application in Clustering, Time Series Analysis (ARIMA and Regime Switching) and Attribute Selection. While almost all commonly used packages in R report AIC and BIC, I couldn't find any that implements MDL and I wanted to see if I can write it myself.</p>

<p>Thank you.</p>
"
"0.125195771459034","0.136398867894095"," 59469","<p>I have a set of samples, for which I know the ""true groups"". For this samples I have about 200 binary variables, I would like to know a method to select the subset of variables, that gives me a clustering as closer as possible of my known groups.</p>

<pre><code># sample labels
labelColors2 &lt;-c(""black"", ""black"",""black"",""black"",""black"",""black"",     ""blue"",""blue"",""blue"",""blue"",""green"", ""green"",
""red"",""red"",""red"",""red"",""red"",""red"",""red"",""red"",""red"",""red"",""red"",""red"")
# data matrix
library(RCurl)
x &lt;- getURL(""https://dl.dropboxusercontent.com/u/10712588/binMatrix"")
tab3 &lt;- read.table(text = x)
colLab &lt;- function(n) {
if(is.leaf(n)) {
a &lt;- attributes(n)
#clusMember a vector designating leaf grouping
#labelColors &lt;- colors # a vector of colors for the above grouping
labCol &lt;- labelColors2[clusMember[which(names(clusMember) == a$label)]]
    attr(n, ""nodePar"") &lt;- c(a$nodePar, list(lab.col = labCol,lab.cex=0.8))
}
n
}
mclust &lt;- hclust(dist(tab3, method =""binary""))
dhc &lt;- as.dendrogram(mclust)
clusMember &lt;- cutree(mclust, k=24)
clusDendro &lt;- dendrapply(dhc, colLab)
plot(clusDendro)
</code></pre>

<p><img src=""http://i.stack.imgur.com/CBNQq.png"" alt=""example of the true groups given by colours"">  </p>

<p>The colors should be grouped, this is my actual way to access the goodness of clustering, visually, but I would like to know a feature selection technique.</p>

<p>thks in advance...  </p>

<p>updating the question, I found the    <strong>klaR::stepclass</strong> function, that should to what I want, or some similar implementation, but I did not find a work around yet.</p>

<pre><code>fac &lt;- as.factor(labelColors2)
mylda &lt;- function(x, grouping) {
clust &lt;- pam(dist(x, method=""binary""), k=4,
    cluster.only = TRUE)
posterior &lt;- matrix(0, 24, 4) 
colnames(posterior) &lt;- c(""black"", ""blue"", ""green"", ""red"")
for(i in 1:nrow(posterior)) posterior[i, clust[i]] &lt;- 1 
l &lt;- list(class=grouping, posterior=posterior)
class(l) &lt;- ""foo""
return(l)

}
</code></pre>

<p>With the function above I can reproduce an output of my classification, similar to what <strong>klaR::ucpm</strong> needs, but I can't manage to run the function</p>

<pre><code>sc_obj &lt;- stepclass(x=tab3, grouping=fac, method=""mylda"", direction=""forward"")

Error in parse(text = x) : &lt;text&gt;:2:0: unexpected end of input
1: fac ~
  ^ 
</code></pre>

<p>Well, I think I had some improvement, I established a ""fitness function"", and with a random search (it is still running, I found a better clustering already</p>

<pre><code>predict.foo &lt;- function(x) x 
for(i in 1:1000000) {
s &lt;- sample(1:ncol(tab3),sample(68:200,1)) 
cr &lt;- ucpm(predict(mylda(tab3[,s], fac))$posterior, fac)$CR 
write.table(matrix(c(cr, s), nrow=1), ""randonSearch.txt"", append=TRUE,         row.names=FALSE, col.names=FALSE)
}
</code></pre>

<p>With this I'm monitoring the <strong>randonSearch.txt</strong> file with:</p>

<pre><code>cut -d "" "" -f1 ../randonSearch.txt | grep 0.8
</code></pre>

<p>I already found a ""Correctness Rate"" of 0.833, check it out</p>

<p><img src=""http://i.stack.imgur.com/a9kgV.png"" alt=""enter image description here""></p>

<p>I think there is still room for improvement, I'm thinking in a genetic algorithm... </p>
"
"0.0885267789745639","0.0964485644340824"," 59554","<p>I'm running an analysis on a few data sets that each typically have 100-200 cases measured across 120-160 variables - something similar to looking at gene expressions. Each variable is a non-centered score for expression of a particular attribute frequency for each case. In many cases though, any given attribute is likely to be 0 (i.e. very sparse for most). </p>

<p>The cases typically fall into 2-4 natural groups, and I'm trying to figure out how to find out which high-expression attributes are most representative for each group and/or which ones are driving the distinctions between group memberships. </p>

<p>I've been experimenting with using correlation clustering and the resulting groups <em>do</em> appear to match the natural groups assigned by other means, so now I'm just looking for a way to ""unpack"" those clusters in terms of their attribute expressions to find out which attributes/variables are the most influential ones in each of my samples. </p>

<p>Given the number of variables involved, it seems like the usual approaches like PCA or discriminate or factor analysis would be very cumbersome. So far, I haven't really found much information on how to deal with variable influence that would fit situations when there are scores or hundreds of variables.</p>

<p>Any suggestions?</p>
"
"0.131306432859723","0.130051217752471"," 62622","<p>I have as an input a number of points that I need to partition into clusters. Each point has a number of features that are ideally to be used to find the similarity between each point and the others. Some of these features are scalar values (one number) and others are vectors.</p>

<p>For example, assume that each point has the following features:</p>

<ol>
<li><p><strong>S1</strong>: scalar value</p></li>
<li><p><strong>V1</strong>: 48 $\times$ 1 vector</p></li>
<li><p><strong>V2</strong>: 48 $\times$ 1 vector</p></li>
</ol>

<p>For example one point may have (S1,V1, V2) as (100, {0, 100, 20, 30}, {75,0,10, 5})</p>

<p>My hypothesis is to use <a href=""http://en.wikipedia.org/wiki/Cosine_similarity"" rel=""nofollow"">cosine similarity</a> to find how similar the vector V1 or V2 of one point is to the vector V1 or V2 of another point. I have already computed the similarity matrices between all points in terms of V1 and V2 similarities.</p>

<p>By exploring the standard clustering algorithms in R, I have found that k-means turns to  use the Euclidean distance, which might be suitable for clustering points according to their scalar values, because [subject unclear] doesn't work for the situation where I have hybrid types of features (scalars and vectors). Also the K-medoid clustering seems to be supporting only the Euclidean and the Manhattan distances.</p>

<p>I think what should be done is to generate one more distance/similarity matrix between all points based on the scalar value, so that we end with three similarity matrices that show the similarity between each point and the other points according to each feature regardless of it being a scalar or a vector, and use those matrices for finding the neighbourhood of points while clustering.</p>

<p>I wonder if there is an implementation for a clustering algorithm that accepts as an input the similarity matrices (or alternatively the dissimilarity/distance matrices) between vector features of multiple points and uses them for clustering?</p>
"
"0.0791807582464896","0.0539163866017192"," 63546","<p><em>[The initial title ""Measurement of similarity for hierarchical clustering trees"" was later changed by @ttnphns to better reflect the topic]</em></p>

<p>I am performing a number of <strong>hierarchical cluster analyses</strong> on a dataframe of patient records (e.g. similar to <a href=""http://www.biomedcentral.com/1471-2105/5/126/figure/F1?highres=y"">http://www.biomedcentral.com/1471-2105/5/126/figure/F1?highres=y</a>) </p>

<p>I am experimenting with different <strong>distance</strong> measures, different parameter weights and different hierarcical <strong>methods</strong>, to understand their impact on the final clusters / structure/view of the tree (dendrogram). My question whether there is a standard calculation / measure to calculate the difference between different hierarchical trees, and how to implement this in R (e.g. to quantify that some trees are nearly identical, and that some are drastically different).</p>
"
"0.104746297470869","0.101892393242403"," 64131","<p>I am trying to apply R depmixS4 package in order to cluster time series with model based clustering. The model consists of K components, each being a first order Markov models. The Expectation-Maximization algorithm is then used to estimate model parameters. </p>

<p>My time series are multivariate and of arbitrary length ( i.e. can be 400, can be 1). </p>

<p>Now, there are 2 problems:</p>

<ol>
<li>depmixS4 is oriented towards Hidden Markov Models, not the basic first order ones. </li>
<li>I do not completely understand the E and M-steps of model based clustering when applied to first-order Markov model components. Overwhelmingly, scientific literature talks about Hidden Markov Models. </li>
</ol>

<p>However, it seems to me that simple first-order Markov models can be seen as a particular case of hidden ones (where the response probability distribution is just identity, response equals state - thus the states are actually visible). So the clustering process does not have to decode the hidden states. However, the E-M process itself is still unclear, and I am not sure if I can apply the depmixS4 package methods by adjusting them or should I develop my own algorithm in R. </p>

<p>The mixture Markov model used in my research, is the following:</p>

<p>$$p(v | \Theta) = \sum_{t=1\ldots K}p(c_k)p_k(v | \Theta_k),$$
 where $v=v_1,\ldots,v_L$ - vector of arbitrarily length, and 
$$p_k(v |\Theta_k)=p(v_1| \Theta_{k_i})\prod_{i=2\ldots L} p(v_i |v_{i-1}, \Theta_{k_T}).$$ 
Given are also vector of initial state prior distribution and transition probability matrix $T_p$. </p>
"
"0.104746297470869","0.0611354359454417"," 64723","<p>I have run a sequence analaysis using the Optimal Matching algorithm. Afterwards, I have clustered the resulting distance matrice using the Ward algorithm and calculated silhouettes as measures of cluster quality and to identify representative sequences. </p>

<p>Now, I am curious whether it is possible to estimate the sequences of the cluster centroids which, to my knowledege, must not be an original data point. How can I estimate the sequence of a centroid?</p>

<p>To get an idea of the different steps of the analysis, consider this manual example[1]:</p>

<pre><code>library(TraMineR) 
library(WeightedCluster) 
data(mvad) 
mvad.alphabet &lt;- c(""employment"", ""FE"", ""HE"", ""joblessness"", ""school"", ""training"") 
mvad.labels &lt;- c(""Employment"", ""Further Education"", ""Higher Education"", ""Joblessness"", ""School"", ""Training"") 
mvad.scodes &lt;- c(""EM"", ""FE"", ""HE"", ""JL"", ""SC"", ""TR"") 

## Define sequence objects
mvad.seq &lt;- seqdef(mvad[, 17:86], alphabet = mvad.alphabet, states = mvad.scodes, labels = mvad.labels, weights = mvad$weight, xtstep = 6)

## Computing OM dissimilarities
mvad.dist &lt;- seqdist(mvad.seq, method=""HAM"", sm=""CONSTANT"")

## Clustering
wardCluster &lt;- hclust(as.dist(mvad.dist), method = ""ward"", members = mvad$weight)
clust4 &lt;- cutree(wardCluster, k = 4)

## Silhouettes
sil &lt;- wcSilhouetteObs(mvad.dist, clust4, weights = mvad$weight, measure = ""ASWw"")

## Sequence index plots ordered by representativeness
seqIplot(mvad.seq, group = clust4, sortv = sil)
</code></pre>

<p>In this example, it would be for example interesting to see whether the sequence of third cluster's centroid differes from the most representative, original sequences in the cluster which are printed at the very top of the sequence index plot. In other cases, the centroid sequence may even have a more idealtype character which does not exist in the original dataset but reflects certain typical structures.</p>

<p><sub>[1] See for the example Studer, Matthias (2013). WeightedCluster Library Manual: A practical guide to creating typologies of trajectories in the social sciences with R. LIVES Working Papers, 24.</sub></p>
"
"0.21320071635561","0.230276497795691"," 65411","<p>I have carried out a clustering of coordinate points (longitude, latitude) and found surprising, adverse results from clustering criteria for the optimal number of clusters. The criteria are taken from the <code>clusterCrit()</code> package. The points which I am trying to cluster on a plot (the geographic characteristics of the data set is clearly visible) :</p>

<blockquote>
  <p><img src=""http://i.stack.imgur.com/EAgVj.jpg"" alt=""Plot of all observations""></p>
</blockquote>

<p>The full procedure was the following :</p>

<ol>
<li>Carried out hierarchical clustering on 10k points and saved
medoids for 2 : 150 clusters.</li>
<li>Took the medoids from (1) as seeds for kmeans clustering of 163k observations. </li>
<li>Checked 6 different clustering criteria for the optimal number of clusters.</li>
</ol>

<p>Only 2 clustering criteria gave results that make sense for me â€“ the Silhouette and Davies-Bouldin criteria. For both of them one should look for the maximum on the plot. It seems both give the answer â€œ22 Clusters is a good numberâ€. For the graphs below: on the x axis is the number of clusters and on the y axis the value of the criterion, sorry for the wrong descriptions on the image. Silhouette and Davies-Bouldin respectively :</p>

<blockquote>
  <p><img src=""http://i.stack.imgur.com/9tlDB.jpg"" alt=""Silhoette Criterion Plot"">
  <img src=""http://i.stack.imgur.com/USELa.jpg"" alt=""Davies-Bouldin Criterion Plot""></p>
</blockquote>

<p>Now letâ€™s look at Calinski-Harabasz and Log_SS values. The maximum is to be found on the plot. The graph indicates that the higher the value the better the clustering. Such a steady growth is quite surprising, I think 150 clusters is already a quite high number. Below the plots for Calinski-Harabasz and Log_SS values respectively.</p>

<blockquote>
  <p><img src=""http://i.stack.imgur.com/toHAM.jpg"" alt=""Calinski-Harabasz Criterion Plot"">
  <img src=""http://i.stack.imgur.com/yJiG0.jpg"" alt=""Log_SS Criterion Plot""></p>
</blockquote>

<p>Now for the most surprising part the last two criteria. For the Ball-Hall the biggest difference between two clusterings is desired and for Ratkowsky-Lance the maximum. Ball-Hall and Ratkowsky-Lance plots respectively :</p>

<blockquote>
  <p><img src=""http://i.stack.imgur.com/09zWT.jpg"" alt=""Ball-Hall Criterion Plot"">
  <img src=""http://i.stack.imgur.com/UpE3b.jpg"" alt=""Ratkowsky-Lance Criterion Plot""></p>
</blockquote>

<p>The last two criteria give completely adverse answers (the smaller the number of clusters the better) than the 3rd and 4th criteria. How is that possible? For me it seems like only the first two criteria were able to make any sense of the clustering. A Silhouette width of around 0.6 is not that bad. Should I just skip the indicators that give strange answers and believe in those that give reasonable answers? </p>

<p><em>Edit: Plot for 22 clusters<img src=""http://i.stack.imgur.com/gNbON.jpg"" alt=""22 cluster solution""></em></p>

<hr>

<p><strong>Edit</strong></p>

<p>You can see that the data is quite nicely clustered in 22 groups so criteria indicating that you should choose 2 clusters seem to have weaknesses, the heuristic isn't working properly. It is ok when I can plot the data or when the data can be packed in less than 4 principal components and then plotted. But if not? How should I choose the number of clusters other than by using a criterion? I have seen tests which indicated Calinski and Ratkowsky as very good criteria and still they give adverse results for an seemingly easy data set. So maybe the question shouldn't be ""why are the results differing"" but ""how much can we trust those criteria?"".</p>

<p>Why is an euclidian metric not good? I am not really interested in the actual, exact distance between them. I understand the true distance is spheric but for all points A,B,C,D if Spheric(A,B) > Spheric(C,D) than also Euclidian(A,B) > Euclidian(C,D) which should be sufficient for for a clustering metric. </p>

<p>Why I want to cluster those points? I want to build a predictive model and there is a lot of information contained in the location of each observation. For each observation I also have cities and regions. But there are too many different cities and I don't want to make for example 5000 factor variables; therefore I thought about clustering them by coordinates. It worked pretty well as the densities in different regions are different and the algorithm found it, 22 factor variables would be all right. I could also judge the goodness of the clustering by the results of the predictive model but I am not sure if this would be wise computationally. Thanks for the new algorithms, I will definitely try them if they work fast on huge data sets.</p>
"
"0.111978502191171","0.0762492851663023"," 65589","<p>I am currently implementing a Kmeans clustering algorithm in R. I am not using any packages and I wrote it from scratch. I am using only one set of initial guesses, and my action upon finding an empty cluster is to select a new data point randomly and use that as the new mean for the empty cluster.</p>

<p>I have gathered from reading online that the solution does not always converge, and it is highly sensitive to the initial means, so when I see that behavior I am not surprised. But I am finding that sometimes my solution is actually cycling between two or more different solutions. So I have two questions associated with this observation:</p>

<p>1) Within a solution cycle, one solution is always better than the others as measured by the total sum of squared distances of all points to their nearest clusters. So this implies that not only does the algorithm not necessarily find the global optimum, but also it sometimes does not even improve the total sum of squared distances from one iteration to the next? I thought the solution was at least always improving... </p>

<p>2) What is the best way to get around this problem? Do I have to program it to recognize cycles and then select the iteration in the cycle with the lowest total distance? Or is there an easier way? </p>

<p>Any help would be greatly appreciated.<br>
Thanks.</p>
"
"0.118771137369734","0.125804902070678"," 66346","<p>I have a mixture Markov model (containing K clusters, or components) that I am trying to train, e.g perform clustering over a set of varying length sequences. Each component of the model is a first order Markov chain. The model is defined by the following:</p>

<p><img src=""http://i.stack.imgur.com/O4TuQ.gif"" alt=""enter image description here""></p>

<p>where</p>

<p><img src=""http://i.stack.imgur.com/HjzXm.gif"" alt=""enter image description here""></p>

<p>is the model describing behaviour of sequences in the k-th cluster.</p>

<p>Each kth model component(cluster) has initial probability distribution vector (theta_KI) and a transition matrix (theta_tK) - see equation 2. Each sequence seq_i in the set of sequences has a different length L (so the L in the 2nd equation would be different for each sequence in the set).  </p>

<p>There is n possible visible states in the model and m sequences in each cluster, and the total amount of sequences is M.</p>

<p>In this case, how would I define e- and m-steps for my EM algorithm? My understanding is that I have to maximize log likelihood function of the model, which would be a sum of likelihoods across all vectors v_i ( synonym with sequence seq_i)-  something like:</p>

<p><img src=""http://i.stack.imgur.com/05Mmi.gif"" alt=""enter image description here""></p>

<p>However, I get stuck not knowing if this is the right definition for what needs to be maximized and which arguments need to be found for that. I have looked for literature but it is very sparse, almost nonexistent. </p>

<p>Another question: is there actually an R library that has functions that can be extended to include such a model for EM algorithm? </p>
"
"0.104746297470869","0.142649350539364"," 66728","<p>I'm trying to write my own code for cluster-robust (AKA panel-robust, AKA heteroskedasticity and serial-correlation-consistent) standard errors, so that I can make a couple of small extensions.  But I can't get my results to match Stata's (in which this procedure is routine), so I am probably missing some detail.  Would appreciate pointers on the code, below.</p>

<p>The model is 
$$
y_{ig} = \alpha_g + X'_{ig}\beta + \epsilon_{ig}
$$
The groupwise mean is the subtracted from every term above, getting rid of the $\alpha$, and leaving de-meaned group-varying variables:
$$
ydm_{ig} = Xdm'_{ig}\beta + \epsilon dm_{ig}
$$</p>

<p>The variance of $\beta$ is estimated as </p>

<p>$$
\left(\displaystyle\sum_G Xdm_g'Xdm_g\right)^{-1} \displaystyle\sum_G Xdm_g' \epsilon dm_g \epsilon dm_g' Xdm_g \left(\displaystyle\sum_G Xdm_g'Xdm_g\right)^{-1}
$$</p>

<p>Standard errors are the square root of the diagonal of this matrix, inflated by $G/(G-1) \times (N-1)/(N-K)$.</p>

<p>This is all textbook econometrics.  Standard in stata, and code exists to do it in R.  (<a href=""http://people.su.se/~ma/clustering.pdf%E2%80%8E"" rel=""nofollow"">this</a>, for example)</p>

<p>I want my code to not rely on <code>plm</code>, <code>lmtest</code> or <code>sandwich</code>.</p>

<p>The following script is supposed to implement the math above, simulating a panel dataset in which outcomes are autocorrelated and groups have different forms of heteroskedasticity.  It gives the right point estimates but the standard errors are bigger than those given by stata:</p>

<pre><code>rm(list=ls()) 
set.seed(999)
N = 1000
G = 200
x = rnorm(N)
z = rexp(N)
obs = N/G
fe = data.frame(ID=1:G,fe = rnorm(G)+5)
fe = data.frame(ID = rep(fe$ID,obs),fe = rep(fe$fe,obs))
t = rep(1,G); for (i in 2:obs) {t = c(t,rep(i,G))}
data = data.frame(y=x+z+fe$fe+x*rnorm(N,mean=0,sd=fe$fe*runif(N)), ID=fe$ID,x,z,t)
write.csv(data,""testdata.csv"")

demean = function(var,ID){
    dat = data.frame(var,ID)
    library(doBy)
    means = summaryBy(var~ID,data=dat,fun=mean)
    d = data.frame(var,ID)
    a = merge(d,means,by=""ID"")
    adm = a[,2]-a[,3]
    adm
    }
xdm = demean(data$x,data$ID)
ydm = demean(data$y,data$ID)
zdm = demean(data$z,data$ID)

mdm = lm(ydm~xdm+zdm-1)
summary(mdm)

e = mdm$resid

lpm = cbind(xdm,zdm)

bread = matrix(0,ncol(lpm),ncol(lpm))
tofu = matrix(0,ncol(lpm),ncol(lpm))
K = mdm$rank
    for (i in 1:G){
    	X = lpm[data$ID==unique(data$ID)[i],]
    bread = t(X)%*%X +bread

    r = e[data$ID==unique(data$ID)[i]]
    tofu = t(X) %*% r %*% t(r) %*% X + tofu
    }
bread = solve(bread)
vcv = bread%*%tofu%*%bread
se = G/(G-1)*(N-1)/(N-K)*sqrt(diag(vcv))
summary(mdm)
se
</code></pre>

<p>Using that <code>testdata.csv</code> file, I can compare to Stata:</p>

<pre><code>clear all
insheet using ""testdata.csv""
xtset id t
xtreg y x z,fe cluster(id)
</code></pre>

<p>My R-code:</p>

<pre><code>1&gt; se
       xdm        zdm 
0.16946120 0.08793485 
</code></pre>

<p>Stata's output:</p>

<pre><code>             |               Robust
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           x |   1.223739   .1992449     6.14   0.000     .8308371    1.616642
           z |    .876592   .0960943     9.12   0.000     .6870981    1.066086
</code></pre>

<p>Mine are consistently more optimistic than Stata's.  Anybody help me figure out why?  I feel like it is probably some small bug, but I can't pinpoint where.</p>

<p><strong>EDIT</strong>:  The answer to the question is <a href=""http://www.stata.com/support/faqs/statistics/intercept-in-fixed-effects-model/"" rel=""nofollow"">here</a> and <a href=""http://www.stata.com/manuals13/xtxtreg.pdf"" rel=""nofollow"">here</a>.  Stata doesn't run a textbook ``within'' estimator, rather it adds the averages back into each variable, which is what allows for the estimation of a constant term (which has the interpretation of the mean of the fixed effects, and which allows for prediction.  When I alter my code to copy those procedures, my SE's are equivalent up to the 3rd or 4th decimal point.</p>
"
"0.104746297470869","0.0407569572969611"," 67380","<p>My question is probably elementary, and I apologize for that. I am reading Kogan's ""Introduction to Clustering Large and High-Dimensional Data""; I am interested in understanding batch K-means and K-means and use it in $\operatorname{R}$. In the textbook it is stated that both algorithms need an initial choice of</p>

<ol>
<li>the number of clusters $K$</li>
<li>An initial partition of the given dataset</li>
</ol>

<p>Using such entries, the algorithms can perform the learning exercise. Kogan states that the initial partition is usually found using a Principal Direction Divisive Partitioning (PDDP) algorithm. </p>

<p>Looking at the K-means function <code>kmeans</code> in $\operatorname{R}$ I have noticed the absence of the initial partition as argument of the function itself. One can specify the number of clusters or a set of initial centers.</p>

<p>Moreover, the default K-mean algorithm used by <code>kmeans</code> is the one by  Hartigan and Wong (1979). Unfortunately I have no access to the original paper, and I could not run through the original code, searching for the initial partition.</p>

<p>My questions are:</p>

<ul>
<li>is there an initial partition choice hidden somewhere in <code>kmeans</code>? If yes, how is it chosen?</li>
<li>In absence of initial partition choice, how does <code>kmeans</code> begins to run (a high level overview would be great!)?</li>
</ul>

<p>I thank you all.</p>
"
"0.0791807582464896","0.0269581933008596"," 67686","<p>I am exploring the flexibility of partitional clustering algorithms. In particular, I would like to introduce more general distances than the ones which are used by default. </p>

<p>Let us consider, for simplicity, <code>dbscan</code> contained in the R-package <code>fpc</code>. It allows the user to specify a ""data matrix, data.frame, dissimilarity matrix or dist-object"". </p>

<p>My idea would be to compute the distance matrix of the given data w.r.t. my chosen distance, and run <code>dbscan</code>. </p>

<p>Here comes the point where I am stuck. Is it true that specifying a distance matrix should lead inevitably to a hierarchical clustering? My intuition says that a hierarchical clustering in presence of  a distance matrix makes more sense that a partitional one on the elements of the matrix itself. As I am no expert in clustering, I cannot judge the above statement properly.</p>

<p>Would you use a partitional algorithm on the distance matrix of a given dataset? Is this correct?</p>

<p>Thank you all!</p>
"
"0.131306432859723","0.0812820110952944"," 68345","<p>This question is related to my quest of clustering the sequences using mixture Markov modeling. </p>

<p>I have trouble understanding Dirichlet priors in the context of MAP-estimate (Mixture Markov Models). Namely, my priors end up being (much) larger than one.<br>
I have non-informative priors defined as following:</p>

<p>$$
p(\theta_n^{j}|a_n^{j})=\frac{\Gamma(\sum_{m=1}^{M}(a_{nm}^{j}+1))}{\prod_{m=1}^{M}\Gamma(a_{nm}^{j}+1)}*\prod_{m=1}^{M}(\theta_{nm}^{j})^{a_{nm}^{j}},
$$</p>

<p>where each $a_n^{j}$ is a <em>M</em>-vector with components $a_{nm}^{j}&gt;0$. <em>j</em> denotes the jth component in the mixture and <em>n</em> is the number of the row of the TPM ( transition probability matrix). Then I use the sum of log(Dirichlet Priors) across each row and each component. </p>

<p>The most confusing aspects are:</p>

<p>1) In all literature the Dirichlet prior formula is given as:</p>

<p>$$
p(\theta_n^{j}|a_n^{j})=\frac{\Gamma(\sum_{m=1}^{M}(a_{nm}^{j}))}{\prod_{m=1}^{M}\Gamma(a_{nm}^{j})}*\prod_{m=1}^{M}(\theta_{nm}^{j})^{a_{nm}^{j}-1},
$$</p>

<p>(notice the -1 in the exponent term). Is there possibly a typo in the article or can the -1 term be skipped? </p>

<p>2) The article sets $a_n^{j}$ equal to 10% of the corresponding relative frequencies of the TPM of the original counts across all sequences ( if our model has just 1 component). Then, let us consider such example:</p>

<p>there are 3 possible states in the Markov Chain, and the <em>n</em>-th row transition probabilities are 0.1, 0.8, 0.1. Let $a_n^j$ be equal to (0.01,0.03,0.06). Then, following the formula, my prior will be ( calculated in R) :</p>

<p>according to the first version of formula: <strong>1.961152</strong></p>

<p>according to the second version of the formula: <strong>245.144</strong></p>

<p>This has been computed by defining a function in R:</p>

<pre><code>    &gt; dirichletPrior&lt;-function(matrix_row,alpha_row){
        dirichl&lt;-((gamma(sum(alpha_row+1)))/(prod(gamma(alpha_row+1))))*prod(matrix_row^(alpha_row))
        dirichl
     }
</code></pre>

<p>The results seem a bit like a nonsense to me, and I do not understand where is my logic faulty. </p>

<p>3) Is ""Dirichlet prior"" a probability density function or a likelihood function? What would it mean if the result for each row is larger than 1? Are alpha multinomial parameters like I gave in the example, meaningful at all? What do they have in common with concentration parameter? </p>

<p>The article I have been referring to is located under the following link: http://www.cs.uoi.gr/~kblekas/papers/C19.pdfâ€Ž. </p>
"
"0.0969762275752854","0.0660338179744218"," 69046","<p>I have calculated log-likelihood distances between 50 sequences according to the Formula (1): </p>

<p>$$
D(X_i,X_j)= 1/2(\log p(X_i|Mod_j)+\log p(X_j|Mod_i)),  
$$
where $
p(X_i|Mod_j)
$ is the likelihood of sequence $X_i$ being produced by model $Mod_j$, where $Mod_j$ is a corresponding Markov model of the given $Seq_j$, defined by its Transition Probability Matrix and Start Probabilities Vector.  The measure is symmetrical as seen from the definition. To make the measure more ""legible"" and similar to the traditional measures, I compute distance$=(1-D)$ from formula (1). Thus, $D(X_i,X_i) = 0$ and the distance increases if the likelihood decreases. </p>

<p>Now, I have a 50x50 Distance Matrix.I have run a ""meaningfullness"" check, and it seemed ok for me - i.e. more similar sequences had smaller distance and very different ones had very large distance. The distances seemed to satisfy the triangle inequality. However, I have noticed that:</p>

<p>1)  the shorter sequences seem to be ""closer"" to all other sequences than longer ones. It seems that this distance measure is biased to favor short distances. </p>

<p>2) I have tried PAM-clustering with the distance matrix by converting my distance matrix to dist object in <code>R</code> by using as.dist(), and my results were very bad, even for 2 clusters or 49 ( max avg.silhouette width produced by <code>R</code> function pam was 0.28). With some numbers of clusters the avg.silhouette widths were even negative. </p>

<p>I am coming to conclusion that my way of computing medoids is invalid/conceptually wrong. What could be the problem? Can log-likelihood distance matrix be used with medoids clustering at all? </p>

<p>edit: I am including the heatmap of the distance matrix, where x- and y-axis represent sequences (1 through 50th). It looks strange to me but I cannot pinpoint what exactly doesn't feel right. </p>

<p><img src=""http://i.stack.imgur.com/RcSBc.png"" alt=""heatmap""></p>
"
"0.0395903791232448","0"," 69514","<p>I have a clustering results by different unsupervised algorithms in the form of coordinates (x,y) and class (it's currently binary). I would like evaluate the quality of clustering by some value. Most of the used techniques assuming that the closely located entries in the plot are related, and if they are from different classes that's not good.</p>

<p>I have checked the R packages like clusterSim and clv (cluster evaluatin) by all of them seems to work with original data, but not with the already produced coordinates.</p>

<p>Can you please suggest me the quantitative evaluation technique for described situation? </p>

<p>PS: My data is imbalanced with ratio around 1:10</p>
"
"0.0791807582464896","0.0808745799025788"," 69740","<p>Background - I want to cluster analyze a mixed dataset, clustering the variables on the basis of correlational similarity. SPSS gives me this option, but doesn't allow me to evaluate the clustering solutions by providing statistical measures of heterogeneity change (e.g. pseudo F statistic) or direct measures of heterogeneity (e.g. CCC).</p>

<p>As such, I'm learning R. I've managed to cluster my variables using both the varclus and hclustvar procedures, both of which generate nice dendrograms that I can interpret visually. However I'm struggling to get R to provide me with some actual numbers, such as the statistics mentioned above, which might indicate what constitutes the ""best"" clustering solution. How can I do this? I've been through the documentation for both the Hmisc and ClustOfVar packages and can't find any way to do this.</p>

<p>I've read <a href=""http://stats.stackexchange.com/questions/49549/how-to-choose-clusters-from-variable-clustering-varclus-procedure"">elsewhere</a> that clustering criteria can be applied using the NBClust package, but as far as I can see the NBClust function only works on dissimilarity matrices / distance measures, which aren't an option for me as I'm interested in correlational similarity.</p>

<p>Any suggestions? </p>
"
"0.0395903791232448","0.0539163866017192"," 70041","<p>Is it possible to filter only positive correlations on R?</p>

<p>The point is to make clusters of time series using the correlation as a distance measure, but without clustering the series that have negative correlations.
Thanks!</p>
"
"0.0559892510955854","0.0762492851663023"," 70844","<p>I have a data set with 20,000 discussion forum threads, many only one or two posts, some up to 400-posts. I have 5,000 individuals who participated in these threads. I want to calculate the strength of the relationship between two people based on how many threads they have participated in for export to Gephi and clustering to see if I can find any clear groups - are there any pre-existing algorithms to do this?</p>

<p>I hacked up my own function in R, which looks at the length of the thread (smaller thread means stronger connection to other participants - too large threads are excluded), number of times posting (two people posting twice each in a thread are more strongly related), etc. It kind of works, but I'm sure I'm reinventing the wheel, there must be some existing algorithms/approaches etc? (I'm using R, but even a general algorithm is welcome). </p>
"
"0.118771137369734","0.107832773203438"," 71031","<p>I am attempting to cluster data using <a href=""http://www.stat.washington.edu/mclust/"" rel=""nofollow"">Mclust</a>. The data is originally from a dissimilarity matrix, transformed via multidimensional scaling in R (<code>MASS::isoMDS</code>). As I experiment with different numbers of dimensions, I have found that with larger numbers of dimensions (~50 relative to ~10 for the smaller values tried) certain model types return <code>NA</code> values for the BIC with larger numbers of components, i.e. clusters.</p>

<p>The Mclust manual states</p>

<blockquote>
  <p>The missing values correspond to models and numbers of clusters for
  which parameter values could not be fit (using the default
  initialization). For multivariate data, the default initialization for
  all models uses the classification from hierarchical clustering based
  on an unconstrained model.</p>
</blockquote>

<p>Can anyone explain what causes the failure of the parameter values to fit, and how that relates to the number of dimensions being used? Knowing this might help me better understand the potential value or lack thereof of those later dimensions.</p>

<p>In case it is relevant, in the cases I have tried the model type VVV stops at a lower number of components followed by EVI and VEV, then by VEI and VII, with the other 4 (EEI, EII, EEE, and EEV) returning values up to the default max of 9 components. I can run any number of other numbers of dimensions to derive a clearer pattern if that is helpful.</p>
"
"0.153332879014365","0.111369209277941"," 73162","<p>I'm attempting to estimate the effect of 2 drugs (<code>drug1</code>, <code>drug2</code>) on the likelihood of a patient falling (<code>event</code>).  The patients can fall more than once and can be put on or taken off of the the drugs at any point.  </p>

<p>My question is how the data should be structured with regard to the time period (days), specifically whether there needs to be overlap between the days.  There are two reasons why I think my structure is wrong, the first being a seemingly incorrect <code>N</code>.  I am also getting some errors where the time period is a single day (i.e. <code>time1=4</code>, <code>time2=4</code>) and am unsure how these should be coded.  Should the start time of subsequent entries be the stop time of the previous entry?  I've tried it both ways (with and without overlap), and while having overlap gets rid of the warning, the <code>N</code> is still incorrect. </p>

<pre><code>Warning message:
In Surv(time = c(0, 2, 7, 15, 20, 0, 18, 27, 32, 35, 39, 46, 53,  :
  Stop time must be &gt; start time, NA created
</code></pre>

<p>Right now I have the data set up where the beginning of the next entry is the next day.  Unique patients are identified by their <code>chart numbers</code>.  </p>

<pre><code>Time1    Time2    Drug1    Drug2   Event    ChartNo
    0        2        1        0       0        123
    3       10        1        1       1        123
   11       14        1        1       1        123
    0       11        0        1       0        345
    0       19        1        0       1        678
    0        4        0        1       0        900
    5       18        1        1       0        900
</code></pre>

<p>Patient 123 was on drug1 at the start to day 2, after which point they had drug2 added.  They went from day 3 to day 10 on both drugs before falling the first time, then fell a second time on day 14 while still on both drugs.  Patient 345 went 11 days on drug2 without falling (then was censored), etc.</p>

<p>The actual estimation looks like this:</p>

<pre><code>S &lt;- Srv(time=time1, time2=time2, event=event)
cox.rms &lt;- cph(S ~ Drug1 + Drug2 + cluster(ChartNo), surv=T)
</code></pre>

<p>My main concern is that the <code>n</code> for my analysis is reported to be <code>2017</code> (the number of rows in the data), when in actuality I only have <code>314</code> unique patients.  I am unsure if this is normal or the result of some error I've made along the way.</p>

<pre><code>&gt; cox.rms$n
Status
No Event    Event 
    1884      133 
</code></pre>

<p>The same is true when using <code>coxph()</code> from the survival package.</p>

<pre><code> n= 2017, number of events= 133
</code></pre>

<p>The number of events is correct however.  </p>

<p><a href=""http://stats.stackexchange.com/questions/58079/extended-cox-model-with-continuous-time-dependent-covariate-how-to-structure-d"">This Post</a> seems to have it set up with the 'overlap' I described, but I am unsure about the <code>N</code>, and they don't seem to be clustering by <code>ID</code>.  </p>
"
"0.0685725481323742","0.0311286403182345"," 74495","<p>I am trying to use R to do Kmeans clustering and as most people I ran into the challenge of determining when to finish. I have 10,000 items and potentially 10 times of that down the road. My goal is to create a series of clusters with minimal size (e.g. 50 items per cluster) OR reasonably similar items. In other words, I don't want any of my output clusters to be too small (even if the items are quite different from each other), but I also don't mind if the clusters are too big as long as the items are similar enough.</p>

<p>I imagine I can use some kind of divisive hierarchical approach. I can start by building a small number of clusters and examine each cluster to determine if it needs to be split into more clusters. I can keep doing this till all clusters meet my stopping criteria.</p>

<p>I wonder if anyone knows good information on how other people do this? </p>
"
"0.0791807582464896","0.0539163866017192"," 76316","<p>Currently I am working on some subspace clustering issues. I found one useful package in R called orclus, which implemented one subspace clustering algorithm called orclus.</p>

<p>As stated in the package description, there are two key parameters to be determined. One is the subspace dimensionality and the other one is the cluster number. It is stated that to determine the optimal value of subspace dimensionality, one statistic, the cluster sparsity coefficient can be used. The closer the statistic to zero, the better the performance. However, when actually trying this implementation, I found that the statistic is minimal when subspace dimensionality is 1; and the larger the subspace dimensionality, the larger the statistic. Does it make sense? I was not expecting such monotonic trend. </p>
"
"0.0559892510955854","0.0762492851663023"," 76373","<p>I have a lottery style dataset we produce internally (example below). I am trying to figure out which numbers appear most frequently together. Example questions:
What are the top 10 pair of numbers that appear most frequently together?
What are the top 10 three numbers that appear most frequently together?</p>

<p>What methods/techniques would I need to use to answer these questions?</p>

<p>I was thinking clustering at first but a test run (ward) I performed, didn't return the results I was expecting. </p>

<p><img src=""http://i.stack.imgur.com/Cc06x.png"" alt=""enter image description here""></p>
"
"0.125195771459034","0.136398867894095"," 77027","<p>For non-statisticians like me, it is very difficult to capture the idea of <code>VI</code> metric (variation of information) even after reading the relevant paper by Marina Melia ""<a href=""http://www.sciencedirect.com/science/article/pii/S0047259X06002016"">Comparing clusterings - An information based distance</a>"" (Journal of Multivariate Analysis, 2007). In fact, I am not familiar with many of the clusterings' terms out there.  </p>

<p>Below is a MWE and I would like to know what does the output mean in the different metrics used.  I have these two clusters in R and in the same order of id:  </p>

<pre><code>&gt; dput(a)
structure(c(4L, 3L, 4L, 4L, 4L, 4L, 4L, 3L, 4L, 4L, 3L, 3L, 3L, 
4L, 4L, 4L, 4L, 4L, 3L, 4L, 4L, 4L, 3L, 4L, 4L, 3L, 4L, 4L, 4L, 
1L, 1L, 4L, 4L, 4L, 4L, 4L, 4L, 3L, 4L, 4L, 3L, 4L, 4L, 2L, 2L, 
4L, 3L, 3L, 2L, 2L, 2L, 4L, 3L, 4L, 4L, 3L, 1L, 4L, 3L, 4L, 4L, 
4L, 3L, 4L, 4L, 4L, 4L, 2L, 2L, 2L, 4L, 3L, 4L, 4L, 4L, 4L, 4L, 
4L, 4L, 4L, 4L, 3L, 4L, 4L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 3L, 4L, 
4L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 3L, 4L, 4L, 4L, 4L, 2L, 2L, 4L
), .Label = c(""1"", ""2"", ""3"", ""4""), class = ""factor"")
&gt; dput(b)
structure(c(4L, 3L, 4L, 4L, 4L, 4L, 4L, 3L, 4L, 4L, 3L, 3L, 3L, 
4L, 4L, 3L, 4L, 4L, 3L, 4L, 4L, 4L, 3L, 4L, 4L, 3L, 4L, 4L, 4L, 
1L, 1L, 4L, 4L, 4L, 4L, 4L, 4L, 3L, 4L, 4L, 3L, 4L, 4L, 2L, 2L, 
4L, 3L, 3L, 2L, 2L, 2L, 4L, 3L, 4L, 4L, 3L, 1L, 4L, 3L, 4L, 4L, 
3L, 3L, 4L, 4L, 4L, 4L, 2L, 2L, 2L, 4L, 3L, 3L, 4L, 4L, 4L, 4L, 
4L, 4L, 4L, 4L, 3L, 4L, 4L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 3L, 4L, 
4L, 3L, 4L, 4L, 4L, 4L, 4L, 3L, 3L, 4L, 4L, 4L, 4L, 2L, 2L, 4L
), .Label = c(""1"", ""2"", ""3"", ""4""), class = ""factor"")
</code></pre>

<p>Now doing comparisons based on the <code>VI</code> as well as other metrics / indices and in chronological order of their appearance in literature.  </p>

<pre><code>library(igraph)
  # Normalized Mutual Information (NMI) measure 2005:
compare(a, b, method = c(""nmi"")) 
[1] 0.8673525
  # Variation of Information (VI) metric 2003:
compare(a, b, method = c(""vi"")) 
[1] 0.2451685
  # Jaccard Index 2002:
clusteval::cluster_similarity(a, b, similarity = c(""jaccard""), method = ""independence"") 
[1] 0.8800522
  # van Dongen S metric 2000:
compare(a, b, method = c(""split.join"")) 
[1] 8
  # Adjusted Rand Index 1985:
compare(a, b, method = c(""adjusted.rand"")) 
[1] 0.8750403
  # Rand Index 1971:
compare(a, b, method = c(""rand"")) 
[1] 0.9374788
</code></pre>

<p>As you can see, the <code>VI</code> value was different from all the others.  </p>

<ul>
<li>What does this value tell (and how is it related to the figure below)? </li>
<li>What are the guidelines for considering this value low or high? </li>
<li>Are there any guidelines defined?  </li>
</ul>

<p>Maybe experts in the field can provide some sensible descriptions for laymen like me when trying to report such results. I would really appreciate if someone would provide also guidelines for other metrics as well (when to consider the value is large or small, i.e., in relation to a similarity between two clusters).  </p>

<p>I have read related CV threads <a href=""http://stats.stackexchange.com/questions/24961/comparing-clusterings-rand-index-vs-variation-of-information"">here</a> and <a href=""http://stats.stackexchange.com/questions/15548/validation-of-clustering-results/15600?noredirect=1#comment150263_15600"">here</a>, but still couldn't grasp the intuition behind <code>VI</code>. Can someone explain this in plain English?  </p>

<p>The below figure is figure 2 from the above mentioned paper about <code>VI</code>.  </p>

<p><img src=""http://i.stack.imgur.com/OnCzM.png"" alt=""enter image description here""></p>
"
"0.0969762275752854","0.0660338179744218"," 77660","<p>I have 11 scale parameters for each of 218 observations belonging to subjects, I did standardized PCA to reduce dimensionality of the data and found two meaningful components. Using Euclidean distances this was followed by cluster analysis of these two components (explaining about 75% of the variance) with bottom-up approach using the hierarchical agglomerative clustering (HAC) by <code>FactoMineR</code> R package and Ward's linkage method.
The optimal number of clusters was 4 as suggested by the package based on minimizing the ratio of two successive partition inter-clusters inertia gains.<br>
This is just the number of observations per cluster:  </p>

<pre><code>&gt; table(df$clust)

  1   2   3   4 
  6  21  46 145
</code></pre>

<p>These 4 clusters turned out to be clinically important and subjects with cluster 1 were severely affected by disease. Cluster 4 were non-reactive subjects, Cluster 3 showed some reaction, and finally cluster 2 was like a special entity protected from disease. I don't know if these clusters can assume some kind of ordinal ranking or not. It is difficult to judge from the theoretical point of view related to the field, but I can say that cluster 4->3->1 is somehow showing some direction, and hence could be regarded as ordinal, on the other hand, cluster 2 is a little bit different but very important as subjects with this clusters were protected from disease. So, I am really confused as whether to consider these 4 clusters ordinal or not.  </p>

<p>Suppose that I have another set of 11 new readings of the scale parameters for one subject as new data, what statistical analysis would be useful to predict the membership of this subject to those 4 clusters? Could you please refer to a similar example with R code if possible? that would be greatly appreciated.  </p>

<p>Providing a professional answer would be highly esteemed, but also recommending some books using R code would also be encouraged, as I am searching for such a book that covers this topic thoroughly, many books are out there but it is difficult to judge which one would do the job. May be someone, has more experience with this kind of problems and can give a word of advise here.  </p>
"
"0.0969762275752854","0.0660338179744218"," 77672","<p>Suppose I have 50 scale parameters, these are all genes measured for one sample from a subject at the clinic, after data reduction by PCA, two meaningful components were extracted. This was followed by cluster analysis and turned out to be 4 meaningful clusters of subjects based on the two components of these 50 genes.  </p>

<p>Since investigating 50 genes for one subject would be costy, one would like to reduce that number so that the same clustering pattern can still be obtained but with minimal costs possible ( there should be some measures here to say acceptable clustering or not, I wonder what kind of measures would fit this case though).  </p>

<p>Of course, the more genes investigated, the more information gained, but there should be some measure to tell when to stop wasting more money when the same result is <em>satisfactorily</em> achievable will less number of genes.  </p>

<p>Is there any R package that already implemented this approach? what would be the statistical approach in this case to select the most important genes that would preserve the clustering pattern? what criteria to be used in order to reach the minimum clustering pattern?  </p>
"
"0.131306432859723","0.113794815533412"," 78148","<p>I have biological time series (9 years long) of the biomass of species which logically exhibit a seasonal pattern. I would like to cluster them into a few groups based on their typical seasonal evolution (e.g. spring vs. summer species). 
To do so, I was advised to use Fourier transform in order to decompose their signal into <code>N</code> harmonics (e.g. 3: annual, bi-annual and tri-annual seasonal cycles) and use the amplitudes and phases of these in a Principal Components Analysis (PCA; which would work as the harmonics are orthogonal/uncorrelated).</p>

<p>I know there are already some similar subjects in this Forum, yet some aspects remain unclear to me. My questions are:</p>

<p>(1) When I reconstruct the time evolution from the <code>N</code> first harmonics computed from the Discrete Fourier Transform (DFT), the explained variability of the original signal (the RÂ² of the linear model between recomposed signal and the original data) is sometimes only 0.40 (<code>N=3</code>) or 0.60 (<code>N=5</code>). In your experience, does it mean the data are not suited for this approach, does that invalidate the approach? Is there more pre-processing I could do to fix that (e.g., smoothing the signals, â€¦)? Some species exhibit sudden increases spaced by total absence, and I wonder if this doesnâ€™t call for the need of higher frequency harmonics; should I expect difficulties there and how to tackle them?</p>

<p>(2) Beside DFT which appears limited here, I considered using continuous Fourier Transform through a Fast Fourier Transform (FFT) algorithm and working on the power spectrum of each time series. I wonder if this could allow me to select <code>N'</code> so-called â€œharmonicsâ€ by selecting the <code>N'</code> highest peaks in the periodogram and then calculating the corresponding amplitude and phase to be used in a following PCA... Does that make sense? 
How to concretely use the info given by a FFT algorithm in R (such as <code>fft()</code> or <code>spec.pgram()</code>) in order to run a subsequent PCA (or any other clustering method)? [any R code snippet would be very welcome]</p>

<p>(3) How to reconstruct the signal from selected harmonics in the continuous case (FFT)? I can easily do this in the DFT case, but I am stupidly blocked in the continuous caseâ€¦ Any R code snippet is of course very welcome.</p>

<p>Any help regarding these questions would be very appreciated.
Links toward concrete examples, especially with associated R code, would be very helpful too (as well as method name or keywords).
Thank you. </p>

<p>PS: in case it is useful: The time series are of equal length and pre-processed to have uniform sampling intervals; stationarity may be assumed; no long-term trend is in the way.
I divided the time series in 52 equally-spaced observations per year (i.e., 468 observations over the 9 years).</p>
"
"0.0685725481323742","0.062257280636469"," 78322","<p>I have two parts of a multidimensional data set, let's call them <code>train</code> and <code>test</code>. And I want to built a model based on the train data set and then validate it on the test data set.
The number of clusters is known.</p>

<p>I tried to apply k-means clustering in R and I got an object that contains the centers of clusters:</p>

<pre><code>kClust &lt;- kmeans(train, centers=N, nstart=M)
</code></pre>

<p>Is there a function in R that takes the centers of clusters that were found and assigns clusters to my test data set?</p>

<p>What are the other methods/algorithms that I can try?</p>
"
"0.0395903791232448","0"," 78820","<p>I have a dataset of tasks. Each task:</p>

<ul>
<li>has a duration (difftime in seconds) </li>
<li>belongs to a project (project name factor)</li>
<li>has an owner (user name factor)</li>
</ul>

<p>In R , I can simulate my data by something like:</p>

<pre><code>set.seed(1)
dat &lt;- sample(data.frame(duration=runif(N,1,8),
                         owner=sample(gl(N/50,50)),    
                         project=sample(gl(N/60,60))))
</code></pre>

<p>Is it possible to estimate duration using owner / project categorical variables? What are the best methods to explore: classification, estimation, clustering, trees, something else? </p>

<p>I am not looking for an exact solution but rather I want to have a methodology to deal with this kind of problem.</p>
"
"0.131306432859723","0.130051217752471"," 78938","<p>I've been working on SOMs and how to get the best clustering results. 
One approach could be to try many runs and choose the clustering with the lowest within sum of squared errors.</p>

<p>However, I do not only want to initialize random values and have several tries, but also want to choose good parameters.
I have read in ""Influence of Learning Rates and Neighboring Functions on Self-Organizing Maps"" (Stefanovic 2011) that if you do not know which parameters for the neighborhood function and learning rate to choose, it is probably the best option to choose a gaussian function and a nonlinear learning rate.</p>

<p>My data is a time series lets say:</p>

<pre><code>matrix(c(sample(seq(from = 10, to = 20, by = runif(1,min=1,max=3)), size = 5000, replace = TRUE),(sample(seq(from = 15, to = 22, by = runif(1,min=1,max=4)), size = 5000, replace = TRUE)),(sample(seq(from = 18, to = 24, by = runif(1,min=1,max=3)), size = 5000, replace = TRUE))),nrow=300,ncol=50,byrow = TRUE) -&gt; data
</code></pre>

<p>which has 300 observations with 50 values each. 100 observations each tend to be more similar.</p>

<p>I'm working with the kohonen package.</p>

<p>The code:</p>

<pre><code>grid&lt;-somgrid(4,3,""hexagonal"")

kohonen&lt;-som(data,grid)

matplot(t(kohonen$codes),col=kohonen$unit.classif,type=""l"")
</code></pre>

<p>gives me clusters with values <strong>between 10 to 22</strong>, which is similar to the obersations</p>

<p><img src=""http://i.stack.imgur.com/eNd7N.jpg"" alt=""enter image description here""></p>

<p>I tried the ""som"" package, too, which offers a gaussian neighborhood function and a inverse-time learning rate.</p>

<pre><code>som&lt;-som(data,4,3,init=""random"",alphaType=""inverse"",neigh=""gaussian"")

som$visual[,4]&lt;-with(som$visual,interaction(som$visual[,1],som$visual[,2]))

som$visual[,4]&lt;-as.numeric(as.factor(som$visual[,4]))

matplot(t(som$code),col=som$visual[,4],type=""l"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/vaSJV.jpg"" alt=""enter image description here""></p>

<p>Here I get clusters with values <strong>between 15 and 18</strong>, so all clusters ""shrink"" and get more similar. With different input series I get the same phenomena</p>

<p>My two questions:</p>

<ol>
<li>Why do clusters from the self-organizing map with the som package get so extraordinarily similar and shrink to a much smaller range, even though it is said that you get good clusters with gaussian neigborhood function and non linear learning rate?</li>
<li>How can I avoid this range shrinking with a gaussian neigborhood function and a non linear learning rate in order to get appropriate clusters?</li>
</ol>
"
"0.0685725481323742","0.0311286403182345"," 80004","<p>I'm trying to assess the uncertainty in hierarchical cluster analysis. It is a dataset composed of 409 observations and 27 variables (with a value ranging form 0 to 100). The dataset represents immunohistochemical scores in a gastrointestinal cancers.</p>

<p>A meaninful clustering of observations and markers is observed with Pearson uncentered distance and average linkage.</p>

<pre><code>hc &lt;- hclust(Dist(t(imputedMatrix), method=""pearson""), method=""average"")
hr &lt;- hclust(Dist(imputedMatrix, method=""pearson""), method=""average"")
heatmap.2(imputedMatrix, Rowv=as.dendrogram(hr), Colv=as.dendrogram(hc),   col=greenred(100), scale=""none"", ColSideColors=patientcolors, density.info=""none"", trace=""none"")

pv2 &lt;- pvclust(imputedMatrix, method.dist=""uncentered"", method.hclust=""average"", nboot=10000)
plot(pv2, hang=-1)
pvrect(pv2, alpha=0.95)

clsig &lt;- unlist(pvpick(pv2, alpha=0.90, pv=""au"", type=""geq"", max.only=TRUE)$clusters) 
    dend_colored &lt;- dendrapply(as.dendrogram(pv2$hclust), dendroCol, keys=clsig,     xPar=""edgePar"", bgr=""black"", fgr=""red"", pch=20)
heatmap.2(imputedMatrix, Rowv=as.dendrogram(hr), Colv=dend_colored, col=greenred(1
</code></pre>

<p>However, when using pvclust to assess their uncertainty, many small low-levels subclusters are highlighted as significant, but not any higher level one. Also, a group of tumours (to the right in the plots) is indeed a control group that should be clearly distinguished at the highest level from the other ones. pvclust shows even there the same kind of pattern. </p>

<p>Indeed the clusters of interest are the magenta, green and red in the color bar. Does this pvclust results support their existance (versus a clustering artefact by chance)? </p>

<p>How could these pvclust results be interpreted? Maybe I am using the tool in a wrong way? Or the wrong tool for this kind of data?</p>

<p>Thank you very much in advance.</p>

<p><img src=""http://i.stack.imgur.com/nedQr.png"" alt=""pvclust dendrogram"">
<img src=""http://i.stack.imgur.com/KoaHW.png"" alt=""hclust and pvclust, the latter in red in the dendrogram""></p>
"
"0.125195771459034","0.102299150920571"," 81396","<p>I'm trying to compile a list of clustering algorithms that are:</p>

<ol>
<li>Implemented in R</li>
<li>Operate on sparse <em>data matrices</em> (not (dis)similarity matrices), such as those created by the <a href=""http://www.rdocumentation.org/packages/Matrix/functions/sparseMatrix"" rel=""nofollow"">sparseMatrix</a> function.</li>
</ol>

<p>There are several other questions on CV that discuss this concept, but none of them link to R packages that can operate directly on sparse matrices:</p>

<ol>
<li><a href=""http://stats.stackexchange.com/questions/10411/clustering-large-and-sparse-datasets"">Clustering large and sparse datasets</a></li>
<li><a href=""http://stats.stackexchange.com/questions/44640/clustering-high-dimensional-sparse-binary-data"">Clustering high-dimensional sparse binary data</a></li>
<li><a href=""http://stats.stackexchange.com/questions/10122/looking-for-sparse-and-high-dimensional-clustering-implementation"">Looking for sparse and high-dimensional clustering implementation</a></li>
<li><a href=""http://stats.stackexchange.com/questions/9778/space-efficient-clustering/"">Space-efficient clustering</a></li>
</ol>

<p>So far, I've found exactly one function in R that can cluster sparse matrices:</p>

<h1><a href=""http://www.rdocumentation.org/packages/skmeans/functions/skmeans"" rel=""nofollow"">skmeans</a>: spherical kmeans</h1>

<p>From the <a href=""http://cran.r-project.org/web/packages/skmeans/index.html"" rel=""nofollow"">skmeans package</a>. kmeans using <a href=""http://en.wikipedia.org/wiki/Cosine_similarity"" rel=""nofollow"">cosine distance</a>.  Operates on dgTMatrix objects. Provides an interface to a genetic k-means algorithm, pclust, CLUTO, gmeans, and kmndirs.</p>

<p>Example:</p>

<pre><code>library(Matrix)
set.seed(42)

nrow &lt;- 1000
ncol &lt;- 10000
i &lt;- rep(1:nrow, sample(5:100, nrow, replace=TRUE))
nnz &lt;- length(i)
M1 &lt;- sparseMatrix(i = i,
                   j = sample(ncol, nnz, replace = TRUE),
                   x = sample(0:1 , nnz, replace = TRUE), 
                   dims = c(nrow, ncol))
M1 &lt;- M1[rowSums(M1) != 0, colSums(M1) != 0]

library(skmeans)
library(cluster)
clust_sk &lt;- skmeans(M1, 10, method='pclust', control=list(verbose=TRUE))
summary(silhouette(clust_sk))
</code></pre>

<hr>

<p>The following algorithms get honerable mentions: they're not quite clustering algorithms, but operate on sparse matrices.</p>

<h1><a href=""http://www.rdocumentation.org/packages/arules/functions/apriori"" rel=""nofollow"">apriori</a>: association rules mining</h1>

<p>From the <a href=""http://cran.r-project.org/web/packages/arules/index.html"" rel=""nofollow"">arules package</a>. Operates on ""transactions"" objects, which can be coerced from ngCMatrix objects.  Can be used to make recommendations.</p>

<p>example:</p>

<pre><code>library(arules)
M1_trans &lt;- as(as(t(M1), 'ngCMatrix'), 'transactions')
rules &lt;- apriori(M1_trans, parameter = 
list(supp = 0.01, conf = 0.01, target = ""rules""))
summary(rules)
</code></pre>

<h1><a href=""http://www.rdocumentation.org/packages/irlba/functions/irlba"" rel=""nofollow"">irlba</a>:  sparse SVD</h1>

<p>From the <a href=""http://cran.r-project.org/web/packages/irlba/index.html"" rel=""nofollow"">irlba package</a>.  Does SVD on sparse matrices.  Can be used to reduced the dimensionality of sparse matrices prior to clustering with traditional R packages.</p>

<p>example:</p>

<pre><code>library(irlba)
s &lt;- irlba(M1, nu = 0, nv=10)
M1_reduced &lt;- as.matrix(M1 %*% s$v)
    clust_kmeans &lt;- kmeans(M1, 10)
    summary(silhouette(clust_kmeans$cluster, dist(M1_reduced)))
</code></pre>

<h1><a href=""http://cran.r-project.org/web/packages/apcluster/index.html"" rel=""nofollow"">apcluster</a>:  Affinity Propagation Clustering</h1>

<pre><code>library(apcluster)
sim &lt;- crossprod(M1)
sim &lt;- sim / sqrt(sim)
clust_ap &lt;- apcluster(sim) #Takes a while
</code></pre>

<p>What other functions are out there?</p>
"
"0.28273185896753","0.301992169444568"," 81727","<p>I have 4 clusters (see plot below) extracted from data of medical samples <code>N=218</code> measured for 11 genes/predictors <code>P=11</code> by this method: first PCA analysis validated to have 2 important PCs that explained 75% of the data, then different clustering algorithms, distances, linkages (in hierarchical approach only) were compared: the majority support the presence of 4 distinct clusters. Taking the scientific hypothesis into consideration, clusters out of $K$-means algorithm were found the most plausible and were the most balanced clusters too: class #1 <code>n=12</code>, class #2 <code>n=21</code>, class #3 <code>n=79</code>, and class #4 <code>n=106</code>.  </p>

<p>Projecting the observations on plane 1-2 component scores, revealed the below scatter plot with each cluster color coded.
<img src=""http://i.stack.imgur.com/09sTF.png"" alt=""enter image description here""></p>

<p><strong>The aim is to find a global optimum classifier using R after doing PLS to the data.</strong>  </p>

<p>Knowing that these 4 clusters were actually the product of latent PCA components, it was natural to think of PCR as a next step to predict classes, but that approach turned out to be sub-optimal for two reasons: first, results do not related to probibilities (0-1), second, it does not relate well with the classes as the outcome variable. As many know, this would be better solved with PLS-DA method + softmax to find probabilities of class (0-1).  </p>

<p>However, many reports confirm the superiority of using LDA as a second step using the <em>scores</em> of PLS, given that same standardization parameters (mean, sd) be used of the training set on the holdout-test set, even using the PLS projections out of the training set on the test set in order to get the <em>scores</em> which would be the <em>actual</em> holdout-test set to validate the classifier in question.  </p>

<p>From the methodology point of view, this path is potentially encompassed with many dangers and subtle errors when one is un/misinformed about the tools used in context.  </p>

<p>The <code>caret</code> package which is unique of its kind given the consistent infrastructure it provides to train and validate an array of different models making use of <em>de facto</em> standard respective R-packages, and hence <code>caret</code> promotes itself as a road map to a validated modeling leveraging off R rich libraries. As heart to blood vessels, so <code>caret</code> to other packages in my opinion. That being said, unwatchful playing with the heart could cost you dearly, and might lead also to a stand-still or a <em>model-arrest</em> of your data. R is free, many free books out there, but buying <code>caret</code> only book paid off, i.e. <code>Applied Predictive Modeling</code>. The help files, companion website (very appealing btw), are great resources but they won't substitute the text inside the book IMHO. However, in the book, I couldn't find a direct answer to the PLS-Classifier two step method amid others. The potential with <code>caret</code> is immense, thanks to Max Kuhn and his colleagues, that primarily encouraged me to post this question.  </p>

<p>Back to the example above and the methodology of wish:<br>
<strong>Data splitting:</strong>  </p>

<p>Training set (77% <code>n=168</code>) for 10K-cross-validation: tuning (model-specific parameters, feature selection <code>P=11</code>, and cost to deal with imbalanced clusters). For CV this would be roughly <code>n=150</code> for fitting the model using differnt parameters of wish and 'n=17<code>for evaluation of parameters (I would call the</code>n=17' the CV-test to avoid confusion later on). Repetition = 5, so this will make 10 folds x 5 times = 50 training folds (<code>n=150</code> each) and 50 CV-test folds (<code>n=17</code> each). Holdout-test set (23% <code>n=50</code>).  </p>

<p><strong>Q1</strong> I know that one can do parameters' tuning along with feature selection at the same time (i.e., parallel), but how to evaluate the cost/weights if one would like to evaluate cost-sensitive models (SVM, CART, C5.0) using the PLS scores to counteract class imbalance?    </p>

<p><strong>Q2</strong> What is the alternative approach when reserving a separate data set for cost evaluation (i.e., <code>evaluation set</code>), as recommended, is not possible given the small sample size in this case? can one do tuning of model parameter, feature selection, and cost for imbalance all three at the same time? if not what is the best practice in this case?  </p>

<p><strong>Q3</strong> Given the small sample size, is bootstrapping preferred to CV? if yes how would it be implemented to do exhaustive tuning like above for the PLS scores?    </p>

<p><strong>Q4</strong> Given the imbalance above, is there a way to ensure that each CV training fold would include the minimum number of <code>hard</code> class(es) in order to have good estimation on the CV-test fold? is there any argument to pass to ensure presence of the small classes each time fold would be generated?  </p>

<p><strong>PLS special notes</strong>  </p>

<p>This is the approach in my mind (please correct me if I am missing something somewhere during the course):  </p>

<p>In each CV iteration on the many CV-traning folds, there should be a unique PLS projection matrix for each iteration that would be used in the next second step of getting PLS scores for the the respective CV-test set inheriting the same standardization parameters (<code>mean</code>, <code>sd</code>), this means that two things would be inherited; the PLS projection and the standardization parameters (mean and sd) in order to apply them to the CV-test folds, this way, given the example here, 50 values would be returned hoping to reach the best parameter in question. One complication though, there should be an argument to specify the desired number of PLS components to retain and to be used in calculation of scores out of each CV-test fold (better to be pre-defined in a previous tuning step may be). My expectation, is that after deciding on the best model, there should be a way to get the PLS projection matrix for the whole training set (i.e.<code>n=168</code>) along with (<code>mean</code>, <code>sd</code>) to apply them on the holdout-test to validate the best model. So in total, there would be 50 different PLS projection matrices, means, sds from CV step and 1 extra frothe whole training set, am I right?<br>
Feature selection in this method would entail two things: predictors space and the PLS components space.</p>

<p><strong>Q5</strong> How to perform these two selections (predictor and PLS component) in <code>caret</code>? this is because feature selection here is different than otherwise since here we deal with scores rather than the observations themselves to determine best predictors that to construct the PLS components.</p>

<p><strong>Note:</strong> When one is happy with the best final model, it would be recommended to fit the model on the whole data set <code>n=218</code> to get the correct estimates withe the least uncertainty.  </p>

<p>A similar procedure is implemented in <code>caret::train()</code> function that can be fed with <code>preProc</code> argument to specify the type of desired pre-processing of data (most are mentioned in the help system but I couldn't find <strong>PLS</strong> among them, better if with an argument to specify the desired components similar to PCA pre-processing). I am aware of the fact, that inheriting pre-processing parameters to holdout test set and to CV-test, can only be performed using the <code>predict.train()</code> function, as opposed to calling the generic <code>predict()</code> function to the <code>$finalModel</code> that won't inherit pre-processing parameters.     </p>

<p><strong>Q6</strong> How to implement this strategy (if correctly described) to train and validate the two-step PLS-[classifier] methodology using PLS scores subspace instead of observations making use of <code>caret</code> infrastructure?  </p>

<p>Thanks in advance.</p>
"
"0.0791807582464896","0.0808745799025788"," 82981","<p>I am exploring hierarchical logistic regression, using glmer from the lme4 package. To my understanding, one of the first steps in multilevel modeling is to estimate the degree of clustering of level-1 units within level-2 units, given by the intraclass correlation (to ""justify"" the additional cost of estimating parameters to account for the clustering). When I run a fully unconditional model with glmer</p>

<pre><code>fitMLnull &lt;- glmer(outcome ~ 1 + (1|level2.ID), family=binomial)
</code></pre>

<p>the glmer fit gives me the variance in the intercept for outcome at level-2, but the residual level-1 variance in outcome is nowhere to be found. I read somewhere (can't track it down now) that the residual level-1 variance is <em>not</em> estimated in HGLM (or at least in glmer). Is this true? If so, is there an alternative way to approximate the degree of clustering in the data? If not, how can I access this value to calculate the ICC?</p>
"
"0.0685725481323742","0.062257280636469"," 83446","<p>Suppose you want to find clusters based on a set of variables $Y$, and that you want to estimate the effects of some variables $X$ on membership in those clusters. Here is how I am doing it now.</p>

<p>Step 1: Perform model-based clustering on the variables $Y$ (using the <code>mclust</code> package for this).</p>

<p>Step 2: Optimize a multinomial regression model with cluster membership as the outcome variable.</p>

<p>It seems like there must be a better way in which the models are estimated simultaneously. Anyone know a good tool in R for this and, even better, a good set of references for (a) the statistical model that the package implements, and (b) how to use the package?</p>

<p>Thanks</p>
"
"NaN","NaN"," 85757","<p>I have used <code>hclust</code> function from R for the hierarchical clustering of vectors which are already labeled. </p>

<pre><code>dissimilarity &lt;- 1 - cor(data)
distance &lt;- as.dist(dissimilarity)
plot(hclust(distance),  main=""Dissimilarity = 1 - Correlation"", xlab="""")
</code></pre>

<p>Now I want to evaluate if the vectors with the same label are clustered in the same group. However, I don't know how to find the optimal cutting points in the deprogram. Is there a package for it?</p>

<p>Thanks for your help.</p>
"
"0.0791807582464896","0.0269581933008596"," 86318","<p>I have a semi-small matrix of <strong>binary features</strong> of dimension 250k x 100. Each row is a user and the columns are binary ""tags"" of some user behavior e.g. ""likes_cats"".</p>

<pre><code>user  1   2   3   4   5  ...
-------------------------
A     1   0   1   0   1
B     0   1   0   1   0
C     1   0   0   1   0
</code></pre>

<p>I would like to fit the users into 5-10 clusters and analyze the loadings to see if I can interpret groups of user behavior. There appears to be quite a few approaches to fitting clusters on binary data - what do we think might be the best strategy for this data?</p>

<ul>
<li><p>PCA</p></li>
<li><p>Making a <em>Jaccard Similarity</em> matrix, fitting a hierarchical cluster and then using the top ""nodes"".</p></li>
<li><p>K-medians</p></li>
<li><p>K-medoids</p></li>
<li><p><a href=""http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Clustering/Proximus"">Proximus</a>?</p></li>
<li><p>Agnes</p></li>
</ul>

<p>So far I've had some success with using hierarchical clustering but I'm really not sure it's the best way to go..</p>

<pre><code>tags = read.csv(""~/tags.csv"")
d = dist(tags, method = ""binary"")
hc = hclust(d, method=""ward"")
plot(hc)
cluster.means = aggregate(tags,by=list(cutree(hc, k = 6)), mean)
</code></pre>

<p><img src=""http://i.stack.imgur.com/cyT6t.png"" alt=""enter image description here""></p>
"
"0.0559892510955854","0.0381246425831512"," 86479","<p><img src=""http://i.stack.imgur.com/o5e0M.png"" alt=""enter image description here""></p>

<p>I got this plot when I plotted a clustering object in R. If I run <code>km &lt;- clara(data, 2)</code>, then <code>plot(km)</code>, I get a similar plot. How do we interpret this plot with respect to the clustering? How useful would a similar plot be if I have more than five clusters? To be more precise, <strong>how are principal components related to clusters?</strong></p>
"
"0.0885267789745639","0.0964485644340824"," 86645","<p>I have done some clustering to a matrix with 30 random variables , each variable has 13000 observations ). i got 10 clusters</p>

<p>and now i need to test how good the clustering is by calculating the variance in each cluster.
does anyone knows how can i calculate the variance?</p>

<p>i can easily calculate the variance of each column in my matrix (e.g the variance of each random variable) but i want to calculate the variance of the whole cluster.</p>

<p>does anyone know how it can be done?</p>

<p>e.g.</p>

<pre><code>data &lt;- data.frame(x=c(2,2,2,3,7),
               y=c(30,40,40,30,10),
               z=c(1,2,3,4,5),
               cluster=c('a','a','c','a','c'))

candidates &lt;- dlply(data,.(cluster),function(data){
 laply(data[,-4],var)
})
</code></pre>

<p>This gives variance per column for each cluster label (a,c). I don't think it's the right approach.  </p>
"
"0.0791807582464896","0.107832773203438"," 88151","<p>Is it possible to use a Gaussian Process to relate multiple independent input variables (X1, X2, X3) to an output variable (Y)? </p>

<p>More specifically, I would like to produce a regression graph like the example shown below where confidence interval reduces around clusters of data (i.e. variance is high at x = 1 where there is no data, but x = 0.3 the regression is tight due to the clustering of input variables) and Instead of having one input variable on the x-axis, where would be multiple inputs.</p>

<p><img src=""http://i.stack.imgur.com/4YEAb.png"" alt=""enter image description here""></p>

<p>For example, is it possible to develop a regression relationship that relates the price of houses (HPrice = [125000, 63000, 500000]) to Floor Area (FArea = [856,497,1300]) and the Number of Bedrooms (BedR = [2,2,4])?</p>

<p>Ideally I would like to do this in R, and wonder if there are any recommendations/example available?</p>

<p>Thanks!</p>
"
"0.0395903791232448","0.0539163866017192"," 88789","<p>In SPSS, the user can check the relative variable importance in a clustering result and produce a graph like the following: </p>

<p><img src=""http://i.stack.imgur.com/f4UUd.png"" alt=""enter image description here""></p>

<p>link:
<a href=""http://pic.dhe.ibm.com/infocenter/spssstat/v20r0m0/index.jsp?topic=%2Fcom.ibm.spss.statistics.help%2Fidh_twostep_main.htm"" rel=""nofollow"">http://pic.dhe.ibm.com/infocenter/spssstat/v20r0m0/index.jsp?topic=%2Fcom.ibm.spss.statistics.help%2Fidh_twostep_main.htm</a></p>

<p>Then, we can identify variables dominating predictor importance or having the most impact at determining clusters. Does anyone know how importance is computed here? I would like to implement this metric in R, if it is not already there.</p>
"
"0.193952455150571","0.176090181265125"," 89204","<p>I'm looking for advice on how to analyze complex survey data with multilevel models in R. I've used the <code>survey</code> package to weight for unequal probabilities of selection in one-level models, but this package does not have functions for multilevel modeling. The <code>lme4</code> package is great for multilevel modeling, but there is not a way that I know to include weights at different levels of clustering. <a href=""http://www.statmodel.com/download/asparouhovgmms.pdf"">Asparouhov (2006)</a> sets up the problem:</p>

<blockquote>
  <p>Multilevel models are frequently used to analyze data from cluster sampling designs. Such sampling designs however often use unequal probability of selection at the cluster level and at the individual level. Sampling weights are assigned at one or both levels to reflect these probabilities. If the sampling weights are ignored at either level the parameter estimates can be substantially biased.</p>
</blockquote>

<p>One approach for two-level models is the multilevel pseudo maximum likelihood (MPML) estimator that is implemented in MPLUS (<a href=""http://www.statmodel.com/download/SurveyJSM1.pdf"">Asparouhov et al, ?</a>). <a href=""http://www.biomedcentral.com/1471-2288/9/49"">Carle (2009)</a> reviews major software packages and makes a few recommendations about how to proceed: </p>

<blockquote>
  <p>To properly conduct MLM with complex survey data and design weights, analysts need software that can include weights scaled outside of the program and include the ""new"" scaled weights without automatic program modification. Currently, three of the major MLM software programs allow this: Mplus (5.2), MLwiN (2.02), and GLLAMM. Unfortunately, neither HLM nor SAS can do this.</p>
</blockquote>

<p><a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3630376/"">West and Galecki (2013)</a> give a more updated review, and I'll quote the relevant passage at length:</p>

<blockquote>
  <p>Occasionally, analysts wish to fit LMMs to survey data sets collected from samples with complex designs (see Heeringa et al, 2010, Chapter 12). Complex sample designs are generally characterized by division of the population into strata, multi-stage selection of clusters of individuals from within the strata, and unequal probabilities of selection for both clusters and the ultimate individuals sampled. These unequal probabilities of selection generally lead to the construction of sampling weights for individuals, which ensure unbiased estimation of descriptive parameters when incorporated into an analysis. These weights might be further adjusted for survey nonresponse and calibrated to known population totals. Traditionally, analysts might consider a design-based approach to incorporating these complex sampling features when estimating regression models (Heeringa et al., 2010). More recently, statisticians have started to explore model-based approaches to analyzing these data, using LMMs to incorporate fixed effects of sampling strata and random effects of sampled clusters.</p>
  
  <p>The primary difficulty with the development of model-based approaches to analyzing these data has been choosing appropriate methods for incorporating the sampling weights (see Gelman, 2007 for a summary of the issues). Pfeffermann et al. (1998), Asparouhov and Muthen (2006), and Rabe-Hesketh and Skrondal (2006) have developed theory for estimating multilevel models in a way that incorporates the survey weights, and Rabe-Hesketh and Skrondal (2006), Carle (2009) and Heeringa et al. (2010, Chapter 12) have presented applications using current software procedures, but this continues to be an active area of statistical research. Software procedures capable of fitting LMMs are at various stages of implementing the approaches that have been proposed in the literature thus far for incorporating complex design features, and analysts need to consider this when fitting LMMs to complex sample survey data. Analysts interested in fitting LMMs to data collected from complex sample surveys will be attracted to procedures that are capable of correctly incorporating the survey weights into the estimation procedures (HLM, MLwiN, Mplus, xtmixed, and gllamm), consistent with the present literature in this area.</p>
</blockquote>

<p>This brings me to my question: does anyone have best practice recommendations for fitting LMMs to complex survey data in R?</p>
"
"0.131306432859723","0.162564022190589"," 91134","<p>consider the following example data:</p>

<pre><code>df1 &lt;- data.frame(customer=c(rep(""customer1"",5),rep(""customer2"",10),rep(""customer3"",7)),
                  money_spent=sample(22))

df2 &lt;- data.frame(customer=c(""customer1"",""customer2"",""customer3""),
                  origin=c(""US"",""US"",""UK""),
                  industry_sector=c(""IS1"",""IS2"",""IS3""),
                  currency=c(""USD"",""USD"",""GBP""))
</code></pre>

<p>My actual data consists of about 200000 rows and I would like to examine it in terms of, for instance, do customers from the US spent more money compared to customers from other countries. I would also like to see whether the amount of money spent depends on the industry sector and so on. I have some more explanatory variables apart from origin, industry sector and currency which I would like to look into. Also the number of records for the customers differ so that it might make sense to average the money spent for each customer.</p>

<p>I am not sure about how to best analyse this data. I first thought of cluster analysis, in particular, hierarchical clustering but am not sure whether it can be applied to such data and, in particular, how to structure the data to be put into the function. The R function <code>hclust</code> takes a matrix as the input but how would I structure such a matrix in terms of my data? Could k-means clustering be a better alternative? </p>

<p>Another approach would probably to analyse this data using boxplots and an one-way ANOVA approach to see whether ""money spent"" differs between different countries or industry sectors. However, this approach does not test whether the variables are dependend on each other. To look into this, I have been advised to apply a decision tree first and then do some statistical significance analysis. However, from what I have read so far I cannot see how decision trees can help me to detect variable dependencies.</p>

<p>So, I am wondering whether there any other/better techniques/functions out there which are more suitable for such data? Maybe a time series analysis is more appropriate since we have also recorded the dates when customers spent money.</p>
"
"0.137145096264748","0.140078881432055"," 91348","<p>I am working on data analysis.</p>

<p>Given a group of data vectors, each of them has the same dimension. Each element in a vector is a floating point number. </p>

<pre><code>V1 [  ,   ,   , â€¦ ] 
V2[  ,   ,   , â€¦ ] 
...
Vn [  ,   ,   , â€¦ ] 
</code></pre>

<p>Suppose that each vector has M numbers. M can be 10000.</p>

<p>n can be 200. </p>

<p>I need to find out how to partition the n vectors into sub-groups such that each vector in one subgroup can be represented by a basic vector in the subgroup. </p>

<p>For example, </p>

<p>W = union of V1, V2, V3 â€¦ Vn</p>

<p>Find subgroup i, j, â€¦ t :</p>

<pre><code>Gi = [  V1, V6, V3, V5, â€¦ , Vx ]
Gj = [V22, V11, V56, V45, â€¦ , Vy]
â€¦
Gt = [V78, V90, V9, V12, â€¦ , Vz]
</code></pre>

<p>Such that :</p>

<p>Union of Gi , Gj, â€¦ , Gt is equal to W and there is no overlap among  all Gi , Gj, â€¦ , Gt. </p>

<p>Also , each subgroup has a basic vector that has strong correlation with all other element vector in the subgroup. For example, in Gi, we may have vector Vx as the basic vector such that all other vectors have <strong>strong (linear) correlation</strong>  with Vx. <strong>Here, we measure the linear correlation betwwen two vectors not two data points.</strong> </p>

<p>Moreover, we need to minimize the number of the subgroups, here, it is  "" t "" . It means that given 200 vectors ( n = 200), we prefer a subgroup G1, G2, â€¦, Gt, and t is minimized. For example, we prefer t = 5 over t = 6. if t is more than 10, it may not be useful. </p>

<p>My questions:
What kind of knowledge domain this problem belongs to ? </p>

<p>Is it a clustering analysis ? But, in cluster analysis, one data point is a number, but, here one data point  is a vector.</p>

<p>Are there some statistics models or algorithm can be used to do this kind of analysis ?  Are there some software tools or packages that solve this problem ? </p>

<p>If my questions are not a good fit for this forum, please tell me where I should post it. </p>

<p>R packages do the clustering for data points not for data vector by correlation.</p>

<p>Any help would be appreciated. </p>
"
"0.0969762275752854","0.110056363290703"," 92329","<p>I'm a little new to data mining and would definitely appreciate some tips.
I'm using clustering algorithms looking for possible grouping in some variables described below.
I've been using the Excel data mining add-in which connects to SSAS and uses the EM algorithm by default.  I'm also using R, so far with the Kmeans algorithm.</p>

<p>I have two independent variables X, and G which are integers,
and three dependent variables A, B, C which are related to X and G by the equations below:</p>

<p>$$A=100\left(\frac{2G\cdot.06+K}{2G\cdot.4+2\cdot.06+K+X}\right)$$</p>

<p>$$B=100\left(\frac X{2G\cdot.4+2G\cdot0.06+K+X}\right)$$</p>

<p>C=100-((((2xG)x(0.06)+K)/((2xG)x(.4)+(2xG)x(0.06)+K+X)))x100 + (X/((2xG)x(.4)+(2xG)x(0.06)+K+X))X100)</p>

<p>where K is an arbitrary constant which varies.
Most of the data follows the formulas above with some variation.</p>

<p>I'm using clustering algorithms to look for groupings in the A,B, C variables.
Any advice on strategies for looking for clusters, and how to tell when I've been successful would be greatly appreciated.</p>
"
"0.0885267789745639","0.0723364233255618"," 92985","<p>Recently I have come across usage of cluster plot, which combines k-mean clustering along with PCA. The plot shows different clusters plotted using first two PCs. I have checked some of the threads (<a href=""http://stats.stackexchange.com/questions/31083/how-to-produce-a-pretty-plot-of-the-results-of-k-means-cluster-analysis"">here</a> and <a href=""http://www.dataminingblog.com/combining-pca-and-k-means/"" rel=""nofollow"">here</a>) regarding the usage. </p>

<p><strong>I want to know, during generating a cluster plot, does the data is clustered first and then PCA is done, or the reverse way (PCA followed by k-mean clustering)?</strong></p>

<p>Because the second link says PCA is done followed clustering. But in the first link where an example is shown to generate a cluster plot, data is clustered first and then the cluster plot is generated. </p>

<p>Regarding interpretation, does the plot has to be interpreted as the number of clusters generated or are there any extra points to interpret?</p>
"
"0.0559892510955854","0.0762492851663023"," 93815","<p>I have some experiences with time series modelling, in the form of simple ARIMA models and so on. Now I have some data that exhibits volatility clustering, and I would like to try to start with fitting a GARCH (1,1) model on the data. </p>

<p>I have a data series and a number of variables I think influence it. So in basic regression terms, it looks like: </p>

<p>$$
y_t = \alpha + \beta_1 x_{t1} + \beta_2 x_{t2} + \epsilon_t .
$$</p>

<p>But I am at a complete loss at how to implement this into a GARCH (1,1) - model? I've looked at the <code>rugarch</code>-package and the <code>fGarch</code>-package in <code>R</code>, but I haven't been able to do anything meaningful besides the examples one can find on the internet. </p>
"
"0.0559892510955854","0.0762492851663023"," 94240","<p>I'd like to get clusters with a maximum inner distance threshold.</p>

<p>Now I use <code>hc &lt;- hclust(d)</code> and <code>cutree(hc, numofclasses)</code>.</p>

<p>But I would like to use something like in python:</p>

<pre><code>&gt;&gt;&gt; cl = HierarchicalClustering(data, lambda x,y: abs(x-y))
&gt;&gt;&gt; cl.getlevel(10)     # get clusters of items closer than 10
</code></pre>

<p>How can I get this in R?</p>
"
"0.0685725481323742","0.062257280636469"," 95321","<p>I was thinking of devising a clustering algorithm (for fun and kicks) that would cluster data by looking at the distribution of the data at multiple scales.</p>

<p>For example say my data was distributed on a 2d grid of 1000 x 1000.</p>

<p>Are there algorithms out there that cluster this data by looking at the data by dividing this space up into say</p>

<ol>
<li>10x10</li>
<li>then 20x20</li>
<li>then 40x40</li>
<li>and so forth</li>
</ol>

<p>Would appreciate links to pseudo code or implementations in R, Java, Python, matlab -- preferably open source.</p>

<p>Additional notes:</p>

<ol>
<li>I am not looking for hierarchical clustering where I find clusters within clusters</li>
<li>I would be interested in other definitions of multiscalar</li>
</ol>
"
"0.0791807582464896","0"," 95793","<p>Is there a reason to prefer squaring or not squaring the dissimilarities when clustering with Ward's method?  </p>

<p>The question is motivated by the following statement in the <a href=""http://stat.ethz.ch/R-manual/R-patched/library/stats/html/hclust.html"" rel=""nofollow"">documentation</a> for R's <code>hclust()</code> function:  </p>

<blockquote>
  <p>Two different algorithms are found in the literature for Ward clustering. The one used by option ""<code>ward.D</code>"" (equivalent to the only Ward option ""<code>ward</code>"" in R versions &lt;= 3.0.3) <em>does not</em> implement Ward's (1963) clustering criterion, whereas option ""<code>ward.D2</code>"" implements that criterion (Murtagh and Legendre 2013). With the latter, the dissimilarities are <em>squared</em> before cluster updating.</p>
</blockquote>

<p>Does squaring improve the algorithm?</p>
"
"0.118771137369734","0.125804902070678"," 95844","<p>Given the following data frame:</p>

<pre><code>df &lt;- data.frame(x1 = c(26, 28, 19, 27, 23, 31, 22, 1, 2, 1, 1, 1),
                 x2 = c(5, 5, 7, 5, 7, 4, 2, 0, 0, 0, 0, 1),
                 x3 = c(8, 6, 5, 7, 5, 9, 5, 1, 0, 1, 0, 1),
                 x4 = c(8, 5, 3, 8, 1, 3, 4, 0, 0, 1, 0, 0),
                 x5 = c(1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0),
                 x6 = c(2, 3, 1, 0, 1, 1, 3, 37, 49, 39, 28, 30))
</code></pre>

<p>Such that</p>

<pre><code>&gt; df
   x1 x2 x3 x4 x5 x6
1  26  5  8  8  1  2
2  28  5  6  5  1  3
3  19  7  5  3  1  1
4  27  5  7  8  1  0
5  23  7  5  1  1  1
6  31  4  9  3  0  1
7  22  2  5  4  1  3
8   1  0  1  0  0 37
9   2  0  0  0  0 49
10  1  0  1  1  0 39
11  1  0  0  0  0 28
12  1  1  1  0  0 30
</code></pre>

<p>I would like to group these 12 individuals using hierarchical clusters, and using the correlation as the distance measure. So this is what I did:</p>

<pre><code>clus &lt;- hcluster(df, method = 'corr')
</code></pre>

<p>And this is the plot of <code>clus</code>:</p>

<p><img src=""http://i.stack.imgur.com/ALfbr.png"" alt=""dendogram""></p>

<p>This <code>df</code> is actually one of 69 cases I'm doing cluster analysis on. To come up with a cutoff point, I have looked at several dendograms and played around with the <code>h</code> parameter in <code>cutree</code> until I was satisfied with a result that made sense for most cases. That number was <code>k = .5</code>. So this is the grouping we've ended up with afterwards:</p>

<pre><code>&gt; data.frame(df, cluster = cutree(clus, h = .5))
   x1 x2 x3 x4 x5 x6 cluster
1  26  5  8  8  1  2       1
2  28  5  6  5  1  3       1
3  19  7  5  3  1  1       1
4  27  5  7  8  1  0       1
5  23  7  5  1  1  1       1
6  31  4  9  3  0  1       1
7  22  2  5  4  1  3       1
8   1  0  1  0  0 37       2
9   2  0  0  0  0 49       2
10  1  0  1  1  0 39       2
11  1  0  0  0  0 28       2
12  1  1  1  0  0 30       2
</code></pre>

<p>However, I am having trouble interpreting the .5 cutoff in this case. I've taken a look around the Internet, including the help pages <code>?hcluster</code>, <code>?hclust</code> and <code>?cutree</code>, but with no success. The farthest I've become to understanding the process is by doing this:</p>

<p>First, I take a look at how the merging was made:</p>

<pre><code>&gt; clus$merge
      [,1] [,2]
 [1,]   -9  -11
 [2,]   -8  -10
 [3,]    1    2
 [4,]  -12    3
 [5,]   -1   -4
 [6,]   -3   -5
 [7,]   -2   -7
 [8,]   -6    7
 [9,]    5    8
[10,]    6    9
[11,]    4   10
</code></pre>

<p>Which means everything started by joining observations 9 and 11, then observations 8 and 10, then steps 1 and 2 (i.e., joining 9, 11, 8 and 10), etc. Reading about the <code>merge</code> value of <code>hcluster</code> helps understand the matrix above.</p>

<p>Now I take a look at each step's height:</p>

<pre><code>&gt; clus$height
[1] 1.284794e-05 3.423587e-04 7.856873e-04 1.107160e-03 3.186764e-03 6.463286e-03 
    6.746793e-03 1.539053e-02 3.060367e-02 6.125852e-02 1.381041e+00
&gt; clus$height &gt; .5
[1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE
</code></pre>

<p>Which means that clustering stopped only in the final step, when the height finally goes above .5 (as the Dendogram had already pointed, BTW).</p>

<p>Now, here is my question: <strong>how do I interpret the heights?</strong> Is it the ""remainder of the correlation coefficient"" (please don't have a heart attack)? I can reproduce the height of the first step (joining of observations 9 and 11) like so:</p>

<pre><code>&gt; 1 - cor(as.numeric(df[9, ]), as.numeric(df[11, ]))
[1] 1.284794e-05
</code></pre>

<p>And also for the following step, that joins observations 8 and 10:</p>

<pre><code>&gt; 1 - cor(as.numeric(df[8, ]), as.numeric(df[10, ]))
[1] 0.0003423587
</code></pre>

<p>But the next step involves joining those 4 observations, and I don't know:</p>

<ol>
<li>The correct way of calculating this step's height</li>
<li>What each of those heights actually means.</li>
</ol>
"
"0.0885267789745639","0.0723364233255618"," 96946","<p>I am reading a code for a Bayesian clustering method. Both prior and the likelihood are normally distributed. If I understood correctly, such cases are called ""conjugate priors""</p>

<p>My question is about calculating the posterior mean and variance. So it is implemented in the code as following</p>

<pre><code>d = nrow (data.i)##number of attributes
n = ncol (data.i)##number of replication for each attribute
Smatrix = matrix (1, nrow=d, ncol=d)##correlation of attributes
Imatrix = Smatrix - 1
diag (Imatrix) = 1

prior.precision = solve (sdWICluster^2 * Smatrix + sdTSampling^2 * Imatrix) #inverting the prior correlation matrix (prior.precision)
prior.mean = cluster.mean # mean of each clusters

sample.precision = sdResidual^(-2) * Imatrix
sample.mean = apply (data.i, 1, mean)#mean for each cluster

post.cov = solve (as.matrix(prior.precision + n*sample.precision)) # posterior covariance matrix
post.mean = as.vector (post.cov %*% (prior.precision %*% prior.mean + n*sample.precision %*% sample.mean)) # posterior of the mean
</code></pre>

<p>it seems the code has take the following formula, </p>

<p>$\mu_{po} = C_{po}\times((\mu_{pr} \times \tau_{pr})+(n\times\tau_{li}\times\mu_{li}))$</p>

<p>$C_{po} = \tau_{pr} + n\times\tau_{li}$</p>

<p>As I said, this is the case of conjugates of normal distributions with unknown mean but variance known; however, it does not fit to the formula I have from <a href=""http://www.people.fas.harvard.edu/~plam/teaching/methods/conjugacy/conjugacy_print.pdf"" rel=""nofollow"">here</a>. (or may be it does by my eye is not able to catch). I appreciate if someone make some comments about the code and the formula</p>
"
"NaN","NaN"," 99794","<p>I used <code>kmeans</code> command on my data-frame (as suggested in ""R and Data Mining: Examples and Case studies""). Now my data is clustered into x number of cluster.<br>
What they don't tell you in this book is what to do next.<br>
How can I get <code>characteristics</code> specific for <code>cluster 1</code>.<br>
Or how does <code>cluster 2</code> differ from <code>cluster 3</code>?</p>

<p>Example of my data-frame after clustering.  </p>

<pre><code>Cluster   Char1   Char2   Char3   Char4   Char4
1         0.00    0.02    1.23    3.21    2.34
1         2.12    12.1    1.42    1.31    2.04
2         5.35    59.2    0.01    9.32    9.33
3         5.23    10.3    8.13    0.72    0.91   
...    
</code></pre>

<p>Is there more informative guide on the internet? How can I investigate these things?</p>
"
"0.111978502191171","0.114373927749453"," 99881","<p>My ultimate goal is to run a cluster analysis on a data set with > 1 million records. The input variables for the cluster analysis will be the results of a Principal Component Analysis, as well as other variables not included in the PCA, for a total of maybe 10 variables input into the clustering (the variables I input into the PCA were all very highly correlated with one another while the other variables are not so I chose not to include them in the PCA).   </p>

<pre><code>#read data
mydata &lt;- read.csv('mydata.csv') 

#import library for robust methods because my data contained outliers
library(rrcov) 

#run robust PCA method called PcaCov
pcaR &lt;- PcaCov(~., mydata, na.action=na.omit, center=TRUE, scale = TRUE, k=8)

#look at results
summary(pcaR)
screeplot(pcaR)
pcaR@loadings
</code></pre>

<p>From the results, I have decided I would like to retain the first three components, which capture ~87% of the total variance in the dataset. </p>

<p>Now I want to extract/save/export these first three components for use in the cluster analysis with my other variables. How do I do this? </p>
"
"0.172570461734686","0.197908278398117","100322","<p>I hope this is not too much to read, but I tried to give you a specific overview over my problem.</p>

<p>I am currently trying to model the German electricity market, with a special focus on <a href=""http://www.tennettso.de/site/en/Transparency/publications/network-figures/use-of-balancing-power"" rel=""nofollow"">balancing power</a>.</p>

<p>Considering this setting I have a data set consisting of hourly observations of balancing power in Megawatts for the year of 2013. As the application of this data set would not represent any uncertainty in the data I wanted to use a little trick to ""assume"" uncertainty. While this is really only a basic approximation my model has to have some sort of ""uncertainty"", else there will be a big mistake. In order to do this, I divided my data set into five blocks (too many more would be to much for my calculation) of varying Megawatt-steps calculating the ""probability"" of a specific value to be in one of these blocks using the <a href=""http://en.wikipedia.org/wiki/Cumulative_distribution_function#Complementary_cumulative_distribution_function_.28tail_distribution.29"" rel=""nofollow"">Complementary cumulative distribution function</a>.</p>

<p>What I mean by that is that I took the aforementioned function and tried to best approximate it in a (5-)piece-wise fashion. Unfortunately I had to do this in my example by a ""sense of proportion"". I hope the picture makes it a little more clear (On the x-axis the values are in Megawatts). </p>

<p><strong>EDIT:</strong> To be clear, I am not trying to best-possibly approximate the whole-function in a sense of a non parametric boundary estimation as user603 kindly described in his answer. (Or maybe this can be done using non parametric boundary estimation...?) I just want to fit the five (or whatever number) blocks best-possible to the function in a fashion that, e.g. the Sum of squared Errors is at a minimum. Also: i would love the knot location to be estimated.</p>

<p><img src=""http://i.stack.imgur.com/xh9Q2.png"" alt=""(5-)Piece-wise approximation of the CCDF""></p>

<p>The following code example should further explain what I do:</p>

<pre><code># This function divides the input data into 5 different blocks.
# v, w, x and y are all Megawatt values dividing the data set
getCall_block &lt;- function(v, w, x, y, data){

  n &lt;- nrow(data)
  #' parameters for function
  data_v &lt;- subset(data, data[,2] &lt;= v, select = c(colnames(data)[1],
                                                   colnames(data)[2]))
  data_w &lt;- subset(data, data[,2] &gt; v &amp; data[,2] &lt;= w, select = c(colnames(data)[1],
                                                                  colnames(data)[2]))
  data_x &lt;- subset(data, data[,2] &gt; w &amp; data[,2] &lt;= x, select = c(colnames(data)[1],
                                                                  colnames(data)[2]))
  data_y &lt;- subset(data, data[,2] &gt; x &amp; data[,2] &lt;= y, select = c(colnames(data)[1],
                                                                  colnames(data)[2]))
  data_r &lt;- subset(data, data[,2] &gt; y, select = c(colnames(data)[1],
                                                  colnames(data)[2]))
  #' 
  df_v &lt;- getAverage(data, data_v, 0, v)
  df_w &lt;- getAverage(data, data_w, v, w)
  df_x &lt;- getAverage(data, data_x, w, x)
  df_y &lt;- getAverage(data, data_y, x, y)
  df_r &lt;- getAverage(data, data_r, y, max(data[,2]))

  return(rbind(df_v, df_w, df_x, df_y, df_r))
}
</code></pre>

<p>And the getAverage function...</p>

<pre><code>#This function is called in the above mentioned function. 
#It calculates the mean average of the points of the CCDF lying in one block. 
#v and w are the input parameters in Megawatts. 
#They can be understood as starting point and end point of a given block.

getAverage &lt;- function(data_c, data, v, w){
  #' Imposing greater equal on v if v &gt; 0
  if(w &gt; v){
    if (v &gt; 0){
      v &lt;- v + 1
    } else {
      v &lt;- 0
    }
  #' Initializing div, df, mean, n, sum
    div &lt;- 0
    mean &lt;- 0
    n  &lt;- nrow(data)
    sum &lt;- 0
    df &lt;- data.frame(ID = data[,1], call_freq = numeric(n))
  #' Creating inverse ECDF based on dataset
    F1 &lt;- ecdf(data_c[,2])
  #' Building sum over every data point in the interval [v;w]/(v;w]
    for (i in seq(v, w, 1)){
      sum &lt;- sum + (1 - F1(i))
      div &lt;- div + 1
    }
    mean &lt;- sum / div
    df$call_freq = mapply(function(x) return (x),  mean)
    return (merge(data, df, by = ""ID""))
    } else {
      cat(""Check input parameters. v is greater than w."")
  }
}
</code></pre>

<p>My questions are the following:</p>

<p>I) Does this way of approximating a CCDF sound okay for you guys? What did I overlook or completely mess up?</p>

<p>II) Is there a way to automatically approximate the block sizes, in order to not rely on a sense of proportion. E.g. minimizing the sum of squared errors between the piece-wise function and the original function. Maybe this could be done by using a clustering algorithm like k-means? But then how to cluster this CCDF...?</p>

<p>III) Is there a far more easier way to approximate any function in a piece-wise fashion?</p>

<p>Thanks in advance!</p>
"
"0.197951895616224","0.226448823727221","101077","<p>I have very big data and low number of observations. So I decided to use PCA to reduce dimension of the data. The following is R example (just an dummy example - for workout):</p>

<pre><code>xmat &lt;- matrix(sample(-1:1, 100000, replace = TRUE), ncol = 1000)
colnames(xmat) &lt;- paste (""V"", 1:1000, sep ="""")
rownames(xmat) &lt;- paste(""S"", 1:100, sep = """")
</code></pre>

<p>In this example dataset I have <code>1000</code> variables and <code>100</code> observations / subjects. </p>

<p>I am doing PCA. Lets say.</p>

<pre><code>out &lt;- princomp(xmat)
Error in princomp.default(xmat) : 
  'princomp' can only be used with more units than variables
</code></pre>

<p>Q1: is there a way to reduce dimensionality with <code>p &gt; n</code> ? I would like to use all variables information as opposed to representative ones. Without having proper solution I went anyway to use cluster analysis of variables to categorize the variables and pick the randomly from the clusters. </p>

<p>To create a list of representative variables I tried to cluster the variables.</p>

<pre><code># cluster variables 
d &lt;- dist(t(xmat), method = ""euclidean"") # distance matrix
fit &lt;- hclust(d, method=""ward"")
plot(fit)
groups = cutree(fit,40)
groupd &lt;- data.frame(var = names(groups), group = groups)
</code></pre>

<p>What I am thinking is randomly pick one variable from each group above and use this in PCA. Assume that I have the following y variable.</p>

<pre><code>set.seed(1234)
yvar.d &lt;- data.frame (subject = c(paste(""S"", 1:100, sep = """")), yvar = rnorm (100, 50,10))
</code></pre>

<p><strong>Here is my question</strong>: </p>

<ol>
<li>What could be statistical challenge of using cluster analysis ?</li>
<li><p>Can we use PCA scores in predictions of y. How ? Just multiple
regression or we can introduce something such as variance explained
by each components in the model ?</p>

<p><strong>Edits:</strong></p>

<p>Based on the discussions (see the comments below), I am using different function to do PC analysis.</p></li>
</ol>

<p>""The calculation is done by a singular value decomposition of the (centered and possibly scaled) data matrix, not by using eigen on the covariance matrix. This is generally the preferred method for numerical accuracy. The print method for these objects prints the results in a nice format and the plot method produces a scree plot."" - from function help. </p>

<pre><code>     out1 &lt;- prcomp(xmat)
      out1$x[1:3,1:3]
                      PC1        PC2       PC3
S1  2.940862 -2.7379835  6.527103
S2 -1.081124 -0.5294796 -0.276591
S3  2.375710  0.4505205 -4.236289

   out1$sdev
 screeplot(out1,npcs=30, type=""lines"",col=3) # 30 PCA plotted
</code></pre>

<p><img src=""http://i.stack.imgur.com/gMJys.jpg"" alt=""enter image description here""></p>

<pre><code> out1$rotation
</code></pre>

<p>I also come to see an example in SO <a href=""http://stackoverflow.com/questions/10876040/principal-component-analysis-in-r"">how to use PCA in prediction</a>. Here is my workout: </p>

<pre><code>## take our training and test sets
YY &lt;-  yvar.d$yvar 
prop &lt;- 0.5
train = sample(1:length(YY), round(length(YY)*prop,0))


# data for testing model purpose 
testid = setdiff (1:length(YY), train)
YY1 &lt;- YY
newXPCA &lt;- data.frame(out1$x)
test.data &lt;- data.frame (y = YY1[testid],newXPCA[testid,]) 
test.data[1:10,1:10]

train.data &lt;- data.frame(y= YY1[train],newXPCA [train,])
train.data[1:10,1:10]

## fit the PCA
pc &lt;- prcomp(train.data[, -1])
trainwPC &lt;- data.frame (y = train.data$y, pc$x)

model1 &lt;- lm(y ~ ., data = trainwPC)

#predict() method for class ""prcomp""
test.p &lt;- predict(pc, newdata = test.data)
pred &lt;- predict(model1, newdata = data.frame(test.p), type = ""response"")
pred 
Warning message:
In predict.lm(model1, newdata = data.frame(test.p), type = ""response"") :
  prediction from a rank-deficient fit may be misleading
</code></pre>

<p>I just adopted this script from the SO link, I am not sure about accuracy of the script. </p>

<p>I still have technical questions remaining such as clarification to <strong>remaining question 2</strong> above: </p>

<p>(1) If I want to split data into training and test set by sampling <code>50% of data</code> (as show in the script). Should I do just multiple regression with y and the <code>out1$x</code> ? how many components to use ? is variance of each component play role in good model selection such as avoid over-fitting ? How ? </p>

<p>(2) Clustering (using x clusters) vs PCA analysis (with subset of x components vs all ) what would be statistically favorite for predictions in the situations where have <code>p &gt; n</code> ? As I said to my mind the PCA analysis can use all information but I do not know if there is downside of such information such as <code>over-fitting</code> and ""error consumption"". </p>

<p>Worked example appreciated.   </p>
"
"0.0885267789745639","0.120560705542603","101254","<p>I have some problems in finding the outliers using clustering. </p>

<p>The data.frame is ~20000 observations and each row has mixed types of variables(numeric, nominal and binary). What I want to do is to detect the outliers by clustering.</p>

<p>I have calculated the dissimilarity matrix using daisy() function in R:    </p>

<pre><code>diss = daisy(data,metric=""gower"")
</code></pre>

<p>And I know I can use pam() and hclust() functions to do the clustering. But how do I find the outliers after that?</p>

<p>Here is my R code to find the outliers from pam():</p>

<pre><code>kmedoid = pam(diss,k=10,diss=T)
centers = kmedoid$id.med
distMat = as.matrix(diss)   
distances = rep(-99,20000)   
for (k in 1:20000) {   
  distances[k] = min(distMat[centers,k])   
}   
outliers = order(distances, decreasing=T)[1:5]   
outliers = data[outliers]   
outliers
</code></pre>

<p>I don't know whether it is correct, because the result seems to be pretty different each time when I tried different value of k in pam(). </p>

<p>So the main question is: <strong>Once I have the ""kmedoid"" and ""hc"" calculated below, how do I find the outliers?</strong> </p>

<pre><code>kmedoid = pam(data,k=10,diss=T)
hc = hclust(data)
</code></pre>

<p>I did search Google, but there wasn't much info about this. I am not fluent in programming, so just using existing package and function in R would help me a lot:)</p>

<p>And <strong>is there any better method to find the outliers?</strong> </p>

<p>Thanks! </p>
"
"0.0885267789745639","0.0723364233255618","102984","<p>I have a dataset consists of 5 features : A, B, C, D, E. They are all numeric values. Instead of doing a density-based clustering, what I want to do is to cluster the data in a decision-tree-like manner.</p>

<p>The approach I mean is something like this:</p>

<p>The algorithm may divide the data into X initial clusters based on feature C, i.e. the X clusters may have small C, medium C, large C and very large C values etc. Next, under each of the X cluster nodes, the algorithm further divide the data into Y clusters based on feature A. The algorithm continues until all the features are used.</p>

<p>The algorithm that I described above is like a decision-tree algorithm. But I need it for unsupervised clustering, instead of supervised classification. </p>

<p>My questions are the following:</p>

<ol>
<li>Do such algorithms already exists? What is the correct name for such algorithm</li>
<li>Is there a R/python package/library which has an implementation of this kind of algorithms?</li>
</ol>
"
"0.0685725481323742","0.062257280636469","103244","<p>I'm running a hierarchical clustering on a sample of data using the steps below:</p>

<pre><code>library(RODBC)

setwd('D:/r/cluster2')
channel &lt;- odbcConnectExcel('cluster.xls')
data &lt;- sqlFetch(channel, 'clust9')

y9 &lt;- data.frame(inf=data$infest, faible=data$faible, moyen=data$moyen, fort=data$fort, lon=data$Lon, lat=data$Lat)

y9 &lt;- na.omit(y9) # listwise deletion of missing
y9.use &lt;- y9
y9 &lt;- scale(y9) # standardize variables

wss &lt;- (nrow(y9)-1)*sum(apply(y9,2,var))
for (i in 2:15) wss[i] &lt;- sum(kmeans(y9, centers=i)$withinss)

plot(1:15, wss, type=""b"", xlab=""Number of Clusters"", ylab=""Within groups sum of squares"")

# K-Means Cluster Analysis
fit &lt;- kmeans(y9, 5) # 5 cluster solution

aggregate(y9,by=list(fit$cluster),FUN=mean)

y9 &lt;- data.frame(y9, fit$cluster)

# Ward Hierarchical Clustering

d &lt;- dist(y9, method = ""euclidean"") # distance matrix

fit &lt;- hclust(d, method=""ward"") 

plot(fit) # display dendogram

rect.hclust(fit, k=5, border=""red"")
</code></pre>

<p>and i got this results:</p>

<p><img src=""http://i.stack.imgur.com/65YdS.png"" alt=""enter image description here""></p>

<p>But when I did the same steps the next day I got different results: 
<img src=""http://i.stack.imgur.com/clS5h.png"" alt=""enter image description here""></p>

<p>They are not different in everything, but there are individual that they now belong to another cluster!</p>

<p>so I don't know why  this behavior? i'm interested in interpreting and explaning the results, so when i get different results each time, that will make my previous interpretation wrong, what can I do for now ?</p>
"
"0.104746297470869","0.122270871890883","103280","<p>I'm attempting a project where I need to statistically rank available cars based on several variables such as cost, mpg, seating, milage, etc.. I wish to rank these cars in order decide which car would be the best choice (highest ""worth"") to buy (or best several cars if I was informing multiple people of the best cars to get). As the list of available cars changes from day to day, I will also need to re-run the code every day to allow the rankings to give me the best decision for this new day. </p>

<p>What statistical methods should I use to go about this ranking system? I plan on determining which factors I find most important so the variables used will be subjective in choice. I thought about trying MDS or clustering but I didn't know if that would be relevant since I'm already subjectively determining what variables are to be used. I don't see how regression can be used since I can't get a handle on the ""worth"" of previous cars as that is what I'm trying to rank by. Also, I will be attempting this in R so any helpful packages/functions would be great to know as well.</p>

<p>Any help with how to go about this ranking scheme would be helpful as I'm at a loss.</p>

<p>Thanks so much</p>
"
"NaN","NaN","103482","<p>I am looking for existing implementations for co-clustering (aka biclustering). I came up with <code>biclust</code> function available in MATLAB, but still I am wondering if there are more implementations for this subject, either in MATLAB or R.</p>
"
"0.0791807582464896","0.0539163866017192","104093","<p>In hierarchical clustering procedure, a distance matrix is used to construct a dendrogram with an appropriate method of clustering. In the process of constructing a dendrogram, a <strong>cophenetic matrix</strong> is computed. </p>

<p>I understand that such a cophenetic matrix is used to assess clustering consistency. </p>

<p>How can I use a cophenetic matrix as such to plot the dendrogram ? </p>
"
"0.111978502191171","0.0571869638747267","107448","<p>I'd like to cluster points based on a distance criteria. As
I want to cluster spatial points I am using euclidean distance 
and a hierachical cluster approach. In a final step I'd like to 
cut the dendrogramm at a specific distance to make sure that all 
cluster means are spatially separated by a minimum distance specified. </p>

<p>Here the R code so far:</p>

<pre><code># Creating the spatial points
set.seed(1)
x &lt;- runif(100,0,150)
set.seed(5)
y &lt;- runif(100,0,150)
df &lt;- data.frame(x,y)
plot(df)

# Clustering using a hierachical approach based on euclidean
hc &lt;- hclust(dist(df,method = ""euclidean""), method=""average"")

# Specify a distance of minimum 60 units between cluster means?
df$memb &lt;- cutree(hc, h = 60)

# plot clusters in colors
plot(df$x,df$y,col=df$memb)

# calculate cluster means
cent &lt;- NULL
for(k in 1:length(unique(df$memb))){
      cent &lt;- rbind(cent, colMeans(df[df$memb == k, , drop = FALSE]))
}
cent &lt;- as.data.frame(cent)

# plot points on top of clusters
points(cent$x,cent$y,pch=15)
</code></pre>

<p>However when I am cross-checking the distance between the cluster means, I observe
minimum distances that are slightly below my threshold (e.g. 60):</p>

<pre><code># Summary of distance between cluster means
summary(dist(cent))
</code></pre>

<p>I guess that is related to the clustering method ""average"" which is different
from the cluster means I am calculating thereafter. However, how can I achieve,
that the distance between cluster means (or centroids, etc) is at least 60 units?
What should I use as a measure of cluster mean, and what can I use as a clustering method?</p>
"
"0.0969762275752854","0.110056363290703","107530","<p>i need to impute a dataset all categorical variables before doing analysis. I can just simply <strong>impute</strong> with mode of all data or a variable. </p>

<p>I belief that better option will be to <strong>classify</strong> the subjects (observations) using some short of <strong>clustering algorithm</strong> and then use this information to <strong>impute</strong> the data. The following is small data (though the real data is really big) and the idea.</p>

<pre><code>md &lt;- data.frame(V1 = c(""AA"", ""AA"", ""AA"", NA, ""AB""), V2 = c(""AB"", ""AB"", ""BB"", ""BB"", ""BB""), V3 = c(""BB"", NA, ""BB"", ""BB"", ""BB""), V4 = c(""AA"", ""AA"", ""AA"", ""AA"", ""AA""), 
 V5=c(rep(""AB"", 5)), V6 = c(""BB"", ""BB"", ""AB"", ""AB"", NA), V7 = c(""AB"", ""AB"", ""BB"", ""BB"", ""BB""))

 md
    V1 V2   V3 V4 V5   V6   V7
1   AA AB   BB AA   AB   BB AB
2   AA AB &lt;NA&gt; AA   AB   BB AB
3   AA BB   BB AA   AB   AB BB
4 &lt;NA&gt; BB   BB AA   AB   AB BB
5   AB BB   BB AA   AB &lt;NA&gt; BB
</code></pre>

<p>In visual observation, the sample 1 and 2 are more similar in one cluster while 3,4,5 are in second cluster. The side is hand drawn dendogram (I could not due HC because of missing values). </p>

<p>Now I would like to impute all missing values based on the similarity. For example, missing value in column 1 is most likely to be AA as sample 4 is more similar to 3 than 1, 2, or 5. Similarly the 5 column missing value is BB as the <strong>neighbor</strong> in the cluster is also BB. Similarly column 6 value should be AB as the closest similar has AB. and so on.</p>

<p><img src=""http://i.stack.imgur.com/DIcPA.jpg"" alt=""enter image description here""></p>

<p>Thus completed data would look like:</p>

<p><img src=""http://i.stack.imgur.com/xa5j3.png"" alt=""enter image description here""></p>

<p>How can we perform this ? </p>
"
"0.0685725481323742","0.0933859209547035","108256","<p>I wish to test my time series data for volatility clustering, i.e. conditional heteroskedasticity.</p>

<p>So far, I have used the ACF test on the squared and absolute returns of my data, as well as the Ljung-Box test on the squared data (i.e. McLeod.Li.test).</p>

<p>In a recent paper (<a href=""http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1771862"" rel=""nofollow"">http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1771862</a>, the test is reported on page 8) co-authored by a well-known researcher, they have employed the White test (<a href=""http://ideas.repec.org/a/ecm/emetrp/v48y1980i4p817-38.html"" rel=""nofollow"">http://ideas.repec.org/a/ecm/emetrp/v48y1980i4p817-38.html</a>) to directly test for heteroskedasticity.</p>

<p>I have tried the same approach, however was unable to do so.
From my understanding, the White test needs residual variance (usually from a linear regression model) as an input.</p>

<p>Now my question is: How did the researchers perform the White test? I do not understand which inputs they used for their White test.</p>

<p>While searching for solutions, I have found the sandwich package which uses the vcovHC and vcovHAC functions to estimate a heteroskedasticity-consistent covariance matrix, however the input is also a fitted linear regression model..</p>
"
"0.0969762275752854","0","109266","<p>I have a table of similarities expressed through cosines and am trying to do some cluster analysis in R, using <code>hclust</code> and <code>method=ward</code>.</p>

<p>First I need to turn cosines into squared Euclidean distances, knowing that $d=2(1-\cos)$. No problem. I turned <code>myData</code> into <code>myDataDist</code>.</p>

<p>But then when I use <code>hclust (myDataDist, method=ward)</code> it gives me an error:</p>

<pre><code>must have n &gt;= 2 objects to cluster
</code></pre>

<p>The craziest thing is that if I turn the table of cosines into Euclidean distances with the <code>dist</code> function: <code>myDataDist &lt;- dist(myData, method = ""euclidean"")</code> it works just fine, but then the dendrogram plotted by <code>hclust</code> is wrong. (I know because I tried with another clustering program.)</p>

<p>Has anybody checked the code of <code>dist</code> or <code>ward</code> methods in R? Why doesn't <code>hclust</code> work as it should, with Euclidean squared distances computed manually as $d=2(1-\cos)$?</p>
"
"0.0685725481323742","0.062257280636469","109273","<p>I am working on creating a cluster analysis for some very basic data in r for Windows [Version 6.1.76]. The groups themselves are countries and then I have 2 column with continuous numerical variables. I have applied a Ward Hierachical Method to the data </p>

<pre><code># Applying Ward Hierarchical Clustering
d = dist(conversion_set, method=""euclidean"")
fit = hclust(d, method=""ward"")
</code></pre>

<p>But I don't feel this represents what I am really trying to get to as it is just taking into account the first variable and disregarding the second. Is there a way to include both variables into the clustering calculations?</p>

<p>My data looks similar to this</p>

<p>Country - Var 1 - Var 2</p>

<p>US - 10 - 20</p>

<p>Canada - 5 - 30</p>

<p>....</p>
"
"0.0395903791232448","0","109352","<p>I have a situation where we have a number of quantitative features / variables (p) than the number of samples (n). My objective is to classify these samples into groups (may be hierarchical). I can see a good discussion about this in this <a href=""http://stats.stackexchange.com/questions/15564/clustering-of-10s-of-millions-of-high-dimensional-data"">Q/A</a> post, here at CV. I am aware of discussion about clustering based on high-dimensional data in <a href=""http://en.wikipedia.org/wiki/Clustering_high-dimensional_data"" rel=""nofollow"">wiki</a> and its needs.</p>

<p>Here is data example for workout:</p>

<pre><code>set.seed(123)
# matrix of X variable 
xmat &lt;- matrix(sample(-1:1, 2000000, replace = TRUE), ncol = 10000)
colnames(xmat) &lt;- paste (""M"", 1:10000, sep ="""")
rownames(xmat) &lt;- paste(""sample"", 1:200, sep = """")
</code></pre>

<p>Here are my questions:</p>

<ol>
<li><p>What would be best approach ? </p></li>
<li><p>I am interested to find implementation codes for a suitable method (may be Subspace clustering or Projected clustering or Correlation clustering or Hybrid approaches) for my case.  </p></li>
</ol>
"
"NaN","NaN","109597","<p>I am attempting simple Ward type clustering. However, the R package is proving several choices to use for the distance matrix. I am wondering how I am supposed to determine the right distance matrix method.</p>

<p>Are there any generally acceptable criteria for specific sets of problems?</p>
"
"NaN","NaN","109726","<p>I have a large scatterplot, with about 100,000 (x,y) points. The x coordinate is the set of numbers from (1 to ~100,000) - in other words, no 2 points have the same x-coordinate. The y is mostly constant (around 50-70 in value), but there are key ""regions"" where the y value spikes to ~120 or drops to ~20. How would I statistically differentiate these regions?</p>

<p>Clustering?
Any other ideas?</p>

<p>As a bonus, if you know R, if you could reference a certain method that would be helpful as well.</p>
"
"0.104746297470869","0.0815139145939222","109754","<p><img src=""http://i.stack.imgur.com/QbZ5T.png"" alt=""Consider the following plot.""></p>

<p>I want to identify the regions that are considerably higher than the highest cluster. (The obvious regions which should be identified as their own clusters, notably at the x coordinate ~10 e+07. How would I be able to identify that using a clustering algorithm?</p>

<p>I am using R algorithm kmeans in the picture above. with 6 centers:</p>

<p><code>kmeans(numbers_vector, centers=6, nstart=10)</code></p>

<p>What can I do to alleviate the inadequacy of this algorithm? Use a different clustering algorithm? Have more centers? But If I have more centers, it identifies many more regions in the center (namely clusters 3,5, and 6). Any ideas? </p>

<p>Here is histogram and density. It is important to note that the histogram and density plot DO NOT show the spike ~x=10e+07, because the number of points involved in the spike ~20, perhaps are completely overshadowed by the ~107,350 points plotted.</p>

<p><img src=""http://i.stack.imgur.com/1zrGE.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/Od8v7.png"" alt=""enter image description here""></p>

<p>Data: x (88289 obs.);   Bandwidth 'bw' = 0.7574</p>

<pre><code>   x                 y            
</code></pre>

<p>Min.   : -1.272   Min.   :0.000e+00<br>
 1st Qu.: 40.114   1st Qu.:4.110e-06<br>
 Median : 81.500   Median :3.167e-05<br>
 Mean   : 81.500   Mean   :6.035e-03<br>
 3rd Qu.:122.886   3rd Qu.:2.886e-03<br>
 Max.   :164.272   Max.   :4.827e-02  </p>
"
"0.0969762275752854","0.0220112726581406","109949","<blockquote>
  <p>The one used by option ""ward.D"" (equivalent to the only Ward option
  ""ward"" in R versions &lt;= 3.0.3) does not implement Ward's (1963)
  clustering criterion, whereas option ""ward.D2"" implements that
  criterion (Murtagh and Legendre 2014).</p>
</blockquote>

<p>(<a href=""http://stat.ethz.ch/R-manual/R-patched/library/stats/html/hclust.html"" rel=""nofollow"">http://stat.ethz.ch/R-manual/R-patched/library/stats/html/hclust.html</a>)</p>

<p>Apparently ward.D does not implement Ward's criterion properly. Nonetheless it seems to do a good job regarding the clusterings it produces. What does method=""ward.D"" implement if it is not Ward's criterion?</p>

<p><strong>References</strong></p>

<p>Murtagh, F., &amp; Legendre, P. (2014). Wardâ€™s hierarchical agglomerative clustering method: which algorithms implement Wardâ€™s criterion?. <em>Journal of Classification</em>, <strong>31</strong>(3), 274-295.</p>
"
"0.0885267789745639","0.0723364233255618","110205","<p>I have two questions about output of hierarchical clustering and improving the output.</p>

<p>I'm trying to learn more about performing hierarchical clustering in R so I started looking at a simple dataset I created of sushi rolls at a <a href=""http://www2.beyondmenu.com/22361/hicksville/blue-fish-hicksville-11801.aspx?r=22361"" rel=""nofollow"">local restaurant</a>. I went though every roll on the menu and created a distinct list of the union of all ingredients.</p>

<p>Then for each roll I put a 1 if it had that particular ingredient or a 0 for not having it. I then calculated distance based on Jaccard distance and created a dendogram using four different methods.</p>

<p>So my first question is how to interpret these correctly. Since many rolls overlap on quite a bit of ingredients, single-linkage is producing clusters that aren't significantly different from each other? Complete-linkage and the two methods are better reflecting the diversity in the rolls where some are significantly different? <strong>I'm not sure I really understand the subtleties between these four methods</strong>.</p>

<p><img src=""http://i.stack.imgur.com/f2dcn.png"" alt=""enter image description here""></p>

<p>My second question is <strong>how would one handle related attributes</strong>? Say for example there exists {Tuna, White Tuna, Spicy Tuna} as three different ingredients. I think we would agree that the distance between {Tuna,Avocado} &amp; {Salmon, Avocado} should be larger than {Tuna, Avocado} &amp; {Spicy Tuna, Avocado}. Are the attributes typically collapsed into {Tuna} or is there another way to reflect the relationship?</p>
"
"0.0685725481323742","0.062257280636469","110622","<p>I have some prior knowledge of grouping, but this may be incorrect or is not sufficient as I need larger number of groups (i.e. subgroups). For example in the following data I have 3 groups in addition to two variables. I would like to use the group information (as prior knowledge) (here 3 groups) to create meaningful groups (here 9 groups/clusters). Is there a correct way to perform such analysis.</p>

<pre><code># Dummy data 
group &lt;- rep(1:3, each =3000)
X &lt;- c(rnorm(1000, 0.1, 0.04), rnorm(1000,0.2, 0.04), rnorm(1000, 0.4, 0.02),
       rnorm(1000, 0.4, 0.04), rnorm(1000,0.5, 0.08), rnorm(1000, 0.6, 0.12), 
       rnorm(1000, 0.7, 0.08), rnorm(1000,0.8, 0.1), rnorm(1000, 0.9, 0.06)
)

Y &lt;-  c(rnorm(1000, 0.5, 0.04), rnorm(1000,0.6, 0.04), rnorm(1000, 0.7, 0.04),
       rnorm(1000, 0.35, 0.12), rnorm(1000,0.45, 0.04), rnorm(1000, 0.3, 0.02), 
       rnorm(1000, 0.55, 0.09), rnorm(1000,0.65, 0.12), rnorm(1000, 0.65, 0.04)
)
</code></pre>

<p>Prior information of 3 clusters:</p>

<pre><code>col = c(""red"", ""cyan"", ""green"")
plot(cbind(X,Y), col = col[group], pch = ""."")
</code></pre>

<p><img src=""http://i.stack.imgur.com/v5xUI.jpg"" alt=""enter image description here""></p>

<p>Clustering analysis assuming 9 clusters.</p>

<pre><code>cl &lt;- kmeans(cbind(X,Y), 9)

colrs &lt;- c(""red"",""purple"", ""yellow"", ""tan"", ""pink"", ""cyan"", ""blue"", ""green"", ""black"")
plot(cbind(X,Y), col = colrs[cl$cluster], pch = ""."")
</code></pre>

<p><img src=""http://i.stack.imgur.com/tecHn.jpg"" alt=""enter image description here""></p>
"
"0.0685725481323742","0.062257280636469","110722","<p>I have a large data set that I would like to cluster using spherical K means algorithm. However, I am relatively new to this subject and R in general. Most of my knowledge is self taught and I am still in the beginning stages- I have read all about K means clustering in the past few days and I would like to apply it to my own project (184 rows of 4000+ columns containing measurements in decimal values). How and where do I start?</p>

<p>I am trying to teach myself how to go about this in R but I can't seem to find examples online on how to do this, or at least they don't apply to my case. I was wondering if anyone here would know about a step by step tutorial or has access to a script in which a spherical k means clustering has been conducted in R, and if you could share it with me. I have found a few papers but they are very advanced, and most are for text clustering and not numeric values.</p>

<p>I hope this question is not too vague. Does anyone have experience in this subject and could guide me on which steps I have to take to get started? Absolute beginner here, so I apologize if I am completely wrong to ask such a question on this platform.</p>

<p>Thank you for taking time out of your day to read this!</p>
"
"0.118771137369734","0.125804902070678","111145","<p>I have two variables - X and Y and I need to make cluster maximum (and optimal) = 5. Let's ideal plot of variables is like following:</p>

<p><img src=""http://i.stack.imgur.com/WW2Ya.jpg"" alt=""enter image description here""></p>

<p>I would like to make 5 clusters of this. Something like this:</p>

<p><img src=""http://i.stack.imgur.com/ImEqZ.jpg"" alt=""enter image description here""></p>

<p>Thus I think this is mixture model with 5 clusters. Each clusters have center point and a confidence circle around it.</p>

<p>The clusters are not always pretty like this, they look like the following, where sometime two clusters are close together or one or two clusters are completely missing.</p>

<p><img src=""http://i.stack.imgur.com/ingLA.jpg"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/LJDmZ.jpg"" alt=""enter image description here""></p>

<p>How can fit mixture model and perform classification (clustering) in this situation effectively?</p>

<p>Example:</p>

<pre><code>set.seed(1234)
X &lt;- c(rnorm(200, 10, 3), rnorm(200, 25,3),
        rnorm(200,35,3), rnorm(200,65, 3), rnorm(200,80,5))
Y &lt;- c(rnorm(1000, 30, 2))
plot(X,Y, ylim = c(10, 60), pch = 19, col = ""gray40"")
</code></pre>
"
"0.0559892510955854","0","112749","<p>I have a dataset of items and I want to measure how my clustering method works. I'm using R and simple k-means clustering. I have a lets say gold standard of my clusters and I want to see how good is the result of my clustering algorithm based on the features I used. Is there any straightforward way to do that in R? I was thinking about something like Jaccard similarity per item. I mean the intersection/union of clusters containing each item and somehow mixing them together!</p>
"
"0.0685725481323742","0.062257280636469","113232","<p>I'm trying to run a discrete-time multilevel hazard analysis comparable to the model proposed by Barber et al. I am attempting to model the hazard of migrating internationally using predictors at the individual, household, community, and regional levels.
(Most of the variables of interest are at the individual, community, and regional levels--I just need to account for clustering by household)</p>

<p>Is there a package that will allow me to do this in R?  I've used lme4 for regular multilevel modeling, but can it also be used for multilevel survival models?  How would I go about coding such a model? (And if this can't be done in R, can it be done in Stata, or do I have to bite the bullet and buy and learn HLM or MLwiN?)  </p>

<p>Please let me know if you need any additional information.  Thanks!</p>

<p>ETA: Barber et al. refers to:
<a href=""http://onlinelibrary.wiley.com/doi/10.1111/0081-1750.00079/abstract"" rel=""nofollow"">Barber, Jennifer S., Susan A. Murphy, William Axinn, and Jerry Maples. 2000. ""Discrete-Time Multilevel Hazard Analysis."" Sociological Methodology, 30: 201-235.</a></p>
"
"0.125195771459034","0.153448726380857","113462","<p>I am performing the hierarchical clustering analysis on a dataset of 25 viral populations using 3 viral components (variables) to construct a dendrogram with average method and correlation distance calculation. We firstly used the <code>hclust()</code> method generate a plot, but we still need to support the dendrogram by statistical analysis. So after transposing the data, we choose <code>pvclust()</code> to generate the dendrogramm. However, the plots constructed by <code>pvclust</code> and the one generated by <code>hclust</code> are totally different. We used the same data and same parameters (average method and correlation distance), but the results are so different. Why might this be?
Here is the dataset <a href=""https://github.com/yingfengisu/RSHOP/blob/master/TVA1.csv"" rel=""nofollow"">https://github.com/yingfengisu/RSHOP/blob/master/TVA1.csv</a></p>

<h1>hclust</h1>

<blockquote>
  <p><img src=""http://i.stack.imgur.com/PaHfr.png"" alt=""enter image description here""></p>
</blockquote>

<pre><code>######### hclust method #############
sd.data=scale(tav.data)
dd=as.dist(1-cor(t(sd.data)))  # correlation-based distance
plot(hclust(dd, method=""average""), main=""Average Linkage with Correlation-Based Distance"",xlab="""", sub="""", labels=tav.labs)
</code></pre>

<blockquote>
  <p><img src=""http://i.stack.imgur.com/i02XB.png"" alt=""enter image description here""></p>
</blockquote>

<hr>

<h1>pvclust</h1>

<blockquote>
  <p><img src=""http://i.stack.imgur.com/EDRjX.png"" alt=""enter image description here""> </p>
</blockquote>

<pre><code>######### pvclust method ################
tav.data0 = tav.data[,c(1,2,3)]
rownames(tav.data0)&lt;-tav.labs
tav.data0 = as.data.frame(t(tav.data0))
sd.data0 = scale(tav.data0)
library(pvclust)
result=pvclust(sd.data0,method.hclust=""average"", method.dist=""correlation"",nboot=100,r=seq(0.7,1.4,by=.1))
plot(result)
</code></pre>

<blockquote>
  <p><img src=""http://i.stack.imgur.com/CH3Fl.png"" alt=""enter image description here""></p>
</blockquote>
"
"0.131306432859723","0.0812820110952944","113504","<p>I have some code that looks for clusters in x,y data. To check the number of clusters I use, I want to get the BIC. This is not possible (easily) using <code>kmeans()</code>, and so I've switched to the <a href=""http://cran.r-project.org/web/packages/mclust/index.html"" rel=""nofollow"">mclust package</a>. Specifically, I'm trying to replace <code>kmeans()</code> from the R stats package, with <code>Mclust()</code> from the mclust package. </p>

<p>Using <code>Mclust()</code> requires me to specify which model should be used for the clustering. According to <code>?Mclust</code>, the following models can be used in <code>Mclust()</code>:</p>

<pre><code>univariate mixture      
""E""  =   equal variance (one-dimensional)
""V""  =   variable variance (one-dimensional)
multivariate mixture        
""EII""    =   spherical, equal volume
""VII""    =   spherical, unequal volume
""EEI""    =   diagonal, equal volume and shape
""VEI""    =   diagonal, varying volume, equal shape
""EVI""    =   diagonal, equal volume, varying shape
""VVI""    =   diagonal, varying volume and shape
""EEE""    =   ellipsoidal, equal volume, shape, and orientation
""EEV""    =   ellipsoidal, equal volume and equal shape
""VEV""    =   ellipsoidal, equal shape
""VVV""    =   ellipsoidal, varying volume, shape, and orientation
single component        
""X""  =   univariate normal
""XII""    =   spherical multivariate normal
""XXI""    =   diagonal multivariate normal
""XXX""    =   ellipsoidal multivariate normal
</code></pre>

<p>I'm presuming that k-means in stats is a ""spherical, unequal volume"" model, ie. to get <code>k-means(x = data, centers = 6)</code> to match <code>mclust()</code>, I should use <code>mclust(data, G = 6, modelNames = c(""VII""))</code>. </p>

<p>However, in the limited tests I've done, this gives different cluster centroids. The example below uses 6 clusters with some test data. The centroids obtained through each method are shown.</p>

<p><img src=""http://i.stack.imgur.com/NJrQ0.png"" alt=""enter image description here""></p>

<p>Can anyone confirm which <code>mclust()</code> model is equivalent to <code>kmeans()</code>?</p>
"
"0.0685725481323742","0.0933859209547035","113680","<p>My problem is the very idea of how to start the analysis of 2D point patterns, specifically how to find linear trends within their spatial pattern. </p>

<p>I have XY data points which are organized like in the plot, which I have manually marked with red lines in order to highlight what should be determined:</p>

<p><img src=""http://i.stack.imgur.com/pbHhA.png"" alt=""enter image description here""></p>

<p><a href=""http://stats.stackexchange.com/questions/82598/clustering-2d-data-using-kernel-density-methods"">This post</a> suggests the use of kernel density estimation (which is the usual method I guess), but I would like to find out if there is any simpler way of doing this, avoiding the use of spatial statistics. R solution is preferred.</p>

<p><strong>UPDATE:</strong></p>

<p>When using the solution suggested by whuber (Hough transform) I get these lines as the most frequent ones in my scatterplot:</p>

<p><img src=""http://i.stack.imgur.com/yMSNz.png"" alt=""enter image description here""></p>

<p>Obviously these are not the lines I was searching for, but nevertheless it would be interesting to determine the very points that lie on the red line, for example.  </p>
"
"NaN","NaN","114100","<p>Are there any packages that implement the Autoclass/ Naive Bayes Clustering algorithm in R or Python? </p>

<p>Alternatively, what are some other clustering algorithms that can handle both categorical and numeric variables that are implemented in either R or Python?</p>
"
"0.104746297470869","0.122270871890883","114184","<p>Specifically, are there any binomial regression models that use a kernel with heavier tails and higher kurtosis than the standard kernels (logistic/probit/cloglog)?</p>

<p>As a function of the linear predictor $\textbf{x}'\mathbf{\hat{\beta}}$, the logistic distribution</p>

<ul>
<li>Underestimates the probability of my data being in the tails of the distribution</li>
<li>Underestimates the kurtosis, or clustering of data, in the middle of the distribution:</li>
</ul>

<p>This can be seen from a diagnostic plot of my fit:</p>

<p><img src=""http://i.stack.imgur.com/ar6OW.png"" alt=""enter image description here""></p>

<ul>
<li>The red line is the logistic CDF, representing a perfect fit</li>
<li>The black line represents the fitted probabilities from my dataset (calculated by binning observations into 0.1 intervals of $\textbf{x}'\mathbf{\hat{\beta}}$, where $\mathbf{\hat{\beta}}$ is obtained from my fit)</li>
<li>The grey bars in the background represent number of observations on which the true probabilities are based upon</li>
<li>The grey areas are where the tail 10% of the data lie (5% each side).</li>
</ul>

<p>Ideally, any solution would use R.</p>

<h2>Edit</h2>

<p>Why am I talking about CDFs? Our GLM equation is:</p>

<p>$$\mathbb{P}(Y = 1) = \mathbb{E}[Y] = g^{-1}(\textbf{x}'\mathbf{\beta})$$</p>

<p>Where $g$ is the link function.</p>

<p>Further, if $g^{-1}$ is a valid probability distribution (i.e. monotonically increasing from 0 to 1, indeed the case with probit, logit, cloglog), then consider a latent (not directly observed) continuous random variable $Y^{*}$ whose distribution (CDF) is given by $g^{-1}$. Then by definition</p>

<p>$$\mathbb{P}(Y^{*} \leq \textbf{x}'\mathbf{\beta}) = g^{-1}(\textbf{x}'\mathbf{\beta})$$</p>

<p>Equating the two equations above, we see the probability of $Y=1$ is exactly equal to the CDF of $Y^{*}$</p>

<p>$$\mathbb{P}(Y = 1) = \mathbb{P}(Y^{*} \leq \textbf{x}'\mathbf{\beta})$$</p>

<p>Hence I talk interchangeably about the expected response $\mathbb{E}[Y]$ and CDF of $Y^{*}$ over linear-predictor ($\textbf{x}'\mathbf{\hat{\beta}}$) space.</p>
"
"0.0559892510955854","0.0381246425831512","115715","<p>I've been scratching my head about this problem for some time:</p>

<p>I have a big gene expression dataset (20k genes x 200 samples) in matrix A and i have a subset of this dataset (i.e. 40 genes x 200 samples) copied in matrix B. I would like to find groups of genes in matrix A that resemble as close as possible the patterns in matrix B.</p>

<p>I am looking into various clustering methods including KNN and SOTA where for example I would be training KNN with matrix B and cluster matrix A...but I'm not sure this is actually giving me the answer I want.</p>

<p>any suggestions?</p>
"
"0.0685725481323742","0.0933859209547035","116136","<p>Say you've $N$ functions $f_N(x)$ defined on a regular grid $x$.  You don't know the form of $f(x)$, you've only got several realizations of it.  The different functions are related to each other somehow, but you don't know exactly how.  You want to pick $n$ representative ones, or equivalently, group them into $n$ clusters.</p>

<p>Here is an example:</p>

<pre><code>N = 1000
library(MASS)
B = mvrnorm(N,mu=c(0,0,.0001),Sigma = matrix(c(1,.5,-.3,.5,.5,-.2,-.3,-.2,1),3))
X = cbind(1,x)
dim(B)
dim(X)
f = B[,1:2]%*%t(X)+as.matrix(rexp(N,rate = (abs(B[,3]))^.1))%*%t(as.matrix(sin(x)))
plot(1:10,cex=0,xlim=c(0,10),ylim=c(-10,10))
for (i in 1:nrow(f)){
    lines(x,f[i,],col=rgb(1,0,0,.2))
    }
</code></pre>

<p>(A plot will pop up)</p>

<p>I want to group them somehow.  I'm not sure how.  Intuitively, I want them organized by where they start, whether they go up or down, and how wiggly they are.  But we're supposing that I didn't know the data generating process, I've only got that matrix <code>f</code>.</p>

<p>I was thinking I could do a taylor expansion on each row of <code>f</code>, out to some arbitrary order $t$.  This would give me an approximation $f(x) = X'\beta$, where $X$ is $1,x,x^2,...,x^t$ and $\beta$ is the transformed Taylor approximation coefficients.  I was thinking of then k-means clustering by those $\beta$'s.  </p>

<p>My question:  does that approach make sense?  I just came up with it, but I don't know if any better methods have already been figured out.  </p>

<p>And if it does make sense, how do you do a Taylor series in R, getting what I'm conceptualizing as $\beta$?</p>
"
"0.0969762275752854","0.0880450906325624","116994","<p>I learned in elementary statistics that, with a general linear model, for inferences to be valid, observations must be independent. When clustering occurs, independence may no longer hold leading to invalid inference unless this is accounted for. One way to account for such clustering is by using mixed models. I would like to find an example dataset, simulated or not, which demonstrates this clearly. I tried using one of the sample datasets on the <a href=""http://www.ats.ucla.edu/stat/stata/library/cpsu.htm"" rel=""nofollow"">UCLA site for analysing clustered data</a></p>

<pre><code>&gt; require(foreign)
&gt; require(lme4)
&gt; dt &lt;- read.dta(""http://www.ats.ucla.edu/stat/stata/seminars/svy_stata_intro/srs.dta"")

&gt; m1 &lt;- lm(api00~growth+emer+yr_rnd, data=dt)
&gt; summary(m1)

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 740.3981    11.5522  64.092   &lt;2e-16 ***
growth       -0.1027     0.2112  -0.486   0.6271    
emer         -5.4449     0.5395 -10.092   &lt;2e-16 ***
yr_rnd      -51.0757    19.9136  -2.565   0.0108 * 


&gt; m2 &lt;- lmer(api00~growth+emer+yr_rnd+(1|dnum), data=dt)
&gt; summary(m2)

Fixed effects:
             Estimate Std. Error t value
(Intercept) 748.21841   12.00168   62.34
growth       -0.09791    0.20285   -0.48
emer         -5.64135    0.56470   -9.99
yr_rnd      -39.62702   18.53256   -2.14
</code></pre>

<p>Unless I'm missing something, these results are similar enough that I wouldn't think the output from <code>lm()</code> is invalid.  I have looked at some other examples (e.g. <a href=""http://www.bristol.ac.uk/cmm/learning/course-topics.html#m05"" rel=""nofollow"">5.2 from the Bristol University Centre for Multilevel Modelling</a>) and found the standard errors are also not terribly different (I am not interested in the random effects themselves from the mixed model, but it is worth noting that the ICC from the mixed model output is 0.42).</p>

<p>So, my questions are 1) under what conditions will the standard errors be markedly different when clustering occurs, and 2) can someone provide an example of such a dataset (simulated or not).</p>
"
"0.111978502191171","0.133436249041029","117783","<p>Despite having only a single binary outcome for each ID, there are multiple correlated measurements for the same test for each ID at different timepoints. The individual IDÂ´s are obviously independent, but the measurements of the same test at different time-points are not independent. The aim is to see whether the test can discern if the patients is cured or not. </p>

<p>-I cannot decide whether IÂ´d need to fit a mixed-level logistic regression model or if a regular logistic regression model would suffice. Is it possible somehow to fit a logistic regression model with time varying covariates like in cox models?</p>

<p>-Is it possible to use a clustering or CART based model (allowing for the longitudinal independent variable) instead that would be easier to comprehend in a clinical setting?</p>

<p>-I could turn the question around and do repeated measure Anova with <code>summary(aov(val~Long_term*time+Error(id),data=stat))</code>- however, 1)only time and the interaction term are significant, leaving me uncertain how to treat the other main effect of Long_term in face of only a significant interaction and 2)it feels contraintuitive to set an independent variable as a dependent variable when using anova.</p>

<pre><code>&gt; dput(stat)
structure(list(Long_term = structure(c(2L, 2L, 2L, 2L, 2L, 1L,  1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L,  2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L,  2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L,  2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L,  1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L,  2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L,  2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L,  2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L,  2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L,  1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L,  1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L,  2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L,  2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L,  1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L,  2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L,  2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L,  1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L,  2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L,  2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L,  1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 1L, 2L), .Label = c(""No"",  ""Yes""), class = ""factor""), id = c(1L, 2L, 3L, 4L, 5L, 6L, 7L,  8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 1L,  2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L,  16L, 17L, 18L, 19L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,  11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 1L, 2L, 3L, 4L,  5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,  19L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L,  14L, 15L, 16L, 17L, 18L, 19L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L,  9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 1L, 2L,  3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L,  17L, 18L, 19L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L,  12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 1L, 2L, 3L, 4L, 5L, 6L,  7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L,  1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L,  15L, 16L, 17L, 18L, 19L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L,  10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 1L, 2L, 3L,  4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L,  18L, 19L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L,  13L, 14L, 15L, 16L, 17L, 18L, 19L, 1L, 2L, 3L, 4L, 5L, 6L, 7L,  8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 1L,  2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L,  16L, 17L, 18L, 19L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L,  11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 1L, 2L, 3L, 4L,  5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L,  19L), time = c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,  0, 0, 0, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,  12, 12, 12, 12, 12, 12, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18,  18, 18, 18, 18, 18, 18, 18, 18, 18, 24, 24, 24, 24, 24, 24, 24,  24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 30, 30, 30, 30,  30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 36,  36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,  36, 36, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42,  42, 42, 42, 42, 42, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48,  48, 48, 48, 48, 48, 48, 48, 48, 54, 54, 54, 54, 54, 54, 54, 54,  54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 60, 60, 60, 60, 60,  60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 66, 66,  66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66,  66, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72,  72, 72, 72, 72, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78,  78, 78, 78, 78, 78, 78, 78, 84, 84, 84, 84, 84, 84, 84, 84, 84,  84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 90, 90, 90, 90, 90, 90,  90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 90, 96, 96, 96,  96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96, 96,  102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102,  102, 102, 102, 102, 102, 102), val = c(273, 194, 618, 755, 802,  395, 2438, 482, 502, 692, 607, 618, 579, 864, 579, 453, 673,  572, 707, 57, 373, 1197, 1026, NA, 697, 712, NA, NA, 616, NA,  NA, NA, NA, NA, NA, NA, 76, 1128, 560, 76, 819, 982, 303, 1294,  267, 1117, 346, 996, 652, 95, 951, 3250, 1584, 948, 981, 465,  411, 57, 197, 535, 498, 87, 1382, 210, 1649, 96, 450, 252, 42,  1086, 2137, 1395, 464, 1388, 532, 67, 25, 230, 566, 545, 38,  691, 216, 1412, 33, 151, 113, 29, 663, 806, 528, 240, 1508, 421,  50, 39, NA, 182, 412, 32, 414, 232, 868, 791, 201, 86, 33, 250,  345, 224, 381, 1069, 536, NA, NA, NA, 500, 312, NA, 287, 97,  227, 653, 69, 69, NA, NA, 225, 308, 256, 963, 420, NA, NA, NA,  368, 605, NA, 399, 69, 77, 20, 39, 70, NA, 122, 306, 103, 175,  807, 530, NA, NA, NA, 246, 443, NA, 363, 87, 39, NA, 25, 63,  NA, 163, 289, 172, 128, 1019, 582, NA, NA, NA, 231, 820, NA,  284, NA, 40, NA, NA, NA, NA, 238, 288, 217, NA, 903, 471, NA,  NA, NA, 236, 577, NA, 461, NA, 691, NA, NA, NA, NA, 158, 170,  168, NA, 681, 434, NA, NA, NA, 399, 634, NA, 85, NA, 83, NA,  NA, NA, NA, 72, 419, NA, NA, 912, NA, NA, NA, NA, NA, 635, NA,  295, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,  NA, NA, 138, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,  NA, NA, NA, NA, NA, NA, NA, NA, 251, NA, NA, NA, NA, NA, NA,  NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 37, NA, NA, NA,  NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 132,  NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA)), row.names = c(NA,  -323L), .Names = c(""Long_term"", ""id"", ""time"", ""val""), class = ""data.frame"") 
</code></pre>
"
"0.0559892510955854","0.0381246425831512","118272","<p>I am trying to implement K means clustering in R, 
Here is what my data look like:</p>

<pre><code> Seq                RegionNames(Zip) X%year(PercentChange)
4002                   53147      -1.683282e-02
4003                   28504      -1.807185e-02
4004                   10591      -5.432917e-03
4005                   96761       1.151578e-02
4006                   32750       5.905045e-03
4007                   54904      -1.193602e-04
4008                   97140       2.667454e-02
4009                   33774       1.932240e-02
4010                   43616      -1.159712e-03
4011                   89011       3.021237e-02
</code></pre>

<p>I am trying to cluster zip codes (<code>RegionNames</code>) based on percentage change in 5 years(<code>X5Year</code>)</p>

<p>Here is my code</p>

<pre><code>    newallHomesZip &lt;- data.frame(allHomesZip$RegionName,allHomesZip$X5Year) #Making new data frame with only zip and 5 yearly percentage increase
    (cl &lt;- kmeans(na.omit(newallHomesZip), 2))  #omiting Null values and trying to form two clusters
    plot(x, col = cl$cluster) #plotting
</code></pre>

<p>here is the plot I get :</p>

<p><img src=""http://i.stack.imgur.com/7CJwL.png"" alt=""enter image description here""></p>

<p>Clearly the clusters are not good.</p>

<p>I don't really understand the Kmean method, I want to form clusters based on percent change as magnitude and zip code as identifier</p>
"
"0.118771137369734","0.0898606443361987","119922","<p>We have a longitudinal panel of X users with their online spending patterns and are trying to measure certain metrics within the panel. We have time series information about the users such as their total online spending, browsing habits, spending per online merchant etc. We also have cross sectional fixed data about the user like their geolocation, some demographic info etc. We are trying to look at a certain time series metric across the population of say X users. </p>

<p>Examples of what we are trying to measure</p>

<ol>
<li>Total growth rate of spending month over month </li>
<li>Spend per transaction month over month </li>
<li>Spend per merchant per month  </li>
<li>Other monthly (or period of choice) metrics</li>
</ol>

<p>If all X users were reporting in the panel the entire time the exercise is easy, we simply calculate metric we need. </p>

<p>However only a small percent (15%) of the panel is reporting the entire length of the panel. Most users come into the panel late or drop out early. For each individual in the panel, the exact lifespan in the panel is fairly random. Moreover, some month, their usage is not complete, its partial and thus should not count, but this is a secondary concern.</p>

<p>The primary challenge is how to accurately calculate the metrics in question given this setup. One solution would be to construct a panel of users who are only present in the panel the entire length of the panel (lets call it Full Life User Panel). This would be a small % of users and assume that the users that are not in that panel behave in the same way as the users who are.  </p>

<p>We are not aiming to measure the effect of one set of parameters on another. I.e. we are not trying to predict the spending at a specific merchant given information about the usersâ€™ other spending patterns and fixed attributed.</p>

<p>We can try to cluster the users using longitudinal clustering or some kind of latent growth curve analysis to â€œimputeâ€ the missing data. I havenâ€™t found any landmark canonical material on this topic and would appreciate any help in addressing the question or references.  </p>
"
"0.0791807582464896","0.0808745799025788","121506","<p>I have both numeric and binary data in my data set with 73 observations.
I read a lot about which distance metric and which clustering technique to use especially from this web site.
I decided to use Gower distance metrics and K-medoids.
In R, I used package ""cluster"", and function ""daisy"" with metric=""gower"".
So I got a 73*73 matrix.
Now, as I understood, this is not a distance matrix, it is a similarity matrix that I am confused what to do after now.
I use function pam: pam(x, k, diss = inherits(x, ""dist"")...
Should I use the 73*73 matrix which I got from daisy function? </p>
"
"0.0395903791232448","0","122532","<p>I have 17 numeric and 5 binary (0-1) variables, with 73 samples in my dataset. I know that the Gower distance is a good metric for datasets with mixed variables.</p>

<p>When I use daisy function in cluster package, with metric=""gower"" I don't want to standardize my numeric variables, bcs. they are already in percent (0.25, 0.35...).</p>

<p>I also don't want it, becuase I couldn't get any good results when I used this Gower distance matrix for clustering with pam {cluster}, and I thought this standardization process may cause bad results. </p>

<p>So, is there any way not to standardize?</p>
"
"0.142745141943918","0.149537151138645","123040","<p>I have a matrix of 336x256 floating point numbers (336 bacterial genomes (columns) x 256 normalized tetranucleotide frequencies (rows), e.g. every column adds up to 1).</p>

<p>I get nice results when I run my analysis using principle component analysis. First I calculate the kmeans clusters on the data, then run a PCA and colorize the data points based on the initial kmeans clustering in 2D and 3D:</p>

<pre><code>library(tsne)
library(rgl)
library(FactoMineR)
library(vegan)
# read input data
mydata &lt;-t(read.csv(""freq.out"", header = T, stringsAsFactors = F, sep = ""\t"", row.names = 1))
# Kmeans Cluster with 5 centers and iterations =10000
km &lt;- kmeans(mydata,5,10000)
# run principle component analysis
pc&lt;-prcomp(mydata)
# plot dots
plot(pc$x[,1], pc$x[,2],col=km$cluster,pch=16)
    # plot spiderweb and connect outliners with dotted line
    pc&lt;-cbind(pc$x[,1], pc$x[,2])
    ordispider(pc, factor(km$cluster), label = TRUE)
ordihull(pc, factor(km$cluster), lty = ""dotted"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/QviDR.png"" alt=""enter image description here""></p>

<pre><code># plot the third dimension
pc3d&lt;-cbind(pc$x[,1], pc$x[,2], pc$x[,3])
    plot3d(pc3d, col = km$cluster,type=""s"",size=1,scale=0.2)
</code></pre>

<p><img src=""http://i.stack.imgur.com/fNiri.png"" alt=""enter image description here""></p>

<p>But when I try to swap the PCA with the t-SNE method, the results look very unexpected:</p>

<pre><code>tsne_data &lt;- tsne(mydata, k=3, max_iter=500, epoch=500)
plot(tsne_data[,1], tsne_data[,2], col=km$cluster, pch=16)
    ordispider(tsne_data, factor(km$cluster), label = TRUE)
ordihull(tsne_data, factor(km$cluster), lty = ""dotted"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/6WKYl.png"" alt=""enter image description here""></p>

<pre><code>plot3d(tsne_data, main=""T-SNE"", col = km$cluster,type=""s"",size=1,scale=0.2)
</code></pre>

<p><img src=""http://i.stack.imgur.com/Hqqpf.png"" alt=""enter image description here""></p>

<p>My question here is why the kmeans clustering is so different from what t-SNE calculates. I would have expected an even better separation between the clusters than what the PCA does but it looks almost random to me. Do you know why this is? Am I missing a scaling step or some sort of normalization?</p>

<p>Thanks for your advice.</p>
"
"0.0395903791232448","0","123706","<p>I am performing a cluster analysis with a 4K by 200+ table and my data mostly looks like this:    </p>

<pre><code>item1   item2 item3 item4
 21      35    0     17
  0      0     0     0
  0      0     23    0
  0      32    0     0
  0      0     0     0
 34      0     0     0
</code></pre>

<p>Majority are zero's, and because of that I can't create a proper clustering. Should I remove some data or perform factor analysis. I am using R. </p>
"
"0.0395903791232448","0","123787","<p>I have a sample data below that is from a large data set, where each participant is given multiple condition for scoring.</p>

<pre><code> Participant&lt;-c(""p1"",""p1"",""p2"",""p2"",""p3"",""p3"")
 Condition&lt;-c( ""c1"",""c2"",""c1"",""c2"",""c1"",""c2"")
 Score&lt;-c(4,5, 5,7,8,2)
 T&lt;-data.frame(Participant, Condition, Score)
</code></pre>

<p>I am trying to use K-mean clustering to split participants in different groups, is there any good way to do it, considering the condition is not numeric?</p>

<p>thanks!</p>
"
"0.0559892510955854","0.0381246425831512","124182","<p>I am fitting a geeglm model and it is not recognizing the clustering.</p>

<p>my model is</p>

<pre><code>geeglm(formula = Consumption~Price + Income + Embargod + Observation, 
   id=Id, 
   waves=Observation,
   corstr=""ar1"")
</code></pre>

<p>and the (partial) output is</p>

<pre><code>Estimated Correlation Parameters:
  Estimate Std.err
  alpha        0       0
Number of clusters:   342   Maximum cluster size: 1 
</code></pre>

<p>Why is it saying that I have 342 clusters and the max cluster size is 1?</p>

<p>Id is a factor variable with 18 levels and 19 observations in each, so there should be 18 clusters with a max size of 19.</p>
"
"0.0559892510955854","0.0381246425831512","124475","<p>I want to decide the appropriate number of clusters after using <code>hclust()</code> and drawing a dendogram. I don't have any idea about how many clusters should be there while doing clustering. If I can get the overall similarity measure of each step in hierarchical clustering process I can decide the number of clusters which is the step with largest gap of overall similarity measure. Can I get it using r?</p>
"
"NaN","NaN","127310","<p>I am running hierarchical clustering with a distance matrix M_norm:</p>

<pre><code>hc &lt;- hclust(M_norm^2, method = ""ward.D"")
plot(hc,  cex = 1, hang = -1)
</code></pre>

<p>When I use different rownames and colnames in M_norm, the resulting dendrogram changes a little bit: heights where certain branches are joined are not the same as before. The height of the final join is also different.</p>

<p>The order of rows and columns in the input matrix is now different, but the distances between units are the same. I understand that the order of units at the bottom of the picture can change, but how can this happen? Is the implementation of this algorithm not deterministic?</p>
"
"0.104746297470869","0.0815139145939222","127536","<p>Sampling weights, the inverse probability of a unit's selection into the sample, and other more complex and adjusted weights are very often used in the social sciences. There is statistical software that allows weighting of observations/cases, like the <code>hclust</code> function from the <code>R</code>-package <code>cluster</code>. </p>

<p>In regression analysis, there is an ongoing debate when the usage of observation weights is appropriate (see e.g. Winship/Radbill 1994). I could not find anything concerning observation weights in textbooks about cluster analysis, if weighting is discussed, it is mostly about variable weighting. One exemption is the manual of the <code>R</code>-package <code>WeightedCluster</code>, which discusses observation weighting in more detail. The documentation of the <code>cluster</code> package is not very helpful, as it only shows a trivial example using the weighting option <code>hclust(..., members=""..."")</code> where the number or weight of cases is untouched.</p>

<ol>
<li>Therefore, I am looking for references and recommendations with observation/case weighting in cluster analysis, especially hierarchical cluster analysis. </li>
<li>As I could not find the actual formula for the <code>hclust(..., members=""..."")</code> function : Which parameters changes in the hierarchical cluster algorithm if one uses observation weights? How does that affect the algorithm?</li>
</ol>

<p>In order to get an idea of the difference between clustering with and without case weights, here is an example using weights from survey data and the R-code:
<img src=""http://i.stack.imgur.com/BYiLY.png"" alt=""Reweighting of clustering by using membership""></p>

<pre><code>require(survey)
data(api)
whc &lt;- hclust(dist(apiclus2[, c(""pct.resp"", ""meals"")]), method=""ward.D2"", 
              members=apiclus2$pw)
uwhc &lt;- hclust(dist(apiclus2[, c(""pct.resp"", ""meals"")]), method=""ward.D2"")
opar &lt;- par(mfrow = c(1, 2))
plot(whc,  labels = FALSE, hang = -1, main = ""Weighted survey data"")
plot(uwhc, labels = FALSE, hang = -1, main = ""Unweighted survey data"")
</code></pre>

<h3>References</h3>

<ul>
<li>Studer, M., 2013: WeightedCluster Library Manual. A practical guide to creating typologies of trajectories in the social sciences with R. LIVES Working Papers 24. Lausanne.</li>
<li>Winship, C. &amp; L. Radbill, 1994: Sampling Weights and Regression Analysis. Sociological Methods &amp; Research 23: 230â€“257.</li>
</ul>
"
"0.0791807582464896","0.0539163866017192","130974","<p>I need to use binary variables (values 0 &amp; 1) in k-means. But k-means only works with continuous variables. I know some people still use these binary variables in k-means ignoring the fact that k-means is only designed for continuous variables. This is unacceptable to me.</p>

<p>Questions:  </p>

<ol>
<li>So what is the statistically / mathematically correct way of using binary variables in k-means / hierarchical clustering?</li>
<li>How to implement the solution in SAS / R?</li>
</ol>
"
"0.153332879014365","0.167053813916911","132629","<p>I was trying to implement some clustering tendency tools in R, namely the Hopkin's index and the Coxâ€“Lewis index.</p>

<p>Here is the <a href=""https://books.google.com.sg/books?id=QgD-3Tcj8DkC&amp;pg=PA899&amp;lpg=PA899&amp;dq=clustering+tendency+%5BSergios_Theodoridis,_Konstantinos_Koutroumbas&amp;source=bl&amp;ots=lUUv2H7uh4&amp;sig=oq2Ax1wtX2MnHSrMvuOqLSSj9cU&amp;hl=en&amp;sa=X&amp;ei=VuitVO6kBoySuATXo4KYCA&amp;ved=0CCMQ6AEwAQ#v=onepage&amp;q=clustering%20tendency%20%5BSergios_Theodoridis%2C_Konstantinos_Koutroumbas&amp;f=false"" rel=""nofollow"">link</a> at page 901 to show what they are</p>

<p>This is what I managed to come up with in R. For the distance, I use the Euclidean distance. I also use additional R packages: data.table and RANN</p>

<pre><code>library(data.table)
library(RANN)
hopkins=function (data)
{
   #Number of samples
   m = round(0.2 * nrow(data))

   #Index of my data to be choosen as data samples
   sample = round(runif(m, 0, nrow(data)))

   #Get distance to nearest neigbour for each sample (W)
   #Get rid of first column as it is 0
   nearestW = nn2(data,data[sample,],k=2)$nn.dists[,-1]

   #Get the random sample from uniform distribution for each column
   samp = function(data,no.of.samples,min,max)
   {
    #Get the first and last quantile for every column
    quantile = quantile(x=data,probs=c(0.25,0.75))
    points = runif(n=no.of.samples,min=quantile[1],max=quantile[2])
    return(points)
   }

   uniform.sample = data[, lapply(.SD,samp,no.of.samples=m), .SDcols=names(data)]

  #Get distance to nearest data for each uniform sample (U)
  nearestU = nn2(data,uniform.sample,k=1)$nn.dists[,1]

  #Apply Hopkins index
  return(sum(nearestU)/(sum(nearestW)+sum(nearestU)))
}

CoxLouis=function (data)
{
  #Number of samples
  m = round(0.2 * nrow(data))

  #Get the random sample from uniform distribution for each column
  samp = function(data,no.of.samples,min,max)
  {
    #Get the first and last quantile for every column
    quantile = quantile(x=data,probs=c(0.25,0.75))
    points = runif(n=no.of.samples,min=quantile[1],max=quantile[2])
    return(points)
  }

  uniform.sample = data[, lapply(.SD,samp,no.of.samples=m), .SDcols=names(data)]

  #Get distance to nearest data for each uniform sample (U)
  nearest = nn2(data,uniform.sample,k=1)
  nearestU = nearest$nn.dists[,1]

  #Get the index of the nearest data
  nearest.dataindex = nearest$nn.idx[,1]

  #Get distance to nearest neigbour for each data (W)
  #Get rid of first column as it is 0
  nearestW = nn2(data,data[nearest.dataindex,],k=2)$nn.dists[,-1]

  #Apply Coxâ€“Lewis index
  return(mean(nearestU/nearestW))
}
</code></pre>

<p>Running hopkins(data.table(iris[,1:4])) gives 0.756792 and Coxâ€“Lewis(data) gives 2.709349.</p>

<p>On the other hand, the lecture notes from this <a href=""http://www.cs.rpi.edu/~zaki/www-new/pmwiki.php/Dmcourse/Main?action=download&amp;upname=chap18.pdf"" rel=""nofollow"">link</a> gives a much higher value of 0.935 for the Hopkin's index. I suspect that it has something to do with the way I am creating samples from a uniform distribution. </p>

<p>What I did was to generate uniform samples at random along each dimension. As for the min and max values, I use the first and last quantile respectively instead of the data min and max value. I thought that it will be more robust.</p>

<p>Can someone enlighten me if I am indeed going on the right direction for that ?  </p>
"
"0.0791807582464896","0.0539163866017192","132676","<p>I have read some topics in this web side that, it is not true to use Gower's dissimilarity matrix for Ward's clustering algorithm. </p>

<p>I have mixed type variables, first I had a dissimilarity matrix with Gower's formula in R (daisy function). I had a distance matrix, elements are between 0-1. So at the same time my data is standardized.</p>

<p>Then I used Ward's technique with hclust function.</p>

<p>I had a good dendogram. So, I used Gower to standardize my data set and than I used Wards method, why this way is not true?</p>
"
"0.0885267789745639","0.0723364233255618","133682","<p>So, I'm relatively new to using Gower's distance to do cluster analysis. I've done some research on this for a little while and like the fact it can incorporate categorical variables. To get a better understanding of how it works in practice, I tried simulating data and playing around with the PAM function in R. I simulated my data as such:</p>

<pre><code>let &lt;- LETTERS[1:5]
dich &lt;- c(1L,0L)
a &lt;- as.data.frame(cbind(rnorm(50, 100, 25), sample(let, 50, replace=T), sample(dich, 50, replace=T)))
colnames(a) &lt;- c(""iq"",""let"",""dich"")
rownames(a) &lt;- 1:50
</code></pre>

<p>I then ran it through PAM with various values for number of clusters (k). When I generate the silhouette plots to check how well the observations fit the assigned cluster, it always seems to be at it's best at k=10, with an average silhouette width of .49 (one observation was put into its own cluster).</p>

<p>Obviously this is no coincidence (5 letters times 2 values of dichotomous variable = 10). My assumption is that the algorithm makes the cuts at the categorical level before evaluating the continuous. Is this correct? Or is it the way I'm generating the data that is creating this? I also made sure there wasn't balanced amounts of observations across the categorical variables.</p>

<p>If my assumption is accurate, I don't know if I want cluster assignments made that way. I'll be using demographic data, and I don't want one cluster to be completely males and one cluster completely females. Any insight would be most helpful!</p>

<p>Here's my clustering code:</p>

<pre><code>dist &lt;- daisy(a, metric=""gower"")
clust &lt;- pam(dist, k=10, diss=T)
plot(clust)
</code></pre>
"
"0.0685725481323742","0.062257280636469","134538","<p>I have multiple dataframes each representing traffic speed for each day of the year (366 dataframes for 366 days of the year). The raws of the dataframe are timestamp from 00:00 to 23:55 at 5 minute intervals and the columns are mileposts at 0.5 mile intervals and the entries are speed of traffic corresponding to the specific time and milepost. </p>

<p>I want to group days of similar traffic conditions to examine daily traffic patterns/variations, which is standard for traffic analysis at a macro level, e.g., examining traffic patterns during weekdays and weekends.</p>

<p>To do this, I will have to measure similarity of the dataframes and apply clustering algorithms. Any idea on how to calculate similarity of dataframes and cluster them? Any R package that can do this?</p>

<p>Thanks</p>
"
"0.0791807582464896","0.0808745799025788","134842","<p><strong>Scenario</strong></p>

<p>I have a project about fraud detection where i need to find outliers by kmeans. </p>

<ol>
<li><p>I have a dataset about bank credits length of 1000.  There are 21</p></li>
<li><p>columns (14 categorical, 7 numeric columns).</p></li>
</ol>

<p><strong>Issue</strong></p>

<p>I want to find outliers by clustering data and I need to put all outliers inside the same cluster. How can I achieve this with R.</p>

<p><strong>my tries</strong></p>

<p>I have tried by <code>""lofactor""</code>, but categorical columns caused me an error. </p>

<p>I deleted categorical columns, then it worked. </p>

<p><strong>results</strong></p>

<p>But I shouldn't delete categorical columns since they are also important for determining outliers.</p>

<p>So how can I achieve to find outlier pattern in R?</p>
"
"NaN","NaN","134862","<p>Can you please tell me when to use the K-mean clustering and hierarchical clustering algorithm and what is the different between them...</p>

<p>Regards,</p>

<p>Rahul</p>
"
"0.201872115699962","0.190329720497016","136594","<p>I'm trying to plot all the steps of a k-means algorithm with r, but I can't. </p>

<p>The k-means algorithm works in this way:</p>

<ul>
<li>Step 1. Initialize the center of the clusters</li>
<li>Step 2. Assign the closest initial centers to each data point</li>
<li>Step 3. Set the position of each cluster to the mean of all data points belonging to that cluster</li>
<li>Step 4. Assign the closest cluster to each data point </li>
<li>Step 5. Repeat steps 3-4 until convergence</li>
</ul>

<p>I plot the dataset and initial centers of clusters (Step 1). Too, I can plot the new cluster centers and show which point belongs to each cluster (Step 3 and 4). But I don't know how to plot the Step 2. I need <strong>the very first initial centers membership of each point</strong>, before the first iteration, but <code>kmeans()</code> doesn't give it you. How could I calculate this?</p>

<p>Here is my code:</p>

<pre><code>set.seed(2009)
points1 &lt;- data.frame(x=rnorm( 50,1,0.1), y=rnorm(50,5,0.1))
points2 &lt;- data.frame(x=rnorm( 50,5,0.1),  y=rnorm(50,5,0.1))
points3 &lt;- data.frame(x=rnorm(200,3,0.8),y=rnorm(200,3,0.8))
df &lt;- rbind(points1, points2, points3)

p &lt;- ggplot(df, aes(x, y))
p + geom_point(size=7, color=""grey"") + labs(title=""Initial configuration"")

y &lt;- c(4.88871745,4.88099143,3.69713723)
x &lt;- c(0.75606015,1.26736958,3.04961545)
kcenters &lt;- data.frame(x,y)

p + geom_point(size=7, color=""grey"") + 
    geom_point(data=kcenters, aes(x, y), size=7, color=""black"", shape=""x"") + 
    labs(title=""Initial centers"")

dfCluster &lt;- kmeans(df, centers=kcenters, iter.max=1)

p + geom_point(size=7, aes(colour=as.factor(dfCluster$cluster))) + 
        geom_point(data=data.frame(dfCluster$center), aes(x, y), size=7, 
               color=""black"", shape=""x"") + 
    theme(legend.position=""none"") + 
    labs(title=""First iteration"")
</code></pre>

<p>My goal would be to show the initial center membership of each point in ""Initial centers"" plot.</p>

<hr>

<p>Edit:</p>

<p>I think I did not explain myself properly.</p>

<p>On this website there is a simulation showing what I would like to get:</p>

<p><a href=""http://www.onmyphd.com/?p=k-means.clustering"" rel=""nofollow"">http://www.onmyphd.com/?p=k-means.clustering</a></p>

<p>When you click the ""Iteration"" button the first time (click1), the initial centers are placed. Pressing a second time (click 2), points are assigned to closer center, and painted with different colors. When you click the third time (click3), new centers are calculated, and when you press for the fourth time (click4), points are assigned to closer center again.</p>

<p>When you run <code>kmeans()</code> and stop it at the first iteration, you get the new centers of the clusters ( click3 ), <code>dfCluster$center</code>, and cluster membership of each point (click4), <code>dfCluster$cluster</code>, but you do not get the initial center membership of each point (click 2), which is exactly what I'm looking for.</p>

<hr>

<p>I finally accomplished what I wanted: a step-by-step k-means. Sorry if the code it's not perfect, I'm a newbie with R.</p>

<pre><code>#How does k-means work

library(ggplot2)

set.seed(2009)
points1&lt;-data.frame(x=rnorm(50,1,0.1),y=rnorm(50,5,0.1))
points2&lt;-data.frame(x=rnorm(50,5,0.1),y=rnorm(50,5,0.1))
points3&lt;-data.frame(x=rnorm(200,3,0.8),y=rnorm(200,3,0.8))
df&lt;-rbind(points1,points2,points3)

#plot initial points
p &lt;- ggplot(df, aes(x, y))
p + geom_point(size=7, color=""grey"")

#set initial centers
kcenters&lt;-df[c(49,26,297),]

#plot centers
p + geom_point(size=7, color=""grey"") + geom_point(data=kcenters, aes(x, y), size=7, color=""black"", shape=""x"")

#assignment (to calculate distances to initial centers and to allocate points to the cluster to which they are closest)
library(reshape)
distances &lt;- melt(as.matrix(dist(df,diag=T,upper = T)), varnames = c(""row"", ""col""))
dist_center1&lt;-subset(distances,col==49,select = value)
dist_center2&lt;-subset(distances,col==26,select = value)
dist_center3&lt;-subset(distances,col==297,select = value)
dist_centers&lt;-data.frame(dist_center1,dist_center2,dist_center3)
colnames(dist_centers)&lt;-c(""dist_center1"",""dist_center2"",""dist_center3"")
dist_centers$cluster&lt;-apply(dist_centers, 1, which.min)
df&lt;-cbind(df,dist_centers)

#plot assignment
p + geom_point(size=7, aes(colour=as.factor(df$cluster))) + geom_point(data=kcenters, aes(x, y), size=7, color=""black"", shape=""x"") + theme(legend.position=""none"")

#calculate new centers
x&lt;-tapply(df$x,df$cluster,mean)
y&lt;-tapply(df$y,df$cluster,mean)
kcenters&lt;-data.frame(x,y)

#plot new centers
p + geom_point(size=7, aes(colour=as.factor(df$cluster))) + geom_point(data=kcenters, aes(x, y), size=7, color=""black"", shape=""x"") + theme(legend.position=""none"")
</code></pre>

<p>And then, you can continue the procedure slightly adjusting the code above:</p>

<pre><code>#assignment
df&lt;-rbind(df[,1:2],kcenters)
row.names(df) &lt;- NULL
distances &lt;- melt(as.matrix(dist(df,diag=T,upper = T)), varnames = c(""row"", ""col""))
dist_center1&lt;-subset(distances,col==301,select = value)
dist_center2&lt;-subset(distances,col==302,select = value)
dist_center3&lt;-subset(distances,col==303,select = value)
dist_centers&lt;-data.frame(dist_center1,dist_center2,dist_center3)
colnames(dist_centers)&lt;-c(""dist_center1"",""dist_center2"",""dist_center3"")
dist_centers$cluster&lt;-apply(dist_centers, 1, which.min)
df&lt;-cbind(df[1:300,],dist_centers[1:300,])

#plot assignment
p + geom_point(size=7, aes(colour=as.factor(df$cluster))) + geom_point(data=kcenters, aes(x, y), size=7, color=""black"", shape=""x"") + theme(legend.position=""none"")

#calculate new centers
x&lt;-tapply(df$x,df$cluster,mean)
y&lt;-tapply(df$y,df$cluster,mean)
kcenters&lt;-data.frame(x,y)

#plot new centers
p + geom_point(size=7, aes(colour=as.factor(df$cluster))) + geom_point(data=kcenters, aes(x, y), size=7, color=""black"", shape=""x"") + theme(legend.position=""none"")
</code></pre>

<p>If you run <code>kmeans()</code> with the same initial centers and stop it on first iteration, <code>dfCluster&lt;-kmeans(df,centers=kcenters, iter.max = 1)</code>, you get the follow centers:</p>

<pre><code>&gt; dfCluster$centers
         x        y
1 1.129419 4.905327
2 2.928011 2.880839
3 4.715513 4.766608
</code></pre>

<p>These centers do not match with the ones I get in the first iteration of my procedure (#calculate new centers). I have to run it for 14 times (#assigment and #calculate new centers) to obtain them. I don't know the meaning of ""iteration"" in the <code>kmeans()</code> procedure. Does anybody know?</p>
"
"0.104746297470869","0.0611354359454417","137291","<p>My goal is to cluster time series based on their DTW distance. Therefore I've calculated full distance matrices as input for several clustering algorithms. I first had a look at hierarchical methods, since the number of clusters don't have to be specified at the beginning (moreover k-means is problematic because of the problem of averaging time series under DTW and k-medoids is expensive). </p>

<p>Single linkage (not really useful), complete linkage and average linkage (UPGMA/WPGMA) are unproblematic methods, another criterion which seems to be often used is the Ward method (in R: <code>ward.D2</code> for the <code>hclust</code>-function). I've seen at least <a href=""http://www.prasa.org/proceedings/2012/prasa2012-14.pdf"" rel=""nofollow"">one paper</a> which uses the Ward method with DTW distances, however I am bit skeptical about the usage of Ward in this context. The ward distance of two clusters $A,B$ is defined, according to that paper, as: </p>

<p>$$D_{AB}=\frac{||c_A-c_B||^2}{1/|A|+1/|B|}$$</p>

<p>Where $c_A,c_B$ is the centroid of A, B. </p>

<p>My questions are:  </p>

<ol>
<li><p>How the centroids are calculated if only a distance matrix is given? (I'm guessing this calculation can't be applied when DTW distances are used.) </p></li>
<li><p>Since the application of the Ward method with DTW distances seems to be questionable, are there any other alternative (hierarchical) clustering methods, which can be used with a DTW distance matrix?  </p></li>
</ol>
"
"0.0969762275752854","0.110056363290703","138591","<p>I have a dataset in CSV format that looks as follows:</p>

<pre><code>guid,eventA.location,eventA.time,eventB.location,eventB.time,...
a12b,server3,1424474828.1804667,server7,1424474828.1804668,...
a12c,server3,1424474829.4444667,server2,1424474838.3334668,...
</code></pre>

<p>Each row has a unique <code>guid</code>, and the columns come in pairs of location and time.  The locations are one of a small set of 10 possible values, <code>server1</code> through <code>server10</code>.  The times are in seconds since epoch.  There are 400 <code>guid</code>s, and about 40 events (so about 80 columns).  Some cells may have NA values, but not too many, so I'm happy to get rid of the rows that have them.</p>

<p><strong>How do I perform a k-means clustering on this data, and then create a nice plot of it?</strong>  Not sure how to go about handling the non-numeric data, the N/A data, the fact that the time scale is very tight (within 30s, so relative to the absolute values of these since-epoch times, the differences look negligible but really aren't), etc.</p>

<p>Here's what I've tried so far, but not gotten very far, and the error messages don't make much sense to me:</p>

<pre><code>&gt; x &lt;- read.csv('/path/to/file')
&gt; km &lt;- kmeans(x, 3) 
Error in do_one(nmeth) : NA/NaN/Inf in foreign function call (arg 1)
In addition: Warning messages:
1: In do_one(nmeth) : NAs introduced by coercion
2: In do_one(nmeth) : NAs introduced by coercion
&gt; km &lt;- kmeans(na.omit(x), 3)
Error in sample.int(m, k) : invalid first argument
&gt; km &lt;- kmeans(factor(na.omit(x)), 3)
Error in sort.list(y) : 'x' must be atomic for 'sort.list'
Have you called 'sort' on a list?
</code></pre>

<p>I've also run <code>daisy(na.omit(x))</code> but I'm not sure what to make of the output:</p>

<pre><code>Dissimilarities :
dissimilarity(0)

Metric :  mixed ;  Types = N, I, N, I, N, I, N, I, N, I, N, I, N, I, N, I, N, I, N, I, N, I, N, I, A, I, N, I, N, I, N, I, N, I, N, I, N, I, A, I, N, I, N, I, N, I, N, I, A, N, I, N, I, N, I, N, I, N, I, N, I, N, A, N, I, N, I, N, I, N, I, N, I, N, I, N, I, N, I, A, I, N, I, N, I, N, I, N, I, N, I, N
Number of objects : 0
There were 50 or more warnings (use warnings() to see the first 50)
</code></pre>
"
"0.131306432859723","0.113794815533412","139490","<p>I am looking to group/merge nodes in a graph using graph clustering in 'r'.</p>

<p>Here is a stunningly toy variation of my problem.</p>

<ul>
<li>There are two ""clusters""</li>
<li>There is a ""bridge"" connecting the clusters</li>
</ul>

<p>Here is a candidate network:<br>
<img src=""http://i.stack.imgur.com/2dqy4.png"" alt=""enter image description here""></p>

<p>When I look at the connection distance, the ""hopcount"", if you will, then I can get the following matrix :</p>

<pre><code> mymatrix &lt;- rbind(
     c(1,1,2,3,3,3,2,1,1,1),
     c(1,1,1,2,2,2,1,1,1,1),
     c(2,1,1,1,1,1,1,1,2,2),
     c(3,2,1,1,1,1,1,2,3,3),
     c(3,2,1,1,1,1,1,2,3,3),
     c(3,2,1,1,1,1,1,2,2,2),
     c(2,1,1,1,1,1,1,1,2,2),
     c(1,1,1,2,2,2,1,1,1,1),
     c(1,1,2,3,3,2,2,1,1,1),
     c(1,1,2,3,3,2,2,1,1,1))
</code></pre>

<p>Thoughts here:</p>

<ul>
<li>By luck or due to the simplicity of the toy the matrix has obvious patches this is not going to be the case in the (very large) matrix.  If I randomized the relationship between point and row then it would not be so clean.</li>
<li>I might have got one wrong - so if I have a typo, let me know.</li>
<li>Hop-count here is shortest number of hops to connect point on row i with point on column j.  A self-hop is still a hop, so the diagonal is all ones.</li>
</ul>

<p>So in this matrix larger distance (hops) has a higher number.  If I wanted a matrix showing ""connectivity"" instead of distance, then I could do a dot-inverse, where each cell of the matrix is replaced with its multiplicative inverse.</p>

<p><strong>Questions:</strong>   </p>

<p>To help me find my own way:</p>

<ul>
<li>What are the terms for reducing the number of nodes on a graph by combining them?  Is it clustering, merging, munging - what are the words that I should use?</li>
<li>What are the proven techniques?  Is there a textbook on the topic?  Can you point to papers or websites?</li>
<li>Now I tried to look here first  - it is a great ""first check"" spot. 
I didn't find what I was looking for.  If I missed it (not unlikely)
can you point me to an answered question or two on the topic here at
CV?</li>
</ul>

<p>To get me where I am going:    </p>

<ul>
<li>Is there an 'R' package that will properly cluster the nodes on the network?</li>
<li>Could you point me to example code to do this?</li>
<li>Is there an 'R' package that will graphically present the resulting reduced network?</li>
<li>Could you point me to example code to do this?</li>
</ul>

<p>Thanks in advance.</p>
"
"0.0969762275752854","0.0880450906325624","140523","<h2>Outline of clustering technique using Random Forest</h2>

<p>A synthetic data is created by randomly sampling from the data of interest. It is used as the base line to measure the ""structureness"" or ""clustering"" in the data of interest.
The real data and synthetic data are combined and fed into the randomForest() to do classification.The distance matrix is calculated from the proximity measure of the outputted random forest and is clustering is done on the distance matrix using other clustering techniques.</p>

<h2>Issues:</h2>

<p>1)Sampling technique </p>

<p>The paper by Shi et al.  (<a href=""http://labs.genetics.ucla.edu/horvath/RFclustering/RFclustering/RandomForestHorvath.pdf"" rel=""nofollow"">http://labs.genetics.ucla.edu/horvath/RFclustering/RFclustering/RandomForestHorvath.pdf</a>)
describes two sampling techniques- (1)random sampling from the product of empirical marginal distributions of the variables of the data and (2)random sampling (uniform distribution) from the hyper rectangle containing the data.</p>

<p>2)No. of forests</p>

<p>Shi et al. reported that ""RF dissimilarity can vary considerably as a function of the particular realization of the synthetic data"". So a number of forests are grown and are combined to get the final result.</p>

<h2>Question:</h2>

<p>Which sampling technique does randomForest() function from randomForest package uses ? Also, how many forests are grown?</p>
"
"0.118771137369734","0.107832773203438","140839","<p>Example of data set</p>

<pre><code>PRODUCT_ID household_key   BASKET_ID DAY QUANTITY SALES_VALUE STORE_ID RETAIL_DISC TRANS_TIME WEEK_NO COUPON_DISC
1     1082185          2375 26984851472   1        1        1.21      364        0.00          6       1           0
2     1036325          2375 26984851472   1        1        0.99      364       -0.30          6       1           0
3     1033142          2375 26984851472   1        1        0.82      364        0.00          6       1           0
4     8160430          2375 26984851472   1        1        1.50      364       -0.39          6       1           0
5     1004906          2375 26984851472   1        1        1.39      364       -0.60          6       1           0
6     6423775          2375 26984851516   1        1        2.00      364       -0.79          6       1           0
7     9487839          2375 26984851516   1        1        2.00      364       -0.79          6       1           0
8      826249          2375 26984851516   1        2        1.98      364       -0.60          6       1           0
9     1102651          2375 26984851516   1        1        1.89      364        0.00          6       1           0
10    1043142          2375 26984851516   1        1        1.57      364       -0.68          6       1           0
11    1085983          2375 26984851516   1        1        2.99      364       -0.40          6       1           0
12     981760          1364 26984896261   1        1        0.60    31742       -0.79          6       1           0
13     842930          1364 26984896261   1        1        2.19    31742        0.00          6       1           0
14     920955          1364 26984896261   1        1        3.09    31742        0.00          6       1           0
15     937406          1364 26984896261   1        1        2.50    31742       -0.99          6       1           0
16     897044          1364 26984896261   1        1        2.99    31742       -0.40          6       1           0
17    1048462          1130 26984905972   1        1        1.19    31642       -0.80          5       1           0
18     833715          1130 26984905972   1        2        0.34    31642       -0.32          5       1           0
19     866950          1130 26984905972   1        2        0.34    31642       -0.32          5       1           0
20    1022843          1130 26984905972   1        2        0.34    31642       -0.32          5       1           0
21    1071333          1130 26984905972   1        2        0.34    31642       -0.32          5       1           0
22     923972          1173 26984945254   1        1        0.67      412        0.00          7       1           0
23    1131351          1173 26984945254   1        1        0.88      412        0.00          7       1           0
24     824399          1173 26984945254   1        2        1.98      412        0.00          7       1           0
25    1082185            98 26984951769   1        1        0.39      337        0.00          7       1           0
26     965138            98 26984951769   1        2        3.00      337       -0.08          7       1           0
27     878302            98 26984951769   1        3        0.90      337        0.00          7       1           0
28    1087347            98 26984951769   1        1        0.25      337        0.00          7       1           0
29     985911            98 26984951769   1        1        1.25      337       -0.34          7       1           0
30     877180          1172 26985025264   1        1        2.29      396        0.00          4       1           0
   COUPON_MATCH_DISC MANUFACTURER DEPARTMENT BRAND COMMODITY_DESC SUB_COMMODITY_DESC
1                  0            2         34     2            293                166
2                  0           69         34     1            300                391
3                  0            2         34     2            213               1499
4                  0           69         34     1            215               1513
5                  0           69         34     1            235               1695
6                  0          586         18     2             74                941
7                  0          586         18     2             74                941
8                  0           69         18     1             16               1005
9                  0         1266         18     2            231               1593
10                 0          321         11     2             39               2097
11                 0          586         18     2             74               2251
12                 0           69         18     1            104                697
13                 0           69         18     1             72                944
14                 0         3664         22     2             26                970
15                 0         2209         23     2             36               1197
16                 0         1075         18     2             81               2034
17                 0         1273         18     2             16               1233
18                 0         1002         18     2            275               1790
19                 0         1002         18     2            275               1790
20                 0         1002         18     2            275               1790
21                 0         1002         18     2            275               1790
22                 0          353         11     2             44                420
23                 0           69         11     1            105               1100
24                 0         1573         11     2             45               1958
25                 0            2         34     2            293                166
26                 0           69         18     1             74               1900
27                 0          544         18     2             15               1985
28                 0          544         18     2             15               1985
29                 0           69         18     1             17               2167
30                 0         2296         11     2            273                237
</code></pre>

<p>As you can see from above this data is category data. I want to cluster this data by using K MEANS to find similar behaviour within households. There are 2500 unique households and each row represents each transaction made for each unique product. there are in total 93000 unique products.</p>

<p>I understand I need to pre process this data one step more before I can use the K means clustering. </p>

<p>Something I thought of was </p>

<pre><code>library(dplyr)
randomtest &lt;- mydata %&gt;% group_by(household_key) %&gt;% 
  summarise_each(funs(sum),QUANTITY, SALES_VALUE, RETAIL_DISC, COUPON_DISC)
</code></pre>

<p>the code above gives me the sum of quantity, sales_value, retail_disc and coupon_disc for each household. I used this data frame to do my clustering by removing the household key.</p>

<p>due to lack of reputations I can not plot the results I got.</p>

<p>However looking at this result I don't think there is anything meaningful I can find which describes the behaviour for the households.</p>

<p>I am in need of some serious help where someone can suggest me some more ideas of pre processing this data so when I do the clustering it outputs me something meaningful.</p>

<p>I have tried many other things aswell but each time I got results which didn't make sense. I am sure I am close to the answer just need some help with silly mistakes.</p>

<p>Any help would be much appreciated.</p>

<p>Thank you</p>
"
"0.0395903791232448","0.0539163866017192","141280","<p>I have run k-means clustering. I have also plotted the results using the following code in R: </p>

<pre><code>library(cluster)
library(fpc)
km &lt;- kmeans(Mydata,3)
clusplot(data, km$cluster, color=TRUE, shade=T,   lines=0)
</code></pre>

<p><img src=""http://i.stack.imgur.com/2cZFZ.png"" alt=""enter image description here""></p>

<p>I do not understand what the ""component 1"" and ""component 2"" in the graph are. I also have no clue about what is meant by ""These two components explain 46.78% of the point variability"". </p>

<p>What are the components? How are they helpful in understanding the clustered data? </p>
"
"0.0685725481323742","0.062257280636469","145385","<p>I found the best k value after running a silhouette test to get k=21. On running clara() on the dataset of 13805 points, I found a pretty interesting trend: Non-zero memberships, but zero values of average, max dissimilarity on R, specifically for clusters <strong>2,4,9,10,15,16,17,18 &amp; 19</strong>.</p>

<p>The data set represents demand frequency from different locations. You will find that there is a fair amount of repetition.I need to cluster all these demand points as they represent a sample demand scenario.</p>

<p>Data: <a href=""https://www.dropbox.com/s/anv7791olo8fur0/Demand.csv?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/anv7791olo8fur0/Demand.csv?dl=0</a></p>

<p>Cluster image: <a href=""http://imgur.com/ltVS7EG"" rel=""nofollow""><img src=""http://i.imgur.com/ltVS7EG.jpg"" title=""source: imgur.com"" /></a></p>

<p><strong>R Code</strong></p>

<pre><code>    #find k value for 13805 demand points. 
    &gt; shadowsize&lt;-numeric(91)
    &gt; for(i in 10:100)
    +     shadowsize[[i]]&lt;-clara(a13kinput4clara,i)$silinfo$avg.width
    &gt; which.max(shadowsize)
    [1] 21
    &gt; set.seed(123432341)
    &gt; clara13k&lt;-clara(a13kinput4clara,21)
    &gt;clara13k$clusinfo
             size    max_diss  av_diss   isolation
       [1,]  313    0.0153895 0.0031959   0.7490261
       [2,] 1205        0         0           0 
       [3,]  439    0.0482945 0.0001100  1.2317786
       [4,] 1029        0         0           0
       [5,]  597 0.066592748 0.0009597415 1.8877388
       [6,]  384 0.130181675 0.0102797877 1.7529969
       [7,]  309 0.114705153 0.0003712141 1.0298820
       [8,] 1403 0.008978207 0.0018877912 0.2847452
       [9,]  136        0         0           0
      [10,]  764        0         0           0
      [11,]  268 0.114271631 0.0023798307 1.9138448
      [12,]  429 0.026879530 0.0001879687 1.3082594
      [13,]  691 0.032642864 0.0005439049 1.1087782
      [14,]  798 0.142868007 0.0010571184 2.5221135
      [15,]  385        0         0           0
      [16,] 1194        0         0           0
      [17,]  850        0         0           0
      [18,]  429        0         0           0
      [19,] 1108        0         0           0
      [20,]    5        0         0           0
      [21,] 1069 0.112005995 0.0006286585 2.0899605
</code></pre>

<p><strong>Questions</strong></p>

<ol>
<li><p>Are the zero values characteristic of outliers? Why is this happening? Could it be because of the significant demand point overlap (for certain locations)?</p></li>
<li><p>I used Clara() as I understood it was a variant of pam(). Is the Clara() function flawed? Are there any alternatives I can use for k-medoid clustering of such fairly large datasets,if so?</p></li>
</ol>

<p>Thank You in advance,</p>

<p>Metalxenophobe</p>
"
"0.0395903791232448","0.0539163866017192","145691","<p>I am working on a data clustering and don't know how I can achieve it with R !
I am working on a data set of 50 observations each of 8 variables. What i want is to have clusters gathering the observations with high homogeneity. 
Kmeans isn't giving me that in terms of data visualization !
How can I do that ?</p>
"
"0.0559892510955854","0.0381246425831512","147404","<p>Is there any built in method to cluster data with one categorical dimension in R? Basically, I have a data set including week of the year and if an event happened in that week. I wanted to use clustering to predict future occurrence of that event in a specific week. I tried SVM and it was pretty promising but I was looking for a more convincing approach. I though maybe clustering is a good option but based on what I saw, week of the year is the only feature for my problem which I believe wont give any helpful information for clustering. I wonder if I am using a wrong method for my problem.</p>

<p>Thanks</p>
"
"0.0559892510955854","0.0381246425831512","148417","<p>Is it OK to use <code>kmeans</code> with binary variables? I mean Euclidean distance? I guess the binary variables will be the ones that get the most power to determine the result.</p>

<p>Look at the following example:</p>

<pre><code>data= data.frame(a=c(1,0,1,1), b=c(0.1,.2,.6,.8))
plot(data)
kmeans(data,2)
## Clustering vector: [1] 1 2 1 1
</code></pre>

<p>So the result is determined by the binary variable.</p>

<p>Is there a way to treat binary variables differently? Should I use Manhattan distance for all variables?</p>
"
"NaN","NaN","148597","<p>I am working on a clustering model with the kmeans() function in the package stats and I have a question about the output. </p>

<p>My data is a sample from several tech companies and AAPL._UP is a variable equal to ""1"" if apple was up on that particular day. </p>

<p>I ran a kmeans algorithm with a k=16 and it gave me some output. I can interpret most of it but I'm just not sure what I'm looking at here. Can some one let me know what these numbers mean?</p>

<p>There is a picture of what I am looking at <img src=""http://imgur.com/OnKHCLX.jpg"" alt=""picture""></p>

<p>If it helps, here are the cluster assignments <img src=""http://imgur.com/jom3h05.jpg"" alt=""picture""></p>
"
"0.0791807582464896","0.0808745799025788","149254","<p>I have to separate 425 observations based on certain variables numbering 32.</p>

<p>1)I used PCA to reduce the dimensionality of Data, which gave me 32 components out of which 5 components accounted for 75% of the variance.</p>

<p><img src=""http://i.stack.imgur.com/F5pPi.png"" alt=""Spider plot of clusters![][1]"">2)I used Elbow plot to determine no of clusters that is 6. I further used these 5 components as variables for kmeans clustering. when plotted using clusplot() Clusters are not well separated.</p>

<p><img src=""http://i.stack.imgur.com/R11XN.png"" alt=""Clusters""> </p>

<p>Does this mean that clustering was not successful.(between_SS / total_SS =  67.9 %). Is there any other way to determine effective ness of clustering? Can somebody suggest a better way to obtain better clusters may be PAM, Hierarchical clustering ...?</p>
"
"0.0969762275752854","0.0660338179744218","149707","<p>I have data that refer to the number of occurrences of specific variable in samples:</p>

<pre><code>       V1  V2  V3 ...
sample1 0   2   1
sample2 7   1   0
sample3 1   4   1  
....
</code></pre>

<p>The data refers to the occurrence of genes(V1...) in different genomes (sample1..).</p>

<p>I want to perform a cluster analysis combined with an heat map.
I used the function <code>heatmap.2</code> in the <code>gplot</code> package in R.
I used Euclidian distances for calculating the distance among the samples. 
The clustering algorithm is the default one for the function <code>hclust</code> in R (<code>hclust(d, method = ""complete"", members = NULL)</code>).
However, I am not completely sure it is the right method. 
Any suggestion on how to choose the right method to calculate the distances among my samples?</p>

<p><strong>EDIT</strong>
The aim is to describe the distribution of the variables (genes) among the samples (genome), and cluster the samples(genomes) according with the values that each variables assume (meaning, how many specific genes are present)</p>
"
"0.0395903791232448","0.0539163866017192","149852","<p>I want to calculate the cophenetic correlation coefficient.
reading previous posts  </p>

<p><a href=""http://stats.stackexchange.com/questions/92546/comparison-of-cophenetic-correlation-coefficients-on-different-data-sets"">Comparison of cophenetic correlation coefficients on different data sets</a></p>

<p><a href=""http://stats.stackexchange.com/questions/33066/on-cophenetic-correlation-for-dendrogram-clustering"">On cophenetic correlation for dendrogram clustering</a></p>

<p><a href=""http://stackoverflow.com/questions/5639794/in-r-how-can-i-plot-a-similarity-matrix-like-a-block-graph-after-clustering-d"">http://stackoverflow.com/questions/5639794/in-r-how-can-i-plot-a-similarity-matrix-like-a-block-graph-after-clustering-d</a></p>

<p>I used the <code>cophenetic</code> function in the package <code>stats</code>. 
As far as I understand the results are cophenetic distances for the hierarchical clusteringis, in a new object of class <code>dis</code>. </p>

<pre><code>coph&lt;-cophenetic(hclsut_result)
</code></pre>

<p>To have an overview I clustered the cophenetic matrix, and I obtained the same clustering  as the one performed on my dataset.</p>

<p>However, I wanted to have an unique value that indicate the fidelity with wich my clsutering represent my distance matrix. Therefore, I correlated the <code>dis_matrix_for_my_dataset</code> with the <code>coph</code>.</p>

<pre><code>cor(euclidian_dist, coph)
</code></pre>

<p>Am I understanding right that the value I obtain indicates the <strong>cophenetic correlation coefficient</strong>?</p>
"
"0.137145096264748","0.124514561272938","151961","<p>*Please note this question is about the Platt probabilistic output and SVM class assignment, not about the code or the package itself. It just happens to be the code where I stumbled on the issue.</p>

<p>In <a href=""http://stats.stackexchange.com/questions/147260/including-class-probabilities-might-skew-a-model-in-caret"">another question</a> I asked about bad models coming from <code>caret</code> and associated <code>kernlab</code> when <code>prob.model=TRUE</code>. I found the answer myself, in both <a href=""http://stackoverflow.com/questions/29766951/different-results-with-caret-when-classprobs-true"">stackoverflow</a> and <a href=""http://r.789695.n4.nabble.com/Inconsistent-results-between-caret-kernlab-versions-td4680500.html"" rel=""nofollow"">from Max Kuhn himself</a>:</p>

<blockquote>
<pre><code>&gt; predict(newSVM, df[43,-1]) [1] O32078 10 Levels: O27479 O31403 O32057 O32059 O32060 O32078 ... O32676
&gt; predict(newSVM, df[43,-1], type = ""probabilities"")
     O27479     O31403     O32057    O32059    O32060     O32078
[1,] 0.08791826 0.05911645 0.2424997 0.1036943 0.06968587 0.1648394
     O32089     O32663     O32668     O32676
[1,] 0.04890477 0.05210836 0.09838892 0.07284396
</code></pre>
  
  <p>Note that, based on the probability model, the class with the largest
  probability is O32057 (p = 0.24) while the basic SVM model predicts
  O32078 (p = 0.16).</p>
  
  <p><strong>Somebody (maybe me) saw this discrepancy and that led to me to follow
  this rule:</strong></p>

<pre><code>if(prob.model = TRUE) use the class with the maximum probability   
  else use the class prediction from ksvm().
</code></pre>
  
  <p>Therefore:</p>

<pre><code>predict(svm.m1, df[43,-1])
 [1] O32057
 10 Levels: O27479 O31403 O32057 O32059 O32060 O32078 ... O32676
</code></pre>
</blockquote>

<p>Isn't that innacurate? <code>kernlab</code> searches for the optimal probability cutoff that minimizes error, that's why the assigned class and the maximum probability don't match: they don't have to.</p>

<p>Check this reproducible example. I excluded two cherrypicked <code>virginica</code> samples.</p>

<pre><code>require(kernlab);require(caret);
#kernel=polynomial; degree=3; scale=0.1; C=0.31
set.seed(101);SVM&lt;-ksvm(Species~., data=iris[-c(135,150),], kernel='polydot',C=.31, kpar=list( scale=.1, degree=3), prob.model=T)
</code></pre>

<p>Here's the resulting model </p>

<pre><code>&gt; SVM
Support Vector Machine object of class ""ksvm"" 

SV type: C-svc  (classification) 
 parameter : cost C = 0.31 

Polynomial kernel function. 
 Hyperparameters : degree =  3  scale =  0.1  offset =  1 

Number of Support Vectors : 58 

Objective Function Value : -1.4591 -0.7955 -10.2392 
Training error : 0.033784 
Probability model included. 
</code></pre>

<p>Now let's check the predicted class probabilities in those two samples</p>

<pre><code>&gt; predict(SVM, iris[c(135,150),-5], type=""probabilities"")
          setosa versicolor virginica
[1,] 0.008286638  0.4414114  0.550302
[2,] 0.013824451  0.3035556  0.682620
</code></pre>

<p>And the class predictions</p>

<pre><code>&gt; predict(SVM, iris[c(135,150),-5])
[1] versicolor virginica 
Levels: setosa versicolor virginica
</code></pre>

<p>Sample 150 was assigned to <code>virginica</code>, with a class probability of around 0.68. Sample 135 was assigned to <code>versicolor</code> with a probability of around 0.44, yet <code>virginica</code> probability nicely sits around 0.55.
Looking at several CV folds, we perceive that kernlab only assigns <code>virginica</code> when its probability is over a given value (way higher than 0.5). That's the cutoff I mentioned, and it happens thanks to the well known bad clustering in <code>iris</code> between <code>virginica</code> and <code>versicolor</code>.</p>

<p>So, am I right on these suppositions and therefore is <code>caret</code> class assignment model (maximum probability) wrong?</p>

<p>EDIT:
I've been experimenting with pairwise probability coupling of Platt scaling (logistic regression fit), isotononic regression and a model I'm working on. A weakness (?) I perceived in Platt's model is the probability isn't bound to be 0.5 when the binary SVM decision output is 0, which is the expected result as the instance would lie exactly on the separating hyperplane.</p>
"
"0.0395903791232448","0.0539163866017192","152129","<p>I am working on a <code>data.frame</code> with <strong>both categorical and metric</strong> variables</p>

<pre><code># example data
a &lt;- as.factor(c(""A"",""A"",""B"",""C"",""D"",""A"",""C"",""A"",""C"",""C""))
b &lt;- rep(1:5,2)
c &lt;- as.factor(c(""elephant"",""elephant"",""cat"",""dog"",""cat"",""elephant"",
                 ""cat"",""elephant"",""dog"",""dog""))
df &lt;- data.frame(a,b,c)
</code></pre>

<p>I run a cluster analysis on this example data</p>

<pre><code># Dissimilarity Matrix Calculation

library(cluster)

x &lt;- daisy(df, metric = c(""gower""),
    stand = FALSE, type = list())

# Hierarchical Clustering

z &lt;- agnes(x, diss = inherits(x, ""dist""), metric = ""euclidean"",
      stand = FALSE, method = ""single"", par.method,
      trace.lev = 0, keep.diss = TRUE)
</code></pre>

<p>and receive this dendrogram</p>

<pre><code>plot(z,  main=""plotit"", which.plot = 2)
</code></pre>

<p><img src=""http://i.stack.imgur.com/ZTJNP.png"" alt=""dendrogram""></p>

<ul>
<li>How do I know where to cut the tree?</li>
</ul>

<p>I could do something like </p>

<pre><code>cutree(z, k = 2, h=0.3)
</code></pre>

<p>but the values chosen for <code>k</code> and <code>h</code>would be entirely arbitrary. I work on a large data set where I can't rely on information I see in the plot in this example?</p>

<ul>
<li>Is there a heuristic to determine the number of clusters?</li>
<li>Is there a heuristic to determine the cutting height of the tree?</li>
</ul>
"
"0.181425909124269","0.20001384226738","152359","<p>I am trying to construct (undirected) <strong>social network based on co-occurence of individuals</strong>. Clustering algorithm will be later applied on this network to find some distinct subgroups. Issue is that studied animal species has <strong>very short longevity</strong> (or rather very high mortality due to predators). <strong>It causes that not all of the relationships in my network may have existed at the same time.</strong> If you look on the diagram below, the <em>""red""</em> individuals are almost extincted after 3-4 years*, but they have the <em>""longest""</em> time to <em>""meet""</em> other individuals, whereas <em>""blue""</em> individuals have only two years to <em>""meet""</em> others.</p>

<p><img src=""http://i.stack.imgur.com/J9nhk.png"" alt=""enter image description here""></p>

<p><em>Theoretically I can assume that each individual has expected longevity less than 10 years. Therefore not catching of ""red"" individual 5 or 6 years after tagging does not necessarily means that is dead.</em></p>

<p><strong>How to include this time effect into social network?</strong></p>

<p><strong>Specific questions I want to answer:</strong>
<strong>First question: Are observed social connections distinct from a connections explained solely by shared space use? i.e., How to test if associations are random or preferred?</strong></p>

<p>If answer to first question will be that associations between individuals are <strong>NOT</strong> random, then I have a second qeustion...</p>

<p><strong>Does social structure correlates with genetic relatedness? i.e., are closely related individuals more often together?</strong> (DNA profiles of all inividuals are bolow)</p>

<p>Here I created some data structurally similar to my database:</p>

<pre><code>data &lt;- data.frame(obs_date = c(""C1"",""C2"",""C3"",""C4"",""C5"",""C6"",""C1"",""C2"",
                                ""C3"",""C4"",""C1"",""C2"",""C3"",""C1"",""C2"",""C3"",
                                ""C4"",""C5"",""C6"",""C7"",""C1"",""C3"",""C4"",""C5"",
                                ""C6"",""C7"",""C8"",""C3"",""C4"",""C5"",""C6"",""C7"",
                                ""C3"",""C4"",""C5"",""C6"",""C3"",""C4"",""C5"",""C3"",
                                ""C4"",""C5"",""C6"",""C5"",""C6"",""C7"",""C8"",""C5"",
                                ""C5"",""C6"",""C7"",""C8"",""C5"",""C6"",""C7"",""C7"",
                                ""C7"",""C8"",""C7"",""C8"",""C7"",""C8"",""C7"",""C8""),
                   ind_id = rep(LETTERS[1:20], times = c(6,4,3,7,1,6,5,4,
                                               3,2,2,4,1,4,3,1,2,2,2,2)),
                   obs = rep(c(""seen"",""not_seen"",""seen"",""not_seen"",""seen"",
                               ""not_seen"",""seen"",""not_seen"",""seen""),
                               times = c(3,1,4,1,9,1,9,3,33)))
</code></pre>

<p>Here I added genetic structure. Data are completely fabricated, but they should reflect close genetic relatedness between same collor individuals. Aditionally <em>""violet""</em> individuals are offsprings of <em>""blue""</em>, <em>""blue""</em> are offsprings of <em>""green""</em>, <em>""green""</em> are offsprings of <em>""red""</em>. </p>

<pre><code>gen.raw &lt;- matrix(c(""a"",""g"",""g"",""g"",""c"",""g"",""a"",""a"",""g"",""g"",""g"",""g"",""t"",""c"",""t"",""c"",""t"",""t"",""a"",""a"",""t"",""t"",""a"",""a"",
                    ""a"",""g"",""g"",""g"",""c"",""g"",""a"",""a"",""g"",""g"",""g"",""g"",""c"",""c"",""t"",""c"",""t"",""t"",""a"",""a"",""t"",""c"",""a"",""a"",
                    ""a"",""g"",""g"",""g"",""c"",""g"",""g"",""a"",""g"",""g"",""g"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""a"",""a"",""t"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""t"",""t"",""g"",""g"",""a"",""g"",""g"",""g"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""a"",""a"",""a"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""t"",""t"",""g"",""g"",""a"",""g"",""g"",""g"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""a"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""t"",""t"",""g"",""g"",""a"",""g"",""g"",""g"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""a"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""t"",""t"",""g"",""g"",""g"",""g"",""g"",""c"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""a"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""t"",""t"",""g"",""a"",""c"",""g"",""t"",""c"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""a"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""t"",""t"",""g"",""a"",""c"",""g"",""t"",""c"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""a"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""t"",""t"",""g"",""a"",""c"",""g"",""t"",""c"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""a"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""t"",""t"",""g"",""a"",""c"",""g"",""t"",""c"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""a"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""t"",""t"",""g"",""a"",""c"",""g"",""t"",""c"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""a"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""t"",""t"",""g"",""a"",""c"",""g"",""t"",""c"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""a"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""t"",""t"",""g"",""a"",""c"",""g"",""t"",""c"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""t"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""t"",""t"",""g"",""a"",""c"",""g"",""t"",""c"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""t"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""t"",""t"",""g"",""a"",""c"",""g"",""t"",""c"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""t"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""c"",""t"",""g"",""a"",""c"",""g"",""g"",""c"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""t"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""c"",""t"",""g"",""a"",""c"",""g"",""g"",""c"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""t"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""c"",""t"",""g"",""a"",""c"",""g"",""g"",""c"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""t"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""c"",""t"",""g"",""a"",""c"",""g"",""c"",""c"",""g"",""t"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""t"",""c"",""a"",""a""),
                    byrow = TRUE, ncol = 24)
rownames(gen.raw) &lt;- LETTERS[1:20]
</code></pre>

<p>Ok, source data are given above. Now I will create <strong>two distance matrices</strong>. First is <strong>association matrix</strong> derived from co-occurence data represented by <strong>OR-SP index</strong>. Observed  Roost-Sharing Proportion is calculated for each pair of individuals by <strong>dividing the number of days two individuals were found together by the number of all possible days they could be together</strong> (overlap bewteen first and last recordngs of both individuals).</p>

<pre><code># matrix of days roosting together
EG &lt;- expand.grid(unique(data$ind_id), unique(data$ind_id))

data_seen &lt;- subset(data, obs == ""seen"")

my.length.dt &lt;- numeric(nrow(EG))
for (i in 1:nrow(EG)) {
my.length.dt[i] &lt;- length(intersect(as.vector(data_seen$obs_date[data_seen$ind_id == EG[i, 1]]),
                                    as.vector(data_seen$obs_date[data_seen$ind_id == EG[i, 2]])))
days.together &lt;- matrix(my.length.dt, byrow = TRUE, ncol = length(unique(data$ind_id)))
    colnames(days.together) &lt;- rownames(days.together) &lt;- unique(data$ind_id)
}
days.together

# matrix of all possible potentional roosting days
EG &lt;- expand.grid(unique(data$ind_id), unique(data$ind_id))
my.length.rdp &lt;- numeric(nrow(EG))
for (i in 1:nrow(EG)) {
my.length.rdp[i] &lt;- length(intersect(as.vector(data$obs_date[data$ind_id == EG[i, 1]]),
                                     as.vector(data$obs_date[data$ind_id == EG[i, 2]])))
roosting_days_possible &lt;- matrix(my.length.rdp, byrow = TRUE, ncol = length(unique(data$ind_id)))
    colnames(roosting_days_possible) &lt;- rownames(roosting_days_possible) &lt;- unique(data$ind_id)
}
roosting_days_possible

# OBSERVED ROOST-SHARING PROPORTION
OSP &lt;- days.together/roosting_days_possible
OSP[ is.nan(OSP) ] &lt;- 0
diag(OSP) &lt;- 0

# So here is association matrix derived from co-occurence data
round(OSP,2)
# social distance matrix
soc_dist &lt;- as.dist(OSP)
</code></pre>

<p>Next step is to take DNA sequences and make genetic relatedness matrix</p>

<pre><code># creating matrix of relatedness
library(ape)
gen.str &lt;- as.DNAbin(gen.raw)
my.gen.dist &lt;- dist.dna(gen.str)
fit &lt;- hclust(my.gen.dist, method=""ward"")
plot(fit) # display dendogram 
</code></pre>

<p>Finally, here I <strong>compare social distance with genetic distance by Mantel test</strong>.</p>

<pre><code>library(ade4)
mantel.rtest(soc_dist, my.gen.dist, nrepet = 9999)
</code></pre>

<p><strong>Does its result (p > 0.05) mean that there is no correlation between social and genetic structure?</strong></p>

<p><strong>Is this appropriate solution to answer my question? Any ideas?</strong></p>

<p>I also found that for social structure might be better this type of graph instead of dendrogram. Good for finding distinct social group.</p>

<pre><code># Show social structure
library(igraph)
g &lt;- graph.adjacency(OSP, weighted=TRUE, mode =""undirected"")
g &lt;- simplify(g)
# set labels and degrees of vertices
V(g)$label &lt;- V(g)$name
V(g)$degree &lt;- degree(g)
wc &lt;- walktrap.community(g)
plot(wc, g)
</code></pre>
"
"0.0395903791232448","0.0539163866017192","153895","<p>I've with me 50 MB data from a machine consisting of event logs such as device status, warning and error. I wish to perform text mining on the same to find correlation between errors i.e. one error could trigger another in future and take a prescriptive action on it. I've used hierarchical clustering in R to generate a dendrogram but the result couldn't yeild expected insight. Hence, I've planned to perform Apriori Algorithm on the same. My queries are:</p>

<p>Is Hierarchical Clustering suggested to find such correlations?
Is Apriori an apt algorithm in such situation and why? Or, is there any other approach to solve this problem?</p>

<p>Any help would be appreciated. 
Thanks</p>
"
"0.0559892510955854","0.0762492851663023","153946","<p>I've with me 50 MB data from a machine consisting of event logs such as device status, warning and error. I wish to perform text mining on the same to find correlation between errors i.e. one error could trigger another in future and take a prescriptive action on it.
I've used hierarchical clustering in R to generate a dendrogram but the result couldn't yeild expected insight. Hence, I've planned to perform Apriori Algorithm on the same. 
My queries are:</p>

<ol>
<li>Is Hierarchical Clustering suggested to find such correlations?</li>
<li>Is Apriori an apt algorithm in such situation and why? Or, is there any other approach to solve this problem?</li>
</ol>

<p>The logs that I've looks like this :</p>

<p><img src=""http://i.stack.imgur.com/nKdSM.png"" alt=""enter image description here""></p>
"
"0.104746297470869","0.0611354359454417","154871","<p>I have always heard that I can reduce dimensionality of a matrix using SVD. So, I'd like to ask something hypothetically. Suppose that the following matrix A has a high dimensionality and I want to reduce it for applying, let's say, a clustering analysis.</p>

<pre><code>2 0 8 6 0
1 6 0 1 7
5 0 7 4 0
7 0 8 5 0
0 10 0 0 7
</code></pre>

<p>Using software R, I could do something like this:</p>

<pre><code>s&lt;-svd(A,2)
u&lt;-as.matrix(r$u)                         #$
v&lt;-as.matrix(r$v[,1:2])                   #$
d&lt;-as.matrix(diag(r$d)[1:2,1:2])
newdata &lt;- u%*%d%*%t(v)
</code></pre>

<p>The variable <code>s</code> would contain the factorized matrices $A = UDV$. And then I could remultiply them to get back the values. For example, R generated the following ones for me: </p>

<p><img src=""http://i.stack.imgur.com/YzUx8.png"" alt=""From top down matrices A, D, V""></p>

<p>However, how can I use those matrices for my clustering problem? I know that I need to multiply them for getting my data back, but, even removing rows in D and V, I still get the same number of dimensions. As I want to reduce dimensionality for another task(like the clustering), I'd need less dimensions(columns). That doesn't seem the case. </p>

<p>Am I doing something wrong? Could anybody help to understand this better? Or provide a tutorial to see the process happening? Is PCA a better solution to my problem by using the most representative principal components as the input for my clustering algorithm?</p>
"
"0.111978502191171","0.133436249041029","155989","<p>I'm a Software Engineer trying to learn how to do a <a href=""http://en.wikipedia.org/wiki/Principal_component_analysis"" rel=""nofollow"">Principal Components Analysis</a> in Python or R.</p>

<p>I've found a few <a href=""https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/"" rel=""nofollow"">links</a> which do a good job of explaining the concept from a high-level.  However, I haven't seen any examples which walk you through all of the steps from start to finish.</p>

<p>For example, letâ€™s say you have a 50-dimensioned dataset, which has 50 columns of varying data types (boolean, float, integer, varchar etc.)â€¦ Do those values need to be scaled or normalized to something like 0.0..1.0? Or can the PCA algorithm handle those disparate data types?</p>

<p>Ideally, I want to see something which does a walkthrough of each step and explains it on the way. <strong>Especially starting with disparate data which needs to be scaled or normalized.</strong> All examples I've seen online, including ones which use well-known example data sets (such as the <a href=""http://archive.ics.uci.edu/ml/datasets/Iris"" rel=""nofollow"">Iris dataset</a>), start with pristine data where all of the columns are the same data type. I'm starting with a large dataset with many columns of varying data types.  What do I do?</p>

<p>Incidentally, after applying PCA to my dataset, I plan on running it through clustering (k-means probably).</p>

<p><strong>Update 9/10/2015</strong></p>

<p>Since this question has been marked as off-topic, I'm not able to submit or select an answer.  In any case, I found two links from Sebastian Raschka to be very helpful:</p>

<ul>
<li><a href=""http://sebastianraschka.com/Articles/2014_pca_step_by_step.html"" rel=""nofollow"">Implementing a Principal Component Analysis (PCA) in Python step by step</a></li>
<li><a href=""http://nbviewer.ipython.org/github/rasbt/pattern_classification/blob/master/preprocessing/feature_encoding.ipynb"" rel=""nofollow"">Tips and Tricks for Encoding Categorical Features in Classification Tasks</a></li>
</ul>
"
"0.0559892510955854","0.0381246425831512","157666","<p>I wish to try clustering a matrix of numerical data using swarm intelligence.
(Matrix is 28000 X 53 and sparse). I'm working in R and found the REPPlab package and used the EPPlab function. My question is how do I go from here to actually cluster my data? I know PSO can be used for clustering as well as optimisation but I'm stuck! </p>
"
"0.0791807582464896","0.0808745799025788","157686","<p>While estimating from the <strong>survey data</strong> involving <strong>stratification</strong> &amp; <strong>clustering</strong> survey design and using survey package of <strong>r</strong>, is it possible to estimate at the <strong>cluster level</strong>? For eg; for following survey design:</p>

<pre><code>data(api)
## syntax for stratified cluster sample
dclus=svydesign(id=~dnum, strata=~stype, weights=~pw, data=apistrat, nest=TRUE)
</code></pre>

<p>This is an example which is reproduced from the survey package. Here, <em>dnum</em> is district, <em>stype</em> is Elementary/Middle/High School and <em>pw</em> is weight (inverse of probalitlity of the students selected from each of the strata). In this case, can I estimate population parameter at district level? For example, to estimate total enrollment for each of the district:</p>

<pre><code>svyby(~enroll,~dnum, design=dclus, svytotal)
</code></pre>

<p>I got the following output: </p>

<pre><code>dnum   enroll       se
 19   21751.32   21751.32
 20   10494.50   10494.50
 25    9416.73    9416.73
 27   26923.30   26923.30
 40   14843.30   14843.30
 41   25774.43   25774.43
</code></pre>

<p>I believe this estimation is not correct, as no where I have given the break up of total students at district level (cluster level). 
Any help would be greatly appreciated.</p>
"
"0.0559892510955854","0.0762492851663023","157849","<p>I have a df containing samples of a time varying quantity (namely, exposure to electromagnetic field generated by a GSM station), and I use the mixtools package to find a fitting mixture model (tipically, a 2- or 3-component guassian mixture) because I assume that (or rather, want to test if) samples come from different pdf's based on the hour of the day (e.g., with larger mean during peak hours and lower ones during off-peak hours).</p>

<p>It seems to me that normalmixEM function included in the package does not return any threshold values that can be used to assign samples to the various distributions: how can I achieve that?</p>

<p>Should that function be unavailable in the mixtools package, could you please point me to a more general tool that would help me clustering values, also considering that it should make its own initial guess of the threshold values?</p>

<p>Many thanks!
Nicola</p>
"
"0.0791807582464896","0.0539163866017192","158496","<p>I have a set of 400 nucleotide sequences and want to cluster them on basis of similarity. For clustering, I am expecting a similarity &lt;=45% among members of a cluster. Also, there will be a few sequences that do not show similarity to any other member. Is there any clustering approach that allow us to set a cut-off for relation (similarity) between members? and can keep the members with very low similarity to a ""unclustered"" set?</p>

<p>I have generated the percentage identity matrix (400 x 400) using clustal-omega, and using this matrix for clustering by ""affinity-propagation"" approach but not getting good results.</p>

<p>p.s. I have had used ""cd-hit"" and ""uclust"" already but they are not recommended for cases when expected sequence similarity is below 70%.</p>

<p>Link to my question on BioStar - <a href=""https://www.biostars.org/p/147913/"" rel=""nofollow"">https://www.biostars.org/p/147913/</a></p>

<p>Bade</p>
"
"0.111978502191171","0.114373927749453","158716","<p>I am using FactomineR to explore a set of continuous variables in a large set of sites (ecological data). I did a PCA and found the relevant principal components and their scores and such. Afterwards I do a hierarchical clustering on the resulting PCA using HCPC with K-means clustering of the sites. The result comes up with 3 clusters, which confirms what I expected when seeing the PCA plot.
The data I am using (just for learning this stuff) can be found on <a href=""http://datadryad.org/resource/doi:10.5061/dryad.rg832/1"" rel=""nofollow"">http://datadryad.org/resource/doi:10.5061/dryad.rg832/1</a></p>

<p>The code I am using is the following:</p>

<pre><code>pca &lt;- PCA(pca_data_jouffray, graph=FALSE, scale.unit = TRUE)
hcpc &lt;- HCPC(pca, min = 3, max=10, iter.max=10, graph=FALSE)
</code></pre>

<p>My question is the following: I can see the significance of each variable for each cluster with (in this case for cluster 1)</p>

<pre><code>hcpc$desc.var$quanti$`1`

              v.test Mean in category Overall mean sd in category Overall sd          p.value
Macroalgae 11.646270        38.144928    15.142384      23.438186 18.6474238 2.397240e-31
Sand        9.303437        21.561594    11.087748      13.893514 10.6290048 1.359779e-20
CCA        -4.386715         3.094203     6.719785       3.940017  7.8031164 1.150752e-05
Hard.coral -5.755929         6.797101    18.303808       7.952790 18.8740653 8.616669e-09
Complexity -5.934810         1.702899     2.283113       0.737548  0.9230203 2.941863e-09
Turf.algae -7.446795        30.673913    48.649007      14.649733 22.7893314 9.563492e-14
</code></pre>

<p>But what I would like to know is if I can find out if the complete cluster is significantly different from the overall mean. So a p-value for each cluster as a whole, not split up by variable. I would think that there could be a test to see if the mean euclidean distances between the individuals of a cluster is significantly different of the overall mean euclidean distance between all individuals?</p>

<p>Is this possible? </p>
"
"NaN","NaN","159427","<p>I have a time series data in R, and I am using functional clustering. I would like to interpret a figure that is output below the code. Furthermore, I would like to control line colors and thickness in the figure.</p>

<pre><code>library(fda.usc)
data(phoneme)
mlearn&lt;-phoneme$learn[c(1:50,101:150,201:250),]
out.fd1=kmeans.fd(mlearn,ncl=3,draw=TRUE)
</code></pre>
"
"NaN","NaN","159462","<p>I have a table of GPS data and I need to associate each record to the closest from a list of locations.  It's like doing clustering but I already have the centers of the clusters.</p>

<p>In this case the centers are latitude, longitude pairs.</p>
"
"0.0685725481323742","0.062257280636469","160132","<p>I want to do a k-means clustering on a dataset containing 22 numerical variables between 0 and 100 and 75 observations using R. I read this post 
<a href=""http://stats.stackexchange.com/questions/133656/how-to-understand-the-drawbacks-of-k-means"">How to understand the drawbacks of K-means</a> on k-means clustering assumptions.  My question are: </p>

<p>1- How can I check that my clusters are Spherical or not? having 22 variables, I cannot visualize them.</p>

<p>2- For checking if ""all variables have the same variance"" do I need to test them statistically?</p>

<p>3- For checking ""the prior probability for all k clusters are the same, i.e. each cluster has roughly equal number of observations"" what should I do if I do not expect my data to have clusters of almost the same size? In other words, it is natural for my data to have clusters of different sizes.</p>
"
"0.0969762275752854","0.0880450906325624","161073","<p>I am working with GPS data for density based clustering in R.</p>

<p>Let's suppose, I have produced a path out of the following dataset. Now, where the density of plot is high enough (as shown in graph) over any particular area, it should produce a cluster.</p>

<p>Suppose this is my ggplot produced from a dataset as:</p>

<pre><code>   Lat          Long
92.14894444 50.01011111
92.14894444 50.01011111
92.14825    50.01491667
92.15875    50.01502778
92.15708333 49.98458333
92.16005556 49.98566667
92.16266667 49.99105556
92.16119444 50.00330556
92.16475    50.01558333
....
</code></pre>

<p>**I don't have to predefine numbers of clusters. So i think it's good to use density based clustering algorithm in this. If you have any other solution to produce similar results, you are welcome.</p>

<p><img src=""http://i.stack.imgur.com/YBtBU.png"" alt=""enter image description here""></p>

<p>Now, I want to produce these black circles over my ggplot.</p>

<p>I tried using density based clustering but it's not producing very good results.</p>

<p>Now, when I look at the clusters using density based, they are not meaningful. Some clusters have points which are too far. I want dense clusters but not that big in size (lets suppose within a range of 1 km radius). The output I want to produce is shown in ggplot.</p>

<p>This is what I have produced till now..</p>

<pre><code>library(ggplot2)
sp &lt;- ggplot(df, aes(x=Lat, y=Long )) +geom_point()
sp + geom_density2d()
</code></pre>

<p><img src=""http://i.stack.imgur.com/2YJ4y.png"" alt=""enter image description here""></p>
"
"0.0791807582464896","0.0539163866017192","161449","<p>I would like to apply a clustering algorithm to a set where each observation does not have the same number of ""qualities"". I have searched extensively online for this problem and were unable to find a solution or an idea on how to tackle this. This is my first stats.stackexchange post and I'm still trying to figure out how to format tables and whatnot, so my apologies for the strange look below.</p>

<p>The following is a dumbed down view of what my data looks like now:</p>

<p><code>Person    | Quality
        1  | x 
        1  | y 
        2  | r 
        2  | d 
        2  | x 
        3  | y</code></p>

<p>In other words, person 1 may have 2 conditions, or ""qualities"", while person 2 has 3 different conditions. I would like to group these three people into clusters, yet none of them have the same number of qualities used to describe that person. This presents a problem when attempting to apply a cluster algorithm in R. I know this is a very general question, but does anyone have experience with this kind of structure? Does there exist an algorithm that can account for this?</p>
"
"0.0559892510955854","0","161675","<p>I am trying to perform a clustering analysis for a csv file with 50k+ rows, 10 columns. I tried k-mean, hierarchical and model based clustering methods. Only k-mean works because of the large data set. However, k-mean does not show obvious differentiations between clusters. So I am wondering is there any other way to better perform clustering analysis? Thanks in advanced!</p>

<p>The data looks like this</p>

<pre><code>Revenue  Employee  Longitude Latitude  LocalEmployee BooleanQuestions ...
1000     100       xxxx      xxxx      10
...                                                                   ...
</code></pre>

<p>Here is part of my code:</p>

<pre><code>mydata &lt;- scale(mydata)
wss &lt;- (nrow(mydata)-1)*sum(apply(mydata,2,var))
for(i in 2:15)wss[i]&lt;- sum(fit=kmeans(mydata,centers=i,15)$withinss)
plot(1:15,wss,type=""b"",main=""15 clusters"",xlab=""no. of cluster"",ylab=""with clsuter sum of squares"")

fit &lt;- kmeans(mydata,7)
clusplot(mydata, fit$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)
</code></pre>

<p><img src=""http://i.stack.imgur.com/YWUTR.png"" alt=""enter image description here""></p>
"
"0.0559892510955854","0.0762492851663023","161974","<p>I am trying to understand the difference between the varclus function and the hclustvar function for clustering in R. I understand that in the varclus function you can specify a similarity measure, i.e. spearman, pearson. Can I do something similar using the hclustvar function? What is the similarity measure set as default in the hclustvar function in R? </p>
"
"0.0559892510955854","0","162018","<p>I am trying to perform a clustering analysis for a csv file with 50k+ rows, 10 columns. I tried k-mean, hierarchical and model based clustering methods. Only k-mean works because of the large data set. However, k-mean does not show obvious differentiations between clusters. So I am wondering is there any other way to better perform clustering analysis?</p>

<p>The data looks like this</p>

<pre><code>Revenue  Employee  Longitude Latitude  LocalEmployee BooleanQuestions ...
1000     100       xxxx      xxxx      10
...                                                                   ...
</code></pre>

<p>Here is part of my code:</p>

<pre><code>mydata &lt;- scale(mydata)
wss &lt;- (nrow(mydata)-1)*sum(apply(mydata,2,var))
for(i in 2:15)wss[i]&lt;- sum(fit=kmeans(mydata,centers=i,15)$withinss)
plot(1:15,wss,type=""b"",main=""15 clusters"",xlab=""no. of cluster"",ylab=""with clsuter sum of squares"")

fit &lt;- kmeans(mydata,7)
clusplot(mydata, fit$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)
</code></pre>
"
"0.0395903791232448","0","162897","<p>I am new to clustering algorithms. I want to cluster road junctions based on traffic, that is, intersections which have traffic between each other should be in one cluster. I also have a similarity matrix, in which am using traffic between junctions as my similarity measure, I came across single linkage algorithm, should I go with it?</p>
"
"0.0969762275752854","0.0880450906325624","163162","<p>I am trying to do clustering on a distance matrix which contains numeric data. But I am not sure how to decide upon the number of clusters or value k for clara function in R. But after running it with some random number of clusters, I ran silhouette function on it and summary gives me like this:</p>

<p>Cluster sizes and average silhouette widths:  </p>

<pre><code>           7            3            4            5            7            4 
 0.222273330 -0.001592881  0.117937463  0.121326365  0.137911639  0.161932689 
Individual silhouette widths:
    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
-0.10410  0.08961  0.12500  0.14140  0.19840  0.30580 
</code></pre>

<p>This is the result for value of k=6. If I change it to say 5 or 4, I obtain silhouette for each cluster and also mean value. How do I decide upon the number of clusters? Do I need to plot like mean silhouette vs k? How do we do something like this in a large dataset with around million observations?</p>
"
"0.0685725481323742","0.0933859209547035","163477","<p>I am looking for a little guidance as to the correct approach to this problem. We have a list of IDs and roughly 8 different numerical variables such as quantity and revenue. Each ID is unique to the dataset.</p>

<p>We are looking to group these customers together based on the 8 different variables. </p>

<p>At first we looked into k-means clustering, however that only accounted for one variable and the groups were unhelpful (ie there were 2 groups with just one customer in them and then rest were all grouped together, when we tried more clusters it became even more convoluted.)</p>

<p>We are attempting to do this in R and were curious if anyone had any recommendations as to which approach we should take? </p>

<p>Also, this is not homework for a stats class, this is a real business application. </p>
"
"0.0559892510955854","0.0762492851663023","164333","<p>I have been looking at some tutorials and articles and couldn't get a scenario where two variables are in different scales and used in modeling.</p>

<p>So, firstly lets assume I have one metric of numeric type, other in percentages, and other in decimals. </p>

<ol>
<li>If I want to use those variables in a regression model for
prediction then do I need to do some standardization before fitting a<br>
model to the variables? If so how do we it in R or Python?</li>
<li>Moreover, if I want to use these features in k-means
clustering, do I need to follow the same steps as mentioned above?</li>
</ol>
"
"NaN","NaN","164720","<p>I have a dataset with 200K samples (cases) and 30 variables. Every distance-based method for clustering or dimension reduction technique that I use, such as <em>DBSCAN</em>, <em>Hierarchical Clustering</em>, <em>LLE</em>, <em>Isomap</em> and ... fail to run on my machine (normally I get <code>R Session Terminated</code> error) due to the large distance file being generated. (<em>Distance calculation requires o(n^2) time and space</em>)</p>

<p>Is there any solution to handle this problem? Is there any good package for the mentioned clustering or dimensionality reduction in R or Matlab that is suitable ?</p>
"
"0.0559892510955854","0","166536","<p>I have a dataset with 3.205 observations and 6 variables about viewing habits from Rio and Sao Paulo in Brazil. They're supposed to have different habits so I need these data to form natural clusters (e.g., popular in both cities, one or the other, or neither). Here's what the 20 first rows look like:</p>

<pre><code>Program Shr_RIO Shr_SP  Rat_RIO Rat_SP  Fid_RIO Fid_SP
000903  0,475   0,383   10,835  13,451  18,671  18,636
000891  0,188   0,392   4,782   15,069  19,919  27,576
000891  0,255   0,628   6,146   24,163  18,871  28,396
000891  0,467   1,073   11,671  40,993  27,096  54,080
005831  1,677   0,465   34,284  15,517  21,914  12,847
005831  1,296   0,341   28,660  11,704  22,661  17,542
005831  0,443   0,215   9,014   7,142   13,920  11,603
005831  2,209   0,492   47,295  16,633  36,377  18,491
005831  1,049   0,194   23,343  6,613   32,202  7,857
000021  14,568  14,478  220,870 376,907 48,186  42,395
000021  13,244  13,085  209,847 299,642 40,093  37,626
000021  11,369  10,633  184,305 265,684 37,995  37,035
001055  1,459   0,359   31,598  12,559  38,428  29,078
001055  0,499   0,984   12,330  36,874  45,344  44,331
001055  0,351   0,509   7,867   17,797  17,732  41,494
000400  8,726   7,288   148,966 200,682 51,861  36,761
000400  6,840   6,762   139,347 196,219 35,954  37,387
000400  6,192   6,789   116,733 192,109 36,731  33,938
000400  8,281   6,795   160,616 198,423 41,236  34,247
</code></pre>

<p>Each row refers to different days the program was aired, that's why you see multiple rows for the same <code>Program</code>. I know that I could summarize one <code>Program</code> per row, so I could try some hierarchical clustering, but I also know that those means have a lot of variance. Is there any way that I could try some model-based clustering or another pattern recognition algorithm to a data like that? Is there any other way to summarize and have those variance well represented into the clusters? </p>
"
"0.0685725481323742","0.0311286403182345","167734","<p>The data contains information about the genes causing breast cancer in our body. My aim is to find the most effective gene on breast cancer in our body. The code that I have first divides my sample by clustering and then shows number of genes which can separate samples into those clusters. To do the second function it uses "" mRMR.classic (mRMRe.ensemble)"" package from R. We want in this code is that instead of dividing samples by hierarchial clustering, we want to define clusters by ourselves and then this should find probesets to differentiate those two groups. However, I don't have any idea about how I can make this change. If you help me, I'm really gonna be happy.</p>
"
"0.118771137369734","0.0718885154689589","168202","<p>I'm currently checking some clustering evaluation indexes in R, and now I'm using Silhouette and its respective function in R, ""silhouette"" (in ""cluster"" package). To test the method, I used the following code:</p>

<pre><code>data &lt;- matrix(c(1,2, 2,1, 1,1, 2,2, 8,9, 9,8, 9,9, 8,8, 1,15, 2,15, 1,14, 2,14),12,2,byrow=T)
clust &lt;- c(1,1,1,1, 2,2,2,2, 3,3,3,3)
diss &lt;- as.matrix(dist(data))
sil &lt;- mean(silhouette(clust,dmatrix=diss)[,3])
</code></pre>

<p>Using this data and with ""clust"" being the obtained configuration (from k-means), I would evaluate the silhouette of this configuration by the mean of the silhouettes for each datum. The point is that I searched for its use with k-means and found this page:</p>

<p><a href=""https://stat.ethz.ch/pipermail/r-help/2008-March/155939.html"" rel=""nofollow"">https://stat.ethz.ch/pipermail/r-help/2008-March/155939.html</a></p>

<p>And it's recommended to use the squared distance matrix instead, making <code>sil &lt;- mean(silhouette(clust,dmatrix=diss^2)[,3])</code>. This use changes the result from 0.8793842 to 0.9850074.</p>

<p>The point for me is the evaluation of the configuration itself, and as I created the data to clearly show three groups, the higher silhouette for this configuration makes more sense to me than the lower one.</p>

<p>I'm not sure if I understood it right, but the use of the squared distance matrix on a k-means clustering evaluation is because of the squared distance of its cost functions. But is its use needed? I mean, the evaluation using the distance matrix would be enough to evaluate two different configurations (both resulting from k-means) and point which one is better.</p>

<p>So, should I use the squared distance for a k-means clustering evaluation? And as I'm evaluating the configuration, shouldn't the same distance matrix be used to evaluate many different methods?</p>

<p>Thanks in advance!</p>
"
"0.125195771459034","0.136398867894095","168367","<p>I have the following dataset: consider a dataset $X$ of $1400 \times 600$. The rows represent households at time $1 \leq t \leq 14$. So I have $100$ households. The columns represent the programs that they have watched on that day and for how long (in minutes).</p>

<p>My question is how to ""aggregate"" the data. The following few ideas came to mind:</p>

<ul>
<li>Take the mean over the days and then aggregate to household level.</li>
<li>Take the sum over the days and then aggregate the data.</li>
<li>(*) Cluster the household days and take the largest cluster as the most representative viewing statement of that household.</li>
</ul>

<p>I would like to do (*) method, however the question is which algorithm is the most suitable to do this?</p>

<p>I have thought up the following trivial algorithm:</p>

<ol>
<li>Take the first vector of the household and then calculate its euler distance with each of the 13 remaining vectors.</li>
<li>Aggregate those vectors whose distance is less than $\epsilon$ with the first vector and repeat this process with the remaining vectors.</li>
<li>Take the vector that has been aggregated with the most as the representative vector.</li>
</ol>

<p>The problem is choosing an appropriate $\epsilon$. Not sure whether this is a real clustering algorithm (maybe someone can give its name?). My question is now: <strong>what existing algorithm can I use for the cluster step?</strong></p>

<p>Note: I cannot normalize the data, as far as I know, because even though this data set is rather small. For big data sets, the zero values will become non-zero and we need to have a special way to store it.</p>

<p>Note: with sparse I mean lots of zeroes. </p>

<p>I looked into the ideas of BellKor's Pragmatic Chaos of the Netflix prize and it seems that they first removed the individual effects and then the program specific effects. I was wondering how they did without having the sparse matrix of netflix scores transformed into a dense matrix.</p>
"
"0.0885267789745639","0.0723364233255618","168784","<p>I am trying choose best $k$ from the consensus clustering using the Cophenetic Correlation Coefficient (CCC). I tried as follows. The correlation coefficients values are poor, i.e., <code>k=2 (0.2110048)</code>, <code>k=3 (0.1934558)</code>, <code>k=4 (0.175295)</code>. Please suggest whether am following correct method. </p>

<pre><code># consensus clustering
library(Biobase)
data(geneData)
d = geneData
# median center genes
dc = sweep(d, 1, apply(d,1,median))

rcc = ConsensusClusterPlus(dc, maxK=4, reps=100, pItem=0.8, pFeature=1, title=""example"", 
                           distance=""pearson"", innerLinkage=""ward.D"", 
                           finalLinkage=""ward.D"", clusterAlg=""hc"")

# Cophenetic Correlation Coefficient
k2 &lt;- rcc[[2]]$consensusMatrix
d1 &lt;- as.dist(t(k2))
hc &lt;- hclust(d1, ""ward.D"")
d2 &lt;- cophenetic(hc)
cor(d1, d2)
# 0.2110048

k3 &lt;- rcc[[3]]$consensusMatrix
d1 &lt;- as.dist(t(k3))
hc &lt;- hclust(d1, ""ward.D"")
d2 &lt;- cophenetic(hc)
cor(d1, d2)
# 0.1934558

k4 &lt;- rcc[[4]]$consensusMatrix
d1 &lt;- as.dist(t(k4))
hc &lt;- hclust(d1, ""ward.D"")
d2 &lt;- cophenetic(hc)
cor(d1, d2)
# 0.175295
</code></pre>
"
"0.0969762275752854","0.0660338179744218","168922","<p>My research buddy and I are conducting cluster analysis on survey data using a 7-point relevance scale (1=Not relevant, 7=Extremely Relevant). </p>

<p>We have 58 variables, arranged in 10 groups of variables (like marketing channel as a variable group, consisting of variables asking respondents to classify the relevance of a type of marketing channel to their business).</p>

<p>The individual variables have very varying distributions of responses, and the overall distribution across all responses is bimodal at 1 and 7, but still with a significant share of responses in the middle values. </p>

<p>We aim to find underlying constructs (in this instance, business models), by clustering (N=635) on their responses to these 58 variables. The research is highly exploratory, and we only have intuition for the variable relationships, both in and between variable groups. We have also conducted a PCA analysis giving 16 factors, which seem to make intuitive sense, but where we thereafter have some trouble applying and interpreting factor scores to a cluster context.</p>

<p>We would greatly appreciate any pointers to what clustering method would be appropriate for this type of dataset? If so, what would be the R package for this approach?</p>
"
"0.131306432859723","0.14630761997153","172617","<p><strong>Problem:</strong></p>

<p>I am figuring out the best way to find clusters for a dataset with observations that are densely packed together. The dataset is retail stores with three numeric variables based on operations metrics.</p>

<p>I do not know how to create a simulated dataset for an example like this. I have densely clustered data and outliers, but under 4k observations. </p>

<p><strong>Business objective:</strong></p>

<p>We need to separate the dataset into groups based on several variables.</p>

<p>The goal is to narrow down the stores with greater priority. Later on, we will use inference statistics for determining the cause of the operation metrics stated. Segmenting the stores based on priority makes sense through the three operations variables included.</p>

<p>I tried two different types of partitioning clustering methods, k-values, and different variables, but all yeilded poor validation results. Hereâ€™s the steps I took:</p>

<p><strong>Clustering with 2/3 variables:</strong></p>

<ol>
<li><p>Standardize in daisy dissimilarity matrix with euclidean distance <code>daisy()</code> function from <code>cluster</code> package in CRAN.</p></li>
<li><p>Chose k for k-means by looking at SSE chart <code>kmeans()</code> function.</p></li>
<li><p>Chose k for k-medoid by <code>pamk()</code> function in <code>fpc</code> package in CRAN for highest average silhouette width among clusters - resulted in a 0.23 average silhouette width. K-medoid was used with the <code>pam()</code> function from <code>cluster</code> package in CRAN.</p></li>
<li><p>Choose clustering algorithm by dunn-index - highest clustering result was k-medoids with 0.002. I used the <code>cluster-stats()</code> function in <code>fpc</code>.</p></li>
</ol>

<p><strong>Clustering with all three variables:</strong>
-same procedure as above.</p>

<p><strong>Result:</strong>
K-medoids with 2 clusters using two variables represented the algorithm with the highest dunn-indes. </p>

<p><strong>Overview:</strong>
After selecting the optimal number of clusters for each clustering method and comparing the best one using dunn-index, the results have overlap. </p>

<p>What is the recommended method for performing cluster analysis on densely clustered datasets? Do I need to perform clustering multiple times in order to segment the data further? </p>

<p><strong>EDIT: Added scatterplot showing clustering with 3 variables</strong></p>

<p><a href=""http://i.stack.imgur.com/ZoVyj.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZoVyj.png"" alt=""Scatterplot with cluster labels color-coded""></a></p>
"
"0.0395903791232448","0","173217","<p>I have high dimensional ($m \approx 2k$), high sample (n=140,000) dataset in R that I load into memory run PCA on it (returns $m \approx 400$ components to cover 95% of variance) then I run k means clustering on this dataset. However, even with wildly different number of clusters (from 1 to 1000) I always get the same total sum of squares. Though the cluster assigned to the datapoints I have inspected seem reasonable at first sight (they change seemingly appropriately with the number of clusters.) </p>

<p>So codewise:</p>

<pre><code>trans = preProcess(train, method=c('BoxCox','center','scale','pca'))
train_pc = predict(trans, train)
kNumbers = c(1,5,10,15,20,25,100,1000)
for (i in kNumbers) {
    model = kmeans(train_pc, centers=i, nstart=10)
    cat(model$totss + '\n')
}
</code></pre>

<p>Things I have tried:</p>

<ul>
<li>Playing with <code>iter_max</code> from 10 to 100 (then usually no warning messages about lack of convergence)</li>
<li>Increasing the <code>n_start</code> (1 to 10)</li>
<li>Preprocessing the data <code>BoxCox</code>, <code>center</code>, <code>scale</code></li>
</ul>

<p>Ultimately, this is just a part of feature engineering so if there is a way to get some generic way of informing the subsequent algorithm of some sort of idea of ""big picture"" closeness, then I am all up for that as well.</p>

<p>Any ideas?</p>
"
"0.0885267789745639","0.120560705542603","173723","<p>I am developing a semi-supervised method for identifying anomalies in a time series with multiple states. Let's consider this example time series in which there are two states e.g. state 1 and 2 with mean power 0 and 100, respectively. </p>

<p><a href=""http://i.stack.imgur.com/C8KM0.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/C8KM0.png"" alt=""enter image description here""></a></p>

<p>As we can see, whenever the system enters into a particular state, it stays there for a fixed amount of time (with some variance) before transitioning into another state during the normal operation. e.g. 20 seconds in state 1, and 40 seconds in state 2, approximately. I am looking for an semi-supervised method to detect the anomalous state in which system enters into that state but stays there for a long/short period of time than the usual duration, as marked in read in the above figure. </p>

<p>So there are two obvious questions to ask here:  </p>

<ol>
<li>Identifying the number of states from the give time series. I use a clustering method (such as k-means) to find it.</li>
<li>Learning the state properties such as power-level and state duration. </li>
</ol>

<p>I am able to detect the states and their corresponding power level (or fitting to a distribution) using a HMM from the given normal time series. But how to learn the state duration? After learning these parameters, I am planning to use a finite state machine to model the states and their properties to detect the anomalies in an online manner. Any suggestions??</p>
"
"0.0791807582464896","0.0808745799025788","173951","<p>My data has 192 variables in total (30k rows) and most of the variables have missing values. I want to do a classification model on this data e.g.</p>

<p>Only 3 dependent variables (excluding the customer ID) have 100% fill rate. Overall only 38 variables have less than 25% fill rate.</p>

<pre><code>Code used:

for(i in colnames(model_data)) {
    NA_count[i] = sum(is.na(model_data[,i]))
}

per_NA = data.frame(NA_count/nrow(model_data))
</code></pre>

<p>I have never worked this sparse data before. In my earlier experiences, we used to do the imputation mostly by the mean/median. I have also heard people using clustering for imputation but this data is so sparse (for almost all variables) that i am not sure if clustering will help (neither will knn impute). Please advice what method should be used for the same.</p>

<p>Also the predictor variable has 7% event rate</p>

<pre><code>prop.table(table(model_data_final$dependent))
         0          1 
0.92813333 0.07186667 
</code></pre>

<p>Again this is a new territory for me as i have never worked with low event rate data. I read a lot of articles wherein people have mentioned various techniques such as weighted RF, oversampling etc. Can someone please give some intuition on the same?</p>

<p>And why can't we just create a model on the data as is and then while validation decide the threshold of probability based on the performance on validation data?</p>

<p>Can't post the data for privacy reasons.</p>

<p>Thanks in advance</p>
"
"0.0395903791232448","0.0539163866017192","174476","<p>how to best predict data like this which contains multiple levels of nearly constant data?</p>

<p>Simple linear models even with weights (exponential) did not cut it.</p>

<p>I experimented with some clustering and then robust linear regression but my problem is that the relationship between these levels of constant data is lost.</p>

<p>Here is the data from the picture:</p>

<pre><code>structure(list(date = structure(c(32L, 10L, 11L, 14L, 5L, 6L, 
1L, 2L, 12L, 9L, 19L, 13L, 4L, 17L, 15L, 3L, 18L, 7L, 8L, 21L, 
16L, 22L, 28L, 29L, 30L, 26L, 27L, 31L, 20L, 23L, 24L, 25L), .Label = c(""18.02.13"", 
""18.03.13"", ""18.11.13"", ""19.08.13"", ""19.11.12"", ""20.01.13"", ""20.01.14"", 
""20.02.14"", ""20.05.13"", ""20.08.12"", ""20.09.12"", ""21.04.13"", ""21.07.13"", 
""21.10.12"", ""21.10.13"", ""22.04.14"", ""22.09.13"", ""22.12.13"", ""23.06.13"", 
""25.01.15"", ""25.03.14"", ""25.05.14"", ""26.02.15"", ""26.03.15"", ""26.04.15"", 
""26.10.14"", ""26.11.14"", ""27.07.14"", ""27.08.14"", ""28.09.14"", ""28.12.14"", 
""29.03.10""), class = ""factor""), amount = c(-4, -12.4, -9.9, -9.9, 
-9.94, -14.29, -9.97, -9.9, -9.9, -9.9, -9.9, -9.9, -9.9, -9.9, 
-9.9, -9.9, -9.9, -4, -4, -11.9, -11.9, -11.9, -11.9, -11.98, 
-11.98, -11.9, -13.8, -11.64, -11.96, -11.9, -11.9, -11.9)), .Names = c(""date"", 
""amount""), class = ""data.frame"", row.names = c(NA, -32L))
</code></pre>

<p><a href=""http://i.stack.imgur.com/DWypm.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/DWypm.jpg"" alt=""regression for multiple levels""></a></p>

<h1>revisiting rollmedian</h1>

<p>@Gaurav - you asked: Have you tried building a model with moving averages? as ARIMA didn't work - I did not try it. But I have now.</p>

<pre><code>zoo::rollmedian(rollTS, 5)
</code></pre>

<p>Seems to get the pattern of the data. However I wonder now how to reasonably forecast it. Is this possible?</p>

<p><a href=""http://i.stack.imgur.com/dPhK8.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/dPhK8.png"" alt=""rollmedian""></a></p>
"
"0.118771137369734","0.107832773203438","174556","<p>I want to use k-means to cluster my data.  I have broken one column into 4 dummy variables and I have normalized all of the data to mean=0 and sd=1.  Will k-means work with these dummy variables?</p>

<p>I have run the k-means in R and the results look pretty good, but are much more dependent on the value of these dummy variables than the rest of the data.  My 'between_SS / total_ss' = 58%</p>

<p>Data Sample:</p>

<pre><code>num_months, sales, dummy_a, dummy_b, dummy_c, dummy_d
10, 102.33, 1, 0, 0, 0
5.7, 57.5, 0, 0, 0, 1
21.3, 152.88, 0, 1, 0, 0
</code></pre>

<p>Code:</p>

<pre><code>library(""ggplot2"")
library(""scatterplot3d"")

mydata &lt;- read.csv(""data.csv"", stringsAsFactors = FALSE) 
data &lt;- scale(data)

km &lt;- kmeans(data, 4)     #Break into 4 clusters

##...combine the dummy variables into 1 field so I can use it as the 3rd dimension to graph it

results$color[results$cluster1==1] &lt;- ""red""
results$color[results$cluster2==1] &lt;- ""blue""
results$color[results$cluster3==1] &lt;- ""green""
results$color[results$cluster4==1] &lt;- ""orange""
with(results, {
    s3d &lt;- scatterplot3d(num_months, sales, dummy_combined,       
                  color=color, pch=19)       
     s3d.coords &lt;- s3d$xyz.convert(num_months, sales, dummy_combined)
})
</code></pre>

<p>edit:
Here is some code for my comment below.  It uses kmeans to cluster 3-dimensional data, 2 of which are binary data. It looks like it does a fine job clustering.</p>

<pre><code>seed(2015)
v1 &lt;- c(runif(500, min = -10, -5), runif(500, min = 5, 10))
v2 &lt;- round(runif(1000, min=0, max=1))
v3 &lt;- round(runif(1000, min=0, max=1))
v1 &lt;- scale(v1)
v2 &lt;- scale(v2)
v3 &lt;- scale(v3)

mat &lt;- matrix(c(v1,v2),nrow=length(v1))

k &lt;- kmeans(mat,4)

plot3d(v1, v2, v3,  size=7, col = k$cluster)        
</code></pre>

<p><a href=""http://i.stack.imgur.com/d7OtI.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/d7OtI.png"" alt=""enter image description here""></a></p>
"
"0.0685725481323742","0.062257280636469","174570","<p>I have some data that should be randomly assigned to treatment $T$, and am running some tests on observables to give evidence that this is indeed the case.</p>

<p>Let's focus on an outcome I'll call $X$, which is discrete, so the most natural test of if $X \perp T$ is Pearson's $\chi^2$. The Pearson test (via <code>chisq.test</code> in R) gives a $p$-value of .008, which at first glance indicates a fair amount of correlation between $X$ and $T$. Code was specifically:</p>

<pre><code>library(data.table)
analysis_data[ , chisq.test(x, treatment)$p.value]
</code></pre>

<p>However, there is a fair amount of meaningful clustering in the data, which is correlated with $X$ (e.g., as cluster size increases, average $X$ decreases).</p>

<p>To deal with this, I tried to implement a blocked bootstrap as follows:</p>

<ol>
<li><p>Assign cluster ID number</p></li>
<li><p>For each iteration $b=1,\ldots,B$:</p>

<p>a. draw $n$ cluster IDs at random (with replacement, where $n$ is the total number of clusters and create a sample consisting of those $n$ clusters of observations.</p>

<p>b. Calculate the Pearson's $\chi^2$ statistic (via <a href=""https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test#Test_of_independence"" rel=""nofollow"">Wikipedia</a>) within each sample via <code>chisq.test(x, treatment)$statistic</code></p></li>
<li><p>Calculate $\tau_0$, the $\chi^2$ value in the original sample.</p></li>
<li><p>Calculate the $p$-value as the proportion of $\tau_b$ which exceed $\tau_0$, i.e.</p></li>
</ol>

<p>$$p = \frac{1}{B}\sum_{b=1}^B \mathbb{1}[\tau_b &gt; \tau_0]$$</p>

<p>(in code:</p>

<pre><code>analysis_data[ , cluster_id := .GRP, by = cluster_vars]
setkey(analysis_data, cluster_id)
test_dist &lt;- replicate(BB, analysis_data[
  .(sample(unique(cluster_id), rep = TRUE)),
  chisq.test(x, treatment)$statistic])
    t0 &lt;- analysis_data[, chisq.test(x, treatment)$statistic]
mean(test_dist &gt; t0)
</code></pre>

<p>)</p>

<p>When I do this, the $p$ value I get out (with <code>BB</code> = 10000) is .81. That strikes me as an almost unbelievably large change.</p>

<p>What's more, when I restrict attention to the subsample of size-1 clusters, I am met again with a large (though partially mediated) difference between the bootstrapped and the parametric result-- .28 from <code>chisq.test</code> vs. .88 from my bootstrap procedure.</p>

<p>Am I doing something wrong? What might be causing such large-magnitude changes in $p$-value between the procedures?</p>
"
"NaN","NaN","175547","<p>I have 200 15x15 matrices containing correlation values between 15 nodes at 200 different time points. I want to cluster the 200 matrices using k-means clustering. Is there a way to do this, probably in Matlab or R?</p>

<p>Update: I was able to solve it in matlab by unwraping each 15x15 matrix into vectors and apply kmeans().</p>
"
"0.125195771459034","0.0852492924338092","176578","<p>I have multiple images from a 3D-Scanner in point cloud form. Part of the image is a fixture to hold the object to be scanned. I want to extract the object itself by classifying the fixture and the object in two separate classes = estimating a discriminant hyperplane, that cuts the cloud exactly at the points where the fixture touches the object. </p>

<p>2D Image of an SVM estimation (via the excellent klaR package) as an example:
<a href=""http://i.stack.imgur.com/vsohB.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/vsohB.png"" alt=""enter image description here""></a> 
Some bullet points about the external conditions:</p>

<ul>
<li>The objects vary in shape and size. </li>
<li>The position of the fixture varies a bit between images, since it is flexible</li>
<li>The origin of the images varies, since the calibration is sometimes off (due to temparature changes etc.)</li>
</ul>

<p><strong>The problem resulting from this</strong>: </p>

<p>The absolute position of an estimated hyperplane is the same, independent of the data. The point, where the point cloud of the object ends and the point cloud of the fixture begins, varies,  but this is the point where the hyperplane is needed in every picture. Optimally, they would change position dependent on the data. </p>

<p>Some bullet points about what i have tried so far:</p>

<ul>
<li>Despite knowing I exclude myself from excellent libraries such as PCL, I use R</li>
<li>Clustering works rather badly, since the point where the fixture touches the object is too large so it separates them not very well.</li>
<li>LDA, QDA, RPart, KNN and SVMLight give me between 70-90% accuracy when comparing to a manual classification, when centering and standardizing the images by themselves</li>
</ul>

<p><strong>My Question(s):</strong></p>

<p>What would you propose could be done about the fix hyperplanes? Do you think one could derive some sort of parameter from the image so the planes will be moved accordingly? </p>

<p>Is there maybe another Discriminant analysis method, that would be better suited here, preferrably with an R implementation? </p>

<p>Is there a feature that i should calculate, that could help me in the separation when added as a discriminating variable?</p>
"
"0.111978502191171","0.114373927749453","177796","<p>I am trying to make group together different datasets using unsupervised algorithms (clustering). The problem is that I have many features (~500) and a small amount of cases (200-300).</p>

<p>So far I used to do only classification problems for which I always had labeled data as training sets. There I used some criterion (i.e. random.forest.importance or information.gain) for preselection of the features and then I used a sequential forward selection for different learners to find the relevant features.</p>

<p>Now I see that in case of unsupervised learning I have neither any criterion for preselection nor can I use the sequential forward selection (at least not in the mlr package).</p>

<p>I was wondering if I could do a principal component analysis before to find a small number of features to fead to my clustering algorithm. Or do you have any other idea?</p>

<p>Thanks</p>

<p>edit:</p>

<p>Ok, so after some research online I can update my question a bit:
First of all I have read some articles that discourage the use of PCA before clustering algorithms, due to two reasons:</p>

<ul>
<li><p>The PCs are functions of all features so it is hard to relate the result to the inital data set and thus it is harder to interpret</p></li>
<li><p>Moreover, if you have the problem that in truth only a very small fraction of your features are helpful to do the clustering, it is not said that these features are also describing the largest variance among the samples (which is what the PCs do)</p></li>
</ul>

<p>So PCA is off the table...</p>

<p>Now I am back to my initial idea to do a sequential forward selection for clustering.</p>

<p>What performance measure would you recommend? (I thought about the Dunn-Index)
Which clustering algorithm would lead to clusters of more or less the same size? (for hierarchical clustering I usually get one cluster with a single outlier and another with all the rest -> so I would need something that somehow protects against outliers)</p>

<p>Hope you guys can help me...</p>
"
"0.0395903791232448","0","178383","<p>I ran a hierarchical clustering fit with the <code>ward.D2</code> option in R's <code>hclust</code> function. My understanding is that then the distance criterion is Euclidean distance squared. </p>

<p>I then read in the docs that the fit's <code>height</code> attribute is </p>

<pre><code>a set of n-1 real values...The clustering height: that is the value of the criterion associated with the clustering method for the particular agglomeration.
</code></pre>

<p>Am I correct that then plotting <code>fit$height</code> vs index is like plotting group number vs. SSE given that I've used Ward's method? If not, what exactly is this criterion?</p>

<p>Thanks for being patient with a newbie.</p>
"
"0.111978502191171","0.114373927749453","179373","<p>I want to cluster high dimensional sparse data (100k rows and 2k columns, 10-20  non-zero values per row). Each row represents a person and each column an attribute this person does or does not posess (the columns may not be  independent from each other):</p>

<pre><code>person attr1 attr2 attr3 ...
pers1      1    0     1
pers2      0    0     1
pers3      0    0     1
pers4      0    1     0
</code></pre>

<p>To me this sounds like a subspace clustering issue so I wanted to apply CLIQUE and Entropy weighted kmeans (EWKM) both implemented in R. But CLIQUE does not seem to converge after 5 hours of calculation and EWKM finishes after 1 Iteration.</p>

<p>EWKM quits because after initialization it has a solution with a ridiculous small delta (below 1.0e-20) no matter what k or lambda i choose. The cluster results from different runs share no similarities. Therefore I do not believe that the results are more than arbitrary. Is EWKM even suited for this kind of data? The entropy in each column is VERY low (only 0's and 1's but almost always 0, even for attributes that are comparatively often shared).</p>

<p>T-SNE produces a plot like this on a random subset of the data:
<a href=""http://i.stack.imgur.com/Qng3T.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Qng3T.png"" alt=""T-SNE Plot""></a>
To me it looks like there are a handful of obvious cluster centers but not really strong partitions. Do you have an idea what algorithms may produce good results? Is EWKM even suited for this kind of data?</p>
"
"0.0791807582464896","0.107832773203438","181166","<p>I have a data set of deposits and withdrawals from bank locations, so each record includes a bank identifier, date stamp, number of deposits, and number of withdrawals. I have included reproducible code below.  Note that I have hundreds of days for each of thousands of agents. I have a few questions:</p>

<ol>
<li><p>I would like to use the forecast package, but I'd like it to take into account day of week patterns and month of year seasonality. The one worry I have about using the ""frequency"" flag is that I have some randomly missing days for each agent. How should I best deal with this?</p></li>
<li><p>Deposits and withdrawals cannot be negative so all of the data is non-negative. Can I force the forecast to be non-negative? I know I can use the lambda=0 flag, but this only works when everything is strictly positive (but not applicable in this case because deposits and withdrawals can be zero)</p></li>
<li><p>Is there anything I can do to increase predictive accuracy by ""clustering"" banks? Right now, I'm only using a particular bank's data, but given that I have data from thousands of more banks, perhaps I can take advantage of this?</p></li>
</ol>

<pre class=""lang-r prettyprint-override""><code>library(lubridate)
library(dplyr)
library(forecast)

Date = c(today()-1, today()-3, today()-4, today()-5) %&gt;% as.POSIXct
D    = c(10,3,4,3)
W    = c(13,2,4,4)
Bank = c(rep(1,4), rep(2,4))
A    = cbind(Date,D,W) %&gt;% as.data.frame 
A$Date = A$Date %&gt;% as.POSIXct(origin=""1970-01-01"")
B   = A
B$D = B$D - 1
B$W = B$W + 3
A   = A %&gt;% rbind(B)
A   = A %&gt;% cbind(Bank,.)
</code></pre>
"
"0.0685725481323742","0.062257280636469","182336","<p>I have a data-set with data in following format:</p>

<p><a href=""http://i.stack.imgur.com/OejUP.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/OejUP.jpg"" alt=""Format of data""></a></p>

<p>Basically, data represents whether a particular word is present in that file or not. So say If file with doc-id 1 contains word1 then we put 'Yes' else 'No'. I need to cluster this files in k clusters.</p>

<p>Now here attributes are categorical (i.e. Yes or No) except docid column which I remove in preprocessing. I tried various clustering algorithms (ROCK, K-Modes etc) in WEKA and R for clustering this data but in most of them almost all files are forming a single cluster with rest k-1 clusters getting only 1 or 2 doc-ids.</p>

<p>My question is which clustering algorithm would be best for such data and why my data is forming a single cluster with almost all documents and rest clusters remaining sparse?</p>
"
"0.167967753286756","0.216039641304523","182988","<p>I've run a fully within-subjects repeated-measures ANOVA using the <code>aov()</code> function. My dependent variable is not normally distributed, so I'm very interested in running assumption tests on my analysis. It seems that just calling <code>plot()</code> on the output doesn't work for repeated-measures, so I've manually taken the residuals and the fitted values for a model of interest, and have plotted them against each other. I'm assuming that this is how I would plot to test for the assumption of Homoskedasticity.</p>

<p>The plot comes out with 2 vertical bands (please see the image below). It turns out the fitted values are all centred around 2 values (although according to <code>==</code> they are not exactly equal), where one is the negative of the other.</p>

<p>I have 2 questions:</p>

<p>1) Is this the correct way to manually test the assumption homoskedasticity? If not, how would I go about it from repeated-measures designs (since just calling <code>plot()</code> doesn't work)?</p>

<p>2) If it is correct, what is this plot telling me? Why are the fitted values so clustered? What can I conclude from it?</p>

<p>Thanks heaps for any input here. Also, if you know of better ways to check (preferably plot) for assumptions in rm-ANOVAs, that would be useful information as well.</p>

<p>I've included some mock data here to replicate the scenario:</p>

<pre><code>#Create mock data (there's probably a more efficient way to do this.. would also be nice to know! :) )
p &lt;- sort(rep(1:20,8))
y &lt;- rep(rep(1:2,4),20)
z &lt;- rep(rep(c(1,1,2,2),2),20)
w &lt;- rep(c(1,1,1,1,2,2,2,2),20)
x &lt;- rnorm(160,10,2)

d &lt;- data.frame(x,p=factor(p),y=factor(y),z=factor(z),w=factor(w))

#Run repeated-measures ANOVA
ex.aov &lt;- aov(x ~ y*z*w + Error(p/(y*z*w)), d)

#Try to plot full object (doesn't work)
plot(ex.aov)

#Try to plot section of object (doesn't work)
plot(ex.aov[[""p:y:z""]])

#Plot residuals against fitted (custom ""skedasticity"" plot - works)
plot(residuals(ex.aov[[""p:y:z""]])~fitted(ex.aov[[""p:y:z""]]))
</code></pre>

<p><a href=""http://i.stack.imgur.com/pBexJ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pBexJ.png"" alt=""enter image description here""></a></p>

<p><strong>Begin Edit</strong></p>

<p>In light of the information provided by @Stefan , I've added some additional details below, using the improved data structure he proposed:</p>

<pre><code># Set seed to make it reproducible
set.seed(12)

#New variable names and generation
subj &lt;- sort(factor(rep(1:20,8)))
x1 &lt;- rep(c('A','B'),80)
x2 &lt;- rep(c('A','B'),20,each=2)
x3 &lt;- rep(c('A','B'),10, each=4)
outcome &lt;- rnorm(80,10,2)

d3 &lt;- data.frame(outcome,subj,x1,x2,x3)

#Repeated measures ANOVA
ex.aov &lt;- aov(outcome ~ x1*x2*x3 + Error(subj/(x1*x2*x3)), d3)

#proj function
ex.aov.proj &lt;- proj(ex.aov)

# Check for normality by using last error stratum
qqnorm(ex.aov.proj[[9]][, ""Residuals""])
# Check for heteroscedasticity by using last error stratum
plot(ex.aov.proj[[9]][, ""Residuals""])
</code></pre>

<p>The resulting plots are below:</p>

<p><a href=""http://i.stack.imgur.com/wFiYy.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/wFiYy.png"" alt=""Repeated measures normality check?""></a></p>

<p><a href=""http://i.stack.imgur.com/siHVi.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/siHVi.png"" alt=""Repeated measures homoskedasticity check?""></a></p>

<p>Can anyone interpret the images above (especially the last one)? It looks like there is clustering and pattern structure. Can it be used to infer the presence of heteroskedasticity?</p>
"
"0.0395903791232448","0.0539163866017192","187595","<p>I frequently come across data sets that have both categorical and numeric data.  I think this is just a fact of life where the data is not all in one category.  I'm basically trying to find some practical approaches or strategies on how to go about solving this problem.</p>

<p>In many books and initial searches on google I tend to get some sort of Kmeans clustering and a lot of phd-looking papers.  I think this is a common problem so I'm wondering if anyone can guide me to some practical suggestions or methods that I can further investigate and implement in R or Python?</p>

<p>I'm basically lost and just looking for some direction!</p>
"
"0.153332879014365","0.153132662757169","188235","<p>Recently I tried to cluster the <code>semeion.data</code> file available in <a href=""http://archive.ics.uci.edu/ml/datasets/Semeion+Handwritten+Digit"" rel=""nofollow"">UCI repository</a> using <code>kmeans</code> clustering as well as using the <code>neuralgas</code> method available in R's <code>flexcust</code> package. This file contains handwritten digits and pixel values are either 0 or 1 in an image of 16 X 16 pixels. </p>

<p>I have slightly modified the <code>semeion.data</code> file in that it contains just 257 columns with 256 pixel-values, as usual, and the last column is the digit label for that image: (0,1,2..9). Data set size is 1593 X 257.</p>

<p>I removed the 257th column and then tried to cluster the remaining dataset first, using k-means and then using <code>neuralgas</code> from the <code>flexclust</code> package. The results from <code>neuralgas</code> method are quite superior. The R-code and the results in both cases are as follows:</p>

<pre><code># R-code using kmeans without neuralgas
data &lt;- read.csv(""mod_semeion.csv"", header=FALSE)   # Data has no headers   
df   &lt;- data[,-257]                                 # df Data frame without digits labels
cl   &lt;- kmeans(df, 10 , iter.max = 500, nstart=200) # create 10 clusters
table(cl$cluster,data[,257])                         # Which cluster has which digits

       0   1   2   3   4   5   6   7   8   9
  1    0   0   1   4   9  69   1   5   8  75    # Cluster 1 has both digits 5 and 9
  2   85   0   0   0   0   1   1   0   0   1    # Cluster 2 has mainly digit 2
  3   70   0   0   0   0   2  38   0   1   1
  4    0  55   6   0  19   6   5 131   5   8    # Cluster 4 has mainlt digit 7
  5    0   2   0  96   0  63   1   0  19  44
  6    0  99  14   7   9   0   0  20   1   3
  7    0   4  96  43   2   6   0   0  17   6
  8    3   1   3   0 119   2   1   1   0   1    # Has mostly digit 4
  9    2   0  35   9   0   3   0   1 103  19
  10   1   1   4   0   3   7 114   0   1   0    # has mostly digit 6
</code></pre>

<p>Columnwise, for example, digit 7 is mostly clustered in cluster 4,
and digit 6 is mostly clustered in cluster 10. </p>

<pre><code># Distribution of digits in the data-set are as:
table(data[,257])   # Also, the column totals in each of the above 10 columns   (0-9)   
  0   1   2   3   4   5   6   7   8   9 
161 162 159 159 161 159 161 158 155 158 

# Cluster-wise points distribution are as:
table(cl$cluster)    # row-population/per-cluster
  1   2   3   4   5   6   7   8   9  10 
 172  88 112 235 225 153 174 131 172 131 

# Average relative cluster purity with kmeans: 
#  (max-of-a-digit)/cluster-population = (max-per-row/row-population)
((75/172) + (85/88) + (70/112) + (131/235) + (96/225) + (99/153) + (96/174) + 
 (119/131) + (103/172) + (114/131))/10
[1] 0.6587315  
</code></pre>

<p>Using the neural gas code, the average relative cluster purity is around 70% as against 65.8% as above. The code and the results are as below:</p>

<pre><code># R-code with neuralgas method
library(flexclust)  
obj &lt;- new(""cclustControl"")
obj@iter.max &lt;- 500
cl1 &lt;- cclust(df, 10, dist=""manhattan"", method=""neuralgas"", control=obj)
table(cl1@cluster, data[,257])  # which cluster has which digits

       0   1   2   3   4   5   6   7   8   9
  1  150   0   0   0   2   2  38   0   1   3    # Has mostly digit 0
  2    2   0  22   7   0   0   0   0  94  11    # has mostly digit 8
  3    0 105  24  11   9   0   0  25   2   6
  4    6   3   4   1   7  11 113   1   2   1
  5    2   1   4   0 118   1   0   0   0   0
  6    0  45   0   0  17   2   4 129   3   4
  7    1   2   1   1   4  71   1   3   8  29
  8    0   2   0 135   0  69   5   0  25  37
  9    0   4 104   3   1   3   0   0   8   1
  10   0   0   0   1   3   0   0   0  12  66


# Cluster-wise points distribution is as:
table(cl1@cluster)      # row-wise-total / per-cluster

  1   2   3   4   5   6   7   8   9  10 
196 136 182 149 126 204 121 273 124  82 

# Average relative cluster purity with neuralgas
((150/196) + (94/136) + (105/182) + (113/149) + (118/126) + (129/204) +  
 (71/121) + (135/273) + (104/124) + (66/82))/10
[1] 0.7085526
</code></pre>

<p>As can be seen from the above two cross-tables, neuralgas improves 
cluster purity and some digits are recognized much better using
neuralgas. For example, most 0's are now clustered together in cluster 1.</p>

<p>My question is: What is the neural gas method? Is it possible to explain how it works conceptually / in simple terms?</p>
"
"0.111978502191171","0.152498570332605","188661","<p>I have cross-sectional data with ~800 individuals nested within 6 countries, across 3 time points. Each individual is sampled only once, so time is cross-sectional here too. The number of individuals within each country ranges between ~75 to ~200. </p>

<p>I would like to use fixed effects logistic regression to model the data (the outcome variable consists of the number of successes out of the total number of trials). I know I can use a conditional likelihood estimator to avoid incidental parameter bias, but this does not allow me to report the results as unconditional predicted probabilities (the best format for my audience). </p>

<p>I'm therefore considering including dummy variables for countries in the model. With the small number of countries and large number of observations within each, is this a reasonable approach? </p>

<p>The model (in <code>R</code> syntax) would look like this:</p>

<pre><code>glm(cbind(success, total - success) ~ var1 * factor(time) + factor(country),
    family = bimonial(link = ""logit""),
    data = dat)
</code></pre>

<p>where <code>factor(country)</code> produces 5 dummy variables for the 6 countries and <code>factor(time)</code> produces 2 dummy variables for the 3 time periods. I'm interested in how the slope of <code>var1</code> changes across the 3 time periods, while controling for the clustering by country.</p>

<p>(Note: I get very similar results to that obtained from the conditional likelihood approach, but I can report unconditional predicted probabilities.)</p>
"
"0.0395903791232448","0","188816","<p>I am trying to do k-means clustering in R using the cclust package.
In k-means clustering, the initial centroid assignment greatly affects the final allocation. The kmeans package has an nstart option, which guarantees that your results are based on 'nstart' number of initial configurations. Is there an equivalent option for the same in the cclust package?</p>

<p>Thanks.</p>

<p>TAK</p>
"
"0.0685725481323742","0.0933859209547035","189135","<p>I have some data that I want to get the important variables from. I want to use random forest to get this information. The problem is that the data does not have labels. From what I understand random forest is a supervised learning algorithm, but one can generate synthetic data to help with the prediction.</p>

<p>Can random forest look for an underlying pattern in the data? If so how would I do this? Essentially can I get the important variables from this unlabeled data and how?</p>

<p>How does this compare to clustering?</p>

<p>Some source:</p>

<p><a href=""https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#unsup"" rel=""nofollow"">https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#unsup</a></p>
"
"0.0685725481323742","0.0311286403182345","189163","<p>I want to use the <code>blockcluster</code> package in <code>R</code> to perform co-clustering on my data. But the function requires the number of row and column clusters to be prespecified. How do you decide the numbers of row and column clusters? Shall I do k-means clustering on the rows and columns separately prior to the use of <code>blockcluster</code>? </p>
"
"0.118771137369734","0.0718885154689589","189164","<p>I have a data set with 9000 instances and 40 attributes of mixed data, that is categorical and numeric. My target is to group them into clusters using whichever clustering algorithm works best. I've heard/read that for such a data set Gower distance is suitable. My question is can I combine two (or n) metrics for calculating distances between instances, for example I would like to use let's say Euclidean distance on numeric attributes and Gower distance on categorical attributes. I could always divide my data set into two data sets, one with numeric attributes and the other with categorical. But how could one interpret each result? Summing them up just sounds ... wrong.</p>

<p>My second question is what exactly does Gower distance do with numeric values? Does my first question even make sense? <br>
<br>
Here is a snippet of my code, I am using R and functions <code>daisy</code>, <code>agnes</code>from package <code>cluster</code>: <br></p>

<pre><code>df.diss &lt;- daisy(df, metric = ""gower"", type = list(numeric = c(1, 4, 6, 8, 9, 11, 12, 13, 14, 17 : 37), symm = c(2, 3, 5, 7)), stand = FALSE)
df.clust &lt;- agnes(df.diss)
</code></pre>

<p>Using these functions or even R is not a must.</p>
"
"0.111978502191171","0.133436249041029","189490","<p>The basic idea of the problem is that I need to cluster a set of points for which I have a dissimilarity matrix.</p>

<p>I have a dataset of around 4600 points (latitudes and longitudes). I have also precomputed the dissimilarity matrix which is nothing but a <code>timeMatrix</code> (travel time between all pair of points).</p>

<p>In the time matrix if a point is not reachable to another within a certain time, say 5 minutes, then the value in the matrix corresponding to those points is marked as Long.MAX_VALUE. Otherwise, the time is calculated by querying a Trip Planner API. (So this is just to reduce the number of queries to the API)</p>

<p>I am using <code>pam</code> (Partitioning Around Mediods) method in R. </p>

<p>The code that I am using is:</p>

<pre><code>kmed &lt;- pam(timeMatrix, 100, diss = TRUE)
</code></pre>

<p>When I try to visualize this using <code>clusplot</code>, I get really weird plots.</p>

<pre><code>clusplot(timeMatrix, kmed$cluster, diss=TRUE)
</code></pre>

<p>The image for this plot is:<a href=""http://i.stack.imgur.com/CrNZu.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/CrNZu.jpg"" alt=""clusplot output when using timeMatrix""></a></p>

<p>I also tried passing the dataset of points (<code>lmks</code>) into the clusplot function, but I got a similar plot for it as well.</p>

<pre><code>clusplot(lmks, kmed$cluster, diss=FALSE)
</code></pre>

<p>The image for the plot is: <a href=""http://i.stack.imgur.com/F5i0S.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/F5i0S.jpg"" alt=""clusplot output when using lmks dataset(latitudes and longitudes of the points)""></a></p>

<p>I request you to please help me understand what is happening and how to deal with it. There are <code>ellipses</code> on the points describing the clusters, but I don't know about the <code>pink colored noise</code> on the top.</p>

<p>Also, it would be great if you can suggest me about some other clustering method in which I can pass in the <code>timeMatrix</code>.</p>

<p>Thanks. Sorry, if this question is trivial, as I am a beginner in cluster analysis and R.</p>
"
"0.0685725481323742","0.062257280636469","189878","<p>I am using the unsupervised form of <code>randomForest</code> (R) as part of a clustering scheme for 1000 peaks based on 30 features. I use the proximity matrix produced by randomForest as input to a clustering algorithm such as Ward's hierarchical algorithm in <code>hclust</code>. The difficulty I am facing is that I want to quantify how ""certain"" the randomForest is of its proximity matrix. I notice that on subsequent runs (or different seeds) of randomForest, my clustering output tends to change quite a bit (based on different #'s of peaks in each cluster, or looking at the MDS plot). So the proximity matrix is not constant (expected to some degree in a randomized algorithm), but as this may greatly affect reproducibility of / confidence in the downstream clusters, I want to know <em>how</em> un-constant it is.</p>

<p>How do I quantify the confidence of unsupervised randomForest in its creation of the proximity matrix? </p>
"
"0.0791807582464896","0.0539163866017192","190139","<p>I want to perform Bayesian hierarchical clustering in R. I have 4 variables that 3 of them are nominal and 1 is discrete. So my data are mixture of nominal and discrete data. Data are:</p>

<pre><code>U.code  degree  A   B   kind  

450     dma     7   yes T
66      d3      1   no  S
107     d3      1   no  S
135     d4      1   no  S
165     dmb     6   yes S
54      dmb     1   yes S
317     d3      0   no  T
411     d3      2   yes S
21      d2      1   yes S
312     d3      1   yes S
362     d3      1   no  S
392     d2      1   no  S
706     d3      3   no  S
1019    d1      5   yes S
0       d2      3   yes S
56      dmb     3   yes T
231     d4      1   yes S
270     d3      1   yes T
146     d2      6   yes T
736     d3      2   no  S
</code></pre>

<p>On the other hand, I know that there are 2 functions such as ""bclust"" and ""bhc"" to perform Bayesian hierarchical clustering. I want to know that if performing bclust or bhc in R to do Bayesian clustering sounds good? and is it right?</p>
"
"0.111978502191171","0.114373927749453","190168","<p>i'm not really experienced in spatial stats yet, but i'm growing into it. </p>

<p>I basically want to ascertain if certain values in a raster are a) autocorrelated and b) are more likely to exist in a certain spatial area (k-means? i'm unsure)</p>

<p>the data is a single raster showing land use change from 1 year to another (sorry I can't post the data) and there are about 50 possible changes (some far more prevalent than others). Quickly viewing the raster, it is clear that some changes are more prevalent in northern extremes, some in areas of upland farming etc etc, patterns do exist. But I want to prove this with stats. </p>

<p>For a) Local Moran's I (using a simple binary queen's spatial-weights matrix) gives us indication of spatial autocorrelation - this is useful for finding 'clumps' of similar data, correct?</p>

<p>For b) I'd like to explore whether each change combination is more likely to exist in a certain part of the UK (in Scotland, in western extremes etc). Would this be some sort of k-means clustering?</p>

<p>I'm doing all this in R,</p>

<p>thanks for any help 
(<a href=""http://gis.stackexchange.com/questions/90691/r-raster-package-morans-i-interpretation/108558#108558"">this question</a> had some good info re Moran's I),
(this <a href=""http://stats.stackexchange.com/questions/9739/clustering-spatial-data-in-r"">question</a> seems to start out with a similar goal, finding regional patterns in surface sea temps but fizzled out).</p>
"
"0.104746297470869","0.122270871890883","190284","<p>I am using the <code>mice</code> package in R to create multivariate imputed datasets. For this, I am using <code>mice(data, meth= ""rf"")</code>function. I want all the variables to use this method for prediction because there are many non-linear relationships and hypothetical interactions between several of the variables. <a href=""http://aje.oxfordjournals.org/content/early/2014/01/12/aje.kwt312.full"" rel=""nofollow"">http://aje.oxfordjournals.org/content/early/2014/01/12/aje.kwt312.full</a></p>

<p>My dataset is also a hierarchical dataset with clustering. Is there a way to take this clustering into account when specifying the prediction matrix for the <code>mice</code> function? I am aware that if you use other methods like <code>meth = 2l.pan</code> or <code>meth =2l.norm</code>, you can take clustering into account, but I want to avoid these methods because I think the non-linear relationships trump the homogeneity introduced by the cluster effect. </p>

<p>If there is a way to use the random forest method with MICE and take clustering into account, could you also provide some mock code for specifying the prediction matrix?</p>
"
"0.118771137369734","0.0718885154689589","190938","<p>based on customer data I want to perform a clustering using different clustering algorithms (K-Means, Expectation Maximization, etc.) in R. The most attributes were engineered pursuing the goal to be basically meaningful for a customer segmentation.</p>

<p>Without feature selection, the results are very poor regarding evaluation criteria like ASW, BSS, WSS, etc.</p>

<p>My question now is whether I need to do a feature selection technique (wrapper/filter) or just select the features I think are most valuable for segmenting the customers. I found very different sources regarding this issue. The most authors say the features have to be selected concerning the business objective. Other sources propose feature selection methods for unsupervised learning. Is that really useful for a customer segmentation or is it only needed for image segmentation for instance?</p>

<p>My opinion is: Attributes might be economic valuable even so not useful for the clustering process and vice versa. This would mean I select manually the features.</p>

<p>I performed already a PCA which resulted also in poor results regarding clustering evaluation criteria. Therefore I obviously have to select only a few attributes in order to obtain a clear and stable clustering.</p>
"
"0.0559892510955854","0.0381246425831512","191033","<p>I've just started reading about clustering and classification. It's a djungle, a fascinating one. Currently, however I have a rather urgent task, i.e to perform a sort of cluster analysis in the sense that I'd like to cluster my patients according to their phenotypes (biomarkers - continuous and categorical variables) and examine whether survival differs according to cluster. I'm not interested in any specific predictor, the purpose is merely to examine whether there are specific clusters of patients and whether the phenotypes associate with outcomes.</p>

<p>I'm looking for general advice on what type of <strong>method</strong> to use as well as recommended <strong>R package</strong>. I have 10 variables that are relevant for the phenotype. I could attach some data but I doubt it would contribute to the question, which is of more general character.</p>

<p>Thanks in advance.</p>

<p><strong>Update</strong>:
I'm looking for pros and cons of various techniques, with application to these kind of data. And I humbly understand that clustering may not be that straight forward.</p>
"
"0.0791807582464896","0.0269581933008596","193698","<p>Why pvclust is clustering the coloumns while hclust is clustering the rows. I am confused since pvclust function uses hclust for clustering. I tried both function with the same dataset, but the dendrograms they generate is different; hclust with dist function clusters the rows (customers in my data set), pvclust clusters coloumns (rules in my data set). I use ward.D2 with ""euclidean"" distancefor both function. Thanks in advance.</p>
"
"0.0969762275752854","0.110056363290703","194768","<p>I am trying to understand how to interpret the results I get from LDA.</p>

<p>Running from the iris dataset in R, I can see the discriminant coefficients are in the model and then I can plot the model to see the clustering.  However none of this appears interpretable to me.</p>

<pre><code>library(MASS)
mdl&lt;-lda(Species~.,data=iris)
mdl
</code></pre>

<p>Mdl Output Below</p>

<pre><code>    Call:
lda(Species ~ ., data = iris)

Prior probabilities of groups:
    setosa versicolor  virginica 
 0.3333333  0.3333333  0.3333333 

Group means:
           Sepal.Length Sepal.Width Petal.Length Petal.Width
setosa            5.006       3.428        1.462       0.246
versicolor        5.936       2.770        4.260       1.326
virginica         6.588       2.974        5.552       2.026

Coefficients of linear discriminants:
                    LD1         LD2
Sepal.Length  0.8293776  0.02410215
Sepal.Width   1.5344731  2.16452123
Petal.Length -2.2012117 -0.93192121
Petal.Width  -2.8104603  2.83918785

Proportion of trace:
   LD1    LD2  
0.9912 0.0088 
</code></pre>

<p><strong>What I would like to be able to say is if sepal.length is y and petal.width is x then this flower will be z.</strong>  Clearly I wish to apply this to other datasets.  Alternatively I could get that from a tree structure.</p>

<p><strong>As a secondary question, how do you know how well the model fits?</strong> Looking at the correlations in this dataset, it appears to me that Petal.Length &amp; Petal.Width are strongly correlated so I would think the appropriate thing to do would be to drop one of them.  I know I can look at the accuracy on a test dataset, but is there anything here that tells me in the model how good this model is, an equivalent R^2?</p>

<p><strong>Finally, why are their only 2 discriminant variables given that there are 3 variables?</strong></p>

<p>I've seen some questions on this but never giving a full answer as to if and how this is possible to interpret.  To me the value in these models is being able to say, if you do x then you get y, you can then take that knowledge and apply it to the appropriate business context.  If this is not possible then where is the value in it?</p>
"
"0.0559892510955854","0.0762492851663023","195096","<p>Probably a simple question but I'm trying to interpret BIC for k-means.</p>

<p>I have some k-means clustering and calculating BIC gives me a negative value, with a plot something like this:</p>

<pre><code>-75000 |                 xxxxxxxxxxx
       |            xxxxx           xxxxx
(BIC)  |        xxxx
       |     xxx
       |   xx
-80000 | x
       ------------------------------------
         2           (k)  25             50
</code></pre>

<p>I've searched around but I can't find any results that show a plot like this, apart from on another unanswered question (<a href=""http://datascience.stackexchange.com/questions/6508/k-means-incoherent-behaviour-choosing-k-with-elbow-method-bic-variance-explain"" title=""here"">here</a>).</p>

<p>Does a ""smaller"" BIC mean that my best number of clusters is ""2"" (most negative), or ""25"" (closer to zero), or is my plot just broken?</p>
"
"0.0685725481323742","0.062257280636469","197669","<p>I'm doing the following maximum likelihood estimation using mle2 function from <em><a href=""https://cran.r-project.org/web/packages/bbmle/index.html"" rel=""nofollow"">bbmle</a></em> package:</p>

<pre><code>library(bbmle)
llik.probit2&lt;- function(aL,beta, Ks, Kw, Bs, Bw, dta){
  Y &lt;- as.matrix(dta$qualified)
  sce1 &lt;- as.matrix(dta$eds)
  wce1 &lt;- as.matrix(dta$edw)
  sce1_obs &lt;- (as.matrix(dta$eds_obs))
  wce1_obs &lt;- (as.matrix(dta$edw_obs))
  obs &lt;- as.matrix(dta$CombinedObservable1)
  c &lt;- as.matrix(dta$const)
  phi &lt;- pnorm(ifelse(Y == 0, -1, 1) * (-aL*c + beta*obs + (Bs+Bs*Ks-aL)*sce1 -beta*Ks*sce1_obs + (Bw+Bw*Kw-aL)*wce1 - beta*Kw*wce1_obs), log.p = TRUE)
  -sum(phi)
}

starting.pointmle2 &lt;- list(aL=1.4, beta=0.3, Ks=0.5, Kw=0.5, Bs=0.5, Bw=0.5)

result1 &lt;- mle2(llik.probit2, start = starting.pointmle2, data=list(dta=Mydata), skip.hessian=FALSE)
</code></pre>

<p>I need to estimate clustered standard  errors of this model, clustering on variable <em>c_id</em>.  I'm trying to implement the <em>sandwich</em> estimator but cannot retrieve 'meat' part from mle2 output. I have also tried with <em>nlm</em>, and <em>optim</em> (mle2 is basically wrapper for these methods)
Any advice how to do it? In general is there any package which provides functions to calculate clustered standard errors for MLE estimation of a general function?  </p>

<p>Here is is a small sample of the data file (MyData) in csv format:</p>

<pre><code>const,qualified,eds,edw,eds_obs,edw_obs,CombinedObservable1,c_id
1,0,0,1,0,0,-0.6838316166,1
1,1,0,1,0,0,-0.1433684328,1
1,0,0,1,0,0.0758113685,0.0758113685,1
1,0,1,0,0.2084778637,0,0.2084778637,34
1,0,0,1,0,0,-0.1622519262,34
1,0,0,1,0,0,-0.5061082675,34
1,0,0,1,0,0,-0.6952748613,34
1,0,1,0,0.9883178986,0,0.9883178986,34
1,0,0,1,0,0,-0.5311805315,34
1,0,0,1,0,0,2.7546881325,34
1,1,1,0,-0.2174263974,0,-0.2174263974,34
1,0,0,1,0,0,-0.4397037288,1
1,0,0,1,0,0,-0.3189328097,1
1,0,0,1,0,0,-0.6132276964,12
1,0,0,1,0,0,0.2459941348,12
1,0,0,1,0,0.0589196966,0.0589196966,12
</code></pre>
"
"0.125195771459034","0.0852492924338092","199821","<p>I am doing a project on the Gap Statistic from Tibshirani etc <a href=""http://www.web.stanford.edu/%7Ehastie/Papers/gap.pdf"" rel=""nofollow"">http://www.web.stanford.edu/%7Ehastie/Papers/gap.pdf</a></p>

<p>On page 4 of the pdf in section 4 on ""The computational implementation of the gap statistic"" two possible reference distributions to select Monte-Carlo samples from:</p>

<p>a) generate each reference feature uniformly overthe range of observed values for that feature</p>

<p>b) generate reference features from a uniform distribution over a box aligned with the principal components of the data. (The details involve assuming the columns have mean zero and computing the singular value decomposition, transforming the data, picking uniform random samples over the ranges of the columns in the transformed matrix, then back transforming the sample.)</p>

<p>The comments say the latter takes account of the shape of the distribution, and makes the procedure rotationally invariant, as long as the clustering method is invariant.</p>

<p>I can't tell from the help which method is used in the R function <code>clusGap</code> in the cluster library. I have tried looking at the code for the function (see below) and I think it doesn't do the singular value decomposition b) method - but just does the a) method. (I was looking at the part of the code following <code>for (b in 1:B)</code>). My question is am I right?</p>

<pre><code>&gt; clusGap
function (x, FUNcluster, K.max, B = 100, verbose = interactive(), 
    ...) 
{
    stopifnot(is.function(FUNcluster), length(dim(x)) == 2, K.max &gt;= 
        2, (n &lt;- nrow(x)) &gt;= 1, ncol(x) &gt;= 1)
    if (B != (B. &lt;- as.integer(B)) || (B &lt;- B.) &lt;= 0) 
        stop(""'B' has to be a positive integer"")
    if (is.data.frame(x)) 
        x &lt;- as.matrix(x)
    ii &lt;- seq_len(n)
    W.k &lt;- function(X, kk) {
        clus &lt;- if (kk &gt; 1) 
            FUNcluster(X, kk, ...)$cluster
            else rep.int(1L, nrow(X))
            0.5 * sum(vapply(split(ii, clus), function(I) {
                xs &lt;- X[I, , drop = FALSE]
                sum(dist(xs)/nrow(xs))
            }, 0))
        }
        logW &lt;- E.logW &lt;- SE.sim &lt;- numeric(K.max)
        if (verbose) 
            cat(""Clustering k = 1,2,..., K.max (= "", K.max, ""): .. "", 
                sep = """")
        for (k in 1:K.max) logW[k] &lt;- log(W.k(x, k))
        if (verbose) 
            cat(""done\n"")
        xs &lt;- scale(x, center = TRUE, scale = FALSE)
        m.x &lt;- rep(attr(xs, ""scaled:center""), each = n)
        V.sx &lt;- svd(xs)$v
    rng.x1 &lt;- apply(xs %*% V.sx, 2, range)
    logWks &lt;- matrix(0, B, K.max)
    if (verbose) 
        cat(""Bootstrapping, b = 1,2,..., B (= "", B, "")  [one \"".\"" per sample]:\n"", 
            sep = """")
    for (b in 1:B) {
        z1 &lt;- apply(rng.x1, 2, function(M, nn) runif(nn, min = M[1], 
            max = M[2]), nn = n)
        z &lt;- tcrossprod(z1, V.sx) + m.x
        for (k in 1:K.max) {
            logWks[b, k] &lt;- log(W.k(z, k))
        }
        if (verbose) 
            cat(""."", if (b%%50 == 0) 
                paste(b, ""\n""))
    }
    if (verbose &amp;&amp; (B%%50 != 0)) 
        cat("""", B, ""\n"")
    E.logW &lt;- colMeans(logWks)
    SE.sim &lt;- sqrt((1 + 1/B) * apply(logWks, 2, var))
    structure(class = ""clusGap"", list(Tab = cbind(logW, E.logW, 
        gap = E.logW - logW, SE.sim), n = n, B = B, FUNcluster = FUNcluster))
}
&lt;bytecode: 0x0000000010248578&gt;
&lt;environment: namespace:cluster&gt;
Warning messages:
1: In .HTMLsearch(query) : Unrecognized search field: title
2: In .HTMLsearch(query) : Unrecognized search field: keyword
3: In .HTMLsearch(query) : Unrecognized search field: alias
</code></pre>
"
"0.0791807582464896","0.107832773203438","200254","<p>I have a dataset of two columns (we can call them x and y).
I understand that for cross-validation I need to split my data into k partitions, and for that the general consensus is that I use createFolds. However, I am a little confused about the output. It seems like I can't use createFolds on my entire dataset, it seems like I have to specify something like <strong>createFolds(data$x, k=5)</strong>. Then I get an output of 5 folds with a row of numbers, and I haven't been able to find documentation on what the output means exactly. </p>

<p>My second question is how to apply cross-validation in R to my test dataset? As I understand it, in R all I need to do to perform k-means clustering on is to specify kmeans(data, centres,...). So if I apply that command to my held-out dataset, how can I use that for cross validation on my test dataset? Thank you! </p>
"
"0.0969762275752854","0.0880450906325624","200285","<p>I am hoping someone could offer me some wisdom and a few R tricks.</p>

<p>I was lucky enough to get a pretty interesting data set for a study that I am doing. There is approximately 5000+ rows of data spanning 5 years for 100 different sales people. The data is structured per below.</p>

<blockquote>
  <p>User || Month || NumberOfClientsSeen || AverageSatisfactionScore ||
  Sales || Leads</p>
</blockquote>

<p>I want to see if there are correlations between the NumberOfClientsSeen, AverageSatisfactionScore, Sales and/or Leads. My first inclination would be to load up a Matrix into R, and run RCORR from the Hmisc libary. </p>

<pre><code>&gt; library(Hmisc)
&gt; cordata &lt;- data[NumberOfClientsSeen, AverageSatisfactionScore, Sales, Leads]
&gt; results &lt;- rcorr(as.matrix(cordata), type=""pearson"")
## hourray for R, N and P-values! ##
</code></pre>

<p>I realize that this probably isn't correct. I know my users have a ton of variability and that there is seasonality in my data from month to month. The correlation values I'm seeing could be representations of my months or my weird users, since these groupings / clusterings aren't being taken into consideration in my correlation analysis; the way I am analyzing my data right now, I am effectively saying that all months are equal (which they aren't) and that all users are pretty much the same (which they aren't). I need to take users and months into consideration in my analysis. </p>

<p>My hypothesis / goals still relates to investigating interactions / correlations between the NumberOfClientsSeen, AverageScore, Sales and/or Leads. </p>

<p>Is there an way to do a correlation for my measures that take the clustering/variances of users / months into consideration? </p>

<p>Should I be adjusting my data before running rcorr, and removing monthly / user variances? </p>

<p>Perhaps I should be averaging my data somehow to group my data points?</p>

<p>Any insights are welcome, and appreciated.</p>

<p>Thanks!</p>

<ul>
<li>JSM</li>
</ul>
"
"0.0559892510955854","0.0381246425831512","200687","<p>I am using ClustOfVar package in R to cluster both nominal and interval variables for dimensionality reduction.</p>

<p>Can someone please explain what is squared loading values. how to pick representative from each cluster based on squared loading? I am working on an anonymised data without data dictionary.</p>

<p>Here is the sample data generated in R to test the clustering. My original data is much similar to this.</p>

<pre><code>m&lt;- matrix(sample(c(1:40),1000, replace = T),100,byrow=T)

df&lt;- as.data.frame(m)

df$V11&lt;- as.factor(sample(c('one','two','three'),50,replace =T))

x.quanti&lt;-df[,sapply(df,is.numeric)]

x.quali&lt;-df[,sapply(df,is.factor)]

tree&lt;- hclustvar(x.quanti,x.quali)

part_hier&lt;-cutreevar(tree,6)

part_hier$var
</code></pre>
"
"0.0685725481323742","0","203063","<p>I'm trying to perform a spatial clustering assignment by minimizing spatial distance while maximizing total weight within each cluster. </p>

<h3>My Data</h3>

<p>My data contains 3 columns and approximately 170 rows (example shown below)</p>

<pre><code>AvgWeight   lat          long
87.799      33.888102   -84.29321
165.258     31.459666   -83.51083
148.733     44.916657   -97.11346
484.038     43.020762   -88.26852
74.175      39.849156   -75.18159
83.861      42.02933    -93.60966
235.524     36.022863   -79.77895
</code></pre>

<h3>Goal</h3>

<p>I'd like to be able to cluster all of my spatial locations together while also maximizing the sum of my <code>AvgWeight</code> column within each cluster up to some adjustable constraint (Say 10,000 lbs). </p>

<h3>Additional Notes</h3>

<p>My previous clustering experience has been limited to <code>kmeans</code> in <code>R</code> so I was hoping that there would be some sort of variation I could implement to redefine my problem to fit with that methodology. However, I'm not sure how to go about minimizing geographic distance while maximizing my weight variable. I'm open to any suggestions on alternative methodologies preferably ones which can be implemented with <code>R</code>.</p>
"
"0.0395903791232448","0","203185","<p>I have used HDDC (high dimensional data clustering) from the HDClassify package in R where I have around 400+ variables and 350 obs and have obtained 6 clusters. I need to identify the most distinguishing features in each cluster. I have tried comparing the within cluster and in between cluster variance but due to large number of variables, it gets confusing. Is there an easier way to do that , say using Scree plots or anything else?</p>
"
"NaN","NaN","203793","<p>I am working out with some spatial data from Yelp dataset. I'd like to take a single city, plot all the restaurants and then check wheter there's some clustering that affects the grade that people give to a restaurant, i.e. people tend to value worse those restaurant in poor neighbourhoods.</p>

<p>However, at this point I am stucked as I don't know which steps to follow. I have been reading literature and Blogs but I haven't managed to figure out how to tackle the issue.</p>

<p>If you could just give me some ideas or basic points of how to solve it i'd really appreciate!! </p>

<p>Thank you very much!</p>
"
"0.0395903791232448","0.0539163866017192","204195","<p>Which clustering algorithm I should be using for the below type of dataset.
Say I have dataset with two variables (one is Age and another is say var). 
I converted them to the binary format as shown below. Kindly help me with the following questions</p>

<p>1.Should I use k-means directly on the data ?</p>

<p>2.Can I use hierarchical clustering ? if so which distance measuring algorithm</p>

<p>3.Or is there any better approach for this feature transformation and algorithm</p>

<pre><code>Age var
24  x1
54  x2
18  x1
45  x3
30  x2
</code></pre>

<p>I converted the categorical feature to this way</p>

<pre><code>Age var.x1 var.x2 var.x3
24  1       0      0
54  0       1      0
18  1       0      0
45  0       0      1
30  0       1      0
</code></pre>
"
"0.142745141943918","0.179444581366374","204760","<p>I would like to know how I can use clustering methods in R (in this case, Kmeans) if I have an ""unkind"" input matrix (I get this error log: </p>

<blockquote>
  <p>The TSS matrix is indefinite. There must be too many missing values. The index cannot be calculated.)</p>
</blockquote>

<p>I could see that I might get this error if my matrix produces negative eigenvalues (like, here: <a href=""http://stackoverflow.com/questions/20669596/nbclust-package-error"">http://stackoverflow.com/questions/20669596/nbclust-package-error</a>), but what I'm missing is the ""next step"" part. I could see a suggestion was to ""go back to the Data"", but what should I do then? Is there any transformation or something that might help? (I'm pretty new to R and clustering in general...)</p>

<p>The Data I'm using are the result of a survey (which I briefly transformed and scaled via the <code>scale</code> function in R) so I was wondering if there were some algorithms or methods I could use in order to go on with my analysis (from literature I couldn't find great help). Or, if you think this is unfixable or simply non the best solution, do you have any other suggestion for clustering my data? What I'm willing to do is to identify some clusters of possible users/customers of some services, depending on their usual habits (e.g.: if they use many social networks they will be more likely to use chat/whatsapp/app to ask for bank account information - I have both the information of their social network usage and their ways of communicating with a ""bank assistant"").</p>

<p>The Dataset consists of 994 rows and 103 columns. Don't know if it may help, but the code is simply this:</p>

<pre><code>Data2&lt;- read.csv(...)
bDataScale &lt;- scale(Data2)
nc &lt;- NbClust(bDataScale, min.nc=2, max.nc=993, method=""kmeans"")
</code></pre>

<p>And I get:</p>

<blockquote>
  <p>Error in NbClust(bDataScale, min.nc = 2, max.nc = 993, method = ""kmeans"") : 
    The TSS matrix is indefinite. There must be too many missing values. The index cannot be calculated.</p>
</blockquote>

<p>Thank you in advance for your help or any corrections,</p>

<p>Julia</p>

<p>P.S.: as it would be logical to expect, I get the same error also with the unscaled matrix.</p>
"
"0.104746297470869","0.101892393242403","204876","<p>This question related to this <a href=""http://stats.stackexchange.com/questions/204455/reducing-number-of-labels-in-a-dataset"">other one</a>, for which I have devised a strategy and now want some feedback on it.</p>

<p>My data consists of 434042 rows, each corresponding to an observation tagged with 1 of 20 labels. The variables for each observation are:</p>

<ul>
<li>5 are binary (T/F).</li>
<li>4 are categorical.</li>
<li>1 is ordinal (ranking).</li>
<li>3 is continuous.</li>
<li>1 is the time component.</li>
<li>1 label, corresponding to one of 20 each observation can take.</li>
</ul>

<p>My current logic is that to measure similarity, one must calculate the distance between observations, and then the distance between the center (or mean) of each category. Categories that are close together can be then grouped into a broader one.</p>

<p>To compute this, what I am doing is:</p>

<ul>
<li>Calculate the ratio of T/F observations within each label.</li>
<li>Calculate the % occurrence of each categorical value per categorical variable, for each label.</li>
<li>Treat the ordinal value as a continuous one.</li>
<li>Take the mean and standard deviation of each continuous variable.</li>
<li>Remove the time component from the analysis.</li>
</ul>

<p>I then pass on these label-specific summary variables to the <code>hclust</code>function in R, so the clustering algorithm can suggest me which categories I can group together.</p>

<p>Does this approach seem valid? Any recommendation or suggestion would be greatly appreciated.</p>
"
"0.0559892510955854","0.0762492851663023","205604","<p>I'm running a regression in R's <code>plm</code> package similar to this post <a href=""http://stackoverflow.com/questions/33155638/clustered-standard-errors-in-r-using-plm-with-fixed-effects"">Clustered standard errors in R using plm (with fixed effects)</a>. I.e. panel data with fixed effects and the within-model from <code>plm</code>.</p>

<p>My Question is the following: I'm trying to figure out how to cluster my standard errors according to a different variable than the variable called state from the dataset <code>Cigar</code>, which is seemingly automatically used by the <code>cluster = 'group'</code> option in <code>vcovHC</code>. Specifically, if I e.g. have a variable called <code>id</code>, how can I tell <code>vcovHC</code> to use it as my cluster?</p>

<p>A very related question is the process of how <code>vcocHC</code> is selecting the variable for clustering, is it always just the first column in the dataset?</p>
"
"NaN","NaN","205618","<p>I want to make sense out of my samples by visualizing my hierarchical clustering of a large dataset into a dendrogram in R. However as I have over 450k samples, I cannot make any sense out of the dendrogram. Is there a way to simplify the visualization of the tree?</p>
"
"0.104746297470869","0.122270871890883","207404","<p>First of all, I know that this question has been addressed a certain number of times, but I didn't find an answer concerning the <strong>clustering of variables</strong>, instead of observations.</p>

<p>Concretely, I am using the function <code>varclus</code> from the package <code>Hmisc</code> to perform variables clustering.</p>

<p>As an example, I want to perform a cluster analysis on the variables of the dataset <code>ionosphere</code> (available in the package <code>dprep</code>).</p>

<p>My code is as follow :</p>

<pre><code>&gt; library(Hmisc) 
&gt; library(dprep) 
&gt; data(ionosphere) 
&gt; iono_min_l_col &lt;- ionosphere[-length(ionosphere)] 
&gt; iono_mx_min_l_col &lt;- data.matrix(iono_min_l_col) 
&gt; iono_clus &lt;- varclus(iono_mx_min_l_col)
&gt; plot(iono_clus)
</code></pre>

<p>When using <code>plot()</code> I get the following dendrogram :</p>

<p><a href=""http://i.stack.imgur.com/LSDiI.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/LSDiI.png"" alt=""enter image description here""></a></p>

<p>However, I don't know in how many clusters I should group my variables.
I can used the following code to know in which cluster are my variables (e.g. V1 Cluster 1, V2 Cluster 3, etc...), but I don't know <strong>how to get the optimal number of clusters</strong> ? (i.e. <strong><em>k</em></strong> in the following code)</p>

<pre><code>&gt; groups &lt;- cutree(varclus(iono_mx_min_l_col)$hclust, k= ***???***)
</code></pre>

<p>Does someone know how to get this optimal number, <strong><em>k</em></strong> ?</p>

<p>Thanks a lot !</p>
"
"0.0791807582464896","0.0539163866017192","208235","<p>I have 3 sets of data that will cluster into 3 distinct groups. Each group is unbalanced meaning that there are different number of points in each cluster (cluster1 = 300, cluster2=50, cluster3=900). I'd like to know the best way to determine how and where to add a fourth cluster to these groups (i.e. which group can best accommodate for another cluster?). </p>

<p>What I've done to this point is preformed k-means clustering on each group. I've also computed the sum of squared error (SSE) for each group for k=2..15:</p>

<pre><code>wss = (nrow(data)-1)*sum(apply(data,2,var))
for (i in 2:15) wss[i] = sum(kmeans(data,centers=i)$withinss)
</code></pre>

<p>Now I have a table that looks something like this: </p>

<pre><code>""m""                ""d""                  ""e""
2813569.28725861    472355.029297394    5784658.19776383
1904107.38583311    318157.708967953    3506296.79084521
1648681.74129464    276421.573303097    2925602.69727677
1524909.05220068    259298.616113487    2685211.28641095
1465477.09738752    249822.780393088    2568056.73068996
1436268.67341786    244545.97418366 2492074.85372952
1418250.72662858    241004.184066676    2446687.20606313
1405885.99063985    237817.978851278    2411366.20542939
1395267.90044412    235054.310901762    2398095.04077986
1386557.42023853    232570.376337123    2374974.99416473
1379531.10300039    231250.918255804    2371436.20025802
1366720.2571021 229001.055906548    2352940.58741131
1360624.70769575    227234.173112882    2339408.89209271
1350787.58878321    226286.438463845    2333231.7286477
1345737.53064247    224540.981656591    2325617.52627248
</code></pre>

<p>Each line corresponds to the SSE for a particular k (row) in a particular group (column). I'm under the assumption that it would be incorrect to simply compare the row for each k and take the maximum value and use that corresponding group to add the cluster to. Am I correct in thinking this? Is there a better way to go about this? </p>

<p>Clarification Edit:
The three clusters in the table provided above (m, d, e) are the three distinct, initial datasets. By themselves they form 3 distinct cluster. Now, say I'd like to split one of these clusters into 2. Which do I choose? How do I determine which one to split?</p>
"
"0.0395903791232448","0.0539163866017192","208432","<p>If i have a classification or clustering model built for retail customers till last year.How do i check if my model is still valid to this year's data?</p>

<p>We can check accuracy of model to new data but what are the other methods to check?</p>
"
"0.0559892510955854","0.0381246425831512","208570","<p>I have performed differential analysis with DESeq2, edgeR and voom-limma. My goal was to evaluate each methods on my data.
Now I would like to perform clustering with the genes found differentially expressed (DEG) by each method to see how well these genes discriminat my two conditions. To do so, I have to shoot my counts through some type of variance stabilizing transformation</p>

<p>Should I do all my clusterings with rlog data and the 3 sets of DEG ?</p>

<p>I wonder if it is relevant to do a clustering with DEG from voom on data that have been transform with rlog or with DEG from DESeq2 on data that have been transform by voom ?</p>

<p>Any clues ?
Thanks</p>
"
"NaN","NaN","208621","<p>I am trying to perform a clustering analysis in R according to Spearman correlation coefficient.</p>

<p>Could it happen that the analysis identifies only one big cluster as in the figure attached (the code is below the figure)? How would you interpret that? Please not that the same code generates a more meaningful dendogram if I change the matrix I use (in this case <code>mat_settings_old</code>).</p>

<p><a href=""http://i.stack.imgur.com/XIisq.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/XIisq.png"" alt=""enter image description here""></a></p>
"
"0.0559892510955854","0.0381246425831512","209364","<p>I'm working on segmentation/clustering and trying to use Gaussian Mixture Modelling for Model-Based Clustering. I'm using the R package Mclust in order to come up with the best fit for my data.</p>

<p>All data is transformed to a uniform distribution with mean zero, standard deviation one (I know, not Gaussian) and the variables included are chosen based on earlier attempts using k-means, where the given variables seemed to be discriminating. Of course, k-means comes with some drawbacks (lack of statistical foundation, no control of cross-correlation etc,), and that's the reason I want to use Model-Based Clustering (or latent class analysis, with the package poLCA).</p>

<p>When using mclustBIC, many of the possible BICs are actually NA. I tried to reduce the dimension of the data, but this didn't improve the output. For example the VEV is only calculated for nr clusters 1:3, while it looks like it could improve for more clusters (see plot below).</p>

<p>Someone who experienced similar problems? And can someone help me into the right direction for finding the best model, using mclust? I would like to calculate other BICs with a higher number of clusters.</p>

<p>Help would be appreciated!</p>

<p><a href=""http://i.stack.imgur.com/YboGv.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YboGv.png"" alt=""enter image description here""></a></p>
"
"0.0885267789745639","0.0723364233255618","209446","<p>I have a very large data set with 9000 observations and 25 categorical variables, which I've transformed into binary data and preformed hierarchical clustering and K-modes clustering in R. </p>

<pre><code>library(klaR)
cluster &lt;- list()
for(k in 1:8) 
{
cluster[[paste0(""k."", k)]] &lt;- kmodes(data, k,iter.max=100)
}
</code></pre>

<p>I would like to know </p>

<p>1) if it's better to specify the number of modes <code>k</code> (where the algorithm chooses a random set of distinct rows from the data as the initial modes) or to specify the initial starting values/modes myself (give it a set of initial distinct cluster modes in replace of <code>k</code>). If the later, how do you decide on meaningful initial modes? For example for <code>k=4</code>, can I specify the initial modes to be 4 rows from the hierarchical binary clustering output where I cut the tree at <code>k=4</code>?</p>

<p>2) how many times I should run the algorithm and </p>

<p>3) if 100 iterations is adequate.</p>
"
"0.125195771459034","0.136398867894095","211102","<p>I have two data sets which contains information about subsystems in a bacterial metabolic model<br>
DataSet1: Behavior data of the subsystems<br>
DataSet2: Structural data of the same subsystems<br>
Then perform hierarchical clustering on the two data sets and obtain two dendrograms. My goal is to find how similar these dendrograms are.
I used FM index as suggested in <a href=""https://cran.r-project.org/web/packages/dendextend/vignettes/introduction.html"" rel=""nofollow"">this article</a> as it gives some sort of confidence interval on the final answer.</p>

<p>This is the code I used:</p>

<pre><code>#Hierarchical clustering for dataset 1
dInit &lt;- dist(initDataSource)
hcInit = hclust(dInit)

#Hierarchical clustering for dataset 2
dFinal &lt;- dist(finalDataSource)
hcFinal = hclust(dFinal)

#calculating the FM index
FM_index(cutree(hcInit, k=2), cutree(hcFinal , k=3))
</code></pre>

<p>Output:</p>

<pre><code>[1] 0.7462025
attr(,""E_FM"")
[1] 0.6253888
attr(,""V_FM"")
[1] 0.007626263
</code></pre>

<p>I need some help interpreting the output of FM index function. As stated in <a href=""https://cran.r-project.org/web/packages/dendextend/vignettes/introduction.html"" rel=""nofollow"">this article</a> there are three values in the output; FM index, expected FM index and the variance.  </p>

<p><strong>Question:</strong>
Is the variance value some sort of a p-value in which we could accept the or reject the H0 (null hypothesis -  the two trees are not similar). For example in the below scenario as variance is 0.007 (which is less than 0.01) we could reject H0 and the two dendrograms are similar by a value of 0.7462. </p>

<p>In a case where we get a very high FM index but a fairly high variance does that mean we are unable to reject the H0 and hence the dendrograms are not similar? </p>

<pre><code>&gt; FM_index(cutree(hcInit, k=3), cutree(hcFinal, k=3))
[1] 0.8461141
attr(,""E_FM"")
[1] 0.5515411
attr(,""V_FM"")
[1] 0.01347924
</code></pre>
"
"0.0885267789745639","0.0964485644340824","211661","<p>I have a matrix of pearson correlations that I would like to cluster on similarity and identify correlated networks. However the variables are part of groups and I'm not interested correlations within groups; but between groups. </p>

<p>I hypothesize that there shouldn't be any correlation within groups (or at least random) and that there is a cluster of variables among the groups  (hope I'm clear). Specifically I would like to identify a cluster of correlated variables that consists of  one variable from group1, one of group2, one of group3, etc. </p>

<p>Are there clustering methods that can take into account leveled data (preferably in R or Python), would it be possible?</p>

<pre><code>Group   Var A   B   C   D   E   F   â€¦
1   A   1.00    -0.85   -0.85   -0.78   0.68    -0.87   
1   B   0.20    1.00    -0.10   0.20    -0.40   0.78    
1   C   0.10    0.30    1.00    0.40    -0.71   0.89    
2   D   0.30    0.10    0.79    1.00    -0.45   0.66    
2   E   0.90    -0.70   -0.71   -0.45   1.00    -0.71   
2   F   0.40    0.78    0.89    0.66    -0.71   1.00    
4   G   0.20    -0.59   -0.43   -0.71   0.09    -0.17   
4   H   0.40    -0.81   -0.71   -0.72   0.44    -0.55   
4   I   0.90    -0.52   -0.59   -0.24   0.71    -0.69   
5   J   0.48    -0.49   -0.56   -0.13   0.80    -0.58   
</code></pre>
"
"0.104746297470869","0.0815139145939222","212293","<p>I have seen other users ask about recreating SAS's CCC output in other programs. This question, <a href=""http://stats.stackexchange.com/questions/29114/cubic-clustering-criterion-in-r"">Cubic clustering criterion in R</a>, has an answer that says to use <code>NbClust</code> to calculate, but that function does not handle large datasets well. It makes a call to <code>dist</code> that must allocate a 50 gig object. I have tried replacing the function with <code>cluster::daisy</code>, and <code>proxy::dist</code> from this SO question with the same memory problems.</p>

<p>Avoiding the <code>dist</code> call altogether may be the best option. I am looking to other options to recreate it. In this question <a href=""http://stats.stackexchange.com/questions/9016/how-to-define-number-of-clusters-in-k-means-clustering/9019#9019"">How to define number of clusters in K-means clustering?</a>, a user goes through the math provided by SAS. But I do not have the stats chops to translate that into R code. </p>

<p>Keeping it simple, I have <code>kmeans</code> output that provides total sum of squares (tot.ss), within.ss, between.ss, and I also calculated the $R^2$. </p>

<pre><code>kmeans(x = mydata, centers = 23, iter.max = ITER)
Within cluster sum of squares by cluster:
 [1]  91248.77  72122.06  78680.32  90402.25  86341.35 153533.51  73988.63  64903.32
 [9]  38334.98  84125.14  92366.93  74721.24 110313.76  96859.55  84516.37  56068.08
[17]  76201.69  86194.35  59526.00  53709.75  72503.21  50767.36  80531.94
 (between_SS / total_SS =  36.5 %)

Available components:

[1] ""cluster""      ""centers""      ""totss""        ""withinss""     ""tot.withinss"" ""betweenss""   
[7] ""size""         ""iter""         ""ifault""
</code></pre>

<p><strong><em>Can I calculate the CCC using these measures?</em></strong></p>

<hr>

<p>The second question has a long description from the SAS pdf. But I saw a <a href=""http://www.palgrave-journals.com/jibs/journal/v37/n4/fig_tab/8400206t2.html"" rel=""nofollow"">simplified equation here</a>.</p>

<p><a href=""http://i.stack.imgur.com/wKiHu.gif"" rel=""nofollow""><img src=""http://i.stack.imgur.com/wKiHu.gif"" alt=""enter image description here""></a> </p>

<p>where $E(R^2)$ is the expected $R^2$, and $R^2$ is the observed $R^2$, and $K$ is the variance-stabilizing transformation.</p>

<p>*Can this equation be completed by R's <code>kmeans</code> output and a calculated $R^2$</p>

<p><strong><em>Edit</em></strong></p>

<p>One reason why I am focusing on <code>kmeans</code> is that SAS users utilize <code>PROC FASTCLUS</code> when running large datasets. It is equivalent to R's kmeans function. The package <code>NbClust</code> calculates the CCC that I'm looking for, but it does it on the full data with euclidean distance, which is impossible for most computers. That is equivalent to SAS's <code>PROC CLUSTER</code>.</p>
"
"0.0685725481323742","0.0311286403182345","213093","<p>I'm interested in generating correlation network maps, and have come across qgraph in R. While the program is straightforward to use with cross-sectional data (and no clustering), I was wondering if there is a way to implement in a multilevel context to derive a network map for the sample as a whole, while controlling for non-independence? Alternatively, is it possible to extract key stats (e.g., closeness, betweenness, strength, and expected force) on an individual-by-individual (or cluster by cluster) basis, weight these values, and then produce a network map from this?</p>

<p>I should also point out that I am a novice with respect to R.</p>

<p>Thanks in advance.</p>
"
"0.0685725481323742","0.0311286403182345","213313","<p>How can I calculate the probability of membership with R's <code>kmeans</code> output?</p>

<p>The output of <code>kmeans</code> is as follows:</p>

<pre><code>k &lt;- kmeans(iris[-5], 3)
str(k)
# List of 9
# $ cluster     : int [1:150] 2 3 3 3 2 2 2 2 3 3 ...
# $ centers     : num [1:3, 1:4] 6.31 5.18 4.74 2.9 3.62 ...
# ..- attr(*, ""dimnames"")=List of 2
# .. ..$ : chr [1:3] ""1"" ""2"" ""3""
# .. ..$ : chr [1:4] ""Sepal.Length"" ""Sepal.Width"" ""Petal.Length"" ""Petal.Width""
# $ totss       : num 681
# $ withinss    : num [1:3] 118.65 6.43 17.67
# $ tot.withinss: num 143
# $ betweenss   : num 539
# $ size        : int [1:3] 96 33 21
# $ iter        : int 2
# $ ifault      : int 0
# - attr(*, ""class"")= chr ""kmeans""
</code></pre>

<p>Is this enough information to copy what we get from <code>Mclust</code>?</p>

<pre><code>library(mclust)
m &lt;- Mclust(iris[-5])
head(m$z)
#           [,1]         [,2]
# [1,] 1.0000000 2.513157e-11
# [2,] 0.9999999 5.556411e-08
# [3,] 1.0000000 3.635438e-09
# [4,] 0.9999999 8.611811e-08
# [5,] 1.0000000 8.504494e-12
# [6,] 1.0000000 1.400364e-12
</code></pre>

<p>Obvious question is ""Why not use <code>mclust</code>?"". My data is too large to computationally do hierarchical clustering. I have tried with <code>Mclust</code>, <code>NbClust</code>, <code>vegan</code>, and many others. The call to <code>dist</code> that all of the functions use max out after a few hundred thousand rows.</p>

<p>I have seen some talk about probabilistic-D, and ""soft"" clustering, but I do not know how to implement it without changing the output of the original clusters from <code>kmeans</code>.</p>

<p><strong>Edit</strong>
I know that SAS is able to export probabilities with <code>PROC FASTCLUS</code>, but from what I hear it is taking a sample of the data to get the probabilities. That might be one route to take if I could figure out how it's doing the subsetting.</p>
"
"0.125195771459034","0.136398867894095","215961","<p>I have a set of distributions corresponding to predictions for how each of hundreds of players will perform.  I am looking to identify the distinct distributions of players.  In other words, I'm looking to identify the <a href=""https://stats.stackexchange.com/questions/57921/how-to-prove-the-number-of-distinct-distributions-in-a-group-of-distributions"">distinct distributions in a group of distributions</a>.</p>

<p>I know <code>Mclust()</code> can perform clustering on a vector, e.g.:</p>

<pre><code>library(""mclust"")

mydata &lt;- c(1,1,2,2,3,3,5,7,8,9,10)

summary(Mclust(mydata), parameters=TRUE)
Mclust(mydata)$classification
</code></pre>

<p>However, my data are a series of vectors (i.e., distributions)---one vector for each player, e.g.:</p>

<pre><code>set.seed(12345)
playerA &lt;- rnorm(10, mean=1, sd=.1)
playerB &lt;- rnorm(100, mean=1, sd=1)
playerC &lt;- rnorm(10, mean=2, sd=1)
playerD &lt;- rnorm(5, mean=2, sd=2)
playerE &lt;- rnorm(2, mean=3, sd=1)
playerF &lt;- rnorm(20, mean=5, sd=1)
playerG &lt;- rnorm(100, mean=7, sd=.5)
playerH &lt;- rnorm(10, mean=8, sd=2)
playerI &lt;- rnorm(5, mean=9, sd=1)
playerJ &lt;- rnorm(10, mean=10, sd=.5)
</code></pre>

<p>How can I perform clustering to identify the distinct clusters of players based on their distributions, focusing on differences in their means, rather than their variances.  I don't want to just cluster the mean values, though, because I want to take into account the variances to know whether their means are in the same or in a different cluster (e.g., high variability in two players' distributions may indicate that two players with different means are in the same cluster).  Ideally, I'd like two players with the same mean and different variability distributions to be in the same cluster.  Is there a way to do this using the <code>mclust</code> or another package in R?  I've considered doing pairwise t-tests, but this seems that it would be heavily dependent on the sample size in each distribution (which I'd rather it not be <em>too</em> dependent on sample size, if possible).  I've also considered comparisons based on effect size (Cohen's d).  I'm not sure what other options there are (e.g., Tukey's HSD, hierarchical clustering, etc.)</p>
"
"0.0559892510955854","0.0381246425831512","216046","<p>I am trying to find a clustering solution with the help of <code>flexclust</code> package in R. The following code has been adapted from the vignette for the <code>flexclust</code> package:</p>

<pre><code>library(flexclust)
library(ISLR)

Auto &lt;- read.table(Auto)
AutoMinus &lt;- Auto[ -c(8:9)]
AutoMinus.mat &lt;- as.matrix(AutoMinus)

# Setting the parameters

fc_cont &lt;- new(""flexclustControl"")  
fc_cont@tolerance &lt;- 0.01   
fc_cont@iter.max &lt;- 25
fc_cont@verbose &lt;- 1
fc_family &lt;- ""kmeans""             

seed1 &lt;- 12345
fc_seed &lt;- seed1
num_clusters &lt;- 3
set.seed(fc_seed)

AutoMinus.cl &lt;- kcca(AutoMinus.mat, k = num_clusters, save.data = TRUE, control = fc_cont, family = kccaFamily(fc_family))
summary(AutoMinus.cl)

cluster info:
  size  av_dist max_dist separation
1  122 263.0698 525.3528   480.3347
2  180 217.1523 610.9503   478.7658
3   90 290.9777 905.5731   551.2422
</code></pre>

<p>Every time I change the seed, the output changes. I am evaluating different outputs based on lowest <code>av_dist</code>, lowest <code>max_dist</code> and minimum <code>separation</code>. My understanding is <code>separation</code> is within a given cluster. I tried to find the definition of <code>separation</code> in the documentation, but couldn't find it. My questions are:</p>

<ol>
<li>How do I set the seed that will give me the best/optimal(?) solution?</li>
<li>Are there any general good practices for initializing a seed value?</li>
<li>Is my understanding of <code>separation</code> correct?</li>
</ol>

<p>Thank you!</p>
"
"0.125195771459034","0.102299150920571","217467","<p>I'm working on a project that requires some clustering analysis.  In performing the analysis, I noticed something that seemed odd to me.  I understand that in k-means the total sum of squares (total distance of all observations from the global center) equals the between sum of squares (distance between the centroids) plus the total within sum of squares (sum of the distances of each observation to its centroid).  But I also see that total sum of squares is not exactly equal to the total variance of the distribution, which I don't understand.  What I've noticed is that the two numbers (total sum of squares from k-means analysis vs. total variance) get closer to each other as the sample sizes get larger.  Here's a quick simulation in R that shows what I'm talking about:</p>

<pre><code>require(dplyr)
compare_sd_km &lt;- function(numObs){
    set.seed(3)
    x &lt;- rnorm(numObs, 0, 1)
    y &lt;- rnorm(numObs, 0, 1)

    km &lt;- kmeans(data.frame(x,y), centers = 5)

    kmeansMeasure &lt;- (km$betweenss + km$tot.withinss) / numObs
    varianceMeasure &lt;- var(x) + var(y)
    return(c(kmeansMeasure, varianceMeasure))
}

numObsMultipliers &lt;- c(1:5)
numObsMultipliers &lt;- sapply(numObsMultipliers, function(x) 10^x)
comparisons &lt;- lapply(numObsMultipliers, function(x)  compare_sd_km(x))
comparisons &lt;- as.data.frame(do.call(rbind, comparisons))
comparisons$observations &lt;- numObsMultipliers
colnames(comparisons) &lt;- c('kmeansMeasure', 'varianceMeasure', 'observations')
comparisons &lt;- mutate(comparisons, difference = kmeansMeasure - varianceMeasure)
comparisons &lt;- comparisons[, c(3,1,2,4)]

#### RESULT ####
  observations kmeansMeasure varianceMeasure     difference
1           10      1.142764        1.269738 -0.12697379740
2          100      1.920365        1.939762 -0.01939762318
3         1000      1.988562        1.990553 -0.00199055288
4        10000      2.031035        2.031238 -0.00020312381
5       100000      2.007437        2.007457 -0.00002007457
</code></pre>

<p>Any ideas what's going on here?  Rounding issues?  Something having to do with how the algorithms are implemented?  Am I calculating something incorrectly? Or do total sum of squares and total variance actually mean something substantively different?  Thanks for any help anyone can provide.</p>
"
"0.0885267789745639","0.0482242822170412","218104","<p>I calculated the Cosine distances for binary data and got the relations between different variables.</p>

<p>I need to cluster them. I tried passing the cosine matrix directly to the (clustering) function but got an error. So, I tried passing the cosine matrix as a parameter to the <code>dist</code> function (<code>dist</code> computes Euclidean distances between all rows in a matrix.) </p>

<pre><code>d &lt;- dist(cosine(matrix1))
</code></pre>

<p>and then plotted the result with <code>hc&lt;- hclust(d)</code>. I am getting good output but want to check whether I am doing anything wrong.</p>

<p>Does it makes sense to pass cosine distances to the <code>dist</code> function? </p>
"
"0.131306432859723","0.130051217752471","218253","<p>I am working on a project where I am given a large table of numbers, in which we are hoping to see certain patterns. For example (using <code>R</code>):</p>

<pre><code>set.seed(77)
mat &lt;- matrix(rnorm(100), 10, 10)
colnames(mat) &lt;- letters[1:10]
rownames(mat) &lt;- 1:10
round(mat, 3)

        a      b      c      d      e      f      g      h      i      j
1  -0.550 -2.941 -0.254 -2.362  0.003 -1.305  0.399 -1.638  0.751  0.877
2   1.091 -0.243  1.519 -0.551 -0.531  0.887  0.027 -0.332 -0.067  0.835
3   0.640 -0.141  1.781 -0.305 -0.710  2.336 -1.001  0.448 -0.504 -0.048
4   1.043 -0.033 -0.879 -0.750 -0.291  0.503  0.009  0.272 -0.160 -3.410
5   0.170  0.280 -1.529  0.144  0.885 -2.268 -0.164 -0.254 -0.093 -1.513
6   1.138  0.590  0.136 -0.549 -0.154 -2.032  0.423  2.348  0.474  0.252
7  -0.971  1.024 -0.709  0.160 -0.954 -0.138 -0.424 -0.213  0.131 -0.473
8  -0.132  2.107 -1.410 -0.088  0.667 -0.953 -0.470  0.051  0.717  0.977
9   0.146  0.155  1.831  0.081  0.388  1.578  0.172 -2.246 -0.003  2.435
10  1.441  0.913  1.290  0.899  0.549 -1.248  1.847  0.920 -2.177 -0.082
</code></pre>

<p>My goal is to be able to sort arbitrary sized matrices (not necessarily square) by switching rows and columns to minimize cell-wise differences. For example, all the values on line 1 should stay on line 1, but perhaps it makes more sense distance-wise for line 1 to appear on line 8, and vice versa regarding the columns.  This problem reminds me of finding the inverse of a matrix using linear algebra.</p>

<p>Note: I realize there is not a unique solution to this problem, so resampling is OK (ideally, rerunning the function will produce different clusterings). As a starting point, optimally larger values would float to the top while smaller values would tend toward the bottom.  </p>

<p>In the above example, perhaps the largest numbers are all in row 9 and so one move might be to bring it to the top. Similarly, perhaps the largest numbers are found in column c and that is moved to the left. Basically, I want to reorder the rows and columns of the table so that the magnitude of the difference between each entry (e.g. a1 vs a2/b1; c2 vs. the four surrounding entries) is minimized - but with the restriction that the operations happen row- and column-wise.</p>

<p>I am primarily interested in a theoretical approach that will show me how to accomplish this, but will eventually be implementing the solution in <code>R</code>, so assistance on that front would also be appreciated.</p>

<p>To make this more concrete: this idea is to be applied to the results from large-scale simulation studies, where the entries might be Type I error or power rates, the rows might pertain to a design condition (like sample size) and the columns might pertain to different multiple comparison procedures or something like that. The goal is to rearrange the entries of this table so that we might be able to glance at the table and see where clusters of ""good"" procedures are and where the ""bad"" ones are.</p>
"
"0.0685725481323742","0.062257280636469","218301","<p>I have a matrix of pairwise Gower dissimilarities (i.e. a non-Euclidean metric) among 48 objects derived from observations of 54 variables.</p>

<p>I want to use fuzzy clustering to obtain a matrix of group membership probabilities for these objects.</p>

<p>I successfully applied function <code>fanny()</code> from package {cluster} with k (number of groups) = 4.</p>

<p>However, for each sample, the resulting group membership probabilities for groups 1, 2 and 4 are identical. That is, they differ <em>between samples</em>, but <em>within a sample</em> they are identical, and differ from the membership probability for group 3.</p>

<p>I tested the function with k of 5, 6 and 7 and obtained the same result. For each k, the group membership probabilities were identical (within an object), except for group 3.</p>

<p>Does anyone know why this might be happening? I've read up on the fanny() algorithm a little but I'm out of my depth.</p>

<p>Reproducible code (NB: actual data follow due to size):</p>

<pre><code>install.packages(""cluster"")
library(""cluster"")
WRS_fanny &lt;- fanny(WRS_abiotic_gower, k = 4, memb.exp = 1.6)
boxplot(WRS_fanny$membership) # Note identical probabilities except group 3
</code></pre>

<p>Used dump() to generate this expression to recreate my data (warning: 294 lines):</p>

<pre><code>WRS_abiotic_gower &lt;-
structure(c(0.146778126529408, 0.233395130892759, 0.230707834408444, 
0.107759542541761, 0.138550796015413, 0.250065889538291, 0.296457361775224, 
0.304125211829354, 0.249782393842249, 0.282431780327495, 0.25970664977036, 
0.251596311648883, 0.224076607905674, 0.251036300151136, 0.196484552504834, 
0.271536343873702, 0.265568257890046, 0.301386804224653, 0.333063701373033, 
0.243600972251144, 0.270222184812288, 0.202047498980117, 0.25607063775203, 
0.28432667481917, 0.24092419965811, 0.275398059343259, 0.353270223771568, 
0.410364714292939, 0.313001378801364, 0.31925605598341, 0.357701120460457, 
0.374474795997226, 0.189102942457199, 0.202055462100059, 0.247583708657342, 
0.30145619413158, 0.296669857875146, 0.339864474596941, 0.229211008702165, 
0.352892440280298, 0.419167648505487, 0.375568557956858, 0.408480716527057, 
0.357157487108125, 0.32104738586095, 0.340665462218034, 0.41112214935789, 
0.23830604032674, 0.220478976300685, 0.144406224105715, 0.131515885009171, 
0.216908812847801, 0.243564070814677, 0.303148174678537, 0.259847622410418, 
0.247305687153991, 0.247843057599577, 0.26631554306528, 0.199890462938375, 
0.28044167137188, 0.236573345659676, 0.264633258654337, 0.250360278612784, 
0.282905316954283, 0.28148360026523, 0.172675632607707, 0.227157538886476, 
0.236004956068931, 0.275979625457648, 0.277843987366973, 0.262417869004574, 
0.281232496061673, 0.329892724118953, 0.394824895894531, 0.30265363625644, 
0.30621966032146, 0.34479178515701, 0.374683684901749, 0.21424820401553, 
0.195572916452556, 0.241393648501288, 0.316572600921197, 0.287133512655544, 
0.328215112402773, 0.193182214935908, 0.310935856747777, 0.373166961778132, 
0.332783318074841, 0.372234839389727, 0.330623742512386, 0.292078433204333, 
0.319545503079129, 0.361321516105163, 0.113028782648282, 0.206941169638988, 
0.20512668072602, 0.24722346523851, 0.308687244396935, 0.204828946418389, 
0.178919076974325, 0.183060785216104, 0.141776960257189, 0.134674581851856, 
0.25068097397968, 0.123958293735287, 0.141125483626521, 0.207978432341651, 
0.164949716625781, 0.190523417026307, 0.309000477229615, 0.277339046266308, 
0.313375054970243, 0.146793857852403, 0.137385394610465, 0.167578392653176, 
0.16804367314817, 0.171809114878496, 0.381176414723976, 0.457052466454844, 
0.282197699921021, 0.280739699172475, 0.302250296861788, 0.304432773653582, 
0.15957945901212, 0.235669994183549, 0.178160953420343, 0.282371461962546, 
0.257100104835312, 0.482706722426562, 0.369944192447982, 0.309346584211358, 
0.397210372391609, 0.380583112959886, 0.407248184382802, 0.346695417169584, 
0.307512606916341, 0.269834685656257, 0.409392428767859, 0.217511509030952, 
0.197481259895392, 0.275939181123016, 0.341312211621435, 0.228966719294035, 
0.188816895891077, 0.212202206485866, 0.194008885613182, 0.166989986763656, 
0.302101187053358, 0.162535754002973, 0.189263438443349, 0.23307417010102, 
0.199298488543463, 0.229916212413065, 0.31125875417897, 0.239297777451497, 
0.305776102461332, 0.173662348649529, 0.190044248152056, 0.206426618003735, 
0.227187931768048, 0.214824072433246, 0.4073186103118, 0.462804478914285, 
0.323506990767481, 0.317824328727341, 0.335245136426698, 0.327528602013808, 
0.185454797734023, 0.208467311764833, 0.135322000283485, 0.288836940944495, 
0.230306186711877, 0.468573333942388, 0.33313401147229, 0.321113074211735, 
0.391514664657562, 0.389710934558845, 0.400420981243808, 0.354101457148739, 
0.320629082713082, 0.286100626725499, 0.421671705461507, 0.091850113801114, 
0.246467223254679, 0.293547268483122, 0.297659239034206, 0.25115255052786, 
0.262514071750454, 0.238133038325391, 0.212967201979357, 0.17308253395046, 
0.2183920945381, 0.176101353198788, 0.261139515088514, 0.25711901874235, 
0.272641088130901, 0.330933912386848, 0.250239547033252, 0.236589210839988, 
0.172880533811394, 0.250922420164451, 0.268025493833902, 0.23575916232682, 
0.267647109568082, 0.373655060045483, 0.433323699677838, 0.31391657516819, 
0.31852460096763, 0.351258764865077, 0.367159819051078, 0.16890813868034, 
0.185782013611323, 0.226257665322096, 0.24834469424724, 0.272259607740838, 
0.354585293042834, 0.229701146851508, 0.362102176288281, 0.400717876478831, 
0.36797070590943, 0.409120131929636, 0.34498319198954, 0.313386283542958, 
0.331016784762028, 0.384254565970275, 0.22441721238447, 0.290598912970728, 
0.298643689422108, 0.250361457068466, 0.244928708085188, 0.237713058207576, 
0.222774529861816, 0.210265153710359, 0.235732385132754, 0.20156761561874, 
0.262015784293767, 0.259883660151181, 0.288278458785541, 0.333285662718476, 
0.232716863284767, 0.254484337442487, 0.187205171211865, 0.243100325138505, 
0.267048020813995, 0.242204808310273, 0.261920262726918, 0.380811106291807, 
0.446215322818477, 0.316835287136048, 0.321422073497405, 0.348976941617111, 
0.382762337202703, 0.182491613841568, 0.215977568407392, 0.226450258451914, 
0.283179718136393, 0.274849250694097, 0.369155318338422, 0.220594923901695, 
0.355713153122709, 0.412492946083189, 0.382970482523399, 0.431482115624372, 
0.34255354874582, 0.347795938665851, 0.34955910382006, 0.420245136110264, 
0.157812677685306, 0.272010868942531, 0.229427228507228, 0.145577425709495, 
0.263408013897356, 0.321273834678714, 0.253868813151803, 0.303315771029787, 
0.250638531578023, 0.202686048164595, 0.23542311767865, 0.262524337712699, 
0.213762545265516, 0.198592502804593, 0.289755608760079, 0.291055349247771, 
0.21471221044029, 0.233375528166048, 0.214477494584816, 0.227000414408994, 
0.253378560228127, 0.327233596552946, 0.256069951388544, 0.241731882033173, 
0.239086155905498, 0.301355179254736, 0.268623586092357, 0.216004437464011, 
0.278442814851357, 0.351355765511319, 0.306053836025685, 0.385664345928335, 
0.325740726127523, 0.224406635623224, 0.272261471513539, 0.269731865914167, 
0.301560017458275, 0.225721846096926, 0.229981204614567, 0.242518355133113, 
0.328463376740603, 0.329111361926846, 0.264116154396599, 0.217010031466845, 
0.313943303264908, 0.363879592011647, 0.257108399675198, 0.358515803949734, 
0.321567175057474, 0.296784046059782, 0.301131134388745, 0.331842299269796, 
0.253436530987525, 0.259557088485383, 0.301283759065815, 0.343075324682922, 
0.283687903142085, 0.283592914904744, 0.264059283720674, 0.287527855889611, 
0.188324610310514, 0.230742624643993, 0.228533751670285, 0.193858413416879, 
0.196462593078236, 0.26351603992366, 0.320788863971364, 0.24636937131769, 
0.318763516486424, 0.426609873329301, 0.366219134961131, 0.378499967117754, 
0.337661758976989, 0.271583947417078, 0.314249960298854, 0.270201122918241, 
0.246946815781063, 0.215102704787387, 0.236134173342779, 0.286232978295626, 
0.350696874593699, 0.164627981394906, 0.205527499202434, 0.270434850920206, 
0.243134003521468, 0.351050952165932, 0.219183969305488, 0.242246176734031, 
0.172067162427782, 0.128130370968335, 0.134122968993382, 0.257281689657044, 
0.299403006982152, 0.422642508788569, 0.228408163742806, 0.11957268044525, 
0.159438669856483, 0.14868234714726, 0.136918222749012, 0.286755138375814, 
0.354634021682536, 0.181731358904613, 0.193212405876045, 0.224667151942299, 
0.203734363544203, 0.243482257894911, 0.289366500456441, 0.252436960541827, 
0.405666704589028, 0.373065503109924, 0.524026234338112, 0.435325641487625, 
0.2554099549592, 0.333929718431462, 0.296689595118324, 0.317661622659771, 
0.259343552195757, 0.220649936386088, 0.163406146367423, 0.385366843416753, 
0.17361870009857, 0.202311917654341, 0.227649249888414, 0.272307292144977, 
0.209258820602416, 0.19319876548376, 0.176481382795848, 0.155899766048858, 
0.20344820709624, 0.264821880206708, 0.261204867940355, 0.327272602852553, 
0.189095535111204, 0.133280535811196, 0.113962382478816, 0.133224369074271, 
0.140065242738296, 0.324893451169475, 0.363116475102291, 0.237103967908121, 
0.242041052773962, 0.270256227204134, 0.287338997227553, 0.173155053909376, 
0.189424483944132, 0.187175303832514, 0.348464842038111, 0.288446217937334, 
0.470765306868106, 0.343131708273399, 0.252938015143712, 0.336888277495862, 
0.301191263319341, 0.356500481509341, 0.301312706960525, 0.267001426016706, 
0.237755741422434, 0.373181832858731, 0.210970882604491, 0.251097175595452, 
0.263098712824128, 0.239269004043719, 0.227818986615428, 0.204715476224745, 
0.207743502877001, 0.217031686744224, 0.250740570083614, 0.228549792422889, 
0.32207055314749, 0.214569147096824, 0.168053926099824, 0.181896968150084, 
0.17623993474032, 0.188910784920792, 0.329236353931487, 0.406757003827438, 
0.21912863831964, 0.222918637259912, 0.233057683009712, 0.279883139925161, 
0.254158158514965, 0.20158263820538, 0.240723178073065, 0.357575151419053, 
0.315084566107509, 0.420672711597265, 0.367274077713619, 0.249934355335581, 
0.318059702631574, 0.31801009061305, 0.307702024506444, 0.272796141745462, 
0.248783767482011, 0.224074353894395, 0.36569822014514, 0.187028191009587, 
0.241049094175669, 0.179339223070881, 0.156157306332836, 0.227513629426693, 
0.202623928640432, 0.246026699193367, 0.33156658615291, 0.315376540691078, 
0.289484137778374, 0.184409694862884, 0.21876550228219, 0.191376542617272, 
0.225070995275441, 0.210475769133431, 0.406320939903412, 0.470184911435262, 
0.283288686913745, 0.310862038618383, 0.326344244418466, 0.354367208429555, 
0.145546897876123, 0.236312084449821, 0.228164131483865, 0.299496818623249, 
0.262731741465948, 0.481980331569159, 0.365549756268789, 0.304206259846763, 
0.404226312619428, 0.378666518283215, 0.427829473290829, 0.410076196767098, 
0.354844586875853, 0.322995610040716, 0.442244062657121, 0.264970270646247, 
0.108746238733874, 0.193352911535009, 0.266719534733286, 0.227013610318163, 
0.238449830517912, 0.347321221639697, 0.323042902522987, 0.309345473650974, 
0.152174920727296, 0.213300152179139, 0.214985014168209, 0.235666919359841, 
0.252242068781554, 0.42709913553219, 0.485189051211046, 0.293088179216135, 
0.329679671628233, 0.33035798397786, 0.346527393314769, 0.179522572086687, 
0.278345689579152, 0.232251261676667, 0.287551912653317, 0.278265167614771, 
0.484670259785051, 0.377718756380607, 0.359121022926639, 0.445377414543919, 
0.401184662139802, 0.451462061652599, 0.400960453366808, 0.357848562271756, 
0.32669976446978, 0.453658939812252, 0.240146917990639, 0.230300655485122, 
0.311042661085334, 0.317612855832712, 0.337564267124495, 0.349956630228225, 
0.262974415301128, 0.242242668807266, 0.253380207155479, 0.289677826481165, 
0.293660554070656, 0.293778024707369, 0.316830788871491, 0.350345490654708, 
0.395195391031732, 0.33513375069398, 0.337179146242041, 0.371234921705163, 
0.39695509160543, 0.242891611224244, 0.250345225015281, 0.335701615323364, 
0.312734019064895, 0.325318419658978, 0.337878664200963, 0.253340263691725, 
0.374161000604498, 0.42535575946078, 0.357273207425347, 0.390814126325831, 
0.37145277419384, 0.331269154759241, 0.35876947913107, 0.352172345260752, 
0.142765226117177, 0.235679681653265, 0.184818072498785, 0.191253138910713, 
0.348695224683716, 0.310876918777759, 0.356345526377818, 0.119856934587125, 
0.18558608732922, 0.218834189019569, 0.222467207064484, 0.214665638336601, 
0.409416658003954, 0.486474197840857, 0.297515550101008, 0.295394752220839, 
0.322174630895094, 0.325738052143967, 0.132596245884175, 0.246268939070485, 
0.237445084809875, 0.296085703183638, 0.30967198389911, 0.514235890211653, 
0.410886481296546, 0.356238394380928, 0.444278663072277, 0.413799296345893, 
0.423845525648207, 0.385961408460294, 0.331592257597649, 0.288799203168772, 
0.442915990670296, 0.197022259697264, 0.175507567684546, 0.194255251918258, 
0.33082685023071, 0.309916538911981, 0.31654082290413, 0.108210668457586, 
0.190395423695379, 0.19168132680145, 0.208726249609115, 0.189951871401416, 
0.380558729308507, 0.450713427014334, 0.29221794299639, 0.289788592951392, 
0.318621931141074, 0.342289641611819, 0.111392920848469, 0.238016295941358, 
0.233024569363059, 0.283486747172187, 0.296984641557404, 0.501886736277605, 
0.354691839582922, 0.340610686032757, 0.422424356332051, 0.384338929876594, 
0.40094086257798, 0.379526405421168, 0.312309452815408, 0.288243747328821, 
0.442756689368451, 0.0978020742858896, 0.14238454435033, 0.224074878211512, 
0.272789288372412, 0.336958268324688, 0.230384388811536, 0.153772250555725, 
0.131424564857863, 0.135138107161081, 0.133890774320702, 0.272263168743922, 
0.334611561341041, 0.201012647896807, 0.201584220387737, 0.208839157389968, 
0.256579613931989, 0.195090930348769, 0.239667311414397, 0.229812436589804, 
0.325585931893025, 0.283778980685324, 0.478526612005615, 0.356639373275897, 
0.241909246023086, 0.305460399238216, 0.27867526153966, 0.322412981417394, 
0.277171351783218, 0.229900157930983, 0.195715920825032, 0.39572261842839, 
0.0885419178364805, 0.260829077668881, 0.285604227212454, 0.358309944800871, 
0.191297070527283, 0.126235720835498, 0.109249713673671, 0.14254483589829, 
0.114098495759652, 0.30083929190514, 0.37020704542986, 0.1958702964902, 
0.197870970618193, 0.236326226170556, 0.252004892063645, 0.172915994311976, 
0.243168478593422, 0.215401946288142, 0.34974266738716, 0.299793278166279, 
0.495198823266587, 0.367763979943646, 0.242791995631133, 0.337673441490117, 
0.307139045113223, 0.337144126944256, 0.307001967739705, 0.247488011208742, 
0.193434061388901, 0.428281034806268, 0.284047884441708, 0.297179320626955, 
0.398472912489279, 0.189422027088779, 0.139174701703958, 0.13546793385265, 
0.161371430634921, 0.168372359428332, 0.311782400400275, 0.399530125695683, 
0.204724384598228, 0.201801211542734, 0.228630624490025, 0.244227565066874, 
0.231236249512786, 0.282283778304694, 0.270960643953082, 0.339857569718471, 
0.326823797526486, 0.50790279274637, 0.416875008383526, 0.282152460994984, 
0.357858660816522, 0.333641345463762, 0.329844433417204, 0.303489492239208, 
0.248763158823102, 0.18972472338955, 0.421183075535043, 0.238876684527487, 
0.345436792453969, 0.336061495089022, 0.263723105832666, 0.269158312858497, 
0.257569542737216, 0.254101022613423, 0.232340609914943, 0.288048749871818, 
0.23893258972811, 0.235307782197714, 0.245368590919873, 0.218291406006654, 
0.316141295974322, 0.24955937510941, 0.275398481408491, 0.397779405844502, 
0.33235739043674, 0.474488798783166, 0.379555493234099, 0.117498927565631, 
0.150698463433954, 0.139639944784483, 0.201302528451723, 0.147727036921414, 
0.110443516933975, 0.207973626843108, 0.2349144537945, 0.261908907223673, 
0.282645509769686, 0.276899906898298, 0.298349708345499, 0.269341140045141, 
0.280406506125865, 0.288913750994849, 0.347891634321888, 0.303388807579801, 
0.281966082920489, 0.30160734823309, 0.312672145150089, 0.289560106847955, 
0.232441075572291, 0.251245870030593, 0.307878521523741, 0.253177898598542, 
0.367223667445015, 0.261958529238038, 0.281362238133638, 0.28801781739124, 
0.270009069548595, 0.299522785484746, 0.257219705028688, 0.221415622921201, 
0.255820765239848, 0.28828981955977, 0.31671582928228, 0.363745505468365, 
0.347939689625386, 0.345267789614021, 0.350823668439948, 0.364495975648929, 
0.366870882161216, 0.385114088795543, 0.394103211364437, 0.395775639313918, 
0.431759745459385, 0.285752359669562, 0.218397737527923, 0.267855815397473, 
0.276016579434273, 0.212495440354826, 0.291102629917947, 0.213970649897841, 
0.366795421768988, 0.346449786640165, 0.342290797217175, 0.415599276580519, 
0.380336211685697, 0.349467695816019, 0.364180183329975, 0.337126810775529, 
0.181804519234005, 0.18062902539593, 0.205007684433831, 0.215420308066645, 
0.374535630704164, 0.448671109202557, 0.274007859625578, 0.289718981226722, 
0.317449365559063, 0.344436310953225, 0.0867015061200141, 0.214204649166454, 
0.215447575226592, 0.264794874484955, 0.282581926460504, 0.484608914272993, 
0.348667454528334, 0.32893804711248, 0.414555319322939, 0.385124997651254, 
0.388063868142764, 0.354930509942296, 0.314210175323553, 0.299205156883234, 
0.421392543584554, 0.100685059696492, 0.0866995692711282, 0.0969095988503732, 
0.303792476166297, 0.382897246418713, 0.200846642991439, 0.207028847006805, 
0.235355971825502, 0.242986796699078, 0.207057482359656, 0.2687412718472, 
0.221962952421413, 0.344331259710073, 0.330688322004791, 0.474942339215751, 
0.385638847415413, 0.251038412436203, 0.331532314029424, 0.3114598183918, 
0.347948011007012, 0.270448613080749, 0.244297789908784, 0.204995495462879, 
0.350844315404954, 0.100911244045508, 0.103185755602834, 0.303917784269364, 
0.362849882087611, 0.202367178640256, 0.223049633935698, 0.249463049219082, 
0.283916535235254, 0.181320446598951, 0.245084575129691, 0.214677803390247, 
0.366103538705351, 0.305876565476593, 0.494825605888924, 0.361478033843841, 
0.248975697592638, 0.337313396652572, 0.301301052404624, 0.341462191338638, 
0.312817106125548, 0.27311011394114, 0.239013977072195, 0.396845918590528, 
0.107924690281289, 0.270050462910416, 0.348802966709437, 0.184161895866205, 
0.19278817419054, 0.208020692062944, 0.242414996189593, 0.220172380542586, 
0.233584142548741, 0.217618000702973, 0.345697450080864, 0.313839355336015, 
0.434547000124388, 0.359186021565848, 0.267208517073225, 0.32330173927609, 
0.30518274090545, 0.34529655389533, 0.262508097267023, 0.227176931450904, 
0.204689406161101, 0.361105343888977, 0.30571330885646, 0.367599747989144, 
0.217475081775334, 0.225772086317862, 0.254256629249112, 0.247586478503354, 
0.202555021235713, 0.238681639695241, 0.197751473539488, 0.37176189816412, 
0.317335702325625, 0.49533353822436, 0.363789246350344, 0.254818852710107, 
0.310725186653894, 0.293853581633784, 0.345640005875408, 0.289048614316403, 
0.248412043950396, 0.214293019014908, 0.387475214055207, 0.127715354802369, 
0.223361776933178, 0.20504999812424, 0.19419269144831, 0.222443662573356, 
0.363848133977628, 0.310299864585179, 0.356418806901375, 0.426643530706364, 
0.384599879799222, 0.456927976196206, 0.3856587938183, 0.235397342626925, 
0.276899592755987, 0.231249267705255, 0.22606279999052, 0.183932484187818, 
0.183332809218275, 0.233138159218437, 0.319631133798301, 0.276221233500258, 
0.249619172891433, 0.234864688890259, 0.2534011126405, 0.422547172125712, 
0.354971954715825, 0.399303016835905, 0.425140805653638, 0.396632912535974, 
0.473365299786863, 0.387309733261674, 0.290139703736607, 0.255261478630608, 
0.192826774657939, 0.196399242991551, 0.221163737535614, 0.245646551500451, 
0.270630500732627, 0.350486952979949, 0.0812114642376115, 0.108500113815981, 
0.152246295653802, 0.277240627186762, 0.276720261356458, 0.31032055938721, 
0.388673098686631, 0.349432716682413, 0.475610259715878, 0.438195382601128, 
0.235959199160122, 0.316260782927578, 0.245911688896529, 0.23743988210527, 
0.218712106892772, 0.178816541166305, 0.178230220796603, 0.379833762104026, 
0.0784463617489709, 0.129997140263035, 0.291085148419097, 0.273257526667692, 
0.306557221428694, 0.402153280417114, 0.364282996217115, 0.463352273325088, 
0.431063703093528, 0.222304690496031, 0.291674227758824, 0.241221392372485, 
0.210744722052842, 0.194787521424541, 0.167160218115454, 0.150755475480528, 
0.37885166947424, 0.135279579197833, 0.327926450188976, 0.298559515112429, 
0.318417810243592, 0.409380013643058, 0.364524911114103, 0.488857362760066, 
0.466801626528242, 0.231271124782347, 0.290704260349066, 0.257441555634786, 
0.219539177105795, 0.185785463529339, 0.190539666493475, 0.164885095495906, 
0.400661447880847, 0.351670664342995, 0.293106495448539, 0.278927612321467, 
0.41056998548736, 0.352891397933641, 0.518833547693511, 0.491661977916097, 
0.229382920274352, 0.292629764083707, 0.226989823245877, 0.216704505053967, 
0.182937025703239, 0.158253530543942, 0.158729282293997, 0.356682252224064, 
0.189510186934412, 0.200144695957883, 0.267522818117134, 0.247590590638521, 
0.490647848397164, 0.325163873345429, 0.291292316062298, 0.390440113550956, 
0.357936796896738, 0.405263499885725, 0.370252764588065, 0.322692323050225, 
0.306120087223696, 0.421325571167844, 0.167539006000027, 0.269020719809449, 
0.223042339002822, 0.38760404469828, 0.275161689978891, 0.242363068276669, 
0.299763159742022, 0.295721854984686, 0.316816505510713, 0.280958787086803, 
0.242920147198185, 0.27387569756998, 0.346646422960218, 0.296032978704237, 
0.171471904196341, 0.455226711060175, 0.306302086043222, 0.270092447713217, 
0.330576945364237, 0.328073318126048, 0.379232696180287, 0.305191216068878, 
0.287928376535759, 0.281784904626341, 0.400754470522899, 0.183297028965347, 
0.439603470995397, 0.324142275296304, 0.416842097225044, 0.334144320356283, 
0.331280399040641, 0.361642381662566, 0.395220939617569, 0.348409804637902, 
0.33437908671065, 0.413423564393322, 0.413752235847781, 0.309263065288595, 
0.335337798740467, 0.313696283268453, 0.324026564106507, 0.370850321127701, 
0.367711294816184, 0.318257687382319, 0.282250272793762, 0.405957887669014, 
0.289062022286134, 0.500989225264607, 0.469590271510581, 0.465402652530193, 
0.4884729148024, 0.474410320519324, 0.453387206453657, 0.472498468887031, 
0.428171148682882, 0.415945155744945, 0.361501568460874, 0.351686833699556, 
0.425332115878206, 0.420330132379335, 0.3849261129704, 0.416859526057763, 
0.408207585207454, 0.144955775692482, 0.162810593772454, 0.222289443174366, 
0.174772219403487, 0.153368630325598, 0.212286474750986, 0.27740056732249, 
0.107301136884498, 0.159183884060135, 0.189989069644874, 0.172538300775432, 
0.202263114833207, 0.260439883936975, 0.118388242752457, 0.165081626468155, 
0.140322059005816, 0.185185767078914, 0.260065955820617, 0.137793449327493, 
0.147346740832392, 0.191053507374825, 0.322728564748582, 0.105659036172812, 
0.205131400541044, 0.265657065294917, 0.138708922010367, 0.250058789949686, 
0.310423445004488), class = c(""dissimilarity"", ""dist""), Labels = c(""1"", 
""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", ""11"", ""12"", ""13"", 
""14"", ""15"", ""16"", ""17"", ""18"", ""19"", ""20"", ""21"", ""22"", ""23"", ""24"", 
""25"", ""26"", ""27"", ""28"", ""29"", ""30"", ""31"", ""32"", ""33"", ""34"", ""35"", 
""36"", ""37"", ""38"", ""39"", ""40"", ""41"", ""42"", ""43"", ""44"", ""45"", ""46"", 
""47"", ""48""), Size = 48L, Metric = ""mixed"", Types = c(""I"", ""I"", 
""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", 
""I"", ""I"", ""I"", ""N"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", 
""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", 
""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I"", ""I""
))
</code></pre>
"
"0.0791807582464896","0.0808745799025788","218796","<p>I have unsupervised data it is a mix of continuous and categorical data. Now I want to classify the test data into three categories on basis of my unsupervised data.</p>

<p>The approach I took is first do the clustering of unsupervised data, use this categorised data as a base data for preparing a new model that predicts on top of it.</p>

<p>I want know whether this approach correct or not or is there better way for classifying test set? Particular algorithm I need to follow for this?</p>

<p>I am doing this in R.</p>

<p>The approach is to modify the training set data so that this can be used to properly predict the test data. Here target variable is missing in train and test set.</p>
"
"NaN","NaN","218913","<p>I am new to clustering.  My apology if this has been asked before.</p>

<p>I'd like to differentiate two distinct linear populations within sample matrix, and tag them differently.  Apparently k-means couldn't do a good job, which is shown in the plot below.  Does anybody have any good suggestion?</p>

<p>Example code:</p>

<pre><code>x &lt;- c(1:1000)
y &lt;- x
y2 &lt;- 10 * x
sample &lt;- matrix(c(x, x, y, y2), nrow = 2000)
</code></pre>

<p><a href=""http://i.stack.imgur.com/O3OGC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/O3OGC.png"" alt=""enter image description here""></a></p>

<p>Thanks a lot!</p>
"
"0.0791807582464896","0.0539163866017192","220449","<p>I have the following dataframe </p>

<pre><code>&gt; str(kmeans)
'data.frame':   29271 obs. of  3 variables:
 $ x: num  -4.5 -1.514 -0.203 -2.016 -3.525 ...
 $ y: num  -1.127 1.195 0.622 3.138 -1.382 ...
 $ z: num  -0.607 -2.354 1.121 0.318 1.958 ...
</code></pre>

<p>x, y and z are basically PC1, PC2 and PC3 components after PCA of the original dataset. The original dataset consisted of 10 features. </p>

<p>I have performed K-means on kmeans dataset. Below is the graph
<a href=""http://i.stack.imgur.com/6LyBz.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6LyBz.png"" alt=""K means clustering""></a> </p>

<p>How do I extract the original features from these clusters. For example if I identify cluster 1 then what are the values of corresponding attributes of the original 10 features not of the principal components.</p>
"
"0.250391542918067","0.170498584867618","221144","<p>(This post is a repost of a question I posted yesterday (now deleted), but I've tried to scale back volume of words and simplify what I'm asking)</p>

<p>I'm hoping to get some help interpreting a kmeans script and output I have created. This is in the context of text analysis. I created this script after reading several articles online on text analysis. I have linked to some of them them below.</p>

<p>Sample r script and corpus of text data I will refer to throughout this post:</p>

<pre><code>library(tm) # for text mining

## make a example corpus
# make a df of documents a to i
a &lt;- ""dog dog cat carrot""
b &lt;- ""phone cat dog""
c &lt;- ""phone book dog""
d &lt;- ""cat book trees""
e &lt;- ""phone orange""
f &lt;- ""phone circles dog""
g &lt;- ""dog cat square""
h &lt;- ""dog trees cat""
i &lt;- ""phone carrot cat""
j &lt;- c(a,b,c,d,e,f,g,h,i)
x &lt;- data.frame(j)    

# turn x into a document term matrix (dtm)
docs &lt;- Corpus(DataframeSource(x))
dtm &lt;- DocumentTermMatrix(docs)

# create distance matrix for clustering
m &lt;- as.matrix(dtm)
d &lt;- dist(m, method = ""euclidean"")

# kmeans clustering
kfit &lt;- kmeans(d, 2)
#plot â€“ need library cluster
library(cluster)
clusplot(m, kfit$cluster)
</code></pre>

<p>That's it for the script. Below are the output of some of the variables in the script:</p>

<p>Here's x, the data frame x that was transformed into a corpus:</p>

<pre><code> x
                       j
    1 dog dog cat carrot
    2      phone cat dog
    3     phone book dog
    4     cat book trees
    5       phone orange
    6  phone circles dog
    7     dog cat square
    8      dog trees cat
    9   phone carrot cat
</code></pre>

<p>An here's the resulting document term matrix dtm:</p>

<pre><code>    &gt; inspect(dtm)
&lt;&lt;DocumentTermMatrix (documents: 9, terms: 9)&gt;&gt;
Non-/sparse entries: 26/55
Sparsity           : 68%
Maximal term length: 7
Weighting          : term frequency (tf)

    Terms
Docs book carrot cat circles dog orange phone square trees
   1    0      1   1       0   2      0     0      0     0
   2    0      0   1       0   1      0     1      0     0
   3    1      0   0       0   1      0     1      0     0
   4    1      0   1       0   0      0     0      0     1
   5    0      0   0       0   0      1     1      0     0
   6    0      0   0       1   1      0     1      0     0
   7    0      0   1       0   1      0     0      1     0
   8    0      0   1       0   1      0     0      0     1
   9    0      1   1       0   0      0     1      0     0
</code></pre>

<p>And here is the distance matrix d</p>

<pre><code>&gt; d
         1        2        3        4        5        6        7        8
2 1.732051                                                               
3 2.236068 1.414214                                                      
4 2.645751 2.000000 2.000000                                             
5 2.828427 1.732051 1.732051 2.236068                                    
6 2.236068 1.414214 1.414214 2.449490 1.732051                           
7 1.732051 1.414214 2.000000 2.000000 2.236068 2.000000                  
8 1.732051 1.414214 2.000000 1.414214 2.236068 2.000000 1.414214         
9 2.236068 1.414214 2.000000 2.000000 1.732051 2.000000 2.000000 2.000000
</code></pre>

<p>Here is the result, kfit:</p>

<pre><code>&gt; kfit
K-means clustering with 2 clusters of sizes 5, 4

Cluster means:
         1        2        3        4        5        6        7        8        9
1 2.253736 1.194938 1.312096 2.137112 1.385641 1.312096 1.930056 1.930056 1.429253
2 1.527463 1.640119 2.059017 1.514991 2.384158 2.171389 1.286566 1.140119 2.059017

Clustering vector:
1 2 3 4 5 6 7 8 9 
2 1 1 2 1 1 2 2 1 

Within cluster sum of squares by cluster:
[1] 13.3468 12.3932
 (between_SS / total_SS =  29.5 %)

Available components:

[1] ""cluster""      ""centers""      ""totss""        ""withinss""     ""tot.withinss"" ""betweenss""    ""size""         ""iter""        
[9] ""ifault""      
</code></pre>

<p>Here is the resulting plot:
<a href=""http://i.stack.imgur.com/vBvTa.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/vBvTa.png"" alt=""enter image description here""></a></p>

<p>I have several questions about this:</p>

<ol>
<li>In calculating my distance matrix d (a parameter used in kfit calculation) I did this: <code>d &lt;- dist(m, method = ""euclidean"")</code>. Another article I encountered did this: <code>d &lt;- dist(t(m), method = ""euclidean"")</code>. Then, separately on a <a href=""http://stackoverflow.com/questions/38003622/within-the-context-of-a-document-term-matrix-what-exactly-are-x-and-y-axis-in-k"">SO question</a> I posted recently someone commented ""kmeans should be run on the data matrix, not on the distance matrix!"". Presumably they mean <code>kmeans()</code> should take m instead of d as input. Of these 3 variations which/who is ""right"". Or, assuming all are valid in one way or another, which would be the conventional way to go in setting up an initial baseline model?</li>
<li>As I understand it, when kmeans function is called on d, what happens is that 2 random centroids are chosen (in this case k=2). Then r will look at each row in d and determine which documents are closest to which centroid. Based on the matrix d above, what would that actually look like? For example if the first random centroid was 1.5 and the second was 2, then how would document 4 be assigned? In the matrix d doc4 is 2.645751 2.000000 2.000000 so (in r) mean(c(2.645751,2.000000,2.000000)) = 2.2 so in the first iteration of kmeans in this example doc4 is assigned to the cluster with value 2 since it's closer to that than to 1.5. After this the mean of the cluster is reclauculated as a new centroid and the docs reassigned where appropriate. Is this right or have I completely missed the point?</li>
<li>In the kfit output above what is ""cluster means""? E.g., Doc3 cluster 1 has a value of 1.312096. What is this number in this context? [edit, since looking at this again a few days after posting I can see that it's the distance of each document to the final cluster centers. So the lowest number (closest) is what determines which cluster each doc is assigned].</li>
<li>In the kfit output above, ""clustering vector"" looks like it's just what cluster each doc was assigned to. OK.</li>
<li>In the kfit output above, ""Within cluster sum of squares by cluster"". What is that? <code>13.3468 12.3932 (between_SS / total_SS =  29.5 %)</code>. A measure of the variance within each cluster, presumably meaning a lower number implies a stronger grouping as opposed to a more sparse one. Is that a fair statement? What about the percentage  given 29.5%. What's that? Is 29.5% ""good"". Would a lower or higher number be preferred in any instance of kmeans? If I experimented with different numbers of k, what would I be looking for to determine if the increasing/decreasing number of clusters has helped or hindered the analysis?</li>
<li>The screenshot of the plot goes from -1 to 3. What is being measured here? As opposed to education and earnings, height and weight, what is the number 3 at the top of the scale in this context?</li>
<li>In the plot the message ""These two components explain 50.96% of the point variability"" I already found some detailed info <a href=""http://stats.stackexchange.com/questions/141280/understanding-cluster-plot-and-component-variability"">here</a> (in case anyone else comes across this post - just for completeness of understanding kmeans output wanted to add here.).</li>
</ol>

<p>Here's some of the articles I read that helped me to create this script: </p>

<ul>
<li><a href=""https://rstudio-pubs-static.s3.amazonaws.com/31867_8236987cf0a8444e962ccd2aec46d9c3.html"" rel=""nofollow"">Basic Text Mining in R</a></li>
<li><a href=""https://eight2late.wordpress.com/2015/07/22/a-gentle-introduction-to-cluster-analysis-using-r/"" rel=""nofollow"">A Gentle Introduction into Cluster Analysis using R (text mining based article)</a></li>
</ul>
"
"0.0685725481323742","0.062257280636469","221832","<p>I have a dataset like df and I want to cluster the data in R. These variables are binary showing that a person uses a programming language or not. I have these questions:</p>

<p>1- How can I visualize my data? Up to now, I tried heatmap (with clusters on both rows and columns)</p>

<p>2- What is the best distance calculation method for this data?</p>

<p>3- What is the best clustering method for this data?</p>

<p>4- Should I consider normalizing the variables i.e. (value-mean)/sd before clustering? All the variables are either zero or one but the variables have different standard deviations.</p>

<pre><code>  language &lt;- c(
  ""R, Matlab"",
  ""Assembly, R, Go, Rust"",
  ""Java, Javascript, Ruby, SQL"",
  ""Java, Ruby"",
  ""C, C++"",
  ""PHP, Javascript, Ruby, Assembly, Swift, R, Matlab, Go, Haskell"",
  ""R"",
  ""Perl, Javascript, R"",
  ""Javascript, Ruby, Bash"",
  ""Python, PHP, Javascript"",
  ""Java"",
  ""Java, C""
)

df &lt;-as.data.frame(language,stringsAsFactors = FALSE)


df &lt;- reshape2::recast(
  data =  setNames(strsplit(language, "", "", T), language), 
  formula = L1~value, 
  fun.aggregate = length
)

str(df)
</code></pre>
"
"0.0885267789745639","0.0482242822170412","221850","<p>I'm reading up on kmeans and following a blog post to do some text analysis.</p>

<p>I watched a <a href=""https://www.youtube.com/watch?v=Ao2vnhelKhI"" rel=""nofollow"">helpful video</a> by Andrew Ng fro Coursera which really helped my understanding of what is going on. Here is a screen shot from the video:</p>

<p><a href=""http://i.stack.imgur.com/XdHhG.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/XdHhG.png"" alt=""kmeans clustering""></a></p>

<p>So far kmeans makes sense, start with K cluster centroids, assign each point to a cluster based on distance (Euclidean?), recalculate mean, repeat. </p>

<p>But I'm also following <a href=""https://rstudio-pubs-static.s3.amazonaws.com/31867_8236987cf0a8444e962ccd2aec46d9c3.html"" rel=""nofollow"">this</a> blog post on text analysis in R. Following the article I make a document term matrix.</p>

<p>Context is online survey results. Let's say there are e.g. 10k survey results and a total of 15k ""tokens"", so a dtm of 10k*15k.</p>

<p>Further down the article we are shown an example of kmeans clustering on the dtm:</p>

<pre><code>library(fpc)   
d &lt;- dist(t(dtmss), method=""euclidian"")   
kfit &lt;- kmeans(d, 2)   
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)  
</code></pre>

<p>And it works.</p>

<p>But I'm trying to understand how. An example in kmeans clustering I found online was weight and height on the x,y axis for determining how to split a population for clothing sizes of small, medium and large. That makes sense.</p>

<p>But in the context of a DTM, what would be on the x and y axis in the screen shot above? It's just not clicking. </p>
"
"0.0685725481323742","0.062257280636469","221925","<p>I have a <code>blackbox</code> function which takes finite number of integers <em>V1, V2, Vn</em> parameters and based on time series variable produce a scalar response. I would like to find parameters which maximize the response. I'm not seeking the single maximum response but more a group of parameter similarities (like their ratio V1/V2, etc.) which results into well maximized response. I was reading about various techniques but cannot find the right one. Not sure if I should calculate the ratio (V1/V2, etc.) and include them in clustering together with V1...Vn? or I can apply some machine learning which tests those combinations for me.<br>
The following code is just an illustration of the data structure, the actual number of rows is from 10K to 10M.</p>

<p></p>

<pre><code>df = data.frame(V1=c(11L,15L,15L,16L),
                V2=c(20L,20L,25L,14L),
                V3=c(20L,15L,24L,50L),
                V4=c(18L,22L,30L,60L),
                reponse=c(1.04,1.21,0.97,1.00))
df
#  V1 V2 V3 V4 reponse
#1 11 20 20 18    1.04
#2 15 20 15 22    1.21
#3 15 25 24 30    0.97
#4 16 14 50 60    1.00
</code></pre>

<p>Expected output could be list clusters of parameters, or their relationships, that produce well maximized response.</p>
"
"0.14813363449167","0.115278083540847","223035","<p>I am an ecology graduate with a decent practical familiarity with statistics in R, but limited experience of approaches such as PCA, and Cluster Analysis. 
I am currently faced with the challenge of trying to apply my skills to an entirely unfamiliar problem:  my dad is writing a book on archaeological finds of blades, has collected data on 176 finds and has tasked me with analysing it.  </p>

<p>The data selected for analysis is structured thus:   </p>

<pre><code> Blade.length     Max.width     Max.thickness     Shape     Broken.back        Type   

 Min.   :165.0   Min.   :20.00   Min.   : 3.500   A   :70   Min.   :0.0000   Cs   :39  
 1st Qu.:220.0   1st Qu.:28.75   1st Qu.: 5.000   B   : 8   1st Qu.:0.0000   Hbs  :15  
 Median :270.0   Median :34.00   Median : 6.000   C   :14   Median :0.0000   Lbs  :17  
 Mean   :311.5   Mean   :35.20   Mean   : 6.464   D   :14   Mean   :0.2686   Ls   :23  
 3rd Qu.:353.0   3rd Qu.:39.00   3rd Qu.: 7.875   E   :30   3rd Qu.:0.5000   Ns   :43  
 Max.   :760.0   Max.   :62.00   Max.   :11.000   F   :12   Max.   :1.0000   Small:35  
 NA's   :9       NA's   :4       NA's   :86       NA's:28   NA's   :1        NA's : 4 
</code></pre>

<p>Shape is a variable of categories pertaining to the shape of the tip of the blades - these categories are in no particular order.  Broken.back is a different way of looking at ""shape"", effectively binary, although some cases are ""in between"" and have been entered as 0.5.   ""Type"" is a supplementary variable referring to what each blade has been identified as, using a pre-existing typology. Part of the exercise is to examine if this pre-existing typology is fit for purpose. </p>

<p>The dataset is, necessarily, incomplete, with NAs in all variables, although blades with lots of missing data have been excluded from the analysis.  Within the sample remaining, the most incomplete column is blade thickness, with 48% NAs.  </p>

<p>So far I have attempted to visualise the data by means of factorial analysis of mixed data, with imputation, using packages MissMDA and FactoMineR.   However I've found myself bewildered by the number of options and what approach is appropriate for the sort of data I have. </p>

<p>More importantly, I am looking to conduct hierarchical cluster analysis of the data to examine the relatedness of different finds and try and statistically define types (<a href=""http://www.r-bloggers.com/hierarchical-clustering-in-r-2/"" rel=""nofollow"">http://www.r-bloggers.com/hierarchical-clustering-in-r-2/</a>), so far using HCLUST, Dist, and vegdist (package: Vegan).   However, I am not clear as to;  </p>

<ul>
<li>How to manage, prepare or transform the types of data I have for this type of analysis.</li>
<li>What dissimilarity index method would be most appropriate in this context.</li>
<li>What type of clustering / linkage method would be most appropriate in this context. </li>
</ul>

<p>Sorry for the long question. As you can see I am quite bewildered and out of my depth.  Thanks in advance. </p>
"
"0.0559892510955854","0.0381246425831512","224445","<p>I have a matrix of data (215 rows, 618 cols) the data is xy positional data from a square surface. Most of the data is 0, and very few are 1. When I plot this data I see that the 1's form 2 small clusters...I'd like to use a clustering technique to automatically colour the clusters and to know how many 1's (cells) make up each cluster..?
Can I use kmeans or DBSCAN for this..? the examples i've seen answered seem to be on xy numbers data (if that makes sense) and not xy positional data with only 1's &amp; 0's.<a href=""http://i.stack.imgur.com/bEMBW.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bEMBW.jpg"" alt=""enter image description here""></a></p>

<p>Any help would be appreciated.
Paul.</p>
"
"0.104746297470869","0.0611354359454417","224449","<p>I am conducting clustering analysis in which I am using three clustering algorithms <code>K-means</code>, <code>Spectral Clustering</code>, and <code>Hierarchical clustering</code> on 3 datasets in UCI repository. </p>

<p>I have used <code>R</code> packages to conduct clustering analysis and got the results such as Size of clusters, cluster vector, cluster means, Within cluster sum of squares, and grouping of cluster by Class. </p>

<p>Following is an example of my <code>K-means</code> on the Pima Indian diabetes data in UCI repository:</p>

<pre><code>diabetes &lt;- read.csv(url(""http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data""), header = FALSE)

names(diabetes)&lt;- c(""No.ofTimesPregnant"", ""GlucoseConcentration"", ""BloodPressure"", ""TricepSkinThickness"", ""insulin"", ""BMI"", ""PedigreeFunction"", ""Age"", ""Class"") 

set.seed(20)

KmeansCluster &lt;- kmeans(diabetes[, 1:8], 4, nstart = 20, iter.max=10)

pcol &lt;- as.character(diabetes$Class)
pairs(diabetes[1:8], pch = pcol, col = c(""green"", ""red"") KmeansCluster$cluster])
KmeansCluster
table(KmeansCluster$cluster, diabetes$Class)
</code></pre>

<p>I wish to know how I can compare the results of each clustering algorithm? So that I can say that particular clustering algorithm is best for this dataset. More specifically to say, what metric should I choose and how I can get that metric in <code>R</code> (For example, it would be helpful if you could tell me how to get those metric on my above <code>R</code> code for <code>K-means</code>)?.  </p>

<p>As I know the diameter of the cluster and average distance of each cluster is used as a measure to compare clustering algorithm in general. </p>
"
"0.125195771459034","0.170498584867618","224509","<p>I'm conducting a meta-analysis on standardised mean difference scores. Some studies provide multiple effect sizes, thereby violating the assumption of independence. An example is given below (all effect sizes were calculated with regard to a pre-test). In study A, all participants received the same treatment (watching a video), and were tested repeatedly. In study B, there were two different treatment groups (one group watched a video, the other group listened to an audio book), and everyone was tested once. Study C provided only one effect size.</p>

<pre><code>study        treatment          testing_moment         effect_size

A            video              immediately            0.6
A            video              delayed                0.5
B            video              immediately            0.9
B            audio_book         immediately            0.7
C            audio_book         delayed                0.4
</code></pre>

<p>I'm using the <em>metafor</em> package in <em>R</em>, in which you can fit a multilevel model to account for non-independent sampling errors. </p>

<p>What I've done:</p>

<pre><code>rma.mv(effect_size_vector, variance_vector, mods = ~ testing_moment, 
  random = ~ 1 | treatment/study, data = rev)
</code></pre>

<p>Could anyone please have a look whether this approach is correct? I'm especially unsure about whether I've correctly indicated the clustering using slash (/) (this decision was based on <a href=""http://conjugateprior.org/2013/01/formulae-in-r-anova/"" rel=""nofollow"">this page</a>), and whether the model as a result indeed takes into account the non-independence of effect sizes. </p>

<p>I'm also wondering whether somehow it should be corrected that the samples in study A are dependent and in study B they are independent. Or is that already accounted for by virtue of the treatment variable being the same for both samples in study A?</p>
"
"0.131306432859723","0.0975384133143533","224543","<p>I use currently the function hclust() for Dendogram in R. It looks like:</p>

<pre><code>res.hc &lt;- hclust(d, method = ""ward.D2"" )
</code></pre>

<p>My special interest is to understand, what the method I have to use for my data and where is a difference. 
I already took a look to the R Documentation <code>?hclust</code></p>

<p>The documentation is very poor, I could find only this part:</p>

<blockquote>
  <p>A number of different clustering methods are provided. Ward's minimum
  variance method aims at finding compact, spherical clusters. The
  complete linkage method finds similar clusters. The single linkage
  method (which is closely related to the minimal spanning tree) adopts
  a â€˜friends of friendsâ€™ clustering strategy. The other methods can be
  regarded as aiming for clusters with characteristics somewhere between
  the single and complete link methods. Note however, that methods
  ""median"" and ""centroid"" are not leading to a monotone distance
  measure, or equivalently the resulting dendrograms can have so called
  inversions or reversals which are hard to interpret, but note the
  trichotomies in Legendre and Legendre (2012).</p>
  
  <p>Two different algorithms are found in the literature for Ward
  clustering. The one used by option ""ward.D"" (equivalent to the only
  Ward option ""ward"" in R versions &lt;= 3.0.3) does not implement Ward's
  (1963) clustering criterion, whereas option ""ward.D2"" implements that
  criterion (Murtagh and Legendre 2014). With the latter, the
  dissimilarities are squared before cluster updating. Note that
  agnes(<em>, method=""ward"") corresponds to hclust(</em>, ""ward.D2"").</p>
</blockquote>

<p>Can anybody provide me better introduction to this method and probably try to explain, how I can select the right method for my data (my data is very similar to ibis setosa dataset).</p>
"
"NaN","NaN","224804","<p>Iâ€™m looking to implement density-based clustering with R or Mathematica on a giant file (600,000 points on a 3 billion x 3 billion plane).
Is DBSCAN the right method for data that is this sparse? I am also anticipating a huge amount of noise. Which parameters would I tweak to account for this?</p>
"
"0.0395903791232448","0.0539163866017192","226226","<p>I'm new to predictive analytics. I have data variables which are highly skewed, I want to normalize those for better predictions. I've used normalization,standardization. but they gave same data distributions as before. how can I bring my data to Normality,and what techniques should I use.
Is normalizing data variables necessary in every case (clustering, regression, classification) ? 
please help with an example if possible.
Thank you.</p>
"
"0.125195771459034","0.0852492924338092","226907","<p>I am new to R and I am trying to complete a project in R. I have two data-sets one containing information on companies and the second containing information on tenders. The company data-sets has columns Company Name, Company industry, location, description while the Tender data-set has tender description, tender industry and location. I want to compare and match the company description to tender description, company industry with tender industry and company location with tender location to find the most appropriate tenders that a company can target. All the columns involve text and I have already used the text mining package to experiment the data sets separately. I also used clustering to group companies in the same industry and svm to automatically put new companies to the groups. But I have no idea on how to relate these two datasets and then match them. I would like to use multiple variables to match companies with tenders.</p>

<p>Category:
Do the company specialties match the contract CPV descriptions.
Do the company SIC code descriptions match the contract CPV descriptions.
Do the company specialties match the contract long descriptions.
Do the company SIC code descriptions match the contract long descriptions.</p>

<p>Category history:
Has the company won a contract with these CPV codes before.
Has the company won a contract in this cluster (descriptions) before.
Has another company in the cluster won a contract with these CPV codes before.
Has another company in the cluster won a contract in this cluster (descriptions) before.</p>

<p>Relationship:
Has the company won a contract from this buyer before.
Has the company won a contract from a similar buyer lately (industry/geography cluster).</p>

<p>In my dataset I have all the columns like company specialities, CPV code descriptions, SIC code descriptions etc that are necessary.
Any ideas and example codes will be appreciated. Thank you very much.  </p>
"
"0.0885267789745639","0.0723364233255618","229148","<p>I am trying to get the p-values for hierarchical clustering analysis on the following dataset.The dendrograms generated by pvclust and hclust are completely different.Because the pvclust mentioned they used the same method as hclust, it should be identical.</p>

<pre><code>&gt;test=read.delim(""test1.txt"", header=T)
&gt; test

  S1 S2 S3 S4 S5 S6 S7 S8 S9 S10
1  1  1  1  1  1  1  0  1  1   0
2  0  0  1  0  0  0  0  0  0   0
3  1  0  0  1  1  0  0  0  1   1
4  1  0  1  0  1  1  0  1  0   1
5  0  1  0  1  0  0  0  0  1   0
6  1  0  1  0  1  1  0  0  0   1
7  1  1  0  1  0  0  1  0  1   0
8  1  1  0  1  0  1  1  0  1   0
9  1  0  1  0  1  1  0  1  0   0

&gt; div.norm=decostand(test,""normalize"")
&gt; div.ch=vegdist(div.norm,""bray"")
&gt; div.ch.UPGMA=hclust(div.ch,method = ""average"")
&gt; plot(div.ch.UPGMA)
</code></pre>

<p>This generates the following dendrogram:
<a href=""http://i.stack.imgur.com/WFNKU.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/WFNKU.jpg"" alt=""Dendrogram using hclust""></a></p>

<p>Then I tried to run the same dataset using pvclust.</p>

<pre><code>&gt; test.tr=t(test)
&gt; result=pvclust(test.tr, method.dist=""cor"", method.hclust=""average"", nboot=1000)
&gt; plot(result)
</code></pre>

<p>I get the following dendrogram which is different from the one generated by hclust.
<a href=""http://i.stack.imgur.com/4Okw4.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4Okw4.jpg"" alt=""Dendrogram using pvclust""></a>
Some suggested that I should not transpose the data. But that produces a dendrogram where the columns are clustered (I don't want that).</p>

<p>Any help would be greatly appreciated!</p>
"
"0.0885267789745639","0.0964485644340824","229479","<p>I need use ROCK (RockCluster) algorithm for binary data in R. My binary data looks this:</p>

<pre><code>objects cat1    cat2    cat3    cat4 ...
A       TRUE    FALSE   FALSE   FALSE
B       TRUE    FALSE   TRUE    FALSE
C       TRUE    FALSE   FALSE   FALSE
D       FALSE   TRUE    TRUE    TRUE
E       TRUE    TRUE    TRUE    TRUE
F       TRUE    FALSE   TRUE    FALSE
</code></pre>

<p>Now I need clasify these objects A-F to clusters. I apply this procedure <a href=""https://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Clustering/RockCluster#Dataset"" rel=""nofollow"">https://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Clustering/RockCluster#Dataset</a> But I have several problems.</p>

<ol>
<li>I import data from CSV file. <code>db &lt;- read.csv(file=""file.csv"", header=TRUE, sep=""|"")</code> Fields are 1 (TRUE) and 0 (FALSE).</li>
<li>I convert this data: <code>x &lt;- as.dummy(db[-1]</code>). After this step all columns in x are duplicated with 1 and 0. Why? It is correct please?</li>
<li><code>rc &lt;- rockCluster(x, n=4, debug=TRUE)</code></li>
<li><code>rf &lt;- fitted(rc)</code> Why <code>fitted</code> and when rather use <code>predict(rc, x)</code>?</li>
<li><code>table(db$objects, rf$cl)</code>
After I get this output:
<code>
     1 NA
A   1    0
B   1    0
C   1    0
D   0    1
E   0    1
F   0    1</code></li>
</ol>

<p>What way I can read this output? What objects are in clusters with other? What objects are the most similar please?</p>
"
"0.118771137369734","0.125804902070678","230717","<p>I have question about output of <code>PROXIMUS</code> algorithm. In the example of
use <a href=""https://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Clustering/Proximus#Algorithm"" rel=""nofollow"">here</a>.
In Case Study section is all clear, but I do not know what way they use
for get clustered words to groups: </p>

<pre><code>Group 1 (computers) = {intel, computer, software, linux, windows, Firefox, explorer, programming}
Group 2 (authors) = {kuth, shakespeare, grisham, asimov, book} 
Group 3 (noise) = {love}
</code></pre>

<p>How can I get this output for my data? </p>

<p>Next in
the example section is graph. How do I read this graph?</p>

<p>My data are:</p>

<pre><code>objects cat1    cat2    cat3    cat4 ...
A       TRUE    FALSE   FALSE   FALSE
B       TRUE    FALSE   TRUE    FALSE
C       TRUE    FALSE   FALSE   FALSE
D       FALSE   TRUE    TRUE    TRUE
E       TRUE    TRUE    TRUE    TRUE
F       TRUE    FALSE   TRUE    FALSE
</code></pre>

<p>After apply of <code>Proximus</code> algorithm I get this output:</p>

<pre><code>pr &lt;- proximus(x, max.radius=8, debug=TRUE)
#Non-Zero: 55
#Sparsity: 0.48
#  0 [6,3,5] 1 &gt;
#  1 [3,3,5] 1 * 1
#  1 [3,1,0] 1 &gt;
#  2 [1,1,0] 1 * 2
#  2 [2,1,0] 1 &gt;
#  3 [1,1,0] 1 * 3
#  3 [1,1,0] 1 * 4


summary(pr)
#Size Length Radius Error Fnorm Jsim Valid
#1    3     16      5  0.16     3 0.81  TRUE
#2    1      9      0  0.00     0 1.00  TRUE
#3    1      4      0  0.00     0 1.00  TRUE
#4    1      2      0  0.00     0 1.00  TRUE
</code></pre>

<p>So it means that 3 objects are in one cluster and all other objects have
own cluster. What way I can use for get list of objects in cluster? And I get this: <a href=""http://i.stack.imgur.com/uQAxz.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/uQAxz.png"" alt=""graph""></a> </p>

<p>How do I read this graph? </p>

<p>Many thanks for your help!</p>
"
"0.0559892510955854","0.0381246425831512","230735","<p>I want to implement semi supervised hierarchical clustering technique on iris data. I want to use 10% label (90% class label missing) training data. 
I have tried following R code -</p>

<pre><code>library(tibble)
library(DBI)
library(lpSolve)
library(RSSL)

library(dplyr,warn.conflicts = FALSE)
library(ggplot2,warn.conflicts = FALSE)

### Calling dataset
data(iris)
data&lt;-iris
data2&lt;-data[,1:4]

### Training and test data...
inTrain = sample(1:150,120)
dfTrain=data[inTrain,]
dfTest=data[-inTrain,]


#### 10% labeling is Semi Supervised datasets....
# Randomly remove labels
dfTrain &lt;- data %&gt;% add_missinglabels_mar(data[,5]~.,prob=0.90)
</code></pre>

<p>It will be helpful if anyone can write some R code or give idea.</p>
"
"0.0559892510955854","0.0381246425831512","230838","<p>I have a data set n=175 and for 2 different clustering (A and B) I have 5 and 6 clusters. The table for similarity of clusterings is below. First I calculated the Rand Index both manually with Excel and with ""cluster_similarity"" function in R and I got 63,4%.
Than I calculated the Adjusted Rand index both with Excel and ""adjustedRandIndex"" function in R. I got 0,003 even not %3. Why is this big difference? I am very confused, I was planning to use Rand Index for my paper work but I am afraid if I have to use the adjsuted one. There are some zeros and ones in the table, may be those are problem.</p>

<p><a href=""http://i.stack.imgur.com/oF7gh.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/oF7gh.png"" alt=""n=175 for both clustering A and clustering B""></a></p>
"
"0.0559892510955854","0.0381246425831512","232269","<p><strong>Clustering sea waves data in R</strong></p>

<p>I have proceed in clustering of storm's energy data using different clustering  methods (kmeans, hclust, agnes, funny) in R (always in 5 clusters) but even if it is easy to choose the best method for my work, I need a computational (and not theoretical) method or a package to compare and evaluate the methods via their results. Do you believe that there is something?</p>

<p>Thanks in advance,</p>
"
"0.0885267789745639","0.120560705542603","232590","<p>I have a number of groups with monthly data from 2010 to 2016. It's over 80 groups. I succesfully ran an ARIMA model with the montly data but with the sales data summed up (without groups). </p>

<p>Now I'd like to compare the performance with a per group model that runs an ARIMA model for each group and maybe later consider another type of grouping (geographical location, clustering, etc.)</p>

<p>I ran my original model with the following code:</p>

<pre><code>        Datos &lt;- read.csv(""C:/Users/borja.sanz/Desktop/Borja/Forecasting/V`enter code here`entas/Datos para Forecast.csv"")
        options(scipen=999)
        library(lubridate)
        Datos$Fecha = dmy(Datos$Fecha)

        #Declare time series
        tsDatos&lt;-ts(Datos$VentaLocal,start = c(2010,1),frequency = 12)
        plot(tsDatos)
        library(forecast)
        library(dplyr)

        #AutoArima Model
        m_aa = auto.arima(tsDatos)
        f_aa = forecast(m_aa, h=36)
        plot(f_aa)

#Create the forecasts along with the lower and upper bound
    forecast_df = data.frame(prediction=f_aa$mean,
                             abajo=f_aa$lower[,2],
                             arriba=f_aa$upper[,2],
                             date=last_date + seq(1/12, 3, by=1/12))
    forecast_df
</code></pre>

<p>This is how my data looks like:</p>

<pre><code>       Group    Year    Month   Date    Sales
1   2010    1   1/01/2010   134536.625
1   2010    2   1/02/2010   117506.625
1   2010    3   1/03/2010   132153.75
1   2010    4   1/04/2010   129723.125
1   2010    5   1/05/2010   135834.5
1   2010    6   1/06/2010   130115.375
1   2010    7   1/07/2010   144716
1   2010    8   1/08/2010   137195
1   2010    9   1/09/2010   137522.875
1   2010    10  1/10/2010   187063
1   2010    11  1/11/2010   162002.75
1   2010    12  1/12/2010   262297.375
1   2011    1   1/01/2011   177291.25
1   2011    2   1/02/2011   154816
1   2011    3   1/03/2011   171231.125
1   2011    4   1/04/2011   217717
1   2011    5   1/05/2011   178767.75
1   2011    6   1/06/2011   180817.75
1   2011    7   1/07/2011   216927.125
1   2011    8   1/08/2011   204509.125
1   2011    9   1/09/2011   199449.5
1   2011    10  1/10/2011   243812.125
1   2011    11  1/11/2011   232135.875
1   2011    12  1/12/2011   330854.75
1   2012    1   1/01/2012   217123.875
1   2012    2   1/02/2012   200558
1   2012    3   1/03/2012   215689.5
1   2012    4   1/04/2012   245500.25
1   2012    5   1/05/2012   219687.25
1   2012    6   1/06/2012   243345.625
1   2012    7   1/07/2012   249042
1   2012    8   1/08/2012   198443.75
1   2012    9   1/09/2012   209157.375
1   2012    10  1/10/2012   234089
1   2012    11  1/11/2012   237531
1   2012    12  1/12/2012   365301.25
1   2013    1   1/01/2013   211129.375
1   2013    2   1/02/2013   185249.625
1   2013    3   1/03/2013   256565.625
1   2013    4   1/04/2013   183549.5
1   2013    5   1/05/2013   189698.25
1   2013    6   1/06/2013   207955.625
1   2013    7   1/07/2013   230764.125
1   2013    8   1/08/2013   212551.625
1   2013    9   1/09/2013   201329.5
1   2013    10  1/10/2013   242745.125
1   2013    11  1/11/2013   261893.375
1   2013    12  1/12/2013   418313.25
1   2014    1   1/01/2014   205532.75
1   2014    2   1/02/2014   170487.75
1   2014    3   1/03/2014   196077
1   2014    4   1/04/2014   221760.875
1   2014    5   1/05/2014   198185
1   2014    6   1/06/2014   204919.25
1   2014    7   1/07/2014   218972.75
1   2014    8   1/08/2014   221439.875
1   2014    9   1/09/2014   195888.375
1   2014    10  1/10/2014   234595.75
1   2014    11  1/11/2014   259712.875
1   2014    12  1/12/2014   355691.875
1   2015    1   1/01/2015   205156.25
1   2015    2   1/02/2015   185358.875
1   2015    3   1/03/2015   218555.75
1   2015    4   1/04/2015   204233.625
1   2015    5   1/05/2015   212160.625
1   2015    6   1/06/2015   207217.25
1   2015    7   1/07/2015   225723.75
1   2015    8   1/08/2015   205902.625
1   2015    9   1/09/2015   196940.625
1   2015    10  1/10/2015   250916
1   2015    11  1/11/2015   236835.125
1   2015    12  1/12/2015   358327.625
2   2010    1   1/01/2010   227175.875
2   2010    2   1/02/2010   205042
2   2010    3   1/03/2010   239206.375
2   2010    4   1/04/2010   212059.875
2   2010    5   1/05/2010   232789
2   2010    6   1/06/2010   247876.125
2   2010    7   1/07/2010   278557
2   2010    8   1/08/2010   270410.125
2   2010    9   1/09/2010   251060.375
2   2010    10  1/10/2010   302738.625
2   2010    11  1/11/2010   266869.75
2   2010    12  1/12/2010   272978.75
2   2011    1   1/01/2011   238614.5
2   2011    2   1/02/2011   224240.375
2   2011    3   1/03/2011   245457.375
2   2011    4   1/04/2011   238583.5
2   2011    5   1/05/2011   252392.75
2   2011    6   1/06/2011   256749.5
2   2011    7   1/07/2011   264736.125
2   2011    8   1/08/2011   256414
2   2011    9   1/09/2011   242335.125
2   2011    10  1/10/2011   305224.75
2   2011    11  1/11/2011   289199.875
2   2011    12  1/12/2011   281807.75
2   2012    1   1/01/2012   244886.125
2   2012    2   1/02/2012   232062.375
2   2012    3   1/03/2012   264991.75
2   2012    4   1/04/2012   232750.5
2   2012    5   1/05/2012   248498.375
2   2012    6   1/06/2012   264290.875
2   2012    7   1/07/2012   272689.75
2   2012    8   1/08/2012   260441.25
2   2012    9   1/09/2012   251852.375
2   2012    10  1/10/2012   305929.625
2   2012    11  1/11/2012   276711.625
2   2012    12  1/12/2012   278672.875
2   2013    1   1/01/2013   242613.875
2   2013    2   1/02/2013   227575.75
2   2013    3   1/03/2013   250318.875
2   2013    4   1/04/2013   250150.375
2   2013    5   1/05/2013   258467.25
2   2013    6   1/06/2013   261359.25
2   2013    7   1/07/2013   279113.75
2   2013    8   1/08/2013   258699
2   2013    9   1/09/2013   244841.375
2   2013    10  1/10/2013   308197.25
2   2013    11  1/11/2013   284195.5
2   2013    12  1/12/2013   287718.75
2   2014    1   1/01/2014   239510.375
2   2014    2   1/02/2014   216338.125
2   2014    3   1/03/2014   245626.75
2   2014    4   1/04/2014   230619.875
2   2014    5   1/05/2014   251758.875
2   2014    6   1/06/2014   254946.75
2   2014    7   1/07/2014   276268.75
2   2014    8   1/08/2014   266151.75
2   2014    9   1/09/2014   245859.375
2   2014    10  1/10/2014   317797.5
2   2014    11  1/11/2014   283786.625
2   2014    12  1/12/2014   289767.875
2   2015    1   1/01/2015   244008
2   2015    2   1/02/2015   228638
2   2015    3   1/03/2015   260056
2   2015    4   1/04/2015   232560.875
2   2015    5   1/05/2015   252642.125
2   2015    6   1/06/2015   249018.5
2   2015    7   1/07/2015   278113.125
2   2015    8   1/08/2015   255851
2   2015    9   1/09/2015   263046.625
2   2015    10  1/10/2015   344240.75
2   2015    11  1/11/2015   295486.125
2   2015    12  1/12/2015   293499.375
</code></pre>

<p>I only included two groups in the sample. I would like to use a function like one of the apply (tapply, lapply, sapply, etc.) that can run an AUTO.ARIMA model per group. Then I would like to obtain the forecast for each group for x number of months and also if I could visualize the model coefficients.</p>
"
"0.0791807582464896","0.0808745799025788","232631","<p>I was searching for appliction of unsupervised learning in trading and came across this <a href=""https://www.r-bloggers.com/artificial-intelligence-in-trading-k-means-clustering/"" rel=""nofollow"">site</a>. I have understood most part of the code but some I have no idea whats happening.</p>

<p>This code is used for optimizing cluster number</p>

<pre><code>#optimal number of clusters
wss = (nrow(nasa)-1)*sum(apply(nasa,2,var))
for (i in 2:15) wss[i] = sum(kmeans(nasa, centers=i)$withinss)
wss=(data.frame(number=1:15,value=as.numeric(wss)))
</code></pre>

<p>what is the principle behind this optimization code?</p>

<p>The second code which I didnt undertand is of autocorrelation code?</p>

<pre><code>autocorrelation=head(cbind(kmeanObject$cluster,lag(as.xts(kmeanObject$cluster),-1)),-1)
xtabs(~autocorrelation[,1]+(autocorrelation[,2]))

y=apply(xtabs(~autocorrelation[,1]+(autocorrelation[,2])),1,sum)
x=xtabs(~autocorrelation[,1]+(autocorrelation[,2]))
</code></pre>

<p><code>autocorrelation</code> variable is the columns binded of cluster and one day lagged cluster value. From this why they used <code>xtabs</code> instead of <code>cov</code> function to calculate correlation.</p>

<p>The last doubt is how to get and read percentage table?</p>

<pre><code>    1         2     3         4     5
1   0.11    0.25    0.30    0.22    0.12
2   0.01    0.59    0.17    0.21    0.02
3   0.02    0.49    0.21    0.24    0.04
4   0.03    0.40    0.26    0.27    0.04
5   0.19    0.18    0.24    0.23    0.17
</code></pre>

<p>here 2x2 is 0.59 and 3x2 is 0.49 what this means?</p>

<p>lets say i did a kmeans clustering using new data</p>

<pre><code>newkmeanObject=kmeans(new_nasa,5,iter.max=10)
newkmeanObject$cluster
</code></pre>

<p>and obtained last cluster values as 2 so what is this means?</p>
"
"NaN","NaN","233801","<p>We are trying to interpret a heatmap that looks like this:</p>

<p><a href=""http://i.stack.imgur.com/syvVF.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/syvVF.png"" alt=""enter image description here""></a></p>

<p>... and plotted with this plotting code:</p>

<pre><code>heatmap(ourdata,
    col=cluster_colors,
    distfun = function(x) dist(x, method=""euclidean""),
    hclustfun = function(x) hclust(x, method=""complete""),
    margins=c(8,18),
);
</code></pre>

<p>Visually, it seems quite clear that there are different ""heights"" between the branches in the dendrogram, which we assume would correspond to relative distances between the columns and/or clusters.</p>

<p><strong>Question 1:</strong> Is this correct, do the heights correspond to clustering distances?</p>

<p><strong>Question 2:</strong> Where can we find evidence of this? We haven't been able to find the answer from <code>help(heatmap)</code> nor <code>help(hclust)</code> (nor <code>help(dist)</code>, although that is not expected either).</p>
"
