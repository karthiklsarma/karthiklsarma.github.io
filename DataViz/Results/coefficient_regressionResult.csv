"V1","V2","V3","V4"
"0.0304713817668003","0.029037395206952","  2854","<p>Dear all, 
I was encouraged to ask this question here as well as on stackoverflow and would be very appreciative of any answers...</p>

<p>Due to hetereoscedasticity I'm doing bootstrapped linear regression (appeals more to me than robust regression). I'd like to create a plot along the lines of what I've done in the script here. However the <code>fill=int</code> is not right since <code>int</code> should (I believe) be calculated using a bivariate normal distribution. </p>

<ul>
<li>Any idea how I could do that in this setting? </li>
<li>Also is there a way for <code>bootcov</code> to return bias-corrected percentiles?</li>
</ul>

<p>sample script:</p>

<pre><code>library(ggplot2) 
library(Hmisc) 
library(Design) # for ols()

o&lt;-data.frame(value=rnorm(10,20,5),
              bc=rnorm(1000,60,50),
              age=rnorm(1000,50,20),
              ai=as.factor(round(runif(1000,0,4),0)),
              Gs=as.factor(round(runif(1000,0,6),0))) 

reg.s&lt;-function(x){      
    ols(value~as.numeric(bc)+as.numeric(age),data=x,x=T,y=T)-&gt;temp 
    bootcov(temp,B=1000,coef.reps=T)-&gt;t2 

    return(t2) 
    } 

dlply(o,.(ai,Gs),function(x) reg.s(x))-&gt;b.list 
llply(b.list,function(x) x[[""boot.Coef""]])-&gt;b2 

ks&lt;-llply(names(b2),function(x){ 
    s&lt;-data.frame(b2[[x]]) 
    s$ai&lt;-x 
    return(s) 
    }) 


ks3&lt;-do.call(rbind,ks) 
ks3$ai2&lt;-with(ks3,substring(ai,1,1)) 

ks3$gc2&lt;-sapply(strsplit(as.character(ks3$ai), ""\\.""), ""[["", 2) 


k&lt;-ks3 
j&lt;-dlply(k,.(ai2,gc2),function(x){ 
    i1&lt;-quantile(x$Intercept,probs=c(0.025,0.975))[1] 
    i2&lt;-quantile(x$Intercept,probs=c(0.025,0.975))[2] 

    j1&lt;-quantile(x$bc,probs=c(0.025,0.975))[1] 
    j2&lt;-quantile(x$bc,probs=c(0.025,0.975))[2] 

    o&lt;-x$Intercept&gt;i1 &amp; x$Intercept&lt;i2 

    p&lt;-x$bc&gt;j1 &amp; x$bc&lt;j2 

    h&lt;-o &amp; p 
    return(h) 
    }) 

m&lt;-melt(j) 
ks3$int&lt;-m[,1]   

ggplot(ks3,aes(x=bc,y=Intercept,fill=int)) +
  geom_point(,alpha=0.3,size=1,shape=21) +
  facet_grid(gc2~ai2,scales = ""free_y"")+theme_bw()-&gt;plott 
plott&lt;-plott+opts(panel.grid.minor=theme_blank(),panel.grid.major=theme_blank()) 
plott&lt;-plott+geom_vline(x=0,color=""red"") 
plott+xlab(""BC coefficient"")+ylab(""Intercept"") 
</code></pre>
"
"0.0867230728520531","0.0826418755551823","  3497","<p>I have a fairly larege file 100M rows and 30 columns or so on which I would like to run multiple regressions. I have specialized code to run the regressions on the entire file, but what I would like to do is draw random samples from the file and run them in R.
The strategy is:
              randomly sample N rows from the file without replacement
              run a regression and save the coefficients of interest
              repeat this process M times with different samples 
              for each coefficient calculate the means and standard errors  of 
                the coefficeints over M runs.</p>

<p>I would like to interpret the mean computed over M runs as an estimate of the values of the coefficients computed on the whole data set, and the stadard errors of the means as estimates of the standard errors of the coefficients computed on the entire data set.</p>

<p>Experiments show this to be a promising strategy, but I am not sure about the underlying theory. Are my estimators consistent efficient and unbiased? If they are consistent how quickly should they converge? What tradeoffs of M and N are best?</p>

<p>I would very much appreciate it if someone could point me to the papers, books etc. with the relevanth theory.</p>

<p>Best regards and many thanks,</p>

<p>Joe Rickert</p>
"
"0.102279800236246","0.0974665016465992","  5087","<p>There are numerous procedures for functional data clustering based on orthonormal basis functions. I have a series of models built with the GAMM models, using the <code>gamm()</code> from the mgcv package in R. For fitting a long-term trend, I use a thin plate regression spline. Next to that, I introduce a CAR1 model in the random component to correct for autocorrelation. For more info, see eg the paper of Simon Wood on <a href=""http://r.789695.n4.nabble.com/attachment/2063352/0/tprs.pdf"">thin plate regression splines</a> or his <a href=""http://rads.stackoverflow.com/amzn/click/1584884746"">book on GAM models</a>.</p>

<p>Now I'm a bit puzzled in how I get the correct coefficients out of the models. And I'm even less confident that the coefficients I can extract, are the ones I should use to cluster different models. </p>

<p>A simple example, using:</p>

<pre><code>#runnable code
require(mgcv)
require(nlme)
library(RLRsim)
library(RColorBrewer)

x1 &lt;- 1:1000
x2 &lt;- runif(1000,10,500)

fx1 &lt;- -4*sin(x1/50)
fx2 &lt;- -10*(x2)^(1/4)
y &lt;- 60+ fx1 + fx2 + rnorm(1000,0,5)

test &lt;- gamm(y~s(x1)+s(x2))
# end runnable code
</code></pre>

<p>Then I can construct the original basis using smoothCon :</p>

<pre><code>#runnable code
um &lt;- smoothCon(s(x1),data=data.frame(x1=x1),
         knots=NULL,absorb.cons=FALSE)
#end runnable code
</code></pre>

<p>Now,when I look at the basis functions I can extract using </p>

<pre><code># runnable code
X &lt;- extract.lmeDesign(test$lme)$X
Z &lt;- extract.lmeDesign(test$lme)$Z

op &lt;- par(mfrow=c(2,5),mar=c(4,4,1,1))
plot(x1,X[,1],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,X[,2],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,8],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,7],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,6],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,5],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,4],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,3],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,2],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,1],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
par(op)
# end runnable code
</code></pre>

<p>they look already quite different. I can get the final coefficients used to build the smoother by</p>

<pre><code>#runnable code
Fcoef &lt;- test$lme$coef$fixed
Rcoef &lt;- unlist(test$lme$coef$random)
#end runnable code
</code></pre>

<p>but I'm far from sure these are the coefficients I look for. I fear I can't just use those coefficients as data in a clustering procedure. I would really like to know which coefficients are used to transform the basis functions from the ones I get with <code>smoothCon()</code> to the ones I extract from the lme-part of the gamm-object. And if possible, where I can find them. I've read the related articles, but somehow I fail to figure it out myself. All help is appreciated.</p>
"
"0.109866129394437","0.104695817324578","  5135","<p>the help pages in R assume I know what those numbers mean. I don't :)
I'm trying to really intuitively understand every number here. I will just post the output and comment on what I found out. There might (will) be mistakes, as I'll just write what I assume. Please correct me, and I will edit the wrong parts.<br>
Mainly I'd like to know what the t-value in the coefficients mean, and why they print the residual standard error. I hope someone can clarify that.</p>

<pre><code>Call:
lm(formula = iris$Sepal.Width ~ iris$Petal.Width)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.09907 -0.23626 -0.01064  0.23345  1.17532 
</code></pre>

<p>A 5-point-summary of the residuals (Their mean is always 0, right?). The numbers can be used  (I'm guessing here) to quickly see if there are any big outliers. Also you can already see it here if the residuals are far from normally distributed (they should be normally distributed).</p>

<pre><code>Coefficients:
                 Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)       3.30843    0.06210  53.278  &lt; 2e-16 ***
iris$Petal.Width -0.20936    0.04374  -4.786 4.07e-06 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>Estimates $\hat{\beta_i}$ , computed by least squares regression. Also, the standard error $\sigma_{\beta_i}$ . I'd like to know how this is calculated.<br>
Also, no idea where the t value and the corresponding p come from. I know $\hat{\beta}$ should be normal distributed, but how is the t value calculated?</p>

<pre><code>Residual standard error: 0.407 on 148 degrees of freedom
</code></pre>

<p>$\sqrt{ \frac{1}{n-p} \epsilon^T\epsilon }$ , I guess. But why do we calculate that, and what does it say us?</p>

<pre><code>Multiple R-squared: 0.134,  Adjusted R-squared: 0.1282 
</code></pre>

<p>$ R^2 = \frac{s_\hat{y}^2}{s_y^2} $ , which is $ \frac{\sum_{i=1}^n (\hat{y_i}-\bar{y})^2}{\sum_{i=1}^n (y_i-\bar{y})^2} $ . The ratio is close to 1 if the points lie on a straight line, and 0 if they are random.<br>
What is the adjusted R-squared?</p>

<pre><code>F-statistic: 22.91 on 1 and 148 DF,  p-value: 4.073e-06 
</code></pre>

<p>F and p for the <strong>whole</strong> model, not only for single $\beta_i$s as previous. The F value is $ \frac{s^2_{\hat{y}}}{\sum\epsilon_i} $ . The bigger it grows, the more unlikely it is that the $\beta$'s do not have any effect at all.</p>
"
"0.0806196982594614","0.076825726438694","  5304","<p>Dear everyone - I've noticed something strange that I can't explain, can you? In summary: the manual approach to calculating a confidence interval in a logistic regression model, and the R function <code>confint()</code> give different results.</p>

<p>I've been going through Hosmer &amp; Lemeshow's <em>Applied logistic regression</em> (2nd edition).  In the 3rd chapter there is an example of calculating the odds ratio and 95% confidence interval.  Using R, I can easily reproduce the model:</p>

<pre><code>Call:
glm(formula = dataset$CHD ~ as.factor(dataset$dich.age), family = ""binomial"")

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.734  -0.847  -0.847   0.709   1.549  

Coefficients:
                             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                   -0.8408     0.2551  -3.296  0.00098 ***
as.factor(dataset$dich.age)1   2.0935     0.5285   3.961 7.46e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 136.66  on 99  degrees of freedom
Residual deviance: 117.96  on 98  degrees of freedom
AIC: 121.96

Number of Fisher Scoring iterations: 4
</code></pre>

<p>However, when I calculate the confidence intervals of the parameters, I get a different interval to the one given in the text:</p>

<pre><code>&gt; exp(confint(model))
Waiting for profiling to be done...
                                 2.5 %     97.5 %
(Intercept)                  0.2566283  0.7013384
as.factor(dataset$dich.age)1 3.0293727 24.7013080
</code></pre>

<p>Hosmer &amp; Lemeshow suggest the following formula:</p>

<p>$$
e^{[\hat\beta_1\pm z_{1-\alpha/2}\times\hat{\text{SE}}(\hat\beta_1)]}
$$
</p>

<p>and they calculate the confidence interval for <code>as.factor(dataset$dich.age)1</code> to be (2.9, 22.9).</p>

<p>This seems straightforward to do in R:</p>

<pre><code># upper CI for beta
exp(summary(model)$coefficients[2,1]+1.96*summary(model)$coefficients[2,2])
# lower CI for beta
exp(summary(model)$coefficients[2,1]-1.96*summary(model)$coefficients[2,2])
</code></pre>

<p>gives the same answer as the book.</p>

<p>However, any thoughts on why <code>confint()</code> seems to give different results?  I've seen lots of examples of people using <code>confint()</code>.</p>
"
"0.068136080998913","0.0649295895722714","  5354","<p>I've got some data about airline flights (in a data frame called <code>flights</code>) and I would like to see if the flight time has any effect on the probability of a significantly delayed arrival (meaning 10 or more minutes). I figured I'd use logistic regression, with the flight time as the predictor and whether or not each flight was significantly delayed (a bunch of Bernoullis) as the response. I used the following code...</p>

<pre><code>flights$BigDelay &lt;- flights$ArrDelay &gt;= 10
delay.model &lt;- glm(BigDelay ~ ArrDelay, data=flights, family=binomial(link=""logit""))
summary(delay.model)
</code></pre>

<p>...but got the following output.</p>

<pre><code>&gt; flights$BigDelay &lt;- flights$ArrDelay &gt;= 10
&gt; delay.model &lt;- glm(BigDelay ~ ArrDelay, data=flights, family=binomial(link=""logit""))
Warning messages:
1: In glm.fit(x = X, y = Y, weights = weights, start = start, etastart = etastart,  :
  algorithm did not converge
2: In glm.fit(x = X, y = Y, weights = weights, start = start, etastart = etastart,  :
  fitted probabilities numerically 0 or 1 occurred
&gt; summary(delay.model)

Call:
glm(formula = BigDelay ~ ArrDelay, family = binomial(link = ""logit""),
    data = flights)

Deviance Residuals:
       Min          1Q      Median          3Q         Max
-3.843e-04  -2.107e-08  -2.107e-08   2.107e-08   3.814e-04

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  -312.14     170.26  -1.833   0.0668 .
ArrDelay       32.86      17.92   1.833   0.0668 .
---
Signif. codes:  0 Ã¢***Ã¢ 0.001 Ã¢**Ã¢ 0.01 Ã¢*Ã¢ 0.05 Ã¢.Ã¢ 0.1 Ã¢ Ã¢ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.8375e+06  on 2291292  degrees of freedom
Residual deviance: 9.1675e-03  on 2291291  degrees of freedom
AIC: 4.0092

Number of Fisher Scoring iterations: 25
</code></pre>

<p>What does it mean that the algorithm did not converge? I thought it be because the <code>BigDelay</code> values were <code>TRUE</code> and <code>FALSE</code> instead of <code>0</code> and <code>1</code>, but I got the same error after I converted everything. Any ideas?</p>
"
"0.0430930413588572","0.0410650781176591","  5434","<p>I am doing multiple regression with some data (5 predictors, 1 response). Since the response is discrete and non-negative, I thought I would try Poisson regression. However, the data are significantly overdispersed (variance > mean), so I am now trying negative binomial regression.</p>

<p>I was able to fit the model with this code.</p>

<pre><code>library(MASS)
model.nb &lt;- glm.nb(Response ~ Pred1 + Pred2 + Pred3 + Pred4 + Pred5 - 1, data=d)
</code></pre>

<p>Now I would like to see if I can get a better fit by including interactions between the predictors. However, when I try to do so, I get the following error.</p>

<pre><code>&gt; model.nb.intr &lt;- glm.nb(Response ~ Pred1 * Pred2 * Pred3 * Pred4 * Pred5 - 1, data=d)
Error: no valid set of coefficients has been found: please supply starting values
</code></pre>

<p>Any ideas what may be causing this?</p>
"
"0.0430930413588572","0.0410650781176591","  5952","<p>Is there a way of plotting the regression line of a piecewise model like this, other than using <code>lines</code> to plot each segment separately, or using <code>geom_smooth(aes(group=Ind), method=""lm"", fill=FALSE)</code> ?</p>

<pre><code>m.sqft &lt;- mean(sqft)
model &lt;- lm(price~sqft+I((sqft-m.sqft)*Ind))
# sqft, price: continuous variables, Ind: if sqft&gt;mean(sqft) then 1 else 0

plot(sqft,price)
abline(reg = model)
Warning message:
In abline(reg = model) :
  only using the first two of 3regression coefficients
</code></pre>

<p>Thank you.</p>
"
"0.134074079773921","0.127764538910589","  6141","<p>I am now writing my bachelors thesis and I have come across some difficulties. I am about to do some panel regressions with time and entity fixed effects and I would therefore like to use the plm package. But when I do add fixed effects and want to have heteroscedasticity robust standard errors they seem to be incorrect.</p>

<p>Does anyone know why the HC standard errors differ?</p>

<p>Here is my code</p>

<pre><code># Load data
load(file=""panel"")
attach(panel)

# Load packages
library(lmtest)
library(plm)


# Create two models. The lm.model is a linear model and as the
# LAND variable is a factor variable representing countries
# (Land = Country in Swedish) this model will have entity fixed
# effects. In the plm.model the plm package is used and
# individual effects and within model is turned on (which is
# the same as entity fixed effects)
lm.model&lt;-lm(NETTOSPARANDE ~ EURO + LAND, data=panel)
plm.model&lt;-plm(NETTOSPARANDE ~ EURO, index=c(""LAND"",""Ã…R""), effect=""individual"", model=""within"", data=panel)

# When looking at the coefficents without heteroscadisity robust
# standard errors they are identical. They do also have the same
# value in stata.
coeftest(lm.model)[1:2,]
coeftest(plm.model)

# But when looking at the coefficents using heteroscadisity
# robust standard errors the lm.model and the plm.model produces
# different standard errors.
coeftest(lm.model, vcov.=vcovHC(lm.model, method=""white2"", type=""HC1""))[1:2,]
coeftest(plm.model, vcov.=vcovHC(plm.model, method=""white2"", type=""HC1""))
</code></pre>

<p>If you want to test the data it can be found here (the panel file)
  [1]: <a href=""https://sourceforge.net/projects/emumoralhazard/files/"" rel=""nofollow"">https://sourceforge.net/projects/emumoralhazard/files/</a> <em>R-data</em></p>

<p>And here is my output</p>

<pre><code>1&gt; # Load data
1&gt; load(file=""panel"")

1&gt; attach(panel)

1&gt; # Load packages
1&gt; library(lmtest)
Loading required package: zoo

1&gt; library(plm)
Loading required package: kinship
Loading required package: survival
Loading required package: splines
Loading required package: nlme
Loading required package: lattice
[1] ""kinship is loaded""
Loading required package: Formula
Loading required package: MASS
Loading required package: sandwich

1&gt; # Create two models. The lm.model is a linear model and as the
1&gt; # LAND variabel is a factor variable representing countries
1&gt; # (Land = Country in swedish) this model will have entity fixed
1&gt; # effects. In the plm.model the plm package is used and
1&gt; # individual effects and within model is turned on (which is
1&gt; # the same as entity fixed effects)
1&gt; lm.model&lt;-lm(NETTOSPARANDE ~ EURO + LAND, data=panel)

1&gt; plm.model&lt;-plm(NETTOSPARANDE ~ EURO, index=c(""LAND"",""Ã…R""), effect=""individual"", model=""within"", data=panel)

1&gt; # When looking at the coefficients without heteroscedasticity robust
1&gt; # standard errors they are identical. They do also have the same
1&gt; # value in Stata.
1&gt; coeftest(lm.model)[1:2,]
             Estimate Std. Error   t value     Pr(&gt;|t|)
(Intercept) -3.731024  0.7731778 -4.825570 1.726921e-06
EURO1        2.187170  0.4076720  5.365024 1.112984e-07

1&gt; coeftest(plm.model)

t test of coefficients:

      Estimate Std. Error t value  Pr(&gt;|t|)    
EURO1  2.18717    0.40767   5.365 1.113e-07 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 


1&gt; # But when looking at the coefficients using heteroscedasticity 
1&gt; # robust standard errors the lm.model and the plm.model produces
1&gt; # different standard errors.
1&gt; coeftest(lm.model, vcov.=vcovHC(lm.model, method=""white2"", type=""HC1""))[1:2,]
             Estimate Std. Error    t value     Pr(&gt;|t|)
(Intercept) -3.731024  0.3551280 -10.506138 5.102122e-24
EURO1        2.187170  0.3386029   6.459395 2.009894e-10

1&gt; coeftest(plm.model, vcov.=vcovHC(plm.model, method=""white2"", type=""HC1""))

t test of coefficients:

      Estimate Std. Error t value  Pr(&gt;|t|)    
EURO1  2.18717    0.33849  6.4615 1.983e-10 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>
"
"NaN","NaN","  6214","<p>How should I define a model formula in R, when one (or more) exact linear restrictions binding the coefficients is available. As an example, say that you know that b1 = 2*b0 in a simple linear regression model. </p>

<p>Thank you!</p>
"
"0.035185320931284","0.0502942438178979","  6400","<p>I am conducting a mulitple first order regression analysis of genetic data. The vectors of y-values do not all follow a normal distribution, therefore I need to implement a non-parametric regression using ranks.</p>

<p>Is the <code>lm()</code> function in R suitable for this, i.e.,</p>

<pre><code>lin.reg &lt;- lm(Y~X*Z)
</code></pre>

<p>where Y, X and Z are vectors of ordinal categorical variables?</p>

<p>I am interested in the p-value assigned to the coefficient of the interaction term in the first order model. The <code>lm()</code> function obtains this from a t-test, i.e., is the interaction coefficient significantly different from zero.</p>

<p>Is the automatic implementation of a t-test to determine this p-value appropriate when the regression model is carried out on data as described?</p>

<p>Thanks.</p>

<p><strong>EDIT</strong></p>

<p>Sample data for clarity:</p>

<pre><code>Y &lt;- c(4, 1, 2, 3) # A vector of ranks
X &lt;- c(0, 2, 1, 1) # A vector of genotypes (0 = aa, 1 = ab, 2 = bb)
Z &lt;- c(2, 2, 1, 0)
</code></pre>
"
"0.0430930413588572","0.0410650781176591","  6734","<p>I have been reading the description of ridge regression in <em><a href=""http://rads.stackoverflow.com/amzn/click/007310874X"" rel=""nofollow"">Applied Linear Statistical Models</em>, 5th Ed</a> chapter 11. The ridge regression is done on body fat data available <a href=""http://www.cst.cmich.edu/users/lee1c/spss/V16_materials/DataSets_v16/BodyFat-TxtFormat.txt"" rel=""nofollow"">here</a>. </p>

<p>The textbook matches the output in SAS, where the back transformed coefficients are given in the fitted model as:<br>
$$
Y=-7.3978+0.5553X_1+0.3681X_2-0.1917X_3
$$</p>

<p>This is shown from SAS as:</p>

<pre><code>proc reg data = ch7tab1a outest = temp outstb noprint;
  model y = x1-x3 / ridge = 0.02;
run;
quit;
proc print data = temp;
  where _ridge_ = 0.02 and y = -1;
  var y intercept x1 x2 x3;
run;
Obs     Y    Intercept       X1         X2         X3

 2     -1     -7.40343    0.55535    0.36814    -0.19163
 3     -1      0.00000    0.54633    0.37740    -0.13687
</code></pre>

<p>But R gives very different coefficients:</p>

<pre><code>data &lt;- read.table(""http://www.cst.cmich.edu/users/lee1c/spss/V16_materials/DataSets_v16/BodyFat-TxtFormat.txt"", 
                   sep="" "", header=FALSE)
data &lt;- data[,c(1,3,5,7)]
colnames(data)&lt;-c(""x1"",""x2"",""x3"",""y"")
ridge&lt;-lm.ridge(y ~ ., data, lambda=0.02)   
ridge$coef
coef(ridge)

&gt;   ridge$coef
       x1        x2        x3 
10.126984 -4.682273 -3.527010 
&gt;   coef(ridge)
                   x1         x2         x3 
42.2181995  2.0683914 -0.9177207 -0.9921824 
&gt; 
</code></pre>

<p>Can anyone help me understand why?</p>
"
"0.0746393370862076","0.0711268017165705","  7595","<p>I've been using the <code>lm</code> function in R to do demand modeling (tons of steel to be predicted by various economic indicators).  I used $R^2$  and $F$ to report on the strength of the model.  However, when I use the R function <code>lqs</code> (""resistant regression"") and then type in <code>summary(model_name)</code> I do not get any statistics that I can use to report on the strength of the regression model.  Any suggestions? </p>

<p>EDIT:
Thanks for your quick response. I don't have a problem with lqs(). The problem is that when I type in summary(Model) I do not get any goodness of fit information (e.g., adjusted R squared) as I do when I enter summary(x) where X is a model created using the lm function. I'd like to have something to show the strength of the model. I""m using MASS. See below. Regards, Bill Yarberry</p>

<p>library(MASS)</p>

<pre><code>M10 = lqs(agri ~ p12+p1+p11+p5+p8+p6+p25+p50+p35, data = agri_data2) summary(M10) Length Class Mode
crit 1 -none- numeric
sing 1 -none- character coefficients 10 -none- numeric
bestone 10 -none- numeric
fitted.values 103 -none- numeric
residuals 103 -none- numeric
scale 2 -none- numeric
terms 3 terms call
call 3 -none- call
xlevels 0 -none- list
model 10 data.frame list
</code></pre>
"
"0.0867230728520531","0.0918243061724248","  7720","<p>I am new to R, ordered logistic regression, and <code>polr</code>.</p>

<p>The ""Examples"" section at the bottom of the help page for <a href=""http://stat.ethz.ch/R-manual/R-patched/library/MASS/html/polr.html"">polr</a> (that fits a logistic or probit regression model to an ordered factor response) shows</p>

<pre><code>options(contrasts = c(""contr.treatment"", ""contr.poly""))
house.plr &lt;- polr(Sat ~ Infl + Type + Cont, weights = Freq, data = housing)
pr &lt;- profile(house.plr)
plot(pr)
pairs(pr)
</code></pre>

<ul>
<li><p>What information does <code>pr</code> contain?  The help page on <a href=""http://stat.ethz.ch/R-manual/R-patched/library/stats/html/profile.html"">profile</a> is
generic, and gives no guidance for polr.</p></li>
<li><p>What is <code>plot(pr)</code> showing?  I see six graphs. Each has an X axis that is
numeric, although the label is an indicator variable (looks like an input variable that is an indicator for an ordinal value).  Then the Y axis
is ""tau"" which is completely unexplained.</p></li>
<li><p>What is <code>pairs(pr)</code> showing?  It looks like a plot for each pair of input
variables, but again I see no explanation of the X or Y axes.</p></li>
<li><p>How can one understand if the model gave a good fit?
<code>summary(house.plr)</code> shows Residual Deviance 3479.149 and AIC (Akaike
Information Criterion?) of 3495.149.  Is that good?  In the case those
are only useful as relative measures (i.e. to compare to another model
fit), what is a good absolute measure?  Is the residual deviance approximately chi-squared distributed?  Can one use ""% correctly predicted"" on the original data or some cross-validation?  What is the easiest way to do that?</p></li>
<li><p>How does one apply and interpret <code>anova</code> on this model?  The docs say ""There are methods for the standard model-fitting functions, including predict, summary, vcov, anova.""  However, running <code>anova(house.plr)</code> results in <code>anova is not implemented for a single ""polr"" object</code></p></li>
<li><p>How does one interpret the t values for each coefficient?  Unlike some
model fits, there are no P values here.</p></li>
</ul>

<p>I realize this is a lot of questions, but it makes sense to me to ask as one bundle (""how do I use this thing?"") rather than 7 different questions.  Any information appreciated.</p>
"
"0.068136080998913","0.0519436716578171","  7775","<p>Does anyone have suggestions or packages that will calculate the coefficient of partial determination?</p>

<p>The coefficient of partial determination can be defined as the percent of variation that cannot be explained in a reduced model, but can be explained by the predictors specified in a full(er) model. This coefficient is used to provide insight into whether or not one or more additional predictors may be useful in a more fully specified regression model.</p>

<p>The calculation for the partial r^2 is relatively straight forward after estimating your two models and generating the ANOVA tables for them. The calculation for the partial r^2 is:</p>

<p>(SSEreduced - SSEfull) / SSEreduced</p>

<p>I've written this relatively simple function that will calculate this for a multiple linear regression model. I'm unfamiliar with other model structures in R where this function may not perform as well:</p>

<pre><code>partialR2 &lt;- function(model.full, model.reduced){
    anova.full &lt;- anova(model.full)
    anova.reduced &lt;- anova(model.reduced)

    sse.full &lt;- tail(anova.full$""Sum Sq"", 1)
    sse.reduced &lt;- tail(anova.reduced$""Sum Sq"", 1)

    pR2 &lt;- (sse.reduced - sse.full) / sse.reduced
    return(pR2)

    }
</code></pre>

<p>Any suggestions or tips on more robust functions to accomplish this task and/or more efficient implementations of the above code would be much appreciated.</p>
"
"0.101062140164153","0.0963061447907242","  7996","<p>I am evaluating a scenario's output parameter's dependence on three parameters: A, B and C. For this, I am conducting the following experiments:</p>

<ul>
<li>Fix A+B, Vary C - Total four sets of (A+B) each having 4 variations of C</li>
<li>Fix B+C, Vary A - Total four sets of (B+C) each having 3 variations of C</li>
<li>Fix C+A, Vary B - Total four sets of (C+A) each having 6 variations of C</li>
</ul>

<p>The output of any simulation is the value of a variable over time. For instance, A could be the area, B could be the velocity and C could be the number of vehicles. The output variable I am observing is the number of car crashes over time. </p>

<p>I am trying to determine which parameter(s) dominate the outcome of the experiment. By dominate, I mean that sometimes, the outcomes just does not change when one of the parameters change but when some other parameter is changed even by a small amount, a large change in the output is observed. I need to capture this effect and output some analysis from which I can understand the dependence of the output on the input parameters. A friend suggested Sensitivity Analysis but am not sure if there are simpler ways of doing it. Can someone please help me with a good (possibly easy because I don't have a Stats background) technique? It would be great if all this can be done in R.</p>

<p><strong>Update:</strong> 
I used linear regression to obtain the following:</p>

<pre><code>lm(formula = T ~ A + S + V)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.35928 -0.06842 -0.00698  0.05591  0.42844 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -0.01606    0.16437  -0.098 0.923391    
A            0.80199    0.15792   5.078 0.000112 ***
S           -0.27440    0.13160  -2.085 0.053441 .  
V           -0.31898    0.14889  -2.142 0.047892 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.1665 on 16 degrees of freedom
Multiple R-squared: 0.6563, Adjusted R-squared: 0.5919 
F-statistic: 10.18 on 3 and 16 DF,  p-value: 0.0005416 
</code></pre>

<p>Does this mean that the output depends mostly on A and less on V?</p>
"
"0.136426928841569","0.136197456102909","  8340","<p><strong>Update: I wanted to clarify that this is a simulation. Sorry if I confused everyone. I have also used meaningful names for my variables.</strong></p>

<p>I am not a statistician so please correct me if I make a blunder in explaining what I want. In regard to my <a href=""http://stats.stackexchange.com/questions/7996/what-is-a-good-way-of-estimating-the-dependence-of-an-output-variable-on-the-inpu"">previous question</a>, I have reproduced parts of my question here for reference.</p>

<blockquote>
  <p>I am evaluating a scenario's output
  dependence on three
  variables: Area, Speed and NumOfVehicles. For this, I am
  conducting the following experiments:</p>
  
  <ul>
  <li>Fix Area+Speed, Vary NumOfVehicles - Total four sets of (Area+Speed) each having 4 variations of NumOfVehicles</li>
  <li>Fix Speed+NumOfVehicles, Vary Area - Total four sets of (Speed+NumOfVehicles) each having 3 variations of Area</li>
  <li>Fix NumOfVehicles+Area, Vary Speed - Total four sets of (NumOfVehicles+Area) each having 6 variations of Speed</li>
  </ul>
  
  <p>The output of any simulation is the
  value of a variable over time. The output
  variable I am observing is the time at which 80% of the cars crashe.</p>
  
  <p>I am trying to determine which
  parameter(s) dominate the outcome of
  the experiment. By dominate, I mean
  that sometimes, the outcomes just does
  not change when one of the parameters
  change but when some other parameter
  is changed even by a small amount, a
  large change in the output is
  observed. I need to capture this
  effect and output some analysis from
  which I can understand the dependence
  of the output on the input parameters.
  A friend suggested Sensitivity
  Analysis but am not sure if there are
  simpler ways of doing it. Can someone
  please help me with a good (possibly
  easy because I don't have a Stats
  background) technique? It would be
  great if all this can be done in R.</p>
</blockquote>

<p>My previous result was not very satisfactory looking at the regression results. So what I did was that I went ahead and repeated all my experiments 20 times each with different variations of each variable (so for instance, instead of 4 variations of Area, I now have 8 and so on). Following is the summary I obtained out of R after using linear regression:</p>

<pre><code>Call:
lm(formula = T ~ Area + Speed + NumOfVehicles)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.13315 -0.06332 -0.01346  0.04484  0.29676 

Coefficients:
                 Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)      0.04285    0.02953   1.451    0.148    
Area             0.70285    0.02390  29.406  &lt; 2e-16 ***
Speed           -0.15560    0.02080  -7.479 2.12e-12 ***
NumOfVehicles   -0.27447    0.02927  -9.376  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.08659 on 206 degrees of freedom
Multiple R-squared: 0.8304, Adjusted R-squared: 0.8279 
F-statistic: 336.2 on 3 and 206 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>as opposed to my previous result:</p>

<pre><code>lm(formula = T ~ Area + Speed + NumOfVehicles)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.35928 -0.06842 -0.00698  0.05591  0.42844 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   -0.01606    0.16437  -0.098 0.923391    
Area           0.80199    0.15792   5.078 0.000112 ***
Speed         -0.27440    0.13160  -2.085 0.053441 .  
NumOfVehicles -0.31898    0.14889  -2.142 0.047892 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.1665 on 16 degrees of freedom
Multiple R-squared: 0.6563, Adjusted R-squared: 0.5919 
F-statistic: 10.18 on 3 and 16 DF,  p-value: 0.0005416 
</code></pre>

<p>From my understanding, my current results have a lower standard error so that is good. In addition the Pr value also seems quite low which tells me that this result is better than my previous result. So can I go ahead and say that A has the maximum effect on the output and then come S and V in that order? Can I make any other deductions from this result?</p>

<p>Also, I was suggested that I look into adding additional variates like $A^2$ etc. but if $A$ is the area, what does saying ""time"" depends on $A^2$ actually mean? </p>
"
"0.075412822378","0.0821301562353182","  8511","<p>Christopher Manning's <a href=""http://nlp.stanford.edu/~manning/courses/ling289/logistic.pdf"" rel=""nofollow"">writeup on logistic regression in R</a> shows a logistic regression in R as follows:</p>

<pre><code>ced.logr &lt;- glm(ced.del ~ cat + follows + factor(class), 
  family=binomial)
</code></pre>

<p>Some output:</p>

<pre><code>&gt; summary(ced.logr)
Call:
glm(formula = ced.del ~ cat + follows + factor(class),
    family = binomial(""logit""))
Deviance Residuals:
Min            1Q    Median       3Q      Max
-3.24384 -1.34325   0.04954  1.01488  6.40094

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   -1.31827    0.12221 -10.787 &lt; 2e-16
catd          -0.16931    0.10032  -1.688 0.091459
catm           0.17858    0.08952   1.995 0.046053
catn           0.66672    0.09651   6.908 4.91e-12
catv          -0.76754    0.21844  -3.514 0.000442
followsP       0.95255    0.07400  12.872 &lt; 2e-16
followsV       0.53408    0.05660   9.436 &lt; 2e-16
factor(class)2 1.27045    0.10320  12.310 &lt; 2e-16
factor(class)3 1.04805    0.10355  10.122 &lt; 2e-16
factor(class)4 1.37425    0.10155  13.532 &lt; 2e-16
(Dispersion parameter for binomial family taken to be 1)
Null deviance: 958.66 on 51 degrees of freedom
Residual deviance: 198.63 on 42 degrees of freedom
AIC: 446.10
Number of Fisher Scoring iterations: 4
</code></pre>

<p>He then goes into some detail about how to interpret coefficients, compare different models, and so on.  Quite useful.</p>

<p>However, how much variance does the model account for?  A <a href=""http://www.ats.ucla.edu/stat/stata/output/old/lognoframe.htm"" rel=""nofollow"">Stata page on logistic regression</a> says:</p>

<blockquote>
  <p>Technically, R2 cannot be computed the same way in logistic regression as it is in OLS regression. The pseudo-R2, in logistic regression, is defined as 1 - L1/L0, where L0 represents the log likelihood for the ""constant-only"" model and L1 is the log likelihood for the full model with constant and predictors. </p>
</blockquote>

<p>I understand this at the high level. The constant-only model would be without any of the parameters (only the intercept term).  Log likelihood is a measure of how closely the parameters fit the data.  In fact, Manning sort of hints that the deviance might be -2 log L. Perhaps null deviance is constant-only and residual deviance is -2 log L of the model?  However, I'm not crystal clear on it.</p>

<p>Can someone verify how one actually computes the pseudo-R^2 in R using this example?</p>
"
"0.075412822378","0.0821301562353182","  8545","<p>I have some problems in using (and finding) the Chow test for structural breaks in a regression analysis using R. I want to find out if there are some structural changes including another variable (represents 3 spatial subregions).</p>

<p>Namely, is the regression with the subregions better than the overall model. Therefore I need some statistical validation. </p>

<p>I hope my problem is clear, isn't it?</p>

<p>Kind regards<br>
marco</p>

<p>Toy example in R:</p>

<pre><code>library(mlbench) # dataset
data(""BostonHousing"")

# data preparation
BostonHousing$region &lt;- ifelse(BostonHousing$medv &lt;= 
                               quantile(BostonHousing$medv)[2], 1, 
                        ifelse(BostonHousing$medv &lt;= 
                               quantile(BostonHousing$medv)[3], 2,
                        ifelse(BostonHousing$medv &gt; 
                               quantile(BostonHousing$medv)[4], 3, 1)))

BostonHousing$region &lt;- as.factor(BostonHousing$region)

# regression without any subregion 
reg1&lt;- lm(medv ~ crim + indus + rm, data=BostonHousing)

summary(reg1)

# are there structural breaks using the factor ""region"" which
# indicates 3 spatial subregions
reg2&lt;- lm(medv ~ crim + indus + rm + region, data=BostonHousing)
</code></pre>

<p>------- subsequent entry</p>

<p>I struggled with your suggested package ""strucchange"", not knowing how to use the ""from"" and ""to"" arguments correctly with my factor ""region"". Nevertheless, I found one hint to calculate it by hand (https://stat.ethz.ch/pipermail/r-help/2007-June/133540.html). This results in the following output, but now I am not sure if my interpetation is valid. The results from the example above below.</p>

<p>Does this mean that region 3 is significant different from region 1? Contrary, region 2 is not? Further, each parameter (eg region1:crim) represents the beta for each regime and the model for this region respectively? Finally, the ANOVA states that there is a signif. difference between these models and that the consideration of regimes leads to a better model?</p>

<p>Thank you for your advices!
Best Marco</p>

<pre><code>fm0 &lt;- lm(medv ~ crim + indus + rm, data=BostonHousing)
summary(fm0)
fm1 &lt;- lm(medv  ~ region / (crim + indus + rm), data=BostonHousing)
summary(fm1)
anova(fm0, fm1)
</code></pre>

<p>Results:</p>

<pre><code>Call:
lm(formula = medv ~ region/(crim + indus + rm), data = BostonHousing)

Residuals:
       Min         1Q     Median         3Q        Max 
-21.079383  -1.899551   0.005642   1.745593  23.588334 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)    12.40774    3.07656   4.033 6.38e-05 ***
region2         6.01111    7.25917   0.828 0.408030    
region3       -34.65903    4.95836  -6.990 8.95e-12 ***
region1:crim   -0.19758    0.02415  -8.182 2.39e-15 ***
region2:crim   -0.03883    0.11787  -0.329 0.741954    
region3:crim    0.78882    0.22454   3.513 0.000484 ***
region1:indus  -0.34420    0.04314  -7.978 1.04e-14 ***
region2:indus  -0.02127    0.06172  -0.345 0.730550    
region3:indus   0.33876    0.09244   3.665 0.000275 ***
region1:rm      1.85877    0.47409   3.921 0.000101 ***
region2:rm      0.20768    1.10873   0.187 0.851491    
region3:rm      7.78018    0.53402  14.569  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 4.008 on 494 degrees of freedom
Multiple R-squared: 0.8142,     Adjusted R-squared: 0.8101 
F-statistic: 196.8 on 11 and 494 DF,  p-value: &lt; 2.2e-16

&gt; anova(fm0, fm1)
Analysis of Variance Table

Model 1: medv ~ crim + indus + rm
Model 2: medv ~ region/(crim + indus + rm)
  Res.Df     RSS Df Sum of Sq     F    Pr(&gt;F)    
1    502 18559.4                                 
2    494  7936.6  8     10623 82.65 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.035185320931284","0.0502942438178979","  8661","<p>I'm trying to undertake a logistic regression analysis in <code>R</code>. I have attended courses covering this material using STATA. I am finding it very difficult to replicate functionality in <code>R</code>. Is it mature in this area? There seems to be little documentation or guidance available. Producing odds ratio output seems to require installing <code>epicalc</code> and/or <code>epitools</code> and/or others, none of which I can get to work, are outdated or lack documentation. I've used <code>glm</code> to do the logistic regression. Any suggestions would be welcome.  </p>

<p>I'd better make this a real question. How do I run a logistic regression and produce odds rations in <code>R</code>?  </p>

<p>Here's what I've done for a univariate analysis:  </p>

<p><code>x = glm(Outcome ~ Age, family=binomial(link=""logit""))</code>  </p>

<p>And for multivariate:  </p>

<p><code>y = glm(Outcome ~ Age + B + C, family=binomial(link=""logit""))</code>  </p>

<p>I've then looked at <code>x</code>, <code>y</code>, <code>summary(x)</code> and <code>summary(y)</code>.  </p>

<p>Is <code>x$coefficients</code> of any value?</p>
"
"0.0304713817668003","0.029037395206952","  8750","<p>If I have an arima object like <code>a</code>:</p>

<pre><code>set.seed(100)
x1 &lt;- cumsum(runif(100))
x2 &lt;- c(rnorm(25, 20), rep(0, 75))
x3 &lt;- x1 + x2

dummy = c(rep(1, 25), rep(0, 75))

a &lt;- arima(x3, order=c(0, 1, 0), xreg=dummy)
print(a)
</code></pre>

<p>.</p>

<pre><code>Series: x3 
ARIMA(0,1,0)                    

Call: arima(x = x3, order = c(0, 1, 0), xreg = dummy) 

Coefficients:
        dummy
      17.7665
s.e.   1.1434

sigma^2 estimated as 1.307:  log likelihood = -153.74
AIC = 311.48   AICc = 311.6   BIC = 316.67
</code></pre>

<p>How do calculate the R squared of this regression?</p>
"
"0.0304713817668003","0.029037395206952","  9027","<p>I have two logistic regression models in R made with <code>glm()</code>.  They both use the same variables, but were made using different subsets of a matrix.  Is there an easy way to get an average model which gives the means of the coefficients and then use this with the predict() function?</p>

<p>[ sorry if this type of question should be posted on a programming site let me know and I'll post it there ]</p>

<p>Thanks</p>
"
"0.0812570180448007","0.0774330538852055","  9237","<p>I have several dependent variables that are measures of racial disproportionality; I've calculated them as:</p>

<p>% of events caused by racial minority group / % of events caused by racial majority group</p>

<p>I have a dependent variable for each racial minority group in my sample. I am running longitudinal Generalized Estimating Equations (GEE) on these models, however I am somewhat stumped as to which family is appropriate for these dependent variables. The probability range for my ratios are truncated at 0, as it's not possible to have negative values in my DVs. This makes me question the validity of using a Gaussian family for my models.</p>

<p>The idea behind these variables is that a ratio greater than 1 indicates some level of greater burden of events that a given racial minority is bearing compared to the racial majority, and a ratio less than 1 indicates the opposite.</p>

<ul>
<li>What would be the most appropriate family to use for my GEE regressions?</li>
</ul>

<p>EDIT:</p>

<p>I misspoke about the racial disproportionality measure I was using. The correct formula is:</p>

<p>% events by minority / % of total enrollment that is minority OVER
% events by non-minority / % of total enrollment that is non-minority</p>

<p>Because they are ratios, the number of observations with value less than 1 is comparable to the number of observations greater than 1, with the lower bound being 0 and the upper bound being non-bounded. Looking at the histograms of my response variables, they definitely seem to fit a negative binomial distribution better than the normal. The QIC (GEE adjustment to AIC) confirms this suspicion. My questions now are:</p>

<ul>
<li>Can I trust this evidence to move forward with the negative binomial family?</li>
<li>If so, how do I possibly interpret the exponentiated coefficients from the resulting models? They don't see to be Incidence Rate Ratios, as one would interpret them to be from count variables...</li>
</ul>
"
"0.0963589698356145","0.0918243061724248"," 10036","<p>I am using R to replicate a study and obtain mostly the same results the author reported. At one point, however, I calculate marginal effects that seem to be unrealistically small. I would greatly appreciate if you could have a look at my reasoning and the code below and see if I am mistaken at one point or another.</p>

<p>My sample contains 24535 observations, the dependent variable <code>x028bin</code> is a binary variable taking on the values 0 and 1, and there are furthermore 10 explaining variables. Nine of those independent variables have numeric levels, the independent variable <code>f025grouped</code> is a factor consisting of different religious denominations.</p>

<p>I would like to run a probit regression including dummies for religious denomination and then compute marginal effects. In order to do so, I first eliminate missing values and use cross-tabs between the dependent and independent variables to verify that there are no small or 0 cells. Then I run the probit model which works fine and I also obtain reasonable results:</p>

<pre><code>probit4AKIE &lt;- glm(x028bin ~ x003 + x003squ + x025secv2 + x025terv2 + x007bin + x04chief + x011rec + a009bin + x045mod + c001bin + f025grouped, family=binomial(link=""probit""), data=wvshm5red2delna, na.action=na.pass)

summary(probit4AKIE)
</code></pre>

<p>However, when calculating marginal effects with all variables at their means from the probit coefficients and a scale factor, the marginal effects I obtain are much too small (e.g. 2.6042e-78). The code looks like this:</p>

<pre><code>ttt &lt;- cbind(wvshm5red2delna$x003,
    wvshm5red2delna$x003squ, wvshm5red2delna$x025secv2, wvshm5red2delna$x025terv2,
wvshm5red2delna$x007bin, wvshm5red2delna$x04chief, wvshm5red2delna$x011rec,
    wvshm5red2delna$a009bin, wvshm5red2delna$x045mod, wvshm5red2delna$c001bin,
wvshm5red2delna$f025grouped, wvshm5red2delna$f025grouped,wvshm5red2delna$f025grouped,
    wvshm5red2delna$f025grouped,wvshm5red2delna$f025grouped,wvshm5red2delna$f025grouped,
wvshm5red2delna$f025grouped,wvshm5red2delna$f025grouped,
wvshm5red2delna$f025grouped) #I put variable ""f025grouped"" 9 times because this variable consists of 9 levels

ttt &lt;- as.data.frame(ttt)

xbar &lt;- as.matrix(mean(cbind(1,ttt[1:19]))) #1:19 position of variables in dataframe ttt

betaprobit4AKIE &lt;- probit4AKIE$coefficients

zxbar &lt;- t(xbar) %*% betaprobit4AKIE

scalefactor &lt;- dnorm(zxbar)

marginprobit4AKIE &lt;- scalefactor * betaprobit4AKIE[2:20] 

#(2:20 are the positions of variables in the output of the probit model 'probit4AKIE' 
#(variables need to be in the same ordering as in data.frame ttt), the constant in   
#the model occupies the first position)

marginprobit4AKIE #in this step I obtain values that are much too small
</code></pre>
"
"0.068136080998913","0.0649295895722714"," 10316","<p>I'm working on a multiple logistic regression in R using <code>glm</code>. The predictor variables are continuous and categorical. An extract of the summary of the model shows the following:</p>

<pre><code>Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   2.451e+00  2.439e+00   1.005   0.3150
Age           5.747e-02  3.466e-02   1.658   0.0973 .
BMI          -7.750e-02  7.090e-02  -1.093   0.2743
...
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Confidence intervals:</p>

<pre><code>                  2.5 %       97.5 %
(Intercept)  0.10969506 1.863217e+03
Age          0.99565783 1.142627e+00
BMI          0.80089276 1.064256e+00
...
</code></pre>

<p>Odd ratios:</p>

<pre><code>                 Estimate Std. Error   z value Pr(&gt;|z|)
(Intercept)  1.159642e+01  11.464683 2.7310435 1.370327
Age          1.059155e+00   1.035269 5.2491658 1.102195
B            9.254228e-01   1.073477 0.3351730 1.315670
...
</code></pre>

<p>The first output shows that $Age$ is significant. However, the confidence interval for $Age$ includes the value 1 and the odds ratio for $Age$ is very close to 1. What does the significant p-value from the first output mean? Is $Age$ a predictor of the outcome or not?</p>
"
"0.075412822378","0.0821301562353182"," 10444","<p>In general, I standardize my independent variables in regressions, in order to properly compare the coefficients (this way they have the same units: standard deviations). However, with panel/longitudinal data, I'm not sure how I should standardize my data, especially if I estimate a hierarchical model.</p>

<p>To see why it can be a potential problem, assume you have $i = 1, \ldots, n$ individuals measured along $t=1,\ldots, T$ periods and you measured a dependent variable, $y_{i,t}$ and one independent variable $x_{i,t}$. If you run a complete pooling regression, then it's ok to standardize your data in this way: $x.z = (x- \text{mean}(x))/\text{sd}(x)$, since it will not change t-statistic. On the other hand, if you fit an unpooled regression, i.e., one regression for each individual, then you should standardize your data by individual only, not the whole dataset (in R code): </p>

<pre><code>for (i in 1:n) {
  for ( t in 1:T) x.z[i] =  (x[i,t] - mean(x[i,]))/sd(x[i,]) 
}
</code></pre>

<p>However, if you fit a simple hierarchical model with a varying intercept by individuals, then you are using a shrinkage estimator, i.e, you are estimating a model between pooled and unpooled regression. How should I standardize my data? Using the whole data like a pooled regression? Using only individuals, like in the unpooled case?</p>
"
"0.052777981396926","0.0502942438178979"," 10697","<p>I'm studying R package dlm. So far it seems very powerful and flexible package, with nice programming interfaces and good documentation.</p>

<p>I've been able to successfully use dlmMLE and dlmModARMA to estimate the parameters of AR(1) process:</p>

<pre><code>u &lt;- arima.sim(list(ar = 0.3), 100)
fit &lt;- dlmMLE(u, parm = c(0.5, sd(u)),
              build = function(x)
                dlmModARMA(ar = x[1], sigma2 = x[2]^2))
fit$par
</code></pre>

<p>Now I'm trying to use similar code to estimate the parameters of simple linear regression model:</p>

<pre><code>r &lt;- rnorm(100)
u &lt;- -1*r + 0.5*rnorm(100)
fit &lt;- dlmMLE(u, parm = c(0, 1),
              build = function(x)
                dlmModReg(x[1]*r, FALSE, dV = x[2]^2))
fit$par
</code></pre>

<p>I expect fit$par to be close to c(-1, 0.5), but I keep getting something like</p>

<pre><code>[1] -0.0002118851  0.4884367070
</code></pre>

<p>The coefficient -1 is not estimated correctly. However, the strange thing is that the variance of the noise is returned correctly.</p>

<p>I understand that max-likelihood estimation might fail given bad initial values, but I observed that the likelihood function returned by dlmLL is very flat in the first coordinate.</p>

<p>So I wonder: can such model be estimated at all using dlm? I believe the model is ""non-singular"", however I'm not sure how the likelihood function is calculated inside the dlm.</p>

<p>Any hint greatly appreciated.</p>
"
"0.052777981396926","0.0502942438178979"," 11096","<p>How can I interpret the main effects (coefficients for dummy-coded factor) in a Poisson regression?</p>

<p>Assume the following example:</p>

<pre><code>treatment     &lt;- factor(rep(c(1, 2), c(43, 41)), 
                        levels = c(1, 2),
                        labels = c(""placebo"", ""treated""))
improved      &lt;- factor(rep(c(1, 2, 3, 1, 2, 3), c(29, 7, 7, 13, 7, 21)),
                        levels = c(1, 2, 3),
                        labels = c(""none"", ""some"", ""marked""))    
numberofdrugs &lt;- rpois(84, 10) + 1    
healthvalue   &lt;- rpois(84, 5)   
y             &lt;- data.frame(healthvalue, numberofdrugs, treatment, improved)
test          &lt;- glm(healthvalue~numberofdrugs+treatment+improved, y, family=poisson)
summary(test)
</code></pre>

<p>The output is:</p>

<pre><code>Coefficients:
                 Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)       1.88955    0.19243   9.819   &lt;2e-16 ***
numberofdrugs    -0.02303    0.01624  -1.418    0.156    
treatmenttreated -0.01271    0.10861  -0.117    0.907   MAIN EFFECT  
improvedsome     -0.13541    0.14674  -0.923    0.356   MAIN EFFECT 
improvedmarke    -0.10839    0.12212  -0.888    0.375   MAIN EFFECT 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>I know that the incident rate for <code>numberofdrugs</code> is <code>exp(-0.023)=0.977</code>. But how do I interpret the main effects for the dummy variables? </p>
"
"0.0761784544170006","0.0798528368191181"," 11107","<p>I need to do a logistic regression using R on my data. My response variable (<code>y</code>) is survival at weaning (<code>surv=0</code>; did not <code>surv=1</code>) and I have several independent variables which are binary and categoricals in nature.</p>

<p>I am following some examples on this website <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/dae/logit.htm</a> and trying to run some models.</p>

<p>Running the model: </p>

<pre><code>&gt; mysurv2 &lt;- glm(surv~as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
                 as.factor(pmtone), family=binomial(link=""logit""), data=ap)
&gt; summary(mysurv2)

Call:
glm(formula = surv ~ as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
    as.factor(pmtone), family = binomial(link = ""logit""), data = ap)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.2837  -0.5121  -0.5121  -0.5058   2.0590  

Coefficients:
                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -0.01135    0.23613  -0.048  0.96166    
as.factor(PTEM)2   -0.74642    0.24482  -3.049  0.00230 ** 
as.factor(PTEM)3   -1.95401    0.23259  -8.401  &lt; 2e-16 ***
as.factor(pshiv)2  -0.02638    0.06784  -0.389  0.69738    
as.factor(presp)2   0.74549    0.10532   7.079 1.46e-12 ***
as.factor(presp)3   0.66793    0.66540   1.004  0.31547    
as.factor(pmtone)2  0.54699    0.09678   5.652 1.58e-08 ***
as.factor(pmtone)3  1.82337    0.75409   2.418  0.01561 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 7892.6  on 8791  degrees of freedom
Residual deviance: 7252.8  on 8784  degrees of freedom
  (341 observations deleted due to missingness)
AIC: 7268.8

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Adding the <code>na.action=na.pass</code> at the end of the model gave me an error message. I thought that this would take care NA's in my independent variables.</p>

<pre><code>&gt; mysurv &lt;- glm(surv~as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
                as.factor(pmtone), family=binomial(link=""logit""), data=ap, 
                na.action=na.pass)
Error: NA/NaN/Inf in foreign function call (arg 1)
</code></pre>

<p>Since this is my first time to venture into logistic regression, I am wondering whether there is any package in R that would be more suitable?</p>

<p>I am also tryng to understand the regression coefficients. The independent variables used in the model are:</p>

<ol>
<li><p>rectal temperature: </p>

<ul>
<li><code>(PTEM)1</code> = newborns with rectal temp. below 35.4 0C</li>
<li><code>(PTEM)2</code> = newborns with rectal temp. between 35.4 to 36.9 0C</li>
<li><code>(PTEM)3</code> = newborns with rectal temp. above 37.0 0C</li>
</ul></li>
<li><p>shivering:</p>

<ul>
<li><code>(pshiv)1</code> = newborns that were not shivering</li>
<li><code>(pshiv)2</code> = newborns that were shivering</li>
</ul></li>
<li><p>respiration:</p>

<ul>
<li><code>(presp)1</code> = newborns with normal respiration</li>
<li><code>(presp)2</code> = newborns with slight respiration problem</li>
<li><code>(presp)3</code> = newborns with poor respiration</li>
</ul></li>
<li><p>muscle tone:</p>

<ul>
<li><code>(pmtone)1</code> = newborns with normal muscle tone</li>
<li><code>(pmtone)2</code> = newborns with moderate muscle tone</li>
<li><code>(pmtone)1</code> = newborns with poor muscle tone</li>
</ul></li>
</ol>

<p>Looking at the coefficients, I got the following:</p>

<pre><code>                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -0.01135    0.23613  -0.048  0.96166    
as.factor(PTEM)2   -0.74642    0.24482  -3.049  0.00230 ** 
as.factor(PTEM)3   -1.95401    0.23259  -8.401  &lt; 2e-16 ***
as.factor(pshiv)2  -0.02638    0.06784  -0.389  0.69738    
as.factor(presp)2   0.74549    0.10532   7.079 1.46e-12 ***
as.factor(presp)3   0.66793    0.66540   1.004  0.31547    
as.factor(pmtone)2  0.54699    0.09678   5.652 1.58e-08 ***
as.factor(pmtone)3  1.82337    0.75409   2.418  0.01561 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>In my other analysis, I found that newborns:  </p>

<p>a) with higher rectal temperature<br>
b) do not shiver<br>
c) good respiration and<br>
d) good muscle tone at birth were more likely to survive.  </p>

<p>I am a bit confused with the coefficients I am getting above. I am wondering whether whether I am not interpreting the results correctly or is it something else?</p>
"
"0.0272544323995652","0.0389577537433628"," 11178","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/8661/logistic-regression-in-r-odds-ratio"">Logistic Regression in R (Odds Ratio)</a>  </p>
</blockquote>



<p>I need to do a logistic regression in R. My response variable is <code>surv=0</code>; <code>surv=1</code> and I have about 18 predictor variables.</p>

<p>After reading my model, I got the table of Coefficients below and I need to go through some steps, which I am not familiar with, until I get to the odds ratios.</p>

<p>This is my first time to do a logistic regression in R and your help would be appreciated.</p>

<pre><code>Call:
glm(formula = surv ~ as.factor(tdate) + as.factor(line) + as.factor(wt) + 
    as.factor(crump) + as.factor(pind) + as.factor(pcscore) + 
    as.factor(ptem) + as.factor(pshiv) + as.factor(pincis) + 
    as.factor(presp) + as.factor(pmtone) + as.factor(pscolor) + 
    as.factor(ppscore) + as.factor(pmstain) + as.factor(pbse) + 
    as.factor(psex) + as.factor(pgf), family = binomial(link = ""logit""), 
    data = ap)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.9772  -0.5896  -0.4419  -0.3154   2.8264  

Coefficients:
                           Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                -0.59796    0.27024  -2.213 0.026918 *  
as.factor(tdate)2009-09-08  0.43918    0.19876   2.210 0.027130 *  
as.factor(tdate)2009-09-11  0.27613    0.20289   1.361 0.173514    
as.factor(tdate)2009-09-15  0.58733    0.19232   3.054 0.002259 ** 
as.factor(tdate)2009-09-18  0.52823    0.20605   2.564 0.010360 *  
as.factor(tdate)2009-09-22  0.45661    0.19929   2.291 0.021954 *  
as.factor(tdate)2009-09-25 -0.09189    0.21740  -0.423 0.672526    
as.factor(tdate)2009-09-29 -0.15696    0.28369  -0.553 0.580076    
as.factor(tdate)2010-01-26  1.39260    0.21049   6.616 3.69e-11 ***
as.factor(tdate)2010-01-29  1.67827    0.21099   7.954 1.80e-15 ***
as.factor(tdate)2010-02-02  1.35442    0.21292   6.361 2.00e-10 ***
as.factor(tdate)2010-02-05  1.36856    0.21439   6.383 1.73e-10 ***
as.factor(tdate)2010-02-09  1.18159    0.21951   5.383 7.33e-08 ***
as.factor(tdate)2010-02-12  1.40457    0.22001   6.384 1.73e-10 ***
as.factor(tdate)2010-02-16  1.01063    0.21783   4.639 3.49e-06 ***
as.factor(tdate)2010-02-19  1.54992    0.21535   7.197 6.14e-13 ***
as.factor(tdate)2010-02-23  0.85695    0.33968   2.523 0.011641 *  
as.factor(line)2           -0.26311    0.07257  -3.625 0.000288 ***
as.factor(line)5            0.06766    0.11162   0.606 0.544387    
as.factor(line)6           -0.30409    0.12130  -2.507 0.012176 *  
as.factor(wt)2             -0.33904    0.10708  -3.166 0.001544 ** 
as.factor(wt)3             -0.28976    0.13217  -2.192 0.028359 *  
as.factor(wt)4             -0.50470    0.16264  -3.103 0.001915 ** 
as.factor(wt)5             -0.74870    0.20067  -3.731 0.000191 ***
as.factor(crump)2           0.07537    0.10751   0.701 0.483280    
as.factor(crump)3          -0.14050    0.13217  -1.063 0.287768    
as.factor(crump)4          -0.20131    0.16689  -1.206 0.227724    
as.factor(crump)5          -0.23963    0.20778  -1.153 0.248803    
as.factor(pind)2           -0.29893    0.10752  -2.780 0.005434 ** 
as.factor(pind)3           -0.40828    0.12436  -3.283 0.001027 ** 
as.factor(pind)4           -0.73021    0.14947  -4.885 1.03e-06 ***
as.factor(pind)5           -0.68878    0.17650  -3.902 9.52e-05 ***
as.factor(pcscore)2        -0.52667    0.13606  -3.871 0.000108 ***
as.factor(ptem)2           -0.72600    0.08964  -8.099 5.52e-16 ***
as.factor(ptem)3           -0.79145    0.10503  -7.536 4.86e-14 ***
as.factor(ptem)4           -0.89956    0.10331  -8.707  &lt; 2e-16 ***
as.factor(ptem)5           -0.90181    0.10721  -8.412  &lt; 2e-16 ***
as.factor(pshiv)2           0.25236    0.07713   3.272 0.001068 ** 
as.factor(pincis)2          0.02327    0.07216   0.323 0.747041    
as.factor(presp)2           0.43746    0.11598   3.772 0.000162 ***
as.factor(pmtone)2          0.34515    0.11178   3.088 0.002016 ** 
as.factor(pscolor)2         0.53469    0.26851   1.991 0.046443 *  
as.factor(ppscore)2         0.25664    0.08751   2.933 0.003361 ** 
as.factor(pmstain)2        -0.48619    0.84408  -0.576 0.564611    
as.factor(pbse)2           -0.28248    0.07335  -3.851 0.000117 ***
as.factor(psex)2           -0.18240    0.06385  -2.857 0.004280 ** 
as.factor(pgf)12            0.10329    0.14314   0.722 0.470554    
as.factor(pgf)21           -0.06481    0.10772  -0.602 0.547388    
as.factor(pgf)22            0.39584    0.12740   3.107 0.001890 ** 
as.factor(pgf)31            0.18820    0.10082   1.867 0.061936 .  
as.factor(pgf)32            0.39662    0.13963   2.841 0.004504 ** 
as.factor(pgf)41            0.09178    0.10413   0.881 0.378106    
as.factor(pgf)42            0.21056    0.14906   1.413 0.157787    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 7812.9  on 8714  degrees of freedom
Residual deviance: 6797.4  on 8662  degrees of freedom
  (418 observations deleted due to missingness)
AIC: 6903.4

Number of Fisher Scoring iterations: 5
</code></pre>
"
"0.0545088647991304","0.0649295895722714"," 11236","<p>I've been using R's <code>lm</code> to do some linear regression, but decided to give <code>MCMCregress</code> a try to get a feel for how it works. As expected, I got basically the same coefficients, but the extra <code>sigma2</code> value puzzles me.</p>

<p>When I do a <code>qqmath</code> plot of the coefficients, I get the following graph, and I'm puzzled by the sigma2 plot. It's obviously not linear, but I'm not sure if that's meaningful in this context. I assume it's sigma <em>squared</em>, and when I took the square root and plotted it, the line was straighter, but still curved.</p>

<p>I guess my question boils down to: what is sigma2 telling me about the MCMC regression fit, and is a graph of it useful or should I ignore the graph and focus on something else? (All of the diagnostics and graphs I've done on my original <code>lm</code> fit seem to indicate that the fit is good, so I'm also wondering if the MCMC regression gives me more information or not.)</p>

<p><img src=""http://i.stack.imgur.com/wayi4.png"" alt=""qqmath plot of MCMCregress results""></p>

<p>(If I need to provide the actual data, I can. I'm hoping that an answer depends more on what sigma2 is rather than on specific values.)</p>
"
"0.125636725583038","0.119724247531067"," 11498","<p>I'm fitting a linear model where the response is a function both of time and of static covariates (i.e. ones that are independent of time). The ultimate goal is to identify significant effects of the static covariates.</p>

<p>Is this the best general strategy for selecting variables (in R, using the <code>nlme</code> package)? Anything I can do better?</p>

<ol>
<li>Break the data up by groups and plot it against time. For continuous covariates, bin it and plot the data in each bin against time. Use the group-specific trends to make an initial guess at what time terms to include-- time, time^n, sin(2*pi*time)+cos(2*pi*time), log(time), exp(time), etc.</li>
<li>Add one term at a time, comparing each model to its predecessor, never adding a higher order in the absence of lower order terms. Sin and cos are never added separately. <strong><em>Is it acceptable to pass over a term that significantly improves the fit of the model if there is no physical interpretation of that term?</em></strong>.</li>
<li>With the full dataset, use forward selection to add static variables to the model and then relevant interaction terms with each other and with the time terms. <strong><em>I've seen some strong criticism of stepwise regression, but doesn't forward selection ignore significant higher order terms if the lower order terms they depend on are not significant? And I've noticed that it's hard to pick a starting model for backward elimination that isn't saturated, or singular, or fails to converge. How do you decide between variable selection algorithms?</em></strong></li>
<li>Add random effects to the model. <strong><em>Is this as simple as doing the variable selection using <code>lm()</code> and then putting the final formula into <code>lme()</code> and specifying the random effects? Or should I include random effects from the very start?</em></strong>. Compare the fits of models using a random intercept only, a random interaction with the linear time term, and random interaction with each successive time term. </li>
<li>Plot a semivariogram to see if an autoregressive error term is needed. <strong><em>What should a semivariogram look like if the answer is 'no'? A horizontal line? How straight, how horizontal? Does including autoregression in the model again require checking potential variables and interactions to make sure they're still relevant?</em></strong></li>
<li>Plot the residuals to see if the variance changes as a function of fitted value, time, or any of the other terms. If it does, weigh the variances appropriately (for <code>lme()</code>, use the <code>weights</code> argument to specify a <code>varFunc()</code>) and compare to the unweighted model to see if this improves the fit. <strong><em>Is this the right sequence in which to do this step, or should it be done before autocorrelation?</em></strong>.</li>
<li>Do <code>summary()</code> of the fitted model to identify significant coefficients for numeric covariates. Do <code>Anova()</code> of the fitted model to identify significant effects for qualitative covariates.</li>
</ol>
"
"NaN","NaN"," 11703","<p>Assume the following easy example of a glm regression with an offset:</p>

<pre><code>numberofdrugs&lt;-rpois(84, 10)
healthvalue&lt;-rpois(84,75)
age&lt;-rnorm(84,50,5)
test&lt;-glm(healthvalue~age, family=poisson, offset=log(numberofdrugs))
summary(test)
fitted(test) #how to get one of these values manually?
</code></pre>

<ul>
<li>How can I compute the fitted values manually? </li>
<li>Also, why is there no estimation of log(numberofdrugs)? 
<ul>
<li>In the book <a href=""http://rads.stackoverflow.com/amzn/click/0412317605"" rel=""nofollow""><em>Generalized Linear Models</em></a> on page 205-207 there is an example where the offset is estimated. It was done to see if the coefficient is close to one. It's 0.903 (see page 207 if you've this classic book) and from this follows, that there is nearly a constant rate in the number of damage incident!</li>
</ul></li>
</ul>

<p>Previous related questions asked: </p>

<ul>
<li><a href=""http://stats.stackexchange.com/questions/11182/when-to-use-an-offset-in-a-poisson-regression"">When to use an offset?</a></li>
<li><a href=""http://stats.stackexchange.com/questions/11595/whether-to-use-an-offset-in-a-poisson-regression-when-predicting-total-career-goa"">Whether to use an offset when predicting hockey scores?</a></li>
</ul>
"
"0.105555962793852","0.100588487635796"," 12590","<p>I would like to do something in R that SAS can do using SAS's proc mixed (there is some way to do in STATA es well), namely fitting the so called Bivariate model from Reitsma et al (2005). This model is a special mixed model where the variance depends on the study (see below). Googling and talking to some people familiar with the model did not yield a straightforward approach that is fast at the same time (i.e. a nice high level model fitting function). I am nevertheless sure, there is something fast in R that one can built on.</p>

<p>In a nutshell one is faced with the following situation: Given pairs of proportions $(p_1,p_2)$  in $[0,1]^2$ one would like to fit a bivariate normal to the logit-transformed pairs. Since the proportions come from a 2x2 table (i.e. binomial data) each logit transformed observed proportion has a variance estimate that is to be included in the fitting process, say $(s_1, s_2)$. So one would like to fit a bivariate normal to the pairs, where the covariance matrix $\Sigma$ <em>depends</em> on the observation, i.e. </p>

<p>$(\text{logit}(p_1),\text{logit}(p_2)) \sim N((mu_1, mu_2), \Sigma + S)$</p>

<p>where S is the diagonal matrix with $(s_1, s_2)$ and depends entirely on the data but varies from observation to observation. mu and Sigma are the same for all observation though.</p>

<p>Right now I am using a call to <code>optim()</code> (using BFGS) to estimate the five parameters ($\mu_1$, $\mu_2$, and three parameters for $\Sigma$). Nevertheless this is painfully slow, and especially unsuitable for simulation. Also one of my aims is to introduce regression coefficients for mu later, increasing the number of parameters.</p>

<p>I tried speeding up fitting by supplying starting values and I also thought about computing gradients for the five parameters. Since the likelihood becomes quite complex due to the addition of $S$, I felt the risk of introducting errors this way was too big and did not attempt it yet, nor did I see a way to check my calculations.</p>

<p>Is the calculation of the gradients typically worthwhile? How do you check them?</p>

<p>I am aware of other optimizer besides <code>optim()</code>, i.e. <code>nlm()</code> and I also know about the CRAN Task view: Optimization. Which ones a are worth a try?</p>

<p>What kind of tricks are there to speed up <code>optim()</code> besides reducing accuracy?</p>

<p>I would be very grateful for any hints.</p>
"
"0.0609427635336005","0.0580747904139041"," 12709","<p>I have a large dataset and have performed a multilevel regression in Stata, the model is the following:</p>

<p><code>xtmixed dependent independen1 independent2 independent3  independent4    || independendt5:</code></p>

<p>So there is one grouping factor: <code>independent5</code></p>

<p>In R I did the following:</p>

<pre><code>lmer(dependent ~ independent1 + indepdendent2 + independent3 + independent4 + 1 | independent5, REML=TRUE) 
</code></pre>

<p>A few questions: is this identical?</p>

<p>Stata output gives me number of groups, 100, while R gives number of groups as 99.
Furthermore, the variances and standard deviations are not the same.
Also I would like to know how to obtain p values and coefficients from the R ouput.
I have done <code>fixef(model)</code> and <code>ranef(model)</code> but when I do <code>coef(model)</code> it says: <code>Error in coef(model) : unable to align random and fixed effects</code></p>

<p>Also Stata only gives me 1 coefficient for each predictor while the R <code>fixef(model)</code> gives one for each group. </p>

<p>So someone familiar with both Stata and R could help me with this.</p>
"
"0.0746393370862076","0.0592723347638087"," 13091","<p>I have this model:</p>

<pre><code>model &lt;- zelig(dv~(product*intervention), model = ""negbin"", data = data)
</code></pre>

<p>intervention has <strong>two levels</strong>: neutral(=0), treatment(=1)<br />
product has <strong>two levels</strong>: product1(=0), product2(=1)</p>

<p>I build f_all to just have one factor with 4 groups for comparison analysis.</p>

<p>Thus I have <strong>4 groups</strong> in f_all<br />
1. product1-neutral<br />
2. product1-treatment<br />
3. product2-neutral<br />
4. product2-treament<br /></p>

<p><strong>My interaction hypothesis is that treatment only works for product2.</strong></p>

<p>Zelig gives me my predicted significant interaction. <br /></p>

<p>Yet, I need planned contrasts to test my specific hypothesis: c(-1,1,0,0) and c(0,0,1,-1)</p>

<p>I researched and found a description of doing this with multcomp on this page: <a href=""http://stats.stackexchange.com/questions/12993/how-to-setup-and-interpret-anova-contrasts-with-the-car-package-in-r"">post comparisons</a></p>

<p>The regression output shows my predicted interaction</p>

<pre><code>(Intercept)  1.34223    0.08024  16.728   &lt;2e-16 ***
product      0.08747    0.08025   1.090   0.2757
intervention 0.07437    0.07731   0.962   0.3361
interaction  0.45645    0.22263   2.050   0.0403 * 
</code></pre>

<p>However, it said multcomp and the glht function is for linear models, but I am using a negbin model.</p>

<p><strong>3 Questions regarding this problem:</strong><br />
1. Can I do planned comparisons on my negbin model using multcomp?<br />
2. If not what appropriate method is there to do this for my negbin model?<br />
3. Based on R using treatment contrasts per default could I just interpret the interaction coefficient as the contrast comparing product2-neutral versus product2-treatment? Can I then interpret the intervention coefficient as contrast comparing product1-neutral versus product1-treament?</p>
"
"0.052777981396926","0.0335294958785986"," 13152","<p>I always use <code>lm()</code> in R to perform linear regression of $y$ on $x$. That function returns a coefficient $\beta$ such that $$y = \beta x.$$</p>

<p>Today I learned about <strong>total least squares</strong> and that <code>princomp()</code> function (principal component analysis, PCA) can be used to perform it. It should be good for me (more accurate). I have done some tests using <code>princomp()</code>, like:</p>

<pre><code>r &lt;- princomp(Â ~Â xÂ +Â y)
</code></pre>

<p>My problem is: how to interpret its results? How can I get the regression coefficient? By ""coefficient"" I mean the number $\beta$ that I have to use to multiply the $x$ value to give a number close to $y$.</p>
"
"0.101062140164153","0.0875510407188402"," 13353","<p>I will give my examples with R calls. First a simple example of a linear regression with a dependent variable 'lifespan', and two continuous explanatory variables.</p>

<pre><code>data.frame(height=runif(4000,160,200))-&gt;human.life
human.life$weight=runif(4000,50,120)
human.life$lifespan=sample(45:90,4000,replace=TRUE)
summary(lm(lifespan~1+height+weight,data=human.life))

Call:
lm(formula = lifespan ~ 1 + height + weight, data = human.life)

Residuals:
Min       1Q   Median       3Q      Max 
-23.0257 -11.9124  -0.0565  11.3755  23.8591 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 63.635709   3.486426  18.252   &lt;2e-16 ***
height       0.007485   0.018665   0.401   0.6884    
weight       0.024544   0.010428   2.354   0.0186 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 13.41 on 3997 degrees of freedom
Multiple R-squared: 0.001425,   Adjusted R-squared: 0.0009257 
F-statistic: 2.853 on 2 and 3997 DF,  p-value: 0.05781
</code></pre>

<p>In order to find the estimate of 'lifespan' when the value of 'weight' is 1, I add (Intercept)+height=63.64319</p>

<p>Now what if I have a similar data frame, but one where one of the explanatory variables is categorical?</p>

<pre><code>data.frame(animal=rep(c(""dog"",""fox"",""pig"",""wolf""),1000))-&gt;animal.life
animal.life$weight=runif(4000,8,50)
animal.life$lifespan=sample(1:10,replace=TRUE)
summary(lm(lifespan~1+animal+weight,data=animal.life))

Call:
lm(formula = lifespan ~ 1 + animal + weight, data = animal.life)

Residuals:
Min      1Q  Median      3Q     Max 
-4.7677 -2.7796 -0.1025  3.1972  4.3691 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 5.565556   0.145851  38.159  &lt; 2e-16 ***
animalfox   0.806634   0.131198   6.148  8.6e-10 ***
animalpig   0.010635   0.131259   0.081   0.9354    
animalwolf  0.806650   0.131198   6.148  8.6e-10 ***
weight      0.007946   0.003815   2.083   0.0373 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 2.933 on 3995 degrees of freedom
Multiple R-squared: 0.01933,    Adjusted R-squared: 0.01835 
F-statistic: 19.69 on 4 and 3995 DF,  p-value: 4.625e-16
</code></pre>

<p>In this case, to find the estimate of 'lifespan' when the value of 'weight' is 1, should I add each of the coefficients for 'animal' to the intercept: (Intercept)+animalfox+animalpig+animalwolf? Or what is the proper way to do this?</p>

<p>Thanks
Sverre</p>
"
"0.0430930413588572","0.0410650781176591"," 13550","<p>I have a simple matrix:</p>

<pre><code>     [,1] [,2] [,3]
[1,]    1    2    3
[2,]    4    5    6
[3,]    7    8    9
[4,]   10   11   12
</code></pre>

<p>I have to calculate linear regression and orthogonal regression using lm() and prcomp() respectively. (for orthogonal see: <a href=""http://my.safaribooksonline.com/book/programming/r/9780596809287/beyond-basic-numerics-and-statistics/recipe-id264"" rel=""nofollow"">here</a>)</p>

<p>Assume that the first column is the the X and M the matrix I wrote before.</p>

<p><strong>LINEAR REG.</strong></p>

<pre><code>mod1 &lt;- lm(M[,1] ~ M[,2] + M[,3] + 0)
</code></pre>

<p>Its output is (coefficient):</p>

<pre><code>Coefficients: M[, 2]  M[, 3]  
     2      -1
</code></pre>

<p>Ok, I have these coefficients.</p>

<p>Now for </p>

<p><strong>ORTHOGONAL REG.</strong></p>

<pre><code>mod2 &lt;- prcomp(~ M[,1] + M[,2] + M[,3])
</code></pre>

<p>Its output is:</p>

<pre><code>             PC1        PC2        PC3
M[, 1] 0.5773503  0.0000000  0.8164966
M[, 2] 0.5773503 -0.7071068 -0.4082483
M[, 3] 0.5773503  0.7071068 -0.4082483
</code></pre>

<p>The question is: out to interpret prcomp() result instead of lm() result ?
Using lm() the coefficients are using to predict the X values.</p>

<p>What about prcomp() ?</p>

<p>Thank you!</p>
"
"0","0.029037395206952"," 13615","<p>At the moment I'm using linear regression of 4 series with:</p>

<pre><code>mod &lt;- lm(x ~ y + z + v + 0) # I need zero intercept
</code></pre>

<p>I'm using the linear regression to calculate the coefficients of <code>y</code>, <code>z</code> and <code>v</code> to predict the <code>x</code> value.</p>

<p>Is there something more accurate then lm? 
For example, I heard about orthogonal regression; could it be good?</p>
"
"0.068136080998913","0.0649295895722714"," 13617","<p>I've been implementing the GLMNET version of elastic net for linear regression with another software than R. I compared my results with the R function glmnet in lasso mode on <a href=""http://www.stanford.edu/~hastie/Papers/LARS/diabetes.data"">diabetes data</a>.</p>

<p>The variable selection is ok when varying the value of the parameter (lambda) but I obtain slightly different values of coefficients. For this and other reasons I think it comes from the intercept in the update loop, when I compute the current fit, because I don't vary the intercept (which I take as the mean of the target variable) in the whole algorithm : as explained in Trevor Hastie's article ( <a href=""http://www.jstatsoft.org/v33/i01/paper"">Regularization Paths for Generalized Linear Models via Coordinate Descent</a>, Page 7, section 2.6):</p>

<blockquote>
  <p>the intercept is not regularized, [...] for all values of [...] lambda [the L1-constraint parameter]</p>
</blockquote>

<p>But despite the article, the R function glmnet does provide different values for the intercept along the regularization path (the lambda different values). Does anyone has a clue about how the values of the Intercept are computed?</p>
"
"0.068136080998913","0.0649295895722714"," 14005","<p>I'm trying to understand how exactly factors work in R. Let's say I want to run a regression using some sample data in R:</p>

<pre><code>&gt; data(CO2)
&gt; colnames(CO2)
[1] ""Plant""     ""Type""      ""Treatment"" ""conc""      ""uptake""   
&gt; levels(CO2$Type)
[1] ""Quebec""      ""Mississippi""
&gt; levels(CO2$Treatment)
[1] ""nonchilled"" ""chilled""   
&gt; lm(uptake ~ Type + Treatment, data = CO2)

Call:
lm(formula = uptake ~ Type + Treatment, data = CO2)

Coefficients:
 (Intercept)   TypeMississippi  Treatmentchilled  
       36.97            -12.66             -6.86  
</code></pre>

<p>I understand that <code>TypeMississippi</code> and <code>Treatmentchilled</code> are treated as booleans: For each row, the initial uptake is <code>36.97</code>, and we subtract <code>12.66</code> if it's of type Mississippi and <code>6.86</code> if it was chilled. I'm having trouble understanding something like this:</p>

<pre><code> &gt; lm(uptake ~ Type * Treatment, data = CO2)

 Call:
 lm(formula = uptake ~ Type * Treatment, data = CO2)

 Coefficients:
                 (Intercept)                   TypeMississippi  
                      35.333                            -9.381  
            Treatmentchilled  TypeMississippi:Treatmentchilled  
                      -3.581                            -6.557  
</code></pre>

<p>What does it mean to multiply two factors together in an <code>lm</code>?</p>
"
"0.0826872055888527","0.0875510407188402"," 14399","<p>For my microsimulation, I want to use R to predict values and draw a random sample based on this prediction.</p>

<p>To clarify my point: I want to simulate the number of chronic conditions people suffer from ($y_t$) at a certain point in time. I have a few waves of panel data available to estimate a relation between age, sex, number of chronic conditions in the previous observation period (plus some others that I might include in later stages).</p>

<p>Suppose my regression model is $y_t = Î²_0 + Î²_1 age + Î²_2 sex + Î²_3y_{t-1} +u $.   </p>

<p>Since R provides me with coefficients for the betas, it is easy to predict $y_t$ given the independent variables. However, this is not what I want to do. Instead, I want my population of about 1300 individuals to resemble the variance in the possible outcomes of $y_t$ (otherwise after a few steps my simulated population wonâ€™t include those unlucky ones with much more chronic conditions than the average).   </p>

<p>I believe what I have to do is to draw a random sample from the distribution of the predicted value $y_t$, conditional on the independent variables. I further believe this can be done by drawing random numbers with mean $Î²$ and variance $var(Î²)$, multiplied by the actual values of the independent variables.  </p>

<p>So my question is: Is this the correct approach? Will this produce reliable values? Or do I need to take possible covariation of the independent variables etc. into consideration?</p>

<p><strong>Edit</strong>: Another point came to my mind. Does it make a difference whether $y_t$ or $y_t - y_{t-1}$ is my left hand side variable?</p>

<p>Thank you for your ideas.</p>
"
"0.0430930413588572","0.0410650781176591"," 14546","<p>I have run the following logistic regression:</p>

<pre><code>glm(formula = DecisionasReceiver ~ L1 + L2 + L3, 
  family = binomial(""logit""), data = lue)
</code></pre>

<p>where L1 L2 and L3 code for differences in condition of no.GREEN.
L1: 1,-1,0,0 : is there a difference in DecisionasReceiver as no.GREEN changes from 1 to 2?</p>

<p>L2: 0,1,-1,0: is there a difference in DecisionasReceiver as no.GREEN changes from 2 to 3?</p>

<p>L2: 0,0,1,-1: is there a difference in DecisionasReceiver as no.GREEN changes from 3 to 4?</p>

<p>And I'm running this regression both for the cases where MessageReceived is BLUE and for MessageReceived RED. </p>

<p>I have the following output:</p>

<pre><code>   Coefficients:
      Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -0.9535     0.3659  -2.606  0.00916 ** 
L1            2.2753     0.5406   4.209 2.56e-05 ***
L2            3.1234     0.7318   4.268 1.97e-05 ***
L3            1.9369     0.8134   2.381  0.01726 *  
</code></pre>

<p>Looking at my graph it seems strange that the coefficients are positive and that the intercept is -0.953. How exactly should I interpret these results in the light of the graph? </p>

<p><img src=""http://dl.dropbox.com/u/22681355/graph.png"" alt=""""></p>
"
"0.0497595580574717","0.047417867811047"," 15145","<p>I have performed the path analysis using the <code>sem</code> function in R. The model which I fitted consists of both direct and indirect paths. I have some trouble in interpreting the estimates of the SEM coefficients. </p>

<ul>
<li>Does R gives the value of total effect = (direct effect + indirect effect) directly or do I have to multiply the coefficients which are on the indirect path and then add them to the coefficients which is on the direct path? This is the usual way of doing path analysis with the raw/absolute correlation coefficients.</li>
</ul>

<p>For example consider X (independent variable), Y (dependent variable) and M (Mediating variable). </p>

<p>The raw/absolute correlation/ standardized regression coefficients between them are X and Y  -0.06; X and M 0.22 and M and Y 0.28 whereas on the path analysis/sem in R, the above coefficients are X and Y -0.13; X and M 0.22 and M and Y 0.31. </p>

<ul>
<li>Thus  is the total effect of X and Y  equal to -0.13?</li>
<li>Alternatively how should I interpret this coefficient considering the effect of variable M into the account?</li>
</ul>
"
"0.13342816860689","0.13925845528839"," 15160","<p>I have a large dataset with patients and I'm studying a rare outcome (~ 2%) and death is a competing risk (mean age ~69 years). I've used the R ""cmprsk"" package for my statistics and it seems that competing risks and the Cox regression are performing similarly although the competing risk analysis is more conservative giving hazard ratios closer to 1.</p>

<p>I've been suggested to do a Poisson regression on the data but the results don't make any sense and I would be really grateful to get some input on the benefits of doing this kind of analysis on survival data. I've created this simulation for creating a dataset with similar risk factors:</p>

<pre><code>library(""cmprsk"")
# The time for the study
accrual_time &lt;- 10
followup_time &lt;- 1

base_risk &lt;- list(""event"" = .015, ""cmprsk"" = .1)

risk_factors &lt;- list(list(""frequency""=.1, 
                ""event"" = base_risk$event*.5, 
                ""cmprsk"" = base_risk$cmprsk*2),
        list(""frequency""=.05, 
                ""event"" = base_risk$event*1, 
                ""cmprsk"" = base_risk$cmprsk*1),
        list(""frequency""=.05, 
                ""event"" = base_risk$event*-.5, 
                ""cmprsk"" = base_risk$cmprsk*0))

# Number of subjects
n &lt;- 5000

# Create base time, sequential inclusion
time_in_study &lt;- rep(c(1:n)/n*accrual_time + followup_time, 1)

set.seed(100)

# Create empty sets
x &lt;- matrix(0, ncol=length(risk_factors), nrow=n)
time_2_event &lt;- rep(0, n)
time_2_comprsk &lt;- rep(0, n)

# Create each studied observation and outcome
for(i in 1:n){
    # Set base risk
    event_risk &lt;- base_risk$event 
    comp_risk &lt;- base_risk$cmprsk

    for(j in 1:length(risk_factors)){
        x[i, j] &lt;- rbinom(1, 1, risk_factors[[j]]$frequency)[1]

        # If there is a risk factor defined
        if (x[i, j] &gt; 0){
            event_risk &lt;- event_risk +
                    risk_factors[[j]]$event
            comp_risk &lt;- comp_risk + 
                    risk_factors[[j]]$cmprsk
        }
    }

    # Time 2 event/risk is 1/rate meaning that higher number -&gt; shorter time
    time_2_event[i] &lt;- rexp(1, rate=event_risk)[1]
    time_2_comprsk[i] &lt;- rexp(1, rate=comp_risk)[1]
}

cn &lt;- c()
for(i in 1:length(risk_factors)){
    ev_rsk &lt;- risk_factors[[i]]$event/base_risk$event+1
    cmp_rsk &lt;- risk_factors[[i]]$cmprsk/base_risk$cmprsk+1
    name &lt;- paste(""Risk factor no: "", i, ""\n * ev="", ev_rsk, "" cr="", cmp_rsk, "" *"", sep="""")
    cn &lt;- c(cn, name)
}
colnames(x) &lt;- cn

# Select the event that happens first: study ends, evenent occurs, a competing event occurs
time &lt;- apply(cbind(time_in_study, time_2_event, time_2_comprsk), 1, min)

# Outcome identifiers
event &lt;- (time_2_event == time) + 0
comprsk &lt;- (time_2_comprsk == time) + 0
cens &lt;- event+2*(event==0 &amp; comprsk==1)

out.cox_ev &lt;- coxph(Surv(time, event)~x)
summary(out.cox_ev)

out.crr_ev &lt;- crr(time, cens, x, failcode=1)
summary(out.crr_ev)

out.cox_cmprsk &lt;- coxph(Surv(time, comprsk)~x)
summary(out.cox_cmprsk)

out.crr_cmprsk &lt;- crr(time, cens, x, failcode=2)
summary(out.crr_cmprsk)
</code></pre>

<p>The output makes sense but when I do a:</p>

<pre><code>out.glm_pr &lt;- glm(event ~ x, family=""poisson"")
summary(out.glm_pr)
</code></pre>

<p>It gives estimates of:</p>

<ul>
<li>RF 1 ~ .14 </li>
<li>RF 2 ~ .41 </li>
<li>RF 3 ~ -.23</li>
</ul>

<p>My questions: </p>

<ul>
<li>Is the glm() code correct or should I somehow transform my data?</li>
<li>Does the Poisson output make any sense and how should if so interpret it?</li>
<li>What are the benefits/pitfalls in using Poisson regression for survival data?</li>
</ul>

<p>Thanks!</p>

<hr>

<h2>UPDATE</h2>

<p>After adding exp(out.glm_pr$coefficients) the results are almost identical to the competing risk regression, here's a forest plot that compares the three:</p>

<p><img src=""http://i.stack.imgur.com/14Zt0.png"" alt=""A forestplot comparing the different methods - Poisson: 1.152  1.509  0.794, CRR: 1.151 1.524 0.812, Cox PH: 1.897 1.931 0.798""></p>

<p>The x-axis is perhaps not entirely valid (should be ""incident rate ratios"" for the Poisson regression) but why are the outcomes for CRR &amp; poisson almost identical?</p>

<p>As for testing over-dispersion I've found these two methods:</p>

<pre><code>&gt; library(qcc)
&gt; qcc.overdispersion.test(event)

Overdispersion test Obs.Var/Theor.Var Statistic p-value
       poisson data         0.9391878      4695 0.99902
&gt; 
&gt; library(pscl)
&gt; out.glm_nb &lt;- glm.nb(event ~ x)
Warning messages:
1: In theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace = control$trace &gt;  :
  iteration limit reached
2: In theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace = control$trace &gt;  :
  iteration limit reached
&gt; odTest(out.glm_nb)
Likelihood ratio test of H0: Poisson, as restricted NB model:
n.b., the distribution of the test-statistic under H0 is non-standard
e.g., see help(odTest) for details/references

Critical value of test statistic at the alpha= 0.05 level: 2.7055 
Chi-Square Test Statistic =  -0.0139 p-value = 0.5 
</code></pre>

<p>I conclude that there isn't any evidence of over-dispersion or are there other methods better suited for testing over-dispersion in this kind of survival data?</p>

<p>The quasipoisson analysis gives similar values:</p>

<pre><code>&gt; out.glm_quasi_pr &lt;- glm(event ~ x, family=quasipoisson(link=""log""))
&gt; round(exp(out.glm_quasi_pr$coefficients), 3)
(Intercept)       xRF 1       xRF 2       xRF 3 
      0.059       1.152       1.509       0.794 
</code></pre>
"
"0.0430930413588572","0.0410650781176591"," 15307","<p>I am curious about the consequences of changing the order of the explanatory variables in a binary logistic regression. In a recent series of logistic regressions I ran in SPSS, I found that changing the order of the explanatory variables ($z, y, x$ instead of $x, y, z$) resulted in different coefficient values and significance levels. Investigating further, I got the same results in <code>R</code> using the same orders. Clearly, shifting the predictors around matters in terms of the resultsâ€”my question is how? (And yes, I checked to make sure I wasn't entering the variables stepwise.)</p>
"
"0.052777981396926","0.0502942438178979"," 15469","<p>Recently we discussed on SO how to update a standard linear regression summary with NeweyWest standard errors. I used <code>coeftest</code>from the <code>sandwich</code> package. It was told to use unclass to update my already existing summary like this: </p>

<pre><code>library(sandwich)
library(lmtest)
temp.lm &lt;- lm(runif(100) ~ rnorm(100))
temp.summ &lt;- summary(temp.lm)
temp.summ$coefficients &lt;- unclass(coeftest(temp.lm, vcov. = NeweyWest)
</code></pre>

<p>Now I wonder whether the joint parameters shown in the summary aren't affected at all when using a NeweyWest VC matrix? I mean with this code they are not affected obviously â€“Â but is this correct? Note this is not a syntax but a stats question :) Stuff like</p>

<pre><code>Residual standard error: 1.177 on 83 degrees of freedom  
Multiple R-squared: 0.7265, Adjusted R-squared:  0.71 
F-statistic:  44.1 on 5 and 83 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>remains the same. Are there any cases that need adjustment as well?</p>
"
"0.0967596325610309","0.100588487635796"," 15577","<p>I'm trying to run a zero-inflated regression for a continuous response variable in R. I'm aware of a gamlss implementation, but I'd really like to try out this algorithm by Dale McLerran that is conceptually a bit more straightforward. Unfortunately, the code is in SAS and I'm not sure how to re-write it for something like nlme. </p>

<p>The code is as follows:</p>

<pre><code>proc nlmixed data=mydata;
  parms b0_f=0 b1_f=0 
        b0_h=0 b1_h=0 
        log_theta=0;


  eta_f = b0_f + b1_f*x1 ;
  p_yEQ0 = 1 / (1 + exp(-eta_f));


  eta_h = b0_h + b1_h*x1;
  mu    = exp(eta_h);
  theta = exp(log_theta);
  r = mu/theta;


  if y=0 then
     ll = log(p_yEQ0);
  else
     ll = log(1 - p_yEQ0)
          - lgamma(theta) + (theta-1)*log(y) - theta*log(r) - y/r;


  model y ~ general(ll);
  predict (1 - p_yEQ0)*mu out=expect_zig;
  predict r out=shape;
  estimate ""scale"" theta;
run;
</code></pre>

<p>From: <a href=""http://listserv.uga.edu/cgi-bin/wa?A2=ind0805A&amp;L=sas-l&amp;P=R20779"" rel=""nofollow"">http://listserv.uga.edu/cgi-bin/wa?A2=ind0805A&amp;L=sas-l&amp;P=R20779</a></p>

<p><strong>ADD:</strong></p>

<p>Note: There are no mixed effects present here - only fixed.</p>

<p>The advantage to this fitting is that (even though the coefficients are the same as if you separately fit a logistic regression to P(y=0) and a gamma error regression with log link to E(y | y>0)) you can estimate the combined function E(y) which includes the zeroes. One can predict this value in SAS (with a CI) using the line <code>predict (1 - p_yEQ0)*mu</code> .</p>

<p>Further, one is able to write custom contrast statements to test the significance of predictor variables on E(y). For example, here is another version of the SAS code I have used:</p>

<pre><code>proc nlmixed data=TestZIG;
      parms b0_f=0 b1_f=0 b2_f=0 b3_f=0
            b0_h=0 b1_h=0 b2_h=0 b3_h=0
            log_theta=0;


        if gifts = 1 then x1=1; else x1 =0;
        if gifts = 2 then x2=1; else x2 =0;
        if gifts = 3 then x3=1; else x3 =0;


      eta_f = b0_f + b1_f*x1 + b2_f*x2 + b3_f*x3;
      p_yEQ0 = 1 / (1 + exp(-eta_f));

      eta_h = b0_h + b1_h*x1 + b2_h*x2 + b3_h*x3;
      mu    = exp(eta_h);
      theta = exp(log_theta);
      r = mu/theta;

      if amount=0 then
         ll = log(p_yEQ0);
      else
         ll = log(1 - p_yEQ0)
              - lgamma(theta) + (theta-1)*log(amount) -                      theta*log(r) - amount/r;

      model amount ~ general(ll);
      predict (1 - p_yEQ0)*mu out=expect_zig;
      estimate ""scale"" theta;
    run; 
</code></pre>

<p>Then to estimate ""gift1"" versus ""gift2"" (b1 versus b2) we can write this estimate statement:</p>

<pre><code>estimate ""gift1 versus gift 2"" 
 (1-(1 / (1 + exp(-b0_f -b1_f))))*(exp(b0_h + b1_h)) - (1-(1 / (1 + exp(-b0_f -b2_f))))*(exp(b0_h + b2_h)) ; 
</code></pre>

<p>Can R do this?</p>
"
"NaN","NaN"," 15814","<p>In R, I am trying to reproduce the predict() output value of the orthogonal polynomial regression below.  Based on my understanding of polynomial regression, I get 0.03869436 which is different from 0.05947406.  Can anyone please help me by providing the explicit formulation of the predict output as a function of the fit model coefficients and p variable?</p>

<pre><code>&gt; q0 &lt;- c(0.200,0.100,0.050,0.025)
&gt; p0 &lt;- c(0.325,0.409,0.477,0.534)
&gt; p &lt;- 0.4612118
&gt; fit &lt;- lm(q0 ~ poly(p0,3))
&gt; predict(fit, newdata = list(p0 = p))
         1 
0.05947406
&gt; # which is not the same as f(p) = (beta_0) + (beta_1)*p + (beta_2)*(p^2) + (beta_3)*(p^3) below
&gt; as.numeric(fit$coefficients[1] + fit$coefficients[2]*p + fit$coefficients[3]*(p^2) + fit$coefficients[4]*(p^3))
[1] 0.03869436
</code></pre>

<p>My internet searches have not turned up anything yet.  Thank you.</p>
"
"0.052777981396926","0.0502942438178979"," 15900","<p>I plan to do a simulation study where I compare the performance of several robust correlation techniques with different distributions (skewed, with outliers, etc.). With <em>robust</em>, I mean the ideal case of being robust against a) skewed distributions, b) outliers, and c) heavy tails.</p>

<p>Along with the Pearson correlation as a baseline, I was thinking to include following more robust measures:</p>

<ul>
<li>Spearman's $\rho$</li>
<li>Percentage bend correlation (Wilcox, 1994, [1])</li>
<li>Minimum volume ellipsoid, minimum covariance determinant (<code>cov.mve</code>/ <code>cov.mcd</code> with the <code>cor=TRUE</code> option)</li>
<li>Probably, the winsorized correlation</li>
</ul>

<p>Of course there are many more options (especially if you include robust regression techniques as well), but I want to restrict myself to the mostly used/ mostly promising approaches.</p>

<p><strong>Now I have three questions (feel free to answer only single ones):</strong></p>

<ol>
<li><strong>Are there other robust correlational methods I could/ should include?</strong></li>
<li><strong>Which robust correlation techniques are</strong> <em><strong>actually</em></strong>  <strong>used in your field?</strong>
<sub>(Speaking for psychological research: Except Spearman's $\rho$, I have never seen any robust correlation technique outside of a technical paper. Bootstrapping is getting more and more popular, but other robust statistics are more or less non-existent so far).</sub></li>
<li><strong>Are there already systematical comparisons of multiple correlation techniques that you know of?</strong></li>
</ol>

<p>Also feel free to comment the list of methods given above.</p>

<hr>

<p>[1] Wilcox, R. R. (1994). The percentage bend correlation coefficient. <em>Psychometrika</em>, 59, 601-616.</p>
"
"0.101414888671788","0.104695817324578"," 17552","<p>I have a gene expression data-set with log2-transformed expression values (no NAs) for 495 genes for 59 samples for which values of a continuous response variable (r) are also known (no NAs). I want to use leave-one-out cross validation to test if r of the test sample can be predicted from the sample's gene expression.</p>

<p>For this, I intend to use the <a href=""http://cran.r-project.org/web/packages/samr/index.html"" rel=""nofollow"">samr</a> R package for Significance Analysis of Microarrays to identify significant genes associated with r in the training set of samples. Then, I want to generate a linear model using the significant genes as variables, which will then be used to predict r of the test sample. I have tried the following code to begin with, but when I generate the model and examine it, I see many NAs in the model summary, which makes me suspect that I am doing something wrong.</p>

<p>Can someone tell me what I might be doing wrong?</p>

<p>Secondly, I will appreciate any comment on the use of nperms (in SAM) with a value of 100. Is it too low for an expression data-set for 495 genes. </p>

<pre><code># rVals with the r values is read as a vector from a row of a table for phenotypic data read from a tab-delimited file with sample-names as column names and phenotype features as row-names
# geneVals is the log2-transformed gene expression data-set read as a matrix from a tab-delimited file with sample-names as column names and gene-names as row-names

# Perform SAM with FDR of 5% and obtain list of significant genes

sam &lt;- SAM(x=geneVals, y=rVals, resp.type=c(""Quantitative""),
testStatistic=c(""standard""), regression.method=c(""standard""), logged2=TRUE, 
fdr.output=0.05, eigengene.number=1, knn.neighbors=10, nperms=100, 
genenames=as.vector(rownames(geneVals)))

sigGenes &lt;- rbind(sam$siggenes.table$genes.up, sam$siggenes.table$genes.lo)

# Generate linear model
toModel &lt;- data.frame(t(rbind(rVals, geneVals)), check.names=FALSE)
myModel &lt;- lm(toModel[c('r', sigGenes[,c(""Gene ID"")])])

# Examine model
summary(myModel)

...output...

Call:
lm(formula = toModel[c(""rVals"", sigGenes[, c(""Gene ID"")])])

Residuals:
ALL 59 residuals are 0: no residual degrees of freedom!

Coefficients: (58 not defined because of singularities)
           Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)   -18.29363         NA      NA       NA
`let-7e`       -1.70545         NA      NA       NA
`miR-125a-5p`   2.43177         NA      NA       NA
`miR-151-5p`    2.67439         NA      NA       NA
...
</code></pre>
"
"0.0304713817668003","0.029037395206952"," 18208","<p>I'm wondering how to interpret the coefficient standard errors of a regression when using the display function in R.</p>

<p>For example in the following output:</p>

<pre><code>lm(formula = y ~ x1 + x2, data = sub.pyth)
        coef.est coef.se
(Intercept) 1.32     0.39   
x1          0.51     0.05   
x2          0.81     0.02   

n = 40, k = 3
residual sd = 0.90, R-Squared = 0.97
</code></pre>

<p>Does a higher standard error imply greater significance? </p>

<p>Also for the residual standard deviation, a higher value means greater spread, but the R squared shows a very close fit, isn't this a contradiction?</p>
"
"0.0609427635336005","0.0580747904139041"," 18490","<p>I asked this question on Stackoverflow:
<a href=""http://stackoverflow.com/questions/8142118/incidence-rate-ratios-in-r-poisson-regression"">http://stackoverflow.com/questions/8142118/incidence-rate-ratios-in-r-poisson-regression</a>
 and was advised to post here instead.
I have data that looks like this</p>

<pre><code>   sex agecat  cases population

1 male    0-4  12     126526
2 male    5-9  12     128375
3 female  0-4  11     129280
4 male    10-14 4     127910
5 female  0-4  13     127158
6 male    0-4  8      125125
</code></pre>

<p>I want to duplicate the output I get in stata with this command</p>

<pre><code>poisson cases i.agecat, exp(pop) irr
</code></pre>

<p>which gives output such as:</p>

<pre><code>   cases |        IRR   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
---------+----------------------------------------------------------------
  agecat |
      2  |   .5125755   .0530442    -6.18   0.000     .4578054    .6669639
      3  |    .323456   .0381304    -9.60   0.000     .2665044    .4172274
population | (exposure)
</code></pre>

<p>in R with a command such as</p>

<pre><code>glm(cases~agecat, family = poisson(link = ""log"")
</code></pre>

<p>I know I need to exponentiate the coefficients and confidence intervals, but I think I also need some kind of offset so so that the intercept is zero; and adjust for per unit population vs baseline. </p>

<p>Can anyone help/advise ?</p>

<p>Thanks
EDIT: The question on SO has been answered, but I posted more detail here. In particular, I think the issue has to do with adjusting for population size in stata with exp(pop) - and how to replicate this in R. </p>
"
"0.15537417023505","0.142367543061687"," 18576","<p><strong>Update</strong>: Sorry for another update but I've found some possible solutions with fractional polynomials and the competing risk-package that I need some help with.</p>

<hr>

<h2>The problem</h2>

<p>I can't find an easy way to do a time dependent coefficient analysis is in R. I want to be able to take my variables coefficient and do it into a time dependent coefficient (not variable) and then plot the variation against time:</p>

<p>$\beta_{my\_variable}=\beta_0+\beta_1*t+\beta_2*t^2...$</p>

<h2>Possible solutions</h2>

<h3>1) Splitting the dataset</h3>

<p>I've looked at <a href=""http://anson.ucdavis.edu/~johnson/st222/lab8/lab8.htm"">this</a> example (Se part 2 of the lab session) but the creation of a separate dataset seems complicated, computationally costly and not very intuitive...</p>

<h3>2) Reduced Rank models - The coxvc package</h3>

<p>The <a href=""https://www.msbi.nl/dnn/Research/SurvivalAnalysis/Coxmodelswithtimevaryingeffects.aspx"">coxvc package</a> provides an elegant way of dealing with the problem - here's a <a href=""https://openaccess.leidenuniv.nl/bitstream/handle/1887/4918/Appendix.pdf?sequence=5"">manual</a>. The problem is that the author is no longer developing the package (last version is since 05/23/2007), after some e-mail conversation I've gotten the package to work but one run took 5 hours on my dataset (140 000 entries) and gives extreme estimates at the end of the period. You can find a slightly updated <a href=""http://pastebin.com/uiPy0ueF"">package here</a> - I've mostly just updated the plot function. </p>

<p>It might be just a question of tweaking but since the software doesn't easily provide confidence intervals and the process is so time consuming I'm looking right now at other solutions. </p>

<h3>3) The timereg package</h3>

<p>The impressive <a href=""http://cran.r-project.org/web/packages/timereg/index.html"">timereg package</a> also addresses the problem but I'm not certain of how to use it and it doesn't give me a smooth plot.</p>

<h3>4) Fractional Polynomial Time (FPT) model</h3>

<p>I found Anika Buchholz' excellent dissertation on <a href=""http://deposit.ddb.de/cgi-bin/dokserv?idn=1008218782&amp;dok_var=d1&amp;dok_ext=pdf&amp;filename=1008218782.pdf"">""Assessment of timeâ€“varying longâ€“term effects of therapies and prognostic factors""</a> that does an excellent job covering different models. She concludes that <a href=""http://onlinelibrary.wiley.com/doi/10.1002/bimj.200610328/abstract"">Sauerbrei et al's proposed FPT</a> seems to be the most appropriate for time-dependent coefficients:</p>

<blockquote>
  <p>FPT is very good at detecting time-varying effects, while the Reduced Rank approach results in far too complex models, as it does not include selection of time-varying effects.</p>
</blockquote>

<p>The research seems very complete but it's slightly out of reach for me. I'm also a little wondering since she happens to work with Sauerbrei. It seems sound though and I  guess the analysis could be done with the <a href=""http://cran.r-project.org/web/packages/mfp/index.html"">mfp package</a> but I'm not sure how.</p>

<h3>5) The cmprsk package</h3>

<p>I've been thinking of doing my competing risk analysis but the calculations have been to time-consuming so I switched to the regular cox regression. The <a href=""http://www.inside-r.org/packages/cran/cmprsk/docs/crr"">crr</a> has thoug an option for time dependent covariates:</p>

<pre><code>....
cov2        matrix of covariates that will be multiplied 
            by functions of time; if used, often these 
            covariates would also appear in cov1 to give 
            a prop hazards effect plus a time interaction
....
</code></pre>

<p>There is the quadratic example but I'm don't quite follow where the time actually appears and I'm not sure of how to display it. I've also looked at the test.R file but the example there is basically the same...</p>

<h2>My example code</h2>

<p>Here's an example that I use to test the different possibilities</p>

<pre><code>library(""survival"")
library(""timereg"")
data(sTRACE)

# Basic cox regression    
surv &lt;- with(sTRACE, Surv(time/365,status==9))
fit1 &lt;- coxph(surv~age+sex+diabetes+chf+vf, data=sTRACE)
check &lt;- cox.zph(fit1)
print(check)
plot(check, resid=F)
# vf seems to be the most time varying

######################################
# Do the analysis with the code from #
# the example that I've found        #
######################################

# Split the dataset according to the splitSurv() from prof. Wesley O. Johnson
# http://anson.ucdavis.edu/~johnson/st222/lab8/splitSurv.ssc
new_split_dataset = splitSuv(sTRACE$time/365, sTRACE$status==9, sTRACE[, grep(""(age|sex|diabetes|chf|vf)"", names(sTRACE))])

surv2 &lt;- with(new_split_dataset, Surv(start, stop, event))
fit2 &lt;- coxph(surv2~age+sex+diabetes+chf+I(pspline(stop)*vf), data=new_split_dataset)
print(fit2)

######################################
# Do the analysis by just straifying #
######################################
fit3 &lt;- coxph(surv~age+sex+diabetes+chf+strata(vf), data=sTRACE)
print(fit3)

# High computational cost!
# The price for 259 events
sum((sTRACE$status==9)*1)
# ~240 times larger dataset!
NROW(new_split_dataset)/NROW(sTRACE)

########################################
# Do the analysis with the coxvc and   #
# the timecox from the timereg library #
########################################
Ft_1 &lt;- cbind(rep(1,nrow(sTRACE)),bs(sTRACE$time/365,df=3))
fit_coxvc1 &lt;- coxvc(surv~vf+sex, Ft_1, rank=2, data=sTRACE)

fit_coxvc2 &lt;- coxvc(surv~vf+sex, Ft_1, rank=1, data=sTRACE)

Ft_3 &lt;- cbind(rep(1,nrow(sTRACE)),bs(sTRACE$time/365,df=5))
fit_coxvc3 &lt;- coxvc(surv~vf+sex, Ft_3, rank=2, data=sTRACE)

layout(matrix(1:3, ncol=1))
my_plotcoxvc &lt;- function(fit, fun=""effects""){
    plotcoxvc(fit,fun=fun,xlab='time in years', ylim=c(-1,1), legend_x=.010)
    abline(0,0, lty=2, col=rgb(.5,.5,.5,.5))
    title(paste(""B-spline ="", NCOL(fit$Ftime)-1, ""df and rank ="", fit$rank))
}
my_plotcoxvc(fit_coxvc1)
my_plotcoxvc(fit_coxvc2)
my_plotcoxvc(fit_coxvc3)

# Next group
my_plotcoxvc(fit_coxvc1)

fit_timecox1&lt;-timecox(surv~sex + vf, data=sTRACE)
plot(fit_timecox1, xlab=""time in years"", specific.comps=c(2,3))
</code></pre>

<p>The code results in these graphs: Comparison of <a href=""http://i.stack.imgur.com/e2aSr.png"">different settings for coxvc</a> and  of the 
<a href=""http://i.stack.imgur.com/zXz1H.png"">coxvc and the timecox</a> plots. I guess the results are ok but I don't think I'll be able to explain the timecox graph - it seems to complex...</p>

<h2>My (current) questions</h2>

<ul>
<li>How do I do the FPT analysis in R?</li>
<li>How do I use the time covariate in cmprsk?</li>
<li>How do I plot the result (preferably with confidence intervals)?</li>
</ul>
"
"0.0812570180448007","0.0774330538852055"," 18880","<p>Recently I have opened a question here to understand the output of a GARCH model.
My goal is to understand if the series I'm checking is heteroscedastic or not.</p>

<p>I'm using the <code>garch()</code> function from the <a href=""http://cran.r-project.org/web/packages/tseries/index.html"" rel=""nofollow"">tseries</a> package.</p>

<p>First I built a linear regression like this:</p>

<pre><code>mod &lt;- lm(a ~ b)
</code></pre>

<p>Then I need to check if the residuals of this linear regression present heteroscedasticity.</p>

<p>I did:</p>

<pre><code>g &lt;- garch(resid(mod), order(c(1,1)))
</code></pre>

<p>and then </p>

<pre><code>summary(g)
</code></pre>

<p>I get the follow output:</p>

<pre><code>&gt; summary(g) 

Call: 
garch(x = lm(A ~ B)$resi, order = order(c(1, 1))) 

Model: 
GARCH(1,2) 

Residuals: 
    Min      1Q  Median      3Q    Max 
-4.2058 -1.0262  0.1404  1.1069  3.6553 

Coefficient(s): 
    Estimate  Std. Error  t value Pr(&gt;|t|)    
a0 3.361e-04  9.352e-05    3.594 0.000326 *** 
a1 3.045e-01  4.486e-02    6.787 1.14e-11 *** 
a2 1.209e-06  8.855e-02    0.000 0.999989    
b1 4.938e-01  1.060e-01    4.660 3.17e-06 *** 
--- 
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Diagnostic Tests: 
    Jarque Bera Test 

data:  Residuals 
X-squared = 18.84, df = 2, p-value = 8.108e-05 


    Box-Ljung test 

data:  Squared.Residuals 
X-squared = 49.7251, df = 1, p-value = 1.769e-12 
</code></pre>

<p>Thanks to the user who answered my question, I now understand that the <code>ao</code> is the intercept and the other <code>a1</code> and <code>b1</code> are the coefficients I need to check to understand if this time series is heteroscedastic or not.</p>

<p>The problem (doubt) is that now I also see <code>a2</code> in the regression table: What does it stand for?</p>

<p>Is it correct to say that if all coefficients have a $p$-value above 0.05 (...)</p>
"
"0.0914141453004008","0.0871121856208561"," 19888","<p>I want to fit a regression model to see whether these is changes in the proportion of First-year students over years.  I have count data for the total count of First-year students (FirstTimeStudents)  and total count of all students (TotalStudents).  I want to fit a GEE model with this data and wrote the following code.  My output for coefficients are all NA</p>

<pre><code>combFirstYear = geeglm(cbind(data$FirstTimeStudents, data$TotalStudents -
data$FirstTimeStudents) ~ Year, family=binomial(logit), data=data, id=DeptID,
corstr=""independence"")

summary(combFirstYear)
</code></pre>

<p>The reason I fit the model this way is because I read on <a href=""http://plantecology.syr.edu/fridley/bio793/glm.html"" rel=""nofollow"">this website</a> about doing similar thing using glm and it works.  But for some reasons, it does not work with geeglm.   If anyone can think of different way of answering this question in terms of proportion, I'd really appreciate it too.</p>

<p>It says: ""R can handle this using glm with the binomial(link=""logit"") family, with a dependent variable that is actually a two-vector object, the first being the number of 'successes' and the second the number of 'failures'.""</p>

<blockquote>
  <p>Here's an example by modeling hemlock cover with respect to total
  cover.  The example is a bit forced (as these are not actual samples
  of individuals) but you get the idea.  We start by generating the
  summed cover of all species for each plot:</p>
  
  <p><code>sumcover = tapply(dat</code>$<code>cover,dat</code>$plotID,sum)`</p>
  
  <p><code>coverdat = data.frame(names(sumcover),sumcover)</code></p>
  
  <p>Now we merge these data with our dat5 hemlock object, based on plotID:</p>
  
  <p><code>dat6 = merge(dat5,coverdat,all.x=T,by.x=1,by.y=1)</code></p>
  
  <p>And create our response variables of hemlock cover (successes) and
  total cover minus hemlock cover (failures):</p>
  
  <p><code>cover.y = cbind(dat6$cover,dat6$sumcover-dat6$cover)</code></p>
  
  <p>We can now model these data as a binomial process of varying number of
  trials per observation: <code>glm6 =
  glm(cover.y~disturb*elev,data=dat6,family=binomial)</code></p>
  
  <p><code>summary(glm6)</code></p>
</blockquote>
"
"0.105869651339174","0.100887413963854"," 20417","<p>I am working on a Monte Carlo study of bootstrapping in an AR(1) model for a homework assignment (I'm using Matlab). The goal is to say something about the empirical rejection probabilities of the bootstrap in this specific context. However, I've been running into some computational problems. </p>

<p>Some context first. Suppose one has a regression $y_t = \gamma + \rho y_{t-1} + \varepsilon_t$, and wants to test the two-tailed hypothesis $\rho = \rho_0$ by the bootstrap. To do this, one first estimates the regression by, say, OLS, and then uses the obtained residuals $\hat \varepsilon_t$ and coefficients $\hat \gamma, \; \hat \rho$ for constructing the bootstrap samples. However, in this case, the bootstrap sample has to be constructed recursively. What is even worse for the computations, I have to repeat these calculations many times using different simulated data sets <em>and</em> different values of $\rho$. (The reason is that this allows one to get some sense of the empirical level of the test at different values of $\rho$.) </p>

<p>So far I have been doing it in the naive way of just estimating many regressions, generating the bootstrap samples, and so on. I was able to eliminate quite a few loops by using the <code>filter</code> function when generating bootstrap samples but still computational times are quite huge. I compared it with the built-in bootstrap in <a href=""http://gretl.sourceforge.net/"" rel=""nofollow"">gretl</a>, and it is many times slower. (I'm aware that gretl is written in C, but I sense that there is more to this than that.)</p>

<p>Hence, my question would be: what general advice could you give to implement bootstrapping efficiently in a matrix based language? Are there some smart ways to avoid estimation of many OLS regressions and generation of the bootstrap samples? Although I'm using Matlab for the computations, but if you have some general advice on say, doing this with R, all the help would be very much appreciated. Also, since this is a homework assignment I have to do, using existing libraries is not really an option. :)</p>
"
"0.0304713817668003","0.029037395206952"," 20438","<p>I fitted a GEE model using the function <code>genZcor</code> with user defined
correlation matrix. I want to get the var-cov matrix of the regression
coefficients. But the output provides only limited information.</p>

<p>I would be very much thankful if you could kindly let me know how to get
it since I am struggling lot getting this.</p>
"
"0.139637413476259","0.133066061520206"," 20452","<p>My primary question is how to interpret the output (coefficients, F, P) when conducting a Type I (sequential) ANOVA? </p>

<p>My specific research problem is a bit more complex, so I will break my example into parts. First, if I am interested in the effect of spider density (X1) on say plant growth (Y1) and I planted seedlings in enclosures and manipulated spider density, then I can analyze the data with a simple ANOVA or linear regression. Then it wouldn't matter if I used Type I, II, or III Sum of Squares (SS) for my ANOVA. In my case, I have 4 replicates of 5 density levels, so I can use density as a factor or as a continuous variable. In this case, I prefer to interpret it as a continuous independent (predictor) variable. In R I might run the following:</p>

<pre><code>lm1 &lt;- lm(y1 ~ density, data = Ena)
summary(lm1)
anova(lm1)
</code></pre>

<p>Running the anova function will make sense for comparison later hopefully, so please ignore the oddness of it here. The output is:</p>

<pre><code>Response: y1
          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
density    1 0.48357 0.48357  3.4279 0.08058 .
Residuals 18 2.53920 0.14107 
</code></pre>

<p>Now, let's say I suspect that the starting level of inorganic nitrogen in the soil, which I couldn't control, may have also significantly affected the plant growth. I'm not particularly interested in this effect but would like to potentially account for the variation it causes. Really, my primary interest is in the effects of spider density (hypothesis: increased spider density causes increased plant growth - presumably through reduction of herbivorous insects but I'm only testing the effect not the mechanism). I could add the effect of inorganic N to my analysis. </p>

<p>For the sake of my question, let's pretend that I test the interaction density*inorganicN and it's non-significant so I remove it from the analysis and run the following main effects:</p>

<pre><code>&gt; lm2 &lt;- lm(y1 ~ density + inorganicN, data = Ena)
&gt; anova(lm2)
Analysis of Variance Table

Response: y1
           Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
density     1 0.48357 0.48357  3.4113 0.08223 .
inorganicN  1 0.12936 0.12936  0.9126 0.35282  
Residuals  17 2.40983 0.14175 
</code></pre>

<p>Now, it makes a difference whether I use Type I or Type II SS (I know some people object to the terms Type I &amp; II etc. but given the popularity of SAS it's easy short-hand). R anova{stats} uses Type I by default. I can calculate the type II SS, F, and P for density by reversing the order of my main effects or I can use Dr. John Fox's ""car"" package (companion to applied regression). I prefer the latter method since it is easier for more complex problems.</p>

<pre><code>library(car)
Anova(lm2)
            Sum Sq Df F value  Pr(&gt;F)  
density    0.58425  1  4.1216 0.05829 .
inorganicN 0.12936  1  0.9126 0.35282  
Residuals  2.40983 17  
</code></pre>

<p>My understanding is that type II hypotheses would be, ""There is no linear effect of x1 on y1 given the effect of (holding constant?) x2"" and the same for x2 given x1. I guess this is where I get confused. <strong>What is the hypothesis being tested by the ANOVA using the type I (sequential) method above compared to the hypothesis using the type II method?</strong></p>

<p>In reality, my data is a bit more complex because I measured numerous metrics of plant growth as well as nutrient dynamics and litter decomposition. My actual analysis is something like:</p>

<pre><code>Y &lt;- cbind(y1 + y2 + y3 + y4 + y5)
# Type II
mlm1 &lt;- lm(Y ~ density + nitrate + Npred, data = Ena)
Manova(mlm1)

Type II MANOVA Tests: Pillai test statistic
        Df test stat approx F num Df den Df  Pr(&gt;F)    
density  1   0.34397        1      5     12 0.34269    
nitrate  1   0.99994    40337      5     12 &lt; 2e-16 ***
Npred    1   0.65582        5      5     12 0.01445 * 


# Type I
maov1 &lt;- manova(Y ~ density + nitrate + Npred, data = Ena)
summary(maov1)

          Df  Pillai approx F num Df den Df  Pr(&gt;F)    
density    1 0.99950     4762      5     12 &lt; 2e-16 ***
nitrate    1 0.99995    46248      5     12 &lt; 2e-16 ***
Npred      1 0.65582        5      5     12 0.01445 *  
Residuals 16                                           
</code></pre>
"
"0.068136080998913","0.0649295895722714"," 20645","<p>Possible warning: basic question ahead.</p>

<p>Let's say that I model whether I wear red shoes depending on the weather. Red shoes, which is my dependent variable, is a dichotomous variable as I either wear them or don't. Weather is a variable with five 'levels' and I'm trying to find the probability that I will wear read shoes.</p>

<p>Let's say I model out this relationship with a logistic regression model in R:</p>

<pre><code>mod = glm(shoes ~ weather, data=mydat, family=binomial(link=""logit""))
</code></pre>

<p>Now, what I am interested in is finding what are the probabilities for wearing red shoes for each of the five 'levels' in weather. So perhaps I might have a 20% probability of wearing red shoes when cold, 30% when mild, 40% when warm, and so forth.</p>

<p>I'm wondering if modeling is a requirement for finding this information?
If so, how does one go from somewhat meaningful regression coefficients to meaningful probabilities in R?</p>
"
"0.0746393370862076","0.0711268017165705"," 20672","<p>I have two continuous variables, X and Y, that are correlated - they are not independent. To correct for non-independence, I have a known correlation structure, a matrix S.</p>

<p>If one calls <code>gls(Y ~ X, correlation = S)</code>, what I think happens is that, internally, gls() transforms X and Y in some way so that the regression ends up being <code>S^(-1)*Y = S^(-1) * X</code>.</p>

<p>How is this transformation actually performed? From the literature I've consulted, I've seen everything from:</p>

<pre><code>X.transformed &lt;- solve(chol(S)) %*% X 
#The inverse of the Choleski decomposition of S times the vertical vector X, 
#which in my case does nothing to the data
</code></pre>

<p>to</p>

<pre><code>X.transformed &lt;- chol(solve(S)) %*% X 
# which has negative values and gives meaningless values of X
</code></pre>

<p>Another method I've seen is transforming the dependent variable by </p>

<pre><code>chol(solve(S)) %*% Y 
</code></pre>

<p>and the independent variable by </p>

<pre><code>chol(solve(S)) %*% cbind(1,X) 
</code></pre>

<p>and doing the linear model using the transformed intercept terms in the first column of the X matrix: </p>

<pre><code>lm(Y ~ X - 1)
</code></pre>

<p>On a related note, is there any point to manually transforming the data in order to plot it? Do the transformed values have any meaning, or are they simply there to estimate regression coefficients? (In other words, if X is a variable of body mass figures, X values are not necessarily errant if they're negative since they're still linear?) I suppose it would follow from this that an $R^2$ statistic on transformed variables is also meaningless?</p>
"
"0.0609427635336005","0.0580747904139041"," 20939","<p>I have a regression with a harmonic effect of day of the year, which interacts with other variables. I am not sure how to interpret the coefficients. My model is:</p>

<pre><code>m1 &lt;- lme(lcount ~ AirT + sin(2*pi/360*DOY) + cos(2*pi/360*DOY) + 
          AirT*sin(2*pi/360*DOY) + AirT*cos(2*pi/360*DOY) + RainAmt + RainAmt*AirT,
          random = ~1|plot))
</code></pre>

<p>I get significant interaction effects of air temperature with the linearized harmonic day of the year (DOY) function. My response variable is the log of animal counts on each day. I want to describe how the effect of air temperature on animal observations changes depending on the day of the year.</p>

<p>Does anyone have suggestions on how I can interpret my beta values and/or how I can visualize the effect? I am using R but am not that skilled. The package I used for analyzing my data is nlme.</p>

<p>EDIT: <strong>My primary goals are (1) to describe the response of animals to environmental variables and (2) to predict future activity periods (i.e. when and under what conditions should a research bother trying to catch these animals).</strong> So if there is a better way to model this data, I would be interested in hearing it (such as cubic splines - see comments below).</p>
"
"0.0545088647991304","0.0649295895722714"," 21043","<p>R linear regression seems to fail if my predictor variance is very small, but nonzero:</p>

<pre><code>&gt; reg = lm(V1~V2,data)
&gt; summary(reg)

Call:
lm(formula = V1 ~ V2, data = data)

Residuals:
Min      1Q  Median      3Q     Max
-15.968  -4.898   1.627   5.218   8.468

Coefficients: (1 not defined because of singularities)
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  11.7963     0.6036   19.54   &lt;2e-16 ***
V2                NA         NA      NA       NA
</code></pre>

<p>Here is an excerpt from my data... (it comes out this way after boxcox...)</p>

<pre><code>&gt; print(data$V2,digits=15)
 [1] -0.640196668095416 -0.640196668115515 -0.640196668075674 -0.640196668083867
 [5] -0.640196668103316 -0.640196668073982 -0.640196668094188 -0.640196668081038
etc
</code></pre>

<p>And here is the regression working if I manually remove the mean value:</p>

<pre><code>&gt; shifted = data$V2+0.640196668
&gt; shifted
 [1] -9.541601e-11 -1.155150e-10 -7.567402e-11 -8.386702e-11 -1.033160e-10
 [6] -7.398204e-11 -9.418799e-11 -8.103807e-11 -8.288503e-11 -9.411305e-11
etc

&gt; reg = lm(data$V1~shifted)
&gt; reg

Coefficients:
(Intercept)      shifted
  6.308e+00   -5.771e+10
</code></pre>

<p>Can anyone tell me if I'm using <code>lm</code> wrong?  Thank you...</p>
"
"0.068136080998913","0.0649295895722714"," 21067","<p>It's been a while since I've thought about or used a robust logistic regression model. However, I ran a few logits yesterday and realized that my probability curve was being affected by some 'extreme' values, and particularly low ones. However, when I went to run a robust logit model, I got the same results as I did in my logit model.</p>

<p>Under what circumstances should a robust logit produce different results from a traditional logit model? (in terms of coefficients)</p>

<p>R Code:</p>

<pre><code>&gt; library(Design)
&gt; ddist&lt;- datadist(dlmydat)
&gt; options(datadist='ddist')
&gt; me = lrm(factor(dlstatus) ~ dlour_bid, data=dlmydat)
&gt; me

Logistic Regression Model

lrm(formula = factor(dlstatus) ~ dlour_bid, data = dlmydat)


Frequencies of Responses
  1   2 
906 154 

       Obs  Max Deriv Model L.R.       d.f.          P          C        Dxy      Gamma      Tau-a         R2      Brier 
      1060      3e-05     170.11          1          0       0.81      0.619      0.621      0.154      0.263      0.105 

          Coef      S.E.      Wald Z P
Intercept -5.233549 0.3731235 -14.03 0
dlour_bid  0.005367 0.0004925  10.90 0

&gt; library(car)
&gt; dlmod = glm(factor(dlstatus) ~ dlour_bid, data=dlmydat, family=binomial(link=""logit""))
&gt; summary(dlmod)

Call:
glm(formula = factor(dlstatus) ~ dlour_bid, family = binomial(link = ""logit""), 
    data = dlmydat)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.2345  -0.5687  -0.3059  -0.1739   2.6999  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -5.2335492  0.3731235  -14.03   &lt;2e-16 ***
dlour_bid    0.0053667  0.0004925   10.90   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 878.61  on 1059  degrees of freedom
Residual deviance: 708.50  on 1058  degrees of freedom
AIC: 712.5

Number of Fisher Scoring iterations: 6
</code></pre>
"
"0.0304713817668003","0.029037395206952"," 21137","<p>Considering a simple linear regression model Y= beta0+beta1 x X, with beta0 and beta1 computed, I have to estimate the expected X given a new Y and 95% confidence intervals. 
I used the formula X=(Y-beta0)/slope. How can I compute in R the 95% interval for the calculated value of ind, given a new value of the height?</p>

<blockquote>
  <p>head(df)</p>
</blockquote>

<pre><code>   ind height
1 4.27    174
2 3.60    159
3 3.61    175

summary(lm(df$ind~df$height))

Call:
lm(formula = df$ind ~ df$height)

Residuals:
 Min       1Q   Median       3Q      Max 

-0.56263 -0.27596  0.03866  0.26632  0.55440 

Coefficients:
         Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept) -0.968895   1.512371  -0.641  0.52903   

df$height    0.027871   0.008985   3.102  0.00562 **

Residual standard error: 0.3287 on 20 degrees of freedom
Multiple R-squared: 0.3248,     Adjusted R-squared: 0.2911 
F-statistic: 9.622 on 1 and 20 DF,  p-value: 0.005621 
</code></pre>

#

<p>I tried:</p>

<pre><code>pred.frame &lt;- data.frame(ind=seq(3,5,0.25)) 
predict(bclm,int=""c"",level=0.95,data=pred.frame)
    fit      lwr      upr
1  174.5780 169.3146 179.8414
2  166.7696 163.6419 169.8973
3  166.8862 163.7806 169.9917
...............
</code></pre>
"
"0.0430930413588572","0.0410650781176591"," 21257","<p>I am performing simple (first-order terms) linear regression on data having several categorical variables (i.e. factors), and it is often desired that for each factor, one of the levels should add nothing to the regressand, and the other levels should add positive values to the regressand.  When I perform a regression analyses, however, I often get a lot of negative coefficients.</p>

<p>Is there a <strong>non-manual</strong> way of choosing which levels of a factor should be used as regressor variables in order to maximize the number of positive coefficients in the equation?  In other words, how can I get R to do this (somewhat tedious) task for me?</p>
"
"0.0215465206794286","0.0410650781176591"," 21565","<p>I see a similar constrained regression here:</p>

<p><a href=""http://stats.stackexchange.com/questions/12484/constrained-linear-regression-through-a-specified-point"">Constrained linear regression through a specified point</a></p>

<p>but my requirement is slightly different. I need the coefficients to add up to 1. Specifically I am regressing the returns of 1 foreign exchange series against 3 other foreign exchange series, so that investors may replace their exposure to that series with a combination of exposure to the other 3, but their cash outlay must not change, and preferably (but this is not mandatory), the coefficients should be positive. </p>

<p>I have tried to search for constrained regression in R and Google but with little luck. </p>
"
"0.0430930413588572","0.0205325390588295"," 21596","<p>Sorry if this is a newb question; I'm trying to teach myself statistics for the first time.  I think I have the basic procedure down, but I'm struggling to execute it with R.</p>

<p>So, I'm trying to evaluate the significance of regression coefficients in a multiple linear regression of form</p>

<p>$$ \hat y = X \hat \beta$$</p>

<p>I think the t-statistic for testing $H_0: \hat \beta_j = 0, H_a: \hat \beta_j \neq 0$ is given by</p>

<p>$$t_0 = \frac{\hat \beta_j - 0}{\text{se}(\hat \beta_j)} = \frac{\hat \beta_j}{\sqrt{\hat \sigma^2 C_{jj}}} = \frac{\hat \beta_j}{\sqrt{C_{jj} SS_{Res}/(n-p)}}$$
where $C_{jj}$ is the $j^{th}$ diagonal entry in $(X&#39;X)^{-1}$.</p>

<p>So far, so good. I know how to calculate all of these values using matrix operations in R.  But in order to reject the null, the book says I need
$$|t_0| &gt; t_{\alpha/2,n-p}$$</p>

<p><strong>How can I compute this critical value $t_{\alpha/2,n-p}$ using R?</strong>  Right now the only way I know how to find these values is by looking in the table in the back of the book.  There must be a better way.</p>
"
"0.0430930413588572","0.0410650781176591"," 22462","<p>Let's say that I am putting together a logistic regression model where I am predicting 
something (y) based on the day of the week. However, the model needs to account for each single day.</p>

<p>Therefore, instead of:</p>

<pre><code>y = B0 + B1*(day)
</code></pre>

<p>Where day is a categorical variable with 7 levels.</p>

<p>It would be:</p>

<pre><code>y = B0 + B1*(monday) + B2*(tuesday) + B3*(wednesday) + ... + B7*(sunday)
</code></pre>

<p>I'm basically thinking that each day needs a separate coefficient because each 
has a different affect on y. However, I think each will need to be a dummy variable 
so that for monday, 1 is for monday, and 0 for not monday, and so forth. </p>

<p>I'm just curious if there is a statistical logic to doing it the second way 
with separate days? What's the best way to do this?</p>
"
"0.0791669720953889","0.0754413657268469"," 24072","<p>I am running the following unit root test (Dickey-Fuller) on a time series using the <code>ur.df()</code> function in the <code>urca</code> package.</p>

<p>The command is:</p>

<pre><code>summary(ur.df(d.Aus, type = ""drift"", 6))
</code></pre>

<p>The output is:</p>

<pre><code>############################################### 
# Augmented Dickey-Fuller Test Unit Root Test # 
############################################### 

Test regression drift 


Call:
lm(formula = z.diff ~ z.lag.1 + 1 + z.diff.lag)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.266372 -0.036882 -0.002716  0.036644  0.230738 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)  0.001114   0.003238   0.344  0.73089   
z.lag.1     -0.010656   0.006080  -1.753  0.08031 . 
z.diff.lag1  0.071471   0.044908   1.592  0.11214   
z.diff.lag2  0.086806   0.044714   1.941  0.05279 . 
z.diff.lag3  0.029537   0.044781   0.660  0.50983   
z.diff.lag4  0.056348   0.044792   1.258  0.20899   
z.diff.lag5  0.119487   0.044949   2.658  0.00811 **
z.diff.lag6 -0.082519   0.045237  -1.824  0.06874 . 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.06636 on 491 degrees of freedom
Multiple R-squared: 0.04211,    Adjusted R-squared: 0.02845 
F-statistic: 3.083 on 7 and 491 DF,  p-value: 0.003445 


Value of test-statistic is: -1.7525 1.6091 

Critical values for test statistics: 
      1pct  5pct 10pct
tau2 -3.43 -2.86 -2.57
phi1  6.43  4.59  3.78
</code></pre>

<ol>
<li><p>What do the significance codes (Signif. codes) mean? I noticed that some of them where written against: z.lag.1, z.diff.lag.2, z.diff.lag.3 (the ""."" significance code) and z.diff.lag.5 (the ""**"" significance code).</p></li>
<li><p>The output gives me two (2) values of test statistic: -1.7525 and 1.6091. I know that the ADF test statistic is the first one (i.e. -1.7525). What is the second one then?</p></li>
<li><p>Finally, in order to test the hypothesis for unit root at the 95% significance level, I need to compare my ADF test statistic (i.e. -1.7525) to a critical value, which I normally get from a table. The output here seems to give me the critical values through. However, the question is: which critical value between ""tau2"" and ""phi1"" should I use.</p></li>
</ol>

<p>Thank you for your response.</p>
"
"0","0.029037395206952"," 24073","<p>I've been reading about stacked regression, as described, for example, <a href=""http://www.ulb.ac.be/di/map/gbonte/ecares/mod_aver.pdf"" rel=""nofollow"">here</a>. It seems it's important that when you regress against the first predictors, you require that the regression coefficients are non-negative. How does one do that in R? </p>
"
"0.0806196982594614","0.076825726438694"," 24131","<p>I am using <code>plm()</code> to estimate fixed-effects models of the form</p>

<pre><code>y ~ x + time + time:fixed_trait
</code></pre>

<p>where <code>fixed_trait</code> is a variable that varies across individuals but is constant within individuals.</p>

<p>The point of interacting <code>time</code> with <code>fixed_trait</code> is to permit the effect of <code>fixed_trait</code> to vary across time.  (I am working here from Paul Allison's recent booklet on fixed effects.  Citation appended.)</p>

<p><code>plm()</code> has no trouble estimating coefficients and standard errors for such models.  But <code>summary.plm()</code> can't calculate R^2 for these models.  This is the problem that I would like to fix.</p>

<p>Here is a minimal example:</p>

<pre><code>library(plm)
tmp &lt;- data.frame(ID=rep(1:3, 2), year=rep(0:1, each=3), 
                  y=rnorm(6), const=rep(1:3, 2))
fe1 &lt;- plm(y ~ year,              index=c('ID', 'year'), data=tmp)
fe2 &lt;- plm(y ~ year + year:const, index=c('ID', 'year'), data=tmp)
summary(fe1)  # works fine
summary(fe2)  # Error in crossprod(t(X), beta) : non-conformable arguments
</code></pre>

<p>Delving into <code>plm:::summary.plm</code> makes the problem clearer.  To calculate R^2, <code>plm</code> tries to do this:</p>

<pre><code>beta &lt;- coef(fe2)
X &lt;- model.matrix(fe2)
crossprod(t(X), beta)
</code></pre>

<p>This doesn't work because <code>beta</code> only includes estimates for <code>year1</code> and <code>year0:const</code>, while <code>X</code> also includes a column for <code>year1:const</code>.  In other words, <code>X</code> includes columns for both <code>year0:const</code> and <code>year1:const</code>, and it's impossible to estimate both of those coefficients.</p>

<p>A workaround is to create the interaction term ""by hand"" before entering it into the formula: </p>

<pre><code>tmp$yearXconst &lt;- tmp$year*tmp$const
fe3 &lt;- plm(y ~ year + yearXconst, index=c('ID', 'year'), data=tmp)
summary(fe3)  # works fine
</code></pre>

<p>But this is cumbersome.  Short of this, is there anything that I can do to make <code>summary.plm</code> work with such models?</p>

<p>===</p>

<p>Allison, Paul D. 2009. Fixed Effects Regression Models. Los Angeles, CA: Sage.  See especially pages 19-21.</p>
"
"0.0430930413588572","0.0205325390588295"," 24193","<p>I am performing a returns analysis. The idea is to regress a time-series of returns on the returns of various asset classes. The beta coefficients must be constrained such that sum of the coefficients is 1 and no coefficient is less than 0 or greater than 1. These beta coefficients can then be interpreted as explaining what % of returns are explained by exposure to the various asset classes.</p>

<p>Are there any packages in R that let me setup the above regression and benefit from the attendant reporting on model fit statistics? Or do I need to do some homework on setting up constrained least squares optimization in R (please provide any references to recommended R packages)?</p>
"
"0.0304713817668003","0.029037395206952"," 24249","<p>I am doing some multiple regression using R's <code>lm()</code> method. I wonder whether there is an easy way to limit the impact of some predictors. I think <code>lm()</code> results give too much weight to certain predictors and I would like to somehow cap coefficients of certain predictors. Is it doable?</p>
"
"0.0861860827177143","0.0821301562353182"," 24948","<p>I have a reasonable understanding of why multicollinearity is a problem is regression models, along the <a href=""http://stats.stackexchange.com/questions/1149/is-there-an-intuitive-explanation-why-multicollinearity-is-a-problem-in-linear-r"">lines</a> of this excellent post.</p>

<p>To summarise my understanding, for a regression model of $y = \alpha + \beta_1x + \beta_2z$ (where $x$ and $z$ are correlated), beta coefficient estimates (as well as being unstable) are difficult to interpret, as a situation where you might increase $z$ without increasing $x$ is unlikely to occur, and not supported by the data.</p>

<p>I understand multicollinearity is less harmful to purely <em>predictive</em> as opposed to explanatory or descriptive models.</p>

<p>I'm interested in another interpretation:</p>

<p><em>If I decided to increase $z$, and let $x$ vary as it pleases in reaction, what would I see happen to $y$, accounting for the fact that $x$ is likely to move with $z$, and also have it's own effect?</em></p>

<p>In other words, accepting the causal interpretation that $x$ and $z$ both cause $y$, and are themselves correlated to some extent (.7 say), how would all three variables move if $z$ is (linearly) increased by some amount?</p>

<p>I've tried to model this sort of thing before, fitting $y = \alpha + \beta_1x + \beta_2z$ (model 1), and $x = \alpha + \beta_1z$ (model 2). Hypothetical increased $z$ values are produced, and resulting $x$ values are predicted with model 2. The hypothetical $x$ and $z$ values are used to predict $y$ using model 1. However this feels very unsatisfactory, complicated simulations are required to capture uncertainty (I used <code>sim</code> in <code>arm</code>). Additionally, my gut tells me that apart from being painfully inelegant, it's a bad idea for other reasons I can't put my finger on.</p>

<ul>
<li>Is such an 'observational'/conditional-when-I-feel-like-it interpretation possible?</li>
<li>Does anyone know of a better method for this interpretation?</li>
<li>Can anyone recommend a paper or <code>R</code> package along these lines?</li>
<li>Is the above multi-model mess at-all valid?</li>
</ul>

<p>I'm aware that a model along the lines of $y = \alpha + \beta_1z$ would yield a similar answer to the two-stage mess above, but would lose information in $x$.</p>

<p>I understand that these ideas are similar to structural equation modelling, but apart from having scant knowledge of SEM, I'm yet to find an <code>R</code> package which allows flexibly extending these models with different link functions for proportional odds models, etc.</p>
"
"0.0806196982594614","0.076825726438694"," 25702","<p>I have spent much time looking for a special package that could run the Pesaran(2007) unit root test (which assumes cross-sectional dependence unlike most others) and I have found none. So, I decided to do it manually; however, I don't know where I'm going wrong, because my results are very different from Microsoft Excel's results (in which it is done very easily).</p>

<p>My data frame is made up of 22 countries with 506 observations of daily price indices. Following is the model to run using the Pesaran(2007) unit root test:</p>

<p>(i) With an intercept only</p>

<p>$$\Delta Y_{i,t} = a_i + b_iY_{i,t-1} + c_i\overline{Y}_{t-1} + d_i\Delta\overline{Y}_{t-1}+ e_i\Delta\overline{Y}_{t-2}+ f_i\Delta\overline{Y}_{i,t-1}+ g_i\Delta\overline{Y}_{i,t-2} + \varepsilon_{i,t}$$</p>

<p>where $\overline{Y}$ is the cross-section average of the observations across countries at each time $t$ and $b$ is the coefficient of interest to us because it will allow us to compute the ADF test statistic and then determine whether the process is stationary or not.</p>

<p>I constructed each of these variables in the following way:</p>

<p>$\Delta Y_t$</p>

<pre><code>dif.yt = diff(yt) 
## yt is the object containing all the observations for a specific country 
## (e.g. Australia)
</code></pre>

<p>$Y_{t-1}$</p>

<pre><code>yt.lag.1 = lag(yt, -1)
</code></pre>

<p>$\overline{Y}_{t-1}$</p>

<pre><code>ybar.lag.1 = lag(c(rowMeans(x)), -1) 
## x is the object containing my entire data frame
</code></pre>

<p>$\Delta \overline{Y}_{t-1}$</p>

<pre><code>dif.ybar.lag.1 = diff(ybar.lag.1)
</code></pre>

<p>$\Delta \overline{Y}_{t-2}$</p>

<pre><code>dif.ybar.lag.2 = diff(lag(c(rowMeans(x)), -2))
</code></pre>

<p>$\Delta Y_{t-1}$</p>

<pre><code>dif.yt.lag.1 = diff(yt.lag.1)
</code></pre>

<p>$\Delta Y_{t-2}$</p>

<pre><code>dif.yt.lag.2 = diff(lag(yt, -2)
</code></pre>

<p>After constructing each variable individually, I then run the linear regression</p>

<pre><code>reg = lm(dif.yt ~ yt.lag.1[-1] + ybar.lag.1[-1] + dif.ybar.lag.1 + 
                  dif.ybar.lag.2 + dif.yt.lag.1 + dif.yt.lag.2)
summary(reg)
</code></pre>

<p>It is obvious that the explanatory variables in my regression equation differ in length, so I'd like to know whether there is a way in R to make all the variables of equal length (perhaps with a function).</p>

<p>Also, I'd like to know whether the procedure I used was correct and if there are more optimal ways.</p>
"
"0.0545088647991304","0.0519436716578171"," 25839","<p>First off, I'll say I am a biologist and new to the statistics side of things so excuse my ignorance</p>

<p>I have a data set that consists of a binary outcome and then a bunch of trinary explanatory variables that looks something like this:</p>

<pre><code>head()
 Category block21_hap1 block21_hap2 block21_hap3 block21_check
1        1            1            1            0             2
2        1            2            0            0             2
3        1            1            0            1             2
4        1            1            0            1             2
5        1            1            1            0             2
6        1            1            1            0             2
</code></pre>

<p>A quick summary of the data</p>

<pre><code>summary()
Category block21_hap1 block21_hap2 block21_hap3 block21_check
 1:718    0:293        0:777        0:1026       2:1467       
 0:749    1:709        1:577        1: 390                    
          2:465        2:113        2:  51  
</code></pre>

<p>and another summary grouped by outcome levels</p>

<pre><code>by(hap.ped.final, hap.ped.final$Category, summary)
hap.ped.final$Category: 1
 block21_hap1 block21_hap2 block21_hap3 block21_check
 0:146        0:374        0:518        2:718        
 1:336        1:286        1:174                     
 2:236        2: 58        2: 26                     
---------------------------------------------------------------------------- 
hap.ped.final$Category: 0
 block21_hap1 block21_hap2 block21_hap3 block21_check
 0:147        0:403        0:508        2:749        
 1:373        1:291        1:216                     
 2:229        2: 55        2: 25          
</code></pre>

<p>So I am trying to run logistic regression on this data. When I do this:</p>

<pre><code>fit = glm(Category~ block21_hap1 + block21_hap2 + block21_hap3, data = hap.ped.final ,family = ""binomial"")
summary(fit)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.301  -1.177   1.059   1.177   1.200  

Coefficients: (1 not defined because of singularities)
                             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)                 -0.039221   0.280110  -0.140    0.889
hap.ped.final$block21_hap11  0.123555   0.183087   0.675    0.500
hap.ped.final$block21_hap12  0.009111   0.295069   0.031    0.975
hap.ped.final$block21_hap21 -0.084334   0.183087  -0.461    0.645
hap.ped.final$block21_hap22 -0.013889   0.337468  -0.041    0.967
hap.ped.final$block21_hap31  0.201113   0.183087   1.098    0.272
hap.ped.final$block21_hap32        NA         NA      NA       NA

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2033  on 1466  degrees of freedom
Residual deviance: 2028  on 1461  degrees of freedom
AIC: 2040

Number of Fisher Scoring iterations: 3
</code></pre>

<p>So I don't really know what a singularity is or what's going wrong here that is throwing up NA's as a result of my analysis. Is it my data, or what I'm doing to it.
I tried googling the warning (or whatever you might call it) and I got some pages talking about collinearity and multilinearity, which I do not understand at all. 
Again, sorry for lack of knowledge here. I wish I had done more maths in undergrad. </p>
"
"0.114267681625501","0.10889023202607"," 25988","<p>An assumption of the ordinal logistic regression is the proportional odds assumption. Using R and the 2 packages mentioned I have 2 ways to check that but I have questions in each one.</p>

<p>1) Using the rms package</p>

<p>Given the next commands</p>

<pre><code>library(rms)
ddist &lt;- datadist(Ki67,Cyclin_E)
options(datadist='ddist')
f &lt;- lrm(grade ~Ki67+Cyclin_E);f
sf &lt;- function(y)
c('Y&gt;=1'=qlogis(mean(y &gt;= 1)),'Y&gt;=2'=qlogis(mean(y &gt;= 2)),'Y&gt;=3'=qlogis(mean(y &gt;= 3)))
s &lt;- summary(grade ~Ki67+Cyclin_E, fun=sf)
plot(s,which=1:3,pch=1:3,xlab='logit',main='',xlim=c(-2.5,2.5))
</code></pre>

<p>I have</p>

<pre><code>lrm(formula = grade ~ Ki67 + Cyclin_E)

Frequencies of Missing Values Due to Each Variable
   grade     Ki67 Cyclin_E 
       0        0        3 


                     Model Likelihood     Discrimination    Rank Discrim.    
                        Ratio Test            Indexes          Indexes       

Obs            42    LR chi2     11.38    R2       0.268    C       0.728    
 1             11    d.f.            2    g        1.279    Dxy     0.456    
 2             15    Pr(&gt; chi2) 0.0034    gr       3.592    gamma   0.458    
 3             16                         gp       0.192    tau-a   0.308    
max |deriv| 1e-07                         Brier    0.166                     


         Coef    S.E.   Wald Z Pr(&gt;|Z|)
y&gt;=2     -0.1895 0.8427 -0.22  0.8221  
y&gt;=3     -2.0690 0.9109 -2.27  0.0231  
Ki67      0.0971 0.0330  2.94  0.0033  
Cyclin_E -0.0076 0.0227 -0.33  0.7387 
</code></pre>

<p>The <code>s</code> table gives: (unfortunately I don't know how to upload a graph made in R)</p>

<pre><code>grade    N=45

+--------+-------+--+----+---------+----------+
|        |       |N |Y&gt;=1|Y&gt;=2     |Y&gt;=3      |
+--------+-------+--+----+---------+----------+
|Ki67    |[ 2, 9)|12|Inf |0.6931472|-1.0986123|
|        |[ 9,16)|12|Inf |0.3364722|-2.3978953|
|        |[16,24)|10|Inf |2.1972246| 0.0000000|
|        |[24,44]|11|Inf |2.3025851| 1.5040774|
+--------+-------+--+----+---------+----------+
|Cyclin_E|[ 3,16)|15|Inf |1.0116009|-0.1335314|
|        |[16,22)| 7|Inf |1.7917595|-0.9162907|
|        |[22,33)|10|Inf |1.3862944|-0.8472979|
|        |[33,80]|10|Inf |0.4054651|-0.4054651|
|        |Missing| 3|Inf |      Inf| 0.6931472|
+--------+-------+--+----+---------+----------+
|Overall |       |45|Inf |1.1284653|-0.4054651|
+--------+-------+--+----+---------+----------+
</code></pre>

<p>Where for the Ki67 I see that 3 out of the 4 differences  <code>logit(P[Y&gt; = 2])-logit(P[Y&gt; = 3])</code> are close to 2. Only the last one is quite lower (around 0.8). But here Ki67 is continuous and not categorical so I don't know if the results of the table are correct and there isn't any p-value to decide. By the way I run the above in SPSS and I didn't reject the assumption.</p>

<p>2) Using the VGAM package</p>

<p>Here using the next commands I have the model under the assumption of proportional odds</p>

<pre><code>library(VGAM)
fit1 &lt;- vglm(grade ~Ki67+Cyclin_E,family=cumulative(parallel=T))
summary(fit1)
</code></pre>

<p>And the results</p>

<pre><code>Coefficients:
                Estimate Std. Error  z value
(Intercept):1  0.1894723   0.820442  0.23094
(Intercept):2  2.0690395   0.886732  2.33333
Ki67          -0.0970972   0.032423 -2.99467
Cyclin_E       0.0075887   0.021521  0.35261

Number of linear predictors:  2 

Names of linear predictors: logit(P[Y&lt; = 1]), logit(P[Y&lt; = 2])

Dispersion Parameter for cumulative family:   1

Residual deviance: 79.86801 on 80 degrees of freedom

Log-likelihood: -39.93401 on 80 degrees of freedom

Number of iterations: 5 
</code></pre>

<p>While using the next commands I have the model without the assumption of proportional odds</p>

<pre><code>fit2 &lt;- vglm(grade ~Ki67+Cyclin_E,family=cumulative(parallel=F))
</code></pre>

<p>where unfortunately i receice the next message </p>

<blockquote>
  <p>Warning message: In vglm.fitter(x = x, y = y, w = w, offset = offset,
  Xm2 = Xm2,  :   convergence not obtained in 30 iterations</p>
</blockquote>

<p>However if I type <code>summary(fit2)</code> I get results but again I don't know if they are correct. My intention was to use the next commands and get the answer but know I doubt if this is correct (by the way if I do it I get <code>p-value=0.6</code>. </p>

<pre><code>pchisq(deviance(fit1)-deviance(fit2),
df=df.residual(fit1)-df.residual(fit2),lower.tail=FALSE)
</code></pre>

<p>So, regarding the methods mentioned above does anyone knows whether the results I get are valid or in the case of the VGAM package is there any way to increase the number of itterations?Is there any other way to check it? </p>
"
"0.0977258320053917","0.0853662733540307"," 26180","<p>I run an ordinal regression model and I wanted to check the proportional odds assumption. In order to do that I used the <code>VGAM</code> package and I run olr twice, the first under the assumption and the second without the assumption. Below is the code and the results</p>

<pre><code>&gt; fit1 &lt;- vglm(stage ~Ki67+Cyclin_E,family=cumulative(parallel=T))
&gt; summary(fit1)

Call:
vglm(formula = stage ~ Ki67 + Cyclin_E, family = cumulative(parallel = T))

Pearson Residuals:
                     Min       1Q  Median      3Q    Max
logit(P[Y&lt; = 1]) -3.1177 -0.43593 0.37246 0.53111 1.4927
logit(P[Y&lt; = 2]) -3.8479  0.14119 0.18785 0.28679 1.9903

Coefficients:
                Estimate Std. Error  z value
(Intercept):1  2.2414705   1.091225  2.05409
(Intercept):2  3.2164214   1.178916  2.72829
Ki67          -0.1157273   0.039889 -2.90124
Cyclin_E       0.0085266   0.028626  0.29786

Number of linear predictors:  2 

Names of linear predictors: logit(P[Y&lt; = 1]), logit(P[Y&lt; = 2])

Dispersion Parameter for cumulative family:   1

Residual deviance: 50.82946 on 62 degrees of freedom

Log-likelihood: -25.41473 on 62 degrees of freedom

Number of iterations: 5 


&gt; fit2 &lt;- vglm(grade ~Ki67+Cyclin_E,family=cumulative(parallel=F),maxit=50)
&gt; summary(fit2)

Call:
vglm(formula = grade ~ Ki67 + Cyclin_E, family = cumulative(parallel = F), 
    maxit = 50)

Pearson Residuals:
                     Min       1Q   Median      3Q    Max
logit(P[Y&lt; = 1]) -1.1870 -0.65271 -0.23199 0.44910 3.4798
logit(P[Y&lt; = 2]) -2.6235 -0.70599  0.27305 0.72691 2.8544

Coefficients:
               Estimate Std. Error   z value
(Intercept):1 -0.059702   0.928078 -0.064328
(Intercept):2  2.687277   1.050909  2.557097
Ki67:1        -0.100832   0.047754 -2.111483
Ki67:2        -0.101817   0.036567 -2.784377
Cyclin_E:1     0.018768   0.022708  0.826474
Cyclin_E:2    -0.015416   0.022927 -0.672390

Number of linear predictors:  2 

Names of linear predictors: logit(P[Y&lt; = 1]), logit(P[Y&lt; = 2])

Dispersion Parameter for cumulative family:   1

Residual deviance: 78.93318 on 78 degrees of freedom

Log-likelihood: -39.46659 on 78 degrees of freedom

Number of iterations: 34 
</code></pre>

<p>In order to check if the difference of the two models is significant I run the next command</p>

<pre><code>pchisq(deviance(fit2)-deviance(fit1),df=df.residual(fit2)-df.residual(fit1),lower.tail=FALSE)
[1] 0.03072927
</code></pre>

<p>As you see the result is that the 2 models differ and so the proportional odds assumption isn't true. But if you see the coefficients about the Ki67 (Cyclin is not significantly important so i guess i can skip it) they are almost the same. In that case what should I do? I believe that I could stick with the model under the po assumption but I'd like to know what others think</p>
"
"0.0430930413588572","0.0205325390588295"," 26234","<p>(this question addresses an expanded case of <a href=""http://stats.stackexchange.com/questions/21257/how-can-factor-levels-be-automatically-chosen-in-r-to-maximize-the-number-of-pos"">How can factor-levels be automatically chosen in R to maximize the number of positive coefficients in a regression model?</a>)</p>

<p>I am performing linear regression (using R) on data having both categorical (factor) and numeric variables, and fitting to a model having the form <code>y ~ (.)^2</code> (i.e. including all first order and second-order interaction terms).</p>

<p>The question is:  is there is a <strong>programmatic</strong> way of determining a coefficient vector among the set of optimal vectors $\Theta_{opt}$, where the length of the vector is minimized under the constraint that all elements of the vector are positive.</p>

<p>There may be cases where it is impossible to find an all-positive coefficient vector in $\Theta_{opt}$, but let's assume that the particular data which is being analyzed allows for such vectors to exist.</p>

<p>Perhaps one could start out with an initial (least-squares) optimal coefficient vector, and then manipulate this vector based on certain rules that depend on the nature of the terms which the coefficients are associated with.  I can deduce some general rules for doing these manipulations, but don't know how to algorithmically perform the manipulations in a manner such that I arrive at a minimal-length parameter vector (parameters==0 don't count towards the vector's length).  This may be heading the wrong direction though...?</p>
"
"0.0609427635336005","0.0580747904139041"," 26288","<p>I'm trying to understand how to interpret log odds ratios in logistic regression. Let's say I have the following output:</p>

<pre><code>&gt; mod1 = glm(factor(won) ~ bid, data=mydat, family=binomial(link=""logit""))
&gt; summary(mod1)

Call:
glm(formula = factor(won) ~ bid, family = binomial(link = ""logit""), 
    data = mydat)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.5464  -0.6990  -0.6392  -0.5321   2.0124  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -2.133e+00  1.947e-02 -109.53   &lt;2e-16 ***
bid          2.494e-03  5.058e-05   49.32   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 83081  on 80337  degrees of freedom
Residual deviance: 80645  on 80336  degrees of freedom
AIC: 80649

Number of Fisher Scoring iterations: 4
</code></pre>

<p>So my equation would look like:
$$\Pr(Y=1) = \frac{1}{1 + \exp\left(-[-2.13 + 0.002\times(\text{bid})]\right)}$$</p>

<p>From here I calculated probabilities from all bid levels. 
<img src=""http://i.stack.imgur.com/5mLa9.png"" alt=""enter image description here""></p>

<p>I have been using this graph to say that at a 1000 bid, the probability of winning is x. At any given bid level, the probability of winning is x.</p>

<p>I have a feeling that my interpretation is wrong because I'm not considering that these are log-odds. How should I really be interpreting this plot/these results?</p>
"
"0.0817632971986956","0.0779155074867256"," 26831","<p>Still on running logistic regression models and would like to ask a few questions around it.</p>

<p><strong>Question 1</strong>:
Is there a simple way of getting the p-values of each independent factor in a logistic regression model. For example, I am running this model:</p>

<pre><code>mymod3 &lt;- as.formula(surv~as.factor(tdate)+as.factor(sline)+as.factor(pgrp)
                                          +as.factor(weight5)+as.factor(backfat5)
                                          +as.factor(srect2)+as.factor(bcs)
                                          +as.factor(agit)+as.factor(uscore)
                                          +as.factor(loco)+as.factor(teat2)
                                          +as.factor(uscoref)+as.factor(colos)
                                          +as.factor(tb5)+as.factor(nerve)
                                          +as.factor(feed5)+as.factor(fos)
                                          +as.factor(gest3)+as.factor(int3)
                                          +as.factor(psex)+as.factor(bwt5)
                                          +as.factor(presp2)+as.factor(mtone2)
                                          +as.factor(pscolor)+as.factor(pmstain)
                                          +as.factor(pshiv)+as.factor(ppscore)
                                          +as.factor(pincis)+as.factor(prectem5)
                                          +as.factor(pcon12)+as.factor(crum5)
                                          +as.factor(pindx5))

sofNoMis3 &lt;- apf[which(complete.cases(apf[,all.vars(mymod3)])),]
FulMod3 &lt;- glm(mymod3,family=binomial(link=""logit""),data=sofNoMis3)
summary(FulMod3)
</code></pre>

<p>I am using this to look at the significant level of each factor:</p>

<pre><code>anova(FulMod3,test=""Chisq"")
</code></pre>

<p>and got this:</p>

<pre><code>Analysis of Deviance Table

Model: binomial, link: logit

Response: surv

Terms added sequentially (first to last)


                    Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                                 7791     7096.2              
as.factor(tdate)    15    50.71      7776     7045.4 9.215e-06 ***
as.factor(sline)     1    13.90      7775     7031.5 0.0001924 ***
as.factor(pgrp)      3     8.83      7772     7022.7 0.0316335 *  
as.factor(weight5)   4     7.18      7768     7015.5 0.1268943    
as.factor(backfat5)  4     3.86      7764     7011.7 0.4258074    
as.factor(srect2)    1     0.15      7763     7011.5 0.6987832    
as.factor(bcs)       3     6.46      7760     7005.1 0.0910745 .  
as.factor(agit)      2    13.44      7758     6991.6 0.0012075 ** 
as.factor(uscore)    2     2.16      7756     6989.5 0.3401845    
as.factor(loco)      2     1.58      7754     6987.9 0.4530983    
as.factor(teat2)     2    25.45      7752     6962.4 2.980e-06 ***
as.factor(uscoref)   2     0.48      7750     6962.0 0.7861675    
as.factor(colos)     1     1.06      7749     6960.9 0.3034592    
as.factor(tb5)       4    49.22      7745     6911.7 5.265e-10 ***
as.factor(nerve)     2     0.99      7743     6910.7 0.6105452    
as.factor(feed5)     4    11.79      7739     6898.9 0.0190170 *  
as.factor(fos)       1    47.10      7738     6851.8 6.732e-12 ***
as.factor(gest3)     2    22.60      7736     6829.2 1.235e-05 ***
as.factor(int3)      2     6.61      7734     6822.6 0.0367298 *  
as.factor(psex)      1     9.50      7733     6813.1 0.0020493 ** 
as.factor(bwt5)      4   348.42      7729     6464.7 &lt; 2.2e-16 ***
as.factor(presp2)    1   106.23      7728     6358.4 &lt; 2.2e-16 ***
as.factor(mtone2)    1    34.13      7727     6324.3 5.146e-09 ***
as.factor(pscolor)   1    12.57      7726     6311.7 0.0003928 ***
as.factor(pmstain)   1     0.30      7725     6311.4 0.5845095    
as.factor(pshiv)     1    32.29      7724     6279.2 1.328e-08 ***
as.factor(ppscore)   1    16.71      7723     6262.4 4.351e-05 ***
as.factor(pincis)    1     0.02      7722     6262.4 0.8892848    
as.factor(prectem5)  4   126.06      7718     6136.4 &lt; 2.2e-16 ***
as.factor(pcon12)    1    17.88      7717     6118.5 2.350e-05 ***
as.factor(crum5)     4    15.25      7713     6103.2 0.0042137 ** 
as.factor(pindx5)    4    25.58      7709     6077.6 3.838e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>but it does not always agree with the final model after applying backward elimination:</p>

<p>Example: </p>

<p>these three factors were not significant above but they still appeared in the final model below</p>

<pre><code>as.factor(weight5)   4     7.18      7768     7015.5 0.1268943    
as.factor(backfat5)  4     3.86      7764     7011.7 0.4258074    
as.factor(srect2)    1     0.15      7763     7011.5 0.6987832    

step(FulMod3,direction=""backward"",trace=FALSE)
</code></pre>

<p>which gives:</p>

<pre><code>Call:  glm(formula = surv ~ as.factor(tdate) + as.factor(pgrp) + as.factor(weight5) + 
    as.factor(backfat5) + as.factor(srect2) + as.factor(agit) + 
    as.factor(uscore) + as.factor(teat2) + as.factor(uscoref) + 
    as.factor(fos) + as.factor(gest3) + as.factor(int3) + as.factor(psex) + 
    as.factor(bwt5) + as.factor(presp2) + as.factor(mtone2) + 
    as.factor(pscolor) + as.factor(pshiv) + as.factor(ppscore) + 
    as.factor(prectem5) + as.factor(pcon12) + as.factor(pindx5), 
    family = binomial(link = ""logit""), data = sofNoMis3)

Coefficients:
               (Intercept)  as.factor(tdate)2009-09-11  as.factor(tdate)2009-09-15  as.factor(tdate)2009-09-18  as.factor(tdate)2009-09-22  
                   1.34799                     0.18414                    -0.19490                    -0.15552                    -0.16822  
as.factor(tdate)2009-09-25  as.factor(tdate)2009-09-29  as.factor(tdate)2010-01-26  as.factor(tdate)2010-01-29  as.factor(tdate)2010-02-02  
                   0.60046                     0.80784                    -1.03442                    -1.30562                    -1.01486  
as.factor(tdate)2010-02-05  as.factor(tdate)2010-02-09  as.factor(tdate)2010-02-12  as.factor(tdate)2010-02-16  as.factor(tdate)2010-02-19  
                  -1.04438                    -0.89311                    -1.06260                    -0.79833                    -1.09651  
as.factor(tdate)2010-02-23            as.factor(pgrp)2            as.factor(pgrp)3            as.factor(pgrp)4         as.factor(weight5)2  
                  -0.55411                     0.12659                    -0.04727                     0.21817                    -0.22592  
       as.factor(weight5)3         as.factor(weight5)4         as.factor(weight5)5        as.factor(backfat5)2        as.factor(backfat5)3  
                  -0.10143                    -0.31562                    -0.37656                    -0.19883                    -0.01188  
      as.factor(backfat5)4        as.factor(backfat5)5          as.factor(srect2)2            as.factor(agit)2            as.factor(agit)3  
                   0.08293                    -0.17116                    -0.18201                    -0.49145                    -0.36659  
        as.factor(uscore)2          as.factor(uscore)3           as.factor(teat2)2           as.factor(teat2)3         as.factor(uscoref)2  
                  -0.12265                     0.15334                     0.16575                     0.21520                     0.24166  
       as.factor(uscoref)3             as.factor(fos)2           as.factor(gest3)2           as.factor(gest3)3            as.factor(int3)2  
                  -0.24363                    -0.29506                     0.09747                     0.81894                    -0.25595  
          as.factor(int3)3            as.factor(psex)2            as.factor(bwt5)2            as.factor(bwt5)3            as.factor(bwt5)4  
                  -1.21086                     0.20025                     0.30753                     0.29614                     0.56753  
          as.factor(bwt5)5          as.factor(presp2)2          as.factor(mtone2)2         as.factor(pscolor)2           as.factor(pshiv)2  
                   0.86479                    -0.29270                    -0.40912                    -0.72782                    -0.33848  
       as.factor(ppscore)2        as.factor(prectem5)2        as.factor(prectem5)3        as.factor(prectem5)4        as.factor(prectem5)5  
                  -0.25958                     0.73842                     0.77476                     0.92158                     0.96269  
        as.factor(pcon12)2          as.factor(pindx5)2          as.factor(pindx5)3          as.factor(pindx5)4          as.factor(pindx5)5  
                   0.38119                     0.43199                     0.44496                     0.73458                     0.59771  

Degrees of Freedom: 7791 Total (i.e. Null);  7732 Residual
Null Deviance:      7096 
Residual Deviance: 6102         AIC: 6222
</code></pre>

<p><strong>Question 2</strong>:</p>

<p>I would like to calculate the standard errors of the odds ratio of each factor level </p>

<pre><code>exp(NewMod3$coefficients)  #Odds ratios
</code></pre>

<p><strong>Question 3:</strong></p>

<p>Lastly, to tell whether the levels of each factor are significantly different or not </p>

<pre><code>               (Intercept) as.factor(tdate)2009-09-11 as.factor(tdate)2009-09-15 as.factor(tdate)2009-09-18 as.factor(tdate)2009-09-22 
                 3.8496863                  1.2021883                  0.8229141                  0.8559688                  0.8451676 
as.factor(tdate)2009-09-25 as.factor(tdate)2009-09-29 as.factor(tdate)2010-01-26 as.factor(tdate)2010-01-29 as.factor(tdate)2010-02-02 
                 1.8229563                  2.2430525                  0.3554327                  0.2710041                  0.3624544 
as.factor(tdate)2010-02-05 as.factor(tdate)2010-02-09 as.factor(tdate)2010-02-12 as.factor(tdate)2010-02-16 as.factor(tdate)2010-02-19 
                 0.3519109                  0.4093819                  0.3455567                  0.4500787                  0.3340336 
as.factor(tdate)2010-02-23           as.factor(pgrp)2           as.factor(pgrp)3           as.factor(pgrp)4        as.factor(weight5)2 
                 0.5745817                  1.1349500                  0.9538339                  1.2437928                  0.7977835 
       as.factor(weight5)3        as.factor(weight5)4        as.factor(weight5)5       as.factor(backfat5)2       as.factor(backfat5)3 
                 0.9035410                  0.7293337                  0.6862173                  0.8196866                  0.9881871 
      as.factor(backfat5)4       as.factor(backfat5)5         as.factor(srect2)2           as.factor(agit)2           as.factor(agit)3 
                 1.0864697                  0.8426844                  0.8335940                  0.6117399                  0.6930936 
        as.factor(uscore)2         as.factor(uscore)3          as.factor(teat2)2          as.factor(teat2)3        as.factor(uscoref)2 
                 0.8845715                  1.1657233                  1.1802836                  1.2401126                  1.2733576 
       as.factor(uscoref)3            as.factor(fos)2          as.factor(gest3)2          as.factor(gest3)3           as.factor(int3)2 
                 0.7837753                  0.7444886                  1.1023798                  2.2681046                  0.7741829 
          as.factor(int3)3           as.factor(psex)2           as.factor(bwt5)2           as.factor(bwt5)3           as.factor(bwt5)4 
                 0.2979401                  1.2217088                  1.3600609                  1.3446543                  1.7639063 
          as.factor(bwt5)5         as.factor(presp2)2         as.factor(mtone2)2        as.factor(pscolor)2          as.factor(pshiv)2 
                 2.3745019                  0.7462454                  0.6642372                  0.4829602                  0.7128545 
       as.factor(ppscore)2       as.factor(prectem5)2       as.factor(prectem5)3       as.factor(prectem5)4       as.factor(prectem5)5 
                 0.7713779                  2.0926314                  2.1700692                  2.5132469                  2.6187261 
        as.factor(pcon12)2         as.factor(pindx5)2         as.factor(pindx5)3         as.factor(pindx5)4         as.factor(pindx5)5 
                 1.4640265                  1.5403203                  1.5604231                  2.0845978                  1.8179532 
</code></pre>

<p>Example:</p>

<p>I would like to have a table like this:</p>

<pre><code>Factor           levels  Odds ratio

Parity group      (1)    1.00Â±standard error   a
                   2     1.50Â±standard errror  b
                  3-4    1.17Â±standard error   c
                   &gt;5    1.19Â±standard error   c
</code></pre>

<p>I would really appreciate your help on these 3 areas.</p>

<p>Baz</p>
"
"0.0430930413588572","0.0410650781176591"," 27443","<p>I was wondering how you would generate data from a Poisson regression equation in R? I'm kind of confused how to approach the problem. </p>

<p>So if I assume we have two predictors $X_1$ and $X_2$ which are distributed $N(0,1)$. And the intercept is 0 and both of the coefficients equal 1. Then my estimate is simply:</p>

<p>$$\log(Y) = 0+ 1\cdot X_1 + 1\cdot X_2$$</p>

<p>But once I have calculated log(Y) - how do I generate poisson counts based on that? What is the rate parameter for the Poisson distribution? </p>

<p>If anyone could write a brief R script that generates Poisson regression samples that would be awesome!</p>

<p>Thanks!</p>
"
"0.0430930413588572","0.0410650781176591"," 28472","<p>A Regression with ARIMA errors is given by the following formula (saw on Hyndman et al, 1998):</p>

<p>$Y_t = b_0 + b_1 X_{1,t} + \dots + b_k X_{k,t} + N_t$</p>

<p>where $N_t$ is modeled as an ARIMA process.</p>

<p>If we have that the model for $N_t$ is ARIMA$(0,0,0)$, then $N_t = e_t$, and $Y_t$ is modeled by an ordinary regression.</p>

<p>Suppose the following data:</p>

<pre><code>a &lt;- structure(c(29305, 9900, 9802, 17743, 49300, 17700, 24100, 11000, 
10625, 23644, 38011, 16404, 14900, 16300, 18700, 11814, 13934, 
12124, 18097, 30026, 3600, 15700, 12300, 14600), .Tsp = c(2010.25, 
2012.16666666667, 12), class = ""ts"")
b &lt;- structure(c(1.108528016, 1.136920872, 1.100239002, 1.057191265, 
1.044200511, 1.102063834, 1.083847756, 1.068585841, 1.084879628, 
1.232979511, 1.168894672, 1.257302058, 1.264967051, 1.234793782, 
1.306452369, 1.252644047, 1.178593218, 1.124432965, 1.132878661, 
1.189926986, 1.17249669, 1.176285957, 1.176552, 1.179178082), .Tsp = 
c(2010.25, 2012.16666666667, 12), class = ""ts"")
</code></pre>

<p>If I model it using <code>auto.arima</code> function, I have:</p>

<pre><code>auto.arima(a, xreg=b)
Series: a 
ARIMA(0,0,0) with zero mean     

Coefficients:
              b
      15639.266
s.e.   1773.186

sigma^2 estimated as 101878176:  log likelihood=-255.33
AIC=514.65   AICc=515.22   BIC=517.01

lm(a~b)

Call:
lm(formula = a ~ b)

Coefficients:
(Intercept)            b  
      48638       -26143  
</code></pre>

<p>Coefficients from the models differ. Shouldn't they be the same? What am I missing?</p>
"
"0.129279124076572","0.116351054666701"," 28688","<p>I ran <code>lm()</code> on my data with models selected by individual <code>lm</code>'s of each characteristic and then combined the top $R^2$ based on $p$-value. For instance, the first few characteristics are taken, then the rest are evaluated if they have $p&lt;.005$. My characteristics contain some duplication: for instance, I have a characteristic and its normalized variant in test P. My $p$-values are all very small but my diagrams do not look correct for R and T. (Referring to this blog post: <a href=""http://www.findnwrite.com/musings/evaluating-linear-regression-model-in-r/"" rel=""nofollow"">Evaluating Linear Regression Model in R</a>.)</p>

<p>In test P (and T) there is one outlier according to Cooks Distance. How do I find and eliminate that instance?</p>

<p>According to this tutorial on <a href=""http://www.montefiore.ulg.ac.be/~kvansteen/GBIO0009-1/ac20092010/Class8/Using%20R%20for%20linear%20regression.pdf"" rel=""nofollow"">Using R for Linear Regression</a>,</p>

<blockquote>
  <p>The plot in the upper left shows the residual errors plotted versus
  their fitted values.  The residuals should be randomly distributed
  around the horizontal line representing a residual error of zero; that
  is, there should not be a distinct trend in the distribution of
  points.</p>
</blockquote>

<p>Test P looks ok in the residual error but test R and T have a grouping what does that mean and how do I account for it?</p>

<blockquote>
  <p>The plot in the lower left is a standard Q-Q plot, which should
  suggest that the  residual errors are normally distributed.  The
  scale-location plot in the upper right shows the square root of the
  standardized residuals (sort of a square root of relative error) as a
  function of the fitted values.  Again, there should be no obvious
  trend in this plot.</p>
</blockquote>

<p>Again Test P looks ok in the standard Q-Q plot but test R and T have a grouping what does that mean and how do I account for it?</p>

<p>Also what is the coefficients on the output. I notice it lists the characteristics and a p value but i don't understand what it means.</p>

<p>And finally how do I make predictions using the model I created? </p>

<p><strong>Test P</strong>
F-statistic: 2.684 on 280 and 2221 DF,  p-value: &lt; 2.2e-16 
<img src=""http://i.stack.imgur.com/10A8r.png"" alt=""enter image description here""></p>

<p><strong>Test R</strong>
F-statistic: 3.691 on 258 and 2243 DF,  p-value: &lt; 2.2e-16 
<img src=""http://i.stack.imgur.com/jy6IR.png"" alt=""enter image description here""></p>

<p><strong>Test T</strong>
F-statistic: 4.029 on 268 and 2233 DF,  p-value: &lt; 2.2e-16 
<img src=""http://i.stack.imgur.com/bs69P.png"" alt=""enter image description here""></p>

<p>edit after running gls my p looks like this</p>

<p><img src=""http://i.stack.imgur.com/cUBGB.png"" alt=""""></p>
"
"0.068136080998913","0.0519436716578171"," 28957","<p>I am interested in understanding the graph plots we get after running <code>lm()</code> command (for linear regression) in R like, for example</p>

<pre><code>lm.mod1 = lm(y ~ x1 + x2)
</code></pre>

<p>I then get the do the summary by:</p>

<pre><code>summary(lm.mod1)
</code></pre>

<p>I get the result as: </p>

<pre><code> Residuals:
  Min      1Q  Median      3Q     Max
 -750.32 -160.54  -49.83  115.83 2923.74

 Coefficients:
                           Estimate Std. Error     t value Pr(&gt;|t|)    
(Intercept)               -345.1552      37.0393   -9.319   &lt;2e-16 ***
         x1                52.9091       2.4929    21.224   &lt;2e-16 ***
         x2                8.9669        0.5395    16.620   &lt;2e-16 ***

Residual standard error: 274.4 on 1985 degrees of freedom
Multiple R-squared: 0.2059, Adjusted R-squared: 0.2051 
F-statistic: 257.3 on 2 and 1985 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>I then do the plotting by </p>

<pre><code>par(mfrow = c(2,2))
plot(lm.mod1)
</code></pre>

<p>I get 4 graphs (I can't post the graphs since I am a new user and my experience level is below 10. :/)</p>

<p>My questions are : </p>

<ol>
<li><p>How do they calculate F-statistics and t-value?</p></li>
<li><p>Could someone explain me the what do we interpret with the last two graphs i.e. $\text{Scale-Location vs. (Standardized residuals)}^{1/2}$ and $\text{Residuals vs. Leverage}$. What do you mean by Leverage?</p></li>
<li><p>What do you mean by Cook's Distance? I saw it on wikipedia but I didnt get it. </p></li>
<li><p>How could we suggest if our model is a good model or not?</p></li>
</ol>
"
"0.158435019173425","0.156371158759334"," 29336","<p><strong>Introduction</strong> </p>

<p>I am aiming to forecast the annual growth rates for a number of macroeconomic indicators (denote one by $Y_t$).
One of the tasks is to test the forecasting performance of rival time series models with and without exogenous variables ($X_t$, a $T\times k$ matrix). The list of rival models include:</p>

<ol>
<li>AR(I)MA model (annual growth rates are unlikely to have ""unit Roo"", though the latter is either assumed or tested) $$A(L)Y_t =\mu+ B(L)\varepsilon_t$$</li>
<li>linear regression model with ARMA errors $$Y_t = X_t\beta + \eta_t, \ \ A(L)\eta_t =  B(L)\varepsilon_t $$</li>
<li>lagged dependent variable model (autoregressive model with exogenous variables)
$$A(L)Y_t = X_t\beta + \varepsilon_t$$ </li>
<li>linear regression model
$$Y_t = X_t\beta + \varepsilon_t$$</li>
</ol>

<p>Where $\varepsilon_t$ is assumed to be a strong white noise, zero-mean constant variance i.i.d. process; $A(L)$ and $B(L)$ are autoregressive (of order $p$) and moving average (of order $q$) polynomials with $L$ - a back-shift (lag) operator.</p>

<p>Note that the primary and the only goal is forecasting performance, thus any ""good"" properties of parameter estimates are of the secondary concern. All I need is to test for the most parsimonious, robust to starting conditions forecaster. Decision will be made with one of the <code>accuracy()</code> options, but first I need to obtain the material for the comparison.</p>

<p>Models 1. and 2. are estimated by <code>auto.arima()</code> with default <code>""CSS-ML""</code> estimation method. Models 3. and 4. are estimated by ordinary least squares (<code>lm()</code>). $T$ is about $40$ quarters.</p>

<p><strong>Approaches tried so far</strong></p>

<p>To make the jack-knifed residuals the first approach denoted by ""rolling"" has been implemented. Starting from feasibly large sub-sample of time series data, parameters are estimated and an $h$ ahead forecast is done by the <code>predict()</code> function (EDIT: it is the same suggestion as in the first part Rob's answer to the second question). After that one point is added and the estimation\prediction steps are repeated. </p>

<p>A weak point of such experiments is that the number of time ticks (sample size) used to estimate the parameters is different. While I would like to test the robustness to the starting conditions, keeping the sample size for estimation fixed.</p>

<p>Bearing this in mind I tried to set the several subsequent values (EDIT: for the interval $k+p+q&lt;t_0&lt;t_1&lt;T-h+1$) in $Y_t$ being missing values (NA). In models 2.-4. this also implies dropping the corresponding subsequent rows in data matrix $X_t$. Prediction for 3. and 4. is straightforward (the same <code>predict()</code> with omitted $X_t$ data rows works well). All my concerns are about models 1. and 2.</p>

<p>With just the AR($p$) part the predictions are done sequentially $Y_{t+1|t} = \hat A(L)Y_t$. But with the presence of MA($q$) one could not (?) use the estimated parameters directly. From the Brockwell and Davis ""Introduction to Time Series and Forecasting"" Chapter 3.3 it follows that one needs an innovation algorithm to recursively estimate the $\theta_{n,j}$ from the specific system of equations that involves estimated autoregressive and moving average parameters. EDIT: these $\theta_{n,j}$ parameters are used to make the ARMA prediction, not the originally estimated parameters $\theta_{j}$. However it is remarked in the same chapter that $\theta_{n,j}$ asymptotically approaches $\theta_{j}$ if the process is invertible. It is not evident that 30-40 points is enough for the asymptotic result to be used even if is invertible. </p>

<p>Notes: I don't want to restrict $q$ to zero, since I am not doing so in true out-of-sample forecasting. EDIT: also not that it is not missing value imputation problem, but forecasting experiment, that the trajectory is not supposed to bridge two sub-samples by this imputing the missing values.</p>

<p><strong>Questions</strong></p>

<ol>
<li>Does <code>auto.arima()</code> performs correctly with the presence of missing values inside of the sample? [Already answered by Rob.]</li>
<li>(The actually crucial part of this post) How to correctly forecast (NOT impute) these missed points from the ARMA model when both $p&gt;0$ and $q&gt;0$? (I hope there are the ways already implemented in R language, but I simply is missing something.)</li>
</ol>

<p>EDIT: since the parameters for ARMA parts are estimated correctly could I legally rearrange the arima object to include the estimated parameters and the data only for the first subsample and then use a predict function? </p>

<p>EDIT2: I have tried to modify the estimated <code>mod</code> structure - the resulting forecast from <code>predict.Arima</code> is identical (double precision differences) to the forecast where I use the estimated MA and AR coefficients predicting $Y_{t+1|t}$ directly as $\hat A(L)(Y_t-X_t\hat \beta)+ X_t\hat \beta+\hat B(L)\hat \varepsilon_t $, without <code>KalmanForecast()</code>. This was expected since state space representation is supplied with the same estimated $\theta_j$, not $\theta_{n,j}$. So the only question remains is the difference between $\theta_j$ and $\theta_{n,j}$ significant to influence the point forecasts? I hope the answer is negative.</p>
"
"0.075412822378","0.0718638867059034"," 29390","<p>I'm running a generalized linear model (quasi-poisson regression) as a cron job in R that trains on data from an SQL query. The SQL query pulls data from the last 30 days. Depending on the sample of data from the last 30 days, the regression coefficients of course change. As a result, I'm then writing the regression coefficients generated in R back to an SQL table, with year/month/day as the the primary key. Thus, I can see the daily change in regression coefficients based on the most recent 30 days of data. </p>

<p>My question is this: how do I best interpret the change in regression coefficients over time? </p>

<p>If I can see a broad overview of techniques, I can hopefully find the one most appropriate given my data. </p>

<p>My objective is to reduce the RMSE of a predictive model. The response variable is the number of events in the 30 day interval from the current time. I have multiple predictors, including interaction terms. </p>

<p>Edit: it is not a requirement to only query the last 30 days of data. I simply wish to weight data closer in time more heavily. A discrete-time (days, in this case) weighted regression on all data would probably be ideal.</p>
"
"0.0430930413588572","0.0410650781176591"," 29449","<p>Could you please shed some lights about how to interpret linear regresssion results (2-stage vs. 1 stage)?</p>

<p>For example, I have the following:</p>

<pre><code>lmStage1 &lt;- lm(y~x1)
lmStage2 &lt;- lm(residuals(lmStage1)~x2)
summary(lmStage2)
</code></pre>

<p>vs.</p>

<pre><code>lmAll &lt;- lm(y~x1+x2)
summary(lmAll)
</code></pre>

<hr>

<p>How do I interpret and compare the coefficients/t-stats, etc. of the above two models?</p>

<p>And how do I compare the two approaches and what observations/diagnosis/studies shall I draw from the above two models?</p>

<p>In general, I feel that I am quite weak in drawing observations and obtaining intuitions from regression studies... are there books focusing on these interpretations and intuitions?</p>

<p>Thanks a lot!</p>
"
"0.0914141453004008","0.0871121856208561"," 29981","<p>Let's have some linear model, for example just simple ANOVA:</p>

<pre><code># data generation
set.seed(1.234)                      
Ng &lt;- c(41, 37, 42)                    
data &lt;- rnorm(sum(Ng), mean = rep(c(-1, 0, 1), Ng), sd = 1)      
fact &lt;- as.factor(rep(LETTERS[1:3], Ng)) 

m1 = lm(data ~ 0 + fact)
summary(m1)
</code></pre>

<p>Result is as follows:</p>

<pre><code>Call:
lm(formula = data ~ 0 + fact)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.30047 -0.60414 -0.04078  0.54316  2.25323 

Coefficients:
      Estimate Std. Error t value Pr(&gt;|t|)    
factA  -0.9142     0.1388  -6.588 1.34e-09 ***
factB   0.1484     0.1461   1.016    0.312    
factC   1.0990     0.1371   8.015 9.25e-13 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.8886 on 117 degrees of freedom
Multiple R-squared: 0.4816,     Adjusted R-squared: 0.4683 
F-statistic: 36.23 on 3 and 117 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>Now I try two different methods to estimate confidence interval of these parameters</p>

<pre><code>c = coef(summary(m1))

# 1st method: CI limits from SE, assuming normal distribution
cbind(low = c[,1] - qnorm(p = 0.975) * c[,2], 
    high = c[,1] + qnorm(p = 0.975) * c[,2])

# 2nd method
confint(m1)
</code></pre>

<h2>Questions:</h2>

<ol>
<li>What is the distribution of estimated linear regression coefficients? Normal or $t$?</li>
<li>Why do both methods yield different results? Assuming normal distribution and correct SE, I'd expect both methods to have the same result.</li>
</ol>

<p>Thank you very much!</p>

<p>data ~ 0 + fact</p>

<p><strong>EDIT after an answer</strong>:</p>

<p>The answer is exact, this will give exactly the same result as <code>confint(m1)</code>!</p>

<pre><code># 3rd method
cbind(low = c[,1] - qt(p = 0.975, df = sum(Ng) - 3) * c[,2], 
    high = c[,1] + qt(p = 0.975, df = sum(Ng) - 3) * c[,2])
</code></pre>
"
"0.105555962793852","0.100588487635796"," 30035","<p>For a linear regression with multiple groups (natural groups defined a priori) is it acceptable to run two different models on the same data set to answer the following two questions?</p>

<ol>
<li><p>Does each group have a non-zero slope and non-zero intercept and what are the parameters for each within group regression?</p></li>
<li><p>Is there, regardless of group membership, a non-zero trend and non-zero intercept and what are the parameters for this across groups regression?</p></li>
</ol>

<p>In R, the first model would be <code>lm(y ~ group + x:group - 1)</code>, so that the estimated coefficients could be directly interpreted as the intercept and slope for each group.The second model would be <code>lm(y ~ x + 1)</code>. </p>

<p>The alternative would be <code>lm(y ~ x + group + x:group + 1)</code>, which results in a complicated summary table of coefficients, with within group slopes and intercepts having to be calculated from the differences in slopes and intercepts from some reference. Also you have to reorder the groups and run the model a second time anyway in order to get a p-value for the last group difference (sometimes). </p>

<p>Does this using two separate models negatively affect inference in any way or this standard practice?</p>

<p>To put this into context, consider x to be a drug dosage and the groups to be different races. It may be interesting to know the dose-response relationship for a particular race for a doctor, or which races the drug works for at all, but it may also be interesting sometimes to know the dose-response relationship for the entire (human) population regardless of race for a public health official. This is just an example of how one might be interested in both within group and across group regressions separately. Whether a dose-response relationship should be linear isn't important.</p>
"
"0.0929636479491388","0.104695817324578"," 30061","<p>What approaches exist to observe the time lag between two variables?</p>

<p>I need to analyze the relationship between blood pressure and some other factor, such as exercise. The data set I am drawing from has around 1800 individuals, with an average of 100 entries a piece. It is generally known that there is a strong relationship between exercise level and blood pressure. However, if a person increases their steps to 8000+ a day, how long will it take for their blood pressure to drop? I am new to this type of analysis, and this is a challenge I have been thinking about for weeks. </p>

<p>I don't know if anyone wants to comment on possible approaches to this challenge or any issues surrounding it.</p>

<p>Some issues I have been dealing with:</p>

<ol>
<li><p>Is it better to treat this as a times series analysis or longitudinal data analysis?</p>

<p>My understanding is that time series usually focuses on one variable with no missing data and is observed at consistent intervals, where as longitudinal is over a longer period and has inconsistent time intervals, dropouts, and missing data.</p>

<p>The data I have seems to fit the longitudinal description more, but it also seems like time series could be used if I averaged the values by week so there would be no missing entries. I'm not sure about the pros and cons of each approach.</p></li>
<li><p>Should I be fitting a causal model, or would some other method like regression be more helpful?</p>

<p>I've been looking at various possible causal models, for example Marginal Structural Models (MSM) or Structural Nested Models (SNM), but there seem to be very little information on their application. I did find one R package that applied inverse probability weights and then used Cox proportional hazards regression model on a survival object (MSM), but that seemed to be focus on weighting for confounding and right censoring. Its result was a correlation coefficient, which I don't think helps me.</p>

<p>So I'm not sure if fitting a causal model is what I want, because that seems to be more focused on the making intellectually satisfying assumptions about relationships within the data and then determining the degree of causality, rather than providing information about time lag.</p>

<p>If anyone knows about MSM, SNM, their use in R, or how they might relate to this problem, that would be awesome to hear.</p></li>
<li><p>What about survival analysis or SEM?</p>

<p>I haven't explored these options very in-depth yet but they sound potentially relevant.</p></li>
</ol>

<p>I've kind of stalled, so any hints about what direction I might want to go would be really appreciated. </p>

<p>Thanks in advance.</p>
"
"0.0304713817668003","0.029037395206952"," 30377","<p>I have the following model that I am running in JAGS from R:</p>

<pre><code> model {
        for( i in 1:nData){
          y[i] ~ dnorm(mu[i], tau)
          mu[i] &lt;- b0 + inprod(b[],x[i,])              
          }

        tau ~ dgamma(.01,.01)
        b0 ~ dnorm(0,.0001)
        for (j in 1:nPredictors){
          b[j] ~ dgamma(2,2)
          }
        }
</code></pre>

<p>It is a simple regression model with a gamma prior on the beta coefficients. I can run the model and get some reasonable results, but my boss wants to know whether the Bayesian flavor is better than a normal OLS regression, and to this end he wants me to calculate an R^2 statistic for both models. I am fairly new to Bayesian statistics, so if anyone could share some R code to illustrate how it is done, I would be very grateful!</p>
"
"NaN","NaN"," 30822","<p>I am familiar with the debate surrounding Bates's decision to exclude p-values for mixed effects regression coefficients in lmer. However, I operate in a very p-value-focused discipline and am trying to clear something up. </p>

<p>What are the degrees of freedom for group-level predictors in a two-level model? I can use pvals.fnc() to obtain p-values for these coefficients but something tells me it is way overestimating the degrees of freedom for the tests, using the individual-level sample size and not the group-level sample size. Any help would be greatly appreciated. </p>
"
"0.122644945798043","0.0909014254011799"," 31592","<p>I am a bit puzzled about the behavior of uncorrelated predictors in logistic regression. 
As in OLS, I thought that if two predictors (<code>rv1</code> and <code>rv2</code>) are uncorrelated, then the regression weights of <code>rv1</code> will not change from a regression that only includes <code>rv1</code> to one that includes <code>rv1</code> and <code>rv2</code>. 
However, it seems to be the case that this is not true in logistic regression and coefficients change between the two regression models, even if the predictors are uncorrelated.</p>

<p>I have pasted some R syntax below that demonstrates this behavior.</p>

<p>Why is this the case and how do the regression weights from the two regressions (the one with only rv1 and the other one with rv1 and rv2) relate to each other? Is there a way to know what the regression weight of rv1 will be if one knows the regression weight of rv1 in the regression that includes both predictors?</p>

<p>Thanks!
P.S. This post is crossposted at another unrelated stat answer site.</p>

<pre><code>library(MASS)

#generate lots of data (a little bit weird data handling, I know)
n &lt;- 10000
rdta &lt;- as.data.frame(mvrnorm(n=n,c(0,0),matrix(c(1,0,0,1),2,2),empirical=TRUE))
names(rdta) &lt;- c(""rv1"",""rv2"")

#confirm that preds are uncorrelated
cov(rdta$rv1,rdta$rv2)

rv1 &lt;- rdta$rv1
    rv2 &lt;- rdta$rv2

rv1ry &lt;- 1
rv2ry &lt;- 1

#generate binary data from known regression coefficients
ylinp &lt;- (1 / (1+exp(-(-1 + rv1*rv1ry + rv2*rv2ry))))
y &lt;- rbinom(n,1,ylinp) 
glm(y~rv1+rv2,family=binomial(link='logit'))
glm(y~rv1,family=binomial(link='logit'))
glm(y~rv2,family=binomial(link='logit'))

#confirm that OLS regression works as expected (regression weights do not change)
rv1y &lt;- .222
rv2y &lt;- .333
y &lt;- rv1y * rv1 + rv2y * rv2 + rnorm(n,0,.5)
lm(y~rv1+rv2)
lm(y~rv1)
lm(y~rv2)   
</code></pre>

<p>I am not sure if it is expected to also paste relevant output here, but here goes:
OLS results</p>

<blockquote>
  <p>lm(y~rv1+rv2)</p>
</blockquote>

<pre><code>Call:
lm(formula = y ~ rv1 + rv2)

Coefficients:
(Intercept)          rv1          rv2  
 0.001096     0.220051     0.333072  
</code></pre>

<blockquote>
  <p>lm(y~rv1)</p>
</blockquote>

<pre><code>Call:
lm(formula = y ~ rv1)

Coefficients:
(Intercept)          rv1  
 0.001096     0.220051  
</code></pre>

<blockquote>
  <p>lm(y~rv2)  </p>
</blockquote>

<pre><code>Call:
lm(formula = y ~ rv2)

Coefficients:
(Intercept)          rv2  
 0.001096     0.333072  
</code></pre>

<p>Logistic regression results</p>

<blockquote>
  <p>glm(ry~rv1+rv2,family=binomial(link='logit'))</p>
</blockquote>

<pre><code>Call:  glm(formula = ry ~ rv1 + rv2, family = binomial(link = ""logit""))

Coefficients:
(Intercept)          rv1          rv2  
     -1.001        1.916        2.469  
</code></pre>

<blockquote>
  <p>glm(ry~rv1,family=binomial(link='logit'))</p>
</blockquote>

<pre><code>Call:  glm(formula = ry ~ rv1, family = binomial(link = ""logit""))

Coefficients:
(Intercept)          rv1  
    -0.5495       1.0535  
</code></pre>

<blockquote>
  <p>glm(ry~rv2,family=binomial(link='logit'))</p>
</blockquote>

<pre><code>Call:  glm(formula = ry ~ rv2, family = binomial(link = ""logit""))

Coefficients:
(Intercept)          rv2  
-0.6538       1.6140  
</code></pre>
"
"0.052777981396926","0.0502942438178979"," 31735","<p>Related my earlier question <a href=""http://stats.stackexchange.com/questions/31597/graphing-a-probability-curve-for-a-logit-model-with-multiple-predictors"">here</a>. Having ""mastered"" linear regression, I'm trying to learn everything I can about logistic regression and am having issues turning largely ""useless"" coefficients into meaningful information.</p>

<p>I asked in my previous question about graphing the probability curve for every permutation of a logit model. However, I was working on just plotting the main curve and was having some issues. </p>

<p>If I was running a logit with just one predictor, I'd run the following:</p>

<pre><code>mod1 = glm(factor(won) ~ as.numeric(bid), data=mydat, family=binomial(link=""logit""))

all.x &lt;- expand.grid(won=unique(won), bid=unique(bid))
y.hat.new &lt;- predict(mod1, newdata=all.x, type=""response"")
plot(bid&lt;-000:250,predict(mod1,newdata=data.frame(bid&lt;-c(000:250)),type=""response""), lwd=5, col=""blue"", type=""l"")
</code></pre>

<p>However, what about with multiple predictors? I tried to use the mtcars data set to mess around and couldn't get it.</p>

<p>Any suggestion on how to plot the main probability curve for the logit model.</p>

<pre><code>head(mtcars)

m1 = glm(vs ~ disp + wt, data=mtcars, family=binomial(link=""logit""))
summary(m1)

all.x &lt;- expand.grid(vs=unique(mtcars$vs), disp=unique(mtcars$disp), wt=unique(mtcars$wt))

y.hat.new &lt;- predict(m1, newdata=all.x, type=""response"")
plot(disp&lt;-000:250,predict(m1,newdata=data.frame(disp&lt;-c(000:250), wt&lt;-c(0,250)),type=""response""), lwd=5, col=""blue"", type=""l"")
</code></pre>

<p>EDIT = OR is my previous question what I need to do. Since Y = B0 + B1X1 + B2X2, a one unit change in X1 is associated with a exp(B1) change in Y, regardless of the value of B2. Then would it be possible that my original question is really all that should be done. </p>

<p>My appologies on my stupidity if that is indeed the case.</p>
"
"0.146262632480641","0.133572017951979"," 32040","<p>I'm trying to analyze effect of Year on variable logInd for particular group of individuals (I have 3 groups). <strong>The simplest model:</strong></p>

<pre><code>&gt; fix1 = lm(logInd ~ 0 + Group + Year:Group, data = mydata)
&gt; summary(fix1)

Call:
lm(formula = logInd ~ 0 + Group + Year:Group, data = mydata)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.5835 -0.3543 -0.0024  0.3944  4.7294 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
Group1       4.6395740  0.0466217  99.515  &lt; 2e-16 ***
Group2       4.8094268  0.0534118  90.044  &lt; 2e-16 ***
Group3       4.5607287  0.0561066  81.287  &lt; 2e-16 ***
Group1:Year -0.0084165  0.0027144  -3.101  0.00195 ** 
Group2:Year  0.0032369  0.0031098   1.041  0.29802    
Group3:Year  0.0006081  0.0032666   0.186  0.85235    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.7926 on 2981 degrees of freedom
Multiple R-squared: 0.9717,     Adjusted R-squared: 0.9716 
F-statistic: 1.705e+04 on 6 and 2981 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>We can see the Group1 is significantly declining, the Groups2 and 3 increasing but not significantly so.</p>

<p><strong>Clearly the individual should be random effect, so I introduce random intercept effect for each individual:</strong></p>

<pre><code>&gt; mix1a = lmer(logInd ~ 0 + Group + Year:Group + (1|Individual), data = mydata)
&gt; summary(mix1a)
Linear mixed model fit by REML 
Formula: logInd ~ 0 + Group + Year:Group + (1 | Individual) 
   Data: mydata 
  AIC  BIC logLik deviance REMLdev
 4727 4775  -2356     4671    4711
Random effects:
 Groups     Name        Variance Std.Dev.
 Individual (Intercept) 0.39357  0.62735 
 Residual               0.24532  0.49530 
Number of obs: 2987, groups: Individual, 103

Fixed effects:
              Estimate Std. Error t value
Group1       4.6395740  0.1010868   45.90
Group2       4.8094268  0.1158095   41.53
Group3       4.5607287  0.1216522   37.49
Group1:Year -0.0084165  0.0016963   -4.96
Group2:Year  0.0032369  0.0019433    1.67
Group3:Year  0.0006081  0.0020414    0.30

Correlation of Fixed Effects:
            Group1 Group2 Group3 Grp1:Y Grp2:Y
Group2       0.000                            
Group3       0.000  0.000                     
Group1:Year -0.252  0.000  0.000              
Group2:Year  0.000 -0.252  0.000  0.000       
Group3:Year  0.000  0.000 -0.252  0.000  0.000
</code></pre>

<p>It had an expected effect - the SE of slopes (coefficients Group1-3:Year) are now lower and the residual SE is also lower.</p>

<p><strong>The individuals are also different in slope so I also introduced the random slope effect:</strong></p>

<pre><code>&gt; mix1c = lmer(logInd ~ 0 + Group + Year:Group + (1 + Year|Individual), data = mydata)
&gt; summary(mix1c)
Linear mixed model fit by REML 
Formula: logInd ~ 0 + Group + Year:Group + (1 + Year | Individual) 
   Data: mydata 
  AIC  BIC logLik deviance REMLdev
 2941 3001  -1461     2885    2921
Random effects:
 Groups     Name        Variance  Std.Dev. Corr   
 Individual (Intercept) 0.1054790 0.324775        
            Year        0.0017447 0.041769 -0.246 
 Residual               0.1223920 0.349846        
Number of obs: 2987, groups: Individual, 103

Fixed effects:
              Estimate Std. Error t value
Group1       4.6395740  0.0541746   85.64
Group2       4.8094268  0.0620648   77.49
Group3       4.5607287  0.0651960   69.95
Group1:Year -0.0084165  0.0065557   -1.28
Group2:Year  0.0032369  0.0075105    0.43
Group3:Year  0.0006081  0.0078894    0.08

Correlation of Fixed Effects:
            Group1 Group2 Group3 Grp1:Y Grp2:Y
Group2       0.000                            
Group3       0.000  0.000                     
Group1:Year -0.285  0.000  0.000              
Group2:Year  0.000 -0.285  0.000  0.000       
Group3:Year  0.000  0.000 -0.285  0.000  0.000
</code></pre>

<h3><strong>But now, contrary to the expectation, the SE of slopes (coefficients Group1-3:Year) are now much higher, even higher than with no random effect at all!</strong></h3>

<p>How is this possible? I would expect that the random effect will ""eat"" the unexplained variability and increase ""sureness"" of the estimate!</p>

<p>However, the residual SE behaves as expected - it is lower than in the random intercept model.</p>

<p><a href=""http://artax.karlin.mff.cuni.cz/~ttel5535/pub/my_so_question_data.R"" rel=""nofollow"">Here is the data</a> if needed.</p>

<h2>Edit</h2>

<p>Now I realized astonishing fact. If I do the linear regression for each individual separately and then run ANOVA on the resultant slopes, <strong>I get exactly the same result as the random slope model!</strong> Would you know why?</p>

<pre><code>indivSlope = c()
for (indiv in 1:103) {
    mod1 = lm(logInd ~ Year, data = mydata[mydata$Individual == indiv,])
    indivSlope[indiv] = coef(mod1)['Year']
}

indivGroup = unique(mydata[,c(""Individual"", ""Group"")])[,""Group""]


anova1 = lm(indivSlope ~ 0 + indivGroup)
summary(anova1)

Call:
lm(formula = indivSlope ~ 0 + indivGroup)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.176288 -0.016502  0.004692  0.020316  0.153086 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
indivGroup1 -0.0084165  0.0065555  -1.284    0.202
indivGroup2  0.0032369  0.0075103   0.431    0.667
indivGroup3  0.0006081  0.0078892   0.077    0.939

Residual standard error: 0.04248 on 100 degrees of freedom
Multiple R-squared: 0.01807,    Adjusted R-squared: -0.01139 
F-statistic: 0.6133 on 3 and 100 DF,  p-value: 0.6079 
</code></pre>

<p><a href=""http://artax.karlin.mff.cuni.cz/~ttel5535/pub/my_so_question_data.R"" rel=""nofollow"">Here is the data</a> if needed.</p>
"
"0.0609427635336005","0.0580747904139041"," 32616","<p>In my data, some individuals have missing data on the central predictor (father missed the intake assessment). Comparing the DVs' means for those with a missing/non-missing predictor yielded some sizeable effects.</p>

<p>Now I want to find out whether the systematic missings may have led me to underestimate the size of the OLS regression coefficients. What's a good way to do this?</p>

<p>Simply comparing the variances of the DVs in the group without missings to the group with missings is easy to do?<br>
But conceptually I want to know whether the whole sample has significantly less variability when I leave the group with missings (and significantly-lower-than-average-scores) out, not whether the two groups (with missings and without missings) have different variances.<br>
All that I found so far was about independent samples, not about subsets.</p>

<p>Also, just to bring me up to speed: heteroscedasticity is usually used in the context of residual variance, right? What's a good term for constricted variance that would give me better luck with google?</p>
"
"0.052777981396926","0.0502942438178979"," 32657","<p>I was playing with the <a href=""http://cran.r-project.org/web/packages/TSA/index.html"" rel=""nofollow"">TSA</a> package in R and wanted to test the <code>arimax</code> function to the solution provided in Pankratz's <em>Forecasting with Dynamic Regression Models</em>, chapter 8. The savings rate and the function seems to provide similar results as the ones in the book except for the IO weights which are quite different. I bet there is a transformation that I might be missing.</p>

<p>Any help on understanding why IO coefficients are so different would be appreciated...</p>

<p>the solution states </p>

<pre><code>AO @ t=82,43,89
LS @ t=99
IO @ t=62,55
</code></pre>

<p>with Parameters estimates</p>

<pre><code>C = 6.1635
w82 = 2.3346
w99 = -1.5114
w43 = 1.1378
w62 = 1.4574
w55 = -1.4915
w89 = -1.0702
AR1 = 0.7976
MA2 = -0.3762
</code></pre>

<p>To fit the model in R, I used
(<code>saving</code> is the data)</p>

<pre><code>arimax(saving, order = c(1,0,2), fixed=c(NA,0,NA,NA,NA,NA,NA,NA,NA,NA), io=c(55,62), 
       xreg=data.frame(AO82=1*(seq(saving)==82),
                       AO43=1*(seq(saving)==43),
                       AO89=1*(seq(saving)==89),
                       LS99=1*(seq(saving)&gt;=99)),
       method='ML')
</code></pre>

<p>The savings rate data is (100 points)</p>

<p>4.9
5.2
5.7
5.7
6.2
6.7
6.9
7.1
6.6
7
6.9
6.4
6.6
6.4
7
7.3
6
6.3
4.8
5.3
5.4
4.7
4.9
4.4
5.1
5.3
6
5.9
5.9
5.6
5.3
4.5
4.7
4.6
4.3
5
5.2
6.2
5.8
6.7
5.7
6.1
7.2
6.5
6.1
6.3
6.4
7
7.6
7.2
7.5
7.8
7.2
7.5
5.6
5.7
4.9
5.1
6.2
6
6.1
7.5
7.8
8
8
8.1
7.6
7.1
6.6
5.6
5.9
6.6
6.8
7.8
7.9
8.7
7.7
7.3
6.7
7.5
6.4
9.7
7.5
7.1
6.4
6
5.7
5
4.2
5.1
5.4
5.1
5.3
5
4.8
4.7
5
5.4
4.3
3.5</p>

<p>here it is my output</p>

<pre><code>&gt; arimax(saving, order = c(1,0,2),fixed=c(NA,0,NA,NA,NA,NA,NA,NA,NA,NA),io=c(55,62),xreg=data.frame(AO82=1*(seq(saving)==82),
+ AO43=1*(seq(saving)==43),AO89=1*(seq(saving)==89),LS99=1*(seq(saving)&gt;=99)),method='ML')

Call:
arimax(x = saving, order = c(1, 0, 2), xreg = data.frame(AO82 = 1 * (seq(saving) == 
    82), AO43 = 1 * (seq(saving) == 43), AO89 = 1 * (seq(saving) == 
    89), LS99 = 1 * (seq(saving) &gt;= 99)), fixed = c(NA, 0, NA, NA, NA, NA, 
    NA, NA, NA, NA), method = ""ML"", io = c(55, 62))

Coefficients:
         ar1  ma1     ma2  intercept    AO82    AO43     AO89     LS99    IO-55   IO-62
      0.7918    0  0.3406     6.0628  2.3800  1.1297  -1.0466  -1.4885  -0.5958  0.5517
s.e.  0.0674    0  0.1060     0.3209  0.3969  0.3780   0.3835   0.5150   0.4044  0.3772

sigma^2 estimated as 0.2611:  log likelihood = -75.57,  aic = 169.14
</code></pre>
"
"0.0867230728520531","0.0918243061724248"," 32735","<p>As a financial institution, we often run into analysis of time series data. A lot of times we end up doing regression using time series variables. As this happens, we often encounter residuals with time series structure that violates basic assumption of independent errors in OLS regression.  Recently we are building another model in which I believe we have regression with autocorrelated errors.The residuals from linear model have <code>lm(object)</code> has clearly a AR(1) structure, as evident from ACF and PACF.  I took two different approaches, the first one was obviously to fit the model using Generalized least squares <code>gls()</code> in R. My expectation was that the residuals from gls(object) would be a white noise (independent errors).  But the residuals from <code>gls(object)</code> still have the same ARIMA structure as in the ordinary regression. Unfortunately there is something wrong in what I am doing that I could not figure out.  Hence I decided to manually adjust the regression coefficients from the linear model (OLS estimates). Surprisingly that seems to be working when I plotted the residuals from adjusted regression (the residuals are white noise). I really want to use <code>gls()</code> in <code>nlme</code> package so that coding will be lot simpler and easier. What would be the approach I should take here? Am I supposed to use REML? or is my expectation of non-correlated residuals (white noise) from gls() object wrong?</p>

<pre><code>gls.bk_ai &lt;- gls(PRNP_BK_actINV ~ PRM_BK_INV_ENDING + NPRM_BK_INV_ENDING, 
                 correlation=corARMA(p=1), method='ML',  data  = fit.cap01A)

gls2.bk_ai  &lt;- update(gls.bk_ai, correlation = corARMA(p=2))

gls3.bk_ai &lt;- update(gls.bk_ai, correlation = corARMA(p=3))

gls0.bk_ai &lt;- update(gls.bk_ai, correlation = NULL)

anova(gls.bk_ai, gls2.bk_ai, gls3.bk_ai, gls0.bk_ai)  
     ## looking at the AIC value, gls model with AR(1) will be the best bet

acf2(residuals(gls.bk_ai)) # residuals are not white noise
</code></pre>

<p>Is there something wrong with what I am doing???????</p>
"
"0.052777981396926","0.0335294958785986"," 32839","<p>I have a response variable with 2 categories and $500$ predictor variables. The $500$ coefficient $a_1, a_2, a_3, \ldots, a_{500}$ ranges $(-1, 1)$. Positive $a_i$ indicates category A; negative $a_i$ indicates category B. The larger the coefficient, the stronger it indicates its correspondent category. I get the coefficient from a researcher, who score each coefficient from -1 to 1 based on the importance and influence in classification.</p>

<p>To classify an object that have attributes $x_1, x_2, x_3,\ldots,x_{500}$, I am thinking of using logistic regression. But I do not know how to deal with continuous data (the range of the coefficient is continuous from -1 to 1). Is logistic regression viable? </p>

<p>If not, will someone help with other methods and post your code? I prefer using R.</p>
"
"0.0914141453004008","0.0774330538852055"," 33013","<p>I would like to test the difference in response of two variables to one predictor. Here is a minimal reproducible example. </p>

<pre><code>library(nlme) 
## gls is used in the application; lm would suffice for this example
m.set &lt;- gls(Sepal.Length ~ Petal.Width, data = iris, 
               subset = Species == ""setosa"")
m.vir &lt;- gls(Sepal.Length ~ Petal.Width, data = iris, 
               subset = Species == ""virginica"")
m.ver &lt;- gls(Sepal.Length ~ Petal.Width, data = iris, 
               subset = Species == ""versicolor"")
</code></pre>

<p>I can see that the slope coefficients are different:</p>

<pre><code>m.set$coefficients
(Intercept) Petal.Width 
  4.7771775   0.9301727
m.vir$coefficients
(Intercept) Petal.Width 
  5.2694172   0.6508306 
m.ver$coefficients
(Intercept) Petal.Width 
   4.044640    1.426365 
</code></pre>

<p>I have three questions:</p>

<ol>
<li>How can I test the difference between slopes?</li>
<li>How can I test the difference between residual variances?</li>
<li>What is a simple, effective way to present these comparisons? </li>
</ol>

<p>A related question, <a href=""http://stats.stackexchange.com/questions/1735/method-to-compare-variable-coefficient-in-two-regression-models"">Method to compare variable coefficient in two regression models</a>, suggests re-running the model with a dummy variable to differentiate the slopes, are there options that would allow the use of independent data sets? </p>
"
"0.139781890921503","0.13925845528839"," 33105","<p>I'm working on an ongoing data analysis project about a series of live educational seminars. Each of my data points represents one such event, and for each one I have a multitude of categorical variables, as well as a couple quantitative ones that are my desired response variables (total revenue and number of attendees).</p>

<p>One trend I'm interested in looking at is how the frequency of these events affects my two response variables. Over the years, we have increased the frequency of the events and I'd like to determine whether or not it makes sense to continue doing so. I've created a couple of variables to help track this frequency:</p>

<p><code>NEAREST.SEM</code> - the number of days between this event and the nearest one to it chronologically in either direction</p>

<p><code>LAST.SEM</code> - the number of days between this event and the nearest one to it chronologically <em>before</em> it</p>

<p><code>WEEKLY.SEMS</code> - the total number of events held during the 7-day period starting on Monday within which this event falls</p>

<p>Depending on how I do the analysis, these three variables seem to have varying significance, but the one that seems to consistently come out on top is <code>NEAREST.SEM</code>, which I have found to be significant at the 0.01 level in one test and the 0.001 level in another. The other two variables are significant in predicting revenue but not number of attendees, which is not ideal since we are more interested in number of attendees. (The data for revenue is not representative of the total revenue for each event due to certain special offers for repeat customers that aren't taken into account there.)</p>

<p>Increasing the frequency of events seems to decrease each event's individual performance, but has so far increased overall performance. I'd like to determine the ""turning point"" at which overall performance will either dip or level off. Unfortunately, this is going to be tough to predict because my best-fitted variable, <code>NEAREST.SEM</code>, isn't as good a representation of increased frequency. Note, for example, that it would look exactly the same whether 4 or 5 events were held per week--it would always have the value of 1 in such situations. In fact, any time that events are grouped in clusters of consecutive days, we'll always get 1 for them on this variable...</p>

<p>One option would be to just use <code>WEEKLY.SEMS</code> as a predictor of revenue, which it is well correlated with, but as I said, we'd much rather do this analysis based on number of attendees, a better measure of an event's success.</p>

<p>So I really have two questions here:</p>

<ol>
<li><p>Any suggestions on my dilemma of which variable to use and how to deal with the problems I laid out above?</p></li>
<li><p>Once I decide on a predictor factor, how can I go about estimating the average decrease in revenue increasing to various frequencies will have? Should I run a multiple regression using all my variables and use the coefficient on the predictor factor? Or should I run a regression with just the one factor and my response and use that coefficient? Or is there a better test than regression to use?</p></li>
</ol>

<p>(By the way, I'm using R for my analysis and I'd appreciate any advice specifically tailored for that language.)</p>

<p>UPDATE: I have tried creating two new measures, one that's the average distance in days of the nearest event on either side, and one that's the number of events within 3 days in both directions...neither of them had any significant correlation. I'm running out of ideas here...</p>
"
"0.0691025985081098","0.0658506226617377"," 33174","<p>I'm failing to understand the value of the intercept value in a multiple linear regression with categorical values. Taking the ""warpbreaks"" data set as an example, when I do:</p>

<pre><code>&gt; lm(breaks ~ wool, data=warpbreaks)

Call:
lm(formula = breaks ~ wool, data = warpbreaks)

Coefficients:
(Intercept)        woolB
     31.037       -5.778
</code></pre>

<p>I'm able to understand that the value of intercept is the mean value of breaks when wool equals ""A"", and that adding up the ""woolB"" coefficient to the intercept value I get the mean value of breaks when wool equals ""B"". However, if I also consider the tension variable in the model, I'm unable to figure out the meaning of the intercept value:</p>

<pre><code>&gt; lm(breaks ~ wool + tension, data=warpbreaks)

Call:
lm(formula = breaks ~ wool + tension, data = warpbreaks)

Coefficients:
(Intercept)        woolB     tensionM     tensionH
     39.278       -5.778      -10.000      -14.722
</code></pre>

<p>I thought it would be the mean value of breaks when either wool equals ""A"" or tension equals ""L"", but that isn't true for this dataset.</p>

<p>Any clues on interpreting the value of intercept?</p>
"
"0.102279800236246","0.104963924850184"," 33463","<p>I've got a dataset for Temperature &amp; KwH and I'm currently performing the regression below. (further regression based on coeffs is performed within PHP)</p>

<pre><code># Some kind of List structure..
UsageDataFrame  &lt;- data.frame(Energy, Temperatures);

# lm is used to fit linear models. It can be used to carry out regression,
# single stratum analysis of variance and analysis of covariance (although
# aov may provide a more convenient interface for these).
LinearModel     &lt;- lm(Energy ~ 1+Temperatures+I(Temperatures^2), data = UsageDataFrame)

# coefficients
Coefficients    &lt;- coefficients(LinearModel)

system('clear');

cat(""--- Coefficients ---\n"");
print(Coefficients);
cat('\n\n');
</code></pre>

<p>The issue comes with our data, we can't ensure there isn't random communication failures or just random errors. This can leave us with values like</p>

<pre><code>Temperatures &lt;- c(16,15,13,18,20,17,20);
Energy &lt;- c(4,3,3,4,0,60,4)

Temperatures &lt;- c(17,17,14,17,21,16,19);
Energy &lt;- c(4,3,3,4,0,0,4)
</code></pre>

<p>Now as humans we can clearly see that the 60 for Kwh is a mistake based on the temperature, however we have over 2,000 systems each with multiple meters and each in different locations all over the country.. and with different levels of normal Energy usage.</p>

<p>A normal dataset would be 48 values for both Temperatures &amp; Energy per day, per meter. In a full year its likely we could have around 0-500 bad points per meter out of 17520 points.</p>

<p>I've read other posts about the <code>tawny</code> package however I've not really seen any examples which would me to pass a <code>data.frame</code> and it process them through cross analysis.</p>

<p>I understand not much can be done, however big massive values surely could be stripped based on the temperature? And the number of times it occurs..  </p>

<p>Since R is maths based I see no reason to move this into any other language.</p>

<p>Please note: I'm a Software Developer and have never used R before.</p>

<p>-- Edit --</p>

<p>Okay here's a real world example, seems this meter is a good example. You can see the Zeros are building up then a massive value is inserted. ""23, 65, 22, 24"" being examples of this. This happens when its in comms failure and it holds the data value and continues to add it up on the device. </p>

<p>(Just to say the comms failures are out of my hands nor can I change the software)</p>

<p>However because Zero is a valid value im wanting to remove any massive numbers against the temperatures or Zeros where its clear they are an Error.</p>

<p>The thought of detecting this and averaging the data back isn't a fix for this either, however it was discussed but since this meter data is every 30mins and comms failures can happen for days.</p>

<p>Most systems are using more Energy then this so its perhaps a bad example from a removing Zero's point of view.</p>

<p>Energy: <a href=""http://pastebin.com/gBa8y5sM"" rel=""nofollow"">http://pastebin.com/gBa8y5sM</a>
Temperatures: <a href=""http://pastie.org/4371735"" rel=""nofollow"">http://pastie.org/4371735</a></p>

<p>(Pastebin seems to have gone down for me after posting such a big file)</p>
"
"0.0457070726502004","0.0580747904139041"," 33857","<p>I'm trying to calculate logistic regression coefficients by defining the log-likelihood function and using maximum likelihood.</p>

<p>In some cases when the initial (start) values I gave to the maximum likelihood were not correct I got wrong results for the logistic regression (different from the ones I get when using <code>glm</code> for example).</p>

<p>Given the input data and y values, what should be the optimum initial values for logistic regression (or, in other words, what are the values that are being used in <code>glm</code>)?</p>
"
"0.0977258320053917","0.100887413963854"," 34080","<p>EDIT: I have solved this problem myself. The problem with the simulation below is that the omitted variable should not be included in the 'true model'. I have written a blog post with a more detailed analysis <a href=""http://diffuseprior.wordpress.com/2012/08/15/probit-models-with-endogeneity/"" rel=""nofollow"">here</a>.</p>

<p>I am trying to calculate the Average Structural Function (ASF) for a binary response regression model with an endogenous variable. The ASF is known as the policy relevant result obtained from these models because it shows how the conditional probability of the outcome (one or zero) changes in response to changes in any of the explanatory variables.</p>

<p>To estimate the regression model, I have used a two-step control function approach, wherein the first stage regression residuals ($\textbf{v}_{i}$) are included as a right-hand-side variable in the second stage probit regression Ã  la Rivers and Vuong (1988). </p>

<p>Based on my reading of a paper by Blundell and Powell (2004) (and also <a href=""http://www.cemfi.es/~arellano/binary-endogeneity.pdf"" rel=""nofollow"">these lecture notes</a>) the ASF can be calculated as follows:</p>

<p>$P(y|\bar{\textbf{X}},v)=\widehat{ASF}=\frac{1}{N}\sum^{N}_{i} \Phi(\bar{\textbf{X}}\boldsymbol{\hat{\beta}}+\rho \hat{\textbf{v}_{i}}) $</p>

<p>where the $\textbf{X}$ values are held at a constant level (say their mean), and we average over all of the first-stage residuals (multiplied by the second stage coefficient $\rho$). In effect, this formalization will allow one to calculate how the probability of the outcome varies as the one of the x-variables changes, while all of the other values are (typically) held at their means.</p>

<p>Or so you would think. However, I have attempted this calculation on a simple simulation with R and have not been able to replicate the ASF. My R code is below. Basically, this is a simple setup where we want to measure the effect of y1 on y2 (the binary outcome). There is one omitted variable (x1) that renders y1 endogenous the regression equation of interest.</p>

<p>A picture of my attempt is:</p>

<p><img src=""http://i.stack.imgur.com/OZBA8.jpg"" alt=""enter image description here""></p>

<p>When $x_1$ is available, everything should be fine. Just estimate a standard probit of $y_2$  on $x_1$ and $y_1$. The ASF for this is just the normal CDF for changes in $y_1$. When $x_1$ is not observed, it becomes necessary to instrument $y_1$. </p>

<p>From the IV regression I have calculated the ASF as in the above, and plotted this with comparisons to the model where $x_1$ is observed (the blue line in the picture), and also where $x_1$ is not observed and $y_1$ is not instrumented (the green line).</p>

<p>The red line is my attempt to construct the ASF from the method described in the above. It is clear that this line is not matching the blue line as it should. I have gone wrong somewhere here but I am not sure where. Would somebody be able to help me with this please? </p>

<pre><code>rm(list=ls())
x1 &lt;- rnorm(10000)
x2 &lt;- rnorm(10000)
y1 &lt;- 1 + 0.5*x1 + x2 + rnorm(10000)
y2 &lt;- ifelse(0.5 + 0.5*y1 - 1.5*x1 + rnorm(10000) &gt; 0, 1, 0)

# true
r1 &lt;- glm(y2~y1+x1,binomial(link=""probit""))
data &lt;- data.frame(cbind(seq(-4,6,0.2),mean(x1)))
names(data) &lt;- c(""y1"",""x1"")
asf1 &lt;- cbind(data$y1,pnorm(predict(r1,data)))
plot(asf1,type=""l"",col=""blue"",xlab=""y1"",ylab=""P(y2)"")

# no endog correction
r2 &lt;- glm(y2~y1,binomial(link=""probit""))
data &lt;- data.frame(cbind(seq(-4,6,0.2)))
names(data) &lt;- c(""y1"")
asf2 &lt;- cbind(data$y1,pnorm(predict(r2,data)))
lines(asf2,type=""l"",col=""green"")

# control function approach
v1 &lt;- (residuals(lm(y1~x2)))/sd(residuals(lm(y1~x2)))
r3 &lt;- glm(y2~y1+v1,binomial(link=""probit""))
# proceedure to get asf
asf3 &lt;- cbind(seq(-4,6,0.2),NA)
for(i in 1:dim(asf3)[1]){
    dat2 &lt;- data.frame(cbind(asf3[i,1],v1))
    names(dat2) &lt;- c(""y1"",""v1"")
    asf3[i,2] &lt;- mean(pnorm(predict(r3,dat2)))
}
lines(asf3,type=""l"",col=""red"")
</code></pre>
"
"0.0746393370862076","0.0592723347638087"," 34263","<p>Last month I asked this question <a href=""http://stats.stackexchange.com/questions/31597/graphing-a-probability-curve-for-a-logit-model-with-multiple-predictors"">here</a>.</p>

<p>After thinking about it recently, I was wondering if it makes sense to think about logit probabilities in that regards. Since the predictor of a coefficient shows the log odds change in the response variable independent of all other predictors, we would expect that plotting bid vs pr(outcome), with the curve representing a different predictor is simply not useful. So if the coefficient for variable x is 0.5, that would be the log odds change regardless of the values for y, z, or f. Therefore, I'm wondering if it makes sense to make such a graph.</p>

<ol>
<li><p>Am I thinking about logistic regression correctly? Since logit coefficients are independent of the other predictors, wouldn't a plot like that be largely ""useless.""</p></li>
<li><p>If that is the case, what should be the main use for predicted probabilities when using logit models?</p></li>
</ol>

<p>Just some sample code if you wish: </p>

<pre><code>df=data.frame(income=c(5,5,3,3,6,5),
              won=c(0,0,1,1,1,0),
              age=c(18,18,23,50,19,39),
              home=c(0,0,1,0,0,1))
str(df)

md1 = glm(factor(won) ~ income + age + home, 
          data=df, family=binomial(link=""logit""))
</code></pre>

<p>Thanks!</p>
"
"0.0967596325610309","0.0922061136661462"," 34319","<p>Let's say I have the following logistic regression models:</p>

<pre><code> df=data.frame(income=c(5,5,3,3,6,5),
                  won=c(0,0,1,1,1,0),
                  age=c(18,18,23,50,19,39),
                  home=c(0,0,1,0,0,1))

&gt; md1 = glm(factor(won) ~ income + age + home, 
+           data=df, family=binomial(link=""logit""))
&gt; md2 = glm(factor(won) ~ factor(income) + factor(age) + factor(home), 
+           data=df, family=binomial(link=""logit""))
&gt; summary(md1)

Call:
glm(formula = factor(won) ~ income + age + home, family = binomial(link = ""logit""), 
    data = df)

Deviance Residuals: 
      1        2        3        4        5        6  
-1.0845  -1.0845   0.8017   0.4901   1.7298  -0.8017  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  4.784832   6.326264   0.756    0.449
income      -1.027049   1.056031  -0.973    0.331
age          0.007102   0.097759   0.073    0.942
home        -0.896802   2.252894  -0.398    0.691

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 8.3178  on 5  degrees of freedom
Residual deviance: 6.8700  on 2  degrees of freedom
AIC: 14.87

Number of Fisher Scoring iterations: 4

&gt; summary(md2)

Call:
glm(formula = factor(won) ~ factor(income) + factor(age) + factor(home), 
    family = binomial(link = ""logit""), data = df)

Deviance Residuals: 
         1           2           3           4           5           6  
-6.547e-06  -6.547e-06   6.547e-06   6.547e-06   6.547e-06  -6.547e-06  

Coefficients: (3 not defined because of singularities)
                  Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)      2.457e+01  1.310e+05       0        1
factor(income)5 -4.913e+01  1.605e+05       0        1
factor(income)6 -2.573e-30  1.853e+05       0        1
factor(age)19           NA         NA      NA       NA
factor(age)23   -1.383e-30  1.853e+05       0        1
factor(age)39   -3.479e-14  1.605e+05       0        1
factor(age)50           NA         NA      NA       NA
factor(home)1           NA         NA      NA       NA

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 8.3178e+00  on 5  degrees of freedom
Residual deviance: 2.5720e-10  on 1  degrees of freedom
AIC: 10
</code></pre>

<p>So depending on the mode of the predictors, R produced different outputs. For factors, R splits out the coefficients into separate categories for the levels, but not for the model with numeric predictors. I'm wondering about a couple things.</p>

<ol>
<li><p>Is it ever useful to have the response categories expressed as individual rows?</p></li>
<li><p>To express the general regression equation, how does one go from a model with the categories expressed in an individual equation to an equation with a single B_i. So, for example, if gender has two coefficients, 3.5 for Male and 2.3 for Female, how does one use that in an equation such that (besides converting them into numeric values):</p></li>
</ol>

<p>Y = B0 + B1 (Gender)</p>
"
"0.068136080998913","0.0649295895722714"," 34859","<p>I would like to find predictors for a continuous dependent variable out of a set of 30 independent variables. I am using Lasso regression as implemented in the <a href=""http://cran.r-project.org/web/packages/glmnet/index.html"">glmnet</a> package in R. Here is some dummy code:</p>

<pre><code># generate a dummy dataset with 30 predictors (10 useful &amp; 20 useless) 
y=rnorm(100)
x1=matrix(rnorm(100*20),100,20)
x2=matrix(y+rnorm(100*10),100,10)
x=cbind(x1,x2)

# use crossvalidation to find the best lambda
library(glmnet)
cv &lt;- cv.glmnet(x,y,alpha=1,nfolds=10)
l &lt;- cv$lambda.min
alpha=1

# fit the model
fits &lt;- glmnet( x, y, family=""gaussian"", alpha=alpha, nlambda=100)
res &lt;- predict(fits, s=l, type=""coefficients"")
res 
</code></pre>

<p>My questions is how to interpret the output:</p>

<ul>
<li><p>Is it correct to say that in the final output all predictors that show a coefficient different from zero are related to the dependent variable? </p></li>
<li><p>Would that be a sufficient report in the context of a journal publication? Or is it expected to provide test-statistics for the significance of the coefficients? (The context is human genetics)</p></li>
<li><p>Is it reasonable to calculate p-values or other test-statistic to claim significance? How would that be possible? Is a procedure implemented in R? </p></li>
<li><p>Would a simple regression plot (data points plotted with a linear fit) for every predictor be a suitable way to visualize this data?</p></li>
<li><p>Maybe someone can provide some easy examples of published articles showing the use of Lasso in the context of some real data &amp; how to report this in a journal?</p></li>
</ul>
"
"0.0497595580574717","0.0592723347638087"," 34997","<p>I want to do an ordinal logistic regression in R without the proportionality odds assumption. I know this can be done directly using <code>vglm()</code> function in <code>R</code> by setting <code>parallel=FALSE</code>.</p>

<p>But my problem is how to fix a particular set of coefficients in this regression setup? For example, say the dependent variable $Y$ is discrete and ordinal and can take values $Y = 1$, $2$, or $3$. If the regressors are $X_{1}$ and $X_{2}$, then the regression equations are</p>

<p>$$ \begin{aligned} 
{\rm logit} \big( P(Y \leq 1) \big) &amp;= \alpha_{1} + \beta_{11}X_{1} + \beta_{12}X_{2} \\
{\rm logit}\big(P(Y \leq 2) \big) &amp;= \alpha_{2} + \beta_{21}X_{1} + \beta_{22}X_{2} 
\end{aligned} $$</p>

<p>I want to set $\beta_{11}$ and $\beta_{22}$ to $1$. Please let me know how can I achieve this. Also if <code>R</code> can't do this, could you also please let me know if I can achieve this in any other statistical software?</p>
"
"0.0914141453004008","0.0871121856208561"," 35719","<p>I am just learning R. I have developed a regression model with six predictor variables. While developing it, I found the relationships are not very linear. So, maybe because of this the predictions of my model are not exact.</p>

<p>Here is Headers of my data set:</p>

<pre><code>1.bouncerate(To be predicted)
2.avgServerResponseTime
3.avgServerConnectionTime
4.avgRedirectionTime
5.avgPageDownloadTime
6.avgDomainLookupTime
7.avgPageLoadTime
</code></pre>

<p>Sample datasets:</p>

<pre><code>28.57142857,4.132,0.234,0,0.505,0,14.168
42.85714286,3.356777778,0.090777778,0.077333333,0.459,0.105444444,14.78644444
0,3.372,0.1105,0.0015,0.425,0.1305,34.3425
33.33333333,3.583,0.218,0,0.385,0.649,11.816
66.66666667,2.438,0.234,0,0.3405,0,8.645
100,2.805,0.179666667,3.203666667,0.000333333,0.11,13.47066667
66.66666667,0.977,0,0.003,0,0,12.847
0,2.776,0,7.888,0,0,14.393
100,2.59,0.261,0,0.517,0,6.216
</code></pre>

<p>Here is the summary of my model:</p>

<pre><code>Call:
lm(formula = y ~ x_1 + x_2 + x_3 + x_4 + x_5 + x_6)

Residuals:
     Min       1Q   Median       3Q      Max 
-125.302  -26.210    0.702   26.261  111.511 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 48.62944    0.27999 173.684  &lt; 2e-16 ***
x_1         -0.67831    0.08053  -8.423  &lt; 2e-16 ***
x_2          0.07476    0.49578   0.151 0.880143    
x_3         -0.22981    0.06489  -3.541 0.000399 ***
x_4          0.01845    0.09070   0.203 0.838814    
x_5          3.76952    0.67006   5.626 1.87e-08 ***
x_6          0.07698    0.01565   4.919 8.75e-07 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 33.76 on 19710 degrees of freedom
Multiple R-squared: 0.006298,   Adjusted R-squared: 0.005995 
F-statistic: 20.82 on 6 and 19710 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>plot with all single variable are below:
<img src=""http://i.stack.imgur.com/jsW0j.png"" alt=""bouncerate vs avgServerConnectionTime"">
<img src=""http://i.stack.imgur.com/uhrya.png"" alt=""bouncerate vs ServerResponseTime"">
<img src=""http://i.stack.imgur.com/iuROe.png"" alt=""bouncerate vs avgRedirectionTime"">
<img src=""http://i.stack.imgur.com/wbfsP.png"" alt=""bouncerate vs avgDomainLookupTime"">
<img src=""http://i.stack.imgur.com/arTC6.png"" alt=""bouncerate vs avgPageLoadTime""></p>

<p>I have certain questions about this model:  </p>

<ol>
<li>Is there any way to improve the accuracy of this model?  </li>
<li>Which of the values is most useful: residual standard error, degrees of freedom, multiple R-squared, adjusted R-squared, F-statistics, or p-values for choosing best model?  </li>
<li>Is it appropriate to use polynomial transformations with these data?  </li>
<li>In case I do use polynomial terms in my model, which degree is most appropriate?  </li>
</ol>
"
"0.0304713817668003","0.029037395206952"," 35936","<p>I am new to R and trying to practice with some exercises. Given a data set with 40  observations and 5 variables. Spending is the the response and there are 4 predictors. I started with a linear model Residuals:</p>

<pre><code>    Min      1Q  Median      3Q     Max 
-51.082 -11.320  -1.451   9.452  94.252 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  22.55565   17.19680   1.312   0.1968    
sex         -22.11833    8.21111  -2.694   0.0101 *  
status        0.05223    0.28111   0.186   0.8535    
income        4.96198    1.02539   4.839 1.79e-05 ***
verbal       -2.95949    2.17215  -1.362   0.1803    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 22.69 on 42 degrees of freedom
Multiple R-squared: 0.5267, Adjusted R-squared: 0.4816 
F-statistic: 11.69 on 4 and 42 DF,  p-value: 1.815e-06 
</code></pre>

<p>First, is this what they mean by fit regression model and Secondly, how do I compute the correlation of the residuals with the fitted values? </p>
"
"0.0304713817668003","0.029037395206952"," 36064","<p>I am looking at some simple regression models using both R and the <code>statsmodels</code> package of Python. I've found that, when computing the coefficient of determination, <code>statmodels</code> uses the following formula for $R^2$:
$$
R^2 = 1 - \frac{SSR}{TSS}~~~~~~(\text{centered})
$$
where $SSR$ is the sum of squared residuals, and $TSS$ is the total sum of squares of the model. (""Centered"" means that the mean has been removed from the series.) However, the same calculation in R yields a different result for $R^2$. The reason is that R seems to be calculating $R^2$ as:
$$
R^2 = 1 - \frac{SSR}{TSS}~~~(\text{uncentered})
$$
So, what gives? Presumably there's some reason to prefer one over the other in certain situations. I haven't been able to find any information online about the cases where one of the above formulae should be preferred. </p>

<p>Can someone please explain why one is better than the other?</p>
"
"0.0914141453004008","0.0871121856208561"," 36221","<p>After development of recommendation engine with the R, before removal of outliers from data-set value of residual standard error was 1351 and after removal of outlier its 656. Still there is no accurate prediction which gives 10% correct(near) prediction. For more fitting i also have tried polynomial model with two ,three and four degree but still no improvement. Is there any most important thing to consider without R-squared or adjusted R-squared.   </p>

<p>Where i am using dataset with linear regression model for prediction of product purchase revenue on the base of total numbers of time product added to cart, removed from cart, total numbers of page views of product page. For checking model prediction accuracy i am considering only minimum residual standard error.</p>

<p>Here is Model summary</p>

<pre><code>&gt; summary(model_out)

Call:
lm(formula = yitemrevenue_out ~ xcartaddtotalrs_out + xcartremove_out + 
    xproductviews_out + xuniqprodview_out + xprodviewinrs_out, 
    data = as)

Residuals:
    Min      1Q  Median      3Q     Max 
-2671.1  -173.6   -83.4   -42.9 14288.6 

Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)          3.992e+01  1.254e+01   3.183  0.00147 ** 
xcartaddtotalrs_out -7.888e-03  2.570e-03  -3.070  0.00216 ** 
xcartremove_out     -3.410e+01  2.431e+01  -1.403  0.16076    
xproductviews_out    1.248e+01  1.222e+00  10.215  &lt; 2e-16 ***
xuniqprodview_out   -1.350e+01  1.487e+00  -9.076  &lt; 2e-16 ***
xprodviewinrs_out    3.705e-04  5.151e-05   7.193 7.62e-13 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 656.4 on 3721 degrees of freedom
Multiple R-squared: 0.1398, Adjusted R-squared: 0.1386 
F-statistic: 120.9 on 5 and 3721 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>Thanks</p>
"
"0.0457070726502004","0.0580747904139041"," 37395","<p>What is the canonical example which show situation when robust linear regression has advantage over least square linear regression ? I was trying to simulate situation when some errors (20% of them) are generated from t-student distribution and 80% are from normal - both distribution with the same variance ! on datasets with 50 observation, and I cant see clearly that robust regression is better, here is my R code for this experiment :</p>

<pre><code>library(MASS)
n=50 # size of datasets
N=1000 # number of regressions
wynik=matrix(0,N,2) # matrix with estimated coefficients
v=5  # parametr of t-student distribution
Sd=(v/(v-2))^.5 # standard deviation of gaussian distribution
a=1 # coefficient 

for(i in 1:N){

x=rnorm(n,mean=1,sd=Sd)

e_norm&lt;-rnorm(n,sd=Sd)
e_t&lt;-rt(10, df=v )

y_norm=a*x+e_norm
y_t=a*x+c(e_t,rnorm(40,sd=Sd)) # wariant 2 czÄ™Å›Ä‡ to outliery

Zm1=lm(y_t~x)$coef[2]
    Zm2=rlm(y_t~x)$coef[2]

wynik[i,1]=Zm1
wynik[i,2]=Zm2
plot(1,1,main=paste(i))
}

plot(density(wynik[,1]),main=""density of LS estimator(black) and robust estimator (green)"")
lines(density(wynik[,2]),col=""green"")
# average values of LS and robust estimator
colMeans(wynik)
</code></pre>
"
"0.0879633023282099","0.100588487635796"," 37840","<p>Okay, so I am trying to understand linear regression. I've got a data set and it looks all quite alright, but I am confused. This is my linear model-summary:</p>

<pre><code>Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 0.2068621  0.0247002   8.375 4.13e-09 ***
temp        0.0031074  0.0004779   6.502 4.79e-07 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.04226 on 28 degrees of freedom
Multiple R-squared: 0.6016, Adjusted R-squared: 0.5874 
F-statistic: 42.28 on 1 and 28 DF,  p-value: 4.789e-07 
</code></pre>

<p>so, the p-value is really low, which means it is very unlikely to get the correlation between x,y just by chance.
If I plot it and then draw the regression line it looks like this:
<a href=""http://s14.directupload.net/images/120923/l83eellv.png"" rel=""nofollow"">http://s14.directupload.net/images/120923/l83eellv.png</a>
(Had it in as a picture but I am - as a new user - currently not allowed to post it)
Blue lines = confidence interval
Green lines = prediction interval</p>

<p>Now, a lot of the points do not fall into the confidence interval, why would that happen? I think none of the datapoints falls on the regression line b/c they are just quite far away from each other, but what I am not sure of: Is this a real problem? They still are around the regression line and you can totally see a pattern. But is that enough?
I'm trying to figure it out, but I just keep asking myself the same questions over and over again.</p>

<p>What I thought of so far:
The confidence interval says that if you calculate CI's over and over again, in 95% of the times the true mean falls into the CI.
So: It it is not a problem that the dp do not fall into it, as these are not the means really.
The prediction interval on the other hand says, that if you calculate PI's over and over again, in 95% of the times the true VALUE falls into the interval. So, it is quite important to have the points in it (which I do have).
Then I've read the PI always has to have a wider range than the CI. Why is that?
This is what I have done:</p>

<pre><code>conf&lt;-predict(fm, interval=c(""confidence""))
prd&lt;-predict(fm, interval=c(""prediction""))
</code></pre>

<p>and then I plotted it by:</p>

<pre><code>matlines(temp,conf[,c(""lwr"",""upr"")], col=""red"")
matlines(temp,prd[,c(""lwr"",""upr"")], col=""red"")
</code></pre>

<p>Now, if I calculate CI and PI for additional data, it does not matter how wide I choose the range, I get the exact same lines as above. I cannot understand. What does that mean?
This would then be:</p>

<pre><code>conf&lt;-predict(fm,newdata=data.frame(x=newx), interval=c(""confidence""))
prd&lt;-predict(fm,newdata=data.frame(x=newx), interval=c(""prediction""))
</code></pre>

<p>for new x I chose different sequences.
If the sequence has a different # of observations than the variables in my regression, I am getting a warning. Why would that be?</p>
"
"0.101062140164153","0.0963061447907242"," 38491","<p>If we have a spatial autoregressive process, we can estimate a model to control for the autoregression with a spatial lag,
$$y=\rho W y + X\beta + \epsilon$$
Where $\rho$ is the strength of the spatial correlation, and $W$ is a matrix of spatial weights. The <code>spdep</code> package for R contains the <code>lagsarlm</code> command which is designed to estimate precisely this model. The package contains methods for creating the weights. But there seems to be some discrepancy between the model fit between <code>lagsarlm()</code> and <code>lm()</code> fitted to what should be a similar model.</p>

<p>As an example, consider the example given with <code>?lagsarlm</code> in R. </p>

<pre><code>library(spdep)
data(oldcol)
COL.lag &lt;- lagsarlm(CRIME ~ INC + HOVAL, data=COL.OLD,
                nb2listw(COL.nb, style=""W""), method=""eigen"", quiet=TRUE)
summary(COL.lag)
Residuals:
      Min        1Q    Median        3Q       Max 
-37.68585  -5.35636   0.05421   6.02013  23.20555 

Type: lag 
Coefficients: (asymptotic standard errors) 
             Estimate Std. Error z value  Pr(&gt;|z|)
(Intercept) 45.079251   7.177347  6.2808 3.369e-10
INC         -1.031616   0.305143 -3.3808 0.0007229
HOVAL       -0.265926   0.088499 -3.0049 0.0026570

Rho: 0.43102, LR test value: 9.9736, p-value: 0.001588
Asymptotic standard error: 0.11768
    z-value: 3.6626, p-value: 0.00024962
Wald statistic: 13.415, p-value: 0.00024962
</code></pre>

<p>We can estimate what (I think) should be the same model by computing the actual spatial lag variable,</p>

<pre><code>crime.lag &lt;- lag.listw(nb2listw(COL.nb, style=""W""), COL.OLD$CRIME)
linearlag &lt;- lm(CRIME ~ crime.lag + INC + HOVAL, data=COL.OLD)
Residuals:
    Min      1Q  Median      3Q     Max 
-38.644  -6.103   0.266   6.563  21.610 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 38.18099    9.21531   4.143 0.000149 ***
crime.lag    0.55733    0.15029   3.709 0.000570 ***
INC         -0.86584    0.35541  -2.436 0.018864 *  
HOVAL       -0.26358    0.09136  -2.885 0.005986 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 10.12 on 45 degrees of freedom
Multiple R-squared: 0.6572, Adjusted R-squared: 0.6343 
F-statistic: 28.75 on 3 and 45 DF,  p-value: 1.543e-10 
</code></pre>

<p>The two models, which I think should be identical, are in fact significantly different from each other in every parameter and in model fit (with the <code>linearlag</code> model providing significantly lower AIC). Are there reasons why this should be? Why should I just not use the second model and abandon the special methods?</p>
"
"0.0691025985081098","0.076825726438694"," 39000","<p>I assessed the internal reliability of a self-created scale with eight items ($N = 150$) by calculating Cronbachâ€™s $\alpha$. It appears that one item correlates low with the overall score of the scale (item 4 in the example below). The corrected item-total correlation, i.e. the correlation of this item with the scale total excluding that item, is only $r= .046$. </p>

<pre><code>library(psych)
scale&lt;-mydata[,c(24,25,26,27,28,29,30,31)]
alpha(scale)

Reliability analysis   
Call: alpha(x = scale)

          0.62      0.64    0.66      0.18  4.3 0.79

 Reliability if an item is dropped:
      raw_alpha std.alpha G6(smc) average_r
item1      0.56      0.59    0.62      0.17
item2      0.53      0.57    0.58      0.16
item3      0.54      0.56    0.58      0.16
item4      0.66      0.67    0.67      0.23
item5      0.60      0.62    0.63      0.19
item6      0.55      0.59    0.62      0.17
item7      0.58      0.61    0.63      0.18
item8      0.63      0.65    0.67      0.21

 Item statistics 
        n    r r.cor r.drop mean   sd
item1 144 0.60  0.51  0.395  4.5 0.71
item2 145 0.65  0.62  0.499  4.6 0.71
item3 142 0.67  0.64  0.484  4.5 0.72
item4 146 0.33  0.15  0.046  4.6 0.81
item5 147 0.51  0.41  0.298  4.9 0.41
item6 139 0.59  0.50  0.404  4.4 0.82
item7 136 0.53  0.43  0.339  4.2 1.03
item8 135 0.39  0.21  0.190  4.3 0.94

Non missing response frequency for each item
         1    2    3    4    5 miss
item1 0.01 0.01 0.04 0.34 0.60 0.04
item2 0.01 0.01 0.03 0.24 0.71 0.03
item3 0.00 0.01 0.11 0.28 0.60 0.05
item4 0.01 0.03 0.05 0.14 0.77 0.03
item5 0.00 0.00 0.02 0.11 0.87 0.02
item6 0.01 0.02 0.10 0.29 0.58 0.07
item7 0.04 0.02 0.15 0.25 0.54 0.09
item8 0.02 0.03 0.11 0.32 0.52 0.10
</code></pre>

<p><strong>PROBLEM</strong>: I would like to report this low correlation with the degrees of freedom in parentheses and the significance level in the main text. Yet, I am not sure whether I calculated the correct p-value. What I did is a simple regression with item 4 as the dependent variable:</p>

<pre><code>scale &lt;- as.data.frame(scale)
summary(lm(item4 ~ item1+item2+item3+item5+item6+item7+item8, data=scale))

Call:
lm(formula = item4 ~ item1 + item2 + item3 + item5 + item6 + 
    item7 + item8, data = scale)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.4256 -0.0465  0.2869  0.3500  1.3405 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  1.65098    1.00288   1.646 0.102492    
item1        0.12916    0.11560   1.117 0.266262    
item2        0.02387    0.12921   0.185 0.853760    
item3       -0.07323    0.12718  -0.576 0.565921    
item5        0.64204    0.18636   3.445 0.000802 ***
item6       -0.04596    0.10230  -0.449 0.654120    
item7       -0.13217    0.08030  -1.646 0.102545    
item8        0.05609    0.08758   0.641 0.523136    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.8235 on 113 degrees of freedom
  (29 observations deleted due to missingness)
Multiple R-squared: 0.1385,     Adjusted R-squared: 0.08518 
F-statistic: 2.596 on 7 and 113 DF,  p-value: 0.01604
</code></pre>

<p><strong>QUESTION:</strong> Is it correct if I report something like ""Item 4 correlates only weakly with the overall score of the scale $(r(113)= .046, p= .02)$"" - or did I make a rather large error in reasoning here? </p>
"
"0.109866129394437","0.104695817324578"," 40499","<p>When using the <code>step.plr()</code> function in the <a href=""http://cran.r-project.org/web/packages/stepPlr/index.html"" rel=""nofollow"">stepPlr</a> package, if my predictors are factors, do I need to encode my predictors as dummy variables manually before passing it to the function? I do know that I can specify ""level"", but how  the ""level"" parameter works is confusing to me. 
My understanding is that I need to tell <code>step.plr()</code> explicitly which factors should be encoded as dummy variables and thus leaving one factor out intentionally. </p>

<p>Let's consider a simple example. Suppose I have 1 categorical predicator with 4 levels and binary response. Normally, if I use <code>glm()</code> to fit a logistic regression model, <code>glm()</code> would automatically convert the categorical predicator into 3 dummy variables. Now in <code>stepPlr()</code>, do I specify the ""level"" parameter for that predictor with 4 levels or 3 levels? The ""Help"" section is vague, and says: </p>

<blockquote>
  <p>If the j-th column of x is discrete, level[[ j ]] is the set of levels for the categorical factor.</p>
</blockquote>

<p>Does it mean I should tell <code>step.plr()</code> about all 4 levels, or I should make an intelligent decision myself and tell <code>step.plr()</code> to use only 3 levels? </p>

<p>==============UPDATE (16 Oct 2012)=============</p>

<p>The following example will demonstrate what is the problem with <code>step.plr()</code>'s automatic dummy variable encoding. It is a slight modification of the code in the function's help section. 
     set.seed(100)</p>

<pre><code>n &lt;- 100
p &lt;- 3
z &lt;- matrix(sample(seq(3),n*p,replace=TRUE),nrow=n)
x &lt;- data.frame(x1=factor(z[ ,1]),x2=factor(z[ ,2]),
                x3=factor(sample(seq(3), n, replace=TRUE, prob=c(0.2, 0.5, 0.3))),
                x4=factor(sample(seq(3), n, replace=TRUE, prob=c(0.1, 0.3, 0.6))))
y &lt;- sample(c(0,1),n,replace=TRUE)
fit &lt;- step.plr(x,y, cp=""aic"")
summary(fit)
</code></pre>

<p>And here's an excerpt of the result:</p>

<pre><code>Call:
plr(x = ix0, y = y, weights = weights, offset.subset = offset.subset, 
    offset.coefficients = offset.coefficients, lambda = lambda, 
    cp = cp)

Coefficients:
      Estimate Std.Error z value Pr(&gt;|z|)
Intercept  0.91386   5.04780   0.181    0.856
x4.1       1.33787   4.61089   0.290    0.772
x4.2      -1.70462   4.91240  -0.347    0.729
x4.3       0.36675   3.18857   0.115    0.908
x3.1:x4.1  7.04901  14.35112   0.491    0.623
x3.1:x4.2 -5.50973  15.53674  -0.355    0.723
x3.1:x4.3 -0.50012   7.95651  -0.063    0.950
</code></pre>

<p>You can see that all levels, that is, (1,2,3), are used to fit the model. But normally you only need two dummy variables to encode a predictor with 3 levels.
On the other hand, if you use <code>glm()</code>: </p>

<pre><code>glm(y~.^2, data=x, family=binomial)
</code></pre>

<p>you will get the correct dummy variable encoding.</p>
"
"0.0304713817668003","0.029037395206952"," 40745","<p>I am using earth package for the following data.</p>

<pre><code>x &lt;- c(127, 128, 255, 256, 511, 512, 600, 700, 800, 900, 1000, 1023, 1100,
       1200, 1300, 1400, 1500, 1600, 2047, 2048, 2100, 2200, 2300, 2400, 2500,
       2600, 2700, 2800, 3000, 3100, 3200, 3300, 3500, 4063, 4064, 4100, 4200,
       5200, 5400)

y &lt;- c(0.59, 0.61, 0.59, 1.55, 1.33, 3.50, 1.00, 1.22, 2.50, 3.00, 3.79,
       3.98, 4.33, 4.45, 4.59, 4.72, 4.82, 4.90, 4.96, 7.92, 5.01, 5.01,
       4.94, 5.05, 5.04, 5.03, 5.06, 5.10, 5.04, 5.06, 7.77, 5.07, 5.08,
       5.08, 5.12, 5.12, 5.08, 5.17, 5.18) 
</code></pre>

<p>After building the model, </p>

<pre><code>model&lt;-earth(y~x)
</code></pre>

<p>I get following regression model. </p>

<pre><code>summary(model)
Call: earth(x=x, y=y)
coefficients
(Intercept)  5.225822553
h(1400-x)   -0.003820087
</code></pre>

<p>Is there any possibility that I can increase somehow the number of knots or regression splines?</p>
"
"0.0430930413588572","0.0410650781176591"," 41006","<p>Pardon my naÃ¯vetÃ© if this is a dumb question, but I'm new to R.  I'm trying to do an ordered logit regression.  I'm running the model like so (just a dumb little model estimating number of firms in a market from income and population measures).  My question is about predictions.</p>

<pre><code>nfirm.opr&lt;-polr(y~pop0+inc0, Hess = TRUE)
pr_out&lt;-predict(nfirm.opr)
</code></pre>

<p>When I run predict (which I'm trying to use to get the predicted y), the outputs are either 0, 3, or 27, which in no way reflects what should seem to be the prediction based upon my manual predictions from the coefficient estimates and intercepts.  Does anyone know how get ""accurate"" predictions for my ordered logit model?</p>

<p><strong>EDIT</strong></p>

<p>To clarify my concern, my response data has observations across all the levels</p>

<pre><code>&gt;head(table(y))
y
0  1  2  3  4  5 
29 21 19 27 15 16 
</code></pre>

<p>where as my predict variable seems to be bunching up</p>

<pre><code>&gt; head(table(pr_out))
pr_out
0     1   2   3   4   5 
117   0   0 114   0   0 
</code></pre>
"
"0.0304713817668003","0.029037395206952"," 41168","<p>I have the model that I need to estimate, $$ Y = \pi_0 + \pi_1 X_1 + \pi_2 X_2 + \pi_3 X_3 + \varepsilon, $$
with $\sum_k \pi_k = 1 \text{ for }k \geq 1$ and $\pi_k\ge0 \text{ for }k \geq 1$. </p>

<p>Elvis <a href=""http://stats.stackexchange.com/questions/21565/how-do-i-fit-a-constrained-regression-in-r-so-that-coefficients-total-1"">answer to another question</a> solves this for the case of $\pi_0 = 0$. Here's his/her code of this solution:</p>

<pre><code>   &gt; library(""quadprog"");
   &gt; X &lt;- matrix(runif(300), ncol=3)
   &gt; Y &lt;- X %*% c(0.2,0.3,0.5) + rnorm(100, sd=0.2)
   &gt; Rinv &lt;- solve(chol(t(X) %*% X));
   &gt; C &lt;- cbind(rep(1,3), diag(3))
   &gt; b &lt;- c(1,rep(0,3))
   &gt; d &lt;- t(Y) %*% X  
   &gt; solve.QP(Dmat = Rinv, factorized = TRUE, dvec = d, Amat = C, bvec = b, meq = 1)
   $solution
   [1] 0.2049587 0.3098867 0.4851546

   $value
   [1] -16.0402

   $unconstrained.solution
   [1] 0.2295507 0.3217405 0.5002459

   $iterations
   [1] 2 0

   $Lagrangian
   [1] 1.454517 0.000000 0.000000 0.000000

   $iact
   [1] 1
</code></pre>

<p><strong>How can I adjust this code such that it can estimate an intercept?</strong></p>

<p>This has been cross-posted <a href=""http://www.talkstats.com/showthread.php/29630-Constrained-lm%28%29-coefficients-in-R"">here</a> because my group in my assignment is getting annoyed that I haven't estimated this regression yet. I will answer this question here if/when 
the other forum participants get there first. </p>
"
"0.190293718142101","0.162739657002927"," 41362","<p>I have no formal training in statistics so please correct me if I use the wrong terms as I try to explain my problem.</p>

<p>I have a set of data (less than 80 points) with essentially 1 single outcome (a float we will call <code>dcl</code>) that can potentially depends on 10 of other variables, most of them floats maybe one or two boolean.</p>

<p>While I might ask some multi-variate regression question later, let's start with something simple. </p>

<p>Historically, people in my field have focused on the strongest correlation between <code>dcl</code> and variable <code>J</code> and some of my data suggests some other dependence on a float <code>Re</code> which is I'm sure at least weakly correlated with 'J' as they share some variables in their respective expressions. So my first question is how do I test the correlation and/or the independence of 'Re' and 'J' on the outcome 'dlc'? Intuitively and physically, I expect 'dlc' to depend strongly on 'J' and weakly on 'Re'. How do I prove this with a statistical analysis?</p>

<p>Here are a few graphs to illustrate the data:</p>

<p><img src=""http://i.stack.imgur.com/dVRMa.png"" alt=""Re vs J"">   </p>

<p><img src=""http://i.stack.imgur.com/hNPvb.png"" alt=""dcl vs J""></p>

<p><img src=""http://i.stack.imgur.com/D3YJe.png"" alt=""dcl vs Re""></p>

<p>Final point, in terms of software, I have python and R installed, I'm fairly proficient in python but I just installed R and know pretty much nothing about it.</p>

<p>EDIT 1: </p>

<p>Following gung's suggestion, I ran my dataset through R:</p>

<pre><code>Call:
lm(formula = dcl ~ J + I(J^2) + Re + I(Re^2), data = df)

Residuals:
    Min      1Q  Median      3Q     Max 
-9.0078 -3.7930 -0.4458  2.0869 21.2538 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  1.648e+01  1.232e+00  13.380  &lt; 2e-16 ***
J           -2.662e+00  3.773e-01  -7.054 6.58e-10 ***
I(J^2)       1.096e-01  2.071e-02   5.293 1.10e-06 ***
Re           1.966e-06  1.621e-05   0.121    0.904    
I(Re^2)      2.191e-11  3.441e-11   0.637    0.526    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 5.369 on 77 degrees of freedom
Multiple R-squared: 0.4831, Adjusted R-squared: 0.4562 
F-statistic: 17.99 on 4 and 77 DF,  p-value: 1.818e-10
</code></pre>

<p>So now I need some help to decipher this (but I will look into R documentation too). I don't know if it's relevant but on physical grounds only, it's probable the dependency in J is $dcl \sim \frac{1}{\sqrt{J}}$. Can I put this directly into the model? Does that already tell me something about the dependency on $J$ vs $Re$?</p>

<pre><code>Call:
lm(formula = dcl ~ J + I(J^(-0.5)) + Re + I(Re^(-0.5)), data = df)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.8119 -3.0097 -0.8504  1.8506 12.1439 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   8.175e-01  1.634e+00   0.500   0.6184    
J            -2.946e-01  1.258e-01  -2.343   0.0217 *  
I(J^(-0.5))   4.516e+00  7.053e-01   6.403 1.09e-08 ***
Re            3.332e-05  6.684e-06   4.985 3.72e-06 ***
I(Re^(-0.5))  6.009e+02  1.354e+02   4.438 2.98e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 4.426 on 77 degrees of freedom
Multiple R-squared: 0.6487, Adjusted R-squared: 0.6305 
F-statistic: 35.55 on 4 and 77 DF,  p-value: &lt; 2.2e-16

&gt; model = lm(dcl ~ J+I(J^(-0.5)) + Re+I(Re^(-0.5)), data=df)
&gt; summary(model)
</code></pre>

<p><strong>EDIT 2</strong>: OK I'm starting to understand things better. Also, again based on physical grounds, I would think that the dependency is more something like $dcl ~ \frac{1}{\sqrt{J}} Re^{n}$ with possibly other variables in that product that I ignore. So when I enter such model in R (can I still use <code>lm</code> for something non-linear?), here is what I get:</p>

<pre><code>Call:
lm(formula = dcl ~ I(J^(-0.5)) * I(Re^(-0.1)), data = df)

Residuals:
     Min       1Q   Median       3Q      Max 
-10.5363  -3.0192  -0.2556   1.4373  17.1494 

Coefficients:
                         Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)               -43.220      9.164  -4.716 1.03e-05 ***
I(J^(-0.5))                63.813     11.088   5.755 1.62e-07 ***
I(Re^(-0.1))              124.245     24.038   5.169 1.77e-06 ***
I(J^(-0.5)):I(Re^(-0.1)) -142.744     27.269  -5.235 1.36e-06 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 4.668 on 78 degrees of freedom
Multiple R-squared: 0.6042, Adjusted R-squared: 0.5889 
F-statistic: 39.68 on 3 and 78 DF,  p-value: 1.122e-15
</code></pre>

<p>Does that 4th line tell me something about the dependence between $J$ and $Re$? What kind of tools could I use to get an estimate on the exponent on Re? Because right now I'm just trying a few different numbers empirically to see how the errors evolve. Next for me is to plot the dcl vs the new model and see how well the data collapses visually...</p>

<p>EDIT 3:</p>

<p>In the end, I used <code>nls</code> to explore the possible exponents of my fit. I also removed some outliers in my data that used a different experimental method. I settled on a fit that gave me decent Pr(>|t|) and residuals and which visually produce a decent fit. The last outlier is actually another experiment with a different setup, but one that I trust. So in a sense it's good that it shows up as an outlier as it hints at other parameters that need to be explored. Thank you gung, I accept your answer as I believe it guided me in the right direction.</p>

<pre><code>&gt; model = nls(L.D ~ C*I(J^(c1))*I(Re_s^(c2)), start=list(C=10,c1=-0.25,c2=-0.25),data=df)
&gt; summary(model)
Formula: L.D ~ C * I(J^(c1)) * I(Re_s^(c2))
Parameters:
   Estimate Std. Error t value Pr(&gt;|t|)    
C  57.20389   26.40011   2.167   0.0337 *  
c1 -0.27721    0.05901  -4.698 1.27e-05 ***
c2 -0.16424    0.04936  -3.327   0.0014 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
Residual standard error: 4 on 70 degrees of freedom
Number of iterations to convergence: 8 
Achieved convergence tolerance: 2.91e-06 
</code></pre>

<p><img src=""http://i.stack.imgur.com/dfgWX.png"" alt=""enter image description here""></p>
"
"0.0457070726502004","0.0435560928104281"," 41577","<p>I am trying to get some bootstrap CIs for coefficients obtained by robust regression. I have influential values and thus switched to <code>rlm</code>.</p>

<p>The data are clustered and within clusters, the variance of my DV (""DurchlÃ¤ssigkeit"") = 0.</p>

<p>Is this all sensible for my clustered data?</p>

<pre><code>&gt; dput(d)
structure(list(PorenflÃ¤che = c(4990L, 7002L, 7558L, 7352L, 7943L,
7979L, 9333L, 8209L, 8393L, 6425L, 9364L, 8624L, 10651L, 8868L,
9417L, 8874L, 10962L, 10743L, 11878L, 9867L, 7838L, 11876L, 12212L,
8233L, 6360L, 4193L, 7416L, 5246L, 6509L, 4895L, 6775L, 7894L,
5980L, 5318L, 7392L, 7894L, 3469L, 1468L, 3524L, 5267L, 5048L,
1016L, 5605L, 8793L, 3475L, 1651L, 5514L, 9718L), P.Perimeter = c(2791.9,
3892.6, 3930.66, 3869.32, 3948.54, 4010.15, 4345.75, 4344.75,
3682.04, 3098.65, 4480.05, 3986.24, 4036.54, 3518.04, 3999.37,
3629.07, 4608.66, 4787.62, 4864.22, 4479.41, 3428.74, 4353.14,
4697.65, 3518.44, 1977.39, 1379.35, 1916.24, 1585.42, 1851.21,
1239.66, 1728.14, 1461.06, 1426.76, 990.388, 1350.76, 1461.06,
1376.7, 476.322, 1189.46, 1644.96, 941.543, 308.642, 1145.69,
2280.49, 1174.11, 597.808, 1455.88, 1485.58), P.Form = c(0.0903296,
0.148622, 0.183312, 0.117063, 0.122417, 0.167045, 0.189651, 0.164127,
0.203654, 0.162394, 0.150944, 0.148141, 0.228595, 0.231623, 0.172567,
0.153481, 0.204314, 0.262727, 0.200071, 0.14481, 0.113852, 0.291029,
0.240077, 0.161865, 0.280887, 0.179455, 0.191802, 0.133083, 0.225214,
0.341273, 0.311646, 0.276016, 0.197653, 0.326635, 0.154192, 0.276016,
0.176969, 0.438712, 0.163586, 0.253832, 0.328641, 0.230081, 0.464125,
0.420477, 0.200744, 0.262651, 0.182453, 0.200447), DurchlÃ¤ssigkeit = c(6.3,
6.3, 6.3, 6.3, 17.1, 17.1, 17.1, 17.1, 119, 119, 119, 119, 82.4,
82.4, 82.4, 82.4, 58.6, 58.6, 58.6, 58.6, 142, 142, 142, 142,
740, 740, 740, 740, 890, 890, 890, 890, 950, 950, 950, 950, 100,
100, 100, 100, 1300, 1300, 1300, 1300, 580, 580, 580, 580), Gebiete = structure(c(1L,
1L, 1L, 1L, 2L, 2L, 2L, 2L, 6L, 6L, 6L, 6L, 4L, 4L, 4L, 4L, 3L,
3L, 3L, 3L, 7L, 7L, 7L, 7L, 9L, 9L, 9L, 9L, 10L, 10L, 10L, 10L,
11L, 11L, 11L, 11L, 5L, 5L, 5L, 5L, 12L, 12L, 12L, 12L, 8L, 8L,
8L, 8L), .Label = c(""6.3"", ""17.1"", ""58.6"", ""82.4"", ""100"", ""119"",
""142"", ""580"", ""740"", ""890"", ""950"", ""1300""), class = ""factor"")), .Names = c(""PorenflÃ¤che"",
""P.Perimeter"", ""P.Form"", ""DurchlÃ¤ssigkeit"", ""Gebiete""), row.names = c(""1"",
""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", ""11"", ""12"", ""13"",
""14"", ""15"", ""16"", ""17"", ""18"", ""19"", ""20"", ""21"", ""22"", ""23"", ""24"",
""25"", ""26"", ""27"", ""28"", ""29"", ""30"", ""31"", ""32"", ""33"", ""34"", ""35"",
""36"", ""37"", ""38"", ""39"", ""40"", ""41"", ""42"", ""43"", ""44"", ""45"", ""46"",
""47"", ""48""), class = ""data.frame"")

## do robust regression and bootstrap the coefficients, allowing for clustered data
## by putting ""Gebiet"" as strata argument (?),
## dv variation within clusters/Gebiet = 0!
bs &lt;- function(formula, data, indices) {
  d &lt;- data[indices, ] # allows boot to select sample
  fit &lt;- rlm(formula, data = d)
  return(coef(fit))
}

results &lt;- boot(data = d, statistic = bs, strata = d$Gebiete,
                R = 199, formula = DurchlÃ¤ssigkeit ~ P.Perimeter + P.Form)

# get 99% confidence intervals
boot.ci(results, type=""bca"", index=1, conf = .99) # intercept
boot.ci(results, type=""bca"", index=2, conf = .99) # P.Perimeter
boot.ci(results, type=""bca"", index=3, conf = .99) # P.Form
</code></pre>
"
"0.068136080998913","0.0649295895722714"," 43315","<p>The model that I created in R is:</p>

<blockquote>
  <p>fit &lt;- lm(hired ~ educ + exper + sex, data=data)</p>
</blockquote>

<p>what I am unsure of is how to fit to model to predict probability of interest where p = pr(hiring = 1).</p>

<p>Any help would be appreciated thanks,
Clay </p>

<p><strong>Edit:</strong>
This is the computer output for what I have computed so far. I am unsure if this is even a step in the right direction to find the answer to this question.</p>

<p>What I am trying to do is, Fit a logistic regression model to predict the probability of being hired using years of education, years of experience and sex of job applicants.</p>

<pre><code> &gt; test&lt;-glm(hired ~ educ + exper + sex, data=data, family=binomial(link=""logit""))
 &gt; summary(test)

 Call:
 glm(formula = hired ~ educ + exper + sex, family = binomial(link = ""logit""), 
     data = data)

 Deviance Residuals: 
     Min       1Q   Median       3Q      Max  
 -1.4380  -0.4573  -0.1009   0.1294   2.1804  

 Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)  
 (Intercept) -14.2483     6.0805  -2.343   0.0191 *
 educ          1.1549     0.6023   1.917   0.0552 .
 exper         0.9098     0.4293   2.119   0.0341 *
 sex           5.6037     2.6028   2.153   0.0313 *
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

 (Dispersion parameter for binomial family taken to be 1)

     Null deviance: 35.165  on 27  degrees of freedom
 Residual deviance: 14.735  on 24  degrees of freedom
 AIC: 22.735

 Number of Fisher Scoring iterations: 7
</code></pre>
"
"0.0710998907892006","0.0871121856208561"," 43675","<p>I am working on a housing problem in which I use dichotomous and ratio data to predict
housing production (units constructed in a year-ratio) in a 17 year time period. At this time, I am using OLS and as I get better at stats, I shall attempt this problem using time-series analysis.  That said, I have used R to standardize all of my ratio predicting data and left the dichotomous data raw.  And I have also transformed the response variable to a Natural log to normalize the distribution (i.e. many, many zeros>>yes, I know Poisson or Zero-populated counts in the future).</p>

<p>I have read the post on ""interpret coefficients from a quantile regression on standardized data"" and also the ""convert my unstandardized independent variables to standardized."" Based on those, I think that can do the following interpretation based on the following output. The variable <code>region_id</code> is dichotomous, <code>supply</code> is standardized.</p>

<pre><code>Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)          2.687e+00  2.171e-01  12.379  &lt; 2e-16 ***

region_id            1.805e+00  1.383e-01  13.049  &lt; 2e-16 ***

supply              -2.205e+01  2.204e+00 -10.005  &lt; 2e-16 ***
</code></pre>

<p>Region Interpretation:<br>
For every on city that is located in the Houston region, you can expect that annual housing production will increase by 1.8%.  </p>

<p>Supply Interpretation:<br>
For every one-unit increase in the standard deviation of housing supply, you can expect that annual housing production will decrease by -22.05%.</p>

<p>Nota bene.<br>
I am not a stats or math person at all,
but I have been using R for the past three years
and I am quite familiar with OLS, but if you throw
up an equation it will look ""appropriately"" Greek to me. :)</p>
"
"0.0691025985081098","0.0658506226617377"," 44281","<p>In R, is there a predefined function that will give me the log hazard ratio and its standard error for a black male (as shown in the example below) given the output of coxph regression?</p>

<pre><code>library(survival)
library(KMsurv)

#Kidney transplant data from Klein and Moeshberger. Massage data to make
#results look like those in book
data(kidtran)
data2 &lt;- kidtran
data2$Gender &lt;- ""male""
    data2[data2$gender==2,7] &lt;- ""female""
data2$Race &lt;- ""white""
    data2[data2$race==2,8] &lt;- ""black""
data2$Gender &lt;- as.factor(data2$Gender)
data2$Race &lt;- as.factor(data2$Race)
data2$Race &lt;- relevel(data2$Race,ref=""white"")

fit2 &lt;- coxph(Surv(time,delta) ~ Gender * Race, data=data2)
summary(fit2)

#Relative log risk for a black male (reference white female) from
#page 252 in Klein and Moeshberger
(coef(fit2)[3] + coef(fit2)[2] + coef(fit2)[1])
sqrt(sum(diag(fit2$var)) + 2*fit2$var[2,1] + 2*fit2$var[3,1] + 2*fit2$var[3,2])

#Let's use predict
black.male &lt;- data.frame(
  Gender=""male"",
  Race=""black""
)

white.female &lt;- data.frame(
  Gender=""female"",
  Race=""white""
)

bm &lt;- predict(fit2,newdata=black.male,se.fit=TRUE)

#bm in terms of original coefficients
coef(fit2)[3]*(1-fit2$means[3]) + coef(fit2)[2]*(1-fit2$means[2]) + coef(fit2)[1]*(1-fit2$means[1])

wf &lt;- predict(fit2,newdata=white.female,se.fit=TRUE)

#Relative log risk and se for a black male (reference white female) 
bm$fit - wf$fit
sqrt(bm$se.fit^2 + wf$se.fit^2)
</code></pre>
"
"0.068136080998913","0.0649295895722714"," 44502","<p>I have obtained some strange results for the following data when conducting Poisson regression in R.</p>

<pre><code>&gt; RHT1b
   f x1 y1 y2
1 f1  0 35  1
2 f2  2 70  4
3 f3  0  5  1
4 f4  9 37  4
5 f5  0  3  0

&gt; summary(amod2b)

Call:
glm(formula = x1 ~ y2, family = poisson, data = RHT1b)

Deviance Residuals: 
       1         2         3         4         5  
-0.00008  -1.71860  -0.00008   1.36550   0.00000  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   -26.61    9977.85  -0.003    0.998
y2              7.08    2494.46   0.003    0.998

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 24.9766  on 4  degrees of freedom
Residual deviance:  4.8182  on 3  degrees of freedom
AIC: 15.485

Number of Fisher Scoring iterations: 18
</code></pre>

<p>I had an issue with the data earlier where x1 row 4 read ""8"", and row 5 read ""1"". When I ran the regression the results seemed reliable and were nearly significant. After correcting the data to how it is shown above and updating the model, I have results which literally do not make sense to me. </p>

<p>My question is, why has this happened? Is is possible the value of 1 changing to 0 exceeded some sort of threshold of zero counts that the Poisson model can't handle? Would a negative binomial model be more appropriate?</p>
"
"0.109866129394437","0.104695817324578"," 44922","<p>I am trying to recreate analysis of diamond data in R and compare it to minitab output cited in <a href=""http://www.amstat.org/publications/jse/v9n2/datasets.chu.html"" rel=""nofollow"">http://www.amstat.org/publications/jse/v9n2/datasets.chu.html</a> </p>

<p>I'm almost certain its the same data set yet my results are different. I suspect I'm modeling it differently. I also noticed that I'm missing colourD and some others. Did the the minitab model somehow pick different dummys to exclude?</p>

<p><strong>R OUTPUT</strong></p>

<pre><code>require(""Ecdat"")

data(Diamond)

Diamond$ln_price &lt;- log(Diamond$price)

summary(lm(ln_price~carat+colour+clarity+certification, data=Diamond))


Call:
lm(formula = ln_price ~ carat + colour + clarity + certification, 
    data = Diamond)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.31236 -0.11520  0.01613  0.10833  0.36339 

Coefficients:
                   Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)       6.8011915  0.0496111 137.090  &lt; 2e-16 ***
carat             2.8550130  0.0369676  77.230  &lt; 2e-16 ***
colourE          -0.0295093  0.0404611  -0.729  0.46638    
colourF          -0.1063582  0.0379807  -2.800  0.00544 ** 
colourG          -0.2063494  0.0389848  -5.293 2.35e-07 ***
colourH          -0.2878756  0.0394752  -7.293 2.81e-12 ***
colourI          -0.4165565  0.0413818 -10.066  &lt; 2e-16 ***
clarityVS1       -0.2019313  0.0310634  -6.501 3.41e-10 ***
clarityVS2       -0.2985406  0.0333027  -8.964  &lt; 2e-16 ***
clarityVVS1      -0.0007058  0.0311121  -0.023  0.98192    
clarityVVS2      -0.0966174  0.0289396  -3.339  0.00095 ***
certificationHRD -0.0088557  0.0208641  -0.424  0.67155    
certificationIGI -0.1827107  0.0249516  -7.323 2.33e-12 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.1382 on 295 degrees of freedom
Multiple R-squared: 0.9723,     Adjusted R-squared: 0.9712 
F-statistic: 863.6 on 12 and 295 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p><strong>MINITAB OUTPUT</strong></p>

<pre><code>ln_price = 6.08 + 2.86 Carat + 0.417 D + 0.387 E + 0.310 F + 0.210 G + 0.129 H
          + 0.299 IF + 0.298 VVS1 + 0.202 VVS2 + 0.0966 VS1 + 0.0089 GIA
           - 0.174 IGI

Predictor        Coef       StDev          T        P       
Constant      6.07724     0.04809     126.37    0.000
Carat         2.85501     0.03697      77.23    0.000      
D             0.41656     0.04138      10.07    0.000       
E             0.38705     0.03082      12.56    0.000       
F             0.31020     0.02748      11.29    0.000       
G             0.21021     0.02836       7.41    0.000       
H             0.12868     0.02852       4.51    0.000       
IF            0.29854     0.03330       8.96    0.000       
VVS1          0.29783     0.02810      10.60    0.000       
VVS2          0.20192     0.02534       7.97    0.000       
VS1           0.09661     0.02492       3.88    0.000       
GIA           0.00886     0.02086       0.42    0.672       
IGI          -0.17385     0.02867      -6.06    0.000       

S = 0.1382      R-Sq = 97.2%     R-Sq(adj) = 97.1%

Analysis of Variance

Source            DF          SS          MS         F        P
Regression        12     197.939      16.495    863.64    0.000
Residual Error   295       5.634       0.019
Total            307     203.574
</code></pre>

<p><strong>UPDATE</strong>
Glen_b's solution produces the correct results, but the output is not desirable. </p>

<pre><code>                       Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)           6.3846350  0.0411520 155.148  &lt; 2e-16 ***
carat                 2.8550130  0.0369676  77.230  &lt; 2e-16 ***
C(colour, base = 6)1  0.4165565  0.0413818  10.066  &lt; 2e-16 ***
C(colour, base = 6)2  0.3870472  0.0308241  12.557  &lt; 2e-16 ***
C(colour, base = 6)3  0.3101983  0.0274791  11.288  &lt; 2e-16 ***
C(colour, base = 6)4  0.2102072  0.0283593   7.412 1.32e-12 ***
C(colour, base = 6)5  0.1286809  0.0285231   4.511 9.31e-06 ***
</code></pre>

<p>Is there a way to get the original factor character displayed instead of the level?</p>

<p><strong>UPDATE</strong></p>

<p>Using relevel to set the base to the same bases as the minitab produces the same results and is still readable.</p>

<pre><code>  Diamond$colour &lt;- relevel(Diamond$colour, ref=""I"")

  Coefficients:
                     Estimate Std. Error t value Pr(&gt;|t|)    
  (Intercept)       6.3846350  0.0411520 155.148  &lt; 2e-16 ***
  carat             2.8550130  0.0369676  77.230  &lt; 2e-16 ***
  colourD           0.4165565  0.0413818  10.066  &lt; 2e-16 ***
  colourE           0.3870472  0.0308241  12.557  &lt; 2e-16 ***
  colourF           0.3101983  0.0274791  11.288  &lt; 2e-16 ***
  colourG           0.2102072  0.0283593   7.412 1.32e-12 ***
  colourH           0.1286809  0.0285231   4.511 9.31e-06 ***
</code></pre>
"
"0.075412822378","0.0718638867059034"," 45449","<p>I have a large set of predictors (more than 43,000) for predicting a dependent variable which can take 2 values (0 or 1). The number of observations is more than 45,000. Most of the predictors are unigrams, bigrams and trigrams of words, so there is high degree of collinearity among them. There is a lot of sparsity in my dataset as well. I am using the logistic regression from the glmnet package, which works for the kind of dataset I have. My problem is how can I report p-value significance of the predictors. I do get the beta coefficient, but is there a way to claim that the beta coefficients are statistically significant?</p>

<p>Here is my code:</p>

<pre><code>library('glmnet')
data &lt;- read.csv('datafile.csv', header=T)
mat = as.matrix(data)
X = mat[,1:ncol(mat)-1] 
y = mat[,ncol(mat)]
fit &lt;- cv.glmnet(X,y, family=""binomial"")
</code></pre>

<p>Another question is:
I am using the default alpha=1, lasso penalty which causes the additional problem that if two predictors are collinear the lasso will pick one of them at random and assign zero beta weight to the other. I also tried with ridge penalty (alpha=0) which assigns similar coefficients to highly correlated variables rather than selecting one of them. However, the model with lasso penalty gives me a much lower deviance than the one with ridge penalty. Is there any other way that I can report both predictors which are highly collinear?</p>
"
"0.0545088647991304","0.0649295895722714"," 45489","<p>I used Tobit regression and wanted to test if the coefficients are significant. The problem is that when I run the R code I get t-values instead of z-values. I found code on how to calculate p-values for z-values and I'm wondering if you can use the same code for the t-values that you use for the z-values to calculate the p-values. </p>

<p><strong>This is the code for when you have z-values:</strong></p>

<pre><code>ctable &lt;- coef(summary(m))
pvals &lt;- 2 * pt(abs(ctable[, ""z value""]), df.residual(m), lower.tail = FALSE)
cbind(ctable, pvals)
</code></pre>

<p><strong>This is the code I used for the t-values:</strong></p>

<pre><code>ctable &lt;- coef(summary(m))
pvals &lt;- 2 * pt(abs(ctable[, ""t value""]), df.residual(m), lower.tail = FALSE)
cbind(ctable, pvals)

                    Value  Std. Error   t value         pvals
(Intercept):1   209.559678 32.50165463  6.447662  3.327237e-10
(Intercept):2    4.184758  0.05227107 80.058783 7.628269e-246    
read             2.697963  0.62024382  4.349843  1.737355e-05   
math             5.914589  0.70502334  8.389210  8.842612e-16
proggeneral     -12.714341 12.36481457 -1.028268  3.044547e-01
progvocational  -46.143265 13.65615960 -3.378934  8.002457e-04
</code></pre>
"
"0.0215465206794286","0.0205325390588295"," 45754","<p>I have the following output from a logistic regression model.</p>

<pre><code>Coefficients:
                             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                -2.6023448  0.0694696 -37.460  &lt; 2e-16 ***
our_bid                     0.0039520  0.0007646   5.169 2.35e-07 ***
our_bid:zipcode10000:14849  0.0019334  0.0009006   2.147 0.031807 *  
our_bid:zipcode14850:19699  0.0022905  0.0009514   2.407 0.016064 *  
our_bid:zipcode19700:29999 -0.0009483  0.0008583  -1.105 0.269231    
our_bid:zipcode30000:31999 -0.0016309  0.0011028  -1.479 0.139161    
our_bid:zipcode32000:34999  0.0016241  0.0007856   2.067 0.038688 *  
our_bid:zipcode35000:42999  0.0023549  0.0008541   2.757 0.005831 ** 
our_bid:zipcode43000:49999  0.0007096  0.0008104   0.876 0.381286    
our_bid:zipcode50000:59999  0.0006533  0.0009269   0.705 0.480942    
our_bid:zipcode60000:69999  0.0030564  0.0008169   3.742 0.000183 ***
our_bid:zipcode7000:9999   -0.0027419  0.0012699  -2.159 0.030847 *  
our_bid:zipcode70000:79999  0.0013243  0.0007809   1.696 0.089921 .  
our_bid:zipcode80000:89999  0.0038726  0.0008006   4.837 1.32e-06 ***
our_bid:zipcode90000:96999  0.0038746  0.0007817   4.957 7.18e-07 ***
our_bid:zipcode97000:99820  0.0009085  0.0010044   0.905 0.365726    
---
</code></pre>

<p>I am using these coefficients to draw the predicted probabilities such that.</p>

<p>$$\text{Prob} = \frac{1}{1 + e^{-z}}$$</p>

<p>where</p>

<p>$$z = B_0 + B_1X_1 + \dots + B_nX_n.$$</p>

<p>I realize that interpreting these interaction terms can be challenging. However, I generate the main regression equation and use that to formulate the probability curve. However, I'm not sure how to make sense of any of the ""our_bid:zipcode"" variables? </p>

<p>What about if my model output was: (instead saving zipcode as a factor, I make it a continuous variable)</p>

<pre><code>Coefficients:
                             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                -2.6023448  0.0694696 -37.460  &lt; 2e-16 ***
our_bid                     0.0039520  0.0007646   5.169 2.35e-07 ***
our_bid:zipcode             0.0019334  0.0009006   2.147 0.031807 *  
</code></pre>

<p>Would interpretation being easier with this approach? Keeping with the log-odds, how can I make sense of the log-odds effect that this model expresses for the interaction term?</p>
"
"0.12583105938145","0.119909435959664"," 46096","<p>I'm working in R, using glm.nb (of the MASS package) to model count data with a negative binomial regression model.  I'd like to compare the relative importance of each of my predictor variables regarding their impact on the response variable (note: the predictors each have quite different scales - sometimes by orders of magnitude).  Unfortunately, the output from R gives me results as unstandardized (<em>b</em>) coefficients (""estimates"").  I'm hoping someone can give me a hint as to how to go about getting standardized (<em>beta</em>) coefficients from the NB regression model... or another 'better' way to determine the relative importance of each of my predictors on my response variable.</p>

<p>I've investigated several potential ways like: </p>

<ol>
<li>using the R package 'relimpo' (as suggested in a comment to <a href=""http://stats.stackexchange.com/a/7118"">http://stats.stackexchange.com/a/7118</a>), but it does not work on a NB regression model, thus completely changing the assumptions I should be accounting for and making the outcomes very different; </li>
<li>mean-centering and scaling my data, which changes the interpretation and makes it so that I can't use NB model due to response variables now having negative values; </li>
<li>scaling-only, so that I can still run a NB model... which I <em>thought</em> would only affect the scale of the coefficients without changing their direction (viz., <a href=""http://stats.stackexchange.com/a/29784"">http://stats.stackexchange.com/a/29784</a> ) - but I do get some positive coefficients that flip to neg. and vice-verse... which seems strange to me and makes me wonder whether I'm making a mistake.</li>
</ol>

<p>I've benefited from looking at <a href=""http://stats.stackexchange.com/q/29781"">When should you center your data &amp; when should you standardize?</a> (and the suggested links from comments on the question such as <a href=""http://andrewgelman.com/2009/07/when_to_standar/"" rel=""nofollow"">http://andrewgelman.com/2009/07/when_to_standar/</a> and <a href=""http://stats.stackexchange.com/q/7112"">When and how to use standardized explanatory variables in linear regression</a> and <a href=""http://stats.stackexchange.com/q/19216"">Variables are often adjusted (e.g. standardised) before making a model - when is this a good idea, and when is it a bad one?</a>).  </p>

<p>Bottom line: I have not yet found a way to use a NB model in R (which I have statistically confirmed is more appropriate than lm, glm, or poisson for modeling my data) and still get at the relative importance - or at least to the standardized beta coefficients - for my predictors...</p>

<p>The R scripts is something like this:</p>

<pre><code>library(""MASS"")
nb = glm.nb(responseCountVar ~ predictor1 + predictor2 + 
  predictor3, data=myData, control=glm.control(maxit=125))
summary(nb)

scaled_nb = glm.nb(scale(responseCountVar, center = FALSE) ~ scale(predictor1, center = FALSE) + scale(predictor2, center = FALSE) + 
  scale(predictor3, center = FALSE), data=myData, control=glm.control(maxit=125))
summary(scaled_nb)
</code></pre>
"
"0.052777981396926","0.0502942438178979"," 46117","<p>I used a stepwise multiple regression to generate a model and I am trying to appropriately report the results by indicating the (additional) variance explained by each included factor. I'm not sure how to extract that information, however.</p>

<p>Basically my code thus far is simply:</p>

<pre><code>init &lt;- lm(dep ~ fac1 + fac2 + fac3 + fac4,data=data)
final &lt;- stepAIC(init, direction=""both"")
</code></pre>

<p>or </p>

<pre><code>final &lt;- step(init, direction=""both"")
</code></pre>

<p>In the case of my data, the final model is <code>final ~ fac1 + fac2 + fac3</code></p>

<p>At this point, I want to know how much variance the most important factor explains, how much additional variance is explained by adding a second factor and so on. I can get the coefficients, but I'm not sure how to extract the R/R^2 for each factor.</p>

<p>Where is that reported?</p>

<p>Thanks in advance.</p>
"
"0.0609427635336005","0.0580747904139041"," 46205","<p>I have a large amount of vegetation data that has been broken down into 13 habitat classes. I am trying to determine which vegetation tends to fall into or is absent from which habitat with any sort of significance. I have been put onto running a multinomial logistic regression, specifically using glmnet (as I have approximately 200 variables, and only about 260 observations).</p>

<p>Running cv.glmnet using the code:</p>

<pre><code>cv&lt;-cv.glmnet(data,Class,family=""multinomial"",nfolds=50,standardize=FALSE)
</code></pre>

<p>I get a list of numbers that I am struggling to understand, however I found the code:</p>

<pre><code>coef(cv, s=cv$lambda.1se)
</code></pre>

<p>Which returns the coefficients for each variable for each habitat class for the lambda that is 1 SE larger than the minimum Lambda value (which as far as I can tell the generally accepted lambda value).</p>

<pre><code>(Intercept)                                              0.7914263664   
Salix                                                    0.0000000000  
Mash                                                     0.0000000000   
Pin                                                      0.0000000000   
Choke                                                    .          
Betula                                                   0.0025260258   
Ideae                                                    0.0000000000   
Leather                                                  0.0000000000
</code></pre>

<p>What I'm wondering, using these coefficients, is it possible to state that those values with the largest magnitude (either closest to -1 and +1) are the most important in defining that class, which those close to 0 are unimportant, and those with periods were removed during the cv.glmnet. So in this case the plant ""Betula"" would be more influential than all others, and ""Choke"" was so uninfluential that it was removed? Also, no idea what intercept means, but I imagine I can find that one on my own.</p>
"
"0.0806196982594614","0.0658506226617377"," 46312","<p>I'm working in R, using glm.nb (of the MASS package) to model count data with a negative binomial regression model.  I'd like to get the standardized (<em>beta</em>) coefficients from the model, but am given the unstandardized (<em>b</em> ""Estimate"") coefficients.</p>

<p>The R documentation does not seem to show of a way to retrieve the standardized beta weights easily for a negative bionomial regression model.</p>

<p>The R script is something like:</p>

<pre><code>library(""MASS"")
nb = glm.nb(responseCountVar ~ predictor1 + predictor2 + 
    predictor3 + predictor4 + predictor5 + predictor6 + 
    predictor7 + predictor8 + predictor9 + predictor10 + 
    predictor11 + predictor12 + predictor13 + predictor14 + 
    predictor15 + predictor16 + predictor17 + predictor18 + 
    predictor19 + predictor20 + predictor21,
    data=myData, control=glm.control(maxit=125))
summary(nb)
</code></pre>

<p>and the output of the above is:</p>

<pre><code>Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-5.1462  -1.0080  -0.4247   0.2277   3.4336  

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -3.059e+00  3.782e-01  -8.088 6.05e-16 ***
predictor1    -2.447e+00  4.930e-01  -4.965 6.88e-07 ***
predictor2    -1.004e+00  1.313e-01  -7.650 2.00e-14 ***
predictor3     1.158e+00  1.440e-01   8.047 8.46e-16 ***
predictor4     1.334e+00  7.034e-02  18.970  &lt; 2e-16 ***
predictor5     9.862e-01  2.006e-01   4.915 8.87e-07 ***
predictor6     1.166e+00  2.378e+00   0.490  0.62392    
predictor7    -1.057e-01  1.494e-01  -0.707  0.47936    
predictor8     4.051e-01  7.318e-02   5.536 3.10e-08 ***
predictor9    -3.320e-01  1.132e-01  -2.933  0.00336 ** 
predictor10    3.761e-01  1.561e-01   2.409  0.01600 *  
predictor11    8.660e-02  4.332e-02   1.999  0.04557 *  
predictor12   -1.583e-01  2.044e-01  -0.774  0.43872    
predictor13    6.404e-02  3.972e-03  16.122  &lt; 2e-16 ***
predictor14    4.264e-03  2.297e-04  18.563  &lt; 2e-16 ***
predictor15    3.279e-03  5.697e-04   5.755 8.68e-09 ***
predictor16    3.487e-03  3.447e-03   1.012  0.31177    
predictor17    1.534e-04  1.647e-04   0.931  0.35182    
predictor18   -7.606e-05  9.021e-05  -0.843  0.39917    
predictor19    2.536e-04  1.733e-05  14.633  &lt; 2e-16 ***
predictor20    2.997e-02  4.977e-03   6.021 1.73e-09 ***
predictor21    2.756e+01  3.508e+00   7.856 3.98e-15 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for Negative Binomial(0.9232) family taken to be 1)

    Null deviance: 5631.1  on 1835  degrees of freedom
Residual deviance: 2120.7  on 1814  degrees of freedom

                                AIC: 19268    
Number of Fisher Scoring iterations: 1    
                              Theta: 0.9232 
                          Std. Err.: 0.0282 
                 2 x log-likelihood: -19221.9910
</code></pre>

<p><strong>My question is</strong>:  Is there a way to get the beta weights, or do I need to try to convert my unstandardized b coefficients to standardized beta coefficients (if so, how would I do that)?</p>
"
"0.052777981396926","0.0502942438178979"," 46434","<p>The <code>summary.rq</code> function from the <a href=""http://cran.r-project.org/web/packages/quantreg/quantreg.pdf"">quantreg vignette</a> provides a multitude of choices for standard error estimates of quantile regression coefficients. What are the special scenarios where each of these becomes optimal/desirable?</p>

<ul>
<li><p>""rank"" which produces confidence intervals for the estimated parameters by inverting a rank test as described in Koenker (1994). The default option assumes that the errors are iid, while the option iid = FALSE implements the proposal of Koenker Machado (1999). See the documentation for rq.fit.br for additional arguments.</p></li>
<li><p>""iid"" which presumes that the errors are iid and computes an estimate of the asymptotic covariance matrix as in KB(1978).</p></li>
<li><p>""nid"" which presumes local (in tau) linearity (in x) of the the conditional quantile functions and computes a Huber sandwich estimate using a local estimate of the sparsity.</p></li>
<li><p>""ker"" which uses a kernel estimate of the sandwich as proposed by Powell(1990).</p></li>
<li><p>""boot"" which implements one of several possible bootstrapping alternatives for estimating standard errors.</p></li>
</ul>

<p>I have read at least 20 empirical papers where this is applied either in the time-series or the cross-sectional dimension and haven't seen a mention of standard error choice. </p>
"
"0.0963589698356145","0.0826418755551823"," 46978","<p>I am fitting a <em>Fixed-Effects</em> model, with intercepts at <code>cluster</code> level.</p>

<p>One of the most direct ways is probably to use the <code>-plm-</code> package. Another well-known possibility is to apply OLS (i.e. to adopt <code>-lm-</code>) to the <em>demeaned data</em>, where the means are taken at the clustering level.</p>

<p>This second approach is usually referred to as the <strong>within transformation</strong>. It is quite convenient from a computational standpoint, because we are still controlling unobserved heterogeneity at clustering level, but we do not need to estimate all the time-fixed intercepts.</p>

<p>I have tried both of these approaches, and I came to a strange result. In practice, the coefficient of the regressor of interest, <code>x</code>, is the same in both cases. However, its standard error (and actually all the other relevant quantities of the regression: R squared, F test, etc.) is different.</p>

<p>Please, notice that I have carefully read both the <em>R documentation</em> about <code>-plm-</code> and the <a href=""http://www.google.it/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;ved=0CD4QFjAB&amp;url=http://www.jstatsoft.org/v27/i02/paper&amp;ei=7f3mUP_0DYrXtAaD7oDADw&amp;usg=AFQjCNFu_xrsnFYsC8j8DDh9mRQnoyQ6jg&amp;bvm=bv.1355534169,d.bGE"" rel=""nofollow"">related paper of the authors</a>, where it is stated that the package apply the <em>within transformation</em> and then apply OLS, as I did...</p>

<p>The R script is:</p>

<pre><code># set seed, load packages, create fake sample

set.seed(999)
library(plyr)
library(plm)

dat &lt;- expand.grid(id=factor(1:3), cluster=factor(1:6))
dat &lt;- cbind(dat, x=runif(18), y=runif(18, 2, 5))


############################
#   FE model using -plm-   #
############################

# model fit  
fe.1 &lt;- plm(y ~ x, data=dat, index=""cluster"", model=""within"")

# estimated coefficient and standard error of x
b.1 &lt;- summary(fe.1)$coefficients[,1]
    se.1 &lt;- summary(fe.1)$coefficients[,2]


######################################
#   OLS on within-transformed data   #
######################################

# augmenting data frame with cluster-mean centered variables 
dat.2 &lt;- ddply(dat, .(cluster), transform, dem_x=x-mean(x), dem_y=y-mean(y))

# model fit
fe.2 &lt;- lm(dem_y ~ dem_x - 1, data=dat.2)

# estimated coefficient and standard error of x
b.2 &lt;- summary(fe.2)$coefficients[1,1]
    se.2 &lt;- summary(fe.2)$coefficients[1,2]


#########################
#   models comparison   #
#########################

b.1; b.2
se.1; se.2

summary(fe.1)
summary(fe.2)
</code></pre>

<p>Notice that in the second model it is necessary to manually eliminate the intercept from the model. </p>
"
"0.13282167379153","0.119909435959664"," 47258","<p>I have following data stored in a file. I am applying 'glm' in R to find linear regression equation to best predict the 'output'. </p>

<pre><code>&gt; tmpData
   logOfOutput randomSample multiplied part1 part2 randNormalMean100Std20 output
1    0.0000000           33         11     1    19               89.65387      1
2    0.6931472           76         24     2    18              128.23471      2
3    1.0986123           12         39     3    17              103.70930      3
4    1.3862944           68         56     4    16               99.12617      4
5    1.6094379           50         75     5    15               95.68173      5
6    1.7917595            7         96     6    14              129.27551      6
7    1.9459101           70        119     7    13              104.59333      7
8    2.0794415           55        144     8    12              102.15247      8
9    2.1972246           20        171     9    11               72.43795      9
10   2.3025851           24        200    10    10               80.63634     10
11   2.3978953           32        231     9    11              105.03423     11
12   2.4849067           97        264     8    12               78.10613     12
13   2.5649494           28        299     7    13              107.95286     13
14   2.6390573           99        336     6    14               80.07396     14
15   2.7080502           66        375     5    15              102.01156     15
16   2.7725887           95        416     4    16              119.07361     16
17   2.8332133           42        459     3    17               64.19354     17
18   2.8903718           53        504     2    18              106.23402     18
19   2.9444390           85        551     1    19              151.07976     19
20   2.9957323           48        600     0    20               82.78324     20
</code></pre>

<p>I am using the following code to perform the same </p>

<pre><code>fn = ""delnowSample.txt""
tmpData = read.table(fn, header = TRUE,  sep= ""\t"" , blank.lines.skip = TRUE)
cnames = colnames(tmpData)
(fmla &lt;- as.formula(paste(cnames[length(cnames)], "" ~ "", paste(cnames[1:(length(cnames)-1)],collapse= ""+"")))  )
model &lt;- try(glm(formula = fmla, family=binomial(), na.action=na.omit, data=tmpData));
summary(model) 
</code></pre>

<p>The output that I get is as follow: </p>

<pre><code>&gt; summary(model)

Call:
glm(formula = as.formula(paste(dep, "" ~ "", paste(xn, collapse = ""+""))), 
    family = gaussian(), na.action = na.omit)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-0.37926  -0.11242  -0.03441   0.16087   0.28200  

Coefficients: (1 not defined because of singularities)
                                            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                                0.2638036  0.3078536   0.857  0.40592    
unlist(tmpData[""logOfOutput""])             0.9202273  0.2727884   3.373  0.00455 ** 
unlist(tmpData[""randomSample""])            0.0026201  0.0018177   1.441  0.17145    
unlist(tmpData[""multiplied""])              0.0288073  0.0012359  23.308 1.34e-12 ***
unlist(tmpData[""part1""])                   0.2106002  0.0403442   5.220  0.00013 ***
unlist(tmpData[""part2""])                          NA         NA      NA       NA    
unlist(tmpData[""randNormalMean100Std20""]) -0.0006214  0.0024922  -0.249  0.80673    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for gaussian family taken to be 0.04403284)

    Null deviance: 665.00000  on 19  degrees of freedom
Residual deviance:   0.61646  on 14  degrees of freedom
AIC: 1.1676

Number of Fisher Scoring iterations: 2
</code></pre>

<p>To a large extent it is predicting the Pr(z) correctly as we can see the probabilities of random variable are not significant. The R-square is also high (1-residual.deviance/null.deviance), close to 1.  </p>

<p>Question 1:
In the above data 'part1+part2' is equal to output variable. Is 'glm' not able to identify such type of relations? </p>

<p>Question 2: 
Why the degree of freedom of  null and residual deviance are different? </p>

<p>Question 3:
I need to convert the output variable into categorical variable (i.e. Everything &lt;=10 is 'no' and more than this is 'yes'). What is the best way to call 'glm', when the response variable is 'categorical'. I tried converting 'no' to '0' and 'yes' to 1, and called glm as follows:</p>

<pre><code> model &lt;- try(glm(formula = as.formula(paste(dep, "" ~ "", paste(xn, collapse= ""+""))), family=binomial(), na.action=na.omit));
</code></pre>

<p>I am getting warning message with this code. Also, I am not sure if this is the correct way to call categorical variable. </p>

<p>Edit:</p>

<p>I have the following categorical data:</p>

<pre><code>&gt; tmpData
           x1  x2 x3  y1
1  0.16294456   1  1  no
2  0.80494934   2  2  no
3  0.28962222   1  3  no
4  0.07177347   2  4  no
5  0.54830544   1  5  no
6  0.67655327   2  6  no
7  0.45189608   1  7  no
8  0.82412502   2  8  no
9  0.09076793   1  9  no
10 0.12221227   2 10  no
11 0.56751754 111 11 yes
12 0.04970992 222 12 yes
13 0.56162037 111 13 yes
14 0.96617891 222 14 yes
15 0.50994534 112 15 yes
16 0.70093692 212 16 yes
17 0.02034940 212 17 yes
18 0.78356903 121 18 yes
19 0.58439662 213 19 yes
20 0.31729282 212 20 yes
</code></pre>

<p>And the following code:</p>

<pre><code>  fn = ""delnowSample.txt""
  tmpData = read.table(fn, header = TRUE,  sep= ""\t"" , blank.lines.skip = TRUE)
  tmpData
  model &lt;- glm(formula = 'y1~x1+x2+x3', family=binomial(), na.action=na.omit, data=tmpData)
  summary(model) 
</code></pre>

<p>This one doesn't seem to be working?? </p>
"
"0.0746393370862076","0.0711268017165705"," 47302","<p>I am currently working on time series modeling, especially on stationarity tests. For this purpose, I am extensively using Pfaff's book ""<a href=""http://www.springer.com/statistics/statistical+theory+and+methods/book/978-0-387-75966-1"" rel=""nofollow"">Analysis of integrated and cointegrated time series with R</a>"" and I have some questions :</p>

<ol>
<li><p>On page 63, there is a nice ordinogram (Figure 3.3) explaining how all the ADF tests are related, and what should be the underlying decision tree. First of all, one needs to estimate the ADF equation with a linear trend and test for $\pi=0$ (this statistic is called <code>tau3</code> in the associated package <a href=""http://cran.r-project.org/web/packages/urca/index.html"" rel=""nofollow"">urca</a>). If we reject the null hypothesis, then there is no unit root. If we cannot reject, we test for $\beta_2=0$ given $\pi=0$ (this statistic is called <code>phi3</code> in <code>urca</code>). If we reject, then Pfaff writes ""test again for a unit root using a standardized normal"" with no further explanation.</p>

<p>Does anyone understand what he is talking about? Does this ""normal test"" appear somewhere in the urca implementation?</p></li>
<li><p>Suppose the test <code>tau3</code> for $\pi=0$ is rejected. Then the conclusion should be that there is no unit root but a trend in the series (the series is trend stationary). I have at disposal  the underlying linear regression result given by <code>ur.df()</code> from the package <code>urca</code>. Is it correct to conclude that there is actually no trend when the p-value of the t-statistic for the trend coefficient is significant? </p></li>
</ol>

<p>Thanks in advance for your help.</p>
"
"0.052777981396926","0.0335294958785986"," 47795","<p>I am currently carrying out an investigation to find if certain factors such as playing home or away or position of a footballer affects overall pass completion using logistic regression. I am using R to compute my data. In my current section in which I am trying to analyse uses the data of every player to convey a general conclusion to whether or not the position of a player affects the successfulness of pass completion. </p>

<p>so far I have computed:</p>

<pre><code>test.logit &lt;- glm( cbind(Total.Successful.Passes.All,Total.Unsuccessful.Passes.All) ~
                   as.factor(Position.Id), data=passes.data, family = ""binomial"")

summary(test.logit)
</code></pre>

<p>and my output was:</p>

<pre><code>Coefficients:
                        Estimate Std. Error z value Pr(&gt;|z|)    

(Intercept)              0.28482    0.01256   22.67   &lt;2e-16 

as.factor(Position.Id)2  0.99768    0.01438   69.38   &lt;2e-16 

as.factor(Position.Id)4  1.06679    0.01398   76.29   &lt;2e-16 

as.factor(Position.Id)6  0.68090    0.01652   41.23   &lt;2e-16 

Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 32638  on 10269  degrees of freedom
Residual deviance: 26499  on 10266  degrees of freedom

AIC: 60422

Number of Fisher Scoring iterations: 4
</code></pre>

<p>the intercept is goalkeepers,position.Id 2 is for a defender, 4 = midfielder and 6 = striker</p>

<p>Is this a good set of results to come to a conclusion? and with the large deviances?</p>
"
"0.0929636479491388","0.104695817324578"," 48410","<p>I'm modeling the effect of a categorical predictor on a binary dependent variable using logistic regression. I'm comparing models with/without the predictor using a likelihood-ratio test.</p>

<p>Two categories of the predictor are associated with values of 1 only (no 0s) for the dependent variable. Regression coefficients for these categories (expressed as changes in log(odds) compared to a reference category) are very large and highly suspicious, as this reference category is always associated with response values of 1 (but for one case), and I would thus expect regression coefficients close to 0 for these two categories. Comparisons between the reference category and other categories having more balanced distribution of 1 and 0s matches what I'm expecting from visual inspection of the data. 
Removing cases associated with these two 'problematic' categories does not change the logLikelihood of the models, but because it changes the number of parameters it affects the results of the likelihood ratio test.</p>

<p>Models are fitted using the glm function with binomial family and logit link in R.</p>

<p>My question therefore is: what model (or procedure) should I use to: </p>

<p>(1) test the global significance of the effect of the predictor on the dependent variable? Should I keep data from the 'problematic' categories in the model or not before conducted the likelihood ratio test?</p>

<p>(2) compare these two 'problematic' categories with others?</p>

<p>Any hint appreciated,</p>
"
"0.0430930413588572","0.0410650781176591"," 48854","<p>I have linear regression code written in R and I have to do the same thing in Java. I used <a href=""http://commons.apache.org/math/apidocs/overview-summary.html"" rel=""nofollow"">Apache Commons math</a> library for this. I used the same data in R code and in Java code, but I got different intercept value. I could not figure out what stupid thing I have done in the code.</p>

<p><strong>R Code:</strong></p>

<pre><code>test_trait &lt;- c( -0.48812477 , 0.33458213, -0.52754476, -0.79863471, -0.68544309, -0.12970239,  0.02355622, -0.31890850,0.34725819 , 0.08108851)
geno_A &lt;- c(1, 0, 1, 2, 0, 0, 1, 0, 1, 0)
geno_B &lt;- c(0, 0, 0, 1, 1, 0, 0, 0, 0, 0) 
fit &lt;- lm(test_trait ~ geno_A*geno_B)
fit
</code></pre>

<p><strong>R Output</strong>:</p>

<pre><code>Call:
lm(formula = test_trait ~ geno_A * geno_B)

Coefficients:
  (Intercept)         geno_A         geno_B  geno_A:geno_B  
    -0.008235      -0.152979      -0.677208       0.096383 
</code></pre>

<p><strong>Java Code (includes EDIT1):</strong></p>

<pre><code>package linearregression;
import org.apache.commons.math3.stat.regression.SimpleRegression;
public class LinearRegression {
    public static void main(String[] args) {

        double[][] x = {{1,0},
                        {0,0},
                        {1,0},
                        {2,1},
                        {0,1},
                        {0,0},
                        {1,0},
                        {0,0},
                        {1,0},
                        {0,0}
        };

        double[]y = { -0.48812477,
                       0.33458213,
                      -0.52754476,
                      -0.79863471,
                      -0.68544309,
                      -0.12970239,
                       0.02355622,
                      -0.31890850,
                       0.34725819,
                       0.08108851
        };
        SimpleRegression regression = new SimpleRegression(true);
        regression.addObservations(x,y);

        System.out.println(""Intercept: \t\t""+regression.getIntercept());
// EDIT 1 -----------------------------------------------------------
System.out.println(""InterceptStdErr: \t""+regression.getInterceptStdErr());
System.out.println(""MeanSquareError: \t""+regression.getMeanSquareError());
System.out.println(""N: \t\t\t""+regression.getN());
System.out.println(""R: \t\t\t""+regression.getR());
System.out.println(""RSquare: \t\t""+regression.getRSquare());
System.out.println(""RegressionSumSquares: \t""+regression.getRegressionSumSquares());
System.out.println(""Significance: \t\t""+regression.getSignificance());
System.out.println(""Slope: \t\t\t""+regression.getSlope());
System.out.println(""SlopeConfidenceInterval: ""+regression.getSlopeConfidenceInterval());
System.out.println(""SlopeStdErr: \t\t""+regression.getSlopeStdErr());
System.out.println(""SumOfCrossProducts: \t""+regression.getSumOfCrossProducts());
System.out.println(""SumSquaredErrors: \t""+regression.getSumSquaredErrors());
System.out.println(""XSumSquares: \t\t""+regression.getXSumSquares());
// EDIT1 ends here --------------------------------------------------

    }
}
</code></pre>

<p><strong>Java Output:</strong></p>

<pre><code>Intercept:      -0.08732359363636362
</code></pre>

<p><strong>Java Output of EDIT1:</strong></p>

<pre><code>Intercept:      -0.08732359363636362
InterceptStdErr:    0.17268454347538026
MeanSquareError:    0.16400973355415271
N:          10
R:          -0.3660108396736771
RSquare:        0.13396393475863017
RegressionSumSquares:   0.20296050132281976
Significance:       0.2982630977579106
Slope:          -0.21477287227272726
SlopeConfidenceInterval: 0.4452137360615129
SlopeStdErr:        0.193067188937234
SumOfCrossProducts:     -0.945000638
SumSquaredErrors:   1.3120778684332217
XSumSquares:        4.4
</code></pre>

<p>I will greatly appreciate your help. Thanks !</p>
"
"0.0812570180448007","0.0774330538852055"," 48922","<p>I am trying to estimate a selection model of the form:</p>

<p>$Z_i = 1[\alpha_0 + \alpha_1X_{1,i} + \alpha_2X_{2,i} + \delta_i$ > 0]</p>

<p>$Y_i = \beta_0 + \beta_1X_{1,i} + Z_i + \epsilon_i$</p>

<p>where $1[]$ denotes the indicator function.</p>

<p>The purpose of the model is to calculate the indirect effect of $X_1$ on $Y$ through $Z$, as well as the the direct effect.</p>

<p>My first question is how to go about estimating this type of model, and how this estimation can be achieved in R. As far as I see it I have a few possible approaches:</p>

<p>(1) Use a standard Heckman selection model, using OLS for both the reduced form and structural equations, using ivreg() in R. This will obviously ignore the constraint that $Z$ is bounded between 0 and 1.</p>

<p>(2) Estimate the first stage with a probit model (i.e. $\delta_i \sim N(0,1)$), and the second stage using standard OLS. I understand that I could do this via manual 2SLS, but as far as I am aware the standard errors will be incorrect? Am I right in that this model is feasible, and if so, can you direct me to a method of achieving this in R?</p>

<p>(3) Build a switching regression model (tobit-5) using the selection() function from sampleSelection package in R. I believe this model will estimate two equations for  $Y$, one for where $Z_i=0$ and one where $Z_i=1$, and with a unique intercept and coefficients for each of the regressors in the outcome equations.</p>

<p>The question then is how to get an estimate of the indirect effect of $X_1$ for each of these methods.</p>

<ul>
<li><p>If I use (1) or (2) then I imagine it might be possible to calculate the average marginal effect of $Z$ on $Y$, and the average marginal effect of $X_1$ on $Z$, then approximate the indirect effect by multiplying the two values?</p></li>
<li><p>If (3) then could I take the fitted value under the estimated model for $Y$ where $Z=0$, and compare the mean to the mean of the fitted values under the estimated model for $Y$ where $Z=1$? This would then give me an estimate of the marginal effect of $Z$? Then use the same method as above and multiple this effect by the marginal effect of $X_1$ on $Z$?</p></li>
</ul>

<p>Many thanks in advance!</p>
"
"0.109866129394437","0.104695817324578"," 49607","<h1>Background</h1>

<h2>Introduction</h2>

<p>I have a data set consisting of data collected from a questionnaire that I wish to validate. I have chosen to use confirmatory factor analysis to analyse this data set.</p>

<h2>Instrument</h2>

<p>The instrument consists of 11 subscales. There is a total of 68 items in the 11 subscales. Each item is scored on an integer scale between 1 to 4.</p>

<h2>Confirmatory factor analysis (CFA) setup</h2>

<p>I use the <code>sem</code> package to conduct the CFA. My code is as below:</p>

<pre><code>cov.mat &lt;- as.matrix(read.table(""http://dl.dropbox.com/u/1445171/cov.mat.csv"", sep = "","", header = TRUE))
rownames(cov.mat) &lt;- colnames(cov.mat)

model &lt;- cfa(file = ""http://dl.dropbox.com/u/1445171/cfa.model.txt"", reference.indicators = FALSE)
cfa.output &lt;- sem(model, cov.mat, N = 900, maxiter = 80000, optimizer = optimizerOptim)

Warning message:
In eval(expr, envir, enclos) : Negative parameter variances.
Model may be underidentified.
</code></pre>

<p>Straight off you might notice a few anomalies, let me explain.</p>

<ul>
<li>Why is the optimizer chosen to be <code>optimizerOptim</code>? </li>
</ul>

<p>ANS: I originally stuck with the default <code>optimizerSem</code> but no matter how many iterations I run, either I run out of memory first (8GB RAM setup) or it would report <code>no convergence</code> Things ""seemed"" a little better when I switched to <code>optimizerOptim</code> where by it would conclude successfully but throws up the error that the model is underidentified. Upon closer inspection, I realise that the output shows <code>convergence</code> as <code>TRUE</code> but <code>iterations</code> is <code>NA</code> so I am not sure what is exactly happening.</p>

<ul>
<li>The <code>maxiter</code> is too high.</li>
</ul>

<p>ANS: If I set it to a lower value, it refuses to converge, although as mentioned above, I doubt real convergence actually occurred.</p>

<h1>Problem</h1>

<p>So by now I guess that the model is really underidentified so I looked for resources to resolve this problem and found:</p>

<ul>
<li><a href=""http://davidakenny.net/cm/identify_formal.htm"" rel=""nofollow"">http://davidakenny.net/cm/identify_formal.htm</a></li>
<li><a href=""http://faculty.ucr.edu/~hanneman/soc203b/lectures/identify.html"" rel=""nofollow"">http://faculty.ucr.edu/~hanneman/soc203b/lectures/identify.html</a></li>
</ul>

<p>I followed the 2nd link quite closely and applied the t-rule:</p>

<ul>
<li>I have 68 observed variables, providing me with 68 variances and 2278 covariances between variables = <strong>2346 data points</strong>.</li>
<li>I also have 68 regression coefficients, 68 error variances of variables, 11 factor variances and 55 factor covariances to estimate making it a total of 191 parameters.</li>
<li>Since I will be fixing the variances of the 11 latent factors to 1 for scaling, I would remove them from the parameters to estimate making it a total of <strong>180 parameters to estimate</strong>.
<ul>
<li>My degrees of freedom is therefore 2346 - 180 = 2166, making it an over identified model by the t-rule.</li>
</ul></li>
</ul>

<h1>Questions</h1>

<ol>
<li>Is the low variance of some of my items a possible cause for the underidentification? I asked a previous question on items with zero variance which led me to think about items which are very close to zero. Should they be removed too? <a href=""http://stats.stackexchange.com/questions/49359/confirmatory-factor-analysis-using-sem-what-do-we-do-with-items-with-zero-varia"">Confirmatory factor analysis using SEM: What do we do with items with zero variance?</a></li>
<li>After reading much, I surmise that the underidentification might be a case of empirical underidentification. Is there a systematic way of diagnosing what kind of underidentification it is? And what are my options to proceed with my analysis?</li>
</ol>

<p>I have more questions but let's take it at these 2 for now. Thanks for any help!</p>
"
"0.0914141453004008","0.0871121856208561"," 49638","<p>I have written an R script for obtaining bootstrapped standard errors in the linear regression setting.</p>

<p>In practice, first in a model building step I select the final model to be applied at each bootstrapped sample (for simplicity suppose that it is a simple univariate linear model).</p>

<p>Then I simulate <code>B = 1000</code> bootstrap samples (with replacement from the initial true dataset). For each simulated sample I fit the initially defined univariate regression model to the simulated data and store the estimated coefficient of the covariate of interest in <code>b_x</code>.</p>

<p>Suppose now for simplicity that <code>B = 10</code>. At the end of the <code>B</code> simulations, I obtain the output matrix <code>boot_out</code> in this form:</p>

<pre><code>boot_out &lt;- read.table(text = ""

iter      b_x
    1       1.19
    2       0.81
    3       1.21
    4       1.05
    5       0.99
    6       1.11
    7       1.09
    8       0.88
    9       0.91
    10      1.12"",

header=TRUE)
</code></pre>

<p>Now I need to compute the bootstrapped standard error of the effect of the regressor of interest, call it <code>se_boot</code>.</p>

<p>To this aim, I used:</p>

<pre><code>se_boot &lt;- sd(boot_out[,""b_x""])
se_boot
</code></pre>

<p>but I get a unexpected result: the bootstrapped standard error of <code>b_x</code> is <strong>higher</strong> than the standard error estimated through the initial model fitted to the true data..</p>

<p>By looking at my <code>b_x</code> values I found that the estimates of the partial effect of interest vary within a wide range, but I guessed that 1000 bootstrap replications were sufficient for refining the estimated standard error of <code>b_x</code>..</p>

<p>Before trying to do even more replications, could you please tell me whether the above passages are right?</p>

<p>Thanks a lot for any help.</p>
"
"0.0304713817668003","0.029037395206952"," 49666","<p>I have a linear regression with two explanatory variables. One of them is a two level categorical variable. When I perform the fit, R tells me that the intercept is highly non significant. When I force the intercept to be zero in the regression, it tells me that the coefficient for the 0-level of the categorical variable is highly non significant. What should I do?</p>
"
"NaN","NaN"," 49939","<p>What is the meaning of <code>t value</code> and <code>Pr(&gt;|t|)</code> when using <code>summary()</code> function on linear regression model in R?</p>

<pre><code>Coefficients:
                              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                    10.1595     1.3603   7.469 1.11e-13 ***
log(var)                        0.3422     0.1597   2.143   0.0322 *
</code></pre>
"
"0.0457070726502004","0.0580747904139041"," 51347","<p>I'd like to perform an exponential regression with multiple independent variables (similar to the LOGEST function in Excel)</p>

<p>I'm trying to model the function $Y = b {m_1}^{x_1}{m_2}^{x_2}$  where $b$ is a constant, $x_1$ and $x_2$ are my independent variables, and $m_1$ and $m_2$ are the coefficients of the independent variables.</p>

<p>I think I can linearize the function by doing something like <code>glm(log(Y) ~ x1 + x2)</code> but I don't totally understand why that would work. Also, I'd like to run a true non-linear regression if there is such a thing.</p>

<p>My goal is to run both a linear and an exponential regression, and find the best fit line based on the higher $R^2$ value. </p>

<p>I would also really appreciate your help in understanding how to plot the predicted curve in a scatter plot of my data as well.</p>
"
"0.0430930413588572","0.0410650781176591"," 51412","<p>I'm performing a multiple logistic regression with a large amount of columns. I want to set all the variables over a certain p value to have 0 coefficients, then test the model on the test data. </p>

<p>Since there's so many variables I'm using, it's impractical to manually go in and set the coefficients. So I'm looking for something like the following.</p>

<pre><code>log_results &lt;- glm(formula, data, family);
log_results_sig &lt;- get_sig_only(log_results, p value threshold);
</code></pre>

<p>This way I can use the results with the predict() function, and export the results easily for use in another program. </p>

<p>Additionally, if anyone knows a way to automatically extract the significant variables and refit with them, I would appreciate that too.</p>

<p>Thanks.</p>
"
"0.068136080998913","0.0649295895722714"," 52132","<p>I am currently working on a regression model where I have only categorical/factor variables as independent variables. My dependent variable is a logit transformed ratio.</p>

<p>It is fairly easy just to run a normal regression in R, as R automatically know how to code dummies as soon as they are of the type ""factor"". However this type of coding also implies that one category from each variable is used as a baseline, making it hard to interpret.</p>

<p>My professor have told me to just use effect coding instead (-1 or 1), as this implies the use of the grand mean for the intercept.</p>

<p>Does anyone know how to handle that?</p>

<p>Until now I have tried:</p>

<pre><code>gm &lt;- mean(tapply(ds$ln.crea, ds$month,  mean))
model &lt;- lm(ln.crea ~ month + month*month + year + year*year, data = ds, contrasts = list(gm = contr.sum))

Call:
lm(formula = ln.crea ~ month + month * month + year + year * 
    year, data = ds, contrasts = list(gm = contr.sum))

Residuals:
     Min       1Q   Median       3Q      Max 
-0.89483 -0.19239 -0.03651  0.14955  0.89671 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -3.244493   0.204502 -15.865   &lt;2e-16 ***
monthFeb    -0.124035   0.144604  -0.858   0.3928    
monthMar    -0.365223   0.144604  -2.526   0.0129 *  
monthApr    -0.240314   0.144604  -1.662   0.0993 .  
monthMay    -0.109138   0.144604  -0.755   0.4520    
monthJun    -0.350185   0.144604  -2.422   0.0170 *  
monthJul     0.050518   0.144604   0.349   0.7275    
monthAug    -0.206436   0.144604  -1.428   0.1562    
monthSep    -0.134197   0.142327  -0.943   0.3478    
monthOct    -0.178182   0.142327  -1.252   0.2132    
monthNov    -0.119126   0.142327  -0.837   0.4044    
monthDec    -0.147681   0.142327  -1.038   0.3017    
year1999     0.482988   0.200196   2.413   0.0174 *  
year2000    -0.018540   0.200196  -0.093   0.9264    
year2001    -0.166511   0.200196  -0.832   0.4073    
year2002    -0.056698   0.200196  -0.283   0.7775    
year2003    -0.173219   0.200196  -0.865   0.3887    
year2004     0.013831   0.200196   0.069   0.9450    
year2005     0.007362   0.200196   0.037   0.9707    
year2006    -0.281472   0.200196  -1.406   0.1625    
year2007    -0.266659   0.200196  -1.332   0.1855    
year2008    -0.248883   0.200196  -1.243   0.2164    
year2009    -0.153083   0.200196  -0.765   0.4461    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.3391 on 113 degrees of freedom
Multiple R-squared: 0.3626, Adjusted R-squared: 0.2385 
F-statistic: 2.922 on 22 and 113 DF,  p-value: 0.0001131 
</code></pre>
"
"0.152356908834001","0.14518697603476"," 52252","<p>I have a regression model that looks like this: $$Y = \beta_0+\beta_1X_1 + \beta_2X_2 + \beta_3X_3 +\beta_{12}X_1X_2+\beta_{13}X_1X_3+\beta_{123}X_1X_2X_3$$</p>

<p>...or in R notation: <code>y ~ x1 + x2 + x3 + x1:x2 + x1:x3 + x1:x2:x3</code></p>

<p>Let's say $X_1$ and $X_2$ are categorical variables and $X_3$ is numeric. The complication is that $X_1$ has three levels $X_{1a}, X_{1b}, X_{1c}$ and instead of standard contrasts, I need to test: </p>

<ul>
<li>Whether the intercept for level $X_{1a}$ significantly differs from the average intercept for levels $X_{1b}$ and $X_{1c}$.</li>
<li>Whether the response of $X_2$ is significantly different between level $X_{1a}$ and the average of levels $X_{1b}$ and $X_{1c}$.</li>
<li>Whether the slope of $X_3$ is significantly different between level $X_{1a}$ and the average of levels $X_{1b}$ and $X_{1c}$.</li>
</ul>

<p>Based on <a href=""http://stats.stackexchange.com/questions/32188/how-to-interpret-these-custom-contrasts"">this post</a> it seems like the matrix I want is...</p>

<pre><code> 2
-1
-1
</code></pre>

<p>So I do <code>contrasts(mydata$x1)&lt;-t(ginv(cbind(2,-1,-1)))</code>. The estimate of $\beta_1$ changes, but so do the others. I can reproduce the new estimate of $beta_1$ by subtracting the predicted values of the $X_1b$ and $X_1c$ group means (when $X_3=0$ and $X_2$ is at its reference level) from twice the value of $X_1a$ at those levels. But I can't trust that I specified my contrast matrix correctly unless I can also similarly derive the other coefficients.</p>

<p>Does anybody have any advice for how to wrap my head around the relationship between cell means and contrasts? Thanks. Is there a standard name for this type of contrast?</p>

<hr>

<p>Aha! According to the <a href=""http://www.ats.ucla.edu/stat/r/library/contrast_coding.htm"">link posted in Glen_b's answer</a>, the bottom line is, you can convert ANY comparison of group means you want into an R-style contrast attribute as follows:</p>

<ol>
<li>Make a square matrix. The rows represent the levels of your factor and the columns represent contrasts. Except the first one, which tells the model what the intercept should represent.</li>
<li>If you want your intercept to be the grand mean, fill the first column with all of the same non-zero value, doesn't matter what. If you want the intercept to be one of the level means, put a number in that row and fill the rest with zeros. If you want the intercept to be a mean of several levels, put numbers in those rows and zeros in the rest. If you want it to be a weighted mean, use different numbers, otherwise use the same number. <em>You can even put in negative values in the intercept column and that probably means something too, but it completely changes the other contrasts, so I have no idea what that's for</em></li>
<li>Fill in the rest of the columns with positive and negative values indicating what levels you want compared to what others. I forget why summing to zero is important, but adjust the values so that the columns do sum to zero.</li>
<li>Transpose the matrix using the <code>t()</code> function.</li>
<li>Use <code>ginv()</code> from the <code>MASS</code> package or <code>solve()</code> to get the inverse of the transposed matrix.</li>
<li>Drop the first column, e.g. <code>mycontrast&lt;-mycontrast[,-1]</code>. You now have a p x p-1 matrix, but the information you put in for your intercept was encoded in the matrix as a whole during step 5.</li>
<li>If you want labels in the summary output more pleasant to read than <code>lm()</code> et al.'s default output, name your matrix's columns accordingly. The intercept will always automatically get named <code>(Intercept)</code> however.</li>
<li>Make your matrix the new contrast for the factor in question, e.g. <code>contrasts(mydata$myfactor)&lt;-mymatrix</code></li>
<li>Run <code>lm()</code> (and probably many other functions that use formulas) as normal in standard R without having to load <code>glht</code>, <code>doBy</code>, or <code>contrasts</code>.</li>
</ol>

<p>Glen_b, thank you, and thank you UCLA Statistical Consulting Group. My applied stats prof spent several days handwaving on this topic, and I was still left with no idea how to actually write my own contrast matrix. And now, an hour of reading and playing with R, and I finally think I get it. Guess I should have applied to UCLA instead. Or University of StackExchange.</p>
"
"0.0430930413588572","0.0205325390588295"," 52256","<p>I'm working with some data and I used R to the a linear regression model <code>Y = aX + b</code>.
The code I used was
<code>summary(lm(Y~X))</code>
What I got was</p>

<pre><code>              Estimate  Std. Error t value Pr(&gt;|t|)    
(Intercept) -0.3884045  0.0260232  -14.93   &lt;2e-16 ***
X            0.0062095  0.0004635   13.40   &lt;2e-16 ***
</code></pre>

<p>What I want to test now is the null hypothesis <code>H0: a=0</code>, that is, the case where the slope is zero.</p>

<p>I'm confused about how to do that. I tried using the offset parameter ( the idea was to subtract the 'a' coefficient found previously in the former fit ), but I'm not sure it is the correct way to test this hypothesis. 
What I did was:</p>

<p><code>summary(lm(Y~X,offset=0.0062095*X)</code></p>

<p>and I got:</p>

<pre><code>             Estimate   Std. Error t value Pr(&gt;|t|)    
(Intercept) -3.884e-01  2.602e-02  -14.93   &lt;2e-16 ***
X           -2.464e-08  4.635e-04    0.00        1  
</code></pre>

<p>Is it right? Am I now supposed to reject H0 since the p-value found was 1?</p>
"
"0.0215465206794286","0.0410650781176591"," 52282","<p>I have a formal model from which I'm deriving some parameters that I would like to estimate. I haven't done this kind of thing before, and I'd like to have some help to solve this issue in R. I observe $z_i$ (money) and four characteristics of population. I want to regress these characteristics on $z_i$. My regression model should be $$z_i = \frac{\beta N}{\gamma} \ \textbf{1}_{\{\text{type 1}\}} + (\tilde{\alpha} + \tilde{\beta}) \frac{\beta N}{\gamma} \ \textbf{1}_{\{\text{type 2}\}} + (1 - \tilde{\beta}) \frac{\beta N}{\gamma} \ \textbf{1}_{\{\text{type 3}\}} + (\tilde{\alpha}) \frac{\beta N}{\gamma}  \ \textbf{1}_{\{\text{type 4}\}} + \epsilon_i$$ where $\epsilon \sim N(\mu, \sigma^2)$</p>

<p>I want the weighting parameters $\tilde{\alpha}$ and $\tilde{\beta}$ to be obtained from a OLS regression, and I want all my coefficients to sum to 1. </p>

<p>Parameters in the equation above are not identified, so I propose to solve this by re-parametrization:$$D_1 = \frac{\beta N}{\gamma} \quad D_2 = (\tilde{\alpha} + \tilde{\beta}) \frac{\beta N}{\gamma} \quad D_3 = (1 - \tilde{\beta}) \frac{\beta N}{\gamma} \quad D_4 = (\tilde{\alpha}) \frac{\beta N}{\gamma}$$</p>

<p>From this I can find $$\frac{D_2}{D_1} = \tilde{\alpha} + \tilde{\beta} \qquad \frac{D_3}{D_1} = 1 - \tilde{\beta} \qquad \frac{D_4}{D_1} = \tilde{\alpha}$$</p>

<p>I want to estimate the equation above in R to get $\tilde{\alpha}$ and $\tilde{\beta}$. How can I do that?</p>

<p>Thanks</p>
"
"0.068136080998913","0.0519436716578171"," 52475","<p>I have been running some binomial logistic regressions in R on a data set and I realised that the p-values of the estimated coefficients are not computed based upon a Normal distribution. For e.g. I have the following result from the glm() function:</p>

<pre><code>    Coefficients:

                                   Estimate Std. Error z value Pr(&gt;|z|) 
    (Intercept)                    -1.6127     0.1124 -14.347  &lt; 2e-16 ***
    relevel(Sex, ""M"")F             -0.3126     0.1410  -2.216   0.0267 *  
    relevel(Feed, ""Bottle"")Both    -0.1725     0.2056  -0.839   0.4013    
    relevel(Feed, ""Bottle"")Breast  -0.6693     0.1530  -4.374 1.22e-05 ***
</code></pre>

<p>I previously thought that the beta hats (estimated coefficients) are asymptotically normal and the p-value should be calculated using the normal distribution. But it seems from the p-values here that a t-distribution is used to compute them (I think). </p>

<p>I know that the asymptotic condition does not hold when sample size is small but if so, what is the process that causes the estimator to be distributed with a Students distribution? And how do I find out the degrees of freedom for such a distribution?</p>
"
"0.0812570180448007","0.0774330538852055"," 53273","<p>I'd like to run a Weibull regression with the pre-defined scale and shape parameters of the Weibull distribution. I'm using <code>survreg()</code> from <code>library(survival)</code> in R and as I understand in the <code>survreg</code> terminology:</p>

<pre><code>scale = 1/(rweibull shape)
intercept = log(rweibull scale)
</code></pre>

<p>Specifying the <code>shape</code> parameter (sic!) by running <code>survreg(Surv(y)~x, scale=1/rweibull_shape)</code> seems to work and, as expected, affects mainly the significance of the regression coefficient for the explanatory variable and not the coefficient itself. </p>

<p>As for the scale parameter I assumed I needed to use <code>offset</code>:</p>

<pre><code>survreg(Surv(y)~x+offset(rep(log(rweibull_scale),length(x))), scale=1/rweibull_shape)
</code></pre>

<p>However, specifying the scale this way doesn't seem to affect the model's log likelihood and the significance of regression coefficients, which is surprising given that Weibull variance depends on both shape and scale. What am I doing wrong?  </p>

<p>Full example: </p>

<pre><code>## offset â€“0.1
summary(survreg(y)~x+offset(rep(-0.1,length(x))), dist=""weibull"", scale=0.143))

Call:
survreg(formula = Surv(y) ~ x + 
offset(rep(-0.1, 4)), dist = ""weibull"", scale = 0.143)
                   Value Std. Error    z        p
(Intercept)        0.690     0.1599 4.31 1.61e-05
          x        0.406     0.0715 5.68 1.31e-08

Scale fixed at 0.143 

Weibull distribution
Loglik(model)= -6.5   Loglik(intercept only)= -15.1
Chisq= 17.22 on 1 degrees of freedom, p= 3.3e-05 
Number of Newton-Raphson Iterations: 7 
n= 4 


## offset â€“10
summary(survreg(y)~x+offset(rep(-10,length(x))), dist=""weibull"", scale=0.143))
Call:
survreg(formula = Surv(y) ~ x + 
offset(rep(-10, 4)), dist = ""weibull"", scale = 0.143)

                   Value Std. Error     z        p
(Intercept)        10.590    0.1599 66.23 0.00e+00
          x        0.406     0.0715 5.68 1.31e-08

Scale fixed at 0.143 

Weibull distribution
Loglik(model)= -6.5   Loglik(intercept only)= -15.1
Chisq= 17.22 on 1 degrees of freedom, p= 3.3e-05 
Number of Newton-Raphson Iterations: 7 
n= 4 
</code></pre>
"
"0.136838784658047","0.142253603433141"," 53375","<p>I'm trying to test whether 4 different slopes from a 3-way interaction in multiple regression are significantly different from zero.  The four lines are plotted at 2 levels of each of the 2 moderators (lo-lo, hi-lo, lo-hi, hi-hi).</p>

<p>Here's how we could test the significance slopes in a 2-way interaction model.  I would like to extend this method to test the significance of slopes in a 3-way interaction model in multiple regression. A 2-way interaction in multiple regression takes the following form:</p>

<pre><code> y = a + b1(X) + b2(Z) + b3(X)(Z)
</code></pre>

<p>With a 2-way interaction, one can examine whether the slopes at various levels of the moderator, Z, are significantly different from zero using the following equations:</p>

<pre><code> b1 at Z = b1 + b3(Z)
</code></pre>

<p>where b1 is the slope of the predicted effects of X on Y at any particular value of Z</p>

<pre><code> SE(b1 at Z) = (var(b1) + (Z^2)(var(b3) + (2Z)(cov(b1,b3))^(1/2)
</code></pre>

<p>where var(b1) is the variance of the b1 regression coefficient, var(b3) is the variance of the b3 regression coefficient, cov(b1,b3) is the covariance between the b1, b3 regression coefficients</p>

<pre><code> t = (b1 at Z)/SE(b1 at Z)
 df = N - k - 1
</code></pre>

<p>where <em>N</em>=sample size and <em>k</em>=number of predictors</p>

<p>One can then test the significance of each slope using a t-test.  My question is, how can I extend this to the significance of slopes in a 3-way interaction model?  See my example R syntax below:</p>

<pre><code>set.seed(123)
predictor &lt;- rnorm(1000, 10, 5)
moderator1 &lt;- rnorm(1000, 100, 25)
moderator2 &lt;- rnorm(1000, 50, 20)
outcome &lt;- predictor*moderator1*moderator2*rnorm(20, 30)/10000
mydata &lt;- data.frame(predictor, moderator1, moderator2, outcome)

model &lt;- lm(outcome ~ predictor + moderator1 + moderator2 + predictor*moderator1 + predictor*moderator2 + moderator1*moderator2 + predictor*moderator1*moderator2, data=mydata)
plotData &lt;- expand.grid(
                    predictor = pretty(qnorm(pnorm(c(-1, 1)), mean = mean(mydata$predictor, na.rm = TRUE), sd = sd(mydata$predictor, na.rm = TRUE))),
                    moderator1 = qnorm(pnorm(c(-1, 1)), mean = mean(mydata$moderator1, na.rm = TRUE), sd = sd(mydata$moderator1, na.rm = TRUE)),
                    moderator2 = qnorm(pnorm(c(-1, 1)), mean = mean(mydata$moderator2, na.rm = TRUE), sd = sd(mydata$moderator2, na.rm = TRUE))
                    )

plotData$outcome &lt;- predict(model, newdata = plotData, level = 0)
    plotData$mod1 &lt;- factor(plotData$moderator1, labels = c(""Lo mod1"", ""Hi mod1""))
    plotData$mod2 &lt;- factor(plotData$moderator2, labels = c(""Lo mod2"", ""Hi mod2""))

mod1Lo_mod2Lo &lt;- plotData[plotData$mod1==""Lo mod1"" &amp; plotData$mod2==""Lo mod2"",]
mod1Hi_mod2Lo &lt;- plotData[plotData$mod1==""Hi mod1"" &amp; plotData$mod2==""Lo mod2"",]
mod1Lo_mod2Hi &lt;- plotData[plotData$mod1==""Lo mod1"" &amp; plotData$mod2==""Hi mod2"",]
mod1Hi_mod2Hi &lt;- plotData[plotData$mod1==""Hi mod1"" &amp; plotData$mod2==""Hi mod2"",]

#Generate Plot
plot(mod1Lo_mod2Lo$predictor, mod1Lo_mod2Lo$outcome, lty=1, lwd=2, type='l', xlab=""predictor"", ylab=""outcome"", ylim=c(min(plotData$outcome), max(plotData$outcome)))
lines(mod1Hi_mod2Lo$predictor, mod1Hi_mod2Lo$outcome, lty=2, lwd=2)
lines(mod1Lo_mod2Hi$predictor, mod1Lo_mod2Hi$outcome, lty=1, lwd=2, col=""gray"")
lines(mod1Hi_mod2Hi$predictor, mod1Hi_mod2Hi$outcome, lty=2, lwd=2, col=""gray"")
legend(""topleft"", legend=c(""lo Mod1, lo Mod2"",""hi Mod1, lo Mod2"",""lo Mod1, hi Mod2"",""hi Mod1, hi Mod2""), lty=c(1,2,1,2), lwd=c(2,2,2,2), col=c(""black"",""black"",""grey"",""grey""))
</code></pre>

<p>How can I test whether the slopes of each of these four lines, separately, is different from zero?</p>

<p>Many thanks in advance!</p>
"
"0.0430930413588572","0.0410650781176591"," 54683","<p>I am struggling with a linear regression model of the shape $y = a + b_1\text{month} + b_2\text{year}$. I have 12 months for each year and 10 years. My dependent variable is a log transformed ratio. I have understood that much that when setting such a model up in R, R automatically picks a level for each variable to go into the intercept.  March and 2005 goes into the intercept, in order to provide a baseline for comparison for the other factors.</p>

<p>As stated my problem is that i cannot really figure out what the intercept represents. Is it simply an average of the ratio of March and 2005 or what is it?</p>

<pre><code>Residuals:                  
Min 1Q  Median  3Q  Max 
-0.90339    -0.16789    -0.00373    0.15472 0.88338 

Coefficients:                   
    Estimate    Std. Error  t   value   Pr(&gt;|t|)
(Intercept) -3.586154   0.131642    -27.242 &lt;2.00E-16   ***
MONTHJan    0.381735    0.140731    2.713   0.007875    **
MONTHFeb    0.256457    0.140731    1.822   0.071426    .
MONTHApr    0.072824    0.140731    0.517   0.605981    
MONTHMay    0.207984    0.140731    1.478   0.142613    
MONTHJun    -0.008194   0.140731    -0.058  0.953686    
MONTHJul    0.363693    0.140731    2.584   0.011217    *
MONTHAug    0.195791    0.140731    1.391   0.16727 
MONTHSep    0.212562    0.140731    1.51    0.134124    
MONTHOct    0.124234    0.140731    0.883   0.379495    
MONTHNov    0.204009    0.140731    1.45    0.15032 
MONTHDec    0.175348    0.140731    1.246   0.215711    
YEAR1999    0.477663    0.128469    3.718   0.000333    ***
YEAR2000    -0.027343   0.128469    -0.213  0.83189 
YEAR2001    -0.166637   0.128469    -1.297  0.197612    
YEAR2002    -0.060508   0.128469    -0.471  0.638684    
YEAR2003    -0.173492   0.128469    -1.35   0.179948    
YEAR2004    0.003592    0.128469    0.028   0.977753    
YEAR2006    -0.283261   0.128469    -2.205  0.029776    *
YEAR2007    -0.267752   0.128469    -2.084  0.03972 *
YEAR2008    -0.240654   0.128469    -1.873  0.063985    .
---                 
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1                  

Residual standard error: 0.3147 on 99 degrees of freedom                    
Multiple R-squared: 0.4167,                 
Adjusted R-squared: 0.2988                  
F-statistic: 3.536 on 20 and 99 DF,  p-value: 1.491e-05 
</code></pre>
"
"0.075412822378","0.0718638867059034"," 56066","<p>I understand that the Wald test for regression coefficients is based on the following property  that holds asymptotically (e.g. Wasserman (2006): <a href=""http://rads.stackoverflow.com/amzn/click/1441923225"">All of Statistics</a>, pages 153, 214-215):
$$
\frac{(\hat{\beta}-\beta_{0})}{\widehat{\operatorname{se}}(\hat{\beta})}\sim \mathcal{N}(0,1)
$$
Where $\hat{\beta}$ denotes the estimated regression coefficient, $\widehat{\operatorname{se}}(\hat{\beta})$ denotes the standard error of the regression coefficient and $\beta_{0}$ is the value of interest ($\beta_{0}$ is usually 0 to test whether the coefficient is significantly different from 0). So the size $\alpha$ Wald test is: reject $H_{0}$ when $|W|&gt; z_{\alpha/2}$ where
$$
W=\frac{\hat{\beta}}{\widehat{\operatorname{se}}(\hat{\beta})}.
$$</p>

<p>But when you perform a linear regression with <code>lm</code> in R, a $t$-value instead of a $z$-value is used to test if the regression coefficients differ significantly from 0 (with <code>summary.lm</code>). Moreover, the output of <code>glm</code> in R sometimes gives $z$- and sometimes $t$-values as test statistics. Apparently, $z$-values are used when the dispersion parameter is assumed to be known and $t$-values are used when the dispersion parameter is esimated (see <a href=""https://stat.ethz.ch/pipermail/r-help/2006-March/100878.html"">this link</a>).</p>

<p>Could someone explain, why a $t$-distribution is sometimes used for a Wald test even though the ratio of the coefficient and its standard error is assumed to be distributed as standard normal?</p>

<p><strong>Edit after the question was answered</strong></p>

<p><a href=""http://stats.stackexchange.com/a/61292/21054"">This post</a> also provides useful information to the question.</p>
"
"0.161335343802939","0.14313992729436"," 56237","<p>I'm a beginner in R and Im wondering how to interprete my results.....
My question is about the results that I got after I did a regression on the Translog production function for panel data:
$ log(y)=log(A) + \alpha_{K} log(K) + \alpha_{L} log(L) + \beta_{KL} log(K)log(L) + \beta_{L^2} log^2(L) + \beta_{K^2} log^2(K)$</p>

<p>L stands for labour and K for Kapital.</p>

<p>The results I got for the Within, Random and first difference a the following:
Within:</p>

<pre><code>  #Within
    Coefficients :
  Estimate  Std. Error  t-value Pr(&gt;|t|)    
     K   1.0902e-05  1.0654e-06  10.2326   &lt;2e-16 ***
     L  -2.4009e-06  1.5086e-07 -15.9150   &lt;2e-16 ***
     LK  1.9788e-03  3.6069e-03   0.5486   0.5833    
     LL  3.0511e-02  1.3141e-03  23.2173   &lt;2e-16 ***
     KK  5.0333e-02  2.6650e-03  18.8868   &lt;2e-16 ***
     ---
     Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

    Total Sum of Squares:    6886.3
        Residual Sum of Squares: 1983.9
  R-Squared      :  0.71191 
  Adj. R-Squared :  0.69692 
    F-statistic: 10729.1 on 5 and 21709 DF, p-value: &lt; 2.22e-16


&gt; #regression random translog
&gt; tl.random&lt;-plm(Y ~ K + L + LK + LL + KK, data=panel, model=""random"")
 &gt; summary(tl.random)
 Oneway (individual) effect Random Effect Model 
(Swamy-Aroras transformation)

 Call:
 plm(formula = Y ~ K + L + LK + LL + KK, data = panel, model = ""random"")

  Balanced Panel: n=462, T=48, N=22176

  Effects:
               var std.dev share
  idiosyncratic 0.09139 0.30230 0.397
   individual    0.13856 0.37224 0.603
  theta:  0.8836  

  Residuals :
Min.  1st Qu.   Median  3rd Qu.     Max. 
 -3.16000 -0.14200  0.00724  0.15400  4.89000 

   Coefficients :
                 Estimate  Std. Error  t-value Pr(&gt;|t|)    
   (Intercept)  1.6266e+00  3.9030e-02  41.6763   &lt;2e-16 ***
    K            9.0932e-06  1.0552e-06   8.6178   &lt;2e-16 ***
    L           -2.5192e-06  1.5023e-07 -16.7684   &lt;2e-16 ***
   LK           2.7566e-03  3.6102e-03   0.7636   0.4451    
   LL           2.9491e-02  1.3138e-03  22.4474   &lt;2e-16 ***
   KK           4.8817e-02  2.6659e-03  18.3117   &lt;2e-16 ***
   ---
  Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

  Total Sum of Squares:    7183.6
  Residual Sum of Squares: 2070.2
  R-Squared      :  0.71181 
  Adj. R-Squared :  0.71162 
  F-statistic: 10951.9 on 5 and 22170 DF, p-value: &lt; 2.22e-16

  &gt; #regression first difference translog
   &gt; tl.fd&lt;-plm(Y ~ K + L + LK + LL + KK-1, data=panel, model=""fd"")
   &gt; summary(tl.fd)
    Oneway (individual) effect First-Difference Model


       #First difference regression
     Call:
      plm(formula = Y ~ K + L + LK + LL + KK - 1, data = panel, model = ""fd"")

      Balanced Panel: n=462, T=48, N=22176

      Residuals :
     Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
    -1.4900 -0.0321  0.0199  0.0202  0.0715  0.9860 

          Coefficients :
          Estimate  Std. Error t-value  Pr(&gt;|t|)    
      K   2.3847e-07  2.8965e-06  0.0823 0.9343856    
      L  -8.0238e-07  2.3128e-07 -3.4693 0.0005229 ***
     LK -2.6986e-02  6.7755e-03 -3.9829 6.831e-05 ***
     LL  5.6920e-02  2.3933e-03 23.7830 &lt; 2.2e-16 ***
     KK  3.7811e-02  5.1254e-03  7.3773 1.674e-13 ***
    ---
       Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

        Total Sum of Squares:    426.54
       Residual Sum of Squares: 269.92
        R-Squared      :  0.38799 
          Adj. R-Squared :  0.3879 
</code></pre>

<p>My question are:</p>

<p>1) Is there a reason why the estimation for coefficient for LK is not significant in both within and random? but in first diff?</p>

<p>2) Why give within and random so similar results, and why first difference is different from them?</p>

<p>3)Can I interpret Standard error and R squared?
Is there anything else I can interpret? Which is the best model of the three?</p>

<p>Thank you so much for your help! </p>
"
"0.0304713817668003","0.029037395206952"," 57012","<p>If I try and fit the linear regression </p>

<pre><code>lm(y~V1+V3,data=x)
</code></pre>

<p>with data:</p>

<pre><code>structure(list(V1 = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L,    
1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L,    
3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L    
), .Label = c(""A"", ""B"", ""C"", ""D""), class = ""factor""), V2 = c(14.95,
    14.95, 14.95, 14.95, 14.95, 14.95, 14.95, 14.95, 14.95, 14.95,
    12.59, 12.59, 12.59, 12.59, 12.59, 12.59, 12.59, 12.59, 12.59,
    12.59, 10.55, 10.55, 10.55, 10.55, 10.55, 10.55, 10.55, 10.55,
    10.55, 10.55, 15.5, 15.5, 15.5, 15.5, 15.5, 15.5, 15.5, 15.5,
    15.5, 15.5), V3 = c(3.33, 3.33, 3.33, 3.33, 3.33, 3.33, 3.33,
    3.33, 3.33, 3.33, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99, 3.99,
    3.99, 3.99, 4.02, 4.02, 4.02, 4.02, 4.02, 4.02, 4.02, 4.02, 4.02,
    4.02, 3.96, 3.96, 3.96, 3.96, 3.96, 3.96, 3.96, 3.96, 3.96, 3.96
    ), y = c(3.87904870689558, 4.53964502103344, 8.11741662829825,
    5.14101678284915, 5.25857547032189, 8.43012997376656, 5.9218324119784,
    2.46987753078693, 3.62629429621295, 4.10867605980008, 4.44816359487892,
    2.71962765411473, 2.8015429011881, 2.22136543189024, 0.88831773049185,
    5.57382627360616, 2.99570095645848, -1.93323431325928, 3.40271180312737,
    1.05441718454413, -3.54258964789476, 3.25620068273364, -3.20803558645792,
    -0.831129834329122, -0.000314142794054927, -8.49354648593931,
    11.7022963559562, 6.22698494269212, -4.10509549609558, 15.0305193685594,
    -1.98486866562679, 3.77692131760739, 3.26675717101425, 13.2397209466905,
    6.33304746822537, 9.02654811195804, 6.77162595721038, 6.65703634166947,
    17.9488182721157, 5.19383211472586)), .Names = c(""V1"", ""V2"",
    ""V3"", ""y""), row.names = c(NA, -40L), class = ""data.frame""
</code></pre>

<p>the coefficient on V3 is NA. I assume this is because somehow V3 is a linear combination of V1.</p>

<p>V1 is a factor that seems to default to dummy coding with the level 'A' set as the reference level. So, how is there a perfect linear combination in V3 and dummy codes for 'B', 'C', and 'D' (as I assume 'A' is the reference level)""?</p>
"
"0.129279124076572","0.116351054666701"," 57031","<p>I have data from a survey experiment in which respondents were randomly assigned to one of four groups:</p>

<pre><code>&gt; summary(df$Group)
       Control     Treatment1     Treatment2     Treatment3 
            59             63             62             66 
</code></pre>

<p>While the three treatment groups do vary slightly in the stimulus applied, the main distinction that I care about is between the control and treatment groups. So I defined a dummy variable <code>Control</code>:</p>

<pre><code>&gt; summary(df$Control)
     TRUE FALSE 
       59   191 
</code></pre>

<p>In the survey, respondents were asked (among other things) to choose which of two things they preferred: </p>

<pre><code>&gt; summary(df$Prefer)
      A   B  NA's 
    152  93   5 
</code></pre>

<p>Then, after receiving some stimulus as determined by their treatment group (and none if they were in the control group), respondents were asked to choose between the same two things:</p>

<pre><code>&gt; summary(df$Choice)
  A    B 
149  101 
</code></pre>

<p>I want to know if the being in one of the three treatment groups had an effect on the choice that respondents made in this last question. My hypothesis is that respondents who received a treatment are more likely to choose <code>A</code> than <code>B</code>.     </p>

<p>Given that I am working with categorical data, I have decided to use a logit regression (feel free to chime in if you think that's incorrect). Since respondents were randomly assigned, I am under the impression that I shouldn't necessarily need to control for other variables (e.g. demographics), so I have left those out for this question. My first model was simply the following:</p>

<pre><code>&gt; x0 &lt;- glm(Product ~ Control + Prefer, data=df, family=binomial(link=""logit""))
&gt; summary(x0)

Call:
glm(formula = Choice ~ Control + Prefer, family = binomial(link = ""logit""), 
    data = df)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.8366  -0.5850  -0.5850   0.7663   1.9235  

Coefficients:
                    Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)           1.4819     0.3829   3.871 0.000109 ***
ControlFALSE         -0.4068     0.3760  -1.082 0.279224    
PreferA              -2.7538     0.3269  -8.424  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 328.95  on 244  degrees of freedom
Residual deviance: 239.69  on 242  degrees of freedom
  (5 observations deleted due to missingness)
AIC: 245.69

Number of Fisher Scoring iterations: 4
</code></pre>

<p>I am under the impression that the intercept being statistically significant is not something that holds interpretable meaning. I thought perhaps that I should include an interaction term as follows:</p>

<pre><code>&gt; x1 &lt;- glm(Choice ~ Control + Prefer + Control:Prefer, data=df, family=binomial(link=""logit""))
&gt; summary(x1)

Call:
glm(formula = Product ~ Control + Prefer + Control:Prefer, family = binomial(link = ""logit""), 
    data = df)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.5211  -0.6424  -0.5003   0.8519   2.0688  

Coefficients:
                                 Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                         3.135      1.021   3.070  0.00214 ** 
ControlFALSE                       -2.309      1.054  -2.190  0.02853 *  
PreferA                            -5.150      1.152  -4.472 7.75e-06 ***
ControlFALSE:PreferA                2.850      1.204   2.367  0.01795 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 328.95  on 244  degrees of freedom
Residual deviance: 231.27  on 241  degrees of freedom
  (5 observations deleted due to missingness)
AIC: 239.27

Number of Fisher Scoring iterations: 5
</code></pre>

<p>Now the respondents status as in a treatment group has the expected effect. Was this a valid set of steps? How can I interpret the interaction term <code>ControlFALSE:PreferA</code>? Are the other coefficients still the log odds?</p>
"
"0.0304713817668003","0"," 57402","<p>I am doing Cox proportional regression. I first did Cox regression with only one variable <code>var</code>; its coefficient 0.8752721 was about what I expected and is easy to interpret in my situation. But when I added some other variables, the coefficient of <code>var</code> in the model became negative (-0.894304). I have performed the stepwise process. The negative coefficient seems wrong and is hard to interpret in my problem.</p>

<p>Where am I wrong? Thanks.</p>
"
"0.052777981396926","0.0502942438178979"," 57448","<p>I am running a model (logistic regression) with 20 independent variables in R. </p>

<p>Before running the model I calculated the correlation between all the variables and finally selected my variables by also checking ""visually"" the histograms of each variable in the case of presence and again in the case of absence. In situations where I don't see any obvious distribution associated to both presence &amp; absence, I discard the variable.</p>

<p>I would like to make ""official"" calculations for the level of relation between Presence/Absence and each variable (how much each variable contributes to the Presence/Absence), for example with <code>Cramer's V index</code>, but the available function I find is from the package <code>vcd</code> and has some limitations: 
doesn't give the <code>Cramer's V</code> (as well as the Phi-Coefficient Contingency Coeff.) for each independent variable, and it doesn't run for one independent variable.</p>

<p>I might be missing some other obvious way to do this. Any help is appreciated.</p>
"
"0.0806196982594614","0.0658506226617377"," 57811","<p>I'm trying to do LASSO in R with the package glmpath. However, I'm not sure if I am using the accompanying prediction function <em>predict.glmpath()</em> correctly. Suppose I fit some regularized binomial regression model like so:</p>

<pre><code>fit &lt;- glmpath(x = data$x, y=data$y, family=binomial)
</code></pre>

<p>Then I can use predict.glmpath() to estimate the value of the response variable $y$ at $x$ for varying values of $\lambda$ through</p>

<pre><code>pred &lt;- predict.glmpath(fit, newx = x, mode=""lambda"", s=seq(0,10,1),type=""response"")
</code></pre>

<p>However, in the help file it can be seen that there is also an option <em>newy</em>. How should one interpret the result when calling <em>predict.glmpath()</em> with <em>newy = some.y</em>? </p>

<p><strong>[Edit]</strong> An additional question came to mind:</p>

<p>The option <em>type</em> can have the following values, according to the help file:</p>

<pre><code>                      description in help file

""response""            the estimated responses are returned
""loglik""              the log-likelihoods are returned
""coefficients""        the coefficients are returned. The coefficients for the initial input variables are returned (rather than the standardized coefficients)
""link""(default)       the linear predictors are returned
</code></pre>

<p>However, to which linear predictors and coefficients are they referring to? Surely not those of the original model?</p>
"
"0.0963589698356145","0.0918243061724248"," 58101","<p>I am doing predictions on monthly temperature data for 100 years, from 1901 to 2000 (i.e 1200 data points). I want to know if the method I follow is correct because in my output, I do not see the requisite ""randomness"" of temperature being reproduced in the prediction.  </p>

<p>Here is a link to the plot of the prediction (in red)<br>
<a href=""https://docs.google.com/file/d/0B1Lm03a_91xiYks5TVJDYU05VUE/edit?usp=sharing"" rel=""nofollow"">https://docs.google.com/file/d/0B1Lm03a_91xiYks5TVJDYU05VUE/edit?usp=sharing</a>  </p>

<p>EDIT: added the ACF and PACF of the detrended and de-seasonalised time series:
<a href=""https://docs.google.com/file/d/0B1Lm03a_91xia2RTOHZrajJtZXM/edit?usp=sharing"" rel=""nofollow"">https://docs.google.com/file/d/0B1Lm03a_91xia2RTOHZrajJtZXM/edit?usp=sharing</a></p>

<p>Below is the dput() of my data:</p>

<pre><code>&gt; dput(fr.monthly.temp.ts)
structure(c(2.7, 0.4, 4.7, 10, 13, 16.9, 19.2, 18.3, 15.7, 10.6,   
4.9, 3.5, 4.1, 3.2, 7.5, 10.3, 10, 15.1, 18.2, 17.4, 15, 10.2, 
6.3, 3.5, 3.8, 5.9, 7.6, 7.1, 12.9, 14.9, 17.6, 17.3, 15.5, 12.1, 
6.9, 2.7, 3, 4.6, 5.5, 10.3, 13.6, 16.3, 20.2, 18.5, 13.9, 11.2, 
5.4, 4.8, 1.7, 4, 7.4, 9.3, 11.9, 16.5, 20, 17.6, 14.7, 8.4, 
5.5, 3.8, 4.3, 3.1, 5.6, 8.5, 12.6, 16.1, 18.2, 18.9, 16, 12.7, 
7.4, 2.3, 2.5, 2.1, 6.3, 8.4, 12.7, 15.1, 16.5, 17.9, 16.2, 11.6, 
7.6, 5.6, 1.7, 4.8, 5, 7.7, 14.2, 16.8, 17.9, 17.1, 14.8, 12.1, 
6.5, 3.6, 2.2, 2, 4.7, 10.4, 12.8, 14.2, 16.3, 18, 14.2, 12.2, 
5, 4.9, 4, 5.4, 6.6, 8.5, 11.9, 16.1, 16.4, 17.3, 14.2, 11.9, 
5.9, 6, 1.6, 4.5, 6.4, 8.3, 13.6, 16.1, 20.8, 20.7, 17.5, 11.3, 
7.3, 6.6, 4.6, 6.8, 8.4, 9.2, 13.8, 15.5, 17.9, 15.5, 12.5, 10, 
5.5, 5.8, 5.4, 4.7, 7.9, 9.1, 13, 15.8, 16.5, 17.6, 15.4, 12.3, 
9.2, 4, 0.7, 6.5, 7.4, 11.2, 12.2, 15.3, 17.3, 18.2, 15.3, 10.6, 
6.3, 5.7, 3.5, 4.3, 5.7, 8.5, 14.2, 17, 17.2, 17.5, 14.7, 9.6, 
4.6, 7, 6.4, 4.8, 5.9, 9.5, 13.8, 14, 17.4, 18.4, 14.5, 11.5, 
7, 4.3, 1.1, 1.4, 4.4, 6.7, 15.1, 17.6, 18.3, 17.2, 16.4, 9.4, 
7.3, 1.4, 3.7, 5.4, 6.5, 8.4, 14.2, 15, 18, 18.1, 15.4, 9.7, 
6.4, 6.9, 3.3, 3.7, 6.2, 7.8, 13.8, 16.3, 15.9, 18.9, 16.2, 8.8, 
4.6, 5.5, 5, 6.4, 8.2, 9.9, 14.4, 16, 17.4, 16.5, 15.2, 11.5, 
6, 4, 6.4, 4.2, 7.2, 8.9, 13.7, 16.9, 20.6, 18, 17, 14.1, 4.7, 
4.5, 3.4, 4.7, 6.6, 8, 14.8, 16.3, 16.7, 16.9, 13.7, 9.2, 5.4, 
4.5, 3.7, 6.3, 7.6, 9.4, 12.2, 14.1, 19.9, 18.8, 15.1, 12.3, 
5.3, 3.8, 3.8, 2.4, 6.4, 9.2, 14.1, 16.2, 18, 15.9, 15.2, 11.7, 
7.1, 4.5, 4.8, 5.6, 4.3, 9.1, 12.9, 17, 18, 17.6, 13.3, 11.8, 
4.9, 3.9, 4.1, 8.3, 7.2, 10.3, 11.6, 14.5, 18.2, 18.7, 17.3, 
11.5, 8.3, 2.5, 4.3, 4.5, 7.7, 9.8, 13.7, 15.7, 18, 17.8, 15.2, 
11.3, 6.7, 2.9, 5, 6.4, 7.1, 9.3, 11.8, 16.1, 20.5, 19.3, 15.8, 
11.5, 8.2, 3.7, 0.3, -0.2, 6.7, 7.8, 13.2, 16.3, 19.1, 18.1, 
18.4, 11.4, 7.3, 6.4, 5.8, 3.3, 7, 9.7, 12.1, 17.7, 17.3, 18.2, 
15.9, 11.9, 8.6, 4.5, 3.7, 3.3, 5.8, 8.8, 13.8, 17.5, 17.7, 17, 
12.8, 10.6, 8.2, 3.2, 4.8, 1.4, 5.5, 8, 12.1, 15.8, 17.4, 20.4, 
17.2, 11, 7.4, 5, 1.8, 4.3, 7.8, 10.1, 13.1, 15.4, 19.5, 20.1, 
16.7, 12, 5.5, 0.3, 3.3, 3.1, 6.3, 10.4, 13.8, 17.2, 20, 17.5, 
17.1, 11.9, 5.8, 7.6, 2.6, 5.1, 6.2, 9.1, 11.6, 17.2, 19.5, 18.1, 
16.1, 10.7, 7, 3.9, 6.5, 4.6, 7.9, 8.3, 13.4, 16.1, 17.2, 18, 
16, 9.1, 6.6, 4.2, 5.3, 6.9, 5.6, 9.9, 14.2, 16.6, 18.6, 19.1, 
15.5, 11.7, 6.3, 3.2, 4.4, 3.9, 8.8, 7.7, 11.7, 16.8, 17.5, 18.2, 
15.6, 11.3, 9.3, 2.5, 5.3, 4.7, 5.4, 10.2, 11.5, 16.4, 17.3, 
18.1, 15.2, 10.3, 8.7, 2.6, -0.9, 4.5, 7.1, 9.6, 13.5, 17.1, 
17.1, 17.5, 15.6, 10.6, 7.6, 1.1, 0.7, 4.5, 7.3, 8.2, 10.3, 16.8, 
19.3, 16.9, 15.5, 10.8, 6.6, 3.7, -0.2, -0.1, 7.7, 10.6, 13.1, 
16.7, 18.1, 18.7, 16.7, 13.2, 5.5, 4.8, 4.8, 5.3, 8, 11.5, 14.2, 
16.4, 19.2, 19.2, 16, 12.4, 5.9, 3.4, 5.1, 2.2, 5.1, 11.1, 13.4, 
16, 18.6, 20.6, 15.2, 10.1, 7.1, 3.4, -1, 7.1, 8.4, 11.9, 14.8, 
17.8, 20, 18.1, 16.7, 12.3, 6.5, 4.8, 1.7, 6.4, 6.7, 11.2, 13.1, 
15.7, 18.9, 17.9, 16.2, 11.3, 7.1, 2.1, 1, 1.3, 7.3, 11.3, 14.8, 
17.9, 20.4, 20.9, 17.6, 12.1, 8.3, 3.8, 5.7, 4.5, 9.5, 10.4, 
14, 15.8, 17, 17.8, 15.5, 11.4, 7.2, 4.6, 4.5, 5.4, 5.7, 11.7, 
12.2, 16.8, 20.6, 19.8, 18.6, 13.4, 6.4, 5.1, 3, 6.4, 8, 8.7, 
14.2, 18.3, 20.2, 18.6, 15.2, 11.4, 7.4, 1.1, 4.6, 4.7, 5.8, 
9.1, 11.8, 16.1, 18.7, 17.5, 16.5, 10.5, 8.7, 4.9, 2.7, 2.8, 
8.1, 11.2, 14.5, 17.9, 20.2, 18.9, 13.1, 10.9, 5.5, 3.5, 1.1, 
3, 7.5, 10.1, 14.8, 15.4, 18, 18.8, 16.2, 12.1, 7, 6.8, 1.7, 
2.3, 7.5, 8.6, 12.6, 16, 16.4, 16.9, 15.5, 12.4, 8, 6.2, 4.4, 
3.6, 4.6, 10.3, 12.5, 16.4, 19.1, 19.2, 15.7, 10.4, 6.7, 6.4, 
4.4, -1.8, 6.7, 8.1, 13.8, 14.4, 17.8, 16.4, 16.4, 10.6, 5.3, 
5.2, 3.1, 6.9, 9.8, 9.6, 11.5, 17, 18.5, 17.6, 15.1, 11.8, 6.8, 
3.6, 3.7, 6.2, 4.9, 7.9, 13.9, 15.6, 17.9, 18.4, 17.3, 11.4, 
6.7, 5.1, 3.4, 4.5, 8.6, 10.2, 13.8, 17, 20.3, 18.9, 17.2, 12.2, 
6.8, 5.7, 3.5, 5, 8, 9.6, 14.5, 17.6, 16.8, 17.3, 14.5, 11.1, 
8.4, 3.5, 3.6, 7.6, 8.3, 11.7, 12.5, 16.6, 17.7, 18, 18.5, 12.3, 
6.4, 4.5, 4.8, 3.7, 3.9, 9.1, 11.5, 15.8, 17.6, 18.6, 15.5, 11.9, 
5.4, 1.3, -1.6, -0.3, 6.5, 9.6, 12.2, 15.8, 18.5, 16.5, 15.2, 
11.5, 9.3, 1.3, 1.5, 5.2, 5.6, 9.6, 14.5, 16.8, 19.6, 18.2, 16.7, 
9.6, 7.2, 3.2, 3.6, 1.7, 6.6, 8.7, 12.7, 16.1, 16.7, 17.1, 13.7, 
12.2, 6.3, 5.7, 2.6, 7.9, 6.2, 10.5, 13.2, 17, 16.8, 17.2, 16.6, 
12.7, 5, 5.3, 3.5, 5.5, 7.7, 8.8, 12.5, 15.6, 19.8, 18.1, 15.3, 
13.2, 7.1, 3, 3.3, 4.3, 6.8, 9.9, 11.8, 15.9, 17.8, 17.2, 15.1, 
13.5, 6.8, 3, 4.8, 2.1, 6.2, 9.2, 13.2, 15, 19.1, 18.1, 15.9, 
13.1, 7.1, 1.4, 4.1, 4.3, 4.4, 7.6, 12.8, 17.6, 17.8, 18.3, 16.6, 
11.3, 8.7, 2.6, 3.1, 4.2, 3.8, 10.5, 13.7, 14.8, 19.7, 18.7, 
15.7, 12.3, 5.8, 4.9, 3.2, 5.5, 7.9, 8.9, 11.7, 14.3, 18, 17.1, 
13.3, 10.9, 7.3, 4.5, 3, 3.4, 6.1, 7.6, 13.5, 17, 18.1, 19.9, 
16.7, 10.6, 6.8, 3.7, 6.2, 5.5, 7.3, 9.4, 12.5, 15.9, 17.7, 18.6, 
14.5, 8.2, 7.4, 6.8, 6.4, 5.5, 5.3, 9, 12.1, 15.9, 19.1, 19.8, 
16.1, 10.4, 6.7, 3.1, 4.1, 4.8, 6, 8.9, 14, 18.8, 20.1, 19, 14.8, 
11.8, 6.6, 3.1, 3.7, 6.6, 8.3, 8.3, 12.1, 14.8, 17.8, 16.9, 14.7, 
12.9, 7, 5.3, 3.3, 4, 7.2, 7.8, 12.3, 15.2, 17.3, 17.2, 15.6, 
11.8, 6.7, 5.1, 1.3, 4, 6.6, 8.2, 12.3, 16.5, 18.5, 17.1, 15.7, 
12.4, 6.7, 5.7, 2.2, 6.3, 6.2, 8.4, 11.9, 15, 16.4, 18.6, 16.5, 
10.8, 5.8, 3.1, 3.3, 2.9, 9.2, 10, 12.6, 16, 17.5, 18.8, 16.2, 
11.2, 7.2, 3.8, 4.6, 5, 6.3, 9.3, 13.4, 17.4, 20.1, 18, 17.4, 
11.4, 8.3, 4.9, 5.5, 2.5, 7, 8.9, 11.5, 17.1, 22.2, 19.3, 16.3, 
11.9, 7.6, 4.5, 4.2, 3.5, 5.2, 9.6, 10.4, 15.8, 18.8, 18.4, 14.7, 
11.9, 9, 4.5, -1, 3.8, 5.2, 9.7, 12.5, 15.3, 19.4, 17.6, 17.3, 
12.3, 4.4, 5.6, 3.9, -0.6, 5.9, 6.9, 13.7, 16.9, 18.7, 17.6, 
14.9, 13.1, 7.9, 5, -0.8, 3.7, 4.8, 10.9, 11.4, 15, 18.6, 18.6, 
17.8, 12.4, 7.1, 5.2, 6.4, 4.9, 6.5, 10.1, 13.8, 16.2, 17.8, 
18.7, 15.7, 12.9, 6.3, 6, 4.2, 5.6, 9.3, 8.2, 15.3, 16.9, 20.2, 
19.5, 16.5, 13.2, 7, 5.6, 4.8, 8.8, 8.7, 8.9, 15.3, 16, 19.7, 
20.4, 15.9, 13.3, 7.2, 3.1, 3.9, 1.9, 9, 8.7, 11.7, 14.9, 19.6, 
20.7, 17.9, 10.9, 6.9, 3.6, 2.8, 4.9, 7.6, 9.5, 15.3, 16.1, 19.1, 
19.9, 15.5, 9.6, 9, 4.8, 5.9, 3.5, 7, 10.4, 14.1, 17.3, 17.8, 
18.7, 14.7, 10.4, 4.8, 6.2, 5.2, 5.1, 9.4, 8.7, 13.6, 17.1, 21.4, 
19.9, 15, 12, 10.2, 6.5, 4.5, 7.5, 6.5, 9.9, 13.6, 16.1, 21.1, 
20.2, 14.5, 14.6, 7.5, 3.8, 5, 2.9, 6, 10, 12.2, 17.5, 18.7, 
18.2, 14.2, 11.9, 6.9, 3.4, 2.3, 6.9, 9.3, 10, 14.2, 16.3, 18.6, 
21, 17, 12.4, 8.4, 5.5, 5, 5.9, 8.1, 9, 14.9, 17, 18.5, 19.4, 
16.1, 11.6, 5.2, 4.5, 5.3, 4.3, 8, 10, 15.2, 16.3, 20.2, 19.4, 
17.9, 12.2, 6.4, 5, 3.7, 6.6, 7.5, 9.9, 15, 17.8, 17.5, 19.6, 
16.9, 12.2, 8.2, 7.1), .Tsp = c(1901, 2000.91666666667, 12), class = ""ts"")  
</code></pre>

<p>I run <code>stl()</code> on it to remove the seasonality:  </p>

<pre><code># calculate and remove the seasonality  
fr.monthly.temp.ts.stl &lt;- stl(fr.monthly.temp.ts, s.window=""periodic"")    # get the    components  
fr.monthly.temp.seas &lt;- fr.monthly.temp.ts.stl$time.series[,""seasonal""]  
#plot(fr.monthly.temp.seas)  

fr.monthly.temp.ts.noseas &lt;- fr.monthly.temp.ts - fr.monthly.temp.seas  
#plot(fr.monthly.temp.ts.noseas)  
</code></pre>

<p>Then remove the trend with a regression:</p>

<pre><code>fr.mtrend.noseas &lt;- lm(fr.monthly.temp.ts.noseas~t)  
summary(fr.mtrend.noseas)  
</code></pre>

<p>and then use the residuals of this model to fit an ARIMA model (after checking the ACF and PACF for which one is appropriate):</p>

<pre><code># create time series of residuals..this is our ""detrended"" series..for now use only linear trend result  
fr.monthly.temp.ts.new &lt;- ts(fr.mtrend.noseas$resid, start=c(1901,1), frequency=12)
#plot.ts(fr.monthly.temp.ts.new, main=""Detrended and de-seasonalized time series"")

# ARIMA 1,1,1  
fit6 &lt;- arima(fr.monthly.temp.ts.new,order=c(1,1,1))  
fit6  
tsdiag(fit6)  
</code></pre>

<p>I then make a prediction on the stationary time series:</p>

<pre><code>#forecast for the stationary TS, for next 50 yrs months  
forecast &lt;- predict(fit6,n.ahead=600)  
</code></pre>

<p>And then add back the trend and seasonality:</p>

<pre><code>t.new &lt;- (n+1):(n+600)  

#initial time series = stationaryTS + seasonality + trend  
fr.monthly.temp.ts.init &lt;- fr.monthly.temp.ts.new + fr.monthly.temp.seas +
                            fr.mtrend.noseas$coefficients[1] + t * fr.mtrend.noseas$coefficients[2]  

#same for the prediction: we need to add seasonality and trend  
pred.Xt &lt;- forecast$pred + fr.monthly.temp.seas[1:(1+50*12 - 1)] + 
                                fr.mtrend.noseas$coefficients[1] + t.new * fr.mtrend.noseas$coefficients[2]  

plot(fr.monthly.temp.ts.init,type=""l"",xlim=c(1940,2060))  
lines(pred.Xt,col=""red"",lwd=2)  
</code></pre>

<p>So going back to my question: Do I need to add some white noise to the prediction to be able to realistically predict temperature? And more generally, is my method correct?</p>
"
"0.091874672876503","0.0963061447907242"," 58446","<p>I'm trying to compare a few regression models for my data. For linear regression everything is quite understandable, but robust and quantile regressions are not so obvious. I could not find almost anything about calculating confidence interval for these regression models unless I looked for something wrong.</p>

<p>This is my code in R</p>

<pre><code># Robust linear modeling
library(MASS)
library(robustbase)
library(robust)
set.seed(343); 
x &lt;- rnorm(1000)
y &lt;- x + 2*rnorm(1000)

lm1&lt;-lm(y~x); rlm1&lt;-rlm(y~x); rlm2 &lt;- lmRob(y~x); rlm3 &lt;- lmrob(y~x)
cbind(summary(lm1)$coeff,  confint(lm1)) 
cbind(summary(rlm1)$coeff, confint(rlm1))
cbind(summary(rlm2)$coeff, confint(rlm2))
cbind(summary(rlm3)$coeff, confint(rlm3))
</code></pre>

<p>This code produces the following result:</p>

<pre><code>&gt; cbind(summary(lm1)$coeff,  confint(lm1))
                   Estimate Std. Error   t value     Pr(&gt;|t|)      2.5 %     97.5 %
(Intercept) -0.06973191 0.06408983 -1.088034 2.768429e-01 -0.1954982 0.05603438
x            0.97647196 0.06619635 14.751145 1.071805e-44  0.8465720 1.10637196
&gt; cbind(summary(rlm1)$coeff, confint(rlm1))
                  Value Std. Error    t value 2.5 % 97.5 %
(Intercept) -0.06131788 0.06714405 -0.9132288    NA     NA
x            0.96016596 0.06935096 13.8450275    NA     NA
&gt; cbind(summary(rlm2)$coeff, confint(rlm2))
    Error in UseMethod(""vcov"") : 
      no applicable method for 'vcov' applied to an object of class ""lmRob""
&gt; cbind(summary(rlm3)$coeff, confint(rlm3))
              Estimate Std. Error    t value     Pr(&gt;|t|)      2.5 %     97.5 %
(Intercept) -0.0568964 0.06608987 -0.8608945 3.895029e-01 -0.1865874 0.07279464
x            0.9612520 0.06821558 14.0913850 2.921913e-41  0.8273896 1.09511448
</code></pre>

<p>It's easy to spot that linear model works OK and only one robust regression gives a sensible result. Another observation is that lmrob(), which produces some actual confidence interval, calculates it in the same manner as lm(), with using 1.96 as the student coefficient.</p>

<p>Is it a correct way to produce a confidence interval for the robust regression model? May the same method be used for the quantile regression model?</p>
"
"0.0575854987567582","0.0548755188847815"," 58811","<p>1. Which one is NOT a linear regression models? Please give a 1-2 sentences brief
explanation to your choice.<br>
(a)  $y_i = Î²_0 +\exp(Î²_1x_i)+E_i, i = 1, 2, \ldots, n$<br>
(b)  $y_i = Î²_0 + Î²_1x_i + Î²_2 x_{ii} + E_i , i = 1, 2, \ldots, n$<br>
(c)  $y_i =Î²_0\exp(x_i)+Î²_2x_i^7 +E_i, i=1, 2,\ldots, n$  </p>

<p>2. Suppose $X$ and $Y$ has linear correlation coefficient $r = 0.5$, and there are 77 observations, what is the test statistic for the hypothesis test  </p>

<p>$$H0:Î²_1=0 \quad\text{vs.}\quad Ha:Î²_1\neq0 $$</p>

<p>where $Î²_1$ comes from the simple linear regression model below? Please give a 1-2
sentences brief explanation to your choice.   $\quad Y = Î²_0 + Î²_1X + E$  </p>

<p>(a). Not enough information<br>
(b). 5<br>
(c). 0.25  </p>

<p>3. Which model is more possible to have smaller $R^2$? Please give a 1-2 sentences brief explanation to your choice.<br>
A: $Y=Î²_0+Î²_1X_1+E$<br>
B: $Y=Î²_0^*+Î²_1^*X_1+Î²_2^*X_2+E^*$<br>
where $Y$ and $X_1$ in model A and B are the same.</p>

<p>(a). Not enough information<br>
(b). Model A<br>
(c). Model B  </p>
"
"0.0990319907421009","0.101630883224332"," 59952","<p>In the ""Dynamic Linear Models with R"" book, the Regression models section reads: ""The static regression linear model corresponds to the case where $W_t = 0$ for any $t$, so that $\theta_t = \theta$ is constant over time.""  </p>

<p>I am not understanding what this means, because when I fit a <code>dlmModReg</code> with <code>dW = 0</code> (default values) and then plot the predicted values of the state vectors, my regression coefficients vary over time, and eventually stabilize to a value similar to that of standard least-squares regression.</p>

<p>Plot of regression slope coefficient with <code>dW = 0</code>: </p>

<p><img src=""http://i.stack.imgur.com/I5MLB.jpg"" alt=""dW = 0""></p>

<p>However, if I use <code>dlmMLE</code> to find the MLE of <code>dW</code> before fitting the model, plotting the predicted values of the state vectors results in non-sensible values with multiple large discontinuities.</p>

<p>Plot of regression slope coefficient with <code>dW</code> MLE and no intercept: </p>

<p><img src=""http://i.stack.imgur.com/VzdIB.jpg"" alt=""dW != 0 no intercept""></p>

<p>Plot of regression slope with <code>dW</code> MLE with intercept:
<img src=""http://i.stack.imgur.com/2Yu7P.jpg"" alt=""dw != 0 with intercept""></p>

<p>I've yet to find any literature on how the intercept coefficient gets set, but I'm observing a near perfect linear relationship between the intercept and slope coefficient.  The inclusion of an intercept term in the parameter MLE also causes my <code>dV</code> to drop from 16 to 0.003.  Can anyone point me to any references which discuss how the intercept term is set on each update?</p>

<p>Plot of intercept and slope with <code>dW</code> MLE:
<img src=""http://i.stack.imgur.com/TjvTW.jpg"" alt=""intercept and slope""></p>

<p><strong>My questions are:</strong></p>

<ol>
<li>Why are the regression coefficients dynamic/time-varying even with <code>dW = 0</code> in the first example?  If the regression coefficient changes at every time step regardless of whether <code>dW</code> is 0 or not due to the measurement update stage $\beta_t = \hat{\beta_t} + K_t(y_t - \alpha_t - \hat{\beta_t}x_t)$ ($K$ is the Kalman gain), what's the difference between simple linear regression and dynamic linear regression except for some random variation added in the time update equation $\hat{P_t} = P_{t-1} + dW$ ($P$ is estimate error covariance)?</li>
<li>Why does the plot of regression coefficients have so many discontinuities when my <code>dW != 0</code>?</li>
<li>What is the relationship between the intercept and the slope coefficients?  Plotting them reveals a near-perfect linear relationship, but I can't find any literature explaining this.  I haven't found the intercept term included in any formulation of the Kalman update equations.</li>
</ol>

<p>Would anyone be willing to take a look at my data/code?</p>
"
"0.0967596325610309","0.0922061136661462"," 60476","<p>I've run a regression on U.S. counties, and am checking for collinearity in my 'independent' variables.  Belsley, Kuh, and Welsch's <em>Regression Diagnostics</em> suggests looking at the Condition Index and Variance Decomposition Proportions:</p>

<pre><code>library(perturb)
## colldiag(, scale=TRUE) for model with interaction
Condition
Index   Variance Decomposition Proportions
           (Intercept) inc09_10k unins09 sqmi_log pop10_perSqmi_log phys_per100k nppa_per100k black10_pct hisp10_pct elderly09_pct inc09_10k:unins09
1    1.000 0.000       0.000     0.000   0.000    0.001             0.002        0.003        0.002       0.002      0.001         0.000            
2    3.130 0.000       0.000     0.000   0.000    0.002             0.053        0.011        0.148       0.231      0.000         0.000            
3    3.305 0.000       0.000     0.000   0.000    0.000             0.095        0.072        0.351       0.003      0.000         0.000            
4    3.839 0.000       0.000     0.000   0.001    0.000             0.143        0.002        0.105       0.280      0.009         0.000            
5    5.547 0.000       0.002     0.000   0.000    0.050             0.093        0.592        0.084       0.005      0.002         0.000            
6    7.981 0.000       0.005     0.006   0.001    0.150             0.560        0.256        0.002       0.040      0.026         0.001            
7   11.170 0.000       0.009     0.003   0.000    0.046             0.000        0.018        0.003       0.250      0.272         0.035            
8   12.766 0.000       0.050     0.029   0.015    0.309             0.023        0.043        0.220       0.094      0.005         0.002            
9   18.800 0.009       0.017     0.003   0.209    0.001             0.002        0.001        0.047       0.006      0.430         0.041            
10  40.827 0.134       0.159     0.163   0.555    0.283             0.015        0.001        0.035       0.008      0.186         0.238            
11  76.709 0.855       0.759     0.796   0.219    0.157             0.013        0.002        0.004       0.080      0.069         0.683            

## colldiag(, scale=TRUE) for model without interaction
Condition
Index   Variance Decomposition Proportions
           (Intercept) inc09_10k unins09 sqmi_log pop10_perSqmi_log phys_per100k nppa_per100k black10_pct hisp10_pct elderly09_pct
1    1.000 0.000       0.001     0.001   0.000    0.001             0.003        0.004        0.003       0.003      0.001        
2    2.988 0.000       0.000     0.001   0.000    0.002             0.030        0.003        0.216       0.253      0.000        
3    3.128 0.000       0.000     0.002   0.000    0.000             0.112        0.076        0.294       0.027      0.000        
4    3.630 0.000       0.002     0.001   0.001    0.000             0.160        0.003        0.105       0.248      0.009        
5    5.234 0.000       0.008     0.002   0.000    0.053             0.087        0.594        0.086       0.004      0.001        
6    7.556 0.000       0.024     0.039   0.001    0.143             0.557        0.275        0.002       0.025      0.035        
7   11.898 0.000       0.278     0.080   0.017    0.371             0.026        0.023        0.147       0.005      0.038        
8   13.242 0.000       0.001     0.343   0.006    0.000             0.000        0.017        0.129       0.328      0.553        
9   21.558 0.010       0.540     0.332   0.355    0.037             0.000        0.003        0.003       0.020      0.083        
10  50.506 0.989       0.148     0.199   0.620    0.393             0.026        0.004        0.016       0.087      0.279        
</code></pre>

<p><code>?HH::vif</code> suggests that VIFs >5 are problematic:</p>

<pre><code>library(HH)
## vif() for model with interaction
        inc09_10k           unins09          sqmi_log pop10_perSqmi_log      phys_per100k      nppa_per100k       black10_pct        hisp10_pct 
         8.378646         16.329881          1.653584          2.744314          1.885095          1.471123          1.436229          1.789454 
    elderly09_pct inc09_10k:unins09 
         1.547234         11.590162 

## vif() for model without interaction
        inc09_10k           unins09          sqmi_log pop10_perSqmi_log      phys_per100k      nppa_per100k       black10_pct        hisp10_pct 
         1.859426          2.378138          1.628817          2.716702          1.882828          1.471102          1.404482          1.772352 
    elderly09_pct 
         1.545867 
</code></pre>

<p>Whereas John Fox's <em>Regression Diagnostics</em> suggests looking at the square root of the VIF:</p>

<pre><code>library(car)
## sqrt(vif) for model with interaction
        inc09_10k           unins09          sqmi_log pop10_perSqmi_log      phys_per100k      nppa_per100k       black10_pct        hisp10_pct 
         2.894589          4.041025          1.285917          1.656597          1.372987          1.212898          1.198428          1.337705 
    elderly09_pct inc09_10k:unins09 
         1.243879          3.404433 
## sqrt(vif) for model without interaction
        inc09_10k           unins09          sqmi_log pop10_perSqmi_log      phys_per100k      nppa_per100k       black10_pct        hisp10_pct 
         1.363608          1.542121          1.276251          1.648242          1.372162          1.212890          1.185108          1.331297 
    elderly09_pct 
         1.243329 
</code></pre>

<p>In the first two cases (where a clear cutoff is suggested), the model is problematic only when the interaction term is included.</p>

<p>The model with the interaction term has until this point been my preferred specification.</p>

<p>I have two questions given this quirk of the data:</p>

<ol>
<li>Does an interaction term always worsen the collinearity of the data?</li>
<li>Since the two variables without the interaction term are not above the threshold, am I ok using the model with the interaction term.  Specifically, the reason I think this might be ok is that I'm using the King, Tomz, and Wittenberg (2000) method to interpret the coefficients (negative binomial model), where I generally hold the other coefficients at the mean, and then interpret what happens to predictions of my dependent variable when I move <code>inc09_10k</code> and <code>unins09</code> around independently and jointly.</li>
</ol>
"
"0.0967596325610309","0.0922061136661462"," 60648","<p>I am trying to forecast electricity consumption in GWh for 2 years ahead (from June 2013 ahead), using R (the forecast package). For that purpose, I tried regression with ARIMA errors. I fitted the model using the <code>auto.arima</code> function, and I used the following variables in the <code>xreg</code> argument in the <code>forecast.Arima</code> function: </p>

<p>- Heating and Cooling Degree Days,<br>
- Dummies for all 12 months and<br>
- Moving holidays dummies (Easter and Ramadan)  </p>

<p>I have several questions regarding the model:</p>

<p>1) Is it correct to use all 12 dummies for monthly seasonality, since when I tried to include 11, the function returned error. The <code>Auto.arima</code> function returned the model ARIMA(0,1,2)</p>

<p>2)The model returned the following coefficients (I won't specify all of them as there are too many coefficients):</p>

<pre><code>ma1      ma2     HDD     CDD   January  February  March     April
-0.52 -0.16      0.27    0.12  525.84   475.13    472.57    399.01
</code></pre>

<p>I am trying to determine the influence of the temperature component over electricity load. In percentages, (interpreting the coefficients just as with the usual regression) the temperature components (<code>HDD</code>+<code>CDD</code>) account for 11,3% of the electricity consumption. Isn't this too little, considering the fact that the electricity consumption is mostly influenced by the weather component? On the other hand, taking look at the dummies' coefficients, it turns out that the seasonality accounts for the greater part of the load. Why is this? Is the model completely incorrect?</p>

<p>I tried linear regression, and the temperature component accounts for 20%, but it is still a low percentage. Why is this?</p>

<p>3) I am obviously making some mistakes in the use of <code>forecast.Arima</code> or the plot function parameters since when I plot the forecasts, I get a picture of the original time series which is continued (merged) with the forecasts for the whole time series period (from 2004 until 2015). I don't know how to explain this better, I tried to paste the picture, but it seems I cannot paste pictures here.</p>
"
"0.0914141453004008","0.0871121856208561"," 60760","<p>let <code>m</code> be my matrix of data</p>

<pre><code>      x_i y_i
 [1,] 0.0   0
 [2,] 0.0   0
 [3,] 0.0   0
 [4,] 0.0   0
 [5,] 0.1   0
 [6,] 0.2   0
 [7,] 0.3   0
 [8,] 0.4   0
 [9,] 0.5   0
[10,] 0.6   0
[11,] 0.0   1
[12,] 0.0   1
[13,] 0.0   1
[14,] 0.9   1
[15,] 1.0   1
</code></pre>

<p>My aim is to study the logistic regression <code>y~x</code>, where the covariate <code>x</code> has observations <code>m[,1]</code> and similarly for <code>y</code>.
Please note that we have no complete separation in the data <em>but</em> the ""anomalous"" entries in rows <code>m[11,], m[12,]</code> and <code>m[13,]</code> all correspond to observations with <code>x_i=0</code>.</p>

<p>I expect <code>glm</code> to diverge as the likelihood function reaches no maximum in the ray  $k\beta$, for $k\rightarrow \infty$ and $\beta=(-0.7,1)$. </p>

<p>Using <code>glm</code> with 1 iteration I get the output </p>

<pre><code>  Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  -1.2552     0.7648  -1.641    0.101
x             1.6671     1.7961   0.928    0.353

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 19.095  on 14  degrees of freedom
Residual deviance: 18.275  on 13  degrees of freedom
AIC: 22.275

Number of Fisher Scoring iterations: 1
</code></pre>

<p>with an error message (the algorithm does not converge). 
Moreover, with the default number of iterations <code>(=25)</code> the output is</p>

<pre><code>Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  -1.1257     0.7552  -1.491    0.136
x             1.4990     1.6486   0.909    0.363

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 19.095  on 14  degrees of freedom
Residual deviance: 18.246  on 13  degrees of freedom
AIC: 22.246

Number of Fisher Scoring iterations: 4
</code></pre>

<p>and no error warning. </p>

<p>I see a contradiction; even in presence of 1 iteration the algorithm does not converge but the output is ""finite"" (I have not explicitly computed the inverse of the Hessian of the likelihood function, unfortunately). Moreover, with 25 iterations the warning message disappears and the output is still finite.</p>

<p>What do you think about this situation?
 Is it possible that <code>glm</code> stops automatically after the first iteration?
Thank you, Avitus</p>
"
"0.068136080998913","0.0649295895722714"," 60872","<p>I have just read a paper in which the authors carried out a multiple regression with two predictors. The overall r-squared value was 0.65. They provided a table which split the r-squared between the two predictors. The table looked like this:</p>

<pre><code>            rsquared beta    df pvalue
whole model     0.65   NA  2, 9  0.008
predictor 1     0.38 1.01 1, 10  0.002
predictor 2     0.27 0.65 1, 10  0.030
</code></pre>

<p>In this model, ran in <code>R</code> using the <code>mtcars</code> dataset, the overall r-squared value is 0.76.</p>

<pre><code>summary(lm(mpg ~ drat + wt, mtcars))

Call:
lm(formula = mpg ~ drat + wt, data = mtcars)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.4159 -2.0452  0.0136  1.7704  6.7466 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   30.290      7.318   4.139 0.000274 ***
drat           1.442      1.459   0.989 0.330854    
wt            -4.783      0.797  -6.001 1.59e-06 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 3.047 on 29 degrees of freedom
Multiple R-squared:  0.7609,    Adjusted R-squared:  0.7444 
F-statistic: 46.14 on 2 and 29 DF,  p-value: 9.761e-10
</code></pre>

<p>How can I split the r-squared value between the two predictor variables?</p>
"
"0.102279800236246","0.104963924850184"," 60952","<p>I would like to compare models selected with ridge, lasso and elastic net. Fig. below shows coefficients paths using all 3 methods: ridge (Fig A, alpha=0), lasso (Fig B; alpha=1) and elastic net (Fig C; alpha=0.5). The optimal solution depends on the selected value of lambda, which is chosen based on cross validation.</p>

<p><img src=""http://i.stack.imgur.com/e8dUs.jpg"" alt=""Profiles of coefficients for ridge (A, alpha=0), lasso (B, alpha=1) and elastic net (C, alpha=0.5) regression. Numbers at the top of the plot represent the size of the models.The optimal solution depends on the selected value of lambda. Selection of lambda is based on cross validation. ""></p>

<p>When looking at these plots, I would expect the elastic net (Fig C) to exhibit a grouping effect. However it is not clear in the presented case. The coefficients path for lasso and elastic net are very similar. What could be the reason for this ? Is it just a coding mistake ? I used the following code in R: </p>

<pre><code>library(glmnet)
X&lt;- as.matrix(mydata[,2:22])
Y&lt;- mydata[,23]
par(mfrow=c(1,3))
ans1&lt;-cv.glmnet(X, Y, alpha=0) # ridge
plot(ans1$glmnet.fit, ""lambda"", label=FALSE)
    text (6, 0.4, ""A"", cex=1.8, font=1)
    ans2&lt;-cv.glmnet(X, Y, alpha=1) # lasso
    plot(ans2$glmnet.fit, ""lambda"", label=FALSE)
text (-0.8, 0.48, ""B"", cex=1.8, font=1)
ans3&lt;-cv.glmnet(X, Y, alpha=0.5) # elastic net 
plot(ans3$glmnet.fit, ""lambda"", label=FALSE)
text (0, 0.62, ""C"", cex=1.8, font=1)
</code></pre>

<p>The code used to plot elastic net coefficients paths is exactly the same as for ridge and lasso. The only difference is in the value of alpha. 
Alpha parameter for elastic net regression was selected based on the lowest MSE (mean squared error) for corresponding lambda values. </p>

<p>Thank you for your help !</p>
"
"0.052777981396926","0.0502942438178979"," 60958","<p>it happened to me that in a logistic regression in R with <code>glm</code> the Fisher scoring iterations in the output are less than the iterations selected with the argument <code>control=glm.control(maxit=25)</code> in <code>glm</code> itself.</p>

<p>I see this as the effect of divergence in the iteratively reweighted least
squares algorithm behind <code>glm</code>. </p>

<p>My question is: under which criteria does <code>glm</code> stop the iterations and provides with a partial output? I was thinking about something like ""when the new coefficients-old coefficients &lt; epsilon, then STOP"". Is this the case? If not, what does make <code>glm</code> stop?
Thanks,
Avitus</p>
"
"0.0304713817668003","0.029037395206952"," 61747","<p>I have a set of values $x$ and $y$ which are theoretically related exponentially:</p>

<p>$y = ax^b$</p>

<p>One way to obtain the coefficients is by applying natural logarithms in both sides and fitting a linear model:</p>

<pre><code>&gt; fit &lt;- lm(log(y)~log(x))
&gt; a &lt;- exp(fit$coefficients[1])
&gt; b &lt;- fit$coefficients[2]
</code></pre>

<p>Another way to obtain this is using a nonlinear regression, given a theoretical set of start values:</p>

<pre><code>&gt; fit &lt;- nls(y~a*x^b, start=c(a=50, b=1.3))
</code></pre>

<p>My tests show better and more theory-related results if I apply the second algorithm. However, I would like to know the statistical meaning and implications of each method.</p>

<p>Which of them is better?</p>
"
"0.0621994475718397","0.0711268017165705"," 61805","<p><strong>The situation</strong></p>

<p>I have a dataset with one dependent $y$ and one independent variable $x$. I want to fit a continuous piecewise linear regression with $k$ known/fixed breakpoints occurring at $(a_{1}, a_{2}, \ldots, a_{k})$. The breakpoins are known without uncertainty, so I don't want to estimate them. Then I fit a regression (OLS) of the form
$$
y_{i} = \beta_{0} + \beta_{1}x_{i} + \beta_{2}\operatorname{max}(x_{i}-a_{1},0) +  \beta_{3}\operatorname{max}(x_{i}-a_{2},0) +\ldots+ \beta_{k+1}\operatorname{max}(x_{i}-a_{k},0) +\epsilon_{i}
$$
Here is an example in <code>R</code></p>

<pre><code>set.seed(123)
x &lt;- c(1:10, 13:22)
y &lt;- numeric(20)
y[1:10] &lt;- 20:11 + rnorm(10, 0, 1.5)
y[11:20] &lt;- seq(11, 15, len=10) + rnorm(10, 0, 2)
</code></pre>

<p>Let's assume that the breakpoint $k_1$ occurs at $9.6$:</p>

<pre><code>mod &lt;- lm(y~x+I(pmax(x-9.6, 0)))
summary(mod)

Coefficients:
                    Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)          21.7057     1.1726  18.511 1.06e-12 ***
x                    -1.1003     0.1788  -6.155 1.06e-05 ***
I(pmax(x - 9.6, 0))   1.3760     0.2688   5.120 8.54e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>The intercept and slope of the two segments are: $21.7$ and $-1.1$ for the first and $8.5$ and $0.27$ for the second, respectively.</p>

<p><img src=""http://i.stack.imgur.com/2GsoE.png"" alt=""Breakpoint""></p>

<hr>

<p><strong>Questions</strong></p>

<ol>
<li>How to easily calculate the intercept and slope of each segment? Can the model be reparemetrized to do this in one calculation?</li>
<li>How to calculate the standard error of each slope of each segment?</li>
<li>How to test whether two adjacent slopes have the same slopes (i.e. whether the breakpoint can be omitted)?</li>
</ol>
"
"0.114267681625501","0.10889023202607"," 62125","<p>I'm interested in the effect on beta estimation of including/excluding independent variables in linear regression. </p>

<p>I've made this data below:</p>

<pre><code>set.seed(50)
predictor1 &lt;- rnorm(10, 3, 1)
predictor2 &lt;- rnorm(10, 6, 1)
</code></pre>

<p>I've also simulated a model using these data, with an intercept of 2, a beta of 50 for <code>predictor1</code> and a beta
of 2 for <code>predictor2</code>:</p>

<pre><code>response &lt;- 2 + (50 * predictor1) + (2*predictor2)
</code></pre>

<p>A linear regression in <code>R</code> correctly calculates the intercept and both the betas:</p>

<pre><code>summary(lm(response ~ predictor1 + predictor2))

Call:
lm(formula = response ~ predictor1 + predictor2)

Residuals:
       Min         1Q     Median         3Q        Max 
-4.150e-14 -6.833e-15  1.100e-15  9.879e-15  2.757e-14 

Coefficients:
             Estimate Std. Error   t value Pr(&gt;|t|)    
(Intercept) 2.000e+00  8.194e-14 2.441e+13   &lt;2e-16 ***
predictor1  5.000e+01  7.730e-15 6.468e+15   &lt;2e-16 ***
predictor2  2.000e+00  1.353e-14 1.479e+14   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 2.081e-14 on 7 degrees of freedom
Multiple R-squared:      1,  Adjusted R-squared:      1 
F-statistic: 2.095e+31 on 2 and 7 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>However, a linear regression with only <code>predictor2</code> in the model calculates a beta of -3.467 for <code>predictor2</code>:   </p>

<pre><code>summary(lm(response ~ predictor2))

Call:
lm(formula = response ~ predictor2)

Residuals:
    Min      1Q  Median      3Q     Max 
-75.201 -24.049   5.193  38.423  56.074 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  171.133    177.588   0.964    0.363
predictor2    -3.467     30.872  -0.112    0.913

Residual standard error: 47.58 on 8 degrees of freedom
Multiple R-squared:  0.001574,  Adjusted R-squared:  -0.1232 
F-statistic: 0.01261 on 1 and 8 DF,  p-value: 0.9134
</code></pre>

<p>Here is plot of <code>response</code> against <code>predictor1</code> and <code>predictor2</code>, with line of best fit calculated through regression:</p>

<pre><code> library(ggplot2)
ggplot(dat, aes(predictor2 , response )) + geom_point() + 
  geom_point(aes(predictor1 , response )) + 
  geom_abline(intercept = 2, slope = 2) + 
  geom_abline(intercept = 2, slope = 50) + 
  ylim(0, 300) + 
  xlim(0,10)
</code></pre>

<p><img src=""http://i.stack.imgur.com/fXEE8.jpg"" alt=""enter image description here""></p>

<p>And here is plot of just <code>response</code> against just <code>predictor2</code>, against with line of best fit calculated through regression:</p>

<pre><code> ggplot(dat, aes(predictor2 , response )) + 
      geom_point() + 
      geom_abline(intercept = 171.133, slope = -3.467) 
</code></pre>

<p><img src=""http://i.stack.imgur.com/svEXN.jpg"" alt=""enter image description here""></p>

<p>I have two questions:</p>

<ol>
<li><p>How did the linear regression arrive at a beta of -3.467 in the second model, when the real beta is 2? Or in other words, how has excluding <code>predictor1</code> from the second model caused <code>predictor2</code>'s beta to drop by over 5?</p></li>
<li><p>Is someone able to provide a visual display of how linear regression calculates betas?</p></li>
</ol>
"
"0.0457070726502004","0.0580747904139041"," 62180","<p>I want to do some regression analysis that constrains the coefficients to vary smoothly as a function of their sequence.
It is similar to the ""Phoneme Recognition"" example in the part 5 ""Basis Expansions and Regularization"" of <em><a href=""http://www.stanford.edu/~hastie/local.ftp/Springer/OLD//ESLII_print4.pdf"" rel=""nofollow"">The Elements of Statistical Learning</a></em> (Hastie, Tibshirani &amp; Friedman, 2008).</p>

<p>in the book it says:</p>

<blockquote>
  <p>The smooth red curve was obtained through a very simple use of natural cubic splines. We can represent the coefficient function as an expansion of splines $\beta(f)=\sum_{m=1}^M h_m(f)\theta_m$. In practice this means that $\beta=\mathbf{H}\theta$ where, $\mathbf{H}$ is a $p Ã— M$ basis matrix of natural cubic splines, defined on the set of frequencies. Here we used $M = 12$ basis functions, with knots uniformly placed over the integers 1, 2, . . . , 256 representing the frequencies. Since $x^T\beta=x^T\mathbf{H}\theta$, we can simply replace the input features $x$ by their filtered versions $x^* = \mathbf{H}^T x$, and fit $\theta$ by linear logistic regression on the $x^*$. The red curve is thus $\hat\beta(f) = h(f)^T\hat \theta$.</p>
</blockquote>

<p>But I am not sure about how to create the basis matrix $\mathbf{H}$. When using ns() function in R, how to set the knots?</p>

<p>Any hint will be appreciated.</p>
"
"0.103465538715443","0.105639041939177"," 62208","<p>I've tried to simulate data for a power analysis of a logistic regression. The results of the power analysis look reasonable: power=90% for a sample of 6000 persons. But I feel that the analysis lacks something. So, my question is: when generating the data should I include something about how the variables are correlated, or their covariance, other than just defining their linear relationship as I have done in the example below, and if so where do I write that into the code?</p>

<p>I know other questions look like this but I'm not confident that they answer this question.</p>

<pre><code>library(plyr) # functions
## Define Function
simfunktion &lt;- function() {
   # Number in each sample
  antal &lt;- 6000
  beta0 &lt;- log(0.16) # logit in reference group
  beta1 &lt;- log(1.1)  # logit given smoking
  beta2 &lt;- log(1.1)  # logit given SNP(genevariation)
  beta3 &lt;- log(1.2)  # logit for interactioncoefficient for SNP*rygning
   ## Smoking variable, with probabilities defined according to empirical studies.
  smoking  &lt;- sample(x = 0:2, size = antal, replace = TRUE, prob = c(40,25,40))
   ## SNP variables with probabilities defined according to empirical studies
  SNP      &lt;- sample(x = 0:2, size = antal, replace = TRUE, prob = c(40,40,20))
   ## calculated probabilites given the model:
  pi.x     &lt;- exp( beta0 + beta1*smoking + beta2*SNP + beta3*smoking*SNP) / 
              ( 1 + exp(beta0 + beta1*smoking + beta2*SNP + beta3*smoking*SNP) )
   ## binoial events given the probabilities:
  sim.y    &lt;- rbinom( n = antal, size = 1, prob = pi.x)  
  sim.data &lt;- data.frame(sim.y, smoking, SNP)
   #################### p-value of the interaction is extracted:
   ## the model is run:
  glm1     &lt;- glm( data = sim.data, formula = sim.y ~ smoking + SNP + smoking:SNP, 
                  family=binomial )
   ## p-value of the interactionterm is extracted:
  summary(    glm( data = sim.data, formula = sim.y ~ smoking + SNP + smoking:SNP, 
                  family=binomial ))$coef[4,4]
}
pvalue     &lt;- as.vector(replicate( 100 , simfunktion()))
mean(pvalue &lt; 0.05)
</code></pre>
"
"0.091874672876503","0.0963061447907242"," 62427","<p>I am working to investigate association between environmental pollution and daily hospital admission due to various causes.   This outcome data has excess zeros on days when there are no admissions for specific causes.  I would like to adjust for temperature and humidity using smooth terms.  Usually Poisson generalized additive models with smoothing parameters for time trend and meteorological variables are used to model such associations.   However due to excessive zeros I was advised to consider zero inflated models.  How can I implement this in R and address non parametric associations of temperature, humidity and time trend? </p>

<ol>
<li>What is the criterion to use zero inflated models for a count data?</li>
<li>How do I run zero inflated model and how do I check the fit?</li>
</ol>

<p>The following is a sample GAM model in R using mgcv package:</p>

<pre><code>Log {E (hospital admission)} = Î±+Î²(pollutant)+s(time)+s(temperature) + s(Relative Humidity) + DOW + flu
Where: 
E (admission) = expected count of cause specific admissions on day t
Î²= regression coefficient of the pollutant
pollutant = air pollutant (PM10, ozone, NO2) level at time t
s = smooth function using natural or penalized spline
dow =  vector of regression coefficient associated with indicator variables for day of the week (DOW).
Flu= weekly influenza count
Time, temperature and relative humidity are covariates 
</code></pre>

<p>Thanks</p>
"
"0.0791669720953889","0.0838237396964966"," 62646","<p>I've got data on mail volume sent by household for seven age groups, with 12 years of data for each age group. I originally ran a simple regression on each age group individually and realized I needed to dig deeper. My aim now is to pool the data (giving me 84 observations) and try to identify some period effects (or year effects, whichever you prefer). My pooled data are currently organized like this (PPHPY stands for Pieces per Household Per Year):</p>

<pre><code>Age Group    Year   PPHPY
1            2001   127.62
1            2002   144.47
1            2003   111.70
1            2004   95.96
1            2005   96.46
1            2006   139.91
1            2007   85.52
1            2008   75.43
1            2009   109.34
1            2010   53.16
1            2011   64.09
1            2012   50.94        
2            2001   176.48
2            2002   172.86
2            2003   137.79
.              .      .
.              .      .
.              .      .
7            2012   163.39
</code></pre>

<p>I first regressed PPHPY on year and year dummies (leaving the intercept as 0 to avoid perfect multicollinearity). This gave me period effects for the aggregated data (ie something like a period effect across all age groups, I think). This looked like the following:</p>

<pre><code>&gt; ## Generate YearDummy using factor()
&gt;
&gt; YearDummy &lt;- factor(YearVar)
&gt;
&gt; ## Check to see that YearDummy is indeed a factor variable
&gt;
&gt; is.factor(YearDummy)
[1] TRUE
&gt;
&gt; ## (...+0) ensures intercept is left out and thus YearDummy1 remains in.
    ## One or the other must be subtracted out to avoid perfect mutlicollinearity
&gt;
&gt; LSDVYear &lt;- lm(PPHPY ~ YearVar + YearDummy + 0, data=maildatapooled)
&gt; summary(LSDVYear)
Call:
lm(formula = PPHPY ~ YearVar + YearDummy + 0, data = maildatapooled)
Residuals:
Min 1Q Median 3Q Max
-99.658 -39.038 8.814 43.670 82.300
Coefficients: (1 not defined because of singularities)
Estimate Std. Error t value Pr(&gt;|t|)
YearVar 5.743e-02 9.851e-03 5.830 1.45e-07 ***
YearDummy2001 1.099e+02 2.795e+01 3.930 0.000193 ***
YearDummy2002 1.209e+02 2.796e+01 4.324 4.85e-05 ***
YearDummy2003 7.791e+01 2.797e+01 2.786 0.006819 **
YearDummy2004 8.053e+01 2.797e+01 2.879 0.005251 **
YearDummy2005 6.887e+01 2.798e+01 2.461 0.016236 *
YearDummy2006 6.572e+01 2.799e+01 2.348 0.021618 *
YearDummy2007 5.975e+01 2.799e+01 2.134 0.036210 *
YearDummy2008 5.836e+01 2.800e+01 2.084 0.040696 *
YearDummy2009 4.119e+01 2.801e+01 1.471 0.145745
YearDummy2010 3.056e+01 2.801e+01 1.091 0.278990
YearDummy2011 1.472e+01 2.802e+01 0.525 0.600951
YearDummy2012 NA NA NA NA
---
Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
Residual standard error: 52.44 on 72 degrees of freedom
Multiple R-squared: 0.9316, Adjusted R-squared: 0.9202
F-statistic: 81.71 on 12 and 72 DF, p-value: &lt; 2.2e-16
</code></pre>

<p>What I want, however, is to tease out period effects for each age group individually. This is what I'm not sure how to set up. I was hoping someone might help me devise some code in R that would kick out those period effects for <em>each</em> of the seven age groups using the pooled data, as well as help me understand the problem conceptually. </p>

<p>EDIT: I forgot to mention that I see I must include an interaction term involving the time dummies to allow the coefficients to vary across age groups. I'm just having difficulty constructing the proper interaction term and resulting regression equation.</p>

<p>EDIT 2: I came up with two models and ran them. I felt like the question had evolved at this point and might merit a new post, which is can be found <a href=""http://stats.stackexchange.com/questions/62755/period-effects-in-pooled-time-series-data-in-r"">here</a>.</p>
"
"0.111959005629311","0.106690202574856"," 62755","<p>This is closely related to a question I asked yesterday but I've now got a much more complete answer on which I was hoping to get feedback. The previous question was just looking for conceptual advice and was very helpful. You can find the relevant data and introduction <a href=""http://stats.stackexchange.com/questions/62646/pooled-time-series-regression-in-r"">here</a>.</p>

<p>I wanted to find period effects for each age group. I've run two regressions using dummies as part of an interaction term. I'm hoping to see if my method is flawed and if my interpretation of the results is correct or not. They first regression is as follows:</p>

<pre><code>&gt; ## Generate YearDummy and AgeGroupDummy using factor()
&gt; 
&gt; YearDummy &lt;- factor(YearVar)
&gt; AgeGroupDummy &lt;- factor(AgeGroup)
&gt; 
&gt; ## Check to see that YearDummy and CohortDummy are indeed factor variables
&gt; 
&gt; is.factor(YearDummy)
[1] TRUE
&gt; is.factor(AgeGroupDummy)
[1] TRUE
&gt; ## Regress on AgeGroup and include AgeGroup*YearDummy interaction terms
&gt; 
&gt; PooledOLS1 &lt;- lm(PPHPY ~ AgeGroup + AgeGroup*YearDummy + 0, 
data=maildatapooled)
&gt; summary(PooledOLS1)
Call:
lm(formula = PPHPY ~ AgeGroup + AgeGroup * YearDummy + 0, data = 
maildatapooled)
Residuals:
 Min 1Q Median 3Q Max 
-38.852 -10.632 3.298 11.275 26.481 
Coefficients:
 Estimate Std. Error t value Pr(&gt;|t|) 
AgeGroup 26.2212 3.5070 7.477 3.84e-10 ***
YearDummy1 119.8836 15.6840 7.644 1.99e-10 ***
YearDummy2 123.7458 15.6840 7.890 7.55e-11 ***
YearDummy3 103.2660 15.6840 6.584 1.28e-08 ***
YearDummy4 97.7102 15.6840 6.230 5.06e-08 ***
YearDummy5 103.3295 15.6840 6.588 1.26e-08 ***
YearDummy6 103.2330 15.6840 6.582 1.29e-08 ***
YearDummy7 84.8291 15.6840 5.409 1.16e-06 ***
YearDummy8 70.7114 15.6840 4.509 3.09e-05 ***
YearDummy9 90.9566 15.6840 5.799 2.65e-07 ***
YearDummy10 50.0885 15.6840 3.194 0.00224 ** 
YearDummy11 37.7004 15.6840 2.404 0.01933 * 
YearDummy12 33.1947 15.6840 2.116 0.03846 * 
AgeGroup:YearDummy2 1.8066 4.9597 0.364 0.71695 
AgeGroup:YearDummy3 -3.8022 4.9597 -0.767 0.44632 
AgeGroup:YearDummy4 -1.7436 4.9597 -0.352 0.72640 
AgeGroup:YearDummy5 -6.0494 4.9597 -1.220 0.22735 
AgeGroup:YearDummy6 -6.7992 4.9597 -1.371 0.17552 
AgeGroup:YearDummy7 -3.6752 4.9597 -0.741 0.46158 
AgeGroup:YearDummy8 -0.4799 4.9597 -0.097 0.92323 
AgeGroup:YearDummy9 -9.8190 4.9597 -1.980 0.05232 . 
AgeGroup:YearDummy10 -2.2452 4.9597 -0.453 0.65241 
</code></pre>

<p>My interpretation of the interaction term coefficients is that they represent the difference in slope of AgeGroup between the period of the corresponding YearDummy and the AgeGroup slope at the top of the results. This is kind of like the AgeGroup effect across different periods.</p>

<p>My second regression is as follows:</p>

<pre><code>&gt; ## Regress YearVar and Include YearVar*AgeGroupDUmmy
&gt; 
&gt; PooledOLS2 &lt;- lm(PPHPY ~ YearVar + YearVar*AgeGroupDummy + 0, 
data=maildatapooled)
&gt; summary(PooledOLS2)
Call:
lm(formula = PPHPY ~ YearVar + YearVar * AgeGroupDummy + 0, data = 
maildatapooled)
Residuals:
 Min 1Q Median 3Q Max 
-29.345 -9.325 -0.915 8.540 40.150 
Coefficients:
 Estimate Std. Error t value Pr(&gt;|t|) 
YearVar -7.089 1.252 -5.664 3.04e-07 ***
AgeGroupDummy1 142.292 9.211 15.447 &lt; 2e-16 ***
AgeGroupDummy2 185.508 9.211 20.139 &lt; 2e-16 ***
AgeGroupDummy3 218.170 9.211 23.685 &lt; 2e-16 ***
AgeGroupDummy4 255.733 9.211 27.763 &lt; 2e-16 ***
AgeGroupDummy5 278.180 9.211 30.200 &lt; 2e-16 ***
AgeGroupDummy6 300.910 9.211 32.667 &lt; 2e-16 ***
AgeGroupDummy7 282.325 9.211 30.650 &lt; 2e-16 ***
YearVar:AgeGroupDummy2 -1.737 1.770 -0.981 0.3298 
YearVar:AgeGroupDummy3 -2.401 1.770 -1.357 0.1792 
YearVar:AgeGroupDummy4 -3.772 1.770 -2.131 0.0366 * 
YearVar:AgeGroupDummy5 -2.915 1.770 -1.647 0.1040 
YearVar:AgeGroupDummy6 -3.587 1.770 -2.026 0.0465 * 
YearVar:AgeGroupDummy7 -2.372 1.770 -1.340 0.1846 
---
Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
Residual standard error: 14.97 on 70 degrees of freedom
Multiple R-squared: 0.9946, Adjusted R-squared: 0.9935 
F-statistic: 917.9 on 14 and 70 DF, p-value: &lt; 2.2e-16
</code></pre>

<p>My interpretation of the interaction term coefficients here is that they represent the difference in slope of YearVar between the corresponding AgeGroup in the interaction term and the YearVar result at the very top. That is, they are something like a period effect across the different age groups.</p>

<p>Can anyone see a problem with what I've done here or with my interpretation? This second regression is the closest thing to period effects across distinct age groups that I've been able to muster. Any critiques/new ideas are welcome.</p>
"
"0.0929636479491388","0.104695817324578"," 63039","<p>I've set up a Bayesian regression model in WinBUGS to determine values for the unknown parameters (b1, b2, b3, b4) and intercept value (b0) in a linear regression model. The code is as follows:</p>

<pre><code>model {
for (i in 1:(J-1)) {
  FC[i]       ~ dnorm(mu[i], tau)
  mu[i] &lt;- b0 + b1*(Factor_b1[i]-mean_Factor_b1) + b2*(Factor_b2[i]-mean_Factor_b2) + b3*(Factor_b3[i]-mean_Factor_b3) + b4*(Factor_b4[i]-mean_Factor_b4) 
}

b0        ~ dflat()
b1         ~ dflat()
b2         ~ dflat()
b3         ~ dflat()
b4         ~ dflat()
tau         &lt;- 1/sigma2
log(sigma2) &lt;- 2*log.sigma
log.sigma    ~ dflat()
}

Inits:
list(b0 =0,b1 = 0, b2 =0, b3 = 0, b4 =0, log.sigma=0)

Data1

list(J = 20, FC = c(1.87315166256848, 1.87315166256848, 
1.87315166256848, 1.8708501655802, 1.8708501655802, 1.8708501655802, 
1.93248104062608, 1.93248104062608, 1.93248104062608, 1.93248104062608, 
1.80846914258265, 1.80846914258265, 1.80846914258265, 2.10555453929548, 
2.10555453929548, 2.10555453929548, 2.10555453929548, 2.10555453929548, 
2.12908503670568, 2.12908503670568), Factor_b1 = c(7.0057890192535, 
7.0057890192535, 7.0057890192535, 7.05012252026906, 7.05012252026906, 
7.05012252026906, 7.13329595489607, 7.13329595489607, 7.13329595489607, 
7.13329595489607, 7.13329595489607, 7.13329595489607, 7.13329595489607, 
7.11720550316434, 7.11720550316434, 7.11720550316434, 7.11720550316434, 
7.11720550316434, 7.14124512235049, 7.14124512235049), mean_Factor_b1 = 7.09846620316814, 
Factor_b2 = c(7.2211050981825, 7.2211050981825, 7.2211050981825, 
7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 
7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 
7.2211050981825, 7.2211050981825, 7.37650812632622, 7.37650812632622, 
7.37650812632622, 7.37650812632622, 7.37650812632622, 7.46565531013406, 
7.46565531013406), mean_Factor_b2 = 7.28441087641358, Factor_b3 = c(2.37954613413017, 
2.37954613413017, 2.37954613413017, 2.28238238567653, 2.28238238567653, 
2.28238238567653, 2.28238238567653, 2.28238238567653, 2.28238238567653, 
2.28238238567653, 2.28238238567653, 2.28238238567653, 2.28238238567653, 
2.33214389523559, 2.33214389523559, 2.33214389523559, 2.33214389523559, 
2.33214389523559, 2.33214389523559, 2.33214389523559), mean_Factor_b3 = 2.31437347629025, 
Factor_b4 = c(2.06686275947298, 2.06686275947298, 2.06686275947298, 
2.09186406167839, 2.09186406167839, 2.09186406167839, 2.10413415427021, 
2.10413415427021, 2.10413415427021, 2.10413415427021, 2.06686275947298, 
2.06686275947298, 2.06686275947298, 2.32238772029023, 2.32238772029023, 
2.32238772029023, 2.32238772029023, 2.32238772029023, 2.2082744135228, 
2.2082744135228), mean_Factor_b4 = 2.15608963937253)
</code></pre>

<p>This WinBUGS code returns the following outputs for the unknown intercept (b0) and parameter (b1, b2, b3, b4) values:</p>

<pre><code>     node    mean    sd  MC error   2.5%    median  97.5%   start   sample
b0  1.957   0.009337    3.764E-5    1.939   1.957   1.976   1001    56000
b1  0.1068  0.3296  0.001438    -0.5529 0.1072  0.7615  1001    56000
b2  0.5977  0.2758  0.001068    0.05286 0.5967  1.147   1001    56000
b3  0.1892  0.4394  0.001825    -0.6871 0.1899  1.061   1001    56000
b4  0.5757  0.1886  7.423E-4    0.1986  0.5765  0.9472  1001    56000
</code></pre>

<p>MY PROBLEM: When I compare these Bayesian estimates with results from a linear MLE regression in R, I seem to be getting a different result for the intercept value (b0). The code for the R linear regression is as follows:</p>

<pre><code>FC = c(1.87315166256848, 1.87315166256848, 1.87315166256848, 1.8708501655802, 1.8708501655802, 1.8708501655802, 1.93248104062608, 1.93248104062608, 1.93248104062608, 1.93248104062608, 1.80846914258265, 1.80846914258265, 1.80846914258265, 2.10555453929548, 2.10555453929548, 2.10555453929548, 2.10555453929548, 2.10555453929548, 2.12908503670568, 2.12908503670568)

b1 = c(7.0057890192535, 7.0057890192535, 7.0057890192535, 7.05012252026906, 7.05012252026906, 7.13329595489607, 7.13329595489607, 7.13329595489607, 7.13329595489607, 7.13329595489607, 7.13329595489607, 7.13329595489607, 7.11720550316434, 7.11720550316434, 7.11720550316434, 7.11720550316434,  7.11720550316434, 7.14124512235049, 7.14124512235049) 

b2 = c(7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 7.2211050981825, 7.37650812632622, 7.37650812632622, 7.37650812632622, 7.37650812632622, 7.37650812632622, 7.46565531013406, 7.46565531013406)

b3 = c(2.37954613413017, 2.37954613413017, 2.37954613413017, 2.28238238567653, 2.28238238567653, 2.28238238567653, 2.28238238567653, 2.28238238567653, 2.28238238567653, 2.28238238567653, 2.28238238567653, 2.28238238567653, 2.28238238567653, 2.33214389523559, 2.33214389523559, 2.33214389523559, 2.33214389523559, 2.33214389523559, 2.33214389523559, 2.33214389523559) 

b4 = c(2.06686275947298, 2.06686275947298, 2.06686275947298, 2.09186406167839,       2.09186406167839, 2.09186406167839, 2.10413415427021, 2.10413415427021, 2.10413415427021, 2.10413415427021, 2.06686275947298, 2.06686275947298, 2.06686275947298, 2.32238772029023, 2.32238772029023, 2.32238772029023, 2.32238772029023, 2.32238772029023, 2.2082744135228, 2.2082744135228)

# ======================= Linear Model =======================


lmfit_Linear_Model_Test =lm(FC ~ (b1 + b2 + b3 + b4))

print (summary(lmfit_Linear_Model_Test))
</code></pre>

<p>And the results from this MLE regressions are as follows:</p>

<pre><code>Call:
lm(formula = FC ~ (b1 + b2 + b3 + b4))

Residuals:
     Min       1Q   Median       3Q      Max 
-0.05837 -0.00823 -0.00044  0.01307  0.04593 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)   -5.247      2.305   -2.28   0.0379 * 
b1             0.105      0.298    0.35   0.7280   
b2             0.674      0.195    3.45   0.0035 **
b3             0.177      0.394    0.45   0.6599   
b4             0.529      0.141    3.75   0.0019 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.0359 on 15 degrees of freedom
Multiple R-squared: 0.931,  Adjusted R-squared: 0.913 
F-statistic: 50.8 on 4 and 15 DF,  p-value: 0.0000000153 
</code></pre>

<p>SUMMARY: Why is the intercept value (b0) coming to -5.247 with the MLE model and 1.957 with the Bayesian model? Should they not be the same?</p>
"
"0.0304713817668003","0.029037395206952"," 63055","<p>I am doing survival analysis using ridge regression. I'm using this R command:</p>

<pre><code>coxph(Surv(time, status) ~ ridge(x1, x2, x3), data=DATA)
</code></pre>

<p>As far as I know, <code>lambda</code> (the regulation parameter) is estimated using cross validation, but then this R code should result in different results with different random seeds. But I got always the same coefficients; how can that happen? </p>

<p>Is <code>coxph(Surv()~.)</code> not a commonly used approach? Should I use <code>glmnet</code> or any other functions?  </p>
"
"0.129279124076572","0.123195234352977"," 63222","<p>How do I get p-values using the <code>multinom</code> function of <code>nnet</code> package in <code>R</code>?</p>

<p>I have a dataset which consists of â€œPathology scoresâ€ (Absent, Mild, Severe) as outcome variable, and two main effects: Age (two factors: twenty / thirty days) and Treatment Group (four factors: infected without ATB; infected + ATB1; infected + ATB2; infected + ATB3).</p>

<p>First I tried to fit an ordinal regression model, which seems more appropriate given the characteristics of my dependent variable (ordinal). However, the assumption of odds proportionality was severely violated (graphically), which prompted me to use a multinomial model instead, using the <code>nnet</code> package.  </p>

<p>First I chose the outcome level that I need to use as baseline category: </p>

<pre><code>Data$Path &lt;- relevel(Data$Path, ref = ""Absent"")
</code></pre>

<p>Then, I needed to set baseline categories for the independent variables:</p>

<pre><code>Data$Age &lt;- relevel(Data$Age, ref = ""Twenty"")
Data$Treat &lt;- relevel(Data$Treat, ref=""infected without ATB"") 
</code></pre>

<p>The model:</p>

<pre><code>test &lt;- multinom(Path ~ Treat + Age, data = Data) 
# weights:  18 (10 variable) 
initial value 128.537638 
iter 10 value 80.623608 
final  value 80.619911 
converged
</code></pre>

<p>The output:</p>

<pre><code>Coefficients:
         (Intercept)   infected+ATB1   infected+ATB2   infected+ATB3    AgeThirty
Moderate   -2.238106   -1.1738540      -1.709608       -1.599301        2.684677
Severe     -1.544361   -0.8696531      -2.991307       -1.506709        1.810771

Std. Errors:
         (Intercept)    infected+ATB1   infected+ATB2   infected+ATB3    AgeThirty
Moderate   0.7880046    0.8430368       0.7731359       0.7718480        0.8150993
Severe     0.6110903    0.7574311       1.1486203       0.7504781        0.6607360

Residual Deviance: 161.2398
AIC: 181.2398
</code></pre>

<p>For a while, I could not find a way to get the $p$-values for the model and estimates when using <code>nnet:multinom</code>. Yesterday I came across a post where the author put forward a similar issue regarding estimation of $p$-values for coefficients (<a href=""http://stats.stackexchange.com/questions/9715/how-to-set-up-and-estimate-a-multinomial-logit-model-in-r"">How to set up and estimate a multinomial logit model in R?</a>). There, one blogger suggested that getting $p$-values from the <code>summary</code> result of <code>multinom</code> is pretty easy, by first getting the $t$values as follows: </p>

<pre><code>pt(abs(summary1$coefficients / summary1$standard.errors), df=nrow(Data)-10, lower=FALSE) 

         (Intercept)   infected+ATB1   infected+ATB2   infected+ATB3    AgeThirty
Moderate 0.002670340   0.08325396      0.014506395     0.02025858       0.0006587898
Severe   0.006433581   0.12665278      0.005216581     0.02352202       0.0035612114
</code></pre>

<p>According to Peter Dalgard, ""There's at least a factor of 2 missing for a two-tailed $p$-value. It is usually a mistake to use the $t$-distribution for what is really a $z$-statistic; for aggregated data, it can be a very bad mistake.""
According to Brian Ripley, ""it is also a mistake to use Wald tests for <code>multinom</code> fits, since they suffer from the same (potentially severe) problems as binomial fits. 
Use profile-likelihood confidence intervals (for which the package does provide software), or if you must test, likelihood-ratio tests (ditto).""</p>

<p>I just need to be able to derive reliable $p$-values.</p>
"
"0.136838784658047","0.13632636995676"," 63233","<h2>Background</h2>

<p>In a paper from Epstein (1991): <a href=""http://journals.ametsoc.org/doi/pdf/10.1175/1520-0442%281991%29004%3C0365%3AOODCVF%3E2.0.CO%3B2"" rel=""nofollow"">On obtaining daily climatological values from monthly means</a>, the formulation and an algorithm for calculating Fourier interpolation for periodical and even-spaced values are given.</p>

<p>In the paper, the goal is to <strong>obtain daily values from monthly means</strong> by interpolation.</p>

<p>In short, it is assumed that unknown daily values can be represented by the sum of harmonic components:
$$
y(t) = a_{0} + \sum_{j}\left[a_{j}\,\cos(2\pi jt/12)+b_{j}\,\sin(2\pi jt/12)\right]
$$
In the paper $t$ (time) is expressed in months.</p>

<p>After some derviation, it is shown that the terms can be calculated by:
$$
\begin{align}
a_{0} &amp;= \sum_{T}Y_{T}/12 \\
a_{j} &amp;= \left[ (\pi j/12)/\sin(\pi j/12)\right] \times \sum_{T}\left[Y_{T}\,\cos(2\pi jT/12)/6 \right]~~~~~~~j=1,\ldots, 5 \\
b_{j} &amp;= \left[ (\pi j/12)/\sin(\pi j/12)\right] \times \sum_{T}\left[Y_{T}\,\sin(2\pi jT/12)/6 \right]~~~~~~~j=1,\ldots, 5 \\
a_{6} &amp;= \left[ (\pi j/12)/\sin(\pi j/12)\right]\times \sum_{T}\left[Y_{T}\cos(\pi T)/12\right] \\
b_{6} &amp;= 0
\end{align}
$$
Where $Y_{T}$ denote the monthly means and $T$ the month.</p>

<p><a href=""http://journals.ametsoc.org/doi/pdf/10.1175/1520-0493%281995%29123%3C2251%3ATIODSU%3E2.0.CO%3B2"" rel=""nofollow"">Harzallah (1995)</a> summarizes this aproach as follows: ""The interpolation is carried out by adding zeros to the spectral coefficients of data and by performing an inverse Fourier transform to the resulting extended coefficients. The method is equivalent to applying a rectangular filter to Fourier coefficients.""</p>

<hr>

<h2>Questions</h2>

<p>My goal is to use the above methodology for interpolation of <strong>weekly means to obtain daily data</strong> (see <a href=""http://stats.stackexchange.com/questions/59418/interpolation-of-influenza-data-that-conserves-weekly-mean/63135#63135"">my previous question</a>). In summary, I have 835 weekly means of count data (see the example dataset at the bottom of the question). There are quite a few things that I don't understand before I can apply the approach outlined above:</p>

<ol>
<li>How would the formulas have to be changed for my situation (weekly instead of monthly values)?</li>
<li>How could the time $t$ be expressed? I assumed $t/835$ (or $t/n$ with $n$ data points in general), is that correct?</li>
<li>Why does the author calculate 7 terms (i.e. $0\leq j \leq 6$)? How many terms would I have to consider?</li>
<li>I understand that the question can probably be solved by using a <a href=""http://stats.stackexchange.com/questions/60500/how-to-find-a-good-fit-for-semi-sinusoidal-model-in-r?lq=1"">regression approach</a> and using the predictions for interpolation (thanks to Nick). Still, some things are unclear to me: How many terms of harmonics should be included in the regression? And what period should I take? How can the regression be done to ensure that the weekly means are preserved (as I don't want an exact harmonic fit to the data)?</li>
</ol>

<p>Using the <a href=""http://stats.stackexchange.com/questions/60500/how-to-find-a-good-fit-for-semi-sinusoidal-model-in-r?lq=1"">regression approach</a> (which is also explained in <a href=""http://www.stata-journal.com/sjpdf.html?articlenum=st0116"" rel=""nofollow"">this paper</a>), I managed to get an exact harmonic fit to the data (the $j$ in my example would run through $1, \ldots, 417$, so I fitted 417 terms). <strong>How can this approach be modified -$~$if possible$~$- to achieve the conservation of the weekly means?</strong> Maybe by applying correction factors to each regression term?</p>

<p>The plot of the exact harmonic fit is:</p>

<p><img src=""http://i.stack.imgur.com/7XuxU.png"" alt=""Exact harmonic fit""></p>

<p><strong>EDIT</strong></p>

<p>Using the <a href=""http://cran.r-project.org/web/packages/signal/"" rel=""nofollow"">signal package</a> and the <code>interp1</code> function, here's what I've managed to do using the example data set from below (many thanks to @noumenal). I use <code>q=7</code> as we have weekly data:</p>

<pre><code># Set up the time scale

daily.ts &lt;- seq(from=as.Date(""1995-01-01""), to=as.Date(""2010-12-31""), by=""day"")

# Set up data frame 

ts.frame &lt;- data.frame(daily.ts=daily.ts, wdayno=as.POSIXlt(daily.ts)$wday,
                       yearday = 1:5844,
                       no.influ.cases=NA)

# Add the data from the example dataset called ""my.dat""

ts.frame$no.influ.cases[ts.frame$wdayno==3] &lt;- my.dat$case

# Interpolation

case.interp1 &lt;- interp1(x=ts.frame$yearday[!is.na(ts.frame$no.influ.case)],y=(ts.frame$no.influ.cases[!is.na(ts.frame$no.influ.case)]),xi=ts.frame$yearday, method = c(""cubic""))

# Plot subset for better interpretation
par(bg=""white"", cex=1.2, las=1)
plot((ts.frame$no.influ.cases)~ts.frame$yearday, pch=20,
     col=grey(0.4),
     cex=1, las=1,xlim=c(0,400), xlab=""Day"", ylab=""Influenza cases"")
lines(case.interp1, col=""steelblue"", lwd=1)
</code></pre>

<p><img src=""http://i.stack.imgur.com/R1FE8.png"" alt=""Cubicinterpo""></p>

<p>There are two issues here:</p>

<ol>
<li>The curve seem to fit ""too good"": it goes through every point </li>
<li>The weekly means are not conserved</li>
</ol>

<p><strong>Example dataset</strong></p>

<pre><code>structure(list(date = structure(c(9134, 9141, 9148, 9155, 9162, 
9169, 9176, 9183, 9190, 9197, 9204, 9211, 9218, 9225, 9232, 9239, 
9246, 9253, 9260, 9267, 9274, 9281, 9288, 9295, 9302, 9309, 9316, 
9323, 9330, 9337, 9344, 9351, 9358, 9365, 9372, 9379, 9386, 9393, 
9400, 9407, 9414, 9421, 9428, 9435, 9442, 9449, 9456, 9463, 9470, 
9477, 9484, 9491, 9498, 9505, 9512, 9519, 9526, 9533, 9540, 9547, 
9554, 9561, 9568, 9575, 9582, 9589, 9596, 9603, 9610, 9617, 9624, 
9631, 9638, 9645, 9652, 9659, 9666, 9673, 9680, 9687, 9694, 9701, 
9708, 9715, 9722, 9729, 9736, 9743, 9750, 9757, 9764, 9771, 9778, 
9785, 9792, 9799, 9806, 9813, 9820, 9827, 9834, 9841, 9848, 9855, 
9862, 9869, 9876, 9883, 9890, 9897, 9904, 9911, 9918, 9925, 9932, 
9939, 9946, 9953, 9960, 9967, 9974, 9981, 9988, 9995, 10002, 
10009, 10016, 10023, 10030, 10037, 10044, 10051, 10058, 10065, 
10072, 10079, 10086, 10093, 10100, 10107, 10114, 10121, 10128, 
10135, 10142, 10149, 10156, 10163, 10170, 10177, 10184, 10191, 
10198, 10205, 10212, 10219, 10226, 10233, 10240, 10247, 10254, 
10261, 10268, 10275, 10282, 10289, 10296, 10303, 10310, 10317, 
10324, 10331, 10338, 10345, 10352, 10359, 10366, 10373, 10380, 
10387, 10394, 10401, 10408, 10415, 10422, 10429, 10436, 10443, 
10450, 10457, 10464, 10471, 10478, 10485, 10492, 10499, 10506, 
10513, 10520, 10527, 10534, 10541, 10548, 10555, 10562, 10569, 
10576, 10583, 10590, 10597, 10604, 10611, 10618, 10625, 10632, 
10639, 10646, 10653, 10660, 10667, 10674, 10681, 10688, 10695, 
10702, 10709, 10716, 10723, 10730, 10737, 10744, 10751, 10758, 
10765, 10772, 10779, 10786, 10793, 10800, 10807, 10814, 10821, 
10828, 10835, 10842, 10849, 10856, 10863, 10870, 10877, 10884, 
10891, 10898, 10905, 10912, 10919, 10926, 10933, 10940, 10947, 
10954, 10961, 10968, 10975, 10982, 10989, 10996, 11003, 11010, 
11017, 11024, 11031, 11038, 11045, 11052, 11059, 11066, 11073, 
11080, 11087, 11094, 11101, 11108, 11115, 11122, 11129, 11136, 
11143, 11150, 11157, 11164, 11171, 11178, 11185, 11192, 11199, 
11206, 11213, 11220, 11227, 11234, 11241, 11248, 11255, 11262, 
11269, 11276, 11283, 11290, 11297, 11304, 11311, 11318, 11325, 
11332, 11339, 11346, 11353, 11360, 11367, 11374, 11381, 11388, 
11395, 11402, 11409, 11416, 11423, 11430, 11437, 11444, 11451, 
11458, 11465, 11472, 11479, 11486, 11493, 11500, 11507, 11514, 
11521, 11528, 11535, 11542, 11549, 11556, 11563, 11570, 11577, 
11584, 11591, 11598, 11605, 11612, 11619, 11626, 11633, 11640, 
11647, 11654, 11661, 11668, 11675, 11682, 11689, 11696, 11703, 
11710, 11717, 11724, 11731, 11738, 11745, 11752, 11759, 11766, 
11773, 11780, 11787, 11794, 11801, 11808, 11815, 11822, 11829, 
11836, 11843, 11850, 11857, 11864, 11871, 11878, 11885, 11892, 
11899, 11906, 11913, 11920, 11927, 11934, 11941, 11948, 11955, 
11962, 11969, 11976, 11983, 11990, 11997, 12004, 12011, 12018, 
12025, 12032, 12039, 12046, 12053, 12060, 12067, 12074, 12081, 
12088, 12095, 12102, 12109, 12116, 12123, 12130, 12137, 12144, 
12151, 12158, 12165, 12172, 12179, 12186, 12193, 12200, 12207, 
12214, 12221, 12228, 12235, 12242, 12249, 12256, 12263, 12270, 
12277, 12284, 12291, 12298, 12305, 12312, 12319, 12326, 12333, 
12340, 12347, 12354, 12361, 12368, 12375, 12382, 12389, 12396, 
12403, 12410, 12417, 12424, 12431, 12438, 12445, 12452, 12459, 
12466, 12473, 12480, 12487, 12494, 12501, 12508, 12515, 12522, 
12529, 12536, 12543, 12550, 12557, 12564, 12571, 12578, 12585, 
12592, 12599, 12606, 12613, 12620, 12627, 12634, 12641, 12648, 
12655, 12662, 12669, 12676, 12683, 12690, 12697, 12704, 12711, 
12718, 12725, 12732, 12739, 12746, 12753, 12760, 12767, 12774, 
12781, 12788, 12795, 12802, 12809, 12816, 12823, 12830, 12837, 
12844, 12851, 12858, 12865, 12872, 12879, 12886, 12893, 12900, 
12907, 12914, 12921, 12928, 12935, 12942, 12949, 12956, 12963, 
12970, 12977, 12984, 12991, 12998, 13005, 13012, 13019, 13026, 
13033, 13040, 13047, 13054, 13061, 13068, 13075, 13082, 13089, 
13096, 13103, 13110, 13117, 13124, 13131, 13138, 13145, 13152, 
13159, 13166, 13173, 13180, 13187, 13194, 13201, 13208, 13215, 
13222, 13229, 13236, 13243, 13250, 13257, 13264, 13271, 13278, 
13285, 13292, 13299, 13306, 13313, 13320, 13327, 13334, 13341, 
13348, 13355, 13362, 13369, 13376, 13383, 13390, 13397, 13404, 
13411, 13418, 13425, 13432, 13439, 13446, 13453, 13460, 13467, 
13474, 13481, 13488, 13495, 13502, 13509, 13516, 13523, 13530, 
13537, 13544, 13551, 13558, 13565, 13572, 13579, 13586, 13593, 
13600, 13607, 13614, 13621, 13628, 13635, 13642, 13649, 13656, 
13663, 13670, 13677, 13684, 13691, 13698, 13705, 13712, 13719, 
13726, 13733, 13740, 13747, 13754, 13761, 13768, 13775, 13782, 
13789, 13796, 13803, 13810, 13817, 13824, 13831, 13838, 13845, 
13852, 13859, 13866, 13873, 13880, 13887, 13894, 13901, 13908, 
13915, 13922, 13929, 13936, 13943, 13950, 13957, 13964, 13971, 
13978, 13985, 13992, 13999, 14006, 14013, 14020, 14027, 14034, 
14041, 14048, 14055, 14062, 14069, 14076, 14083, 14090, 14097, 
14104, 14111, 14118, 14125, 14132, 14139, 14146, 14153, 14160, 
14167, 14174, 14181, 14188, 14195, 14202, 14209, 14216, 14223, 
14230, 14237, 14244, 14251, 14258, 14265, 14272, 14279, 14286, 
14293, 14300, 14307, 14314, 14321, 14328, 14335, 14342, 14349, 
14356, 14363, 14370, 14377, 14384, 14391, 14398, 14405, 14412, 
14419, 14426, 14433, 14440, 14447, 14454, 14461, 14468, 14475, 
14482, 14489, 14496, 14503, 14510, 14517, 14524, 14531, 14538, 
14545, 14552, 14559, 14566, 14573, 14580, 14587, 14594, 14601, 
14608, 14615, 14622, 14629, 14636, 14643, 14650, 14657, 14664, 
14671, 14678, 14685, 14692, 14699, 14706, 14713, 14720, 14727, 
14734, 14741, 14748, 14755, 14762, 14769, 14776, 14783, 14790, 
14797, 14804, 14811, 14818, 14825, 14832, 14839, 14846, 14853, 
14860, 14867, 14874, 14881, 14888, 14895, 14902, 14909, 14916, 
14923, 14930, 14937, 14944, 14951, 14958, 14965, 14972), class = ""Date""), 
    cases = c(168L, 199L, 214L, 230L, 267L, 373L, 387L, 443L, 
    579L, 821L, 1229L, 1014L, 831L, 648L, 257L, 203L, 137L, 78L, 
    82L, 69L, 45L, 51L, 45L, 63L, 55L, 54L, 52L, 27L, 24L, 12L, 
    10L, 22L, 42L, 32L, 52L, 82L, 95L, 91L, 104L, 143L, 114L, 
    100L, 83L, 113L, 145L, 175L, 222L, 258L, 384L, 755L, 976L, 
    879L, 846L, 1004L, 801L, 799L, 680L, 530L, 410L, 302L, 288L, 
    234L, 269L, 245L, 240L, 176L, 188L, 128L, 96L, 59L, 63L, 
    44L, 52L, 39L, 50L, 36L, 40L, 48L, 32L, 39L, 28L, 29L, 16L, 
    20L, 25L, 25L, 48L, 57L, 76L, 117L, 107L, 91L, 90L, 83L, 
    76L, 86L, 104L, 101L, 116L, 120L, 185L, 290L, 537L, 485L, 
    561L, 1142L, 1213L, 1235L, 1085L, 1052L, 987L, 918L, 746L, 
    620L, 396L, 280L, 214L, 148L, 148L, 94L, 107L, 69L, 55L, 
    69L, 47L, 43L, 49L, 30L, 42L, 51L, 41L, 39L, 40L, 38L, 22L, 
    37L, 26L, 40L, 56L, 54L, 74L, 99L, 114L, 114L, 120L, 114L, 
    123L, 131L, 170L, 147L, 163L, 163L, 160L, 158L, 163L, 124L, 
    115L, 176L, 171L, 214L, 320L, 507L, 902L, 1190L, 1272L, 1282L, 
    1146L, 896L, 597L, 434L, 216L, 141L, 101L, 86L, 65L, 55L, 
    35L, 49L, 29L, 55L, 53L, 57L, 34L, 43L, 42L, 13L, 17L, 20L, 
    27L, 36L, 47L, 64L, 77L, 82L, 82L, 95L, 107L, 96L, 106L, 
    93L, 114L, 102L, 116L, 128L, 123L, 212L, 203L, 165L, 267L, 
    550L, 761L, 998L, 1308L, 1613L, 1704L, 1669L, 1296L, 975L, 
    600L, 337L, 259L, 145L, 91L, 70L, 79L, 63L, 58L, 51L, 53L, 
    39L, 49L, 33L, 47L, 56L, 32L, 43L, 47L, 19L, 32L, 18L, 34L, 
    39L, 63L, 57L, 55L, 69L, 76L, 103L, 99L, 108L, 131L, 113L, 
    106L, 122L, 138L, 136L, 175L, 207L, 324L, 499L, 985L, 1674L, 
    1753L, 1419L, 1105L, 821L, 466L, 274L, 180L, 143L, 82L, 101L, 
    72L, 55L, 71L, 50L, 33L, 26L, 25L, 27L, 21L, 24L, 24L, 20L, 
    18L, 18L, 25L, 23L, 13L, 10L, 16L, 9L, 12L, 16L, 25L, 31L, 
    36L, 40L, 36L, 47L, 32L, 46L, 75L, 63L, 49L, 90L, 83L, 101L, 
    78L, 79L, 98L, 131L, 83L, 122L, 179L, 334L, 544L, 656L, 718L, 
    570L, 323L, 220L, 194L, 125L, 95L, 77L, 46L, 42L, 29L, 35L, 
    21L, 29L, 16L, 14L, 19L, 15L, 19L, 18L, 21L, 10L, 14L, 7L, 
    7L, 5L, 9L, 14L, 11L, 18L, 22L, 39L, 36L, 46L, 44L, 37L, 
    30L, 39L, 37L, 45L, 71L, 59L, 57L, 80L, 68L, 88L, 72L, 74L, 
    208L, 357L, 621L, 839L, 964L, 835L, 735L, 651L, 400L, 292L, 
    198L, 85L, 64L, 41L, 40L, 23L, 18L, 14L, 22L, 9L, 19L, 8L, 
    14L, 12L, 15L, 14L, 4L, 6L, 7L, 7L, 8L, 13L, 10L, 19L, 17L, 
    20L, 22L, 40L, 37L, 45L, 34L, 26L, 35L, 67L, 49L, 77L, 82L, 
    80L, 104L, 88L, 49L, 73L, 113L, 142L, 152L, 206L, 293L, 513L, 
    657L, 919L, 930L, 793L, 603L, 323L, 202L, 112L, 55L, 31L, 
    27L, 15L, 15L, 6L, 13L, 21L, 10L, 11L, 9L, 8L, 11L, 7L, 5L, 
    1L, 4L, 7L, 2L, 6L, 12L, 14L, 21L, 29L, 32L, 26L, 22L, 44L, 
    39L, 47L, 44L, 93L, 145L, 289L, 456L, 685L, 548L, 687L, 773L, 
    575L, 355L, 248L, 179L, 129L, 122L, 103L, 72L, 72L, 36L, 
    26L, 31L, 12L, 14L, 14L, 14L, 7L, 8L, 2L, 7L, 8L, 9L, 26L, 
    10L, 13L, 13L, 5L, 5L, 3L, 6L, 1L, 10L, 6L, 7L, 17L, 12L, 
    21L, 32L, 29L, 18L, 22L, 24L, 38L, 52L, 53L, 73L, 49L, 52L, 
    70L, 77L, 95L, 135L, 163L, 303L, 473L, 823L, 1126L, 1052L, 
    794L, 459L, 314L, 252L, 111L, 55L, 35L, 14L, 30L, 21L, 16L, 
    9L, 11L, 6L, 6L, 8L, 9L, 9L, 10L, 15L, 15L, 11L, 6L, 3L, 
    8L, 4L, 7L, 7L, 13L, 10L, 23L, 24L, 36L, 25L, 34L, 37L, 46L, 
    39L, 37L, 55L, 65L, 54L, 60L, 82L, 55L, 53L, 61L, 52L, 75L, 
    92L, 121L, 170L, 199L, 231L, 259L, 331L, 357L, 262L, 154L, 
    77L, 34L, 41L, 21L, 17L, 16L, 7L, 15L, 11L, 7L, 5L, 6L, 13L, 
    7L, 6L, 8L, 7L, 1L, 11L, 9L, 3L, 9L, 9L, 8L, 15L, 19L, 16L, 
    10L, 12L, 26L, 35L, 35L, 41L, 34L, 30L, 36L, 43L, 23L, 55L, 
    107L, 141L, 217L, 381L, 736L, 782L, 663L, 398L, 182L, 137L, 
    79L, 28L, 26L, 16L, 14L, 8L, 4L, 4L, 6L, 6L, 11L, 4L, 5L, 
    7L, 7L, 6L, 8L, 2L, 3L, 3L, 1L, 1L, 3L, 3L, 2L, 8L, 8L, 11L, 
    10L, 11L, 8L, 24L, 25L, 25L, 33L, 36L, 51L, 61L, 74L, 92L, 
    89L, 123L, 402L, 602L, 524L, 494L, 406L, 344L, 329L, 225L, 
    136L, 136L, 84L, 55L, 55L, 42L, 19L, 28L, 8L, 7L, 2L, 7L, 
    6L, 4L, 3L, 5L, 3L, 3L, 0L, 1L, 2L, 3L, 2L, 1L, 2L, 2L, 9L, 
    4L, 9L, 10L, 18L, 15L, 13L, 12L, 10L, 19L, 15L, 22L, 23L, 
    34L, 43L, 53L, 47L, 57L, 328L, 552L, 787L, 736L, 578L, 374L, 
    228L, 161L, 121L, 96L, 58L, 50L, 37L, 14L, 9L, 6L, 15L, 12L, 
    9L, 1L, 6L, 4L, 7L, 7L, 3L, 6L, 9L, 15L, 22L, 28L, 34L, 62L, 
    54L, 75L, 65L, 58L, 57L, 60L, 37L, 47L, 60L, 89L, 90L, 193L, 
    364L, 553L, 543L, 676L, 550L, 403L, 252L, 140L, 125L, 99L, 
    63L, 63L, 76L, 85L, 68L, 67L, 38L, 25L, 24L, 11L, 9L, 9L, 
    4L, 8L, 4L, 6L, 5L, 2L, 6L, 4L, 4L, 1L, 5L, 4L, 1L, 2L, 2L, 
    2L, 2L, 3L, 4L, 4L, 7L, 5L, 2L, 10L, 11L, 17L, 11L, 16L, 
    15L, 11L, 12L, 21L, 20L, 25L, 46L, 51L, 90L, 123L)), .Names = c(""date"", 
""cases""), row.names = c(NA, -835L), class = ""data.frame"")
</code></pre>
"
"NaN","NaN"," 63350","<p>I have some data that is bounded between 0 and 1. I have used the <code>betareg</code> package in R to fit a regression model with the bounded data as the dependent variable. My question is: how do I interpret the coefficients from the regression?</p>
"
"0.0545088647991304","0.0649295895722714"," 63436","<p>I sometimes have to vectorise the Huber weights from a robust regression and use them in a lm.
Recently I've had to do something similar for a logistic model but I'm slightly worried because I don't get very similar results</p>

<pre><code>library(robustbase)
data(vaso)
ROB &lt;- glmrob(Y ~Volume+Rate, family=binomial(""logit""), data=vaso)
ROB
glm(Y ~Volume+Rate,data=vaso,family=binomial(""logit""))
glm(Y ~Volume+Rate,data=vaso,weights=ROB$w.r,family=binomial(""logit""))
</code></pre>

<p>The coefficients from the weighted glm are more similar to the robust regression than the unweighted glm, but is there a way to make them the same? I can get the same results with a robust (rlm) and weighted lm but this doesn't seem to be the case with glm. I haven't looked at the glm robust regression in detail so what I'm asking may be impossible...</p>

<p>Thanks for your help</p>
"
"0.0430930413588572","0.0410650781176591"," 63494","<p>In <code>R</code> I have a categorical variable that I performed logistic regression on and got the following result:</p>

<pre><code>glm(formula = mortality ~ SMOKE, family = binomial, data = c.data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.2155  -0.2155  -0.2155  -0.1860   2.8515  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -4.0483     0.3189 -12.694   &lt;2e-16 ***
SMOKEN        0.2968     0.3559   0.834    0.404    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 492.45  on 2369  degrees of freedom
Residual deviance: 491.72  on 2368  degrees of freedom
AIC: 495.72

Number of Fisher Scoring iterations: 6
</code></pre>

<p>Is the value for the intercept the same as <code>SMOKEY</code> (has a history of smoking)?</p>
"
"0.101414888671788","0.0885887685054121"," 63597","<p>I have data in a dataframe with the following columns: date, time, symbol, price
I am attempting to run the following model in R</p>

<pre><code> price ~ factor(time) + factor(symbol)*factor(time) + 0
</code></pre>

<p>where the first coefficient comes from a dummy variable of the time column in the dataframe and the second coefficient comes from the product of the dummy variable of the time column and the symbol column. I used lm() to attempt to do the model.</p>

<pre><code> fit&lt;-lm(price~factor(time) + factor(symbol)*factor(time) + 0, data=mydata) 
</code></pre>

<p>However, that didn't work as it gets me some really weird coefficients that didn't say anything. I guessing as that was matching effects in some way and not just the straight forward product.
I tried making a new variable by multiplying the product of the factors and then putting it in the regression but that didn't work as r told me that * is not in the possible operations for factor().
So, how can you multiply two factors in a linear regression?
Your help is much appreciated, thank you in advance.</p>

<p>edit: when I say weird I guess I should more say not what I want. I get coefficients for the firm, then coefficients for time then coefficients for firm and time. However, I only want the coefficients for firm and time to impact the model and ultimately I am really interested in what this does to the intraday effects which are approximated using the time dummy variables. If I am getting the interaction effects of the firm, time and then firm and time then this will impact the first coefficient as opposed to only firm and time affecting the first coefficient?</p>
"
"0.0215465206794286","0.0410650781176591"," 63652","<p>I've been looking into the boot package in R and while I have found a number of good primers on how to use it, I have yet to find anything that describes exactly what is happening ""behind the scenes"". For instance, in this <a href=""http://www.ats.ucla.edu/stat/r/dae/zinbreg.htm"">example</a>, the guide shows how to use standard regression coefficients as a starting point for a bootstrap regression but doesn't explain what the bootstrap procedure is actually doing to derive the bootstrap regression coefficients. It appears there is some sort of iterative process that is happening but I can't seem to figure out exactly what is going on.</p>
"
"0.0914141453004008","0.0871121856208561"," 63913","<p>I conducted an experiment in a factorial design: I measured light (PAR) in three herbivore treatments as well as six nutrient treatments. The experiment was blocked.</p>

<p>I've run the linear model as follows (you can download the data from my website to replicate)</p>

<pre><code>dat &lt;- read.csv('http://www.natelemoine.com/testDat.csv')
mod1 &lt;- lm(light ~ Nutrient*Herbivore + BlockID, dat)
</code></pre>

<p>The residual plots look pretty good</p>

<pre><code>par(mfrow=c(2,2))
plot(mod1)
</code></pre>

<p>When I look at the ANOVA table, I see main effects of Nutrient and Herbivore. </p>

<pre><code>anova(mod1)

Analysis of Variance Table 

Response: light 
                    Df  Sum Sq Mean Sq F value    Pr(&gt;F)     
Nutrient             5  4.5603 0.91206  7.1198 5.152e-06 *** 
Herbivore            2  2.1358 1.06791  8.3364 0.0003661 *** 
BlockID              9  5.6186 0.62429  4.8734 9.663e-06 *** 
Nutrient:Herbivore  10  1.7372 0.17372  1.3561 0.2058882     
Residuals          153 19.5996 0.12810                       
--- 
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
</code></pre>

<p>However, the regression table shows non-significant main effects and significant interactions.</p>

<pre><code>summary(mod1)

Call: 
lm(formula = light ~ Nutrient * Herbivore + BlockID, data = dat) 

Residuals: 
     Min       1Q   Median       3Q      Max  
-0.96084 -0.19573  0.01328  0.24176  0.74200  

Coefficients: 
                           Estimate Std. Error t value Pr(&gt;|t|)     
(Intercept)                1.351669   0.138619   9.751  &lt; 2e-16 *** 
Nutrientb                  0.170548   0.160064   1.066  0.28833     
Nutrientc                 -0.002172   0.160064  -0.014  0.98919     
Nutrientd                 -0.163537   0.160064  -1.022  0.30854     
Nutriente                 -0.392894   0.160064  -2.455  0.01522 *   
Nutrientf                  0.137610   0.160064   0.860  0.39129     
HerbivorePaired           -0.074901   0.160064  -0.468  0.64049     
HerbivoreZebra            -0.036931   0.160064  -0.231  0.81784     
... 
Nutrientb:HerbivorePaired  0.040539   0.226364   0.179  0.85811     
Nutrientc:HerbivorePaired  0.323127   0.226364   1.427  0.15548     
Nutrientd:HerbivorePaired  0.642734   0.226364   2.839  0.00513 **  
Nutriente:HerbivorePaired  0.454013   0.226364   2.006  0.04665 *   
Nutrientf:HerbivorePaired  0.384195   0.226364   1.697  0.09168 .   
Nutrientb:HerbivoreZebra   0.064540   0.226364   0.285  0.77594     
Nutrientc:HerbivoreZebra   0.279311   0.226364   1.234  0.21913     
Nutrientd:HerbivoreZebra   0.536160   0.226364   2.369  0.01911 *   
Nutriente:HerbivoreZebra   0.394504   0.226364   1.743  0.08338 .   
Nutrientf:HerbivoreZebra   0.324598   0.226364   1.434  0.15362     
--- 
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.3579 on 153 degrees of freedom 
Multiple R-squared:  0.4176,    Adjusted R-squared:  0.3186  
F-statistic: 4.219 on 26 and 153 DF,  p-value: 8.643e-09 
</code></pre>

<p>I know that this question has been previously <a href=""http://stats.stackexchange.com/questions/20002/regression-vs-anova-discrepancy"">asked and answered</a> in <a href=""http://stats.stackexchange.com/questions/28938/why-do-linear-regression-and-anova-give-different-p-value-in-case-of-consideri"">multiple posts</a>. In the earlier posts, the issue revolved around the different types of SS used in anova() and lm(). However, I don't think that is the issue here. First of all, the design is balanced:</p>

<pre><code>with(dat, tapply(light, list(Nutrient, Herbivore), length))
</code></pre>

<p>Second, using the Anova() option doesn't change the anova table. This isn't a surprise because the design is balanced.</p>

<pre><code>Anova(mod1, type=2)
Anova(mod1, type=3)
</code></pre>

<p>Changing the contrast doesn't change the results (qualitatively). I still get pretty much backwards intepretations from anova() vs. summary().</p>

<pre><code>options(contrasts=c(""contr.sum"",""contr.poly""))
mod2 &lt;- lm(light ~ Nutrient*Herbivore + BlockID, dat)
anova(mod2)
summary(mod2)
</code></pre>

<p>I'm confused because everything I've read on regression not agreeing with ANOVA implicates differences in the way R uses SS for summary() and anova() functions. However, in the balanced design, the SS types are equivalent, and the results here don't change. How can I have completely opposite interpretations depending on which output I use?</p>
"
"0.0861860827177143","0.0821301562353182"," 63927","<p>I am struggling to fit a simple logistic regression for one dependent value (group) by one independent qualitative variable (dilat) measured twice independently (rater).</p>

<p>I try many solutions and think according <a href=""http://www.ats.ucla.edu/stat/mult_pkg/whatstat/"" rel=""nofollow"">http://www.ats.ucla.edu/stat/mult_pkg/whatstat/</a> that the solution is a Mixed Effects Logistic Regression.</p>



<pre class=""lang-r prettyprint-override""><code>glmer_dilat&lt;-glmer(group ~ dilat + (1 | rater), data = ex, family = binomial)
summary(glmer_dilat)
</code></pre>



<pre class=""lang-r prettyprint-override""><code>Generalized linear mixed model fit by the Laplace approximation 
Formula: group ~ dilat + (1 | rater) 
   Data: ex 
   AIC   BIC logLik deviance
 105.5 112.5 -49.74    99.48
Random effects:
 Groups Name        Variance Std.Dev.
 rater  (Intercept)  0        0      
Number of obs: 76, groups: rater, 2

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.8473     0.4880   1.736   0.0825 .
dilat        -1.2827     0.5594  -2.293   0.0219 *
</code></pre>

<p>But the result is the same without !</p>

<pre class=""lang-r prettyprint-override""><code>summary(glm(group ~ dilat, data =ex, family = binomial))

glm(formula = group ~ dilat, family = binomial, data = ex)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.552  -0.999  -0.999   1.367   1.367  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.8473     0.4879   1.736   0.0825 .
dilat        -1.2826     0.5594  -2.293   0.0219 *
</code></pre>

<p>What is the solution?</p>

<p>please find my data set here after applying a dput command to it.</p>

<pre class=""lang-r prettyprint-override""><code>structure(list(id = structure(c(38L, 11L, 6L, 28L, 3L, 30L, 39L, 4L, 8L, 12L, 32L, 29L, 34L, 35L, 33L, 16L, 27L, 5L, 36L, 10L, 9L, 14L, 1L, 13L, 31L, 2L, 17L, 7L, 19L, 20L, 18L, 21L, 23L, 15L, 24L, 25L, 37L, 26L, 38L, 11L, 6L, 28L, 3L, 30L, 39L, 4L, 8L, 12L, 32L, 29L, 34L, 35L, 33L, 16L, 27L, 5L, 36L, 10L, 9L, 14L, 1L, 13L, 31L, 2L, 17L, 7L, 19L, 20L, 18L, 21L, 22L, 23L, 15L, 24L, 37L, 26L), .Label = c(""1038835"", ""2025267"", ""2053954"", ""3031612"", ""40004760"", ""40014515"", ""40040532"", ""40092413"", ""40101857"", ""40105328"", ""4016213"", ""40187296"", ""40203950"", ""40260642"", ""40269263"", ""40300349"", ""40308059"", ""40327146"", ""40333651"", ""40364468"", ""40435267"", ""40440293"", ""40443920"", ""40485124"", ""40609779"", ""40628741"", ""40662695"", ""5025220"", ""E9701737"", ""M/377313"", ""qsc22913"", ""QSC29371"", ""QSC43884"", ""QSC62220"", ""QSC75555"", ""QSC92652"", ""QSD01289"", ""QSD02237"", ""U/FY0296"" ), class = ""factor""), group = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), rater = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), dilat = c(1L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L), midbrain_atroph = c(1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), quadrigemi_atroph = c(1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), hum_sig = c(1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), flower_sig = c(0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), fp_atroph = c(0L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), scp_atroph = c(0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L)), .Names = c(""id"", ""group"", ""rater"", ""dilat"", ""midbrain_atroph"", ""quadrigemi_atroph"", ""hum_sig"", ""flower_sig"", ""fp_atroph"", ""scp_atroph""), class = ""data.frame"", row.names = c(NA, -76L))
</code></pre>
"
"0.068136080998913","0.0649295895722714"," 64242","<p>I have the following toy data:</p>

<pre><code>x &lt;- structure(c(2L, 2L, 3L, 1L, 2L, 3L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 3L, 2L, 2L, 2L, 2L, 3L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 2L, 2L, 
2L, 3L, 2L, 2L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 3L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 3L, 3L, 2L, 3L, 2L, 3L, 
3L, 2L, 2L, 2L, 3L, 2L, 3L, 3L, 2L, 2L, 2L, 2L, 3L, 3L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 2L, 2L, 2L, 2L, 2L, 3L, 
3L, 2L, 2L, 2L, 3L, 3L, 3L), .Label = c(""1"", ""2"", ""3""), class = ""factor"")

y &lt;- structure(c(2L, 2L, 3L, 1L, 2L, 3L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 
2L, 3L, 2L, 1L, 2L, 2L, 3L, 2L, 2L, 3L, 2L, 3L, 2L, 3L, 1L, 2L, 
2L, 3L, 2L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 2L, 3L, 3L, 2L, 2L, 2L, 
2L, 3L, 2L, 2L, 2L, 1L, 2L, 3L, 2L, 2L, 3L, 3L, 1L, 3L, 2L, 3L, 
3L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 2L, 2L, 2L, 3L, 2L, 1L, 
3L, 2L, 2L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 2L, 2L, 2L, 2L, 1L, 3L, 
3L, 3L, 2L, 2L, 3L, 3L, 2L), .Label = c(""1"", ""2"", ""3""), class = ""factor"")

z &lt;- structure(c(1L, 1L, 3L, 2L, 1L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 3L, 1L, 1L, 1L, 1L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 3L, 1L, 1L, 
1L, 3L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 3L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 2L, 1L, 1L, 1L, 1L, 3L, 3L, 1L, 3L, 1L, 3L, 
3L, 1L, 1L, 1L, 3L, 1L, 3L, 3L, 1L, 1L, 1L, 1L, 3L, 3L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 3L, 
3L, 1L, 1L, 1L, 3L, 3L, 3L), .Label = c(""1"", ""2"", ""3""), class = ""factor"")
</code></pre>

<p>I have replaced the 1's in <code>x</code> with 2's in <code>z</code> and vice versa. Now when I do an ordered logistic regression in R with <code>polr</code> (library MASS), I get the following coefficients: </p>

<pre><code>f1 &lt;- polr(y~x,Hess=TRUE)
f2 &lt;- polr(y~z,Hess=TRUE)

coef(summary(f1))
       Value Std. Error  t value
x2  25.95727  0.3028808 85.70127
x3  30.21524  0.5463144 55.30742
1|2 24.02167  0.3480269 69.02246
2|3 27.77068  0.3432316 80.90944

coef(summary(f2))
         Value   Std. Error       t value
z2  -21.495979 6.530398e-10 -3.291680e+10
z3    4.257964 8.119540e-01  5.244095e+00
1|2  -1.935599 3.567345e-01 -5.425880e+00
2|3   1.813399 3.411874e-01  5.314964e+00
</code></pre>

<p>It seems that something is not correct. Why relabeling the levels is changing dramatically the estimates for the SEs?</p>
"
"0.0430930413588572","0.0410650781176591"," 64268","<p>Hi currently I am conducting simple linear regression on two variables for data of different regions. I know that I can use the lmList function to get the coefficients at once for all the regions. But can I generate the Q-Q residual plot for all the regions in one graph with different panels at once? And for the output for lmList function, only the coefficients are displays, without R-square for each regression. How can I see that?</p>

<p>Thanks a lot for help in advance!</p>
"
"0.0806196982594614","0.076825726438694"," 64788","<p>I performed multivariate logistic regression with the dependent variable <code>Y</code> being death at a nursing home within a certain period of entry and got the following results (note if the variables starts in <code>A</code> it is a continuous value while those starting in <code>B</code> are categorical):</p>

<pre><code>Call:
glm(Y ~ A1 + B2 + B3 + B4 + B5 + A6 + A7 + A8 + A9, data=mydata, family=binomial)
Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.0728  -0.2167  -0.1588  -0.1193   3.7788  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  20.048631  6.036637   3.321 0.000896 ***
A1           0.051167   0.016942   3.020 0.002527 ** 
B2          -0.664940   0.304299  -2.185 0.028878 *  
B3          -2.825281   0.633072  -4.463 8.09e-06 ***
B4          -2.547931   0.957784  -2.660 0.007809 ** 
B5          -2.862460   1.385118  -2.067 0.038774 *  
A6          -0.129808   0.041286  -3.144 0.001666 ** 
A7           0.020016   0.009456   2.117 0.034276 *  
A8          -0.707924   0.253396  -2.794 0.005210 ** 
A9           0.003453   0.001549   2.229 0.025837 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 485.10  on 2206  degrees of freedom
Residual deviance: 417.28  on 2197  degrees of freedom
AIC: 437.28

Number of Fisher Scoring iterations: 7

 (Intercept)           A1           B2           B3           B4           B5           A6           A7           A8           A9 
5.093426e+08 1.052499e+00 5.143045e-01 5.929197e-02 7.824340e-02 5.712806e-02 8.782641e-01 1.020218e+00 4.926657e-01 1.003459e+00 

                   2.5 %       97.5 %
(Intercept) 3.703525e+03 7.004944e+13
A1          1.018123e+00 1.088035e+00
B2          2.832698e-01 9.337710e-01
B3          1.714448e-02 2.050537e-01
B4          1.197238e-02 5.113460e-01
B5          3.782990e-03 8.627079e-01
A6          8.099945e-01 9.522876e-01
A7          1.001484e+00 1.039302e+00
A8          2.998207e-01 8.095488e-01
A9          1.000416e+00 1.006510e+00
</code></pre>

<p>As you can see, all of the variables are ""significant"" in that their p values are below the usual threshold of 0.05. However looking at the coefficients, I'm not quite sure what to make of these results. It seems that although these variables contribute to the model, looking at the odds ratios, they don't seem to really seem to have much predictive power. Of note, when I calculated the AUC, I got approximately 0.8. </p>

<p>Can I say that this model is better at predicting against mortality (e.g. predicting that seniors will live past the prescribed period) compared to predicting for mortality?</p>
"
"0.052777981396926","0.0502942438178979"," 65258","<p>Consider the Challenger-Disaster:</p>

<pre><code>Temp &lt;- c(66,67,68,70,72,75,76,79,53,58,70,75,67,67,69,70,73,76,78,81,57,63,70)
Fail &lt;- factor(c(0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,1,1,1))
shuttle &lt;- data.frame(Temp, Fail)
colnames(shuttle) &lt;- c(""Temp"", ""Fail"")
</code></pre>

<p>Now I can fit a logistic model which will explain the ""Fail"" of O-ring seals by Temperature:</p>

<pre><code>fit &lt;- glm(Fail~Temp,data=shuttle, family=binomial); fit
</code></pre>

<p>The R output looks like this:</p>

<pre><code> Call:  glm(formula = Ausfall ~ Temp, family = binomial, data =
 shuttle)

 Coefficients: (Intercept)         Temp  
     15.0429      -0.2322  

 Degrees of Freedom: 22 Total (i.e. Null);  21 Residual Null Deviance:  
 28.27  Residual Deviance: 20.32    AIC: 24.32
</code></pre>

<h3>Questions</h3>

<ul>
<li><strong>In general, how do you predict probabilities for specific data in logistic regressions using R?</strong></li>
<li><strong>Or specifically, what is the command to calculate the probability of a ""Fail"" if temperature is at 37Â°?</strong> (which it was in the night before the Challenger disaster).</li>
</ul>

<p>I thought it would be something like this:</p>

<pre><code>predict(fit, Temp=37)
</code></pre>

<p>but it won't give me ""0.9984243"" (which I calculated myself with:  </p>

<pre><code>exp(15.0429 + (37*(-0.2322))) / 1+ exp(15.0429 + (37*(-0.2322)))
</code></pre>

<p>The method <code>predict</code> returns a matrix of numbers that makes no sense to me.</p>
"
"0.0609427635336005","0.0435560928104281"," 65356","<p>I am interested to know, how to get the coefficient from VGAM package (generalized Poisson regression), because, when I want to get the coefficient, is just can to get the estimate with @ to get the estimate. An other coefficient can to get all of the part from coefficient with ""$""...</p>

<p>My program like this:</p>

<pre><code>library(""VGAM"")  
fit &lt;- vglm(y ~ x1, genpoisson,trace = TRUE)  
summary(fit)  
hasilgp &lt;- summary(fit)  
koefgp &lt;- hasilgp@coefficients  
</code></pre>

<p>And only the estimate can to attach...
Any one help me...</p>
"
"0.0497595580574717","0.0711268017165705"," 65437","<p>I've got a data set which has volume of mail sent from 2001-2012, recorded by age group. That is, for seven different age groups, I have each age group's annual volume of mail sent over that time period.</p>

<p>I've run seven regressions in R--one for each age group--which simply regress volume of mail sent on time. My hypothesis is that these seven relationships are structurally similar (that is, while the coefficients may be different due to the initial volume of mail having been different for each age group in 2001, the <em>relationship</em> between volume of mail sent and time is not significantly different since 2001), or that there are no significant ""age group effects.""</p>

<p>I don't think this can be accomplished through a Chow Test, since the dependent variables are different in all seven regressions (maybe there's a way to run a Chow Test on these data if they are pooled?). Does anybody know of a way I might be able to do this, particularly in R?</p>
"
"0.0914141453004008","0.0871121856208561"," 65690","<p>I fit a logistic on three numeric continuous variables, followed by a categorical factor [Y, N].</p>

<pre><code>logit2A &lt;- glm(DisclosedDriver ~ VehDrvr_Dif+POL_SEQ_NUM+PRMTOTAL+SAFE_DRVR_PLEDGE_FLG, data = DF, family = ""binomial"") 
</code></pre>

<p>Fit looks wonderful.</p>

<pre><code>Coefficients:
                    Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)           -2.204e+00  2.253e-01  -9.782  &lt; 2e-16 ***
VehDrvr_Dif            2.918e-01  1.026e-01   2.845 0.004440 ** 
POL_SEQ_NUM           -1.893e-01  5.617e-02  -3.370 0.000751 ***
PRMTOTAL               1.109e-04  5.526e-05   2.006 0.044804 *  
SAFE_DRVR_PLEDGE_FLGY -7.220e-01  1.633e-01  -4.422 9.76e-06 ***
</code></pre>

<p>So obviously R took the Safe_Drvr_Pledge_Flg categorical factor variable and placed all 'N' values in reference or intercept as opposed to the listed 'Y'.</p>

<p>Now I want to take my fit and calculate the probabilities that my model determines. And here comes the error:</p>

<pre><code>&gt; DF$P_GLM&lt;- predict.glm(logit2A, DF, type=""response"", se.fit=FALSE)
    Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) : 
factor SAFE_DRVR_PLEDGE_FLG has new levels 
</code></pre>

<p>Umm... no it doesn't, because I just fit the model with the exact same data I'm trying to use for the prediction. What's the problem?</p>

<p>Trying to respond to first comment:
Don't know what you mean. I've got 3500 rows of data... It's a logistic regression on 4 continuous variables and one categorical. The categorical has two values, Y or N. My glm fit give the numbers given. I just want to plug it all back in with the predict function and it gives me that error. Here's the categorical variable:</p>

<pre><code> &gt; DF$SAFE_DRVR_PLEDGE_FLG
 [1] Y Y Y Y Y Y Y N Y Y Y Y Y Y Y Y Y Y Y N Y Y N Y Y N Y Y N Y Y Y Y Y Y Y Y Y Y N Y Y N Y Y Y N Y Y Y Y Y N Y Y Y Y Y Y
 [60] Y Y Y Y N Y Y Y Y Y Y Y Y N Y Y Y N N Y N Y Y Y Y Y N Y Y N Y N N Y Y Y N Y Y Y Y N Y Y Y Y Y N Y N Y N Y Y Y Y Y N Y
 [119] N Y Y Y Y Y Y Y Y N Y Y Y Y Y Y N Y Y Y N Y Y Y N Y Y Y N N Y N N N Y N Y Y Y N N Y Y N Y Y Y Y N N Y Y Y Y N N Y N N
 Levels:  N Y
</code></pre>

<p>What do you mean by a working example? The fit works. The probability output of the predict function doesn't...</p>
"
"0.0304713817668003","0.029037395206952"," 65900","<p>I'm not used to using variables in the date format in R. I'm just wondering if it is possible to add a date variable as an explanatory variable in a linear regression model. If it's possible, how can we interpret the coefficient? Is it the effect of one day on the outcome variable? </p>

<p>See my <a href=""https://gist.github.com/pachevalier/5966314"" rel=""nofollow"">gist</a> with an example what I'm trying to do. </p>
"
"0.0304713817668003","0.029037395206952"," 66253","<p>I am running a Cox regression analysis in R where both my predictors are binary (<code>Animal_Number</code>: <em>single</em> vs. <em>paired</em>; and <code>Treatment</code>: <em>control</em> vs. <em>exposed</em>). I therefore have four groups (<em>single control</em>, <em>paired control</em>, <em>single exposed</em> and <em>paired exposed</em>). When I run my Cox regression, I get the following output (excerpt):</p>

<pre><code>                                        coef exp(coef) se(coef)      z Pr(&gt;|z|)   
Animal_NumberPaired                   0.2617    1.2991   0.3303  0.792  0.42829   
TreatmentExposed                     -1.4031    0.2458   0.4715 -2.976  0.00292 **
Animal_NumberPaired:TreatmentExposed  1.0246    2.7861   0.5817  1.762  0.07814 .
</code></pre>

<p>As I understand it, the three lines presented here are simply the levels of the factors being compared to the baseline experimental group, which was set as <em>single control</em>, i.e., the first line is comparing single control animals with paired control animals.</p>

<p>If this is the case, how then does one find information (<code>coefs</code>, <code>z</code>, <code>p-value</code>, etc.) for purely the effect of <code>Animal_Number</code>, <code>Treatment</code> and their interaction? I can seem to do this in SPSS, and get the following:</p>

<pre><code>                           B    SE      Wald    df  Sig.   Exp(B)
Animal_Number             .244  .331    .545    1   .461   1.276
Treatment               -1.283  .472    7.399   1   .007    .277
Animal_Number*Treatment   .959  .582    2.717   1   .099   2.609
</code></pre>

<p>But I want to be able to do it in R. I considered using likelihood ratio tests, to compare full and reduced models, but then information about the coefficients and z is lost. </p>
"
"0.052777981396926","0.0335294958785986"," 66390","<p>Perhaps this is more of a programming question than a stats question, but I'm sure someone here has the answer.  I am new to R and am trying to run a linear regression on multiple subsets (""Cases"") of data in a single file.  I have 50 different cases, so I don't want to have to run 50 different regressions...be nice to automate this.  I have found and experimented with the <code>ddply</code> method, but this, for some reason, returns the same coefficients to me for each case.  Code I'm using is as follows:</p>

<p><code>ddply(MyData, ""Case"", function(x) coefficients(lm(Y~X1+X2+X3, MyData)))</code></p>

<p>Results I get, again, are the same coefficients for each ""Case"".  Any ideas on how I can improve my code so that the regression runs once for each case and gives me unique coefficients for each case?</p>
"
"NaN","NaN"," 66946","<p>When you predict a fitted value from a logistic regression model, how are standard errors computed?  I mean for the <em>fitted values</em>, not for the coefficients (which involves Fishers information matrix).</p>

<p>I only found out how to get the numbers with <code>R</code> (e.g., <a href=""https://stat.ethz.ch/pipermail/r-help/2010-August/248241.html"">here</a> on r-help, or <a href=""http://stackoverflow.com/questions/14423325/confidence-intervals-for-predictions-from-logistic-regression"">here</a> on Stack Overflow), but I cannot find the formula.</p>

<pre><code>pred &lt;- predict(y.glm, newdata= something, se.fit=TRUE)
</code></pre>

<p>If you could provide online source (preferably on a university website), that would be fantastic.</p>
"
"0.0746393370862076","0.0711268017165705"," 67275","<p>I'm reading a paper that does not report the coefficients from two OLS regressions. In both cases there is 1 response variable and 1 predictor variable.  The predictor variable is the same in both cases. I know the subject matter of the paper well, which leads me to believe that the means of the slopes are almost certainly between 0 and 1. Although the slopes for these two regressions are not reported, the author does report that neither slope is significantly different from 0 (p â‰¥ 0.05).</p>

<p>If neither slope is different from 0, but both slopes are between 0 and 1, could the slopes be different from each other?</p>

<p>To try to figure this out, I did a quick test in R.  I used two slopes that were very different (0.99 and 0.01), but chose s.e.'s for each that would make them barely ""insignificant"". To compare the slopes, I used the formula from the answer to <a href=""http://stats.stackexchange.com/questions/55501/test-a-significant-difference-between-two-slope-values"">THIS</a> question.</p>

<pre><code>pnorm( 
  (0.99 - 0.01)/ #difference between means
  sqrt(0.61^2 + 0.0062^2), #sqrt of sum squares of s.e.'s
  lower.tail=FALSE
)
</code></pre>

<p>OK, so this quick-and-dirty test suggests that the two slopes in the author's analysis can't be different.  </p>

<p>Is it necessarily true that if two slopes are between 0 and 1, and neither different from 0, that they cannot be significantly different from each other?</p>
"
"0.068136080998913","0.0649295895722714"," 67460","<p>I fitted the following multinomial regression:</p>

<pre><code>library(car)
p1&lt;-c(1,2,3,4,3,4,3,4,3,2,1,2,1,2,1,2,3,4,3,2,3,4,3,2,2,2,3,4,3,3,4,3,4)

d1&lt;-c(1,2,3,4,3,4,3,4,3,2,1,2,1,2,1,2,3,4,3,2,3,4,3,2,1,2,3,4,3,2,2,2,1)

d1&lt;-as.ordered(d1)

library(nnet)
test&lt;-multinom(p1~d1)
predi&lt;-expand.grid(d1=c(""1"",""2"",""3"",""4""))

pre&lt;-predict(test,predi,type=""probs"")
</code></pre>

<p>The output is a table of the predicted probabilities for every coefficient. I can also order the results for the confidence interval of the coefficents with:</p>

<pre><code>confint(test)
</code></pre>

<p>My question is: is it possible to get the results for the confidence interval for the predicted probabilities? It means for every amount in the ""pre"" output! 
PS: I found a similar question here in 
[""plotting confidence intervals""][1]<a href=""http://stats.stackexchange.com/questions/29044/plotting-confidence-intervals-for-the-predicted-probabilities-from-a-logistic-re"">Plotting confidence intervals for the predicted probabilities from a logistic regression</a></p>

<p>The main answer is perfect for my question, but I do not know how to combine with multinomial regression. 
I hope you understand my bad english :) Thank you for your help</p>
"
"0.0963589698356145","0.0918243061724248"," 67790","<p>I am dealing with a unreplicated factorial design. I have some illustrative examples but I need to simulate some unreplicated factorial designs. I do not how and what to use. Can $R$ handle this?</p>

<p>For example, I would like to analyse a $2^{4}$ factorial design (factors are A, B, C and D) with only one run and 15 contrasts. I have a single column for response. I would like to compare some methods in the literature to see which method detects active effects better. Thus, I set the active effects to have the same magnitude of $1.5\sigma$ and I would like to generate $100$ response vectors using errors that are i.i.d. with $\mathcal N(0 ,1)$. My true model has four active effects and I would like to simulate $100$ response vectors using this true model $y=3+1.5A+1.5B+1.5C+1.5BC$. But I do not know how to generate data like this using R. </p>

<p><img src=""http://i.stack.imgur.com/e7F8i.png"" alt=""An example from Montgomery for $2^4$ unreplicated factorial design""></p>

<hr>

<p>Thanks gung for your reply. I just wrote a simple code before I saw your answer here. I think, I need to build up a bit more R knowledge. Anyway, here it is:</p>

<p>For the analysis of unreplicated factorial designs with $k$ factors and $p=2^{k}-1$ factorial effects (the main effects and interactions), the following model is generally used</p>

<p>\begin{equation}
y=\sum\limits_{i=0}^{p}x_{i}\beta_{i}+\varepsilon_{i}
\end{equation}</p>

<p>So, Firstly I introduced my sign table for $2^{4}$ and $\beta$ coefficients of so-called active effects. </p>

<p>Sign table consists of rows (runs) and columns (contrasts with general mean).
<img src=""http://i.stack.imgur.com/e7F8i.png"" alt=""enter image description here""></p>

<p>And then, I created my regression equation with magnitudes of active effects and zeros of remaining inactive effects. My simulated model, for example, was $y=3+1.5A+1.5B+1.5C+1.5BC$.
<img src=""http://i.stack.imgur.com/9N4GA.png"" alt=""enter image description here""></p>

<p>And then, I run the code below</p>

<pre><code>x=read.csv(""sign2.txt"", header=TRUE)
sign= as.matrix(x)
is.matrix(sign)

y=read.csv(""beta2.txt"", header=TRUE)
beta= as.matrix(y)
is.matrix(beta)

signt=t(sign)

bs=t(beta %*% signt)

epsilon=matrix( rnorm(16*1,mean=0,sd=1), 16, 1) 

response=bs+epsilon
</code></pre>

<p>However, unfortunately, it's for one simulation. I will put a loop command to run the simulation n-times.</p>
"
"0.052777981396926","0.0502942438178979"," 68351","<p>I have a question regarding to the concept of robust standard errors. What I found about that topic is, that one can estimate the robust standard error for regression coefficients to eliminate problems with heteroscedasticity (when one wants to interpret a model). I want to know if there is a way not only to determine robust standard errors of coefficients but also of the standard error of the overall regression (residual standard error). When its possible, how can I calculate such a value in general?</p>

<p>Because I'm using R its also interesting for me if there is a <code>R-function</code> for this problem (I only know the <code>sandwich-package</code> for the normal robust SE of the coefficients).</p>

<p>Thanks.</p>
"
"0.068136080998913","0.0649295895722714"," 69088","<p>For those of you familiar with <a href=""http://en.wikipedia.org/wiki/Exploratory_factor_analysis"" rel=""nofollow"">Exploratory Factor Analysis</a> (EFA) and <a href=""http://en.wikipedia.org/wiki/Random_forest"" rel=""nofollow"">Random Forest</a> (RF), I have recently had an idea of combining these two methods to reduce the number of potential predictor variables for use in a parsimonious binary logistic regression model. For the purposes of this post, assume large <em>n</em> (200k or more) and 1000 potential predictor variables.</p>

<p>To employ this idea, the first step would be to perform an EFA with all potential predictor variables using <code>proc varclus</code>. Additionally, using <code>randomForest</code> to rank all potential predictor variables by <code>IncNodePurity</code> (<a href=""http://en.wikipedia.org/wiki/Gini_coefficient"" rel=""nofollow"">Gini Index</a>). </p>

<p>After these two methods are independently used, I propose retaining the variable with the largest <code>IncNodePurity</code> (from RF) within each factor (from EFA).</p>

<p>Does anyone have any thoughts/concerns with this methodology (or lack thereof) for feature selection? I am aware that this ""picking and choosing"" of methods may be complete garbage, but I had this random thought and wanted to share. Thanks!</p>
"
"0.0430930413588572","0.0410650781176591"," 69208","<p>After creating a <a href=""http://en.wikipedia.org/wiki/Random_forest"" rel=""nofollow"">Random Forest</a> object using <code>randomForest</code> with around 500 candidate variables, I used <code>importance(object)</code> to display <em>IncNodePurity</em> for each of the candidate variables in relation to the binary outcome of interest (Payment/No Payment). </p>

<p>I am aware that <em>IncNodePurity</em> is the total decrease in node impurities, measured by the <a href=""http://en.wikipedia.org/wiki/Gini_coefficient"" rel=""nofollow"">Gini Index</a> from splitting on the variable, averaged over all trees. What I don't know is what should be the cutoff for candidate variables to be retained after making use of <code>randomForest</code> for feature selection in regards to binary logistic regression models. For example, the smallest <em>IncNodePurity</em> among my 498 variables is 0.03, whereas the largest <em>IncNodePurity</em> is 96.68.
In summary, I have one main question:</p>

<p>Is there a cutoff for <em>IncNodePurity</em>? If yes, what is it?</p>

<p>If no, how do you determine the cutoff? Do you simply take 10 candidate variables with the largest <em>IncNodePurity</em> if you want a model with only 10 predictor variables?</p>

<p>Any thoughts or references are greatly appreciated. Thanks!</p>
"
"0.101062140164153","0.0963061447907242"," 69371","<p>I have a set of predictors in a linear regression, as well as three control variables. The issue here is that one of my variables of interest is only statistically significant if the control variables are included in the final model. However, the control variables themselves are not statistically significant.</p>

<p>Here is how the multicollinearity of all my variables look like (including control variables):</p>

<pre><code> &gt; vif(lm(return ~ EQ + EFF + SIZE + MOM + MSCR + UMP, data = as.data.frame(port.df)))
       EQ      EFF     SIZE      MOM     MSCR      UMP 
 3.687171 3.481672 2.781901 1.064312 1.438596 1.003408

 &gt; vif(lm(return ~ EQ + MOM + MSCR, data = as.data.frame(port.df)))
       EQ      MOM     MSCR 
 1.359992 1.048142 1.412658 
</code></pre>

<p>My variables of interest are <strong>EQ, MOM and MSCR</strong>, and the control variables are <em>EFF, SIZE and UMP</em>. EQ is only significant if the three control var are included, and becomes insignificant when they are not:</p>

<ul>
<li><p>Here are the coefficients (1rst row) and t-stats (2nd row) when control variables are included (notice that EQ is statistically significant)</p>

<pre><code>       intercept           EQ          EFF        SIZE         MOM       MSCR          UMP
[1,] 0.005206246 -0.006310531 0.0001229055 0.004125551 0.007738259 0.00473377 5.838596e-06
[2,] 1.866628909 -1.746583234 0.0388823612 1.178460997 2.145062820 2.08131100 1.994863e-01
</code></pre></li>
<li><p>Now, here is the result of the regression when the control variables are excluded (notice that EQ is NOT statistically significant anymore)</p>

<pre><code>       intercept           EQ         MOM       MSCR
[1,] 0.007313402 -0.002111833 0.007128606 0.00668364
[2,] 2.652662996 -0.595391117 2.036985378 2.80177366
</code></pre></li>
</ul>

<p>The problem is that when I include my control variables, all my variables of interest are significant, but my control variables are not.</p>

<p>Which variables should I include in my final model? How should I structure my final model then, given the fact that the model will be used for forecasting?</p>

<p>Thank you,</p>
"
"0.0812570180448007","0.0871121856208561"," 69524","<p>I am trying to fit a nonlinear regression model in R using <code>nls()</code>. I have a form of the equation I want to fit to:</p>

<p>$$y = (a \times x_{1}^c +b \times x_{2}^d) (x_{3}^e)$$</p>

<p>where the coefficients to be found in regression are a,b,c,d, and e. My data is output from a simulation model where $x_{1}$, $x_{2}$, and $x_{3}$ are all integers from $0$ to $10$, with the condition that $x_{1} + x_{2} + x_{3} \le 10$. $y$ is also integer valued and ranges from $0$ to roughly $1000$. The objective is to fit these data to a rate function that will be used in a Markov Chain.</p>

<p>When I try to fit this regression model directly using <code>nls()</code>, my <code>nlsResiduals</code> plot looks like this:</p>

<p><img src=""http://i.stack.imgur.com/6scJ3.png"" alt=""nls residuals""></p>

<p>I know that autocorrelated residuals are problematic, and that non-normal residuals can also be problematic. How can I fix this problem? I was thinking of using transforms on the data like</p>

<p>$$\log(y) = \log((a \times x_{1}^c +b \times x_{2}^d) (x_{3}^e))$$</p>

<p>or</p>

<p>$$y^{1/n} = ((a \times x_{1}^c +b \times x_{2}^d) (x_{3}^e))^{1/n}$$  where $n &gt; 1$. I've noticed if $n$ increases, my autocorrelation graph and QQ-plot look ""better"" (i.e., more scattered and more normal, respectively). </p>

<p>Both of these seem to correct a lot (but not all) of the autocorrelated residuals, and help to make the residuals more normally distributed. Am I on the right track here, or am I committing some cardinal sin in statistics? Once I settle on a transformation, how can I tell which is best?</p>

<p>Any help, suggestions, or comments are very appreciated.</p>
"
"0.035185320931284","0.0502942438178979"," 69528","<p>This is my R code and running result:(See below)</p>

<p>How to judge is the linear regression model appropriate for this data set? Except R^2 value, Can the</p>

<p>p-value in last row of the running result mean something? What does this      p-value mean and can it mean the linear regression model appropriate for this data set? Why?</p>

<p>Thanks in advance.</p>

<pre><code>x=c(7,12,10,10,14,25,30,25,18,10,4,6)    
y=c(128,213,191,178,205,446,540,547,324,117,75,107)    
list(x,y)    
reg1 &lt;- lm(y~x)    
summary(reg1)    
plot(x, y)
abline(reg1)    
reg2 &lt;- lm(y~x-1)    
reg2    
summary(reg2)
#------------------    
Call:
lm(formula = y ~ x)

Residuals:
    Min      1Q  Median      3Q     Max 
-55.805 -21.085   3.139  14.946  80.859 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  -22.753     21.846  -1.041    0.322    
x             19.556      1.335  14.652 4.38e-08 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 37.23 on 10 degrees of freedom
Multiple R-squared:  0.9555,    Adjusted R-squared:  0.951 
F-statistic: 214.7 on 1 and 10 DF,  p-value: 4.38e-08
</code></pre>
"
"0.0861860827177143","0.0821301562353182"," 70249","<p>I would like to use GLM and Elastic Net to select those relevant features + build a linear regression model (i.e., both prediction and understanding, so it would be better to be left with relatively few parameters). The output is continuous. It's $20000$ genes per $50$ cases. I've been reading about the <code>glmnet</code> package, but I'm not 100% sure about the steps to follow:</p>

<ol>
<li><p>Perform CV to choose lambda:<br>
<code>cv &lt;- cv.glmnet(x,y,alpha=0.5)</code><br>
<strong>(Q1)</strong> given the input data, would you choose a different alpha value?<br>
<strong>(Q2)</strong> do I need to do something else before build the model?</p></li>
<li><p>Fit the model:<br>
<code>model=glmnet(x,y,type.gaussian=""covariance"",lambda=cv$lambda.min)</code><br>
<strong>(Q3)</strong> anything better than ""covariance""?<br>
<strong>(Q4)</strong> If lambda was chosen by CV, why does this step need <code>nlambda=</code>?<br>
<strong>(Q5)</strong> is it better to use <code>lambda.min</code> or <code>lambda.1se</code>?</p></li>
<li><p>Obtain the coefficients, to see which parameters have fallen out ("".""):<br>
<code>predict(model, type=""coefficients"")</code></p>

<p>In the help page there are many <code>predict</code> methods (e.g., <code>predict.fishnet</code>, <code>predict.glmnet</code>, <code>predict.lognet</code>, etc). But any ""plain"" predict as I saw on an example.<br>
<strong>(Q6)</strong> Should I use <code>predict</code> or <code>predict.glmnet</code> or other?</p></li>
</ol>

<p>Despite what I've read about regularization methods, I'm quite new in R and in these statistical packages, so it's difficult to be sure if I'm adapting my problem to the code. Any suggestions will be welcomed.</p>

<p><strong>UPDATE</strong><br>
<a href=""http://www.jstatsoft.org/v28/i05/paper"">Based on</a> ""As previously noted, an object of class train contains an element called <code>finalModel</code>, which is the fitted model with the tuning parameter values selected by resampling. This object can be used in the traditional way to generate predictions for new samples, using that model's
predict function.""  </p>

<p>Using <code>caret</code> to tune both alpha and lambda:    </p>

<pre><code>  trc = trainControl(method=cv, number=10)  
  fitM = train(x, y, trControl = trC, method=""glmnet"")  
</code></pre>

<p>Does <code>fitM</code> replace previous step 2? If so, how to specify the glmnet options (<code>type.gaussian=""naive"",lambda=cv$lambda.min/1se</code>) now?<br>
And the following <code>predict</code> step, can I replace <code>model</code> to <code>fitM</code>?</p>

<p>If I do  </p>

<pre><code>  trc = trainControl(method=cv, number=10)  
  fitM = train(x, y, trControl = trC, method=""glmnet"")  
  predict(fitM$finalModel, type=""coefficients"")
</code></pre>

<p>does it make sense at all or am I incorrectly mixing both package vocabulary?</p>
"
"0.0430930413588572","0.0410650781176591"," 70482","<p>I've got some ordinal variables b and a and a categorized variable c. I would like to fit a multinomial logit regression from the library car.
I tried to ignore the ordinal scale.
 I have the following data:</p>

<pre><code> a&lt;-c( 3, 4,   4,   4,   3, 4,   3, 3, 4,   2, 2, 4,   3, 3, 3, 1,   3, 2, 2, 3, 3, 1,   3, 2, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 2, 3, 1,   2, 2, 2, 2, 3, 2, 3, 4,   4,   3, 3, 2, 2, 3, 3, 3, 2, 1,   1,   1,   1,   1,   1,   2, 3, 4,   3, 3, 4,   3, 4,   3, 2, 3, 3, 3, 3, 3, 4,   3, 4,   3, 3, 2, 3, 3, 3, 3, 2, 2, 3, 3, 3, 2, 3, 2, 3, 1,   2, 2, 1,   1,   4,   3, 3, 2, 2, 2, 2, 2, 2, 3, 4,   4,   4,   3, 3, 3, 3, 3, 4,   4,   3, 3, 2, 3, 3, 3, 3, 4,   3, 4,   2, 2, 3, 3, 3, 2, 2, 3, 2, 4,   2, 2, 2, 2, 2, 1,   2, 2, 1,   1,   3, 4,   3, 3, 2, 3, 3, 3, 3, 2, 3, 3, 3, 2, 1)


 c&lt;-c(5 ,3 ,4 ,3 ,4 ,4 ,2 ,2 ,3 ,4 ,2 ,5, 3, 5, 4 ,3 ,2 ,4 ,4 ,4, 4 ,4, 4, 2 ,3, 4 ,2 ,3 ,3 ,3 ,4 ,3 ,3 ,2 ,2 ,3 ,3 ,3 ,3 ,4 ,2 ,4 ,3, 3, 3, 4, 4, 3, 3 ,2 ,3 ,3 ,3 ,3, 4 ,4, 4, 3, 2, 2 ,4 ,4 ,3 ,3 ,2 ,2 ,1 ,2 ,2 ,2 ,1 ,2 ,5 ,2 ,3 ,3 ,2, 4 ,3 ,1 ,2 ,3 ,2 ,3 ,3 ,3 ,3 ,3 ,3 ,2 ,2 ,2 ,2 ,3 ,2 ,4 ,3, 3 ,2 ,3, 2, 4, 3, 3, 3 ,3 ,4 ,2 ,2 ,4 ,3 ,3 ,3 ,3 ,3 ,2 ,3, 3 ,3, 3, 4 ,4 ,4 ,1 ,3 ,3 ,3 ,4 ,4 ,4 ,3 ,2 ,4 ,4 ,2 ,4 ,4 ,4 ,4 ,2 ,3 ,3, 2, 2 ,3 ,2 ,3 ,4 ,5 ,2, 3 ,3 ,2 ,3 ,2 ,2 ,3 ,2 ,2 ,4 ,4 ,3 ,3 ,2, 4 ,4 ,2 ,4 ,3 ,4, 4, 3 ,2 ,3)
b&lt;-c(3 ,2, 2, 2, 3, 2, 3, 3, 4, 1, 2, 2, 4, 2, 3, 1, 3, 1, 2, 4, 2, 1, 3, 2, 2, 2, 1, 3, 3, 3, 2, 2, 2, 2, 1, 3, 1, 3, 2, 3, 1 ,3 ,3 ,2, 2, 3, 1, 3, 2, 2, 2, 2, 2, 2, 3, 4, 3, 3, 2, 1, 4, 3 ,3 ,2 ,2, 1, 2, 2, 2, 2, 1, 2, 5, 3, 3, 4, 3, 4, 1, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 2, 3, 2 ,4 ,2 ,3 ,2 ,2 ,2 ,4 ,2 ,2 ,2 ,2 ,2 ,2, 1, 5 ,4 ,3 ,2, 2 ,2 ,2 ,2 ,4 ,2 ,2 ,4 ,3 ,3 ,1 ,2 ,2 ,2 ,2 ,2 ,2, 2, 2, 2, 2, 2 ,2 ,2 ,2 ,2 ,2 ,2, 2, 2, 2 ,2 ,2 ,2 ,2 ,5 ,4 ,3 ,2 ,1 ,1 ,1 ,4 ,3 ,2 ,2 ,3 ,3 ,3 ,2 ,2 ,2 ,2 ,2, 3, 2 ,2 ,2 ,2 ,2 ,1)
</code></pre>

<p>now I ignored the ordinal scale and treated them as factors to fit the multinomial logit regression </p>

<pre><code>require(car)
a&lt;-as.factor(a)    
b&lt;-as.factor(b)
c&lt;-as.factor(c)
multinom(formula = a ~ b + c)

Call:
multinom(formula = a ~ b + c)

Coefficients:
  (Intercept)        b2       b3       b4        b5         c2         c3         c4        c5
2   0.3410779  1.009797 41.80056 45.22081 -13.02923 -0.5229982  0.9216514  0.2170273 -18.03928
3  -1.4697131  2.698228 44.91938 47.04268 -16.24570 -0.7341395  0.7088424  1.2495310  20.70641
4 -46.0095393 33.603384 75.13911 79.00502  56.91264 -7.4198320 13.0220759 14.2526951  33.85774

Std. Errors:
  (Intercept)        b2        b3        b4           b5           c2        c3        c4           c5
2   1.2654428 0.6530052 0.4659520 0.5495402          NaN 1.337075e+00 1.4180126 1.4993079 8.028986e-16
3   1.6649206 0.9361438 0.5123106 0.5879588 2.446562e-15 1.640462e+00 1.7003411 1.7558418 8.601766e-01
4   0.3399454 0.4767032 0.3699569 0.4144527 3.321501e-11 6.973173e-08 0.6549144 0.6953767 8.601766e-01

Residual Deviance: 328.1614
AIC: 382.1614  
</code></pre>

<p>I think I found the mistake....the column b5 is empty for a1 and a2. </p>

<pre><code>table(b,c,a)
, , a = 1

   c
b    1  2  3  4  5
  1  0  3  2  2  0
  2  1  7  1  0  0
  3  0  0  0  0  0
  4  0  0  0  0  0
  5  0  0  0  0  0

, , a = 2

   c
b    1  2  3  4  5
  1  1  5  2  2  0
  2  1 12 21  4  0
  3  0  1  6  1  0
  4  0  2  1  1  0
  5  0  0  0  0  0
</code></pre>

<p>But do you know how to solve this problem?</p>
"
"0.052777981396926","0.0502942438178979"," 70598","<p>I am estimating an instrumental variables linear regression that has a large number of indicator (factor) variables.  I don't particularly care about the coefficient estimates on those indicator variables.  In Stata's ivreg2 package there is a ""partial"" option that applies the Frisch-Waugh-Lovell theorem to orthogonalize the dependent and exogenous variables to the indicator variables.  After this transformation the indicator variables are not estimated because they do not affect the coefficients on the variables I am interested in.</p>

<p>My question is, is there something like this in R?  It doesn't have to be part of an IV regression package but I am looking to orthogonalize one set of variables to another set of variables.  This seems like something that would have already been implemented.  Thanks.</p>
"
"0.0457070726502004","0.0435560928104281"," 70675","<p>How should I read the output of the function <code>ar</code> in R. For example, take this VAR model:</p>

<pre><code>library(tseries)
data(USeconomic)
US.ar &lt;- ar(cbind(GNP, M1), method=""ols"",
            dmean=T, intercept=F)
</code></pre>

<p>(from the book <code>Introductory Time Series with R</code> by Cowpertwait)</p>

<p>So <code>Us.ar</code> produces the following output:</p>

<pre><code>&gt; US.ar

Call:
ar(x = cbind(GNP, M1), method = ""ols"", dmean = T, intercept = F)

$ar
, , 1

         GNP    M1
GNP  1.27181 1.167
M1  -0.03383 1.588

, , 2

         GNP      M1
GNP -0.00423 -0.6942
M1   0.06354 -0.4839

, , 3

         GNP      M1
GNP -0.26715 -0.5103
M1  -0.02859 -0.1295


$var.pred
       GNP    M1
GNP 618.69 16.38
M1   16.38 23.90
</code></pre>

<p>and <code>US.ar$ar</code> gives this other representation:</p>

<pre><code>&gt; US.ar$ar

, , GNP

           GNP          M1
1  1.271812104 -0.03383385
2 -0.004229937  0.06353801
3 -0.267154022 -0.02858942

, , M1

         GNP         M1
1  1.1674655  1.5876695
2 -0.6941813 -0.4838919
3 -0.5103451 -0.1294549
</code></pre>

<p>As I understand it, <code>,,1</code> refers to the order of the coefficient in the autoregression model. In the second representation, <code>, , M1</code> refers to the columns, whereas <code>GNP         M1</code> describe rows and the numbers describe the order. I think this output is describing the following model:</p>

<p>$$GNP_{t} = 1.27 GNP_{t-1} + 1.167 M1_{t-1} - 0.004 GNP_{t-2} - 0.6942 M1_{t-2} - 0.2671 GNP_{t-3} - 0.5104 M1_{t-3}$$</p>

<p>$$M1_{t} = - 0.03 GNP_{t-1} + 1.58 M1_{t-1} + 0.063 GNP_{t-2} - 0.48M1_{t-2} - 0.02 GNP_{t-3} - 0.12 M1_{t-3}$$</p>

<p>Therefore, in matrix notation, this should be equal to:
$$
\begin{pmatrix}
GNP_{t} \\
M1_{t} \\
\end{pmatrix} = \left[ \begin{pmatrix}
1.27 &amp; 1.167 \\
-0.03 &amp; 1.58 \\
\end{pmatrix}x + 
\begin{pmatrix}
-0.004 &amp; -0.6942 \\
0.063 &amp; -0.48 \\
\end{pmatrix}x^{2} +
\begin{pmatrix}
-0.267 &amp; -0.5104 \\
-0.02 &amp; -0.12 \\
\end{pmatrix}x^{3} \right] 
\begin{pmatrix}
GNP_{t} \\
M1_{t} \\
\end{pmatrix}$$</p>

<p>However, the book says that the model is this:</p>

<p><img src=""http://i.stack.imgur.com/lXHJA.png"" alt=""enter image description here""></p>

<p>As you can see, cross-terms are flipped. Is this a mistake?</p>
"
"0.0609427635336005","0.0871121856208561"," 70699","<p>I have an independent variable called ""quality""; this variable has 3 modalities of response (bad quality; medium quality; high quality). I want to introduce this independent variable into my multiple linear regression. When I have a binary independent variable (dummy variable, I can code <code>0</code> / <code>1</code>) it is easy to introduce it into a multiple linear regression model.</p>

<p>But with 3 modalities of response, I have tried to code this variable like this :</p>

<pre><code>Bad quality      Medium quality      High quality

     0                1                  0
     1                0                  0
     0                0                  1
     0                1                  0
</code></pre>

<p>But there is a problem when I try to do my multiple linear regression: the modality <code>Medium quality</code> gives me <code>NA</code>:  </p>

<pre><code>Coefficients: (1 not defined because of singularities) 
</code></pre>

<p>How can I code this variable ""quality"" with 3 modalities? Do I have to create a variable as a factor (<code>factor</code> in <code>R</code>) but then can I introduce this factor in a multiple linear regression?</p>
"
"0.0545088647991304","0.0519436716578171"," 70764","<p>I am looking at a time series which has no obvious trend, but seems to have an intercept a little above zero. The results I get for <a href=""http://rss.acs.unt.edu/Rdoc/library/urca/html/ur.df.html"" rel=""nofollow""><code>ur.df</code></a> function in R is the following:</p>

<pre><code>logprice_df &lt;- ur.df(test3, lags = 1, type= 'trend')
summary(logprice_df)

############################################### 
# Augmented Dickey-Fuller Test Unit Root Test # 
############################################### 

Test regression trend 


Call:
lm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.50614 -0.04394  0.00134  0.03859  0.64408 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  1.841e-02  8.268e-03   2.226  0.02626 *  
z.lag.1     -1.573e-02  5.635e-03  -2.791  0.00537 ** 
tt           9.234e-06  1.080e-05   0.855  0.39272    
z.diff.lag   1.411e-01  3.364e-02   4.195 3.01e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.07512 on 865 degrees of freedom
Multiple R-squared: 0.02651,    Adjusted R-squared: 0.02314 
F-statistic: 7.852 on 3 and 865 DF,  p-value: 3.572e-05 


Value of test-statistic is: -2.791 2.6012 3.8997 

Critical values for test statistics: 
      1pct  5pct 10pct
tau3 -3.96 -3.41 -3.12
phi2  6.09  4.68  4.03
phi3  8.27  6.25  5.34
</code></pre>

<p>The problem is that I do not understand how one shall interpret these values. What is <code>F-statistic</code>? And what is the probabilities below <code>Pr(&gt;|t|)</code> in the first table, and also <code>Value of test-statistic</code>?</p>

<p>I really appreciate any help.</p>
"
"0.0691025985081098","0.0658506226617377"," 70866","<p>I have a modelling dilemma. I am creating a model that attempts to predict demand (leads not sales) based upon the correlation to advertising spend. We know that without advertising spend, demand is driven by seasonality. So our models include seasonal factors like month of the year and even day of the week. 
If I were building a regular linear regression model, I would fit a linear regression model to a training dataset, to get estimates of the coefficients of the seasonal factors and advertising spend to demand. In order to get an estimate of future baseline demand, I would forecast demand using all the coefficients from the model and then I would estimate a baseline by setting adspend equal to zero. 
For ARIMA models, there are additional factors such as AR and MA terms. Would I estimate my baseline the same way by just setting the coefficient on advertising spend equal to zero?
Thanks for any thoughts.</p>
"
"0.0304713817668003","0.029037395206952"," 71001","<p>I would like to test a linear hypothesis in a median regression model similar to example below.</p>

<pre><code>require(AER);require(car);require(quantreg)
data(""CPS1985"")

#Regular linear model
model.lm &lt;- lm(wage ~ ethnicity + age*gender,CPS1985)
summary(model.lm)
linearHypothesis(model.lm,""age + age:genderfemale = 0"")

#Quantile regression
model.quant &lt;- rq(wage ~ ethnicity + age*gender,tau=0.5,data=CPS1985)
</code></pre>

<p>I'm not sure, however, how to execute a linear Hypothesis for <code>model.quant</code> to test that the sum of the coefficients on age and age:gender are equal to zero.</p>
"
"0.0430930413588572","0.0410650781176591"," 71117","<p>Using R and ggplot, I would like to plot my output variable with one of my predictor variables adjusted for the other variables in the model. (I'm asking this question here, because I don't actually know if this is statistically a good idea! Please let me know if not!)</p>

<p>A model using dtcars data (nabbed from <a href=""http://stats.stackexchange.com/questions/60872/how-to-split-r-squared-between-predictor-variables-in-multiple-regression"">here</a>).</p>

<pre><code>mod &lt;- lm(mpg ~ drat + wt, mtcars)
</code></pre>

<p>I would like to produce this graph, but with <code>wt</code> adjusted for <code>drat</code>.</p>

<pre><code>require(ggplot2)
ggplot(mtcars, aes(wt, mpg)) + geom_point()
</code></pre>

<p><img src=""http://i.stack.imgur.com/rSTYl.png"" alt=""GGplot example""></p>

<p>Is there a way I can get the adjusted values for <code>wt</code> out of <code>mod</code>? Does this even make sense to do? Or should I just settle with presenting univariate graphs and coefficients in tables?</p>
"
"0.052777981396926","0.0502942438178979"," 71150","<p>I have two regression models and I want to subtract one beta coefficient in model 1 from model2. </p>

<p>Obviously I can just do this on a calculator, but does anyone know of a test I could use in R to test the difference for me, and give me back confidence intervals or standard errors. </p>

<p>I've tried running a t.test:</p>

<pre><code>t.test(betamod1,betamod2)  
</code></pre>

<p>and get this error</p>

<pre><code>not enough 'x' observations
</code></pre>

<p>I have also tried trying to calculate a z-score using </p>

<pre><code>z = (betamod1-betamod2) / sqrt(stanerrbetamod1^2 + stanerrbetamod2^2)
</code></pre>

<p>Thank you in advance</p>
"
"0.0609427635336005","0.0435560928104281"," 71832","<p>I have a Poisson model with varying densities:</p>

<pre><code>set.seed(1)
df = data.frame(density = 1:5, events = rpois(2000, 1:5))
</code></pre>

<p>If I regress on this, I get that the intercept is approximately <code>log(3)</code>, which makes sense because 3 is the mean of 1:5.</p>

<pre><code>glm(events ~ 1, df, family = poisson)  # returns 1.089
</code></pre>

<p>But now suppose I want to read back the coefficients of density:</p>

<pre><code>glm(events ~ as.factor(density), df, family = poisson)
</code></pre>

<p>(For simplicity I've used <code>density</code> as both the ID of the field and its density.) I would expect the coefficient of <code>density[i]</code> to be <code>log(3-i)</code> because the intercept would still be 3. However, it doesn't seem like the intercept remains 3 - in this case, the intercept is set to <code>log(1)</code>. In playing around with this, it seems like glm sets the intercept to be the coefficient of the first factor.</p>

<p>Now I'm starting to wonder what the p values in a glm regression indicate. Is the null hypothesis that <code>density[i]</code> is the same as the intercept (aka <code>density[1]</code>)? Or is it that <code>density[i] = mean(density)</code>?</p>
"
"0.0861860827177143","0.0821301562353182"," 71924","<p><em><a href=""http://stackoverflow.com/q/19186966/1414455"">Cross-posted from SO.</a></em></p>

<p>I am trying to replicate the results of <code>bgtest</code> from the <a href=""http://cran.r-project.org/web/packages/lmtest/lmtest.pdf"" rel=""nofollow""><code>lmtest</code></a> R package.</p>

<p>I am using the following dataset:</p>

<pre><code>           rs   month   r20
1    2.365042  1952m3  4.33
2    2.317500  1952m4  4.23
3    2.350833  1952m5  4.36
4    2.451833  1952m6  4.57
5    2.466167  1952m7  4.36
6    2.468417  1952m8  4.11
7    2.485583  1952m9  4.20
8    2.415125 1952m10  4.19
9    2.389875 1952m11  4.15
10   2.418167 1952m12  4.22
11   2.396042  1953m1  4.13
12   2.401042  1953m2  4.10
13   2.400833  1953m3  4.04
14   2.383500  1953m4  3.94
15   2.366708  1953m5  3.95
16   2.365625  1953m6  4.02
17   2.348583  1953m7  3.98
18   2.334375  1953m8  3.94
19   2.133542  1953m9  3.78
20   2.097375 1953m10  3.80
21   2.097708 1953m11  3.78
22   2.130583 1953m12  3.83
23   2.096000  1954m1  3.79
24   2.064042  1954m2  3.79
25   2.115083  1954m3  3.76
26   2.047333  1954m4  3.71
27   1.713875  1954m5  3.65
28   1.606167  1954m6  3.61
29   1.561667  1954m7  3.35
30   1.613292  1954m8  3.36
31   1.621083  1954m9  3.35
32   1.587667 1954m10  3.35
33   1.637792 1954m11  3.38
34   1.865917 1954m12  3.51
35   2.356417  1955m1  3.64
36   3.810000  1955m2  3.85
37   3.797000  1955m3  3.83
38   3.906000  1955m4  4.15
39   3.937000  1955m5  4.21
40   3.969000  1955m6  4.33
41   3.971000  1955m7  4.47
42   4.005000  1955m8  4.84
43   4.072000  1955m9  4.68
44   4.071000 1955m10  4.50
45   4.104000 1955m11  4.64
46   4.072000 1955m12  4.70
47   4.071000  1956m1  4.84
48   5.218000  1956m2  4.87
49   5.165000  1956m3  5.02
50   5.008000  1956m4  4.85
51   4.955000  1956m5  5.12
52   5.136000  1956m6  5.25
53   4.977000  1956m7  5.27
54   4.027000  1956m8  5.20
55   5.091000  1956m9  5.35
56   4.991000 1956m10  5.33
57   5.020000 1956m11  5.50
58   4.858000 1956m12  5.29
59   4.553000  1957m1  4.91
60   4.148000  1957m2  4.93
61   4.099000  1957m3  5.08
62   3.914000  1957m4  5.11
63   3.921000  1957m5  5.43
64   3.854000  1957m6  5.55
65   3.845000  1957m7  5.60
66   4.121000  1957m8  5.75
67   6.605000  1957m9  5.98
68   6.603000 1957m10  5.84
69   6.459000 1957m11  5.89
70   6.375000 1957m12  5.81
71   6.127000  1958m1  5.66
72   6.014000  1958m2  5.65
73   5.523000  1958m3  5.64
74   5.179000  1958m4  5.45
75   4.816000  1958m5  5.46
76   4.294000  1958m6  5.45
77   4.159000  1958m7  5.46
78   3.760000  1958m8  5.49
79   3.625000  1958m9  5.36
80   3.584000 1958m10  5.35
81   3.305000 1958m11  5.36
82   3.152000 1958m12  5.36
83   3.107000  1959m1  5.20
84   3.276000  1959m2  5.20
85   3.287000  1959m3  5.24
86   3.283000  1959m4  5.22
87   3.382000  1959m5  5.28
88   3.452000  1959m6  5.18
89   3.484000  1959m7  5.13
90   3.488000  1959m8  5.21
91   3.472000  1959m9  5.33
92   3.386000 1959m10  5.06
93   3.400000 1959m11  5.04
94   3.687000 1959m12  5.21
95   4.538000  1960m1  5.34
96   4.554000  1960m2  5.43
97   4.621000  1960m3  5.53
98   4.652000  1960m4  5.59
99   4.556000  1960m5  5.62
100  5.681000  1960m6  5.92
101  5.546000  1960m7  5.97
102  5.588000  1960m8  5.95
103  5.565000  1960m9  5.97
104  5.090000 1960m10  5.97
105  4.639000 1960m11  5.97
106  4.349000 1960m12  6.01
107  4.165000  1961m1  6.01
108  4.399000  1961m2  6.04
109  4.485000  1961m3  6.05
110  4.407000  1961m4  6.01
111  4.436000  1961m5  6.08
112  4.537000  1961m6  6.33
113  6.688000  1961m7  6.52
114  6.700000  1961m8  6.63
115  6.552000  1961m9  6.65
116  5.727000 1961m10  6.33
117  5.389000 1961m11  6.34
118  5.403000 1961m12  6.41
119  5.242000  1962m1  6.35
120  5.531000  1962m2  6.26
121  4.405000  1962m3  6.25
122  4.052000  1962m4  6.24
123  3.816000  1962m5  6.25
124  3.921000  1962m6  6.24
125  3.887000  1962m7  5.98
126  3.752000  1962m8  5.77
127  3.635000  1962m9  5.27
128  3.858000 1962m10  5.37
129  3.689000 1962m11  5.42
130  3.717000 1962m12  5.36
131  3.491000  1963m1  5.54
132  3.426000  1963m2  5.74
133  3.756000  1963m3  5.69
134  3.709000  1963m4  5.50
135  3.635000  1963m5  5.31
136  3.702000  1963m6  5.28
137  3.761000  1963m7  5.20
138  3.723000  1963m8  5.22
139  3.674000  1963m9  5.21
140  3.745000 1963m10  5.26
141  3.739000 1963m11  5.51
142  3.721000 1963m12  5.63
143  3.758000  1964m1  5.64
144  4.307000  1964m2  5.85
145  4.302000  1964m3  5.76
146  4.302000  1964m4  5.93
147  4.384000  1964m5  5.90
148  4.464000  1964m6  5.97
149  4.654000  1964m7  6.02
150  4.656000  1964m8  6.00
151  4.703000  1964m9  6.00
152  4.698000 1964m10  6.06
153  6.630000 1964m11  6.23
154  6.627000 1964m12  6.41
155  6.543000  1965m1  6.41
156  6.442000  1965m2  6.43
157  6.549000  1965m3  6.53
158  6.375000  1965m4  6.61
159  6.364000  1965m5  6.76
160  5.542000  1965m6  6.78
161  5.630000  1965m7  6.80
162  5.559000  1965m8  6.65
163  5.559000  1965m9  6.35
164  5.440000 1965m10  6.37
165  5.395000 1965m11  6.40
166  5.521000 1965m12  6.59
167  5.483000  1966m1  6.52
168  5.620000  1966m2  6.61
169  5.604000  1966m3  6.77
170  5.638000  1966m4  6.78
171  5.659000  1966m5  6.82
172  5.728000  1966m6  7.03
173  6.679000  1966m7  7.29
174  6.726000  1966m8  7.41
175  6.747000  1966m9  7.29
176  6.513000 1966m10  6.96
177  6.738000 1966m11  6.97
178  6.527000 1966m12  6.78
179  6.080000  1967m1  6.58
180  6.035000  1967m2  6.49
181  5.495000  1967m3  6.50
182  5.412000  1967m4  6.46
183  5.248000  1967m5  6.65
184  5.275000  1967m6  6.86
185  5.345000  1967m7  6.92
186  5.291000  1967m8  6.90
187  5.475000  1967m9  6.98
188  5.726000 1967m10  7.00
189  7.553000 1967m11  7.22
190  7.484000 1967m12  7.20
191  7.520000  1968m1  7.28
192  7.374000  1968m2  7.28
193  7.108000  1968m3  7.29
194  7.080000  1968m4  7.34
195  7.241000  1968m5  7.50
196  7.242000  1968m6  7.87
197  7.059000  1968m7  7.63
198  6.945000  1968m8  7.63
199  6.577000  1968m9  7.64
200  6.493000 1968m10  7.70
201  6.789000 1968m11  7.93
202  6.777000 1968m12  8.17
203  6.728000  1969m1  8.47
204  7.711000  1969m2  8.61
205  7.782000  1969m3  8.81
206  7.798000  1969m4  8.90
207  7.850000  1969m5  9.46
208  7.880000  1969m6  9.31
209  7.830000  1969m7  9.19
210  7.790000  1969m8  9.49
211  7.811000  1969m9  9.21
212  7.743000 1969m10  8.95
213  7.738000 1969m11  9.29
214  7.650000 1969m12  9.04
215  7.550000  1970m1  9.03
216  7.600000  1970m2  8.79
217  7.270000  1970m3  8.75
218  6.940000  1970m4  8.94
219  6.190000  1970m5  9.40
220  6.870000  1970m6  9.58
221  6.850000  1970m7  9.33
222  6.820000  1970m8  9.19
223  6.820000  1970m9  9.28
224  6.810000 1970m10  9.15
225  6.810000 1970m11  9.51
226  6.820000 1970m12  9.62
227  6.790000  1971m1  9.51
228  6.750000  1971m2  9.35
229  6.660000  1971m3  9.07
230  5.920000  1971m4  9.07
231  5.650000  1971m5  9.03
232  5.590000  1971m6  9.08
233  5.570000  1971m7  9.22
234  5.750000  1971m8  8.96
235  4.830000  1971m9  8.50
236  4.630000 1971m10  8.51
237  4.480000 1971m11  7.79
238  4.360000 1971m12  8.10
239  4.360000  1972m1  7.93
240  4.370000  1972m2  7.90
241  4.340000  1972m3  8.16
242  4.300000  1972m4  8.26
243  4.270000  1972m5  8.60
244  5.210000  1972m6  9.32
245  5.600000  1972m7  9.23
246  5.790000  1972m8  9.36
247  6.440000  1972m9  9.54
248  6.740000 1972m10  9.46
249  6.880000 1972m11  9.45
250  7.760000 1972m12  9.62
251  8.210000  1973m1  9.56
252  8.080000  1973m2  9.65
253  8.070000  1973m3 10.01
254  7.670000  1973m4  9.93
255  7.330000  1973m5 10.02
256  7.060000  1973m6 10.15
257  8.270000  1973m7 10.60
258 10.910000  1973m8 11.30
259 10.970000  1973m9 11.55
260 10.770000 1973m10 11.28
261 11.730000 1973m11 12.00
262 12.460000 1973m12 12.50
263 12.090000  1974m1 12.89
264 11.920000  1974m2 13.50
265 11.950000  1974m3 13.68
266 11.520000  1974m4 14.21
267 11.360000  1974m5 13.80
268 11.230000  1974m6 14.38
269 11.200000  1974m7 14.88
270 11.240000  1974m8 15.29
271 11.060000  1974m9 14.95
272 10.930000 1974m10 15.68
273 10.980000 1974m11 16.75
274 10.990000 1974m12 17.18
275 10.590000  1975m1 16.02
276  9.880000  1975m2 14.58
277  9.500000  1975m3 13.43
278  9.260000  1975m4 13.89
279  9.470000  1975m5 14.53
280  9.430000  1975m6 14.41
281  9.710000  1975m7 13.93
282 10.430000  1975m8 13.87
283 10.360000  1975m9 13.79
284 11.420000 1975m10 14.66
285 11.100000 1975m11 14.81
286 10.820000 1975m12 14.79
287  9.990000  1976m1 13.79
288  8.760000  1976m2 13.46
289  8.460000  1976m3 13.88
290  9.060000  1976m4 13.77
291 10.440000  1976m5 13.59
292 10.960000  1976m6 14.09
293 10.870000  1976m7 14.16
294 10.880000  1976m8 14.33
295 12.050000  1976m9 14.79
296 14.000000 1976m10 16.03
297 14.140000 1976m11 15.79
298 13.780000 1976m12 15.48
299 12.730000  1977m1 14.48
300 11.020000  1977m2 13.93
301  9.920000  1977m3 13.25
302  8.240000  1977m4 13.05
303  7.400000  1977m5 12.69
304  7.450000  1977m6 13.26
305  7.430000  1977m7 13.62
306  6.540000  1977m8 13.12
307  5.680000  1977m9 11.88
308  4.530000 1977m10 10.98
309  4.960000 1977m11 11.28
310  6.370000 1977m12 11.16
311  5.810000  1978m1 11.06
312  5.960000  1978m2 11.75
313  5.930000  1978m3 11.72
314  6.730000  1978m4 12.39
315  8.400000  1978m5 12.72
316  9.170000  1978m6 12.79
317  9.220000  1978m7 12.72
318  8.900000  1978m8 12.55
319  8.980000  1978m9 12.64
320  9.860000 1978m10 12.91
321 11.510000 1978m11 13.16
322 11.570000 1978m12 13.22
323 11.860000  1979m1 13.68
324 12.630000  1979m2 13.94
325 11.350000  1979m3 12.35
326 11.320000  1979m4 11.68
327 11.350000  1979m5 11.94
328 12.570000  1979m6 12.69
329 13.320000  1979m7 12.25
330 13.320000  1979m8 12.30
331 13.380000  1979m9 12.60
332 13.380000 1979m10 13.16
333 15.330000 1979m11 14.54
334 15.900000 1979m12 14.72
335 15.790000  1980m1 14.17
336 16.140000  1980m2 14.45
337 16.180000  1980m3 14.70
338 16.170000  1980m4 14.27
339 16.090000  1980m5 14.01
340 15.800000  1980m6 13.78
341 14.550000  1980m7 13.07
342 14.860000  1980m8 13.58
343 14.400000  1980m9 13.38
344 14.290000 1980m10 13.12
345 13.950000 1980m11 13.22
346 13.070000 1980m12 13.67
347 12.820000  1981m1 13.96
348 12.090000  1981m2 13.89
349 11.530000  1981m3 13.68
350 11.330000  1981m4 13.64
351 11.350000  1981m5 14.31
352 12.090000  1981m6 14.57
353 13.150000  1981m7 15.14
354 13.420000  1981m8 15.09
355 13.960000  1981m9 15.59
356 15.550000 1981m10 15.95
357 14.080000 1981m11 15.44
358 14.510000 1981m12 15.65
359 14.160000  1982m1 15.58
360 13.300000  1982m2 14.74
361 12.480000  1982m3 13.72
362 12.890000  1982m4 13.96
363 12.530000  1982m5 13.69
364 12.230000  1982m6 13.56
365 11.280000  1982m7 13.20
366 10.080000  1982m8 12.23
367  9.910000  1982m9 11.40
368  8.910000 1982m10 10.50
369  9.220000 1982m11 10.64
370  9.960000 1982m12 11.34
371 10.590000  1983m1 11.60
372 10.740000  1983m2 11.50
373 10.470000  1983m3 10.97
374  9.840000  1983m4 10.56
375  9.700000  1983m5 10.65
376  9.470000  1983m6 10.39
377  9.370000  1983m7 10.95
378  9.340000  1983m8 11.07
379  9.160000  1983m9 10.67
380  8.840000 1983m10 10.61
381  8.840000 1983m11 10.29
382  8.870000 1983m12 10.35
383  8.870000  1984m1 10.28
384  8.850000  1984m2 10.42
385  8.430000  1984m3 10.23
386  8.380000  1984m4 10.40
387  8.820000  1984m5 10.93
388  8.860000  1984m6 11.15
389 10.970000  1984m7 11.67
390 10.210000  1984m8 10.98
391 10.020000  1984m9 10.78
392  9.850000 1984m10 10.69
393  9.230000 1984m11 10.32
394  9.100000 1984m12 10.46
395 10.550000  1985m1 10.96
396 12.690000  1985m2 11.06
397 12.930000  1985m3 10.90
398 11.930000  1985m4 10.68
399 11.940000  1985m5 10.88
400 11.890000  1985m6 10.70
401 11.390000  1985m7 10.44
402 10.960000  1985m8 10.37
403 11.060000  1985m9 10.39
404 11.050000 1985m10 10.22
405 11.110000 1985m11 10.37
406 11.150000 1985m12 10.45
407 11.980000  1986m1 10.80
408 12.020000  1986m2 10.40
409 11.060000  1986m3  9.39
410  9.990000  1986m4  8.76
411  9.700000  1986m5  9.00
412  9.320000  1986m6  9.23
413  9.450000  1986m7  9.37
414  9.390000  1986m8  9.41
415  9.610000  1986m9  9.97
416 10.250000 1986m10 10.62
417 10.630000 1986m11 10.80
418 10.660000 1986m12 10.69
419 10.520000  1987m1 10.09
420 10.290000  1987m2  9.83
421  9.350000  1987m3  9.16
422  9.430000  1987m4  9.12
423  8.460000  1987m5  8.82
424  8.540000  1987m6  8.90
425  8.840000  1987m7  9.23
426  9.790000  1987m8  9.20
427  9.690000  1987m9  9.98
428  9.450000 1987m10  9.88
429  8.430000 1987m11  9.20
430  8.190000 1987m12  9.57
431  8.370000  1988m1  9.57
432  8.790000  1988m2  9.38
433  8.270000  1988m3  9.12
434  7.740000  1988m4  9.12
435  7.540000  1988m5  9.27
436  8.880000  1988m6  9.32
437 10.050000  1988m7  9.51
438 11.130000  1988m8  9.47
439 11.530000  1988m9  9.60
440 11.540000 1988m10  9.23
441 12.070000 1988m11  9.30
442 12.540000 1988m12  9.46
443 12.450000  1989m1  9.35
444 12.390000  1989m2  9.15
445 12.410000  1989m3  9.26
446 12.470000  1989m4  9.52
447 12.540000  1989m5  9.52
448 13.590000  1989m6  9.88
449 13.290000  1989m7  9.53
450 13.320000  1989m8  9.37
451 13.440000  1989m9  9.62
452 14.460000 1989m10  9.81
453 14.450000 1989m11  9.99
454 14.500000 1989m12  9.96
455 14.500000  1990m1 10.28
456 14.450000  1990m2 10.72
457 14.570000  1990m3 11.46
458 14.590000  1990m4 11.77
459 14.500000  1990m5 11.49
460 14.380000  1990m6 11.01
461 14.320000  1990m7 11.03
462 14.310000  1990m8 11.41
463 14.260000  1990m9 11.32
464 13.370000 1990m10 11.12
465 12.920000 1990m11 10.94
466 12.960000 1990m12 10.40
467 13.000000  1991m1 10.22
468 12.390000  1991m2  9.89
469 11.640000  1991m3 10.06
470 11.250000  1991m4  9.99
471 10.840000  1991m5 10.15
472 10.720000  1991m6 10.34
473 10.520000  1991m7 10.10
474 10.200000  1991m8  9.89
475  9.660000  1991m9  9.54
476  9.860000 1991m10  9.62
477  9.980000 1991m11  9.68
478 10.100000 1991m12  9.56
479  9.970000  1992m1  9.34
480  9.800000  1992m2  9.21
481 10.100000  1992m3  9.54
482  9.970000  1992m4  9.33
483  9.430000  1992m5  8.99
484  9.420000  1992m6  9.02
485  9.430000  1992m7  8.90
486  9.650000  1992m8  9.13
487  9.160000  1992m9  9.12
488  7.470000 1992m10  9.24
489  6.490000 1992m11  8.84
490  6.390000 1992m12  8.84
491  6.050000  1993m1  8.92
492  5.370000  1993m2  8.63
493  5.380000  1993m3  8.33
494  5.330000  1993m4  8.39
495  5.300000  1993m5  8.60
496  5.190000  1993m6  8.39
497  5.130000  1993m7  7.96
498  5.060000  1993m8  7.39
499  5.170000  1993m9  7.18
500  5.150000 1993m10  7.09
501  4.950000 1993m11  7.06
502  4.870000 1993m12  6.46
503  4.890000  1994m1  6.41
504  4.760000  1994m2  6.83
505  4.830000  1994m3  7.47
506  4.880000  1994m4  7.83
507  4.810000  1994m5  8.24
508  4.880000  1994m6  8.55
509  5.090000  1994m7  8.41
510  5.340000  1994m8  8.52
511  5.390000  1994m9  8.72
512  5.440000 1994m10  8.63
513  5.630000 1994m11  8.53
514  5.870000 1994m12  8.44
515  5.930000  1995m1  8.61
516  6.160000  1995m2  8.52
517  6.090000  1995m3  8.50
518  6.300000  1995m4  8.39
519  6.200000  1995m5  8.18
520  6.370000  1995m6  8.16
521  6.620000  1995m7  8.36
522  6.590000  1995m8  8.24
523  6.520000  1995m9  8.09
524  6.530000 1995m10  8.34
525  6.380000 1995m11  8.01
526  6.220000 1995m12  7.94
</code></pre>

<p>which is saved as <code>ukrates.csv</code>.</p>

<p>Here is the code to attempt to reproduce the <code>bgtest</code> module.</p>

<pre><code>rm(list = ls())

library(zoo)
library(lmtest)
library(dynlm)

# read in the data
dfUK = read.csv('./data/ukrates.csv', header = TRUE)
summary(dfUK)

# run the time series regression
zooUK = zoo(dfUK[, c('rs', 'r20')], order.by = as.yearmon(dfUK$month, 
                                                              '%Ym%m'))
    zooUKAug = merge(zooUK, 
                     'drs' = diff(zooUK$rs, 1), 
                 'ldr20' = lag(diff(zooUK$r20, 1), -1))
lmUK2 = dynlm(drs ~ ldr20, data = zooUKAug)

# Breusch-Godfrey regression
zooUKBG = merge(zooUKAug, 'resid' = resid(lmUK2))
lmBG = dynlm(as.formula(paste('resid',  
                              '~', 
                              attr(lmUK2$terms, 'term.labels'),
                              ' + L(resid, 1)')),
             data = zooUKBG) 

# BG test using lmtest package
bgtest(lmUK2, order = 1, type = 'Chisq') # 14.5614

# attempt to recreate BG-test 
length(lmBG$residuals)*
      sum(lmBG$fitted^2)/sum(lmBG$residuals^2)
</code></pre>

<p>This is based on the following code for computing the chi-squared statistic directly from the <code>bgtest</code> function code:</p>

<pre><code>&gt; bgtest
function (formula, order = 1, order.by = NULL, type = c(""Chisq"", 
    ""F""), data = list(), fill = 0) 
{
    dname &lt;- paste(deparse(substitute(formula)))
    if (!inherits(formula, ""formula"")) {
        X &lt;- if (is.matrix(formula$x)) 
                formula$x
        else model.matrix(terms(formula), model.frame(formula))
        y &lt;- if (is.vector(formula$y)) 
                formula$y
        else model.response(model.frame(formula))
    }
    else {
        mf &lt;- model.frame(formula, data = data)
        y &lt;- model.response(mf)
        X &lt;- model.matrix(formula, data = data)
    }
    if (!is.null(order.by)) {
        if (inherits(order.by, ""formula"")) {
            z &lt;- model.matrix(order.by, data = data)
            z &lt;- as.vector(z[, ncol(z)])
        }
        else {
            z &lt;- order.by
        }
        X &lt;- as.matrix(X[order(z), ])
        y &lt;- y[order(z)]
    }
    n &lt;- nrow(X)
    k &lt;- ncol(X)
    order &lt;- 1:order
    m &lt;- length(order)
    resi &lt;- lm.fit(X, y)$residuals
        Z &lt;- sapply(order, function(x) c(rep(fill, length.out = x), 
            resi[1:(n - x)]))
        if (any(na &lt;- !complete.cases(Z))) {
            X &lt;- X[!na, , drop = FALSE]
            Z &lt;- Z[!na, , drop = FALSE]
            y &lt;- y[!na]
            resi &lt;- resi[!na]
            n &lt;- nrow(X)
        }
        auxfit &lt;- lm.fit(cbind(X, Z), resi)
        cf &lt;- auxfit$coefficients
    vc &lt;- chol2inv(auxfit$qr$qr) * sum(auxfit$residuals^2)/auxfit$df.residual
    names(cf) &lt;- colnames(vc) &lt;- rownames(vc) &lt;- c(colnames(X), 
        paste(""lag(resid)"", order, sep = ""_""))
    switch(match.arg(type), Chisq = {
        bg &lt;- n * sum(auxfit$fitted^2)/sum(resi^2)
            p.val &lt;- pchisq(bg, m, lower.tail = FALSE)
            df &lt;- m
            names(df) &lt;- ""df""
        }, F = {
            uresi &lt;- auxfit$residuals
        bg &lt;- ((sum(resi^2) - sum(uresi^2))/m)/(sum(uresi^2)/(n - 
            k - m))
        df &lt;- c(m, n - k - m)
        names(df) &lt;- c(""df1"", ""df2"")
        p.val &lt;- pf(bg, df1 = df[1], df2 = df[2], lower.tail = FALSE)
    })
    names(bg) &lt;- ""LM test""
    RVAL &lt;- list(statistic = bg, parameter = df, method = paste(""Breusch-Godfrey test for serial correlation of order up to"", 
        max(order)), p.value = p.val, data.name = dname, coefficients = cf, 
        vcov = vc)
    class(RVAL) &lt;- c(""bgtest"", ""htest"")
    return(RVAL)
}
&lt;environment: namespace:lmtest&gt;
</code></pre>

<p>I am wondering why I am getting the different results.</p>
"
"0.114013470672957","0.108647984268766"," 72421","<p>I have data for a network of weather stations across the United States. This gives me a data frame that contains date, latitude, longitude, and some measured value. Assume that data are collected once per day and driven by regional-scale weather (no, we are not going to get into that discussion). </p>

<p>I'd like to show graphically how simultaneously-measured values are correlated across time and space. My goal is to show the regional homogeneity (or lack thereof) of the value that is being investigated. </p>

<h2>Data set</h2>

<p>To start with, I took a group of stations in the region of Massachusetts and Maine. I selected sites by latitude and longitude from an index file that is available on NOAA's FTP site.</p>

<p><img src=""http://i.stack.imgur.com/aZm4N.jpg"" alt=""enter image description here""></p>

<p>Straight away you see one problem: there are lots of sites that have similar identifiers or are very close. FWIW, I identify them using both the USAF and WBAN codes. Looking deeper in to the metadata I saw that they have different coordinates and elevations, and data stop at one site then start at another. So, because I don't know any better, I have to treat them as separate stations. This means the data contains pairs of stations that are very close to each other.</p>

<h2>Preliminary Analysis</h2>

<p>I tried grouping the data by calendar month and then calculating the ordinary least squares regression between different pairs of data. I then plot the correlation between all pairs as a line connecting the stations (below). The line color shows the value of R2 from the OLS fit. The figure then shows how the 30+ data points from January, February, etc. are correlated between different stations in the area of interest. </p>

<p><img src=""http://i.stack.imgur.com/X4YZI.jpg"" alt=""correlation between daily data during each calendar month""></p>

<p>I've written the underlying codes so that the daily mean is only calculated if there are data points every 6-hour period, so data should be comparable across sites.</p>

<h3>Problems</h3>

<p>Unfortunately, there is simply too much data to make sense of on one plot. That can't be fixed by reducing the size of the lines. </p>

<p>I've tried plotting the correlations between the nearest neighbors in the region, but that turns into a mess very quickly. The facets below show the network without correlation values, using $k$ nearest neighbors from a subset of the stations. This figure was just to test the concept.
<img src=""http://i.stack.imgur.com/NWzm2.jpg"" alt=""enter image description here""></p>

<p>The network appears to be too complex, so I think I need to figure out a way to reduce the complexity, or apply some kind of spatial kernel.</p>

<p>I am also not sure what is the most appropriate metric to show correlation, but for the intended (non-technical) audience, the correlation coefficient from OLS might just be the simplest to explain. I may need to present some other information like the gradient or standard error as well.</p>

<h3>Questions</h3>

<p>I'm learning my way into this field and R at the same time, and would appreciate suggestions on:</p>

<ol>
<li>What's the more formal name for what I'm trying to do? Are there some helpful terms that would let me find more literature? My searches are drawing blanks for what must be a common application.</li>
<li>Are there more appropriate methods to show the correlation between multiple data sets separated in space?</li>
<li>... in particular, methods that are easy to show results from visually?</li>
<li>Are any of these implemented in R?</li>
<li>Do any of these approaches lend themselves to automation?</li>
</ol>
"
"0.101414888671788","0.0885887685054121"," 72468","<p>I am interested in finding a procedure to simulate data that are consistent with a specified mediation model.  According to the general linear structural equation model framework for testing mediation models first outlined by <a href=""https://umdrive.memphis.edu/grelyea/public/PUBH%207152-Stat%20Methods%20II/Chapter%2010/Mediation/Baron_&amp;_Kenny_1986.pdf"" rel=""nofollow"">Barron and Kenny (1986)</a> and described elsewhere such as <a href=""http://www.psor.ucl.ac.be/personal/yzerbyt/Judd%20et%20al.%20HRMSP%202013.pdf"" rel=""nofollow"">Judd, Yzerbyt, &amp; Muller (2013)</a>, mediation models for outcome $Y$, mediator $\newcommand{\med}{\rm med} \med$, and predictor $X$ and are governed by the following three regression equations:
\begin{align}
Y    &amp;= b_{11} + b_{12}X + e_1                \tag{1}  \\
\med &amp;= b_{21} + b_{22}X + e_2                \tag{2}  \\
Y    &amp;= b_{31} + b_{32}X + b_{32} \med + e_3  \tag{3}
\end{align}
The indirect effect or mediation effect of $X$ on $Y$ through $\med$ can either be defined as $b_{22}b_{32}$ or, equivalently, as $b_{12}-b_{32}$.  Under the old framework of testing for mediation, mediation was established by testing $b_{12}$ in equation 1, $b_{22}$ in equation 2, and $b_{32}$ in equation 3.</p>

<p>So far, I have attempted to simulate values of $\med$ and $Y$ that are consistent with values of the various regression coefficients using <code>rnorm</code> in <code>R</code>, such as the code below:</p>

<pre><code>x   &lt;- rep(c(-.5, .5), 50)
med &lt;- 4 + .7 * x + rnorm(100, sd = 1) 

# Check the relationship between x and med
mod &lt;- lm(med ~ x)
summary(mod)

y &lt;- 2.5 + 0 * x + .4 * med + rnorm(100, sd = 1)

# Check the relationships between x, med, and y
mod &lt;- lm(y ~ x + med)
summary(mod)

# Check the relationship between x and y -- not present
mod &lt;- lm(y ~ x)
summary(mod)
</code></pre>

<p>However, it seems that sequentially generating $\med$ and $Y$ using equations 2 and 3 is not enough, since I am left with no relationship between $X$ and $Y$ in regression equation 1 (which models a simple bivariate relationship between $X$ and $Y$) using this approach.  This is important because one definition of the indirect (i.e., mediation) effect is $b_{12}-b_{32}$, as I describe above.</p>

<p>Can anyone help me find a procedure in R to generate variables $X$, $\med$, and $Y$ that satisfy constraints that I set using equations 1, 2, and 3?</p>
"
"0.0861860827177143","0.0821301562353182"," 72569","<p>What does it mean when two random effects are highly or perfectly correlated?<br>
That is, in R when you call summary on a mixed model object, under ""Random effects"" ""corr"" is 1 or -1.</p>

<pre><code>summary(model.lmer) 
Random effects:
Groups   Name                    Variance   Std.Dev.  Corr                 
popu     (Intercept)             2.5714e-01 0.5070912                      
          amdclipped              4.2505e-04 0.0206167  1.000               
          nutrientHigh            7.5078e-02 0.2740042  1.000  1.000        
          amdclipped:nutrientHigh 6.5322e-06 0.0025558 -1.000 -1.000 -1.000
</code></pre>

<p>I know this is bad and indicates that the random effects part of the model is too complex, but I'm trying to understand</p>

<ul>
<li>1)what is doing on statistically  </li>
<li>2)what is going on practically with
the structure of the response variables.</li>
</ul>

<p><strong>Example</strong></p>

<p>Here is an example based on ""<a href=""http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;ved=0CDYQFjAC&amp;url=http://glmm.wdfiles.com/local--files/examples/Banta_ex.pdf&amp;ei=hTNYUpuzBu7J4APN5YHYBg&amp;usg=AFQjCNG65VjvqOLeYLFxJZnzmlMevgEbuA&amp;bvm=bv.53899372,d.dmg"">GLMMs in action: gene-by-environment interaction in total fruit production of wild populations of Arabidopsis thaliana</a>""
by Bolker et al</p>

<p>Download data</p>

<pre><code>download.file(url = ""http://glmm.wdfiles.com/local--files/trondheim/Banta_TotalFruits.csv"", destfile = ""Banta_TotalFruits.csv"")
dat.tf &lt;- read.csv(""Banta_TotalFruits.csv"", header = TRUE)
</code></pre>

<p>Set up factors</p>

<pre><code>dat.tf &lt;- transform(dat.tf,X=factor(X),gen=factor(gen),rack=factor(rack),amd=factor(amd,levels=c(""unclipped"",""clipped"")),nutrient=factor(nutrient,label=c(""Low"",""High"")))
</code></pre>

<p>Modeling log(total.fruits+1) with ""population"" (popu) as random effect</p>

<pre><code>model.lmer &lt;- lmer(log(total.fruits+1) ~ nutrient*amd + (amd*nutrient|popu), data= dat.tf)
</code></pre>

<p>Accessing the Correlation matrix of the random effects show that everything is perfectly correlated</p>

<pre><code>attr(VarCorr(model.lmer)$popu,""correlation"")

                         (Intercept) amdclipped nutrientHigh amdclipped:nutrientHigh
(Intercept)                       1          1            1                      -1
amdclipped                        1          1            1                      -1
nutrientHigh                      1          1            1                      -1
amdclipped:nutrientHigh          -1         -1           -1                       1
</code></pre>

<p>I understand that these are the correlation coefficients of two vectors of random effects coefficients, such as</p>

<pre><code>cor(ranef(model.lmer)$popu$amdclipped, ranef(model.lmer)$popu$nutrientHigh)
</code></pre>

<p>Does a high correlation mean that the two random effects contain redundant information?  Is this analogous to multicollinearity in multiple regression when a model with highly correlated predictors should be simplified?</p>
"
"0.0710998907892006","0.0677539221495548"," 73567","<p>I have a logistic regression model with several variables and one of those variables (called x3 in my example below) is not significant. However, x3 should remain in the model because it is scientifically important.</p>

<p>Now, x3 is continuous and I want to create a plot of the predicted probability vs x3. Even though x3 is not statistically significant, it has an effect on my outcome and therefore it has an effect on the predicted probability. This means that I can see from the graph, that the probability changes with increasing x3. However, how should I interpret the graph and the change in the predicted probability, given that x3 is indeed not statistically significant?</p>

<p>Below is a simulated data in R set to illustrate my question. The graph also contains a 95% confidence interval for the predicted probability (dashed lines):</p>

<pre><code>&gt; set.seed(314)
&gt; n &lt;- 300
&gt; x1 &lt;- rbinom(n,1,0.5)
&gt; x2 &lt;- rbinom(n,1,0.5)
&gt; x3 &lt;- rexp(n)
&gt; logit &lt;- 0.5+0.9*x1-0.5*x2
&gt; prob &lt;- exp(logit)/(1+exp(logit))
&gt; y &lt;- rbinom(n,1,prob)
&gt; 
&gt; model &lt;- glm(y~x1+x2+x3, family=""binomial"")
&gt; summary(model)

Call:
glm(formula = y ~ x1 + x2 + x3, family = ""binomial"")

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.0394  -1.1254   0.5604   0.8554   1.4457  

Coefficients:
            Estimate Std. Error z value Pr(    &gt;|z|)    
(Intercept)   1.1402     0.2638   4.323 1.54e-05 ***
x1            0.8256     0.2653   3.112  0.00186 ** 
x2           -1.1338     0.2658  -4.266 1.99e-05 ***
x3           -0.1478     0.1249  -1.183  0.23681    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 373.05  on 299  degrees of freedom
Residual deviance: 341.21  on 296  degrees of freedom
AIC: 349.21

Number of Fisher Scoring iterations: 3

&gt; 
&gt; dat &lt;- data.frame(x1=1, x2=1, x3=seq(0,5,0.1))
&gt; preds &lt;- predict(model, dat,type = ""link"", se.fit = TRUE )
&gt; critval &lt;- 1.96
&gt; upr &lt;- preds$fit + (critval * preds$se.fit)
&gt; lwr &lt;- preds$fit - (critval * preds$se.fit)
&gt; fit &lt;- preds$fit
    &gt; 
    &gt; fit2 &lt;- mod$family$linkinv(fit)
    &gt; upr2 &lt;- mod$family$linkinv(upr)
    &gt; lwr2 &lt;- mod$family$linkinv(lwr)
    &gt; 
    &gt; plot(dat$x3, fit2, lwd=2, type=""l"", main=""Predicted Probability"", ylab=""Probability"", xlab=""x3"", ylim=c(0,1.00))
&gt; lines(dat$x3, upr2, lty=2)
    &gt; lines(dat$x3, lwr2, lty=2)
</code></pre>

<p><img src=""http://i.stack.imgur.com/ljW7W.png"" alt=""enter image description here""></p>

<p>Thanks!</p>

<p>Emilia</p>
"
"0.052777981396926","0.0335294958785986"," 74549","<p>This is from the book <em>The statistical sleuth--A course in methods of Data analysis</em> Chapter 20, Exercise 12(c)-(e). I am using logistic regression to predict carrier with possible predictors <code>CK</code> and <code>H</code>. Here is my solution:</p>

<pre><code>Carrier &lt;- c(0,0,0,0,0,1,1,1,1,1)  
CK      &lt;- c(52,20,28,30,40,167,104,30,65,440)  
H       &lt;- c(83.5,77,86.5,104,83,89,81,108,87,107)  
logCK   &lt;- log(CK)  
fit4    &lt;- glm(Carrier~logCK+H, family=""binomial"", control=list(maxit=100))  
Warning message:  
glm.fit: fitted probabilities numerically 0 or 1 occurred   
summary(fit4)
## 
## Call:
## glm(formula = Carrier ~ logCK + H, family = ""binomial"", control = list(maxit = 100))
## 
## Deviance Residuals: 
##        Min          1Q      Median          3Q         Max  
## -1.480e-05  -2.110e-08   0.000e+00   2.110e-08   1.376e-05  
##
## Coefficients:  
##              Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept)   -2292.8  4130902.8  -0.001        1  
## logCK           315.6   589675.2   0.001        1  
## H                11.5    21279.6   0.001        1
</code></pre>

<p>This results appear to be weird, because it seems that all coefficients are not significant.  Also the next question is to do a drop-in-deviance test for this full model and the reduced model that neither of <code>logCK</code> and <code>H</code> is useful predictor. I get:  </p>

<pre><code>fit5 &lt;- glm(Carrier~1, family=""binomial"")  
1-pchisq(deviance(fit5)-deviance(fit4), df.residual(fit5)-df.residual(fit4))  
## [1] 0.0009765625
</code></pre>

<p>So the p-value indicates that at least one of <code>logCK</code> and <code>H</code> is useful. Then I'm stuck at the next question, it asks me to calculate odds ratio for a woman with (CK, H)=(300,100) over one with (CK, H)=(80, 85).  </p>

<p>But how can I get a meaningful result with all coefficients in this model ranging so wildly? Is there anything wrong with the way I did this logistic regression? Are there any remedial measures?  </p>
"
"0.0609427635336005","0.0580747904139041"," 74628","<p>I have completed analysis on the effects of two drug treatments over a period of time on the CD4 cell count of a number of patients. I have taken the square root of the initial CD4 count as a covariate and I have taken a summary measure of the 'slopes' for each patient. </p>

<p>My model is the following:</p>

<pre><code>&gt; slopes.aov &lt;- aov(individual.slope.trans[-29] ~ sqrt(initialCD4)[-29] + treatment.fac[-29])

&gt; summary(slopes.aov)
                   Df Sum Sq  Mean Sq F value Pr(&gt;F)
sqrt(initialCD4)[-29]   1 0.0060 0.006027   0.990  0.322
treatment.fac[-29]      1 0.0082 0.008184   1.344  0.249
Residuals             109 0.6638 0.006090         
</code></pre>

<p>I am quite new to data analysis I am not quite sure how to interpret this model?</p>

<p>Can I still use the regression coefficients to describe my summary measure even though we have no significance. I am really struggling with this. Also how can we describe to effect of the covariate.?</p>

<p>I understand that I have no evidence to suggest that the treatments have an effect on the 'slopes' (my summary measure) and so I have no evidence to say that one treatment is performing better than another.      </p>
"
"0.0845124072264899","0.104695817324578"," 76490","<h2>Background</h2>

<p>A laboratory wants to evaluate whether a certain form of <a href=""http://en.wikipedia.org/wiki/Polyacrylamide_gel_electrophoresis"" rel=""nofollow"">gel electrophoresis</a> is suited as a classification method for the quality of a certain substance. Several gels were loaded, each with a clean sample of the substance and with a sample that contains impurities. In addition, a molecular marker was also loaded which serves as a reference. The following picture illustrates the setup (the picture doesn't show the actual experiment, I have taken it from Wikipedia for illustration):</p>

<p><img src=""http://i.stack.imgur.com/vC53q.png"" alt=""Example of a gel electrophoresis""></p>

<p>Two parameters were measured for each gel and each lane:</p>

<ol>
<li>The <strong>molecular weight</strong> (that is how ""high up"" a compound wandered during the electrophoresis)</li>
<li>The <strong>relative quantity.</strong> The total quantity of each lane is normalized to 1 and the density of each band is measured which results in the relative quantity of each band.</li>
</ol>

<p>A scatterplot of the relative quantity vs. molecular weight is then produced which could look something like this (it's artificial data):</p>

<p><img src=""http://i.stack.imgur.com/ndzh9.png"" alt=""Example scatterplot""></p>

<p>This graphic can be read as follows: Both the ""good"" (blue points) and ""impure"" (red points) substance exhibit two bands, one at around a molecular weight of 120 and one at around 165. The bands of the ""impure"" substance at a molecular weight around 120 are considerably less dense than the ""good"" substance and can be well distinguished.</p>

<hr>

<h2>Goal</h2>

<p>The goal is to determine two boxed (see graphic below) which determine a ""good"" substance. These boxes will then be used for classification of the substance in the future into ""good"" and ""impure"". If a substance exhibits lanes that fall within the boxes it is classified as ""good"" and else as ""impure"".</p>

<p>These decision-rules should be <em>simple</em> to apply for someone in the laboratory. That's why it should be boxes instead of curved decision boundaries.</p>

<p>False-negatives (i.e. classify a sample as ""impure"" when it's really ""good"") are considered worse than false-positives. That is, an emphasis should be placed on the <a href=""http://en.wikipedia.org/wiki/Sensitivity_and_specificity#Sensitivity"" rel=""nofollow"">sensitivity</a>, rather than on the <a href=""http://en.wikipedia.org/wiki/Sensitivity_and_specificity#Specificity"" rel=""nofollow"">specificity</a>.</p>

<p><img src=""http://i.stack.imgur.com/vhzEW.png"" alt=""Example decision boxed""></p>

<hr>

<h2>Question</h2>

<p>I'm am no expert in machine learning. I know, however, that there are quite a few machine learning algorithms/techniques that could be helpful: $k$-nearest neighbors (e.g. <code>knn</code> in <code>R</code>), classification trees (e.g. <code>rpart</code> or <code>ctree</code>), support vector machines (<code>ksvm</code>), logistic regression, boosting and bagging methods and many more.</p>

<p>One problem of many of those algorithms is that they don't provide a simple ruleset or linear boundaries. In addition, the <strong>sample size</strong> is around <strong>70.</strong></p>

<p>My questions are:</p>

<ul>
<li>Has anyone an idea of how to proceed here?</li>
<li>Does it make sense to split the dataset into training- and test-set?</li>
<li>What proportion of the data should the training set be (I thought around a 60/40-split).</li>
<li>What, in general, is the workflow for such an analysis? Something like: Splitting dataset -> fit algorithm on the training set -> predict outcome for the test set?</li>
<li>How to avoid overfitting (i.e. boxes that are too small)?</li>
<li>What is a good statistic to assess the predictive performance in this case? AUC? Accurary? Positive predictive value? <a href=""http://en.wikipedia.org/wiki/Matthews_correlation_coefficient"" rel=""nofollow"">Matthews correlation coefficient</a>?</li>
</ul>

<p>Assume that I'm familiar with <code>R</code> and the <code>caret</code> package. Thank you very much for you time and help.</p>

<hr>

<h2>Example data</h2>

<p>Here is an example dataset.</p>

<pre><code>structure(list(mol.wt = c(125.145401455869, 118.210252208676, 
165.048583787746, 126.003687476776, 170.149347112565, 127.761533014759, 
155.523172614798, 120.094514977175, 161.234986765321, 168.471542655269, 
156.522990530521, 154.377948321209, 165.365756398877, 167.965538771316, 
116.132241687833, 115.143539160903, 156.696830822196, 162.578494491556, 
136.830624758899, 123.886594633942, 124.247484227948, 126.257226352824, 
160.684010454816, 166.618872115047, 126.599387146887, 165.690375912529, 
159.786861142652, 114.520735974329, 125.753594471656, 157.551537154148, 
157.320636890647, 171.5759136115, 158.580005438661, 125.647463565197, 
130.404710783509, 127.128218318572, 162.144126888907, 161.804616951055, 
167.917268243627, 168.582197247178), rel.qtd = c(57.68339235957, 
54.0514508510085, 25.0703901938793, 37.6933881305906, 36.6853653723001, 
53.6650555524679, 52.268438087776, 52.8621831466857, 43.1242291166037, 
46.6771236380788, 38.0328239221277, 40.0454611708371, 44.6406366176158, 
40.8238699987682, 51.9464749018547, 54.0302533272953, 37.9792331383524, 
48.3853988095525, 38.2093977349102, 42.2636098418388, 42.9876895407144, 
40.8018728193786, 40.1097096927465, 38.7432550253867, 39.2633283608111, 
43.4673723102812, 53.3740718733815, 49.1067921475768, 52.3002598744634, 
44.9847844953241, 44.3014423068017, 44.0191971364465, 47.0805245356855, 
55.0124134796556, 57.9938440244052, 62.8314454977068, 45.8093815891894, 
43.2300677500964, 39.4801550161538, 51.6253515591173), quality = structure(c(2L, 
2L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 1L), .Label = c(""bad"", ""good""), class = ""factor"")), .Names = c(""mol.wt"", 
""rel.qtd"", ""quality""), row.names = c(10L, 14L, 47L, 16L, 57L, 
54L, 45L, 12L, 43L, 67L, 25L, 21L, 1L, 55L, 20L, 22L, 37L, 15L, 
8L, 38L, 46L, 64L, 51L, 65L, 52L, 61L, 63L, 32L, 50L, 27L, 19L, 
69L, 23L, 42L, 6L, 48L, 11L, 13L, 5L, 71L), class = ""data.frame"")
</code></pre>
"
"0.0609427635336005","0.0435560928104281"," 76714","<p>I'm using the ANES dataset, which is a repeated cross-sectional study of public opinion and knowledge.  I am trying to predict knowledge by individual level characteristics.  The data is organized by year, and each respondent is grouped by age into an age cohort.  </p>

<p>Essentially, I am trying to run multiple multinomial regression models (the knowledge question has 3 levels: correct, incorrect, and don't know), but I want predictions specific to year the questions were asked and the cohort the respondent belongs to.  </p>

<p>Basically, for each year and each <code>cohort = multinom(knowledge ~ gender + education +...)</code></p>

<p>I have 8 years and 8 cohorts.  I tried to do a for loop within a for loop, but for whatever reason it is not working.  I tried:</p>

<pre><code>for(i in 1:length(year)){
   for(j in 1:length(cohort)){
      model &lt;- multinom(knowledge ~ gender + education + income)
  }
}
</code></pre>

<p>Can someone help me figure out what I'm doing wrong, please?  Right now, all this does is give me coefficients for the same year and cohort 100 times...</p>
"
"0.0430930413588572","0.0410650781176591"," 76850","<p>I am trying to do a multinomial logistic regression on some data that I generated. I am using R and the package mlogit. My data looks like the following:</p>

<pre><code>Class X1 X2  X3
V +0.0655197 +0.6418541 +1.8110291
V-0.6713268 -0.0262458 -0.3602958
V +0.2357610 -0.3602958 -0.6943458
M +0.3900129 +0.5583416 -1.7800082
M +0.5714871 -0.2767833 +0.5583416
M +1.0732807 -1.7800082 -0.3602958
S +0.9553640 -0.3602958 +0.6418541
S +0.1139899 +0.1356030 +0.3889280
S +0.4717283 -0.2852090 -1.1229880
</code></pre>

<p>My model is</p>

<pre><code>Class = B1*X1 + B2*X2 + B3*X3
</code></pre>

<p>So far, I have:</p>

<pre><code>library(mlogit);
allData &lt;- read.table(""Features/AllFeatures.dat"", header=TRUE);
allData$Class&lt;-as.factor(allData$Class);
mlData&lt;-mlogit.data(allData, choice=""Class"");
myData&lt;- mlogit(Class~1|X1 + X2 + x3, data = mlData);
print(summary(myData));
</code></pre>

<p>Which gives me:</p>

<pre><code>Coefficients :
                           Estimate Std. Error t-value  Pr(&gt;|t|)    
S:(intercept)                -0.6832392  0.0951834 -7.1781 7.068e-13 ***
V:(intercept)                -0.6696254  0.0943282 -7.0989 1.258e-12 ***
S:X1                         -0.1362492  0.1134039 -1.2015    0.2296    
V:X1                         -0.0052649  0.1128722 -0.0466    0.9628    
S:X2                         -0.0198451  0.0973608 -0.2038    0.8385    
V:X2                          0.0183261  0.0974789  0.1880    0.8509    
S:X3                          0.1728694  0.1110473  1.5567    0.1195    
V:X3                          0.0230260  0.1101147  0.2091    0.8344
</code></pre>

<p>However in the following: <a href=""http://cran.r-project.org/web/packages/mlogit/vignettes/Exercises.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/mlogit/vignettes/Exercises.pdf</a></p>

<p>The author gets:</p>

<pre><code>Coefficients :
Estimate Std. Error t-value Pr(&gt;|t|)
ic -0.00623187 0.00035277 -17.665 &lt; 2.2e-16 ***
oc -0.00458008 0.00032216 -14.217 &lt; 2.2e-16 ***
</code></pre>

<p>How can I change my function call to get the same? I want the actual coefficients for the model, not for the comparisons.</p>

<p>Also: What is the difference between 'wide' and 'long'? What form is my data in?</p>

<p>Also: Do you have some mathematical references for this type of multinomial logistic regression aimed at engineers?</p>

<p>Thanks!</p>
"
"0.0609427635336005","0.0435560928104281"," 76925","<p>Is it possible to penalize coefficients toward a number other than zero in a ridge regression in R?</p>

<p>For example, let's say I have dependent variable Y and independent variables X1,X2,X3, and X4. Because of the multicollinear nature of the ivs, ridge regression is appropriate. But say I'm fairly certain that the coefficient of X1 is near 5, X2 is near 1, X3 is near -1, and X4 is near -5. </p>

<p>Is there a ridge package and method in R where I can implement penalties on the coefficients of the ivs that penalize them toward those numbers instead of 0? I'd love to see an example in R with my example data, if possible. Thank you.</p>
"
"0.0430930413588572","0.0410650781176591"," 76997","<p>What is the apropriate statistic to measure the goodness-of-fit in Boosted Regression Tree (or Gradient Boosting Regression) with continuous response?
How can I calculate the coefficient of determination (RÂ²) in the train and test data? 
If I calculate the RÂ² as bellow, How can I calculate the intercept-only model?</p>

<p>RÂ² = 1âˆ’L1/L0, where L1 and L0 are the log likelihoods of the model under consideration and an
intercept-only model, respectively (see <a href=""http://www.stata-journal.com/sjpdf.html?articlenum=st0087"" rel=""nofollow"">http://www.stata-journal.com/sjpdf.html?articlenum=st0087</a>).</p>

<p>I'm using the package ""dismo"" in R, so if any one have a solution in R it will be great.</p>

<p>Example with binary data just to show the procedure:</p>

<pre><code>library(dismo)

data(Anguilla_train)

angaus.tc5.lr005 &lt;- gbm.step(data=Anguilla_train, gbm.x = 3:13, gbm.y = 2, family = ""bernoulli"", tree.complexity = 5, learning.rate = 0.005, bag.fraction = 0.5 , keep.fold.models = TRUE, keep.fold.vector = TRUE, keep.fold.fit = TRUE)
</code></pre>

<p>Thank you in advance!</p>
"
"0.035185320931284","0.0502942438178979"," 77838","<p>I have two linear regressions. The linear coefficients of both of them are very close to $1$. The first plot seems reasonable linear regression, while the second one is tricky since if the bottom left part or the top right part of the data (they are quite random) are fitted, we can not get a close-to-1 coefficient.</p>

<p>The question is how to distinguish the two regressions after I get their very similar regression coefficient values? Instead of by looking at the plots, a statistic is preferred to describe the difference of the two regressions. Any ideas? </p>

<p><img src=""http://i.stack.imgur.com/nqFjm.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/TfdGK.png"" alt=""enter image description here""> </p>
"
"0.0215465206794286","0.0410650781176591"," 77915","<p>I have a time series $X_t$, which is shown in the first plot. In the second plot, I am doing a linear regression on $X_t\sim X_{t-1}$. The regression line is very close to $y=x$. But this is tricky since if if we look at the bottom left or the top right part of the data, they are almost random. From the diagnosis of residuals, it is not a good regression either. But the model passes all the $t$ tests and $F$ tests. How can I say it's not a good model then? Is there a statistic  to describe (not visually) the failure of this modelling?</p>

<p>Here are the <code>R</code> codes I used to generate the plots:</p>

<pre><code># Generating X_t
x=c(arima.sim(list(order = c(1,0,0),ar=0.1),n=64,sd=1),3,5,7,11,14,17,rep(20,64)+arima.sim(list(order = c(1,0,0),ar=0.1),n=64,sd=1))
# Regression X_t~X_{t-1}
reg=lm(x[2:length(x)]~x[1:(length(x)-1)])
# Plotting
par(mfrow=c(3,2))
plot(x,xlab='',ylab=expression(X[t]),ty='l')
plot(x[1:(length(x)-1)],x[2:length(x)],xlab=expression(x[t-1]),ylab=expression(x[t]) ,main=paste('coeff= ',round(reg$coefficients[2],2)))
# Plotting the regression line
abline(reg,col=2)
# Plotting the residual diagnose
plot(reg)
</code></pre>

<p><img src=""http://i.stack.imgur.com/2xJ65.png"" alt=""enter image description here""></p>
"
"0.0430930413588572","0.0410650781176591"," 78149","<p>I'm running a linear regression. The response variable is a proportion, and has quite a lot of zeros. The predictor variable is normal distributed. </p>

<p>My two variables look something like these:</p>

<pre><code>set.seed(50)
y &lt;- sample(c(rep(0, 20), seq(0, 1, by=0.01)), 100)
set.seed(50)
x &lt;- rnorm(100)
</code></pre>

<p>And my model looks something like this:</p>

<pre><code>summary(lm(y ~ x))

Call:
lm(formula = y ~ x)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.42550 -0.31907 -0.02064  0.29710  0.59214 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.410990   0.033378  12.313   &lt;2e-16 ***
x           -0.006252   0.033574  -0.186    0.853    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.3316 on 98 degrees of freedom
Multiple R-squared:  0.0003537, Adjusted R-squared:  -0.009847 
F-statistic: 0.03468 on 1 and 98 DF,  p-value: 0.8527
</code></pre>

<p>Is there a statistic that will indicate if the model is zero inflated? </p>
"
"0.052777981396926","0.0502942438178979"," 78284","<p>I loaded the data to R and fitted it using GLM function.</p>

<blockquote>
  <p>fit.glm = glm(y~ aX+bZ+cW)</p>
</blockquote>

<p>Then, I found the cuttinf-point using ""segmented"" tool of R.</p>

<blockquote>
  <p>o&lt;-segmented(fit.glm,seg.Z=~X,psi=10)</p>
</blockquote>

<p>Now I have the cut-point and two different slope of X. </p>

<blockquote>
  <p>Call: segmented.glm(obj = fit.glm1, seg.Z = ~PTH, psi = 10)</p>
  
  <p>Meaningful coefficients of the linear terms: (Intercept)          X   Z       W</p>
  
  <p>19.43840           2.29574           0.08701           8.75784      </p>
  
  <p>Estimated Break-Point(s) psi1.PTH : 8.5 </p>
  
  <p>Degrees of Freedom: 324 Total (i.e. Null);  314 Residual Null
  Deviance:     17320  Residual Deviance: 7645      AIC: 1971</p>
</blockquote>

<p>However, I'm trying to get estimated y value of two linear regression models using the result of 'segmented'.</p>

<p>Is it possible using R, or I have to substitute Z and W with mean values of Z &amp; W, and calculate the y value myself?</p>
"
"0.110147477177496","0.112461348053768"," 78455","<p>Disclaimer: Statistics is not my strong side, so if my question is nonsense I apologize. I'm a beginner, but really wanting to understand this.</p>

<p>My question is: why do I get so widely different parameter estimates when using different transformations on my data in a non-linear regression ?</p>

<p>I'm trying to do a nonlinear regression and to estimate the uncertainty of the fit (confidence interval) using linear approximation. From my understanding the more linear-like the shape of the nonlinear function, the more accurate will the confidence interval calculation by linear approximation be.  I therefore want to transform the data to make it as linear as possible. The errors in $y$ can be assumed to be log-normal. My data is monotonic and assumed to follow a power function in most cases.</p>

<p>$$ y = a*(x-x_0)^b $$</p>

<p>where $y$ is river discharge, $x$ is an arbitrary water level in the river and $x_0$ is the water level where where discharge $y$ is 0. This can be rewritten as log transformed, and nice and linear
$$ log(y) = a + b \times log(x-x_0) $$.</p>

<p>I need to estimate the parameters $a$, $b$ and $x_0$, so to do so simultaneously I use nonlinear regression. I also have some data that follows quadratic functions, so I would like to set up (and understand) a non-linear method.</p>

<p>I use r and <code>nlsLM()</code> from <code>minpack.lm</code> to carry out the non-linear regression.
Here is some example code:</p>

<pre><code>library(minpack.lm)

xdata &lt;- c(19,  21,  24,    25, 29, 34, 35, 40, 40, 46, 48, 48, 52, 56, 57, 65, 65, 68)
ydata &lt;- c(10,  11, 14, 20, 24, 50, 42, 96, 89, 134,    135,    161,    171,    218,    261,    371,    347,    393)
df&lt;-data.frame(x=xdata, y=ydata)

#weights applied in the case of no transformation (relative error assumed to be the same for all y data)
W&lt;-1/ydata

# NLS regression with weights, no transformation
nlsmodel1&lt;-nlsLM(y ~ a*(x-x0)^b,data=df,start=list(a=0.1, b=2.5,x0=0))

# log transformed
nlsmodel2&lt;-nlsLM(log(y) ~ a+b*(log(x-x0)),data=df,start=list(a=0.1, b=2.5,x0=0))
&gt; coef(nlsmodel1)
          a           b          x0 
0.005158377 2.719693093 4.896772931 
&gt; coef(nlsmodel2)
        a         b        x0 
-8.683758  3.445699 -4.139127 

&gt; exp(-8.683758)
[1] 0.0001693136
</code></pre>

<p>I understand that the weights are very important, and can have a say in the differences here, but not by this much? My judgement of the two parameter sets is that <code>nlsmodel1</code> performs ""better"", and that the <code>b</code> coefficient is too high in the fit from <code>nlsmodel2</code>. <code>nlsmodel2</code> does a poor job in the upper end of the data, with large residuals there. But why are they so different? I feel like I'm doing something very silly here, and is unable to see the error. I have tried some other transformations, for example only transforming LHS as <code>log(y)</code>, but the problem remains.</p>

<p>I appreciate any tips that can help me improve, and not the least understand, the transformed fit.</p>

<p>Cheers</p>

<p>Related <a href=""http://stats.stackexchange.com/questions/58928/nonlinear-regression-confidence-intervals-on-transformed-or-untransformed-param"">post #1</a> and <a href=""http://stats.stackexchange.com/questions/69524/on-nonlinear-regression-fits-and-transformations"">post #2</a></p>
"
"0.0609427635336005","0.0580747904139041"," 78633","<p>I'm currently playing around with linear regression in R, and I've come up with a regression that fits data quite well. I'm just having some problems with interpreting the coefficients of my model. I know how to interpret log-log models in a simpler form, but when I have interactions I'm not quite sure how to interpret them.</p>

<p>Here's my output from R:</p>

<pre><code>Call:
lm(formula = log(y) ~ log(x1) + x2 * log(x1) + x3 * log(x1) + 
I(x3^2), data = Data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.56943 -0.12082  0.00012  0.11123  0.54579 

Coefficients:
                Estimate   Std. Error t value          Pr(&gt;|t|)    
(Intercept) -2.393889950  0.545879641  -4.385 0.000025149470154 ***
log(x1)      0.497477722  0.056113496   8.866 0.000000000000009 ***
x2          -0.000264760  0.000055476  -4.773 0.000005220020368 ***
x3           0.041126987  0.017930934   2.294           0.02357 *  
I(x3^2)     -0.000688879  0.000231778  -2.972           0.00358 ** 
log(x1):x2   0.000031580  0.000006691   4.720 0.000006494076511 ***
log(x1):x3   0.003145219  0.001277909   2.461           0.01528 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.1932 on 119 degrees of freedom
Multiple R-squared: 0.9865,     Adjusted R-squared: 0.9859 
F-statistic:  1454 on 6 and 119 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>I've been Googling for the past hour, but I can only find answers to some simpler models like the answer given here: <a href=""http://stats.stackexchange.com/questions/18480/interpretation-of-log-transformed-predictor"">Interpretation of log transformed predictor</a> or <a href=""http://www.ats.ucla.edu/stat/sas/faq/sas_interpret_log.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/sas/faq/sas_interpret_log.htm</a></p>

<p>I hope someone out there can help me with interpreting the interaction terms and the polynomial term in my model. </p>
"
"0.122096950516762","0.123195234352977"," 78663","<p>Is any one here familiar with an R package called Zelig?</p>

<p>I have a data frame like this:</p>

<pre><code>IQ   AGE
80   50
100  18
90   25
</code></pre>

<p>etc.</p>

<p>What I need to do is build a model of IQ given AGE, I am running these commands:
<code>z.out &lt;- zelig(IQ~AGE,data=df,model=""ls"")</code>
this runs the what-if given age 110, what would be the IQ
<code>x.out &lt;- setx(z.out, AGE=110)</code>
This is a simulation model where given the age 110, after running 1 million runs of simulation, what would be the IQ with 95% confidence interval.
<code>s.out &lt;- sim(z.out,x.out, num=1000000, level=95)</code></p>

<p>I have a hard time understanding from what pool of data the <code>sim()</code> function draws the numbers. I read though the docs, but they are written for Ph.D. students, if not more advanced readers. I have asked the Zelig creators this question multiple times but they are directing me to the docs which I read multiple times, with no luck. However, one of the  person that works with Zelig sent me this email:</p>

<blockquote>
  <p>Suppose that you fit 
  $$\text{IQ} = a + \text{Age} * b + e$$
  Then you get a table of regression coefficients where a=50, b=2, and their standard errors are something like $\text{s.e.}(a)=\sqrt{10}$ and $\text{s.e.}(b)=1$. These are all hypothetical examples. 
  In maximum likelihood estimation, this regression output is another way of saying that $a$ and $b$ are distributed bivariate normal with means $[50,2]$ and there's a variance-covariance matrix that looks something like this (all numbers are made up):
  $$\begin{array}{cc}
10 &amp; cov(a,b) \\
cov(a,b) &amp; 1 \\
\end{array}$$
  So, the variance of $a$ is 10, the variance of $b$ is 1, and their covariance is $cov(a,b)$. It won't be shown in your regression table, but Zelig remembers it for you. Let's pretend it's 3.
  This variance-covariance matrix is the inverse of the Hessian I mentioned earlier. Don't worry about it. For this example, you need only remember that $\text{mean}(a,b) = [50,2]$ and $cov(a,b)=\begin{array}{cc}10&amp;3\\3&amp;1\end{array}$. For purposes of plain text email, I'm representing matrices with columns separated by commas and rows by semicolons.
  In addition, suppose that the error term $e$ is distributed with mean 0 and s.d.=1.
  Now, one way to predict what IQ you might get for somebody aged 88, based on this regression table, is exactly what you would expect: you simply calculate 50 + 88 * 2 = 226. This is your point estimate. The 95% confidence interval around this point estimate is a function of the standard errors of the coefficient estimates of $a=50$ and $b=2$, and the exact formula for that is in any econometrics textbook.
  Simulation makes it unnecessary to dig up that textbook. Instead, for 1000 rounds, <code>sim()</code> will come up with 1000 different pairs of $(a,b)$ estimates drawn from the bivariate normal with mean=[50,2] and cov=$\begin{array}{cc}10&amp;3\\3&amp;1\end{array}$. One such pair might be $(47,1.5)$; another might be $(52,3)$; yet another might be $(10,5)$. 
  Whatever they are, <code>sim()</code> plugs them into the formula and gives you 1000 different estimates for the IQ. Their average is your point estimate. If you stack them from lowest to highest, the ends of the 95% confidence interval are the top 25th value and the bottom 25th. That's it. That's all that <code>sim()</code> does.</p>
</blockquote>

<p>Given the above explanation, can anybody tell me in lay terms, what numbers <code>sim()</code> is picking? How are those numbers in pool generated? I would greatly appreciate if anyone brings some light into this.</p>
"
"0.068136080998913","0.0519436716578171"," 79107","<p>I'm trying my hand at resampling techniques with a dataset I have, and I think either I'm missing a conceptual point with bootstrapping, or I'm doing something incorrectly in <code>R</code>. Basically, I'm trying to use it in a correlation/regression framework, and I'm able to get the original coefficients, the bootstrap bias, and the bootstrap coefficients but I can't find a way to have <code>R</code> easily display the bootstrap model $R^2$ (when I'm working with several predictors), the Pearson $r$, or the $p$-values for individual regression coefficients. (I'm using the <code>Boot</code> function in the <code>car</code> package).</p>

<p>A secondary question...the more general function <code>boot</code> in the <code>boot</code> package requires defining a function to use as an argument. The function must include an argument for the original data set, and a second argument which is a set of indices, frequencies, or weights for the bootstrap sample. I'm a little confused by this. What conceptually are these indices I am specifying, and how do I specify them syntactically within my function?</p>
"
"0.0430930413588572","0.0410650781176591"," 79399","<p>I have run a multiple regression in which the model as a whole is significant and explains about 13% of the variance. However, I need to find the amount of variance explained by each significant predictor. How can I do this using R?</p>

<p>Here's some sample data and code:</p>

<pre><code>D = data.frame(
    dv = c( 0.75, 1.00, 1.00, 0.75, 0.50, 0.75, 1.00, 1.00, 0.75, 0.50 ),
    iv1 = c( 0.75, 1.00, 1.00, 0.75, 0.75, 1.00, 0.50, 0.50, 0.75, 0.25 ),
    iv2 = c( 0.882, 0.867, 0.900, 0.333, 0.875, 0.500, 0.882, 0.875, 0.778, 0.867 ),
    iv3 = c( 1.000, 0.067, 1.000, 0.933, 0.875, 0.500, 0.588, 0.875, 1.000, 0.467 ),
    iv4 = c( 0.889, 1.000, 0.905, 0.938, 0.833, 0.882, 0.444, 0.588, 0.895, 0.812 ),
    iv5 = c( 18, 16, 21, 16, 18, 17, 18, 17, 19, 16 ) )
fit = lm( dv ~ iv1 + iv2 + iv3 + iv4 + iv5, data=D )
summary( fit )
</code></pre>

<p>Here's the output with my actual data:</p>

<pre><code>Call: lm(formula = posttestScore ~ pretestScore + probCategorySame + 
    probDataRelated + practiceAccuracy + practiceNumTrials, data = D)

Residuals:
    Min      1Q  Median      3Q     Max 
-0.6881 -0.1185  0.0516  0.1359  0.3690 

Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)
 (Intercept)        0.77364    0.10603    7.30  8.5e-13 ***
 iv1                0.29267    0.03091    9.47  &lt; 2e-16 ***
 iv2                0.06354    0.02456    2.59   0.0099 **
 iv3                0.00553    0.02637    0.21   0.8340
 iv4               -0.02642    0.06505   -0.41   0.6847
 iv5               -0.00941    0.00501   -1.88   0.0607 .  
--- Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.18 on 665 degrees of freedom
 Multiple R-squared:  0.13,      Adjusted R-squared:  0.123
 F-statistic: 19.8 on 5 and 665 DF,  p-value: &lt;2e-16
</code></pre>

<p>This question has been answered <a href=""http://stats.stackexchange.com/questions/60872/how-to-split-r-squared-between-predictor-variables-in-multiple-regression"">here</a>, but the accepted answer only addresses uncorrelated predictors, and while there is an additional response that addresses correlated predictors, it only provides a general hint, not a specific solution. I would like to know what to do if my predictors are correlated.</p>
"
"0.0545088647991304","0.0649295895722714"," 79830","<p>I'm using R to develop regression models, and I need to compare two different models' performance. The question that arises is, ""Is Model 1 statistically better than model 2?"" and I don't seem to have a way to answer that question.</p>

<p>Background: Model 1 consists of Variable A regressed on the endpoint. Model 2 consists of Variables B, C, and D regressed on the endpoint. Both models are developed using lm - ordinary least squares, nothing too fancy here.</p>

<p>Given that these are not nested models, I cannot compare them using ANOVA.</p>

<p>I can look at the R2 of actual vs predicted for each model, and I see that Model 2 is better, but how do I determine if it is statistically significantly better?</p>

<p>I've also used the Concordance Correlation Coefficient, but again, I can't find a way to prove significance. The best I've come up with is that the rho for Model 2 is better than Model 1, but that rho value is within the 95% confidence limits of the rho for Model 1.</p>

<p>I should throw in there that my assessment of predicted vs actual has been on a 60 observation hold-out set (240 observations in the training set).</p>
"
"NaN","NaN"," 79942","<p>I have elemental concentrations for 18 elements in 36 samples, determined by neutron activation analysis. I would like to calculate correlation coefficients, etc. by regression analysis according to the York method.</p>
"
"0.0430930413588572","0.0410650781176591"," 80312","<p>I have carried out this linear regression that includes month coded as a dummy variable:</p>

<pre><code>library(plyr)
set.seed(1)
y &lt;- rnorm(120)
x1 &lt;- c(rep(""adult"", 60), rep(""juvenile"", 60))
x2 &lt;- c(rep(""male"", 60), rep(""female"", 60))
x3 &lt;- unlist(llply(month.abb, function(x) rep(x, 10)))

summary(lm(y ~ x1 + x2 + x3))

Call:
lm(formula = y ~ x1 + x2 + x3)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.46354 -0.51524 -0.03981  0.57625  1.95041 

Coefficients: (2 not defined because of singularities)
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  0.12073    0.28564   0.423    0.673
x1juvenile   0.00663    0.40396   0.016    0.987
x2male            NA         NA      NA       NA
x3Aug       -0.37510    0.40396  -0.929    0.355
x3Dec       -0.24718    0.40396  -0.612    0.542
x3Feb        0.12812    0.40396   0.317    0.752
x3Jan        0.01147    0.40396   0.028    0.977
x3Jul        0.32385    0.40396   0.802    0.424
x3Jun        0.02273    0.40396   0.056    0.955
x3Mar       -0.25440    0.40396  -0.630    0.530
x3May        0.01341    0.40396   0.033    0.974
x3Nov        0.22012    0.40396   0.545    0.587
x3Oct       -0.01502    0.40396  -0.037    0.970
x3Sep             NA         NA      NA       NA

Residual standard error: 0.9033 on 108 degrees of freedom
Multiple R-squared:  0.04703,   Adjusted R-squared:  -0.05003 
F-statistic: 0.4845 on 11 and 108 DF,  p-value: 0.9093
</code></pre>

<p>I now want to present the results of this linear regression within a table. Instead of presenting the beta for every month, is there a way to summarise the overall effect of month on <code>y</code> within the same table? For example, would if be acceptable to summarise the beta, se, t value and p value of <code>x3</code> by using their mean values across months?</p>
"
"0.068136080998913","0.0519436716578171"," 80880","<p>I have seen several articles and CrossValidated questions on bootstrapping ( <a href=""http://stats.stackexchange.com/questions/41625/can-i-use-boostrapping-why-or-why-not"">this</a>, <a href=""http://stats.stackexchange.com/questions/59829/which-bootstrapped-regression-model-should-i-choose"">this</a> or <a href=""http://stats.stackexchange.com/questions/64813/two-ways-of-using-bootstrap-to-estimate-the-confidence-interval-of-coefficients"">this</a> for example); there are a lot of theoretical and statistical explanations, however since they are so theory based, I am afraid I might be understanding the use wrongly. Hence my questions:</p>

<p>1) When I make a non-parametric bootstrapping (changing the sample for every run) with logistic regression on my data, I basically will end up with several different coefficients for each predictor for each run. Eventually I'll have the confidence interval for each predictor as well. I understand until that point. My question is; assuming that the distribution is normal, when I want to come up with a final model on practice, can I just take the mean of the confidence intervals for each predictor and consider this as my final model coefficient?</p>

<p>2) If the answer to question #1 is yes, is this the only way of choosing coefficients while bootstrapping? If not, what else? I encountered in a few more articles a method called ""bagging"". This seems to be my main purpose. </p>

<p>3) This one is more of a curiosity question: Can above methodology be applied to the categorical predictors when they are assigned with Weight Of Evidences? I know we can split the  categorical predictors into dummy variables; but how would I treat each coefficient if I want to use WOE methodology?</p>
"
"0.052777981396926","0.0502942438178979"," 80888","<p>I am trying to understand how to get the coefficient of a multiple linear regression. </p>

<p>The formula is:</p>

<p>$b = (X'X)^{-1}(X')Y$</p>

<p>I try to calculate $b$ without package and with the <code>lm</code> package inside R. </p>

<p>Doing so, I got different results. </p>

<p>I want to know why. Did I made a mistake? Or does the <code>lm</code> package calculate differently because of the intercept?</p>

<pre><code>&gt; y &lt;-  c(1,2,3,4,5)
&gt; x1 &lt;- c(1,2,3,4,5)
&gt; x2 &lt;- c(1,4,5,7,9)
&gt; Y &lt;- as.matrix(y)
&gt; X &lt;- as.matrix(cbind(x1,x2))
&gt; beta = solve(t(X) %*% X) %*% (t(X) %*% Y) ; beta
            [,1]
x1  1.000000e+00
x2 -1.421085e-14
&gt; model &lt;- lm(y~x1+x2) ; model$coefficients
 (Intercept)           x1           x2 
1.191616e-15 1.000000e+00 1.192934e-15 
</code></pre>

<p><img src=""http://i.stack.imgur.com/KHD2q.png"" alt=""3d""></p>

<h1>Update</h1>

<p>As Alex and the other told me, it was a question of roundoff error. Therefore, I decided to take another data from the book ""Essential Statistics for business and economics"" by Anderson and all. In this case, the coefficients are the same in both <code>lm</code> function and in my own matrix.</p>

<pre><code>&gt; y &lt;- c(9.3, 4.8, 8.9, 6.5, 4.2, 6.2, 7.4, 6, 7.6, 6.1)
&gt; x0 &lt;- c(1,1,1,1,1,1,1,1,1,1) 
&gt; x1 &lt;-  c(100,50,100,100,50,80,75,65,90,90)
&gt; x2 &lt;- c(4,3,4,2,2,2,3,4,3,2)
&gt; Y &lt;- as.matrix(y)
&gt; X &lt;- as.matrix(cbind(x0,x1,x2))

&gt; beta = solve(t(X) %*% X) %*% (t(X) %*% Y);beta
         [,1]
x0 -0.8687015
x1  0.0611346
x2  0.9234254
&gt; model &lt;- lm(y~+x1+x2) ; model$coefficients
(Intercept)          x1          x2 
 -0.8687015   0.0611346   0.9234254 
</code></pre>
"
"0.052777981396926","0.0502942438178979"," 81699","<p>I'm new to data analysis so this is kind of a simple question.</p>

<p>I would like to understand why I cannot reproduce a survival curve generated by a fitted exponential model from Stata. I use the coefficients and make my function in R to plot but it looks nothing similar. I believe it's one of those daft problems where I am not interpreting something properly. I illustrate below.</p>

<p>First, some data in Stata:</p>

<pre><code>use http://www.ats.ucla.edu/stat/data/uis.dta, clear
gen id = ID
drop ID
stset time, failure(censor)
</code></pre>

<p>Then we can fit a null exponential model</p>

<pre><code>streg, dist(exponential) nohr
</code></pre>

<p>Which gives the following output:</p>

<pre><code>Exponential regression -- log relative-hazard form 

No. of subjects =          628                     Number of obs   =       628
No. of failures =          508
Time at risk    =       147394
                                                   LR chi2(0)      =     -0.00
Log likelihood  =    -1043.531                     Prob &gt; chi2     =         .

------------------------------------------------------------------------------
          _t |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
       _cons |  -5.670383   .0443678  -127.80   0.000    -5.757342   -5.583424
------------------------------------------------------------------------------
</code></pre>

<p>I take the survivor function from Stata's documentation:</p>

<p>$$
S(t)=\exp(-\lambda_{j} t_{j})
$$</p>

<p>So, in R, I plot this out with the following:</p>

<pre><code>S &lt;- function(x) exp(-5.670383*x) # 'x' acts like 't'
curve(S, 0, 1000)
</code></pre>

<p>This curve is not equivalent to Stata's, given by:</p>

<pre><code>stcurve, surv
</code></pre>

<p>Where did I go wrong in my interpretation? Is my equation using the correct parameterization?</p>

<p>P.S. Why am I reproducing these curves? A little to do with overlaying curves but now that I have this problem, I have to know where I went wrong.</p>
"
"0.068136080998913","0.0649295895722714"," 82153","<p>I have a multivariate time series dataset including interacting biological and environmental variables (plus possibly some exogenous variables). Beside seasonality, there is no clear long-term trend in the data. My purpose is to see which variables are related to each other. Forecasting is not really looked for. </p>

<p>Being new to time-series analysis, I read several references. As far as I understand, Vector Autoregressive (VAR) model would be appropriate, but I donâ€™t feel comfortable with seasonality and most examples I found concerned economics field (as often with time series analysisâ€¦) without seasonality.</p>

<p>What should I do with my seasonal data?
I considered deseasonalizing them â€“ for example in R, I would use <code>decompose</code> and then use the <code>$trend + $rand</code> values to obtain a signal which appears pretty stationary (as judged per <code>acf</code>).
Results of the VAR model are confusing me (a 1-lag model is selected while I would have intuitively expected more, and only coefficients for autoregression â€“ and not for regression with other lagged variables - are significant). 
Am I doing anything wrong, or should I conclude that my variables are not (linearly) related / my model is not the good one (subsidiary question: is there a non-linear equivalent to VAR?).</p>

<p>[Alternatively, I read I could probably use dummy seasonal variables, though I canâ€™t figure out exactly how to implement it].</p>

<p>Step-by-step suggestions would be very appreciated, since details for experienced users might actually be informative to me (and R code snippets or links towards concrete examples are very welcome, of course). Thank you.</p>
"
"0.0812570180448007","0.0871121856208561"," 82277","<h3>Background</h3>

<p>I have a data set of patients who were operated on at two different hospitals, A and B. Lymph nodes were removed from each patient during the operation and counted, this is saved as <code>LN_reviewed</code> for each patient. I want to know how much variability there is in the lymph node number that is <strong>not</strong> accounted for by gender, the year of the operation, or the age of the patient when operated on. My assumption (hypothesis) is that any additional variability is likely due to the pathologist at the institution (this was not actually measured in my study).</p>

<h3>My question</h3>

<p>What is the best way to go about estimating the variability in lymph node number that is not accounted for by gender (a factor), year (a continuous variable), or age (a continuous variable)?</p>

<h3>My initial attempt at answering this question</h3>

<p>I built a linear regression model using the number of lymph nodes as the response variable and the gender, operation year, and operation age as predictors. I am not sure how to interpret the results to answer my specific question. Should I be looking at the R squared? If so, is there a way to get a confidence interval for it? Thanks to anyone who can help. If you think I am going about this the wrong way, please let me know.</p>

<pre><code>Call:
lm(formula = LN_reviewed ~ Gender + Operation__year + Operation__age, data = sample_data)

Residuals:
    Min      1Q  Median      3Q     Max 
-49.436 -15.280  -0.495  13.450  61.564 

Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)     -1.190e+04  1.459e+03  -8.159 9.95e-14 ***
GenderMALE      -5.542e+00  4.685e+00  -1.183    0.239    
Operation__year  5.980e+00  7.296e-01   8.196 8.01e-14 ***
Operation__age  -2.524e-01  1.675e-01  -1.507    0.134    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 22.46 on 158 degrees of freedom
Multiple R-squared:  0.2999,    Adjusted R-squared:  0.2866 
F-statistic: 22.56 on 3 and 158 DF,  p-value: 3.268e-12
</code></pre>
"
"0.0430930413588572","0.0410650781176591"," 82346","<p>I just did a binary linear regression in R with a dataset that has 100000 lines. The output of the regression is, that almost every parameter is highly significant. I wouldn't expect that when I look at the <a href=""http://picpaste.com/yGsn701X.png"" rel=""nofollow"">boxplots</a>. Did I do something wrong in my code or can that be right?</p>

<pre><code>Call:
glm(formula = damage ~ dist_gerst + dist_gew + dist_hunt + dist_kart + 
    dist_mais + dist_raps + dist_road1 + dist_road2 + dist_road3 + 
    dist_road4 + dist_roada + dist_rog + dist_rmr + dist_ruben + 
    dist_sg + dist_wald + dist_hecke + dist_weize + dist_wg + 
    dist_bra, family = binomial(logit), data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.3963  -0.0024   0.2446   0.4947   5.1474  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -9.7073574  0.5951280 -16.311  &lt; 2e-16 ***
dist_gerst   0.0355374  0.0053699   6.618 3.65e-11 ***
dist_gew     0.0033584  0.0012147   2.765 0.005698 ** 
dist_hunt    0.0531545  0.0017567  30.259  &lt; 2e-16 ***
dist_kart    0.0472300  0.0022333  21.148  &lt; 2e-16 ***
dist_mais    0.0578135  0.0031780  18.192  &lt; 2e-16 ***
dist_raps    0.0470257  0.0021689  21.682  &lt; 2e-16 ***
dist_road1  -0.0135003  0.0023328  -5.787 7.15e-09 ***
dist_road2   0.0304884  0.0016027  19.023  &lt; 2e-16 ***
dist_road3  -0.0003806  0.0011631  -0.327 0.743505    
dist_road4  -0.0515227  0.0048316 -10.664  &lt; 2e-16 ***
dist_roada  -0.0186244  0.0050640  -3.678 0.000235 ***
dist_rog    -0.0403263  0.0031825 -12.671  &lt; 2e-16 ***
dist_rmr    -0.1133255  0.0045872 -24.705  &lt; 2e-16 ***
dist_ruben   0.1168154  0.0032703  35.721  &lt; 2e-16 ***
dist_sg     -0.0450639  0.0020300 -22.199  &lt; 2e-16 ***
dist_wald    0.1127090  0.0035169  32.047  &lt; 2e-16 ***
dist_hecke   0.1065537  0.0028434  37.474  &lt; 2e-16 ***
dist_weize  -0.1496686  0.0038303 -39.075  &lt; 2e-16 ***
dist_wg     -0.0937316  0.0051061 -18.357  &lt; 2e-16 ***
dist_bra    -0.0599667  0.0023916 -25.074  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 64559  on 53510  degrees of freedom
Residual deviance: 32357  on 53490  degrees of freedom
  (46231 observations deleted due to missingness)
AIC: 32399

Number of Fisher Scoring iterations: 9
</code></pre>
"
"0.068136080998913","0.0649295895722714"," 82509","<p>I have time series data on fish catches from 1950-2011. </p>

<p>I wish to implement a regression model with varying coefficients. I'm aware that cox models etc. exist and implementation via the <code>survival</code> package in R. My data is not survival data, it's just several variables with fish catches and year.</p>

<p>Is there a way in R to implement such models? I've yet to come across this but I don't think it's unreasonable to want to model such data without it being survival data.</p>

<p>I want to model <code>inlandfao</code> from <code>marinefao</code>. </p>

<p>Here is my data and some plots:</p>

<pre><code>fishdata &lt;- read.csv(""http://dl.dropbox.com/s/4w0utkqdhqribl4/fishdata.csv"", header=T)

require(reshape2)
require(ggplot2)
theme_set(theme_bw())
require(scales)

df2 &lt;- data.frame(cbind(year,totalmarinefao, totalinlandfao))
df2
dd &lt;- melt(df2, id.vars = ""year"")
dd
pp &lt;- ggplot(dd, aes(year, value, colour=variable)) + geom_point() + geom_line(size=1)
pp_final &lt;- pp +  xlab(""Year"") + ylab(""Catches (Tons)"") + ggtitle(""Time Series of Variables (1950-2011)"") 
pp_final
pp_final2 &lt;- pp_final +  scale_colour_discrete(name = ""Variable - Catches (FAO)"", breaks = c(""totalmarinefao"", ""totalinlandfao""),
                                               labels=c(""Marine"", ""Inland"")) + 
  scale_shape_discrete(name = ""Variable (FAO)"", breaks = c(""totalmarinefao"", ""totalinlandfao""), labels=c(""Marine"", ""Inland"")) + 
  scale_x_continuous(breaks=seq(1950,2011,10)) + scale_y_continuous(labels=comma)

pp_final2
pp_3 &lt;- pp_final2 + theme(axis.text.x  = element_text(vjust=1, size=16)) + theme(axis.title.x = element_text(size=20))
pp_4 &lt;- pp_3 + theme(axis.text.y = element_text(vjust=0, size=16)) + theme(axis.title.y = element_text(size=20, vjust=0.2))
pp_5 &lt;- pp_4 + theme(plot.title = element_text(lineheight=.8, face=""bold"", size=20))
pp_5

qplot(marinefao, inlandfao, data=fishdata, main=""Scatterplot of the Marine &amp; \n Inland 
fish Catches (Tons)"", xlab=""Marine Catches"", ylab=""Inland Catches"") + 
scale_x_continuous(labels = comma) + scale_y_continuous(labels = comma)
</code></pre>

<p>From these plots, a linear model isn't appropriate. I have fitted GAMs etc. to these data. </p>

<p>Let me more if you require details.</p>
"
"0.0457070726502004","0.0580747904139041"," 83012","<p>From Robert Kabacoff's <a href=""http://www.statmethods.net/advstats/bootstrapping.html"">Quick-R</a> I have </p>

<pre><code># Bootstrap 95% CI for regression coefficients 
library(boot)
# function to obtain regression weights 
bs &lt;- function(formula, data, indices) {
  d &lt;- data[indices,] # allows boot to select sample 
  fit &lt;- lm(formula, data=d)
  return(coef(fit)) 
} 
# bootstrapping with 1000 replications 
results &lt;- boot(data=mtcars, statistic=bs, 
     R=1000, formula=mpg~wt+disp)

# view results
results
plot(results, index=1) # intercept 
plot(results, index=2) # wt 
plot(results, index=3) # disp 

# get 95% confidence intervals 
boot.ci(results, type=""bca"", index=1) # intercept 
boot.ci(results, type=""bca"", index=2) # wt 
boot.ci(results, type=""bca"", index=3) # disp
</code></pre>

<p>How can I obtain the p-values $H_0:\, b_j=0$ of the bootstrap regression coefficients?</p>
"
"0.0609427635336005","0.0580747904139041"," 83133","<p>So, I have a homework assignment in which I'm being asked to compare the fit of two similar models by comparing their $R^2$ and AIC.  Both models were run in R, one using the <code>lm</code> command (for OLS) and the other the <code>glm</code> command; the former yielded an adjusted $R^2$ of 0.82, and the second model, with the same DV and covariates, produced an AIC of 365.96.</p>

<p>The only difference between the models is the ""g"" in the <code>lm</code>/<code>glm</code> command. The coefficients and standard errors they returned are virtually identical.</p>

<p>How does one compare $R^2$ and AIC? How can I tell which regression model fits the data better?</p>
"
"0.0304713817668003","0.029037395206952"," 83364","<p>I have been reading a number of papers where researchers have created risk scores based on logistic regression models. Often they refer to ""<a href=""http://www.ncbi.nlm.nih.gov/pubmed/15122742"" rel=""nofollow"">Sullivan's method</a>"" but I have no access to this paper and the explanations provided are far from clear. I noticed that Dr. Harrell's excellent RMS package provides a nomogram function which is in a way similar to creating a risk score (albeit with a very pretty graphical output).</p>

<p>It seems after tinkering around with it that the way it works is by dividing the beta coefficients by the smallest beta coefficient and then multiplying a constant to create points for categorical variables. However I cannot for the life of me figure out what is going on with continuous variables. I've spent hours searching google without much luck, and I would appreciate if someone could shed some light on this for me. Thanks!</p>
"
"0.13147045173735","0.130978139616752"," 83493","<p>I have a data set on which I'm trying to do regression, and failing.</p>

<p>The situation:</p>

<ul>
<li>Thousands of battle robot operators are fighting battles among each other using battle robots.</li>
<li>Some battle robots are strong and powerful, and others are weak; the strong ones win more often and deal more damage.</li>
<li>Robot operators vary in skill, with the more skilled ones winning more often, and delivering more damage</li>
<li>We have some summary information about the outcomes of their battles, but not all of the details.</li>
<li>We know what battle robots they used in their battles, and how many times (including how many of those battles they won), and we know the total damage they dealt (of two kinds, damageA and damageB) in total</li>
<li>Some robots are better at inflicting damageA, while others damageB</li>
<li>For unknown battle robot operators based only on what robots they have used in battles (and how many times), we would like to estimate how much damage of each kind they would achieve, and what % of battles they have most likely won</li>
</ul>

<p>For example:</p>

<ul>
<li>John has used Robot A for 4 battles, and Robot B for 2 battles, and has dealt 240 units worth of DamageA</li>
<li>James has used Robot A for 1 battle, and Robot B for 10 battles, and has dealt 1010 units worth of DamageA</li>
</ul>

<p>I can therefore estimate that Robot A probably deals 10 units of Damage A per battle, while Robot B deals 100 units of Damage A per battle, and thus if asked to estimate Damage A dealt by Matthew who has only played each of the two robots for 2 battles each, will estimate at 220 == (10*2 + 100*2).</p>

<p>Unfortunately, the real data are not as clean and straightforward, probably because:</p>

<ul>
<li>There is a significant variance due to robot operator skill, e.g., a good operator could deal 20 units of damage with Robot A while a bad one only 5 units. </li>
<li>There is some random variance due to opponents drawn in case of a small sample (e.g. somebody draws a strong opponent and loses despite having a better robot than the opponent), although eventually it would even out</li>
<li>There may be some minor selection bias in that the best robot operators manage to pick the best robots to take into battle more often</li>
</ul>

<p>The real data set is available here (630k entries of known battle operator results):</p>

<p><a href=""http://goo.gl/YAJp4O"" rel=""nofollow"">http://goo.gl/YAJp4O</a></p>

<p>The data set is organized as follows, with one robot operator entry per row:</p>

<ul>
<li>Column 1 with no label - operator ID</li>
<li>battles - total battles this operator has participated in</li>
<li>victories - total battles this operator has won</li>
<li>defeats - total battles this operator has lost</li>
<li>damageA - total Damage A points inflicted</li>
<li>damageB - total Damage B points inflicted</li>
<li>130 pairs of columns as follows:
<ul>
<li>battles_[robotID] - battles using robot [robotID]</li>
<li>victories_[robotID] - victories attained using robot [robotID]</li>
</ul></li>
</ul>

<p>What I've done so far:</p>

<ul>
<li>Tried a couple of linear models using R <code>biglm</code> package which build a formula such as <code>damageA ~ 0 + battles_1501 + battles_4201 + ...</code> to try to get fitted ""expected"" values for each of the robots.</li>
<li>Same, but removing the forced origin intercept by not including <code>0 +</code> in the formula</li>
<li>Same, but also included the <code>victories_[robotID]</code> in the independent variables</li>
<li>Same as before, but only selecting those robot operators whose victory numbers are close to their defeat numbers</li>
<li>A linear regression model for <code>damageA ~ 0 + battles_1501 + battles_non_1501</code> where <code>battles_non_1501</code> are all the battles in robots other than robot model 1501. Then repeated for all the other robot types.</li>
</ul>

<p>I did sanity checks by looking at the predicted damageA and damageB values, as well as comparing the victories/battles ratio with the actual victories/battles ratio that we can actually precisely calculate for each of the robots.</p>

<p>In all cases while the results weren't completely off, they were sufficiently off to see that the model isn't quite working. For example, some robots achieved negative damage numbers which shouldn't really happen as you cannot do negative damage in a battle. </p>

<p>In case where I also used the known <code>victories_[robotID]</code> values in the formula, many of the <code>battle_[robotID]</code> coefficients ended up being somewhat large negative numbers, so I tried estimating for the ""average"" operator by <code>battle_[robotID] + victories_[robotID] / 2</code> but that also didn't give reasonable results.</p>

<p>I'm somewhat out of ideas now.</p>
"
"0.0710998907892006","0.0774330538852055"," 84054","<p>I encountered a real-world problem where I want to model the effectiveness of various advertising media of a brand (measured in terms of sales). Basically, the Y in this case is weekly sales, and the X's are media investments in newspaper, magazine, display boards, tv, radio and online, as well as incentive, which is a percentage (like 10% off the original).</p>

<p>There are a few problems with the modelling work:</p>

<ul>
<li><p>all variables should have positive coefficients. Typically, more advertising or incentive is at least as good as less advertising/incentive (maybe this is not true if you buy all of the advertising spots in the world, as then your consumer will start to hate your brand, but this is not going to happen here). However, when I fit a typical regression (e.g. lm, glm, gls etc), some coefficients turn out to be negative (as data may be a bit noiser than expected, hence causing this problem?). I wonder if this can be controlled (I know in nonlinear regressions you can set constraints for parameters)</p></li>
<li><p>there should be some sort of diminishing marginal return of advertising spendings, but I am not exactly sure how to model that. Some ideas include using a log or square root transformation, another idea may be to use a nonlinear regression and estimator something like a*newspaper^b, where a is some coefficient, and b is an exponent between 0 and 1.</p></li>
<li><p>this is serial correlation, but this may not be exactly important here as the goal is only to estimate the parameters (if I use a regression I think I still get the unbiased estimators right? Autocorrelation only screws up the p-values, which is ignored here). Also, how to deal with seasonalities? I don't have much data (2 years) so maybe there is nothing we can do about it, but I have seen adding cos(0.0172*time) + sin(0.0172*time) to the regression equation to adjust for seasonal changes.</p></li>
</ul>

<p>Thanks.</p>
"
"0.101414888671788","0.104695817324578"," 85909","<p>The <code>plm</code> function of the <code>plm</code> library in R is giving me grief over having duplicate time-id couples, even when I'm running a model that I don't think should need a time variable at all (see reproducible example below).</p>

<p>I can think of three possibilities:</p>

<ol>
<li>My understanding of fixed effects regression is wrong, and they really do require unique time indices (or time indices at all!).</li>
<li>plm() is just being overly-finicky here and should relax this requirement.</li>
<li>The particular estimation technique that plm() uses--the within transformation--requires time indices, even though the order doesn't seem to matter and the less computationally-efficient version (including dummies in a straight-up OLS model) doesn't need them.</li>
</ol>

<p>Any thoughts?</p>

<pre><code>set.seed(1)
n &lt;- 1000
test &lt;- data.frame( grp = as.factor(rep( letters, (n/length(letters))+1 ))[seq(n)], x = runif(n), z = runif(n) )
test$y &lt;- with( test, 2*x + 3*z + rnorm(n) )
lm( y ~ x + z, data = test )
lm( y ~ x + z + grp, data = test )

require(plm)
# Model fails if I don't specify a time index, despite effect = ""individual""
plm( y ~ x + z, data = test, model = ""within"", effect=""individual"", index = ""grp"" ) 
# Create time variable and add it to the index but still specify individual FE not time FE also
library(plyr)
test &lt;- ddply( test, .(grp), function(dat) transform( dat, t = seq(nrow(dat)) ) )
# Now plm() works; note coefficients clearly include the fixed effects, as they match the lm() version above
plm( y ~ x + z, data = test, model = ""within"", effect=""individual"", index = c(""grp"",""t"") ) 
# Scramble time variables and show they don't matter as long as they're unique within a cluster
test &lt;- ddply( test, .(grp), function(dat) transform( dat, t = sample(t) ) )
plm( y ~ x + z, data = test, model = ""within"", effect=""individual"", index = c(""grp"",""t"") ) 
# Add a duplicate time entry and show that it causes plm() to fail
test[ 2, ""t"" ] &lt;- test[ 1, ""t"" ] 
plm( y ~ x + z, data = test, model = ""within"", effect=""individual"", index = c(""grp"",""t"") ) 
</code></pre>

<p><strong>Why this matters</strong></p>

<p>I'm trying to bootstrap my model, and when I do the requirement that the index-time pairs be unique is causing headaches which seem unnecessary if (2) is true.</p>
"
"0.075412822378","0.0718638867059034"," 85913","<p>I want to fit a DLM with time-varying coefficients, i.e. an extension to the usual linear regression,</p>

<p>$y_t = \theta_1 + \theta_2x_2$.</p>

<p>I have a predictor ($x_2$) and a response variable ($y_t$), marine &amp; inland annual fish catches respectively from 1950 - 2011. I want the DLM regression model to follow,</p>

<p>$y_t = \theta_{t,1} + \theta_{t,2}x_t$</p>

<p>where the system evolution equation is</p>

<p>$\theta_t = G_t \theta_{t-1}$</p>

<p>from page 43 of Dynamic Linear Models With R by Petris et al.</p>

<p>Some coding here,</p>

<pre><code>fishdata &lt;- read.csv(""http://dl.dropbox.com/s/4w0utkqdhqribl4/fishdata.csv"", header=T)
x &lt;- fishdata$marinefao
    y &lt;- fishdata$inlandfao

lmodel &lt;- lm(y ~ x)
summary(lmodel)
plot(x, y)
abline(lmodel)
</code></pre>

<p>Clearly time-varying coefficients of the regression model are more appropriate here. I follow his example from pages 121 - 125 and want to apply this to my own data. This is the coding from the example</p>

<pre><code>############ PAGE 123
require(dlm)

capm &lt;- read.table(""http://shazam.econ.ubc.ca/intro/P.txt"", header=T)
capm.ts &lt;- ts(capm, start = c(1978, 1), frequency = 12)
colnames(capm)
plot(capm.ts)
IBM &lt;- capm.ts[, ""IBM""]  - capm.ts[, ""RKFREE""]
x &lt;- capm.ts[, ""MARKET""] - capm.ts[, ""RKFREE""]
x
plot(x)
outLM &lt;- lm(IBM ~ x)
outLM$coef
    acf(outLM$res)
qqnorm(outLM$res)
    sig &lt;- var(outLM$res)
sig

mod &lt;- dlmModReg(x,dV = sig, m0 = c(0, 1.5), C0 = diag(c(1e+07, 1)))
outF &lt;- dlmFilter(IBM, mod)
outF$m
    plot(outF$m)
outF$m[ 1 + length(IBM), ]

########## PAGES 124-125
buildCapm &lt;- function(u){
  dlmModReg(x, dV = exp(u[1]), dW = exp(u[2:3]))
}

outMLE &lt;- dlmMLE(IBM, parm = rep(0,3), buildCapm)
exp(outMLE$par)
    outMLE
    outMLE$value
mod &lt;- buildCapm(outMLE$par)
    outS &lt;- dlmSmooth(IBM, mod)
    plot(dropFirst(outS$s))
outS$s
</code></pre>

<p>I want to be able to plot the smoothing estimates <code>plot(dropFirst(outS$s))</code> for my own data, which I'm having trouble executing. </p>

<p><strong>UPDATE</strong></p>

<p>I can now produce these plots but I don't think they are correct.</p>

<pre><code>fishdata &lt;- read.csv(""http://dl.dropbox.com/s/4w0utkqdhqribl4/fishdata.csv"", header=T)
x &lt;- as.numeric(fishdata$marinefao)
    y &lt;- as.numeric(fishdata$inlandfao)
xts &lt;- ts(x, start=c(1950,1), frequency=1)
xts
yts &lt;- ts(y, start=c(1950,1), frequency=1)
yts

lmodel &lt;- lm(yts ~ xts)
#################################################
require(dlm)
    buildCapm &lt;- function(u){
  dlmModReg(xts, dV = exp(u[1]), dW = exp(u[2:3]))
}

outMLE &lt;- dlmMLE(yts, parm = rep(0,3), buildCapm)
exp(outMLE$par)
        outMLE$value
mod &lt;- buildCapm(outMLE$par)
        outS &lt;- dlmSmooth(yts, mod)
        plot(dropFirst(outS$s))

&gt; summary(outS$s); lmodel$coef
       V1              V2       
 Min.   :87.67   Min.   :1.445  
 1st Qu.:87.67   1st Qu.:1.924  
 Median :87.67   Median :3.803  
 Mean   :87.67   Mean   :4.084  
 3rd Qu.:87.67   3rd Qu.:6.244  
 Max.   :87.67   Max.   :7.853  
 (Intercept)          xts 
273858.30308      1.22505 
</code></pre>

<p>The intercept smoothing estimate (V1) is far from the lm regression coefficient. I assume they should be nearer to each other. </p>
"
"0.121885527067201","0.116149580827808"," 86273","<p>I'm trying to calculate the log-likelihood for a generalized nonlinear least squares regression for the function $f(x)=\frac{\beta_1}{(1+\frac x\beta_2)^{\beta_3}}$ optimized by the <code>gnls</code> function in the R package <code>nlme</code>, using the variance covariance matrix generated by distances on a a phylogenetic tree assuming Brownian motion (<code>corBrownian(phy=tree)</code> from the <code>ape</code> package). The following reproducible R code fits the gnls model using x,y data and a random tree with 9 taxa:</p>

<pre><code>require(ape)
require(nlme)
require(expm)
tree &lt;- rtree(9)
x &lt;- c(0,14.51,32.9,44.41,86.18,136.28,178.21,262.3,521.94)
y &lt;- c(100,93.69,82.09,62.24,32.71,48.4,35.98,15.73,9.71)
data &lt;- data.frame(x,y,row.names=tree$tip.label)
model &lt;- y~beta1/((1+(x/beta2))^beta3)
f=function(beta,x) beta[1]/((1+(x/beta[2]))^beta[3])
start &lt;- c(beta1=103.651004,beta2=119.55067,beta3=1.370105)
correlation &lt;- corBrownian(phy=tree)
fit &lt;- gnls(model=model,data=data,start=start,correlation=correlation)
logLik(fit) 
</code></pre>

<p>I would like to calculate the log-likelihood ""by hand"" (in R, but without use of the <code>logLik</code> function) based on the estimated parameters obtained from <code>gnls</code> so it matches the output from <code>logLik(fit)</code>. NOTE: I am not trying to estimate parameters; I just want to calculate log-likelihood of the parameters estimated by the <code>gnls</code> function (although if someone has a reproducible example of how to estimate parameters without <code>gnls</code>, I would be very interested in seeing it!). </p>

<p>I'm not really sure how to go about doing this in R. The linear algebra notation described in Mixed-Effects Models in S and S-Plus (Pinheiro and Bates) is very much over my head and none of my attempts have matched <code>logLik(fit)</code>. Here are the details described by Pinheiro and Bates:</p>

<p>The log-likelihood for the generalized nonlinear least squares model  $y_i=f_i(\phi_i,v_i)+\epsilon_i$ where $\phi_i=A_i\beta$ is calculated as follows:</p>

<p>$l(\beta,\sigma^2,\delta|y)=-\frac 12 \Bigl\{ N\log(2\pi\sigma^2)+\sum\limits_{i=1}^M{\Bigl[\frac{||y_i^*-f_i^*(\beta)||^2}{\sigma^2}+\log|\Lambda_i|\Bigl]\Bigl\}}$</p>

<p>where $N$ is the number of observations, and $f_i^*(\beta)=f_i^*(\phi_i,v_i)$.</p>

<p>$\Lambda_i$ is positive-definite, $y_i^*=\Lambda_i^{-T/2}y_i$ and $f_i^*(\phi_i,v_i)=\Lambda_i^{-T/2}f_i(\phi_i,v_i)$</p>

<p>For fixed $\beta$ and $\lambda$, the ML estimator of $\sigma^2$ is </p>

<p>$\hat\sigma(\beta,\lambda)=\sum\limits_{i=1}^M||y_i^*-f_i^*(\beta)||^2 / N$</p>

<p>and the profiled log-likelihood is</p>

<p>$l(\beta,\lambda|y)=-\frac12\Bigl\{N[\log(2\pi/N)+1]+\log\Bigl(\sum\limits_{i=1}^M||y_i^*-f_i^*(\beta)||^2\Bigl)+\sum\limits_{i=1}^M\log|\Lambda_i|\Bigl\}$</p>

<p>which is used with a Gauss-Seidel algorithm to find the ML estimates of $\beta$ and $\lambda$. A less biased estimate of $\sigma^2$ is used:</p>

<p>$\sigma^2=\sum\limits_{i=1}^M\Bigl|\Bigl|\hat\Lambda_i^{-T/2}[y_i-f_i(\hat\beta)]\Bigl|\Bigl|^2/(N-p)$</p>

<p>where $p$ represents the length of $\beta$.</p>

<p>I have compiled a list of specific questions that I am facing:</p>

<ol>
<li>What is $\Lambda_i$? Is it the distance matrix produced by <code>big_lambda &lt;- vcv.phylo(tree)</code> in <code>ape</code>, or does it need to be somehow transformed or parameterized by $\lambda$, or something else entirely?</li>
<li>Would $\sigma^2$ be <code>fit$sigma^2</code>, or the equation for the less biased estimate (the last equation in this post)?</li>
<li>Is it necessary to use $\lambda$ to calculate log-likelihood, or is that just an intermediate step for parameter estimation? Also, how is $\lambda$ used? Is it a single value or a vector, and is it multiplied by all of $\Lambda_i$ or just off-diagonal elements, etc.?</li>
<li>What is $||y-f(\beta)||$? Would that be <code>norm(y-f(fit$coefficients,x),""F"")</code> in the package <code>Matrix</code>? If so, I'm confused about how to calculate the sum $\sum\limits_{i=1}^M||y_i^*-f_i^*(\beta)||^2$, because <code>norm()</code> returns a single value, not a vector.</li>
<li>How does one calculate $\log|\Lambda_i|$? Is it <code>log(diag(abs(big_lambda)))</code> where <code>big_lambda</code> is $\Lambda_i$, or is it <code>logm(abs(big_lambda))</code> from the package <code>expm</code>? If it is <code>logm()</code>, how does one take the sum of a matrix (or is it implied that it is just the diagonal elements)?</li>
<li>Just to confirm, is $\Lambda_i^{-T/2}$ calculated like this: <code>t(solve(sqrtm(big_lambda)))</code>?</li>
<li>How are $y_i^*$ and $f_i^*(\beta)$ calculated? Is it either of the following:</li>
</ol>

<p><code>y_star &lt;- t(solve(sqrtm(big_lambda))) %*% y</code></p>

<p>and</p>

<p><code>f_star &lt;- t(solve(sqrtm(big_lambda))) %*% f(fit$coefficients,x)</code></p>

<p>or would it be</p>

<p><code>y_star &lt;- t(solve(sqrtm(big_lambda))) * y</code></p>

<p>and</p>

<p><code>f_star &lt;- t(solve(sqrtm(big_lambda))) * f(fit$coefficients,x)</code> ?</p>

<p>If all of these questions are answered, in theory, I think the log-likelihood should be calculable to match the output from <code>logLik(fit)</code>. Any help on any of these questions would be greatly appreciated. If anything needs clarification, please let me know. Thanks!</p>

<p><strong>UPDATE</strong>: I have been experimenting with various possibilities for the calculation of the log-likelihood, and here is the best I have come up with so far. <code>logLik_calc</code> is consistently about 1 to 3 off from the value returned by <code>logLik(fit)</code>. Either I'm close to the actual solution, or this is purely by coincidence. Any thoughts?</p>

<pre><code>  C &lt;- vcv.phylo(tree) # variance-covariance matrix
  tC &lt;- t(solve(sqrtm(C))) # C^(-T/2)
  log_C &lt;- log(diag(abs(C))) # log|C|
  N &lt;- length(y)
  y_star &lt;- tC%*%y 
  f_star &lt;- tC%*%f(fit$coefficients,x)
  dif &lt;- y_star-f_star  
  sigma_squared &lt;-  sum(abs(y_star-f_star)^2)/N
  # using fit$sigma^2 also produces a slightly different answer than logLik(fit)
  logLik_calc &lt;- -((N*log(2*pi*(sigma_squared)))+
       sum(((abs(dif)^2)/(sigma_squared))+log_C))/2
</code></pre>
"
"0.0746393370862076","0.0592723347638087"," 86313","<p>I am computing multiple regression coefficients for these variables (for several cases):</p>

<pre><code>         y,x1,x2,x3
     log(y)= a(log(x1))+b(log(x2))+c(x3)+d
     res=lm.fit(log(y)~log(x1)+log(x2)+(x3))
</code></pre>

<p>All values of <code>y,x1,x2,x3</code> are between <code>0 and 1</code>.</p>

<p>The values of the coefficients range between <code>-10 to 10</code> for <code>a</code></p>

<p>The values of the coefficients range between <code>-5 to 5</code> for <code>b</code></p>

<p>The values of the coefficients range between <code>-5</code> to <code>5</code> for <code>c</code></p>

<p>The values of the coefficients range between <code>-3</code> to <code>3</code> for <code>d</code></p>

<p>I wonder if we should expect a certain range for each coefficient.or in other words,how to know that my regression analyses did a good job.</p>
"
"0.075412822378","0.0821301562353182"," 86952","<p>I'm trying to regress some simple pooled data. My data has 60 observations and three columns: Weight, Height, and Sex (female=1, male=0).</p>

<p>If I regress thus, Weight ~ Height + Sex, my model is fairly satisfactory, but the residuals are not homoscedastic (green errors are male, blue female):</p>

<p><img src=""http://i1267.photobucket.com/albums/jj541/nbahmanyar/Rplot_zps69001b34.png"" alt=""plot""></p>

<p>I tried regressing on the log of Weight and/or Height, but that didn't do much. What should I do to make the residuals homescedastic and/or make my model more accurate? Any help would be appreciated.</p>

<p><strong>Edit</strong></p>

<p>Doing a generalized regression model gives the following.</p>

<pre><code>Generalized least squares fit by REML
  Model: Weight ~ h + s 
  Data: P149 
       AIC      BIC    logLik
  514.2221 524.4374 -252.1111

Variance function:
 Structure: Different standard deviations per stratum
 Formula: ~1 | Sex 
 Parameter estimates:
        0         1 
1.0000000 0.6685307 

Coefficients:
                 Value Std.Error   t-value p-value
(Intercept)  27.197499  51.88129  0.524226  0.6022
h             1.852382   0.75634  2.449128  0.0174
s           -25.284478   5.53300 -4.569755  0.0000

 Correlation: 
  (Intr) h     
h -0.997       
s -0.524  0.466

Standardized residuals:
       Min         Q1        Med         Q3        Max 
-1.6655243 -0.6879858 -0.1839396  0.5628971  3.9857544 

Residual standard error: 22.13369 
Degrees of freedom: 60 total; 57 residual
</code></pre>

<p>With this s. residual plot:</p>

<p><img src=""http://i1267.photobucket.com/albums/jj541/nbahmanyar/Rplot1_zps5ee264a0.png"" alt=""""></p>

<p>Could someone please explain how precisely this model is different from a standard multiple regression model? Thanks.</p>
"
"0.0609427635336005","0.0580747904139041"," 87071","<p>I am looking to use a mathematical model developed by Firbank &amp; Watkinson (1985) J. App. Ecol. 22:503-517 for the analysis of competition between plants grown in mixture.</p>

<p>The model is as follows:</p>

<p>$$W_{A}=W_{mA}\left(1 + a_{A}\left(N_{A}+\alpha N_{B}\right)\right)^{-b_{A}}$$</p>

<p>where $W_{A}$ is the mean yield per plant of species $A$ grown in the experiment, $W_{mA}$ is the mean yield of isolated plants of species $A$, $a_{A}$ is the surface area required to reach size $W_{mA}$, $N_{A}$ is the planting density of species $A$, $\alpha$ is the competition coefficient, and $-b_{A}$ is the 'resource use efficiency' parameter. </p>

<p>The model is a regression model as I understand it. I have data for density of species $A$ and $B$ and ($N_{A}$ and $N_{B}$) as well as the response variable $W_{A}$. I am unsure how I can use R to estimate the remaining values, most important of which is the competition coefficient, $\alpha$. If there is any more information that I need to provide please let me know.</p>
"
"0.091874672876503","0.0875510407188402"," 87105","<p>I have answered all of the following questions I need someone to verify me or if there is a better approach I would like to know about.</p>

<blockquote>
  <p>Q1) Look at your model summary to find the x variable whose model coefficient is most significantly different from 0. (You don't have to write R code to find this other variableâ€“just read your model summary.)<br>
      Q2) Make a simple linear regression model for PBE vs. this x.<br>
      Q3) Make a scatterplot of PBE vs. this x.<br>
      Q4) Add the simple regression line to your scatterplot.<br>
      Q5) Include a reasonable title and axis labels.  </p>
</blockquote>

<hr>

<pre><code>A0) beeflm = lm(PBE ~., data=beef)

A1) (coef(beeflm))
  (Intercept)          YEAR           CBE 
2693.01348650   -1.28693774   -1.84919910 
          PPO           CPO           PFO 
  -0.99901169   -1.73045916    1.27410503 
         DINC           CFO         RDINC 
  -2.49792219    1.04485422    1.32154103 
          RFP 
  -0.01729997 

A2) PBEvsDINC = lm(PBE~DINC, data=beef)

A3,A5) plot(beef$PBE, beef$DINC, main=""Beef PBE vs DINC"", xlab=""DINC"", ylab=""PBE"")   
(Am I right?) 
Error in plot.new() : figure margins too large

A4) abline(PBEvsDINC)
</code></pre>

<p>And finally here's the question I don't know the answer:</p>

<blockquote>
  <p>Q: Are the coefficients (y-intercept and slope in the x direction) the same for this second simple linear regression model as they are in the first multiple regression model?</p>
</blockquote>

<p>Here's the data for beef data.frame:</p>

<pre><code>YEAR    PBE     CBE     PPO     CPO     PFO     DINC    CFO     RDINC   RFP
1925    59.7    58.6    60.5    65.8    65.8    51.4    90.9    68.5     877
1926    59.7    59.4    63.3    63.3      68    52.6    92.1    69.6     899
1927      63    53.7    59.9    66.8    65.5    52.1    90.9    70.2     883
1928      71    48.1    56.3    69.9    64.8    52.7    90.9    71.9     884
1929      71      49      55    68.7    65.6    55.1    91.1    75.2     895
1930    74.2    48.2    59.6    66.1    62.4    48.8    90.7    68.3     874
1931    72.1    47.9      57    67.4    51.4    41.5      90      64     791
1932      79      46    49.5    69.7    42.8    31.4    87.8    53.9     733
1933    73.1    50.8    47.3    68.7    41.6    29.4      88    53.2     752
1934    70.2    55.2    56.6    62.2    46.4    33.2    89.1      58     811
1935    82.2    52.2    73.9    47.7    49.7      37    87.3    63.2     847
1936    68.4    57.3    64.4    54.4    50.1    41.8    90.5    70.5     845
1937      73    54.4    62.2      55    52.1    44.5    90.4    72.5     849
1938    70.2    53.6    59.9    57.4    48.4    40.8    90.6    67.8     803
1939    67.8    53.9      51    63.9    47.1    43.5    93.8    73.2     793
1940    63.4    54.2    41.5    72.4    47.8    46.5    95.5    77.6     798
</code></pre>

<h2>    1941      56      60    43.9    67.4    52.2    56.3    97.5    89.5     830</h2>

<p>Here is the t-value results, so is CBE the most significant variable according to this?:</p>

<pre><code>summary(lm(formula = PBE ~ ., data = beef))

Call:
lm(formula = PBE ~ ., data = beef)
Residuals:
Min 1Q Median 3Q Max 
-1.1986 -0.5658 -0.1001 0.7067 1.3251
Coefficients:
Estimate Std. Error t value
(Intercept) 2693.0135 1337.9523 2.013
YEAR -1.2869 0.6970 -1.846
CBE -1.8492 0.2834 -6.525
PPO -0.9990 0.4051 -2.466
CPO -1.7305 0.5685 -3.044
PFO 1.2741 1.9645 0.649
DINC -2.4979 2.6018 -0.960
CFO 1.0449 1.0678 0.979
RDINC 1.3215 1.7117 0.772
RFP -0.0173 0.1131 -0.153
Pr(&gt;|t|) 
(Intercept) 0.084020 . 
YEAR 0.107337 
CBE 0.000326 ***
PPO 0.043064 * 
CPO 0.018743 * 
PFO 0.537314 
DINC 0.368995 
CFO 0.360411 
RDINC 0.465337 
RFP 0.882717 
---
Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
Residual standard error: 1.238 on 7 degrees of freedom
Multiple R-squared: 0.986, Adjusted R-squared: 0.968 
F-statistic: 54.77 on 9 and 7 DF, p-value: 1.165e-05
</code></pre>
"
"NaN","NaN"," 87117","<p>I have 20 independent variables that sum to one. I have 181 vectors so my X matrix is 181 by 20 so I do have an overdetermined system. However, when I run lm() in R with this data it gives me nonsensical data with equal coefficients for every single independent variable. Also, the intercept is -1*coefficient. Could someone please explain why linear regression doesn't work in this situation?</p>
"
"NaN","NaN"," 87345","<p>I have tried calculating the AIC of a linear regression in R but without using the <code>AIC</code> function, like this:</p>

<pre><code>lm_mtcars &lt;- lm(mpg ~ drat, mtcars)

nrow(mtcars)*(log((sum(lm_mtcars$residuals^2)/nrow(mtcars))))+(length(lm_mtcars$coefficients)*2)
[1] 97.98786
</code></pre>

<p>However, <code>AIC</code> gives a different value:</p>

<pre><code>AIC(lm_mtcars)
[1] 190.7999
</code></pre>

<p>Could somebody tell me what I'm doing wrong?</p>
"
"0.0304713817668003","0.029037395206952"," 87553","<p>i have fitted a Poisson regression to my claim frequency data. my predictor is make of vehicle.</p>

<p>I have obtained the following result:</p>

<pre><code>Coefficients:
              Estimate  Std. Error  z value Pr(&gt;|z|)  
(Intercept)  -19.99774  1138.82118  -0.018  0.98599  
make2         -0.30873    0.20550  -1.502   0.13302 
make3         -0.39177    0.21129  -1.854   0.06372 
make4         -0.38375    0.13388  -2.866   0.00415 **
</code></pre>

<p>How do I interpret the negative coefficients? Thanks</p>
"
"0.0457070726502004","0.0580747904139041"," 87578","<p>I am estimating cross-sectional regressions - fragment:</p>

<blockquote>
  <p>lm(rate~liqamih.log+cap.log+F1+F2, data=x)</p>
</blockquote>

<p>of the R code listed below.</p>

<p>F1 and F2 are the coefficients estimates of time series model.</p>

<p>In such case we need to deal with so called ""error in variables problem"". In literature (links: gendocs.ru/docs/23/22031/conv_1/file1.pdfâ€Ž (page 1091) and papers.ssrn.com/sol3/papers.cfm?abstract_id=6992 (whole article)) the MLE method is one of the effective solution of this problem.</p>

<p>I would like to implement method introduced by KIM (reference to literature-links above) in my R code below. HOW TO DO THAT ?</p>

<pre><code>data&lt;-read.table(""reg5-dane.csv"", head=T, sep="";"", dec="","")
  data$indx &lt;- as.numeric(gl(123*334,334,123*334))
lst1 &lt;- split(data[,-53],data[,53]) #max 53 variables in ""lst2"" regression
any(sapply(lst1,nrow)!=123)
#[1] FALSE
lst2 &lt;- lapply(lst1[sapply(lst1,function(x) !(all(rowSums(is.na(x))&gt;0)))],
               function(x) summary(lm(rate~liqamih.log+cap.log+F1+F2, data=x)) )
capture.output(lst2,file=""nooldor_regr_summ.txt"")
# - f2 -f6 /+f2 -f5 +F6 - f6 - f2 - liq + f6 - f6 +f2 - f4 - f4 +f3

capture.output(lst2,file=""nooldor_regr_summ.csv"")
# HERE - above- IS THE CORE / most important part of question ... for now you can skip what is below 
f4 &lt;- function(meanmod, dta, varmod) {
  assign("".dta"", dta, envir=.GlobalEnv)
  assign("".meanmod"", meanmod, envir=.GlobalEnv)
  m1 &lt;- lm(.meanmod, .dta)
  ans &lt;- ncvTest(m1, varmod)
  remove("".dta"", envir=.GlobalEnv)
  remove("".meanmod"", envir=.GlobalEnv)
  ans
}
library(car)
lst3 &lt;- lapply(lst1[sapply(lst1,function(x) !(all(rowSums(is.na(x))&gt;0)))],
               function(x) f4(rate~cap.log, x))
lst4 &lt;- lapply(lst1[sapply(lst1,function(x) !(all(rowSums(is.na(x))&gt;0)))],
               function(x) durbinWatsonTest(lm(rate~cap.log, x)))
lst5 &lt;- lapply(lst1[sapply(lst1,function(x) !(all(rowSums(is.na(x))&gt;0)))],
               function(x) vif(lm(rate~cap.log, x)))

res5 &lt;- do.call(rbind,lst5)
res4 &lt;- do.call(rbind,lapply(lst4,function(x) unlist(x[-4])))
library(tseries)

res6 &lt;- do.call(rbind,lapply(lst1[sapply(lst1,function(x) !(all(rowSums(is.na(x))&gt;0)))],function(x) {resid &lt;- residuals(lm(rate~.,data=x)); unlist(jarque.bera.test(resid)[1:3])}) )

capture.output(lst2,file=""nooldor_regr_summ.txt"")
capture.output(lst3,file=""nooldor_arch_test.txt"")
capture.output(lst4,file=""nooldor_durbin.txt"")
capture.output(lst5,file=""nooldor_vif.txt"")
</code></pre>

<p>There is 123 time observations on 334 subjects observations. For each of 123 time points I am running one regressions with 334 subjects (so I am repeating it 123 times for each point of time).
I post this topic also in stackoverflow.com - because I need help from one who have strong background in statistics/econometrics and also in R programming.
I would appreciate your valuable help.
Thank you.</p>
"
"0.105555962793852","0.0922061136661462"," 87650","<p>I ran into (what I think is) an inconsistency when running a random-intercept model (using the <em>lmer</em> function in the <em>lme4</em> package in R).</p>

<p>Here is what I do: I first run a model with a set of covariates; then I run the same model re-scaling (linearly transforming) one of the regressors. To my knowledge, this should change <strong>only the coefficient</strong> of the variable that is linearly transformed. And indeed, this is what happens when I run this ""experiment"" with a simple linear regression model and with a logistic model.</p>

<p>This code replicates the ""normal"" behaviour:</p>

<pre><code># Create three random independent variables
x1 &lt;- rnorm(20)
x2 &lt;- rnorm(20)
x3 &lt;- as.factor(sample(0:2, 20, replace = TRUE))
# Their random coefficients
coef1 &lt;- runif(1, -1, 1)
coef2 &lt;- runif(1, -1, 1)
# Create a continuous dependent variable and a binomial one
y1 &lt;- coef1 * x1 + coef2 * x2 + runif(20)
y2 &lt;- y1
y2[which(y1 &gt; quantile(y1, 0.5))] &lt;- 1
y2[which(y1 &lt;= quantile(y1, 0.5))] &lt;- 0
# Finally, a linear transformation of x1
x1.trans &lt;- x1*3
</code></pre>

<p>So, let us run an OLS model:</p>

<pre><code>lm &lt;- lm(y1 ~ x1 + x2 + x3)
summary(lm)
# OLS model with one variable linearly transformed
lm.bis &lt;- lm(y1 ~ x1.trans + x2 + x3)
summary(lm.bis)
</code></pre>

<p>The coefficients of <em>x1</em> and <em>x1.trans</em> are different, <strong>but the R-square of the two models is the same</strong>:</p>

<pre><code>summary(lm)$r.sq == summary(lm.bis)$r.sq
</code></pre>

<p>The same with a logistic model:</p>

<pre><code>logm &lt;- glm(y2 ~ x1 + x2, family=""binomial"")
summary(logm)
logm.bis &lt;- glm(y2 ~ x1.trans + x2, family=""binomial"")
summary(logm.bis)
</code></pre>

<p>Even in this case, <strong>the log-likelihood of the two models is the same</strong>:</p>

<pre><code>logLik(logm) == logLik(logm.bis)
</code></pre>

<p>So far, so good. However, when I do the same with a hierarchical model, <strong>the log-likelihood (and consequently the AIC and BIC) of the two models are different</strong>, although the coefficient of the transformed variable remains significant with the same z value and the other coefficients are the same.</p>

<pre><code># Multilevel model
mm &lt;- lmer(y1 ~ x1 + x2 + (1 | x3))
summary(mm)
mm.bis &lt;- lmer(y1 ~ x1.trans + x2 + (1 | x3))
summary(mm.bis)
logLik(mm) == logLik(mm.bis) ### FALSE! ###
</code></pre>

<p>Why? Also the ""REML criterion at convergence"" is obviously different. I don't understand this result. This is probably due to my moderate knowledge of the math of hierarchical models. I'd be very happy if some of you could show me what's the trick here.</p>

<p>Since we then use AIC and BIC to compare models, I am puzzled by the fact that a simple transformation that shouldn't change anything makes one model better (or worse) than another.</p>
"
"0.052777981396926","0.0335294958785986"," 87814","<p>Suppose I have a set of (discrete) variables, say $X_1,\dots,X_n$. Each $i$ belongs to either class A or class B. When it belongs to A the contribution is $Y_{A,i}f(X_i)$, and when it belongs to B the contribution is $Y_{B,i}g(X_i)$, so that the final outcome is $Z=\sum_{i=1}^N Y_{A,i}f(X_i) I_{\{i \in A\}} + Y_{B,i}g(X_i) I_{\{i \in B\}}$, where $I_{\{.\}}$ denotes the indicator-function.  </p>

<p>The values for $X_1,\dots,X_n$ are given, as are the functions $f$ and $g$. Now suppose I have $M$ obervations for $Z$, what method do you advice to</p>

<p>(i)  Identify whether $i$ is in $A$ or $B$</p>

<p>(ii) Estimate the values for $Y_{A,i}$ (if $i\in A)$ and $Y_{B,i}$ (if $i \in B$).</p>

<p>I tried to estimate the coefficients $Y_{A,i}$ and $Y_{B,i}$ using standard linear regression, assuming that $i\in A$ for all $i$ or $i \in B$ for all $i$, but that gives bad results.</p>

<p>(iii) What if I have $M$ different inputs $X_{j,1},\dots,X_{j,N}$, for $j=1,\dots,M$ and one observation for $Z$ for each input?</p>

<p>Edit:</p>

<p>As an example, take the following R-code:</p>

<pre><code>N=50
M=10
X=rbinom(N*M,1,0.5)-rbinom(N*M,1,0.5) #Generate -1, 0, 1 variables
dim(X) &lt;- c(N,M)
pA=1/2 #pB=1-pA
XA = X #f(x)=x
XB = -X^2 + 1 #g(x)=-x^2+1
isA = as.logical(rbinom(M,1,pA))
YA = rnorm(M,1,5) #generate coefficients
YB = rnorm(M,1,5) #generate coefficients
Z = XA[,isA] %*% YA[isA] + XB[,!isA] %*% YB[!isA] #Calculate Z without noise
Z = Z+rnorm(N,sd=sqrt(var(Z))) #add noise 
</code></pre>

<p>Thus given the observations <code>Z</code> and the input data <code>XA, XB</code> (these are known) I want to estimate <code>isA, YA[isA]</code> and <code>YB[!isA]</code>.</p>
"
"0.0806196982594614","0.0658506226617377"," 87872","<p>When I do a (logistic) regression in R, I run something like this:</p>

<pre><code>mydata &lt;- read.csv(""data.csv"")
mylogit &lt;- glm(a ~ c+d, data = mydata, family=""binomial"")
summary(mylogit)
</code></pre>

<p>As of a few months ago, the output for the coefficients might look like this:</p>

<pre><code>Call:
glm(formula = a ~ c + d, family = ""binomial"", data = mydata)
...
Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.6476     0.1898  -8.680  &lt; 2e-16 ***
c             2.4558     0.3414   7.194 6.29e-13 ***
d             2.3783     0.4466   5.326 1.01e-07 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Trying it today (with a newer version of R), the output looks like the following:</p>

<pre><code>Call:
glm(formula = a ~ c + d, family = ""binomial"", data = mydata)
...
Coefficients: (1 not defined because of singularities)
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.6709     0.1924  -8.683  &lt; 2e-16 ***
c1            2.4961     0.3476   7.181 6.94e-13 ***
cc           18.2370   979.6100   0.019    0.985    
d1            2.4524     0.4630   5.296 1.18e-07 ***
dd                NA         NA      NA       NA    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>What do the ""c1"", ""cc"", etc fields mean?  I can't seem to find this any documentation, but perhaps I am looking in the wrong places?</p>
"
"0.068136080998913","0.0649295895722714"," 87946","<p>I have lots of time series with periods: day, week or month. With <code>stl()</code> function or with <code>loess(x ~ y)</code> I can see how trends of particular time series look. I need to detect if trend of time series is increasing or decreasing. How can I manage that?</p>

<p>I tried to compute linear regression coefficients with <code>lm(x ~ y)</code> and play with slope coefficient. (<code>If |slope|&gt;2 and slope&gt;0 then</code> increasing trend, <code>else if |slope|&gt;2 and slope&lt;0</code> â€“ decreasing).
Maybe there is another and more effective method for trend detection? Thank you!</p>

<p>For example: I have <code>timeserie1</code>, <code>timeserie2</code>. I need a simple algorithm that would tell me that <code>timeserie2</code> is an increasing algorithm, and in <code>timeserie1</code>, the trend isn't increasing or decreasing. What criteria should I use?</p>

<p><code>timeserie1</code>:</p>

<pre><code>1774 1706 1288 1276 2350 1821 1712 1654 1680 1451 1275 2140 1747 1749 1770 1797 1485 1299 2330 1822
1627 1847 1797 1452 1328 2363 1998 1864 2088 2084  594  884 1968 1858 1640 1823 1938 1490 1312 2312
1937 1617 1643 1468 1381 1276 2228 1756 1465 1716 1601 1340 1192 2231 1768 1623 1444 1575 1375 1267
2475 1630 1505 1810 1601 1123 1324 2245 1844 1613 1710 1546 1290 1366 2427 1783 1588 1505 1398 1226
1321 2299 1047 1735 1633 1508 1323 1317 2323 1826 1615 1750 1572 1273 1365 2373 2074 1809 1889 1521
1314 1512 2462 1836 1750 1808 1585 1387 1428 2176 1732 1752 1665 1425 1028 1194 2159 1840 1684 1711
1653 1360 1422 2328 1798 1723 1827 1499 1289 1476 2219 1824 1606 1627 1459 1324 1354 2150 1728 1743
1697 1511 1285 1426 2076 1792 1519 1478 1191 1122 1241 2105 1818 1599 1663 1319 1219 1452 2091 1771
1710 2000 1518 1479 1586 1848 2113 1648 1542 1220 1299 1452 2290 1944 1701 1709 1462 1312 1365 2326
1971 1709 1700 1687 1493 1523 2382 1938 1658 1713 1525 1413 1363 2349 1923 1726 1862 1686 1534 1280
2233 1733 1520 1537 1569 1367 1129 2024 1645 1510 1469 1533 1281 1212 2099 1769 1684 1842 1654 1369
1353 2415 1948 1841 1928 1790 1547 1465 2260 1895 1700 1838 1614 1528 1268 2192 1705 1494 1697 1588
1324 1193 2049 1672 1801 1487 1319 1289 1302 2316 1945 1771 2027 2053 1639 1372 2198 1692 1546 1809
1787 1360 1182 2157 1690 1494 1731 1633 1299 1291 2164 1667 1535 1822 1813 1510 1396 2308 2110 2128
2316 2249 1789 1886 2463 2257 2212 2608 2284 2034 1996 2686 2459 2340 2383 2507 2304 2740 1869  654
1068 1720 1904 1666 1877 2100  504 1482 1686 1707 1306 1417 2135 1787 1675 1934 1931 1456 1363 2027
1740 1544 1727 1620 1232 1199
</code></pre>

<p><code>timeserie2</code>:</p>

<pre><code> 122  155  124   97  155  134  115  122  162  115  102  163  135  120  139  160  126  122  169  154
 121  134  143  100  121  182  139  145  135  147   60   58  153  145  130  126  143  129   98  171
 145  107  133  115  113   96  175  128  106  117  124  107  114  172  143  111  104  132  110   80
 159  131  113  123  123  104  101  179  127  105  133  127  101   97  164  134  124   90  110  102
  90  186   79  145  130  115   79  104  191  137  114  131  109   95  119  173  158  137  128  119
 109  120  182  140  133  113  121  110  122  159  129  124  119  109  108   95  167  138  125  105
 139  118  115  166  140  112  116  139  121  109  164  135  118  121  112  111  102  169  136  151
 132  135  130  112  156  134  121  116  114   91   86  141  160  116  118  112   84  114  165  141
 109  123  122  110  100  162  145  121  118  115  107  103  162  142  130  139  134  121  118  164
 147  125  120  134  107  130  158  141  144  148  124  135  118  212  178  154  167  155  176  143
 201  170  144  138  152  136  123  223  189  160  153  190  136  144  276  213  199  211  196  170
 179  460  480  499  550  518  493  557  768  685  637  593  507  611  569  741  635  563  577  498
 456  446  677  552  515  441  438  462  530  699  629  555  641  625  544  585  705  584  553  622
 506  500  533  777  598  541  532  513  434  510  714  631 1087 1249 1102  913  888 1147 1056 1073
1075 1136  927  922 1066 1074  996 1189 1062  999  974 1174 1097 1055 1053 1097 1065 1171  843  441
 552  779  883  773  759  890  404  729  703  810  743  743  946  883  813  876  841  742  715  960
 862  743  806  732  669  621
</code></pre>
"
"NaN","NaN"," 88240","<p>I used R to estimate a regression with both numeric and categorical variables, and obtained coefficient estimates. </p>

<p>However, when I try to make predictions using new data, there appear to be some problems with dimensions. </p>

<p>Is there anything that must be done to the code so that R handles both types of variables? Is there a command other than <code>predict(...)</code> to combine my coefficients with the new data?</p>
"
"0.068136080998913","0.0519436716578171"," 88263","<p>I would like to perform a linear regression. However, the predictor variables are families of variables indexed by time. Let's say the regression problem is:</p>

<p>target ~ x(1)+x(2)+...+x(40)+y(1)+...+y(40)</p>

<p>and imagine the x(t) and y(t) are measurements in 40 successive years. The variables inside the families x(t),y(t) are thus highly collinear.</p>

<p>The usual way to deal with this problem would be variable selection. However, in the problem at hand it is much desired that the coefficients the x(t) resp. y(t) variables are chosen smoothly in time, so setting some of them to 0 is not a good choice.  </p>

<p>My idea was to choose the coefficients for the variable families as the values of penalised splines with knots every 5 years.</p>

<p>Is there a way to do this in R?<br>
Or is there another way of smoothing the coefficients without the use of splines?</p>
"
"0.0304713817668003","0.029037395206952"," 88722","<p>I am building a regression model of time series data in R, where my primary interest is the coefficients of the independent variables. The data exhibit strong seasonality with a trend.</p>

<p><img src=""http://i.stack.imgur.com/GYxaU.png"" alt=""Original data""></p>

<p>The model looks good, with four of the six regressors significant:
<img src=""http://i.stack.imgur.com/ZmoSd.png"" alt=""Model""></p>

<p>Here are the OLS residuals:
<img src=""http://i.stack.imgur.com/EIybo.png"" alt=""Residuals""></p>

<p>I used auto.arima to select the sARIMA structure, and it returns the model (0,1,1)(1,1,0)[12].</p>

<pre><code>fit.ar &lt;- auto.arima(at.ts, xreg = xreg1, stepwise=FALSE, approximation=FALSE)
summary(fit.ar)

Series: at.ts 
ARIMA(0,1,1)(1,1,0)[12]                    

Coefficients:
          ma1    sar1      v1       v2      v3       v4         v5
      -0.7058  0.3974  0.0342  -0.0160  0.0349  -0.0042  -113.4196
s.e.   0.1298  0.2043  0.0239   0.0567  0.0555   0.0333   117.1205

sigma^2 estimated as 3.86e+10:  log likelihood=-458.13
AIC=932.26   AICc=936.05   BIC=947.06

Training set error measures:
                   ME     RMSE      MAE       MPE     MAPE      MASE
Training set 7906.896 147920.3 103060.4 0.1590107 3.048322 0.1150526
</code></pre>

<p>My question is this: based on the parameter estimates and s.e. of the regressors, I believe that none of them are significant - is this correct, and if so, what does it imply if my goal is to interpret the relative importance of these predictors as opposed to forecasting?</p>

<p>Any other advice relative to the process of building this model is welcome and appreciated.</p>

<p>Here are the ACF and PACF for the residuals:</p>

<p><img src=""http://i.stack.imgur.com/a3Gvy.png"" alt=""ACF-PACF""></p>

<pre><code>&gt; durbinWatsonTest(mod.ols, max.lag=12)
 lag Autocorrelation D-W Statistic p-value
   1     0.120522674     1.6705144   0.106
   2     0.212723044     1.4816530   0.024
   3     0.159828108     1.5814771   0.114
   4     0.031083831     1.8352377   0.744
   5     0.081081308     1.6787808   0.418
   6    -0.024202465     1.8587561   0.954
   7    -0.008399949     1.7720761   0.944
   8     0.040751905     1.6022835   0.512
   9     0.129788310     1.4214391   0.178
  10    -0.015442379     1.6611922   0.822
  11     0.004506292     1.6133994   0.770
  12     0.376037337     0.7191359   0.000
 Alternative hypothesis: rho[lag] != 0
</code></pre>
"
"0.0304713817668003","0.029037395206952"," 89033","<p>I have fitted a zero-inflated model with a random effect using a negative binomial distribution in R, using the function glmmadmb. This is due to a large number of zeros and over dispersion. </p>

<p>For a standard poisson regression I know that one must test collinearity, leverage and stability of coefficients (DF Beta). I can do all this in R using various functions for a poisson regression, but none exist for glmmadmb that I can find. I wondered if anyone knew a way to test these? </p>

<p>Thanks</p>
"
"0.0304713817668003","0.029037395206952"," 89130","<p>Consider this logistic regression:</p>

<pre><code>mtcars$vs &lt;- as.factor(mtcars$vs)
log_reg_mtcars &lt;- glm(am ~ vs*wt +vs*mpg, family = ""binomial"", mtcars)
</code></pre>

<p>I tried using the effects package to extract some coefficients from the model:</p>

<pre><code>as.data.frame(effect(""vs"", log_reg_mtcars)) 

NOTE: vs is not a high-order term in the model
  vs       fit       se       lower     upper
1  0 0.3957869 2.916877 0.002150238 0.9950031
2  1 0.0336822 2.181009 0.000484832 0.7146704
</code></pre>

<p>Could anyone explain why the coefficients/standard error for <code>vs</code> given by this code are different. Or in other words, what is the effects package doing?</p>

<pre><code>as.data.frame(summary(log_reg_mtcars)$coef)[2,1:2]

     Estimate Std. Error
vs1 -40.70047   40.53197
</code></pre>
"
"0.0806196982594614","0.076825726438694"," 89474","<p>I ran this ordinal logistic regression in R:</p>

<pre><code>mtcars_ordinal &lt;- polr(as.factor(carb) ~ mpg, mtcars)
</code></pre>

<p>I got this summary of the model:</p>

<pre><code>summary(mtcars_ordinal)

Re-fitting to get Hessian

Call:
polr(formula = as.factor(carb) ~ mpg, data = mtcars)

Coefficients:
      Value Std. Error t value
mpg -0.2335    0.06855  -3.406

Intercepts:
    Value   Std. Error t value
1|2 -6.4706  1.6443    -3.9352
2|3 -4.4158  1.3634    -3.2388
3|4 -3.8508  1.3087    -2.9425
4|6 -1.2829  1.3254    -0.9679
6|8 -0.5544  1.5018    -0.3692

Residual Deviance: 81.36633 
AIC: 93.36633 
</code></pre>

<p>I can get the log odds of the coefficient for <code>mpg</code> like this:</p>

<pre><code>exp(coef(mtcars_ordinal))
 mpg 
0.7917679 
</code></pre>

<p>And the the log odds of the thresholds like:</p>

<pre><code>exp(mtcars_ordinal$zeta)

       1|2         2|3         3|4         4|6         6|8 
0.001548286 0.012084834 0.021262900 0.277242397 0.574406353 
</code></pre>

<p>Could someone tell me if my interpretation of this model is correct:</p>

<blockquote>
  <p>As <code>mpg</code> increases by one unit, the odds of moving from category 1 of <code>carb</code> into any of the other 5 categories, decreases by -0.23. If the log odds crosses the threshold of 0.0015, then the predicted value for a car will be category 2 of <code>carb</code>.  If the log odds crosses the threshold of 0.0121, then the predicted value for a car will be category 3 of <code>carb</code>, and so on.</p>
</blockquote>
"
"0.0963589698356145","0.0918243061724248"," 89692","<p>My data has 3 major inputs: <code>BLDDAY</code> (a factor), <code>BLDMNT</code> (a factor), and <code>D_BLD_SER</code> (days as an integer variable).  The output is whether input variable has any impact on failure.  My model is: <code>model = glm(FAILED~BLDDAY+BLDMNT+D_BLD_SER, family=""binomial"", data=data_list)</code>.  (I used <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">UCLA's statistics help site's guide to logistic regression in R</a> to build this model.)  </p>

<p>Output: </p>

<pre><code>Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.3282  -0.9123  -0.8128   1.4056   2.1053  

Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)     -0.7672583  0.1317132  -5.825 5.70e-09 ***
BLDDAYMonday    -0.1545646  0.0839380  -1.841  0.06556 .  
BLDDAYSaturday  -0.1257976  0.2028259  -0.620  0.53511    
BLDDAYSunday    -0.1183008  0.1868713  -0.633  0.52669    
BLDDAYThursday  -0.2007452  0.0772653  -2.598  0.00937 ** 
BLDDAYTuesday    0.0480453  0.0758603   0.633  0.52651    
BLDDAYWednesday -0.0358585  0.0760027  -0.472  0.63707    
BLDMNTAug        0.3009445  0.1405545   2.141  0.03226 *  
BLDMNTDec        0.5562170  0.1338467   4.156 3.24e-05 ***
BLDMNTFeb        0.3334978  0.2133475   1.563  0.11801    
BLDMNTJan        0.4076504  0.2277978   1.790  0.07353 .  
BLDMNTJul        0.1306585  0.1415302   0.923  0.35591    
BLDMNTJun       -0.0357361  0.1428105  -0.250  0.80241    
BLDMNTMar        0.4570491  0.1949815   2.344  0.01907 *  
BLDMNTMay       -0.2292620  0.1614577  -1.420  0.15562    
BLDMNTNov        0.3060012  0.1334034   2.294  0.02180 *  
BLDMNTOct        0.2390501  0.1341877   1.781  0.07484 .  
BLDMNTSep        0.2481405  0.1384901   1.792  0.07317 .  
D_BLD_SER       -0.0020960  0.0003367  -6.225 4.82e-10 ***

(Dispersion parameter for binomial family taken to be 1)
    Null deviance: 10288  on 8182  degrees of freedom
Residual deviance: 10154  on 8164  degrees of freedom
AIC: 10192
Number of Fisher Scoring iterations: 4
</code></pre>

<p>The ANOVA table is the following:</p>

<pre><code>anova(model, test=""Chisq"")
Analysis of Deviance Table
Model: binomial, link: logit
Response: FAILED
Terms added sequentially (first to last)

          Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                       8182      10288              
BLDDAY     6   20.392      8176      10268  0.002357 ** 
BLDMNT    11   70.662      8165      10197 9.142e-11 ***
D_BLD_SER  1   43.797      8164      10154 3.642e-11 ***
</code></pre>

<p>My questions are:</p>

<ol>
<li><p>Although the p-values for all three components are less than 0.05, which can be considered as significant, the deviance reduced due to each component is less than 1% of the total deviance. <strong>Normally the interpretation of output like this is input parameter affects output and it's better to use this parameter then using noting.</strong> But does it really make sense of taking this parameter as significant input?</p></li>
<li><p>The p-values for <code>BLDDAY</code> and <code>BLDMNT</code> given by <code>anova()</code> is the overall p-value,  which is significant, but <code>summary()</code> gives detailed impact of each factor level. If I consider the p-values for each factor overall <code>BLDDAY</code> is significant but individually only <code>BLDDAYThursday</code> is significant. I am bit confused not as whether to consider <code>BLDDAY</code> as significant input, or Thursday only, or Thursday &amp; Friday both.</p></li>
</ol>
"
"0.0967596325610309","0.100588487635796"," 89714","<p>I have ran a negative binomial regression. I'm guessing the use of a negative binomial regression is not ideal given my design, but I'm hoping I can 'get away with it', as it seems to be working fairly well. </p>

<p>Here's my design:</p>

<ul>
<li><p>Count the the total number of geese in a flock</p></li>
<li><p>Count the number of these that are vigilant.  </p></li>
<li><p>Then divide number vigilant/number in    flock, to get a percentage
that are vigilant  </p></li>
<li>Predictor variables are    flock size (count),    water height in
metres, date, subspecific identity of the    flock and
the part of my study site that    the flock were in (ssp.zone)</li>
</ul>

<p>Below is a plot of my response variable (<code>alert.integer</code>)</p>

<p><img src=""http://i.stack.imgur.com/RbEJU.jpg"" alt=""enter image description here""></p>

<p>Below is a summary of the model</p>

<pre><code>Call:
glm.nb(formula = alert.integer ~ ssp * water.height + ssp * ssp.zone + 
    ssp * count + ssp * I(as.Date(datetime)), data = dataScanSampling_sub, 
    init.theta = 1.168496598, link = log)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.0035  -0.9556  -0.3382   0.3886   2.0942  

Coefficients:
                                                    Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                                        3.971e+02  6.239e+01   6.365 1.95e-10 ***
sspLight-bellied Brent Goose                      -4.369e+02  7.294e+01  -5.990 2.10e-09 ***
water.height                                       2.978e-01  8.719e-02   3.415 0.000637 ***
ssp.zonelight zone                                -1.214e+00  2.103e-01  -5.775 7.70e-09 ***
count                                             -5.226e-03  8.506e-04  -6.143 8.09e-10 ***
I(as.Date(datetime))                              -2.461e-02  3.904e-03  -6.303 2.93e-10 ***
sspLight-bellied Brent Goose:water.height          2.743e-03  1.015e-01   0.027 0.978438    
sspLight-bellied Brent Goose:ssp.zonelight zone    1.344e+00  2.635e-01   5.102 3.36e-07 ***
sspLight-bellied Brent Goose:count                 4.174e-03  8.766e-04   4.761 1.93e-06 ***
sspLight-bellied Brent Goose:I(as.Date(datetime))  2.726e-02  4.564e-03   5.973 2.33e-09 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for Negative Binomial(1.1685) family taken to be 1)

    Null deviance: 558.21  on 367  degrees of freedom
Residual deviance: 433.87  on 358  degrees of freedom
AIC: 3236.4

Number of Fisher Scoring iterations: 1


              Theta:  1.1685 
          Std. Err.:  0.0894 

 2 x log-likelihood:  -3214.4300 
</code></pre>

<p>And here are residuals from model:</p>

<p><img src=""http://i.stack.imgur.com/hnBEW.jpg"" alt=""enter image description here""></p>

<p>So taking into account the description of my design, the plot of the response variable (<code>alert.integer</code>), the residuals plot and the output from <code>summary</code>(), can I get away with a negative binomial regression model here? </p>
"
"0.114013470672957","0.100887413963854"," 89760","<p>I am trying to use R to find the optimal solution for my problem with positive coefficients. Here are my data:  </p>

<pre><code>      th inp      tcyc        tinst     tmem      tcom
  1   2   2  26219765385  1975872868  52449810   782964
  2   2   4  38080459431  3155342008  76744867  1878903
  3   2   8  64572439641  6230494010 137754355  4351706
  4   2  16 140168021516 13757989992 285524252 10605705
  5   2  32 308925389816 31497131498 628391048 26040711
  6   4   2  13206650786   988226883  25631315   844126
  7   4   4  19078145632  1577873809  37085281  2125333
  8   4   8  33742095874  3114415906  65962626  5222236
  9   4  16  70956149286  6881357755 134957687 12180392
  10  4  32 153411672670 15754506070 296548768 31057252
  11  8   2   6572843040   494094967  12380740   808816
  12  8   4   9452222628   788984621  17538152  2034061
  13  8   8  16765943294  1557329849  30549900  5016827
  14  8  16  34677550217  3440679505  61614420 12493699
  15  8  32  74852648112  7876116794 133525620 29824686
  16 16   2   3252373719   247026385   5958559   672396
  17 16   4   4669800482   394452497   8097991  1676579
  18 16   8   8269859136   778889584  13651458  4196829
  19 16  16  16353025378  1720301596  26775255 10393194
  20 16  32  37113657641  3938965759  55505822 25011009
  21 32   2   1630888153   123512114   2683400   461526
  22 32   4   2293598746   197173135   3682504  1213596
  23 32   8   4045995970   389408822   5858031  3055324
  24 32  16   8217603991   860041282  10973460  7502244
  25 32  32  17978101850  1969647650  22909347 17953100
  26 48   2   1064344042    82295143   1822133   381178
  27 48   4   1523091067   131488491   2331228   949354
  28 48   8   2677097592   259536252   3552229  2381626
  29 48  16   5400541381   573140686   6489032  5875310
  30 48  32  11837404077  1313066425  13318331 13968230
</code></pre>

<p>I use linear regression in R, <code>s &lt;- lm(tcyc ~ 0+tinst+tmem+tcom, data=fit)</code>, to get the optimal value with intercept 0. But I get negative coefficients which does not make any sense.</p>

<pre><code>coef(s)

 tinst      tmem      tcom 
20.8745 -281.2288 -320.7204 
</code></pre>

<p>I am not sure whether is it the best way to model and find the optimal parameter for <code>tinst</code>, <code>tmem</code> and <code>tcom</code>. How do you find positive coefficients for the model?</p>

<p>Further explaining this problem in Detail:::</p>

<p>Background:
Trying to predict the execution time of an application in the future many-core systems empirically by learning the application behavior. As it is a multithreaded program, it will have communication contnention bottleneck if the application demands high inter-core communication. The general system equation looks like</p>

<p>Total executiong time cycles (T_cyc) = Total cycles spent in Instruction (T_inst) + Total cycle spent in Memory instructions (T_mem) + Total cycle spent in Communication (T_com)</p>

<p>i,e T_cyc=T_inst+T_mem+T_com.</p>

<p>If I use a simulator I can get the T_inst,T_mem and T_com directly and find out the independent contribution of each component to the T_cyc. But using a hardware, I can only get the counts or number of events. Ie, N_inst, N_mem and N_com. 
So what I have is </p>

<p>T_cyc= a* N_inst + b* N_mem + c* N_com</p>

<p>Where a,b,c has to be determined.</p>

<p>I tried solving the problem using lsqnonneg (non-negative least square method)  in MATLAB to find the a,b,c. At times from the data I get b and c value ZERO which is totally meaningless.</p>

<p>Things to notice:
N_inst is a very high value. N_mem and N_com are bit lower in magnitude and hence I face this problem of b and c results as ZERO. </p>

<p>Questions:
1.  Is this a proper tool to solve such a linear equation system? If not, what else should I try?
2.  Is it a problem due to the sample size fed to the solver?
3.  I see that for most applications trend of N_cyc, N_inst,N_mem are monotonic but N_com is non-monotonic and can it affect the solved values? If so, how to isolate this component and find its contribution individually?</p>
"
"0","0.029037395206952"," 89795","<p>Sorry for my bad English, could you suggest R-code for the implementation of difference-in-difference regression? I don't understand how many coefficients I need.
In my analysis I compare the effect of a new law on the stock exchange volume, I have 2 periods and 2 samples. 
Thanks a lot!</p>

<p>Thanks Jeremy, but in the Imbens's model the regression contains a series of multiplications between variables and dummies. I used: </p>

<pre><code>lr1 &lt;- lm(VOLUME ~ DUMMYCAP + DUMMYTIME + DUMMYCAP*DUMMYTIME ) 
</code></pre>

<p>but the result of regression is not significant. </p>

<p>This question is about code and its economic significance. I'm sorry if this post is not appropriate for the site. But if someone has made a test of this type and knows how to write the code I would be grateful </p>
"
"0.110147477177496","0.104963924850184"," 89897","<p>I'm trying to improve my statistics knowledge using football(soccer) results. So, this is a self study problem. You don't have to provide a complete solution. Pointing me in the right direction is sufficient as well.</p>

<p>I have a book where the author tries to show how a current season home edge can be estimated using information from earlier seasons. He writes ""In order to incorporate both aspects we have treated the home edge of the last 10 years as a regression problem."" And continues by presenting this formula:</p>

<p>$$
HE_{est} = c_1HE_{last3} + c_2HE_{current}
$$</p>

<p>$HE_{current}$ is the current season home edge which is updated using information from the last three seasons ($HE_{last3}$) to arrive at a home edge estimate $HE_{est}$.</p>

<p>He then presents the following table where $N$ is the number of match days:</p>

<pre><code>N | 0 | 6 | 11| 17| 22| 27| 33|
c1|.89|.80|.67|.62|.61|.53|.52|
c2| 0 |.11|.24|.29|.31|.40|.42|
</code></pre>

<p>Here's what I understand he is doing:
First of all, this is a multivariate linear regression problem with $c_1$ and $c_2$ being the coefficients. Since the formula is probably an attempt at predicting the season home edge from running season information, I assume that $HE_{est}$, the dependent variable, is actually the future home advantage. So, I figure, he calculated the home edge of each season of the last ten years and the home edge of the preceding seasons. Finally, he also calculated the home edge at each $N$ for each season (e.g. $HE$ at $N=6$, at $N=11$ and so on)  and estimated the coefficients $c_1$ and $c_2$ using the linear model above.</p>

<p>I tried to do this with R myself and I couldn't reproduce the numbers in the table above. My data looks like this:</p>

<pre><code>data &lt;- matrix(c(.15,.46,.46,.5,.02,.38,.21,.4,.43,.26,.43,.18,.57,.54,.16,.42,.20,.49,.47,.18,.43,.28,.47,.45,.14,.41,.19,.51,.46,.2,.4,.19,.53,.48,.2,.51,.49,.34,.37,.4),5,8)

colnames(data) &lt;-c(""HE.6"",""HE.11"",""HE.17"",""HE.22"",""HE.27"",""HE.33"",""HE.curr"",""HE.last3"")
</code></pre>

<p>This is only the information of 5 years. However, it's the same years he used for his data set, just the first five years are missing. So, the resulting coefficients should be good approximations of what he calculated. However, they are not. If I do e.g.:</p>

<pre><code>lm(HE.curr ~ HE.last3 + HE.17,as.data.frame(data))
</code></pre>

<p>I get:</p>

<pre><code>Coefficients:
(Intercept)         HE.17     HE.last3  
    0.09440      0.79265     -0.07591 
</code></pre>

<p>This result is incorrect. <code>HE.17</code> is $HE_{current}$ at $N=17$. The coefficient $c_2$ should be 0.29 but according to this linear model it's 0.79. Since my results are wrong and he doesn't report any intercept (except maybe $c_1=0.89$ at $N=0$ which I'm not sure about), I get the impression, I'm using the wrong model here. What am I missing?</p>
"
"0.139781890921503","0.133203739841069"," 90503","<p>I have zero inflated response variable I am trying to predict. I am facing few issues applying different regression models that should correct for this.</p>

<p>This is my 10,000 obs dataframe</p>

<pre><code>    e_weight       left_size         right_size        time_diff        
 Min.   :0.000   Min.   :  1.000   Min.   :  1.000   Min.   :      737  
 1st Qu.:0.000   1st Qu.:  1.000   1st Qu.:  1.000   1st Qu.:  4669275  
 Median :0.000   Median :  3.000   Median :  3.000   Median : 12263474  
 Mean   :0.022   Mean   :  6.194   Mean   :  5.469   Mean   : 21000288  
 3rd Qu.:0.000   3rd Qu.:  5.000   3rd Qu.:  5.000   3rd Qu.: 25420278  
 Max.   :3.000   Max.   :792.000   Max.   :792.000   Max.   :155291532
</code></pre>

<p>Here the frequency count for my 3 variables
<img src=""http://i.stack.imgur.com/1yIvx.jpg"" alt=""enter image description here"">
Indeed I have a problem with zeros...</p>

<p>I tried respectively a Zero-Inflated Negative Binomial Regression and a Zero-inflated Poisson Regression</p>

<pre><code>library(pscl)
m1 &lt;- zeroinfl(e_weight ~ left_size*right_size | time_diff, data = s)
summary(m1)

# Call:
# zeroinfl(formula = e_weight ~ left_size * right_size | time_diff, data = s)
#
# Pearson residuals:
#     Min      1Q  Median      3Q     Max 
# -1.4286 -0.1460 -0.1449 -0.1444 19.6054 
#
# Count model coefficients (poisson with log link):
#                        Estimate Std. Error z value Pr(&gt;|z|)    
#  (Intercept)          -3.8826386  0.0696970 -55.707  &lt; 2e-16 ***
#  left_size             0.0022261  0.0006195   3.594 0.000326 ***
#  right_size            0.0033622         NA      NA       NA    
#  left_size:right_size  0.0001715         NA      NA       NA    
# 
# Zero-inflation model coefficients (binomial with logit link):
#               Estimate Std. Error  z value Pr(&gt;|z|)    
# (Intercept)  1.753e+01  6.011e+00    2.916  0.00354 ** 
#  time_diff   -3.342e-04  1.059e-06 -315.773  &lt; 2e-16 ***
#  ---
#  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
#
# Number of iterations in BFGS optimization: 28 
#  Log-likelihood: -1053 on 6 Df
#  Warning message:
#  In sqrt(diag(object$vcov)) : NaNs produced
</code></pre>

<p>and </p>

<pre><code>library(MASS)
m2 &lt;- glm.nb(e_weight ~ left_size*right_size + time_diff, data = s) 
</code></pre>

<p>which gives</p>

<pre><code>There were 22 warnings (use warnings() to see them)
warnings()
Warning messages:
1: glm.fit: algorithm did not converge
...
21: glm.fit: algorithm did not converge
22: In glm.nb(e_weight ~ left_size * right_size + time_diff,  ... :
  alternation limit reached
</code></pre>

<p>If  I ask a summary for the second model </p>

<pre><code>summary(m2)

# Call:
# glm.nb(formula = e_weight ~ left_size * right_size + time_diff, 
#     data = s, init.theta = 0.1372733321, link = log)
#
# Deviance Residuals: 
#     Min       1Q   Median       3Q      Max  
# -3.4645  -0.2331  -0.1885  -0.1266   2.7669  
# 
# Coefficients:
#                        Estimate Std. Error z value Pr(&gt;|z|)    
# (Intercept)          -3.239e+00  1.090e-01 -29.699  &lt; 2e-16 ***
# left_size            -4.462e-03  1.835e-03  -2.431 0.015047 *  
# right_size           -7.144e-03  2.118e-03  -3.374 0.000742 ***
# time_diff            -6.013e-08  8.584e-09  -7.005 2.48e-12 ***
# left_size:right_size  4.691e-03  2.749e-04  17.068  &lt; 2e-16 ***
# ---
# Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
#
# (Dispersion parameter for Negative Binomial(0.1374) family taken to be 1)
# 
#     Null deviance: 1106.5  on 9999  degrees of freedom
# Residual deviance:  958.5  on 9995  degrees of freedom
# AIC: 1967.2
# 
# Number of Fisher Scoring iterations: 12
# 
# 
#              Theta:  0.1373 
#          Std. Err.:  0.0223 
# Warning while fitting theta: alternation limit reached 
#
#
# 2 x log-likelihood:  -1955.2260
</code></pre>

<p>Also both models have very low p-values for heteroskedasticity </p>

<pre><code>bptest(m1)
# 
#   studentized Breusch-Pagan test
#
# data:  m1
# BP = 244.832, df = 3, p-value &lt; 2.2e-16
#
bptest(m2)
# 
#   studentized Breusch-Pagan test
#
# data:  m2
# BP = 277.2589, df = 4, p-value &lt; 2.2e-16
</code></pre>

<p>How should I approach this regression. Would make sense to simply add 1 to all my dataframe before running any regression?</p>
"
"0.101414888671788","0.0885887685054121"," 90906","<p>I'm new to R and logistic regression and have to admit that I don't really know how to interpret the result. I'm trying to compute a pretty simple model with 2 predictors (A and B). When I first try to compute models with the predictors one by one they are both significant. When I put them together and add an interaction term they lose their significance (but the interaction term is weakly significant). I interpret this as A and B are overlapping and no longer significant when the oter parameter is hold constant. Right?</p>

<p>But now to the part I don't know how to interpret. I make predictions from my models (see code below) and then run t-tests for the predictions vs. the depending variable. I think this should give a hint on how good the model is (is there a better way?). When I do it this way I get a much lower p-value for the model with both A and B. I think this is contradictory. The first part tells me that A doesn't provide any significant information to the model when combined with B, but on the other hand I get much better predictions. I guess something is really wrong, but I can't figure out what. Can you help me?</p>

<pre><code>model1=glm(f~A, , family=binomial(link=""logit""))
model2=glm(f~B,   family=binomial(link=""logit""))
model3=glm(f~A*B, family=binomial(link=""logit""))
summary(model1)
summary(model2)
summary(model3)
p1=predict(model1, newdata=data, type=""response"", na.rm=TRUE)
p2=predict(model2, newdata=data, type=""response"", na.rm=TRUE)
p3=predict(model3, newdata=data, type=""response"", na.rm=TRUE)
t.test(p1~f)
t.test(p2~f)
t.test(p3~f)
</code></pre>

<p>Part of the output:  </p>

<pre><code>&gt; summary(model1)
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.9756     0.3499  -5.647 1.64e-08 ***
A            -0.5898     0.2119  -2.784  0.00537 ** 

&gt; summary(model2)
              Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)  8.354e-01  1.309e+00   0.638   0.5234  
B           -1.028e-04  5.122e-05  -2.007   0.0447 *

&gt; summary(model3)
Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)  1.254e+00  1.705e+00   0.735    0.462  
A            1.589e+00  9.743e-01   1.631    0.103  
B           -1.324e-04  7.333e-05  -1.805    0.071 .
A:B         -9.418e-05  4.632e-05  -2.033    0.042 *

&gt; t.test(p1~f)
t = -2.614, df = 11.83, p-value = 0.02286

&gt; t.test(p2~f)
t = -1.8702, df = 15.679, p-value = 0.08024

&gt; t.test(p3~f)
t = -4.9777, df = 17.344, p-value = 0.0001084
</code></pre>
"
"0.0304713817668003","0.029037395206952"," 91386","<p>Can anyone expalin to me in simple terms what happens when we use weights in <code>regsubsets</code> or <code>lm</code> in R? What effect do weights have on a linear regression? 
for example : </p>

<pre><code>Model1&lt;-lm(Ozone~Solar.R,data=airquality)
summary(Model1)
#Coefficients:
#            Estimate Std. Error t value Pr(&gt;|t|)    
#(Intercept) 18.59873    6.74790   2.756 0.006856 ** 
#Solar.R      0.12717    0.03278   3.880 0.000179 ***
Model1&lt;-lm(Ozone~Solar.R,data=airquality,weights=(2*seq(nrow(airquality),1,-1)))
summary(Model1)
#Coefficients:
#            Estimate Std. Error t value Pr(&gt;|t|)    
#(Intercept) 18.57106    6.26067   2.966 0.003704 ** 
#Solar.R      0.10824    0.02927   3.699 0.000341 ***
</code></pre>

<p>please explain the changes in intercepts and slope.</p>
"
"0.091874672876503","0.0875510407188402"," 91592","<p>Can anyone point me towards a good explanation of when a residualized variable in a regression will give you the same answer as using a non-residualized variable with controls? </p>

<p>For instance, say I want to know the effect of a variable x on y and need to control for a and b. In a classic linear model framework I can either add a and b as covariates (i.e., control variables) to the model of y on x, or I can first regress x on a and b, and then use the residuals from this regression (the residualized x) to predict y. Both will give the same coefficient for x.</p>

<p>This works in the linear model case, but does a residualized x give the same coefficient as x with controls for other types of models, e.g., logit models or poisson models? My own simple simulations suggest they do not (see R code below), but I am trying to understand why, and if residualization can ever be used in place of adding controls outside of the linear model framework. Can anyone point me towards a good explanation?</p>

<pre><code>#generate the data
n=10000
set.seed(3345)
a=rnorm(n); b=rnorm(n)
x = .4*a + .4*b*b + rnorm(n)
y = .5*x + .3*a + .3*b*b + rnorm(n)

## LINEAR MODEL ####
#a model with controls gets the right coefficient
summary(lm(y ~ x + a + I(b^2)))
residmod=lm(x ~ a + I(b^2))
x.resid=resid(residmod)
#using a residualized variable gets the same coefficient
summary(lm(y ~ x.resid))

## LOGIT MODEL ####
y=.5*x + .3*a + .3*b*b + rlogis(n)
ydichot=ifelse(y &gt;0, 1, 0)
#a model with controls gets the right coefficient
summary(glm(ydichot ~ x + a + I(b^2), family=binomial))
#using a residualized variable does NOT get the same coefficient
summary(glm(ydichot ~ x.resid, family=binomial))

## POISSON MODEL ####
mu=exp(.5*x + .3*a + .3*b*b)
ycount=rpois(n, mu)
summary(glm(ycount ~ x + a + I(b^2), family=poisson))
#using a residualized variable does NOT get the same coefficient
summary(glm(ycount ~ x.resid, family=poisson))
</code></pre>
"
"0.068136080998913","0.0649295895722714"," 91700","<p>I am trying to understand the steps behind the linear regression process. I already have a linear model like:</p>

<p><code>lmodel1 &lt;- lm(y~x1+x2+x3, data=dataset)</code></p>

<p>for which R calculates several different things (<code>Coefficients, Intercept, Residuals, F-statistic</code> and <code>p-value</code>) among  others.</p>

<p>At this point, i am mostly interested in <code>F-statistic</code> and <code>p-value</code>.
So far, i have concluded to the following:</p>

<p>The process is iterative and begins taking under consideration every variable. In order to achieve an optimal <code>y</code> some <code>x</code> variables have to be ""taken out"". This comes as a result of calculating F-statistic, which quantifies the importance of each <code>xi</code> and the dependent variable <code>y</code>.
When <code>F value</code> is smaller than <code>p-value</code>(?) that variable is removed.
Next step of the process is to compare that <code>F-statistic</code> of a <code>xi</code> independent variable, with an <code>F-to-enter</code> and <code>F-to-remove</code> in order see if the removed variable will be re-inserted to the equation.(?)</p>

<p>Now, please do correct me if i am wrong regarding the steps desribed above.
Is that what happens under <code>lm()</code>'s hood. Are those the right variables.?</p>

<p>R-wise speaking how does these values can be shown, inserted or calculated in a multiple linear regression model.? How is <code>ANOVA</code> related to the above?</p>

<p>I am afraid R's <code>summary</code> and <code>help</code>  take too much for granted.</p>

<p>Thanks in advance for any suggestions.</p>
"
"0.0430930413588572","0.0410650781176591"," 91747","<p>I have a logistic regression with data that are kind of like this:</p>

<pre><code>y &lt;- rep(c(""A"", ""A"", ""B""), each = 30)
x &lt;- c(     rep(1, 12), rep(2, 18), rep(3, 16), rep(4, 12), rep(5, 2),
            rep(1, 3), rep(2, 5), rep(3, 8), rep(4, 10), rep(5, 4)  )

da &lt;- data.frame(y = y, x = x)
table(da)
   x
y    1  2  3  4  5
  A 12 18 16 12  2
  B  3  5  8 10  4
</code></pre>

<p>I'd like to show that there are more A's than B's in <code>y</code> (instead of a Bernoulli with <code>p</code> = 0.5), even after controlling for <code>x</code>, so I fitted two logistic regression models and used an ANOVA to compare them.</p>

<pre><code>mlogis.x              &lt;- glm(y ~ x,     family = binomial, da)
mlogis.x.no_intercept &lt;- glm(y ~ x + 0, family = binomial, da)

summary(mlogis.x.no_intercept)
summary(mlogis.x)

anova(mlogis.x.no_intercept, mlogis.x, test = ""Chisq"")
</code></pre>

<p>I have a few questions:</p>

<ul>
<li>Does what I did make sense overall?</li>
<li>Is it okay to not have an intercept in the more basic model and then add it to the full(er) model?</li>
<li>The coefficient for <code>x</code> changes sign between the two models, how should I interpret this?</li>
</ul>
"
"0.075412822378","0.0821301562353182"," 91903","<p>Is the probability calculated by a logistic regression model (the one that is logit transformed) the fit of cumulative distribution function of successes of original data (ordered by the X variable)?</p>

<p><strong>EDIT:</strong> In other words - how to plot the probability distribution of the original data that you get when you fit a logistic regression model?</p>

<p>The motivation for the question was Jeff Leak's example of regression on the Raven's score in a game and whether they won or not (from Coursera's Data Analysis course). Admittedly, the problem is artificial (see @FrankHarrell's comment below). Here is his data with a mix of his and my code:</p>

<pre><code>download.file(""http://dl.dropbox.com/u/7710864/data/ravensData.rda"", 
              destfile=""ravensData.rda"", method=""internal"")
load(""ravensData.rda"")

plot(ravenWinNum~ravenScore, data=ravensData)
</code></pre>

<p><img src=""http://i.stack.imgur.com/Cr5ka.png"" alt=""enter image description here"">  </p>

<p>It doesn't seem like good material for logistic regression, but let's try anyway:</p>

<pre><code>logRegRavens &lt;- glm(ravenWinNum ~ ravenScore, data=ravensData, family=binomial)
summary(logRegRavens)
# the beta is not significant

# sort table by ravenScore (X)
rav2 = ravensData[order(ravensData$ravenScore), ]

# plot CDF
plot(sort(ravensData$ravenScore), cumsum(rav2$ravenWinNum)/sum(rav2$ravenWinNum), 
         pch=19, col=""blue"", xlab=""Score"", ylab=""Prob Ravens Win"", ylim=c(0,1), 
         xlim=c(-10,50))
# overplot fitted values (Jeff's)
points(ravensData$ravenScore, logRegRavens$fitted, pch=19, col=""red"")
# overplot regression curve
curve(1/(1+exp(-(logRegRavens$coef[1]+logRegRavens$coef[2]*x))), -10, 50, add=T)
</code></pre>

<p>If I understand logistic regression correctly, R does a pretty bad job at finding the right coefficients in this case.  </p>

<p><img src=""http://i.stack.imgur.com/Cb6o8.png"" alt=""enter image description here""></p>

<ul>
<li>blue = original data to be fitted, I believe (CDF)  </li>
<li>red = prediction from the model (fitted data = projection of original data onto regression curve)</li>
</ul>

<p><strong>SOLVED</strong><br>
 - lowess seems to be a good non-parametric estimator of the original data = what is being fitted (thanks @gung). Seeing it allows us to choose the right model, which in this case would be adding squared term to the previous model (@gung)<br>
 - Of course, the problem is pretty artificial and modelling it rather pointless in general (@FrankHarrell)<br>
 - in regular logistic regression it's not CDF, but point probabilities - first pointed out by @FrankHarrell; also my embarrassing inability to calculate CDF pointed out by @gung.</p>
"
"0.091874672876503","0.0787959366469562"," 92132","<p>Let's say I have a function to simulate data for negative binomial regression: </p>

<pre><code>simnegbin = function(X, beta, alpha) {
lambda = exp(1 + X %*% beta)
y=NULL
for (j in 1:length(lambda)) y = c(y,rnbinom(1,mu = lambda[j],size = alpha))
return(y)
}
</code></pre>

<p>And I call that function like this:</p>

<pre><code>set.seed(123)
x1 = rnorm(100)
X = matrix(c(x1,x1^2), ncol = 2) # data with quadratic effect
beta = c(0.5,-0.5) # coefficients one with negative sign
alpha = 1
y = simnegbin(X, beta, alpha)
</code></pre>

<p>And then I fit two models to this simulated data. </p>

<pre><code>NB = MASS::glm.nb(y ~ X)
Normal = glm(y ~ X)
</code></pre>

<p>And look at the resulting coefficients. Nothing too strange. The model assuming a normal distribution is slightly higher, but not too bad. </p>

<pre><code>coef(NB)
coef(Normal)
</code></pre>

<p>Normal:</p>

<pre><code>(Intercept)      X1          X2 
  2.3997424   0.4019599  -0.5234614 
</code></pre>

<p>NB: </p>

<pre><code>(Intercept)      X1          X2 
  0.8992080   0.3573460  -0.4023558
</code></pre>

<p>But then when I simply switch the sign to positive of the coefficient in a second call to the function:</p>

<pre><code>X = matrix(c(x1,x1^2), ncol = 2)
beta = c(0.5,0.5) # coefficients with positive sign
alpha = 1
y = simnegbin(X, beta, alpha)
</code></pre>

<p>And then I fit two models using the same simulated data. </p>

<pre><code>NB = MASS::glm.nb(y ~ X)
Normal = glm(y ~ X)

coef(Normal)
coef(NB)
</code></pre>

<p>Normal: </p>

<pre><code>(Intercept)      X1          X2 
    1.342937    3.162534    4.130669 # Biased term 
</code></pre>

<p>NB:</p>

<pre><code>(Intercept)     X1          X2 
0.9097782   0.2905278   0.4587727
</code></pre>

<p>The quadratic coefficient of the glm assuming the normal distribution is <strong>super biased</strong>. </p>

<p>Does anyone have any intuition about why this would be? </p>
"
"0.0304713817668003","0.029037395206952"," 92205","<p>I am using <code>randomForest</code> to generate a model, and at the end I don't know how I can get the final coefficients that the model is fitting. I know that for linear regression, you just type <code>summary(lm)</code> where <code>lm</code> is your model, and you get the coefficients.</p>
"
"0.122096950516762","0.116351054666701"," 92737","<p>In my data, I have two treatment conditions with repeated measures for each subject. I would like to run a mixed logistic regression separately for each of my two conditions where my binary outcome DV (dependent variable) is regressed on my IV (independent variable) and also have a random slope and intercept fitted for each subject.</p>

<p>So, I run the following:</p>

<pre><code>modelT0 &lt;- glmer(DV ~ IV + (1|subject) + (0 + IV|subject), data = D0, family = binomial)
modelT1 &lt;- glmer(DV ~ IV + (1|subject) + (0 + IV|subject), data = D1, family = binomial)
</code></pre>

<p>In the above, D0 and D1 are data sets restricted to treatment conditions 0 and 1, respectively. What I would like to do is compare the estimated fixed effects coefficient on IV across conditions to see if it significantly changes. To do this, I pool D0 and D1 into a single data set, D, and create a treatment indicator that takes value 0 in D0 and 1 in D1. I then run:</p>

<pre><code>model &lt;- glmer(DV ~ IV + treatment + treatment:IV + (1 + treatment|subject:treatment) 
               + (0 + IV + treatment:IV|subject:treatment), data = D, family = binomial)
</code></pre>

<p>I should be able to look at the fixed effects coefficient on treatment:IV to get my answer, but the issue is that for whatever combination of random effects I seem to specify, the coefficients from the pooled regression are slightly different from the regressions specified by treatment. So for instance, the fixed effect coefficient on treatment:IV plus the one on IV in model is not equal to the coefficient on IV in model1.</p>

<p>Any idea about what I might be doing wrong or how to answer the question I have? Thanks!</p>

<p>EDIT:</p>

<p>As per Henrik's suggestion, I'm copying the random effects output of the models below:</p>

<p>summary(modelT0):</p>

<pre><code>    Random effects:
    Groups    Name        Variance  Std.Dev. 
    subject   (Intercept) 1.412e-07 0.0003758
    subject.1 IV          1.650e+00 1.2844341
</code></pre>

<p>summary(modelT1):</p>

<pre><code>    Random effects:
    Groups    Name        Variance Std.Dev.
    subject   (Intercept) 0.00378  0.06148 
    subject.1 IV          0.26398  0.51379 
</code></pre>

<p>summary(model):</p>

<pre><code>    Random effects:
    Groups              Name         Variance  Std.Dev. Corr 
    subject.treatment   (Intercept)  0.0005554 0.02357       
                        treatment    0.0066042 0.08127  -0.88
    subject.treatment.1 IV           1.6500112 1.28453       
                        IV:treatment 1.0278663 1.01384  -0.93
</code></pre>
"
"0.0845124072264899","0.0885887685054121"," 92838","<p>I'm currently working with the method proposed by Koenker (2004) and Lamarche(2010) on fixed effects for quantile regression, for this I'm using the RQPD code in R. I would like to get the predicted values for each firm after running the RQPD code.</p>

<p>The problem is that, after the running the code is only possible to retrieve the residuals. I know that the observed values of Y minus the residuals will give me the predicted values; but I need to run some simulations and for that I need to know how to get the predicted values ""by hand"" based on the Î²0 and Î²1 coefficients.</p>

<p>I tried to get the predicted values as in the OLS: Î²0 + Î²1 (x) + the individual effect of the corresponding firm, using the following code: </p>

<pre><code>require(rqpd) # it works only for version 2.15.3
set.seed(10)
m &lt;- 3   
n &lt;- 10   
s &lt;- as.factor(rep(1:n,rep(m,n)))   
x &lt;- exp(rnorm(n*m))           
X &lt;- cbind(1,x)
u &lt;- x*rnorm(m*n) + (1-x)*rf(m*n,3,3)
a &lt;- rep(rnorm(n),rep(m,n))      
y &lt;- a + u
fit &lt;- rqpd(y ~ x | s, panel(lambda = 1))
summary(fit)
fitcoe&lt;-dcoef(fit)

res&lt;-residuals(fit) # list of 100 residuals, I assumed the following:
res25&lt;-res[1:30] # residuals for the 0.25 quantile
res50&lt;-res[31:60] # residuals for the 0.50 quantile
res75&lt;-res[61:90] # residuals for the 0.75 quantile
FE&lt;-res[91:100] # individual effects for each firm

fitted1&lt;-y-res25 # fitted values for the 0.25 quantile
fit1.1&lt;-fitcoe[1,]+ fitcoe[2,]*x[1]+FE[1] # example for 1st observation
</code></pre>

<p>Unfortunately this is not working, for example for the first observation: when I substract the residuals from the observed values of Y, I get a predicted value of -0.8690 and when I calculate the predicted values by hand (fit1.1), I get a value of -1.2748. Do I miss something on my way to calculate the predicted values? </p>
"
"0.052777981396926","0.0335294958785986"," 93390","<p>This question is about understanding the logistic regression output using R.  Here is my sample data frame:</p>

<pre><code>    Drugpairs             AdverseEvent  Y    N
1   Rebetol + Pegintron       Nausea   29 1006
2   Rebetol + Pegintron      Anaemia   21 1014
3   Rebetol + Pegintron     Vomiting   14 1021
4   Ribavirin + Pegasys       Nausea    5  238
5   Ribavirin + Pegasys      Anaemia   12  231
6   Ribavirin + Pegasys     Vomiting    1  242
7 Ribavirin + Pegintron       Nausea   15  479
8 Ribavirin + Pegintron      Anaemia    7  487
9 Ribavirin + Pegintron     Vomiting    9  485
</code></pre>

<p>This basically describes the number of times a particular drug pair has caused a medically adverse event. (<code>Y=yes, N=no</code>). I ran a logistic regression on this dataset in R using the following commands:</p>

<pre><code>mod.form    = ""cbind(Y,N) ~ Drugpairs * AdverseEvent""
glmhepa.out = glm(mod.form, family=binomial(logit), data=hepatitis.df)
</code></pre>

<p>The summary output was as follows (only showing the coefficients table):  </p>

<pre><code>                                                      Estimate Std. Error z value
(Intercept)                                          -3.8771     0.2205 -17.586
DrugpairsRibavirin + Pegasys                          0.9196     0.3691   2.491
DrugpairsRibavirin + Pegintron                       -0.3652     0.4399  -0.830
AdverseEventNausea                                    0.3307     0.2900   1.140
AdverseEventVomiting                                 -0.4123     0.3479  -1.185
DrugpairsRibavirin + Pegasys:AdverseEventNausea      -1.2360     0.6131  -2.016
DrugpairsRibavirin + Pegintron:AdverseEventNausea     0.4480     0.5457   0.821
DrugpairsRibavirin + Pegasys:AdverseEventVomiting    -2.1191     1.1013  -1.924
DrugpairsRibavirin + Pegintron:AdverseEventVomiting   0.6678     0.6157   1.085
</code></pre>

<p>I understand that the coefficients give probabilistic odds. I am curious however, as to why there are no coefficients for the <code>AdverseEventAnaemea</code> and also why is there no coefficient for any combination of the drugs and the adverse event anaemea? (The last 4 rows are the combination effects of drugs and adverse events)</p>
"
"0.105555962793852","0.100588487635796"," 93392","<p>First of all, sorry i am new about this and any helps are really welcome.</p>

<p>I am reading a reaserch paper where the authors report: <em>Stepwise forward regression (Zar 1996) was used to select the most informative variables, which were included in a multiple (linear) regression model. A 5% significance level was chosen as a threshold for the inclusion of the model variables.</em></p>

<p>with a private email the first author told me that the variable selection was performed using stepAIC of MASS library using direction ""forward"" and they considered only for the final model the variables with a significance level of &lt; 5%.</p>

<p>using junk data i tried to rewrite the analysis in order to understand the procedure</p>

<pre><code>state.x77
st = as.data.frame(state.x77) str(st) colnames(st)[4] = ""Life.Exp""
colnames(st)[6] = ""HS.Grad"" st[,9] = st$Population * 1000 / st$Area colnames(st)[9] = ""Density""
str(st) model1 = lm(Life.Exp ~ Population + Income + Illiteracy + Murder + + HS.Grad + Frost + Area + Density, data=st)
model1.stepAIC &lt;- stepAIC(model1, direction=c(""both""))
summary(model1.stepAIC)


Call:
lm(formula = Life.Exp ~ Population + Murder + HS.Grad + Frost, 
    data = st)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.47095 -0.53464 -0.03701  0.57621  1.50683 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  7.103e+01  9.529e-01  74.542  &lt; 2e-16 ***
Population   5.014e-05  2.512e-05   1.996  0.05201 .  
Murder      -3.001e-01  3.661e-02  -8.199 1.77e-10 ***
HS.Grad      4.658e-02  1.483e-02   3.142  0.00297 ** 
Frost       -5.943e-03  2.421e-03  -2.455  0.01802 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.7197 on 45 degrees of freedom
Multiple R-squared:  0.736,     Adjusted R-squared:  0.7126 
F-statistic: 31.37 on 4 and 45 DF,  p-value: 1.696e-12
</code></pre>

<p>followint the protocol of the paper the final model is </p>

<pre><code>Life.Exp ~ Murder + HS.Grad + Frost (final model) 
</code></pre>

<p>because Population is > 0.05.</p>

<p>I wish to know if this final model approach is correct, and then:</p>

<pre><code>fmodel = lm(Life.Exp ~ Murder + HS.Grad + Frost, data=st)
summary(fmodel)

Call:
lm(formula = Life.Exp ~ Murder + HS.Grad + Frost, data = st)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.5015 -0.5391  0.1014  0.5921  1.2268 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 71.036379   0.983262  72.246  &lt; 2e-16 ***
Murder      -0.283065   0.036731  -7.706 8.04e-10 ***
HS.Grad      0.049949   0.015201   3.286  0.00195 ** 
Frost       -0.006912   0.002447  -2.824  0.00699 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.7427 on 46 degrees of freedom
Multiple R-squared:  0.7127,    Adjusted R-squared:  0.6939 
F-statistic: 38.03 on 3 and 46 DF,  p-value: 1.634e-12
</code></pre>
"
"0.0914141453004008","0.0774330538852055"," 93417","<p>This question is a prolongation of this question: <a href=""http://stats.stackexchange.com/questions/11096/how-to-interpret-coefficients-in-a-poisson-regression"">How to interpret coefficients in a Poisson regression?</a></p>

<p>If we follow the (almost) exact same routine, but we add correlation between the variablese treatment and improved (just for the sake of my question, which is interpreting the output), we get:</p>

<pre><code>treatment     &lt;- factor(rep(c(1, 2), c(43, 41)), 
                        levels = c(1, 2),
                        labels = c(""placebo"", ""treated""))
improved      &lt;- factor(rep(c(1, 2, 3, 1, 2, 3), c(29, 7, 7, 13, 7, 21)),
                        levels = c(1, 2, 3),
                        labels = c(""none"", ""some"", ""marked""))    
numberofdrugs &lt;- rpois(84, 10) + 1    
healthvalue   &lt;- rpois(84, 5)   
y             &lt;- data.frame(healthvalue, numberofdrugs, treatment, improved)
test          &lt;- glm(healthvalue~numberofdrugs+treatment+improved + treatment:improved, y, family=poisson)
summary(test)
</code></pre>

<p>Note the $\textbf{ treatment:improved}$ term I added inside the glm function. </p>

<p>Now, we get the following output:</p>

<pre><code>    Call:
glm(formula = healthvalue ~ numberofdrugs + treatment + improved + 
    treatment:improved, family = poisson, data = y)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.9261  -0.8733  -0.0296   0.5473   2.3358  

Coefficients:
                                 Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                      1.553051   0.184229   8.430   &lt;2e-16 ***
numberofdrugs                    0.004298   0.014242   0.302   0.7628    
treatmenttreated                 0.007399   0.149440   0.050   0.9605    
improvedsome                     0.358897   0.164891   2.177   0.0295 *  
improvedmarked                  -0.178360   0.203756  -0.875   0.3814    
treatmenttreated:improvedsome   -0.330336   0.265310  -1.245   0.2131    
treatmenttreated:improvedmarked  0.050617   0.260203   0.195   0.8458    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 97.805  on 83  degrees of freedom
Residual deviance: 89.276  on 77  degrees of freedom
AIC: 383.29

Number of Fisher Scoring iterations: 5
</code></pre>

<p>If we ignore what seems to be insignificant coefficients, I can ask my question:</p>

<p>I understand that, as in the original post, treatment=placebo and improved=none is the base level for those variables, and thus are set to zero. My question is, why does it not exist any interaction terms with the base lavels for treatment=placebo and improved=none?</p>

<p>I thought setting the base levels to zero was just a construct, and in my mind there should still exist correlation between them...(?)</p>
"
"0.0621994475718397","0.0711268017165705"," 93423","<p>please help a sociologist struggling to get to grips with R and statistics in general..!</p>

<p>I've got a data set with 11 variables. I need to investigate the possible relationships between one of the variables (percentage of smokers in a population) and the rest of them- things like unemployment rate, education level, and so on. I'm working purely with linear regression.</p>

<p>I'm getting there with correlation, but having looked around at numerous examples on the web, I'm still unsure with regards to method. Apologies if it's completely obvious, but I can't seem to find an answer anywhere.</p>

<p>As I see it, it seems sensible to look for correlation between smoking and all the other variables firstly with scatterplots, and then by calculating the correlation coefficient if there is an identifiable linear relationship (Pearson's r or Spearman, depending on normality). If we then continue and do separate simple linear regressions between smoking rate and our identified variable, that all seems well and good.</p>

<p>But where does multiple regression fit in here? Does it make any sense, statistically speaking, to do simple linear regression first and then multiple linear regression also? Or would it be best to just jump straight from testing correlation to multiple regression?</p>

<p>Any help much appreciated!</p>
"
"0.101414888671788","0.0885887685054121"," 93454","<p><strong>Base Data</strong>: I have ~1,000 people marked with assessments: '1,' [good] '2,' [middle] or '3' [bad] -- these are the values I'm trying to predict for people in the future. In addition to that, I have some demographic information: gender (categorical: M / F), age (numerical: 17-80), and race (categorical: black / caucasian / latino).</p>

<p><strong>I mainly have four questions:</strong></p>

<ol>
<li><p>I was initially trying to run the dataset described above as a multiple regression analysis. But I recently learned that since my dependent is an ordered factor and not a continuous variable, I should use ordinal logistic regression for something like this. I was initially using something like <code>mod &lt;- lm(assessment ~ age + gender + race, data = dataset)</code>, can anybody point me in the right direction?</p></li>
<li><p>From there, assuming I get coefficients I feel comfortable with, I understand how to plug solely numerical values in for x1, x2, etc. -- but how would I deal with race, for example, where there are multiple responses: black / caucasian / latino? So if it tells me the caucasian coefficient is 0.289 and somebody I'm trying to predict is caucasian, how do I plug that back in since the value's not numerical?</p></li>
<li><p>I also have random values that are missing -- some for race, some for gender, etc. Do I have to do anything additional to make sure this isn't skewing anything? (I noticed when my dataset gets loaded into R-Studio, when the missing data gets loaded as <code>NA</code>, R says something like <code>(162 observations deleted due to missingness)</code> -- but if they get loaded as blanks, it does nothing.)</p></li>
<li><p>Assuming all of this works out and I have new data with gender, age, and race that I want to predict on -- is there an easier way in R to run all of that through whatever my formula with new coefficients turns out to be, rather than doing it manually? (If this question isn't appropriate here, I can take it back to the R forum.)</p></li>
</ol>
"
"0.0914141453004008","0.0871121856208561"," 93594","<p>I ran a regression in which I have an interaction term of two factor variables.</p>

<p>The one of the variables has 2 levels: Condition_A &amp; Condition_B (therefore the first one is the reference category) but the other one is called Group.Membership has 3 levels: Group_1, Group_2 &amp; Group_3 (therefore the first one is the reference category).</p>

<p>What I am not sure about is how to interpret the output.
What I get as part of the lm() ouput is : </p>

<pre><code>    ConditionB:Group.Membership.L         (Beta and p-value)
    ConditionB:Group.Membership.Q         (Beta and p-value)
</code></pre>

<p>Should I interpret the .L as the coefficient for Group_2 compared to the reference category (Group_1), and the same goes for .Q?
Or there is another interpretation of this type of output?</p>

<p>Thank you for your help!</p>

<p>EDIT ----------------</p>

<p>I have another question. I have tested the interaction term of the Condition variable with many other factor variables. Why sometimes I get in the ouput the .L and .Q in the end, whereas in other occasions I get the levels of the second variable of the interaction term without the .L and Q.?
Am I doing something wrong in my lm() ?</p>
"
"0.109866129394437","0.096642292914995"," 94581","<p>I have a ordinal dependendent variable, easiness, that ranges from 1 (not easy) to 5 (very easy).  Increases in the values of the independent factors are associated with an increased easiness rating.</p>

<p>Two of my independent variables (<code>condA</code> and <code>condB</code>) are categorical, each with 2 levels, and 2 (<code>abilityA</code>, <code>abilityB</code>) are continuous.</p>

<p>I'm using the <a href=""http://cran.r-project.org/web/packages/ordinal/index.html"">ordinal</a> package in R, where it uses what I believe to be</p>

<p>$$\text{logit}(p(Y \leqslant g)) = \ln \frac{p(Y \leqslant g)}{p(Y &gt; g)} = \beta_{0_g} - (\beta_{1} X_{1} + \dots + \beta_{p} X_{p}) \quad(g = 1, \ldots, k-1)$$<br>
(from @caracal's answer <a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">here</a>)</p>

<p>I've been learning this independently and would appreciate any help possible as I'm still struggling with it.  In addition to the tutorials accompanying the ordinal package, I've also found the following to be helpful: </p>

<ul>
<li><a href=""http://stats.stackexchange.com/questions/89474/interpretation-of-ordinal-logistic-regression"">Interpretation of ordinal logistic regression</a></li>
<li><a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">Negative coefficient in ordered logistic regression</a></li>
</ul>

<p>But I'm trying to interpret the results, and put the different resources together and am getting stuck. </p>

<ol>
<li><p>I've read many different explanations, both abstract and applied, but am still having a hard time wrapping my mind around what it means to say: </p>

<blockquote>
  <p>With a 1 unit increase in condB (i.e., changing from one level to the next of the categorical predictor), the predicted odds of observing Y = 5 versus Y = 1 to 4 (as well as the predicted odds of observed Y = 4 versus Y = 1 to 3) change by a factor of exp(beta) which, for diagram, is exp(0.457) = 1.58. </p>
</blockquote>

<p>a. Is this different for the categorical vs. continuous independent variables?<br>
b. Part of my difficulty may be with the cumulative odds idea and those comparisons. ... Is it fair to say that going from condA = absent (reference level) to condA = present is 1.58 times more likely to be rated at a higher level of easiness?  I'm pretty sure that is NOT correct, but I'm not sure how to correctly state it.</p></li>
</ol>

<p>Graphically,<br>
1. Implementing the code in <a href=""http://stats.stackexchange.com/questions/89474/interpretation-of-ordinal-logistic-regression"">this post</a>, I'm confused as to why the resulting 'probability' values are so large.<br>
2. The graph of p (Y = g) in <a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">this post</a> makes the most sense to me ... with an interpretation of the probability of observing a particular category of Y at a particular value of X.  The reason I am trying to get the graph in the first place is to get a better understanding of the results overall.</p>

<p>Here's the output from my model:</p>

<pre><code>m1c2 &lt;- clmm (easiness ~ condA + condB + abilityA + abilityB + (1|content) + (1|ID), 
              data = d, na.action = na.omit)
summary(m1c2)
Cumulative Link Mixed Model fitted with the Laplace approximation

formula: 
easiness ~ illus2 + dx2 + abilEM_obli + valueEM_obli + (1 | content) +  (1 | ID)
data:    d

link  threshold nobs logLik  AIC    niter     max.grad
logit flexible  366  -468.44 956.88 729(3615) 4.36e-04
cond.H 
4.5e+01

Random effects:
 Groups  Name        Variance Std.Dev.
 ID      (Intercept) 2.90     1.70    
 content  (Intercept) 0.24     0.49    
Number of groups:  ID 92,  content 4 

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)    
condA              0.681      0.213    3.20   0.0014 ** 
condB              0.457      0.211    2.17   0.0303 *  
abilityA           1.148      0.255    4.51  6.5e-06 ***
abilityB           0.577      0.247    2.34   0.0195 *  

Threshold coefficients:
    Estimate Std. Error z value
1|2   -3.500      0.438   -7.99
2|3   -1.545      0.378   -4.08
3|4    0.193      0.366    0.53
4|5    2.121      0.385    5.50
</code></pre>
"
"0.0746393370862076","0.0711268017165705"," 95709","<p>I am fitting a regression with ARMA errors using the base R function <code>arima()</code> and the <code>Arima()</code> function from the <code>forecast</code> package.  </p>

<p>The estimated coefficients from both are identical. My problem comes from using <code>arima.errors()</code> on these two models, and using <code>tsdisplay()</code> to view these structural residuals (that is, the residuals straight from the regression, before any ARMA model is fit on them). These ARMA errors (and their corresponding ACFs, PACFs) are different between the two, and I don't know why. Even more curious is that the final residuals from both are in fact the same, which would make me think the structural residuals would have to be the same. I have put a MWE below.</p>

<pre><code>library('forecast')
data(usconsumption, package='fpp')

fit1 = arima(usconsumption[ ,1], xreg=usconsumption[ ,2], order=c(2,0,0))
tsdisplay(arima.errors(fit1), main=""ARIMA errors, arima function"") # not the same as the other


fit2 = Arima(usconsumption[,1], xreg=usconsumption[,2], order=c(2,0,0))
dev.new()
tsdisplay(arima.errors(fit2), main=""ARIMA errors, Arima function"") # not the same as the other


View(cbind(resid(fit1), resid(fit2))) # final residuals are the same
</code></pre>

<p>Note this example is from <a href=""https://www.otexts.org/fpp/9/1"" rel=""nofollow"">https://www.otexts.org/fpp/9/1</a></p>
"
"0.0304713817668003","0.029037395206952"," 95795","<p>from what I have studied in the data mining course (please correct me if I'm wrong) - in logistic regression, when the response variable is binary, then from the ROC curve we can determine the threshold.</p>

<p>Now I'm trying to apply the logistic regression for an ordinal categorical response variable with  more than two categories (4).
I used the function <code>polr</code> in r:</p>

<pre><code>&gt; polr1&lt;-polr(Category~Division+ST.Density,data=Versions.data)
&gt; summary(polr1)

Re-fitting to get Hessian

Call:
polr(formula = Category ~ Division + ST.Density, data = Versions.data)

Coefficients:
               Value Std. Error t value
DivisionAP   -0.8237     0.5195  -1.586
DivisionAT   -0.8989     0.5060  -1.776
DivisionBC   -1.5395     0.5712  -2.695
DivisionCA   -1.8102     0.5240  -3.455
DivisionEM   -0.5580     0.4607  -1.211
DivisionNA   -1.7568     0.4704  -3.734
ST.Density    0.3444     0.0750   4.592

Intercepts:
    Value   Std. Error t value
1|2 -1.3581  0.4387    -3.0957
2|3 -0.5624  0.4328    -1.2994
3|4  1.2661  0.4390     2.8839

Residual Deviance: 707.8457 
AIC: 727.8457  
</code></pre>

<p>How should I interpret the Intercepts?
and how can I determine the threshold for each group?</p>

<p>Thanks</p>
"
"0.052777981396926","0.0502942438178979"," 95832","<p>I have two data sets </p>

<ol>
<li>Train data  </li>
<li>Test data (with no dependent variable values but I
have data on independent variable or you can say I need to
forecast).</li>
</ol>

<p>Using the training data (which has some <code>NA</code>s in the cell) I performed ordinary least square regression (OLS) using <code>lm()</code> in R and fitted the model &amp; I got the $\beta $ coefficients of the regression model. (All is good so far!)</p>

<p>Now, in the process of prediction for the fitted values, I have some missing values for some cells in the test dataset. I used function <code>predict()</code> as follows: </p>

<pre><code> predict(ols, test_data.df, interval= ""prediction"", na.action=na.pass)
</code></pre>

<p>for the cell (or cells) with <code>NA</code> value the entire row is discarded in generating the output (<code>yhat</code>). Is there any function that could generate the <code>yhat</code> values (other than <code>NA</code>s) for the test data without discarding any rows with missing value in the cell. </p>
"
"0.075412822378","0.0821301562353182"," 95891","<p>I'm running a logistic regression model where anecdotally I expected age to be a very large factor. If you see from the charts I made in Excel before running the model through R, this is how the support lines up by age:</p>

<p><img src=""http://i.stack.imgur.com/oEVZQ.jpg"" alt=""enter image description here""></p>

<p>Looks pretty significant.</p>

<p>Though when I run the model, as you can see below, age is the <em>only</em> thing that's not significant -- which was very surprising:</p>

<pre><code>&gt; attach(mydata) 
&gt; 
&gt; # Define variables 
&gt; 
&gt; Y &lt;- cbind(support)
&gt; X &lt;- cbind(sex, region, age, supportscore1, supportscore2, county)
&gt;
&gt; # Logit model coefficients 
&gt; 
&gt; logit &lt;- glm(Y ~ X, family=binomial (link = ""logit""), na.action = na.exclude) 
&gt; 
&gt; summary(logit) 

Call:
glm(formula = Y ~ X, family = binomial(link = ""logit""), na.action = na.exclude)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.1019  -0.7609   0.5231   0.7101   2.3965  

Coefficients:
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)            4.013446   0.440962   9.102  &lt; 2e-16 ***
Xsex                  -0.229256   0.104859  -2.186 0.028792 *  
Xregion               -1.103308   0.091497 -12.058  &lt; 2e-16 ***
Xage                   0.004569   0.003209   1.424 0.154512    
Xsupportscore1        -0.019262   0.005732  -3.360 0.000778 ***
Xsupportscore2         0.019810   0.005264   3.764 0.000168 ***
Xcounty               -0.047581   0.011161  -4.263 2.02e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2871.5  on 2072  degrees of freedom
Residual deviance: 2245.5  on 2066  degrees of freedom
  (66 observations deleted due to missingness)
AIC: 2259.5

Number of Fisher Scoring iterations: 4
</code></pre>

<p>My only guess on this is that the previous support scores (both 0-100 numerical values) I'm using may have already taken age into account, and the model doesn't want to count it twice. Though, to compare, region and county are just two different ways of cutting up the geography -- and those both seem significant.</p>

<p>Can somebody let me know what you would think if your model told you that age wasn't significant when in clearly is? Trying to figure out if there's a way of thinking about it that I'm missing or if something in my code is wrong.</p>

<p>Thanks!</p>

<p>--
<strong>EDIT</strong></p>

<p>Pairs plot added to show correlation (despite some factors being categorical):</p>

<pre><code>pairs(~sex + region +  age + supportscore1 + supportscore2 + county, data=mydata)
</code></pre>

<p><img src=""http://i.stack.imgur.com/N2IG4.jpg"" alt=""enter image description here""></p>
"
"0.0963589698356145","0.0918243061724248"," 95974","<p>This is a follow-up question from this post, here:
<a href=""http://stackoverflow.com/questions/14423325/confidence-intervals-for-predictions-from-logistic-regression"">Confidence intervals for predictions from logistic regression</a></p>

<p>The answer from @Gavin is excellent, but I have some additional questions which I think would be useful for others. I am working with a Poisson model, so basically it is the same approach described in the other post, only <code>family=poisson</code> instead of <code>family=binomial</code>.</p>

<p>To my first question:
@Gavin writes:</p>

<pre><code>mod &lt;- glm(y ~ x, data = foo, family = binomial)
preddat &lt;- with(foo, data.frame(x = seq(min(x), max(x), length = 100))
preds &lt;- predict(mod, newdata = preddata, type = ""link"", se.fit = TRUE)
</code></pre>

<p>What is the point of the second line there? Is it necessary to create a data.frame with minimum and maximum of the explanatory variable? Could I not, for some explanatory variable(s) <code>x</code> (stored in some data frame <code>data</code>), just go from the first line and directly to the third?</p>

<p>To my second question:
In the beginning of his answer @Gavin writes:</p>

<blockquote>
  <p>The usual way is to compute a confidence interval on the scale of the
  linear predictor, where things will be more normal (Gaussian) and then
  apply the inverse of the link function to map the confidence interval
  from the linear predictor scale to the response scale.</p>
</blockquote>

<p>Why are ""things"" more normal on the scale of the linear predictor(s)? Is this also the case when I do my Poisson regression?
I assume the reason for using critical value 1.96 when constructing the CI's, is because of the assumptions that ""things"" are normal. Can somebody explain this further?</p>

<p>My third question:</p>

<p>Is there a relationship between the standard deviation which we get by using <code>se.fit=TRUE</code>  in predict() and the standard deviations of the coefficients of the explanatory variables, which we simply get from <code>summary(mod)</code>? (<code>mod</code> is some glm object)</p>
"
"0.13342816860689","0.13925845528839"," 95994","<p>I`d like to extract the parameters of a two-component mixture distribution of noncentral student t distributions which first has to be fitted to a one-dimensional sample.</p>

<p>My question is closely related to this thread, but as pointed out I want to use Student t components for the mixture:
<a href=""http://stats.stackexchange.com/questions/10062/which-r-package-to-use-to-calculate-component-parameters-for-a-mixture-model?newreg=fe1454a4702e4532a03bd2c705fe3b02"">Which R package to use to calculate component parameters for a mixture model</a></p>

<p>There are many packages for R that are capable of handling mixture distributions in one way or another. Some in the context of a Bayesian framework requiring kernels. Some in a regression framework. Some in a nonparametric framework. ...</p>

<p>In general the ""mixdist""-package seems to come closest to my wish. This package fits parametric mixture distributions to a sample of data. Unfortunately it doesn`t support the student t distribution.</p>

<p>I have also tried to manually set up a likelihood function as described here:
<a href=""http://stackoverflow.com/questions/6485597/r-how-to-fit-a-large-dataset-with-a-combination-of-distributions"">http://stackoverflow.com/questions/6485597/r-how-to-fit-a-large-dataset-with-a-combination-of-distributions</a>
But my result is far from perfect.</p>

<p>The ""gamlss.mx""-package might be helping, but originally it seems to be set up for another context, i.e. regression. I tried to regress my data on a constant and then extract the parameters for the estimated mixture error distribution. Is this a valid approach? </p>

<p>But with this approach the estimated parameters seem to be not directly accessable individually by some command (such as fit1$sigma). And more importantly there seem to be serious estimation problems even in pretty simple and nonambiguous cases.
E.g. in example 2 (see syntax below) I simulated a mixture which looks like this:</p>

<p><img src=""http://i.stack.imgur.com/MG7AA.jpg"" alt=""kernel density estimate of the mixture""></p>

<p>When trying to fit a two-component student t mixture to these data either I get this error message (the deeper meaning of which I don't understand):</p>

<p><img src=""http://i.stack.imgur.com/UPvg4.jpg"" alt=""enter image description here""></p>

<p>or I get wrong results (convergenve is reached only after approximately two hours as can be seen from the output):</p>

<p><img src=""http://i.stack.imgur.com/HjlfW.jpg"" alt=""enter image description here""></p>

<p>The means could be estimated well, but both the variance and the degrees of freedom are estimated badly. In the TF2 implementation of the student t, the sigma parameter denotes the standard deviation. Its estimate is NEGATIVE for the first component! And for the second component the degrees of freedom estimate is also NEGATIVE. Probably one should not use these results in practice :(</p>

<p>By the way: Is there a way to restrict these degree-of-freedom coefficient estimates to be natural numbers? </p>

<p>The following syntax is my gamlss.mx-setup so far:</p>

<pre><code>library(gamlss.dist)
library(gamlss.mx)
library(MASS)

# example 1 (real data):
data(geyser)
plot(density(geyser$waiting) )
fit1 &lt;- gamlssMX( waiting~1,data=geyser,family=""TF2"",K=2 )
fit1
# works fine

# example 2 (simulated data):
N &lt;- 100000
components &lt;- sample(1:2,prob=c(0.6,0.4),size=N,replace=TRUE)
mus &lt;- c(3,-6)    # denotes the mean of component 1 and 2, respectively
sds &lt;- c(1,9)     # ... the standard deviations
nus &lt;- c(25,3)    # ... the degrees of freedom
mixsim &lt;-data.frame(rTF2( N,mu=mus[components],sigma=sds[components],nu=nus[components] ))
colnames(mixsim) &lt;- ""MCsim""
plot(density(mixsim$MCsim) , xlim=c(-50,50))
fit2 &lt;- gamlssMX(MCsim~1,data=mixsim,family=""TF2"",K=2)
fit2
# error message or strange results (this also happens when using a sample of S&amp;P500 returns)
</code></pre>

<p>I would be very grateful for any advice!
I've read through many related manuals and vignettes so far but I`m still lost.</p>

<p>Thanks a lot in advance!!
Jo</p>
"
"0.185350179257505","0.167080142810292"," 96010","<p>I am struggling to fit alternative count models into my data. I guess my problem is just too many zeros.</p>

<p>This is my data</p>

<pre><code>&gt; summary(smpl)
    response        predict1          predict2        
 Min.   :0.000   Min.   :   1.00   Min.   :    22005  
 1st Qu.:0.000   1st Qu.:   3.00   1st Qu.:  4669705  
 Median :0.000   Median :   8.00   Median : 12540318  
 Mean   :0.017   Mean   :  23.27   Mean   : 20382574  
 3rd Qu.:0.000   3rd Qu.:  20.00   3rd Qu.: 25468156  
 Max.   :3.000   Max.   :1584.00   Max.   :145348049

&gt; table(smpl$response)
  0   1   2   3 
987  10   2   1 
</code></pre>

<p>I tried three regressions: basic Poisson, negative binomial and zero-inflated but the only formula returning coefficients without warnings is the Poisson:</p>

<pre><code>&gt; summary(glm(response ~ ., data = smpl, family = poisson))

Call:
glm(formula = response ~ ., family = poisson, data = smpl)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.3871  -0.2214  -0.1722  -0.1148   4.7861  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -3.472e+00  3.521e-01  -9.862  &lt; 2e-16 ***
predict1     3.229e-03  7.271e-04   4.442 8.93e-06 ***
predict2    -6.258e-08  3.060e-08  -2.045   0.0409 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 150.67  on 999  degrees of freedom
Residual deviance: 135.84  on 997  degrees of freedom
AIC: 170.06

Number of Fisher Scoring iterations: 8
</code></pre>

<p>The negative binomial returns a warnings on both the convergence and the alternation limit</p>

<pre><code>summary(glm.nb(response ~ ., data = smpl))

Call:
glm.nb(formula = response ~ ., data = smpl, init.theta = 0.04901296596, 
    link = log)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.28844  -0.17677  -0.14542  -0.09808   2.38314  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -3.899e+00  4.587e-01  -8.499  &lt; 2e-16 ***
predict1     1.226e-02  2.144e-03   5.720 1.06e-08 ***
predict2    -5.982e-08  3.407e-08  -1.756   0.0791 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for Negative Binomial(0.049) family taken to be 1)

    Null deviance: 69.927  on 999  degrees of freedom
Residual deviance: 55.940  on 997  degrees of freedom
AIC: 152.37

Number of Fisher Scoring iterations: 1


              Theta:  0.0490 
          Std. Err.:  0.0251 
Warning while fitting theta: alternation limit reached 

 2 x log-likelihood:  -144.3700 
Warning messages:
1: glm.fit: algorithm did not converge 
2: In glm.nb(response ~ ., data = smpl) : alternation limit reached
</code></pre>

<p>and the zero-inflated (from the <code>pscl</code> package) doesn't return anything at all</p>

<pre><code>&gt; summary(zeroinfl(response ~ ., data = smpl, dist = ""negbin""))

Call:
zeroinfl(formula = response ~ ., data = smpl, dist = ""negbin"")

Pearson residuals:
     Min       1Q   Median       3Q      Max 
-0.45252 -0.08817 -0.05515 -0.04210 19.56118 

Count model coefficients (negbin with log link):
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -1.477e+00         NA      NA       NA
predict1     2.678e-03         NA      NA       NA
predict2    -1.160e-07         NA      NA       NA
Log(theta)  -1.241e+00         NA      NA       NA

Zero-inflation model coefficients (binomial with logit link):
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  4.869e+00         NA      NA       NA
predict1    -1.329e-01         NA      NA       NA
predict2    -1.346e-07         NA      NA       NA
Error in if (getOption(""show.signif.stars"") &amp; any(rbind(x$coefficients$count,  : 
  missing value where TRUE/FALSE needed
</code></pre>

<p>Then my questions are: </p>

<ol>
<li>Is there anything I can do in terms of ""formula tweaking"" with the negative binomial (to avoid the warnings) and with the zero-inflated (to get the coefficients)?</li>
<li>Looking only at the results above (thus including problems with convergence and alternation limit) should I select the negative binomial model since it seems, looking at the AIC, to fit better than the Poisson in my data?</li>
</ol>
"
"0.075412822378","0.0821301562353182"," 96236","<p>I am following an example <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">here</a> on using Logistic Regression in R. However, I need some help interpreting the results. They do go over some of the interpretations in the above link, but I need more help with understanding a goodness of fit for Logistic Regression and the output that I am given.</p>

<p>For convenience, here is the summary given in the example:</p>

<pre><code>## Call:
## glm(formula = admit ~ gre + gpa + rank, family = ""binomial"", 
##     data = mydata)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.627  -0.866  -0.639   1.149   2.079  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -3.98998    1.13995   -3.50  0.00047 ***
## gre          0.00226    0.00109    2.07  0.03847 *  
## gpa          0.80404    0.33182    2.42  0.01539 *  
## rank2       -0.67544    0.31649   -2.13  0.03283 *  
## rank3       -1.34020    0.34531   -3.88  0.00010 ***
## rank4       -1.55146    0.41783   -3.71  0.00020 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 499.98  on 399  degrees of freedom
## Residual deviance: 458.52  on 394  degrees of freedom
## AIC: 470.5
## 
## Number of Fisher Scoring iterations: 4
</code></pre>

<ol>
<li>How well did Logistic Regression fit here?</li>
<li>What exactly are the Deviance Residuals? I believe they are the average residuals per quartile. How do I determine if they are bad/good/statistically significant?</li>
<li>What exactly is the <code>z-value</code> here? Is it the normalized standard deviation from the mean of the Estimate assuming a mean of 0? </li>
<li>What exactly are Signif. codes?</li>
</ol>

<p>Any help is greatly appreciated! You do not have to answer them all!</p>
"
"0.0609427635336005","0.0580747904139041"," 96863","<p>I was reading <a href=""http://anythingbutrbitrary.blogspot.com/2012/04/lm-function-with-categorical-predictors.html"" rel=""nofollow"">""The lm() function with categorical predictors""</a>, and am confused.</p>

<ol>
<li><p>What does the regression model with a categorical predictor look
like, with the following R code:</p>

<pre><code>n = 30
sigma = 2.0

AOV.df &lt;- data.frame(category = c(rep(""category1"", n)
                                , rep(""category2"", n)
                                , rep(""category3"", n)), 
                            j = c(1:n
                                , 1:n
                                , 1:n),
                            y = c(8.0  + sigma*rnorm(n)
                                , 9.5  + sigma*rnorm(n)
                                , 11.0 + sigma*rnorm(n))
                  )

AOV.lm &lt;- lm(y ~ category, data = AOV.df)
summary(AOV.lm)
</code></pre>

<p>The output is:</p>

<pre><code>Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)         8.4514     0.3405  24.823  &amp;lt; 2e-16 ***
categorycategory2   0.8343     0.4815   1.733   0.0867 .  
categorycategory3   3.0017     0.4815   6.234 1.58e-08 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 1.865 on 87 degrees of freedom
Multiple R-squared: 0.3225, Adjusted R-squared: 0.307 
F-statistic: 20.71 on 2 and 87 DF,  p-value: 4.403e-08 
</code></pre>

<p>Is the model:</p>

<p><em>y = 8.4514 + 0.8343 for category 1</em>,</p>

<p><em>y = 8.4514 + 0.8343 for category 2</em> or</p>

<p><em>y = 8.4514 + 3.0017 for cateogry 3</em>?</p></li>
<li><p>What is the model if the R code looks like:</p>

<pre><code>&gt; X &lt;- read.table(""http://www.stat.umn.edu/geyer/5102/data/ex5-4.txt"", header=T) 
&gt; lm(y ~ color + x * color, data=X)

Call:
lm(formula = y ~ color + x * color, data = X)

Coefficients:
 (Intercept)    colorgreen      colorred             x  colorgreen:x  
    13.96118       0.25243       6.10543       0.97241       0.07347  
  colorred:x  
     0.01962
</code></pre></li>
</ol>

<p>Thanks!</p>
"
"0.068136080998913","0.0649295895722714"," 96999","<p>I'm working on a model that requires me to look for predictors for a rare event (less than 0.5% of the total of my observations). My total sample is a significant part of the total population (50,000 cases). My final objective is to obtain comparable probability values for all the non-events, without the bias of the groups difference in the logistic regression. </p>

<p>I've been reading the info in the following link: </p>

<p><a href=""http://gking.harvard.edu/files/gking/files/0s.pdf"" rel=""nofollow"">http://gking.harvard.edu/files/gking/files/0s.pdf</a></p>

<p>It advises me first to use a sample of my original sample, containing all the events (1) and a random sample of 1-5 times bigger of the non-event (0) sample.</p>

<p>Then it suggests using weights based on the proportion of the sample 1s to 0s. In the section 4.2 of the linked text, he offers a ""easy to implement"" weighted log-likelihood that can be implemented in any logit function.</p>

<p>I wish to implement these weights somehow with R's glm(...,family=binomial(link=""logit"")) or similar function ( the ""weights"" parameter is not for frequency weighting), but I don't really know how to apply this weighting.</p>

<p>Does anybody knows how to make it or any other alternative suggestion?  </p>

<p><strong>Edit1:</strong> As suggested bellow, is Firth's method for bias-correction by penalizing the likelihood in the <code>logistf</code> package a correct approach in this case? I'm not much knowledgeable in statistics, and, while I understand the input and the coefficients/output of the logistic model, what happens in between is still quite a mystery to me, sorry.</p>
"
"0.0430930413588572","0.0410650781176591"," 97250","<p>How does the stepwise regression method work for <code>both</code> direction in R with the <code>step()</code> function.</p>

<p>I would think that one variable will be placed into the model and then another that will improve the measuring criteria and the significance of the older variable gets assessed. If the older variable's coefficient is not significant the variable will be removed and a next variable will be placed into the model and so forth.</p>

<p>I am not a 100% sure if this is how the step function with <code>both</code> do it, but can someone please inform me if this is correct, if not how does the <code>both</code> direction criteria implement the stepwise regression in R with <code>step()</code>. </p>
"
"0.0609427635336005","0.0580747904139041"," 97347","<p>How can I improve the accuracy of my logistic regression code, which tests the accuracy using the 10-fold cross-validation technique? I have implemented this code using <code>glmfit</code> and <code>glmval</code>. The desired accuracy is somewhat higher and it requires the parameters to be found using maximum likelihood estimator. Also, when I run this code in MATLAB, I get the following error</p>

<blockquote>
  <p>Warning: X is ill conditioned, or the model is overparameterized, and some coefficients are not identifiable. You should use caution in making predictions. In glmfit at 245 In LR at 8</p>
</blockquote>

<p>The code is:</p>

<pre><code>function LR( X,y)
y(y==-1)=0;
X=[ones(size(X,1),1) X];
disp(size(X,2));
indices = crossvalind('Kfold',y,10);
for i = 1:10
    test = (indices == i); train = ~test;
    b = glmfit(X(train,:),y(train),'binomial','logit');
    y_hat= glmval(b,X(test,:),'logit');
    y_true=y(test,:);
    error(i)=mean(abs(y_true-y_hat));
end
accuracy=(1-error)*100;
fprintf('accuracy= %f +- %f\n',mean(accuracy),std(accuracy));
end
</code></pre>
"
"NaN","NaN"," 97385","<p>I work with a regression with ARMA errors and I want to use LASSO to shrink the coefficients and select my variables. This topic is discussed in the article <a href=""http://www.sciencedirect.com/science/article/pii/S0378375812000948"" rel=""nofollow"">Wu et al. (2012)</a>.
So, the problem I have to minimize is:</p>

<p>$$
L(\xi)=||Y-X\xi||^2+\lambda_1\sum_{i=1}^{d}\frac{|\beta_i|}{|\beta_{i}^{OLS}|}+\lambda_2\sum_{i=1}^{p}\frac{|\phi_i|}{|\phi_{i}^{OLS}|}+\lambda_3\sum_{i=1}^{q}\frac{|\theta_i|}{|\theta_{i}^{OLS}|}
$$
where: $\xi=[\beta,\phi,\theta]$ and $\xi^{OLS}=[\beta^{OLS},\phi^{OLS},\theta^{OLS}]$ is the least squares estimate.</p>

<p>Do you know if there is a function in R that can solve this minimization? Something similar with glmnet / l1ce / lars, but with multiple constraints on the parameters?</p>

<p>Thank you!</p>
"
"0.0746393370862076","0.0711268017165705","100453","<p>I am running a glm in R on data with quite many predictors (~50), both initially continuous and factors. The response is binary and the volume of the data is OK (~100K rows), in order to model non-linear relationships, I convert the continuous variables to factors as well. In the end this results in some levels of some variables being insignificant. For example, variable ""age"" is binned into 20-30, 40-50, 50-60, 60+ and only the first two are significant.</p>

<p>All this results in a reasonable model with AUC 0.86 and residuals looking random with a small bias. </p>

<p>I understand that R converts each factor level to a binary variable and then runs the regression, and I'd still like to improve the model. Would you please help me understand if: </p>

<ul>
<li>converting all the variables' factor levels to binary myself,</li>
<li>selecting only the variables that were significant in the initial model</li>
<li>then re-fitting the model with the new (reduced) variable set</li>
</ul>

<p>sound like a good idea?</p>

<p>Or should I continue to use predict() on the model with many insignificant coefficients for factor levels happily as before?</p>
"
"0.0430930413588572","0.0410650781176591","100670","<p>What is the purpose of the ANOVA table? I once learned that you can only interpret the significance (p-value) of a multi-level discrete variable, or an interaction effect using the ANOVA table. Why? Why can't you use the p-value outputs of the regression? Why do people look at the ANOVA table in practice? </p>

<p>GLM</p>

<pre><code>Coefficients:
                                    Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)                          -1.9800     1.3697  -1.446    0.148
ConnectivityHIGH                      1.9214     1.6361   1.174    0.240
SusceptibilityHIGH                    0.8636     1.7183   0.503    0.615
ConnectivityHIGH:SusceptibilityHIGH  -0.6555     2.1348  -0.307    0.759
</code></pre>

<p>ANOVA</p>

<pre><code>                            Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)
NULL                                           19     3.6712         
Connectivity                 1  2.43379        18     1.2374   0.1187
Susceptibility               1  0.19710        17     1.0403   0.6571
Connectivity:Susceptibility  1  0.09598        16     0.9443   0.7567
</code></pre>
"
"0.0304713817668003","0.029037395206952","101046","<p>I am doing a quantile regression in R with some data and then i want to test the accuracy of the coefficients on a another data set (hold out set). But i am not sure how to go about measuring the accuracy on the hold out set in R.</p>

<p>Is there any ideas?</p>
"
"0.158179547328907","0.141314625492221","101187","<p>I know that this has been discussed before, but those discussions did not really answer my questions. I know how the ADF test works, but I am having trouble interpreting the output for the three options using the <code>ur.df</code> function in R (package: <code>urca</code>). Could someone walk me through the interpretations?
More specifically, what are <code>tau1</code>, <code>tau2</code>, <code>phi1</code>, <code>phi2</code>, and <code>phi3</code>? </p>

<pre><code>summary(ur.df(tcm.ts, type=""none"",selectlags=""BIC""))
summary(ur.df(tcm.ts, type=""drift"",selectlags=""BIC""))
summary(ur.df(tcm.ts, type=""trend"",selectlags=""BIC""))

&gt; summary(ur.df(tcm.ts, type=""none"",selectlags=""BIC""))

############################################### 
# Augmented Dickey-Fuller Test Unit Root Test # 
############################################### 

Test regression none 


Call:
lm(formula = z.diff ~ z.lag.1 - 1 + z.diff.lag)

Residuals:
    Min      1Q  Median      3Q     Max 
-95.199 -23.380  -6.608  26.885  86.560 

Coefficients:
           Estimate Std. Error t value Pr(&gt;|t|)   
z.lag.1     0.04398    0.01205   3.650  0.00183 **
z.diff.lag -0.03722    0.24417  -0.152  0.88053   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 45 on 18 degrees of freedom
Multiple R-squared:  0.7091,    Adjusted R-squared:  0.6768 
F-statistic: 21.94 on 2 and 18 DF,  p-value: 1.492e-05


Value of test-statistic is: 3.6495 

Critical values for test statistics: 
      1pct  5pct 10pct
tau1 -2.66 -1.95  -1.6

&gt; summary(ur.df(tcm.ts, type=""drift"",selectlags=""BIC""))

############################################### 
# Augmented Dickey-Fuller Test Unit Root Test # 
############################################### 

Test regression drift 


Call:
lm(formula = z.diff ~ z.lag.1 + 1 + z.diff.lag)

Residuals:
    Min      1Q  Median      3Q     Max 
-69.366 -24.625  -3.018  34.165  82.227 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept) -75.21181   45.89715  -1.639   0.1196  
z.lag.1       0.09756    0.03467   2.814   0.0119 *
z.diff.lag   -0.20396    0.25469  -0.801   0.4343  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 43.03 on 17 degrees of freedom
Multiple R-squared:  0.3596,    Adjusted R-squared:  0.2843 
F-statistic: 4.773 on 2 and 17 DF,  p-value: 0.02264


Value of test-statistic is: 2.814 8.6258 

Critical values for test statistics: 
      1pct  5pct 10pct
tau2 -3.75 -3.00 -2.63
phi1  7.88  5.18  4.12

&gt; summary(ur.df(tcm.ts, type=""trend"",selectlags=""BIC""))

############################################### 
# Augmented Dickey-Fuller Test Unit Root Test # 
############################################### 

Test regression trend 


Call:
lm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag)

Residuals:
   Min     1Q Median     3Q    Max 
-86.46 -14.84   5.56  20.87  70.29 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)  91.4039    92.1407   0.992   0.3360  
z.lag.1      -0.1127     0.1082  -1.042   0.3129  
tt           13.0810     6.4318   2.034   0.0589 .
z.diff.lag   -0.1287     0.2369  -0.543   0.5946  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 39.54 on 16 degrees of freedom
Multiple R-squared:  0.4911,    Adjusted R-squared:  0.3957 
F-statistic: 5.148 on 3 and 16 DF,  p-value: 0.01109


Value of test-statistic is: -1.042 8.1902 6.7579 

Critical values for test statistics: 
      1pct  5pct 10pct
tau3 -4.38 -3.60 -3.24
phi2  8.21  5.68  4.67
phi3 10.61  7.24  5.91
</code></pre>
"
"0.0746393370862076","0.0592723347638087","102695","<p>I'm using R to run some logistic regression. My variables were continuous, but I used cut to bucket the data. Some particular buckets for these variables always result in dependent variable being equal to 1. As expcted, the coefficient estimate for this bucket is very high, but the p-value is also high. There are about ~90 observations in either these buckets, and around 800 total observations, so I don't think it's a problem of sample size. Also, this variable should not be related to other variables, which would naturally reduce their p-values.</p>

<p>Are there any other plausible explanations for the high p-value?</p>

<p>Example:</p>

<pre><code>myData &lt;- read.csv(""application.csv"", header = TRUE)
myData$FICO &lt;- cut(myData$FICO, c(0, 660, 680, 700, 720, 740, 780, Inf), right = FALSE)
myData$CLTV &lt;- cut(myData$CLTV, c(0, 70, 80, 90, 95, 100, 125, Inf), right = FALSE)
fit &lt;- glm(Denied ~ CLTV + FICO, data = myData, family=binomial())
</code></pre>

<p>Results are something like this:</p>

<pre><code>Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.53831  -0.77944  -0.62487   0.00027   2.09771  

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)     -1.33630    0.23250  -5.747 9.06e-09 ***
CLTV(70,80]     -0.54961    0.34864  -1.576 0.114930    
CLTV(80,90]     -0.51413    0.31230  -1.646 0.099715 .  
CLTV(90,95]     -0.74648    0.37221  -2.006 0.044904 *  
CLTV(95,100]     0.38370    0.37709   1.018 0.308906    
CLTV(100,125]   -0.01554    0.25187  -0.062 0.950792    
CLTV(125,Inf]   18.49557  443.55550   0.042 0.966739    
FICO[0,660)     19.64884 3956.18034   0.005 0.996037    
FICO[660,680)    1.77008    0.47653   3.715 0.000204 ***
FICO[680,700)    0.98575    0.30859   3.194 0.001402 ** 
FICO[700,720)    1.31767    0.27166   4.850 1.23e-06 ***
FICO[720,740)    0.62720    0.29819   2.103 0.035434 *  
FICO[740,780)    0.31605    0.23369   1.352 0.176236    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1037.43  on 810  degrees of freedom
Residual deviance:  803.88  on 798  degrees of freedom
AIC: 829.88

Number of Fisher Scoring iterations: 16
</code></pre>

<p>FICO in the range [0, 660) and CLTV in the range (125, Inf] indeed always results in Denial = 1, so their coefficients are very large, but why are they also ""insignificant""?</p>
"
"0.0867230728520531","0.0734594449379399","103129","<p>I am trying to understand what the reported intercept is showing when I use <code>arima()</code> with <code>xreg=</code>. The documentation says</p>

<p>""If am xreg term is included, a linear regression (with a constant term if include.mean is true and there is no differencing) is fitted with an ARMA model for the error term.""</p>

<p>Thus I expect the intercept shown to come from the regression using <code>xreg=</code> as the X variables, before any arima model is done on those residuals. </p>

<p>However I tried to double check this by actually doing the regression with <code>lm()</code> and the intercept from that does not match what is reported from <code>arima()</code> (although the slope coefficient is pretty close). </p>

<p>Here is my example:</p>

<pre><code>set.seed(456)
v = rnorm(100,1,1)
x = cumsum(v)  ; x = as.xts(ts(x)) 

# Fit AR(1) after taking out a time trend (aka, drift)
model5 = arima(x, order=c(1,0,0), xreg=1:length(x), include.mean=TRUE)
# Coefficients:
#         ar1     intercept  1:length(x)
#       0.8995     0.8815       1.1113
# s.e.  0.0422     1.6193       0.0265


# Double check
MyTime = 1:length(x)
model5_Part1 = lm(x ~ MyTime )
# Coefficients:
#      (Intercept)       MyTime  
#         1.856           1.096
</code></pre>

<p>The intercepts do not match, thus I do not know what the intercept is showing from the arima with xreg.</p>

<p>Note the example shown is based on ""Issue 2"" shown here <a href=""http://www.stat.pitt.edu/stoffer/tsa3/Rissues.htm"" rel=""nofollow"">http://www.stat.pitt.edu/stoffer/tsa3/Rissues.htm</a></p>

<p>Also note that this isn't a problem particular to modeling drift. Here is another example, where in addition to the intercept not matching, even the slope coefficient on the <code>xreg=</code> variable doesn't match what is shown from using <code>lm()</code>. This example has nothing to do with drift and uses the cars dataset as if it were time series data.</p>

<pre><code>data(cars)
cars = as.xts(ts(cars, start=c(1980,1), freq=12))
model6 = arima(cars$speed, xreg=cars$dist, order=c(1,0,0), include.mean=TRUE)
# Coefficients:
#         ar1    intercept   dist
#       0.9979    15.2890  -0.0172
# s.e.  0.0030    10.5452   0.0055

model6_Part1 = lm(cars$speed ~ cars$dist)
# Coefficients:
#      (Intercept)    cars$dist  
#        8.2839        0.1656 
</code></pre>

<p>Intercepts do not match, slope coefficient does not match.</p>
"
"0.068136080998913","0.0649295895722714","103340","<p>I have a question about SAS and R. For a research, I used a longitudinal data and I initially used SAS (<code>GLIMMIX</code>) and then I analyzed the data with R (<code>glmer</code>) programming. There are differences between p-values of SAS and R. I expected that regression coefficient and standard error could be different for R and SAS. But there are differences for p value for some variables, which are significant in R, are not significant in SAS. </p>

<p>My R model and SAS model are respectively :</p>

<pre><code>#R
m3.glmm &lt;- glmer(y ~ timebefore + timeafter + x1 + x2 +...+ x11 +      
                     (1+timebefore+timeafter|id), 
                 data=data, family=binomial(link=""logit""), nAGQ=3)

#SAS
proc glimmix data=data METHOD=QUAD(QPOINTS=3) NOCLPRINT ;
  class id x2 x3 x4 x5;
  model y(event='1')=timebefore timeafter x1 x2 x3 x4 x5 
        x6 x7  x8 x9 x10 x11 /solution CL link = logit dist = binary;
  random intercept timebefore timeafter/subject = id GCORR SOLUTION;
run;
</code></pre>

<p>Eg: variable ""x1""(defined as age) was significant (p val= 0.04) in SAS but not in R (p val=0.1). But others were similar. It means that significant variables in SAS are found significant in R, or insignificant variables in SAS are insignificant in R. </p>

<p>Does anybody know about the differences?</p>
"
"0.0497595580574717","0.047417867811047","103598","<p>I'm wondering how to explain the <code>logSigma</code> and its p.value in a censored regression model:</p>

<pre><code>require(cenReg)
data( ""Affairs"", package = ""AER"" )
estResult &lt;- censReg( affairs ~ age + yearsmarried + religiousness +
                           occupation + rating, data = Affairs )
summary(estResult)

Call:
censReg(formula = affairs ~ age + yearsmarried + religiousness + 
    occupation + rating, data = Affairs)

Observations:
         Total  Left-censored     Uncensored Right-censored 
           601            451            150              0 

Coefficients:
              Estimate Std. error t value  Pr(&gt; t)    
(Intercept)    8.17420    2.74145   2.982  0.00287 ** 
age           -0.17933    0.07909  -2.267  0.02337 *  
yearsmarried   0.55414    0.13452   4.119 3.80e-05 ***
religiousness -1.68622    0.40375  -4.176 2.96e-05 ***
occupation     0.32605    0.25442   1.282  0.20001    
rating        -2.28497    0.40783  -5.603 2.11e-08 ***
logSigma       2.10986    0.06710  31.444  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Newton-Raphson maximisation, 7 iterations
Return code 1: gradient close to zero
Log-likelihood: -705.6 on 7 Df
</code></pre>

<p>My question is how to explain the logSigma and its significant p.value in the above model.</p>
"
"0.035185320931284","0.0335294958785986","103638","<p>I'm using R and the package boot.</p>

<p>my boot function returns the coefficients and the r.square and sqrt of the r.square </p>

<pre><code> rsq2 &lt;- function(formula, data, indices) {
     d &lt;- data[indices,] # allows boot to select sample 
     fit &lt;- lm(formula, data=d)
     return(c( coef(fit),summary(fit)$r.square,sqrt(summary(fit)$r.square)))
 }

boot(data=mtcars,rsq2,1000,formula=""mpg~wt"")

#ORDINARY NONPARAMETRIC BOOTSTRAP

Call:
boot(data = mtcars, statistic = rsq2, R = 1000, formula = ""mpg~wt"")


Bootstrap Statistics :
      original        bias    std. error
t1* 37.2851262  0.1170681005  2.32470420
t2* -5.3444716 -0.0557873180  0.70523904
t3*  0.7528328  0.0019940314  0.05758118
t4*  0.8676594  0.0004980607  0.03362469
</code></pre>

<p>I determine a pvalue using the original value and the std. error determined by the boot program</p>

<pre><code>pvalue of the regression coefficient
2*pnorm(-abs(-5.3444716/0.70523904))
[1] 3.502706e-14
pvalue of the CC
 2*pnorm(-abs(0.8676594/0.03362469))
[1] 7.947938e-147
</code></pre>

<p>Why are these not the same value?</p>

<p>In a simple univariable the linear regression of the p-value of the coefficient and Pearson's CC are the same value?</p>
"
"0.0457070726502004","0.0580747904139041","103842","<p>Is there a way to statistically compare r-squared across 2 groups using nested models in multigroup analysis? I know how to use lavaan to test various other parameters across groups (e.g. regression coefficients, intercepts, variances...etc), but I can't find documentation for how to compare R-squared. </p>

<p>Here is an example of what I mean:</p>

<pre><code>HS.model &lt;-  'visual  =~ x1 + x2 + x3
              textual =~ x4 + x5 + x6
              speed   =~ x7 + x8 + x9 
              visual + textual + speed ~ grade
              textual + speed ~ visual''
fit &lt;- cfa(HS.model, data=HolzingerSwineford1939, group=""sex"")
summary(fit, fit.measures=TRUE, rsquare=TRUE)
</code></pre>

<p>This produces (among other things) the following R-squared statistics:</p>

<pre><code>R-Square Group 1:

x1                0.519
x2                0.110
x3                0.375
x4                0.705
x5                0.729
x6                0.662
x7                0.458
x8                0.668
x9                0.266
visual            0.057
textual           0.217
speed             0.194

R-Square Group 2:

x1                0.669
x2                0.266
x3                0.304
x4                0.763
x5                0.721
x6                0.731
x7                0.320
x8                0.495
x9                0.515
visual            0.057
textual           0.258
speed             0.393
</code></pre>

<p>If I wanted to determine if ""grade"" and ""vision"" (predictors in the example) accounted for significantly more variability in speed for boys (.194) than girls (.393), how would I do that? </p>
"
"0.125636725583038","0.0915538363472867","104460","<p>I have a binomial variable that I regress against different categorical variables which I have contrasted to build a reference of an individual Female, Married, aged 35-45, High education :</p>

<pre><code>Call:
glm(formula = wpti ~ gender.f + is.married.f + age.f + edu.f, 
family = binomial(link = ""logit""))

Coefficients:
                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -1.303107   0.046522 -28.010  &lt; 2e-16 ***
gender.fMale       -0.537958   0.036833 -14.605  &lt; 2e-16 ***
is.married.fSingle  0.012196   0.040584   0.301 0.763792    
age.f&lt;25           -0.298081   0.078040  -3.820 0.000134 ***
age.f25-35         -0.121670   0.051283  -2.373 0.017667 *  
age.f45-55         -0.006033   0.049078  -0.123 0.902162    
age.f&gt;55            0.239855   0.058727   4.084 4.42e-05 ***
edu.fLow           -0.340115   0.049879  -6.819 9.18e-12 ***
edu.fMedium        -0.298925   0.041729  -7.163 7.86e-13 ***
</code></pre>

<p>I was expecting to see all the coefficients (and not only the one for gender) to change if I contrast the gender factor with Female as reference, but they all remain the same, as if being a man or woman has no effect. </p>

<pre><code>Call:
glm(formula = wpti ~ gender.f + is.married.f + age.f + edu.f, 
family = binomial(link = ""logit"")) 

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -1.841064   0.048257 -38.151  &lt; 2e-16 ***
gender.fFemale      0.537958   0.036833  14.605  &lt; 2e-16 ***
is.married.fSingle  0.012196   0.040584   0.301 0.763792    
age.f&lt;25           -0.298081   0.078040  -3.820 0.000134 ***
age.f25-35         -0.121670   0.051283  -2.373 0.017667 *  
age.f45-55         -0.006033   0.049078  -0.123 0.902162    
age.f&gt;55            0.239855   0.058727   4.084 4.42e-05 ***
edu.fLow           -0.340115   0.049879  -6.819 9.18e-12 ***
edu.fMedium        -0.298925   0.041729  -7.163 7.86e-13 ***
</code></pre>

<p>However, if I remove the gender in the regression, the coefficients for the other variables are different : </p>

<pre><code>Call:
glm(formula = wpti ~ is.married.f + age.f + edu.f, family = binomial(link = ""logit""))

Coefficients:
                    Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -1.562816   0.043369 -36.035  &lt; 2e-16 ***
is.married.fSingle  0.071780   0.040378   1.778 0.075455 .  
age.f&lt;25           -0.330773   0.077982  -4.242 2.22e-05 ***
age.f25-35         -0.134088   0.051091  -2.625 0.008677 ** 
age.f45-55          0.004803   0.048860   0.098 0.921694    
age.f&gt;55            0.224112   0.058419   3.836 0.000125 ***
edu.fLow           -0.411232   0.049424  -8.321  &lt; 2e-16 ***
edu.fMedium        -0.332353   0.041488  -8.011 1.14e-15 ***
</code></pre>

<p>So I am confused : I was under the impression that the coefficients could show the difference of likeliness of having a result positive or negative if only ONE variable would vary. Now I dont know what to think. Can somebody explain how to interpret the results, and why it is different if I remove one categorical variable (eg. gender here) althought it seems it has no effect on the others ?</p>
"
"NaN","NaN","104571","<p>I have measurements for daily space heating vs daily mean outdoor temperature for two different control strategies. The data is shown here:</p>

<p><img src=""http://i.stack.imgur.com/c4L6Y.png"" alt=""enter image description here""></p>

<p>I have also performed linear regression, of the form:</p>

<pre><code>lm(Energy ~ Control * MeanOutdoorTemp)
</code></pre>

<p>This yields four coefficients:</p>

<pre><code>Coefficients:
                           Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                170.8293     4.2083  40.594  &lt; 2e-16 ***
ControlMPC                 -30.7044     4.9025  -6.263 1.38e-07 ***
MeanOutdoorTemp             -6.2924     1.6466  -3.821 0.000413 ***
ControlMPC:MeanOutdoorTemp   0.8211     1.7162   0.478 0.634709    
</code></pre>

<p>I'd like to test whether these two regression lines cross at zero energy. What would be the statistically correct way to do that?</p>
"
"0.0497595580574717","0.0711268017165705","104585","<p>Using R, I want to run a linear regression to estimate the abnormal return on days with positive, negative and neutral news (CLASS). I'm a beginner in R, as well as in using regression models! 
First of all the data structure is as follows. CONTROLVAR just represents all the columns I use as control variables.</p>

<pre><code>DATE &lt;- c(""1"",""2"",""3"",""4"",""5"",""6"",""7"",""1"",""2"",""3"",""4"",""5"",""6"",""7"")
COMP &lt;- c(""A"", ""A"", ""A"", ""A"", ""A"", ""A"", ""A"", ""B"", ""B"", ""B"", ""B"", ""B"", ""B"", ""B"")
RET &lt;- c(-2.0,1.1,3,1.4,-0.2, 0.6, 0.1, -0.21, -1.2, 0.9, 0.3, -0.1,0.3,-0.12)
CLASS &lt;- c(""positive"", ""negative"", ""aneutral"", ""positive"", ""positive"", ""negative"", ""aneutral"", ""positive"", ""negative"", ""negative"", ""positive"", ""aneutral"", ""aneutral"", ""aneutral"")
LITIGATION &lt;- c(1,0,0,0,0,0,0,0,0,0,0,0,0,0)
POLLUTION  &lt;- c(1,0,1,1,0,0,0,0,1,0,0,0,0,0)
LAYOFF     &lt;- c(0,1,0,0,0,0,0,0,0,0,0,0,0,0)
CONTROLVAR &lt;- c(""11"",""13"",""13"",""14"",""13"",""14"",""12"",""11"",""13"",""13"",""14"",""13"",""14"",""12"")

mydf &lt;- data.frame(DATE, COMP, RET, CLASS, LITIGATION, POLLUTION, LAYOFF, CONTROLVAR, stringsAsFactors=F)

mydf 

#    DATE COMP   RET    CLASS LITIGATION POLLUTION LAYOFF CONTROLVAR
# 1     1    A -2.00 positive          1         1      0         11
# 2     2    A  1.10 negative          0         0      1         13
# 3     3    A  3.00 aneutral          0         1      0         13
# 4     4    A  1.40 positive          0         1      0         14
# 5     5    A -0.20 positive          0         0      0         13
# 6     6    A  0.60 negative          0         0      0         14
# 7     7    A  0.10 aneutral          0         0      0         12
# 8     1    B -0.21 positive          0         0      0         11
# 9     2    B -1.20 negative          0         1      0         13
# 10    3    B  0.90 negative          0         0      0         13
# 11    4    B  0.30 positive          0         0      0         14
# 12    5    B -0.10 aneutral          0         0      0         13
# 13    6    B  0.30 aneutral          0         0      0         14
# 14    7    B -0.12 aneutral          0         0      0         12
</code></pre>

<p>aneutral (neutral) will be the reference category. I would also like to see the effect of certain subjects of the article on the abnormal return.</p>

<p>I'd like to include interaction, so my model looks like this:</p>

<pre><code>mymodel &lt;- lm(RET ~ CLASS * (LITIGATION + POLLUTION + LAYOFF)   # Interaction Variables
               + CONTROLVAR,     # Control Variables
               data=mydf)
</code></pre>

<p>Now I don't really understand how to interpret the coefficients I get and I would like to plot a regression line or anything that visualizes these effects to better understand the results. What's a good way of doing this for a three-class-problem like this? <code>abline()</code> doesn't seem to work, because there are too many variables.</p>

<p>Thank you!</p>
"
"0.0967596325610309","0.0922061136661462","104595","<p>I've been reading <a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments/35994#35994?newreg=4676cdfba08b48449c8c06ba4bdcb71a"">Simulation of Logistic Regression Power Analysis - Designed Experiments</a>, <a href=""http://sas-and-r.blogspot.com/2009/06/example-72-simulate-data-from-logistic.html"" rel=""nofollow"">http://sas-and-r.blogspot.com/2009/06/example-72-simulate-data-from-logistic.html</a>, and <a href=""http://stats.stackexchange.com/questions/22406/power-analysis-for-ordinal-logistic-regression?lq=1"">Power analysis for ordinal logistic regression</a> and I'm still a little lost on how to do a power analysis for my data. I want to be able to determine what N I should have if I have an interaction between a categorical variable (with 3 levels) and a continuous variable.</p>

<p><a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments/35994#35994?newreg=4676cdfba08b48449c8c06ba4bdcb71a"">Simulation of Logistic Regression Power Analysis - Designed Experiments</a> provides some information, but I can't figure out how to simulate the relationship between the categorical and continuous variables and outcome.</p>

<blockquote>
  <p>set.seed(1)<br></p>
  
  <p>repetitions = 1000<br>
  N = 10000<br>
  n = N/8<br>
  var1  = c(   .03,    .03,    .03,    .03,    .06,    .06,    .09,   .09)<br>
  var2  = c(     0,      0,      0,      1,      0,      1,      0,     1)<br>
  rates = c(0.0025, 0.0025, 0.0025, 0.00395, 0.003, 0.0042, 0.0035, 0.002)<br></p>
  
  <p>var1    = rep(var1, times=n)<br>
  var2    = rep(var2, times=n)<br>
  var12   = var1**2<br>
  var1x2  = var1 *var2<br>
  var12x2 = var12*var2<br></p>
  
  <p>significant = matrix(nrow=repetitions, ncol=7)<br></p>
  
  <p>startT = proc.time()[3]<br>
  for(i in 1:repetitions){<br>
   responses          = rbinom(n=N, size=1, prob=rates)<br>
   model              = glm(responses~var1+var2+var12+var1x2+var12x2, <br>
                            family=binomial(link=""logit""))<br>
   significant[i,1:5] = (summary(model)$coefficients[2:6,4]&lt;.05)&lt;br&gt;
&gt;      significant[i,6]   = sum(significant[i,1:5])&lt;br&gt;
&gt;      modelDev           = model$null.deviance-model$deviance<br>
   significant[i,7]   = (1-pchisq(modelDev, 5))&lt;.05<br>
  }<br>
  endT = proc.time()[3]<br>
  endT-startT<br></p>
  
  <p>sum(significant[,1])/repetitions      # pre-specified effect power for var1<br>
  [1] 0.042<br>
  sum(significant[,2])/repetitions      # pre-specified effect power for var2<br>
  [1] 0.017<br>
  sum(significant[,3])/repetitions      # pre-specified effect power for var12<br>
  [1] 0.035<br>
  sum(significant[,4])/repetitions      # pre-specified effect power for var1X2<br>
  [1] 0.019<br>
  sum(significant[,5])/repetitions      # pre-specified effect power for var12X2<br>
  [1] 0.022<br>
  sum(significant[,7])/repetitions      # power for likelihood ratio test of model<br>
  [1] 0.168<br>
  sum(significant[,6]==5)/repetitions   # all effects power<br>
  [1] 0.001<br>
  sum(significant[,6]>0)/repetitions    # any effect power<br>
  [1] 0.065<br>
  sum(significant[,4]&amp;significant[,5])/repetitions   # power for interaction terms<br>
  [1] 0.017<br></p>
</blockquote>

<p>I feel like I should be able to adapt the code from <a href=""http://stats.stackexchange.com/questions/22406/power-analysis-for-ordinal-logistic-regression?lq=1"">Power analysis for ordinal logistic regression</a> and that this would be a better, more succinct option</p>

<blockquote>
  <p>library(rms)</p>
  
  <p>tmpfun &lt;- function(n, beta0, beta1, beta2) { <br>
     x &lt;- runif(n, 0, 10) <br>
     eta1 &lt;- beta0 + beta1*x <br>
     eta2 &lt;- eta1 + beta2 <br>
     p1 &lt;- exp(eta1)/(1+exp(eta1)) <br>
     p2 &lt;- exp(eta2)/(1+exp(eta2)) <br>
     tmp &lt;- runif(n) <br>
     y &lt;- (tmp &lt; p1) + (tmp &lt; p2) <br>
     fit &lt;- lrm(y~x) <br>
     fit$stats[5] <br>
  } <br></p>
  
  <p>out &lt;- replicate(1000, tmpfun(100, -1/2, 1/4, 1/4)) <br>
  mean( out &lt; 0.05 ) <br></p>
</blockquote>

<p>but I'm not completely sure how to do so. I'm assuming tmpfun(100,-1/2, 1/4,1/4) is specifying the N and betas that you want, but how do I adjust tmpfun to another (categorical) variable and include an interaction term? Ultimately the equation should include 6 betas: the intercept, the beta for x, the beta for z1, the beta for z2, the interaction term between x and z1, and the interaction term between x and z2. </p>

<p>Finally, I can't find any reliable sources on what sorts of ""effect sizes"" I should be using as small or medium. </p>

<p>Let me know if I can provide more information!</p>
"
"0.035185320931284","0.0502942438178979","104722","<p>I'm calculating a penalized least squares regression (PLS). Two influence variables are connected to a latent variable. This latent variable has an influence (beta coefficient) of 0.6 on the response (or latent variable of the response).</p>

<p>Both influence variables are correlated (outer loading) with 0.6 and 0.8 to the latent variable. </p>

<p>Does anyone know how to (back)-calculate a beta coefficient for each influence variable and not just the latent variable connected to the influence variables?</p>

<p>I'm using a mix of the smartPLS programming tool and R.</p>

<p>EDIT:
<a href=""http://stackoverflow.com/questions/24446300/sempls-package-backcalculating-influence"">http://stackoverflow.com/questions/24446300/sempls-package-backcalculating-influence</a></p>
"
"0.0609427635336005","0.0580747904139041","104733","<p>I have a vector of data with their standard errors:</p>

<pre><code>Estimate &lt;- c(0.254719513441046, 0.130492717014416, 0.0386710035855823, 0.14118562325405, 0.160649388742147, 0.60363287936294, 0.173485345603584, 0.425817607348994, 0.128802795868366, 0.104136474748465)
SE &lt;- c(0.126815201703205, 0.240179692822184, 0.248612907189712, 0.379800224602374, 0.0799874163236805, 0.170568135051654, 0.108163615496468, 0.0585237357996271, 0.16702614577514, 0.124308993809982)
</code></pre>

<p>I need to form a prediction interval for new elements that will be drawn from the same parent population. At first I thought I would do a linear regression to estimate the mean and the error on the mean:</p>

<pre><code>summary(lm(Estimate ~ 1, weights = 1/SE^2))
Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.28014    0.04996   5.607 0.000331 ***
</code></pre>

<p>But when I try naively to call <code>predict()</code> here is what I get:</p>

<pre><code>predict(lm(Estimate ~ 1, weights = 1/SE^2), newdata = data.frame(x = 0), interval = ""prediction"")
        fit       lwr      upr
1 0.2801405 -2.860802 3.421083
Warning message:
In predict.lm(lm(Estimate ~ 1, weights = 1/SE^2), newdata = data.frame(x = 0),  :
  Assuming constant prediction variance even though model fit is weighted
</code></pre>

<p>Did I do this right? Is the 95% confidence interval really [-2.86 3.42]? Do I need to worry about the warning?</p>
"
"NaN","NaN","104890","<p>I'm working with some real world data and the regression models are yielding some counterintuitive results. Normally I trust the statistics but in reality some of these things can not be true. The main problem that I am seeing is that an increase in one variable is causing an increase in the response when, in fact in reality, they must be negatively correlated.</p>

<p>Is there a way to force a specific sign for each of the regression coefficients? Any R code to do this would be appreciated as well.</p>

<p>Thanks for any and all help!</p>
"
"0.052777981396926","0.0502942438178979","105115","<p>I cannot understand the usage of polynomial contrasts in regression fitting. In particular, I am referring to an encoding used by <code>R</code> in order to express an interval variable (ordinal variable with equally spaced levels), described at <a href=""http://www.ats.ucla.edu/stat/r/library/contrast_coding.htm#ORTHOGONAL"">this page</a>.</p>

<p>In the example of <a href=""http://www.ats.ucla.edu/stat/r/library/contrast_coding.htm#ORTHOGONAL"">that page</a>, if I understood correctly, R fits a model for an interval variable, returning some coefficients which weights its linear, quadratic, or cubic trend. Hence, the fitted model should be:</p>

<p>$${\rm write} = 52.7870 + 14.2587X - 0.9680X^2 - 0.1554X^3,$$</p>

<p>where $X$ should take values $1$, $2$, $3$, or $4$ according to the different level of the interval variable.</p>

<p>Is this correct? And, if so, what was the purpose of polynomial contrasts?</p>
"
"0.0430930413588572","0.0205325390588295","105424","<p>I'm trying to do a linear regression in R, however I have the added constraint the coefficients from the linear regression need to sum to a user given value between 0 and 1. (I understand forcing the coefficients like this will make the fit rather poor, but it's a needed constraint)</p>

<p>I've been though the documentation for lm() and the glmc package, as well as some similar questions, but none seem to tackle how to get the coefficients to sum to a specific value.</p>

<p>For data T, N, and target sum for coefficients p:</p>

<pre><code>Regression &lt;- function(T, N, p) {    
     fit &lt;- lm(T[,1] ~ N)
     coef &lt;- coef(fit)
...
}
</code></pre>

<p>Ideally I want sum(coef[-1]) = p (ignoring the intercept)</p>

<p>Sorry I can't provide more code, but I don't think I can do what I would like with lm().</p>

<p>Does anyone know of a way to do this, or of a package that will let me do this? </p>
"
"0.0609427635336005","0.0580747904139041","105633","<p>There are times when one might want to estimate a prevalence ratio or relative risk, in preference to an odds ratio, for data with binary outcomes - say, if the outcome in question isn't rare, so the RR ~ OR relationship doesn't hold.</p>

<p>I've implemented a model in R to do that, as follows:</p>

<pre><code>uni.out &lt;- glm(Death ~ onset, family= binomial(link=log), data=data)
</code></pre>

<p>But I'm continually getting convergence issues, even when providing starting values (such as the coefficient estimates pulled from a logistic regression), or turning up the number of allowed iterations. I've also tried <code>glm2</code> without any success.</p>

<p>The two ideas I have from here are to either fit a poisson model to the same data using a sandwich estimator for the variance, or fitting the model using MCMC and taking the standard error of the posterior (this is being used alongside multiple imputation, so I can't just report the posterior). The problem is, I have no idea how to implement either one of these in <code>R</code>, nor if they're the best solution.</p>

<p>Additionally, while using a model like:</p>

<pre><code>glm(Death ~ age, family= binomial(link=log),start=c(-3.15,0.03),data=data)
</code></pre>

<p>I'm regularly get an error message ""Error: cannot find valid starting values: please specify some"", but not always. What is generating this message?</p>
"
"0.122096950516762","0.123195234352977","106259","<p>I use SPSS, but am forced to use R for exact logistic regression.  So I'm brand new to R (and hating it so far) and also new to logistic regression.  I've read the original elrm paper and looked at examples of its use. However, I can't find information on the questions below (after the data description).</p>

<p>The fit of two models of cognitive processing was compared for each subject in each of 3 conditions. My binary dependent variable is whether the difference in model fits was significant or not (my ""Success"" variable below). I have three experimental Conditions: 0, 1, and 2.  0 is my reference group.  My question is:  is there an overall effect of Condition? If so, which conditions differ?  The specific alternative hypothesis is that the proportion/probability of ""success"" should be greater in conditions 0 and 1 than in condition 2.  My data look like</p>

<p><img src=""http://i.stack.imgur.com/OAFtP.gif"" alt=""original data""></p>

<p>...and so on. SPSS actually creates the dummy variables for you on the fly but they are easy enough to create explicitly.</p>

<p><strong>Question 1:</strong>  I have read that to use elrm you have to enter the data such that the response variable represents success/number of trials. And as far as I can tell elrm doesn't create dummy variables automatically.  I've seen examples of tables representing this data structure, but can't find any step-by-step examples of getting raw data into that format, espescially given a one-variable 3-levels situation.  Is there an example out there that I'm missing?  If not, is this what the data should look like? </p>

<p><img src=""http://i.stack.imgur.com/aVxOL.gif"" alt=""reformated data""></p>

<p>I'm not sure how I'd enter the dummy variables into the formula...just as separate variables?</p>

<p><strong>Question 2:</strong>  I can see how I can get the tests of the coefficients of the dummy variables. But I can't figure out how to get a test of the overall effect of the independent variable. I need to evaluate the overall effect of Condition before looking at individual conditions.  Is there a way to get that out of elrm? (I found an example of this done for the aod package which runs regular logistic regression but not exact logistic regression.)</p>

<p><strong>Question 3:</strong>  I can't find a description of what the p-value for individual coeffeicients represents in elrm.  Is this is for the Wald test?</p>
"
"0.132988012834533","0.126729582400196","106347","<p>I am somewhat familiar with various ways of testing mediation for factors in different types of regression analysis.  (I'm using R and currently working with a multilevel binary logistic regression.)  But now I have a situation in which I'd like to test whether one interaction between factors mediates another, and I'm not sure how this could be done properly.  </p>

<p>To give a simplified example of what I am interested in doing:</p>

<p>I have a multilevel binary model using student characteristics to predict pass/fail for students who are in a control versus experimental group.  </p>

<p>Let's say that the ""intervention"" appears to affect women more strongly than men, because the interaction gender*intervention is significant.  But then adding in a number of co-variates (and their interactions with the intervention),  results in a decrease in the magnitude of the coefficient and the significance of the fenale*intervention interaction, suggesting that once we control for these co-variates and their interactions with the intervention, differences between how the intervention ""affects"" men and women are no longer significantly different.  </p>

<p>I would like to be able to say something about mediation, and I understand how to test the individual factors for mediation of the gender*intervention interaction, but what if there is another interaction, such as (hours spent on childcare)*intervention, which I think may mediate the gender*intervention interaction?  Is there a way to test whether the first interaction mediates the second one?</p>

<p>EDIT:</p>

<p>As requested, here is a simple example equation which I think explains what I want to do.  For the purposes of simplicity, I am specifying this as a simple binary logistic regression model instead of a multilevel binary logistic regression model.  </p>

<p>Let's suppose there are three IV being used as factors:
intervention = whether student was in control or experimental group
gender
GPA</p>

<p>And the DV is whether the student passed or failed.  </p>

<p>And let's suppose we consider the following models (I'm just listing the factors here, without coefficients or error terms, for ease of readability):</p>

<p>M1: OUTCOME = INTERVENTION + GENDER + INTERVENTION*GENDER</p>

<p>M2: OUTCOME = INTERVENTION + GENDER + GPA + INTERVENTION*GENDER + INTERVENTION*GPA</p>

<p>Suppose in M1 that INTERVENTION*GENDER was significant, so that women benefited significantly more than men from the intervention.  </p>

<p>Then supposed that INTERVENTION*GENDER was not significant (and the female*experimental coefficient had a smaller magnitude) in M2, and we suspect that this is because INTERVENTION*GPA mediates INTERVENTION*GENDER.   </p>

<p>What I would like to know is if there is a way to test whether or not INTERVENTION*GPA mediates INTERVENTION*GENDER for these two models....</p>
"
"NaN","NaN","106394","<p>I have 15 subjects each with 200 trials &amp; I'd like to run separate regressions for each subject.</p>

<p>If I just run the regression on the whole dataset I am able to generate odds ratios / confidence intervals from it:</p>

<pre><code>acc.log = glm(response ~ phdot + sdot, data=fullerr, family=""binomial"")
summary(acc.log)

exp(acc.log$coefficients)
exp(confint(acc.log))
</code></pre>

<p>But then when I fit each subject separately (I've tried using <code>by</code>), I am an unsure how to then extract information like odds ratios (standard errors &amp; p values are also no longer shown in the output).</p>

<pre><code>split.acc &lt;- by(fullerr, fullerr[,""subject""], function(fullerr){ 
                  glm(response ~ phdot + sdot, data=fullerr, family=""binomial"")
                })
</code></pre>
"
"0.0806196982594614","0.076825726438694","107507","<p>Lets say that I have ""y"" that I want to model with linear regression. ""x"" and ""z"" are the things I'm interrested in showing folks, but I also have things that I want to adjust for, but not really show in my plot.</p>

<p>Now, I would like to show my coefficients in a plot, but I would like to keep the things I adjusted for out of it. So perhaps this could be coronary artery calcification modeled as ""CAC ~ SomeBloodStuff + Bloodpressure + BMI + Smoking_status"". The BMI and Smoking_status would be something that I would want to take into account, but just note that I have adjusted for them.</p>

<p>I gather this is how to do the model:</p>

<pre><code>MyModel &lt;- lm( CAC ~ SomeBloodstuff + BP + BMI + Smoking_status, data=MyData)
summary(MyModel) 
coefficients(MyModel)
confint(MyModel, level=0.95) # Seems like a legit model.
</code></pre>

<p>The coefplot function in arm library gives just the sort of presentation that I want, but shows all the dimensions.</p>

<pre><code>library(arm)
coefplot(MyModel)
</code></pre>

<p>How could I leave those few variables out of the plot while keeping them in the regression and get a plot that looks like what the coefplot produces?</p>
"
"0.0609427635336005","0.0580747904139041","107643","<p>Let a linear regression model obtained by the R function lm would like to know if it is possible to obtain by the Mean Squared Error command.</p>

<p>I had the FOLLOWING output of an example</p>

<pre><code>&gt; lm &lt;- lm(MuscleMAss~Age,data)
&gt; sm&lt;-summary(lm)
&gt; sm

Call:
lm(formula = MuscleMAss ~ Age, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-16.1368  -6.1968  -0.5969   6.7607  23.4731 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 156.3466     5.5123   28.36   &lt;2e-16 ***
Age          -1.1900     0.0902  -13.19   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 8.173 on 58 degrees of freedom
Multiple R-squared:  0.7501,    Adjusted R-squared:  0.7458 
F-statistic: 174.1 on 1 and 58 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>Multiple R-squared is the sum square error? if the answer is no could explain the meaning of Multiple R-squared and Multiple R-squared</p>
"
"0.101062140164153","0.0963061447907242","108088","<p>I have some elementary problems understanding the consequences of using/adding a lagged  dependent variable in my  predictive model.  Iâ€™m trying to predict values $Y_{i,t+\tau}$  for  $\tau=1-3$ with:</p>

<p>$Y_{i,t+1}=a+bY_{i,t}+cX_{i,t}+e_{i,t+1}$</p>

<p>$Y_{i,t+2}=a+bY_{i,t}+cX_{i,t}+e_{i,t+2}$</p>

<p>$Y_{i,t+3}=a+bY_{i,t}+cX_{i,t}+e_{i,t+3}$ </p>

<p>I already performed a pooled regression where you basically ignore individual firm effects and time-effects and treat every subject equally. As I am trying to forecast different levels (in USD) and my data appears to be extremely tailed as it covers a few extremely large subjects (with extremely high values) but also many small subjects the predictions of the model perform rather poor as the intercept $a$ that is equal for all subject seems largely responsible for this. A fixed model however with individual intercepts is not valid with Lagged dependent variables as the LDV is correlated with the within errors. To account for the heavy tailed errors I already estimated the pooled model with the rlm package (robust lm) that produced slightly better results but overall they appear still very unsatisfactory.</p>

<p>I further read that adding LDVâ€™s results in biased and inconsistent estimators as there is severe correlation between the predictor variables and the model errors and that regular procedures for autocorrelation are not valid anymore. One solution I came across is the use of Instrumental Variables with an Anderson-Hsiao Estimator (i.e using a lag -2 that is not correlated with the error term (with non-autocorrelation assumed but how can you assume no autocorrelation if you incorporate a lag?) Another one is the Arellano Bond GMM estimator, however applying GMM you have to set up moment conditions and I have no idea how to do that and I donâ€™t know exactly how this methods work. What I care about is to obtain an unbiased estimator with valid coefficients and not about standard errors as I donâ€™t do inference. Are there any other strategies to cope with LDVâ€™s I am currently unaware of and what is the best/ideal/easiest way to deal with such matter? Do you best take care of some issues while you ignore others (e.g. autocorrelation)? Iâ€™m a little bit lost here.</p>
"
"0.052777981396926","0.0502942438178979","108302","<p>I am running multiple linear regression with R.</p>

<pre><code>mod=lm(varP ~ var1 +var2+var3+var4)
</code></pre>

<p>The table is:</p>

<pre><code>all:
lm(formula = varP ~ var1 + var2 + var3 + var4)

Residuals:
    Min      1Q  Median      3Q     Max     
-4.9262 -0.6985  0.0472  0.7319  4.3305 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.700823   0.084737   8.271 1.45e-15 ***
var1      1.080172   0.175348   6.160 1.59e-09 ***
var2     -0.057803   0.007777  -7.432 5.25e-13 ***
var3     -9.924772   4.268235  -2.325   0.0205 *  
var4     -0.015104   0.001290 -11.710  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.139 on 460 degrees of freedom
Multiple R-squared:  0.657, Adjusted R-squared:  0.654 
F-statistic: 220.3 on 4 and 460 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>it means that my model explains 65.4% of the variance.
But now, I would like to determine the importance of each predictor.</p>

<p>I was using: </p>

<pre><code>lm.sumSquares(mod) 
</code></pre>

<p>Is dR-sqr relevant to interpret this importance ?</p>

<pre><code>              SS       dR-sqr pEta-sqr  df        F p-value
(Intercept)   88.73054 0.0510   0.1294   1  68.4015  0.0000
var4         177.88026 0.1022   0.2296   1 137.1262  0.0000
var2          71.65234 0.0412   0.1072   1  55.2361  0.0000
var1          49.22579 0.0283   0.0762   1  37.9477  0.0000
var3           7.01377 0.0040   0.0116   1   5.4069  0.0205

Error (SSE)  596.71237     NA       NA 460       NA      NA    
Total (SST) 1739.76088     NA       NA  NA       NA      NA
</code></pre>
"
"0.12583105938145","0.113247800628571","108315","<p>I am running multinomial logistic regression analysis on my data.  The response variable is the number of calves produced each year (0,1, or 2).  I am trying to evaluate the influence of the <em>X</em> variables on the odds of producing a calf.  My <em>X</em> variables are predation risk (WR; continuous), age of mother (age; categorical or continuous), time (wolf; categorical).</p>

<p>First, I have 4 different age classification schemes (i only show 2) - I want to know which one of the age of mother would be ""best"" to use. I could use it as continuous variable - or as categories based on biological reasoning for senescence in older moose (old ladies don't invest in reproduction as much).  So, I thought I would use a likelihood ratio test.</p>

<pre><code>library(mlogit)

modata.model1 &lt;- mlogit(no.C ~ 1 | 1, data=modata, reflevel=""1"", na.action = na.omit) 
modata.model2 &lt;- mlogit(no.C ~ 1 | age, data=modata, reflevel=""1"", na.action = na.omit) 
modata.model3 &lt;- mlogit(no.C ~ 1 | age2, data=modata, reflevel=""1"", na.action = na.omit)  
</code></pre>

<hr>

<pre><code> lrtest(modata.model2,modata.model3)

Likelihood ratio test

Model 1: no.C ~ 1 | age
Model 2: no.C ~ 1 | age2
  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    
1   4 -213.22                         
2   4 -207.57  0 11.309  &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>QUESTION: to interpret this output - there was a significant difference in the loglikelihood when we comparing the continuous age to a categorical age with 2 classes.  The loglik is smaller for model 1 and therefore it would be better to use? Or do I have that backwards? </p>

<p>Next I was going to use the Walds test to evaluate nested models.  To see if the addition of a variable was worth it.</p>

<pre><code>modata.model7 &lt;- mlogit(no.C ~ 1 | age+WR, data=modata, reflevel=""1"", na.action = na.omit) 

modata.model8 &lt;- mlogit(no.C ~ 1 | age+WR+wolf, data=modata, reflevel=""1"", na.action = na.omit) 
</code></pre>

<hr>

<pre><code>Wald test

Model 1: no.C ~ 1 | age + WR
Model 2: no.C ~ 1 | age + WR + wolf
  Res.Df Df  Chisq Pr(&gt;Chisq)
1    244                     
2    242  2 0.5828     0.7472
</code></pre>

<p>QUESTION: this tells me that there is no significant improvement when there is an additional variable of wolf added??  So, then I can use the smaller model or do I use the one with the smaller Res.DF?</p>

<p>In addition to confirming my interpretations of the results I have 2 side questions...  </p>

<p>1)to get the null model for <code>mlogit</code> library - is my <code>modata.model1</code> correct?  I want the intercept only model to compare against.</p>

<p>2) Hosmer and Lemshow suggest by getting Wald values to get significance levels for each coefficient - in mlogit, thats the same as using <code>summary(model)</code> and there they provide the t-values with p instead of needing to do an additional Walds test? (NOTE in the below model i use 2 category age class instead of continuous)</p>

<pre><code>summary(modata.model8)

Call:
mlogit(formula = no.C ~ 1 | age2 + WR + wolf, data = modata, 
    na.action = na.omit, reflevel = ""1"", method = ""nr"", print.level = 0)

Frequencies of alternatives:
    1     0     2 
0.652 0.244 0.104 

nr method
6 iterations, 0h:0m:0s 
g'(-H)^-1g = 7.4E-06 
successive function values within tolerance limits 

Coefficients :
              Estimate Std. Error t-value Pr(&gt;|t|)   
0:(intercept)  0.38730    0.40144  0.9648 0.334657   
2:(intercept) -2.40317    1.04546 -2.2987 0.021523 * 
0:age21       -1.39607    0.43989 -3.1737 0.001505 **
2:age21        0.53952    1.07275  0.5029 0.615012   
0:WR          -1.46584    0.64797 -2.2622 0.023686 * 
2:WR          -0.19214    0.60856 -0.3157 0.752206   
0:wolf1        0.56055    0.63292  0.8857 0.375797   
2:wolf1        0.42642    0.72375  0.5892 0.555744   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Log-Likelihood: -203.11
McFadden R^2:  0.053606 
Likelihood ratio test : chisq = 23.009 (p.value = 0.00079359)
</code></pre>
"
"0.114013470672957","0.100887413963854","108374","<p>I have a monthly time series with an intervention and I would like to quantify the effect of this intervention on the outcome. I realize the series is rather short and the effect is not yet concluded.</p>

<p><strong>The Data</strong></p>

<pre><code>  cds&lt;- structure(c(2580L, 2263L, 3679L, 3461L, 3645L, 3716L, 3955L, 
    3362L, 2637L, 2524L, 2084L, 2031L, 2256L, 2401L, 3253L, 2881L, 
    2555L, 2585L, 3015L, 2608L, 3676L, 5763L, 4626L, 3848L, 4523L, 
    4186L, 4070L, 4000L, 3498L), .Dim = c(29L, 1L), .Dimnames = list(
        NULL, ""CD""), .Tsp = c(2012, 2014.33333333333, 12), class = ""ts"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/lNOEk.jpg"" alt=""enter image description here""></p>

<p><strong>The methodology</strong></p>

<p>1) The pre-intervention series (up until October 2013) was used with the <code>auto.arima</code> function. The model suggested was ARIMA(1,0,0) with non-zero mean. The ACF plot looked good.</p>

<pre><code>pre&lt;-window(cds,start = c(2012,01), end=c(2013,09))

mod.pre&lt;-auto.arima(log(pre))

Coefficients:
         ar1  intercept
      0.5821     7.9652
s.e.  0.1763     0.0810

sigma^2 estimated as 0.02709:  log likelihood=7.89
AIC=-9.77   AICc=-8.36   BIC=-6.64
</code></pre>

<p>2) Given the plot of the full series, the pulse response was chosen below, with T = Oct 2013,</p>

<p><img src=""http://i.stack.imgur.com/YU3nB.jpg"" alt=""enter image description here""></p>

<p>which according to cryer and chan can be fit as follows with the arimax function:</p>

<pre><code>   mod.arimax&lt;-arimax(log(cds), order=c(1,0,0), seasonal=list(order=c(0,0,0),frequency=12), include.mean = TRUE,       
            xtransf=data.frame(Oct13=1*(seq(cds)==22)),
            transfer=list(c(1,1))
          )

    mod.arimax


Series: log(cds) 
ARIMA(1,0,0) with non-zero mean 

Coefficients:
         ar1  intercept  Oct13-AR1  Oct13-MA0  Oct13-MA1
      0.7619     8.0345    -0.4429     0.4261     0.3567
s.e.  0.1206     0.1090     0.3993     0.1340     0.1557

sigma^2 estimated as 0.02289:  log likelihood=12.71
AIC=-15.42   AICc=-11.61   BIC=-7.22
</code></pre>

<p>The residuals from this appeared OK:</p>

<p><img src=""http://i.stack.imgur.com/wvdXD.jpg"" alt=""enter image description here""></p>

<p>The plot of fitted and actuals:</p>

<pre><code>plot(fitted(mod.arimax),col=""red"", type=""b"")
lines(window(log(cds),start=c(2012,02)),type=""b"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/kJ1pj.jpg"" alt=""enter image description here""></p>

<p><strong>The Questions</strong></p>

<p>1) Is this methodology correct for intervention analysis?</p>

<p>2) Can I look at estimate/SE for the components of the transfer function and say that the effect of the intervention was significant?</p>

<p>3) How can one visualize the transfer function effect (plot it?)</p>

<p>4) Is there a way to estimate how much the intervention increased the output after 'x' months? I guess for this (and maybe #3) I am asking how to work with an equation of the model - if this were simple linear regression with dummy variables (for example) I could run scenarios with and without the intervention and measure the impact - but I am just unsure how to work this this type of model.</p>

<p><strong>ADD</strong></p>

<p>Per request, here are the residuals from the two parametrizations.</p>

<p>First from the fit:</p>

<pre><code>fit &lt;- arimax(log(cds), order = c(1,0,0), 
              xtransf = data.frame(Oct13a = 1*(seq_along(cds)==22), Oct13b = 1*(seq_along(cds)==22)),
              transfer = list(c(0,0), c(1,0)))

plot(resid(fit), type=""b"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/sqMZN.jpg"" alt=""enter image description here""></p>

<p>Then, from this fit</p>

<pre><code>mod.arimax&lt;-arimax(log(cds), order=c(1,0,0), seasonal=list(order=c(0,0,0),frequency=12), include.mean = TRUE,       
                   xtransf=data.frame(Oct13=1*(seq(cds)==22)),
                   transfer=list(c(1,1))
)

mod.arimax
plot(resid(mod.arimax), type=""b"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/DjAyu.jpg"" alt=""enter image description here""></p>
"
"0.101062140164153","0.0963061447907242","108522","<p>I have a quadratic regression y against x and I'm interested  in the value x where y is the maximum (ymax->x). I can compute x(ymax) but I'm also interested in the standard error or confidence interval of x(ymax) to get x(ymax) +/- standard error or, respectively, lower and upper xmax confidence interval. </p>

<p>One solution could be to use bootstrap methods and computing new estimates of beta but the real data I have is large and needs one hour to compute the model (mixed model with variance-covariance structure). </p>

<p>Another possibility would be to use the standard errors of the betas to compute a confidence band and to find the cutting points of the horizontal through maximum of the curve and the upper ""confidence curve"" (see below). But I'm not sure if this method is a valid way to get confidence interval.</p>

<p>Is there anyone who can give me some advice how to cope with this?</p>

<p>Thanks in advance.</p>

<hr>

<p><img src=""http://i.stack.imgur.com/enW6I.jpg"" alt=""enter image description here""></p>

<pre><code># simulate data set
b0 &lt;- 100
b1 &lt;- 100
b2 &lt;- -2
(x &lt;- rep(1:50,2))
(y &lt;- b0 + b1*x + b2*x^2 + rnorm(length(x),0,200))

# fit model
fit &lt;- lm(y~x + I(x^2))
summary(fit)

# visualize
plot(y~x)
lines(fitted(fit), col=""red"")

# retrieve coefficients
beta &lt;- fit$coef

# build function for optimization
f1 &lt;- function(x) sum(beta * c(1,x,x^2))

# compute xmax
(xymax &lt;- optimize(f1,interval=c(0,50), maximum=TRUE))

# add line at xmax
# add line at xmax
abline(v=xymax$max, col=""blue"")
    abline(h=xymax$obj, col=""blue"")

# compute confidence band
cilbeta &lt;- beta - qnorm(0.975,0,1)*summary(fit)$coef[,""Std. Error""]
    ciubeta &lt;- beta + qnorm(0.975,0,1)*summary(fit)$coef[,""Std. Error""]
f2 &lt;- function(x) { sum(c(1,x,x^2)*cilbeta) }
f3 &lt;- function(x) { sum(c(1,x,x^2)*ciubeta) }
# curve(f2,from=1,to=50)  does not work !!
ycil = c()
yciu = c()
for (i in 1:50) {
    ycil &lt;- c(ycil,f2(i))
    yciu &lt;- c(yciu,f3(i))
}
lines(1:50,ycil, lty=2)
lines(1:50,yciu, lty=2)

# cutting points between upper ""confidence curve"" horizontal line
# through y.max


# x.max = -b2/(2b3) = -0.5*b2/b3
# var(x.max) = 0.5^2*var(b2/b3)
# var.b23 = b2^2/b3^2*[var(b2)/b2^2 -2*cov(b2,b3)/(b2*b3) + var(b3)/b3^2]
(var.b23 &lt;- coef0[2]^2/coef0[3]^2*(vcov0[2,2]/coef0[2]^2 
            -2*vcov0[2,3]/(coef0[2]*coef0[3]) + vcov0[3,3]/coef0[3]^2
            )
        )
(var.xmax &lt;- (-0.5)^2*var.b23)
(se.xmax &lt;- sqrt(var.xmax))
# confidence interval
paste(formatC(x.max,digits=2,format=""f""),"" (""
      , formatC(x.max-2*se.xmax,digits=2, format=""f""),""-""
      , formatC(x.max+2*se.xmax,digits=2,format=""f""),"")""
      , sep=""""
      )
# [1] ""24.77 (24.12-25.42)""
</code></pre>
"
"0.052777981396926","0.0502942438178979","108529","<p>I am trying to compare OLR, ridge and lasso in my situation. I could calculate SE for OLR and lasso but not for ridge. The following is <code>Prostrate</code> data from <code>lasso2</code> package. </p>

<pre><code>require(lasso2)
require(MASS)
data(Prostate)

fit.lm = lm(lpsa~.,data=Prostate)
summary(fit.lm)$coefficients
               Estimate  Std. Error    t value     Pr(&gt;|t|)
(Intercept)  0.669399027 1.296381277  0.5163597 6.068984e-01
lcavol       0.587022881 0.087920374  6.6767560 2.110634e-09
lweight      0.454460641 0.170012071  2.6731081 8.956206e-03
age         -0.019637208 0.011172743 -1.7575995 8.229321e-02
lbph         0.107054351 0.058449332  1.8315753 7.039819e-02
svi          0.766155885 0.244309492  3.1360054 2.328823e-03
lcp         -0.105473570 0.091013484 -1.1588785 2.496408e-01
gleason      0.045135964 0.157464467  0.2866422 7.750601e-01
pgg45        0.004525324 0.004421185  1.0235545 3.088513e-01

fit.rd = lm.ridge(lpsa~.,data=Prostate, lamda = 6.0012)
#summary(fit.rd)$coefficients, doesnot provide SE 

lfit = l1ce(lpsa~.,data=Prostate,bound=(1:500)/500)
summary(lfit[[10]])$coefficients
                 Value  Std. Error   Z score  Pr(&gt;|Z|)
(Intercept) 2.43614448 2.130515543 1.1434530 0.2528505
lcavol      0.03129045 0.125288320 0.2497475 0.8027826
lweight     0.00000000 0.274549270 0.0000000 1.0000000
age         0.00000000 0.018287840 0.0000000 1.0000000
lbph        0.00000000 0.095587974 0.0000000 1.0000000
svi         0.00000000 0.390936045 0.0000000 1.0000000
lcp         0.00000000 0.149824868 0.0000000 1.0000000
gleason     0.00000000 0.260274039 0.0000000 1.0000000
pgg45       0.00000000 0.007285054 0.0000000 1.0000000
</code></pre>

<p>I have a couple of questions: </p>

<p>(1) How can we calculate Std. Error in case of ridge regression ?</p>

<p>(2) Is it valid to compare <strong>Std. Error</strong> for deciding which (<code>ridge</code>, <code>lasso</code> or <code>OLS</code>) method to use ? Or there are other methods ? If so how can I get them ? </p>
"
"0.052777981396926","0.0502942438178979","109077","<p>I am trying to check the assumptions of a two-way ANCOVA. 
So in my model I have</p>

<ul>
<li>two factors (F1, F2)</li>
<li>one dummy coded two level covariate (C)</li>
<li>one dependent variable (D)</li>
</ul>

<p>In order to check the assumption of homogeneity of regression slopes I tried
to perform an ANOVA with type 3 sums for the model D ~ F1*F2*C to see whether any
interactions with the covariate might be significant.
Using the Anova function from the car package this corresponds to</p>

<pre><code>modd&lt;-aov(D~F1*F2*C)
Anova(modd,type=3)
</code></pre>

<p>However, I encounter the following Error message:</p>

<pre><code>Error in Anova.III.lm(mod, error, singular.ok = singular.ok, ...) : 
 there are aliased coefficients in the model
</code></pre>

<p>My question is, whether it makes sense for the homogeneity test to force R to compute the
ANOVA anyway by supplying the singular.ok=T option or what else I should do in order to 
check the assumption.</p>
"
"0.16996991845461","0.166879331116645","109464","<p>I am new to regression and having problem in solving Heteroscedasticity in OLS. Have done lots of homework and test before seeking your advice. Sharing the background and what I have done to solve the problem. Hope you can share your thoughts if my approach was correct.</p>

<p><strong>Objectives:</strong></p>

<ol>
<li>To find the relationship (model) between an explanatory variable (x) and an explained variable (y) using OLS regression.</li>
<li>if a model (relationship) is found, its usefulness and accuracy of prediction will be studied.</li>
</ol>

<p><strong>Dataset (Cross-sectional):</strong></p>

<ol>
<li>Have 4 datasets, with each 350 sample size.</li>
<li>Each dataset obtained using different intensity of experiment and this is already captured by the explanatory variable in x.</li>
<li>Due to the heterogenity of data, not possible to lump all into a single dataset.</li>
</ol>

<p><strong>Requirement:</strong></p>

<p>One common and statistically acceptable model for all the 4 datasets using OLS</p>

<p><strong>Steps Followed:</strong></p>

<ol>
<li><p>Explanatory Analysis: Found Non-linear relationship </p></li>
<li><p>As intending to use OLS, did 3 transformations of variables in attempt to have linearity:
a) ln(x) ~ ln(y);
b) ln(x) ~ y;
c) x ~ ln(y).
<strong>Note:</strong> Kept d) x ~ y as benchmark</p></li>
<li><p>Did heteroscedasticity test using Breusch-Pagan (BP) test in R for 2(a)-(d) for all the datasets in attempt to find valid model(s).
On the best case i.e 2b), only 2 out of 4 datasets passed the BP test (p-value>0.05)</p></li>
<li><p>As the aim is to have one common model for all the 4 datasets, another variable transformation is done using Tukey's Ladder of Transformation in attempt to have homoscedasticity:
a) ? ? {-2,-1,-0.5, 0.5, 1, 2} is used for x/y/x and y for each of the models in 2(a)-(d). Have total of 64 models (16 x 4) to consider. X and Y refer to the transformed x and y;
b) Now have 2 models passed BP test for 3 out of 4 datasets in the best case;
c) The one that failed has p-value &lt;2.20E-16.</p></li>
<li><p>[deadlock unable to find one valid model that passes all the 4 datasets]</p></li>
<li><p>Proceeded to take the two valid models in Step 4 and done inference Test:
a) the p-values for t-test and F-test are below 0.05 for all the 4 datasets;
b) R-square are above 0.9402 for all the 4 datasets.</p></li>
<li><p>Did cross validation and selected the best model using the smallest mean square error against the two ""valid"" models. Did back transformation on the original scale first before the selection is done so that its apple to apple data comparison. The mean average percentage error for the best model is below 10%</p></li>
<li><p>Now tried to use the best model for prediction:
a) Selected 20 random x values which were not part of the dataset;
b) Predicted y and compared it against Measured y;
c) the  mean average percentage error is below 8% and within the model's mean average percentage error i.e below 10%.</p></li>
</ol>

<p><strong>The problem:</strong></p>

<p>With the steps above I am unable to get a model that passes the heteroscedasticity test all the 4 datasets. Have I done anything incorrectly or is there anything more can be done in Step 4? </p>

<p>Believe mis-specification issue has duly been attended. Not intending to use GLS as I need to use .OLS</p>

<p>I have used heteroscedasticity robust standard errors as a remedy of heteroscedasticity on the one dataset that failed BP test per the Youtube below.
Refer - <a href=""https://www.youtube.com/watch?v=hFoDDwTF4KY"" rel=""nofollow"">https://www.youtube.com/watch?v=hFoDDwTF4KY</a></p>

<p>The standard error increased and t-value decreased for Y for the HC3 corrected dataset. 
But the Y= a  + b X model remain the same.</p>

<p>Is it sufficient to show the p-value for t-test and F-test for the corrected dataset are still below 0.05 hence its ok to use the same Y= a+bX though it failed the BP test earlier?</p>

<p>Hope you can share your thoughts as I am new to regression. </p>

<p>Using many reference books to learn such as </p>

<ol>
<li>Introduction to Econometrics by Wooldridge</li>
<li>Basic Econometrics by Gujerati</li>
<li>Regression Analysis by Example by Chatterjee</li>
</ol>

<p><strong>Original:</strong></p>

<pre><code>Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)          -0.612116   0.009006  -68.76   &lt;2e-16 ***
Y                     5.955984   0.039653  145.65   &lt;2e-16 ***
---

Residual standard error: 0.04138 on 348 degrees of freedom
Multiple R-squared:  0.9832,    Adjusted R-squared:  0.9831 
F-statistic: 2.092e+04 on 1 and 348 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p><strong>Heteroskedasticity Robust Standard Errors corrected using HC3:</strong></p>

<pre><code>Coefficients:
                     Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)          -0.61212    0.01767  -33.77   &lt;2e-16 ***
Y                     5.95598    0.08432   69.12   &lt;2e-16 ***
---

Residual standard error: 0.04138 on 348 degrees of freedom
Multiple R-squared:  0.9832,    Adjusted R-squared:  0.9831 
F-statistic:  4640 on 1 and 348 DF,  p-value: &lt; 2.2e-16

Note: Heteroscedasticity-consistent standard errors using adjustment hc3 
</code></pre>

<p>Thanks</p>
"
"0.052777981396926","0.0502942438178979","109673","<p>I have built a cox model in R using the coxph function in the survival package, and now I need to replicate the model in SQL for scoring.  From my understanding, the model has the form described on the bottom of page 2 of this document, <a href=""http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-cox-regression.pdf"" rel=""nofollow"">http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-cox-regression.pdf</a>, which gives it semi-parametric flexibility. Since there is an unspecified alpha term, I cannot just take the coefficients and use the model like a typical linear model or generalized linear model (and exponentiate). There are ways to estimate this alpha term, and I believe this added term to the hazard is needed to specify the complete model.  If this is the case, how do I get my hands on this alpha term?</p>
"
"0.0457070726502004","0.0580747904139041","110005","<p>I've seen a few similar questions about constraining coefficients so they sum to 1, but I'm not sure if there's a simple change in these approaches to allow the sum to be anything in [0,1]. I need to implement this in R.</p>

<p><a href=""http://stats.stackexchange.com/questions/21565/how-do-i-fit-a-constrained-regression-in-r-so-that-coefficients-total-1"">How do I fit a constrained regression in R so that coefficients total = 1?</a></p>

<p><a href=""http://stats.stackexchange.com/questions/12484/constrained-linear-regression-through-a-specified-point"">Constrained linear regression through a specified point</a></p>

<p>The data sets I'm doing the regression on vary in size, so my model looks something like this:
$Y = {\pi}_{1}{X}_{1}+{\pi}_{2}{X}_{2}+...+{\pi}_{n}{X}_{n}+\epsilon \quad s.t. \quad \sum_{i=1}^{n}{\pi}_{i} \le 1$ and ${\pi}_{i} \ge 0 \quad \forall i$</p>

<p>Before I realized I needed these constraints, I was using quadratic programming, finding $min||Y - (\sum_{i=1}^{n}{\pi}_{i}{X}_{i})||^{2}$ </p>

<p>Thanks!</p>

<p>Edit: I'm not sure that what I'm doing actually has any validity, I'm just experimenting at the moment. Essentially it's sort of a mixture model where $Y$ has contributions from ${X}_{1}...{X}_{n}$ and then from some other source, and I want to see if I can estimate what proportion of Y comes from any X. The reason they don't necessarily sum to 1 is because of that unknown other source. Again, I'm just experimenting, so not sure if this will even work, just curious.  </p>
"
"0.0963589698356145","0.0918243061724248","110136","<p>I'm working on a prediction model for a continuous variable (amount of medicine injected) .I use R for modeling.My project flow is to multiply the prediction of a glm (logistic regression) model that is used to predict 0/1 if a medicine was injected at all with an lm (linear regression) model that is used predict amount of medicine injected - this model works rather good In R .My problem is that when I move this model to MSSQL I get different values for the prediction (i.e. for a random row the value in the R is 400 and in SQL the value for the same row is 640.The model in SQL is made by attaching the models coefficiants from the glm model to produce the glm prediction values and then multiplying it with the lm model prediction values. I don't understand why there is a difference if I use the same coefficients?</p>

<p>Here is the code for the lm and glm models in r:</p>

<pre><code>d7_lm&lt;-lm(Ttl_Inject~UserSource+IsNewIndividual+IsCross,data=train)
d7_glm&lt;-glm(Is_Injected~UserSource+IsNewIndividual+IsCross,data=train)
</code></pre>

<p>Here is a part of the r code for the prediction:</p>

<pre><code>demo$d7_lm_pred&lt;-predict(d7_lm,newdata=demo,type='response')
    demo$d7_glm_pred_response&lt;-ifelse(predict(d7_glm,newdata=demo,type='response')&gt;0.5,1,0)
demo$glm01_lm_response&lt;-demo$d7_lm_pred*demo$d7_glm_pred_response # this is used for a container of the prediction model's values.
</code></pre>

<p>Here is a part of the SQL code : </p>

<pre><code>select TOP 1000*, InjectionAmount_pred= (-2.213e -1.180e+00*(case when User='IAF' then 1 else 0 end)-1.665e+00*(case when UserSource='Viral' then 1 else 0 end)
+IsNewIndividual  *  1.167e+00+IsCross )

* IIF((1 / (1 + EXP(-(-1.346e-03+1.140e-02*(case when UserSource='IAF' then 1 else 0 end) -2.975e-03*(case when UserSource='Viral' then 1 else 0 end)
-IsNewIndividual  * 1.503e-04 +IsCross ))))&gt;0.5,1,0) 
</code></pre>
"
"0.0304713817668003","0.029037395206952","110148","<p>I have run a <code>multinomial logistic regression</code> test for the interaction between species of deer, days a camera trap was in the field and type of reaction. </p>

<p>The model with the best AIC value was: </p>

<pre><code>Coefficients:
   (Intercept) speciesmuntjac   speciesroe speciessika        days
r   -0.7471023      0.6263753 -0.005967869 -0.74253017 -0.05189515
sr   0.6909319      0.5552278 -0.355611180 -0.01622306 -0.03178001

Residual Deviance: 971.6464 
AIC: 991.6464 
</code></pre>

<p>But I also got the following </p>

<pre><code>    (Intercept) speciesmuntjac speciesroe speciessika       days
r  0.0071741793     0.17599897  0.9865350   0.2095687 0.08669276
sr 0.0001402185     0.08536257  0.1331651   0.9574258 0.08829020
</code></pre>

<p>I'm just wondering if anyone knows why this is considered the best model when neither days nor deer react (r) or strongly react (sr) higher than chance?</p>
"
"0.0430930413588572","0.0410650781176591","110155","<p>I have run a multinomial logistic regression test for the interaction between species of deer, days a camera trap was in the field and type of reaction.</p>

<p>The model with the best AIC value was:</p>

<pre><code>Coefficients:
   (Intercept) speciesmuntjac   speciesroe speciessika        days
r   -0.7471023      0.6263753 -0.005967869 -0.74253017 -0.05189515
sr   0.6909319      0.5552278 -0.355611180 -0.01622306 -0.03178001

Residual Deviance: 971.6464 
AIC: 991.6464 
</code></pre>

<p>But I also got the following</p>

<pre><code>    (Intercept) speciesmuntjac speciesroe speciessika       days
r  0.0071741793     0.17599897  0.9865350   0.2095687 0.08669276
sr 0.0001402185     0.08536257  0.1331651   0.9574258 0.08829020
</code></pre>

<p>I'm just wondering if anyone would know what the interaction between species and days is in relation to r (reaction) and sr (strong reaction)? I know I can't reject the null hypothesis that there is no effect of species or days on r or sr but beyond that I'm lost!</p>
"
"0.0963589698356145","0.0918243061724248","110301","<p>I made this linear regression that shows how well estimated animal locations (longitude) predict actual animal locations. </p>

<pre><code>estimate &lt;- c(-1.514276, -1.513683, -1.514253, -1.514207, -1.513557, -1.513634, -1.513870, -1.511210, -1.511552, -1.511772, -1.511580, -1.511802, -1.509500, -1.510037, -1.510214)

actual &lt;- c(-1.514255, -1.514053, -1.514527, -1.514223, -1.513672, -1.513729, -1.513934, -1.511118, -1.511567, -1.511658, -1.511585, -1.511830, -1.509438, -1.509843, -1.510080)

lm_longitude &lt;- lm(actual ~ estimate)
summary(lm_longitude)

Call:
lm(formula = actual ~ estimate)

Residuals:
       Min         1Q     Median         3Q        Max 
-2.630e-04 -3.825e-05  8.945e-06  6.530e-05  1.645e-04 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.09325    0.02706   3.445  0.00435 ** 
estimate     1.06167    0.01790  59.325  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.000112 on 13 degrees of freedom
Multiple R-squared:  0.9963,    Adjusted R-squared:  0.996 
F-statistic:  3519 on 1 and 13 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>As you can see, estimated locations are very good predictors for actual locations. I was initially alarmed at the residuals vs fitted values plot. It appears to shows residuals that are correlated with the fitted values:</p>

<pre><code>library(ggplot2)
df_lm_longitude &lt;- ggplot2::fortify(lm_longitude) 
ggplot(df_lm_longitude, aes(.fitted, .resid)) + geom_point() + stat_smooth()
</code></pre>

<p><img src=""http://i.stack.imgur.com/0c7Hk.jpg"" alt=""enter image description here""></p>

<p>But change the scale of the y axis, and residuals vs fitted values plot looks perfect:</p>

<pre><code>ggplot(df_lm_longitude, aes(.fitted, .resid)) + geom_point() + stat_smooth() + ylim(-0.01, 0.01)
</code></pre>

<p><img src=""http://i.stack.imgur.com/NHres.jpg"" alt=""enter image description here""></p>

<p>So one of the assumptions of linear regression is that residuals should not be correlated with fitted values. In the model above, the residuals are correlated with the fitted values at a large scale. But zoom out to a small scale, and residuals are not correlated at all?</p>

<p>What resolution should I be using for y axis in residuals vs fitted values plot?</p>
"
"0.0430930413588572","0.0410650781176591","110578","<p>I am trying to reproduce table 2.1 (p. 41) of LeSage and Pace (2009) by means of the <strong>open R software</strong>.</p>

<p>This is a table containing the direct, indirect and total impacts of a SAR model (since the regression coefficients cannot be interpreted directly because of spillovers), <strong>partitioned by W-order</strong>.</p>

<p>I am able to produce the first part of the table ( impacts() in R ), but for the moment I don't have a clue on <strong>how to split these impacts per power of W in the R software</strong>.</p>

<p>Please, could somebody give me a hint on how to do this? I would be very grateful!</p>

<p>Thank you very much in advance,</p>

<p>Janka</p>
"
"NaN","NaN","110597","<p>My question is simple: is there a function in <code>R</code> which estimates the linear regresion model in a similar fashion as <code>lm</code>, but only using the means, variances, and covariance (correlations), i.e. the sufficient statistics? I am looking for a function to which I can input these statistics (plus sample size) and it returns regression coefficients and tests.</p>
"
"NaN","NaN","110773","<p>I have fitted a zero-inflated beta regression model to my data in R, using the gamlss package. </p>

<p>However, I am unsure of how to assess the fit of the model to my data, i.e. finding a coefficient of determination. </p>

<p>Does anyone know if this can be easily calculated from a zero-inflated beta regression model?</p>

<p>Thanks in advance</p>
"
"0.118246329960506","0.112681644735122","111383","<p>I have been reading several CV posts on binary logistic regression but I am still confused for my current situation.</p>

<p>I am attempting to fit a binary logistic regression to a series of continuous and categorical variables in order to predict the mortality or the survival of animals (<code>qual_status</code>). Please see the <code>str</code> below:</p>

<pre><code>&gt; str(logit)
'data.frame':   136 obs. of  9 variables:
 $ id         : Factor w/ 135 levels ""01001"",""01002"",..: 26 27 28 29 30 31 32 33 34 35 ...
 $ gear       : Factor w/ 2 levels ""j"",""sc"": 2 1 1 2 1 2 1 2 2 1 ...
 $ depth      : num  146 163 179 190 194 172 172 175 240 214 ...
 $ length     : num  37 35 42 38 37 41 37 52 38 37 ...
 $ condition  : Factor w/ 4 levels ""1"",""2"",""3"",""4"": 1 1 4 1 4 2 2 1 2 1 ...
 $ in_water   : num  80 45 114 110 60 121 56 140 93 68 ...
 $ in_air     : num  60 136 128 136 165 118 220 90 177 240 ...
 $ delta_temp : num  8.5 8.4 8.3 8.5 8.5 8.6 8.6 8.7 8.7 8.7 ...
 $ qual_status: Factor w/ 2 levels ""0"",""1"": 1 1 2 1 2 1 2 1 1 1 ...
</code></pre>

<p>I have no issues fitting an the following additive binary logistic regression with the <code>glm</code> function:</p>

<p><code>glm(qual_status ~ gear + depth + length + condition + in_water + in_air + delta_temp, data = logit, family = binomial)</code></p>

<p>...but I am also interested at how these predictor variables interact with one another and possibly influence survival. However, when I attempt the following interactive binary logistic regression:</p>

<p><code>glm(qual_status ~ gear * depth * length * condition * in_water * in_air * delta_temp, data = logit, family = binomial)</code></p>

<p>I receive a warning message <code>""glm.fit: fitted probabilities numerically 0 or 1 occurred""</code>, along with missing coefficients due to singularities (NA or &lt;2e-16 <em>*</em>) when I use <code>summary</code>:</p>

<pre><code>Call:
glm(formula = qual_status ~ gear * depth * length * condition * 
    in_water * in_air * delta_temp, family = binomial, data = logit)

Deviance Residuals: 
  [1]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
 [36]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
 [71]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
[106]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0

Coefficients: (122 not defined because of singularities)
                                                            Estimate Std. Error    z value Pr(&gt;|z|)    
(Intercept)                                                1.419e+30  5.400e+22   26274077   &lt;2e-16 ***
gearsc                                                    -1.419e+30  5.400e+22  -26274077   &lt;2e-16 ***
depth                                                      1.396e+28  4.040e+20   34539471   &lt;2e-16 ***
length                                                     6.807e+28  1.836e+21   37079584   &lt;2e-16 ***
condition2                                                -3.229e+30  8.559e+22  -37727993   &lt;2e-16 ***
condition3                                                 1.747e+31  4.636e+23   37671986   &lt;2e-16 ***
condition4                                                 9.007e+31  2.388e+24   37724167   &lt;2e-16 ***
in_water                                                  -4.540e+28  1.263e+21  -35935748   &lt;2e-16 ***
in_air                                                    -4.429e+28  1.182e+21  -37470809   &lt;2e-16 ***
delta_temp                                                -1.778e+28  3.237e+21   -5492850   &lt;2e-16 ***
gearsc:depth                                              -1.396e+28  4.040e+20  -34539471   &lt;2e-16 ***
gearsc:length                                             -6.807e+28  1.836e+21  -37079584   &lt;2e-16 ***
depth:length                                              -9.293e+26  2.450e+19  -37930778   &lt;2e-16 ***
gearsc:condition2                                          1.348e+30  3.567e+22   37809001   &lt;2e-16 ***
gearsc:condition3                                          2.816e+30  7.495e+22   37575317   &lt;2e-16 ***
gearsc:condition4                                                 NA         NA         NA       NA    
</code></pre>

<p>Fitting only the continuous variables to a binary logistic regression doesn't yield any warnings or singularities but the addition of the ordinal predictor variables causes issues. Along with avoiding these warnings, is there a function/package that can handle dummy variables (I believe that is what I am looking for) in logistic regressions in <code>R</code>?</p>
"
"0.0430930413588572","0.0410650781176591","111555","<p>Imagine the following type of dataset: I have a dependent variable Y, two independent variables X and Z, and a variable that can separate the dataset in two smaller datasets.</p>

<p><img src=""http://i.stack.imgur.com/Tp2Qt.jpg"" alt=""enter image description here""></p>

<p>I estimated 2 identical OLS regressions for each group separately:</p>

<p><code>Alldata_A&lt;-subset(Alldata, group==""A"")</code></p>

<p><code>Alldata_B&lt;-subset(Alldata, group==""B"")</code></p>

<p><code>OLS_A &lt;- lm(Y~Z+X,Alldata_A)</code></p>

<p><code>OLS_B &lt;- lm(Y~Z+X,Alldata_B)</code></p>

<p>Now I am looking for a way to compare both models and to test whether the coefficients significantly differ for the two groups.</p>

<p>This shouldn't be difficult with an F-test, but I do not manage to get a result using R language.</p>

<p>I would be very grateful if someone can suggest which R-function to use for this!</p>
"
"0.0304713817668003","0.029037395206952","111559","<p>In R, when I have a (generalized) linear model (<code>lm</code>, <code>glm</code>, <code>gls</code>, <code>glmm</code>, ...), how can I test the coefficient (regression slope) against any other value than 0? In the summary of the model, t-test results of the coefficient are automatically reported, but only for comparison with 0. I want to compare it with another value.</p>

<p>I know I can use a trick with reparametrizing <code>y ~ x</code> as <code>y - T*x ~ x</code>, where <code>T</code> is the tested value, and run this reparametrized model, but I seek simpler solution, that would possibly work on the original model.</p>
"
"NaN","NaN","111738","<p>I'd like to modify the answer to <a href=""http://stats.stackexchange.com/questions/21565/how-do-i-fit-a-constrained-regression-in-r-so-that-coefficients-total-1"">this question</a> to allow weighted observations.  I think all I need to do is weight the inputs X and Y.  </p>

<pre><code>X = w * X
Y = w * Y
</code></pre>

<p>the other parts of the procedure should follow.  Please correct me if I am wrong.</p>
"
"0.152469724035564","0.145294482140594","112241","<p><strong>Summary:</strong> Is there any statistical theory to support the use of the $t$-distribution (with degrees of freedom based on the residual deviance) for tests of logistic regression coefficients, rather than the standard normal distribution?</p>

<hr>

<p>Some time ago I discovered that when fitting a logistic regression model in SAS PROC GLIMMIX, under the default settings, the logistic regression coefficients are tested using a $t$ distribution rather than the standard normal distribution.$^1$ That is, GLIMMIX reports a column with the ratio $\hat{\beta}_1/\sqrt{\text{var}(\hat{\beta}_1)}$ (which I will call $z$ in the rest of this question), but also reports a ""degrees of freedom"" column, as well as a $p$-value based on assuming a $t$ distribution for $z$ with degrees of freedom based on the residual deviance -- that is, degrees of freedom = total number of observations minus number of parameters. At the bottom of this question I provide some code and output in R and SAS for demonstration and comparison.$^2$</p>

<p>This confused me, since I thought that for generalized linear models such as logistic regression, there was no statistical theory to support the use of the $t$-distribution in this case. Instead I thought what we knew about this case was that</p>

<ul>
<li>$z$ is ""approximately"" normally distributed;</li>
<li>this approximation might be poor for small sample sizes;</li>
<li>nevertheless it <em>cannot</em> be assumed that $z$ has a $t$ distribution like we can assume in the case of normal regression.</li>
</ul>

<p>Now, on an intuitive level, it does seem reasonable to me that if $z$ is approximately normally distributed, it might in fact have some distribution that is basically ""$t$-like"", even if it is not exactly $t$. So the use of the $t$ distribution here does not seem crazy. But what I want to know is the following:</p>

<ol>
<li>Is there in fact statistical theory showing that $z$ really does follow a $t$ distribution in the case of logistic regression and/or other generalized linear models?</li>
<li>If there is no such theory, are there at least papers out there showing that assuming a $t$ distribution in this way works as well as, or maybe even better than, assuming a normal distribution?</li>
</ol>

<p>More generally, is there any actual support for what GLIMMIX is doing here other than the intuition that it is probably basically sensible?</p>

<p>R code:</p>

<pre><code>summary(glm(y ~ x, data=dat, family=binomial))
</code></pre>

<p>R output:</p>

<pre><code>Call:
glm(formula = y ~ x, family = binomial, data = dat)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.352  -1.243   1.025   1.068   1.156  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.22800    0.06725   3.390 0.000698 ***
x           -0.17966    0.10841  -1.657 0.097462 .  
---
  Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1235.6  on 899  degrees of freedom
Residual deviance: 1232.9  on 898  degrees of freedom
AIC: 1236.9

Number of Fisher Scoring iterations: 4
</code></pre>

<p>SAS code:</p>

<pre><code>proc glimmix data=logitDat;
    model y(event='1') = x / dist=binomial solution;
run;
</code></pre>

<p>SAS output (edited/abbreviated):</p>

<pre><code>The GLIMMIX Procedure

               Fit Statistics

-2 Log Likelihood            1232.87
AIC  (smaller is better)     1236.87
AICC (smaller is better)     1236.88
BIC  (smaller is better)     1246.47
CAIC (smaller is better)     1248.47
HQIC (smaller is better)     1240.54
Pearson Chi-Square            900.08
Pearson Chi-Square / DF         1.00


                       Parameter Estimates

                         Standard
Effect       Estimate       Error       DF    t Value    Pr &gt; |t|

Intercept      0.2280     0.06725      898       3.39      0.0007
x             -0.1797      0.1084      898      -1.66      0.0978
</code></pre>

<p>$^1$Actually I first noticed this about <em>mixed-effects</em> logistic regression models in PROC GLIMMIX, and later discovered that GLIMMIX also does this with ""vanilla"" logistic regression.</p>

<p>$^2$I do understand that in the example shown below, with 900 observations, the distinction here probably makes no practical difference. That is not really my point. This is just data that I quickly made up and chose 900 because it is a handsome number. However I do wonder a little about the practical differences with small sample sizes, e.g. $n$ &lt; 30.</p>
"
"0.121885527067201","0.116149580827808","112380","<p>I have a problem to build and to explain the linear multiple regression.</p>

<p>I have a data set called <code>Cars93</code> with 26 variables (numeric and not numeric) and 93 observations. This data set you can find in the <code>MASS</code> R package. I want to build a linear regression model for predicting the price of a car. Then I have to do a variable selection (forward and backward stepwise) using AIC and BIC in R.  My knowledge in R is too little thatÂ´s why I have some problems. I really hope you can help me! </p>

<p>1) The data set has some missing values </p>

<p>I just solved this problem like this:</p>

<pre><code> Cars93 [! Complete.cases (Cars93)] 
 Cars93new &lt;- na.omit (Cars93) 
 Cars93 = Cars93new 
</code></pre>

<p>I think some informations are going lost. Is there another solution to eliminate the missing values? </p>

<p>2) Some variables from the dataset are not numeric
I tried to convert these values into numerical values like this:</p>

<pre><code>Cars93 $ airbags = factor (Cars93 $ airbags, labels = c (2,1,0)) 
Cars93 $ airbags 
  [1] 0 2 1 2 1 1 1 1 1 1 2 0 1 2 0 1 2 2 1 0 1 1 1 1 0 2 0 0 0 1 1 1 1 0 1 1 2 2 
[39] 0 0 0 0 1 1 2 2 2 0 0 1 1 2 1 0 0 1 1 1 1 0 1 1 0 0 0 2 0 2 1 1 0 0 1 0 1 1 
[77] 1 0 0 0 1 2 
Levels: 2 1 0 
</code></pre>

<p>I did the same with other not numeric variables.</p>

<p>Afterwards I tried to build a linear model regression with all variables:</p>

<pre><code>Modell=lm(Price~Horsepower+EngineSize+MPG.city+MPG.highway+Rev.per.mile+Man.trans.avail+Fuel.tank.capacity+Passengers+Length+Wheelbase+Width+Turn.circle+Weight+Rear.seat.room+Luggage.room+Origin+AirBags+Type+Cylinders+Weight+PRM)
summary(Modell)
</code></pre>

<p>But the output does make any sense:</p>

<pre><code>Call:
lm(formula = Price ~ Horsepower + EngineSize + MPG.city + MPG.highway + 
    Rev.per.mile + Man.trans.avail + Fuel.tank.capacity + Passengers + 
    Length + Wheelbase + Width + Turn.circle + Weight + Rear.seat.room + 
    Luggage.room + Origin + AirBags + Type + Cylinders + Weight + 
    RPM)

Residuals:
    Min      1Q  Median      3Q     Max 
-9.4893 -2.3664 -0.0062  2.1180 18.1112 

Coefficients:
                    Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)        81.335018  37.993697   2.141 0.036826 *  
Horsepower          0.123535   0.049355   2.503 0.015372 *  
EngineSize         -0.615828   3.047223  -0.202 0.840602    
MPG.city           -0.392888   0.470385  -0.835 0.407259    
MPG.highway         0.013646   0.428978   0.032 0.974740    
Rev.per.mile        0.001498   0.002511   0.597 0.553206    
Man.trans.availYes -1.600967   2.480497  -0.645 0.521387    
Fuel.tank.capacity  0.462731   0.572169   0.809 0.422219    
Passengers          0.615593   1.823089   0.338 0.736925    
Length              0.074875   0.130511   0.574 0.568547    
Wheelbase           0.740146   0.343760   2.153 0.035796 *  
Width              -1.745792   0.571082  -3.057 0.003473 ** 
Turn.circle        -0.695287   0.415708  -1.673 0.100203    
Weight             -0.004068   0.006255  -0.650 0.518171    
Rear.seat.room      0.101150   0.420050   0.241 0.810619    
Luggage.room        0.176183   0.367199   0.480 0.633306    
Originnon-USA       1.881047   1.762845   1.067 0.290696    
AirBagsDriver only -3.294049   1.888346  -1.744 0.086777 .  
AirBagsNone        -8.535307   2.289737  -3.728 0.000464 ***
TypeLarge          -1.692122   3.999146  -0.423 0.673887    
TypeMidsize         2.684947   2.639047   1.017 0.313504    
TypeSmall           1.913341   2.896592   0.661 0.511710    
TypeSporty          4.686129   3.268426   1.434 0.157407    
Cylinders4         -3.126727   4.554852  -0.686 0.495360    
Cylinders5         -4.732933   7.498898  -0.631 0.530605    
Cylinders6          0.224795   5.695793   0.039 0.968664    
Cylinders8          4.020677   7.255406   0.554 0.581755    
RPM                -0.002778   0.002450  -1.134 0.261805    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 5.009 on 54 degrees of freedom
  (11 observations deleted due to missingness)
Multiple R-squared:  0.8313,    Adjusted R-squared:  0.747 
F-statistic: 9.859 on 27 and 54 DF,  p-value: 1.014e-12
</code></pre>

<p>I have 4 times ""Cylinders"" and ""Type"" and twice ""AirBag"" in the summary. I dont know why... And only 4 variables are significant in the model. Can somebody tell me, where is the mistake in my model?</p>

<p>I also would like to know how to test other assumptions in R for multiple linear model.</p>
"
"NaN","NaN","112442","<p>While building a regression model in R (<code>lm</code>), I am frequently getting this message</p>

<pre><code>""there are aliased coefficients in the model""
</code></pre>

<p>What exactly does it mean?</p>

<p>Also, due to this <code>predict()</code> is also giving a warning.</p>

<p>Though it's just a warning, I want to know how can we detect/remove aliased coefficients before building a model.</p>

<p>Also, what are the probable consequences of neglecting this warning?</p>
"
"0.0990319907421009","0.101630883224332","112481","<p>I am attempting to plot hurdle regression output with an interaction term in R, but I have some trouble doing so with the count portion of the model. </p>

<pre><code>Model &lt;- hurdle(Suicide. ~ Age + gender + bullying*support, dist = ""negbin"", link = ""logit"")
</code></pre>

<p>Since I am unaware of any packages that would allow me to plot the hurdle model in its entirety, I estimated each component separately (binomial logit and negative binomial count), using the <code>MASS</code> package, and I am attempting to plot the results. </p>

<p>So far, I have had the best luck using the <code>visreg</code> package for plotting, but I would like to hear other suggestions. I have been able to reproduce and successfully plot the original logistic output from the hurdle model in <code>MASS</code>, but not the negative binomial count data (i.e., the parameter estimates from <code>MASS</code> are not the same as they are in the hurdle regression output).</p>

<p>I would greatly appreciate any insight regarding how others have plotted hurdle regression results in the past, or how I might be able to reproduce negative binomial coefficients originally obtained from the hurdle model using <code>glm.nb()</code> in <code>MASS</code>. </p>

<p>Here is what I am using to plot my data:</p>

<pre><code>##Logistic 

logistic&lt;-glm(SuicideBinary ~ Age + gender + bullying*support, data = sx, family=""binomial"")

data(""sx"", package = ""MASS"")
##Linear scale
visreg(logistic, ""bullying"", by=""support"", xlab = ""bullying"", ylab = ""Log odds (Suicide yes/no)"")

##Logistic/probability scale
visreg(logistic, ""bullying"", by=""support"", scale = ""response"", xlab = ""bullying"", ylab = ""P(Initial Attempt)"")

##Count model

NegBin&lt;-glm.nb(Suicide. ~ Age + gender + bullying*support, data = sx)

data(""sx"", package = ""MASS"")
##Linear scale
visreg(NegBin, ""bullying"", by=""support"", xlab = ""bullying"", ylab = ""Count model (number of suicide attempts)"")

##Logistic/probability scale
visreg(NegBin, ""bullying"", by=""support"", scale = ""response"", xlab = ""bullying"", ylab = ""P(Subsequent Attempts)"")
</code></pre>
"
"0.0430930413588572","0.0410650781176591","112598","<p>I performed a polynomial regression using the following formula:</p>

<pre><code>lm(deviance ~ poly(myDF$distance,3,raw=T))
</code></pre>

<p>However, the summary output states that only the third term is significant:</p>

<pre><code>Coefficients:
                                     Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)                         -0.014825   0.095987  -0.154   0.8774  
poly(myDF$distance, 3, raw = T)1     0.031286   0.143283   0.218   0.8273  
poly(myDF$distance, 3, raw = T)2    -0.080363   0.065591  -1.225   0.2215  
poly(myDF$distance, 3, raw = T)3     0.021517   0.009377   2.295   0.0224 * 
</code></pre>

<p>How is this to be interpreted? My first guess is that only the full third-degree model (including the lower degree terms) fits the data significantly better than the null hypothesis. Is this correct? Put simply: Does the non-significance of the first and second degree terms impair the goodness of the model?</p>
"
"0.129458553897935","0.116873261230088","112670","<p>I'm working on a dataset with the following variables:</p>

<pre><code>Y:  a boolean telling whether the subject has experienced a seizure
ID:  the id of the subject
Sess:  the subject's session number (a subject has been observed multiple times)
X3:  numeric measurements of the subject's behavior during the session 
X4:  ""
X6:  ""
</code></pre>

<p>The idea is to see if the behavior measurements(X3,X4,X6) can be used predict the Seizure status (Y) in the population.  If there were just one session per subject I'd model the data with a logistic regression, but since each subject has multiple sessions the observations cannot be assumed independent.</p>

<p>It seems a GEE logistic model makes the most sense for this dataset, but I'm having trouble understanding the results when compared to a regular glm.  When introducing correlation structure to the GEE equation the signs of the coefficients are reversed, and the predicted values are opposite of what they would be with an independent correlation structure.</p>

<pre><code>library(""geepack"")
#regular logistic model
mod0 = glm(Y~X3+X4+X6, family=binomial(""logit""), data=mice)
#gee logistic model, independent correlation structure
mod1 = geeglm(Y~X3+X4+X6, id=ID, family=binomial(""logit""), corstr=""indep"", scale.fix=T, waves=Sess, data=mice)
#gee logistic model, exchangeable correlation structure
mod2 = geeglm(Y~X3+X4+X6, id=ID, family=binomial(""logit""), corstr=""exchangeable"", scale.fix=T, waves=Sess, data=mice)
</code></pre>

<p>As expected, the parameter estimates of the glm model(mod0) and the independent correlation gee model(mod1) are the same.  But when an exchangeable correlation structure(mod2) is introduced, the estimates are completely different and change sign.</p>

<pre><code>&gt; 
&gt; summary(mod1)                    

Call:
geeglm(formula = Y ~ X3 + X4 + X6, family = binomial(""logit""), 
    data = mice, id = ID, waves = Sess, corstr = ""indep"", scale.fix = T)

 Coefficients:
            Estimate Std.err  Wald Pr(&gt;|W|)    
(Intercept)   -1.494   0.459 10.59  0.00114 ** 
X3            -2.377   0.626 14.42  0.00015 ***
X4             1.090   0.398  7.50  0.00619 ** 
X6             1.233   0.403  9.36  0.00222 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Scale is fixed.

Correlation: Structure = independenceNumber of clusters:   28   Maximum cluster size: 4 
&gt; summary(mod2)

Call:
geeglm(formula = Y ~ X3 + X4 + X6, family = binomial(""logit""), 
    data = mice, id = ID, waves = Sess, corstr = ""exchangeable"", 
    scale.fix = T)

 Coefficients:
            Estimate Std.err Wald Pr(&gt;|W|)   
(Intercept)  -0.8928  0.4255 4.40   0.0359 * 
X3            0.1059  0.0383 7.64   0.0057 **
X4           -0.0427  0.0299 2.04   0.1535   
X6           -0.0528  0.0213 6.16   0.0131 * 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Scale is fixed.

Correlation: Structure = exchangeable  Link = identity 

Estimated Correlation Parameters:
      Estimate Std.err
alpha     1.09   0.133
Number of clusters:   28   Maximum cluster size: 4 
</code></pre>

<p>My biggest concern is that the predicted Y's of the two GEE models show opposite trends, i.e. behaviors that would predict a positive seizure status in mod1, predict a negative seizure status in mod2.    </p>

<pre><code>plot(mod1$fitted, mod2$fitted)
</code></pre>

<p><img src=""http://i.stack.imgur.com/nAogH.png"" alt=""scatter plot of fitted values from mod1, mod2""></p>

<p>Also, what's with the estimated correlation parameter of alpha = 1.09 in mod2?  Unless I'm interpreting this wrong, shouldn't this always fall between -1 and 1?</p>

<pre><code>Correlation: Structure = exchangeable  Link = identity 

Estimated Correlation Parameters:
      Estimate Std.err
alpha     1.09   0.133
</code></pre>

<p>It seems odd to me that the results are completely flipped depending on whether or not the observations are assumed to have dependence structure.  Can anyone else offer insight on this behavior?</p>

<p>Here is the data:</p>

<pre><code>&gt; dput(mice)
structure(list(Y = c(0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 
0L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L
), ID = c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 
5L, 5L, 6L, 6L, 6L, 7L, 7L, 8L, 8L, 9L, 9L, 10L, 10L, 11L, 11L, 
11L, 12L, 12L, 12L, 13L, 13L, 13L, 13L, 14L, 14L, 14L, 14L, 15L, 
15L, 15L, 15L, 16L, 16L, 16L, 16L, 17L, 17L, 17L, 18L, 18L, 18L, 
19L, 19L, 19L, 20L, 20L, 20L, 21L, 21L, 21L, 22L, 22L, 22L, 23L, 
23L, 23L, 23L, 24L, 24L, 24L, 24L, 25L, 25L, 25L, 25L, 26L, 26L, 
26L, 26L, 27L, 27L, 27L, 27L, 28L, 28L, 28L, 28L), Sess = c(2L, 
3L, 4L, 2L, 3L, 4L, 2L, 3L, 4L, 2L, 3L, 4L, 2L, 3L, 4L, 2L, 3L, 
4L, 3L, 4L, 3L, 4L, 3L, 4L, 3L, 4L, 1L, 3L, 4L, 1L, 3L, 4L, 1L, 
2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 2L, 
3L, 4L, 2L, 3L, 4L, 2L, 3L, 4L, 2L, 3L, 4L, 1L, 2L, 3L, 1L, 2L, 
3L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 
4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L), X3 = c(0.511015060794264, 
0.898356533693696, 0.798280430157052, 1.31144372617517, 0.829923189452201, 
0.289089506643144, -0.763028944257538, -0.944459588217789, -1.16474609928919, 
-0.182524267014845, -0.338967193889711, -0.896037509887988, 0.00426073081308205, 
0.0576592603165749, -1.4984737260339, 1.34752684212464, 0.106461438449047, 
-0.108424579472268, -2.85991432039569, -0.230115838261355, -1.54479536845993, 
-1.23693649938367, -1.53704616612456, -1.04825100254239, 0.142768659484482, 
0.28358135516745, -0.236302896321009, 0.708743856986942, -0.507503006972081, 
0.401550711842527, -0.16928449007327, 0.867816722958898, 0.487459858572122, 
1.35172112260613, 0.14742652989871, 0.742155288774287, 0.348552056119878, 
-0.82489952485408, 0.0366834636917457, -0.731010479377091, 0.979544093857171, 
1.4161996712129, 0.661035838980077, 0.600235250313596, -1.10872641912335, 
-0.212101744145196, -0.919575135240643, -0.813993077336991, -0.547068540188791, 
-0.0260198210967738, -0.0962240349391501, -0.251025721625606, 
0.894913664382802, -0.21993004239326, 0.0628839847717805, 1.77763503559622, 
0.718459471596243, 0.984412886705251, 0.77603470471174, 0.486187732642953, 
1.78012655684609, 1.31622243756713, 1.29635178661133, 0.427995111986702, 
0.993748401511881, 0.387623239882247, 0.42006794384777, -0.815889182132972, 
-0.897540332229183, -1.041943103505, 0.379425827374942, -1.00707718576756, 
-0.889182530787803, 0.148432805676879, -0.287928359114935, -0.747152636892815, 
-1.41003790431546, -0.611571256991109, -1.02569548477235, -1.02700056733181, 
-1.45808867127733, -1.47973458605138, 2.23643966561508, 2.69397876103083, 
0.81841473415516, 2.12167589051282, 0.267133799544379, -0.326215175076418, 
-1.08788244901967, -1.18733017947214), X4 = c(0.050598970482242, 
-0.0279694583060402, 0.999225143631274, 0.199872317584803, 0.779316284168575, 
-0.3552692229881, -0.232161792808608, -0.333479851296274, -0.748169603107953, 
-0.57785843363913, -0.480747933235349, -0.740466500603612, -0.618559437949564, 
-0.591541699294345, -0.538855647639331, 0.431376763414175, -0.327931008191724, 
-0.469416282917978, -0.659224551441466, -0.55285236403596, -0.637082867133913, 
-0.780321541069982, -0.40539035027884, -0.54024676972473, -0.185562290173831, 
0.054439450703482, 0.624097793456316, 0.24018937319873, -0.264194638773171, 
-0.389590537012038, -0.42771343162755, -0.738790918078674, -0.122411831542788, 
0.600119921164627, 0.0442597161778152, -0.0955011351192086, -0.521259643827527, 
-0.550050365103255, -0.504566887441653, -0.506571005423286, 0.523650149759566, 
0.341920916685254, -0.396343801985993, -0.366532239883921, -0.739276449002057, 
-0.56054127343218, -0.587601788901296, -0.56798329186843, -0.454937006653748, 
-0.672730639942183, -0.564864467446687, -0.678853515419629, 0.573072971483937, 
0.596973680548765, 0.0403978228634349, 1.93617633381248, 2.54301964691615, 
0.363075891004736, 0.0205658396444095, 0.560923287570261, 1.24212005971229, 
2.32518793880728, 2.69979166871713, 0.626868716830008, 0.219463581391793, 
0.236477261174534, -0.115429539698909, -0.49754151674106, -0.40827433350644, 
-0.0433283703798658, -0.578451015506926, -0.714208713291922, 
-0.802387726290423, -0.836794085697031, -0.471800405613954, -0.668030208971065, 
-0.610945789491312, -0.780838257914176, -0.411360572155088, -0.494388869332376, 
-0.63231547268951, -0.743853022088574, 4.90627675753856, 3.38455016460328, 
0.859445571488139, 2.42212262705776, -0.324759764820016, -0.541581784452693, 
-0.485324968098865, -0.770539730529603), X6 = c(0.0150287583709043, 
-0.151984283645294, -0.347950002037732, 0.379891135882966, 0.129107019894704, 
-0.314047917638528, -0.516381047940779, -0.751192211830495, -0.884460389494645, 
-0.462363867892961, -0.397583161539858, -0.559528880497725, -0.842987555132397, 
-0.922797893301111, -1.01175722882932, 0.32346425626624, -0.610909601293237, 
-0.605155952259822, -1.29840867980623, 0.0793710626694382, -0.806959976634144, 
-0.674523251142452, -0.960113466801064, -0.783836535852452, -0.0665321645536412, 
0.482235339656537, -0.319499220427413, -0.115345965733089, -0.30806448545927, 
0.251747727063608, -0.305013811851957, -0.931916656036151, 0.415032839884745, 
0.337184728843034, 0.0584335852357015, -0.0712185313438638, 0.78632612201797, 
0.490831043388539, 0.8902425262631, 0.160088439571744, 0.90343086944952, 
0.928495373121098, -0.389259569427933, -0.304578433259833, -0.593364448723133, 
-0.411333868741105, -0.882691663964141, -0.91208274239495, -0.708633954450382, 
-0.339396626779965, -0.420927315080057, -0.421383857909298, 0.407474183771483, 
0.629710767351175, -0.438726438495567, 2.40977730548689, 2.47250810430208, 
0.783562677342961, 0.781304150896319, 0.563221804716475, 1.85514126067038, 
1.30723846671955, 1.94869625911545, 0.876751836832149, 0.626629859119409, 
0.067113945916172, 3.54280776301513, 0.0082773667305384, -0.311414481848668, 
-0.732779325538588, -0.594477082903005, -1.0385239418576, -1.04141739541776, 
-0.99472304141247, -0.599659297534257, -0.804801224448196, -1.13096932958525, 
-0.641957537144073, -0.722959119516237, -0.671146043591047, -0.714432955420477, 
-0.766750949574034, 1.20993739830475, 2.79011376379402, 2.64532317075082, 
2.54251033029822, -0.539516582572252, -0.6419726544563, -0.663768795224503, 
-0.644829826467113)), .Names = c(""Y"", ""ID"", ""Sess"", ""X3"", ""X4"", 
""X6""), row.names = c(NA, -90L), class = ""data.frame"")
</code></pre>
"
"0.114013470672957","0.108647984268766","112760","<p>I have used <code>mlogit</code> package and I am trying to summarize the results I have from my model.  I have a question regarding the reference value and will get to that in a moment.</p>

<pre><code>redata.full &lt;- mlogit(no.C~ 1| WR+age+age2+BP+noC.1yr, data=redata, reflevel=""0"", na.action=na.fail)

no.C = number of offspring    
WR = risk
age+age2 = the non-linear relationship that as an individual ages their production decreases
BP = browsing pressure
noC.1yr = number of offspring produced the year before
</code></pre>

<p>I recognize that my data is ordinal in nature, but Im following other people's methods who have done this and used the reference based approach rather than ordinal logistic regression.  However, I am still shakey on justification other than citing the other person and saying ""he did it too!""  If anyone has a suggestion I would appreciate it.</p>

<p>My results for this model are: </p>

<pre><code>Call:
mlogit(formula = no.C ~ 1 | WR + age + age2 + BP + noC.1yr, data = redata, 
    na.action = na.fail, reflevel = ""0"", method = ""nr"", print.level = 0)

Frequencies of alternatives:
       0        1        2 
0.233766 0.675325 0.090909 

nr method
5 iterations, 0h:0m:0s 
g'(-H)^-1g = 2.16E-07 
gradient close to zero 

Coefficients :
               Estimate Std. Error t-value Pr(&gt;|t|)  
1:(intercept) -0.281226   1.225763 -0.2294  0.81854  
2:(intercept) -0.605312   1.997179 -0.3031  0.76183  
1:WR           0.847273   0.518854  1.6330  0.10248  
2:WR           1.347976   0.689916  1.9538  0.05072 .
1:age          0.314075   0.275486  1.1401  0.25425  
2:age         -0.422368   0.395240 -1.0686  0.28523  
1:age2        -0.018998   0.014446 -1.3151  0.18847  
2:age2         0.022572   0.018949  1.1912  0.23359  
1:BP          -0.143720   0.173585 -0.8280  0.40770  
2:BP          -0.074553   0.331108 -0.2252  0.82185  
1:noC.1yr      0.574304   0.377821  1.5200  0.12850  
2:noC.1yr      1.251673   0.626033  1.9994  0.04557 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Log-Likelihood: -116.6
McFadden R^2:  0.079844 
Likelihood ratio test : chisq = 20.236 (p.value = 0.0271)

exp(cbind(OddsRatio = coef(redata.full), ci))
              OddsRatio      2.5 %    97.5 %
1:(intercept) 0.7548580 0.06831155  8.341351
2:(intercept) 0.5459038 0.01089217 27.360107
1:WR          2.3332750 0.84394900  6.450831
2:WR          3.8496270 0.99577472 14.882511
1:age         1.3689929 0.79782462  2.349065
2:age         0.6554925 0.30209181  1.422317
1:age2        0.9811815 0.95379086  1.009359
2:age2        1.0228284 0.98553735  1.061530
1:BP          0.8661299 0.61634947  1.217136
2:BP          0.9281585 0.48504538  1.776078
1:noC.1yr     1.7758933 0.84686698  3.724076
2:noC.1yr     3.4961862 1.02497823 11.925441
</code></pre>

<p>I would like confirmation of my interpretations:
The model is better than a null - obtained from the likelihood ratio test.</p>

<p>Question: How do I test how well the model is actually working (i.e., goodness of fit)?  Hosmer-Lemshow test? Ive read warnings about using the McFaddin's Pseudo R where they really aren't applicable to multinomial regressions.  Ive found a HL test with <code>ResourceSelection</code> library and it says my model is NOT doing well at all.  Now what?</p>

<p>Interpretation:
WR and noC.1yr are the only variables that are coming out as slightly significant.  But this is only between the reference value of 0 and production of 2 calves.  It is not significantly different between 0 or 1 for these variables.  </p>

<p>Question: Ive been trying to find somewhere in the vignette what the t-value is - it is just a t-test?  How would I refer to the estimate as being significant?  ""The estimated odds for 2-offspring being produced versus 0 were 3.85 (95% CI = 1.0-14.88) which was significant (t= 1.99, P=0.05)""</p>

<p>Referring to my statement regarding setting the reference value.  When I run this exact same model using my other options of 0 or 1 offspring - I get completely different results of which variables are significant.  If I use 2 as the reference value then Age+WR+noC.yr are significant.  If I use 1, then Age only is sig.  So, which one to use?  I have read you want to pick one that is most relevant to your hypothesis, but in this case I could motivate any of the 3 levels.  </p>
"
"0.068136080998913","0.0649295895722714","112860","<p>I am doing a time series regression between 2 variables. I used the dynlm library in R. I'm trying to understand how to interpret the results. </p>

<p>Could you please point out where I am getting it wrong: </p>

<p>1) The R squared seems very low -- this indicates a weak linear relationship. Or too many outliers. 
2) Normal QQ plot shows that there are a significant number of outliers -- at a later date in the time series? 
3) Does the 'Residuals vs Fitted' plot show that the model is a pretty good fit for most of the data, except for those outliers? </p>

<p>Any suggested readings (esp open-sourced materials available online) also appreciated. </p>

<hr>

<pre><code>Call:
dynlm(formula = P ~ L(B))
Residuals:
    Min      1Q  Median      3Q     Max 
-63.711 -27.687 -14.907   2.364 146.157 
Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 17.485051  13.500833   1.295    0.197     
L(B)         0.019422   0.002384   8.146 5.84e-14
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
Residual standard error: 48.17 on 182 degrees of freedom
Multiple R-squared:  0.2672,    Adjusted R-squared:  0.2632 
F-statistic: 66.36 on 1 and 182 DF,  p-value: 5.838e-14
</code></pre>

<p><img src=""http://i.stack.imgur.com/xJbHF.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/6cpT2.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/JZbmX.png"" alt=""enter image description here""></p>
"
"0.0806196982594614","0.076825726438694","112997","<p>I am new to modelling percentage data, and I would be greatfull for some advice. I have proportion data (0,1] on a percentage of money sent by Player B to Player A. Participants received an amount of money, and could decide what percentage they will send back. I have two categorical predictors (1<sup>st</sup> with 3 factors, 2<sup>nd</sup> with 2); one continuous predictor; and one nesting factor (class). Since the data are bound between 0 and 1, I figured out that the best option would be Beta regression. I tried to use <code>hglm</code> package which fitted well, however, since the data are one-inflated (many people chose to send back the full amount), I am looking for other options.</p>

<p>As most appropriate seem to be <code>gamlss</code> package, which can use BEOI (Beta One Inflated) distribution. I used this code:</p>

<pre><code>m1 &lt;- gamlss(percent~cat1+cat2+continous, random(class), family=BEOI, data=dat, 
             mixture=""gq"", K=1)
</code></pre>

<p>From what I understand from package help files, this should be the simplest option. However, it produces very different results from the <code>hglm</code>command. Especially the standard errors are higher than beta coefficients, leading to non-significant results. I tried to specify other other functions in the model (e.g., <code>K, sigma.formula, nu.formula, mixture</code> etc.), but these are beyond my understanding, and I am not really sure what I did there.</p>

<p>I would very much appreciate any suggestions regarding either how to better specify the model, or simple explanations of <code>gamlss</code> function.</p>
"
"0.0609427635336005","0.0580747904139041","113251","<p>In my design, I have two groups of subjects and every subject is tested in four different conditions. So, I have a within-subject factor ('span_num', which ranges from 0 to 3) and a between-subject factor (group, which can be 'Linear' or 'U-shape').</p>

<p>My goal is to show that the slope between the spans (from 0 to 3) is higher in the Linear group than in the U-shape group.</p>

<p>The slopes look very different and are significantly different when I compare the regression models I get when I ignore that my within-factor is a within-factor and treat it as a between-factor.</p>

<p>I compared the slopes like this (but I don't trust the comparison because I don't trust the SE values of the slopes because I am treating my within-factor like a between-factor):</p>

<pre><code>linear_lm &lt;- lm(RT ~ span_num, dat = data[data$group == ""Linear"",])
ushape_lm &lt;- lm(RT ~ span_num, dat = data[data$group == ""U-shape"",])
linear_intercept &lt;- summary(linear_lm)$coefficients[[1]]; linear_ise &lt;- summary(linear_lm)$coefficients[[3]]; linear_slope &lt;- summary(linear_lm)$coefficients[[2]]; linear_sse &lt;- summary(linear_lm)$coefficients[[4]]
ushape_intercept &lt;- summary(ushape_lm)$coefficients[[1]]; ushape_ise &lt;- summary(ushape_lm)$coefficients[[3]]; ushape_slope &lt;- summary(ushape_lm)$coefficients[[2]]; ushape_sse &lt;- summary(ushape_lm)$coefficients[[4]]

z_intercept &lt;- (ushape_intercept - linear_intercept) / sqrt(linear_ise^2 + ushape_ise^2)   #z = -0.45; p = .67, n.s.
z_slope &lt;- (ushape_slope - linear_slope) / sqrt(linear_sse^2 + ushape_sse^2)   #z = -1.50; p = .93, sig.
</code></pre>
"
"0.068136080998913","0.0519436716578171","113502","<p>I am trying to run a regression using about 80 independent variables. The problem is that the last 20+ coefficients return NA. If I condense the range of data to within 60, I get coefficients for everything just fine. Why am I getting these NAs and how do I resolve it? I need to reproduce this code using all of these variables.</p>

<pre><code>composite &lt;- read.csv(""file.csv"", header = T, stringsAsFactors = FALSE)
composite &lt;- subset(composite, select = -ncol(composite))
composite &lt;- subset(composite, select = -Date)
model1 &lt;- lm(indepvariable ~., data = composite, na.action = na.exclude)
</code></pre>

<p>composite is a data frame with 82 variables.</p>

<p>UPDATE:</p>

<p>What I have done is found a way to create an object that contains only the significantly correlated variables, to narrow the number of independent variables down. </p>

<p>I have a variable now: sigvars, which is the names of an object that sorted a correlation matrix and picked out only the variables with correlation coefficients >0.5 and &lt;-0.5. Here is the code:</p>

<pre><code>sortedcor &lt;- sort(cor(composite)[,1])
regvar = NULL

k = 1
for(i in 1:length(sortedcor)){
  if(sortedcor[i] &gt; .5 | sortedcor[i] &lt; -.5){
    regvar[k] = i
  k = k+1
 }
}
regvar

sigvars &lt;- names(sortedcor[regvar])
</code></pre>

<p>However, it is not working in my lm() function:</p>

<pre><code>model1 &lt;- lm(data.matrix(composite[1]) ~ sigvars, data = composite)
</code></pre>

<p>Error: Error in model.frame.default(formula = data.matrix(composite[1]) ~ sigvars,  : 
  variable lengths differ (found for 'sigvars')</p>
"
"0.0861860827177143","0.0821301562353182","113598","<p>This question is about three-way interaction and the possibility of applying without second lower terms with keeping the main variables in the equation not like the other questions. In fact the other answers suggest there is possibility of applying . I am not here to find the best solution because I know it and I already included in my question, but to know whether is it possible regardless if it is preferable or not. thank you and please open my question for discussion </p>

<p>The widely known regression equation for assessing the three-way interaction is</p>

<p>$$ Y= B_1 X+B_2 Z+B_3 W +B_4XZ+B_5XW+B_6ZW+B_7XZW+B_0 $$</p>

<p>All lower order terms is included in the regression  equation for the B7 coefficient  to
represent  the  effect  of the  three-way  interaction  on  Y. </p>

<p>Is there possible way to skip the lower order terms and include only the higher term? as in:</p>

<p>$$ Y= B_1 X+B_2 Z+B_3 W +B_4XZW+B_0 $$</p>

<p>And how many observations do I need to perform such equation if X &amp; Z are continuous variables and W is dummy variable ?</p>

<p>I will be thankful if anyone can provide me with any suggestions </p>
"
"0.0826872055888527","0.0787959366469562","113951","<p>I have a generic question about whether it might sometimes make sense to fix specific regression coefficients to predetermined values. And if this makes sense in particular cases, how do you best go about it?</p>

<p>In my case, I have about 1,600 observations but I am interested chiefly in a variable for which about 600 observations are missing. If I run a normal regression (OLS,GLS,CLM) all the variables that have missing values are dropped, and I am wondering whether it is possible to ""save"" all the observations to determine the coefficients for the variables for which I have full information, and then run a separate regression to determine the coefficients of the variables with all the missing values.</p>

<p>In simple formulas it looks about like this:</p>

<h1>Step 1</h1>

<p>For variables <code>x1,x2,x3,x4</code> I have 1,600 observations</p>

<pre><code>glm.core &lt;- glm(y ~ x1 + x2 + x3 + x4) # determine the coefficients in the regression
beta &lt;- glm.core$coef
</code></pre>

<h1>Step 2</h1>

<p>For variables <code>z1</code> and <code>z2</code> I only have a 1,000 observations</p>

<pre><code>glm.main &lt;- glm(y ~ beta[1]*x1 + beta[2]*x2 + beta[3]*x3 + beta[4]*x4 + z1 + z2)
</code></pre>

<p>So I want to predetermine the betas for which I have full information and then fix their values in the main regression. (Maybe this is a completely pointless idea and if so please tell me)</p>

<p>I know something like that can be achieved with <code>offset</code> BUT this does not work for factor variables and <code>x2</code> and <code>x3</code> are factors (<code>x2</code> has values 'university', 'firm', 'government', and <code>x3</code> has values 'known' , 'unknown', 'not relevant').</p>

<ol>
<li><p>Is an alternative to solving this problem to reduce the response variable <code>y</code> with the fitted values of the <code>glm.core</code> model and run the regression like that?</p></li>
<li><p>If this is a sensible option</p>

<ul>
<li>What happens to the errors?</li>
<li>How to calculate degrees of freedom?</li>
<li>Would this work as well with ordered logit (CLM) models where the fitted values are percentages?</li>
</ul></li>
</ol>

<p>Are there better ways of dealing with this problem?</p>

<p>PS: I am using R software, forgot to mention this initially...</p>
"
"0.052777981396926","0.0502942438178979","114221","<p>I plotted normal probability plot in R using <code>qqnorm</code> and <code>qqline</code>. I want some help on:</p>

<ol>
<li>How to estimate ""probability"" that a data has normal distribution? (I read in a paper that a probability of 0.10 is required to assume that a data is normally distributed). </li>
<li>Also, how to calculate correlation coefficient for a normal probability plot in R?</li>
<li>Is the normality test valid for nonlinear regression too? (This might be a silly question, excuse me for that!)</li>
</ol>

<p>Thanks!</p>
"
"0.101414888671788","0.104695817324578","114468","<p>I am using the metafor package in R. I have fit a random effects model with a continuous predictor as follows </p>

<pre><code>SIZE=rma(yi=Ds,sei=SE,data=VPPOOLed,mods=~SIZE)
</code></pre>

<p>Which yields the output:</p>

<pre><code>R^2 (amount of heterogeneity accounted for):            63.62%
Test of Moderators (coefficient(s) 2): 
QM(df = 1) = 9.3255, p-val = 0.0023

Model Results:

                 se    zval    pval   ci.lb   ci.ub    
intrcpt  0.3266  0.1030  3.1721  0.0015  0.1248  0.5285  **
SIZE     0.0481  0.0157  3.0538  0.0023  0.0172  0.0790  **
</code></pre>

<p>Below I have plotted the regression.The effect sizes are plotted proportionally to the inverse of the standard error. I realize that this is a subjective statement, but the R2 (63% variance explained) value seems a lot larger than is reflected by the modest relationship shown in the plot (even taking weights into account).</p>

<p><img src=""http://i.stack.imgur.com/3JNmM.jpg"" alt=""enter image description here""></p>

<p>To show you what I mean, If I then do the same regression with the lm function (specifying study weights in the same way):</p>

<pre><code>lmod=lm(Ds~SIZE,weights=1/SE,data=VPPOOLed)
</code></pre>

<p>Then the R2 drops to 28% variance explained. This seems closer to the way things are (or at least, my impression of what kind of R2 should correspond to the plot). </p>

<p>I realize, after having read this article (including the meta-regression section): (<a href=""http://www.metafor-project.org/doku.php/tips:rma_vs_lm_and_lme"">http://www.metafor-project.org/doku.php/tips:rma_vs_lm_and_lme</a>), that differences in the way the lm and rma functions apply weights can influence the model coefficients. However, it is still unclear to me why the R2 values are so much larger in the case of meta-regression. Why does a model that looks to have a modest fit account for over half the heterogeneity in effects?</p>

<p>Is the larger R2 value because the variance is partitioned differently in the meta analytic case? (sampling variability v other sources) Specifically, does the R2 reflect the percent of heterogeneity accounted for <em>within the portion that cant be attributed to sampling variability</em>?. Perhaps there is a difference between ""variance"" in a non-meta-analytic regression and ""heterogeneity"" in a meta-analytic regression that I am not appreciating.</p>

<p>I'm afraid subjective statements like ""It doesn't seem right"" are all I have to go on here. Any help with interpreting R2 in the meta-regression case would be much appreciated. </p>
"
"0.0304713817668003","0.029037395206952","114583","<p>I want to simulate a binary response variable which depends on two normally distributed continuous variables, and I want to have more 1s than 0s in the response variable. I wonder how this can be done such that a logistic regression will not identify a significant interaction term.</p>

<p>My current approach in R looks like this:</p>

<pre><code>n = 1e5
x1 = rnorm(n)
x2 = rnorm(n)
y = x1+x2+rnorm(n)
y = ifelse(y &gt; 2, 1, 0)
df=data.frame(x1=x1, x2=x2, y=y)
summary(glm(y ~ x1*x2, df, family=binomial(logit)))$coefficients
</code></pre>

<p>This usually results in a highly significant interaction term, even though the y is just the sum of x1 and x2.
So how can I simulate a y which depends on both x1 and x2, but not on their interaction?</p>
"
"0.0914141453004008","0.0871121856208561","114675","<p>I really want to understand how the math is working here. I am trying to get the standard error of the fitted values for a time series regression model. In the non-time series regression, I know I can take the transpose of the data multiplied by the variance - covariance matrix of the model coefficients and then multiply by the data values again to get the standard errors of the fitted values.</p>

<p>But I'm not sure how to do this when I am including an autoregressive term.</p>

<pre><code>require(forecast)
require(tserieS)
</code></pre>

<p>Response variable</p>

<pre><code>Sablects &lt;- rnorm(10)
</code></pre>

<p>Covariates</p>

<pre><code>my.xreg &lt;- cbind(rnorm(10),rbinom(10,1,0.5))
</code></pre>

<p>In my actual data, values are normalized so I set the intercept equal to zero here.</p>

<pre><code>m4&lt;-arima(Sablects, order=c(2,0,0),fixed=c(0,NA,0,NA,NA),xreg=my.xreg) 
</code></pre>

<p>The predict function will give me standard errors on my in-sample prediction (the fitted values of my model).</p>

<pre><code>my.se &lt;- predict(m4, newxreg = my.xreg, n.ahead = 10)$se         

my.se
</code></pre>

<p>Now to compare the output of my.se, I want to do this mathematically but I don't know what to use for the values of the ar2 term. I use 1's as a placeholder to demonstrate that my output does not equal the values from <code>my.se</code> above</p>

<pre><code>C &lt;- cbind(rep(1, nrow(my.xreg)), my.xreg[, 1], my.xreg[, 2])

C
</code></pre>

<p>I think this value should equal the first value in my.se, but is not producing the same value as my.se</p>

<pre><code>sqrt(t(C[1, ]) %*% vcov(m4) %*% C[1, ])
</code></pre>

<p>Also, I'm not so great with matrix multiplication but here is my work around for getting all of the se values.</p>

<pre><code>se.output &lt;- matrix(nrow=nrow(C))
</code></pre>

<p>Specify that the max number of i is equal to number of rows of <code>C</code>.</p>

<pre><code>  for(i in 1:nrow(C)){

    # Loop through your multiplication for each row (i) of `C`. For each iteration, save the new data into the new row of se.output

    se.output[i] &lt;- sqrt(t(C[i, ]) %*% vcov(m4) %*% C[i, ])  
    }

se.output
</code></pre>
"
"0.114013470672957","0.108647984268766","114728","<p>I'm trying to run a QAP logistic regression to predict the odds of a tie in a social network (represented as a binary adjacency matrix) given two independent variables (also binary matrices) but am getting opposite results depending on whether I run the analysis in R or UCINET.</p>

<p>All three matrices are rectangular (30 x 75). The 30 rows are people I've interviewed and the 75 columns are the entire population (including the 30 interviewees). All matrices include the person IDs as row and column names.</p>

<p>Running the analysis in R (see code at the bottom of the question), I get the following output:</p>

<pre><code>            Estimate  Exp(b)       Pr(&lt;=b) Pr(&gt;=b) Pr(&gt;=|b|)
(intercept) -5.298525  0.004998961 0.0001  0.9999  0.0001   
indep1       1.797138  6.032358591 0.9693  0.0307  0.2393   
indep2       3.194162 24.389724184 1.0000  0.0000  0.0030   
</code></pre>

<p>But after exporting the variables to .csv files and re-running it in UCINET, I get:</p>

<pre><code>                  1       2       3       4       5       6       7       8       9 
               Coef OddsRat     Sig      SD     Avg     Min     Max   P(ge)   P(le) 
            ------- ------- ------- ------- ------- ------- ------- ------- ------- 
1 Intercept  -3.931   0.020   0.000   0.540  -2.785  -3.931  -1.783       1   0.000 
2    indep1   2.391  10.925   0.003   1.508  -0.239  -5.089  15.437   0.003   0.998 
3    indep2   1.458   4.296   0.000   0.504  -0.026 -15.991   1.458   0.000       1 
</code></pre>

<p>Any ideas why this might be happening?</p>

<p>In case it's important, the (QAP) correlation coefficient between the two independent variables is 0.382</p>

<p>I've only included the 30 interviewees in the matrix rows because they are the only people from whom there might be a tie. The networks and QAP regressions are directed.</p>

<p>Incidentally, if I run the QAP logit using full 75x75 adjacency matrices (all people in the columns also appear as rows), I get the same output in both programs.</p>

<p>I also have a related question... A colleague suggested I could run the analysis using the 75x75 matrices but replace the rows of people I haven't interviewed with NAs. This gives me the same results in R and UCINET. Does this seem like a sensible approach, rather than using rectangular matrices?</p>

<p>Thanks!</p>

<p>Reproducible example in R:</p>

<pre><code>library(sna)

# row and column labels
rowIDs &lt;- c(""1"",  ""2"",  ""3"",  ""5"",  ""9"",  ""16"", ""18"", ""19"", ""26"", ""27"", ""34"", ""35"", ""36"", ""40"", ""46"", ""49"", ""60"", ""64"", ""70"", ""72"", ""73"", ""74"", ""75"", ""76"", ""77"", ""82"", ""85"", ""86"", ""97"", ""100"")
colIDs &lt;- c(""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", ""12"", ""13"", ""14"", ""15"", ""16"", ""17"", ""18"", ""19"", ""20"", ""21"", ""23"", ""26"", ""27"", ""34"", ""35"", ""36"", ""38"", ""40"", ""41"", ""43"", ""45"", ""46"", ""47"", ""49"", ""51"", ""52"", ""53"", ""57"", ""59"", ""60"", ""61"", ""62"", ""63"", ""64"", ""65"", ""66"", ""67"", ""68"", ""70"", ""72"", ""73"", ""74"", ""75"", ""76"", ""77"", ""78"", ""79"", ""81"", ""82"", ""83"", ""84"", ""85"", ""86"", ""87"", ""88"", ""89"", ""91"", ""92"", ""93"", ""94"", ""95"", ""96"", ""97"", ""100"", ""101"")

# create matrices
adj.dep &lt;- as.matrix( read.csv(header=F, text=
               ""0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0""))

adj.indep1 &lt;- as.matrix( read.csv(header=F, text=
               ""0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1""))

adj.indep2 &lt;- as.matrix( read.csv(header=F, text=
               ""0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0
                1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0
                0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1""))

# assign row/column names
rownames(adj.dep) &lt;- rowIDs
colnames(adj.dep) &lt;- colIDs
rownames(adj.indep1) &lt;- rowIDs
colnames(adj.indep1) &lt;- colIDs
rownames(adj.indep2) &lt;- rowIDs
colnames(adj.indep2) &lt;- colIDs

# set up independent variables
g.indeps &lt;- array(dim=c(2, nrow(adj.indep1), ncol(adj.indep1)))
g.indeps[1,,] &lt;- adj.indep1
g.indeps[2,,] &lt;- adj.indep2

# run the analysis
# (warning, this command takes a bit of time to run with 10,000 reps)
nl &lt;- netlogit(adj.dep, g.indeps, reps=10000, nullhyp=""qap"")
# print output
summary(nl)
</code></pre>
"
"0.175331250648991","0.171853861176301","115065","<p>I am working with data from a computer task which has 288 total trials, each of which can be categorically classified according to <strong>Trial Type</strong>, <strong>Number of Stimuli</strong>, and <strong>Probe Location</strong>.  Because I want to also examine a continuous variable, the total Cartesian <strong>Distance</strong> between stimuli per trial (divided by number of stimuli to control for varying numbers), I have opted to use a mixed linear model with repeated measures.  In addition to each of these task variables, I am also interested in whether folks in various diagnostic groups perform differently on the task, as well as whether or not there is a <strong>Dx</strong> interaction with any of the above variables.  Thus (if I'm not mistaken), I have the following effects in my model:</p>

<p><strong>Trial Type</strong>, a fixed effect
<strong>Number of Stimuli</strong>, a fixed effect
<strong>Probe Location</strong>, a fixed effect
<strong>Dist</strong>(ance), a fixed effect
<strong>Dx</strong>, a fixed effect
<strong>Dx*Trial Type</strong>, a fixed effect
<strong>Dx*Number of Stimuli</strong>, a fixed effect
<strong>Dx*Probe Location</strong>, a fixed effect
<strong>Dx*Dist</strong>, a fixed effect
<strong>Trial</strong>, a random effect, nested within
<strong>SubID</strong>, a random effect</p>

<p>Based on my examination of documentation, it seems that the nesting of random effects does not seem to be important to lme4, and so I specify my model as follows:</p>

<p><code>tab.lmer &lt;- lmer(Correct ~  Dx+No_of_Stim+Trial_Type+Probe_Loc+Dist+Dx*No_of_Stim+Dx*Trial_Type+Dx*Probe_Loc+Dx*Dist+(1|Trial)+(1|SubID),data=bigdf)</code></p>

<p>This would be my first question: <strong>1) Is the above model specification correct?</strong></p>

<p>Assuming so, I am a bit troubled by my results, but as I read and recall my instruction on such models, I am wondering if interpretation of particular coefficients is bad practice in this case:</p>

<pre><code>Linear mixed model fit by REML ['merModLmerTest']
Formula: Correct ~ Dx + No_of_Stim + Trial_Type + Probe_Loc + Dist + Dx *  
    No_of_Stim + Dx * Trial_Type + Dx * Probe_Loc + Dx * Dist +  
    (1 | Trial) + (1 | SubID)
   Data: bigdf

REML criterion at convergence: 13600.4

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-2.89810 -0.03306  0.27004  0.55363  2.81656 

Random effects:
 Groups   Name        Variance Std.Dev.
 Trial    (Intercept) 0.013256 0.11513 
 SubID    (Intercept) 0.006299 0.07937 
 Residual             0.131522 0.36266 
Number of obs: 15840, groups:  Trial, 288; SubID, 55

Fixed effects:
                         Estimate Std. Error         df t value Pr(&gt;|t|)    
(Intercept)             4.196e-01  4.229e-02  4.570e+02   9.922  &lt; 2e-16 ***
DxPROBAND               8.662e-02  4.330e-02  2.920e+02   2.000  0.04640 *  
DxRELATIVE              9.917e-02  4.009e-02  2.920e+02   2.474  0.01394 *  
No_of_Stim3            -9.281e-02  1.999e-02  4.520e+02  -4.642 4.53e-06 ***
Trial_Type1             3.656e-02  2.020e-02  4.520e+02   1.810  0.07097 .  
Probe_Loc1              3.502e-01  2.266e-02  4.520e+02  15.456  &lt; 2e-16 ***
Probe_Loc2              3.535e-01  3.110e-02  4.520e+02  11.369  &lt; 2e-16 ***
Dist                    1.817e-01  2.794e-02  4.520e+02   6.505 2.06e-10 ***
DxPROBAND:No_of_Stim3  -1.744e-02  1.759e-02  1.548e+04  -0.992  0.32144    
DxRELATIVE:No_of_Stim3 -2.886e-02  1.628e-02  1.548e+04  -1.773  0.07628 .  
DxPROBAND:Trial_Type1  -9.250e-03  1.777e-02  1.548e+04  -0.521  0.60267    
DxRELATIVE:Trial_Type1  1.336e-02  1.645e-02  1.548e+04   0.812  0.41682    
DxPROBAND:Probe_Loc1   -8.696e-02  1.993e-02  1.548e+04  -4.363 1.29e-05 ***
DxRELATIVE:Probe_Loc1  -4.287e-02  1.845e-02  1.548e+04  -2.323  0.02018 *  
DxPROBAND:Probe_Loc2   -1.389e-01  2.735e-02  1.548e+04  -5.079 3.83e-07 ***
DxRELATIVE:Probe_Loc2  -8.036e-02  2.532e-02  1.548e+04  -3.173  0.00151 ** 
DxPROBAND:Dist         -3.920e-02  2.457e-02  1.548e+04  -1.595  0.11066    
DxRELATIVE:Dist        -1.485e-02  2.275e-02  1.548e+04  -0.653  0.51390    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>In general, these results make sense to me.  The troubling portion, however, comes in the positive, significant (yes, I am using LmerTest) p-value for DxProband, particularly in light of the fact that in terms of performance means, Probands are performing worse than Controls.  So, this mismatch concerns me.  Examining the corresponding ANOVA:</p>

<pre><code>&gt; anova(tab.lmer)
Analysis of Variance Table of type 3  with  Satterthwaite 
approximation for degrees of freedom
               Sum Sq Mean Sq NumDF   DenDF F.value    Pr(&gt;F)    
Dx             0.8615  0.4308     2   159.0   1.412   0.24662    
No_of_Stim     0.6984  0.6984     1   283.5  37.043 3.741e-09 ***
Trial_Type     8.3413  8.3413     1   283.5   4.456   0.03565 *  
Probe_Loc     25.7223 12.8612     2   283.5 116.405 &lt; 2.2e-16 ***
Dist           5.8596  5.8596     1   283.5  43.399 2.166e-10 ***
Dx:No_of_Stim  1.4103  0.7051     2 15483.7   1.590   0.20395    
Dx:Trial_Type  2.0323  1.0162     2 15483.7   0.841   0.43128    
Dx:Probe_Loc   3.5740  0.8935     4 15483.7   7.299 7.224e-06 ***
Dx:Dist        0.3360  0.1680     2 15483.7   1.277   0.27885    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>...the results seem to more or less line up with the regression, with the exception of the <strong>Dx</strong> variable.  So, my second question is <strong>2) Can anyone clarify what is going on with the Dx variable? Is interpreting individual coefficients from the regression model bad practice in this case?</strong></p>

<p>Finally, as a basic (and somewhat embarrassing) afterthought, <strong>3) If I attempt to reduce the model, I should favor the model with the <em>lower</em> REML, yes?</strong></p>

<p>In summation,
<strong>1) Is the above model specification correct?</strong>
<strong>2) Can anyone clarify what is going on with the Dx variable? Is interpreting individual coefficients from the regression model bad practice in this case?</strong>
<strong>3) If I attempt to reduce the model, I should favor the model with the <em>lower</em> REML, yes?</strong></p>

<p><strong>ADDENDUM</strong></p>

<p>By request, I'll describe the task and data a little further.  The data come from a computer task in which participants are presented a number of stimuli, either two or three, in various locations about the screen.  These stimuli can either be ""targets"" or ""distractors"".  After these stimuli, a probe stimulus is presented; if it appears in a position where a previous target has appeared, participants should respond ""yes""; if it appears in the position of a previous distractor or elsewhere, the correct answer is ""no.""  There are 288 trials of this nature; some have two stimuli and some have three, and some lack distractors entirely.  The variables in my model, then, can be elaborated as follows:</p>

<p><strong>Number of Stimuli:</strong> 2 or 3 (2 levels)</p>

<p><strong>Trial Type:</strong> No Distractor (0) or Distractor (1) (2 levels)</p>

<p><strong>Probe Location:</strong> Probe at Target (1), Probe at Distractor (2), or Probe Elsewhere (0) (3 levels)</p>

<p><strong>Distance:</strong> Total Cartesian distance between stimuli, divided by number of stimuli per trial (Continuous)</p>

<p><strong>Dx:</strong> Participant's clinical categorization</p>

<p><strong>Sub ID:</strong> Unique subject identifier (random effect)</p>

<p><strong>Trial:</strong> Trial number (1:288) (random effect)</p>

<p><strong>Correct:</strong> Response classification, either incorrect (0) or correct (1) per trial</p>

<p>Note that the task design makes it inherently imbalanced, as trials without distractors cannot have Probe Location ""Probe at Distractor""; this makes R mad when I try to run RM ANOVAs, and it is another reason I opted for a regression.</p>

<p>Below is a sample of my data (with SubID altered, just in case anyone might get mad):</p>

<pre><code>     SubID      Dx Correct No_of_Stim Trial_Type Probe_Loc      Dist Trial
1 99999999 PROBAND       1          3          0         1 0.9217487     1
2 99999999 PROBAND       0          3          0         0 1.2808184     2
3 99999999 PROBAND       1          3          0         0 1.0645292     3
4 99999999 PROBAND       1          3          1         2 0.7838786     4
5 99999999 PROBAND       0          3          0         0 1.0968788     5
6 99999999 PROBAND       1          3          1         1 1.3076598     6
</code></pre>

<p>Hopefully, with the above variable descriptions these data should be self-explanatory.</p>

<p>Any assistance that people can provide in this matter is very much appreciated.</p>

<p>Sincerely,
peteralynn</p>
"
"0.101062140164153","0.0963061447907242","115188","<p>I am trying to look at whether 2 variables (one dichotomous categorical and one continuous) predict the occurrence of a dichotomous categorical dependent variable.</p>

<pre><code>dependent variable is LENIpos - 0 = no event, 1 = event
predictor variables are Hip.Prox.Femur - 0 = no hip fracture, 1 = hip fracture
                and     age (continuous)
</code></pre>

<p>Both predictor variables have significant p values in separate chi square test and Mann Whitney U test respectively.</p>

<p>When I run a logistic regression <code>glm(LENIpos ~ age + Hip.Prox.Femur, family = ""binomial)</code>, the variables come out as not significant. (1)</p>

<p>However, when I run the logistic regression with interactions <code>glm(LENIpos ~ age * Hip.Prox.Femur...)</code> (2), they are no both significant.  How is this to be interpreted?</p>

<p>Example R outputs:</p>

<p>(1)</p>

<pre><code>Call: glm(formula = LENIpos ~ age + Hip.Prox.Fem, family = ""binomial"", 
    data = dvt)

Deviance Residuals: 
     Min       1Q   Median       3Q      Max  
 -0.9346  -0.7826  -0.4952  -0.3374   2.1897  

Coefficients:
                         Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)              -3.46888    1.00693  -3.445 0.000571 ***
age                       0.02122    0.01519   1.397 0.162535  
Hip.Prox.Femhip fracture  0.72410    0.57790   1.253 0.210212    

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 145.23  on 151  degrees of freedom
Residual deviance: 135.48  on 149  degrees of freedom
AIC: 141.48

Number of Fisher Scoring iterations: 5
</code></pre>

<p>(2)</p>

<pre><code>glm(formula = LENIpos ~ age * Hip.Prox.Fem, family = ""binomial"", 
    data = dvt)

Deviance Residuals: 
        Min       1Q   Median       3Q      Max  
    -1.0364  -0.7815  -0.5373  -0.1761   2.3443  

Coefficients:
                             Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)                  -5.89984    1.98289  -2.975  0.00293 **
age                           0.05851    0.02818   2.076  0.03788 * 
Hip.Prox.Femhip fracture      5.04990    2.46269   2.051  0.04031 * 
age:Hip.Prox.Femhip fracture -0.06058    0.03339  -1.814  0.06965 . 


(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 145.23  on 151  degrees of freedom
Residual deviance: 131.82  on 148  degrees of freedom
AIC: 139.82

Number of Fisher Scoring iterations: 6
</code></pre>
"
"0.114013470672957","0.108647984268766","115304","<p>I am learning about building linear regression models by looking over someone elses R code.  Here is the example data I am using:</p>

<pre><code>v1  v2  v3  response
0.417655013 -0.012026453    -0.528416414    48.55555556
-0.018445979    -0.460809371    0.054017873 47.76666667
-0.246110341    0.092230159 0.057435968 49.14444444
-0.521980295    -0.428499038    0.119640369 51.08888889
0.633310578 -0.224215856    -0.153917427    48.97777778
0.41522316  0.050609412 -0.642394965    48.5
-0.07349941 0.547128578 -0.539018121    53.95555556
-0.313950353    0.207853678 0.713903994 48.16666667
0.404643796 -0.326782199    -0.785848428    47.7
0.028246796 -0.424323318    0.289313911 49.34444444
0.720822953 -0.166712488    0.323246062 50.78888889
-0.430825851    -0.308119827    0.543823856 52.65555556
-0.964175294    0.661700584 -0.11905972 51.03333333
-0.178955757    -0.11148414 -0.151179885    48.28888889
0.488388035 0.515903257 -0.087738159    48.68888889
-0.097527627    0.188292773 0.207321867 49.86666667
0.481853599 0.21142728  -0.226700254    48.38888889
1.139561277 -0.293574756    0.574855693 54.55555556
0.104077762 0.16075114  -0.131124443    48.61111111
</code></pre>

<p>I read in the data and use a call to <code>lm()</code> to build a model:</p>

<pre><code>&gt; my_data&lt;- read.table(""data.csv"", header = T, sep = "","")
&gt; my_lm &lt;- lm(response~v1 + v2 + v3 + v1:v2 + v1:v3 + v2:v3, data=my_data)
&gt; summary(my_lm)

Call:
lm(formula = response ~ v1 + v2 + v3 + v1:v2 + v1:v3 + v2:v3, 
data = my_data)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.0603 -0.6615 -0.1891  1.0395  1.8280 

Coefficients:
         Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  49.33944    0.42089 117.226  &lt; 2e-16 ***
v1            0.06611    0.82320   0.080  0.93732    
v2           -0.36725    1.06359  -0.345  0.73585    
v3            0.72741    1.00973   0.720  0.48508    
v1:v2        -2.54544    2.21663  -1.148  0.27321    
v1:v3         0.80641    2.77603   0.290  0.77640    
v2:v3       -12.16017    3.62473  -3.355  0.00573 ** 
--- 
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 1.375 on 12 degrees of freedom
Multiple R-squared:  0.697, Adjusted R-squared:  0.5455 
F-statistic:   4.6 on 6 and 12 DF,  p-value: 0.01191
</code></pre>

<p>Following along with their code I then use a call to <code>anova()</code>:</p>

<pre><code>&gt; my_lm_anova &lt;- anova(my_lm)
&gt; my_lm_anova
Analysis of Variance Table

Response: response
          Df  Sum Sq Mean Sq F value   Pr(&gt;F)   
v1         1  0.0010  0.0010  0.0005 0.982400   
v2         1  0.2842  0.2842  0.1503 0.705036   
v3         1  9.8059  9.8059  5.1856 0.041891 * 
v1:v2      1  4.3653  4.3653  2.3084 0.154573   
v1:v3      1 16.4582 16.4582  8.7034 0.012141 * 
v2:v3      1 21.2824 21.2824 11.2545 0.005729 **
Residuals 12 22.6921  1.8910                    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>However, I am not sure:</p>

<ol>
<li>Why I would use the call to ANOVA in this situation, and</li>
<li>What the ANOVA table is telling me about the predictor variables.</li>
</ol>

<p>From the code they appear to use the ANOVA table as follows.  For predictor variable v1, the result of </p>

<ul>
<li>Adding the 'Sum Sq' entry for v1 together with half of the 'Sum Sq' entry for v1:v2 and half of the 'Sum Sq' entry for v1:v3, </li>
<li>Dividing by the sum of the entire 'Sum Sq' column, and</li>
<li>Multiplying by 100</li>
</ul>

<p>gives the percent of variance of the response variable that is explained by predictor variable v1 in the <code>lm()</code> model.  I don't see why this is nor why half of the 'Sum Sq' entry for v1:v2 is attributed to v1 and half to v2.  Is this just convenience?</p>
"
"0.0304713817668003","0.029037395206952","115343","<p>I have done a linear regression in R, using glm function. The calculated intercept says 0.98, but when I plot it, it does not seem to hit the estimated intercept on Y axis. Its far below. Here are my data and function: </p>

<pre><code>event = c(2.2, 6.4, 3.4, 10.2, 4.45, 2.65, 8.25, 4.65, 3, 6.5, 5.25, 
8.65, 7.25, 6.4, 7.75, 7.45)

c(230208, 813178, 316617, 1531919, 576869, 270148, 1090947, 562643, 
439885, 745741, 666454, 1078175, 924429, 784333, 1091289, 948062)

fit=glm(event~size)

Call:  glm(formula = chr.co.count.wt ~ size)

Coefficients:
(Intercept)         size  
  9.783e-01    6.528e-06  

Degrees of Freedom: 15 Total (i.e. Null);  14 Residual
Null Deviance:      83.08 
Residual Deviance: 2.849    AIC: 23.8




plot(size,events,col=""blue"",pch=16,xlab=""size"",ylab=""events"",ylim=c(0,12),frame.plot=FALSE,xlim=c(0,2000000),axes = F)

axis(side = 1,at = c(0,0.5e6,1e6,1.5e6),labels =  c(0,0.5e6,1e6,1.5e6))
axis(side = 2,at = seq(from = 0,to = 12,by = 0.5),labels = seq(from = 0,to = 12,by = 0.5))
abline(fit.wt)
</code></pre>

<p><img src=""http://i.stack.imgur.com/iqDK2.png"" alt=""enter image description here""></p>

<p>Why is this discrepancy ? Am i missing something here ? I have also checked the std. err which is 0.27, still higher than what is being observed on plot. </p>

<p>Thank you.</p>
"
"NaN","NaN","115748","<p>What is the best way to simulate ANOVA data for a 2 x 2 design with interaction using a regression approach? I want to generate the data so that I know the true regression coefficients of the model when running lm() in R. </p>

<pre><code>lm(A * B, data=df)
</code></pre>

<p>Thanks in advance</p>
"
"0.0609427635336005","0.0580747904139041","115843","<p>I want to perform an exploratory Cox regression analysis of medical data using R. I am practicing using the pbc data from the survival function.</p>

<p>Would you recommend performing a backward selection multivariate analysis? Are there any summary data / tables I should create for covariates before modelling? Are there any model diagnostics I should perform? And what would be the consequence of doing this?</p>

<p>I would be very grateful for your help and examples using R; also easy to understand literature recommendations (paper, book, and so on) would be nice. </p>

<hr>

<p>To renew my former question: I understand that a stepwise backward regression will lead to inflated coefficients, deflated p-values, and inflated model fit statistics. However, this approach is very common in medical reports. Would it be possible to draw the conclusion that a covariate is independently associated with an outcome, irrespective the above mentioned drawbacks? And when yes, how reliable would it be?</p>

<p>And again <em>being a Little afraid to ask this</em> what would be the best way in R to perform such an analysis?</p>
"
"0.052777981396926","0.0502942438178979","115965","<p>I know there are similar questions on here, but I can't quite find an answer that covers all of what I need. I am running multiple regression in r with two predictor variables and sometimes an interaction term e.g.: </p>

<pre><code>model1 = lm(Measure1 ~ Variable 1 + Variable 2)
model2 = lm(Measure1 ~ Variable 1 + Variable 2 + Variable2:Variable 3)
</code></pre>

<p>I am first wondering, what is the best way to calculate the effect size specifically of variable 2 in both instances. I know because the second formula includes an interaction I can't necessarily use the standard coefficient values, and I'd like to get the effect size in a consistent way between the two formulas. Also, if it's important, the DV is continuous, but the variables are dummy coded variables (e.g. on/off a drug and gender). Along these lines, is there a good way to determine when I <em>should</em> use an interaction in the equation when I have <em>many</em> dependent variables I want to look at? Creating a plot of each manually doesn't seem like the most efficient way to do soâ€¦ </p>
"
"0.140741283725136","0.150882731453694","116007","<p>I have a fairly simple dataset looking at the relationship between the first nesting date of a bird in a given year (Date) and the birds overall fledgling production from that year (Fledge; count data from 0-3 fledglings). I want to determine the optimal laying date for this species (i.e. the date where fledgling production is highest); however, I have been struggling to work out which statistical analysis is most appropriate for my data.</p>

<p>Most birds produce no fledglings in a year, so there are many zero values in the data. With this in mind, I thought that a zero inflated poisson regression might be most appropriate. To test this in R, I fitted a regular glm with poisson distribution (model1 below) and a zero inflated poisson model using zeroinfl() from the pscl library (model2 below). I then compared the two using vuong test statistic (output below).</p>

<pre><code>model1&lt;-glm(Fledge~Date,data=OPT,family=""poisson"")
model2&lt;-zeroinfl(Fledge~Date,data=OPT,dist=""poisson"")
vuong(model1,model2)

Vuong Non-Nested Hypothesis Test-Statistic: 4.25169 
(test-statistic is asymptotically distributed N(0,1) under the
null that the models are indistinguishible)
in this case:
model1 &gt; model2, with p-value 1.0608e-05
</code></pre>

<p>According to the vuong output, a regular non-zero inflated poisson regression is most appropriate. This wasn't that surprising as, from my understanding, the zeros in my data are 'true zeroes' i.e. they are legitimate data points and not due to sampling technique or design. Next I tested for overdispersion by fitting a glm with a quasipoisson distribution to check the dispersion parameter (model3).</p>

<pre><code>model3&lt;-glm(Fledge~Date,data=OPT,family=""quasipoisson"")
summary(model3)

Call:
glm(formula = Fledge ~ Date, family = ""quasipoisson"", data = OPT)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.7877  -0.6258  -0.5578  -0.4504   3.3648  

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept) -0.15109    0.54874  -0.275  0.78315   
Date        -0.03288    0.01133  -2.901  0.00385 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for quasipoisson family taken to be 1.296628)

Null deviance: 455.36  on 642  degrees of freedom
Residual deviance: 443.40  on 641  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 6
</code></pre>

<p>As you can see, the output showed a dispersion parameter close to one (1.297), and a null deviance (455.36) very close to the residual deviance (443.40). So I concluded that overdispersion was not a big problem, and a poisson, rather than negative binomial, distribution was required.</p>

<p>After doing all this, I was fairly confident that my regular poisson regression (model1) was best for my data. HOWEVER, when I plotted the model outputs of the zero inflated and non-zero inflated data, the non-zero inflated model output didn't appear to fit my data at all while the zero inflated model fitted much better.</p>

<pre><code>ggplot(OPT,aes(x=Date,y=Fledge))+
geom_point()+
theme_bw()+
geom_line(data=cbind(OPT,optpred=predict(model1)),aes(y=optpred),size=1,colour=""red"")+
geom_line(data=cbind(OPT,optpred2=predict(model2)),aes(y=optpred2),size=1,colour=""blue)
</code></pre>

<p><img src=""http://i.stack.imgur.com/GFp5F.jpg"" alt=""enter image description here""></p>

<p>As you can see, the non-zero inflated model (red line) doesn't seem to fit the data at all, as it is predicting fledgling production less than 0 on all dates (obviously not biologically possible)! Conversely, the zero inflated model (blue line) seems to fit the data very well. So I have two questions to ask;</p>

<ol>
<li><p>Were the methods I used to test zero inflation (and overdispersion) appropriate and interpreted correctly?</p></li>
<li><p>If so, is my application of a poisson regression appropriate? Or are there other more appropriate options for my data outside of the two I've tried here?</p></li>
</ol>

<p>I've included a dput() of my data below to allow for replication.</p>

<p>Thanks for the help!</p>

<pre><code>structure(list(Date = c(45L, 40L, 42L, 41L, 39L, 34L, 40L, 44L, 
36L, 32L, 33L, 89L, 58L, 50L, 46L, 56L, 48L, 69L, 64L, 56L, 61L, 
58L, 66L, 63L, 57L, 58L, 60L, 65L, 31L, 48L, 42L, 41L, 46L, 38L, 
59L, 41L, 65L, 41L, 34L, 41L, 36L, 60L, 42L, 39L, 43L, 46L, 47L, 
38L, 38L, 71L, 65L, 51L, 42L, 37L, 51L, 41L, 65L, 59L, 44L, 50L, 
51L, 47L, 40L, 53L, 56L, 62L, 50L, 46L, 51L, 55L, 50L, 46L, 45L, 
39L, 36L, 52L, 50L, 73L, 42L, 38L, 51L, 49L, 43L, 45L, 44L, 76L, 
68L, 65L, 70L, 56L, 40L, 45L, 49L, 52L, 66L, 80L, 45L, 42L, 44L, 
37L, 48L, 43L, 53L, 31L, 47L, 49L, 44L, 46L, 54L, 55L, 48L, 53L, 
55L, 72L, 54L, 45L, 83L, 59L, 48L, 47L, 52L, 72L, 51L, 70L, 48L, 
44L, 42L, 38L, 48L, 43L, 45L, 39L, 45L, 42L, 64L, 46L, 56L, 34L, 
50L, 48L, 47L, 47L, 60L, 50L, 61L, 40L, 72L, 63L, 55L, 66L, 69L, 
66L, 61L, 60L, 60L, 40L, 70L, 45L, 40L, 41L, 42L, 71L, 54L, 45L, 
52L, 48L, 40L, 39L, 49L, 42L, 43L, 53L, 38L, 53L, 52L, 68L, 61L, 
62L, 87L, 41L, 45L, 37L, 44L, 45L, 43L, 72L, 39L, 56L, 34L, 74L, 
62L, 46L, 43L, 47L, 35L, 54L, 61L, 44L, 49L, 54L, 61L, 37L, 51L, 
48L, 52L, 48L, 48L, 44L, 45L, 44L, 45L, 68L, 61L, 87L, 51L, 52L, 
50L, 50L, 56L, 55L, 56L, 57L, 65L, 41L, 63L, 76L, 52L, 62L, 50L, 
50L, 54L, 63L, 48L, 54L, 46L, 57L, 54L, 52L, 45L, 41L, 54L, 74L, 
69L, 68L, 51L, 60L, 54L, 44L, 67L, 52L, 49L, 43L, 41L, 44L, 49L, 
46L, 43L, 46L, 49L, 46L, 47L, 54L, 55L, 67L, 52L, 55L, 52L, 49L, 
50L, 51L, 57L, 48L, 34L, 54L, 49L, 47L, 71L, 62L, 43L, 45L, 45L, 
49L, 58L, 57L, 55L, 54L, 52L, 51L, 41L, 54L, 70L, 52L, 53L, 53L, 
50L, 71L, 56L, 48L, 33L, 43L, 41L, 68L, 42L, 38L, 39L, 46L, 55L, 
64L, 62L, 56L, 69L, 44L, 49L, 54L, 86L, 46L, 46L, 50L, 44L, 45L, 
55L, 55L, 52L, 49L, 49L, 56L, 41L, 34L, 50L, 62L, 39L, 41L, 56L, 
42L, 40L, 43L, 44L, 45L, 43L, 48L, 41L, 45L, 62L, 49L, 47L, 49L, 
63L, 69L, 46L, 53L, 49L, 59L, 54L, 33L, 46L, 44L, 49L, 36L, 41L, 
33L, 66L, 56L, 67L, 43L, 66L, 31L, 51L, 59L, 57L, 51L, 39L, 44L, 
31L, 40L, 39L, 42L, 27L, 43L, 42L, 78L, 60L, 70L, 64L, 67L, 66L, 
67L, 66L, 62L, 58L, 51L, 50L, 60L, 38L, 45L, 34L, 69L, 38L, 45L, 
39L, 44L, 39L, 44L, 43L, 46L, 37L, 59L, 74L, 59L, 39L, 43L, 40L, 
38L, 45L, 45L, 42L, 36L, 33L, 51L, 64L, 52L, 40L, 89L, 49L, 37L, 
51L, 70L, 65L, 71L, 62L, 61L, 68L, 59L, 54L, 75L, 57L, 55L, 58L, 
52L, 58L, 45L, 50L, 41L, 64L, 49L, 50L, 67L, 54L, 43L, 49L, 54L, 
55L, 53L, 53L, 59L, 47L, 47L, 48L, 45L, 50L, 39L, 48L, 51L, 54L, 
44L, 43L, 56L, 51L, 38L, 71L, 62L, 56L, 65L, 69L, 68L, 52L, 47L, 
47L, 47L, 80L, 51L, 48L, 36L, 32L, 39L, 45L, 31L, 43L, 57L, 65L, 
60L, 62L, 36L, 53L, 64L, 57L, 43L, 71L, 66L, 63L, 49L, 39L, 49L, 
43L, 32L, 47L, 44L, 35L, 35L, 41L, 54L, 50L, 44L, 44L, 48L, 50L, 
41L, 40L, 46L, 48L, 38L, 43L, 54L, 52L, 36L, 62L, 72L, 47L, 66L, 
50L, 51L, 50L, 56L, 47L, 67L, 50L, 35L, 40L, 43L, 42L, 31L, 35L, 
43L, 46L, 45L, 46L, 39L, 40L, 40L, 39L, 36L, 45L, 43L, 44L, 44L, 
44L, 38L, 49L, 52L, 49L, 43L, 42L, 47L, 56L, 51L, 51L, 51L, 59L, 
64L, 46L, 40L, 75L, 65L, 51L, 91L, 56L, 83L, 56L, 57L, 58L, 51L, 
50L, 56L, 40L, 69L, 54L, 45L, 35L, 41L, 48L, 60L, 54L, 39L, 39L, 
31L, 92L, 39L, 66L, 56L, 48L, 44L, 40L, 42L, 47L, 51L, 47L, 45L, 
49L, 69L, 48L, 42L, 58L, 56L, 58L, 61L, 42L, 36L, 47L, 52L, 45L, 
54L, 55L, 62L, 48L, 44L, 54L, 51L, 46L, 44L, 50L, 37L, 33L, 40L, 
57L, 54L, 64L, 59L, 69L, 46L, 40L, 51L, 53L, 79L, 60L), Fledge = c(1L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 
1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 
1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 3L, 0L, 1L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 
0L, 0L, 2L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 2L, 0L, 0L, 0L, 0L, 
0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 2L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 1L, 2L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 
0L, 1L, 2L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 3L, 0L, 0L, 1L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
2L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 2L, 1L, 2L, 0L, 0L, 0L, 1L, 1L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 
1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 3L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 2L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 2L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 
0L, 1L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 2L, 2L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
2L, 0L, 2L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 
1L, 0L, 2L, 0L, 0L, 0L, 1L, 0L, 0L, 2L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 1L, 1L, 0L, 2L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L)), .Names = c(""Date"", ""Fledge""), class = ""data.frame"", row.names = c(NA, 
    -643L))
</code></pre>
"
"0.068136080998913","0.0649295895722714","116196","<p>My goal is to investigate a dependent variable which is metric (time in hours). The independent variables include 3 metric, 2 binary (factors), and one factor variable, which consists of 11 districts of a city.</p>

<p>I tried to conduct a GLM.</p>

<p>Can I put all this together in one model? It seems to be difficult to interpret the output!
Should I rather use different/various models, with only one factor per regression?
If I use the GLM which kind of family and link function should I use?</p>

<p>The idle time seems to be a right skewed distribution and thefore I chose a gamma family with a inverse link function. </p>

<p>The output of a model wich contains all the independent variables as decribed above: </p>

<p><img src=""http://i.stack.imgur.com/N1hEi.jpg"" alt=""enter image description here""></p>

<p>How can I eleminate the NAs? Or what do they actually mean? </p>

<p>Moreover the ANOVA test was conducted to get a closer look on the district variable called Bezirk, which shows massive differences in the mean value! Is this consistent with small coefficients in the GLM regression? (The means vary between from 3,7 in T.Mitte and 15 in T.Treptow)</p>

<p>Best regards</p>
"
"0.0929636479491388","0.0805352440958291","116272","<p>I have read similar posts to this but my problem is not resolved by the answers given. I want to do a v simple linear regression to see if bite incidence is related to district, zone (vacc or control) and year. As you can see in the output one of the districts RORYA is given NA coefficients, and I get the message ""Coefficients: (1 not defined because of singularities)"". I have read up on this and it seems its to do with co-linearity of factors. One solution given is to add -1 to the call, which removes the intercept but does not solve my problem as RORYA district still has NAs in the summary output.</p>

<p>Another solution I have tried is changing the order of the explanatory variables in the call. This does change things...Rorya district suddenly has coefficients but the Zone variable becomes NA'd. Neither of which is good as I would like a coefficent for all the explanatory variables.</p>

<p>I was wondering whether anyone might know why this is happening and whether there is a solution to this problem so that all the variables can have coefficients?</p>

<p>Thanks in advance.</p>

<p>A Reproducible example:</p>

<pre><code>df &lt;- structure(list(DISTRICT = structure(c(1L, 6L, 5L, 3L, 2L, 4L, 
1L, 6L, 5L, 3L, 2L, 4L, 1L, 6L, 5L, 3L, 2L, 4L, 1L, 6L, 5L, 3L, 
2L, 4L), .Label = c(""BUNDA"", ""MASWA"", ""MUSOMA"", ""RORYA"", ""SERENGETI"", 
""TARIME""), class = ""factor""), zone = structure(c(2L, 2L, 2L, 
1L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 
2L, 2L, 1L, 1L, 1L), .Label = c(""c"", ""v""), class = ""factor""), 
year = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 
2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L), .Label = c(""2010"", 
""2011"", ""2012"", ""2013""), class = ""factor""), bites = c(7.461327937, 
NA, NA, NA, 35.16164185, 26.39109338, 57.89990479, 1.47191729, 
3.608371422, 51.36718605, NA, 16.21167165, 46.85713945, 15.89670673, 
5.212092054, 259.8137381, 30.80276062, 20.73585909, 10.44585911, 
9.420270656, 7.617673001, 307.4586643, 27.31565565, 30.16124958
), deaths = c(0, NA, NA, NA, 0, 1.508062479, 0.298453117, 
0, 0, 0, NA, 2.262093719, 0.298453117, 0.294383458, 0, 2.233355915, 
0.581184163, 1.131046859, 0.298453117, 0.588766916, 1.202790474, 
2.977807887, 0, 1.885078099)), .Names = c(""DISTRICT"", ""zone"", 
""year"", ""bites"", ""deaths""), row.names = c(NA, -24L), class = ""data.frame"")
</code></pre>

<p>Code:</p>

<pre><code>summary(df )
names(df)
attach(df)
is.numeric(year)
df$year  &lt;- as.factor(as.character(df$year))
is.factor(df$year)

model1 &lt;- lm(bites ~   zone + DISTRICT-1 +year, data = df)
summary(model1)

&gt; sessionInfo()
R version 3.1.0 (2014-04-10)
Platform: x86_64-apple-darwin13.1.0 (64-bit)

locale:
[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8

attached base packages:
[1] grid      stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] ggplot2_1.0.0

loaded via a namespace (and not attached):
[1] colorspace_1.2-4 digest_0.6.4     gtable_0.1.2     MASS_7.3-34      munsell_0.4.2   plyr_1.8.1       proto_0.3-10     Rcpp_0.11.2     
[9] reshape2_1.4     scales_0.2.4     stringr_0.6.2    tools_3.1.0  
</code></pre>
"
"0.0304713817668003","0.029037395206952","116560","<p>It seems odd to scale a categorical variable, but I need to get the correct coefficients for each of my variables in linear regression. Is it correct to scale the same way you would with continuous variables, or what is the right thing to do here? </p>

<p>For example if x is categorical and y is continuous:</p>

<pre><code>model=lm(DV ~ scale(x) + scale(y), data=myData)
</code></pre>

<p>Is the above the right thing to do?</p>
"
"0.202174206049786","0.17553454438919","116562","<p>I recently started transitioning from JMP to R and to get started, I've been trying to reproduce some of my old JMP results in R. However, when I run a multiple regression with one continuous variable (income) and one categorical variable (condition) predicting a continuous variable (psc), the results from the 2 programs differ.</p>

<p>Here's my JMP model and results:
<img src=""http://i.stack.imgur.com/ZQnL8.png"" alt=""JMP model""></p>

<p><img src=""http://i.stack.imgur.com/Rrmyv.png"" alt=""JMP results""></p>

<p>And here's my R code and results:</p>

<pre><code>&gt; library(plyr)

&gt; # load data files
&gt; online &lt;- read.csv('r_online.csv')
&gt; paper &lt;- read.csv('r_paper.csv')

&gt; # define conditions for online data
&gt; online$condition &lt;- NA
&gt; levels(online$condition) &lt;- c('wc','fd')

&gt; online[!is.na(online$Ntrl1), 'condition'] &lt;- 'wc'
&gt; online[!is.na(online$Ntrl3), 'condition'] &lt;- 'fd'

&gt; online$condition &lt;- factor(online$condition)

&gt; # merge online and paper data
&gt; mydata &lt;- rbind.fill(online, paper)

&gt; # exclude dropped data
&gt; mydata &lt;- subset(mydata, Class &lt; 5)

&gt; # calcualte psc
&gt; psc &lt;- ((8-mydata$PSF1r)+(8-mydata$PSF2r)+mydata$PSF3+(8-mydata$PSF4r)+(8-mydata$PSF5r)+mydata$PSF6)/6
&gt; mydata$psc &lt;- psc

&gt; # save income and condition as values
&gt; income &lt;- mydata$Income
&gt; condition &lt;-mydata$condition

&gt; # psc by income and condition
&gt; psc.income.regress &lt;- lm(psc ~ income * condition)
&gt; summary(psc.income.regress)

Call:
lm(formula = psc ~ income * condition)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.7275 -0.3585  0.0731  0.5122  1.1602 

Coefficients:
                   Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)         3.89116    0.50804   7.659 1.96e-09 ***
income              0.13393    0.07494   1.787   0.0813 .  
conditionwc        -1.53409    0.69323  -2.213   0.0325 *  
income:conditionwc  0.21807    0.10291   2.119   0.0402 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.7742 on 41 degrees of freedom
  (4 observations deleted due to missingness)
Multiple R-squared:  0.4149,    Adjusted R-squared:  0.3721 
F-statistic:  9.69 on 3 and 41 DF,  p-value: 5.854e-05
</code></pre>

<p>So, R-squared, R-squared Adjusted, Overall F, Overall p, and p and t for the interaction are the same in both R and JMP, but p and t for the main effects and all of the Estimates are different.</p>

<p>I did some reading and found that this occurs because JMP calculates Type-III sums of squares, while R calculates Type-I SS. So far, though, I haven't been able to figure out how to get R to calculate Type-III SS in the same way as JMP.</p>

<p>One site said that I could get Type-III SS by changing the last part of my R code to this:</p>

<pre><code>&gt; ### alternative method suggested for getting type-III SS ###
&gt; options(contrasts=c(""contr.sum"",""contr.poly""))
&gt; psc.income.regress &lt;- lm(psc ~ income * condition)
&gt; drop1(psc.income.regress,~.,test=""F"")

Single term deletions

Model:
psc ~ income * condition
                 Df Sum of Sq    RSS      AIC F value    Pr(&gt;F)    
&lt;none&gt;                        24.576 -19.2208                      
income            1   13.3659 37.941  -1.6778 22.2985 2.729e-05 ***
condition         1    2.9354 27.511 -16.1434  4.8972   0.03253 *  
income:condition  1    2.6917 27.267 -16.5438  4.4906   0.04018 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1


&gt; summary(psc.income.regress)

Call:
lm(formula = psc ~ income * condition)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.7275 -0.3585  0.0731  0.5122  1.1602 

Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)        3.12412    0.34662   9.013 2.82e-11 ***
income             0.24297    0.05145   4.722 2.73e-05 ***
condition1         0.76704    0.34662   2.213   0.0325 *  
income:condition1 -0.10904    0.05145  -2.119   0.0402 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.7742 on 41 degrees of freedom
  (4 observations deleted due to missingness)
Multiple R-squared:  0.4149,    Adjusted R-squared:  0.3721 
F-statistic:  9.69 on 3 and 41 DF,  p-value: 5.854e-05
</code></pre>

<p>Now all the estimates for income, interaction, and the intercept are the same in R as in JMP, but condition is still different.</p>

<p>Another person suggested that I recode my conditions as numeric contrasts (instead of having them as factors) and center everything, so I changed the end of my code to this:</p>

<pre><code>&gt; ### 2nd alternative method: change condition to numeric contrast and center variables ###
&gt; condition_c &lt;- ifelse(condition == 'fd', +.5, -.5)
&gt; condition_c &lt;- scale(condition_c, scale=F,center=T)
&gt; income_c &lt;- scale(income,scale=F,center=T)
&gt; psc.income.regress &lt;- lm(psc ~ income_c * condition_c)
&gt; summary(psc.income.regress)

Call:
lm(formula = psc ~ income_c * condition_c)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.7275 -0.3585  0.0731  0.5122  1.1602 

Coefficients:
                     Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)           4.67891    0.11588  40.379  &lt; 2e-16 ***
income_c              0.22739    0.05241   4.338 9.13e-05 ***
condition_c           0.14813    0.23615   0.627   0.5340    
income_c:condition_c -0.21807    0.10291  -2.119   0.0402 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.7742 on 41 degrees of freedom
  (4 observations deleted due to missingness)
Multiple R-squared:  0.4149,    Adjusted R-squared:  0.3721 
F-statistic:  9.69 on 3 and 41 DF,  p-value: 5.854e-05
</code></pre>

<p>Doing this, the p and t values for the interaction and condition become the same as in JMP, but now income and all the estimates are different.</p>

<p>I've tried to be as thorough as I can in trying finding an answer on my own, but I've run out of ideas so any help would be immensely appreciated. All relevant R files can be found here: <a href=""https://www.dropbox.com/s/eoup5im2iko1ro6/R.zip?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/eoup5im2iko1ro6/R.zip?dl=0</a></p>
"
"0.035185320931284","0.0502942438178979","116681","<p>I am working on a large linear regression with a volume metric as my dependent. Right now I am multiplying the model.matrix by the respective coefficients to get to the relative volume contribution by variable.</p>

<pre><code>decomp =  t(apply(model.matrix(fit$terms, data = Data[Data$RetailRead == ""Retail"",]), 1, function(x) {x*fit$coef})) 
</code></pre>

<p>However, this leads to some of the volume being meaningless due to a factor variables within the model. The factor variables need to be added to the base category in order to get to the total overall volume for that variable. </p>

<pre><code>asgn = attr(model.matrix(fit$terms, data = Data), ""assign"") #find indexes which need to be summed

merged =data.frame(t(apply(decomp, 1, function(x) {tapply(x, asgn, sum)})))#sum each appropriate collumn
colnames(merged) = c(""Intercept"", attr(terms(fit), ""term.labels"")) #label collumns
</code></pre>

<p>The code I have written to summarize the volume driven per variable is clunky and inefficient - there are several further steps then above to allow each column of the variable decomposition to be able to stand alone. I am surprised that there is little literature on doing this within R and I wonder if any one know of a package to better perform this task.</p>
"
"0.0430930413588572","0.0615976171764886","116825","<p>I'm exploring linear regressions in R and Python, and usually get the same results but this is an instance I do not. </p>

<p>I added the sum of <code>Agriculture</code> and <code>Education</code> to the <code>swiss</code> dataset as an additional explanatory variable, with <code>Fertility</code> as the regressor.</p>

<p>R gives me an <code>NA</code> for the $\beta$ value of <code>z</code>, but Python gives me a numeric value for <code>z</code> and a warning about a very small eigenvalue. Is there a way to make Python and statmodels explicitly tell me that <code>z</code> adds no information to the regressor?</p>

<p>Additionally, I originally did this analysis in an iPython notebook, where there is no need to do an explicit <code>print</code> of the regression summary results <code>reg_results</code>, and when the <code>print</code> command is omitted there is no warning about the low eigenvalues which makes it more difficult to know that <code>z</code> is worthless.</p>

<p>R code:</p>

<pre><code>data(swiss)
swiss$z &lt;- swiss$Agriculture + swiss$Education
formula &lt;- 'Fertility ~ .'
print(lm(formula, data=swiss))
</code></pre>

<p>R output:</p>

<pre><code>Call:
lm(formula = formula, data = swiss)

Coefficients:
     (Intercept)       Agriculture       Examination         Education
         66.9152           -0.1721           -0.2580           -0.8709
        Catholic  Infant.Mortality                 z
          0.1041            1.0770                NA
</code></pre>

<p>Python Code:</p>

<pre><code>import statsmodels.formula.api as sm
import pandas.rpy.common as com

swiss = com.load_data('swiss')

# get rid of periods in column names
swiss.columns = [_.replace('.', '_') for _ in swiss.columns]

# add clearly duplicative data
swiss['z'] = swiss['Agriculture'] + swiss['Education']

y = 'Fertility'
x = ""+"".join(swiss.columns - [y])
formula = '%s ~ %s' % (y, x)
reg_results = sm.ols(formula, data=swiss).fit().summary()
print(reg_results)
</code></pre>

<p>Python output:</p>

<pre><code>                            OLS Regression Results
==============================================================================
Dep. Variable:              Fertility   R-squared:                       0.707
Model:                            OLS   Adj. R-squared:                  0.671
Method:                 Least Squares   F-statistic:                     19.76
Date:                Thu, 25 Sep 2014   Prob (F-statistic):           5.59e-10
Time:                        22:55:42   Log-Likelihood:                -156.04
No. Observations:                  47   AIC:                             324.1
Df Residuals:                      41   BIC:                             335.2
Df Model:                           5
====================================================================================
                       coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------------
Intercept           66.9152     10.706      6.250      0.000        45.294    88.536
Agriculture          0.1756      0.062      2.852      0.007         0.051     0.300
Catholic             0.1041      0.035      2.953      0.005         0.033     0.175
Education           -0.5233      0.115     -4.536      0.000        -0.756    -0.290
Examination         -0.2580      0.254     -1.016      0.315        -0.771     0.255
Infant_Mortality     1.0770      0.382      2.822      0.007         0.306     1.848
z                   -0.3477      0.073     -4.760      0.000        -0.495    -0.200
==============================================================================
Omnibus:                        0.058   Durbin-Watson:                   1.454
Prob(Omnibus):                  0.971   Jarque-Bera (JB):                0.155
Skew:                          -0.077   Prob(JB):                        0.925
Kurtosis:                       2.764   Cond. No.                     1.11e+08
==============================================================================

Warnings:
[1] The smallest eigenvalue is 3.87e-11. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.
</code></pre>

<p>```</p>
"
"0.0746393370862076","0.0592723347638087","117052","<p>I have been trying to replicate the results of the Stata option <code>robust</code> in R. I have used the <code>rlm</code> command form the MASS package and also the command <code>lmrob</code> from the package ""robustbase"". In both cases the results are quite different from the ""robust"" option in Stata. Can anybody please suggest something in this context?</p>

<p>Here are the results I obtained when I ran the robust option in Stata:</p>

<pre><code>. reg yb7 buildsqb7 no_bed no_bath rain_harv swim_pl pr_terrace, robust

Linear regression                                      Number of obs =    4451
                                                       F(  6,  4444) =  101.12
                                                       Prob &gt; F      =  0.0000
                                                       R-squared     =  0.3682
                                                       Root MSE      =   .5721

------------------------------------------------------------------------------
             |               Robust
         yb7 |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
   buildsqb7 |   .0046285   .0026486     1.75   0.081    -.0005639     .009821
      no_bed |   .3633841   .0684804     5.31   0.000     .2291284    .4976398
     no_bath |   .0832654   .0706737     1.18   0.239    -.0552904    .2218211
   rain_harv |   .3337906   .0395113     8.45   0.000     .2563289    .4112524
     swim_pl |   .1627587   .0601765     2.70   0.007     .0447829    .2807346
  pr_terrace |   .0032754   .0178881     0.18   0.855    -.0317941    .0383449
       _cons |   13.68136   .0827174   165.40   0.000     13.51919    13.84353
</code></pre>

<p>And this is what I obtained in R with the lmrob option:</p>

<pre><code>&gt; modelb7&lt;-lmrob(yb7~Buildsqb7+No_Bed+Rain_Harv+Swim_Pl+Gym+Pr_Terrace, data&lt;-bang7)
&gt; summary(modelb7)

Call:
lmrob(formula = yb7 ~ Buildsqb7 + No_Bed + Rain_Harv + Swim_Pl + Gym + Pr_Terrace, 
    data = data &lt;- bang7)
 \--&gt; method = ""MM""
Residuals:
      Min        1Q    Median        3Q       Max 
-51.03802  -0.12240   0.02088   0.18199   8.96699 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 12.648261   0.055078 229.641   &lt;2e-16 ***
Buildsqb7    0.060857   0.002050  29.693   &lt;2e-16 ***
No_Bed       0.005629   0.019797   0.284   0.7762    
Rain_Harv    0.230816   0.018290  12.620   &lt;2e-16 ***
Swim_Pl      0.065199   0.028121   2.319   0.0205 *  
Gym          0.023024   0.014655   1.571   0.1162    
Pr_Terrace   0.015045   0.013951   1.078   0.2809    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Robust residual standard error: 0.1678 
Multiple R-squared:  0.8062,    Adjusted R-squared:  0.8059 
</code></pre>
"
"0.0609427635336005","0.0435560928104281","117340","<p>My situation:</p>

<ul>
<li>small sample size: 116</li>
<li>binary outcome variable</li>
<li>long list of explanatory variables: 50</li>
<li>explanatory variables did not come from the top of my head; their choice was based on the literature.</li>
</ul>

<p>Following a suggestion to a previous <a href=""http://stats.stackexchange.com/questions/102892/how-to-select-a-subset-of-variables-from-my-original-long-list-in-order-to-perfo"">question</a> of mine, I have run LASSO (using R's glmnet package) in order to select the subset of exaplanatory variables that best explain variations in my binary outcome variable.</p>

<p>I have calculated lambda.min through cross-validation (cv.glmnet command) and got the correspondent coefficients for my explanatory variables. For 6 of my total 50 explanatory variables, the coefficients were non-zero. Are those coefficients comparable, i.e. can I say that the variables with the highest ones are the most important? If they are not comparable, can I run logistic regression (using glm) with those 6 variables and then compare them in terms of coefficients and p-values?</p>
"
"0.0215465206794286","0.0410650781176591","117437","<p>I want to replicate a fuzzy regression using a linear programming problem approach. </p>

<p>I have the following information: "" A fuzzy regression analysis with only one independent variable X results in the following bivariate regression model:
$$ \hat{Y}=\tilde{A_o}+\tilde{A_1}X,$$</p>

<p>where $\tilde{A_o}$ is a is a fuzzy intercept, $\tilde{A_1}$is a fuzzy slope coefficient, the parameters are expressed as $\tilde{A_i}=(m_i,c_i)$ where $m_i$ is a centre  and $c_i$ is the fuzzy half-width.""</p>

<p>To determine the fuzzy coefficients the following linear programming problem is used:</p>

<p>minimize $$ S= nc_0 + c_1\sum_{i=1}^{n}|X_i|$$</p>

<p>subject to $$c_0\geqslant0,\geqslant0,$$</p>

<p>$$\sum_{j=0}^{l}m_iX_{ij}+(1-h)\sum_{j=0}^{l}c_i|X_{ij}| \geqslant Y_i+(1-h), \mbox{for i=1 to n}$$</p>

<p>$$\sum_{j=0}^{l}m_iX_{ij}-(1-h)\sum_{j=0}^{l}c_i|X_{ij}| \geqslant Y_i-(1-h), \mbox{for i=1 to n}$$</p>

<p>where $h=0$</p>

<p>I have the following data :
$[X_i : Y_i]=[(2:14),(4:16),(6:14),(8:18),(10:18),(12:22),(14:18),
(16:22)]$</p>

<p>How to  solve the LP using R?</p>
"
"0.0646395620382857","0.0615976171764886","117450","<p>I have a confusing situation where I have strongly conflicting results from two ways of analyzing my simple data. I measure two binary variables from each participant, AestheticOnly and ChoiceVA. I want to know if AestheticOnly depends on ChoiceVA and whether this relation is different in two different experiments. Here is my participant count data:</p>

<pre><code>Experiment 1
                 AestheticOnly
                 0   1  All
ChoiceVA A      35   6   41
         V      20  13   33
         All    55  19   74

Experiment 2
                 AestheticOnly
                 0   1  All
ChoiceVA A      12  10   22
         V      31  11   42
         All    43  21   64
</code></pre>

<p>I run a logistic regression where AestheticOnly is modelled by ChoiceVA, Experiment, and the interaction:</p>

<pre><code>&gt; mod &lt;- glm( AestheticOnly ~ ChoiceVA*Experiment, data = d, family=binomial)
&gt; summary(mod)

Call:
glm(formula = AestheticOnly ~ ChoiceVA * Experiment, family = binomial, 
    data = d)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.1010  -0.7793  -0.5625   1.2557   1.9605  

Coefficients:
                     Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)           -3.3449     0.9820  -3.406 0.000659 ***
ChoiceVAV              3.5194     1.2630   2.787 0.005327 ** 
Experiment             1.5813     0.6153   2.570 0.010170 *  
ChoiceVAV:Experiment  -2.1866     0.7929  -2.758 0.005820 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 166.16  on 137  degrees of freedom
Residual deviance: 157.01  on 134  degrees of freedom
AIC: 165.01

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Apparently all factors are significant. But, this just doesn't make sense to me. For example, looking at the main effect of experiment should be equivalent to performing a Fisher's Exact test comparing 55 and 19 with 43 and 21 (bottom lines of each table). This is obviously not significant (p=.452). So why does the regression model give such a different result? Any help much appreciated.</p>
"
"0.0967596325610309","0.0922061136661462","117593","<p>This is a really simple problem I am having, yet for the life of me I can't find a solution searching around. In theory I can simply recode the data, but that is an extreme solution I would rather not use if I don't have to. </p>

<p>I am simply trying to do a logistic regression with an ordered factor as my predictor. For a toy data set, consider:</p>

<pre><code>  radiation leukemia other total
1         0       13   378   391
2       1-9        5   200   205
3     10-49        5   151   156
4     50-99        3    47    50
5   100-199        4    31    35
6       200       18    33    51
</code></pre>

<p>I want to execute the following:</p>

<pre><code>glm(cbind(leukemia,other)~radiation,data=leuk,family=binomial(""logit""))
</code></pre>

<p>That is, leukemia are the ""successes"" and other are the ""failures"". Basically, trying to predict dose-response relationship between radiation and the proportional mortality rates for leukemia. However, this model is oversaturated:</p>

<pre><code>Call:  glm(formula = cbind(leukemia, other) ~ radiation, family = binomial(""logit""), 
    data = leuk)

Coefficients:
     (Intercept)      radiation1-9    radiation10-49  radiation100-199  
         -3.3699           -0.3189           -0.0379            1.3223  
    radiation200    radiation50-99  
          2.7638            0.6184  

Degrees of Freedom: 5 Total (i.e. Null);  0 Residual
Null Deviance:      54.35 
Residual Deviance: -3.331e-15   AIC: 33.67
</code></pre>

<p>I don't want each level of radiation as a factor to be its own predictor variable; that makes no sense, especially when you only have a small number of data points (note, this isn't actually the real data I am using, this is just a toy example that is similar). In any case, how do I force R to simply consider the factor radiation as a single variable with multiple levels? For example, if I do the following:</p>

<pre><code>x&lt;-c(0,1,2,3,4,5)
glm(cbind(leukemia,other)~x,data=leuk,family=binomial(""logit""))

Call:  glm(formula = cbind(leukemia, other) ~ x, family = binomial(""logit""), 
    data = leuk)

Coefficients:
(Intercept)            x  
    -3.9116       0.5731  

Degrees of Freedom: 5 Total (i.e. Null);  4 Residual
Null Deviance:      54.35 
Residual Deviance: 10.18        AIC: 35.84
</code></pre>

<p>This is more in line with what I want. But I am nervous about using that x variable in the regression for fear of changing the interpretation of the results. Similarly, I'd prefer to avoid an irritating system of dummy variables. </p>

<p>How do I go about doing this? Or is there a better workaround altogether for studying this type of relationship that I am not considering?</p>
"
"0.0929636479491388","0.0885887685054121","117631","<p>I measure two binary responses from each participant (ChoiceVA = V or A, AestheticOnly = 0 or 1). There are two experiments (between-participant). I want to test the following hypotheses:</p>

<p>AestheticOnly depends on Experiment (main effect)
AestheticOnly depends on ChoiceVA (main effect)
The way AestheticOnly depends on Experiment depends on ChoiceVA (interaction)</p>

<p>Here is my data. The first number in each cell is the proportion of participants scoring 1 for AestheticOnly, and the second number is the n for participants in that cell.</p>

<pre><code>                         ChoiceVA               
                        A       V     All

Experiment  1      0.1463  0.3939  0.2568
                       41      33      74

            2      0.4545  0.2619  0.3281
                       22      42      64

            All    0.2540  0.3200  0.2899
                       63      75     138
</code></pre>

<p>Just from looking at the data it is pretty obvious that neither main effect is significant (e.g. for ChoiceVA, bottom row, .25 of 63 participants is not significantly different from .32 of 75 participants). In my naivity I thought perhaps I could test these hypotheses with a straightforward binary logistic regression:</p>

<pre><code>&gt; mod &lt;- glm( AestheticOnly ~ Experiment+ChoiceVA+Experiment*ChoiceVA, data = d, family=binomial )
&gt; summary(mod)

Call:
glm(formula = AestheticOnly ~ Experiment + ChoiceVA + Experiment *
    ChoiceVA, family = binomial, data = d)

Deviance Residuals:
    Min       1Q   Median       3Q      Max 
-1.1010  -0.7793  -0.5625   1.2557   1.9605 

Coefficients:
                      Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)            -1.7636     0.4419  -3.991 6.57e-05 ***
Experiment2             1.5813     0.6153   2.570  0.01017 * 
ChoiceVAV               1.3328     0.5676   2.348  0.01887 * 
Experiment2:ChoiceVAV  -2.1866     0.7929  -2.758  0.00582 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 166.16  on 137  degrees of freedom
Residual deviance: 157.01  on 134  degrees of freedom
AIC: 165.01

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Clearly, the main effects are not being tested here in the way I hoped. I believe that this model, in testing main effects, rather than testing e.g. ChoiceVA=A against ChoiceVA=V across both levels of Experiment, is confining itself to that comparison only when Experiment=1. Can a model be constructed that instead tests the main effects in the way I would like?</p>

<p>This is related to a previous question (<a href=""http://stats.stackexchange.com/questions/117450/logistic-regression-gives-very-different-result-to-fishers-exact-test-why"">Logistic regression gives very different result to Fisher&#39;s exact test - why?</a>), but when I asked it I understand this even worse than I do now and consequently the question was so unclear that I need to start again.</p>
"
"0.140741283725136","0.139706232827494","117664","<p>In order to run Lasso and elastic net multiple regressions on my company's SAS server (which doesn't support R), I've been working on a coordinate descent macro for performing least squares regressions (as described in the 2010 paper <a href=""http://www.jstatsoft.org/v33/i01/paper"" rel=""nofollow"">""Regularization Paths for Generalized Linear Models via Coordinate Descent""</a> by Jerome Friedman, Trevor Hastie, and Rob Tibshirani). </p>

<p>Ideally, I would like the coefficient estimates from my SAS algorithm to match the outcomes from the  <a href=""http://cran.r-project.org/web/packages/glmnet/index.html"" rel=""nofollow"">glmnet package</a> in R written by Friedman, et al. which also implements coordinate descent for least squares regression. </p>

<p>I've decided to test the algorithm on the Fitness data from SAS documentation, with Oxygen as the response variable: </p>

<pre><code>data fitness;
  input Age Weight Oxygen RunTime RestPulse RunPulse MaxPulse @@;
  datalines;
   44 89.47 44.609 11.37 62 178 182   40 75.07 45.313 10.07 62 185 185
   44 85.84 54.297  8.65 45 156 168   42 68.15 59.571  8.17 40 166 172
   38 89.02 49.874  9.22 55 178 180   47 77.45 44.811 11.63 58 176 176
   40 75.98 45.681 11.95 70 176 180   43 81.19 49.091 10.85 64 162 170
   44 81.42 39.442 13.08 63 174 176   38 81.87 60.055  8.63 48 170 186
   44 73.03 50.541 10.13 45 168 168   45 87.66 37.388 14.03 56 186 192
   45 66.45 44.754 11.12 51 176 176   47 79.15 47.273 10.60 47 162 164
   54 83.12 51.855 10.33 50 166 170   49 81.42 49.156  8.95 44 180 185
   51 69.63 40.836 10.95 57 168 172   51 77.91 46.672 10.00 48 162 168
   48 91.63 46.774 10.25 48 162 164   49 73.37 50.388 10.08 67 168 168
   57 73.37 39.407 12.63 58 174 176   54 79.38 46.080 11.17 62 156 165
   52 76.32 45.441  9.63 48 164 166   50 70.87 54.625  8.92 48 146 155
   51 67.25 45.118 11.08 48 172 172   54 91.63 39.203 12.88 44 168 172
   51 73.71 45.790 10.47 59 186 188   57 59.08 50.545  9.93 49 148 155
   49 76.32 48.673  9.40 56 186 188   48 61.24 47.920 11.50 52 170 176
   52 82.78 47.467 10.50 53 170 172
   ;
run;
</code></pre>

<p>Here's my first attempt at writing the code for a simple OLS model. (I realize running a data set inside a macro loop is bad form &amp; slows down execution times - this is just a first pass at the problem.)</p>

<p>For the example here I'm fitting a model for a single value of lambda and alpha in an elastic net model. I'm achieving the closest match to glmnet output when I standardize the six predictor variables using proc standard. Initial values for the coefficients are fit via proc reg. Output coefficient values are then converted back to the original unstandardized scale (scroll to bottom of the code below). </p>

<pre><code>            /* Calculate mean and stnd dev values for standardizing fitness variables. */
            proc means data=fitness mean std;
               var Oxygen Age Weight RunTime RestPulse RunPulse MaxPulse;
               output out=fitness_mean_std;
            run;

            data fitness_mean_std (drop=_TYPE_ _FREQ_);
            set fitness_mean_std;
               if _STAT_ in ('MEAN','STD');
            run;

            %let t=7;
            data _null_;
            set fitness_mean_std;
               if _STAT_='MEAN' then do;
                 array mean[1:&amp;t] Oxygen Age Weight RunTime RunPulse RestPulse MaxPulse;
                 do m = 1 to &amp;t;
                    call symputx(cats('mean',m),mean[m],'g');
                 end;
               end;
               else if _STAT_='STD' then do;
                 array std[1:&amp;t] Oxygen Age Weight RunTime RunPulse RestPulse MaxPulse;
                 do s = 1 to &amp;t;
                     call symputx(cats('std',s),mean[s],'g');
                 end;
               end;
            run;

            /* Create input dataset for coordinate descent macro. */
            proc standard data=fitness mean=0 std=1 out=fitness_stnd;
               var Age Weight RunTime RunPulse RestPulse MaxPulse;
            run;

            proc reg data=fitness_stnd outest=params_stnd;
               model Oxygen = Age Weight RunTime RunPulse RestPulse MaxPulse;
            run;
            quit;

            %let t=6;
            data _null_;
            set params_stnd;
               array x[0:&amp;t] Intercept Age Weight RunTime RunPulse RestPulse MaxPulse;
               do _n_ = 0 to &amp;t;
                  call symputx(cats('p',_n_),x[_n_],'g');
               end;
            run;
            %put &amp;p0 &amp;p1 &amp;p2 &amp;p3 &amp;p4 &amp;p5 &amp;p6;

            %macro assignvar(k);
            data fitness_array (drop=Oxygen Age Weight RunTime RunPulse RestPulse MaxPulse);
            set fitness_stnd;
               y=Oxygen;
               array a[6] Age Weight RunTime RunPulse RestPulse MaxPulse;
               array x[6];
               %do i=1 %to 6;
                 x[&amp;i]=a[&amp;i];
               %end;
            run;
            %mend;
            %assignvar(6)    

            /* Coordinate descent macro. */
            %macro test(dataset=, numvars=, numiter=, lambda=, alpha=);
               %do i=1 %to &amp;numiter;
                 %do j=1 %to &amp;numvars;
                    data &amp;dataset (keep=y x1-x&amp;numvars);
                    set &amp;dataset end=end_data;
                       array x[&amp;numvars] x1-x&amp;numvars;
                       %let gamma = %sysevalf(&amp;lambda*&amp;alpha);

                       /* Calculate partial residuals for fitting coefficients.*/
                       yhat_&amp;j = &amp;p0 - &amp;&amp;p&amp;j*x[&amp;j];
                       %do k=1 %to &amp;numvars;
                            yhat_&amp;j = yhat_&amp;j + &amp;&amp;p&amp;k*x[&amp;k];
                       %end;
                       if _n_=1 then z_&amp;j = x&amp;j*(y - yhat_&amp;j);                                           else z_&amp;j = x&amp;j*(y - yhat_&amp;j) + z_&amp;j; end;
                       if end_data then do;
                          z_avg_&amp;j = z_&amp;j/_n_;
                          if (z_avg_&amp;j &gt; 0 and &amp;gamma &lt; abs(z_avg_&amp;j)) then do;
                             p&amp;j = (z_avg_&amp;j - &amp;gamma)/(1 + &amp;lambda - &amp;gamma);
                             call symputx(""p&amp;j"", p&amp;j, 'g');
                          end;  
                          else if (z_avg_&amp;j &lt; 0 and &amp;gamma &lt; abs(z_avg_&amp;j)) then do;
                             p&amp;j = (z_avg_&amp;j + &amp;gamma)/(1 + &amp;lambda - &amp;gamma);
                             call symputx(""p&amp;j"", p&amp;j, 'g');
                          end;
                          else if &amp;gamma &gt;= abs(z_avg_&amp;j) then do;
                             p&amp;j = 0;
                             call symputx(""p&amp;j"", p&amp;j, 'g');
                          end;
                       end;
                       retain z_&amp;j;
                    run;
                 %end;
               %end;
               %put _user_;
            %mend;
            %test(dataset=fitness_array, numvars=6, numiter=50, lambda=.1, alpha=.5)            

            /* Return regression coefficients in original scale. */
                %let p0_unstand = %sysevalf(&amp;p0-(&amp;p1*&amp;mean2/&amp;std2)-(&amp;p2*&amp;mean3/&amp;std3)-(&amp;p3*&amp;mean4/&amp;std4)-(&amp;p4*&amp;mean5/&amp;std5)-(&amp;p5*&amp;mean6/&amp;std6)-(&amp;p6*&amp;mean7/&amp;std7));
                %let p1_unstand = %sysevalf(&amp;p1/&amp;std2);
                %let p2_unstand = %sysevalf(&amp;p2/&amp;std3);
                %let p3_unstand = %sysevalf(&amp;p3/&amp;std4);
                %let p4_unstand = %sysevalf(&amp;p4/&amp;std5);
                %let p5_unstand = %sysevalf(&amp;p5/&amp;std6);
                %let p6_unstand = %sysevalf(&amp;p6/&amp;std7);

                %put 
                p0_unstand = &amp;p0_unstand 
                p1_unstand = &amp;p1_unstand 
                p2_unstand = &amp;p2_unstand 
                p3_unstand = &amp;p3_unstand 
                p4_unstand = &amp;p4_unstand 
                p5_unstand = &amp;p5_unstand 
                p6_unstand = &amp;p6_unstand;
                /*
                p0_unstand = 107.068671308076 
                p1_unstand = -0.24178321947146 
                p2_unstand = -0.05008520720235
                p3_unstand = -2.47736090772018 
                p4_unstand = -0.16124847253703 
                p5_unstand = -0.03822018686055
                p6_unstand = 0.06524084784678
                */
</code></pre>

<p>The corresponding commands in R:</p>

<pre><code>&gt; elasticnet_fit = glmnet(x, y, family=""gaussian"", lambda=.1, alpha=.5); 
&gt; coef(elasticnet_fit);   
7 x 1 sparse Matrix of class ""dgCMatrix""
                      s0
(Intercept) 105.10824556
x1           -0.22996264
x2           -0.05775625
x3           -2.64766834
x4           -0.01998125
x5           -0.26222028
x6            0.18003526
</code></pre>

<p>My question: The coefficients output from the SAS macro doesn't match the output from glmnet, although the values are close. Is there a flaw in my code or should I not be too concerned? Thanks!</p>
"
"0.0691025985081098","0.0658506226617377","117816","<p>I tried gradient boosting models using both <code>gbm</code> in R and <code>sklearn</code> in Python. However, neither of them can provide the coefficients of the model. For <code>gbm</code> in R, it seems one can get the tree structure, but I can't find a way to get the coefficients. For <code>sklearn</code> in Python, I can't even see the tree structure, not to mention the coefficients. Can anyone give me some help? </p>

<p>After searching online for couple of hours, I still can't find the answer. I can find similar questions since 2009, but no answers. Like the followings:</p>

<ul>
<li><p><a href=""http://r.789695.n4.nabble.com/GBM-package-Extract-coefficients-td963379.html"" rel=""nofollow"">GBM package: Extract coefficients </a> (r-help thread)</p></li>
<li><p><a href=""https://stackoverflow.com/questions/24478868/"">Implementing Gradient Boosted Regression Trees in production - Mathemtically describing the learnt model</a> (SO thread)</p></li>
</ul>

<p>This make me wonder if R and Python are mainly used by academic people, and thus majority of the users don't care about how to use them in industry. For example, if you want to implement the results in some real-time platform which doesn't run Python, what would you do?</p>
"
"0.0609427635336005","0.0580747904139041","117867","<p>I have a basic linear regression model I fitted to a time series. Unfortunately I have to account for autocorrelation and heteroskedasicity in the model and I have done so with the NeweyWest function from the sandwich package in R while analyzing the coefficients. </p>

<p>Now I would like to create prediction intervals using the predict() function (or any other function) while utilizing the NeweyWest matrix/SEs.</p>

<p>As this is the first quesiton I post on here and my experinece in R is very limited here is some information:</p>

<pre><code>LMModel = lm(Return~Sentiment, data=Time Series)
</code></pre>

<p><strong>This is the function I used for my coefficient testing:</strong></p>

<pre><code>coeftest(LMModel , vcov=NeweyWest(LMModel , lag=27, ar.method=""ols""))
</code></pre>

<p><strong>I would like thsi function to use NeweyWest in some way:</strong></p>

<pre><code>predict(LMModel, newdata, interval = ""prediction"", level = 0.95) 
</code></pre>

<p>Thanks a lot in advance!</p>
"
"0.0457070726502004","0.0580747904139041","117910","<p>According to Wikipedia (source of all truth and knowledge...),
<a href=""http://en.wikipedia.org/wiki/Generalized_least_squares#Properties"" rel=""nofollow"">http://en.wikipedia.org/wiki/Generalized_least_squares#Properties</a></p>

<p>a weighted least square regression is equivalent to a standard least square regression, if the variables have been previously ""decorrelated"" using a Cholesky decomposition.</p>

<p>I made up then a very simple example with the function pgls from the package CAPER to test it, where the correlation arises from a phylogeny tree:</p>

<pre><code>tree.mod:
((A:0.2,(B:0.1,C:0.1):0.1):0.1,((E:0.1,F:0.1):0.1,D:0.2):0.1);
</code></pre>

<p>The two approaches are compared here:</p>

<pre><code>library(caper)

## Data
species = c(""A"",""B"",""C"",""D"",""E"",""F"")
gene = c(0.1,0.2,0.3,0.5,0.6,0.7)
pheno = c( 0,0,0,1,1,1)
data=data.frame(species,gene,pheno)

## Phylogeny
tree = read.tree( ""small/tree_small.mod"" )

## GLS regression
cat(""\n     ===&gt; GLS\n"")
cdata   = comparative.data( phy = tree, data = data, names.col = ""species"" )
res = pgls( pheno~gene, cdata )
print(summary(res))

## Cholesky
cat(""\n     ===&gt; Cholesky\n"")
corr = vcv( tree )
cholesky = chol( corr )
invCho = solve( cholesky )
data.gene =  invCho %*% as.vector( data$gene )
data.pheno =  invCho %*% as.vector( data$pheno )
res=lm( data.pheno ~ data.gene )
print(summary(res))
</code></pre>

<p>and yield the outputs:</p>

<pre><code>====&gt; GLS
Coefficients:
        Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept) -0.13000    0.27261 -0.4769  0.65834  
gene         1.63333    0.59489  2.7456  0.05161 .


=====&gt;Cholesky
Coefficients:
        Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)  0.02214    0.28551   0.078    0.942  
data.gene    1.29188    0.35006   3.690    0.021 *
</code></pre>

<p>as you can see the results are different...</p>

<p>Does anyone have a clue why?</p>
"
"0.068136080998913","0.0649295895722714","117912","<p>The summary command of a plm regression with the (factor=""twoways"") argument reports the same coefficients as a plm regression with manual time dummies (and the default factor=""individual"") but a much lower total sum of squares (and therefore a much lower R2). Why is this the case?</p>

<p><strong>Example:</strong></p>

<pre><code>library(plm)

individual &lt;- c(rep(""AT"",5),rep(""BE"",5),rep(""DE"",5))
time &lt;- c(rep(2006:2010,3))
set.seed(123)
y &lt;- c(rep(rnorm(15, sd=2)))
x &lt;- c(rep(rnorm(15, sd=3)))

Test &lt;- data.frame(individual=individual, time=time, x=x, y=y)
year.f = factor(Test$time)
dummies = model.matrix(~year.f)
Test &lt;- cbind(Test,dummies)
remove(dummies,year.f)

fe_manual &lt;- plm(y~ x+year.f2007+year.f2008+year.f2009+
year.f2010,data=Test,model=""within"")
summary(fe_manual)

fe_twoways &lt;- plm(y~ x, data=Test,model=""within"",effect=""twoways"")
summary(fe_twoways)
</code></pre>

<p>The problem doesn't look very bad here (we go from total sum of squares of 38.7 in the manual model to 30.1 in the twoways model), but in my real sample, this problem gets me from an R2 of 42% to one of 3%. The problem gets much bigger with more data.</p>

<p>Any help would be appreciated!</p>

<p>edit: You can see that the twoways model calculates time fixed effects by running:</p>

<pre><code>fixef(fe_twoways)
</code></pre>

<p>Strangely, these coefficients are again different from the manual model.</p>
"
"NaN","NaN","118102","<p>Has any body encountered a problem finding numerical values of b-coefficients while developing partial least squares regression model from spectroscopic data using <code>pls</code> package in R? If so, how could I get those numbers so as I could develop a b-coefficient plot using the numbers?</p>
"
"0.202174206049786","0.192659865793013","118215","<p>This is my first post on StackExchange, but I have been using it as a resource for quite a while, I will do my best to use the appropriate format and make the appropriate edits. Also, this is a multi-part question. I wasn't sure if I should split the question into several different posts or just one. Since the questions are all from one section in the same text I thought it would be more relevant to post as one question.</p>

<p>I am researching habitat use of a large mammal species for a Master's Thesis. The goal of this project is to provide forest managers (who are most likely not statisticians) with a practical framework to assess the quality of habitat on the lands they manage in regard to this species. This animal is relatively elusive, a habitat specialist, and usually located in remote areas. Relatively few studies have been carried out regarding the distribution of the species, especially seasonally.  Several animals were fitted with GPS collars for a period of one year. One hundred locations (50 summer and 50 winter) were randomly selected from each animal's GPS collar data. In addition, 50 points were randomly generated within each animal's home range to serve as ""available"" or ""pseudo-absence"" locations. The locations from the GPS collars are coded a 1 and the randomly selected available locations are coded as 0.</p>

<p>For each location, several habitat variables were sampled in the field (tree diameters, horizontal cover, coarse woody debris, etc) and several were sampled remotely through GIS (elevation, distance to road, ruggedness, etc). The variables are mostly continuous except for 1 categorical variable that has 7 levels.</p>

<p>My goal is to use regression modelling to build resource selection functions (RSF) to model the relative probability of use of resource units. I would like to build a seasonal (winter and summer) RSF for the population of animals (design type I) as well as each individual animal (design type III).</p>

<p>I am using R to perform the statistical analysis.</p>

<p>The <strong>primary text</strong> I have been using isâ€¦</p>

<ul>
<li>""Hosmer, D. W., Lemeshow, S., &amp; Sturdivant, R. X. 2013. Applied Logistic Regression. Wiley, Chicester"".</li>
</ul>

<p>The majority of the examples in Hosmer et al. use STATA, <strong>I have also been using the following 2 texts for reference with R</strong>.</p>

<ul>
<li>""Crawley, M. J. 2005. Statistics : an introduction using R. J. Wiley,
Chichester, West Sussex, England.""</li>
<li>""Plant, R. E. 2012. Spatial Data Analysis in Ecology and Agriculture 
Using R. CRC Press, London, GBR.""</li>
</ul>

<p>I am currently following the steps in <strong>Chapter 4 of Hosmer et al. for the ""Purposeful Selection of Covariates""</strong> and have a few questions about the process. I have outlined the first few steps in the text below to aid in my questions.</p>

<ol>
<li>Step 1: A univariable analysis of each independent variable (I used a
univariable logistic regression). Any variable whose univariable test
has a p-value of less than 0.25 should be included in the first
multivariable model.</li>
<li>Step 2: Fit a multivariable model containing all covariates
identified for inclusion at step 1 and to assess the importance of
each covariate using the p-value of its Wald statistic. Variables
that do not contribute at traditional levels of significance should
be eliminated and a new model fit. The newer, smaller model should be
compared to the old, larger model using the partial likelihood ratio
test.</li>
<li>Step 3: Compare the values of the estimated coefficients in the
smaller model to their respective values from the large model. Any
variable whose coefficient has changed markedly in magnitude should
be added back into the model as it is important in the sense of
providing a needed adjustment of the effect of the variables that
remain in the model. Cycle through steps 2 and 3 until it appears that all of the important variables are included in the model and those excluded are clinically and/or statistically unimportant. Hosmer et al. use the ""<em>delta-beta-hat-percent</em>""
as a measure of the change in magnitude of the coefficients. They
suggest a significant change as a <em>delta-beta-hat-percent</em> of >20%. Hosmer et al. define the <em>delta-beta-hat-percent</em> as 
$\Delta\hat{\beta}\%=100\frac{\hat{\theta}_{1}-\hat{\beta}_{1}}{\hat{\beta}_{1}}$.
Where $\hat{\theta}_{1}$ is the coefficient from the smaller model and $\hat{\beta}_{1}$ is the coefficient from the larger model.</li>
<li>Step 4: Add each variable not selected in Step 1 to the model
obtained at the end of step 3, one at a time, and check its
significance either by the Wald statistic p-value or the partial
likelihood ratio test if it is a categorical variable with more than
2 levels. This step is vital for identifying variables that, by
themselves, are not significantly related to the outcome but make an
important contribution in the presence of other variables. We refer
to the model at the end of Step 4 as the <em>preliminary main effects
model</em>.</li>
<li>Steps 5-7: I have not progressed to this point so I will leave these
steps out for now, or save them for a different question.</li>
</ol>

<p><strong>My questions:</strong> </p>

<ol>
<li>In step 2, what would be appropriate as a traditional level of
significance, a p-value of &lt;0.05 something larger like &lt;.25?</li>
<li>In step 2 again, I want to make sure the R code I have been using for the partial likelihood test is correct and I want to make sure I am interpreting the results correctly. Here is what I have been doing&hellip;<code>anova(smallmodel,largemodel,test='Chisq')</code> If the p-value is significant (&lt;0.05) I add the variable back to the model, if it is insignificant I proceed with deletion?</li>
<li>In step 3, I have a question regarding the <em>delta-beta-hat-percent</em> and when it is appropriate to add an excluded variable back to the model. For example, I exclude one variable from the model and it changes the $\Delta\hat{\beta}\%$ for a different variable by >20%. However, the variable with the >20% change in $\Delta\hat{\beta}\%$ seems to be insignificant and looks as if it will be excluded from the model in the next few cycles of Steps 2 and 3. How can I make a determination if both variables should be included or excluded from the model? Because I am proceeding by excluding 1 variable at a time by deleting the least significant variables first, I am hesitant to exclude a variable out of order.</li>
<li><p>Finally, I want to make sure the code I am using to calculate $\Delta\hat{\beta}\%$ is correct. I have been using the following code. If there is a package that will do this for me or a more simple way of doing it I am open to suggestions.  </p>

<p><code>100*((smallmodel$coef[2]-largemodel$coef[2])/largemodel$coef[2])</code></p></li>
</ol>
"
"0.101414888671788","0.104695817324578","118394","<p>I have a dataset in which individuals were assessed at two time points during the study on a cognitive test, as such I was wondering which statistical model would be more appropriate for my data, either linear regression or mixed effects models? </p>

<p>The average length of follow up for my data is 59 months with a standard deviation of 43.03 (range is 0.63-167 months) with 88 (33%) of people having data for only one time point. </p>

<p>For linear regression, the approach I was thinking utilising was taking the delta of the test score between the two time points and regressing that against time (months between test scores). </p>

<p>If I used mixed effects models, the main issue I have is how to handle individuals who have only wave of data? While I know mixed effects models are especially robust in regards to the analysis of unbalanced data, would 33% missingnes cause issues?</p>

<p>Just sample R code highlighting the output using either linear regression or mixed models.</p>

<pre><code>fm1 &lt;- lm(mmse_difference ~ mmse_months_between*ORgrs_apoe, data = dat.wide)
summary(fm1)

Call:
lm(formula = mmse_difference ~ mmse_months_between * ORgrs_apoe, 
    data = newdat)

Residuals:
    Min      1Q  Median      3Q     Max 
-20.960  -3.957   1.854   5.200  12.550 

Coefficients:
                               Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)                    -2.74185    2.20667  -1.243    0.216
mmse_months_between            -0.01768    0.03051  -0.579    0.563
ORgrs_apoe                      0.35163    1.17782   0.299    0.766
mmse_months_between:ORgrs_apoe -0.01973    0.01748  -1.129    0.261

Residual standard error: 7.3 on 170 degrees of freedom
  (88 observations deleted due to missingness)
Multiple R-squared:  0.08481,   Adjusted R-squared:  0.06866 
F-statistic: 5.251 on 3 and 170 DF,  p-value: 0.001725
Num. obs. 174

fm2 &lt;- lme(mmse ~ mmse_months*ORgrs_apoe, random = ~mmse_months|patientid, data = dat.long, method = ""ML"", na.action = na.exclude)
summary(fm2)
Linear mixed-effects model fit by maximum likelihood
 Data: dat.long 
       AIC      BIC    logLik
  2797.467 2829.537 -1390.733

Random effects:
 Formula: ~mmse_months | patientid
 Structure: General positive-definite, Log-Cholesky parametrization
            StdDev    Corr  
(Intercept) 7.2972822 (Intr)
mmse_months 0.1132399 0.85  
Residual    2.9431616       

Fixed effects: mmse ~ mmse_months * ORgrs_apoe 
                           Value Std.Error  DF   t-value p-value
(Intercept)            24.635821 1.0959420 231 22.479130  0.0000
mmse_months            -0.069918 0.0223198 172 -3.132544  0.0020
ORgrs_apoe             -1.283348 0.6062892 231 -2.116726  0.0354
mmse_months:ORgrs_apoe -0.024952 0.0130561 172 -1.911103  0.0577
 Correlation: 
                       (Intr) mms_mn ORgrs_
mmse_months             0.438              
ORgrs_apoe             -0.882 -0.377       
mmse_months:ORgrs_apoe -0.357 -0.891  0.397

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-3.48643949 -0.31734164  0.07636708  0.26575764  2.49901891 

Number of Observations: 407
Number of Groups: 233 
</code></pre>

<p>Thanks.     </p>
"
"0.106649836183801","0.116149580827808","118621","<p>I am trying to perform a relatively simple multiple regression with a single breakpoint using the <em>segmented</em> package in R. My question is how to handle the interaction between the predictor variables as follows.</p>

<p>My model is: $$Runoff = \beta_0+\beta_1(A+\beta_2P)$$ where there is a breakpoint at some value of $(A+\beta_2P)$. I expect $\beta_0$ and $\beta_1$ to be different on each side of the breakpoint, and $\beta_2$ to be the same (in fact I'll just say <em>a priori</em> that $\beta_2$ should not vary across the breakpoint, but this would be interesting to test)</p>

<p>My question is how to implement this using <em>segmented</em> (or a similar breakpoint linear regression model). Options are:<br>
1. Run some loop through values of $\beta_2$ and create an intermediate variable $\tau = (A+\beta_2P)$, then it's a simple application of <em>segmented</em>. My concern is that I'd like to get a fitted value of $\beta_2$ from the data and this seems like a crude way of doing it.<br>
2. Some clever way of using <em>segmented</em> on the fully explicit model that will estimate $\beta_0, \beta_1,$ and $\beta_2$<br>
any help appreciated</p>

<p><strong>ADDENDUM:</strong></p>

<p>To describe what I'm trying to do. This is for a watershed runoff generation project. My hypothesis is that runoff amount is a function of both precipitation and soil water table position, and that there is a very strong threshold response in the latter. Whenever it rains, the water table goes up, but if it does not reach the threshold no runoff is generated. If enough rain falls to raise the water table above the threshold, then runoff is generated. Thus, runoff is a function of $(Ant + Pcp)$ with a notable breakpoint. </p>

<p>So, in the model as formulated in my original question $\beta_0$ and $\beta_1$ are the slope and intercept of the runoff response, and $\beta_2$ is a coefficient that converts rainfall depth to water table rise, which I assume is something like the porosity of the soil.  </p>

<p>I'm interested in (in order of importance): (1) is this model better than just a linear relationship between runoff and precipitation?; (2) can the model estimate where the water table threshold is?; (3) Can the model estimate the ""porosity"" coefficient (this would be a nice check against physical reality and I think also improve the physical fidelity of the estimated threshold position).  </p>

<p>Below is a graph using a hand-picked value of $\beta_2$, which let me do a piece wise linear regression. Solid line is the piece wise, dashed is a linear regression between runoff and precip alone. I'd like to be able to estimate the parameters of the model ($\beta_0, \beta_1, \beta_2$, and the breakpoint position) from the data. Ultimately, I have three replicates of three watershed types, so I'm interested in comparisons there as well.</p>

<p>Thanks for the help,</p>

<p><img src=""http://i.stack.imgur.com/L5JuQ.jpg"" alt=""enter image description here""></p>
"
"0.091874672876503","0.0875510407188402","119738","<p>I've used an ordinary least square linear regression model in R that looks something like this:</p>

<pre><code>ols &lt;- lm(DV ~ IV1 + IV2)
</code></pre>

<p>When I type this:</p>

<pre><code>summary(ols) 
</code></pre>

<p>I get a table showing Estimate, Std Error, t value and P(>|t|) for each coefficient. I also get the residual standard error, multiple r-square, adjusted r-square, f-statistic, and p-value for the model.</p>

<p>And I've used a robust linear regression model that looks something like this:</p>

<pre><code>roblm &lt;- rlm(DV ~ IV1 + IV2) 
</code></pre>

<p>When I type this:</p>

<pre><code>summary(roblm)
</code></pre>

<p>I get a table showing Value, Std Error, and t value for each coefficient. But I don't get a p-value for each coefficient. Similarly, I get the residual standard error for the model, but I don't get multiple r-square, adjusted r-square, f-statistic, and p-value for the model.</p>

<p>Why aren't these additional statistics provided for the robust linear regression model? Do they not make sense in the context of this model? If these statistics do make sense, how would I go about getting them? Any help would be greatly appreciated.</p>
"
"0.0806196982594614","0.0658506226617377","120008","<p>I am interested in fitting an ARIMAX model using R.
As known, ARIMAX can be understood as a composition of ARIMA models and regression models with exogenous (independent) variables. I have a time series $Y_i$, and want to estimate the ARIMA and nonlinear coefficients. The nonlinear model is the following:</p>

<p>$y_i=Î²_0+Î²_1t_i+Î²_2d+Î²_3 sin(2Ï€t_i/Î²_4 )+Î²_5 (-1^{t_i})+Îµ_i$,  nonlinear 
regression with an exogenous variable.
Where 
$t_i$ =1, 2â€¦, 60
and</p>

<p>d = dummy variable with 20 0's and 40 number 1's</p>

<pre><code>d=c(rep(0,20),rep(1,40))
</code></pre>

<p>And an ARIMA model (1,1,1) for $Y_i$. Therefore, I want to estimate simultaneously the $Î²_i$ and the ARIMA coefficients in order to avoid the confusion between the exogenous coefficients and ARIMA coefficients.  I know that $arima()$ can deal with this formulation but, how do the nonlinear model can be set within function function?. It seems that the <em>xreg</em> term only deals with linear parameters.</p>
"
"0.0621994475718397","0.0592723347638087","120201","<p>Being aware of <a href=""http://www.ssicentral.com/lisrel/techdocs/HowLargeCanaStandardizedCoefficientbe.pdf"" rel=""nofollow"">that article</a>, I am curious about the question how big standardized coefficients can get. I had a discussion with my professor about that issue and she was arguing standardized coefficients (beta) in multiple linear regressions can not become greater than |1|. I have also heard that predictors with standardized coefficients greater than 1 should not be be included/appear in multiple linear regression. When I recently estimated a multiple linear regression in R using lm(), I estimated the standardized coefficients with lm.beta() function from the package 'lm.beta'. In the results I could observe a standardized coefficient greater than one. Right now I am just not sure about what is the truth.</p>

<p>Can standardized coefficients become greater than |1|?
If yes, what does that mean and should they be excluded from the model?
If yes, why?</p>

<p>I would be very thankful, if somebody could make this issue clear for me. </p>

<p>Thanks in advance!!</p>
"
"0.0545088647991304","0.0649295895722714","120443","<p>I have run a linear regression with the following equation (in r):</p>

<pre><code>lm(formula = logTotal ~ Continent + logArea + Method + Servs)
</code></pre>

<p>where <code>Total</code> is $/ha/year (numeric), <code>Area</code> is hectare (numeric), <code>Continent</code> and <code>Method</code> are factors and <code>Servs</code> is numeric.  It returns the output:</p>

<pre><code>Call:
lm(formula = logTotal ~ Continent + logArea + Method + Servs)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.99416 -0.26931 -0.00622  0.28885  1.19875 

Coefficients:
                       Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)            -1.82886    0.71446  -2.560 0.016903 *  
ContinentAsia           3.82452    0.60471   6.325 1.28e-06 ***
ContinentAustralasia    4.52516    0.96517   4.688 8.35e-05 ***
ContinentEurope         2.18022    0.48260   4.518 0.000130 ***
ContinentGlobal         2.44750    0.74092   3.303 0.002881 ** 
ContinentNorth America  2.35244    0.55281   4.255 0.000256 ***
ContinentSouth America  3.67853    0.61454   5.986 2.99e-06 ***
logArea                 0.03643    0.03583   1.017 0.318911    
MethodCVM              -0.18171    0.43296  -0.420 0.678300    
MethodOther hedonic    -1.53284    0.79781  -1.921 0.066165 .  
MethodValue Transfer    0.98101    0.29773   3.295 0.002941 ** 
Servs                   0.10723    0.04273   2.509 0.018948 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.552 on 25 degrees of freedom
  (65 observations deleted due to missingness)
Multiple R-squared:  0.7582,    Adjusted R-squared:  0.6518 
F-statistic: 7.127 on 11 and 25 DF,  p-value: 2.501e-05  
</code></pre>

<p>I wish to predict <code>Total</code> based on various inputs, however I'm a bit lost on fully understanding the output. If I wished to predict ""Total"" on the basis of:</p>

<pre><code>Continent:Global, Area:1 hectare, Method:CVM, Servs:11
</code></pre>

<p>is the following equation correct?</p>

<pre><code>exp(Total) = 2.44750 + exp(1*0.03643) - 0.18171 + (11*0.10723)
</code></pre>

<p>I have read UCLA's statistics help site's <a href=""http://www.ats.ucla.edu/stat/mult_pkg/faq/general/log_transformed_regression.htm"" rel=""nofollow"">FAQ on log transformed regression</a>. I feel like I've oversimplified it but I just keep reading that link over and over and still not fully understanding.  Also read <a href=""http://stats.stackexchange.com/questions/20397/how-to-interpret-logarithmically-transformed-coefficients-in-linear-regression"">How to interpret logarithmically transformed coefficients in linear regression?</a>.</p>
"
"NaN","NaN","120472","<p>I'm running a beta regression in <code>R</code> where the outcome variable is continuous but bounded between 0-1. I need the results to be understandable to an audience with a very basic stats background. I need some help interpreting the coefficients. My model is very basic and look like:</p>

<p><code>betareg(Y ~ ANE + factor(Year) +factor(Month) +factor(Sector))</code></p>

<p>I'm not sure how to interpret the coefficient on <code>ANE</code>. Below are the results I got for the <code>ANE</code> variable:</p>

<blockquote>
<pre><code>Coefficients (mean model with logit link):
              Estimate  Std. Error  z value  Pr(&gt;|z|) 
(Intercept) 1.036e+00 : 7.735e-02 : 13.395 : &lt; 2e-16 ***
ANE        -1.693e-05 : 6.948e-06 : -2.437 : 0.014803 *
</code></pre>
</blockquote>
"
"0.13342816860689","0.115039593499105","120549","<p>It is a basic question but I could not find clear answer on my reading. I am trying to find independent predictors of Infant.Mortality in data frame 'swiss' in R. </p>

<pre><code>&gt; head(swiss)
             Fertility Agriculture Examination Education Catholic Infant.Mortality
Courtelary        80.2        17.0          15        12     9.96             22.2
Delemont          83.1        45.1           6         9    84.84             22.2
Franches-Mnt      92.5        39.7           5         5    93.40             20.2
Moutier           85.8        36.5          12         7    33.77             20.3
Neuveville        76.9        43.5          17        15     5.16             20.6
Porrentruy        76.1        35.3           9         7    90.57             26.6
</code></pre>

<p>Following are the results using lm and I find only Fertility to be a significant predictor: </p>

<pre><code>&gt; fit = lm(Infant.Mortality~., data=swiss)
&gt; summary(fit)

Call:
lm(formula = Infant.Mortality ~ ., data = swiss)

Residuals:
    Min      1Q  Median      3Q     Max 
-8.2512 -1.2860  0.1821  1.6914  6.0937 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  8.667e+00  5.435e+00   1.595  0.11850
Fertility    1.510e-01  5.351e-02   2.822  0.00734    #  &lt;&lt;&lt;&lt; NOTE P VALUE HERE
Agriculture -1.175e-02  2.812e-02  -0.418  0.67827
Examination  3.695e-02  9.607e-02   0.385  0.70250
Education    6.099e-02  8.484e-02   0.719  0.47631
Catholic     6.711e-05  1.454e-02   0.005  0.99634

Residual standard error: 2.683 on 41 degrees of freedom
Multiple R-squared:  0.2439,    Adjusted R-squared:  0.1517 
F-statistic: 2.645 on 5 and 41 DF,  p-value: 0.03665
</code></pre>

<p>Following are the graphs:</p>

<pre><code>plot(fit)
</code></pre>

<p><img src=""http://i.stack.imgur.com/lopHb.png"" alt=""enter image description here""></p>

<p>On performing stepwise regression, following are the results: </p>

<pre><code>&gt; step &lt;- stepAIC(fit, direction=""both""); 
Start:  AIC=98.34
Infant.Mortality ~ Fertility + Agriculture + Examination + Education + 
    Catholic

              Df Sum of Sq    RSS     AIC
- Catholic     1     0.000 295.07  96.341
- Examination  1     1.065 296.13  96.511
- Agriculture  1     1.256 296.32  96.541
- Education    1     3.719 298.79  96.930
&lt;none&gt;                     295.07  98.341
- Fertility    1    57.295 352.36 104.682

Step:  AIC=96.34
Infant.Mortality ~ Fertility + Agriculture + Examination + Education

              Df Sum of Sq    RSS     AIC
- Examination  1     1.320 296.39  94.551
- Agriculture  1     1.395 296.46  94.563
- Education    1     5.774 300.84  95.252
&lt;none&gt;                     295.07  96.341
+ Catholic     1     0.000 295.07  98.341
- Fertility    1    72.609 367.68 104.681

Step:  AIC=94.55
Infant.Mortality ~ Fertility + Agriculture + Education

              Df Sum of Sq    RSS     AIC
- Agriculture  1     4.250 300.64  93.220
- Education    1     6.875 303.26  93.629
&lt;none&gt;                     296.39  94.551
+ Examination  1     1.320 295.07  96.341
+ Catholic     1     0.255 296.13  96.511
- Fertility    1    79.804 376.19 103.758

Step:  AIC=93.22
Infant.Mortality ~ Fertility + Education

              Df Sum of Sq    RSS     AIC
&lt;none&gt;                     300.64  93.220
- Education    1    21.902 322.54  94.525
+ Agriculture  1     4.250 296.39  94.551
+ Examination  1     4.175 296.46  94.563
+ Catholic     1     2.318 298.32  94.857
- Fertility    1    85.769 386.41 103.017
&gt; 
&gt; 
&gt; step$anova
Stepwise Model Path 
Analysis of Deviance Table

Initial Model:
Infant.Mortality ~ Fertility + Agriculture + Examination + Education + 
    Catholic

Final Model:
Infant.Mortality ~ Fertility + Education


           Step Df     Deviance Resid. Df Resid. Dev      AIC
1                                      41   295.0662 98.34145
2    - Catholic  1 0.0001533995        42   295.0663 96.34147
3 - Examination  1 1.3199421028        43   296.3863 94.55125
4 - Agriculture  1 4.2499886025        44   300.6363 93.22041
&gt; 
&gt; 
</code></pre>

<p>Summary shows Education also has trend towards significant association: </p>

<pre><code>summary(step)

Call:
lm(formula = Infant.Mortality ~ Fertility + Education, data = swiss)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.6927 -1.4049  0.2218  1.7751  6.1685 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  8.63758    3.33524   2.590 0.012973
Fertility    0.14615    0.04125   3.543 0.000951
Education    0.09595    0.05359   1.790 0.080273

Residual standard error: 2.614 on 44 degrees of freedom
Multiple R-squared:  0.2296,    Adjusted R-squared:  0.1946 
F-statistic: 6.558 on 2 and 44 DF,  p-value: 0.003215
</code></pre>

<p>What do I conclude? Is Education an important predictor or not?</p>

<p>Also, do the graphs using plot(fit) add any significant information?</p>

<p>Thanks for your help.</p>

<hr>

<p>Edit: 
I ran shapiro test on all columns and found 2 are not normally distributed: </p>

<pre><code>Fertility : P= 0.3449466 (Normally distributed) 
Agriculture : P= 0.1930223 (Normally distributed) 
Examination : P= 0.2562701 (Normally distributed) 
Education : P= 1.31202e-07 (--- NOT Normally distributed! ---) 
Catholic : P= 1.20461e-07 (--- NOT Normally distributed! ---) 
Infant.Mortality : P= 0.4978056 (Normally distributed) 
</code></pre>

<p>Does that make a difference? </p>
"
"0.12583105938145","0.119909435959664","120892","<p>I have on question regarding standardized coefficients (beta) in linear models. I have already asked one question <a href=""http://stats.stackexchange.com/questions/120201/magnitude-of-standardized-coefficients-beta-in-multiple-linear-regression"">here</a>. From the answers I assume that I should use R's <code>scale()</code> function on the dependent variable as well as on all independent variables (IV), to estimate the standardized coefficients for the model. But when I used the <code>scale()</code> function on an IV, which belongs to the factor class I get following error message:</p>

<p><code>Error in colMeans(x, na.rm = TRUE) : 'x' must be numeric</code></p>

<p>To illustrate my problem here is a MWE:</p>

<p>First the linear model with unstandardized coefficients:</p>

<pre><code>&gt; data(ChickWeight)
&gt; aa &lt;- lm(weight ~ Time + Diet, data=ChickWeight)
&gt; summary(aa)

Call: 
lm(formula = weight ~ Time + Diet, data = ChickWeight)

Residuals:
     Min       1Q   Median       3Q      Max 
-136.851  -17.151   -2.595   15.033  141.816 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  10.9244     3.3607   3.251  0.00122 ** 
Time          8.7505     0.2218  39.451  &lt; 2e-16 ***
Diet2        16.1661     4.0858   3.957 8.56e-05 ***
Diet3        36.4994     4.0858   8.933  &lt; 2e-16 ***
Diet4        30.2335     4.1075   7.361 6.39e-13 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 35.99 on 573 degrees of freedom
Multiple R-squared:  0.7453,    Adjusted R-squared:  0.7435 
F-statistic: 419.2 on 4 and 573 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>Now I want to estimate the standardized coefficients using the <code>scale</code> function, which results in following error message:</p>

<pre><code>&gt; bb &lt;- lm(scale(weight) ~ scale(Time) + scale(Diet), data=ChickWeight)
Error in colMeans(x, na.rm = TRUE) : 'x' must be numeric
</code></pre>

<p>As I figured out by myself the error message appears, because <code>Diet</code> belongs to the factor class and is not a numeric variable as required from the <code>scale()</code> function. I tried the following alternatively by including the <code>Diet</code> variable without <code>scale()</code>:</p>

<pre><code>&gt; cc &lt;- lm(scale(weight) ~ scale(Time) + Diet, data=ChickWeight)
&gt; summary(cc)

Call:
lm(formula = scale(weight) ~ scale(Time) + Diet, data = ChickWeight)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.92552 -0.24132 -0.03652  0.21151  1.99538 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -0.24069    0.03415  -7.048 5.25e-12 ***
scale(Time)  0.83210    0.02109  39.451  &lt; 2e-16 ***
Diet2        0.22746    0.05749   3.957 8.56e-05 ***
Diet3        0.51356    0.05749   8.933  &lt; 2e-16 ***
Diet4        0.42539    0.05779   7.361 6.39e-13 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.5064 on 573 degrees of freedom
Multiple R-squared:  0.7453,    Adjusted R-squared:  0.7435 
F-statistic: 419.2 on 4 and 573 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>My question now is, if this is the right way to estimate the standardized coefficients for a model with both numeric and factor variables?</p>

<p>Thank you very much in advance for an answer.</p>

<p>Regards,</p>

<p>Magnus</p>
"
"0.143058729415231","0.142253603433141","121255","<p>I am working on a dataset with a continuous response (which could be dichotomized), one continuous covariate, and multiple categorical variables. The continuous covariate (weight) is directly correlated to the response, and must be accounted for so that we can determine which of the categorical variables are most influential to the response. Here is <a href=""http://pastebin.com/891mheRf"" rel=""nofollow"">example data</a>.</p>

<p>Each row is an individual subject, with the continuous response, the covariate of underlying primary importance (weight), then 10 categorical variables that are to be tested (individuals can score yes = 1 to multiple categories). </p>

<p>My first thought in working with this data was a linear model, with stepwise elimination of categorical variables.</p>

<pre><code> lm(Response~Weight+var1+var2...+var11)
</code></pre>

<p>However, I believe there is extensive collinearity, since some variables may be eliminated early, but then are significant if you add them back into the model at the end. I'm curious if there is a better way to approach this data in R, that may help sort through which of the variables are of most importance to influencing the response. My two thoughts are</p>

<p>1) Building a single model with the continuous covariate and 5 categorical variables that were selected to be of most interest before the study, and refrain from any stepwise reduction of this model</p>

<p>2) Some sort of princicpal component regression, which I know little about at this point and thus wanted to ask advice before proceeding down that path</p>

<p>To help visualize the data, and the effect of Weight on the Response, I've constructed the follow plots. In the second plot, I attempt to control for the natural Response~Weight relationship.</p>

<pre><code> #GRAPH
 library(ggplot2)
 library(reshape2)

 Data &lt;- read.table(""Fake Data.txt"",header=TRUE)
 #Creating long format for ggplot2
 Data2&lt;-melt(Data, id.vars = c(""Subject"",""Response"",""Weight""), measure.vars = c(""var1"",""var2"",""var3"",""var4"",""var5"",""var6"",""var7"",""var8"",""var10"",""var11""))

 #Adding in weight to the varibles to be plotted
 Data2&lt;-rbind(Data2,Data2[1:31,])
 levels(Data2$variable)&lt;-c(levels(Data2$variable),""Weight"")
 Data2[311:341,4]&lt;-""Weight""
 Data2[311:341,5]&lt;-1

 #Removing rows where the categorical variable is 0=No
 for(i in 1:length(Data2[,1])){
 if(Data2[i,5]==0)Data2[i,]&lt;-NA
 }
 Data3&lt;-na.omit(Data2)

 #Plotting Response vs Weight for each 'Yes' group for the categorical variables
  scatter &lt;- ggplot(Data3, aes(Weight, Response, colour = variable))
 scatter + geom_point(aes(color = variable), size = 3) + geom_smooth(method = ""lm"",aes(fill = variable), alpha = 0.1) + facet_wrap(~variable)+ guides(fill=FALSE,color=FALSE) 
</code></pre>

<p><img src=""http://i.stack.imgur.com/AgQog.jpg"" alt=""enter image description here""></p>

<pre><code> #Zeroing the Response~Weight relationship to remove its influence. Correction coefficients from linear model fit to Response~Weight
 Data4&lt;-Data3
 Data4$Response&lt;-Data4$Response-(0.01494*(Data4$Weight)+ 84.67715)

 #Plotting Response vs Weight for each 'Yes' group for the categorical variables for zeroed Response~Weight relationship (as seen in bottom right facet)
 scatter2 &lt;- ggplot(Data4, aes(Weight, Response, colour = variable))
 scatter2 + geom_point(aes(color = variable), size = 3) + geom_smooth(method = ""lm"",aes(fill = variable), alpha = 0.1) + facet_wrap(~variable)+ guides(fill=FALSE,color=FALSE) 
</code></pre>

<p><img src=""http://i.stack.imgur.com/JfCrn.jpg"" alt=""enter image description here""></p>

<p>This second plot helps to show how, when the Response~Weight relationship is controlled for, variables like 'var10' have no influence on the response, while variables like 'var11' have all individuals below that zero-centered mean. Thus, from a visual test, I could identify var11 as a categorical variable of interest that negatively influences our response.</p>

<p>Additionally, this plot shows some of the confounding in this dataset, as you can see certain categorical variables 'clump'/are only documented in certain weight ranges. This is due to the underlying biology.</p>

<p>As a final note, I wonder if it is appropriate to use the corrected response in the second plot as the 'Response' for a linear model, thus eliminating the need for a 'Weight' covariate, or if it is incorrect to use such a transformation</p>

<p>Any thoughts are much appreciated</p>
"
"0.0646395620382857","0.0615976171764886","121566","<p>Here is a problem that was puzzling me. Suppose I simulate the AR(2) process with constant and trend using the code below (I apologize for inefficiency and inelegance - the aim was to get job done at this point; also - it may seem strangely constructed, but it has some other purpose too for which is irrelevant here).</p>

<p>My question is - why the constant estimates are so poor? The true value is <code>70</code> but if we average 1000 regressions each over 1000 observations I get an average of <code>381.9234</code>. </p>

<p>Is it because I interpret something wrong or the did I make a mistake somwhere?</p>

<pre><code>set.key(123)

#parameter values
V=7
P=10
S=4
r1 = 50/(50+P)
r2 = V/(30+V)
mu = 10*P
l2 = 10*(S+V)
a0 = 10*V
d0 = 10*P
a1 = 0
d1 = P+V
s2 = 2*(P+V+S)

#simulate and estimate the parameters
data&lt;-NULL
data50 &lt;- NULL

for (firm in 1:1000){

  y_zero &lt;- rnorm(1, mean = mu, sd = l2)
  gamma_0 &lt;- rnorm(1, mean = a0, sd = d0)
  gamma_1 &lt;- rnorm(1, mean = a1, sd = d1)

  y_first &lt;- r1*y_zero + gamma_0 + gamma_1 + rnorm(1, mean = 0, sd = s2)
  y_second &lt;- r1*y_first - r2*(y_first - y_zero) + gamma_0 + 2*gamma_1 + rnorm(1, mean = 0, sd = s2)
  y_third &lt;- r1*y_second - r2*(y_second - y_first) + gamma_0 + 3*gamma_1 + rnorm(1, mean = 0, sd = s2)
  y_fourth &lt;- r1*y_third - r2*(y_third - y_second) + gamma_0 + 4*gamma_1 + rnorm(1, mean = 0, sd = s2)

  column &lt;- cbind(""firm"" = firm, ""t"" = 1:4, ""y"" = c(y_first, y_second, y_third, y_fourth))

  data &lt;- rbind(data, column)
  ###################################################################################
  firm50 &lt;- NULL

  y_fifth &lt;- r1*y_fourth - r2*(y_fourth - y_third) + gamma_0 + 5*gamma_1 + rnorm(1, mean = 0, sd = s2)
  y_sixth &lt;- r1*y_fifth - r2*(y_fifth - y_fourth) + gamma_0 + 6*gamma_1 + rnorm(1, mean = 0, sd = s2)

  y_previous1 &lt;- y_sixth
  y_previous2 &lt;- y_fifth

  firm50 &lt;- cbind(""firm"" = firm, ""t"" = c(5,6), ""y"" = c(y_fifth, y_sixth), ""ro1-ro2"" = c(y_fourth, y_fifth), ""ro2"" = c(y_third, y_fourth))

  for (run in 1:5000){
    time &lt;- run + 6

    the_y &lt;- r1 * y_previous1 - r2 * (y_previous1 - y_previous2) + gamma_0 + time*gamma_1 + rnorm(1, mean = 0, sd = s2)

    firm50 &lt;- rbind(firm50, cbind(""firm"" = firm, ""t"" = time, ""y"" = the_y, ""ro1-ro2"" = y_previous1, ""ro2"" = y_previous2))

    y_previous2 &lt;- y_previous1
    y_previous1 &lt;- the_y

  }
  firm50 &lt;- cbind(firm50, ""gamma0"" = gamma_0, ""gamma1"" = gamma_1)
  data50 &lt;- rbind(data50, firm50)
}

#estimate the coefficients
data &lt;- data.table(as.data.frame(data50))[t %in% c(4000:5000)]
coefs &lt;- NULL
for(i in 1:1000){
  coefs &lt;- rbind(coefs, t(coef(arima(data[firm==i, y], c(2,0,0), xreg = data[firm==i, t])))
}
</code></pre>
"
"0.0430930413588572","0.0410650781176591","121823","<p>I am quite new in the R universe, so please excuse me if the question is too simple..</p>

<p>I would like to perform a logistic regression on a marketing data set (only categorical variables), of the form [outcome, X1,X2,X3,X4,X5,X6]</p>

<p>I split the data set into a training set and a validation set.</p>

<p>My problem: Predictor X1 has originally 3 levels. The model using glm retains only 2 of these 3 levels.</p>

<p>When I try to run the model on the validation set (where X1 still has 3 levels) I get an error message stating that the factor X1 has now a new level. </p>

<p>How can I prevent the glm function from excluding factor levels? I don't mind if their coefficients are set to zero. </p>

<p>Thanks for any help on this. Tried all sites, but to no avail. </p>
"
"0.0867230728520531","0.0826418755551823","122039","<p><strong>SOLVED</strong>: an elastic net model, as any other logistic regression model, will not generate more coefficients than input variables. Check Zach's answer to understand how from an (apparent) low number of inputs, more coefficients can be generated. The cause of this question was a code bug, as the users pointed out.</p>

<p>This is a simple question. I've fitted a model with 1334 variables using elastic net to perform feature selection and regularization. I'm now trying to interpret the obtained coefficients in order to find correlations between the input variables and the output. The only problem is that instead of the (expected) 1335 coefficients (intercept+1334), extracting the coefficients through <code>coef(model,s=""lambda.min"")</code> yields around 1390 coefficients. This seems highly counterintuitive and stops me from mapping a single coefficient to a single input variable, so I suppose I'm not understanding some of the insides of the elastic net. Any idea would be very helpful. Thanks in advance.</p>

<p>PS: just in case someone wonders so, I've not included interaction terms nor any synthetic variable, just the original 1334 ones.</p>

<p>PS2: elastic net references:</p>

<ul>
<li>Mathematical paper: <a href=""http://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&amp;%20Hastie.pdf"" rel=""nofollow"">http://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&amp;%20Hastie.pdf</a></li>
<li>R package tutorial: <a href=""http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html"" rel=""nofollow"">http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html</a></li>
</ul>

<p>PS3: about the code used to fit the model:</p>

<p>it is a 250 line script, so unless you specifically need it, I think it'd only clutter the question. Basically, the algorithm takes as an input a data frame of 1393 colums, where the last one is the target variable and the first 1392 are the input variables. So, after separating those into two matrices, input and output, the actual model fitting is done in this call:</p>

<p><code>cv.glmnet(x=input_matrix,y=output_matrix,family=""binomial"",type.measure=""auc"")</code></p>

<p>If you need to, I can actually generate a reproducible file with the data I use and the whole script. </p>
"
"0.170321786344734","0.162306424444284","122270","<p>This is a long post but it is <em>not</em> conceptually difficult. Please bear with me.</p>

<p>I am trying to model the <strong>seasonality</strong> of production volume of an agricultural commodity. I do not care about the structure of the model (as long as it produces sensible output) nor about explaining seasonality; my goal is to adjust the series for seasonality before proceeding to forecasting. </p>

<p>I know the seasonality is due to supply (weather is a key determinant) as well as demand (higher demand around Christmas and Easter, etc.). </p>

<p>My <strong>data is weekly</strong>, and I am following the approach by prof. Hyndman of modelling the time series as an <strong>ARMA process with Fourier terms</strong> to account for seasonality (see <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">here</a>). I also include Christmas and Easter dummies together with Fourier terms. </p>

<p>You might think I do not need a Christmas dummy since Christmas is always the 25th of December and thus Fourier terms would account for it, unlike for Easter which jumps back and forth in March-April from year to year. However, Christmas sometimes occur in week 51 and sometimes in week 52 (at least in my calendar), and Fourier terms will not account for that; hence a Christmas dummy is actually relevant.</p>

<p>Actually, I would like to include up to four dummies for Christmas (from two weeks before Christmas to one week after Christmas) as the seasonal effect is not concentrated on just the Christmas week but also spread out a little in time. The same is with Easter.</p>

<p>Thus I end up with $4+4=8$ dummies, up to 25 pairs of Fourier terms and an unknown ARMA order, which could be as high as (5,5), perhaps. This gives me an <strong>awfully large pool of models to choose from</strong>. Thus I have trouble replicating the approach used in the source above because of <strong>limited computational resources</strong>. Recall that in the original source there were no dummies, so the model pool was $2^{4+4}=256$ times smaller than mine and it was possible to try out a reasonably large proportion of all possible models, and then choose one of them by AIC. </p>

<p>Even worse, I would like to repeat my exercise multiple times (100 times) using a rolling window on the data I have. Thus the <strong>computational requirements</strong> for my exercise <strong>become exorbitant</strong>. I am looking for a solution, and hope someone could help me out.</p>

<p>Here are a few ideas I have:</p>

<ol>
<li><p>Drop the ARMA terms all together. </p>

<ul>
<li>This will make the estimates of coefficients on Fourier terms and dummies biased and inconsistent, thus the seasonal component (the Fourier terms times their coefficients plus dummies times their coefficients) might be rubbish. However, if ARMA patterns are pretty weak, maybe the bias and inconsistency of the coefficient estimates will not be too bad?</li>
</ul></li>
<li><p>Estimate only some models from the pool of all possible models. E.g. select the ARMA order by <code>auto.arima()</code> function in <code>forecast</code> package in <code>R</code>: <code>auto.arima()</code> does not consider all possible ARMA orders but only non-restricted ones (e.g. an AR(2) model with the AR(1) coefficient restricted to zero is not considered) and takes some other shortcuts. Also, I could force all dummies to be present without questioning whether all of them are really relevant. </p>

<ul>
<li>This indeed reduces the model pool considerably (by a factor of $2^{4+4}=256$ only due to dummies), but it comes at a cost of likely inclusion of irrelevant variables. Not sure how bad that could be. </li>
</ul></li>
<li><p>Use a different estimation method: conditional sum of squares (CSS) instead of maximum likelihood (ML) when possible, something else? </p>

<ul>
<li>Unfortunately, CSS in place of ML does not seem to reduce the computational time enough in practice (I tested a few examples).</li>
</ul></li>
<li><p>Do the estimation in two (or more) steps: select the ARMA order independently of the number of Fourier terms and inclusion/exclusion of dummies, then select the number of Fourier terms, then select the number of dummies, or something similar. </p>

<ul>
<li>This would reduce the model pool to the following: the number of all possible ARMA models (e.g. $2^{5+5}$) plus 26 (0 pairs of Fourier terms, 1 pair of Fourier terms, ..., 25 pairs of Fourier terms) plus $2^{4+4}$ (all combinations of dummies). This particular division into stages certainly does not make sense, I admit. But perhaps there exists a smart way to divide model selection into steps? I guess we need mutual orthogonality of regressors to be able to consider their inclusion/exclusion from the model one by one, or something similar. I need some insight here...</li>
</ul></li>
<li><p>Do something along the lines of Cochrane-Orcutt correction: <strong>(1)</strong> estimate a model without any ARMA terms (thus a simple OLS regression of production volume on Fourier terms and dummies); <strong>(2)</strong> examine the ARMA patterns in residuals and determine the relevant ARMA order; <strong>(3)</strong> estimate the corresponding ARMA model with Fourier terms and dummies as exogenous regressors. <br>(I know Cochrane-Orcutt does not consider ARMA(p,q) but just AR(1), but I thought maybe I could generalize it? Not sure, perhaps I am making a mistake here?)</p>

<ul>
<li>This would reduce the model pool to be estimated by a factor of $2^{5+5}$ if we would otherwise consider ARMA(5,5) and all sub-models nested in it, which is very nice. However, Cochrane-Orcutt correction is not as good as getting the model order correct in the first place. (I lack a theoretical argument there, but I suspect you do not always end up with the correct ARMA order if you use Cochrane-Orcutt or something similar.)</li>
</ul></li>
</ol>

<p>I would appreaciate any comments on the ideas above and, even more, your own ideas on how to save computational time.</p>

<p><strong>Thank you!</strong></p>

<p>P.S. Here is the data (2007 week 1 to 2012 week 52): <br><code>6809  8281  8553  8179  8257  7426  8098  8584  8942  9049  9449 10099 10544  6351  8667  9395  9062  8602 10500  8723  9919  9120 10068  9937  9998  9379  9432  8751 10250  7690  6857  8199  8299  9204 11755  9861 10193 10452  9769 10289 10399 10181 10419 10324 11347 11692 12571 13049 13712 15381 13180  3704  5833  9909  8979  8883  9599  9034  9617  9710  9878 10872 12246  7122  8071  9656  9157  9729  8814  8704 10724  9294 10176 10394  9633 10872 10353  9400  9692  9244  8921  7698  8191  8494  8681  9420  8947  9287 10043  9953 10136 10235 10027 10137 11187 10383 11321 12023 11824 13752 13907 14658 14634  5353  5603 11607  9512  9537  9775  8558  9227  9146  9673 10225 10794 10646 10545 12375  7742  9269 11134  9588 10890 10061 10244 11275  9428 10301 11039 10779 10580  9215  9538  9717  9966 10678 10821 10898 11458 11090 12080 10904 12990 12325 12257 12246 12351 12408 11732 12702 14181 14963 15027 16657 17766  8265  7552 11067 10808 10891 10967 10826 10929 10891  9293 10902 11640 11921 13320  7751  9929 11632 11073 10955 11785 10533 10629 11535 12513 11780 12639 12280 12245 11817 11275 10242 10625  9884 10182 11048 12120 12697 13132 12597 13516 13536 13279 13935 13539 14335 13748 13988 14497 15255 15346 17626 17716 10713  7752  9309 11133 11853 11570 10320 10817 10982 10585 10941 12801 12165 11899 10941 12729 13763  8130  9888 11889 13036 10413 12609 10992 13201 11833 13159 12880 12160 13277 12758 10897 11197 12169 12639 13788 13819 14804 14613 14992 16079 16648 16376 16207 15906 15301 16194 17327 18890 18710 19953 22369 16499  9721 11785 12528 13994 12990 13226 12883 13613 14382 15829 15525 16395 17175 18334  9286 15530 16510 15370 13019 16526 14425 16542 15328 16993 17607 16734 15826 16555 16064 14412 14355 15238 15064 15317 15533 16542 16474 17354 18359 18459 17312 17499 18298 17301 17795 17867 18453 19372 19394 21897 23674 23219  7356</code></p>
"
"0.052777981396926","0.0335294958785986","122580","<p>I'm running a logistic regression with backard selection method. I get coefficients with p-values>.10. Here's an example:</p>

<pre><code>    DF   Estimate   Error   Chi-Square  Pr &gt; ChiSq  Estimate    Exp(Est)
Intercept   1   -30,32       11,48       6,97        0,01            -     
v1  1    0,001       0,00        9,70        0,00        0,10        1,00   
v2  1   -0,001       0,00        2,84        0,09       -0,07        1,00   
v3  1    0,000       0,00        0,12        0,73        0,01        1,00   
v4  1   -0,000       0,00        0,11        0,74       -0,01        1,00   
v5  1   -0,000       0,00        0,74        0,39       -0,03        1,00   
v6  1    0,000       0,00        0,58        0,45        0,02        1,00   
v7  1   -0,005       0,00        3,98        0,05       -0,07        1,00   
v8  1    0,002       0,01        0,04        0,84        0,01        1,00   
v9  1   -0,016       0,05        0,09        0,76       -0,02        0,98   
v10 1    0,014       0,03        0,29        0,59        0,03        1,01   
v11 1    0,102       0,03        14,77       0,00        0,09        1,11   
v12 1    0,009       0,01        1,27        0,26        0,05        1,01   
v13 1   -0,017       0,01        2,39        0,12       -0,05        0,98   
v14 1   -0,005       0,01        0,48        0,49       -0,03        1,00   
</code></pre>

<p>My question is, if the algorithm selects best variables, how is it be possible that keeps the variables that have p-values greater than 0.1? I know that the effect is reflected in the value of the coefficient but the pvalue shows the probability that having that value in that coefficient is only a coincidence, and the coefficient is 0 (considering all the other variables). So why is still keeping those?</p>
"
"NaN","NaN","122640","<p>I have the following table:</p>

<pre><code>d &lt;- read.table(textConnection(""y x1 x2 x3 x4
                  40 5 10 8 2 
                  60 9 19 9 9 
                  75 18 27 19 5 
                  80 15 36 25 20 
                  115 25 45 39 30 
                  120 35 48 40 19""), header=TRUE)
</code></pre>

<p>I did linear regression analysis:</p>

<pre><code>summary(lm(y~., data=d))

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept) 30.50427    7.24960   4.208    0.149
x1           2.46856    1.13807   2.169    0.275
x2          -0.04313    0.91344  -0.047    0.970
x3          -0.45983    1.01308  -0.454    0.729
x4           1.35522    0.82608   1.641    0.348

Residual standard error: 4.855 on 1 degrees of freedom
Multiple R-squared:  0.9951,    Adjusted R-squared:  0.9756 
F-statistic: 51.02 on 4 and 1 DF,  p-value: 0.1046
</code></pre>

<p>How can I figure out if I need to do nonlinear-regression analysis or not?</p>
"
"0.052777981396926","0.0502942438178979","122875","<p>I am doing multiple linear regression analysis in R and I got the following summary:</p>

<pre><code>Call:
lm(formula = Y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + 
    X10 + X11 + X12 + X13)

Residuals:
ALL 20 residuals are 0: no residual degrees of freedom!

Coefficients: (151 not defined because of singularities)
                   Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)       -15462.94         NA      NA       NA
X1                    63.31         NA      NA       NA
X2                  1363.12         NA      NA       NA
X31,266,019,376     5518.54         NA      NA       NA
X31,483,786,035    29894.78         NA      NA       NA
X31,619,000,000    39338.01         NA      NA       NA
X31,687,000,000    65308.07         NA      NA       NA
X31,720,264,324    35548.79         NA      NA       NA
X31,749,000,000    31693.75         NA      NA       NA

.......................................................

X13692,062,808           NA         NA      NA       NA
X13693,179,733           NA         NA      NA       NA
X13724,817,439           NA         NA      NA       NA

Residual standard error: NaN on 0 degrees of freedom
Multiple R-squared:      1, Adjusted R-squared:    NaN 
F-statistic:   NaN on 19 and 0 DF,  p-value: NA
</code></pre>

<p>Could anybody explain what does that result mean? And what should I do?</p>

<p>Thank you!</p>

<p>Another question.  What should I do if the following error appears: </p>

<pre><code>Error in step(model) : 
number of rows in use has changed: remove missing values?
</code></pre>
"
"0.0215465206794286","0.0410650781176591","122935","<p>I have a design involving 1 between-subjects categorical factor, 1 between-subjects continuous factor and 2 within-subjects categorical factors.</p>

<p>This is theoretically a case of multiple regression, but as in this question <a href=""http://stats.stackexchange.com/questions/48455/anova-or-regression-1-continuous-factor-1-categorical-factor-with-continuous"">here</a>, I was able to use <code>lm()</code> in <code>R</code> to specify my model (actually, using the <code>aov.car()</code> function in the <code>afex</code> package).</p>

<p>This means that my output was in the form of an ANOVA table, with F and p-values. Can I report my analysis as though it was an ANOVA, or should I find a way to force the output of regression coefficients and report the analysis as though it were a multiple regression? I know that I shouldn't use an ANCOVA in this case, because my continuous predictor variable is certainly not a nuisance variable!</p>
"
"0.0806196982594614","0.076825726438694","123725","<p>I would like to know the best way to estimate a principal component's latest value, if I only have partial information about the latest variable data points:</p>

<p>Assuming I have 5 variables:</p>

<pre><code>&gt; head(rr)
                     USD           EUR           JPY           KRW
2010-02-12  2.648171e-03 -0.0016930704  0.0007601882  0.0010594185
2010-02-15  2.789115e-05 -0.0014012767 -0.0002707860  0.0010577844
2010-02-16 -7.923771e-03  0.0037916403 -0.0096992764  0.0005012469
2010-02-17  2.479928e-03 -0.0056491918 -0.0031176212  0.0044569950
2010-02-18  1.302002e-03 -0.0002913731 -0.0034022229 -0.0010499123
2010-02-19  9.438061e-04 -0.0006170278 -0.0047146407 -0.0023523910
                     ZAR
2010-02-12 -0.0041776791
2010-02-15 -0.0038725131
2010-02-16  0.0004297328
2010-02-17  0.0091811611
2010-02-18  0.0009181197
2010-02-19 -0.0038729740
</code></pre>

<p>And their principal component loadings: </p>

<pre><code>&gt; eigen(cor(rr))
$values
[1] 1.9858909 1.3388494 0.8530661 0.5337612 0.2884322

$vectors
            [,1]        [,2]       [,3]       [,4]       [,5]
[1,]  0.61173691 -0.23437356  0.1077678  0.2219310  0.7141286
[2,] -0.02097264  0.76835192 -0.3309138 -0.3429975  0.4266665
[3,]  0.57193284 -0.05324265  0.1981990 -0.7311797 -0.3100829
[4,] -0.24235617 -0.58496196 -0.6108094 -0.4126492  0.2360413
[5,] -0.48938168 -0.09843319  0.6830162 -0.3580450  0.3951064
</code></pre>

<p>Usually to get the first principal component I would do </p>

<pre><code>&gt; rr %*% eigen(cor(rr))$vectors[, 1] 
</code></pre>

<p>so basically apply the PC1 loadings to the variables. </p>

<p>But what if I want to calculate the latest value of the principal component, but I only have 3 of the needed 5 data points for the variables?</p>

<p>So for example say I have missing values for today in ZAR and JPY:</p>

<pre><code>                     USD          EUR           JPY           KRW           ZAR
2014-11-05  0.0047538401  0.000676224 -0.0045230806 -0.0050895701 -0.0038969653
2014-11-06  0.0051497837 -0.002184867  0.0028327631  0.0045528606 -0.0046996766
2014-11-07 -0.0019551509  0.000684713 -0.0001589247 -0.0008318505 -0.0020331115
2014-11-10 -0.0013404183 -0.001190019 -0.0014909916 -0.0018504471 -0.0005352551
2014-11-11  0.0001727452  0.003490628 -0.0070546749 -0.0071174438  0.0030901127
2014-11-12 -0.0008993663 -0.001086504            NA  0.0023363802            NA
</code></pre>

<p>What are my options for estimating the value of PC1 for 2014-11-12? I know that if I were to use the covariance matrix, then loadings would be equal to regression coefficients, but that would be for the PC being the independent variable, and here I need it the other way around? So basically, how do I estimate the latest PC1 value with only 3 out of the 5 input variables to hand?</p>
"
"0.0967596325610309","0.0922061136661462","124233","<p>I am running following data and code for analyzing non-linear regression and to get simplest equation of curve that fits the data:</p>

<pre><code>&gt; dput(ddf)
structure(list(xx = 1:23, yy = c(10L, 9L, 11L, 9L, 7L, 6L, 9L, 
8L, 5L, 4L, 6L, 6L, 5L, 4L, 6L, 8L, 4L, 6L, 8L, 11L, 8L, 10L, 
9L)), .Names = c(""xx"", ""yy""), row.names = c(NA, -23L), class = ""data.frame"")
&gt; 
&gt; head(ddf)
  xx yy
1  1 10
2  2  9
3  3 11
4  4  9
5  5  7
6  6  6
</code></pre>

<p><img src=""http://i.stack.imgur.com/vkx9G.png"" alt=""enter image description here""></p>

<pre><code>&gt; fit = lm(yy ~ poly(xx, 9), data=ddf)
&gt; summary(fit)

Call:
lm(formula = yy ~ poly(xx, 9), data = ddf)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.9890 -1.2031  0.1086  0.7493  2.4248 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)   7.347826   0.356758  20.596 2.62e-11
poly(xx, 9)1 -0.880172   1.710953  -0.514 0.615582
poly(xx, 9)2  7.821383   1.710953   4.571 0.000524  # NOTE THIS
poly(xx, 9)3  0.424579   1.710953   0.248 0.807892
poly(xx, 9)4 -2.151779   1.710953  -1.258 0.230641
poly(xx, 9)5 -0.876964   1.710953  -0.513 0.616857
poly(xx, 9)6 -0.961726   1.710953  -0.562 0.583610
poly(xx, 9)7 -0.002171   1.710953  -0.001 0.999007
poly(xx, 9)8 -0.051884   1.710953  -0.030 0.976269
poly(xx, 9)9  0.840177   1.710953   0.491 0.631571

Residual standard error: 1.711 on 13 degrees of freedom
Multiple R-squared:  0.6451,    Adjusted R-squared:  0.3993 
F-statistic: 2.625 on 9 and 13 DF,  p-value: 0.05575
</code></pre>

<p>If I use 'raw=TRUE' : </p>

<pre><code>&gt; fit = lm(yy ~ poly(xx, 9, raw=TRUE), data=ddf)
&gt; summary(fit)

Call:
lm(formula = yy ~ poly(xx, 9, raw = TRUE), data = ddf)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.9890 -1.2031  0.1086  0.7493  2.4248 

Coefficients:
                           Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)               2.844e+00  1.529e+01   0.186    0.855
poly(xx, 9, raw = TRUE)1  1.310e+01  2.711e+01   0.483    0.637
poly(xx, 9, raw = TRUE)2 -8.439e+00  1.723e+01  -0.490    0.632  # NOTE THIS
poly(xx, 9, raw = TRUE)3  2.637e+00  5.432e+00   0.485    0.635
poly(xx, 9, raw = TRUE)4 -4.719e-01  9.715e-01  -0.486    0.635
poly(xx, 9, raw = TRUE)5  5.112e-02  1.048e-01   0.488    0.634
poly(xx, 9, raw = TRUE)6 -3.400e-03  6.937e-03  -0.490    0.632
poly(xx, 9, raw = TRUE)7  1.355e-04  2.757e-04   0.492    0.631
poly(xx, 9, raw = TRUE)8 -2.967e-06  6.032e-06  -0.492    0.631
poly(xx, 9, raw = TRUE)9  2.739e-08  5.578e-08   0.491    0.632

Residual standard error: 1.711 on 13 degrees of freedom
Multiple R-squared:  0.6451,    Adjusted R-squared:  0.3993 
F-statistic: 2.625 on 9 and 13 DF,  p-value: 0.05575
</code></pre>

<p>I find that if I do not use 'raw=TRUE', one P value (2nd) is significant, but it is not significant if I use 'raw=TRUE'. Why does this occur and what does it mean?</p>

<p>I asked above question at stackoverflow but was advised to post here. Thanks for your help.</p>
"
"0.0746393370862076","0.0711268017165705","124500","<p>I would like to run by you an algorithm for predicting one of two values from a testing data set, based on a linear model applied to a training set. Please let me know whether this algorithm makes sense, whether it can be improved and whether there is already a well established algorithm to accomplish the same result, possibly already encapsulated in some R routine.</p>

<p>Given: Two dataframes, <code>training</code> and <code>testing</code>, comprising two columns: <code>Y</code> and <code>X</code>, in this order. The <code>Y</code> columns of both dataframes take values in the set {2,3}.</p>

<p>Assignment: Predict <code>testing$Y</code> from <code>testing$X</code> based on a linear model with coefficients obtained from the linear regression <code>training$Y ~ training$X</code>.</p>

<p>Suggested solution (R based pseudo-code):</p>

<ol>
<li><code>m &lt;- lm(Y ~ X, data = training)</code></li>
<li><code>p &lt;- predict(training, new_data = testing, interval = ""prediction"")</code></li>
<li>for every row of <code>p</code> do as follows:</li>
<li><code>if p$upr &lt;= 2 or (p$lwr &lt;= 2 &lt; p$upr) or p@fit &lt;= 2, then set p$fit &lt;- 2</code></li>
<li><code>else if p$lwr &gt;= 3 or (p$lwr &lt; 3 &lt;= p$upr) or p$fit &gt;= 3, then set pfit &lt;- 3</code></li>
<li><code>else set p$fit &lt;- round(p$fit)</code></li>
</ol>
"
"0.0806196982594614","0.0658506226617377","124616","<p>I am testing the logistic regression classifier in R. I created some test data like this:</p>

<pre><code>x=runif(10000)
y=runif(10000)
df=data.frame(x,y,as.factor(x-y&gt;0))
</code></pre>

<p>basically I am sampling the 2D unit square [0,1] and classifying a point belonging to class A or B depending on which side of y=x it lies.</p>

<p>I generated a scatter plot of the data like below:</p>

<pre><code>names(df) = c(""feature1"", ""feature2"", ""class"")
levels=levels(df[[3]])
obs1=as.matrix(subset(df,class==levels[[1]])[,1:2])
obs2=as.matrix(subset(df,class==levels[[2]])[,1:2])
# make scatter plot
dev.new()
plot(obs1[,1],obs1[,2],xlab=""x"",ylab=""y"",main=""scatter plot"",pch=0,col=colors[[1]])
points(obs2[,1],obs2[,2],xlab=""x"",ylab=""y"",main=""scatter plot"",pch=1,col=colors[[2]])
</code></pre>

<p>it gives me below graph:</p>

<p><img src=""http://i.stack.imgur.com/5zN4y.png"" alt=""scatter plot""></p>

<p>Now I tried running LR (logistic regression) on this data using code below:</p>

<pre><code>model=glm(class~.,family=""binomial"",data=df)
summary(model) # prints summary
</code></pre>

<p>here are the results:</p>

<pre><code>Call:
glm(formula = class ~ ., family = ""binomial"", data = df)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-0.11832   0.00000   0.00000   0.00000   0.08847  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  5.765e-01  1.923e+01   0.030    0.976
feature1     9.761e+04  8.981e+04   1.087    0.277
feature2    -9.761e+04  8.981e+04  -1.087    0.277

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1.3863e+04  on 9999  degrees of freedom
Residual deviance: 2.9418e-02  on 9997  degrees of freedom
AIC: 6.0294

Number of Fisher Scoring iterations: 25
</code></pre>

<p>I also get these warning messages:</p>

<pre><code>Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
</code></pre>

<p>If I try plotting the ROC curve using a varying threshold, I get following graph (AUC=1 which is good):
<img src=""http://i.stack.imgur.com/xbyPX.png"" alt=""enter image description here""></p>

<p><strong>Could someone please explain why the algorithm does not converge and coefficient estimates are not statistically significant (high std. error in coeff estimates)?</strong></p>

<p>I also compared to LDA:</p>

<pre><code>lda_classifier=lda(class~., data=df)
</code></pre>

<p>gives:</p>

<pre><code>Call:
lda(class ~ ., data = df)

Prior probabilities of groups:
 FALSE   TRUE 
0.5007 0.4993 

Group means:
       feature1  feature2
FALSE 0.3346288 0.6676169
TRUE  0.6710111 0.3380432

Coefficients of linear discriminants:
               LD1
**feature1  4.280490
feature2 -4.196388**
</code></pre>
"
"0.052777981396926","0.0335294958785986","124975","<p>Straight to the problem I want to test the hypothesis that two coefficients of multinomial regression model are equal between them. For example, considering a model where Y is 3 level categorical variable: A, B, C, and two covariates, X1 and X2 both dichotomous, we will have 6 coefficients. (A is the baseline of Y and X1=0 X2=0 is the baseline for covariates)</p>

<pre><code>-              B        C
Intercept      c1       c2
X1             c3       c4
X2             c5       c6
</code></pre>

<p>Through the standard error of the coefficients (in a rough way) I can test the hypotesis that any of the coefficients are equal to the underlined ones in the baseline (the log odds ratios of A), but how can I test the hypotesis that c3=c4?
Last thing I'm an R user.</p>
"
"0.0963589698356145","0.0918243061724248","125130","<p>I realize that a similar question to this has been asked, but it was not ultimately resolved. I have tried the suggestions posted to that question <a href=""https://stackoverflow.com/questions/23347467/is-there-any-way-to-fit-a-glm-so-that-all-levels-are-included-i-e-no-refer"">here</a>, but have had no success. I am using the following code:       </p>

<pre><code>allinfa4.exp = glm(survive ~ year + julianvisit + class + sitedist + roaddist
+ ngwdist, family = binomial(logexp(alldata$expos)), data=alldata)
summary(allinfa4.exp)

 Call:
glm(formula = survive ~ year + julianvisit + class + sitedist + 
roaddist + ngwdist, family = binomial(logexp(alldata$expos)), 
data = alldata)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.6435   0.3477   0.4164   0.4960   0.9488  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  4.458e+00  7.117e-01   6.265 3.74e-10 ***
year2013     3.680e-01  1.862e-01   1.976  0.04819 *  
year2014     2.136e-02  1.802e-01   0.119  0.90564    
julianvisit -5.714e-03  3.890e-03  -1.469  0.14192    
classb       2.863e-02  2.194e-01   0.131  0.89615    
classc      -2.394e-01  2.277e-01  -1.051  0.29304    
classd      -1.868e-01  2.479e-01  -0.754  0.45109    
classe      -4.500e-01  2.076e-01  -2.167  0.03021 *  
classf      -5.728e-01  2.005e-01  -2.858  0.00427 ** 
classg      -8.495e-01  3.554e-01  -2.390  0.01684 *  
classh      -1.858e-01  2.224e-01  -0.835  0.40351    
classi      -3.196e-01  4.417e-01  -0.724  0.46932    
sitedist    -2.607e-04  5.043e-04  -0.517  0.60520    
roaddist     6.768e-05  4.311e-04   0.157  0.87525    
ngwdist     -5.751e-05  9.456e-05  -0.608  0.54306
</code></pre>

<p>The main thing to note here is that I have two categorical variables, <code>year</code> and <code>class</code>, and R has combined the first level of each (2012 and class a) into a reference level intercept term. Not only do I need to know the intercept term for these levels individually, but I also need to know the base intercept terms itself (beta0), just as SAS produces. </p>

<p>I have tried changing the contrasts and deviation coding to accomplish this, but although doing so allows me to extract different levels, it changes the way they are calculated and still does not produce beta0. I've also tried adding +0 and -1, but this also does not provide what I need. Is what I'm trying to do simply impossible in R? It may seem like a strange request, but beta0 is necessary to convert the results of logistic exposure (special kind of logistic regression for nest-survival data) to daily survival rates. Any help would be hugely appreciated. Thanks!</p>

<p>Here is an example of SAS output I want to emulate (taken from a similar analysis done by my lab mate) :</p>

<pre><code>Parameter Estimates
Parameter   Estimate    Standard Error  DF  t Value Pr &gt; |t|              
beta0       7.8404      2.8479          19  2.75    0.0127  
NT         -3.8786      1.8831          19  -2.06   0.0534  
bgdensity  -0.1127      0.1614          19  -0.70   0.4935  
nwh         1.3466      1.4625          19  0.92    0.3687      
NRD        -2.6981      1.9496          19  -1.38   0.1824      
NAGW       -0.4898      2.2518          19  -0.22   0.8301      
</code></pre>
"
"0.0304713817668003","0.029037395206952","125211","<p>I am new to machine learning/statistical modelling.</p>

<p>I am trying to run a classification on a highly sparse dataset with 100 features, most of which are categorical (TRUE/FALSE) with the remaining values missing. To handle missing values, I filled the missing spots with the text 'Nothing', thereby creating a new level.</p>

<p>Next, I am trying to run a logistic regression using a penalty (glmnet package). When I check the coefficients, I see dummy variables corresponding to 'Nothing' having the higher coefficients.</p>

<p>How should I remove these coefficients? What would be a better approach to this?</p>

<p>Or should I just use trees? Please suggest the best way forward.</p>

<p>Thanks!</p>
"
"0.129930408420542","0.130006662643686","125414","<p>I have two variables:</p>

<ul>
<li>urban areas</li>
<li>protected areas.</li>
</ul>

<p>My observations are urban areas and protected areas in each year. But these observations are the cumulative ones, so observations in each variable have auto-correlation.</p>

<p>Can I use the general correlation such as yielded by <code>cor()</code> in R to measure the correlation between these two variables? If not, which indicator or method can I use?</p>

<p>I have the scatter plot: the horizontal variable is urban area in a specific year, and the vertical variable is another one in that specific year. And these two variables are increasing as years pass. I can see these two variables present a linear relationship. And my purpose is to find a indicator which can measure this linear relationship. I actually have tested the linear regression: the urban area as independent variable, the protected area as dependent variable, and I put 14 pairs of each year into the regression model, and the coefficients can pass the t-test, and model can pass the t-test, the $R^2$ can reach more than 0.9. </p>

<p>I want to research the relationship between urban development and protected area development. And the scatter plot below is urban and protected area pairs on global scale for 1950-2014 with 5 year intervals (except for 2010 and 2014).</p>

<p>I want to test two questions: First, are these two areas (urban and protected areas) both increasing over the research period? Second, does urbanization (here I mean the development of urban area) cause the development of protected areas?</p>

<p>I want to use some correlation analysis to solve the first question, such as correlation, linear regression or MIC value. However, because my data are time series, I'm not sure it can be used in the calculation of correlation? So I raise this question. In addition, I don't know other methods that could be used to measure strength of linear relationship between two time series. </p>

<p>And for the second question, I want to use Granger causality test to test the causality relationship between these two areas statistically. I know the result of Granger causality can't be sure to determine the causality relationship. And in my opinion, the reasons to improve the development of urban areas or protected areas are both complex, and some of them may be shared. At this level, I simply want to test the causality relationship between these two variables.</p>

<p><img src=""http://i.stack.imgur.com/tHlOm.jpg"" alt=""scatter plot between urban and farm land, the point is a variable pair in a specific year""></p>
"
"0.155480846643247","0.14816390098891","125453","<p>I have used the â€˜polrâ€™ function in the MASS package to run an ordinal logistic regression for an ordinal categorical response variable with 15 continuous explanatory variables.</p>

<p>I have used the code (shown below) to check that my model meets the proportional odds assumption following advice provided in <a href=""http://www.ats.ucla.edu/stat/r/dae/ologit.htm"">UCLA's guide</a>. However, Iâ€™m a little worried about the output implying that not only are the coefficients across various cutpoints similar, but they are exactly the same (see graphic below). </p>

<pre><code>FGV1b &lt;- data.frame(FG1_val_cat=factor(FGV1b[,""FG1_val_cat""]), 
                    scale(FGV1[,c(""X"",""Y"",""Slope"",""Ele"",""Aspect"",""Prox_to_for_FG"", 
                          ""Prox_to_for_mL"", ""Prox_to_nat_border"", ""Prox_to_village"", 
                          ""Prox_to_roads"", ""Prox_to_rivers"", ""Prox_to_waterFG"", 
                          ""Prox_to_watermL"", ""Prox_to_core"", ""Prox_to_NR"", ""PCA1"", 
                          ""PCA2"", ""PCA3"")]))
b     &lt;- polr(FG1_val_cat ~ X + Y + Slope + Ele + Aspect + Prox_to_for_FG + 
                            Prox_to_for_mL + Prox_to_nat_border + Prox_to_village + 
                            Prox_to_roads + Prox_to_rivers + Prox_to_waterFG + 
                            Prox_to_watermL + Prox_to_core + Prox_to_NR, 
              data=FGV1b, Hess=TRUE)
</code></pre>

<p>View a summary of the model:</p>

<pre><code>summary(b)
(ctableb &lt;- coef(summary(b)))
q        &lt;- pnorm(abs(ctableb[, ""t value""]), lower.tail=FALSE) * 2
(ctableb &lt;- cbind(ctableb, ""p value""=q))
</code></pre>

<p>And now we can look at the confidence intervals for the parameter estimates:</p>

<pre><code>(cib &lt;- confint(b)) 
confint.default(b)
</code></pre>

<p>But these results are still quite hard to interpret, so let's convert the coefficients into odds ratios</p>

<pre><code>exp(cbind(OR=coef(b), cib))
</code></pre>

<p>Checking the assumption. So the following code will estimate the values to be graphed. First it shows us the logit transformations of the probabilities of being greater than or equal to each value of the target variable</p>

<pre><code>FG1_val_cat &lt;- as.numeric(FG1_val_cat)
sf &lt;- function(y) {
  c('VC&gt;=1' = qlogis(mean(FG1_val_cat &gt;= 1)),
    'VC&gt;=2' = qlogis(mean(FG1_val_cat &gt;= 2)),
    'VC&gt;=3' = qlogis(mean(FG1_val_cat &gt;= 3)),
    'VC&gt;=4' = qlogis(mean(FG1_val_cat &gt;= 4)),
    'VC&gt;=5' = qlogis(mean(FG1_val_cat &gt;= 5)),
    'VC&gt;=6' = qlogis(mean(FG1_val_cat &gt;= 6)),
    'VC&gt;=7' = qlogis(mean(FG1_val_cat &gt;= 7)),
    'VC&gt;=8' = qlogis(mean(FG1_val_cat &gt;= 8)))
}
(t &lt;- with(FGV1b, summary(as.numeric(FG1_val_cat) ~ X + Y + Slope + Ele + Aspect + 
                             Prox_to_for_FG + Prox_to_for_mL + Prox_to_nat_border + 
                             Prox_to_village + Prox_to_roads + Prox_to_rivers + 
                             Prox_to_waterFG + Prox_to_watermL + Prox_to_core + 
                             Prox_to_NR, fun=sf)))
</code></pre>

<p>The table above displays the (linear) predicted values we would get if we regressed our dependent variable on our predictor variables one at a time, without the parallel slopes assumption. So now, we can run a series of binary logistic regressions with varying cutpoints on the dependent variable to check the equality of coefficients across cutpoints</p>

<pre><code>par(mfrow=c(1,1))
plot(t, which=1:8, pch=1:8, xlab='logit', main=' ', xlim=range(s[,7:8]))
</code></pre>

<p><img src=""http://i.stack.imgur.com/4Uicq.jpg"" alt=""polr assumption check""></p>

<p>Apologies that I am no statistics expert and perhaps I am missing something obvious here. However, I have spent a long time trying to figure out if there is a problem in how I tested the model assumption and also trying to figure out other ways to run the same kind of model. </p>

<p>For example, I read in many help mailing lists that others use the vglm function (in the VGAM package) and the lrm function (in the rms package) (for example see here:  <a href=""http://stats.stackexchange.com/questions/25988/proportional-odds-assumption-in-ordinal-logistic-regression-in-r-with-the-packag"">Proportional odds assumption in ordinal logistic regression in R with the packages VGAM and rms</a>). I have tried to run the same models but am continuously coming up against warnings and errors.</p>

<p>For example, when I try to fit the vglm model with the â€˜parallel=FALSEâ€™ argument (as the previous link mentions is important for testing the proportional odds assumption), I encounter the following error:</p>

<blockquote>
  <p>Error in lm.fit(X.vlm, y = z.vlm, ...) : NA/NaN/Inf in 'y'<br>
  In addition: Warning message:<br>
  In Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals = residuals,  :
    fitted values close to 0 or 1</p>
</blockquote>

<p>I would like to ask please if there is anyone who might understand and be able to explain to me why the graph I produced above looks as it does. If indeed it means that something isnâ€™t right, could you please help me find a way to test the proportional odds assumption when just using the polr function. Or if that is just not possible, then I will resort to trying to use the vglm function, but would then need some help to explain why I keep getting the error given above.</p>

<p>NOTE: As a background, there are 1000 datapoints here, which are actually location points across a study area. I am looking to see if there are any relationships between the categorical response variable and these 15 explanatory variables. All of those 15 explanatory variables are spatial characteristics (for example, elevation, x-y coordinates, proximity to forest etc.). The 1000 datapoints were randomly allocated using a GIS, but I took a stratified sampling approach. I made sure that 125 points were randomly chosen within each of the 8 different categorical response levels. I hope this information is also helpful.</p>
"
"0.0430930413588572","0.0410650781176591","125523","<p>What I'm trying to do is to construct a linear model in a form like</p>

<p>$$
Y = \beta_0X_0-\beta_1X_1+\beta_2X_2 + \beta_3
$$</p>

<p>where $\beta_0$, $\beta_1$ and $\beta_2$ are coefficient of predictors $X_0$, $X_1$ and \beta_2 respectively. And, They all are <strong>positive</strong>, which means my assumption of the model is that $X_1$ has negative effect towards the response $Y$. Perhaps, I misunderstand the concept of regression, but if anyone has an idea how to achieve this in R, please enlighten me. Or any other approach apart from regression model. Thanks in advance.</p>
"
"0.0621994475718397","0.0711268017165705","126179","<p>I've been using R to analyze my data (as shown in example below) and <code>lm.beta</code> from the <code>QuantPsyc</code> package to get the standardized regression coefficients. </p>

<p>My understanding is that the absolute value of the standardized regression coefficients should reflect its importance as a predictor. I was also under the impression (and the intuition) that the variable with the largest absolute value should be the most significant independent predictor and should have the <em>lowest</em> p-value. However, I'm not finding that in my data.</p>

<p>For example (taken from my data), I have a multiple regression with dependent variable <code>y</code> and 7 independent variables <code>x1:x7</code>. </p>

<pre><code>    Call:
lm(formula = y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7)
</code></pre>

<p>For 3 of the variables, the beta values and the p-values make sense to me (the greater the magnitude of beta, the lower p-value), but for 4 of them this is not the case. I'll show only the p-values and betas for those 4 to keep this short.</p>

<pre><code>    x1          x2          x3          x7
p   0.006635    0.00004683  0.000152    0.022427
ÃŸ   0.15707977  0.24149287  0.27171665  0.16583391 
</code></pre>

<p>As you can see, <code>x2</code> has a lower p-value than <code>x3</code>, but <code>x3</code> has a larger value for beta. Similarly, <code>x7</code> has a larger beta value than <code>x1</code>, but is less significant. </p>

<p>I've searched for an explanation but have found conflicting information. Is that because there's no straightforward answer to this question? Am I doing something wrong? </p>
"
"0.101062140164153","0.0875510407188402","126768","<hr>

<h2>Original</h2>

<p>I have fitted an ordered logistic regression in R using the <code>polr</code> function, but I am having some trouble bringing the model coefficients into Excel and getting the probabilities there. </p>

<p>For explanatory variables <code>FlowMonth2, Orders_Apt, GeoUnits, HomeOwner, Platform, CreditScore</code>, my coefficients for the model are as follows: </p>

<pre><code>                                Value Std. Error  t value
FlowMonth2Aug                 0.12321    0.03852   3.1990
FlowMonth2Dec                 0.31092    0.03854   8.0672
FlowMonth2Feb                 0.02497    0.03873   0.6447
FlowMonth2Jan                -0.01874    0.03940  -0.4757
FlowMonth2Jul                 0.02924    0.03886   0.7525
FlowMonth2Jun                -0.02618    0.04054  -0.6456
FlowMonth2Mar                 0.09369    0.03739   2.5054
FlowMonth2May                -0.08169    0.03581  -2.2811
FlowMonth2Nov                 0.32610    0.03889   8.3841
FlowMonth2Oct                 0.45240    0.03708  12.2009
FlowMonth2Sep                 0.22771    0.04015   5.6711
Orders_Apty                   0.03786    0.02206   1.7160
GeoUnits1                    -0.04070    0.03260  -1.2487
GeoUnits2                     0.11923    0.03735   3.1920
GeoUnitsOther                 0.30464    0.20803   1.4644
GeoUnits5                    -0.19669    0.01892 -10.3942
HomeOwnery                    0.16577    0.02828   5.8624
PlatformMobile               -0.32933    0.01631 -20.1882
CreditScore525 - 600          1.01909    0.02937  34.7036
CreditScore600 - 700          1.12578    0.02953  38.1284
CreditScore700 - 800          1.29098    0.03091  41.7694
CreditScore800 - 900          1.43500    0.03085  46.5179
CreditScore900+               1.33816    0.02851  46.9414
CreditScoreHit with No Score  0.33832    0.03424   9.8812
CreditScoreNo Hit             0.37199    0.06443   5.7737
</code></pre>

<p>The intercepts are </p>

<pre><code>Intercepts:
                Value    Std. Error t value 
0|1              -1.2788   0.0377   -33.9349
1|2              -0.6609   0.0371   -17.8175
2|3              -0.1683   0.0369    -4.5571
3|4               0.1520   0.0369     4.1159
4|5               0.3813   0.0370    10.3163
5|6               0.5615   0.0370    15.1714
6|7               0.7314   0.0371    19.7357
7|8               0.8551   0.0371    23.0486
8|9               0.9608   0.0371    25.8740
9|10              1.0510   0.0372    28.2760
10|11             1.1342   0.0372    30.4826
11|12             1.2607   0.0373    33.8295
12|13             1.4770   0.0374    39.5140
13|14             1.5414   0.0374    41.1957
14|15             1.5827   0.0374    42.2710
15|16             1.6127   0.0375    43.0505
16|Still Active   1.6358   0.0375    43.6499
</code></pre>

<hr>

<p>Now when I bring this into Excel, I bring in the coefficients, select certain values to add together, say <code>FlowMonth2 = ""Aug"", Orders_Apt = ""n"", GeoUnits = ""5"", HomeOwner = ""y"", Platform = ""Desktop"", CreditScore = ""800 - 900""</code>. </p>

<p>I add these values together to get my logit statistic, $T = \mathbf{x}\mathbf{\beta}$, and then I add this $T$ to each different intercept to get $\beta_{0, i} - T$ for $1 \leq i \leq 17$ where the $17$th stage is transition from 16 to Still Active. </p>

<p>I then take $\mathrm{logit}(\beta_{0, i} - T)$ or ${1 \over 1 + \exp(-[\beta_{0, i} - T])} = \Pr(\text{being in the $i$th stage})$</p>

<p>But when I try to do this in Excel, and compare it to the output of <code>predict</code> in R, then I can't get these values to match up? What am I doing wrong in Excel? </p>

<hr>

<h2>Edit</h2>

<p>To compare the values from R and Excel, it's by more than a rounding error that they differ: </p>

<p>R: </p>

<pre><code>0                                                0.048650293
1                                                0.037989009
2                                                0.047738406
3                                                0.041799312
4                                                0.035787006
5                                                0.031644868
6                                                0.032650167
7                                                0.025400235
8                                                0.022740618
9                                                0.020074660
10                                               0.019006405
11                                               0.029758088
12                                               0.052613086
13                                               0.015949280
14                                               0.010274309
15                                               0.007485204
16                                               0.005777627
Still Active                                     0.514661427
</code></pre>

<p>Excel: </p>

<pre><code>0|1 0.048648622
1|2 0.086640673
2|3 0.134381676
3|4 0.176177948
4|5 0.211958542
5|6 0.243615257
6|7 0.27626595
7|8 0.301669593
8|9 0.32439208
9|10    0.344464821
10|11   0.363487303
11|12   0.393228837
12|13   0.44584823
13|14   0.461809529
14|15   0.472089045
15|16   0.479571379
16|Still Active 0.485339204
</code></pre>

<hr>

<h2>Edit 2</h2>

<p>Why are there only 17 intercepts in Excel, but 18 predicted points in R? </p>
"
"0.0609427635336005","0.0580747904139041","128704","<p>We have 2 correlated variables and a lot of binomial factors (around 200),
here illustrated with just $f1$ and $f2$:</p>

<pre><code>x &lt;- rnorm(100)
y &lt;- rnorm(100)
f1 &lt;- rbinom(100, 1, 0.5)
f2 &lt;- rbinom(100, 1, 0.5)
</code></pre>

<p>Which gives four possible groups: A $(f1=1,f2=1)$, B $(f1=0,f2=1)$, C $(f1=1,f2=0)$, and D $(f1=0,f2=0)$.</p>

<p>We then run the model</p>

<pre><code>&gt; glm(y ~ x * f1 + x * f2)

Call:
glm(formula = y ~ x * f1 + x * f2)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-2.72028  -0.58501   0.03167   0.60097   1.86332  

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept) -0.03188    0.17388  -0.183   0.8549  
x            0.08105    0.20540   0.395   0.6940  
f1           0.26823    0.19309   1.389   0.1681  
f2          -0.34568    0.19488  -1.774   0.0793 .
x:f1         0.10301    0.20183   0.510   0.6110  
x:f2        -0.25875    0.20828  -1.242   0.2172  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for gaussian family taken to be 0.8906953)

    Null deviance: 88.754  on 99  degrees of freedom
Residual deviance: 83.725  on 94  degrees of freedom
AIC: 280.02

Number of Fisher Scoring iterations: 2
</code></pre>

<p>We can simplify this output and make a regression ($y = a + b \times x$) for each group by doing:</p>

<p>$a_A = (-0.03188) + (0.26823) + (-0.34568)=-0.10932806$
$b_A = (0.08105) + (0.10301) + (-0.25875)=-0.07468630$</p>

<p>$a_B = (-0.03188) + (-0.34568)=-0.37755949$
$b_B = (0.08105) + (-0.25875)=-0.17769345$</p>

<p>And the same for the C and D groups. My question is: How do I calculate a standard deviation or confidence intervals for the individual group slopes. Is it additive like the estimates or is it the means or something else?
Thank you for any help.</p>
"
"0.091874672876503","0.0875510407188402","128754","<p>I am using R to run some negative binomial regression models. 
For model 1 I have the number of network in-degrees as the dependent variable, and Twitter followers, friends and number of Twitter statuses as independent variables. Model 2 has the number of network out-degrees as the dependent and number of Twitter followers, friends and number of Twitter statuses as independent variables.</p>

<p>the first model</p>

<pre><code>summary(m1 &lt;- glm.nb(INdegrees ~ Followers + Friends + Statuses, data = list_indegrees))
</code></pre>

<p>converges just fine. But when I run the second model i get the warning message:
<code>glm.fit: algorithm did not converge</code>:</p>

<pre><code>summary(m2 &lt;- glm.nb(OUTdegrees ~ Followers + Friends + Statuses, data = list_outdegrees))

Call:
glm.nb(formula = OUTdegrees ~ Friends + Statuses + Followers, 
data = list_outdegrees, init.theta = 0.7029123173, link = log)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-7.1442  -1.0481  -0.7852  -0.2878  14.6127  

Coefficients:
         Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) 4.149e+00  1.022e-02 406.058  &lt; 2e-16 ***
Friends     1.834e-06  4.214e-07   4.353 1.35e-05 ***
Statuses    1.600e-07  1.208e-07   1.325    0.185    
Followers   6.440e-07  8.629e-09  74.639  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for Negative Binomial(0.7029) family taken to be 1)

Null deviance: 22689  on 17244  degrees of freedom
Residual deviance: 20537  on 17241  degrees of freedom
AIC: 179693

Number of Fisher Scoring iterations: 1


          Theta:  0.70291 
      Std. Err.:  0.00653 

2 x log-likelihood:  -179683.26300 
Warning message:
glm.fit: algorithm did not converge 
</code></pre>

<p>Also, when I take out <code>Followers</code> from the model â€“ it converges, but since <code>Followers</code> is an important predictor it wouldn't make any sense to take it out.</p>

<p>This is the first time I'm running negative binomial regression and don't really know what's the deal with the convergence.</p>
"
"0.0609427635336005","0.0435560928104281","128815","<p>I'm trying to estimate the value of a property depending on the property characteristics. I did some research and I found out, that it would be better to use the <strong>Hedonic Model/Regression</strong> instead of <strong>Linear Square Regression</strong>.</p>

<p>After reading a couple of papers about it, I still have some questions.</p>

<p>I work with <strong>R</strong>, so I have the data (information about other properties) saved as a data.frame, with the following columns (c stands for characteristic).</p>

<pre><code>----------------------------------
| price | c1 | c2 | c3 | c4 | c5 |
----------------------------------
</code></pre>

<p>My questions:</p>

<ol>
<li>I know how to estimate the coefficients with the <strong>Least Square Regression</strong>, but how do I do it with the <strong>Hedonic Regression</strong>? I know, that in <strong>R</strong> is no function for it.</li>
<li>The environment characteristics (air pollution, criminality rate, etc.) are almost the same, because the properties are in the same district. The <strong>Last Square Regression</strong> gives them a very small coefficient, but they have a big importance in real life. How can I tell the regression, that they have a big importance?</li>
<li>As I understood so far, if an attribute of an observation is missing, I should not use the observation, is that right?</li>
<li>In the calculation of the coefficients, should I use only the date from nearby (example: same district) real estates or it would be better to use all real estates from the town?</li>
</ol>

<p>Could somebody please give me a hint?</p>

<p>Thank you very much!       </p>
"
"0.0746393370862076","0.0711268017165705","129657","<p>What is the fastest algorithm for fitting a simple logistic 'random effects' type model, with only one level of categorical predictors? </p>

<p>Another way of putting it might be a logistic regression with a Gaussian prior on the coefficients, or ""with shrinkage"".</p>

<p>I'm looking for a very fast and reliable implementation to use in a production environment. This means that the algorithm would need to have a low risk of 'hanging', and a not-drastically-variable time to converge.</p>

<p>There would be between 1 and 5000 data points per 'cell', and 5-100 groups/categories. It would need to exploit sufficient statistics (take counts of group data). Second-level nesting a bonus, but not essential.</p>

<p>This could be done via <code>lme4</code> in <code>R</code>. However, is there a library (e.g. stand-alone C++) which is more efficient for this narrowly-defined type of model?</p>

<p>EDIT: Goal is inference over prediction - specifically, comparison of group estimates (with standard errors), construction of confidence intervals etc.</p>

<p>EDIT: Just to make it clear, I wouldn't be fitting a 'mixed model' so to speak - there would be no fixed effect. The data would be a very long two-column ('successes', 'failures') contingency table, with highly variable n counts.</p>

<p>EDIT: I need the degree of 'shrinkage' in the individual estimates to be informed by the group level variance (as opposed to banging a Jeffery's prior on each individual estimate, or using an Agresti-Coull (1998) type interval).</p>
"
"0.0304713817668003","0.029037395206952","129788","<p>I am running a linear regression model in R:</p>

<pre><code>data(iris)
fit1.iris = lm(Sepal.Length ~ Petal.Length+Petal.Width , data=iris) 
summary(fit1.iris)
</code></pre>

<p>These are my coefficients:</p>

<pre><code>Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   4.19058    0.09705  43.181  &lt; 2e-16 ***
Petal.Length  0.54178    0.06928   7.820 9.41e-13 ***
Petal.Width  -0.31955    0.16045  -1.992   0.0483 * 
</code></pre>

<p>I am trying to plot the density curve for parameter estimates, and below is how I did it for intercept. Am I doing it right ?</p>

<pre><code>  fit_iris = lm(Sepal.Length~ Petal.Length+Petal.Width , data=iris, x=TRUE, y=TRUE)
  summary(fit_iris)
  x_iris = seq(0, 10, length.out=1000)
  plot(density(dnorm(x,4.190582,0.09705)), type='l')
</code></pre>
"
"0.0746393370862076","0.0592723347638087","130643","<p>I tried a regression in the form ${\rm logit}(Y) = {\rm coefficient}\times X + 0 + e$, where $Y$ is a binomial variable and $X$ is a factor variable with $n$ levels. I noticed that removing the intercept yields higher $p$ values. I'm wondering how to interpret it though.</p>

<p>Since removing the intercept makes it equal to $0$, I believe that the coefficients returned are relative to a $0$ probability of the event $Y$ and that all $X$ factors are in the $0$ state. But I think this is impossible isn't it?</p>

<p>$X$ are mutually exclusive factors, therefore it's impossible to have a case where no factor is $1$, at least in the presented observations. And it cannot be interpreted like the coefficient is relative to hypothetical cases in which really no one of the factors is present, because we have no data like that.</p>

<p>Regarding $Y$ having a $0$ intercept, wouldn't it mean forcing the probability of the event to $0$ when none of the factors is present? Again, this is an impossible case.</p>

<p>Nonetheless this kind of regression would allow me to retrieve pure probability range of the event Y given a factor by transforming the coefficients in the confidence intervals given as $\exp({\rm coefficient})/(1 + \exp({\rm coefficient}))$, and the $p$ values would test whether this probability is not $50\%$. This could also be a valuable result, since it would give independent probabilities for each factor.</p>

<p>Am I wrong?</p>
"
"0.121885527067201","0.10889023202607","131456","<p>I'm exploring the effects of removing the intercept in a logistic regression model.</p>

<p>Assume a model:</p>

<p>$$logit(Y = 1) = \beta_1 x + \beta_2z + 0$$</p>

<p>with $x$ and $z$ being categorical variables with 2 levels each and no intercept.</p>

<p>I understood that having no intercept with categorical predictors produce coefficients that compare the $P(Y = 1)$ in each level of the two predictor against a null case where $P(Y=1) = 0.5$ or $logit(Y=1) = 0$.</p>

<p>I noticed a phenomenon that can understand. Using glm() function in R if you change the order of the variable in the right hand part of the formula, the coefficients change too. But even more oddly, the coefficient of the first variable is always the same.</p>

<p>Here's an <code>R</code> demo:</p>

<pre><code>y &lt;- as.factor(sample(rep(1:2), 30, T))
x &lt;- as.factor(sample(rep(1:2), 30, T))
z &lt;- as.factor(sample(rep(1:2), 30, T))

coef(glm(y ~ x + z - 1, binomial)
#        x1         x2         z2 
#-0.1764783  0.3260739 -0.1335192

coef(glm(y ~ z + x - 1, binomial))
#        z1         z2         x2 
#-0.1764783 -0.3099976  0.5025523 
</code></pre>

<p>As you can see the first predictors have the same coefficient while the other are different in the two models.</p>

<p>Here is what I expected and instead behave differently than what I though:</p>

<ol>
<li>Since every level of the two predictors is compared to the same null case, I expected to have the same coefficients in the two models, independently from the order in which I use them.</li>
<li>I expected to see the coefficients of every level of every predictor, instead the coefficient for the 1 level of the second predictor is not shown.</li>
<li>I therefore assume that only the first variable is compared against the null case, while the second is compared against a reference level; but what is this level? Is it $P(Y = 1 | X = 1 \cap Z = 1)$? Reproducing one of the models WITH the intercept we get:</li>
</ol>

<p>`(for some reason stackexchange don't understand the following is code without the tick)   </p>

<pre><code>coef(glm(y ~ x + z - 1, binomial)
#        x1         x2         z2 
#-0.1764783  0.3260739 -0.1335192

coef(glm(y ~ x + z, binomial))
#(Intercept)         x2          z2 
#-0.1764783   0.5025523  -0.1335192
</code></pre>

<p>As expected x1 become the intercept, and x2 is likely relative to x1. z1 is missing also in this case and z2 is the same as in the model without intercept.</p>

<p>Thus should I assume that the comparison against the null case $P(Y = 1) = 0.5$ is made only for the first variable in a formula, while the other are compared against the usual intercept?
Is this behavior normal?
What about the fact that the first coefficient has the same value whichever the order of the predictors in the formula?
What if I want to compare all level of each predictor against the null case and have a coefficient for all levels?
Or it's theoretically impossible for some reason I don't get?</p>
"
"0.125636725583038","0.112681644735122","131459","<p>I'm trying to implement a joint test of the two coefficients comprising a quadratic term in a 2-stage least squares regression.  The quadratic term is endogenous.  I'm using <code>AER</code> in R, and <code>ivreg</code>'s <code>anova</code> method is not giving me the same result as the manual Wald test that I'm checking it with.  I'd basically like to know whether my own manual method is correct or not.  If not, why not, and if so, what <code>AER</code> is doing differently.</p>

<pre><code>1&gt; rm(list=ls())
1&gt; set.seed(1)
1&gt; N &lt;- 100
1&gt; z &lt;- rnorm(N) #The instrument
1&gt; u &lt;- rnorm(N) #The error term
1&gt; x &lt;- 1 + z - .1*z^2 + u + rnorm(N) # x is correlated with the error term u (endogeneity) and the instrument z
1&gt; ex &lt;- 1 + rnorm(N) #an exogenous variable
1&gt; y &lt;- 1 + x-.1*x^2  + ex + u 
1&gt; x2 &lt;- x^2
1&gt; z2 &lt;- z^2
1&gt; summary(lm(y~x+x2+ex))

Call:
lm(formula = y ~ x + x2 + ex)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.91064 -0.57302  0.04697  0.43678  1.62413 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.51137    0.13751   3.719 0.000337 ***
x            1.22946    0.05578  22.042  &lt; 2e-16 ***
x2          -0.03512    0.02145  -1.637 0.104893    
ex           0.97401    0.07933  12.278  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.7699 on 96 degrees of freedom
Multiple R-squared:  0.8952,    Adjusted R-squared:  0.892 
F-statistic: 273.5 on 3 and 96 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>BIAS</p>

<pre><code> 1&gt; library(AER)

1&gt; miv = ivreg(y~x+x2+ex|z+z2+ex)
1&gt; summary(miv)

Call:
ivreg(formula = y ~ x + x2 + ex | z + z2 + ex)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.1212 -0.7533 -0.2623  0.6458  3.5927 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   1.4767     0.4785   3.086  0.00265 ** 
x             1.0678     0.1351   7.902 4.58e-12 ***
x2           -0.2185     0.1014  -2.154  0.03373 *  
ex            0.8690     0.1409   6.167 1.65e-08 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 1.245 on 96 degrees of freedom
Multiple R-Squared: 0.7259, Adjusted R-squared: 0.7173 
Wald test: 43.79 on 3 and 96 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>MUCH BETTER WITH THE VALID INSTRUMENTS</p>

<p>Wald test using built-in method, versus manual Wald test:</p>

<pre><code>1&gt; mnull = ivreg(y~ex)
        1&gt; anova(miv,mnull,vcov=vcov(miv))
        Wald test

        Model 1: y ~ x + x2 + ex | z + z2 + ex
        Model 2: y ~ ex
          Res.Df Df      F    Pr(&gt;F)    
        1     96                        
        2     98 -2 31.512 3.011e-11 ***
        ---
        Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

        1&gt; #Wald test
        1&gt; B = coef(miv)
        1&gt; r = matrix(c(0,0),2)
        1&gt; R = t(matrix(c(   0,1,0,0
        1+      ,0,0,1,0),4))
        1&gt; V = vcov(miv)
        1&gt; W = t(R%*%B -r) %*% solve(R%*%V%*%t(R)) %*% (R%*%B -r)
        1&gt; W/2
                [,1]
        [1,] 31.5123
</code></pre>

<p>Why is my wald statistic twice the one given by the method?</p>

<pre><code>    1&gt; 1-pf(W/2,nrow(R),N-nrow(R))
                 [,1]
    [1,] 2.706191e-11
</code></pre>

<p>And why are the p-values similar, but not identical?</p>
"
"0.0609427635336005","0.0580747904139041","133488","<p>I am trying to run regression on financial data in R. I am new to regression analysis so I am finding it to difficult to interpret certain scenarios. I have the code as follows:</p>

<pre><code>#regression analysis
fit &lt;- lm(fiveMinReturns~RegressionData, data=maindata)
summary(fit) # show results
#correlation
cor(maindata$fiveMinReturns,maindata$RegressionData,use=""everything"")
</code></pre>

<p>My output is: </p>

<pre><code>Call:
lm(formula = fiveMinReturns ~ RegressionData, data = maindata)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.205790 -0.001144 -0.000062  0.001117  0.156418 

Coefficients:
                Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)    6.346e-05  8.785e-06   7.223 5.09e-13 ***
RegressionData 1.597e-07  1.432e-08  11.155  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.004035 on 210912 degrees of freedom
Multiple R-squared:  0.0005896, Adjusted R-squared:  0.0005849 
F-statistic: 124.4 on 1 and 210912 DF,  p-value: &lt; 2.2e-16

cor(maindata$fiveMinReturns,maindata$RegressionData,use=""everything"")
[1] 0.02428219
</code></pre>

<p>p-value is very small that means two variables are tightly coupled, but correlation is small too.
My question is how do I evaluate this situation?
Can we say that this equation will give correct results almost every time?
Which scenario suggests both p-value and correlation both to be really small?
What measures should i take to improve the result? </p>
"
"0.114267681625501","0.101630883224332","133494","<p>I have been running logit regressions on large samples in order of hundreds of thousands in a preliminary study that will eventually end up in non-parametric tests. Both my dependent variable and my independent variable of interest are binary for now. I started with a very underspecified model containing only my IV of interest in a single logit regression. Since the dataset is quite large, I haven't been putting much trust into the significance level of my coefficients which seem to be almost always perfect. Instead, I have been calculating odds ratios that can give me some kind of indication of the predictive power of the model. While I was running my underspecified regression (<code>glm(dv ~ iv, family = binomial(link = logit)</code>) on preliminary noisy data the odds ratios seemed to be at an acceptable level, so I decided to proceed to the next step, clean the data and import the control variables, etc...</p>

<p>Now the issue is: since I have started using the full clean data with real control variables, the odds ratios of my variable of interest have started exploding.</p>

<p>Consider this:</p>

<pre><code>glm(clean_dv ~ clean_iv, family = binomial(link = logit))
</code></pre>

<blockquote>
  <p>clean_iv coefficient: 4.619625</p>
  
  <p>clean_iv stderr: 0.267083</p>
  
  <p>clean_iv odds: 101.45602</p>
</blockquote>

<pre><code>glm(clean_dv ~ clean_iv+noisy_cv1+noisy_cv2, family = binomial(link = logit))
</code></pre>

<blockquote>
  <p>clean_iv coefficient: 6.233e+00</p>
  
  <p>clean_iv stderr: 2.727e-01</p>
  
  <p>clean_iv odds: 509.3612309</p>
</blockquote>

<pre><code>glm(clean_dv ~ clean_iv+clean_cv1+clean_cv2, family = binomial(link = logit))
</code></pre>

<blockquote>
  <p>clean_iv coefficient: 5.582e+00</p>
  
  <p>clean_iv stderr: 2.369e-01</p>
  
  <p>clean_iv odds: 265.6611359</p>
</blockquote>

<p>The control variables behave just normally. They are significant and have acceptable odds ratios.</p>

<p>Better odds ratios should supposedly be good news, but at this level I don't know how to interpret them anymore. Any help is appreciated.</p>
"
"0.12583105938145","0.126571071290756","133571","<p>I know there are already lots of questions around this topic (especially <a href=""http://stats.stackexchange.com/questions/16390/when-to-use-generalized-estimating-equations-vs-mixed-effects-models/16415#16415"">this one</a> and <a href=""http://stats.stackexchange.com/questions/24689/interpreting-coefficients-of-ordinal-logistic-regression-when-there-is-clusterin/29701#29701"">this one</a>) but I haven't really seen anything that directly helps me (It will be obvious I'm not a great statistician, but I'll do my best to explain). </p>

<p>I am running an ordinal regression in R (<code>clm</code> and <code>clmm</code>). My response variable is a rating between 0 and 4. I have two types of explanatory variables: individual and scenario variables [let's say <code>IVs</code> and <code>SVs</code>]. </p>

<p>Six different scenario variables (all dummies with at most 4 different values) represent potential collaboration scenarios that get rated by the respondent (between 0 and 4) creating the response variable. (Research design is a conjoint analysis; there are a total of 192 different scenarios possible)</p>

<p>On top of that I have a variety of individual characteristics about the respondent (age, gender, work experience, networking skills, ...) all derived from a survey.</p>

<p>Every respondent rates between 3 and 16 different scenarios (average 8.1); every scenario is rated by at least 8 respondents. Every respondent and every scenario have a unique identifier (called <code>IVid</code> and <code>SVid</code>). So they are non nested within each other.</p>

<p>Thus the basic regression looks like this:</p>

<pre><code>clm.base &lt;- clm(rating ~ SVs + IVs, data = dt) 
</code></pre>

<p>The hypothesis I am trying to test is that there are specific individual characteristics, that will influence the rating of the scenarios, independent of the actual content of the scenarios. Basically, some people are more or less favourable to all types of collaboration scenarios. </p>

<p>Now a reviewer of my paper asks me to include individual fixed effects (which in management [my field] basically means dummies for each individual). My assumption originally was that this would result in all individual variables being dropped. This is exactly what happens when I use another model (package <code>lfe</code>)</p>

<pre><code>felm.complete &lt;- felm(rating ~ SVs + IVs | SVid + IVid | 0 | IVid, data = dt) 
</code></pre>

<p>In this regression basically all my variables are perfectly collinear as expected.
However, when I approximate this in the ordinal package, there is no perfect collinearity. I presume this is related that <code>clmm</code> adds so-called 'random effects'. The regression takes a couple of minutes to run but eventually returns results</p>

<pre><code>clmm.complete &lt;- clmm(rating ~ SVs + IVs + (1|SVid) + (1|IVid), data = dt)
</code></pre>

<p>Now, the results here are pretty useless:</p>

<ul>
<li>All but one of my IVs are insignificant</li>
</ul>

<p>I am trying to understand what exactly happens when adding the <code>(1|IVid)</code> term in the <code>clmm</code> model. If it basically adds something like an individual dummy than the fact almost everything is now insignificant is no surprise. The coefficients of the <code>IVid</code> dummies would capture the effect I am looking for (some people rate all scenarios higher or lower, regardless of scenario content) most accurately.</p>

<p>Now I wonder whether this interpretation is correct or whether the results I got from running the simple <code>clm</code> regression are just not reliable? </p>

<p>Concretely, I'd like to find out:</p>

<ul>
<li>What happens when adding a random effect to <code>clmm</code></li>
<li>A laymen explanation of how the Laplace approximation works</li>
<li>How to group errors around individuals when running <code>clm</code></li>
<li>Is it possible to extract the coefficients of these random effects <code>(1|id)</code> for as far as there is such a thing?</li>
</ul>
"
"0.0304713817668003","0.029037395206952","133979","<p>Consider that we have a problem with 4 variables (y, x1, x2 and x3) and we want to do a multiple linear regression model. As we need to know which variables are the most important in the problem, we look for it with a step selection as (it's just an example, we could also used back, both...) :</p>

<pre><code>g0 = lm(Y~1,data=dat)
gxf = formula(gx)
forward=step(g0,scope=gxf,direction=""forward"",test=""F"")
</code></pre>

<p>Suppose that this function says to us that our model should be y ~ ax1 + bx3. If we now do a summary to the object ""forward"" and we get this:</p>

<pre><code>Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.071923   0.150266   0.479    0.636    
X1           0.009716   0.001890   5.140 2.09e-05 ***
X3          -0.013497   0.009230  -1.462    0.155    
</code></pre>

<p>Do we should change our model to y ~ x1? Why isn't significative x3? And in case we change to only y ~ x1, if we do a lm(y~x3) and in a summary of this model now x3 is also significative, what model is better? The one that have a better r^2?</p>
"
"NaN","NaN","134415","<p>I am running a regression in R, and wanted to find the right way to calculate the $R^2$. I have an identity that I am empirically testing with data that is y = x1 - x2 + x3 (unfortunately dont have an MLE) First, I do:</p>

<pre><code>y ~ lm(y ~ x1 + x2 + x3, data=data1) 
</code></pre>

<p>I get the coefficients for x1, x2, and x3 as 1,-1, and 1, and the R^2 as 1.</p>

<p>In the next step, I have to force coefficients of both x1, and x2 to 1, and -1 respectively. I use <a href=""http://stackoverflow.com/questions/8234972/how-to-set-the-coefficient-value-in-regression-r"">http://stackoverflow.com/questions/8234972/how-to-set-the-coefficient-value-in-regression-r</a></p>

<pre><code>y ~ lm(y ~ offset(x1) + offset(-1*x2)), data=data1)
</code></pre>

<p>Note I have omitted x3 in this model, and then have only the constant term. But I get the $R^2$ to be 0.00. I expected the $R^2$ to drop, but not to zero. I believe the constant term should capture/explain some of the variation. So, I think I may not be correctly capturing the $R^2$ for the second model, and have to manually calculate the $R^2$? Any idea on how to calculate/derive the $R^2$ when using offset? </p>
"
"0.114013470672957","0.108647984268766","134499","<h2>Can anyone explain How I can interpret my result from the below model :</h2>

<p>I am trying to build the linear regression model for finding the transaction behavior of the customer in bank accounts. I have created the below model and done the nvcTest for the same. I need some one help to summary the result of this model.</p>

<pre><code>&gt; str(trans_data)
'data.frame':   8597 obs. of  20 variables:
 $ cust_id          : int  1 1 1 1 1 1 1 1 1 1 ...
 $ acc_type_id      : int  3 3 3 3 3 3 3 3 3 3 ...
 $ loc_id           : int  260 144 362 321 114 331 343 17 345 284 ...
 $ tx_type          : Factor w/ 2 levels ""CR"",""DB"": 1 1 1 2 1 1 1 2 2 2 ...
 $ total_bal        : num  8.36e+08 8.70e+08 9.69e+08 8.93e+08 9.60e+08 ...
 $ age              : int  30 30 30 30 30 30 30 30 30 30 ...
 $ gender           : Factor w/ 2 levels ""F"",""M"": 2 2 2 2 2 2 2 2 2 2 ...
 $ city             : chr  ""New Orleans"" ""New Orleans"" ""New Orleans"" ""New Orleans"" ...
 $ county           : chr  ""Orleans"" ""Orleans"" ""Orleans"" ""Orleans"" ...
 $ state            : chr  ""LA"" ""LA"" ""LA"" ""LA"" ...
 $ category         : Factor w/ 6 levels ""HNI"",""LNI"",""MNI"",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ employment_status: Factor w/ 3 levels ""SAL"",""SEL"",""UNE"": 1 1 1 1 1 1 1 1 1 1 ...
 $ marital_status   : Factor w/ 2 levels ""MARRIED"",""UNMARRIED"": 1 1 1 1 1 1 1 1 1 1 ...
 $ branch_num       : int  1 1 1 1 1 1 1 1 1 1 ...
 $ acc_type         : Factor w/ 3 levels ""current"",""savings"",..: 3 3 3 3 3 3 3 3 3 3 ...
 $ acc_sub_type     : Factor w/ 6 levels ""eqty"",""fd"",""gold"",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ yyyy             : int  2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...
 $ mm               : int  1 1 1 1 1 1 1 1 1 1 ...
 $ dd               : int  3 4 8 9 11 12 13 17 21 22 ...
 $ tx_amt           : num  78626273 33171055 98937915 75726140 67162109 ...


model1&lt;-lm(formula = tx_amt ~ tx_type + total_bal +   age + gender + category + employment_status + marital_status + acc_type + 
acc_sub_type + yyyy + mm + dd , data=trans_data)

&gt; summary(model1)

Call:
lm(formula = tx_amt ~ tx_type + total_bal + age + gender + category + 
    employment_status + marital_status + acc_type + acc_sub_type + 
    yyyy + mm + dd, data = trans_data)

Residuals:
      Min        1Q    Median        3Q       Max 
-54030777 -24870155    230953  24933123  52907029 

Coefficients: (2 not defined because of singularities)
                          Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)              1.901e+09  1.161e+09   1.638   0.1015  
tx_typeDB               -6.713e+05  6.245e+05  -1.075   0.2824  
total_bal                1.118e-05  3.877e-04   0.029   0.9770  
age                     -7.315e+03  8.957e+04  -0.082   0.9349  
genderM                  2.784e+05  3.258e+06   0.085   0.9319  
categoryLNI             -1.648e+04  3.142e+06  -0.005   0.9958  
categoryMNI             -3.674e+05  2.095e+06  -0.175   0.8608  
categoryNNI              8.522e+05  5.419e+06   0.157   0.8751  
categoryXXX              2.184e+06  2.425e+06   0.901   0.3677  
categoryYYY             -1.949e+06  1.469e+06  -1.327   0.1847  
employment_statusSEL    -1.219e+06  2.755e+06  -0.443   0.6581  
employment_statusUNE     1.606e+06  1.551e+06   1.035   0.3006  
marital_statusUNMARRIED         NA         NA      NA       NA  
acc_typesavings         -1.882e+06  1.192e+06  -1.580   0.1143  
acc_typesecurity         1.909e+05  5.429e+06   0.035   0.9719  
acc_sub_typefd          -1.019e+07  1.288e+07  -0.791   0.4292  
acc_sub_typegold         9.310e+05  1.522e+06   0.612   0.5406  
acc_sub_typemutual_fnd  -1.245e+06  1.041e+06  -1.196   0.2317  
acc_sub_typenormal       1.446e+05  5.311e+06   0.027   0.9783  
acc_sub_typerd                  NA         NA      NA       NA  
yyyy                    -9.185e+05  5.763e+05  -1.594   0.1110  
mm                          -1.904e+05  8.972e+04  -2.123   0.0338 *
dd                       2.334e+04  3.547e+04   0.658   0.5105  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 28850000 on 8576 degrees of freedom
Multiple R-squared:  0.003322,  Adjusted R-squared:  0.0009977 
F-statistic: 1.429 on 20 and 8576 DF,  p-value: 0.09664

library(car)

durbinWatsonTest(model1)
 lag Autocorrelation D-W Statistic p-value
   1   -0.0008975802      2.001674   0.916
 Alternative hypothesis: rho != 0

ncvTest(model1)
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.132947    Df = 1     p = 0.7153958

model4&lt;-lm(formula = tx_amt ~ cust_id + acc_type_id + loc_id + tx_type + total_bal +   age + gender + category + employment_status + marital_status + acc_type + 
acc_sub_type + yyyy + mm + dd , data=trans_data)

 summary(model4)

Call:
lm(formula = tx_amt ~ cust_id + acc_type_id + loc_id + tx_type + 
    total_bal + age + gender + category + employment_status + 
    marital_status + acc_type + acc_sub_type + yyyy + mm + dd, 
    data = trans_data)

Residuals:
      Min        1Q    Median        3Q       Max 
-54109589 -24870576    214972  24958230  52899612 

Coefficients: (4 not defined because of singularities)
                          Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)              1.891e+09  1.161e+09   1.629   0.1033  
cust_id                  1.915e+05  1.822e+05   1.051   0.2933  
acc_type_id             -4.634e+04  2.656e+06  -0.017   0.9861  
loc_id                  -1.941e+03  2.173e+03  -0.893   0.3718  
tx_typeDB               -6.715e+05  6.245e+05  -1.075   0.2824  
total_bal                1.602e-05  3.877e-04   0.041   0.9670  
age                      9.159e+04  9.163e+04   1.000   0.3175  
genderM                 -4.463e+06  4.155e+06  -1.074   0.2828  
categoryLNI             -2.748e+06  3.170e+06  -0.867   0.3860  
categoryMNI             -2.091e+06  2.305e+06  -0.907   0.3644  
categoryNNI             -6.203e+06  6.973e+06  -0.890   0.3737  
categoryXXX             -1.115e+06  2.649e+06  -0.421   0.6738  
categoryYYY             -1.290e+06  1.242e+06  -1.039   0.2989  
employment_statusSEL    -4.898e+06  2.978e+06  -1.645   0.1001  
employment_statusUNE            NA         NA      NA       NA  
marital_statusUNMARRIED         NA         NA      NA       NA  
acc_typesavings         -1.708e+06  1.084e+07  -0.158   0.8747  
acc_typesecurity         1.317e+05  5.429e+06   0.024   0.9807  
acc_sub_typefd          -1.049e+07  1.209e+07  -0.868   0.3856  
acc_sub_typegold         9.841e+05  3.047e+06   0.323   0.7467  
acc_sub_typemutual_fnd  -1.282e+06  2.874e+06  -0.446   0.6557  
acc_sub_typenormal              NA         NA      NA       NA  
acc_sub_typerd                  NA         NA      NA       NA  
yyyy                    -9.124e+05  5.764e+05  -1.583   0.1134  
mm                      -1.905e+05  8.972e+04  -2.124   0.0337 *
dd                       2.302e+04  3.548e+04   0.649   0.5164  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 28850000 on 8575 degrees of freedom
Multiple R-squared:  0.003415,  Adjusted R-squared:  0.0009741 
F-statistic: 1.399 on 21 and 8575 DF,  p-value: 0.1055

ncvTest(model4)
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.08904766    Df = 1     p = 0.7653914
</code></pre>
"
"0.035185320931284","0.0502942438178979","135292","<p>How do you fit a multilevel regression model in R where you want to include a group-level regression? I think that the answer is using STAN, but I've been trying to use lmer() from the lme4 package.</p>

<p>Say you have data on radon levels in houses within counties. You have a house-level predictor ""Dust"" and a county-level measure of ""Uranium"" and ""Plutonium.""</p>

<p>County level: $ \alpha_j \sim N\big (\gamma_0 + \gamma_1 plut_j + \gamma_2 uran_j, \sigma^2_{alpha}\big)$</p>

<p>House level: $radon_i = \alpha_{j[i]} + \beta dust_i + \epsilon_i$</p>

<p>How would you fit a model at the county level, like the following:
county ~ uranium + plutonium
and then incorporate the new county coefficients in the first model?</p>
"
"0.110147477177496","0.104963924850184","135675","<p>I'm trying to create a regression model for a set of data that includes time and temperature, among others, for 30 minutes intervals throughout the course of a month. I want to build a model that shows how these factors affect air conditioning energy usage. Basically, given time and temperature, I want to predict AC energy usage. This sounds easy enough, and the model for one household would be easily found with:</p>

<pre><code>model = lm(ACEnergy ~ Time + Temperature, data)
</code></pre>

<p>One wrinkle that I wanted to add in is a conditional coefficient on the time and temperature data to differentiate between weekends and weekdays (because household behavior is very different based on whether it is a weekend/weekday) and another conditional coefficient for temperature above and below 70 degrees (because people may use energy very differently at this cutoff). If this isn't clear, here is the basis for the model I want to create:</p>

<p>$$Y = \beta_0 + \beta_1 \times \rm{WeekendTime} + \beta_2 \times \rm{WeekdayTime} + \beta_3 \times \rm{LowTemp} + \beta_4 \times \rm{HighTemp} + \beta_5 \times \rm{LowTemp} + \epsilon$$</p>

<p>Where Y is AC usage for a given household and time. If the given time stamp is during a weekday, $\beta_1$ is equal to 0 and vice versa. If the temperature is below 70 degrees, $\beta_4$ is equal to 0 and vice versa.</p>

<p>I'm having trouble figuring out a way to do this in <strong>R</strong>. Given a chunk of data for a particular household, I can easily separate the chunk into separate lists  where one list has all the records with temperature above 70 degrees, another with all the records with timestamps indicating weekends, etc. I can then find the regression models for each of these lists. The problem is that each sub-list will not just have a different coefficient for the differing time or temperature category, but for all variables and intercepts. This makes it impossible to build the model in the way that I want. Or is there a better way to do this altogether?</p>

<p>I would appreciate any help you can offer. I am new to both R and the finer points of regression.</p>

<p>For clarity, here is an example of some of my data:</p>

<pre><code>Record    TimeStamp            Energy  Temp

1         2009-08-17 16:45:00   0.19    75    
2         2009-08-17 17:15:00   0.28    76   
3         2009-08-17 17:45:00   0.20    76   
4         2009-08-17 18:15:00   0.32    76   
5         2009-08-17 18:45:00   0.27    66   
</code></pre>
"
"0.102279800236246","0.104963924850184","136012","<p>I want to do the following:</p>

<p>1) OLS regression (no penalization term) to get beta coefficients $b_{j}^{*}$; $j$ stands for the variables used to regress. I do this by </p>

<pre><code>lm.model = lm(y~ 0 + x)
betas    = coefficients(lm.model)
</code></pre>

<p>2) Lasso regression with a penalization term, the selection criteria shall be the Bayesian Information Criteria (BIC), given by</p>

<p>$$\lambda _{j} = \frac{\log (T)}{T|b_{j}^{*}|}$$</p>

<p>where $j$ stands for the variable/regressor number, $T$ for the number of observations, and $b_{j}^{*}$ for the initial betas obtained in step 1). I want to have regression results for this specific $\lambda_j$ value, which is different for each regressor used. Hence if there are three variables, there will be three different values $\lambda_j$. </p>

<p>The OLS-Lasso optimization problem is then given by</p>

<p>$$\underset{b\epsilon \mathbb{R}^{n} }{min} = \left \{ \sum_{t=1}^{T}(y_{t}-b^{\top} X_{t}  )^{2} + T\sum_{j=1}^{m} ( \lambda_{t}|b_{j}| )\right \}$$</p>

<p>How can I do this in R with either the lars or glmnet package? I cannot find a way to specify lambda and I am not 100% sure if I get the correct results if I run </p>

<pre><code>lars.model &lt;- lars(x,y,type = ""lasso"", intercept = FALSE)
predict.lars(lars.model, type=""coefficients"", mode=""lambda"")
</code></pre>

<p>I appreciate any help here.</p>

<hr>

<p><strong>Update:</strong></p>

<p>I have used the following code now:</p>

<pre><code>fits.cv = cv.glmnet(x,y,type=""mse"",penalty.factor = pnlty)
lmin    = as.numeric(fits.cv[9]) #lambda.min
fits    = glmnet(x,y, alpha=1, intercept=FALSE, penalty.factor = pnlty)
coef    = coef(fits, s = lmin)
</code></pre>

<p>In line 1 I use cross validation with my specified penalty factor ($\lambda _{j} = \frac{\log (T)}{T|b_{j}^{*}|}$), which is different for each regressor. 
Line 2 selects the ""lambda.min"" of fits.cv, which is the lambda that gives minimum mean cross-validation error.
Line 3 performs a lasso fit (<code>alpha=1</code>) on the data. Again I used the penalty factor $\lambda$.
Line 4 extracts the coefficients from fits which belong to the ""optimal"" $\lambda$ chosen in line 2.</p>

<p>Now I have the beta coefficients for the regressors which depict the optimal solution of the minimization problem </p>

<p>$$\underset{b\epsilon \mathbb{R}^{n} }{min} = \left \{ \sum_{t=1}^{T}(y_{t}-b^{\top} X_{t}  )^{2} + T\sum_{j=1}^{m} ( \lambda_{t}|b_{j}| )\right \}$$</p>

<p>with a penalty factor $\lambda _{j} = \frac{\log (T)}{T|b_{j}^{*}|}$. The optimal set of coefficients is most likely a subset of the regressors which I initially used, this is a consequence of the Lasso method which shrinks down the number of used regressors.</p>

<p>Is my understanding and the code correct? </p>
"
"0.068136080998913","0.0649295895722714","136040","<p>I'm implementing a logistic regression model in R and I have 80 variables to chose from. I need to automatize the process of variable selection of the model so I'm using the step function.</p>

<p>I've no problem using the function or finding the model, but when I look at the final model I find that some of the variables chosen by the step function are not significant (I look at this using the summary function and looking at the fourth column in $coef, this is the Wald Test). This is a problem because I need all the variables included in the model to be significant.</p>

<p>Is there any function or any way to get the best model based on AIC or BIC methods but that also consider that all the coefficients must be significant?
Thanks</p>
"
"0.0304713817668003","0.029037395206952","136071","<p>I am hoping someone can check this code to ensure that I have interpreted the various pieces of PCA correctly. I am trying to figure out a way to identify the leading contributors to the performance of multiple securities. For example, one idea I had was to run a multivariate regression using the securities returns as dependent variables and include things like oil, the dollar, the euro, treasury yields, etc.
E.g.,
SBUX + AAPL + MCD + BAC + TWTR = intercept + oil + dollar + euro + steel + gold + e</p>

<p>I then thought that PCA would probably be better suited for this type of exercise. Here is my code from R. The csv file consists of a matrix of 900 securities and 30 rows (30 daily returns for 900 securities)</p>

<pre><code>FD &lt;- read.csv(""U:/Personal Projects/R/Data Files/FD Securities Jan 2015.csv"")

#Removes columns with any na values
FD1 &lt;- FD[, sapply(FD, function(x) !any(is.na(x)))]

#removes ""zero/constant variance"" columns, which I think are NaNs I couldnt erase using is.nan
FD2 &lt;- FD1[,apply(FD1, 2, var, na.rm=TRUE) != 0]

#Calc PCs using singe value decomposition (prcomp). Should data be a correlation matrix of the variables? I get reasonable looking results both ways, i.e., PC1 explains 30-50% of variance, PC2 ~10%-15%, etc.
FD2.pca &lt;- prcomp(cor(FD2), retx = TRUE, scale = TRUE)
summary(FD2.pca)
plot(FD2.pca)

#These are the 'loadings', i.e., coefficients used for each linear combination?
as.matrix(FD2.pca$rotation[,1])

#I think these ""scores"" are the coefficients of interest, as they incorporate the factor     weightings because the output is pca$rotation * scale (stddev of each factor)
as.matrix(FD2.pca$x[,1]) as.matrix(FD2.pca$x[,2]) as.matrix(FD2.pca$x[,3])     as.matrix(FD2.pca$x[,4])

#Scatterplot of the first two principal components. Not sure if this is right of if $x should be used.
plot(x = FD2.pca$rotation[,1], FD2.pca$rotation[,2], xlab = ""PC1"", ylab = ""PC2"", main=""Principal Component Analysis:"")
</code></pre>
"
"0.075412822378","0.0821301562353182","136094","<p>I was trying to get an intuition for the interpretation of the coefficients in a logistic regression that was intended to reproduce to some extent that presented in a youtube video (<a href=""http://youtu.be/vq-_4kWmzTo"" rel=""nofollow"">http://youtu.be/vq-_4kWmzTo</a>). So I created a fictitious data set reflecting the chances of getting accepted (Accepted: int: 0 0 ... 1 0) into a college as related to SAT scores (int 1136 1347 1504) and family/ethnic background (categories = ""red"" vs ""blue""). </p>

<pre><code>fit &lt;- glm(Accepted ~ Background - 1,data=dat, family=""binomial"")
exp(cbind(OR = coef(fit),confint(fit)))
</code></pre>

<p>yielded:</p>

<pre><code>                      OR     2.5 %    97.5 %
Backgroundblue 0.7088608 0.5553459 0.9017961
Backgroundred  1.7352941 1.3632702 2.2206569
</code></pre>

<p>The interpretation seemed easy: Red applicants have 1.7 times more chances of getting in over the rest; blue applicants were at a disadvantage, and had  7 over 10 odds of getting in.</p>

<p>However, the more complete model,</p>

<pre><code>fit &lt;- glm(Accepted ~ SAT.scores + Background - 1,data=dat, family=""binomial"")
exp(cbind(OR = coef(fit),confint(fit)))
</code></pre>

<p>yielded coefficients for background that are difficult to reconcile or interpret:</p>

<pre><code>                         OR        2.5 %       97.5 %
SAT.scores     1.008558e+00 1.006940e+00 1.010297e+00
Backgroundblue 8.730056e-06 8.459031e-07 7.634723e-05
Backgroundred  2.329513e-05 2.426748e-06 1.929259e-04
</code></pre>

<p>Can you help point out what I am missing? Thank you.</p>

<p>Thanks to the enlightening answer from Maarten below, I was able to make some progress, and obtain the correct Odds Ratios without and with the SAT confounder:</p>

<p>Here is just regressing to Background (""Red"" versus ""Blue""):</p>

<pre><code>fit &lt;- glm(Accepted ~ Background, data = dat, family = ""binomial"")
exp(cbind(Odds_Ratio_RedvBlue = coef(fit), confint(fit)))

                        Odds_Ratio_RedvBlue             2.5 %       97.5 %
(Intercept)             0.7088608                     0.5553459   0.9017961
Backgroundred           2.4480042                     1.7397640   3.4595454
</code></pre>

<p>Which brought up a couple of additional questions (probably very basic): 1. Why is the Odds Ratio of the (Intercept) - 0.7088608 - the same for this model as for the <em>Odds</em> - as opposed to <em>Odds Ratio</em> - of the Background Blue in the model without the intercept above? And 2. Shouldn't the OddsRatio of Blue and Red be the reciprocal of each other $OddsRatioBlue = 1 / OddsRatioRed$?</p>
"
"0.109866129394437","0.104695817324578","136146","<p>I'm trying to manually demean panel data by both time demeaning and cross-sectional demeaning, yet haven't been able to do it well. The best thing I can think of is looping through the means for each year and then for each entity, plus for each variable. Which seems like it would get slow with a lot of a data (especially if I plan on doing simulations). Is there a better way to do this?</p>

<p>I couldn't find a function in R to do it either.</p>

<p>Here is what I have so far: (DATA is a data frame that of the format (n,T, Variables) where n is the entity id and T is the time id); </p>

<pre><code>demean=function(DATA){

names=colnames(DATA)
T=(max(DATA[,2])-min(DATA[,2])+1)
N=max(DATA[,1])

##Cross-Sectional Demeaning
widedata=reshape(DATA, direction=""wide"", v.names=names[-c(1:2)],     idvar=names[2], timevar=names[1])
crossmean=matrix(NA, ncol=length(colnames(widedata))-1)
crossmean[,1:length(t(crossmean[1,]))]=colMeans(widedata[,-c(1)], na.rm=TRUE)
crosswidedata=widedata
for(i in 1:T){
crosswidedata[i,-c(1)]=widedata[i,-c(1)]-crossmean
}

crossdemeaned=reshape(crosswidedata, direction=""long"", times=names[2] )

##Time Demeaning
widedata=reshape(crossdemeaned, direction=""wide"", v.names=names[-c(1:2)], idvar=names[1], timevar=names[2])
timemean=matrix(NA, ncol=length(colnames(widedata))-1)
timemean[,1:length((t(timemean[1,])))]=colMeans(widedata[,-c(1)], na.rm=TRUE)
timewidedata=widedata
for(i in 1:N){
timewidedata[i,-c(1)]=widedata[i,-c(1)]-timemean
}

demeaned=reshape(timewidedata, direction=""long"", times=names[2] )


demeaned=demeaned[order(demeaned[,names[2]],demeaned[,names[1]]),]

return(demeaned)
}
</code></pre>

<p>I simulate a random spatial autoregressive panel data set with fixed and time effects, demean the data, and then run a test within regression of the demeaned data as following:</p>

<pre><code>    ##Creating a Random Spatial Autoregressive Panel Dataset with Fixed and Time Effects

library(spdep)
library(splm)

set.seed(44222)

################################################################
# RANDOMLY INSERT A CERTAIN PROPORTION OF NAs INTO A DATAFRAME #
################################################################
NAins &lt;-  NAinsert &lt;- function(df, prop = .1){
n &lt;- nrow(df)
m &lt;- ncol(df)
num.to.na &lt;- ceiling(prop*n*m)
id &lt;- sample(0:(m*n-1), num.to.na, replace = FALSE)
rows &lt;- id %/% m + 1
cols &lt;- id %% m + 1
sapply(seq(num.to.na), function(x){
        df[rows[x], cols[x]] &lt;&lt;- NA
    }
)
return(df)
}
############## df means data frame, and prop is the proportion of missing ##data desired

############################
########CREATES A RANDOM WEIGHT MATRIX
############################
DAG.random &lt;- function(v, nedges=1) {
edges.max &lt;- v*(v-1)/2
# Assert length(v)==1 &amp;&amp; 1 &lt;= v
# Assert 0 &lt;= nedges &lt;= edges.max
index.edges &lt;- lapply(list(1:(v-1)), function(k) rep(k*(k+1)/2, v-k)) 
index.edges &lt;- index.edges[[1]] + 1:edges.max
graph.adjacency &lt;- matrix(0, ncol=v, nrow=v)
graph.adjacency[sample(index.edges, nedges)] &lt;- 1
graph.adjacency
}

###################################
###### Create Data ################
###################################

n=50                                    ##Number of Observations
T=20                                    ##Number of years
connect=6                               ##average number of connections
W=DAG.random(n,n*.5*connect)            ##Creates a random weight matrix      
W=W+t(W)
W=sweep(W, 1, rowSums(W), FUN=""/"")      ##Row Normalizes the Random Weight

v=3                                     ##Number of Independent Variables

X=matrix(data=NA, nrow=n*T, ncol=(v+2))
for(jjj in 1:T){
X[((jjj-1)*n+1):(jjj*n),1]=c(1:n)
X[((jjj-1)*n+1):(jjj*n),2]=jjj
for(jj in 3:(v+2)){
X[((jjj-1)*n+1):(jjj*n),jj]=rnorm(n, 1, 2)
}
}


sigma=1                             ##standard deviation of model
p=.9                                ##coefficient on spatial lag of Y
B0=matrix(1, nrow=n, ncol=1)
B1=matrix(2, v , 1)
B2=matrix(.5, v, 1)
yrcoef=matrix(runif(1:(T), -20,20), (T), 1)     #Year Fixed Effects
yrcoef[1]=0
alpha=runif(n, -20,20)                      # Nation Fixed Effects

e=matrix(rnorm(n*T,0,sigma), n*T, 1)

inv=solve((diag(n)-p*W))
Y=matrix(data=NA, nrow=n*T, ncol=1)

for(jjj in 1:T){
invY= as.matrix(alpha) + X[((jjj-1)*n+1):(jjj*n),(3:(2+v))] %*% B1 +    yrcoef[jjj,1] + e[((jjj-1)*n+1):(jjj*n),1]
Y[((jjj-1)*n+1):(jjj*n),1]=inv %*% invY
}

Y=data.frame(Y)
X=data.frame(X)
DATA=cbind(X,Y)
names=c(""N"", ""T"", ""X1"", ""X2"", ""X3"", ""Y"")
colnames(DATA)=names

DATA=demean(DATA)

model1=as.formula(paste(names[length(names)], paste(names[-c(1:2,length(names))], collapse= "" + ""), sep="" ~ ""))

lw=mat2listw(W)

mod=spml(model1, data=DATA, listw=lw, model=""within"", effect=c(""twoways""),    lag=TRUE, spatial.error=""none"")

effects(mod)
</code></pre>

<p>However, the time period effects are non-zero (the spatial fixed effects are numerically zero). I can't seem to figure out why the spatial fixed effects is working while the time period effects portion is not.</p>

<p>Any thoughts?</p>
"
"0.0430930413588572","0.0410650781176591","136563","<p>I am using the <code>nnls()</code> function from the <code>nnls package</code> in R to do a linear regression for regressors $x_i$ and observations $y$.
The function delivers beta coefficients $\beta_i\geq{0}, \forall i$. However, is it possible to apply the constraints only to <strong>some</strong> regressors so that</p>

<p>$$\beta_k \geq 0 \quad k \in \{1...,10\}, k\neq i \\ 
\beta_i \in \mathbb{R} \quad i \in \{1...,10\}$$</p>

<p>given that I have 10 regressor variables?</p>

<p><code>nnls()</code> offers the possibility to enforce some coefficients to be negative and others to be positive. I only want the positive constraint for some of them, the other ones can be either positive or negative.</p>
"
"0.052777981396926","0.0502942438178979","136619","<p>I want to create a linear regression model to predict an output that uses two different coefficients based on some threshold within the data. For example: <code>df</code>:</p>

<pre><code>Value   Temperature
8.2     70
3.2     51
5.8     54
7.2     61
</code></pre>

<p>and so on. For this data, I want to figure out how to make the following model:</p>

<pre><code>Value = B0 + B1(HighTemp) + B2(LowTemp)
</code></pre>

<p>Where B1 is 0 if the temperature is below 55, and B2 is 0 is the temperature is above 55. I tried the following:  </p>

<pre><code>fit  = lm(Value ~ I(Temperature&gt;55), data=df) 
fit2 = lm(Value ~ Temperature * I(Temperature&gt;55), data=df) 
</code></pre>

<p><code>fit</code> only gives me a coefficient for when the temperature is above 55, and <code>fit2</code> gives output that I don't fully understand. I was also thinking of creating a third column, <code>HighorLow</code>, with an indicator variable (1 or 0) for whether or not the temperature is high or low. The I would have:</p>

<pre><code>fit = lm(Value~Temperature:HighorLow, data = df)
</code></pre>

<p>Does anyone have any input? </p>
"
"0.118246329960506","0.119724247531067","136843","<p>I have a Cox proportional hazards model in R (see made-up example below) that models the effect of some variable, say weight. From this model, I'd like to extrapolate what a change in weight from say 90 to 60 would mean to survival, taking into account the fact that for such a change occurring at say age 40, certain amount of risk has already accumulated (and assuming weight change is instantaneous).</p>

<p>I've attached some code which involves</p>

<ol>
<li>fitting the Cox model (using age as the time scale);</li>
<li>extracting the predicted cumulative survival $S(t)$ using survfit for weight=90 and 60;</li>
<li>getting the cumulative hazard $H(t) = -\log(S(t))$;</li>
<li>getting the ""instantaneous"" hazard $h(t)$ via differencing $H(t)$ (plus small fudge factor to avoid zero hazard), which seems to do the job but probably a bit hacky;</li>
<li>adding a constant to the $\log(h(t))$ for all timepoints after the change, equivalent to the $\beta$ coefficient from the Cox regression times the <em>difference</em> in weights (90-60=30);</li>
<li>get the new survival functions $S^\prime(t)$ as $\exp(-{\rm cumsum}(\exp(\log(h^\prime(t)))))$.</li>
</ol>

<p>This procedure produces reasonable results (plotted as $1 - S(t)$), but is it correct or am I just lucky?</p>

<p><img src=""http://i.stack.imgur.com/ax5Bu.png"" alt=""enter image description here""></p>

<pre><code>library(survival)
set.seed(1)
rm(list=ls())

# Simulate some semi-realistic data
n      &lt;- 1e3
age    &lt;- round(runif(n, 1, 60))
weight &lt;- round(rnorm(n, 70, 10))
height &lt;- round(runif(n, 1.3, 1.9), 2)
sex    &lt;- sample(c(""M"", ""F""), length(age), replace=TRUE, prob=c(0.7, 0.3))
d.time &lt;- ceiling(rexp(n, weight / 1e4))
cens   &lt;- round(runif(n, 1, 60))
death  &lt;- d.time &lt;= cens
d.time &lt;- pmin(d.time, cens)
d      &lt;- data.frame(age=age, weight=weight, height=height, difftime=d.time, 
                     time=d.time + age, sex=sex, death=death)

s     &lt;- coxph(Surv(age, time, death) ~ height + weight, data=d)
d.new &lt;- data.frame(weight=c(60, 90), height=1.7)
sf    &lt;- survfit(s, d.new)

# The cumulative hazard function H is -log(S(t)) where S(t) is the survivor function
# (aka cumulative survival)
S &lt;- sf$surv[,2]

# Assume we start off with high weight
H &lt;- -log(S)

# The hazard is the derivative (here, finite difference) of the cumulative hazard H
# But the hazard can't be zero exactly as when we take log hazard, won't make sense
h &lt;- diff(c(0, H)) + 1e-6

# We introduce a changepoint in the hazard, but must make sure that the
# hazard does not become negative - this is naturally achieved because the
# Cox model is linear in the log-hazard. This means that the final survivor
# function will always be monotonically decreasing for any value of delta in 
# (-Inf, +Inf); delta &gt; 0 increases hazard, delta &lt; 0 decreases hazard
delta &lt;- coef(s)[""weight""] * (d.new$weight[1] - d.new$weight[2])
logh  &lt;- log(h)
age   &lt;- 40
logh[sf$time &gt; age] &lt;- logh[sf$time &gt; age] + delta
h     &lt;- exp(logh)

# Get the new cumulative hazard and new survivor functions
H &lt;- cumsum(h)
S &lt;- exp(-H)

# Compare original survivor function with modified one
plot(sf, lwd=5, col=1:2, conf.int=FALSE, mark=NA, fun=""event"",
     xlab=""Age"", ylab=""Cumulative risk"")
lines(c(0, sf$time), 1 - c(1, S), type=""s"", col=3, lwd=5)
abline(v=age, lty=2)
legend(x=""topleft"", legend=c(""Weight=60"", ""Weight=90"", ""Weight decreased 90 to 60""),
       col=1:3, lwd=5)
</code></pre>
"
"0.075412822378","0.0821301562353182","136925","<p>I have some short grouped time series data. I would like to fit a dynamic multilevel regression model in R, with random coefficients for the mean and first order auto-correlation in each group, and with no cross correlation between the two variance parameters; i.e. this model:</p>

<p>$y_{i,t} - \mu_{i} = \rho_{i} (y_{i,t-1} - \mu_{i}) + \epsilon_{i,t}$</p>

<p>$\mu_{i} = \mu + v_{\mu}$ </p>

<p>$\rho_{i} = \rho + v_{\rho}$ </p>

<p>$\epsilon_{i,t} \sim N(0,\sigma_\epsilon^2), v_{\mu} \sim N(0,\sigma_\mu^2), v_{\rho} \sim N(0,\sigma_\rho^2)$ </p>

<p>The best I can do so far in R is:</p>

<pre><code>library(nlme)
library(dplyr)

#create toy data set
df0 &lt;- Orthodont %&gt;% 
  group_by(Subject) %&gt;% 
  mutate(lag1=lag(distance)) %&gt;% 
  filter(!is.na(lag1))

#multilevel model, mean (not Subject specific mean) centered
m1 &lt;- lme(fixed = distance ~ I(lag1 - mean(distance)), data=df0, 
          random= list(Subject = pdDiag(~ + I(lag1 - mean(distance)))) )
m1
# Linear mixed-effects model fit by REML
#   Data: df0 
#   Log-restricted-likelihood: -164.8976
#   Fixed: distance ~ I(lag1 - mean(distance)) 
#              (Intercept) I(lag1 - mean(distance)) 
#               25.7907622                0.8289975 
# 
# Random effects:
#  Formula: ~+I(lag1 - mean(distance)) | Subject
#  Structure: Diagonal
#          (Intercept) I(lag1 - mean(distance)) Residual
# StdDev: 7.892818e-05                0.3031237 1.675277
# 
# Number of Observations: 81
# Number of Groups: 27 
</code></pre>

<p>This 1) does not estimate ($\mu$) and 2) centres the distance lag ($y_{i,t-1}$) on the population mean ($\mu$) rather than the subject ($i$) specific mean ($\mu_i$) that I desire. </p>
"
"0.075412822378","0.0821301562353182","137673","<p>This is the data set I have:</p>

<pre><code>vector &lt;- c( -7.459981, 13.26651, 12.10128, 2.380662, 26.42393)
</code></pre>

<p>Doing an estimation of the coefficient with a linear regression in Java and R I got different results. This is my code in Java.</p>

<pre><code>List&lt;Double&gt; y= new ArrayList&lt;&gt;(Arrays.asList(
        -7.4599812, 13.2665113, 12.1012781, 2.3806622, 26.4239262
        ));
SimpleRegression ls = new SimpleRegression();
List&lt;Double&gt; yNew = new ArrayList&lt;Double&gt;();
for (int i = 0; i &lt; y.size(); i++) {
    ls.addData(i, y.get(i));
}

double alpha0 = ls.getIntercept();
double beta0 = ls.getSlope();
double coefficients[] = { alpha0, beta0 };
</code></pre>

<p>This is the output result:</p>

<pre><code>alpha0 = -2.0339138199999995
beta0  =  5.68819657
</code></pre>

<p>In R it seems to be that we should start for x with 1 instead of 0:</p>

<blockquote>
  <p>lsfit(x, y, wt = NULL, intercept = TRUE, tolerance = 1e-07, yname =
  NULL)</p>
</blockquote>

<p>Starting with 1 instead of 0 in R, I get:</p>

<pre><code>vector &lt;- c( -7.459981, 13.26651, 12.10128, 2.380662, 26.42393)
vecx &lt;- c(1,2,3,4,5)
lsfit(vecx, vector)
alpha = -7.72211 
beta = 5.688197
</code></pre>

<p>When I start with 0 in R, I get: </p>

<pre><code>vector &lt;- c( -7.459981, 13.26651, 12.10128, 2.380662, 26.42393)
vecx &lt;- c(0,1,2,3,4)
lsfit(vecx, vector)
alpha = -2.033915  
beta = 5.688197
</code></pre>

<p>which is the same value as my code in Java. On the internet I have found that the values of my Java code and with vecx &lt;- c(0,1,2,3,4) in R are the values which are correct but I am not sure. </p>

<p><strong>My question is: What is the truth? To start with 0 or with 1 to estimate the coefficients?</strong> </p>

<p>Thanks for your answer. </p>
"
"0.0770871758684916","0.0734594449379399","138276","<p>I conducted some experiments where I modified two variables (var 1 and var 2), and measured an outcome variable (yield). For each combination of var 1 and var 2, I assigned a TestNumber. Some combinations of var 1 and var 2 were tested with a single individual (such as TestNumber 9 in the figure), some were tested with two individuals (such as TestNumber 6 in the figure), and others were tested with three individuals (such as TestNumber 1 in the figure).</p>

<p>I'd like to determine if there is any systematic difference between the three individuals (orange, purple, and black), so that I can know if I can pool their results together for downstream analyses. But I want to take into account the fact that some TestNumbers generally performed better than others.</p>

<p>I've done linear regression, trying to predict the yield using the individual as the predictor variable:</p>

<pre><code>Call:
lm(formula = exampleData$Yield ~ exampleData$individual)
 Residuals:
     Min       1Q   Median       3Q      Max 
 -0.56544 -0.19684  0.03037  0.23367  0.39646 

Coefficients:
                              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                   0.58854    0.05335  11.031   &lt;2e-16 ***
exampleData$individualorange  0.11139    0.07627   1.460    0.149    
exampleData$individualpurple  0.08079    0.07399   1.092    0.279    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.2614 on 70 degrees of freedom
Multiple R-squared:  0.03184,   Adjusted R-squared:  0.004178 
F-statistic: 1.151 on 2 and 70 DF,  p-value: 0.3222
</code></pre>

<p>However, I don't think that's good enough, because it ignores the information from var1 and var2--one of the individuals might be present at a higher frequency in the higher TestNumbers, which have a higher yield.</p>

<p>If I try <code>summary(lm(exampleData$Yield ~ exampleData$var1 + exampleData$var2 + exampleData$individual))</code> then I also get a large p-value for the slope on exampleData$individual, but is that sufficient to determine that the three individuals are not distinguishable?</p>

<p>Any pointing in the right direction would be much appreciated. Thanks very much!</p>

<p><img src=""http://i.stack.imgur.com/6USl8.png"" alt=""enter image description here""></p>

<p><strong>Data</strong></p>

<pre><code>individual condition var1 var2 Yield
black      1         10   15   0.2131
purple     1         10   15   0.3189
orange     1         10   15   0.9993
purple     2         12   15   0.1667
orange     2         12   15   0.2423
black      2         12   15   0.1462
purple     3         13   15   0.1876
black      3         13   15   0.2297
black      4         18   15   0.8693
purple     4         18   15   0.6991
orange     5         25   15   0.804
black      5         25   15   0.2084
purple     5         25   15   0.9396
black      6         30   15   0.0231
purple     6         30   15   0.5337
orange     7         10   45   0.4939
orange     8         12   45   0.446
orange     9         13   45   0.7962
black     10         15   45   0.6812
purple    10         15   45   0.5147
orange    10         15   45   0.439
black     11         17   45   0.2627
purple    11         17   45   0.4074
orange    11         17   45   0.9948
black     12         20   45   0.9716
purple    12         20   45   0.8603
orange    12         20   45   0.9434
black     13         26   45   0.6757
purple    13         26   45   0.8618
black     14         28   45   0.3611
purple    14         28   45   0.7874
orange    14         28   45   0.2899
black     15         30   45   0.5435
purple    15         30   45   0.4882
black     16         34   45   0.5456
purple    16         34   45   0.491
orange    16         34   45   0.619
purple    17         45   45   0.698
black     17         45   45   0.3917
orange    17         45   45   0.9568
black     18         50   45   0.9163
purple    18         50   45   0.3317
purple    19         55   45   0.8703
orange    20         56   45   0.4262
black     20         56   45   0.4751
black     21         59   45   0.6788
purple    21         59   45   0.9172
orange    21         59   45   0.5628
orange    22         65   45   0.9585
purple    22         65   45   0.9056
black     22         65   45   0.9022
orange    23         10   90   0.8286
purple    23         10   90   0.703
black     23         10   90   0.8413
orange    24         15   90   0.9259
purple    24         15   90   0.903
black     24         15   90   0.5991
orange    25         20   90   0.7303
purple    25         20   90   0.5813
black     26         21   90   0.9131
orange    26         21   90   0.7542
purple    26         21   90   0.9896
orange    27         28   90   0.5668
purple    27         28   90   0.8928
purple    28         30   90   0.8766
orange    29         35   90   0.6794
black     30         41   90   0.8845
black     31         45   90   0.8067
orange    31         45   90   0.6489
purple    32         46   90   0.6454
black     32         48   90   0.985
orange    32         48   90   0.9922
purple    33         50   90   0.8317
</code></pre>
"
"0.0806196982594614","0.0658506226617377","138938","<p>What is the correct way to calculate the standard errors of the coefficients in a weighted linear regression?</p>

<p>The regression equation I am using is $y_i = a + bx_i$, and I have weights, $w_i = 1/\sigma_i$.  The numerical recipes formula for a straight line fit, and the formula given in ""An introduction to error analysis"" by J. R Taylor, (and Wikipedia too) state that the standard error in the $b$ coefficient is calculated as $$\sigma_b = \sqrt{\frac{\sum w_i}{\sum w_i\sum w_i x_i^2-(\sum w_i x_i)^2}}$$ (or alternatively in matrix form the standard errors are, $\sigma^2 = (X'WX)^{-1}$).  This formula can be derived from propagation of errors. </p>

<p>Using R's $lm()$ function (and python's StatsModels), I get a standard error in the $b$ coefficient which appears* to be calculated as $$\sigma_b = \sigma_e\sqrt{\frac{\sum w_i}{\sum w_i\sum w_i x_i^2-(\sum w_i x_i)^2}}$$
where $\sigma_e^2 = \sum w_i(y_i - a - bx_i)^2/(N-2)$ (alternatively, $\sigma^2 = \sigma_e^2(X'WX)^{-1}$ ).  So they are the same, except for the $\sigma_e$ multiplier in R and StatsModel.</p>

<p>Is it possible that these actually different measures that are just being called the same thing? Is one preferred over the other for an estimate of the standard error?</p>

<p>*I say ""appears"" because I couldn't find the actual formula anywhere.</p>

<p>edited because I had omitted the weight terms in the denominators.   </p>
"
"0.110147477177496","0.0974665016465992","139528","<p>When modelling continuous proportions (e.g. proportional vegetation cover at survey quadrats, or proportion of time engaged in an activity), logistic regression is considered inappropriate (e.g. <a href=""http://www.esajournals.org/doi/full/10.1890/10-0340.1"" rel=""nofollow"">Warton &amp; Hui (2011) The arcsine is asinine: the analysis of proportions in ecology</a>). Rather, OLS regression after logit-transforming the proportions, or perhaps beta regression, are more appropriate.</p>

<p>Under what conditions do the coefficient estimates of logit-linear regression and logistic regression differ when using R's <code>lm</code> and <code>glm</code>?</p>

<p>Take the following simulated dataset, where we can assume that <code>p</code> are our raw data (i.e. continuous proportions, rather than representing ${n_{successes}\over n_{trials}}$):</p>

<pre><code>set.seed(1)
x &lt;- rnorm(1000)
a &lt;- runif(1)
b &lt;- runif(1)
logit.p &lt;- a + b*x + rnorm(1000, 0, 0.2)
p &lt;- plogis(logit.p)

plot(p ~ x, ylim=c(0, 1))
</code></pre>

<p><img src=""http://i.stack.imgur.com/AzWOX.png"" alt=""enter image description here""></p>

<p>Fitting a logit-linear model, we obtain:</p>

<pre><code>summary(lm(logit.p ~ x))
## 
## Call:
## lm(formula = logit.p ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.64702 -0.13747 -0.00345  0.15077  0.73148 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.868148   0.006579   131.9   &lt;2e-16 ***
## x           0.967129   0.006360   152.1   &lt;2e-16 ***
## ---
## Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
## 
## Residual standard error: 0.208 on 998 degrees of freedom
## Multiple R-squared:  0.9586, Adjusted R-squared:  0.9586 
## F-statistic: 2.312e+04 on 1 and 998 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>Logistic regression yields:</p>

<pre><code>summary(glm(p ~ x, family=binomial))
## 
## Call:
## glm(formula = p ~ x, family = binomial)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -0.32099  -0.05475   0.00066   0.05948   0.36307  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  0.86242    0.07684   11.22   &lt;2e-16 ***
## x            0.96128    0.08395   11.45   &lt;2e-16 ***
## ---
## Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 176.1082  on 999  degrees of freedom
## Residual deviance:   7.9899  on 998  degrees of freedom
## AIC: 701.71
## 
## Number of Fisher Scoring iterations: 5
## 
## Warning message:
## In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>Will the logistic regression coefficient estimates always be unbiased with respect to the logit-linear model's estimates?</p>
"
"0.035185320931284","0.0502942438178979","139928","<p>how to simulate data (in R) to generate , sample values
1) variables with specific correlation values for a particular model AND 
2) with predefined regression coefficients? 
3) Can we also set the mean and SD in the same process? 
4) Also how does one simulate the p value/significance of the variable. </p>

<p>This is for imitating existing models for analysis and teaching purposes</p>

<p>Sorry for not being specific : this is for multiple regression, sample values. I would like to specify the mean and SD if possible (apparently not, I can specify only one in order to specify the regression coefficients?)</p>

<p>Thanks for the help.</p>
"
"0.075412822378","0.0821301562353182","139988","<p>For example, if you have a logistic regression on certain dataset:</p>

<pre><code>fit &lt;- glm(y ~ x, data = test, family = ""binomial"")
</code></pre>

<p>If you do <code>predict(fit, newdata, type = ""link"", se = TRUE)</code>, you will get a column named <code>se.fit</code>, which is the standard error for each predicted y value.</p>

<p>My questions are:  </p>

<ol>
<li><p>How is the MSE value for the link function is computed here?  </p>

<p>The variance of the fitting coefficients are basically the MSE times the variance-covariance matrix, there should be a way to compute the MSE value first. But for response variables that have 0 and 1 values, the link function corresponds to 0 and infinity. In this case, how does the model compute this value? Is there any way I can get the MSE value for the <code>glm</code> fitting in R?</p></li>
<li><p>Is <code>se.fit</code> the standard error for the link function value of the fitted line at point <code>x0</code>, or the standard error for the predicted link function value of <code>y</code> at point <code>x0</code>?</p></li>
</ol>
"
"0.0304713817668003","0","140381","<p>I have extremely large number of observations (8524152) of soil moisture, precipitation, evapotranspiration, delta precipitation, and delta evapotranspiration. I ran a multiple linear regression model and my result looks like </p>

<pre><code>Call:
lm(formula = SMDI ~ ET + delta_ET + PRCP + delta_PRCP, data = regData)

Residuals:
 Min  vvvv     1Q   Median       3Q      Max 
-10414.0     67.1    133.9    192.2   8737.3 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -87.508196   0.797889 -109.67   &lt;2e-16 ***
ET            0.083853   0.001225   68.46   &lt;2e-16 ***
delta_ET      0.267973   0.001270  211.04   &lt;2e-16 ***
PRCP          0.237649   0.003255   73.02   &lt;2e-16 ***
delta_PRCP    0.257458   0.003250   79.23   &lt;2e-16 ***



Residual standard error: 1705 on 8524147 degrees of freedom
Multiple R-squared:  0.4424,    Adjusted R-squared:  0.4424 
F-statistic: 1.691e+06 on 4 and 8524147 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>The t-stat for evapotranspiration (ET), Precipitation (PRCP), delta_PRCP, and delta_ET are same, and the combined p-value is also extremely small. allmost &lt; 2.2e-16. is this possible?</p>

<p>Juvin</p>
"
"0.0304713817668003","0.029037395206952","140509","<p>I used logistic regression and found that my model fits well: </p>

<pre><code>Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.6434  -1.4623   0.8704   0.9013   1.0066  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   0.41595    0.02115   19.67   &lt;2e-16 ***
init_att_cnt  0.02115    0.00146   14.48   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
(Dispersion parameter for binomial family taken to be 1)

Null deviance: 154956  on 122239  degrees of freedom
Residual deviance: 154746  on 122238  degrees of freedom
AIC: 154750
</code></pre>

<p>The chi-squared test is hightly statisticaly significant: <code>p = 9.642755e-48</code>. I decided to check the Nagelkerke $R^2$ statistic, </p>

<pre><code>R2 &lt;- R2/(1-exp((-mylogit$null.deviance)/n))
</code></pre>

<p>but it was $R^2 = 0.001350927$. This is unbelievable, why is $R^2$ so small, if my model fits well?</p>
"
"0.129930408420542","0.11762507572524","140600","<p>I'm running a fixed effects logistic regression in R. The model consists of a binary outcome and two binary predictors, with no interaction term. On the log-odds scale, and as an odds-ratio, the coefficient for one of the predictors (<code>carbf</code> in the mocked-up example below) indicates that the expected probability of Y=1 (""success"") is different between the two levels of the factor (i.e., the effect is significant). </p>

<p>When I use the <code>effects</code> package to get marginal predicted probabilities, the 95% CIs for the two levels of <code>carbf</code> overlap considerably, indicating there is no evidence of a difference in the expected probability of Y=1 between the two factor levels.</p>

<p>When I use the <code>mfx</code> package to get average marginal effects for the coefficients (i.e., for the expected <em>difference</em> in the probability of Y=1 between the two factor levels), I do get a significant difference.</p>

<p><strong>I'm confused as to whether this discrepancy is because:</strong> </p>

<p><strong>1) the output from the model and the <code>mfx</code> package is an expected <em>difference</em> in the probability of Y=1 between factor levels, rather than predicted probabilities for each level.</strong></p>

<p><strong>2) of the way the <code>effects</code> package is calculating the marginal effect.</strong> </p>

<p>In an effort to determine this, I modified the source code from the <code>mfx</code> package to give me average marginal effects for each level of the <code>carbf</code> factor. The 95% CIs for these predictions <em>do not</em> overlap, indicating a significant difference. This makes me wonder why I get such different results using the <code>effects</code> package. Or is it that I'm just confused about the difference between marginal effects for coefficients and for predicted probabilities?</p>

<pre><code>#####################################
# packages
library(effects)
library(mfx)
library(ggplot2)

# data
data(mtcars)
carsdat &lt;- mtcars
carsdat$carb &lt;- ifelse(carsdat$carb %in% 1:3, 0, 1)
facvars &lt;- c(""vs"", ""am"", ""carb"")
carsdat[, paste0(facvars, ""f"")] &lt;- lapply(carsdat[, facvars], factor)

# model
m1 &lt;- glm(vsf ~ amf + carbf, 
    family = binomial(link = ""logit""), 
    data = carsdat)
summary(m1)


#####################################
# effects package
eff &lt;- allEffects(m1)
plot(eff, rescale.axis = FALSE)
eff_df &lt;- data.frame(eff[[""carbf""]])
eff_df 

#   carbf   fit    se  lower upper
# 1     0 0.607 0.469 0.3808 0.795
# 2     1 0.156 0.797 0.0375 0.469


#####################################
# mfx package marginal effects (at mean)
mfx1 &lt;-logitmfx(vsf ~ amf + carbf, data = carsdat, atmean = TRUE, robust = FALSE)
mfx1 

#         dF/dx Std. Err.     z  P&gt;|z|
# amf1    0.217     0.197  1.10 0.2697
# carbf1 -0.450     0.155 -2.91 0.0037

# mfx package marginal effects (averaged)
mfx2 &lt;-logitmfx(vsf ~ amf + carbf, data = carsdat, atmean = FALSE, robust = FALSE)
mfx2

#         dF/dx Std. Err.     z  P&gt;|z|
# amf1    0.177     0.158  1.12 0.2623
# carbf1 -0.436     0.150 -2.90 0.0037


#####################################
# mfx source code
fit &lt;- m1
x1 = model.matrix(fit)  
be = as.matrix(na.omit(coef(fit)))
k1 = length(na.omit(coef(fit)))
fxb = mean(plogis(x1 %*% be)*(1-plogis(x1 %*% be))) 
vcv = vcov(fit)

# data frame for predictions
mfx_pred &lt;- data.frame(mfx = rep(NA, 4), se = rep(NA, 4), 
    row.names = c(""amf0"", ""amf1"", ""carbf0"", ""carbf1""))
disc &lt;- rownames(mfx_pred)

# hard coded prediction estimates and SE  
disx0c &lt;- disx1c &lt;- disx0a &lt;- disx1a &lt;- x1 
disx1a[, ""amf1""] &lt;- max(x1[, ""amf1""]) 
disx0a[, ""amf1""] &lt;- min(x1[, ""amf1""]) 
disx1c[, ""carbf1""] &lt;- max(x1[, ""carbf1""]) 
disx0c[, ""carbf1""] &lt;- min(x1[, ""carbf1""])
mfx_pred[""amf0"", 1] &lt;- mean(plogis(disx0a %*% be))
mfx_pred[""amf1"", 1] &lt;- mean(plogis(disx1a %*% be))
mfx_pred[""carbf0"", 1] &lt;- mean(plogis(disx0c %*% be))
mfx_pred[""carbf1"", 1] &lt;- mean(plogis(disx1c %*% be))
# standard errors
gr0a &lt;- as.numeric(dlogis(disx0a %*% be)) * disx0a
gr1a &lt;- as.numeric(dlogis(disx1a %*% be)) * disx1a
gr0c &lt;- as.numeric(dlogis(disx0c %*% be)) * disx0c
gr1c &lt;- as.numeric(dlogis(disx1c %*% be)) * disx1c
avegr0a &lt;- as.matrix(colMeans(gr0a))
avegr1a &lt;- as.matrix(colMeans(gr1a))
avegr0c &lt;- as.matrix(colMeans(gr0c))
avegr1c &lt;- as.matrix(colMeans(gr1c))
mfx_pred[""amf0"", 2] &lt;- sqrt(t(avegr0a) %*% vcv %*% avegr0a)
mfx_pred[""amf1"", 2] &lt;- sqrt(t(avegr1a) %*% vcv %*% avegr1a)
mfx_pred[""carbf0"", 2] &lt;- sqrt(t(avegr0c) %*% vcv %*% avegr0c)
mfx_pred[""carbf1"", 2] &lt;- sqrt(t(avegr1c) %*% vcv %*% avegr1c)  

mfx_pred$pred &lt;- rownames(mfx_pred)
    mfx_pred$lcl &lt;- mfx_pred$mfx - (mfx_pred$se * 1.96)
mfx_pred$ucl &lt;- mfx_pred$mfx + (mfx_pred$se * 1.96)

#          mfx    se   pred     lcl   ucl
# amf0   0.366 0.101   amf0  0.1682 0.563
# amf1   0.543 0.122   amf1  0.3041 0.782
# carbf0 0.601 0.107 carbf0  0.3916 0.811
# carbf1 0.165 0.105 carbf1 -0.0412 0.372

ggplot(mfx_pred, aes(x = pred, y = mfx)) +
    geom_point() +
    geom_errorbar(aes(ymin = lcl, ymax = ucl)) +
    theme_bw()
</code></pre>
"
"NaN","NaN","140966","<p>I'm writing a program for evaluating real estates and I don't really understand the differences between some robust regression models, that's why I don't know which one to choose.</p>

<p>I tried <code>lmrob</code>, <code>ltsReg</code> and <code>rlm</code>. for the same data set, all three methods gave me different values for the coefficients.</p>

<p>I thought that it is best to use <code>ltsReg</code> because, <code>summary(ltsReg())</code> provides information about <code>R-squared</code> and <code>p-values</code> and this will help me to decide if upon accepting or dismissing the model.</p>

<p>Do you think that <code>ltsReg</code> is a good choice?</p>

<p>EDIT:
I've just read on <a href=""http://web.maths.unsw.edu.au/~adelle/Garvan/Assays/GoodnessOfFit.html"">Goodness-of-Fit Statistics</a> that the adjusted R-squared is generally the best indicator of quality fit</p>
"
"0.0609427635336005","0.0774330538852055","141422","<p>I'm trying to estimate the value of a real estate upon its characteristics. To do so, I'm using the <code>Hedonic Model</code> and I'm doing the regression using <code>lmRob (R package: robust)</code>.</p>

<p>There is a problem with the model and I don't know how to solve it.</p>

<p>If the real estate for which I want to evaluate the price, has more characteristics than the real estates used for the model creation, the evaluation wouldn't be complete.</p>

<p><strong>Example:</strong></p>

<p>Let's say that I want to evaluate a house which has 4 rooms, 190 mÂ², an inside pool, 3 balconies and a sauna.</p>

<p>None of the other house from the neighborhood (houses which will be used for the regression) have an inside pool, the rest of the characteristics are similar.</p>

<p>This means that the house with the inside pool must have a bigger price than the other, but the regression will not take into account that the house has a inside pool.</p>

<p>My question is: how can I find the coefficient of the inside pool, in order to use it for the evaluation.</p>

<p>Thank you! </p>
"
"0.105869651339174","0.0931268436589426","141423","<p>I use the <code>decompose</code> function in <code>R</code> and come up with the 3 components of my monthly time series (trend, seasonal and random). If I plot the chart or look at the table, I can clearly see that the time series is affected by seasonality.</p>

<p>However, when I regress the time series onto the 11 seasonal dummy variables, all the coefficients are not statistically significant, suggesting there is no seasonality.</p>

<p>I don't understand why I come up with two very different results. Did this happen to anybody? Am I doing something wrong?</p>

<hr>

<p>I add here some useful details.</p>

<p>This is my time series and the corresponding monthly change. In both charts, you can see there is seasonality (or this is what I would like to assess). Especially, in the second chart (which is the monthly change of the series) I can see a recurrent pattern (high points and low points in the same months of the year).</p>

<p><img src=""http://i.stack.imgur.com/rILAU.jpg"" alt=""TimeSeries""></p>

<p><img src=""http://i.stack.imgur.com/LdVnv.jpg"" alt=""MonthlyChange""></p>

<p>Below is the output of the <code>decompose</code> function. I appreciate that, as @RichardHardy said, the function does not test whether there is actual seasonality. But the decomposition seems to confirm what I think.</p>

<p><img src=""http://i.stack.imgur.com/ZaVRV.jpg"" alt=""Decompose""></p>

<p>However, when I regress the time series on 11 seasonal dummy variables (January to November, excluding December) I find the following:</p>

<pre><code>    Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
    (Intercept) 5144454056  372840549  13.798   &lt;2e-16 ***
    Jan     -616669492  527276161  -1.170    0.248    
    Feb     -586884419  527276161  -1.113    0.271    
    Mar     -461990149  527276161  -0.876    0.385    
    Apr     -407860396  527276161  -0.774    0.443    
    May     -395942771  527276161  -0.751    0.456    
    Jun     -382312331  527276161  -0.725    0.472    
    Jul     -342137426  527276161  -0.649    0.520    
    Aug     -308931830  527276161  -0.586    0.561    
    Sep     -275129629  527276161  -0.522    0.604    
    Oct     -218035419  527276161  -0.414    0.681    
    Nov     -159814080  527276161  -0.303    0.763
</code></pre>

<p>Basically, all the seasonality coefficients are not statistically significant.</p>

<p>To run linear regression I use the following function:</p>

<p><code>lm.r = lm(Yvar~Var$Jan+Var$Feb+Var$Mar+Var$Apr+Var$May+Var$Jun+Var$Jul+Var$Aug+Var$Sep+Var$Oct+Var$Nov)</code></p>

<p>where I set up Yvar as a time series variable with monthly frequency (frequency = 12).</p>

<p>I also try to take into account the trending component of the time series including a trend variable to the regression. However, the result does not change.</p>

<pre><code>                  Estimate Std. Error t value Pr(&gt;|t|)    
    (Intercept) 3600646404   96286811  37.395   &lt;2e-16 ***
    Jan     -144950487  117138294  -1.237    0.222    
    Feb     -158048960  116963281  -1.351    0.183    
    Mar      -76038236  116804709  -0.651    0.518    
    Apr      -64792029  116662646  -0.555    0.581    
    May      -95757949  116537153  -0.822    0.415    
    Jun     -125011055  116428283  -1.074    0.288    
    Jul     -127719697  116336082  -1.098    0.278    
    Aug     -137397646  116260591  -1.182    0.243    
    Sep     -146478991  116201842  -1.261    0.214    
    Oct     -132268327  116159860  -1.139    0.261    
    Nov     -116930534  116134664  -1.007    0.319    
    trend     42883546    1396782  30.702   &lt;2e-16 ***
</code></pre>

<p>Hence my question is: am I doing something wrong in the regression analysis?</p>
"
"0.0609427635336005","0.0435560928104281","141583","<p>I am working on a few (both simple and multivariable) regression analyses, and I have cases where the residuals are non-normal, to varying degrees. As I've understood, the Gauss-Markov theorem states that normality of residuals is not necessary for the coefficient point estimates to be correct, i.e., I can trust that the regression summary tells me the BLUE coefficient estimates. However, the standard errors may be biased, and thus, the corresponding t- and p-values may not be correct.</p>

<p>A previous question (<a href=""http://stats.stackexchange.com/questions/83012/how-to-obtain-p-values-of-coefficients-from-bootstrap-regression"">How to obtain p-values of coefficients from bootstrap regression?</a>) asked if it was possible to calculate p-values from bootstrapped coefficients and their CIs. However, if I bootstrap the coefficients and use the bootstrapped mean and the bootstrapped SE to re-calculate t- and p-values, would that be a sound approach?</p>

<p>Here is the code I use, in R:</p>

<pre><code># create linear model
mod &lt;- lm(Y~X,data=dataset); summary(mod)

# create function to return coefficient
mod.bootstrap &lt;- function(data, indices) {    
d &lt;- data[indices, ]
mod &lt;- lm(Y~X, data=d)  
return(coef(mod))
}

# set seed
set.seed(1234)

# begin bootstrap using the boot package
mod.boot &lt;- boot(data=dataset, statistic=mod.bootstrap, R=2000)

# now here is how I re-calculate t- and p-values
bootmean &lt;- mean(mod.boot$t[,2])
booter &lt;- sd(boot.t[,2])
tval &lt;- bootmean/booter
p &lt;- 2*pt(-abs(tval),df=mod$df.residual)
</code></pre>

<p>The new t- and p-values come out fairly close to the LM summary, albeit a bit more conservative, as expected. Is this something I could report? I am very new at this, so I can't really tell if my logic is valid or not.</p>

<p>Edit: Clarified a bit.</p>
"
"0.075412822378","0.0821301562353182","141603","<p>So I'm playing around with logistic regression in R, using the mtcars dataset, and I decide to create a logistic regression model on the 'am' parameter (that is manual or automatic transmission for those of you familiar with the mtcars-dataset).</p>

<pre><code>Call:
glm(formula = am ~ mpg + qsec + wt, family = binomial, data = mtcars)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-4.484e-05  -2.100e-08  -2.100e-08   2.100e-08   5.163e-05  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)    924.89  883764.07   0.001    0.999
mpg             20.65   18004.32   0.001    0.999
qsec           -55.75   32172.52  -0.002    0.999
wt            -111.33  103183.48  -0.001    0.999

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 4.3230e+01  on 31  degrees of freedom
Residual deviance: 6.2903e-09  on 28  degrees of freedom
AIC: 8

Number of Fisher Scoring iterations: 25
</code></pre>

<p>Now, at first sight this looks like a terrible regression, right? The standard errors are HUGE, the z-values are all close to zero and the corresponding probabilities are all close to one. HOWEVER, the residual deviance is extremely small! </p>

<p>I decide to check how well the model does as a classification model by running:</p>

<pre><code>pred &lt;- predict(logit_fit, data.frame(qsec = mtcars$qsec, wt = mtcars$wt, mpg = mtcars$mpg), type = ""response"") # Make a prediction of the probabilities on our data
mtcars$pred_r &lt;- round(pred, 0) # Round probabilities to closest 0 or 1
table(mtcars$am, mtcars$pred_r) # Check if results of classification is any good.
</code></pre>

<p>Indeed, the model perfectly predicts the data:</p>

<pre><code>     0  1
  0 19  0
  1  0 13
</code></pre>

<p>Have I completely misunderstood how to interpret model data? Am I overfitting massively or what's going on here? What's going on?</p>
"
"0.0861860827177143","0.0821301562353182","141684","<p>I'm fitting a fixed effect model with <code>plm</code> and know that I'm dealing with multi-collinearity between two of the independent variables.  I working on identifying multicolliearity in models as a practice and have identified the variable with <code>alias()</code>, then verified with <code>vif()</code>.  I was also able to use <code>kappa()</code> to show a very large conditional number verifying the multicollinearity.</p>

<p>My question is why does <code>plm()</code> ommit this multicolliearity variable from the coefficients?  There is no output clarifying why and I couldn't find anything in the documentation.  Stata automatically omits this variable and I'm curious if <code>plm()</code> does a check and then omits.</p>

<p>Multicollinearity variable <code>dfmfd98</code></p>

<p>Reproducible example : </p>

<p>dput : </p>

<pre><code>data &lt;- 
structure(list(lexptot = c(8.28377505197124, 9.1595012302023, 
8.14707583238833, 9.86330744180814, 8.21391453619232, 8.92372556833205, 
7.77219149815994, 8.58202430280175, 8.34096828565733, 10.1133857229336, 
8.56482997492403, 8.09468633074053, 8.27040804817704, 8.69834992618814, 
8.03086333985764, 8.89644392254136, 8.20990433577082, 8.82621293136669, 
7.79379981225575, 8.16139809188569, 8.25549748271241, 8.57464947213076, 
8.2714431846277, 8.72374048671495, 7.98522888221012, 8.56460042433047, 
8.22778847721461, 9.15431416391622, 8.25261818916933, 8.88033778695326
), year = c(0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 
1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 
1L), dfmfdyr = c(0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 
0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0), dfmfd98 = c(1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 0, 0, 0, 0), nh = c(11054L, 11054L, 11061L, 11061L, 
11081L, 11081L, 11101L, 11101L, 12021L, 12021L, 12035L, 12035L, 
12051L, 12051L, 12054L, 12054L, 12081L, 12081L, 12121L, 12121L, 
13014L, 13014L, 13015L, 13015L, 13021L, 13021L, 13025L, 13025L, 
13035L, 13035L)), .Names = c(""lexptot"", ""year"", ""dfmfdyr"", ""dfmfd98"", 
""nh""), class = c(""tbl_df"", ""data.frame""), row.names = c(NA, -30L
))
</code></pre>

<p>Regression code : </p>

<pre><code>library(plm)
lm &lt;- plm(lexptot ~ year + dfmfdyr + dfmfd98 + nh, data = data, model = ""within"", index = ""nh"")
summary(lm)
</code></pre>

<p>Output : </p>

<pre><code>Oneway (individual) effect Within Model

Call:
plm(formula = lexptot ~ year + dfmfdyr + dfmfd98 + nh, data = data, 
    model = ""within"", index = ""nh"")

Balanced Panel: n=15, T=2, N=30

Residuals :
     Min.   1st Qu.    Median   3rd Qu.      Max. 
-4.75e-01 -1.69e-01  4.44e-16  1.69e-01  4.75e-01 

Coefficients :
        Estimate Std. Error t-value Pr(&gt;|t|)  
year     0.47552    0.23830  1.9955  0.06738 .
dfmfdyr  0.34635    0.29185  1.1867  0.25657  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Total Sum of Squares:    5.7882
Residual Sum of Squares: 1.8455
R-Squared      :  0.68116 
      Adj. R-Squared :  0.29517 
F-statistic: 13.8864 on 2 and 13 DF, p-value: 0.00059322
</code></pre>
"
"0.114914776956952","0.123195234352977","141820","<p>I want to find which soil variables better explain plant productivity, using a database that contains information for about 100 forests plots across Europe.
These plots have only one species per plot, but overall there are 4 different species in the dataset. These plots also have different climate conditions (temperature, precipitation,...). My final goal is finding out which combination of the more than 20 different soil variables better explain plant productivity. However, both climate and species may confound the analysis because both affect plant growth (some species grow more than others, and plants grown in warmer climates may grow more). I am only interested in plant growth due to soil characteristics, so I need to get rid of the species and climate effects on plant productivity that may confound the analysis. According to what I have read I could just include all variables in the model: soil, climate and species (factor of 4 levels), like this:</p>

<pre><code>fit &lt;- lm(scale(IVMean)~scale(SILT)+scale(SAND)+scale(PHCACL2)+scale(OC)+
                        scale(EXCHCA)+scale(EXCHK)+scale(EXCHMG)+scale(EXCHMN)+
                        scale(EXCHNA)+scale(EXCHAL)+scale(EXCHFE)+scale(N_NO3)+
                        scale(S_SO4)+scale(N_NH4)+scale(BS)+scale(CN)+scale(Temp)+
                        scale(Precip)+scale(Rad)+scale(PET)+species)
</code></pre>

<p>IVMean = mean stem volume increment (productivity). Note climate variables (temperature, precipitation, radiation and potential evapotranspiration -PET-) and species at the end, and the standardisation of all variables with <code>scale()</code>.</p>

<p>After this, I could run a stepwise regression analysis to preliminarily find which variables are the most important explaining plant productivity.</p>

<pre><code>library(MASS)
step &lt;- stepAIC(fit, direction=""backward"")
step$anova # display results
</code></pre>

<p>Which renders the following best minimal model:</p>

<pre><code>Final Model:
scale(IVMean) ~ scale(PHCACL2) + scale(EXCHCA) + scale(EXCHMG) + 
    scale(EXCHMN) + scale(BS) + scale(Temp) + scale(PET) + species

&gt; model &lt;- lm(scale(IVMean) ~ scale(PHCACL2) + scale(EXCHCA) + scale(EXCHMG) + 
+               scale(EXCHMN) + scale(BS) + scale(Temp) + scale(PET) + species, 
+             data = icp)
&gt; summary(model)

Call:
lm(formula = scale(IVMean) ~ scale(PHCACL2) + scale(EXCHCA) + 
    scale(EXCHMG) + scale(EXCHMN) + scale(BS) + scale(Temp) + 
    scale(PET) + species, data = icp)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.13836 -0.41522 -0.02816  0.35094  1.65587 

Coefficients:
                   Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)        -0.37587    0.16967  -2.215 0.030897 *  
scale(PHCACL2)      0.58776    0.20617   2.851 0.006128 ** 
scale(EXCHCA)      -0.38061    0.19025  -2.001 0.050381 .  
scale(EXCHMG)      -0.37374    0.14686  -2.545 0.013769 *  
scale(EXCHMN)       0.13102    0.09970   1.314 0.194241    
scale(BS)           0.39502    0.19428   2.033 0.046871 *  
scale(Temp)         1.34654    0.32033   4.204 9.74e-05 ***
scale(PET)         -0.62177    0.29749  -2.090 0.041250 *  
speciesoak         -1.24553    0.34788  -3.580 0.000726 ***
speciespicea_abies  1.38679    0.25031   5.540 8.79e-07 ***
speciesscots_pine   0.02627    0.25960   0.101 0.919769    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.6411 on 55 degrees of freedom
Multiple R-squared:  0.6522,    Adjusted R-squared:  0.5889 
F-statistic: 10.31 on 10 and 55 DF,  p-value: 1.602e-09
</code></pre>

<p>The final model includes 5 soil variables, 2 out of 4 climate variables, and species. So far so good?</p>

<p>However, this seems to be not good enough for my supervisor. Rather, he asked me to do an analysis of the residuals to â€œget rid of climate and species effectsâ€! To be honest, I have no idea what he is talking about, and I was afraid to ask because he sounded like something I should know since my childhood. Perhaps he meant I should study which SOIL variables can explain the residuals of productivity ~ climate * species? Please, help me find out which type of analysis of the residuals would make sense to focus on soil effects eliminating climate and species effects.</p>

<p>This is the only thing I can think of:  </p>

<pre><code># Study the importance of confounding effects:
confounding     &lt;- IVMean ~ (Temp + Precip + PET + Rad) * species 
confounding.res &lt;- residuals(confounding)
lm(confounding.res ~scale(SILT)+scale(SAND)+scale(PHCACL2)+scale(OC)+scale(EXCHCA)+
                    scale(EXCHK)+scale(EXCHMG)+scale(EXCHMN)+scale(EXCHNA)+
                    scale(EXCHAL)+scale(EXCHFE)+scale(N_NO3)+scale(S_SO4)+
                    scale(N_NH4)+scale(BS)+scale(CN))
</code></pre>

<p>This way maybe I could study which soil variables explain what climate and species effects could not explain? I donâ€™t know if it makes any sense. I am open to suggestions and alternatives. </p>
"
"0.118246329960506","0.112681644735122","141844","<p>I tried to plot the results of an ordered logistic regression analysis by calculating the probabilities of endorsing every answer category of the dependent variable (6-point Likert scale, ranging from ""1"" to ""6""). However, I've received strange probabilities when I calculated the probabilities based on this formula: $\rm{Pr}(y_i \le k|X_i) = \rm{logit}^{-1}(X_i\beta)$.</p>

<p>Below you see how exactly I tried to calculate the probabilities and plot the results of the ordered logistic regression model (<code>m2</code>) that I fitted using the <code>polr</code> function (<code>MASS</code> package). The probabilities (<code>probLALR</code>) that I calculated and used to plot an ""expected mean score"" are puzzling as the expcected mean score in the plot increases along the RIV.st continuum while the coefficient for <code>RIV.st</code> is negative (-0.1636). I would have expected that the expected mean score decreases due to the negative main effect of <code>RIV.st</code> and the irrelevance of the interaction terms for the low admiration and low rivalry condition (LALR) of the current 2 by 2 design (first factor = <code>f.adm</code>; second factor = <code>f.riv</code>; dummy coding 0 and 1).</p>

<p>Any idea of how to make sense of the found pattern? Is this the right way to calculate the probabilities? The way I used the intercepts in the formula to calculate the probabilities might be problematic (cf., <a href=""https://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression"">Negative coefficient in ordered logistic regression</a>).</p>

<pre><code>m2 &lt;- polr(short.f ~ 1 + f.adm*f.riv + f.adm*RIV.st + f.riv*RIV.st, data=sampleNS)

# f.adm  = dummy (first factor of 2 by 2 design);
# f.riv  = dummy (second factor of 2 by 2 design);
# RIV.st = continuous predictor (standardized)
summary(m2)
Coefficients:
                Value Std. Error t value
f.adm1         1.0203    0.14959  6.8203
f.riv1        -0.8611    0.14535 -5.9240
RIV.st        -0.1636    0.09398 -1.7403
f.adm1:f.riv1 -1.2793    0.20759 -6.1625
f.adm1:RIV.st  0.0390    0.10584  0.3685
f.riv1:RIV.st  0.6989    0.10759  6.4953

Intercepts:
    Value    Std. Error t value 
1|2  -2.6563   0.1389   -19.1278
2|3  -1.2139   0.1136   -10.6898
3|4  -0.3598   0.1069    -3.3660
4|5   0.9861   0.1121     8.7967
5|6   3.1997   0.1720    18.6008
</code></pre>

<p>Here you see how I tried to calculate the probabilities (<code>probLALR</code>) for 1 of the 4 conditions of the 2 by 2 design:</p>

<pre><code>inv.logit  &lt;- function(x){ return(exp(x)/(1+exp(x))) }
Pred       &lt;- seq(-3, 3, by=0.01)
b = c(-2.6563,-1.2139,-0.3598,0.9861,3.1997) # intercepts of model m2
a = c(1.0203,-0.8611,-0.1636,-1.2793,0.0390,0.6989) # coefficients of m2
probLALR   &lt;- data.frame(matrix(NA,601,5))
for (k in 1:5){ 
    probLALR[,k] &lt;- inv.logit(b[k] + a[1]*0 + a[2]*0 + 
                               a[3]*Pred  + a[4]*0*0 + 
                               a[5]*Pred*0 + a[6]*Pred*0)
}

plot(Pred,probLALR[,1],type=""l"",ylim=c(0,1))   # p(1)
lines(Pred,probLALR[,2],col=""red"")             # p(1 or 2)
lines(Pred,probLALR[,3],col=""green"")           # P(1 or 2 or 3)
lines(Pred,probLALR[,4],col=""orange"")          # P(1 or 2 or 3 or 4)
lines(Pred,probLALR[,5],col=""orange"")          # P(1 or 2 or 3 or 4 or 5)

# option response functions:

orc = matrix(NA,601,6)
orc[,6] = 1-probLALR[,5]        # prob of 6
orc[,5]= probLALR[,5]-probLALR[,4]  # prob of 5
orc[,4]= probLALR[,4]-probLALR[,3]  # prob of 4
orc[,3]= probLALR[,3]-probLALR[,2]  # prob of 3
orc[,2]= probLALR[,2]-probLALR[,1]  # prob of 2
orc[,1]= probLALR[,1]           # prob of 1


plot(Pred,orc[,1],type=""l"",ylim=c(0,1))   # p(1)
lines(Pred,orc[,2],col=""red"")             # p(2)
lines(Pred,orc[,3],col=""green"")           # P(3)
lines(Pred,orc[,4],col=""orange"")          # P(4)
lines(Pred,orc[,5],col=""purple"")          # P(5)
lines(Pred,orc[,6],col=""purple"")          # P(6)

# mean score

mean = orc[,1]*1+orc[,2]*2+orc[,3]*3+orc[,4]*4+orc[,5]*5+orc[,6]*6
plot(Pred,mean,type=""l"",xlab=""RIV.st"",ylab=""expected mean score"",ylim=c(1,6))  
</code></pre>
"
"0.0304713817668003","0.029037395206952","141884","<p>I want to know about the relationship between a multiple linear regression model and the Kalman Filter. Mainly, I want to know how I can write a multiple regression model using Kalman Filter. For that reason, I simulate some data and I run the code $lm(Y \sim X_1 + X_2)$ in R project. From that I take the estimated coefficients. I want now a code in R, in order to estimate the coefficients using Kalman Filter so as to compare my results. </p>

<p>Lets suppose that my dataset is produced by the code:</p>

<pre><code>a=c(10,3,-5)
n=100
x=matrix(rnorm(n*2), byrow=F, ncol=2)
y=cbind(rep(1,n),x)%*%a + 2*rnorm(n)
lm(y~x[,1]+x[,2])
</code></pre>

<p>Thank you...</p>
"
"0.0746393370862076","0.0711268017165705","142594","<p>I'm executing a few test runs of a lasso regression with the glmnet package in R using the diabetes dataset (<a href=""http://www4.stat.ncsu.edu/~boos/var.select/diabetes.tab.txt"" rel=""nofollow"">http://www4.stat.ncsu.edu/~boos/var.select/diabetes.tab.txt</a>). Iâ€™m choosing a single lambda value = 0.1 (not a sequence of lambda values) and alpha=1:</p>

<pre><code>&gt; set.seed(1);
  elasticnet_fit &lt;- glmnet(data_matrix, y, family=""gaussian"", lambda=.1, alpha=1); 
  coef(elasticnet_fit);
11 x 1 sparse Matrix of class ""dgCMatrix""
                       s0
(Intercept) -299.82915923
x1            -0.02102016
x2             5.63508548
x3             1.10288671
x4            -0.73514322
x5             0.42465094
x6            -0.03356997
x7             5.39641052
x8            59.76829212
x9             0.27508521
x10          -22.34877815
</code></pre>

<p>I noticed the glmnet documentation reads as follows when selecting lambda values:</p>

<blockquote>
  <p>WARNING: use with care. Do not supply a single value for lambda (for
  predictions after CV use predict() instead). Supply instead a
  decreasing sequence of lambda values. glmnet relies on its warms
  starts for speed, and its often faster to fit a whole path than
  compute a single fit.</p>
</blockquote>

<p>Are the coefficient estimates invalid when supplying only a single lambda value rather than a sequence? (Cross-posting from Stack Overflow.)</p>
"
"0.0710998907892006","0.0677539221495548","142693","<p><strong><em>Is the following a reasonable illustration of the OVB problem?</em></strong></p>

<p>We build up fictional data around the regression line:</p>

<p>$$y = 7.2 + 2.3 \, x_1 + 0.1 \, x_2 + 1.5 \, x_3 + 0.013 \, x_4 + eps$$</p>

<p>by using this function:</p>

<pre><code>correlatedValue = function(x, r){
  r2 = r**2
  ve = 1 - r2
  SD = sqrt(ve)
  e  = rnorm(length(x), mean = 0, sd = SD)
  y  = r * x + e
}
</code></pre>

<p>-thank you, @gung for this post:
<a href=""http://stats.stackexchange.com/questions/38856/how-to-generate-correlated-random-numbers-given-means-variances-and-degree-of"">How to generate correlated random numbers (given means, variances and degree of correlation)?</a></p>

<p>And the following function, which generates four variables (<strong><em>x1</em></strong>, <strong><em>x2</em></strong>, <strong><em>x3</em></strong> and <strong><em>x4</em></strong>) as well as noise (<strong><em>eps</em></strong>). <strong><em>x1</em></strong> and <strong><em>x3</em></strong> are sample from normal distributions; <strong><em>x2</em></strong> is extracted from a uniform; and <strong><em>x4</em></strong> from a Poisson.</p>

<pre><code>variables &lt;- function(){
x &lt;- rnorm(1000)
x1 &lt;- 50 + 15 * x
x3 &lt;- 28 + 11 * correlatedValue(x = x, r = 0.6)
x2 &lt;- runif(1000, 0, 100)
x4 &lt;- rpois(1000,50)
eps &lt;- rnorm(1000,5, 7)
y = 7.2 + 2.3 * x1 + 0.001 * x2 + 1.5 * x3 + 0.013 * x4 + eps
dat &lt;- as.data.frame(cbind(y, x1, x2, x3, x4))
c &lt;- as.numeric(coef(lm(y ~ x2 + x3 + x4, dat))[3])
d &lt;- as.numeric(coef(lm(y ~ x1 + x2 + x3 + x4, dat))[4])
c(c,d)
}
</code></pre>

<p><strong><em>x1</em></strong> and <strong><em>x3</em></strong> are highly influential on <strong><em>y</em></strong> and are correlated with each other, setting the values up to observe <strong><em>OVB</em></strong>. <strong><em>x2</em></strong> and <strong><em>x4</em></strong> are less influential.</p>

<p>Here is the plotting of <strong><em>y</em></strong> against <strong><em>x1</em></strong>, <strong><em>x2</em></strong>, <strong><em>x3</em></strong>  and <strong><em>x4</em></strong>, and <strong><em>x1</em></strong> over <strong><em>x3</em></strong> with added regression lines:</p>

<p><img src=""http://i.stack.imgur.com/I4u0S.png"" alt=""enter image description here""></p>

<p>And following is the variance-covariance matrix:</p>

<pre><code>             y           x1           x2         x3          x4
y   1.00000000  0.944410945  0.014421682 0.77571067 -0.01463981
x1  0.94441094  1.000000000 -0.001726526 0.56504020 -0.03562991
x2  0.01442168 -0.001726526  1.000000000 0.03537959  0.02253922
x3  0.77571067  0.565040198  0.035379590 1.00000000  0.02573827
x4 -0.01463981 -0.035629906  0.022539218 0.02573827  1.00000000
</code></pre>

<p>Predictably, the regression including all variables shows similar coefficients to the initial equation:</p>

<pre><code>coef(lm(y~.,dat))[2:5]
         x1          x2          x3          x4 
2.253353226 0.004899445 1.547915198 0.017710038 
</code></pre>

<p>Wrapping up, a quick simulation is carried out to obtain the mean of the <strong><em>x3</em></strong> coefficient in 1,000 simulations <em>WITHOUT</em> including <strong><em>x1</em></strong> (""coef_x3"") and then <em>WITH</em> <strong><em>x1</em></strong> (""coef_x3_full""):</p>

<pre><code>coef_x3 &lt;- NULL
coef_x3_full &lt;- NULL
for (i in 1:1000){
  coef_x3[i] = variables()[1]
  coef_x3_full[i] = variables()[2]
}
mean(coef_x3)
mean(coef_x3_full)
</code></pre>

<p>obtaining a coefficient for <strong><em>x3</em></strong> of <strong>3.383</strong> when <strong><em>x1</em></strong> is excluded versus a coefficient for <strong><em>x3</em></strong> of <strong>1.502</strong> when included. So when <strong><em>x1</em></strong> is included we have an unbiased estimation of the true <strong><em>x3</em></strong> coefficient (<strong><em>1.5</em></strong>), whereas the estimation is biased when we exclude <strong><em>x1</em></strong>.</p>
"
"0.068136080998913","0.0519436716578171","143155","<p>I am trying to calculate multiple regression in R without intercept.</p>

<p>My data is as follow:</p>

<pre><code> y=c(60.323,61.122,60.171,61.187,63.221,63.639,64.989,63.761,66.019,67.857,68.169,66.513,68.655,69.564,69.331,70.551)
    x1=c(83,88.5,88.2,89.5,96.2,98.1,99,100,101.2,104.6,108.4,110.8,112.6,114.2,115.7,116.9)
    x2=c(107.608,108.632,109.773,110.929,112.075,113.27,115.094,116.219,117.388,118.734,120.445,121.95,123.366,125.368,127.852,130.081)
</code></pre>

<p>In this case, (I believe?) I am getting the coefficients WITH intercept:</p>

<pre><code>lm(formula = y ~ x1 + x2)
</code></pre>

<p>I would like to get the coefficients WITHOUT intercept. I tried this:</p>

<pre><code>lm(formula = y ~ x1 + x2 -1)
</code></pre>

<p>Is this correct? If so, my question would be: How can I calculate WITHOUT intercept without changing the x values (on the right side of the tilda), but by changing something on the y value (on the left side of the tilda). For instance:</p>

<pre><code>lm(formula = y -1 ~ x1 + x2)
</code></pre>

<p>Gets a different (and presumably incorrect coefficient estimation).</p>

<p>I know your question is... why do you have to only change the y values? The reason is because I am writing code in C to do this, and I do not want to change the dimensions of X by adding a -1 at the end because that would require dynamic array allocation, which is very meticulous for me.</p>
"
"0.0430930413588572","0.0410650781176591","143335","<p>We are modeling a discrete choice scenario, with alternative-specific coefficients. We also break the assumption of independence of irrelevant alternatives. To model this, we are using an alternative-specific multinomial probit regression. This is implemented in Stata as <code>asmprobit</code>. However, as described in <a href=""http://www.stata.com/manuals13/rasmprobit.pdf"" rel=""nofollow"">the documentation</a>, there is a limit on the number of alternatives: 20! We have up to 120 alternatives per case! Uh oh.</p>

<p>Why is there this limit? Is there anything that can be done to increase it? Is this model (or a good alternative) implemented in R or elsewhere, presumably without this limit?</p>
"
"0.075412822378","0.0718638867059034","143341","<p>I am working with a regression model from which I would like to compute standardized regression coefficients.  I am writing primarily regarding an observed discrepancy between coefficients obtained by standardizing the variables before regressing and those computed using lm.beta().  In the former case, I regress scaled variables (using scale()) and obtain the following output:</p>

<pre><code>Call:
lm(formula = correct.sc ~ P1_PO8_PatD.sc + P1_PO8_Third.sc + 
    P3a_FC1_D.sc + P3a_FC1_PatT.sc + P3a_FC1_Third.sc + P3b_CP1_T.sc + 
    P3b_CP1_PatT.sc + dx:P1_PO8_PatD.sc)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.77325 -0.45863 -0.05418  0.50733  2.10668 

Coefficients:
                          Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)                0.06382    0.10936   0.584  0.56161   
P1_PO8_PatD.sc            -0.25313    0.17836  -1.419  0.16084   
P1_PO8_Third.sc            0.32583    0.12458   2.615  0.01118 * 
P3a_FC1_D.sc              -0.63229    0.31296  -2.020  0.04768 * 
P3a_FC1_PatT.sc           -0.73578    0.25622  -2.872  0.00558 **
P3a_FC1_Third.sc           1.15503    0.43505   2.655  0.01007 * 
P3b_CP1_T.sc               0.38482    0.18897   2.036  0.04598 * 
P3b_CP1_PatT.sc           -0.24783    0.17129  -1.447  0.15299   
P1_PO8_PatD.sc:dxPROBAND   0.60647    0.33286   1.822  0.07328 . 
P1_PO8_PatD.sc:dxRELATIVE  0.03085    0.25768   0.120  0.90509   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.8746 on 62 degrees of freedom
Multiple R-squared:  0.332, Adjusted R-squared:  0.2351 
F-statistic: 3.424 on 9 and 62 DF,  p-value: 0.001782
</code></pre>

<p>Using lm.beta(), I obtain the following:</p>

<pre><code>&gt; lm.beta(mod.complete)
           P1_PO8_PatD           P1_PO8_Third              P3a_FC1_D 
           -0.25313301             0.32583258            -0.63228654 
          P3a_FC1_PatT          P3a_FC1_Third              P3b_CP1_T 
           -0.73578305             1.15502579             0.38482487 
          P3b_CP1_PatT  P1_PO8_PatD:dxPROBAND P1_PO8_PatD:dxRELATIVE 
           -0.24782599             0.33812822             0.03085196 
Warning message:
In b * sx : longer object length is not a multiple of shorter object length
</code></pre>

<p>The discrepancy, then, comes for the coefficients associated with the interaction terms (where P1_PO8_PatD is a continuous variable and Dx is a categorical variable).  I believe I understand why this discrepancy arises--namely, lm.beta() likely uses the SD for the Dx-subsetted P1_PO8_PatD (and hence the warning)--but I am primarily wondering which is more ""correct.""  My inkling is to lean towards those obtained from the regression model using the scaled variables, but I can convince myself to go either way; any input would be much appreciated.</p>

<p>I didn't feel this example needed accompanying data, but I can provide it if desired.</p>
"
"0.114013470672957","0.108647984268766","143399","<p>I am trying to do some survival analysis in R and as a starting point, I want to make sure I can replicate a previous analysis. I notice differences and I will demonstrate them here. I feel like there is a daft explanation the user community can provide.</p>

<p>Let's start by using the ovarian dataset in R. We will fit a weibull distribution with residual disease and ECOG performance status as covariates. Then we will print the output using proportional hazards specification to match Stata's HR output.</p>

<pre><code>require(survival)
require(flexsurvreg)
require(dplyr)
attach(ovarian)
ovarian &lt;- ovarian %&gt;% mutate(resid.ds=resid.ds-1, ecog.ps=ecog.ps-1, futime=futime/365.25) # Make it 0,1
write.dta(ovarian %&gt;% mutate(resid.ds=resid.ds+1, ecog.ps=ecog.ps+1), ""data/ovarian.dta"") # Write dta
s.weib &lt;- flexsurvreg(Surv(futime, fustat) ~ age + resid.ds + ecog.ps, data=ovarian, dist=""weibull"") # Fit weibull

# Function to convert AFT to PH
flexsurvPHcoef &lt;- function(x) return(c(exp(x$coef[-(1:2)]*(-1)*exp(x$coef[""shape""])), exp(x$coef[""shape""]), exp(-x$coef[""scale""])))
flexsurvPHcoef(s.weib)
     age     resid.ds      ecog.ps        shape        scale 
1.150309872 2.702038142 1.060599568 1.752446996 0.002799146 
</code></pre>

<hr>

<p>Now let's compare to Stata.</p>

<pre><code>quietly stset futime, f(fustat)
streg age i.resid_ds i.ecog_ps, d(weib)
Weibull regression -- log relative-hazard form 

No. of subjects =           26                     Number of obs   =        26
No. of failures =           12
Time at risk    =  42.67761807
                                                   LR chi2(3)      =     17.88
Log likelihood  =   -20.828884                     Prob &gt; chi2     =    0.0005

------------------------------------------------------------------------------
          _t | Haz. Ratio   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
         age |    1.15031   .0515502     3.12   0.002     1.053583    1.255916
  2.resid_ds |   2.702038    2.00098     1.34   0.180     .6329054    11.53571
   2.ecog_ps |     1.0606   .6620784     0.09   0.925     .3120252    3.605066
       _cons |   .0000336   .0000906    -3.82   0.000     1.69e-07    .0066602
-------------+----------------------------------------------------------------
       /ln_p |   .5610131    .238929     2.35   0.019     .0927209    1.029305
-------------+----------------------------------------------------------------
           p |   1.752447   .4187103                      1.097156     2.79912
         1/p |   .5706307   .1363402                      .3572551    .9114478
------------------------------------------------------------------------------
</code></pre>

<p>We can see the resid.ds and ecog.ps are the same. As well, the shape. But the scale is off. </p>

<p><strong>So my question is, any thoughts on why only the scale parameter is different?</strong></p>

<hr>

<p>Let's move on to estimation. flexsurvreg has an interesting ability to predict at multiple time points. Let's assume a woman is 45, has residual disease and ECOG is 0.</p>

<pre><code>summary(s.weib, newdata=data.frame(age=45, resid.ds=1, ecog.ps=0), t=c(1:5))
age=45, resid.ds=1, ecog.ps=0 
  time       est         lcl       ucl
1    1 0.9517266 0.807744496 0.9930684
2    2 0.8464501 0.488174229 0.9681961
3    3 0.7122949 0.193976895 0.9285058
4    4 0.5702529 0.038982054 0.8762493
5    5 0.4358519 0.002925942 0.8097636
</code></pre>

<p>Not the most precise. Anyways, how does this compare to how weibull is parameterized (PH). From Stata's manual:
$$
S = exp(-exp(x{B}){t}^p)
$$</p>

<p>No we have to use the log scale coefficients. But let's go ahead and try and predict this manually.</p>

<pre><code>p.weib &lt;- function(cons, age, resid, t, p) return(exp(-exp(cons+age*45+resid)*t^p))
coef &lt;- flexsurvPHcoef(s.weib)
data.frame(time=1:5) %&gt;% mutate(S=p.weib(log(coef[""scale""]), log(coef[""age""]), log(coef[""resid.ds""]), time, coef[""shape""]), S=round(S, 5))
  time       S
1    1 0.01617
2    2 0.00000
3    3 0.00000
4    4 0.00000
5    5 0.00000
</code></pre>

<p>That obviously didn't really work out. What happens if we substitute the scale from Stata (-10.30166)?</p>

<pre><code>data.frame(time=1:5) %&gt;% mutate(S=p.weib(-10.30166, log(coef[""age""]), log(coef[""resid.ds""]), time, coef[""shape""]), S=round(S, 5))
  time       S
1    1 0.95173
2    2 0.84645
3    3 0.71230
4    4 0.57025
5    5 0.43585
</code></pre>

<p>It's just as flexsurvreg predicted. So now that I write this, maybe I've transformed the AFT scale incorrectly. Back to my original question, why are the scales different?</p>

<p>Finally, some aside questions. I couldn't reproduce my actual data problem. I can't really put up that much data here. Again, <code>summary.flexsurvreg</code> gives me predicted estimates that are not the same as the <code>p.weib</code>. But, when I substitute the scale from Stata for p.weib, I get estimates different to the original summary.flexsurvreg, albeit much closer than with the scale from my log(coef) of PH. Any thoughts?</p>
"
"0.0691025985081098","0.0658506226617377","143570","<p>I have a dataset of retail products which contains weekly sales for 12 different items in a single category. For each item, I have three dummy variables representing different types of advertising (FrontCover,BackCover,Inside) that could be run for that item that week. The data is weekly, and seasonal for a year so I have the frequency set to 52.</p>

<p>I have a two part question:</p>

<p><strong>1. How can I convert the advertising coefficients to % lifts when the data is seasonal?</strong></p>

<p>What I can do now is subset the data for a single item, and run multiple regression using tslm() from the R forecast package and read the coefficients to determine the lift for that item. However with tslm() I also have 51 other seasonal coefficients. How can I state this as a %?</p>

<pre><code>    x
(Intercept) 601.7857143
data.subset$Ad.Front	249.4285714
    data.subset$Ad.Inside    243.4285714
data.subset$Ad.Back 78
season2 92.5
season3 113.2142857
season4 -31.71428571
season5 189.7142857
season6 -25.21428571
season7 124.5
season8 77.21428571
season9 71.71428571
season10    -25.5
season11    -161.2142857
season12    47.21428571
season13    -13
season14    -47.5
season15    9.214285714
season16    -33.5
season17    -76.5
season18    54.71428571
season19    52.71428571
season20    -90.78571429
season21    -27.28571429
season22    -124.2857143
season23    -101.2857143
season24    -23.71428571
season25    38.71428571
season26    -225.2142857
season27    -47.78571429
season28    -46
season29    27
season30    43.28571429
season31    1498.5
season32    791.7142857
season33    666.7857143
season34    1913.5
season35    1657
season36    119
season37    -205.7857143
season38    -420.2142857
season39    -152.7857143
season40    -360.2142857
season41    -123.7857143
season42    77.21428571
season43    -40.78571429
season44    -10.78571429
season45    -48.78571429
season46    73.21428571
season47    81.21428571
season48    26.21428571
season49    -1.785714286
season50    25.21428571
season51    -105.2142857
season52    -161.2142857
</code></pre>

<p><strong>2. My second question is how do I extrapolate this for the entire category?</strong>
If I have the above information for 12 items, how can I look at the coefficients collectively? That is, I want to say ""Front page advertising has x% lift for Category A"".</p>
"
"0.0609427635336005","0.0580747904139041","143943","<p>I have a need to do realtime predictions for individual rows of data based on a previously computed randomForest algorithm.  How can I run the ""predict"" command without recomputing ""fit"" on the entire training data set each time?  </p>

<p>I am using R and here's the line of code that computes ""fit"" by applying the randomForest algorithm on the training set.</p>

<pre><code>fit &lt;- randomForest(formula2, data=training, importance=TRUE, ntree=2000, na.action = na.omit)
</code></pre>

<p>And here's the predict command - I want to be able to run this without having to recompute fit every time.  Is this possible?</p>

<pre><code>outp_rf &lt;- predict(fit, testing)
</code></pre>

<p>For LogisticRegression, I know the coefficients so I can rerun the logistic function to compute the outcome.  However not sure how I can do it for RandomForest.</p>
"
"0.052777981396926","0.0502942438178979","144076","<p>I have a bunch of data that I fit a linear regression to, and now I need to find the variance of my slope. Is there an analytical way to get this?</p>

<p>If an example is necessary, consider this my data in R:</p>

<pre><code>x &lt;- c(1:6)
y &lt;- c(18, 14, 15, 12, 7, 6)
lm(y ~ x)$coefficients
</code></pre>

<p>So I have a slope estimate of <code>-2.4</code>, but I want to know the variance of that estimate.</p>

<p>After looking at previous questions, I've seen a few equations for estimating the slope parameter, but I'm a little confused about what the differences between equations are and what approach is valid for my problem.</p>

<p>For example, the answers in <a href=""http://stats.stackexchange.com/questions/122406/the-variance-of-linear-regression-estimator-beta-1"">this question</a> say that $\newcommand{\Var}{\rm Var}\newcommand{\slope}{\rm slope}\Var[\slope] = \frac{V[Y]}{\sum\left(\frac{x_i-\bar{x}}{\sum(x_i-\bar{x})^2}\right)}$.</p>

<p><a href=""http://stats.stackexchange.com/questions/12186/expected-value-and-variance-of-estimation-of-slope-parameter-beta-1-in-simple"">This question</a> says that $\Var[\slope] = \frac{V[Y]}{\sum(x_i-\bar{x})^2}$.</p>

<p>And if I look at the output in R (as a ""check"" mechanism), I'm given two other ways I could potentially calculate the slope variance (one using the standard error, another given the covariance matrix). I feel like I'm missing something key because all these estimates give me similar (but not the same) answer.</p>
"
"0.089582012671609","0.0853662733540307","144096","<p>I have data from gene expression arrays and I have clinical data associated with the samples used. I am using gene expression (discrete), age at diagnosis (discrete) and ethnicity (categorical) to build a regression model. I'd like to predict gene expression by age at diagnosis and ethnicity so the model would be something like:</p>

<p>$$y = b_0 + b_1x_1 + b_2x_2 + \epsilon$$</p>

<p>in R it looks something like:</p>

<pre><code>y &lt;- as.numeric(gene_expression_myFavoriteGenes)
age &lt;- age_diagnosis 
ethnicity &lt;- ethnicity 
l &lt;- lm(y ~ age + ethnicity)
</code></pre>

<p>now...when I look at the coefficients I get something like:</p>

<pre><code>Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)      -0.442785   0.151175  -2.929 0.008299 ** 
age_diagnosis    -0.005616   0.002422  -2.319 0.031090 *  
Caucasian        -0.910633   0.115870  -7.859 1.53e-07 ***
Hispanic         -0.801088   0.125429  -6.387 3.13e-06 ***
Honduran         -0.682405   0.210694  -3.239 0.004114 ** 
Peurto Rican     -0.679251   0.209620  -3.240 0.004100 ** 
South Asian      -1.237134   0.213569  -5.793 1.14e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.1815 on 20 degrees of freedom
Multiple R-squared:  0.7795,    Adjusted R-squared:  0.7023 
F-statistic:  10.1 on 7 and 20 DF,  p-value: 2.21e-05
</code></pre>

<p>I'm a lil confused on how to interpret the data.
I first built the model without correcting for ethnicity, nothing was significant. 
then I added ethnicity, checked with <code>anova(fist_lm, second_lm)</code> and the difference is significant.</p>

<p>so now I have all ethnicity associated with a significant p-value and and adjusted R^2 of 0.7 ...so, my questions, sorry if they sound silly, are:</p>

<ul>
<li>in this specific case, can age + ANY ethnicity significantly predict expression of myFavoriteGene?</li>
<li>models with other genes only have one or two significant ethnicity, does that mean that those are the only ones that can reliably predict expression of myFavoriteGene?</li>
</ul>

<p>------- edit ---------</p>

<p>here an example of how my data look:</p>

<pre><code>&gt; data[1:10,1:6]
           skin.AA_2  skin.AA_3  skin.AA_4  skin.AA_5  skin.AA_6   skin.AA_7
100_g_at   5.5526731  4.7001569  5.3724104  5.3700587  5.7571421  5.76974711
1000_at    7.7757596  5.4761710  7.3896019  5.5514442  8.2559761  7.14107706
1001_at    1.4554496  0.3315354  1.3311387  1.4979105  2.0579317  1.50734217
1002_f_at -0.4427732 -0.7381431 -0.3714425 -0.3159300 -0.2270056  0.31245288
1003_s_at  1.7908548  1.2590320  1.4839795  1.6727171  1.8550568  1.99870500
1004_at    1.8082815  0.9940647  1.4085169  1.8658939  1.9275267  2.25192977
1005_at    3.1792907 10.2456153  6.1170771  9.9058017  8.3695269  5.02225258
1006_at   -0.3059731 -0.8761517 -0.7151807 -0.4620902 -0.5923052  0.02495093
1007_s_at  9.9911387 10.2839949 10.1105075  9.9944011 10.3866696 10.31211765
1008_f_at  7.9190579  4.5957139  4.0043624  4.6297893  4.2067368  7.62499810

&gt; clinicalData[1:10,1:5]  
patient_ID        ethnicity age_diagnosis age colname_data_matrix_SB
1      AA1005 African American            39  47              skin.AA_1
2      AA1007        Caucasian            32  50              skin.AA_3
3      AA1008        Caucasian            50  50              skin.AA_4
4      AA1009        Caucasian            18  68              skin.AA_5
5      AA1010        Caucasian            31  40              skin.AA_6
6      AA1015        Caucasian            41  43                       
7      AA3001         Hispanic            49  49              skin.AA_7
8      AA3006         Hispanic            48  51              skin.AA_8
9      AA3007         Hispanic            51  51              skin.AA_9
10     AA3008         Hispanic            49  50             skin.AA_10
</code></pre>
"
"0.0304713817668003","0.029037395206952","144152","<p>While I am reasonably comfortable with performing and interpreting the output from logistic regression using glm in R, I had a question about the mechanics of the calculation to better understand what is going on.</p>

<p>I am trying to fit a logistic model,</p>

<p>$\log \large( \frac{p}{1-p} \large) = \beta_0 + \beta_1 x_1 + \beta_2 x_2$</p>

<p>where the input data consists of single rows for each observation with success/failure coded as 0/1 and the dependent numerical values $x_1,x_2$.</p>

<p>How are the individual 0/1 values converted into the LHS of equation above and used for the fitting of the coefficients?</p>
"
"0.105869651339174","0.108647984268766","144247","<p>This is my first time attempting to build a linear regression model and I am not sure what to do next given the results I have.</p>

<p>I have a data set with 24 predictors and 1 response and there are 999 rows in the data set.  The data can be found <a href=""http://pastebin.com/Whv6tgiv"" rel=""nofollow"">here</a>.  I am trying to build a linear regression model with the end goal of being able to predict the response variable.</p>

<p>I decided to log transform the response variable as this resulted in the histogram looking more Normally distributed:
<img src=""http://i.imgur.com/vK1hF7Q.png"" alt=""Untransformed response"">
<img src=""http://i.imgur.com/eVERAw8.png"" alt=""Log transformed response""></p>

<p>My first linear regression model was the following:</p>

<pre><code>Call:
lm(formula = log(y) ~ ., data = the_data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.70154 -0.13329  0.01642  0.14626  1.10267 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  5.112026   0.061692  82.863  &lt; 2e-16 ***
v1           0.136381   0.024563   5.552 3.64e-08 ***
v2           0.069991   0.024519   2.855  0.00440 ** 
v3           0.034584   0.024504   1.411  0.15845    
v4           0.031069   0.024562   1.265  0.20620    
v5          -0.078188   0.024556  -3.184  0.00150 ** 
v6          -0.007898   0.024579  -0.321  0.74803    
v7          -0.062695   0.024613  -2.547  0.01101 *  
v8          -0.007664   0.024553  -0.312  0.75501    
v9          -0.019956   0.024558  -0.813  0.41664    
v10          0.025143   0.024561   1.024  0.30623    
v11          0.003946   0.024583   0.161  0.87250    
v12          0.024605   0.024551   1.002  0.31650    
v13         -0.005139   0.024558  -0.209  0.83429    
v14         -0.071931   0.024534  -2.932  0.00345 ** 
v15          0.002112   0.024552   0.086  0.93145    
v16         -0.050584   0.024510  -2.064  0.03930 *  
v17          0.012561   0.024491   0.513  0.60815    
v18         -0.029778   0.024515  -1.215  0.22478    
v19          0.030362   0.024559   1.236  0.21666    
v20         -0.022925   0.024519  -0.935  0.35002    
v21         -0.003210   0.024532  -0.131  0.89591    
v22          0.027668   0.024617   1.124  0.26132    
v23         -0.009467   0.024557  -0.386  0.69993    
v24          0.003332   0.024567   0.136  0.89214    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.223 on 974 degrees of freedom
Multiple R-squared:  0.07552,   Adjusted R-squared:  0.05274 
F-statistic: 3.315 on 24 and 974 DF,  p-value: 1.603e-07
</code></pre>

<p>The R-squared value (0.076) tells me that this model is not very good, correct?</p>

<p>Looking at the Residuals vs Fitted and Scale-Location plots I think that there is some pattern (upside down parabola).<br>
<img src=""http://i.imgur.com/C7i2kgA.png"" alt=""First model residuals""></p>

<p>So I decided to fit the following model which has first order interactions</p>

<pre><code>Call:
lm(formula = log(y) ~ .^2, data = the_data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.60878 -0.11444  0.00161  0.11522  0.71236 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  4.510425   0.372884  12.096  &lt; 2e-16 ***
v2           0.417113   0.211401   1.973 0.048879 *  
v3           0.554384   0.218996   2.531 0.011577 *  
...   
v12          0.644474   0.219123   2.941 0.003378 ** 
...
v1:v2       -0.501372   0.088464  -5.668 2.12e-08 ***
... 
v1:v5        0.226257   0.091674   2.468 0.013823 *  
...
v1:v12      -0.265495   0.089155  -2.978 0.003003 ** 
...
v1:v16      -0.226053   0.090448  -2.499 0.012674 *  
...
v2:v9        0.175206   0.088957   1.970 0.049285 *  
...
v2:v12      -0.558579   0.087183  -6.407 2.73e-10 ***
...
v2:v16       0.230151   0.088761   2.593 0.009716 ** 
v2:v17      -0.235761   0.087570  -2.692 0.007267 ** 
...
v2:v22       0.219280   0.085991   2.550 0.010984 *  
...
v3:v5       -0.305964   0.087337  -3.503 0.000489 ***
...
v3:v9       -0.303287   0.086619  -3.501 0.000492 ***
...
v4:v5       -0.382494   0.091162  -4.196 3.07e-05 ***
...
v5:v13       0.222259   0.089072   2.495 0.012816 *  
...
v6:v9        0.185673   0.090535   2.051 0.040656 *  
v6:v10       0.216236   0.090258   2.396 0.016849 *  
...
v6:v17       0.255650   0.087794   2.912 0.003707 ** 
...
v6:v22      -0.198706   0.089566  -2.219 0.026838 *  
...
v7:v15      -0.192992   0.089102  -2.166 0.030652 *  
...
v8:v16       0.230660   0.090440   2.550 0.010971 *  
...
v9:v13      -0.190571   0.087136  -2.187 0.029070 *  
...
v9:v23       0.322755   0.088111   3.663 0.000268 ***
...
v12:v17      0.182697   0.090321   2.023 0.043480 *  
...
v12:v22     -0.229433   0.089649  -2.559 0.010700 *  
...
v13:v14      0.232871   0.086159   2.703 0.007043 ** 
...
v14:v15      0.206125   0.089665   2.299 0.021810 *  
...
v15:v16     -0.203924   0.091111  -2.238 0.025523 *  
...   
v22:v23     -0.184572   0.090622  -2.037 0.042055 *  
...
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.2055 on 698 degrees of freedom
Multiple R-squared:  0.4369,    Adjusted R-squared:  0.1949 
F-statistic: 1.805 on 300 and 698 DF,  p-value: 1.863e-10
</code></pre>

<p>I have omitted multiple lines that are not significant at the 0.05 level or below in the output (denoted by ...) for brevity.</p>

<p>The residual plots for this model are shown below:
<img src=""http://i.imgur.com/VFk6ClY.png"" alt=""Second interaction model residuals""></p>

<p>The R-squared value is greatly improved (0.437 versus 0.076) but still nowhere close to what I think I need for a good model (R-squared > 0.8).</p>

<p>I am unsure of how to proceed - what to try next in order to get a better model.</p>
"
"0.0304713817668003","0.029037395206952","144603","<p>I have built a logistic regression where the outcome variable is being cured after receiving treatment (<code>Cure</code> vs. <code>No Cure</code>). All patients in this study received treatment. I am interested in seeing if having diabetes is associated with this outcome. </p>

<p>In R my logistic regression output looks as follows: </p>

<pre><code>Call:
glm(formula = Cure ~ Diabetes, family = binomial(link = ""logit""), data = All_patients)
...
Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   1.2735     0.1306   9.749   &lt;2e-16 ***
Diabetes     -0.5597     0.2813  -1.990   0.0466 *  
...
    Null deviance: 456.55  on 415  degrees of freedom
Residual deviance: 452.75  on 414  degrees of freedom
  (2 observations deleted due to missingness)
AIC: 456.75
</code></pre>

<p>However, the confidence interval for the odds ratio <strong>includes 1</strong>:</p>

<pre><code>                   OR     2.5 %   97.5 %
(Intercept) 3.5733333 2.7822031 4.646366
Diabetes    0.5713619 0.3316513 1.003167
</code></pre>

<p>When I do a chi-squared test on these data I get the following:</p>

<pre><code>data:  check
X-squared = 3.4397, df = 1, p-value = 0.06365
</code></pre>

<p>If you'd like to calculate it on your own the distribution of diabetes in the cured and uncured groups are as follows:</p>

<pre><code>Diabetic cure rate:      49 /  73 (67%)
Non-diabetic cure rate: 268 / 343 (78%)
</code></pre>

<p>My question is: Why don't the p-values and the confidence interval including 1 agree? </p>
"
"0.0457070726502004","0.0580747904139041","144609","<p>I am examining BMI effect on wage. I have included a lot of explanatory variables in the regression and all (most) coefficients were significant, but once I add ability which correlates with education (already in regression) slightly (34%) BMI measures turn insignificant. </p>

<p>Here is the question, what does it mean if BMI turns insignificant once we add another variable in the regression? </p>

<p>Estimated by OLS, cross sectional. Adj R  0.60, F=160. </p>

<p>PS: I added a variable and variable of interest turned insignificant, not vice versa. 
Thank you for all the examples below, they address the same topic, but different issue. </p>
"
"0.0609427635336005","0.0580747904139041","145053","<p>I have a linear regression model:</p>

<pre><code>model &lt;- lm(data=df, var1~var2+var3+var4+var5+var6+var7)
</code></pre>

<p>Hypothesis about absence of heteroscedasticity is rejected as Breusch-Pagan test gives small p-value:</p>

<pre><code>bptest(model)$p.value
#BP 
#1.014577e-06
</code></pre>

<p>But when I use robust estimations for parameters of the model:</p>

<pre><code>library(""sandwich"")
coeftest(model, vcov. = vcovHC(model))
</code></pre>

<p>... for all parameters the value <code>Pr(&gt;|t|)</code> is reducing. So the coefficients seem to become more significant although robust estimators usualy do vice versa.</p>

<p>Would you explain the matter of why this happens? Or robust estimators don't reduce statistical significance of parameters for some special cases? Would you mention these cases?</p>

<p>Thank you.</p>
"
"0.052777981396926","0.0502942438178979","145315","<p>I have age as a covariate in my material. A continuous variable. The age varies between 18-70 years.</p>

<p>I'm into a logistic regression and do not really know how to treat the variable. As a linear effect or as a polynomial?</p>

<pre><code>   gender       passinggrade age    prog
1    man          FALSE      69     FRIST
2    man             NA      70     FRIST
3 woman             NA       65     FRIST
4 woman           TRUE       68      FRIST
5 woman             NA       65     NMFIK
6    man          FALSE      70     FRIST
</code></pre>

<p>my model;</p>

<pre><code>mod.fit&lt;-glm(passinggrade ~prog+gender+age,family=binomial,data=both)
</code></pre>

<p>summary(mod.fit)</p>

<pre><code>Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  2.42653    0.28096   8.636  &lt; 2e-16 ***
progLARAA    0.44931    0.25643   1.752 0.079746 .  
progNASTK   -0.15524    0.26472  -0.586 0.557597    
progNBFFK    0.12091    0.65460   0.185 0.853462    
progNBIBK   -0.18850    0.37656  -0.501 0.616659    
progNDATK   -2.84617    0.73077  -3.895 9.83e-05 ***
progNFYSK    0.64391    0.19634   3.280 0.001040 ** 
progNMATK    0.18424    0.16451   1.120 0.262733    
progNMETK    0.22433    0.29086   0.771 0.440554    
progNMFIK    0.38877    0.42152   0.922 0.356373    
progNSFYY    0.97205    0.29320   3.315 0.000915 ***
progSMEKK   -0.58043    0.18185  -3.192 0.001414 ** 
genderman   -0.05623    0.10477  -0.537 0.591496        
age         -0.11780    0.01028 -11.462  &lt; 2e-16 ***
</code></pre>

<p>how would you treat the variable age?
and how should I interpret the results for age?</p>
"
"NaN","NaN","145836","<p><img src=""http://i.stack.imgur.com/QPcxY.png"" alt=""enter image description here""></p>

<p>I am doing a project on cloud cover and cosmic rays and have undertaken a regression model in R. Above is the regression diagnostic plot and from the QQ plot I can see that the tails are skewed, meaning it isn't a normal distribution. How significantly will this affect my results? Below is the results, showing no relationship. Would I even expect data from the natural world to follow a normal distribution?</p>

<p>Here is the output for this particular model:</p>

<pre><code>Call:
lm(formula = magadanlc ~ magadancr, data = lc)      Output: 1
Residuals:
    Min      1Q  Median      3Q     Max 
-26.038 -11.044  -1.454  10.202  39.652 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)
(Intercept) 15.049904  12.494996   1.204    0.229
magadancr    0.002291   0.001492   1.536    0.126

Residual standard error: 13.19 on 314 degrees of freedom
  (2 observations deleted due to missingness)
Multiple R-squared:  0.007456,  Adjusted R-squared:  0.004295 
F-statistic: 2.359 on 1 and 314 DF,  p-value: 0.1256
</code></pre>
"
"0.143963746891895","0.142676349100432","145849","<p>Iâ€™ve got a question concerning the R package <em>strucchange</em> that I use for testing and dating structural breaks in my PhD thesis.  To be specific, I use the generalized fluctuation test framework with CUSUM/MOSUM and in particular Moving Estimates (<strong>ME</strong>) tests for my analysis. Thus, the following description focuses on the ME test, but in principle is more general to all fluctuation tests.</p>

<p><strong>The problem:</strong> I am testing time series data for structural breaks with the ME test that draws on the function <strong>efp</strong> provided by strucchange. Given the nature of time series data, I want to tackle potential heteroskedasticity and autocorrelation in the data. Strucchange provides some functionality with respect to calculating <em>heteroskedasticity</em> (<strong>HC</strong>) and <em>autocorrelation</em> (<strong>HAC</strong>) consistent covariance matrices,  e.g., the approaches suggested by Newey-West (1987) or Andrews (1991). </p>

<p>However, this functionality in strucchange is limited to the function <strong>gefp</strong> that calculates Generalized Empirical M-Fluctuation Processes that as far as I know does not allow to perform estimates-based tests such as the ME test. Thus, I cannot use <strong>efp</strong> to estimate ME tests (or other tests that are available in this function) using HAC covariance matrices. </p>

<p><strong>The question:</strong> Does anybody know how I could make use of the <strong>efp</strong> function in <em>strucchange</em> for testing and dating structural changes but use HAC covariance matrices to take heteroskedasticity and autocorrelation into account? Maybe there is some way to use the sandwich package for this?</p>

<p><strong>Many thanks for any help!</strong></p>

<p>Here is a minimal working example to show the problem</p>

<pre><code>library(foreign)
library(strucchange)

data(""Nile"")

#using the function efp to perform a moving estimates test
#assuming sperical disturbances
ocus.nile &lt;- efp(Nile ~ 1, type = ""ME"")
sctest(ocus.nile)

#applying the vcov function with the kernHAC option to take heteroskedasticity and autocorrelation does not work, i.e., the option is not used and the result is the same
sctest(ocus.nile, vcov=kernHAC)

#using the function gefp to perform a generalized M-fluctuation process however works with vcov
#assuming spherical disturbances
ocus.nile2 &lt;- gefp(Nile ~ 1, fit = lm)
sctest(ocus.nile2)

#controlling for heteroskedasticity and autocorrelation using an appropriate covariance matrix changes the result, i.e. works
ocus.nile2 &lt;- gefp(Nile ~ 1, fit = lm, vcov= kernHAC)
sctest(ocus.nile2)
</code></pre>

<p><strong>Some background</strong></p>

<p>Though probably not necessary, here is some more in-depth background about the problem for the interested reader (and the archive). The formulas are taken from Zeileis et al., 2005, â€Monitoring structural change in dynamic econometric modelsâ€. </p>

<p>The ME test is used to detect structural breaks in the standard linear regression model over time. What it does it in essence partitioning the data and rather than estimating the regression based on the whole sample, it sequentially moves â€œthroughâ€ time in a fixed-width windows containing only a sub-sample of the observations and in each window it estimates the model. These estimates are used to the computation of empirical fluctuation processes that capture fluctuations in regression coefficients and residuals over time. Significant fluctuations of the coefficients are signs of a structural break in the regression. The test statistic of the Moving estimates test is</p>

<p><img src=""http://i.stack.imgur.com/qQ0FY.png"" alt=""Moving estimates test statistic""></p>

<p>where <em>n</em> is the number of observations, <em>h</em> is the bandwith (how many percent of the total number of observations are used for the window), <em>nh</em> is thus the size of the window, Q_(n)=X_(n)^T X_(N)/n, i=[k+t(n-k)], and sigma^2 is an estimate of the variance. The way I understand the above statistic is that it compares the difference between the sub-sample estimate of beta with the whole sample (the window) estimate and how this difference develops over time. A zero difference would indicate a sub-sample estimate that perfectly equals the whole-sample estimate, which would indicate perfect stability of the coefficient. In my understanding, the efp function in strucchange calculates sigma^2 based on the standard OLS residuals u^ i.e., sigma^2=(1/n-k)âˆ‘_(i=1)^n u_i^2 . Thus, in the presence of heteroskedasticity or autocorrelation, the OLS assumption of spherical disturbances will be violated. Thus, ideally, sigma^2 should be estimated based on a HAC covariance matrix to avoid wrong inference.</p>

<p>The question that comes to my mind is whether there is a way to use the ME test based on a HAC estimate. If not, it seems to me that it is limited to spherical disturbances of residuals, which seems to be violated in most applications.</p>
"
"0.068136080998913","0.0519436716578171","145949","<p>I'm doing a linear regression, in R. The values are like this -</p>

<pre><code>u  &lt;-  c(1,2,3,4,5,6,7,8,9,10)
v &lt;- c(21,22,23,24,25,26,27,28,29,30)
w &lt;- c(41,42,43,44,45,46,47,48,49,50)
y &lt;- c(128.2305,132.4040,140.1732,147.3236, 154.5410, 158.7206, 165.1761, 169.7121,178.9751,181.0309)
</code></pre>

<p>If I call linear regression function, it's returning a model, which is disregarding v and w.</p>

<pre><code>model &lt;- lm(y~u+v+w)

Coefficients:
(Intercept)            u            v            w   
    122.074        6.101           NA           NA  

summary(model)
</code></pre>

<p>Output: </p>

<pre><code>Call:
lm(formula = y ~ u + v + w)

Residuals:
       Min       1Q   Median       3Q      Max 
-2.05143 -0.92734  0.04845  0.73362  1.99357 

Coefficients: (2 not defined because of singularities)
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 122.0743     1.0197  119.72 2.65e-14 ***
u             6.1008     0.1643   37.12 3.04e-10 ***
v                 NA         NA      NA       NA    
w                 NA         NA      NA       NA    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 1.493 on 8 degrees of freedom
Multiple R-squared:  0.9942,    Adjusted R-squared:  0.9935 
F-statistic:  1378 on 1 and 8 DF,  p-value: 3.04e-10
</code></pre>

<p>I tried to fit a linear model before with different values of y,u,v (with two predictor variables, w was absent), and there also, v was being assigned NA, and only u was getting co-efficients. What's happening?</p>
"
"0.0963589698356145","0.0918243061724248","146046","<p>I'm investigating whether there is a relationship between the day of the week and an outcome value using linear regression in <code>R</code>, and would like to understand how to interpret the residual plots.</p>

<p><strong>Data</strong></p>

<p>Example dummy data (the mean and SD are based on actual data I have): </p>

<pre><code>set.seed(14)
mon &lt;- data.frame(id=seq(6, 60*7, by=7), value = rnorm(60, 4372, 145))
tue &lt;- data.frame(id=seq(7, 60*7, by=7), value = rnorm(60, 4433, 206))
wed &lt;- data.frame(id=seq(1, 60*7, by=7), value = rnorm(60, 4671, 143))
thu &lt;- data.frame(id=seq(2, 60*7, by=7), value = rnorm(60, 4555, 154))
fri &lt;- data.frame(id=seq(3, 60*7, by=7), value = rnorm(60, 4268, 149))
sat &lt;- data.frame(id=seq(4, 60*7, by=7), value = rnorm(60, 1579, 110))
sun &lt;- data.frame(id=seq(5, 60*7, by=7), value = rnorm(60, 1136, 68))
startdate &lt;- seq.Date(as.Date(""2014-01-01""), by=""day"", length.out=(60*7) )
id &lt;- seq(1, 60*7)
wd &lt;- weekdays(startdate)
df &lt;- data.frame(id, startdate, wd)
days &lt;- rbind(mon, tue, wed, thu, fri, sat, sun)
df &lt;- merge(df, days)

head(df)
  id  startdate        wd    value
1  1 2014-01-01 Wednesday 4593.117
2  2 2014-01-02  Thursday 4686.159
3  3 2014-01-03    Friday 4352.982
4  4 2014-01-04  Saturday 1825.172
5  5 2014-01-05    Sunday 1206.759
6  6 2014-01-06    Monday 4276.032
</code></pre>

<p>which looks like</p>

<pre><code>library(ggplot2)
ggplot(data=df, aes(x=startdate, y=value, colour=wd)) +
  geom_point() +
  geom_smooth( alpha=.3, size=1, aes(fill=wd)) +
  facet_wrap(~wd) 
</code></pre>

<p><img src=""http://i.stack.imgur.com/lX6CG.png"" alt=""data plot""></p>

<p><strong>Model</strong></p>

<p>Modelling the data using <code>fit &lt;- lm(data=df, value ~ wd)</code> produces the coefficients:</p>

<pre><code>summary(fit)
....
Coefficients:
             Estimate Std. Error  t value Pr(&gt;|t|)    
(Intercept)  4242.60      18.83  225.319  &lt; 2e-16 ***
wdMonday      148.93      26.63    5.593 4.07e-08 ***
wdSaturday  -2661.93      26.63  -99.965  &lt; 2e-16 ***
wdSunday    -3113.78      26.63 -116.933  &lt; 2e-16 ***
wdThursday    299.65      26.63   11.253  &lt; 2e-16 ***
wdTuesday     189.04      26.63    7.099 5.51e-12 ***
wdWednesday   412.52      26.63   15.492  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 145.9 on 413 degrees of freedom
Multiple R-squared:  0.9896,    Adjusted R-squared:  0.9894 
F-statistic:  6539 on 6 and 413 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>The plot of the data, and the coefficients seem to suggests there is a relationship between day of the week and the outcome value. </p>

<p>However, I know I also need to consider the residual plots when interpreting the validity of a model. For this example the residual plots are:</p>

<pre><code>par(mfrow=c(2,2))
plot(fit)
</code></pre>

<p><img src=""http://i.stack.imgur.com/gzpGZ.png"" alt=""residual plots""></p>

<p><strong>Question</strong></p>

<p>Through various stats courses/uni/research (e.g. <a href=""http://stats.stackexchange.com/questions/76226/interpreting-the-residuals-vs-fitted-values-plot-for-verifying-the-assumptions"">this question</a>) I know that for a good linear model you are looking for unbiased homoscedastic residuals. But my knowledge on this subject is a bit rusty. Therefore, do my residuals suggest a linear model is not an appropriate fit for the data? And/or is there another aspect to this that I should be considering, or have I completely missed the point all together?</p>
"
"0.13282167379153","0.106586165297479","146421","<p>I am using rlm robust linear regression of MASS package on modified iris data set as follows:</p>

<pre><code>&gt; myiris = iris
&gt; myiris$Species = as.numeric(myiris$Species)
&gt; head(myiris)
  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2       1
2          4.9         3.0          1.4         0.2       1
3          4.7         3.2          1.3         0.2       1
4          4.6         3.1          1.5         0.2       1
5          5.0         3.6          1.4         0.2       1
6          5.4         3.9          1.7         0.4       1

&gt; library(MASS)
&gt; rmod = rlm(Species~., data=myiris)
&gt; rmod
Call:
rlm(formula = Species ~ ., data = myiris)
Converged in 6 iterations

Coefficients:
 (Intercept) Sepal.Length  Sepal.Width Petal.Length  Petal.Width 
  1.14943807  -0.11067690  -0.02603537   0.21581357   0.63793686 

Degrees of freedom: 150 total; 145 residual
Scale estimate: 0.191 
&gt; 
&gt; sumrmod = summary(rmod)
&gt; sumrmod

Call: rlm(formula = Species ~ ., data = myiris)
Residuals:
     Min       1Q   Median       3Q      Max 
-0.59732 -0.15769  0.01089  0.10955  0.56317 

Coefficients:
             Value   Std. Error t value
(Intercept)   1.1494  0.2056     5.5906
Sepal.Length -0.1107  0.0579    -1.9128
Sepal.Width  -0.0260  0.0599    -0.4346
Petal.Length  0.2158  0.0571     3.7821
Petal.Width   0.6379  0.0948     6.7287

Residual standard error: 0.1913 on 145 degrees of freedom
</code></pre>

<p>This does not give p.values so I calculated them as follows (using pt function of base R):</p>

<pre><code>&gt; dd = data.frame(sumrmod$coefficients)                             #$
&gt; dd$p.value =  pt(dd$t.value, sumrmod$df[2])                       #$
&gt; dd
                   Value Std..Error    t.value    p.value
(Intercept)   1.14943807 0.20560264  5.5905804 0.99999995
Sepal.Length -0.11067690 0.05786107 -1.9128044 0.02887227
Sepal.Width  -0.02603537 0.05991073 -0.4345693 0.33226054
Petal.Length  0.21581357 0.05706173  3.7821068 0.99988663
Petal.Width   0.63793686 0.09480869  6.7286751 1.00000000
</code></pre>

<p>However, these are not correct since ordinary lm function and other regression functions show that Petal.Length and Petal.Width are highly significant in this regression:</p>

<pre><code>&gt; summary(lm(Species~., data=myiris))

Call:
lm(formula = Species ~ ., data = myiris)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.59215 -0.15368  0.01268  0.11089  0.55077 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   1.18650    0.20484   5.792 4.15e-08 ***
Sepal.Length -0.11191    0.05765  -1.941   0.0542 .  
Sepal.Width  -0.04008    0.05969  -0.671   0.5030    
Petal.Length  0.22865    0.05685   4.022 9.26e-05 ***
Petal.Width   0.60925    0.09446   6.450 1.56e-09 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.2191 on 145 degrees of freedom
Multiple R-squared:  0.9304,    Adjusted R-squared:  0.9285 
F-statistic: 484.5 on 4 and 145 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>Where is the error? Am I not using correct method to calculate p.value here?</p>

<p>Edit: As suggested (further) by @Glen_b in the comments: </p>

<pre><code>&gt; dd$p.value =  2*pt(abs(dd$t.value), sumrmod$df[2], lower.tail=FALSE)      #$
&gt; dd
               Value Std..Error    t.value      p.value
(Intercept)   1.14943807 0.20560264  5.5905804 1.089792e-07
Sepal.Length -0.11067690 0.05786107 -1.9128044 5.774455e-02
Sepal.Width  -0.02603537 0.05991073 -0.4345693 6.645211e-01
Petal.Length  0.21581357 0.05706173  3.7821068 2.267410e-04
Petal.Width   0.63793686 0.09480869  6.7286751 3.691993e-10
</code></pre>

<p>These seem to be correct (finally).</p>
"
"0.0806196982594614","0.076825726438694","146431","<p>In the output of felm function which is a function for the Linear Models with Multiple Fixed Effects, two R-squared information are provided: Multiple R-squared(full model) and Multiple R-squared(proj model).</p>

<p>How to interpret the Multiple R-squared(proj model)? I guess the R-squared(full model) refer to the effect of all the variables (x1, f1, f2 and f3) in the model, in which f1, f2 and f3 serves like categorical variables in regression. And Multiple R-squared(proj model) refers to the effect of purely f1, f2 and f3 when x1 is not included in the model. Am I right? I need to understand this in order to calculate the effect size of the independent variable (x1 in this case). Thanks!</p>

<pre><code>   summary(est &lt;- felm(y ~ x1 | f1 + f2 + f3))

Call:
   felm(formula = y ~ x1 | f1 + f2 + f3) 

Residuals:
     Min       1Q   Median       3Q      Max 
-2.35266 -0.57436 -0.00792  0.61786  2.13316 

Coefficients:
   Estimate Std. Error t value Pr(&gt;|t|)    
x1   2.4043     0.1217   19.75   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

    Residual standard error: 0.9832 on 76 degrees of freedom
    Multiple R-squared(full model): 0.9058   Adjusted R-squared: 0.8773 
    Multiple R-squared(proj model): 0.8369   Adjusted R-squared: 0.7876 
    F-statistic(full model):31.79 on 23 and 76 DF, p-value: &lt; 2.2e-16 
    F-statistic(proj model): 390.1 on 1 and 76 DF, p-value: &lt; 2.2e-16 
    *** Standard errors may be too high due to more than 2 groups and exactDOF=FALSE
</code></pre>
"
"0.0609427635336005","0.0435560928104281","146615","<p>I am trying to work with Poisson regression. I came across this video which is very helpful - <a href=""https://www.youtube.com/watch?v=HntUY8SsYZg"" rel=""nofollow"">https://www.youtube.com/watch?v=HntUY8SsYZg</a>. In the video one of parameters (Race) is categorical and treated vide factor() in R.</p>

<p>Ideally the model should show coefficient of every parameter - Race, Income and GPA. But for the categorical variable Race - it shows coefficient for value of categorical variable. [Video at 09:18]</p>

<pre><code>                 CO        2.5 %     97.5 %
(Intercept) 0.003004161 0.0007308336 0.01167968
Race2       2.811686825 1.5523542481 5.62637686
Race3       1.310301471 0.5854852933 2.98624050
Income      1.066367468 1.0494809143 1.08382221
GPA         1.129708044 0.9120963633 1.40195401
</code></pre>

<p>Can someone explain this ? why is every value of categorical variable having a different coefficient ?</p>
"
"0.075412822378","0.0615976171764886","146665","<p>The answer to <a href=""http://stats.stackexchange.com/questions/41129/interpreting-coefficients-of-an-interaction-between-categorical-and-continuous-v/41292#41292"">Interpreting coefficients of an interaction between categorical and continuous variable</a> contains a phrase that seems to have some significant impact on how coefficients are interpreted in a multiple-regression when a factor is introduced: </p>

<blockquote>
  <p>If treatment contrasts for a categorical variable are present in a model, the estimation of further effects is based on the reference level of the categorical variable. [..]</p>
  
  <p>note that the estimation of the coefficients is based on the references categories of the factors (if treatment contrasts are employed). In this case the effects hold for $race = white$, $sex = male$, and $educa = 1$. They do not test an overall influence of the numeric variables irrespective of the levels of the factors.</p>
</blockquote>

<p><strong>Question:</strong> How does including a factor into a multiple regression affect the interpretation of the <em>other</em> coefficients (whether numeric predictors or interactions)?</p>

<p>Consider this example from <a href=""http://www.jstatsoft.org/v08/i15"" rel=""nofollow"">Fox 2003</a>: </p>

<pre><code>require(effects)
require(lmtest)
Arrests$year &lt;- as.factor(Arrests$year)
arrests.mod &lt;- glm(released ~ employed + citizen + checks
                         + colour*year + colour*age,
                         family=binomial, data=Arrests)
</code></pre>

<p>Which yields: </p>

<pre><code>&gt; coeftest(arrests.mod)

z test of coefficients:

                       Estimate Std. Error  z value  Pr(&gt;|z|)    
(Intercept)           0.3444334  0.3100749   1.1108 0.2666514    
employedYes           0.7350645  0.0847701   8.6713 &lt; 2.2e-16 ***
citizenYes            0.5859841  0.1137717   5.1505 2.598e-07 ***
checks               -0.3666425  0.0260322 -14.0842 &lt; 2.2e-16 ***
colourWhite           1.2125167  0.3497751   3.4666 0.0005272 ***
year1998             -0.4311794  0.2603589  -1.6561 0.0977023 .  
year1999             -0.0944343  0.2615447  -0.3611 0.7180519    
year2000             -0.0108975  0.2592073  -0.0420 0.9664655    
year2001              0.2430630  0.2630151   0.9241 0.3554129    
year2002              0.2129549  0.3532786   0.6028 0.5466444    
age                   0.0287279  0.0086191   3.3330 0.0008590 ***
colourWhite:year1998  0.6519565  0.3134898   2.0797 0.0375555 *  
colourWhite:year1999  0.1559504  0.3070430   0.5079 0.6115161    
colourWhite:year2000  0.2957537  0.3062034   0.9659 0.3341076    
colourWhite:year2001 -0.3805413  0.3040538  -1.2516 0.2107305    
colourWhite:year2002 -0.6173178  0.4192551  -1.4724 0.1409086    
colourWhite:age      -0.0373729  0.0102003  -3.6639 0.0002484 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Given that we have <code>employed={no,yes}</code> and <code>citizen={no,yes}</code> and the factor <code>year={1997,..,2002}</code> in the model... </p>

<p>Does this imply that the coefficient <code>colourWhite:age = -0.0373729</code> is <em>strictly</em> limited to describing <em>only</em> the interaction between colour and age for people who are unemployed, non-citizen and arrested in 1997? </p>
"
"0.129279124076572","0.116351054666701","146919","<p>Please, be kind, as I'm totally noob in stats and R...</p>

<p>I'm the owner of a small restaurant in a commercial center, and I managedd to collect two main dataset, commercial-center (cc) and restaurant (rest).</p>

<p>cc <a href=""https://www.dropbox.com/s/7k6k1rjimrcfqlq/cc.csv?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/7k6k1rjimrcfqlq/cc.csv?dl=0</a> <br/>
DAY 10:00-12.00 12:00-14:00 14:00-16:00 16:00-18:00 18:00-20:00 20:00-22:00 SUB-TOTAL<br/>
01/01/2012  0   825,55  534,85  879,7   964,725 161,975 3366,8</p>

<p>till today</p>

<p>rest <a href=""https://www.dropbox.com/s/rtoqwg64tu4poxs/rest.csv?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/rtoqwg64tu4poxs/rest.csv?dl=0</a> <br/>
is a database of all the restaurant sales from 04 dec 2014<br/>
I have managed to make it in a similar fashion as the above format<br/>
It contains many NA values (periods we were closed)</p>

<p>Also, I gathered other data: <br/>
events <br/>
is a dataset containing holidays and other events (it has to be categorized, as some increase sales, some the opposite)</p>

<p>weather <br/>
a collection of calculated observation for a spot 2 km away from the restaurant, each observation (wind speed, temp, cloudness and rain) have been turned in a value on a scale of 4 values</p>

<p>My objectives are:</p>

<ol>
<li>understand 'rest' seasonalities (daily/weekly, yearly)</li>
<li>understand how weather and other independet variables (not fixed events i.e. Easter or concerts) modify 'rest' and find a coefficient to apply to the model </li>
<li>forecast next days, weeks, months sales movements</li>
<li>verificate the restaurant trend, net of seasonalities and other variables</li>
</ol>

<p>An SD up to 30-40% on model/observed is still a good point to me as long as error distribution is not well spread but with a solid peak on 0</p>

<p>After many testing and try, I have found that the best model to choose is TBATS (BATS is deadly slow to me), expecially for managing multiple seasonality (which is a main point in my study).</p>

<p>The method I was thinking was to:</p>

<ol>
<li>Find indipendent variables coefficient for 'rest' and 'cc' and apply them to the data</li>
<li>Load 'rest' and 'c'c, with modifications at point 1, in an msts object and throw them to TBATS</li>
<li>Find correlations or other kind of link between the seasonalities of 'rest' and 'cc', get coefficients and use them for modeling future 'rest' data</li>
</ol>

<p>Not sure whether I can post multiple questions in here, if not just answer to this please: am I choosing the proper work method, or is anything better out there?</p>

<p>I'm struggling myself with many questions:</p>

<p><ol>
<li>How can I properly describe the data I have in R for using in TBATS, with sampling every 2 hours, starting at 10 and finishing at 22? Should I create a model for each column?</li>
<li>TBATS is not allowing for independent variables (events), is it? How to manage them?</li>
<li>TBATS does not accept NA values? How to manage the closing periods or days (NA) in the 'rest' dataset? Some weeks we were closed on Mondays, some Tuesday, some others we were always opening; we have some afternoon openings.... big mess... Maybe use regressions..?</li>
<li>If I do:</p>

<blockquote>
  <p>export &lt;- data.frame(DAYS=date,tbats.components(cc-SUBTOTAL.msts.tbats),errors=resid(cc-SUBTOTAL.msts.tbats))</li>
  </ol>
  how should I interpretate data? <br/>
  i.e. If I do level+season1+season2+errors=cObserved, I get figures which are slightly different from the ""real"" observed: sd(cObserved/observed)=3% which is fine for my objectives... is it correct?</p>
</blockquote>

<ol start=""5"">
<li>In TBATS, is trend some kind of SMA? at which window?</li>
</ol>
"
"0.0545088647991304","0.0519436716578171","147083","<p>What is the baseline level for a factor-by-factor interaction term in multiple regression? </p>

<p>Consider this example from <a href=""http://www.jstatsoft.org/v08/i15"" rel=""nofollow"">Fox 2003</a>. In the regression below, these two variables are categorical: <code>year={1997,..,2002}</code> and <code>colour={black,white}</code>. </p>

<pre><code>require(effects)
require(lmtest)
Arrests$year &lt;- as.factor(Arrests$year)
arrests.mod &lt;- glm(released ~ employed + citizen + checks
                         + colour*year + colour*age,
                         family=binomial, data=Arrests)
</code></pre>

<p>Which yields: </p>

<pre><code>&gt; coeftest(arrests.mod)

z test of coefficients:

                       Estimate Std. Error  z value  Pr(&gt;|z|)    
(Intercept)           0.3444334  0.3100749   1.1108 0.2666514    
employedYes           0.7350645  0.0847701   8.6713 &lt; 2.2e-16 ***
citizenYes            0.5859841  0.1137717   5.1505 2.598e-07 ***
checks               -0.3666425  0.0260322 -14.0842 &lt; 2.2e-16 ***
colourWhite           1.2125167  0.3497751   3.4666 0.0005272 ***
year1998             -0.4311794  0.2603589  -1.6561 0.0977023 .  
year1999             -0.0944343  0.2615447  -0.3611 0.7180519    
year2000             -0.0108975  0.2592073  -0.0420 0.9664655    
year2001              0.2430630  0.2630151   0.9241 0.3554129    
year2002              0.2129549  0.3532786   0.6028 0.5466444    
age                   0.0287279  0.0086191   3.3330 0.0008590 ***
colourWhite:year1998  0.6519565  0.3134898   2.0797 0.0375555 *  
colourWhite:year1999  0.1559504  0.3070430   0.5079 0.6115161    
colourWhite:year2000  0.2957537  0.3062034   0.9659 0.3341076    
colourWhite:year2001 -0.3805413  0.3040538  -1.2516 0.2107305    
colourWhite:year2002 -0.6173178  0.4192551  -1.4724 0.1409086    
colourWhite:age      -0.0373729  0.0102003  -3.6639 0.0002484 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>In the table above, I interested in identifying the baseline level for the factor by factor interaction term... For instance, group <code>colourWhite:year1998</code> is compared to which other group? </p>

<p>Is <code>colourWhite:year1997</code> the baseline level, or perhaps <code>colourBlack:year1997</code>? </p>
"
"0.0812570180448007","0.0871121856208561","147119","<p>I have a plot of residual values of a linear model in function of the fitted values where the heteroscedasticity is very clear. However I'm not sure how I should proceed now because as far as I understand this heteroscedasticity makes my linear model invalid. (Is that right?)</p>

<ol>
<li><p>Use robust linear fitting using the <code>rlm()</code> function of the <code>MASS</code> package because it's apparently robust to heteroscedasticity. </p></li>
<li><p>As the standard errors of my coefficients are wrong because of the heteroscedasticity, I can just adjust the standard errors to be robust to the heteroscedasticity? Using the method posted on Stack Overflow here: <a href=""http://stackoverflow.com/questions/4385436/regression-with-heteroskedasticity-corrected-standard-errors"">Regression with Heteroskedasticity Corrected Standard Errors</a></p></li>
</ol>

<p>Which would be the best method to use to deal with my problem? If I use solution 2 is my predicting capability of my model completely useless?</p>

<p>The Breusch-Pagan test confirmed that the variance is not constant.</p>

<p>My residuals in function of the fitted values looks like this:  </p>

<p><a href=""http://i.stack.imgur.com/OtlGC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/OtlGC.png"" alt=""http://i.gyazo.com/9407a829a168492b31dfa3d1dd33a21d.png""></a> </p>

<p>(larger version)</p>

<p><img src=""http://i.gyazo.com/9407a829a168492b31dfa3d1dd33a21d.png"" width=""600""></p>
"
"0.0691025985081098","0.076825726438694","147170","<p>I have a plot of residual values of a linear model in function of the fitted values where the heteroscedasticity is very clear. However I'm not sure how I should proceed now because as far as I understand this heteroscedasticity makes my linear model invalid right?</p>

<p>So I have been doing some reading about this subjet and I found two suggestions in other stackoverflow threads.</p>

<p>1) Use robut linear fitting using the rlm() function of the MASS package because it's apparently robust to heteroscedasticity. </p>

<p>2) As the standard errors of my coefficients are wrong because of the heteroscedasticity, I can just adjust the standard errors to be robust to the heteroskedasticity? Using the method posted <a href=""http://stackoverflow.com/questions/4385436/regression-with-heteroskedasticity-corrected-standard-errors"">here</a></p>

<p>Which would be the best method to use to deal with my problem?
If I use solution 2 is my predicting capability of my model completely useless?</p>

<p>My residuals in function of the fitted values looks like this <a href=""http://i.gyazo.com/9407a829a168492b31dfa3d1dd33a21d.png"" rel=""nofollow"">http://i.gyazo.com/9407a829a168492b31dfa3d1dd33a21d.png</a> and the Breusch-Pagan test confirmed that the variance is not constant.</p>
"
"0.0430930413588572","0.0410650781176591","147417","<p>I ran the following regression using R. </p>

<pre><code>libor &lt;- ts(diff(Libor))
ois &lt;- ts(diff(OIS))
x &lt;- ts(diff(Repo-OIS))
vix &lt;- ts(diff(VIX))
cds &lt;- ts(diff(CDS))
treasury &lt;- ts(diff(log(P_treasury)))
mbs &lt;- ts(diff(log(P_MBS)))
rrp &lt;- ts(diff(log(RRP)))
axx &lt;- ts.intersect(mbs, treasury, rrp, cds, libor, ois, x, vix)
reg3 &lt;- lm(libor~ois+x+vix+cds+treasury+mbs+rrp, data=axx, subset=418:521)
</code></pre>

<p>which gave me the following output:</p>

<pre><code>t test of coefficients:

               Estimate  Std. Error t value Pr(&gt;|t|)  
(Intercept)  8.6279e-04  8.0537e-04  1.0713  0.28675  
ois          1.1427e-01  5.5089e-02  2.0742  0.04076 *
x           -1.0914e-02  2.0758e-02 -0.5258  0.60028  
vix         -1.3155e-04  1.8298e-04 -0.7190  0.47394  
cds          7.9692e-05  1.0590e-04  0.7525  0.45358  
treasury    -3.3914e-01  1.9171e-01 -1.7690  0.08010 .
mbs         -4.0022e-03  1.3883e-02 -0.2883  0.77376  
rrp          1.6299e-05  1.5772e-04  0.1033  0.91791
</code></pre>

<p>While the variables <code>libor, ois, x ,vix</code> are in percent the variables <code>treasury, mbs and rrp</code> are in log and in million of USD. So it is a lin- log model. The variable treasury increased during the analysed time period by 47% (from 1671382 mio USD to 2461389 mio USD.). Now I want to calculate the impact of the variable <code>treasury</code> on my dependent variable (<code>libor</code>). following my book this is done: Î”y=(Î²1/100)%Î”x  --> = (-0.33914/100)*47=0.1593958. So I can say, that if the variable treasury is increased by 47% my dependent variable will decline by 0.1593958% or 15.94 basispoints. Is this interpretation correct?</p>

<p>My question arises because in a working paper they calculated (B1*100*47). I couldn't figure out why they multiply their B1 with 100 instead of divide it by 100. Any ideas? Many thanks!</p>
"
"0.068136080998913","0.0649295895722714","147554","<p>I am currently stuck with a problem regarding predictions from linear regressions. I estimated a simple (multivariate) regression model y = b0 + b1 * x + b2 * X, where x is my variable of interest and X is a matrix of controls, using the <code>lm()</code> fct in R.</p>

<p>Now I want to predict y for two different values of x. Finally, I want to know whether those two predictions of y are statistically different from each other. </p>

<p>So far, I used <code>predict(model, se.fit = TRUE, interval = ""prediction"")</code> and got a point prediction as well as the corresponding prediction interval. Using the prediction intervals of the two points, I decided whether there are statistically different based on the overlapping of the prediction intervals.</p>

<p>I got almost no significant differences using this technique even when the estimated coefficients are significant. Is this the right track, or are there different techniques one can use?</p>

<p>Thanks for your help!</p>
"
"0.052777981396926","0.0502942438178979","147843","<p>Take this example output.</p>

<pre><code>Residuals:
     Min       1Q   Median       3Q      Max 
-2.35775 -0.49911  0.07299  0.58762  2.54753 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 26.31154    0.44184  59.550  &lt; 2e-16 ***
x        2.52228    0.28907   8.726 4.95e-13 ***
I(y^2)  -0.52882    0.04042 -13.082  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.8906 on 75 degrees of freedom
Multiple R-squared:  0.8838,    Adjusted R-squared:  0.8807 
F-statistic: 285.1 on 2 and 75 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>I know how to change the standard error for the coefficients for a linear model to be robust  to heteroscedasticity using coeftest(regression_result, df = Inf, var_cov)</p>

<p>But what about the residual standard error of the model that you see when you call summary(lm()) at the end of the output, don't I need to compensate that as well to be robust to heteroscedasticity? How do I do that in R?</p>
"
"NaN","NaN","147976","<p>I have a dataset with the dependent variable presence / absence (0 and 1) for a certain species. I have three categorised IV's (2 IV's with 3 categories and 1 with 2 categories). To test the response of the presence / absence to the 3 IV's, I have to perform a binomial regression, right?</p>

<p>I can run a model in R which looks like this:</p>

<pre><code>glm(species.presenceabsence~TREATMENT+WEEK+TYPE, data=speciesdata, family=binomial))
</code></pre>

<p>The outcome looks like this:</p>

<pre><code>Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)    -19.4779  2955.2811  -0.007    0.995
TREATMENTHigh  -19.1984  2955.2812  -0.006    0.995
TREATMENTLow    -0.9217     0.8101  -1.138    0.255
WEEK10          18.2767  2955.2812   0.006    0.995
WEEK15          19.1984  2955.2812   0.006    0.995
TYPEF           -0.3083     0.7881  -0.391    0.696
</code></pre>

<p>What I don't understand is why R doesn't give the coefficients of all the IV's. How do I interpret these data? I'm a bit surprised that none of the coefficients has a significant effect.</p>
"
"0.0691025985081098","0.0548755188847815","148007","<p>Following the explanations in <a href=""http://stats.stackexchange.com/questions/147083/what-is-the-baseline-level-in-a-factor-by-factor-interaction/147088#147088"">What is the baseline level in a factor-by-factor interaction?</a>, it is my understanding that a factor-by-factor interaction term has no literal interpretation. At the very least, it has no clear, straightforward interpretation... </p>

<p>Consider this example from <a href=""http://www.jstatsoft.org/v08/i15"" rel=""nofollow"">Fox 2003</a>. In the regression below, these two variables are categorical: <code>year={1997,..,2002}</code> and <code>colour={black,white}</code>. </p>

<pre><code>require(effects)
require(lmtest)
Arrests$year &lt;- as.factor(Arrests$year)
arrests.mod &lt;- glm(released ~ employed + citizen + checks
                         + colour*year + colour*age,
                         family=binomial, data=Arrests)
</code></pre>

<p>Which yields: </p>

<pre><code>&gt; coeftest(arrests.mod)

z test of coefficients:

                       Estimate Std. Error  z value  Pr(&gt;|z|)    
(Intercept)           0.3444334  0.3100749   1.1108 0.2666514    
employedYes           0.7350645  0.0847701   8.6713 &lt; 2.2e-16 ***
citizenYes            0.5859841  0.1137717   5.1505 2.598e-07 ***
checks               -0.3666425  0.0260322 -14.0842 &lt; 2.2e-16 ***
colourWhite           1.2125167  0.3497751   3.4666 0.0005272 ***
year1998             -0.4311794  0.2603589  -1.6561 0.0977023 .  
year1999             -0.0944343  0.2615447  -0.3611 0.7180519    
year2000             -0.0108975  0.2592073  -0.0420 0.9664655    
year2001              0.2430630  0.2630151   0.9241 0.3554129    
year2002              0.2129549  0.3532786   0.6028 0.5466444    
age                   0.0287279  0.0086191   3.3330 0.0008590 ***
colourWhite:year1998  0.6519565  0.3134898   2.0797 0.0375555 *  
colourWhite:year1999  0.1559504  0.3070430   0.5079 0.6115161    
colourWhite:year2000  0.2957537  0.3062034   0.9659 0.3341076    
colourWhite:year2001 -0.3805413  0.3040538  -1.2516 0.2107305    
colourWhite:year2002 -0.6173178  0.4192551  -1.4724 0.1409086    
colourWhite:age      -0.0373729  0.0102003  -3.6639 0.0002484 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>In the table above, how would one interpret the coefficient for e.g. <code>colourWhite:year1998</code> (significant at 5%)?  </p>

<p>Since the baseline level is <code>colourBlack:year1997</code> (or the <code>Intercept</code>), the level for Blacks in 1998 would be computed as follows: </p>

<pre><code>Intercept + year1998
</code></pre>

<p>Whereas the level for Whites in 1998 would be: </p>

<pre><code>Intercept + year1998 + colourWhite + colourWhite:year1998
</code></pre>

<p>Thus it seems to me that the coefficient for <code>colourWhite:year1998</code> doesn't stand for much, really. At least it doesn't look like having any intuitive, straightforward interpretation. Does it? </p>
"
"0.111849830561289","0.119909435959664","148913","<p>I am new in R and itâ€™s my first time using it so Iâ€™ll appreciate the help. I am estimating income elasticity for electricity consumption using budget shares. I have data for 8 regions categorized into 5 classes depending on the household size (from A to E). I am regressing budget shares (wi) on log of income(lxi), classes dummy of household size (D) and the interaction between log of income and household class size (lxi*D):</p>

<blockquote>
  <p>Wi = C + lxi + D + lxi*D + u</p>
</blockquote>

<p>I have an unbalanced panel data for 2067 observations saved in .csv format. I attached my data, transformed the Date from factor, made a new data frame including the new date, and finally set the data as a panel data as the code below:</p>

<blockquote>
  <p>mydata&lt;-read.csv(""C:/Users/Fadhila/Desktop/Remeasuring 2015/DataClass-unbalanced.csv"", header=T)</p>
  
  <p>attach(mydata) </p>
  
  <p>date&lt;- as.Date(factor(Date),format= ""%m/%d/%Y"")</p>
  
  <p>ndata=data.frame(Class,date,lxi,wi)</p>
  
  <p>ndata&lt;-plm.data(ndata, index=c(""Class"", ""date""))</p>
</blockquote>

<p>I have regressed my model before using Pooled OLS, Fixed (twoways), and Random and both Hausman test and F-test suggested using Random which I doubt so I thought to test for heteroskedasticity and outliers. So I plotted the below model , plotted the leverage versus the residual and compared them with cooks distance:</p>

<blockquote>
  <p>r &lt;- lm (wi~lxi + Class:lxi, data=ndata)</p>
  
  <p>summary(r)</p>
  
  <p>par(mfrow=c(2,2))</p>
  
  <p>plot (r)</p>
  
  <p>windows()</p>
  
  <p>with(ndata, plot(lxi, cooks.distance(r)))</p>
  
  <p>identify(ndata$lxi, cooks.distance(r)) </p>
</blockquote>

<p><img src=""http://i.stack.imgur.com/1imMP.jpg"" alt=""residuals plot ""></p>

<p><img src=""http://i.stack.imgur.com/PdTBk.jpg"" alt=""residual versus cooks distance""></p>

<p>Than to estimate how many points are far from the leverage points, but it seems that am doing something wrong as I got all the points to be twice greater than the leverage.</p>

<blockquote>
  <p>lev = hatvalues(r)</p>
  
  <p>lev</p>
  
  <p>4/2067</p>
  
  <p>lev[lev>2*4/2067]</p>
</blockquote>

<p>However, I plotted the below to see </p>

<blockquote>
  <p>plot(ndata$lxi, rstandard(r))</p>
  
  <p>identify(ndata$lxi,rstandard(r))</p>
  
  <p>plot(ndata$lxi, lev)</p>
  
  <p>identify(ndata$lxi, lev)</p>
  
  <p>windows()</p>
  
  <p>plot(ndata$lxi, ndata$Class)</p>
  
  <p>identify(ndata$lxi, ndata$Class)</p>
</blockquote>

<p><img src=""http://i.stack.imgur.com/gdERl.jpg"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/JPyuU.jpg"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/ULWNr.jpg"" alt=""enter image description here""></p>

<p>To double check and see which points influence the results I made the below:</p>

<blockquote>
  <p>outs &lt;- influencePlot(r)</p>
  
  <p>n &lt;-2</p>
  
  <p>Cooksdist &lt;- as.numeric(tail(row.names(outs[order(outs$CookD), ]), n))</p>
  
  <p>Levr &lt;- as.numeric(tail(row.names(outs[order(outs$Hat), ]), n))</p>
  
  <p>StdRes &lt;- as.numeric(tail(row.names(outs[order(outs$StudRes), ]), n))</p>
  
  <p>plot(ndata$lxi, ndata$wi)</p>
  
  <p>abline(r, col = ""blue"")</p>
  
  <p>Warning message:
  In abline(r, col = ""blue"") :
    only using the first two of 41 regression coefficients*</p>
  
  <p>points(ndata$lxi[Cooksdist], ndata$wi[Cooksdist], col = ""red"", pch = 0, lwd = 15)</p>
  
  <p>points(ndata$lxi[Levr], ndata$wi[Levr], col = ""blue"", pch = 25, lwd = 8)</p>
  
  <p>points(ndata$lxi[StdRes], ndata$wi[StdRes], col = ""green"", pch = 20, lwd = 5)</p>
  
  <p>text(ndata$lxi[as.numeric(row.names(outs))], 
         ndata$wi[as.numeric(row.names(outs))], 
       labels = round(ndata$wi[as.numeric(row.names(outs))], 3),
       pos = 1)</p>
  
  <p>identify(ndata$lxi, ndata$wi)</p>
</blockquote>

<p>and got the below, but not sure how I identify the points:
<img src=""http://i.stack.imgur.com/QJmoI.jpg"" alt=""enter image description here""></p>

<p>my questions are:</p>

<ol>
<li><p>How to identify to deal with the outliers! After correcting for the leverage point error (all points far from leverage point)</p></li>
<li><p>Do I need to use â€œplmâ€ or itâ€™s ok since I identified my data as panel?</p></li>
<li><p>How to do two way tests for Random?</p></li>
<li><p>Testing for heteroskedasticity by:</p></li>
</ol>

<blockquote>
  <p>p&lt;-plm(wi~lxi + Class+Class:lxi, data=ndata1)</p>
  
  <p>summary(p)</p>
  
  <p>bptest(p)</p>
</blockquote>

<p>the null was rejected(Null: homoskedastic). Does this solve the heteroskedasticity problem:</p>

<blockquote>
  <p>vcovHC(p, omega = Null, type=""HC4"")</p>
  
  <p>coeftest(p, df=Inf, vcov=vcovHC(p, type=""HC4""))</p>
</blockquote>

<p>or I need to remove the insignificant points!</p>
"
"0.0963589698356145","0.0826418755551823","149012","<p><a href=""http://en.wikipedia.org/wiki/Discrete_choice#F._Logit_with_variables_that_vary_over_alternatives_.28also_called_conditional_logit.29"" rel=""nofollow"">Conditional logistic regression</a> is a <a href=""http://en.wikipedia.org/wiki/Fixed_effects_model"" rel=""nofollow"">fixed effects model</a>. If you're modeling the dependent variable $y$, a glm fixed effect model doesn't actually model $y$. Instead, the glm fixed effect models measure $y-mean(y)$ for a particular group. I think that this is <em>not</em> the case for a conditional logistic regression. The coefficients of the regression can be interpreted in the space of $y$. Is that correct?</p>

<p>My particular situation:
I am running a conditional logit with <a href=""https://stat.ethz.ch/R-manual/R-devel/library/survival/html/clogit.html"" rel=""nofollow"">clogit</a> in R, from the <code>survival</code> package. Are the coefficients returned to be interpreted in the space of $y$, or in the space of something like $y-mean(y)$? </p>

<p>Normally the difference isn't very relevant; one would interpret the coefficient roughly the same either way. However, in my case one of the independent variables is fitted as a spline. Specifically, it is a restricted cubic spline, as calculated from <code>rcspline.eval</code> in the <a href=""http://cran.r-project.org/web/packages/Hmisc/Hmisc.pdf"" rel=""nofollow"">Hmisc</a> package. <code>clogit</code> produces a coefficient for each knot of the spline, and in order to interpret the overall effect of the variable one needs to reconstruct the spline from the coefficients (using <code>rcspline.restate</code>). I want to make sure that I should be looking at the shape of this spline in the range of $y$ (which in my case is 0-100) or in the range of something like $y-mean(y)$ (in this case, $mean(y)$ is the same for all groups: 50). If it is the case that the space is shifted this will be particularly weird for a spline, because presumably the knots should also be shifted somehow.</p>
"
"0.0845124072264899","0.096642292914995","149064","<p>I have a nominal categorical predictor and a continuous dependent variable..I want to perform linear regression using lm in R. If the contrasts are such that the resulting dummy variables are uncorrelated then the regression is merely the direct linear combination of dummy variables weighted by their respective coefficients obtained from regression  of continuous variable with individual dummy variable..To have this advantage what way should the categorical predictor be contrast coded?I found this method <a href=""http://www.psychstat.missouristate.edu/multibook/mlt08m.html"" rel=""nofollow"">here</a> ..
It is helpful but the only problem is the order seems to be important here..The relation between only adjacent categories can  be interpreted from the result of linear regression..</p>

<p>So my question is - for nominal categorical predictor is there anyway to get good insights about dependent variable at category level of the predictor  from regression analysis.</p>

<p><strong>edit</strong> :</p>

<p>I'd like to  provide some clarifications here</p>

<p>Why do i need uncorrelated dummies? </p>

<p>bcoz in case of uncorrelated dummies i need not worry about which dummy enters the regression model first. The p value for the dummy1 is different when it enters the model second when compared to that when it enters first..By 'enters the model' i mean stepwise linear regression..So to avoid that problems i want them to be uncorrelated.</p>

<p>But if you see the pain vs treatment regression from the link provided by me the order certainly matters while doing contrast coding..I have no prior knowledge of the categories of my nominal category variable..so i cant order them like in pain vs treatment case. For more details - my dependent variable is Sales and category variable is product category which has 15 categories.</p>
"
"0.068136080998913","0.0649295895722714","149322","<p>I am trying to use multiple regression for a time series dataset. I have values corresponding to a variable measured by 24 hrs for 4 months. Since there was a pattern which repeated every 24 hours I used 23 dummy variables for the hourly variations in values.</p>

<p>I used log transformation of the dependent variable before performing multiple regression. The fitted coefficients were highly significant and the R-squared was around 0.99.
However, when I look at the Residuals vs fitted plot, it seems sort of weird. According to the plots <a href=""http://www.r-bloggers.com/model-validation-interpreting-residual-plots/"" rel=""nofollow"">here</a>, my plot is neither biased nor heteroskedastic, but it also doesn't look like random noise. Can someone help me find the issue here? 
<img src=""http://i.stack.imgur.com/sCGPx.png"" alt=""enter image description here"">
Also please find below a plot of the observed and fitted model for first 500 hrs<img src=""http://i.stack.imgur.com/jiTts.png"" alt=""Observed Values VS Time in hours overlaid by fitted model in red""></p>
"
"0.052777981396926","0.0335294958785986","151463","<p>Generally, coeficients and their p values are focussed upon while assessing the regression output. However, there are other things mentioned. How can we analyze the output of glm without the coefficients: </p>

<pre><code>&gt; summary(mod)

Call:
glm(formula = outvar ~ ., family = binomial, data = mydf)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.3537  -0.8172  -0.6462   1.2131   2.2757  

Coefficients:
....
....


(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 8973.8  on 7760  degrees of freedom
Residual deviance: 8526.6  on 7752  degrees of freedom
AIC: 8544.6

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Does above part of glm output can be used to comment anything about the regression performed without coefficients being available?</p>
"
"0.190293718142101","0.172039065974523","151657","<p>I am running X-13 SEATS on r for monthly data in six years of observations and I think I got a (sufficiently) reasonable fit for the ARIMA model, but the output also shows me that my original series does not have significant seasonality, as it follows:</p>

<pre><code> Call:
seas(x = data_r[, 1], transform.function = ""log"", regression.aictest = NULL, 
    outlier = NULL, arima.model = ""(0 1 1)(1 1 0)"")

Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
AR-Seasonal-12     -0.6194     0.1110  -5.581 2.39e-08 ***
MA-Nonseasonal-01   0.6220     0.1093   5.690 1.27e-08 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

SEATS adj.  ARIMA: (0 1 1)(1 1 0)  Obs.: 60  Transform: log
AICc: 773.4, BIC: 778.4  QS (no seasonality in final):    0  
Box-Ljung (no autocorr.): 20.04   Shapiro (normality): 0.9754
    &gt; 
                qs p-val
    qsori        0     1
    qsorievadj   0     1
    qsrsd        0     1
    qssadj       0     1
    qssadjevadj  0     1
    qsirr        0     1
    qsirrevadj   0     1
</code></pre>

<p>(Still, there is also the fact that the irregular component seems to dominate the SI ratio for some specific months in some years. So maybe there is some dummy variable in the pre-adjustment that I am missing (right?)) </p>

<p>But when I run a regression on Stata for yearly and monthly dummies on the original series -- assuming the seasonality is deterministic --, I cannot reject with an F test that they are all equal to zero. What does this show me? That my ARIMA fit is not correct?</p>

<p>Also, if someone could point me out the difference in interpretation that you should have when running a regression on seasonal dummies and deseasonalizing data with a X-13 SEATS, it would be also very helpful. Maybe that is what I am missing here.</p>

<p>Edit: is it by any chance a common practice, in some particular situations (when you are deseasonalizing a set of series), still deseasonalize a given series even if that series does not show significant seasonality?</p>

<p>Edit2: Adding the results of the automatic adjustment:</p>

<pre><code>Coefficients:
                   Estimate Std. Error z value Pr(&gt;|z|)    
Constant            59.1761    38.0551   1.555  0.11994    
Easter[15]        -903.6151   341.1891  -2.648  0.00809 ** 
MA-Nonseasonal-01    0.4974     0.1138   4.370 1.24e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

SEATS adj.  ARIMA: (0 1 1)  Obs.: 60  Transform: none
AICc: 925.6, BIC: 933.2  QS (no seasonality in final):    0  
Box-Ljung (no autocorr.):  21.9   Shapiro (normality): 0.9498 *

            qs p-val
qsori        0     1
qsorievadj   0     1
qsrsd        0     1
qssadj       0     1
qssadjevadj  0     1
qsirr        0     1
qsirrevadj   0     1 
</code></pre>

<p>I also, I get the following error for the monthplot function with the automatic adjustment: </p>

<pre><code>Error in `[.default`(x$data, , ""seasonal"") : subscript out of bounds
</code></pre>

<p>Following this result from the automatic adjustment, the use of the dummy for easter, with the original specification, does not change that much the first output:</p>

<pre><code>Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
Easter[15]        -0.08307    0.02690  -3.088  0.00202 ** 
AR-Seasonal-12    -0.63353    0.10816  -5.858  4.7e-09 ***
MA-Nonseasonal-01  0.50391    0.12075   4.173  3.0e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

SEATS adj.  ARIMA: (0 1 1)(1 1 0)  Obs.: 60  Transform: log
AICc: 767.9, BIC: 774.3  QS (no seasonality in final):    0  
Box-Ljung (no autocorr.): 29.37   Shapiro (normality): 0.9721  
            qs p-val
qsori        0     1
qsorievadj   0     1
qsrsd        0     1
qssadj       0     1
qssadjevadj  0     1
qsirr        0     1
qsirrevadj   0     1
</code></pre>

<p>Most recent observation: Now I Think I am fairly sure that there is no significant seasonality in this series, but I would be thankful if someone could show me other problems that I might not be considering. Still, I would like a possible canonical/scholarly answer on why I can reject the null hypothesis for the whole set of seasonal dummies being zero (though I had a small result for the F test with my data, ~4, but I still reject the null) and still get a reasonable ARIMA fit with which I cannot reject no seasonality in my original data. Does that have something to do with the difference of the adjustment with ARIMA models and deterministic seasonality? An intuitive answer on this difference would be of some help.</p>
"
"0.146262632480641","0.127764538910589","152958","<p>I have following simple X and Y vectors: </p>

<pre><code>&gt; X
[1] 1.000 0.063 0.031 0.012 0.005 0.000
&gt; Y
[1] 1.000 1.000 1.000 0.961 0.884 0.000
&gt; 
&gt; plot(X,Y)
</code></pre>

<p><img src=""http://i.stack.imgur.com/xTk8u.png"" alt=""enter image description here""></p>

<p>I want to do regression using log of X. To avoid getting log(0), I try to put +1 or +0.1 or +0.00001 or +0.000000000000001 : </p>

<pre><code>&gt; summary(lm(Y~log(X)))
Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : 
  NA/NaN/Inf in 'x'
&gt; summary(lm(Y~log(1+X)))

Call:
lm(formula = Y ~ log(1 + X))

Residuals:
       1        2        3        4        5        6 
-0.03429  0.22189  0.23428  0.20282  0.12864 -0.75334 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)   0.7533     0.1976   3.812   0.0189 *
log(1 + X)    0.4053     0.6949   0.583   0.5910  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.4273 on 4 degrees of freedom
Multiple R-squared:  0.07838,   Adjusted R-squared:  -0.152 
F-statistic: 0.3402 on 1 and 4 DF,  p-value: 0.591

&gt; summary(lm(Y~log(0.1+X)))

Call:
lm(formula = Y ~ log(0.1 + X))

Residuals:
       1        2        3        4        5        6 
-0.08099  0.20207  0.23447  0.21870  0.15126 -0.72550 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)    1.0669     0.3941   2.707   0.0537 .
log(0.1 + X)   0.1482     0.2030   0.730   0.5058  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.4182 on 4 degrees of freedom
Multiple R-squared:  0.1176,    Adjusted R-squared:  -0.103 
F-statistic: 0.5331 on 1 and 4 DF,  p-value: 0.5058

&gt; summary(lm(Y~log(0.00001+X)))

Call:
lm(formula = Y ~ log(1e-05 + X))

Residuals:
       1        2        3        4        5        6 
-0.24072  0.02087  0.08796  0.13872  0.14445 -0.15128 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)     1.24072    0.12046  10.300 0.000501 ***
log(1e-05 + X)  0.09463    0.02087   4.534 0.010547 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.1797 on 4 degrees of freedom
Multiple R-squared:  0.8371,    Adjusted R-squared:  0.7964 
F-statistic: 20.56 on 1 and 4 DF,  p-value: 0.01055

&gt; 
&gt; summary(lm(Y~log(0.000000000000001+X)))

Call:
lm(formula = Y ~ log(1e-15 + X))

Residuals:
        1         2         3         4         5         6 
-0.065506  0.019244  0.040983  0.031077 -0.019085 -0.006714 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)     1.06551    0.02202   48.38 1.09e-06 ***
log(1e-15 + X)  0.03066    0.00152   20.17 3.57e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.04392 on 4 degrees of freedom
Multiple R-squared:  0.9903,    Adjusted R-squared:  0.9878 
F-statistic: 406.9 on 1 and 4 DF,  p-value: 3.565e-05
</code></pre>

<p>The output is different in all cases. What is the correct value to put to avoid log(0) in regression? What is the correct method for such situations.</p>

<p>Edit: my main aim is to improve prediction of the regression model by adding log term, i.e.: lm(Y ~ X + log(X))</p>
"
"NaN","NaN","153480","<p>I need to implement a SAR model with no covariates. To be more specific, the regression I have to estimate is y=bWy+e where: </p>

<ul>
<li>y is the dependent variable;</li>
<li>b is the coefficient parameter to be estimated;</li>
<li>W is the adjacency matrix;</li>
<li>e is the error.</li>
</ul>

<p>My idea was to use the lagsarlm function of the spdep package. But I've gone through spdep documentation and it seems that this function works only adding covariates: i.e. y=bWy+cX+e, and I don't know how to erase the X term.</p>

<p>Note: For those who are acquainted with network analysis literature and not with spatial econometrics, in a way this is a method to estimate a parameter for bonachich centrality.</p>
"
"NaN","NaN","154485","<p>As a prequel to a question about linear-mixed models in R, and to share as a reference for beginner/intermediate statistics aficionados, I decided to post as an independent ""Q&amp;A-style"" the steps involved in the ""manual"" computation of the coefficients and predicted values of a simple linear regression.</p>

<p>The example is with the R in-built dataset, <code>mtcars</code>,  and would be set up as miles per gallon consumed by a vehicle acting as the independent variable, regressed over the weight of the car (continuous variable), and the number of cylinders as a factor with three levels (4, 6 or 8) without interactions.</p>

<p>EDIT: If you are interested in this question, you will definitely find a detailed and satisfactory answer in this <a href=""http://madrury.github.io/jekyll/update/2016/07/20/lm-in-R.html"">post by Matthew Drury outside CV</a>.</p>
"
"0.161239396518923","0.137188797211954","154669","<p>I'm working on a small project where I need to create a multivariate linear regression model to predict the frequency of some airline companies. I'm a bit confused as I don't know if I have to remove the intercept because its <code>Pr(&gt;|t|)</code> had, after removing the first variable <code>dist</code>, the biggest value among the other values. Here is what I get after removing <code>dist</code>: </p>

<pre><code>flights_lm = lm(freq~dist+capa+nbrt+depf+lcco+prbi)
summary(flights_lm)
##################################################################
# &gt; summary(flights_lm)
# 
# Call:
#   lm(formula = freq ~ dist + capa + nbrt + depf + lcco + prbi)
# 
# Residuals:
#   Min      1Q  Median      3Q     Max 
# -204884  -12347    1145   12382  297908 
# 
# Coefficients:
#               Estimate  Std. Erro  t value Pr(&gt;|t|)    
# (Intercept)  1.857e+04  1.487e+04   1.248  0.21437    
# dist        -5.145e+00  6.729e+00  -0.765  0.44610    
# capa        -7.928e+01  6.540e+01  -1.212  0.22784    
# nbrt         7.665e+01  7.188e+00  10.663  &lt; 2e-16 ***
# depf         3.408e-05  1.204e-05   2.832  0.00546 ** 
# lcco         3.531e+04  2.151e+04   1.642  0.10339    
# prbi         4.084e+00  2.017e+01   0.203  0.83988    
# ---
#   Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1                                      
#                                                                                                     
# Residual standard error: 60280 on 116 degrees of freedom                                            
# Multiple R-squared:  0.8719,  Adjusted R-squared:  0.8653                                           
# F-statistic: 131.6 on 6 and 116 DF,  p-value: &lt; 2.2e-16                                             
#####################################################################

flights_lm2 = update(flights_lm, .~. -prbi)
summary(flights_lm2)
####################################################################
# Call:
#   lm(formula = freq ~ dist + capa + nbrt + depf + lcco)
# 
# Residuals:
#   Min      1Q  Median      3Q     Max 
# -204913  -12471    1098   12201  297917 
# 
# Coefficients:
#               Estimate Std. Error t value Pr(&gt;|t|)    
# (Intercept)  1.918e+04  1.450e+04   1.323  0.18854    
# dist        -5.132e+00  6.701e+00  -0.766  0.44528    
# capa        -7.813e+01  6.488e+01  -1.204  0.23093    
# nbrt         7.665e+01  7.158e+00  10.708  &lt; 2e-16 ***
# depf         3.406e-05  1.199e-05   2.842  0.00529 ** 
# lcco         3.506e+04  2.138e+04   1.639  0.10382    
# ---
#   Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
# 
# Residual standard error: 60030 on 117 degrees of freedom
# Multiple R-squared:  0.8719,  Adjusted R-squared:  0.8664 
# F-statistic: 159.3 on 5 and 117 DF,  p-value: &lt; 2.2e-16
#####################################################################

flights_lm3 = update(flights_lm2, .~. -dist)
summary(flights_lm3)
#####################################################################
# Call:
#   lm(formula = freq ~ capa + nbrt + depf + lcco)
# 
# Residuals:
#   Min      1Q  Median      3Q     Max 
# -206975  -12147    4077   11489  297630 
# 
# Coefficients:
#               Estimate Std. Error t value Pr(&gt;|t|)    
# (Intercept)  1.526e+04  1.355e+04   1.127  0.26212    
# capa        -9.031e+01  6.279e+01  -1.438  0.15303    
# nbrt         7.705e+01  7.127e+00  10.811  &lt; 2e-16 ***
# depf         3.302e-05  1.189e-05   2.778  0.00637 ** 
# lcco         3.329e+04  2.122e+04   1.569  0.11939    
# ---
#   Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
# 
# Residual standard error: 59920 on 118 degrees of freedom
# Multiple R-squared:  0.8713,  Adjusted R-squared:  0.8669 
# F-statistic: 199.6 on 4 and 118 DF,  p-value: &lt; 2.2e-16
################################################################
</code></pre>
"
"0.125636725583038","0.112681644735122","154782","<p>I'm attempting logistic regression in R for a survey for 613 students. I'm looking to see if there is an association between my <strong>Dependent Variable</strong> (called 'BinaryShelter', coded as 0 or 1, signifying whether students took shelter during a tornado warning) and my <strong>5 independent/predictor variables</strong>. My categorical IV's have anywhere from 3 to 11 distinct levels/categories within them. The other two IV's are binary coded as 0 or 1. The first 10 surveys and R output are given below: </p>

<pre><code>    Survey  KSCat   WSCat   PlanHome    PlanWork    KLNKVulCat  BinaryShelter
    1       J       B       1           1           A           1
    2       A       B       1           0           NA          1
    3       B       B       1           1           C           1
    4       B       D       1           1           A           0
    5       B       D       1           1           A           1
    6       G       E       1           1           A           0
    7       A       A       1           1           B           1
    8       C       F       NA          1           C           0
    9       B       B       1           1           A           1
    10      C       B       0           0           NA          1



Call:
glm(formula = BinaryShelter ~ KSCat + WSCat + PlanHome + PlanWork + 
KLNKVulCat, family = binomial(""logit""), data = mydata)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.0583  -1.3564   0.7654   0.8475   1.6161  

Coefficients:
              Estimate   St. Error  z val   Pr(&gt;|z|)  
(Intercept)    0.98471    0.43416   2.268   0.0233 *
KSCatB        -0.63288    0.34599  -1.829   0.0674 .
KSCatC        -0.14549    0.27880  -0.522   0.6018  
KSCatD         0.59855    1.12845   0.530   0.5958  
KSCatE        15.02995 1028.08167   0.015   0.9883  
KSCatF         0.61015    0.68399   0.892   0.3724  
KSCatG        -1.60723    1.54174  -1.042   0.2972  
KSCatH        -1.57777    1.26621  -1.246   0.2127  
KSCatI        -2.06763    1.18469  -1.745   0.0809 .
KSCatJ        -0.23560    0.65723  -0.358   0.7200  
WSCatB        -0.30231    0.28752  -1.051   0.2931  
WSCatC        -0.49467    1.26400  -0.391   0.6955  
WSCatD         0.52501    0.71082   0.739   0.4601  
WSCatE        -0.32153    0.63091  -0.510   0.6103  
WSCatF        -0.51699    0.74680  -0.692   0.4888  
WSCatG        -0.64820    0.39537  -1.639   0.1011  
WSCatH        -0.05866    0.89820  -0.065   0.9479  
WSCatI       -17.07156 1455.39758  -0.012   0.9906  
WSCatJ       -16.31078  662.38939  -0.025   0.9804  
PlanHome       0.27095    0.28121   0.964   0.3353  
PlanWork       0.24983    0.24190   1.033   0.3017  
KLNKVulCatB    0.17280    0.42353   0.408   0.6833  
KLNKVulCatC   -0.12551    0.24777  -0.507   0.6125  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 534.16  on 432  degrees of freedom
Residual deviance: 502.31  on 410  degrees of freedom
  (180 observations deleted due to missingness)
AIC: 548.31

Number of Fisher Scoring iterations: 14

&gt; Anova(ShelterYorN, Test = ""LR"")
Analysis of Deviance Table (Type II tests)

Response: BinaryShelter
          LR Chisq Df Pr(&gt;Chisq)
KSCat       13.3351  9     0.1480
WSCat       14.3789  9     0.1095
PlanHome     0.9160  1     0.3385
PlanWork     1.0583  1     0.3036
KLNKVulCat   0.7145  2     0.6996
</code></pre>

<p>My questions are:</p>

<p><strong>1)</strong> Does a very large St. Deviation (like the one for KSCatE) indicate that I should not use that level of that categorical IV if I want the model to fit the data better? The ones that had such large St. Deviations were from small groups. Should I not include data from very small groups? For instance if only 2 or 3 people picked category 'E' for KSCat, should I exclude that data?</p>

<p><strong>2)</strong> When using factors for my categorical data, or when adding in more than one IV, sometimes my beta coefficients flip signs. Does this mean I should test for interaction and then try to conduct some form of a PCA or jump straight to doing a PCA?</p>

<p>These next questions may be better asked on stack overflow, but I figured I'd give it a shot here:</p>

<p><strong>3)</strong> I do not want a particular level of the categorical variables to be the reference level. I know that R automatically picks the reference level (A if letters, and the first one if numbers). As in the answer to this question (<a href=""http://stats.stackexchange.com/questions/60817/significance-of-categorical-predictor-in-logistic-regression"">Significance of categorical predictor in logistic regression</a>), I tried fitting the model without an intercept by adding - 1 to the formula to see all coefficients directly. But when I do this, the results only show the 'A' level of the first variable and none of the others. For example, I can see results for 'KSCatA' but not 'WSCatA' or 'KLNKVulCatA'. </p>

<p><strong>4)</strong> How does R handle missing observations for logistic regression? For example survey #10 was missing the 'KLNKVulCat' Variable, but not any of the other IV's. Would R or any other statistical languages not use any of the information for this particular person, or just that particular variable?</p>

<p>Any help is greatly appreciated, thank you.</p>
"
"0.105555962793852","0.100588487635796","155040","<p>I am struggling with interpreting coefficients from a multiple regression analysis with multiple categorical (dummy) variables. I am running a linear mixed model with biodiversity (<code>LnS_Add1</code>) as independent variable, and several continuous and categorical dependent variables.</p>

<p>With a single categorical/dummy variable (e.g. <code>LnS_Add1 ~ AREA_AM_2.5 + System_Type3</code>; where <code>AREA_AM_2.5</code> is continuous and <code>System_Type3</code> is categorical with 3 levels, i.e. <em>Arable</em>, <em>Grassland</em> and <em>Orchard</em>) this is pretty straightforward. In this case the intercept represents the mean of the reference dummy variable (e.g. <em>Arable</em>) and the mean of the 2nd and 3rd levels <em>Grassland</em> and <em>Orchard</em> can be calculated manually by adding intercept to the slope coefficient.</p>

<pre><code>globmod1 &lt;- lmer(LnS_Add1 ~ AREA_AM_2.5 + System_Type3 + 
     (1|Study_Code/Pair_Code), data1_plant)
summary(globmod1)
</code></pre>

<p>Which returns</p>

<pre><code>Fixed effects:
                        Estimate Std. Error t value
(Intercept)            0.3585534  0.1238470   2.895
AREA_AM_2.5            0.0004256  0.0001371   3.104
System_Type3Grassland -0.5227684  0.0915722  -5.709
System_Type3Orchard   -0.4057969  0.5477567  -0.741
</code></pre>

<p>To get a summary output that shows the means of both <em>Arable</em>, <em>Grassland</em> and <em>Orchard</em> in R I suppress the intercept by adding a -1 (or +0) to the model.</p>

<pre><code>globmod1.coef &lt;- lmer(LnS_Add1 ~ AREA_AM_2.5 + System_Type3 -1 +
                   (1|Study_Code/Pair_Code), data1_plant)
summary(globmod1.coef)
</code></pre>

<p>Which returns:</p>

<pre><code>Fixed effects:
                        Estimate Std. Error t value
AREA_AM_2.5            0.0004256  0.0001371   3.104
System_Type3Arable     0.3585534  0.1238470   2.895
System_Type3Grassland -0.1642149  0.1341851  -1.224
System_Type3Orchard   -0.0472434  0.5457304  -0.087
</code></pre>

<p>But what do I do if I have multiple categorical variables (e.g. <code>LnS_Add1 ~ AREA_AM_2.5 + System_Type3 + Habitat2</code>; where Habitat2 is a categorical variable with 3 levels, i.e. <em>Farm aggregated</em>, <em>Outside field</em>, and <em>Within field</em>)?. Now the intercept represents the mean of the reference level of a combination of <code>System_Type3</code> and <code>Habitat2</code> (e.g. all data in arable systems and measured at farm aggregate level). But what I am interested in are the means for the different levels of each of my 2 categorical variables, holding everything else constant.</p>

<p>How do I create a summary table that contains means of all levels of all categorical variables in my model? The -1 command doesnt help me anymore, as it removes the intercept but the intercept now represents a mean of 2 reference dummy variables. I am only interested here in the fixed effect estimates, not in any hypothesis testing.</p>
"
"0","0","155459","<p>I'm trying to predict the outcome ""Decision"" in the function of Age, Gender, Occupation, .... </p>

<p>The independent variable ""Occupation"" is known to be significant. But when I do the logistic model, each sub-group (modality) of it is not.</p>

<p>Should I regroup the levels having the same value of estimated coefficient? (which I guess doesn't make many sense because the levels are not statistically significant)</p>

<p>The variable Occupation has 74 different sub-groups.</p>

<p>And another problem is that when checking the multicollinearity, the function VIF in R doest work, it produces the NaN value, may be its due to the large number of sub-groups of Occupation.</p>

<p><img src=""http://i.stack.imgur.com/ruScu.png"" alt=""Summary(Logistic Regression)""></p>
"
"0.0609427635336005","0.0580747904139041","155509","<p>Working on a linear regression problem in R, I created a first model </p>

<pre><code>flights_lm = lm(freq~dist+capa+nbrt+depf+lcco+prbi)
</code></pre>

<p>where freq is frequency, dist is distance, capa for capacity, nrt stands for number of roads, fuel is depf. I then started to remove variables which doesn't contribute to the model explination, and ended up with
 Coefficients:</p>

<pre><code>              Estimate Std. Error t value Pr(&gt;|t|)    
 (Intercept) 2.822e+02  6.239e+03   0.045  0.96400    
 nbrt        8.072e+01  6.515e+00  12.390  &lt; 2e-16 ***
 depf        3.052e-05  1.128e-05   2.704  0.00784 ** 
</code></pre>

<p>meaning that the frequency was well explained by nbrt and depf. Then I proceeded to remove points that affect the model (I guess they're called outliers). for that I used the R function <code>Cooks.distance</code> and couldn't get rid of all the points. each time I apply the function and plot it according to the model some other points pop out of the 0.65 limit that set for the model.
I have one doubt though, once I remove the points, Do I need to restart the process from the beginning (meaning creating the model with all the variables then delete each that p-value is the biggest until I get all the p values &lt;0.05)
or just delete the points and plot the model ?
Another question is :  could the function log help me with this case ?  </p>
"
"0.0430930413588572","0.0410650781176591","156564","<p>I just developed a logistic regression model predicting customer churn (i.e how likely is a customer to leave us in the future?)</p>

<p>To understand the impact of my independent variables I calculated Odds ratio using the following function. </p>

<p><code>exp(trainingmodel$coefficients)</code></p>

<p>Where trainingmodel is the name of my model.</p>

<p>And I get the following results:</p>

<pre><code>AIRTIME 
9.789127e-01

Site.Report.By.Vehicle1
1.241823e+00
</code></pre>

<p>Both AIRTIME and Site Report are a feature of product we offer. In my dataset, AIRTIME is a continuous variable whereas Site.Report.By.Vehicle1 is a categorical variable with just two levels, ie. someone using the Site Report or not?</p>

<p>Can someone please help me to understand how to interpret the above number for AIRTIME and Site Report?</p>
"
"0.0879633023282099","0.0838237396964966","156619","<p>I'm using the concept of Hedonic regression in order to model the prices for real estates. I'm having some trouble with my approach.</p>

<p><strong>What I have and what I do</strong></p>

<ul>
<li>my data consists out of real estates with following charcteristics: <code>price | livingArea | propertyArea | condoFloorNumber | roomCount | elevator | garage | quiet | etc.</code></li>
<li>I run a robust regression without intercept <code>lmRob(price ~ . -1)</code></li>
</ul>

<p><strong>What I want</strong></p>

<ul>
<li>a model with which I can predict the price of real estates, but which are not in the used data set</li>
<li>also it would be nice to have some constraints on the coefficients</li>
</ul>

<p><strong>Problems</strong></p>

<ul>
<li>very often I get bad values for the coefficients <code>ex: bathroomCount = -80000</code>. it's not possible that with a additive bathroom , the price of the house will sink with <code>80.000â‚¬</code></li>
<li><p>also I tried to use the function <code>pcls</code> in order to put some constraints on the coefficients, but this method gave very bad results. In the plot <code>Y = price</code> and <code>X = livingArea</code>. as you can see, the regression line isn't correct.
<img src=""http://i.stack.imgur.com/7PHp1.png"" alt=""enter image description here""></p>

<ul>
<li>another thought was to transform the regression problem into a maximization or minimization problem, but didn't managed to do it</li>
<li>also I tried to use different regression methods <code>lm, lmrob, ltsReg, MARS</code>, but they also give me bad coefficients. (sometimes this bad coefficients make a good price estimation)</li>
<li>I think that the big number of dummy variables damages a little bit the regression</li>
</ul></li>
</ul>

<p>Is my approach false?</p>

<p>Does someone have some hints, tricks for me? (<em>I'm not a statistician</em>)</p>

<p><strong>[UPDATE]</strong></p>

<p><img src=""http://i.stack.imgur.com/a2kLe.png"" alt=""price ~ livingArea""></p>

<p>This is how the plotted data looks like. LivingArea is the only non-dummy variable.</p>

<p><strong>[UPDATE 2]</strong></p>

<pre><code>y = bX 

     means

y = b_0*X_0 + b_1*X_1 + ... + b_k*X_k

     which is an equation system like this:

y[0] = b_0*X_0[0] + b_1*X_1[0] + ... + b_k*X_k[0]
.
.
.
y[n] = b_0*X_0[n] + b_1*X_1[n] + ... + b_k*X_k[n]
</code></pre>

<p>Did I got it right? </p>

<p>If so, isn't possible to add some inequality constraints equation to it. example:</p>

<pre><code>b_0 &gt;= 2000
b_2 &lt;= b_0/2
</code></pre>

<p><strong>[UPDATE 3]</strong></p>

<p>I'm running the regression without intercept, because if all the characteristics of a real estate = 0, then of course it'S price = 0. Nobody would pay for an apartment with 0mÂ².
<img src=""http://i.stack.imgur.com/LYPB0.png"" alt=""enter image description here"">
but it seems that the regression line where it was used an intercept (blue) looks far more better than the regression line without intercept (green). I can't understand why it is so. and why doesn't the regression line without intercept start at the point (0,0)?</p>
"
"0.052777981396926","0.0502942438178979","156780","<p>I am running a logistic regression with 5 continuous independent variables (IV). The problem is that IV4 when taken alone has a positive correlation with outcome (coeff > 0), and when taken with the other variables has a negative correlation (coeff &lt; 0). I evaluated correlation between IV4 and the other variables, and the results are: 
IV4 vs. IV1 (-0.51), IV4 vs. IV2 (-0.48), IV4 vs. IV3 (0.61) and IV4 vs. IV5 (0.73).</p>

<p>I ran other logistic regressions <em>eliminating one at a time all the other variables</em> to look if one of them was responsible for the sign change, and I noticed that when eliminating IV1, the sign of V4 coefficient became positive.</p>

<p>Thus, it seems that IV1 changes the sign of the coefficient of IV4. 
Is there someone who knows what might be the cause and (possibly) the solution?</p>

<p>Practically, do I have to eliminate the IV4 (or IV1) from the model and explain why?</p>

<p>Thanks a lot for answering</p>

<p>Leonardo Frazzoni, MD</p>
"
"0.0691025985081098","0.076825726438694","157142","<p>A couple years ago I performed a linear regression on data that looked like this:</p>

<pre><code>   company year     y       x1      x2      x3      x4
1        A 2012  1.83  34811.8 14755.5   278.2     0.0
2        B 2012  3.87  10435.5  9692.6   522.2   317.9
3        C 2012 19.76 199670.6 23428.7 10675.5  2815.8
4        D 2012  1.22   3204.4  2087.5  2282.8  2804.1
5        E 2012  0.00      5.2    53.5     0.2   193.8
6        F 2012  0.81 161936.0 25777.9  2364.8   540.6
7        G 2012  1.22   1479.3    28.6     0.4     3.9
8        H 2012  2.24   9716.3   888.2  2073.9  1059.1
9        I 2012 25.25 331396.9 15162.0 87062.1 32724.7
10       J 2012  0.20   9812.0 10363.4    49.9 36664.9
11       K 2012  1.02  62715.3  5746.5  1007.7   866.3
12       L 2012  3.87 121397.5  5842.2  1481.6   621.0
13       M 2012 12.22 243189.5 50370.8 16747.1 23025.8
14       N 2012 18.33 147305.6 87916.3 15098.3 16449.7
15       O 2012  0.61  20699.1  8345.6     0.0    26.4
16       P 2012  2.44  30735.1  1840.6  4900.1     0.0
</code></pre>

<p>Each row is a different company and the main objective was to interpret the coefficients. Its been 3 years since that regression and I want to look at it again with data from each year so the dataset would look like this:</p>

<pre><code>   company year     y       x1      x2       x3      x4
1        A 2012  1.83  34811.8 14755.5    278.2     0.0
2        B 2012  3.87  10435.5  9692.6    522.2   317.9
3        C 2012 19.76 199670.6 23428.7  10675.5  2815.8
4        D 2012  1.22   3204.4  2087.5   2282.8  2804.1
5        E 2012  0.00      5.2    53.5      0.2   193.8
6        F 2012  0.81 161936.0 25777.9   2364.8   540.6
7        G 2012  1.22   1479.3    28.6      0.4     3.9
8        H 2012  2.24   9716.3   888.2   2073.9  1059.1
9        I 2012 25.25 331396.9 15162.0  87062.1 32724.7
10       J 2012  0.20   9812.0 10363.4     49.9 36664.9
11       K 2012  1.02  62715.3  5746.5   1007.7   866.3
12       L 2012  3.87 121397.5  5842.2   1481.6   621.0
13       M 2012 12.22 243189.5 50370.8  16747.1 23025.8
14       N 2012 18.33 147305.6 87916.3  15098.3 16449.7
15       O 2012  0.61  20699.1  8345.6      0.0    26.4
16       P 2012  2.44  30735.1  1840.6   4900.1     0.0
17       A 2013  0.20   4832.1 10691.6      0.6     0.0
18       B 2013  3.02  12575.8  1270.3    106.6   368.0
19       C 2013 16.00 184628.5 38269.7   5343.1  4645.6
20       D 2013  1.76   4684.6  1445.2   2150.1  1727.0
21       E 2013  1.27      4.3    22.9     38.3   314.6
22       F 2013  0.39 141808.6 26368.8    673.6  2259.2
23       G 2013  0.59    986.3    38.6      7.0     5.8
24       H 2013  2.83  20111.4  3518.3    549.5    59.6
25       I 2013 21.17 303925.9 20248.0 107366.7 19979.1
26       J 2013  1.37   7792.8 16000.7     33.5 39541.7
27       K 2013  1.66 141071.9 11136.1    162.2     0.0
28       L 2013  3.80 130359.7  8882.5     40.5   520.8
29       M 2013 10.63 280250.3 39029.7  16208.6 29284.3
30       N 2013 19.41 145278.1 55141.6  14115.5  1783.4
31       O 2013  0.98   1517.6  3610.4      0.0   547.3
32       P 2013  3.32 101484.2  1140.5   5489.9     0.0
33       A 2014  0.10      0.0  9520.7      0.9     0.0
34       B 2014  4.02  14886.8  2331.5      0.0   631.8
35       C 2014 14.22 143760.9 50222.1   6118.1  4342.1
36       D 2014  0.88    936.1  1802.7   1273.6  4394.3
37       E 2014  0.78    231.5    15.8     64.1   291.9
38       F 2014  0.78 244303.2 29148.3   3161.4  4908.1
39       G 2014  0.78   1032.6    30.3      1.3     7.8
40       H 2014  2.55  26322.6 11726.1   2859.2     0.0
41       I 2014 21.96 614241.5  9138.2  94273.7 17702.0
42       J 2014  1.27   8946.5 13853.7    693.9 19672.0
43       K 2014  1.18 164269.7  7088.1     29.7   825.0
44       L 2014  2.35 107152.3  3275.2     94.7   490.9
45       M 2014  8.73 284267.4 51896.4  12838.1 28019.5
46       N 2014 20.69  84554.6 32341.0  11408.2   624.9
47       O 2014  1.08      0.0  7663.2      0.0     0.0
48       P 2014  3.63 109392.9  5229.2   4691.0    11.1
</code></pre>

<p>When I think about this dataset I don't immediately think its a time series but I also don't think I should be ignoring year all together and regressing it like so in R:</p>

<pre><code>lm(y ~ x1 + x2 + x3 + x4)
</code></pre>

<p>So I'm wondering how I should model this dataset. Should I just include dummy variables for year or are there better approaches here?</p>
"
"0.0861860827177143","0.0821301562353182","157597","<p>I am trying to understand how to interpret log-linear models for contingency tables, fitted by way of Poisson GLMs. </p>

<p>Consider this example from CAR (Fox and Weisberg, 2011, p. 252). </p>

<pre><code>require(car)
data(AMSsurvey)
(tab.sex.citizen &lt;- xtabs(count ~ sex + citizen, data=AMSsurvey))
</code></pre>

<p>Yielding:</p>

<pre><code>        citizen
sex      Non-US  US
  Female    260 202
  Male      501 467
</code></pre>

<p>Then we fit the model of (mutual) independence: </p>

<pre><code>AMS2 &lt;- as.data.frame(tab.sex.citizen)
(phd.mod.indep &lt;- glm(Freq ~ sex + citizen, family=poisson, data=AMS2))
pchisq(2.57, df=1, lower.tail=FALSE)
</code></pre>

<p>Outputting: </p>

<pre><code>&gt; (phd.mod.indep &lt;- glm(Freq ~ sex + citizen, family=poisson, data=AMS2))

Call:  glm(formula = Freq ~ sex + citizen, family = poisson, data = AMS2)

Coefficients:
(Intercept)      sexMale    citizenUS  
     5.5048       0.7397      -0.1288  

Degrees of Freedom: 3 Total (i.e. Null);  1 Residual
Null Deviance:      191.5 
Residual Deviance: 2.572    AIC: 39.16
&gt; pchisq(2.57, df=1, lower.tail=FALSE)
[1] 0.1089077
</code></pre>

<p>The p value is close to 0.1 indicating weak evidence to reject independence. However, let us <strong>assume</strong> that we have sufficient evidence to reject the NULL (i.e. for our purposes, the 0.10 p value is indicative of an association between the two variables). </p>

<p><strong>Question</strong>: How, then, do we interpret this loglinear model? </p>

<p>(Do we fit the saturated model (i.e. <code>update(phd.mod.indep, . ~ . + sex:citizen)</code>)? Do we interpret the estimated regression coefficients? In CAR they stop at this point, because of weak evidence for rejecting the NULL, but I'm interested in understanding the mechanics of the interpretation of this simple log-linear model <em>as if</em> the ""interaction"" were significant...)</p>
"
"0.068136080998913","0.0649295895722714","157694","<p>I have a formula (not mine):</p>

<pre><code>NY/A = (Passing Yards - Sack Yards) / (Passes Attempted + Times Sacked)
</code></pre>

<p>and this formula correlates with wins at a correlation coefficient of 0.50. </p>

<p>Edit: I have the data for wins and passing yards, sack yards, passes attempted, and times sacked. NY/A stands for net yards per attempt, and I would like to correlate this with wins at a coefficient of at least 0.52. I can put some weight on times sacked or sack yards. </p>

<p>My goal is to increase the correlation to at least 0.52. How can I do this? Is there a regression I can run in R?</p>
"
"0.101062140164153","0.0963061447907242","157763","<p>I recently ran a test on our application that models a fairly contrived example:</p>

<p>Users are expected to add things to a bucket. We count the things in this bucket. A binary condition variable exists which could have an effect on the number of things users put things in a bucket. </p>

<p>The question I'm attempting to answer is if the variable has an effect on the number of things dropped in the bucket.</p>

<p>In statistical language:</p>

<p><em>H0:</em> The binary variable has no effect on the # of things</p>

<p><em>H-alt:</em> The binary variable increases the # of things in the bucket</p>

<p><strong>Question 1:</strong>
What is the most appropriate statistical test to apply to infer if a difference exists in the counts?</p>

<p>I started with a student t test, but the assumption of normality and sample of continuous outcome variable is violated. The data are counts and I believe that the poisson distribution is the most appropriate. Chi square is tempting the data are not appropriate for a contingency table since each user drops 0 or more things in a bucket.</p>

<p><strong>Question 2:</strong></p>

<p>I think a general linear regression attempts to answer my question. Is my interpretation of the results correct?</p>

<pre><code>group1 &lt;- append(rep(0, 100), rpois(50, 110))
group2 &lt;- append(rep(0, 100), rpois(50, 100))
df1 &lt;- data.frame(cnt = group1,
                  gr  = TRUE)
df2 &lt;- data.frame(cnt = group2,
                  gr  = FALSE)
dat &lt;- rbind(df1, df2)
fit &lt;- glm(data=dat, cnt ~ gr, family='poisson')
summary(fit)
# Coefficients:
#              Estimate Std. Error z value Pr(&gt;|z|)    
# (Intercept)  3.48493    0.01430 243.771  &lt; 2e-16 ***
#grTRUE        0.11840    0.01964   6.027 1.67e-09 ***

fit$coefficients[[1]]

prTrue &lt;- exp(fit$coefficients[[1]] + fit$coefficients[[2]] * 1)
(prTrue) / (1 + prTrue)  # =&gt; 0.9734889

prFalse &lt;- exp(fit$coefficients[[1]] + fit$coefficients[[2]] * 0)
(prFalse) / (1 + prFalse) # =&gt; 0.9702558
</code></pre>

<p>Interpretation:</p>

<p>The sign of the grTrue coeffient is positive so the presence of the variable increases the number of things dropped in the bucket.</p>

<p>The presence of the variable increases the likelihood of increasing the # of things in the bucket by about 0.3% (0.973 - 0.970)</p>

<p>Thanks in advance. As you can tell, I'm kind of a noob at this and keep thinking myself in circles.</p>
"
"0.0430930413588572","0.0205325390588295","157907","<p>I'm trying to understand Principal Component Regression(PCR) using dummy data (no multicollinearity). I have managed to regress dependent variable on scores but how do I convert the coefficients back to coefficients of original independent data? Below is my example using R</p>

<pre><code># create dummy data for testing
a1&lt;-1:10
a2&lt;-rnorm(10,0,5)  # uncorrelated to a1, for testing purposes
a&lt;-cbind(a1,a2)
y&lt;-a1-2*a2   # dependent variable

pr&lt;-princomp(a,cor=TRUE)
s&lt;-pr$scores
m2&lt;-lm(y~s)

# results
m2$coefficients 
(Intercept)     sComp.1     sComp.2 
 2.461620   -7.370569  -11.432588
</code></pre>

<p><strong>How do I convert <code>m2$coefficients</code> back to coefficients in terms of original data?</strong></p>

<p>From what I have learned so far, I only need to multiply EigenVectors with the above coefficients. But the coefficients have dimension of 3x1 and eigenvectors are 2x2!</p>
"
"0.0430930413588572","0.0410650781176591","158028","<p>I have a model for survival after an injury that is borderline passing the Schoenfeld test for the proportional hazards assumption (<code>cox.zph()</code> in R). </p>

<p>However, suspecting that there would be an increased mortality within the first month I fitted an Aalen additive regression model also hinting at a varying coefficient since the cumulative coefficient drops steeply within the first 4 days, then gradually continues to decline before it stabilizes at a nearly constant level after a year for my variable of interest. </p>

<p>Now - how can I accommodate such a model in R? </p>

<ul>
<li>Introduce an interaction term [for my variable of interest] varying with time? </li>
<li>Create different variables for time 0-4, 4-365 and 365 and then also expand the dataset with three rows per patient?</li>
</ul>
"
"NaN","NaN","158056","<p>I developed a negative binomial generalized linear regression model in R but now need to put it into Java. What equation do I use given these coefficients? (Sample below is just an example.)</p>

<p>As mentioned in <a href=""http://www.ats.ucla.edu/stat/r/dae/nbreg.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/dae/nbreg.htm</a></p>

<pre><code>## glm.nb(formula = daysabs ~ math + prog, data = dat, init.theta = 1.032713156, 
##     link = log)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -2.155  -1.019  -0.369   0.229   2.527  
## 
## Coefficients:
##                Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)     2.61527    0.19746   13.24  &lt; 2e-16 ***
## math           -0.00599    0.00251   -2.39    0.017 *  
## progAcademic   -0.44076    0.18261   -2.41    0.016 *  
## progVocational -1.27865    0.20072   -6.37  1.9e-10 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</code></pre>

<p>Thanks</p>
"
"0.105869651339174","0.108647984268766","158366","<p>I'm trying to fit a multiple regression model with pairwise deletion in the context of missing data.  <code>lm()</code> uses listwise deletion, which I'd prefer not to use in my case.  I'd also prefer not to use multiple imputation or FIML.  How can I do multiple regression with pairwise deletion in R?</p>

<p>I have tried the <code>mat.regress()</code> function of the <code>psych</code> package, which fits regression models to correlation/covariance matrices (which can be obtained from pairwise deletion), but the regression model does not appear to include an intercept parameter.</p>

<p>Here's what I've tried (small example):</p>

<pre><code>set.seed(33333)
y &lt;- rnorm(1000)
x1 &lt;- y*2 + rnorm(1000, sd=.2)
x2 &lt;- y*5 + rnorm(1000, sd=.5)

y[sample(1:1000, 10)] &lt;- NA
x1[sample(1:1000, 10)] &lt;- NA
x2[sample(1:1000, 10)] &lt;- NA

mydata &lt;- data.frame(y, x1, x2)
covMatrix &lt;- cov(mydata, use=""pairwise.complete.obs"")

#Listwise Deletion
listwiseDeletion &lt;- lm(y ~ x1 + x2, data=mydata)
observations &lt;- length(listwiseDeletion$na.action) #30 rows deleted due to listwise deletion

coef(listwiseDeletion)
(Intercept)          x1          x2 
0.001995527 0.245372245 0.100001989

#Pairwise Deletion --- but missing intercept
pairwiseDeletion &lt;- mat.regress(y=""y"", x=c(""x1"",""x2""), data=covMatrix, n.obs=observations)
pairwiseDeletion$beta
       y
x1 0.1861277
x2 0.1251995

#Pairwise Deletion --- tried to add intercept, but received error when fitting model
mydata$intercept &lt;- 0
covMatrixWithIntercept &lt;- cov(mydata, use=""pairwise.complete.obs"")

pairwiseDeletionWithIntercept &lt;- mat.regress(y=""y"", x=c(""intercept"",""x1"",""x2""), data=covMatrixWithIntercept, n.obs=observations)
Something is seriously wrong the correlation matrix.
In smc, smcs were set to 1.0
Warning messages:
1: In cov2cor(C) :
  diag(.) had 0 or NA entries; non-finite result is doubtful
2: In cor.smooth(R) :
  I am sorry, there is something seriously wrong with the correlation matrix,
cor.smooth failed to  smooth it because some of the eigen values are NA.  
Are you sure you specified the data correctly?
</code></pre>

<p>So, how can I obtain an intercept parameter using <code>mat.regress</code>, or how can I obtain parameter estimates from pairwise deletion using another method or package in R?  I've seen <a href=""https://stats.stackexchange.com/questions/107597/is-there-a-way-to-use-the-covariance-matrix-to-find-coefficients-for-multiple-re"">matrix calculations</a> to do this, but, ideally, there'd be a package that also outputs regression diagnostics, fit stats, etc.  Also, preferably, the method would be able to fit interaction terms.</p>
"
"0.0691025985081098","0.076825726438694","158433","<p>I keep trying to perform parametric bootstrap on simple regression analysis to grasp the concept. The internet is full of tutorials on non-parametric one, but I found no explanation or steps concerning parametric bootstrap, so I did it on my own. Since I'm not sure if what was done is o.k., I kindly ask you to correct if I'm wrong (... or praise me if I'm right:) ).</p>

<pre><code>library(car)
library(boot)
attach(Anscombe) # I'm going to use Anscombe data

lm.out&lt;-lm(education~income, data=Anscombe) #simple regression to obtain coef.
regre.mle&lt;-coef(lm.out)
</code></pre>

<p>The model was built, so I was able to get sample $\sigma$ for errors</p>

<pre><code>mean(lm.out$resid) # 3.957485e-15
sd(lm.out$resid)   # 34.58725

regre.sim &lt;- function(data, mle){
  n &lt;- dim(data)[1]
  data$education &lt;- mle[2]*data$income+mle[1]+rnorm(n, mean=0, sd=34.58725)
  return(data)
}

regre.stat&lt;- function(data) {
  lm.out&lt;-lm(education~income, data=data)
  return(lm.out$coefficients)
}

boot.out&lt;-boot(Anscombe, statistic=regre.stat, R=100,
               ran.gen=regre.sim,
               sim=""parametric"", mle=regre.mle)

boot.ci(boot.out, type = ""basic"", index = 1) #for intercept
boot.ci(boot.out, type = ""basic"", index = 2) #for beta coef
</code></pre>

<p>Finally I get this:</p>

<pre><code>BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
Based on 99 bootstrap replicates

CALL : 
boot.ci(boot.out = boot.out, type = ""basic"", index = 2)

Intervals : 
Level      Basic         
95%   ( 0.0359,  0.0727 )  
Calculations and Intervals on Original Scale
Some basic intervals may be unstable
</code></pre>

<p>But the question is - this is it (parametric bootstrap on regression)? </p>
"
"0.0457070726502004","0.0580747904139041","159316","<p>I have been running logistic regression in R, and have been having an issue where as I include more predictors the z-scores and respective p-values approach 0 and 1 respectively.  For example if have few predictors:</p>

<pre><code>&gt; model1
b17 ~ i74 + i73 + i72 + i71
&gt; step1&lt;-glm(model1,data=newdat1,family=""binomial"")
Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -6.9461     1.8953  -3.665 0.000247 ***
i74           0.6842     0.9543   0.717 0.473384    
i73           1.7691     4.8008   0.368 0.712502    
i72           0.5134     2.0142   0.255 0.798812    
i71          -0.6753     4.9173  -0.137 0.890771    
</code></pre>

<p>The results appear to be fairly reasonable; however, if I have more predictors:</p>

<pre><code> &gt; model1
b17 ~ i90 + i89 + i88 + i87 + i86 + i85 + i84 + i83 + i82 + i81 + 
i80 + i79 + i78 + i77 + i76 + i74 + i73 + i72 + i71
&gt; step1&lt;-glm(model1,data=newdat1,family=""binomial"")
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -4.887e+02  3.503e+05  -0.001    0.999
i90          1.431e-01  1.009e+04   0.000    1.000
i89          8.062e+01  1.027e+05   0.001    0.999
i88          9.738e+01  7.398e+04   0.001    0.999
i87         -1.980e+01  9.469e+03  -0.002    0.998
i86          9.829e+00  1.098e+05   0.000    1.000
i85          5.917e+01  3.074e+04   0.002    0.998
i84         -2.373e+01  1.378e+05   0.000    1.000
i83          7.257e+00  2.173e+05   0.000    1.000
i82         -1.397e+01  1.894e+05   0.000    1.000
i81          6.503e+01  1.373e+05   0.000    1.000
i80          3.728e+01  4.904e+04   0.001    0.999
i79          1.010e+02  5.556e+04   0.002    0.999
i78         -2.628e+01  1.546e+05   0.000    1.000
i77          4.725e+01  3.027e+05   0.000    1.000
i76         -6.517e+01  1.509e+05   0.000    1.000
i74          1.267e+01  1.175e+05   0.000    1.000
i73          2.796e+02  5.280e+05   0.001    1.000
i72         -2.533e+02  4.412e+05  -0.001    1.000
i71         -1.240e+02  4.387e+05   0.000    1.000
</code></pre>

<p>I know it is hard to say exactly what is going on without seeing the data, but the predictors are all 5-point Likert Scale items.  However, are there any thoughts to what is occurring here?  I don't have much experience with logistic regression, so I apologize if the question seems naive, but is there a certain threshold of predictors where logistic regression falls apart due to having such a large amount of predictors what is ultimately a very small amount of variance?  Is the potentially a multi-co-linearity issue?  Finally, when I run OLS regression on the data I get results that make more sense (or at least appear to), is it okay/what are the consequences of running OLS regression on a binary outcome?  Thank you!</p>
"
"0.136353252253142","0.1523392900566","159355","<p>I performed regression with robust variances (after Stata 12.1 lnskew transformation). A question of overfitting has been raised.</p>

<p>To summarise what I did: </p>

<ol>
<li>Comparison of disease B (disgrp=2) versus disease C (disgrp=3)
patients with 45-54 dependent observations (FibrosisP, continuous variable) in each disease group. Each patient had observations taken from Regions P, Q and R and Walls X, Y and Z (i.e. 9 observations per patient). </li>
<li>lnskew0 transformation (natural log transformation with zero skew of
resulting distribution) of FibrosisP to give lfibr. </li>
<li>Simple and then multiple regression with clustered robust variances/standard errors (clustered by patient and using independent categorical variables Disease, Wall and Region).</li>
</ol>

<p>Note that Disease A is excluded from this analysis (and is not provided in data set below).</p>

<p>$$ multipleregression: lfibr \sim Disease + Wall + Region $$</p>

<p>I have copied the original Stata v12.1 log file below, which will hopefully tell the whole story. </p>

<pre><code>. gen disgrp=.
(153 missing values generated)

. replace disgrp=1 if disease==""A""
(54 real changes made)

. replace disgrp=2 if disease==""B""
(54 real changes made)

. replace disgrp=3 if disease==""C""
(45 real changes made)

. lnskew0 lfibr= fibrosisp

       Transform |         k     [95% Conf. Interval]       Skewness
-----------------+--------------------------------------------------
  ln(fibrosis-k) |   .0116473      (not calculated)        -2.77e-08

    . gen region1=.
(153 missing values generated)

. replace region1=1 if region==""P""
(51 real changes made)

. replace region1=2 if region==""Q""
(51 real changes made)

. replace region1=3 if region==""R""
(51 real changes made)

. gen wall1=.
(153 missing values generated)

. replace wall1=1 if wall==""X""
(51 real changes made)

. replace wall1=2 if wall==""Y""
(51 real changes made)

. replace wall1=3 if wall==""Z""
(51 real changes made)

. **Comparing C with B**

. xi:regress lfibr disgrp if disgrp &gt; 1  , cluster(pat)

Linear regression                                      Number of obs =      99
                                                       F(  1,    10) =   20.51
                                                       Prob &gt; F      =  0.0011
                                                       R-squared     =  0.3884
                                                       Root MSE      =   .5833

                             (Std. Err. adjusted for 11 clusters in patientid)
------------------------------------------------------------------------------
             |               Robust
       lfibr |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      disgrp |   .9241372    .204061     4.53   0.001     .4694609    1.378813
       _cons |  -4.667246   .4602243   -10.14   0.000    -5.692689   -3.641802
------------------------------------------------------------------------------

. xi:regress lfibr region1 if disgrp &gt; 1  , cluster(pat)

Linear regression                                      Number of obs =      99
                                                       F(  1,    10) =    3.17
                                                       Prob &gt; F      =  0.1055
                                                       R-squared     =  0.0068
                                                       Root MSE      =  .74333

                             (Std. Err. adjusted for 11 clusters in patientid)
------------------------------------------------------------------------------
             |               Robust
       lfibr |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
     region1 |   .0748154   .0420368     1.78   0.105    -.0188483    .1684792
       _cons |   -2.54854    .177876   -14.33   0.000    -2.944872   -2.152207
------------------------------------------------------------------------------

. xi:regress lfibr i.region1 if disgrp &gt; 1  , cluster(pat)
i.region1         _Iregion1_1-3       (naturally coded; _Iregion1_1 omitted)

Linear regression                                      Number of obs =      99
                                                       F(  2,    10) =    5.59
                                                       Prob &gt; F      =  0.0235
                                                       R-squared     =  0.0612
                                                       Root MSE      =  .72645

                             (Std. Err. adjusted for 11 clusters in patientid)
------------------------------------------------------------------------------
             |               Robust
       lfibr |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
 _Iregion1_2 |   .4400513   .1398977     3.15   0.010     .1283397    .7517628
 _Iregion1_3 |   .1496308   .0845103     1.77   0.107    -.0386698    .3379314
       _cons |   -2.59547   .1905071   -13.62   0.000    -3.019946   -2.170993
------------------------------------------------------------------------------

. xi:regress lfibr i.wall1 if disgrp &gt; 1  , cluster(pat)
i.wall1           _Iwall1_1-3         (naturally coded; _Iwall1_1 omitted)

Linear regression                                      Number of obs =      99
                                                       F(  2,    10) =    6.17
                                                       Prob &gt; F      =  0.0180
                                                       R-squared     =  0.0630
                                                       Root MSE      =  .72575

                             (Std. Err. adjusted for 11 clusters in patientid)
------------------------------------------------------------------------------
             |               Robust
       lfibr |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
   _Iwall1_2 |   .3285724   .1654396     1.99   0.075    -.0400499    .6971948
   _Iwall1_3 |   .4356131   .1305289     3.34   0.008     .1447766    .7264496
       _cons |  -2.653637   .1780801   -14.90   0.000    -3.050425    -2.25685
------------------------------------------------------------------------------

. xi3:regress lfibr disgrp*i.region1*i.wall1 if disgrp &gt; 1 , cluster(pat)
i.region1         _Iregion1_1-3       (naturally coded; _Iregion1_1 omitted)
i.wall1           _Iwall1_1-3         (naturally coded; _Iwall1_1 omitted)

Linear regression                                      Number of obs =      99
                                                       F(  8,    10) =       .
                                                       Prob &gt; F      =       .
                                                       R-squared     =  0.5401
                                                       Root MSE      =  .55355

                             (Std. Err. adjusted for 11 clusters in patientid)
------------------------------------------------------------------------------
             |               Robust
       lfibr |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      disgrp |   .5419458   .4154947     1.30   0.221    -.3838342    1.467726
 _Iregion1_2 |  -.2963738   1.409317    -0.21   0.838    -3.436529    2.843781
 _Iregion1_3 |  -.2791626   1.335259    -0.21   0.839    -3.254304    2.695979
   _Iwall1_2 |  -1.039268     .97762    -1.06   0.313    -3.217541    1.139005
   _Iwall1_3 |  -.9227228   1.131622    -0.82   0.434    -3.444133    1.598687
    _IdiXre2 |    .305921   .5859783     0.52   0.613    -.9997201    1.611562
    _IdiXre3 |   .2146185   .5228838     0.41   0.690    -.9504393    1.379676
    _IdiXwa2 |   .5887627   .4158743     1.42   0.187    -.3378629    1.515388
    _IdiXwa3 |   .5677226   .5322211     1.07   0.311      -.61814    1.753585
   _Ire2Xwa2 |   .9560212   1.372943     0.70   0.502    -2.103087    4.015129
   _Ire2Xwa3 |   1.876106   1.632401     1.15   0.277    -1.761111    5.513323
   _Ire3Xwa2 |   .1403149   1.711091     0.08   0.936    -3.672233    3.952863
   _Ire3Xwa3 |   .5961959   1.627029     0.37   0.722     -3.02905    4.221442
_IdiXre2Xwa2 |  -.4387073   .5346165    -0.82   0.431    -1.629907    .7524925
_IdiXre2Xwa3 |  -.7328102   .7126107    -1.03   0.328    -2.320606    .8549855
_IdiXre3Xwa2 |  -.1024311   .6268405    -0.16   0.873    -1.499119    1.294257
_IdiXre3Xwa3 |  -.3174033   .6961228    -0.46   0.658    -1.868461    1.233655
       _cons |  -4.217918   .9792797    -4.31   0.002     -6.39989   -2.035947
------------------------------------------------------------------------------

. xi3:regress lfibr disgrp i.region1 i.wall1 if disgrp &gt; 1 , cluster(pat)
i.region1         _Iregion1_1-3       (naturally coded; _Iregion1_1 omitted)
i.wall1           _Iwall1_1-3         (naturally coded; _Iwall1_1 omitted)

Linear regression                                      Number of obs =      99
                                                       F(  5,    10) =   10.60
                                                       Prob &gt; F      =  0.0010
                                                       R-squared     =  0.5127
                                                       Root MSE      =  .53177

                             (Std. Err. adjusted for 11 clusters in patientid)
------------------------------------------------------------------------------
             |               Robust
       lfibr |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      disgrp |   .9241372   .2084032     4.43   0.001     .4597858    1.388488
 _Iregion1_2 |   .4400513   .1421362     3.10   0.011      .123352    .7567505
 _Iregion1_3 |   .1496308   .0858625     1.74   0.112    -.0416828    .3409444
   _Iwall1_2 |   .3285724   .1680868     1.95   0.079    -.0459482    .7030931
   _Iwall1_3 |   .4356131   .1326175     3.28   0.008     .1401229    .7311033
       _cons |  -5.118535   .4532473   -11.29   0.000    -6.128433   -4.108637
------------------------------------------------------------------------------
</code></pre>

<p>csv data:</p>

<pre><code>""row"",""PatientID"",""Disease"",""Wall"",""Region"",""FibrosisP""
""1"",1,""C"",""X"",""P"",0.11574464021797
""2"",1,""C"",""X"",""Q"",0.06409239204845
""3"",1,""C"",""X"",""R"",0.05589004594181
""4"",2,""C"",""X"",""P"",0.08452786770152
""5"",2,""C"",""X"",""Q"",0.19765474370344
""6"",2,""C"",""X"",""R"",0.29491566808792
""7"",3,""C"",""X"",""P"",0.13849556170319
""8"",3,""C"",""X"",""Q"",0.21529108879539
""9"",3,""C"",""X"",""R"",0.23260346696877
""10"",4,""C"",""X"",""P"",0.03242538798989
""11"",4,""C"",""X"",""Q"",0.18213249953927
""12"",4,""C"",""X"",""R"",0.0464009382069
""13"",17,""C"",""X"",""P"",0.12925196186539
""14"",17,""C"",""X"",""Q"",0.16685146683109
""15"",17,""C"",""X"",""R"",0.16298253982187
""16"",5,""B"",""X"",""P"",0.06082167946576
""17"",5,""B"",""X"",""Q"",0.06179248715729
""18"",5,""B"",""X"",""R"",0.04635879285168
""19"",6,""B"",""X"",""P"",0.0512284261286
""20"",6,""B"",""X"",""Q"",0.05560175796177
""21"",6,""B"",""X"",""R"",0.05038057719884
""22"",7,""B"",""X"",""P"",0.03485909775192
""23"",7,""B"",""X"",""Q"",0.07526805988175
""24"",7,""B"",""X"",""R"",0.03989544438546
""25"",8,""B"",""X"",""P"",0.05069990522336
""26"",8,""B"",""X"",""Q"",0.11638788902232
""27"",8,""B"",""X"",""R"",0.23086071670409
""28"",9,""B"",""X"",""P"",0.12712370092246
""29"",9,""B"",""X"",""Q"",0.05070659692429
""30"",9,""B"",""X"",""R"",0.06183074530974
""31"",10,""B"",""X"",""P"",0.04509566111129
""32"",10,""B"",""X"",""Q"",0.09050081347533
""33"",10,""B"",""X"",""R"",0.05178363738579
""52"",1,""C"",""Y"",""P"",0.14421181658066
""53"",1,""C"",""Y"",""Q"",0.1299066509205
""54"",1,""C"",""Y"",""R"",0.14904819595697
""55"",2,""C"",""Y"",""P"",0.08801608368174
""56"",2,""C"",""Y"",""Q"",0.24864891863453
""57"",2,""C"",""Y"",""R"",0.15962998919524
""58"",3,""C"",""Y"",""P"",0.4272296674396
""59"",3,""C"",""Y"",""Q"",0.2593375589095
""60"",3,""C"",""Y"",""R"",0.26700346966879
""61"",4,""C"",""Y"",""P"",0.14002780500134
""62"",4,""C"",""Y"",""Q"",0.28346720806288
""63"",4,""C"",""Y"",""R"",0.19312813953225
""64"",17,""C"",""Y"",""P"",0.17668051188556
""65"",17,""C"",""Y"",""Q"",0.18609876357474
""66"",17,""C"",""Y"",""R"",0.26587590290484
""67"",5,""B"",""Y"",""P"",0.05356234036154
""68"",5,""B"",""Y"",""Q"",0.04731210983269
""69"",5,""B"",""Y"",""R"",0.04877515848359
""70"",6,""B"",""Y"",""P"",0.06240572241178
""71"",6,""B"",""Y"",""Q"",0.13301297541279
""72"",6,""B"",""Y"",""R"",0.17973855854636
""73"",7,""B"",""Y"",""P"",0.06463245380331
""74"",7,""B"",""Y"",""Q"",0.10244742460486
""75"",7,""B"",""Y"",""R"",0.0599854720435
""76"",8,""B"",""Y"",""P"",0.05824947941558
""77"",8,""B"",""Y"",""Q"",0.11926213239492
""78"",8,""B"",""Y"",""R"",0.04685947691071
""79"",9,""B"",""Y"",""P"",0.06752011460398
""80"",9,""B"",""Y"",""Q"",0.09542812038592
""81"",9,""B"",""Y"",""R"",0.08668150350578
""82"",10,""B"",""Y"",""P"",0.06486814661182
""83"",10,""B"",""Y"",""Q"",0.05854476138367
""84"",10,""B"",""Y"",""R"",0.04438863783229
""103"",1,""C"",""Z"",""P"",0.05133333746688
""104"",1,""C"",""Z"",""Q"",0.14821006659988
""105"",1,""C"",""Z"",""R"",0.08174176027544
""106"",2,""C"",""Z"",""P"",0.23884995419341
""107"",2,""C"",""Z"",""Q"",0.2099355433643
""108"",2,""C"",""Z"",""R"",0.13176723596276
""109"",3,""C"",""Z"",""P"",0.46479557484677
""110"",3,""C"",""Z"",""Q"",0.33304596595977
""111"",3,""C"",""Z"",""R"",0.29770388592371
""112"",4,""C"",""Z"",""P"",0.15308213537672
""113"",4,""C"",""Z"",""Q"",0.28081619128875
""114"",4,""C"",""Z"",""R"",0.24592983188039
""115"",17,""C"",""Z"",""P"",0.21312809357862
""116"",17,""C"",""Z"",""Q"",0.23336174725733
""117"",17,""C"",""Z"",""R"",0.22714195157817
""118"",5,""B"",""Z"",""P"",0.06818263568709
""119"",5,""B"",""Z"",""Q"",0.07257093444773
""120"",5,""B"",""Z"",""R"",0.08201262934886
""121"",6,""B"",""Z"",""P"",0.0644884733419
""122"",6,""B"",""Z"",""Q"",0.11937946452025
""123"",6,""B"",""Z"",""R"",0.07081608918845
""124"",7,""B"",""Z"",""P"",0.06720225949377
""125"",7,""B"",""Z"",""Q"",0.12509595330262
""126"",7,""B"",""Z"",""R"",0.06657357031905
""127"",8,""B"",""Z"",""P"",0.05878644062606
""128"",8,""B"",""Z"",""Q"",0.26638352132337
""129"",8,""B"",""Z"",""R"",0.06789933388591
""130"",9,""B"",""Z"",""P"",0.0908078338911
""131"",9,""B"",""Z"",""Q"",0.17670466924957
""132"",9,""B"",""Z"",""R"",0.10642489420997
""133"",10,""B"",""Z"",""P"",0.05107976253608
""134"",10,""B"",""Z"",""Q"",0.07242867177979
""135"",10,""B"",""Z"",""R"",0.05074329491013
</code></pre>

<p>My interpretation is that we have 99 observations for 3 categorical variables (2-3 categories each) in the regression analysis; Root MSE = 0.53177.</p>

<p>Is overfitting a valid concern here? If so, is there any way to address it?</p>

<p>A general answer would be helpful. Moreover, I'm trying to move to R (rather than Stata), so advice on replicating the analysis and/or addressing overfitting with R would be gratefully received.</p>

<p>ADDENDUM1:
I've partially worked out how to replicate analysis in R. Haven't yet replicated lnskew0, though <a href=""https://rpubs.com/chrisbrunsdon/skewness"" rel=""nofollow"">https://rpubs.com/chrisbrunsdon/skewness</a> looks similar.</p>

<pre><code>p2.df &lt;- read.table(""data_above.csv"", header=TRUE, sep="","")

library(foreign)
library(sandwich)
library(lmtest)
library(DAAG)

options(digits = 8)  # for more exact comparison with Stata's output

#create ln FibrosisP (need to replicate lnskew0 from Stata - manual k entered here from Stata calculation)

p2.df$FibrosisPln &lt;- log(p2.df$FibrosisP-0.0116473)

p1.df &lt;- p2.df[c(""PatientID"", ""DiseaseG"", ""WallG"", ""RegionG"", ""FibrosisPln"")]

p1.df$DWR &lt;- paste(p1.df$DiseaseG, p1.df$WallG, p1.df$RegionG)

p.df &lt;- pdata.frame(p1.df, index = c(""PatientID"", ""DWR""), drop.index = F, row.names = T)

# tools_reg.R from http://www.existencia.org/pro/?p=134
source(""tools_reg.R"")
mod &lt;- lm(FibrosisPln~factor(DiseaseG)+factor(WallG)+factor(RegionG),data=p.df)
get.coef.clust(mod, p.df$PatientID) # identical to Stata output
</code></pre>

<p>ADDENDUM2:
I have performed 10-fold cross-validation with CVlm from R package DAAG. However, I am uncertain how to interpret the results. Does the presence of overlapping lines in the plot for all 10 folds suggest no overfitting is present?</p>

<pre><code>library(foreign)
library(sandwich)
library(lmtest)
library(DAAG)
CVlm(df=p.df, form.lm=mod, m=10, plotit = c(""Observed"",""Residual""), main=""Small symbols show cross-validation predicted values"", legend.pos=""topleft"", printit=TRUE)
</code></pre>

<p><img src=""http://i.stack.imgur.com/HqTzi.png"" alt=""CVlm plot""></p>

<pre><code>t test of coefficients:

                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)        -3.2703     0.1441  -22.70  &lt; 2e-16 ***
factor(DiseaseG)C   0.9241     0.2084    4.43  2.5e-05 ***
factor(WallG)Y      0.3286     0.1681    1.95   0.0536 .  
factor(WallG)Z      0.4356     0.1326    3.28   0.0014 ** 
factor(RegionG)Q    0.4401     0.1421    3.10   0.0026 ** 
factor(RegionG)R    0.1496     0.0859    1.74   0.0847 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>ADDENDUM3:
After a lot of searching, the only explanation I could find online (which I presume is accurate) is given at <a href=""http://rstatistics.net/regression-modelling/"" rel=""nofollow"">http://rstatistics.net/regression-modelling/</a></p>

<blockquote>
  <p>""The fitted lines of different colors are parallel and on-top of each other. Indicating a stable model direction and less influence of outliers.""</p>
</blockquote>
"
"0.068136080998913","0.0649295895722714","160021","<p><a href=""http://stats.stackexchange.com/questions/18480/interpretation-of-log-transformed-predictor"">Interpretation of log transformed predictor</a> neatly explains how to interpret a <code>log</code> transformed predictor in OLS. Does the interpretation change if there are 0s in the data and the transformation becomes <code>log(1 + x)</code> instead? </p>

<p>Some authors (e.g. Fox and Weisberg 2011) recommend adding a <code>start</code> (i.e. a positive constant) if a <code>log</code> transformation is necessary to correct skewness and improve symmetry, but the data contains zeros. </p>

<p>Consider a variation of the <code>Ornstein</code> example in CAR (p. 303): </p>

<pre><code>require(car)
data(Ornstein)
boxplot(Ornstein$interlocks, horizontal = T) 
</code></pre>

<p><img src=""http://i.stack.imgur.com/j8XSa.png"" alt=""enter image description here""></p>

<p>The data is clearly right skewed, and contains 0s. </p>

<pre><code>summary(powerTransform(1 + Ornstein$interlocks))
    ## bcPower Transformation to Normality 
    ## 
    ##                         Est.Power Std.Err. Wald Lower Bound Wald Upper Bound
    ## 1 + Ornstein$interlocks    0.1248    0.053           0.0209           0.2287
## 
## Likelihood ratio tests about transformation parameters
##                              LRT df      pval
## LR test, lambda = (0)   5.502335  1 0.0189911
## LR test, lambda = (1) 262.431991  1 0.0000000
</code></pre>

<p>The <code>powerTransform()</code> function suggests that a <code>log(1 + x)</code> transformation here could be useful. </p>

<pre><code>boxplot(log(1 + Ornstein$interlocks), horizontal = T)
</code></pre>

<p><img src=""http://i.stack.imgur.com/W3YlX.png"" alt=""enter image description here""></p>

<p>As you can see, symmetry is indeed improved. </p>

<p><strong>Question:</strong> If this transformed variable were to be included in an OLS regression as an IV, would the coefficient estimates still have the usual interpretation of <code>log</code> transformed variables? </p>
"
"0.0609427635336005","0.0580747904139041","160096","<p>I am going through the LAB section Â§6.6 on Ridge Regression/Lasso in the book <a href=""http://www-bcf.usc.edu/~gareth/ISL/index.html"" rel=""nofollow"">'An Introduction to Statistical Learning with Applications in R'</a> by James, Witten, Hastie, Tibshirani (2013).</p>

<p>More specifically, I am trying to do apply the scikit-learn <code>Ridge</code> model to the 'Hitters' dataset from the R package 'ISLR'. I have created the same set of features as shown in the R code. However, I cannot get close to the results from the <code>glmnet()</code> model. I have selected one L2 tuning parameter to compare. ('alpha' argument in scikit-learn).</p>

<p><strong>Python:</strong><BR></p>

<pre><code>regr = Ridge(alpha=11498)
regr.fit(X, y)
</code></pre>

<p><a href=""http://nbviewer.ipython.org/github/JWarmenhoven/ISL-python/blob/master/Notebooks/Chapter%206.ipynb"" rel=""nofollow"">http://nbviewer.ipython.org/github/JWarmenhoven/ISL-python/blob/master/Notebooks/Chapter%206.ipynb</a></p>

<p><strong>R:</strong></p>

<p>Note that the argument <code>alpha=0</code> in <code>glmnet()</code> means that a L2 penalty should be applied (Ridge regression). The documentation warns not to enter a single value for <code>lambda</code>, but the result is the same as in ISL, where a vector is used.</p>

<pre><code>ridge.mod &lt;- glmnet(x,y,alpha=0,lambda=11498)
</code></pre>

<p>What causes the differences?</p>

<p><B>Edit:</B><BR>
When using <code>penalized()</code> from the penalized package in R, the coefficients are the same as with scikit-learn.</p>

<pre><code>ridge.mod2 &lt;- penalized(y,x,lambda2=11498)
</code></pre>

<p>Maybe the question could then also be: 'What is the difference between <code>glmnet()</code> and <code>penalized()</code> when doing Ridge regression?</p>

<p><B>New python wrapper for actual Fortran code used in <em>R</em> package glmnet</B><BR>
<a href=""https://github.com/civisanalytics/python-glmnet"" rel=""nofollow"">https://github.com/civisanalytics/python-glmnet</a></p>
"
"0.0914141453004008","0.0871121856208561","160109","<p>I'm trying to fit a (logistic) regression model to predict the successful funding of crowdfunding ventures (0/1) based on a series of IV with different level of measurement. One of these IVs is a categorial variable that indicates the nature of the venture (like technology, media, etc.). From visually inspecting the IVs I can see that the other IVs take different values relative to the venture category.</p>

<p><strong>My question is, how do I properly incorporate the categorial variable into a regression model (using R)?</strong></p>

<p>I do realize I could simply add the categorial variable with all the other IVs like this:</p>

<pre><code>glm(success ~ IV1 + IV2 + IV3 + ... + ventureCategory +, data=data, family=""binomial"")
</code></pre>

<p>From what I understand this would only return the overall influence of the venture categories on successful funding, but not the potential interaction between venture category and IV1 to IVn.</p>

<p>If I was to include interactions, how would I know which interactions to add?
And if I assume the venture category to be influential on all the IVs do I include separate interactions for every IV with the ventureCategory like this</p>

<pre><code>glm(success ~ IV1 + Iv1:ventureCategory + IV2 + Iv2:ventureCategory + ... + ventureCategory, data=data, family=""binomial"")
</code></pre>

<p>It seems that model quickly becomes quiet messy and I got something mixed up here.</p>

<p>Finally, I read two studies, who include this exact variable differently.
Here is a link to one of them: <a href=""https://balsa.man.poznan.pl/indico/getFile.py/access?contribId=5&amp;resId=0&amp;materialId=paper&amp;confId=44"" rel=""nofollow"">https://balsa.man.poznan.pl/indico/getFile.py/access?contribId=5&amp;resId=0&amp;materialId=paper&amp;confId=44</a></p>

<p>Their model simply states they controled for categories (see table p.12) without providing any coefficients or p values. I read this in other papers too, but do not understand how the control variable actually contributes to the model in this case.</p>

<p>I also read through some of the threads here, but couldn't find anything that really helped me understand the underlying rationale.</p>

<p>Update: I have 5 IV in total, excluding the categorial one.</p>

<p>Best</p>
"
"0.0304713817668003","0.029037395206952","160163","<p>This is probably a fairly silly question, but I have the following regression model:</p>

<pre><code>&gt; print(summary(step1))

Call:
lm(formula = model1, data = newdat1)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.66219 -0.00725 -0.00725 -0.00725  1.28056 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.15116    0.05622   2.689  0.00778 ** 
i8           0.18362    0.07986   2.299  0.02253 *  
i7           0.01749    0.08089   0.216  0.82903    
i6           0.51675    0.06042   8.553 3.27e-15 ***
i5           0.13824    0.06254   2.210  0.02823 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.3131 on 198 degrees of freedom
Multiple R-squared:  0.7797,    Adjusted R-squared:  0.7753 
F-statistic: 175.2 on 4 and 198 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>I am attempting to obtain the B-values of the regression (not the betas), which of these values are the B's?  If they are not found in the model how are they obtained?  Thank you!</p>
"
"0.091874672876503","0.0963061447907242","160316","<p>I have a dataset consisting of about 600 observations. Each observation has around 100 attributes. One of the attributes I want to predict. Since the attribute that I want to predict can only have non-negative integer values, I was looking for ways to predict count data and found that there are various options, such as Poisson regression or negative binomial regression.</p>

<p>For my first try I used negative binomial regression in <code>R</code>:</p>

<pre><code>#First load the data into a dataset
dataset &lt;- test_observations[, c(5:8, 54)]

#Create the model
fm_nbin &lt;- glm.nb(NumberOfIncidents ~ ., data = dataset[10:600, ] )
</code></pre>

<p>I then wanted to see how to predicted values look like:</p>

<pre><code>#Create data to test prediction
newdata &lt;- dataset[1:10, ]

#Do the prediction
predict(fm_nbin, newdata, type=""response"")
</code></pre>

<p>Now the problem is the output looks like this:</p>

<pre><code>     1         2         3         4         5         6         7         8         9        10 
0.2247337 0.2642789 0.2205408 0.2161833 0.1794224 0.2081522 0.2412996 0.2074992 0.2213011 0.2100026 
</code></pre>

<p>The problem with this is that I expected that the predicted values are integers, since that is the whole purpose of using a negative binomial regression. What am I missing here?</p>

<p>Furthermore, I would like to evaluate my predictions in terms of mean squared error and mean absolute error, as well as a correlation coefficient. However, I couldn't find a way to get these easily, without doing all the calculations manually. Is there any built-in function for this?</p>
"
"0.068136080998913","0.0649295895722714","160435","<p>I'm diving into arima models and was trying to repreduce the results of auto regression.</p>

<p>here is a reproducable example:</p>

<pre><code>set.seed(1)
z=arima.sim(n = 101, list(ar = c(0.8)))
</code></pre>

<p>when running ar(1) without an intercept </p>

<pre><code>&gt; ceof(arima(z, order = c(1,0,0),include.mean =FALSE))
ar1 
0.7622461
</code></pre>

<p>when comparing to a linear regression </p>

<pre><code>&gt; coef(lm(z[2:101] ~ z[1:100] + 0))
z[1:100] 
0.7586725 
</code></pre>

<p>which are very similar and can be explained by the different methods used.
However when I do this comparison with models that include an intercept, I get again similar results in the ar1 coefficient but very different measures for the intercept. while the intercept that I get in the arima model is the one that makes less sense to me.</p>

<pre><code>&gt; coef(arima(z, order = c(1,0,0)))
      ar1 intercept 
0.7274511 0.4241322 
&gt; coef(lm(z[2:101] ~ z[1:100]))
(Intercept)    z[1:100] 
  0.1578015   0.7130261 
</code></pre>

<p>Any ideas on these differencing and in what way the arima procedure is different?</p>
"
"0.0545088647991304","0.0649295895722714","160545","<p>I recently ran two tests in R - one using glm() and one using lm() with the goal being to test the relationship between a binary response and binary predictor.  I ran glm() first and got an estimate of -0.68 for the predictor coefficient which I thought was pretty good.  P&lt;.05 and AIC of 653.  </p>

<p>When I ran lm() however I got an estimate of -.14, a multiple r-squared of .008, P&lt;.05.  </p>

<p>My understanding is that linear regression is usually a poor choice for a categorical response compared with logistic regression, but when is this not the case? I noticed in this post <a href=""http://statisticalhorizons.com/linear-vs-logistic"" rel=""nofollow"">http://statisticalhorizons.com/linear-vs-logistic</a> that the author states there's middle ground where it does make sense to use linear regression.  Are there any common rules (or rules of thumb you personally use) that determine when to try out linear regression on a categorical response?  Do any of these differ from the author's cases?</p>
"
"0.114267681625501","0.101630883224332","160675","<p>I'm interested in determining both the slope regression coefficient and plotting regression lines for autocorrelated time-series datasets of rainfall.  Specifically, I'd like to identify the best approach in R that would allow me to visualize the regression line on the original (undifferenced) time-series plot when I need to difference the data to remove stationarity (i.e, when d>0 in an arima model).</p>

<p>As a start, I'm exploring the use of auto.arima (from the forecast package) and sarima (from the astsa package) which can output regression coefficients in the presence of autocorrelation.</p>

<p>For example:</p>

<ol>
<li><p>Using auto.arima. The 'drift' of -5.009 represents the slope (see <a href=""http://robjhyndman.com/hyndsight/arima-trends/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/arima-trends/</a>) </p>

<pre><code>&gt; min.ar &lt;- auto.arima(dec.yr.mmin$min_prcp)
&gt; summary(min.ar)

Series: dec.yr.mmin$min_prcp 
ARIMA(1,1,0) with drift         

Coefficients:
          ar1    drift
      -0.5138  -5.0089
s.e.   0.2465   5.7986

sigma^2 estimated as 949:  log likelihood=-57.82
AIC=121.64   AICc=124.31   BIC=123.34

Training set error measures:
                     ME     RMSE      MAE       MPE     MAPE      MASE       ACF1
Training set -0.9479987 28.52129 23.83494 -2.484233 16.12547 0.7957998 -0.2617352
</code></pre></li>
<li><p>Using sarima to fit the model and output the slope</p>

<pre><code>  &gt; fit.min &lt;- sarima(dec.yr.mmin$min_prcp, 1,1,0,                       reg=dec.yr.mmin$decade)
initial  value 3.542448 
  iter   2 value 3.488927
  iter   3 value 3.386967
  iter   4 value 3.383464
  iter   5 value 3.382408
  iter   6 value 3.382051
  iter   7 value 3.382024
  iter   8 value 3.382020
  iter   9 value 3.381925
  iter   9 value 3.381925
  iter   9 value 3.381925
  final  value 3.381925 
  converged
  initial  value 3.400729 
  iter   2 value 3.399523
  iter   3 value 3.399490
  iter   4 value 3.399488
  iter   4 value 3.399488
  iter   4 value 3.399488
  final  value 3.399488 
  converged
</code></pre></li>
</ol>

<p>3.Output coefficients</p>

<pre><code>      &gt; fit.min$fit$coef
             ar1       xreg 
      -0.5137696 -0.5009045 
</code></pre>

<ol start=""4"">
<li><p>For comparison, this is the output from an OLS regression which may give an incorrect slope due to autocorrelation.</p>

<pre><code>  &gt; m3 &lt;- lm(dec.yr.mmin$min_prcp ~ dec.yr.mmin$decade)
  &gt; summary(m3)

  Call:
  lm(formula = dec.yr.mmin$min_prcp ~ dec.yr.mmin$decade)

  Residuals:
      Min      1Q  Median      3Q     Max 
  -45.504  -8.048   1.892  13.650  38.357 

  Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)   
  (Intercept)        1014.1570   319.9461   3.170  0.00807 **
  dec.yr.mmin$decade   -0.4222     0.1580  -2.672  0.02032 * 
  ---
  Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

  Residual standard error: 23.83 on 12 degrees of freedom
  Multiple R-squared:  0.3731,  Adjusted R-squared:  0.3209 
  F-statistic: 7.142 on 1 and 12 DF,  p-value: 0.02032
</code></pre>

<p>`</p></li>
</ol>

<p>The output from sarima identifies the slope coefficient and also intercept when d=0. When differencing is required (e.g., ARIMA (1,1,0) as output above), sarima only outputs the slope. </p>

<p>My question: when d = 1 or more, what approaches in R would allow me to add/visualize the regression line onto the original undifferenced time-series plot. Is it possible to derive the fitted values of the regression line, or derive intercept values from sarima/auto.arima or other package?</p>

<p>Many thanks in advance for your suggestions.</p>
"
"0.0806196982594614","0.076825726438694","160721","<p>I am trying to replicate the <a href=""http://www.pnas.org/content/110/15/5802.full"" rel=""nofollow"">Kosinski, Stillwell, &amp; Graepel (2013) study</a> about predicting private traits and attributes from Facebook like data for study purposes. First I have admit, however, that I am quite a newbie to data science and building prediction models in R.</p>

<p>My Data:</p>

<ol>
<li>A sparse matrix (dim 237 x 43232) that contains 237 users and if they liked one of the 43232 fb-pages or not (indicated by 1 for liked and 0 for not-liked). The row-names contain user.ids.</li>
<li>A data.frame with columns containing the user.ids, age, gender, and several survey scores, for this example lets take the SOP2 score (optimism-pessimism score).</li>
</ol>

<p>My goal is to predict the SOP2 score with the user likes. Kosinski et al. describe their model building in <a href=""http://www.pnas.org/content/110/15/5802/F1.expansion.html"" rel=""nofollow"">this graphic</a>. So far I have done a SVD using the irlab R package:</p>

<pre><code>Comps.likes &lt;- irlba(Likes.matrix, nu = 100, nv = 100)
</code></pre>

<p>And this is where I am stuck .. how to go on from here?</p>

<p>From what I assume I should get to something like this:</p>

<pre><code>fit &lt;- lm(SOP2 ~ Comps.likes$d, data = someDataFrame)
</code></pre>

<p>or an equivalent using the caret package.</p>

<p>What I am trying to figure out:</p>

<ol>
<li>The step missing is how to get from the Comps.likes (SVD step) to
the regression formula with a coefficient for each Facebook like to
predict SOP2.</li>
<li>The step to actually predict SOP2 from a user vector
with the likes per user.id.</li>
</ol>

<p>Any help or hints to further resources?</p>
"
"0.0609427635336005","0.0580747904139041","161121","<p>I have an R question. I'm wondering why there is a difference in p-values in the original regression analysis using lm versus in the k-fold cross-validation using the DAAG package.</p>

<p>So, first I run the regression.</p>

<pre><code>Model = lm(ExampleData$DependentVariable ~ ExampleData$IV1  + 
           ExampleData$IV2  + ExampleData$IV3  + ExampleData$IV4)
</code></pre>

<p>This gives me the p-values for the predictors.</p>

<pre><code>Coefficients:
                Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)     -55.6644    23.4690  -2.372  0.01958 * 
ExampleData$IV1   1.2118     0.6277   1.931  0.05631 .
ExampleData$IV2   6.2636     2.0563   3.046  0.00295 **
ExampleData$IV3   2.1531     0.7490   2.875  0.00492 **
ExampleData$IV4  -5.4468     1.8859  -2.888  0.00473 **
</code></pre>

<p>Then, I go to cross-validate the model using cv.lm in the DAAG package.</p>

<pre><code>cv.lm(df=ExampleData, Model_forCV, m=5)
</code></pre>

<p>This gives me the cross-validation results along with the p-values for the predictors.</p>

<pre><code>Response: DependentVariable
           Df Sum Sq Mean Sq F value  Pr(&gt;F)    
IV1         1  26755   26755    3.23 0.07541 .  
IV2         1 104332  104332   12.58 0.00059 ***
IV3         1  36119   36119    4.36 0.03938 *  
IV4         1  69167   69167    8.34 0.00473 ** 
Residuals 102 845806    8292   
</code></pre>

<p>Why are the p-values different?</p>

<p>Thank you! </p>
"
"0.0545088647991304","0.0649295895722714","161234","<p>Les say I have a data set with several measures and one factor (classification) like the one bellow (for the sake of simplicity, I'm simulating 10 rows and 5 variables only)</p>

<p>I'd like to know how much each variable contribute to the overall classification. I thought about running a linear regression, but I'm wondering if it makes sense to use it to ""explain"" a factor </p>

<p>When I run <code>lm(classification ~ ., data =data)</code> I get a warning   saying </p>

<pre><code>Warning messages:
1: In model.response(mf, ""numeric"") :
  using type = ""numeric"" with a factor response will be ignored
2: In Ops.factor(y, z$residuals) : â€˜-â€™ not meaningful for factors
</code></pre>

<p>but I do get a result (intercept and coefficients for each variable).</p>

<p>My questions are: do they make any sense? And: is there a better way to get to the answer I'm looking for?</p>

<pre><code>   classification  variable_1  variable_2 variable_3 variable_4 variable_5
1               5 -0.90174176 -0.64796703  1.2106427 -0.9229394 -0.6578518
2               5  1.75760214  0.18486432  0.2018499  0.1301168 -0.6510428
3               8 -0.29445029 -0.23108298 -2.6244614  0.3745607  0.3124868
4               4  0.78639724  1.04943276 -0.6047869 -0.4275781  0.6395614
5               3 -2.06554518  0.07336021  2.8142735  1.0558045 -0.1818247
6               4  0.04374419 -0.13775079  0.6132946 -0.5890983  1.9965892
7              10 -1.46731867  1.00367532 -0.8626940 -1.8378582  0.2702731
8               8  0.27206146 -0.13775707  2.6827356  1.5554446  0.1549394
9               5  0.58075881  2.03567118  0.2056770 -0.2935464 -1.3586576
10              9  0.57725709 -0.25396790  0.6640166 -1.9626897  0.3650243
</code></pre>

<p>Code to reproduce it:</p>

<pre><code>data &lt;- data.frame(classification=sample(3:10,replace=TRUE,size=10))

for(i in 1:5){
  data[,paste0(""variable_"",i)]&lt;-rnorm(10)
}
</code></pre>

<p>thanks</p>
"
"0.0304713817668003","0.029037395206952","161517","<p>I am trying to reproduce a table 3.3 in Elements of statistical learning. Specifically, I am trying to get the coefficient estimates for ridge regression and lasso. I know that the estimate can be a bit off depending on the seeding value, but I personally think it is significantly off. The code is below, and would appreciate if anyone can give me some help. </p>

<p>A link to data is here : <a href=""http://statweb.stanford.edu/~tibs/ElemStatLearn/index.html"" rel=""nofollow"">http://statweb.stanford.edu/~tibs/ElemStatLearn/index.html</a> </p>

<p>And a link to textbook is here : <a href=""http://web.stanford.edu/~hastie/local.ftp/Springer/OLD/ESLII_print4.pdf"" rel=""nofollow"">http://web.stanford.edu/~hastie/local.ftp/Springer/OLD/ESLII_print4.pdf</a></p>

<p>Relevant pages are page 61 and 63.</p>

<pre><code>library(glmnet)
Prostate &lt;- read.table(""prostate.data"", sep = """")
train &lt;- Prostate[,10]
Y &lt;- Prostate[,9]
X &lt;- Prostate[, -c(9,10)]
X.scaled &lt;- scale(X, TRUE, TRUE)

set.seed(1)
cv1 &lt;- cv.glmnet(X.scaled[train,], Y[train], alpha = 0)
plot(cv1)
ridge.r &lt;- glmnet(X.scaled[train,], Y[train], alpha = 0, lambda=cv1$lambda.min)
coef(ridge.r) 
                    s0
(Intercept)  2.467025897
lcavol       0.486393030
lweight      0.599533912
age         -0.014470377
lbph         0.137317539
svi          0.674949305
lcp         -0.110476104
gleason      0.019892200
pgg45        0.006930003

cv2 &lt;- cv.glmnet(X.scaled[train,], Y[train], alpha = 1)
plot(cv2)
lasso.r &lt;- glmnet(X.scaled[train,], Y[train], alpha = 1, lambda =cv2$lambda.min)
coef(lasso.r)
                      s0
(Intercept)  2.466987400
lcavol       0.556835090
lweight      0.605923475
age         -0.016916532
lbph         0.138909124
svi          0.700170345
lcp         -0.170851957   
gleason      .           
pgg45        0.008051421
</code></pre>

<p>The output table from the textbook is below.
<img src=""http://i.stack.imgur.com/4Nooa.png"" alt=""enter image description here""></p>
"
"0.0215465206794286","0.0410650781176591","161614","<p>I want to solve the first exercice of the Multiple Regression Chapter of R. Hyndman's online book on Time Series Forecasting (see <a href=""https://www.otexts.org/fpp/5/8"" rel=""nofollow"">https://www.otexts.org/fpp/5/8</a>). I use <code>R</code> with <code>fpp</code> package as wanted in the exercise.</p>

<p>I am blocked in the following question:
c. Use R to fit a regression model to the logarithms of these sales data with a linear trend, seasonal dummies and a â€œsurfing festivalâ€ dummy variable.</p>

<p>Indeed, I don't know how to make the function <code>tslm</code> work with my dummy vector for the surfing festival. Here is my code.</p>

<pre><code>library(fpp)
log_fancy = log(fancy)
dummy_fest_mat = matrix(0, nrow=84, ncol=1)
for(h in 1:84)
    if(h%%12 == 3)   #this loop builds a vector of length 84 with
        dummy_fest_mat[h,1] = 1   #1 corresponding to each month March
dummy_fest_mat[3,1] = 0 #festival started one year later

dummy_fest = ts(dummy_fest_mat, freq = 12, start=c(1987,1))
fit = tslm(log_fancy ~ trend + season + dummy_fest)
</code></pre>

<p>When I do <code>summary(fit)</code>, I see that the regression coefficients have been well calculated, but when I continue with <code>forecast(fit)</code>
I get the following error : </p>

<pre><code>Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) : 
  variables have not equal length (found for 'factor(dummy_fest)')
In addition: Warning message:
'newdata' had 50 rows but variables found have 84 rows 
</code></pre>

<p>But what is even stranger is that when I do <code>forecast(fit, h=84)</code>, it works!!
I don't know what is happening here, can someone explain me?</p>
"
"0.0609427635336005","0.0580747904139041","161797","<p>Say I have some predictors, and I know how they affect some dependent variable:</p>

<pre><code>#predictors
x1&lt;- seq(0,10,0.1)
x2&lt;-runif(101,0,1)
#specify how predictors affect dependent variable y
y&lt;- 15*x1 + 10*x1*x2
#introduce random error
y.err&lt;- rnorm(101,0.01)
y&lt;- y + y.err
</code></pre>

<p>I can then model <code>y</code> as a function of <code>x1</code> and <code>x2</code> like this:</p>

<pre><code>fit&lt;- lm(y ~ x1 + x1*x2)
</code></pre>

<p>which yields this output:</p>

<pre><code>summary(fit)

Call:
lm(formula = y ~ x1 + x1 * x2)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.76646 -0.59886 -0.09115  0.70549  2.85311 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.38875    0.41630   0.934    0.353    
x1          14.93601    0.07409 201.585   &lt;2e-16 ***
x2          -0.74815    0.79518  -0.941    0.349    
x1:x2       10.10469    0.13698  73.768   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 1.025 on 97 degrees of freedom
Multiple R-squared:  0.9997,    Adjusted R-squared:  0.9997 
F-statistic: 1.184e+05 on 3 and 97 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>So, total model $R^2$ is 0.997. </p>

<p>I would like to know what percentage of that $R^2$ value can be attributed to x1, x2, and the interaction of x1 and x2. I am also aware of a previous post on this topic linked <a href=""http://stats.stackexchange.com/questions/60872/how-to-split-r-squared-between-predictor-variables-in-multiple-regression"">here</a>, where the user has an identical question. However, the solution proposed (as I read it) was to run the correlations individually, square them, and they will sum to the full model $R^2$. This is not the case here. </p>
"
"0.168628197212325","0.14691888987588","161941","<p>Something I rather vaguely asked a few months back, saw the tumbleweed roll by (actually hacked some hardware in the time, to get a few answers) before the question was deleted, so I'll try again on a slightly more specific note, as there will be a cleaner / quicker way to get the answer.</p>

<p>If I know a regression equation derives a particular answer from a given set of variables and coefficients, but I don't know the values for at least one set, how can I model this in R / get R to have a guess at deriving the unknowns.    </p>

<p>I ask as I was curious as to the maths embedded in number of Bioelectrical Impedance Analysis (BIA) devices I had around (A mix of Salter and Withings).
A quick Google revealed <a href=""http://pubs.sciepub.com/ijcn/2/1/1/"" rel=""nofollow"">dozens of published regression formula</a>, <a href=""http://ajcn.nutrition.org/content/64/3/436S.full.pdf"" rel=""nofollow"">to estimate: Total body Water / Fat / Lean Mass</a>, so was interested in which had made their way into the devices eg.</p>

<blockquote>
  <p>Lukaski &amp; Bolonchuk's (1988):</p>
  
  <p><strong>TBW</strong> = (0.372 * HeightCM * HeightCM /
  Resistance)  + (0.142 * massKg) - (0.069 * ageYears) + (3.05 * isMale)</p>
</blockquote>

<p>...</p>

<blockquote>
  <p>Matias et al. (2015): </p>
  
  <p><strong>TBW</strong> = 0.286 + (0.195 * HeightCM * HeightCM /
  Resistance) + (0.385 * massKg) + (5.086 * isMale)</p>
</blockquote>

<p>Anyway the devices show the values of all but any Resistance (R) and possibly imaginary Rectance (X) values used (Withings), so thought I'd pester / learn a bit of R to see if there was an alternative to an afternoon without socks, but with a multimeter, a few bits of wire and a tweaked iPad.</p>

<p><strong>FYI:</strong> My original five min play, in April, with a <strong>Salter 9141 WH3R</strong> suggested that:-</p>

<blockquote>
  <p><strong>BodyFat %</strong>  ~= x + (0.1 * AgeYears) + (0.4 * MassKg) - (8 * isMale) + y</p>
  
  <p><strong>TBW %</strong>  ~= x - (0.2 * AgeYears) - (0.27 * MassKg) + (7.58 * isMale) + y</p>
</blockquote>

<p>where x:  is some combination of: </p>

<p><strong>a * HeightCM^2 [+/] b * Resistance + c * HeightCM</strong></p>

<p>and with a <strong>Withings WS-50</strong> that:-</p>

<blockquote>
  <p><strong>BodyFat %</strong>  ~= X + (0.12 * AgeYears) + (0.29 * BodyMassKg) - (16.64 * isMale) + (5 * isJapanese) + z</p>
</blockquote>

<p>where X:  is some combination of: </p>

<p><strong>a * HeightCM^2 [+/] b * Resistance + c * HeightCM + d * Reactance</strong></p>

<p>So is there a way that I a can ger <strong>R</strong> to have a guess at solving these unknowns, or must I stick with a: screwdriver, multimeter, tweaked iPad and / or logging proxy (mitProxy / HoneyProxy).</p>

<pre><code># Quick play to try to identify a few of the constants used by the Salter 9141 WH3R body fat scale
# sex ( 1 = male, 0 = female)
sex &lt;- c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)
ageYrs &lt;- c(33, 33, 43, 43, 53, 53, 33, 43, 43, 53, 33, 43, 53, 33, 33, 43, 43, 53, 53, 33, 43, 43, 53, 33, 43, 53)
heightCM &lt;- c(191, 191, 191, 191, 191, 191, 181, 181, 181, 181, 171, 171, 171, 191, 191, 191, 191, 191, 191, 181, 181, 181, 181, 171, 171, 171)
heightCM2 &lt;- heightCM * heightCM
massKg &lt;- c(81.6, 83, 81.6, 83, 81.6, 83, 81.6, 81.6, 83, 81.6, 81.6, 81.6, 81.6, 81.6, 83, 81.6, 83, 81.6, 83, 81.6, 81.6, 83, 81.6, 81.6, 81.6, 81.6)
bodyWaterPct &lt;- c(62, 61.6, 59.9, 59.6, 57.9, 57.5, 58, 55.9, 55.5, 53.9, 53.2, 51.2, 49.2, 54.3, 54, 52.3, 52, 50.3, 50, 50.4, 48.4, 47.9, 46.4, 45.7, 43.6, 41.6)
bodyFatPct &lt;- c(17.1, 17.7, 18.1, 18.7, 19.2, 19.8, 22.6, 23.7, 24.2, 24.7, 29.1, 30.1, 31.1, 25.2, 25.7, 26.2, 26.7, 27.2, 27.7, 30.5, 31.6, 32.3, 32.9, 37, 38.1, 39.1)

bodyFat.Salter = data.frame(sex, ageYrs, heightCM, heightCM2, massKg, bodyWaterPct, bodyFatPct)
bodyFat.Salter
summary(bodyFat.Salter)
fitBodyFat1 &lt;- lm ( bodyFatPct ~ sex + ageYrs + heightCM + massKg, data=bodyFat.Salter)
fitBodyFat2 &lt;- lm ( bodyFatPct ~ sex + ageYrs + heightCM2 + massKg, data=bodyFat.Salter)
fitBodyFat3 &lt;- lm ( bodyFatPct ~ sex + ageYrs + heightCM + heightCM2 + massKg, data=bodyFat.Salter)
fitBodyWater1 &lt;- lm ( bodyWaterPct ~ sex + ageYrs + heightCM + massKg, data=bodyFat.Salter)
fitBodyWater2 &lt;- lm ( bodyWaterPct ~ sex + ageYrs + heightCM2 + massKg, data=bodyFat.Salter)
fitBodyWater3 &lt;- lm ( bodyWaterPct ~ sex + ageYrs + heightCM + heightCM2 + massKg, data=bodyFat.Salter)
summary(fitBodyFat1)
summary(fitBodyFat2)
summary(fitBodyFat3)
summary(fitBodyWater1)
summary(fitBodyWater2)
summary(fitBodyWater3)

library(glmulti)
fitBodyFatG1 &lt;- glm ( bodyFatPct ~ sex + ageYrs + heightCM + heightCM2 + massKg, data=bodyFat.Salter)
test.model1 &lt;- glmulti(fitBodyFatG1, level = 1, crit=""aicc"")
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>&gt; summary(fitBodyFat1)

Call:
lm(formula = bodyFatPct ~ sex + ageYrs + heightCM + massKg, data = bodyFat.Salter)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.42607 -0.20940  0.08889  0.15588  0.27051 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 101.747418   6.105948  16.664 1.39e-13 ***
sex          -8.007692   0.092545 -86.528  &lt; 2e-16 ***
ageYrs        0.105000   0.005899  17.801 3.80e-14 ***
heightCM     -0.591111   0.006422 -92.051  &lt; 2e-16 ***
massKg        0.400794   0.079446   5.045 5.39e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.2359 on 21 degrees of freedom
Multiple R-squared:  0.9988,    Adjusted R-squared:  0.9986 
F-statistic:  4442 on 4 and 21 DF,  p-value: &lt; 2.2e-16

&gt; summary(fitBodyFat2)

Call:
lm(formula = bodyFatPct ~ sex + ageYrs + heightCM2 + massKg, 
    data = bodyFat.Salter)

Residuals:
    Min      1Q  Median      3Q     Max 
-0.5345 -0.3141  0.1192  0.2024  0.3494 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  4.843e+01  8.409e+00   5.759 1.02e-05 ***
sex         -8.008e+00  1.238e-01 -64.696  &lt; 2e-16 ***
ageYrs       1.050e-01  7.889e-03  13.310 1.05e-11 ***
heightCM2   -1.626e-03  2.365e-05 -68.758  &lt; 2e-16 ***
massKg       3.972e-01  1.063e-01   3.738  0.00121 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.3156 on 21 degrees of freedom
Multiple R-squared:  0.9979,    Adjusted R-squared:  0.9975 
F-statistic:  2481 on 4 and 21 DF,  p-value: &lt; 2.2e-16

&gt; summary(fitBodyFat3)

Call:
lm(formula = bodyFatPct ~ sex + ageYrs + heightCM + heightCM2 + 
    massKg, data = bodyFat.Salter)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.12607 -0.02949 -0.01111  0.03162  0.17393 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 249.776918   9.660823   25.86  &lt; 2e-16 ***
sex          -8.007692   0.026174 -305.94  &lt; 2e-16 ***
ageYrs        0.105000   0.001668   62.94  &lt; 2e-16 ***
heightCM     -2.225111   0.104938  -21.20 3.53e-15 ***
heightCM2     0.004500   0.000289   15.57 1.20e-12 ***
massKg        0.400794   0.022469   17.84 9.48e-14 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.06673 on 20 degrees of freedom
Multiple R-squared:  0.9999,    Adjusted R-squared:  0.9999 
F-statistic: 4.448e+04 on 5 and 20 DF,  p-value: &lt; 2.2e-16

...

Call:
lm(formula = bodyWaterPct ~ sex + ageYrs + heightCM + massKg, 
    data = bodyFat.Salter)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.19309 -0.12797 -0.06759  0.18355  0.31346 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.639382   4.922117   0.130  0.89788    
sex          7.576923   0.074602 101.565  &lt; 2e-16 ***
ageYrs      -0.202500   0.004755 -42.587  &lt; 2e-16 ***
heightCM     0.432037   0.005177  83.460  &lt; 2e-16 ***
massKg      -0.269841   0.064043  -4.213  0.00039 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.1902 on 21 degrees of freedom
Multiple R-squared:  0.999, Adjusted R-squared:  0.9988 
F-statistic:  5087 on 4 and 21 DF,  p-value: &lt; 2.2e-16

&gt; summary(fitBodyWater2)

Call:
lm(formula = bodyWaterPct ~ sex + ageYrs + heightCM2 + massKg, 
    data = bodyFat.Salter)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.25119 -0.16896 -0.09268  0.25881  0.39269 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  3.960e+01  6.631e+00   5.972  6.3e-06 ***
sex          7.577e+00  9.760e-02  77.635  &lt; 2e-16 ***
ageYrs      -2.025e-01  6.221e-03 -32.553  &lt; 2e-16 ***
heightCM2    1.188e-03  1.865e-05  63.728  &lt; 2e-16 ***
massKg      -2.670e-01  8.378e-02  -3.187  0.00443 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.2488 on 21 degrees of freedom
Multiple R-squared:  0.9982,    Adjusted R-squared:  0.9979 
F-statistic:  2970 on 4 and 21 DF,  p-value: &lt; 2.2e-16

&gt; summary(fitBodyWater3)

Call:
lm(formula = bodyWaterPct ~ sex + ageYrs + heightCM + heightCM2 + 
    massKg, data = bodyFat.Salter)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.078205 -0.027991 -0.002778  0.038408  0.069017 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -1.200e+02  6.685e+00  -17.95 8.46e-14 ***
sex          7.577e+00  1.811e-02  418.32  &lt; 2e-16 ***
ageYrs      -2.025e-01  1.154e-03 -175.41  &lt; 2e-16 ***
heightCM     1.763e+00  7.262e-02   24.28 2.58e-16 ***
heightCM2   -3.667e-03  2.000e-04  -18.34 5.63e-14 ***
massKg      -2.698e-01  1.555e-02  -17.35 1.59e-13 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.04618 on 20 degrees of freedom
Multiple R-squared:  0.9999,    Adjusted R-squared:  0.9999 
F-statistic: 6.911e+04 on 5 and 20 DF,  p-value: &lt; 2.2e-16

...

Initialization...
TASK: Exhaustive screening of candidate set.
Fitting...

After 50 models:
Best model: bodyFatPct~1+sex+ageYrs+heightCM+heightCM2+massKg
Crit= -53.5831033999792
Mean crit= 117.663945821469
Completed.
</code></pre>

<p><strong>Withings</strong></p>

<pre><code>...
fitBodyFat1 &lt;- lm ( bodyFatPct ~ sex + ageYrs + heightCM + massKg, data=bodyFat.Withings)
fitBodyFat2 &lt;- lm ( bodyFatPct ~ sex + ageYrs + heightCM2 + massKg, data=bodyFat.Withings)
fitBodyFat3 &lt;- lm ( bodyFatPct ~ sex + ageYrs + heightCM + heightCM2 + massKg, data=bodyFat.Withings)
summary(fitBodyFat1)
summary(fitBodyFat2)
summary(fitBodyFat3)

library(glmulti)
fitBodyFatG1 &lt;- glm ( bodyFatPct ~ sex + ageYrs + heightCM + heightCM2 + massKg, data=bodyFat.Withings)
test.model1 &lt;- glmulti(fitBodyFatG1, level = 1, crit=""aicc"")
</code></pre>

<p><strong>Output:</strong></p>

<pre><code>Call:
lm(formula = bodyFatPct ~ sex + ageYrs + heightCM + massKg, data = bodyFat.Withings)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.3348 -0.5042  0.0597  0.5361  6.5767 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 120.39668    9.97149  12.074  &lt; 2e-16 ***
sex         -16.64221    0.19636 -84.754  &lt; 2e-16 ***
ageYrs        0.12039    0.01490   8.082 2.78e-13 ***
heightCM     -0.61641    0.01784 -34.552  &lt; 2e-16 ***
massKg        0.29065    0.11652   2.494   0.0138 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.9957 on 139 degrees of freedom
Multiple R-squared:  0.9848,    Adjusted R-squared:  0.9844 
F-statistic:  2251 on 4 and 139 DF,  p-value: &lt; 2.2e-16

...

Initialization...
TASK: Exhaustive screening of candidate set.
Fitting...

After 50 models:
Best model: bodyFatPct~1+sex+ageYrs+heightCM+massKg
Crit= 414.926000298605
Mean crit= 752.216409496029
Completed.
</code></pre>
"
"0.068136080998913","0.0649295895722714","162113","<p>Let's say I have factor variable Var_1 with 3 levels, ""1"", ""2"", and ""3"". ""1"" is the base level of the factor variable. So when I run a linear regression, I get a p value for each level of that variable. For example, the P value for ""Var_12"" is .05 and ""Var_13"" is .15. In this situation, the following code will work:</p>

<pre><code>p_values &lt;- summary(model_result)$coefficients[substr(names(coef(model_result)),1,nchar(test_var))==test_var,4]
</code></pre>

<p>Now, if I have another variable factor variable named ""Var11"" with levels ""A"" ""B"" and ""C"", and I run the model, and extract the p values with the code above, I am also going to get the p values from Var_11 because the substring from character 1 to 4 is Var_1, the same for ""Var_1"" and ""Var_11"".</p>

<p>I could rename the variables.
I could change the levels from ""1"" ""2"" and ""3"" to letters.</p>

<p>I don't really fancy either solution, is there a better solution for this? Is there a better way in general to extract the p values for one variable?</p>

<p>Thank you!!</p>
"
"0.0746393370862076","0.0711268017165705","162174","<p>I have performed a Poisson Regression in R, but I got strange results that I cannot find an answer for.
My data is like this:</p>

<pre><code> Aspect_16    Nr_Pereti
1   E         49
2   ENE       73
3   ESE       29
4   N         84
5   NE        77
6   NNE       99
7   NNW       77
8   NW        92
9   S         19
10  SE        20
11  SSE       9
12  SSW       17
13  SW        23
14  W         39
15  WNW       56
16  WSW       25
</code></pre>

<p>The Nr_Pereti variable are counts for each level in the 'Aspect_16' column.
The model formula and results are:</p>

<p>summary(model_nr_exp)</p>

<pre><code>Call:
glm(formula = tab_gen_exp$Nr_Pereti ~ tab_gen_exp$Aspect_16, 
    family = poisson)

Deviance Residuals: 
 [1]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0

Coefficients:
                         Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                3.8918     0.1429  27.243  &lt; 2e-16 ***
tab_gen_exp$Aspect_16ENE   0.3986     0.1847   2.159 0.030886 *  
tab_gen_exp$Aspect_16ESE  -0.5245     0.2343  -2.239 0.025169 *  
tab_gen_exp$Aspect_16N     0.5390     0.1798   2.998 0.002714 ** 
tab_gen_exp$Aspect_16NE    0.4520     0.1827   2.473 0.013386 *  
tab_gen_exp$Aspect_16NNE   0.7033     0.1747   4.026 5.66e-05 ***
tab_gen_exp$Aspect_16NNW   0.4520     0.1827   2.473 0.013386 *  
tab_gen_exp$Aspect_16NW    0.6300     0.1769   3.562 0.000368 ***
tab_gen_exp$Aspect_16S    -0.9474     0.2703  -3.505 0.000456 ***
tab_gen_exp$Aspect_16SE   -0.8961     0.2653  -3.377 0.000733 ***
tab_gen_exp$Aspect_16SSE  -1.6946     0.3627  -4.673 2.97e-06 ***
tab_gen_exp$Aspect_16SSW  -1.0586     0.2815  -3.761 0.000169 ***
tab_gen_exp$Aspect_16SW   -0.7563     0.2528  -2.992 0.002769 ** 
tab_gen_exp$Aspect_16W    -0.2283     0.2146  -1.064 0.287468    
tab_gen_exp$Aspect_16WNW   0.1335     0.1956   0.683 0.494845    
tab_gen_exp$Aspect_16WSW  -0.6729     0.2458  -2.738 0.006182 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 2.9506e+02  on 15  degrees of freedom
Residual deviance: 2.1316e-14  on  0  degrees of freedom
AIC: 120.29

Number of Fisher Scoring iterations: 3
</code></pre>

<p>I don't have any Deviance Residuals for the model, and when I try to plot the model, it gives me this error:</p>

<pre><code>Error in qqnorm.default(rs, main = main, ylab = ylab23, ylim = ylim, ...) : 
  y is empty or has only NAs
In addition: Warning messages:
1: not plotting observations with leverage one:
  1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16 
2: In min(x) : no non-missing arguments to min; returning Inf
3: In max(x) : no non-missing arguments to max; returning -Inf
</code></pre>

<p>What did I do wrong here? Thanks.</p>
"
"0.0304713817668003","0.029037395206952","162699","<p>At the zero-order level, X is not correlated with Y. When I add X and A into a regression analysis to predict Y, only A is a significant predictor. A itself is correlated highly with Y at zero-order. However, when I add an interaction term into the analysis, A*X and X significantly predicts Y, and A no longer predicts. What does this mean? What kind of variable is X, and what kind of variable is Y?</p>

<pre><code># additive model
Call:lm(formula = Y ~ X + A, data = dat)
Residuals:
Min      1Q  Median      3Q     Max 
-30.247  -8.150  -1.416   8.692  24.263 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  67.3713     9.8646   6.830 2.04e-09 ***
X             0.4805     2.1935   0.219    0.827
A           -10.4961     1.8172  -5.776 1.69e-07 ***

# interaction model
Call:lm(formula = Y ~ X * A, data = dat)

Residuals:
Min      1Q  Median      3Q     Max 
-20.778  -8.834  -2.135   8.925  24.410 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)     -17.620     40.762  -0.432   0.6668  
X                30.294     14.057   2.155   0.0345 *
A                10.275      9.841   1.044   0.2999  
X:A              -7.297      3.400  -2.146   0.0352 *
</code></pre>
"
"0.105869651339174","0.100887413963854","163181","<p>I'm running a logistic regression to find a relationship between falls and drugs taken by someone. What happens is that every time I re-run the algorithm it gives a different result. </p>

<p>The table is this:</p>

<pre><code>caseID fallFlag hypSeds antiPsycho antiHypertensives NSAIDs centralMuscleRelax
     1     TRUE   FALSE      FALSE             FALSE  FALSE              TRUE
     2    FALSE    TRUE      FALSE             TRUE   FALSE              FALSE
     3     TRUE   FALSE      TRUE              FALSE  TRUE               TRUE
     4    FALSE   FALSE      FALSE             FALSE  FALSE              FALSE
     5     TRUE   FALSE      TRUE              FALSE  FALSE              FALSE
     6    FALSE   FALSE      FALSE             FALSE  FALSE              FALSE
</code></pre>

<p>The <code>TRUE</code> flags mean that the individual took that medicine, and <code>FALSE</code> otherwise. </p>

<p>The algorithm to perform the logistic regression is the following</p>

<pre><code># Match column labels
cols &lt;- c(""hypSeds"", ""antiPsycho"", ""antiHypertensives"", ""NSAIDs"", ""centralMuscleRelax"")

# Data frame to store the OR and CIs 
coefficients &lt;- data.frame(drugNames=cols)

# This loop run through the match labels
# - perform a logistic regression for each classifier
# - get the OR and CIs coefficients and store the coefficients into a data frame

for(i in 1:length(cols)){
  eqString  &lt;- as.formula(paste(""fallFlag"", cols[i], sep=""~""))
  model     &lt;- glm(eqString, observation, family=""binomial"")
  modelCoef &lt;- exp(cbind(coef(model), confint(model)))

  coefficients$OR[i]    &lt;- modelCoef[2] # odds ratios
  coefficients$CIMin[i] &lt;- modelCoef[4] # lower confidence limit
  coefficients$CIMax[i] &lt;- modelCoef[6] # upper confidence limit
}
</code></pre>

<p>In this algorithm I run a logistic regression on each of the drug categories against the <code>fallFlag</code>. Then, I exponentiate the coefficients to find the odds ratios.  </p>

<p>Every time I restart the R studio and run this algorithm it results differently. For example, here is an actual result:  </p>

<pre><code>      drugNames          OR        CIMin     CIMax
1     hypSeds            1.4347210 1.2534578 1.643824
2     antiPsycho         2.1583970 1.8225014 2.564792
3     antiHypertensives  1.0327465 0.9041444 1.179742
4     NSAIDs             0.9857518 0.8824338 1.101139
5     centralMuscleRelax 0.9597043 0.7240041 1.271461
</code></pre>

<p>But the previous result was:</p>

<pre><code>      drugNames          OR        CIMin     CIMax
1     hypSeds            1.2870853 1.1286756 1.468686
2     antiPsycho         1.9665091 1.6684292 2.324333
3     antiHypertensives  1.1718176 1.0218085 1.344455
4     NSAIDs             1.0263196 0.9178526 1.147658
5     centralMuscleRelax 1.2014783 0.8928298 1.621132
</code></pre>

<p>As you can see the results were very different, and this has been happening every time I load and build the observation table again. It's important to note that all the runs have been performed in the same machine. </p>
"
"0.0609427635336005","0.0580747904139041","164017","<p>I have a set of noisy data that can be described by a functional form.</p>

<p>For each observation f(x), where x is an index that runs from 0-100, I know that f(x)=g(x+1)/g(x)-g(x+1). I would like to find a way of fitting f(x). I also know that f(x) must be smooth. How could I do this?  </p>

<p>My idea is to try and fit this data using penalized splines. I choose a spline basis and a smoothing factor, and then find the coefficients of a regression on the spline basis that produce a curve f(x). I then optimize the coefficients to produce a curve such that when it is transformed it fits my data. A minimal reproducible example in R is below. </p>

<pre><code>require(dplyr)
require(gam)

target = c(0.132167681875765,0.804942648636132,0.60485585022111,1.02164234486286,0.58437549344597,0.88268397325963)

to_optim = function(par,target,knots,smooth,range) {

spline_reg = function(range,knots,par) bs(range,knots) %*% par

distance = function(fitted,target,smooth) sum((fitted-target)^2) +t(as.matrix(diff(fitted))) %*% 
  (as.matrix(diff(fitted))) * (smooth)

fitted = spline_reg(range,knots,par)

crs = fitted/lag(fitted)-fitted
crs=crs[3:length(crs)]
target=target[3:length(target)]

to_ret = distance(crs,target,smooth)

return(to_ret)

}



my_range = seq(1,6)
mypars = 4
smooth=.8

fit = optim(c(runif(mypars)),to_optim,lower=c(rep(-10,mypars)),
            upper=c(rep(10,mypars)),smooth=smooth,knots=mypars,target=target,range=my_range,
            method=""L-BFGS-B"")


par(mfrow=c(1,2))
bs(my_range,mypars) %*% fit$par %&gt;% plot

test = bs(my_range,mypars) %*% fit$par
plot(test/lag(test)-test~target)
abline(0,1)
</code></pre>
"
"0.0430930413588572","0.0410650781176591","164101","<p>I used the multiple regression models to derive the outcomes after using AIC pairwise comparison and deleted the outliers, high leverage points. </p>

<p>And it seems good, the adjusted R^2 acheived 0.9543, see below:</p>

<pre><code>Call:
lm(formula = P ~ V + EF + W + H, data = PS)

Residuals:
Min      1Q     Median      3Q     Max 
-22.448 -14.576   2.949  12.524  26.034 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -2.186e+03  5.209e+02  -4.196 0.000201 ***
V           1.511e-01  1.033e-02  14.633 9.96e-16 ***
EF           1.183e+01  3.239e+00   3.653 0.000917 ***
W           -1.135e+00  4.205e-01  -2.698 0.011028 *  
H            3.192e+01  8.482e+00   3.763 0.000678 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 15.79 on 32 degrees of freedom
(13 observations deleted due to missingness)
Multiple R-squared:  0.9593, Adjusted R-squared:  0.9543 
F-statistic: 188.8 on 4 and 32 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>However, when I was inspecting the residuals, they are fairly high.
With the R^2 higher than 0.95. I expect the residuals should be around 5% but actually not. It's kind of weird. See below for the (actual - fitted)/actual.</p>

<pre><code>tidy((PS$P-fitted(fita2))/PS$P)
         x
1   0.01778472
2  -0.19525412
3  -0.01824948
4  -0.24418585
5  -0.06475068
6   0.65211477
7   0.58158990
8  -0.14876657
9  -0.27050744
10  0.15220738
11  0.14239352
12  0.02274694
13  0.10920921
14  0.04696290
15 -0.07793881
16 -0.32173830
17 -0.60332883
18 -1.47499192
19 -0.70576325
20 -0.04088402
21 -1.11266825
22 -1.13704286
23 -0.72987082
24 -0.50573858
25  0.38938329
26  0.57790490
27  0.21233140
28 -0.42890622
29  0.27630390
</code></pre>

<p>Plot of Residuals vs Fitted:</p>

<p><a href=""http://i.stack.imgur.com/c0KpM.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/c0KpM.png"" alt=""enter image description here""></a></p>

<p>Normal Q-Q plot:</p>

<p><a href=""http://i.stack.imgur.com/R5jBB.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/R5jBB.png"" alt=""enter image description here""></a></p>
"
"0.106649836183801","0.101630883224332","164228","<p>GLM (family=binomial) is foucusd on when the response is dichotomous(yes/no, male/female, etc..). I'm wondering how to judge if the model we built is good eough? As we know, in OLS regression some criterion like R^2 and adjusted R^2 can tell us how much variations are explained but not for GLM. See example I performed:</p>

<pre><code>    &gt; summary(fit.full)
    Call:
    glm(formula = ynaffair ~ gender + age + yearsmarried + children + 
    +religiousness + education + occupation + rating, family = binomial(), 
    data = Affairs)

    Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
    -1.6575  -0.7459  -0.5714  -0.2552   2.5099  

    Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
    (Intercept)    0.71792    0.96165   0.747 0.455336    
    gendermale     0.28665    0.23973   1.196 0.231811    
    age           -0.04494    0.01831  -2.454 0.014142 *  
    yearsmarried   0.09686    0.03236   2.993 0.002758 ** 
    childrenyes    0.37088    0.29466   1.259 0.208147    
    religiousness -0.32230    0.09003  -3.580 0.000344 ***
    education      0.01795    0.05088   0.353 0.724329    
    occupation     0.03210    0.07194   0.446 0.655444    
    rating2       -0.02312    0.58177  -0.040 0.968303    
    rating3       -0.84532    0.57619  -1.467 0.142354    
    rating4       -1.13916    0.55740  -2.044 0.040981 *  
    rating5       -1.61050    0.56649  -2.843 0.004470 ** 
    ---
    Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

    (Dispersion parameter for binomial family taken to be 1)

    Null deviance: 675.38  on 600  degrees of freedom
    Residual deviance: 608.22  on 589  degrees of freedom
    AIC: 632.22
</code></pre>

<p>After removed the insignificant variables, the reduced model look like below,although the AIC decreasd, we still do not know if this is the model with the lowest AIC we can achieved:</p>

<pre><code>    &gt; summary(fit.reduced)
    Call:
    glm(formula = ynaffair ~ age + yearsmarried + religiousness + 
        +rating, family = binomial(), data = Affairs)

    Deviance Residuals: 
    Min        1Q      Median      3Q      Max  
   -1.5117  -0.7541  -0.5722  -0.2592   2.4123  

    Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
    (Intercept)    1.10220    0.71849   1.534 0.125014    
    age           -0.03588    0.01740  -2.062 0.039224 *  
    yearsmarried   0.10113    0.02933   3.448 0.000565 ***
    religiousness -0.32571    0.08971  -3.631 0.000282 ***
    rating2        0.11848    0.57258   0.207 0.836068    
    rating3       -0.70168    0.56671  -1.238 0.215658    
    rating4       -0.96190    0.54230  -1.774 0.076109 .  
    rating5       -1.49502    0.55550  -2.691 0.007118 ** 
    ---
    Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

    (Dispersion parameter for binomial family taken to be 1)

    Null deviance: 675.38  on 600  degrees of freedom
    Residual deviance: 613.63  on 593  degrees of freedom
    AIC: 629.63
</code></pre>

<p>And we perform the ANOVA, suggesting that the reduced model with
four predictors fits as well as the full model:</p>

<pre><code>    &gt; anova(fit.reduced, fit.full, test=""Chisq"")
    Analysis of Deviance Table

    Model 1: ynaffair ~ age + yearsmarried + religiousness + +rating
    Model 2: ynaffair ~ gender + age + yearsmarried + children + 
             +religiousness + education + occupation + rating
    Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)
     1       593     613.63                     
     2       589     608.22  4   5.4124   0.2475
</code></pre>
"
"0.0408816485993478","0.0519436716578171","164362","<p>I regressed the dependent variable Rating (numeric) on Judge which is categorical. The output of the first model is given at the end of the question. Only Judge John Foy and Linda Murphy came out to be significant. In the next step of modeling, should I consider only 3 factors which are John, Linda and rest judges, and run the regression again?</p>

<p>Please guide me to take the next step to create the final model. Ultimately I want to know that should we combine all the factors which are not significant and run the regression again with new set of factors of a categorical variable.</p>

<pre><code>&gt; levels(wine_data$Judge)
[1] ""DaniÃƒÂ¨le Meulders""   ""Francis Schott""      ""Jamal Rayyis""       
[4] ""Jean-Marie Cardebat"" ""John Foy""            ""Linda Murphy""       
[7] ""Olivier Gergaud""     ""Robert Hodgson""      ""Tyler Colman""       

&gt;wine_lm &lt;- lm(Rating ~ Judge, data = wine_data)

&gt;summary(wine_lm)

# Call:
# lm(formula = Rating ~ Judge)
# 
# Residuals:
#    Min     1Q Median     3Q    Max 
# -7.850 -1.625  0.325  1.400  4.825 
# 
# Coefficients:
#                          Estimate Std. Error t value Pr(&gt;|t|)    
# (Intercept)               13.6000     0.5164  26.337  &lt; 2e-16 ***
# JudgeFrancis Schott        1.2500     0.7303   1.712  0.08877 .  
# JudgeJamal Rayyis          1.0750     0.7303   1.472  0.14285    
# JudgeJean-Marie Cardebat  -1.0500     0.7303  -1.438  0.15232    
# JudgeJohn Foy              3.0250     0.7303   4.142  5.4e-05 ***
# JudgeLinda Murphy          2.0500     0.7303   2.807  0.00558 ** 
# JudgeOlivier Gergaud       1.0000     0.7303   1.369  0.17269    
# JudgeRobert Hodgson       -1.4000     0.7303  -1.917  0.05690 .  
# JudgeTyler Colman         -0.4500     0.7303  -0.616  0.53858   
# ---
# Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
# 
# Residual standard error: 2.309 on 171 degrees of freedom
# Multiple R-squared:  0.2713,  Adjusted R-squared:  0.2372 
# F-statistic: 7.957 on 8 and 171 DF,  p-value: 4.345e-09
</code></pre>
"
"0.0545088647991304","0.0519436716578171","164421","<p>I have a balanced panel data (N= 190, T=5) on income and personal characteristics of the householder.</p>

<p>I would like to estimate the coefficients and variances of a temporary and of a permanent income shock. </p>

<p>I fitted an OLS regression model as follows:
$log(income)=sex+age+age^2+study+ public administration + type of house + country$</p>

<p>I then took the residuals and regressed them on a ARMA (1,1) model. The coefficient of AR(1) should be the permanent part and should be around 1, the MA(1) should be the temporary part. I also found that the AR(1) coefficient is only 0.2 instead of about 1. Moreover, AUTOARIMA fits them with an ARMA(2,2). How can I read the coefficients of the ARMA (2,2) to find out the temporary and permanent shock on the income?</p>

<p>I would like also to find the variance of the income (which is the residuals of the ARMA regression) but is it right even if the model is not correct?</p>
"
"0.0430930413588572","0.0410650781176591","164438","<p>I just ran a linear regression in R, where the following is my result:</p>

<pre><code>Call:
lm(formula = Posttest ~ TotalHints + Pretest, 
    data = all)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.51904 -0.09000  0.01243  0.11979  0.41820 

Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)    
    (Intercept)      0.3205183  0.0450642   7.112 1.88e-10 ***
    TotalHints      -0.0007066  0.0003323  -2.127    0.036 *  
    Pretest         0.4323069  0.0770656   5.610 1.88e-07 ***
</code></pre>

<p>All terms in the regression significantly contribute to the model, but the coefficient term of TotalHints is significant with an estimate of... 0?  I'm not sure how to conceptually understand this. Does that mean the model is saying that the TotalHints term significantly had no effect?</p>
"
"0.0408816485993478","0.0519436716578171","164448","<p>I regressed the dependent variable <code>Rating</code> (numeric) on <code>Judge</code> which is categorical. The output of the first model is given at the end of the question. Only Judge John Foy and Linda Murphy came out to be significant. In the next step of modeling, should I consider only 3 factors which are John, Linda and rest judges, and run the regression again?</p>

<p>Please guide me to take the next step to create the final model.
Ultimately I want to know that should we combine all the factors which are not significant and run the regression again with new set of factors of a categorical variable.</p>

<pre><code>wine_lm &lt;- lm(Rating ~ Judge, data = wine_data)

summary(wine_lm)

# Call:
# lm(formula = Rating ~ Judge)
# 
# Residuals:
#    Min     1Q Median     3Q    Max 
# -7.850 -1.625  0.325  1.400  4.825 
# 
# Coefficients:
#                          Estimate Std. Error t value Pr(&gt;|t|)    
# (Intercept)               13.6000     0.5164  26.337  &lt; 2e-16 ***
# JudgeFrancis Schott        1.2500     0.7303   1.712  0.08877 .  
# JudgeJamal Rayyis          1.0750     0.7303   1.472  0.14285    
# JudgeJean-Marie Cardebat  -1.0500     0.7303  -1.438  0.15232    
# JudgeJohn Foy              3.0250     0.7303   4.142  5.4e-05 ***
# JudgeLinda Murphy          2.0500     0.7303   2.807  0.00558 ** 
# JudgeOlivier Gergaud       1.0000     0.7303   1.369  0.17269    
# JudgeRobert Hodgson       -1.4000     0.7303  -1.917  0.05690 .  
# JudgeTyler Colman         -0.4500     0.7303  -0.616  0.53858   
# ---
# Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
# 
# Residual standard error: 2.309 on 171 degrees of freedom
# Multiple R-squared:  0.2713,  Adjusted R-squared:  0.2372 
# F-statistic: 7.957 on 8 and 171 DF,  p-value: 4.345e-09
</code></pre>
"
"0.0691025985081098","0.076825726438694","164541","<p>I am attempting to do a logistic regression bootstrap with R. The problem is I get high SE's. I'm not sure what to do about this or what it means. Does it mean that bootstrap does not work well for my particular data? Here is my code:</p>

<pre><code>get.coeffic = function(data, indices){
  data    = data[indices,]
  mylogit = glm(F~B+D, data=data, family=""binomial"")
  return(mylogit$coefficients)
}

Call:
boot(data = Pres, statistic = logit.bootstrap, R = 1000)

Bootstrap Statistics :
       original      bias    std. error
t1* -10.8609610 -23.0604501  338.048398
t2*   0.2078474   0.4351766    6.387781
</code></pre>

<p>I also want to know that after bootstrapping, how would this help with my final regression model? That is, how do I find what regression coefficient do I use in my final model?</p>

<pre><code>&gt; fit &lt;- glm(F ~ B + D , data = President, family = ""binomial"")
&gt; summary(fit)
Call:
glm(formula = F ~ B + D, family = ""binomial"", data = President)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.7699  -0.5073   0.1791   0.8147   1.2836  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept) -14.57829    8.98809  -1.622   0.1048  
B             0.15034    0.14433   1.042   0.2976  
D             0.13385    0.08052   1.662   0.0965 .
- --
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 23.508  on 16  degrees of freedom
Residual deviance: 14.893  on 14  degrees of freedom
AIC: 20.893

Number of Fisher Scoring iterations: 5
</code></pre>
"
"0.0826872055888527","0.0875510407188402","164705","<p>I'm working on analyzing a time series of physical variables in many lakes in Florida for an associate, and I've run into an issue. I'm attempting to run a regression for each time series of physical variables in each lake. I can get regression results in R easily, but they don't match up with my coworker's JMP results. Anyway, here's a sample from the data:</p>

<pre><code>Year = seq(1987,2015)
TP = c(14, 12, 14, 14, 17, 16, 15, 12, 18, 14, 15, 18, 18, 21, 21, 17, 17, 20, 19, 17, 18, 18, 26, 20, 18, 21, 21, 20, 18)
summary(lm(TP~Year))
</code></pre>

<p>gives </p>

<pre><code>Call:
lm(formula = TP ~ Year)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.7310 -1.3724 -0.4305  0.9685  6.3675 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -502.90542   98.13981  -5.124 2.18e-05 ***
Year           0.26010    0.04904   5.303 1.35e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 2.21 on 27 degrees of freedom
Multiple R-squared:  0.5102,    Adjusted R-squared:  0.4921 
F-statistic: 28.12 on 1 and 27 DF,  p-value: 1.35e-05
</code></pre>

<p>His JMP analysis spits out the following:</p>

<pre><code>Parameter Estimates
Term        Estimate    Std Error   t Ratio Prob&gt;|t|
Intercept   -500.4634   96.74332    -5.17   &lt;.0001*
Year        0.2588707   0.048347    5.35    &lt;.0001*
</code></pre>

<p>For all lakes and all parameters of interest, the SS, slope estimates, etc. are all slightly off. I have looked into different types of Sum of Squares for ANOVA, but changing to different types (e.g. Type III using Anova()) still doesn't get the results to match up. What am I missing? Any assistance would be appreciated.</p>

<p>Edit: Thanks for y'all's help. Sorry for the belated response, I had to meet up with my colleague. To address the questions:</p>

<ul>
<li>I have hardcoded the data in my question, but it's merely a subset of a much larger dataset from Excel. We are using the same data and the remainder of my code is working properly. <a href=""https://www.dropbox.com/s/k21v38sfdbm0ola/LWFormatted.csv?dl=0"" rel=""nofollow"">Here's what the actual data look like.</a></li>
<li>I know OLS isn't great, but it's being used for some really basic trend descriptions for informing stakeholders. I may pursue a better option in the future.</li>
<li>The JMP model is setup using Y by X with the Bivariate option, then applying a linear regression. Below is a screenshot.</li>
</ul>

<p>Thanks again for your help!</p>

<p><a href=""http://i.stack.imgur.com/ndtBr.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ndtBr.jpg"" alt=""JMP Input""></a></p>
"
"0.052777981396926","0.0502942438178979","164973","<p>I am trying to fit a survival analysis in <code>R</code> with non-recurrent events and time-varying coefficients. The baseline distribution is exponential or Weibull and the frailty distribution is gamma distributed. I have roughly 900.000 rows. </p>

<p>So far I have tried the <code>parfm</code> and <code>frailtypack</code>. Though, neither has worked â€“ they just keep running and never return. The calls for <code>parfm</code> and <code>frailtypack</code> are similar to respectively: </p>

<pre><code>frailtyPenal(Surv(stop-start,event)~.-start-stop-event-year+cluster(temp$year), 
             data= regressionData, hazard=""Weibull"", RandDist=â€Gammaâ€)

parfm(Surv(stop-start,event)~.-start-stop-event-year,cluster=â€yearâ€, 
      regressionData, dist=""exponential"", frailty = ""gamma"")
</code></pre>

<p>Where <code>event</code> is zero-one coded. My guess so far is to use the <code>lme4</code> package with the function <code>glmer</code> where the family is Poisson, the respond are zero-one coded, the offset is the difference in time and random effect is an intercept for the year factor. I.e. something like:</p>

<pre><code>glmer(event ~.-start-stop-event-year+(1|year), family = Poisson(), offset=stop-start)
</code></pre>

<p>I know that this will yield Gaussian distributed random effects and not Gamma. Further, I am not sure that I get the model I want. My goal is to have exponential distributed conditional waiting times and hence I chose the Poisson distribution. Question is whether this is correct? Any suggestions on other packages that will do the job?</p>
"
"0.0545088647991304","0.0649295895722714","165018","<p>I think I understand why orthogonality matters when doing regression with polynomial fits (so that the linear and quadratic, cubic, etc... can be evaluated independently). However, I don't understand what orthogonality even means when it comes to doing simply a linear regression. Specifically, in a regression with both continuous (age) and categorical (sex) variables, how I set up my continuous variable in the model will affect all coefficients.</p>

<pre><code>options(contrasts = c(""contr.treatment"", ""contr.poly""))
agevar=1:60
sexvar &lt;- rep(c(""male"",""female""),each=30)
set.seed(8093)
xvals &lt;- sample(-100:100,60)
m1 &lt;- summary(lm(xvals~sexvar*poly(agevar,1,raw=T))) # uses raw contrasts (i.e. 1:60)
# m1b &lt;- summary(lm(xvals~sexvar*agevar)) # alternative to m1, same result

coef(m1)

                                    Estimate Std. Error    t value  Pr(&gt;|t|)
(Intercept)                      25.13963663 28.0506961  0.8962215 0.3739710
sexvar1                          35.44078606 28.0506961  1.2634548 0.2116605
poly(agevar, 1, raw = T)          0.09955506  0.7997637  0.1244806 0.9013805
sexvar1:poly(agevar, 1, raw = T) -1.26395996  0.7997637 -1.5804168 0.1196438

m2 &lt;- summary(lm(xvals~sexvar*poly(agevar,1,raw=F))) # uses set of contrasts that sum to zero

coef(m2)

                                    Estimate Std. Error    t value   Pr(&gt;|t|)
(Intercept)                        28.176066   13.85039  2.0343159 0.04666608
sexvar1                            -3.109993   13.85039 -0.2245419 0.82315296
poly(agevar, 1, raw = F)           13.354858  107.28465  0.1244806 0.90138052
sexvar1:poly(agevar, 1, raw = F) -169.554469  107.28465 -1.5804168 0.11964377
</code></pre>

<p>Any idea on why the result is different and which way is correct? My actual data set uses the same variables, but is much larger with an unequal number of males/females and different sample size at each age. </p>
"
"0.0806196982594614","0.076825726438694","165110","<p>I'm struggling with the interpretation of a regression model where a categorial variable (5 levels) is dummy coded. Here is the result of my calculation in R:</p>

<pre><code>Call:
lm(formula = DV ~ Age + Gender + factor(Categorial) + 
Continuous 1 + Continuous 2 + Continuous 3, 
data = dat)

Residuals:
 Min       1Q   Median       3Q      Max 
-1.30058 -0.25326  0.00349  0.28123  1.49877 

Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)           -0.42367    0.30694  -1.380  0.16842   
Age                   -0.05949    0.02026  -2.936  0.00356 **
Gender                -0.01800    0.04828  -0.373  0.70952   
factor(Categorial)2   -0.30625    0.12645  -2.422  0.01596 * 
factor(Categorial)3   -0.03441    0.07752  -0.444  0.65736   
factor(Categorial)4   -0.12603    0.09914  -1.271  0.20453   
factor(Categorial)5   -0.08417    0.13269  -0.634  0.52630    
Continuous 1           0.12080    0.04346   2.779  0.00575 **
Continuous 2          -0.06592    0.04383  -1.504  0.13354   
Continuous 3          -0.06230    0.03475  -1.793  0.07392 . 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.4259 on 336 degrees of freedom
  (6 observations deleted due to missingness)
Multiple R-squared:  0.1315,    Adjusted R-squared:  0.1057 
F-statistic: 5.089 on 10 and 336 DF,  p-value: 6.353e-07
</code></pre>

<p>Ok. Age, Factor 2 of the categorial variable and the first continuous variable are significant predictors of the dependent variable. so far so good. </p>

<p>What I'm not understanding is:</p>

<ol>
<li><p>The reference category of the dummy coded categorial variable is the intercept and the first category of the categorial variable. right? How do I interpret this? </p></li>
<li><p>When doing an anova with the categorial variable as a independent variable, this factor is a significant predictor. With the results of the linear model, one could conclude that this is only due to category 2, right?</p></li>
<li><p>Can I test contrasts with this linear regression model (e.g. Category1 vs. Category2)?</p></li>
<li><p>Should I include interactions?</p></li>
</ol>

<p>I'd be glad for any help :-)</p>
"
"0.068136080998913","0.0649295895722714","166485","<p>I created for the following data set a multiple regression. Now I would like to forecast the next 20 data points.</p>

<pre><code>&gt; dput(datSel)
structure(list(oenb_dependent = c(1.0227039, -5.0683144, 0.6657713, 
3.3161374, -2.1586704, -0.7833623, -0.2203209, 2.416144, -1.7625406, 
-0.1565037, -7.9803936, 9.4594715, -4.8104584, 8.4827107, -6.1895262, 
1.4288595, 1.4896459, -0.4198522, -5.1583964, 5.2502294, 1.0567102, 
-1.0923342, -1.5852298, 0.6061936, -0.3752335, 2.5008664, -1.3999729, 
2.2802166, -2.1468756, -1.4890328, -0.79254376, 3.21804705, -0.94407886, 
-0.27802316, -0.20753079, -1.12610048, 2.0883735, -0.7424854, 
0.44203729, -1.48905938, 1.39644424, -3.8917377, 11.25665848, 
-9.22884035, 3.26856762, -0.00179541, -2.39664325, 4.00455574, 
-5.60891295, 4.6556348, -4.40536951, 6.64234497, -7.34787319, 
7.56303006, -8.23083674, 4.43247855, 1.31090412), carReg = c(0.73435946, 
0.24001161, 16.90532537, -14.60281976, 6.47603166, -8.35815849, 
3.55576685, 7.10705794, -4.6955223, 10.9623709, 5.5801857, -6.4499936, 
-9.46196502, 9.36289122, -8.52630424, 5.45070994, -4.5346405, 
-2.26716538, 2.56870398, 0.013737, 5.7750101, -27.1060826, 1.08977179, 
4.94934712, 17.55391859, -13.91160577, 10.38981128, -11.81349246, 
-0.0831467, 2.79748237, 1.84865463, -1.98736934, -6.24191695, 
13.33602659, -3.86527871, 0.78720993, 4.73360651, -4.1674034, 
9.37426802, -5.90660464, -0.4915792, -5.84811629, 9.67648643, 
-6.96872719, -7.6535767, 0.24847595, 0.18685263, -2.28766949, 
1.1544631, -3.87636933, -2.4731545, 4.33876671, 1.08836339, 5.64525271, 
1.90743854, -3.94709355, -0.84611324), cpi = c(1.16, -3.26, 0.22, 
-3.51, 0.84, -2.81, -0.34, -4.57, -0.12, -3.95, -1.37, -2.73, 
0.35, -5.38, -4.43, -3.08, 0.74, -3.03, -1.09, -2, 0.35, -1.52, 
1.28, 0.2, -0.25, -4.55, -2.49, -4.24, -0.31, -2.96, -2.24, -0.46, 
-0.06, -2.67, -1.27, -1.4, -0.7, -0.96, -2.18, -2.53, -0.52, 
-1.74, -2.18, -1.4, -0.34, -0.09, -1.65, -1.15, -0.17, -2.01, 
-1.38, -1.24, 0.09, -2.44, -1.92, -2.61, -0.34), primConstTot = c(-0.33334, 
-0.93333, -0.16667, -0.33333, -0.16667, -0.86666, -0.3, -0.4, 
-0.26667, -1.56667, -0.73333, 0.1, -0.23333, -0.26667, -1.5774, 
-0.19284, 0.38568, -2.42423, -0.93663, 0.08265, -0.63361, 0.0551, 
-0.49587, 2.39668, -1.70798, -3.36085, -2.56196, 0.16529, 0, 
-1.84572, -1.3774, -0.49586, -1.70798, -1.90081, -0.55096, -0.77134, 
-0.16529, -0.30303, -0.17066, -0.23853, -0.64401, -1.52657, -1.57426, 
-0.28623, -0.54861, -1.07336, -0.71558, 0.02385, -0.38164, -1.09721, 
0, 0.14311, -0.38164, -1.02566, -0.42934, -0.35779, -0.4532), 
    resProp.Dwell = c(0.8, -4, -3.2, 2.7, -1.6, -1, -2.4, -0.4, 
    -0.8, 1, -12.1, 0.2, -5.2, 3.7, -2.7, -1.7, 1.5, 0.7, -7.9, 
    0.3, 0.3, 1.4, -3.3, -1, -1.6, 1.5, 0.5, 1.5, -1, -2.2, -3.5, 
    0.5, 0.5, -0.9, -0.4, -3.4, 0.9, 0.1, -0.2, -2.8, -0.8, -6.2, 
    11.3, -4.6, 1, 1.1, -1.7, 4.1, -5, 2.3, -2.3, 4.6, -6.3, 
    6.3, -6.9, 0, 2.4), cbre.office.primeYield = c(0, 0, 0.15, 
    0.15, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.2, 0.15, 0.1, 
    0.05, 0.15, 0.3, 0.35, 0.4, 0.3, 0.2, 0, -0.15, -0.85, -1, 
    -0.85, -0.75, -0.1, 0, 0, 0, 0.05, 0.05, 0.05, 0.05, 0, 0, 
    0, 0.2, 0.2, 0.2, 0.2, 0, 0, 0, 0, 0.25, 0.25, 0.25, 0.25, 
    0, 0, 0, 0, 0, 0, 0), cbre.retail.capitalValue = c(-1882.35294, 
    230.76923, -230.76923, -226.41509, -670.78117, -436.13707, 
    -222.22223, 0, -205.91233, -202.16847, 0, -393.5065, -403.91909, 
    -186.30647, -539.81107, -748.11463, -764.70588, -311.47541, 
    -301.42782, -627.09677, -480, 720, 782.6087, 645.96273, 251.42857, 
    1386.66667, -533.33334, -533.33333, -533.33333, 0, 0, -1024.56141, 
    -192.10526, 0, -730, 0, 0, 0, 0, 0, -834.28571, 0, -1450.93168, 
    0, 0, 0, -700.78261, 0, 0, 0, 0, 0, 0, 0, -1452, 0, 0)), .Names = c(""oenb_dependent"", 
""carReg"", ""cpi"", ""primConstTot"", ""resProp.Dwell"", ""cbre.office.primeYield"", 
""cbre.retail.capitalValue""), row.names = c(NA, -57L), class = ""data.frame"")
&gt; 
&gt; fit &lt;- lm(oenb_dependent ~ carReg + cpi + primConstTot + 
+             resProp.Dwell + cbre.office.primeYield + cbre.retail.capitalValue , data = datSel)
&gt; summary(fit) # show results

Call:
lm(formula = oenb_dependent ~ carReg + cpi + primConstTot + resProp.Dwell + 
    cbre.office.primeYield + cbre.retail.capitalValue, data = datSel)

Residuals:
   Min     1Q Median     3Q    Max 
-5.166 -1.447 -0.162  1.448  7.903 

Coefficients:
                          Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)               0.831630   0.492297    1.69    0.097 .  
carReg                    0.085208   0.039600    2.15    0.036 *  
cpi                      -0.349192   0.212044   -1.65    0.106    
primConstTot              0.752772   0.383810    1.96    0.055 .  
resProp.Dwell             0.994356   0.086812   11.45  1.4e-15 ***
cbre.office.primeYield    1.274734   1.212782    1.05    0.298    
cbre.retail.capitalValue  0.000528   0.000643    0.82    0.416    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 2.24 on 50 degrees of freedom
Multiple R-squared:  0.754, Adjusted R-squared:  0.725 
F-statistic: 25.6 on 6 and 50 DF,  p-value: 1.2e-13
</code></pre>

<p>I tried the following:</p>

<pre><code>vals.multipleRegr &lt;- forecast(fit, h = 20)
Error: could not find function ""forecast""
</code></pre>

<p>However, this does not work as the function forecast cannot be found. I am using the following packages in my code, <code>library(bootstrap)</code>, <code>library(DAAG)</code> and <code>library(relaimpo)</code>. </p>

<p>Any suggestion how to forecasting using multiple regression?</p>

<p>I appreciate your replies!</p>
"
"0.0812570180448007","0.0871121856208561","166987","<p>I've read other similar questions on the site about logistic regression and I've read some articles/book chapters on this, but still I'm a little bit confused about that. I'll try to be as clearer as I can.</p>

<p>I have a medical case-control study, with many variables which could be used as predictors of the binary output variable, thus logistic regression is the best fit.</p>

<p>I have made some code in R, based on a previous question I made, like this:</p>

<pre><code>model&lt;-glm(Case ~ X + Y, data=data,    
family=binomial(logit));
</code></pre>

<p>where Case is the output variable, thus being 0 or 1 if it is a control or a case, respectively; X and Y are the input variables. I then use the output model to compute the area under the curve like this:</p>

<pre><code>aucCP=auc(Case~predict(model), data=data);
</code></pre>

<p>Okay, now the troubles begin. First, I understand that the object ""model"" is the output of the logistic regression model, thus being the log(odds) of the probability that model is Case for each couple of data in X and Y. Am I right?
Then, I know I can express the object model with an equation, being model:</p>

<pre><code>Coefficients:
(Intercept)         X            Y      
  -1.142005    -0.047981     0.020145     
</code></pre>

<p>thus being model=-1.14- 0.05X+ 0.02Y. Right?
Now the biggest problem: could ""model"" be considered as new variable, a combined predictor of X and Y, using which I predict Case?</p>
"
"0.068136080998913","0.0519436716578171","167440","<p>I am trying to get the bootstrapped confidence intervals of the coefficients for an ordinal logistic regression. 
Here below, my R code on fake data (reproducible example here below). This one does not work properly.</p>

<p>I suppose I need to enter a list of data with one line for each of the 20 subjects (this is the most simple way to proceed). Then the bootstrap with randomly select 20 rows (using sampling with replacement) to generate a new data set with 20 rows. That data set is converted into a new table of counts and a coefficient value is computed from that new â€œbootstrappedâ€ table.  This is repeated for each bootstrap sample. I can't get it! Thanks for your help.</p>

<pre><code>####################
library(rms)
x=c(1,2,3,2,3,1,2,3,3,3,2,2,1,2,1,2,3,2,1,2)
y=c(""math"",""eco"",""eco"",""lit"",""lit"",""eco"",""eco"",""math"",""math"",""lit"",""lit"",""math"",""eco"",""eco"",""math"",""lit"",""lit"",""math"",""eco"",""math"")
Dataset&lt;-data.frame(x,y)
h &lt;- orm(x ~ y)
h

# calculate coefficients using bootstrap
library(boot)
logit.bootstrap &lt;- function(data, indices) {
d&lt;-data[indices,]
fit&lt;-orm(x ~ y, data=data[indices,])
return(coefficients(fit))
}

# bootstrapping with 1000 replications
logit.boot &lt;- boot(data=Dataset, statistic=logit.bootstrap,R=1000)

# view results
logit.boot
plot(logit.boot)

# get 95% confidence interval
boot.ci(logit.boot, type=""all"")
############################
</code></pre>
"
"NaN","NaN","167523","<p>I have run a probit regression and the size of my coefficients seem to be quite big with respect to other similar studies.  For example, 0.254 vs 1.207 - does this mean anything in particular or is it all just relative to your model etc.?</p>
"
"0.0430930413588572","0.0410650781176591","167757","<p>I've run a probit regression in R with a random effect and can find no way to get the marginal effects with s.e. and p values.  I have therefore tried to calculate the marginal effects 'by hand' by using the probit scalars and regression coefficients.  However, I do not know how to get p values or standard errors and as far as I have found there is no easy way to do this for a mixed effects probit regression.</p>

<p>My model m1 is</p>

<pre><code> m1&lt;-glmer(Success~Name.Origin+(1|Job.ID),family=binomial(link=""probit""))
</code></pre>
"
"0.105869651339174","0.100887413963854","167825","<p>I am trying to use ""Cursor"" , ""PostCursor"" and ""CTLE"" to predict ""left"", and I added interactions and quandratic in the model.</p>

<pre><code>  &gt;left_int3&lt;-lm(Left ~ Cursor + PostCursor + CTLE + I(Cursor^2) + I
               (PostCursor^2), data = QPI)
  &gt;summary(left_int3)

  Coefficients:
                    Estimate Std. Error t value Pr(&gt;|t|)    
  (Intercept)     -412.58163   71.34574  -5.783 8.16e-09 ***
  Cursor            21.46885    2.85689   7.515 7.63e-14 ***
  PostCursor         2.96808    0.38768   7.656 2.62e-14 ***
  CTLE              -0.20459    0.01884 -10.858  &lt; 2e-16 ***
  I(Cursor^2)       -0.22646    0.02837  -7.982 2.09e-15 ***
  I(PostCursor^2)    0.24471    0.04070   6.013 2.06e-09 ***
  ---
  Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
  Residual standard error: 2.171 on 2794 degrees of freedom
  Multiple R-squared:  0.4174,  Adjusted R-squared:  0.4164 
  F-statistic: 400.4 on 5 and 2794 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>Then I inspected the 4 assumptions of regression and found that normality, linearity and constant variance are violated so need to transform:</p>

<pre><code> **HOMOSCEDASTICITY**

 &gt; ncvTest(left_int3)
 Non-constant Variance Score Test 
 Variance formula: ~ fitted.values 
 Chisquare = 3.505792    Df = 1     p = 0.06115458 
 &gt; spreadLevelPlot(left_int3)
 Suggested power transformation:  1.12032

 **Linearity**

 &gt; boxTidwell(Left ~ Cursor + PostCursor + CTLE + I(Cursor^2) + I
             (PostCursor^2), data = QPI)  #

                 Score Statistic   p-value MLE of lambda
 Cursor                 7.162587 0.0000000      7.123073
 PostCursor            -3.534346 0.0004088     16.129858
 CTLE                  -1.921833 0.0546268      3.891245
 I(Cursor^2)           -7.641956 0.0000000      4.145477
 I(PostCursor^2)        4.937534 0.0000008      8.687134

 **Normality**

&gt; summary(powerTransform(QPI$Left))
bcPower Transformation to Normality 

           Est.Power Std.Err. Wald Lower Bound Wald Upper Bound
QPI$Left    3.7107   0.4409           2.8466           4.5749

Likelihood ratio tests about transformation parameters
                         LRT df         pval
LR test, lambda = (0) 72.13642  1 0.000000e+00
LR test, lambda = (1) 38.30386  1 6.054269e-10

**Independence** 

boxTidwell(Left ~ Cursor + PostCursor + CTLE + I(Cursor^2) + 
         I(PostCursor^2), data = QPI)  #
                Score Statistic   p-value MLE of lambda
Cursor                 7.162587 0.0000000      7.123073
PostCursor            -3.534346 0.0004088     16.129858
CTLE                  -1.921833 0.0546268      3.891245
I(Cursor^2)           -7.641956 0.0000000      4.145477
I(PostCursor^2)        4.937534 0.0000008      8.687134
</code></pre>

<p>Then I performed the transformations and fit again, but the R^2 is still low, so I am wondering my transformations are correct or not.</p>

<pre><code> &gt;QPI$Left&lt;-QPI$Left^3.7107  
 &gt;QPI$Cursor&lt;-QPI$Cursor^7.123
 &gt;QPI$PostCursor&lt;-QPI$PostCursor^16.129
 &gt;QPI$CTLE&lt;-QPI$CTLE^3.891245
 &gt;left_int3&lt;-lm(Left ~ Cursor + PostCursor + CTLE + I(Cursor^2) + 
             I(PostCursor^2), data = QPI)
 &gt;summary(left_int3)
 &gt;Coefficients:
                  Estimate Std. Error t value Pr(&gt;|t|)    
 (Intercept)      1.455e+07  6.651e+05  21.880  &lt; 2e-16 ***
 Cursor           2.299e-06  1.302e-06   1.766  0.07754 .  
 PostCursor       1.150e-06  2.147e-06   0.536  0.59231    
 CTLE            -4.772e+00  4.548e-01 -10.493  &lt; 2e-16 ***
 I(Cursor^2)     -2.162e-18  6.854e-19  -3.154  0.00163 ** 
 I(PostCursor^2) -2.775e-19  5.977e-19  -0.464  0.64253    
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

 Residual standard error: 1175000 on 2794 degrees of freedom
 Multiple R-squared:  0.3942,    Adjusted R-squared:  0.3932 
 F-statistic: 363.7 on 5 and 2794 DF,  p-value: &lt; 2.2e-16
</code></pre>
"
"0.0430930413588572","0.0410650781176591","167854","<p>I am running a probit glmer, with a binary response varaible and a categorical explanatory variable with three dummy levels and have tried to calculate the marginal effect using the following code:</p>

<pre><code>    ProbitScalar&lt;- mean(dnorm(predict(m1,type = ""link""))) 
</code></pre>

<p>The ProbitScalar value is then multiplied by the coefficient estimates from the regression output.</p>

<p>I get the following values: </p>

<p>-0.2946806 (referring to the intercept and reference level)
-0.1527443
-0.07252501</p>

<p>I am slightly confused how to interpret them as they seem quite low compared to what I would expect from the raw data.</p>

<p>Is it correct that the second variable has a 15% lower chance of achieving success (the binary response variable) than the reference group and the final variable has a 7% less chance of achieving success than the reference group?</p>
"
"0.0826872055888527","0.0963061447907242","167897","<p>I have the data:</p>

<pre><code>Distances   Diversity
-300        3.532833
-300        3.319447
-300        3.331814
-300        3.284599
-150        3.167693
-150        3.343932
-150        3.400182
-150        3.347922
-50         3.185409
-50         3.590527
-50         3.163942
-50         3.102254
 50         3.382986
 50         2.78799
 50         3.204374
 50         2.756762
 150        2.784996
 150        3.206704
 150        2.431388
 150        2.911236
 300        2.10763
 300        2.393464
 300        3.527539
 300        2.552804
</code></pre>

<p>After investigating the data it seems that there is a slightly curved relationship:
<img src=""http://imgur.com/QTQDVnS.jpg"" alt=""graph of Diversity against distance""></p>

<p>I have performed a simple polynomial regression and linear regression with the code:</p>

<pre><code>m1 &lt;- lm(Diversity ~ Distances + I(Distances^2), data = Data)
m2 &lt;- lm(Diversity ~ Distances, data = Data) 
</code></pre>

<p>However the output shows that the linear model is a better predictor with a p-value of &lt;0.001 while the polynomial is not significant (p>0.1).</p>

<p>This confuses me as the predicted values from m1 seem to better match the trend shown in the data:</p>

<pre><code> plot(Diversity ~ Distances)
 lines(lowess(Diversity ~ Distances))
 lines(Distances, predict(m1), col = ""red"")
 lines(Distances, predict(m2), col = ""blue"")
</code></pre>

<p><img src=""http://i.imgur.com/3Ymwnry.jpg"" alt=""Trend lines""></p>

<p>can someone explain why the p values suggest that the polynomial regression is such a poor predictor? </p>

<p>Thank you in advance</p>

<p>Summary (m1):</p>

<pre><code>Call:
lm(formula = Diversity ~ Distances + I(Distances^2), data = Data.Col)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.50157 -0.12025 -0.04278  0.11443  0.91834 

Coefficients:
                 Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)     3.131e+00  9.027e-02  34.689  &lt; 2e-16 ***
Distances      -1.305e-03  3.221e-04  -4.051 0.000576 ***
I(Distances^2) -1.454e-06  1.685e-06  -0.863 0.397985    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.309 on 21 degrees of freedom
Multiple R-squared:  0.4496,    Adjusted R-squared:  0.3971 
F-statistic: 8.576 on 2 and 21 DF,  p-value: 0.001894
</code></pre>

<p>Summary(m2):
    Call:
    lm(formula = Diversity ~ Distances, data = Data.Col)</p>

<pre><code>Residuals:
     Min       1Q   Median       3Q      Max 
-0.57667 -0.15650 -0.00791  0.08949  0.84323 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  3.0757678  0.0627046  49.052  &lt; 2e-16 ***
Distances   -0.0013049  0.0003203  -4.074 0.000503 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.3072 on 22 degrees of freedom
Multiple R-squared:  0.4301,    Adjusted R-squared:  0.4042 
F-statistic:  16.6 on 1 and 22 DF,  p-value: 0.0005031
</code></pre>
"
"0.0430930413588572","0.0410650781176591","168336","<p>I ran a multiple regression analysis and got significant results for lFreq, Len variables, and interaction lFreq x Len. Now I need to report these results and I am a bit confused whether F(7, 924) = 9.876 and R^2= 0.06961 stand for all variables? It seems to me that each variable should have its own F and R^2 values... How should I calculate them? </p>

<pre><code>Call:
lm(formula = duration ~ lFreq * Len * group, data = df.eyes)

 Residuals:
  Min        1Q    Median        3Q       Max 
-0.079878 -0.010185 -0.001281  0.009893  0.192742 

Coefficients:
                   Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)       0.2547662  0.0110199  23.119   &lt;2e-16 ***
lFreq            -0.0038043  0.0012332  -3.085   0.0021 ** 
Len              -0.0034335  0.0014160  -2.425   0.0155 *  
group2            0.0186209  0.0156136   1.193   0.2333    
lFreq:Len         0.0003494  0.0001617   2.160   0.0310 *  
lFreq:group2     -0.0005048  0.0017458  -0.289   0.7725    
Len:group2       -0.0035563  0.0020054  -1.773   0.0765 .  
lFreq:Len:group2  0.0001673  0.0002290   0.731   0.4652    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.01765 on 924 degrees of freedom
Multiple R-squared:  0.06961,   Adjusted R-squared:  0.06256 
F-statistic: 9.876 on 7 and 924 DF,  p-value: 6.388e-12
</code></pre>
"
"0.0609427635336005","0.0580747904139041","168482","<p>I am running a probit regression with a random effect:</p>

<pre><code>m1&lt;-glmer(Binary~Explan+(1|Random),family=binomial(link=""probit""))
</code></pre>

<p>where Explan is a three-level categorical variable. </p>

<p>I want to calculate the mean predicted probabilities for each level of Explan. I tried doing so using this code:</p>

<pre><code>newdata=data.frame(Explan=""First"")
predict(m1,newdata,type=""response"")
</code></pre>

<p>where First is a level of the categorical Explan variable.</p>

<p>However I get the following error message:</p>

<pre><code>Error: (p &lt;- ncol(X)) == ncol(Y) is not TRUE
</code></pre>

<p>Were this a logit model, I would simply strip the model of the intercept and then back-transform the model summary coefficients to get the predicted values that I'm after, but I am unsure of how I would go about this with a mixed-effects probit model. </p>

<p>Any help in extracting the predicted probabilities would be greatly appreciated.</p>
"
"0.0691025985081098","0.076825726438694","168857","<p>I have a fairly large dataset of the following form, and I want to run a linear regression returning coefficients for each factor:</p>

<pre><code>Case    Variable1   Variable2   Result
1       Factor1     FactorA     50
2       Factor2     FactorA     60
3       Factor1     FactorB     55
4       Factor2     FactorB     65
...     ...         ...         ...
</code></pre>

<p>Running a linear regression using <code>lm()</code> on this would be very straightforward, but the size of the dataset seems to be too large.  I have about 1,000,000 cases, with about 10,000 factors in each variable.</p>

<p><code>lm()</code> (or other standard linear regression methods) translates this to an extremely wide matrix where each factor is a Boolean variable, correct?  So ~ 20,000 wide x 1,000,000 tall?  Running <code>lm()</code> on just a 25,000 case sample still takes several minutes and over a gb of memory.</p>

<p>My initial thought was to attack this regression problem using package <code>biglm</code>, but for it to behave properly, I believe <code>biglm</code> requires every factor to be present in every ""chunk"" of data it digests.  This would not occur in my data; some of the factors are only present a few times.  This is called <em>rank deficiency</em>, I believe. (However, an answer at <a href=""http://stackoverflow.com/questions/10502882/r-biglm-with-categorical-variables"">this StackOverflow question</a> indicates there might be a workaround?)</p>

<p>So my question: is there a better way to structure my data to run this regression?  Is there a better package or approach I should be using to run this analysis?</p>
"
"0.068136080998913","0.0649295895722714","169291","<p>I have a logistic regression model below, predicting a dichotomous variable <em>type</em> from a single continuous predictor <em>fatigue</em>. Using the coefficients below I can obtain the increase in the odds of a positive <em>type</em> from a 1 unit increase in fatigue.</p>

<p>Also I believe by forming the model expression</p>

<pre><code>logit(type) = 0.3134 - 91.1171 * fatigue 
</code></pre>

<p>I can obtain the odds of a positive <em>type</em> for a given value of <em>fatigue</em> by plugging it in, say for a value <em>fatigue</em> = 1.</p>

<p><strong>However</strong>, what I want to do is to obtain the odds of a positive <em>type</em> for a range of <em>fatigue</em> values, i.e. <strong>&lt;= 0</strong>. Is this possible?</p>

<pre><code>## Call:
## glm(formula = type ~ fatigue, family = binomial(), data = myData)

## Deviance Residuals:
## Min 1Q Median 3Q Max
## -1.6703 -1.3104 0.8369 1.0049 1.4695
##
## Coefficients:
## Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept) 0.3134 0.1496 2.095 0.0362 *
## fatigue -91.1171 36.3785 -2.505 0.0123 *
## ---
## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
##
## (Dispersion parameter for binomial family taken to be 1)
##
## Null deviance: 282.84 on 210 degrees of freedom
## Residual deviance: 276.03 on 209 degrees of freedom
## AIC: 280.03
##
## Number of Fisher Scoring iterations: 4
</code></pre>
"
"0.035185320931284","0.0335294958785986","169334","<p>I'm trying to use the <code>circular</code> package in R to perform regression of a circular response variable and linear predictor, and I do not understand the coefficient value I'm getting. I've spent considerable time searching in vain for an explanation that I can understand, so I'm hoping somebody here may be able to help.</p>

<p>Here's an example:</p>

<pre><code>library(circular)

# simulate data
x &lt;- 1:100
set.seed(123)
y &lt;- circular(seq(0, pi, pi/99) + rnorm(100, 0, .1))

# fit model
m &lt;- lm.circular(y, x, type=""c-l"", init=0)

&gt; coef(m)
[1] 0.02234385
</code></pre>

<p>I don't understand this coefficient of 0.02 -- I would expect the slope of the regression line to be very close to pi/100, as it is in garden variety linear regression:</p>

<pre><code>&gt; coef(lm(y~x))[2]
         x
0.03198437
</code></pre>

<p>Does the circular regression coefficient not represent the change in response angle per unit change in the predictor variable? Perhaps the coefficient needs to be transformed via some link function to be interpretable in radians? Or am I thinking about this all wrong? Thanks for any help you can offer.</p>
"
"0.0812570180448007","0.0774330538852055","169438","<p>As we all know, there are 2 methods to evaluate the logistic regression model and 
they are testing very different things</p>

<ol>
<li><p>Predictive power:</p>

<p>Get a statistic that measures how well you can predict the dependent variable 
based on the independent variables. The well-know Pseudo R^2 are McFadden 
(1974) and Cox and Snell (1989).</p></li>
<li><p>Goodness-of-fit statistics</p>

<p>The test is telling whether you could do even better by making the model more 
complicated, which is actually testing whether there are any non-linearities or 
interactions.</p>

<p>I implemented both tests on my model, which added quadratic and interaction<br>
already: </p>

<pre><code>&gt;summary(spec_q2)

Call:
glm(formula = result ~ Top + Right + Left + Bottom + I(Top^2) + 
 I(Left^2) + I(Bottom^2) + Top:Right + Top:Bottom + Right:Left, 
 family = binomial())

 Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.955431   8.838584   0.108   0.9139    
Top          0.311891   0.189793   1.643   0.1003    
Right       -1.015460   0.502736  -2.020   0.0434 *  
Left        -0.962143   0.431534  -2.230   0.0258 *  
Bottom       0.198631   0.157242   1.263   0.2065    
I(Top^2)    -0.003213   0.002114  -1.520   0.1285    
I(Left^2)   -0.054258   0.008768  -6.188 6.09e-10 ***
I(Bottom^2)  0.003725   0.001782   2.091   0.0366 *  
Top:Right    0.012290   0.007540   1.630   0.1031    
Top:Bottom   0.004536   0.002880   1.575   0.1153    
Right:Left  -0.044283   0.015983  -2.771   0.0056 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
(Dispersion parameter for binomial family taken to be 1)
Null deviance: 3350.3  on 2799  degrees of freedom
Residual deviance: 1984.6  on 2789  degrees of freedom
AIC: 2006.6
</code></pre></li>
</ol>

<p>and the predicted power is as below, the MaFadden is 0.4004, and the value between 0.2~0.4 should be taken to present very good fit of the model(Louviere et al (2000), Domenich and McFadden (1975))                                                :</p>

<pre><code> &gt; PseudoR2(spec_q2)
    McFadden     Adj.McFadden        Cox.Snell       Nagelkerke McKelvey.Zavoina           Effron            Count        Adj.Count 
   0.4076315        0.4004680        0.3859918        0.5531859        0.6144487        0.4616466        0.8489286        0.4712500 
         AIC    Corrected.AIC 
2006.6179010     2006.7125925 
</code></pre>

<p>and the goodness-of-fit statistics:</p>

<pre><code> &gt; hoslem.test(result,phat,g=8)

     Hosmer and Lemeshow goodness of fit (GOF) test

  data:  result, phat
  X-squared = 2800, df = 6, p-value &lt; 2.2e-16
</code></pre>

<p>As my understanding, GOF is actually testing the following null and alternative hypothesis:</p>

<pre><code>  H0: The models does not need interaction and non-linearity
  H1: The models needs interaction and non-linearity
</code></pre>

<p>Since my models added interaction, non-linearity already and the p-value shows H0 should be rejected, so I came to the conclusion that my model needs interaction, non-linearity indeed. Hope my interpretation is correct and thanks for any advise in advance, thanks. </p>
"
"0.052777981396926","0.0502942438178979","169523","<p>I'm new to forecasting and trying to create a model to forecast one step ahead weekly sales from my company. The variables I've identified for the model are Lag 1 Markdown spend and lag 1 sales, and I've included dummy variables for monthly seasonality and some promotional weekends we run, resulting in the below model:</p>

<pre><code>&gt; summary(fit)

Call:
lm(formula = Sales.RtlT ~ L1.MD + P1 + P2 + P3 + P4 + P5 + P6 + 
    P7 + P8 + P9 + P10 + P11 + L1.Sales, data = lux)

Residuals:
 Min       1Q   Median       3Q      Max 
-10.7028  -2.8850  -0.2948   2.5488  15.3845 

Coefficients:
        Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  5.17443    1.86670   2.772 0.006115 ** 
L1.MD        0.16591    0.02353   7.050 3.06e-11 ***
P1           0.21394    1.84689   0.116 0.907902    
P2           0.47421    1.83599   0.258 0.796462    
P3          -1.16762    1.75653  -0.665 0.507013    
P4          -0.77594    1.84712  -0.420 0.674892    
P5           3.36375    1.76142   1.910 0.057650 .  
P6          -1.11759    1.56631  -0.714 0.476384    
P7           1.76297    1.79197   0.984 0.326429    
P8           5.13988    1.66437   3.088 0.002309 ** 
P9           5.39127    1.52419   3.537 0.000506 ***
P10          6.58703    1.61126   4.088 6.37e-05 ***
P11          7.02233    1.60600   4.373 2.00e-05 ***
L1.Sales     0.63513    0.04829  13.152  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 4.795 on 194 degrees of freedom
Multiple R-squared:  0.8115,    Adjusted R-squared:  0.7988 
F-statistic: 64.23 on 13 and 194 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>my weekly sales data isn't stationary, but if I difference it once it is, as shown below:</p>

<p><a href=""http://i.stack.imgur.com/VspqS.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/VspqS.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/cIN6S.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/cIN6S.png"" alt=""enter image description here""></a></p>

<p>However I can't fit lm with the differenced values. Is it preferable to have a stationary time series for a forecast like this, and if so how should I run the regression to account for this? </p>
"
"0.068136080998913","0.0649295895722714","171151","<p>I am trying to build a multiple regression model using R. I have a number of predictor variables. I have some basic domain knowledge for which I am trying to build the model. To start with, I included a few predictor variables based on domain knowledge and high correlation coefficients with the response variable, while excluding some other predictors due to multicollinearity. I would like to figure out if I should include some interaction terms. But, due to large number of predictors, I am having a hard time trying to figure out which all interaction terms I should include in the model. Based on what I have read on this site about automated model selection (thanks, @gung et. al), I am trying to avoid using it. </p>
"
"0.052777981396926","0.0335294958785986","171448","<p>Doing ridge regression in R I have discovered</p>

<ul>
<li><p><code>linearRidge</code> in the <code>ridge</code> package - which fits a model, reports coefficients and p values but nothing to measure the overall goodness of fit</p></li>
<li><p><code>lm.ridge</code> in the <code>MASS</code> package - which reports coefficients and GCV but no p values for parameters</p></li>
</ul>

<p>How can I get all of these things (goodness of fit, coefficients and p values) from the same ridge regression?  I'm new to R so not familiar with facilities that may be available e.g. for computing $r^2$ from the data and fitted coefficients.</p>
"
"0.0373196685431038","0.0355634008582852","171785","<p>I am running multiple linear regression with categorical variables and I need confidence interval 95% for standardized regression coefficient. I searched around and found 2 methods:</p>

<ol>
<li><p>Using the <code>QuantPsyc</code> package, with the function <code>lm.beta</code>. However, using <code>lm.beta</code> I can only get the standardized coefficients whereas I need with their 95% CI too. Is there a way?</p></li>
<li><p>To extract standardized regression coefficient, first standardize all the variables involved, and then run it in linear regression then you'll get estimates for standardized coefficients.</p></li>
</ol>

<p>So here is my model:</p>

<pre><code>model1 &lt;- lm(Life_Satisfaction ~ Subjective + Age + Sex + CountryCat11 + 
                                 CountryCat12 + CountryCat13 + CountryCat14 + 
                                 CountryCat15 + CountryCat16 + CountryCat17 + 
                                 CountryCat18 + CountryCat19 + CountryCat20 + 
                                 CountryCat23 + CountryCat25 + CountryCat28 + 
                                 CountryCat29 + CountryCat30 + Education_ISCED1 + 
                                 Education_ISCED2 + Education_ISCED3 + 
                                 Education_ISCED4 + Education_ISCED5 + 
                                 Education_ISCED6 + Education_stillinschool + 
                                 Education_None + Education_other, data=lifesat)

lm.beta (model1)
</code></pre>

<p>I ran that, but I cannot get the 95% CI.</p>

<p>So I tried the scale method:</p>

<pre><code>model2 &lt;- lm(scale(Life_Satisfaction) ~ scale(Subjective) + scale(Age) + 
                                        scale(Sex) + scale(CountryCat11) + 
                                        scale(CountryCat12) + scale(CountryCat13) + 
                                        scale(CountryCat14) + scale(CountryCat15) + 
                                        scale(CountryCat16) + scale(CountryCat17) + 
                                        scale(CountryCat18) + scale(CountryCat19) + 
                                        scale(CountryCat20) + scale(CountryCat23) + 
                                        scale(CountryCat25) + scale(CountryCat28) + 
                                        scale(CountryCat29) + scale(CountryCat30) + 
                                    scale(Education_ISCED1) + scale(Education_ISCED2) + 
                                    scale(Education_ISCED3) + scale(Education_ISCED4) + 
                                    scale(Education_ISCED5) + scale(Education_ISCED6) + 
                               scale(Education_stillinschool) + scale(Education_None) + 
                                        scale(Education_other), data=lifesat)

summary(model2)
</code></pre>

<p>I ran that, and I got the standardized regression and 95% CI but it was different from the standardized regression results I got from SPSS? Did I do it wrong?</p>
"
"0.052777981396926","0.0502942438178979","171879","<p>I have the R output for the logistic regression model. It seems that only the intercept and psa are statistically significant. Does that mean I should remove sorbets_psa and cinko from my model and create a new model as new.model = glm(status ~ psa,family = binomial(link =""probit""))</p>

<pre><code>Call:
glm(formula = status ~ psa + serbest_psa + cinko, family = binomial(link =""probit""), data = data)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.3285  -0.6773  -0.6261  -0.5604   1.9500  

Coefficients:
      Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -0.9697009  0.2409856  -4.024 5.72e-05 ***
psa          0.0444376  0.0094368   4.709 2.49e-06 ***
serbest_psa -0.0440718  0.0250486  -1.759   0.0785 .  
cinko       -0.0006923  0.0016984  -0.408   0.6835    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 534.27  on 477  degrees of freedom
Residual deviance: 477.07  on 474  degrees of freedom
AIC: 485.07

Number of Fisher Scoring iterations: 6
</code></pre>
"
"0.0457070726502004","0.0435560928104281","172003","<p>I'd like to fit integer coefficients, e.g. summing to 10, to a regression equation. The absolute values of the coefficients (i.e. predicted y) aren't important, I just want to retain the appropriate relative values. The use case is for an easily interpretable scoring system.</p>

<p>For example, this regression yields the following coefficients (ignoring the intercept):</p>

<pre><code>set.seed(0)
y &lt;- rnorm(100)
x &lt;- matrix(rnorm(300), ncol=3)
m &lt;- lm(y ~ x)
(coef &lt;- m$coefficients[-1])
#          x1          x2          x3 
#  0.12100965  0.05506511  0.14708549 
</code></pre>

<p>Rounding with the below code yields a rounding error (sums to 11):</p>

<pre><code>round(10 * coef / sum(coef))
# x1 x2 x3 
#  4  2  5 
</code></pre>

<p>A method like this also doesn't guarantee maximally similar weights to the regression equation. </p>

<p>This was asked <a href=""http://www.researchgate.net/post/How_do_I_transform_beta_coefficients_into_integer_values"" rel=""nofollow"">here</a> without satisfactory answers, and might be addressed in <a href=""http://www.jstor.org/stable/2347432"" rel=""nofollow"">this paywalled research paper</a>.</p>

<p><strong>Edit:</strong> looks like <a href=""http://stackoverflow.com/questions/792460/how-to-round-floats-to-integers-while-preserving-their-sum"">How to round floats to integers while preserving their sum?</a> may be able to help minimize the roundoff error. If my question is further specified as minimizing the error of a predicted (scaled) y, I'm unsure whether this is an equivalent optimization.</p>
"
"0.0609427635336005","0.0435560928104281","172690","<p>I'm running some basic regressions which can be specified compactly as the <code>formula</code>: <code>y~treatment*dummy</code>.</p>

<p>Say there are several ($m$) treatments, $T_1,\ldots,T_m$ (with $T_1$ being the reference/control); the dummy is also multifaceted (categorical), taking $n$ values $D_1,\ldots,D_n$</p>

<p>Then (suppressing the observation index and error) the above formula specification basically returns the specified formula as</p>

<p>$$y=T^TBD$$</p>

<p>Where $T$ is the $m\times 1$ vector $[1, T_2, \ldots, T_m]$ of treatment indicators (excluding the reference treatment), $B$ is the $m\times n$ matrix of coefficients $\{\beta_{i,j}\}_{i=1,j=1}^{m\quad n}$, and $D$ is the $n\times 1$ vector $[1, D_2,\ldots, D_n]$ of dummy indicators (excluding the reference category).</p>

<p>This is all well and good, but the resulting coefficients in $B$ don't really have any clean interpretation, especially for my application. In particular, I'm looking for significant treatment effects--consider trying to answer the following: was Treatment 5 significantly better among individuals in category 3?</p>

<p>In the above specification, we'd be examining $\mathbb{E}[y|T_5,D_3]-\mathbb{E}[y|T_1,D_3]=\beta_{5,0}+\beta_{5,3}$, so we <em>could</em> add the coefficients we get out and use, e.g., a Wald test to determine significance.</p>

<p>However, consider the equivalent specification (I'm 100% sure someone besides has written it this way before since it took me all of 20 minutes to come up with):</p>

<p>$$y = \delta_0 + \sum_{j=2}^n\beta_j D_j + \sum_{i=2}^m \sum_{j=1}^n \gamma_{i,j}T_iD_j$$</p>

<p>Now the treatment effect is $\mathbb{E}[y|T_5,D_3]-\mathbb{E}[y|T_1,D_3]=\gamma_{5,3}$.</p>

<p>So this latter formulation has the convenient property that we can read our treatment effects right off our regression summary (especially including standard errors); its major drawback is that there's no way to supply this as an R <code>formula</code> parsimoniously, or at least I can't see a way to.</p>

<p>Does anyone have any experience with some secret <code>formula</code> or package to deal with this (I imagine exceedingly common) specification?</p>
"
"0.118246329960506","0.112681644735122","172943","<p>I'm trying to understand the output of <code>glm</code> when a categorical variable has more than 2 categories.</p>

<p>I'm analysing if age affects death. Age is a categorical variable with 4 categories</p>

<p>I use the following code in R:</p>

<pre><code>mydata &lt;- read.delim(""Data.txt"", header = TRUE)
mydata$Agecod &lt;- factor(mydata$Agecod)
mylogit &lt;- glm(Death ~ Agecod, data = mydata, family = ""binomial"")
summary(mylogit)
</code></pre>

<p>Obtaining the following output: </p>

<pre><code>Call:
glm(formula = Death ~ Agecod, family = ""binomial"", data = mydata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.4006  -0.8047  -0.8047   1.2435   2.0963  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.5108     0.7303   0.699   0.4843  
Agecod2      -0.6650     0.7715  -0.862   0.3887  
Agecod3      -1.4722     0.7658  -1.922   0.0546 .
Agecod4      -2.5903     1.0468  -2.474   0.0133 *

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 237.32  on 184  degrees of freedom
Residual deviance: 223.73  on 181  degrees of freedom
  (1 observation deleted due to missingness)
AIC: 231.73

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Since I have p-values for <code>Agecod2</code>, <code>Agecod3</code> and <code>Agecod4</code> and only <code>Agecod4</code> has a significant p-value my questions are:</p>

<ol>
<li>Is really <code>Age</code> associated with death?</li>
<li>Is only the 4th age category associated with death?</li>
<li>What happens with the first category since I don't have its p-value?</li>
</ol>

<p>Update:</p>

<p>Since Antoni Parellada says â€œIt seems as though you have proven that old age is a good predictor of deathâ€ and Gung points â€œYou cannot tell from your output if Age is associated with deathâ€ Iâ€™m still confused.</p>

<p>I understand that â€œInterceptâ€ is representing Agecod1 and is the â€œreference levelâ€. According to Gung â€œThe Estimates for the rest are the differences between the indicated level and the reference level. The associated p-values are for the tests of the indicated level vs. the reference level in isolation.â€ </p>

<p>My question now is: </p>

<p>Since Agecod4 p-value (0.0133) is significantly different from Agecod1 (reference lelvel) it doesnâ€™t mean that age is associated with death?</p>

<p>I have also tried to perform a nested test with the following command:</p>

<pre><code>anova(mylogit, test=""LRT"")
</code></pre>

<p>Obtaining:</p>

<pre><code>       Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)   
NULL                     184     237.32            
Agecod  3   13.583       181     223.73 0.003531 *
</code></pre>

<p>Does it mean that Age is definitively associated with death?</p>

<p>Update2:</p>

<p>I have solved my problem using binary logistic regression in SPSS. The output is the same than â€œmylogitâ€ but with SPSS I obtain a global p-value for the overall variable Agecod which is 0.008.</p>

<p>I donâ€™t know if is possible to obtain this â€œglobal p-valueâ€ with R, but since I know that I can use SPSS is not a big problem for me.</p>
"
"0.0861860827177143","0.0821301562353182","173076","<p>I was following the procedure in a statistics textbook to run a multinomial logistic regresion using <code>mlogit</code>. However, the Odds Ratios calculated seemed too high for some of the variables (>1000). Can someone take a look at this and check wether I am doing everything correctly? The data can be downloaded from <a href=""https://dl.dropboxusercontent.com/u/14303378/LogisticRegressionSample.csv"" rel=""nofollow"">here</a>. I prepared the data with the following commands:</p>

<pre><code>#read in the data
test&lt;-read.csv(file=""LogisticRegressionSample.csv"",sep="","")
#trasnform data into the correct form for mlogit
mlogitData&lt;-mlogit.data(test,choice=""Outcome"",shape=""wide"")
#build model
MLogitFit&lt;-mlogit(Outcome~1|V1+V2+V3+V4+V5+V6+V7+V8,reflevel=3,data=mlogitData)
#summary of the model
summary(MLogitFit)
#OddsRatios
data.frame(exp(MLogitFit$coefficients))
# confidence Interval of the odds Ratios
exp(confint(MLogitFit))
</code></pre>

<p>The summary of mlogit gives me:</p>

<pre><code>    Call:
mlogit(formula = Outcome ~ 1 | V1 + V2 + V3 + V4 + V5 + V6 + 
    V7 + V8, data = mlogitData, reflevel = 3, method = ""nr"", 
    print.level = 0)

Frequencies of alternatives:
      Z       A       B 
0.43333 0.25556 0.31111 

nr method
7 iterations, 0h:0m:0s 
g'(-H)^-1g = 1.56E-06 
successive function values within tolerance limits 

Coefficients :
               Estimate Std. Error t-value  Pr(&gt;|t|)    
A:(intercept)  -6.74640    5.97451 -1.1292 0.2588147    
B:(intercept)  -7.12401    4.50350 -1.5819 0.1136759    
A:V1            3.65979    3.90808  0.9365 0.3490331    
B:V1            4.24363    3.25687  1.3030 0.1925822    
A:V2          -15.11554    6.92901 -2.1815 0.0291475 *  
B:V2           -4.88778    3.65249 -1.3382 0.1808302    
A:V3            1.71465    6.57907  0.2606 0.7943839    
B:V3            2.94335    3.96557  0.7422 0.4579497    
A:V4           -1.70660    1.58849 -1.0744 0.2826633    
B:V4           -1.67210    1.17575 -1.4222 0.1549820    
A:V5            1.18494    1.60760  0.7371 0.4610682    
B:V5            1.03084    1.25573  0.8209 0.4116971    
A:V6            8.28902    2.51631  3.2941 0.0009873 ***
B:V6            3.44578    1.91844  1.7961 0.0724727 .  
A:V7           -1.34395    2.67943 -0.5016 0.6159612    
B:V7            1.04803    1.95147  0.5370 0.5912343    
A:V8           -7.46263    4.12978 -1.8070 0.0707577 .  
B:V8            0.21861    2.13596  0.1023 0.9184810    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Log-Likelihood: -64.636
McFadden R^2:  0.33149 
Likelihood ratio test : chisq = 64.1 (p.value = 1.0515e-07)
</code></pre>

<p>Running <code>data.frame(exp(MLogitFit$coefficients))</code> to calculate the odds ratios gives:</p>

<pre><code>              exp.MLogitFit.coefficients.
A:(intercept)                1.175103e-03
B:(intercept)                8.055280e-04
A:V1                         3.885310e+01
B:V1                         6.966040e+01
A:V2                         2.725226e-07
B:V2                         7.538147e-03
A:V3                         5.554743e+00
B:V3                         1.897938e+01
A:V4                         1.814819e-01
B:V4                         1.878524e-01
A:V5                         3.270504e+00
B:V5                         2.803423e+00
A:V6                         3.979917e+03
B:V6                         3.136764e+01
A:V7                         2.608125e-01
B:V7                         2.852036e+00
A:V8                         5.741439e-04
B:V8                         1.244345e+00
</code></pre>

<p>I obtained the confidence interavls with: <code>exp(confint(MLogitFit))</code>:</p>

<pre><code>                     2.5 %       97.5 %
A:(intercept) 9.650816e-09 1.430830e+02
B:(intercept) 1.182216e-07 5.488637e+00
A:V1          1.831725e-02 8.241213e+04
B:V1          1.176881e-01 4.123248e+04
A:V2          3.446800e-13 2.154711e-01
B:V2          5.864847e-06 9.688857e+00
A:V3          1.394913e-05 2.211978e+06
B:V3          7.994348e-03 4.505896e+04
A:V4          8.066986e-03 4.082774e+00
B:V4          1.875058e-02 1.881996e+00
A:V5          1.400307e-01 7.638467e+01
B:V5          2.392271e-01 3.285238e+01
A:V6          2.870699e+01 5.517731e+05
B:V6          7.303065e-01 1.347282e+03
A:V7          1.366460e-03 4.978060e+01
B:V7          6.223884e-02 1.306918e+02
A:V8          1.752860e-07 1.880591e+00
B:V8          1.891518e-02 8.185990e+01
</code></pre>

<p>The predicted Probabilities are as following:</p>

<pre><code>fitted(MLogitFit, outcome=FALSE)
                 Z            A          B
 [1,] 0.2790108926 3.880184e-01 0.33297074
 [2,] 0.5191458618 2.900625e-01 0.19079169
 [3,] 0.7263001933 1.633014e-02 0.25736966
 [4,] 0.8386056883 3.700203e-03 0.15769411
 [5,] 0.8050365007 7.487290e-03 0.18747621
 [6,] 0.7855655154 3.860347e-02 0.17583101
 [7,] 0.7878404896 7.992930e-03 0.20416658
 [8,] 0.8386056883 3.700203e-03 0.15769411
 [9,] 0.7878404896 7.992930e-03 0.20416658
[10,] 0.4363708036 2.827104e-01 0.28091885
[11,] 0.6126060746 3.320075e-02 0.35419317
[12,] 0.0274357267 8.418204e-01 0.13074390
[13,] 0.1438998597 5.869087e-01 0.26919146
[14,] 0.1850027820 2.105586e-01 0.60443858
[15,] 0.8427092407 5.933393e-03 0.15135737
[16,] 0.1537160539 4.929905e-01 0.35329341
[17,] 0.0434283140 6.358897e-01 0.32068201
[18,] 0.1868202029 1.141679e-01 0.69901186
[19,] 0.3064594418 1.156597e-01 0.57788084
[20,] 0.5737141160 6.734724e-02 0.35893865
[21,] 0.5841338911 1.374758e-01 0.27839031
[22,] 0.0866451414 4.019366e-01 0.51141821
[23,] 0.2794060013 9.964607e-02 0.62094793
[24,] 0.0252343516 7.343045e-01 0.24046118
[25,] 0.1314775919 4.602643e-01 0.40825811
[26,] 0.0274357267 8.418204e-01 0.13074390
[27,] 0.1303195991 6.649645e-01 0.20471586
[28,] 0.2818251202 4.896734e-01 0.22850146
[29,] 0.0063990341 8.874618e-01 0.10613917
[30,] 0.0002408527 9.742025e-01 0.02555668
[31,] 0.0523052465 7.073015e-01 0.24039322
[32,] 0.3287956423 2.756959e-01 0.39550841
[33,] 0.0419093705 7.521689e-01 0.20592173
[34,] 0.0523052465 7.073015e-01 0.24039322
[35,] 0.3287956423 2.756959e-01 0.39550841
[36,] 0.0100998700 7.475180e-01 0.24238212
[37,] 0.1609808596 2.268570e-01 0.61216212
[38,] 0.0119603037 8.065964e-01 0.18144331
[39,] 0.0697132279 4.549378e-01 0.47534896
[40,] 0.5756435353 6.315652e-02 0.36119994
[41,] 0.4689676672 6.796615e-02 0.46306619
[42,] 0.2652679745 6.358962e-02 0.67114240
[43,] 0.7870195702 2.038999e-03 0.21094143
[44,] 0.6438437943 9.222002e-03 0.34693420
[45,] 0.7462282258 5.881047e-04 0.25318367
[46,] 0.3532662528 2.193975e-01 0.42733620
[47,] 0.9563852795 4.133754e-05 0.04357338
[48,] 0.9079031419 2.786314e-03 0.08931054
[49,] 0.0220230156 8.017508e-01 0.17622619
[50,] 0.2268852285 1.745210e-01 0.59859376
[51,] 0.2268852285 1.745210e-01 0.59859376
[52,] 0.0751929214 6.261548e-01 0.29865225
[53,] 0.9426667411 4.520877e-06 0.05732874
[54,] 0.0212631471 6.729961e-01 0.30574075
[55,] 0.0212631471 6.729961e-01 0.30574075
[56,] 0.9218535421 1.166953e-02 0.06647693
[57,] 0.6374868816 3.856300e-02 0.32395012
[58,] 0.2920703240 2.410709e-01 0.46685876
[59,] 0.7047942848 1.728601e-02 0.27791970
[60,] 0.1850395244 5.297673e-01 0.28519316
[61,] 0.4402296785 8.870861e-03 0.55089946
[62,] 0.6781988218 3.852569e-04 0.32141592
[63,] 0.9889453179 4.036588e-05 0.01101432
[64,] 0.1618635354 8.011851e-02 0.75801796
[65,] 0.3008372801 9.835522e-02 0.60080750
[66,] 0.0740319347 4.284039e-01 0.49756417
[67,] 0.5529727485 1.768537e-01 0.27017351
[68,] 0.7824740564 5.001713e-03 0.21252423
[69,] 0.5343045050 5.865850e-02 0.40703700
[70,] 0.4564647083 1.733995e-01 0.37013579
[71,] 0.4711837972 8.449081e-03 0.52036712
[72,] 0.9154349308 2.364316e-02 0.06092191
[73,] 0.1858643216 2.217595e-01 0.59237621
[74,] 0.3770813535 9.943397e-02 0.52348468
[75,] 0.8124141650 3.243679e-04 0.18726147
[76,] 0.3195206223 2.932236e-01 0.38725578
[77,] 0.8615871019 5.063299e-04 0.13790657
[78,] 0.8615871019 5.063299e-04 0.13790657
[79,] 0.8254986241 2.059378e-03 0.17244200
[80,] 0.1208591778 4.615235e-01 0.41761730
[81,] 0.0035765650 9.093754e-01 0.08704806
[82,] 0.7583239965 3.544345e-02 0.20623255
[83,] 0.8141948591 5.016280e-03 0.18078886
[84,] 0.1204323818 2.545405e-01 0.62502710
[85,] 0.9594950290 3.694056e-05 0.04046803
[86,] 0.6858228916 1.691396e-01 0.14503752
[87,] 0.8254986241 2.059378e-03 0.17244200
[88,] 0.8254986241 2.059378e-03 0.17244200
[89,] 0.2463233530 2.793410e-01 0.47433568
[90,] 0.5674338104 1.448538e-02 0.41808081
</code></pre>

<p>To assess multicolinearity I calculated the VIF statistic but using the a glm model of the same dataset.</p>

<pre><code>fullmod&lt;-glm(as.factor(Outcome)~.,data=test,family=binomial())
vif(fullmod)
      V1       V2       V3       V4       V5       V6       V7       V8 
1.789116 1.822252 2.216444 1.320244 1.821820 1.439183 1.512865 1.121805 
</code></pre>
"
"0.0545088647991304","0.0649295895722714","173132","<p>*********Okay so I figured out what was wrong! I wasn't centering the date like the lm.ridge function does. However I still cannot reproduce the intercept that lm.ridge gives me.</p>

<p>According to my research you can simulate a ridge regression by adding ""phony data"" to the end of a normal OLS regression.  One of many places that corroborate this notion is this CV thread: <a href=""http://stats.stackexchange.com/questions/137057/phoney-data-and-ridge-regression-are-the-same"">Phoney data and ridge regression are the same?</a></p>

<p>However I fail to replicate the results in R. Here are my three variables: </p>

<pre><code>&gt; test_0
12    34    24    64   746    24    23    42     7     8     3     4    45   675     3     4    34    43  56   674     3     4    54    34    23    34   435    56    56   234   657    89   980     8    76    65 45564    67    76   789

&gt; test_1
34    24    64   746    24    23    42     7     8     3     4    45   675     3     4    34    43    56 674     3     4    54    34    23    34   435    56    56   234   657    89   980     8    76    65 45564  67    76   789     6

&gt; test_2
24    64   746    24    23    42     7     8     3     4    45   675     3     4    34    43    56   674 3     4    54    34    23    34   435    56    56   234  657    89   980     8    76    65 45564    67 76   789     6     5
</code></pre>

<p>I then append 2 new rows (for the number of independent vars). To test_0 I append two zeros. To test_1 I append a sqrt(.5) and 0. To test_2 I append a 0 and sqrt(.5)</p>

<pre><code>&gt; a = c(test_0, 0, 0)
&gt; b = c(test_1, (sqrt(.5)), 0)
&gt; c = c(test_2, 0, (sqrt(.5)))
</code></pre>

<p>Then I run two models, <code>lm</code> and <code>lm.ridge</code>:</p>

<pre><code>&gt;reg = lm(a~b+c)
&gt;ridge = lm.ridge(test_0~test_1+test_2, lambda=.5)
&gt; reg
 Call:
 lm(formula = a ~ b + c)

 Coefficients:
 (Intercept)            b            c  
  1305.42310     -0.02926     -0.02862  

&gt; ridge
                      test_1        test_2 
 1374.16801379   -0.03059968   -0.02996396 
</code></pre>

<p>The coefficients are different but they should be the same. Why is this the case?</p>

<p><em>I have also tried the above using a lambda of 1 and still get the inconsistency.</em></p>
"
"0.0646395620382857","0.0513313476470739","173207","<p>I have a dataset gpa2 ddata that can be found here <a href=""https://www.dropbox.com/s/7rphi1k9pxert1a/ddata.csv?dl=1"" rel=""nofollow"">https://www.dropbox.com/s/7rphi1k9pxert1a/ddata.csv?dl=1</a></p>

<p>I estimate the model colgpa = athelte.</p>

<pre><code>gpa2 &lt;- read.csv(~""/path/ddata.csv"")
model1 &lt;- lm(formula = colgpa ~ athlete, data = gpa2)
summary(model1)
</code></pre>

<p>And now I want to see if I can get <strong>Std.Error by this formula</strong>
$$se(\beta_j) = \frac{\hat \sigma_u}{SST_j(1âˆ’R_j^2)}$$
where $$SST_j = \sum_{i=1}^n (x_{ij} - \bar x_j)^2$$ is the total sample variation in $x_j$
and $R_j^2$ is the $R^2$ from regressing $x_j$ on all the other independent variables.</p>

<p>From this answer 
<a href=""http://stats.stackexchange.com/questions/44838/how-are-the-standard-errors-of-coefficients-calculated-in-a-regression"">How are the standard errors of coefficients calculated in a regression?</a>
we know that the standard error of the estimated slope, $se(\beta_1)$ in our case, is
$$\sqrt{\widehat{\textrm{Var}}(\hat{b})} = \sqrt{[\hat{\sigma}^2  (\mathbf{X}^{\prime} \mathbf{X})^{-1}]_{22}} = \sqrt{\frac{n \hat{\sigma}^2}{n\sum x_i^2 - (\sum x_i)^2}}.$$</p>

<p>I do this with an anova-table, and try to </p>

<pre><code>anova(model1) # the anova table
# now I take out elements of the anova
model1_hatsigmau &lt;- anova(model1)[[3]][2] #takes row 3 column 2 in the anova-table.
model1_MSathlete &lt;- anova(model1)[[3]][1]
model1_SSathlete &lt;- anova(model1)[[2]][1]
numerator &lt;- n*model1_hatsigmau
meanathlete &lt;- mean(gpa2$athlete)
denominator &lt;- n*model1_SSathlete - (n*meanathlete)^2 
sqrt(numerator /  denominator) # should be se(beta_1) for model1. 
</code></pre>

<p>But I get </p>

<pre><code>&gt; sqrt(numerator /  denominator) # should be se(beta_1) for model1.
[1] 0.270643
</code></pre>

<p>And not $0.04824$ as in the summary-output (see below). So the problem is that $$0.270643 \neq 0.04824$$</p>

<pre><code>&gt; summary(model1)

Call:
lm(formula = colgpa ~ athlete, data = gpa2)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.66603 -0.43603  0.00397  0.46397  1.61851 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  2.66603    0.01045 255.212  &lt; 2e-16 ***
athlete     -0.28453    0.04824  -5.898 3.97e-09 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.656 on 4135 degrees of freedom
Multiple R-squared:  0.008343,  Adjusted R-squared:  0.008104 
F-statistic: 34.79 on 1 and 4135 DF,  p-value: 3.966e-09
</code></pre>
"
"0.103465538715443","0.0915538363472867","173335","<p>I have two regression models (see output below). I want to put the outputs of model1 and model2 side by side, and compare them.</p>

<p>In Stata I would run </p>

<blockquote>
  <p>reg colgpa athlete
  outreg2  colgpa athlete using comparison.tex, append
  reg colgpa athlete sat
  outreg2  colgpa athlete sat using comparison.tex, append</p>
</blockquote>

<p>and then open <em>comparison.tex</em> How do I do it in R using knitr?</p>

<p>I am looking for a command that would output summary(model1) and summary(model2) in the same console.</p>

<hr>

<p>Some background information:</p>

<p>model1:</p>

<pre><code>&gt; summary(model1)

Call:
lm(formula = colgpa ~ athlete, data = gpa2)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.66603 -0.43603  0.00397  0.46397  1.61851 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  2.66603    0.01045 255.212  &lt; 2e-16 ***
athlete     -0.28453    0.04824  -5.898 3.97e-09 ***
---
Signif. codes:  
0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.656 on 4135 degrees of freedom
Multiple R-squared:  0.008343,  Adjusted R-squared:  0.008104 
F-statistic: 34.79 on 1 and 4135 DF,  p-value: 3.966e-09
</code></pre>

<p>model2:</p>

<pre><code>&gt; summary(model2)

Call:
lm(formula = colgpa ~ athlete + sat, data = gpa2)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.84611 -0.38276  0.03056  0.42472  1.76647 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  6.801e-01  7.134e-02   9.533   &lt;2e-16 ***
athlete     -5.061e-02  4.499e-02  -1.125    0.261    
sat          1.917e-03  6.823e-05  28.092   &lt;2e-16 ***
---
Signif. codes:  
0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.6012 on 4134 degrees of freedom
Multiple R-squared:  0.1673,    Adjusted R-squared:  0.1669 
F-statistic: 415.3 on 2 and 4134 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>I can of course type</p>

<blockquote>
  <p>summary(model1)</p>
  
  <p>summary(model2)</p>
</blockquote>

<p>And let the reader eyeball back and forth. But I want them side by side. This, I can do using <code>stargazer(model1, model2, title=""Regression Results"", align=TRUE)</code> but then I cannot connect it to knitr so that when I knit HTML it produces the output correctly. My goal is namely to go from the Rmd file to a published report directly.</p>

<p>So, to summarize, I am looking for a command that would output summary(model1) and summary(model2) in the same console. This would solve muy problem (although perhaps in an ugly way). If you know a more elegant solution -- perhaps to go from knitr to latex or another approach enterily, my ears are huge. </p>

<p><code>stargazer(model1, model2, title=""Regression Results"", align=TRUE)</code> outputs</p>

<pre><code>% Table created by stargazer v.5.2 by Marek Hlavac, Harvard University. E-mail: hlavac at fas.harvard.edu
% Date and time: Sun, Sep 20, 2015 - 14:56:02
% Requires LaTeX packages: dcolumn 
\begin{table}[!htbp] \centering 
      \caption{Regression Results} 
      \label{} 
    \begin{tabular}{@{\extracolsep{5pt}}lD{.}{.}{-3} D{.}{.}{-3} } 
    \\[-1.8ex]\hline 
    \hline \\[-1.8ex] 
     &amp; \multicolumn{2}{c}{\textit{Dependent variable:}} \\ 
    \cline{2-3} 
    \\[-1.8ex] &amp; \multicolumn{2}{c}{colgpa} \\ 
    \\[-1.8ex] &amp; \multicolumn{1}{c}{(1)} &amp; \multicolumn{1}{c}{(2)}\\ 
    \hline \\[-1.8ex] 
     athlete &amp; -0.285^{***} &amp; -0.051 \\ 
      &amp; (0.048) &amp; (0.045) \\ 
      &amp; &amp; \\ 
     sat &amp;  &amp; 0.002^{***} \\ 
      &amp;  &amp; (0.0001) \\ 
      &amp; &amp; \\ 
     Constant &amp; 2.666^{***} &amp; 0.680^{***} \\ 
      &amp; (0.010) &amp; (0.071) \\ 
      &amp; &amp; \\ 
    \hline \\[-1.8ex] 
    Observations &amp; \multicolumn{1}{c}{4,137} &amp; \multicolumn{1}{c}{4,137} \\ 
    R$^{2}$ &amp; \multicolumn{1}{c}{0.008} &amp; \multicolumn{1}{c}{0.167} \\ 
    Adjusted R$^{2}$ &amp; \multicolumn{1}{c}{0.008} &amp; \multicolumn{1}{c}{0.167} \\ 
    Residual Std. Error &amp; \multicolumn{1}{c}{0.656 (df = 4135)} &amp; \multicolumn{1}{c}{0.601 (df = 4134)} \\ 
    F Statistic &amp; \multicolumn{1}{c}{34.790$^{***}$ (df = 1; 4135)} &amp; \multicolumn{1}{c}{415.289$^{***}$ (df = 2; 4134)} \\ 
    \hline 
    \hline \\[-1.8ex] 
    \textit{Note:}  &amp; \multicolumn{2}{r}{$^{*}$p$&lt;$0.1; $^{**}$p$&lt;$0.05; $^{***}$p$&lt;$0.01} \\ 
    \end{tabular} 
    \end{table} 
</code></pre>

<p>ïœ€</p>
"
"0.0430930413588572","0.0410650781176591","173343","<p>Suppose we have a Gaussian regression model, y_i~N(Î±+Î²_1 x_i+Î²_2 z_i,Ïƒ). This model is fitted to the trees data in R where y is Height, x is Girth and z is Volume. 
Following is the result from the simple linear regression:</p>

<p>Call:
lm(formula = Height ~ Girth + Volume)</p>

<p>Coefficients:
(Intercept)        Girth       Volume<br>
    83.2958      -1.8615       0.5756</p>

<p>Generate random values for Height based on the model with Ïƒ = 5 that corresponds with each pair of values for Girth and Volume.</p>
"
"0.0304713817668003","0.029037395206952","173568","<p>In helping us understand how to fit a logistic regression in <code>R</code>, we are told to first replace 0 and 1 in the response variable by 0.05 and 0.95, respectively and second to take the logit transform of the resulting response variable. Last we fit these data using iterative re-weighted least squares method. </p>

<p>Then we are asked to use 0.005 and 0.995 instead of 0.05 and 0.95. Then the resulting coefficients are quite <strong>different</strong>.</p>

<p>My question is in <code>glm</code> function, how are 0 and 1 dealt with? Are they replaced by some numbers as above? What numbers are used by default and why are they used? How sensitive is the choice of these numbers?</p>
"
"0.0575854987567582","0.0548755188847815","173629","<p>When applying the ""urca"" package function <code>ur.df</code>, like </p>

<pre><code>summary(ur.df(data$col1, type = c(""none""), lags = 12, selectlags = c(""AIC"")))
</code></pre>

<p>I get following result:</p>

<pre><code>############################################### 
# Augmented Dickey-Fuller Test Unit Root Test # 
############################################### 

Test regression trend 


Call:
lm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag)

Residuals:
      Min        1Q    Median        3Q       Max 
-12928366  -2888728   1284718   4218373   7179531 

Coefficients:
                 Estimate    Std. Error  t value  Pr(&gt;|t|)   
(Intercept)  5.391984e+07  1.638362e+07  3.29108 0.0043123 **
z.lag.1     -2.438154e+00  7.557134e-01 -3.22629 0.0049588 **
tt           6.579260e+05  2.730453e+05  2.40959 0.0275861 * 
z.diff.lag1  1.712004e+00  6.595980e-01  2.59553 0.0188537 * 
z.diff.lag2  1.402824e+00  6.379412e-01  2.19899 0.0420083 * 
z.diff.lag3  1.321555e+00  5.294537e-01  2.49607 0.0231329 * 
z.diff.lag4  1.099430e+00  4.720412e-01  2.32910 0.0324428 * 
z.diff.lag5  8.132753e-01  4.181477e-01  1.94495 0.0685140 . 
z.diff.lag6  1.797331e-01  3.654326e-01  0.49184 0.6291254   
z.diff.lag7  5.890640e-01  2.939590e-01  2.00390 0.0612825 . 
z.diff.lag8  3.919041e-01  2.794371e-01  1.40248 0.1787705   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 6708593 on 17 degrees of freedom
Multiple R-squared:  0.7237276, Adjusted R-squared:  0.5613144 
F-statistic: 4.253547 on 10 and 17 DF,  p-value: 0.003348755


Value of test-statistic is: -3.2263 3.9622 5.2635 

Critical values for test statistics: 
      1pct  5pct 10pct
tau3 -4.15 -3.50 -3.18
phi2  7.02  5.13  4.31
phi3  9.31  6.73  5.61
</code></pre>

<p>Now the question:</p>

<ol>
<li>I do understand that ""-3.2263"" is the critical value (t-value)</li>
<li><strong>There is a unit root</strong> with trend since -3.2263 > -3.18 (tau3@10pct)
This means the time-series is <strong>non-stationary</strong> at a 10% significance level.</li>
<li>But, what is the meaning of ""p-value: 0.003348755""? Should I list this value in a table summarizing my unit root test results or rather mark the 0.1 significance level (*10%)?</li>
</ol>

<p>The <a href=""http://www.inside-r.org/packages/cran/urca/docs/ur.df"" rel=""nofollow"">documentation</a> says that critical values are based on Hamilton (1994) and Dickey and Fuller (1981)"". </p>
"
"0.035185320931284","0.0502942438178979","173828","<p>I have a data set shown below, the 1st, 2nd,3rd column are dependent variable(dv), and 2 independent variables (iv1 &amp; iv2) respectively, I expected the regression coefficient of the ""iv1"" shows a positive value, as there is a positive correlation between dv and iv1. However, The result shows a negative regression coefficient for iv1 (beta_iv1 = -0.55), I am wondering why this happened, I appreciate if anyone can help.</p>

<p>dv    iv1     iv2</p>

<p>1     0.00    7.70<br>
1     2.90    0.00<br>
1     0.00    7.70<br>
1     2.90    0.50<br>
1     0.00    9.50<br>
1     1.50    7.70<br>
1     5.70    0.50<br>
1     7.10    2.30<br>
1     5.70    4.10<br>
1     0.00    4.10<br>
1     4.30    4.10<br>
1     0.00    10.00<br>
1     0.00    4.10<br>
1     2.90    0.50<br>
1     0.00    9.50<br>
0     0.00    9.50<br>
1     0.00    5.90<br>
1     0.00    4.10<br>
1     1.50    5.90<br>
1     5.70    2.30<br>
1     1.50    0.00<br>
0     0.00    10.00<br>
1     5.70    0.00<br>
1     5.70    0.50<br>
1     4.30    2.30<br>
0     0.00    10.00<br>
1     2.90    5.90<br>
1     0.00    5.90<br>
1     0.00    5.90<br>
1     2.90    2.30<br>
1     1.50    2.30<br>
1     2.90    0.50<br>
1     5.70    4.10<br>
1     1.50    0.00<br>
1     0.00    7.70  </p>

<p>I run this using R with package ""logistf"" which overcomes separation problem of logistic regression. The code I run this data set is as below:</p>

<blockquote>
  <p>library(logistf);<br>
      tempT=read.table(fileS);<br>
     fit&lt;-logistf(dv ~ iv1+iv2, data=tempT);</p>
</blockquote>

<p>and the result shows below:  </p>

<pre><code>           coef  se(coef) lower 0.95  upper 0.95     Chisq      p
</code></pre>

<blockquote>
  <p>(Intercept)  9.0086382 5.1741382   1.650380 61.61244068 7.6897111 
  0.005553652<br>
  tempT[, 2]  -0.5509122 0.6567110  -6.013208  1.55280975 0.5490404 0.458710039<br>
  tempT[, 3]  -0.9051062 0.5597601  -6.317335 -0.06328166 4.8315401 0.027943657</p>
</blockquote>

<p>Likelihood ratio test=7.213821 on 2 df, p=0.02713555, n=35</p>
"
"0.091874672876503","0.0875510407188402","173996","<p>I'm using R (package lmer) to run linear mixed model My study looks at allergy levels of skin patches from patients and readings (repeated 5 times) are measured over 4 time points.</p>

<p>I need to determine if the allergy level for skin patch changes over time
(e.g., if allergy level from skin patch 1 for patient 1 at time 0 is different from allergy level for skin patch 1 for patient 1 at time 1 etc.) I do not want to see the difference between skin patch 1 and skin patch 2. Using package lmer:  </p>

<pre><code>model &lt;- lmer(allergy_level ~ time +(time|patient/patch))
</code></pre>

<p><strong>Results from this model indicate that time is not significant - the average patient allergy level for individual skin patches does not change over time</strong> (see below for output). However, <strong>I need to be able to tell if there is a significant difference for individual patches for individual patients over time</strong>.</p>

<p>If I run individual regression models for each skin patch for each patient, this will result in a large number of models as I have There are 16 skin patches per patient. (10 patients in total) 5 readings are taken at each of the 4 time points. I thought linear mixed models would be an appropriate method to answer my question (I need to be able to tell if there is a significant difference for individual patches for individual patients over time). </p>

<p>Output:</p>

<pre><code>Random effects:
 Groups   Name        Variance Std.Dev. Corr             
 ID:patch (Intercept) 17.4109  4.1726                    
          time1        2.7109  1.6465   -0.30            
          time2        3.0082  1.7344   -0.26  0.60      
          time3        5.7643  2.4009   -0.35  0.15  0.54
 patch    (Intercept) 19.1576  4.3769                    
          time1        0.2103  0.4586   -0.56            
          time2        0.4372  0.6612   -0.94  0.48      
          time3        0.5895  0.7678   -0.48  0.96  0.49
 Residual              4.9467  2.2241                    
Number of obs: 2956, groups:  ID:patch, 149; patch, 16

Fixed effects:
            Estimate Std. Error t value
(Intercept)  6.44763    1.15028   5.605
time1       -0.01907    0.21237  -0.090
time2       -0.03172    0.24759  -0.128
time3       -0.01124    0.29940  -0.038

model1: AllergyLevel ~ 1 + (1 + time | patch/ID)
model2: AllergyLevel ~ time + (1 + time | patch/ID)
         Df   AIC   BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)
model11 22 14281 14413 -7118.5    14237                         
model12 25 14287 14437 -7118.4    14237 0.0208      3     0.9992
</code></pre>

<p>I have extracted the random coefficients from model 1:</p>

<pre><code>ranef(model1)

`ID:patch`
      (Intercept)       time1        time2        time3
1:11    5.9845070  0.34088535  0.431998708  1.590906238
1:12    5.1236456 -0.03178611 -0.149784278 -0.116150278
1:13    6.3746877 -0.76853294 -0.550037715  0.842518786
   :
   :
</code></pre>
"
"0.0914141453004008","0.0871121856208561","174057","<p>This is probably an embarrassingly easy question, but where else can I turn to... </p>

<p>I'm trying to put together examples of regression with mixed effects using <code>lmer</code> {lme4}, so that I can present [R] code that automatically downloads toy datasets in Google Drive and run every instance in <a href=""http://stats.stackexchange.com/a/13173/67822"">this blockbuster post</a>. </p>

<p>And starting with the first case (i.e. <code>V1 ~ (1|V2) + V3</code>, where <code>V3</code> is a continuous variable acting as a fixed effect, and <code>V2</code> is <code>Subjects</code>, both trying to account for <code>V1</code>, a continuous DV), I was expecting to retrieve different intercepts for each one of the <code>Subjects</code> and a single slope for all of them. Yet, this was not the case consistently.</p>

<p>I don't want to bore you with the origin or meaning of the datasets below, because I'm sure most of you get the idea without much explaining. So let me show you what I get... If you're so inclined you can just copy and paste in [R]... it should work if you have {lme4} in your Environment:</p>

<h1>Expected Output:</h1>

<pre><code>politeness &lt;- read.csv(""http://www.bodowinter.com/tutorial/politeness_data.csv"")
head(politeness)

  subject   gender scenario  attitude frequency
1      F1      F        1      pol     213.3
2      F1      F        1      inf     204.5
3      F1      F        2      pol     285.1
4      F1      F        2      inf     259.7    


library(lme4)

fit &lt;- lmer(frequency ~ (1|subject) + attitude, data = politeness)

coefficients(fit)
            $subject
               (Intercept) attitudepol
            F1    241.1352   -19.37584
            F2    266.8920   -19.37584
            F3    259.5540   -19.37584
            M3    179.0262   -19.37584
            M4    155.6906   -19.37584
            M7    113.2306   -19.37584
</code></pre>

<h1>Surprising Output:</h1>

<pre><code>library(gsheet)
recall &lt;- read.csv(text = 
    gsheet2text('https://drive.google.com/open?id=1iVDJ_g3MjhxLhyyLHGd4PhYhsYW7Ob0JmaJP8MarWXU',
              format ='csv'))
head(recall)

 Subject Time Emtl_Value Recall_Rate Caffeine_Intake
1     Jim    0   Negative          54              95
2     Jim    0    Neutral          56              86
3     Jim    0   Positive          90             180
4     Jim    1   Negative          26             200

fit &lt;- lmer(Recall_Rate ~ (1|Subject) + Caffeine_Intake, data = recall)

coefficients(fit)
        $Subject
               (Intercept) Caffeine_Intake
        Jason     51.51206        0.013369
        Jim       51.51206        0.013369
        Ron       51.51206        0.013369
        Tina      51.51206        0.013369
        Victor    51.51206        0.013369
</code></pre>

<p>Here is the output of (<code>summary(fit)</code>):</p>

<pre><code>Linear mixed model fit by REML ['lmerMod']
Formula: Recall_Rate ~ (1 | Subject) + Caffeine_Intake
   Data: recall

REML criterion at convergence: 413.9

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-1.54125 -0.98422  0.04967  0.81465  1.83317 

Random effects:
 Groups   Name        Variance Std.Dev.
 Subject  (Intercept)   0.0     0.00   
 Residual             601.2    24.52   
Number of obs: 45, groups:  Subject, 5

Fixed effects:
                Estimate Std. Error t value
(Intercept)     51.51206    5.92408   8.695
Caffeine_Intake  0.01337    0.03792   0.353

Correlation of Fixed Effects:
            (Intr)
Caffen_Intk -0.787
</code></pre>

<h1>Question:</h1>

<p><strong>Why are all the Intercepts for the different subjects the same in the second example? The structure of the datasets and the <code>lmer</code> syntax appear very similar... and the boxplots don't seem to support the result:</strong></p>

<p><a href=""http://i.stack.imgur.com/xXYdS.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xXYdS.png"" alt=""enter image description here""></a></p>

<p>Thank you in advance!</p>
"
"0.121885527067201","0.116149580827808","174136","<p>I have a dataset of a metric predictor variable $X$, and an ordered categorical predicted value $Y$ for several individuals. The dataset are from two groups $G_1$ and $G_2$. I want to estimate $Y$ from $X$, and I want to be able to compare the forecast accuracy of models, in group and individual level. For example, I want to know if these models helps to estimate $Y$ from a $X$ for a new user of a category, or a new experiment from the same user of a known category?</p>

<p>In <a href=""https://en.wikipedia.org/wiki/Ordered_probit"" rel=""nofollow"">ordered probit</a>, we suppose that $Y^*$ is the exact but unobserved dependent variable, and $X$ is the vector of independent variables, and $\beta$ is the a regression coefficient which we wish to estimate.</p>

<p>$Y^* = \mathbf{x}' \beta + \epsilon$</p>

<p>We can not observer $y*$ directly, but we instead can only observe the categories of response:</p>

<p>$
Y= \begin{cases}
0~~ \text{if}~~y^* \le 0, \\
1~~ \text{if}~~0&lt;y^* \le \mu_1, \\
2~~ \text{if}~~\mu_1 &lt;y^* \le \mu_2 \\
\vdots \\
N~~ \text{if}~~ \mu_{N-1} &lt; y^*.
\end{cases}
$</p>

<p>I came across this article from Gelman et al. that describes Bayesian Hierarchical Model: <a href=""https://en.wikipedia.org/wiki/Ordered_probit"" rel=""nofollow"">Multilevel (Hierarchical) Modeling: What It Can and Cannot Do</a>, which has been implemented in Python <a href=""http://nbviewer.ipython.org/github/fonnesbeck/multilevel_modeling/blob/master/multilevel_modeling.ipynb"" rel=""nofollow"">here</a>.</p>

<p>I am processing data in R, and I have selected a <strong>thresholded Bayesian hierarchical model</strong> to use with the <strong>generalized linear model</strong>. I have calculated the parameters of it using MCMC. My question is that how should I compare accuracy of ordered probit, and the equivalent Bayesian hierarchical model in R?</p>

<p>Gelman has used <a href=""https://en.wikipedia.org/wiki/Root-mean-square_deviation"" rel=""nofollow"">RMSE</a> for comparison using cross-validation. First he <em>removed single data points and checked the prediction from the model fit to the rest of the data, then removed single counties and performed the same procedure. For each cross-validation step, we compare complete-pooling, no-pooling, and multilevel estimates.</em></p>

<p>I have done MCMC simulation using RJags, which gave me the posterior distribution of the parameters, but how can I compare posterior distribution with a single point estimate of <strong>ordered probit</strong> to compare accuracy? Should I do as Gleman did and use RMSE? How? Or should I compare posterior distribution with results of several experiments with ordered probit? Is <a href=""http://www.stat.columbia.edu/~gelman/presentations/ggr.pdf"" rel=""nofollow"">posterior predictive check</a> usable here? I usually prefer cross-validation, but I don't know how to do this here.</p>

<p>PS: The notion of <strong>Goodness of fit</strong> in Bayesian analysis is ambigious to me. <a href=""http://people.stat.sfu.ca/~tim/papers/survey.pdf"" rel=""nofollow"">This paper</a> states:</p>

<blockquote>
  <p>GOODNESS-OF-FIT:</p>
  
  <p>In Bayesian statistics, there is no consensus on the
  correct"" approach to the assessment of goodness-of fit. When Bayesian
  model assessment is considered, it appears that the prominent modern
  approaches are based on the posterior predictive distribution (Gelman,
  Meng and Stern 1996).</p>
</blockquote>
"
"0.0867230728520531","0.0826418755551823","174557","<p>I'm currently running a ridge regression in R using the <code>glmnet</code> package, however, I recently ran into a new problem and was hoping for some help in interpreting my results. My data can be found here: <a href=""https://www.dropbox.com/sh/hpxu3t0vqkrzfgf/AAB6F-yMYMfuI5E__gfDuW6sa?dl=0"" rel=""nofollow"">https://www.dropbox.com/sh/hpxu3t0vqkrzfgf/AAB6F-yMYMfuI5E__gfDuW6sa?dl=0</a></p>

<p>My data consists of a 26531x428 observation matrix <code>x</code> and a 26531x1 response vector <code>y</code>. I am attempting to determine the optimal value of <code>lambda.min</code>, and when I run the code</p>

<p><code>&gt; lambda=cv.glmnet(x=x,y=y,weights=weights,alpha=0,nfolds=10,standardize=FALSE)</code></p>

<p>I get</p>

<p><code>$lambda.min
[1] 2.123479
$lambda.1se
[1] 619.0054</code></p>

<p>which are results I would expect. However, I would like to add a slight tweak to this regression. I have prior knowledge of each of my 428 coefficients, and instead of shrinking each coefficient towards 0, as is the default with ridge regression, I would like to shrink each coefficient towards a specific value other than 0. After reaching out to Dr. Trevor Hastie, one of the creators of <code>glmnet</code>, he told me that this could be achieved by running the same code after substituting <code>y</code> with <code>y2</code>, where <code>y2 = y - x%*%d</code> and <code>d</code> is a 428x1 vector of coefficient priors. He said to then add <code>d</code> to my new coefficients, which would give me my prior-informed coefficients. After rerunning the code</p>

<p><code>&gt; lambda=cv.glmnet(x=x,y=y2,weights=weights,alpha=0,nfolds=10,standardize=FALSE)</code></p>

<p>I unfortunately get</p>

<p><code>$lambda.min
[1] 220.3026
$lambda.1se
[1] 220.3026</code></p>

<p>The results of <code>plot(lambda)</code> look like this
<a href=""http://i.stack.imgur.com/ivP0b.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ivP0b.png"" alt=""lambda plot""></a></p>

<p>Does anyone know why <code>glmnet</code> can't find a suitable <code>lambda.min</code>? Could it be because my vector of priors contains estimates that are too far off? Any help would be greatly appreciated!</p>
"
"0.0609427635336005","0.0580747904139041","175203","<p>I'm reading <a href=""https://onlinecourses.science.psu.edu/stat504/node/177"" rel=""nofollow"">this tutorial</a> to understand how to interpret the coefficients of an ordinal logistic regression which assumes the proportional odds. </p>

<p>They use a dataset about a cheese tasting experiment. Subjects were randomly assigned to taste one of four different cheeses (A,B,C,D). Response categories are 1 = strong dislike to 9 = excellent taste.</p>

<pre><code>m1=polr(response~cheese,weights=N,data=dati)
summary(m1)

Re-fitting to get Hessian

Call:
polr(formula = response ~ cheese, data = dati, weights = N)

Coefficients:
         Value Std. Error t value
cheeseB -3.352     0.4287  -7.819
cheeseC -1.710     0.3715  -4.603
cheeseD  1.613     0.3805   4.238

Intercepts:
    Value    Std. Error t value 
1|2  -5.4674   0.5236   -10.4413
2|3  -4.4122   0.4278   -10.3148
3|4  -3.3126   0.3700    -8.9522
4|5  -2.2440   0.3267    -6.8680
5|6  -0.9078   0.2833    -3.2037
6|7   0.0443   0.2646     0.1673
7|8   1.5459   0.3017     5.1244
8|9   3.1058   0.4057     7.6547

Residual Deviance: 711.3479 
AIC: 733.3479 
</code></pre>

<p>The tutorial's author writes:</p>

<blockquote>
  <p>we see that the implied ordering of cheeses in terms of quality is D >
  A > C > B. Furthermore, D is significantly better preferred than A,
  but A is not significantly better than C.</p>
</blockquote>

<p>Is this correct? 
I do agree that cheese B and C are significantly worse than A, and that D is significantly better than A, but I don't understand why cheese A should not be significantly better than C, as the author claims.</p>

<p>This are instead my conclusions:<br>
Since $\beta_B \neq 0$ and $\beta_B &lt; 0$, then $B&lt;A$.<br>
Since $\beta_C \neq 0$ and $\beta_C &lt; 0$, then $C&lt;A$.<br>
Since $\beta_D \neq 0$ and $\beta_D &gt; 0$, then $D&gt;A$.<br>
So, $D&gt;A&gt;B$ and $D&gt;A&gt;B$.
But since $\beta_B &lt; \beta_C$, then $D&gt;A&gt;B&gt;C$.<br>
So, I would say instead that cheese A is significantly better than C.</p>
"
"NaN","NaN","175216","<p>I have a binary dependent variable (<code>R</code>) and four numeric independent variables (<code>Q, M, S, T</code>) and want to examine coefficients for them.</p>

<p>Here is my glm code in R:</p>

<pre><code>fit = glm(R ~ Q + M + S + T, data=data, family=binomial())
</code></pre>

<p>When I run <code>predict(fit)</code>, I get a lot of predicted values greater than one (but none below 0 so far as I can tell). I have tried bayesglm and glmnet per suggestions to similar questions but both are a little over my head and the output I did get didn't seem to fix my problem.</p>

<p>I want to know:
A) Is this typical of logistic regression?
B) If not, how do I fix it?</p>
"
"0.052777981396926","0.0502942438178979","175613","<p>I have a data frame like this</p>

<pre><code>  head(mydata)
    y         class
  -0.06047565     1
   0.76982251     1
   3.05870831     1
   2.07050839     1
   2.62928774     1
   4.71506499     1
   3.96091621     1
   2.73493877     1
   3.81314715     1
   4.55433803     1
   2.22408180     2
   2.35981383     2
   3.40077145     2
   4.11068272     2
   4.44415887     2
   7.78691314     2
   7.49785048     2
   6.03338284     2
   9.70135590     2
   9.52720859     2
</code></pre>

<p>where <code>y</code> are numeric values and <code>class</code> are factors</p>

<pre><code>class(mydata$class)
[1] ""factor""
</code></pre>

<p>I would like to perform a linear regression with zero slope and intercept that depends on the class.</p>

<p>However when I do</p>

<pre><code>lm(y ~ class,data = mydata)
</code></pre>

<p>I get this result which I don't understand</p>

<pre><code>Call:
lm(formula = y ~ class, data = mydata)

Coefficients:
(Intercept)       class2       class3       class4       class5  
      2.825        2.884        5.001        8.497       10.917 
</code></pre>

<p>Why there is only one intercept? And what are the values for each class, intercepts or slopes?</p>

<p>Many thanks</p>
"
"0.0746393370862076","0.0711268017165705","175654","<p>I understand that you have to run the resulting regression line through the logistic function to get the predicted probability:</p>

<pre><code>am.glm &lt;- glm(am ~ hp + wt, data=mtcars, family=binomial)
newdata &lt;- data.frame(hp=120, wt=2.8)
p1 &lt;- predict(am.glm, newdata, type=""response"") 
p2 &lt;- 1/(1+exp(-(am.glm$coefficients[1] +
                 am.glm$coefficients[2]*newdata[1,1] + 
                 am.glm$coefficients[3]*newdata[1,2])))
p1 - p2
##            1 
## 1.110223e-16
</code></pre>

<p>Now I want to build two scoring model with the aim in mind to be usable with a hand calculator only: </p>

<ol>
<li>First model: I want to just take the two variables ($hp$, $wt$), multiply them by some factor and add them. The resulting number should be compared to a threshold number which then gives me the decision.</li>
<li>Second model: I want to have certain ranges of the two variables. Depending on the range the variable falls into I am given a number. At the end I simply add all numbers to arrive at my threshold number which again gives me the decision.</li>
</ol>

<p>As an example from the area of credit scoring where these scorecards are used quite heavily (source: <a href=""https://www.eflglobal.com/insights-sbbns-credit-risk-problem-loan-management-workshop/"" rel=""nofollow"">https://www.eflglobal.com/insights-sbbns-credit-risk-problem-loan-management-workshop/</a>):</p>

<p><a href=""http://i.stack.imgur.com/RQnHQ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/RQnHQ.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/5IX9K.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5IX9K.png"" alt=""enter image description here""></a></p>

<p><strong>My question</strong><br>
How to go about and esp. how to transform the logistic regression coefficients to be able to build the two (or any of the two) models? </p>

<p>Perhaps you can even demonstrate the steps in R, making use of the above <code>mtcars</code> logistic regression.</p>
"
"0.105869651339174","0.108647984268766","175767","<p>Logistic regression models the relationship between a set of independent variables and the probability that a case is a member of one of the categories of the dependent variable. If the probability is greater than 0.5, the case is classified in the modeled category.  If the probability is less than 0.50, the case is classified in the other category. The problem is that when I run the model with my dataset, the probabilities are far from 0.5, in fact it never gets to that value.</p>

<p>Here is part of My dataset:</p>

<pre><code>  sum_profit   direction   profit_cl1
   10           up          0.00
   0            Not_up     -0.03
  -5            Not_up      0.04
  -5            Not_up     -0.04
</code></pre>

<p>I want to find a relationship between the price of oil and the stock price of a Colombian oil company. So the variable 'sum_profit' is the sum of the change in the stock price in the next ten minutes. The variable 'profit_cl1' shows me the net change in the oil price in the last 10 minutes. </p>

<p>So what I want to know is that if the oil price changes in the last 10 minutes how would I expect the stock price direction to be in the following 10 minutes (Up or Down).</p>

<p>The problem is that my probabilities once I run the logistic regression are far from 0.5 even though the model is significant </p>

<pre><code>    glm.fit=glm(formula = direction ~ profit_cl1, family = binomial, data = datos)

    Deviance Residuals: 
      Min       1Q   Median       3Q      Max  
    -0.6786  -0.6786  -0.6131  -0.6131   1.8783  

    Coefficients:
                    Estimate Std. Error z value Pr(&gt;|z|)    
    (Intercept)     -1.57612    0.01618 -97.394   &lt;2e-16 ***
    profit_cl1       0.22485    0.02288   9.829   &lt;2e-16 ***
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

    (Dispersion parameter for binomial family taken to be 1)

     Null deviance: 48530  on 50309  degrees of freedom
    Residual deviance: 48434  on 50308  degrees of freedom
    AIC: 48438

    Number of Fisher Scoring iterations: 4 
</code></pre>

<p>The code to get the probabilities:</p>

<pre><code>   log.probs=predict(glm.fit, type=""response"")
   mean(log.probs)=0.1873 
</code></pre>

<p>the 0.1873 is very far from 0.5.</p>

<p>Sorry but I did not know where else to look for help! I appreciate any suggestion!</p>
"
"0.0879633023282099","0.0922061136661462","175770","<p>This question is more of theoretical. I am not sure if this is the right place, but still giving it a try. </p>

<p>I have two variables &mdash; direct cost and indirect cost. When sales persons go for a sales pitch to a customer they know about direct cost that they are going to incur for this service, but they don't know much about indirect cost (they will come to know about it in latter stages). An estimate of indirect cost at this stage will be valuable for sales persons. </p>

<p>I am trying to predict indirect cost as a function of direct cost. I am doing this via a simple linear regression. I plotted scatter plot between direct cost and indirect cost and see a <strong>good linear relationship</strong> between them. I also see that direct cost and indirect cost are <strong>highly corelated</strong> to each other with correlation coefficient as 0.98, so I expected a very good prediction accuracy. But surprisingly, my prediction accuracy is not so good. I have around 200,000 points in my training data and average prediction error on training data is 17 %. Though adjusted R-Square value is 0.97. I am using <code>lm()</code> function from R.       </p>

<p>My question is that in case of simple linear regression, in general, should we expect better prediction accuracy if dependent and independent variables are highly correlated or is it my misconception? If we expect good accuracy, am I missing something here. Please note that I have also tried centering these variables around mean. </p>
"
"0.052777981396926","0.0502942438178979","175956","<p>For a (fictional) <strong>multiple logistic regression</strong>, let's consinder a DV 'hired' (0,1) and <strong>three dichotomous IVs</strong> 'college_degree' (0,1), 'affluent' (0,1) and 'recommendated' (0,1) for <em>N</em> = 1,000 participants.</p>

<p>Running a logistic regression and generating predicted probabilities of being hired using the <code>predict</code> function for a <code>glm</code> object works well. For every respondent I have a probability value ranging continuously from 0 to 1.</p>

<p>Since I do have a base distribution of all three IVs and the DV, I want a kind of simulator that predicts the <strong>percentage/proportion</strong> of the DV using each indivduals predicted probability.</p>

<p>Let's say in the sample 20% are hired, 50% have a college degree, 10% are affluent and 35% are recommendated. I want to use the predicted values to see how much would the <strong>proportion of 'hired' goes</strong> up, when I, e.g., <strong>change the proportion of recommendations to 50%</strong>. I guess, I could also use the equation with the coefficients of the logit model, but would need to run it for every individual.</p>

<p>Is there any way to implement this in R (well or Excel, if that is easier)?</p>
"
"0.0621994475718397","0.0711268017165705","176084","<p>I want to predict the impact of oil price over a Colombian oil company's stock price. I plan to use a multinomial regression for this with a categorical variable (Up, Down or Neutral given the direction of the stock price). Here is part of my dataset:</p>

<pre><code>Minute  ecopet  profit  sum_profit   direccion  cl1_chg   sum_cl1    direccion_cl1
571     2160     0       10           Up         -0.03     0.00      Down
572     2160     0        0           Neutral     0.07    -0.03      Down
573     2160     0       -5           Down       -0.08     0.04      Up
574     2160     0       -5           Down       -0.07    -0.04      Down
575     2160     5       -5           Down       -0.08    -0.11      Down
576     2165     0       -25          Down        0.00    -0.19      Down
577     2165     0       -25          Down       -0.05    -0.19      Down
578     2165     0       -15          Down       -0.17    -0.24      Down
579     2165     5       -15          Down       -0.06    -0.41      Down
580     2170     0       -20          Down        0.03    -0.47      Down
581     2170    -10       0           Neutral     0.04    -0.44      Down
</code></pre>

<p>My dependent variable is 'direccion'. But as you can see it has 3 response classes.the code I am using in R for the multinomial regression is:</p>

<pre><code>glm.fit=multinomial(direccion~direccion_cl1, data=datos)
</code></pre>

<p>I am working with intraday information and plan to predict what happens when the oil moves up/ down (in the previous 10 minutes) and how it impacts the stock price in the next 10 minutes.</p>

<p>The problem is that once I run the regression, what I get for glm.fit does not include the Coefficients for the level ""Down"". Would you know why is that? I get this:</p>

<pre><code> Call:
 multinom(formula = direccion ~ direccion_cl1, data = datos)

 Coefficients:
          (Intercept) direccion_cl1Up     
 Neutral   1.0505813       0.1955194 
 Up       -0.2513035       0.3936570 

 Residual Deviance: 90752.54 
 AIC: 90764.54 
</code></pre>

<p>Additional to this, when I use the function predict to see how well my model works I get this error message:</p>

<pre><code> log.probs=predict(glm.fit, ""probs"")
 Error in eval(expr, envir, enclos) : object 'direccion_cl1' not found
</code></pre>

<p>Thanks a lot!</p>
"
"0.0609427635336005","0.0580747904139041","176147","<p>Im trying to estimate the linear curve (y~x) where I know intercept must be normally distributed around -100, and slope always positive and normally distributed around 2 (blue continous line in plots below).</p>

<p>The example.data.1 below is ""clean"" and the linear regression (red dashed line) is ok. The resulting red dashed line is what I want.</p>

<p>But example.data.2 has many measurement errors so the red dashed line becomes unrealistic. The resulting line should be parallel with the blue line, but lower.</p>

<p>How can I assign a strong prior similar to the blue line in the plots, so that I get a posterior reasonably similar to the blue line?</p>

<pre><code>example.data.1 &lt;- structure(list(x = c(1.36, 2.22, 2.53, 3.09, 3.44, 3.25, 3.15, 
                                       3.21, 3.57, 3.63, 3.51, 2.85, 2.56, 2.25, 1.61, 1.35, 1, 1.6, 
                                       1.92, 1.9, 2.3, 2.61, 3.9, 3.74, 3.77, 3.77, 3.49, 3.37, 3.35, 
                                       2.79, 2.31, 1.88, 1.5, 1.18, 1.83, 2.32, 3.06, 3.37, 3.77, 3.82, 
                                       3.75, 3.72, 3.53, 3.35, 3.67, 3.18, 3.11, 2.43, 1.9, 1.39, 1.17, 
                                       1.48, 2.05, 2.62, 3.08, 3.65, 3.92, 4.08, 4.1, 3.47, 3.84, 3.45, 
                                       2.87, 2.83, 2.49, 1.87, 2.06, 2.49, 1.78, 2.33, 2.95, 3.73, 3.64, 
                                       3.62, 4.1, 3.85, 4.06, 3.67, 3.3, 2.86, 2.46, 2.32, 2.08, 1.64, 
                                       1.96), y = c(-101.04, -99.42, -98.33, -96.88, -95.22, -91.89, 
                                                    -91.63, -90.19, -92.98, -95.58, -95.69, -96.32, -96.94, -98.25, 
                                                    -100.11, -100.81, -101.87, -99.72, -99.94, -100.87, -100.38, 
                                                    -98.64, -93.38, -92.98, -93.39, -93.76, -93.25, -93.12, -94.46, 
                                                    -96.45, -97.46, -99.75, -100.09, -101.62, -101.1, -97.8, -96.33, 
                                                    -96.21, -94.37, -93.18, -93.32, -93.73, -94.13, -94.4, -94.63, 
                                                    -94.83, -96.29, -98.11, -100.2, -100.82, -101.56, -101.35, -100.61, 
                                                    -98.65, -97.37, -95.36, -95.45, -95.33, -95.63, -95.26, -97.08, 
                                                    -97.1, -97.14, -96.9, -98.17, -99.47, -100.17, -100.58, -100.55, 
                                                    -99.94, -99.02, -97.3, -96.25, -95.44, -95.69, -95.21, -95.87, 
                                                    -95.87, -97.71, -96.91, -97.62, -97.94, -98.9, -99.79, -99.88
                                       )), .Names = c(""x"", ""y""), row.names = c(NA, -85L), class = ""data.frame"")

example.data.2 &lt;- structure(list(x = c(3.11, 3.46, 3.42, 3.34, 3.3, 2.45, 4, 4.2, 
                                       4.08, 3.57, 1.97, 1.83, 1.07, 0.68, 0.54, 0.47, 0.63, 3.19, 3.52, 
                                       3.49, 3.47, 3.36, 2.76, 3.42, 3.17, 3.54, 2.56, 1.06, 1.09, 0.84, 
                                       0.64, 0.61, 0.74, 0.49, 3.49, 3.56, 3.46, 3.25, 3.72, 3.57, 3.58, 
                                       2.62, 1.99, 1.85, 1.04, 1.06, 0.62, 0.49, 0.48, 0.68, 0.5, 3.63, 
                                       3.71, 3.75, 3.67, 3.78, 3.52, 3.04, 2.26, 1, 1.17, 1.01, 0.92, 
                                       0.65, 0.54, 0.36, 0.38, 0.3, 3.08, 3.79, 3.9, 3.5, 3.4, 2.57, 
                                       3.03, 1.93, 2.02, 1.5, 0.67, 0.63, 0.72, 0.6, 0.67, 0.63, 0.53
), y = c(-105.28, -104.1, -104.81, -104.34, -104.37, -105.31, 
         -103.59, -103.32, -102.66, -103.57, -103.73, -104.47, -97.69, 
         -92.56, -95.9, -95.72, -107.6, -104.39, -105.12, -104.18, -104.46, 
         -102.19, -103.59, -103.38, -103.48, -102.84, -96.52, -88.54, 
         -90.36, -93.7, -85.21, -89.68, -99.47, -91.92, -104.58, -103.91, 
         -104.47, -104.49, -104.41, -104.41, -102.6, -98.65, -87.98, -89.23, 
         -86.34, -94.21, -91.57, -84.62, -84.14, -95.33, -102.14, -104.18, 
         -103.8, -102.47, -101.75, -101.73, -102.84, -97.49, -92.67, -91.72, 
         -80.45, -80.97, -84.94, -80.2, -81.05, -77.84, -82.72, -91.75, 
         -105.19, -104.66, -104.36, -104.31, -103.57, -102.68, -98.4, 
         -89.48, -85.92, -84.59, -84.49, -81.13, -83.28, -83.12, -85.62, 
         -85.89, -90.07)), .Names = c(""x"", ""y""), row.names = c(NA, -85L
         ), class = ""data.frame"")


lm.1 &lt;- coefficients(lm(example.data.1$y~example.data.1$x))
lm.2 &lt;- coefficients(lm(example.data.2$y~example.data.2$x))

library(ggplot2)
p &lt;- ggplot(example.data.1, aes(x=x, y=y))
p &lt;- p + geom_point()
p &lt;- p + geom_abline(intercept=(-100), slope=2, color=""blue"")
p &lt;- p + geom_abline(intercept=lm.1[1], slope=lm.1[2], color=""red"", linetype=""dashed"")
p &lt;- p + xlim(0, 10)
p &lt;- p + ylim(-110, -50)
p 

p &lt;- ggplot(example.data.2, aes(x=x, y=y))
p &lt;- p + geom_point()
p &lt;- p + geom_abline(intercept=(-100), slope=2, color=""blue"")
p &lt;- p + geom_abline(intercept=lm.2[1], slope=lm.2[2], color=""red"", linetype=""dashed"")
p &lt;- p + xlim(0, 10)
p &lt;- p + ylim(-110, -50)
p
</code></pre>

<p>I need to do this for tens of thousands of data-groups like each example above, so a fast algorithm is important. I have tried to use Stan, Jags, and arm-package, but don't understand how to tell those functions what I want.</p>

<p>My limited knowledge about statistics lead me to think that a bayesian approach is best, but I could be wrong.</p>
"
"0.139637413476259","0.120393103280186","176203","<p>I'm running a basic <a href=""https://stat.ethz.ch/R-manual/R-patched/library/stats/html/wilcox.test.html"" rel=""nofollow"">Mann-Whithney U test in R</a> between two groups; each group represents the abundance values of a particular bacteria within an animal.  Therefore a 0 means that bacteria is not present within the animal:</p>

<pre><code>a &lt;- c(0,0,12,0,0,76,0,0,81,0,0,0,0,0)
b &lt;- c(427,928,0,127,0,0,189,0,0,0,0,0,312,583,0)
wilcox.test(a,b,exact=FALSE)
</code></pre>

<p>The returned p-value is 0.1362, however I would have expected it to be &lt;0.05 since the two groups are quite different (at least IMHO).  I take it that the abundance of zeros is causing this.</p>

<p><a href=""http://i.stack.imgur.com/PnyTk.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/PnyTk.png"" alt=""enter image description here""></a></p>

<p>Is there another test to use in order to check whether these two group are different?  It looks like a zero-inflated negative binomial regression was suggested <a href=""http://stats.stackexchange.com/a/85625/82427"">here</a>, however I'm not familiar with that test or whether it applies to my data set.</p>

<p>Can I simply omit the zeros? <code>wilcox.test(a[a!=0],b[b!=0],exact=FALSE)</code> yields a p-value of 0.02, but I'm not sure if this is a good approach.</p>

<p><strong>UPDATE</strong></p>

<p>Given tristan's update, I've looked into zero-inflated count data regression.  I'm not sure if I'm running things properly (since I'm new to the models) but I'll post code and results:</p>

<pre><code>library(pscl)
library(lmtest)
df&lt;-data.frame('Abundance'=append(a,b))
df$Group &lt;- 'groupA'
df[(length(a) + 1):(length(a) + length(b)),2] &lt;- 'groupB'
summary(m1 &lt;- zeroinfl(Abundance ~ Group | Group, data = df))
summary(mnull &lt;- update(m1, . ~ 1))
lrtest(m1, mnull)
</code></pre>

<p>Returns:</p>

<pre><code>Call:
zeroinfl(formula = Abundance ~ Group | Group, data = df)

Pearson residuals:
      Min        1Q    Median        3Q       Max 
-0.864258 -0.864258 -0.728962 -0.002854  4.105212 

Count model coefficients (poisson with log link):
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  3.53223    0.07647   46.19   &lt;2e-16 ***
GroupgroupB  2.52612    0.07898   31.98   &lt;2e-16 ***

Zero-inflation model coefficients (binomial with logit link):
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   0.5878     0.5578   1.054    0.292
GroupgroupB  -0.3001     0.7764  -0.387    0.699
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Number of iterations in BFGS optimization: 10 
Log-likelihood:  -655 on 4 Df
&gt; summary(mnull &lt;- update(m1, . ~ 1))

Call:
zeroinfl(formula = Abundance ~ 1, data = df)

Pearson residuals:
    Min      1Q  Median      3Q     Max 
-0.8018 -0.8018 -0.8018 -0.1681  6.8098 

Count model coefficients (poisson with log link):
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  5.51672    0.01911   288.6   &lt;2e-16 ***

Zero-inflation model coefficients (binomial with logit link):
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   0.4353     0.3870   1.125    0.261
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Number of iterations in BFGS optimization: 8 
Log-likelihood: -1705 on 2 Df
&gt; lrtest(m1, mnull)
Likelihood ratio test

Model 1: Abundance ~ Group | Group
Model 2: Abundance ~ 1
  #Df   LogLik Df  Chisq Pr(&gt;Chisq)    
1   4  -654.97                         
2   2 -1705.50 -2 2101.1  &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.0806196982594614","0.076825726438694","176351","<p>I know there is an analytic solution to the following problem (OLS). Since I try to learn and understand the principles and basics of MLE, I implemented the fisher scoring algorithm for a simple linear regression model. </p>

<p>$$
y = X\beta + \epsilon\\
\epsilon\sim N(0,\sigma^2)
$$</p>

<p>The loglikelihood for $\sigma^2$ and $\beta$ is given by:
$$
-\frac{N}{2}ln(2\pi)-\frac{N}{2}ln(\sigma^2)-\frac{1}{2\sigma^{2}}(y-X\beta)^{'}(y-X\beta)
$$ </p>

<p>To compute the score function $S(\theta)$, where $\theta$ is the vector of parameters $(\beta,\sigma^{2})^{'}$, I take the first partial derivatives with respect to $\beta$ and $\sigma^{2}$:
$$
\frac{\partial L}{\partial \beta} = \frac{1}{\sigma^{2}}(y-X\beta)^{'}X
\\
\frac{\partial L}{\partial \sigma^2} = -\frac{N}{\sigma^{2}}+\frac{1}{2\sigma^{4}}(y-X\beta)^{'}(y-X\beta)
$$</p>

<p>Then the Fisher scoring algorithm is implemented as:
$$
\theta_{j+1} = \theta_{j} - (S(\theta_{j})S(\theta_{j})^{'})S(\theta_{j})
$$</p>

<p>Please note, the following code is a very naive implementation (no stopping rule, etc.)</p>

<pre class=""lang-R prettyprint-override""><code>library(MASS)
x &lt;- matrix(rnorm(1000), ncol = 2)
y &lt;- 2 + x %*% c(1,3) + rnorm(500)

fisher.scoring &lt;- function(y, x, start = runif(ncol(x)+1)){
    n &lt;- nrow(x)
    p &lt;- ncol(x)
    theta &lt;- start
    score &lt;- rep(0, p+1)
    for (i in 1:1e5){
        # betas
        score[1:p] &lt;- (1/theta[p+1]) * t((y - x%*%theta[1:p])) %*% x
        # sigma
        score[p+1] &lt;- -(n/theta[p+1]) + (1/2*theta[p+1]^2) * crossprod(y - x %*% theta[1:p])
        # new
        theta &lt;- theta - MASS::ginv(tcrossprod(score)) %*% score
    }
    return(theta)
}

# Gives the correct result 
lm.fit(cbind(1,x), y)$coefficients
# Does not give the correct result
fisher.scoring(y, cbind(1,x))
# Even if you start with the correct values
fisher.scoring(y, cbind(1,x), start=c(2,1,3,1))
</code></pre>

<p><strong>My Question</strong></p>

<p><em>What did I miss? Where is my mistake?</em></p>
"
"0.0430930413588572","0.0410650781176591","176768","<p>I am fitting a simple regression model on my data and I am using <code>nls()</code>
the only problem is it does not provide the coefficient of determination $R^2$ and adjusted $R^2$.</p>

<p>My question is how can I calculate both of $R^2$ and adjusted $R^2$.</p>

<p>The data and codes are </p>

<pre><code># generate data

beta &lt;- 0.012

n &lt;- 300

Data&lt;- data.frame(y = exp(beta * seq(n)) + rnorm(n), x = seq(n))

# plot data

plot(Data$x, Data$y)

# fit non-linear model

mod &lt;- nls(y ~ exp(a + b * x), data = Data, start = list(a = 0, b = 0))

# add fitted curve

lines(Data$x, predict(mod, list(x = Data$x)))
</code></pre>
"
"0.068136080998913","0.0649295895722714","176918","<p>In modeling claim count data in an insurance environment, I began with Poisson but then noticed overdispersion. A Quasi-Poisson better modeled the greater mean-variance relationship than the basic Poisson, but I noticed that the coefficients were identical in both Poisson and Quasi-Poisson models. </p>

<p>If this isn't an error, why is this happening? What is the benefit of using Quasi-Poisson over Poisson?</p>

<p><strong>Things to note:</strong></p>

<ul>
<li>The underlying losses are on an excess basis, which (I believe) prevented the Tweedie from working - but it was the first distribution I tried. I also examined NB, ZIP, ZINB, and Hurdle models, but still found the Quasi-Poisson provided the best fit. </li>
<li>I tested for overdispersion via dispersiontest in the AER
package. My dispersion parameter was approximately 8.4, with p-value
at the 10^-16 magnitude.  </li>
<li>I am using glm() with family = poisson or quasipoisson and a log link
for code. </li>
<li>When running the Poisson code, I come out
with warnings of ""In dpois(y, mu, log = TRUE) : non-integer x = ..."".</li>
</ul>

<p><strong>Helpful SE Threads per Ben's guidance:</strong></p>

<ol>
<li><a href=""http://stats.stackexchange.com/questions/11182/when-to-use-an-offset-in-a-poisson-regression"">Basic Math of Offsets in Poisson regression</a></li>
<li><a href=""http://stats.stackexchange.com/questions/167964/poisson-glm-with-non-count-data-rate-data?lq=1"">Impact of Offsets on Coefficients</a></li>
<li><a href=""http://stats.stackexchange.com/questions/175349/in-a-poisson-model-what-is-the-difference-between-using-time-as-a-covariate-or?"">Difference between using Exposure as Covariate vs Offset</a></li>
</ol>
"
"0.0430930413588572","0.0410650781176591","177522","<p><a href=""http://i.stack.imgur.com/LKZnw.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/LKZnw.png"" alt=""enter image description here""></a></p>

<p>Here is my multiple regression model diagnostic plot.  my r2 is 89 adj r squared is 88 but my residuals vs fitted and scale-Location doesn't look good. how do i improve this model or fixing the residuals issue</p>

<pre><code>Residuals:
   Min     1Q Median     3Q    Max 
-29454  -2250   1234   2622  24778 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 34789.903   7678.279   4.531 2.58e-05 ***
age             48.076      2.213  21.725  &lt; 2e-16 ***
income        -140.777     23.370  -6.024 8.81e-08 ***
language       55.671     38.910   1.431    0.157    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 7759 on 65 degrees of freedom
Multiple R-squared:   0.89, Adjusted R-squared:  0.8849 
F-statistic: 175.3 on 3 and 65 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>Also what is the meaning for -ve slope for income coefficient?</p>
"
"0.0746393370862076","0.0711268017165705","177556","<p>I recently read an article <a href=""http://Simultaneous%20Confidence%20Intervals%20Based%20on%20the%20Percentile%20Bootstrap%20Approach"" rel=""nofollow"">Simultaneous Confidence Intervals Based on the Percentile Bootstrap Approach</a> in which the authors present the following algorithm to compute a simultaneous confidence interval.  I was interested in implementing the first algorithm in R.  The algorithm is shown here:</p>

<p><a href=""http://i.stack.imgur.com/0l19J.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/0l19J.png"" alt=""enter image description here""></a></p>

<p>Would this awesome community mind reviewing my R code below to (1) verify that I've implemented the algorithm correctly and optionally (2) help me gain an intuitive understanding of why this algorithm works?  I read the article, but the notation, and explanation is a bit confusing.  Feel free to answer only one or both of my questions.  Here is my R Code, which uses a toy example with the built-in R datasets ""trees.""  I tried to comment my code and name variables in a way similar to the published algorithm. </p>

<pre><code>library(MASS)
alpha&lt;-.05
B=100000
#Use toy example from R's trees dataset
myfit&lt;-lm(Height~Girth+Volume, data=trees)
myfit
param&lt;-coef(myfit)
coefficients&lt;-length(param)
mycov&lt;-vcov(myfit)
#Simulate 10K bootstrap samples using the estimated mean vector and covariance matrix
#from the regression model
#Step 1.
theata.tilde.b&lt;-mvrnorm(B, mu=param, Sigma=mycov)
head(theata.tilde.b)

#Step 2
rbj&lt;-apply(theata.tilde.b, 2, rank)

#Step 3.
sample.b.rank.upper&lt;-apply(rbj, 1, max)
sample.b.rank.lower&lt;-apply(rbj, 1, min)

boot.df&lt;-data.frame(cbind(theata.tilde.b, rbj, sample.b.rank.upper, sample.b.rank.lower))
names(boot.df)&lt;-c(""B0"", ""B1"", ""B2"", ""Rank0"", ""Rank1"", ""Rank2"", ""SampleBRankUpper"", ""SampleBRankLower"")

upper.index&lt;-B*(1-alpha/2)
lower.index&lt;-B*(alpha/2)
#Sort datasets by max and then min rank of the sample-b rank
upper&lt;-(boot.df[order(boot.df$SampleBRankUpper),])
    lower&lt;-(boot.df[order(boot.df$SampleBRankLower),])

#Step 5
upper[upper.index,1:coefficients]
lower[lower.index,1:coefficients]
</code></pre>

<p>Thanks!</p>
"
"0.0609427635336005","0.0580747904139041","177650","<p>I have a binary response variable and a categorical predictor variable. If I test for associations between the 2 variables using chi-square test , it turns out to be significant. However, if I do a logistic regression with the same set of variables, the predictor is not significant. Why does this happen?</p>

<pre><code>  table(Data1$pred,Data1$target)

                            0    1
  Level1                    1    0
  Level2                    4    0
  Level3                   98    1
  Level4                 2056   22
  Level5                    1    0
  Level6                    2    0
  Level7                  311    0
  Level8                    6    1
  Level9                  131    7
  Level10                  49    2

  chisq.test(table(Data1$pred,Data1$target))

  Pearson's Chi-squared test

  data:  tabletable(Data1$pred,Data1$target)
  X-squared = 34.2614, df = 9, p-value = 8.037e-05
</code></pre>

<p>Logistic Regression on the same</p>

<pre><code>  logit.glm &lt;- glm(as.factor(target) ~ pred,                  
               data=Data1, family=binomial(link=""logit"")
  summary(logit.glm)
  Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.5553  -0.1459  -0.1459  -0.1459   3.0315  

  Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)
  (Intercept)   -2.057e+01  1.773e+04  -0.001    0.999
  Data1Level2   -6.313e-06  1.982e+04   0.000    1.000
  Data1Level3    1.598e+01  1.773e+04   0.001    0.999
  Data1Level4    1.603e+01  1.773e+04   0.001    0.999
  Data1Level5   -6.312e-06  2.507e+04   0.000    1.000
  Data1Level6   -6.312e-06  2.172e+04   0.000    1.000
  Data1Level7   -6.312e-06  1.776e+04   0.000    1.000
  Data1Level8    1.877e+01  1.773e+04   0.001    0.999
  Data1Level9    1.764e+01  1.773e+04   0.001    0.999
  Data1Level10   1.737e+01  1.773e+04   0.001    0.999

  (Dispersion parameter for binomial family taken to be 1)

   Null deviance: 356.09  on 2691  degrees of freedom
   Residual deviance: 333.06  on 2682  degrees of freedom
   AIC: 353.06

   Number of Fisher Scoring iterations: 19
</code></pre>
"
"0.101062140164153","0.0963061447907242","177654","<p>When running an ordered logistic regression using the <code>polr</code> function of the <code>MASS</code>package (DV is low, medium, high) and have a look at the summary I get Î²s for every IV and the intercepts for low|medium and medium|high.</p>

<p>The <code>predict</code>function for assessing the probabilities (<code>type='p'</code>) or the classes (<code>type='class'</code>) also works just fine.</p>

<p>However I want to calculate the probabilities myself in order to use them with different data sets.</p>

<p>If I use the following code for a <em>logistic model with a binary (!) dependent variable</em>, I can exactly replicate the <code>predict</code> - outcome:</p>

<p><code>log_pred &lt;- (logit_model$coefficients[1] + logit_model$coefficients[2]*IV_1 + logit_model$coefficients[3]*IV_2)</code></p>

<ul>
<li><code>logit_model</code> is my <code>glm</code>-object</li>
<li><code>logit_model$zeta[1]</code> is the first intercept</li>
<li><code>logit_model$zeta[2]</code> is the second intercept</li>
<li><code>logit_model$coefficients[1]</code> is the Î² of IV_1</li>
<li><code>logit_model$coefficients[2]</code> is the Î² of IV_2</li>
</ul>

<p>the only thing I have to do now, to get the predicted probabilities is:</p>

<p><code>log_pred_probs &lt;- exp(log_pred)/(1+exp(log_pred))</code></p>

<p>If I understand all the posts on ordered logistic regression I read correctly, the only thing I have to change with a <code>polr</code> object with the 3 ""groups"" of low, medium, and high would be to:</p>

<ul>
<li>run the <code>log-pred</code>part for each group using their own intercepts, let's call them <code>log_pred1</code> and <code>log_pred2</code></li>
<li>and to, then, run the following code (similar to the logistic model above):
<code>log_pred_probs1 &lt;- exp(log_pred1)/(1+exp(log_pred1)+exp(log_pred2))</code> for ""low""
<code>log_pred_probs2 &lt;- exp(log_pred2)/(1+exp(log_pred1)+exp(log_pred2))</code> for ""medium""
<code>log_pred_probs3 &lt;- 1/(1+exp(log_pred1)+exp(log_pred2))</code> for ""high""</li>
</ul>

<p>I think there are at least two problems ('cause this doesn't work at all):</p>

<ol>
<li>I need the Î²-coefficients for every level of the dependent variable, and <code>summary(polr-object)</code>does only show the Î²s for the first group (so does <code>$coefficients</code>)</li>
<li>and I am not sure about the computation of the predicted probabilities for group 3, ""high"".</li>
</ol>

<p>So these are the questions in short: <strong>How do I assess the Î²-coefficients for every level of the DV in a <code>polr</code>object?</strong></p>

<p>And</p>

<p><strong>How do I compute the predicted probabilities for every level of the DV myself?</strong></p>
"
"0.140168356127281","0.14518697603476","177823","<p><strong>I need to perform manually two-stage Least Squares(to illustrate its advantages)</strong>, where the first stage is <em>repeated median estimate</em> and the second stage should be weighted least squares, where weights are obtained(as far, as I understand) from polynomial regression of first-stage residuals on regressors.</p>

<p>Suppose I have generated the following heteroscedastic model:</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=Y_i%20%3D%20b_0%2Bb_1%20X_i%20%2B%20%5Cepsilon_i"" alt=""Y_i = b_0+b_1 X_i + \epsilon_i""></p>

<p>where error depends on regressor:</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=%5Cepsilon_i%20%5Csim%20N[0%2C[X_i-1]%5E2]"" alt=""\epsilon_i \sim N(0,(X_i-1)^2)""></p>

<pre><code>set.seed(100)
b&lt;-c(12,7.25) ## my coefficients
num&lt;-50 ## number of observations

raw_x&lt;-runif(num,min=0,max=2) ## regressors

my_y&lt;-as.vector(b%*%t(data.frame(rep(1,num),raw_x))+
     rnorm(num,mean=0,sd=(raw_x-1)^2)) ## observations

l&lt;-lm(my_y~raw_x) ## let's create linear model

plot(fitted(l),residuals(l)) ## we see heteroskedasticity
## we got to higher values, our residuals explode

abline(0,0)
title(""Residual vs Fit. value"");
</code></pre>

<p><a href=""http://i.stack.imgur.com/4S6pY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4S6pY.png"" alt=""Residual vs fit value""></a></p>

<p>So I perform repeated median regression (formula in the <a href=""http://www.cs.huji.ac.il/~werman/Papers/rep.pdf"" rel=""nofollow"">Introduction</a>):</p>

<pre><code>## Generating first model using repeated median

## slope 

fij = function(i,j)
{
  (my_y[i]-my_y[j])/(raw_x[i]-raw_x[j])
}

bij&lt;-outer(1:num,1:num,fij) ##NaN's were produced on the diagonal    

rowmeds &lt;- apply(bij, 1, median,na.rm=TRUE)
b_med&lt;-median(rowmeds)

# colmeds &lt;- apply(bij, 2, median,na.rm=TRUE) ## column medians are the same
# b_med3&lt;-median(colmeds)

## Intercept    

## med(y_i - b*x_i)

a_med&lt;-median(my_y-b_med*raw_x)
</code></pre>

<p><strong>The fit is extremely accurate!</strong> In this example <code>a_med</code> is 11.97634 and <code>b_med</code> equals 7.27022.</p>

<p>Now I perform 2nd order polynomial regression of residuals:</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=%5Chat%7B%5Cepsilon_i%7D%3Da_0%20%2B%20a_1X_i%2Ba_2X_i%5E2%2B%5Cdelta_i"" alt=""\hat{\epsilon_i}=a_0 + a_1X_i+a_2X_i^2+\delta_i""></p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=%5Chat%7B%5Cepsilon%7D%20%3D%20%5Cbegin%7Bpmatrix%7D%201%20%26%20X_1%20%26%20X_1%5E2%20%5C%5C%20%5Cvdots%20%5C%5C%201%20%26%20X_m%20%26%20X_m%5E2%20%5Cend%7Bpmatrix%7D%5Cbegin%7Bpmatrix%7Da_0%20%5C%5C%20a_1%20%5C%5C%20a_2%20%5Cend%7Bpmatrix%7D%2B%5Cdelta"" alt=""\hat{\epsilon} = \begin{pmatrix} 1 &amp; X_1 &amp; X_1^2 \\ \vdots \\ 1 &amp; X_m &amp; X_m^2 \end{pmatrix}\begin{pmatrix}a_0 \\ a_1 \\ a_2 \end{pmatrix}+\delta""></p>

<p>so that (<strong>X</strong> here is m x 3 matrix):</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=%5Chat%7Ba%7D%20%3D%20%5BX%5ETX%5D%5E%7B-1%7DX%5ET%5Chat%7B%5Cepsilon%7D"" alt=""\hat{a} = [X^TX]^{-1}X^T\hat{\epsilon}""></p>

<p>I was told that as long as residual variances can be roughly estimated from only one observation, actual fit from this model can be used; residual variances = coefficients for the weighted least squares:</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=%5Chat%7B%5Chat%7B%5Csigma%7D%7D%3DX%5Chat%7Ba%7D"" alt=""\hat{\hat{\sigma}}=X\hat{a}""></p>

<pre><code>## Obtaining 2nd order polynomial estimator for residuals

Xmatr = t(rbind(rep(1,num),raw_x,(raw_x)^2))

## coef_var = (X^T*X)^(-1)*X^T^e
coef_var&lt;-solve(t(Xmatr)%*%Xmatr)%*%t(Xmatr)%*%t(my_y-c(a_med,b_med)%*%rbind(rep(1,num),raw_x))

## Obtaining sigma (residual deviation) esitmate

my_sigma&lt;-t(Xmatr%*%coef_var)

## performing regression with sigma-weights

b_wls&lt;-lm(as.numeric(my_y)~raw_x,weights=as.numeric(my_sigma^2))$coef

## final plot

library(scales)
plot(raw_x,my_y, pch=20,col=alpha(""salmon"",0.6))

abline(b[1],b[2], col=""black"") ## real line
abline(a_med,b_med,col=""blue"") ## repeated median fit
abline(b_wls, col=""magenta"")
legend('bottomright', c(""Real"",""Repeat median"",""Two-Level LS"") , 
       lty=1, col=c('black', 'blue','magenta'), bty='n', cex=.75)
</code></pre>

<p>The resulting fit is always worse (and sometimes turns into complete garbage). <strong>Please, can you explain me what I'm doing wrong? I need to obtain the result where two-level LS is better than repeated median fit(provided the error depends on regressors as shown before)</strong>.</p>

<p><a href=""http://i.stack.imgur.com/Wo0ZM.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Wo0ZM.png"" alt=""enter image description here""></a>
<a href=""http://i.stack.imgur.com/huTRw.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/huTRw.png"" alt=""enter image description here""></a></p>

<p>EDIT: Using R function lm seems to produce the same picture:</p>

<pre><code>Xmatr = t(rbind(rep(1,num),raw_x,(raw_x)^2))
residual&lt;-as.vector(my_y-c(a_med,b_med)%*%rbind(rep(1,num),raw_x))
polycoef&lt;-lm(residual ~ poly(raw_x, 2, raw=TRUE))$coefficients
my_sigma&lt;-t(Xmatr%*%polycoef)
</code></pre>

<p>EDIT2: Checking the quality of the residual fit (as far as I understand what's going on):</p>

<pre><code>plot(raw_x,abs(residual))
lines(sort(raw_x),(sort(raw_x)-1)^2) ## real residuals
lines(sort(raw_x), my_sigma[order(raw_x)],col = ""magenta"") ## fitted residuals
legend('topleft', c(""Real res"",""Fitted res"") , 
       lty=1, col=c('black','magenta'), bty='n', cex=.75)
</code></pre>

<p><a href=""http://i.stack.imgur.com/3J5D3.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/3J5D3.png"" alt=""enter image description here""></a></p>

<p>EDIT3: As long as my standard deviation is $(X_i-1)^2$, so the variance has power 4 and maybe I should do for the residual fit</p>

<pre><code>residual&lt;-abs(as.vector(my_y-c(a_med,b_med)%*%rbind(rep(1,num),raw_x)))
</code></pre>

<p>this makes residual fit by the quadratic function better, but the regression line moves even more far than before.
<a href=""http://i.stack.imgur.com/nt1Ij.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/nt1Ij.png"" alt=""enter image description here""></a></p>
"
"0.052777981396926","0.0335294958785986","177869","<p>I am running mixed effects regression in R, utilizing <code>glmer</code>, and am hoping someone can help clarify the difference between using <code>coef</code> and <code>ranef</code> on the results. Specifically, I have fixed effects $f_1,f_2,f_3$ and random effects $r_1,r_2,r_3.$ When I run <code>coef</code> I get certain coefficent values for each of the fixed and random effects. Additionally when I use the <code>ranef</code> function I get coefficients for my random effects. These two coefficients are not equal for each respective random effect $r_1,r_2,r_3.$ Why are these coefficients different and what information each coefficient tells us?</p>
"
"0.0430930413588572","0.0410650781176591","177903","<p>I fitted an ordinal logistic regression but I'm unable to interpret the coefficients. Can anyone assist in this regard? Here is the output generated: </p>

<pre><code>Call:
polr(formula = factor(grade) ~ factor(Month) + Day, data = myData, 
    Hess = TRUE)

Coefficients:
                  Value Std. Error t value
factor(Month)4 1.405114    0.51547  2.7259
Day            0.007672    0.01944  0.3947

Intercepts:
    Value   Std. Error t value
1|2 -0.6785  0.7019    -0.9667
2|3  1.6767  0.7162     2.3412

Residual Deviance: 333.602 
AIC: 341.602 
</code></pre>

<p>The grade is factored: </p>

<p>1 = good<br>
2 = very good<br>
3 = excellent</p>

<p>Month is factored:</p>

<p>3 = March<br>
4 = April</p>

<p>The grade is the response while month and day are my explanatory variables.</p>
"
"0.0746393370862076","0.0711268017165705","177921","<p>I have this data plotted as a scatter plot in Excel:  </p>

<p><a href=""http://i.stack.imgur.com/G45wZ.jpg""><img src=""http://i.stack.imgur.com/G45wZ.jpg"" alt=""enter image description here""></a></p>

<p>I had done a regression in Excel, and the p value was 2.14E-05 while the R- value was 0.32. I was told the R value was too low compared to the significance of the p value, and was told to control for the dispersion of the data by running it through R with GLM with quasipoisson error.</p>

<p>This gave me</p>

<pre><code>glm(formula = encno ~ temp, family = quasipoisson(link = log), 
    data = encnotemp)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-6.008  -2.431  -1.021   1.353   9.441  

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 2.005807   0.174628  11.486  &lt; 2e-16 ***
temp        0.029065   0.006528   4.453 1.53e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for quasipoisson family taken to be 10.19898)

    Null deviance: 1807.4  on 171  degrees of freedom
Residual deviance: 1620.1  on 170  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 5
</code></pre>

<p>How do I analyse this output? </p>

<p>The problem is that the scatterplot data is too dispersed, and I would like to make a scatterplot from the quasipoisson GLM output that shows less dispersed (more fitted) data points. Will this be possible?</p>
"
"0.127074446292276","0.13925845528839","177960","<p>I'm trying to assess the effect of showing more impressions on a user. I want to study if users who saw more ads are more likely to make a purchase onsite. To do so I've created a multilevel model. I grouped users into 10 groups averaging their scores (we score users based on a number of factors). So basically I end up having 10 groups (from 0 to 9), where on group 9 I assume to have the best users, and on group 0 the worst.</p>

<pre><code>  picbucket mcuserid impressions mediacostcpm is_buyer gr.impressions gr.mediacostcpm
1         0 1           1        0.460        0       3.632794        2.767509
2         0 2           2        5.000        0       3.632794        2.767509
3         0 3           1        4.590        0       3.632794        2.767509
4         0 4           1        0.590        0       3.632794        2.767509
5         0 5           1        5.000        0       3.632794        2.767509
6         0 6           1        0.315        0       3.632794        2.767509
</code></pre>

<p>I think a multilevel model could be advantageous here because I'm expecting to see different effects on each group. On the best users I'm expecting an additional impression could have a higher impact, whereas on bad users and additional impression may be worthless. It could also be possible the opposite though. So that users which generally higher score will convert even without the need of serving them more impressions, whereas on mid groups additional impressions tend to change their behaviour. </p>

<p>A good model representation could be:</p>

<p>$y_{i} = \alpha_{j[i]} + X_{i}\beta + \epsilon_{i}$</p>

<p>The second level of the model will then be:</p>

<p>$\alpha_{j} = \mu_{\alpha} + \eta_{j}, \text{ with } \eta_{j} \sim N(0, \sigma_{\alpha}^{2})$</p>

<p>On the first level I want to include as a predictor how many impression a user saw. On the second level I want to include the average impressions a user saw within its group and the average media cost for the impressions we served on that user. I've used the package <code>lme4</code> in R to build my model.</p>

<pre><code>glmer(formula = is_buyer ~ impressions + mediacostcpm + (1 + 
    gr.impressions + gr.mediacostcpm | picbucket), data = new.df, 
    family = binomial())
             coef.est coef.se
(Intercept)  -7.42     0.33  
impressions   0.00     0.02  
mediacostcpm  0.03     0.01  

Error terms:
 Groups    Name            Std.Dev. Corr        
 picbucket (Intercept)     7.86                 
           gr.impressions  2.22     -0.99       
           gr.mediacostcpm 0.57     -0.68  0.60 
 Residual                  1.00                 
---
number of obs: 103146, groups: picbucket, 10
AIC = 2755.4, DIC = 2680.5
deviance = 2708.9 
</code></pre>

<p>This is my first experiment with multilevel modeling so I would like to make sure I don't misunderstand the results of my model. </p>

<p>From what I see here, the <code>impressions</code> predictor on the first level is useless. Its coefficient is zero and its standard deviation is very small. This could be due to the fact I'm including a group average on the second level for the impression count (<code>gr.impressions</code>). So, on any group (<code>picbucket</code>), serving more impressions than the average doesn't tell us much about the likelihood of a cookie to convert.</p>

<p>The average media cost on the first level is however an interesting one. Generally, within a group, if I spend a bit more for every impression I should increase the probability of generating conversions. This is probably due to inventory quality. Better inventory costs more, but also has better changes to be viewable inventory.</p>

<p>The coefficients on the group level instead tell you how much they contribute on explaining the group slope. So in this case the average number of impressions at the group level seems to explain quite a significant part of the group slope. </p>

<p>Interestingly, at the group level <code>gr.impressions</code> seem to be a very useful predictor, but at the within group level its usefulness is limited. The opposite applies to the <code>mediacostcpm</code>.</p>

<p>Am I interpreting these results correctly? How can I tell if the model has a good fit? Please note I've used a binomial regression because the dependent variable, <code>is_buyer</code> can take only 0 or 1 (one being the user made a purchase).</p>
"
"0.0746393370862076","0.0711268017165705","177987","<p>Good afternoon,
I need help in intpretting the significance results when performing a linear regression with a categorical interaction.  I'm running this analysis in R.</p>

<p>The question is whether I can use qsec as an explantory variable in the below model given that one of the categorical variables is not significantly different to the other.</p>

<p>Model &amp; Outputs</p>

<pre><code>lm(mpg~qsec+am+qsec*am, data=mtcars)

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)  -9.0099     8.2179  -1.096  0.28226   
qsec          1.4385     0.4500   3.197  0.00343 **
am1         -14.5107    12.4812  -1.163  0.25481   
qsec:am1      1.3214     0.7017   1.883  0.07012 . 
</code></pre>

<p>The second coefficient is not statistical significant - though only just with a p value of 0.07.
Does this mean that I should look for another explanatory variable?</p>

<p>When I run the regression on one of the values - am1 - alone, then it is still significantly different to zero.  As is am0 from the above results.</p>

<p>When I look at the data, the slope appears significantly different so I think I can use it however the understanding of the theory says no - which is frustrating.  I would add my plot to highlight my point but I'm struggling to load it.</p>

<p>I hope this isn't a stupid question to ask...I've found some good explanations of categorical variables in general, however nothing that tells me how I should interpret this secondary variable.</p>

<p>James</p>

<p>** edit changed data to be qsec per question below</p>
"
"0.068136080998913","0.0649295895722714","178102","<p>I have this data plotted as a scatter plot in Excel</p>

<p><img src=""http://i.stack.imgur.com/8xm8M.jpg"" alt=""enter image description here""></p>

<p>I had done a regression in Excel, and the p value was 2.14E-05 while the 
R- value was 0.32. I was told the R value was too low compared to the significance of the p value, and was told to control for the dispersion of the data by running it through R with GLM with quasipoisson error. </p>

<p>This gave me </p>

<pre><code>glm(formula = encno ~ temp, family = quasipoisson(link = log), 
    data = encnotemp)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-6.008  -2.431  -1.021   1.353   9.441  

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 2.005807   0.174628  11.486  &lt; 2e-16 ***
temp        0.029065   0.006528   4.453 1.53e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for quasipoisson family taken to be 10.19898)

    Null deviance: 1807.4  on 171  degrees of freedom
Residual deviance: 1620.1  on 170  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 5
</code></pre>

<p>How do I analyse this output? And is it possible for me to make a scatter plot from this GLM output, that increases the fit of the data points and help sort out the outliers?</p>
"
"0.0545088647991304","0.0649295895722714","178452","<p>I have an irregularly lagged and unbalanced time series. I want to run a regression fixed effect regression on my data but I'm not totally sure of the implication of the irregular intervals (the unbalancedness is taken into account by <code>plm()</code>). The output of the regression seems not to consider the issue.</p>

<p>Please consider the following</p>

<pre><code># Sample data
require(WDI)
df &lt;- WDI(country = c(""BR"",""US"", ""CA""), start = 2000, end = 2010,
          indicator = c('EN.ATM.CO2E.PC', 'NY.GDP.PCAP.CD'))

# Remove random years
df &lt;- df[-which(df$year==2005),]
    df &lt;- df[-which(df$year==2003),]
df &lt;- df[-which(df$year==2006 &amp; df$country == ""Brazil""),]

require(plm)
summary(plm(EN.ATM.CO2E.PC ~ NY.GDP.PCAP.CD, data = df, index = c('country', 'year'),
            method = ""within"", balanced = FALSE))
</code></pre>

<p>which outputs</p>

<pre><code>Oneway (individual) effect Within Model

Call:
plm(formula = EN.ATM.CO2E.PC ~ NY.GDP.PCAP.CD, data = df, index = c(""country"", 
    ""year""), method = ""within"", balanced = FALSE)

Unbalanced Panel: n=3, T=8-9, N=26

Residuals :
   Min. 1st Qu.  Median 3rd Qu.    Max. 
 -1.540  -0.289   0.110   0.543   1.170 

Coefficients :
                  Estimate  Std. Error t-value Pr(&gt;|t|)   
NY.GDP.PCAP.CD -6.8273e-05  2.1762e-05 -3.1373 0.004788 **
---
Signif. codes:  
0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Total Sum of Squares:    16.404
Residual Sum of Squares: 11.333
R-Squared      :  0.3091 
      Adj. R-Squared :  0.26155 
F-statistic: 9.84253 on 1 and 22 DF, p-value: 0.0047884
</code></pre>
"
"0.0609427635336005","0.0580747904139041","178492","<p>One can perform a logit regression in R using such code:</p>

<pre><code>&gt; library(MASS)
&gt; data(menarche)
&gt; glm.out = glm(cbind(Menarche, Total-Menarche) ~ Age,
+                                              family=binomial(logit), data=menarche)
&gt; coefficients(glm.out)
(Intercept)         Age 
 -21.226395    1.631968
</code></pre>

<p>It looks like the optimization algorithm has converged - there is information about steps number of the fisher scoring algorithm:</p>

<pre><code>Call:
glm(formula = cbind(Menarche, Total - Menarche) ~ Age, family = binomial(logit), 
    data = menarche)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.0363  -0.9953  -0.4900   0.7780   1.3675  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -21.22639    0.77068  -27.54   &lt;2e-16 ***
Age           1.63197    0.05895   27.68   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 3693.884  on 24  degrees of freedom
Residual deviance:   26.703  on 23  degrees of freedom
AIC: 114.76

Number of Fisher Scoring iterations: 4
</code></pre>

<p>I am curious about what optim algorithm it is? Is it Newton-Raphson algorithm (second order gradient descent)? Can I set some parameters to use Cauchy algorithm (first order gradient descent)?</p>
"
"0.0545088647991304","0.0649295895722714","179002","<p>If I have a fixed effects regression model over time and across 20 different  individuals (actual human beings) as follows:</p>

<pre><code>Model Formula: freeradicals ~ metabolicox + exercise

Coefficients:
 metabolicox          exercise
 0.25                 1.011
</code></pre>

<p>My interpretation of this therfore is that for every 1 unit rise in the <code>metabolicox</code> variable I get a 0.25 unit rise in the <code>freeradicals</code> while holding <code>excercise</code> constant. Similarly, for every 1 unit rise in the <code>exercise</code> variable I get a ~1 unit rise in the <code>freeradicals</code> while holding <code>metabolicox</code> constant.</p>

<p>My question therefore is: how can I assess whether the dose-response relationship between these unit rises? Do I have to do several regressions while multiplying the independent variable of interest by some constant. For example, I could change the formula to:</p>

<pre><code>Model Formula: freeradicals ~ 2*metabolicox + exercise
</code></pre>

<p>which would assess how a doubling in <code>metabolicox</code> works, and then do that for different constants from 2 up to 10 in increments of 1 for example.</p>
"
"0.12583105938145","0.119909435959664","179049","<p>I'm trying to learn some basic Machine Learning and some basic R. I have made a very naive implementation of $L_2$ regularization in R based on the formula:</p>

<p>$\hat w^{ridge} = (X^TX +\lambda I)^{-1} X^T y$ </p>

<p>My code looks like this:</p>

<pre><code>fitRidge &lt;- function(X, y, lambda) {
     # Add intercept column to X:
  X &lt;- cbind(1, X)
     # Calculate penalty matrix:
  lambda.diag &lt;- lambda * diag(dim(X)[2])
     # Apply formula for Ridge Regression:
  return(solve(t(X) %*% X + lambda.diag) %*% t(X) %*% y)
}
</code></pre>

<p>Note that I'm not yet trying to find an optimal $\lambda$, I'm simply estimating $\hat w^{ridge}$ for a given $\lambda$. However, something seems off. When I enter $\lambda = 0$ I get the expected OLS result. I checked this by applying lm.ridge(lambda = 0) on the same dataset and it gives me the same coefficients. However, when I input any other penalty, like $\lambda=2$ or $\lambda=5$ my coefficients and the coefficients given by lm.ridge disagree wildly. I tried looking at the implementation of lm.ridge but I couldn't work out what it does (and therefore what it does differently).</p>

<p>Could anyone explain why there is a difference between my results and the results from lm.ridge? Am I doing something wrong in my code? I've tried playing around with <code>scale()</code> but couldn't find an answer there.</p>

<p>EDIT:</p>

<p>To see what happens, run the following:</p>

<pre><code>library(car)
X.prestige &lt;- as.matrix.data.frame(Prestige[,c(1,2,3,5)])
y.prestige &lt;- Prestige[,4]

fitRidge(X.prestige, y.prestige, 0)
coef(lm.ridge(formula = prestige~education+income+women+census, data = Prestige, lambda = 0))
fitRidge(X.prestige, y.prestige, 2)
coef(lm.ridge(formula = prestige~education+income+women+census, data = Prestige, lambda = 2))
</code></pre>

<p>EDIT2:</p>

<p>Okay, so based on responses below, I've gotten a somewhat clearer understanding of the problem. I've also closely re-read the section about RR in TESL by Hastie, Tibshirani and Friedman, where I discovered that the intercept is often estimated simply as the mean of the response. It seems that many sources on RR online are overly vague. I actually suspect many writers have never implemented RR themselves and might not have realized some important things as many of them leave out 3 important facts:</p>

<ol>
<li>Intercept is not penalized in the normal case, the formula above only applies to the other coefficients.</li>
<li>RR is not equivariant under scaling, i.e. different scales gives different results even for the same data.</li>
<li>Following from 1, how one actually estimates intercept.</li>
</ol>

<p>I tried altering my function accordingly:</p>

<pre><code>fitRidge &lt;- function(X, Y, lambda) {
  # Standardize X and Y
  X &lt;- scale(X)
  Y &lt;- scale(Y)
  # Generate penalty matrix
  penalties &lt;- lambda * diag(ncol(X))
  # Estimate intercept
  inter &lt;- mean(Y)
  # Solve ridge system
  coeff &lt;- solve(t(X) %*% X + penalties, t(X) %*% Y)
  # Create standardized weight vector
  wz &lt;- c(inter, coeff )
  return(wz)
}
</code></pre>

<p>I still don't get results equivalent to lm.ridge though, but it might just be a question of translating the formula back into the original scales. However, I can't seem to work out how to do this. I thought it would just entail multiplying by the standard deviation of the response and adding the mean, as usual for standard scores, but either my function is still wrong or rescaling is more complex than I realize.</p>

<p>Any advice?</p>
"
"0.17795199074502","0.178998525623479","179105","<p>I get a couple of puzzling results in my (repeated event) cox model when I introduce interaction effects. I will here pose several questions about interaction effects (in survival analysis context) in order to â€“ hopefullyâ€“ once for all to get the answers to these questions. I've checked similar posts, related to this matter (<a href=""http://stats.stackexchange.com/questions/147310/r-dichotomous-time-interaction-in-a-cox-model"">1</a>, <a href=""http://stats.stackexchange.com/questions/161849/interpretation-interaction-in-cox-regression"">2</a>, <a href=""http://stats.stackexchange.com/questions/175525/cox-ph-interaction-model-test-p-value-equivalence"">3</a>, <a href=""http://stats.stackexchange.com/questions/32225/cox-proportional-hazard-model-and-interpretation-of-coefficients-when-higher-cas"">4</a>, <a href=""http://stats.stackexchange.com/questions/137180/interpretation-of-interaction-between-covariates-and-time-in-cox-regression"">5</a>, <a href=""http://stats.stackexchange.com/questions/157275/cox-regression-testing-for-effect-in-subgroup"">6</a>, <a href=""http://stats.stackexchange.com/questions/46322/understanding-signficant-interaction-with-non-significant-main-effects"">7</a>, <a href=""http://stats.stackexchange.com/questions/163299/cox-time-series-data-analysis-of-interaction-terms"">8</a> ), and some of them are unanswered, while the others are answered ambiguously. Some of them are helpful. In general,  I belive there is a need (and interest in) for some clarification about interaction effects â€“ a quite complicated area for all quantitative methodsâ€“focused student/professionals. </p>

<p>Ultimately, my questions relate to the logic behind interactions and their subsequent interpretation in the analysis. Below I present 5 different scenarios/models derived from my data analysis â€“ but I extend them a bit to also include other examples that might be of help for me and (hopefully) for other people on this website.  </p>

<p>For every scenario, I provide my own interpretations (in order to capture the essence and logic, they're not comprehensive interpretations) â€“ so those of you who are able to answer, please reject or support them. If possible, provide a correct answer and elaborate why something was incorrect. </p>

<ul>
<li><strong>Scenario 1</strong></li>
</ul>

<p>Suppose that I have a model with 2 covariates where one of the covariates is my main explanatory variable (note that it makes sense to have this variable without an interaction term as well). Guided by my theoretical considerations, I (also) introduce an interaction term between them. </p>

<p>My main explanatory variable <strong>(X)</strong> is on the scale 0 to 10 (think of number of appearances) and the other covariate <strong>(D)</strong> is also a continuous variable (ranging from 0 to 10). The model with interaction term:  </p>

<pre><code>model.1&lt;â€“coxph(start, stop, event)~X+D+X:D+cluster(ID)+strata(enum), data=mydata)  

                    exp(coef) exp(-coef) lower .95 upper .95
X                    1.069     0.9356    0.9798     1.166
D                    1.046***  0.9561    1.0213     1.071
X*D                  1.000     0.9999    0.9876     1.013
</code></pre>

<p>Suppose now that in model with only X+D (with no interaction term), my main variable X was significant. It is not significant in the interaction model (see above result). </p>

<p><strong>My interpretation</strong> 1) I simply state that there were no interaction effects between X and D. However, while the  D variable is significant (with increasing hazard rate) the X is not. Thus, my main explanatory variable is not sufficient to explain this. Alternatively, 2) I state that there were no interaction effects, and the coef. of X in the interaction model does not make any sense or is hard to interpret. I don't even show this results, but put it on a note. </p>

<p><strong>Question:</strong> how should I interpret interaction effects between two continuous variables in this model?  </p>

<ul>
<li><strong>Scenario 2</strong></li>
</ul>

<p>In this scenario the X variable is still a continuous variable 0-10, but the D-variable is now dichotomous. </p>

<pre><code>                    exp(coef) exp(-coef) lower .95 upper .95
X                   1.0677.    0.9366    0.9933     1.148
D                   1.3628***  0.7338    1.1351     1.636
X*D                 0.9994     1.0006    0.9150     1.092
</code></pre>

<p><strong>My interpretation</strong>: ""X:D"" is decreasing, i.e. when D=0 and X increasing, the hazard for experiencing the event is decreasing(weak), but the effect is not significant. When ""D"" is = 1, the hazard is increasing. </p>

<ul>
<li><strong>Scenario 3</strong></li>
</ul>

<p>""X"" is till continuous, but the ""D"" is now categorical (0 = no appearances, 1 = one appearance, 2 = two appearances, 3 = three appearances).  </p>

<pre><code>                     exp(coef) exp(-coef) lower .95 upper .95
X                     1.0491***  0.9532    1.0226     1.076
factor(D)1            1.2237     0.8172    0.8350     1.793
factor(D)2            1.7871.    0.5596    0.9910     3.223
factor(D)3            1.0578     0.9453    0.4625     2.420
X*factor(D)1          0.9849     1.0153    0.9336     1.039
X*factor(D)2          0.9859     1.0143    0.9021     1.077
X*factor(D)3          1.0390     0.9625    0.9230     1.170   
</code></pre>

<p><strong>Question</strong>: How should I interpret the interaction term here? </p>

<ul>
<li><strong>Scenario 4</strong></li>
</ul>

<p>Now the ""X"" becomes dichotomous (1/0) and the ""D"" remains categorical as in Scenario 3. </p>

<pre><code>                   exp(coef) exp(-coef) lower .95 upper .95
X                    1.386**   0.7214    1.1315     1.698
factor(D)1           1.195     0.8370    0.8435     1.692
factor(D)2           1.659.    0.6029    0.9635     2.855
factor(D)3           1.061     0.9425    0.4820     2.336
X*factor(D)1         0.900     1.1111    0.5848     1.385
X*factor(D)2         0.986     1.0142    0.4979     1.952
X*factor(D)3         1.352     0.7394    0.5097     3.589
</code></pre>

<p><strong>My interpretation</strong>: The interaction term is not significant, as in all Scenarios. But the interpretation would be that when X is = 1, the D = 1 and D = 2 are decreasing (compared to D=0) but when X=1 and D=3, the hazard is increasing. </p>

<ul>
<li><strong>Scenario 5</strong></li>
</ul>

<p>Suppose now that the ""X"" and the ""D"" variables are exactly the same as in the previous scenario. However, this time, variable ""X"" violates the PH assumption. So I am introducing an interaction term between X and stop/start time (years). I know that some would argue that one needs to split the data before doing this, while others would not necessary recommend this. This is somehow a side-debate here. Interesting, but not really relevant here for our example. It's also been discussed elsewhere here. Nevertheless, here is the model: </p>

<pre><code>            exp(coef) exp(-coef) lower .95 upper .95
X             1.5848*    0.6310    1.0795    2.3268
factor(D)1    1.1301     0.8849    0.9192    1.3893
factor(D)2    1.6507**   0.6058    1.1655    2.3378
factor(D)3    1.2698     0.7875    0.7991    2.0179
X*stop        0.9488*    1.0540    0.9026    0.9973
</code></pre>

<p><strong>My interpretation</strong>: The interaction with time does correct for the violation of the assumption: X is decreasing with years. However, X alone is increasing. What is going on here? It doesn't make any sense to me. Unless, the X = 0 (alone), and X = 1 with * stop in the model. If so, the interpretation is then that X = 1 * stop is decreasing over time, while when X = 0, the hazard rate increases with 1.58. </p>

<p><strong>EDIT (additional information):</strong>  </p>

<p>The variables ""X""  and ""D"" are actually discrete (1, 2, 3, 4,..10) but they are treated as continuous. </p>

<p>I use conditional model ( or ""PWP""-model), and the time scale is ""time since entry"". </p>

<p>Both X and D are time-dependent (or time-varying) variables. </p>
"
"0.0929636479491388","0.104695817324578","179250","<p>I am conducting a multifactorial analyisis involving categorical variables by using R. The response is â€œyesâ€ or â€œnoâ€ (Iâ€™m therefore using binary logistic regression) and the predictors have 2 up to 3 levels. The sample size is quite small (n = 230).</p>

<p>I tried before both the glm and rsm packages, but one predictor (B2) had an extremely high standard error; it was a separation problem since one of its level has zero observation. I have been then suggested to rely on penalized logistic regression and in particular the brglm package.</p>

<p>Here as follows, you can see the new output (brglm) for the model containing all the variables:</p>

<pre><code>Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)  1.64053    0.78655   2.086   0.0370 * 
B1x         -0.07434    0.71502  -0.104   0.9172   
B1y         -0.20966    0.59622  -0.352   0.7251   
B2x          0.09004    0.57168   0.158   0.8748   
B2y         -2.68270    1.67192  -1.605   0.1086   
B3x         -0.53347    0.33882  -1.574   0.1154   
B4x         -0.59495    0.30320  -1.962   0.0497 (*) 
B5x         -0.79962    0.47249  -1.692   0.0906 . 
B5y         -1.42761    0.59344  -2.406   0.0161 (*) 
B6x          1.36948    0.49224   2.782   0.0054 (**)
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 275.58  on 229  degrees of freedom
Residual deviance: 270.93  on 220  degrees of freedom
Penalized deviance: 253.9314 
AIC:  290.93 
</code></pre>

<p>As you can see, the problem regarding the huge standard error has been finally solved but there are still several issues:</p>

<p>1) In the description of the bglrm package, they say:</p>

<p>â€œIt is not advised to use methods associated with model comparison (add1, drop1,anova, etc.) on objects of class ""brglm"". Model comparison when estimation is performed using the modified scores or the penalized likelihood is an on-going research topic and will be implemented as soon as it is concludedâ€</p>

<p><strong>How can I safely remove non significant predictors?</strong> <strong>Have I only to rely on the p-value in the above output?</strong> 
That means that I have to remove from a further model predictors from B1 to B4, is that correct? (that was also case when I used a non penalized logistic regression model (such as lrm), but then I could use drop1).</p>

<p>2) <strong>How can I detect multicollinearity issues in brglm?</strong> </p>

<p>By using a non penalized likelihood model, I know that there is multicollinearity between B5 and B6, but how can I prove it now?</p>

<p>3) <strong>How can I calculate in brglm indexes such as C and Nagelgerkeâ€™s R</strong>? </p>

<p>4) In the description of the package, it is also written: â€œThe use of Akaike's information criterion (AIC) for model selection when method = ""brglm.fit"" is controversial.â€  </p>

<p>Is it safe or not to publish AIC in a paper?</p>

<p>Thank you very much in advance!</p>
"
"NaN","NaN","179329","<p>If your logistic regression fit has coefficients with the following attributes, do you look at the values of <code>Pr(Z&gt;|z|)</code> are smaller than 0.95 to determine whether that variable is needed at a 5% level of significance? </p>

<p>ie. If <code>Pr(&gt;|z|)</code> is 0.964, this variable is not needed at 5% significance.</p>

<p><a href=""http://i.stack.imgur.com/6UTBa.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6UTBa.png"" alt=""enter image description here""></a></p>
"
"0.091874672876503","0.0963061447907242","179498","<p>I'm working on a project with R and I don't think I'm using the appropriate linear regression or plot, I've made both but they don't seem to match.  The study is an ANOVA comparing $CO_2$ emissions per capita with 5 groups of income levels and a relevant linear regression.  For the linear regression I want use $CO_2$ as the dependent variable and $GDP$ as the independent variable and the 5 $income$ levels as dummy variables.</p>

<p>Begin by ordering the variables and remove the intercept:</p>

<pre><code>income_factor = factor(Data01$income, levels=c(""Low income"", 
""Lower middle income"", ""Upper middle income"", ""High income: OECD"", ""High
income: nonOECD"")) 

lm.r = lm(CO2 ~ income_factor -1, data=Data01)
</code></pre>

<p>Gives</p>

<pre><code>summary(lm.r)
Coefficients:
                              Estimate Std. Error t value Pr(&gt;|t|)    
income_factorLow income             0.2318     0.6943   0.334  0.73902    
income_factorLower middle income    1.7727     0.6355   2.789  0.00603 ** 
income_factorUpper middle income    4.7685     0.6271   7.604 4.12e-12 ***
income_factorHigh income: OECD      8.7926     0.7305  12.036  &lt; 2e-16 ***
income_factorHigh income: nonOECD  19.4642     1.3667  14.242  &lt; 2e-16 ***
</code></pre>

<p>So that we may write the linear regression in the form:</p>

<p>$$ CO_2 = \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_4 + \beta_5 X_5 $$</p>

<p>Where $X_i$ is a dummy variable 1 at the level of income and 0 otherwise</p>

<p>For the corresponding plot I used:</p>

<pre><code> plot &lt;- ggplot(data=Data01, aes(x=GDP, y=CO2, colour=factor(income)))
 plot + stat_smooth(method=lm, fullrange=FALSE) + geom_point()
</code></pre>

<p>Which gives the graph</p>

<p><a href=""http://i.stack.imgur.com/5Iw53.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5Iw53.png"" alt=""CO2 ~ GDP""></a></p>

<p>But here is my confusion, it looks like there is the <em>lm</em> term in the plot, but I don't think it is using the same values taken from the previous linear regression.  As Looking at summary from the linear regression, High income: OECD the estimate is 8.79, but the line for it is pretty much flat.</p>

<p>While I was typing this I realized that the graph has $GDP$ as the X-axis, but is not included in the linear regression.  Would multiplying by $income$_$factor*GDP$ help?</p>
"
"0.0609427635336005","0.0580747904139041","179748","<p>I have a dataset, say $A$:</p>

<pre><code>x    y
20   3.4
30   3.3
35   4.5 
</code></pre>

<p>I am fitting a regression model, a mixed model of R's <code>lme4</code> family to be exact, to predict $y$ given $x$. 
I have a set of <code>newdata</code> which don't have an observed $y$. 
Let $\hat{y}$ be the predicted value for this set $B$:</p>

<pre><code>x   yhat
20   3.3
100  6.6 
</code></pre>

<p>I need to report whether the predicted value is expected to be reasonably accurate. 
As you can see in dataset $B$, <code>x[2]</code> is somewhat of an outlier, and therefore $\hat{y}$ is also a value which is rare.
It happens to fall outside the 95% confidence interval of predicted values. $\hat{y}$ is very close to a real observation , in real life. </p>

<p>What kind of metric is used to report that $\hat{y}$ is actually quite a good prediction? 
I hope my question is clear even though I've struggled to explain it...</p>

<h3>Edited to add in response to comment below:</h3>

<p>The model has only one fixed parameter ($x$, continuous) while also having a random effects group parameter . 
Model looks like this </p>

<pre><code>LMER2&lt;-lmer(y~x + (1 |group), training_data)
</code></pre>

<p><code>lme4</code> does give me the coefficients and standard errors , and I have used the <code>bootMer</code> function to calculate $\hat{y}$ confidence interval following <a href=""http://www.r-bloggers.com/confidence-intervals-for-prediction-in-glmms/"" rel=""nofollow"">this article</a>.</p>
"
"0.0609427635336005","0.0580747904139041","180217","<p>I'm using time series data containing both trend and seasonality. I also have 2 endogenous predictor variables that I would like to include in my model.</p>

<p>In R I've used the forecast package to develop a dynamic regression model with use of <code>auto.arima()</code> and the <code>xreg</code> argument from the <code>forecast package</code>. I understand this procedure takes a regression and then attempts to fit the residuals with an ARMA Model.</p>

<p>I've also developed what seems to be an appropriate model using the forecasting Module in SPSS by specifying a Seasonal ARIMA model and including my covariates. However, one of the coefficients on one of my endogeneous predictors has a negative sign which makes no sense intuitively. </p>

<p>I've read Dr. Hyndman's article <a href=""http://robjhyndman.com/hyndsight/arimax/"" rel=""nofollow"">The ARIMAX model muddle</a> and found it to be extremely insightful and useful. However, I have not been able to find any documentation on what type of statistical procedure SPSS uses to fit an ARIMA model with covariates, so I'm not sure how I should interpret the coefficients or how concerned I should be with a flipped sign. Any help clarifying the modelling procedure used by SPSS would be tremendously appreciated. </p>
"
"0.075412822378","0.0718638867059034","180285","<p>I'm pretty new to the concepts of stationarity/cointegration. I am using the ""urca"" package in ""Rstudio"" to run my tests.</p>

<p>I have been trying to run cointegration tests, but the frustrating thing is that I haven't been able to find two series that are non-stationary, even when I try using examples cited by cointegration tutorials. My $p$-value is always too big such that I have to reject the null straight away. However, if I look at the $t$-values and compare them to the critical values, they seem to suggest otherwise. </p>

<p>Should I then ignore the $p$-value in the ADF test?
Here are my test results. My two price series are <code>XLE US Equity</code> and <code>CO1 Comdty</code> (Brent 1st futures) from 01/01/2010 - today (5/11/2015).</p>

<p>Any help/elaboration will be very much appreciated, thank you!</p>

<pre><code>&gt; testXLE&lt;-ur.df(XLE,type=""drift"",selectlags=""AIC"")
&gt; summary(testXLE)

############################################### 
# Augmented Dickey-Fuller Test Unit Root Test # 
############################################### 

Test regression drift 


Call:
lm(formula = z.diff ~ z.lag.1 + 1 + z.diff.lag)

Residuals:
     Min       1Q   Median       3Q      Max 
-10.3948  -2.5809   0.6846   2.7908  10.1940 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)  6.58864    3.43524   1.918   0.0596 .
z.lag.1     -0.08584    0.04533  -1.894   0.0628 .
z.diff.lag   0.05529    0.12544   0.441   0.6609  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 4.162 on 64 degrees of freedom
Multiple R-squared:  0.05337,   Adjusted R-squared:  0.02379 
F-statistic: 1.804 on 2 and 64 DF,  p-value: 0.1729


Value of test-statistic is: -1.8936 1.8395 

Critical values for test statistics: 
      1pct  5pct 10pct
tau2 -3.51 -2.89 -2.58
phi1  6.70  4.71  3.86
</code></pre>

<p>My interpretation of the results:</p>

<blockquote>
  <ul>
  <li>according to p-value (0.1729>0.05) do not reject null; series is stationary   </li>
  <li>t-value = (-1.8936>-2.89) --> do not reject null hypothesis; series is not stationary  </li>
  <li>t-value = (1.8395&lt;4.71) --> do not reject a0=0 --> there is no drift</li>
  </ul>
</blockquote>

<p>Conclusion: The series is non-stationary: Random Walk with no drift.</p>
"
"0.101062140164153","0.0963061447907242","180302","<p>IÂ´m fairly new to the tools of statistics and I need some help.
IÂ´m working in R.
I have a list of students, their age, sex, test results(continuous variable) and education (a categorical variable. 0 if student has no education, 1 if student has education). This is stored in the data frame df.</p>

<p>I have assigned each student an age group depending on their age. The groups are 5-10,11-15, 16-20, 21-25. So now I have a new categorical variable with 4 levels.</p>

<p>I have done logistic regression for each age group. I did the test in R like this:</p>

<pre><code>model1 = glm(education ~ test results + sex + age, data = df, family = binomial())
</code></pre>

<p>From the logistic regression result I compute the odds ratio (exp(coefficient)) for the test results variable. For each age group I got an odds ratio and confidence interval. There are some differences in the odds ratio between age groups. The odd ratio gets closer to 1 with higher age group. I interpret this in the following way; as the age of students is higher there is less effect from education on test result.
My question is how can I test if the difference in odds ratios is significant, i.e. what test can I use in R ?</p>

<p>Here is a subset of the data:</p>

<pre><code>student sex age testScore education ageGroup

1        1   10   0.12       1        1

2        1    8   0.08       1        1

3        2   16   0.85       0        3

4        2   20   0.12       0        3

5        1   22   1.02       0        4

6        2   18   0.98       1        3

7        1   19   0.46       1        3

8        1   16   0.83       0        3

9        2   12   0.26       1        2

10       2   14   0.46       0        2
</code></pre>

<p>I have been searching books and the web without success, canÂ´t seem to find an example that I can relate to.
Any suggestions would be appreciated.</p>
"
"0.114267681625501","0.116149580827808","180447","<p>I have some data on patients presenting to emergency departments after sustaining self-inflicted gunshot injuries, stored in a data frame (""SIGSW,"" which is ~16,000 observations of 47 variables) in R. I want to create a model that helps a physician predict, using several objective covariates, the ""pretest probability"" of the self-shooting being a suicide attempt, or a negligent discharge. The covariates are largely categorical variables, but a few are continuous or binary. My outcome, suicide attempt or not, is coded as a binary/indicator variable, ""SI,"" so I believe a binary logistic regression to be the appropriate tool.  </p>

<p>In order to construct my model, I intended to individually regress SI on each covariate, and use the p-value from the likelihood ratio test for each model to inform which covariates should be considered for the backward model selection. </p>

<p>For each model, SI~SEX, SI~AGE, etc, I receive the following error:</p>

<pre><code>&gt;glm(SI ~ SEX, family = binomial, data=SIGSW)
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: algorithm did not converge
</code></pre>

<p>A little Googling revealed that I perhaps need to increase the number of iterations to allow convergence. I did this with the following:</p>

<pre><code>&gt;glm(SI ~ SEX, family = binomial, data=SIGSW, control = list(maxit = 50))

Call:  glm(formula = SI ~ SEX, family = binomial, data = SIGSW, control = list(maxit = 50))

Coefficients:
(Intercept)          SEX  
 -3.157e+01   -2.249e-13  

Degrees of Freedom: 15986 Total (i.e. Null);  15985 Residual
Null Deviance:      0 
Residual Deviance: 7.1e-12  AIC: 4
Warning message:
glm.fit: fitted probabilities numerically 0 or 1 occurred 
</code></pre>

<p>This warning message, after a little Googling, suggests a ""perfect separation,"" which, as I understand it, means that my predictor is ""too good."" Seeing as how this happens with all of the predictors, I'm somewhat skeptical that they're all ""too good."" Am I doing something wrong? </p>

<p>Edit: In light of the answers, here is a sample of the data (I only selected a few of the variables for space concerns):</p>

<pre><code>   SIGSW.AGENYR_C SIGSW.SEX SIGSW.RACE_C SIGSW.SI
1              19      Male        White        0
2              13      Male        Other        0
3              18      Male   Not Stated        0
4              15      Male        White        0
5              23      Male        White        0
6              11      Male        Black        0
7              16      Male   Not Stated        1
8              21      Male   Not Stated        0
9              14      Male        White        0
10             41      Male        White        0
</code></pre>

<p>And here is the crosstabulation of SEX and SI, showing that SI is coded as an indicator variable, and that there are both men and women with SI, so sex is not a perfect predictor. </p>

<pre><code>  &gt;table(SIGSW$SEX, SIGSW$SI)        
              0     1
  Unknown     1     3
  Male    11729  2121
  Female   1676   457
</code></pre>

<p>Does the small cell size represent a problem?</p>
"
"0.0545088647991304","0.0649295895722714","180483","<p>I am considering a Bayesian Gaussian spatial regression model</p>

<pre><code>y(s) = x(s) b + w(s) + e 
</code></pre>

<p>where </p>

<pre><code>w|theta ~ N(0,k2 H(phi))
e ~ N(0,tau2)
</code></pre>

<p>Assuming the range <code>phi</code> and the <code>tau2/sigma2</code> ratio fixed, a normal prior on the regression coefficients <code>b</code> and an inverse gamma on <code>sigma2</code> reduces the problem to a conjugate Bayesian linear regression problem. Hence no need for MCMC and I can use the <code>bayesGeostatExact</code> function.</p>

<p>My question is: if we assume instead that <code>phi</code> and the <code>tau2/sigma2</code> ratio are NOT fixed we need to specify priors for the spatial parameters as well. 
Assuming a uniform prior for <code>phi</code> and inverse gamma priors for <code>tau2</code> and <code>sigma2</code> ratio makes the conjugacy still hold?</p>

<p>I don't understand why in this case we need to use a MCMC simulation (through the <code>spLM</code> function) and there is no equivalent <code>bayesGeostatExact</code> function if the spatial parameters are not fixed.</p>

<p>Thanks</p>
"
"0.105869651339174","0.100887413963854","180854","<p>I am trying to test for cointegration between two series that based on qualitative reasoning, should be cointegrated. They are the prices of XLE ETF (<code>XLE US equity</code>) and 1st futures of Brent (<code>CO1 Comdty</code>). However, the results that I arrive at using two different methods both show that there exists no cointegration between the two series - not sure if my execution or the interpretation of the data is wrong? </p>

<p>(Both XLE and Brent 1st Futures have been tested for non-stationarity using ADF test from ""urca"" package)</p>

<p><strong>1st test - Engle Granger 2-step test:</strong><br>
In doing this, I referenced <em>Using R to Test Pairs of Securities for Cointegration</em> by Paul Teetor</p>

<p><strong>(1)</strong> Conducting Spread</p>

<pre><code>&gt; M&lt;-lm(XLE~Brent+0,data=XLE.Brent)
&gt; beta&lt;-coef(M)[1]
&gt; spread&lt;-XLE.Brent$XLE-beta*XLE.Brent$Brent
&gt; 
&gt; summary(M)

Call:
lm(formula = XLE ~ Brent + 0, data = XLE.Brent)

Residuals:
   Min      1Q  Median      3Q     Max 
-20.363  -9.543  -2.909  13.294  36.269 

Coefficients:
     Estimate Std. Error t value Pr(&gt;|t|)    
&gt;Brent  0.74962    0.02004    37.4   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 16.37 on 68 degrees of freedom
Multiple R-squared:  0.9536,    Adjusted R-squared:  0.953 
F-statistic:  1399 on 1 and 68 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p><strong>(2)</strong> Testing the stationarity of the spread using ADF test (from package ""urca""):</p>

<pre><code>&gt; spread.ADF&lt;-ur.df(spread,type=""none"",selectlags=""AIC"")
&gt; summary(spread.ADF)

############################################### 
# Augmented Dickey-Fuller Test Unit Root Test #  
</code></pre>

#########################################

<pre><code>Test regression none 


Call:
lm(formula = z.diff ~ z.lag.1 - 1 + z.diff.lag)

Residuals:
   Min      1Q  Median      3Q     Max 
 -6.1449 -2.2523  0.5559  2.9194  8.4567 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
z.lag.1    -0.0003928  0.0266919  -0.015    0.988
z.diff.lag  0.1207084  0.1278700   0.944    0.349

Residual standard error: 3.443 on 65 degrees of freedom
Multiple R-squared:  0.01395,   Adjusted R-squared:  -0.01639 
F-statistic: 0.4596 on 2 and 65 DF,  p-value: 0.6335


Value of test-statistic is: -0.0147 

Critical values for test statistics: 
    1pct  5pct 10pct
tau1 -2.6 -1.95 -1.61
</code></pre>

<p>My interpretation: since $t$-value = <code>-0.0147</code> is bigger than <code>-1.61</code>, do not reject null. Spread is not stationary. Hence no cointegration between XLE and Brent.</p>

<p><strong>Second Test: Johansen Test</strong></p>

<pre><code>&gt; XLE.brent.coint&lt;-ca.jo(data.frame(XLE,Brent),type=""trace"",ecdet=""trend"",K=2,spec=""longrun"")
&gt; summary(XLE.brent.coint)
&gt;
&gt;###################### 
&gt;# Johansen-Procedure # 
&gt;###################### 
&gt;
&gt;Test type: trace statistic , with linear trend in cointegration 
&gt;
&gt;Eigenvalues (lambda):
&gt;[1] 8.179514e-02 6.025284e-02 2.775558e-17
&gt;
&gt;Values of teststatistic and critical values of test:
&gt;
&gt;        test 10pct  5pct  1pct
&gt;r &lt;= 1 | 4.16 10.49 12.25 16.26
&gt;r = 0  | 9.88 22.76 25.32 30.45
&gt;
&gt;Eigenvectors, normalised to first column:
&gt;(These are the cointegration relations)
&gt;
&gt;          XLE.l2   Brent.l2   trend.l2
&gt;XLE.l2   1.000000  1.0000000  1.0000000
&gt;Brent.l2 1.467806 -0.4346323  0.1610563
&gt;trend.l2 1.896366 -0.4903454 -0.8891875
&gt;
&gt;Weights W:
&gt;(This is the loading matrix)
&gt;
&gt;            XLE.l2    Brent.l2      trend.l2
&gt;XLE.d   -0.01629102 -0.13534537 -4.695795e-17
&gt;Brent.d -0.03819241 -0.03886418  5.127543e-17
</code></pre>

<p>My interpretation: Since $t$-value for <code>r=0: 9.88&lt;22.76</code>, do not reject null. Hence <code>r=0</code>, there exists no cointegration between <strong>XLE and Brent</strong>.</p>

<p>Additionally, I have carried out cointegration tests (both methods) on <strong>US 10 year and 2 year yields</strong>, and the results on both tell me that the series are not co-integrated, which does not make sense intuitively. Something must be wrong with the way I'm doing the tests!</p>

<p><a href=""http://i.stack.imgur.com/eApO4.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/eApO4.jpg"" alt=""enter image description here""></a>
<a href=""http://i.stack.imgur.com/sf2HV.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/sf2HV.jpg"" alt=""enter image description here""></a></p>
"
"0.0977258320053917","0.100887413963854","181333","<p>I am analyzing count data with a lot of zeros and found that although the data do not fit a poisson glm, they fit the zero-inflated poisson (ZIP) regression significantly better than the standard poisson glm. </p>

<p>This analysis is for a BACI study, in which I have data before, during, and after the treatment, in three zones: control, on-trail (treatment1) and off-trail (treatment2). </p>

<p>I am interested in the difference in change of detection rate of a species between the on-trail (treatment) site vs. control site, before and after the treatment. I have performed contrast on this data to determine this difference with a simple linear regression model (using lm), but I'm unsure how to find this difference using the zero-inflated poisson model. </p>

<p>The results of my ZIP model are here (ZP = Zone+Phase combined into one variable; the ""After"" phase is called ""Open"" in the dataset)</p>

<pre><code>Call:
zeroinfl(formula = Deer ~ ZP | ZP, data = zinb)

Pearson residuals:
    Min      1Q  Median      3Q     Max 
-0.6756 -0.5180 -0.4137 -0.1243 14.8998 

Count model coefficients (poisson with log link):
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  1.076730   0.031034  34.695  &lt; 2e-16 ***
ZPCDuring    0.080611   0.055391   1.455    0.146    
ZPCOpen     -0.696793   0.092432  -7.538 4.76e-14 ***
ZPFBefore    0.062467   0.042638   1.465    0.143    
ZPFDuring    0.112727   0.067928   1.659    0.097 .  
ZPFOpen     -0.765391   0.080475  -9.511  &lt; 2e-16 ***
ZPTBefore   -0.008428   0.045729  -0.184    0.854    
ZPTDuring   -0.063361   0.063193  -1.003    0.316    
ZPTOpen     -0.717266   0.078422  -9.146  &lt; 2e-16 ***

Zero-inflation model coefficients (binomial with logit link):
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.40428    0.06617   6.110 9.97e-10 ***
ZPCDuring    0.85610    0.11076   7.729 1.08e-14 ***
ZPCOpen      0.79893    0.13448   5.941 2.84e-09 ***
ZPFBefore   -0.04894    0.09287  -0.527    0.598    
ZPFDuring    1.51339    0.13035  11.610  &lt; 2e-16 ***
ZPFOpen      0.20254    0.12932   1.566    0.117    
ZPTBefore   -0.08598    0.09830  -0.875    0.382    
ZPTDuring    0.98729    0.11720   8.424  &lt; 2e-16 ***
ZPTOpen      0.17416    0.12823   1.358    0.174    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Number of iterations in BFGS optimization: 25 
Log-likelihood: -8572 on 18 Df
</code></pre>

<p>and the code i have been using to get the contrasts using the lm model is here (based on <a href=""http://people.stat.sfu.ca/~cschwarz/Stat-650/Notes/PDFbigbook-R/R-part013.pdf"" rel=""nofollow"">this tutorial</a>): </p>

<pre><code>result.lm &lt;- lm(Deer ~ Zone + PhaseBDO + Zone:PhaseBDO, data=counts)
options(contrasts=c(unordered=""contr.sum"", ordered=""contr.poly"")) 
result.lsmo.SP &lt;- lsmeans::lsmeans(result.lm, ~Zone:PhaseBDO)

contrast(result.lsmo.SP, list(bact=c(1, 0, -1, 0, 0, 0, -1, 0, 1)))
confint(contrast(result.lsmo.SP, list(bact=c(1, 0, -1, 0, 0, 0, -1, 0, 1))))
</code></pre>

<p>Can anyone suggest how I can calculate the contrast from my ZIP regression model to test for significant differences between the difference in detection rates between the two time periods (before/after) in the two zones (control/treatment)? </p>

<p>For example, I want to be able to say if there was a significantly larger increase (or decrease) in the detection rate of deer after the treatment was implemented, in the treatment zone compared with the control zone. </p>

<p>Thanks in advance for your suggestions.</p>
"
"0.052777981396926","0.0502942438178979","181400","<p>I am working with mtcars dataset and using linear regression</p>

<pre><code>data(mtcars)
fit &lt;- lm(mpg ~.,mtcars)    
summary(fit)
</code></pre>

<p>When I fit the model with lm it shows the result like this</p>

<pre><code>Call:
lm(formula = mpg ~ ., data = mtcars)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.5087 -1.3584 -0.0948  0.7745  4.6251 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept) 23.87913   20.06582   1.190   0.2525  
cyl6        -2.64870    3.04089  -0.871   0.3975  
cyl8        -0.33616    7.15954  -0.047   0.9632  
disp         0.03555    0.03190   1.114   0.2827  
hp          -0.07051    0.03943  -1.788   0.0939 .
drat         1.18283    2.48348   0.476   0.6407  
wt          -4.52978    2.53875  -1.784   0.0946 .
qsec         0.36784    0.93540   0.393   0.6997  
vs1          1.93085    2.87126   0.672   0.5115  
amManual     1.21212    3.21355   0.377   0.7113  
gear4        1.11435    3.79952   0.293   0.7733  
gear5        2.52840    3.73636   0.677   0.5089  
carb2       -0.97935    2.31797  -0.423   0.6787  
carb3        2.99964    4.29355   0.699   0.4955  
carb4        1.09142    4.44962   0.245   0.8096  
carb6        4.47757    6.38406   0.701   0.4938  
carb8        7.25041    8.36057   0.867   0.3995  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 2.833 on 15 degrees of freedom
Multiple R-squared:  0.8931,    Adjusted R-squared:  0.779 
F-statistic:  7.83 on 16 and 15 DF,  p-value: 0.000124
</code></pre>

<p>I found that none of variables are marked as significant at 0.05 significant level.</p>

<p>How to make variables significant at 0.05 significant level?</p>
"
"0.068136080998913","0.0649295895722714","181489","<p>I have to estimate a number of regressions where a lot of autcorrelation is present. Now, for historical reasons, this autocorrelation is resolved using an iterative Prais-Winsten estimation (a modification of the Cochraneâ€“Orcutt estimation). I have found some R-code which performs this procedure:</p>

<pre><code>    prais.winsten.lm &lt;- function(mod){
    X &lt;- model.matrix(mod)
    y &lt;- model.response(model.frame(mod))
    e &lt;- residuals(mod)
 n &lt;- length(e)
 names &lt;- colnames(X)
 rho &lt;- sum(e[1:(n-1)]*e[2:n])/sum(e^2)
 y &lt;- c(y[1] * (1 - rho^2)^0.5, y[2:n] - rho * y[1:(n-1)])
 X &lt;- rbind(X[1,] * (1 - rho^2)^0.5, X[2:n,] - rho * X[1:(n-1),])
 mod &lt;- lm(y ~ X - 1)
 result &lt;- list()
 result$coefficients &lt;- coef(mod)
     names(result$coefficients) &lt;- names
 summary &lt;- summary(mod, corr = F)
 result$cov &lt;- (summary$sigma^2) * summary$cov.unscaled
     dimnames(result$cov) &lt;- list(names, names)
 result$sigma &lt;- summary$sigma
 result$rho &lt;- rho
 class(result) &lt;- 'prais.winsten'
 result
 }
</code></pre>

<p>Now, this code works fine when all the other regressors are exogeneous. But, in my case, a part of X is endogeneous turning the standard ols regression performed in the above code not usable.</p>

<p>I was thinking about modifying the above code into the following:</p>

<pre><code>  prais.winsten.plm &lt;- function(mod){
  X &lt;- model.matrix(mod,component=""projected"")
  Z &lt;- model.matrix(mod,component=""instruments"")
  y &lt;- model.response(model.frame(mod))
  e &lt;- residuals(mod)
  n &lt;- length(e)
  names &lt;- colnames(X)
  rho &lt;- sum(e[1:(n-1)]*e[2:n])/sum(e^2)
  y &lt;- c(y[1] * (1 - rho^2)^0.5, y[2:n] - rho * y[1:(n-1)])
  X &lt;- rbind(X[1,] * (1 - rho^2)^0.5, X[2:n,] - rho * X[1:(n-1),])
  Z &lt;- rbind(Z[1,] * (1 - rho^2)^0.5, Z[2:n,] - rho * Z[1:(n-1),])
  mod &lt;- ivreg(y ~ X -1|Z)
  result &lt;- list()
  result$coefficients &lt;- coef(mod)
      names(result$coefficients) &lt;- names
  summary &lt;- summary(mod, corr = F)
  cov &lt;- (summary$sigma^2) * summary$cov.unscaled
  result$se&lt;- sqrt(diag(cov))
      dimnames(cov) &lt;- list(names, names)
      result$sigma &lt;- summary$sigma
      result$rho &lt;- rho
  class(result) &lt;- 'prais.winsten'
  result
  }
</code></pre>

<p>I have however no idea if such an approach is correct, especially since I found no similar ways in dealing with this problem.</p>
"
"0.0806196982594614","0.076825726438694","181603","<p>I am enrolled in a <a href=""http://www.youtube.com/playlist?list=PLA89DCFA6ADACE599"" rel=""nofollow"">machine learning course</a> for machine learning where we have a lab to implement linear regression
I am attempting to do it in R to get a better understanding of the material and of R for myself (i don't intend to submit this as a lab as the course doesn't use R) but am coming up against a wall</p>

<p>My understanding of the process is as follows</p>

<ul>
<li><p>User Generates a model based on the hypothesis
$h_\theta(x) = \theta^TX= \theta_0x_0 +\theta_1x_1+\dots$</p></li>
<li><p>Take error rate of your model by using squared error cost function, then iterate, create a new hypothesis and get the error rate of this. Continue through $n$ iterations based on the formula
$J(\theta_0,\theta_1)=\frac{1}{2m}\displaystyle\sum_1^m(h_\theta(x^{(i)})âˆ’y^{(i)})^2$. </p></li>
<li><p>Take all the error rates you have recorded based on the cost history and use <code>gradient descent</code> to find automatically the optimal values of your hypothesis.</p></li>
</ul>

<p>Using the code on <a href=""http://www.r-bloggers.com/linear-regression-by-gradient-descent/"" rel=""nofollow"">R-Bloggers</a> where the gradient descent is implement below based on vectors <code>x</code> and <code>y</code></p>

<pre><code># add a column of 1's for the intercept coefficient
X &lt;- cbind(1, matrix(x))

# gradient descent
for (i in 1:num_iters) {
 error &lt;- (X %*% theta - y)
 delta &lt;- (t(X) %*% error) / length(y)
 theta &lt;- theta - alpha * delta
 cost_history[i] &lt;- cost(X, y, theta)
 theta_history[[i]] &lt;- theta
}
</code></pre>

<p>I was wondering if people could help me tease out the logic</p>

<ol>
<li><p>Why is the number 1 applied to the matrix <code>X</code>. Is this so that X has 2 columns so that it can be multiplied by theta - y?</p></li>
<li><p>What is the formula delta actually calculating and why is the Transpose of X being used</p></li>
</ol>

<p>Conceptually I think i understand the overall process but i just need to relate this back to the R code as i want to grasp the concept before proceeding to Multiple linear regression</p>
"
"0.101414888671788","0.0885887685054121","181665","<p>My question relates to calculating the uncertainty associated with the estimation of slopes in a varying intercept, varying slope hierarchical model.</p>

<p>I would like to calculate the effect of a treatment in different districts. If I ran a simple linear regression in which there was no pooling at district level, my model would look something like:</p>

<pre><code>    fm1 &lt;- lm(response ~ treatment*district)
</code></pre>

<p>I could then find the effect of treatment in district j by summing the coefficient for treatment with that of the interaction term treatment:districtj, and the standard error for that estimate could be found with:</p>

<pre><code>    sqrt(vcov(fm1)[2,2] + vcov(fm1)[4,4] + 2*vcov(fm1)[2,4])
</code></pre>

<p>I get stuck when moving to a multilevel model with random slopes along the lines of:</p>

<pre><code>    fm2 &lt;- lmer(response ~ treatment + (1 + treatment | district))
</code></pre>

<p>I can again find the effect of treatment in district j easily by summing the coefficient for treatment with the random slope estimate for district j. I would like to account for the uncertainty of both coefficients and I'm not sure how to do that. I can use the arm package to extract the standard errors associated with the slope estimates for each district and I know the standard error of the treatment estimate but I don't know how to estimate their correlation.  </p>

<p>It seems like this should be an easy question to answer but I haven't been able to find it. Hoping someone can point me in the right direction.</p>
"
"0","0.029037395206952","181695","<p>In linear regression, if I have a model,</p>

<pre><code>b0 + b1x1 + b2x2 + b3x3 + b4x4 = y
</code></pre>

<p>and I want to fix some of the coefficients ,say b1 = 1 and b3 = 2, I could just do the following</p>

<pre><code>b0 + b2x2 + b4x4 = y - x1 - 2x3
</code></pre>

<p>and just fit a linear regression on the other three parameters on the new y. Is there a way to do this for logistic regression? The sigmoid function seems to complicate things. Im looking to do this in r, so if theres an easy way to do it in r, that would be very appreciated.</p>
"
"0.0304713817668003","0","181790","<p>I would like to know how I can go about examining the results of a partial least squares regression. Specifically, I am interested to know what the coefficient is for each component, and what the linear combination of variables within each component looks like.</p>

<p>Lets consider this as an example:</p>

<pre><code>library(AppliedPredictiveModeling)
library(plsr)
data(solubility)

trainingData &lt;- solTrainXtrans
trainingData$Solubility &lt;- solTrainY

plsFit &lt;- plsr(Solubility ~ ., data = trainingData, ncomp=8)
</code></pre>

<p>Furthermore, if somebody could also help me understand what the different attributes within the mvr object are I would be greatly appreciative.</p>

<pre><code>&gt; attributes(plsFit)
$names
 [1] ""coefficients""    ""scores""          ""loadings""        ""loading.weights""
 [5] ""Yscores""         ""Yloadings""       ""projection""      ""Xmeans""         
 [9] ""Ymeans""          ""fitted.values""   ""residuals""       ""Xvar""           
[13] ""Xtotvar""         ""fit.time""        ""ncomp""           ""method""         
[17] ""call""            ""terms""           ""model""          
</code></pre>

<p>How do coefficients, scores, loadings, and weights all relate to each other?</p>

<p>Thanks!</p>
"
"0.075412822378","0.0821301562353182","182286","<p>I am doing a regression analysis for an ordinal response variable with 5 explanatory variables. I will be using the <code>polr()</code> or <code>lrm()</code> functions to do the ordinal logistic regression. For my non-ordinal response variables (e.g., count and binary data), I have been using glmulti for model selection, but this doesn't seem to be compatible with the <code>polr()</code> and <code>lrm()</code> R functions. I've also tried <code>stepAIC()</code>, <code>step()</code> and <code>leap()</code> functions without any luck. The summary of the <code>polr()</code> regression shows an AIC score.</p>

<pre><code>&gt; model1 &lt;- polr(x ~ Age + Gender + StudentType + StudentYear + RacialGroup,
+ data = question8a, Hess =TRUE)
&gt; summary(model1)
Call:
polr(formula = x ~ Age + Gender + StudentType + 
    StudentYear + RacialGroup, data = question8a, Hess = TRUE)

Coefficients:
                                   Value Std. Error  t value
Age                             -0.16691    0.04925 -3.38872
GenderWoman                      0.05514    0.24655  0.22366
StudentTypeUndergraduatestudent -1.36414    0.50748 -2.68807
StudentYear2ndyear              -0.02042    0.29600 -0.06899
StudentYear3rdyear              -0.05997    0.38253 -0.15676
StudentYear4+years               0.89921    0.66430  1.35363
StudentYear4thyear               0.25324    0.42433  0.59680
RacialGroupNon-Indigenous       -2.13460    0.42163 -5.06268

Intercepts:
    Value   Std. Error t value
1|2 -9.9335  1.5283    -6.4999
2|3 -8.3051  1.4752    -5.6298
3|4 -7.2498  1.4567    -4.9770
4|5 -4.8720  1.4240    -3.4214

Residual Deviance: 657.086 
AIC: 681.086 
</code></pre>

<p>I tried to follow this suggestion: <a href=""http://statistics.unl.edu/faculty/bilder/categorical/Chapter5/glmultiFORpolr.R"" rel=""nofollow"">http://statistics.unl.edu/faculty/bilder/categorical/Chapter5/glmultiFORpolr.R</a>, but wasn't able to get it to work. </p>

<p>Has anyone been able to get this to work? Or do I need to compare the 2^5 = 32 model AIC scores by hand? </p>
"
"0.0609427635336005","0.0580747904139041","182595","<p>I'm trying to use sparse linear model for my data,input x(29*50),output y(29*1). In R, the package of <strong><em>glmnet</em></strong> can be used. </p>

<p>Firstly, cv.glmnet() choose lambda and coefficients(at min error), here with leave-one-out cv method,and then plot it. </p>

<pre><code>cv.fit = cv.glmnet(x,y,family=""gaussian"",nfolds=29)

plot(cv.fit)
</code></pre>

<p><a href=""http://i.stack.imgur.com/PEEeb.png"" rel=""nofollow"">the plot of mse aganist log(lambda) in cv model</a></p>

<p>Next, print the coefficients</p>

<pre><code>coef(cv.fit,s=""lambda.min"")
</code></pre>

<blockquote>
  <p>51 x 1 sparse Matrix of class ""dgCMatrix""               </p>

<pre><code>              1
</code></pre>
  
  <p>(Intercept)   267.7241</p>
  
  <p>cluster_0  .<br>
  cluster_1     .<br>
  cluster_2     .<br>
  cluster_3     .<br>
  cluster_4     .<br>
  ...</p>
  
  <p>cluster_47    .<br>
  cluster_48    .<br>
  cluster_49    .  </p>
</blockquote>

<p>Finally, to measure the model's ability for prediction, accuracy is calculated(defined as 1 minus average absolute error divided by numeric range of y)</p>

<pre><code>py &lt;- predict(cv.fit,newx=x,s=""lambda.min"")
py
</code></pre>

<blockquote>
  <p>V1     267.7241</p>
  
  <p>V2     267.7241</p>
  
  <p>...</p>
  
  <p>v29    267.7241  </p>
</blockquote>

<pre><code>ave_abs_error &lt;- mean(abs(py-y))
n_range &lt;- max(y)-min(y)
acc &lt;- 1-ave_abs_error/n_range
acc
</code></pre>

<blockquote>
  <blockquote>
    <p>0.918365</p>
  </blockquote>
</blockquote>

<p>Although the acc(0.918365) is very high, there is a serious problem. As seen from the plot above, the lambda.min is very large(73.03439),and all coefficients  are zero(only with intercept value 267.7241), all predicted py are the same as intercept.
That's really weird! </p>

<p>I searched lots of threads in forum, here<a href=""http://stats.stackexchange.com/questions/72251/an-example-lasso-regression-using-glmnet-for-binary-outcome"">http://stats.stackexchange.com/questions/72251/an-example-lasso-regression-using-glmnet-for-binary-outcome</a> explains that there is no local min for too few observations and all coefficients were shrunk to zero with the shrinkage penalties.</p>

<p>Does anybody has other interpretations?</p>

<p>Thanks in advance!</p>
"
"0.0770871758684916","0.0918243061724248","182950","<pre><code>head(jobshop)
  X totalcost units goal.sd weight stamp chisel detail rush labor cost   lost  manager room.temp music shift mach.hrs plant breakdown rework
1 1  90751.53   423     0.1   4.48     4      7     No  Yes  1.47 4.71 0.8317     Alan     74.71  None     2    1.277   Old         1  0.114
2 2 100456.65   554     1.0   4.35     2      3    Yes   No  1.26 4.82 0.4951    Devon     75.37   Pop     1    1.317   New         0  0.000
3 3 128574.01   607     0.5   5.00     2      4     No   No  1.20 5.32 0.5584 Beatrice     75.29  None     1    1.071   Old         0  0.101
4 4  73996.67   347     1.0   5.39     2      4     No   No  1.46 5.44 0.4562  Ebrahim     75.27  Soul     1    1.375   New         0  0.000
5 5  98494.52   510     1.0   4.80     2      4     No   No  1.23 4.98 0.5018  Ebrahim     75.31  Soul     1    1.455   New         0  0.000
6 6  66745.85   419     0.5   3.99     2      4     No   No  1.21 4.60 0.4100  Ebrahim     75.38  Soul     1    1.065   New         0  0.000

my.fit &lt;-lm(totalcost ~ units + goal.sd + weight + stamp + chisel + detail + rush + labor + cost +  lost + manager + room.temp + music + shift + mach.hrs + plant + breakdown + rework, data=jobshop)

goalfact &lt;- as.factor(my.fit$goal.sd) 
stampfact &lt;- as.factor(my.fit$stamp)
chiselfact &lt;- as.factor(my.fit$chisel)
detailfact &lt;- as.factor(my.fit$detail)
rushfact &lt;- as.factor(my.fit$rush)
mgrfact &lt;- as.factor(my.fit$manager)
shiftfact &lt;- as.factor(my.fit$shift)
plntfact &lt;- as.factor(my.fit$plant)
summary(my.fit)

Call:
lm(formula = totalcost ~ units + goal.sd + weight + stamp + chisel + 
    detail + rush + labor + cost + lost + manager + room.temp + 
    music + shift + mach.hrs + plant + breakdown + rework, data = jobshop)

Residuals:
    Min      1Q  Median      3Q     Max 
-118706   -2007     286    2766   18836 

Coefficients: (5 not defined because of singularities)
                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)     -1.065e+05  1.493e+04  -7.129 3.73e-12 ***
units            1.913e+02  2.579e+00  74.151  &lt; 2e-16 ***
goal.sd         -1.519e+03  1.208e+03  -1.257  0.20946    
weight           6.554e+03  5.787e+02  11.326  &lt; 2e-16 ***
stamp           -1.412e+03  7.177e+02  -1.967  0.04972 *  
chisel           1.120e+02  5.057e+02   0.221  0.82487    
detailYes        6.880e+02  1.071e+03   0.643  0.52079    
rushYes          8.511e+02  9.893e+02   0.860  0.39006    
labor            1.112e+04  3.842e+03   2.894  0.00398 ** 
cost             5.920e+03  9.230e+02   6.413 3.40e-10 ***
lost             1.775e+04  6.871e+03   2.583  0.01010 *  
managerBeatrice  1.751e+03  1.235e+03   1.417  0.15714    
managerCarl     -1.137e+04  1.678e+03  -6.774 3.67e-11 ***
managerDevon    -1.164e+04  1.721e+03  -6.767 3.84e-11 ***
managerEbrahim  -1.107e+04  1.679e+03  -6.597 1.11e-10 ***
room.temp        2.108e+01  1.817e+02   0.116  0.90765    
musicPop                NA         NA      NA       NA    
musicRock               NA         NA      NA       NA    
musicSoul               NA         NA      NA       NA    
shift                   NA         NA      NA       NA    
mach.hrs         2.683e+04  1.870e+03  14.353  &lt; 2e-16 ***
plantOld                NA         NA      NA       NA    
breakdown       -3.701e+01  9.261e+02  -0.040  0.96814    
rework          -8.327e+03  1.121e+04  -0.743  0.45795    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 8361 on 481 degrees of freedom
Multiple R-squared:   0.93, Adjusted R-squared:  0.9274 
F-statistic: 355.2 on 18 and 481 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>Hi, I am taking a first course in regression and was trying to fit the above model. I tried to factor or make dummy variables of a few of the predictors but somehow I am getting the <code>Coefficients: (5 not defined because of singularities)</code> and <code>NA</code> for music and breakdown.
Please tell me what I'm doing wrong. I am stuck!</p>
"
"0.0457070726502004","0.0580747904139041","183206","<p>Simply put, I'd like to know how the plm package in R calculates the residuals of a random-effect regression.</p>

<p>I ask this because i'm getting some ""weird"" outputs. Let-me reproduce them here using the Grunfeld data for four firms, like Gujarati in his Basic Econometrics do:</p>

<pre><code>require(plm)
require(foreign)

Grunfeld&lt;-read.dta(""Data.dta"")
Grunfeld&lt;-pdata.frame(Grunfeld,index = c(""id"",""t""))

grun.re &lt;- plm(Y~X2+X3,data=Grunfeld,model=""random"",index=""id"")

#Means by id
X2M&lt;-tapply(Grunfeld$X2,Grunfeld$id,FUN = mean)
X3M&lt;-tapply(Grunfeld$X3,Grunfeld$id,FUN = mean)
YM&lt;-tapply(Grunfeld$Y,Grunfeld$id,FUN = mean)

#Random Effect: Fit the model and the calculate residuals ""by hand""
fit.re&lt;-grun.re$coefficients[1]+grun.re$coefficients[2]*Grunfeld$X2+grun.re$coefficients[3]*Grunfeld$X3
    calcResid.re&lt;-(Grunfeld$Y-fit.re)

#Random Effect:
head(cbind(grun.re$residuals,Grunfeld[,11:13],calcResid.re))

  grun.re$residuals   alphaRE       eRE        uRE calcResid.re
1         99.395803 -169.9282 116.23154  -53.69666    -53.69666
2         18.023715 -169.9282  34.85946 -135.06874   -135.06874
3        -39.256625 -169.9282 -22.42089 -192.34909   -192.34908
4         -2.857048 -169.9282  13.97869 -155.94951   -155.94951
5        -28.334107 -169.9282 -11.49837 -181.42656   -181.42656
6          6.475226 -169.9282  23.31096 -146.61723   -146.61723
</code></pre>

<p>In this table, uRE is the overall residual of the regression provided by Stata (which is identical to Gretl's) and calcResid.re is the manually calculated residuals from the fitted model. So, Stata, Gretl and I did the same. But what plm package do?</p>

<p>We can se that calcResid.re and uRE are equals. But the residuals provided by the plm estimation (grun.re$residuals) completely differs.</p>

<p>Here is a link to the dataset and results: <a href=""https://www.dropbox.com/s/v6uex5ziee8xroj/Data.dta?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/v6uex5ziee8xroj/Data.dta?dl=0</a></p>
"
"0.13282167379153","0.126571071290756","183320","<p>I have the following dataframe on which I did logistic regression with response as outcome. There are some good predictors in these variables so I expected significant variables.</p>

<pre><code>structure(list(response = c(0L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 
    0L, 0L, 1L, 0L, 1L, 0L), HIST1H3F_rna = c(1.09861228866811, 0.693147180559945, 
    2.07944154167984, 1.09861228866811, 1.79175946922805, 0, 0, 0, 
    2.39789527279837, 1.38629436111989, 1.6094379124341, 1.6094379124341, 
    0.693147180559945, 1.79175946922805, 0), NCF1_rna = c(2.77258872223978, 
    3.09104245335832, 2.63905732961526, 2.19722457733622, 2.30258509299405, 
    2.56494935746154, 3.09104245335832, 3.98898404656427, 2.56494935746154, 
    4.06044301054642, 3.87120101090789, 2.07944154167984, 3.49650756146648, 
    3.17805383034795, 3.95124371858143), WDR66_rna = c(5.06890420222023, 
    4.49980967033027, 5.11799381241676, 3.40119738166216, 3.25809653802148, 
    4.02535169073515, 5.8348107370626, 5.89440283426485, 3.87120101090789, 
    5.67675380226828, 5.35185813347607, 4.15888308335967, 6.23441072571837, 
    5.91889385427315, 3.68887945411394), PTH2R_rna = c(0.693147180559945, 
    5.08759633523238, 0.693147180559945, 1.09861228866811, 0, 6.01126717440416, 
    6.56526497003536, 5.18178355029209, 0, 4.36944785246702, 2.19722457733622, 
    1.09861228866811, 3.49650756146648, 1.38629436111989, 5.93753620508243
    ), HAVCR2_rna = c(4.48863636973214, 3.40119738166216, 3.09104245335832, 
    2.94443897916644, 3.2188758248682, 3.76120011569356, 3.95124371858143, 
    2.83321334405622, 2.07944154167984, 4.36944785246702, 3.58351893845611, 
    1.94591014905531, 4.23410650459726, 3.43398720448515, 2.56494935746154
    ), CD200R1_rna = c(2.484906649788, 2.94443897916644, 0.693147180559945, 
    1.94591014905531, 0.693147180559945, 2.89037175789616, 2.56494935746154, 
    1.6094379124341, 1.6094379124341, 1.94591014905531, 2.19722457733622, 
    0.693147180559945, 4.26267987704132, 1.6094379124341, 0.693147180559945
    )), .Names = c(""response"", ""HIST1H3F_rna"", ""NCF1_rna"", ""WDR66_rna"", 
    ""PTH2R_rna"", ""HAVCR2_rna"", ""CD200R1_rna""), row.names = c(NA, 
    -15L), class = ""data.frame"")
</code></pre>

<p>However, running the following lines and getting a summary of the model I find that all variables have a p-value of 1 and the standard errors seem so high. What's going on here?</p>

<pre><code>fullmod &lt;- glm(response ~ ., data=final_model,family='binomial')
summary(fullmod)
Call:
glm(formula = response ~ ., family = ""binomial"", data = final_model)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-6.515e-06  -2.404e-06  -2.110e-08   2.110e-08   7.470e-06  

Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   1.460e+02  5.598e+05       0        1
HIST1H3F_rna  2.135e+01  5.145e+05       0        1
NCF1_rna     -4.133e+01  3.388e+05       0        1
WDR66_rna     1.296e+01  6.739e+05       0        1
PTH2R_rna     1.975e+00  3.775e+05       0        1
HAVCR2_rna   -2.477e+01  1.191e+06       0        1
CD200R1_rna  -1.420e+01  1.315e+06       0        1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.0190e+01  on 14  degrees of freedom
Residual deviance: 2.2042e-10  on  8  degrees of freedom
AIC: 14

Number of Fisher Scoring iterations: 25
</code></pre>

<hr>

<p>In response to your comments I'll show the feature selection step (and the complete dataframe I'm working with below that).  </p>

<pre><code># forward  feature selection 
library('boot')
z = c()
nullmod &lt;- glm(response ~ 1, data=final_model, family='binomial') ## â€˜emptyâ€™ 
fullmod &lt;- glm(response ~ ., data=final_model, family='binomial') ## Full model
first = T
for(x in 1:ncol(final_model)){
  stepmod &lt;- step(nullmod, scope=list(lower=formula(nullmod), upper=formula(fullmod)),
                  direction=""forward"", data=final_model, steps=x, trace=F)
  cv.err  &lt;- cv.glm(data=final_model, glmfit=stepmod, K=nrow(final_model))$delta[1]
  if (first == T){
    first=F
    final_features &lt;- stepmod
  }else{
    if (cv.err &lt; min(z)){ final_features &lt;- stepmod }
  }
  z[x] &lt;- cv.err
  print(paste(x,cv.err))
  print(colnames(final_features$model))
}

plot(z, main='Forward Feature Selection GLM Final Model', 
     xlab='Number of Steps', ylab='LOOCV-error', col='red', type='l')
points(z)
colnames(final_features$model)
summary(final_features)

structure(list(response = c(0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 
0L, 1L, 0L, 1L, 1L, 1L), HIST1H3F_rna = c(1.09861228866811, 2.07944154167984, 
1.09861228866811, 1.79175946922805, 0, 0, 0, 2.39789527279837, 
1.38629436111989, 1.6094379124341, 1.6094379124341, 0.693147180559945, 
2.19722457733622, 2.39789527279837, 2.89037175789616), NCF1_rna = c(2.77258872223978, 
2.63905732961526, 2.19722457733622, 2.30258509299405, 2.56494935746154, 
3.09104245335832, 3.98898404656427, 2.56494935746154, 4.06044301054642, 
3.87120101090789, 2.07944154167984, 3.49650756146648, 2.07944154167984, 
2.07944154167984, 1.09861228866811), WDR66_rna = c(5.06890420222023, 
5.11799381241676, 3.40119738166216, 3.25809653802148, 4.02535169073515, 
5.8348107370626, 5.89440283426485, 3.87120101090789, 5.67675380226828, 
5.35185813347607, 4.15888308335967, 6.23441072571837, 4.0943445622221, 
4.21950770517611, 3.95124371858143), PTH2R_rna = c(0.693147180559945, 
0.693147180559945, 1.09861228866811, 0, 6.01126717440416, 6.56526497003536, 
5.18178355029209, 0, 4.36944785246702, 2.19722457733622, 1.09861228866811, 
3.49650756146648, 0, 0.693147180559945, 1.38629436111989), 
HAVCR2_rna = c(4.48863636973214, 
3.09104245335832, 2.94443897916644, 3.2188758248682, 3.76120011569356, 
3.95124371858143, 2.83321334405622, 2.07944154167984, 4.36944785246702, 
3.58351893845611, 1.94591014905531, 4.23410650459726, 1.38629436111989, 
1.09861228866811, 1.38629436111989), CD200R1_rna = c(2.484906649788, 
0.693147180559945, 1.94591014905531, 0.693147180559945, 2.89037175789616, 
2.56494935746154, 1.6094379124341, 1.6094379124341, 1.94591014905531, 
2.19722457733622, 0.693147180559945, 4.26267987704132, 1.94591014905531, 
0, 0.693147180559945), GDF7 = c(0.2232, -0.7281, 0.0655, -0.7919, 
0.175, 0.0891, 0.4396, -0.2774, -0.4079, 0.4069, 0.3057, 0.7371, 
-0.4978, -0.5096, -0.0827), HS1BP3 = c(0.2232, -0.7281, 0.0655, 
-0.7919, 0.175, 0.0891, 0.4396, -0.2774, -0.4079, 0.4069, 0.3057, 
0.7371, -0.4978, -0.5096, -0.0827), NKAIN3 = c(0.4072, 0.3216, 
-0.5466, -0.1588, 0.4515, 0.2849, 0.1675, 0.0847, 0.6601, 0.6331, 
-0.135, 1.3532, -0.503, -0.1241, 0.2061), UG0898H09 = c(0.4072, 
0.3216, -0.5466, -0.1588, 0.4515, 0.2849, 0.1675, 0.0847, 0.6601, 
0.6331, -0.135, 1.3532, -0.503, -0.1241, 0.2061), C15orf41 = c(0.122, 
-0.7519, -1.1267, -0.7882, -0.1117, -0.5105, -0.3905, -0.6834, 
-0.5944, 0.0714, -0.8134, -0.0115, -1.1112, -1.1488, -0.4878), 
    FAM98B = c(-0.1871, -0.7519, -1.1267, -0.7882, -0.1117, -0.5105, 
    -0.3905, -0.6834, -0.5944, 0.0714, -0.8134, -0.0115, -1.1112, 
    -1.1488, -0.4878), SPRED1 = c(-0.1871, -0.7519, -1.1267, 
    -0.7882, -0.1117, -0.5105, -0.3905, -0.6834, -0.5944, 0.0714, 
    -0.8134, -0.0115, -1.1112, -1.1488, -0.4878), MPDZ_ex = c(1, 
    0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0), TPR_ex = c(0, 
    0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), BUB1B_ex = c(0, 
    0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0), APC_ex = c(0, 
    0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0), ATM_ex = c(0, 
    0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0), DYNC1LI1_ex = c(0, 
    0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0), TTK_ex = c(0, 
    0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0), PSMG2_ex = c(1, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), NegRegMitosis = c(1, 
    0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0), brca1ness = c(0.037719, 
    0.900878, 0.013261, 0.900878, 0.659963, 0.005629, 9.8e-05, 
    0.996336, 0.910072, 0.850776, 0.000613, 0.104428, 0.978114, 
    0.938767, 0.041696), Methylation = c(0L, 0L, 0L, 1L, 1L, 
    1L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L), LinoleicAcid_Metab = structure(c(2L, 
    2L, 2L, 2L, 1L, 3L, 2L, 2L, 1L, 5L, 2L, 5L, 1L, 2L, 2L), .Label = c(""CYP2E1_high"", 
    ""CYP2E1_med"", ""high"", ""low"", ""PLA2G2A_high""), class = ""factor""), 
    Neuro_lr = structure(c(2L, 2L, 1L, 1L, 3L, 3L, 3L, 1L, 3L, 
    1L, 1L, 3L, 3L, 1L, 1L), .Label = c(""1"", ""2"", ""3"", ""4""), class = ""factor""), 
    NOX_signalling = structure(c(2L, 2L, 2L, 2L, 1L, 2L, 1L, 
    2L, 1L, 1L, 2L, 1L, 1L, 2L, 2L), .Label = c(""high"", ""low""
    ), class = ""factor"")), .Names = c(""response"", ""HIST1H3F_rna"", 
""NCF1_rna"", ""WDR66_rna"", ""PTH2R_rna"", ""HAVCR2_rna"", ""CD200R1_rna"", 
""GDF7"", ""HS1BP3"", ""NKAIN3"", ""UG0898H09"", ""C15orf41"", ""FAM98B"", 
""SPRED1"", ""MPDZ_ex"", ""TPR_ex"", ""BUB1B_ex"", ""APC_ex"", ""ATM_ex"", 
""DYNC1LI1_ex"", ""TTK_ex"", ""PSMG2_ex"", ""NegRegMitosis"", ""brca1ness"", 
""Methylation"", ""LinoleicAcid_Metab"", ""Neuro_lr"", ""NOX_signalling""
), row.names = c(NA, -15L), class = ""data.frame"")
</code></pre>

<p>Summary now gives the following:</p>

<pre><code>Call:
glm(formula = response ~ NegRegMitosis, family = ""binomial"", 
    data = final_model)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-3.971e-06  -3.971e-06   3.971e-06   3.971e-06   3.971e-06  

Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)       25.57   76367.61       0        1
NegRegMitosis    -51.13  111790.71       0        1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.0728e+01  on 14  degrees of freedom
Residual deviance: 2.3655e-10  on 13  degrees of freedom
AIC: 4

Number of Fisher Scoring iterations: 24
</code></pre>

<p>Again even in a single predictor model, my p-value is 1. The predictor in this case is equal to the response, so it should predict perfectly. Then why is my pvalue 1?</p>
"
"0.0861860827177143","0.0821301562353182","183528","<p>I have three questions about the assumptions of the logistic regression:</p>

<ol>
<li><p>I read that the percentages of zeros and ones should be equal. If there's a data set where one of them is abundand, i.e. there are 80% zeros and 20% ones can I somehow put different weights in my glm? There's also the weight-function, but I don't understand what it's exactly for... </p></li>
<li><p>I didn't really get what the pseudo-coefficients of determination tell me - Do this, i.e. the Nagelkerke's index tell me something about the assumptions of my model, how much they are fullfilled or just how much my predicted model differ from the data points I've observed. </p></li>
<li><p>I also red for the assumptions that there should be at least 25 data points / group, what exactly is my group? When I i.e. have the mtcars-dataset in R </p>

<p>data(""mtcars"") </p></li>
</ol>

<p>and I want to look</p>

<pre><code>glm(vs ~ carp + disp, family = binomial) 
</code></pre>

<p>what are my groups? (maybe this is also a false point of view, but I'm really irritated...)</p>

<p>Best wishes </p>

<p>Marry</p>
"
"0.106649836183801","0.10889023202607","183699","<p>I encountered a strange phenomenon when calculating pseudo R2 for logistic models when using aggregated files: the results are simply too good to be true. An example (but as far as I can see, every aggregated file offers similar problems):</p>

<pre><code> library(pscl)
 cuse &lt;- read.table(""http://data.princeton.edu/wws509/datasets/cuse.dat"",
               header=TRUE)

 head(cuse)
 cuse.fit &lt;- glm( cbind(using, notUsing) ~ age + education + wantsMore, 
             family = binomial, data=cuse)

 summary(cuse.fit)
 pR2(cuse.fit)     
</code></pre>

<p>The results are:</p>

<pre><code>&gt; summary(cuse.fit)

Call:
glm(formula = cbind(using, notUsing) ~ age + education + wantsMore, 
family = binomial, data = cuse)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.5148  -0.9376   0.2408   0.9822   1.7333  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -0.8082     0.1590  -5.083 3.71e-07 ***
age25-29       0.3894     0.1759   2.214  0.02681 *  
age30-39       0.9086     0.1646   5.519 3.40e-08 ***
age40-49       1.1892     0.2144   5.546 2.92e-08 ***
educationlow  -0.3250     0.1240  -2.620  0.00879 ** 
wantsMoreyes  -0.8330     0.1175  -7.091 1.33e-12 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 165.772  on 15  degrees of freedom
Residual deviance:  29.917  on 10  degrees of freedom
AIC: 113.43

Number of Fisher Scoring iterations: 4

&gt; pR2(cuse.fit)
         llh      llhNull           G2     McFadden         r2ML 
 -50.7125647 -118.6401419  135.8551544    0.5725514    0.9997947 
       r2CU 
  0.9997950 
</code></pre>

<p>The last three outcomes from pscl function pR2 present McFadden's pseudo r-squared,  Maximum likelihood pseudo r-squared (Cox &amp; Snell) and Cragg and Uhler's or Nagelkerke's pseudo r-squared. The calculation seems to be flawless, but the outcomes close to 1 seem to good to be true.</p>

<p>Using weight instead of cbind:</p>

<pre><code>cuse2 = rbind(cuse,cuse)
cuse2$using.contraceptive=1
    cuse2$using.contraceptive[1:nrow(cuse)]=0
cuse2$freq = cuse2$notUsing
cuse2$freq[1:nrow(cuse)] = cuse2$using[1:nrow(cuse)]
cuse.fit2 = glm(using.contraceptive ~ age + education + wantsMore,
            weight=freq, family = binomial, data = cuse2)
summary(cuse.fit2)
round(pR2(cuse.fit2),5)
</code></pre>

<p>produces different logistic regression coefficients, and slightly different pseudo R2's for r2ml and r2CU and a large difference for McFadden R2:</p>

<pre><code>&gt; round(pR2(cuse.fit2),5)
         llh     llhNull          G2    McFadden        r2ML 
  -933.91920 -1001.84677   135.85515     0.06780     0.98567 
        r2CU 
     0.98567 
</code></pre>

<p>Full expansion results in very different estimates from pR2:</p>

<pre><code> cuse3 = rbind(cuse[rep(1:nrow(cuse), cuse[[""notUsing""]]), ],
          cuse[rep(1:nrow(cuse), cuse[[""using""]]), ])
 cuse3$using.contraceptive=1
     cuse3$using.contraceptive[1:sum(cuse$notUsing)]=0
 summary(cuse3)
 cuse.fit3 = glm(using.contraceptive ~ age + education + wantsMore,
            family = binomial, data = cuse3)
 summary(cuse.fit3)
 round(pR2(cuse.fit3),5)

 &gt; round(pR2(cuse.fit3),5)
         llh     llhNull          G2    McFadden        r2ML 
  -933.91920 -1001.84677   135.85515     0.06780     0.08106 
        r2CU 
     0.11376 
</code></pre>

<p>This indicates a logistic model which explains very little, which is a little bit more believable than the near perfect results from the aggregated files. Is there a more correct, and preferably more consistent, way to calculate the pseudo R2's? </p>
"
"0.0304713817668003","0","183845","<p>I'm going crazy, because I can't find a simple description how the coefficients are calculated in R statistics in the multivariable logistic regression (and I'm not a mathematician). 
Are they standardised? So i.e. when I have x ~ y1 + y2 and the coefficient for y1 = 0.2, is this the coefficient in the model when the parameter y2 is 0, the mean of y2 or somehow all the parameters of y2? </p>

<p>Sorry, I'm stuck on this simple question... </p>

<p>p.s.: I also have an interaction y1:y2 if this changes anything...</p>
"
"NaN","NaN","183854","<p>I want to estimate a vector-valued model</p>

<p>$$\mathbf{y}_t = a\mathbf{y}_{t-1}+b\mathbf{y}_{t-2}+\cdots$$</p>

<p>Here, each $\mathbf{y}_t\in\mathbb{R}^n$ and the coefficients $a,b,\dotsc$ are real numbers (unlike in VAR, where they are matrices). I wonder what the best way to do this in R is. Applying the function <code>ar</code> doesn't seem to work for me ([i] <strong>is this only for series that lie in $\mathbb{R}^1$</strong>?), and when I apply standard linear regression, I'm not sure how I should test for the order of the model, etc. [ii] <strong>Should I go for the $R^2$ value and significance of the coefficients?</strong></p>

<p>Also, I have my data in a matrix $Y$ where each column corresponds to one time point. Currently, I regress as follows </p>

<pre><code>Y = c(Y[,10], Y[,9],..., Y[,2])
Y.l1 = c(Y[,9],..., Y[,1])
fit = lm(Y ~ 0 + Y.l1)
</code></pre>

<p>[iii] <strong>Is there a more elegant way?</strong></p>
"
"0.091874672876503","0.0875510407188402","183908","<p>I have a binary logistic regression with just one binary fixed factor predictor. The reason I don't do it as a Chi square or Fisher's exact test is that I also have a number of random factors (there are multiple data points per individual and individuals are in groups, although I don't care about coefficients or significances for those random variables). I do this with R glmer.</p>

<p>I would like to be able to express the coefficient and associated confidence interval for the predictor as a risk ratio rather than an odds ratio. This is because (maybe not for you but for my audience) risk ratio is much easier to understand. Risk ratio here is the relative increase in chance of the outcome being 1 rather than 0 if the predictor is 1 rather than 0.</p>

<p>The odds ratio is trivial to get from the coefficient and associated CI using exp(). To convert an odds ratio to a risk ratio, you can use ""RR = OR / (1 â€“ p + (p x OR)), where p is the risk in the control group"" (source: <a href=""http://www.r-bloggers.com/how-to-convert-odds-ratios-to-relative-risks/"" rel=""nofollow"">http://www.r-bloggers.com/how-to-convert-odds-ratios-to-relative-risks/</a>). But, you need the risk in the control group, which in my case means the chance that the outcome is 1 if the predictor is 0. I believe the intercept coefficient from the model is in fact the odds for this chance, so I can use prob=odds/(odds+1) to get that. I'm pretty much so-far-so-good on this as far as the central estimate for the risk ratio goes. But what worries me is the associated confidence interval, because the intercept coefficient also has its own associated CI. Should I use the central estimate of the intercept, or to be conservative should I use whatever limits of the intercept CI make my relative risk CI widest? Or am I barking up the wrong tree entirely?</p>
"
"0.0430930413588572","0.0410650781176591","183926","<p>I'm doing a cox regression analysis. My model is given by</p>

<pre><code>final_model  = coxph(S ~ gearbox_model + cumGwh + manufac + turbine_model + 
                  gearbox_model:cumGwh + cumGwh:turbine_model + 
                  manufac:turbine_model, timelist),
</code></pre>

<p>where </p>

<pre><code>S = Surv(tdm$start, tdm$end, tdm$delta)
</code></pre>

<p>and timelist consists of my data. Now, from R I get the following (which is just the weird part of the analysis. All the other coefficients are estimated by R).</p>

<pre><code>Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

                                 exp(coef) exp(-coef) lower .95 upper .95
manufacManufacturer2:turbine_model12        NA         NA        NA        NA
manufacManufacturer3:turbine_model12        NA         NA        NA        NA
</code></pre>

<p>I've read somewhere that I should consider collinearity, but I dont know how. Also, I've read that it could be due to not having enough data. For example, there is no events/deaths/ (delta = 1) for the combination of turbine_model =3 and Manufacturer 3.</p>

<pre><code>, , turbine_model = 3
              delta
  Manufacturer1   2
  Manufacturer2   0
  Manufacturer3   0
  Manufacturer4  15
</code></pre>

<p>But I'm kinda clueless how to proceed.</p>
"
"0.068136080998913","0.0649295895722714","184137","<p>I would like to get the predicted values (with confidence intervals) for a multinomial logistic regression. I know this could be done with predict but in my case I have clustered standard errors in the following way:</p>

<pre><code>multinom &lt;- mlogit(Y ~0| X1+ X2 , data)
cl.mlogit   &lt;- function(fm, cluster){
  M &lt;- length(unique(cluster))
  N &lt;- length(cluster)
  K &lt;- length(coefficients(fm))
  dfc &lt;- (M/(M-1))
  uj  &lt;- apply(estfun(fm),2, function(x) tapply(x, cluster, sum));
  vcovCL &lt;- dfc*sandwich(fm, meat.=crossprod(uj)/N)
 coeftest(fm, vcovCL) 
}
cl.mlogit(multinom, data$group)
</code></pre>

<p>How I could use these results to get the predicted probabilities (with confidence intervals) for X1=1 and X2=0 for example and compare it with predicted probalities for X1=2 and X2=0. </p>

<p>Also, how could I get a confidence interval for that difference? In Stata prvalue do this last thing by using the delta method to get the confidence interval <a href=""http://www.indiana.edu/~jslsoc/stata/ci_computations/xulong-prvalue-23aug2005.pdf"" rel=""nofollow"">http://www.indiana.edu/~jslsoc/stata/ci_computations/xulong-prvalue-23aug2005.pdf</a>. Is there an easy way to do it in R?</p>
"
"0.0545088647991304","0.0649295895722714","184341","<p>I am using the Deming function provided by Terry T. on <a href=""http://www.mail-archive.com/r-help@r-project.org/msg85070.html"">this archived r-help thread</a>.  I am comparing two methods, so I have data that look like this:</p>

<pre><code>y  x     stdy   stdx
1  1.2   0.23   0.67
2  1.8   0.05   0.89
4  7.5   1.13   0.44
... ...  ...   ...
</code></pre>

<p>I have done my Deming regression (also called ""total least squares regression"") and I get a slope and intercept. I would like to get a correlation coefficient so I've start calculating the $R^2$. I have manually entered the formula: </p>

<pre><code>R2 &lt;- function(coef,i,x,y,sdty){
    predy    &lt;- (coef*x)+i
    stdyl    &lt;- sum((y-predy)^2)   ### The calculated std like if it was a lm (SSres)
    Reelstdy &lt;- sum(stdy)          ### the real stdy from the data  (SSres real)
    disty    &lt;- sum((y-mean(y))^2) ### SS tot
    R2       &lt;- 1-(stdyl/disty)    ### R2 formula
    R2avecstdyconnu &lt;- 1-(Reelstdy/disty) ### R2 with the known stdy
    return(data.frame(R2, R2avecstdyconnu, stdy, Reelstdy))
}
</code></pre>

<p>This formula works and gives me output.</p>

<ul>
<li>Which of the two $R^2$s makes more sense? (I personally think of both of them as kind of biased.)  </li>
<li>Is there a way to get a correlation coefficient from a total least squared regression?</li>
</ul>

<p>OUTPUT FROM THE DEMING REGRESSION:</p>

<pre><code>Call:
deming(x = Data$DS, y = Data$DM, xstd = Data$SES, ystd = Data$SEM,     dfbeta = T)

               Coef  se(coef)         z            p
Intercept 0.3874572 0.2249302 3.1004680 2.806415e-10
Slope     1.2546922 0.1140142 0.8450883 4.549709e-02

   Scale= 0.7906686 
&gt; 
</code></pre>
"
"0.0806196982594614","0.076825726438694","184699","<p>I have a question regarding p-values in the linear regression.The purpose of using the linear regression model is mainly to predict future values with accuracy. As I read, when the purpose is prediction, I can somehow not may too much attention to multicollinearity and the assumption of the linear model.
Some of my predictors are categorical variables: can I include the levels having non-significant p-values when I am only interested in prediction?</p>

<p>The output is as follows:</p>

<pre><code>lm(formula = log(cost1) ~ log(cost2) + program + location+ month + type, data=data)
Residuals:
Min       1Q   Median       3Q      Max
-0.88768 -0.10647  0.00169  0.09248  0.91612
Coefficients:
                          Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)              0.1526869  0.1186113   1.287  0.19858
log(cost2)               0.9812236  0.0072015 136.253  &lt; 2e-16 ***
program1                -0.0055709  0.0475793  -0.117  0.90684 
program2                -0.0007374  0.0593048  -0.012  0.99008  
program3                 0.0531385  0.0734250   0.724  0.46958 
program4                 0.0712944  0.0472402   1.509  0.13188
locationA                0.0210172  0.0319844   0.657  0.51141
locationB                0.0415091  0.0298623   1.390  0.16514
locationC                0.0898111  0.0316606   2.837  0.00474 ** 
month02                 -0.0631733  0.0454815  -1.389  0.16545  
month03                  0.0195483  0.0449924   0.434  0.66412 
month04                  0.0037596  0.0446384   0.084  0.93291
month05                  0.0387446  0.0422586   0.917  0.35966    
month06                  0.0899078  0.0494497   1.818  0.06963 .
month07                  0.0974763  0.0459993   2.119  0.03457 * 
month08                  0.0351214  0.0472294   0.744  0.45744 
month09                  0.0652653  0.0629235   1.037  0.30013
month10                 -0.5510485  0.1986461  -2.774  0.00574 ** 
TypeI                   -0.0815081  0.0821450  -0.992  0.32155
TypeII                   0.0340512  0.0436612   0.780  0.43582 
TypeIII                  0.0703337  0.0268013   2.624  0.00895 **
TypeIV                  -0.0658808  0.0411735  -1.600  0.11021   
TypeV                    0.1327603  0.0331560   4.004 7.16e-05 ***
TypeVI                   0.0994576  0.0264572   3.759  0.00019 ***
</code></pre>

<p>This model gave me the best prediction among other combination of predictors. Can I still use this model even though not all p-values are significant or no?</p>

<p>I would greatly appreciate your help!
Thank you</p>
"
"0.0545088647991304","0.0519436716578171","184944","<p>My model produces some results, but seems to drop several factors for some reason. This then leads (I believe) to the error I get when trying to run prediction with the model on the test data-set.</p>

<p>Note that SUBJECT should contain around 18 unique departments, but the regression only seems to look at 8 of them.</p>

<p>Data pulled from <a href=""https://data.cityofboston.gov/City-Services/311-Service-Requests/awu8-dc52"" rel=""nofollow"">here</a> with the datediffs created as the difference between CLOSED_DT and TARGET_DT and categorical variables created for time of day submitted using OPEN_DT and categorical month variables created out of OPEN_DT.</p>

<hr>

<pre><code>### TRAINING DATASET: running regression on close date compared to target
LM.train = lm(train$datediffs ~ as.factor(train$SUBJECT)+
              as.factor(train$daytime) + as.factor(train$month))
summary(LM.train)

Call:
lm(formula = train$datediffs ~ as.factor(train$SUBJECT) + as.factor(train$daytime) + 
        as.factor(train$month))

Residuals:
     Min       1Q   Median       3Q      Max 
-17934.2     -1.4     13.5     25.2   1497.3 

Coefficients:
                                                           Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                                                136.1612    12.2502  11.115  &lt; 2e-16 ***
as.factor(train$SUBJECT)Boston Water &amp; Sewer Commission   -114.4284   121.0248  -0.945  0.34441    
as.factor(train$SUBJECT)Civil Rights                        39.9356    59.0766   0.676  0.49904    
as.factor(train$SUBJECT)Inspectional Services             -134.6127    12.0019 -11.216  &lt; 2e-16 ***
as.factor(train$SUBJECT)Mayor's 24 Hour Hotline           -139.2901    13.8814 -10.034  &lt; 2e-16 ***
as.factor(train$SUBJECT)Parks &amp; Recreation Department     -404.6200    12.3268 -32.824  &lt; 2e-16 ***
as.factor(train$SUBJECT)Property Management               -174.4930    12.6617 -13.781  &lt; 2e-16 ***
as.factor(train$SUBJECT)Public Works Department           -153.2878    11.9246 -12.855  &lt; 2e-16 ***
as.factor(train$SUBJECT)Transportation - Traffic Division -142.7941    12.1351 -11.767  &lt; 2e-16 ***
as.factor(train$daytime)3pm to 7pm                           5.2794     2.1009   2.513  0.01197 *  
as.factor(train$daytime)7pm to midnight                     -4.5244     2.8398  -1.593  0.11111    
as.factor(train$daytime)Midnight to 10am                    -0.5208     1.9152  -0.272  0.78569    
as.factor(train$month)2                                     10.4747     3.4356   3.049  0.00230 ** 
as.factor(train$month)3                                     -6.1202     3.9623  -1.545  0.12244    
as.factor(train$month)4                                     -9.2142     4.1532  -2.219  0.02652 *  
as.factor(train$month)5                                    -11.4799     4.0921  -2.805  0.00503 ** 
as.factor(train$month)6                                     -9.6011     4.1127  -2.335  0.01957 *  
as.factor(train$month)7                                      0.4905     3.8696   0.127  0.89914    
as.factor(train$month)8                                      1.6532     3.7653   0.439  0.66061    
as.factor(train$month)9                                    -15.2938     3.7509  -4.077 4.56e-05 ***
as.factor(train$month)10                                   -23.4418     4.0240  -5.826 5.70e-09 ***
as.factor(train$month)11                                    -4.3002     4.3591  -0.986  0.32389    
as.factor(train$month)12                                    -9.6494     4.3658  -2.210  0.02709 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 361.3 on 214420 degrees of freedom
  (85557 observations deleted due to missingness)
Multiple R-squared:  0.02863,   Adjusted R-squared:  0.02853 
F-statistic: 287.3 on 22 and 214420 DF,  p-value: &lt; 2.2e-16


###################
############### PREDICTION ##################
###################

predict(LM.train, newdata=test)
Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) : 
      factor as.factor(train$SUBJECT) has new levels Animal Control, Boston Police Department, City Hall Truck, Consumer Affairs &amp; Licensing, CRM Application, Disability Department, Neighborhood Services, Veterans, Women's Commission, Youthline
</code></pre>
"
"0.0609427635336005","0.0580747904139041","185141","<p>So I am using panel data that includes both categorical and numerical variables. I am trying to get a regression equation in R, the issue is that I am not getting any information on my dummy variables (CP, IP, UP). I am going to paste my code and my output... 
**> palldata &lt;- plm.data(alldata, indexes=3)</p>

<blockquote>
  <p>fixed &lt;- plm(Y~NX+I+C+G+CP+IP+UP, palldata, model=""within"")</p>
  
  <p>summary(fixed)</p>
</blockquote>

<p>Oneway (individual) effect Within Model</p>

<p>Call:
plm(formula = Y ~ NX + I + C + G + CP + IP + UP, data = palldata, 
    model = ""within"")</p>

<p>Balanced Panel: n=3, T=16, N=48</p>

<p>Residuals :
     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. 
-1.22e+11 -1.36e+10 -2.52e+09  0.00e+00  1.51e+10  1.84e+11</p>

<p>Coefficients :</p>

<p>Estimate Std. Error t-value  Pr(>|t|)  </p>

<p>NX  1.201395   0.092049 13.0517 3.443e-16 ***</p>

<p>I     1.119823   0.020110 55.6842 &lt; 2.2e-16 ***</p>

<p>C    1.228684   0.033998 36.1403 &lt; 2.2e-16 ***</p>

<p>G   -0.055998   0.053056 -1.0554    0.2974    </p>

<hr>

<p>Signif. codes:  0 â€˜<strong><em>â€™ 0.001 â€˜</strong>â€™ 0.01 â€˜</em>â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
Total Sum of Squares:    1.8266e+26
Residual Sum of Squares: 8.868e+22
R-Squared      :  0.99951 
      Adj. R-Squared :  0.85375 
F-statistic: 21102.8 on 4 and 41 DF, p-value: &lt; 2.22e-16**</p>

<p>I already coded my dummy variables as 0 or 1. There is variance within them so that isn't the issue. What can I do to get information on my dummy variables? 
Thank you so much for your help.</p>
"
"0.0430930413588572","0.0410650781176591","185166","<p>I thought I've understood the output of the logistic regression in R (also I learned a lot through stackexchange), but somehow my vizualization tells me something different.
The output of the glm in R is: </p>

<pre><code>Call:
glm(formula = Zustand ~ Temp + Tag + Gehege + Temp:Gehege + Tag:Gehege + 
    Individuum + Gehege:Individuum + Temp:Individuum + Tag:Individuum, 
    family = binomial)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.9257  -0.5462  -0.4408  -0.3106   2.8510  

Coefficients:
                               Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                   -3.124244   1.302784  -2.398 0.016479 *  
Temp                           0.083585   0.068098   1.227 0.219664    
Tag                           -0.005485   0.013584  -0.404 0.686364    
Gehegeneues                    3.646637   1.249182   2.919 0.003509 ** 
IndividuumJoachim             -0.769787   0.927953  -0.830 0.406791    
IndividuumMary                -0.420745   0.797966  -0.527 0.598005    
Temp:Gehegeneues              -0.169516   0.062047  -2.732 0.006294 ** 
Tag:Gehegeneues               -0.057955   0.017154  -3.379 0.000729 ***
Gehegeneues:IndividuumJoachim  0.728588   0.329791   2.209 0.027158 *  
Gehegeneues:IndividuumMary     0.951688   0.396865   2.398 0.016484 *  
Temp:IndividuumJoachim         0.015466   0.049143   0.315 0.752979    
Temp:IndividuumMary           -0.012944   0.044467  -0.291 0.770985    
Tag:IndividuumJoachim         -0.035718   0.019177  -1.863 0.062532 .  
Tag:IndividuumMary             0.016538   0.019597   0.844 0.398724  
</code></pre>

<p>But my vizualization with the visreg-package...</p>

<pre><code>visreg(Ergebnis3, ""Gehege"", by = ""Individuum"", type = ""contrast"", scale = 'response', rug = F, ylim = c(0.0,0.6), main = ""Preening / Moulting"", xlab = ""Enclosure"", ylab = ""Likelihood"")
</code></pre>

<p>...looks like this:</p>

<p><a href=""http://i.stack.imgur.com/YxQc4.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YxQc4.jpg"" alt=""enter image description here""></a></p>

<p>""Individuum = Georg"" and ""Gehege = altes"" is my reference level and I thought the coefficient of ""Gehege = neues"" (3.64) means that the probability of ""Zustand"" in ""Gehege = neues"" is 3.64 times higher than in ""Gehege = altes"" or not? But in the graphic it's lower. Also for changing continuous variables ""Temp"" and ""Tag"" (here it's shown i.e. for 19.5 Â°C) ""Gehege = neues"" keeps to be lower for most of the times. The odds Ratio for ""Gehege = neues"" are also about 38, this high number is a bit weird... </p>

<p>I hope this is enough information to help, I have a real complex question I want to answer.</p>
"
"0.149398240610625","0.136672841339219","185583","<p>If the interaction happens between a <code>continuous</code> and a <code>discrete</code> variable it is (if I'm not mistaken) relatively straightforward. The mathematical expression is:</p>

<p>$\hat Y=\hatÎ²_0+\hatÎ²_1X_1+\hatÎ²_2X_2+\hatÎ²_3X_1âˆ—X_2+\epsilon$</p>

<p>So if we take my favorite dataset <code>mtcars{datasets}</code> in R, and we carry out the following regression:</p>

<pre><code>(fit &lt;- lm(mpg ~ wt * am, mtcars))

Call:
lm(formula = mpg ~ wt * am, data = mtcars)

Coefficients:
(Intercept)           wt           am        wt:am  
     31.416       -3.786       14.878       -5.298  
</code></pre>

<p><code>am</code>, which dummy-codes for the type of transmission in the car <code>am Transmission (0 = automatic, 1 = manual)</code> will give us an intercept of <code>31.416</code> for <code>manual</code> (<code>0</code>), and <code>31.416 + 14.878 = 46.294</code> for <code>automatic</code> (<code>1</code>). The slope for weight is <code>-3.786</code>. And for the interaction, when <code>am</code> is <code>1</code> (automatic), the regression expression will have the added term, $-5.298*1*\text {weight}$, which will add to $-3.786*\text {weight}$, resulting in a slope of $-9.084*\text {weight}$. So <strong><em>we are changing the slope with the interaction.</em></strong></p>

<p>But when it is two <code>continuous</code> variables that are interacting, <strong><em>are we really creating an infinite number of slopes? How do express the output without corny  sentences like ""the slope we would get with cars that weight $0\,\text{lbs.}$, or $1\,\text{lb.}$?</em></strong> For example, take the explanatory variables <code>wt</code> (weight) and <code>hp</code> (horsepower) and the regressand <code>mpg</code> (miles per gallon):</p>

<pre><code>(fit &lt;- lm(mpg ~ wt * hp, mtcars))

Call:
lm(formula = mpg ~ wt * hp, data = mtcars)

Coefficients:
(Intercept)           wt           hp        wt:hp  
   49.80842     -8.21662     -0.12010      0.02785
</code></pre>

<p><strong><em>How do we read the output?</em></strong> There seems to be one single intercept <code>49.80842</code>, whereas it would make sense to have two different intercepts to give flexibility to the fit, as in the prior scenario (<strong><em>what am I missing?</em></strong>). We have a slope for <code>wt</code> and a slope for hp (<code>-8.21662 -0.12010 = -8.33672</code>, <strong><em>is that right?</em></strong>). And finally the more intriguing <code>0.02785</code>. So, yes, are we constrained to expressing this with absurd scenarios, such as <strong><em>if we had cars with $1\text{hp}$ we would have a modified slope for the weight equal to $(-8.21662 + 0.02785)*1*\text{weight}$?</em></strong> Or is there a more sensible way to look at this term?</p>

<p><strong>SOLUTION:</strong></p>

<p>[Quick note, safe to skip: I really appreciate the answers and help provided, and will accept - it is rather difficult with such outstanding Answers, though. So please don't take this edit as anything more than a way of sharing what I've been doing for a little while this morning: basically hacking away at the R coefficients until I got what I wanted because despite the generous help provided I still couldn't ""see"" how one of the coeff's worked. Also, all this pre-emption will be erased shortly.]</p>

<p>We can ""prove"" how these coefficients ""work"" by simply taking the first values of <code>mpg</code>, <code>wt</code> and <code>hp</code>, which happen to be for the glamorous Mazda RX4:</p>

<p><img src=""http://i.stack.imgur.com/Gunpa.jpg"" width=""300"" height=""150""></p>

<p><a href=""http://www.classicandperformancecar.com"" rel=""nofollow"">credit here</a></p>

<p>These are:</p>

<pre><code>          mpg cyl disp  hp drat   wt  qsec vs am gear carb
Mazda RX4  21   6  160 110  3.9 2.62 16.46  0  1    4    4
</code></pre>

<p>And simply run <code>predict(fit)[1] Mazda RX4</code>, which returns a $\hat y$ value of  $23.09547$. No matter what, I have to rearrange the coefficient to get this number - all possible permutations if necessary! No just kidding. Here it is:</p>

<p><code>coef(fit)[1] + (coef(fit)[2] * mtcars$wt[1]) + (coef(fit)[3] * mtcars$hp[1]) 
+ (coef(fit)[4] * mtcars$wt[1] * mtcars$hp[1])</code> $= 23.09547$. </p>

<p>The math expression is:</p>

<p>$\small \hat Y=\hat Î²_0 (=1^{st}\,\text{coef})\,+\,\hatÎ²_1 (=2^{nd}\,\text{coef})\,*wt \,+\, \hatÎ²_2 (=3^{rd}\,\text{coef})\,*hp \,+\, [\hatÎ²_3(=4^{th}\,\text{coef})\, *wt\,âˆ—\,hp]$</p>

<p>So, as pointed out in the answers, there is only one intercept (the first coefficient), but there are two <em>""private""</em> slopes: one for each explanatory variable, <em>plus</em> one <em>""shared""</em> slope. This shared slope allows obtaining uncountably infinite slopes if we ""zip"" through $\mathbb{R}$ for all the theoretically possible realizations of one of the variables, and at any point we combine ($+$) the <em>""shared""</em> coefficient <em>times</em> the remaining random variable (e.g. for <code>hp = 100</code>, it would be <code>0.02785 * 100 * wt</code>) with its <em>""private</em>"" slope (<code>-8.21662 * wt</code>). I wonder if I can call it a <em>convolution</em>...</p>

<p>We can also see that this is the right interpretation running: </p>

<pre><code>y &lt;- coef(fit)[1] + (coef(fit)[2] * mtcars$wt[1]) + (coef(fit)[3] * mtcars$hp[1]) + (coef(fit)[4] * mtcars$wt[1] * mtcars$hp[1])
identical(as.numeric(predict(fit)[1]), as.numeric(y)) TRUE
</code></pre>

<p>Having rediscovered the wheel we see that the ""shared"" coefficient is positive (0.02785), leaving one loose end, now, which is the explanation as to why the weight of the vehicle as a predictor for ""gas-guzzliness"" is buffered for higher horse-powered cars... We can see this effect (thank you @Glen_b for the tip) with the $3\,D$ plot of the predicted values in this regression model, which conforms to the following <em>parabolic hyperboloid</em>:</p>

<p><a href=""http://i.stack.imgur.com/VeVpK.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/VeVpK.png"" alt=""enter image description here""></a></p>
"
"0.0609427635336005","0.0580747904139041","186283","<p>I am trying to run a log log regression of the form LNX ~ LNP, where X = Units Sold and P = Price (in reality, I would have a number of other variables included in the model).  The data is retail data; ultimately, I want to run the exact same regression above for each individual item.  so, LNX ~ LNP for Item # 1, LNX ~ LNP for Item #2, and so forth.</p>

<p>I have tried a number of methods, but for each method, the intercept and coefficient are coming out the same for all items (ie, intercept for item # 1 = intercept for item # 2 and so on).  This is clearly incorrect and is an artifact of my being a novice to R.</p>

<p>Below is a toy data set that is similar in nature to a real data set I'm working with.  </p>

<p><a href=""http://i.stack.imgur.com/ZAf2m.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZAf2m.png"" alt=""enter image description here""></a></p>

<p>Below is one of the codes that I've tried.</p>

<pre><code>T&lt;-read.csv(""ElasticityToy.csv"")
X = log(T$Units)
    P = log (T$Price)
model&lt;-lapply(1:2, function (i) lm(formula = X ~ P, data = T))
</code></pre>

<p>This, again, gives the exact same intercept and coefficient for each both item # 1 and item # 2.  There is the added difficulty that this is panel data, so any input here is appreciated as well.</p>

<p>Ultimately, I'm looking to elasticity for each item (impossible given the simple form I've presented here, I know, but I'm looking more for code help at this point as I'm new to R).</p>

<p>I appreciate any help you can give.  Mods, if this is the incorrect place to post this, can you please point me in the correct direction?</p>
"
"NaN","NaN","186309","<p>This is my exam preparation, which will be held completely in R. Teacher said that similar tasks will be there with very limited amount of time given for solving it. Here I need to recover linear regression values (i.e., where are a-i missing values) having only this R output. I don't have the input data so I can't just copy the command and reproduce the result.</p>

<p><a href=""http://i.stack.imgur.com/4SbUj.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4SbUj.png"" alt=""Given coefficient is R output. I don&#39;t have the input data.""></a></p>

<p>My question is how can I make it using R? Are there any specific commands? Or could it be done only manually? If it is possible only by manual calculation than please explain how to do it more effectively.</p>

<p>Please, describe it in detail.</p>

<p>Thank you so much for your help and deep explanation.</p>
"
"0.101414888671788","0.104695817324578","186725","<p>Short version: How would one be able to quantify an intervention effect in time-series analysis when the intervention decreases seasonal amplitude variation but doesn't directly effect the median?</p>

<p><a href=""https://www.dropbox.com/s/hb3g7j17igeqnoc/dat.csv?dl=0"" rel=""nofollow"">Here</a> is a link to my raw data.</p>

<p>I have a complex time-series of daily incidence numbers for a population over 7 years, totaling 2557 observations. There is a strong weekly and yearly seasonality (high incidence in winter months and low incidence in summer months). There is a baseline negative trend which is orders of magnitude smaller than the seasonality. An intervention was introduced at time = 1700. This intervention should theoretically not cause a level shift. My aim is to detect whether the intervention increases the baseline negative trend.</p>

<p>I have attempted to fit a dynamic linear regression with ARIMA errors in R using <code>auto.arima()</code> in the <code>forecast</code> package. I modeled the weekly season using a dummy variable for each weekday and the weekend. I modeled the monthly seasonality with harmonics using <code>fourier()</code> function in the <code>forecast</code> package. An the intervention effect was coded in by specifying the time index and post-intervention times as independent variables using the methods described in <a href=""http://isites.harvard.edu/fs/docs/icb.topic79832.files/L06_Program_Evaluation_2/Segmented_Regression.Wagner.2002.pdf"" rel=""nofollow"">Segmented regression analysis of interrupted time series studies in medication use research</a>. With these variables specified <code>auto.arima()</code> suggests an ARMA(7,7) process. The coefficients for baseline trend and post-intervention trend are however non-significant.  </p>

<p>I am concerned that by using fourier terms to model away the seasonality I am artificially removing any intervention effect, as visual analysis of the time series indicates that the intervention is specifically decreasing incidence during the winter months and therefore reducing the yearly seasonal variability. </p>
"
"0.105555962793852","0.0922061136661462","186845","<p>I created some data using the following code:</p>

<pre><code>set.seed(1221)
x &lt;- runif(500)
y &lt;- runif(500,0,2)
z &lt;- rep(0,500)
z[-0.8*x + y - 0.75 &gt; 0] &lt;- 1
plot(x,y,col=as.factor(z))
</code></pre>

<p>This produces the following plot</p>

<p><a href=""http://i.stack.imgur.com/ycWdr.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ycWdr.png"" alt=""enter image description here""></a></p>

<p>The data is linearly separable. Then, I applied the glm function to create a logistic regression model.</p>

<pre><code>df &lt;- data.frame(class = z, x = x, y = y)
model &lt;- glm(z ~ x + y, family = binomial, data = df)
</code></pre>

<p>This produces the following output:</p>

<pre><code>summary(model)
Call:
glm(formula = z ~ x + y, family = binomial, data = df)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-8.127e-04  -2.000e-08  -2.000e-08   2.000e-08   7.699e-04  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)    -1062      52666   -0.02    0.984
x              -1163      57197   -0.02    0.984
y               1433      70408    0.02    0.984

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 6.8274e+02  on 499  degrees of freedom
Residual deviance: 1.3345e-06  on 497  degrees of freedom
AIC: 6

Number of Fisher Scoring iterations: 25
</code></pre>

<p>The result surprised me, first because the parameter estimates are huge, and second because I was expecting such estimates to be close to the original decision boundary function, i.e. <code>-0.8x + y - 0.75 = 0</code>.</p>

<p>I then used the <a href=""http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html"" rel=""nofollow"">glmnet</a> package to see if I could solve this issue. This package creates a penalised logistic regression model in order to deal with the large values in the parameter estimates. The code I used is the following:</p>

<pre><code>library(glmnet)
cvfit &lt;- cv.glmnet(as.matrix(df[,-1]), as.factor(df$class), family =   ""binomial"", type.measure = ""class"")
plot(cvfit)
</code></pre>

<p><a href=""http://i.stack.imgur.com/vH4AV.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/vH4AV.png"" alt=""enter image description here""></a></p>

<p>And the coefficients for the optimal penalty strength are:</p>

<pre><code>coef(cvfit, s = ""lambda.min"")
3 x 1 sparse Matrix of class ""dgCMatrix""
                    1
(Intercept) -84.01446
x           -91.40983
y           113.18736
</code></pre>

<p>Such coefficients are smaller than the ones obtained with the <code>glm</code> function. Still they are not the same as the decision boundary function. </p>

<p>Does anybody know why this is happening? Any help is greatly appreciated.</p>
"
"0.146135613236117","0.13925845528839","187100","<p>I have a certain knowledge in stochastic processes (specially analysis of nonstationary signals), but in addition to be a beginner in R, I have never worked with regression models before.
Well, I have some doubts on understanding the outcome of the function summary() in R, when using with the results of a glm model fitted to my data. Well, suppose I used the following command to fit a generalized linear model to my data:**</p>

<pre><code>glm_model &lt;- glm(Output ~ (Input1*Input2) + Input3 + Input4, data = mydata)
</code></pre>

<p>Then I use summary(glm_model) to obtain the following:</p>

<pre><code>Call: 
glm(formula = Output ~ (Input1*Input2) + Input3 + Input4, data = mydata)
Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-7.4583  -0.8985   0.1628   1.0670   6.0673  
Coefficients:

Estimate Std. Error t value Pr(&gt;|t|)    

(Intercept)        8.522e+00  6.553e-02 130.041  &lt; 2e-16 ***

Input1            -3.819e-04  3.021e-05 -12.642  &lt; 2e-16 ***

Input2            -2.557e-04  2.518e-05 -10.156  &lt; 2e-16 ***

Input3            -3.202e-02  1.102e-02  -2.906  0.00367 ** 

Input4            -1.268e-01  7.608e-02  -1.666  0.09570 .  

Input1:Input2      1.525e-08  2.521e-09   6.051 1.53e-09 ***


Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for gaussian family taken to be 2.487504)
    Null deviance: 18544  on 5959  degrees of freedom
Residual deviance: 14811  on 5954  degrees of freedom
  (1708 observations deleted due to missingness)
AIC: 22353
Number of Fisher Scoring iterations: 2
</code></pre>

<p>From a estimation theory perspective, I understand that ""estimate"" and ""Std. Error"" are the estimates and the standard deviation of the unknown parameters (beta1, beta2,...) of my model. However, there are some things I do not understand:</p>

<p>1) How can I assess how good my fit is from the output of <code>summary()</code>? We could not use only the information of the standard deviation of the parameter estimators to assess the goodness-of-fit. I would expect to have access to the sampling distribution of a given parameter estimator to know the % of estimates within +- 1std, +-0.5std or any +-x*std, for example. Other option would be knowing the theoretical distribution of the parameter estimator, so as to try to calculate its Cramer Rao Lower Bound and compare with the calculated std.</p>

<p>2) What does the t value (or Pr(>|t|) ) have to do with the goodness-of-fit? Since I am not familiar with regression models, I do not know the connection between the student t distribution and the estimation of the model parameters. What does it mean? Is the parameter estimator of the glm model distributed according to the student t pdf (like the sample estimator for small samples of an unknown population)? What conclusions should I take from Pr(>|t|)?</p>

<p>3) Do we have a more general form of assessing the goodness-of-fit, like a measure of the variability of the data my model can capture, maybe a table of critical values for such a measure given a certain significance level?** </p>

<p>4) When fitting a glm model, do we need to specify a significance level? If yes, why such an information is not provided by the summary function?</p>

<p>5) The summary function outputs some measures based on information theory, like AIC: 22353. Can we define an optimal reference value for AIC? What is a good AIC value? My intuition is that we could not do so, like other information theory measures (mutual information, entropym,...)</p>

<p>Thank you for your help!</p>
"
"0.0806196982594614","0.076825726438694","187181","<p>For a linear regression model I tried on a dataset, when I fitted OLS, the output is as follows:</p>

<pre><code>fit = lm(price ~., data = art)

# Coefficients:
#                                  Estimate Std. Error t value Pr(&gt;|t|)    
# (Intercept)                     2.326e+03  8.863e+02   2.625 0.008777 ** 
# temp_stagelate                  3.029e-01  8.735e-02   3.467 0.000544 ***
# temp_stagemature                4.009e-01  1.154e-01   3.473 0.000533 ***
# temp_stagemiddle                5.346e-01  8.646e-02   6.184 8.55e-10 ***
# temp_stagena                    1.766e-01  8.645e-02   2.042 0.041322 *  
# tonedark                       -4.306e-01  5.511e-02  -7.814 1.20e-14 ***
# tonelight                      -4.267e-01  6.214e-02  -6.866 1.05e-11 ***
# subjectflower-animal           -3.997e-01  6.972e-02  -5.734 1.24e-08 ***
# subjectlandscape                1.609e-02  6.429e-02   0.250 0.802480    
# size.square                     2.454e-04  1.696e-05  14.469  &lt; 2e-16 ***
# size.sqq                       -6.205e-09  9.412e-10  -6.593 6.44e-11 ***
# coloringink and color           3.665e-01  5.522e-02   6.637 4.81e-11 ***
# further.inscribed.or.signedyes  2.997e-01  7.868e-02   3.810 0.000146 ***
# signedyes                       9.487e-01  2.058e-01   4.610 4.45e-06 ***
# inscribedyes                    1.865e-01  5.966e-02   3.126 0.001812 **
#   
# Residual standard error: 0.6806 on 1511 degrees of freedom
# Multiple R-squared:  0.726,    Adjusted R-squared:  0.7186 
</code></pre>

<p>and when I tried to fit a weighted least squares (WLS) model, </p>

<pre><code>gls(price ~. , data=art, weights = varFixed(~size.square)) 
</code></pre>

<p>the output is of the following:</p>

<pre><code># Standardized residuals:
#     Min          Q1         Med          Q3         Max 
# -4.27128213 -0.58938641 -0.06659419  0.53758125  6.03938177 
# 
# Coefficients:
#                                    Value Std.Error    t-value p-value
# (Intercept)                    1851.4155  924.1690   2.003330  0.0454
# temp_stagelate                    0.3717    0.1020   3.645983  0.0003
# temp_stagemature                  0.6992    0.1257   5.563261  0.0000
# temp_stagemiddle                  0.4881    0.1032   4.729729  0.0000
# temp_stagena                      0.2973    0.0978   3.038328  0.0024
# tonedark                         -0.4489    0.0571  -7.867553  0.0000
# tonelight                        -0.4451    0.0627  -7.093028  0.0000
# subjectflower-animal             -0.3605    0.0716  -5.036949  0.0000
# subjectlandscape                  0.0625    0.0669   0.934295  0.3503
# size.square                       0.0003    0.0000  13.128687  0.0000
# size.sqq                          0.0000    0.0000  -5.179399  0.0000
# coloringink and color             0.4053    0.0560   7.235716  0.0000
# further.inscribed.or.signedyes    0.3425    0.0866   3.955705  0.0001
# signedyes                         0.8654    0.2979   2.905369  0.0037
# inscribedyes                      0.3211    0.0574   5.593102  0.0000
# 
# Residual standard error: 0.01437576 
# Degrees of freedom:  1511 residual
</code></pre>

<p>I get a much smaller residual standard error. I am wondering if the two residual errors I have are comparable, and if the smaller residual standard error in the WLS model indicates that the WLS yields a better fit? </p>

<p>Does a smaller residual standard error in general indicate a better fit?</p>
"
"0.105555962793852","0.100588487635796","187487","<p>Let's say we have data that looks like this:</p>

<pre><code>set.seed(1)
b0 &lt;- 0 # intercept
b1 &lt;- 1 # slope
x &lt;- c(1:100) # predictor variable
y &lt;- b0 + b1*x + rnorm(n = 100, mean = 0, sd = 200) # predicted variable
</code></pre>

<p>We fit a simple linear model:</p>

<pre><code>mod.1 &lt;- lm(y~x) 
summary(mod.1) 
#             Estimate   Std. Error  t value  Pr(&gt;|t|)
# (Intercept) 26.3331    36.3795     0.724    0.471
# x           0.9098     0.6254      1.455    0.149 
b0.est &lt;- summary(mod.1)$coefficients[1,1]
b1.est &lt;- summary(mod.1)$coefficients[2,1]
</code></pre>

<p>And a model where we (1) subtract off the intercept term fit in the first model from the dataset and (2) prevent the intercept term from being fit (or in other words, force the model through zero):</p>

<pre><code>mod.2  &lt;- lm(y - b0.est  ~ 0 + x) 
summary(mod.2) 
#             Estimate   Std. Error t value   Pr(&gt;|t|)   
# x           0.9098     0.3088     2.946     0.00401 **
b1.est.2 &lt;- summary(mod.2)$coefficients[1,1]
</code></pre>

<p>As to be expected the slope parameter stays the same (0.9098).</p>

<p>However, while the slope parameter was not significant in the first model, it is in the second model (the standard error on the estimate in the second model is much lower than in the first model, 0.3088 vs. 0.6254).</p>

<p>The data is the same shape in both models with the same slope parameter being estimated by the two models. <strong>How is it the second model is so much more ""certain"" of the slope parameter estimate?</strong></p>

<p><strong>Or to put it another way, how are these standard errors calculated?</strong> </p>

<p>Using the equation for standard error I found <a href=""http://stattrek.com/regression/slope-test.aspx?Tutorial=AP"" rel=""nofollow"">here</a>, I calculated the standard errors for model 1 and 2 this way:</p>

<pre><code># Model 1
DF &lt;- length(x)-2 
y.est &lt;- b0.est + b1.est*x 
numerator &lt;- sqrt(sum((y - y.est)^2)/DF) 
denominator &lt;- sqrt(sum((x - mean(x))^2))
numerator/denominator 
# SE = 0.6254
</code></pre>

<p>This matches the R output.</p>

<pre><code># Model 2
DF &lt;- length(x)-1 
y.est &lt;- b1.est.2*x 
numerator &lt;- sqrt(sum((y - (y.est+b0.est))^2)/DF) 
denominator &lt;- sqrt(sum((x - mean(x))^2))
numerator/denominator 
# SE = 0.6223
</code></pre>

<p>This doesn't match the R output which has the SE = 0.3088. </p>

<p>What am I missing?</p>
"
"0.0430930413588572","0.0410650781176591","187509","<p>With a small book-exercise with four metric variables on 10 cases (one dependent/outcome, three independent/predictor) I ran <em>linear regression</em> in <code>SPSS</code> and <code>R</code>, and <em>ANOVA</em> (in <code>SPSS</code> declaring the predictors as ""covariates"").<br>
I found the output of the <em>SSQ</em> (Sum-of-Squares) different - and obviously from this also the F-test statistic and the p-values. Except from the last predictor the displayed values are different (the predictors in <code>R</code>may be reordered and the analysis be rerun to find all <code>SPSS</code>- coefficients).                   </p>

<p>By reengineering the computations in matrix-formulae I could reproduce the SPSS-values as well as the R-values and found, that <code>SPSS</code> uses the (partial) SSQ based on the logic of the ""usefulness""-coefficients for each predictor (which is sort of semipartial coefficient), while <code>R</code> simply uses the (hierarchically) partial SSQ. <em>(Unfortunately I'm not sure how to express that two approaches correctly so this toy-characterizing might be improved)</em> .             </p>

<p><strong><em>Q:</em></strong> Has that property of different focuses/philosophies been discussed anywhere? Is there some advantage of one over the other?   </p>

<p><hr>
Data: (taken from M. Backhaus et al., multivariate Verfahren)             </p>

<pre><code>predictors                   outcome-item
---------------------------+-------------
Preis   VerkFoer  Vertreter  Absatzmenge
12.50      2000      109      2298
10.00       550      107      1814
 9.95      1000       99      1647
11.50       800       70      1496
12.00         0       81       969
10.00      1500      102      1918
 8.00       800      110      1810
 9.00      1200       92      1896
 9.50      1100       87      1715
12.50      1300       79      1699
</code></pre>

<p>The comparision of the output:
<a href=""http://i.stack.imgur.com/zNJDB.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/zNJDB.png"" alt=""bild""></a></p>
"
"0.0609427635336005","0.0580747904139041","187658","<p>I ran a logistic regression in R using driving data from about 10,000 people. The model included age, years of driving experience, as well as 4 driving test results. The dependent variable was whether or not they had been involved in a crash recently (yes or no, a categorical variable). </p>

<p>The coefficients of the model are given below:</p>

<pre><code>                    Estimate     Std.Err       z value     Pr(&gt;|z|)    
(Intercept)        -1.450041     0.207144      -7.000      2.56e-12 ***
riding experience  -0.014115     0.003697      -3.818      0.000134 ***
age                -0.034544     0.003608      -9.575       &lt; 2e-16 ***
test 1              0.261485     0.088645       2.950      0.003180 ** 
test 2              0.090102     0.051328       1.755      0.079184 .  
test 3              0.228918     0.073666       3.108      0.001887 ** 
test 4              0.070106     0.063652       1.101      0.270729    
</code></pre>

<p>Firstly, with 10,000 people am I right in thinking that p-values aren't going to be that useful?</p>

<p>I calculated the probabilities of being involved in a crash with a 1 unit increase in each variable by doing <code>exp(variable)</code> to get the odds and then, <code>probability = odds/(1+odds)</code>. It gave me:</p>

<pre><code>(Intercept)        0.1899952          
ridingexp          0.4964712
age                0.4913648
test 1             0.5650012
test 2             0.5225104
test 3             0.5569810   
test 4             0.5175193
</code></pre>

<p>These seem awfully high! It is like saying that an increase in age of 1 year makes you 49% less likely to be involved in a crash? Surely that can't be right.</p>
"
"0.0609427635336005","0.0580747904139041","187796","<p>I've been trying to fit exactly the same logistic regression model (same data) in SAS and R. As far as the coefficients are concerned I didn't notice any differences. 
However, when I tried to perform some of the Goodness of fit tests (Pearson residuals and Deviance residuals GOF tests ) I noticed there is huge difference on how they are computed.
It's hard to bring in some reproducible data here but that's my output:</p>

<ol>
<li>R

<blockquote>
  <p>1 - pchisq(deviance(modelx),df.residual(modelx))</p>
</blockquote></li>
</ol>

<p>[1] 0.0003661318</p>

<blockquote>
  <p>1 - pchisq(sum(residuals(modelx, type = ""pearson"")^2),df.residual(modelx))</p>
</blockquote>

<p>[1] 0.4574779</p>

<blockquote>
  <p>deviance(modelx)</p>
</blockquote>

<p>[1] 3284.208</p>

<blockquote>
  <p>df.residual(modelx)</p>
</blockquote>

<p>[1] 3015</p>

<blockquote>
  <p>sum(residuals(modelx, type = ""pearson"")^2)</p>
</blockquote>

<p>[1] 3022.632</p>

<p>While in SAS its:</p>

<p>Criterion | Value | DF | Value/DF | Pr. > chi-sq.</p>

<p>Deviance | 2347.8792 | 2116 | 1.1096 | 0.0003 </p>

<p>Pearson | 2126.1138 | 2116 | 1.0048 | 0.4343 </p>

<p>the probabilities are similar but values and the degrees of freedom are completely different. </p>

<p>I've read that both the statistic and DF in SAS are calculated using ""profiles"" (<a href=""http://support.sas.com/resources/papers/proceedings14/1485-2014.pdf"" rel=""nofollow"">http://support.sas.com/resources/papers/proceedings14/1485-2014.pdf</a>, page 3) but I still don't understand how those profiles are calculated - I have 7 predictors in my data, each with 3,4,5,5,5,6,6 categories - or why one would use profiles at all.</p>

<p>Any ideas?</p>
"
"0.105555962793852","0.100588487635796","188098","<p>I am trying to estimate a model for an event modelled by probability of happening which is a linear function of x (distributed normally) plus an error term, u.</p>

<p>Then I simulate whether the event really happened for each X comparing the probability of it happening against a uniformly distributed random variable.</p>

<p>So, I wrote a little function that simulates this model for a given b0, b1, X (mean and sd.) and error term (mean = 0 and sd.):</p>

<pre><code>SAMPLE_SIZE = 10000

underlying &lt;- function(b0, b1, mean_x, sd_x, sd_u) {
  xs &lt;- rnorm(SAMPLE_SIZE, mean_x, sd_x)
  us &lt;- rnorm(SAMPLE_SIZE, 0.0, sd_u)
  ys &lt;- b0 + (b1 * xs) + us
  ws &lt;- runif(SAMPLE_SIZE) &lt; ys  
  list(ws = ws, ys = ys, xs = xs, us = us)
}
</code></pre>

<p>It neatly returns both the probability of the event taking place in the ys component plus a simulation on the ws component.</p>

<p>I then tested whether I can correctly estimate b0 and b1 using linear regressions. And I got a very weird result.</p>

<p>This is how I simulated the samples and did the regressions:</p>

<pre><code>b1s &lt;- seq(from = 0.0, to = 1.0, length.out = 100)

datasets &lt;- lapply(b1s, FUN = function(x) underlying(0.5, x, 1.0, 0.2, 0.05)) 
regs     &lt;- lapply(datasets,  FUN = function(x) lm(data = x, ws ~ xs))
b0s_hat = sapply(regs, function(x) x$coefficients[[1]])
    b1s_hat = sapply(regs, function(x) x$coefficients[[2]])
</code></pre>

<p>So, for different b1s (and b0 = 0.5) I can plot the estimated b0 and b1 against the real b1:</p>

<pre><code>plot(b1s, b0s_hat)
plot(b1s, b1s_hat)
</code></pre>

<p>And what we get for b1s_hat looks sigmoid-ish like a cumulative distribution function, and b0s_hat looks like a bell curve (like the density function).</p>

<p>I thought I could recover the coefficients using the linear regression. What exactly is smelling weird here?</p>
"
"0.0812570180448007","0.0871121856208561","188112","<p>I am studying logistic regressions and I wonder why are estimators biased when the independent variables have low variance (maybe low variance compared to its mean, but anyway).</p>

<p>I simulate the underlying model as a linear function of a single variable <code>x</code> and I do not include an error term. <code>x</code> is generated from a normal distribution, with mean <code>mx</code> and sd <code>sx</code>.</p>

<p><code>f</code> is a helper to map the probabilities using a logistic function</p>

<p>I use <code>mx = 1.0</code>, and sample <code>sx</code> from a uniform distribution from 0 to 1, so I can estimate the model for different values of <code>sx</code>.</p>

<pre><code>SAMPLE_SIZE = 1000
set.seed(100)

f &lt;- function(v) exp(v) / (1 + exp(v));

sim = function(b0, b1, mx, sx) {
  xs &lt;- rnorm(SAMPLE_SIZE, mean = mx, sd = sx)
  ps &lt;- f(b0 + b1 * xs)
  ys &lt;- rbinom(SAMPLE_SIZE, 1, ps)
  glm(ys ~ xs, family = binomial)
}  


sx &lt;- runif(n = 1000, min = 0.05, max = 1.0)
b0 = 1.5
b0s &lt;- sapply(sx, function(v) {
  sim(b0 = b0, b1 = 1.0, mx = 1.0, sx = v)$coefficients[[1]]
})
</code></pre>

<p>And then I plot the error between the estimated <code>b0</code> coefficient and the real one, for different values of <code>sx</code>:</p>

<pre><code>plot(sx, b0s - b0)
</code></pre>

<p>What I get is that the error gets smaller the greater <code>sx</code> is.</p>

<p>From common linear regressions, we know that the estimators get more precise the larger the variance in the independent variables. But that does not say anything about the biases. </p>

<p>How to interpret this result? Are the estimators really biased in logistic regressions? What's missing here? Is there any problem related to numerical estimates here?</p>

<p><a href=""http://i.stack.imgur.com/aj8md.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/aj8md.png"" alt=""Estimation error vs. standard deviation in X""></a></p>
"
"0.0746393370862076","0.0711268017165705","188115","<p>I currently have 3 models to predict Y from a linear combination of independent variables:</p>

<p>Model 1: Y ~ A + B</p>

<p>Model 2: Y ~ A + C</p>

<p>Model 3: Y ~ A + D</p>

<p>Now, I want to compare their in-sample fitting and out-of-sample prediction performances. I could split the data into training and testing sets, run the linear regression (lm) on the training set, and predict Y of the testing set with the result. However, the results vary depends on seeds used to split the data set.</p>

<p>I came across ""Cross Validation"" concept, but got confused on how to use it both in in-sample fitting and out-of-sample prediction. Across folds, the coefficients obtained from linear regression on training set will be different. This will affect the prediction part on the testing set as well. (Also that for every fold, the training and testing sets keep changing.)</p>

<p>Could someone help me with how to actually use cross-validation in this setting? Thank you!</p>
"
"0","0.029037395206952","188753","<p>I'm attempting to predict vegetation productivity based on climatic and land use variables (the latter are categorical). I found that there is a multicollinearity problem between the predictors (especially land use) as seen from the Variance Inflation Factor (VIF of the Ordinary Least Squares Regression). </p>

<p>Although my knowledge of lasso regression is basic, I assume lasso regression might solve the multicollinearity problem and also select variables that are driving the system. I appreciate an R code for estimating the standardized beta coefficients for the predictors or approaches on how to proceed.</p>

<pre><code>Variable           Coeff.  Std Coeff.  VIF    Std Error    t      P  Value 
Constant          -0.228   0            0      0.086       -2.644  0.008  
Precipitation      &lt;.001   0.151       2.688   &lt;.001        8.541  0.0  
Solar Rad          0.002   0.343       2.836   &lt;.001        18.939 &lt;.001  
Temp              -0.116  -1.604       28.12   0.004       -28.11  0.0  
Water Stress       0.881   0.391       2.352   0.037        23.7   &lt;.001  
Vapor Pressure     0.135   1.382       30.49   0.006        23.259 0.0    
  1               -0.103   -0.109      52.086  0.074       -1.398  0.162    
  2               -0.14    -0.048      6.49    0.079       -1.761  0.078   
  3               -0.11    -0.048      10.007  0.077       -1.42   0.156    
  4               -0.104   -0.234      236.288 0.073       -1.416  0.157    
  5               -0.097   -0.242      285.244 0.073       -1.331  0.183    
  6               -0.104   -0.09       35.067  0.074       -1.406  0.16    
  8               -0.119   -0.261      221.361 0.073       -1.629  0.103 
ELEVATION          &lt;.001   -0.115      3.917   &lt;.001       -5.381  &lt;.001
Condition Number: 59.833 
Mean of Correlation Matrix: 0.221 1st    
Eigenvalue divided by m: 0.328
</code></pre>
"
"0.0430930413588572","0.0410650781176591","189005","<p>I am trying to simulate from observed data that I have fit to a zero-inflated poisson regression model. I fit the data in R using zeroinfl() from the package pscl, but I am having trouble figuring out how to derive the ZIP distribution from the coefficient estimates. I know how to derive the predicted counts from these coefficient estimates (more information here: <a href=""http://www.ats.ucla.edu/stat/stata/faq/predict_zip.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/stata/faq/predict_zip.htm</a>), but can anyone help me understand how to find/derive estimates for my distribution parameters (i.e. lambda for the Poisson distribution, p for the Bernoulli distribution) that I can then sample from? Thanks!</p>
"
"0.0861860827177143","0.0821301562353182","189188","<p>If I create a linear model in R, I get a p-value for the whole model. When I create a logistic regression model, I don't. Why is this?</p>

<p><strong>Linear Regression</strong></p>

<pre><code>x&lt;-rnorm(100)
y&lt;-x+rnorm(100)
summary(lm(y~x))

 Call: lm(formula = y ~ x)

 Residuals:
      Min       1Q   Median       3Q      Max 
 -2.46237 -0.52810 -0.04574  0.48878  2.81002 

 Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)     (Intercept) -0.02318    0.09394  -0.247    0.806     x            1.10130    0.09421  11.690   &lt;2e-16***
 --- Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

 Residual standard error: 0.9374 on 98 degrees of freedom Multiple
 R-squared:  0.5824,    Adjusted R-squared:  0.5781  F-statistic: 136.7 on
 1 and 98 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p><strong>Logistic Regression</strong></p>

<pre><code>x&lt;-rnorm(100)
y&lt;-factor(c(rep(""ONE"",50),rep(""TWO"",50)))
summary(glm(y~x,family = ""binomial""))

 Call: glm(formula = y ~ x, family = ""binomial"")

 Deviance Residuals: 
      Min        1Q    Median        3Q       Max  
 -1.20658  -1.18093  -0.00499   1.17444   1.21414  

 Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|) (Intercept)  3.857e-05  .000e-01   0.000    1.000 x           -3.924e-02  2.055e-01  -0.191    0.849

 (Dispersion parameter for binomial family taken to be 1)

    Null deviance: 138.63  on 99  degrees of freedom Residual deviance: 138.59  on 98  degrees of freedom AIC: 142.59

 Number of Fisher Scoring iterations: 3
</code></pre>
"
"0.0430930413588572","0.0410650781176591","189969","<p>am interested in determining the association between an outcome and several predictors using relative risk ratios. I would like to use Poisson regression. As outlined in this article (<a href=""http://www.cmaj.ca/content/184/8/895?related-urls=yes&amp;legid=cmaj;184/8/895"" rel=""nofollow"">http://www.cmaj.ca/content/184/8/895?related-urls=yes&amp;legid=cmaj;184/8/895</a>), I should be reporting coefficients with robust standard errors.</p>

<p>I would like to perform variable selection in R. My plan was to do the following:</p>

<p>1) glm with family =poisson(link = ""log"")
2) run step on the full model
3) run the optimal model, and report robust standard errors</p>

<p>I am hoping someone could clarify whether the Log-Likelihood of the regular and robust model will be different? If it is, will my model selection procedure give the wrong answer? If wrong, are there any potential solutions?</p>

<p>thank you</p>

<p>mark</p>
"
"0.052777981396926","0.0502942438178979","190586","<p>I am using cross correlation to demonstrate a potential link between two time series (ext &amp; co). Both series are strongly autocorrelated, so it is difficult to assess the dependence between the two series. For a quick preliminary analysis, the cross correlation shows a clear (somehow delayed) link between the two time series, although it might spurious. <a href=""http://i.stack.imgur.com/eHUnj.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/eHUnj.jpg"" alt=""CCF""></a>. Prewhitening seems to be the best option; I will prewhiten my x variable by fitting an ARIMA process and then use the coefficients to filter my variable y. My question is if I should estimate the coefficients of the ARIMA process (for example using <code>auto.arima</code>) using my series x or by using the residuals of the OLS regression of x on y.</p>
"
"0.0746393370862076","0.0592723347638087","190748","<p>I am using <code>faraway::choccake</code> data, and I want to fit a linear model. 
I have used the following code:</p>

<pre><code>library(faraway)
attach(chocake)
choccake                 #to have a sense of the data 
choccake.lm&lt;-lm(breakang~recipe+batch+temp,data=choccake)
summary(choccake.lm) 
</code></pre>

<p>I have fitted linear model using 'lm' in R before too. But, here the output of <code>summary(choccake.lm)</code> looks a little different. </p>

<p>Here is the output (I'm attaching the choccake data too).</p>

<p><pre> > choccake
    recipe batch temp breakang
    1        1     1  175       42
    2        1     1  185       46
    3        1     1  195       47
    4        1     1  205       39
    5        1     1  215       53
    6        1     1  225       42
    7        1     2  175       47
    8        1     2  185       29
    9        1     2  195       35
    10       1     2  205       47
    11       1     2  215       57
    12       1     2  225       45
    13       1     3  175       32
    14       1     3  185       32
    15       1     3  195       37
    16       1     3  205       43
    17       1     3  215       45
    18       1     3  225       45
    19       1     4  175       26
    20       1     4  185       32
    21       1     4  195       35
    22       1     4  205       24
    23       1     4  215       39
    24       1     4  225       26
    25       1     5  175       28
    26       1     5  185       30
    27       1     5  195       31
    28       1     5  205       37
    29       1     5  215       41
    30       1     5  225       47
    31       1     6  175       24
    32       1     6  185       22
 33       1     6  195       22
34       1     6  205       29
35       1     6  215       35
36       1     6  225       26
37       1     7  175       26
38       1     7  185       23
39       1     7  195       25
40       1     7  205       27
41       1     7  215       33
42       1     7  225       35
43       1     8  175       24
44       1     8  185       33
45       1     8  195       23
46       1     8  205       32
47       1     8  215       31
48       1     8  225       34
49       1     9  175       24
50       1     9  185       27
51       1     9  195       28
52       1     9  205       33
53       1     9  215       34
54       1     9  225       23
55       1    10  175       24
56       1    10  185       33
57       1    10  195       27
58       1    10  205       31
59       1    10  215       30
60       1    10  225       33
61       1    11  175       33
62       1    11  185       39
63       1    11  195       33
64       1    11  205       28
65       1    11  215       33
66       1    11  225       30
67       1    12  175       28
68       1    12  185       31
69       1    12  195       27
70       1    12  205       39
71       1    12  215       35
72       1    12  225       43
73       1    13  175       29
74       1    13  185       28
75       1    13  195       31
76       1    13  205       29
77       1    13  215       37
78       1    13  225       33
79       1    14  175       24
80       1    14  185       40
81       1    14  195       29
82       1    14  205       40
83       1    14  215       40
84       1    14  225       31
85       1    15  175       26
86       1    15  185       28
87       1    15  195       32
88       1    15  205       25
89       1    15  215       37
90       1    15  225       33
91       2     1  175       39
92       2     1  185       46
93       2     1  195       51
94       2     1  205       49
95       2     1  215       55
96       2     1  225       42
97       2     2  175       35
98       2     2  185       46
99       2     2  195       47
100      2     2  205       39
101      2     2  215       52
102      2     2  225       61
103      2     3  175       34
104      2     3  185       30
105      2     3  195       42
106      2     3  205       35
107      2     3  215       42
108      2     3  225       35
109      2     4  175       25
110      2     4  185       26
111      2     4  195       28
112      2     4  205       46
113      2     4  215       37
114      2     4  225       37
115      2     5  175       31
116      2     5  185       30
117      2     5  195       29
118      2     5  205       35
119      2     5  215       40
120      2     5  225       36
121      2     6  175       24
122      2     6  185       29
123      2     6  195       29
124      2     6  205       29
125      2     6  215       24
126      2     6  225       35
127      2     7  175       22
128      2     7  185       25
129      2     7  195       26
130      2     7  205       26
131      2     7  215       29
132      2     7  225       36
133      2     8  175       26
134      2     8  185       23
135      2     8  195       24
136      2     8  205       31
137      2     8  215       27
138      2     8  225       37
139      2     9  175       27
140      2     9  185       26
141      2     9  195       32
142      2     9  205       28
143      2     9  215       32
144      2     9  225       33
145      2    10  175       21
146      2    10  185       24
147      2    10  195       24
148      2    10  205       27
149      2    10  215       37
150      2    10  225       30
151      2    11  175       20
152      2    11  185       27
153      2    11  195       33
154      2    11  205       31
155      2    11  215       28
156      2    11  225       33
157      2    12  175       23
158      2    12  185       28
159      2    12  195       31
160      2    12  205       34
161      2    12  215       31
162      2    12  225       29
163      2    13  175       32
164      2    13  185       35
165      2    13  195       30
166      2    13  205       27
167      2    13  215       35
168      2    13  225       30
169      2    14  175       23
170      2    14  185       25
171      2    14  195       22
172      2    14  205       19
173      2    14  215       21
174      2    14  225       35
175      2    15  175       21
176      2    15  185       21
177      2    15  195       28
178      2    15  205       26
179      2    15  215       27
180      2    15  225       20
181      3     1  175       46
182      3     1  185       44
183      3     1  195       45
184      3     1  205       46
185      3     1  215       48
186      3     1  225       63
187      3     2  175       43
188      3     2  185       43
189      3     2  195       43
190      3     2  205       46
191      3     2  215       47
192      3     2  225       58
193      3     3  175       33
194      3     3  185       24
195      3     3  195       40
196      3     3  205       37
197      3     3  215       41
198      3     3  225       38
199      3     4  175       38
200      3     4  185       41
201      3     4  195       38
202      3     4  205       30
203      3     4  215       36
204      3     4  225       35
205      3     5  175       21
206      3     5  185       25
207      3     5  195       31
208      3     5  205       35
209      3     5  215       33
210      3     5  225       23
211      3     6  175       24
212      3     6  185       33
213      3     6  195       30
214      3     6  205       30
215      3     6  215       37
216      3     6  225       35
217      3     7  175       20
218      3     7  185       21
219      3     7  195       31
220      3     7  205       24
221      3     7  215       30
222      3     7  225       33
223      3     8  175       24
224      3     8  185       23
225      3     8  195       21
226      3     8  205       24
227      3     8  215       21
228      3     8  225       35
229      3     9  175       24
230      3     9  185       18
231      3     9  195       21
232      3     9  205       26
233      3     9  215       28
234      3     9  225       28
235      3    10  175       26
236      3    10  185       28
237      3    10  195       27
238      3    10  205       27
239      3    10  215       35
240      3    10  225       35
241      3    11  175       28
242      3    11  185       25
243      3    11  195       26
244      3    11  205       25
245      3    11  215       38
246      3    11  225       28
247      3    12  175       24
248      3    12  185       30
249      3    12  195       28
250      3    12  205       35
251      3    12  215       33
252      3    12  225       28
253      3    13  175       28
254      3    13  185       29
255      3    13  195       43
256      3    13  205       28
257      3    13  215       33
258      3    13  225       37
259      3    14  175       19
260      3    14  185       22
261      3    14  195       27
262      3    14  205       25
263      3    14  215       25
264      3    14  225       35
265      3    15  175       21
266      3    15  185       28
267      3    15  195       25
268      3    15  205       25
269      3    15  215       31
270      3    15  225       25</pre></p>

<pre><code>choccake.lm&lt;-lm(breakang~recipe+batch+temp,data=choccake)

&gt; summary(choccake.lm)

Call:
lm(formula = breakang ~ recipe + batch + temp, data = choccake)

Residuals:
 Min       1Q   Median       3Q      Max 
-15.1851  -2.5682  -0.0419   2.7553  13.4816 

Coefficients:
         Estimate Std. Error t value Pr(&gt;|t|)    
      (Intercept)  16.22698    3.63640   4.462 1.22e-05 ***
      recipe2      -1.47778    0.71744  -2.060   0.0404 *  
      recipe3      -1.52222    0.71744  -2.122   0.0348 *  
      batch2       -1.27778    1.60424  -0.796   0.4265    
      batch3       -9.88889    1.60424  -6.164 2.79e-09 ***
       batch4      -13.55556    1.60424  -8.450 2.36e-15 ***
       batch5      -14.44444    1.60424  -9.004  &lt; 2e-16 ***
      batch6      -18.11111    1.60424 -11.289  &lt; 2e-16 ***
      batch7      -19.50000    1.60424 -12.155  &lt; 2e-16 ***
      batch8      -19.44444    1.60424 -12.121  &lt; 2e-16 ***
      batch9      -19.50000    1.60424 -12.155  &lt; 2e-16 ***
      batch10     -18.00000    1.60424 -11.220  &lt; 2e-16 ***
      batch11     -16.94444    1.60424 -10.562  &lt; 2e-16 ***
      batch12     -15.88889    1.60424  -9.904  &lt; 2e-16 ***
      batch13     -14.94444    1.60424  -9.316  &lt; 2e-16 ***
      batch14     -18.94444    1.60424 -11.809  &lt; 2e-16 ***
      batch15     -20.22222    1.60424 -12.605  &lt; 2e-16 ***
      temp          0.15803    0.01715   9.215  &lt; 2e-16 ***
      ---
     Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

     Residual standard error: 4.813 on 252 degrees of freedom
     Multiple R-squared:  0.6783,    Adjusted R-squared:  0.6566 
     F-statistic: 31.25 on 17 and 252 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>My questions: </p>

<ol>
<li>the variables are 'recipe', 'batch' and 'temp'..then why for different values of recipe and batch it's showing different coefficient? it seems from the result that there are 17 dependent variables.</li>
<li>why there's no mention of recipe1 and batch1? </li>
<li>is it by any chance computing for several different regression lines?</li>
</ol>
"
"0.0710998907892006","0.0871121856208561","191063","<p>need help.
I use <code>rms</code> and can't understand different between <code>orm</code> and <code>lrm</code> when i used <code>contrasts</code>. For example:</p>

<pre><code>x &lt;- factor(rbinom(100,2,0.6), labels = c(""a"",""b"",""c""), ordered = TRUE)
y &lt;- factor(rbinom(100,1,0.5), labels=c(""no"",""yes""))
l&lt;-lrm(x~y);l
Logistic Regression Model

lrm(formula = x ~ y)
                     Model Likelihood     Discrimination    Rank Discrim.    
                        Ratio Test            Indexes          Indexes       
Obs           100    LR chi2      0.51    R2       0.006    C       0.529    
 a             24    d.f.            1    g        0.133    Dxy     0.059    
 b             40    Pr(&gt; chi2) 0.4764    gr       1.143    gamma   0.117    
 c             36                         gp       0.024    tau-a   0.039    
max |deriv| 1e-10                         Brier    0.181                     

      Coef    S.E.   Wald Z Pr(&gt;|Z|)
y&gt;=b   1.0188 0.2988  3.41  0.0007  
y&gt;=c  -0.7162 0.2884 -2.48  0.0130  
y=yes  0.2642 0.3715  0.71  0.4769

o&lt;-orm(x~y);l;o
Logistic (Proportional Odds) Ordinal Regression Model

orm(formula = x ~ y)
                     Model Likelihood          Discrimination          Rank Discrim.    
                        Ratio Test                 Indexes                Indexes       
Obs           100    LR chi2      0.51    R2                  0.006    rho     0.071    
 a             24    d.f.            1    g                   0.133                     
 b             40    Pr(&gt; chi2) 0.4764    gr                  1.143                     
 c             36    Score chi2   0.51    |Pr(Y&gt;=median)-0.5| 0.259                     
Unique Y        3    Pr(&gt; chi2) 0.4766                                                  
Median Y        2                                                                       
max |deriv| 7e-05                                                                       

      Coef    S.E.   Wald Z Pr(&gt;|Z|)
y&gt;=b   1.0188 0.2988  3.41  0.0007  
y&gt;=c  -0.7162 0.2884 -2.48  0.0130  
y=yes  0.2642 0.3715  0.71  0.4769  
</code></pre>

<p>We can see, that results <code>orm</code> and <code>lrm</code> are equal. When we use <code>contrasts</code> results are different:</p>

<pre><code>contrast(l,list(y=""no""),list(y=""yes""))
     Contrast      S.E.      Lower     Upper     Z Pr(&gt;|z|)
11 -0.2642454 0.3714673 -0.9923081 0.4638172 -0.71   0.4769
Confidence intervals are 0.95 individual intervals
</code></pre>

<p>and </p>

<pre><code>contrast(o,list(y=""no""),list(y=""yes""))

Contrast      S.E.      Lower   Upper    Z Pr(&gt;|z|)
11 0.7545878 0.3714672 0.02652544 1.48265 2.03   0.0422

Confidence intervals are 0.95 individual intervals
</code></pre>

<p>Why <code>orm</code> contrast aren't equal beta regression coefficient as <code>lrm</code> contrast? p.s. sorry for bad English</p>
"
"0.0304713817668003","0.029037395206952","191434","<p>I want to do a logistic regression simulation using R </p>

<p>I use this code</p>

<pre><code>set.seed(666)
age = rnorm(60)         
blood_pressure = rnorm(60)
race = sample(c(rep(1,30),rep(0,30)))
inactivity = sample(c(rep(1,30),rep(0,30)))
weight = rnorm(60)

z=1+1*age+blood_pressure*2+3*weight+3*inactivity+0*race
pr=exp(z)/(1+exp(z))
y=rbinom(60,1,pr)

df = data.frame(y=y,age,blood_pressure,inactivity,weight,race)
glm(y~age+blood_pressure+inactivity+weight+race,data=df,family=binomial(link='logit'),control = list(maxit = 50))
</code></pre>

<p>I got very strange result from it.</p>

<pre><code>Coefficients:
   (Intercept)             age  blood_pressure      inactivity          weight            race  
        -39.75           46.64          106.65          143.52          229.75          100.87  
</code></pre>

<p>And it says the model doesn't converge.</p>

<p>Does someone know what's wrong and how to fix it?</p>
"
"0.068136080998913","0.0649295895722714","191506","<p>My dependent variable has 4 categories, but when I run the multinomial logistic regression using the package <code>nnet</code> with function <code>multinom</code> the results only show 3 categories. </p>

<p>I've tried changing the category numbers from 0,2,3,4 to 1,2,3,4, and also tried using names instead of numbers for the categories but it still wont show all 4 categories in the results. </p>

<p>Also, when I changed the categories to names instead of numbers, the resulting p values for each category drastically changed. Why is this? 
The p values were acquired using these commands</p>

<pre><code>z &lt;- summary(siglm)$coefficients/summary(siglm)$standard.errors
p &lt;- (1 - pnorm(abs(z), 0, 1)) * 2
p
</code></pre>
"
"0.105869651339174","0.0931268436589426","191891","<p>I'm hoping someone can help clarify a few things for me.</p>

<p>I ran some relatively simple logistic regressions in r and am having trouble with interpretation.  I'm interested in the effects of elevation and a species diversity index on the presence/absence of a disease in individual animals.</p>

<p>I ran a simple model of: <code>Result~Elevation+Diversity</code> which gave this result</p>

<pre><code>Call:
glm(formula = Test_Result ~ Elevation + Simpsons_Diversity, family = binomial, 
    data = XXXXXX)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.8141  -0.6984  -0.5317  -0.4143   2.3337  

Coefficients:
                     Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -2.118e+00  1.594e-01 -13.289  &lt; 2e-16 
Elevation           1.316e-04  2.247e-05   5.855 4.76e-09 
Simpsons_Diversity -9.907e-01  2.725e-01  -3.635 0.000278 

    Null deviance: 3015.2  on 3299  degrees of freedom
Residual deviance: 2923.6  on 3297  degrees of freedom
AIC: 2929.6
</code></pre>

<p>I have a strong suspicion that diversity decreases with increasing elevation which I have confirmed although the relationship isn't quite as strong as I thought. When I run a model with an interaction term <code>elevation*diversity</code> I get:</p>

<pre><code>Call:
glm(formula = Test_Result ~ Elevation_1000 + Simpsons_Diversity_100 + 
    Elevation_1000 * Simpsons_Diversity_100, family = binomial, 
    data = XXXXXXX)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.7908  -0.6959  -0.5437  -0.3963   2.4215  

Coefficients:
                                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                           -2.014422   0.179507 -11.222  &lt; 2e-16 
Elevation_1000                         0.112466   0.027433   4.100 4.14e-05 
Simpsons_Diversity_100                -0.015851   0.005780  -2.743   0.0061  
Elevation_1000:Simpsons_Diversity_100  0.001408   0.001200   1.173   0.2406   

    Null deviance: 3015.2  on 3299  degrees of freedom
Residual deviance: 2922.2  on 3296  degrees of freedom
AIC: 2930.2

Number of Fisher Scoring iterations: 5
</code></pre>

<p>Showing that adding the interaction term doesn't really help the fit of the model (AIC = 2930) and the interaction term itself is not significant (p-value=0.24).</p>

<p>Am I on the right track so far?</p>

<p>If I am, I understand how to convert coefficients to odds ratios and interpret those.  My main question is can I plot the predicted probabilities for a combination of elevation and diversity where each variable is allowed to vary? Or is this essentially plotting the interaction?  </p>

<p>I was able to create a dataframe where I varied elevation and diversity and I used my simple non-interaction model to obtain predicted probabilities using the PREDICT fuction) for those combinations, but I want to make sure that I am doing things correctly.  I've attached the plot of predicted probs for different levels of diversity. </p>

<p><a href=""http://i.stack.imgur.com/NINBE.gif"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NINBE.gif"" alt=""Elevation vs. Predicted Probabilities for various levels of diversity)""></a></p>
"
"0.134074079773921","0.13937949699337","192173","<p>I'm working on the similarity of categorical regression with exclusively  dummy variables and ANOVA. There are lots of references, like Gujarati &amp; Porter (2009), which have mentioned that those two are equivalent. Everything is okay when distribution of residuals is normal, variances are homogeneous and regression model is significant. My questions are there. We have a category with 3 levels (red,blue,green), a numeric variable ""allscore""( -5 &lt;= allscore &lt;= +5). I played with R and made data and ran models (regression and variance).</p>

<pre><code># creating data 
bluescore  &lt;- rnorm(n=100, mean=-1, sd=1)
redscore   &lt;- rnorm(n=100, mean=2,  sd=1)
greenscore &lt;- rnorm(n=100, mean=.1, sd=2)
for (i in 1:100) {
  if (bluescore[i] &lt; -5)  bluescore[i]  &lt;- -5
  if (bluescore[i] &gt; 5)   bluescore[i]  &lt;-  5
  if (redscore[i] &lt; -5)   redscore[i]   &lt;- -5
  if (redscore[i] &gt; 5)    redscore[i]   &lt;-  5
  if (greenscore[i] &lt; -5) greenscore[i] &lt;- -5
  if (greenscore[i] &gt; 5)  greenscore[i] &lt;-  5
}
color &lt;- as.factor(c(rep(1,100), rep(2,100), rep(3,100)))
allscore &lt;- c(bluescore, redscore, greenscore)
table &lt;- data.frame(color, allscore)
randtable &lt;- table[sample(nrow(table)),]
finaltable &lt;- data.frame(randtable$color, randtable$allscore)
colnames(finaltable) &lt;- c(""color"", ""score"")
# plot
plot(randtable$allscore ~ randtable$color, data=finaltable)
# saving data for SPSS
library(rio)
export(finaltable, ""dummy.sav"")
write.csv(finaltable, ""finaltable.csv"")
# making dummy variables
dummyred   &lt;- NULL
dummygreen &lt;- NULL
dummyblue  &lt;- NULL
for(i in 1:NROW(finaltable)) {
  if (randtable$color[i]==2) dummyred[i]=1 else dummyred[i]=0
      if (randtable$color[i]==3) dummygreen[i]=1 else dummygreen[i]=0
  if (randtable$color[i]==1) dummyblue[i]=1 else dummyblue[i]=0
}
t1 = cbind(randtable, dummyred, dummygreen)
# run regression model 
mosel.1 &lt;- lm(formula = allscore~dummyred + dummygreen + dummyblue -1, data=t1)
ttt &lt;- summary(mosel.1)
ttt

# **test of homogenity**
# Bartlettâ€™s test
bartlett.test(randtable$allscore ~ randtable$color, data=finaltable)
# Leveneâ€™s test
library(car)

leveneTest(randtable$allscore ~ randtable$color, data=finaltable)
# Fligner-Killeen test
fligner.test(randtable$allscore ~ randtable$color, data=finaltable)
# ANOVA mode
hh &lt;- aov(randtable$allscore ~ randtable$color, data=finaltable)
hh
summary(hh)
# post hoc test
TukeyHSD(hh)
</code></pre>

<p>Output would be something like this:  </p>

<p><a href=""http://i.stack.imgur.com/v0o8x.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/v0o8x.png"" alt=""enter image description here""></a></p>

<pre><code>Call:
lm(formula = allscore ~ dummyred + dummygreen + dummyblue - 1, 
    data = t1)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.0152 -0.7880  0.0043  0.8088  3.3731 

Coefficients:
           Estimate Std. Error t value Pr(&gt;|t|)    
dummyred    2.02102    0.13273  15.227  &lt; 2e-16 ***
dummygreen  0.01525    0.13273   0.115    0.909    
dummyblue  -1.04294    0.13273  -7.858 7.24e-14 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 1.327 on 297 degrees of freedom
Multiple R-squared:  0.4971,    Adjusted R-squared:  0.4921 
F-statistic: 97.87 on 3 and 297 DF,  p-value: &lt; 2.2e-16

&gt;  
&gt; # test of homogenity
&gt; # Bartlettâ€™s test
&gt; bartlett.test(randtable$allscore ~ randtable$color, data=finaltable)

    Bartlett test of homogeneity of variances

data:  randtable$allscore by randtable$color
Bartlett's K-squared = 94.825, df = 2, p-value &lt; 2.2e-16

&gt; # Leveneâ€™s test
&gt; library(car)
&gt; 
&gt; leveneTest(randtable$allscore ~ randtable$color, data=finaltable)
Levene's Test for Homogeneity of Variance (center = median)
       Df F value    Pr(&gt;F)    
group   2  43.995 &lt; 2.2e-16 ***
      297                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; # Fligner-Killeen test
&gt; fligner.test(randtable$allscore ~ randtable$color, data=finaltable)

    Fligner-Killeen test of homogeneity of variances

data:  randtable$allscore by randtable$color
Fligner-Killeen:med chi-squared = 66.204, df = 2, p-value = 4.207e-15

&gt; # ANOVA mode
&gt; hh &lt;- aov(randtable$allscore ~ randtable$color, data=finaltable)
&gt; hh
Call:
   aov(formula = randtable$allscore ~ randtable$color, data = finaltable)

Terms:
                randtable$color Residuals
Sum of Squares         484.3572  523.2176
Deg. of Freedom               2       297

Residual standard error: 1.327281
Estimated effects may be unbalanced
&gt; summary(hh)
                 Df Sum Sq Mean Sq F value Pr(&gt;F)    
randtable$color   2  484.4  242.18   137.5 &lt;2e-16 ***
Residuals       297  523.2    1.76                   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; # post hoc test
&gt; TukeyHSD(hh)
  Tukey multiple comparisons of means
    95% family-wise confidence level

Fit: aov(formula = randtable$allscore ~ randtable$color, data = finaltable)

$`randtable$color`
         diff        lwr       upr p adj
2-1  3.063958  2.6218117  3.506104 0e+00
3-1  1.058184  0.6160382  1.500330 1e-07
3-2 -2.005774 -2.4479195 -1.563628 0e+00
</code></pre>

<ul>
<li>Is variance homogeneity check essential for regression model as assumption (because it compares means and equivalent to ANOVA)?</li>
<li>What is assumption for this regression model?</li>
<li>How can I interpret ""greendummy"" variable insignificance? Can I omit it from model? What theory support this omission? Is it means green color has no effect on scores? Is it equivalent to heterogeneity of variances?</li>
<li>How about ANOVA model, what can I say about the results?  </li>
<li>Can I remove green level from ANOVA?</li>
</ul>

<blockquote>
  <p>Gujarati, Damodar N.; Porter, Dawn C. (2009): Basic econometrics. 5th
  ed. Boston: McGraw-Hill Irwin (The McGraw-Hill series, economics).</p>
</blockquote>
"
"0.102279800236246","0.104963924850184","192479","<p>I've been getting very odd factors in more complex models and simplify to this minimal case that behaves oddly vs my intuition.</p>

<p>Suppose we have the generative model, where everything is IID Normal</p>

<p>$A \sim N(0,1)$
$B \sim N(0,1)$</p>

<p>$Y_1 = A + \epsilon$ ; $Y_2 = A + \epsilon$ ; $Y_3 = A + \epsilon$</p>

<p>$X_1 = A + B + \epsilon$ ; $X_2 = A + B + \epsilon $ ; $X_3 = A + B + \epsilon$</p>

<p>Where each $\epsilon$ is independant, and in practice we divide by $Y$ and $X$ by $\sqrt{2}$ and $\sqrt{3}$ respectively to standardize.</p>

<p>The task is then to find the latent factors $A$ and $B$ from observed $X$s and $Y$s, pretending of course that we don't know the coefficients happen to all be 1.0, but <strong>we do know that $Y$ contains no $B$</strong></p>

<p>This should be perfect for SEM/CFA, but when I try it fails to correctly identify factor A by an amount that can be corrected manually (so the solution does exist!).  I'm using lavaan, and can't tell if it is a flaw in the software, the method, or my understanding.</p>

<p>In code:</p>

<pre><code>library(lavaan)
library(data.table)

N = 100000

DT =  data.table(A = rnorm(N), B = rnorm(N))

DT[, Y1 := (A + rnorm(N))/sqrt(2)]
DT[, Y2 := (A + rnorm(N))/sqrt(2)]
DT[, Y3 := (A + rnorm(N))/sqrt(2)]
DT[, X1 := (A + B + rnorm(N))/sqrt(3)]
DT[, X2 := (A + B + rnorm(N))/sqrt(3)]
DT[, X3 := (A + B + rnorm(N))/sqrt(3)]

model = 'FA =~ Y1 + Y2 + Y3 + X1 + X2 + X3
FB =~ X1 + X2 + X3
FA ~~ 0*FB
'

fit = sem(model, data= DT, std.lv = TRUE, std.ov = FALSE)
summary(fit)
</code></pre>

<p>and the summary of our fit is</p>

<pre><code>lavaan (0.5-20) converged normally after  20 iterations

  Number of observations                        100000

  Estimator                                         ML
  Minimum Function Test Statistic                9.770
  Degrees of freedom                                 6
  P-value (Chi-square)                           0.135

Parameter Estimates:

  Information                                 Expected
  Standard Errors                             Standard

Latent Variables:
                   Estimate  Std.Err  Z-value  P(&gt;|z|)
  FA =~                                               
    Y1                0.711    0.003  227.070    0.000
    Y2                0.708    0.003  226.475    0.000
    Y3                0.709    0.003  226.770    0.000
    X1                0.580    0.003  176.083    0.000
    X2                0.577    0.003  175.597    0.000
    X3                0.579    0.003  176.073    0.000
  FB =~                                               
    X1                0.579    0.003  183.034    0.000
    X2                0.580    0.003  183.912    0.000
    X3                0.575    0.003  181.946    0.000

Covariances:
                   Estimate  Std.Err  Z-value  P(&gt;|z|)
  FA ~~                                               
    FB                0.000                           

Variances:
                   Estimate  Std.Err  Z-value  P(&gt;|z|)
    Y1                0.498    0.003  160.623    0.000
    Y2                0.497    0.003  161.192    0.000
    Y3                0.497    0.003  160.910    0.000
    X1                0.333    0.002  141.244    0.000
    X2                0.329    0.002  139.731    0.000
    X3                0.337    0.002  143.257    0.000
    FA                1.000                           
    FB                1.000                           
</code></pre>

<p>So far so good, but if we take the scores for those factors then they are not uncorrelated!  Furthermore, if we regress Y1 on those factors, then Y1 appears to depend on factor B, which we specified is not the case.</p>

<pre><code>DT = cbind(DT, predict(fit))

DT[, cor(FB, FA)]
#[1] 0.2219724

summary(DT[, lm(Y1 ~ FA + FB)])


Call:
lm(formula = Y1 ~ FA + FB)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.67157 -0.39035  0.00245  0.38889  2.32116 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -0.002491   0.001823  -1.367    0.172    
FA           0.945142   0.002102 449.682   &lt;2e-16 ***
FB          -0.231733   0.002351 -98.586   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.5764 on 99997 degrees of freedom
Multiple R-squared:  0.6691,    Adjusted R-squared:  0.6691 
F-statistic: 1.011e+05 on 2 and 99997 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>There does appear to be a correct solution though, because if we take those regression coefficients and make a new factor A, then orthogonality is achieved and the Ys all only depend on this new factor</p>

<pre><code>DT[, CA := 0.94*FA - 0.235*FB]

DT[, cor(CA,FB)]
#[1] -0.00160991
</code></pre>

<p>What is going on?</p>

<p><strong>EDIT</strong></p>

<p>As far as I can tell, it is fitting the weights correctly, this diagram purports to show the fitted model (<code>semPaths</code>) and the residual variances and loadings all look correct.  So perhaps it is the <code>predict</code> function where the issue lies?</p>

<p><a href=""http://i.stack.imgur.com/wwv3C.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/wwv3C.png"" alt=""SEMdiagram""></a></p>
"
"0.109866129394437","0.0805352440958291","193000","<p>So I'm trying to fit a hurdle model with the count distribution as negative binomial.  I get the following outputs for assuming negative binomial and poisson:</p>

<pre><code>&gt; hurdle(degree ~ dc, data = data, dist = ""negbin"")

Call:
hurdle(formula = degree ~ dc, data = data, dist = ""negbin"")

Count model coefficients (truncated negbin with log link):
(Intercept)           dc  
     0.2428       0.1035  
Theta = 0.8815 

Zero hurdle model coefficients (binomial with logit link):
(Intercept)           dc  
    -0.8512       0.1649

&gt; hurdle(degree ~ dc, data = data, dist = ""poisson"")

Call:
hurdle(formula = degree ~ dc, data = data, dist = ""poisson"")

Count model coefficients (truncated poisson with log link):
(Intercept)           dc  
    0.68283      0.08584  

Zero hurdle model coefficients (binomial with logit link):
(Intercept)           dc  
    -0.8512       0.1649  
</code></pre>

<p>From a regression in python based on estimates of mean of non zero data vs. regressor, I get:</p>

<p>m = 0.08374289, b =  0.7132967</p>

<p>Which is far from what the negative binomial estimates, but the Poisson gets it pretty close.  However a vuong test tells me that the negative binomial is far better:</p>

<pre><code>&gt; vuong(mod_pois, mod_nb_hurdle)
Vuong Non-Nested Hypothesis Test-Statistic: -114.0873 
(test-statistic is asymptotically distributed N(0,1) under the
 null that the models are indistinguishible)
in this case:
model2 &gt; model1, with p-value 0 
</code></pre>

<p>The non-zero data is overdispersed, but it looks like the variance is constant*mean, so I know Poisson shouldn't be used, but why is a Poisson hurdle so much better at predicting log(expected value)? </p>
"
"0.0691025985081098","0.076825726438694","193417","<p>I have an experiment where we measure the energy used by a building and want to regress this energy linearly against so-called degree-days, calculated with two different methods. The data looks like this:</p>

<p><a href=""http://i.stack.imgur.com/eR1yF.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/eR1yF.png"" alt=""enter image description here""></a></p>

<p>A regression line has been added to each group, that has been forced to go through the origin.</p>

<p>I want to compute the slope of these lines (with std. error), but I'm not sure what is the right way. My data looks like this:</p>

<pre><code>&gt; alvDegreeDays[sample(nrow(alvDegreeDays), 4),]
          Energy  BaseTemp DegreeDays
Feb 2014   984.7 Estimated   365.9771
Mar 2014   864.7 Estimated   307.2246
Apr 20151  512.8       SIA    50.0000
Sep 2015   239.2 Estimated    95.4787
</code></pre>

<p>I've tried this first:</p>

<blockquote>
  <p>lm(Energy ~ DegreeDays * BaseTemp + 0, alvDegreeDays)</p>
</blockquote>

<pre><code>Call:
lm(formula = Energy ~ DegreeDays * BaseTemp + 0, data = alvDegreeDays)

Coefficients:
            DegreeDays       BaseTempEstimated             BaseTempSIA  
                 2.436                  23.094                 174.390  
DegreeDays:BaseTempSIA  
                 1.181  
</code></pre>

<p>But this yields <code>BaseTempEstimated</code> and <code>BaseTempSIA</code> terms which are, in effect, intercept terms.</p>

<p>Next I tried the following:</p>

<blockquote>
  <p>(foo &lt;- lm(Energy ~ DegreeDays + DegreeDays:BaseTemp + 0, alvDegreeDays))</p>
</blockquote>

<pre><code>Call:
lm(formula = Energy ~ DegreeDays + DegreeDays:BaseTemp + 0, data = alvDegreeDays)

Coefficients:
                  DegreeDays  DegreeDays:BaseTempEstimated        DegreeDays:BaseTempSIA  
                       4.401                        -1.897                            NA  
</code></pre>

<p>This looks better, but when I try to call <code>predict</code> on this model I get weird error messages:</p>

<pre><code>&gt; predict(foo, list(DegreeDays = 1, BaseTemp = ""Estimated""))
       1 
2.504507 
Warning message:
In predict.lm(foo, list(DegreeDays = 1, BaseTemp = ""Estimated"")) :
  prediction from a rank-deficient fit may be misleading
</code></pre>

<p>Any idea what I may be doing wrong (or right) here?</p>
"
"0.0304713817668003","0.029037395206952","193435","<p>I have a model that I need to estimate,</p>

<p><a href=""http://i.stack.imgur.com/WPJZK.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/WPJZK.jpg"" alt=""enter image description here""></a> 
where </p>

<p><a href=""http://i.stack.imgur.com/Qv7L1.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Qv7L1.jpg"" alt=""enter image description here""></a></p>

<p>I've seen a similar example  (<a href=""http://stats.stackexchange.com/questions/41168/constrained-regression-in-r-coefficients-positive-sum-to-1-and-non-zero-interc"">Constrained Regression in R: coefficients positive, sum to 1 and non-zero intercept</a>) but without the second part of the regression, i.e. the relationship with betas and theta0. I have not been able to modify that solution. </p>

<p>The model comes from MSCI Private and Public Real Estate - What's the link ( June 2010). My intention is to use this model for other illiquid assets as well.</p>
"
"0.0806196982594614","0.0658506226617377","194293","<p>I want to be able to calculate the confidence interval from the estimated coefficient and respective standard errors.</p>

<p>I have a linear regression model which can be summarized (in R):</p>

<pre><code>summary(fit1)

Call:
lm(formula = bwt ~ height + weight + parity, data = data)

Residuals:
    Min      1Q  Median      3Q     Max 
-66.913 -10.624   0.991  10.979  55.621 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 31.19217   13.56879   2.299   0.0217 *  
height       1.24964    0.23083   5.414 7.48e-08 ***
weight       0.06781    0.02823   2.402   0.0164 *  
parity1     -1.83309    1.19838  -1.530   0.1264    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 17.9 on 1170 degrees of freedom
Multiple R-squared:  0.04898,   Adjusted R-squared:  0.04654 
F-statistic: 20.08 on 3 and 1170 DF,  p-value: 1.071e-12
</code></pre>

<p>With this model I can calculate the respective confidence intervals:</p>

<pre><code>&gt; confint(fit1)
                  2.5 %     97.5 %
(Intercept)  4.57029503 57.8140351
height       0.79676227  1.7025207
weight       0.01243198  0.1231932
parity1     -4.18429933  0.5181151
</code></pre>

<p>I would expect the intervals of the predictor <em>height</em> to be given by</p>

<p>$$
1.24964 \pm (1.96*0.23083) = [0.7972132,1.702067]
$$</p>

<p>where 1.24964 is the estimated value for the coefficient and 0.23083 is the standard error for this coefficient. The numbers are close but not quite the same.</p>

<p>What am I doing wrong?</p>
"
"0.125636725583038","0.119724247531067","194597","<p>I am doing a meta-regression with metafor package in R. The mixed-effect model for proportion is used to assess the linearity between study performed year and medication prevalence. Here below is my script in R:</p>

<pre><code>model_A &lt;- rma.glmm(xi=A, ni=Sample, measure=""PLO"", mods=~year)
print(model_A)
</code></pre>

<p>And results I got from R are:</p>

<pre><code>Mixed-Effects Model (k = 32; tau^2 estimator: ML)

tau^2 (estimated amount of residual heterogeneity):     1.6349
tau (square root of estimated tau^2 value):             1.2786
I^2 (residual heterogeneity / unaccounted variability): 99.40%
H^2 (unaccounted variability / sampling variability):   168.00

Tests for Residual Heterogeneity: 
Wld(df = 30) = 2221.4535, p-val &lt; .0001
LRT(df = 30) = 3187.7073, p-val &lt; .0001

Test of Moderators (coefficient(s) 2): 
QM(df = 1) = 22.7322, p-val &lt; .0001

Model Results:

          estimate        se     zval    pval      ci.lb      ci.ub
intrcpt  -554.8145  116.4605  -4.7640  &lt;.0001  -783.0728  -326.5561  ***
year        0.2767    0.0580   4.7678  &lt;.0001     0.1630     0.3905  ***

---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>Followed by this model, I would also like to perform a scatterplot in R. So my script is:</p>

<pre><code>wi &lt;- 0.5/sqrt(dat$vi)
preds &lt;- predict(model_A, transf = transf.ilogit, addx=TRUE)
plot(year, transf.ilogit(dat$yi), cex=wi)
lines(year, preds$pred)
</code></pre>

<p>The plot I got is: 
<a href=""http://i.stack.imgur.com/7Ej3P.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7Ej3P.png"" alt=""enter image description here""></a></p>

<p>Apparently, it doesn't seem right!. So my questions are:</p>

<ol>
<li><p>Did I use the right model with <code>rma.glmm</code>?</p></li>
<li><p>How could I weight individual study (<code>cex=wi</code>?)? How to calculate standard error for individual study?</p></li>
<li><p>How could I fit a right estimated line in scatterplot?</p></li>
</ol>

<p>Many thanks.</p>

<p>Updates:</p>

<p>Followed by Wolfgang's suggestions, I managed to rescale the bubble and get predicted line fitted (the model remains the same):</p>

<p><a href=""http://i.stack.imgur.com/u8N0t.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/u8N0t.png"" alt=""enter image description here""></a></p>

<p>Obviously, the line wasn't straight! Should I change model into polynomial regression? Or is that normal with this graph?</p>

<p>I tried polynomial model like:</p>

<blockquote>
  <p>model1&lt;-rma.glmm(xi=A, ni=Sample, measure=""PLO"", mods=~year+I(year^2))</p>
</blockquote>

<p>The error came with ""Error in print(model1) : 
  error in evaluating the argument 'x' in selecting a method for function 'print': Error: object 'model1' not found""</p>

<p>And I tried another model:</p>

<blockquote>
  <p>model2: model2&lt;-rma.glmm(xi=A, ni=Sample, measure=""PLO"", mods=~year+year^2)</p>
</blockquote>

<p>I got exactly the same result as original model, which has only the year as covariate fitted. I am not sure where the problem is....</p>

<p>Many thanks!</p>

<p>Min</p>
"
"0.105555962793852","0.0922061136661462","195120","<p>I recently ran a beta regression model in R using the <code>betareg</code> package. I am modeling a continuous dependent variable (a fraction out of 1) that is bound between 0 and 1, as a function of a continuous variable that only takes on positive values. Model code, and code to generate residual vs. fitted is here:</p>

<pre><code>fit &lt;- betareg(y ~ x, data=d)
plot(residuals(fit) ~ fitted(fit))
</code></pre>

<p>The residual vs. fitted plot looks like this:
<a href=""http://i.stack.imgur.com/5hVHy.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5hVHy.png"" alt=""enter image description here""></a></p>

<p>So like... what is going on here. Is this normal for beta regression, or have I mis-specified my model somehow?</p>

<p>Histogram of dependent variable, <code>y</code>:
<a href=""http://i.stack.imgur.com/bB2wK.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bB2wK.png"" alt=""enter image description here""></a></p>

<p>Histogram of independent variable, <code>x</code>:
<a href=""http://i.stack.imgur.com/8T7gq.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/8T7gq.png"" alt=""enter image description here""></a></p>

<p>output from <code>summary(fit)</code></p>

<pre><code>Call:
betareg(formula = relEM ~ mat, data = d1)

Standardized weighted residuals 2:
    Min      1Q  Median      3Q     Max 
-2.0716 -0.3940 -0.1730  0.4468  2.0633 

Coefficients (mean model with logit link):
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.863468   0.062820  13.745   &lt;2e-16 ***
mat         -0.053734   0.005667  -9.482   &lt;2e-16 ***

Phi coefficients (precision model with identity link):
      Estimate Std. Error z value Pr(&gt;|z|)    
(phi) 0.261182   0.005735   45.54   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Type of estimator: ML (maximum likelihood)
Log-likelihood: 1.156e+04 on 3 Df
Pseudo R-squared: 0.03853
Number of iterations: 14 (BFGS) + 1 (Fisher scoring) 
</code></pre>
"
"0.139637413476259","0.126729582400196","195359","<p>I have a set of complex survey data with sampling weights. I am using the <code>svyglm()</code> function from the <code>survey</code> package in R to describe the relationship between 2 variables in a GLM. I am using the quasipoisson family because both variables are over-dispersed. </p>

<p>The GLM output is as follows:</p>

<pre><code>hlsereg &lt;- svyglm(formula = HLSEPALLACRESFIX ~ HLSE_ACRE, sbdiv, family = quasipoisson)

Survey design:
svydesign(id = ~1, weights = ~spwgtdividedby3, data = sportsbind)

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  5.489465   0.414979  13.228   &lt;2e-16 ***
HLSE_ACRE   -0.002744   0.001118  -2.454   0.0144 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for quasipoisson family taken to be 2.601914e+15)

Number of Fisher Scoring iterations: 12
</code></pre>

<p>I have used the <code>predict()</code> and <code>lines()</code> function to plot this model output:</p>

<pre><code>acreaxis &lt;- seq(0,2000,.1)
hlse = predict(hlsereg, list(HLSE_ACRE = acreaxis))
    plot(jitter(sportsbind$HLSE_ACRE,  amount = 2.5), jitter(sportsbind$HLSEPALLACRESFIX),pch = 16,  xlab = ""Acres"", ylab = ""Price per person per acre"",  xlim = c(0, 350), ylim = c(0,35), col=alpha(""red"",.35), font = 2, font.lab = 2)
    lines(acreaxis, hlse, lwd=4, col = ""red"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/3EUZ6.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/3EUZ6.png"" alt=""enter image description here""></a></p>

<p>This plots a line given by the regression output of an intercept at 5.5 and a very slow negative slope of -.003, but I'm uncertain if this is a correct representation of the line.</p>

<p>I have found others using the <code>predict(..., type = ""response"")</code> option, which is shown in various plots of quasipoisson models, including the one found by @Glen_b at <a href=""http://stats.stackexchange.com/a/177926/45582"">this question</a> and for <a href=""http://stats.stackexchange.com/questions/38201/problems-plotting-glm-data-of-binomial-proportional-data?rq=1"">binomial GLMs here</a>. The <code>predict.glm()</code> help page notes for the <code>type</code> argument that: ""The default is on the scale of the linear predictors; the alternative ""response"" is on the scale of the response variable."" I just don't understand what that means.  The ""response"" type yields a very different prediction line, which is curved and at a much higher value (note the scale of the y-axis, with an intercept at ~250):</p>

<pre><code>hlse = predict(hlsereg, list(HLSE_ACRE = acreaxis), type = ""response"")
plot(jitter(sportsbind$HLSE_ACRE,  amount = 2.5), jitter(sportsbind$HLSEPALLACRESFIX),pch = 16,  xlab = ""Acres"", ylab = ""Price per person per acre"",  xlim = c(0, 350), ylim = c(0,400), col=alpha(""red""),     font = 2, font.lab = 2)
lines(acreaxis, hlse, lwd=4, col = ""black"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/jnY9T.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/jnY9T.png"" alt=""enter image description here""></a></p>

<p>I have also tried to run a GLM using the negative binomial distribution, but despite inputting the quasipoisson coefficient values for starting values, the model can't find valid coefficients (I have purged all zeros from the data):</p>

<pre><code> hlsereg.nb &lt;- glm.nb(HLSEPALLACRESFIX~HLSE_ACRE,data = model.frame(sbdiv.scaledweights), start = c(5.45, -.003))
Error: no valid set of coefficients has been found: please supply starting values
In addition: Warning message:
glm.fit: fitted rates numerically 0 occurred 
</code></pre>

<p>My questions:</p>

<p>1) What is the most appropriate illustration of the GLM output from a quasipoisson family?<br>
2) If the negative binomial is more appropriate to describe this relationship, why can't it find a coefficient? If I figure out how to get it to find a coefficient, how would I visualize that output?</p>
"
"0.0457070726502004","0.0580747904139041","195478","<p>Resampling is usually used to find the best tuning parameters for a model. However, for some models, such as linear regression model, there is no tuning parameters. In this case, what can we get from resampling on them?</p>

<p>In particular, in R caret package, you can train a linear regression model by using cross validation control function. In this case, how is the coefficient estimated? On the whole training sample? If so, what extra information can we get from applying CV on linear regression models?</p>

<p>Thank you.</p>
"
"0.0575854987567582","0.076825726438694","196630","<p>When estimating a Fixed Effects model on panel data and an equivalent dummy variables regression, the coefficient estimates and associated SEs are identical. However, the R-squared and F-statistic are noticeably different (e.g. R-sq from dummy regression is usually much higher than R-sq from FE specification). </p>

<p>Once we obtain the R-squared &amp; F-stat from the dummy variables regression, how can one adjust them to retrieve the same results as from the FE specification? </p>

<p>Consider this example: </p>

<pre><code>library(foreign);library(plm);library(stargazer)
wagepan&lt;-read.dta(""http://fmwww.bc.edu/ec-p/data/wooldridge/wagepan.dta"")

# Generate pdata.frame:
wagepan.p &lt;- pdata.frame(wagepan, index=c(""nr"",""year"") )

# Estimate FE parameter in 3 different ways:
wagepan.p$yr&lt;-factor(wagepan.p$year)

# Estimate dummy vars and FE models
reg.fe &lt;-(plm(lwage~married+union+yr*educ,data=wagepan.p, model=""within""))
reg.dum&lt;-( lm(lwage~married+union+yr*educ+factor(nr), data=wagepan.p))

stargazer(reg.fe,reg.dum,type=""text"",model.names=FALSE,
      keep=c(""married"",""union""),omit.stat=c(""ser""),
      column.labels=c(""Within"",""Dummies""))
</code></pre>

<p>Which will yield: </p>

<pre><code>=================================================================
                             Dependent variable:                 
             ----------------------------------------------------
                                    lwage                        
                      Within                    Dummies          
                        (1)                       (2)            
-----------------------------------------------------------------
married              0.055***                   0.055***         
                      (0.018)                   (0.018)          

union                0.083***                   0.083***         
                      (0.019)                   (0.019)          

-----------------------------------------------------------------
Observations           4,360                     4,360           
R2                     0.171                     0.616           
Adjusted R2            0.049                     0.560           
F Statistic  48.907*** (df = 16; 3799) 10.900*** (df = 560; 3799)
=================================================================
Note:                                 *p&lt;0.1; **p&lt;0.05; ***p&lt;0.01
</code></pre>

<p>How can I adjust the <code>Model 2</code> R2 and F-stat (<code>0.616</code> and <code>10.9</code>, respectively) to retrieve the same figures as in <code>Model 1</code> (<code>0.171</code> and <code>48.9</code>)? </p>
"
"0.0977258320053917","0.108647984268766","196901","<p>I'm trying to figure out how to find the marginal effect of an interaction term from a restricted cubic spline in a non-linear model.  The post <a href=""http://stats.stackexchange.com/questions/134526/nonlinear-effect-in-an-interaction-term"">Nonlinear effect in an interaction term</a> is a good start on modeling the nonlinear effects and how to get plots, but does not address finding the marginal effect.  </p>

<p>The package <a href=""http://maartenbuis.nl/software/postrcspline.html"" rel=""nofollow"">postrcspline</a> in <code>STATA</code> has a function <a href=""http://repec.org/bocode/m/mfxrcspline.html"" rel=""nofollow"">mfxrcspline</a> which ""displays the marginal effect of a restricted cubic spline,""
 which is exactly what I am after. (See Figure 1 below)  </p>

<p>R does not seem to offer this feature as conveniently ,so I'm trying to figure out how to get these same results.</p>

<p>As I understand it, suppose I have a multi-variable regression with restricted cubic splines and an interaction:</p>

<p>$$y = \beta_{0} + \beta_{1}x1 + \beta_{2} \mathcal{f}(x2) + \beta_{3} \mathcal{f}(x2) \cdot x1 + \epsilon$$</p>

<p>where $\mathcal{f}(x2)$ is a spline of the time-series (year)</p>

<p>The marginal effect of $\frac{\partial y}{\partial x1}$ is:</p>

<p>$$\frac{\partial y}{\partial x1} = \beta_{1} + \beta_{3} \mathcal{f}(x2)$$</p>

<p>where $\beta_{3}$ is the coefficient on the spline and $ \mathcal{f}(x2)$ is a design matrix for each year in the regression that causes the slope to change for each $y$.  </p>

<p>To say in words, I would like to find the marginal effect of $y$ for each year $x2$ in the spline given $\beta_{3}$.  </p>

<p>In other words, it shows for each value of the spline variable how much the expected value of your explained variable changes for a unit change in the spline variable. It is the first derivative of the curve.</p>

<p>This appears to be simple matrix multiplication to plot the marginal effect, but I'm not sure how to statistically do this.  </p>

<p>Here is a plot to illustrate what I'm after:</p>

<p><strong>Figure 1:</strong> The left plot shows the results of the regression using a restricted cubic spline and the right provides the marginal effect--note the changes on the y-axis.
<a href=""http://i.stack.imgur.com/uqcX4.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/uqcX4.png"" alt=""Figure 1""></a></p>

<hr>

<p>Here is an R example to demonstrate the nonlinear effect from the regression (left plot in Figure 1):</p>

<pre><code>library(rms)
set.seed(5)
# Fit a complex model and approximate it with a simple one
x1 &lt;- runif(200)
x2 &lt;- runif(200)
y &lt;- x1 + x2 + rnorm(200)
f &lt;- ols(y ~ x1 + rcs(x2,4)  + rcs(x2,4)*x1)
ddist &lt;- datadist(x1,x2)
options(datadist='ddist')
plot(Predict(f))
</code></pre>

<p><a href=""http://i.stack.imgur.com/DAuXS.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/DAuXS.png"" alt=""enter image description here""></a></p>
"
"0.0609427635336005","0.0580747904139041","196939","<p>I'm relatively new to R and statistics and am trying to figure out how to do a multi-period, multi-variate lagged regression.</p>

<p>The dependent variable depends <strong>purely</strong> on the past n values of each of the independent variables; it does not depend on the present values of the dependent variables, nor on the past values of the independent variable.</p>

<p>I am struggling with two things: </p>

<p>(a) Finding a way to build it in R that will easily generalise to ~20 variables and ~15 time-periods. The best I've gotten to is like the <code>dynlm</code> statement in the below chunk of code. Is there a better way?</p>

<pre><code>x1 &lt;- rnorm(100)
x2 &lt;- rnorm(100)
x3 &lt;- rnorm(100)

y_ &lt;- 0.5 * x1[1:97] + 0.3 * x2[1:97] + 0.2 * x3[1:97] + 0.4 * x1[2:98] + 0.2 * x2[2:98] + 0.1 * x3[2:98] + 0.3 * x1[3:99] + 0.1 * x2[3:99] + 0.001 * x2[3:99]

y &lt;- c(0,0,0,y_)
df &lt;- cbind(x1, x2, x3, y)

dynlm_model_1 &lt;- dynlm(y ~ L(x1,1) + L(x1,2) + L (x1,3) + L(x2,1) + L(x2,2) + L(x2,3) + L(x3,1) + L(x3,2) + L(x3,3), df)
dynlm_model_1
</code></pre>

<p>(b) This model returns a result that is very far from the formula I used to construct the dependent variable. (I did not even use a error term.) What am I doing wrong?</p>

<pre><code>Time series regression with ""zoo"" data:
Start = 1, End = 100

Call:
dynlm(formula = y ~ L(x1, 1) + L(x1, 2) + L(x1, 3) + L(x2, 1) + 
    L(x2, 2) + L(x2, 3) + L(x3, 1) + L(x3, 2) + L(x3, 3), data = df)

Coefficients:
(Intercept)     L(x1, 1)     L(x1, 2)     L(x1, 3)     L(x2, 1)     L(x2, 2)     L(x2, 3)     L(x3, 1)     L(x3, 2)  
   -0.26415      0.18877           NA           NA      0.01039           NA           NA     -0.17065           NA  
   L(x3, 3)  
         NA
</code></pre>

<p>Let me know if any additional information is required. Thanks in advance!</p>
"
"0.0914141453004008","0.0871121856208561","197488","<p>How would you go about making an unbiased comparison of two interventions (old vs. New <em>prtcl.binary</em> (0 and 1; individual worksheets vs. group work).) when there's a negative longitudinal slope present (longitudinal meaning the data is measured over time, but not repeatedly since each student only had one measurement)?
<a href=""http://i.stack.imgur.com/jRP29.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/jRP29.png"" alt=""enter image description here""></a></p>

<p>The switch happened in 2014 (NOTE: there's little data for 2015, ~5 cases, so you can pretty much group it with 2014) But the mean comparison seems a little questionable when the previous intervention was having an effect already, see the negative slope before 2014.</p>

<p>So if my outcome is a count (i.e., number of days student did Y behavior), that is overdispersed (so I'm using a negative binomial dist.)</p>

<p>I was concerned with just comparing the one protocol to another using <code>prtcl.binary</code> as a dummy variable, so I added <code>year</code> to adjust the means for the the linear effect (black line) and an interaction between <code>year</code> and <code>prtcl.binary</code>, the differential, to compare the slopes. But I'm not sure this fixes my problem with interpreting the main effect of <code>prtcl.binary</code>.</p>

<pre><code>MASS::glm.nb(formula = y.count ~ 1 + prtcl.binary + year + prtcl.binary : year, data = d0)
</code></pre>

<p>Someone told me this is a good case for detrending data (but I'm not fond of the idea of interpreting the regression coefficients of residuals; i.e., I'm dumb). Someone else suggested I used a mixed model and use year as a random effect.
I thought it might make sense to use the 2008 to 2013 data to predict a value for 2014 with a standard error and then compare it with the mean and standard error measured in just 2014.</p>

<p>Suggestions?</p>
"
"0.0609427635336005","0.0435560928104281","197566","<p>I have 2 dependent variables which depend upon on 5 independent variables. So, I performed multivariate multiple linear regression in R and got the coefficients for my variable of interest for both the dependent variable. For one dependent variable, I got a coefficient of -4 and for the other dependent variable for the same variable of interest, I got a coefficient of 8. These are hypothetical numbers. I want to statistically compare if there is any relationship between 2 dependent variables. What is the way to do that in R?</p>

<p>Any help is appreciated. Thank you.</p>
"
"0.052777981396926","0.0502942438178979","197634","<p>What is the correct way to compare correlation between 2 dependent variables in R?</p>

<p>Thanks</p>

<p>Edit:
Here is the edited question and apologize for not asking this correctly before:</p>

<p>I acquired 2 measures from 2 different experiments and I want to know whether these 2 measures are correlated. </p>

<p>The problem is Measure 1 is confounded by some other covariates. So, I went ahead and did multiple regression and found the coefficient of my main effect. Since this is an estimate it has a mean with a deviation.</p>

<p>I could do cor(mean(parameter_estimate, Measure_2)) but I need to know if this correlation is significant. Is this right as I dont incorporate the spread of the estimate (variance)? My guess is the mean may be significant but with the standard error of the estimate, the correlation may become insignificant.</p>

<p>Thank you for your help</p>

<p>Regards</p>
"
"0.0914141453004008","0.0774330538852055","198268","<p>I'm am trying to predict disease states in a medical setting where I have three subject groups (1,2,3). I have cross-validated a multinomial logistic regression model using the following</p>

<pre><code>cvfit=cv.glmnet(Xtrain, ytrain, family=""multinomial"", type.multinomial = ""grouped"", parallel = TRUE, standardize=TRUE)
</code></pre>

<p>where Xtrain is a 42x20 matrix with 42 observations and 20 predictors.</p>

<p>If I run the following to get the coefficients of the model</p>

<pre><code>coef(cvfit)
</code></pre>

<p>I get the following output</p>

<pre><code>$`1`
21 x 1 sparse Matrix of class ""dgCMatrix""
                    1
(Intercept)  2.519025
V1           2.955347
V2           .       
V3           .       
V4          -3.508274
V5           .       
V6           .       
V7           .       
V8           .       
V9           .       
V10          .       
V11          .       
V12          .       
V13          .       
V14          .       
V15          .       
V16          .       
V17          .       
V18          .       
V19          .       
V20         -2.108070

$`2`
21 x 1 sparse Matrix of class ""dgCMatrix""
                     1
(Intercept)  1.5460376
V1          -5.2882709
V2           .        
V3           .        
V4           0.4144632
V5           .        
V6           .        
V7           .        
V8           .        
V9           .        
V10          .        
V11          .        
V12          .        
V13          .        
V14          .        
V15          .        
V16          .        
V17          .        
V18          .        
V19          .        
V20          1.4674672

$`3`
21 x 1 sparse Matrix of class ""dgCMatrix""
                     1
(Intercept) -4.0650622
V1           2.3329236
V2           .        
V3           .        
V4           3.0938106
V5           .        
V6           .        
V7           .        
V8           .        
V9           .        
V10          .        
V11          .        
V12          .        
V13          .        
V14          .        
V15          .        
V16          .        
V17          .        
V18          .        
V19          .        
V20          0.6406032
</code></pre>

<p>I would like to be able to say something concerning the risk of being in one group compared to another based on increments in the predictors with non-zero coefficients, however, I cannot seem to find any information as to which class the cvglmnet() function uses as base in order to calculate the risks. </p>

<p>Does anyone know this, or have an idea on how to interpret the results for use in a model?</p>

<p><strong>EDIT:</strong></p>

<p>I realize now that I may have overlooked a crucial detail. In ""The Elements of Statistical Learning: Data Mining, Inference, and Prediction"" by Hastie, T et al (2009), it is stated on page 657 that a multiclass logistic model can be described as</p>

<p>$P(Y=k|X=x) = \frac{\exp{(\beta_{k0}+x^{T}\beta_{k})}}{\sum_{l=1}^{K}\exp{(\beta_{l0}+x^{T}\beta_{l})}}$</p>

<p>where I can see that the denominator is just a normalization factor. I guess this means that I can interpret the obtained coefficients above directly for each subject group. Or is this wrongly interpreted?</p>
"
"0.110147477177496","0.104963924850184","198315","<p>I am currently getting slightly confused with how a rolling forecast should be setup in R, as in how the data should be organised in order to <em>train</em> and <em>test</em> my model. I feel there is a large gap in my understanding somewhere.</p>

<p>I have seen many examples of forecasting methods, but not many on time-series (using lagged variables) that go into detail, i.e. perform everything manually using <code>predict()</code>. Instead it normally just points to a built in R package. <strong>My question has to do with the training of the model - the alignment of the data for the training.</strong></p>

<p>I know that the regression equation (assuming I am performing a linear regression) looks like this:</p>

<p>$$ y_t = \beta_1 y_{t-1} + \beta_2 y_{t-2} + \beta_3 x_{t-1} + \beta_4 x_{t-2} + \varepsilon_t $$ </p>

<p>So I have my outcome variable, $y$, being explained by two of its own lagged values, plus two lagged values of a second variable, $x$. Each variable has its own coefficient, all of which my model is estimating.</p>

<p>Let's say I have a <em>data.table</em> (or data.frame), where each row consists of the data aligned according to the equation above. Each row is one day, and represents the given equation. I have five columns, and for this example say 200 rows/days.</p>

<blockquote>
  <p>Day  |  $y_t$  |  $y_{t-1}$  |  $y_{t-2}$  |  $x_{t-1}$  |  $x_{t-2}$</p>
  
  <p>001  | <em>Values</em> --></p>
  
  <p>002  | <em>Values</em> --></p>
  
  <p>003  | <em>Values</em> --></p>
</blockquote>

<p>I use the above equation as the formula to fit my model to obtain estimates for the four coefficients (neglecting $\varepsilon$) using a fixed 40-day time frame.</p>

<pre><code>model &lt;- lm(y ~ y_1 + y_2 + x_1, x_2,            ## my regression formula
            data = input_data[1:40])             ## my data.table 
</code></pre>

<p>Now I want to make a prediction using the fitted <code>model</code>.
I do this by using <code>predict()</code> in <strong>R</strong> as follows:
I take the next (41st) row of data, minus the outcome variable</p>

<pre><code>my_pred &lt;- predict(model, newdata = input_data[41][, outcome := NULL])
</code></pre>

<p>And then calculate my error:</p>

<pre><code>my_error &lt;- input_data[41, outcome] - my_pred
</code></pre>

<p>I then shift everything forward one row, so still a 40-day frame <code>input_data[2:41]</code> updating the coefficients for all variables and predicting the following outcome variable, <code>input_data[42]</code>. This is yielding terrible results for my model, with overall accuracies not much better than a naÃ¯ve forecast, i.e. random guessing.</p>

<p>Should I realign the data for the training segment, so that each row rather represents the data I had on that day? This would mean adding one more column, $x_t$.</p>

<p>Any other suggestions or comments?</p>

<p>Thanks.</p>
"
"0.114013470672957","0.108647984268766","198372","<p>I have a series of single-armed trials where the outcome is a binary response. Imagine a trial where you have no control arm; you merely give 100 patients a procedure (which can be done in many different ways) and see how many are 'well' (more later) at the end of the year. There are hundreds of these trials for me to look at.</p>

<p>I believe I can meta-analyse these as big group as follows, assuming x is the number well, n is n, and they're in df.</p>

<pre><code>model &lt;- rma(measure=""PLO"", xi=x, ni=n, data=df) #PLO = logit transformed proportion (log odds)
print(res, digits=3) #This will print the log odds
predict(model, transf=transf.ilogit, digits=3) #This will back-transform with the inverse logit transformation
</code></pre>

<p>I can plot this quite nicely with:</p>

<pre><code>forest(model,transf=transf.ilogit)
</code></pre>

<p>The thing is, as alluded to, there are lots of different ways to do the procedure and lots of different classifications of whether the patient is 'well'.</p>

<p>I want to do meta-regression/MV analysis on these trials (I may have over 100) to see if the characteristics of the trial predict the outcomes significantly.</p>

<p>I've done a lot of reading e.g <a href=""http://www.metafor-project.org/doku.php/tips:regression_with_rma"" rel=""nofollow"">http://www.metafor-project.org/doku.php/tips:regression_with_rma</a> but my problem is all the examples of meta-regression seem to treat each 'row' equally, when of course they should be weighted by n.</p>

<p>I was wondering if it would be valid to supply my predictors in question merely via the mods argument and otherwise performing the analysis as I did for the meta-analysis, e.g.:</p>

<pre><code>model_2 &lt;- rma(measure=""PLO"", xi=x, ni=n, data=df, mods=~predictor1 + predictor2 + predictor3)
</code></pre>

<p>If I do I end up with something like:</p>

<pre><code>Mixed-Effects Model (k = 60; tau^2 estimator: REML)

tau^2 (estimated amount of residual heterogeneity):     0.3651 (SE = 0.0908)
tau (square root of estimated tau^2 value):             0.6042
I^2 (residual heterogeneity / unaccounted variability): 81.40%
H^2 (unaccounted variability / sampling variability):   5.38
R^2 (amount of heterogeneity accounted for):            0.00%

Test for Residual Heterogeneity: 
QE(df = 57) = 311.1484, p-val &lt; .0001

Test of Moderators (coefficient(s) 2,3): 
QM(df = 2) = 0.2739, p-val = 0.8720

Model Results:

                      estimate      se     zval    pval    ci.lb   ci.ub     
intrcpt                  1.1155  0.2997   3.7220  0.0002   0.5281  1.7030  ***
predictor1               0.0974  0.2763   0.3525  0.7244  -0.4441  0.6390     
predictor2              -0.0818  0.2085  -0.3923  0.6949  -0.4905  0.3269  
</code></pre>

<p>1) Is this the appropriate way of doing this?</p>

<p>2) Also, when I used to do patient-level multivariate regression, my practice was to include variables in the multivariate analysis if they were significant on univariate analysis; is this standard practice for my example, too? As in should I supply them individually as single <code>mods=~predictor</code> and look for significance before including them in a model?</p>

<p>Thank you</p>
"
"0.129458553897935","0.116873261230088","198484","<p>Consider this example:</p>

<pre><code>foo &lt;-data.frame(x=c(0.010355057,0.013228936,0.016313905,0.019261687,0.021710159,0.023973474,0.025968176,0.027767232,0.029459730,0.030213807,0.023582566,0.008689883,0.006558429,0.005144958),
                 y=c(971.3800,1025.2271,1104.1505,1034.2607,902.6324,713.9053,621.4824,521.7672,428.9838,381.4685,741.7900, 979.7046,1065.5245,1118.0616))
Model3 &lt;- lm(y~poly(x,3),data=foo)
Model4 &lt;- lm(y~poly(x,4),data=foo)
</code></pre>

<p>For <code>Model3</code>, the <code>poly(x,3)</code> term is not significant:</p>

<pre><code>&gt; summary(Model3)

Call:
lm(formula = y ~ poly(x, 3), data = foo)

Residuals:
   Min     1Q Median     3Q    Max 
-76.47 -51.61  -0.55  38.22 100.57 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   829.31      17.85  46.463 5.14e-13 ***
poly(x, 3)1  -819.37      66.78 -12.269 2.37e-07 ***
poly(x, 3)2  -373.05      66.78  -5.586 0.000232 ***
poly(x, 3)3   -87.85      66.78  -1.315 0.217740    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 66.78 on 10 degrees of freedom
Multiple R-squared:  0.9483,    Adjusted R-squared:  0.9328 
F-statistic: 61.15 on 3 and 10 DF,  p-value: 9.771e-07
</code></pre>

<p>However, for <code>Model4</code> it is:</p>

<pre><code>&gt; summary(Model4)

Call:
lm(formula = y ~ poly(x, 4), data = foo)

Residuals:
    Min      1Q  Median      3Q     Max 
-34.344 -19.982   1.229  18.499  33.116 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  829.310      7.924 104.655 3.37e-15 ***
poly(x, 4)1 -819.372     29.650 -27.635 5.16e-10 ***
poly(x, 4)2 -373.052     29.650 -12.582 5.14e-07 ***
poly(x, 4)3  -87.846     29.650  -2.963 0.015887 *  
poly(x, 4)4  191.543     29.650   6.460 0.000117 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 29.65 on 9 degrees of freedom
Multiple R-squared:  0.9908,    Adjusted R-squared:  0.9868 
F-statistic: 243.1 on 4 and 9 DF,  p-value: 3.695e-09
</code></pre>

<p>Why does this happen? Note that the estimate of all coefficients is the same in both cases, since the polynomials are orthogonal. However, the significance is not. This seems to me difficult to understand: if I performed a degree 3 regression, it looks like I could drop the <code>poly(x, 4)3</code> term, thus reverting to a degree 2 orthogonal regression. However, if I performed a degree 4 regression, I shouldn't, even though the coefficients of the common terms have exactly the same estimate. What do I conclude? Probably that one should never trust subset selection :) An <code>anova</code> analysis says that the difference among the degree 2, degree 3 and degree 4 models is significant:</p>

<pre><code>&gt; Model2 &lt;- lm(y~poly(x,2),data=foo)     
&gt; anova(Model2,Model3,Model4)
Analysis of Variance Table

Model 1: y ~ poly(x, 2)
Model 2: y ~ poly(x, 3)
Model 3: y ~ poly(x, 4)
  Res.Df   RSS Df Sum of Sq       F    Pr(&gt;F)    
1     11 52318                                   
2     10 44601  1      7717  8.7782 0.0158868 *  
3      9  7912  1     36689 41.7341 0.0001167 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>EDIT: following a suggestion in comments, I add the residual vs fitted plots for <code>Model2</code>, <code>Model3</code> and <code>Model4</code></p>

<p><a href=""http://i.stack.imgur.com/9ZU8h.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9ZU8h.png"" alt=""enter image description here""></a>` </p>

<p>It's true that the maximum residual error is more or less the same for <code>Model2</code> and <code>Model3</code>, and it becomes nearly one third going from <code>Model3</code> to <code>Model4</code>. There seems to be still some kind of trend in the residuals, though it is less evident than for <code>Model2</code> and <code>Model3</code>. However, why does this invalidate the <em>p</em>-values? Which hypothesis of the linear model paradigm is violated here? I seem to remember that the residuals only had to be uncorrelated with the predictor. However, if they also have to uncorrelated among themselves, then clearly this assumption is violated and the <em>p</em>-values based on the t-test are invalid.</p>
"
"0.0304713817668003","0","198655","<p>Can anyone please explain the identity link and log link in Poisson regression with simple example? For example, I have run a script using the <code>mtcars</code> data set in R.  </p>

<pre><code>head(mtcars) 
glm(mpg~disp+hp+wt, data=mtcars, family=poisson(link=""â€Œâ€‹identity""))
</code></pre>

<p><code>mpg</code>, <code>disp</code>, and <code>wt</code> are variables in the data set. I got the results:  </p>

<pre><code>(Intercept)= 35.205669
Coefficient of disp=-0.004313
Coefficient of hp=-0.028214
Coefficient of wt=-3.102409
</code></pre>

<p>How do I interpret these coefficients? How does the model look like mathematically? What does <code>link=""identity""</code> mean in the model?</p>
"
"0.105555962793852","0.0922061136661462","198801","<p>Logistic Regression using R's <code>glm</code> package is giving me the following summary (snap of the few first variables). </p>

<p><strong>My Data Set:</strong></p>

<ul>
<li>Dimensions: 1252 rows and 224 columns (after using model.matrix). The Data  has been standardized.</li>
<li>Response variable is binary.</li>
<li>Trying to predict if an employee will leave the company, based on employee attributes </li>
</ul>

<p><a href=""http://i.stack.imgur.com/Ai42v.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Ai42v.png"" alt=""enter image description here""></a></p>

<p><strong>My Understanding:</strong></p>

<p>The model does not give a good fit because:</p>

<ol>
<li>Residual Deviance > Null Deviance. </li>
<li>p.value = 1 - pchisq(3676.5, 817) turns out to be 0.</li>
<li>The first warning about 0 or 1 fitted probability message suggests that due to some predictor(s) the model might be giving perfect predictions </li>
<li>Second warning on â€˜rank deficiencyâ€™ suggests that there might be predictors that are linearly dependent on one another.</li>
</ol>

<p><strong>My Questions:</strong></p>

<ol>
<li>How can I improve the model? I would like to see Residual Deviance &lt; Null Deviance. I will invest time on dropping the linearly dependent variables in the model, but is there anything I should do first to test the â€˜modelâ€™ itself, before revisiting my data? I am asking this because SVM worked quite well on the same data set. </li>
<li>Why do I have such extreme coefficient values?</li>
<li>Many answers to other posts state that â€˜AICâ€™ is used to compare different</li>
<li>The summary parameters (coefficients , std error and p-values) for many dummy factors obtained via model.matrix, like GSS_SEXM,  is shown as 'NA'. Why is it so? </li>
<li>logistic models. What is meant by â€˜differentâ€™ here? Models trained on different data sets that bear different coefficients, like say different set of attributes? </li>
</ol>
"
"0.0575854987567582","0.0439004151078252","198925","<p>Although there has been some detailed discussions about power analysis on this website (for example <a href=""http://stats.stackexchange.com/questions/22406/power-analysis-for-ordinal-logistic-regression/22410#22410"">here</a> and <a href=""http://stackoverflow.com/questions/27234696/how-do-you-conduct-a-power-analysis-for-logistic-regression-in-r"">here</a>), the answer provided to this question has  outlines the steps to simulating a power analysis, <a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments"">here</a>.</p>

<p>Say we take some data (data was linked to a <a href=""http://stats.stackexchange.com/questions/59829/which-bootstrapped-regression-model-should-i-choose"">bootstrapping question</a>)</p>

<p>We create a regression that will predict <code>admit</code> based on the two continous variables <code>gpa</code> and <code>gre</code></p>

<ul>
<li>Now we have a <code>n=400</code>. </li>
<li>We can then elect our power level, <code>alpha = 0.5</code></li>
<li>The effect size you would like to detect, e.g., odds ratios  (we obtain this from our regression)</li>
</ul>

<p>So in following the detailed method provided by @gung <a href=""http://stats.stackexchange.com/questions/35940/simulation-of-logistic-regression-power-analysis-designed-experiments"">here</a>, I want to run the simulation. Here is the code I have adjusted, but my output is not correct. Can someone outline what I have not understood</p>

<pre><code>mydata &lt;- read.csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
head(mydata)

set.seed(1234)

my.mod &lt;- glm(admit ~ gre + gpa , data = mydata, family = ""binomial"")


repetitions &lt;- length(mydata$admit)

gre &lt;- mydata$gre
    gpa &lt;- mydata$gpa


significant = matrix(nrow=repetitions, ncol=4)

for(i in 1:repetitions){
  responses          = mydata$admit
      #responses          = rbinom(n=N, size=1, prob=mydata$admit)      # we can interchange this comment
  model              = glm(responses ~ gre + gpa, family = binomial(link=""logit""))
  significant[i,1:2] = (summary(model)$coefficients[2:3,4]&lt;.05)
      significant[i,3]   = sum(significant[i,1:2])
      modelDev           = model$null.deviance-model$deviance
  significant[i,4]   = (1-pchisq(modelDev, 2))&lt;.05
}



sum(significant[,1])/repetitions      # pre-specified effect power for gre

sum(significant[,2])/repetitions      # pre-specified effect power for gpa

sum(significant[,4])/repetitions  # power for likelihood ratio test of model

sum(significant[,3]==2)/repetitions   # all effects power

sum(significant[,3]&gt;0)/repetitions    # any effect power
</code></pre>
"
"0.068136080998913","0.0649295895722714","199912","<p>I have a large dataset with 4000 variables and 15000 observations. I am looking to build a predictive model using logistic regression. I believe that the glmnet package (using elastic net) is the best tool to use with such a large set of variables. Every variable of the 4000 is a moving average. I have split the dataset into two - training and testing.</p>

<p>When I run the code with glmnet I find something unusual happening. As I increase the number of variables for glmnet to select the model probabilities get more and more extreme which causes the misclassification rate to converge to 0%. I realise something is wrong but I cannot figure what it is.</p>

<p>Here is the code I have used:</p>

<pre><code>x &lt;- as.matrix(training[1:4000])
newx &lt;- as.matrix(testing[1:4000])

model &lt;- cv.glmnet(x, y, alpha = 0.5, family = 'binomial')

predict(model, type = ""coefficients"",s = model$lambda.min)
predict(model, newx, type= ""response"",s = model$lambda.min)
</code></pre>

<p>Is this overfitting?
I also read that categorical variables need to worked around with glmnet - none of the 4000 are categorical but they are grouped by external categorical vars.</p>

<p>I'm desperate for some help!</p>
"
"NaN","NaN","199970","<p>I have been provided a sample logistic regression as follows:</p>

<p><code>glm(formula = output ~ X1 + X2 + X3 + X4 + X5 + X1:term + term:X5 - 1, family=""binomial"", data=mydata)</code></p>

<p>There are a few things I'm confused by here:</p>

<p>1) What is going on with the X1:term + term:X5 terms? What do they mean in the context of glm()?</p>

<p>2) There does not seem to be an intercept term in the output under <code>Coefficients</code>. Could this be for any other reason than there simply not being an intercept term?</p>

<p>3) The AIC for the model is 50000. How should I interpret this? Can I interpret this without more models to compare to? If it is not useful, what else should I be looking for instead?</p>
"
"0.0691025985081098","0.076825726438694","200031","<p>I have very easy question that I'm hoping someone can assist me with:</p>

<p>I ran an example logistic regression using this R code:</p>

<pre><code>     hours &lt;- c(0.5, 0.75, 1, 1.25, 1.5, 1.75, 1.75, 2, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 4, 4.25, 4.5, 4.75, 5, 5.5)
        pass &lt;- c(0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1)
        data &lt;- data.frame(hours, pass)
        mylogit &lt;- glm(pass ~ hours, data = data, family = ""binomial"") #Activates the logistic regression model
        summary(mylogit) #Summary of the model

    Call:
    glm(formula = pass ~ hours, family = ""binomial"", data = data)

    Deviance Residuals: 
         Min        1Q    Median        3Q       Max  
    -1.70557  -0.57357  -0.04654   0.45470   1.82008  

    Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)  
    (Intercept)  -4.0777     1.7610  -2.316   0.0206 *
    hours         1.5046     0.6287   2.393   0.0167 *
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

    (Dispersion parameter for binomial family taken to be 1)

        Null deviance: 27.726  on 19  degrees of freedom
    Residual deviance: 16.060  on 18  degrees of freedom
    AIC: 20.06

    Number of Fisher Scoring iterations: 5

    round(exp(cbind(OR = coef(mylogit), confint(mylogit))),3)

               OR 2.5 % 97.5 %
   (Intercept) 0.017 0.000  0.281
    hours       4.503 1.698 23.223
</code></pre>

<p>I know that by taking the exponent of the log-odds/coefficients for hours the odds of passing increase by a factor of 4.503 for a one-unit change in hours.  However, given that the explanatory variable (hours) is continuous, what is considered a 'one-unit change' i.e. going from 1 to 2 hours as one unit? or from 1.75 to 1.76 hours as one unit?  Also, is this interpretation of one-unit the same for regular OLS regression as well? I'm seeking to better understand the rules R applies to creating its regression coefficients.  </p>
"
"0.143058729415231","0.13632636995676","200155","<p>Just writing with a question about fixed effects when you have panel data with</p>

<ul>
<li>More than two time periods</li>
<li>Clusters, as in individual students within schools</li>
<li>An intervention given to some of the clusters mid-way through panel</li>
</ul>

<p>In particular, I'm trying to understand the diff-in-diff model used in a particular paper, ""<em>The Effects of Targeted Recruitment and Comprehensive Supports for Low-Income High Achievers at Elite Universities: Evidence from Texas Flagships</em>,"" by Rodney Andrews, Scott Imberman, and Michael Lovenheim.
You can find a copy of their paper here: 
<a href=""https://www.msu.edu/~imberman/LOS-CS%20-%209-4-15.pdf"" rel=""nofollow"">https://www.msu.edu/~imberman/LOS-CS%20-%209-4-15.pdf</a></p>

<p>Their regression equation is at the very bottom of page 17 (which is what I'm curious about).</p>

<p>Here's a short, super simplified summary, for the sake of my question, which is really about diff-in-diff and fixed effects, rather than the particulars of the paper itself:</p>

<p>The authors have a bunch of high schools in Texas. Each of these schools has many students. The schools and students are observed for a few years. Then some of the schools receive an intervention, meant to increase students' college-going and eventually their earnings as adults. Pretend for this example it's just a college scholarship program.* We observe the students in all these schools as they complete high school (or don't), enter college (or don't), and hopefully earn adult incomes. </p>

<p>The diff-in-diff part is comparing the change in adult earnings across cohorts of students that went to schools that received the intervention, compared to the trend among student cohorts that didn't attend the intervention high schools. </p>

<p>I've created some pretend data that tries to mimic the authors' real data in greatly simplified form. You can find that here, along with my (probably wrong) R script:
<a href=""https://drive.google.com/folderview?id=0B6Sk_VEqK32Gb1M4bVQxYVIzOTQ&amp;usp=sharing"" rel=""nofollow"">https://drive.google.com/folderview?id=0B6Sk_VEqK32Gb1M4bVQxYVIzOTQ&amp;usp=sharing</a></p>

<p>(You'll need to paste the cells into excel, which I used to create the values, since google docs doesn't have the same formulas. Apologies for the inconvenience.)</p>

<p>It's got 8 variables</p>

<ul>
<li>""id"": a row index variable</li>
<li>""student"": indexes students within schools, across cohorts. So the index goes from 1 to 20 within each school. But there are only 5 students per cohort, or high school graduating class, because there are 4 time periods. (not sure if that was smart).</li>
<li>""random_uniform"": just a uniform random variable between 0.01 and 0.99. I just used this to create the next variable.</li>
<li>""test_score"": a covariate, student test score. All are normally distributed with a standard deviation of 4, and a mean that's specific to the school and graduating cohort. For interest, I made it so that some schools started with lower overall means, but each school's mean score improved a little over time (about 10 points). All the scores are around 40-60. </li>
<li>""school"": a factor variable that indicates the school. There are 4 schools.</li>
<li>""treat_indicator"": a factor variable that is 0 before the intervention, and 1 after the intervention at the schools that receive the intervention (schools 1 and 2).</li>
<li>""time_period"": a factor variable that denotes the graduating class cohort. There are 4. </li>
<li>""adult_earnings"": a numeric variable that's a function of the students' high school test score (""test_score""), plus a bunch of noise. For the kids that received the ""college scholarship"" intervention--kids in the latter 2 cohorts at schools 1 &amp; 2--I've also added an additional earnings bump between 1,000 and 2,000 dollars, to simulate a treatment effect. </li>
</ul>

<p><strong>So my question is, how do I find the true effect of the intervention, if I want to use both high school graduating cohort (time) and school (cluster) fixed effects?</strong> </p>

<p>My R script is in that shared folder, but I'm not sure it's correct. The regression equation I gave R was</p>

<pre><code>fixedreg &lt;- lm(adult_earnings ~ treat_indicator + test_score + school + time_period, 
                     data=mydata)
</code></pre>

<p>Does that model the time and cluster fixed effects correctly, and create an unbiased coefficient on the ""treat_indicator"" variable?</p>

<p>Any insight would be much appreciated. Thanks!</p>

<hr>

<p>*Or read the actual paper and laugh at my ridiculous attempt to simplify all this.</p>
"
"0.0304713817668003","0.029037395206952","200198","<p>I'm trying to build a product recommender system. I'm collecting users data from social media like number of mutual friends,age,gender,career for users u1..u50. u1 is target user and I want to apply regression and find correlation coefficient between u1-u2,u1-u3 and so on(and give more weightage to user's rating with higher correlation coefficient). Is it possible to do like this and any ideas how to implement it? </p>
"
"0.068136080998913","0.0649295895722714","200587","<p>I have a set of data I'm analyzing in R with 2 explanatory variables (X1 and X2), and one response variable (Y). </p>

<pre><code>X1&lt;-c(1,2,2,4,5,8,5,4,3,2,1,0,1,2,3,4,6,6,5,4,3,2,1,0,1,1,3,4,3,6,5,5,3,2,1)
X2&lt;-c(20,40,50,40,50,50,50,30,10,5,10,20,10,10,10,10,50,80,20,10,20,40,40,40,5,20,30,40,50,60,20,20,10,20,10)
Y&lt;-c(70,140,200,240,250,250,250,230,160,105,60,20,60,110,160,210,250,250,250,210,170,140,90,40,55,120,180,240,250,250,250,220,160,120,60)

MyData&lt;-data.frame(X1,X2,Y)
</code></pre>

<p>My goal is to calculate an equation that will allow me to predict Y based on future X1 and Y1 values. In the past I have used a linear regression like so:</p>

<pre><code>MyFit&lt;-lm(Y~X1+X2,data=MyData)
</code></pre>

<p>And then use this formula to predict Y</p>

<pre><code>Y= coefficients(MyFit)[1]+coefficients(MyFit)[2]*X1+coefficients(MyFit)[3]*X2
</code></pre>

<p>In the above dummy set, Y is strongly driven by X1. But the issue with my real data is that at some point Y gets saturated, so that further increases in X1 do not bring about increases in Y (for this data the saturation value is 250, but the actual values begin to slow down and form a saturation curve as it approaches peak value, as opposed to an absolute saturation point). The result is that the linear model will always over-predict the peak Y values. This can be seen in a plot here:</p>

<pre><code>plot(Y,type=""l"",ylim=c(0,350))
lines(X1,col='red')
lines(X2,col='blue')
lines(coefficients(MyFit)[1]+coefficients(MyFit)[2]*X1+coefficients(MyFit)[3]*X2,col='green')
</code></pre>

<p>How can I correct for this. Is it possible to do a multiple non-linear regression for this data? Or is there some other technique I can use here?</p>
"
"0.109866129394437","0.096642292914995","200708","<p>I have a data on some overall conversion rates (i.e. out of x users visiting, y buy something hence y/x is my conversion rate, essentially proportions) over a time period, now this overall proportion can be broken by if they came from channel 1, channel 2 or channel 3 and for each channel there would be again similar proportions. My objective is to see how these proportions from different channels impact the overall proportion</p>

<p>I have run a simple linear regression in R and below is the result. </p>

<pre><code>Call:
lm(formula = target_variable ~ . - date, data = data_lcr)

Residuals:
  Min        1Q    Median        3Q       Max 
-0.034173 -0.003217 -0.000704  0.002331  0.073845 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -0.0049876  0.0006139  -8.124  7.4e-15 ***
exp1         0.0785438  0.0086230   9.109  &lt; 2e-16 ***
exp2         0.0290531  0.0175517   1.655   0.0987 .  
exp3        -0.1026385  0.0080550 -12.742  &lt; 2e-16 ***
exp4         1.0760312  0.0669632  16.069  &lt; 2e-16 ***
exp5         0.2466149  0.0195844  12.592  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.007503 on 358 degrees of freedom
Multiple R-squared:  0.9843,    Adjusted R-squared:  0.9841 
F-statistic:  4503 on 5 and 358 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>The Model has great R-squared which is significant, all variables turn out to be significant. Next I am checking if my residuals are normally distributed</p>

<pre><code>&gt;  skewness(fitlm$residuals)
    [1] 2.863341
    &gt; kurtosis(fitlm$residuals)
[1] 33.83711

Shapiro-Wilk normality test

data:  fitlm$residuals
W = 0.72781, p-value &lt; 2.2e-16

Anderson-Darling normality test

data:  fitlm$residuals
A = 17.485, p-value &lt; 2.2e-16
</code></pre>

<p>These tests suggest that my residuals are not normally distributed. Should I still consider the model based on R-squared and F-Value or make some corrections? Please suggest</p>

<p>Here is the residual plot:
<a href=""http://i.stack.imgur.com/pUIfB.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pUIfB.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/ZWTux.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZWTux.png"" alt=""enter image description here""></a></p>

<p><strong>EDIT</strong>
After removing outliers:
    <a href=""http://i.stack.imgur.com/bmCBB.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bmCBB.png"" alt=""enter code here""></a></p>
"
"0.0457070726502004","0.0435560928104281","201462","<p>I'm fitting a logistic regression model with <code>patient_group</code> (0,1) as response variable and the explanatory variable being an interaction between two SNPs. When running summary for the model, the alert 'Coefficients: (1 not defined because of singularities)' is shown, and I guess it is due to the fact that the combination AACT has 0 observations. </p>

<p>My question is whether the statistics are still valid, or is there a better way to analyse this kind of data? (The SNPs are located close to each other and are most likely strongly linked.)</p>

<pre><code>&gt; table(data$SNP1, data$SNP2)    
     CC CT
  TT 27  9
  AT 83 14
  AA 47  0
&gt; model &lt;- glm(patient_group ~ SNP1 * SNP2, data=data, family=""binomial"")
&gt; summary(model)
Call:
glm(formula = patient_group ~ SNP1 * SNP2, family = ""binomial"", 
data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.2735  -0.9072  -0.7679   1.4742   1.8365  

Coefficients: (1 not defined because of singularities)
              Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)    -1.4816     0.4954  -2.991  0.00279 **
SNP1AT          0.8065     0.5471   1.474  0.14048   
SNP1AA          0.4112     0.5978   0.688  0.49158   
SNP2CT          1.7047     0.8339   2.044  0.04093 * 
SNP1AT:SNP2CT  -2.3289     1.0833  -2.150  0.03157 * 
SNP1AA:SNP2CT       NA         NA      NA       NA   

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 218.19  on 179  degrees of freedom
Residual deviance: 212.31  on 175  degrees of freedom
(26 observations deleted due to missingness)
AIC: 222.31

Number of Fisher Scoring iterations: 4
</code></pre>
"
"0.0430930413588572","0.0410650781176591","201487","<p>I have two independent variable and one dependent variable. This is my summary when I use <code>lm(y~x_1+x_2)</code>:</p>

<pre><code>Residuals:
    Min      1Q  Median      3Q     Max 
-22.265  -9.563  -1.916   6.405  39.319 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)  23.0107    18.2849   1.258  0.21407   
x_1          23.6386     6.8479   3.452  0.00114 **
x_2          -0.7147     0.3014  -2.371  0.02163 * 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 14.84 on 50 degrees of freedom
Multiple R-squared:  0.2018,    Adjusted R-squared:  0.1699 
F-statistic: 6.321 on 2 and 50 DF,  p-value: 0.00357
</code></pre>

<p>I got stuck because neither the F value and R squared are very significant. However the p-value is less than 0.05. Does it mean that y depends on both variables? What should I do next in my regression analysis?</p>
"
"0.0621994475718397","0.0711268017165705","202181","<p>I'm getting some odd coefficients when I apply <code>lm</code> to dates that have been processed and rounded using the <code>lubridate</code> package.  MWE:  </p>

<pre><code>library(ggplot2)
library(lubridate)
library(dplyr)

lakers$month &lt;- ymd(lakers$date) %&gt;% round_date(unit = 'month')
items_by_month &lt;- lakers %&gt;% group_by(month) %&gt;% summarize(count = n()) %&gt;%
    mutate(count = count / 1000)

ggplot(data = items_by_month, aes(x = month, y = count)) + 
    geom_line() +
    stat_smooth(method = 'lm', data = items_by_month)

model &lt;- lm(data = items_by_month, count ~ month)
summary(model)
time &lt;- max(items_by_month$month) - min(items_by_month$month)
coef(model)['month'] * as.numeric(time)
</code></pre>

<p>The plot indicates that <code>ggplot</code>, at least, understands what's going on with the regression model.<br>
<a href=""http://i.stack.imgur.com/HYTks.png""><img src=""http://i.stack.imgur.com/HYTks.png"" alt=""Plot with monthly totals and regression line""></a></p>

<p>But in <code>summary(model)</code> the coefficient on <code>month</code> is on the order of 10^-7, which is about 5 orders of magnitude too small:  the plot shows an increase of about 2.5 between the first and last dates, but the last line shows an increase of about 2.5 * 10^-5.  </p>

<p>Note that I've divided the <code>count</code> column by 10^3, in order to get values that are easier to read (and closer to my actual use case).  But that shouldn't effect either the plot or <code>lm</code>.  Also, I know there are more sophisticated techniques than linear regression for analyzing time series data; but I'm just looking at gross trends over time, not factor out seasonal patterns, etc.  </p>
"
"0.0806196982594614","0.076825726438694","202264","<p>I am doing a comparison between mlogit in R and statsmodels in python and have had trouble getting them to produce the same result. I'm wondering if the difference is a result of libraries or I am specifying something incorrectly. Any help would be appreciated.</p>

<p>I am using the ""TravelMode"" dataset to test the two.
In R:
</p>

<pre><code>&gt; library(""mlogit"")
&gt; library(""AER"")
&gt; data(""TravelMode"", package=""AER"")
&gt; write.csv(TravelMode, ""travelmode.csv"")
&gt; TM &lt;- mlogit.data(TravelMode, choice = ""choice"", shape = ""long"", 
                    chid.var = ""individual"", alt.var = ""mode"", drop.index = TRUE)
&gt; TMlogit = mlogit(mFormula(choice ~ vcost), TM)
&gt; summary(TMlogit)
Call:
mlogit(formula = mFormula(choice ~ vcost), data = TM, method = ""nr"", 
    print.level = 0)

Frequencies of alternatives:
    air   train     bus     car 
0.27619 0.30000 0.14286 0.28095 

nr method
4 iterations, 0h:0m:0s 
g'(-H)^-1g = 0.000482 #'
successive function values within tolerance limits 

Coefficients :
                    Estimate Std. Error t-value  Pr(&gt;|t|)    
train:(intercept) -0.3885180  0.2622157 -1.4817 0.1384272    
bus:(intercept)   -1.3712065  0.3599380 -3.8096 0.0001392 ***
car:(intercept)   -0.8711172  0.3979705 -2.1889 0.0286042 *  
vcost             -0.0138883  0.0055318 -2.5106 0.0120514 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Log-Likelihood: -280.54
McFadden R^2:  0.011351 
Likelihood ratio test : chisq = 6.4418 (p.value = 0.011147)
</code></pre>

<p>In statsmodels:
</p>

<pre><code>&gt; import pandas as pd
&gt; import statsmodels.formula.api as smf
&gt; TM = pd.read_csv('travelmode.csv')
&gt; TM = pd.concat([TM, pd.get_dummies(TM['mode'])], axis=1)
&gt; TMlogit = smf.mnlogit('choice ~ train + bus + car + vcost -1', TM)
&gt; TMlogit_fit = TMlogit.fit()
Optimization terminated successfully.
         Current function value: 0.550273
         Iterations 6
&gt; TMlogit_fit.summary()
&lt;class 'statsmodels.iolib.summary.Summary'&gt;
""""""
                          MNLogit Regression Results                          
==============================================================================
Dep. Variable:                      y   No. Observations:                  840
Model:                        MNLogit   Df Residuals:                      836
Method:                           MLE   Df Model:                            3
Date:                Thu, 17 Mar 2016   Pseudo R-squ.:                 0.02145
Time:                        15:04:48   Log-Likelihood:                -462.23
converged:                       True   LL-Null:                       -472.36
                                        LLR p-value:                 0.0001497
=================================================================================
y=choice[yes]       coef    std err          z      P&gt;|z|      [95.0% Conf. Int.]
---------------------------------------------------------------------------------
train            -0.3249      0.172     -1.891      0.059        -0.662     0.012
bus              -1.4468      0.205     -7.070      0.000        -1.848    -1.046
car              -0.7247      0.157     -4.603      0.000        -1.033    -0.416
vcost            -0.0105      0.002     -6.282      0.000        -0.014    -0.007
=================================================================================
""""""
</code></pre>

<p>I would think the values of the coefficients would be closer to each other when comparing between the two models. Any help would be appreciated.</p>
"
"NaN","NaN","202642","<p>I want to estimate the following equation in R:</p>

<pre><code>W=(1-p)*(Î±*X + Î²*Y) + p*Z+Ïµ
</code></pre>

<p>It has two coefficients for the variables <code>X</code> and <code>Y</code>, namely <code>(1-p)</code> and <code>Î±</code> resp. <code>(1-p)</code> and <code>Î²</code>.</p>

<p>What I have tried so far is: <code>lm(W~(X+Y)+Z)</code>, trying to indicate with the brackets () that <code>X+Y</code> is an additional block. This is wrong. Unfortunately, <code>R</code> believes the equation is:</p>

<pre><code>W=Î±*X+Î²*Y+p*Z+Ïµ
</code></pre>

<p>How can I use a second coefficient in the regression?</p>
"
"NaN","NaN","202963","<p>I am learning R and how to do regressions in a biological context, so please forgive me.</p>

<p>I am stumped on how to test if a slope parameter is less than a certain number at the alpha = 0.05 level. These are my two correlation(?) coefficients:</p>

<blockquote>
  <p>reg.fit &lt;- lm(gro3 ~ gro2) # fit linear model
  reg.fit</p>
  
  <p>Call:
  lm(formula = gro3 ~ gro2)</p>
  
  <p>Coefficients:</p>
  
  <p>(Intercept),         gro2<br>
     -0.8426,       0.3582    </p>
</blockquote>

<p>I <em>think</em> I should use a t-test to test these somehow. But I'm not sure where to start.</p>
"
"0.0430930413588572","0.0410650781176591","203132","<p>Let's say I have the following regression:</p>

<pre><code>mort_probit &lt;- glm(mort ~ age + I(age*age) + hs_grad + some_college + college + post_grad + black + hisp + other + rich + middle_class, family = binomial(""probit""), data=data)
</code></pre>

<p>This is set up such that the base case is a poor white.  I want to test if a wealthy black individual has a lower risk for mortality than a poor white.</p>

<p>I set up a linear combination of coefficients in the following way:</p>

<pre><code>summary(glht(mort_probit, linfct = c(""(black + rich) = 0 "")))
</code></pre>

<p>But I'm not sure if this answers my question.  What exactly is this command doing?  I'm guessing it sets ""black"" and ""rich"" to 1 and test if that minus the base case = 0.  </p>

<p>What is the correct way of doing this? </p>
"
"0.0960751430929111","0.0985964391432319","203816","<p>I am trying to duplicate the results from <code>sklearn</code> logistic regression library using <code>glmnet</code> package in R.</p>

<p>From the <code>sklearn</code> logistic regression <a href=""http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"" rel=""nofollow"">documentation</a>, it is trying to minimize the cost function under l2 penalty
$$\min_{w,c} \frac12 w^Tw + C\sum_{i=1}^N \log(\exp(-y_i(X_i^Tw+c)) + 1)$$</p>

<p>From the <a href=""https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet_beta.html#log"" rel=""nofollow"">vignettes</a> of <code>glmnet</code>, its implementation minimizes a slightly different cost function
$$\min_{\beta, \beta_0} -\left[\frac1N \sum_{i=1}^N y_i(\beta_0+x_i^T\beta)-\log(1+e^{(\beta_0+x_i^T\beta)})\right] + \lambda[(\alpha-1)||\beta||_2^2/2+\alpha||\beta||_1]$$</p>

<p>With some tweak in the second equation, and by setting $\alpha=0$, $$\lambda\min_{\beta, \beta_0} \frac1{N\lambda} \sum_{i=1}^N \left[-y_i(\beta_0+x_i^T\beta)+\log(1+e^{(\beta_0+x_i^T\beta)})\right] + ||\beta||_2^2/2$$</p>

<p>which differs from <code>sklearn</code> cost function only by a factor of $\lambda$ if set $\frac1{N\lambda}=C$, so I was expecting the same coefficient estimation from the two packages. But they are different. I am using the dataset from UCLA idre <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">tutorial</a>, predicting <code>admit</code> based on <code>gre</code>, <code>gpa</code> and <code>rank</code>. There are 400 observations, so with $C=1$, $\lambda = 0.0025$.</p>

<pre><code>#python sklearn
df = pd.read_csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
y, X = dmatrices('admit ~ gre + gpa + C(rank)', df, return_type = 'dataframe')
X.head()
&gt;  Intercept  C(rank)[T.2]  C(rank)[T.3]  C(rank)[T.4]  gre   gpa
0          1             0             1             0  380  3.61
1          1             0             1             0  660  3.67
2          1             0             0             0  800  4.00
3          1             0             0             1  640  3.19
4          1             0             0             1  520  2.93

model = LogisticRegression(fit_intercept = False, C = 1)
mdl = model.fit(X, y)
model.coef_
&gt; array([[-1.35417783, -0.71628751, -1.26038726, -1.49762706,  0.00169198,
     0.13992661]]) 
# corresponding to predictors [Intercept, rank_2, rank_3, rank_4, gre, gpa]


&gt; # R glmnet
&gt; df = fread(""http://www.ats.ucla.edu/stat/data/binary.csv"")
&gt; X = as.matrix(model.matrix(admit~gre+gpa+as.factor(rank), data=df))[,2:6]
&gt; y = df[, admit]
&gt; mylogit &lt;- glmnet(X, y, family = ""binomial"", alpha = 0)
&gt; coef(mylogit, s = 0.0025)
6 x 1 sparse Matrix of class ""dgCMatrix""
                    1
(Intercept)      -3.984226893
gre               0.002216795
gpa               0.772048342
as.factor(rank)2 -0.530731081
as.factor(rank)3 -1.164306231
as.factor(rank)4 -1.354160642
</code></pre>

<p>The <code>R</code> output is somehow close to logistic regression without regularization, as can be seen <a href=""http://stats.stackexchange.com/questions/203740/logistic-regression-scikit-learn-vs-statsmodels"">here</a>. Am I missing something or doing something obviously wrong?</p>

<p>Update: I also tried to use <code>LiblineaR</code> package in <code>R</code> to conduct the same process, and yet got another different set of estimates (<code>liblinear</code> is also the solver in <code>sklearn</code>):</p>

<pre><code>&gt; fit = LiblineaR(X, y, type = 0, cost = 1)
&gt; print(fit)
$TypeDetail
    [1] ""L2-regularized logistic regression primal (L2R_LR)""
    $Type
[1] 0
$W
            gre          gpa as.factor(rank)2 as.factor(rank)3 as.factor(rank)4         Bias
[1,] 0.00113215 7.321421e-06     5.354841e-07     1.353818e-06      9.59564e-07 2.395513e-06
</code></pre>

<p>Update 2: turning off standardization in <code>glmnet</code> gives:</p>

<pre><code>&gt; mylogit &lt;- glmnet(X, y, family = ""binomial"", alpha = 0, standardize = F)
&gt; coef(mylogit, s = 0.0025)
6 x 1 sparse Matrix of class ""dgCMatrix""
                     1
(Intercept)      -2.8180677693
gre               0.0034434192
gpa               0.0001882333
as.factor(rank)2  0.0001268816
as.factor(rank)3 -0.0002259491
as.factor(rank)4 -0.0002028832
</code></pre>
"
"0.0609427635336005","0.0580747904139041","203929","<p>I have a dataset with the same dependent and independent variables as those for a logistic regression model whose equation has been published in the literature. How do I go about testing whether that equation fits well with my data, since their model was obviously fitted with a different dataset?</p>

<p>In other words I want to know if their model can be generalisable to a different sample/population.</p>

<p>I want to do this in R and all the searches I have done seem to only discuss how to fit a model with my data using the glm() function. I can fit a new model with my data and will therefore get different coefficients to those published, how do I then compare and contrast the two?</p>
"
"0.0867230728520531","0.0918243061724248","204145","<p><strong>Background and Problem</strong></p>

<p>I have a question concerning a meta-analysis combining effects from between- and within-subject designs using log-odds ratios (OR) as the metric of interest. I am familiar with conducting meta-analyses and will be undertaking my calculations in R (using the <code>metafor</code> and <code>lme4</code> packages). To provide greater context, the studies in question ask research subjects to make a binary decision with respect to a personal preference across one of two conditions. In some cases, each participant is assigned to a single condition (making only a single binary response); in others, each subject takes part in both conditions (making two binary responses). For now, presume I have the raw data in all cases. The issue I face is how best to calculate an OR that is comparable across design and whether I should take the correlation between conditions into account for the within-subject designs.</p>

<p><strong>My Current Approach</strong></p>

<p>I presently use logistic regression to estimate the OR for between-subject designs. The slope represents the OR and the sampling variance can be calculated by squaring the SE of the slope coefficient. Using this approach produces estimates comparable to equations reported in common texts such The Handbook of Research Synthesis and Meta-Analysis, 2nd Edition (p. 243). I then extend this approach to use a multilevel logistic regression model including a random intercept by subject to estimate the OR for within-subject designs while account for the dependency between conditions. The OR and sampling variance are otherwise calculated in the same fashion. </p>

<p><strong>My Questions:</strong></p>

<p>With this in mind, I would like to ask:</p>

<ol>
<li>Is it reasonable to meta-analytically aggregate OR calculated using standard and multilevel logistic regression?</li>
<li>Would it be better to use standard logistic regression for both designs (ignoring the correlation between conditions for the within-subject designs)?</li>
</ol>
"
"0.101062140164153","0.0963061447907242","204763","<p>Using linear regression as an equation for prediction is straightforward with,</p>

<p>$$ Y_i = \beta_0 + \beta_1 X_i. $$</p>

<p>Once the betas are estimated I can insert different values of $X$ to use as a what-if analysis for different scenarios. </p>

<p>But trying to do the same with ARIMA models is proving difficult to translate. For example with an ARIMA(2,1,1) model, how do I create an equation where I can try out different scenarios to see how the projection changes? </p>

<p>Below I have the output for a projection of sales based on past sales and extra regressors. I see that a unit change in <code>poc0_3_PER</code> results in a <code>135.2229</code> change in sales. But how do I account for the moving average and auto-regression components?</p>

<pre><code>arima(ts.count, order=c(2,1,1), xreg=df.back[3:4])

Call:
arima(x = ts.count, order = c(2, 1, 1), xreg = df.back[3:4])

Coefficients:
          ar1     ar2     ma1  poc0_3_PER
      -0.4569  0.2458  0.9455    135.2229
</code></pre>

<p>I have <code>ar1</code> and <code>ar2</code> estimates along with <code>ma1</code> and the extra regressors. How do I convert this into a working equation wherein I can try out different scenarios for the extra regressors to see how the prediction is affected?</p>

<p>I'm hoping that the solution is not an equation like <a href=""http://stats.stackexchange.com/questions/69407/how-do-i-write-a-mathematical-equation-for-arima-2-1-0-x-0-2-2-period-12?rq=1"">this post here</a>. I do have SARIMA models at times with orders like <code>SARIMA(2,0,1)(1,0,1)[12]</code>.</p>
"
"0.0621994475718397","0.0592723347638087","204839","<p>Further to <a href=""http://stats.stackexchange.com/questions/200460/multiple-imputation-for-predictive-analysis-using-mice-package-in-r"">my prior question</a> on multivariable adjustment in regression models, using covariates which are available only for some cases, I have researched in some detail the main methods for limited dependent variables, including Heckman correction or tobit models. However, I fear that they do not apply to my issue, which has more to do with <strong>limited independent variables</strong>.</p>

<p>In particular, I am giving below an example of the dataset and the possible analysis in R (disregard the overfitting, it's just to make an example, my actual dataset has at least 10,000 cases):</p>

<pre><code>dep &lt;- c(8, 9, 21, -3, 4, 6, 9, 10, 8, 9, 11, 39, 91, 51, 38, 28, 21)
cov1 &lt;- c(68, 58, 42, 19, 39, 49, 29, 38, 25, 22, 19, 36, 39,90, 105, 73, 25)
cov2 &lt;- c(0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0)
cov3 &lt;- c(0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1)
cov4 &lt;- c(NA, NA, NA, NA, NA, NA, 56, 33, 45, 44, 56, 49, 36, 39, 40, 41, 59)
cov5 &lt;- c(NA, NA, NA, NA, NA, NA, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0)
mydata &lt;- data.frame(cbind(dep, cov1, cov2, cov3, cov4, cov5)) 
mydata

reg1 &lt;- lm(dep ~ cov1 + cov2, data = mydata, na.action = na.omit)
anova(reg1)
summary(reg1)

reg2 &lt;- lm(dep ~ cov1 + cov2 + cov3 + cov4 + cov5, data = mydata, na.action = na.omit)
anova(reg2)
summary(reg2)
</code></pre>

<p>What should I do to best adjust for covariates cov1, cov2, cov3, cov4 and cov5, having dep as dependent variable, given that cov4 and cov5 are available only for patients with cov3 = 1? </p>

<p>Should I discard all cases with cov3 = 0, or should I conduct two separate analyses and then pool the regression coefficients according to their standard error? Or is there any other more reasonable approach?</p>

<p>Unfortunately I did not find anything meaningful searching Google, Google Scholar, or PubMed:</p>

<p><a href=""https://www.google.it/search?q=limited+independent+variable&amp;nirf=limited+dependent+variable"" rel=""nofollow"">https://www.google.it/search?q=limited+independent+variable&amp;nirf=limited+dependent+variable</a></p>

<p><a href=""https://scholar.google.it/scholar?hl=en&amp;q=limited+independent+variable"" rel=""nofollow"">https://scholar.google.it/scholar?hl=en&amp;q=limited+independent+variable</a></p>

<p><a href=""http://www.ncbi.nlm.nih.gov/pubmed/?term=limited+independent+variable"" rel=""nofollow"">http://www.ncbi.nlm.nih.gov/pubmed/?term=limited+independent+variable</a>*</p>

<p>To further clarify what is at stake, this is my real problem: I want to create a clinical prediction score (to predict prognosis and future quality of life) for patients undergoing myocardial perfusion imaging (a non-invasive cardiac test used in subjects with or at risk for coronary artery disease). The imaging test follows immediately an exercise stress test in fit patients, and a pharmacologic stress test in those who are not fit. The latter test is worse than the former, and does not provide several important prognostic features (eg maximum heart rate, or workload), so I must include exercise test variables in the multivariable model. But if I do so, I lose more than 1000 patients who only underwent a pharmacologic stress test.</p>
"
"0.0963589698356145","0.0918243061724248","205123","<p>To idetifying the important activity performed from users who have been converted in last N days. So, I have tried GLM, Rpart and Random forest models which can give me the impoprtant activities (in terms of Data Sciece its highly significant variables). Now If I want to extract the influencer counts for each important activity. i.e. count 5 for viewed_product activity means every user who converts into customer performs five product views. </p>

<p>I have tried my GLM with response variable as IsConverted and rest of the variables are frequencies of all activity performed at user level.  </p>

<pre><code>Call: 
glm(formula = regression_input2$IsConverted ~ ., family = binomial(), 
    data = regression_input2)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.2285  -0.8820  -0.8245   1.3572   1.6479  

Coefficients: (1 not defined because of singularities)
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)           -0.904264   0.068017 -13.295  &lt; 2e-16 ***
view_product         0.029021   0.007867   3.689 0.000225 ***
view_collection     -0.034973   0.054757  -0.639 0.523018    
view_brand           0.047889   0.020289   2.360 0.018258 *  
search_category     -0.028920   0.032899  -0.879 0.379384    
search               0.172942   0.053855   3.211 0.001322 ** 
remov_product       -0.178905   0.151888  -1.178 0.238845    
payment                0.321474   1.054034   0.305 0.760371    
like_product          0.047789   0.035914   1.331 0.183305        
checkout_unsuccessful        NA         NA      NA       NA    
checkout_successful    0.397584   0.973795   0.408 0.683066    
added_product          0.179261   0.097749   1.834 0.066671 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2174.3  on 1668  degrees of freedom
Residual deviance: 2086.3  on 1657  degrees of freedom
AIC: 2110.3

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Apart from these, I have also tried Random forest and Rpart models. 
Here is the output of variable importance of Random forest.</p>

<pre><code>                      IncNodePurity
view_product           51.6447716
view_collec            16.9695232
view_brands            31.8345159
search_category        20.6999952
search                 18.0962766
remov_product_cart     6.6511766
payment                2.2859159
like_product          14.4360793
checkout_unsuccessful  0.2139582
checkout_successful    2.6284091
added_product          14.7047717
</code></pre>

<p>So, with the above utilities how can I get that counts of influencer activity which affects user's convergence. </p>
"
"0.0746393370862076","0.0592723347638087","205586","<p>Consider a data set where you have a <code>tenure</code> variable that takes non-negative values (e.g. from <code>0</code> to <code>44</code>) and a <code>tenurel</code> dummy that is <code>FALSE</code> when <code>tenure</code> is <code>0</code> and <code>TRUE</code> otherwise. How do you interepret their coefficients when both are included in a linear regression?</p>

<p>Take this example (originally from Wooldridge 2008): </p>

<pre><code>library(foreign);library(lmtest)
wage1 &lt;- read.dta(""http://fmwww.bc.edu/ec-p/data/wooldridge/wage1.dta"")
wage1$tenurel &lt;- as.logical(wage1$tenure)

coeftest(fitnestint &lt;- lm(log(wage) ~ educ+exper+tenurel+tenure, data=wage1))
## 
## t test of coefficients:
## 
##              Estimate Std. Error t value  Pr(&gt;|t|)    
## (Intercept) 0.2757858  0.1042105  2.6464  0.008381 ** 
## educ        0.0899010  0.0074526 12.0630 &lt; 2.2e-16 ***
## exper       0.0038215  0.0017323  2.2060  0.027819 *  
## tenurelTRUE 0.0732658  0.0480708  1.5241  0.128084    
## tenure      0.0200777  0.0033542  5.9859 4.008e-09 ***
## ---
## Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p><strong>Question:</strong> So how do you interpret the coefficients for <code>tenurel</code> (<code>0.073</code>) and <code>tenure</code> (<code>0.020</code>)? </p>

<hr>

<p>In my understanding <code>tenurel</code> would represent the effect on Y of ""having some tenure when <code>tenure=0</code>"" (even if its literal interpretation would seem nonsensical, as it can happen for main-effect regressors in usual interactions) and <code>tenure</code> would represent the <em>additional</em> effect on Y ""of one more year of tenure"". </p>

<p>It seems to me that in this specification <code>tenure</code> is a ""nested interaction"", insofar as if we add an interaction term between the two regressors, it would be undefined: </p>

<pre><code>coef(fitint &lt;- lm(log(wage)~educ+exper+tenurel*tenure, data=wage1))
##        (Intercept)               educ              exper 
##        0.275785819        0.089900995        0.003821454 
##        tenurelTRUE             tenure tenurelTRUE:tenure 
##        0.073265829        0.020077744                 NA 
</code></pre>

<p>Indeed, the interaction term <code>tenurel:tenure</code> would be identical to <code>tenure</code>: </p>

<pre><code>with(wage1, all.equal(tenure, tenurel*tenure))  ##interaction term identical to tenure
## [1] TRUE
</code></pre>
"
"0.052777981396926","0.0502942438178979","205614","<p>I'm running some regression analyses and got pretty confused about R's output when it comes to robust regression models.
When I run a <code>OLS</code> -- using the command <code>lm()</code> -- I get this as output:</p>

<blockquote>
  <p>Coefficients:</p>

<pre><code>                Estimate Std. Error t value Pr(&gt;|t|)
</code></pre>
</blockquote>

<p>But when I run a robust linear model using the <code>rlm()</code> command, the output looks like so:</p>

<blockquote>
  <p>Coefficients:  </p>

<pre><code>                Value   Std. Error t value
</code></pre>
</blockquote>

<p>How do I get  the <code>p-values</code> and the significance-levels in an <code>rlm</code> then? Without that, the whole analysis is somewhat pointless.
Unfortunately, I couldn't find an answer to that anywhere, so I hope someone around here can help me out. Thanks a lot!!</p>
"
"0.0746393370862076","0.0711268017165705","206039","<p>I am looking at a logistic regression model for predicting hospital acquired infection likelihood (HAI) from predictors of whether germs are found on the  x number of patients (Patient), x number of environmental spots (Env), x number of air samples (Air) or x number of nurses' hands (Hand).</p>

<pre><code>   Month Patient Env Air Hand HAI HAIcat BedOccupancy
      1       4   0   0    1   1    yes            9
      2       2   0   2    0   0     no            9
      3       2   1   0    1   0     no            5
      4       1   2   0    2   2    yes            7
      5       2   3   0    1   1    yes            6
      6       1   2   0    0   1    yes            5
      7       4   0   0    2   1    yes            7
      8       2   0   0    1   3    yes            7
      9       3   2   2    0   1    yes            8
     10       3   0   0    1   1    yes            8
</code></pre>

<p>For example for Month 1, the percentage of HAI would be HAI/BedOccupancy=1/9.
So I'd like to know if bed occupancy or other contamination is significant in predicting HAI. I run a Logistic regression, but it says it's junk. What does a statistician do now?</p>

<pre><code>model&lt;-glm(cbind(MR$HAI,MR$BedOccupancy)~MR$Patient+MR$Env+MR$Air+MR$Hand,family = ""binomial"")
</code></pre>

<p>But I get a bad fit and non-significant correlation:</p>

<pre><code>Call:
glm(formula = cbind(MR$HAI, MR$BedOccupancy) ~ MR$Patient + MR$Env + MR$Air + 
        MR$Hand, family = ""binomial"")

Deviance Residuals: 
       1         2         3         4         5         6         7         8         9        10  
-0.12882  -1.08046  -1.33787   0.01400  -0.10685  -0.02229  -0.04008   1.03688   0.75723  -0.23824  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -1.30758    1.34049  -0.975    0.329
MR$Patient  -0.22920    0.39350  -0.582    0.560
    MR$Env      -0.02415    0.37672  -0.064    0.949
MR$Air      -0.46851    0.64611  -0.725    0.468
    MR$Hand      0.16054    0.58277   0.275    0.783

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 6.6594  on 9  degrees of freedom
Residual deviance: 4.6929  on 5  degrees of freedom
AIC: 30.911

Number of Fisher Scoring iterations: 5
</code></pre>
"
"0.161598905095714","0.159127177705929","206042","<p>I implement <code>n</code> permutations into a regression analysis, to test the model for stability. Thus I obtain <code>n</code> odds ratios (ORs) and <code>n</code> associated 95% CI intervals. </p>

<p>Each permutation represents a matched-pair study. We pair similar <code>case</code>'s with <code>control</code>'s and then run a conditional logistic regression to obtain a measure of association between the outcome of interest and exposure variable (treatment status).</p>

<p>Taking the following example I have implemented into a <code>R</code> script.
In short what I have done is:</p>

<ol>
<li>Take a portion of the a given population</li>
<li>We assign at dummy variable to the population (1/0) to indicate treatment status</li>
<li>based on a set of parameters we pair those with treatment status==1 to equivalent treatment status==0</li>
<li>we define an outcome of interest that we wish to measure if treatment had an effect on the outcome</li>
<li>We conduct a logistic regression to determine the ORs associated with treatment status</li>
<li>we repeat this n time, each time obtaining an ORs and associated 95% confidence interval</li>
</ol>

<p><strong>But what I am not sure, is how I can report on the spread of my data. I generate a different odds ratio and 95% CI for each permutation.</strong></p>

<p>Taking the following hypothetical example, we run a simulation 100 times. It only takes a minute to simulate.</p>

<p>We take an worked exampled from the <a href=""https://cran.r-project.org/web/packages/Matching/Matching.pdf"" rel=""nofollow"">Matching package</a> in R. </p>

<pre><code>set.seed(123)    
# preamble, prepare the data for the simulation
    #1.
    library(Matching)
    library(survival)
    #2.
    require(doParallel)
    cl&lt;-makeCluster(2)
    registerDoParallel(cl)
    #3.
    clusterEvalQ(cl,library(Matching))
    clusterEvalQ(cl,library(survival))

    m &lt;- 100


    Result = foreach(i=1:m,.combine=cbind) %do%{

      # attach the data
      data(lalonde)

      # we want to assess if treatment is associated with greater odds for the outcome of interest
      # lets create our hypothetical outcome of interest
      lalonde$success &lt;- with(lalonde, ifelse(re78 &gt; 8125, 1, 0))

      # lets take a portion of the original population, say only 395
      n &lt;- sample(1:445,420, replace = F)
      n &lt;- sort(n, decreasing = F)
      lalonde &lt;- lalonde[n,]
      head(lalonde$age)

      # taking from the example from GenMatch (in Matching package)
      #The covariates we want to match on
      # but we only include some of the original variables (we come back to the others later)
      X = cbind(lalonde$age, lalonde$educ, lalonde$black, lalonde$hisp, 
                lalonde$married, lalonde$nodegr)

      #The covariates we want to obtain balance on
      BalanceMat &lt;- X

      # creat our matrix
      genout &lt;- GenMatch(Tr=lalonde$treat, X=X, BalanceMatrix=BalanceMat, 
                         pop.size=16, max.generations=10, wait.generations=1)


      # match our collisions on a 1-1 basis
      mout &lt;- Match(Y=NULL, Tr=lalonde$treat, X=X, Weight.matrix=genout, ties = F, replace = F)
      summary(mout)

      # here we create our case and control populations
      treat &lt;- lalonde[mout$index.treat,]
          control &lt;- lalonde[mout$index.control,]

      # and we want to apply a unique identifier for each pair
      # we call this command during the regression
      treat$Pair_ID &lt;- c(1:length(treat$age))
      control$Pair_ID &lt;- treat$Pair_ID 

      # finally we combine the data
      matched &lt;- rbind(treat, control)

      # now we run a conditional logitic regression on the paired data to determine the Odds Ratio associated with treatment
      # we account for the difference in pairs by the strata() command
      # we account for some of the original matching parameters that we removed from the matching process
      model_1 &lt;- clogit(success ~ treat + strata(Pair_ID) + re74, matched, method = 'efron')
      summary(model_1)

      OR_M1 &lt;- exp(model_1$coefficients[1])
      CI_U1 &lt;- exp(confint(model_1))[1,2]
      CI_L1 &lt;- exp(confint(model_1))[1,1]

      Result &lt;- rbind(OR_M1, CI_U1, CI_L1)

    }
</code></pre>

<p>To summarise the script:</p>

<ul>
<li>we take 420 people from the original population (of 445)</li>
<li>we define the outcome of interest is. That is if the person had <code>re78 &gt; 8125</code> yes or no</li>
<li>for each treat==1, we find an equivalent treat==0 based on age, educ,  black, hisp, married, nodegr. We only want exact 1-1 matching</li>
<li>we assign an unique indicator variable for each pair 1,2,3.....x</li>
<li>We then develop a regression model to determine the OR for our outcome of interest (<code>re78 &gt; 8125</code>) associated with the treatment status (=1 relative to =0). </li>
<li>we save the ORs and 95%CI</li>
</ul>

<p>We can then plot the ORs and shade the 95%CI</p>

<pre><code>plot(Result[1,], ylim=c(0,2.5))
polygon(c(1:m,m:1), c(Result[3,],Result[2,]),col=adjustcolor(""grey"", alpha=0.4), border = NA)
</code></pre>

<p><strong>But how can I summarise the several ORs I obtained, the spread of it and/or an associated confidence level?</strong></p>

<p><strong>EDIT</strong>
Am I able to assess my study as if it was a meta-analysis. If so, one could implement the solution proposed by @Bernd Weiss <a href=""http://stats.stackexchange.com/questions/9483/how-to-calculate-confidence-intervals-for-pooled-odd-ratios-in-meta-analysis?rq=1"">here</a>?</p>

<p>For this we need to obtain the natural log of the ORs and the std. err.?</p>

<p>We update the last part of the command to:</p>

<pre><code>.......    
model_1 &lt;- clogit(success ~ treat + strata(Pair_ID) + re74, matched, method = 'efron')
      summary(model_1)

  OR_M1 &lt;- exp(model_1$coefficients[1])

  l_OR_T2 &lt;- model_1$coefficients[1]
  s_e &lt;- coef(summary(model_1))[1,3]

  CI_U1 &lt;- exp(confint(model_1))[1,2]
  CI_L1 &lt;- exp(confint(model_1))[1,1]

  Result &lt;- rbind(OR_M1, l_OR_T2, s_e, CI_U1, CI_L1)
</code></pre>

<p>Using we can then call upon the <code>metagen()</code>, command</p>

<pre><code>library(meta)
or.fem &lt;- metagen(as.numeric(Result[2,]), as.numeric(Result[3,]), sm = ""OR"")
</code></pre>

<p>Where <code>as.numeric(Result[2,])</code> is the log(OR) and <code>as.numeric(Result[3,])</code> is the std. err. Thus we obtain a 95% CI ...... But have we introduced a bias in the CI by the imputations. We see our 95% range is significant (greater than 1), however for each permutation, we only get a lower 95% CI > 1 </p>

<pre><code>sum(as.numeric(Result[5,])&gt;1.00)
</code></pre>

<p>times. Therefore I think the large <code>n</code> and thus <code>degrees of freedom</code> in the meta-analysis are giving us a significant result </p>
"
"0.075412822378","0.0821301562353182","206058","<p>Using R, I can only find tools for performing L1 and/or L2 regularized linear regression (lars, glmnet) and tools for constrained linear regression (quadprog , or lsei {limSolve} , where the inequality and equality constraints can be only given in the form Ax = b , Gx &lt;= h).</p>

<p>It seems inutitive for me that the possibility of combining both should be required very oft when solving specific regression problems, but so far I havent been able to do it. </p>

<p>Instead of providing information on my specific set of constraints and algebraic system, IÂ´d be interested to know if this is a problem I can actually solve using the above mentioned packages? Are there any packages at all in R built for both parameter regularization and specific parameter penalties? </p>

<p>Update: For better understanding: I am not trying to combine different regularization methods (like in elastic net), nor trying to combine different parameter constraints. My goal is to combine regularization with specific coefficient constraints, so for example:</p>

<p>Find the most sparse solution (penalizing absolute values through LASSO) of a linear regression y = bx which satisfy the coefficient constraints bA &lt; h for some given matrix A and threshold h. </p>

<p>min($\parallel$ $\beta$x-y$\parallel_2^2$ + $\lambda$ $\mid$ $\beta$ $\mid_1$ ), s.t   $\beta$ A $\leqslant$ h</p>
"
"0.068136080998913","0.0649295895722714","206702","<p>I have some data to fit a logistic regression, although the data seems quite good, the resulted fit does not look as expected.</p>

<blockquote>
<pre><code>  paramValue      normality
1  3.69             0
2  1.16             0
3  6.12             1
4  2.78             1
5  1.45             1
6  3.56             0
</code></pre>
</blockquote>

<pre><code>mylogit &lt;- glm(normality ~paramValue,  family = binomial(link=""logit""))
summary(mylogit)
</code></pre>

<blockquote>
<pre><code>Call:
glm(formula = normality ~ paramValue, family = binomial(link = ""logit""))

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-2.44994  -0.73312   0.08151   0.63377   1.41140  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)  -1.9945     0.9531  -2.093   0.0364 *
paramValue    1.2582     0.5655   2.225   0.0261 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 29.065  on 20  degrees of freedom
Residual deviance: 19.746  on 19  degrees of freedom
AIC: 23.746

Number of Fisher Scoring iterations: 5
</code></pre>
</blockquote>

<pre><code>    plot(paramValue,normality)

    x &lt;- seq(-1, 6, 0.1)

curve(predict(mylogit,data.frame(paramValue=x),type=""response""),add=TRUE, col=""red"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/1f0DY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1f0DY.png"" alt=""enter image description here""></a></p>

<p>Did I do something wrong? Is there any way to force the regression to cross the origin?</p>
"
"0.0304713817668003","0.029037395206952","206735","<p>I've created an example table (just in order to create a function) with:</p>

<pre><code>ex&lt;-data.frame(b=c(rep('A',50),rep('B',30), rep('C',20)), 
fl=round(runif(100,0,1),0),r=runif(100,0,0.5))
ex2&lt;-cbind(ex,model.matrix(~b-1,ex))
lineal&lt;-ex2$bB+ex2$bA*ex$fl+ex$fl
ex$clase&lt;-round(1/(1+exp(-lineal)),0)
</code></pre>

<p>Then I run a logistic regression model (MASS library)</p>

<pre><code>fm&lt;-as.formula(clase~b+fl+r)
modT&lt;-glm(clase~1, family=binomial, data = ex)
modT&lt;-stepAIC(modT, scope = fm, family=binomial, data =ex, k = 4)
summary(modT)
</code></pre>

<p>As you can see coefficients are not significant, but I've created the class using them. So I don't understand why this is happening.</p>

<p><a href=""http://i.stack.imgur.com/yR4jV.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/yR4jV.png"" alt=""enter image description here""></a></p>
"
"0.0806196982594614","0.076825726438694","206870","<p>By converting and by trying to interpret the parameters of a logistic regression ran in R, I just find them to be overestimated. Therefore I tried to compute them myself but I can not obtain the same values reported by the regression.</p>

<p>I used this web-page for computations:
<a href=""http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/mult_pkg/faq/general/odds_ratio.htm</a></p>

<p>Let say we only focus on the LagC parameter:</p>

<p><strong>Logistic Regression</strong></p>

<pre><code>&gt; model &lt;- glmer(RepT2 ~ DistractorC1 + DistractorC2 + LagC + DistractorC1:LagC + DistractorC2:LagC + (LagC | Subject) + (1 | Item),
                data = DF,
                family = binomial(link = ""logit""),
                control = glmerControl(optimizer = ""bobyqa""))
&gt; summary(model)

  Fixed effects:
                    Estimate Std. Error z value Pr(&gt;|z|)    
  (Intercept)       -0.81039    0.22040  -3.677 0.000236 ***
  DistractorC1       0.33129    0.06393   5.182  2.2e-07 ***
  DistractorC2       0.03436    0.10011   0.343 0.731467    
  LagC               2.09567    0.12725  16.469  &lt; 2e-16 ***
  DistractorC1:LagC -0.21654    0.12770  -1.696 0.089932 .  
  DistractorC2:LagC -0.84018    0.20055  -4.189  2.8e-05 ***
</code></pre>

<p>Odds of the parameters:</p>

<pre><code>&gt; show(Odds &lt;- exp(summary(model)$coefficients[,""Estimate""])

  (Intercept)      DistractorC1      DistractorC2              LagC DistractorC1:LagC DistractorC2:LagC 
    0.4446833         1.3927594         1.0349529         8.1308503         0.8052993         0.4316343 
</code></pre>

<p>Probabilities of the parameters:</p>

<pre><code>&gt; show(P &lt;- Odds / (1 + Odds))

  (Intercept)      DistractorC1      DistractorC2              LagC DistractorC1:LagC DistractorC2:LagC 
    0.3078068         0.5820725         0.5085881         0.8904812         0.4460752         0.3014976 
</code></pre>

<p><strong>My Estimations</strong></p>

<pre><code>&gt; Means &lt;- DF %&gt;%
    group_by(Subject, Lag) %&gt;%
    filter(RepT1 == 1) %&gt;%
    summarise(repok = sum(RepT2) / (n())) %&gt;%
    group_by(Lag) %&gt;%
    summarise(Means = mean(repok))

&gt; show(Means)

     Lag     Means
  (fctr)     (dbl)
1   Lag3 0.1972174
2   Lag8 0.5475624
</code></pre>

<p>Odds of the parameter:</p>

<pre><code>&gt; OddsLag3 &lt;- 0.1972174 / (1-0.1972174)
&gt; OddsLag8 &lt;- 0.5475624 / (1-0.5475624)
&gt; OddsLagC &lt;- OddsLag8 / OddsLag3
&gt; show(OddsLagC)

[1] 4.926377
</code></pre>

<p>Probabilities of the parameter:</p>

<pre><code>&gt; show(OddsLag / (1 + OddsLag))

[1] 0.8312628
</code></pre>

<p>We can see that it is close, but not accurate. Does anyone have an explanation?
Note that I compute a mean for each subject and then only a mean for each condition. I also did estimate the parameters without taking into account the subjects, but still, the mismatch was here.</p>

<p><a href=""http://i.stack.imgur.com/YRdPf.png"" rel=""nofollow"">Graphical representation</a></p>
"
"0.052777981396926","0.0502942438178979","207427","<p>I am confused with the answer from
<a href=""http://stackoverflow.com/questions/23282048/logistic-regression-defining-reference-level-in-r"">http://stackoverflow.com/questions/23282048/logistic-regression-defining-reference-level-in-r</a></p>

<p>It said if you want to predict the probability of ""Yes"", you set as  <code>relevel(auth$class, ref = ""YES"")</code>. However, in my experiment, if we have a binary response variable with ""0"" and ""1"". We only get the estimation for probability of ""1"" when we set <code>relevel(factor(y),ref=""0"")</code>.</p>

<pre><code>n &lt;- 200
x &lt;- rnorm(n)
sumx &lt;- 5 + 3*x
exp1 &lt;- exp(sumx)/(1+exp(sumx))
y &lt;- rbinom(n,1,exp1) #probability here is for 1
model1 &lt;- glm(y~x,family = ""binomial"")
summary(model1)$coefficients
            Estimate Std. Error  z value     Pr(&gt;|z|)
(Intercept) 5.324099  1.0610921 5.017565 5.233039e-07
x           2.767035  0.7206103 3.839849 1.231100e-04
model2 &lt;- glm(relevel(factor(y),ref=""0"")~x,family = ""binomial"")

summary(model2)$coefficients
            Estimate Std. Error  z value     Pr(&gt;|z|)
(Intercept) 5.324099  1.0610921 5.017565 5.233039e-07
x           2.767035  0.7206103 3.839849 1.231100e-04
</code></pre>

<p>I think if we want to get probability of ""Yes"", we should set <code>relevel(auth$class, ref = ""No"")</code>, am I correct? And what is reference level here means? Actually, what is glm() to predict in default if we use response other than ""0"" and ""1""? </p>
"
"0.114914776956952","0.116351054666701","207608","<p>I'd like to run a probit regression on the ""B1_df"" data frame with 3 categorical outcome variables (rank 1,2 or 3). I cannot use glm because there are 3 outcome variables.  I would like to be able to tie out the results from polr() and mlogit().  I am getting reasonable results from polr() but strange results from mlogit() I believe due to my data frame construction.</p>

<p>Basically I have 3 machine B1, B2 and B3 and each have 5 runs that are ranked 1 to 3 and I am using probit to tell me which machine has the highest probability of returning the highest rank. </p>

<pre><code>First with polr():

require(ggplot2)
require(MASS)
require(mlogit)

machine = c(rep(""B1"",5), rep(""B2"",5),rep(""B3"",5))
rank = c(rep(3,5), rep(2,5),rep(1,5))
#rank = c(c(3,3,3,3,1), rep(2,5),rep(1,5)) # see *** comment below
dat = data.frame(machine = machine, rank = rank)
dat$B1 =  c(rep(1,5), rep(0,5),rep(0,5))
    dat$B2 =  c(rep(0,5), rep(1,5),rep(0,5))
dat$B3 =  c(rep(0,5), rep(0,5),rep(1,5))
B1_df = dat[,1:3]
B1_df
b1=polr(formula = as.factor(rank)~ as.factor(B1), data= B1,  Hess = FALSE, model = TRUE,method = c(""probit""))
b1

   machine rank B1
1       B1    3  1
2       B1    3  1
3       B1    3  1
4       B1    3  1
5       B1    3  1
6       B2    2  0
7       B2    2  0
8       B2    2  0
9       B2    2  0
10      B2    2  0
11      B3    1  0
12      B3    1  0
13      B3    1  0
14      B3    1  0
15      B3    1  0
&gt; b1=polr(formula = as.factor(rank)~ as.factor(B1), data= B1,  Hess = FALSE, model = TRUE,method = c(""probit""))
&gt; b1
Call:
polr(formula = as.factor(rank) ~ as.factor(B1), data = B1, Hess = FALSE, 
    model = TRUE, method = c(""probit""))

Coefficients:
as.factor(B1)1 
      8.599074 

Intercepts:
         1|2          2|3 
0.0002318407 4.4165977032 

Residual Deviance: 13.86319 
AIC: 19.86319 
</code></pre>

<p>Question:  Does the coef of 8.5 indicate that by setting B1 =1 the z-score would increase by 8.5 giving a higher probability of getting a higher rank? I was thinking that it did but then I uncommented this line:</p>

<pre><code>rank = c(c(3,3,3,3,1), rep(2,5),rep(1,5))
</code></pre>

<p>so now B1 does not have all 3's it has 4 3's and one 1. I was expecting the coef, intercepts, deviance to change but they don't.  Please uncomment ** and run. Any idea why no change?</p>

<p>Now I'd like to try to get those same results in mlogit:</p>

<pre><code>B1_df2 = mlogit.data(B1_df, shape = ""wide"", choice =""rank"", id.var= ""B1"") #configure the data frame with mlogit.data
B1_df2
summary(mlogit(rank ~  0 | B1 ,data=B1_df2, Probit=TRUE))  # call mlogit 

&gt; summary(mlogit(rank ~  0 | B1 ,data=B1_df2, Probit=TRUE))  # call mlogit

Call:
mlogit(formula = rank ~ 0 | B1, data = B1_df2, Probit = TRUE, 
    method = ""nr"", print.level = 0)

Frequencies of alternatives:
       1        2        3 
0.533333 0.400000 0.066667 

nr method
16 iterations, 0h:0m:0s 
g'(-H)^-1g = 7.84E-07 
gradient close to zero 

Coefficients :
                 Estimate  Std. Error t-value Pr(&gt;|t|)
2:(intercept) -8.9248e-17  6.3246e-01  0.0000   1.0000
3:(intercept) -1.6669e+01  1.8625e+03 -0.0089   0.9929
2:B1          -1.0986e+00  1.3166e+00 -0.8345   0.4040
3:B1           1.5570e+01  1.8625e+03  0.0084   0.9933

Log-Likelihood: -11.683
McFadden R^2:  0.11726 
Likelihood ratio test : chisq = 3.1037 (p.value = 0.21186)
</code></pre>

<p>You can see the mlogit coefs are nowhere near the 8.5 and there seem to be duplicates.</p>

<p>For mlogit() I am looking at page 22 here for the pure multinomial model:</p>

<p><a href=""https://cran.r-project.org/web/packages/mlogit/mlogit.pdf"" rel=""nofollow"">https://cran.r-project.org/web/packages/mlogit/mlogit.pdf</a></p>

<p>Any idea how to properly set up these models to get them to tie?</p>
"
"0.0304713817668003","0.029037395206952","208277","<p>I find it difficult to connect the coefficients of a regression model that includes splines to the actual prediction equation. For example, how could that be done with the following model?</p>

<pre><code>&gt; library(rms)
&gt; x &lt;- 1:11
&gt; y &lt;- c(0.2,0.40, 0.6, 0.75, 0.88, 0.99, 1.1, 1.15, 1.16, 1.16, 1.16 )
&gt; dd &lt;- datadist(x); options(datadist='dd')
&gt;  f &lt;- ols(y ~ rcs(x, c(3, 5, 7, 9)))
&gt; f  


  Linear Regression Model

ols(formula = y ~ rcs(x, c(3, 5, 7, 9)))

            Model Likelihood     Discrimination    
               Ratio Test           Indexes        
Obs       11    LR chi2     66.08    R2       0.998    
sigma 0.0201    d.f.            3    R2 adj   0.996    
d.f.       7    Pr(&gt; chi2) 0.0000    g        0.383    

Residuals

  Min        1Q    Median        3Q       Max 
-0.027360 -0.011739  0.001227  0.009892  0.031166 

           Coef    S.E.   t     Pr(&gt;|t|)
Intercept  0.0465 0.0224  2.08 0.0762  
x          0.1741 0.0072 24.18 &lt;0.0001 
x'        -0.1004 0.0311 -3.23 0.0144  
x''        0.0542 0.0913  0.59 0.5715  


&gt; Function(f)
function(x = 6){
  0.046475489 + 0.17411942*x - 0.002790266*pmax(x-3,0)^3 + 
  0.0015048699*pmax(x-5,0)^3 + 0.0053610582*pmax(x-7,0)^3 - 
  0.0040756621*pmax(x-9,0)^3 
}
</code></pre>
"
"0.068136080998913","0.0649295895722714","208571","<p>I am trying to model my dependent variable (ordinal - three levels) using a set of independent variables (5 ordinal and 10 numeric). I am using <code>lrm</code> function in ""rms"" package of R. I am conducting principle component regression. <code>S1</code>, <code>C5</code>, <code>C2</code>, <code>C3</code>, <code>S7</code> and <code>S4</code> are the selected independent variables using PCA. </p>

<pre><code>          Coef         S.E.   Wald   Z    Pr(&gt;|Z|)
          y&gt;=2      -1.0469 0.6092 -1.72  0.0857  
          y&gt;=3      -8.5826 1.0354 -8.29  &lt;0.0001 
          S1=Simple -2.9091 0.6112 -4.76  &lt;0.0001 
          C5         0.8389 0.1475  5.69  &lt;0.0001 
          C2         1.4904 0.1889  7.89  &lt;0.0001 
          C3         1.2139 0.1908  6.36  &lt;0.0001 
          S7         0.8803 0.2701  3.26  0.0011  
          S4=TN     -1.2460 0.4659 -2.67  0.0075  
</code></pre>

<p>I understand, the output of the ordinal regression model is given by,</p>

<pre><code>ln(Fij/ 1-Fij) = Boj + B1X1 + B2X2 + .....BkXk

where Fi1 is probability that Y=1, 
Fi2 is probability that Y=2, 
Fi3 is probability that Y=3
B0, B1.....Bk - coefficients
X0, X1.....Xk - Independent variables
</code></pre>

<p>My question is, how do we interpret negative coefficients here? Also, does ranking the values of Wald statistics from largest to smallest indicate descending strength of evidence of an association with the dependent variable?</p>
"
"0.068136080998913","0.0649295895722714","208765","<p>I know its possible to extract r-squared values to quantify the 'goodness-of-fit' of regressions in R, with something to the effect of:</p>

<pre><code>fit &lt;- lm(y ~ x1 + x2 + x3, data=mydata)  # Not actual data
r-sq &lt;- summary(fit)$r.squared # or $adj.r.squared
</code></pre>

<p>I've recently been using the <code>cumSeg</code> package for step-function regressions, but it doesn't appear to offer this functionality, though it does provide residuals as a vector.</p>

<p>Is there some way to extract an r-squared (or adj. r squared) that I don't know about? Or can it be calculated 'de novo' with something that <code>cumSeg</code> does actually provide?</p>

<p><strong>EDIT</strong>
This is the output of <code>summary()</code> for my stepfunction created via <code>cumSeg</code>. Perhaps someone more mathematically versed with stepfunctions knows if the  nomenclature for an r-squared (or whatever the equivalent is) is just different and the data I'm looking for is actually there (or if it is even a legitimate question to ask for an R-squared for stepfunctions?! I'm assuming it should be calculable from any fitted model really.</p>

<pre><code>&gt; summary(stepfunc)
              Length Class  Mode   
coefficients   3     -none- numeric
residuals     16     -none- numeric
effects       16     -none- numeric
rank           1     -none- numeric
fitted.values 16     -none- numeric
assign         0     -none- NULL   
qr             5     qr     list   
df.residual    1     -none- numeric
epsilon        1     -none- numeric
it             1     -none- numeric
psi            1     -none- numeric
beta.c         1     -none- numeric
gamma.c        1     -none- numeric 
V             16     -none- numeric
y             16     -none- numeric
id.group      16     -none- numeric
est.means      2     -none- numeric
n.psi          1     -none- numeric
</code></pre>
"
"0.0304713817668003","0","208895","<p>I have a logistic regression model where Pstatus (a binary variable is coded as 1,2 (1 being apart, 2 being together in R see code below) has a coefficient of 0.8. I am wondering how I should interprete this coefficient of 0.8. And the dependent variable is alcohol consumption. </p>

<p>Is this positive coefficient telling me that being together increases the likelihood/log(odds ratio) of alcohol consumption? </p>

<pre><code>   logit.model&lt;-glm(alc~sex+age+famsize+Pstatus+Medu+Fedu+studytime+
   activities+romantic+famrel+freetime+goout+health+absences+grades,
   data=h,family=""binomial"")

   **Result** 
   PstatusT       0.800220   0.281027   2.847 0.004407 ** 

   Pstatus   : Factor w/ 2 levels ""A"",""T"": 1 2 2 2 2 2 2 1 1 2 ...
</code></pre>
"
"0.0430930413588572","0.0410650781176591","209030","<p>I fitted a Cox PH model in R with the survival package and the <code>coxph</code> function.
I get the beta estimates from this model.
How can I use these coefficients to manually predict on new data, like the predict function does.</p>

<p>In a linear regression this is just the matrix multiplication <code>X %*% beta</code> if $X$ is the data and $beta$ is the vector of coefficients.</p>

<p>How is this in the Cox model? I also see that predict has several options for types of predictions.</p>

<p>here is a minimal example:</p>

<pre><code>library(survival)
data(""ovarian"")
m &lt;- coxph(formula = Surv(futime, fustat) ~., data=ovarian)
</code></pre>

<p>these two give different results:</p>

<pre><code>head(as.matrix(ovarian[, -c(1:2)]) %*% m$coefficients)

      [,1]
1 10.102002
2 10.371810
3  9.706097
4  6.820160
5  7.357138
6  7.627324

head(predict(m, ovarian))
          1           2           3           4           5           6 
 2.66935119  2.93915962  2.27344680 -0.61249088 -0.07551308  0.19467374 
</code></pre>
"
"0.106649836183801","0.10889023202607","209766","<p>In order to explain the distribution of a species (LO), I have run a glm (family=binary, link=logit) in R 3.1.2. I have presence absence data for my species, and a number of dataset describing the climate and landscape as explanatory values. My dataset looks like this in R:</p>

<pre><code>'data.frame':   72920 obs. of  17 variables:
 $ LO : int  1 1 1 1 1 1 1 1 1 1 ...
 $ MAG: int  0 0 0 0 0 0 0 0 1 0 ...
 $ PCR: int  0 1 0 0 0 0 0 0 0 0 ...
 $ WAT: int  0 0 0 0 0 0 0 0 0 0 ...
 $ SVE: int  0 0 0 0 0 0 0 0 0 0 ...
 $ ARA: int  0 0 1 0 0 0 0 1 0 0 ...
 $ GRA: int  0 0 0 1 0 1 1 0 0 1 ...
 $ FOR: int  0 0 0 0 0 0 0 0 0 0 ...
 $ MHF: num  32 39.2 29.2 36 39.2 ...
 $ B13: int  100 99 112 114 104 109 107 105 106 113 ...
 $ B12: int  420 421 474 485 438 454 435 427 427 472 ...
 $ MTP: int  43 81 13 4 3 3 2 21 98 2 ...
 $ BI6: int  72 72 74 70 74 68 69 70 70 84 ...
 $ BI4: int  4821 4886 4947 4997 4859 4971 4909 5009 5173 4901 ...
 $ ALT: num  18.2 20.1 132.8 166.5 54.8 ...
 $ BI1: num  18.6 18.6 18.5 18.2 18.6 ...
 $ URB: int  1 0 0 0 1 0 0 0 0 0 ...
</code></pre>

<p>LO is my species, and 1 symbolizes presence, while 0 is absence of the species.</p>

<p>I type the following into R, and get a model output:</p>

<pre><code>glm(formula = LO ~ MAG + PCR + WAT + FOR + SVE + ARA + GRA + 
    MHF + B13 + B12 + MTP + BI4 + ALT + BI1 + URB, family = binomial(""logit""), 
    data = data)
</code></pre>

<p>At first glance, my resulting model look okay, with significant interactions are found. But problematically, I found it actually predicts where my species is not likely to be found. What I want is of course a model describing where I am likely to find my species. I hope someone can tell me what has gone wrong in my model, making it predict absence instead of presence.</p>

<p>I have worked with my data in arcGIS, and when using the regression results to look at the predicted distribution, I find that the regression predicts that my species will be where it is actually absent, while being absent from all the geographic regions where it is actually found. This is why I know that absence, not presence, is being described.</p>

<p>The output of my model looks like this:</p>

<pre><code>&gt; model2&lt;-glm(LO~MAG+PCR+WAT+FOR+SVE+ARA+GRA+MHF+B13+B12+MTP+BI4+ALT+BI1+URB, data=data, family=binomial(""logit""))
&gt; summary(model2)

Call:
glm(formula = LO ~ MAG + PCR + WAT + FOR + SVE + ARA + GRA + 
    MHF + B13 + B12 + MTP + BI4 + ALT + BI1 + URB, family = binomial(""logit""), 
    data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.7059  -0.4574   0.2278   0.5829   2.9743  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -4.960e+00  1.279e-01 -38.786  &lt; 2e-16 ***

MAG          4.116e-01  5.463e-02   7.534 4.92e-14 ***
PCR          2.129e-01  5.317e-02   4.004 6.24e-05 ***
WAT         -1.245e+00  6.946e-02 -17.920  &lt; 2e-16 ***
FOR         -4.349e-01  4.514e-02  -9.635  &lt; 2e-16 ***
SVE         -1.305e+00  6.734e-02 -19.376  &lt; 2e-16 ***
ARA          9.591e-01  4.787e-02  20.038  &lt; 2e-16 ***
GRA         -7.206e-01  5.232e-02 -13.773  &lt; 2e-16 ***
MHF         -1.181e-02  1.165e-03 -10.137  &lt; 2e-16 ***
B13          1.058e-02  8.541e-04  12.383  &lt; 2e-16 ***
B12         -6.434e-04  1.160e-04  -5.547 2.91e-08 ***
MTP         -2.785e-03  5.401e-04  -5.156 2.52e-07 ***
BI4          3.914e-04  1.229e-05  31.849  &lt; 2e-16 ***
ALT          2.641e-03  2.982e-05  88.581  &lt; 2e-16 ***
BI1          2.611e-01  2.355e-03 110.867  &lt; 2e-16 ***
URB          5.909e-01  7.080e-02   8.346  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 98797  on 72919  degrees of freedom
Residual deviance: 54626  on 72904  degrees of freedom
AIC: 54658

Number of Fisher Scoring iterations: 5
</code></pre>
"
"0.091874672876503","0.0963061447907242","210015","<p>I have a statistical question.</p>

<p>I have data from an experiment with two conditions (dichotomous IV: 'condition'). I also want to make use of another IV which is metric ('hh'). My DV is also metric ('attention.hh'). I've already run a multiple regression model with an interaction of my IVs. Therefore, I centered the metric IV by doing this:</p>

<pre><code>hh.cen &lt;- as.numeric(scale(data$hh, scale = FALSE))
</code></pre>

<p>with these variables I ran the following analysis:</p>

<pre><code>model.hh &lt;- lm(attention.hh ~ hh.cen * condition, data = data)
summary(model.hh)
</code></pre>

<p>The results are as follows:</p>

<pre><code>Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)        0.04309    3.83335   0.011    0.991
hh.cen             4.97842    7.80610   0.638    0.525
condition          4.70662    5.63801   0.835    0.406
hh.cen:condition -13.83022   11.06636  -1.250    0.215
</code></pre>

<p>However, the theory behind my analysis tells me, that I should expect a quadratic relation of my metric IV (hh) and the DV (but only in one condition).</p>

<p>Looking at the plot, one could at least imply this relation:
<a href=""http://i.stack.imgur.com/k47jD.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/k47jD.png"" alt=""enter image description here""></a></p>

<p>Of course I want to test this statistically. However, I'm struggling now if and when to center the metric IV, because the interaction is still important (as there is only a quadratic relation in one condition).</p>

<p>Do I first center the variable and then compute the quadratic term or the other way round?
Do I compute the interaction with both the linear and the quadratic term or only one of them?</p>

<p>My gut feeling would suggest something like this:</p>

<pre><code>hh.sqr &lt;- hh * hh

sqr.model.hh &lt;- lm(attention.hh ~ hh.sqr + hh.cen * condition, data = data)
    summary(sqr.model.hh)
</code></pre>

<p>In a nutshell, I want to test whether there is a quadratic relation in one of my conditions.
However, I am not sure which terms I have to include into the model (or whether I calculate hh.sqr * condition vs. hh.cen * condition -- or both)?!</p>
"
"0.0770871758684916","0.0826418755551823","210900","<p>I have one outcome/dependent variable that can be ordinal or nominal and 3 independent variables. I have learned so far how to perform ordinal and multinomial logistic regression in SPSS between a single independent variable and the outcome variable.</p>

<p>However, I am more interested in examining the effect of the combinations of independent variables on the outcome variable. I could not find an answer online and I am new to regressions.</p>

<p>For example, the first independent variable X1 levels are friends and public, and he second independent variable X2 levels are location and time. So, I want to examine the effect on the outcome var. (what is the coefficient or estimate) in the following four cases that as a mix of X1 and X2 level:</p>

<ul>
<li>X1= friends and X2= location</li>
<li>X1= friends and X2= time</li>
<li>X1= public and X2= location </li>
<li>X1= public and X2= time</li>
</ul>

<p>I would really appreciate it if you have any thought of to perform such nested analysis whether ordinal or multinomial regression. The only way I thought of is to have the different combinations of X1 and X2 merged in one column and have another column for the outcome variable and run the analysis in SPSS but I am not sure if this the right way.
If there is no way to do that is SPSS, I would be happy to know to do it in R.</p>

<p>Many thanks.</p>
"
"0.146262632480641","0.14518697603476","211518","<p>I am trying to understand how KRR works for drug-protein-interaction and many aspects of it seem very confusing.</p>

<p>Supposing I have a data set as follows of Drug-Protein interactions; values show how tightly a drug binds to a target, some of the interactions are missing (NaN), and those are the ones I am trying to predict.
Numbers I am giving here are only and only made-up numbers for the sake of explanation, since I cannot copy the entire data set as it contains 100 drugs and 100 proteins. So every number you see here is just a random number!</p>

<pre><code>           [,Protein1] [,Protein2] [,Protein3] [,Protein4] [,Protein5] [,Protein6]
[Drug1,]  6.763232 8.97455 5.655 3.3245454 NaN 3.9232321
[Drug2,]  1.211123 2.34343 9.344 NaN 5.6445 4.343
[Drug3,]  1.3429286  2.8805642 6.1998635 Nan 2.328635 9.34343
[Drug4,]  6.5210577  7.1228635 NaN 4.1228635 4.9998635 6.002805
[Drug5,]  NaN  0.9230754 8.34343 9.09098 7.66575 3.9900
[Drug6,] 1.2167197 0.6700215 0.999 NaN 5.553 1.34343
</code></pre>

<p>The approach used in drug discovery is then to compute similarities between proteins and similarities between drugs.</p>

<p>Therefore, there is a <strong>Drug Kernel</strong> computed to show similarities between all drugs (e.g. from online databases).</p>

<pre><code>           [,Drug1] [,Drug2] [,Drug3] [,Drug4] [,Drug5] [,Drug6]
[Drug1,]  6.454 8.788 5.655 3.3245454 3.32233 3.9232321
[Drug2,]  6.211123 7.34343 9.344 1.2121 5.6445 4.343
[Drug3,]  5.3429286  2.8805642 6.1998635 6.7765 2.328635 9.34343
[Drug4,]  4.5210577  1.1228635 7.34 2.1228635 3.9998635 5.002805
[Drug5,]  9.34  0.9230754 1.34343 9.09098 7.66575 3.9900
[Drug6,]  1.2167197 0.6700215 1.999 1.23 5.553 1.34343
</code></pre>

<p>And then protein similarities are computed based on some approach. This matrix will be the <strong>Protein Kernel</strong>.</p>

<pre><code>           [,Protein1] [,Protein2] [,Protein3] [,Protein4] [,Protein5] [,Protein6]
[Protein1,]  50 80 90 10 20 30
[Protein2,]  60 70 10 10 35 75
[Protein3,]  99 89 51 69 48 10
[Protein4,]  10 54 68 97 64 17
[Protein5,]  60 58 95 64 10 16
[Protein6,]  88 14 97 63 63 10
</code></pre>

<p>Then the Kronecker Product is computed for Drug Kernel and Protein Kernel, which directly relates protein-drug pairs.</p>

<p><a href=""http://i.stack.imgur.com/1CRW4.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1CRW4.png"" alt=""enter image description here""></a></p>

<p>Here K is the matrix containing Kronecker Products. So basically, it's a bigger matrix, for this case where we have 6 Proteins and 6 Drugs, the K matrix becomes a 36 x 36 matrix.</p>

<p>Now alpha coefficients are computed for Kernel Ridge Regression with the following formula.</p>

<p><a href=""http://i.stack.imgur.com/iJppt.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/iJppt.png"" alt=""enter image description here""></a></p>

<p>K is the kernel matrix that relates drug-target pairs [therefore, Kronecker Products]
y is the vector with the labels (binding affinities) [So I assume it is just the vector version of the very first matrix in this post, that is the Drug-Protein interaction matrix, <strong>is this correct?</strong>]
I is the identity matrix (of the same size as the kernel matrix),
lambda is the regularization parameter, set preferably to 0.1.</p>

<p>Up to here, I have been able to do everything in R. But my problem starts when I have to do the actual prediction. I do not understand the idea behind KRR, and how to predict those NaN values based on the Kronecker Product K matrix values..</p>

<p>The formula for KRR is:
To compute the prediction for the test point using the equation for g(x) this is the formula</p>

<p><a href=""http://i.stack.imgur.com/HE2zZ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/HE2zZ.png"" alt=""enter image description here""></a>
where x is a test point and x_iâ€™s are training points</p>

<p>My biggest confusion here is, <strong><em>WHAT should I actually put instead of X and X_i?</em></strong> Out of all the matrices I have, which is X for the formula above and which one contains the X_i values?
And how actually can the values in the K matrix be the basis for predicting the values in the very first matrix here?!</p>

<p><strong>Any help and guidance</strong> will be extremely appreciated as I am very confused understanding how KRR works, especially understanding how it works for Drug-Target interaction when having Kronecker Products. So any input here will be really welcome</p>

<p>(<a href=""http://arxiv.org/pdf/1601.01507.pdf"" rel=""nofollow"">http://arxiv.org/pdf/1601.01507.pdf</a>  A paper analyzing what I am trying to do, i.e. relating drugs to proteins by Kronecker Products and then applying KRR, reading the whole paper didn't really clear up anything for me.)</p>
"
"0.068136080998913","0.0649295895722714","211562","<p>I'm trying to learn how to use R by replicating the regression of the <a href=""http://dx.doi.org/10.2307/2118477"" rel=""nofollow"">MRW 1992</a> paper (see Table 1). I've done this in Mathematica, and I got the right coefficients, even for the intercept.</p>

<p>The following R script is what I'm using for this example:</p>

<pre><code>library(foreign)
mrw &lt;- read.dta(""https://www.nuffield.ox.ac.uk/teaching/economics/bond/mrw.dta"")

mrw$popgrowth &lt;- mrw$popgrowth/100 #the data is in percentage points
mrw$gdpgrowth &lt;- mrw$gdpgrowth/100 #the data is in percentage points

noil &lt;- mrw[mrw$n==1,] #non-oil countries

form1noil &lt;- log(rgdpw85) ~ 1+log(i_y)+log(popgrowth+0.05) #we have to add 0.05 (see paper)
noil.lm &lt;- lm(form1noil, data = noil)
summary(noil.lm) #the intercept has the wrong value !
#Call:
#lm(formula = form1noil, data = noil)
#
#Residuals:
#     Min       1Q   Median       3Q      Max 
#-1.79144 -0.39367  0.04124  0.43368  1.58046 
#
#Coefficients:
#                      Estimate Std. Error t value Pr(&gt;|t|)    
#(Intercept)            -1.1279     1.4274  -0.790 0.431371    
#log(i_y)                1.4240     0.1431   9.951  &lt; 2e-16 ***
#log(popgrowth + 0.05)  -1.9898     0.5634  -3.532 0.000639 ***
#---
#Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
#
#Residual standard error: 0.6891 on 95 degrees of freedom
#Multiple R-squared:  0.6009,   Adjusted R-squared:  0.5925 
#F-statistic: 71.51 on 2 and 95 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>The strange thing is that I get the wrong intercept, but all the remaining betas are correct, and so are the R^2 and the std errors. Why is that?</p>

<p>Any help would be appreciated.</p>

<p>EDIT:  Mathematica Code and output (data is imported from an excel file, with same values. sav is the i_y column)</p>

<pre><code>YL85 = data[[1, 2 ;; n, 3]];
sav = data[[1, 2 ;; n, 6]]/100;
popgr = data[[1, 2 ;; n, 5]]/100;

logYL85 = Log[YL85];
logsav = Log[sav];
logrates = Log[popgr + 0.05];

lm = LinearModelFit[
   Transpose[{logsav, logrates, logYL85}], {x1, x2}, {x1, x2}];

Normal[lm]

5.42988 + 1.42401 x1 - 1.98977 x2
</code></pre>
"
"0.105555962793852","0.0922061136661462","211951","<p>Out of curiosity, I conducted the following simulation (code below). Why is it that when the variance of the error term is large coefficient associated with the intercept is biased? Can you recommend some reference that discusses this? Or better yet is there a formal proof of that?</p>

<pre><code>rm(list=ls())
set.seed(12345)
m  &lt;- 10000
x1 &lt;- runif(m,0,100)       # random numbers from uniform distribution
x2 &lt;- 1:10000
u  &lt;- rnorm(m,0,100)       # random numbers from standard normal distribution
y  &lt;- 5*x1 + 2*x2 + 10 + u # generating y series
data  &lt;- cbind(x1, x2, y)
beta1 &lt;- c()
beta2 &lt;- c()
beta3 &lt;- c()
R2    &lt;- c()
n       &lt;- 1000 # number of loops
ksubset &lt;- 100  # length of subset
for (i in 1:n){
  datam &lt;- data.frame(data[sample(nrow(data),ksubset), ])
  ols   &lt;- summary(lm(y~x1+x2, data=datam))
  beta1 &lt;- append(beta1, ols$coefficients[1])
  beta2 &lt;- append(beta2, ols$coefficients[2])
  beta3 &lt;- append(beta3, ols$coefficients[3])
  R2    &lt;- append(R2,    ols$r.squared)
}

results &lt;- c(mean(beta1), mean(beta2), mean(beta3))
results
## [1] 7.290909 5.027431 2.000345
</code></pre>

<p>The code takes thousand random samples of size 100 from the population and calculates a regression in each. Then I take the average of each of the estimated coefficients, which in theory should be equal to the original model. It works great for the slopes, but not for the intercept.</p>

<p><strong>Update</strong></p>

<p>I have noticed that with the population set at 10,000, the bias persists. If I increase the size of the population to 1,000,000, the bias disappears. The size of the target population was insufficient. Anyway, @Maarten's answer is a step in the right direction.</p>

<p><strong>Update2</strong></p>

<p>Entire population results:</p>

<pre><code>&gt; summary(lm(y~x1+x2))       
Call:
lm(formula = y ~ x1 + x2)

Residuals:
    Min      1Q  Median      3Q     Max 
-367.24  -66.37   -0.33   66.64  385.26 

Coefficients:
             Estimate Std. Error  t value Pr(&gt;|t|)    
(Intercept) 7.0388165  2.6626801    2.644  0.00822 ** 
x1          5.0296554  0.0348480  144.331  &lt; 2e-16 ***
x2          2.0002959  0.0003465 5772.764  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 100 on 9997 degrees of freedom
Multiple R-squared:  0.9997,  Adjusted R-squared:  0.9997 
F-statistic: 1.667e+07 on 2 and 9997 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>I updated the question, including simulation results (result). I also present the model results with the entire population, which in effect is biased due to the huge variance. The simulation, as expected, reproduce those results correctly. I repeat, my mistake is in the size of the target population.</p>
"
"0.118246329960506","0.112681644735122","212026","<p>Lets say I have a multiple regression model, where I predict <code>y</code> from predictors <code>x1</code> and <code>x2</code>. Here are some example data and code for R.</p>

<pre><code>#generate data
x1 &lt;- seq(1,10,by=0.1)
x2 &lt;- rnorm(91, 100, sd=4)
y &lt;- x1*0.3 + x2 * 0.1
#add noise.
y&lt;- y + rnorm(length(y))

#model and summary
model &lt;- lm(y~x1 + x2)
summary(model)
Call:
lm(formula = y ~ x1 + x2)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.32345 -0.67679 -0.01036  0.77334  2.60149 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  4.15715    2.73185   1.522   0.1317    
x1           0.23871    0.03996   5.973 4.83e-08 ***
x2           0.06152    0.02721   2.261   0.0262 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 1.001 on 88 degrees of freedom
Multiple R-squared:  0.3214,    Adjusted R-squared:  0.306 
F-statistic: 20.84 on 2 and 88 DF,  p-value: 3.888e-08
</code></pre>

<p>Now, I want to plot how each predictor, <code>x1</code> and <code>x2</code>, affects <code>y</code>, over the range of <code>x1</code> and <code>x2</code> in the data set. To do this, I first extract model coefficients, and generate ranges and means of each predictor. I then calculate the values of <code>y</code> over the range of each predictor, holding the other predictor constant at its mean. For example:</p>

<pre><code>#grab model coefficients
p&lt;-coef(model)

#calculate effect sizes over the range of each predictor, at the mean of all other values
m.x1 &lt;- mean(x1)
m.x2 &lt;- mean(x2)

#get range of each predictor, subdivide into 100 increments for plotting.
range.x1 &lt;- seq(min(x1),max(x1), by = (max(x1) - min(x1)) / 99)
range.x2 &lt;- seq(min(x2),max(x2), by = (max(x2) - min(x2)) / 99)

effect.x1 &lt;- p[1] + p[2]*range.x1 + p[3]*m.x2
effect.x2 &lt;- p[1] + p[2]*m.x1     + p[3]*range.x2 

#plot effects over range
plot(effect.x1, lwd=0, ylim=c(10,13))
lines(smooth.spline(effect.x1), lwd = 2)
lines(smooth.spline(effect.x2), lwd = 2, lty = 2)
legend(5,13, legend = c('x1','x2'), lty = c(1,2),lwd=2, box.lwd=0)
</code></pre>

<p>Here is the plot:</p>

<p><a href=""http://i.stack.imgur.com/trncW.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/trncW.png"" alt=""enter image description here""></a></p>

<p>My questions:</p>

<ol>
<li><p>I would like to shade a region of uncertainty around each of these effect lines. My first intuition would be to use the standard error estimate for each predictor from the multiple regression output. However, Each effect estimate also includes uncertainty associated with the estimate of the intercept and the estimate of the other predictor in the model. How can I incorporate all of these sources of uncertainty to generate uncertainty bounds for each of my effect estimates?</p></li>
<li><p>Presuming there is a good answer to question 1, how would I best go about implementing this in R? (I realize this question may be better posed on stackoverflow).</p></li>
</ol>
"
"0.0609427635336005","0.0580747904139041","212446","<p>I'd like to use elastic net regression for coefficient estimate and parameter selection on a data set that includes nested structure. I've been experimenting with lassop{MMS} to do so. I'm not a statistician by training, and I'm having a difficult time deciphering how to translate the example provided with the documentation to a real-data context.</p>

<pre><code>    require(lme4)
    require(lmerTest)
    require(MMS)
    data(grouseticks)
    ?grouseticks # sample data w/ multiple grouping levels
    n&lt;-length(grouseticks$TICKS)
#two dummy variables for additional fixed effects that we'll assume will be selected out
    dv1&lt;-rnorm(n, mean = 0, sd = 1)
    dv2&lt;-rnorm(n, mean=3, sd=2)
#sample saturated ME model, two terms for random intercept. I'm trying to write this in lassop syntax. 
sat_lmm&lt;- lmer(TICKS~YEAR+HEIGHT+YEAR+dv1+dv2+HEIGHT:dv1+(1|BROOD)+(1|LOCATION), data=grouseticks, REML=FALSE)
summary(sat_lmm)
</code></pre>

<p>How would set up the random effects and grouping matrices to mimic the above model formulation? Feel free to rip into this, I know my grouping and random effects matrices are desperately wrong.</p>

<pre><code>x&lt;-getME(sat_lmm,name = c( ""X""))
x&lt;-x[,c(""(Intercept)"" , ""HEIGHT"",""dv1"" ,""HEIGHT:dv1"",  ""dv2""  , ""YEAR96"" , ""YEAR97"")]
#rearrange variables so that first 3 collumns will be frozen in
y&lt;-as.numeric(getME(sat_lmm,name = c( ""y"")))

# this was my naive guess at handling  random effects
zlx&lt;-cbind( factor(grouseticks$BROOD, labels=seq(length(unique(grouseticks$BROOD)))),
            factor(grouseticks$LOCATION, labels=seq(length(unique(grouseticks$LOCATION)))))

#dummy grouping variable
gx&lt;-rbind(rep(1, length(n)) , 
          rep(1, length(n)))

require(glmnet)
lam&lt;-cv.glmnet(x, y, alpha=0.8, standardize=TRUE)
plot(lam)
#value of lambda that gives minimum cross-validation error
lammin&lt;-lam$lambda.min
lamlse&lt;-lam$lambda.lse
melasso.minlam&lt;-lassop(data=x,
                       Y=y,
                       z=zlx, 
                       mu=lammin,
                       fix=3,
                       D=TRUE,
                       alpha=0.8,
                       showit=F)
#as this stands, it won't run.
print(melasso.minlam)
</code></pre>
"
"0.0621994475718397","0.0592723347638087","212453","<p>In my research project I have to do a regression of the financial risk on the business risk of the year before.
As a reference, I have a paper showing the results for several countries. The paper states that, for the country I am interested in, the coefficient of the regression is negative. their year span :1995-2008</p>

<p>I have to perform the same analysis at region level for the country I am interested in. I find a negative lagged correlation coefficient. However, the coefficient in my regression is positive and significant. My year span:2000-20014.</p>

<p>Should I not have a negative coefficient as in the country-level regression (paper).
Thank you.</p>
"
"0.101062140164153","0.0963061447907242","212903","<p>I have the data <a href=""https://docs.google.com/spreadsheets/d/1lEzUt0QdFCp1ho-iWd4HzEIZoo8IyAM8YP2gu-K7BQo/edit?usp=sharing"" rel=""nofollow"">here</a>.But When I tried to build the logistic regression model using glm function its shows NA in TotalVisits. I have found similar question on stack overflow but that is answered for linear model.  </p>

<pre><code> str(quality)
'data.frame':   131 obs. of  14 variables:
 $ MemberID            : int  1 2 3 4 5 6 7 8 9 10 ...
 $ InpatientDays       : int  0 1 0 0 8 2 16 2 2 4 ...
 $ ERVisits            : int  0 1 0 1 2 0 1 0 1 2 ...
 $ OfficeVisits        : int  18 6 5 19 19 9 8 8 4 0 ...
 $ Narcotics           : int  1 1 3 0 3 2 1 0 3 2 ...
 $ DaysSinceLastERVisit: num  731 411 731 158 449 ...
 $ Pain                : int  10 0 10 34 10 6 4 5 5 2 ...
 $ TotalVisits         : int  18 8 5 20 29 11 25 10 7 6 ...
 $ ProviderCount       : int  21 27 16 14 24 40 19 11 28 21 ...
 $ MedicalClaims       : int  93 19 27 59 51 53 40 28 20 17 ...
 $ ClaimLines          : int  222 115 148 242 204 156 261 87 98 66 ...
 $ StartedOnCombination: logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
 $ AcuteDrugGapSmall   : int  0 1 5 0 0 4 0 0 0 0 ...
 $ PoorCare            : int  0 0 0 0 0 1 0 0 1 0 ...



table(is.na(quality))
FALSE 
1834
</code></pre>

<p>My data does not contain any NA values.</p>

<pre><code>set.seed(100)
split &lt;- sample.split(quality$PoorCare, SplitRatio = .5)
train &lt;-subset(quality, split ==TRUE)
test &lt;- subset(quality, split ==FALSE)
</code></pre>

<p>Building the model using all variable </p>

<pre><code>log.Quality &lt;- glm(PoorCare ~ ., data = train, family = 'binomial')

summary(log.Quality)      
Call:
glm(formula = PoorCare ~ ., family = ""binomial"", data = train)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.5679  -0.6384  -0.3604  -0.1154   2.1298  

Coefficients: (1 not defined because of singularities)
                          Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)              -3.583178   1.807020  -1.983   0.0474 *
MemberID                 -0.008742   0.010988  -0.796   0.4263  
InpatientDays            -0.106578   0.095632  -1.114   0.2651  
ERVisits                  0.275225   0.310364   0.887   0.3752  
OfficeVisits              0.126433   0.066140   1.912   0.0559 .
Narcotics                 0.190862   0.106890   1.786   0.0742 .
DaysSinceLastERVisit     -0.001221   0.002026  -0.603   0.5467  
Pain                     -0.020104   0.023057  -0.872   0.3832  
TotalVisits                     NA         NA      NA       NA  
ProviderCount             0.046297   0.040637   1.139   0.2546  
MedicalClaims             0.025123   0.030564   0.822   0.4111  
ClaimLines               -0.010384   0.012746  -0.815   0.4152  
StartedOnCombinationTRUE  2.205058   1.724923   1.278   0.2011  
AcuteDrugGapSmall         0.217813   0.139890   1.557   0.1195  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 72.549  on 64  degrees of freedom
Residual deviance: 49.213  on 52  degrees of freedom
AIC: 75.213

Number of Fisher Scoring iterations: 6
</code></pre>

<p>Can anyone provide me a good explanation why this is happening ? </p>
"
"0.0867230728520531","0.0734594449379399","212990","<p>Hello I am having some troubles in R when I try to make a summary of a quantile regression with my data.</p>

<p>When I try this:</p>

<pre><code>df &lt;- read.csv(""https://raw.githubusercontent.com/swhatelse/rq_problem/master/data.csv"")
fit &lt;- rq(data=df,formula=time_per_pixel~vector_length,tau=.05,method=""fn"")
summary(fit,se=""nid"")
</code></pre>

<p>I get this error:</p>

<blockquote>
  <p>: Error in base::backsolve(r, x, k = k, upper.tri = upper.tri, transpose = transpose,  : 
  :   singular matrix in 'backsolve'. First zero on the diagonale [1]
  : More over : Warning message:
  : In summary.rq(fit, se = ""nid"") : 14688 non-positive fis</p>
</blockquote>

<p>If I try other se methods, the coefficients returned by the summary are not correct. For example when I check the coef by directly printing the result of the regression I get this:</p>

<pre><code>Call:
rq(formula = time_per_pixel ~ vector_length, tau = 0.05, data = df, method = ""fn"")

Coefficients:
(Intercept) vector_length 
5.493212e-11  2.338409e-11
</code></pre>

<p>And when I use </p>

<pre><code>summary(fit,se=""rank"")
</code></pre>

<p>I get:</p>

<pre><code>Call: rq(formula = time_per_pixel ~ vector_length, tau = 0.05, data = df, 
method = ""fn"")

tau: [1] 0.05

Coefficients:
             coefficients lower bd upper bd
(Intercept)   0            0        0       
vector_length 0            0        0       
</code></pre>

<p>Same problem with iid and boot methods:</p>

<pre><code>Call: rq(formula = time_per_pixel ~ vector_length, tau = 0.05, data = df, 
    method = ""fn"")

tau: [1] 0.05

Coefficients:
              Value   Std. Error t value Pr(&gt;|t|)
(Intercept)   0.00000 0.00000    0.04217 0.96637 
vector_length 0.00000 0.00000    0.13871 0.88968 
</code></pre>

<p>It seems that it comes from the fact that I have heteroscedastic data but I am not sure because with the sample of data engel given with the quantreg library which is heteroscedastic, I have no problem. </p>

<p>In the quantreg vignette they suggest to go with a logarithmic scale with heteroscedastic data. But when I try to come back in a normal scale my coefficients are not correct.
The log converted coeff are:</p>

<pre><code>fit_log &lt;- rq(data=df,formula=I(log(time_per_pixel))~vector_length,tau=.05,method=""fn"")
exp(fit_log$coefficients)

Degrees of freedom: 23120 total; 23118 residual
 (Intercept) vector_length 
1.274531e-10  1.093925e+00
</code></pre>

<p>The correct coeff are:</p>

<pre><code>fit &lt;-rq(data=df,formula=time_per_pixel~vector_length,tau=.05,method=""fn"")
fit
Call:
rq(formula = time_per_pixel ~ vector_length, tau = 0.05, data = df, 
   method = ""fn"")

Coefficients:
  (Intercept) vector_length 
 5.493212e-11  2.338409e-11 
</code></pre>

<p>So I would like to understand why it does not work with my data and how can I deal with that?</p>

<p>Thanks by advance.</p>
"
"0.0746393370862076","0.0711268017165705","213117","<p>My question concerns the calculation of sampling variance for studies using meta-regression or systematic review methods. I am using raw mean differences from different surveys conducted in the same country in the same year, the measurement scales are the same, so i am using raw mean differences. To see the variation in these survey, I intend to use meta data from the source surveys. </p>

<p>However, Running the model in R to see the confidence interval of the model coeffs shows me the following message <strong><em>""   Cannot compute confidence interval for the amount of (residual) heterogeneity with non-positive sampling variances in the data.""</em></strong></p>

<p>How can I find the confidence interval for the model coefficients to know which one to use as a moderator ? </p>
"
"0.0746393370862076","0.0592723347638087","213253","<p>In general, my question is how to estimate some prediction intervals in the case of penalized linear models (in particular, I think about the glmnet R package). I understood that the introduction of a penalization in the objective function generates a shrinkage effect, which is a bias on the estimated coefficients. 
I understand that in this case the calculation of the uncertainties is troublesome</p>

<p><a href=""https://air.unimi.it/retrieve/handle/2434/153099/133417/phd_unimi_R07738.pdf"" rel=""nofollow"">https://air.unimi.it/retrieve/handle/2434/153099/133417/phd_unimi_R07738.pdf</a></p>

<p>(see sections 3.2 and 3.3 the quoted papers)</p>

<p>Two bootstrap methods (random x vs fixed x) are discussed in the context of standard linear models here</p>

<p><a href=""http://stats.stackexchange.com/questions/64813/two-ways-of-using-bootstrap-to-estimate-the-confidence-interval-of-coefficients?rq=1"">Two ways of using bootstrap to estimate the confidence interval of coefficients in regression</a></p>

<p>but again the focus is on the beta coefficients.
However, I am not interested in the estimate of the confidence intervals on the beta coefficients, but only on the predicted values.
For instance, consider the following R code</p>

<pre><code>library(glmnet)


# Generate data
set.seed(19875)  # Set seed for reproducibility
n &lt;- 1000  # Number of observations
p &lt;- 5000  # Number of predictors included in model
real_p &lt;- 15  # Number of true predictors
x &lt;- matrix(rnorm(n*p), nrow=n, ncol=p)
y &lt;- apply(x[,1:real_p], 1, sum) + rnorm(n)

# Split data into train (2/3) and test (1/3) sets
train_rows &lt;- sample(1:n, .66*n)
x.train &lt;- x[train_rows, ]
x.test &lt;- x[-train_rows, ]

y.train &lt;- y[train_rows]
y.test &lt;- y[-train_rows]



fit.elnet &lt;- glmnet(x.train, y.train, family=""gaussian"", alpha=.5)

yhat &lt;- predict(fit.elnet, s=fit.elnet$lambda, newx=x.test)
</code></pre>

<p>Does anybody know how to calculate a meaningful confidence interval for yhat?</p>

<p>Thanks!</p>
"
"0.118178950386495","0.100762969098475","213982","<p>I am trying to use ""propodds""  in the VGAM function in R, but am not sure if I am doing it right and don't really understand how to analyze the output I got so far to check to see if I am using it right. Any help on how to correctly use ""propodds"" or analyze the output would be appreciated. This is what I have so far:</p>

<pre><code>    &gt; fittest &lt;-vglm(rp ~ is.native + is.male + age2 + is.debt + oh + ms + cjs, propodds, data = dummydata2)
&gt; fittest
Call:
vglm(formula = rp ~ is.native + is.male + age2 + is.debt + oh + 
    ms + cjs, family = propodds, data = dummydata2)

    Coefficients:
(Intercept):1 (Intercept):2 (Intercept):3 (Intercept):4 (Intercept):5     is.native 
  2.827674173  -0.463602645  -0.474290665  -0.614877500  -2.514394420  -0.063546621 
      is.male          age2       is.debt            oh            ms           cjs 
  0.114052675   0.067835161  -0.058563607  -0.089420626   0.109135966   0.003937505 

Degrees of Freedom: 52000 Total; 51988 Residual
Residual deviance: 24702.04 
Log-likelihood: -12351.02 
&gt; summary(fittest)

Call:
vglm(formula = rp ~ is.native + is.male + age2 + is.debt + oh + 
    ms + cjs, family = propodds, data = dummydata2)

Pearson residuals:
                    Min      1Q  Median      3Q     Max
logit(P[Y&gt;=2])  -5.1080  0.1300  0.2803  0.3016  0.3335
logit(P[Y&gt;=3])  -0.6976 -0.5876 -0.5490  0.5937 14.7717
logit(P[Y&gt;=4]) -13.1157 -0.5173 -0.4831  0.6080  3.0626
logit(P[Y&gt;=5])  -4.0174 -0.4072 -0.3746  1.0167  1.2176
logit(P[Y&gt;=6])  -0.6164 -0.5749 -0.1610 -0.1541  3.6060

Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept):1  2.827674   0.079827  35.423  &lt; 2e-16 ***
(Intercept):2 -0.463603   0.068894  -6.729 1.71e-11 ***
(Intercept):3 -0.474291   0.068901  -6.884 5.83e-12 ***
(Intercept):4 -0.614878   0.069009  -8.910  &lt; 2e-16 ***
(Intercept):5 -2.514394   0.074892 -33.573  &lt; 2e-16 ***
is.native     -0.063547   0.062409  -1.018  0.30857    
is.male        0.114053   0.039694   2.873  0.00406 ** 
age2           0.067835   0.024789   2.737  0.00621 ** 
is.debt       -0.058564   0.052983  -1.105  0.26902    
oh            -0.089421   0.057526  -1.554  0.12008    
ms             0.109136   0.041587   2.624  0.00868 ** 
cjs            0.003938   0.043653   0.090  0.92813    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Number of linear predictors:  5 

Names of linear predictors: 
logit(P[Y&gt;=2]), logit(P[Y&gt;=3]), logit(P[Y&gt;=4]), logit(P[Y&gt;=5]), logit(P[Y&gt;=6])

Dispersion Parameter for cumulative family:   1

Residual deviance: 24702.04 on 51988 degrees of freedom

Log-likelihood: -12351.02 on 51988 degrees of freedom

Number of iterations: 4 

Exponentiated coefficients:
is.native   is.male      age2   is.debt        oh        ms       cjs 
0.9384304 1.1208112 1.0701889 0.9431182 0.9144608 1.1153140 1.0039453 
</code></pre>

<p>A little background on my data that may help: I am trying to determine if risk preferences (variable ""rp"" in the code) is determined by immigration status (variable ""is.native"" in the code, which is a dummy variable where 0 = native and 1 = immigrant). I have a few factors that I want to control for since they may affect risk preferences [age2, is.debt, oh(owns home), ms (marital status), and cjs (current job status)]. Based on similar research the best way to analyze this is the cumulative logistic regression and they seemed to look at the proportional odds. The data came from the 2014 Health and Retirement Study which is representative of the US population over age 50. There are about 20,000 participants. </p>

<p>I'm not sure if my model is formatted correctly. ""rp"" has 6 categories - a control group, low risk tolerance (rt), some rt, high rt, substantial rt and ""ignore"" which is answers of ""don't know"" or ""NA"".  All other variables are dummy variables with only options for ""0"" or ""1"" besides ""age2"" which has 6 categories (under 50, 50-60, 60-70, 70-80, 80-90, 90+). Are these dummy variables appropriate to use or should I just use the actual answers provided by the participants?</p>

<p>I know the significant codes in the ""summary"" section tell me gender, age, and marital status are significant at the 1% significance level, but I don't understand any of the other results. Such as, what does it mean that all the intercepts are significant? Is the model as a whole significant? What is the dispersion parameter? What are the exponentiated coefficients? </p>
"
"0.100550429837333","0.0821301562353182","214200","<p>I was wondering if I could have some help analyzing the output from a regression. Some background on what I'm trying to find/my data:</p>

<p>I am trying to determine if immigration status is a determinant of risk preferences. My data comes from the 2014 Health and Retirement Survey with about ~20,000 participants. The risk measure (""rp"" in the equation) is my ""y"" variable. ""rp"" is a categorical variable, with levels of no risk, low risk, some risk, etc. My ""x"" variable is immigration status (""is.native"" in the equation) and is a dummy variable that takes the value of 0 if the person is native to the U.S. and 1 if the person is an immigrant to the U.S.  The other variables in the equation are factors that may effect risk preferences so I want to control for them. My regression is:</p>

<pre><code> fit1_usesrp &lt;-vglm(rp ~ is.native + is.male + oh + cjs +  + age2 + tw,propodds, data = dummydata2)
</code></pre>

<p>and the output is: </p>

<pre><code>fit1_usesrp
Call:
vglm(formula = rp ~ is.native + is.male + oh + cjs + +age2 + 
    tw, family = propodds, data = dummydata2)

Coefficients:
(Intercept):1 (Intercept):2 (Intercept):3 (Intercept):4 (Intercept):5     is.native       is.male 
 2.796024e+00 -4.948484e-01 -5.055242e-01 -6.460196e-01 -2.545089e+00 -7.381110e-02  1.509080e-01 
           oh           cjs          age2            tw 
-2.070633e-02  3.643551e-03  7.149891e-02 -3.092727e-06 

Degrees of Freedom: 52000 Total; 51989 Residual
Residual deviance: 24705.39 
Log-likelihood: -12352.69 

summary(fit1_usesrp)

Call:
vglm(formula = rp ~ is.native + is.male + oh + cjs + +age2 + 
    tw, family = propodds, data = dummydata2)

Pearson residuals:
                    Min      1Q  Median      3Q     Max
logit(P[Y&gt;=2])  -5.0292  0.1305  0.2830  0.2989  0.5201
logit(P[Y&gt;=3])  -0.6676 -0.5986 -0.5517  0.5911 14.8594
logit(P[Y&gt;=4]) -13.1696 -0.5270 -0.4856  0.6051  3.0437
logit(P[Y&gt;=5])  -4.0121 -0.4082 -0.3802  1.0345  1.2412
logit(P[Y&gt;=6])  -0.6133 -0.5764 -0.1601 -0.1548  3.5492

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept):1  2.796e+00  6.689e-02  41.803  &lt; 2e-16 ***
(Intercept):2 -4.948e-01  5.352e-02  -9.246  &lt; 2e-16 ***
(Intercept):3 -5.055e-01  5.353e-02  -9.444  &lt; 2e-16 ***
(Intercept):4 -6.460e-01  5.368e-02 -12.035  &lt; 2e-16 ***
(Intercept):5 -2.545e+00  6.113e-02 -41.635  &lt; 2e-16 ***
is.native     -7.381e-02  6.224e-02  -1.186   0.2357    
is.male        1.509e-01  3.764e-02   4.010 6.08e-05 ***
oh            -2.071e-02  4.747e-02  -0.436   0.6627    
cjs            3.644e-03  4.365e-02   0.083   0.9335    
age2           7.150e-02  2.449e-02   2.920   0.0035 ** 
tw            -3.093e-06  1.479e-06  -2.092   0.0365 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Number of linear predictors:  5 

Names of linear predictors: 
logit(P[Y&gt;=2]), logit(P[Y&gt;=3]), logit(P[Y&gt;=4]), logit(P[Y&gt;=5]), logit(P[Y&gt;=6])

Dispersion Parameter for cumulative family:   1

Residual deviance: 24705.39 on 51989 degrees of freedom

Log-likelihood: -12352.69 on 51989 degrees of freedom

Number of iterations: 4 

Exponentiated coefficients:
is.native   is.male        oh       cjs      age2        tw 
0.9288471 1.1628897 0.9795066 1.0036502 1.0741170 0.9999969 
</code></pre>

<p>All I understand from the output is that the p-values tell me gender, age, and tw (total wealth) are significant. I don't know how to read anything else in the output. Any help would be greatly appreciated. Thank you. </p>
"
"0.090877987331047","0.0799396239731092","214404","<p>Hi I'd like some help plotting the following regression in ""r"": </p>

<pre><code>fit1_usesrp &lt;-vglm(rp ~ is.native + is.male + oh + cjs + age2 + tw ,propodds, data = dummydata
</code></pre>

<p>Through this regression I am interested in finding if immigration status is a determinant of risk preferences. ""rp"" in the regression is a categorical variable that measures respondent's willingness to take risk (no risk tolerance, low risk tolerance, etc.). ""is.native"" is a dummy variable for immigration status (0 = native, 1 = immigrant). oh, cjs, and age2 are all dummy variables for factors that may affect risk preferences. ""tw"" is for total wealth and I used the actual raw values for wealth instead of trying to turn this into a dummy variable. </p>

<p>the output for this regression is:</p>

<pre><code>Call:
vglm(formula = rp ~ is.native + is.male + oh + cjs + age2 + tw, 
    family = propodds, data = dummydata2)

Coefficients:
(Intercept):1 (Intercept):2 (Intercept):3 (Intercept):4 (Intercept):5     is.native       is.male 
 2.796024e+00 -4.948484e-01 -5.055242e-01 -6.460196e-01 -2.545089e+00 -7.381110e-02  1.509080e-01 
           oh           cjs          age2            tw 
-2.070633e-02  3.643551e-03  7.149891e-02 -3.092727e-06 

Degrees of Freedom: 52000 Total; 51989 Residual
Residual deviance: 24705.39 
Log-likelihood: -12352.69 
&gt; summary(fit1_usesrp)

Call:
vglm(formula = rp ~ is.native + is.male + oh + cjs + age2 + tw, 
    family = propodds, data = dummydata2)

Pearson residuals:
                    Min      1Q  Median      3Q     Max
logit(P[Y&gt;=2])  -5.0292  0.1305  0.2830  0.2989  0.5201
logit(P[Y&gt;=3])  -0.6676 -0.5986 -0.5517  0.5911 14.8594
logit(P[Y&gt;=4]) -13.1696 -0.5270 -0.4856  0.6051  3.0437
logit(P[Y&gt;=5])  -4.0121 -0.4082 -0.3802  1.0345  1.2412
logit(P[Y&gt;=6])  -0.6133 -0.5764 -0.1601 -0.1548  3.5492

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept):1  2.796e+00  6.689e-02  41.803  &lt; 2e-16 ***
(Intercept):2 -4.948e-01  5.352e-02  -9.246  &lt; 2e-16 ***
(Intercept):3 -5.055e-01  5.353e-02  -9.444  &lt; 2e-16 ***
(Intercept):4 -6.460e-01  5.368e-02 -12.035  &lt; 2e-16 ***
(Intercept):5 -2.545e+00  6.113e-02 -41.635  &lt; 2e-16 ***
is.native     -7.381e-02  6.224e-02  -1.186   0.2357    
is.male        1.509e-01  3.764e-02   4.010 6.08e-05 ***
oh            -2.071e-02  4.747e-02  -0.436   0.6627    
cjs            3.644e-03  4.365e-02   0.083   0.9335    
age2           7.150e-02  2.449e-02   2.920   0.0035 ** 
tw            -3.093e-06  1.479e-06  -2.092   0.0365 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Number of linear predictors:  5 

Names of linear predictors: 
logit(P[Y&gt;=2]), logit(P[Y&gt;=3]), logit(P[Y&gt;=4]), logit(P[Y&gt;=5]), logit(P[Y&gt;=6])

Dispersion Parameter for cumulative family:   1

Residual deviance: 24705.39 on 51989 degrees of freedom

Log-likelihood: -12352.69 on 51989 degrees of freedom

Number of iterations: 4 

Exponentiated coefficients:
is.native   is.male        oh       cjs      age2        tw 
0.9288471 1.1628897 0.9795066 1.0036502 1.0741170 0.9999969 
</code></pre>
"
"0.115831337698152","0.103887343315634","214449","<p>I am trying to determine if immigration status is a determinant of risk preferences. To do this, I am using the 2014 Health and Retirement Study data which has approximately ~20,000 participants and is representative of residents in the US over age 50. I have two different measures of risk preferences that I am using in this analysis.</p>

<p>The first risk measure is based on a 0-10 rating that participants gave themselves for how risky they are in general situations. The regression using this risk measure looks like this:</p>

<pre><code>fit1_usesRaw &lt;-vglm(Risk_Pct ~ is.native + is.male + oh + cjs + ms + age2 + tw,propodds, data = dummydata2)
</code></pre>

<p>Where the significant variables are ""is.native"", ""is.male"", and ""ms""(marital status). Is.native has a positive coefficient and is.male and ms each have negative coefficients. </p>

<p>The second risk measure is based on the percentage of the participant's retirement accounts kept in stocks. The regression for this risk measure looks like this:</p>

<pre><code>fit2_usesRaw &lt;-vglm(PCT_Stocks_MF_1 ~ is.native + is.male + oh + cjs + ms + age2 + tw + sme,propodds, data = dummydata2)
</code></pre>

<p>Where the significant variables are ""is.native"" (negative coefficient), ""is.male"" (positive coefficient), ""age2"" (positive coefficient), ""tw"" (total wealth, negative coefficient), and ""cjs"" (current job status, negative coefficient). </p>

<p>How can I test to see if the differences between the two risk measures are significant? Is there any way to determine which risk measure is ""right""? I tried a ANOVA test, but I'm unsure if that would be correct. The output for the Anova was:</p>

<pre><code> Anova(w1.mod, test = ""Roy"")

Type II MANOVA Tests: Roy test statistic
          Df test stat approx F num Df den Df    Pr(&gt;F)    
is.native  1  0.008895    7.441      2   1673 0.0006063 ***
is.male    1  0.048695   40.733      2   1673 &lt; 2.2e-16 ***
age2       1  0.020496   17.145      2   1673  4.26e-08 ***
oh         1  0.000329    0.275      2   1673 0.7596626    
ms         1  0.002546    2.130      2   1673 0.1191937    
tw         1  0.002674    2.236      2   1673 0.1071594    
sme        1  0.000688    0.576      2   1673 0.5624582    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Does this tell me there are significant differences between ""is.native"", ""is.male"" and ""age2"" using the two different risk measures? Or am I reading this output completely incorrectly?</p>

<p>Thanks for any help!</p>
"
"NaN","NaN","214512","<p>I'm trying to run a logistic regression with a L2-Penalty on a dataset I have. For the regression coefficient I also want to have the p-values of the signifance or at least the standard errors.
My plan was to do it with Python but unfortunately none of the package fulfilled my needs. </p>

<p>As I need a solution for this problem really quick, I thought to use R in this case. Unfortunately I don't have the time to read all about the package R offers, so I just liked to ask if there is any way to do the above mentioned in R?</p>
"
"0.0304713817668003","0.029037395206952","214608","<p>I have an interpretation problem. As you can see below there's a linear regression output for the CAPM. I don't know how to interpret the significance level. ExIndex has a very low p-value, but the Significance level is 0. So can I reject H0 or not? The same question for the intercept.</p>

<blockquote>
<pre><code>Coefficients:
              Estimate     Std. Error  t value  Pr(&gt;|t|)    
(Intercept) - 0.003258     0.001560    -2.089       0.0377 *  
 ExIndex      0.898980     0.106511     8.440     2.3e-15 ***
 Signif. codes:   0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.02508 on 258 degrees of freedom Multiple
R-squared:  0.2164,    Adjusted R-squared:  0.2133 
F-statistic: 71.24 on 1 and 258 DF,  p-value: 2.304e-15
</code></pre>
</blockquote>
"
"NaN","NaN","214886","<p>Suppose I'm doing regression with training, validation, and test sets. I can find RMSE and R squared (R^2, the coefficient of determination) from the output of my software (such as R's lm() function).</p>

<p>My understanding is that the test RMSE (or MSE) is the measure of goodness of predicting the validation/test values, while R^2 is a measure of goodness of fit in capturing the variance in the training set.</p>

<p>In the real world, what I really care about is generalized prediction accuracy on data I haven't seen. So then what is the utility of the R^2 value compared to RMSE?</p>
"
"0.052777981396926","0.0502942438178979","214892","<p>I'm trying to construct a univariate prediction model using logistic regression in order to predict credit default likelihood from overdue level in telecommunication companies:</p>

<p><a href=""https://drive.google.com/open?id=0BzdYGYN6vfqBeDdKRDFpelFKbTg"" rel=""nofollow"">https://drive.google.com/open?id=0BzdYGYN6vfqBeDdKRDFpelFKbTg</a></p>

<p>For this, I used the function glm and found two problematic ranks:
        RANK_OVERDUE between S/. 3,000 and S/. 5,000 &amp; RANK_OVERDUE More than S/. 5,000.</p>

<p>which have p-values of 0.946 and 0.473:</p>

<pre><code>Call:
glm(formula = impago ~ MONTO_VENCIDO_DOC_IMPAGOS, family = binomial, 
    data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.1355  -0.0569  -0.0569  -0.0569   3.5855  

Coefficients:
                                                  Estimate
(Intercept)                                       -6.42627
RANK_OVERDUE&lt;S/. 0 - S/. 500]         0.69763
RANK_OVERDUE&lt;S/. 1,500 - S/. 3,000]   1.73952
RANK_OVERDUE&lt;S/. 3,000 - S/. 5,000] -10.13980
RANK_OVERDUE&lt;S/. 500 - 1,500]         1.13854
RANK_OVERDUEMÃ¡s de S/. 5,000          0.71916
</code></pre>

<p></p>

<pre><code>                                                 Pr(&gt;|z|)    
(Intercept)                                       &lt; 2e-16 ***
RANK_OVERDUE&lt;S/. 0 - S/. 500]       1.78e-15 ***
RANK_OVERDUE&lt;S/. 1,500 - S/. 3,000] 2.51e-05 ***
RANK_OVERDUE&lt;S/. 3,000 - S/. 5,000]    0.946    
RANK_OVERDUE&lt;S/. 500 - 1,500]       1.23e-06 ***
RANK_OVERDUEMÃ¡s de S/. 5,000           0.473    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 9683.9  on 345828  degrees of freedom
Residual deviance: 9603.5  on 345823  degrees of freedom
AIC: 9615.5

Number of Fisher Scoring iterations: 15
</code></pre>

<p>I would need to know what options I have on order to deal with this situation. Should these ranks be included in the model? I tried to join them into one (overdue over S/. 3,000) but when applying again the model, it continued to be not significant (I obtained a p-value of 0.919).</p>
"
"0.139637413476259","0.120393103280186","214970","<p>I have an Augmented Dickey Fuller Test in R on <code>leg_totalbills</code> that shows I can not reject the null hypothesis: Unit Root. The <code>panel regression</code> that it refers to follows: </p>

<pre><code>Oneway (individual) effect Within Model

t test of coefficients:

                        Estimate  Std. Error t value Pr(&gt;|t|)   
cfcontrol            -6.2618e-02  4.0643e-02 -1.5407 0.123527   
amtsum                7.7718e-05  2.5914e-05  2.9991 0.002735 **
unemplag_1           -8.0897e-03  1.2185e-02 -0.6639 0.506798   
leg_totalbills        3.3680e-02  1.2010e-02  2.8043 0.005083 **
amtsum_post2009years -3.9341e-08  1.2988e-08 -3.0289 0.002480 **
amtsum_5to9exp        4.9215e-05  8.2993e-05  0.5930 0.553236   
amtsum_10to19exp     -2.1085e-05  3.8516e-05 -0.5474 0.584124   
amtsum_20exp          2.2601e-05  2.0130e-05  1.1228 0.261653   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1


Total Sum of Squares:    1755.6
Residual Sum of Squares: 1565.7
R-Squared:      0.1082
Adj. R-Squared: 0.1074
F-statistic: 36.64 on 8 and 2416 DF, p-value: &lt; 2.22e-16
</code></pre>

<p>When I run the same regression without <code>leg_totalbills</code> I get the following output:</p>

<pre><code>Oneway (individual) effect Within Model
Unbalanced Panel: n=10, T=41-444, N=2434

Residuals :
    Min.  1st Qu.   Median  3rd Qu.     Max. 
-2.03000 -0.39500 -0.19800 -0.00907 10.40000 
t test of coefficients:

                        Estimate  Std. Error t value  Pr(&gt;|t|)    
cfcontrol             1.6281e-02  5.2280e-02  0.3114 0.7555022    
amtsum                1.0355e-04  2.6742e-05  3.8723 0.0001107 ***
unemplag_1           -1.1543e-02  1.3483e-02 -0.8561 0.3920043    
amtsum_post2009years -5.3029e-08  1.4348e-08 -3.6960 0.0002239 ***
amtsum_5to9exp        1.3603e-04  1.1062e-04  1.2297 0.2189132    
amtsum_10to19exp      1.8843e-05  2.2286e-05  0.8455 0.3979221    
amtsum_20exp          5.2349e-05  2.8275e-05  1.8514 0.0642311 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1Total Sum of Squares:    1755.6
Residual Sum of Squares: 1722.2
R-Squared:      0.019026
Adj. R-Squared: 0.018893
F-statistic: 6.69688 on 7 and 2417 DF, p-value: 7.058e-08

dftest &lt;- CADFtest(femodel_1$leg_totalbills, max.lag.y = 1)
summary(dftest)
</code></pre>

<p>The Augmented Dickey Fuller test follows:</p>

<pre><code>                                                ADF test
t-test statistic:                          -3.446944e+01
p-value:                                    6.388754e-58
Max lag of the diff. dependent variable:    1.000000e+00

Call:
dynlm(formula = formula(model), start = obs.1, end = obs.T)

Residuals:
   Min     1Q Median     3Q    Max 
 -1840   -369   -253    -13  92819 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 550.710311  86.694498   6.352 2.52e-10 ***
trnd          0.015206   0.060595   0.251    0.802    
L(y, 1)      -0.982389   0.028500 -34.469  &lt; 2e-16 ***
L(d(y), 1)   -0.003719   0.020296  -0.183    0.855    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 2098 on 2428 degrees of freedom
Multiple R-squared:  0.493, Adjusted R-squared:  0.4924 
F-statistic:    NA on NA and NA DF,  p-value: NA
</code></pre>

<p>My interpretation is that I <em>can</em> reject the null hypothesis: Unit Root.</p>

<p>Otherwise, The difference is significant with respect to the R squared. I'm wondering if there is a way to correct for the unit root in <code>leg_totalbills</code> so that I can keep it in the regression.</p>
"
"0.103465538715443","0.105639041939177","215224","<p>I am going to explain my question using a reproducibile toy example. I would like to regress a numerical variable using a multiple regression model with either numerical and categorical variables. I would like to do that without using the functions provided by R, but I am worried that I am not coding the categorical variables properly. These are the toy data:</p>

<pre><code>  mydata &lt;- read.csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")

  mydata$admit &lt;- factor(mydata$admit)
  mydata$gre &lt;- scale(mydata$gre)
  mydata$gpa &lt;- scale(mydata$gpa)
  mydata$rank &lt;- factor(mydata$rank)

  head(mydata)

          admit        gre        gpa rank
        1     0 -1.7980110  0.5783479    3
        2     1  0.6258844  0.7360075    3
        3     1  1.8378321  1.6031352    1
        4     1  0.4527490 -0.5252692    4
        5     0 -0.5860633 -1.2084607    4
        6     1  1.4915613 -1.0245245    2

  model &lt;- lm(gpa ~. , data=mydata)
  #linear multiple regression
  summary(model)
</code></pre>

<p>This is the result using the lm function:</p>

<pre><code>  Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
  (Intercept) -0.04585    0.12924  -0.355   0.7230    
  admit1       0.24980    0.10273   2.432   0.0155 *  
  gre          0.36816    0.04672   7.879 3.24e-14 ***
  rank2       -0.14424    0.13993  -1.031   0.3033    
  rank3        0.14189    0.14723   0.964   0.3358    
  rank4       -0.13094    0.16620  -0.788   0.4313    
</code></pre>

<p>Manually, I am coding the model matrix X in this way:</p>

<pre><code>  mydata$rank2 &lt;- sapply(mydata$rank, function (x){ if(x==2) return(1) else return(0)})
  mydata$rank3 &lt;- sapply(mydata$rank, function (x){ if(x==3) return(1) else return(0)})
  mydata$rank4 &lt;- sapply(mydata$rank, function (x){ if(x==4) return(1) else return(0)})

  X &lt;- data.matrix(mydata[,-c(3,4)])
  Y &lt;- data.matrix(mydata[,3])
  X[,1] &lt;- X[,1] - 1
</code></pre>

<p>So I am creating for each level a binary variable and I am not considering the first level as I saw in the literature. This is the final matrix</p>

<pre><code>  head(X)
       admit        gre rank2 rank3 rank4
  [1,]     0 -1.7980110     0     1     0
  [2,]     1  0.6258844     0     1     0
  [3,]     1  1.8378321     0     0     0
  [4,]     1  0.4527490     0     0     1
  [5,]     0 -0.5860633     0     0     1
  [6,]     1  1.4915613     1     0     0
</code></pre>

<p>but when I compute the regression coefficients in this way:</p>

<pre><code>  Xbeta &lt;- solve(t(X) %*% X) %*% t(X) %*% Y
</code></pre>

<p>I am obtaining different values compared with the ones obtained with lm.
In particular, these are:</p>

<pre><code>               [,1]
  admit  0.23456544
  gre    0.36804870
  rank2 -0.18463006
  rank3  0.09954882
  rank4 -0.17408055
</code></pre>

<p>What I am doing wrong, please? I would like also to compute the residuals, the sd of the coefficients and the t-statistic, but again I am not obtaining the same results of the lm function for them, and I believe it is due to the fact that I am coding in a wrong way the categorical variables.</p>
"
"0.0806196982594614","0.076825726438694","215256","<p>I wanted to do something equivalent to a PCA on a mixed data set containing categorical variables and continuous numerical predictor variables which are normally distributed but measured in very different units. The aims are (a) to explore/describe the variable relationships, and (b) hopefully to reduce the dimensions of the data set for predictive modelling. </p>

<p>Based on this <a href=""http://stats.stackexchange.com/questions/5774/can-principal-component-analysis-be-applied-to-datasets-containing-a-mix-of-cont/5777#5777"">cross validated post</a> I have been using (and liking!) the Factor Analysis of Mixed Data function FAMD() of the FactoMineR package in R.  </p>

<p>But I can't work out if this analysis can be treated the same way as a PCA. Two specific questions:</p>

<ol>
<li>I understand the <a href=""http://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia"">need to scale</a> (ie subtract mean and divide by sd) such numerical variables in a PCA to stop some variables unhelpfully dominating the components. But is this necessary in factor analysis of mixed data as done by the FactoMineR package? Running with both scaled and unscaled as supplementary variables seems to show no difference. 

<ol start=""2"">
<li>Can the principle component dimensions of a mixed data analysis be extracted (from a training set) and applied to a test set, as we might with a true PCA? E.g. if I extract the coordinates of each individual in the training set of Dimension1, then run a regression using the original variables predicting Dimension1, and use the coefficients as the weights of each variable to make a new composite 'Dimension 1 variable' which can be applied to the test set variables - would that be valid?</li>
</ol></li>
</ol>
"
"0.0770871758684916","0.0826418755551823","215447","<p>I have one outcome/dependent variable that can be ordinal or nominal and 3 independent variables. I have learned so far how to perform ordinal and multinomial logistic regression in SPSS between a single independent variable and the outcome variable.</p>

<p>However, I am more interested in examining the effect of the combinations of independent variables on the outcome variable. I could not find an answer online and I am new to regressions.</p>

<p>For example, the first independent variable X1 levels are friends and public, and the second independent variable X2 levels are location and time. So, I want to examine the effect on the outcome var. (what is the coefficient or estimate) in the following four cases that as a mix of X1 and X2 level:</p>

<pre><code>X1= friends and X2= location
X1= friends and X2= time
X1= public and X2= location
X1= public and X2= time
</code></pre>

<p>I would really appreciate it if you have any thought of to perform such nested analysis whether ordinal or multinomial regression. </p>

<p>The only way I thought of is to have the different combinations of X1 and X2 merged in one column and have another column for the outcome variable and run the analysis in SPSS but I am not sure if this the right way. If there is no way to do that is SPSS, I would be happy to know to do it in R.</p>

<p>Many thanks in advance.</p>
"
"0.0812570180448007","0.0871121856208561","215532","<p>I am a clinicians (limited statistical knowledge) who is trying to use mlogit pkg in R to analyze a clinical dataset, running logistic regression on it. I am trying to ascertain if there is any correlation seen in patients with many vars and heart block (var = block (0s and 1s))</p>

<p>I have a dataset with these variables</p>

<pre><code> [1] ""Age""                    ""Sex""                    ""Race""                       ""Obesity""               
 [5] ""CAD""                    ""HTN""                    ""DM""                     ""HLD""                   
 [9] ""CHF""                    ""COPD""                   ""Asthma""                 ""Thyroid.disorder""      
[13] ""Smoking""                ""Illicit.drug.use""       ""Alcohol""                ""INR""                   
[17] ""TB""                     ""AST""                    ""ALT""                    ""Cirrhosis""             
[21] ""Adenosine""              ""Amiodarone""             ""Beta.blocker""           ""CCB""                   
[25] ""Digoxin""                ""TCA""                    ""SSRI""                   ""Antipsychotic""         
[29] ""AV.block""               ""Bundle.branch.block""    ""PAC.PVC""                ""Afib""                  
[33] ""Other.Arrythmia""        ""Nonspecific.ST.wave""    ""Anterioseptal..ST.wave"" ""Anteriolateral.ST.wave""
[37] ""Inferior.ST.wave""       ""Posterior.ST.wave""      ""Axis.deviation""         ""Low.voltage""           
[41] ""Qt.prolongation""        ""Hypertrophy""            ""block""
</code></pre>

<p>Now I have used the mlogit pkg in R</p>

<p>My Code is</p>

<pre><code># Reshaping data
mydata &lt;- mlogit.data(data=mydata, shape=""wide"", choice=""block"")

# Creating Model with all Vars
model &lt;- mlogit(data=mydata, formula=block~0|Age+Sex+Race+Obesity+CAD+HTN+DM+HLD+CHF+COPD+Asthma+Thyroid.disorder+Smoking+Illicit.drug.use+Beta.blocker+CCB+Digoxin+TCA+SSRI+Antipsychotic+Hypertrophy+INR+TB+AST+ALT)
</code></pre>

<p>Generates this error :</p>

<pre><code> Error in solve.default(H, g[!fixed]) : 
 Lapack routine dgesv: system is exactly singular: U[43,43] = 0
</code></pre>

<p>Now if I break the variables into different variables like..</p>

<pre><code>model1 &lt;- mlogit(data=mydata, formula=block~0|Age+Sex+Race+Obesity+CAD+HTN+DM+HLD+CHF+COPD+Asthma+Thyroid.disorder+Smoking)
model2 &lt;- mlogit(data=mydata, formula=block~0|Illicit.drug.use+Beta.blocker+CCB+Digoxin+TCA+SSRI+Antipsychotic+Hypertrophy)
model3 &lt;- mlogit(data=mydata, formula=block~0|INR+TB+AST+ALT)
</code></pre>

<p>IT WORKS without any errors.</p>

<p>But here I would like to know,
how breaking into different models would change my Coefficients?
What should I do to avoid the error and try to incorporate all variables in one model?
How should I interpret my results if break into 3 different models as opposed to one?</p>

<p>Any help is highly appreciated.</p>
"
"0.0867230728520531","0.0734594449379399","215901","<p><strong>USE CASE</strong></p>

<p>Use R to fit/train a binary classification model, then interpret the model for the purpose of manual calculating classifications in Excel, not R.</p>

<p><strong>MODEL COEFFICIENTS</strong></p>

<pre><code>&gt;coef(model1)
#(Intercept) PetalLength  PetalWidth 
#-31.938998   -7.501714   63.670583 

&gt;exp(coef(model1))
#(Intercept)  PetalLength   PetalWidth 
#1.346075e-14 5.521371e-04 4.485211e+27 
</code></pre>

<p><strong>QUESTIONS</strong></p>

<p>(1) what is the classification formula from the fit model in example code below named '<em>model1</em>'?. (is it formula A, B or Neither)? </p>

<p>(2) how does '<em>model1</em>' determine if class == 1 vs. 2?</p>

<ul>
<li>Formula A: 

<blockquote>
  <p>class(Species{1:2}) = (-31.938998) + (-7.501714 * [PetalLength]) + (63.670583 * [PetalWidth])</p>
</blockquote></li>
<li>Formula B: 

<blockquote>
  <p>class(Species{1:2}) = 1.346075e-14 + (5.521371e-04 * [PetalLength]) + (4.485211e+27 * [PetalWidth])</p>
</blockquote></li>
</ul>

<p><strong>R CODE EXAMPLE</strong></p>

<pre><code># Load data (using iris dataset from Google Drive because uci.edu link wasn't working for me today)
#iris &lt;- read.csv(url(""http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data""), header = FALSE)
iris &lt;- read.csv(url(""https://docs.google.com/spreadsheets/d/1ovz31Y6PrV5OwpqFI_wvNHlMTf9IiPfVy1c3fiQJMcg/pub?gid=811038462&amp;single=true&amp;output=csv""), header = FALSE)
dataSet &lt;- iris

#assign column names
names(dataSet) &lt;- c(""SepalLength"", ""SepalWidth"", ""PetalLength"", ""PetalWidth"", ""Species"")

#col names
dsColNames &lt;- as.character(names(dataSet))

#num of columns and rows
dsColCount &lt;- as.integer(ncol(dataSet))
dsRowCount &lt;- as.integer(nrow(dataSet))

#class ordinality and name
classColumn &lt;- 5 
classColumnName &lt;- dsColNames[classColumn]
y_col_pos &lt;- classColumn

#features ordinality
x_col_start_pos &lt;- 1
x_col_end_pos &lt;- 4

# % of [dataset] reserved for training/test and validation  
set.seed(10)
sampleAmt &lt;- 0.25
mainSplit &lt;- sample(2, dsRowCount, replace=TRUE, prob=c(sampleAmt, 1-sampleAmt))

#split [dataSet] into two sets
dsTrainingTest &lt;- dataSet[mainSplit==1, 1:5] 
dsValidation &lt;- dataSet[mainSplit==2, 1:5]
nrow(dsTrainingTest);nrow(dsValidation);

# % of [dsTrainingTest] reserved for training
sampleAmt &lt;- 0.5
secondarySplit &lt;- sample(2, nrow(dsTrainingTest), replace=TRUE, prob=c(sampleAmt, 1-sampleAmt))

#split [dsTrainingTest] into two sets 
dsTraining &lt;- dsTrainingTest[secondarySplit==1, 1:5]
dsTest &lt;- dsTrainingTest[secondarySplit==2, 1:5]
nrow(dsTraining);nrow(dsTest);

nrow(dataSet) == nrow(dsTrainingTest)+nrow(dsValidation)
nrow(dsTrainingTest) == nrow(dsTraining)+nrow(dsTest)

library(randomGLM)

dataSetEnum &lt;- dsTraining[,1:5]
dataSetEnum[,5] &lt;- as.character(dataSetEnum[,5])
dataSetEnum[,5][dataSetEnum[,5]==""Iris-setosa""] &lt;- 1 
dataSetEnum[,5][dataSetEnum[,5]==""Iris-versicolor""] &lt;- 2 
dataSetEnum[,5][dataSetEnum[,5]==""Iris-virginica""] &lt;- 2 
dataSetEnum[,5] &lt;- as.integer(dataSetEnum[,5])

x &lt;- as.matrix(dataSetEnum[,1:4])
y &lt;- as.factor(dataSetEnum[,5:5])

# number of features
N &lt;- ncol(x)

# define function misclassification.rate
if (exists(""misclassification.rate"") ) rm(misclassification.rate);
misclassification.rate&lt;-function(tab){
  num1&lt;-sum(diag(tab))
  denom1&lt;-sum(tab)
  signif(1-num1/denom1,3)
}

#Fit randomGLM model - Ensemble predictor comprised of individual generalized linear model predictors
RGLM &lt;- randomGLM(x, y, classify=TRUE, keepModels=TRUE,randomSeed=1002)

RGLM$thresholdClassProb

tab1 &lt;- table(y, RGLM$predictedOOB)
tab1
# y  1  2
# 1  2  0
# 2  0 12

# accuracy
1-misclassification.rate(tab1)

# variable importance measure
varImp = RGLM$timesSelectedByForwardRegression
sum(varImp&gt;=0)

table(varImp)

# select most important features
impF = colnames(x)[varImp&gt;=5]
impF

# build single GLM model with most important features
model1 = glm(y~., data=as.data.frame(x[, impF]), family = binomial(link='logit'))

coef(model1)
#(Intercept) PetalLength  PetalWidth 
#-31.938998   -7.501714   63.670583 

exp(coef(model1))
#(Intercept)  PetalLength   PetalWidth 
#1.346075e-14 5.521371e-04 4.485211e+27 

confint.default(model1)
#                2.5 %   97.5 %
#(Intercept) -363922.5 363858.6
#PetalLength -360479.0 360464.0
#PetalWidth  -916432.0 916559.4
</code></pre>
"
"0.0304713817668003","0.029037395206952","216119","<p>I am studying how well Kobe Bryant shoots and to do so I have run a logistic regression. The variable shot_made_flag is 0 if missed and 1 if he scored. And I am running the regression against distance from the basket.</p>

<pre><code>  logitshots &lt;- glm(df$shot_made_flag ~ df$shot_distance, family = binomial(link=""logit""))
Call:  glm(formula = df$shot_made_flag ~ df$shot_distance, family = binomial(link = ""logit""))

Coefficients:
 (Intercept)  df$shot_distance  
      0.3681           -0.0441  

Degrees of Freedom: 25696 Total (i.e. Null);  25695 Residual
Null Deviance:      35330 
Residual Deviance: 34290    AIC: 34300
</code></pre>

<p>As you see the coefficient of distance is negative. So what I do next is to compute the probability of scoring if Bryant is 1 meter farther. </p>

<p>To do so I have done it this way, but I get a positive effect, so I am not sure about it. </p>

<pre><code>(exp(coef(logitshots))/(1+exp(coef(logitshots))))
(Intercept) df$shot_distance 
   0.5909933        0.4889768 
</code></pre>

<p>So how would you interpret this? every 1 meter means a 48% more chances of scoring (Lol)? Is this approach the right one? I guess that Kobe scoring from 25 meters is very unlikely (maybe modelling by a quadratic function?)  </p>

<p>I'd really appreciate any interesting insight and help! :)</p>
"
"0.129279124076572","0.123195234352977","216122","<p>As far as I know, the difference between logistic model and fractional response model (frm) is that the dependent variable (Y) in which frm is [0,1], but logistic is {0, 1}. Further, frm uses the quasi-likelihood estimator to determine its parameters. </p>

<p>Normally, we can use <code>glm</code> to obtain the logistic models by <code>glm(y ~ x1+x2, data = dat, family = binomial(logit))</code>. </p>

<p>For frm, we change <code>family = binomial(logit)</code> to <code>family = quasibinomial(logit)</code>.  </p>

<p>I noticed we can also use <code>family = binomial(logit)</code> to obtain frm's parameter since it gives the same estimated values. See the following example</p>

<pre><code>library(foreign)
mydata &lt;- read.dta(""k401.dta"")


glm.bin &lt;- glm(prate ~ mrate + age + sole + totemp, data = mydata
,family = binomial('logit'))
summary(glm.bin)
</code></pre>

<p>return,</p>

<pre><code>Call:
glm(formula = prate ~ mrate + age + sole + totemp, family = binomial(""logit""), 
    data = mydata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.1214  -0.1979   0.2059   0.4486   0.9146  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  1.074e+00  8.869e-02  12.110  &lt; 2e-16 ***
mrate        5.734e-01  9.011e-02   6.364 1.97e-10 ***
age          3.089e-02  5.832e-03   5.297 1.17e-07 ***
sole         3.636e-01  9.491e-02   3.831 0.000128 ***
totemp      -5.780e-06  2.207e-06  -2.619 0.008814 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1166.6  on 4733  degrees of freedom
Residual deviance: 1023.7  on 4729  degrees of freedom
AIC: 1997.6

Number of Fisher Scoring iterations: 6
</code></pre>

<p>And for <code>family = quasibinomial('logit')</code>,</p>

<pre><code>glm.quasi &lt;- glm(prate ~ mrate + age + sole + totemp, data = mydata
,family = quasibinomial('logit'))
summary(glm.quasi)
</code></pre>

<p>return,</p>

<pre><code>Call:
glm(formula = prate ~ mrate + age + sole + totemp, family = quasibinomial(""logit""), 
    data = mydata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.1214  -0.1979   0.2059   0.4486   0.9146  

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  1.074e+00  4.788e-02  22.435  &lt; 2e-16 ***
mrate        5.734e-01  4.864e-02  11.789  &lt; 2e-16 ***
age          3.089e-02  3.148e-03   9.814  &lt; 2e-16 ***
sole         3.636e-01  5.123e-02   7.097 1.46e-12 ***
totemp      -5.780e-06  1.191e-06  -4.852 1.26e-06 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for quasibinomial family taken to be 0.2913876)

    Null deviance: 1166.6  on 4733  degrees of freedom
Residual deviance: 1023.7  on 4729  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 6
</code></pre>

<p>The estimated Beta from both <code>family</code> are the same, but the difference is the SE values.  However, to obtain the correct SE, we have to use <code>library(sandwich)</code> as in this <a href=""http://stackoverflow.com/questions/37584715/fractional-response-regression-in-r"">post</a>.</p>

<p>Now, my questions:</p>

<ol>
<li>What is the difference between these two codes?</li>
<li>Is frm about to obtain robust SE?</li>
</ol>

<p>If my understanding is not correct, please give some suggestions.</p>
"
"0.0304713817668003","0.029037395206952","216247","<p>I ran a linear regression example in R and as a result got the following summary:</p>

<pre><code>Call:
lm(formula = Income ~ Age + Education + Gender, data = income_input)

Residuals:
    Min      1Q  Median      3Q     Max 
-37.340  -8.101   0.139   7.885  37.271 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  7.26299    1.95575   3.714 0.000212 ***
Age          0.99520    0.02057  48.373  &lt; 2e-16 ***
Education    1.75788    0.11581  15.179  &lt; 2e-16 ***
Gender      -0.93433    0.62388  -1.498 0.134443    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 12.07 on 1496 degrees of freedom
Multiple R-squared:  0.6364,    Adjusted R-squared:  0.6357 
F-statistic:   873 on 3 and 1496 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>As you can see, we get the t-values. In this case, how can i change my type of test from t to z?</p>

<p>Furthermore, both t and z tests assume normality. So what if i want to run the Wilcox Test?</p>
"
"0.0609427635336005","0.0580747904139041","217643","<p>I'm trying to find out if my numeric predictors have a linear relation to the logit of my logistic regression. I tried to use the lrm fit in the rms package where I have used 3 knot cubic spline on all numeric predictors like this:</p>

<pre><code>&gt; fit &lt;- lrm(y ~ rcs(x1,3)+rcs(x2,3)+.....)
</code></pre>

<p>There after I used anova on lrm fit. The main question is how do I use the result in anova(fit)? </p>

<p>My understanding is that the wald statistics are just the associated coefficients squared and dived by it's se. But what about the statistics for nonlinear terms here? are they the wald statistics for the coefficients for the squared predictors? </p>

<p>If none of the statistics are significant, can I conclude that there are no quadratic effect from my predictors?</p>
"
"0.0457070726502004","0.0435560928104281","217926","<p><em>Will the signs of coef (Estimate) of lm and glm always be the same?</em> <strong>^</strong></p>

<p>According to below toy example, it seems yes. Can you provide a case where they might be different? (If it matters in my real data the outcome is binary, hence used <code>mpg &gt; 20</code>)</p>

<pre><code># dummy data
d &lt;- mtcars

# fit lm, glm, glm_bi
fit_lm &lt;- lm(mpg &gt; 20 ~ cyl + disp, data = d)
fit_glm &lt;- glm(mpg &gt; 20 ~ cyl + disp, data = d)
fit_glm_bi &lt;- glm(mpg &gt; 20 ~ cyl + disp, family = binomial, data = d)

# Signs are always same?
# lm compared to glm
all.equal(sign(coef(fit_lm)),
          sign(coef(fit_glm)))
# output
# [1] TRUE

# lm compared to glm(family = binomial)
all.equal(sign(coef(fit_lm)),
          sign(coef(fit_glm_bi)))

# output
# [1] TRUE
</code></pre>

<p><strong>^</strong> Very much sounds like a dupe, found this similar post: <a href=""http://stats.stackexchange.com/questions/91666/sign-of-coefficients-in-linear-regression-vs-the-sign-of-correlation"">Sign of coefficients in linear regression vs. the sign of correlation</a>. Let me know if this is a dupe.</p>
"
"0.0578153819013687","0.0918243061724248","217929","<p>I'm performing some quantile regressions of a response variable (a ranking difference) over 14 predictors that describe each element of our ranking. Initially I succeeded in performing such regressions using the <code>QuantReg</code> from <code>statsmodel</code> in Python and then, for curiosity, I started performing the same regressions in R (using the <code>quantreg</code> library).</p>

<p>I've obtained the same models regarding the coefficients returned, but the p-values estimated are much different.</p>

<p>Just to compare, in R I've tested all options in <a href=""http://astrostatistics.psu.edu/datasets/2006tutorial/html/quantreg/html/summary.rq.html"" rel=""nofollow""><code>summary.rq</code></a> to compute confidence intervals/pvalues (rank, iid, nid, ker and boot). I've noticed that the nid and boot results were more similar to the ones obtained from Python. However, the R coefficients are quite higher yet (for example, I get a p-value about 0.0001 from Python while using the nid or boot methods in R, I get p-values about 0.03, 0.06, and so on). Considering what I've read until now, I've got that the boot method is one of the mostly recommended, but it does also has some options (xy, pwy, or mcmb). </p>

<p>In python I'm using:</p>

<pre><code>model = QuantReg(response, X)
fitted = model.fit(q=0.1)
print(fitted.summary())
</code></pre>

<p>In R I'm using:</p>

<pre><code>quant1 &lt;- rq(diff ~ street_wid +  mov_cars + park_cars + mov_ciclyst + landscape + 
                    build_ident + trees + log2(build_height+1) + diff_build + people + 
                    graffiti + catole + liberdade, tau=0.1, data=agrad.l)
summary.rq(quant1, se=""..."")
</code></pre>

<p>Here are my questions:</p>

<ol>
<li>Does anyone have an initial suggestion of using Python or R for quantile regressions? If so, why?</li>
<li>Can anyone suggest some explanations of such differences between Python and R in estimating p-values?</li>
<li>Finally, does anyone know of a tutorial that can help to choose the right configuration of quantile regressions in R (when to use each of such methods)?</li>
</ol>
"
"0.149398240610625","0.142367543061687","218085","<p>I have two questions concerning planned contrasts: </p>

<ol>
<li>I would like to know how factor-based contrasts (obtained through an interaction term) compare to model-paramter-based contrasts (obtained by specifying model parameters). </li>
<li>I would like to know this for a simple case of comparing one condition with another, but ultimately I am interested in comparing one condition vs all other conditions. </li>
</ol>

<p>Below are my attempts at understanding factor-based contrasts and model-parameter based contrasts:</p>

<pre><code>#create some dummy data
data &lt;- mtcars
#create interaction terms
data$interaction &lt;- interaction(mtcars$am, mtcars$vs, sep=""X"")
	data$interaction &lt;- gsub(""^0"", ""am0"", data$interaction)
	data$interaction &lt;- gsub(""^1"", ""am1"", data$interaction)
	data$interaction &lt;- gsub(""0$"", ""vs0"", data$interaction)
data$interaction &lt;- gsub(""1$"", ""vs1"", data$interaction)
	data$interaction &lt;- factor(data$interaction)
	levels(data$interaction)
#[1] ""am0Xvs0"" ""am0Xvs1"" ""am1Xvs0"" ""am1Xvs1""
</code></pre>

<p>From Eric Fuchs' <a href=""http://r-eco-evo.blogspot.nl/2007/10/one-of-most-neglected-topics-in_06.html"" rel=""nofollow"">blogpost</a> I think I figured out how to obtain factor-based contrasts. Let us assume for now that I am interested in the comparison of <code>am0Xvs0</code> vs. <code>am0Xvs1</code>. To this purpose, I create a contrast matrix where I assign equal weights with opposing signs to my two levels of interest, and 0 to the other two levels:</p>

<pre><code>#specify contrasts:
c.f &lt;- c(-1, 1, 0, 0) 
mat.f &lt;- cbind(c.f)
contrasts(data$interaction) &lt;- mat.f
#fit model
fit.f &lt;- aov(mpg~interaction, data)
#get coefficients for contrasts
summary(fit.f, split=list(interaction=list(""am0Xvs0 vs. am0Xvs1""=1)))
</code></pre>

<p>Output:</p>

<pre><code>                                       Df Sum Sq Mean Sq F value   Pr(&gt;F)    
interaction                         3  788.6  262.86   21.81 1.73e-07 ***
  interaction: am0Xvs0 vs. am0Xvs1  1  232.3  232.28   19.27 0.000147 ***
Residuals                          28  337.5   12.05                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<hr>

<p>For comparison, my attempt at model-parameter-based contrasts:</p>

<pre><code>fit.m &lt;- aov(mpg~am*vs, data)
</code></pre>

<p>with $E[mpg]=b_0 + b_1 \text{am} + b_2 \text{vs} + b3 (\text{am} \times \text{vs}).$ Based on Matt Blackwell's <a href=""http://stats.stackexchange.com/a/13168/79643"">answer</a> I think that a comparison of <code>am0Xvs0</code> vs. <code>am0Xvs1</code> means that $H_0: b_1 = 0$ (i.e., the regression weight associated with <code>am</code>).  Therefore:     </p>

<pre><code>## construct contrast matrices
mat.m &lt;- rbind(""am0:vs0 - vs1"" = c(0, 0, 1, 0))
library(car)
lht(fit.m, mat.m)
</code></pre>

<p>Output: </p>

<pre><code>Linear hypothesis test

Hypothesis:
am = 0

Model 1: restricted model
Model 2: mpg ~ am * vs

  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  
1     29 425.84                              
2     28 337.48  1     88.36 7.3311 0.01142 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>The values differ, so one of the two solutions is not correct (and my suspicion is that I did not correctly translate Matt's answer to this case). My question now is: which one of the two approaches is correct and how can the other be rewritten correctly? </p>

<hr>

<p>Ultimately, I am interested in the comparison of 1 of the 4 levels of the interaction term vs. the other 3. Let's say I want to compare <code>am0Xvs0</code> vs. the other 3 conditions. The factor-based version is simply an extension of what I have written above (if what I wrote above was correct): </p>

<pre><code>#create contrast matrix
c.f2 &lt;- c(1, -1/3, -1/3, -1/3) 
mat.f2 &lt;- cbind(c.f2)
contrasts(data$interaction) &lt;- mat.f2
#fit model
fit.f2 &lt;- aov(mpg~interaction, data)
#get coefficients for contrasts
summary(fit.f2, split=list(interaction=list(""am0Xvs0 vs. rest""=1)))
</code></pre>

<p>Output: </p>

<pre><code>                                Df Sum Sq Mean Sq F value   Pr(&gt;F)    
interaction                      3  788.6   262.9   21.81 1.73e-07 ***
  interaction: am0Xvs0 vs. rest  1  487.8   487.8   40.48 6.95e-07 ***
Residuals                       28  337.5    12.1                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Unfortunately I would not know how to start with the parameters for the model-parameter-based version, which is what I am ultimately interested in. </p>
"
"0.068136080998913","0.0649295895722714","218477","<p>I have a database which contains 100k records. It includes 2 continuous and 6 categorical variables. The output is categorical as well and it can take one of 8 different values (e.g. 1, 2, 3...8). My goal is:</p>

<ol>
<li><p>Investigate which of the variables are the most significant ones to determine my output.</p></li>
<li><p>After the analysis, to be able to calculate the probability for any of those outputs to happen if I only know what are the values for (e.g.) two variables? For example, to have some coefficients for every possible categorical value in order to calculate the probability...</p></li>
</ol>

<p>I tried this with the logistic regression but somehow I have big deviation from manually calculated probability (e.g. when I use the number of positive outputs and the total number of the records contained within my database). Anybody has a better idea how I could analyse this? Sth better than logistic regression?
Thanks in advance!</p>
"
"0.068136080998913","0.0649295895722714","218486","<p>How can we do weighted ridge regression in R?</p>

<p>In MASS package in R, I can do weighted linear regression by passing a weight parameter to <code>lm</code>. It can be seen that the model with weights is different from the one without weights.</p>

<pre><code># with weights
&gt; model = lm( y ~ X - 1, weights = w)
&gt; model$coeff
X(Intercept) XSepal.Length  XSepal.Width XPetal.Length  XPetal.Width
-2.1135159    -0.1890203     1.8198141    -1.1771699     2.2840825 

# without weights
&gt; model = lm( y ~ X - 1)
&gt; model$coeff
X(Intercept) XSepal.Length  XSepal.Width XPetal.Length  XPetal.Width 
-5.23869771    0.09802533    2.16742901   -1.07331102    2.40425352 
</code></pre>

<p>However, when I try to replicate the same with <code>lm.ridge</code>, model generated with and without weights are same.</p>

<pre><code>  # with weights
  &gt; model = lm.ridge(y ~ X - 1, lambda=lmd, weights = w)
  &gt; model
  X(Intercept) XSepal.Length  XSepal.Width XPetal.Length  XPetal.Width 
  -5.17253104    0.08770593    2.15946954   -1.06284572    2.38714738

  # without weights
  &gt; model = lm.ridge(y ~ X - 1, lambda=lmd, weights = w)
  &gt; model
  X(Intercept) XSepal.Length  XSepal.Width XPetal.Length  XPetal.Width 
  -5.17253104    0.08770593    2.15946954   -1.06284572    2.38714738
</code></pre>

<p>Edit 1: 
In linear model, I can calculate stderr of coefficients as follows:</p>

<pre><code>rss = sum( residuals( model, type=""pearson"")^2 )
dispersion = rss / model$df.residual
stderr = sqrt( diag(vcov(model)) ) / sqrt(dispersion)
</code></pre>
"
"0.11728440310428","0.139706232827494","219288","<p>UPDATE: This problem was (embarrassingly) solved by specifying the intercept in the regression equation as shown below: </p>

<pre><code>lcs ~ 1 + 0*Y1_bl_ctr #gamma is set to 0 for equivilance with the t-test
</code></pre>

<hr>

<p>This question is distinct enough from (<a href=""http://stats.stackexchange.com/questions/219040/change-score-model-in-lavaan"">Change Score Model in lavaan</a>) that I am migrating it here, but I am including the link for reference purposes. </p>

<p>At the aforementioned post, I was attempting to calculate a latent change score for two waves of observation. As indicated there, I believe the model I specified was over-identified. </p>

<p>Having thought about this for a day and reading a little more, I think I may have been over-complicating the problem.</p>

<p>In a recent paper by Colman and colleagues (<a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3794455/"" rel=""nofollow"">http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3794455/</a>), it was shown how under certain conditions a latent change score model is equivalent to a simple t-test. </p>

<p>The authors specify a path model in the paper as follows: </p>

<p><a href=""http://i.stack.imgur.com/X6Z3U.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/X6Z3U.jpg"" alt=""enter image description here""></a>
As noted in the path diagram, the authors state that the model is equivalent to a paired sample t-test when $\gamma=0$. </p>

<p>The authors additionally make the following constraints on the model: </p>

<ol>
<li>No $Y_2$ residual error; </li>
<li>Intercept of $Y_2$ is set to zero; </li>
<li>The auto-regressive path is set to 1; </li>
<li>The $LCS$ to $Y_2$ path is set to 1.</li>
</ol>

<p>NOTE: Within the body of the paper, the authors also specify that they are baseline-centering the $Y_1$ and $Y_2$ values. </p>

<p>The authors supply the data for their paper at (<a href=""http://trippcenter.uchc.edu/modeling"" rel=""nofollow"">http://trippcenter.uchc.edu/modeling</a>). For convenience, I provide the <code>dput</code> output of the data in the following code chunk and assign it to an object named <code>dat</code>.</p>

<pre><code>dat &lt;- structure(list(Y1 = c(7.6, 8.4, 8.4, 8.7, 9.7, 10.9, 9.7, 7.4, 
7.9, 9.3, 10.3, 11.3, 14, 8.6, 13.3, 10.9, 7.9, 9.5, 9.9, 11.5, 
11.8, 9.9, 8.8, 10.6, 11.3, 7.6, 8, 8.1, 8.6, 8.2, 7.4, 8.5, 
9.3, 9.4, 10.6, 10.3, 10.8, 8.2, 10.1, 7.8, 7.2, 7.8, 8.1, 8.1, 
8.2, 9.1, 8.7, 8.6, 7.6, 8.8, 10.3, 11.3, 7.7, 7.8, 8.5, 8.4, 
8.3, 9.2, 9.9, 9.8, 10.9, 7.8, 10.1, 9.5, 8, 8.3, 8.2, 7.8, 8.8, 
9.4, 8.7, 8.8, 10.8, 11.5, 7.6, 7.6, 7.9, 8.2, 9.7, 10, 8.8, 
10, 12.7, 7.5, 13.4, 8.3, 13.8, 14, 8.4, 14, 10, 9.5, 6.2, 11.7, 
9.7, 14, 7.3), Y2 = c(7.9, 7.8, 5.9, 7.5, 7.5, 9.1, 8, 7.9, 6.7, 
8.1, 8.5, 12, 14, 6.9, 10, 10.9, 7.2, 8.9, 10.8, 7.1, 7.4, 9.7, 
10.3, 9.3, 11.6, 8, 7.1, 6.7, 8.2, 9.3, 7.6, 9.9, 8.9, 8.8, 7.2, 
10.1, 6.7, 6.2, 8.9, 7.3, 7.6, 7.5, 7.3, 9.6, 8.1, 7.8, 8.7, 
8.4, 11.4, 9, 10.2, 12.5, 7, 8.7, 8, 7.2, 8.9, 10.4, 9.4, 10.8, 
9.9, 6.3, 5.7, 10.1, 7.8, 8.2, 7.4, 7.7, 11.8, 7.1, 6.8, 8.1, 
9.2, 10.2, 8.4, 7.1, 9, 6.9, 8.7, 8.8, 9.3, 8.6, 8.5, 7.7, 13.8, 
8.7, 10.5, 14, 10.1, 14, 14, 9, 14, 14, 10.3, 14, 8.4)), .Names = c(""Y1"", 
""Y2""), class = ""data.frame"", row.names = c(NA, -97L))
</code></pre>

<p>In <code>laavan</code> I am trying to implement the model as follows: </p>

<pre><code># baseline mean center Y1 and Y2
dat$Y1_bl_ctr = dat$Y1 - mean(dat$Y1)
dat$Y2_bl_ctr = dat$Y2 - mean(dat$Y1)

test &lt;- '
  # measurement model
    lcs =~ 1*Y2_bl_ctr #4 - the LCS to Y2 path is set to 1
  # regressions
    lcs ~ 0*Y1_bl_ctr #gamma is set to 0 for equivilance with the t-test
    Y2_bl_ctr ~ 1*Y1_bl_ctr #3 - The auto-regressive path is set to 1
  # residual error
    lcs ~~ 1*lcs 
    Y2_bl_ctr ~~ 0*Y2_bl_ctr #1 - No Y2 residual error
'


summary(test &lt;- lavaan(test
                       ,data=dat
                       ,int.lv.free = TRUE #intercepts of LCS is to be estimated
                       ,int.ov.free = FALSE #2- Intercept of Y2 is set to zero;
                       )
        )
</code></pre>

<p>As is probably obvious, this overly-restricted specification results in an unestimated model.</p>

<pre><code>#Error in lav_syntax_parse_rhs(rhs = rhs.formula[[2L]], op = op) : 
#  lavaan ERROR: I'm confused parsing this line: offsetY2_bl_ctr 
</code></pre>

<p>If I respecify the measurement model as <code>lcs =~ Y2_bl_ctr</code>, I get the following output from <code>laavan</code>.</p>

<pre><code>#lavaan (0.5-20) converged normally after   7 iterations
#
#  Number of observations                            97
#
#  Estimator                                         ML
#  Minimum Function Test Statistic               11.543
#  Degrees of freedom                                 1
#  P-value (Chi-square)                           0.001
#
#Parameter Estimates:
#
#  Information                                 Expected
#  Standard Errors                             Standard
#
#Latent Variables:
#                   Estimate  Std.Err  Z-value  P(&gt;|z|)
#  lcs =~                                              
#    Y2_bl_ctr         1.780    0.128   13.928    0.000
#
#Regressions:
#                   Estimate  Std.Err  Z-value  P(&gt;|z|)
#  lcs ~                                               
#    Y1_bl_ctr         0.000                           
#  Y2_bl_ctr ~                                         
#    Y1_bl_ctr         1.000                           
#
#Variances:
#                   Estimate  Std.Err  Z-value  P(&gt;|z|)
#    lcs               1.000                           
#    Y2_bl_ctr         0.000  
</code></pre>

<p>However, I still don't appear to get an intercept estimated - just the $Y_2$ to $LCS$ path coefficient. </p>

<pre><code>t.test(dat$Y1,dat$Y2,paired=TRUE)

#   Paired t-test
#
#data:  dat$Y1 and dat$Y2
#t = 2.1734, df = 96, p-value = 0.03221
#alternative hypothesis: true difference in means is not equal to 0
#95 percent confidence interval:
# 0.03423662 0.75545410
#sample estimates:
#mean of the differences 
#              0.3948454
</code></pre>

<p>The 0.395 value is the correct value based on the paper. </p>

<p>Any thoughts on how this model can be specified in <code>laavan</code> in order to produce equivalent results to the t-test?</p>
"
"0.105555962793852","0.0922061136661462","219679","<p>I would like to know how to find out the analytical solution of a simple linear regression with fixed intercept = 0:</p>

<p>$$ s = e^{-ht}$$
$$ y = -ln(s)  = h\cdot t$$</p>

<p>Here ist the background: I have three survival probabilities $s$ at 30, 90 and 180 days. Obviously, I have at day = 0 100% survival, so I  include this <em>observation</em>. I know that this is contested (<a href=""http://stats.stackexchange.com/questions/102709/when-forcing-intercept-of-0-in-linear-regression-is-acceptable-advisable"" title=""here"">here</a>) but I think in this special case it makes sense. The data I use for fitting the linear regression:</p>

<pre><code>&gt;     obs
    t    s          y
1   0 1.00 0.00000000
2  30 0.98 0.02020271
3  90 0.90 0.10536052
4 180 0.80 0.22314355
</code></pre>

<p>If I fit with simple regression I get this:</p>

<pre><code>&gt;     (fit1 &lt;- lm(y~t, data=obs))

Call:
lm(formula = y ~ t, data = obs)

Coefficients:
(Intercept)            t  
  -0.008464     0.001275
</code></pre>

<p>This can be obtained analytically if the following function is derived:</p>

<p>$$f(h) = \sum (y_i - ht_i)^2$$</p>

<p>which gives:</p>

<p>$$ \frac{\sum (y_i-\bar{y})\cdot (t_i-\bar{t})}{\sum (t_i-\bar{t})^2}$$</p>

<hr>

<p>UPDATE 1: This is the result of the minimization of 
$$f(h) = \sum (y_i - c - ht_i)^2$$. The correct result (see answers):
$$ \frac{\sum (y_i\cdot t_i)}{\sum t_i^2}$$</p>

<hr>

<p>The analytical results is:</p>

<pre><code>yc &lt;- with(obs,y-mean(y))
tc &lt;- with(obs, t -  mean(t))
sum(yc*tc)/sum(tc^2)
[1] 0.001275204
</code></pre>

<p>The same as coefficient in the fit1. Now, if I fix intercept to intercept=0 I get this:</p>

<pre><code>&gt;     (fit2 &lt;- lm(y~0+t, data=obs))

Call:
lm(formula = y ~ 0 + t, data = obs)

Coefficients:
   t  
0.001214  
</code></pre>

<p>I'm wondering how I can get an analytical solution for this. How I have to consider the fix intercept in the function $f(h)$ above?</p>

<p>Any idea is appreciated.</p>

<p><a href=""http://i.stack.imgur.com/ilwvG.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ilwvG.png"" alt=""enter image description here""></a></p>

<hr>

<p>Here is the way I constructed the data:</p>

<pre><code>set.seed(123)
# Hazard ratio
h &lt;- 0.7
# Number of observation
n &lt;- 50
# Model: exponential
t &lt;- rexp(n,h)
# scale to days
t &lt;- t*365.25
hist(t)
t &lt;- sort(t)
# Put data into a dataframe
df0 &lt;- data.frame(t=t)
head(df0)
# Compute probablities
df0$s &lt;- 1 - c(1:n)/n
head(df0)
# Extract survival probabilities at 30,90 and 180 days
df0$t2 &lt;- ceiling(df0$t/30)*30
# Select survival probablity 30, 90, 180 days
library(sqldf)
obs &lt;- sqldf(""SELECT t2 t, MAX(s) s FROM df0 WHERE t2 IN (30,90,180) GROUP BY t2"")
# Add survival probability=1 at day 0
obs &lt;- rbind(data.frame(t = 0, s = 1), obs)
# s = e^(-ht)  =&gt; y = -ln(s) = h*t
obs$y &lt;- -log(obs$s)
plot(y~t, data=obs)
fit1 &lt;- lm(y~t, data=obs)
abline(fit1,lty=2)
fit2 &lt;- lm(y~0+t, data=obs)
abline(fit2,lty=2, col=""red"")
legend(""topleft"", legend=c(""fit1"",""fit2""), col=c(1,2), lty=c(2,2))
</code></pre>
"
"0.0621994475718397","0.0711268017165705","219684","<p>I am trying to create a logistic regression model to predict whether a customer given a loan will be a bad or a good customer: bad meaning missing a certain amount of payments and good meaning frequent enough and in time with payments. For the purpose of the model I have coded Bad as 1 and Good as 0 and tried different combinations with the variables. </p>

<p>One of the models I have built has an AIC of 5383.7 and Gini coefficient of 0.416733. This is the result after I play around with the threshold:</p>

<pre><code>     FALSE TRUE
  0  3327  638
  1   165   95
</code></pre>

<p>So the model guessed that 165 customers would be good, but they are bad, but also put 638 good customers into the bad customers group.</p>

<p>The second model I built has an AIC of 5734.6 (350.9 higher), but its Gini is 0.4190394 and is slightly better at predicting the bad customers:</p>

<pre><code>     FALSE TRUE
  0  3537  673
  1   177  105
</code></pre>

<p>[UPDATE] Okay. After checking a few things - It turns out that one of the variables has missing values and the model excludes the observations that have them by default. Hence the difference in observations in my models. I know about multiple imputation, but I don't really feel alright with it. My question is should I impute the missing data or should I exclude it from the data set so I can compare models with different number of variables?</p>
"
"0.0806196982594614","0.076825726438694","219828","<p>I am doing logistic regression in R on a binary dependent variable with only one independent variable. I found the odd ratio as 0.99 for an outcomes. This can be shown in following. Odds ratio is defined as, $ratio_{odds}(H) = \frac{P(X=H)}{1-P(X=H)}$. As given earlier $ratio_{odds} (H) = 0.99$ which implies that $P(X=H) = 0.497$ which is close to 50% probability. This implies that the probability for having a H cases or non H cases 50% under the given condition of independent variable. This does not seem realistic from the data as only ~20% are found as H cases. Please give clarifications and proper explanations of this kind of cases in logistic regression.</p>

<p>I am hereby adding the results of my model output:</p>

<pre><code>M1 &lt;- glm(H~X, data=data, family=binomial())
summary(M1)

Call:
glm(formula = H ~ X, family = binomial(), data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.8563   0.6310   0.6790   0.7039   0.7608  

Coefficients:
                Estimate      Std. Error      z value     Pr(&gt;|z|)    
(Intercept)    1.6416666      0.2290133      7.168      7.59e-13 ***
   X          -0.0014039      0.0009466     -1.483      0.138    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1101.1  on 1070  degrees of freedom
Residual deviance: 1098.9  on 1069  degrees of freedom
  (667 observations deleted due to missingness)
AIC: 1102.9

Number of Fisher Scoring iterations: 4


exp(cbind(OR=coef(M1), confint(M1)))
Waiting for profiling to be done...
                                      OR           2.5 %       97.5 %
(Intercept)                    5.1637680       3.3204509     8.155564
     X                         0.9985971       0.9967357     1.000445
</code></pre>

<p>I have 1738 total dataset, of which H is a dependent binomial variable. There are 19.95% fall in (H=0) category and remaining are in (H=1) category. Further this binomial dependent variable compare with the covariate X whose minimum value is 82.23, mean value is 223.8 and maximum value is 391.6. The 667 missing values correspond to the covariate X i.e 667 data for X is missing in the dataset out of 1738 data.</p>
"
"0.129279124076572","0.109506874980424","220001","<p>I'm trying to model a logistic regression in R between two simple variables:</p>

<ul>
<li>Rating: An independent ordered categorical one, ranging from 1 to 99 (1, 2, 3, 4, 5, 99 in particular, 1 is the best)</li>
<li>Result: A dependent binary variable (0-1, not accepted/accepted)</li>
</ul>

<p>The formula I use is </p>

<pre><code>glm(formula = result_dummy ~ best_rating, family = binomial(link = ""logit""), 
    data = cd[1:10000, ])
</code></pre>

<p>result_dummy is a 0/1 numerical variable (original result column was a factor) and scaled_rating is the rating column after use the R <code>scale</code> function.</p>

<p>My thought here was to find a negative correlation (low rating -> more probability to accept) but the more samples I use the more odd results I find:</p>

<pre><code>10 samples:

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)     0.6484     0.7413   0.875    0.382
scaled_rating  -5.9403     5.8179  -1.021    0.307
</code></pre>

<hr>

<pre><code>100 samples:
Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)   -0.09593    0.27492  -0.349  0.72714   
scaled_rating -5.06251    1.76645  -2.866  0.00416 **
</code></pre>

<hr>

<pre><code>1000 samples:

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -0.03539    0.09335  -0.379    0.705    
scaled_rating -6.81964    0.62003 -10.999   &lt;2e-16 ***
</code></pre>

<hr>

<pre><code>10000 samples:
Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)     0.2489     0.0291   8.553   &lt;2e-16 ***
scaled_rating  -7.2319     0.2004 -36.094   &lt;2e-16 ***
</code></pre>

<hr>

<p>Notes:
I know that after the fit I should check residual plot, normality assumptions, etc. etc. but nonetheless I find really strange this behaviour.</p>

<p>I also have similar results using simply the rating column instead of the scaled one.</p>

<p>Edit:
The <code>rating</code> variable is not really an ordinal one, so as pointed out by @Scortchi maybe it would be better to treat it as a categorical one.
I have surely better results and model stability, obviously the model is a simple one and the residual error would be always high (because some variables as not been included in the model).
Indeed, including the frequency table as requested shows that the rating variable IS NOT sufficient for having a clear separation between the result outcome.</p>

<pre><code>          0      1
  1    2881  42564
  2   13878 129292
  3   36839 179500
  4   43511  97148
  5   37330  47002
  6   31801  21228
  7   19096   6034
  99  10008      3
</code></pre>
"
"0.105555962793852","0.100588487635796","220317","<p>I'm running a fixed-effects Poisson regression and get different results in Stata and R. Unfortunately I cannot upload and share the data due to legal restrictions.</p>

<p>My code in R is (formula shortened for illustration):</p>

<pre><code>library(pglm)    
pdf &lt;- pdata.frame(data, index=c(""id"",""timevar""))    
model &lt;- pglm(y ~ x1 + x3 + x3_lag + x3_lag2 + season + x4 + x1*x3_lag + x1*x3_lag2+x1*x4,
              data = pdf,
              effect = ""individual"",
              model = ""within"",
              family = ""poisson"")
</code></pre>

<p>where <code>season</code> controls for seasonal effects, <code>Y</code> is a count variable and <code>x3</code> is a treatment. </p>

<p><code>summary(model)</code> yields this:</p>

<pre><code>Maximum Likelihood estimation
Newton-Raphson maximisation, 4 iterations
Return code 2: successive function values within tolerance limit
Log-Likelihood: -47369.66 
19  free parameters
Estimates:
                   Estimate Std. error t value Pr(&gt; t)
x1            -1.031e-04        Inf       0       1
x3            -1.196e-02        Inf       0       1
x3_lag        -4.783e-02        Inf       0       1
x3_lag2       -5.159e-02        Inf       0       1
season02      -7.038e-02        Inf       0       1
season03       9.323e-02        Inf       0       1
season04       1.257e-01        Inf       0       1
season05       1.427e-01        Inf       0       1
season06      -1.217e-01        Inf       0       1
season07      -1.566e+01        Inf       0       1
season08      -2.095e-01        Inf       0       1
season09      -1.886e-01        Inf       0       1
season10       4.488e-02        Inf       0       1
season11      -9.954e-02        Inf       0       1
season12       8.201e-02        Inf       0       1
x4             6.055e-01        Inf       0       1
x1:x3_lag      1.888e-05        Inf       0       1
x1:x3_lag2     3.529e-05        Inf       0       1
x1:x4          4.948e-04        Inf       0       1
</code></pre>

<p>In Stata14 I used:</p>

<pre><code>xtset id timevar
xtpoisson x1 x3 x3_lag x3_lag2 season x4 x1#x3_lag x1#x3_lag2 x1#x4, fe
</code></pre>

<p>The results in Stata (see below) yield standard errors and where the standard errors are significant the coefficients are very much the same as in R, so I assume in both cases the same model was calculated. The issue is: why do I get standard errors in Stata but ""Inf"" in R?</p>

<pre><code>Conditional fixed-effects Poisson regression    Number of obs     =    110,233
Group variable: id                              Number of groups  =     15,945

                                                Obs per group:
                                                              min =          2
                                                              avg =        6.9
                                                              max =         13

                                                Wald chi2(19)     =     816.49
Log likelihood  = -47369.663                    Prob &gt; chi2       =     0.0000

--------------------------------------------------------------------------------------
           y         |      Coef.   Std. Err.      z    P&gt;|z|     [95% Conf. Interval]
---------------------+----------------------------------------------------------------
                  x1 |  -.0001031   .0002119    -0.49   0.626    -.0005184    .0003121
                  x3 |  -.0119605   .0058094    -2.06   0.040    -.0233467   -.0005744
              x3_lag |  -.0478287   .0122489    -3.90   0.000    -.0718361   -.0238212
                     |
           x3_lag#x1 |   .0000189   .0000225     0.84   0.401    -.0000252     .000063
                     |
             x3_lag2 |  -.0515859   .0138096    -3.74   0.000    -.0786523   -.0245195
                     |
          x3_lag2#x1 |   .0000353   .0000245     1.44   0.149    -.0000127    .0000833
                     |
              season |
                 02  |  -.0703797   .0211975    -3.32   0.001    -.1119261   -.0288333
                 03  |    .093219   .0233858     3.99   0.000     .0473836    .1390544
                 04  |   .1256642   .0270927     4.64   0.000     .0725635    .1787649
                 05  |   .1426421   .0335767     4.25   0.000     .0768331    .2084512
                 06  |  -.1217462    .046838    -2.60   0.009     -.213547   -.0299454
                 07  |  -18.24382   741.5026    -0.02   0.980    -1471.562    1435.075
                 08  |  -.2095094   .0373193    -5.61   0.000    -.2826539   -.1363649
                 09  |  -.1886333   .0341701    -5.52   0.000    -.2556054   -.1216611
                 10  |    .044879   .0278778     1.61   0.107    -.0097604    .0995184
                 11  |  -.0995352   .0249665    -3.99   0.000    -.1484686   -.0506018
                 12  |   .0819983   .0214643     3.82   0.000      .039929    .1240675
                     |
                  x4 |   .6055111   .0754535     8.02   0.000      .457625    .7533972
                     |
               x4#x1 |   .0004948   .0001416     3.49   0.000     .0002173    .0007723
--------------------------------------------------------------------------------------
</code></pre>

<p>If you have any suggestions on how I could provide a replicable example (maybe with some sample panel data, any suggestions?), I will gladly do so.</p>

<p><strong>EDIT</strong></p>

<p>A smaller model produces identical results in R and Stata. It seems that with poor models R is more restrictive and does not show standard errors.</p>
"
"0.052777981396926","0.0502942438178979","220364","<p>So, im in a bit of trouble here. I am using R (i'm very new at this), and i'm trying to plot the probability effects of a interaction effect, using the effects package. </p>

<p>This is what the plot shows<a href=""http://i.stack.imgur.com/bBR1O.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bBR1O.jpg"" alt=""enter image description here""></a></p>

<p>However, when looking at the logistic regression model: it shows a b coefficient of B -1.333**, ExpB.27 indicating a negative moderation effect.</p>

<p>My quistion: how do i interpret this plot? and how does this relate to the findings? </p>

<p>Thank you guys in advance</p>

<p>Update:
the code i used is: </p>

<pre><code>data.mod &lt;-glm(outc_bin1~ctr_projsize+ctrfirmage+ctr_avgfirmsize+ctr_unirep+ctr_EPO+ctr_avginv+ctr_funding+ctr_projage+ctr_patent+techdiv+involvement+geolog+tech2+techdiv:involvement+tech2:involvement+geolog:involvement, family=binomial(link = ""logit""), data=data, x=TRUE)

plot(effect(""techdiv:involvement"", data.mod, xlevels=list(involvement=c(1, 2, 3, 4)))
</code></pre>

<p>Regression output:</p>

<p><a href=""http://i.stack.imgur.com/pjQOH.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pjQOH.jpg"" alt=""enter image description here""></a></p>
"
"0.068136080998913","0.0649295895722714","220429","<p>I'm running a meta-regression/multi-level analysis that contains only categorical variables. </p>

<p>The printout of the data is as follows:</p>

<blockquote>
  <p>res.fe</p>
</blockquote>

<pre><code>Multivariate Meta-Analysis Model (k = 19; method: REML)

Variance Components: none

Test for Residual Heterogeneity: 
QE(df = 7) = 5.9504, p-val = 0.5456

Test of Moderators (coefficient(s) 2,3,4,5,6,7,8,9,10,11,12): 
QM(df = 11) = 39.3316, p-val &lt; .0001

Model Results:

                             estimate      se     zval    pval    ci.lb    ci.ub     
intrcpt                        0.8995  0.0807  11.1477  &lt;.0001   0.7414   1.0577  ***
as.factor(Z)2          -0.1090  0.0825  -1.3213  0.1864  -0.2708   0.0527     
as.factor(Z)3          -0.1299  0.1785  -0.7276  0.4668  -0.4797   0.2199     
as.factor(Z)4          -0.2180  0.2015  -1.0820  0.2793  -0.6128   0.1769     
as.factor(Z)5           0.4280  0.1510   2.8352  0.0046   0.1321   0.7240   **
as.factor(W)1            0.1059  0.1091   0.9707  0.3317  -0.1079   0.3196     
as.factor(W)2            0.1215  0.1584   0.7673  0.4429  -0.1889   0.4319     
as.factor(W)3            0.3696  0.1141   3.2381  0.0012   0.1459   0.5933   **
as.factor(U)2   -0.0575  0.1289  -0.4463  0.6554  -0.3101   0.1950     
as.factor(V)3             -0.1709  0.0981  -1.7417  0.0816  -0.3632   0.0214    .
as.factor(V)4             -0.2247  0.2495  -0.9007  0.3677  -0.7138   0.2643     
as.factor(V)5             -0.4078  0.1181  -3.4542  0.0006  -0.6391  -0.1764  ***

---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>Variable Z has levels 1, 2, 3, 4, and 5, but the printout only shows levels 2-5; variable V has levels 0, 3, 4, and 5, but only prints levels 3-5;variable U has levels 1 and 2, but the printout only shows level 2.</p>

<p>My understanding is that the intercept is the value when all other variables are explanatory variables are equal to zero. </p>

<p>Does this mean that the intercept is equal to the omitted variables with their respective levels? (i.e. intercept = Z(level=1) + U(level=1)+ V(level=0))</p>

<p>If so, how would I obtain estimates for each of the omitted variables at specified levels?</p>

<p>Thanks for any information that you can provide me with. </p>
"
"0.121885527067201","0.10889023202607","220868","<p>The goal of this regression is to determine whether the amount of leaf disk that an insect consumed varied by what tree the leaf material came from. I'll acknowledge upfront that my coding is rarely pretty/efficient, but hopefully it works (usually).</p>

<ul>
<li>Variables:

<ul>
<li>Response: pctrans; the percent of a 7 mm diameter leaf disk that was consumed.  Values have been transformed to fit (0,1).</li>
<li>Explanatory: tree; a categorical (factor) variable of six tree types.</li>
</ul></li>
</ul>

<p>When I use betareg(), which as I understand it, is best suited to data of this sort, I get no significance:</p>

<pre><code>model.beta &lt;- betareg(pctrans ~ tree, data=BT.data, link=""logit"")
modelnull.beta &lt;- betareg(pctrans ~ tree, data=BT.data, link=""logit"")
lrtest(model1.beta, modelnull.beta)
</code></pre>

<p>Results:</p>

<pre><code>Call:
betareg(formula = pctrans ~ tree, data = BT.data, link = ""logit"")

Standardized weighted residuals 2:
    Min      1Q  Median      3Q     Max 
-2.7716 -0.5800  0.0472  0.5351  3.5109 

Coefficients (mean model with logit link):
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -1.111504   0.069191 -16.064  &lt; 2e-16 ***
treeBC3F3   -0.050940   0.095889  -0.531  0.59525    
treeD54     -0.279927   0.096470  -2.902  0.00371 ** 
treeD58     -0.034000   0.095716  -0.355  0.72242    
treeEllis1  -0.006764   0.095175  -0.071  0.94334    
treeQing     0.785992   0.094003   8.361  &lt; 2e-16 ***

Phi coefficients (precision model with identity link):
      Estimate Std. Error z value Pr(&gt;|z|)    
(phi)   3.5549     0.1352   26.29   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Type of estimator: ML (maximum likelihood)
Log-likelihood: 529.8 on 7 Df
Pseudo R-squared: 0.1105
Number of iterations: 20 (BFGS) + 2 (Fisher scoring) 

Likelihood ratio test

Model 1: pctrans ~ tree
Model 2: pctrans ~ 1
  #Df LogLik Df  Chisq Pr(&gt;Chisq)    
1   7 529.82                         
2   2 460.70 -5 138.25  &lt; 2.2e-16 ***
</code></pre>

<p>As I've been told, since the model is significantly worse than the null, no comparisons can be made between treatment means.</p>

<p>HOWEVER...
If I run the same model using glm the model is significantly better than the null.</p>

<pre><code>beta.glm &lt;- glm(pctrans ~ tree, data=BT.data, family=quasibinomial)
</code></pre>

<p>Results:</p>

<pre><code>Call:
glm(formula = pctrans ~ tree, family = quasibinomial, data = BT.data)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-0.94474  -0.38492  -0.08785   0.22725   1.80291  

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -1.22601    0.07643 -16.042  &lt; 2e-16 ***
treeBC3F3    0.06826    0.10660   0.640  0.52205    
treeD54     -0.33864    0.11312  -2.994  0.00281 ** 
treeD58     -0.19878    0.11062  -1.797  0.07260 .  
treeEllis1  -0.07763    0.10808  -0.718  0.47276    
treeQing     0.88596    0.09978   8.879  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for quasibinomial family taken to be 0.2069603)

    Null deviance: 307.54  on 1240  degrees of freedom
Residual deviance: 267.59  on 1235  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 4

Analysis of Deviance Table
Model: quasibinomial, link: logit
Response: pctrans
Terms added sequentially (first to last)

     Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                  1240     307.54              
tree  5   39.951      1235     267.59 &lt; 2.2e-16 ***
</code></pre>

<p>Where do I go from here?</p>
"
"0.0806196982594614","0.076825726438694","221427","<p>I want to understand the interpretation of logistic regression coefficients in terms of an increase in probability of dependent variable being 1. </p>

<p>I tested a logistic regression model in R and got the following coefficients (all statistically significant):</p>

<pre><code>&gt; mud$coefficients
  (Intercept)          var1          var2          var3          var4
-3.557573e+00  1.051031e-01  4.937244e-07 -1.308386e-06  3.937646e-01
</code></pre>

<p>Raising these numbers to the power of e resulted in numbres below. I would interpret them so that a 1 unit increase in var1 would increase the probability of dependent variable being 1 by 11% and 1 000 000 unit decrease in var3 would increase that probability by 1%. </p>

<pre><code>&gt; exp(mud$coefficients)
(Intercept)       var1        var2        var3        var4
0.02850792  1.11082516  1.00000049  0.99999869  1.48255150
</code></pre>

<p>As suggested in a previous a question (<a href=""http://stats.stackexchange.com/a/24422/121763"">http://stats.stackexchange.com/a/24422/121763</a>), below is what I should actually do to find the probabilities.</p>

<pre><code>&gt; exp(mud$coefficients)/(1+exp(mud$coefficients))
(Intercept)       var1        var2        var3        var4 
0.02771774  0.52625162  0.50000012  0.49999967  0.59718862
</code></pre>

<p>So which numbers should I use if I'd like to express the effect of independent variables on the probability of and event occurring? E.g. in case of var1 is it 11% or 53% or something else? </p>
"
"0.111849830561289","0.113247800628571","221510","<p>I'm new to logistic regression analysis, and was unable to find an answer elsewhere in Cross Validated or Stack Overflow. </p>

<p>Consider a standard logistic regression analysis of a binary outcome (admission to college) based on continuous covariates gre score and high school gpa, and ordinal categorical rank prestige of the undergraduate institution (data from the nice UCLA stats dept. logistic regression in R tutorial: <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/dae/logit.htm</a>)</p>

<pre><code>&gt; admissions.data &lt;- read.csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
&gt; admissions.data$rank &lt;- as.factor(admissions.data$rank)
&gt; summary(admissions.data)
     admit             gre             gpa        rank
 Min.   :0.0000   Min.   :220.0   Min.   :2.260   1: 61
 1st Qu.:0.0000   1st Qu.:520.0   1st Qu.:3.130   2:151
 Median :0.0000   Median :580.0   Median :3.395   3:121
 Mean   :0.3175   Mean   :587.7   Mean   :3.390   4: 67
 3rd Qu.:1.0000   3rd Qu.:660.0   3rd Qu.:3.670
 Max.   :1.0000   Max.   :800.0   Max.   :4.000

&gt; fit1 &lt;- glm(admit ~ gre + gpa + rank, data = admissions.data, family=""binomial"")
&gt; summary(fit1)

Call:
glm(formula = admit ~ gre + gpa + rank, family = ""binomial"",
    data = admissions.data)

Deviance Residuals:
    Min       1Q   Median       3Q      Max
-1.6268  -0.8662  -0.6388   1.1490   2.0790

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -3.989979   1.139951  -3.500 0.000465 ***
gre          0.002264   0.001094   2.070 0.038465 *
gpa          0.804038   0.331819   2.423 0.015388 *
rank2       -0.675443   0.316490  -2.134 0.032829 *
rank3       -1.340204   0.345306  -3.881 0.000104 ***
rank4       -1.551464   0.417832  -3.713 0.000205 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 499.98  on 399  degrees of freedom
Residual deviance: 458.52  on 394  degrees of freedom
AIC: 470.52

Number of Fisher Scoring iterations: 4

# Odds Ratios
&gt; exp(coef(fit1))
(Intercept)         gre         gpa       rank2       rank3       rank4
  0.0185001   1.0022670   2.2345448   0.5089310   0.2617923   0.2119375

# 95% confidence intervals
&gt; exp(confint(fit1))
Waiting for profiling to be done...
                  2.5 %    97.5 %
(Intercept) 0.001889165 0.1665354
gre         1.000137602 1.0044457
gpa         1.173858216 4.3238349
rank2       0.272289674 0.9448343
rank3       0.131641717 0.5115181
rank4       0.090715546 0.4706961
</code></pre>

<p>My questions are:</p>

<p>1) In R, is there a straight-forward way to determine ORs with 95% CIs for specific values of the covariates? E.g., based on this model, what are the odds of college acceptance for students applying to a rank 2 schools with a gpa of 3 and a gre score of 750, compared with a student applying to a rank 3 school with the same gpa and gre score? I could calculate ORs by hand given the model coefficient estimates and these specific covariate values, but am unsure how to correctly propagate SEs to calculate 95% CIs.</p>

<p>2) Would this particular example be considered a case-control study design, and therefore odds ratios could be estimated, but not predictions? (See: <a href=""http://stats.stackexchange.com/questions/69561/case-control-study-and-logistic-regression"">Case-control study and Logistic regression</a>)</p>
"
"0.0806196982594614","0.076825726438694","221525","<p>I am using the <code>svyglm</code> function in the <code>survey</code> package in <code>R</code> to fit logistic regression models to a stratified, cluster survey. I want to calculate confidence intervals for my regression coefficients. The default method for <code>confint.svyglm</code> says that it creates Wald confidence intervals by adding and subtracting a multiple of the standard error. But the confidence interval this produces is not consistent with the p-value from the model - confidence intervals that do not overlap 0 still have p-values greater than .05.</p>

<p>I tried to replicate the p-value and confidence interval calculations by hand. It appears the p-value is calculated using a t-test, with the df of the t distribution taken from the residual degrees of freedom from the model. So far so good. But the confidence interval provided by <code>confint.svyglm</code> is just coefficient +/- 1.96*standard.error. This seems wrong - for a 95% confidence interval, I think the multiplier for the standard error should be the .975 quantile of a t-distribution with the appropriate degrees of freedom (in my case 10), which can be somewhat different from 1.96 (the .975 quantile of a z-distribution). True? Has anyone else had this problem? I am relatively new to working with survey data. Is there a reason to always use the z-quantile instead of the t-quantile for complex surveys specifically, or is this just a bug in the package?</p>
"
"0.035185320931284","0.0502942438178979","221681","<p>I have a <code>quantile regression model</code> that I fit with the <code>rq()</code> function in the <code>quantreg</code> package in R. </p>

<p>However, since my sample size if fairly small (n = 36) compared to  the number of X variables (= 8), I need to estimate power for the various regression coefficients. How can I do that?</p>

<p>It would be great to be pointed to a function in R that can do that for a <code>quantreg model</code>, but a general explanation of how to calculate power in such a model would be fine as well (I can then code that in R myself). </p>

<p>How can I determine the power for a given <code>quantile regression model</code> with a given dataset?</p>

<p>thanks, Steve</p>
"
"0.13342816860689","0.13925845528839","221880","<p>To explore how the <code>LASSO</code> regression works, I wrote a small piece of code that should optimize <code>LASSO</code> regression by picking the best alpha parameter.</p>

<p>I cannot figure out why the <code>LASSO</code> regression is giving me such unstable results for the alpha parameter after cross validation.</p>

<p>Here is my Python code:</p>

<pre><code>from sklearn.linear_model import Lasso
from sklearn.cross_validation import KFold
from matplotlib import pyplot as plt

# generate some sparse data to play with
import numpy as np
import pandas as pd 
from scipy.stats import norm
from scipy.stats import uniform

### generate your own data here

n = 1000

x1x2corr = 1.1
x1x3corr = 1.0
x1 = range(n) + norm.rvs(0, 1, n) + 50
x2 =  map(lambda aval: aval*x1x2corr, x1) + norm.rvs(0, 2, n) + 500
y = x1 + x2 #+ norm.rvs(0,10, n)

Xdf = pd.DataFrame()
Xdf['x1'] = x1
Xdf['x2'] = x2

X = Xdf.as_matrix()

# Split data in train set and test set
n_samples = X.shape[0]
X_train, y_train = X[:n_samples / 2], y[:n_samples / 2]
X_test, y_test = X[n_samples / 2:], y[n_samples / 2:]

kf = KFold(X_train.shape[0], n_folds = 10, )
alphas = np.logspace(-16, 8, num = 1000, base = 2)

e_alphas = list()
e_alphas_r = list()  # holds average r2 error
for alpha in alphas:
    lasso = Lasso(alpha=alpha, tol=0.004)
    err = list()
    err_2 = list()
    for tr_idx, tt_idx in kf:
        X_tr, X_tt = X_train[tr_idx], X_test[tt_idx]
        y_tr, y_tt = y_train[tr_idx], y_test[tt_idx]
        lasso.fit(X_tr, y_tr)
        y_hat = lasso.predict(X_tt)

        # returns the coefficient of determination (R^2 value)
        err_2.append(lasso.score(X_tt, y_tt))

        # returns MSE
        err.append(np.average((y_hat - y_tt)**2))
    e_alphas.append(np.average(err))
    e_alphas_r.append(np.average(err_2))

## print out the alpha that gives the minimum error
print 'the minimum value of error is ', e_alphas[e_alphas.index(min(e_alphas))]
print ' the minimizer is ',  alphas[e_alphas.index(min(e_alphas))]

##  &lt;&lt;&lt; plotting alphas against error &gt;&gt;&gt;

plt.figsize = (15, 15)
fig = plt.figure()
ax = fig.add_subplot(111)
ax.plot(alphas, e_alphas, 'b-')
ax.plot(alphas, e_alphas_r, 'g--')
ax.set_ylim(min(e_alphas),max(e_alphas))
ax.set_xlim(min(alphas),max(alphas))
ax.set_xlabel(""alpha"")
plt.show()
</code></pre>

<p>If you run this code repeatedly, it gives wildly different results for alpha:</p>

<pre><code>&gt;&gt;&gt; 
the minimum value of error is  3.99254192539
 the minimizer is  1.52587890625e-05
&gt;&gt;&gt; ================================ RESTART ================================
&gt;&gt;&gt; 
the minimum value of error is  4.07412455842
 the minimizer is  6.45622425334
&gt;&gt;&gt; ================================ RESTART ================================
&gt;&gt;&gt; 
the minimum value of error is  4.25898253597
 the minimizer is  1.52587890625e-05
&gt;&gt;&gt; ================================ RESTART ================================
&gt;&gt;&gt; 
the minimum value of error is  3.79392968781
 the minimizer is  28.8971008254
&gt;&gt;&gt; 
</code></pre>

<p>Why is the alpha value not converging properly?  I know that my data is synthetic, but the distribution is the same.  Also, the variation is very small in <code>x1</code> and <code>x2</code>.</p>

<p>what could be causing this to be so unstable?  </p>

<h2>The same thing written in R gives different results - it always returns the highest possible value for alpha as the ""optimal_alpha"".</h2>

<p>I also wrote this in R, which gives me a slightly different answer, which I don't know why?</p>

<pre><code>library(glmnet)
library(lars)
library(pracma)

set.seed(1)
k = 2 # number of features selected 

n = 1000

x1x2corr = 1.1
x1 = seq(n) + rnorm(n, 0, 1) + 50
x2 =  x1*x1x2corr + rnorm(n, 0, 2) + 500
y = x1 + x2 

filter_out_label &lt;- function(col) {col!=""y""}

alphas = logspace(-5, 6, 100)

for (alpha in alphas){
  k = 10
  optimal_alpha = NULL
  folds &lt;- cut(seq(1, nrow(df)), breaks=k, labels=FALSE)
  total_mse = 0
  min_mse = 10000000
  for(i in 1:k){
    # Segement your data by fold using the which() function
    testIndexes &lt;- which(folds==i, arr.ind=TRUE)
    testData &lt;- df[testIndexes, ]
    trainData &lt;- df[-testIndexes, ]

    fit &lt;- lars(as.matrix(trainData[Filter(filter_out_label, names(df))]),
                trainData$y,
                type=""lasso"")
    # predict
    y_preds &lt;- predict(fit, as.matrix(testData[Filter(filter_out_label, names(df))]),
                       s=alpha, type=""fit"", mode=""lambda"")$fit # default mode=""step""

    y_true = testData$y
    residuals = (y_true - y_preds)
    mse=sum(residuals^2)
    total_mse = total_mse + mse
  }
  if (total_mse &lt; min_mse){
    min_mse = total_mse
    optimal_alpha = alpha
  }
}

print(paste(""the optimal alpha is "", optimal_alpha))
</code></pre>

<p>The output from the R code above is: </p>

<pre><code>&gt; source('~.....')
[1] ""the optimal alpha is  1e+06""
</code></pre>

<p>In fact, no matter what I set for the line ""<code>alphas = logspace(-5, 6, 100)</code>"", I always get back the highest value for alpha.</p>

<p>I guess there are actually 2 different questions here :</p>

<ol>
<li><p>Why is the alpha value so unstable for the version written in Python? </p></li>
<li><p>Why does the version written in R give me a different result?  (I realize that the <code>logspace</code> function is different from <code>R</code> to <code>python</code>, but the version written in <code>R</code> always gives me the largest value of <code>alpha</code> for the optimal alpha value, whereas the python version does not).</p></li>
</ol>

<p>It would be great to know these things...</p>
"
"0.052777981396926","0.0502942438178979","222013","<p>I have performed a regression examining data on crime rates. It looks at a standard ratio type variable (beginning crime rate at a point in  time) and a dummy variable (if the countries crime rate is in the top mid or bottom third of beginning crime rates). We consider the effect upon the change in crime over time, crmdelta. The model code and HC standard error regression output is as follows:</p>

<pre><code>df$rank.f &lt;- factor(df$rank)
eqn1 &lt;- lm(crmdelta ~ crmbegin+rank.f+(crmbegin*rank.f), data=df)

                Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)       3.57085    4.04330  0.8832  0.37839  
crmbegin         -0.15049    0.29486 -0.5104  0.61045  
rank.f2          -3.38911    4.22533 -0.8021  0.42361  
rank.f3           1.59112    4.69654  0.3388  0.73518  
crmbegin:rank.f2  0.16871    0.43199  0.3906  0.69661  
crmbegin:rank.f3 -3.12450    1.64205 -1.9028  0.05874 .
</code></pre>

<p>2 questions:</p>

<p>(1) I cant tell what happened to the interaction crmbegin:rank.f1. Is this represented by crmbegin? And if so, is there no lone crmbegin variable even though it was specified in the regression?</p>

<p>(2) How do I precisely interpret the coefficients on the interactions? Is it the effect of the ratio variable given the singular case of whichever dummy is in effect? Ie. crmbegin:rank.f2 is the marginal effect of crmbegin, given we are operating in rank.f2 data subset?</p>

<p>Thanks very much for any help. My first post here and looks like a great community. </p>
"
"0.105555962793852","0.100588487635796","222233","<p>I try to reproduce with <code>optim</code> the results from a simple linear regression fitted with <code>glm</code> or even <code>nls</code> R functions.<br>
The parameters estimates are the same but the residual variance estimate and the standard errors of the other parameters are not the same particularly when the sample size is low. I suppose that this is due differences in the way the residual standard error is calculated between Maximum Likelihood and Least square approaches (dividing by n or by n-k+1 see bellow in the example).<br>
I understand from my readings on the web that optimization is not a simple task but I was wondering if it would be possible to reproduce in a simple way the residual standard error estimate from <code>glm</code> while using <code>optim</code>. </p>

<p>Simulate a small dataset</p>

<pre><code>set.seed(1)
n = 4 # very small sample size !
b0 &lt;- 5
b1 &lt;- 2
sigma &lt;- 5
x &lt;- runif(n, 1, 100)
y =  b0 + b1*x + rnorm(n, 0, sigma) 
</code></pre>

<p>Estimate with optim</p>

<pre><code>negLL &lt;- function(beta, y, x) {
    b0 &lt;- beta[1]
    b1 &lt;- beta[2]
    sigma &lt;- beta[3]
    yhat &lt;- b0 + b1*x
    likelihood &lt;- dnorm(y, yhat, sigma)
    return(-sum(log(likelihood)))
}

res &lt;- optim(starting.values, negLL, y = y, x = x, hessian=TRUE)
estimates &lt;- res$par     # Parameters estimates
se &lt;- sqrt(diag(solve(res$hessian))) # Standard errors of the estimates
cbind(estimates,se)


    &gt; cbind(estimates,se)
      estimates         se
b0     9.016513 5.70999880
b1     1.931119 0.09731153
sigma  4.717216 1.66753138
</code></pre>

<p>Comparison with glm and nls</p>

<pre><code>&gt; m &lt;- glm(y ~ x)
&gt; summary(m)$coefficients
            Estimate Std. Error   t value    Pr(&gt;|t|)
(Intercept) 9.016113  8.0759837  1.116411 0.380380963
x           1.931130  0.1376334 14.030973 0.005041162
&gt; sqrt(summary(m)$dispersion) # residuals standard error
[1] 6.671833
&gt; 
&gt; summary(nls( y ~ b0 + b1*x, start=list(b0 = 5, b1= 2)))

Formula: y ~ b0 + b1 * x

Parameters:
   Estimate Std. Error t value Pr(&gt;|t|)   
b0   9.0161     8.0760   1.116  0.38038   
b1   1.9311     0.1376  14.031  0.00504 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 6.672 on 2 degrees of freedom
</code></pre>

<p>I can reproduce the different residual standard error estimates like this : </p>

<pre><code>&gt; # optim / Maximum Likelihood estimate
&gt; sqrt(sum(resid(m)^2)/n)
[1] 4.717698
&gt; 
&gt; # Least squares estimate (glm and nls estimates)
&gt; k &lt;- 3 # number of parameters
&gt; sqrt(sum(resid(m)^2)/(n-k+1))
[1] 6.671833
</code></pre>
"
"0.0621994475718397","0.0711268017165705","222431","<p>See this file here: <a href=""http://www.math.uvic.ca/~nathoo/stat359-material/decay.TXT"" rel=""nofollow"" title=""Decay.txt"">Decay.TXT</a>. </p>

<p>I first tried to fit the logarithmic model first </p>

<pre><code>dec = read.table('decay.txt', header=T)
attach(dec)
logy &lt;- log(y)
model1 &lt;- lm(logy ~ x)
</code></pre>

<p>And now I try to fit a quadratic model into the data: </p>

<pre><code>model2 &lt;- lm(y ~ x + I(x^2))
</code></pre>

<p>I now attempt to get the r^2 (r-squared values) of the models. </p>

<pre><code>(rsq1 &lt;- summary(model1)$r.squared)
[1] 0.8307964

(rsq2 &lt;- summary(model2)$r.squared)
[1] 0.9079788
</code></pre>

<p>Obviously as seen, here the coefficient of determination of the quadratic regression model is better than the exponential regression model (for this dataset). </p>

<p>However, the critical appraisal of these models says otherwise. For the exponential model, we see that: 
<a href=""http://i.stack.imgur.com/ha55L.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ha55L.jpg"" alt=""Exponential regression""></a></p>

<p>And for the quadratic polynomial model, we see that: 
<a href=""http://i.stack.imgur.com/B30jv.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/B30jv.jpg"" alt=""Quadratic regression""></a></p>

<p>The QQ Plot reveals something strange with the residuals for the Quadratic regression i.e ""Not Normally Distributed"". </p>

<p>Now I am trying to use ANOVA, to compare the two models. And I type: </p>

<pre><code>anova(model1, model2)  
</code></pre>

<p>and there is an error regarding the missing variable <code>y</code>. What is the right way to conduct the <code>anova</code> ?    </p>
"
"0.0621994475718397","0.0711268017165705","222514","<p>See this file here: <a href=""http://www.math.uvic.ca/~nathoo/stat359-material/decay.TXT"" rel=""nofollow"" title=""Decay.txt"">Decay.TXT</a>. </p>

<p>I first tried to fit the logarithmic model first </p>

<pre><code>dec = read.table('decay.txt', header=T)
attach(dec)
logy &lt;- log(y)
model1 &lt;- lm(logy ~ x)
</code></pre>

<p>And now I try to fit a quadratic model into the data: </p>

<pre><code>model2 &lt;- lm(y ~ x + I(x^2))
</code></pre>

<p>I now attempt to get the r^2 (r-squared values) of the models. </p>

<pre><code>(rsq1 &lt;- summary(model1)$r.squared)
[1] 0.8307964

(rsq2 &lt;- summary(model2)$r.squared)
[1] 0.9079788
</code></pre>

<p>Obviously as seen, here the coefficient of determination of the quadratic regression model is better than the exponential regression model (for this dataset). </p>

<p>However, the critical appraisal of these models says otherwise. For the exponential model, we see that: 
<a href=""http://i.stack.imgur.com/ha55L.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ha55L.jpg"" alt=""Exponential regression""></a></p>

<p>And for the quadratic polynomial model, we see that: 
<a href=""http://i.stack.imgur.com/B30jv.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/B30jv.jpg"" alt=""Quadratic regression""></a></p>

<p>The QQ Plot reveals something strange with the residuals for the Quadratic regression i.e ""Not Normally Distributed"". </p>

<p>Now I am trying to use ANOVA, to compare the two models. And I type: </p>

<pre><code>anova(model1, model2)  
</code></pre>

<p>and there is an error regarding the missing variable <code>y</code>, because the response variables are different. </p>

<p>How can I statistically compare the two models? </p>
"
"0.075412822378","0.0821301562353182","222803","<p>I am using elastic net regression on a dataset with quite a small number of observations (clinical risk scores) and large number (1000+) of potential predictor variables (gene expression values). The ultimate aim is to identify variables (genes) that could be explored further experimentally. </p>

<p>However, I noticed that the variables being selected (e.g. coefficients not equal to zero) vary when I leave out a single observation from the dataset (some are maintained, some drop, some are added in), and I would like get some more confidence regarding which variables are relatively robust to such changes.</p>

<ol>
<li><p>Would it be methodologically acceptable to generate either jackknife or bootstrap datasets from my original dataset, and repeat the whole model selection procedure (e.g. repeated selection of tuning parameters based on cross validation and the associated model) for each of these datasets, and then for each of those models determine how often a variable was selected?</p></li>
<li><p>Would it be acceptable to re-run a linear regression analysis with sets of selected genes of decreasing frequency of selection (e.g. only use the top 3 genes selected most often, or the top 4 etc), and select the cut-off of frequency of selection of genes based on the cross validation metrics I get for each of these models? If I do this I see a sort of leveling of cross validation prediction error metrics after a certain point (e.g. adding further variables does not help to reduce prediction error metrics). This second approach feels like a ""wrong"" thing to do. However, it is just to determine a threshold of which subset of selected genes I want to focus on further.</p></li>
</ol>
"
"0.0806196982594614","0.0658506226617377","223379","<p>I'm fitting an <code>arima</code>(1,0,0) model using the <code>forecast</code> package in R on the <code>usconsumption</code> dataset. However, when I mimic the same fit using <code>lm</code>, I get different coefficients. My understanding is that they should be the same (in fact, they give the same coefficients if I model an <code>arima</code>(0,0,0) and <code>lm</code> with only the external regressor, which is related to this post: <a href=""http://stats.stackexchange.com/questions/28472/regression-with-arima0-0-0-errors-different-from-linear-regression"">Regression with ARIMA(0,0,0) errors different from linear regression</a>). </p>

<p>Is this because <code>arima</code> and <code>lm</code> use different techniques to calculate coefficients? If so, can someone explain the difference?  </p>

<p>Below is my code.</p>

<pre><code>&gt; library(forecast)
&gt; library(fpp)
&gt; 
&gt; #load data
&gt; data(""usconsumption"")
&gt; 
&gt; #create equivalent data frame from time-series
&gt; lagpad &lt;- function(x, k=1) {
+   c(rep(NA, k), x)[1 : length(x)] 
+ }
&gt; usconsumpdf &lt;- as.data.frame(usconsumption)
&gt; usconsumpdf$consumptionLag1 &lt;- lagpad(usconsumpdf$consumption)
&gt; 
&gt; #create arima model
&gt; arima(usconsumption[,1], xreg=usconsumption[,2], order=c(1,0,0))

Call:
arima(x = usconsumption[, 1], order = c(1, 0, 0), xreg = usconsumption[, 2])

Coefficients:
         ar1  intercept  usconsumption[, 2]
      0.2139     0.5867              0.2292
s.e.  0.0928     0.0755              0.0605

sigma^2 estimated as 0.3776:  log likelihood = -152.87,  aic = 313.74
&gt; 
&gt; #create lm model
&gt; lm(consumption~consumptionLag1+income, data=usconsumpdf)

Call:
lm(formula = consumption ~ consumptionLag1 + income, data = usconsumpdf)

Coefficients:
    (Intercept)  consumptionLag1           income  
         0.3779           0.2456           0.2614  
</code></pre>
"
"0.0879633023282099","0.100588487635796","223582","<p>I am trying to tie the odds ratio from a 2x2 cross classification table to the intercepts of a logistic regression on those 2 variables. I have a cross classification table that produces 2 odds ratios and the results of a logistic regression of PLACE3 ~ VIOL should produce intecepts should match the odds ratio of the contingency table. i.e. Odds ratio = exp(intercepts)  BUT the POLR package is not producing the correct intercepts.</p>

<p>Here is the data.  In the logistic regression PLACE3 is the outcome and VIOl is the independent variable.   You can see the PLACE3 vs. VIOL contingency table below and the logistic regression of PLACE3 ~ VIOL.  The odds ratios in the contingency table 1.79 and 3.1 are correct but the polr function seems off. Any thoughts on why  exp(summary(m)$zeta) does not produce 1.79 and 3.1?</p>

<p>For reference this is from Lemeshow's Applied Logisitic Regression book page 274.</p>

<pre><code>library(data.table)
aps &lt;- fread('http://www.umass.edu/statdata/statdata/data/aps.dat')
colnames(aps) = c(""ID"",""PLACE"",""PLACE3"",""AGE"",""RACE"",""GENDER"",""NEURO"",""EMOT"",""DANGER"",""ELOPE"",""LOS"",""BEHAV"",""CUSTD"",
                    ""VIOL"")
head(aps)
</code></pre>

<p>Here is  a cross classification table of PLACE3 vs. VIOl variables</p>

<pre><code>table(aps$PLACE3,aps$VIOL) 
      0   1
  0  80 179
  1  26 104
  2  15 104
</code></pre>

<p>using PLACE3 = 0 as the reference the 2 odds ratios from the contingency table are </p>

<pre><code>(104*80)/(179*26)  #1.79
(104*80)/(179*15)  #3.10
</code></pre>

<p>These odds ratios should be the same as exponentiating the slope coefficients  from 
a logistic model  PLACE3 ~ VIOL which is below</p>

<pre><code>aps$constant = rep(1,dim(aps)[1])
m &lt;- polr(as.factor(PLACE3) ~ constant + as.factor(VIOL), data = aps, Hess=TRUE,model=TRUE,method = c(""logistic""))
summary(m)

&gt; summary(m)
Call:
polr(formula = as.factor(PLACE3) ~ constant + as.factor(VIOL), 
    data = aps, Hess = TRUE, model = TRUE, method = c(""logistic""))

Coefficients:
                  Value Std. Error t value
as.factor(VIOL)1 0.8454     0.2112   4.003

Intercepts:
    Value  Std. Error t value
0|1 0.6869 0.1884     3.6464 
1|2 1.8608 0.2032     9.1557 

Residual Deviance: 1031.75 
AIC: 1037.75 
</code></pre>

<p>But you can see the exponentiation of the zeta vector is not 1.79 and 3.10</p>

<pre><code>exp(summary(m)$zeta)

&gt; exp(summary(m)$zeta)
     0|1      1|2 
1.987495 6.429049 
</code></pre>
"
"0.118246329960506","0.119724247531067","223901","<p><strong>Does anyone know how R computes the model matrix from a formula in aov() or lm()?</strong> </p>

<p>I wonder about some things. Just assume the models below makes sense (although it might not). You have factor S with 3 levels and 2 regression variables, x1 &amp; x2. Printing model.matrix gives me 1st row output as below. <strong>I would like to know what R does when reading in the formula (taking into account the formula terms which are either factors or regression variables)</strong>.</p>

<pre><code>~S+x1+x2
(Intercept) S2 S3 x1 x2
</code></pre>

<p>The first case, R assumes S1 to be the intercept and fits the main effects each level of S and each of the regression variables. I think this should be correct.</p>

<pre><code>~S:(x1+x2)
(Intercept) S1:x1 S2:x1 S3:x1 S1:x2 S2:x2 S3:x2
</code></pre>

<p>The second case, R expands the formula such that ~S:x1+S:x2. It considers S1:x1 which is the first term of the formula as a regression coefficient so it fits an ordinary intercept. Also, it fits all the linear combinations of levels of S and the regression variables. </p>

<pre><code>~S*(x1+x2)
(Intercept) S2 S3 x1 x2 S2:x1 S3:x1 S2:x2 S3:x2
</code></pre>

<p>The third case, R expands the formula such that ~S+x1+x2+S:x1+S:x2. It considers S1 as the first term of the formula so it makes it its intercept. The main effects are fit first. Then something weird happens. <strong>Why does S2:x1 appear but not S1:x1 first?</strong> <strong>Furthermore, why does S2:x2 appear but not S1:x2 first?</strong>. </p>

<pre><code>~S*(x1+x2)+F
(Intercept) S2 S3 x1 x2 F2 F3 S2:x1 S3:x1 S2:x2 S3:x2
</code></pre>

<p>The fourth case, I added a factor F with 3 levels into the equation. The formula expands so ~S+x1+x2+F. It appears that all main effect terms are brought forward but the order in which they appear in the equation is preserved. Then followed by interaction terms and after that higher order interaction terms(if there are). Since S1 appears first I would assume that S1 is the intercept. <strong>But why doesn't F1 appear before F2 , why doesn't S1:x1 appear before S2:x1 and why doesn't S1:x2 appear before S2:x2?</strong></p>

<p>I may have a conceptual misunderstanding with intercepts and how it relates to how the coefficients are fitted. Thanks for the help in advance.</p>
"
"0.0609427635336005","0.0580747904139041","224078","<p>I have three related questions on the package CausalImpact in R. The package can be found <a href=""https://github.com/google/CausalImpact"" rel=""nofollow"">here</a> and a reproducible example is below.</p>

<ol>
<li>Do I basically understand correctly, that the model makes ""1-step ahead""
predictions? I assume it works like a simple lm model that makes
lots of regressions for t+1 with predictors values from t-1 and then looks for the most contributory predictors?</li>
<li>When talking about ""coefficients"", does that mean they are (Pearson)
correlation coefficients?</li>
<li>The function <code>plot(impact$model$bsts.model,""coefficients"")</code> produces
a plot with inclusion probabilities ranging from 0 to 1. Is there
any way to access the actual values in a table? I found
that <code>colMeans(impact$model$bsts.model$coefficients)</code> provides some
values but I'd like to have a confirmation for this.</li>
<li>In my code, I changed <code>bsts.model &lt;- bsts(y ~ x1, ss, niter = 1000)</code> to <code>bsts.model &lt;- bsts(y ~ x1 + x2 + x3 + x4 + x5, ss, niter = 1000)</code> in order to get values for different variables that I defined before. I did not change the cbind functions for that, as I weren't sure if that was necessary. Now I wonder what to do when I do have a table with -let's say- 200 predictors? Do I have to enter x1 to x200 after <code>bsts(y ~</code> to find the best predictors?</li>
</ol>

<p>R code below:</p>

<pre><code>install.packages(""devtools"")
library(devtools)
devtools::install_github(""google/CausalImpact"")
#Download the tar from the git and then install the package in RStudio.

library(CausalImpact)

set.seed(1)
x1 &lt;- 100 + arima.sim(model = list(ar = 0.999), n = 100)
x2 &lt;- 50 + arima.sim(model = list(ar = 0.899), n = 100)
x3 &lt;- 80 + arima.sim(model = list(ar = 0.799), n = 100)
x4 &lt;- 1.25 * x1 + rnorm(100)
x5 &lt;- 101 + arima.sim(model = list(ar = 0.999), n = 100)
y &lt;- 1.2 * x1 + rnorm(100)
y[71:100] &lt;- y[71:100] + 10
data &lt;- cbind(y, x1)

dim(data)
head(data)
data
matplot(data, type = ""l"")

time.points &lt;- seq.Date(as.Date(""2014-01-01""), by = 1, length.out = 100)
data &lt;- zoo(cbind(y, x1), time.points)
head(data)

pre.period &lt;- as.Date(c(""2014-01-01"", ""2014-03-11""))
post.period &lt;- as.Date(c(""2014-03-12"", ""2014-04-10""))

impact &lt;- CausalImpact(data, pre.period, post.period)
plot(impact)

summary(impact)
summary(impact, ""report"")
impact$summary

post.period &lt;- c(71, 100)
post.period.response &lt;- y[post.period[1] : post.period[2]]
y[post.period[1] : post.period[2]] &lt;- NA

ss &lt;- AddLocalLevel(list(), y)

bsts.model &lt;- bsts(y ~ x1 + x2 + x3 + x4 + x5, ss, niter = 1000)

impact &lt;- CausalImpact(bsts.model = bsts.model,post.period.response =     post.period.response)

plot(impact)
summary(impact)
summary(impact, ""report"")

plot(impact$model$bsts.model,""coefficients"")
plot(impact$model$bsts.model, ""coef"", inc = .1)
plot(impact$model$bsts.model, ""coef"", inc = .05)

colMeans(impact$model$bsts.model$coefficients)
</code></pre>
"
"0.0963589698356145","0.0826418755551823","224377","<p>I am runnning a Random Coefficient Mixed Model in <code>R</code> using <code>lme</code> in <code>{lme4}</code>. I had to transform my dependent variable by square-root because of problems of uniqual variance of the errors. However, with this formulation of the DV, the interpretation of my coefficients' predictors becames quite tricky.</p>

<p>My sample counts 20,000 observations.</p>

<p>Originally I have thought of switching to Non-Linear Mixed models, but in <em>stackoverflow.com</em> someone suggested that ""fitting a variance structure with the weights parameter"" could be a valid alternative to the use of Non-Linear Mixed Model.</p>

<p>I have thus tried fitting a regression of the kind below with <code>lme</code> <code>{lme4}</code> in <code>R</code>,</p>

<p><strong>note that</strong> part of the heteroskedasticity takes place between groups, the random coefficient model improves the structure of the errors when taking into account for the province and district levels, however the non-normality of the DV causes the errors' distribution to be non-normal too.
The square root transformation makes the DV approximate a normal almost perfectly- see at the end of the post.</p>

<p><code>Model2 &lt;-
  lme(
       fcs ~ hh_size + head_sex + head_age + head_edu + residence_code + head_marital_status + ...,
       random = list(
       dist_code_unique = ~ 1 + some vars,
       prov_code = ~ 1
       ),
       weights =~ fcs_sqrt,
       data = data
  )</code>
where fcs is my original dependent variable, and fcs_sqrt is the square root transformation of it.</p>

<p>The result using and not using weights in terms of standardize residuals is shown in the two graphs below.</p>

<p><strong>The question is</strong>: Am I allowed to give the weights in this manner? Are there any implications for the interpretation of the results?</p>

<p><a href=""http://i.stack.imgur.com/svIQo.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/svIQo.png"" alt=""No weights used""></a>
<a href=""http://i.stack.imgur.com/h4iZh.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/h4iZh.png"" alt=""Weights = fcs_sqrt""></a></p>

<p><a href=""http://i.stack.imgur.com/xv6Ve.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xv6Ve.png"" alt=""enter image description here""></a></p>
"
"0.110147477177496","0.112461348053768","225283","<p>Iâ€™m analyzing crowdsourced Twitter data, where workers labeled tweets. Within my dataset (N=2,400), I have one IV (call it â€˜dsâ€™) with 2 levels that differentiates which dataset the workers labeled. I have four factors of interest (what workers labeled) -- these are my DVs (let's call them f1, f2, f3, f4). Three of those factors are binomial &lt;0,1>, and one multinomial &lt;0, 1, 2>. Even though the latter can be treated as ordinal, I'm working under the assumption it is nominal. Finally, my datasets are of unequal lengths.</p>

<p>My goal is to analyze the relationship between each of the labeled factors for each level of the IV. More specifically, <strong>I want to tease out the different contributions of each of those factors on each dataset quantitatively, i.e., show amount of variance explained</strong> (e.g., ds1 influenced f1 more than f2, while the inverse for ds2). The end game is to model each factor into a scoring function, which allows me to compute a unified score. Hence, I need to back up the parameter weights for this function.</p>

<p>A snippet of my data frame looks like this:</p>

<pre><code>   f1 f2 f3 f4 ds
1   1  0  1  0  1
2   0  0  0  2  1
3   0  0  1  1  2
4   1  1  0  2  2
</code></pre>

<p>What I initially did was to compute correlations between each factor, and used the strengths of those correlations to back up my scoring function. However, given the many posts and tutorials I've been reading, it seems I need to make use of a mix of logistic and multinomial regression. What I have done so far is run binomial logit (using ?glm with class â€˜binomial') on the first 3 factors, and multinomial regression (using ?nnet) on f4. However, it seems I can only assess one outcome variable at a time.</p>

<p>For f1-f3, I have run the following R code:</p>

<pre><code>fit &lt;- glm(f1 ~ ds, data = xx, family = ""binomial"")
summary(fit)
confint.default(fit)
wald.test(b = coef(fit), Sigma = vcov(fit), Terms = 2)
</code></pre>

<p>For f4:</p>

<pre><code>fit &lt;- multinom(f4 ~ ds, data = xx)
summary(fit)
z &lt;- summary(fit)$coefficients/summary(fit)$standard.errors
p &lt;- (1 - pnorm(abs(z), 0, 1))*2
</code></pre>

<p>My questions:</p>

<p><strong>1.</strong> Is running such logistic regression analyses appropriate for what I want to do, namely to tease out contributions of each factor? If so, is it meaningful to compare the coefficients of each factor with the other, when computed separately? Or is simply showing a correlation matrix sufficient in my case?</p>

<p><strong>2.</strong> Are there alternative techniques to assess all outcome variables/DVs at once, with respect to each level of my IV? If so, could you please provide me with some pointers (ideally for R)? I'm now looking into hierarchical multinomial marginal (HMM) models... </p>

<p>If something is unclear above, Iâ€™d be happy to clarify.</p>
"
"NaN","NaN","225645","<p>I am trying to perform a logistic regression with the following code </p>

<blockquote>
  <p><code>Y ~ x1+x2+x3,data=data, family=binomial(link=""logit"")</code>. </p>
</blockquote>

<p>However on inspection of both the outcome and predictors i noticed that they are characterized by spatial auto-correlation. My question is, how do I account for the spatial auto-correlation, to get better coefficients? </p>
"
"0.0914141453004008","0.0774330538852055","225697","<p>Let me give a simple example,</p>

<pre><code>set.seed(100)
disease = sample(c(0,1),100,replace = TRUE)
snp1 = sample(c(""AA"",""AB"",""BB""),100,replace = TRUE)
snp2 = sample(c(""XX"",""XY"",""YY""),100,replace = TRUE)

summary(glm(disease~snp1*snp2, family = binomial))
</code></pre>

<p>output1</p>

<pre><code>Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.55176  -0.94003  -0.00649   0.90052   1.53535  

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   -8.109e-01  6.009e-01  -1.349   0.1772  
snp1AB         5.232e-01  9.718e-01   0.538   0.5903  
snp1BB         1.504e+00  8.580e-01   1.753   0.0796 .
snp2XY         4.074e-16  8.498e-01   0.000   1.0000  
snp2YY         1.504e+00  9.280e-01   1.621   0.1051  
snp1AB:snp2XY  1.135e+00  1.335e+00   0.850   0.3952  
snp1BB:snp2XY  1.542e-01  1.254e+00   0.123   0.9022  
snp1AB:snp2YY -1.216e+00  1.333e+00  -0.912   0.3616  
snp1BB:snp2YY -2.785e+00  1.244e+00  -2.239   0.0252 *
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 138.63  on 99  degrees of freedom
Residual deviance: 127.71  on 91  degrees of freedom
AIC: 145.71
</code></pre>

<p>Output2</p>

<pre><code>snp12 = interaction(snp1,snp2)
summary(glm(disease~snp12, family = binomial))


Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.55176  -0.94003  -0.00649   0.90052   1.53535  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept) -8.109e-01  6.009e-01  -1.349   0.1772  
snp12AB.XX   5.232e-01  9.718e-01   0.538   0.5903  
snp12BB.XX   1.504e+00  8.580e-01   1.753   0.0796 .
snp12AA.XY  -3.990e-16  8.498e-01   0.000   1.0000  
snp12AB.XY   1.658e+00  9.150e-01   1.812   0.0700 .
snp12BB.XY   1.658e+00  9.150e-01   1.812   0.0700 .
snp12AA.YY   1.504e+00  9.280e-01   1.621   0.1051  
snp12AB.YY   8.109e-01  8.333e-01   0.973   0.3305  
snp12BB.YY   2.231e-01  8.199e-01   0.272   0.7855  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 138.63  on 99  degrees of freedom
Residual deviance: 127.71  on 91  degrees of freedom
AIC: 145.71

Number of Fisher Scoring iterations: 4
</code></pre>

<p>So here I did a logistic regression for interaction between, lets say 2 mutations (each with 3 categories). Like shown above I can do it in 2 ways. My questions are,</p>

<ol>
<li>Are both output1 and output2 same ? </li>
<li>If same, which one is more appropriate?</li>
<li>How to interpret the coefficients (and odds ratios) in each case?</li>
</ol>
"
"0.118246329960506","0.112681644735122","226066","<p>This problem has held me up for three days now, so I really hope somebody here has a solution for the problem.</p>

<p>I have a model with an excessive number of zeros, so I use a zero-inflated poisson regression model with the following code and summary.</p>

<pre><code>cr_f1 = formula(cr ~ depth + habtype2 + month + year + lightregime + depth*month + depth*lightregime + depth*habtype2 + habtype2*year | depth + habtype2 + month + year + lightregime + depth*month + depth*lightregime + depth*habtype2)
summary(zeroinfl(cr_f1, dist = ""poisson"", link = ""logit"", data = allUVCdata))

Call:
zeroinfl(formula = cr_f1, data = allUVCdata, dist = ""poisson"", link = ""logit"")

Pearson residuals:
    Min      1Q  Median      3Q     Max 
-1.6430 -0.5680 -0.2893  0.1426 16.8090 

Count model coefficients (poisson with log link):
                            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)                -4.515522   2.182503  -2.069  0.03855 * 
depth                       0.108941   0.072278   1.507  0.13175   
habtype2Pinnacles           0.879765   0.791166   1.112  0.26614   
habtype2Unexposed          -0.604246   0.786129  -0.769  0.44211   
month2                      0.628468   0.380450   1.652  0.09855 . 
month3                      0.309282   0.367690   0.841  0.40026   
month4                      0.649411   0.371667   1.747  0.08059 . 
month5                      0.758717   0.364079   2.084  0.03717 * 
month6                      0.467611   0.341024   1.371  0.17031   
month7                      0.523043   0.343363   1.523  0.12768   
month8                      0.563272   0.356843   1.578  0.11445   
month9                      0.204509   0.400398   0.511  0.60952   
month10                     0.662415   0.341616   1.939  0.05249 . 
month11                     0.934844   0.335077   2.790  0.00527 **
month12                     0.252216   0.360512   0.700  0.48417   
year2013                   -1.271010   1.282158  -0.991  0.32154   
year2014                    1.221887   0.753644   1.621  0.10495   
year2015                   -0.463176   0.771131  -0.601  0.54808   
lightregimeLight            2.754925   1.948779   1.414  0.15746   
depth:month2               -0.019864   0.008906  -2.230  0.02572 * 
depth:month3               -0.014157   0.008106  -1.747  0.08071 . 
depth:month4               -0.020553   0.008332  -2.467  0.01364 * 
depth:month5               -0.021213   0.008373  -2.533  0.01129 * 
depth:month6               -0.013561   0.007393  -1.834  0.06663 . 
depth:month7               -0.015043   0.007544  -1.994  0.04615 * 
depth:month8               -0.017383   0.008011  -2.170  0.03003 * 
depth:month9               -0.012340   0.008990  -1.373  0.16988   
depth:month10              -0.019631   0.007629  -2.573  0.01008 * 
depth:month11              -0.024101   0.007611  -3.167  0.00154 **
depth:month12              -0.014319   0.007952  -1.801  0.07174 . 
depth:lightregimeLight     -0.079860   0.071024  -1.124  0.26084   
depth:habtype2Pinnacles    -0.006819   0.011178  -0.610  0.54182   
depth:habtype2Unexposed     0.014857   0.011103   1.338  0.18086   
habtype2Pinnacles:year2013  1.351509   1.277930   1.058  0.29025   
habtype2Unexposed:year2013  1.538282   1.256047   1.225  0.22069   
habtype2Pinnacles:year2014 -1.213233   0.754305  -1.608  0.10775   
habtype2Unexposed:year2014 -0.495275   0.726863  -0.681  0.49563   
habtype2Pinnacles:year2015  0.389117   0.775476   0.502  0.61582   
habtype2Unexposed:year2015  0.659117   0.750396   0.878  0.37975   

Zero-inflation model coefficients (binomial with logit link):
                        Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)             -4.61555    7.04621  -0.655 0.512442    
depth                    0.28728    0.28211   1.018 0.308524    
habtype2Pinnacles        9.41037    3.82210   2.462 0.013813 *  
habtype2Unexposed        2.11213    1.46465   1.442 0.149282    
month2                   8.67847    3.91193   2.218 0.026523 *  
month3                   7.12210    3.86428   1.843 0.065320 .  
month4                   4.10296    2.41285   1.700 0.089044 .  
month5                  12.76919    4.28035   2.983 0.002852 ** 
month6                   3.57695    2.49820   1.432 0.152198    
month7                   5.85534    3.27394   1.788 0.073700 .  
month8                   5.59503    3.33054   1.680 0.092974 .  
month9                   4.22953    3.76919   1.122 0.261807    
month10                  6.35022    3.59424   1.767 0.077265 .  
month11                  5.92079    3.36405   1.760 0.078404 .  
month12                  4.36214    3.17233   1.375 0.169113    
year2013                -0.18722    0.42651  -0.439 0.660688    
year2014                -1.50194    0.45263  -3.318 0.000906 ***
year2015                -9.79773    4.87536  -2.010 0.044469 *  
lightregimeLight         0.79826    5.62419   0.142 0.887133    
depth:month2            -0.39212    0.16795  -2.335 0.019557 *  
depth:month3            -0.36363    0.16695  -2.178 0.029397 *  
depth:month4            -0.21521    0.10211  -2.108 0.035059 *  
depth:month5            -0.57543    0.16933  -3.398 0.000678 ***
depth:month6            -0.24336    0.10398  -2.341 0.019256 *  
depth:month7            -0.33704    0.13975  -2.412 0.015874 *  
depth:month8            -0.35343    0.14683  -2.407 0.016082 *  
depth:month9            -0.31787    0.16903  -1.881 0.060026 .  
depth:month10           -0.37550    0.16021  -2.344 0.019087 *  
depth:month11           -0.34650    0.14821  -2.338 0.019397 *  
depth:month12           -0.29639    0.14221  -2.084 0.037142 *  
depth:lightregimeLight   0.08117    0.21795   0.372 0.709571    
depth:habtype2Pinnacles -0.57765    0.17049  -3.388 0.000704 ***
depth:habtype2Unexposed -0.17897    0.06252  -2.863 0.004200 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Number of iterations in BFGS optimization: 146 
Log-likelihood: -3977 on 72 Df
</code></pre>

<p>So I included the interaction 'habtype2*year' in the count part of the formula, but now want to include it in the second model aswel (the binomial), but if I do I get the following error:</p>

<pre><code>cr_f1 = formula(cr ~ depth + habtype2 + month + year + lightregime + depth*month + depth*lightregime + depth*habtype2 + habtype2*year | depth + habtype2 + month + year + lightregime + depth*month + depth*lightregime + depth*habtype2 + habtype2*year)

summary(zeroinfl(cr_f1, dist = ""poisson"", link = ""logit"", data = allUVCdata))

Error in solve.default(as.matrix(fit$hessian)) : 
  system is computationally singular: reciprocal condition number = 2.08629e-37
</code></pre>

<p>This also happens if I want to try to include any of the other interaction terms that I still want to put into the model (""month<em>year"", ""month</em>lightregime"" and ""month*habtype2"").</p>

<p>I searched here on the forum and on google, seems like more people have encountered this error (also in other functions that doing a zeroinfl), but I have not found any suitable solution.</p>

<p>Data: sightings of as species on 29 different locations, >5200 observations (including zeros). </p>

<p>What could possibly solve this, so that I can run the model with the interaction terms that I want?</p>

<p><strong>EDIT</strong>: added some new output to give insight to the problem.</p>

<pre><code>allUVCdata$year = as.numeric(as.character(allUVCdata$year))
cr_f1 = formula(cr ~ depth + lightregime + month + year + habtype2 + month*habtype2 + month*year + habtype2*year + depth*month)
summary(hurdle(cr_f1, dist = ""poisson"", link = ""logit"", data = allUVCdata))

Error in solve.default(as.matrix(fit_count$hessian)) : 
  system is computationally singular: reciprocal condition number = 6.23277e-26
&gt; allUVCdata$year = as.factor(as.character(allUVCdata$year))
&gt; table(allUVCdata$year, allUVCdata$cr)

          0    1    2    3    4    5    6    7
  2012  750  149   25   12    3    0    0    0
  2013 1133  209   69   16    4    1    1    0
  2014  844  387  142   42   11    7    0    1
  2015  833  401  125   31    5    3    1    2
&gt; table(allUVCdata$month, allUVCdata$cr)

       0   1   2   3   4   5   6   7
  1  299  53  18   7   1   1   1   2
  10 346 104  40   9   4   4   0   0
  11 328 114  43  17   5   0   0   0
  12 350 112  29  10   2   0   0   0
  2  248  80  16   1   1   0   0   1
  3  303  82  24   6   0   0   1   0
  4  329  93  32   4   0   0   0   0
  5  277 105  28   9   1   1   0   0
  6  312 111  36  12   3   2   0   0
  7  362 113  46  14   5   2   0   0
  8  213 100  25   8   1   1   0   0
  9  193  79  24   4   0   0   0   0
&gt; table(allUVCdata$month, allUVCdata$year)

     2012 2013 2014 2015
  1    41  129   72  140
  10   91  149  152  115
  11  112  121  150  124
  12  112  124  154  113
  2    35  108   79  125
  3    33  149  101  133
  4    88  105  150  115
  5   101  108   95  117
  6    94  115  142  125
  7   118  133  153  138
  8    61  114  100   73
  9    53   78   86   83

table(allUVCdata$habtype2, allUVCdata$year)

            2012 2013 2014 2015
  Exposed     93  138  120  144
  Pinnacles  274  386  339  338
  Unexposed  572  909  975  919
</code></pre>
"
"0.0304713817668003","0.029037395206952","226535","<p>I was interested in knowing if for a quasipoisson regression, the residuals are on a log scale</p>

<p>If this is my model</p>

<pre><code>mdl&lt;-glm(y ~ x1 +x1, family=""quasipoisson"")
</code></pre>

<p>To get the slope on the response scale, I do this</p>

<pre><code>exp(mdl$coefficients[2])-1
</code></pre>

<p>If I want the residuals as well on the response scale, do I have to do the same thing?</p>

<pre><code>exp(mdl$residuals)-1
</code></pre>
"
"0.0430930413588572","0.0410650781176591","227073","<p>I have a small perplexity some of you might be able to help me with. 
I have fitted a linear model in R of the form</p>

<pre><code>fullmodel = lm(Y ~ X1 + X2)
</code></pre>

<p>and I want to obtain Likelihood Ratio Tests on the regression coefficients for <code>X1</code> and <code>X2</code>. 
One way to get them is using:</p>

<pre><code>anova(fullmodel, test=""LRT"")
</code></pre>

<p>But, in my understanding, if I use <code>anova</code> on the full model it removes covariates and performs LRT sequentially, indeed results differed depending on ordering of predictors.
<code>drop1</code>, on the other hand, drops one covariate at a time and leaves the rest untouched; thus I could use:</p>

<pre><code>drop1(fullmodel, test=""Chisq"")
</code></pre>

<p>This should work. Yet, out of curiosity, I also tried the following:</p>

<pre><code>fullmodel = lm(Y ~ X1 + X2)
reducedmodel1 = lm(Y ~ X1)
reducedmodel2 = lm(Y ~ X2)

anova(fullmodel, reducedmodel1, test=""LRT"")
anova(fullmodel, reducedmodel2, test=""LRT"")
</code></pre>

<p>In my understanding, the two procedures (<code>drop1</code> and the two separate <code>anova</code>) have identical meaning and should give exactly the same p-values. That's not the case, though; they differ already at the 3rd decimal number. 
Can anyone explain to me why this happens? Am I doing something wrong?</p>
"
"0.0545088647991304","0.0519436716578171","228257","<p>I have performed mixed effect Cox hazard regressions, and <a href=""http://stats.stackexchange.com/questions/228229/reconstructing-slopes-in-mixed-effect-models"">reconstructed the slopes</a> to get group specific slopes (e.g. sex-specific responses to the explanatory variable). I aim to test whether the slopes differ from one another (e.g. do males and females respond differently to the explanatory variable?). To do this I will use Z-tests (<a href=""http://stats.stackexchange.com/questions/55501/test-a-significant-difference-between-two-slope-values"">here</a> and <a href=""http://stats.stackexchange.com/questions/13112/what-is-the-correct-way-to-test-for-significant-differences-between-coefficients"">here</a>) where</p>

<p>$$Z=  \frac{\beta_1-\beta_2}{\sqrt{{SE_{\beta_1}}^{2}+{SE_{\beta_2}}^2}}$$</p>

<p>However, I have performed my models in R using the coxme package which gives the following output, from which I reconstruct the sex- and group-specific slopes with the included function.</p>

<pre><code>...
Fixed coefficients
                        coef exp(coef)   se(coef)      z    p
SexM             0.091305017 1.0956031 0.09085235   1.00 0.31
GroupG2         -0.036313825 0.9643376 0.08889039  -0.41 0.68
NE              -0.192009224 0.8252993 0.01317388 -14.57 0.00
SexM:GroupG2     0.009757875 1.0098056 0.12750426   0.08 0.94
SexM:NE         -0.212264676 0.8087506 0.02008058 -10.57 0.00
GroupG2:NE      -0.006933708 0.9930903 0.01814987  -0.38 0.70
SexM:GroupG2:NE  0.044999019 1.0460268 0.02756553   1.63 0.10
...


coxSlopeFunc = function(model, nfixed = 1){
    if(nfixed ==1){
        # Slope for Females + G1
        FG1 = model$coefficients[3]
	# Slope for Males + G1
	MG1 = model$coefficients[3] + model$coefficients[5]

        # Slope for Females + G2
        FG2 = model$coefficients[3] + model$coefficients[6]
        # Slope for Males + G2
        MG2 = model$coefficients[3] + model$coefficients[5] + model$coefficients[6] + model$coefficients[7]

        # Sex differences in slope
        SG1 = FG1 - MG1
        SG2 = FG2 - MG2

    matrix(c(FG1,MG1,FG2,MG2,SG1,SG2), ncol = 1, byrow = T)}
}
round(coxSlopeFunc(coxdum),3)

&gt; round(coxSlopeFunc(coxdum),3)
       [,1]
[1,] -0.192
[2,] -0.404
[3,] -0.199
[4,] -0.366
[5,]  0.212
[6,]  0.167
</code></pre>

<p>However, I am unsure how to calculate the SE of the slope for each - should I just sum the standard errors of the components? </p>

<p>$$\frac{(-0.192009224 - (-0.192009224 + -0.212264676))}{\sqrt{0.01317388^2 + (0.01317388 +  0.02008058)^2}}$$</p>
"
"0.118015154118746","0.112461348053768","228316","<p>I want to predict a binary response variable <code>y</code> using logistic regression. <code>x1</code> to <code>x4</code> are the log  of continuous variables and <code>x5</code> to <code>x7</code> are binary variables. </p>

<pre><code>Call:
glm(formula = y ~ x1 + x2 + x3 + x4 + x5 + 
    x6 + x7, family = binomial(), data = df)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.6604  -0.5712   0.4691   0.6242   2.4095  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -2.84633    0.31609  -9.005  &lt; 2e-16 ***
x1             0.14196    0.04828   2.940  0.00328 ** 
x2             4.05937    0.22702  17.881  &lt; 2e-16 ***
x3            -0.83492    0.08330 -10.023  &lt; 2e-16 ***
x4             0.05679    0.02109   2.693  0.00709 ** 
x5             0.08741    0.18955   0.461  0.64467    
x6            -2.21632    0.53202  -4.166  3.1e-05 ***
x7             0.25282    0.15716   1.609  0.10769    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1749.5  on 1329  degrees of freedom
Residual deviance: 1110.5  on 1322  degrees of freedom
AIC: 1126.5

Number of Fisher Scoring iterations: 5
</code></pre>

<p>The output of the GLM shows that most of my variables are significant for my model, but the various goodness of fit test I have done:</p>

<pre><code>anova &lt;- anova(model, test = ""Chisq"")   # Anova
1 - pchisq(sum(anova$Deviance, na.rm = TRUE),df = 7) # Null Model vs Most Complex Model
1 - pchisq(model$null.deviance - model$deviance, 
           df = (model$df.null - model$df.residual )) # Null Deviance - Residual Deviance ~ X^2
hoslem.test(model$y, model$fitted.values, g = 8)     # Homer Lemeshow test
pR2(model)                                            # Pseudo-R^2
</code></pre>

<p>tell me that there is a lack of evidence to support my model.</p>

<p>More over, I have a bimodal deviance plot. I suspect the bimodal distribution is caused by the sparsity of my binary variables.
 <a href=""http://i.stack.imgur.com/J27fL.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/J27fL.png"" alt=""enter image description here""></a></p>

<p>So I calculated the absolute error <code>abs(y - y_hat)</code>, and obtained the following:</p>

<ul>
<li>77% of my absolute errors were in [0;0.25], which I think is very good!</li>
</ul>

<p>On the following plot, Y=1 is red, and Y=0 is green. This model is better at predicting when Y will be 1 than 0.</p>

<p><a href=""http://i.stack.imgur.com/ZEGuv.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZEGuv.png"" alt=""enter image description here""></a></p>

<p>My question is thus the following:</p>

<p>The goodness of fit tests all assume that my null hypothesis follows a Chi square distribution of some sort. Is it correct to conclude that based on my absolute error, my model's prediction is OK, it's just that it doesn't follow a Chi square distribution and thus perform poorly with these tests? </p>
"
"0.0304713817668003","0.029037395206952","228351","<p>As the titles states, I would like to compare two coefficients in my multiple regression model but I'm not quite sure how.</p>

<pre><code>Coefficients:
                 Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)       68.9483    29.7439   2.318 0.024493 *  
Shots.PG          -0.5074     1.4696  -0.345 0.731334    
Shots.OT.PG        7.4992     3.1410   2.388 0.020707 *  
Dribbles.PG        0.6081     0.8121   0.749 0.457401    
Fouled.PG         -0.9856     0.8783  -1.122 0.267031    
Offsides.PG        1.0520     3.0728   0.342 0.733477    
Tackles.PG         0.2705     0.6721   0.402 0.689016    
Fouls.PG          -0.4230     0.7893  -0.536 0.594329    
Ints.PG            0.3414     0.5962   0.573 0.569451    
Shots.Allowed.PG  -3.3604     0.8063  -4.167 0.000119 ***
</code></pre>

<p>Above are the results I've obtained. At first glance I thought it was interesting Shots OT has double the impact of Shots Allowed but I see that their standard errors are significantly different so that worries me.</p>

<p>How would I go about comparing these two values?</p>

<p>Using linear.hypothesis() I get:</p>

<p>Linear hypothesis test</p>

<pre><code>Hypothesis:
Shots.OT.PG  + 2 Shots.Allowed.PG = 0

Model 1: restricted model
Model 2: Points ~ Shots.PG + Shots.OT.PG + Dribbles.PG + Fouled.PG + Offsides.PG + 
    Tackles.PG + Fouls.PG + Ints.PG + Shots.Allowed.PG

  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)
1     52 4488.5                           
2     51 4484.2  1    4.2107 0.0479 0.8277
</code></pre>

<p>How do I interpret this? Does this mean they are not different due to its large P Value. I am trying to find out whether or not the Shots OT has a larger effect on the Points total than the Shots Allowed PG</p>
"
"0.091874672876503","0.0875510407188402","228641","<p>Consider the following dataset I want to use as the independent variables to conduct linear regression on:</p>

<pre><code>set.seed(42)
sa = runif(10)
sb = runif(10)
sc = sb+sa
sd = sb-sa
df = data.frame(sa,sb,sc,sd)
</code></pre>

<p>Now I want to perform tests for multicollinearity. I'm aware of the <code>ppcor</code> package, which calculates the partial correlation between the variables. In this case:</p>

<pre><code>&gt; pcor(df)
$estimate
            [,1]        [,2]       [,3]       [,4]
[1,]  1.00000000  0.06649968 -0.7325597  0.7706902
[2,]  0.06649968  1.00000000 -0.6304810 -0.6870502
[3,] -0.73255975 -0.63048097  1.0000000  0.1308260
[4,]  0.77069021 -0.68705016  0.1308260  1.0000000
</code></pre>

<p>As far as I know, there is no way of telling that <code>sc</code> and <code>sd</code> are linear combinations of <code>sa</code> and <code>sb</code>, just by looking at the estimates (or the other outputs of <code>pcor</code>, for that matter).</p>

<p>The only method that comes to my mind, is applying linear regression on each of the independent variables like so:</p>

<pre><code>summary(lm(sc~sa+sb,df))

Coefficients:
             Estimate Std. Error   t value Pr(&gt;|t|)    
(Intercept) 1.404e-16  3.915e-17 3.587e+00   0.0089 ** 
sa          1.000e+00  4.716e-17 2.121e+16   &lt;2e-16 ***
sb          1.000e+00  4.135e-17 2.418e+16   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 3.672e-17 on 7 degrees of freedom
Multiple R-squared:      1, Adjusted R-squared:      1 

summary(lm(sd~sa+sb,df))

Coefficients:
             Estimate Std. Error   t value Pr(&gt;|t|)    
(Intercept) 1.404e-16  3.915e-17 3.587e+00   0.0089 ** 
sa          1.000e+00  4.716e-17 2.121e+16   &lt;2e-16 ***
sb          1.000e+00  4.135e-17 2.418e+16   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 3.672e-17 on 7 degrees of freedom
Multiple R-squared:      1, Adjusted R-squared:      1 
</code></pre>

<p>I'm wondering two things: </p>

<ol>
<li><p>Is my approach with linear regression reasonable? I think the downside is, that it can only detect linear correlation. But non-linear correlation shouldn't be a problem with linear regression, right?</p></li>
<li><p>Is there an R function/package that automatically checks for multiple correlation?</p></li>
</ol>
"
"0.0806196982594614","0.0658506226617377","228781","<p>I recently estimated some OLS regressions with daily returns scaled by 100 as dependent variable (thus in percentage points). As I learned (and empirically confirmed), this scaling only scales coefficients by 100, but has no impact on statistical significance.</p>

<p>I used the same, scaled returns to estimate a simple EGARCH(1,1) model and found that the scaling directly impacts the coefficient for the unconditional variance <code>omega</code> and the significance of all other coefficients. The results are thus vastly different when using scaled data, especially when it comes to the interpretation of significance.</p>

<p>I used the SP500 data from the ""rugarch"" package in R and the <code>ugarchfit</code> function to produce following example:</p>

<p>Here with normal returns:</p>

<pre><code>        Estimate  Std. Error    t value Pr(&gt;|t|)  
mu      0.000670    0.000211     3.17900 0.001478               
ar1    -0.679036    0.017029   -39.87517 0.000000               
ma1     0.701977    0.016065    43.69611 0.000000               
omega  -0.269569    0.005428   -49.65921 0.000000               
alpha1 -0.197466    0.025594    -7.71537 0.000000               
alpha2  0.129236    0.005627    22.96744 0.000000               
beta1   0.970782    0.000080 12106.51240 0.000000               
gamma1 -0.009223    0.068496    -0.13465 0.892888               
gamma2  0.124195    0.055994     2.21802 0.026553               
shape   4.670759    0.848486     5.50481 0.000000                           
LogLikelihood : 3204.702 
</code></pre>

<p>And here with returns scaled by 100:</p>

<pre><code>        Estimate  Std. Error    t value Pr(&gt;|t|)  
mu      0.067050    0.021114   3.175639 0.001495  
ar1    -0.679038    0.016626 -40.840967 0.000000  
ma1     0.701978    0.016644  42.176242 0.000000  
omega  -0.000460    0.006190  -0.074301 0.940771  
alpha1 -0.197462    0.060240  -3.277926 0.001046  
alpha2  0.129237    0.061149   2.113468 0.034561  
beta1   0.970786    0.003998 242.815325 0.000000  
gamma1 -0.009222    0.073792  -0.124977 0.900542  
gamma2  0.124189    0.075641   1.641818 0.100628  
shape   4.670628    0.881931   5.295909 0.000000  

LogLikelihood : -1400.468 
</code></pre>

<p>Look especially at <code>omega</code> and the significance of <code>gamma2</code>!  </p>

<p>Does anybody know why this is the case?</p>
"
"0.0304713817668003","0.029037395206952","228878","<p>gling with the interpretation of the coefficients of a zero-inflation model and I find no clear answer in the net. Maybe someone can help me and other people in the same situation.</p>

<p>After fitting cancer incidences through a Poisson regression with zero-inflation (zeroinfl package in R), in the logistic component, the coefficient estimate for the age variable is -3.6.</p>

<p>Does that mean that for each additional year of age, the odds of having zero cancer incidences increases by 3.6, or vice versa?</p>

<p>Many thanks, Gion</p>
"
"0.075412822378","0.0821301562353182","228985","<p>I have, as the title suggests, two heavily skewed, overdispersed histograms. The data ranges from 0 minutes to 85334 minutes. 90% of the data is below 15 minutes, and takes the form of a positive-skewed exponential/power distribution. Then, there's just a huge tail. There are two groups with similar data structuresâ€”one for <strong>Conversation A</strong> and <strong>Conversations B</strong>. </p>

<p>I'm solid enough with basic statistics to know that comparing the means, STD, p-values, etc. is pretty useless, but I'm not good enough to know <em>how</em> I can compare these two, or what metrics I can compare with one another to see if being in <strong>A</strong> or <strong>B</strong> has any significant effect on the data. I've done some research, and it looks like <em>negative binomial regression</em> fittings will suit my purposes best.</p>

<p>I'm using the <code>MASS</code> package in R, w/ the calls
<code>glm.nb(conversation$A_times ~ 1)</code>:</p>

<pre><code>Coefficients:
(Intercept)
    5.624

Degrees of Freedom: 1674 (i.e. Null); 1674 Residual
Null Deviance:      1850
Residual Deviance:  1850    AIC: 17130
</code></pre>

<p>and <code>glm.nb(conversation$B_times ~ 1)</code>:</p>

<pre><code>Coefficients:
(Intercept)
    4.768

Degrees of Freedom: 1072 (i.e. Null); 1072 Residual
Null Deviance:      1234
Residual Deviance:  1234    AIC: 12390
</code></pre>

<p>Now, I imagine that the goal here is to compare two coefficients (or sets thereof) for significant differences, but I'm not actually sure what to do with this info. What are some directions I can take to learn more and really figure out what I'm doing? </p>
"
"0.0621994475718397","0.0592723347638087","229235","<p>So I am working on this regression and I am doing it for multiple levels of Females. I have done one regression for Females 18-34 and got this output</p>

<pre><code>F18&lt;-read.csv(""C:/Users/marissa.ferguson/Desktop/Unrated/F18-34.csv"", header = T, sep = "","", na.strings = ""?"")
female&lt;-na.omit(F18)
set.seed(1000)
train.size&lt;-0.8
train.index&lt;- sample.int(length(subfemale$DemoMedianRtg), round(length(subfemale$DemoMedianRtg)*train.size))
train.sample&lt;-subfemale[train.index,]
test.sample&lt;-subfemale[-train.index,]

&gt; Overall&lt;-lm(DemoMedianRtg~ DP+Subscribers+Tier+SubRange+Male.+Female.+Avg.Age+NewTier+NewSubs+Avg.Income, data=train.sample)
&gt; summary(Overall)

Call:
lm(formula = DemoMedianRtg ~ DP + Subscribers + Tier + SubRange + 
Male. + Female. + Avg.Age + NewTier + NewSubs + Avg.Income, 
data = train.sample)

Residuals:
  Min        1Q    Median        3Q       Max 
-0.038269 -0.014386  0.003568  0.012190  0.029538 

Coefficients: (8 not defined because of singularities)
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)      2.258e+00  5.495e-01   4.108 0.000375 ***
DPEarly Fringe   8.178e-03  1.129e-02   0.724 0.475665    
DPEarly Morning  1.012e-02  1.739e-02   0.582 0.566011    
DPLate Fringe    7.304e-02  1.643e-02   4.445 0.000157 ***
DPOvernight      3.327e-02  1.968e-02   1.691 0.103281    
DPPrimeTime      1.999e-02  1.188e-02   1.682 0.105005    
DPWeekend        3.971e-02  1.054e-02   3.769 0.000895 ***
Subscribers     -2.142e-05  5.280e-06  -4.057 0.000428 ***
TierTier2       -9.673e-01  2.090e-01  -4.627 9.79e-05 ***
TierTier3       -1.341e+00  3.010e-01  -4.457 0.000152 ***
SubRange40-50K          NA         NA      NA       NA    
SubRange60-70K          NA         NA      NA       NA    
SubRange80-90K  -3.722e-01  9.075e-02  -4.101 0.000382 ***
SubRange90-100K -1.958e-01  3.001e-02  -6.524 7.82e-07 ***
Male.            9.721e-02  7.822e-02   1.243 0.225490    
Female.                 NA         NA      NA       NA    
Avg.Age                 NA         NA      NA       NA    
NewTierTier2            NA         NA      NA       NA    
NewTierTier3            NA         NA      NA       NA    
NewSubs                 NA         NA      NA       NA    
Avg.Income              NA         NA      NA       NA  
</code></pre>

<p>As you can see Female. is one level. When I do it for another level of females I get this output</p>

<pre><code>F2554&lt;-read.csv(""C:/Users/marissa.ferguson/Desktop/Unrated/F25-54.csv"", header = T, sep = "","", na.strings = ""?"")
 female&lt;-na.omit(F2554)
set.seed(1000)
train.size&lt;-0.8
train.index&lt;- sample.int(length(subfemale$DemoMedianRtg), round(length(subfemale$DemoMedianRtg)*train.size))
train.sample&lt;-subfemale[train.index,]
test.sample&lt;-subfemale[-train.index,]

Overall&lt;-lm(DemoMedianRtg~ Qtr+DP+Subscribers+Tier+SubRange+Male.+Female.+Avg.Age+NewTier+NewSubs+Avg.Income, data=train.sample)

Coefficients: (39 not defined because of singularities)
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)      -2.022e+00  7.314e-01  -2.765 0.005911 ** 
Qtr2015 Q4        3.252e-02  1.602e-02   2.030 0.042898 *  
Qtr2016 Q1        1.529e-02  1.721e-02   0.888 0.374835    
Qtr2016 Q2        1.265e-02  2.170e-02   0.583 0.560224    
DPEarly Fringe    1.724e-02  1.842e-02   0.936 0.349910    
DPEarly Morning  -2.050e-03  3.006e-02  -0.068 0.945656    
DPLate Fringe     4.026e-02  2.070e-02   1.946 0.052270 .  
DPOvernight      -2.201e-02  2.603e-02  -0.846 0.398193    
DPPrimeTime       8.661e-02  1.909e-02   4.538 7.14e-06 ***
DPWeekend         4.809e-02  2.040e-02   2.358 0.018760 *  
Subscribers       1.638e-05  6.596e-06   2.483 0.013360 *  
TierTier2        -1.314e-01  1.155e-01  -1.138 0.255800    
TierTier3         6.884e-01  3.691e-01   1.865 0.062803 .  
SubRange110-120K -3.832e-01  1.126e-01  -3.402 0.000722 ***
SubRange30-40K    8.673e-01  2.440e-01   3.555 0.000414 ***
SubRange40-50K    1.229e-01  8.418e-02   1.460 0.144792    
SubRange50-60K           NA         NA      NA       NA    
SubRange60-70K    7.460e-01  2.604e-01   2.865 0.004347 ** 
SubRange70-80K   -9.732e-02  1.917e-01  -0.508 0.611830    
SubRange80-90K    2.315e-01  1.211e-01   1.912 0.056398 .  
SubRange90-100K   8.464e-02  6.387e-02   1.325 0.185729    
Male.0.24        -2.775e-01  6.075e-02  -4.567 6.25e-06 ***
Male.0.25        -5.249e-02  4.436e-02  -1.183 0.237277    
Male.0.29        -7.252e-02  6.412e-02  -1.131 0.258537    
Male.0.31         1.320e-01  5.029e-02   2.624 0.008956 ** 
Male.0.33         7.011e-02  1.235e-01   0.568 0.570611    
Male.0.34        -1.765e-01  6.584e-02  -2.682 0.007572 ** 
Male.0.35         1.708e-02  7.568e-02   0.226 0.821518    
Male.0.36        -9.714e-02  9.060e-02  -1.072 0.284170    
Male.0.37        -2.328e-01  8.924e-02  -2.609 0.009369 ** 
Male.0.39                NA         NA      NA       NA    
Male.0.4         -9.214e-02  8.566e-02  -1.076 0.282642    
Male.0.41        -1.629e-01  6.081e-02  -2.679 0.007631 ** 
Male.0.42        -2.339e-01  5.697e-02  -4.106 4.71e-05 ***
Male.0.44        -2.548e-01  6.245e-02  -4.079 5.26e-05 ***
Male.0.45        -9.951e-02  8.220e-02  -1.211 0.226632    
Male.0.46        -7.510e-02  1.079e-01  -0.696 0.486864    
Male.0.47        -4.412e-02  5.739e-02  -0.769 0.442357    
Male.0.48        -2.330e-01  6.124e-02  -3.805 0.000159 ***
Male.0.5         -2.787e-01  7.532e-02  -3.700 0.000240 ***
Male.0.51        -2.241e-01  6.402e-02  -3.501 0.000506 ***
Male.0.52        -1.690e-01  5.705e-02  -2.962 0.003203 ** 
Male.0.53        -1.254e-01  8.424e-02  -1.489 0.137125    
Male.0.55        -2.398e-01  5.648e-02  -4.246 2.60e-05 ***
Male.0.56        -3.289e-01  9.453e-02  -3.479 0.000548 ***
Male.0.57        -1.017e-01  8.750e-02  -1.162 0.245633    
Male.0.58        -3.471e-01  8.574e-02  -4.048 6.00e-05 ***
Male.0.59        -3.855e-01  1.067e-01  -3.612 0.000335 ***
Male.0.6          6.813e-01  1.378e-01   4.943 1.06e-06 ***
Male.0.62        -2.782e-01  7.264e-02  -3.830 0.000144 ***
Male.0.63         4.929e-01  1.042e-01   4.730 2.94e-06 ***
Male.0.64                NA         NA      NA       NA    
Male.0.65         4.818e-01  8.043e-02   5.990 4.04e-09 ***
Male.0.66        -4.060e-01  1.459e-01  -2.783 0.005591 ** 
Male.0.71        -2.394e-03  1.239e-01  -0.019 0.984588    
Male.0.76        -1.261e-01  9.176e-02  -1.374 0.169912    
Male.n/a         -1.728e-01  6.243e-02  -2.768 0.005846 ** 
Female.0.29              NA         NA      NA       NA    
Female.0.34              NA         NA      NA       NA    
Female.0.35              NA         NA      NA       NA    
Female.0.36              NA         NA      NA       NA    
Female.0.37              NA         NA      NA       NA    
Female.0.38              NA         NA      NA       NA    
Female.0.4               NA         NA      NA       NA    
Female.0.41              NA         NA      NA       NA    
Female.0.42              NA         NA      NA       NA    
Female.0.43              NA         NA      NA       NA    
Female.0.44              NA         NA      NA       NA    
Female.0.45              NA         NA      NA       NA    
Female.0.47              NA         NA      NA       NA    
Female.0.48              NA         NA      NA       NA    
Female.0.49              NA         NA      NA       NA    
Female.0.5               NA         NA      NA       NA    
Female.0.52              NA         NA      NA       NA    
Female.0.53              NA         NA      NA       NA    
Female.0.54              NA         NA      NA       NA    
Female.0.55              NA         NA      NA       NA    
Female.0.56              NA         NA      NA       NA    
Female.0.58              NA         NA      NA       NA    
Female.0.59              NA         NA      NA       NA    
Female.0.6               NA         NA      NA       NA    
Female.0.61              NA         NA      NA       NA    
Female.0.63              NA         NA      NA       NA    
Female.0.65              NA         NA      NA       NA    
Female.0.66              NA         NA      NA       NA    
Female.0.67              NA         NA      NA       NA    
Female.0.69              NA         NA      NA       NA    
Female.0.71              NA         NA      NA       NA    
Female.0.75              NA         NA      NA       NA    
Female.0.76              NA         NA      NA       NA    
Female.0.77              NA         NA      NA       NA    
Female.n/a               NA         NA      NA       NA    
Avg.Age          -3.740e-03  1.244e-03  -3.006 0.002785 ** 
NewTierTier2      2.158e-01  1.048e-01   2.059 0.040012 *  
NewTierTier3      4.357e-01  1.294e-01   3.367 0.000819 ***
NewTierTier4             NA         NA      NA       NA    
NewSubs           1.056e-05  2.873e-06   3.677 0.000262 ***
Avg.Income       -3.272e-06  1.437e-06  -2.277 0.023193 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I am confused on why Female. and Male. are both different when I have copied and pasted the code and only changed the csv I am using for them. Any help would be appreciated. Is there a way to keep it at one variable? Or are the multiple levels necessary?</p>
"
"0.0806196982594614","0.076825726438694","229336","<p>I have a data set looking into whether a farm experienced a livestock disease or not in the year 2011 and 2012  and if several factors could be predictors for the livestock disease.</p>

<p>The independent variables were also collected for both years though some variables did not change e.g Thistles remained the same for both years.</p>

<p>I am looking for an appropriate method that will allow statistical comparison between the two years rather than treating analysis as two separate sets of analyses (i.e not to treating 2011 and 2012 as two separate data set)</p>

<p>Whilst trying to do the analysis I have created dependent variable as farm having the disease or not between year 2011 and 2012(Orf.Yes.No2011.2012)against the dependent variables using logistic regression:</p>

<p>I'm just wondering whether I doing the right thing or what could be the best statistical approach which will allow for statistical comparison between the two years? Any help will be very much appreciated</p>

<pre><code>Here is the R output and sample of dataset:





 &gt; mod=glm(Orf.Yes.No2011.2012~F2011+ F2012+as.factor(Breed)+ 
                                  D2011+D2012,family=binomial, data=orf)
      summary(mod)

    Call:
    glm(formula = Orf.Yes.No2011.2012 ~ F2011 + F2012 + as.factor(Breed) + 
        D2011 + D2012, family = binomial, data = orf)

    Deviance Residuals: 
       Min      1Q  Median      3Q     Max  
    -1.862  -1.293   1.023   1.065   1.318  

    Coefficients:
                        Estimate Std. Error z value Pr(&gt;|z|)  
    (Intercept)        0.3917290  0.1626769   2.408    0.016 *
    F2011              0.0003269  0.0002782   1.175    0.240  
    F2012             -0.0003596  0.0002786  -1.291    0.197  
    as.factor(Breed)2  0.0558285  0.1489246   0.375    0.708  
    D2011             -0.0311978  0.0272068  -1.147    0.252  
    D2012              0.0226963  0.0274981   0.825    0.409  
    Small sample data set:

    F2011   F2012   Breed   Orf.Yes.No2011  Orf.Yes.No2012  Orf.Yes.No2011.2012
    155     150     1       0               0               0
    740     760     2       0               1               1
    1000    850     1       0               0               0
    1630    1520    1       1               1               1
    0       460     1       0               0               0
    1300    1335    1       0               1               1
    450     450     1       0               0               0
    390     730     1       1               0               1
    390     380     2       0               0               0
    600     600     2       0               0               0
</code></pre>
"
"0.118840444971369","0.119909435959664","229477","<p><br>I am struggling to interpret the results of a binomial logistic regression I did.<br> The experiment has 4 conditions, in each condition all participants receive different version of treatment. <br>DVs (1 per condition)=DE01,DE02,DE03,DE04, <br>all binary (1 - participants take a spec. decision, 0 - don't)
<br>Predictors: FTFinal (continuous, a freedom threat scale)
<br>SRFinal (continuous, situational reactance scale)
<br>TRFinal (continuous, trait reactance scale)
<br>SVO_Type(binary, egoists=1, altruists=0)
<br>After running these binomial (logit) models,<br><br> <code>model_soc_inf&lt;- glm(mydata$DE01~FTFinal+SRFinal+TRFinal+SVO_Type,
                    family=binomial(link='logit'),data=mydata)
model_soc_inf1&lt;- glm(mydata$DE01~FTFinal+SRFinal+TRFinal,
                     family=binomial(link='logit'),data=mydata)
summary(model_soc_inf)
model_pers_inf &lt;- glm(mydata$DE02~FTFinal+SRFinal+TRFinal+SVO_Type,
                      family=binomial(link='logit'),data=mydata)
model_pers_inf1 &lt;- glm(mydata$DE02~FTFinal+SRFinal+TRFinal,
                       family=binomial(link='logit'),data=mydata)
model_pers_inf2 &lt;- glm(mydata$DE02~SRFinal+TRFinal+SVO_Type,
                       family=binomial(link='logit'),data=mydata)
summary(model_pers_inf)
model_soc_uninf&lt;-glm(mydata$DE03~FTFinal+SRFinal+TRFinal+SVO_Type,
                     family=binomial(link='logit'),data=mydata)
model_soc_uninf1&lt;-glm(mydata$DE03~FTFinal+SRFinal+TRFinal,
                      family=binomial(link='logit'),data=mydata)
summary(model_soc_uninf)
model_pers_uninf&lt;-glm(mydata$DE04~FTFinal+SRFinal+TRFinal+SVO_Type,
                      family=binomial(link='logit'),data=mydata)
model_pers_uninf1&lt;-glm(mydata$DE04~FTFinal+SRFinal+TRFinal,
                       family=binomial(link='logit'),data=mydata)
summary(model_pers_uninf)</code><br><br>I ended up with the following<a href=""http://i.stack.imgur.com/4JKLa.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4JKLa.png"" alt=""enter image description here""></a>. Initially I tested 2 models per condition, when condition 2 (DE02 as a DV) got my attention. In model(3)There are two variables, which are significant predictors of DE02 (taking a decision or not) - FTFinal and SVO Type. In context, the values for model (3) would mean that all else equal, being an Egoist (SVO_Type 1) decreases the (log)likelihood of taking a decision in comparison to being an altruist. Also, higher scores on FTFinal(freedom threat) increase the likelihood of taking the decision. So far so good. Removing SVO_Type from the regression (model 4) made the FTFinal coefficient non-significant. Removing FTFinal from the model does not change the significance of SVO_Type.</p>

<p>So I figured:ok, mediaiton, perhaps, or moderation. I tried first to look for mediation in both in R and SPSS. The moderation attempt was in vain: entering an interaction term SVO_Type:FTFinal makes all variables in model(3) non-significant.Here's the code for that:<code>model1&lt;-glm(DE02~FTFinal,family=binomial(link='logit'),data=mydata)
summary(model1)
model2&lt;-glm(DE02~SVO_Type,family=binomial(link='logit'),data=mydata)
summary(model2)
model3&lt;-glm(DE02~FTFinal+SVO_Type,family=binomial(link='logit'),data=mydata)
summary(model3)
interaction&lt;-glm(DE02~SVO_Type+FTFinal+SVO_Type:FTFinal, family =binomial(
  link = ""logit""),data = mydata)</code> <br>As for mediation, I followed  <a href=""http://www.nrhpsych.com/mediation/logmed.html"" rel=""nofollow"">this</a> mediation procedure for logistic regression, but found no mediation. </p>

<p>To sum up:
There is some relationship between SVO_Type and FTFinal, but I have no clue what.
Predicting DE02 from SVO_Type only is not significant.
Predicting DE02 from FTFinal is not significant
Putitng those two in the regression makes them both significant predictors.
Including an interaction between these both in any model, predicting DE02 model makes all variables in the model insignificant.<br>
So I am at a total loss: As far as I know, to test moderation, you need an interaction term. This term is between a categorical var (SVO_Type) and the continuous one(FTFinal), perhaps that goes wrong? And to test mediation outside SPSS, I tried the ""mediate"" package in R, only to discover that there is a ""treatment"" argument in the main funciton, which is to be the treatment variable (exp Vs cntrl). I don't have such, all ppns are subjected to different versions of the same treatment. 
I apologize for <a href=""http://www.filedropper.com/mydata"" rel=""nofollow"">this external way of uploading the dataset</a>, it is way too complicated to reproduce here (I am a noob).
Any help would be greatly appreciated. I have no clue what the relationship between SVO_Final and FTFinal is.</p>
"
"NaN","NaN","229542","<p>Im doing a multiple imputation of a dataset using R's MICE package. </p>

<pre><code>imp &lt;- mice(nhanes, m=5, print = FALSE, seed = 55152)
</code></pre>

<p>I figured out that to pool regression coefficients you really only need to get the mean of the 5 regression coefficients for the 5 datasets. </p>

<p>But now i need to pool means, confidence intervals and standard deviation using Rubin's rules. </p>

<p>How do i do that? </p>

<p>/Kind regards</p>
"
"0.035185320931284","0.0502942438178979","229709","<p>I have several slightly related variables measured in two instruments on the same sample at different time points. I'm trying to know how well the differences between the two instruments can be explained by other variables and the time point.</p>

<p>1.- I'd like to know if my method is adequate: I have chosen to perform a multivariate regression (with lm or glm) that includes a ""day"" as a polinomial variable and the average of both instruments (ex. AVGvar2) and differences between them (ex. DIFvar2) as linear variables. And I perform a StepAIC on the fit. All this for each variable (so that I eventually can say <strong>what variables significantly influde in observed differences</strong> for each variable).</p>

<pre><code>fit &lt;- glm(var1dif~var1+AVGvar2+DIFvar2+AVGvar3+DIFvar3+AVGvar4+DIFvar4+AVGvar5+DIFvar5+day+I(day^2)+I(day^3)+I(day^4)+I(day^5)+I(day^5); step1 &lt;-stepAIC(fit,direction=""both"")
fit2 &lt;- lm(var1dif~var1+AVGvar2+DIFvar2+AVGvar3+DIFvar3+AVGvar4+DIFvar4+AVGvar5+DIFvar5+poly(day,6); step2 &lt;-stepAIC(fit2,direction=""both"")
</code></pre>

<p>(the same would go for DIFvar2, DIFvar3, DIFvar4 and DIFvar5)
Any correction, advice or further step?</p>

<p>2.- When I compare summary(step) and summary(step2) the output (estimates, std error, coefficients) is the same for the variables. The results for variable day differ when the linear model has orthogonal or raw polinomials. Which one is better for my regression?</p>

<p>NOTE: I understand stepwise is frowned upon, but I think for this retrospective analysis it is decent and cost-effective enough.</p>

<p>Thank you.</p>
"
"0.0710998907892006","0.0677539221495548","230022","<p>First off, here is my calculation for the confidence intervals:</p>

<pre><code>BPLpredictions[""upr""] = BPLpredictions$fit + (z * BPLpredictions$se.fit)
BPLpredictions[""lwr""] = BPLpredictions$fit - (z * BPLpredictions$se.fit)
</code></pre>

<p>After calculating the running total for all of my predictions, I end up with the following data frame:</p>

<pre><code>&gt; BPLSeason
        upr       lwr       fit Running fit Running lwr Running upr
1  1.046068 0.6191719 0.8326201   0.8326201   0.6191719    1.046068
2  1.066816 0.6655935 0.8662049   1.6988250   1.2847654    2.112885
3  1.088620 0.7136692 0.9011444   2.5999694   1.9984346    3.201504
4  1.167051 0.8622466 1.0146486   3.6146180   2.8606811    4.368555
5  1.112002 0.7629848 0.9374932   4.5521112   3.6236659    5.480557
6  1.112002 0.7629848 0.9374932   5.4896044   4.3866506    6.592558
7  1.088620 0.7136692 0.9011444   6.3907488   5.1003198    7.681178
8  1.242889 0.9534183 1.0981538   7.4889026   6.0537381    8.924067
9  1.201472 0.9096795 1.0555757   8.5444783   6.9634176   10.125539
10 1.201472 0.9096795 1.0555757   9.6000541   7.8730970   11.327011
11 1.292990 0.9919085 1.1424492  10.7425033   8.8650055   12.620001
12 1.201472 0.9096795 1.0555757  11.7980790   9.7746850   13.821473
13 1.242889 0.9534183 1.0981538  12.8962328  10.7281032   15.064362
14 1.242889 0.9534183 1.0981538  13.9943865  11.6815215   16.307252
15 1.292990 0.9919085 1.1424492  15.1368357  12.6734300   17.600241
16 1.292990 0.9919085 1.1424492  16.2792849  13.6653384   18.893231
17 1.352706 1.0243564 1.1885314  17.4678163  14.6896948   20.245938
18 1.292990 0.9919085 1.1424492  18.6102655  15.6816033   21.538928
19 1.352706 1.0243564 1.1885314  19.7987969  16.7059597   22.891634
20 1.422037 1.0509079 1.2364723  21.0352693  17.7568676   24.313671
</code></pre>

<p>So you understand the context of this data frame, fit is the predicted number of goals in a specific game and upr and lwr and the upper and lower bounds of the 95% CI. </p>

<p>The three additional columns are essentially the running total for each. This may be a dumb question but is it ok for me to calculate the running total for the upr and lwr bounds or should I, for example, use <code>upr[20] - fit[20]</code> and add that to <code>running fit[20]</code></p>

<p>Sorry if this is trivial. I'm brand new to R and Stats. </p>

<p>[Additional Information]</p>

<p>The following negative binomial regression model was used:</p>

<pre><code>mod 1 = glm.nb(Goals ~ Defense, data = Messi.Liga)
</code></pre>

<p>Here is the summary of the model:</p>

<pre><code>Coefficients:
 Estimate Std. Error z value Pr(&gt;|z|) 
(Intercept) 3.05943 1.11817 2.736 0.00622 **
Defense -0.03954 0.01498 -2.639 0.00831 **
</code></pre>

<p>This command was used to predict BPL values:</p>

<pre><code>BPLpredictions = data.frame(predict(mod1, bpl.df, type = ""response"", se.fit = TRUE))
</code></pre>

<p>And finally, this is how I calculated the CI (z = 1.96):</p>

<pre><code>BPLpredictions[""upr""] = BPLpredictions$fit + (z * BPLpredictions$se.fit)
BPLpredictions[""lwr""] = BPLpredictions$fit - (z * BPLpredictions$se.fit)
</code></pre>
"
"0.091874672876503","0.0963061447907242","230201","<p>I'm using the glmnet package in R to do ridge regression. When I have a full set of dummy variables (if you took a horizontal sum of all these dummy variables you would get the constant), ridge regression with lambda = 0 is NOT dropping any of the dummy variables. In contrast, OLS gives the expected result by dropping at least 1 of the dummies to prevent perfect multi-collinearity. I'd like to know why the discrepancy exists. </p>

<pre><code> library(glmnet)
 set.seed(1)
make_dummies_out_of_factors&lt;- function(your_df, names_of_factor_variables) {
  indices&lt;- which(names(your_df) %in% names_of_factor_variables) #Finds columns corresponding to factor variables
  model_matrices_list&lt;- lapply(indices, function(x) {
    model.matrix(~your_df[,x] - 1, your_df)
  })
  #create a model matrix for each factor variable, and stores each one as a list
  model_matrices_together&lt;- do.call(cbind, model_matrices_list)
  #Column bind all model matrices which are stored as lists
  final&lt;- cbind(your_df, model_matrices_together)
  #Column bind all the model matrices to the original data
  final&lt;- final[,-indices]
  #Get rid of the original factor variables

  names(final)&lt;- gsub(""your_df.*\\]"", ""dummy_"", names(final))
  #Give appropriate names to the dummies

  return(final)
}
test_df&lt;- data.frame(numeric1 = rnorm(1000), numeric2 = rnorm(1000), 
                     state = rep(letters[1:4], 250), year = rep(c(""yr1"", ""yr2""), 500)) #This data frame has 2 factor variables
test_df&lt;- make_dummies_out_of_factors(test_df, names_of_factor_variables = c(""state"", ""year""))

linear_alldum&lt;- lm(test_df$numeric2 ~ test_df$numeric1 + test_df$dummy_yr1 + test_df$dummy_yr2 + test_df$dummy_a + 
                     test_df$dummy_b + test_df$dummy_c + test_df$dummy_d)


X_test&lt;- as.matrix(test_df[,-1]) #Remove dependent variable out of X matrix
y_test&lt;- test_df[,1] #This is the dependent variable

ridge_alldum&lt;- glmnet(x = X_test, y = y_test, lambda = seq(200, 0, by = -1), alpha = 0)


comparison = data.frame(as.matrix(coef(ridge_alldum))[,201], coefficients(linear_alldum))
names(comparison)[1]&lt;- ""coefficients_ridge_l0""
names(comparison)[2]&lt;- ""coefficients_linear_reg""
#Note that coefficients aren't identical, and that ridge regression doesn't drop coefficients. 

prediction_linear&lt;- predict(linear_alldum)
prediction_ridge&lt;- predict(ridge_alldum, newx = X_test, s = 0)
predictions&lt;- data.frame(prediction_linear, prediction_ridge = prediction_ridge)
names(predictions)[2]&lt;- ""prediction_ridge""

#Note that the predictions using linear regression and ridge regression aren't the same. 

sapply(predictions, mean) #Means of predictions using linear and ridge.
sapply(predictions, sd) #SDs of predictions using linear and ridge. 
</code></pre>
"
"0.0304713817668003","0.029037395206952","230372","<p>I was wondering if someone on here could help</p>

<p>I recently ran a Spatial Durbin Regression model in R which came back with two of my three independent variables had significant beta coefficients. A colleague then advised me that I should run a sensitivity test using a negative binomial model to see if I get the same results. However the results are different as all my beta coefficients become significant.</p>

<p>What I am trying to understand is would this likely be due to the incorporation of the spatially lagged independent variable (which has a significant rho value in the spatial Durbin regression)? And does it make sense to use the negative binomial as a 'sensitivity test' as I would think the assumptions would be different, particularly around spatial autocorrelation. </p>
"
"0.0861860827177143","0.0821301562353182","230532","<p>Trying to get the Bayes Factor for a correlation between two variables in my data, I tried three different functions. All implement the Jeffreysâ€“Zellnerâ€“Siow (JZS) prior, but I get quite different results with the three approaches. Two questions:</p>

<ol>
<li><p>Is this suspicious, or is it reasonable that they produce different values, as the implementations are slightly different?</p></li>
<li><p>Is there a consensus on the best measure to use?  </p></li>
</ol>

<p>My data:</p>

<pre><code>a=rnorm(100,1,2)
b=rnorm(100,.8,1.5)
myData &lt;- data.frame(a=a, b=b)
</code></pre>

<p>I try the <code>jzs_corbf</code> function, described and implemented <a href=""http://www.ncbi.nlm.nih.gov/pubmed/22798023"" rel=""nofollow"">here</a> (<a href=""http://dsquintana.com/post/98962697485/how-to-calculate-a-bayes-factor-for-correlations"" rel=""nofollow"">shorter version</a>)</p>

<pre><code>cor.resu.a_b &lt;- cor.test(myData$a, myData$b, method=c(""pearson""))
cor.resu.a_b$estimate
n = 100
r = cor.resu.a_b$estimate
jzs_corbf(r,n)
[1] 0.08206358
</code></pre>

<p>I also tried the convenience function from the <code>BayesFactor</code> package:</p>

<pre><code>require(BayesFactor)
regressionBF(b ~ a, data = myData, progress=FALSE)

Bayes factor analysis
--------------
[1] a : 0.2181081 Â±0%

Against denominator:
  Intercept only 
---
Bayes factor type: BFlinearModel, JZS
</code></pre>

<p>And I also tried the a function described <a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4891395/"" rel=""nofollow"">recently</a> (<a href=""https://osf.io/9d4ip/"" rel=""nofollow"">code</a>)</p>

<pre><code>bf10JeffreysIntegrate(n=100, r=r)

      cor 
0.1297927
</code></pre>

<p>While in this case the differences are only numerical, in my real data I get quite big differences that make it more difficult to decide on an interpretation. </p>

<p><a href=""http://stats.stackexchange.com/questions/184950/calculating-bayes-factor-from-a-correlation-coefficient"">Related</a></p>
"
"0.0691025985081098","0.076825726438694","230581","<p>My problem is the following, my data has a lot of branch off points and the tree grows very rapidly. The end result is not readable, the end nodes are overlapped and even conversion to rules is more or less useless. 
I am using the rpart package. </p>

<pre><code>#Scoring model
d = sort(sample(nrow(Memmbers),nrow(Memmbers)* .6))
#select training sample
train&lt;-Memmbers[d, ]
test&lt;-Memmbers[-d, ]


s&lt;-glm(verifikation ~ . - userId,data = Memmbers,family = binomial())
summary(s)

library(ROCR)

#score test data set 
test$score &lt;- predict(s,type='response',test)
pred&lt;-prediction(test$score,test$verifikation)
perf&lt;- performance(pred,""tpr"",""fpr"")
plot(perf)

max(attr(perf,'y.values')[[1]]-attr(perf,'x.values')[[1]])

#get results of terms in regression 
g&lt;-predict(s,type='terms',test)
#function to pick top 3 reasons
#works by sorting coefficient terms in equation
# and selecting top 3 in sort for each loan scored 
ftopk&lt;- function(x,top=3){
  res=names(x)[order(x, decreasing = TRUE)][1:top]
  paste(res,collapse="";"",sep="""")
}
# Application of the function using the top 3 rows
topk=apply(g,1,ftopk,top=3)
#add reason list to scored tets sample
test&lt;-cbind(test, topk)

library(rpart)
library(rattle)

fit1 &lt;- rpart(verifikation ~ . - userId, data = train)
fancyRpartPlot(fit1);
test$t&lt;-predict(fit1,type='class',test)

################## PLot tree with priors 
#score test data 
test$score1 &lt;- predict(fit2,type = 'prob',test)
pred5&lt;-prediction(test$score1[,2],test$verifikation)
perf5&lt;- performance(pred5,""tpr"",""fpr"")

#90-10 priors with smaller complexity parameter to allow more complex trees
fit2 &lt;- rpart(verifikation ~ . - userId , data = train,method = ""class"",parms = list(prior=c(.9,.1)),cp=.0002)
plot(fit2);text(fit2,pos=2,cex=0.1,col=""blue"");

#compare complexity
printcp(fit1)
printcp(fit2)
plotcp(fit2)

#convert trees to rules 
amess&lt;-asRules(fit2)
t.b&lt;-rpart.rules.table(fit2)
library(rattle)
library(rpart.plot)
library(RColorBrewer)

fancyRpartPlot(fit2)
</code></pre>

<p>And here is the output of fancyRpartPlot(fit2) </p>

<p><a href=""http://i.stack.imgur.com/otsP3.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/otsP3.png"" alt=""enter image description here""></a></p>

<p>My goal is to extract some useful rules from the entire process to implement in a score card. </p>
"
"0.118246329960506","0.112681644735122","230709","<p>I'm working on a dataset with around 7000 points, a binary response variable and (at minimum) five predictors, two of which are binary, one is a number, and two are factors. One of those factors (""speaker"") is the participant id, with 26 levels once participants with categorical behaviour are excluded. The other (""vowel"") is a variable with six levels with represents the main condition I'm interested in. I (mostly) have a minimum of 20 tests per participant-condition combination. What I'm specifically trying to investigate is whether the effect of the different conditions differs for different participants. For that reason, I've tried to fit a binomial regression with an interaction between speaker and vowel. Here's the call:</p>

<pre><code>glm_vowel_interactions&lt;-glm(formula = perceptually.rhotic ~ vowel * speaker + function_word + modified_clip_start + prepausal, family = ""binomial"", data =data_excluding_rare_vowels_and_categorical_speakers)
</code></pre>

<p>This appears to work. However, it produces some alarmingly large (> 15 and &lt; -15) coefficient estimates. I ran it with the safeBinaryRegession package, and sure enough, it reports that separation is occurring with 33 points - 30 specific vowel/speaker (participant) combinations (of the 162 total combinations) and two specific values of ""speaker"" (which I think represent separation in the combination of those participants and the reference level for ""vowel""):</p>

<pre><code>Error in glm(formula = perceptually.rhotic ~ vowel * speaker + function_word +  : 
  The following terms are causing separation among the sample points: speaker4, speaker7, vowelNURSE:speaker19, vowellettER:speaker21, vowelNURSE:speaker21, vowelNORTH~FORCE:speaker22, vowelNEAR:speaker24, vowelNURSE:speaker27, vowellettER:speaker28, vowelSTART:speaker28, vowellettER:speaker4, vowelNEAR:speaker4, vowelNORTH~FORCE:speaker4, vowelNURSE:speaker4, vowelSTART:speaker4, vowelNEAR:speaker6, vowelNORTH~FORCE:speaker6, vowelSTART:speaker6, vowellettER:speaker7, vowelNEAR:speaker7, vowelNORTH~FORCE:speaker7, vowelNURSE:speaker7, vowelSTART:speaker7, vowelSTART:speakerb3, vowelNEAR:speakerb5, vowelNURSE:speakerb5, vowelSTART:speakerb5, vowelNEAR:speakerb6, vowelNEAR:speakerb7, vowelSTART:speakerb7, vowelNEAR:speakerb8, vowelNURSE:speakerb8, vowelSTART:speakerb8
</code></pre>

<p>Looking at my data I find that many of these are indeed categorical cells, with participants producing 100% or 0% TRUE of the response variable in that condition. As there are participants who overall produce mostly TRUE (up to 96%) or mostly FALSE (as low as 5%), it's unsurprising that there should be some categorical cells and doesn't necessarily imply that there's anything drastically unusual going on in these cells.</p>

<p>So, how can I investigate the differing effect of this condition (""vowel"") on the response variable for the different participants, while taking into account my other independent variables? I assume I can't interpret the results of the regression that did converge without safeBinaryRegression, as the coefficients and p-values are likely to be inflated? I came across <a href=""http://www.carlislerainey.com/papers/separation.pdf"" rel=""nofollow"">this paper</a> which suggests adding a prior to limit the largest possible coefficient estimates, so that instances with separation aren't infinite - would this be a sensible approach here? If so, is it implemented in R? What packages are recommended? Alternatively, should I be using some other (non regression?) method to investigate this question?</p>

<p>Thanks very much for your help!</p>

<p>Full disclosure: I've cross-posted this on the Maths stackexchange, but my experience is that here is more active!</p>
"
"0.0215465206794286","0.0410650781176591","230797","<p>I am new to R coding and was hoping someone could help. Am trying to create a regression line where the dependent variable is a proportion (I only have the proportion, not the denominator and numerator). With it being a proportion a linear regression line isn't appropriate as it needs to sit between 0-1, I think a sigmoid shape would be best. So far I have had limited success with the Loess function, however ideally I want to be able to gain the coefficients and AIC from the regression. So far the best shape I have been able to obtain is using the binomial family in ggplot2, but I don't think this is an appropriate distribution.</p>

<p>I have attached my code, am wondering if anyone could suggest an improvement.</p>

<pre><code> c &lt;- ggplot(dat, aes(y=ITN_Coverage, x=Study_Date))
c + stat_smooth(method =""glm"",  method.args = list(family=""binomial""), size=0.5, col = ""black"") + geom_point(aes(color = Country)) + 
labs(title = ""Scatter plot: Insecticide treated net coverage against year"", x= ""Study date"", y= ""ITN coverage"")
</code></pre>
"
"0.0430930413588572","0.0410650781176591","230851","<p>When we specify the â€œfamily=â€ argument inside glm() in R, how is the distribution being used to regress between the dependent and independent.?</p>

<p>In simple linear regression( lm() in R ) ,we simply calculate the mean of Y for each X and that becomes the predicted value. How different is this when we mention a family? </p>

<p>I do know that each distribution has few paramters that describes it ( mean, shape, scale etc). So how are they used to get predictions?</p>

<p>Bascially I would like to learn what does it mean to fit a distribution to a data.</p>

<p>Edit : to clarify the question</p>

<p>Lets say I trained my model on a dataset of 1lac obs with 2 independent variables and the coefficients are 1,2. i.e. beta1 = 1 and beta2 = 2</p>

<pre><code>          Y    x1   x2
1st obs.  2    .5    1
2nd obs.  1    .25  .25
</code></pre>

<p>So if I choose Poisson distribution, then</p>

<pre><code>1st obs. mean(mu) = exp(.5*1+1*2) = 12.18
</code></pre>

<p>similarily we get a mean for the second obs and so on. 
Now how is this mean related to what you gonna predict? I am not able to connect this. </p>

<p>Another major concern is also how the shape/scale i.e.(sigma, nu, tau for gamlss) being modeled. However they are secondary and for now wanna focus on glm My questions may be stupid but even any resources/links will be helpful and I shall sincerely read them </p>
"
"0.102279800236246","0.112461348053768","231059","<p>So first of all I did some research on this forum, and I know <a href=""http://stats.stackexchange.com/questions/140991/comparing-difference-between-two-polynomial-regression-models-in-r"">extremely similar</a>  questions have been asked but they usually haven't been answered properly or sometimes the answer are simply not detailed enough for me to understand. So this time my question is : I have two sets of data, on each, I do a polynomial regression like so :</p>

<pre><code>Ratio&lt;-(mydata2[,c(2)])
Time_in_days&lt;-(mydata2[,c(1)])
fit3IRC &lt;- lm( Ratio~(poly(Time_in_days,2)) )
</code></pre>

<p>The polynomial regressions plots are:</p>

<p><a href=""http://i.stack.imgur.com/T7r3i.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/T7r3i.png"" alt=""enter image description here""></a></p>

<p>The coefficients are :</p>

<pre><code>&gt; as.vector(coef(fit3CN))
[1] -0.9751726 -4.0876782  0.6860041
&gt; as.vector(coef(fit3IRC))
[1] -1.1446297 -5.4449486  0.5883757 
</code></pre>

<p>And now I want to know, if there is a way to use an R function to do a test that would tell me whether or not there is a statistical significance in the difference between the two polynomials regression knowing that the relevant interval of days is [1,100].</p>

<p>From what I understood I can not apply directly the anova test because the values come from two different sets of data nor the AIC, which is used to compare model/true data.</p>

<p>I tried to follow the instructions given by @Roland in the related question but I probably misunderstood something when looking at my results :</p>

<p>Here is what I did : </p>

<p>I combined both my datasets into one.</p>

<p><code>f</code> is the variable factor that @Roland talked about. I put 1s for the first set and 0s for the other one.</p>

<pre><code>y&lt;-(mydata2[,c(2)])
x&lt;-(mydata2[,c(1)])
f&lt;-(mydata2[,c(3)])

plot(x,y, xlim=c(1,nrow(mydata2)),type='p')

fit3ANOVA &lt;- lm( y~(poly(x,2)) )

fit3ANOVACN &lt;- lm( y~f*(poly(x,2)) )
</code></pre>

<p>My data looks like this now :</p>

<p><a href=""http://i.stack.imgur.com/dNpMQ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/dNpMQ.png"" alt=""enter image description here""></a></p>

<p>The red one is <code>fit3ANOVA</code> which is still working but I have a problem with the blue one <code>fit3ANOVACN</code> the model has weird results. I don't know if the fit model is correct, I do not understand what @Roland meant exactly.</p>

<p>Considering @DeltaIV solution I suppose that in that case :
<a href=""http://i.stack.imgur.com/HLLp9.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/HLLp9.png"" alt=""enter image description here""></a>
The models are significantly different even though they overlap. Am I right to assume so ?</p>
"
"0.075412822378","0.0718638867059034","231252","<p>I'm doing a regression using R.Initially I used the <code>fit=lm(data)</code>.Got all of my variables are significant including intercept.I checked the VIF using <code>vif(fit)</code> &amp; got maximum VIF as 2.5. But my customer wants model without intercept and I don't have any option other than removing intercept. So I used following line of code
<code>fit=lm(A ~ B+C+D+E+F-1,data=data)</code> , I'm just coding the variables as it is client data &amp; I can't share that.The data set I used in first model is same as the data set used in second model with same set of variables.Only in second model I removed intercept forcefully.
But after running the model I'm seeing my maximum vif is coming 2079.30. I'm not able to understand the reason for such high vif as I used to think that VIF determines how much the variance of a coefficient is â€œinflatedâ€ because of linear dependence with other predictors &amp; it does not depend on intercept.
Can you expert please help me understand why VIF is drastically changed after removing intercept in R</p>
"
"0.0609427635336005","0.0580747904139041","231260","<p>This is my first post here, so please bear with me! I'm comparing several biomarkers with Kaplan-Meier curves and calculating hazard ratios for different risk groups (defined by a certain, well established cut-off value of the biomarker) by using Cox regression in R. We have 3 tiers with a low, intermediate and high risk group, however the low risk group for one biomarker contains no events.</p>

<p>This, so my understanding, leads to quasi-separation in the Cox regression and hence infinite values and large coefficients and SEs. I understand the Likelihood ratio is still valid, but what I'm obviously interested in is a calculation of the HR from exp(coef). A sample of the data is displayed below:</p>

<pre><code>   &gt; head(riskgroups)
   ID FU_3y_death FU_3y_death_days biomarker bm.riskcat
   1           0             1095           58.2    group.3
   2           1               79           11.5    group.2
   22           0             1095           11.7    group.2
   27           0              929            9.0    group.2
   44           0              949            7.0    group.2
   46           0             1095            7.5    group.2
</code></pre>

<p>Now I have found that using Firth's method might allow a workaround, hence I've tried to run the analysis using coxphf with the following code:</p>

<pre><code>cox.groups &lt;- coxphf(riskgroups, formula=Surv(FU_3y_death_days,FU_3y_death) ~ bm.riskcat, pl=T, firth = T)
</code></pre>

<p>Rather bizarrely, this results in the following error:</p>

<blockquote>
  <p>Error in coxphf(riskgroups, formula = Surv(FU_3y_death_days, FU_3y_death) ~  : 
    NA/NaN/Inf in foreign function call (arg 3)</p>
</blockquote>

<p>I would have assumed that this is exactly what coxphf is trying to avoid? When setting pl to FALSE (to base the tests on the Wald method instead of profile penalised LL) I get results with all NaN. Of course the fact that there is no event in the lowest risk group is in itself an important message, but I do require hazard ratios for the second and third tier of risk categories to compare the different biomarkers. Any bright thoughts on this, my research into this has hit a wall after 3 days of reading...</p>
"
"0.0746393370862076","0.0711268017165705","231777","<p>I have a data frame with 1200 observations and 30 variables and I'am trying to do a multinomial logistic regression to explain the intentions of vote of Tunisian citizens using multinom(). my dependent variable has 10 levels.
When I executed the command multinom () i got this warning 
Warning messages: 1: In sqrt(diag(vc)) : NaNs produced </p>

<p>so i reduced the number of the predictor variables to 13 , the levels of my dependent variable to only 3 and the warning message no longer appears , but once I calculate the p.value the mojority of my predictor variables are non significant.</p>

<pre><code>      &gt; str(k)
    'data.frame':   1081 obs. of  19 variables:
     $ URBRUR    : Factor w/ 2 levels ""Rural"",""Urban"": 2 2 2 2 2 2 2 2 2 2  ...
     $ REGION    : Factor w/ 24 levels ""Ariana"",""Beja"",..: 23 23 23 23 23 23 23 23 23 23 ...
     $ classe_age: Factor w/ 5 levels ""60 ans et plus"",..: 3 5 1 1 3 1 5 4 1 2 ...
       $ Q3A       : Factor w/ 5 levels ""Fairly bad"",""Fairly good"",..: 2 1 1 4 4 4 2 4 1 3 ...
       $ Q3B       : Factor w/ 5 levels ""Fairly bad"",""Fairly good"",..: 2 1 1 3 1 4 2 4 1 3 ...
       $ Q7        : Factor w/ 2 levels ""Going in the right direction"",..: 1 2 2 2 2 2 2 2 2 1 ...
       $ Q14       : Factor w/ 4 levels ""Not at all interested"",..: 4 3 3 2 3 3 3 3 3 4 ...
       $ Q27       : Factor w/ 9 levels ""Did not vote for some other reason"",..: 6 6 6 6 6 3 6 6 6 1 ...
       $ Q46A      : num  9 5 8 0 3 3 4 5 0 3 ...
       $ Q63PT1    : Factor w/ 8 levels "" Services gouvernementaux"",..: 5 5 4 4 4 4 5 4 4 5 ...
       $ Q89A      : Factor w/ 9 levels ""Non"",""Oui, autre"",..: 7 1 1 8 5 1 1 1 1 1 ...
       $ Q96       : Factor w/ 3 levels ""No (looking)"",..: 3 2 2 2 1 2 2 3 2 1 ...
       $ Q96_ARB   : Factor w/ 9 levels ""Agriculteur exploitant"",..: 2 6 4 4 1 6 7 4 6 6 ...
       $ Q97       : Factor w/ 4 levels ""Aucune Ã©ducation formelle "",..: 1 3 1 4 4 3 4 3 1 4 ...
       $ Q98B      : Factor w/ 4 levels ""Not at all important"",..: 4 4 4 4 3 4 4 4 4 4 ...
     #the logistic regression
      library(nnet)
      k$out=relevel(k$Q99,ref = ""Nahdha"")
     fit=multinom(out ~ URBRUR+ REGION +    classe_age+ Q3A +Q3B+ Q7 +  Q14+    Q27+ Q46A+  Q63PT1+ Q96+ Q96_ARB+ Q97   + Q98B,data=k,maxit=3000)

     summary(fit)
     #calculate the p.value
     z &lt;- summary(fit)$coefficients/summary(fit)$standard.errors
     p &lt;- (1 - pnorm(abs(z), 0, 1))*2
     p
</code></pre>

<p>this is a part from the output R  </p>

<pre><code>                        (Intercept) URBRUR[T.Urban] REGION[T.Beja] REGION[T.Ben        Arous]
          CPR            0.0000000       0.8006384     0.50724591           0.3490626
          Nahdha         0.6480962       0.9298628     0.09299337           0.2426325
          Nidaa Tounes   0.1547996       0.1210917     0.01340229           0.5486973
                           REGION[T.Bizerte] REGION[T.Gabes] REGION[T.Gafsa]
           CPR                  0.6667980      0.86525482      0.01971166
          Nahdha               0.2933951      0.03008731      0.05240173
          Nidaa Tounes         0.5154798      0.51222561      0.03301253
                         REGION[T.Jendouba] REGION[T.Kairouan] REGION[T.Kasserine]
          CPR                  0.21477728          0.4552543          0.53160327
         Nahdha               0.01548534          0.9322695          0.22102722
         Nidaa Tounes         0.06993081          0.7833111          0.09259959
                       REGION[T.Kebili] REGION[T.Le Kef] REGION[T.Mahdia]
           CPR                0.49607138        0.0000000        0.3084810
           Nahdha             0.09437504        0.6338189        0.1629434
           Nidaa Tounes       0.17968658        0.1360486        0.1955159
</code></pre>

<p>I'm sorry if I am asking a complicated question but I would like an explication for this issue</p>

<pre><code>   &gt; table(k$out)

    Ne pas voter       Nahdha Nidaa Tounes 
     307          292          266 
</code></pre>
"
"0.182895101599049","0.169577550590665","231872","<p>For a better understanding of how r is conducting a logistic regression I created the following test-data (the two predictors and the criterion are binary variables):</p>

<pre><code>   UV1 UV2 AV
1    1   1  1
2    1   1  1
3    1   1  1
4    1   1  1
5    1   1  1
6    1   1  1
7    1   1  1
8    0   0  1
9    0   0  1
10   0   0  1
11   1   1  0
12   1   1  0
13   1   0  0
14   1   0  0
15   1   0  0
16   1   0  0
17   1   0  0
18   0   0  0
19   0   0  0
20   0   0  0
</code></pre>

<p>AV = dependent variable/criterion</p>

<p>UV1 / UV2 = both independant variables/predictors</p>

<p>For measuring the UVs effect on the AV a logistic regression is necessary, as the AV is a binary variable. Hence i used the following code</p>

<pre><code>&gt; lrmodel &lt;- glm(AV ~ UV1 + UV2, data = lrdata, family = ""binomial"")
</code></pre>

<p>including <strong>""family = ""binomial""""</strong>. Is this correct ( I think so :-))?</p>

<p>Regarding my test-data, I was wondering about the whole model, especially
the estimators and sigificance:</p>

<pre><code>&gt; summary(lrmodel)


Call:
glm(formula = AV ~ UV1 + UV2, family = ""binomial"", data = lrdata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.7344  -0.2944   0.3544   0.7090   1.1774  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -4.065e-15  8.165e-01   0.000    1.000
UV1         -1.857e+01  2.917e+03  -0.006    0.995
UV2          1.982e+01  2.917e+03   0.007    0.995

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 27.726  on 19  degrees of freedom
Residual deviance: 17.852  on 17  degrees of freedom
AIC: 23.852

Number of Fisher Scoring iterations: 17
</code></pre>

<ol>
<li><p>Why is UV2 not significant. See therefore that for group AV = 1 there are 7 cases with UV2 = 1, and for group AV = 0 there are only 3 cases with UV2 = 1. 
I was expecting that UV2 is a significant discriminator.</p></li>
<li><p>Despite the not-significance of the UVs, the estimators are - in my opinion- very high (e.g. for UV2 = 1.982e+01). How is this possible?</p></li>
<li><p>Why isn't the intercept 0,5?? We have 5 cases with AV = 1 and 5 cases with AV = 0.</p></li>
</ol>

<p>Further: I created UV1 as a predictor I expected not to be significant:  for group AV = 1 there are 5 cases withe UV1 = 1, and for group AV = 0 there are 5 cases withe UV1 = 1 as well.</p>

<p>The whole ""picture"" I gained from the logistic is confusing me...</p>

<p>What was consuming me more:
When I run a ""NOT-logistic"" regression (by omitting <strong>""family = ""binomial""</strong>)</p>

<pre><code>&gt; lrmodel &lt;- glm(AV ~ UV1 + UV2, data = lrdata,)
</code></pre>

<p>I get the expected results</p>

<pre><code>Call:
glm(formula = AV ~ UV1 + UV2, data = lrdata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.7778  -0.1250   0.1111   0.2222   0.5000  

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)   0.5000     0.1731   2.889  0.01020 * 
UV1          -0.5000     0.2567  -1.948  0.06816 . 
UV2           0.7778     0.2365   3.289  0.00433 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for gaussian family taken to be 0.1797386)

    Null deviance: 5.0000  on 19  degrees of freedom
Residual deviance: 3.0556  on 17  degrees of freedom
AIC: 27.182

Number of Fisher Scoring iterations: 2
</code></pre>

<ol>
<li>UV1 is not significant! :-)</li>
<li>UV2 has an positive effect on AV = 1! :-)</li>
<li>The intercept is 0.5! :-)</li>
</ol>

<p>My overall question: Why isn't logistic regression (including ""family = ""binomial"") producing results as expected, but a ""NOT-logistic"" regression (not including ""family = ""binomial"") does?</p>

<p>Update:
are the observations described above because of the correlation of UV1 and UV 2. Corr = 0.56
After manipulating the UV2's data </p>

<p>AV: 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0</p>

<p>UV1: 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0</p>

<p>UV2: <strong>0, 0, 0,</strong> 1, 1, 1, 1, <strong>1, 1, 1</strong>, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0</p>

<p>(I changed the positions of the three 0s with the three 1s in UV2 to gain a correlation &lt; 0.1 between UV1 and UV2) hence:</p>

<pre><code>UV1 UV2 AV
1    1   0  1
2    1   0  1
3    1   0  1
4    1   1  1
5    1   1  1
6    1   1  1
7    1   1  1
8    0   1  1
9    0   1  1
10   0   1  1
11   1   1  0
12   1   1  0
13   1   0  0
14   1   0  0
15   1   0  0
16   1   0  0
17   1   0  0
18   0   0  0
19   0   0  0
20   0   0  0
</code></pre>

<p>to avoid correlation, my results come closer to my expectations:</p>

<pre><code>Call:
glm(formula = AV ~ UV1 + UV2, family = ""binomial"", data = lrdata)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.76465  -0.81583  -0.03095   0.74994   1.58873  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)  -1.1248     1.0862  -1.036   0.3004  
UV1           0.1955     1.1393   0.172   0.8637  
UV2           2.2495     1.0566   2.129   0.0333 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 27.726  on 19  degrees of freedom
Residual deviance: 22.396  on 17  degrees of freedom
AIC: 28.396

Number of Fisher Scoring iterations: 4
</code></pre>

<p>But why does the correlation influence the results of the logistic regression and not the results of the ""not-logistic"" regression? </p>
"
"0.0457070726502004","0.0435560928104281","232433","<p>I'm doing a liner regression fit using R. I used <code>lm()</code> to do the regression. Then I standardize my data using <code>scale()</code> and again do the regression on standardize data using <code>lm()</code>.</p>

<p>Surprisingly the regression coefficient of one variable was positive before standardization and after standardization I found it is showing negative coefficient. I checked the correlation between that variable and my predictor. It has positive significant correlation.  </p>

<pre><code>data_bd2=data_2[,c(1:3,5:7)] 
str(data_bd2) 
fit_bd=lm(data_bd2) 
vif(fit_bd) 
summary(fit_bd) 
scale_data_bd2=data.frame(scale(data_bd2))
colnames(scale_data_bd2)=colnames(data_bd2) 
fit_bd_std=lm(scale_data_bd2) 
summary(fit_bd_std) 
</code></pre>

<p>Can you please help me understand why sign of regression coefficient differ before and after standardization?</p>
"
"0.052777981396926","0.0502942438178979","232682","<p>I have a <em>balanced panel</em> dataset in which I have observations for N countries across T quarters. I use a <em>fixed effect model</em> to explore the relationship between my variables. Since all countries have the same number of observations, they are all given the same ""weight"" in the regression. I would like to apply a weigthing by GDP such that observations of big countries (in terms of wealth) have more impact on my estimation. My first idea was to compute weights for all countries and rescale the dependent variable. However, I think this method will only affect the (country-specific) intercepts and won't have much impact on the coefficients. I also don't think that controlling for GDP by including it in the regression is the solution. </p>

<p>I am working with the R plm package to estimate my models. I know that the lm function has a weight parameter but couldn't find an equivalent in the plm package. </p>

<p>Any idea how I should proceed?</p>
"
"0.136838784658047","0.130399136480379","232829","<p>I am using CT scans to classify lung cancer into one of two types (Adenocarcinoma vs. Squamous carcinoma; we can abbreviate them A &amp; B). I am applying LASSO penalized logistic regression to a data set containing 756 CT-derived texture features and 12 radiologist identified categorical (yes/no) features. The latter are based on literature for relevance whereas the former are computer generated with no prior proof of being useful. I have 107 cases so my final dataframe (df) is 107 x 768 dimensional:</p>

<p>i)Texture features (mathematical quantities n=756) are continuous variables scaled and centered. Their names are stored in list <code>â€˜texVarsâ€™</code></p>

<p>ii)Semantic features(Qualitative features subjective assessed by experienced radiologist, n=12). These are usually categorical binary inputs of yes / no type. Their names are stored in list <code>â€˜semVarsâ€™</code>.</p>

<p>Following comments from community on my original (very different) model <a href=""http://stats.stackexchange.com/questions/229884/is-my-high-dimensional-data-logistic-regression-workflow-correct"">Is my high dimensional data logistic regression workflow correct?</a>, I performed my LR development in three steps:</p>

<p>1)Feature selection: I used principle components analysis to reduce texture feature-space from 756 to 30. I kept 4 most relevant (from literature) semantic features. This gave me 34 final features. I used the following command:</p>

<pre><code>trans = preProcess(df[,texVars], method=c(""BoxCox"", ""center"",   ""scale"", ""pca""),thresh=.95)  # only column-names matching â€˜texVarsâ€™ are included.
neodf2 &lt;- predict(trans,df[,texVars]).
neodf.sem &lt;- neodf2[,c(""Tumour"",""AirBronchogram"", ""Cavity"", ""GroundglassComponent"",""Shape"")]  # this DF is 107 x 4 dimensional, containing only 4 semantic features (most relevant from prior knowledge).
neodf.tex &lt;- neodf2[,c(""Tumour"",setdiff(names(neodf2),names(neodf.sem)))] # this only has the 30 PCA vectors (labelled PC1 â€“ PC30).
</code></pre>

<p>2) Model development (LASSO) and penalty term tuning (10fold cross-validation) using cv.glmnet command  Deviance was used as determinant of model quality. Using this method, I developed a model incorporating only semantic features, a second model incorporating only texture features, and a third model incorporating both semantic and texture features. Here are the commands:</p>

<pre><code>#Converting to model.matrix for glmnet 
xall &lt;- model.matrix(Tumour~.,neodf2)[,-1]
xtex &lt;- model.matrix(Tumour~.,neodf.tex)[,-1]
xsem &lt;- model.matrix(Tumour~.,neodf.sem)[,-1]
y &lt;- neodf$Tumour
require(glmnet)
grid &lt;- 10^seq(10,-2,length=100)

lasso.all &lt;- cv.glmnet(xall,y,alpha=1,lambda=grid, family=""binomial"", type.measure=""deviance"") 
lasso.tex &lt;- cv.glmnet(xtex,y,alpha=1,lambda=grid, family=""binomial"", type.measure=""deviance"")
lasso.sem &lt;- cv.glmnet(xsem,y,alpha=1,lambda=grid, family=""binomial"", type.measure=""deviance"")
</code></pre>

<p>3) Testing model classification accuracy on entire dataset. The following is the backbone of  bootstrap to generate 95% confidence intervals of predictive accuracy:</p>

<pre><code>pred &lt;- predict(lasso.all, newx = xall, s = ""lambda.min"", ""class"")
tabl &lt;- table(pred,y)
sum(diag(prop.table(tabl)))
</code></pre>

<p>4) As an alternative means to assess model performance than classification accuracy, I used ROC area under curve on entire dataset and compared AUROC curves from different models using DeLong's method (pROC package)</p>

<p>The results are interesting</p>

<pre><code> =================================================
</code></pre>

<p>LR MODEL BASED ON SEMANTIC FEATURES ALONE:
    lasso.sem$lambda.min
     0.01</p>

<p>Plot cv lambda vs. binomial devance <a href=""http://i.stack.imgur.com/6Modw.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6Modw.png"" alt=""cv lambda vs binomia deviance""></a></p>

<pre><code>             Feature          Odds Ratio
1                (Intercept)  0.1292604
2      AirBronchogramPresent  0.1145378
3              CavityPresent 35.4350358
4 GroundglassComponentAbsent  4.3657928
5                 ShapeOvoid  2.4752881

AUC: .84


=================================================    
</code></pre>

<p>LR MODEL BASED ON TEXTURE FEATURES ALONE:</p>

<pre><code>lasso.tex$lambda.min
1e+10   
</code></pre>

<p>Plot  cv lambda vs binomial deviance (texture alone). Note how the 95% CI's are all overlapping! <a href=""http://i.stack.imgur.com/1Mk7M.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1Mk7M.png"" alt="" cv lambda vs binomial deviance ""></a></p>

<pre><code>   Feature OddsRatio
1 (Intercept) 0.6461538



============================================================
</code></pre>

<p>LR MODEL BASED ON TEXTURE + SEMANTIC FEATURES:</p>

<pre><code>lasso.all$lambda.min
0.05 
</code></pre>

<p>Plot  cv lambda vs binomial deviance <a href=""http://i.stack.imgur.com/p1AHX.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/p1AHX.png"" alt=""cv lambda vs binomial deviance""></a></p>

<pre><code>                          Feature   OddsRatio
1                (Intercept)        0.3136489
2                       PC23        0.9404430
3                       PC27        0.8564001
4      AirBronchogramPresent        0.2691959
5              CavityPresent        6.7422427
6 GroundglassComponentAbsent        2.0514275
7                 ShapeOvoid        1.5974378

 AUC : .88
</code></pre>

<p>Plot showing loglambda vs coefficients. The dashed vertical line shows the cross-validated optimum lambda:<a href=""http://i.stack.imgur.com/d6FaO.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/d6FaO.jpg"" alt=""enter image description here""></a></p>

<p>Having rejected the texture only model, which only contains intercept, i was left with two models - semantic and combined texture+semantic. I created ROC curves for both and compared them using DeLong's method:</p>

<pre><code>pred.sem&lt;- predict(lasso.sem, newx = xsem, s = ""lambda.min"")
pred.all&lt;- predict(lasso.all, newx = xall, s = ""lambda.min"")

roc.sem&lt;- roc(y,as.numeric(pred.sem), ci=TRUE, boot.n=1000, ci.alpha=0.95,stratified=FALSE)    

roc.all&lt;- roc(y,as.numeric(pred.all), ci=TRUE, boot.n=1000, ci.alpha=0.95,stratified=FALSE)
</code></pre>

<p>Outputs of ROC analysis are:</p>

<pre><code>data:  roc.sem and roc.all
Z = -2.1212, p-value = 0.0339
alternative hypothesis: true difference in AUC is not equal to 0
sample estimates:
AUC of roc1 AUC of roc2 
  0.8369963   0.8809524 
</code></pre>

<p>Showing that combined model ROC curve is significantly better despite the modest improvement in AUC (83% vs 88%).
Questions are:</p>

<p>a) is my methodology airtight from a publication point of view now? Apologies in advance for any gross errors in my presentation of this problem.</p>

<p>b) what is the formal inference that texture model is intercept only.</p>

<p>c) if texture model is useless, how do its variables become useful once added to semantic features and yield a higher overall accuracy in the combined result? perhaps that means the effect of texture features alone is too small to be detected in this small dataset but becomes apparent when combined with a stronger predictor (i.e., semantic features). </p>

<p>Any further comments are welcome.</p>
"
"0.0430930413588572","0.0410650781176591","233063","<p>I have created a logistic regression in R and would like to use the trained model to create an predict function (lets say in Excel).  How can I convert the coefficients into a predict equation?</p>

<pre><code>glm(formula = is_bad ~ is_rent + dti + bc_util + open_acc +    pub_rec_bankruptcies + 
chargeoff_within_12_mths, family = binomial, data = df)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.8659  -0.5413  -0.4874  -0.4322   2.4289  

Coefficients:
                            Estimate Std. Error  z value Pr(&gt;|z|)    
(Intercept)              -2.9020574  0.0270641 -107.229  &lt; 2e-16 ***
is_rentTRUE               0.3105513  0.0128643   24.141  &lt; 2e-16 ***
dti                       0.0241821  0.0008331   29.025  &lt; 2e-16 ***
bc_util                   0.0044706  0.0002561   17.458  &lt; 2e-16 ***
open_acc                  0.0030552  0.0012694    2.407   0.0161 *  
pub_rec_bankruptcies      0.1117733  0.0163319    6.844 7.71e-12 ***
chargeoff_within_12_mths -0.0268015  0.0564621   -0.475   0.6350    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 173006  on 233017  degrees of freedom
Residual deviance: 170914  on 233011  degrees of freedom
(2613 observations deleted due to missingness)
AIC: 170928

Number of Fisher Scoring iterations: 4
</code></pre>
"
"0.075412822378","0.0821301562353182","233406","<p>I am using the <code>Cochrane.orcutt</code> procedure to do a time series analysis.</p>

<p>I did the initial regression with the <code>lm</code> function, and then past the result to <code>Cochrane.orcutt</code>. </p>

<pre><code>reg &lt;- lm(data$Y ~ data$Y_Lag1 + data$X1 + data$X2)

regfinal &lt;- cochrane.orcutt(reg)
regfinal
Cochrane.Orcutt

Call:
lm(formula = YB ~ XB - 1)

Residuals:
 Min       1Q   Median       3Q      Max 
-0.72984 -0.32750  0.03553  0.17989  0.69595 

Coefficients:
                        Estimate Std. Error t value Pr(&gt;|t|)    
XB(Intercept)                0.39347    0.09500   4.142  0.00252 ** 
XBdata$Y_Lag1                1.05427    0.09556  11.033 1.57e-06 ***
XBdata$X1                   -3.16739    0.80754  -3.922  0.00350 ** 
XBdata$X2                   -3.30504    0.86569  -3.818  0.00410 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.447 on 9 degrees of freedom
Multiple R-squared:  0.9451,    Adjusted R-squared:  0.9208 
F-statistic: 38.76 on 4 and 9 DF,  p-value: 1.115e-05


$rho
[1] -0.6855955

$number.interaction
[1] 8
</code></pre>

<hr>

<p>I am doing a time series with regressors.</p>

<pre><code>Y_t = a + b*Y_t-1 +c*X1_t +d*X2_t
</code></pre>

<p>I just have a few questions.</p>

<ol>
<li><p>This Cochrane procedure looks like it gives back a list, not a lm. I want to plot the fitted vs actuals, and also use the predict function. The code below gives an error (whereas it works on stuff from ""lm"")</p>

<pre><code>plot(data$Y,col=""red"")
    lines(data$Y)
lines(fitted(regfinal),col=""blue"")
</code></pre></li>
<li><p>Is what I am doing with the regression correct for a theory point of view? That is, including in the lagged value of the response? I am new to Regression with ARMA errors. I have seen somewhere that you are supposed to run the standard regression with only the predictors (no Y_Lag1), and then you get the residuals and you build a model with it. The problem is, when I exclude Y_Lag1 from my lm, the significance of everything else drops away:</p></li>
</ol>

  

<pre><code>Coefficients:
                        Estimate Std. Error t value Pr(&gt;|t|)
XB(Intercept)               -0.02025    0.73246  -0.028    0.978
XBdata$X1                   -1.31001    0.98394  -1.331    0.213
XBdata$X2                   -1.32077    1.42550  -0.927    0.376

Residual standard error: 0.8196 on 10 degrees of freedom
Multiple R-squared:  0.2635,    Adjusted R-squared:  0.04256 
F-statistic: 1.193 on 3 and 10 DF,  p-value: 0.3616


$rho
[1] 0.6886191

$number.interaction
[1] 6
</code></pre>
"
"NaN","NaN","234039","<p>I have estimated a linear regression model in R and have extracted the estimated coefficients into a row vector B = (B1, B2, B3, B4). I'm supposed to test the hypothesis H0:r'B=q where r=(0,1,10,1)' (' indicates transpose) and B=(B1, B2, B3, B4)' and q=100 (Î²1 is the intercept) and report the t-statistic for this test. I suspect that the purpose is to impose restrictions on the first model but I have no idea how to go about this and would much appreciate any direction or advice.</p>
"
"NaN","NaN","234057","<p>I have seen the other posts regarding interpretation of slope estimates of a Poisson regression and based on that, this is my interpreation of a poisson regression. Can anyone advise me if I can write the below statement:</p>

<pre><code>mdl&lt;-glm(y ~ year, family=""quasipoisson"")
summary(mdl) 

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.8258  -1.4108  -0.5760   0.8562   3.2575  

Coefficients:
         Estimate Std. Error t value  Pr(&gt;|t|)    
(Intercept) 68.918820  14.838432   4.645 0.0000216 ***
year        -0.034351   0.007531  -4.561 0.0000289 ***

slope&lt;-exp(mdl$coefficients[2])
# 0.9662321

slope - 1
# -0.03376786
</code></pre>

<p>Can I make the following statement: one unit increase in <code>year</code> causes <code>y</code> to decrease by <code>0.033</code>units?</p>

<p>Thanks</p>
"
"0.114267681625501","0.116149580827808","234076","<p>I am looking to do time series forecasting with multiple variables. For example a data frame (df) of 4 different time series might look like this, where each column is its own time series: </p>

<pre><code>    X1 X2 X3 X4
1   4 13  2 81
2  24 91 86 58
3  21 97 39  1    
4   1 56 79 55
5  63  6 91 79
6  66 96 95 81 
</code></pre>

<p>Let's say X1 is 'cost' and the other variables are things like temperature, volume, and #_of_people.</p>

<p>I would like to forecast 'cost' using the other 3 variables. I imagine using something like <strong>Vector Autoregressive Models (VAR) for Multivariate Time Series</strong> can be used to see how each variable impacts the other in each separate time series (one for each variable). </p>

<p>For example, using the <strong>vars</strong> package in r we can run forecasts against the 4 time series and plot the results: </p>

<pre><code>var.2c &lt;- VAR(df, p = 2, type = ""const"")
var.2c.prd &lt;- predict(var.2c, n.ahead = 8, ci = 0.95)
fanchart(var.2c.prd)
</code></pre>

<p><a href=""http://i.stack.imgur.com/lq4VD.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/lq4VD.png"" alt=""enter image description here""></a></p>

<p>As I understand it, 4 separate regression models were built, one for each variable, where all the other variables were considered for each one. In other words, 'cost' was forecasted, taking into consideration not just the 'cost' trends, but also the impact the other 3 variables (X2, X3, and X3) had on 'cost.'</p>

<p>My question is, say I wanted to take a date in the future, on the forecast of cost, and see what happens to that forecasted value when temperature is increased (X2). I am assuming I can just take the coefficients in the 'cost' regression model that was used to forecast 'cost' using VAR, and use them as you would normally. For example, if the coefficient says the 'cost' will increase by 5 dollars for every one unit increase in temperature (X2), then I could take the forecasted value at the date of interest and add the $5, to say that is what would happen to the forecasted value if X2 were to change. </p>

<p>Are my intuitions correct here or am I missing something? Are there better ways to run 'what-if' analyses on forecasted multivariate time series?</p>
"
"0.106649836183801","0.10889023202607","234077","<p>I'm trying to analyze some count data for a few species ('Tetab' indicates the species in the below code). Consulting with a friend who is much more stats literate than I, he suggested analyzing the data with a Poisson regression, and then utilizing confidence intervals to determine which treatments resulted in significantly different count responses. This worked fine for the other two species I analyzed, but I'm getting the error listed in the title. Comparing the code, everything's the same among the different species' analyses, so I'm assuming it has something to do with the data - also because this species is the only one where the zero-inflated poisson regression can't be run. The total count data for the other two species is 33 and 47, but only 22 for Tetab. Could this be related to the error? Is there any workaround for this? The data is heterogenous for variances, so I can't utilize Kruskal-Wallis or multiple comparisons.</p>

<pre><code>&gt; Tetab.pglm &lt;- glm(Count ~ Treatment, data = spond.spp.list[['Tetab']], family = poisson)
&gt; Tetab.zpglm &lt;- zeroinfl(Count ~ Treatment, data = spond.spp.list[['Tetab']], dist = ""poisson"")
Error in solve.default(as.matrix(fit$hessian)) : 
  system is computationally singular: reciprocal condition number = 1.63511e-19

&gt; summary(Tetab.pglm)

Call:
glm(formula = Count ~ Treatment, family = poisson, data = spond.spp.list[[""Tetab""]])

Deviance Residuals: 
 Min        1Q    Median        3Q       Max  
-1.35873  -0.00006  -0.00006   0.07899   2.36154  

Coefficients:
          Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -2.030e+01  4.311e+03  -0.005    0.996
Treatment2   2.004e+01  4.311e+03   0.005    0.996
Treatment3   9.922e-09  6.096e+03   0.000    1.000
Treatment4   2.022e+01  4.311e+03   0.005    0.996

(Dispersion parameter for poisson family taken to be 1)

Null deviance: 54.484  on 51  degrees of freedom
Residual deviance: 23.804  on 48  degrees of freedom
AIC: 68.297

Number of Fisher Scoring iterations: 18

&gt; exp(coef(Tetab.zpglm))
Error in coef(Tetab.zpglm) : object 'Tetab.zpglm' not found
&gt; exp(coef(Tetab.pglm))
 (Intercept)   Treatment2   Treatment3   Treatment4 
1.522998e-09 5.050767e+08 1.000000e+00 6.060920e+08 
&gt; exp(confint(Tetab.zpglm))
Error in confint(Tetab.zpglm) : object 'Tetab.zpglm' not found
&gt; exp(confint(Tetab.pglm))
Waiting for profiling to be done...
Error: no valid set of coefficients has been found: please supply starting values
In addition: Warning messages:
1: glm.fit: fitted rates numerically 0 occurred 
2: glm.fit: fitted rates numerically 0 occurred 
3: glm.fit: fitted rates numerically 0 occurred 
4: glm.fit: fitted rates numerically 0 occurred 
5: glm.fit: fitted rates numerically 0 occurred 
6: glm.fit: fitted rates numerically 0 occurred 
7: glm.fit: fitted rates numerically 0 occurred 
8: glm.fit: fitted rates numerically 0 occurred 
9: glm.fit: fitted rates numerically 0 occurred 
</code></pre>

<p>Thanks for any help you can provide!
Max</p>

<p>Here's the data set:</p>

<pre><code>Species Date    Site    Treatment   Count
Tetab   20160602    2   1   0
Tetab   20160602    2   2   1
Tetab   20160602    2   3   0
Tetab   20160602    2   4   1
Tetab   20160606    1   1   0
Tetab   20160606    1   2   1
Tetab   20160606    1   3   0
Tetab   20160606    1   4   0
Tetab   20160606    2   1   0
Tetab   20160606    2   2   1
Tetab   20160606    2   3   0
Tetab   20160606    2   4   0
Tetab   20160607    2   1   0
Tetab   20160607    2   2   0
Tetab   20160607    2   3   0
Tetab   20160607    2   4   1
Tetab   20160609    1   1   0
Tetab   20160609    1   2   0
Tetab   20160609    1   3   0
Tetab   20160609    1   4   2
Tetab   20160609    2   1   0
Tetab   20160609    2   2   0
Tetab   20160609    2   3   0
Tetab   20160609    2   4   1
Tetab   20160610    1   1   0
Tetab   20160610    1   2   1
Tetab   20160610    1   3   0
Tetab   20160610    1   4   0
Tetab   20160610    2   1   0
Tetab   20160610    2   2   1
Tetab   20160610    2   3   0
Tetab   20160610    2   4   0
Tetab   20160620    1   1   0
Tetab   20160620    1   2   1
Tetab   20160620    1   3   0
Tetab   20160620    1   4   1
Tetab   20160620    2   1   0
Tetab   20160620    2   2   1
Tetab   20160620    2   3   0
Tetab   20160620    2   4   4
Tetab   20160622    1   1   0
Tetab   20160622    1   2   0
Tetab   20160622    1   3   0
Tetab   20160622    1   4   1
Tetab   20160622    2   1   0
Tetab   20160622    2   2   2
Tetab   20160622    2   3   0
Tetab   20160622    2   4   1
Tetab   20160624    2   1   0
Tetab   20160624    2   2   1
Tetab   20160624    2   3   0
Tetab   20160624    2   4   0
</code></pre>
"
"0.0304713817668003","0.029037395206952","234132","<p>When running regressions using lm in R, how do I get standardized slope coefficients to appear as a column with the rest of my results?</p>

<p>I've been using lm.beta to get the betas, but would be very grateful if someone would share how to get those betas to print as a column next to the unstandardized estimate. </p>
"
"0.0430930413588572","0.0410650781176591","234220","<p>I am new to time series analysis. I am trying to use the R package dlm for time-varying regression with multiple regressors. I got the basic dlmModReg to work, so I can get the coefficients for all the regressors as a function of time. My question is, what's the correct way to estimate the relative importance of each regressor at each time point (or within some window)? Thank you!</p>
"
"0.0914141453004008","0.0871121856208561","234470","<p>I have data from these set of experiments:</p>

<p>In each experiment I infect a neuron with a rabies virus. The virus climbs backwards across the dendrites of the infected neuron and jumps across the synapse to the input axons of that neuron. In the input neurons the rabies will then express a marker gene thereby labeling them. This allows me to see which neurons are inputs to the target neuron I infected and thus create a connectivity map of a certain region in the brain.</p>

<p>In each experiment I obtain counts of all the infected input neurons of the target neuron I infected.</p>

<p>Here's a simulation of the data: (3 targets and 5 inputs)</p>

<pre><code>set.seed(1)
probs &lt;- list(c(0.4,0.1,0.1,0.2,0.2),c(0.1,0.3,0.4,0.1,0.1),c(0.1,0.1,0.4,0.2,0.2))
mat &lt;- matrix(unlist(lapply(probs,function(p) rmultinom(1, as.integer(runif(1,50,150)), p))),ncol=3)
inputs &lt;- LETTERS[1:5]
targets &lt;- letters[1:3]
df &lt;- data.frame(input = c(unlist(apply(mat,2,function(x) rep(inputs ,x)))),target = rep(targets ,apply(mat,2,sum)))
</code></pre>

<p>What I'd like to estimate is the effect of each target neuron on these counts, relative to the grand mean. I was thinking that a multinomial regression model is appropriate in this case, where the contrasts are set to the <code>contr.sum</code> option:</p>

<pre><code>library(foreign)
library(nnet)
library(reshape2)

df$input &lt;- factor(df$input,levels=inputs)
df$target &lt;- factor(df$target,levels=targets)
fit &lt;- multinom(input ~ target, data = df,contrasts = list(target = ""contr.sum""))
# weights:  20 (12 variable)
initial  value 505.363505 
iter  10 value 445.057386
final  value 441.645283 
converged
</code></pre>

<p>Which gives me:</p>

<pre><code>&gt; summary(fit)$coefficients
  (Intercept)   target1   target2
B  0.08556288 -1.743854 1.6062660
C  0.55375003 -2.094266 1.2616939
D -0.17624590 -1.364270 0.6284231
E -0.04091248 -1.617374 0.6601274
</code></pre>

<p>So the effects for <code>input A</code> are not reported and I would like to obtain both the effects of all <code>targets</code> on all <code>inputs</code>.</p>

<p>I'm wondering if adding a mean across <code>targets</code> and a mean across <code>inputs</code>, and setting them as baseline <code>dummy</code> variables is a good solution:</p>

<pre><code>#add target mean
mat &lt;- cbind(mat,round(apply(mat,1,mean)))
colnames(mat)[ncol(mat)] &lt;- ""x""
targets &lt;- c(targets,""x"")

#add input mean
mat &lt;- rbind(mat,round(apply(mat,2,mean)))
rownames(mat)[nrow(mat)] &lt;- ""X""
inputs &lt;- c(inputs,""X"")
</code></pre>

<p>So <code>x</code> and <code>X</code> represent the means of <code>targets</code> and <code>inputs</code>, respectively, and are rounded so that they are counts.</p>

<pre><code>df &lt;- data.frame(input = c(unlist(apply(mat,2,function(x) rep(inputs ,x)))),target = rep(targets ,apply(mat,2,sum)))

df$input &lt;- factor(df$input,levels=rev(inputs))
df$target &lt;- factor(df$target,levels=rev(targets))
</code></pre>

<p>And then fit the <code>multinom</code> regression using <code>dummy coding</code>:</p>

<pre><code>fit &lt;- multinom(input ~ target, data = df)
</code></pre>

<p>Thanks</p>
"
"0.0963589698356145","0.0918243061724248","234690","<p>I am new to survival analysis. Below is my data with very unbalanced sample size (treat group has 2 samples with 1 event, 1 censored and control group has 700+ samples). I use Cox regression in 'survival' package in R and results show 3 different tests (likelihood ratio test, log rank test and Wald test). </p>

<pre><code>sample   trt    censor time
A7       TRT     0 1.0219178
BH       TRT     1 0.6136986
SB        C      0 0.7095890
SD        C      0 1.1972603
SE        C      0 3.6191781
..       ..     ..  ..
A1        C      0 4.0082192
</code></pre>

<p>My code:</p>

<pre><code>coxph(Surv(time,censor)~trt, data=dataAll)
</code></pre>

<p>Result:</p>

<pre><code>&gt; coxfit
Call:
coxph(formula = Surv(time, censor) ~ trt, data = dataAll)

  n= 772, number of events= 100 

                 coef exp(coef) se(coef)      z Pr(&gt;|z|)    
trtC -3.80047   0.02236  1.04854 -3.625 0.000289 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

             exp(coef) exp(-coef) lower .95 upper .95
trtC   0.02236      44.72  0.002864    0.1746

Concordance= 0.513  (se = 0.002 )
Rsquare= 0.007   (max possible= 0.73 )
Likelihood ratio test= 5.55  on 1 df,   p=0.01845
Wald test            = 13.14  on 1 df,   p=0.0002895
Score (logrank) test = 38.85  on 1 df,   p=4.579e-10
</code></pre>

<p>My questions are: </p>

<ol>
<li>There are 3 tests giving different p values, and they look quite different with the likelihood ratio test the most conservative. Do they all test for the significant of the Cox coefficient? Which one should I choose?</li>
<li>Give the fact that the treatment group has so few samples, could the p value trustable?</li>
<li>Is it appropriate to apply Cox regression to unbalanced sample? If no, is there any alternative methods?</li>
</ol>

<p>Thanks a lot!</p>

<p>J</p>
"
"0.0545088647991304","0.0649295895722714","234763","<p>I have a fairly large dataset ($\approx 3 \bar{M}$ observations for a dozen candidate predictors) and I would like to perform a logistic regression on that dataset.
I have a problem of separation in that dataset so usual model can't converge. That's why I am using Firth penalization (logistf package for R) to have my model to adjust.</p>

<p>I would like to select the best subset of variables for my final model but I can't find the proper way to do that. I know that stepwise selection is out of question and I usually would use L1 or L2 penalized regression so that some coefficients are reduced to 0.</p>

<p>My problem is : the function I am using to adjust my model doesn't handle extra penalization so no Elasticnet-Firth regression.</p>

<p>Is there, apart from stepwise selection, another way to select my variables ?</p>

<p>Thanks in advance !</p>
"
"0.068136080998913","0.0649295895722714","235402","<p>I am a newbie at R. I am trying to do some logistic regressions. My predictors are categorical, and most have more than two levels.</p>

<p>A couple questions:
1. It looks like R already creates the relevant contrasts for the categorical predictors (I am used to SAS where I need to specify all the contrasts). Is this correct for R? As in, I do <strong>not</strong> need to manually create the contrasts myself?</p>

<ol start=""2"">
<li><p>the ""family=binomial"" step in the glm syntax, will I always need to write this regardless of the number of predictors, and even if the categorical predictors have more than two levels (DV is always binary)?</p></li>
<li><p>For interpreting the results, if I use a categorical predictor (e.g., lettergroup) with different levels (a, b, c, d) as my predictor, with the binary DV as 1=yes and 0=no, if R created the lettergroupb contrast, and it is a significant, positive coefficient, this means b is more likely to say yes compared to a, c, and d? thank you!</p></li>
</ol>
"
