"V1","V2","V3","V4"
"0.110431526074847","0.111803398874989","121408","<p>I have average life expectancy at birth data for an 8 year period and I would like to use that 8 year period to predict the trend for average life expectancy for the next 5 years. I would then like to ask whether this deviates significantly from the actual average life expectancy over the next 5 years.</p>"
"NaN","NaN","<ol>",""
"NaN","NaN","<li>What's the best regression model to fit to the observation base data in order to get predictions for next 5 years?</li>",""
"NaN","NaN","<li>How can I assess whether the difference between the predicted and observed trend is significant?</li>",""
"NaN","NaN","<li>How can I implement #1 and #2 in R?</li>",""
"NaN","NaN","</ol>",""
"NaN","NaN","","<r><regression><time-series><forecasting><life-expectancy>"
"0.292174354895389","0.295803989154981","185449","<p>I've implemented a comparison between the performance of 80%-forecast intervals is in the forecast package - see 1st part of the code below providing a number of hits
This number states, how many times the forcast interval was right for the left-out data entries. Btw regarding variable names: the German ""preis"" means ""price"" and ""absatz"" means ""sales"", i.e.
""preise"" means ""prices"" and ""absaetze"" is the plural for ""sales"".</p>

<p>So, I compared the formula-based prediction interval to what I think bootstrapping is - see 2nd part of the code. But the number of hits in the 2nd case by no means resembles the 80% of the first case.
The following actions did not help to reproduce the 80% : using less data in the given data frame, using median formulas for bootstrapping instead of the upper/lower computation in the loop,
more samples resampling in the resampling.</p>

<p>I cannot imagine the bootstrapping approach performing so bad - what did I do wrong?  </p>

<pre><code>#given

# data frame

preis&lt;-c(1:100)
absatz&lt;-(-2*preis)+1000+rnorm(100)


jeansData&lt;-data.frame(absaetze=absatz,preise=preis)

#### implementation ###


#leave-one-out cross-validation for formula, i.e. with the borders         given     above  
###### (1ST PART) ########

numberOfHits&lt;-0

for(i in (1:100)){

preisCandidateToBeChecked&lt;-preis[i]
absatzCandidateToBeChecked&lt;-absatz[i]

absatzWithoutCandidate&lt;-absatz[-i]
preisWithoutCandidate&lt;-preis[-i]

jeansData&lt;-data.frame            (absaetze=absatzWithoutCandidate,preise=preisWithoutCandidate)
fit&lt;-lm((absaetze~preise), data=jeansData)

#check, if in interval and count as hit, if value is in interval

if(absatzCandidateToBeChecked &lt;= (forecast(fit,     newdata=preisCandidateToBeChecked)$upper[1]) &amp; (absatzCandidateToBeChecked &gt;= (forecast(fit, newdata=preisCandidateToBeChecked)$lower[1])) )
{numberOfHits&lt;-numberOfHits+1}

}

#execute code until here and inspect numberOfHits; the hit rate pretty much resembles the 80% assumed

#then execute the rest

#leave-one-ot cross-validation for bootstrapping (not using the bootstrap function)  ###### (2ND PART) ########


numberOfHits&lt;-0

for(i in (1:100)){

preisCandidateToBeChecked&lt;-preis[i]
absatzCandidateToBeChecked&lt;-absatz[i]

absatzWithoutCandidate&lt;-absatz[-i]
preisWithoutCandidate&lt;-preis[-i]

jeansData&lt;-data.frame(absaetze=absatzWithoutCandidate,preise=preisWithoutCandidate)

#ten or hundred or thousand regressions by bootstrapping

allPredictions&lt;-c()

for(j in (1:10)){

fit&lt;-lm((absaetze~preise), data=jeansData[sample(nrow(jeansData),10,replace=TRUE),])

allPredictions&lt;-c(allPredictions,forecast(fit,     newdata=preisCandidateToBeChecked)$mean)

}

#build and name bootstrapped forecast interval from regressions

upper&lt;-sort(allPredictions)[9]
lower&lt;-sort(allPredictions)[2]

if((absatzCandidateToBeChecked &lt;= upper) &amp; (absatzCandidateToBeChecked &gt;= lower) )
{numberOfHits&lt;-numberOfHits+1}

} #inspect numberOfHitsAgain - it's around 40%. What is foul here?!
</code></pre>
"
"0.110431526074847","0.111803398874989","123889","<p>I recently started a job in power trading. But due to a sudden change in employment I am required to work on econometric models to gauge the supply and demand side of national power markets. So something beween analyst and trader really. Since my job will also be to import power I have to gauge day ahead (spot) prices for a couple of markets. (I've got raw from Reuters for all my explanatory variables and I made pretty good experiences so far with their data)</p>

<p>I got so far: i have the impression that a dynamic regression would do the job quite well. The explanatory variables should be consumption, wind production, hydro production, solar production, nuclear production and gas prices (for the sake of simplicity I assume that consumption already takes care of the variable temperature). There is an additional variable I'd like to include, net import capacity, but I don't want to overdo it to start with. </p>

<p>Since there should be an auto regressive tendency, an ARIMA model should work? As software I chose R as MatLab was too pricey. </p>

<p>I appreciate that this is probably a rather basic question for this community. Nevertheless I would be very grateful if someone took the time to go through the process in an example (of course including R code would be wicked). I did find lots of papers online, nevertheless I would much prefer help from practitioners rather than academics. Also, I would be very interested in any form of relevant remote or f2f courses/seminars. If you know anything relevant please let me know. Equally, if someone with the right skill set is up for remote tutoring, please give me a buzz...</p>

<p>Thanks in advance!</p>

<p>Cheers
Markus</p>
"
"0.468521285665818","0.474341649025257","124707","<p>Sorry for the rather long introduction, but since I was (legitimately) critizised for not explaining my cause and questions enough, I will do so now. </p>

<p>I would like to conduct a <strong><em>(price)-forecast</em></strong> based on a multiple time series VAR-Model (vector autoregressive Model) with multiple endogeneous variables and two exogeneous. Since I am not that skillfull with regards to neither statistics nor R I want to keep is as simple as possible (Trend forecast for 3 months is sufficient).</p>

<p>I am using the ""vars"" - Package, <a href=""http://cran.r-project.org/web/packages/vars/vars.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/vars/vars.pdf</a> and all in all those four functions: decompose(), VARselect(), VAR(), and predict()</p>

<p>I have 1 dependent time series (y, in my model referred to as ""RH"", or ""raRH""), 4-5 endogeneous predictors and 2 exogeneous predictors.
All timeseries have a length of 1-91 observations and are monthly data without any gaps.</p>

<p><strong><em>Data description:</em></strong>
My y (dependent var) are sawlog prices, sawlogs are raw material for plenty of follow up products.<br> My endogeneous (since they all kind of correlate with each other and y) are follow up product-prices or further elaborated sawlogs. <br>My 2 exogeneous predictors are economic indicators similar to BIP etc.</p>

<p>All the time series are <em>non-stationary</em>, since I have read that you should use stationary data in order to gain a valid VAR-Model, I used the decompose() - function in R to split each variable into trend, season and the randwom walk. </p>

<pre><code> raKVH&lt;-decompose(KVH)$random
raKVH&lt;-na.omit(raKVH)
raSNS&lt;-decompose(SNP_S)$random
raSNS&lt;-na.omit(raSNS)
</code></pre>

<p>... and so on for every variable.<br><br>
What I'm interested now in order to do some forecasting are predictions of the randwom walk (right?!). Anyways, I found out that all my data is first-order-integrated, since taking the logarithm makes them all stationary timeseries (ts), tested via Dickey-Fuller-Test. </p>

<p>The picture also provides data example, first picture shows the raw-data, <img src=""http://i.stack.imgur.com/BcMOT.png"" alt=""enter image description here""></p>

<p>second picture the random walks gained by decomposing$random the raw-data. 
<img src=""http://i.stack.imgur.com/96yii.png"" alt=""enter image description here""></p>

<p>I used the command VARselect that automatically computes the optimal lag for my model, whereas tsall is my time-series matrix containing all the timeseries mentioned above.</p>

<pre><code>VARselect(tsall)
</code></pre>

<p>proceeding now with the estimation of the model VAR(p=number of lags given by VARselect), <strong><em>I encountered the following problem</em></strong>: how should I use the attribute ""type"" within the VAR-function? What does ""trend"",""none"", ""const"", ""both"" exactly mean? Since I have stationary data, there won't be any trend right? How can I check if there is a constant? Since the default value is ""const"", I chose to go with that.
<br><br>
<strong><em>The main question I have is the following:</em></strong><br>
How do I get ""real"" forecasts out of the prediction of the randwom walks anyways? If I want to predict the price of yt+3, I need more than the prediction of the random walks here, I need ""real figures"" like in graphic 1. How can I ""add back"" trend and season?</p>

<p>Third picture shows the Forecast of the random walk of my ""target Variable"" Y, but what's the next step here? 
<img src=""http://i.stack.imgur.com/ZtInk.png"" alt=""enter image description here""></p>

<p>Thank you for any help, if my questions/introduction are insufficient, please let me know. I'll try to explain myself better then.</p>
"
"0.156173761888606","0.158113883008419","40749","<p>I have a linear model (with seasonal dummy variables) that produces monthly
forecasts. I'm using R together with the 'forecast' package:</p>

<pre><code>require(forecast)
model = tslm(waterflow ~ rainfall + season, data = model.df, lambda = lambda)
forec = forecast(model, newdata = rainfall.df, lambda = lambda)
</code></pre>

<p>I did a cross-validation and it looks great. Now, what i need is to generate
<em>weekly data points</em> from these month forecasts - in other words, i need to generate a synthetic time-series that have monthly means equal to the forecasts above. So my function would look like:</p>

<pre><code>generate.data = function(monthly.means, start.date, end.date)
{
   #code here
}
</code></pre>

<p>I'm not sure how to do this (interpolation?), so any help is welcome.
Thanks!</p>
"
"0.33129457822454","0.335410196624968","60648","<p>I am trying to forecast electricity consumption in GWh for 2 years ahead (from June 2013 ahead), using R (the forecast package). For that purpose, I tried regression with ARIMA errors. I fitted the model using the <code>auto.arima</code> function, and I used the following variables in the <code>xreg</code> argument in the <code>forecast.Arima</code> function: </p>

<p>- Heating and Cooling Degree Days,<br>
- Dummies for all 12 months and<br>
- Moving holidays dummies (Easter and Ramadan)  </p>

<p>I have several questions regarding the model:</p>

<p>1) Is it correct to use all 12 dummies for monthly seasonality, since when I tried to include 11, the function returned error. The <code>Auto.arima</code> function returned the model ARIMA(0,1,2)</p>

<p>2)The model returned the following coefficients (I won't specify all of them as there are too many coefficients):</p>

<pre><code>ma1      ma2     HDD     CDD   January  February  March     April
-0.52 -0.16      0.27    0.12  525.84   475.13    472.57    399.01
</code></pre>

<p>I am trying to determine the influence of the temperature component over electricity load. In percentages, (interpreting the coefficients just as with the usual regression) the temperature components (<code>HDD</code>+<code>CDD</code>) account for 11,3% of the electricity consumption. Isn't this too little, considering the fact that the electricity consumption is mostly influenced by the weather component? On the other hand, taking look at the dummies' coefficients, it turns out that the seasonality accounts for the greater part of the load. Why is this? Is the model completely incorrect?</p>

<p>I tried linear regression, and the temperature component accounts for 20%, but it is still a low percentage. Why is this?</p>

<p>3) I am obviously making some mistakes in the use of <code>forecast.Arima</code> or the plot function parameters since when I plot the forecasts, I get a picture of the original time series which is continued (merged) with the forecasts for the whole time series period (from 2004 until 2015). I don't know how to explain this better, I tried to paste the picture, but it seems I cannot paste pictures here.</p>
"
"NaN","NaN","25316","<p>How would we measure the predictive power of predictors in time series models. For e.g. in linear regression we have the magnitude and direction of the regression co-efficients and their p-values.</p>"
"NaN","NaN","<p>Is there any measure like that to evaluate the performance of predictors in kalman filter?</p>",""
"NaN","NaN","","<r><regression><time-series><forecasting><kalman-filter>"
"NaN","NaN","169184","<p>I use a Tobit model to predict censored data. I use the <code>AER</code> package in R.</p>

<p>A toy example looks as follows:</p>

<pre><code>library(AER)
N = 10
f = rep(c(""s1"",""s2"",""s3"",""s4"",""s5"",""s6"",""s7"",""s8""),N)
fcoeff = rep(c(-1,-2,-3,-4,-3,-5,-10,-5),N)
set.seed(100) 
x = rnorm(8*N)+1
beta = 5
epsilon = rnorm(8*N,sd = sqrt(1/5))
y.star = x*beta+fcoeff+epsilon ## latent response
y = y.star 
y[y&lt;0]&lt;-0 ## censored response

my.data = data.frame(x,f)
fit &lt;- tobit(y~0+x+f,data=my.data)

my.range = range(y,y.star,predict(fit))
plot(y,ylim = my.range)
lines( ifelse(predict(fit)&gt;0,predict(fit),0),col=""red"")
</code></pre>

<p>The values returned by <code>predict(fit)</code> give me the expected value under the model. How can I derive a e.g. 90% confidence interval around this expected value?</p>
"
"0.110431526074847","0.111803398874989","234446","<p>I have a time series $Y_t$ that is stationary, and several explanatory variables $X$ .. $Z$ (stationary as well). </p>

<p>Is there an R package (or Python one) that can automatically fit all the possible predictive regressions of the form</p>

<p>$Y_t$ = a + b * L(X) + c * L(Y) + d * L(Z) + epsilon</p>

<p>where indicates the vector of lagged X valuesm ie L(X) = ($X_{t-1}$, $X_{t-2}$, ..). The package should tell me automatically (for a given max lag of course) which specification has the best in-sample prediction accuracy (by eventually dropping some of the predictive variables)?</p>

<p>Thanks!</p>
"
"0.33129457822454","0.335410196624968","209874","<p>I have a model fitted with <code>auto.arima</code>, the model is ARIMA(0,1,0)x(0,1,0)[6] with seasonal period 6. The data is bi-monthly so there is an annual seasonality. There is only one regressor indicating an intervention (dummy). </p>

<p>Then I used this model to old data to see what would have happened if the intervention would have done since and earlier period, using the model and forecast from an earlier data. <strong>The thing I do not understand yet</strong> is that if I suppose the intervention only occur in one period, the series only differ in this period. Therefore, there is no persistence on the intervention.</p>

<p>As I understand, the model has ARIMA errors. The error in the intervention period should change and so there should be an effect in the next periods when using forecast to predict futures values. If the intervention occurs in only one period, <strong>why</strong> in the forecast the intervention does not affect futures predictions?</p>

<hr>

<p>EDIT:</p>

<p>The code I am using is</p>

<pre><code>model1&lt;-auto.arima(ts,xreg = X.ts)
</code></pre>

<p>Where <code>X.ts</code> is a <code>ts</code> object with <code>0</code> and a period with intervention. </p>

<p>Then I used </p>

<pre><code>model2&lt;-Arima(Xold, xreg= X.ts.old, model=model1)
</code></pre>

<p>So I used the first model on earlier data to make the following</p>

<pre><code>forecast(model2, xreg=cbin(c(0,1,1,1,1...))
</code></pre>

<p>So I am trying to show what would have been expected from an earlier period (the forecast) if the intervention would have started earlier.</p>

<p>The thing I do not understand yet is that for instance</p>

<pre><code>forecast(model2, xreg=cbin(c(0,1,1,1,0...))
forecast(model2, xreg=cbin(c(0,1,1,1,1...))
</code></pre>

<p>only differ in the periods the <code>xreg</code> differ, with no persistence of these differences. I did not expect this, <strong>why is that?</strong></p>
"
"0.110431526074847","0.111803398874989","158493","<p>I've been using the R forecast package's <em>auto.arima()</em> function to fit an ARIMA model to my time series data. I want to see how good of a fit the ARIMA model is to my original data. I hope to plot my original time series and the ARIMA simulation on the same plot and see how well they match up. How can I do this?</p>"
"NaN","NaN","<p>Thanks!  </p>",""
"NaN","NaN","","<r><regression><forecasting><arima>"
"0.349215147884789","0.353553390593274","159428","<p>I have a set of data, let's say average weight of employees, captured every month over a period of 5 years (2010 - 2014). I cannot find a seasonality trend in the data over these years. Also, I have found that it is not dependent on any other factors.</p>

<p>I am trying to forecast values for 2015 to get a general sense of this data as it is an important metric in the operations of my business. </p>

<p>I have tried ARIMA, R-regression, Exponential smoothing, Excel forecast to find any seasonality whatsoever. However, my efforts are yet to materialize. </p>

<p>My question is: How do I forecast a variable that has no seasonality?</p>

<p>I have attached my data herewith. </p>

<p><strong>Graphs</strong></p>

<p>Yearly Values for years 2010 - 2014</p>

<p><img src=""http://i.stack.imgur.com/rmoeD.jpg"" alt=""enter image description here""></p>

<p>Value Cumulative over 2010 - 2014</p>

<p><img src=""http://i.stack.imgur.com/iwyh8.jpg"" alt=""enter image description here""></p>

<p>All Values from 2010 - 2014</p>

<p><img src=""http://i.stack.imgur.com/dfcGd.jpg"" alt=""enter image description here""></p>

<p><strong>Auto ARIMA in R</strong></p>

<pre><code># Map 1-based optional input ports to variables
dataset1 &lt;- maml.mapInputPort(1) # class: data.frame
library(forecast)


dates &lt;-  dataset1$Date
values &lt;- dataset1$Weight

dates &lt;-  as.Date(dates, format = '%m/%d/%Y')
values &lt;- as.numeric(values)

train_ts &lt;- ts(values, frequency=12)
fit1 &lt;- auto.arima(train_ts)
train_model &lt;- forecast(fit1, h = 12)
plot(train_model)

# produce forecasting
train_pred &lt;- round(train_model$mean,2)
data.forecast &lt;- as.data.frame(t(train_pred))
#colnames(dataset1.forecast) &lt;- paste(""Forecast"", 1:data$horizon, sep="""")

# Select data.frame to be sent to the output Dataset port
maml.mapOutputPort(""data.forecast"");
</code></pre>

<p><strong>Forecasted Value with Auto ARIMA</strong></p>

<pre><code>Date        Weight
01-01-15    11.77
01-02-15    11.76
01-03-15    11.77
01-04-15    11.76
01-05-15    11.77
01-06-15    11.77
01-07-15    11.76
01-08-15    11.77
01-09-15    11.76
01-10-15    11.77
01-11-15    11.77
01-12-15    11.76
</code></pre>

<p><strong>Data</strong></p>

<pre><code>Date        Weight      Cumulative Weight
01-01-10    11.8800     11.8800
01-02-10    10.4000     22.2800
01-03-10    6.9500      29.2300
01-04-10    15.5000     44.7300
01-05-10    17.0400     61.7700
01-06-10    10.4700     72.2400
01-07-10    12.1400     84.3800
01-08-10    2.5800      86.9600
01-09-10    12.6300     99.5900
01-10-10    11.6800     111.2700
01-11-10    9.0700      120.3400
01-12-10    10.8900     131.2300
01-01-11    1.7500      132.9800
01-02-11    -1.7700     131.2100
01-03-11    5.9300      137.1400
01-04-11    -4.9200     132.2200
01-05-11    4.3900      136.6100
01-06-11    1.5100      138.1200
01-07-11    1.2200      139.3400
01-08-11    10.2900     149.6300
01-09-11    13.0600     162.6900
01-10-11    10.1400     172.8300
01-11-11    8.5250      181.3550
01-12-11    6.4350      187.7900
01-01-12    -5.5100     182.2800
01-02-12    -4.3000     177.9800
01-03-12    2.3200      180.3000
01-04-12    4.0700      184.3700
01-05-12    12.2700     196.6400
01-06-12    14.7400     211.3800
01-07-12    8.4600      219.8400
01-08-12    11.6300     231.4700
01-09-12    -0.1500     231.3200
01-10-12    2.5200      233.8400
01-11-12    6.7400      240.5800
01-12-12    35.6300     276.2100
01-01-13    26.4000     302.6100
01-02-13    26.1300     328.7400
01-03-13    16.2100     344.9500
01-04-13    56.0800     401.0300
01-05-13    32.2300     433.2600
01-06-13    17.5100     450.7700
01-07-13    3.6700      454.4400
01-08-13    7.7700      462.2100
01-09-13    -14.2800    447.9300
01-10-13    1.0800      449.0100
01-11-13    9.4000      458.4100
01-12-13    7.3400      465.7500
01-01-14    6.1400      471.8900
01-02-14    3.8200      475.7100
01-03-14    16.7600     492.4700
01-04-14    0.4900      492.9600
01-05-14    17.9800     510.9400
01-06-14    14.8000     525.7400
01-07-14    12.6400     538.3800
01-08-14    5.7300      544.1100
01-09-14    -2.0900     542.0200
01-10-14    9.1300      551.1500
01-11-14    12.5100     563.6600
01-12-14    -1.3900     562.2700
</code></pre>

<p><strong>Actual Values for 2015</strong></p>

<pre><code>Date        Weight
01-01-15    -18.43
01-02-15    13.94
01-03-15    26.14
01-04-15    24.36
01-05-15    18.37
</code></pre>
"
"0.413196935270669","0.418330013267038","89531","<p>I'm expanding a question I posed earlier because I think it was lacking detail. </p>

<p>I'm attempting to forecast daily demand for a restaurant that sells take away food, primarily to office workers on their lunch breaks. They are located in the downtown core of a major city.</p>

<p>They are only open on workdays - no holidays, no weekends. I'm familiar with models that take into account seasonality and trend - Holt-Winters triple exponential smoothing, for example. I'm also familiar with models that take into account complex seasonality and trend - the TBATS package for R, for example.</p>

<p>My problem is that I've identified 8 components that determine sales on a given day:</p>

<ol>
<li>The yearly seasonal component. Sales are lower in the summer, for example, when many office workers are on vacation.</li>
<li>The weekly component. Sales very obviously peak on Thursdays (in the absence of other effects - see below)</li>
<li>The <em>Friday-Long-Weekend-Effect</em>. The weekly sales pattern changes if the coming Friday is a holiday. Wednesday will typically have higher sales, for example.</li>
<li>The <em>Post-Friday-Long-Weekend-Effect</em>. The weekly sales pattern changes if the week before was shortened due to the Friday being a holiday.</li>
<li>The <em>Monday-Long-Weekend-Effect</em>. The weekly sales pattern changes in week $t$ if the Monday in week $t+1$ is a holiday. For example, sales are much lower on Fridays preceding Monday-Long-Weekends. Presumably people are leaving the office early and skipping lunch.</li>
<li>The <em>Post-Monday-Long-Weekend-Effect</em>. The weekly sales pattern changes if the week is shortened due to the Monday being a holiday.</li>
<li>The trend component. </li>
<li>The noise component.</li>
</ol>

<p>If holidays fell on the same date every year, then the ""long-weekend-effects"" would be captured in the yearly seasonal component. However, they don't. </p>

<p>My first thought was to include dummy variables. For example, let $X_{M+1}$ be the ""Monday-long-weekend-effect"" component, and $\beta_{M+1}$ be the associated coefficient, for a given day. Then for the Friday preceding a Monday-Long-Weekend, $X_{M+1}=1$, and for a Friday not preceding a Monday-Long-Weekend, $X=0$.</p>

<p>I'm only using three years of data, so it would be easy for me to change the $X_{M+1}$ values to 0 or 1 by hand for each year. However, I don't know how to include such dummy variables in models like those that I've mentioned.</p>

<p>Any input as to a model that can take into account the components I've mentioned would be greatly appreciated. It seems like I need to capture moving-holiday-effects, day-of-the-week effects, seasonal patterns, and trend, all in one.</p>

<p><strong>Question: Is there a model I can use that can be implemented in R and take into account the components I've listed?</strong></p>

<p><em>My background: I'm a forth year mathematics and economics student. I've also taken statistics classes, and I'm using R to perform my analysis. This is for a final report for a forth year data analysis class.</em></p>
"
"0.156173761888606","0.158113883008419","83433","<p>I would like to ask how the long-term (multiple step ahead) prediction intervals are calculated by function <code>predict.Arima</code> in R. I am particularly interested in ARIMA models, SARIMA models and in ARIMA models with external regressors (include argument xreg => regression with ARIMA errors) </p>
"
"NaN","NaN","116842","<p>I have a SarimaX model with three regressor variables:</p>

<pre><code>ARIMA(1,0,0)(0,1,1)[7]                    

Coefficients:
          ar1       sma1   C1 (for xreg1)   C2 (for xreg2)   C3 (for xreg3)
      -0.0260    -0.9216          -0.0354           0.0316           0.9404
s.e.   0.0291     0.0350           0.0016           0.0017           0.0128
</code></pre>

<p>I would like to know how to use these coefficients to obtain the actual equation, like:</p>

<pre><code>y[t] = f(ar1, sma1, C1|xreg1[t], C2|xreg2[t], C3|xreg3[t])
</code></pre>

<p>I have read the following:</p>

<p><a href=""https://www.otexts.org/fpp/8/9"" rel=""nofollow"">https://www.otexts.org/fpp/8/9</a> - I'm using the forecast package in R, so I'm quite grateful for Mr. Hyndman's work,</p>

<p><a href=""http://people.duke.edu/~rnau/arimreg.htm"" rel=""nofollow"">http://people.duke.edu/~rnau/arimreg.htm</a></p>

<p>and others, and I devised some formulas, but they generated values less acurate than those from the R forecast. Somehow, my error-related terms are probably wrong.</p>

<hr>

<p><strong>EDIT</strong>: This is what I have so far:</p>

<p>$$ \ (1-ar1*B)*(1-B^7)*y_t=$$
$$ = (1-ar1*B)*(1-B^7)*(C1*xreg1_t + C2*xreg2_t+C3*xreg3_t)+ $$
$$ + e_t + sma1*e_{t-7}$$</p>

<p>I would like to know if this formula is correct, could anyone please help? Thank you.</p>
"
"0.156173761888606","0.158113883008419","28472","<p>A Regression with ARIMA errors is given by the following formula (saw on Hyndman et al, 1998):</p>

<p>$Y_t = b_0 + b_1 X_{1,t} + \dots + b_k X_{k,t} + N_t$</p>

<p>where $N_t$ is modeled as an ARIMA process.</p>

<p>If we have that the model for $N_t$ is ARIMA$(0,0,0)$, then $N_t = e_t$, and $Y_t$ is modeled by an ordinary regression.</p>

<p>Suppose the following data:</p>

<pre><code>a &lt;- structure(c(29305, 9900, 9802, 17743, 49300, 17700, 24100, 11000, 
10625, 23644, 38011, 16404, 14900, 16300, 18700, 11814, 13934, 
12124, 18097, 30026, 3600, 15700, 12300, 14600), .Tsp = c(2010.25, 
2012.16666666667, 12), class = ""ts"")
b &lt;- structure(c(1.108528016, 1.136920872, 1.100239002, 1.057191265, 
1.044200511, 1.102063834, 1.083847756, 1.068585841, 1.084879628, 
1.232979511, 1.168894672, 1.257302058, 1.264967051, 1.234793782, 
1.306452369, 1.252644047, 1.178593218, 1.124432965, 1.132878661, 
1.189926986, 1.17249669, 1.176285957, 1.176552, 1.179178082), .Tsp = 
c(2010.25, 2012.16666666667, 12), class = ""ts"")
</code></pre>

<p>If I model it using <code>auto.arima</code> function, I have:</p>

<pre><code>auto.arima(a, xreg=b)
Series: a 
ARIMA(0,0,0) with zero mean     

Coefficients:
              b
      15639.266
s.e.   1773.186

sigma^2 estimated as 101878176:  log likelihood=-255.33
AIC=514.65   AICc=515.22   BIC=517.01

lm(a~b)

Call:
lm(formula = a ~ b)

Coefficients:
(Intercept)            b  
      48638       -26143  
</code></pre>

<p>Coefficients from the models differ. Shouldn't they be the same? What am I missing?</p>
"
"0.156173761888606","0.158113883008419","119946","<p>I have some time series data where I'm modelling temperature as a function of various predictors. On physical grounds, I can expect that</p>

<p>$$\frac{dT}{dt} \propto T_a - T$$</p>

<p>where $T_a$ is the ambient temperature (which can vary over time, but whose values are known). I thus fit models of the form</p>

<p>$$\Delta T(t) \sim \alpha + \beta \left[ T_a(t) -T(t) \right] + \gamma X(t)$$</p>

<p>with $X$ being the other covariates, and $\alpha$, $\beta$ and $\gamma$ are the regression parameters. I can fit these easily enough in R:</p>

<pre><code>lm(diff(T) ~ I(Ta - T) + x, data=df)
</code></pre>

<p>and I can get predictions for the change in $T$. However, what I really want are predictions for $T$ itself. At the moment I'm calculating these via a loop, where I plug $\hat{T}(t)$ into the regression equation to obtain $\hat{\Delta T}(t+1)$.</p>

<p>Is there any R package, probably time series-related, that will do these calculations automatically?</p>

<p>Also, if there are any issues with this approach, I'd be happy to know about them.</p>
"
"0.246932399162397","0.25","120008","<p>I am interested in fitting an ARIMAX model using R.
As known, ARIMAX can be understood as a composition of ARIMA models and regression models with exogenous (independent) variables. I have a time series $Y_i$, and want to estimate the ARIMA and nonlinear coefficients. The nonlinear model is the following:</p>

<p>$y_i=Î²_0+Î²_1t_i+Î²_2d+Î²_3 sin(2Ï€t_i/Î²_4 )+Î²_5 (-1^{t_i})+Îµ_i$,  nonlinear 
regression with an exogenous variable.
Where 
$t_i$ =1, 2â€¦, 60
and</p>

<p>d = dummy variable with 20 0's and 40 number 1's</p>

<pre><code>d=c(rep(0,20),rep(1,40))
</code></pre>

<p>And an ARIMA model (1,1,1) for $Y_i$. Therefore, I want to estimate simultaneously the $Î²_i$ and the ARIMA coefficients in order to avoid the confusion between the exogenous coefficients and ARIMA coefficients.  I know that $arima()$ can deal with this formulation but, how do the nonlinear model can be set within function function?. It seems that the <em>xreg</em> term only deals with linear parameters.</p>
"
"0.191273013919001","0.193649167310371","125308","<p>I just fit a model to a time series. I am now required to generate a 10-year extrapolation forecast of my model. My model includes a time term, a time^2 term, 12 seasonal dummies, and 4 lagged dependent variables. If I am correct, I thought that I could not do an h step ahead forecast because of the lags, so I am trying to run 120 1-step ahead forecasts. I am using the predict() function and trying to make a for-loop for times i=313:432 (that is 120 months following our initial 312 observations).</p>

<p>I am confused with the predict () function and with for loops, and any guidance on how to generate this loop would be extremely appreciated!</p>
"
"0.493864798324795","0.5","29336","<p><strong>Introduction</strong> </p>

<p>I am aiming to forecast the annual growth rates for a number of macroeconomic indicators (denote one by $Y_t$).
One of the tasks is to test the forecasting performance of rival time series models with and without exogenous variables ($X_t$, a $T\times k$ matrix). The list of rival models include:</p>

<ol>
<li>AR(I)MA model (annual growth rates are unlikely to have ""unit Roo"", though the latter is either assumed or tested) $$A(L)Y_t =\mu+ B(L)\varepsilon_t$$</li>
<li>linear regression model with ARMA errors $$Y_t = X_t\beta + \eta_t, \ \ A(L)\eta_t =  B(L)\varepsilon_t $$</li>
<li>lagged dependent variable model (autoregressive model with exogenous variables)
$$A(L)Y_t = X_t\beta + \varepsilon_t$$ </li>
<li>linear regression model
$$Y_t = X_t\beta + \varepsilon_t$$</li>
</ol>

<p>Where $\varepsilon_t$ is assumed to be a strong white noise, zero-mean constant variance i.i.d. process; $A(L)$ and $B(L)$ are autoregressive (of order $p$) and moving average (of order $q$) polynomials with $L$ - a back-shift (lag) operator.</p>

<p>Note that the primary and the only goal is forecasting performance, thus any ""good"" properties of parameter estimates are of the secondary concern. All I need is to test for the most parsimonious, robust to starting conditions forecaster. Decision will be made with one of the <code>accuracy()</code> options, but first I need to obtain the material for the comparison.</p>

<p>Models 1. and 2. are estimated by <code>auto.arima()</code> with default <code>""CSS-ML""</code> estimation method. Models 3. and 4. are estimated by ordinary least squares (<code>lm()</code>). $T$ is about $40$ quarters.</p>

<p><strong>Approaches tried so far</strong></p>

<p>To make the jack-knifed residuals the first approach denoted by ""rolling"" has been implemented. Starting from feasibly large sub-sample of time series data, parameters are estimated and an $h$ ahead forecast is done by the <code>predict()</code> function (EDIT: it is the same suggestion as in the first part Rob's answer to the second question). After that one point is added and the estimation\prediction steps are repeated. </p>

<p>A weak point of such experiments is that the number of time ticks (sample size) used to estimate the parameters is different. While I would like to test the robustness to the starting conditions, keeping the sample size for estimation fixed.</p>

<p>Bearing this in mind I tried to set the several subsequent values (EDIT: for the interval $k+p+q&lt;t_0&lt;t_1&lt;T-h+1$) in $Y_t$ being missing values (NA). In models 2.-4. this also implies dropping the corresponding subsequent rows in data matrix $X_t$. Prediction for 3. and 4. is straightforward (the same <code>predict()</code> with omitted $X_t$ data rows works well). All my concerns are about models 1. and 2.</p>

<p>With just the AR($p$) part the predictions are done sequentially $Y_{t+1|t} = \hat A(L)Y_t$. But with the presence of MA($q$) one could not (?) use the estimated parameters directly. From the Brockwell and Davis ""Introduction to Time Series and Forecasting"" Chapter 3.3 it follows that one needs an innovation algorithm to recursively estimate the $\theta_{n,j}$ from the specific system of equations that involves estimated autoregressive and moving average parameters. EDIT: these $\theta_{n,j}$ parameters are used to make the ARMA prediction, not the originally estimated parameters $\theta_{j}$. However it is remarked in the same chapter that $\theta_{n,j}$ asymptotically approaches $\theta_{j}$ if the process is invertible. It is not evident that 30-40 points is enough for the asymptotic result to be used even if is invertible. </p>

<p>Notes: I don't want to restrict $q$ to zero, since I am not doing so in true out-of-sample forecasting. EDIT: also not that it is not missing value imputation problem, but forecasting experiment, that the trajectory is not supposed to bridge two sub-samples by this imputing the missing values.</p>

<p><strong>Questions</strong></p>

<ol>
<li>Does <code>auto.arima()</code> performs correctly with the presence of missing values inside of the sample? [Already answered by Rob.]</li>
<li>(The actually crucial part of this post) How to correctly forecast (NOT impute) these missed points from the ARMA model when both $p&gt;0$ and $q&gt;0$? (I hope there are the ways already implemented in R language, but I simply is missing something.)</li>
</ol>

<p>EDIT: since the parameters for ARMA parts are estimated correctly could I legally rearrange the arima object to include the estimated parameters and the data only for the first subsample and then use a predict function? </p>

<p>EDIT2: I have tried to modify the estimated <code>mod</code> structure - the resulting forecast from <code>predict.Arima</code> is identical (double precision differences) to the forecast where I use the estimated MA and AR coefficients predicting $Y_{t+1|t}$ directly as $\hat A(L)(Y_t-X_t\hat \beta)+ X_t\hat \beta+\hat B(L)\hat \varepsilon_t $, without <code>KalmanForecast()</code>. This was expected since state space representation is supplied with the same estimated $\theta_j$, not $\theta_{n,j}$. So the only question remains is the difference between $\theta_j$ and $\theta_{n,j}$ significant to influence the point forecasts? I hope the answer is negative.</p>
"
"0.246932399162397","0.25","68802","<p>I'm trying to understand some concepts related to predictive modeling. So let's say that I have the following data sample and am trying to regress <code>sales</code> on <code>clicks</code> and <code>calls</code>. Ultimately, I'm interested in predicting sales figures for any given month (e.g., <code>Jan</code>, <code>Feb</code>, etc.) given a change in clicks or calls. <code>Month</code> is not included in the model, but I'm not sure how I would go about building a model which would allow me to predict sales figures for <code>Feb</code> given in a change in <code>clicks</code> or <code>calls</code>.</p>

<ol>
<li><p>My data  </p>

<pre><code>df = data.frame(month_main=c(""Jan"",""Feb"",""Mar"",""Apr"",""May""),
                sales=c(50,35,60,20,50),
                month_attr=c(""Jan"",""Feb"",""Mar"",""Apr"",""May""),
                clicks=c(300,350,500,550,250),
                calls=c(100,150,200,150,150))
</code></pre></li>
<li><p>Regression Model</p>

<pre><code>m1 = lm(sales ~ clicks + calls, data=df)
summary(m1)
</code></pre></li>
</ol>

<p></p>

<ul>
<li><p>I want to build a model to predict sales for <code>Feb</code> given a change in either predictor</p></li>
<li><p>Model doesn't include <code>month</code>, but I want to predict for a given month.</p></li>
<li><p>How should I be thinking about this problem?</p></li>
<li><p>How would I perform this task in R?</p></li>
</ul>
"
"0.156173761888606","0.158113883008419","69405","<p>I am fitting a model using the <code>auto.arima</code> function in package <code>forecast</code>. I get a model that is AR(1), for example. I then extract residuals from this model. How does this generate the same number of residuals as the original vector? If this is an AR(1) model then the number of residuals should be 1 less than the dimensionality of the original time series. What am I missing?</p>

<p>Example:</p>

<pre><code>require(forecast)
arprocess = as.numeric(arima.sim(model = list(ar=.5), n=100))
#auto.arima(arprocess, d=0, D=0, ic=""bic"", stationary=T)
#  Series: arprocess 
#  ARIMA(1,0,0) with zero mean     

#  Coefficients:
#          ar1
#       0.5198
# s.e.  0.0867

# sigma^2 estimated as 1.403:  log likelihood=-158.99
# AIC=321.97   AICc=322.1   BIC=327.18
r = resid(auto.arima(arprocess, d=0, D=0, ic=""bic"", stationary=T))
&gt; length(r)
  [1] 100
</code></pre>

<p>Update: Digging into the code of auto.arima, I see that it uses Arima which in turn uses <code>stats:::arima</code>. Therefore the question is really how does <code>stats:::arima</code> compute residuals for the very first observation?</p>
"
"0.110431526074847","0.111803398874989","139448","<p>I have a client who has sparse hourly data (by sparse I mean there are too many hours with 0 calls). I used TBATS in R to forecast hourly data for them. Regardless of the point forecast, the actual values are always in the 80% prediction interval. I wonder if there is any specific method/package in R that is specifically used for uni variate forecasting of sparse data.</p>

<p>Thanks</p>
"
"0.27050089040023","0.273861278752583","32313","<p>I have a linear regression model that is used to forecast the 'afluent natural energy' (ANE) of some region.</p>

<p>The predictors for this model are:</p>

<ul>
<li>the previous month ANE (<code>ANE0</code>)</li>
<li>the previous month rain volume (<code>PREC0</code>)</li>
<li>the current month forecast for rain volume (<code>PREC1</code>)</li>
</ul>

<p>We have 7 years of historical data for all of these variables, for each month. The current model just runs a OLS linear regression. I feel there's a lot of improvements to be done, but i'm not a time series specialist.</p>

<p>The first thing I notice is that the predictors are highly correlated (multicollinearity).
I'm not certain of the impacts of multicollinearity on prediction confidence.</p>

<p>I decided to try a time series approach, so I ran a ACF and PACF on the historic data:
The ACF shows a sine wave pattern, and the PACF has a spike at 1 and 2. So I tried both <code>ARIMA (2, 0, 0)</code> and <code>ARIMA(2,0,1)</code> to predict 20 periods ahead.</p>

<p>The ARIMA(2,0,1) shows good results, but I'm not certain as to how to compare it to the linear regression model.</p>

<p>What's the best way to test the performance of these model?  I'm using R as analysis tool (together with the <code>forecast</code> package). </p>
"
"0.156173761888606","0.158113883008419","141339","<p>I'm having trouble in taking a direction of my research project. I have independent variables that are commonly used as economic indicators and I want to include variables/indicators that are not commonly used to improve my eventual forecasts. I have 31 independent variables with 607 monthly observations after making it stationary and applying the scale function.(scale was applied cause my variable series are of different units/measures)
   I used the PCA function and got down to 13 components that capture 80% cumulative of the variance.
   Question is now that I have 13 new independent variables and the one dependent variable that is ternary in the sense that in the 607 observations it indicates 1 for peak, 0 for nothing, and -1 as trough, what model is best for forecasting/predicting the next 1 &amp; -1 of my dependent variable series based on my 13 independent principal components?</p>

<p>FYI: I have looked at VAR, Cointegration, Granger Causality, Multiple Linear Regression, but can't really make sense if what I'm using is correct and appropriate for my topic.</p>
"
"0.110431526074847","0.111803398874989","220830","<p>I need to forecast using <code>HoltWinters</code> with regression parameters using R. But I found there is not any option of <code>xreg</code> in <code>HoltWinters</code> function in R. I thought to use <code>auto.arima</code> with <code>xreg</code> option but my <code>HoltWinters</code> is performing better than <code>auto.arima</code> without any regression parameters.</p>"
"NaN","NaN","<p>Can you please suggest me how to incorporate <code>xreg</code> in <code>HoltWinters</code> function in R?</p>",""
"NaN","NaN","","<r><regression><forecasting><exponential-smoothing>"
"0.191273013919001","0.193649167310371","166953","<p><strong>Issue</strong>: Cannot forecast sales accurately using quantile regression in R. I am using rq function from ""quantreg"" package which is giving me warning ""Result might have Non unique solutions""</p>

<p><strong>Aim</strong>: I am trying to forecast hourly sales of a store using quantile regression. </p>

<p>Below are the columns in my source table for forecasting.</p>

<ul>
<li><em>transaction_date</em> : sales date (input)</li>
<li><em>hr1 to hr24</em> : column with hourly sales info. (24 columns) (input)</li>
<li><em>totala</em> : total of 24 column hr1 to hr24 (not using currently)</li>
<li><em>location, department, sales_type</em>: forecasting will be done for each location, sales_type and department. (used to select data)</li>
<li><em>f1 to f24 :</em> columns I want to forecast for each hour (24 columns) (output)</li>
</ul>

<p>Packages Used: forecast, quantreg, Metrics</p>

<p><strong>Code</strong>: 
I have extracted date features from transaction_date eg. weekend, week of month and also holidays (1 if it is holiday 0 for regular days).</p>

<pre><code>attach(train_data) 
Y &lt;- cbind(hr) 
X &lt;- cbind(transation_date, Years, Months, Days, WeekDay, WeekofYear, Weekend, WeekofMonth, holidays) 

quantreg.all &lt;- rq(Y ~ X, tau = seq(0.05, 0.95, by = 0.05))
prediction_train &lt;- data.frame(predict(quantreg.all))
</code></pre>

<p>I have 19 models in prediction_train for each tau from 0.05 to 0.95, I select best model based on rmse value and than forecast using that tau.</p>

<pre><code>rmse(actual, predicted)
</code></pre>

<p>transaction_date is Date type, quantreg.all is rqs class and rest are numeric.</p>

<p><strong>Note:</strong> Stores are not open 24 hours, hence many hour columns will be 0 (time when store was close). Currently for most of such hours rq is predicting 0 or some negative values.</p>

<p>Weather  does not have major impact on sales.</p>
"
"0.110431526074847","0.111803398874989","107823","<p>I am working with time series values which are all in the closed interval [0, 1]; these values represent relative frequencies, i.e., empirical probabilities. I would like to create a model such that all forecasted values are within [0, 1], but it would also be fine if the model's output was strictly within the open interval (0, 1).</p>

<p>This answered question tackles the lower bound aspect of my question, but not the upper bound aspect: <a href=""http://stats.stackexchange.com/questions/80859/how-to-achieve-strictly-positive-forecasts"">How to achieve strictly positive forecasts?</a></p>

<p>I'd like to use the R <code>forecast</code> package if possible to achieve this, but I am open to other suggestions.</p>
"
"NaN","NaN","63883","<p>Is there a method to find the right distance function in non-parametric regression?
I use some time series to learn forecasting. Series are nonlinear and non-gaussian.
I can get the right dimension and delay. I can find the right bandwidth with the hdrcde library.
I have no problem with kernel functions.<br>
My problem is with distances. I use Euclidian, Cosine and Correlation weighting functions.
These are the kernel which give good results, but one time, Euclidian is good, then after adding some data, one to 5-7 generally, Euclidian give nothing, even with very little change in statistics.
So, my question is if there is a method to choose the right distance. I would like to have advices on that point and on articles that will help solve this problem. What package in R could eventually help?</p>

<p>Thank you. </p>
"
"NaN","NaN","209173","<p>I am working on a project, and I am absolutely new to forecasting and not so strong in statistics. I have an employee data for the last 7 years, along with the other variables like economic growth, employee turnover, vacancies, and some other economical factors. 
I have to do forecasting for the next 5 years. I have some questions: </p>

<ul>
<li>Is it possible to do a time series analysis with more than one explanatory variable? Can this be done in R? </li>
</ul>

<p>I would appreciate any kind of help. Thanks in advance!! </p>
"
"0.312347523777212","0.316227766016838","63681","<p>I have been adamantly searching the web to learn how to successfully implement a dynamic regression time series in the forecast package for R. The time series data that I am using is weekly data (frequency=52) of incoming call volume and prediction variables are mailers sent out every now and then. They are a significant predictor of the data for the week that they hit, the following week, and the week after that. I have created lagged variables and use these three as the predictors. </p>

<p>My main concern is that the arima model is not taking into account the time series frequency. When I tell it to recognize the ts with a frequency of 52 it has an error. 
I have looked at the <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">fortrain function</a> but do not understand it. I also have looked at the tbats suggested but found that those will not work with prediction variables. </p>

<p>The Zoo function recognizes 52 frequency but it is not advised to use with the <a href=""http://stackoverflow.com/questions/16050684/using-the-combination-forecastauto-arima"">forecast package</a>.</p>

<p>Here is the basic code. The problem is that the time series calwater[,5] is not recognized as such. It is imputed as a simple vector as an integer...</p>

<pre><code>#this works without taking into acount the ts
fit2 &lt;- auto.arima(calwater[6:96,5], xreg=calwater[6:96,6:8], d=0)
fccal &lt;- forecast(fit2, xreg=calwater[97:106,6:8], h=10)
fccal
plot(fccal, main=""Forecast Cal Water"", ylab=""Calls"")

#to form a ts object
calincall&lt;-ts(calwater[1:106,5],start=c(2011,23),frequency=52)

#once the ts is added to the model this dispalys
#Error in `[.default`(calincall, 2:100, 1) : incorrect number of dimensions
</code></pre>

<p>Maybe the error is because there is just a little over two years of data. </p>

<pre><code>#Time Series: Start = c(2011, 23), End = c(2013, 24),Frequency = 52 
</code></pre>

<p>I would be very grateful for any guidance in for this particular issue. I am using the forecast package and prefer to continue within the package but I am open to suggestions. </p>
"
"0.246932399162397","0.25","46391","<p>I need to build a model using climate variables (temperature, rainfall) to predict
monthly sales (horizon of 6 months) for certain product. The data has strong seasonality and a standard regression model would works fine, the problem is that the historic data will not be updated, meaning that the observed data points will not be incorporated into the model.</p>

<p>Whats a good way to solve this? What if i split the sales data into levels (say 'WEAK', 'NORMAL', 'HIGH', VERY HIGH') and then use a regression tree? Is there any 'danger' in doing this?</p>

<p>For a standard regression model, how i deal with the seasonality if the new points will not be incorporated?</p>

<p>I'm using R, thanks!</p>
"
"0.27050089040023","0.273861278752583","44617","<p>I'm trying to measure the impact that rainfall causes in the number of incoming calls in a insurance-company. I have 4 years of daily data.</p>

<p>The plots below shown the correlation plot for each year:
<img src=""http://i.stack.imgur.com/KTdSX.png"" alt=""enter image description here""></p>

<p>The same plots as above, but now taken the weekly-mean for each variable:
<img src=""http://i.stack.imgur.com/0iZnQ.png"" alt=""enter image description here""></p>

<p>The rainfall has a <strong>yearly</strong> seasonality and the call-center data has a <strong>weekly pattern</strong>. The idea is to come up with a <em>weekly-based model</em>, so that i can measure the impact that a <em>weekly mean rainfall forecast</em> will cause.</p>

<p>Plotting the whole dataset, taken weekly-means (image below):
<img src=""http://i.stack.imgur.com/CrSb9.png"" alt=""enter image description here""></p>

<p>I'd like some suggestions on how to measure this variable 'impact' - i can try to split the rainfall data into 3 categories (low, normal, high), then build some model.</p>

<p>Thanks for any help! (i'm using R for analysis).</p>
"
"0.191273013919001","0.193649167310371","190487","<p>I recently found this paper </p>

<p><a href=""https://static.googleusercontent.com/media/www.google.com/en//googleblogs/pdfs/google_predicting_the_present.pdf"" rel=""nofollow"">https://static.googleusercontent.com/media/www.google.com/en//googleblogs/pdfs/google_predicting_the_present.pdf</a></p>

<p>where the authors predict economic trends with Google Search Data. At the end of the script, there's an nice description of the R script. Now, I'm still a beginner in R and little new to forecasting. There are three questions yet unanswered.</p>

<p>Assume we have (a) 3-year Google Trends data for ""umbrella"" search (weekly) and (b) 3-year data on the total amount of rain in liters (weekly). For (b) ""weekly"" means that this is the <em>sum</em> of rain for a given week, <em>not the average</em>.</p>

<p><strong>Question 1:</strong> Is it o.k. to use Google Trends data for (a) and compare it with a total amount in (b)? In the paper examples, it says ""The example is based on Fordâ€™s <em>monthly</em> sales.""</p>

<p><strong>Question 2:</strong> Is this test in any way dependent upon stationarity of variables? As there are seasonal trends, I assume it's correct to log(b)?</p>

<p><strong>Question 3:</strong> Provided I have two datasets that have the <em>same frequency</em> (both weekly), can I still apply nowcasting or is a Granger-causality more appropriate?</p>
"
"NaN","NaN","45018","<p>I would like to use the stlf forecast function from the R package forecast (http://cran.r-project.org/web/packages/forecast/index.html) and include regressors in the model.</p>"
"NaN","NaN","<p>Question 1: This can only be done using  method=arima   - right?</p>",""
"NaN","NaN","<p>Question 2: For the model calibration the parameter xreg should work but how can I enter the new data for th forecast? newxreg did not work for me.</p>",""
"NaN","NaN","<p>Thank you</p>",""
"NaN","NaN","","<r><regression><forecasting>"
"0.110431526074847","0.111803398874989","154030","<p>Suppose I observe a random variable $Y$ for a co-variable $p\in\{70,90,100,...,170\}$.</p>

<p><strong>My goal</strong> is create a forecast of $\mathbb{E}(Y)$ for $p\in\{50,70,...,350\}$, i.e., a wider range of $p$ as compared to the observed values of $p$.</p>

<p>Lets assume that $Y\sim Poi\big(\lambda(p)\big)$.</p>

<p>Using this information and a B-Spline (though I could also use just a linear $p$ I'll use a B-Spline to motivate my problem) to fit $\lambda(p)$ I'll get :</p>

<p><img src=""http://i.stack.imgur.com/KsMDa.jpg"" alt=""enter image description here""></p>

<p>Every point corresponds to the naive ML-Estimator, i.e., equals to $\bar{y}$ for a fixed value of $p$. The dotted curve shows the smoothed version using the B-spline setting.</p>

<p><strong>My question</strong> is how do I calculate $\widehat{\lambda}(350)$?</p>

<p>As B-Splines are only defined locally I would use linear extrapolation. </p>

<p>R's <code>gam</code>-function (<code>mgcv</code>-package) calls (if a B-spline is used) the function <code>spline.des</code>which calculates derivatives at the boundary knots of the interior knot-interval to calculate the extrapolation. </p>

<p>I did not find any reference which explains how this is done mathematically to get a better inside.</p>
"
"0.110431526074847","0.111803398874989","101451","<p>I have data like this: </p>

<pre><code>pce        pop        psavert    uempmed    unemploy
507.8      198712     9.8        4.5        2944
510.9      198911     9.8        4.7        2945
516.7      199113     9.8        4.6        2958
513.3      199311     9.8        4.9        3143
518.5      199498     9.8        4.7        3066
</code></pre>

<p>I am trying to use SVM - regression to fit the data like this</p>

<pre><code>svmRbftune &lt;- train(unemploy ~ pce + pop + psavert + uempmed,
                    data = EconomicsTrain, method = ""svmRadial"",
                    tunelength = 14, trControl = trainControl(method = ""cv""))

svmRbfPredict &lt;- predict(svmRbftune, EconomicsTest)
</code></pre>

<p>How can I forecast two to three steps ahead? </p>
"
"NaN","NaN","46246","<p>I would like to use tslm with data that has intraday seasonality and a different pattern on business days and on non-business days."
"NaN","NaN","If data.ts is my time series then I would like to use something like</p>",""
"NaN","NaN","<pre><code>tslm(data.ts~season|businesss.dummy)",""
"NaN","NaN","</code></pre>",""
"NaN","NaN","<p>Thus I want to model season given that the dummy for this hour is True or False.",""
"NaN","NaN","I don't want</p>",""
"NaN","NaN","<pre><code>tslm(data.ts~season + businesss.dummy)",""
"NaN","NaN","</code></pre>",""
"NaN","NaN","<p>as this would just give a parallel shift on business days.",""
"NaN","NaN","I know that I can subset the data before applying the model and thus get business day data and non-business day data only but can I achieve this aim more elegantly using the right formula in tslm?",""
"NaN","NaN","Thanks!</p>",""
"NaN","NaN","","<r><regression><time-series><forecasting>"
"0.27050089040023","0.273861278752583","114675","<p>I really want to understand how the math is working here. I am trying to get the standard error of the fitted values for a time series regression model. In the non-time series regression, I know I can take the transpose of the data multiplied by the variance - covariance matrix of the model coefficients and then multiply by the data values again to get the standard errors of the fitted values.</p>

<p>But I'm not sure how to do this when I am including an autoregressive term.</p>

<pre><code>require(forecast)
require(tserieS)
</code></pre>

<p>Response variable</p>

<pre><code>Sablects &lt;- rnorm(10)
</code></pre>

<p>Covariates</p>

<pre><code>my.xreg &lt;- cbind(rnorm(10),rbinom(10,1,0.5))
</code></pre>

<p>In my actual data, values are normalized so I set the intercept equal to zero here.</p>

<pre><code>m4&lt;-arima(Sablects, order=c(2,0,0),fixed=c(0,NA,0,NA,NA),xreg=my.xreg) 
</code></pre>

<p>The predict function will give me standard errors on my in-sample prediction (the fitted values of my model).</p>

<pre><code>my.se &lt;- predict(m4, newxreg = my.xreg, n.ahead = 10)$se         

my.se
</code></pre>

<p>Now to compare the output of my.se, I want to do this mathematically but I don't know what to use for the values of the ar2 term. I use 1's as a placeholder to demonstrate that my output does not equal the values from <code>my.se</code> above</p>

<pre><code>C &lt;- cbind(rep(1, nrow(my.xreg)), my.xreg[, 1], my.xreg[, 2])

C
</code></pre>

<p>I think this value should equal the first value in my.se, but is not producing the same value as my.se</p>

<pre><code>sqrt(t(C[1, ]) %*% vcov(m4) %*% C[1, ])
</code></pre>

<p>Also, I'm not so great with matrix multiplication but here is my work around for getting all of the se values.</p>

<pre><code>se.output &lt;- matrix(nrow=nrow(C))
</code></pre>

<p>Specify that the max number of i is equal to number of rows of <code>C</code>.</p>

<pre><code>  for(i in 1:nrow(C)){

    # Loop through your multiplication for each row (i) of `C`. For each iteration, save the new data into the new row of se.output

    se.output[i] &lt;- sqrt(t(C[i, ]) %*% vcov(m4) %*% C[i, ])  
    }

se.output
</code></pre>
"
"0.110431526074847","0.111803398874989","215207","<p>I have a question regarding Dynamic regression linear models.
I wonder if it is possible to implement a MLR model (in R) using 'lm' and creating lagged values of predictors and dependent variables.
For example, considering the linear regression described at <a href=""https://www.otexts.org/fpp/5/1"" rel=""nofollow"">https://www.otexts.org/fpp/5/1</a> 
 (""Forecasting: principles and practice""), can I retrive lagged values for savings, income, etc. and the same output ""score"" variable and build a model like below?</p>

<p>require(dplyr)
log.savings_lag1&lt;-lag(log.savings, 1)
........
score_lag1&lt;-lag(score,1)</p>

<p>fit &lt;- lm(score ~ log.savings + log.income +
log.address + log.employed+ log_savings_lag1,score_lag1, data=creditlog)</p>

<p>If this feasible? There is any particular concern I have to be aware?
Thanks in advance!</p>
"
"0.220863052149693","0.223606797749979","103280","<p>I'm attempting a project where I need to statistically rank available cars based on several variables such as cost, mpg, seating, milage, etc.. I wish to rank these cars in order decide which car would be the best choice (highest ""worth"") to buy (or best several cars if I was informing multiple people of the best cars to get). As the list of available cars changes from day to day, I will also need to re-run the code every day to allow the rankings to give me the best decision for this new day. </p>

<p>What statistical methods should I use to go about this ranking system? I plan on determining which factors I find most important so the variables used will be subjective in choice. I thought about trying MDS or clustering but I didn't know if that would be relevant since I'm already subjectively determining what variables are to be used. I don't see how regression can be used since I can't get a handle on the ""worth"" of previous cars as that is what I'm trying to rank by. Also, I will be attempting this in R so any helpful packages/functions would be great to know as well.</p>

<p>Any help with how to go about this ranking scheme would be helpful as I'm at a loss.</p>

<p>Thanks so much</p>
"
"0.427699461384151","0.404145188432738","116145","<p>I have downloaded the daily stock Adjusted Close price of one stock from sep 2011 to till date. As per my study plan, I have plotted some basic plots to understand the daily stock Adjusted closing price.</p>

<p>Here is the xyplot of the stock closing price by date and the code used to plot(My x axis not visible).</p>

<pre><code>Stock_T=stocks[which(symbol=='Stock_T'),]
xyplot(Adj.Close~Date,type='l',data=Stock_T,main='Adj.Close Price of the Stock_T')
</code></pre>

<p><img src=""http://i.stack.imgur.com/Ivlmk.png"" alt=""Timeseries plot of the raw data- Adjusted Closing price of the Stock""></p>

<p>By seeing this plot, the closing price was stable for period but had sudden huge increase in the stock price, it might had some other indicator which caused this much change in the stock price. Now my objective is to learn some ARIMA modeling concepts using this stock prices and try to do some forecasting of the stock price for few weeks. </p>

<p>As I have basic knowledge in ARIMA modeling, and I learned in the books that we should have stationary series before applying the ARIMA Model.</p>

<p>So, now I have plotted the ACF and PACF of the above raw data timeseries.</p>

<pre><code>acf(Stock_T$Adj.Close)
</code></pre>

<p><img src=""http://i.stack.imgur.com/rpy8S.png"" alt=""Raw data ACF Plot""></p>

<pre><code>pacf(Stock_T$Adj.Close)
</code></pre>

<p><img src=""http://i.stack.imgur.com/QKQge.png"" alt=""Raw data PACF plot""></p>

<p>From the above ACF and PACF plot, the series is not stationary and have huge autocorrelation (please correct me if am wrong), by differencing the series we will have stationary series (please correct me if am wrong). Here is the below plot.</p>

<pre><code>Stock_T_d1=diff(Stock_T$Adj.Close)
</code></pre>

<p><img src=""http://i.stack.imgur.com/sD9Tj.png"" alt=""First difference of the raw series""></p>

<p>Here the differencing series and its ACF AND PACF plots. ACF plot shows that there is no auto correlation and the series is stationary (please correct me if I am wrong) but I am unable to interpret the PACF plots, can someone explain it to me?  </p>

<p><img src=""http://i.stack.imgur.com/cAoVf.png"" alt=""ACF plot of Difference series""></p>

<p><img src=""http://i.stack.imgur.com/U10g8.png"" alt=""PACF plot of Difference series""></p>

<p>The above difference series shows some unequal variance in the series and so I am taking log transformation before differencing and its ACF and PACF.</p>

<pre><code>Stock_T_logd1=diff(log(Stock_T$Adj.Close))
</code></pre>

<p><img src=""http://i.stack.imgur.com/MWovq.png"" alt=""Difference Logged series ""></p>

<p><img src=""http://i.stack.imgur.com/oYd2d.png"" alt=""ACF of Difference logged Series""></p>

<p><img src=""http://i.stack.imgur.com/HQxZw.png"" alt=""PACF of Difference logged Series""></p>

<p>Now I will try to ask my questions.</p>

<ol>
<li>Should we have stationary series before we apply ARIMA?</li>
<li>Could you please explain me the ACF and PACF of the original series, and what we should do if we have this kind of series?</li>
<li>Could you please explain me the ACF and PACF of the difference series, and what will be the next step?</li>
<li>Could you please explain me the ACF and PACF of the difference logged series, and what will be the next step?</li>
<li>Should we use difference series or difference logged series?</li>
<li>What will be the ARIMA orders of this series?</li>
<li>Is there any R code to find the ARIMA order automatically of the original series?</li>
</ol>
"
"0.156173761888606","0.158113883008419","175035","<p>I'm using ARIMA models to estimate sales forecast for a company. The company's sales channel is broken down into 4 sales channels and I'm running 4 different models to estimate the sales for each channel. Eventually, I'm going to aggregate the sales of these channels to find the total forecasted sales for the whole company. My questions is, how should i go about finding the confidence interval for the overall forecast? Adding up the confidence intervals of each channel is not correct since that will give me a very large interval.</p>

<p>I'd really appreciate if anyone can give me some idea on how to approach this sort of issue. Thanks in advance!</p>
"
"0.156173761888606","0.158113883008419","95832","<p>I have two data sets </p>

<ol>
<li>Train data  </li>
<li>Test data (with no dependent variable values but I
have data on independent variable or you can say I need to
forecast).</li>
</ol>

<p>Using the training data (which has some <code>NA</code>s in the cell) I performed ordinary least square regression (OLS) using <code>lm()</code> in R and fitted the model &amp; I got the $\beta $ coefficients of the regression model. (All is good so far!)</p>

<p>Now, in the process of prediction for the fitted values, I have some missing values for some cells in the test dataset. I used function <code>predict()</code> as follows: </p>

<pre><code> predict(ols, test_data.df, interval= ""prediction"", na.action=na.pass)
</code></pre>

<p>for the cell (or cells) with <code>NA</code> value the entire row is discarded in generating the output (<code>yhat</code>). Is there any function that could generate the <code>yhat</code> values (other than <code>NA</code>s) for the test data without discarding any rows with missing value in the cell. </p>
"
"0.220863052149693","0.223606797749979","178787","<p>Im really new in regression estimation but my problem here is about forecasting confrontation. </p>

<p>This is my model:</p>

<p>$Y_t = \beta_0 + \beta_1 X_t + \epsilon_t$ </p>

<p>My OLS estimation using r function ""lm"" was:</p>

<pre><code>set.seed(123)
data &lt;- matrix(rnorm(50*2),nrow=50)
m &lt;- data.frame(data )


Model1&lt;- lm(X1 ~ X2 -1 , data = m)
&gt; Modelo1$coef
        X2 
-0.0296194 
</code></pre>

<p>My Quantile Regression (Median, $\tau = 0.5$) was:</p>

<pre><code>&gt; ModeloRQ1&lt;-rq(X1 ~ X2 -1, tau = 0.5,method=""br"", data=m) 

&gt; ModeloRQ1$coef
        X2 
-0.1256418 
</code></pre>

<p>The estimation procedure i understand. </p>

<p>But the Forecasting Procedure i dont understand. 
I know that after making the forecast i should compare using RMSFE statistics, for example.</p>

<p>But when i use the ""forecast"" function gives me the same point forecast (same values) when i use ""predict"" function.</p>

<p>I have read some papers which do not detail this procedure. Only say that ""OLS and QR (0.5) forecasts are Confronted against each other"". </p>

<p>How should i do this procedure? Simply by using the function predict/forecasting? this would be a commonly used procedure?</p>
"
"0.156173761888606","0.158113883008419","180217","<p>I'm using time series data containing both trend and seasonality. I also have 2 endogenous predictor variables that I would like to include in my model.</p>

<p>In R I've used the forecast package to develop a dynamic regression model with use of <code>auto.arima()</code> and the <code>xreg</code> argument from the <code>forecast package</code>. I understand this procedure takes a regression and then attempts to fit the residuals with an ARMA Model.</p>

<p>I've also developed what seems to be an appropriate model using the forecasting Module in SPSS by specifying a Seasonal ARIMA model and including my covariates. However, one of the coefficients on one of my endogeneous predictors has a negative sign which makes no sense intuitively. </p>

<p>I've read Dr. Hyndman's article <a href=""http://robjhyndman.com/hyndsight/arimax/"" rel=""nofollow"">The ARIMAX model muddle</a> and found it to be extremely insightful and useful. However, I have not been able to find any documentation on what type of statistical procedure SPSS uses to fit an ARIMA model with covariates, so I'm not sure how I should interpret the coefficients or how concerned I should be with a flipped sign. Any help clarifying the modelling procedure used by SPSS would be tremendously appreciated. </p>
"
"0.246932399162397","0.25","225094","<p>In the <code>Arima()</code> method, in the <code>forecast</code> package in R, I can provide a vector of parameters to the <code>fixed</code> argument, and the model is estimated while ensuring the provided parameters are fixed to the supplied values.</p>

<p>However, when I do this, the model returns no standard errors for these coefficients. Why is this the case? Is it not possible to estimate standard errors of coefficients that are manually provided? Would love an explanation as to why this might be the case.</p>

<p>Moreover, the <code>forecast</code> method still calculates confidence intervals when forecasting from a model that has fixed parameters. Are these intervals still statistically valid? I would have thought such would rely on the standard errors of the estimated coefficients, which it seems we may not know in the case of manually-entered parameters?</p>
"
