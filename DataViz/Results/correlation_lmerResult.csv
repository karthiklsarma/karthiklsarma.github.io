"V1","V2","V3","V4"
"0.0552368176666107","0.0538968055636295","  3757","<p>In my data, the RT (gaze) of individuals (ID) is examined as a function of a visual conditions, the factor size (small, medium, large). 
Base model:</p>

<pre><code>print(Base &lt;- lmer(RT ~ Size + (1|ID), data=rt), cor=F)
</code></pre>

<p>Random effect:</p>

<pre><code>print(NoCor &lt;- lmer(RT ~ Size + (0+Size|ID) , data=rt))
print(WithCor &lt;- lmer(RT ~ Size + (1+Size|ID), data=rt))
</code></pre>

<p>Addition of ID slopes improves the Base model. My question is, how can a significant random effect (Size/ID) be interpreted when there is no relationship between the random and fixed effect, i.e., when the correlation between the random factor and the fixed facor does not improve the model [the anova(NoCr, WithCor) does not show a significant improvement]?</p>
"
"0.158655679740622","0.154806788004275","  3874","<p>I have data from patients treated with 2 different kinds of treatments during surgery.
I need to analyze its effect on heart rate. 
The heart rate measurement is taken every 15 minutes. </p>

<p>Given that the surgery length can be different for each patient, each patient can have between 7 and 10 heart rate measurements. 
So an unbalanced design should be used. 
I'm doing my analysis using R. And have been using the ez package to do repeated measure mixed effect ANOVA. But I do not know how to analyse unbalanced data. Can anyone help?</p>

<p>Suggestions on how to analyze the data are also welcomed.</p>

<p>Update:<br>
As suggested, I fitted the data using the <code>lmer</code> function and found that the best model is:</p>

<pre><code>heart.rate~ time + treatment + (1|id) + (0+time|id) + (0+treatment|time)
</code></pre>

<p>with the following result:</p>

<pre><code>Random effects:
 Groups   Name        Variance   Std.Dev. Corr   
 id       time        0.00037139 0.019271        
 id       (Intercept) 9.77814104 3.127002        
 time     treat0      0.09981062 0.315928        
          treat1      1.82667634 1.351546 -0.504 
 Residual             2.70163305 1.643665        
Number of obs: 378, groups: subj, 60; time, 9

Fixed effects:
             Estimate Std. Error t value
(Intercept) 72.786396   0.649285  112.10
time         0.040714   0.005378    7.57
treat1       2.209312   1.040471    2.12

Correlation of Fixed Effects:
       (Intr) time  
time   -0.302       
treat1 -0.575 -0.121
</code></pre>

<p>Now I'm lost at interpreting the result. 
Am I right in concluding that the two treatments differed in affecting heart rate? What does the correlation of -504 between treat0 and treat1 means?</p>
"
"0.095672974646988","0.0933520056018673","  5270","<p>I have some data which, after lots of searching, I concluded would probably benefit from using a linear mixed effects model. I think I have an interesting result here, but I am having a little trouble figuring out how to interpret all of the results. This is what I get from the summary() function in R:</p>

<pre><code>&gt; summary(nonzero.lmer)
Linear mixed model fit by REML 
Formula: relative.sents.A ~ relative.sents.B + (1 | id.A) + (1 | abstract) 
   Data: nonzero 
    AIC    BIC logLik deviance REMLdev
 -698.8 -683.9  354.4   -722.6  -708.8
Random effects:
 Groups   Name        Variance   Std.Dev. 
 id.A     (Intercept) 1.0790e-04 0.0103877
 abstract (Intercept) 3.0966e-05 0.0055647
 Residual             2.9675e-04 0.0172263
Number of obs: 146, groups: id.A, 97; abstract, 52

Fixed effects:
                 Estimate Std. Error t value
(Intercept)      0.017260   0.003046   5.667
relative.sents.B 0.428808   0.080050   5.357

Correlation of Fixed Effects:
            (Intr)
rltv.snts.B -0.742
</code></pre>

<p>My question involves the relationship between the dependent variable (""relative.sents.A"") and ""relative.sents.B"" once the random factors are factored out. I gather that the t-value of 5.357 for relative.sents.B should be significant.</p>

<p>But does this show what the direction of the effect is? I am thinking that because the coefficient for the slope is positive that this means that as relative.sents.B increases, so does my dependent variable. Is this correct?</p>

<p>The book I've been using briefly mentions that the correlation reported here is not a normal correlation, but goes into no details. Normally, I'd look there to figure out the direction and magnitude of the effect. Is that wrong?</p>

<p>If I'm wrong on both counts, then what is a good (hopefully reasonably straightforward) way to discover the direction and size of the effect?</p>
"
"0.143509461970482","0.140028008402801","  5333","<p>I have a linear mixed-effect model which I hope will answer the question of whether an increase in the frequency of use of one word leads to an increase of the frequency of use of that word by another person in a conversation, factoring out random effects of subject and topic of conversation. The basic model I've come up with looks like this:</p>

<pre><code>Linear mixed model fit by REML 
Formula: relative.sents.A ~ relative.sents.B + (1 | id.A) + (1 | abstract) 
   Data: nonzero 
    AIC    BIC logLik deviance REMLdev
 -698.8 -683.9  354.4   -722.6  -708.8
Random effects:
 Groups   Name        Variance   Std.Dev. 
 id.A     (Intercept) 1.0790e-04 0.0103877
 abstract (Intercept) 3.0966e-05 0.0055647
 Residual             2.9675e-04 0.0172263
Number of obs: 146, groups: id.A, 97; abstract, 52

Fixed effects:
                 Estimate Std. Error t value
(Intercept)      0.017260   0.003046   5.667
relative.sents.B 0.428808   0.080050   5.357

Correlation of Fixed Effects:
            (Intr)
rltv.snts.B -0.742
</code></pre>

<p>The ""dependent"" variable is relative frequency of use by one person, and the fixed variable is relative frequency of use by another. I decided to see what the R^2 would be:</p>

<pre><code>&gt; cor(nonzero$relative.sents.A, fitted(nonzero.lmer))^2
[1] 0.6705905
</code></pre>

<p>To see what proportion of this is due to the fixed effect, I made a new model with only the random effects:</p>

<pre><code>&gt; summary(r.only.lmer)
Linear mixed model fit by REML 
Formula: relative.sents.A ~ 1 + (1 | id.A) + (1 | abstract) 
   Data: nonzero 
    AIC    BIC logLik deviance REMLdev
 -678.2 -666.3  343.1   -696.7  -686.2
Random effects:
 Groups   Name        Variance   Std.Dev. 
 id.A     (Intercept) 1.2868e-04 0.0113435
 abstract (Intercept) 7.8525e-06 0.0028022
 Residual             3.7643e-04 0.0194017
Number of obs: 146, groups: id.A, 97; abstract, 52

Fixed effects:
            Estimate Std. Error t value
(Intercept) 0.029149   0.002088   13.96
</code></pre>

<p>...and tried the same thing:</p>

<pre><code>&gt; cor(nonzero$relative.sents.A, fitted(r.only.lmer))^2
[1] 0.6882534
</code></pre>

<p>To my surprise, without that fixed effect, R^2 seems to increase!</p>

<p>Does this mean my model is useless? If so, any suggestions on what might be wrong? Or am I somehow misinterpreting these results? </p>
"
"0.108482956331022","0.0882093540041605","  5517","<p>The library languageR provides a method (pvals.fnc) to do MCMC significance testing of the fixed effects in a mixed effect regression model fit using lmer.  However, pvals.fnc gives an error when the lmer model includes random slopes.  </p>

<p>Is there a way to do an MCMC hypothesis test of such models?<br>
If so, how?  (To be accepted an answer should have a worked example in R)
If not, is there a conceptual/computation reason why there is no way?</p>

<p>This question might be related to <a href=""http://stats.stackexchange.com/questions/152/is-there-a-standard-method-to-deal-with-label-switching-problem-in-mcmc-estimatio"">this one</a> but I didn't understand the content there well enough to be certain.</p>

<p><strong>Edit 1</strong>: A proof of concept showing that pvals.fnc() still does 'something' with lme4 models, but that it doesn't do anything with random slope models.</p>

<pre><code>library(lme4)
library(languageR)
#the example from pvals.fnc
data(primingHeid) 
# remove extreme outliers
primingHeid = primingHeid[primingHeid$RT &lt; 7.1,]
# fit mixed-effects model
primingHeid.lmer = lmer(RT ~ RTtoPrime * ResponseToPrime + Condition + (1|Subject) + (1|Word), data = primingHeid)
mcmc = pvals.fnc(primingHeid.lmer, nsim=10000, withMCMC=TRUE)
#Subjects are in both conditions...
table(primingHeid$Subject,primingHeid$Condition)
#So I can fit a model that has a random slope of condition by participant
primingHeid.lmer.rs = lmer(RT ~ RTtoPrime * ResponseToPrime + Condition + (1+Condition|Subject) + (1|Word), data = primingHeid)
#However pvals.fnc fails here...
mcmc.rs = pvals.fnc(primingHeid.lmer.rs)
</code></pre>

<p>It says: Error in pvals.fnc(primingHeid.lmer.rs) : 
  MCMC sampling is not yet implemented in lme4_0.999375
  for models with random correlation parameters</p>

<p>Additional question:  Is pvals.fnc performing as expected for random intercept model?  Should the outputs be trusted?</p>
"
"0.151272255204013","0.147602480923349","  6224","<p>I used the <code>lmer</code> function in the <code>lme4</code> package in order to assess the effects of 2 categorical fixed effects (1Âº Animal Group: rodents and ants; 2Âº Microhabitat: bare soil and under cover) on seed predation (a count dependent variable). I have 2 Sites, with 10 trees per site and 4 seed stations per tree. Site and Tree are my (philosophically) random factors, but given that I have only two level for Site, it must be treated as a fixed factor. I have questions about how to interpret the results:  </p>

<ol>
<li>I made a model selection criterion based on QAICc, but the best model (lower QAICc) does not result in any significant fixed effect and other models with higher QAIC (e.g. the Full Model) did find significant fixed factors. Does this make sense?  </li>
<li>Given a fixed factor that is important to the model, how do I distinguish which level of fixed factor is influencing the response variable?  </li>
</ol>

<p>Finally, correlation between the fixed factors implies an incorrect estimation of the model?  </p>

<pre><code>FullModel=lmer(SeedPredation ~ AnimalGroup*Microhabitat*Site + (1|Site:Tree) + 
                                   (1|obs), data=datos,  family=""poisson"") 

QAICc(FM)104.9896

    enterGeneralized linear mixed model fit by the Laplace approximation 
Formula: SP ~ AG * MH * Site + (1 | Site:Tree) + (1 | obs) 
   Data: datos 
   AIC   BIC logLik deviance
 101.8 125.6  -40.9     81.8
Random effects:
 Groups    Name        Variance Std.Dev.
 obs       (Intercept) 0.20536  0.45317 
 Site:Tree (Intercept) 1.19762  1.09436 
Number of obs: 80, groups: obs, 80; Site:Tree, 20

Fixed effects:
                 Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)       0.01161    0.47608   0.024   0.9805  
AGR             -18.97679 3130.76500  -0.006   0.9952  
MHUC             -1.60704    0.63626  -2.526   0.0115 *
Site2            -0.91424    0.74506  -1.227   0.2198  
AGR:MHUC         19.92369 3130.76508   0.006   0.9949  
AGR:Site2         1.02241 4431.84919   0.000   0.9998  
MHUC:Site2        1.80029    0.86235   2.088   0.0368 *
AGR:MHUC:Site2   -3.49042 4431.84933  -0.001   0.9994  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Correlation of Fixed Effects:
            (Intr) AGR    MHUC   Site2  AGR:MHUC AGR:S2 MHUC:S
AGR          0.000                                            
MHUC        -0.281  0.000                                     
Site2       -0.639  0.000  0.180                              
AGR:MHUC     0.000 -1.000  0.000  0.000                       
AGR:Site2    0.000 -0.706  0.000  0.000  0.706                
MHUC:Site2   0.208  0.000 -0.738 -0.419  0.000    0.000       
AGR:MHUC:S2  0.000  0.706  0.000  0.000 -0.706   -1.000  0.000 code here

BestModel=lmer(SP ~ AG * MH + (1|Site:Tree) + (1|obs), data=datos,  
               family = ""poisson"") 

QAICc(M) 101.4419

Generalized linear mixed model fit by the Laplace approximation 
Formula: SP ~ AG + AG:MH + (1 | Site:Tree) + (1 | obs) 
   Data: datos 
   AIC   BIC logLik deviance
 100.3 114.6 -44.15     88.3
Random effects:
 Groups    Name        Variance Std.Dev.
 obs       (Intercept) 0.76027  0.87194 
 Site:Tree (Intercept) 1.14358  1.06938 
Number of obs: 80, groups: obs, 80; Site:Tree, 20

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   -0.5153     0.4061  -1.269    0.205
AGR          -18.7146  2603.4397  -0.007    0.994
AGA:MHUC      -0.7301     0.5045  -1.447    0.148
AGR:MHUC      17.7221  2603.4397   0.007    0.995
</code></pre>
"
"0.126563449052859","0.105851224804993"," 18088","<p>Suppose I have some measurement for each subject at each site. Two variables, subject and site, are of interest in terms of computing intraclass correlation (ICC) values. Typically I would use function <code>lmer</code> from R package <code>lme4</code>, and run</p>

<pre><code>lmer(measurement ~ 1 + (1 | subject) + (1 | site), mydata)
</code></pre>

<p>The ICC values can be obtained from the variances for the random effects in the above model.</p>

<p>However, I recently read a paper that really puzzles me. Using the above example, the authors calculated three ICC values in the paper with function lme from nlme package: one for subject, one for site, and one for the interaction of subject and site. No further details were given in the paper. I'm confused from the following two perspectives:</p>

<ol>
<li>How to calculate the ICC values with lme? I don't know how to specify those three random effects (subject, site, and their interaction) in lme.</li>
<li>Is it really meaningful to consider the ICC for the interaction of subject and site? From modeling or theoretical perspective, you can calculate it, but conceptually I have trouble interpreting such an interaction.</li>
</ol>
"
"0.135302018298348","0.13201967239689"," 18497","<p>Wikipedia suggests that one way to look at <a href=""http://en.wikipedia.org/wiki/Inter-rater_reliability#Intra-class_correlation_coefficient"" rel=""nofollow"">inter-rater reliability</a> is to use a random effects model to compute <a href=""http://en.wikipedia.org/wiki/Intra-class_correlation_coefficient#.22Modern.22_ICCs"" rel=""nofollow"">intraclass correlation</a>.  The example of intraclass correlation talks about looking at</p>

<p>$$\frac{\sigma_\alpha^2}{\sigma_\alpha^2+\sigma_\epsilon^2}$$</p>

<p>from a model</p>

<p>$$Y_{ij} = \mu + \alpha_i + \epsilon_{ij}$$</p>

<p>""where Y<sub>ij</sub> is the j<sup>th</sup> observation in the i<sup>th</sup> group, Î¼ is an unobserved overall mean, Î±<sub>i</sub> is an unobserved random effect shared by all values in group i, and Îµ<sub>ij</sub> is an unobserved noise term.""</p>

<p>This is an attractive model especially because in my data no rater has rated all things (although most have rated 20+), and things are rated a variable number of times (usually 3-4).</p>

<p>Question #0: Is ""group i"" in that example (""group i"") a grouping of things being rated?</p>

<p>Question #1: If I'm looking for inter-rater-reliability, don't I need a random effects model with two terms, one for the rater, and one for the thing rated?  After all, both have possible variation.</p>

<p>Question #2: How would I best express this model in R?</p>

<p>It looks as if <a href=""http://stats.stackexchange.com/questions/18088/intraclass-correlation-icc-for-an-interaction"">this question</a> has a nice-looking proposal:</p>

<pre><code>lmer(measurement ~ 1 + (1 | subject) + (1 | site), mydata)
</code></pre>

<p>I looked at a <a href=""http://stats.stackexchange.com/questions/10905/how-to-specify-random-effects-in-lme"">couple</a> <a href=""http://stackoverflow.com/questions/1380694/how-to-fit-a-random-effects-model-with-subject-as-random-in-r"">questions</a>, and the syntax of the ""random"" parameter for lme is opaque to me.  I read the <a href=""http://stat.ethz.ch/R-manual/R-patched/library/nlme/html/lme.html"" rel=""nofollow"">help page for lme</a>, but the description for ""random"" is incomprehensible to me without examples.</p>

<p>This question is somewhat similar to a <a href=""http://stats.stackexchange.com/questions/14004/intra-and-inter-rater-reliability-on-the-same-data"">long</a> <a href=""http://stats.stackexchange.com/questions/3539/inter-rater-reliability-for-ordinal-or-interval-data"">list</a> of <a href=""http://stats.stackexchange.com/questions/4009/inter-rater-reliability-using-intra-class-correlation-with-ratings-for-multiple"">questions</a>, with <a href=""http://stats.stackexchange.com/questions/3539/inter-rater-reliability-for-ordinal-or-interval-data"">this</a> the closest.  However, most don't address R in detail.</p>
"
"0.136145029683612","0.132842232831014"," 19772","<p>Can someone please tell me how to have R estimate the break point in a piecewise linear model (as a fixed or random parameter), when I also need to estimate other random effects? </p>

<p>I've included a toy example below that fits a hockey stick / broken stick regression with random slope variances and a random y-intercept variance for a break point of 4. I want to estimate the break point instead of specifying it. It could be a random effect (preferable) or a fixed effect.</p>

<pre><code>library(lme4)
str(sleepstudy)

#Basis functions
bp = 4
b1 &lt;- function(x, bp) ifelse(x &lt; bp, bp - x, 0)
b2 &lt;- function(x, bp) ifelse(x &lt; bp, 0, x - bp)

#Mixed effects model with break point = 4
(mod &lt;- lmer(Reaction ~ b1(Days, bp) + b2(Days, bp) + (b1(Days, bp) + b2(Days, bp) | Subject), data = sleepstudy))

#Plot with break point = 4
xyplot(
        Reaction ~ Days | Subject, sleepstudy, aspect = ""xy"",
        layout = c(6,3), type = c(""g"", ""p"", ""r""),
        xlab = ""Days of sleep deprivation"",
        ylab = ""Average reaction time (ms)"",
        panel = function(x,y) {
        panel.points(x,y)
        panel.lmline(x,y)
        pred &lt;- predict(lm(y ~ b1(x, bp) + b2(x, bp)), newdata = data.frame(x = 0:9))
            panel.lines(0:9, pred, lwd=1, lty=2, col=""red"")
        }
    )
</code></pre>

<p>Output:</p>

<pre><code>Linear mixed model fit by REML 
Formula: Reaction ~ b1(Days, bp) + b2(Days, bp) + (b1(Days, bp) + b2(Days, bp) | Subject) 
   Data: sleepstudy 
  AIC  BIC logLik deviance REMLdev
 1751 1783 -865.6     1744    1731
Random effects:
 Groups   Name         Variance Std.Dev. Corr          
 Subject  (Intercept)  1709.489 41.3460                
          b1(Days, bp)   90.238  9.4994  -0.797        
          b2(Days, bp)   59.348  7.7038   0.118 -0.008 
 Residual               563.030 23.7283                
Number of obs: 180, groups: Subject, 18

Fixed effects:
             Estimate Std. Error t value
(Intercept)   289.725     10.350  27.994
b1(Days, bp)   -8.781      2.721  -3.227
b2(Days, bp)   11.710      2.184   5.362

Correlation of Fixed Effects:
            (Intr) b1(D,b
b1(Days,bp) -0.761       
b2(Days,bp) -0.054  0.181
</code></pre>

<p><img src=""http://i.stack.imgur.com/HnAfg.jpg"" alt=""Broken stick regression fit to each individual""></p>
"
"0.0828552264999161","0.0808452083454443"," 22988","<p>I use lme4 in R to fit the mixed model</p>

<pre><code>lmer(value~status+(1|experiment)))
</code></pre>

<p>where value is continuous, status and experiment are factors, and I get</p>

<pre><code>Linear mixed model fit by REML 
Formula: value ~ status + (1 | experiment) 
  AIC   BIC logLik deviance REMLdev
 29.1 46.98 -9.548    5.911    19.1
Random effects:
 Groups     Name        Variance Std.Dev.
 experiment (Intercept) 0.065526 0.25598 
 Residual               0.053029 0.23028 
Number of obs: 264, groups: experiment, 10

Fixed effects:
            Estimate Std. Error t value
(Intercept)  2.78004    0.08448   32.91
statusD      0.20493    0.03389    6.05
statusR      0.88690    0.03583   24.76

Correlation of Fixed Effects:
        (Intr) statsD
statusD -0.204       
statusR -0.193  0.476
</code></pre>

<p>How can I know that the effect of status is significant? R reports only $t$-values and not $p$-values.</p>
"
"0.197234889993286","0.192450089729875"," 24452","<p>I hope you all don't mind this question, but I need help interpreting output for a linear mixed effects model output I've been trying to learn to do in R. I am new to longitudinal data analysis and linear mixed effects regression. I have a model I fitted with weeks as the time predictor, and score on an employment course as my outcome. I modeled score with weeks (time) and several fixed effects, sex and race. My model includes random effects. I need help understanding what the variance and correlation means. The output is the following:</p>

<pre><code>Random effects  
Group   Name    Variance  
EmpId intercept 680.236  
weeks           13.562  
Residual 774.256  
</code></pre>

<p>The correlaton is .231.</p>

<p>I can interpret the correlation as there is a a positive relationship between weeks and score but I want to be able to say it in terms of ""23% of ..."".</p>

<p>I really appreciate the help. </p>

<hr>

<p>Thanks ""guest"" and Macro for replying. Sorry, for not replying, I was out at a conference and Iâ€™m now catching up. 
Here is the output and the context. </p>

<p>Here is the summary for the LMER model I ran. </p>

<pre><code>&gt;summary(LMER.EduA)  
Linear mixed model fit by maximum likelihood  
Formula: Score ~ Weeks + (1 + Weeks | EmpID)   
   Data: emp.LMER4 

  AIC     BIC   logLik   deviance   REMLdev   
 1815     1834  -732.6     1693    1685

Random effects:    
 Groups   Name       Variance Std.Dev. Corr  
 EmpID   (Intercept)  680.236  26.08133        
          Weeks         13.562 3.682662  0.231   
 Residual             774.256  27.82546        
Number of obs: 174, groups: EmpID, 18


Fixed effects:    
            Estimate Std. Error  t value  
(Intercept)  261.171      6.23     37.25    
Weeks          11.151      1.780    6.93

Correlation of Fixed Effects:  
     (Intr)  
Days -0.101
</code></pre>

<p>I donâ€™t understand how to interpret the variance and residual for the random effects and explain it to someone else. I also donâ€™t know how to interpret the correlation, other than it is positive which indicates that those with higher intercepts have higher slopes and those with those with lower intercepts have lower slopes but I donâ€™t know how to explain the correlation in terms of 23% of . . . . (I donâ€™t know how to finish the sentence or even if it makes sense to do so). This is a different type analysis for us as we (me) are trying to move into longitudinal analyses. </p>

<p>I hope this helps.</p>

<p>Thanks for your help so far. </p>

<p>Zeda</p>
"
"0.15190124858318","0.161690416690889"," 24844","<p>I am running 3 models on 3 subsets of the same data.  The set up is as follows:</p>

<ol>
<li>Outcome (DV) is binary categorical</li>
<li>Time (IV) is repeated twice (pre and post)</li>
<li>Treatement (IV of interest) is binary categorical</li>
</ol>

<p>I am interested to know if at time 2 treatment has had an effect on outcome.  I used the lme4 package and used the following R code:</p>

<pre><code>tot.null&lt;-lmer(as.factor(outcome)~Time+(1|id), family=binomial(link='logit'),
             data=df.total)
tot.mod&lt;-lmer(as.factor(outcome)~trt*Time+(Time|id), 
             family=binomial(link='logit'), data=df.total)
anova(tot.null,tot.mod)
summary(tot.mod)
</code></pre>

<p><strong>Data head</strong></p>

<pre><code>   id             trt Time outcome
1   1 peer discussion   -1       1
2   2 peer discussion   -1       1
3   3 peer discussion   -1       0
4   4 peer discussion   -1       1
5   5 peer discussion   -1       1
</code></pre>

<p><strong>str of data</strong></p>

<pre><code>&gt; str(df.total)
'data.frame':   872 obs. of  4 variables:
 $ id     : int  1 2 3 4 5 6 7 8 9 10 ...
     $ trt    : Factor w/ 2 levels ""peer discussion"",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ Time   : num  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...
     $ outcome: num  1 1 1 1 1 1 1 0 1 0 ...
</code></pre>

<p>The problem is I get an error messoge on the <code>tot.mod</code>:</p>

<pre><code>&gt; tot.mod&lt;-glmer(as.factor(outcome)~trt*Time+(Time|id), family=binomial(link='logit'),
               data=df.total)
Warning message:
In mer_finalize(ans) : false convergence (8)
</code></pre>

<p>I think this is the reason the model is significant but none of the predictors are.  look at the inflated SEs.</p>

<p><strong>Comparison to the null model and the summary of full model</strong></p>

<pre><code>&gt; anova(tot.null,tot.mod)
Data: df.total
Models:
tot.null: as.factor(outcome) ~ Time + (1 | id)
tot.mod: as.factor(outcome) ~ trt * Time + (Time | id)
         Df    AIC    BIC  logLik  Chisq Chi Df            Pr(&gt;Chisq)    
tot.null  3 689.54 703.85 -341.77                                        
tot.mod   7 410.67 444.07 -198.34 286.86      4 &lt; 0.00000000000000022 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
&gt; summary(tot.mod)
Generalized linear mixed model fit by the Laplace approximation 
Formula: as.factor(outcome) ~ trt2 * Time + (Time | id) 
   Data: df.total 
   AIC   BIC logLik deviance
 410.7 444.1 -198.3    396.7
Random effects:
 Groups Name        Variance Std.Dev. Corr  
 id     (Intercept)  396.46  19.911         
        Time        1441.98  37.973   0.470 
Number of obs: 872, groups: id, 436

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept) 10.09866    3.33921   3.024  0.00249 **
trt21        0.01792    5.10796   0.004  0.99720   
Time        -0.93753    5.79560  -0.162  0.87149   
trt21:Time  -0.84882   10.41073  -0.082  0.93502   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Correlation of Fixed Effects:
           (Intr) trt21  Time  
trt21      -0.654              
Time        0.558 -0.365       
trt21:Time -0.311  0.473 -0.557
</code></pre>

<p>What's going on?  Why is the model significant but none of the betas?  In OLS I know this is an indicator of multi-colinearity among predictors.  I don't think that's the reason here.  Please help with understanding this problem as well as the error message (I think they may be connected).  What are some things I should check for?</p>

<p>The other two  models from the same data set (<code>split</code> on a different grouping variable) had no apparent problems.</p>

<p>Thank you in advance.</p>

<p><em>Using R 2.14.2, lme4 v. 0.999375-42 on a win 7 machine</em> </p>
"
"0.166202907140464","0.162170924189109"," 25371","<p>I am using the <code>glmer()</code> function from the lme4 package to run a GLMM using the poisson distribution.  In all the examples that I see, the random effects part of the output has a residual part that has been estimated from the data (surrounded by 2 asterisks on either side in the example below).  This information can then be used in interpreting the amount of variation explained by the random effect.  Here is an example:</p>

<pre><code>&gt; summary(M1)
Linear mixed model fit by REML 
Formula: Richness ~ NAP * fExp + (1 | fBeach) 
Data: RIKZ 
AIC   BIC    logLik   deviance REMLdev
236.5 247.3 -112.2    230.3    224.5
Random effects:
Groups   Name          Variance Std.Dev.
fBeach   (Intercept)   3.3072   1.8186  
**Residual             8.6605   2.9429**
Number of obs: 45, groups: fBeach, 9

Fixed effects:
              Estimate Std. Error t value
(Intercept)   8.8611     1.0208   8.681
NAP          -3.4637     0.6279  -5.517
fExp11       -5.2556     1.5451  -3.401
NAP:fExp11    2.0005     0.9461   2.114

Correlation of Fixed Effects:
           (Intr) NAP    fExp11
NAP        -0.181              
fExp11     -0.661  0.120       
NAP:fExp11  0.120 -0.664 -0.221
</code></pre>

<p>However, when I use my own data, I get output that does not include this information, and I am not sure why.  I want to know how much variation is explained by my random effects, but can't figure out how to access the information necessary to answer the question.  Any clues?  Is this a data/statistics issue or is this a knowing how to access the information issue?  I apologize if I'm asking in the wrong place.  The output I get looks similar to the following output:</p>

<pre><code>Generalized linear mixed model fit by the Laplace approximation 
Formula: y ~ z.score(x1) + z.score(x2) + z.score(x3) + z.score(x4) + z.score(x5) +      z.score(x6) + (1 | RE) 
Data: p 
AIC   BIC logLik deviance
419.5 454.7 -201.8    403.5
Random effects:
Groups Name        Variance Std.Dev.
RE     (Intercept) 0.021605 0.14699 
Number of obs: 600, groups: RE, 40

Fixed effects:
                  Estimate   Std. Error z value Pr(&gt;|z|)    
(Intercept)       1.70591    0.02911    58.60   &lt; 2e-16 ***
z.score(x1)       0.19087    0.03595    5.31    1.10e-07 ***
z.score(x2)      -0.14302    0.04083   -3.50    0.000460 ***
z.score(x3)      -0.16562    0.04020   -4.12    3.79e-05 ***
z.score(x4)       0.13229    0.03425    3.86    0.000112 ***
z.score(x5)      -0.10588    0.03985   -2.66    0.007885 ** 
z.score(x6)       0.17600    0.05798    3.04    0.002401 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Correlation of Fixed Effects:
            (Intr) z.(x1) z.(x2 z.s(x3) z.(x4 z.(x5
z.scr(x1)  -0.051                                   
z.s(x2)     0.038  0.259                            
z.scr(x3)   0.045  0.156  0.113                     
z.(x4      -0.040  0.144 -0.052  0.044              
z.(x5       0.026 -0.368 -0.339 -0.072 -0.073       
z.scor(x6) -0.031 -0.020  0.002 -0.143 -0.004  0.004
</code></pre>

<p>Here is some sample data, to be fit with <code>glmer(y ~ x1 + (1|RE), data=d, family=poisson)</code>.</p>

<pre><code>d &lt;- data.frame(
  y  =  c(3, 5, 2, 6, 3, 7, 2, 3, 0, 4, 0,10, 1, 4, 0, 4, 2, 3, 0, 6, 
          3, 4, 2, 3, 2, 3, 3, 4, 0, 5, 6, 5, 4, 4, 0, 3, 1, 6, 0, 3, 2, 
          2, 1, 6, 2, 7, 0, 2, 0, 4, 0, 6, 4, 5, 1, 5, 1, 4, 1, 2, 3, 6, 
          6, 7, 0, 5, 0, 9, 1, 4, 5, 6, 1, 7, 1, 4, 1, 4, 0, 4, 1, 6, 1, 
          4, 0, 7, 1, 4, 0, 6, 0, 7, 2, 6, 0, 6, 1, 5, 0, 4, 1, 7, 2, 4, 
          1, 5, 1, 7, 2, 5, 0, 4, 3, 5, 1, 4, 0, 3, 0, 6, 0, 8, 3, 9, 0, 
          2, 3, 8, 0, 1, 0, 3, 0, 5, 0, 4, 4, 5, 0, 5, 1, 5, 3, 5, 1, 4, 
          3, 4, 4, 4, 4, 4, 4, 7, 1, 8, 1, 4, 0, 2, 2, 5, 1, 4, 1, 5, 1, 
          4, 2, 4, 2, 4, 0, 6, 1, 6, 0, 6, 1, 2, 1, 3, 1, 8, 1, 6, 1, 6, 
          0, 6, 1, 6, 2, 6, 2, 4, 0, 1, 1, 1, 1, 6, 5, 5, 1, 5, 2, 4, 2, 
          6, 1, 7, 1, 8, 2, 8, 1, 8, 2, 4, 1, 7, 3, 6, 4, 7, 3, 7, 1, 6, 
          3, 5, 1,10, 1, 7, 2, 5, 1, 5, 0, 6, 1, 8, 4, 7, 1, 6, 1, 9, 
          0, 9, 1, 3, 2, 5, 2, 9, 3, 5, 0, 2, 2, 3, 0, 5, 0, 5, 0, 4, 3, 
          6, 1,10, 2, 8, 0, 6, 0, 4, 2, 6, 2, 4, 2, 6, 1, 4, 0, 5, 2, 
          6, 1, 5, 2, 5, 1, 5, 1, 5),
  x1 = rep(c(0.1008, 0.0511, 0.1792, 1.0345), c(80, 80, 80, 60)),
  RE = rep(c(37, 88, 139, 190, 241, 292, 343, 394, 91, 142, 193, 244, 295, 
             346, 397, 40, 94, 145, 43, 196, 247, 298, 349, 400, 301, 352, 
             403, 250, 148, 199, 46, 97, 355, 406, 253, 304, 49, 100, 151, 
             202, 37, 88, 139, 190, 241, 292, 343, 394, 91, 142, 193, 244, 
             295, 346, 397, 40, 43, 94, 145, 247, 298, 349, 196, 400, 199, 
             250, 301, 352, 406, 46, 97, 148, 403, 49, 100, 151, 202, 253, 
             304, 355, 37, 88, 139, 190, 241, 292, 343, 394, 193, 244, 346, 
             397, 295, 40, 91, 142, 43, 94, 145, 196, 46, 97, 148, 151, 247, 
             400, 298, 349, 352, 199, 250, 301, 403, 253, 304, 355, 202, 406, 
             49, 100, 37, 88, 139, 190, 241, 292, 343, 394, 346, 397, 193, 
             244, 295, 40, 91, 142, 43, 94, 145, 196, 247, 298, 349, 400, 
             97, 148, 46, 199, 250, 301), each=2)
)
</code></pre>
"
"0.136145029683612","0.132842232831014"," 26401","<p>Here are some simulated data:</p>

<pre><code>    library(mvtnorm)
    I &lt;- 3 # positions (fixed factor)
    J &lt;- 4 # tubes (random factor)
    K &lt;- 4 # repeats 
    n &lt;- I*J*K
    set.seed(123)
    tube &lt;- rep(1:J, each=I)
    position &lt;- rep(LETTERS[1:I], times=J)
    Mu_i &lt;- 3*(1:I)
    Mu_ij &lt;- c(t(rmvnorm(J, mean=Mu_i)) )  
    tube &lt;- rep(tube, each=K)
    position &lt;- rep(position, each=K)
    Mu_ij &lt;- rep(Mu_ij, each=K)
    dat &lt;- data.frame(tube, position, Mu_ij)
    sigmaw &lt;- 2
    dat$y &lt;- rnorm(n, dat$Mu_ij, sigmaw)
    dat$tube &lt;- factor(dat$tube)

&gt; str(dat)
'data.frame':   48 obs. of  4 variables:
 $ tube    : Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;: 1 1 1 1 1 1 1 1 1 1 ...
     $ position: Factor w/ 3 levels ""A"",""B"",""C"": 1 1 1 1 2 2 2 2 3 3 ...
 $ Mu_ij   : num  2.44 2.44 2.44 2.44 6.13 ...
     $ y       : num  3.24 2.66 1.33 6.01 7.12 ...
&gt; head(dat)
  tube position    Mu_ij        y
1    1        A 2.439524 3.241067
2    1        A 2.439524 2.660890
3    1        A 2.439524 1.327842
4    1        A 2.439524 6.013351
5    1        B 6.129288 7.124989
6    1        B 6.129288 2.196053
</code></pre>

<p>I fit a mixed model with R, it works well:</p>

<pre><code>&gt; library(lme4)
&gt; lmer(y ~ position + (0+position|tube), data=dat)
Linear mixed model fit by REML 
Formula: y ~ position + (0 + position | tube) 
   Data: dat 
   AIC   BIC logLik deviance REMLdev
 212.6 231.3  -96.3    194.8   192.6
Random effects:
 Groups   Name      Variance Std.Dev. Corr          
 tube     positionA 0.30123  0.54885                
          positionB 0.68317  0.82654  -0.695        
          positionC 1.66666  1.29099  -0.408  0.940 
 Residual           3.14003  1.77201                
Number of obs: 48, groups: tube, 4

Fixed effects:
            Estimate Std. Error t value
(Intercept)   3.3533     0.5211   6.435
positionB     3.1098     0.8923   3.485
positionC     5.6138     1.0144   5.534

Correlation of Fixed Effects:
          (Intr) postnB
positionB -0.753       
positionC -0.651  0.744
</code></pre>

<p>But the same model does not work well with SAS:</p>

<pre><code>PROC MIXED DATA=dat ;
CLASS POSITION TUBE ;
MODEL y = POSITION ;
RANDOM POSITION / subject=TUBE type=UN G GCORR ;
RUN; QUIT;
</code></pre>

<p>gives</p>

<pre><code> Estimated G matrix is not positive definite.
                         Estimated G Matrix

               Row    Effect      position    tube        Col1        Col2        Col3

                 1    position    A           1        0.08895     -0.5823     -0.1545
                 2    position    B           1        -0.5823      0.1455      1.2431
                 3    position    C           1        -0.1545      1.2431      1.4835
</code></pre>

<p>Is it possible to remedy this failure ?</p>
"
"0.144232436127838","0.14073344364025"," 30803","<p>I will try to explain my data as good as possible.
So we taged 13 different whales with a tag that records time, depth, speed, angle of descent and ascent 25 samples every second.
The normal diving behaviour of these animals is one deep dive of one hour to 1200 meters followed by a series of 3-7 shallow dives of 20 minutes up to 300 m. Because the tag not always stays the same time in each animal, my data is unbalanced and some tag records have one deep dive and 6 shallow dives while other records have 7 deep dives and 26 shallow dives.
I divided each dive in units of 30 seconds. for each unit I have the next data:
whale number, dive number, total number of fluke strokes in the 30 seconds unit of analysis, mean of the sin of the angle during the 30 seconds unit, swim speed, dive type(ascent or descent), dive direction( if it is a descent or an ascent) and time since the start of the dive and finally my variable response which is presence or absence of one type of fluke stroke called stroke type B.</p>

<p>I think there has to be some autocorrelation between each 30 seconds unit of analyis and need to include it in my model but do not know how!</p>

<p>I am interested to know what affects the presence or absence of the type B stroke (which is a binomial variable with 0 and 1)
so I decide to use a  binomial glmm with whale number as a random effect. I included as well dive number within whale as a random effect.</p>

<p>here is the model.</p>

<pre><code>glmm114&lt;-lmer (StrokeB~ Time * Depth+SINP+flukes*Depth+speed+(1|whale_number)+(0+dive_number|whale_number),data=Luciadeepas, family = binomial)
</code></pre>

<p>this is my final model after taking out the non significative variables, the problem is that due to the interaction a problem appears saying</p>

<pre><code>The false convergence warning message (8)
</code></pre>

<p>I looked on internet and it says is a common problem, and some people says that it doesnt make any change in the output while others says that each variable has to be
divided by 100. but when I do this then the variables that become significant doesnt make any sense. </p>
"
"0.135302018298348","0.13201967239689"," 31204","<p>I want to model two different time variables, some of which are heavily collinear in my data (age + cohort = period). Doing this I ran into some trouble with <code>lmer</code> and and interactions of <code>poly()</code>, but it's probably not limited to <code>lmer</code>, I got the same results with <code>nlme</code> IIRC.</p>

<p>Obviously, my understanding of what the poly() function does is lacking. I understand what <code>poly(x,d,raw=T)</code> does and I thought without <code>raw=T</code> it makes orthogonal polynomials (I can't say I really understand what that means), which makes fitting easier, but doesn't let you interpret the coefficients directly.<br>
I <a href=""http://r.789695.n4.nabble.com/use-of-poly-td847784.html"">read</a> that because I'm using the predict function, the predictions should be the same.</p>

<p>But they aren't, even when the models converge normally. I'm using centered variables and I first thought that maybe the orthogonal polynomial leads to higher fixed effect correlation with the collinear interaction term, but it seems comparable. I've pasted two <a href=""https://gist.github.com/3002722"">model summaries over here</a>.</p>

<p>These plots hopefully illustrate the extent of the difference. I used the predict-function which is only available in the dev. version of lme4 (heard about it <a href=""http://stats.stackexchange.com/a/29749/2795"">here</a>), but the fixed effects are the same in the CRAN version (and they also seem off by themselves, e.g. ~ 5 for the interaction when my DV has a range of 0-4).</p>

<p>The lmer call was</p>

<pre><code>cohort2_age =lmer(churchattendance ~ 
poly(cohort_c,2,raw=T) * age_c + 
ctd_c + dropoutalive + obs_c + (1+ age_c |PERSNR), data=long.kg)
</code></pre>

<p>The prediction was fixed effects only, on fake data (all other predictors=0) where I marked the range present in the original data as extrapolation=F.</p>

<pre><code>predict(cohort2_age,REform=NA,newdata=cohort.moderates.age)
</code></pre>

<p>I can provide more context if need be (I didn't manage to produce a reproducible example easily, but can of course try harder), but I think this is a more basic plea: explain the <code>poly()</code> function to me, pretty please.</p>

<h3>Raw polynomials</h3>

<p><img src=""http://i.stack.imgur.com/2FIIO.jpg"" alt=""Raw polynomials""></p>

<h3>Orthogonal polynomials (clipped, nonclipped at <a href=""http://i.imgur.com/iFmLE.png"">Imgur</a>)</h3>

<p><img src=""http://i.stack.imgur.com/CvVdC.jpg"" alt=""Orthogonal polynomials""></p>
"
"0.229615139152771","0.214709612884295"," 32040","<p>I'm trying to analyze effect of Year on variable logInd for particular group of individuals (I have 3 groups). <strong>The simplest model:</strong></p>

<pre><code>&gt; fix1 = lm(logInd ~ 0 + Group + Year:Group, data = mydata)
&gt; summary(fix1)

Call:
lm(formula = logInd ~ 0 + Group + Year:Group, data = mydata)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.5835 -0.3543 -0.0024  0.3944  4.7294 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
Group1       4.6395740  0.0466217  99.515  &lt; 2e-16 ***
Group2       4.8094268  0.0534118  90.044  &lt; 2e-16 ***
Group3       4.5607287  0.0561066  81.287  &lt; 2e-16 ***
Group1:Year -0.0084165  0.0027144  -3.101  0.00195 ** 
Group2:Year  0.0032369  0.0031098   1.041  0.29802    
Group3:Year  0.0006081  0.0032666   0.186  0.85235    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.7926 on 2981 degrees of freedom
Multiple R-squared: 0.9717,     Adjusted R-squared: 0.9716 
F-statistic: 1.705e+04 on 6 and 2981 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>We can see the Group1 is significantly declining, the Groups2 and 3 increasing but not significantly so.</p>

<p><strong>Clearly the individual should be random effect, so I introduce random intercept effect for each individual:</strong></p>

<pre><code>&gt; mix1a = lmer(logInd ~ 0 + Group + Year:Group + (1|Individual), data = mydata)
&gt; summary(mix1a)
Linear mixed model fit by REML 
Formula: logInd ~ 0 + Group + Year:Group + (1 | Individual) 
   Data: mydata 
  AIC  BIC logLik deviance REMLdev
 4727 4775  -2356     4671    4711
Random effects:
 Groups     Name        Variance Std.Dev.
 Individual (Intercept) 0.39357  0.62735 
 Residual               0.24532  0.49530 
Number of obs: 2987, groups: Individual, 103

Fixed effects:
              Estimate Std. Error t value
Group1       4.6395740  0.1010868   45.90
Group2       4.8094268  0.1158095   41.53
Group3       4.5607287  0.1216522   37.49
Group1:Year -0.0084165  0.0016963   -4.96
Group2:Year  0.0032369  0.0019433    1.67
Group3:Year  0.0006081  0.0020414    0.30

Correlation of Fixed Effects:
            Group1 Group2 Group3 Grp1:Y Grp2:Y
Group2       0.000                            
Group3       0.000  0.000                     
Group1:Year -0.252  0.000  0.000              
Group2:Year  0.000 -0.252  0.000  0.000       
Group3:Year  0.000  0.000 -0.252  0.000  0.000
</code></pre>

<p>It had an expected effect - the SE of slopes (coefficients Group1-3:Year) are now lower and the residual SE is also lower.</p>

<p><strong>The individuals are also different in slope so I also introduced the random slope effect:</strong></p>

<pre><code>&gt; mix1c = lmer(logInd ~ 0 + Group + Year:Group + (1 + Year|Individual), data = mydata)
&gt; summary(mix1c)
Linear mixed model fit by REML 
Formula: logInd ~ 0 + Group + Year:Group + (1 + Year | Individual) 
   Data: mydata 
  AIC  BIC logLik deviance REMLdev
 2941 3001  -1461     2885    2921
Random effects:
 Groups     Name        Variance  Std.Dev. Corr   
 Individual (Intercept) 0.1054790 0.324775        
            Year        0.0017447 0.041769 -0.246 
 Residual               0.1223920 0.349846        
Number of obs: 2987, groups: Individual, 103

Fixed effects:
              Estimate Std. Error t value
Group1       4.6395740  0.0541746   85.64
Group2       4.8094268  0.0620648   77.49
Group3       4.5607287  0.0651960   69.95
Group1:Year -0.0084165  0.0065557   -1.28
Group2:Year  0.0032369  0.0075105    0.43
Group3:Year  0.0006081  0.0078894    0.08

Correlation of Fixed Effects:
            Group1 Group2 Group3 Grp1:Y Grp2:Y
Group2       0.000                            
Group3       0.000  0.000                     
Group1:Year -0.285  0.000  0.000              
Group2:Year  0.000 -0.285  0.000  0.000       
Group3:Year  0.000  0.000 -0.285  0.000  0.000
</code></pre>

<h3><strong>But now, contrary to the expectation, the SE of slopes (coefficients Group1-3:Year) are now much higher, even higher than with no random effect at all!</strong></h3>

<p>How is this possible? I would expect that the random effect will ""eat"" the unexplained variability and increase ""sureness"" of the estimate!</p>

<p>However, the residual SE behaves as expected - it is lower than in the random intercept model.</p>

<p><a href=""http://artax.karlin.mff.cuni.cz/~ttel5535/pub/my_so_question_data.R"" rel=""nofollow"">Here is the data</a> if needed.</p>

<h2>Edit</h2>

<p>Now I realized astonishing fact. If I do the linear regression for each individual separately and then run ANOVA on the resultant slopes, <strong>I get exactly the same result as the random slope model!</strong> Would you know why?</p>

<pre><code>indivSlope = c()
for (indiv in 1:103) {
    mod1 = lm(logInd ~ Year, data = mydata[mydata$Individual == indiv,])
    indivSlope[indiv] = coef(mod1)['Year']
}

indivGroup = unique(mydata[,c(""Individual"", ""Group"")])[,""Group""]


anova1 = lm(indivSlope ~ 0 + indivGroup)
summary(anova1)

Call:
lm(formula = indivSlope ~ 0 + indivGroup)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.176288 -0.016502  0.004692  0.020316  0.153086 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
indivGroup1 -0.0084165  0.0065555  -1.284    0.202
indivGroup2  0.0032369  0.0075103   0.431    0.667
indivGroup3  0.0006081  0.0078892   0.077    0.939

Residual standard error: 0.04248 on 100 degrees of freedom
Multiple R-squared: 0.01807,    Adjusted R-squared: -0.01139 
F-statistic: 0.6133 on 3 and 100 DF,  p-value: 0.6079 
</code></pre>

<p><a href=""http://artax.karlin.mff.cuni.cz/~ttel5535/pub/my_so_question_data.R"" rel=""nofollow"">Here is the data</a> if needed.</p>
"
"0.095672974646988","0.0933520056018673"," 32580","<p>While running a simulation in R, I noticed that in fitting one particular model, R spits out a warning message, but if I simply change the baseline category in the response variable, it converges without complaining. The results produced do not differ appreciably, except that one value in the random effect correlation matrix is changed. I am curious why this occurs, and whether it suggests something interesting about the data set (the response variable is binary valued, and the explanatory variables are continuous-valued).</p>

<pre><code>&gt; library(lme4)
&gt; fit1.MS.eE &lt;- glmer(label ~ zSpec * zF1 + zF2 + (1 + zSpec + zF1 + zF2 | part), data = MSall.eE, family = binomial())
Warning message:
In mer_finalize(ans) : singular convergence (7)
&gt; MSall.eE$label &lt;- relevel(MSall.eE$label, ""e"")
&gt; fit1.MS.eE &lt;- glmer(label ~ zSpec * zF1 + zF2 + (1 + zSpec + zF1 + zF2 | part), data = MSall.eE, family = binomial())
&gt;(no warning message)
</code></pre>

<p>I have uploaded <a href=""http://www.acsu.buffalo.edu/~lovegren/sample.R"" rel=""nofollow"">the data set</a> referred to in the code for interested persons.</p>
"
"0.047836487323494","0.0466760028009337"," 33995","<p>Can anyone tell me under what conditions the beta estimates differ between lm and lmer with a random intercept? I came across a situation where the fixed effect differed considerably. I thought the std errors should change but the fixed effects should remain unchanged. The difference does not seem to be due to having different cluster sizes or having a large number of clusters with a single observation.</p>

<p>I cannot supply the data but have concocted a simplified example below. In this case the correlation within clusters should be negligible. </p>

<pre><code>library(lme4)
x=c(rep(0,10),rep(1,10))
y=rnorm(length(x),mean=3100,sd=400)-200*x
m=c(1,2,3,4,4,6,7,8,8,10,11,12,13,14,15,16,17,18,20,20)
summary(lm(y~x))
summary(lmer(y~x+(1|m)))
</code></pre>
"
"0.118389266011054","0.115517213347279"," 34969","<p>Sorry if I'm missing something very obvious here but I am new to mixed effect modelling. </p>

<p>I am trying to model a binomial presence/absence response as a function of percentages of habitat within the surrounding area. My fixed effect is the percentage of the habitat and my random effect is the site (I mapped 3 different farm sites). </p>

<pre><code>glmmsetaside &lt;- glmer(treat~setas+(1|farm),
       family=binomial,data=territory)
</code></pre>

<p>When <code>verbose=TRUE</code>:</p>

<pre><code>0:     101.32427: 0.333333 -0.0485387 0.138083 
1:     99.797113: 0.000000 -0.0531503 0.148455  
2:     99.797093: 0.000000 -0.0520462 0.148285  
3:     99.797079: 0.000000 -0.0522062 0.147179  
4:     99.797051: 7.27111e-007 -0.0508770 0.145384  
5:     99.797012: 1.45988e-006 -0.0495767 0.141109  
6:     99.797006: 0.000000 -0.0481233 0.136883  
7:     99.797005: 0.000000 -0.0485380 0.138081  
8:     99.797005: 0.000000 -0.0485387 0.138083  
</code></pre>

<p>My output is this:</p>

<pre><code>Generalized linear mixed model fit by the Laplace approximation 
Formula: treat ~ setasidetrans + (1 | farm) 

AIC   BIC logLik deviance
105.8 112.6  -49.9     99.8
Random effects:
 Groups Name        Variance Std.Dev.
farm   (Intercept)  0        0  
Number of obs: 72, groups: farm, 3

Fixed effects:
Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   -0.04854    0.44848  -0.108    0.914
setasidetrans  0.13800    1.08539   0.127    0.899

Correlation of Fixed Effects:
            (Intr)
setasidtrns -0.851
</code></pre>

<p>I basically do not understand why my random effect is 0? Is it because the random effect only has 3 levels? I don't see why this would be the case. I have tried it with lots of different models and it always comes out as 0.</p>

<p>It cant be because the random effect doesn't explain any of the variation because I know the habitats are different in the different farms.</p>

<p>Here is an example set of data using <code>dput</code>:</p>

<pre><code>list(territory = c(1, 7, 8, 9, 10, 11, 12, 13, 14, 2, 3, 4, 5, 
6, 15, 21, 22, 23, 24, 25, 26, 27, 28, 16, 17, 18, 19, 20, 29, 
33, 34, 35, 36, 37, 38, 39, 40, 30, 31, 32, 41, 45, 46, 47, 48, 
49, 50, 51, 52, 42, 43, 44, 53, 55, 56, 57, 58, 59, 60, 61, 62, 
54, 63, 65, 66, 67, 68, 69, 70, 71, 72, 64), treat = c(1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 
0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 
0, 0, 0, 0, 0, 0, 0), farm = c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 
1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 
2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 
3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3), 
built = c(5.202332763, 1.445026852, 2.613422283, 2.261705833, 
2.168842186, 1.267473928, 0, 0, 0, 9.362387965, 17.55433115, 
4.58020626, 4.739300829, 8.638442377, 0, 1.220760647, 7.979990338, 
13.30789514, 0, 8.685544976, 3.71617163, 0, 0, 6.802926951, 
8.925512803, 8.834006678, 4.687723044, 9.878232478, 8.097800267, 
0, 0, 0, 0, 5.639651095, 9.381654651, 8.801754791, 5.692392532, 
3.865304919, 4.493438554, 4.826277798, 3.650995554, 8.20818417, 
0, 8.169597157, 8.62030666, 8.159474015, 8.608979238, 0, 
8.588288678, 7.185700856, 0, 0, 3.089524893, 3.840381223, 
31.98103158, 5.735501995, 5.297691011, 5.17141191, 6.007539933, 
2.703345394, 4.298077606, 1.469986793, 0, 4.258511595, 0, 
21.07029581, 6.737664009, 14.36176373, 3.056631919, 0, 32.49289428, 
0)
</code></pre>

<p>It goes on with around 10 more columns for different types of habitat (like <code>built</code>, <code>setaside</code> is one of them) with percentages in it.</p>
"
"0.136145029683612","0.147602480923349"," 38177","<p>I have a GLMM with Poisson distribution and random spatial block. My experimental design is 2x2 factorial, with 4 blocks, resulting in 16 total data points. Here is the specification of the model in R using the lme4 package.</p>

<pre><code>lmer(rich ~ morph*caged + (1|block), family=poisson, data=bexData)
</code></pre>

<p>When I call summary on this object, I am returned</p>

<pre><code>   AIC   BIC logLik deviance
 18.58 22.44 -4.288    8.576
Random effects:
 Groups Name        Variance Std.Dev.
 block  (Intercept)  0        0      
Number of obs: 16, groups: block, 4
</code></pre>

<p>I have left out the fixed effect parameter tests and correlations for brevity.</p>

<p>Here are my primary questions:</p>

<ol>
<li><p>Can you use this output to calculate overdispersion?  </p>

<ul>
<li>I have read that overdispersion can be calculated as the residual deviance divided by the residual degrees of freedom. Is that 8.576 / (16 - 4)? (Zuur et al., Mixed Effects Models)</li>
</ul></li>
<li>If this calculation is correct, the estimator phi = 0.715. This indicates that there is not overdispersion in my data. 
<ul>
<li>Does this indicate that there is underdispersion? </li>
<li>Is this a problem? </li>
<li>Can anybody offer advice as to thresholds for over/underdispersion at which corrections to the models should be made? Zuur has said in one book that 5 is a common cutoff. Do people agree with that?</li>
<li>How can such corrections be made?</li>
</ul></li>
<li>I've also noticed here that the variance for the random effect is 0. 
<ul>
<li>Does this mean that there are precisely <em>no</em> error correlations between data points within my blocking factor?</li>
<li>If this is so, why would a generalised linear model of the form shown at bottom have an AIC substantially higher, around 55?</li>
<li>is AIC a reasonable method for choosing GLMM over GLM (as suggested by Zuur)?</li>
</ul></li>
</ol>

<p>.</p>

<pre><code>glm(rich ~ morph*caged, data=bexData, family=poisson)
</code></pre>
"
"0.0828552264999161","0.0808452083454443"," 38188","<p>I'm a new user of mixed models and through the material I've been reading there are always probability values (p>t) or (p>z) that estimate the importance of a level of a factor in the model. However, when using the <code>lmer()</code> function in R, which supposedly gives you those probabilities, I simply don't find them. Here is the output: </p>

<pre><code>Linear mixed model fit by REML 
Formula: Temp ~ depth + (1 | locality) 
   Data: qminmatrix 
   AIC   BIC logLik deviance REMLdev
 561.3 581.3 -273.7    551.5   547.3
Random effects:
 Groups   Name        Variance Std.Dev.
 locality (Intercept) 4.7998   2.1909  
 Residual             4.0433   2.0108  
Number of obs: 128, groups: locality, 4

Fixed effects:
            Estimate Std. Error t value
(Intercept)  22.0103     1.1500  19.140
depth1        1.9564     0.6832   2.864
depth10       2.6624     0.5756   4.625
depth5        3.0209     0.4932   6.125
depthWS      -2.2585     0.5444  -4.149

Correlation of Fixed Effects:
        (Intr) depth1 dpth10 depth5
depth1  -0.157                     
depth10 -0.175  0.189              
depth5  -0.213  0.313  0.458       
depthWS -0.191  0.334  0.373  0.441
</code></pre>
"
"0.143509461970482","0.140028008402801"," 38524","<p>I got completely different results from lmer() and lme()! Just look at the coefficients' std.errors. Completely different in both cases. Why is that and which model is correct?</p>

<pre><code>&gt; mix1c = lmer(logInd ~ 0 + crit_i + Year:crit_i + (1 + Year|Taxon), data = datai)
&gt; mix1d = lme(logInd ~ 0 + crit_i + Year:crit_i, random = ~ 1 + Year|Taxon, data = datai)
&gt; 
&gt; summary(mix1d)
Linear mixed-effects model fit by REML
 Data: datai 
       AIC      BIC    logLik
  4727.606 4799.598 -2351.803

Random effects:
 Formula: ~1 + Year | Taxon
 Structure: General positive-definite, Log-Cholesky parametrization
            StdDev       Corr  
(Intercept) 9.829727e-08 (Intr)
Year        3.248182e-04 0.619 
Residual    4.933979e-01       

Fixed effects: logInd ~ 0 + crit_i + Year:crit_i 
                 Value Std.Error   DF   t-value p-value
crit_iA      29.053940  4.660176   99  6.234515  0.0000
crit_iF       0.184840  3.188341   99  0.057974  0.9539
crit_iU      12.340580  5.464541   99  2.258301  0.0261
crit_iW       5.324854  5.152019   99  1.033547  0.3039
crit_iA:Year -0.012272  0.002336 2881 -5.253846  0.0000
crit_iF:Year  0.002237  0.001598 2881  1.399542  0.1618
crit_iU:Year -0.003870  0.002739 2881 -1.412988  0.1578
crit_iW:Year -0.000305  0.002582 2881 -0.118278  0.9059
 Correlation: 
             crit_A crit_F crit_U crit_W cr_A:Y cr_F:Y cr_U:Y
crit_iF       0                                              
crit_iU       0      0                                       
crit_iW       0      0      0                                
crit_iA:Year -1      0      0      0                         
crit_iF:Year  0     -1      0      0      0                  
crit_iU:Year  0      0     -1      0      0      0           
crit_iW:Year  0      0      0     -1      0      0      0    

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-6.98370498 -0.39653580  0.02349353  0.43356564  5.15742550 

Number of Observations: 2987
Number of Groups: 103 
&gt; summary(mix1c)
Linear mixed model fit by REML 
Formula: logInd ~ 0 + crit_i + Year:crit_i + (1 + Year | Taxon) 
   Data: datai 
  AIC  BIC logLik deviance REMLdev
 2961 3033  -1469     2893    2937
Random effects:
 Groups   Name        Variance   Std.Dev. Corr   
 Taxon    (Intercept) 6.9112e+03 83.13360        
          Year        1.7582e-03  0.04193 -1.000 
 Residual             1.2239e-01  0.34985        
Number of obs: 2987, groups: Taxon, 103

Fixed effects:
               Estimate Std. Error t value
crit_iA      29.0539403 18.0295239   1.611
crit_iF       0.1848404 12.3352135   0.015
crit_iU      12.3405800 21.1414908   0.584
crit_iW       5.3248537 19.9323887   0.267
crit_iA:Year -0.0122717  0.0090916  -1.350
crit_iF:Year  0.0022365  0.0062202   0.360
crit_iU:Year -0.0038701  0.0106608  -0.363
crit_iW:Year -0.0003054  0.0100511  -0.030

Correlation of Fixed Effects:
            crit_A crit_F crit_U crit_W cr_A:Y cr_F:Y cr_U:Y
crit_iF      0.000                                          
crit_iU      0.000  0.000                                   
crit_iW      0.000  0.000  0.000                            
crit_iA:Yer -1.000  0.000  0.000  0.000                     
crit_iF:Yer  0.000 -1.000  0.000  0.000  0.000              
crit_iU:Yer  0.000  0.000 -1.000  0.000  0.000  0.000       
crit_iW:Yer  0.000  0.000  0.000 -1.000  0.000  0.000  0.000
&gt; 
</code></pre>
"
"0.151272255204013","0.147602480923349"," 38591","<p>I'm trying to compute a mixed model using jags (R2jags) and I got very strange divergence. The chains started so well, very well according to the results expected (also see <code>lmer</code> output of the same model below). But at certain point, the chains went crazy. Just look at the traceplot for <strong>delta_tau</strong> variable - the chains start so well, but then the green chain goes crazy, then blue and finally red... </p>

<p>Any ideas why this happens? Can't be in initial values, because the chains started so well. Maybe the priors? Why is the system unstable?</p>

<p><img src=""http://i.stack.imgur.com/rn08d.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/v2jof.png"" alt=""enter image description here""></p>

<p><strong>EDIT:</strong> Variables <code>gamma_tau</code> and <code>delta_tau</code> don't fall to exact zero, as you can see on these zoomed-in figures:</p>

<p><img src=""http://i.stack.imgur.com/qkORJ.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/gdEpz.png"" alt=""enter image description here""></p>

<p>This is the jags model:</p>

<pre><code>model {

# likelihood
for (i in 1:N) {
    logInd[i] ~ dnorm(mu[i], eps_tau)
    mu[i] &lt;- alpha[crit[i]] + (beta[crit[i]] + delta[species[i]])*year[i] + gamma[species[i]] # ekviv mix1b/c podle me
}

# priors
eps_tau ~ dgamma(1.0E-3, 1.0E-3) 

for (j in 1:no_crit) {
    alpha[j] ~ dnorm(0, 0.0001)
    beta[j] ~ dnorm(0, 0.0001) 
}

for (k in 1:no_species) {
    gamma[k] ~ dnorm(0, gamma_tau)
    delta[k] ~ dnorm(0, delta_tau)
}

gamma_tau ~ dgamma(1.0E-3, 1.0E-3) 
delta_tau ~ dgamma(1.0E-3, 1.0E-3)
}
</code></pre>

<p>Code used to run jags (using R2jags):</p>

<pre><code>no_crit = length(levels(crit))

win.data = list(logInd = mydata$logInd, crit = (as.integer(crit)), 
    	year = mydata$Year, species = (as.integer(mydata$Taxon)),
    	N = nrow(mydata), no_crit = no_crit, no_species = length(levels(mydata$Taxon))
)

inits = function () { list(
    alpha = rnorm(no_crit, 0, 10000),
    beta = rnorm(no_crit, 0, 10000)
)}  

params = c(""alpha"", ""beta"", ""eps_tau"", ""gamma_tau"", ""delta_tau"")

# ni: 1000 -&gt; .. sec
ni &lt;- 20000
nt &lt;- 8
nb &lt;- 8000
nc &lt;- 3

out &lt;- R2jags::jags(win.data, inits, params, ""model.txt"",
    nc, ni, nb, nt,  
    working.directory = paste(getwd(), ""/tmp_bugs/"", sep = """")
)
R2jags::traceplot(out, mfrow = c(4, 2))
</code></pre>

<p>Here is output from the equivalent <code>lmer</code> model:</p>

<pre><code>&gt; summary(lmer(logInd ~ 0 + crit_i + Year:crit_i + (1 + Year|Taxon), data = datai2))
Linear mixed model fit by REML 
Formula: logInd ~ 0 + crit_i + Year:crit_i + (1 + Year | Taxon) 
   Data: datai2 
  AIC  BIC logLik deviance REMLdev
 8558 8630  -4267     8495    8534
Random effects:
 Groups   Name        Variance   Std.Dev.   Corr  
 Taxon    (Intercept) 1.1682e-12 1.0808e-06       
          Year        5.3860e-07 7.3389e-04 0.000 
 Residual             8.7038e-01 9.3294e-01       
Number of obs: 2987, groups: Taxon, 103

Fixed effects:
               Estimate Std. Error t value
crit_iA      29.0539403  8.8116915   3.297
crit_iF       0.1848404  6.0286726   0.031
crit_iU      12.3405800 10.3326242   1.194
crit_iW       5.3248537  9.7416915   0.547
crit_iA:Year -0.0122717  0.0044174  -2.778
crit_iF:Year  0.0022365  0.0030222   0.740
crit_iU:Year -0.0038701  0.0051799  -0.747
crit_iW:Year -0.0003054  0.0048836  -0.063

Correlation of Fixed Effects:
            crit_A crit_F crit_U crit_W cr_A:Y cr_F:Y cr_U:Y
crit_iF      0.000                                          
crit_iU      0.000  0.000                                   
crit_iW      0.000  0.000  0.000                            
crit_iA:Yer -0.999  0.000  0.000  0.000                     
crit_iF:Yer  0.000 -0.999  0.000  0.000  0.000              
crit_iU:Yer  0.000  0.000 -0.999  0.000  0.000  0.000       
crit_iW:Yer  0.000  0.000  0.000 -0.999  0.000  0.000  0.000
</code></pre>

<p>Thanks in advance!</p>
"
"0.135302018298348","0.13201967239689"," 38616","<p>One of the problems I've always had with mixed models is figuring out data visualizations - of the kind that could end up on a paper or poster - once one has the results.</p>

<p>Right now, I'm working on a Poisson mixed effects model with a formula that looks something like the following:</p>

<p><code> a &lt;- glmer(counts ~ X + Y + Time + (Y + Time | Site) + offset(log(people))</code></p>

<p>With something fitted in glm() one could easily use the predict() to get predictions for a new data set, and build something off of that. But with output like this - how would you construct something like a plot of the rate over time with the shifts from X (and likely with a set value of Y)? I think one could predict the fit well enough just from the Fixed effects estimates, but what about the 95% CI?</p>

<p>Is there anything else someone can think of that help visualize results? The results of the model are below:</p>

<pre><code>Random effects:
 Groups     Name        Variance   Std.Dev.  Corr          
 Site       (Intercept) 5.3678e-01 0.7326513               
            time        2.4173e-05 0.0049167  0.250        
            Y           4.9378e-05 0.0070270 -0.911  0.172 

Fixed effects:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -8.1679391  0.1479849  -55.19  &lt; 2e-16
X            0.4130639  0.1013899    4.07 4.62e-05
time         0.0009053  0.0012980    0.70    0.486    
Y            0.0187977  0.0023531    7.99 1.37e-15

Correlation of Fixed Effects:
         (Intr) Y    time
X      -0.178              
time    0.387 -0.305       
Y      -0.589  0.009  0.085
</code></pre>
"
"0.172476907882941","0.168292721432467"," 40459","<p>I conducted a computer-based assessment of different methods of fitting a particular type of model used in the palaeo sciences. I had a large-ish training set and so I randomly (stratified random sampling) set aside a test set. I fitted $m$ different methods to the training set samples and using the $m$ resulting models I predicted the response for the test set samples and computed a RMSEP over the samples in the test set. This is a single <strong>run</strong>.</p>

<p>I then repeated this process a large number of times, each time I chose a different training set by randomly sampling a new test set.</p>

<p>Having done this I want to investigate if any of the $m$ methods has better or worse RMSEP performance. I also would like to do multiple comparisons of the pair-wise methods.</p>

<p>My approach has been to fit a linear mixed effects (LME) model, with a single random effect for <strong>Run</strong>. I used <code>lmer()</code> from the <strong>lme4</strong> package to fit my model and functions from the <strong>multcomp</strong> package to perform the multiple comparisons. My model was essentially</p>

<pre><code>lmer(RMSEP ~ method + (1 | Run), data = FOO)
</code></pre>

<p>where <code>method</code> is a factor indicating which method was used to generate the model predictions for the test set and <code>Run</code> is an indicator for each particular <strong>Run</strong> of my ""experiment"".</p>

<p>My question is in regard to the residuals of the LME. Given the single random effect for <strong>Run</strong> I am assuming that the RMSEP values for that run are correlated to some degree but are uncorrelated between runs, on the basis of the induced correlation the random effect affords.</p>

<p>Is this assumption of independence <em>between</em> runs valid? If not is there a way to account for this in the LME model or should I be looking to employ another type of statical analysis to answer my question?</p>
"
"0.10696563746014","0.104370715180858"," 43634","<p>I have 299 surveys collected from 299 individuals working at 26 different locations. I want to understand how the location specific features relate to the individual survey scores.  The only inference I have as to location features is gathered from the individual survey scores.  Is it a valid strategy to calculate means for each location based on the individual scores, and include this as a level 2 variable?  Further, does it also make sense to include the same variable but as the level 1 variable, with slope varying freely between locations, if I want to compare the relative usefulness of the mean (best estimate of 'reality') to a persons individual score? (their perception of reality and response biases).  </p>

<p>I feel like I may have some circularity in the logic.  My implementation in R for one of the variables of interest follows, any feedback is welcome!</p>

<pre><code>lmer(X21~X25+meanX25+(X25|X1),data=datai)
Linear mixed model fit by REML 
Formula: X21 ~ X25 + meanX25 + (X25 | X1) 
   Data: datai 
  AIC  BIC logLik deviance REMLdev
 1079 1105 -532.7     1056    1065
Random effects:
 Groups   Name        Variance Std.Dev. Corr   
 X1       (Intercept) 0.384983 0.62047         
          X25         0.012382 0.11127  -1.000 
 Residual             1.936068 1.39143 
Number of obs: 299, groups: X1, 26
Fixed effects:
            Estimate Std. Error t value
(Intercept)  1.13616    0.38013   2.989
X25          0.56683    0.05265  10.766
meanX25      0.33897    0.12213   2.775

Correlation of Fixed Effects:
        (Intr) X25   
X25     -0.119       
meanX25 -0.838 -0.389
</code></pre>
"
"0.172476907882941","0.168292721432467"," 45278","<p>I m working on a piecewise linear growth model and I need help to understand how to write my <code>lmer()</code> code and how to interpret the <code>R</code> output.</p>

<p>My data are the sales return of different IDs over a period of time. I want to know how the sales-return (growth) changes after a certain event (breakpoint).
To define the breakpoint I inserted a coded variable.</p>

<pre><code>df = data.frame (
  ID = c(1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2),
  sales = c(1,4,10,12,20,26,28,30,31,32,33,2,5,9,12,15,19,26,27,29,31,32,34,36),
  var1 = c(1,2,3,3,3,3,3,3,3,3,3,1,2,3,4,4,4,4,4,4,4,4,4,4),
  var2 = c(0,0,0,0,0,0,1,2,3,4,5,0,0,0,0,0,0,0,1,2,3,4,5,6))
</code></pre>

<p>I need to apply a Multi-Level Model -- a 2-level-model to be more precise -- using the <code>lme4</code> package. I'm looking for the correct <code>lmer()</code> code to estimate this equation:</p>

<p>$$Y_{ti} = \pi _{0i} + (\gamma _{00}+\varepsilon _{0i})a_{1ti} + (\gamma _{10}+\varepsilon _{1i})a_{2ti} + e_{ti}$$</p>

<p>My data and variables:</p>

<blockquote>
  <p>level 1: individual ID-level    </p>
  
  <p>level 2: inter-individual level </p>
  
  <p>$\varepsilon _{0i}$: var1 (this is my first coded variable, period 1)</p>
  
  <p>$\varepsilon _{1i}$: var2 (this is my second coded variable, period 2)</p>
  
  <p>sales: dependent variable   ID: random effect (is this correct?)  </p>
  
  <p>var1 and var 2: fixed effects (is this correct?)</p>
</blockquote>

<p>I think the code for my model should be:</p>

<pre><code>test &lt;- lmer(sales ~ 0 + var1 + var2 + (1| ID), data=df)
</code></pre>

<p><strong>Q1:</strong> is this code appropriate?</p>

<p>My Output</p>

<pre><code>&gt; summary(test)
Linear mixed model fit by REML 
Formula: sales ~ 0 + var1 + var2 + (1 | ID) 
   Data: df 
   AIC   BIC logLik deviance REMLdev
 154.1 158.8 -73.05    147.8   146.1
Random effects:
 Groups   Name        Variance Std.Dev.
 ID       (Intercept) 11.646   3.4127  
 Residual             25.902   5.0894  
Number of obs: 24, groups: ID, 2

Fixed effects:
     Estimate Std. Error t value
var1   5.5828     0.7759   7.195
var2   3.5039     0.5646   6.206

Correlation of Fixed Effects:
     var1  
var2 -0.433
</code></pre>

<p><strong>Q2:</strong> Interpretation (sorry, I m not really familiar with statistical interpretation :( )<br>
Is it correct that:   </p>

<ul>
<li>fixed effects: var1 Estimate is my slope parameter for period 1?  </li>
<li>fixed effects: var2 Estimate is my slope parameter for period 2?  </li>
</ul>

<p>So I could say that the sales return growth is smaller in period 2 than in period 1?</p>
"
"0.192538147428251","0.187867287325545"," 48582","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/48696/generalized-linear-mixed-model-in-r-with-repeated-measures"">Generalized Linear Mixed Model in R with repeated measures</a>  </p>
</blockquote>



<p>I am trying to investigate how four variables (var1=continuous, var2=factor, var3=factor, var4=continuous) influence the number of trials individuals approached (out of total nr of trials --> binomial) across two conditions that differed in food availability (food availability 1 = 42 trials; food availability 8 = 35 trials) (n = 19 individuals). The response variable is binomial as it is the number of trials out of total number of trials. I am using the 'lmer' function of the lme4 package.</p>

<p>I thought the additive model I should run would be with random factor ID:</p>

<pre><code>glmer(cbind(appr_Y,appr_N) ~ Condition+Var1+Var2+Var3+Var4+(1|ID), data=dataset, 
      family=binomial)
</code></pre>

<p>However, the result I get shows that Condition is super significant (p &lt; 2e-16) while the other variables aren't, while exploring the data visually shows no difference in the response variable for Condition and the variables having strong effects.</p>

<p>Below a dummy representing the large data table:</p>

<pre><code>Con ID  Var1  Var2  appr_Y  appr_N  Trial_total
1   1   10      y   14      6       20
1   2   4       y   10      10      20
1   3   5       n   5       15      20
1   4   32      n   18      2       20
1   5   11      y   3       17      20
2   1   10      y   20      5       25
2   2   4       y   10      15      25
2   3   5       n   24      1       25
2   4   32      n   11      14      25  
2   5   11      y   7       18      25
</code></pre>

<p>What am I doing wrong? </p>

<p><strong>update</strong>: I analysed the data with GenStat (which doesn't show AIC values) and the output is totally different. In GenStat it asks for the random factor (here ID) and the denominator (here Trial_total), which is different than putting in Appr_Y, Appr_N. </p>

<p><strong>update2</strong>: The above dataset was just a dummy. I hereby provide the 'summary' of the model and the information about the dataset:</p>

<pre><code>&gt; summary(GLMM1)
Generalized linear mixed model fit by the Laplace approximation 
Formula: cbind(Appr_Y, Appr_N) ~ Condition + Var1 + Var2 + Var3 + Var4 + (1 | ID) 
Data: dataset 
AIC   BIC logLik deviance
102.1 113.5 -44.04    88.08
Random effects:
Groups Name        Variance Std.Dev.
ID     (Intercept) 0.59495  0.77133 
Number of obs: 38, groups: ID, 19

Fixed effects:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)       -2.43536    0.60237  -4.043 5.28e-05 ***
Condition8         1.14942    0.12274   9.365  &lt; 2e-16 ***
Var1               0.04524    0.04002   1.130   0.2583    
Var2Paired        -0.35299    0.47970  -0.736   0.4618    
Var3no             0.55914    0.44095   1.268   0.2048    
Var4               0.11996    0.06282   1.909   0.0562 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Correlation of Fixed Effects:
            (Intr) Cndt8- Var1 Var2P Var3no
Cndtn8-strn -0.128                            
Var1        -0.294  0.015                     
Var2unp     -0.474 -0.015 -0.352              
Var3no      -0.178  0.016 -0.310 -0.097       
Var4        -0.664  0.021 -0.078  0.467 -0.134
&gt; str(dataset)
'data.frame':   38 obs. of  9 variables:
$ ID          : Factor w/ 19 levels ""39"",""40"",""41"",..: 1 2 3 4 5 6 7 8 9 10 ...
   $ Appr_Y      : num  3 12 0 7 27 6 12 1 5 17 ...
$ Appr_N      : num  39 30 42 35 15 36 30 41 37 25 ...
    $ Var2        : Factor w/ 2 levels ""paired"",""unpaired"": 2 2 2 2 1 1 2 1 2 1 ...
$ Var1        : num  2 16 19 18 13 11 14 1 8 9 ...
    $ Var3        : Factor w/ 2 levels ""yes"",""no"": 2 2 2 1 2 2 2 1 1 2 ...
$ Var4        : num  2.6 6.87 2.4 1.1 4.32 ...
    $ Condition   : Factor w/ 2 levels ""1"",""8"": 1 1 1 1 1 1 1 1 1 1 ...
$ n           : num  42 42 42 42 42 42 42 42 42 42 ...
</code></pre>

<p>Do I perhaps have to do something with weighing the data as trial nr is not the same across conditions? Or using Appr_Y, total nr of trials instead of Appr_Y, Appr_N ?</p>
"
"0.192538147428251","0.187867287325545"," 48696","<p>I am trying to investigate how four variables (var1=continuous, var2=factor, var3=factor, var4=continuous) influence the number of trials individuals approached (out of total nr of trials --> binomial) across two conditions that differed in food availability (food availability 1 = 42 trials; food availability 8 = 35 trials) (n = 19 individuals). The response variable is binomial as it is the number of trials out of total number of trials. I am using the 'lmer' function of the lme4 package.</p>

<p>I thought the additive model I should run would be with random factor ID:</p>

<pre><code>glmer(cbind(appr_Y,appr_N) ~ Condition+Var1+Var2+Var3+Var4+(1|ID), data=dataset, 
      family=binomial)
</code></pre>

<p>However, the result I get shows that Condition is super significant (p &lt; 2e-16) while the other variables aren't, while exploring the data visually shows no difference in the response variable for Condition and the variables having strong effects.</p>

<p>Below a dummy representing the large data table:</p>

<pre><code>Con ID  Var1  Var2  appr_Y  appr_N  Trial_total
1   1   10      y   14      6       20
1   2   4       y   10      10      20
1   3   5       n   5       15      20
1   4   32      n   18      2       20
1   5   11      y   3       17      20
2   1   10      y   20      5       25
2   2   4       y   10      15      25
2   3   5       n   24      1       25
2   4   32      n   11      14      25  
2   5   11      y   7       18      25
</code></pre>

<p>What am I doing wrong? </p>

<p><strong>update</strong>: I analysed the data with GenStat (which doesn't show AIC values) and the output is totally different. In GenStat it asks for the random factor (here ID) and the denominator (here Trial_total), which is different than putting in Appr_Y, Appr_N. </p>

<p><strong>update2</strong>: The above dataset was just a dummy. I hereby provide the 'summary' of the model and the information about the dataset:</p>

<pre><code>&gt; summary(GLMM1)
Generalized linear mixed model fit by the Laplace approximation 
Formula: cbind(Appr_Y, Appr_N) ~ Condition + Var1 + Var2 + Var3 + Var4 + (1 | ID) 
Data: dataset 
AIC   BIC logLik deviance
102.1 113.5 -44.04    88.08
Random effects:
Groups Name        Variance Std.Dev.
ID     (Intercept) 0.59495  0.77133 
Number of obs: 38, groups: ID, 19

Fixed effects:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)       -2.43536    0.60237  -4.043 5.28e-05 ***
Condition8         1.14942    0.12274   9.365  &lt; 2e-16 ***
Var1               0.04524    0.04002   1.130   0.2583    
Var2Paired        -0.35299    0.47970  -0.736   0.4618    
Var3no             0.55914    0.44095   1.268   0.2048    
Var4               0.11996    0.06282   1.909   0.0562 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Correlation of Fixed Effects:
            (Intr) Cndt8- Var1 Var2P Var3no
Cndtn8-strn -0.128                            
Var1        -0.294  0.015                     
Var2unp     -0.474 -0.015 -0.352              
Var3no      -0.178  0.016 -0.310 -0.097       
Var4        -0.664  0.021 -0.078  0.467 -0.134
&gt; str(dataset)
'data.frame':   38 obs. of  9 variables:
$ ID          : Factor w/ 19 levels ""39"",""40"",""41"",..: 1 2 3 4 5 6 7 8 9 10 ...
   $ Appr_Y      : num  3 12 0 7 27 6 12 1 5 17 ...
$ Appr_N      : num  39 30 42 35 15 36 30 41 37 25 ...
    $ Var2        : Factor w/ 2 levels ""paired"",""unpaired"": 2 2 2 2 1 1 2 1 2 1 ...
$ Var1        : num  2 16 19 18 13 11 14 1 8 9 ...
    $ Var3        : Factor w/ 2 levels ""yes"",""no"": 2 2 2 1 2 2 2 1 1 2 ...
$ Var4        : num  2.6 6.87 2.4 1.1 4.32 ...
    $ Condition   : Factor w/ 2 levels ""1"",""8"": 1 1 1 1 1 1 1 1 1 1 ...
$ n           : num  42 42 42 42 42 42 42 42 42 42 ...
</code></pre>

<p>Do I perhaps have to do something with weighing the data as trial nr is not the same across conditions? Or using Appr_Y, total nr of trials instead of Appr_Y, Appr_N ?</p>
"
"0.178987746151269","0.162170924189109"," 49832","<p>In a multi-level model, what are the practical and interpretation-related implications of estimating versus not-estimating random effect correlation parameters?  The practical reason for asking this is that in the lmer framework in R, there is no implemented method for estimating p-values via MCMC techniques when estimates are made in the model of the correlations between parameters. </p>

<p>For example, looking at <a href=""http://stats.stackexchange.com/questions/13166/rs-lmer-cheat-sheet"">this example</a> (portions quoted below), what are the practical implications of M2 versus M3.  Obviously, in one case P5 will not be estimated and in the other it will.</p>

<p>Questions</p>

<ol>
<li>For practical reasons (the desire to get a p-value through MCMC techniques) one might want to fit a model without correlations between random effects even if P5 is substantially non-zero.  If one does this, and then estimates p-values via the MCMC technique, are the results interpretable?  (I know @Ben Bolker has previously mentioned that <a href=""http://stats.stackexchange.com/questions/5517/how-can-one-do-an-mcmc-hypothesis-test-on-a-mixed-effect-regression-model-with-r"">""combining significance testing with MCMC is a little bit incoherent, statistically, although I understand the urge to do so (getting confidence intervals is more supportable)""</a>, so if it will make you sleep better at night pretend I said confidence intervals.)</li>
<li>If one fails to estimate P5, is that the same as asserting that it is 0?</li>
<li>If P5 really is non-zero, then in what way are the estimated values of P1-P4 affected?</li>
<li>If P5 really is non-zero, then in what way are the estimates of error for P1-P4 affected?</li>
<li>If P5 really is non-zero, then in what ways are interpretations of a model failing to include P5 flawed?</li>
</ol>

<p>Borrowing from @Mike Lawrence's answer (those more knowledgeable than I are free to replace this with full model notation, I'm not entirely confident I can do so with reasonable fidelity):</p>

<p>M2:  <code>V1 ~ (1|V2) + V3 + (0+V3|V2)</code> (Estimates P1 - P4)</p>

<p>M3:  <code>V1 ~ (1+V3|V2) + V3</code> (Estimates P1-P5)</p>

<p><em>Parameters that might be estimated:</em></p>

<p><strong>P1</strong>: A global intercept</p>

<p><strong>P2</strong>: Random effect intercepts for V2 (i.e. for each level of V2, that level's intercept's deviation from the global intercept)</p>

<p><strong>P3</strong>: A single global estimate for the effect (slope) of V3</p>

<p><strong>P4</strong>: The effect of V3 within each level of V2 (more specifically, the degree to which the V3 effect within a given level deviates from the global effect of V3), while enforcing a zero correlation between the intercept deviations and V3 effect deviations across levels of V2.</p>

<p><strong>P5</strong>: The correlation between intercept deviations and V3 deviations across levels of V2</p>

<p>Answers derived from a sufficiently large and broad simulation along with accompanying code in R using lmer would be acceptable.</p>
"
"0.095672974646988","0.0700140042014005"," 50561","<p>I am trying to calculate the Intraclass Correlation for a rater study using R and the library lme4 and the function lmer. The data has the following design: The same 6 raters (at least 4) are rating 25 horses live and all raters are rating a subset of 10 horses on video. </p>

<p>The two way random model applied:</p>

<pre><code>m1 &lt;- lmer(Score ~ -1 + (1|HorseID) + (1|RaterID) + Time, data=mydata)
</code></pre>

<p>The absolute agreement ICC is calculated using the estimated coefficients:</p>

<pre><code>xVars &lt;- function(model) {
exvars = lme4::VarCorr(model)
vars = c(exvars$HorseID[1,1], exvars$RaterID[1,1], attr(exvars,""sc"")^2) 
names(vars) &lt;- c('item var', 'judge var', 'residual var')
vars }

# helper function for ICC(k) variations

icck &lt;- function(variances, k=1) {
icc = variances[1] / (variances[1] + (variances[2] + variances[3]) / k)
names(icc) = c(paste('ICC', k, sep=''))
icc }
</code></pre>

<p>And the ICC as:</p>

<pre><code>ICC.m1 &lt;- icck(xVars(m1))
</code></pre>

<p>I would like to add:</p>

<ul>
<li>Confidence Intervals for the ICC</li>
<li>Cronbach's alpha</li>
</ul>

<p>But I can't figure out at smart way of doing so? Help would be greatly appreciated!!</p>
"
"0.209466539620948","0.204385006865824"," 51005","<p>I know there are a slew of lmer specification questions already floating around.  Please let me know if this is a duplicate, or if it is deemed off-topic, and I'll delete it.</p>

<p>I am using a forward stepwise approach in an attempt to optimially specify the random effects structure for my data.  The base model I am attempting to step-up from has a random effects correlation structure like this:</p>

<pre><code>   AIC   BIC logLik deviance
 38476 38632 -19220    38440
Random effects:
 Groups Name        Variance Std.Dev. Corr          
 SubjID (Intercept)  2.7844  1.6687                 
        SOA.s.c     39.1269  6.2551    0.931        
        SOA.s.c2    15.7080  3.9633   -0.943 -0.997 
Number of obs: 44061, groups: SubjID, 38
</code></pre>

<p><strong>Sub issue</strong>:  I am (uneasily) at peace with the high correlations between the intercept, SOA.s.c, and SOA.s.c2.  Those values make some sense, although they are <strong>much</strong> higher than I would like.  I think I am correct in allowing them to be estimated so long as model still converges... but I stand ready to be corrected (and chided).</p>

<p>My step-up canidate models are the following:</p>

<ol>
<li><code>(1+SOA.s.c+SOA.s.c2|SubjID)+(1+SessionNum.c|SubjID)</code></li>
<li><code>(1+SOA.s.c+SOA.s.c2|SubjID)+(0+SessionNum.c|SubjID)</code></li>
<li><code>(1+SOA.s.c+SessionNum.c+SOA.s.c2|SubjID)</code></li>
</ol>

<p><strong>Actual Question</strong>:  Given that repeated measures were taken for each SubjID along each of the three above listed variables, are each of these three sensible options to test?  In particular, I am concerned with model 1, because it provides the lowest AIC and BIC, but appears to estimate yet another SubjID interept.  Is that really what it is doing?  Is that okay?</p>

<p><strong>For reference</strong>:</p>

<p>The random effects correlations for the first set, <code>(1+SOA.s.c+SOA.s.c2|SubjID)+(1+SessionNum.c|SubjID)</code>, look like this:</p>

<pre><code>   AIC   BIC logLik deviance
 38284 38466 -19121    38242
Random effects:
 Groups Name         Variance  Std.Dev. Corr          
 SubjID (Intercept)   2.687227 1.63928                
        SOA.s.c      39.713777 6.30189   0.968        
        SOA.s.c2     16.853598 4.10531  -0.984 -0.995 
 SubjID (Intercept)   0.224172 0.47347                
        SessionNum.c  0.070583 0.26567  0.977         
Number of obs: 44061, groups: SubjID, 38
</code></pre>

<p>The random effects correlations for the second set, <code>(1+SOA.s.c+SOA.s.c2|SubjID)+(0+SessionNum.c|SubjID)</code>, look like this:</p>

<pre><code>   AIC   BIC logLik deviance
 38307 38472 -19134    38269
Random effects:
 Groups Name         Variance  Std.Dev. Corr          
 SubjID (Intercept)   2.939991 1.71464                
        SOA.s.c      40.131292 6.33493   0.932        
        SOA.s.c2     15.941733 3.99271  -0.941 -0.996 
 SubjID SessionNum.c  0.066763 0.25839                
</code></pre>

<p>The random effects correlations for the second set, <code>(1+SOA.s.c+SOA.s.c2+SessionNUm.c|SubjID)</code>, look like this:</p>

<pre><code>   AIC   BIC logLik deviance
 38286 38477 -19121    38242
Random effects:
 Groups Name         Variance Std.Dev. Corr                 
 SubjID (Intercept)   2.98820 1.72864                       
        SOA.s.c      39.85077 6.31275   0.932               
        SessionNum.c  0.07066 0.26582   0.316  0.050        
        SOA.s.c2     16.94324 4.11622  -0.947 -0.995 -0.052 
</code></pre>
"
"0.10696563746014","0.104370715180858"," 56380","<p>The <code>lme4</code> package in R includes the <code>cake</code> dataset. </p>

<pre><code>library(lme4)
head(cake[,2:4], 20)
   recipe temperature angle
1       A         175    42
2       A         185    46
3       A         195    47
4       A         205    39
5       A         215    53
6       A         225    42
7       B         175    39
8       B         185    46
9       B         195    51
10      B         205    49
11      B         215    55
12      B         225    42
13      C         175    46
14      C         185    44
15      C         195    45
16      C         205    46
17      C         215    48
18      C         225    63
19      A         175    47
20      A         185    29
</code></pre>

<p>I've analysed the <code>cake</code> dataset using two different models below. The first model is a 2 factor ANOVA:</p>

<pre><code>summary(aov(angle ~ temperature + recipe, cake))
             Df Sum Sq Mean Sq F value   Pr(&gt;F)    
temperature   5   2100   420.1   6.918 4.37e-06 ***
recipe        2    135    67.5   1.112     0.33    
Residuals   262  15908    60.7                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>...and the second is a mixed effects model, with <code>temperature</code> as a random effect:</p>

<pre><code>lmer(angle ~ recipe + (1| temperature), data=cake, REML=F)
Linear mixed model fit by maximum likelihood 
Formula: angle ~ recipe + (1 | temperature) 
   Data: cake 
  AIC  BIC logLik deviance REMLdev
 1893 1911 -941.7     1883    1877
Random effects:
 Groups      Name        Variance Std.Dev.
 temperature (Intercept)  6.4399  2.5377  
 Residual                60.2560  7.7625  
Number of obs: 270, groups: temperature, 6

Fixed effects:
            Estimate Std. Error t value
(Intercept)   33.122      1.320  25.093
recipeB       -1.478      1.157  -1.277
recipeC       -1.522      1.157  -1.315

Correlation of Fixed Effects:
        (Intr) recipB
recipeB -0.438       
recipeC -0.438  0.500
</code></pre>

<p>Is someone able to provide a summary of what the mixed effect model has done differently to the ANOVA?</p>
"
"0.135302018298348","0.13201967239689"," 56600","<p>I have produced the following model: </p>

<pre><code>&gt;lmer(TotalPayoff~PgvnD*Type+Type*Asym+PgvnD*Asym+Game*Type+Game*PgvnD+Game*Asym+
                   (1|Subject)+(1|Pairing),REML=FALSE,data=table1)-&gt;m1

PgvnD=A percentage (numeric)
Asym= a factor 0 or 1
Type=a factor 1 or 2
Game= a factor 1 or 2
</code></pre>

<p>from this model the terms <code>Type</code>, <code>Game</code> and <code>PgvnD:Asym</code> were shown to be  significant by removal from the model. <code>PgvnD</code> and <code>Asym</code> on there own were not significant but were left in the model because the interaction between them was. The summary of this model is as follows;</p>

<pre><code>&gt; m7
Linear mixed model fit by maximum likelihood 
Formula: TotalPayoff ~ Type + PgvnD * Asym + Game + (1 | Subject) + (1 |Pairing) 
   Data: table1 
  AIC  BIC logLik deviance REMLdev
 1014 1038 -497.8    995.6   964.4
Random effects:
 Groups   Name        Variance Std.Dev.
 Subject  (Intercept)   0.000   0.0000 
 Pairing  (Intercept) 716.101  26.7601 
 Residual              89.364   9.4533 
Number of obs: 113, groups: Subject, 73; Pairing, 61

Fixed effects:
            Estimate Std. Error t value
(Intercept)   81.727      6.332  12.907
Type2          7.926      2.852   2.779
PgvnD         -8.466      7.554  -1.121
Asym1        -12.167      7.583  -1.604
Game2         15.374      7.147   2.151
PgvnD:Asym1   26.618      9.710   2.741

Correlation of Fixed Effects:
            (Intr) Type2  PgvnD  Asym1  Game2 
Type2       -0.188                            
PgvnD       -0.218 -0.038                     
Asym1       -0.620  0.081  0.189              
Game2       -0.483  0.009 -0.010 -0.015       
PgvnD:Asym1  0.233 -0.267 -0.766 -0.328 -0.011
</code></pre>

<p>Am I interpreting these results correctly?</p>

<ul>
<li><code>TotalPayoff</code> is higher when <code>Type=1</code> than in <code>Type=2</code>, it is also higher when <code>game=2</code> than when <code>game=1</code>. </li>
<li>Also <code>TotalPayoff</code> increases significantly with <code>PgvnD</code> if <code>Asym=1</code> but not if <code>ASym=0</code> (indicated by significant interaction term but non-significant single terms).</li>
</ul>

<p>Also I notice that the <code>Subject</code> random effect has SD and variance of 0. Can this then be removed from the model? What does this really mean?</p>
"
"0.22437301403671","0.218929859143062"," 56695","<p>I understand that we use random effects (or mixed effects) models when we believe that some model parameter(s) vary randomly across some grouping factor. I have a desire to fit a model where the response has been normalized and centered (not perfectly, but pretty close) across a grouping factor, but an independent variable <code>x</code> has not been adjusted in any way. This led me to the following test (using <em>fabricated</em> data) to ensure that I'd find the effect I was looking for if it was indeed there. I ran one <em>mixed</em> effects model with a random intercept (across groups defined by <code>f</code>) and a second <em>fixed</em> effect model with the factor f as a fixed effect predictor. I used the R package <code>lmer</code> for the mixed effect model, and the base function <code>lm()</code> for the fixed effect model. Following is the data and the results. </p>

<p>Notice that <code>y</code>, regardless of group, varies around 0. And that <code>x</code> varies consistently with <code>y</code> within group, but varies much more across groups than <code>y</code></p>

<pre><code>&gt; data
      y   x f
1  -0.5   2 1
2   0.0   3 1
3   0.5   4 1
4  -0.6  -4 2
5   0.0  -3 2
6   0.6  -2 2
7  -0.2  13 3
8   0.1  14 3
9   0.4  15 3
10 -0.5 -15 4
11 -0.1 -14 4
12  0.4 -13 4
</code></pre>

<p>If you're interested in working with the data, here is <code>dput()</code> output:</p>

<pre><code>data&lt;-structure(list(y = c(-0.5, 0, 0.5, -0.6, 0, 0.6, -0.2, 0.1, 0.4, 
-0.5, -0.1, 0.4), x = c(2, 3, 4, -4, -3, -2, 13, 14, 15, -15, 
-14, -13), f = structure(c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 
4L, 4L, 4L), .Label = c(""1"", ""2"", ""3"", ""4""), class = ""factor"")), 
.Names = c(""y"",""x"",""f""), row.names = c(NA, -12L), class = ""data.frame"")
</code></pre>

<p>Fitting the mixed effects model:</p>

<pre><code>&gt; summary(lmer(y~ x + (1|f),data=data))
Linear mixed model fit by REML 
Formula: y ~ x + (1 | f) 
   Data: data 
   AIC   BIC logLik deviance REMLdev
 28.59 30.53  -10.3       11   20.59
Random effects:
 Groups   Name        Variance Std.Dev.
 f        (Intercept) 0.00000  0.00000 
 Residual             0.17567  0.41913 
Number of obs: 12, groups: f, 4

Fixed effects:
            Estimate Std. Error t value
(Intercept) 0.008333   0.120992   0.069
x           0.008643   0.011912   0.726

Correlation of Fixed Effects:
  (Intr)
x 0.000 
</code></pre>

<p>I note that the intercept variance component is estimated 0, and importantly to me, <code>x</code> is not a significant predictor of <code>y</code>.</p>

<p>Next I fit the fixed effect model with <code>f</code> as a predictor instead of a grouping factor for a random intercept:</p>

<pre><code>&gt; summary(lm(y~ x + f,data=data))

Call:
lm(formula = y ~ x + f, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.16250 -0.03438  0.00000  0.03125  0.16250 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -1.38750    0.14099  -9.841 2.38e-05 ***
x            0.46250    0.04128  11.205 1.01e-05 ***
f2           2.77500    0.26538  10.457 1.59e-05 ***
f3          -4.98750    0.46396 -10.750 1.33e-05 ***
f4           7.79583    0.70817  11.008 1.13e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.1168 on 7 degrees of freedom
Multiple R-squared: 0.9484, Adjusted R-squared: 0.9189 
F-statistic: 32.16 on 4 and 7 DF,  p-value: 0.0001348 
</code></pre>

<p>Now I notice that, as expected, <code>x</code> is a significant predictor of <code>y</code>.</p>

<p><strong>What I am looking for</strong> is intuition regarding this difference. In what way is my thinking wrong here? Why do I incorrectly expect to find a significant parameter for <code>x</code> in both of these models but only actually see it in the fixed effect model?</p>
"
"0.0552368176666107","0.0538968055636295"," 57395","<p>I have the following model:</p>

<pre><code>&gt; model1&lt;-lmer(aph.remain~sMFS1+sAG1+sSHDI1+sbare+season+crop
  +(1|landscape),family=poisson)
</code></pre>

<p>...and this is the summary output.  </p>

<pre><code>&gt; summary(model1)
Generalized linear mixed model fit by the Laplace approximation 
Formula: aph.remain ~ sMFS1 + sAG1 + sSHDI1 + sbare + season + crop 
         +      (1 | landscape) 
  AIC  BIC logLik deviance
 4057 4088  -2019     4039
Random effects:
 Groups    Name        Variance Std.Dev.
 landscape (Intercept) 0.74976  0.86588 
Number of obs: 239, groups: landscape, 45

Fixed effects:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  2.6613761  0.1344630  19.793  &lt; 2e-16 
sMFS1        0.3085978  0.1788322   1.726  0.08441   
sAG1         0.0003141  0.1677138   0.002  0.99851    
sSHDI1       0.4641420  0.1619018   2.867  0.00415 
sbare        0.4133425  0.0297325  13.902  &lt; 2e-16 
seasonlate  -0.5017022  0.0272817 -18.390  &lt; 2e-16 
cropforage   0.7897194  0.0672069  11.751  &lt; 2e-16
cropsoy      0.7661506  0.0491494  15.588  &lt; 2e-16 
</code></pre>

<p>Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â </p>

<pre><code>Correlation of Fixed Effects:
           (Intr) sMFS1  sAG1   sSHDI1 sbare  sesnlt crpfrg
sMFS1      -0.007                                          
sAG1        0.002 -0.631                                   
sSHDI1      0.000  0.593 -0.405                            
sbare      -0.118 -0.003  0.007 -0.013                     
seasonlate -0.036  0.006 -0.006  0.003 -0.283              
cropforage -0.168 -0.004  0.016 -0.014  0.791 -0.231       
cropsoy    -0.182 -0.028  0.030 -0.001  0.404 -0.164  0.557
</code></pre>

<p>It is probably overdispersed, but how exactly do I calculate this? </p>

<p>Thanks very much.</p>
"
"0.208514414057075","0.203455979297691"," 58745","<p>EDIT 2: I originally thought I needed to run a two-factor ANOVA with repeated measures on one factor, but I now think a linear mixed-effect model will work better for my data. I think I nearly know what needs to happen, but am still confused by few points.</p>

<p>The experiments I need to analyze look like this: </p>

<ul>
<li>Subjects were assigned to one of several treatment groups</li>
<li>Measurements of each subject were taken on multiple days</li>
<li>So:
<ul>
<li>Subject is nested within treatment</li>
<li>Treatment is crossed with day</li>
</ul></li>
</ul>

<p>(each subject is assigned to only one treatment, and measurements are taken on each subject on each day)</p>

<p>My dataset contains the following information:</p>

<ul>
<li>Subject = blocking factor (random factor)</li>
<li>Day = within subject or repeated measures factor (fixed factor)</li>
<li>Treatment = between subject factor (fixed factor)</li>
<li>Obs = measured (dependent) variable</li>
</ul>

<p><strong>UPDATE</strong>
OK, so I went and talked to a statistician, but he's an SAS user.  He thinks that the model should be:</p>

<p><strong>Treatment + Day + Subject(Treatment) + Day*Subject(Treatment)</strong></p>

<p>Obviously his notation is different from the R syntax, but this model is supposed to account for:</p>

<ul>
<li>Treatment   (fixed)</li>
<li>Day   (fixed)</li>
<li>the Treatment*Day interaction</li>
<li>Subject nested within Treatment  (random)</li>
<li>Day crossed with ""Subject within Treatment""   (random)</li>
</ul>

<p>So, is this the correct syntax to use? </p>

<pre><code>m4 &lt;- lmer(Obs~Treatment*Day + (1+Treatment/Subject) + (1+Day*Treatment/Subject), mydata)
</code></pre>

<p>I'm particularly concerned about whether the Day crossed with ""Subject within Treatment"" part is right.  Is anyone familiar with SAS, or confident that they understand what's going on in his model, able to comment on whether my sad attempt at R syntax matches?</p>

<p>Here are my previous attempts at building a model and writing syntax (discussed in answers &amp; comments):</p>

<pre><code>m1 &lt;- lmer(Obs ~ Treatment * Day + (1 | Subject), mydata)
</code></pre>

<p>How do I deal with the fact that subject is nested within treatment?  How does <code>m1</code> differ from: </p>

<pre><code>m2 &lt;- lmer(Obs ~ Treatment * Day + (Treatment|Subject), mydata)
m3 &lt;- lmer(Obs ~ Treatment * Day + (Treatment:Subject), mydata)
</code></pre>

<p>and are <code>m2</code> and <code>m3</code> equivalent (and if not, why)?</p>

<p>Also, do I need to be using nlme instead of lme4 if I want to specify the correlation structure (like <code>correlation = corAR1</code>)?  According to <a href=""http://circ.ahajournals.org/content/117/9/1238.full"">Repeated Measures</a>, for a repeated-measures analysis with repeated measures on one factor, the covariance structure (the nature of the correlations between measurements of the same subject) is important. </p>

<p>When I was trying to do a repeated-measures ANOVA, I'd decided to use a Type II SS; is this still relevant, and if so, how do I go about specifying that?</p>

<p>Here's an example of what the data look like:</p>

<pre><code>mydata &lt;- data.frame(
  Subject  = c(13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 
               34, 35, 36, 37, 38, 39, 40, 62, 63, 64, 65, 13, 14, 15, 16, 17, 18, 
               19, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 
               40, 62, 63, 64, 65, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 
               29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 62, 63, 64, 65), 
  Day       = c(rep(c(""Day1"", ""Day3"", ""Day6""), each=28)), 
  Treatment = c(rep(c(""B"", ""A"", ""C"", ""B"", ""C"", ""A"", ""A"", ""B"", ""A"", ""C"", ""B"", ""C"", 
                      ""A"", ""A"", ""B"", ""A"", ""C"", ""B"", ""C"", ""A"", ""A""), each = 4)), 
  Obs       = c(6.472687, 7.017110, 6.200715, 6.613928, 6.829968, 7.387583, 7.367293, 
                8.018853, 7.527408, 6.746739, 7.296910, 6.983360, 6.816621, 6.571689, 
                5.911261, 6.954988, 7.624122, 7.669865, 7.676225, 7.263593, 7.704737, 
                7.328716, 7.295610, 5.964180, 6.880814, 6.926342, 6.926342, 7.562293, 
                6.677607, 7.023526, 6.441864, 7.020875, 7.478931, 7.495336, 7.427709, 
                7.633020, 7.382091, 7.359731, 7.285889, 7.496863, 6.632403, 6.171196, 
                6.306012, 7.253833, 7.594852, 6.915225, 7.220147, 7.298227, 7.573612, 
                7.366550, 7.560513, 7.289078, 7.287802, 7.155336, 7.394452, 7.465383, 
                6.976048, 7.222966, 6.584153, 7.013223, 7.569905, 7.459185, 7.504068, 
                7.801867, 7.598728, 7.475841, 7.511873, 7.518384, 6.618589, 5.854754, 
                6.125749, 6.962720, 7.540600, 7.379861, 7.344189, 7.362815, 7.805802, 
                7.764172, 7.789844, 7.616437, NA, NA, NA, NA))
</code></pre>
"
"0.165710452999832","0.161690416690889"," 58900","<p>I have very recently started learning about Generalised Linear Mixed Models and was using R to explore what difference it makes to treat group membership as either fixed or random effect. In particular, I am looking at the example dataset discussed here:</p>

<p><a href=""http://www.ats.ucla.edu/stat/mult_pkg/glmm.htm"">http://www.ats.ucla.edu/stat/mult_pkg/glmm.htm</a></p>

<p><a href=""http://www.ats.ucla.edu/stat/r/dae/melogit.htm"">http://www.ats.ucla.edu/stat/r/dae/melogit.htm</a></p>

<p>As outlined in this tutorial, the effect of Doctor ID is appreciable and I was expecting the mixed model with a random intercept to give better results. However, comparing AIC values for the two methods suggest that this model is worse:</p>

<pre><code>&gt; require(lme4) ; hdp = read.csv(""http://www.ats.ucla.edu/stat/data/hdp.csv"")
&gt; hdp$DID = factor(hdp$DID) ; hdp$Married = factor(hdp$Married)
&gt; GLM = glm(remission~Age+Married+IL6+DID,data=hdp,family=binomial);summary(GLM)

Call:
glm(formula = remission ~ Age + Married + IL6 + DID, family = binomial, 
data = hdp)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.5265  -0.6278  -0.2272   0.5492   2.7329  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -1.560e+01  1.219e+03  -0.013    0.990    
Age         -5.869e-02  5.272e-03 -11.133  &lt; 2e-16 ***
Married1     2.688e-01  6.646e-02   4.044 5.26e-05 ***
IL6         -5.550e-02  1.153e-02  -4.815 1.47e-06 ***
DID2         1.805e+01  1.219e+03   0.015    0.988    
DID3         1.932e+01  1.219e+03   0.016    0.987   

[...]

DID405       1.566e+01  1.219e+03   0.013    0.990    
DID405       1.566e+01  1.219e+03   0.013    0.990    
DID406      -2.885e-01  3.929e+03   0.000    1.000    
DID407       2.012e+01  1.219e+03   0.017    0.987    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 10353  on 8524  degrees of freedom
Residual deviance:  6436  on 8115  degrees of freedom
AIC: 7256

Number of Fisher Scoring iterations: 17


&gt; GLMM = glmer(remission~Age+Married+IL6+(1|DID),data=hdp,family=binomial) ; m

Generalized linear mixed model fit by the Laplace approximation 
Formula: remission ~ Age + Married + IL6 + (1 | DID) 
Data: hdp 
AIC  BIC logLik deviance
7743 7778  -3867     7733
Random effects:
Groups Name        Variance Std.Dev.
DID    (Intercept) 3.8401   1.9596  
Number of obs: 8525, groups: DID, 407

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  1.461438   0.272709   5.359 8.37e-08 ***
Age         -0.055969   0.005038 -11.109  &lt; 2e-16 ***
Married1     0.260065   0.063736   4.080 4.50e-05 ***
IL6         -0.053288   0.011058  -4.819 1.44e-06 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
         (Intr) Age    Marrd1
Age      -0.898              
Married1  0.070 -0.224       
IL6      -0.162  0.012 -0.033


&gt; extractAIC(GLM) ; extractAIC(GLMM)

[1]  410.000 7255.962
[1]    5.000 7743.188
</code></pre>

<p>Thus, my questions are:</p>

<p>(1) Is it appropriate to compare the AIC values provided by the two functions? If so, why does the fixed effect model do better?</p>

<p>(2) What is the best way to identify if fixed or random effects are more important (ie to quantify that the variability due to the doctor is more important than patient characteristics?</p>
"
"0.239841342649939","0.242690451496427"," 59127","<p>Recently, I have done a fairly complex experiment, and I am having trouble coming up with a model that is suitable for the data. I have spent a few days reading about, e.g., when random effects should be nested or crossed, and which variables should be included in a full model. Yet, the literature that is readable for a non-statistician like myself is usually limited to two nesting levels, whereas I (may) have more. The literature that does seem to apply, is too complex for me to understand. I hope you can help me.</p>

<p>My aim is to specify a full model, and start model simplification from there.</p>

<p>The experiment I have done, was as follows:</p>

<ul>
<li>All participants completed three tasks, in a fixed order (Task1, Task2, Task3).</li>
<li>Task 1 consisted of 30 trials, Tasks 2 and 3 both consisted of 10 trials.</li>
<li>Each task was completed three times by each participant (Round1, Round2, Round3).</li>
<li>The manipulation consisted of one factor with three levels (Condition1, Condition2, and Condition3).</li>
<li>The conditions were tied to the rounds, so each participant completed Task 1 in each condition (e.g., 30 trials in Condition1, followed by 30 trials in Condition2, followed by 30 trials in Condition3), before moving on to Task 2.</li>
<li>I used six different stimuli to manipulate these conditions (two for each condition; Stimulus1.1, Stimulus1.2, Stimulus2.1 .. Stimulus3.2)</li>
<li>The dependent variable is binary</li>
</ul>

<p>To complicate matters even more, we used 6 different orders of presenting the stimuli.</p>

<p>I think it's easiest to demonstrate the data structure using an example:</p>

<pre><code>order &lt;- rbind(
c('Stimulus1.1', 'Stimulus2.1', 'Stimulus3.1', 'Stimulus2.2', 'Stimulus3.2', 'Stimulus1.2', 'Stimulus3.1', 'Stimulus1.2', 'Stimulus2.1'),
c('Stimulus1.1', 'Stimulus3.1', 'Stimulus2.1', 'Stimulus3.2', 'Stimulus2.2', 'Stimulus1.2', 'Stimulus2.1', 'Stimulus1.2', 'Stimulus3.1'),
c('Stimulus2.1', 'Stimulus3.1', 'Stimulus1.1', 'Stimulus3.2', 'Stimulus1.2', 'Stimulus2.2', 'Stimulus1.2', 'Stimulus2.1', 'Stimulus3.1'),
c('Stimulus2.1', 'Stimulus1.1', 'Stimulus3.1', 'Stimulus1.2', 'Stimulus3.2', 'Stimulus2.2', 'Stimulus3.2', 'Stimulus2.1', 'Stimulus1.1'),
c('Stimulus3.1', 'Stimulus2.1', 'Stimulus1.1', 'Stimulus2.2', 'Stimulus1.2', 'Stimulus3.2', 'Stimulus1.1', 'Stimulus3.2', 'Stimulus2.1'),
c('Stimulus3.1', 'Stimulus1.1', 'Stimulus2.1', 'Stimulus1.2', 'Stimulus2.2', 'Stimulus3.2', 'Stimulus2.1', 'Stimulus3.2', 'Stimulus1.1'))

test &lt;- expand.grid(trial=1:30, task=c('Task1', 'Task2', 'Task3'), round=c('Round1', 'Round2', 'Round3'), pp=1:12)
test &lt;- test[! (with(test, trial &gt; 10 &amp; task == 'Task2')),]
test &lt;- test[! (with(test, trial &gt; 10 &amp; task == 'Task3')),]
test$taskround &lt;- factor(paste(test$task, test$round, sep=':'))
test$task &lt;- factor(test$task)
test$round &lt;- factor(test$round)
test$stimulus &lt;- factor(unlist(lapply(1:nrow(test), function(x) {order[1 + (test[x, 'pp'] %% 6), as.numeric(test[x,'taskround'])]})))
test$condition &lt;- factor(paste('Condition', substr(as.character(test$stimulus), 9,9), sep=''))
test$response &lt;- factor(rbinom(nrow(test),1, prob=.95))
test$pp &lt;- factor(test$pp)
</code></pre>

<p>The resulting data structure is:</p>

<pre><code>   trial  task  round pp    taskround    stimulus  condition response
1      1 Task1 Round1  1 Task1:Round1 Stimulus1.1 Condition1        1
2      2 Task1 Round1  1 Task1:Round1 Stimulus1.1 Condition1        1
3      3 Task1 Round1  1 Task1:Round1 Stimulus1.1 Condition1        1
4      4 Task1 Round1  1 Task1:Round1 Stimulus1.1 Condition1        1
5      5 Task1 Round1  1 Task1:Round1 Stimulus1.1 Condition1        1
6      6 Task1 Round1  1 Task1:Round1 Stimulus1.1 Condition1        1
7      7 Task1 Round1  1 Task1:Round1 Stimulus1.1 Condition1        1
8      8 Task1 Round1  1 Task1:Round1 Stimulus1.1 Condition1        1
9      9 Task1 Round1  1 Task1:Round1 Stimulus1.1 Condition1        1
10    10 Task1 Round1  1 Task1:Round1 Stimulus1.1 Condition1        1
</code></pre>

<p>I'm mainly interested in the main effect of Condition.</p>

<p>I am anticipating that the response tendency may differ between tasks and participants. This would translate into a different intercept for each task and participant, correct?</p>

<p>I am also anticipating that the manipulation may affect some participants more than others; and I suspect that different participants may approach different tasks in a different way (e.g., Task 1 may elicit more successes that Task 2, but participant 1 may be more sensitive to this task aspect than participant 2).</p>

<p>My current model is:</p>

<p><code>glmer(response ~ condition + (1 | pp/task/round) + (0 + task + condition | pp), data=test, family=binomial)</code></p>

<p>However, I am not sure if this model is correct. My questions are: </p>

<ol>
<li><p>I am uncertain about the nesting of the random effects in the first clause. I believe <code>task/round</code> is correct, but I am not sure whether I should think of these as being nested under participants, or crossed with participants as follows:
<code>glmer(response ~ condition + (1 | task/round) + (1 | pp) + (0 + task + condition | pp), data=test, family=binomial)</code>.</p></li>
<li><p>I am not sure at all about the <code>(0 + task + condition | pp)</code> random
effects. I get correlations between the task and condition random
effects, and I am not sure whether I would want/need these.</p></li>
<li><p>I removed the intercept here, because it was already included in the
first random effects clause, but I'm not sure about that either.</p></li>
<li><p>Whether this model takes into account any order effects, or whether
I should explicitly model those?</p></li>
<li><p>Whether I should take into account differences between stimuli
(and/or the possibility that different participants react
differently to different stimuli), and how I should model these
(crossed, or nested)?</p></li>
<li><p>I'm worried about fatigue effect (lineair or quadratic) over the
course of the experiment AND within tasks. I would like to include
these effects into the model as well, but again, I'm not sure how to
do that.</p></li>
</ol>

<p>So, in summary, I have lots of questions about analyzing these data. I think I am mainly interested in recommendations on how to approach this dataset, and suggestions for a suitable full model. Any other tips/suggestions are very welcome as well.</p>
"
"0.224585387973546","0.219137081015428"," 59912","<p>So I often do little self-experiments where I blind &amp; randomize things; these can be formulated as your normal <em>t</em>-tests, but sometimes the measured metrics have extensive baselines which seem like they could be used for more accurate answers. A bunch of reading upon <em>n</em>-of-1 and single-subject designs suggested that people have been moving to mixed/hierarchical/multilevel models for analyzing such setups (eg. Nelson 2012 <a href=""http://etd.lsu.edu/docs/available/etd-04252012-152015/unrestricted/NelsonDiss.pdf"" rel=""nofollow"">""Hierarchical linear modeling versus visual analysis of single subject design data""</a> or <a href=""http://www.eric.ed.gov/PDFS/EJ800974.pdf"" rel=""nofollow"" title=""Van den Noortgate et al 2007"">""The Aggregation of Single-Case Results using Hierarchical Linear Models""</a>).</p>

<p>As I understand it, the idea is to split the subject's data into experiment vs baseline, and treat those as the groups. I'm trying to understand how sensible this is with a recent experiment, so hopefully someone can point out if I go wrong in using <code>lmer</code> here.</p>

<hr>

<p>We start with a regular linear model which examines purely the experimental data (the numeric <code>Response</code> vs the binary <code>Intervention</code> variables) and ignores the extensive baseline phase before, during, and after the experiment:</p>

<pre><code>R&gt; experiment &lt;- read.csv(""http://dl.dropboxusercontent.com/u/85192141/data.csv"")
R&gt; summary(lm(Response ~ Intervention, data=experiment))

...
Residuals:
    Min      1Q  Median      3Q     Max
-1.0156 -0.8889 -0.0156  0.1111  1.1111

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)    3.0156     0.0889    33.9   &lt;2e-16
Intervention  -0.1267     0.1262    -1.0     0.32

Residual standard error: 0.711 on 125 degrees of freedom
  (145 observations deleted due to missingness)
Multiple R-squared:  0.008, Adjusted R-squared:  6.73e-05
F-statistic: 1.01 on 1 and 125 DF,  p-value: 0.317

R&gt; confint(lm(Response ~ Intervention, data=experiment))
               2.5 % 97.5 %
(Intercept)   2.8397  3.192
Intervention -0.3765  0.123
</code></pre>

<p>The estimated coefficient is not statistically-significant: -0.38-0.12. But it's definitely slanted towards being negative. So this is the 'conservative' case, where we ignore the baseline entirely. What's the optimistic case? Well, it seems to me that the optimistic case is when we take the entire baseline and assume it is exactly the same as the 'off'/0 intervention in the experiment, in which case we get a narrower CI (because our estimate of the intercept has halved its standard error):</p>

<pre><code>R&gt; experiment$Intervention[is.na(experiment$Intervention)] &lt;- 0
R&gt; summary(lm(Response ~ Intervention, data=experiment))

...
Residuals:
    Min      1Q  Median      3Q     Max 
-1.9924 -0.8889  0.0076  1.0076  1.1111 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)    2.9924     0.0375   79.88   &lt;2e-16
Intervention  -0.1036     0.1012   -1.02     0.31

Residual standard error: 0.746 on 458 degrees of freedom
Multiple R-squared:  0.00228,   Adjusted R-squared:  0.000101 
F-statistic: 1.05 on 1 and 458 DF,  p-value: 0.307

R&gt; confint(lm(Response ~ Intervention, data=experiment))
               2.5 %  97.5 %
(Intercept)   2.9188 3.06607
Intervention -0.3025 0.09538
</code></pre>

<p>It's narrowed to -0.30-0.10; still not statistically-significant, but closer.</p>

<p>It seems to me that a hierarchical model ought to produce a CI intermediate between the pessimistic and optimistic cases: it loses some power because it's estimating how different the two phases are before it does any combining.</p>

<p>Here is my multilevel model, split between baseline and experimental phases:</p>

<pre><code>library(lme4)
experiment &lt;- read.csv(""http://dl.dropboxusercontent.com/u/85192141/data.csv"")
experiment$Phase &lt;- ifelse(is.na(experiment$Intervention), TRUE, FALSE)
model &lt;- lmer(Response ~ Intervention + (1|Phase), data=experiment); summary(model)

...
 AIC BIC logLik deviance REMLdev
 286 297   -139      273     278
Random effects:
 Groups   Name        Variance Std.Dev.
 Phase    (Intercept) 0.0106   0.103   
 Residual             0.5057   0.711   
Number of obs: 127, groups: Phase, 1

Fixed effects:
             Estimate Std. Error t value
(Intercept)     3.016      0.136    22.2
Intervention   -0.127      0.126    -1.0

Correlation of Fixed Effects:
            (Intr)
Interventin -0.461

m &lt;- mcmcsamp((lmer(Response ~ Intervention + (1|Phase), data=experiment)), n = 100000)
HPDinterval(m, prob=0.95)$fixef

                lower   upper
(Intercept)  -45.3107 56.6558
Intervention  -0.3742  0.1191
</code></pre>

<p>The estimated CI comes out exactly in the middle, as expected:</p>

<ol>
<li>pessimistic   -0.38 0.12</li>
<li>hierarchical  -0.37 0.11</li>
<li>optimistic    -0.30 0.10</li>
</ol>

<p>So, my basic question is: is this a sane approach to take? It's spitting out answers that seem intuitively correct, but that might just be a coincidence.</p>

<hr>

<p>Incidentally, one might be worried about time trends. The randomization/blocking would fix that in the experimental period but not the baseline. Fortunately, that doesn't seem to be an issue:</p>

<pre><code>experiment$Time &lt;- 1:nrow(experiment)
summary(lmer(Response ~ Intervention + Time + (1|Phase), data=experiment))

...
 AIC BIC logLik deviance REMLdev
 298 312   -144      272     288
Random effects:
 Groups   Name        Variance Std.Dev.
 Phase    (Intercept) 0.0106   0.103   
 Residual             0.5055   0.711   
Number of obs: 127, groups: Phase, 1

Fixed effects:
             Estimate Std. Error t value
(Intercept)   3.42517    0.42325    8.09
Intervention -0.12398    0.12621   -0.98
Time         -0.00132    0.00129   -1.02

Correlation of Fixed Effects:
            (Intr) Intrvn
Interventin -0.128       
Time        -0.947 -0.021
</code></pre>
"
"0.118389266011054","0.13201967239689"," 61008","<p>I'm a psycholinguistics student with few knowledge in statistics and I have some doubts about a Correlation of Fixed Effects in lmer function (lme4 package). So, if my question is stupidâ€¦ hummmâ€¦ I'm sorry!</p>

<p>My response variable is RT (Reaction Time in a self-paced reading experiment) and my independent variables are Ant (PP, NP) and Verbo (SG, PL). </p>

<p>I have modeled the data with intercepts for Sujeitos (the people who are doing the task) and Item (the sentences i've used), asking for principal effects and interactions between the variables. Here is the model <code>lmer(RT~Ant*Verbo+(1|Sujeitos)+(1|Item))</code> and that's the coefficients for fixed effects:</p>

<p><img src=""http://i.stack.imgur.com/MM27e.jpg"" alt=""Table Fixed Effects""></p>

<p>So, I have made a table of the coefficients for the interactions.</p>

<p>My problem is: that -0.705 and -0.716 correlation effects are a problem for my interaction terms? I'm saying this because the coefficients for the condition NP:SG came from the coefficients of SG only:</p>

<p><img src=""http://i.stack.imgur.com/vsYZo.jpg"" alt=""NP:SG""></p>

<p>and the coefficients for PP:PL came from the PP only:</p>

<p><img src=""http://i.stack.imgur.com/t0f8G.jpg"" alt=""PP:PL""></p>

<p>So, to me, there is no problem here, because I'm not contrasting PP:SG x PP (correlation = -0.705) and I'm not contrasting PP:SG x SG ((correlation = -0.716)). But I do that when getting the coefficients for PP:SG:</p>

<p>ï¿¼<img src=""http://i.stack.imgur.com/xBlx7.jpg"" alt=""PP:SG""></p>

<p>In the last case, there is a contrast between PP:SG x PP and SG. So, the correlation could be a problem. Is this correct? And, if so, how can I deal with this? I've read some thinks in <a href=""http://hlplab.wordpress.com/2011/02/24/diagnosing-collinearity-in-lme4/"" rel=""nofollow"">Jaeger's blog</a> and in this book: <code>Howell, 2010. Statistical Methods for Psychology</code>, but it doesn't help much.</p>

<p>Thank you.</p>
"
"0.0552368176666107","0.0808452083454443"," 64226","<p>I was wondering if anyone could enlighten me on the current differences between these two functions. I found the following question: <a href=""http://stats.stackexchange.com/questions/5344/how-to-choose-nlme-or-lme4-r-library-for-mixed-effects-models"">How to choose nlme or lme4 R library for mixed effects models?</a>, but that dates from a couple of years ago. That's a lifetime in software circles.</p>

<p>My specific questions are:</p>

<ul>
<li>Are there (still) any correlation structures in <code>lme</code> that <code>lmer</code> doesn't handle?</li>
<li>Is it possible/recommended to use <code>lmer</code> for panel data?</li>
</ul>

<p>Apologies if these are somewhat basic.</p>

<p>A bit more detail: panel data is where we have multiple measurements on the same individuals, at different points in time. I generally work in a business context, where you might have data for repeat/long-term customers over a number of years. We want to allow for variation over time, but clearly fitting a dummy variable for each month or year is inefficient. However, I'm unclear whether <code>lmer</code> is the appropriate tool for this sort of data, or whether I need the autocorrelation structures that <code>lme</code> has.</p>
"
"0.18563283764074","0.181129496216353"," 65489","<p>My problem can be summarized very simply: I'm using a linear mixed-effects model and I am trying to get p-values using pvals.fnc(). The problem is that this function seems to have some trouble estimating p-values directly from the t-values associated with model coefficients (Baayen et al., 2008), and I don't know what is going wrong with the way I do it (i.e. according to what I have read, it should work). So, I'm explaining my model below, and if you can point out what I am doing wrong and suggest changes I would really appreciate it!</p>

<p><strong>DESCRIPTION</strong>: I have a 2 by 2 within subjects design, fully crossing two <em>categorical</em> factors, ""Gram"" and ""Number"", each with two levels. This is the command I used to run the model:</p>

<pre><code>&gt;m &lt;- lmer(RT ~ Gram*Number + (1|Subject) + (0+Gram+Number|Subject) + (1|Item),data= data)
</code></pre>

<p>If I understand this code, I am getting coefficients for the two fixed effects (Gram and Number) and their interaction, and I am fitting a model that has by-subject intercepts and slopes for the two fixed effects, and a by-item intercept for them. Following Barr et al. (2013), I thought that this code gets rid of the correlation parameters. I don't want estimate the correlations because I want to get the p-values using pvals.fnc (), and I read that this function doesn't work if there are correlations in the model.</p>

<p>The command seems to work:</p>

<pre><code>&gt;m
Linear mixed model fit by REML 
Formula: RT ~ Gram * Number + (1 | Subject) + (0 + Gram + Number | Subject) + (1 |Item) 
   Data: mverb[mverb$Region == ""06v1"", ] 
   AIC   BIC logLik deviance REMLdev
 20134 20204 -10054    20138   20108
Random effects:
 Groups      Name        Variance  Std.Dev. Corr          
 Item       (Intercept)   273.508  16.5381               
 Subject     Gramgram        0.000   0.0000               
             Gramungram   3717.213  60.9689    NaN        
             Number1        59.361   7.7046    NaN -1.000 
 Subject     (Intercept) 14075.240 118.6391               
 Residual                35758.311 189.0987               
Number of obs: 1502, groups: Item, 48; Subject, 32

Fixed effects:
             Estimate Std. Error  t value
(Intercept)    402.520     22.321  18.033
Gram1          -57.788     14.545  -3.973
Number1         -4.191      9.858  -0.425
Gram1:Number1   15.693     19.527   0.804

Correlation of Fixed Effects:
            (Intr) Gram1  Numbr1
Gram1       -0.181              
Number1     -0.034  0.104       
Gram1:Nmbr1  0.000 -0.002 -0.011
</code></pre>

<p>However, when I try to calculate the p-values I still get an error message:</p>

<pre><code>&gt;pvals.fnc(m, withMCMC=T)$fixed
Error in pvals.fnc(m, withMCMC = T) : 
MCMC sampling is not implemented in recent versions of lme4
  for models with random correlation parameters
</code></pre>

<p>Am I making a mistake when I specify my model? Shouldn't pvals.fnc() work if I removed the correlations?</p>

<p>Thanks for your help!</p>
"
"0.224585387973546","0.219137081015428"," 68363","<p>A coworker and I are trying to analyze agreement between two measurement methods.  I apologize in advance for needing some extra explanation due to the fact I'm an engineer whose statistics background is mostly geared toward the relationship between signal-to-noise ratio and bit error rates, and other analysis of random processes.</p>

<p>For method comparison, it is natural to create a Bland-Altman plot (and we've done so).  However our data has some additional characteristics that Bland-Altman style analysis doesn't account for.  Furthermore, we're trying to compare our results to an earlier study that published a correlation coefficient resulting from mixed-effect analysis, unfortunately this publication didn't say whether they were reporting Pearson correlation coefficient or Intra-class correlation (maybe there are others too?).</p>

<p>The characteristics of our data set are:</p>

<ul>
<li>Multiple test subjects</li>
<li>Multiple observation instants for each test subject, sequentially ordered and equally spaced in time</li>
<li>The subjects are time varying, but receiving treatment so that the time dependent changes are not monotonic</li>
<li>At each observation instant, one measurement is made using each methods</li>
</ul>

<p>A statistician here at our university warned us that a simple paired analysis wasn't appropriate because there's a subject-specific effect, and pointed us to mixed-effects analysis but couldn't help further.</p>

<p>I read several articles on mixed-effect analysis, but most of them are a comparison of groups, rather than a group of comparisons, if that makes sense.  The information I found on intra-class correlation said it treats the measurements within the class interchangeably, and that seems suspect here.</p>

<p>This article uses mixed-effect analysis for method comparison, but has repeated measurements instead of a series of time-separated measurements.  It also doesn't cover correlation coefficients on grouped data.</p>

<ul>
<li><a href=""http://www.degruyter.com/view/j/ijb.2008.4.1/ijb.2008.4.1.1107/ijb.2008.4.1.1107.xml"" rel=""nofollow"">Statistical Models for Assessing Agreement in Method Comparison Studies with Replicate Measurements</a></li>
</ul>

<p>Here's what I've done so far, using R:</p>

<p>Load the data</p>

<pre><code>data &lt;- read.table(filename, header=TRUE, sep="","");
</code></pre>

<p>Convert variables to cases, adding factors (is it correct to create a factor for the encounter, since the time indicators are independent for each test subject?):</p>

<pre><code>library(""reshape"")
mdata &lt;- within(melt(data, variable_name=""method"", id=c(""subject"", ""time"")), {
  subject &lt;- factor(subject)
  time &lt;- factor(interaction(subject, time))
  method &lt;- factor(method)
})
</code></pre>

<p>Run linear mixed-effects model.  I've chosen an autocorrelation structure for the random subject/time covariance matrices, because of the nice periodic measurements.</p>

<pre><code>library(nlme)
lm2 &lt;- lme(value ~ method, random = list( ~1|subject, ~1|time ), corr = corAR1(), data = mdata)
</code></pre>

<p>I'd like to know whether I've assigned the right factors to fixed and random effects.  Also, from my research I guess there should be a random effect on method*subject, but it shouldn't have AR(1) structure and I don't know how to give different structure to different random effects.</p>

<p>Finally, I did calculate a correlation coefficient, using intra-class correlation and encounter as the class to get measurements paired properly.  But I don't think this is using the subject grouping, and as I said earlier, I don't feel like treating class members interchangeably is right.</p>

<pre><code>library(psychometric)
r2 &lt;- ICC1.lme(value, time, mdata)
</code></pre>

<p>What would you do differently?  It seems like the <code>lmer</code> function was a bit easier to describe random effect nested groups, but I didn't find a way to control the correlation structure.</p>
"
"0.143509461970482","0.140028008402801"," 70227","<p>I want to model the infection rates in bees based on weather conditions. The weather variables are rolling means for different time periods and durations. Dependent data is infection levels gathered in March and the independent variables are the weather aggregates (e.g. from 30 day period from Jan1-Jan30, 90 day period from Dec1-Feb28), a few thousands of them and highly correlated.</p>

<p>PCA techniques did not work since the infections are not so strongly related to weather. I have also tried Bayesian Model Averaging and Boosted Regression Trees, since variables could be selected based on variable importance they calculate.</p>

<p>But since, my data is longitudinal and my apiaries have a fixed location, I think mixed-models are a good choice. Is there a way to do variable selection based on mixed-models?</p>

<p>What I have done now is to<br>
1. run <code>glmer</code> for each of the independent variables separately,<br>
2. remove those variable whose p-values for fixed-effect estimates are below 0.05 (not sure if this is a right thing - if the estimate for a variable is not significant, that variable being the only one in the model, it is right to drop that variable, is it?)<br>
3. from the variables that are left over, test for correlation between the variables<br>
4. remove the variables that are highly correlated, giving preference to the variable that has the lowest AIC.  </p>

<p>Or should I at this stage, not worry about p-values of Intercepts and only focus on AIC (or BIC)? since some of the variables have high p-values but AICs lower than than those with low p-values. </p>

<p>I have tried reading up a lot, and there is no one fool-proof solution for variable selection, but would like to know if there is anything inherently wrong with my method. As I am not a statistician, equations often look like beautiful Arabic calligraphy and there lies my dead-end.</p>
"
"0.135302018298348","0.13201967239689"," 72569","<p>What does it mean when two random effects are highly or perfectly correlated?<br>
That is, in R when you call summary on a mixed model object, under ""Random effects"" ""corr"" is 1 or -1.</p>

<pre><code>summary(model.lmer) 
Random effects:
Groups   Name                    Variance   Std.Dev.  Corr                 
popu     (Intercept)             2.5714e-01 0.5070912                      
          amdclipped              4.2505e-04 0.0206167  1.000               
          nutrientHigh            7.5078e-02 0.2740042  1.000  1.000        
          amdclipped:nutrientHigh 6.5322e-06 0.0025558 -1.000 -1.000 -1.000
</code></pre>

<p>I know this is bad and indicates that the random effects part of the model is too complex, but I'm trying to understand</p>

<ul>
<li>1)what is doing on statistically  </li>
<li>2)what is going on practically with
the structure of the response variables.</li>
</ul>

<p><strong>Example</strong></p>

<p>Here is an example based on ""<a href=""http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;ved=0CDYQFjAC&amp;url=http://glmm.wdfiles.com/local--files/examples/Banta_ex.pdf&amp;ei=hTNYUpuzBu7J4APN5YHYBg&amp;usg=AFQjCNG65VjvqOLeYLFxJZnzmlMevgEbuA&amp;bvm=bv.53899372,d.dmg"">GLMMs in action: gene-by-environment interaction in total fruit production of wild populations of Arabidopsis thaliana</a>""
by Bolker et al</p>

<p>Download data</p>

<pre><code>download.file(url = ""http://glmm.wdfiles.com/local--files/trondheim/Banta_TotalFruits.csv"", destfile = ""Banta_TotalFruits.csv"")
dat.tf &lt;- read.csv(""Banta_TotalFruits.csv"", header = TRUE)
</code></pre>

<p>Set up factors</p>

<pre><code>dat.tf &lt;- transform(dat.tf,X=factor(X),gen=factor(gen),rack=factor(rack),amd=factor(amd,levels=c(""unclipped"",""clipped"")),nutrient=factor(nutrient,label=c(""Low"",""High"")))
</code></pre>

<p>Modeling log(total.fruits+1) with ""population"" (popu) as random effect</p>

<pre><code>model.lmer &lt;- lmer(log(total.fruits+1) ~ nutrient*amd + (amd*nutrient|popu), data= dat.tf)
</code></pre>

<p>Accessing the Correlation matrix of the random effects show that everything is perfectly correlated</p>

<pre><code>attr(VarCorr(model.lmer)$popu,""correlation"")

                         (Intercept) amdclipped nutrientHigh amdclipped:nutrientHigh
(Intercept)                       1          1            1                      -1
amdclipped                        1          1            1                      -1
nutrientHigh                      1          1            1                      -1
amdclipped:nutrientHigh          -1         -1           -1                       1
</code></pre>

<p>I understand that these are the correlation coefficients of two vectors of random effects coefficients, such as</p>

<pre><code>cor(ranef(model.lmer)$popu$amdclipped, ranef(model.lmer)$popu$nutrientHigh)
</code></pre>

<p>Does a high correlation mean that the two random effects contain redundant information?  Is this analogous to multicollinearity in multiple regression when a model with highly correlated predictors should be simplified?</p>
"
"0.151272255204013","0.147602480923349"," 73238","<p>Lately I keep encountering the same problem and I'm wondering whether other people have been able to get around it. I'm running a mixed effects model using <strong>lmer()</strong>. My model has by-subject and by-item intercepts and slopes, and random correlation parameters between them. Since the current version of lmer() does not have MCMC sampling implemented, I cannot use pvals.fnc(). I get this message:</p>

<pre><code>Error in pvals.fnc(m, withMCMC = T) : 
MCMC sampling is not implemented in recent versions of lme4
for models with random correlation parameters
</code></pre>

<p>pvals.fnc() is also the function I use to get confidence intervals (<strong>HPD95lower</strong> and  <strong>HPD95upper</strong> were two columns in the pvals.fnc output). Does anyone know of an alternative way of getting confidence intervals for the fixed effects estimates in the model? Or does using models with random correlations means that we can no longer get CIs from R? </p>

<p>Thanks!</p>

<p><strong>NOTE</strong>: I've seen this question asked in other forums in slightly different ways. However, the answers always seem to involve (1) calculating something different as an alternative to the confidence intervals, (2) some complicated solution that is unclear  (at least to me) how to implement. I would like to know if there is some alternative way of computing CIs that is both mainstream (so that other researchers can use it) and has a function to do it in R, since I am not a programmer and I feel that trying to create that function myself would be error prone. </p>
"
"0.071754730985241","0.0933520056018673"," 73346","<p>I'm analysing data from our experiment. We had participants in 4 groups, each participant was measured 4 times. We measured cortisol in saliva, so it leads us to the linear mixed models, because the individual cortisol levels have different slopes.
I have fitted following model:</p>

<pre><code>lmer1 &lt;- lmer(Cortisol ~ group*measurement + (1|id), data=df)
</code></pre>

<p>I used treatment codig for both categorical variables, because we are interested in differences between 1st measurement in first group with other measurements. </p>

<p>My problem is, that I get strong correlations between factor levels and I'm not sure, how to solve it. Contrast coding would be one solution, but it would answer different question (as I said, we want to compare differences between 1st group,1st measurement and all the others).</p>

<p>This is my correlation matrix for fixed effect from lmer method (lme4 package):</p>

<pre><code>          (Intr) group2 group3 groupP msrmn2 msrmn3 msrmn4 grp2:2 grp3:2 grpP:2 grp2:3 grp3:3 grpP:3 grp2:4 grp3:4
group2      -0.770                                                                                    
group3      -0.650  0.500                                                                             
groupP      -0.557  0.429  0.362                                                                      
measuremnt2 -0.602  0.464  0.391  0.335                                                               
measuremnt3 -0.598  0.460  0.388  0.333  0.521                                                        
measuremnt4 -0.602  0.464  0.391  0.335  0.524  0.521                                                 
grp2:msrmn2  0.461 -0.600 -0.299 -0.257 -0.765 -0.398 -0.401                                          
grp3:msrmn2  0.390 -0.300 -0.589 -0.217 -0.647 -0.337 -0.339  0.495                                   
grpP:msrmn2  0.329 -0.253 -0.214 -0.578 -0.546 -0.284 -0.287  0.418  0.353                            
grp2:msrmn3  0.461 -0.599 -0.300 -0.257 -0.402 -0.772 -0.402  0.519  0.260  0.220                     
grp3:msrmn3  0.383 -0.295 -0.579 -0.213 -0.333 -0.641 -0.333  0.255  0.501  0.182  0.495              
grpP:msrmn3  0.333 -0.256 -0.216 -0.585 -0.290 -0.557 -0.290  0.222  0.188  0.499  0.430  0.357       
grp2:msrmn4  0.462 -0.598 -0.300 -0.257 -0.402 -0.399 -0.767  0.518  0.260  0.220  0.518  0.256  0.223  
grp3:msrmn4  0.390 -0.300 -0.589 -0.217 -0.339 -0.337 -0.647  0.260  0.510  0.185  0.260  0.501  0.188  0.496
grpP:msrmn4  0.329 -0.253 -0.214 -0.578 -0.287 -0.284 -0.546  0.219  0.185  0.493  0.220  0.182  0.499  0.419  0.353
</code></pre>

<p>Do you have suggestions about how to solve this (reduce collinearity/ignore it)?</p>
"
"0.208514414057075","0.203455979297691"," 76980","<p>I'm trying to analyse some data I've recently gotten my hands on, but I'm not entirely sure which model to use. One suggestion has been a Mixed Model, Repeated Measurements ANOVA, but I'm not sure if that such kind of model can answer the questions of interest.</p>

<p><strong>The data</strong>: 
Two individual persons (A and B) have had a lot of different values (V1, V2, V3, ..., Vn) measured four times (At T0, T1, T2 and T3) - The spacing between times differs.</p>

<p>The different values have been grouped into categories (C1, C2, C3, ..., Cn). One value may belong to none, one or multiple categories. Each of the categories have a continuos value (Response_C1,Response_C1, ..., Response_Cn), which is the sum of the measured values belonging to that category. </p>

<p>In addition to this, person B was given a drug at T1.</p>

<p>What I would like to investigate now, is:</p>

<ol>
<li>Is there any observable effect after administering the drug</li>
<li>On which categories did the drug have an effect</li>
<li>If there is an effct on a category, what is the effect size</li>
<li>How does the effect vary over time</li>
<li>If there is an effect, is the effect observed from the drug at T1 still persistant at T3</li>
</ol>

<p>I realise one of the major pitfalls is the lack of both time points and samples, but it would be appreciated if you could suggest any articles/methods for this type of analysis.</p>

<p><strong>What I have tried so far</strong> is just Repeated Measurements ANOVA, using R:</p>

<pre><code>test.aov &lt;- aov(Response_C ~ Category * Timepoint * Treatment + Error(Sample), data=df)
</code></pre>

<p>But I am not sure that the model is correct, neither am I sure that it actually answers my questions, even if I try to model it as a mixed model. </p>

<p>Any help is much appreciated. Please let me know if any additional information is needed</p>

<p><strong>Edit 1:</strong> After doing some more reading, it seems a Generalised Linear Model with a negative binomial distribution (since this kind of data is usually over-dispersed) might be better suited for this kind of data, but I'm still not sure if such a model would answer the questions. Potentially I could fit a model to each individual category, but that would inflate the Type-I error I guess, and so we would need to correct for multiple testing.</p>

<p><strong>Edit 2:</strong> Some more reading, and I thought the <code>lme4</code> R package would be a good way to fit a Linear mixed model to my data, and just do individual comparisons of each category. Here's the model I tried to fit:</p>

<pre><code>lm1 &lt;- lmer(Response ~ Treatment * Timepoint + (1|Subject), data=my_data)
</code></pre>

<p>First off, I'm not sure whether Timepoint should be a factorial or a numerical value. As I mentioned, timepoints are not evenly distributed (To be precise, I have for time 0, 2days, 14 days, 90days), however, the design is balanced. If I enter the Timepoints as a numerical value, I don't get any estimate of what the value is at any given Timepoint, but just some numbers for Correlation of fixed effects, which I can't really use for anything. On the other hand, if I enter the Timepoints as factors, I do get an estimated value for the effect at each timepoint, but I'm not too sure how certain or reliable this value is.</p>
"
"0.0828552264999161","0.0808452083454443"," 77797","<p>I have some time course data which plotted looks like this:</p>

<p><img src=""http://i.stack.imgur.com/nkdV5.png"" alt=""enter image description here""></p>

<p>I have fitted a mixed model to my raw datapoints with R's <code>lme4::lmer</code>, as seen in <a href=""http://nbviewer.ipython.org/urls/gist.github.com/TheChymera/7669971/raw/e35bc10d5b6443d28eca44bce8cf17eaa7bf1a8a/TC_lme4"" rel=""nofollow"">this code</a>.</p>

<p>In essence I get the following output for my model:</p>

<p>measurement~condition*time:</p>

<pre><code>Random effects:
 Groups   Name        Variance Std.Dev.
 ID       (Intercept) 1118082  1057.4  
 Residual               58680   242.2  
Number of obs: 5784, groups: ID, 12

Fixed effects:
             Estimate Std. Error t value
(Intercept)  2480.379    305.375   8.122
CoIhard       -90.294     12.659  -7.133
Time          -17.326      3.884  -4.461
CoIhard:Time   62.931      5.492  11.458

Correlation of Fixed Effects:
            (Intr) CoIhrd Time  
CoIhard     -0.021              
Time        -0.025  0.611       
CoIhard:Tim  0.018 -0.864 -0.707
</code></pre>

<p>Now, what am I to make of these results? As I predicted in <a href=""http://stats.stackexchange.com/questions/77486/linear-model-or-component-analysis-on-timecourse-data"">this question</a> the factors only get one intercept value each, as if they would follow a steady linear increase/decrease. Obviously my time course does not.</p>

<p>What added information pertaining to the description of the difference between my conditions does fitting this model give me?</p>

<p>Cheers,</p>
"
"0.185269918744954","0.180775381515547"," 79684","<p>Trying to fit a linear mixed effects model with 2 categorical predictors (group &amp; worker) where worker is a random effect and group a fixed effect.  I'm trying to figure out 1) whether I should specify intercept=0 and 2) why these 2 model results seem to give different conclusions about the effect of group.</p>

<p>Model1: tps ~ group + (1 | worker)</p>

<p>Model2: tps ~ group + (1 | worker) + 0</p>

<pre><code>summary(Model1):
Linear mixed model fit by REML ['merModLmerTest']
Formula: tps ~ group + (1 | worker) 
   Data: mydata 

REML criterion at convergence: 3489.872 

Random effects:
 Groups   Name        Variance Std.Dev.
 worker   (Intercept) 1866     43.20   
 Residual             3165     56.26   
Number of obs: 318, groups: worker, 18

Fixed effects:
            Estimate Std. Error     df t value Pr(&gt;|t|)    
(Intercept)    70.15      15.59  11.27   4.501 0.000848 ***
group phone   -20.85      21.75  10.83  -0.959 0.358586    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr)
group phone -0.717

summary(Model2):
Linear mixed model fit by REML ['merModLmerTest']
Formula: tps ~ group + (1 | worker) + 0 
   Data: mydata 

REML criterion at convergence: 3489.872 

Random effects:
 Groups   Name        Variance Std.Dev.
 worker   (Intercept) 1866     43.20   
 Residual             3165     56.26   
Number of obs: 318, groups: worker, 18

Fixed effects:
               Estimate Std. Error    df t value Pr(&gt;|t|)    
group computer    70.15      15.59 11.27   4.501 0.000848 ***
group phone       49.30      15.17 10.40   3.251 0.008291 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            grpcmp
group phone 0.000 
</code></pre>

<p>In the first model the 'phone' effect is the same as the difference between the two groups' effects in model 2 (this makes sense because in model1 the 'computer' group is the baseline).  In model2, both groups' effects are significant, whereas in model1 only the intercept is significant.  </p>

<p>Which is the ""right"" model for a situation where the group predictor is binary?  It must be only one or the other (seems to indicate that model1 is correct, because there the intercept ""is the same as"" the computer group, right? Model2 allows a ""zero"" value for group which doesn't make sense).  Am I right about this?</p>

<p>And how to interpret the fact that in model1 the intercept is significant but 'phone' is not?</p>
"
"0.138092044166527","0.148216215299981"," 81430","<p>I have a mixed model and the data looks like this:</p>

<pre><code>&gt; head(pce.ddply)
  subject Condition errorType     errors
1    j202         G         O 0.00000000
2    j202         G         P 0.00000000
3    j203         G         O 0.08333333
4    j203         G         P 0.00000000
5    j205         G         O 0.16666667
6    j205         G         P 0.00000000
</code></pre>

<p>Each subject provides two datapoints for errorType (O or P) and each subject is in either Condition G (N=30) or N (N=33).  errorType is a repeated variable and Condition is a between variable.  I'm interested in both main effects and the interactions.  So, first an anova:</p>

<pre><code>&gt; summary(aov(errors ~ Condition * errorType + Error(subject/(errorType)),
                 data = pce.ddply))

Error: subject
          Df  Sum Sq  Mean Sq F value Pr(&gt;F)
Condition  1 0.00507 0.005065   2.465  0.122
Residuals 61 0.12534 0.002055               

Error: subject:errorType
                    Df  Sum Sq Mean Sq F value   Pr(&gt;F)    
errorType            1 0.03199 0.03199   10.52 0.001919 ** 
Condition:errorType  1 0.04010 0.04010   13.19 0.000579 ***
Residuals           61 0.18552 0.00304                     
</code></pre>

<p>Condition is not significant, but errorType is, as well as the interaction.</p>

<p>However, when I use lmer, I get a totally different set of results:</p>

<pre><code>&gt; lmer(errors ~ Condition * errorType + (1 | subject),
                    data = pce.ddply)
Linear mixed model fit by REML 
Formula: errors ~ Condition * errorType + (1 | subject) 
   Data: pce.ddply 
    AIC    BIC logLik deviance REMLdev
 -356.6 -339.6  184.3     -399  -368.6
Random effects:
 Groups   Name        Variance Std.Dev.
 subject  (Intercept) 0.000000 0.000000
 Residual             0.002548 0.050477
Number of obs: 126, groups: subject, 63

Fixed effects:
                       Estimate Std. Error t value
(Intercept)            0.028030   0.009216   3.042
ConditionN             0.048416   0.012734   3.802
errorTypeP             0.005556   0.013033   0.426
ConditionN:errorTypeP -0.071442   0.018008  -3.967

Correlation of Fixed Effects:
            (Intr) CndtnN errrTP
ConditionN  -0.724              
errorTypeP  -0.707  0.512       
CndtnN:rrTP  0.512 -0.707 -0.724
</code></pre>

<p>So for lmer, Condition and the interaction are significant, but errorType is not.</p>

<p>Also, the lmer result is exactly the same as a glm result, leading me to believe something is wrong.</p>

<p>Can someone please help me understand why they are so different?  I suspect I am using lmer incorrectly (though I've tried many other versions like (errorType | subject) with similar results.</p>

<p>(I have seen researchers use both approaches in the literature with similar data.)</p>
"
"0.341864343646777","0.333570919488468"," 82102","<p>I hope this is an appropriate forum to post this question. I recently upgraded my R software from 2.15.0 to 3.0.2. I also upgraded the lme4 package from .999999-0 to 1.1-2. After doing so, the results from one of my linear mixed models analyses have changed a bit unexpectedly. In some respects, I was expecting some change, as the lme4 developers very clearly stated that they had made some significant changes to some fundamental components in the package. However, the changes that I am seeing (described below) make me think that something else is awry. I will start by explaining the experimental design, which is quite simple and then the issue at hand.</p>

<p>My experiment is a basic repeated measures design. I used 24 ""Items"" that each appeared in three different ""Conditions"" (SmallClause_Som, NoSmallClause, SmallClause_NoSom). Levels of Condition were rotated across three presentation lists such that each Subject (45 total, each assigned to a particular list) only saw one level of each item.</p>

<p>I used lmer() for the analysis. Condition was entered in as a Fixed effect and ""Subject"" and ""Item"" were entered as Random effects.</p>

<p>The problem:
Using the current version of R 3.0.2 and lme4 1.1-2 with NoSmallClause as the reference level (and no weighting on any of the contrasts), the ConditionSmallClause_Som/NoSmallClause contrast produces a t value of 1.680. </p>

<p>But, when I change reference level to SmallClause_Som (to observe the one remaining contrast) I get not only a change in the polarity of the effect (plus to minus, as expected), but the values change as well.</p>

<p>When I use R 2.15.0 and lme4 .999999-0 (on another computer), I do not experience this issue. I get slightly different values, but they do not change (apart from the polarity) when I change reference level.</p>

<p>My colleague also tried my analysis for me using R 3.0.2 and a version of lme4 (pre version 1.0) (I don't know exactly which version, but it was before the major changes) and he also does not experience the issue.</p>

<p>R 2.15.0 lme4 1.1-2 (older) output:</p>

<pre><code>&gt; #set ref level
&gt; test$Condition &lt;- relevel(test$Condition, ref=""NoSmallClause"")

&gt; #Model 4: Random slopes by Subject and Item
&gt; test.lmer4=lmer(Rating~Condition+(1+Condition|Subject)+(1+Condition|Item),test, REML=FALSE)
&gt; summary(test.lmer4)

Linear mixed model fit by maximum likelihood 
Formula: Rating ~ Condition + (1 + Condition | Subject) + (1 + Condition |      Item) 
   Data: test 
 AIC  BIC logLik deviance REMLdev
 3930 4010  -1949     3898    3902
Random effects:
 Groups   Name                       Variance   Std.Dev. Corr          
 Subject  (Intercept)                0.98998765 0.994981               
          ConditionSmallClause_Som   0.00203374 0.045097 -1.000        
          ConditionSmallClause_NoSom 0.00019873 0.014097  1.000 -1.000 
 Item     (Intercept)                0.96231875 0.980978               
          ConditionSmallClause_Som   0.89924400 0.948285 -0.020        
          ConditionSmallClause_NoSom 0.62128577 0.788217 -0.256  0.361 
 Residual                            1.68810777 1.299272               
Number of obs: 1080, groups: Subject, 45; Item, 24

Fixed effects:
                       Estimate Std. Error t value
(Intercept)                  2.9583     0.2584  11.447
ConditionSmallClause_Som     0.3639     0.2165   1.680
ConditionSmallClause_NoSom   0.1472     0.1878   0.784

Correlation of Fixed Effects:
            (Intr) CnSC_S
CndtnSmlC_S -0.116       
CndtnSmC_NS -0.260  0.392

&gt; #set ref level
&gt; test$Condition &lt;- relevel(test$Condition, ref=""SmallClause_Som"")
&gt; 
&gt; #Model 4: Random slopes by Subject and Item
&gt; test.lmer4=lmer(Rating~Condition+(1+Condition|Subject)+(1+Condition|Item),test, REML=FALSE)
&gt; summary(test.lmer4)

Linear mixed model fit by maximum likelihood 
Formula: Rating ~ Condition + (1 + Condition | Subject) + (1 + Condition |      Item) 
   Data: test 
  AIC  BIC logLik deviance REMLdev
 3930 4010  -1949     3898    3902
Random effects:
 Groups   Name                       Variance  Std.Dev. Corr          
 Subject  (Intercept)                0.9023239 0.949907               
          ConditionNoSmallClause     0.0020340 0.045099 1.000         
          ConditionSmallClause_NoSom 0.0035039 0.059194 1.000  1.000  
 Item     (Intercept)                1.8238288 1.350492               
          ConditionNoSmallClause     0.8992237 0.948274 -0.687        
          ConditionSmallClause_NoSom 0.9804329 0.990168 -0.604  0.670 
 Residual                            1.6881050 1.299271               
Number of obs: 1080, groups: Subject, 45; Item, 24

Fixed effects:
                           Estimate Std. Error t value
(Intercept)                  3.3222     0.3174  10.468
ConditionNoSmallClause      -0.3639     0.2165  -1.680
ConditionSmallClause_NoSom  -0.2167     0.2243  -0.966

Correlation of Fixed Effects:
            (Intr) CndNSC
CndtnNSmllC -0.588       
CndtnSmC_NS -0.521  0.638
</code></pre>

<p>R 3.0.2 and lme4 1.1-2 (newer) output:</p>

<pre><code>&gt; #set ref level
&gt; test$Condition &lt;- relevel(test$Condition, ref=""NoSmallClause"")

&gt; #Model 4: Random slopes by Subject and Item
&gt; summary(test.lmer4)

Linear mixed model fit by maximum likelihood ['lmerMod']
Formula: Rating ~ Condition + (1 + Condition | Subject) + (1 + Condition |      Item) 
   Data: test 

      AIC       BIC    logLik  deviance 
 3942.557  4022.312 -1955.278  3910.557 

Random effects:
 Groups   Name                       Variance Std.Dev. Corr       
 Subject  (Intercept)                0.9522   0.9758              
          ConditionSmallClause_NoSom 0.1767   0.4204    0.03      
          ConditionSmallClause_Som   0.1760   0.4196   -0.15  0.92
 Item     (Intercept)                1.2830   1.1327              
          ConditionSmallClause_NoSom 0.7782   0.8822   -0.41      
          ConditionSmallClause_Som   1.4901   1.2207    0.09  0.41
 Residual                            1.6466   1.2832              
Number of obs: 1080, groups: Subject, 45; Item, 24

Fixed effects:
                           Estimate Std. Error t value
(Intercept)                  2.9583     0.2814  10.512
ConditionSmallClause_NoSom   0.1472     0.2133   0.690
ConditionSmallClause_Som     0.3639     0.2741   1.327

Correlation of Fixed Effects:
            (Intr) CSC_NS
CndtnSmC_NS -0.357       
CndtnSmlC_S -0.007  0.451
&gt; #anova (test.lmer3, test.lmer4)
&gt; 
&gt; #set ref level
&gt; test$Condition &lt;- relevel(test$Condition, ref=""SmallClause_Som"")
&gt; 
&gt; #Model 4: Random slopes by Subject and Item
&gt; test.lmer4=lmer(Rating~Condition+(1+Condition|Subject)+(1+Condition|Item),test, REML=FALSE)
&gt; summary(test.lmer4)
Linear mixed model fit by maximum likelihood ['lmerMod']
Formula: Rating ~ Condition + (1 + Condition | Subject) + (1 + Condition |      Item) 
   Data: test 

      AIC       BIC    logLik  deviance 
 3951.357  4031.113 -1959.679  3919.357 

Random effects:
 Groups   Name                       Variance Std.Dev. Corr       
 Subject  (Intercept)                0.88980  0.9433              
          ConditionNoSmallClause     0.04299  0.2073   0.83       
          ConditionSmallClause_NoSom 0.01562  0.1250   0.90  0.67 
 Item     (Intercept)                2.39736  1.5483              
          ConditionNoSmallClause     0.72053  0.8488   -0.04      
          ConditionSmallClause_NoSom 1.87804  1.3704   -0.16  0.53
 Residual                            1.65166  1.2852              
Number of obs: 1080, groups: Subject, 45; Item, 24

Fixed effects:
                           Estimate Std. Error t value
(Intercept)                  3.3222     0.3525   9.425
ConditionNoSmallClause      -0.3639     0.2004  -1.816
ConditionSmallClause_NoSom  -0.2167     0.2963  -0.731

Correlation of Fixed Effects:
            (Intr) CndNSC
CndtnNSmllC -0.045       
CndtnSmC_NS -0.160  0.514
</code></pre>

<p>My question:
What is going on here? Why is changing the reference level producing a shift from 1.327 to -1.816 in the t scores for the new version of lme4 whereas it produces the same (disregarding sign) value of 1.680/-1.680 in the old version's t scores? Only the older version seems to make sense to me.</p>

<p>1) Am I specifying my model incorrectly for the new version of lme4?</p>

<p>2) Am I missing some basic fundamental fact about how contrasts work? That is, is it possible to get different values just from changing the reference level? (the correlation values look a bit odd in the newer output).</p>

<p>3) Is this a bug in lme4?</p>

<p>4) Some other explanation...?</p>

<p>I have had some other odd issues as well with this same analysis using lme4 1.1-2. For example, if I don't clear the workspace and re-run an analysis, the values also will change between analyses (and also within the analysis as I change the reference level). This never happened to me on the earlier version (and it still does not happen when I run it on the earlier version now).</p>

<p>I hope someone can help with this. I found two other similar questions online (after much searching) but neither had any informative responses.</p>

<p>Thanks DT</p>
"
"0.071754730985241","0.0933520056018673"," 82981","<p>I am exploring hierarchical logistic regression, using glmer from the lme4 package. To my understanding, one of the first steps in multilevel modeling is to estimate the degree of clustering of level-1 units within level-2 units, given by the intraclass correlation (to ""justify"" the additional cost of estimating parameters to account for the clustering). When I run a fully unconditional model with glmer</p>

<pre><code>fitMLnull &lt;- glmer(outcome ~ 1 + (1|level2.ID), family=binomial)
</code></pre>

<p>the glmer fit gives me the variance in the intercept for outcome at level-2, but the residual level-1 variance in outcome is nowhere to be found. I read somewhere (can't track it down now) that the residual level-1 variance is <em>not</em> estimated in HGLM (or at least in glmer). Is this true? If so, is there an alternative way to approximate the degree of clustering in the data? If not, how can I access this value to calculate the ICC?</p>
"
"0.151272255204013","0.147602480923349"," 82984","<p>I am currently running some mixed effect linear models. </p>

<p>I am using the package ""lme4"" in R. </p>

<p>My models take the form:</p>

<pre><code>model &lt;- lmer(response ~ predictor1 + predictor2 + (1 | random effect))
</code></pre>

<p>Before running my models, I checked for possible multicollinearity between predictors.</p>

<p>I did this by:</p>

<p>Make a dataframe of the predictors</p>

<pre><code>dummy_df &lt;- data.frame(predictor1, predictor2)
</code></pre>

<p>Use the ""cor"" function to calculate Pearson correlation between predictors.</p>

<pre><code>correl_dummy_df &lt;- round(cor(dummy_df, use = ""pair""), 2) 
</code></pre>

<p>If ""correl_dummy_df"" was greater than 0.80, then I decided that predictor1 and predictor2 were too highly correlated and they were not included in my models.</p>

<p>In doing some reading, there would appear more objective ways to check for multicollinearity.   </p>

<p>Does anyone have any advice on this?</p>

<p>The ""Variance Inflation Factor (VIF)"" seems like one valid method. </p>

<p>VIF can be calculated using the function ""corvif"" in the AED package (non-cran). The package can be found at <a href=""http://www.highstat.com/book2.htm"">http://www.highstat.com/book2.htm</a>. The package supports the following book:</p>

<p>Zuur, A. F., Ieno, E. N., Walker, N., Saveliev, A. A. &amp; Smith, G. M. 2009. Mixed effects models and extensions in ecology with R, 1st edition. Springer, New York.</p>

<p>Looks like a general rule of thumb is that if VIF is > 5, then multicollinearity is high between predictors. </p>

<p>Is using VIF more robust than simple Pearson correlation?</p>

<p><strong>Update</strong></p>

<p>I found an interesting blog at: </p>

<p><a href=""http://hlplab.wordpress.com/2011/02/24/diagnosing-collinearity-in-lme4/"">http://hlplab.wordpress.com/2011/02/24/diagnosing-collinearity-in-lme4/</a></p>

<p>The blogger provides some useful code to calculate VIF for models from the lme4 package.</p>

<p>I've tested the code and it works great. In my subsequent analysis, I've found that multicollinearity was not an issue for my models (all VIF values &lt; 3). This was interesting, given that I had previously found high Pearson correlation between some predictors. </p>

<p>Kind Regards,
Matt.</p>
"
"0.208514414057075","0.203455979297691"," 83458","<p>My question is about the best way to estimate the effect of a predictor on a dependent variable, while accounting for several other predictors that may correlate with the predictor of interest. I'm using a linear mixed-effects model, using the <code>lmer</code> function from the R <code>lme4</code> package. (Warning: I'm fairly new at this, so their may be some misunderstandings woven through my question.)</p>

<h2>The problem</h2>

<p>To make things a bit more specific, I'll just explain the actual data that I'm working with. I have eye-movement data of participants freely viewing natural scenes. I want to determine whether pupil size predicts the 'visual saliency' (i.e. the conspicuity) of the locations in the image that participants are looking at. But there are many other things that correlate with pupil size, such as luminosity, and this makes the analysis tricky (or does it?).</p>

<h2>Option 1 (simple): Looking at fixed effects</h2>

<p>One option would be to simply create a linear mixed-effects model that has all predictors of saliency that I can think of, including the predictor of interest (<code>pupil_size</code>), as fixed effects and <code>subject</code> and <code>scene</code> as random effects. (To keep things manageable, I'm using a purely additive model, although I suppose that this is a whole topic in itself.)</p>

<pre><code>my_lmer = lmer(saliency ~ brightness + (.. lots of predictors ...)
    + pupil_size + (1|subject) + (1|scene))
</code></pre>

<p>This will give me a t-value for the fixed effect <code>pupil_size</code>. From what I understand, this fixed effect will already be partial, so it's the unique predictive power of pupil size, with any correlations between fixed effects already taken into account. Is my understanding correct?</p>

<h2>Option 2 (complex): Using model comparison</h2>

<p>An alternative approach, which I have from <a href=""http://www.sciencedirect.com/science/article/pii/S0749596X07001398"">Baayen et al. (2008)</a>, is to compare a model without pupil size as fixed effect (<code>simple_model</code>) to a model with pupil size as fixed effect (<code>complex_model</code>).</p>

<pre><code>simple_model = lmer(saliency ~ brightness + (.. lots of predictors ...)
    + (1|subject) + (1|scene))
complex_model = lmer(saliency ~ brightness + (.. lots of predictors ...)
    pupil_size + (1|subject) + (1|scene))
</code></pre>

<p>Now I can use the <code>anova</code> function to compare these two models (see Baayen's paper for an example). This will give me a <code>Chisq Chi</code> value, and I can use this to determine whether adding <code>pupil_size</code> as fixed effect is a justified addition to the model.</p>

<p>Clearly, this model comparison approach is more complex than simply looking at the t-values for fixed effects in a single model. And it seems to me that if <code>pupil_size</code> is a significant predictor (per Option 1), then it must also be a significant addition to the model (per Option 2).</p>

<p>In sum, my question is: <em>Is there any reason to do a model comparison (Option 2), or am I better off just creating a single linear mixed-effects model and seeing whether the t-value associated with <code>pupil_size</code> as fixed effect is sufficiently high (Option 1)?</em></p>
"
"0.23935954322198","0.242535625036333"," 86032","<p>I'm currently working with a data set that has numerous samples collected over time at different sites in a study area, and I'm interested in detecting a trend over time for that area.  I know that in an ideal experimental or balanced situation, using a random slope and intercept model is a great way to get at the overall trend within the study area.  With our data, however, many of the sites are missing samples and a handful of the sites only have one data point.</p>

<p>I'm curious if there's a way to intuitively understand how the sample imbalance will affect the estimate of the overall slope?  To put it differently, are there ways to know if sample imbalances are causing problems,  or are there things I can look for in my model output that would indicate I shouldn't trust what the model is estimating?</p>

<p>I created a contrived example with 20 data points to look at this. I put 10 data points with a slope of 1 into one site (a), and put the other 10 data points with a slope of -1 into unique sites (b through l).  I had assumed that when I looked at both a random intercept and random slope and intercept model that they would be somewhat similar, or that at least the latter would give more weight to the site with good data over time.</p>

<pre><code>&gt; library(lme4)
&gt; set.seed(9999)

&gt; x = c(0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9) + rnorm(20,mean=0,sd=0.1)
&gt; y = c(0,1,2,3,4,5,6,7,8,9,9,8,7,6,5,4,3,2,1,0) + rnorm(20,mean=0,sd=0.1)
&gt; z = c(rep('a',10),'b','c','d','e','f','h','i','j','k','l')
&gt; z = factor(z)

&gt; m0 = lm(y~x)
&gt; m1 = lmer(y~x+(1|z))
&gt; m2 = lmer(y~x+(1+x|z))

&gt; summary(m0)
&gt; summary(m1)
&gt; summary(m2)
&gt; anova(m1,m2)
</code></pre>

<p>As expected, the slope of the linear model was near zero, but the results for the two mixed effects models were nearly opposite.  Even though sites b through l only have one data point, it seems like they contribute more towards the slope because the trend is occurring over so many sites.  The random slope and intercept model was also preferred to using model selection criteria.</p>

<pre><code> &gt; summary(m0)$coefficients
                Estimate Std. Error    t value    Pr(&gt;|t|)
 (Intercept)  4.53784796  1.2586990  3.6051890 0.002023703
 x           -0.01178748  0.2335094 -0.0504797 0.960296079

 &gt; summary(m1)
 Linear mixed model fit by REML ['lmerMod']
 Formula: y ~ x + (1 | z) 

 REML criterion at convergence: 62.0877 

 Random effects:
  Groups   Name        Variance Std.Dev.
  z        (Intercept) 33.30788 5.7713  
  Residual              0.01583 0.1258  
 Number of obs: 20, groups: z, 11

 Fixed effects:
             Estimate Std. Error t value
 (Intercept) -0.03597    1.74163   -0.02
 x            0.99332    0.01386   71.66

 Correlation of Fixed Effects:
   (Intr)
 x -0.036

 &gt; summary(m2)
 Linear mixed model fit by REML ['lmerMod']
 Formula: y ~ x + (1 + x | z) 

 REML criterion at convergence: 31.0386 

 Random effects:
  Groups   Name        Variance Std.Dev. Corr 
  z        (Intercept) 7.78818  2.7907        
      x           0.37691  0.6139   -1.00
  Residual             0.01524  0.1234        
 Number of obs: 20, groups: z, 11

 Fixed effects:
             Estimate Std. Error t value
 (Intercept)   8.2121     0.8566   9.587
 x            -0.8201     0.1882  -4.358

 Correlation of Fixed Effects:
   (Intr)
 x -0.999

 &gt; anova(m1,m2)
 Data: 
 Models:
 m1: y ~ x + (1 | z)
 m2: y ~ x + (1 + x | z)
    Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
 m1  4 66.206 70.189 -29.103   58.206                             
 m2  6 36.745 42.719 -12.372   24.745 33.462      2  5.419e-08 ***
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I see that under this extreme example, the random slope and intercept have an almost perfect correlation.  Is what I can pull from this is that, in a sense, the model gives more value to the sites with only one data point because the overall trend is so strong but across multiple sites, but that I should view the slope estimate this model produces as suspect with such a high correlation?  Is there anything else that should look for?  For my specific study, I could also set some sort of criteria for what level of replication I thought was necessary to make proper inferences, e.g. eliminate all the sites that less than five samples.</p>

<p>Many thanks for your thoughts.</p>
"
"0.209466539620948","0.204385006865824"," 86889","<p>I calculated the least-squares means and standard errors for a linear mixed model. I am attempting to plot the lsmeans and standard errors for the combinations of the two factors, but I notice a discrepancy in what is allegedly significant.</p>

<pre><code>&gt; library(lmerTest)

&gt; print(summary(data_dist))
   ID        interface    direction    error_dist      
12     : 18   fs     :108   depth :72   Min.   :-0.34375  
13     : 18   none   :108   height:72   1st Qu.:-0.13037  
14     : 18   RW_none:  0   width :72   Median :-0.04048  
15     : 18                             Mean   :-0.03464  
16     : 18                             3rd Qu.: 0.06022  
17     : 18                             Max.   : 0.36864  
(Other):108

&gt; print(summary(model_error_dist))
Linear mixed model fit by REML ['merModLmerTest']
Formula: error_dist ~ direction * interface + (1 | ID) + (1 | ID:direction) 
Data: data_master 

REML criterion at convergence: -320.9162 

Random effects:
Groups       Name        Variance Std.Dev.
ID:direction (Intercept) 0.005864 0.07658 
ID           (Intercept) 0.003395 0.05827 
Residual                 0.008463 0.09199 
Number of obs: 216, groups: ID:direction, 36; ID, 12

Fixed effects:
                                Estimate Std. Error         df t value Pr(&gt;|t|)  
(Intercept)                     0.019411   0.031728  35.080000   0.612   0.5446  
directionheight                -0.074568   0.038046  31.210000  -1.960   0.0590 .
directionwidth                 -0.058316   0.038046  31.210000  -1.533   0.1354  
interfacenone                  -0.037312   0.021683 176.810000  -1.721   0.0870 .
directionheight:interfacenone  -0.008412   0.030664 176.810000  -0.274   0.7841  
directionwidth:interfacenone    0.061797   0.030664 176.810000   2.015   0.0454 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) drctnh drctnw intrfc drctnh:
directnhght -0.600                             
directnwdth -0.600  0.500                      
interfacenn -0.342  0.285  0.285               
drctnhght:n  0.242 -0.403 -0.201 -0.707        
drctnwdth:n  0.242 -0.201 -0.403 -0.707  0.500 

&gt; st &lt;- step(model_error_dist)

&gt; print(st)

Random effects:
                   Chi.sq Chi.DF elim.num p.value
(1 | ID)             3.28      1     kept    0.07
(1 | ID:direction)  37.83      1     kept  &lt;1e-07

Fixed effects:
                    Sum Sq Mean Sq NumDF  DenDF F.value elim.num Pr(&gt;F)
direction           0.0446  0.0223     2  22.00  2.6379     kept 0.0940
interface           0.0206  0.0206     1 176.81  2.4308     kept 0.1208
direction:interface 0.0529  0.0265     2 176.81  3.1263     kept 0.0463

Least squares means:
                                 direction interface Estimate Standard Error   DF t-value Lower CI Upper CI p-value   
direction  depth                       1.0        NA   0.0008         0.0298 27.4    0.03  -0.0604   0.0619  0.9800   
direction  height                      2.0        NA  -0.0780         0.0298 27.4   -2.62  -0.1392  -0.0169  0.0143 * 
direction  width                       3.0        NA  -0.0267         0.0298 27.4   -0.89  -0.0878   0.0345  0.3790   
interface  fs                           NA       1.0  -0.0249         0.0229 12.8   -1.09  -0.0744   0.0246  0.2971   
interface  none                         NA       2.0  -0.0444         0.0229 12.8   -1.94  -0.0939   0.0051  0.0747 . 
direction:interface  depth fs          1.0       1.0   0.0194         0.0317 35.1    0.61  -0.0450   0.0838  0.5446   
direction:interface  height fs         2.0       1.0  -0.0552         0.0317 35.1   -1.74  -0.1196   0.0092  0.0909 . 
direction:interface  width fs          3.0       1.0  -0.0389         0.0317 35.1   -1.23  -0.1033   0.0255  0.2283   
direction:interface  depth none        1.0       2.0  -0.0179         0.0317 35.1   -0.56  -0.0823   0.0465  0.5762   
direction:interface  height none       2.0       2.0  -0.1009         0.0317 35.1   -3.18  -0.1653  -0.0365  0.0031 **
direction:interface  width none        3.0       2.0  -0.0144         0.0317 35.1   -0.45  -0.0788   0.0500  0.6523   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

 Differences of LSMEANS:
                                             Estimate Standard Error    DF t-value Lower CI Upper CI p-value   
direction depth-height                            0.1         0.0348  22.0    2.26   0.0066   0.1510   0.034 * 
direction depth-width                             0.0         0.0348  22.0    0.79  -0.0448   0.0996   0.439   
direction height-width                           -0.1         0.0348  22.0   -1.47  -0.1236   0.0209   0.154   
interface fs-none                                 0.0         0.0125 176.8    1.56  -0.0052   0.0442   0.121   
direction:interface  depth fs- height fs          0.1         0.0380  31.2    1.96  -0.0030   0.1521   0.059 . 
direction:interface  depth fs- width fs           0.1         0.0380  31.2    1.53  -0.0193   0.1359   0.135   
direction:interface  depth fs- depth none         0.0         0.0217 176.8    1.72  -0.0055   0.0801   0.087 . 
direction:interface  depth fs- height none        0.1         0.0380  31.2    3.16   0.0427   0.1979   0.004 **
direction:interface  depth fs- width none         0.0         0.0380  31.2    0.89  -0.0437   0.1114   0.381   
direction:interface  height fs- width fs          0.0         0.0380  31.2   -0.43  -0.0938   0.0613   0.672   
direction:interface  height fs- depth none        0.0         0.0380  31.2   -0.98  -0.1148   0.0403   0.335   
direction:interface  height fs- height none       0.0         0.0217 176.8    2.11   0.0029   0.0885   0.036 * 
direction:interface  height fs- width none        0.0         0.0380  31.2   -1.07  -0.1183   0.0368   0.292   
direction:interface  width fs- depth none         0.0         0.0380  31.2   -0.55  -0.0986   0.0566   0.585   
direction:interface  width fs- height none        0.1         0.0380  31.2    1.63  -0.0156   0.1396   0.113   
direction:interface  width fs- width none         0.0         0.0217 176.8   -1.13  -0.0673   0.0183   0.260   
direction:interface  depth none- height none      0.1         0.0380  31.2    2.18   0.0054   0.1606   0.037 * 
direction:interface  depth none- width none       0.0         0.0380  31.2   -0.09  -0.0811   0.0741   0.928   
direction:interface  height none- width none     -0.1         0.0380  31.2   -2.27  -0.1640  -0.0089   0.030 * 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Final model:
lme4::lmer(formula = error_dist ~ direction + interface + (1 | 
    ID) + (1 | ID:direction) + direction:interface, data = data_master, 
    REML = reml, contrasts = l)
</code></pre>

<p>Plotting the lsmeans and standard errors gives me this graph:
<img src=""http://i.stack.imgur.com/YBzp4.png"" alt=""plot of least-squares means and standard errors""></p>

<p>From this plot, I can see that my standard error is huge and most comparisons between means will likely be insignificant. However, in the ""Differences of LSMEANS"" section of the output above, I see four significant differences.</p>

<p>This one I expect, based on the plot above:</p>

<pre><code>direction:interface  depth fs- height none        0.1         0.0380  31.2    3.16   0.0427   0.1979   0.004 **
</code></pre>

<p>This one, for example, surprises me:</p>

<pre><code>direction:interface  height fs- height none       0.0         0.0217 176.8    2.11   0.0029   0.0885   0.036 *
</code></pre>

<p>It seems I don't actually understand what these outputs are telling me. Can somebody please explain the reason for this discrepancy? Thanks!</p>
"
"0.185269918744954","0.180775381515547"," 87050","<p>I want to fit a model without a correlation term between the random effects with <code>lme</code>. In <code>lmer</code> this is fairly straighforward....</p>

<pre><code># lmer without correlation term
m1 &lt;- lmer(distance ~ (1|Subject) + age + (0+age|Subject) + Sex, data = Orthodont)
VarCorr(m1)
# Groups    Name        Std.Dev.
# Subject   (Intercept) 1.474105
# Subject.1 age         0.099979
# Residual              1.402591
</code></pre>

<p>With <code>lme</code> I think I can drop the correlation term using the following specification...</p>

<pre><code># lme without correlation term?
m2 &lt;- lme(distance ~ age + Sex, data = Orthodont, random = list(~ 1 | Subject, ~-1+ age | Subject))
VarCorr(m2)
#             Variance            StdDev    
# Subject =   pdLogChol(1)                  
# (Intercept) 2.172946296         1.47409169
# Subject =   pdLogChol(-1 + age)           
# age         0.009996006         0.09998003
# Residual    1.967260819         1.40259075
</code></pre>

<p>I am not entirely convinced that these are the same models though, partly because I can not find any resources that details how to specify this specific form and partly because the output from <code>print</code> is a little mystifying to me...</p>

<pre><code>m2 
# Linear mixed-effects model fit by REML
#   Data: Orthodont 
#   Log-restricted-likelihood: -218.3227
#   Fixed: distance ~ age + Sex 
# (Intercept)         age   SexFemale 
#  17.5806928   0.6601852  -2.0117005 
# 
# Random effects:
#  Formula: ~1 | Subject
#         (Intercept)
# StdDev:    1.474092
# 
#  Formula: ~-1 + age | Subject %in% Subject
#                age Residual
# StdDev: 0.09998003 1.402591
# 
# Number of Observations: 108
# Number of Groups: 
#                Subject Subject.1 %in% Subject 
#                     27                     27 
</code></pre>

<p>In particular, what is <code>Subject %in% Subject</code> referring to? and why are the residual considered as part of the second random effect term?</p>
"
"0.047836487323494","0.0466760028009337"," 87445","<p>I have fitted random coefficient Poisson analysis in R. I have obtained the following results: </p>

<p>Generalized linear mixed model fit by maximum likelihood ['glmerMod']
Family: poisson ( log )</p>

<p>Formula: frequency ~ 1 + cc + ageveh + make + (1 | AREA) </p>

<p>Data: x </p>

<pre><code>  AIC       BIC    logLik  deviance 
</code></pre>

<p>1359.1477 1389.7370 -672.5739 1345.1477 </p>

<p>Random effects:</p>

<p>Groups Name        Variance Std.Dev.</p>

<p>AREA   (Intercept) 1.323    1.15 </p>

<p>Number of obs: 584, groups: AREA, 8</p>

<p>Fixed effects:
            Estimate Std. Error z value Pr(>|z|) </p>

<p>(Intercept) -0.12902    0.44432  -0.290   0.7715 </p>

<p>ccL          0.05656    0.12371   0.457   0.6475</p>

<p>agevehO      0.02136    0.09264   0.231   0.8177</p>

<p>make2       -0.45454    0.20632  -2.203   0.0276 *</p>

<p>make3       -0.31799    0.21422  -1.484   0.1377 </p>

<h2>make4       -0.29708    0.14469  -2.053   0.0401 *</h2>

<p>Signif. codes:  0 â€˜<strong><em>â€™ 0.001 â€˜</strong>â€™ 0.01 â€˜</em>â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1</p>

<p>Correlation of Fixed Effects:</p>

<pre><code>    (Intr) ccL    agevhO make2  make3
</code></pre>

<p>ccL      0.052    </p>

<p>agevehO -0.179 -0.232   </p>

<p>make2   -0.171 -0.007 -0.001 </p>

<p>make3   -0.156  0.022 -0.078  0.366 </p>

<p>make4   -0.300 -0.235  0.167  0.544  0.522</p>

<p>However I am unable to interpret the results. </p>
"
"0.0855725099681116","0.104370715180858"," 87510","<p>I have obtained the following results in R from a random coefficient Poisson analysis.</p>

<p>Generalized linear mixed model fit by maximum likelihood ['glmerMod']
Family: poisson ( log )
Formula: frequency ~ 1 + insgen + ageveh + make + area + (1 | ID) 
   Data: Panel </p>

<pre><code>  AIC       BIC    logLik   deviance 
</code></pre>

<p>1099.9670 1134.9262  -541.9835  1083.9670 </p>

<p>Random effects:</p>

<p>Groups Name          Variance   Std.Dev.</p>

<p>ID  (Intercept)     1.551e-11     3.939e-06</p>

<p>Number of obs: 584, groups: ID, 584</p>

<p>Fixed effects:</p>

<pre><code>          Estimate    Std. Error   z value   Pr(&gt;|z|)  
</code></pre>

<p>(Intercept)    -22.98292   8432.07738    -0.003     0.9978  </p>

<p>insgenM          0.02616     0.08806      0.297     0.7664  </p>

<p>ageveho          0.05889     0.08586      0.686     0.4928 </p>

<p>make             -0.10447    0.04126     -2.532    0.0113 *</p>

<p>area1             23.68571  8432.07738   0.003    0.9978  </p>

<p>area2             23.85969  8432.07738   0.003    0.9977 </p>

<p>area3             23.77374  8432.07738   0.003   0.9978  </p>

<hr>

<p>Signif. codes:  0 â€˜<strong><em>â€™ 0.001 â€˜</strong>â€™ 0.01 â€˜</em>â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1</p>

<p>Correlation of Fixed Effects:</p>

<pre><code>    (Intr)  insgnM  ageveh  make    area1   area2 
</code></pre>

<p>insgenM   0.000  </p>

<p>ageveho   0.000  -0.037   </p>

<p>make      0.000   0.071 0.108 </p>

<p>area1    -1.000   0.000 0.000  0.000   </p>

<p>area2    -1.000  0.000 0.000  0.000  1.000</p>

<p>area3   -1.000  0.000  0.000  0.000  1.000  1.000       </p>

<p>I have included predictors:gender (Male,female), area which has 4 levels (area 1,2,3,4) and vehicle age (New and old) and make of car.</p>
"
"0.047836487323494","0.0466760028009337"," 87925","<p>I am doing LMM in lme4, and have installed languageR to run pvas.fnc(), but it does not work with random intercept and slope. </p>

<p>This is the message:</p>

<pre><code>Error in pvals.fnc(lmer1) : 
  MCMC sampling is not yet implemented in lme4_0.999375
  for models with random correlation parameters
</code></pre>

<p>For model with only random intercept, this function works fine. Then what should I do to get the p-values?</p>
"
"NaN","NaN"," 92307","<p>I have a dataset with 1206 deputies from two different chambers (1998 and 2002, respectively). In addition, there are 18 parties, and some deputies are in both chambers (the ones who were reelected). I am interested in the relation between party discipline (in a scale from 0 to 100) and the probability of geting appointed as party leader (0 or 1), but I am also interested in separate estimates for each party.</p>

<p>A classmate told me that the following mixed effect model would be appropriate, but I don't know what to do with the correlation within subjects: </p>

<pre><code>model &lt;- glmer(leader ~ discipline + (1 + discipline|chamber:party), 
               data, family=binomial)
</code></pre>

<p>What is the structure of these data and which model would be appropriate in this case (using R)? </p>
"
"0.215774661286819","0.219694014552436"," 93601","<p>I am a complete novice and dummy when it comes to statistics so I apologise in advance...</p>

<p>I have been asked to report the results of my GLMMs (I ran two) in a table. This table must state: effect, standard error, test statistic, and P value, for all fixed effects. </p>

<p>Unfortunately I am struggling to read my output. </p>

<p>The out put is as follows, if anyone would be kind enough to help I would be very grateful and will know for future reference which bit equates to what (also I have been told my degrees of freedom are different for both the tests, could someone explain why this is?).</p>

<pre><code>GLMM 1-run for predictors of step length. 
Response variable = step length. 
fixed effects = depth and direction threshold. 
random factor = individual

Models:
m2: step ~ (1 | ind)
m1: step ~ Depth * threshold + (1 | ind)
Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) 
m2 3 373235 373259 -186615 373229 
m1 8 373225 373290 -186605 373209 19.767 5 0.001382 **
---
Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>.</p>

<pre><code>GLMM 2 -run to investigate potential predictors of PDBA.
response variables = depth and step length. 
fixed effect = direction threshold.
random factor = Individual

Models:
m3: PDBA ~ Depth + (1 | ind) + thresholdepth
m2: PDBA ~ step * threshold + Depth * threshold + (1 | ind)
Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) 
m3 6 -48205 -48157 24109 -48217 
m2 11 -48430 -48341 24226 -48452 235.1 5 &lt; 2.2e-16 ***
---
Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Models:
m4: PDBA ~ step + (1 | ind) + step:threshold
m2: PDBA ~ step * threshold + Depth * threshold + (1 | ind)
Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) 
m4 6 -48206 -48158 24109 -48218 
m2 11 -48430 -48341 24226 -48452 233.81 5 &lt; 2.2e-16 ***
---
Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Hi, I think the package I used was was lme4? </p>

<p>I have run a summary for the first GLMM and this is what I got, I have no idea which parts are relevant though, I assume it doesn't all go in a table?! </p>

<pre><code>&gt; summary(m1)
Linear mixed model fit by REML ['lmerMod']
Formula: step ~ Depth * threshold + (1 | ind) 

REML criterion at convergence: 373184 

Random effects:
 Groups   Name        Variance Std.Dev.
 ind      (Intercept) 196519   443.3   
 Residual             469370   685.1   
Number of obs: 23473, groups: ind, 11

Fixed effects:
                  Estimate Std. Error t value
(Intercept)      160.95895  134.80279   1.194
Depth              0.06438    0.44777   0.144
threshold2        51.18065   17.62222   2.904
threshold3         1.47733   21.43879   0.069
Depth:threshold2  -1.23654    0.60029  -2.060
Depth:threshold3  -0.09587    0.65088  -0.147

Correlation of Fixed Effects:
            (Intr) Depth  thrsh2 thrsh3 Dpth:2
Depth       -0.094                            
threshold2  -0.090  0.712                     
threshold3  -0.075  0.588  0.567              
Dpth:thrsh2  0.071 -0.737 -0.745 -0.435       
Dpth:thrsh3  0.064 -0.674 -0.490 -0.857  0.502
</code></pre>

<p>For GLMM 1 I ran this code -</p>

<pre><code>m1&lt;-lmer(step~Depth*threshold+(1|ind))
m2&lt;-lmer(step~(1|ind))
anova(m1,m2)
</code></pre>

<p>For GLMM 2 I ran this code -</p>

<pre><code>m2&lt;-lmer(PDBA~step*threshold+Depth*threshold+(1|ind))
m3&lt;-update(m2,~.-step*threshold)
anova(m2,m3)
</code></pre>

<p>and this one:</p>

<pre><code>m2&lt;-lmer(PDBA~step*threshold+Depth*threshold+(1|ind))
m4&lt;-update(m2,~.-Depth*threshold)
anova(m2,m4)
</code></pre>

<p>The output from the Anova only gives me one p value for each GLMM and I think I need a p value for each of the fixed effects within the models?</p>

<p>Does anyone know what code I can run to get this?
Thank you</p>
"
"0.145941998977873","0.14240153351978"," 93892","<p>I need to get p values for the fixed effects in the following GLMM's I ran. Does anyone know of code that I can run that will give me the p values I need? At the moment the output from the ANOVA only gives me one p value and I believe I need a separate p value for each of the fixed effects in the models. </p>

<p>Thanks in advance.
Code is as follows -</p>

<p>For GLMM 1 I ran this code -</p>

<pre><code>m1&lt;-lmer(step~Depth*threshold+(1|ind))
m2&lt;-lmer(step~(1|ind))
anova(m1,m2)
</code></pre>

<p>For GLMM 2 I ran this code -</p>

<pre><code>m2&lt;-lmer(PDBA~step*threshold+Depth*threshold+(1|ind))
m3&lt;-update(m2,~.-step*threshold)
anova(m2,m3)
</code></pre>

<p>and this one:</p>

<pre><code>m2&lt;-lmer(PDBA~step*threshold+Depth*threshold+(1|ind))
m4&lt;-update(m2,~.-Depth*threshold)
anova(m2,m4)
</code></pre>

<p>When I ran GLMM 1 code this is what I got:</p>

<pre><code>m2: step ~ (1 | ind)
m1: step ~ Depth * threshold + (1 | ind)
Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)   
m2  3 373235 373259 -186615   373229                            
m1  8 373225 373290 -186605   373209 19.767      5   0.001382 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>summary</p>

<pre><code>&gt; summary(m1)
Linear mixed model fit by REML ['lmerMod']
Formula: step ~ Depth * threshold + (1 | ind) 

REML criterion at convergence: 373184 

Random effects:
 Groups   Name        Variance Std.Dev.
 ind      (Intercept) 196519   443.3   
 Residual             469370   685.1   
Number of obs: 23473, groups: ind, 11

Fixed effects:
                 Estimate Std. Error t value
  (Intercept)      160.95895  134.80279   1.194
Depth              0.06438    0.44777   0.144
threshold2        51.18065   17.62222   2.904
threshold3         1.47733   21.43879   0.069
Depth:threshold2  -1.23654    0.60029  -2.060
Depth:threshold3  -0.09587    0.65088  -0.147

Correlation of Fixed Effects:
            (Intr) Depth  thrsh2 thrsh3 Dpth:2
Depth       -0.094                            
threshold2  -0.090  0.712                     
threshold3  -0.075  0.588  0.567              
Dpth:thrsh2  0.071 -0.737 -0.745 -0.435       
Dpth:thrsh3  0.064 -0.674 -0.490 -0.857  0.502
</code></pre>

<p>OUTPUT FROM SUGGESTED CODE BY SETH (IN COMMENTS)</p>

<pre><code>Models:
m6: step ~ Depth + threshold + (1 | ind)
m5: step ~ Depth + threshold + Depth:threshold + (1 | ind)
   Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)  
m6  6 373227 373275 -186607   373215                           
m5  8 373225 373290 -186605   373209 5.2901      2      0.071 .
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.197234889993286","0.192450089729875"," 94888","<p>I'm analysing some behavioural data using <code>lme4</code> in <code>R</code>, mostly following <a href=""http://www.bodowinter.com/tutorials.html"" rel=""nofollow"">Bodo Winter's excellent tutorials</a>, but I don't understand if I'm handling interactions properly. Worse, no-one else involved in this research uses mixed models, so I'm a bit adrift when it comes to making sure things are right.</p>

<p>Rather than just post a cry for help, I thought I should make my best effort at interpreting the problem, and then beg your collective corrections. A few other asides are:</p>

<ul>
<li>While writing, I've found <a href=""http://stackoverflow.com/questions/17794729/test-for-significance-of-interaction-in-linear-mixed-models-in-nlme-in-r"">this question</a>, showing that <code>nlme</code> more directly give p values for interaction terms, but I think it's still valid to ask with relation to <code>lme4</code>.</li>
<li><code>Livius'</code> answer to <a href=""http://stats.stackexchange.com/questions/63464/is-this-an-acceptable-way-to-analyse-mixed-effect-models-with-lme4-in-r"">this question</a> provided links to a lot of additional reading, which I'll be trying to get through in the next few days, so I'll comment with any progress that brings.</li>
</ul>

<hr>

<p>In my data, I have a dependent variable <code>dv</code>, a <code>condition</code> manipulation (0 = control, 1 = experimental condition, which should result in a higher <code>dv</code>), and also a prerequisite, labelled <code>appropriate</code>: trials coded <code>1</code> for this should show the effect, but trials coded <code>0</code> might not, because a crucial factor is missing.</p>

<p>I have also included two random intercepts, for <code>subject</code>, and for <code>target</code>, reflecting correlated <code>dv</code> values within each subject, and within each of the 14 problems solved (each participant solved both a control and an experimental version of each problem).</p>

<pre><code>library(lme4)
data = read.csv(""data.csv"")

null_model        = lmer(dv ~ (1 | subject) + (1 | target), data = data)
mainfx_model      = lmer(dv ~ condition + appropriate + (1 | subject) + (1 | target),
                         data = data)
interaction_model = lmer(dv ~ condition + appropriate + condition*appropriate +
                              (1 | subject) + (1 | target), data = data)
summary(interaction_model)
</code></pre>

<p>Output:</p>

<pre><code>## Linear mixed model fit by REML ['lmerMod']
## ...excluded for brevity....
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  subject  (Intercept) 0.006594 0.0812  
##  target   (Intercept) 0.000557 0.0236  
##  Residual             0.210172 0.4584  
## Number of obs: 690, groups: subject, 38; target, 14
## 
## Fixed effects:
##                                Estimate Std. Error t value
## (Intercept)                    0.2518     0.0501    5.03
## conditioncontrol               0.0579     0.0588    0.98
## appropriate                   -0.0358     0.0595   -0.60
## conditioncontrol:appropriate  -0.1553     0.0740   -2.10
## 
## Correlation of Fixed Effects:
## ...excluded for brevity.
</code></pre>

<p>ANOVA then shows <code>interaction_model</code> to be a significantly better fit than <code>mainfx_model</code>, from which I conclude that there's a significant interaction present (p = .035).</p>

<pre><code>anova(mainfx_model, interaction_model)
</code></pre>

<p>Output:</p>

<pre><code>## ...excluded for brevity....
##                   Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq)  
## mainfx_model       6 913 940   -450      901                          
## interaction_model  7 910 942   -448      896  4.44      1      0.035 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</code></pre>

<p>From there, I isolate a subset of the data for which the <code>appropriate</code> requirement is met (i.e., <code>appropriate = 1</code>), and for it fit a null model, and a model including <code>condition</code> as an effect, compare the two models using ANOVA again, and lo, find that <code>condition</code> is a significant predictor.</p>

<pre><code>good_data = data[data$appropriate == 1, ]
good_null_model   = lmer(dv ~ (1 | subject) + (1 | target), data = good_data)
good_mainfx_model = lmer(dv ~ condition + (1 | subject) + (1 | target), data = good_data)

anova(good_null_model, good_mainfx_model)
</code></pre>

<p>Output:</p>

<pre><code>## Data: good_data
## models:
## good_null_model: dv ~ (1 | subject) + (1 | target)
## good_mainfx_model: dv ~ condition + (1 | subject) + (1 | target)
##                   Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq)  
## good_null_model    4 491 507   -241      483                          
## good_mainfx_model  5 487 507   -238      477  5.55      1      0.018 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</code></pre>
"
"0.143509461970482","0.140028008402801"," 95105","<p>I've been putting a lot of work over the last few days into bring mixed effects models to bear on some behavioural data I've collected for my thesis, but it's occurred to me that I'm not 100% sure that this kind of model is actually appropriate for my data (I only came across them after starting the experiment).</p>

<p>In an experiment, <code>60 participants</code> completed <code>28 trials</code> of a reasoning task, consisting of <code>14 problems</code> (call them ""A""-""N""), with participants completing each in <code>2 conditions</code>, <code>x</code> (conflict) and <code>y</code> (control).</p>

<pre><code>Participant  Problem
          1  Ax/Ay    Bx/By    Cx/Cy    Dx/Dy  ...  Nx/Ny
          2  Ax/Ay    Bx/By    Cx/Cy    Dx/Dy  ...  Nx/Ny
          3  Ax/Ay    Bx/By    Cx/Cy    Dx/Dy  ...  Nx/Ny
          4  Ax/Ay    Bx/By    Cx/Cy    Dx/Dy  ...  Nx/Ny
          5  Ax/Ay    Bx/By    Cx/Cy    Dx/Dy  ...  Nx/Ny
        ...  ...      ...      ...      ...    ...  ...
         60  Ax/Ay    Bx/By    Cx/Cy    Dx/Dy  ...  Nx/Ny
</code></pre>

<p>I'm interested in the difference in a trial-by-trial variable (let's call it <code>reaction time</code>) between the conflict and control conditions, and would expect it to be higher for the conflict (<code>y</code>) trials.</p>

<p>Obviously, I would expect to find a within-subject correlation - some subjects are generally fast, some generally slow. I would also expect to find a within-problem correlation - some problems are answered faster than others, regardless of condition. To account for these, I include random intercepts for these two factors: </p>

<p><code>(1|participant) + (1|problem)</code>.</p>

<p>The difference between conflict and control conditions may or may not turn out to be the same for each subject, and for each problem. For this reason, I consider including random slopes as well:</p>

<p><code>(1 + condition|participant) + (1 + condition|problem)</code>.</p>

<p>Putting this together, I'm testing a model that looks either like:</p>

<pre><code>null_model = lmer(reaction_time ~ condition(1|participant) + (1|problem), data=data)
condition_model = lmer(reaction_time ~ condition + (1|participant) + (1|problem), data=data)
</code></pre>

<p>or</p>

<pre><code>null_model = lmer(reaction_time ~ (1 + condition|participant) + (1 + condition|problem), data=data)
condition_model = lmer(reaction_time ~ condition(1|participant) + (1|problem), data=data)
</code></pre>

<p>.</p>

<p><strong>Please; have I horribly misunderstood how this is supposed to work?</strong></p>

<p><strong>Edit:</strong> The more traditional approach to analysing this data would be to average across problems within each participant, yielding two data points per participant: <code>conflict condition mean</code> and <code>control condition mean</code>, and then use a paired-samples t test. Reading <a href=""http://stats.stackexchange.com/questions/23276/paired-t-test-as-a-special-case-of-linear-mixed-effect-modeling-still-unresolve"">this question</a>, I thought that this approach should be largely the same as fitting </p>

<p><code>lmer(reaction_time ~ condition + (1|participant), data=data)</code>, </p>

<p>but trying this in R, it seems otherwise.</p>

<p><strong>Edit #2:</strong> Bounty added.</p>
"
"0.095672974646988","0.0933520056018673"," 97834","<p>I ran a mixed model using lme4::glmer for a logistic regression and consistently got these warning messages. I noticed there are still regular results even so, but are they accurate estimates?</p>

<pre><code>    &gt; glmm.ms1&lt;-glmer(as.formula(paste(paste(y[1], x, sep=""~""), mix[1], sep=""+"")),
    +             data=rtf2,control=glmerControl(optimizer=""bobyqa"",
    +             optCtrl=list(maxfun=100000),family=binomial)
Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.8766 (tol = 0.001)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge: degenerate  Hessian with 1 negative eigenvalues

&gt; coef(summary(glmm.ms1))
                       Estimate Std. Error    z value  Pr(&gt;|z|)
(Intercept)           1.810e+00  6.558e-01   2.760464 5.772e-03
lepidays             -3.340e+00  2.770e-01 -12.059620 1.726e-33
cldaysbirth          -1.555e+00  5.224e-01  -2.975934 2.921e-03
rotaarm              -2.057e-01  3.209e-01  -0.641102 5.215e-01
cldaysbirth2         -3.072e-01  2.955e-01  -1.039510 2.986e-01
bfh2                 -1.043e+01  1.160e+03  -0.008996 9.928e-01
bfh3                  4.653e-01  4.806e-01   0.968103 3.330e-01
bfh4                  2.547e-01  4.994e-01   0.509966 6.101e-01
bfh5                  3.744e-01  9.926e-01   0.377213 7.060e-01
ruuska               -1.020e-01  5.928e-02  -1.720396 8.536e-02
genderMale           -4.008e-01  2.645e-01  -1.515453 1.297e-01
epiexlbf              6.078e-04  2.796e-03   0.217391 8.279e-01
haz.epi              -7.211e-02  1.373e-01  -0.525039 5.996e-01
cldaysbirth:rotaarm   6.928e-01  4.771e-01   1.452148 1.465e-01
rotaarm:cldaysbirth2  5.181e-01  3.352e-01   1.545527 1.222e-01
Warning messages:
1: In vcov.merMod(object, use.hessian = use.hessian) :
  variance-covariance matrix computed from finite-difference Hessian is
not positive definite: falling back to var-cov estimated from RX
2: In vcov.merMod(object, correlation = correlation, sigm = sig) :
  variance-covariance matrix computed from finite-difference Hessian is
not positive definite: falling back to var-cov estimated from RX
</code></pre>

<p>Due to data's sensitivity, I can't post the whole process for generating same messages, but I would like to know how to handle these warnings. I don't think it's suitable to keep my eye blind here.</p>
"
"0.135302018298348","0.13201967239689","100000","<p>I am trying to simulate multi-level data for repeated measurements. My design includes just one within subjects factor, no between-subject factor. Consider the case of three treatment conditions with 10 trials in each. This is what I've written so far (using R):</p>

<pre><code>##### repeated measures analysed using lme



rm(list = ls())
set.seed(374)

library(lme4)
library(reshape2)

trials_per_subject &lt;- 30
number_of_subjects &lt;- 30
treatment_conditions &lt;- 3

Ntotal &lt;- number_of_subjects*trials_per_subject

test.df &lt;- data.frame(
                  subject = sort(rep(c(1:number_of_subjects),trials_per_subject)), 
                  trial = rep(c(1:trials_per_subject),number_of_subjects) , 
                  x1 = rep(rep(c(1,0),c(10,20)),number_of_subjects),
                  x2 = rep(rep(c(0,1,0),each=10),number_of_subjects))

# random slopes
beta1 &lt;- rnorm(number_of_subjects,40,10)
beta2 &lt;- rnorm(number_of_subjects,40,10)
# random intercept (subject effect)

subject_effect  &lt;- rnorm(number_of_subjects,400,50)
#trial specific errors
errors  &lt;- rnorm(Ntotal,0,5)




test.df$beta1 &lt;- beta1[test.df$subject]
test.df$beta2 &lt;- beta2[test.df$subject]
test.df$int &lt;- subject_effect[test.df$subject]

# factor variable for further computing in lmer()  
test.df$treatment &lt;- rep(c(1,2,3),each=10,length=Ntotal)
    test.df$treatment &lt;- as.factor(test.df$treatment)

# generate response times
test.df$y &lt;- test.df$int + test.df$x1 * test.df$beta1 + test.df$x2 *  test.df$beta2 + errors




# get correlation matrix
Df &lt;- dcast(test.df,subject~treatment,value.var=""y"",fun.aggregate=mean)[,-1]
cor(Df)

# relevel so that treatment 3 becomes the reference
test.df &lt;- within(test.df, treatment &lt;- relevel(treatment, ref = 3))

# fit model (random intercept + slope model)
re.lm &lt;- lmer(y ~ treatment + (1+treatment|subject), data = test.df) 
summary(re.lm)
</code></pre>

<p>Because for repeated measurements, the correlation structure is important (sphericity), it would be necessary to be able to (indirectly) control for the assumption of sphericity/compound symmetry. To be more concise, I want manipulate whether the variances and covariances are equal (assumption met) or different (assumption violated), see dataset DF above.</p>
"
"0.158655679740622","0.154806788004275","100494","<p>I have a sort of weird and complicated model design, and I'd like to get your opinion on how best to model the error structure. </p>

<p>I have 100 sites, with each site falling into 1 of 4 different forest type categories (so 25 of each forest type). Within each site, I have 4 plots, each one with a different manipulative treatment. The outcome is number of new seedlings within each plot. I'm interested in how forest type, treatment, and their interaction affect seedling growth.</p>

<p>So, the most basic model I can think of is (using nlme):</p>

<p>lme(seedling ~ forest.type + treatment + forest.type*treatment, random=~1|site)</p>

<p>This sort of seems to be right, since each site probably has some random effect on the number of seedlings. But it's also doesn't seem to be totally correct, since treatment is nested within site, and the random intercept might differ across treatments. So another model I've thought of is:</p>

<p>lme(seedling ~ forest.type + treatment + forest.type*treatment, random=~1+treatment|site)</p>

<p>And, while model fits (though lmer refuses to even try), it also doesn't quite seem right, since I don't have any replication of treatment within site. </p>

<p>I know the geographic locations of all of the plots, so I've also tried some models that just use a spatially correlated error structure in place of a random effect, but I have no way to know if treatment affects the correlations, so I don't feel totally comfortable with this approach.</p>

<p>Do either of these above models seems appropriate? Or is there a different model, or a different approach that you'd suggest?</p>

<p>Thanks.</p>
"
"0.108482956331022","0.105851224804993","103104","<p>For a simulation study, I contrast the power of different LMEMs for repeated measures. To get p-values, I use likelihood ratio tests where I compare a model including a fixed treatment effect with one that has the same random effects structure but without having the fixed treatment effect. I want to specify a model in which random intercept and slope are allowed to correlate and one in which it is not allowed. But, when I extract the p-values of both models, it appears that they are exactly the same. Why? I am not an expert for mixed models but that seems weird. Here's my code using lmer syntax:</p>

<pre><code># correlation allowed
  ml3 &lt;- lmer(rt ~ treatment + (1+treatment|subject),data=df)
  ml0 &lt;- lmer(rt ~ 1 + (1+treatment|subject),data=df)
  lrt &lt;- anova(ml3,ml0)
  pVal &lt;- lrt$""Pr(&gt;Chisq)""[2]

# no correlation
  ml2 &lt;- lmer(rt ~ treatment + (1|subject) + (0+treatment|subject) ,data=df)
  ml0 &lt;- lmer(rt ~ 1 + (1|subject) + (0 + treatment|subject),data=df)
  lrt &lt;- anova(ml2,ml0)
  pVal &lt;- lrt$""Pr(&gt;Chisq)""[2]
</code></pre>

<p>Data:</p>

<pre><code>dput(DF)
structure(list(subject = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
4L, 4L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 
7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 7L, 8L, 8L, 8L, 8L, 
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 
8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 9L, 
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 
9L, 9L, 9L, 9L, 9L, 9L, 9L, 9L, 10L, 10L, 10L, 10L, 10L, 10L, 
10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 
10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 10L, 11L, 11L, 
11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 
11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 11L, 
11L, 11L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 
12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 12L, 
12L, 12L, 12L, 12L, 12L, 12L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 
13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 
13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 13L, 14L, 14L, 14L, 
14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 
14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 14L, 
14L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 
15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 15L, 
15L, 15L, 15L, 15L, 15L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 
16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 
16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 16L, 17L, 17L, 17L, 17L, 
17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 
17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 17L, 
18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 
18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 18L, 
18L, 18L, 18L, 18L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 
19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 
19L, 19L, 19L, 19L, 19L, 19L, 19L, 19L, 20L, 20L, 20L, 20L, 20L, 
20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 
20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 20L, 21L, 
21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 
21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 21L, 
21L, 21L, 21L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 
22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 22L, 
22L, 22L, 22L, 22L, 22L, 22L, 22L, 23L, 23L, 23L, 23L, 23L, 23L, 
23L, 23L, 23L, 23L, 23L, 23L, 23L, 23L, 23L, 23L, 23L, 23L, 23L, 
23L, 23L, 23L, 23L, 23L, 23L, 23L, 23L, 23L, 23L, 23L, 24L, 24L, 
24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 
24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 24L, 
24L, 24L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 
25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 
25L, 25L, 25L, 25L, 25L, 25L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 
26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 
26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 26L, 27L, 27L, 27L, 
27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 
27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 27L, 
27L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 
28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 28L, 
28L, 28L, 28L, 28L, 28L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 
29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 
29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 29L, 30L, 30L, 30L, 30L, 
30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 
30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L, 30L
), .Label = c(""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", 
""11"", ""12"", ""13"", ""14"", ""15"", ""16"", ""17"", ""18"", ""19"", ""20"", ""21"", 
""22"", ""23"", ""24"", ""25"", ""26"", ""27"", ""28"", ""29"", ""30""), class = ""factor""), 
    treatment = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
    1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 
    3L, 3L, 3L, 3L, 3L, 3L), .Label = c(""1"", ""2"", ""3""), class = ""factor""), 
    rt = c(551.798792586772, 693.014255128461, 715.599061613616, 
    670.119831344829, 777.748610260388, 736.018489208224, 636.791011800404, 
    864.593711496912, 604.529352905588, 596.673178487122, 860.858066937491, 
    717.975814131377, 531.672833100059, 571.150454430927, 644.315598150879, 
    601.914697283216, 583.92746647402, 702.714068138085, 660.346853172676, 
    541.292786332608, 608.233103066463, 740.593415976325, 686.059921551164, 
    706.522723402261, 567.648255604935, 596.111352599386, 625.779084220279, 
    752.776987343973, 922.314285125596, 720.736074757203, 768.585671134519, 
    539.657760625667, 431.193030969184, 739.341430343149, 581.505474510558, 
    485.905153431116, 524.085545405872, 876.566370460358, 631.259754679943, 
    587.887351105621, 624.365050240473, 642.528438460209, 440.661792577731, 
    517.142782023978, 705.840003729944, 557.122142924839, 645.711579229236, 
    477.292943229673, 578.522058679457, 623.879658296107, 480.855063147831, 
    622.295733392922, 611.623490658329, 594.974733982977, 546.239019853272, 
    551.638287622872, 567.791819285002, 539.239628365136, 541.328446070423, 
    609.931976806498, 549.492601324081, 479.862984098331, 592.411150981731, 
    466.224011597179, 489.388878789762, 565.187127159354, 806.196199699478, 
    565.001071713299, 449.529036961143, 446.824243314547, 357.993777663337, 
    370.054045045062, 546.443479822161, 473.894296409884, 335.821704077378, 
    370.498649523398, 486.052525038318, 436.53292033153, 359.637460079864, 
    333.146018287273, 597.894114487158, 551.993792800734, 518.563432886515, 
    513.629383189428, 572.062676720248, 366.611317255576, 353.934207291842, 
    492.273303938824, 414.632984933654, 456.987565377718, 585.524844348671, 
    453.577328112778, 665.072248688078, 459.204631254183, 452.028605442515, 
    422.731506299078, 522.84363619892, 771.586286956136, 478.422495080758, 
    530.925498291748, 457.030618882822, 446.313635696342, 537.708665959068, 
    815.929601138346, 460.420404065423, 603.027278932425, 538.526470664698, 
    571.491835856551, 567.234631106499, 450.878624452358, 650.340961680322, 
    598.829083718722, 620.85411026516, 573.498196791879, 519.953442801483, 
    1143.14393274202, 505.028670926264, 685.875665196364, 605.316954852204, 
    645.269429082978, 678.192056130499, 605.671978651269, 650.564580984954, 
    641.331733928499, 687.164180542278, 613.873849203194, 789.829709495785, 
    560.793473918547, 707.169378961089, 680.753196215641, 659.262985906231, 
    600.712008959484, 662.275074291484, 661.346206480403, 568.31000899618, 
    661.439508442242, 761.227769640367, 699.901658463283, 631.422448673388, 
    734.257735977184, 585.776345181453, 714.587957176744, 893.931699334816, 
    586.343993838929, 664.205207596859, 828.003782888565, 906.448165648461, 
    584.196768113385, 747.575564348236, 687.698668648395, 398.221092516595, 
    490.332613905338, 502.683377386602, 451.168200674477, 620.606534311108, 
    369.820458042713, 429.483129392912, 628.153937257066, 476.31856841443, 
    608.016880363378, 402.588700424079, 460.302916138576, 341.209753425326, 
    600.165531243784, 454.003777748405, 589.089266888531, 504.033320854066, 
    399.871492203846, 421.426563579218, 423.375093277487, 587.614013919312, 
    689.18637317583, 652.10069672704, 553.995320740249, 570.86533170596, 
    636.399559100471, 801.517490092303, 653.425223465164, 684.914139340214, 
    639.418654954543, 555.718100869331, 677.062768399616, 579.433200999322, 
    561.757369869387, 672.316124102021, 701.108131071079, 588.129426947175, 
    438.090900053591, 562.520435558598, 610.177372103278, 564.672192806652, 
    552.305226838045, 586.912866128373, 872.521433158083, 654.971253063189, 
    575.068762782096, 784.098108527601, 648.265348029739, 590.541840435637, 
    552.569131260877, 554.840084955354, 582.798864712891, 573.196470707737, 
    512.123960183202, 579.838037093289, 710.216216611067, 779.786949219207, 
    615.995650564573, 549.096392807351, 600.781394864656, 415.016144351654, 
    765.924387691343, 401.541419432177, 436.050367769487, 536.508634405116, 
    445.112952169149, 478.003493101049, 509.819044087032, 490.265270275681, 
    594.667876389766, 781.844411855516, 827.832351086729, 379.401116898897, 
    469.280230588986, 397.115839743604, 874.524377877882, 612.130504039819, 
    802.270319490186, 651.842161968928, 581.489774054855, 686.457677143518, 
    570.172663082147, 566.996565453736, 577.947675356248, 494.016772046721, 
    546.065861910691, 506.178677541412, 527.653822550596, 470.043764013502, 
    595.080116592997, 464.590366280242, 684.362069491853, 534.310814471562, 
    545.7046301149, 452.141529834992, 619.652055160652, 568.61376011316, 
    576.847350527713, 514.248803061826, 585.909312171032, 687.892034205561, 
    907.133281713537, 549.603068658537, 617.860688444804, 423.424246676122, 
    524.28348263976, 593.203848577403, 431.733188413523, 476.284708033659, 
    588.88583865225, 437.275988819986, 733.45270912684, 592.366412341047, 
    606.958434204909, 721.61902078205, 604.596941234802, 534.65440311647, 
    526.29928462654, 655.076689084035, 560.740728878595, 591.083376633783, 
    533.331301213643, 750.32841350028, 547.366173885661, 602.313382446308, 
    787.158938523176, 534.80106549099, 454.37886245909, 599.535859565986, 
    607.126697674517, 668.173760533712, 589.060272311024, 590.188448587092, 
    711.910534337354, 528.634489779135, 600.468858484032, 580.666817624455, 
    659.907090614686, 596.395917159692, 994.163737779338, 662.059444540888, 
    637.256716085147, 714.353436812361, 587.212427691626, 676.527668439672, 
    609.004414569998, 667.364031145608, 788.145350832559, 725.891539439069, 
    561.397498270981, 430.027498616446, 500.437956195847, 463.801763917305, 
    458.358780003907, 383.304386810731, 598.957692241571, 409.89510543858, 
    390.650415086637, 552.072907469115, 388.554580583084, 671.244783776164, 
    433.841093781351, 423.187562794827, 502.566122911232, 469.869008810394, 
    547.610270179268, 501.091740213331, 417.336826115574, 500.284514580019, 
    460.835882303962, 650.071068396249, 523.313503950421, 861.366681123829, 
    879.241985731583, 673.655630620254, 448.199583227711, 578.587129494665, 
    654.742597624172, 623.62363768736, 665.526175470944, 942.738238156293, 
    1006.15443845549, 667.679153732234, 711.686114156855, 642.929069350516, 
    685.862290196822, 1062.63097632175, 758.162511396556, 827.547233897549, 
    668.688764986398, 791.497544557741, 838.143090686178, 681.935257212825, 
    758.732997665222, 661.724656922782, 793.560801116029, 896.416624866383, 
    642.617709357462, 633.832129070135, 751.515360586321, 616.151652306802, 
    684.496510560379, 655.310039878885, 710.298048482024, 606.373767619465, 
    754.268924528687, 822.582103710613, 820.556840434073, 733.785411148237, 
    584.830824784288, 588.316573524589, 572.95505735157, 559.402915982595, 
    640.891735376065, 482.407652048448, 569.682285396545, 517.277707765673, 
    698.102946480301, 651.001615070688, 665.691471843539, 511.440973330271, 
    504.930464361447, 613.891964397534, 454.7073692139, 513.19138352863, 
    422.708112768038, 347.049510934991, 523.980248957572, 480.301125161823, 
    633.307276361827, 799.987010744151, 533.354042715484, 410.150445477125, 
    809.568249688128, 531.41976915349, 792.355614308461, 747.208014043674, 
    607.571115317023, 546.485408007754, 633.55875460818, 767.73368427773, 
    676.492693414302, 751.649529779836, 984.189814104173, 684.929427919003, 
    615.787024482925, 567.942282464503, 571.041675151281, 614.028930539252, 
    779.839834490734, 630.179209124113, 651.603035032816, 788.591687415382, 
    799.38918489533, 701.842888543902, 693.932887722425, 624.800556024233, 
    659.981040765196, 542.243217216484, 721.703181143723, 607.818766172507, 
    586.813797432694, 610.206652108693, 837.694469363876, 763.535995041537, 
    758.89830766469, 616.838182390385, 578.107924042397, 628.314074464124, 
    676.917384461922, 635.824489980127, 495.143374853889, 815.582155744321, 
    534.740299502999, 568.853739307473, 771.28519095763, 673.064072347686, 
    713.558399193608, 599.316767121742, 689.796480377465, 673.175516507747, 
    518.390229271871, 784.139459988779, 536.808895738866, 591.342581848355, 
    563.762291009613, 679.413099342014, 490.762928348403, 575.612328735691, 
    504.631070884374, 689.919729220693, 545.809277581445, 641.095314483731, 
    618.332256267043, 641.913937397485, 953.129874375365, 646.628853366556, 
    631.881991258933, 671.647395089865, 503.290799393907, 506.506064370266, 
    529.718096437596, 484.255438291713, 861.643688089666, 625.018895601203, 
    768.279996867868, 596.708155034627, 671.714642028838, 844.072568247028, 
    578.694918479722, 422.588349061727, 594.493346157147, 520.812331184257, 
    741.876339265066, 555.516494731537, 579.386393427601, 622.316950052304, 
    523.488853303438, 595.901305255518, 553.512680895547, 557.643582245011, 
    624.788623102115, 479.715363427417, 574.354660431126, 524.472350214463, 
    660.841590121958, 608.621321258764, 631.743182107793, 711.470012104646, 
    617.432792370567, 573.363544694191, 617.449976333406, 563.127159530709, 
    583.25391667852, 678.936105477067, 572.153554376884, 609.829503412847, 
    606.069768210344, 693.198276061625, 647.952198803514, 508.091779167254, 
    654.226813385831, 530.92016927824, 504.963210966908, 512.488303835862, 
    763.325818301401, 664.628862733417, 477.385861339593, 566.148674353306, 
    578.70068655976, 606.967024346421, 697.460752784057, 662.304772796768, 
    520.905460930742, 629.14344808993, 815.023764792173, 680.359748369552, 
    628.317980877129, 679.505810999772, 512.999611466799, 656.728389486035, 
    548.409794219861, 619.925877003775, 581.949057396067, 663.545400676099, 
    666.518874913722, 691.701483159255, 616.896649470106, 595.504424960074, 
    574.172251324537, 552.787259430621, 678.018976276998, 607.329759814185, 
    611.581207725935, 690.981992177989, 564.832150097104, 540.336710300887, 
    533.631062681699, 553.612294126468, 479.592612257575, 805.559491265258, 
    528.304765655223, 496.66528049325, 503.82305630743, 696.245302300331, 
    566.070769246181, 659.391688013324, 697.528902380277, 524.903347139913, 
    599.821891499886, 605.43297053286, 663.035359384042, 714.444647793395, 
    578.371129029246, 615.320894052349, 586.420779403222, 611.255799029828, 
    583.666658817928, 585.113768358993, 590.122958856932, 629.219469590256, 
    538.928053998428, 686.894125956954, 600.89266348967, 577.953187882193, 
    554.413905790583, 579.409330807908, 745.040084235899, 891.087132273406, 
    552.988614856682, 820.955690300634, 634.469321378978, 673.975047013567, 
    756.507601731563, 639.35059215201, 722.557986588015, 586.447409643988, 
    656.432427481585, 682.783381677787, 644.716232116734, 572.306442663379, 
    517.147784935371, 455.957276558869, 581.819706567048, 484.576715810217, 
    481.058650198264, 769.846887958231, 614.717393882487, 512.55467514312, 
    562.454770697369, 470.842224095898, 473.821865893767, 525.60888531351, 
    504.615217687803, 633.463711580414, 697.478798243637, 542.901502870182, 
    554.078075963646, 552.734146037028, 505.049122827182, 486.831379133217, 
    784.789844765716, 625.603196289942, 670.995369035953, 685.579926259636, 
    540.482850768361, 439.117039069522, 502.605387735171, 683.149979103402, 
    516.322278257158, 642.3671240847, 552.631029671279, 647.736853458454, 
    560.328629000192, 730.565596415312, 618.355157539931, 663.62525661894, 
    762.952777957374, 628.608584740535, 624.968635247218, 661.70575556195, 
    574.702252033339, 612.175432188694, 517.854558597715, 675.314287039473, 
    542.173699486536, 627.693153783529, 692.014742774091, 739.358457751625, 
    594.351746882543, 487.81864701434, 584.028504991851, 772.039898984639, 
    467.800944704621, 699.587386648698, 711.892383008835, 615.312970618784, 
    675.467367812567, 504.86811313757, 549.182867476271, 399.814725143066, 
    558.16544073586, 422.858340371991, 636.197179849367, 452.640724786824, 
    620.185492727861, 463.138266913543, 513.777642749675, 562.005709606924, 
    536.148107655772, 609.263894686695, 569.500530985324, 431.43349765191, 
    584.398797922899, 499.315449753743, 619.406287942484, 530.179301913325, 
    581.286972325074, 609.256211854971, 603.54936265609, 631.687639186526, 
    657.738657905408, 662.587176694764, 566.971357910094, 858.130855520899, 
    636.509916564228, 961.258701590037, 654.287721552112, 722.951283166332, 
    632.132002720104, 487.318833408862, 568.846556179602, 618.040023485574, 
    496.276276900436, 575.584711170303, 581.733646148308, 429.189732200854, 
    542.010860963286, 494.264804962282, 422.488093162063, 458.026827419797, 
    453.276318995818, 469.307056127784, 569.3897375524, 579.968164508765, 
    499.796867133562, 434.308570876294, 715.325075398682, 716.351002032214, 
    628.027210297141, 500.606473063414, 481.830926575354, 613.318935569666, 
    976.743117685661, 644.275338785824, 478.107026795071, 557.000656140104, 
    705.452526296914, 485.589962586432, 499.786618070234, 558.712600821937, 
    590.922333630969, 542.410481740128, 489.828649613243, 588.761989730902, 
    546.978243344109, 606.196264556647, 564.236942812287, 536.823507783435, 
    579.521762239388, 515.499050687035, 630.900495035976, 656.332012380511, 
    587.856990348358, 522.305185738772, 739.045222536055, 730.745657623434, 
    924.844562132056, 784.35778551794, 602.551997131679, 647.756594982111, 
    698.734409940058, 836.546703053691, 698.539340777214, 1026.87197547421, 
    629.904387631378, 1256.38776880042, 550.201102905894, 602.768212743634, 
    721.187968335008, 978.957227830475, 720.414204972345, 674.653879707098, 
    788.5848329187, 719.806764303146, 717.541641935441, 692.033638361742, 
    697.745018761048, 666.284578038868, 772.479906582772, 659.065480010219, 
    820.683359002167, 676.970146466469, 874.368371442289, 727.177048657942, 
    698.30875424695, 779.956903736863, 845.624757018358, 713.683982932567, 
    726.314529163246, 619.106987740244, 602.275574812157, 1012.31009024481, 
    900.357542354599, 644.181000629658, 696.237940779481, 723.968447714559, 
    728.433707295704, 738.277840656323, 624.899240530917, 602.62262026982, 
    741.637155120584, 771.780154658976, 655.858038042129, 746.047425940232, 
    620.210165076071, 727.335465168732, 575.696484591354, 664.112900876799, 
    635.568414156208, 813.389693290661, 782.803606862186, 611.54036820155, 
    626.182943686362, 629.796912199246, 850.589128120044, 751.317837909887, 
    698.370084520991, 684.731100139964, 675.610584876072, 746.655183232173, 
    700.916031879367, 672.999065959173, 754.165715908428, 1000.39209579409, 
    711.494061908177, 641.566680754116, 804.073919378063, 684.705472972499, 
    703.074833865838, 461.242335766073, 427.275150289403, 448.904300292751, 
    504.467553858542, 468.445048327652, 492.18239230431, 441.918536235364, 
    516.654435536123, 552.509951287832, 409.431285285276, 435.346043325067, 
    547.743500224329, 531.812309405229, 364.578903523756, 508.390034089605, 
    379.665384727272, 509.53781675453, 357.585528744983, 441.32777083335, 
    443.431829371385, 606.59281583625, 686.319960511486, 625.7347791126, 
    738.463790041674, 563.274182025531, 464.717827380926, 507.784383381725, 
    588.175980498022, 413.597103546393, 718.241261054521, 748.95432032953, 
    753.3423119569, 624.488936342165, 685.583685719862, 556.272453690569, 
    553.85940984926, 547.648759204925, 600.572261526898, 798.618024234413, 
    726.941645252511, 571.72160960877, 550.034219181198, 627.264280360843, 
    678.989621824996, 643.317665763982, 562.10080966811, 612.803383559254, 
    681.139896626253, 602.318525361381, 725.533189662524, 624.989203686985, 
    757.711095633453, 672.222798550503, 814.596048431427, 661.778896020412, 
    598.861275115565, 720.798422223033, 607.767472082705, 786.918370564599, 
    609.319159908796, 654.545951651791, 590.381966203786, 580.331158503126, 
    515.973182460188, 652.661059652561, 545.988014818838, 695.948607980764, 
    541.582595061958, 589.514921567389, 543.237248780138, 629.781187086802, 
    495.114814971547, 713.705169086826, 604.33689023605, 515.485770936165, 
    519.257552917307, 699.375113218082, 538.736700962025, 475.293688282428, 
    575.221421714611, 504.071893399439, 480.754098260713, 529.183372881182, 
    600.773688635732, 528.753221287108, 623.467259878089, 524.506017554373, 
    578.950778202312, 558.484848311201, 548.077100964434, 628.137735553388, 
    536.855310919075, 671.288977759216, 656.468309447081, 554.138278777839, 
    652.139385673766, 576.644938824018, 623.229206449598, 690.260009908557, 
    908.122945156817, 512.745373098672, 512.90318083329, 732.257651656802, 
    542.85407478119, 497.700590782599, 523.882208542902, 448.976579619777, 
    563.466660067041, 505.398939062326, 668.814284148356, 512.653186957994, 
    503.040433891059, 433.619712591384, 464.197386550985, 408.88198638402, 
    681.920233753602, 504.514813136438, 570.133904166935, 491.416987899975, 
    440.029552147731)), .Names = c(""subject"", ""treatment"", ""rt""
), class = ""data.frame"", row.names = c(NA, -900L))
</code></pre>

<p>Thanks in advance</p>
"
"0.262524476613337","0.264160658535441","103777","<p>After writing this post, I've realized that I am running around in circles, chasing my tail. Any help approaching this problem would be greatly appreciated, as I think I just need to bounce ideas around and don't have colleagues that can help with statistical methods at my new position.</p>

<p>I am working on an incomplete longitudinal/repeated measure dataset, which is very new to me. After spending a week working with the data, I have come to the (possibly incorrect) conclusion that I should be approaching the data with a GLMM or GEE. I say incorrect, because I wonder if I have too many dimensions to model through these approaches (potentially leading to a PCA to reduce my dimensions?). I've been attempting to approach the data with lme4() in Program R, but am having a major issue in wrapping my head around how to account for the various variables of interest. I believe my data has a layer or two of complexity beyond the 'sleepstudy' data in Bates Chapter 3 (on the use of Mixed Effects Models in R), so would appreciate any advice on what I am missing and what direction I may pursue, as my brain has reached it's carrying capacity for selflearning at the moment</p>

<p>My simulated dataset can be found <a href=""http://txt.do/ktr0"" rel=""nofollow"">here</a>. A treatment (drug vs control) was administered to a set of subjects at TimePeriod 1. A Measurement of interest was taken at TimePeriod 0 (before treatment), and at TimePeriods 1, 2, 3, 4, 5 (spaced each 12hrs). These treatments are conducted on newborn babies born at different gestational ages (GA), but are conducted at similar (but not identical) HOL (hour of life). The dataset includes: </p>

<ul>
<li>'GA' (gestational age at birth)</li>
<li>the biological 'Measurement' of interest </li>
<li>'TimePeriod' (which timeperiod the measurement was taken at. 0=predrug/control administration, 1=post, 2=12hrs after, 3=24hrs after, etc)</li>
<li>HOL (hour of life at eachmeasurement)</li>
<li>'Group' (term vs preterm birth, which is a categorization of GA that I don't think is needed)</li>
<li>'Treatment'(drug/control). </li>
</ul>

<p>There are 6 measurements per subject each taken at a similar HOL between subjects. The main question is if treatment (drug/control) affects the measurement of interest. Another question is how GA and HOL relate to the measurement of interest. </p>

<p>We can open <a href=""http://txt.do/ktr0"" rel=""nofollow"">this</a> in R via</p>

<pre><code>newData&lt;-""[paste linked-data here]""
Data &lt;- read.table(text=newData,header=TRUE)
</code></pre>

<p>We then can use ggplot2 to visualize the data, using my 4-dimensions of interest (GA, HOL, Treatment, and Measurement)</p>

<pre><code>library(ggplot2)
library(grid)
s &lt;- ggplot(Data, aes(x=HOL,y=GA, shape=Treatment,fill=Subject,color=Subject))
s &lt;- s+geom_point(aes(size=Measurement))
s + opts(legend.key.height=unit(.3, ""cm""))
</code></pre>

<p>.<img src=""http://i.stack.imgur.com/ergRC.png"" alt=""enter image description here"">
What we see with this plot is GA (gestational age) on the y-axis, and HOL (hour of life on the xaxis). The size of the points give information on the Measurement of interest, while shape gives Treatment group. Clearly, the Measurement increases with increasing GA and HOL, and each subject has a unique GA with a ~15week spread of values, while the 6 measurements per individual are at similar HOLs. It is important to note that this is based on Biological Relevance. In our real data, we expect a strong effect of increasing HOL and GA on Measurement</p>

<p>1) The first question to analyze. Does Treatment (control/drug) affect the measurement of interest vs 'TimePeriod' (which tracks time of dosing, where solid black line is the dose/treatment. X-axis has been 'jittered' for clarity). </p>

<p><img src=""http://i.stack.imgur.com/konzS.png"" alt=""enter image description here""></p>

<p>I envisioned a GLMM where TimePeriod is X, Measurement is Y (graph above), and then HOL and GA are the Random Effects. But then, how do I test for a Treatment effect between Drug/Control? I have created a plot of mean values and confidence intervals (normalized for within-subject variability via Morey 2008), which shows no effect of Treatment, but I wonder how to approach this with a GLMM (if that's appropriate), instead of just visualizing group means and CIs. I believe I understand how to make a GLMM for the Drug or Control group separately, but do not see how to compare the two through a statistical test (beyond plotting them on the same graph). I am clearly overlooking something in my brain-fog.</p>

<p>2) The first question is really my main interest. The second question is then, assuming there is no Treatment effect, can I model the relative importance of HOL and GA on the measurement of interest (again, we believe we will find no Treatment effect, but will find a HOL and GA effect)? This relationship is seen in the first graph I provide (measurement increases as you move up on both axises), but for some reason, I am having a complete mental block on approaching this question statistically.</p>

<p>EDIT (6/26/14): I've been working through this, and have come to this approach.</p>

<pre><code> lmer.full&lt;-lmer(log(Measurement)~HOL*Treatment+GA+(1|Subject))
</code></pre>

<p>This allows a treatment of longitudinal data through the (1|Subject) random effect. It also simplifies the data since 'TimePeriod' and 'HOL' are so heavily correlated. Instead of asking how the Measurement changes per time-period, I can explore the interaction between Hour-of-Life and Treatment. </p>

<p>Here is my new issue. Gestational Age is heavily correlated with the Subject effect (which makes sense since each subject has it's own Gestational Age). My questionis, if due to this high correlation, I should leave out GA, and just focus on the (1|Subject) random effect to control for this variability.</p>

<p>EDIT 2: I have improved the model on advice of Bodo Winter, who has a great personal website with some nice beginner tutorials I highly recommend www.bodo-winter.net</p>

<p>First, I zero-centered my Time parameter</p>

<pre><code> Data$HOL&lt;-Data$HOL-mean(Data$HOL,na.rm=TRUE)
</code></pre>

<p>I also removed all initial measurements before treatment administration</p>

<pre><code> Data2&lt;-subset(Data,Data[,6]&gt;0) 
</code></pre>

<p>This removed convergence issues I was having when adding a correlated random slope/intercept as such</p>

<pre><code> lmer.full.slope&lt;-lmer(log(Measurement)~HOL*Treatment+GA+(1+HOL|Subject))
</code></pre>
"
"0.234349970059352","0.228664780190012","105906","<blockquote>
  <p>The bounty I placed on this question expires in the next 24 hours.</p>
</blockquote>

<p>I have a psychological data set which, traditionally, would be analysed using a paired samples t test.
The design of the experiment is $39 (subjects) \times 7 (targets) \times 2 (conditions)$, and I'm interested in the difference in a given variable between the conditions.</p>

<p>The traditional approach has been to average across targets so that I have 2 observations per participant, and then compare these averages using a paired t test.</p>

<p>I wanted to use a mixed models approach, as has become increasingly popular in this field (i.e. <a href=""http://www.sfs.uni-tuebingen.de/~hbaayen/publications/baayenDavidsonBates.pdf"" rel=""nofollow"">Baayen, Davidson &amp; Bates, 2008</a>), and so the first model I fit, which I thought should approximate the results of the t test, was one with $condition$ as a fixed effect, and random intercepts for $subjects$ (i.e. $var = \alpha + \beta*condition + Intercept(subject) + \epsilon$. Obviously, the full model would also include random intercepts for $targets$.</p>

<p>However, I'm struggling to understand why I achieve pretty divergent results between the two approaches.
Can anyone explain what's going on here?
I've also seen (what I understand to be) a similar question asked <a href=""http://stats.stackexchange.com/questions/23276/paired-t-test-as-a-special-case-of-linear-mixed-effect-modeling-still-unresolve"">here</a>, with an answer about correlation structure which I'm not equipped to understand. If this is also what's at issue here, I would appreciate if anyone could suggest some resources to read up on this.</p>

<p><strong>Edit:</strong> I've posted <a href=""https://gist.github.com/EoinTravers/ce86c93fb42fba284464"" rel=""nofollow"">the example data, and R script, here</a>.</p>

<p><strong>Edit #2 - Bounty added</strong></p>

<p>Some additional points:</p>

<ul>
<li>I'm only analysing the correct responses (think of it as analogous to reaction time), so there are <strong>missing cases</strong> - not every participant provides 7 data points per condition.
<ul>
<li>When I analyse all responsees, rather than just the correct ones, the difference between the two results is reduced, but not eliminated. This suggests to me that the missing cases are a factor here.</li>
</ul></li>
<li>The variable isn't normally distributed. In my final model, I scale it using a Box-Cox transformation, but I omit that here for consistency with the t test.</li>
<li>As pointed out by @PeterFlom, the $df$s differ hugely between the two approaches, but I assume this to be because the t test is being applied to the aggregate data (2 observations per participant, 1 per condition), while the mixed model is applied to raw scores ($&lt;14$ observations per participant, $&lt;7$ per condition).</li>
<li>@BenBolker notes that the t values also differ pretty considerably.</li>
</ul>

<p>My analysis code is below.</p>

<pre><code>&gt;library(dplyr)
&gt;subject_means = group_by(data, subject, condition) %&gt;% summarise(var=mean(var))
&gt;t.test(var ~ condition, data=subject_means, paired=T)

    Paired t-test

data:  var by condition
t = -1.3394, df = 37, p-value = 0.1886
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.14596388  0.02978745
sample estimates:
mean of the differences 
            -0.05808822 

&gt;library(lme4)
&gt;lm.0 = lmer(var ~ (1|subject), data=data)
&gt;lm.1 = lmer(var ~ condition + (1|subject), data=data)
&gt;anova(lm.0, lm.1)

Data: data
Models:
object: var ~ (1 | subject)
..1: var ~ condition + (1 | subject)
       Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)  
object  3 489.09 501.23 -241.55   483.09                           
..1     4 485.81 502.00 -238.90   477.81 5.2859      1     0.0215 *

&gt;library(lmerTest)
&gt;summary(lm.1)$coef

              Estimate Std. Error        df  t value     Pr(&gt;|t|)
(Intercept) 0.11862462 0.02878027  98.60659 4.121734 7.842075e-05
condition   0.09580546 0.04161237 400.27441 2.302331 2.182890e-02
</code></pre>

<p>Notice, specifically, the jump in the p value from $p = .188$ in the t test, to $p = .021$ from either <code>lmer</code> method.</p>

<hr>

<p>I've tried, and failed to provide a reproducible example of this, using the <code>anorexia</code> dataset in the <code>MASS</code> package, so I would assume the problem is something idiosyncratic to my data, but I don't understand what.</p>

<pre><code># Borrowing from http://ww2.coastal.edu/kingw/statistics/R-tutorials/dependent-t.html
&gt;data(anorexia, package=""MASS"")
&gt;ft = subset(anorexia, subset=(Treat==""FT""))
&gt;wgt = c(ft$Prewt, ft$Postwt)
&gt;pre.post = rep(c(""pre"",""post""),c(17,17))
&gt;subject = rep(LETTERS[1:17],2)
&gt;t.test(wgt~pre.post, data=ft.new, paired=T)

    Paired t-test

data:  wgt by pre.post
t = 4.1849, df = 16, p-value = 0.0007003
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
  3.58470 10.94471
sample estimates:
mean of the differences 
               7.264706 

&gt;m = lmer(wgt ~ pre.post + (1|subject), data=ft.new)
&gt;summary(m)$coef

             Estimate Std. Error       df   t value     Pr(&gt;|t|)
(Intercept) 90.494118   1.689013 26.17129 53.578096 0.0000000000
pre.postpre -7.264706   1.735930 15.99968 -4.184908 0.0007002806
</code></pre>
"
"0.126563449052859","0.123493095605825","107865","<p>I'm investigating environmental effects (wind) on acoustic receiver detection probability for two types of transmitters using a binomial glmer. While my model analysis indicates that there's a significant effect between wind speed and transmitter type, graphical visualisation does not confirm this. If I'm correct, an interaction should demonstrate different regression slopes.</p>

<pre><code>m1 &lt;- glmer(cbind(df$Valid.detections, df$False.Detections) ~ Transmitter.depth + 
               Receiver.depth + Water.temperature + Wind.speed + Transmitter + 
               Distance + Habitat + Replicate + (1 | Day) + (Distance | SUR.ID) + Transmitter:Distance + Transmitter.depth:Habitat + 
               Receiver.depth:Habitat + Wind.speed:Transmitter, data=df, family=binomial(link=logit))
</code></pre>

<p>The model summary is as follows:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: cbind(df$Valid.detections, df$False.Detections) ~ Transmitter.depth +  
    Receiver.depth + Water.temperature + Wind.speed + Transmitter +  
    Distance + Habitat + Replicate + (1 | Day) + (Distance |  
    SUR.ID) + Transmitter:Distance + Transmitter.depth:Habitat +      Receiver.depth:Habitat + Wind.speed:Transmitter
   Data: df

     AIC      BIC   logLik deviance df.resid 
  3941.9   4043.8  -1953.9   3907.9     2943 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-9.4911  0.0000  0.0000  0.5666  1.9143 

Random effects:
 Groups Name        Variance Std.Dev. Corr
 SUR.ID (Intercept)  0.33414 0.5781       
        Distance     0.09469 0.3077   1.00
 Day    (Intercept) 15.96629 3.9958       
Number of obs: 2960, groups:  SUR.ID, 20 Day, 6

Fixed effects:
                                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                      3.20222    2.84984   1.124  0.26116    
Transmitter.depth               -0.35015    0.11794  -2.969  0.00299 ** 
Receiver.depth                  -0.57331    0.51919  -1.104  0.26949    
Water.temperature               -0.26595    0.11861  -2.242  0.02495 *  
Wind.speed                       1.31735    1.50457   0.876  0.38127    
TransmitterPT-04                -0.68854    0.08016  -8.590  &lt; 2e-16 ***
Distance                        -0.39547    0.09228  -4.286 1.82e-05 ***
HabitatFinger                   -0.23746    3.57783  -0.066  0.94708    
Replicate2                      -0.21559    0.08009  -2.692  0.00710 ** 
TransmitterPT-04:Distance       -0.27874    0.08426  -3.308  0.00094 ***
Transmitter.depth:HabitatFinger  0.73965    0.28612   2.585  0.00973 ** 
Receiver.depth:HabitatFinger     3.02083    0.74546   4.052 5.07e-05 ***
Wind.speed:TransmitterPT-04     -0.15540    0.06572  -2.364  0.01806 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Trnsm. Rcvr.d Wtr.tm Wnd.sp TrPT-04 Distnc HbttFn Rplct2 TPT-04: Tr.:HF Rc.:HF
Trnsmttr.dp -0.024                                                                               
Recevr.dpth -0.120 -0.267                                                                        
Watr.tmprtr  0.019 -0.159  0.007                                                                 
Wind.speed   0.130  0.073 -0.974  0.040                                                          
TrnsmtPT-04 -0.015  0.027  0.020 -0.018 -0.024                                                   
Distance     0.022 -0.080  0.151 -0.052 -0.141 -0.164                                            
HabitatFngr -0.813  0.010  0.241 -0.025 -0.253  0.009   0.029                                    
Replicate2  -0.067  0.033  0.377 -0.293 -0.394  0.010   0.085  0.103                             
TrnsPT-04:D -0.006  0.043 -0.007 -0.050 -0.003  0.516  -0.373  0.004  0.006                      
Trnsmtt.:HF  0.017 -0.352  0.021  0.055  0.049  0.026  -0.142  0.031 -0.088  0.025               
Rcvr.dpt:HF  0.103  0.189 -0.830  0.051  0.817 -0.036  -0.143 -0.224 -0.385 -0.003  -0.229       
Wnd.:TPT-04 -0.002  0.026 -0.015  0.003 -0.009  0.176  -0.114 -0.002  0.016  0.306  -0.008  0.014
</code></pre>

<p><img src=""http://i.stack.imgur.com/DFxyQ.png"" alt=""enter image description here""></p>

<p>A side question: I noticed a strong negative correlation between the intercept and a dichotome categorical predictor. I wonder if this causes any problems for my data analysis. All the covariates are centered and scaled for numerical stability during modelling.  </p>
"
"0.172918590828624","0.16872368941451","108647","<p>For answering my research question I am interested in the correlation between the random slopes and random intercepts in a multilevel model, estimated using the R library lme4. </p>

<p>The data I have is: Y (test-scores from students), SES (socio-economic status for each student) and schoolid (ID for each school). </p>

<p>I am using the following syntax to estimate random intercepts and slopes for the schools: </p>

<pre><code>library(lme4)
model3 &lt;- lmer(Y ~ SES + (1 + SES | schoolid))
</code></pre>

<p>The reference I used for this syntax is this pdf:</p>

<p><a href=""http://www.bristol.ac.uk/cmm/learning/module-samples/5-concepts-sample.pdf"" rel=""nofollow"">http://www.bristol.ac.uk/cmm/learning/module-samples/5-concepts-sample.pdf</a> </p>

<p>On page 19, a similar analysis is described. It is said that by defining the random intercepts and slopes toghether, it is indirectly specified that we want the random intercepts and slopes to covary. Therefore, also the correlation between random slopes and random intercepts is estimated. Basically, exactly what I need for answering my research hyptohesis. </p>

<p>However, when I look at the results: </p>

<pre><code> summary(model3)
</code></pre>

<p>I am getting the following output:</p>

<pre><code>Linear mixed model fit by REML ['lmerMod']
Formula: Y ~ SES + (1 + SES | schoolid)

REML criterion at convergence: 8256.4

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.1054 -0.6633 -0.0028  0.6810  3.5606 

Random effects:
 Groups   Name        Variance  Std.Dev. Corr
 schoolid (Intercept) 0.6427924 0.80174      
      SES             0.0009143 0.03024  1.00
 Residual             0.3290902 0.57366      
Number of obs: 4376, groups: schoolid, 179

Fixed effects:
             Estimate Std. Error t value
(Intercept) -0.036532   0.060582  -0.603
SES          0.062491   0.009984   6.259

Correlation of Fixed Effects:
    (Intr)
SES 0.226 
</code></pre>

<p>As stated in the output, the correlation between the random slopes and random intercepts equals 1.00. I find this hard to believe. 
When I call in R: </p>

<pre><code>VarCorr(model3)$schoolid
</code></pre>

<p>I am getting the following output which gives the correlations and covariance matrix: </p>

<pre><code>                (Intercept)          SES
(Intercept)  0.64279243 0.0242429680
SES          0.02424297 0.0009143255

attr(,""stddev"")
(Intercept)         SES 
 0.80174337  0.03023782 

attr(,""correlation"")
        (Intercept) SES
(Intercept)           1   1
SES                   1   1
</code></pre>

<p>It seems as if the correlation between the slopes and intercepts was set to 1.00 by R. I did not see this in the output of anyone else when I was searching the internet on references on multilevel modelling. </p>

<p>Does anybody know what can be the cause of this correlation? 
Can it be that the correlation is set to 1.00 because otherwise the model would not be identified? 
Or is it because the variance of the random slopes is so small (0.0009) that the correlation can not be estimated? </p>

<p>I have tried to simulate data in order provide the code for a small reproducible dataset. I was however not yet able to reproduce this output by means of simulated data. As far as I have the code I will eidt my post and add the code.  </p>

<p>Edit: 
In response to a comment by Roman LuÅ¡trik, the following plot: </p>

<pre><code>ggplot(data[1:261,], aes(x = SES, y = Y)) + geom_point() + facet_wrap(~ schoolid) +
   geom_smooth(method=lm)
</code></pre>

<p>As there are in total 179 schools the plot becomes quite chaotic, therefore I included the first 10 schools only to make it readable: </p>

<p><img src=""http://i.stack.imgur.com/gSPI5.jpg"" alt=""enter image description here""></p>
"
"0.126563449052859","0.123493095605825","111150","<p>I have been reading about calculating $R^2$ values in mixed models and after reading the R-sig FAQ, other posts on this forum (I would link a few but I don't have enough reputation) and several other references I understand that using $R^2$ values in the context of mixed models is complicated. </p>

<p>However, I have recently came across these two papers below. While these methods do look promising (to me) I am not a statistician, and as such I was wondering if anyone else would have any insight about the methods they propose and how they would compare to other methods that have been proposed. </p>

<blockquote>
  <p>Nakagawa, Shinichi, and Holger Schielzeth. <a href=""http://onlinelibrary.wiley.com/doi/10.1111/j.2041-210x.2012.00261.x/full"" rel=""nofollow"">""A general and simple method for obtaining R2 from generalized linear mixedâ€effects models.""</a> Methods in Ecology and Evolution 4.2 (2013): 133-142.</p>
  
  <p>Johnson, Paul CD. ""Extension of Nakagawa &amp; Schielzeth's R2GLMM to random slopes models."" Methods in Ecology and Evolution (2014).</p>
</blockquote>

<p>The is method can also be implemented using the r.squaredGLMM function in the <a href=""http://cran.r-project.org/web/packages/MuMIn/index.html"" rel=""nofollow"">MuMIn package</a> which gives the following description of the method.</p>

<blockquote>
  <p>For mixed-effects models, $R^2$ can be categorized into two types. Marginal $R^2$ represents the variance explained by fixed factors, and is defined as:
  $$R_{GLMM}(m)^2 = \frac{Ïƒ_f^2}{Ïƒ_f^2 + \sum(Ïƒ_l^2) + Ïƒ_e^2 + Ïƒ_d^2}$$
  Conditional $R^2$ is interpreted as variance explained by both fixed and random factors (i.e. the entire model), and is calculated according to the equation:
  $$R_{GLMM}(c)^2= \frac{(Ïƒ_f^2 + \sum(Ïƒ_l^2))}{(Ïƒ_f^2 + \sum(Ïƒ_l^2) + Ïƒ_e^2 + Ïƒ_d^2}$$
  where $Ïƒ_f^2$ is the variance of the fixed effect components, and $\sum(Ïƒ_l^2)$ is the sum of all variance components (group, individual, etc.), $Ïƒ_l^2$ is the variance due to additive dispersion and $Ïƒ_d^2$ is the distribution-specific variance.  </p>
</blockquote>

<p>In my analysis I am looking at longitudinal data and I am primarily interested in variance explained by the fixed effects in the model</p>

<pre><code>library(MuMIn) 
library(lme4)

fm1 &lt;- lmer(zglobcog ~ age_c + gender_R2 + ibphdtdep + iyeareducc + apoegeno + age_c*apoegeno + (age_c | pathid), data = dat, REML = FALSE, control = lmerControl(optimizer = ""Nelder_Mead""))

# Jarret Byrnes (correlation between the fitted and the observed values)
r2.corr.mer &lt;- function(m) {
   lmfit &lt;-  lm(model.response(model.frame(m)) ~ fitted(m))
   summary(lmfit)$r.squared
}

r2.corr.mer(fm1)
[1] 0.8857005

# Xu 2003
1-var(residuals(fm1))/(var(model.response(model.frame(fm1))))
[1] 0.8783479

# Nakagawa &amp; Schielzeth's (2013)
r.squaredGLMM(fm1)
      R2m       R2c 
0.1778225 0.8099395 
</code></pre>
"
"0.185269918744954","0.180775381515547","111569","<p>I have an experiment with a design in which subjects answer four items that are of four different types based on two factors (lets call the factors letter: ""a"" X ""b"" and big: ""A"" X ""a"", resulting in four types of questions A, a, B, b). The order of items (called here 1-4) is held constant and each subject answers one item of each type. The types are randomized. A subject can for example get question-type combinations: 1-a, 2-B, 3-b, 4-A; or 1-B, 2-b, 3-a, 4-A; etc.</p>

<p>I am interested in effects of question types, but expect that the random effects may play a role as well. I tried to use the following model:</p>

<pre><code>glmer(answer ~ (1|subject) + (big*letter|item) + big*letter, data = data, family = binomial(link = ""logit""))  
</code></pre>

<p>When I compare this model with one without random slopes:</p>

<pre><code>glmer(answer ~ (1|subject) + (1|item) + big*letter, data = data, family = binomial(link = ""logit""))
</code></pre>

<p>... the first model is not better in any way than the second:</p>

<pre><code>   Df    AIC    BIC  logLik deviance Chisq Chi Df Pr(&gt;Chisq)
m2  6 1242.1 1272.1 -615.04   1230.1                        
m1 15 1261.2 1336.2 -615.60   1231.2     0      9          1
</code></pre>

<p>So, my first question is whether the model is specified correctly given the design I have. The second question would be, why is it that including random slopes does not improve the model, even though it is possible to see from the data, that the effect of question type obviously differs between the items.</p>

<p>Edit:
Summary table for m1:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood ['glmerMod']
 Family: binomial ( logit )
Formula: answer ~ (1 | subject) + (big * letter | item) + big * letter 
   Data: data 

      AIC       BIC    logLik  deviance 
1261.2010 1336.2061 -615.6005 1231.2010 

Random effects:
 Groups  Name               Variance Std.Dev. Corr             
 subject (Intercept)        0.71862  0.8477                    
 item    (Intercept)        0.00000  0.0000                    
         bigTRUE            0.04241  0.2059     NaN            
         letterTRUE         0.10219  0.3197     NaN  1.00      
         bigTRUE:letterTRUE 0.05749  0.2398     NaN -1.00 -1.00
Number of obs: 1097, groups: subject, 275; item, 4

Fixed effects:
                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)          1.8297     0.1798  10.176  &lt; 2e-16 ***
bigTRUE             -0.9339     0.2413  -3.870 0.000109 ***
letterTRUE          -0.7073     0.2734  -2.587 0.009679 ** 
bigTRUE:letterTRUE   0.7458     0.3159   2.361 0.018212 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) bgTRUE ltTRUE
bigTRUE     -0.683              
letterTRUE  -0.602  0.698       
bgTRUE:TRUE  0.521 -0.786 -0.792
</code></pre>
"
"0.10696563746014","0.104370715180858","111915","<p>I just fitted the following linear mixed effects model:</p>

<pre><code>Linear mixed model fit by maximum likelihood  ['lmerMod']
 Formula: price ~ variable + (1 | product)
    Data: podzbior

       AIC       BIC    logLik  deviance  df.resid 
 130840.14 130868.85 -65416.07 130832.14      9674 

Scaled residuals: 
     Min      1Q  Median      3Q     Max 
 -6.2824 -0.3099 -0.0547  0.2201 12.4291 

Random effects:
 Groups           Name     Variance Std.Dev.
 product         (Intercept) 427375   653.7   
 Residual                     25930   161.0   
 Number of obs: 9678, groups: product, 1222

Fixed effects:
                  Estimate Std. Error  t value
 (Intercept)     9.362e+02  1.899e+01    49.29
  variable      -7.521e-04  1.171e-04    -6.42

Correlation of Fixed Effects:
              (Intr)
  variable    -0.050
</code></pre>

<p>That was output from <code>summary(lmerModel)</code>, after the run of <code>lmer</code> I got this warning:</p>

<pre><code>Warning:
  In checkScaleX(X, ctrl = control) :
  Some predictor variables are on very different scales: consider rescaling
</code></pre>

<p>Q1 Predictor variable is numeric from 0 to something like 100k, how It should be scaled? </p>

<p>Random effects with confidence intervals chart for this model looks like this, is it OK?:</p>

<p><img src=""http://i.stack.imgur.com/wR3Lp.png"" alt=""""></p>

<p>I am pretty sure residuals are not OK. What should I do in this case?</p>

<p><img src=""http://i.stack.imgur.com/8hzOE.png"" alt=""""></p>

<p>How can I go deeper with this model diagnostic, besides checking p-values?</p>
"
"0.108482956331022","0.105851224804993","114678","<p>Can anyone tell us how to evaluate the fit of our generalized linear model with a poisson distribution? We can't really tell if the model is a good fit or not. Do you use the deviance to answer this question? If so, what does it tell us in the following example? </p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace
  Approximation) [glmerMod]
 Family: poisson  ( log )
Formula: vok ~ factor(koen) + (1 | group) + factor(obs) + rid + aggr +  
    offset(log(min))
   Data: data

     AIC      BIC   logLik deviance df.resid 
   156.1    172.8    -70.0    140.1       52 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.5286 -0.6338 -0.3348  0.5913  4.8183 

Random effects:
 Groups Name        Variance Std.Dev.
 group  (Intercept) 0        0       
Number of obs: 60, groups:  group, 60

Fixed effects:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -5.40345    0.37230 -14.514  &lt; 2e-16 ***
factor(koen)1  1.13549    0.38823   2.925  0.00345 ** 
factor(obs)2   0.84057    0.51918   1.619  0.10544    
factor(obs)3   0.55973    0.24933   2.245  0.02477 *  
factor(obs)4  -1.24449    0.55967  -2.224  0.02617 *  
rid            0.10088    0.01939   5.203 1.96e-07 ***
aggr           0.05890    0.02868   2.053  0.04003 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) fct()1 fct()2 fct()3 fct()4 rid   
factor(kn)1 -0.705                                   
factor(bs)2 -0.275 -0.107                            
factor(bs)3 -0.342 -0.065  0.302                     
factor(bs)4 -0.206  0.005  0.157  0.355              
rid         -0.106 -0.343  0.304  0.304  0.248       
aggr        -0.106 -0.129  0.103 -0.313 -0.241 -0.287
</code></pre>
"
"0.274799698220133","0.268133222179948","114895","<p>I am a user more familiar with R, and have been trying to estimate random slopes (selection coefficients) for about 35 individuals over 5 years for four habitat variables. The response variable is whether a location was ""used"" (1) or ""available"" (0) habitat (""use"" below).</p>

<p>I am using a Windows 64-bit computer.</p>

<p>In R version 3.1.0, I use the data and expression below. PS, TH, RS, and HW are fixed effects (standardized, measured distance to habitat types). lme4 V 1.1-7. </p>

<pre><code>str(dat)
'data.frame':   359756 obs. of  7 variables:
 $ use     : num  1 1 1 1 1 1 1 1 1 1 ...
 $ Year    : Factor w/ 5 levels ""1"",""2"",""3"",""4"",..: 4 4 4 4 4 4 4 4 3 4 ...
 $ ID      : num  306 306 306 306 306 306 306 306 162 306 ...
 $ PS: num  -0.32 -0.317 -0.317 -0.318 -0.317 ...
 $ TH: num  -0.211 -0.211 -0.211 -0.213 -0.22 ...
 $ RS: num  -0.337 -0.337 -0.337 -0.337 -0.337 ...
 $ HW: num  -0.0258 -0.19 -0.19 -0.19 -0.4561 ...

glmer(use ~  PS + TH + RS + HW +
     (1 + PS + TH + RS + HW |ID/Year),
     family = binomial, data = dat, control=glmerControl(optimizer=""bobyqa""))
</code></pre>

<p>glmer gives me parameter estimates for the fixed effects that make sense to me, and the random slopes (which I interpret as selection coefficients to each habitat type) also make sense when I investigate the data qualitatively. The log-likelihood for the model is -3050.8.</p>

<p>However, most research in animal ecology do not use R because with animal location data, spatial autocorrelation can make the standard errors prone to type I error. While R uses model-based standard errors, empirical (also Huber-white or sandwich) standard errors are preferred. </p>

<p>While R does not currently offer this option (to my knowledge - PLEASE, correct me if I am wrong), SAS does - although I do not have access to SAS, a colleague agreed to let me borrow his computer to determine if the standard errors change significantly when the empirical method is used.</p>

<p>First, we wished to ensure that when using model-based standard errors, SAS would produce similar estimates to R - to be certain that the model is specified the same way in both programs. I don't care if they are exactly the same - just similar.
I tried (SAS V 9.2):</p>

<pre><code>proc glimmix data=dat method=laplace;
   class year id;
   model use =  PS TH RS HW / dist=bin solution ddfm=betwithin;
   random intercept PS TH RS HW / subject = year(id) solution type=UN;
run;title;
</code></pre>

<p>I also tried various other forms, such as adding lines</p>

<pre><code>random intercept / subject = year(id) solution type=UN;
random intercept PS TH RS HW / subject = id solution type=UN;
</code></pre>

<p>I tried without specifying the </p>

<pre><code>solution type = UN,
</code></pre>

<p>or commenting out</p>

<pre><code>ddfm=betwithin;
</code></pre>

<p>No matter how we specify the model (and we have tried many ways), I cannot get the random slopes in SAS to remotely resemble those output from R - even though the fixed effects are similar enough. And when I mean different, I mean that not even the signs are the same. The -2 Log Likelihood in SAS was 71344.94. </p>

<p>I can't upload my full dataset; so I made a toy dataset with only the records from three individuals. SAS gives me output in a few minutes; in R it takes over an hour. Weird. With this toy dataset I'm now getting different estimates for the fixed effects. </p>

<p>My question:
Can anyone shed light on why the random slopes estimates might be so different between R and SAS? Is there anything I can do in R, or SAS, to modify my code so that the calls produce similar results? I'd rather change the code in SAS, since I ""believe"" my R estimates more. </p>

<p>I'm really concerned with these differences and want to get to the bottom of this problem!</p>

<p>My output from a toy dataset that uses only three of the 35 individuals in the full dataset for R and SAS are included as jpegs.</p>

<p><img src=""http://i.stack.imgur.com/ucNnh.jpg"" alt=""R output"">
<img src=""http://i.stack.imgur.com/jUC0K.jpg"" alt=""SAS output 1"">
<img src=""http://i.stack.imgur.com/IfCJm.jpg"" alt=""SAS output 2"">
<img src=""http://i.stack.imgur.com/7XJdA.jpg"" alt=""SAS output 3""></p>

<hr>

<p>EDIT AND UPDATE:</p>

<p>As @JakeWestfall helped discover, the slopes in SAS do not include the fixed effects. When I add the fixed effects, here is the result - comparing R slopes to SAS slopes for one fixed effect, ""PS"", between programs: (Selection coefficient = random slope). Note the increased variation in SAS. </p>

<p><img src=""http://i.stack.imgur.com/JozTd.jpg"" alt=""R vs SAS for PS""></p>
"
"0.158655679740622","0.154806788004275","115356","<p>I'm a beginner in statistics and I have to run multilevel logistic regressions. I am confused with the results as they differ from logistic regression with just one level. </p>

<p>I don't know how to interpret the variance and correlation of the random variables. And I wonder how to compute the ICC.</p>

<p>For example : I have a dependent variable about the protection friendship ties give to individuals (1 is for individuals who can rely a lot on their friends, 0 is for the others). There are 50 geographic clusters of respondant and one random variable which is a factor about the social situation of the neighborhood. Upper/middle class is the reference, the other modalities are working class and underprivileged neighborhoods. </p>

<p>I get these results :</p>

<pre><code>&gt; summary(RLM3)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: Arp ~ Densite2 + Sexe + Age + Etudes + pcs1 + Enfants + Origine3 +      Sante + Religion + LPO + Sexe * Enfants + Rev + (1 + Strate |  
    Quartier)
   Data: LPE
Weights: PONDERATION
Control: glmerControl(optimizer = ""bobyqa"")

     AIC      BIC   logLik deviance df.resid 
  3389.9   3538.3  -1669.9   3339.9     2778 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.2216 -0.7573 -0.3601  0.8794  2.7833 

Random effects:
 Groups   Name           Variance Std.Dev. Corr       
 Neighb. (Intercept)     0.2021   0.4495              
          Working Cl.    0.2021   0.4495   -1.00      
          Underpriv.     0.2021   0.4495   -1.00  1.00
Number of obs: 2803, groups:  Neigh., 50

Fixed effects:
</code></pre>

<p>The differences with the ""call"" part is due to the fact I translated some words.</p>

<p>I think I understand the relation between the random intercept and the random slope for linear regressions but it is more difficult for logistics ones. I guess that when the correlation is positive, I can conclude that the type of neighborhood (social context) has a positive impact on the protectiveness of friendship ties, and conversely. But how do I quantify that ?</p>

<p>Moreover, I find it odd to get correlation of 1 or -1 and nothing more intermediate.</p>

<p>As for the ICC I am puzzled because I have seen a post about lmer regression that indicates that intraclass correlation can be computed by dividing the variance of the random intercept by the variance of the random intercept, plus the variance the random variables, plus the residuals. </p>

<p>But there are no residuals in the results of a glmer. I have read in a book that ICC must be computed by dividing the random intercept variance by the random intercept variance plus 2.36 (piÂ²/3). But in another book, 2.36 was replaced by the inter-group variance (the first level variance I guess). 
What is the good solution ?</p>

<p>I hope these questions are not too confused.
Thank you for your attention !</p>
"
"0.095672974646988","0.0933520056018673","117853","<p>I've just gotten stuck in interpreting the output for a linear mixed effects model. My model includes Week as a time predictor, and scores on a depression scale as outcome. I have not worked with R before, and while the output is mostly clear to me, there's one part I don't understand. What does my correlation of -0.18 in the random effects mean?</p>

<p>Here's the summary for the lmer model I ran:</p>

<pre><code>Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: HAMD ~ 1 + week + (1 + week | id)
   Data: MD

Random effects:
 Groups   Name        Variance Std.Dev. Corr 
 id       (Intercept)  8.769   2.961         
          week         2.098   1.448    -0.18
 Residual             10.974   3.313         
Number of obs: 340, groups:  id, 60

Fixed effects:
            Estimate Std. Error t value
(Intercept)  23.4602     0.5006   46.87
week         -2.3518     0.2165  -10.86
</code></pre>

<p>Would it then be correct to state that <em>participants with a higher initial score decrease their depression score less rapidly over time</em>?</p>

<p>Thank you for taking the time to read this. I hope someone can help me out.</p>
"
"0.171145019936223","0.166993144289373","118172","<p>I fitted a mixed logit model with crossed random effects in <code>lme4_1.1-7::glmer</code> (R version 3.1.1 / OS X 10.9.4 Mavericks).</p>

<p>Had to simplify the maximal random-effect structure justified by the design due to failed convergence; the final model is estimated without any problems:</p>

<pre><code>fitted_1 &lt;- glmer(DV ~ IV1.d*IV2.d + (IV1.d*IV2.d| SubjN) + (1|Items) +  
                       (0+IV1.d|Items) + (0+IV2.d|Items) + (0+IV1.d:IV2.d|Items), 
                  glmerControl(optimizer='bobyqa', optCtrl = list(maxfun=20000)), 
                  data=myPP, family=binomial) 
</code></pre>

<p><code>DV</code> is the binary response variable</p>

<p><code>IV1.d</code> and <code>IV2.d</code> are two within-subjects within-items categorical predictors, two levels each, deviation-contrast coded (values: -.5/.5)</p>

<p>I tried to compute confidence intervals for the beta parameters using profile likelihood via <code>confint.merMod()</code> but the computation seems to be failing.
For all betas, I got values <code>(-Inf Inf)</code> and warning messages of non-monotonic profiles. Reading on [R-sig-ME], this latter issue should mean there is something wonky with the profile. </p>

<p>I tried to simplify the random structure of the model until profile confidence intervals could be computed. Here is the model:</p>

<pre><code>fitted_4 &lt;- glmer(DV ~ IV1.d + IV2.d + IV1.d:IV2.d + (IV1.d + IV2.d| SubjN) +
                       (1|Items) + (0+IV1.d|Items) + (0 +IV2.d|Items), data=myPP, 
                  glmerControl(optimizer='bobyqa', optCtrl = list(maxfun=20000)),
                  family=binomial)
</code></pre>

<ol>
<li><p>I'm not understanding what causes the profile likelihood method to fail for the original <code>fitted_1</code> model but not for <code>fitted_4</code>. </p></li>
<li><p>Is there any other way I could obtain profile CI's for <code>fitted_1</code>?</p></li>
</ol>

<hr>

<pre><code>summary(fitted_1)

##       AIC      BIC   logLik deviance df.resid 
##    1074.0   1168.1   -519.0   1038.0     1362 

##  Scaled residuals: 
##      Min      1Q  Median      3Q     Max 
##  -2.2673 -0.3611 -0.2500 -0.1378  4.5826 

##  Random effects:
##   Groups  Name        Variance  Std.Dev.  Corr             
##   SubjN   (Intercept) 2.424e+00 1.557e+00                  
##           IV1.d       1.990e+00 1.411e+00  0.17            
##           IV2.d       6.065e-01 7.788e-01 -0.97 -0.29      
##           IV1.d:IV2.d 2.172e+00 1.474e+00 -0.19 -0.81  0.39
##   Items   (Intercept) 4.615e-03 6.793e-02                  
##   Items.1 IV1.d       3.233e-13 5.686e-07                  
##   Items.2 IV2.d       9.442e-01 9.717e-01                  
##   Items.3 IV1.d:IV2.d 4.801e-01 6.929e-01                  
##  Number of obs: 1380, groups:  SubjN, 88; Items, 12

##  Fixed effects:
##              Estimate Std. Error z value Pr(&gt;|z|)    
##  (Intercept) -2.40604    0.23196 -10.373   &lt;2e-16 ***
##  IV1.d        0.08249    0.36355   0.227   0.8205    
##  IV2.d        1.11046    0.43579   2.548   0.0108 *  
##  IV1.d:IV2.d  0.16386    0.71246   0.230   0.8181    

##  Correlation of Fixed Effects:
##              (Intr) IV1.d  IV2.d 
##  IV1.d        0.118              
##  IV2.d       -0.434 -0.083       
##  IV1.d:IV2.d -0.090 -0.628  0.064
</code></pre>

<p></p>

<pre><code>confint(fitted_1, method=""profile"", which='beta_')`

##               2.5 % 97.5 %
##   (Intercept)  -Inf    Inf
##   IV1.d        -Inf    Inf
##   IV2.d        -Inf    Inf
##   IV1.d:IV2.d  -Inf    Inf

##  Warning messages:
##  1: In profile.merMod(object, signames = oldNames, ...) :
##    non-monotonic profile
##  2: In profile.merMod(object, signames = oldNames, ...) :
##    non-monotonic profile
##  3: In profile.merMod(object, signames = oldNames, ...) :
##    non-monotonic profile
##  4: In profile.merMod(object, signames = oldNames, ...) :
##    non-monotonic profile
</code></pre>

<p></p>

<pre><code>summary(fitted_4)

##       AIC      BIC   logLik deviance df.resid 
##      1068     1136     -521     1042     1367 

##  Scaled residuals: 
##      Min      1Q  Median      3Q     Max 
##  -2.3575 -0.3555 -0.2522 -0.1613  4.6391 

##  Random effects:
##   Groups  Name        Variance Std.Dev. Corr       
##   SubjN   (Intercept) 2.23144  1.4938              
##           IV1.d       1.53606  1.2394    0.09      
##           IV2.d       0.31120  0.5579   -1.00 -0.18
##   Items   (Intercept) 0.01344  0.1159              
##   Items.1 IV1.d       0.00000  0.0000              
##   Items.2 IV2.d       0.92942  0.9641              
##  Number of obs: 1380, groups:  SubjN, 88; Items, 12

##  Fixed effects:
##              Estimate Std. Error z value Pr(&gt;|z|)    
##  (Intercept) -2.30252    0.21029 -10.949   &lt;2e-16 ***
##  IV1.d        0.17448    0.27729   0.629   0.5292    
##  IV2.d        0.80072    0.36862   2.172   0.0298 *  
##  IV1.d:IV2.d  0.01351    0.41660   0.032   0.9741    

##  Correlation of Fixed Effects:
##              (Intr) IV1.d  IV2.d 
##  IV1.d        0.012              
##  IV2.d       -0.274 -0.010       
##  IV1.d:IV2.d  0.006 -0.255 -0.038
</code></pre>

<p></p>

<pre><code>confint(fitted_4, which='beta_', method='profile')

##                    2.5 %     97.5 %
##  (Intercept) -2.74571641 -1.9052351
##  IV1.d       -0.37989551  0.7320931
##  IV2.d        0.03993436  1.5903197
##  IV1.d:IV2.d -0.80790153  0.8346440
</code></pre>

<hr>

<h2>UPDATE</h2>

<pre><code>## re-compute profiles for both random and fixed effects

pp &lt;- profile(fitted_1)

## 24 warnings with profile(fitted_1) of the types:
## In profile.merMod(fitted_1) : non-monotonic profile
## In optwrap(optimizer, par = start, fn = function(x) dd(mkpar(npar1,  ... :
   # convergence code 1 from bobyqa: bobyqa -- maximum number of function evaluations exceeded

(c_ci &lt;- confint(pp))

## 2.5 % 97.5 %
## .sig01          0    Inf
## .sig02         -1      1
## .sig03         -1      1
## .sig04         -1      1
## .sig05          0    Inf
## .sig06         -1      1
## .sig07         -1      1
## .sig08          0    Inf
## .sig09         -1      1
## .sig10          0    Inf
## .sig11          0    Inf
## .sig12          0    Inf
## .sig13          0    Inf
## .sig14          0    Inf
## (Intercept)  -Inf    Inf
## IV1.d        -Inf    Inf
## IV2.d        -Inf    Inf
## IV1.d:IV2.d  -Inf    Inf


## plot the profiles (all weird)
    ggplot(as.data.frame(pp),aes(.focal,.zeta))+
    geom_point()+geom_line()+
    facet_wrap(~.par,scale=""free_x"")+
    geom_hline(yintercept=0,colour=""gray"")+
    geom_hline(yintercept=c(-1.96,1.96),linetype=2,
               colour=""gray"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/IUtJx.jpg"" alt=""Plot_profile_fitted1.jpeg""></p>

<pre><code>### setting delta to a smaller value to make the profile stepsize smaller
system.time(pp2 &lt;- profile(fitted_1, delta = 0.1))

## user    system   elapsed 
## 64292.282   135.451 75676.403

## Warning messages:
## 1: In profile.merMod(orig.pp, delta = 0.1) : non-monotonic profile
## 2: display list redraw incomplete
## [...]

c_ci2 &lt;- confint(pp2)
c_ci2

## 2.5 % 97.5 %
## .sig01          0    Inf
## .sig02         -1      1
## .sig03         -1      1
## .sig04         -1      1
## .sig05          0    Inf
## .sig06         -1      1
## .sig07         -1      1
## .sig08          0    Inf
## .sig09         -1      1
## .sig10          0    Inf
## .sig11          0    Inf
## .sig12          0    Inf
## .sig13          0    Inf
## .sig14          0    Inf
## (Intercept)  -Inf    Inf
## IV1.d        -Inf    Inf
## IV2.d        -Inf    Inf
## IV1.d:IV2.d  -Inf    Inf


## plot of profiles (delta = 0.1)

ggplot(as.data.frame(pp2),aes(.focal,.zeta))+
    geom_point()+geom_line()+
    facet_wrap(~.par,scale=""free_x"")+
    geom_hline(yintercept=0,colour=""gray"")+
    geom_hline(yintercept=c(-1.96,1.96),linetype=2,
               colour=""gray"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/olzm2.jpg"" alt=""Plot of profiles delta = 0.1""></p>
"
"0.202953027447522","0.198029508595335","120421","<p>I am new to gam, and most of my knowledge comes from this document <a href=""http://www3.nd.edu/~mclark19/learn/GAMS.pdf"">http://www3.nd.edu/~mclark19/learn/GAMS.pdf</a>. Now I am using generalized addictive model with random effects to model some data, where I want to see how ""speedChange"" correlates with ""response"" in my dataset, with consideration of random effects ""user.id""</p>

<p>The code I run is shown as follows:</p>

<pre><code>speed.gammer &lt;- gamm4(response ~ s(speedChange) , data= t, random=~(1|user.id))
</code></pre>

<p>The gam can be plotted as follows:
<img src=""http://i.stack.imgur.com/xWkaU.jpg"" alt=""enter image description here""></p>

<p>I then try to interpret the gam:</p>

<pre><code>summary(speed.gammer$gam)
</code></pre>

<p>which gives the following :</p>

<pre><code>Family: gaussian 
Link function: identity 

Formula:
response ~ s(speedChange)

Parametric coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  2.30618    0.01482   155.6   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Approximate significance of smooth terms:
                 edf Ref.df     F p-value    
s(speedChange) 5.875  5.875 28.61  &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

R-sq.(adj) =  0.0263   
lmer.REML =  14688  Scale est. = 0.57643   n = 5619
</code></pre>

<p>From what I understand from the output, I learned that speedChange is significantly correlates with response, and the non-linear relationship is as shown in the plot. I know the R-squared is small, but that's not what I want to ask. I actually don't understand the mer model.</p>

<p>If I run:</p>

<pre><code>summary(speed.gammer$mer)
</code></pre>

<p>I got the following results:</p>

<pre><code>Linear mixed model fit by REML ['lmerMod']

REML criterion at convergence: 14687.7

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.5908 -0.6500 -0.0454  0.5880  3.7110 

Random effects:
 Groups   Name           Variance Std.Dev.
 user.id  (Intercept)     0.2853  0.5342  
 Xr       s(speedChange) 56.4011  7.5101  
 Residual                 0.5764  0.7592  
 Number of obs: 5619, groups:  user.id, 3042; Xr, 8

Fixed effects:
                    Estimate Std. Error t value
X(Intercept)        2.306181   0.014823  155.58
Xs(speedChange)Fx1 -0.008977   0.115045   -0.08

Correlation of Fixed Effects:
X(Int)
Xs(spdCh)F1 0.004 
</code></pre>

<p>I understand this is an lmerMod. I understand the output for lmer function, but not here. I don't understand what ""X"" means in the fixed effects. From the t-value it seems that the Intercept is significant but not the speedChange. I want to report the result of my analysis, but what is the relationship between the gam results and this mer result? How can I interpret the mer result of  </p>

<pre><code>Xs(speedChange)Fx1 -0.008977   0.115045   -0.08
</code></pre>

<p>together with the gam result:</p>

<pre><code>s(speedChange) 5.875  5.875 28.61  &lt;2e-16 ***
</code></pre>

<p>I don't see any documents that help me to understand the output in order to report the result. Could someone help?</p>
"
"0.158655679740622","0.14073344364025","120768","<p>I'm using <code>glmer()</code> with a binomial response variable. My optimal model has two fixed effects (flow and DNA) which in summary() show a non-significant p value but when I remove each fixed effect in turn from the model the likelihood ratio test comparing the two models shows a significant p value. I'm struggling to understand (1) if this is normal, and (2) how to report the results if the explanatory variables ""flow"" and ""DNA"" are important but their p values in the model are well above 0.05?</p>

<p>Optimal model:</p>

<pre><code>a25 &lt;- glmer(Status_qpcr~(1|Root)+Flow+DNA,
             family=binomial, data=spore)
summary(a25)

Generalized linear mixed model fit by maximum likelihood (Laplace
Approximation) ['glmerMod']  
Family: binomial  ( logit ) 
Formula: Status_qpcr ~ (1 | Root) + Flow + DNA   
Data: spore
      AIC      BIC   logLik deviance df.resid 
     72.9     81.0    -32.4     64.9       52 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.9318 -0.8163  0.4435  0.6848  1.6133 

Random effects:  
  Groups Name        Variance Std.Dev.  
  Root   (Intercept) 0.3842   0.6199   
  Number of obs: 56, groups:  Root, 9

Fixed effects:
Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept) -0.97752    0.79252  -1.233    0.217   
Flow         3.82779    2.27165   1.685    0.092 . 
DNA          0.01616    0.01039   1.556    0.120  
--- 
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
     (Intr) Flow   Flow -0.775        
     DNA    -0.576  0.227
</code></pre>

<p>Likelihood ratio test:</p>

<pre><code>a26 &lt;- update(a25,~.-DNA)
anova(a25,a26)

Data: spore 
Models: 
    a26: Status_qpcr ~ (1 | Root) + Flow 
    a25: Status_qpcr ~ (1 | Root) + Flow + DNA
    Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)   
a26  3 74.802 80.878 -34.401   68.802                            
a25  4 72.897 80.998 -32.448   64.897 3.9049      1    0.04815 *

a27 &lt;- update(a25,~.-Flow)
anova(a25,a27)

Data: spore 
Models: 
    a27: Status_qpcr ~ (1 | Root) + DNA 
    a25: Status_qpcr ~ (1 | Root) + Flow + DNA
    Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)
a27  3 78.440 84.723 -36.220   72.440                             
a25  4 72.897 80.998 -32.448   64.897 7.5427      1   0.006025 **
</code></pre>
"
"0.246116696825003","0.272165526975909","122026","<p>I have a question regarding re-leveling in lme4 1.1-7. </p>

<p><strong>Experimental Design:</strong></p>

<p>Our experiment is an eyetracking while reading study (single sentence stimuli). We are analyzing four different continuous eyetracking DVs  over three different regions of interest.For all DVs, we first removed outliers, then took the log value, then residualized the result (subtracted the actual reading time from a predicted reading time per character - to control for word length differences).</p>

<p>The main manipulation is the categorical factor â€œconditionâ€ which has four levels. Condition is a within-subject (repeated measures) factor that represents four different versions of a single sentential item.</p>

<p>We also have a continuous predictor (Ospan) which is between-subject and is centered. I'm leaving that out of this question though since it does not seem to be related to my problem.</p>

<p>The experimental materials were distributed in a latin square rotation over four presentation lists. This ensured that each particular subject only saw one level of condition for each sentential item, but that each subject would also see an equal number of items representing each level of condition (thus this is a repeated measures design). There are 80 sentence items (each with four levels of condition). There were 45 participants across the four lists (somewhat unbalanced).</p>

<p><strong>My problem (well, one of them):</strong></p>

<p>Working with this model (which omits Ospan) on one eyetracking DV in one region (R02) of the sentence:</p>

<pre><code>(testFirstR02_lmer03 = lmer(RT2LogR ~ condition + (1 + condition | Subject) + (1 + condition | item), data = testFirst[testFirst$Region == ""R02"",],REML = FALSE))
</code></pre>

<p>I obtain convergence when I set the reference level (of condition) to StrongIs or RCE, but the model does not converge when I re-level to a reference level of PseudoC. It produces the following error messages:</p>

<pre><code>Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 1.71338 (tol = 0.002, component 6)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge: degenerate  Hessian with 1 negative eigenvalues
</code></pre>

<p>This is the releveling code that I am using (with variants in accordance to what I am setting the reference level to):</p>

<pre><code>testFirst$condition = factor(testFirst$condition,levels=c(""StrongIs"",""RCE"",""PseudoC"",""NonIs""))
</code></pre>

<p>If I remove the random slope specification of condition for the Item fixed effect, I can get convergence no matter how I set the reference level. </p>

<p>And if I remove only the intercept/slope interaction term for that fixed effect...</p>

<pre><code>(testFirstR02_lmer03 = lmer(RT2LogR ~ condition + (1 + condition | Subject) + (1 | item) + (0 + condition | item), data = testFirst[testFirst$Region == ""R02"",],REML = FALSE))
</code></pre>

<p>... then I get the following:</p>

<pre><code>Warning message:
In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p><strong>My Questions:</strong></p>

<p>1) Shouldn't the data behave similarly despite any re-leveling? I know that one can compute contrasts by hand without even using re-level - so I find this error message a bit confusing</p>

<p>2) I'm not sure how to interpret the final warning messages? What I would ""rescale"" in my variables - they are already scaled.</p>

<p>3) Does this behavior signal something inherently unstable in my data? The releveling problem does seem to only happen when I set the reference to PseudoC. I tried adding <code>complete.cases(testFirst$RT2LogR) &amp;</code> to my data specification, thinking that something was going wrong with NAs, but it did not help.</p>

<p>I have not attached any reproducible data, as it is a large data set. I can provide a link if necessary. Any help would be greatly appreciated.</p>

<p><strong>Edit (summary and singularity tests output)</strong></p>

<pre><code>&gt; #Change ref level
&gt; testFirst$condition = factor(testFirst$condition,levels=c(""PseudoC"",""NonIs"",""StrongIs"",""RCE""))
&gt; contrasts(testFirst$condition)
         NonIs StrongIs RCE
PseudoC      0        0   0
NonIs        1        0   0
StrongIs     0        1   0
RCE          0        0   1


&gt; #Model 3 Include condition slope for item

Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 1.71338 (tol = 0.002, component 6)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge: degenerate  Hessian with 1 negative eigenvalues

&gt; summary(testFirstR02_lmer03.1)
Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: RT2LogR ~ condition + (1 + condition | Subject) + (1 + condition |      item)
   Data: testFirst[testFirst$Region == ""R02"", ]

     AIC      BIC   logLik deviance df.resid 
  2989.1   3159.3  -1469.5   2939.1     6670 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-5.6285 -0.5193  0.0227  0.5468  4.4020 

Random effects:
 Groups   Name              Variance Std.Dev. Corr             
 item     (Intercept)       0.000000 0.0000                    
          conditionNonIs    0.010466 0.1023    NaN             
          conditionStrongIs 0.010966 0.1047    NaN  0.17       
          conditionRCE      0.011935 0.1092    NaN  0.59  0.20 
 Subject  (Intercept)       0.014283 0.1195                    
          conditionNonIs    0.005491 0.0741   -0.61            
          conditionStrongIs 0.013175 0.1148   -0.47  0.75      
          conditionRCE      0.012684 0.1126   -0.55  0.75  0.75
 Residual                   0.083755 0.2894                    
Number of obs: 6695, groups:  item, 80; Subject, 45

Fixed effects:
                   Estimate Std. Error t value
(Intercept)       -0.444822   0.019226 -23.137
conditionNonIs    -0.014357   0.018888  -0.760
conditionStrongIs  0.043049   0.023169   1.858
conditionRCE      -0.002206   0.023181  -0.095

Correlation of Fixed Effects:
            (Intr) cndtNI cndtSI
conditnNnIs -0.476              
cndtnStrngI -0.442  0.496       
conditinRCE -0.489  0.626  0.553


&gt; #Model 3 Exclude condition slope for item

&gt; summary(testFirstR02_lmer03)
Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: RT2LogR ~ condition + (1 + condition | Subject) + (1 | item)
   Data: testFirst[testFirst$Region == ""R02"", ]

     AIC      BIC   logLik deviance df.resid 
  3091.7   3200.7  -1529.9   3059.7     6679 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-5.9890 -0.5127  0.0184  0.5609  4.5757 

Random effects:
 Groups   Name              Variance Std.Dev. Corr             
 item     (Intercept)       0.004827 0.06947                   
 Subject  (Intercept)       0.014292 0.11955                   
          conditionNonIs    0.004862 0.06973  -0.60            
          conditionStrongIs 0.012930 0.11371  -0.45  0.73      
          conditionRCE      0.012017 0.10962  -0.55  0.74  0.73
 Residual                   0.087334 0.29552                   
Number of obs: 6695, groups:  item, 80; Subject, 45

Fixed effects:
                    Estimate Std. Error t value
(Intercept)       -4.476e-01  2.082e-02 -21.503
conditionNonIs    -1.105e-02  1.468e-02  -0.753
conditionStrongIs  4.611e-02  1.992e-02   2.316
conditionRCE       7.436e-05  1.939e-02   0.004

Correlation of Fixed Effects:
            (Intr) cndtNI cndtSI
conditnNnIs -0.544              
cndtnStrngI -0.465  0.628       
conditinRCE -0.535  0.638  0.671
&gt; 


#Singularity test for both models

&gt; tt &lt;- getME(testFirstR02_lmer03.1,""theta"")
&gt; ll &lt;- getME(testFirstR02_lmer03.1,""lower"")
&gt; min(tt[ll==0])
[1] 0

&gt; tt &lt;- getME(testFirstR02_lmer03,""theta"")
&gt; ll &lt;- getME(testFirstR02_lmer03,""lower"")
&gt; min(tt[ll==0])
[1] 0.1889075
</code></pre>
"
"0.202953027447522","0.214531967644946","122717","<p>I have some trouble obtaining equivalent results between an <code>aov</code> between-within repeated measures model and an <code>lmer</code> mixed model.</p>

<p>My data and script look as follows</p>

<pre><code>data=read.csv(""https://www.dropbox.com/s/zgle45tpyv5t781/fitness.csv?dl=1"")
data$id=factor(data$id)
data
   id  FITNESS      TEST PULSE
1   1  pilates   CYCLING    91
2   2  pilates   CYCLING    82
3   3  pilates   CYCLING    65
4   4  pilates   CYCLING    90
5   5  pilates   CYCLING    79
6   6  pilates   CYCLING    84
7   7 aerobics   CYCLING    84
8   8 aerobics   CYCLING    77
9   9 aerobics   CYCLING    71
10 10 aerobics   CYCLING    91
11 11 aerobics   CYCLING    72
12 12 aerobics   CYCLING    93
13 13    zumba   CYCLING    63
14 14    zumba   CYCLING    87
15 15    zumba   CYCLING    67
16 16    zumba   CYCLING    98
17 17    zumba   CYCLING    63
18 18    zumba   CYCLING    72
19  1  pilates   JOGGING   136
20  2  pilates   JOGGING   119
21  3  pilates   JOGGING   126
22  4  pilates   JOGGING   108
23  5  pilates   JOGGING   122
24  6  pilates   JOGGING   101
25  7 aerobics   JOGGING   116
26  8 aerobics   JOGGING   142
27  9 aerobics   JOGGING   137
28 10 aerobics   JOGGING   134
29 11 aerobics   JOGGING   131
30 12 aerobics   JOGGING   120
31 13    zumba   JOGGING    99
32 14    zumba   JOGGING    99
33 15    zumba   JOGGING    98
34 16    zumba   JOGGING    99
35 17    zumba   JOGGING    87
36 18    zumba   JOGGING    89
37  1  pilates SPRINTING   179
38  2  pilates SPRINTING   195
39  3  pilates SPRINTING   188
40  4  pilates SPRINTING   189
41  5  pilates SPRINTING   173
42  6  pilates SPRINTING   193
43  7 aerobics SPRINTING   184
44  8 aerobics SPRINTING   179
45  9 aerobics SPRINTING   179
46 10 aerobics SPRINTING   174
47 11 aerobics SPRINTING   164
48 12 aerobics SPRINTING   182
49 13    zumba SPRINTING   111
50 14    zumba SPRINTING   103
51 15    zumba SPRINTING   113
52 16    zumba SPRINTING   118
53 17    zumba SPRINTING   127
54 18    zumba SPRINTING   113
</code></pre>

<p>Basically, 3 x 6 subjects (<code>id</code>) were subjected to three different <code>FITNESS</code> workout schemes each and their <code>PULSE</code> was measured after carrying out three different types of endurance <code>TEST</code>s.</p>

<p>I then fitted the following <code>aov</code> model :</p>

<pre><code>library(afex)
library(car)
set_sum_contrasts()
fit1 = aov(PULSE ~ FITNESS*TEST + Error(id/TEST),data=data)
summary(fit1)
Error: id
          Df Sum Sq Mean Sq F value   Pr(&gt;F)    
FITNESS    2  14194    7097   115.1 7.92e-10 ***
Residuals 15    925      62                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Error: id:TEST
             Df Sum Sq Mean Sq F value   Pr(&gt;F)    
TEST          2  57459   28729   253.7  &lt; 2e-16 ***
FITNESS:TEST  4   8200    2050    18.1 1.16e-07 ***
Residuals    30   3397     113                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>The result I obtain using</p>

<pre><code>set_sum_contrasts()
fit2=aov.car(PULSE ~ FITNESS*TEST+Error(id/TEST),data=data,type=3,return=""Anova"")
summary(fit2)
</code></pre>

<p>is identical to this.</p>

<p>A mixed model run using <code>nlme</code> gives a directly equivalent result, e.g. using <code>lme</code> :</p>

<pre><code>library(lmerTest)    
lme1=lme(PULSE ~ FITNESS*TEST, random=~1|id, correlation=corCompSymm(form=~1|id),data=data)
anova(lme1)
             numDF denDF   F-value p-value
(Intercept)      1    30 12136.126  &lt;.0001
FITNESS          2    15   115.127  &lt;.0001
TEST             2    30   253.694  &lt;.0001
FITNESS:TEST     4    30    18.103  &lt;.0001


summary(lme1)
Linear mixed-effects model fit by REML
 Data: data 
       AIC      BIC    logLik
  371.5375 393.2175 -173.7688

Random effects:
 Formula: ~1 | id
        (Intercept) Residual
StdDev:    1.699959 9.651662

Correlation Structure: Compound symmetry
 Formula: ~1 | id 
 Parameter estimate(s):
       Rho 
-0.2156615 
Fixed effects: PULSE ~ FITNESS * TEST 
                                 Value Std.Error DF   t-value p-value
(Intercept)                   81.33333  4.000926 30 20.328628  0.0000
FITNESSpilates                 0.50000  5.658164 15  0.088368  0.9308
FITNESSzumba                  -6.33333  5.658164 15 -1.119327  0.2806
TESTJOGGING                   48.66667  6.143952 30  7.921069  0.0000
TESTSPRINTING                 95.66667  6.143952 30 15.570868  0.0000
FITNESSpilates:TESTJOGGING   -11.83333  8.688861 30 -1.361897  0.1834
FITNESSzumba:TESTJOGGING     -28.50000  8.688861 30 -3.280062  0.0026
FITNESSpilates:TESTSPRINTING   8.66667  8.688861 30  0.997446  0.3265
FITNESSzumba:TESTSPRINTING   -56.50000  8.688861 30 -6.502579  0.0000
</code></pre>

<p>Or using <code>gls</code> :</p>

<pre><code>library(lmerTest)    
gls1=gls(PULSE ~ FITNESS*TEST, correlation=corCompSymm(form=~1|id),data=data)
anova(gls1)
</code></pre>

<p>However, the result I obtain using <code>lme4</code>'s <code>lmer</code> is different :</p>

<pre><code>set_sum_contrasts()
fit3=lmer(PULSE ~ FITNESS*TEST+(1|id),data=data)
summary(fit3)
Linear mixed model fit by REML ['lmerMod']
Formula: PULSE ~ FITNESS * TEST + (1 | id)
   Data: data

REML criterion at convergence: 362.4

Random effects:
 Groups   Name        Variance Std.Dev.
 id       (Intercept)  0.00    0.0     
 Residual             96.04    9.8     
...

Anova(fit3,test.statistic=""F"",type=3)
Analysis of Deviance Table (Type III Wald F tests with Kenward-Roger df)

Response: PULSE
                    F Df Df.res    Pr(&gt;F)    
(Intercept)  7789.360  1     15 &lt; 2.2e-16 ***
FITNESS        73.892  2     15 1.712e-08 ***
TEST          299.127  2     30 &lt; 2.2e-16 ***
FITNESS:TEST   21.345  4     30 2.030e-08 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Anybody any thoughts what I am doing wrong with the <code>lmer</code> model? Or where the difference comes from? Could it have to do anything with <code>lmer</code> not allowing negative intraclass corellations or something like that? Given that <code>nlme</code>'s <code>gls</code> and <code>lme</code> do return the correct result, though, I am wondering how this is different in <code>gls</code> and <code>lme</code>? Is it that the option <code>correlation=corCompSymm(form=~1|id)</code> causes them to  directly estimate the intraclass correlation, which can be either positive or negative, whereas <code>lmer</code> estimates a variance component, which cannot be negative (and ends up being estimated as zero in this case)?</p>
"
"0.172476907882941","0.168292721432467","124944","<p>I am doing various analysis on a small sample. Basically, we have an experiment where 14 subjects (UID 1 ~ 14) used one of the 6 instruments (MID 1 ~ 6) on 3 occasions (Sequence 1 ~ 3). Each time an outcome score was registered (between 1 ~ 100). </p>

<p>The test was double blind. The subjects were told they are measuring 3 different conditions while in reality they were either measuring conditions A, B, A or B, A, B (randomly assigned to the machines and users). The objective was to see if <code>A</code> and <code>B</code> are different or not.</p>

<p>To see if there is any significant difference between the ratings for the conditions A and B, I tried to fit a simple, random intercept model using the nlme package in R. I tried:</p>

<pre><code>f.1 &lt;- lme(Score ~ Condition, random = ~1|UID, data)
</code></pre>

<p>However, for some reason <code>lme</code> fails to fit the model: it gives no error or warning but the variance of the fitted random effect is essentially zero:</p>

<pre><code>&gt; summary(f.1)
Linear mixed-effects model fit by REML
 Data: data 
       AIC      BIC   logLik
  349.3259 356.0815 -170.663

Random effects:
 Formula: ~1 | UID
         (Intercept) Residual
StdDev: 0.0009303203 15.98295

Fixed effects: Score ~ Condition 
               Value Std.Error DF   t-value p-value
(Intercept) 77.47619  3.487766 27 22.213700  0.0000
ConditionA  -0.85714  4.932446 27 -0.173776  0.8633
 Correlation: 
           (Intr)
ConditionA -0.707

Standardized Within-Group Residuals:
       Min         Q1        Med         Q3        Max 
-2.9704269 -0.4677603  0.2472873  0.7835730  1.4628682 

Number of Observations: 42
Number of Groups: 14
</code></pre>

<p>I tried doing the same thing using <code>lme4</code> and got the same results. The estimates for the intercept and the <code>Condition</code> factor is almost identical to a linear model if I use <code>lm</code>.</p>

<p>I am trying hard to understand what <code>lme</code> or <code>lmer</code> fail to estimate the random effect. I generated some data by simulation and both routines had no problem fitting the model so I doubt there is something wrong with the syntax of what I have used.</p>

<p>The data is here:</p>

<pre><code>   UID MID Seq Score Condition
1    1   1   1    90  B
2    1   1   2    85  A
3    1   1   3    75  B
4    2   4   1    75  A
5    2   4   2    95  B
6    2   4   3    85  A
7    3   6   1    60  A
8    3   6   2    82  B
9    3   6   3    85  A
10   4   3   1    60  A
11   4   3   2    70  B
12   4   3   3    75  A
13   5   2   1    85  B
14   5   2   2    85  A
15   5   2   3    85  B
16   6   5   1    90  B
17   6   5   2    95  A
18   6   5   3   100  B
19   7   2   1    90  B
20   7   2   2    70  A
21   7   2   3    50  B
22   8   1   1    70  B
23   8   1   2    75  A
24   8   1   3    80  B
25   9   3   1    90  A
26   9   3   2    30  B
27   9   3   3    90  A
28  10   6   1    50  A
29  10   6   2    85  B
30  10   6   3    92  A
31  11   4   1    50  A
32  11   4   2    85  B
33  11   4   3    92  A
34  12   5   1    65  B
35  12   5   2    50  A
36  12   5   3    90  B
37  13   4   1    65  A
38  13   4   2    70  B
39  13   4   3    80  A
40  14   2   1    60  B
41  14   2   2   100  A
42  14   2   3    80  B
</code></pre>
"
"0.262147660453049","0.26403934479378","127479","<p>I'm using a mixed effects model with logistic link function (using lme4 version 1.1-7 in R).  However, I noticed that the estimates of significance for fixed effects change depending on the order of the rows in the dataset.  </p>

<p>That is, if I run a model on a dataset, I get certain estimate for my fixed effect and it has a certain p-value.  I run the model again, and I get the same estimate and p-value.  Now, I shuffle the order of rows (the data is not mixed, just the rows are in a different order).  Running the model a third time, the p-value is very different.</p>

<p>For the data I have, the estimated p-value for the fixed effect can be between p=0.001 and p=0.08.  Obviously, these are crucial differences given conventional significance levels. </p>

<p>I understand that the estimates are just estimated, and there will be differences between values for a number of reasons.  However, the magnitude of the differences for my data seem large to me, and I wouldn't expect the order of my dataframe to have this effect (we discovered this problem by chance when a colleague ran the same model but got different results.  It turned out they had ordered their data frame.).  </p>

<p>Here is the output of my script:
(X and Y are binary variables which are contrast-coded and centred, Group and SubGroup are categorical variables)</p>

<pre><code>&gt; # Fit model
&gt; m1 = glmer(X ~Y+(1+Y|Group)+(1+Y|SubGroup),family=binomial(link='logit'),data=d)
&gt; # Shuffle order of rows
&gt; d = d[sample(1:nrow(d)),]
&gt; # Fit model again
&gt; m2 = glmer(X ~Y+(1+Y|Group)+(1+Y|SubGroup),family=binomial(link='logit'),data=d)
&gt; summary(m1)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
      ['glmerMod']
 Family: binomial  ( logit )
Formula: X ~ Y + (1 + Y | Group) + (1 + Y | SubGroup)
   Data: d

      AIC       BIC    logLik  deviance  df.resid 
 200692.0  200773.2 -100338.0  200676.0    189910 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.1368 -0.5852 -0.4873 -0.1599  6.2540 

Random effects:
 Groups       Name        Variance Std.Dev. Corr 
 SubGroup     (Intercept) 0.2939   0.5421        
              Y1          0.1847   0.4298   -0.79
 Group        (Intercept) 0.2829   0.5319        
              Y1          0.4640   0.6812   -0.07
Number of obs: 189918, groups:  SubGroup, 15; Group, 12

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.0886     0.1325  -8.214   &lt;2e-16 ***
Y1            0.3772     0.2123   1.777   0.0756 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
     (Intr)
Y1 0.112 
&gt;
&gt; # -----------------
&gt; summary(m2)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
      ['glmerMod']
 Family: binomial  ( logit )
Formula: X ~ Y + (1 + Y | Group) + (1 + Y | SubGroup)
   Data: d

      AIC       BIC    logLik  deviance  df.resid 
 200692.0  200773.2 -100338.0  200676.0    189910 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.1368 -0.5852 -0.4873 -0.1599  6.2540 

Random effects:
 Groups       Name        Variance Std.Dev. Corr 
 SubGroup     (Intercept) 0.2939   0.5422        
              Y1          0.1846   0.4296   -0.79
 Group        (Intercept) 0.2829   0.5318        
              Y1          0.4641   0.6813   -0.07
Number of obs: 189918, groups:  SubGroup, 15; Group, 12

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.0886     0.1166  -9.334  &lt; 2e-16 ***
Y1            0.3773     0.1130   3.339 0.000841 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
     (Intr)
Y1 0.074 
</code></pre>

<p>I'm afraid that I can't attach the data due to privacy reasons. </p>

<p>Both models converge.  The difference appears to be in the standard errors, while the differences in coefficient estimates are smaller.  The model fit (AIC etc.) are the same, so maybe there are multiple optimal convergences, and the order of the data pushes the optimiser into different ones.  However, I get slightly different estimates every time I shuffle the data frame (not just two or three unique estimates).  In one case (not shown above), the model did not converge simply because of a shuffling of the rows.</p>

<p>I suspect that the problem lies with the structure of my particular data.  It's reasonably large (nearly 200,000 cases), and has nested random effects.  I have tried centering the data, using contrast coding and feeding starting values to lmer based on a previous fit.  This seems to help somewhat, but I still get reasonably large differences in p-values.  I also tried using different ways of calculating p-values, but I got the same problem.</p>

<p>Below, I've tried to replicate this problem with synthesised data.  The differences here aren't as big as with my real data, but it gives an idea of the problem.</p>

<pre><code>library(lme4)
set.seed(999)

# make a somewhat complex data frame
x = c(rnorm(10000),rnorm(10000,0.1))
x = sample(x)
y = jitter(x,amount=10)
a = rep(1:20,length.out=length(x))
y[a==1] = jitter(y[a==1],amount=3)
y[a==2] = jitter(x[a==2],amount=1)
y[a&gt;3 &amp; a&lt;6] = rnorm(sum(a&gt;3 &amp; a&lt;6))
# convert to binary variables
y = y &gt;0
x = x &gt;0
# make a data frame
d = data.frame(x1=x,y1=y,a1=a)

# run model 
m1 = glmer(x1~y1+(1+y1|a1),data=d,family=binomial(link='logit'))

# shuffle order of rows
d = d[sample(nrow(d)),]

# run model again
m2 = glmer(x1~y1+(1+y1|a1),data=d,family=binomial(link='logit'))

# show output
summary(m1)
summary(m2)
</code></pre>

<p>One solution to this is to run the model multiple times with different row orders, and report the range of p-values.  However, this seems inelegant and potentially quite confusing.</p>

<p>The problem does not affect model comparison estimates (using anova), since these are based on differences in model fit.  The fixed effect coefficient estimates are also reasonably robust.  Therefore, I could just report the effect size, confidence intervals and the p-value from a model comparison with a null model, rather than the p-values from within the main model.</p>

<p>Anyway, has anyone else had this problem?  Any advice on how to proceed?</p>
"
"0.158655679740622","0.154806788004275","131152","<p>Let say I've ran this linear regression:</p>

<pre><code>lm_mtcars &lt;- lm(mpg ~ wt + vs, mtcars)
</code></pre>

<p>I can use <code>anova()</code> to see the amount of variance in the dependent variable accounted for by the two predictors:</p>

<pre><code>anova(lm_mtcars)

Analysis of Variance Table

Response: mpg
          Df Sum Sq Mean Sq  F value    Pr(&gt;F)    
wt         1 847.73  847.73 109.7042 2.284e-11 ***
vs         1  54.23   54.23   7.0177   0.01293 *  
Residuals 29 224.09    7.73                       
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Lets say I now add a random intercept for <code>cyl</code>:</p>

<pre><code>library(lme4)
lmer_mtcars &lt;- lmer(mpg ~ wt + vs + (1 | cyl), mtcars)
summary(lmer_mtcars)

Linear mixed model fit by REML ['lmerMod']
Formula: mpg ~ wt + vs + (1 | cyl)
   Data: mtcars

REML criterion at convergence: 148.8

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-1.67088 -0.68589 -0.08363  0.48294  2.16959 

Random effects:
 Groups   Name        Variance Std.Dev.
 cyl      (Intercept) 3.624    1.904   
 Residual             6.784    2.605   
Number of obs: 32, groups:  cyl, 3

Fixed effects:
            Estimate Std. Error t value
(Intercept)  31.4788     2.6007  12.104
wt           -3.8054     0.6989  -5.445
vs            1.9500     1.4315   1.362

Correlation of Fixed Effects:
   (Intr) wt    
wt -0.846       
vs -0.272  0.006
</code></pre>

<p>The variance accounted for by each fixed effect now drops because the random intercept for <code>cyl</code> is now accounting for some of the variance in <code>mpg</code>:</p>

<pre><code>anova(lmer_mtcars)

Analysis of Variance Table
   Df  Sum Sq Mean Sq F value
wt  1 201.707 201.707 29.7345
vs  1  12.587  12.587  1.8555
</code></pre>

<p>But in <code>lmer_mtcars</code>, how can I tell what proportion of the variance is being accounted for by <code>wt</code>, <code>vs</code> and the random intecept for <code>cyl</code>?</p>
"
"0.197234889993286","0.192450089729875","135416","<p>I have data from a series of psychology experiments in which human subjects completed one of many tasks. Multiple observations are taken per subject. Finally, tasks can be divided into one of two types. I would like to fit a model with a fixed effect for type of task, and then random intercepts for task and subject. Below are simulated data, with the lmer model I am trying to fit.</p>

<pre><code>N = 1000
subject = rep(rnorm(N/2), each=2)             # two obs per subject
task = rep(rnorm(10, sd=.5), each=N/10)       # 10 tasks
error = rnorm(N, sd=.25)
type = rep(c(1, 5), each=N/2)                 # fixed effects of 1 or 5
dv = subject + task + type + error


df = data.frame(subject = rep(1:(N/2), each=2), 
                task    = rep(letters[1:10], each=N/10),
                type    = rep(c('x','y'), each=N/2),
                dv      = dv
)

fit = lmer(dv ~ type + (1 | task:subject) + (1 | task), data=df)
</code></pre>

<p>Here, the outputs of <code>lmer</code> are consistent with the parameters set in the simulation.</p>

<pre><code>Linear mixed model fit by REML ['lmerMod']
Formula: dv ~ type + (1 | task:subject) + (1 | task)
   Data: df

REML criterion at convergence: 1851.7

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.0617 -0.4636  0.0018  0.4857  2.4420 

Random effects:
 Groups       Name        Variance Std.Dev.
 task:subject (Intercept) 0.93276  0.9658  
 task         (Intercept) 0.20511  0.4529  
 Residual                 0.06843  0.2616  
Number of obs: 1000, groups:  task:subject, 500; task, 10

Fixed effects:
            Estimate Std. Error t value
(Intercept)   1.0859     0.2119   5.125
typey         3.9994     0.2996  13.348

Correlation of Fixed Effects:
      (Intr)
typey -0.707
</code></pre>

<p>When I fit my data without the fixed-effect, it estimates the variance for <code>task:subject</code> and <code>task</code>..</p>

<pre><code>Linear mixed model fit by REML ['lmerMod']
Formula: ACC.ser ~ (1 | task:subid) + (1 | task)
   Data: dat.sub

REML criterion at convergence: -350.2

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-1.84997 -0.55776  0.01604  0.53404  2.03947 

Random effects:
 Groups     Name        Variance Std.Dev.
 task:subid (Intercept) 0.023145 0.15214 
 task       (Intercept) 0.003473 0.05893 
 Residual               0.008479 0.09208 

...
</code></pre>

<p>However, when I add in the fixed effect to the model, the variance estimate for task becomes 0. Moreover, when I examine the intercepts for task, they're all 0 as well. What disparities could be causing the addition of the fixed effect in <code>lmer</code> to reduce variance estimates to 0? Task is nested within type, as in the simulation. I'm happy to clarify any details that might be useful.</p>
"
"0.126563449052859","0.123493095605825","135681","<p>I'm wondering about the effect of true correlations among random effects on the standard error of my fixed effects in <code>lme4::lmer</code> models in R. </p>

<p>My assumption is that if there are true correlations--as indicated by a significant improvement in model fit when the correlation parameters are added to the model--then the inclusion of these parameters should improve the precision of the estimates somewhere else in the model. In particular, I would expect the standard error of the fixed effects to be smaller in the model in which the correlation parameters are contributing to the model fit.</p>

<p>However, a number of people have pointed out that the inclusion of correlation parameters do not improve the precision of the estimates of fixed effects even when they ""improve"" model fits:</p>

<ol>
<li><a href=""http://stats.stackexchange.com/questions/49832/in-a-multi-level-model-what-are-the-practical-implications-of-estimating-versus"">In a multi-level model, what are the practical implications of estimating versus not-estimating random effect correlation parameters?</a></li>
<li>I've conducted simulations of my own to the same end and presented them at a recent R user group meeting (Part 3: <a href=""http://github.com/pedmiston/visualizing-lmer"" rel=""nofollow"">http://github.com/pedmiston/visualizing-lmer</a>)</li>
<li>Shravan Vasishth has a few blog posts on a similar question, e.g., <a href=""http://vasishth-statistics.blogspot.com/2014/11/should-we-fit-maximal-linear-mixed.html"" rel=""nofollow"">http://vasishth-statistics.blogspot.com/2014/11/should-we-fit-maximal-linear-mixed.html</a></li>
</ol>

<p>I'm going to push the dialog even farther and challenge someone to demonstrate a situation in which including random effect correlation parameters does anything other than add complexity to the model. </p>

<p>My ignorance might be due to an over-interpretation of fixed effects as the best indicator of ""average behavior"", so I am interested to see the conditions under which random correlation parameters are useful to people who use these models to make inferences (as opposed to simply observing a correlation in the sample).</p>

<p>Thanks for your help.</p>
"
"0.126563449052859","0.123493095605825","135840","<p>I have a data set that I expect there to be some variability among individuals; therefore, I chose to include <code>ID</code> as a random effect in the <code>glmer</code> model. However, when I run the model I get the following warning: </p>

<pre><code>model.5 &lt;- glmer(R0A1 ~ Dist_MP + (1|ID), data=secondorder, family=binomial)

Warning message:
In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: very large eigenvalue
 - Rescale variables?
</code></pre>

<p>If I remove the random effect then the warning doesn't appear; therefore, I would assume that there is not enough variability among individuals (<code>ID</code>) for a random effect to be needed. Would you remove the random effect and just run a glm model? Also, how does the <code>family=binomial</code> code model the 0's and 1's in a data set? Does it consider 1's as the event? </p>

<pre><code>Summary output from glmer model:

Generalized linear mixed model fit by maximum
  likelihood (Laplace Approximation) [glmerMod]
 Family: binomial  ( logit )
Formula: R0A1 ~ Dist_MP + (1 | ID)
   Data: secondorder

     AIC      BIC   logLik deviance df.resid 
 39451.7  39476.5 -19722.8  39445.7    28693 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.0876 -1.0372  0.2758  0.9567  1.7543 

Random effects:
 Groups Name        Variance Std.Dev.
 ID     (Intercept) 0        0       
Number of obs: 28696, groups:  ID, 45

Fixed effects:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  1.679e-01  1.505e-02   11.16   &lt;2e-16 ***
Dist_MP     -1.559e-03  8.771e-05  -17.77   &lt;2e-16 ***
---
Signif. codes:  
0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
        (Intr)
Dist_MP -0.614
</code></pre>
"
"0.162428732935647","0.169808902702831","143843","<p>Iâ€™m running a logit mixed-effects model on binary data with a 2x2 within-subjects design, with subjects and items as crossed random effects, and the two independent variables deviation-contrast coded.</p>

<p>Here are model specification and summary:</p>

<pre><code>mod1 &lt;- glmer(DV ~ devX1*devX2 + (devX1*devX2|Subject) + (devX1*devX2|Item), 
              data=mydata, family=binomial, glmerControl(optimizer='bobyqa', 
              optCtrl=list(maxfun=400000)))

     AIC      BIC   logLik deviance df.resid 
   628.9    734.3   -290.4    580.9      573 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.0527 -0.5025 -0.2217  0.5654  4.0493 

Random effects:
 Groups  Name        Variance Std.Dev. Corr             
 Subject (Intercept) 0.1184   0.3440                    
         devX1       3.5387   1.8812   -0.74            
         devX2       0.2461   0.4961   -0.54  0.06      
         devX1:devX2 4.5912   2.1427    0.32 -0.84  0.07
 Item    (Intercept) 0.5568   0.7462                    
         devX1       0.2693   0.5190    0.48            
         devX2       0.3862   0.6215   -0.31 -0.51      
         devX1:devX2 2.2109   1.4869   -0.57  0.42 -0.31
Number of obs: 597, groups:  Subject, 30; Item, 20

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -1.47781    0.27602  -5.354 8.60e-08 ***
devX1        2.70622    0.55692   4.859 1.18e-06 ***
devX2        0.08229    0.45801   0.180    0.857    
devX1:devX2 -0.41055    0.99645  -0.412    0.680    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) devX1  devX2 
devX1       -0.498              
devX2       -0.179  0.046       
devX1:devX2 -0.021 -0.266 -0.657
</code></pre>

<p>The model doesÂ converge with full random structure without any problems. (It may be worth mentioning that the binned Pearson residual plot reveals that the model has some issues accounting for y = 0 original data points.)  </p>

<p>I'm encounteringÂ big convergence issues as soon as I include a centered continuous covariate (<code>Age</code>)Â asÂ fixed effect. It does not matter how much I simplify the random structure, the model will not converge.</p>

<pre><code>mod1.age &lt;- glmer(DV ~ devX1*devX2*cAge + (devX1*devX2|Subject) + (devX1*devX2|Item), 
                  data=mydata, family=binomial, glmerControl(optimizer='bobyqa', 
                  optCtrl=list(maxfun=400000)))

     AIC      BIC   logLik deviance df.resid 
   624.4    747.4   -284.2    568.4      569 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.9512 -0.5140 -0.2234  0.5361  5.3189 

Random effects:
 Groups  Name        Variance  Std.Dev.  Corr             
 Subject (Intercept) 1.037e-11 3.220e-06                  
         devX1       2.692e+00 1.641e+00  0.28            
         devX2       3.864e-02 1.966e-01  0.08 -0.94      
         devX1:devX2 4.489e+00 2.119e+00 -0.50 -0.97  0.82
 Item    (Intercept) 5.280e-01 7.267e-01                  
         devX1       2.662e-01 5.159e-01  0.79            
         devX2       3.948e-01 6.284e-01 -0.36 -0.48      
         devX1:devX2 2.906e+00 1.705e+00 -0.59  0.02 -0.22
Number of obs: 597, groups:  Subject, 30; Item, 20

Fixed effects:
                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)      -1.3832677  0.0019843  -697.1  &lt; 2e-16 ***
devX1             2.4397103  0.0020486  1190.9  &lt; 2e-16 ***
devX2             0.1386076  0.0019838    69.9  &lt; 2e-16 ***
cAge             -0.0091753  0.0016630    -5.5 3.44e-08 ***
devX1:devX2      -0.3524321  0.0028066  -125.6  &lt; 2e-16 ***
devX1:cAge        0.0150530  0.0019310     7.8 6.41e-15 ***
devX2:cAge        0.0121991  0.0018876     6.5 1.03e-10 ***
devX1:devX2:cAge  0.0005894  0.0019504     0.3    0.763    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
             (Intr) devX1  devX2  cAge   dvX1:X2 dvX1:A dvX2:A
devX1       -0.001                                           
devX2       -0.001  0.001                                    
cAge         0.002 -0.002 -0.002                             
devX1:devX2 -0.002  0.001  0.001 -0.001                      
devX1:cAge  -0.002  0.001  0.001 -0.043  0.002               
devX2:cAge  -0.001  0.001  0.001 -0.040  0.001  -0.017       
dvX1:dvX2:A  0.001 -0.001 -0.001 -0.019 -0.001  -0.020 -0.009
</code></pre>

<p>There is no problem of complete or quasi complete separation between <code>Age</code> and the binary DV. However, there is an almost perfect 1:1 match between <code>Age</code> and <code>Subject</code> (with <code>Subject</code> specified asÂ aÂ random effect in the models). In other words, for most values of <code>Age</code>, there is only one subject corresponding to that value, which makes <code>Age</code> a sort of another version of <code>Subject</code>.</p>

<p>Could this be what is causing severeÂ convergence problems? </p>

<p>If so, would transforming <code>Age</code> into a categorical variable (e.g., with 3 levels) be a suitable solution? I would like to avoid largely arbitrary choices about model specification.</p>

<p>What makes me doubt about this explanation though is that if I remove <code>Subject</code> as random effect, the resulting model still fails to converge.</p>

<pre><code>mod1.age4 &lt;- glmer(DV ~ devX1*devX2*cAge + (devX1*devX2|Item), data=mydata, 
                   family=binomial, glmerControl(optimizer='bobyqa', 
                   optCtrl=list(maxfun=400000)))
</code></pre>
"
"0.165710452999832","0.161690416690889","144815","<p>I'm encountering problems with the results of a <code>glmer</code> model (<code>lme4</code>-package).
Im trying to answer the question, whether a beaver is more likely to be present (<code>Status == 1</code>) or absent (<code>Status == 0</code>) with changing geomorphic and vegetation variables. My model formula looks like this:</p>

<pre><code>model1 &lt;- glmer(Status ~ SlopecatCentered + Canal_width + Distance:Resource_biotopes + 
                         (1 | Location), family=""binomial"", data=Daten12, 
                control=glmerControl(optimizer=""Nelder_Mead""))
</code></pre>

<p>My output looks OK, as far as I can tell, the only peculiar thing being the high estimates of <code>slopecatCentered</code>:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
  ['glmerMod']
Family: binomial  ( logit )
Formula: Status ~ SlopecatCentered + Canal_width + Distance:Resource_biotopes + 
                  (1 | Location)
Data: Datentest
Control: glmerControl(optimizer = ""Nelder_Mead"")

AIC      BIC     logLik    deviance   df.resid 
62.7     77.4    -25.3     50.7       80 

Scaled residuals: 
  Min        1Q    Median        3Q       Max 
-0.095917 -0.003971  0.000000  0.002706  0.079395 

Random effects:
Groups   Name        Variance Std.Dev.
Location (Intercept) 3682     60.68   
Number of obs: 86, groups:  Location, 43

Fixed effects:
                            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                 -18.5782     7.0847  -2.622 0.008734 ** 
SlopecatCentered             20.4162     5.6060   3.642 0.000271 ***
Canal_width                   0.4763     0.1584   3.007 0.002638 ** 
Distance1:Resource_biotopes   1.0442     0.4717   2.214 0.026861 *  
Distance2:Resource_biotopes   1.0379     0.4662   2.226 0.026010 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) SlpctC Cnl_wd Ds1:R_
SlopctCntrd -0.632                     
Canal_width -0.902  0.698              
Dstnc1:Rsr_ -0.663  0.560  0.458       
Dstnc2:Rsr_ -0.677  0.538  0.461  0.787    
</code></pre>

<p>My qqplot looks weird, though, and so does my residual vs. fitted plot:  </p>

<p><img src=""http://i.stack.imgur.com/8SJjD.jpg"" alt=""qqnorm plot with sjp.glmer(model,...)""></p>

<p><img src=""http://i.stack.imgur.com/wyZp7.jpg"" alt=""fitted vs. residual plot using plot(model)""></p>

<p>edit: I just had a closer look on my data: The <code>SlopecatCentered</code>variable is not a perfect predictor, but my random factor <code>Location</code>is causing this problem. In my raw data set, it denotes 43 different locations. One location has two <code>distance</code> in which most of the variables were measured, so my <code>location</code>variable has 43 * 2 = 86 entrys (in fact, that's the length of the data frame): </p>

<pre><code> &gt;Daten12$Loc
[1] 1  1  2  2  3  3  4  4  5  5  6  6  7  7  8  8  9  9  10 10 11 11 12 12 13 13 14 14 15 15 16 16 17
[34] 17 18 18 19 19 20 20 21 21 22 22 23 23 24 24 25 25 26 26 27 27 28 28 29 29 30 30 31 31 32 32 33 33
[67] 34 34 35 35 36 36 37 37 38 38 39 39 40 40 41 41 42 42 43 43
43 Levels: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 ... 43
</code></pre>

<p>I changed that to 1-86 and ran a test model and the plot looked ok (I know that the random effect was futile in that test model, but I wanted to get to the root of the problem).</p>

<p>So apparantly, my raw data frame layout is wrong. But I got samples online to compare, and their layout looks similar, so I just don't know how to fix it.   </p>
"
"0.095672974646988","0.0933520056018673","144904","<p>I have a data set containing various vegetation and geomorphic variables sampled in 3 <code>distances</code> on both <code>sides</code> of 43 drainage ditches (<code>Location</code>). Roughly half of these ditches are occupied by a beaver, the other half is empty. Now I want to run a model with the binomial response variable <code>Status</code> (""beaver == 1"" / ""beaver == 0"")
I'm struggeling with the order and layout of the nested and interaction effects using <code>glmer</code>. So far I've got</p>

<pre><code>fit &lt;- glmer(Status ~ BankslopeScaled + Connectivity + 
                      Canal_width + Distance:Food_crops + 
                      Distance:Edible_trees + 
                (1 | Distance/Side/Location), 
              data, family=binomial(link=""logit"")
</code></pre>

<p>but I'm not sure ifI still have pseudoreplication in my data or whether I correctly applied the formuly in order to estimate the influence of the predictors in every <code>distance</code> on both <code>sides</code> in each <code>Location</code>. </p>

<p>Like, if <code>food_crops</code> in the 3rd <code>distance</code> on the left <code>side</code> is lower than <code>edible_trees</code> in the 2nd <code>distance</code> on the right <code>side</code>, then ...</p>

<p>I kinda feel like there's something wrong with my random effects-term.</p>

<p>My out put looks like this:</p>

<pre><code>summary(fit)

Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
Family: binomial  ( logit )
Formula: Status ~ BankslopeScaled + Connectivity + Canal_width + Distance:Food_crops +  
Distance:Edible_trees + (1 | Distance/Side/Location)
Data: Satz

     AIC      BIC   logLik deviance df.resid 
   314.6    360.8   -144.3    288.6      245 

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-2.18541 -0.71205  0.07243  0.82483  1.75303 

Random effects:
 Groups                   Name        Variance  Std.Dev. 
 Location:(Side:Distance) (Intercept) 2.834e-02 1.683e-01
 Side:Distance            (Intercept) 2.074e-10 1.440e-05
 Distance                 (Intercept) 2.085e-10 1.444e-05
 Number of obs: 258, groups:  Location:(Side:Distance), 258; Side:Distance, 6; Distance, 3

 Fixed effects:
                        Estimate Std. Error z value Pr(&gt;|z|)    
 (Intercept)            -2.86517    0.79747  -3.593 0.000327 ***
 BankslopeScaled         1.76475    0.62541   2.822 0.004776 ** 
 Connectivity            0.10394    0.02729   3.809 0.000140 ***
 Canal_width             0.19138    0.11089   1.726 0.084364 .  
 Distance1:Food_crops    0.03667    0.09366   0.391 0.695441    
 Distance2:Food_crops    0.10852    0.08996   1.206 0.227694    
 Distance3:Food_crops    0.06303    0.08502   0.741 0.458510    
 Distance1:Edible_trees  0.02273    0.01327   1.712 0.086818 .  
 Distance2:Edible_trees -0.01750    0.02992  -0.585 0.558738    
 Distance3:Edible_trees  0.09769    0.07986   1.223 0.221201    
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

 [correlation of fixed effects snipped]    
</code></pre>

<p>A point into the right direction is much appreciated!</p>
"
"0.224585387973546","0.219137081015428","149732","<p>I'm trying to analyze the data from an experiment I conducted, and could use some guidance in relation to fixed vs. random effects.</p>

<p>The experiment was related to risk-seeking behavior in the context of hypothetical gambles, and implemented a 3 (Response Scale: Control vs. RI vs. ABR) x 3 (Stakes) X 5 (Endowment) factorial design. Response Scale was a between-subjects manipulation, and the levels of Stakes and Endowment were combined factorially to produce 15 different gamble scenarios, all of which were evaluated by each participant (i.e. gamble evaluation was within-subjects). The DV of interest for the particular analysis I'm working on is a binary indicator variable called ""Would.Play"" that describes whether a participant would choose to play the gamble if they were to encounter it in real life.</p>

<p>As a preliminary analysis, I'd like to be able to claim that there were no [or, as the data seem to indicate, <em>were</em>] meaningful differences in Would.Play as a result of random assignment to a particular Response Scale condition (designated by the factor variable ""Response.Scale"", ref=""Control"").</p>

<p>I can obviously do this with a binary logit for each of the 15 gambles (designated by the variable ""Gamble.Num""), but I'd like to avoid issues with multiple testing. My preference, therefore, is to fit a single model that accounts for the heterogeneity in gambles by fitting a separate intercept for each gamble.</p>

<p>I've come across two ways to do this, each of which seems to give different results: Dummy ""Fixed Effects"" modeling in glm() and ""random effects"" modeling in glmer() (see output below).</p>

<p>It seems possible that the difference in the estimated coefficients could be the result of the Dummy ""Fixed Effects"" approach taking Gamble.Num==1 as a reference level, but I don't have a very deep understanding of the math underlying these two techniques. I was hoping someone would be able to give me a quick explanation of (a) why the these two models appear to give different results; and (b) whether one of these approaches is better suited to answering my question of interest: is there a unique effect of Response.Scale on Would.Play, taking heterogeneity in gambles into account?</p>

<p>Below is a quick look at the data I'm using, and the output of the two models:</p>

<pre><code>## Data ##
head(analysis.0.data)
 Local.ID Condition Response.Scale RS.Code Gambles.First Gamble.Num Endowment Stakes
1        8         4             RI       1             0          1      -150     10
2        8         4             RI       1             0          2      -150     50
3        8         4             RI       1             0          3      -150    200
4        8         4             RI       1             0          4       -25     10
5        8         4             RI       1             0          5       -25     50
6        8         4             RI       1             0          6       -25    200
  Would.Play Perc.Risk
1          0         4
2          0         6
3          0         5
4          0         3
5          0         5
6          0         7


## Dummy ""Fixed Effects"" Model ##
summary(glm(Would.Play ~ Response.Scale + factor(Gamble.Num), family=""binomial"",     
data=analysis.0.data))

Call:
glm(formula = Would.Play ~ Response.Scale + factor(Gamble.Num), 
    family = ""binomial"", data = analysis.0.data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.7766  -0.7204  -0.4678   0.7006   2.5394  

Coefficients:
                     Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)          -1.14906    0.21987  -5.226 1.73e-07 ***
Response.ScaleRI     -0.06749    0.12815  -0.527  0.59844    
Response.ScaleABR    -0.91035    0.13843  -6.576 4.82e-11 ***
factor(Gamble.Num)2  -0.94090    0.35886  -2.622  0.00874 ** 
factor(Gamble.Num)3  -1.12416    0.37769  -2.976  0.00292 ** 
factor(Gamble.Num)4   0.31966    0.28379   1.126  0.25999    
factor(Gamble.Num)5  -0.63953    0.33303  -1.920  0.05482 .  
factor(Gamble.Num)6  -0.85860    0.35120  -2.445  0.01449 *  
factor(Gamble.Num)7   1.42100    0.26770   5.308 1.11e-07 ***
factor(Gamble.Num)8   0.35620    0.28268   1.260  0.20765    
factor(Gamble.Num)9  -0.51138    0.32379  -1.579  0.11425    
factor(Gamble.Num)10  2.10754    0.27298   7.720 1.16e-14 ***
factor(Gamble.Num)11  0.28248    0.28496   0.991  0.32154    
factor(Gamble.Num)12 -1.02908    0.36760  -2.799  0.00512 ** 
factor(Gamble.Num)13  2.49612    0.28133   8.873  &lt; 2e-16 ***
factor(Gamble.Num)14  1.72839    0.26867   6.433 1.25e-10 ***
factor(Gamble.Num)15  0.08524    0.29204   0.292  0.77039    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2649.2  on 2249  degrees of freedom
Residual deviance: 2096.4  on 2233  degrees of freedom
AIC: 2130.4

Number of Fisher Scoring iterations: 5


## GLMER ""Random-Effects"" Model##
summary(glmer(Would.Play ~ Response.Scale + (1|Gamble.Num), family=""binomial"", 
data=analysis.0.data))
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
[glmerMod]
 Family: binomial  ( logit )
Formula: Would.Play ~ Response.Scale + (1 | Gamble.Num)
   Data: analysis.0.data

     AIC      BIC   logLik deviance df.resid 
  2169.3   2192.1  -1080.6   2161.3     2246 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.9011 -0.5461 -0.3522  0.5439  4.6708 

Random effects:
 Groups     Name        Variance Std.Dev.
 Gamble.Num (Intercept) 1.291    1.136   
Number of obs: 2250, groups:  Gamble.Num, 15

Fixed effects:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)       -0.90254    0.30722  -2.938  0.00331 ** 
Response.ScaleRI  -0.06682    0.12707  -0.526  0.59897    
Response.ScaleABR -0.90170    0.13727  -6.569 5.07e-11 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Rs.SRI
Rspns.SclRI -0.202       
Rspns.ScABR -0.183  0.456
</code></pre>

<p>Thanks!</p>
"
"0.168867126224467","0.2013861800064","151079","<p>I have a data set of 2430 observations, with a binomial dependent variable, 3 categorical fixed effects and 2 categorical random effects (item and subject). I want to to a mixed effects model using glmer. Here is what I entered into R:</p>

<pre><code>modelall&lt;- glmer(moodR ~ group*context*condition + (1|subject) + ``(1|item), data=RprodHSNS, family=""binomial"")`
</code></pre>

<p>I then get the following warnings:</p>

<pre><code>Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.02081 (tol = 0.001, component 11)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
- Rescale variables?`
</code></pre>

<p>This is what my summary looks like:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
Family: binomial  ( logit )
Formula: moodR ~ group * context * condition + (1 | subject) + (1 | item)
Data: RprodHSNS`


AIC      BIC   logLik deviance df.resid
1400.0   1479.8   -686.0   1372.0     2195 `

Scaled residuals: 
Min      1Q  Median      3Q     Max 
-8.0346 -0.2827 -0.0152  0.2038 20.6578 `

Random effects:
Groups  Name        Variance Std.Dev.
item    (Intercept) 1.475    1.215   
subject (Intercept) 1.900    1.378   
Number of obs: 2209, groups:  item, 54; subject, 45
Fixed effects:`
Estimate Std. Error z value Pr(&gt;|z|)`                             
(Intercept)                -0.61448   42.93639  -0.014 0.988582  
group1                     -1.29254   42.93612  -0.030 0.975984    
context1                    0.09359   42.93587   0.002 0.998261   
context2                   -0.77262    0.22894  -3.375 0.000739***
condition1                  4.99219   46.32672   0.108 0.914186
group1:context1            -0.17781   42.93585  -0.004 0.996696
group1:context2            -0.10551    0.09925  -1.063 0.287741
group1:condition1          -3.07516   46.32653  -0.066 0.947075
context1:condition1        -3.47541   46.32648  -0.075 0.940199
context2:condition1        -0.07293    0.22802  -0.320 0.749087
group1:context1:condition1  2.47882   46.32656   0.054 0.957328
group1:context2:condition1  0.30360    0.09900   3.067 0.002165 **

---

Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
Correlation of Fixed Effects:
            (Intr) group1 cntxt1 cntxt2 cndtn1 grp1:cnt1 grp1:2 grp1:cnd1 cnt1:1 cnt2:1 g1:1:1
group1      -1.000                                                                            
context1    -1.000  1.000                                                                
context2     0.001  0.000 -0.001                                                              
condition1  -0.297  0.297  0.297  0.000                                                       
grp1:cntxt1  1.000 -1.000 -1.000  0.001 -0.297                                                
grp1:cntxt2  0.001  0.000  0.000 -0.123  0.000  0.000                                       
grp1:cndtn1  0.297 -0.297 -0.297 -0.001 -1.000  0.297     0.000                               
cntxt1:cnd1  0.297 -0.297 -0.297 -0.001 -1.000  0.297     0.001  1.000                        
cntxt2:cnd1  0.000  0.000 -0.001  0.011  0.001  0.000    -0.197 -0.001    -0.001              
grp1:cnt1:1 -0.297  0.297  0.297  0.001  1.000 -0.297    -0.001 -1.000    -1.000  0.001       
grp1:cnt2:1  0.000  0.000  0.001 -0.198  0.000 -0.001     0.252  0.000     0.001 -0.136  0.000
</code></pre>

<p>Extremely high p-values, which does not seem to be possible. </p>

<p>In a previous post I read that one of the problems could be fixed by increasing the amount of iterations by inserting this bit in the command: glmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun = 100000))</p>

<p>So here's the new command:</p>

<pre><code>modelall&lt;- glmer(moodR ~ group*context*condition + (1|subject) + (1|item), data=RprodHSNS, family=""binomial"", glmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun = 100000)))
</code></pre>

<p>I get one less warning, but the other one is still there:</p>

<pre><code>&gt; Warning message:
In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.005384 (tol = 0.001, component 7)
</code></pre>

<p>The summary also still looks weird:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: moodR ~ group * context * condition + (1 | subject) + (1 | item)
   Data: RprodHSNS
Control: glmerControl(optimizer = ""bobyqa"", optCtrl = list(maxfun = 1e+05))`

AIC      BIC   logLik deviance df.resid 
1400.0   1479.8   -686.0   1372.0     2195

Scaled residuals: 
Min      1Q  Median      3Q     Max 
-8.0334 -0.2827 -0.0152  0.2038 20.6610 

Random effects:
Groups  Name        Variance Std.Dev.
item    (Intercept) 1.474    1.214   
subject (Intercept) 1.901    1.379   
Number of obs: 2209, groups:  item, 54; subject, 45

Fixed effects:
                        Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                -0.64869   26.29368  -0.025 0.980317    
group1                     -1.25835   26.29352  -0.048 0.961830    
context1                    0.12772   26.29316   0.005 0.996124    
context2                   -0.77265    0.22886  -3.376 0.000735 ***
condition1                  4.97325   22.80050   0.218 0.827335    
group1:context1            -0.21198   26.29303  -0.008 0.993567    
group1:context2            -0.10552    0.09924  -1.063 0.287681    
group1:condition1          -3.05629   22.80004  -0.134 0.893365    
context1:condition1        -3.45656   22.80017  -0.152 0.879500    
context2:condition1        -0.07305    0.22794  -0.320 0.748612    
group1:context1:condition1  2.45996   22.80001   0.108 0.914081    
group1:context2:condition1  0.30347    0.09899   3.066 0.002172 ** 

---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
        (Intr) group1 cntxt1 cntxt2 cndtn1 grp1:cnt1 grp1:2 grp1:cnd1 cnt1:1 cnt2:1 g1:1:1
group1      -1.000                                                                            
context1    -1.000  1.000                                                                     
context2     0.000  0.000  0.000                                                              
condition1   0.123 -0.123 -0.123 -0.001                                                       
grp1:cntxt1  1.000 -1.000 -1.000  0.001  0.123                                                
grp1:cntxt2  0.001  0.000  0.000 -0.123  0.001  0.000                                         
grp1:cndtn1 -0.123  0.123  0.123  0.000 -1.000 -0.123    -0.001                               
cntxt1:cnd1 -0.123  0.123  0.123  0.000 -1.000 -0.123     0.000  1.000                        
cntxt2:cnd1  0.000  0.000  0.000  0.011 -0.001  0.000    -0.197  0.001     0.001              
grp1:cnt1:1  0.123 -0.123 -0.123  0.000  1.000  0.123     0.000 -1.000    -1.000 -0.001      
grp1:cnt2:1  0.000 -0.001  0.001 -0.198  0.001 -0.001     0.252 -0.001     0.000 -0.136  0.000
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<p>Does anyone have an idea what I can do to solve this? Or tell me what this warning even means? Please explain in a way that an R-newbie like myself can understand!</p>

<p>Any help is much appreciated!</p>
"
"0.135302018298348","0.13201967239689","153547","<p>I have a very large data set with repeated measurements of same blood value (co) (1 to 7 measurements per patient). Each measurement is coupled with time which is the time interval between surgical operation and blood level measurement. </p>

<p>My aim is to show that this blood value correlates positively with time.</p>

<p>Blood level measurements are highly skewed to <em>right</em> and hence I am using a log-transformation and linear mixed effect regression model (lmer in lme4 package).</p>

<p>I have constructed a null model: </p>

<pre><code>fit1&lt;-(lmer(lgco~(1|id),data=ASR))
</code></pre>

<p>Model 2 includes time as independent variable:</p>

<pre><code>fit2&lt;-(lmer(lgco~time+(1|id),data=ASR))
</code></pre>

<p>Id is the patient number in th dataset.</p>

<p>By using the anova() function I see that fit2 is significantly better than fit1:</p>

<pre><code>&gt; anova(fit1,fit2)
refitting model(s) with ML (instead of REML)

Data: ASR
Models:
fit1: lgco ~ (1 | id)
fit2: lgco ~ time + (1 | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit1  3 342.77 357.50 -168.39   336.77                             
fit2  4 320.64 340.27 -156.32   312.64 24.135      1  8.983e-07 ***
</code></pre>

<p>However I have other data which suggests that the correlation between time and blood value might even more profound, for example quadratic. This would be Model 3.</p>

<p>I tried the following: first I took the square root of the blood value and after that I made the transformation using log.</p>

<pre><code>fit3&lt;-(lmer(lgsqrtco~time+(1|id),data=ASR))
</code></pre>

<p>My question is that can I compare models 2 and 3 in anyway now after the dependent variable has two different transformations in these models. In fit1 and fit2 the transformation is identical, only the independent is added. I assume that with different dependent variable transformation the use of anova() is not allowed: </p>

<pre><code>anova(fit2,fit3)
refitting model(s) with ML (instead of REML)
Data: ASR
Models:
fit2: lgco ~ time + (1 | id)
fit3: lgsqrtco ~ time + (1 | id)
     Df      AIC      BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit2  4   320.64   340.27 -156.32   312.64                             
fit3  4 -1065.66 -1046.03  536.83 -1073.66 1386.3      0  &lt; 2.2e-16 ***
</code></pre>
"
"0.118389266011054","0.13201967239689","153611","<p>I'm revising a paper on pollination, where the data are binomially distributed (fruit matures or does not). So I used <code>glmer</code> with one random effect (individual plant) and one fixed effect (treatment). A reviewer wants to know whether plant had an effect on fruit set -- but I'm having trouble interpreting the <code>glmer</code> results.</p>

<p>I've read around the web and it seems there can be issues with directly comparing <code>glm</code> and <code>glmer</code> models, so I'm not doing that. I figured the most straightforward way to answer the question would be to compare the random effect variance (1.449, below) to the total variance, or the variance explained by treatment. But how do I calculate these other variances? They don't seem to be included in the output below. I read something about residual variances not being included for binomial <code>glmer</code> -- how do I interpret the relative importance of the random effect?</p>

<pre><code>&gt; summary(exclusionM_stem)
Generalized linear mixed model fit by maximum likelihood (Laplace
  Approximation) [glmerMod]
 Family: binomial  ( logit )
Formula: cbind(Fruit_1, Fruit_0) ~ Treatment + (1 | PlantID)

     AIC      BIC   logLik deviance df.resid 
   125.9    131.5    -59.0    117.9       26 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.0793 -0.8021 -0.0603  0.6544  1.9216 

Random effects:
 Groups  Name        Variance Std.Dev.
 PlantID (Intercept) 1.449    1.204   
Number of obs: 30, groups:  PlantID, 10

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)  -0.5480     0.4623  -1.185   0.2359   
TreatmentD   -1.1838     0.3811  -3.106   0.0019 **
TreatmentN   -0.3555     0.3313  -1.073   0.2832   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
           (Intr) TrtmnD
TreatmentD -0.338       
TreatmentN -0.399  0.509
</code></pre>
"
"0.234349970059352","0.228664780190012","154263","<p>I'm trying to plot confidence intervals for linear mixed effects models trained with lme4 and lmerTest in R. I am using <a href=""https://drive.google.com/file/d/0B_jcmrV1IADGX1BPaVU1TnR4anM/view?usp=sharing"" rel=""nofollow"" title=""Zip file on Google Docs"">this data file</a>, which I've shared via Google Drive.</p>

<p>Here is my trained model. The data consists of 8 subjects (SID) and 580 items per subject (UID).</p>

<pre><code>&gt; df &lt;- readRDS(file=""data.Rda"")
&gt; summary(df)
      UID            SID             Y                  X          
 U1     :   8   H1     : 580   Min.   :-1.75000   Min.   :0.00000  
 U10    :   8   H2     : 580   1st Qu.:-0.13330   1st Qu.:0.00000  
 U100   :   8   H3     : 580   Median :-0.02470   Median :0.08054  
 U101   :   8   H4     : 580   Mean   :-0.08563   Mean   :0.14070  
 U102   :   8   H5     : 580   3rd Qu.: 0.00000   3rd Qu.:0.21053  
 U103   :   8   H6     : 580   Max.   : 0.50000   Max.   :1.20000  
 (Other):4592   (Other):1160 
&gt; my.model &lt;- lmer(Y ~ X + (1|UID) + (1|SID), data=df)
&gt; summary(my.model)
Linear mixed model fit by REML 
t-tests use  Satterthwaite approximations to degrees of freedom ['merModLmerTest']
Formula: Y ~ X + (1 | UID) + (1 | SID)
   Data: df

REML criterion at convergence: -10980.2

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-16.4681  -0.2699   0.0042   0.3194   6.7467 

Random effects:
 Groups   Name        Variance  Std.Dev.
 UID      (Intercept) 4.573e-03 0.067624
 SID      (Intercept) 2.185e-06 0.001478
 Residual             4.109e-03 0.064099
Number of obs: 4640, groups:  UID, 580; SID, 8

Fixed effects:
              Estimate Std. Error         df t value Pr(&gt;|t|)    
(Intercept)  8.501e-04  3.255e-03  3.056e+02   0.261    0.794    
X           -6.146e-01  8.861e-03  1.644e+03 -69.362   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
  (Intr)
X -0.383
</code></pre>

<p>I've tried generating confidence intervals for <code>X</code> using a number of approaches, to no success. <strong>With <code>lsmeans</code>, I don't get any output</strong>.</p>

<pre><code>&gt; lsmeans(my.model)
Least Squares Means table:
     Estimate Standard Error DF t-value Lower CI Upper CI p-value
</code></pre>

<p>I can generate confidence intervals using <code>confint</code> with Wald statistics, <strong>but using the default method runs indefinitely</strong>.</p>

<pre><code>&gt; confint(my.model, method=""Wald"")
                   2.5 %       97.5 %
(Intercept) -0.005530357  0.007230525
X           -0.631989857 -0.597255116
&gt; confint(my.model) # This runs indefinitely
Computing profile confidence intervals ...

&gt;
&gt; effect(c(""X""), my.model) # This also runs indefinitely
</code></pre>

<p>I have no problem getting confidence intervals on the example datasets.</p>

<pre><code>&gt; m1 &lt;- lmer(Informed.liking ~ Gender*Information +(1|Consumer), data=ham)
&gt; lsmeans(m1)
Least Squares Means table:
                        Gender Information Estimate Standard Error  DF t-value Lower CI Upper CI p-value    
Gender  1                    1          NA    5.854          0.183  79    32.0     5.49     6.22  &lt;2e-16 ***
Gender  2                    2          NA    5.609          0.185  79    30.3     5.24     5.98  &lt;2e-16 ***
Information  1              NA           1    5.632          0.155 154    36.4     5.33     5.94  &lt;2e-16 ***
Information  2              NA           2    5.831          0.155 154    37.7     5.53     6.14  &lt;2e-16 ***
Gender:Information  1 1      1           1    5.707          0.218 154    26.2     5.28     6.14  &lt;2e-16 ***
Gender:Information  2 1      2           1    5.556          0.220 154    25.2     5.12     5.99  &lt;2e-16 ***
Gender:Information  1 2      1           2    6.000          0.218 154    27.6     5.57     6.43  &lt;2e-16 ***
Gender:Information  2 2      2           2    5.662          0.220 154    25.7     5.23     6.10  &lt;2e-16 ***
</code></pre>

<p>Here is my session info:</p>

<pre><code>&gt; sessionInfo()
R version 3.1.1 (2014-07-10)
Platform: x86_64-w64-mingw32/x64 (64-bit)

locale:
[1] LC_COLLATE=English_United States.1252  LC_CTYPE=English_United States.1252    LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C                           LC_TIME=English_United States.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] effects_3.0-3   pbkrtest_0.4-2  lmerTest_2.0-20 lme4_1.1-7      Rcpp_0.11.6     Matrix_1.2-0   

loaded via a namespace (and not attached):
 [1] acepack_1.3-3.3     bitops_1.0-6        caTools_1.17.1      cluster_2.0.1       colorspace_1.2-6    digest_0.6.8        foreign_0.8-63     
 [8] Formula_1.2-1       gdata_2.16.1        ggplot2_1.0.1       gplots_2.17.0       grid_3.1.1          gridExtra_0.9.1     gtable_0.1.2       
[15] gtools_3.4.2        Hmisc_3.16-0        KernSmooth_2.23-14  lattice_0.20-29     latticeExtra_0.6-26 magrittr_1.5        MASS_7.3-40        
[22] minqa_1.2.4         munsell_0.4.2       nlme_3.1-120        nloptr_1.0.4        nnet_7.3-9          numDeriv_2014.2-1   parallel_3.1.1     
[29] plyr_1.8.2          proto_0.3-10        RColorBrewer_1.1-2  reshape2_1.4.1      rpart_4.1-9         scales_0.2.4        splines_3.1.1      
[36] stringi_0.4-1       stringr_1.0.0       survival_2.38-1     tools_3.1.1
</code></pre>

<p>Can someone explain why <code>lsmeans</code> won't work on my model? Thanks in advance.</p>

<p><strong>Update:</strong> Thanks to @aosmith, I now understand that <code>lsmeans</code> only displays confidence intervals on factors. So here's a related question.</p>

<p>I also tried computing confidence intervals on the fixed effects using the <code>effects</code> package. However, this seems to run indefinitely.</p>

<pre><code>lvls &lt;- c(1:10) / 10
Effect(c(""X""), my.model, xlevels=lvls)
</code></pre>

<p>I don't think it's related to the fact that <code>X</code> is numeric. I tried the following example, and I got </p>

<pre><code>str(mtcars)
m &lt;- lmer(mpg ~ 1 + wt + hp + (1 + wt |gear), data=mtcars)
str(m)
Effect(""wt"", m)
&gt; str(mtcars)
'data.frame':   32 obs. of  11 variables:
 $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...
     $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...
 $ disp: num  160 160 108 258 360 ...
     $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...
 $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...
     $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...
 $ qsec: num  16.5 17 18.6 19.4 17 ...
     $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...
 $ am  : num  1 1 1 0 0 0 0 0 0 0 ...
     $ gear: num  4 4 4 3 3 3 3 4 4 4 ...
 $ carb: num  4 4 1 1 2 1 4 2 2 4 ...
&gt; m &lt;- lmer(mpg ~ 1 + wt + hp + (1 + wt |gear), data=mtcars)
&gt; Effect(""wt"", m)

 wt effect
wt
       2        3        4        5 
24.59905 20.20403 15.80902 11.41400
</code></pre>

<p>Any suggestions why it won't converge in my dataset?</p>
"
"0.143509461970482","0.140028008402801","154293","<p>I've been using lmer's confint procedure to compute bootstrapped confidence intervals for random effects.  I noticed that extracting the theta values using ""getME"" produces estimates that are slightly different from what the summary function provides.  Using the carrots dataset, here's the code:</p>

<pre><code>m &lt;- lmer(Preference ~ sens2+Homesize+(1+sens2|Consumer), data=carrots)

confint(m, method=""boot"", parallel=""multicore"", ncpus=4)
</code></pre>

<p>Now the summary function produces the following (some lines omitted):</p>

<pre><code>REML criterion at convergence: 3748.9

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.5322 -0.5571  0.0308  0.6297  2.8552 

Random effects:
 Groups   Name        Variance Std.Dev. Corr
 Consumer (Intercept) 0.195168 0.44178      
          sens2       0.002779 0.05271  0.18
 Residual             1.070441 1.03462      
Number of obs: 1233, groups:  Consumer, 103

Fixed effects:
              Estimate Std. Error         df t value Pr(&gt;|t|)    
(Intercept)   4.910021   0.070560 101.350000  69.586  &lt; 2e-16 ***
sens2         0.070675   0.009545 102.010000   7.404 3.89e-11 ***
Homesize3    -0.249039   0.105374 100.960000  -2.3 
</code></pre>

<p>Extracting beta (the fixed effects) produces identical estimates to those in the summary function.  But not so with theta:</p>

<pre><code>getME(m,""theta"")

Consumer.(Intercept) Consumer.sens2.(Intercept) 
         0.426995160                0.009354308 
            Consumer.sens2 
               0.050084579 
</code></pre>

<p>I suspect that the discrepancy has something to do with the Cholesky parameterization of the random components as described <a href=""http://www.inside-r.org/packages/cran/lme4/docs/isREML"" rel=""nofollow"">here</a>.  Is there a way to extract the random estimates as they appear in the summary?  For practical purposes, I'd like to extract the random estimates in the same order as produced by the confint procedure, which getME does:</p>

<pre><code>                                     2.5 %      97.5 %
sd_(Intercept)|Consumer         0.35051177  0.52851297
cor_sens2.(Intercept)|Consumer -0.32121878  0.65130350
sd_sens2|Consumer               0.02096827  0.07710528
sigma                           0.98997624  1.08073053
(Intercept)                     4.76700314  5.04259381
sens2                           0.05310008  0.08821807
Homesize3                      -0.45809294 -0.02700430
</code></pre>

<p>That way, one could easily create a middle column, one that contains the estimate, with the upper and lower bounds on either side.  </p>

<p>On another note, confint uses standard deviation and correlations for the intervals.  Is there an option for it to use the unstandardized (variances/covariances) estimates?</p>

<p>Thanks in advance.</p>

<p>Joe</p>
"
"0.136145029683612","0.147602480923349","154700","<p>Are following 2 models really the same? </p>

<pre><code>&gt; library(lme4)
&gt; library(lmerTest)
&gt; lmod = lmer(Reaction ~ Days + (Days|Subject), data=sleepstudy)    
&gt; summary(lmod)
Linear mixed model fit by REML 
t-tests use  Satterthwaite approximations to degrees of freedom ['merModLmerTest']
Formula: Reaction ~ Days + (Days | Subject)
   Data: sleepstudy

REML criterion at convergence: 1743.6

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.9536 -0.4634  0.0231  0.4634  5.1793 

Random effects:
 Groups   Name        Variance Std.Dev. Corr
 Subject  (Intercept) 612.09   24.740       
          Days         35.07    5.922   0.07
 Residual             654.94   25.592       
Number of obs: 180, groups:  Subject, 18

Fixed effects:
            Estimate Std. Error      df t value             Pr(&gt;|t|)    
(Intercept)  251.405      6.825  17.000  36.838 &lt; 0.0000000000000002 ***
Days          10.467      1.546  17.000   6.771           0.00000326 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
     (Intr)
Days -0.138
</code></pre>

<p>And: </p>

<pre><code>&gt; laov = aov(Reaction ~ Days + Error(Subject/Days), data=sleepstudy)   
&gt; summary(laov)

Error: Subject
          Df Sum Sq Mean Sq F value Pr(&gt;F)
Residuals 17 250618   14742               

Error: Subject:Days
          Df Sum Sq Mean Sq F value     Pr(&gt;F)      
Days       1 162703  162703   45.85 0.00000326 ***  
Residuals 17  60322    3548                       
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Error: Within
           Df Sum Sq Mean Sq F value Pr(&gt;F)
Residuals 144  94312   654.9               
</code></pre>

<p>Both are showing similar P values for Days variable. What is the difference between two methods?</p>
"
"0.117174985029676","0.114332390095006","160445","<p>I have computed GLMM using glmer in R. My response variable is species richness and my explanatory variable is grazing treatment (with three categories: cattle, sheep and ungrazed). In the model I have included site as a fixed variable and also a new object with the same number of variations as I have to attempt to account for underdispersal (<code>obs</code>):</p>

<pre><code>model2&lt;-glmer(VegRichness~Grazing+(1|Site)+(1|obs),family=""poisson"",data=veg.rich)
</code></pre>

<p>My output is below and the questions I have about it are:</p>

<p>How do I interpret the fixed effects section?
Cattle grazing seems to be missing in the oputput, is this because it is somehow incorporated into the intercept?</p>

<pre><code>&gt; summary(model2)

Generalized linear mixed model fit by maximum likelihood (Laplace
  Approximation) [glmerMod]

Family: poisson  ( log ) 

Formula: VegRichness ~ Grazing + (1 | Site) + (1 | obs)
   Data: veg.rich


     AIC      BIC   logLik deviance df.resid 
    178.8    185.2    -84.4    168.8       22 

Scaled residuals: 

Min..........           1Q............           Median....       3Q.........        Max

-1.4936...      -0.5698.....       -0.1928...      0.4923...   1.3646 

Random effects:

 Groups  ... Name......        Variance..... Std.Dev.

 obs.........      (Intercept).. 0.00000....  0.0000 

 Site.........     (Intercept).. 0.03596....  0.1896

Number of obs: 27, groups:  obs, 27; Site, 3

Fixed effects:
                .......Estimate.... Std. Error..... z value... Pr(&gt;|z|)    
(Intercept)............      3.55358.......    0.12309.......  28.869.....  &lt; 2e-16 ***                                                                 
GrazingSheep......     0.01242......    0.07876........   0.158.......  0.87467    
GrazingUngrazed -0.27526.....    0.08503........  -3.237......  0.00121 ** 

---
Signif. codes:  0 x***x 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr)....GrzngS

GrazingShe......................... -0.322       
GrzngUngrzd...................... -0.298...  0.466
</code></pre>
"
"0.0828552264999161","0.0808452083454443","160714","<p>This is a bit philosophical: I have mutiple responses at multiple sites in multiple years.
Can I legitimately nest the random effect Site within Year, or must Site and Year be crossed effects? Given Year is temporal, I argue that Site is not strictly the same Site in the next year. I'd like to fit an AR(1) model within each Site and Year, but I don't how to do this if the random effects are crossed. Example:</p>

<pre><code>mod1 &lt;- lme(resp ~1,random=~1|Year/Site,correlation=corAR1(value=0.1,form=~autor|Year/Site),data=dat)
</code></pre>

<p>or</p>

<pre><code>mod2 &lt;- lmer(resp~(1|Year)+(1|Site),data=dat)
</code></pre>
"
"0.10696563746014","0.104370715180858","160943","<p>Reading this <a href=""http://stats.stackexchange.com/a/78830/67822"">post</a> by @gung brought me to try to reproduce his superb illustrations, and led ultimately to question something I had read or heard, but that I'd like to understand more intuitively: Why is an OLS <code>lm</code> controlling for a correlated variable better (can I say 'better' tentatively?) than a mixed-effects model with different intersects and slopes?</p>

<p>Here's the toy example, again trying to parallel the post quoted above:</p>

<p>First the data:</p>

<pre><code>set.seed(0)    
x1 &lt;- c(rnorm(10,3,1),rnorm(10,5,1),rnorm(10,7,1))
x2 &lt;- rep(c(1:3),each=10)
    y1 &lt;- 2  -0.8 * x1[1:10] + 8 * x2[1:10] +rnorm(10)
    y2 &lt;- 6  -0.8 * x1[11:20] + 8 * x2[11:20] +rnorm(10)
    y3 &lt;- 8  -0.8 * x1[21:30] + 8 * x2[21:30] +rnorm(10)
y &lt;- c(y1, y2, y3)
</code></pre>

<p>And the different models:</p>

<pre><code>library(lme4)

fit1 &lt;- lm(y ~ x1)
fit2 &lt;- lm(y ~ x1 + x2)
fit3 &lt;- lmer(y ~ x1 + (1|x2))
fit4 &lt;- lmer(y ~ x1|x2, REML=F)
</code></pre>

<p>Comparing  Akaike information criterion (AIC) between models:</p>

<pre><code>AIC(fit1, fit2, fit3, fit4)

     df      AIC
     df      AIC
fit1  3 184.5330
fit2  4  97.6568
fit3  4 112.0120
fit4  5 114.8401
</code></pre>

<p>So it seems that the best model is <code>lm(y ~ x1 + x2)</code>, which I guess make sense given the strong correlation between <code>x1</code> and <code>x2</code> <code>cor(x1,x2) [1] 0.8619565</code>.</p>

<p>But the question is, What is the intuition behind this behavior, when the mixed model with varying intercepts and slopes seems to result in coefficients that fit the data beautifully?</p>

<pre><code>coef(lmer(y ~ x1|x2))
$x2

      x1       (Intercept)
1 -1.1595730    11.37746
2 -0.2586303    19.38601
3  0.2829754    24.20038

library(lattice)    
xyplot(y ~ x1, groups = x2, pch=19,
           panel = function(x, y,...) {
             panel.xyplot(x, y,...);
             panel.abline(a=coef(fit4)$x2[1,2], b=coef(fit4)$x2[1,1],lty=2,col='blue');
             panel.abline(a=coef(fit4)$x2[2,2], b=coef(fit4)$x2[2,1],lty=2,col='magenta');
             panel.abline(a=coef(fit4)$x2[3,2], b=coef(fit4)$x2[3,1],lty=2,col='green')
           })
</code></pre>

<p><img src=""http://i.stack.imgur.com/NYweu.png"" alt=""enter image description here""></p>

<p>I do realize that the graphical fit of the bivariate OLS can look pretty good as well:</p>

<pre><code>library(scatterplot3d)
plot1 &lt;- scatterplot3d(x1,x2,y, type='h', pch=16,
              highlight.3d=T)
plot1$plane3d(fit2)
</code></pre>

<p><img src=""http://i.stack.imgur.com/Jt2bz.png"" alt=""enter image description here""></p>

<p>... and I don't know if this invalidates the question. </p>
"
"0.0828552264999161","0.0808452083454443","163325","<pre><code>library(lme4)
fm1 &lt;- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)
</code></pre>

<p>The notation <code>(Days | Subject)</code> says to allow the intercept and <code>Days</code> to vary randomly for each level of <code>Subject</code> .</p>

<p>Can you please explain me the result of the following commands ? </p>

<pre><code>attr(summary(fm1)$varcor$Subject,""stddev"")
(Intercept)        Days 
 24.740448    5.922133 

c(sd(ranef(fm1)$Subject[,1]),sd(ranef(fm1)$Subject[,2]))
[1] 21.595943  5.455217

summary(fm1)$sigma
[1] 25.59182

residuals(summary(fm1))

sd(residuals(summary(fm1)))
[1] 0.9183965
</code></pre>

<p><strong>What is the INTERPRETATION of the results found from various commands?</strong> </p>

<p>That is , if one asks me what is the meaning of the results that you have found from <code>sd(ranef(fm1)$Subject[,1])</code>  and <code>attr(summary(fm1)$varcor$Subject,""stddev"")[1]</code> ? Both are standard deviation of <code>Intercept</code> but of course there is difference between these two results . But I don't know what is this ?</p>

<p>In <code>?getMe</code> , it is said that from <code>summary(fm1)$sigma</code> , we found residual standard error . But why doesn't the result match with <code>sd(residuals(summary(fm1)))</code> ?</p>

<p>Also , In <code>summary(fm1)$varcor</code> there is value 0.066 under the column <code>Corr</code> . Does it mean correlation between two random effects <code>(Intercept)</code> and <code>Days</code> is 0.066 ?</p>

<p>Any help is appreciated . Thank you .</p>
"
"0.174916589277601","0.206604421327247","164457","<p>I have seen questions about this on this forum, and I have also asked it myself in a previous post but I still haven't been able to solve my problem. Therefore I am trying again, formulating the question as clearly as I can this time, with as much detailed information as possible. </p>

<p>My data set has a binomial dependent variable, 3 categorical fixed effects and 2 categorical random effects (item and subject). I am using a mixed effects model using glmer. Here is what I entered in R:</p>

<pre><code>modelall&lt;- glmer(moodR ~ group*context*condition + (1|subject) + ``(1|item), data=RprodHSNS, family=""binomial"")`
</code></pre>

<p>I get 2 warnings:</p>

<pre><code>Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.02081 (tol = 0.001, component 11)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
- Rescale variables?`
</code></pre>

<p>My summary looks like this:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
Family: binomial  ( logit )
Formula: moodR ~ group * context * condition + (1 | subject) + (1 | item)
Data: RprodHSNS`


AIC      BIC   logLik deviance df.resid
1400.0   1479.8   -686.0   1372.0     2195 `

Scaled residuals: 
Min      1Q  Median      3Q     Max 
-8.0346 -0.2827 -0.0152  0.2038 20.6578 `

Random effects:
Groups  Name        Variance Std.Dev.
item    (Intercept) 1.475    1.215   
subject (Intercept) 1.900    1.378   
Number of obs: 2209, groups:  item, 54; subject, 45
Fixed effects:`
Estimate Std. Error z value Pr(&gt;|z|)`                             
(Intercept)                -0.61448   42.93639  -0.014 0.988582  
group1                     -1.29254   42.93612  -0.030 0.975984    
context1                    0.09359   42.93587   0.002 0.998261   
context2                   -0.77262    0.22894  -3.375 0.000739***
condition1                  4.99219   46.32672   0.108 0.914186
group1:context1            -0.17781   42.93585  -0.004 0.996696
group1:context2            -0.10551    0.09925  -1.063 0.287741
group1:condition1          -3.07516   46.32653  -0.066 0.947075
context1:condition1        -3.47541   46.32648  -0.075 0.940199
context2:condition1        -0.07293    0.22802  -0.320 0.749087
group1:context1:condition1  2.47882   46.32656   0.054 0.957328
group1:context2:condition1  0.30360    0.09900   3.067 0.002165 **

---

Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
Correlation of Fixed Effects:
            (Intr) group1 cntxt1 cntxt2 cndtn1 grp1:cnt1 grp1:2 grp1:cnd1 cnt1:1 cnt2:1 g1:1:1
group1      -1.000                                                                            
context1    -1.000  1.000                                                                
context2     0.001  0.000 -0.001                                                              
condition1  -0.297  0.297  0.297  0.000                                                       
grp1:cntxt1  1.000 -1.000 -1.000  0.001 -0.297                                                
grp1:cntxt2  0.001  0.000  0.000 -0.123  0.000  0.000                                       
grp1:cndtn1  0.297 -0.297 -0.297 -0.001 -1.000  0.297     0.000                               
cntxt1:cnd1  0.297 -0.297 -0.297 -0.001 -1.000  0.297     0.001  1.000                        
cntxt2:cnd1  0.000  0.000 -0.001  0.011  0.001  0.000    -0.197 -0.001    -0.001              
grp1:cnt1:1 -0.297  0.297  0.297  0.001  1.000 -0.297    -0.001 -1.000    -1.000  0.001       
grp1:cnt2:1  0.000  0.000  0.001 -0.198  0.000 -0.001     0.252  0.000     0.001 -0.136  0.000
</code></pre>

<p>Extremely high p-values, which does not seem to be possible. </p>

<p>In a previous post I read that one of the problems could be fixed by increasing the amount of iterations by inserting the following in the command: glmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun = 100000))</p>

<p>So that's what I did:</p>

<pre><code>modelall&lt;- glmer(moodR ~ group*context*condition + (1|subject) + (1|item), data=RprodHSNS, family=""binomial"", glmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun = 100000)))
</code></pre>

<p>Now, the second warning is gone, but the first one is still there:</p>

<pre><code>&gt; Warning message:
In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.005384 (tol = 0.001, component 7)
</code></pre>

<p>The summary also still looks odd:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: moodR ~ group * context * condition + (1 | subject) + (1 | item)
   Data: RprodHSNS
Control: glmerControl(optimizer = ""bobyqa"", optCtrl = list(maxfun = 1e+05))`

AIC      BIC   logLik deviance df.resid 
1400.0   1479.8   -686.0   1372.0     2195

Scaled residuals: 
Min      1Q  Median      3Q     Max 
-8.0334 -0.2827 -0.0152  0.2038 20.6610 

Random effects:
Groups  Name        Variance Std.Dev.
item    (Intercept) 1.474    1.214   
subject (Intercept) 1.901    1.379   
Number of obs: 2209, groups:  item, 54; subject, 45

Fixed effects:
                        Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                -0.64869   26.29368  -0.025 0.980317    
group1                     -1.25835   26.29352  -0.048 0.961830    
context1                    0.12772   26.29316   0.005 0.996124    
context2                   -0.77265    0.22886  -3.376 0.000735 ***
condition1                  4.97325   22.80050   0.218 0.827335    
group1:context1            -0.21198   26.29303  -0.008 0.993567    
group1:context2            -0.10552    0.09924  -1.063 0.287681    
group1:condition1          -3.05629   22.80004  -0.134 0.893365    
context1:condition1        -3.45656   22.80017  -0.152 0.879500    
context2:condition1        -0.07305    0.22794  -0.320 0.748612    
group1:context1:condition1  2.45996   22.80001   0.108 0.914081    
group1:context2:condition1  0.30347    0.09899   3.066 0.002172 ** 

---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
        (Intr) group1 cntxt1 cntxt2 cndtn1 grp1:cnt1 grp1:2 grp1:cnd1 cnt1:1 cnt2:1 g1:1:1
group1      -1.000                                                                            
context1    -1.000  1.000                                                                     
context2     0.000  0.000  0.000                                                              
condition1   0.123 -0.123 -0.123 -0.001                                                       
grp1:cntxt1  1.000 -1.000 -1.000  0.001  0.123                                                
grp1:cntxt2  0.001  0.000  0.000 -0.123  0.001  0.000                                         
grp1:cndtn1 -0.123  0.123  0.123  0.000 -1.000 -0.123    -0.001                               
cntxt1:cnd1 -0.123  0.123  0.123  0.000 -1.000 -0.123     0.000  1.000                        
cntxt2:cnd1  0.000  0.000  0.000  0.011 -0.001  0.000    -0.197  0.001     0.001              
grp1:cnt1:1  0.123 -0.123 -0.123  0.000  1.000  0.123     0.000 -1.000    -1.000 -0.001      
grp1:cnt2:1  0.000 -0.001  0.001 -0.198  0.001 -0.001     0.252 -0.001     0.000 -0.136  0.000
</code></pre>

<blockquote>
  <p></p>
</blockquote>

<p>What I can do to solve this? Or can anyone tell me what this warning even means? (in a way that an R-newbie like myself can understand)</p>
"
"0.117174985029676","0.114332390095006","169115","<p>I have a dataset that has measurements of resource consumption in buildings for a number of years. I am interested in the differences in resource consumption of buildings in my study area between years (as opposed to differences between individual buildings). I've fitted a Linear Mixed Model to my data with the lme4 package in R using the formula: <code>model = lmer(resource.consumption ~ year + (1|building.id))</code></p>

<p>I would like to put this into a formula or equation format that will allow those unfamiliar with R to be able to understand what is being estimated by this model. However, I am having some trouble figuring out how to go about this given that 'year' is a factor in this scenario. The summary() function gives the following output:</p>

<pre><code>Linear mixed model fit by REML ['lmerMod']
Formula: resource.consumption. ~ year + (1 | building.id)
   Data: year.comp

REML criterion at convergence: 122.8

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.1312 -0.4170 -0.0711  0.3419  5.0172 

Random effects:
 Groups      Name        Variance Std.Dev.
 building.id (Intercept) 0.07294  0.2701  
 Residual                0.04537  0.2130  
Number of obs: 368, groups:  building.id, 107

Fixed effects:
               Estimate Std. Error t value
(Intercept)     1.32746    0.05565  23.855
year2007       -0.24504    0.06029  -4.064
year2008       -0.36634    0.05817  -6.298
year2009       -0.44730    0.05551  -8.057
year2010       -0.47449    0.05391  -8.801
year2011       -0.53752    0.05524  -9.730

Correlation of Fixed Effects:
            (Intr) i.2007 i.2008 i.2009 i.2010
yr2007      -0.696                            
yr2008      -0.710  0.657                     
yr2009      -0.775  0.697  0.714              
yr2010      -0.803  0.720  0.735  0.802       
yr2011      -0.801  0.704  0.722  0.800  0.825
</code></pre>

<p>From <a href=""http://au.mathworks.com/help/stats/fitlme.html"" rel=""nofollow"">here</a> and <a href=""http://au.mathworks.com/help/stats/prepare-data-for-linear-mixed-effects-models.html"" rel=""nofollow"">here</a> I think I've narrowed my options to the following (you'll have to excuse these, they'll be messy but hopefully readable):
$$y_{im} = \beta_0 + \beta_1 year_{im} + b_{0m} +\epsilon_{im} $$
where $i$ is the # of obs., and $m$ is the grouping variable (building.id in this case)</p>

<p>OR</p>

<p>$$ y_{imj} = \beta_0 + \Sigma\beta_{1m}[year]_{im} + b_{0j}[building.id]_j + \epsilon_{imj} $$
where $i$ is the # of obs., $m$ corresponds to year, and $j$ corresponds to building.id.</p>

<p>Are either of these correct? Any help would be hugely appreciated! </p>
"
"0.143509461970482","0.140028008402801","174057","<p>This is probably an embarrassingly easy question, but where else can I turn to... </p>

<p>I'm trying to put together examples of regression with mixed effects using <code>lmer</code> {lme4}, so that I can present [R] code that automatically downloads toy datasets in Google Drive and run every instance in <a href=""http://stats.stackexchange.com/a/13173/67822"">this blockbuster post</a>. </p>

<p>And starting with the first case (i.e. <code>V1 ~ (1|V2) + V3</code>, where <code>V3</code> is a continuous variable acting as a fixed effect, and <code>V2</code> is <code>Subjects</code>, both trying to account for <code>V1</code>, a continuous DV), I was expecting to retrieve different intercepts for each one of the <code>Subjects</code> and a single slope for all of them. Yet, this was not the case consistently.</p>

<p>I don't want to bore you with the origin or meaning of the datasets below, because I'm sure most of you get the idea without much explaining. So let me show you what I get... If you're so inclined you can just copy and paste in [R]... it should work if you have {lme4} in your Environment:</p>

<h1>Expected Output:</h1>

<pre><code>politeness &lt;- read.csv(""http://www.bodowinter.com/tutorial/politeness_data.csv"")
head(politeness)

  subject   gender scenario  attitude frequency
1      F1      F        1      pol     213.3
2      F1      F        1      inf     204.5
3      F1      F        2      pol     285.1
4      F1      F        2      inf     259.7    


library(lme4)

fit &lt;- lmer(frequency ~ (1|subject) + attitude, data = politeness)

coefficients(fit)
            $subject
               (Intercept) attitudepol
            F1    241.1352   -19.37584
            F2    266.8920   -19.37584
            F3    259.5540   -19.37584
            M3    179.0262   -19.37584
            M4    155.6906   -19.37584
            M7    113.2306   -19.37584
</code></pre>

<h1>Surprising Output:</h1>

<pre><code>library(gsheet)
recall &lt;- read.csv(text = 
    gsheet2text('https://drive.google.com/open?id=1iVDJ_g3MjhxLhyyLHGd4PhYhsYW7Ob0JmaJP8MarWXU',
              format ='csv'))
head(recall)

 Subject Time Emtl_Value Recall_Rate Caffeine_Intake
1     Jim    0   Negative          54              95
2     Jim    0    Neutral          56              86
3     Jim    0   Positive          90             180
4     Jim    1   Negative          26             200

fit &lt;- lmer(Recall_Rate ~ (1|Subject) + Caffeine_Intake, data = recall)

coefficients(fit)
        $Subject
               (Intercept) Caffeine_Intake
        Jason     51.51206        0.013369
        Jim       51.51206        0.013369
        Ron       51.51206        0.013369
        Tina      51.51206        0.013369
        Victor    51.51206        0.013369
</code></pre>

<p>Here is the output of (<code>summary(fit)</code>):</p>

<pre><code>Linear mixed model fit by REML ['lmerMod']
Formula: Recall_Rate ~ (1 | Subject) + Caffeine_Intake
   Data: recall

REML criterion at convergence: 413.9

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-1.54125 -0.98422  0.04967  0.81465  1.83317 

Random effects:
 Groups   Name        Variance Std.Dev.
 Subject  (Intercept)   0.0     0.00   
 Residual             601.2    24.52   
Number of obs: 45, groups:  Subject, 5

Fixed effects:
                Estimate Std. Error t value
(Intercept)     51.51206    5.92408   8.695
Caffeine_Intake  0.01337    0.03792   0.353

Correlation of Fixed Effects:
            (Intr)
Caffen_Intk -0.787
</code></pre>

<h1>Question:</h1>

<p><strong>Why are all the Intercepts for the different subjects the same in the second example? The structure of the datasets and the <code>lmer</code> syntax appear very similar... and the boxplots don't seem to support the result:</strong></p>

<p><a href=""http://i.stack.imgur.com/xXYdS.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xXYdS.png"" alt=""enter image description here""></a></p>

<p>Thank you in advance!</p>
"
"0.15190124858318","0.148216215299981","174532","<p>I have been working on my PC to analyse my multilevel data. I am now working on a Mac and have run the same model. Some of the output is the same but some is quite different. I can't seem to work out why. Here is the model:</p>

<pre><code>&gt; loss.2 &lt;- glmer.nb(Loss_across.Chain ~ Posn.c*Valence.c + (Valence.c|mood.c/Chain), data = FinalData_forpoisson, control = glmerControl(optimizer = ""bobyqa"", check.conv.grad = .makeCC(""warning"", 0.05)))
</code></pre>

<p>On the PC I got this output: </p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: Negative Binomial(4.9852)  ( log )
Formula: Loss_across.Chain ~ Posn.c * Valence.c + (Valence.c | mood.c/Chain)
   Data: FinalData_forpoisson
Control: ..3

     AIC      BIC   logLik deviance df.resid 
  1894.7   1945.3   -936.4   1872.7      725 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.3882 -0.7225 -0.5190  0.4375  7.1873 

Random effects:
 Groups       Name        Variance  Std.Dev.  Corr
 Chain:mood.c (Intercept) 8.782e-15 9.371e-08     
              Valence.c   9.608e-15 9.802e-08 0.48
 mood.c       (Intercept) 0.000e+00 0.000e+00     
              Valence.c   1.654e-14 1.286e-07  NaN
Number of obs: 736, groups:  Chain:mood.c, 92; mood.c, 2

Fixed effects:
                 Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)      -0.19255    0.04794  -4.016 5.92e-05 ***
Posn.c           -0.61011    0.04122 -14.800  &lt; 2e-16 ***
Valence.c        -0.27372    0.09589  -2.855  0.00431 ** 
Posn.c:Valence.c  0.38043    0.08245   4.614 3.95e-06 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Posn.c Vlnc.c
Posn.c       0.491              
Valence.c    0.029 -0.090       
Psn.c:Vlnc. -0.090  0.062  0.491
</code></pre>

<p>On the Mac I got this output:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: Negative Binomial(4.9852)  ( log )
Formula: Loss_across.Chain ~ Posn.c * Valence.c + (Valence.c | mood.c/Chain)
   Data: FinalData_forpoisson
Control: ..3

     AIC      BIC   logLik deviance df.resid 
  1894.7   1945.3   -936.4   1872.7      725 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.3882 -0.7225 -0.5190  0.4375  7.1873 

Random effects:
 Groups       Name        Variance  Std.Dev.  Corr
 Chain:mood.c (Intercept) 1.242e-13 3.524e-07     
              Valence.c   4.724e-13 6.873e-07 0.98
 mood.c       (Intercept) 7.998e-16 2.828e-08     
              Valence.c   3.217e-14 1.793e-07 1.00
Number of obs: 736, groups:  Chain:mood.c, 92; mood.c, 2

Fixed effects:
                   Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)       2.947e-05  4.794e-02   0.001    1.000
Posn.c            7.441e-05  4.122e-02   0.002    0.999
Valence.c        -4.011e-05  9.589e-02   0.000    1.000
Posn.c:Valence.c -6.672e-05  8.245e-02  -0.001    0.999

Correlation of Fixed Effects:
            (Intr) Posn.c Vlnc.c
Posn.c       0.491              
Valence.c    0.029 -0.090       
Psn.c:Vlnc. -0.090  0.062  0.491
</code></pre>

<p>Does anyone know why the output might be different across the two platforms and how I might be able to get them to align?</p>
"
"0.095672974646988","0.0933520056018673","176609","<p>I have a question regarding multilevel analysis and nested effect. I have an ordinal score from a single item questionnaire from each participants in line that I would like to correlate to the GINI by States (I also have some covariates such as the mean income by States, the number of inhabitants by State, etc.). I need to compute the correlation by nesting each pp by States. The data look like this :</p>

<pre><code>NB(=pp) NBunhabit   Mean_av_incom   B_genre Gini_state  singleitem  States
46669   77156   31205                2           0,27           3      1
50842   79156   31205                1           0,37           1      2
51709   87156   31205                1           0,47           2      3
52932   97156   31205                2           0,57           3      4
53989   79156   31205                1           0,37           1      2
57486   87156   31205                1           0,47           1      3
57798   97156   31205                1           0,57           1      4
59213   77156   31205                2           0,27           1      1
</code></pre>

<p>Should the code look something like this: </p>

<pre><code>analyse1 &lt;- lmer(singleitem ~ Gini_state + B_genre + NBunhabit + Mean_av_incom + (1 | NB/States), data = data)
</code></pre>

<p>Apparently, this is different way of nesting those kind of variables, but I only know this one (idk if it's correct though). Do you know the other methods through R? What differences does it make?</p>

<p>Thanks a lot !</p>

<p>j</p>
"
"0.197539971211966","0.203455979297691","178152","<p>I want to fit diurnal cortisol profiles using linear mixed models, as was done by previous researchers (e.g. <a href=""http://www.sciencedirect.com/science/article/pii/S0306453005000491"" rel=""nofollow"" title=""Estimating between- and within-individual variation in cortisol levels using multilevel models"">Estimating between- and within-individual variation in cortisol levels using multilevel models</a>). </p>

<p>I have 4 samples of each individual for 1 to 3 days and the exact time of taking. I am especially interested in the individual intercepts and slopes of these profiles (i.e. the effect of time), which I NEED for other analyses, as intercept and slope are indicators of different aspects of stress regulation, theoretically.
Because cortisol is not normally distributed most researches use the natural logarithm before estimation. But when I look at the actual distribution, it looks like a poisson distribution (after rounding up). However, after fitting the model I checked for overdispersion, and unfortunately it is there. Thus, a negative binomial distribution might be more adequate.</p>

<p>So for sake of comparison I utilized all three models using the lme4 package.</p>

<p>For the log model i used</p>

<pre><code>lmer(log(cortisol) ~ time + (time|id) + (1|day/id), data=data)
</code></pre>

<p>This model gives me an unconditional RÂ² of 0.48 and a conditional RÂ² of 0.57. Unfortunately, the random effects are perfectly negative correlated, which is probably due to the low variance of random effects. From a statistical perspective a random intercept random slope model seems not appropriate. 
I also tried setting the random effects being independent of each other by using (zeit||id) instead, but this just gives me no variation for the random intercept.</p>

<p>For the second model i used</p>

<pre><code> glmer(round(cortisol, digits=0) ~ time + (time|id) + (1|day/id), data=data, family = poisson(link=log), control=glmerControl(optimizer=""bobyqa"")
</code></pre>

<p>This model gives me a better unconditional RÂ² of 0.56 and a much better conditional RÂ² of 0.94. Also, the correlation between random effects is -.44, which is what I would expect, and would also would like it to be around. However, I have the big problem of overdispersion (or do I???), which can be accounted for by defining an individual-level random effect as is suggested <a href=""http://r.789695.n4.nabble.com/Mixed-effects-model-for-overdispersed-count-data-td3010455.html"" rel=""nofollow"">here</a>. But this will result in the model been similar to first model; almost no variance of random effects and extreme correlation.</p>

<p>And lastly for the negative binomial model is used</p>

<pre><code>glmer.nb(round(cortisol, digits=0) ~ time + (time|id) + (1|day/id), data=data, control=glmerControl(optimizer=""bobyqa"")
</code></pre>

<p>This model gives me the worst unconditional RÂ² of 0.44 and a conditional RÂ² of 0.48. Also, the variance of the random intercept is zero, therefore correlation between random effects is not to be calculated.</p>

<p>So my questions are</p>

<ol>
<li><p>How wrong are the predictions in the second model (poisson distribution) under the consideration of overdispersion? How bad is overdispersion? [for me it is the best model in terms of variance of random effects, expected correlation of random effects and model fit]</p></li>
<li><p>How could I force random effects to have a considerable variance in the first model (and not being totally correlated)?</p></li>
</ol>

<p>Thank you for your help, and please let me know if I can provide additional information. </p>
"
"0.135864607079633","0.144620305212437","178551","<p>I'm using a binomial glmer mixed effects model using and I have two questions. </p>

<ol>
<li>One variable that I have, 'stimulus' has 12 levels. The levels were not randomly selected, so I have used it as a fixed variable in the basic model but R seems not to like it (at least this is my interpretation) given the way the output looks and the amount of time R takes to process it.</li>
</ol>

<p>m0.1 &lt;- glmer(match ~ Listgp + stimulus + (1|Listener), data = PATdata, family = ""binomial"")</p>

<blockquote>
  <p>summary(m0.1)
  Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [
  glmerMod]
   Family: binomial  ( logit )
  Formula: match ~ Listgp + stimulus + (1 | Listener)
     Data: PATdata</p>
</blockquote>

<pre><code> AIC      BIC   logLik deviance df.resid 
</code></pre>

<p>5154.3   5259.5  -2562.2   5124.3     8193 </p>

<p>Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-25.0764  -0.2706  -0.1939   0.2472  10.5131 </p>

<p>Random effects:
 Groups   Name        Variance Std.Dev.
 Listener (Intercept) 1.743    1.32<br>
Number of obs: 8208, groups:  Listener, 228</p>

<p>Fixed effects:
              Estimate Std. Error z value Pr(>|z|)<br>
(Intercept)     2.7561     0.2657  10.371  &lt; 2e-16 <strong>*
ListgpTA        0.1741     0.3147   0.553 0.580128<br>
ListgpTQ        0.0810     0.2575   0.315 0.753094<br>
stimulushaaDD  -5.4415     0.2071 -26.272  &lt; 2e-16 <em></strong>
stimulushad    -4.2953     0.1822 -23.569  &lt; 2e-16 <strong></em>
stimulushaDD   -5.4946     0.2086 -26.337  &lt; 2e-16 <em></strong>
stimulushid    -5.1519     0.1994 -25.832  &lt; 2e-16 <strong></em>
stimulushiDD   -0.6708     0.1801  -3.724 0.000196 <em></strong>
stimulushiid   -5.8124     0.2186 -26.593  &lt; 2e-16 <strong></em>
stimulushiiDD  -5.5101     0.2091 -26.353  &lt; 2e-16 <em></strong>
stimulushud    -0.2016     0.1915  -1.053 0.292345<br>
stimulushuDD   -5.6188     0.2123 -26.462  &lt; 2e-16 <strong></em>
stimulushuud   -5.6107     0.2121 -26.450  &lt; 2e-16 *</strong></p>

<h2>stimulushuuDD  -5.3207     0.2038 -26.109  &lt; 2e-16 ***</h2>

<p>Signif. codes:  0 â€˜<strong><em>â€™ 0.001 â€˜</strong>â€™ 0.01 â€˜</em>â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1</p>

<p>Correlation of Fixed Effects:
              (Intr) LstgTA LstgTQ stimulushaaDD stimulushad stimulushaDD
ListgpTA      -0.613<br>
ListgpTQ      -0.755  0.636<br>
stimulushaaDD -0.394 -0.007  0.004<br>
stimulushad   -0.440 -0.006  0.005  0.605<br>
stimulushaDD  -0.392 -0.007  0.003  0.555         0.601<br>
stimulushid   -0.407 -0.007  0.004  0.572         0.624       0.569<br>
stimulushiDD  -0.414  0.000  0.001  0.534         0.606       0.530<br>
stimulushiid  -0.376 -0.006  0.003  0.536         0.578       0.533<br>
stimulushiiDD -0.391 -0.007  0.003  0.554         0.600       0.551<br>
stimulushud   -0.386  0.000  0.000  0.497         0.564       0.493<br>
stimulushuDD  -0.385 -0.007  0.003  0.548         0.592       0.545<br>
stimulushuud  -0.386 -0.007  0.003  0.548         0.593       0.545<br>
stimulushuuDD -0.400 -0.007  0.004  0.564         0.613       0.561<br>
              stimulushid stimulushiDD stimulushiid stimulushiiDD stimulushud
ListgpTA<br>
ListgpTQ<br>
stimulushaaDD<br>
stimulushad<br>
stimulushaDD<br>
stimulushid<br>
stimulushiDD   0.554<br>
stimulushiid   0.549       0.506<br>
stimulushiiDD  0.568       0.529        0.533<br>
stimulushud    0.516       0.569        0.471        0.492<br>
stimulushuDD   0.562       0.521        0.527        0.544         0.484<br>
stimulushuud   0.562       0.522        0.528        0.545         0.485<br>
stimulushuuDD  0.579       0.543        0.542        0.560         0.505<br>
              stimulushuDD stimulushuud
ListgpTA<br>
ListgpTQ<br>
stimulushaaDD<br>
stimulushad<br>
stimulushaDD<br>
stimulushid<br>
stimulushiDD<br>
stimulushiid<br>
stimulushiiDD<br>
stimulushud<br>
stimulushuDD<br>
stimulushuud   0.539<br>
stimulushuuDD  0.554        0.554 </p>

<p>So, my question is, can I consider 'stimulus' as a random effect instead?</p>

<blockquote>
  <p>m0.1 &lt;- glmer(match ~ Listgp + (1|stimulus) + (1|Listener), data = PATdata, family = ""binomial"")
  summary(m0.1)
  Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [
  glmerMod]
   Family: binomial  ( logit )
  Formula: match ~ Listgp + (1 | stimulus) + (1 | Listener)
     Data: PATdata</p>
</blockquote>

<pre><code> AIC      BIC   logLik deviance df.resid 
</code></pre>

<p>5218.3   5253.4  -2604.2   5208.3     8203 </p>

<p>Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-21.9276  -0.2804  -0.2059   0.2740   9.4275 </p>

<p>Random effects:
 Groups   Name        Variance Std.Dev.
 Listener (Intercept) 1.676    1.294<br>
 stimulus (Intercept) 4.949    2.225<br>
Number of obs: 8208, groups:  Listener, 228; stimulus, 12</p>

<p>Fixed effects:
            Estimate Std. Error z value Pr(>|z|)<br>
(Intercept)  -1.3754     0.6792  -2.025   0.0429 *
ListgpTA      0.2284     0.3073   0.743   0.4572  </p>

<h2>ListgpTQ      0.1432     0.2513   0.570   0.5687</h2>

<p>Signif. codes:  0 â€˜<strong><em>â€™ 0.001 â€˜</strong>â€™ 0.01 â€˜</em>â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1</p>

<p>Correlation of Fixed Effects:
         (Intr) LstgTA
ListgpTA -0.235<br>
ListgpTQ -0.288  0.636</p>

<blockquote>
  <p></p>
</blockquote>

<p>Appreciating your help,</p>

<p>Shad</p>
"
"0.143509461970482","0.151697009103034","178682","<p>I'm using a binomial glmer mixed effects model using and I have two questions.</p>

<p>One variable that I have, 'stimulus' has 12 levels. The levels were not randomly selected, so I have used it as a fixed variable in the basic model but R seems not to like it (at least this is my interpretation) given the way the output looks and the amount of time R takes to process it.</p>

<pre><code>m0.1 &lt;- glmer(match ~ Listgp + stimulus + (1|Listener), data = PATdata, family = ""binomial"")

summary(m0.1) Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [ glmerMod] Family: binomial ( logit ) Formula: match ~ Listgp + stimulus + (1 | Listener) Data: PATdata
 AIC      BIC   logLik deviance df.resid 
5154.3 5259.5 -2562.2 5124.3 8193

Scaled residuals: Min 1Q Median 3Q Max -25.0764 -0.2706 -0.1939 0.2472 10.5131

Random effects: Groups Name Variance Std.Dev. Listener (Intercept) 1.743 1.32
Number of obs: 8208, groups: Listener, 228

Fixed effects: Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) 2.7561 0.2657 10.371 &lt; 2e-16 * ListgpTA 0.1741 0.3147 0.553 0.580128
ListgpTQ 0.0810 0.2575 0.315 0.753094
stimulushaaDD -5.4415 0.2071 -26.272 &lt; 2e-16 stimulushad -4.2953 0.1822 -23.569 &lt; 2e-16 stimulushaDD -5.4946 0.2086 -26.337 &lt; 2e-16 stimulushid -5.1519 0.1994 -25.832 &lt; 2e-16 stimulushiDD -0.6708 0.1801 -3.724 0.000196 stimulushiid -5.8124 0.2186 -26.593 &lt; 2e-16 stimulushiiDD -5.5101 0.2091 -26.353 &lt; 2e-16 stimulushud -0.2016 0.1915 -1.053 0.292345
stimulushuDD -5.6188 0.2123 -26.462 &lt; 2e-16 stimulushuud -5.6107 0.2121 -26.450 &lt; 2e-16 *

stimulushuuDD -5.3207 0.2038 -26.109 &lt; 2e-16 ***

Signif. codes: 0 â€˜â€™ 0.001 â€˜â€™ 0.01 â€˜â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects: (Intr) LstgTA LstgTQ stimulushaaDD stimulushad stimulushaDD ListgpTA -0.613
ListgpTQ -0.755 0.636
stimulushaaDD -0.394 -0.007 0.004
stimulushad -0.440 -0.006 0.005 0.605
stimulushaDD -0.392 -0.007 0.003 0.555 0.601
stimulushid -0.407 -0.007 0.004 0.572 0.624 0.569
stimulushiDD -0.414 0.000 0.001 0.534 0.606 0.530
stimulushiid -0.376 -0.006 0.003 0.536 0.578 0.533
stimulushiiDD -0.391 -0.007 0.003 0.554 0.600 0.551
stimulushud -0.386 0.000 0.000 0.497 0.564 0.493
stimulushuDD -0.385 -0.007 0.003 0.548 0.592 0.545
stimulushuud -0.386 -0.007 0.003 0.548 0.593 0.545
stimulushuuDD -0.400 -0.007 0.004 0.564 0.613 0.561
stimulushid stimulushiDD stimulushiid stimulushiiDD stimulushud ListgpTA
ListgpTQ
stimulushaaDD
stimulushad
stimulushaDD
stimulushid
stimulushiDD 0.554
stimulushiid 0.549 0.506
stimulushiiDD 0.568 0.529 0.533
stimulushud 0.516 0.569 0.471 0.492
stimulushuDD 0.562 0.521 0.527 0.544 0.484
stimulushuud 0.562 0.522 0.528 0.545 0.485
stimulushuuDD 0.579 0.543 0.542 0.560 0.505
stimulushuDD stimulushuud ListgpTA
ListgpTQ
stimulushaaDD
stimulushad
stimulushaDD
stimulushid
stimulushiDD
stimulushiid
stimulushiiDD
stimulushud
stimulushuDD
stimulushuud 0.539
stimulushuuDD 0.554 0.554
</code></pre>

<p>So, my question is, can I consider 'stimulus' as a random effect instead?</p>

<pre><code>m0.1 &lt;- glmer(match ~ Listgp + (1|stimulus) + (1|Listener), data = PATdata, family = ""binomial"") summary(m0.1) Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [ glmerMod] Family: binomial ( logit ) Formula: match ~ Listgp + (1 | stimulus) + (1 | Listener) Data: PATdata
 AIC      BIC   logLik deviance df.resid 
5218.3 5253.4 -2604.2 5208.3 8203

Scaled residuals: Min 1Q Median 3Q Max -21.9276 -0.2804 -0.2059 0.2740 9.4275

Random effects: Groups Name Variance Std.Dev. Listener (Intercept) 1.676 1.294
stimulus (Intercept) 4.949 2.225
Number of obs: 8208, groups: Listener, 228; stimulus, 12

Fixed effects: Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -1.3754 0.6792 -2.025 0.0429 * ListgpTA 0.2284 0.3073 0.743 0.4572

ListgpTQ 0.1432 0.2513 0.570 0.5687

Signif. codes: 0 â€˜â€™ 0.001 â€˜â€™ 0.01 â€˜â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects: (Intr) LstgTA ListgpTA -0.235
ListgpTQ -0.288 0.636
</code></pre>
"
"0.165710452999832","0.161690416690889","181665","<p>My question relates to calculating the uncertainty associated with the estimation of slopes in a varying intercept, varying slope hierarchical model.</p>

<p>I would like to calculate the effect of a treatment in different districts. If I ran a simple linear regression in which there was no pooling at district level, my model would look something like:</p>

<pre><code>    fm1 &lt;- lm(response ~ treatment*district)
</code></pre>

<p>I could then find the effect of treatment in district j by summing the coefficient for treatment with that of the interaction term treatment:districtj, and the standard error for that estimate could be found with:</p>

<pre><code>    sqrt(vcov(fm1)[2,2] + vcov(fm1)[4,4] + 2*vcov(fm1)[2,4])
</code></pre>

<p>I get stuck when moving to a multilevel model with random slopes along the lines of:</p>

<pre><code>    fm2 &lt;- lmer(response ~ treatment + (1 + treatment | district))
</code></pre>

<p>I can again find the effect of treatment in district j easily by summing the coefficient for treatment with the random slope estimate for district j. I would like to account for the uncertainty of both coefficients and I'm not sure how to do that. I can use the arm package to extract the standard errors associated with the slope estimates for each district and I know the standard error of the treatment estimate but I don't know how to estimate their correlation.  </p>

<p>It seems like this should be an easy question to answer but I haven't been able to find it. Hoping someone can point me in the right direction.</p>
"
"0.10696563746014","0.104370715180858","181844","<p>I'm trying to extract the value of the Variance from a Random Effects model built in R, using the lme4 package:</p>

<pre><code>Linear mixed model fit by REML ['lmerMod']
Formula: PH ~ (1 | gen) + rep
   Data: tab

REML criterion at convergence: 591.3

Scaled residuals: 
 Min       1Q   Median       3Q      Max 
-2.09030 -0.53055  0.04938  0.66689  2.02929 

Random effects:
Groups   Name        Variance Std.Dev.
 gen      (Intercept) 34.99    5.915   
 Residual             75.16    8.670   
Number of obs: 80, groups:  gen, 40

Fixed effects:
        Estimate Std. Error t value
(Intercept)  296.268      3.205   92.45
rep            8.550      1.939    4.41

Correlation of Fixed Effects:
(Intr)
rep -0.907
</code></pre>

<p>I'm interested in the Random effects section - the Variance for the ""gen"" Group (the value is 34.99). Can anyone help?</p>
"
"0.136145029683612","0.147602480923349","186825","<p>I am trying to run a mixed model on over-dispersed non-integer data. My data are not counts, but are zero-inflated and over dispersed. The variable is distance (how far a gps point is from a central location) and as such looks like: 0.33, 64.73, 5.2 etc. I have been using a quasi-Poisson distribution as I have read that quasi can handle non-integer data (both Poisson and negative binomial cannot). I am using the <code>glmmPQL</code> function in package MASS as this allows quasi distributions with a random term (the identity of the individual that the gps point comes from).The functions <code>glmm</code> and <code>lmer</code> do not work with a quasi-Poisson distribution. Plotting the residuals indicates a lack of fit of this model.log-transforming the data to try and make it normal before hand also fails (the Shapiro-test for normality is significant). I am unsure how to fix this, as I seemingly have to use a quasi-distribution (link=""log"") because my data is not counts, non-integer and not normal but there is still overdispersion and lack of fit when using this distribution. </p>

<p>My question therefore is: <strong>How to model over-dispersed, non-integer data in a mixed model when quasi-Poisson does not seem to work?</strong>   </p>

<p>My code so far is:</p>

<pre><code>summary(glmmPQL(distance_from_centroid~Chick.Juv.Adult+Summer_winter, 
                random=~1|markingnumber, family=quasipoisson(link=""log""),
                data=centroid_distances))
</code></pre>

<p>Which results in:  </p>

<pre><code>Linear mixed-effects model fit by maximum likelihood
 Data: centroid_distances 
  AIC BIC logLik
   NA  NA     NA

Random effects:
 `Formula: ~1 | markingnumber
        (Intercept) Residual
StdDev:    1.157381 2.136811

Variance function:
 Structure: fixed weights
 Formula: ~invwt 
Fixed effects: distance._from_centroid ~ Chick.Juv.Adult + Summer_winter 
                      Value  Std.Error  DF   t-value p-value
(Intercept)       2.0670095 0.09403952 695 21.980221  0.0000
Chick.Juv.AdultC -0.2945360 0.06686399 695 -4.405002  0.0000
Chick.Juv.AdultJ -0.2005831 0.06727181 695 -2.981682  0.0030
Summer_winterW    0.1207721 0.04324588 695  2.792684  0.0054
 Correlation: 
                 (Intr) C.J.AC C.J.AJ
Chick.Juv.AdultC -0.565              
Chick.Juv.AdultJ -0.512  0.736       
Summer_winterW   -0.267  0.134  0.043

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-2.53759073 -0.48277169 -0.31041612  0.06314122  7.48672836 

Number of Observations: 1009
Number of Groups: 311 
</code></pre>

<p>Which when plotting the residuals gives me:</p>

<p><a href=""http://i.stack.imgur.com/3SKVU.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/3SKVU.jpg"" alt=""plot of residuals""></a></p>
"
"0.143509461970482","0.140028008402801","186962","<p>Let's suppose I measure weights of eight male and eight female mice, from four litters, that were subjected to two different treatments: a and b</p>

<p>Here are my data:</p>

<pre><code>set.seed(1)
df &lt;- data.frame(sex = c(rep(""m"",8),rep(""f"",8)), treatment = rep(c(rep(""a"",4),rep(""b"",4)),2))
df$weight &lt;- c(rnorm(4,1),rnorm(4,2),rnorm(4,3),rnorm(4,4))
    df$litter &lt;- rep(c(rep(""l1"",2),rep(""l2"",2),rep(""l3"",2),rep(""l4"",2)),2)
</code></pre>

<p>Now I fit a mixed effects model to the interaction between sex and treatment, which are categorical fixed effects, and litter is defined as a random effect:</p>

<pre><code>df$sex = as.factor(df$sex)
df$treatment = as.factor(df$treatment)
df$litter = as.factor(df$litter)


fit &lt;- lmer(weight ~ sex*treatment + (1|litter), data = df)
</code></pre>

<p>And here's the output:</p>

<pre><code>summary(fit)
Linear mixed model fit by REML ['lmerMod']
Formula: weight ~ sex * treatment + (1 | litter)
   Data: df

REML criterion at convergence: 38.6

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-1.47793 -0.54377 -0.01737  0.48467  1.45905 

Random effects:
 Groups   Name        Variance Std.Dev.
 litter   (Intercept) 0.4318   0.6571  
 Residual             0.7574   0.8703  
Number of obs: 16, groups:  litter, 4

Fixed effects:
                Estimate Std. Error t value
(Intercept)      3.54300    0.63662   5.565
sexm            -2.46379    0.61541  -4.004
treatmentb       0.01801    0.90031   0.020
sexm:treatmentb  1.08648    0.87032   1.248

Correlation of Fixed Effects:
            (Intr) sexm   trtmnt
sexm        -0.483              
treatmentb  -0.707  0.342       
sxm:trtmntb  0.342 -0.707 -0.483
</code></pre>

<p>Obviously the ""f"" sex and the ""a"" treatment are set as dummy variables to a zero baseline.
My question relates to the interpretation: is there a way to extract the effect size of the dummy variables? 
Alternatively, if I add an additional sex and treatment variable, for which I set weight to 0, will they therefore serve as dummy variables that really have a zero effect size?</p>
"
"0.126563449052859","0.123493095605825","188301","<p>Following some reading I have tried for the first time to use multilevel poisson model to look at risk factors for disease cases in sheep in 5 English regions. </p>

<p>The overall aim is to find out if the factors could be predictive for within and between region disease cases (5 regions in total). The R output shows that some factors are significant. I also read that multicollinearity can be a problem while the correlation of fixed effects show that there is multicollinearity something cant figure out how to fix.</p>

<p>Could someone help explain in simpler terms how to interpret results from the output. Does the p-values which are significant shown on fixed effects indicate that these factors are the predictors for disease in every region from the 5 regions? </p>

<p>I have also included sample from the dataset.</p>

<pre><code>Postcode    Region.Coding   maxtemp meantemp    mintemp Cases2011
YO7 4DH     1               13.45   9.75        6.05    50
YO62 7JL    1               13.45   9.75        6.05    0
YO62 6RW    1               13.45   9.75        6.05    10
YO62 5HX    1               13.45   9.75        6.05    0
TN27 0DA    2               15.32   11.22       7.13    98
TN26 3TF    2               15.32   11.22       7.13    0
TN26 3EU    2               15.32   11.22       7.13    30
TN25 6AS    2               15.32   11.22       7.13    0
TN25 5PD    2               15.32   11.22       7.13    28
TR7 3HU     3               14.17   10.6        7.06    115
TR27 5EF    3               14.17   10.6        7.06    0
TR10 9DL    3               14.17   10.6        7.06    0
TQ9 7LN     3               14.17   10.6        7.06    23
TQ9 6NQ     3              14.17    10.6        7.06    50

mod1=glmer(Cases2011~maxtemp11+mintemp11+meantemp11+rain2011+Altitude+Flock2011+(1|Region.Coding/Postcode), family=poisson, data=orf)

  Min      1Q  Median      3Q     Max 
-5.6160 -0.1209 -0.0658  0.0206  2.9740 

Random effects:
 Groups                 Name        Variance  Std.Dev.
 Postcode:Region.Coding (Intercept) 44.647031 6.68184 
 Region.Coding          (Intercept)  0.001741 0.04173 
Number of obs: 756, groups:  Postcode:Region.Coding, 752; Region.Coding, 5

Fixed effects:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -9.2474876  3.7713537  -2.452  0.01421 *  
maxtemp11   -1.0416300  0.3833977  -2.717  0.00659 ** 
mintemp11    0.2317166  0.2231959   1.038  0.29919    
meantemp11   1.4316707  0.5645888   2.536  0.01122 *  
rain2011     0.0034593  0.0013515   2.560  0.01048 *  
Altitude     0.0011370  0.0030738   0.370  0.71147    
Flock2011    0.0018647  0.0001878   9.929  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
           (Intr) mxtm11 mintmp11 mentmp11 rn2011 Altitd
maxtemp11   0.028                                       
mintemp11  -0.331 -0.246                                
meantemp11 -0.473 -0.851  0.140                         
rain2011   -0.536 -0.268  0.217    0.409                
Altitude   -0.249 -0.010  0.139    0.062    0.010       
Flock2011  -0.062  0.044 -0.046   -0.048   -0.012 -0.019
</code></pre>
"
"0.244543816336284","0.247133163073042","188361","<p>I'm pretty new in using <code>lmer</code> and be confused about different p-values in Tukey post hoc tests associated with exactly the same estimates. I built a linear mixed model with monetary contributions of human subjects as response variable and their wealth and number of children as explanatory variables. The experiment was designed in a way to contribute for future generations. I don't have repeated measurements of the same individual but some individuals played within the same group. There are several subsets and additional random factors but here I only want to consider the following model where <code>totalcontSubject</code> means contribution of a subject over the entire game, <code>poverty</code> is a factor with 2 levels (rich and poor), and <code>children</code> is a factor with 2 levels (child or noChild). Particularly I'm interested in understanding the fixed effects part of the model.</p>

<pre><code> &gt; summary(TC1)
Linear mixed model fit by REML ['lmerMod']
Formula: totalcontSubject ~ poverty * children + (1 | group_2)
   Data: data

REML criterion at convergence: 414.6

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-2.42955 -0.45554 -0.09361  0.45228  2.33159 

Random effects:
 Groups   Name        Variance  Std.Dev. 
 group_2  (Intercept) 8.611e-15 9.280e-08
 Residual             1.042e+02 1.021e+01
Number of obs: 58, groups:  group_2, 10

Fixed effects:
                             Estimate Std. Error t value
(Intercept)                    16.200      3.228   5.019
povertyrich                     8.600      3.953   2.175
childrennoChild                 2.800      4.565   0.613
povertyrich:childrennoChild    -4.489      5.642  -0.796

Correlation of Fixed Effects:
            (Intr) pvrty chldrC
povertyrch  -0.816              
chldrnnChld -0.707  0.577       
pvrtyrch:C   0.572 -0.701 -0.809
</code></pre>

<p>If I interpret fixed effects of the summary table in the right way, my intercept denotes poor people with children. The estimate also corresponds to the mean value of this combination in my data. According to my calculations the difference to rich people (shown as <code>povertyrich</code>) actually shows the difference of the intercept to rich people with children, even if not explicitly mentioned by <code>povertyrich</code>. This is the first issue I'm a bit confused. A reduced model only with fixed factor poverty is significant better by <code>anova()</code> but it seems data including children are used for this evaluation.</p>

<p>If I run a Tukey post hoc test by means of my TC1 model, I get a significant difference between rich and poor. But the estimates in the summary actually include children. Estimates of intercept and slope are the means of poor people with children and the difference to rich people with children. They don't correspond to the means of poor or rich data irrespective of parenthood. </p>

<pre><code>summary(glht(TC1, linfct=mcp(povertry=""Tukey"")))

         Simultaneous Tests for General Linear Hypotheses

Multiple Comparisons of Means: Tukey Contrasts


Fit: lmer(formula = totalcontSubject ~ poverty * children + (1 | 
    group_2), data = data)

Linear Hypotheses:
                 Estimate Std. Error z value Pr(&gt;|z|)  
rich - poor == 0    8.600      3.953   2.175   0.0296 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
(Adjusted p values reported -- single-step method)
</code></pre>

<p>I get even more confused when I run a Tukey post hoc test for a subset where I coded interactions in a column such as poor people with children and rich people with children. In this output I have exactly the same estimates and parameters for these categories (like in the summary shown before for rich poor people exclusively) but the p-values are different. A visual check indicates that there is a significant difference between <code>richChild</code> and <code>poorChild</code> but outputs of <code>glht</code> <code>Interak</code> shows me it is not. Also, a comparison between models <code>anova()</code> with fixed factor poverty vs. fixed factors poverty and children indicates that I can get rid of the variable children in my model. Before I do so, I would like to understand the outputs better. I also worry about the high value for Residual and the correlations in the summary table. </p>

<pre><code>&gt; summary(glht(TC1_2, linfct=mcp(Interak=""Tukey"")))

         Simultaneous Tests for General Linear Hypotheses

Multiple Comparisons of Means: Tukey Contrasts


Fit: lmer(formula = totalcontSubject ~ Interak + (1 | group_2), data = data)

Linear Hypotheses:
                               Estimate Std. Error z value Pr(&gt;|z|)
poorNoChild - poorChild == 0      2.800      4.565   0.613    0.927
richChild - poorChild == 0        8.600      3.953   2.175    0.129
richNoChild - poorChild == 0      6.911      4.026   1.717    0.312
richChild - poorNoChild == 0      5.800      3.953   1.467    0.454
richNoChild - poorNoChild == 0    4.111      4.026   1.021    0.735
richNoChild - richChild == 0     -1.689      3.316  -0.509    0.956
(Adjusted p values reported -- single-step method)
</code></pre>
"
"0.23918243661747","0.224044813444482","189933","<p>I am seeking advice on how to effectively eliminate autocorrelation from a linear mixed model. My experimental design and explanation of fixed and random factors can be found here from an earlier question I asked: </p>

<p><a href=""http://stats.stackexchange.com/questions/188929/crossed-fixed-effects-model-specification-including-nesting-and-repeated-measure"">Crossed fixed effects model specification including nesting and repeated measures using glmm in R</a></p>

<p>I have treated day as numeric even though I only have four sampling time points (so I could treat it as a categorical predictor as well). Aside: Although four sample points is very few, I donâ€™t think that this is the root of the problem as this same dataset is giving me this residual autocorrelation issues using a different response variable that has 24 time points.</p>

<p>My issue is that I have tried a number of different autocorrelation structures and canâ€™t seem to achieve the random, non-significant residuals needed to confirm a lack of autocorrelation. I am using the function <code>lme</code> in the R package <code>nlme</code> to deal with autocorrelation. </p>

<p>I have tried the various autocorrelation classes  with variations to form</p>

<p>1) <code>corAR1</code> (autoregressive process of order 1).</p>

<p>2) <code>corARMA</code> (autoregressive moving average process)</p>

<p>3) <code>corCAR1</code> (continuous autoregressive process)</p>

<p>4) <code>corGaus</code> (Gaussian spatial correlation)</p>

<p>With form varying in the following ways with these different autocorrelation classes:</p>

<pre><code>form=~1
form=~1| TankNumb/RecruitID2
form=~Day| TankNumb/RecruitID2
</code></pre>

<p>If we look at a model without the time factor ""Day"" added, the ACF and PACF plots look like this. </p>

<pre><code>lme4_lognormal_notime&lt;-lmer(Arealog~Temperature*Culture+(1|TankNumb/RecruitID2), data=growthSR_noNA)

acf(residuals(lme4_lognormal_notime, retype=""normalized""))
pacf(residuals(lme4_lognormal_notime, retype=""normalized""))
</code></pre>

<p><a href=""http://i.stack.imgur.com/xVdrb.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xVdrb.jpg"" alt=""enter image description here""></a></p>

<p>Also, if I look at the residuals of the model without â€œDayâ€ included, I do not see any strong pattern in the residuals that would make me think there is a temporal autocorrelation problem.</p>

<pre><code>plot(residuals(lme4_lognormal_Ben_notime, retype=""normalized"")~growthSR_noNA$Day)
</code></pre>

<p><a href=""http://i.stack.imgur.com/wU1ZC.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/wU1ZC.jpg"" alt=""enter image description here""></a></p>

<p>Now for two different models with autocorrelation structure to hopefully eliminate autocorrelation:</p>

<pre><code>nlme_lognormal_mult_cor&lt;-lme(Arealog~Temperature*Culture*Day, random=~1|TankNumb/RecruitID2,correlation=corAR1(form=~1), data=growthSR_noNA)
</code></pre>

<p><a href=""http://i.stack.imgur.com/Oe3et.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Oe3et.jpg"" alt=""enter image description here""></a></p>

<pre><code>nlme_lognormal_mult_cortime&lt;-lme(Arealog~Temperature*Culture*Day, random=~1|TankNumb/RecruitID2,correlation=corAR1(form=~Day|TankNumb/RecruitID2), data=growthSR_noNA)
</code></pre>

<p><a href=""http://i.stack.imgur.com/pUgA1.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pUgA1.jpg"" alt=""enter image description here""></a></p>

<pre><code>ARMA_nlme_lognormal_mult_cor&lt;-lme(Arealog~Temperature*Culture*Day, random=~1|TankNumb/RecruitID2,correlation=corARMA(form=~1, p=0, q=1), data=growthSR_noNA)
</code></pre>

<p><a href=""http://i.stack.imgur.com/6TMeL.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6TMeL.jpg"" alt=""enter image description here""></a></p>

<p>The AIC suggests that the simplest correlation structure is the best. </p>

<pre><code>AIC(nlme_lognormal_mult,nlme_lognormal_mult_cor, nlme_lognormal_mult_cortime,ARMA_nlme_lognormal_Ben_mult_cor)

                               df      AIC
nlme_lognormal_mult              15 1233.997
nlme_lognormal_mult_cor          16 1184.389
nlme_lognormal_mult_cortime      16 1235.997
ARMA_nlme_lognormal_Ben_mult_cor 16 1198.451
</code></pre>

<p>As I mentioned above, I have tried a number of different <code>cor</code> functions (the four listed above) and different <code>form</code> specifications. They all end up with ACF/PCF plots like the last two models with a first lag at below 0.2 in the ACF plot and a PCF plot with the first three lags around 0.10.</p>

<p>I have also read a number of sites describing how to specify corARMA models based on diagnosing the ACF plots and have tried a number of variations of p and q parameters. </p>

<p>Questions: </p>

<ol>
<li>Does anyone have some advice on which type of correlation structure that might elimate this autocorrelation problem based on the patterns in my ACF/PCF plots? Should I be diagnosing based on a model with or without Day included?</li>
</ol>

<p>2.Is there ever an acceptable level of autocorrelation? 
This post (<a href=""http://stats.stackexchange.com/questions/80823/do-autocorrelated-residual-patterns-remain-even-in-models-with-appropriate-corre"">Do autocorrelated residual patterns remain even in models with appropriate correlation structures, &amp; how to select the best models?</a>) states that small amounts of autocorrelation probably won't impact the model coefficients very much. ""The estimate is slightly larger than zero so will have negligible effect on the model fit and hence you might wish to leave it in the model if there is a strong a priori reason to assume residual autocorrelation."" Potentially there is some autocorrelation that is not being caused by temporal autocorrelation, like outliers? Is there a cut-off, for example, autocorrelation below 0.1? I have extremely small 95% confidence intervals, so it doesn't take a lot of autocorrelation in my models to be significantly too much.</p>

<p>Any advice is appreciated! </p>
"
"0.331475699900345","0.323434298015255","191810","<h2>Study design</h2>

<p>504 individuals were all sampled 2 times. Once before and once after a celebration.</p>

<p>The goal is to investigate if this event (Celebration) as well as working with animals (sheepdog) have an influence on the probability that an individual gets infected by a parasite. (out of 1008 observations only 22 are found to be infected)</p>

<p><strong>Variables</strong></p>

<ul>
<li>dependent variable = ""T_hydat"" (infected or not) </li>
</ul>

<p>(most predictiv variables are categorical) </p>

<ul>
<li>""Celebration"" (yes/no)</li>
<li>""sex"" (m/f)</li>
<li>""RelAge"" (5 levels)</li>
<li>""SheepDog"" (yes/no)</li>
<li>""Area"" (geographical area = 4 levels)</li>
<li>""InfectionPeriodT_hydat"" (continuous --> Nr Days after deworming"")</li>
<li>""Urbanisation (3 levels)</li>
</ul>

<h2>Question 1:</h2>

<p>1) Should I include Individual-ID (""ID"") as a random Effekt as I sampled each Ind. 2 times? (Pseudoreplication?) </p>

<pre><code>mod_fail &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|ID), family = binomial)

Warnmeldungen:
1: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,  :
  failure to converge in 10000 evaluations
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 1.10808 (tol = 0.001, component 10)
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p>--> unfortunately this model fails to converge (is it a problem that ID = 504 levels with only 2 observations per level?) 
Convergence is achieved with glmmPQL() but after droping some unsignficant preditiv variables the model fails to converge again ? What is the Problem here? Could geeglm() be a solution?</p>

<p>In an other attempt I run the model only with ""Area"" (4 levels) as random effect (my expectation is that Ind. in the same geogr. Area are suffering from the same parasite pressure etc.) and received the follwoing p-Values. </p>

<h2>My model in R:</h2>

<pre><code>mod_converges &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|Area), family = binomial)
</code></pre>

<h2>mod_converges output:</h2>

<p>summary(mod_converges)</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + sex + SheepDog + RelAge + Urbanisation +  
    InfectionPeriodT_hydat + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   203.0    262.0    -89.5    179.0      996 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.461 -0.146 -0.088 -0.060 31.174 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.314    0.561   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)            -6.81086    1.96027   -3.47  0.00051 ***
Celebration1            1.36304    0.57049    2.39  0.01688 *  
sexm                   -0.18064    0.49073   -0.37  0.71279    
SheepDog1               2.02983    0.51232    3.96  7.4e-05 ***
RelAge2                 0.34815    1.18557    0.29  0.76902    
RelAge3                 0.86344    1.05729    0.82  0.41412    
RelAge4                -0.54501    1.43815   -0.38  0.70471    
RelAge5                 0.85741    1.25895    0.68  0.49584    
UrbanisationU           0.17939    0.78669    0.23  0.81962    
UrbanisationV           0.01237    0.59374    0.02  0.98338    
InfectionPeriodT_hydat  0.00324    0.01159    0.28  0.77985    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<h2>glmmPQL-Model output below (added after the first reply by usÎµr11852)</h2>

<p>This model converges with ""Sample_ID"" as a random effect, however, as statet by 
<strong>usÎµr11852</strong> the varaince of the random effect is quiet high  4.095497^2 = 16.8. And the std.error of Area5 is far to high (complete separation).
Can I just remove Datapoints from Area5 to overcome this Problem? </p>

<pre><code>#     T_hydat
#  Area   0   1
#     1 226   4
#     2 203   3
#     4 389  15
#     5 168   0  ## here is the problematic cell

Linear mixed-effects model fit by maximum likelihood
 Data: dat 
  AIC BIC logLik
   NA  NA     NA

Random effects:
 Formula: ~1 | Sample_ID
        (Intercept)  Residual
StdDev:    4.095497 0.1588054

Variance function:
 Structure: fixed weights
 Formula: ~invwt 
Fixed effects: T_hydat ~ Celebration + sex + SheepDog + YoungOld + Urbanisation +      InfectionPeriodT_hydat + Area 
                            Value Std.Error  DF    t-value p-value
(Intercept)            -20.271630     1.888 502 -10.735869  0.0000
Celebration1             5.245428     0.285 502  18.381586  0.0000
sexm                    -0.102451     0.877 495  -0.116865  0.9070
SheepDog1                3.356856     0.879 495   3.817931  0.0002
YoungOldyoung            0.694322     1.050 495   0.661017  0.5089
UrbanisationU            0.660842     1.374 495   0.480990  0.6307
UrbanisationV            0.494653     1.050 495   0.470915  0.6379
InfectionPeriodT_hydat   0.059830     0.007 502   8.587736  0.0000
Area2                   -1.187005     1.273 495  -0.932576  0.3515
Area4                   -0.700612     0.973 495  -0.720133  0.4718
Area5                  -23.436977 28791.059 495  -0.000814  0.9994
 Correlation: 
                       (Intr) Clbrt1 sexm   ShpDg1 YngOld UrbnsU UrbnsV InfPT_ Area2  Area4 
Celebration1           -0.467                                                               
sexm                   -0.355  0.018                                                        
SheepDog1              -0.427  0.079  0.066                                                 
YoungOldyoung          -0.483  0.017  0.134  0.045                                          
UrbanisationU          -0.273  0.005 -0.058  0.317 -0.035                                   
UrbanisationV          -0.393  0.001 -0.138  0.417 -0.087  0.586                            
InfectionPeriodT_hydat -0.517  0.804  0.022  0.088  0.016  0.007  0.003                     
Area2                  -0.044 -0.035 -0.044 -0.268 -0.070 -0.315 -0.232 -0.042              
Area4                  -0.213 -0.116 -0.049 -0.186 -0.023 -0.119  0.031 -0.148  0.561       
Area5                   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000

Standardized Within-Group Residuals:
          Min            Q1           Med            Q3           Max 
-14.208465914  -0.093224405  -0.022551663  -0.004948562  14.733133744 

Number of Observations: 1008
Number of Groups: 504 
</code></pre>

<p><strong>Output from logistf (Firth's penalized-likelihood logistic regression)</strong></p>

<pre><code>logistf(formula = T_hydat ~ Celebration + sex + SheepDog + YoungOld + 
    Urbanisation + InfectionPeriodT_hydat + Area, data = dat, 
    family = binomial)

Model fitted by Penalized ML
Confidence intervals and p-values by Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood 

                               coef   se(coef)  lower 0.95  upper 0.95       Chisq
(Intercept)            -5.252164846 1.52982941 -8.75175093 -2.24379091 12.84909207
Celebration1            1.136833737 0.49697927  0.14999782  2.27716500  5.17197661
sexm                   -0.200450540 0.44458464 -1.09803320  0.77892986  0.17662930
SheepDog1               2.059166246 0.47197694  1.10933774  3.12225212 18.92002321
YoungOldyoung           0.412641416 0.56705186 -0.66182554  1.77541644  0.50507269
UrbanisationU           0.565030324 0.70697218 -0.98974390  1.97489240  0.56236485
UrbanisationV           0.265401035 0.50810444 -0.75429596  1.33772658  0.25619218
InfectionPeriodT_hydat -0.003590666 0.01071497 -0.02530179  0.02075254  0.09198425
Area2                  -0.634761551 0.74958750 -2.27274031  0.90086554  0.66405078
Area4                   0.359032194 0.57158464 -0.76903324  1.63297249  0.37094569
Area5                  -2.456953373 1.44578029 -7.36654837 -0.13140806  4.37267766
                                  p
(Intercept)            3.376430e-04
Celebration1           2.295408e-02
sexm                   6.742861e-01
SheepDog1              1.363144e-05
YoungOldyoung          4.772797e-01
UrbanisationU          4.533090e-01
UrbanisationV          6.127483e-01
InfectionPeriodT_hydat 7.616696e-01
Area2                  4.151335e-01
Area4                  5.424892e-01
Area5                  3.651956e-02

Likelihood ratio test=36.56853 on 10 df, p=6.718946e-05, n=1008
Wald test = 32.34071 on 10 df, p = 0.0003512978
</code></pre>

<p>** glmer Model (Edited 28th Jan 2016)**</p>

<p>Output from glmer2var: Mixed effect model with the 2 most ""important"" variables (""Celebration"" = the factor I am interested in and ""SheepDog"" which was found to have a significant influence on infection when data before and after the celebration were analysed separately.) The few number of positives make it impossible to fit a model with more than two explanatory variables (see commet EdM). </p>

<p>There seems to be a strong effect of ""Celebration"" that probably cancels out the effect of ""SheepDog"" found in previous analysis.</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Sample_ID)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   113.0    132.6    -52.5    105.0     1004 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.5709 -0.0022 -0.0001  0.0000 10.3491 

Random effects:
 Groups    Name        Variance Std.Dev.
 Sample_ID (Intercept) 377.1    19.42   
Number of obs: 1008, groups:  Sample_ID, 504

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -19.896      4.525  -4.397  1.1e-05 ***
Celebration1    7.626      2.932   2.601  0.00929 ** 
SheepDog1       1.885      2.099   0.898  0.36919    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.908       
SheepDog1   -0.297 -0.023
</code></pre>

<h2>Question 2:</h2>

<p>2) Can I use drop1() to get the final model and use the p-Values from summary(mod_converges) for interpretation? Does my output tell me if it makes sense to include the random effect (""Area"") ?</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   190.8    210.4    -91.4    182.8     1004 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.369 -0.135 -0.096 -0.071 17.438 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.359    0.599   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)    -5.912      0.698   -8.47  &lt; 2e-16 ***
Celebration1    1.287      0.512    2.51    0.012 *  
SheepDog1       2.014      0.484    4.16  3.2e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.580       
SheepDog1   -0.504  0.027
</code></pre>

<p>I know there are quite a few questions but I would really appreciate some advice from experienced people. Thanks!</p>
"
"0.159209453430407","0.155347127476123","192785","<p><strong>Objective</strong></p>

<p>I have a crossed and implicitly nested design and am trying to validate the correct â€˜maximalâ€™ model (including all linear and pairwise interactions of the variables) for use in <code>lmer()</code>.  I intend to use this as the starting point for some kind of backward stepwise regression, possibly making use of the function <code>mixed()</code> in the  <code>{afex}</code> package.</p>

<p><strong>Experimental design</strong></p>

<p>This a linguistics study.  We have 20 <code>Subjects</code>, each speaking 180 utterances, amounting to 3600 observations in total. Each utterance is initiated via prompting, and an associated Response Time is measured. Log Response Time is the dependent variable. </p>

<p><em>Conditions &amp; Blocks</em></p>

<p>The Response Time for the utterances is affected by 3 <code>Conditions</code> (coded 1 to 3). Each <code>Condition</code> is implemented by prompting the <code>Subject</code> to recite 1 of 4 <code>Blocks</code> of utterances (coded 1 to 12).</p>

<p><em>Words &amp; Tones</em></p>

<p>Each <code>Block</code> brings about its associated <code>Condition</code> via 15-utterance repetition of 3 carefully chosen <code>Words</code>.  There are a total of 12 <code>Words</code> used in the experiment (coded 1 to 12). The <code>Words</code> within each <code>Block</code> can also be categorized by <code>Tone</code> (coded 1 to 2).  There are 6 <code>Words</code> per <code>Tone</code>.  </p>

<p><em>Summary</em></p>

<p>Each of the 20 <code>Subjects</code> utter all 12 <code>Blocks</code> of 15 utterances each.  In doing so, they repeatedly utter all 12 <code>Words</code> (15 utterances per <code>Word</code>), and thereby use both <code>Tones</code> (90 utterances per <code>Tone</code>).</p>

<p>I would like to consider <code>Block</code>, <code>Word</code>, and <code>Subject</code> as random effects, and <code>Condition</code> and <code>Tone</code> as fixed.</p>

<p><strong>Proposed Model</strong></p>

<p>I think the model can be written in the following wayâ€¦</p>

<p><code>RT_log ~ Condition*Tone + (Condition*Tone|Subject) + (Condition|Word) + (Tone|Block)</code></p>

<p><strong>Questions</strong></p>

<p><strong>1.</strong> Is this the 'maximal' model (with linear plus pairwise interactions) appropriate for my experimental design?_ </p>

<p><strong>2.</strong> There is correlation between <code>Block</code> and <code>Condition</code> (there are only 4 possible blocks - out of the total 12 - for each <code>Condition</code>).  There is, similarly, correlation between <code>Word</code> and <code>Tone</code>.  Is it 'okay' to leave this correlation in the model? I don't see a good way of removing it.</p>

<p><strong>3.</strong> How will lme4 handle implicit nesting: I.e., the blocks, which are implicitly nested in the 3 conditions (i.e., only 4 blocks are applicable to each of the 3 conditions, even though the blocks are coded from 1 to 12), and the words, which are implicitly nested within the 2 tones (only 6 words are applicable to each tone, even though words are coded from 1 to 12)?</p>

<p><strong>4.</strong> Some <code>Blocks</code> utilize <code>Words</code> of only a single <code>Tone</code>, whereas other <code>Blocks</code> utilize words of both <code>Tones</code>.  Will that cause problems for the <code>(Tone|Block)</code> term in the model? It will only make sense for certain values of Block.</p>

<p><strong>5.</strong> It has been suggested by some that we might need a ""Subject:Word"" grouping (random effect).  Why might we need this grouping?</p>
"
"0.159209453430407","0.168292721432467","193123","<p>I tried using mixed-models via <code>lmer()</code> from the lme4 package in R on my data, but I encountered some problems with correctly specifying the random terms effects.</p>

<p>I have classic experimental psychology design data, where different participants saw 24 items. Those items were separated in two emotional valences (let's say, 12 negative and 12 positive). Moreover, within each valences, they were presented in two conditions (condition A and B). </p>

<p>As for now, I have a 2 Valences * 2 conditions design. In addition to that, for each item, the participants rated the ""familiarity"" with the stimulus on a visual analog scale, adding a numeric variable. </p>

<p>Something that (according there's only 8 stimuli) looks like that:</p>

<pre><code>Participants Items Valence Condition Familiarity Outcome_Variable
A            I1    Neg     A         5           3
A            I2    Neg     A         8           5
A            I3    Neg     B         4           2
A            I4    Neg     B         3           7
A            I5    Pos     A         0           8
A            I6    Pos     A         1           3
A            I7    Pos     B         2           6
A            I8    Pos     B         6           1
B            I1    Neg     A         5           2
B            I2    Neg     A         8           7
...
</code></pre>

<p>I'm interested in the effect of the condition, valence and familiarity, as well as all their interaction, on my outcome variable.</p>

<p>At first, I tried to run a model expressed by the following equation:</p>

<pre><code>fit &lt;- lmer(Outcome_Variable ~ Valence * Condition * Familiarity +
                               (1|Participants) + (1|Items), data)
</code></pre>

<p>The model converged. However, I have seen many posts with people suggesting to allow for the variation of the slope in addition to the variation of the intercept. They often express this as adding some terms in the random effects specification <code>(1... |Participants)</code>. I tried several possibilities such as:</p>

<pre><code>fit &lt;- lmer(Outcome_Variable ~ Valence * Condition * Familiarity +
                               (1 + (Valence * Condition) |Participants) + 
                               (1 + (Valence * Condition) |Items), data)
</code></pre>

<p>Or even more complex things including the <code>Familiarity</code> variable. But the models often failed to converge. Therefore, I have no idea how to correctly specify the random effects in order to allow for the slope and intercept to vary across items and participants.</p>

<p>Moreover, someone asked me if <strong>""correlations between random effects were modelled""</strong>.... The problem is that I have no idea what he means by that...</p>

<p>Intuitively, I tried to put <code>*</code> instead of <code>+</code> in front of my two random terms.</p>

<pre><code>fit &lt;- lmer(Outcome_Variable ~ Valence * Condition * Familiarity *
 (1|Participants) * (1|Items), data)
</code></pre>

<p>But I have no idea what does it mean and what is the difference of this model with the first one with the <code>+</code>.</p>

<p>I would be very happy if someone could explain how to specify those random effects, and enlighten me on the ""correlations between random effects"" part :) </p>
"
"0.10696563746014","0.104370715180858","193665","<p>I am interested in fitting two models using the 'lavaan' package in R.  I have a psychotherapy data set that has a nested structure - patients nested within therapists.  I'm interested in fitting a model that predicts patients' outcome (a level 1 patient variable) from the length of treatment.  In 'lmer' it would be something like this:</p>

<p>mod1 &lt;- lmer(effect.size ~ Tx.length + (1|Therapist), data = data)</p>

<p>The reason I want to use 'lavaan' is because I am interested in allowing the intercept and slope to correlate in one model but not in the other.  I'm interested in whether the random intercept X slope correlation is different from zero.</p>

<p>Any advice on fitting this model in 'lavaan'?  Are there straightforward ways to specify the nesting of patients within therapists?</p>
"
"0.135302018298348","0.13201967239689","194451","<p>When I fit any model in <code>lmer()</code>, <code>summary</code> identifies a correlation between fixed effect(s) and the (fixed) intercept.  How should I interpret this, and why is it not $0$? In my way of thinking any correlation between the intercept and a fixed effect should be zero, since, in the design matrix $X$, the intercept term is just a column of $1$s, and therefore constant. There is a related post on this topic (<a href=""http://stats.stackexchange.com/questions/49082/lmer-interpretation-of-correlation"">lmer interpretation of correlation</a>), but it doesn't help me much.  Below is model <code>summary</code> output, so you can see what I mean. See the block labeled <code>Correlation of Fixed Effects:</code>, at the very bottom.</p>

<pre><code>>summary(exp2modFull)
Linear mixed model fit by REML ['lmerMod']
Formula: RT_log ~ Condition + (Condition | Subject) + (Condition | Item)
   Data: exp2

REML criterion at convergence: -1978.6

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.7080 -0.5775 -0.0634  0.4801  7.8186 

Random effects:
 Groups   Name            Variance  Std.Dev. Corr       
 Subject  (Intercept)     0.0261472 0.16170             
          Conditionoddman 0.0028821 0.05369  -0.21      
          Conditionhetero 0.0037356 0.06112  -0.46  0.80
 Item     (Intercept)     0.0018914 0.04349             
          Conditionoddman 0.0002885 0.01699  -0.97      
          Conditionhetero 0.0010140 0.03184  -0.65  0.81
 Residual                 0.0320147 0.17893             
Number of obs: 3600, groups:  Subject, 20; Item, 12

Fixed effects:
                 Estimate Std. Error t value
(Intercept)      6.583310   0.038622  170.46
Conditionoddman -0.009109   0.014883   -0.61
Conditionhetero  0.021487   0.018018    1.19

Correlation of Fixed Effects:
            (Intr) Cndtnd
Conditnddmn -0.309       
Conditinhtr -0.472  0.722</code></pre>
"
"0.172476907882941","0.168292721432467","194639","<p>I'm trying to create a nested model in lme4 and would like feedback on whether I've understood its use correctly.</p>

<p>Every individual (n = 144) had two proteins measured after 0, 12 and 72 hours. At each timepoint, the protein was measured twice. It's an observational study where the samples are from different medical centers. A sample of the data looks like this:</p>

<p><code>ID           Sex  Center    Age  Time Rep Pentraxin3          IL6
1    10151   Male       1    56      0   1       0.23          0.98
2    10151   Male       1    56      0   2       0.29          1.05
3    10151   Male       1    56     12   1       1.26          1.18
4    10151   Male       1    56     12   2       1.48          1.23
5    10151   Male       1    56     72   1       0.40          1.86
6    10151   Male       1    56     72   2       0.49          2.13
7    10201 Female       3    59      0   1       0.79          0.50
8    10201 Female       3    59      0   2       1.01          0.51
9    10201 Female       3    59     12   1         NA          2.46
10   10201 Female       3    59     12   2         NA          2.46
11   10201 Female       3    59     72   1         NA          1.62
12   10201 Female       3    59     72   2         NA          1.44</code></p>

<p>What I would like to do is model the effect of Pentraxin3 on IL6 (IL6 ~ Pentraxin3) and take into account the following random effects, where the code next to each statement is what I think is the lme4 code for that particular random effect:</p>

<ol>
<li><p>There can be correlations between the repeated measures at each time point.
(1|Time/Rep)</p>

<ol start=""2"">
<li><p>There can be correlations between the biomarker values measured at different time points within each individual. 
(1|ID/Time)</p></li>
<li><p>There can be correlations between individuals within the same center. (1|Center)</p></li>
<li>There are different slopes between individuals in the change of Pentraxin3 over time. (Pentraxin3 - 1 | ID/Time)</li>
</ol></li>
</ol>

<p>This is the model including fixed effects and random effects:</p>

<pre><code>m6 &lt;- lmer(IL6 ~ Sex + Pentraxin3 + Age + (1|Center) + (1|ID/Time) + (1|Time/Rep) + (Pentraxin3 - 1|ID/Time), data = d1)`
</code></pre>

<p>Is this correct? Should ""Time"" be a fixed effect? </p>

<p>`</p>
"
"0.10696563746014","0.104370715180858","194837","<p>I am having trouble using the correct test and r code for my experiment. Essentially I measured insect emergence daily from artificial streams with 3 treatments.:</p>

<pre><code>CONTROL - With 5 replicate streams  
TREAT 1 - With 5 Replicate streams  
TREAT 2 - With 5 Rep streams.  
TREAT 2- With 5 rep streams  
</code></pre>

<p>I think what I basically want to do is this: Emergence = Treatment + Day + Treatment*Day</p>

<p>Looking for an effect of treatment on insect emergence over time (day) </p>

<p>Update, I have just ran the model below, but it seems to be dropping a treatment group?</p>

<p>Ran this model:</p>

<pre><code>&gt; model6 &lt;- lmer(Emerg ~ Day + Treatment + Day:Treatment + (Day | Stream), insect)
&gt; summary(model6)

Linear mixed model fit by REML ['lmerMod']
Formula: 
Emerg ~ Day + Treatment + Day:Treatment + (Day | Stream)
   Data: insect

REML criterion at convergence: 2070.8

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.0321 -0.4694 -0.0445  0.3883  4.8618 

Random effects:
 Groups   Name        Variance  Std.Dev. Corr
 Stream   (Intercept) 672.73598 25.9372      
          Day           0.01573  0.1254  1.00
 Residual             594.41059 24.3805      
Number of obs: 224, groups:  Stream, 16

Fixed effects:
                        Estimate Std. Error t value
(Intercept)              68.7060    14.6813   4.680
Day                      -0.7632     0.8106  -0.941
TreatmentControl        -26.5467    20.7625  -1.279
TreatmentFluoxetine     -14.0357    20.7625  -0.676
TreatmentMix            -15.0879    20.7625  -0.727
Day:TreatmentControl      0.6181     1.1464   0.539
Day:TreatmentFluoxetine   1.5500     1.1464   1.352
Day:TreatmentMix          1.3808     1.1464   1.204

Correlation of Fixed Effects:
            (Intr) Day    TrtmnC TrtmnF TrtmnM Dy:TrC Dy:TrF
Day         -0.343                                          
TrtmntCntrl -0.707  0.243                                   
TrtmntFlxtn -0.707  0.243  0.500                            
TreatmentMx -0.707  0.243  0.500  0.500                     
Dy:TrtmntCn  0.243 -0.707 -0.343 -0.172 -0.172              
Dy:TrtmntFl  0.243 -0.707 -0.172 -0.343 -0.172  0.500       
Dy:TrtmntMx  0.243 -0.707 -0.172 -0.172 -0.343  0.500  0.500
&gt; 
</code></pre>
"
"0.0828552264999161","0.0808452083454443","195385","<p>I am trying to specify a model in R's lme4 package in which I have 2 correlations between random intercept and random slopes, but the random slopes are not allowed to correlate.</p>

<pre><code>lmer (Y ~ A + B + (1+A+B|Subject), data=mydata)
</code></pre>

<p>is bad because it models correlation between the random slopes for A and B.</p>

<p>Whereas:</p>

<pre><code>lmer (Y ~ A + B + (1+A|Subject) + (1+B|Subject), data=mydata)
</code></pre>

<p>is bad because the random intercept for Subject gets introduced into the model twice. Is there a third way, perhaps more hack-ish?</p>
"
"0.276419754691327","0.284293110946452","197435","<p>This is my first time posting. I hope I've included an appropriate amount of info. I have many questions, but try to highlight the obstacles I've been facing.</p>

<p>I am trying to run three GLMMs in R (3.2.2) on three separate response variables (various measures of sociality between pairs of animals), but with the same set of fixed effects and interactions. Two of the response variables are integers, but the other is a continuous numeric. The response variables are:</p>

<ol>
<li>traveling together- out of all the times I followed my subject, how many of those times were the other individuals present (<strong>Travel</strong>)</li>
<li>proximity- out of all the instantaneous scans to see who was within 5m of my subject, how often was that other individual within 5m (<strong>Within_Five</strong>) </li>
<li>touching- out of all the hours/minutes I watched my subject how many minutes spent touching (<strong>total_touch</strong>)</li>
</ol>

<p>The fixed effects are features of the pair, such as age difference (<strong>Age_Diff</strong>) and whether the partner is the <strong>mother</strong>, <strong>brother</strong>, or <strong>cousin</strong>. I want to have age of the subject (<strong>subject_age</strong>) as an interaction as the social behavior may change with age.</p>

<p>Given Subjects (a subset of the population that I observed) and Partners (anyone in the population who they may be interacting with, including ) are repeated, I am treating both as random effects, e.g. (1 | Partner). </p>

<pre><code>&gt; str(dat)
'data.frame':   954 obs. of  29 variables:
 $ Pair          : Factor w/ 954 levels ""Abrams_Barron"",..: 159 268 269 378     700 334 601 920 179 75 ...
 $ Subject       : Factor w/ 18 levels ""Abrams"",""Barron"",..: 3 6 6 8 14 7 12 18 4 2 ...
  $ Partner       : Factor w/ 54 levels ""Abrams"",""Barron"",..: 54 3 4 7 11 16    18 19 21 23 ...
 $ mother        : Factor w/ 2 levels ""NoMom"",""Mom"": 1 1 1 1 1 1 1 1 1 1 ...
 $ brother       : Factor w/ 2 levels ""NoBro"",""bro"": 2 2 2 2 2 2 2 2 2 2 ...
 $ cousin        : Factor w/ 2 levels ""NoCuz"",""cuz"": 2 1 1 1 1 1 1 1 1 1 ...
 $ subject_age   : num  14.3 17.7 17.7 18.7 19.7 ...
 $ partner_age   : num  8.67 41.69 31.69 12.39 24.68 ...
 $ Age_Diff      : num  -5.59 24.01 14.01 -6.29 5 ...
 $ Total_Scans   : int  314 309 309 313 289 314 310 321 305 283 ...
 $ Total_Hours   : num  43.2 44.3 44.3 45.1 40.9 ...
 $ Total_Minutes : num  2593 2656 2656 2707 2456 ...
 $ Total_Follows : int  45 46 46 47 43 48 46 48 46 45 ...
 $ Within_Five   : int  9 13 12 20 26 10 6 4 30 9 ...
 $ total_touch   : num  0 0 1.77 6.19 31.07 ...
 $ Touch_Rate_Min: num  0 0 0.000667 0.002287 0.012649 ...
 $ Touch_Rate    : num  0 0 0.04 0.137 0.759 ...
 $ Travel        : int  25 28 27 24 30 21 23 5 26 17 ...
 $ Travel_Rate   : num  0.556 0.609 0.587 0.511 0.698 ...
</code></pre>

<p>I have used lme4 in the past, but since some of my response variables have lots of zeros, it seems appropriate to run a zero-inflation model, and I have been trying glmmADMB. With some of the models, I have been getting warning messages.</p>

<p>For the Travel model, I don't think I need a zero-inflation model, but I get a warning when I run a poisson model with glmer</p>

<pre><code>&gt; travel.poisson.FULL2 &lt;- glmer(Travel ~ subject_age*brother + subject_age*Age_Diff + subject_age*cousin + subject_age*mother + log(Total_Follows) + (1|Subject) + (1|Partner), dat, family = poisson)
Warning messages:
1: Some predictor variables are on very different scales: consider rescaling 
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.0120444 (tol = 0.001, component 1)
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: very large eigenvalue
 - Rescale variables?;Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
&gt; summary(travel.poisson.FULL2)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: poisson  ( log )
Formula: Travel ~ subject_age * brother + subject_age * Age_Diff + subject_age *      cousin + subject_age * mother + log(Total_Follows) + (1 |  
    Subject) + (1 | Partner)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
  8989.2   9052.4  -4481.6   8963.2      941 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.5702 -1.7767 -0.3958  0.9003 16.4475 

Random effects:
 Groups  Name        Variance Std.Dev.
 Partner (Intercept) 0.25078  0.5008  
 Subject (Intercept) 0.01283  0.1133  
Number of obs: 954, groups:  Partner, 54; Subject, 18

Fixed effects:
                         Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)            -4.9997116  1.7689865  -2.826  0.00471 ** 
subject_age             0.0246246  0.0138692   1.775  0.07582 .  
brotherbro              1.2107468  0.4163344   2.908  0.00364 ** 
Age_Diff                0.0226314  0.0086427   2.619  0.00883 ** 
cousincuz               1.0076641  0.3756843   2.682  0.00731 ** 
motherMom               1.9488415  0.6762811   2.882  0.00396 ** 
log(Total_Follows)      1.7389194  0.4464744   3.895 9.83e-05 ***
subject_age:brotherbro -0.0362557  0.0260557  -1.391  0.16408    
subject_age:Age_Diff   -0.0002216  0.0003799  -0.583  0.55972    
subject_age:cousincuz  -0.0635028  0.0236412  -2.686  0.00723 ** 
subject_age:motherMom  -0.1105007  0.0413857  -2.670  0.00758 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) sbjct_ brthrb Ag_Dff cosncz mthrMm l(T_F) sbjct_g:b s_:A_D sbjct_g:c
subject_age -0.354                                                                     
brotherbro  -0.014  0.058                                                              
Age_Diff    -0.071  0.443 -0.001                                                       
cousincuz   -0.007  0.095  0.011  0.109                                                
motherMom   -0.001  0.000  0.023 -0.126 -0.004                                         
lg(Ttl_Fll) -0.990  0.227  0.006  0.001 -0.006  0.002                                  
sbjct_g:brt  0.015 -0.056 -0.988  0.005 -0.012 -0.025 -0.008                           
sbjct_g:A_D  0.027 -0.192  0.008 -0.714 -0.150  0.168 -0.002 -0.013                    
sbjct_g:csn  0.006 -0.092 -0.012 -0.106 -0.988  0.004  0.006  0.013     0.147          
sbjct_g:mtM  0.004 -0.002 -0.024  0.123  0.003 -0.991 -0.004  0.028    -0.169 -0.003   
fit warnings:
Some predictor variables are on very different scales: consider rescaling
convergence code: 0
Model failed to converge with max|grad| = 0.0120444 (tol = 0.001, component 1)
Model is nearly unidentifiable: very large eigenvalue
 - Rescale variables?
Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p>Whereas, when I run with glmmadmb, I don't get a warning</p>

<pre><code>&gt; travel.poisson.FULL &lt;- glmmadmb(Travel ~ subject_age*brother + subject_age*Age_Diff + subject_age*cousin + subject_age*mother + log(Total_Follows) + (1|Subject) + (1|Partner), dat, family = ""poisson"", zeroInflation = F)
&gt; summary(travel.poisson.FULL)

Call:
glmmadmb(formula = Travel ~ subject_age * brother + subject_age * 
    Age_Diff + subject_age * cousin + subject_age * mother + 
    log(Total_Follows) + (1 | Subject) + (1 | Partner), data = dat, 
    family = ""poisson"", zeroInflation = F)

AIC: 7430.9 

Coefficients:
                        Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)            -9.52e+00   6.32e+00   -1.50    0.132  
subject_age             3.28e-02   4.56e-02    0.72    0.471  
brotherbro              9.49e-01   4.29e-01    2.21    0.027 *
Age_Diff                2.70e-02   1.28e-02    2.11    0.035 *
cousincuz               5.36e-02   4.27e-01    0.13    0.900  
motherMom              -1.21e+00   8.39e-01   -1.45    0.148  
log(Total_Follows)      2.75e+00   1.60e+00    1.72    0.086 .
subject_age:brotherbro -2.85e-02   2.66e-02   -1.07    0.284  
subject_age:Age_Diff    2.64e-05   4.11e-04    0.06    0.949  
subject_age:cousincuz  -3.11e-03   2.70e-02   -0.12    0.908  
subject_age:motherMom   7.76e-02   4.99e-02    1.56    0.120  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Number of observations: total=954, Subject=18, Partner=54 
Random effect variance(s):
Error in VarCorr(x) : 
  could not find symbol ""rdig"" in environment of the generic function
</code></pre>

<p>For the touching model (which has lots of 0s, so it seemed appropriate to run a zero inflation model), I also got a warning.</p>

<pre><code>touch.poisson.FULL &lt;- glmmadmb(total_touch ~ subject_age*brother + subject_age*Age_Diff + subject_age*cousin + subject_age*mother + log(Total_Scans) +  (1|Subject) + (1|Partner), dat, family = ""poisson"", zeroInflation = T)

&gt;Warning messages:
1: In glmmadmb(total_touch ~ subject_age * brother + subject_age *  :
  non-integer response values in discrete family
2: In glmmadmb(total_touch ~ subject_age * brother + subject_age *  :
  Convergence failed:log-likelihood of gradient= -1.15888
</code></pre>

<p>What accounts for the differences in warnings produced by glmmadmb and glmer?</p>

<p>Some related questions:</p>

<p>Is it appropriate to put all of these effects and interactions into one model? Do these warnings and differences between packages suggest my model is unstable?</p>

<p>I have been trying to learn as much as possible about GLMMs and using them in R. I know there are a lot of different statistical approaches, but I want to ensure I am modeling my data appropriately and producing reliable results. Any and all help or advice is appreciated, and I am happy to provide more information.</p>
"
"0.15190124858318","0.148216215299981","199147","<p>When I use <code>lmer</code> of <code>lme4</code> to fit a random one-variable slope model with random intercept excluded, both levels of the one-variable slope are reported with random variances, as if the slope had two variables (i.e., as if it were a three-level treatment effect).  <strong>How should I interpret this?</strong></p>

<h2>Detailed Example:</h2>

<p><em>Scenario 1</em><br>
Here is what the model looks like with both random slope and intercept included.  Everything works as expected: in line 7, the binary variable <code>Cond1</code> shows up with just one effect (<code>Cond1hetero</code>, the upper level of the two-level categorical)...</p>

<pre><code>&gt; RT_log.CVquestA.lmer=lmer(RT_log~Cond1+(1+Cond1|Subject),data=basedata)
&gt; summary(RT_log.CVquestA.lmer)
...
Random effects:
 Groups   Name        Variance Std.Dev. Corr 
 Subject  (Intercept) 0.026121 0.16162       
          Cond1hetero 0.001366 0.03696  -0.47
 Residual             0.028667 0.16931       
Number of obs: 3321, groups:  Subject, 19

Fixed effects:
            Estimate Std. Error t value
(Intercept)  6.57461    0.03725  176.49
Cond1hetero  0.02815    0.01052    2.68

Correlation of Fixed Effects:
            (Intr)
Cond1hetero -0.412
</code></pre>

<p><em>Scenario 2</em><br>
Here is what the model looks like when I remove the random intercept. Note the extra variance term on line 6 of the block below (i.e., we now see an effect for <code>Cond1non-hetero</code>, the reference level of the <code>Cond1</code> categorical variable, in addition to the upper level <code>Cond1hetero</code>). <strong>I don't know how to interpret or use this output!</strong></p>

<pre><code>&gt; RT_log.CVquestB.lmer=lmer(RT_log~Cond1+(0+Cond1|Subject),data=basedata)
&gt; summary(RT_log.CVquestB.lmer)
...
Random effects:
 Groups   Name            Variance Std.Dev. Corr
 Subject  Cond1non-hetero 0.02612  0.1616       
          Cond1hetero     0.02184  0.1478   0.98
 Residual                 0.02867  0.1693       
Number of obs: 3321, groups:  Subject, 19

Fixed effects:
            Estimate Std. Error t value
(Intercept)  6.57461    0.03725  176.49
Cond1hetero  0.02815    0.01052    2.68

Correlation of Fixed Effects:
            (Intr)
Cond1hetero -0.412
</code></pre>
"
"0.15190124858318","0.148216215299981","200705","<p>When I use <code>lmer</code> of <code>lme4</code> to fit a random one-variable slope model with random intercept excluded, both levels of the one-variable slope are reported with random variances, as if the slope had two variables (i.e., as if it were a three-level treatment effect).  <strong>How should I interpret this?</strong></p>

<h2>Detailed Example:</h2>

<p>Here is what the model looks like with both random slope and intercept included.  Everything works as expected: in line 7, the binary variable <code>Cond1</code> shows up with just one effect (<code>Cond1hetero</code>, the upper level of the two-level categorical)...</p>

<pre><code>&gt; RT_log.CVquestA.lmer=lmer(RT_log~Cond1+(1+Cond1|Subject),data=basedata)
&gt; summary(RT_log.CVquestA.lmer)
...
Random effects:
 Groups   Name        Variance Std.Dev. Corr 
 Subject  (Intercept) 0.026121 0.16162       
          Cond1hetero 0.001366 0.03696  -0.47
 Residual             0.028667 0.16931       
Number of obs: 3321, groups:  Subject, 19

Fixed effects:
            Estimate Std. Error t value
(Intercept)  6.57461    0.03725  176.49
Cond1hetero  0.02815    0.01052    2.68

Correlation of Fixed Effects:
            (Intr)
Cond1hetero -0.412
</code></pre>

<p>Here is what the model looks like when I remove the random intercept. Note the extra variance term on line 6 of the block below (i.e., we now see an effect for <code>Cond1non-hetero</code>, the reference level of the <code>Cond1</code> categorical variable, in addition to the upper level <code>Cond1hetero</code>). <strong>I don't know how to interpret or use this output!</strong></p>

<pre><code>&gt; RT_log.CVquestB.lmer=lmer(RT_log~Cond1+(0+Cond1|Subject),data=basedata)
&gt; summary(RT_log.CVquestB.lmer)
...
Random effects:
 Groups   Name            Variance Std.Dev. Corr
 Subject  Cond1non-hetero 0.02612  0.1616       
          Cond1hetero     0.02184  0.1478   0.98
 Residual                 0.02867  0.1693       
Number of obs: 3321, groups:  Subject, 19

Fixed effects:
            Estimate Std. Error t value
(Intercept)  6.57461    0.03725  176.49
Cond1hetero  0.02815    0.01052    2.68

Correlation of Fixed Effects:
            (Intr)
Cond1hetero -0.412
</code></pre>
"
"0.0828552264999161","0.0808452083454443","203717","<p>I am trying to do model simplification looking at how different factors may affect distance. So I have snails kept in several habitats and I wanted to see if that affects how closely another snail may follow that snail. So I start off with this model: </p>

<pre><code>  model1 &lt;- lmer(sqrt(dist+6)~  (1|snail)+food+stress+food:stress+
       weight+OriginalL+FollowedL)
summary(model1)
</code></pre>

<p>and the summary is this: </p>

<pre><code>  Linear mixed model fit by REML ['lmerMod']
  Formula: sqrt(dist + 6) ~ (1 | snail) + food + stress + food:stress +  
weight + OriginalL + FollowedL

REML criterion at convergence: 561.1

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.2941 -0.7698 -0.3347  0.7515  1.9564 

Random effects:
 Groups   Name        Variance Std.Dev.
 snail    (Intercept) 0.000    0.000   
 Residual             2.334    1.528   
Number of obs: 148, groups:  snail, 37

Fixed effects:
                               Estimate Std. Error t value
(Intercept)                    4.960927   0.662947   7.483
foodSweetPotato               -0.219039   0.357768  -0.612
stressshelter                 -0.246649   0.355999  -0.693
weight                         0.002520   0.063259   0.040
OriginalL                      0.015549   0.013072   1.189
FollowedL                     -0.008044   0.005972  -1.347
foodSweetPotato:stressshelter -0.300143   0.503215  -0.596

Correlation of Fixed Effects:
            (Intr) fdSwtP strsss weight OrgnlL FllwdL
foodSwetPtt -0.309                                   
stressshltr -0.315  0.502                            
weight      -0.615  0.008  0.009                     
OriginalL   -0.617 -0.021  0.032  0.123              
FollowedL   -0.470  0.118  0.059  0.087 -0.004       
fdSwtPtt:st  0.230 -0.707 -0.708 -0.008 -0.024 -0.055
</code></pre>

<p>Should I remove the least significant factor or remove the interactions first?</p>

<p>And after this is it a simple anova between my first model and most simplified model?</p>
"
"0.10696563746014","0.104370715180858","205759","<p>I have a data matrix with 4 independent variables and 1 binary dependent variable, I tried to look at the correlation among 4 independent variables using 'cor' function, the output is shown below: </p>

<pre><code>     V2            V3            V4             V5  
</code></pre>

<p>V2  1.00000000    &nbsp;&nbsp;-0.25589680 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   0.01428574   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  0.6562951  <br/><br/>
V3 -0.25589680   &nbsp;  1.00000000   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.01377967  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  -0.1561370  <br/><br/>
V4  0.01428574   &nbsp;&nbsp; 0.01377967   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1.00000000   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  0.6011102  <br/><br/>
V5  0.65629506  &nbsp;&nbsp;  -0.15613703   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 0.60111022   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  1.0000000  <br/><br/></p>

<hr/> 

<p>then I tried to run a mixed model using package 'glmer' which also give a correlation output 'correlation of fixed effects', in the model all 4 independent variable are fixed term, the output is shown below:</p>

<pre><code>       V5         V2        V3          V4
</code></pre>

<p>V2      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.408  <br/>
V3    &nbsp;&nbsp;&nbsp;&nbsp;  -0.577    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  0.208  <br/>
V4     &nbsp;&nbsp;&nbsp;&nbsp; -0.236    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  0.581     &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-0.110  <br/>
V5       &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0.266   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  -0.794   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  -0.047  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;   -0.775  <br/></p>

<hr/> 

<p>I am wondering why the two outputs are not consistent? since they are based on the same dataset.</p>

<p>Does the glmer output 'correlation of fixed effects' have a different meaning with 'correlation'?</p>

<p>Thanks in advance.</p>

<p>Bo</p>
"
"0.197234889993286","0.192450089729875","207395","<p>I have bird nesting data and I am trying to see whether the nest treatment has any significant effects on the survival of the nestling. My original data set is relatively small (n=101). The response variable is binomial (No treatment = 0,  treatment = 1) as is the fixed effect (survived = 1, died = 0). </p>

<p>A copy of my original data set is available <a href=""https://drive.google.com/file/d/0B2vynKP39eZed1pwMFh4ekhGV00/view?usp=sharing"" rel=""nofollow"">here</a>. </p>

<p>I have obtained the following results from my model:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood  (Laplace Approximation)
  ['glmerMod']
Family: binomial  ( logit )
Formula: Survived ~ Treatment + (1 | Nest) + (1 | Year)
Data: Treatment_original
Control: glmerControl(optimizer = ""bobyqa"", optCtrl = list(maxfun = 4e+05))

  AIC      BIC   logLik deviance df.resid 
109.8    120.2    -50.9    101.8       97 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.9725  0.1557  0.2853  0.3653  1.2021 

Random effects:
Groups Name        Variance Std.Dev.
Nest   (Intercept) 3.2860   1.8127  
Year   (Intercept) 0.5109   0.7148  
Number of obs: 101, groups:  Nest, 39; Year, 7

Fixed effects:
    Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   1.6228     0.7258   2.236   0.0254 *
Treatment     0.9063     0.7676   1.181   0.2377  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
  (Intr)
Treatment -0.152
</code></pre>

<p>To account for the possible influence of small sample size, I produced a bootstrap data set using the following code:</p>

<pre><code>bootstrapdata &lt;- data.frame()
for (i in 1:1000){
  boot &lt;- sample(1:nrow(Treatment_original), replace=TRUE)
  bootdata &lt;- Treatment_original[boot,]
  bootstrapdata &lt;- rbind(bootstrapdata, bootdata)
}
</code></pre>

<p>The bootstrap data set is available <a href=""https://drive.google.com/file/d/0B2vynKP39eZeS3VQVHcwMmw4MkE/view?usp=sharing"" rel=""nofollow"">here</a>.</p>

<p>I then ran the above model on the bootstrap data set, which produced the following results:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
  ['glmerMod']
Family: binomial  ( logit )
Formula: Survived ~ Treatment + (1 | Nest) + (1 | Year)
Data: Treatment_bootstrap
Control: glmerControl(optimizer = ""bobyqa"", optCtrl = list(maxfun = 4e+05))

 AIC      BIC   logLik deviance df.resid 
2957.0   2985.8  -1474.5   2949.0     9996 

Scaled residuals: 
  Min      1Q  Median      3Q     Max 
-3.5915  0.0001  0.0002  0.0026  2.4168 

Random effects:
Groups Name        Variance Std.Dev.
Nest   (Intercept) 511.888  22.625  
Year   (Intercept)   4.251   2.062  
Number of obs: 10000, groups:  Nest, 38; Year, 7

Fixed effects:
    Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  16.0123     1.9144   8.364   &lt;2e-16 ***
Treatment     1.5813     0.1465  10.795   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
  (Intr)
Treatment 0.009 
</code></pre>

<p>I would like to know how to interpret the bootstrap model results. Can I now say that the nest treatment had a positive effect on nestling survival?</p>

<p>The original data set showed no significant effect of nest treatment. Should I rather be using these results and adjusting the p value for false discovery rate?</p>

<p>I am unsure as to which results are correct. Should I report both results? What inferences can I make from these results? </p>
"
"0.15190124858318","0.161690416690889","208006","<p>I have data from a incomplete factorial experiment with repeated measures and potential nesting and am trying to figure out 1) the right way to design the mixed model to analyze the data, and 2) how to code with either <code>lmer</code> or <code>lme</code>.</p>

<p><strong>1: Model design</strong> - I have treatment Till with three levels (N,C,O) and treatment Rot with two levels (W,F).  Each of the Rot treatments only occur in one year, so only W occurred in 2013 and only F occurred in 2014. This is not an ideal design of course, but it's what I have to work with - I'd like to still call the treatment Rot but explain in the manuscript's text the link with year and related caveats. The design is full factorial between Till and Rot except that one combination (W X O) is missing.  There are three replicate Plot#s for each Till level that were measured at 5 unequally spaced Timepoint#s each of the two growing seasons (i.e., under each Rot level). There are a number of response variables I'd like to predict based on those elements of experiment design (Fixed = Till, Rot; Random = Plot#, Timepoint).</p>

<p>My thought is that this design is appropriate but I want to check with those who are more knowledgeable:</p>

<p>Fixed effects = Till crossed with Rot</p>

<p>Random effects = Plot# nested within Till; Timepoint# nested within Rot or just Timepoint# as a random effect on it's own</p>

<p><img src=""http://i.imgur.com/iv2t39j.jpg?1"" alt=""*Image of Experimental Design*""></p>

<p><strong>2: Model syntax</strong> - I plan to test for auto-correlation over Timepoint# using <code>lme</code> or <code>lmer</code> in R.  I know how to do that in <code>lme</code> but I don't think there's functionality to do so in <code>lmer</code>.  I've read a bit about model syntax for both those functions but am still not certain I'm coding my models correctly so a suggestion of how to code the correct model design in either of those functions would be extremely helpful, too. </p>

<p>I have read through a number of Cross-validated and Stack Overflow posts and have read a bit of Zuur 2009 but haven't been able to confidently determine the right model structure or R syntax.  </p>

<p>Any help is very much appreciated - thanks!</p>
"
"0.135302018298348","0.13201967239689","209645","<p>I am a little bit confused about how to operationalize the random factor in a 3-level design. </p>

<p>I think I have 3 level design. Or is this already wrong?</p>

<p>Level 3: Time of measurement (variable name: session)
Level 2: Subjects (27 in total) (variable name: subject)
Level 1: 13 data points per subject per time of measurement (variable name: SSA)</p>

<p>The time variable is the index variable that counts from 1 to 13 for each of the SSA data points. </p>

<p>This is my current syntax for one of my analyses: </p>

<p>Model_1 &lt;-lme(Y ~  Time + SSA + Session, data = YXZ,  random = ~ Time / Session | Subject,  correlation = corCAR1())</p>

<p>I am no quite confused how to properly write the random factor. Do I write <strong>random = ~ Time / Session | Subject</strong> or <strong>random = ~ Session / Time | Subject</strong> or something completely different? </p>

<p>I know that my question may sound silly but I could not find any thread that specifically answered this question and was so easily written that I could understand it :) </p>

<p>If you think, however, that nlme is not the right package for these nested random terms than I would really appreciate it if you would answer my question for the lmer function in lme4. </p>

<p>Thanks a lot! 
Julia</p>
"
"0.219441136745755","0.214117626240387","210757","<p>I have repeated measures of <code>happiness</code> for a sample of participants, and a single measure of <code>satisfaction</code> for each of the participants.</p>

<p>I want to predict <code>satisfaction</code> from the repeated measures of <code>happiness</code>. To do so, I want to create a new variable, called <code>happiness.change</code>, which measures the degree of change/trend in <code>happiness</code> from the first measurement to the last, for each participant (for example, a negative <code>happiness.change</code> if there is a decrease in <code>happiness</code> over time). Then I want to predict <code>satisfaction</code> from <code>happiness.change</code>.</p>

<p>Below (using R) is an excerpt from my data (a sample of 9 participants):</p>

<pre><code>ids &lt;- c(rep(seq(1:5), each = 2), rep(6:9, each = 5))
time &lt;- c(rep(1:2, 5), rep(1:5, 4))
happiness &lt;- c(0.80,0.00,0.75,0.00,0.80,0.00,2.75,2.50,0.40,0.20,
               3.80,0.40,0.00,0.20,3.40,3.00,2.60,3.40,3.80,0.00,
               3.60,3.60,0.20,0.40,1.00,0.40,0.20,1.20,1.20,0.00)
satisfaction &lt;- c(6,6,2,2,3,3,2,2,2,2,5,5,5,5,5,7,7,7,7,7,1,1,1,1,1,2,2,2,2,2)
data &lt;- as.data.frame(matrix(c(ids, time, happiness, satisfaction),
                         nrow = 30,
                         ncol = 4,
                         dimnames = list(c(),c(""id"", ""time"",
                                               ""happiness"", ""satisfaction""))))
print(data)

#    id time happiness satisfaction
# 1   1    1      0.80            6
# 2   1    2      0.00            6
# 3   2    1      0.75            2
# 4   2    2      0.00            2
# 5   3    1      0.80            3
# 6   3    2      0.00            3
# 7   4    1      2.75            2
# 8   4    2      2.50            2
# 9   5    1      0.40            2
# 10  5    2      0.20            2
# 11  6    1      3.80            5
# 12  6    2      0.40            5
# 13  6    3      0.00            5
# 14  6    4      0.20            5
# 15  6    5      3.40            5
# 16  7    1      3.00            7
# 17  7    2      2.60            7
# 18  7    3      3.40            7
# 19  7    4      3.80            7
# 20  7    5      0.00            7
# 21  8    1      3.60            1
# 22  8    2      3.60            1
# 23  8    3      0.20            1
# 24  8    4      0.40            1
# 25  8    5      1.00            1
# 26  9    1      0.40            2
# 27  9    2      0.20            2
# 28  9    3      1.20            2
# 29  9    4      1.20            2
# 30  9    5      0.00            2
</code></pre>

<p>To create <code>happiness.change</code> I was advised to use the coefficients produced by either of these equations:</p>

<pre><code>require(lme4)
model1 &lt;- lmer(happiness ~ time + (time | id), data = data)

require(nlme)
model2 &lt;- lme(happiness ~ time, random = ~1 + time | id, data = data)
</code></pre>

<p>For example, running <code>coef(model1)</code> produces the following coefficients (column <code>time</code>):</p>

<pre><code># $id
#   (Intercept)        time
# 1   0.9936158 -0.05991770
# 2   0.9739595 -0.05674569
# 3   0.9936158 -0.05991770
# 4   2.5539086 -0.31170766
# 5   0.8998610 -0.04478815
# 6   2.0446747 -0.22953078
# 7   3.2906564 -0.43059926
# 8   2.7120530 -0.33722798
# 9   1.0027053 -0.06138450
# 
# attr(,""class"")
# [1] ""coef.mer""
</code></pre>

<p>And then connecting between <code>satisfaction</code> and the coefficients:</p>

<pre><code>coefs1 &lt;- as.data.frame(unlist(coef(model1))[10:18])
satisfactionData &lt;- reshape(data,
                            direction = ""wide"",
                            idvar = ""id"",
                            timevar = ""time"")[c(1,3)]
newData &lt;- cbind(satisfactionData, coefs1)
colnames(newData) &lt;- c(""id"", ""satisfaction"", ""happiness.change"")
print(newData)

#    id satisfaction happiness.change
# 1   1            6      -0.05991770
# 3   2            2      -0.05674569
# 5   3            3      -0.05991770
# 7   4            2      -0.31170766
# 9   5            2      -0.04478815
# 11  6            5      -0.22953078
# 16  7            7      -0.43059926
# 21  8            1      -0.33722798
# 26  9            2      -0.06138450
</code></pre>

<hr>

<p><strong>I have several questions:</strong></p>

<ol>
<li><p>Running <code>model2</code> generates the following error:</p>

<pre><code>Error in lme.formula(happiness ~ time, random = ~1 + time | id, data = data) : 
  nlminb problem, convergence error code = 1
  message = iteration limit reached without convergence (10)
</code></pre>

<p>I don't remember where, but I read that running <code>lme(happiness ~ time, random = ~1 + time | id, control = list(opt = ""optim""), data = data)</code> bypasses this error, and indeed it does. But what exactly does it do? Do the coefficients produced by using this model still represent the change in <code>happiness</code> over time?</p></li>
<li><p>Running <code>model2</code> (with <code>control = list(opt = ""optim"")</code>) produces slightly (<em>very</em> slightly) different coefficients than <code>model1</code>; why? What is the difference between the models?</p></li>
<li><p>What would be the suitable method for testing the relationship between <code>happiness.change</code> and <code>satisfaction</code>? I tried <code>cor.test(newData$satisfaction, newData$happiness.change)</code>, which produced the following results, but I'm not sure how to interpret them (they are significant in the complete data set):</p>

<pre><code>    Pearson's product-moment correlation

data:  newData$satisfaction and newData$happiness.change
t = -0.72735, df = 7, p-value = 0.4906
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 -0.7901064  0.4843018
sample estimates:
       cor 
-0.2650786 
</code></pre></li>
<li><p>As you can see, <code>model1</code> produces negative coefficients for all the participants (<code>model2</code> as well). Even for participants whose change over time is purely positive (for example, a participant with 2 <code>happiness</code> measures: <code>time1 = 0</code> and <code>time2 = 0.8</code>; no such example in the sample above, but it is in my data).</p>

<p>This may be a problem, because I want to be able to distinguish between participants whose change in <code>happiness</code> is positive (<code>happiness</code> increases over time) and participants whose change in <code>happiness</code> is negative (<code>happiness</code> decreases over time); and then see whether there's a difference between these participants in their relationship between <code>happiness.change</code> and <code>satisfaction</code>. </p>

<p>So my question is this: Is it statistically ""legit"" to divide my participants beforehand into groups based on their ""raw"" change in <code>happiness</code> (for example, I would label a participant with <code>time1 = 0</code> and <code>time2 = 0.8</code> as having a positive change), and then model the coefficient for these groups separately?</p>

<p>However, creating sub-groups this way may be difficult with participants with more than 2 measures of <code>happiness</code>, which brings me to my final question:</p></li>
<li><p>If I understand correctly, <code>model1</code> and <code>model2</code> assume there is a linear change over time. However, I checked the whole sample (424 participants), and a cubic model (<code>happiness ~ time</code>) actually explains more of the variance in <code>happiness</code>. So I suppose a cubic change over time is more appropriate. How can I create a new variable reflecting such a change in <code>happiness</code> over time?</p></li>
</ol>

<p>I realize this is quite long and these are a lot of questions, and so any help will be greatly appreciated. If anyone can answer even one of these questions, I will be very grateful.</p>

<p>Thanks!</p>
"
"0.0676510091491738","0.066009836198445","211270","<p>I have a question about my experimental design and how to treat it statistically.</p>

<p>Briefly, I am interesting in how herbicide application affects plant biomass and is it any differences between regions.</p>

<p>We have 5 regions (they are different in mean annual temperature and precipitation), within each region there are 7 plots. On the each plot two types of treatment were applied (control and herbicide, 1 level) and then, plant biomass was cut in duplicate from the each of the ""subplots"" (with different type of treatment). We cut it 10 times per summer. So, this part is probably repeated measures, but what about plots location?</p>

<p>I am going to use <em>lme</em> function:</p>

<p><em>lme(plantbiomass~treatment+region,data,random=~1|plot/duplicate)</em>.</p>

<p>Is that correct? Do I need include ""subplot"" as random factor as well? Or, maybe it would be better to use lmer (as it accounts for possible correlation between time points?).</p>
"
"0.172918590828624","0.180775381515547","212397","<p>I would like to test the effect of a treatment (""crop"") on species richness. I would rather use a glm for richness as it is a kind of count data.</p>

<p>Besides, I have a nested sampling design (5 values per plot, 5 plot per treatment). Thus I should use a GLMM.</p>

<p>So I write my model :</p>

<pre><code>&gt; GLMM_ric = glmer(richness ~ Crop + (1| Plot),  family=poisson)
&gt; summary(GLMM_ric)

Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [
 glmerMod]
 Family: poisson ( log )
 Formula: richness ~ Crop + (1 | Plot)
 Data: Com_agg

 AIC      BIC   logLik deviance df.resid 
433.8    446.9   -211.9    423.8       95 

Scaled residuals: 
   Min       1Q   Median       3Q      Max 
-1.33174 -0.41445 -0.08382  0.39853  1.73324 

Random effects:
 Groups Name        Variance Std.Dev.
  Plot   (Intercept) 0.08432  0.2904  
 Number of obs: 100, groups: Plot, 20

 Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
 (Intercept)   1.9621     0.1503  13.056   &lt;2e-16 ***
 CropM        -0.5351     0.2211  -2.420   0.0155 *  
 CropYR       -0.3814     0.2181  -1.748   0.0804 .  
 CropOR       -0.3393     0.2175  -1.560   0.1188    
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

 Correlation of Fixed Effects:
        (Intr) CropM  CropYR
 CropM  -0.678              
 CropYR -0.686  0.467       
 CropOR -0.687  0.468  0.475
</code></pre>

<p>and then a simpler model to compare with :</p>

<pre><code> &gt; GLMM_ric0 = glmer(richness ~ (1| Plot), data=Com_agg, family=poisson,    glmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun = 100000)))

 &gt;summary(GLMM_ric0)

 Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [ glmerMod]
  Family: poisson ( log )
 Formula: richness ~ (1 | Plot)
    Data: Com_agg
 Control: glmerControl(optimizer = ""bobyqa"", optCtrl = list(maxfun = 1e+05))

 AIC      BIC   logLik deviance df.resid 
 433.3    438.5   -214.7    429.3       98 

 Scaled residuals: 
 Min       1Q   Median       3Q      Max 
 -1.27211 -0.39830 -0.03309  0.38204  1.66734 

 Random effects:
  Groups Name        Variance Std.Dev.
  Plot   (Intercept) 0.1251   0.3537  
 Number of obs: 100, groups: Plot, 20

 Fixed effects:
        Estimate Std. Error z value Pr(&gt;|z|)    
 (Intercept)  1.64739    0.09114   18.07   &lt;2e-16 ***
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>And then I compare both models :</p>

<pre><code>&gt; anova(GLMM_ric0, GLMM_ric)
Data: Com_agg
Models:
GLMM_ric0: richness ~ (1 | Plot)
GLMM_ric: richness ~ Crop + (1 | Plot)
              Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)
GLMM_ric0  2 433.32 438.53 -214.66   429.32                         
GLMM_ric   5 433.84 446.86 -211.92   423.84 5.4851      3     0.1395
</code></pre>

<p>So according to my anova, the factor ""crop"" is not significant. Yet in the summary of my model some of the modalities appear to be significant. How should I interpret this ?</p>

<p>I have looked around for a while (e.g. <a href=""http://stats.stackexchange.com/questions/9587/glmm-test-of-significance"">here</a> or <a href=""http://glmm.wikidot.com/faq"" rel=""nofollow"">here</a>) but I could not find much for this precise situation.</p>
"
"0.178987746151269","0.174645610665195","213072","<p>Iâ€™m using the <code>glmer</code> function from the <code>lme4</code> package in <code>R</code> to model species richness adjacent to aquaculture sites. I have 6 sites: 2 in production, 2 were in production the last years but not anymore at the time of the sampling (fallow), and 2 that were never under production (references). Photographs along transects away from the aquaculture sites were taken each 20-40 m from 0 to 200 m and reference sites were at 1500 m from aquaculture sites. These transects were repeated 7 times over a period of 2 years to determine if the community changed over time.</p>

<p>Iâ€™ve followed the steps described in the excellent book from Zuur et al. (2009) <em><a href=""http://rads.stackoverflow.com/amzn/click/1441927646"" rel=""nofollow"">Mixed Effects Models and Extensions in Ecology with R</a></em> and my best model is:</p>

<p>(<em>Note that predictors</em> <code>Distance</code>, <code>Depth</code> <em>and</em> <code>Beggiatoa.sp.</code> <em>have been standardized to remove an</em> <code>lme4</code> <em>error message.</em>)</p>

<pre><code>glmm.8 &lt;- glmer(sr ~ Distance+Depth+fSubstrate+Beggiatoa.sp.+
                     Distance:Beggiatoa.sp.+(1|fSite),
                glmerControl(optimizer=""bobyqa"", optCtrl=list(maxfun=100000)),
                family=poisson, data=datsc)

summary(glmm.8)

Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
   ['glmerMod']
Family: poisson  ( log )
Formula: sr ~ Distance + Depth + fSubstrate + Beggiatoa.sp. + 
   Distance:Beggiatoa.sp. +      (1 | fSite)
Data: datsc
Control: glmerControl(optimizer = ""bobyqa"", optCtrl = list(maxfun = 1e+05))

   AIC      BIC   logLik deviance df.resid 
2279.7   2328.8  -1129.9   2259.7      992 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.5171 -0.6376 -0.2008  0.4326  4.9375 

Random effects:
Groups Name        Variance Std.Dev.
fSite  (Intercept) 0.1831   0.4279  
Number of obs: 1002, groups:  fSite, 6

Fixed effects:
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)             1.49171    0.50388   2.960 0.003072 ** 
Distance                2.05809    0.59940   3.434 0.000596 ***
Depth                  -0.09093    0.02966  -3.066 0.002171 ** 
fSubstrateCoarse       -0.09929    0.08299  -1.196 0.231514    
fSubstrateFine         -0.62376    0.08606  -7.248 4.24e-13 ***
fSubstrateFloc         -1.75314    0.30211  -5.803 6.51e-09 ***
fSubstrateMedium       -0.35201    0.07625  -4.617 3.90e-06 ***
Beggiatoa.sp.           2.42190    1.09521   2.211 0.027011 *  
Distance:Beggiatoa.sp.  3.30995    1.37755   2.403 0.016271 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Distnc Depth  fSbstC fSbstrtFn fSbstrtFl fSbstM Bggt..
Distance     0.885                                                       
Depth        0.052  0.027                                                
fSubstrtCrs -0.076 -0.028 -0.024                                         
fSubstratFn -0.177 -0.058 -0.145  0.325                                  
fSubstrtFlc  0.047  0.132 -0.022  0.066  0.155                           
fSubstrtMdm -0.088 -0.029 -0.102  0.314  0.380     0.097                 
Beggiat.sp.  0.927  0.947  0.039 -0.024 -0.088     0.080    -0.027       
Dstnc:Bgg..  0.925  0.950  0.037 -0.024 -0.089     0.119    -0.027  0.996
</code></pre>

<p><strong>My question is: How do I validate this model to see if it meets the required assumptions?</strong></p>

<p>I did a series of plots but I'm not sure if they are the appropriate ones and if they are, if they violate the assumptions.</p>



<pre><code>EP &lt;- residuals(glmm.8,type=""pearson"")
plot(EP~fitted(glmm.8))
</code></pre>

<p><a href=""http://i.stack.imgur.com/K8YnQ.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/K8YnQ.jpg"" alt=""enter image description here""></a></p>



<pre><code>qqnorm(EP)
qqline(EP)
</code></pre>

<p><a href=""http://i.stack.imgur.com/owp47.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/owp47.jpg"" alt=""enter image description here""></a></p>



<pre><code>plot(datsc$Distance, EP, xlab=""Distance"", ylab=""Pearson Residuals"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/4X05s.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4X05s.jpg"" alt=""enter image description here""></a></p>



<pre><code>plot(datsc$Depth, EP, xlab=""Depth"", ylab=""Pearson Residuals"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/cKdYC.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/cKdYC.jpg"" alt=""enter image description here""></a></p>



<pre><code>plot(datsc$fSubstrate, EP, xlab=""Substrate"", ylab=""Pearson Residuals"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/LUgG8.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/LUgG8.jpg"" alt=""enter image description here""></a></p>



<pre><code>plot(datsc$Beggiatoa.sp., EP, xlab=""Beggiatoa.sp."", ylab=""Pearson Residuals"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/cg3oO.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/cg3oO.jpg"" alt=""enter image description here""></a></p>



<pre><code>plot(fitted(glmm.8)~predict(glmm.8))
</code></pre>

<p><a href=""http://i.stack.imgur.com/LgHDV.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/LgHDV.jpg"" alt=""enter image description here""></a></p>



<p>I looked on this and other websites and I couldn't find a ""perfect"" method to validate Poisson GLMM models. I believe a good answer to my question would be relevant to many people.  If needed I can provide a subset of my data but this question can probably be answered without it. Still, let me know if need it.</p>


"
"0.135302018298348","0.13201967239689","213470","<p><strong>Background</strong></p>

<p>Although my data should have a multinomial dependent variable, I have settled for a binary as I could not understand too much of MCMCglmm. The data is a time series cross sectional, so am looking at each individual outcome vis-a-vis the others. I don't have much experience with statistics, so I really need to know if am on the right path of actually coming up with values for a dynamic linear model or way off. Since i need effects from the independent variables.</p>

<p><strong><em>The data sample</em></strong></p>

<p>The data is for students who applied for university courses and were admitted within a period of 3 years. The data mainly has the grades in the subjects done in their pre-entry level exams. Each student can do a maximum of 4, but in the model below <code>NA</code> values are filled with <code>0</code> (Not sure if a correct assumption). </p>

<p><strong>The problem</strong></p>

<ol>
<li>How do I get time varying effects?</li>
<li>How do i extract the effect of time? </li>
<li>what does it imply when time is expressed as a random effect?</li>
</ol>

<p>Every input is highly appreciated, Thank you.</p>

<p>Below is a result from the <code>glmer</code> function with formula</p>

<p><code>glmllb2 &lt;- glmer(logi ~ history + c.r.e + economics + geography + literature + f.art + entrepreneurship + luganda + kiswahili + french + i.r.e + historyc + historycsq + (1 | called), family = binomial(""logit""), control = glmerControl(optimizer = ""bobyqa""), nAGQ = 100, data = data.apriori.llb2)</code></p>

<p>where <code>history+c.r.e + ... + i.r.e</code> are subject grades that predict student admission into a course <code>logi</code> (as a binary) while <code>historyc</code> is a grand mean centered variable for history and <code>historycsq</code> is the squared variable for <code>historyc</code>. <code>called</code> is time in years re-scaled to <code>1,2 and 3</code></p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Adaptive Gauss-Hermite Quadrature, nAGQ = 100) [glmerMod]
 Family: binomial  ( logit )
Formula: logi ~ history + c.r.e + economics + geography + literature +  
    f.art + entrepreneurship + luganda + kiswahili + french +      i.r.e + historyc + historycsq + (1 | called)
   Data: data.apriori.llb2
Control: glmerControl(optimizer = ""bobyqa"")

     AIC      BIC   logLik deviance df.resid 
  1778.2   1874.8   -875.1   1750.2     7317 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
 -3.843  -0.123  -0.054  -0.025 213.612 

Random effects:
 Groups Name        Variance Std.Dev.
 called (Intercept) 0.09975  0.3158  
Number of obs: 7331, groups:  called, 3

Fixed effects:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)      -13.27747    0.52128 -25.471  &lt; 2e-16 ***
history            0.55942    0.04656  12.016  &lt; 2e-16 ***
c.r.e              0.45941    0.03652  12.580  &lt; 2e-16 ***
economics          0.69835    0.04509  15.489  &lt; 2e-16 ***
geography          0.49442    0.04137  11.950  &lt; 2e-16 ***
literature         0.77936    0.04129  18.877  &lt; 2e-16 ***
f.art              0.50219    0.04387  11.447  &lt; 2e-16 ***
entrepreneurship   0.46377    0.04504  10.297  &lt; 2e-16 ***
luganda            0.49340    0.07643   6.456 1.08e-10 ***
kiswahili          0.52691    0.10498   5.019 5.20e-07 ***
french             0.65225    0.09133   7.142 9.22e-13 ***
i.r.e              0.59269    0.08265   7.171 7.44e-13 ***
historycsq         0.03721    0.01794   2.075    0.038 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) histry c.r.e  ecnmcs ggrphy litrtr f.art  entrpr lugand kiswhl french i.r.e 
history     -0.559                                                                             
c.r.e       -0.608  0.143                                                                      
economics   -0.340 -0.083 -0.002                                                               
geography   -0.569  0.187  0.624 -0.059                                                        
literature  -0.625  0.169  0.540  0.086  0.709                                                 
f.art       -0.614  0.231  0.588  0.118  0.525  0.585                                          
entrprnrshp -0.554  0.191  0.564 -0.031  0.583  0.654  0.596                                   
luganda     -0.305  0.086  0.289  0.065  0.305  0.337  0.283  0.281                            
kiswahili   -0.242  0.158  0.173  0.073  0.161  0.195  0.195  0.186  0.099                     
french      -0.265  0.079  0.314 -0.002  0.257  0.268  0.290  0.305  0.144  0.094              
i.r.e       -0.256  0.038  0.349  0.041  0.253  0.251  0.231  0.242  0.049  0.083  0.137       
historycsq   0.153 -0.253 -0.235 -0.107 -0.275 -0.204 -0.202 -0.231 -0.106 -0.112 -0.119 -0.102
fit warnings:
fixed-effect model matrix is rank deficient so dropping 1 column / coefficient
</code></pre>
"
"0.160567262912294","0.16872368941451","214645","<p>I'm studying the effect of pH and cross-types on mortality of fish. Treatment is categorical (2 levels: control and low pH) and cross-types is also categorical (4 levels: parents wild male x wild female (WMWF), wild male x farmed female (WMFF), farmed male x wild female (FMWF), and farmed male x farmed female (FMFF)). There was 6 tanks in total (3 control and 3 at low pH) and each tank had 15 fish of each cross-type (60 fish total/tank). Since mortality is a count and that there was higher mortality in one of the control tank, I used Poisson GLMM to account for the tank effect.</p>

<p>Here's the model and summary results:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
Family: poisson  ( log )
Formula: mortality.count ~ Tr * Cross + (1 | Tank)
Data: pHdat

 AIC      BIC   logLik deviance df.resid 
93.8    104.4    -37.9     75.8       15 

Scaled residuals: 
Min      1Q  Median      3Q     Max 
-1.1311 -0.4171 -0.2554  0.1608  1.2889 

Random effects:
Groups Name        Variance Std.Dev.
Tank   (Intercept) 2.225    1.492   
Number of obs: 24, groups:  Tank, 6

Fixed effects:
                Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)       -1.666e+00  1.377e+00  -1.210   0.2264  
TrLOWpH            3.053e+00  1.647e+00   1.854   0.0637 .
CrossFMWF          9.810e-01  6.770e-01   1.449   0.1473  
CrossWMFF          9.810e-01  6.770e-01   1.449   0.1474  
CrossWMWF          2.248e-05  8.165e-01   0.000   1.0000  
TrLOWpH:CrossFMWF -1.754e+00  8.378e-01  -2.094   0.0363 *
TrLOWpH:CrossWMFF -1.243e+00  7.970e-01  -1.560   0.1188  
TrLOWpH:CrossWMWF -6.190e-01  9.415e-01  -0.658   0.5109  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
        (Intr) TrLOWH CrFMWF CrWMFF CrWMWF TLOWH:CF TLOWH:CWMF
TrLOWpH     -0.835                                                
CrossFMWF   -0.358  0.299                                         
CrossWMFF   -0.358  0.299  0.727                                  
CrossWMWF   -0.296  0.248  0.603  0.603                           
TLOWH:CFMWF  0.289 -0.297 -0.808 -0.588 -0.487                    
TLOWH:CWMFF  0.304 -0.313 -0.618 -0.849 -0.512  0.614             
TLOWH:CWMWF  0.257 -0.265 -0.523 -0.523 -0.867  0.520    0.547 
</code></pre>

<p>As you can see, the fish in low pH tanks of the cross FMWF died (weakly but still) significantly less than the baseline (FMFF).</p>

<p>Now I wanted to see if there was significant differences between the cross for each treatment (not only compared to the baseline) so I used <code>lsmeans</code>. Here's the results:</p>

<pre><code>lsmeans(glmm.0,pairwise~Tr*Cross,adjust=""tukey"")
$lsmeans
Tr    Cross     lsmean        SE df  asymp.LCL asymp.UCL
CTRL  FMFF  -1.6658411 1.3770371 NA -4.3647842  1.033102
LOWpH FMFF   1.3873820 0.9065822 NA -0.3894865  3.164251
CTRL  FMWF  -0.6848201 1.2991734 NA -3.2311531  1.861513
LOWpH FMWF   0.6141089 0.9548037 NA -1.2572719  2.485490
CTRL  WMFF  -0.6848677 1.2991756 NA -3.2312050  1.861470
LOWpH WMFF   1.1251162 0.9192163 NA -0.6765147  2.926747
CTRL  WMWF  -1.6658186 1.3770335 NA -4.3647547  1.033117
LOWpH WMWF   0.7683761 0.9422428 NA -1.0783859  2.615138

Results are given on the log (not the response) scale. 
Confidence level used: 0.95 

$contrasts
contrast                     estimate        SE df z.ratio p.value
CTRL,FMFF - LOWpH,FMFF  -3.053223e+00 1.6467764 NA  -1.854  0.5828
CTRL,FMFF - CTRL,FMWF   -9.810210e-01 0.6770180 NA  -1.449  0.8342
CTRL,FMFF - LOWpH,FMWF  -2.279950e+00 1.6738073 NA  -1.362  0.8744
CTRL,FMFF - CTRL,WMFF   -9.809735e-01 0.6770224 NA  -1.449  0.8342
CTRL,FMFF - LOWpH,WMFF  -2.790957e+00 1.6537652 NA  -1.688  0.6954
CTRL,FMFF - CTRL,WMWF   -2.248243e-05 0.8165311 NA   0.000  1.0000
CTRL,FMFF - LOWpH,WMWF  -2.434217e+00 1.6666742 NA  -1.461  0.8284
LOWpH,FMFF - CTRL,FMWF   2.072202e+00 1.5822427 NA   1.310  0.8956
LOWpH,FMFF - LOWpH,FMWF  7.732732e-01 0.4935656 NA   1.567  0.7704
LOWpH,FMFF - CTRL,WMFF   2.072250e+00 1.5822445 NA   1.310  0.8956
LOWpH,FMFF - LOWpH,WMFF  2.622658e-01 0.4206134 NA   0.624  0.9986
LOWpH,FMFF - CTRL,WMWF   3.053201e+00 1.6467732 NA   1.854  0.5828
LOWpH,FMFF - LOWpH,WMWF  6.190059e-01 0.4688054 NA   1.320  0.8914
CTRL,FMWF - LOWpH,FMWF  -1.298929e+00 1.6103573 NA  -0.807  0.9928
CTRL,FMWF - CTRL,WMFF    4.754102e-05 0.4999815 NA   0.000  1.0000
CTRL,FMWF - LOWpH,WMFF  -1.809936e+00 1.5895153 NA  -1.139  0.9483
CTRL,FMWF - CTRL,WMWF    9.809985e-01 0.6770119 NA   1.449  0.8342
CTRL,FMWF - LOWpH,WMWF  -1.453196e+00 1.6029417 NA  -0.907  0.9855
LOWpH,FMWF - CTRL,WMFF   1.298977e+00 1.6103590 NA   0.807  0.9928
LOWpH,FMWF - LOWpH,WMFF -5.110073e-01 0.5164052 NA  -0.990  0.9760
LOWpH,FMWF - CTRL,WMWF   2.279928e+00 1.6738042 NA   1.362  0.8744
LOWpH,FMWF - LOWpH,WMWF -1.542672e-01 0.5563606 NA  -0.277  1.0000
CTRL,WMFF - LOWpH,WMFF  -1.809984e+00 1.5895171 NA  -1.139  0.9483
CTRL,WMFF - CTRL,WMWF    9.809510e-01 0.6770162 NA   1.449  0.8343
CTRL,WMFF - LOWpH,WMWF  -1.453244e+00 1.6029435 NA  -0.907  0.9855
LOWpH,WMFF - CTRL,WMWF   2.790935e+00 1.6537621 NA   1.688  0.6954
LOWpH,WMFF - LOWpH,WMWF  3.567401e-01 0.4927939 NA   0.724  0.9963
CTRL,WMWF - LOWpH,WMWF  -2.434195e+00 1.6666710 NA  -1.461  0.8284

Results are given on the log (not the response) scale. 
P value adjustment: tukey method for comparing a family of 8 estimates 
Tests are performed on the log scale
</code></pre>

<p>Now I don't find the significant difference identified by the GLMM.</p>

<p><strong>Why the GLMM indicates a significant difference and lsmeans not, and why do I get NAs for my df in lsmeans?</strong></p>
"
"0.126563449052859","0.123493095605825","218970","<p>For a specific analysis I want to calculate the <em>variance partition coefficient</em> (VPC). I am using the following formula:</p>

<pre><code> test &lt;- glmer(SocEenz ~ Herkomst + OuderPersoon + statusscore14 + M_SpoWeek + 
             (1|POSCODN), data = dataScaled, family = binomial)

&gt; summary(test)
     Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
       Family: binomial  ( logit )
       Formula: SocEenz ~ Herkomst + OuderPersoon + statusscore14 + M_SpoWeek +  (1 | POSCODN)
       Data: dataScaled

   AIC      BIC   logLik deviance df.resid 
  43707.5  43757.9 -21847.8  43695.5    32684 

Scaled residuals: 
   Min      1Q  Median      3Q     Max 
 -1.3263 -0.8378 -0.7164  1.1263  2.4788 

  Random effects:
 Groups  Name        Variance Std.Dev.
 POSCODN (Intercept) 0.007148 0.08455 
 Number of obs: 32690, groups:  POSCODN, 173

Fixed effects:
          Estimate Std. Error z value Pr(&gt;|z|)    
 (Intercept)   -0.56437    0.01798 -31.387  &lt; 2e-16 ***
 Herkomst1      0.49571    0.02980  16.633  &lt; 2e-16 ***
 OuderPersoon1  0.29911    0.02433  12.295  &lt; 2e-16 ***
 statusscore14 -0.09900    0.01353  -7.316 2.56e-13 ***
 M_SpoWeek     -0.08658    0.01225  -7.067 1.58e-12 ***

 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

   Correlation of Fixed Effects:
        (Intr) Hrkms1 OdrPr1 stts14
Herkomst1   -0.436                     
OuderPersn1 -0.553  0.168              
statusscr14 -0.068  0.233  0.013       
M_SpoWeek   -0.044 -0.029  0.138 -0.034
</code></pre>

<p>Because I only have information in the outcome about POSCODN at random effects I am not sure how to calculate the VPC. How can I get an extra row there with information about residuals. For example: </p>

<pre><code>  Random effects:
 Groups  Name        Variance Std.Dev.
 POSCODN (Intercept) 0.007148 0.08455 
 Residual             258.357 16.0735 
 Number of obs: 32690, groups:  POSCODN, 173
</code></pre>

<p>Or do you have other suggestions how to calculate the VPC?</p>

<p>Thanks!</p>
"
"0.143509461970482","0.140028008402801","220022","<p>I'm working on revising stats for a manuscript involving male reproductive success of deer. We measured three variables (<code>body size</code>, <code>antler size</code>, and <code>age</code>) of male deer in a captive population on an annual basis over a 6 year period, and goal was to determine the relative influence of each variable on annual reproductive success (i.e., number of fawns produced each year / not lifetime reproductive success).  I understand that my predictors are collinear and the issues created; however, there really is no way around including collinear predictors in our model as PCA's, etc. would essentially destroy the core question of our research.  We plan to use model averaging to evaluate predictors in the end so ran global model first.  Here is current code:</p>

<pre><code>global.model = glmer(Fawn ~ Age + I(Age^2) + Age*AvgAge + BodySize + I(BodySize^2) + 
                          BodySize*AvgAge + SSCM + I(SSCM^2) + SSCM*AvgAge + AvgAge + 
                          (1|Sire) + (1|Year), 
                     data=datum, family=poisson, na.action=""na.fail"")
</code></pre>

<p>Quadratic effects were included for predictors due to expected non-linearities.  Two random terms were included to account for the individual potential sires being sampled multiple times during the study and year (input as a factor) effects.  <code>AvgAge</code> is a term related to population demographics, and interactions with predictors are included.</p>

<p>So here are my questions:</p>

<ol>
<li><p>A reviewer suggested that there is temporal correlation in my data (e.g. male age at year one is correlated with male age at year two) that needs to be addressed.  He suggested including a temporal autocorrelation structure, or including Year as a numeric predictor to deal with this.  Am I missing something here or isn't this correlation the whole purpose of including the random effect for each male, coded as <code>(1|Sire)</code> in this case?  Also, including year as a numeric predictor really mucks things up because: </p>

<ol>
<li>I'm not interested in the specific effect of <code>Year</code>,  </li>
<li>there was a good bit of variability in the number of males we measured each year, and </li>
<li>it makes an already complex model more complex.</li>
</ol></li>
<li><p>Is the following code sufficient to use to screen for overdispersion?  So I assume here that if the ratio is &lt; 1 then you likely do not have issues with overdispersion?  </p>

<pre><code>overdisp.glmer(test)
# Residual deviance: 84.153 on 105 degrees of freedom (ratio: 0.801)
</code></pre></li>
</ol>
"
"0.165710452999832","0.161690416690889","221721","<p>I have a dataset containing one dependent variable which is the concentration of antibiotic needed to kill a bacteria, which was measured for several different antibiotics for three different microorganisms. The antibiotics are also divided in two groups based on their origin (synthetic or natural).</p>

<p>The data can be described as follows:</p>

<pre><code> $ ID: Factor w/ 3977 levels ""1"",""2"",""3"",""4"",..: 4 5 9 10 11 12 13 14 15 16 ...
 $ OR: Factor w/ 2 levels ""natural"", ""synthetic"": 2 2 2 2 2 2 2 2 1 2 ...
 $ MC: Factor w/ 3 levels ""M1"",""M2"",""M3"": 1 1 1 1 1 1 1 1 1 1 ...
 $ Y : num  1.745 0.125 2.301 -1.615 -2.026 ... 
</code></pre>

<p>Additionally, as you can see the dataset is quite unbalanced and as a lot of missing values.</p>

<pre><code>                    MR
OR          M1      M2        M3
natural   1267    1032       400
synthetic 2129    2044       944
</code></pre>

<p>I have specified a couple of formulas for the lmer() model.</p>

<pre><code>(a) Y ~ OR * MC + (1|ID) 
(b) Y ~ OR + MC + (1|ID)
(c) Y ~ OR + MC + (OR+MC|ID)
</code></pre>

<p>For model (a), Anova with type 3 error showed that OR:MC is not significant.</p>

<p>Model (b), shows a slope on the residuals, so i tried model c.</p>

<p>Model (c) does not run in R (Error: number of observations (=7816) &lt;= number of random effects (=27839)) so i turned to matlab (also runs on julia), and also shows the residuals to have a slope.</p>

<p>The slope in the residuals can be attributed, from what i understand, to several issues, poorly specified random effects or autocorrelation.
The fact is that there might be autocorrelation as some antibiotics differ from other antibiotics in just a few atoms.</p>

<p>Any idea on how to properly specify the model?</p>

<hr>

<p>Edit:</p>

<p>y=residuals;  x=fitted
<a href=""http://i.stack.imgur.com/kQeLh.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/kQeLh.png"" alt=""residuals vs fitted""></a></p>

<p>y=residuals;  x=observed
<a href=""http://i.stack.imgur.com/1IkYG.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1IkYG.png"" alt=""residuals vs observed""></a></p>

<p>Model d (with a random slope and intercept for all levels of OR:MC)</p>

<pre><code>(b) Y ~ OR * MC + (OR:MC|ID)
</code></pre>

<p>I believe both model b, c and now d are well specified, model b as a logLik of -7981, c of -7944 and d of -7933. Suggesting d is the better.</p>
"
"0.198336769460639","0.213896315973249","222949","<p>I am developing GLMM's in order to assess habitat selection (using GLMMs' coeficients to construct Resource selection functions). 
I have (telemetry) data from 5 study areas, and each area has a different number of individuals monitored. </p>

<p>To develop GLMM's, the dependend variable is binary (1-used locations; 0-available locations), and I have a initial set of 14 continuous variables (8 land cover variables; 2 distance variables, to artificial areas and water sources; 4 topographic variables): a buffer was placed around each location and the area of each land cover within that buffer was accounted for; distances were measured from each point to the nearest feature, and topographic variables were obtained using DEM rasters. I tested for correlation using Spearman's Rank, so not all 14 were used in the GLMMs. All variables were transformed using z-score.</p>

<p>As random effect, I used individual ID (In another question (""GLMM: relationship between AIC, R squared and overdispersion?""), it became clear that using study areas as random effect was not useful nor correct).</p>

<p>I constructed a GLMM with 9 variables (not correlated) and a random effect, then used ""dredge()"" function and ""model.avg(dredge)"" to sort models by AIC values. 
This was the result (only models of AICc lower than 2 represented):</p>

<pre><code>[1]Call:
model.avg(object = dredge.m1.1)

Component model call: 
glmer(formula = Used ~ &lt;512 unique rhs&gt;, data = All_SA_Used_RP_Area_z, family = 
     binomial(link = ""logit""))

Component models: 
          df   logLik    AICc  delta weight
123578     8 -4309.94 8635.89   0.00   0.14
1235789    9 -4309.22 8636.44   0.55   0.10
123789     8 -4310.52 8637.04   1.14   0.08
1235678    9 -4309.75 8637.50   1.61   0.06
12378      7 -4311.78 8637.57   1.67   0.06
1234578    9 -4309.79 8637.58   1.69   0.06
</code></pre>

<p>Variables 1 and 2 represent the distance variables; from 3 to 8 land cover variables, and 9 is a topographic variable.
 Weights seem to be very low, even if I average all those models as it seems to be common when delta values are low. Even with this weights, I constructed GLMMs for each of the combinations, and the results were simmilar for all 6 combinations. Here are the results for the first one (GLMM + overdispersion + r-squared):</p>

<pre><code>Random effects:
 Groups    Name        Variance Std.Dev.
 ID.CODE_1 (Intercept) 13.02    3.608   
Number of obs: 32670, groups:  ID.CODE_1, 55

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -0.54891    0.51174  -1.073 0.283433    
3       -0.22232    0.04059  -5.478 4.31e-08 ***
5       -0.05433    0.02837  -1.915 0.055460 .  
7       -0.13108    0.02825  -4.640 3.49e-06 ***
8       -0.15864    0.08670  -1.830 0.067287 .  
1         0.28438    0.02853   9.968  &lt; 2e-16 ***
2         0.11531    0.03021   3.817 0.000135 ***     
Residual deviance: 0.256           
r.squaredGLMM():
       R2m        R2c 
0.01063077 0.80039950 
</code></pre>

<p>This is what I get from this analysis: </p>

<p>1) Variance and SD of the random effect seems fine (definitely better than the ""0"" I got when using Study Areas as random effect);</p>

<p>2) Estimate values make sense from what I know of the species and the knowledge I have of the study areas;</p>

<p>3) Overdispersion values seem good, and R-squared values don't seem very good (at least when considering only fixed effects) but, as I read in several places, AIC and r-squared are not always in agreement. </p>

<p>4) Weight values seem very low. Does it mean the models are not good?</p>

<p>Then what I did was construct a GLM (""glm()""), so no random effect was used. I used the same set of variables used in [1], and here are the results (only models of AICc lower than 2 represented):</p>

<pre><code>[2] Call:
model.avg(object = dredge.glm_m1.1)

Component model call: 
glm(formula = Used ~ &lt;512 unique rhs&gt;, family = binomial(link = ""logit""), data = 
     All_SA_Used_RP_Area_z)

Component models: 
          df   logLik     AICc   delta weight
12345678   9 -9251.85 18521.70    0.00   0.52
123456789 10 -9251.77 18523.54    1.84   0.21
1345678    8 -9253.84 18523.69    1.99   0.19
</code></pre>

<p>In this case, weight values are higher. </p>

<p>Does this mean that it is better not to use a random effect? (I am not sure I can compare GLMM with GLM results, correct me if I am doing wrong assumptions)</p>
"
"0.165710452999832","0.161690416690889","223008","<p>Good morning all!I am trying to run a binomial gmler model.
My response variable is a binomial variable:  extra pair paternity -->( 1 or 0) I am looking at several continuous variables like weight, tarsus and number of eggs lost. However, I am having problems with my random effects. I would appreciate any help, because I am already 2 days trying to figure out!! Thanks a lot!!!</p>

<p>my data:      </p>

<pre><code>ring_id    nest nest_id number_eggs number_chicks lost_eggs ring_year tarsus weight
1 BD29285 WH00060       6          10            10         0      2016    210   1700
2 BD29286 WH00060       6          10            10         0      2016    200   1510
3 BD29287 WH00060       6          10            10         0      2016    199   1540
4 BD29288 WH00060       6          10            10         0      2016    209   1780
5 BD29289 WH00060       6          10            10         0      2016    199   1670
6 BD29290 WH00060       6          10            10         0      2016    199   1670
  number_epy epy_wpy EPP_nest Epfather
1          0     WPY        0         
2          0     WPY        0         
3          0     WPY        0         
4          0     WPY        0         
5          0     WPY        0         
6          0     WPY        0         
&gt; 
</code></pre>

<p>This is my code</p>

<pre><code>m &lt;- lmer(EPP_nest ~ weight + tarsus + lost_eggs + (1|nest_id) + (1| ring_id) ,family = 'binomial', data=chicks)

summary (m)
</code></pre>

<p>output: </p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [glmerMod
]
 Family: binomial  ( logit )
Formula: EPP_nest ~ weight + tarsus + lost_eggs + (1 | nest_id) + (1 |      ring_id)
   Data: chicks
Control: structure(list(optimizer = c(""bobyqa"", ""Nelder_Mead""), calc.derivs = TRUE,  
    use.last.params = FALSE, restart_edge = FALSE, boundary.tol = 1e-05,  
    tolPwrss = 1e-07, compDev = TRUE, nAGQ0initStep = TRUE, checkControl = structure(list( 
        check.nobs.vs.rankZ = ""ignore"", check.nobs.vs.nlev = ""stop"",  
        check.nlev.gtreq.5 = ""ignore"", check.nlev.gtr.1 = ""stop"",  
        check.nobs.vs.nRE = ""stop"", check.rankX = ""message+drop.cols"",  
        check.scaleX = ""warning"", check.formula.LHS = ""stop"",  
        check.response.not.const = ""stop""), .Names = c(""check.nobs.vs.rankZ"",  
    ""check.nobs.vs.nlev"", ""check.nlev.gtreq.5"", ""check.nlev.gtr.1"",  
    ""check.nobs.vs.nRE"", ""check.rankX"", ""check.scaleX"", ""check.formula.LHS"",  
    ""check.response.not.const"")), checkConv = structure(list( 
        check.conv.grad = structure(list(action = ""warning"",  
            tol = 0.001, relTol = NULL), .Names = c(""action"",  
        ""tol"", ""relTol"")), check.conv.singular = structure(list( 
            action = ""ignore"", tol = 1e-04), .Names = c(""action"",  
        ""tol"")), check.conv.hess = structure(list(action = ""warning"",  
            tol = 1e-06), .Names = c(""action"", ""tol""))), .Names = c(""check.conv.grad"",  
    ""check.conv.singular"", ""check.conv.hess"")), optCtrl = list()), .Names = c(""optimizer"",  
""calc.derivs"", ""use.last.params"", ""restart_edge"", ""boundary.tol"",  
""tolPwrss"", ""compDev"", ""nAGQ0initStep"", ""checkControl"", ""checkConv"",  
""optCtrl""), class = c(""glmerControl"", ""merControl""))

     AIC      BIC   logLik deviance df.resid 
    56.0     77.5    -22.0     44.0      259 

Scaled residuals: 
      Min        1Q    Median        3Q       Max 
-0.001717 -0.001158 -0.000051  0.020852  0.041496 

Random effects:
 Groups  Name        Variance Std.Dev.
 ring_id (Intercept)    0      0.00   
 nest_id (Intercept) 6613     81.32   
Number of obs: 265, groups:  ring_id, 265; nest_id, 45

Fixed effects:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -1.106e+01  2.222e+01  -0.498  0.61854    
weight      -6.211e-04  7.607e-03  -0.082  0.93493    
tarsus      -6.362e-03  1.157e-01  -0.055  0.95615    
lost_eggs   -6.395e+00  1.777e+00  -3.599  0.00032 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
          (Intr) weight tarsus
weight    -0.252              
tarsus    -0.857 -0.272       
lost_eggs  0.212 -0.170 -0.070
convergence code: 0
Model failed to converge with max|grad| = 0.00997492 (tol = 0.001, component 1)
Model is nearly unidentifiable: very large eigenvalue
 - Rescale variables?
Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p>So I am quite lost.... and I would really apreciate any help!!!!! </p>

<p>Thank you very much!!1
Best, Mara</p>
"
"0.202953027447522","0.198029508595335","223439","<p>I am running an analysis on a national sample of 20,000, representative at the province level (34 provinces)</p>

<p>After checking for linearity and normality of my dependent variable I have run a preliminary OLS in order to see how the covariates perform in explaining the variation of the variable of interest. 
I have selected the relevant independent variables following accreditee literature in my field of analysis, explored the covariance matrix in order to avoid problems of multicollinearity etc..</p>

<p>The result from the OLS is good in term of significance level of the coefficients, the sign and the magnitute of the latter fullfil my expectations and match the results find by other analysis.
However, the value of <code>R^2</code> is quite low: only <code>0.09</code> . Thus, knowing that some variation could be explained by the differences between provinces I have first estimate the OLS adding first provincial dummies and after distric dummies (398 districts).</p>

<p>The <code>R^2</code> improved much, reaching respectively the <code>36%</code> and <code>41%</code>.
However, what I would like to see are the underling cause of the regional differences: why do they perform differently? </p>

<p>Among the variables I have some take a unique value according to each observation's province. I cannot use them in the OLS while using the province dummies because there would be perfect collinearity.</p>

<p>In my view using a mixed linear model would help.</p>

<p>I have run a random intercept null.model in which only the dependent variable is regressed against an intercept. For the estimation I have used the command <code>lmer</code> from the <code>{lme4}</code>package in <code>R</code>.</p>

<p>The InterCorrelation Coeffient equals <code>0.30</code>, suggesting that the 30% of the variation happens between groups, the values of the group-mean reliance are quite high too (not less than <code>0.9</code>)- I repeat myself: the sample is province representative.</p>

<p>I finally run a set of random mixed intercept model with 2 levels:</p>

<p>where: <code>i</code> indicates the household and <code>j</code> indicates the province.</p>

<ol>
<li><code>Y_i = beta0j + beta1 X_i + e_ij</code> </li>
<li><code>Y_i = beta0j + beta1 X_i + beta2 Z_j + e_ij</code></li>
<li><code>Y_i = beta0j + beta1 X_i + beta2 W_j + e_ij</code></li>
</ol>

<p>The <strong>first question</strong> is: am I allowed to include a variable which varies only at the provincial level (as Z and W do) given that their coefficient is not computed by taking in cosideration the groups?
As far as I have understood it would be a mistake to use those variables in the random part of the model in order to get random coefficients.</p>

<p>The <strong>second question</strong>: given that by running the command <code>anova()</code>, also in <code>lme4</code>, model 1 is statistically different from model 2, and model 2 is  statistically not different from model 3, can I say that Z and W have the same power in explaing the variation in Y despite the fact that Z's coefficient is significant and W's is not significant?</p>

<p>Think as if Z and W were proxies for the same dimension. They are in fact statistically and conceptually high correlated.</p>

<p>Sorry but I cannot give more details on the actual problem I am woking on.</p>

<p>Thanks in advance. </p>
"
"0.143509461970482","0.140028008402801","224626","<p>I have a vegetation data set that consists of 150 plots that were sampled 1-3 times over a three year period.  Plots are my unit of observation and they are unbalanced (since plots were sampled either once, twice, or three times).  I would like to use mixed-effects models in order to account for variation in both plots and sampling year and to keep my sample size large (instead of conducting my analysis within individual years). </p>

<p>My response variables are cover of vegetation functional groups and predictors include variables related to fire and treatment history.   Additionally, I am not interested in how plots change over time per se, but rather, in aggregating sampling from all three years to increase my sample size and to account for the spatial/temporal correlation that arises from doing so.  It is my assumption that treating plot as a random effect (intercept only) accounts for variation that arises from potential spatial autocorrelation, but my main question is how to account for the repeated measures and if I need to account for the grouping of cells within sampling years:</p>

<p>Potential model:</p>

<pre><code>model &lt;- lmer(response~covariates + (1|Plot) + (1|Year).
</code></pre>

<p>However, I know that is not appropriate to use a random effect with only three levels, year in this case.  I'm hoping for recommendations on how to incorporate year as a random effect.  Is including (Year |  Plot)  recommended?  And if so, how might I interpret that effect, i.e., is it accounting for variation introduced by different sampling year or variation in plots over sampling year?</p>
"
"0.166202907140464","0.162170924189109","226505","<p>I run two <code>lmer</code> tests, one with and one without the interaction term between fixed effects. The problem is that the former gives an output result that makes no sense to the actual data (i.e. negative slope instead of positive), whereas the latter shows the expected output. Why does this happen and even though the interaction is significant (and also makes sense) does it mean that I should not include it in the model due to wrong output? Would it be better to run a model with only the fixed factors and another with the interaction term alone?</p>

<p>Below is the models and their outputs. Thank you!</p>

<p>(WITHOUT INTERACTION TERM)</p>

<pre><code>mTEST&lt;- lmer(amp.sqrt~ treatment + time + axis + (1+treatment|ID))
summary(mTEST)
Linear mixed model fit by REML 
t-tests use  Satterthwaite approximations to degrees of freedom ['merModLmerTest']
Formula: amp.sqrt ~ treatment + time + axis + (1 + treatment | ID)

REML criterion at convergence: 5682.2

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.2769 -0.7678 -0.0236  0.6049  3.5182 

Random effects:
 Groups   Name        Variance Std.Dev. Corr       
 ID       (Intercept)  602.8   24.55               
          treatment2  1028.9   32.08    -0.14      
          treatment3   283.2   16.83    -0.03  0.52
 Residual             2027.6   45.03               
Number of obs: 540, groups:  ID, 21

Fixed effects:
            Estimate Std. Error      df t value Pr(&gt;|t|)    
(Intercept)  115.184      7.546  36.300  15.265  &lt; 2e-16 ***
treatment2     2.644      8.571  18.400   0.308  0.76117    
treatment3    23.365      6.139  19.200   3.806  0.00117 ** 
time7         13.958      4.707 474.800   2.965  0.00318 ** 
time8         21.799      4.787 478.500   4.554  6.7e-06 ***
axis2         60.458      4.746 474.800  12.737  &lt; 2e-16 ***
axis3        128.456      4.746 474.800  27.063  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
           (Intr) trtmn2 trtmn3 time7  time8  axis2 
treatment2 -0.287                                   
treatment3 -0.299  0.506                            
time7      -0.312  0.000  0.000                     
time8      -0.314  0.013  0.008  0.492              
axis2      -0.315  0.000  0.000  0.000  0.000       
axis3      -0.315  0.000  0.000  0.000  0.000  0.500
</code></pre>

<p>(WITH INTERACTION TERM)</p>

<pre><code>mTEST2&lt;- lmer(amp.sqrt~ treatment * time + axis + (1+treatment|ID))
summary(mTEST2)
Linear mixed model fit by REML 
t-tests use  Satterthwaite approximations to degrees of freedom ['merModLmerTest']
Formula: amp.sqrt ~ treatment * time + axis + (1 + treatment | ID)

REML criterion at convergence: 5615.6

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.7117 -0.7237 -0.0390  0.6140  3.3017 

Random effects:
 Groups   Name        Variance Std.Dev. Corr       
 ID       (Intercept)  619.0   24.88               
          treatment2  1061.1   32.58    -0.16      
          treatment3   296.4   17.22    -0.06  0.54
 Residual             1879.0   43.35               
Number of obs: 540, groups:  ID, 21

Fixed effects:
                 Estimate Std. Error      df t value Pr(&gt;|t|)    
(Intercept)       130.587      8.417  55.500  15.515  &lt; 2e-16 ***
treatment2         -3.766     10.713  44.500  -0.352   0.7269    
treatment3        -14.929      8.851  83.600  -1.687   0.0954 .  
time7              -7.697      8.120 471.000  -0.948   0.3436    
time8              -2.628      8.120 471.000  -0.324   0.7464    
axis2              60.458      4.569 471.000  13.232  &lt; 2e-16 ***
axis3             128.456      4.569 471.000  28.113  &lt; 2e-16 ***
treatment2:time7    9.697     11.206 471.000   0.865   0.3873    
treatment3:time7   53.206     11.206 471.000   4.748 2.73e-06 ***
treatment2:time8    8.554     11.396 473.700   0.751   0.4532    
treatment3:time8   62.411     11.289 473.300   5.528 5.35e-08 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) trtmn2 trtmn3 time7  time8  axis2  axis3  trt2:7 trt3:7 trt2:8
treatment2  -0.448                                                               
treatment3  -0.479  0.515                                                        
time7       -0.482  0.379  0.459                                                 
time8       -0.482  0.379  0.459  0.500                                          
axis2       -0.271  0.000  0.000  0.000  0.000                                   
axis3       -0.271  0.000  0.000  0.000  0.000  0.500                            
trtmnt2:tm7  0.349 -0.523 -0.332 -0.725 -0.362  0.000  0.000                     
trtmnt3:tm7  0.349 -0.275 -0.633 -0.725 -0.362  0.000  0.000  0.525              
trtmnt2:tm8  0.344 -0.514 -0.327 -0.356 -0.712  0.000  0.000  0.492  0.258       
trtmnt3:tm8  0.347 -0.272 -0.628 -0.360 -0.719  0.000  0.000  0.261  0.496  0.512
</code></pre>
"
"0.047836487323494","0.0466760028009337","226747","<p>I am trying to wrap my head around the notation for this three-level model. </p>

<p>Level 1: Repeated observations
Level 2: Client
Level 3: Therapist</p>

<p>I am using a baseline intercept model to calculate intra-class correlation coefficients. In order to partition the variance at both the client and therapist level, I have random effects listed for both. </p>

<p>lme4 code:</p>

<pre><code>mod01 &lt;- lmer(var ~ 1 + (1 | client) + (1 | therapist), data = dat10)
</code></pre>

<p>And my notation thus far is:</p>

<p>(ð‘‰ð‘Žð‘Ÿð‘–ð‘Žð‘ð‘™ð‘’)time,client,therapist = ð›¾00 + ð‘¢client + ð‘¢therapist + ð‘’client,therapist</p>

<p>But is that error term correct? I'm trying to keep this as simple as possible for my audience by using the combined equation. Any feedback is greatly appreciated. </p>
"
"0.047836487323494","0.0466760028009337","230721","<p>I have the following model  </p>

<pre><code>fit1 &lt;- glmer(Res~FA+FB+FC+(1|fsite), family=binomial(), data=DATA)
</code></pre>

<p>the result of <code>summary()</code> is:  </p>

<pre><code>summary(fit1)
Generalized linear mixed model fit by maximum likelihood 
 (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: Res ~ FA + FB + FC + (1 | fsite)
   Data: DATA

     AIC      BIC   logLik deviance df.resid 
   202.3    229.9    -92.1    184.3      150 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.1768 -0.6167 -0.4967  0.6815  2.0132 

Random effects:
 Groups Name        Variance Std.Dev.
 fsite  (Intercept) 0        0       
Number of obs: 159, groups:  fsite, 28

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  1.55573    0.55830   2.787 0.005327 ** 
FA2         -0.11914    0.37344  -0.319 0.749692    
FB2         -1.38652    0.39967  -3.469 0.000522 ***
FC2         -0.14976    0.61984  -0.242 0.809076    
FC3         -0.06794    0.63171  -0.108 0.914350    
FC4         -1.20114    0.61670  -1.948 0.051452 .  
FC5         -1.44951    0.62817  -2.308 0.021025 *  
FC6         -1.13590    0.65427  -1.736 0.082538 .  
---
Signif. codes:  0 ?**?0.001 ?*?0.01 ??0.05 ??0.1 ??1

Correlation of Fixed Effects:
        (Intr) fspcs2    FB2    FC2    FC3    FC4    FC5 
FA2     -0.466                                          
FB2     -0.456  0.169                                   
FC2     -0.572 -0.021  0.017                            
FC3     -0.596  0.050  0.036  0.506                     
FC4     -0.582 -0.005  0.020  0.519  0.509              
FC5     -0.558  0.019 -0.038  0.508  0.500  0.511       
FC6     -0.391 -0.101 -0.288  0.485  0.467  0.486  0.492
</code></pre>

<ul>
<li>Why are the variance and Std.Dev of the random effects zero?</li>
<li>How do I check for overdispersion in this model?</li>
<li>What should do if there is overdispersion?</li>
</ul>
"
"0.15190124858318","0.148216215299981","230802","<p>I am analyzing data from cohort of 500 calves investigating the impact of disease on growth.</p>

<p>My outcome variables are normally distributed, continuous data. I am using hierarchical models with calf nested within farms and testing for the longer term impacts of disease.</p>

<p>The problem I am having is with how to include disease data. I have variables for the number of weeks a calf had disease and the total score over a validated threshold for diagnosis</p>

<p><img src=""https://drive.google.com/file/d/0B7BdsyR1JIOjemZlNDlvREtFOVU/view?usp=sharing"" alt=""Histograms of Calf Disease Data""></p>

<p>As I am inexperienced in uploading images, here are the tabulated results of the data above:</p>

<hr>

<pre><code>Disease Duration (weeks) 0   1    2   3   4   5   6 
Frequency               266 128  50  33   8   5   2
</code></pre>

<hr>

<pre><code>Total Score  0   1   2   3    4   5   6   7   8   9  10  13  14  15 
Frequency   266  88  51  30  20  13   2   6   4   5   3   1   2   1 
</code></pre>

<hr>

<p>Obviously, this data is far from normal. But there a lot of levels to use a dummy coded categorical variable, and I think an ordinal scale better represents the data. What do you think it the best way to include this data as an independent variable in my LME models? (n.b. I don't include both in the same model just one or the other)</p>

<p>The models do return results without convergence errors or other warnings when I include these variables but it doesn't feel like very good practice and I am unsure of what sort of transformation I could do to make this data better (e.g. log transformation leaves the data looking very odd and plots of the raw data make it look like a linear relationship is the most likely) </p>

<p>Here is an example of what I would like to improve:</p>

<p>(adj_w_63 - calf weight,
weeks_brd - weeks with disease (as described above),
rid - a normally distributed continuous variable,
milksolids_total - a normally distributed continuous variable)</p>

<pre><code>library(lme4)
model1&lt;-lmer(adj_w_63 ~ weeks_brd + rid + milksolids_total + (1|farm_ac),
 data=comp)
summary(model1)

Linear mixed model fit by REML ['lmerMod']
Formula: adj_w_63 ~ weeks_brd + rid + milksolids_total + (1 | farm_ac)
   Data: comp

REML criterion at convergence: 3247

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.5180 -0.5525 -0.0458  0.5945  6.1674 

Random effects:
 Groups   Name        Variance Std.Dev.
 farm_ac  (Intercept) 30.10    5.487   
 Residual             83.37    9.131   
Number of obs: 443, groups:  farm_ac, 11

Fixed effects:
                 Estimate Std. Error t value
(Intercept)      68.06279    3.30996  20.563
weeks_brd        -1.00200    0.42089  -2.381
rid               0.11010    0.04981   2.210
milksolids_total  0.19904    0.07679   2.592

Correlation of Fixed Effects:
            (Intr) wks_br rid   
weeks_brd   -0.174              
rid         -0.285  0.141       
mlkslds_ttl -0.795  0.038 -0.016
</code></pre>

<p>Thank you so much for your help.</p>
"
"0.143509461970482","0.140028008402801","233831","<p>I would like to make a statistical analysis in R using <code>lmer()</code>.
I analyzed 6 speaker voice during 18 months. The months divided into 4 periods: quarters 1, 2, 3, and 4.</p>

<p>My data:</p>

<p>dependent variable: X continuous
fixed factor: quarters
random factor: speakers</p>

<p>my formula is: </p>

<pre><code>m &lt;- lmer(X ~ quarters + (1 | speaker), data = v)
</code></pre>

<p><strong>the output is</strong>:</p>

<pre><code>Formula: X ~ quarters + (1 | speaker)
   Data: v

     AIC      BIC   logLik deviance df.resid 
  4219.4   4253.0  -2103.7   4207.4     1966 

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-2.60919 -0.72724 -0.05942  0.65965  2.70657 

Random effects:
 Groups   Name        Variance Std.Dev.
 speaker  (Intercept) 0.002817 0.05308 
 Residual             0.493093 0.70221 
Number of obs: 1972, groups:  speaker, 6

Fixed effects:
              Estimate Std. Error         df t value Pr(&gt;|t|)    
(Intercept)    0.17173    0.05105   24.30000   3.364  0.00255
quartersQ2         -0.25136    0.05040 1972.00000  -4.988 6.64e-07
quartersQ4         -0.43192    0.05281 1951.40000  -8.179 4.44e-16
quartersQ5         -0.44146    0.05832 1869.50000  -7.569 5.86e-14
Correlation of Fixed Effects:
      (Intr) TQeQ2  TQeQ4 
quartersQ2 -0.748              
quartersQ4 -0.735  0.719       
quartersQ5 -0.673  0.654  0.630
</code></pre>

<p>My question: why don't I see the factor effect rather than the result for each group in the quarters?</p>

<p>What I am doing wrong?</p>

<hr>

<p>Thank for the answering!
Sorry about it, I am not perfect in statistical analysis.
I would like to test that there is a differences among four quarters.</p>

<p>My first question is that this analysis is correct? How can it interpreted?
And I have show in the example analysis (sleepstudy), that in the output the ""Fixed Effects"" it can be seen the fixed facor names, not the groups names of the fixed factor.
What is a difference? Why in the output can be seen the groups name (excluded the first quarter Q1).</p>

<p>It is not worng that there is a high Correlation of Fixed Effects?</p>

<p>Sorry about my bad knowledge:(</p>

<p>Thank</p>
"
"0.10696563746014","0.0834965721446866","234066","<p>The bird auditory surveys consist of >100 roadside survey routes across Ontario. Bird call count was conducted at 10-20 stations along each survey routes for >10 years. For each station, there is data on the amount of forest harvested within the last 5 years.  An objective is to assess how bird abundance is affected by forest harvesting.</p>

<p>My problem is to how to deal with spatial and temporal autocorrelation and the violation of independence of data. That is, the response variable (abundance of birds) is likely to be spatially and temporally autocorrelated. And, the bird abundance at each station is likely correlated with that at other stations within a route. My approach is to incorporate routes and year as random effects in generalized mixed effects models as shown below (using <code>lme4</code> package). But, I am not sure how well autocorrelation is modeled adequately in this way.</p>

<pre><code>glmer(Abundance ~ Area_harvested + (1 | route) + (1 | Year),
      data = mydata, family = poisson)
</code></pre>

<p>Although I specified Poisson above, negative binomial or zero-inflated models (because there are many zeros; abundance = 0) may be more appropriate.</p>

<p>Could anyone please suggest a proper way to analyze my data? Also, could you please suggest better or proper way to specify random effects given my data?</p>
"
