"V1","V2","V3","V4"
"0","0.117851130197758","  6152","<p>I have a (I suspect) simple question. I have time series cross section data on voting behaviour in the Council of the European Union (the monthly number of yes, no and abstentions for each member state from 1999 to 2007). So basically the variables are counts, thus a Poisson/negative binomial regression would be appropriate, possibly with lagged dependent variables on the right hand side to control for time dependencies. I have seen papers with people using such negative binomial models to forecast, for instance the number of monthly legislative acts adopted in the future, and I have three questions in this regard:</p>

<ol>
<li><p>How can i run a negative binomial regression on panel data without making any inferential mistakes?</p></li>
<li><p>How can I use a negative binomial model with lags to forecast future values of the dependent variable.</p></li>
<li><p>Can this be done in R?</p></li>
</ol>

<p>Thomas</p>
"
"0.185695338177052","0.166666666666667"," 20725","<p>I have a model that looks like </p>

<pre><code>lm(y ~ lag(x, -1) + lag(z, -1))
</code></pre>

<p>So basically, this is a time series regression with exogenous variables, and I want to carry out a rolling analysis of sample forecasts, meaning that:
I first used a subsample (e.g., 1990-1995) for estimation, then I performed a one step ahead forecast, then I added one observation and made another one step ahead forecast, and so on.</p>

<p>I have tried to work with <code>rollapply</code>, defining the model as <code>arima(0,0,0)</code> with <code>xreg=lags</code> of the other variables, but that doesn't work. </p>

<p>Your help would be much appreciated!</p>
"
"0.306381676672686","0.353553390593274"," 33981","<p>I would like to use <a href=""http://en.wikipedia.org/wiki/Exponential_smoothing#Double_exponential_smoothing"">double exponential smoothing</a> to predict prevalence rates of care dependency in Austrian federal states. </p>

<p>My data is very detailed, thus I would like to make use of that in order to refine my predictions. I have the percentage of people in care dependency levels 1â€“7 aged 50â€“99 in 9 Austrian federal states.</p>

<pre><code> str(daten[1:12][daten$jahr&gt;1996,])
'data.frame':   39600 obs. of  12 variables:
 $ age       : num  50 51 52 53 54 55 56 57 58 59 ...
 $ gender    : Factor w/ 2 levels ""male"",""female"": 1 1 1 1 1 1 1 1 1 1 ...
 $ bundesland: Factor w/ 9 levels ""Bgld"",""Ktn"",""Noe"",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ jahr      : num  1997 1997 1997 1997 1997 ...
 $ PfSt0     : num  0.992 0.989 0.985 0.985 0.985 ...
 $ PfSt1     : num  0.001458 0.000967 0.001459 0 0.002199 ...
 $ PfSt2     : num  0.00437 0.00193 0.00802 0.00793 0.00587 ...
 $ PfSt3     : num  0.00146 0.0058 0.00073 0.00433 0.0044 ...
 $ PfSt4     : num  0.000729 0 0.002188 0.002163 0.000733 ...
 $ PfSt5     : num  0 0.000967 0.002188 0.000721 0.002199 ...
 $ PfSt6     : num  0 0.000967 0 0 0 ...
 $ PfSt7     : num  0 0 0.00073 0 0 ...
</code></pre>

<p>DES is a time series analysis method. Time series analysis explains a data series by its past values only. While it is true that I use only past data of care dependency, one could regard age, gender and federal state as explanatory variables. Instead of computing individual double exponential smoothing forecasts for each age, gender, federal state combination, I could assume structural uniformity within these time series. Thus, my data might be regarded a multilevel panel dataset, with 50 observations per year (age groups) nested in 9 federal states each. (I plan to do separate regressions for males and females.) </p>

<p>I would like to use federal state, age and age squared as explanatory variables apart from previous value and previous trend, as done in double exponential smoothing. </p>

<p>However, in panel data analysis, time trends are typically covered by including the year variable in the regression, and rarely ever by including lags. <strong>How could I realize a forcasting method similar to double exponential smoothing in a panel dataset, i.e. including also other explanatory variables?</strong> (Preferably in R)</p>

<p>(Matters are complicated further by the fact that I have 7 instead of 1 dependent variables.)</p>
"
"0.393919298579168","0.353553390593274"," 38491","<p>If we have a spatial autoregressive process, we can estimate a model to control for the autoregression with a spatial lag,
$$y=\rho W y + X\beta + \epsilon$$
Where $\rho$ is the strength of the spatial correlation, and $W$ is a matrix of spatial weights. The <code>spdep</code> package for R contains the <code>lagsarlm</code> command which is designed to estimate precisely this model. The package contains methods for creating the weights. But there seems to be some discrepancy between the model fit between <code>lagsarlm()</code> and <code>lm()</code> fitted to what should be a similar model.</p>

<p>As an example, consider the example given with <code>?lagsarlm</code> in R. </p>

<pre><code>library(spdep)
data(oldcol)
COL.lag &lt;- lagsarlm(CRIME ~ INC + HOVAL, data=COL.OLD,
                nb2listw(COL.nb, style=""W""), method=""eigen"", quiet=TRUE)
summary(COL.lag)
Residuals:
      Min        1Q    Median        3Q       Max 
-37.68585  -5.35636   0.05421   6.02013  23.20555 

Type: lag 
Coefficients: (asymptotic standard errors) 
             Estimate Std. Error z value  Pr(&gt;|z|)
(Intercept) 45.079251   7.177347  6.2808 3.369e-10
INC         -1.031616   0.305143 -3.3808 0.0007229
HOVAL       -0.265926   0.088499 -3.0049 0.0026570

Rho: 0.43102, LR test value: 9.9736, p-value: 0.001588
Asymptotic standard error: 0.11768
    z-value: 3.6626, p-value: 0.00024962
Wald statistic: 13.415, p-value: 0.00024962
</code></pre>

<p>We can estimate what (I think) should be the same model by computing the actual spatial lag variable,</p>

<pre><code>crime.lag &lt;- lag.listw(nb2listw(COL.nb, style=""W""), COL.OLD$CRIME)
linearlag &lt;- lm(CRIME ~ crime.lag + INC + HOVAL, data=COL.OLD)
Residuals:
    Min      1Q  Median      3Q     Max 
-38.644  -6.103   0.266   6.563  21.610 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 38.18099    9.21531   4.143 0.000149 ***
crime.lag    0.55733    0.15029   3.709 0.000570 ***
INC         -0.86584    0.35541  -2.436 0.018864 *  
HOVAL       -0.26358    0.09136  -2.885 0.005986 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 10.12 on 45 degrees of freedom
Multiple R-squared: 0.6572, Adjusted R-squared: 0.6343 
F-statistic: 28.75 on 3 and 45 DF,  p-value: 1.543e-10 
</code></pre>

<p>The two models, which I think should be identical, are in fact significantly different from each other in every parameter and in model fit (with the <code>linearlag</code> model providing significantly lower AIC). Are there reasons why this should be? Why should I just not use the second model and abandon the special methods?</p>
"
"0.185695338177052","0.166666666666667"," 57826","<p>I have a dataset that includes variables about customer income levels.  The income was collected in binned fashion (<code>Which range describes your income? 0-25k, 25k-50k,...</code>).  My question is how best use this for modeling using <code>glmnet</code> and <code>gbm</code> packages in <code>R</code>.</p>

<p>I have looked at the <code>grouped</code> packaged in <code>R</code> but it seems to do everything (coursening and regression) for you. Is there a package that converts binned data back to continuous data for use in with other algos?</p>

<p>EDIT: The current method I'm using is to convert them to the mid-point of the range (<code>0-25k -&gt; 12500</code>), then using an <code>ifelse()</code> stmt to code a few variables to convey the fact that there is a relationship between the levels.</p>

<pre><code>incOver25k &lt;- ifelse(df1$income &gt;= 25000,1,0)
    incOver50k &lt;- ifelse(df1$income &gt;= 50000,1,0)
</code></pre>

<p>Then use these flags instead of using <code>model.matrix()</code>.  </p>

<p>Was curious if there were any better ideas.</p>
"
"0.435494170355693","0.390867979985286"," 58962","<p>I am building a multiple regression model - wrapped in a function - with one dependent variable and a dozen independent variables. The reason why I am building a function is that I need to do this analysis with approximately 75 different datasets. </p>

<p>The challenge is that the independent variables correlate better with the dependent variable when they are lagged in time. Unfortunately, not all time lags are the same for each variable and I would like to determine the optimal mix of time lags for each variable while getting the most optimum Adjusted R^2 value for the multiple regression model. Moreover, after building an initial model I will try to reduce the model using the <code>step(modelbase, direction=""both"")</code> function on the model. </p>

<p>In the approach I currently have I time lag all the independent variables with the same number of weeks. This results in the best possible model where all independent variables have the same time lag, but I believe (with a valid hypothesis supporting this) that there is a better model out there when we differ the time lag for each independent variable. My question is what is the best strategy to determine the best fit model without making the number of options huge. If I want to determine between 0 and 20 weeks time lag in weekly steps for 12 independent variables I am quickly up to trying to find a match between 4.096e+15 variables (=20^12). </p>

<p>I can imagine reducing the problem with the following strategy: Start by finding the best fit model with one independent variable at different time lags. The second step will be to add a second independent variable with its different time lags and find the best model with the two independent variables where the second is tried at different time lags while the first is kept constant. Then add a third variable for which we take a similar approach as the second by keeping the first two variables constant and change try the third with different time lags. Something tells me that this strategy might be decent approach, but something that there also might be a better overall model that contains the not optimal variables for each individual independent variable. </p>

<p>Is there anybody who shine some light on how to tackle this challenge? </p>
"
"0.293610109757352","0.210818510677892"," 70764","<p>I am looking at a time series which has no obvious trend, but seems to have an intercept a little above zero. The results I get for <a href=""http://rss.acs.unt.edu/Rdoc/library/urca/html/ur.df.html"" rel=""nofollow""><code>ur.df</code></a> function in R is the following:</p>

<pre><code>logprice_df &lt;- ur.df(test3, lags = 1, type= 'trend')
summary(logprice_df)

############################################### 
# Augmented Dickey-Fuller Test Unit Root Test # 
############################################### 

Test regression trend 


Call:
lm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.50614 -0.04394  0.00134  0.03859  0.64408 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  1.841e-02  8.268e-03   2.226  0.02626 *  
z.lag.1     -1.573e-02  5.635e-03  -2.791  0.00537 ** 
tt           9.234e-06  1.080e-05   0.855  0.39272    
z.diff.lag   1.411e-01  3.364e-02   4.195 3.01e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.07512 on 865 degrees of freedom
Multiple R-squared: 0.02651,    Adjusted R-squared: 0.02314 
F-statistic: 7.852 on 3 and 865 DF,  p-value: 3.572e-05 


Value of test-statistic is: -2.791 2.6012 3.8997 

Critical values for test statistics: 
      1pct  5pct 10pct
tau3 -3.96 -3.41 -3.12
phi2  6.09  4.68  4.03
phi3  8.27  6.25  5.34
</code></pre>

<p>The problem is that I do not understand how one shall interpret these values. What is <code>F-statistic</code>? And what is the probabilities below <code>Pr(&gt;|t|)</code> in the first table, and also <code>Value of test-statistic</code>?</p>

<p>I really appreciate any help.</p>
"
"0.421117443806158","0.409461512426663"," 79216","<p><strong>Problem</strong>: When trying to calculate the variance of timeseries sums I get a negative variance, mostly due to autocovariances at large lag steps. Does not seem realistic.</p>

<p>I have a timeseries which is calculated from another timeseries using a regression equation.
I would like to propagate the uncertainty in the regression to the final timeseries. Then I want to sum (or take mean values) different segments of the timeseries over different timeperiods, and get the uncertainty of the sums. The timeseries is originally in 1 hour frequency and I want to sum over periods of 1 day (resampling to daily frequency) up to several years. The timeseries is strongly autocorrelated at short lag times.</p>

<p>For getting the variance of the sum (in the case of 3 elements being summed):
$$Var(a+b+c)= \\ Var(a)+Var(b)+Var(c) + 2 \times (Cov(a,b) + Cov(a,c)+Cov(b,c))$$</p>

<p>I use <code>r</code> for the calculations. I get the variances for each timeseries element as $SE^2$, where $SE$ is the standard error (<code>se.fit</code>) returned from r's <code>predict()</code> function using the regression model. The covariances I get from the autocovariance function <code>acf()</code>.</p>

<p>Here is some code and a selection of the data (excuse clumsy R code, I'm very new to R):</p>

<pre><code>#tsY is the predicted timeseries from the regression
tsY=c(81.4,  79.0,  83.4,   81.7,   75.7,   68.3,   62.3,   57.2,   52.6,   48.8,   45.4,   42.6,   39.9,   37.6,   35.6,   33.8,   32.2,   30.8,   29.6,   28.4,   27.3,   26.2,   25.0,   23.9)
#tsSE is the standard error from the prediction (se.fit)
tsSE=c(1.55,  1.49, 1.60,   1.56,   1.41,   1.23,   1.09,   0.97,   0.87,   0.78,   0.71,   0.65,   0.60,   0.55,   0.51,   0.48,   0.45,   0.42,   0.40,   0.38,   0.36,   0.34,   0.32,   0.30)

tsVar=tsSE^2

#create a matrix of the autocovariances at different lag times, diagonal is lag=0
#rows and columns are indicies in timeseries
covmat&lt;-matrix(numeric(0), length(tsY),length(tsY)) 
for ( i in (1:(length(tsY)) ) ) {
  if (i == 1) {
    autocov&lt;-acf(tsY, type='covariance', lag.max= length(tsY))
    autocovvec&lt;-autocov$acf[1:nrow(autocov$acf)]
    covmat[i:length(tsY),i]=autocovvec
  }  else {
    autocov&lt;-acf(tsY[-(1:i-1)], type='covariance', lag.max= length(tsY))
    autocovvec&lt;-autocov$acf[1:nrow(autocov$acf)]
    covmat[i:length(tsY),i]=autocovvec
  }

}

# sum the matrix columns, but not the diagonal
sumofColumns &lt;- rep(NA, ncol(covmat))
for (i in (1:ncol(covmat))) {
  if (i == 1) {
    sumofColumns[i]=sum(covmat[-(1),i])  
  } else{ 
    sumofColumns[i]=sum(covmat[-(1:i),i])  
  }
}

sumofCov=sum(sumofColumns) # sum of the covariance (Cov(a,b) + Cov(a,c)+...)
sumofVar=sum(tsVar) # sum of the variances of each timeseries element
varofSum=sumofVar+2*sumofCov # variance of the sum of the timeseries

# from the covmat the negative variance occurs at larger lag times.
acf(tsY, type='covariance', lag.max= length(tsY))

&gt; sumofCov
[1] -1151.529
&gt; varofSum
[1] -2283.246
</code></pre>

<p><strong>So I have the following questions:</strong></p>

<blockquote>
  <ol>
  <li><p>Did I completely misunderstand how to calculate variance of sums?</p></li>
  <li><p>Is it better to use a cutoff from the max lags to be considered in the autocovariance? If so how would one determine this? This would especially be important with the complete data where the length is several thousand. </p></li>
  </ol>
  
  <p><strike>3. Why is the covariance negative in this sample data at large? When plotting tsY  <code>plot(tsY)</code> it looks like the covariance/correlation should remain positive.</strike> Because it is the variation in direction from their means.</p>
</blockquote>

<p><strong>EDIT:</strong></p>

<blockquote>
  <p>Comment on <strong>question 2</strong> above:
  I have realized that using n-1 lags, as above in the code, does not make a lot of sense. There appear to be few different ways to determine the maximum lags to consider.  Box &amp; Jenkins (1970) suggest n/4 and R by default 10*log10(n). This does not answer the question however, of how to determine an appropriate cutoff for summing the covariances.</p>
  
  <p>Does it make sense to look at the partial autocorrelation (function pacf()), in order not to overestimate the effect of the auto covariance in the summation term? The partial autocorrelation for my data is significantly different from zero only at 1 or 2 lags. Similarly, fitting an AR model using ar() function, I also get an order of 1 or 2.</p>
</blockquote>

<p>Cheers</p>

<p>Related post <a href=""http://stats.stackexchange.com/questions/10943/variance-on-the-sum-of-predicted-values-from-a-mixed-effect-model-on-a-timeserie"">Variance on the sum of predicted values from a mixed effect model on a timeseries</a></p>
"
"0","0.117851130197758"," 83861","<p>Let's say I have the following data on leads, monthly media spend, and clicks</p>

<pre><code>Month  Leads   Media     Clicks
Jan     150    1000       500
Feb     200    1000       550 
March   300    1200       800
...
</code></pre>

<p>Let's say I run a linear regression where y is leads and the predictors are media and clicks. That's good, I know the relationships between these variable and can generate some lags to produce predictions. But what if I had spent 500 (or 2000 or 0) on media, what would have occurred. How do I perform this type of 'counter-factual' analysis where I attempt to find the results of a model if the actual value from one or two of the predictors was lower or higher? What is the standard approach (aka statistically proper approach)? Is it just a matter to ""adjusting"" the data to the 'new' number and rerunning the regression? or maybe simulating a regression 100+ times with 100+ different values for media?</p>
"
"0.393919298579168","0.542115198909686","101187","<p>I know that this has been discussed before, but those discussions did not really answer my questions. I know how the ADF test works, but I am having trouble interpreting the output for the three options using the <code>ur.df</code> function in R (package: <code>urca</code>). Could someone walk me through the interpretations?
More specifically, what are <code>tau1</code>, <code>tau2</code>, <code>phi1</code>, <code>phi2</code>, and <code>phi3</code>? </p>

<pre><code>summary(ur.df(tcm.ts, type=""none"",selectlags=""BIC""))
summary(ur.df(tcm.ts, type=""drift"",selectlags=""BIC""))
summary(ur.df(tcm.ts, type=""trend"",selectlags=""BIC""))

&gt; summary(ur.df(tcm.ts, type=""none"",selectlags=""BIC""))

############################################### 
# Augmented Dickey-Fuller Test Unit Root Test # 
############################################### 

Test regression none 


Call:
lm(formula = z.diff ~ z.lag.1 - 1 + z.diff.lag)

Residuals:
    Min      1Q  Median      3Q     Max 
-95.199 -23.380  -6.608  26.885  86.560 

Coefficients:
           Estimate Std. Error t value Pr(&gt;|t|)   
z.lag.1     0.04398    0.01205   3.650  0.00183 **
z.diff.lag -0.03722    0.24417  -0.152  0.88053   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 45 on 18 degrees of freedom
Multiple R-squared:  0.7091,    Adjusted R-squared:  0.6768 
F-statistic: 21.94 on 2 and 18 DF,  p-value: 1.492e-05


Value of test-statistic is: 3.6495 

Critical values for test statistics: 
      1pct  5pct 10pct
tau1 -2.66 -1.95  -1.6

&gt; summary(ur.df(tcm.ts, type=""drift"",selectlags=""BIC""))

############################################### 
# Augmented Dickey-Fuller Test Unit Root Test # 
############################################### 

Test regression drift 


Call:
lm(formula = z.diff ~ z.lag.1 + 1 + z.diff.lag)

Residuals:
    Min      1Q  Median      3Q     Max 
-69.366 -24.625  -3.018  34.165  82.227 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept) -75.21181   45.89715  -1.639   0.1196  
z.lag.1       0.09756    0.03467   2.814   0.0119 *
z.diff.lag   -0.20396    0.25469  -0.801   0.4343  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 43.03 on 17 degrees of freedom
Multiple R-squared:  0.3596,    Adjusted R-squared:  0.2843 
F-statistic: 4.773 on 2 and 17 DF,  p-value: 0.02264


Value of test-statistic is: 2.814 8.6258 

Critical values for test statistics: 
      1pct  5pct 10pct
tau2 -3.75 -3.00 -2.63
phi1  7.88  5.18  4.12

&gt; summary(ur.df(tcm.ts, type=""trend"",selectlags=""BIC""))

############################################### 
# Augmented Dickey-Fuller Test Unit Root Test # 
############################################### 

Test regression trend 


Call:
lm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag)

Residuals:
   Min     1Q Median     3Q    Max 
-86.46 -14.84   5.56  20.87  70.29 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)  91.4039    92.1407   0.992   0.3360  
z.lag.1      -0.1127     0.1082  -1.042   0.3129  
tt           13.0810     6.4318   2.034   0.0589 .
z.diff.lag   -0.1287     0.2369  -0.543   0.5946  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 39.54 on 16 degrees of freedom
Multiple R-squared:  0.4911,    Adjusted R-squared:  0.3957 
F-statistic: 5.148 on 3 and 16 DF,  p-value: 0.01109


Value of test-statistic is: -1.042 8.1902 6.7579 

Critical values for test statistics: 
      1pct  5pct 10pct
tau3 -4.38 -3.60 -3.24
phi2  8.21  5.68  4.67
phi3 10.61  7.24  5.91
</code></pre>
"
"0.185695338177052","0.166666666666667","111541","<p>Could anyone provide me the details of how to determine the lag order of the  distributed lags for an $\text{ADL}(p,q)$ model in Matlab or another statistical package (and very much preferably in combination with the autoregression lags)? </p>

<p>Full working examples with model selection criteria ($\text{AIC}$ and $\text{BIC}$) seem to be available on the Matlab website for $\text{VAR}$ models, $\text{ARMA}$ models etc. but I can't find one for the $\text{ADL}(p,q)$ model. I would not have a clue how to rewrite those models to $\text{ADL}(p,q)$ myself but I have a vague feeling that such a thing would be possible.</p>

<p>In the end I want to automate this analysis by first checking the lag orders $p$, and $q$ and then using these numbers automatically to create the regressions out of this. So basically I'm looking for a fully working example. (I want to skip part of adding and deleting regressors by hand as much as possible to get a quick idea of the distributed lags of several assets).</p>
"
"NaN","NaN","146579","<p>Please how do i conduct adf regression in r. to be precise i need the codes used in conducting adf regression in r. I am working on time series data and i need to determine how many lags to include in the adf unit roots test.I have tried to perform the unit root tests with a random selection of lags but i am not convinced its the correct thing to do.  I will appreciate a prompt response as i have to hand in the paper by Friday.</p>

<p>Many thanks</p>

<p>Funmi</p>
"
"0.293610109757352","0.263523138347365","147530","<p>I'm trying to perform a lagged linear regression on time series data sourced from ~10,000 hospital patients, for the purpose of estimating causal relationships between administration of a drug and a certain physiological response. For example: Do non-steroidal anti inflammatory drugs cause hypertension?</p>

<p>Basically, the linear model I'm trying to fit is like this:</p>

<p><img src=""http://i.stack.imgur.com/qukXs.png"" alt=""AR/cross-correlation model""></p>

<p>This assumes a maximum of 30 lags. $y$ represents hypertension, $x$ is taking the drug, and $h$ is whether the patient is admitted or not (a covariate). </p>

<p><strong>My question is this</strong>: Given a unique time series for <em>each patient</em> (all truncated to the same length of 30 time points), how can I pool all of the time series data together to estimate things like the cross-correlation (e.g., using <code>ccf</code>) and auto-correlation (<code>acf</code>) over the entire data set? If I were just trying to fit a linear model, this can be done relatively easily using something like the <code>plm</code> library, but I haven't been able to find anything similar for single functions.</p>

<p>For reference, here is a very small example of what my data set looks like (note that I only retained 6 of the 30 total time points for each patient, for brevity):</p>

<pre><code>   patient_id            time     nsaid hypertension admission
1           1               1 0.4427955    0.0000000 0.0000000
2           1               2 1.0000000    0.2097246 0.0000000
3           1               3 0.0000000    0.4916697 0.0000000
4           1               4 0.0000000    1.0000000 0.0000000
5           1               5 0.0000000    0.7902754 0.0000000
6           1               6 0.0000000    0.0000000 0.0000000
7           2               1 0.0000000    0.0000000 0.0000000
8           2               2 0.4104132    0.0000000 0.0000000
9           2               3 0.8236088    0.0000000 1.0000000
10          2               4 1.0000000    0.0000000 0.6994038
11          2               5 0.5895868    0.0000000 0.0000000
12          2               6 0.1763912    0.0000000 0.0000000
</code></pre>
"
"0.297775000191279","0.267261241912424","147619","<p>I'm trying to build a model that can predict streamflow for an alpine (snowmelt-fed) watershed using snow albedo (roughly, the energy reflectance of the snow) data. Albedo controls the melt of the snowpack, and higher albedo means slower melt, and vice versa. I have daily time-series data for both the snow albedo and streamflow, for 12 years from 2002-2013. The albedo time-series was obtained by spatially-averaging albedo data (raster files) from NASA's MODIS satellite.</p>

<p>I have tried various methods (simple regression, GLMs, GAMs, decision trees and random forests) to build the flow prediction model, but all of them fail because of the autocorrelated relationship between albedo and flow. Since the albedo is a snowpack property, there is a lag between it and the flow (related to snowmelt).</p>

<p>The Cross correlation function (CCF) between albedo and flow is shown below:</p>

<p><a href=""http://imgur.com/PpW1Kpy"" rel=""nofollow""><img src=""http://i.imgur.com/PpW1Kpy.png"" title=""source: imgur.com"" /></a></p>

<p>I have tried to include albedo lags of various days into the models, but I'm not able to mimic the distributed lag relationship between albedo and flow. I have tried to add precipitation, temperature and other climatic data to the predictors, but they don't seem to help. There are similar lagged and cross-correlation problems between these other predictors and flow.</p>

<p>The albedo, flow, precipitation and air temperature time-series are shown below:</p>

<p><a href=""http://imgur.com/Kb8Ta6q"" rel=""nofollow""><img src=""http://i.imgur.com/Kb8Ta6q.png"" title=""source: imgur.com"" /></a></p>

<p>Is there a statistical or machine learning technique in R that I can explore to build the albedo-streamflow model?</p>

<p>Thank you.</p>
"
"NaN","NaN","152526","<p>I would like to estimate a model of the following form:</p>

<p>$$
y = \sigma G y + \beta X + \delta G^* X + \epsilon
$$</p>

<p>where $G$ and $G^*$ are quadratic adjacency matrices, $y$ is a vector of a dependent variable subject to peer effects and $X$ is a matrix of controls/exogenous characteristics. $G$ is a binary matrix with entries equal to 1 if two individuals are peers of each other; as such, $G$ is also symmetric with a 0-diagonal. $G^*$ is the row-normalized version of $G$ (in fact, it's a stacked matrix with the row-normalized $G$ along the diagonal an 0 in the off-diagonals because $X$ is a matrix).</p>

<p>Econometricians will immediately recognize the similarity to spatial simultaneous autoregressive lag models, whose general form is</p>

<p>$$
y = \rho W y + \beta X + \epsilon
$$</p>

<p>(with $W = G$). Because of the high similarity, I tried Roger Bivand's <a href=""http://cran.r-project.org/web/packages/spdep/index.html"" rel=""nofollow""><code>spdep</code></a> package for <code>R</code>. The regression command is <code>lagsarlm()</code>, but when I am not mistaken, it only estimates </p>

<p>$$
y = \sigma G y + \beta X + \epsilon
$$</p>

<p>using the following command:</p>

<pre><code>lagsarlm(y ~ x1 + x2, data=reg_data, listw=G, method=""eigen"",
    quiet=FALSE, zero.policy = FALSE, tol.solve=1e-14)
</code></pre>

<p>which works and also returns results, but obviously an entire term is missing. How can I incorporate the $ \delta G^* X$ term? Are there packages that serve my needs better?</p>
"
"NaN","NaN","153480","<p>I need to implement a SAR model with no covariates. To be more specific, the regression I have to estimate is y=bWy+e where: </p>

<ul>
<li>y is the dependent variable;</li>
<li>b is the coefficient parameter to be estimated;</li>
<li>W is the adjacency matrix;</li>
<li>e is the error.</li>
</ul>

<p>My idea was to use the lagsarlm function of the spdep package. But I've gone through spdep documentation and it seems that this function works only adding covariates: i.e. y=bWy+cX+e, and I don't know how to erase the X term.</p>

<p>Note: For those who are acquainted with network analysis literature and not with spatial econometrics, in a way this is a method to estimate a parameter for bonachich centrality.</p>
"
"0.227429413073671","0.204124145231932","155121","<p>First cross-validated question so please be gentle :o)</p>

<p>I have two datasets all gathered and managed in '<strong>R</strong>'... </p>

<p><strong>Dataset 1</strong> - News Corpus. Contains 3,270 entries from the period 1/Apr/13 to 31/Mar/14.  There are often multiple stories on any one day, and indeed days with no stories at all (which I believe makes for an incomplete time series and problems).  The dataset structure is;</p>

<pre><code>Date - (a date)
Domain - (a string) with 8 levels i.e. there are 8 web domains
DomainType - (a string) with 4 levels e.g. ""other news"" or ""technology news""
Sentiment_Title - (a numeric) a score that currently sits in range -4:4
Sentiment_Description - (a numeric) a score that currently sits in range -6:7
Sentiment_Body - (a numeric) a score that currently sits in range -53:146
CCAT - (logical)
ECAT - (logical)
GCAT - (logical)
MCAT - (logical)
</code></pre>

<p><a href=""https://mega.co.nz/#!HVQTkCJJ!UUFJMzN6i0xI_GKDEtzVV1WfUzkphYCEiB36oMsOINo"" rel=""nofollow"">DOWNLOAD corpusData.csv from Mega</a></p>

<p><strong>Dataset 2</strong> - Bitcoin Market Data. 365 day time series of weighted price, volume and intra-day spread for four different exchanges.  </p>

<p><a href=""https://mega.co.nz/#!3RokVLKB!8rsEBIL8N-F-SXP2lucsjnUfo40MBfN13YRFPGAMSlQ"" rel=""nofollow"">DOWNLOAD finData.csv from Mega</a> </p>

<p><strong>The Problem</strong>
What I really want to know is which features (if any) of dataset 1 (the corpus) are significantly related to the time series and how.  I guess the time series also needs leads and lags applied to know which direction any relationship goes and how far away from the story publication date that relationship lays.</p>

<p>I have spent a couple of weeks applying the very basic stats knowledge I have to the task and have spent a couple of hours with a post-grad stats support group who also proved unable to find a method that could be readily applied.</p>

<p>I (we) looked at basic Pearson's and Spearman's, moved on to look at linear regression and generalised linear models and so far there appears to be issues with the residuals that makes the output bunkum apparently.  I believe vector-autoregression could also be applied but we are way off into realms I just don't understand yet.</p>

<p><strong>The Question</strong> Given the datasets (and, ideally R) can anyone suggest or indeed offer up an approach to solving my problem?  Even better some simple explanation of how to interpret the results of any such approach.</p>
"
"0.227429413073671","0.136082763487954","156037","<p>The R Vars package has a Vector Auto Regression function called var. The arguments include (among other things) ""p"" defined as the ""Integer for the lag order"" and ""lag.max,"" which is defined as ""Integer, determines the highest lag order for lag length selection according to the choosen (sic) ic."" See cran.r-project.org/web/packages/vars/vars.pdf.</p>

<p>My questions are:</p>

<p>What is the definition of the ""lag order?""</p>

<p>I thought that ""lag.max"" meant the highest number of lags to consider, but the package documentation defines ""lag.max"" as the highest lag order. So, if the ""lag.max"" is the highest ""lag order"" then, clearly the ""lag order"" argument is not asking for the maximum ""lag order"" because that would be redundant. So, what is the argument for ""lag order"" asking for? ... the minimum lag order? ... the actual order of the lags, i.e., an order such as t-1, t-3, t-2 instead of t-1, t-2, t-3?</p>

<p>The definition of ""lag.max"" uses the term ""lag length."" What is that? I would have thought that it would also be the maximum number of lags to consider, but clearly from the context, that is not the case. So, what are the definitions of ""lag.max"" and ""lag length?""</p>

<p>Statistics would not be hard if statisticians would learn to define their terms!</p>
"
"0.131306432859723","0.117851130197758","163092","<p>Iâ€™m looking to build an ARIMA model in R to help me predict the number of shots a football player is going to take in a game. </p>

<p>I have last season's data to analyse to determine the optimal lags for my AR and MA parameters. I have a data frame in R, with the columns for the player name, date of match and the number of shots. </p>

<p>Unfortunately, I only have a maximum 38 data points for each player which isnâ€™t enough to build a statistically confident model. I suspect I need a way to analyse the data holistically/all-at-once to help me determine the optimal lags.</p>

<p>I donâ€™t, however, know how to do that or even if this is a statistically sound technique. </p>

<p>At the moment I am just analysing my residuals (which have come from a linear regression with independent variables such as Home/Away and Team Possession) with code such as the following:</p>

<pre><code>arima(residuals, order=c(3,0,0))
</code></pre>

<p>Is there a way to instruct R to perform this ARIMA analysis whilst looking at lots of mini-groups (where the groups are categorised by player name)?</p>

<p>Any help would be much appreciated. </p>

<p>Will </p>
"
"0.33218191941496","0.372677996249965","163181","<p>I'm running a logistic regression to find a relationship between falls and drugs taken by someone. What happens is that every time I re-run the algorithm it gives a different result. </p>

<p>The table is this:</p>

<pre><code>caseID fallFlag hypSeds antiPsycho antiHypertensives NSAIDs centralMuscleRelax
     1     TRUE   FALSE      FALSE             FALSE  FALSE              TRUE
     2    FALSE    TRUE      FALSE             TRUE   FALSE              FALSE
     3     TRUE   FALSE      TRUE              FALSE  TRUE               TRUE
     4    FALSE   FALSE      FALSE             FALSE  FALSE              FALSE
     5     TRUE   FALSE      TRUE              FALSE  FALSE              FALSE
     6    FALSE   FALSE      FALSE             FALSE  FALSE              FALSE
</code></pre>

<p>The <code>TRUE</code> flags mean that the individual took that medicine, and <code>FALSE</code> otherwise. </p>

<p>The algorithm to perform the logistic regression is the following</p>

<pre><code># Match column labels
cols &lt;- c(""hypSeds"", ""antiPsycho"", ""antiHypertensives"", ""NSAIDs"", ""centralMuscleRelax"")

# Data frame to store the OR and CIs 
coefficients &lt;- data.frame(drugNames=cols)

# This loop run through the match labels
# - perform a logistic regression for each classifier
# - get the OR and CIs coefficients and store the coefficients into a data frame

for(i in 1:length(cols)){
  eqString  &lt;- as.formula(paste(""fallFlag"", cols[i], sep=""~""))
  model     &lt;- glm(eqString, observation, family=""binomial"")
  modelCoef &lt;- exp(cbind(coef(model), confint(model)))

  coefficients$OR[i]    &lt;- modelCoef[2] # odds ratios
  coefficients$CIMin[i] &lt;- modelCoef[4] # lower confidence limit
  coefficients$CIMax[i] &lt;- modelCoef[6] # upper confidence limit
}
</code></pre>

<p>In this algorithm I run a logistic regression on each of the drug categories against the <code>fallFlag</code>. Then, I exponentiate the coefficients to find the odds ratios.  </p>

<p>Every time I restart the R studio and run this algorithm it results differently. For example, here is an actual result:  </p>

<pre><code>      drugNames          OR        CIMin     CIMax
1     hypSeds            1.4347210 1.2534578 1.643824
2     antiPsycho         2.1583970 1.8225014 2.564792
3     antiHypertensives  1.0327465 0.9041444 1.179742
4     NSAIDs             0.9857518 0.8824338 1.101139
5     centralMuscleRelax 0.9597043 0.7240041 1.271461
</code></pre>

<p>But the previous result was:</p>

<pre><code>      drugNames          OR        CIMin     CIMax
1     hypSeds            1.2870853 1.1286756 1.468686
2     antiPsycho         1.9665091 1.6684292 2.324333
3     antiHypertensives  1.1718176 1.0218085 1.344455
4     NSAIDs             1.0263196 0.9178526 1.147658
5     centralMuscleRelax 1.2014783 0.8928298 1.621132
</code></pre>

<p>As you can see the results were very different, and this has been happening every time I load and build the observation table again. It's important to note that all the runs have been performed in the same machine. </p>
"
"0.131306432859723","0.117851130197758","167142","<p>this is my first post, I'll do my best to be clear and complete.</p>

<p>I am trying to run a pgmm regression (Arellano Bond estimator) following the example online with the EmplUK dataset.</p>

<p>My dataset is unbalanced, with some missing values (that I also removed, without any difference). This is the paste from R' dataframe.</p>

<pre><code> row.names ID   Year    p       I
    1   23  1   1992    NA      NA
    2   22  1   1993    17.01   NA
    3   21  1   1994    15.86   NA
    4   20  1   1995    17.02   7.512347
    5   19  1   1996    20.64   7.685104
    6   18  1   1997    19.11   12.730282
    7   17  1   1998    12.76   12.633871
    8   16  1   1999    17.90   7.416381
    9   15  1   2000    28.66   6.396114
    10  14  1   2001    24.46   9.213729
    11  13  1   2002    24.99   20.117159
    12  12  1   2003    28.85   11.117816
    13  11  1   2004    38.26   11.242638
    14  10  1   2005    54.57   13.015168
    15  9   1   2006    65.16   18.507212
    16  8   1   2007    72.44   18.875281
    17  7   1   2008    96.94   24.459170
    18  6   1   2009    61.74   21.332035
    19  5   1   2010    79.61   17.119038
    20  4   1   2011    111.26  16.941914
    21  3   1   2012    111.63  19.964875
    22  2   1   2013    108.56  28.863894
    23  1   1   2014    99.03   15.182615
    24  45  2   1993    17.01   NA
    25  44  2   1994    15.86   NA
    26  43  2   1995    17.02   NA
    27  42  2   1996    20.64   NA
    28  41  2   1997    19.11   NA
    29  40  2   1998    12.76   NA
    30  39  2   1999    17.90   11.428262
    31  38  2   2000    28.66   20.232613
    32  37  2   2001    24.46   25.811754
    33  36  2   2002    24.99   18.959958
    34  35  2   2003    28.85   20.767074
    35  34  2   2004    38.26   29.260406
    36  33  2   2005    54.57   25.837434
    37  32  2   2006    65.16   32.675618
    38  31  2   2007    72.44   48.415190
    39  30  2   2008    96.94   42.444435
    40  29  2   2009    61.74   40.047462
    41  28  2   2010    79.61   49.090816
    42  27  2   2011    111.26  53.828050
    43  26  2   2012    111.63  61.684020
    44  25  2   2013    108.56  68.394140
    45  24  2   2014    99.03   55.738584
    46  76  3   1984    NA      NA
    47  75  3   1985    NA      NA
    48  74  3   1986    NA      NA
    49  73  3   1987    18.53   NA
    50  72  3   1988    14.91   NA
    51  71  3   1989    18.23   NA
    52  70  3   1990    23.76   17.046268
    53  69  3   1991    20.04   30.191128
    54  68  3   1992    19.32   30.414108
    55  67  3   1993    17.01   27.916000
    56  66  3   1994    15.86   26.437651
    57  65  3   1995    17.02   25.895513
    58  64  3   1996    20.64   26.791996
    59  63  3   1997    19.11   30.074375
    60  62  3   1998    12.76   42.636103
    61  61  3   1999    17.90   46.862510
    62  60  3   2000    28.66   30.154079
    63  59  3   2001    24.46   30.297644
    64  58  3   2002    24.99   34.851205
    65  57  3   2003    28.85   38.854943
    66  56  3   2004    38.26   37.542447
    67  55  3   2005    54.57   38.456399
    68  54  3   2006    65.16   43.465535
    69  53  3   2007    72.44   41.749414
    70  52  3   2008    96.94   48.371262
    71  51  3   2009    61.74   54.914470
    72  50  3   2010    79.61   65.444964
    73  49  3   2011    111.26  76.888119
    74  48  3   2012    111.63  81.833602
    75  47  3   2013    108.56  83.800483
    76  46  3   2014    99.03   79.713947
</code></pre>

<p>my codes are the following:</p>

<pre><code>data &lt;- plm.data(Autoregression,index=c(""ID"",""Year""))


Panel &lt;- subset(data, !is.na(I) )

Are &lt;- pgmm( I~p+lag( I , 0:1) 
            | lag(I, 2:99),
            data = Panel, effect = ""twoways"", model = ""onestep"")
</code></pre>

<p>I have tried also many other versions, including every possible number of the lags, shorter or longer.</p>

<p>The error is the following :</p>

<pre><code>Errore in solve.default(crossprod(WX, t(crossprod(WX, A1)))) : 
  Lapack routine dgesv: system is exactly singular: U[3,3] = 0
Inoltre: Warning message:
In pgmm(I ~ lag(I, 1) + p | lag(I, 2:10), Panel, effect = ""twoways"",  :
  the first-step matrix is singular, a general inverse is used
</code></pre>

<p>Can you please help me? Thanks for the attention, i'll wait for an answer</p>

<p>Regards, Luca.</p>
"
"0.297775000191279","0.267261241912424","173629","<p>When applying the ""urca"" package function <code>ur.df</code>, like </p>

<pre><code>summary(ur.df(data$col1, type = c(""none""), lags = 12, selectlags = c(""AIC"")))
</code></pre>

<p>I get following result:</p>

<pre><code>############################################### 
# Augmented Dickey-Fuller Test Unit Root Test # 
############################################### 

Test regression trend 


Call:
lm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag)

Residuals:
      Min        1Q    Median        3Q       Max 
-12928366  -2888728   1284718   4218373   7179531 

Coefficients:
                 Estimate    Std. Error  t value  Pr(&gt;|t|)   
(Intercept)  5.391984e+07  1.638362e+07  3.29108 0.0043123 **
z.lag.1     -2.438154e+00  7.557134e-01 -3.22629 0.0049588 **
tt           6.579260e+05  2.730453e+05  2.40959 0.0275861 * 
z.diff.lag1  1.712004e+00  6.595980e-01  2.59553 0.0188537 * 
z.diff.lag2  1.402824e+00  6.379412e-01  2.19899 0.0420083 * 
z.diff.lag3  1.321555e+00  5.294537e-01  2.49607 0.0231329 * 
z.diff.lag4  1.099430e+00  4.720412e-01  2.32910 0.0324428 * 
z.diff.lag5  8.132753e-01  4.181477e-01  1.94495 0.0685140 . 
z.diff.lag6  1.797331e-01  3.654326e-01  0.49184 0.6291254   
z.diff.lag7  5.890640e-01  2.939590e-01  2.00390 0.0612825 . 
z.diff.lag8  3.919041e-01  2.794371e-01  1.40248 0.1787705   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 6708593 on 17 degrees of freedom
Multiple R-squared:  0.7237276, Adjusted R-squared:  0.5613144 
F-statistic: 4.253547 on 10 and 17 DF,  p-value: 0.003348755


Value of test-statistic is: -3.2263 3.9622 5.2635 

Critical values for test statistics: 
      1pct  5pct 10pct
tau3 -4.15 -3.50 -3.18
phi2  7.02  5.13  4.31
phi3  9.31  6.73  5.61
</code></pre>

<p>Now the question:</p>

<ol>
<li>I do understand that ""-3.2263"" is the critical value (t-value)</li>
<li><strong>There is a unit root</strong> with trend since -3.2263 > -3.18 (tau3@10pct)
This means the time-series is <strong>non-stationary</strong> at a 10% significance level.</li>
<li>But, what is the meaning of ""p-value: 0.003348755""? Should I list this value in a table summarizing my unit root test results or rather mark the 0.1 significance level (*10%)?</li>
</ol>

<p>The <a href=""http://www.inside-r.org/packages/cran/urca/docs/ur.df"" rel=""nofollow"">documentation</a> says that critical values are based on Hamilton (1994) and Dickey and Fuller (1981)"". </p>
"
"0.321633760451338","0.240562612162344","180285","<p>I'm pretty new to the concepts of stationarity/cointegration. I am using the ""urca"" package in ""Rstudio"" to run my tests.</p>

<p>I have been trying to run cointegration tests, but the frustrating thing is that I haven't been able to find two series that are non-stationary, even when I try using examples cited by cointegration tutorials. My $p$-value is always too big such that I have to reject the null straight away. However, if I look at the $t$-values and compare them to the critical values, they seem to suggest otherwise. </p>

<p>Should I then ignore the $p$-value in the ADF test?
Here are my test results. My two price series are <code>XLE US Equity</code> and <code>CO1 Comdty</code> (Brent 1st futures) from 01/01/2010 - today (5/11/2015).</p>

<p>Any help/elaboration will be very much appreciated, thank you!</p>

<pre><code>&gt; testXLE&lt;-ur.df(XLE,type=""drift"",selectlags=""AIC"")
&gt; summary(testXLE)

############################################### 
# Augmented Dickey-Fuller Test Unit Root Test # 
############################################### 

Test regression drift 


Call:
lm(formula = z.diff ~ z.lag.1 + 1 + z.diff.lag)

Residuals:
     Min       1Q   Median       3Q      Max 
-10.3948  -2.5809   0.6846   2.7908  10.1940 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)  6.58864    3.43524   1.918   0.0596 .
z.lag.1     -0.08584    0.04533  -1.894   0.0628 .
z.diff.lag   0.05529    0.12544   0.441   0.6609  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 4.162 on 64 degrees of freedom
Multiple R-squared:  0.05337,   Adjusted R-squared:  0.02379 
F-statistic: 1.804 on 2 and 64 DF,  p-value: 0.1729


Value of test-statistic is: -1.8936 1.8395 

Critical values for test statistics: 
      1pct  5pct 10pct
tau2 -3.51 -2.89 -2.58
phi1  6.70  4.71  3.86
</code></pre>

<p>My interpretation of the results:</p>

<blockquote>
  <ul>
  <li>according to p-value (0.1729>0.05) do not reject null; series is stationary   </li>
  <li>t-value = (-1.8936>-2.89) --> do not reject null hypothesis; series is not stationary  </li>
  <li>t-value = (1.8395&lt;4.71) --> do not reject a0=0 --> there is no drift</li>
  </ul>
</blockquote>

<p>Conclusion: The series is non-stationary: Random Walk with no drift.</p>
"
"0.416953923968397","0.374227599591874","180854","<p>I am trying to test for cointegration between two series that based on qualitative reasoning, should be cointegrated. They are the prices of XLE ETF (<code>XLE US equity</code>) and 1st futures of Brent (<code>CO1 Comdty</code>). However, the results that I arrive at using two different methods both show that there exists no cointegration between the two series - not sure if my execution or the interpretation of the data is wrong? </p>

<p>(Both XLE and Brent 1st Futures have been tested for non-stationarity using ADF test from ""urca"" package)</p>

<p><strong>1st test - Engle Granger 2-step test:</strong><br>
In doing this, I referenced <em>Using R to Test Pairs of Securities for Cointegration</em> by Paul Teetor</p>

<p><strong>(1)</strong> Conducting Spread</p>

<pre><code>&gt; M&lt;-lm(XLE~Brent+0,data=XLE.Brent)
&gt; beta&lt;-coef(M)[1]
&gt; spread&lt;-XLE.Brent$XLE-beta*XLE.Brent$Brent
&gt; 
&gt; summary(M)

Call:
lm(formula = XLE ~ Brent + 0, data = XLE.Brent)

Residuals:
   Min      1Q  Median      3Q     Max 
-20.363  -9.543  -2.909  13.294  36.269 

Coefficients:
     Estimate Std. Error t value Pr(&gt;|t|)    
&gt;Brent  0.74962    0.02004    37.4   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 16.37 on 68 degrees of freedom
Multiple R-squared:  0.9536,    Adjusted R-squared:  0.953 
F-statistic:  1399 on 1 and 68 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p><strong>(2)</strong> Testing the stationarity of the spread using ADF test (from package ""urca""):</p>

<pre><code>&gt; spread.ADF&lt;-ur.df(spread,type=""none"",selectlags=""AIC"")
&gt; summary(spread.ADF)

############################################### 
# Augmented Dickey-Fuller Test Unit Root Test #  
</code></pre>

#########################################

<pre><code>Test regression none 


Call:
lm(formula = z.diff ~ z.lag.1 - 1 + z.diff.lag)

Residuals:
   Min      1Q  Median      3Q     Max 
 -6.1449 -2.2523  0.5559  2.9194  8.4567 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
z.lag.1    -0.0003928  0.0266919  -0.015    0.988
z.diff.lag  0.1207084  0.1278700   0.944    0.349

Residual standard error: 3.443 on 65 degrees of freedom
Multiple R-squared:  0.01395,   Adjusted R-squared:  -0.01639 
F-statistic: 0.4596 on 2 and 65 DF,  p-value: 0.6335


Value of test-statistic is: -0.0147 

Critical values for test statistics: 
    1pct  5pct 10pct
tau1 -2.6 -1.95 -1.61
</code></pre>

<p>My interpretation: since $t$-value = <code>-0.0147</code> is bigger than <code>-1.61</code>, do not reject null. Spread is not stationary. Hence no cointegration between XLE and Brent.</p>

<p><strong>Second Test: Johansen Test</strong></p>

<pre><code>&gt; XLE.brent.coint&lt;-ca.jo(data.frame(XLE,Brent),type=""trace"",ecdet=""trend"",K=2,spec=""longrun"")
&gt; summary(XLE.brent.coint)
&gt;
&gt;###################### 
&gt;# Johansen-Procedure # 
&gt;###################### 
&gt;
&gt;Test type: trace statistic , with linear trend in cointegration 
&gt;
&gt;Eigenvalues (lambda):
&gt;[1] 8.179514e-02 6.025284e-02 2.775558e-17
&gt;
&gt;Values of teststatistic and critical values of test:
&gt;
&gt;        test 10pct  5pct  1pct
&gt;r &lt;= 1 | 4.16 10.49 12.25 16.26
&gt;r = 0  | 9.88 22.76 25.32 30.45
&gt;
&gt;Eigenvectors, normalised to first column:
&gt;(These are the cointegration relations)
&gt;
&gt;          XLE.l2   Brent.l2   trend.l2
&gt;XLE.l2   1.000000  1.0000000  1.0000000
&gt;Brent.l2 1.467806 -0.4346323  0.1610563
&gt;trend.l2 1.896366 -0.4903454 -0.8891875
&gt;
&gt;Weights W:
&gt;(This is the loading matrix)
&gt;
&gt;            XLE.l2    Brent.l2      trend.l2
&gt;XLE.d   -0.01629102 -0.13534537 -4.695795e-17
&gt;Brent.d -0.03819241 -0.03886418  5.127543e-17
</code></pre>

<p>My interpretation: Since $t$-value for <code>r=0: 9.88&lt;22.76</code>, do not reject null. Hence <code>r=0</code>, there exists no cointegration between <strong>XLE and Brent</strong>.</p>

<p>Additionally, I have carried out cointegration tests (both methods) on <strong>US 10 year and 2 year yields</strong>, and the results on both tell me that the series are not co-integrated, which does not make sense intuitively. Something must be wrong with the way I'm doing the tests!</p>

<p><a href=""http://i.stack.imgur.com/eApO4.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/eApO4.jpg"" alt=""enter image description here""></a>
<a href=""http://i.stack.imgur.com/sf2HV.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/sf2HV.jpg"" alt=""enter image description here""></a></p>
"
"0.293610109757352","0.263523138347365","191851","<p>I am building a VAR model to forecast the price of an asset and would like to know whether my method is statistically sound, whether the tests I have included are relevant and if more are needed to ensure a reliable forecast based on my input variables. </p>

<p>Below is my current process to check for Granger causality and forecast the selected VAR model.</p>

<pre><code>require(""forecast"")
require(""vars"")

#Read Data
da=read.table(""VARdata.txt"", header=T)
dac &lt;- c(2,3) # Select variables
x=da[,dac]

plot.ts(x)
summary(x)

#Run Augmented Dickey-Fuller tests to determine stationarity and differences to achieve stationarity.
ndiffs(x[, ""VAR1""], alpha = 0.05, test = c(""adf""))
ndiffs(x[, ""VAR2""], alpha = 0.05, test = c(""adf""))

#Difference to achieve stationarity
d.x1 = diff(x[, ""VAR1""], differences = 2)
d.x2 = diff(x[, ""VAR2""], differences = 2)

dx = cbind(d.x1, d.x2)
plot.ts(dx)

#Lag optimisation
VARselect(dx, lag.max = 10, type = ""both"")

#Vector autoregression with lags set according to results of lag optimisation. 
var = VAR(dx, p=2)

#Test for serial autocorrelation using the Portmanteau test
#Rerun var model with other suggested lags if H0 can be rejected at 0.05
serial.test(var, lags.pt = 10, type = ""PT.asymptotic"")

#ARCH test (Autoregressive conditional heteroscedasdicity)
arch.test(var, lags.multi = 10)

summary(var)

#Granger Causality test
#Does x1 granger cause x2?
grangertest(d.x2 ~ d.x1, order = 2)

#Does x2 granger cause x1?
grangertest(d.x1 ~ d.x2, order = 2)

#Forecasting
prd &lt;- predict(var, n.ahead = 10, ci = 0.95, dumvar = NULL)
print(prd)
plot(prd, ""single"")
</code></pre>

<p>Is this method sound?</p>
"
"0.306381676672686","0.353553390593274","200155","<p>Just writing with a question about fixed effects when you have panel data with</p>

<ul>
<li>More than two time periods</li>
<li>Clusters, as in individual students within schools</li>
<li>An intervention given to some of the clusters mid-way through panel</li>
</ul>

<p>In particular, I'm trying to understand the diff-in-diff model used in a particular paper, ""<em>The Effects of Targeted Recruitment and Comprehensive Supports for Low-Income High Achievers at Elite Universities: Evidence from Texas Flagships</em>,"" by Rodney Andrews, Scott Imberman, and Michael Lovenheim.
You can find a copy of their paper here: 
<a href=""https://www.msu.edu/~imberman/LOS-CS%20-%209-4-15.pdf"" rel=""nofollow"">https://www.msu.edu/~imberman/LOS-CS%20-%209-4-15.pdf</a></p>

<p>Their regression equation is at the very bottom of page 17 (which is what I'm curious about).</p>

<p>Here's a short, super simplified summary, for the sake of my question, which is really about diff-in-diff and fixed effects, rather than the particulars of the paper itself:</p>

<p>The authors have a bunch of high schools in Texas. Each of these schools has many students. The schools and students are observed for a few years. Then some of the schools receive an intervention, meant to increase students' college-going and eventually their earnings as adults. Pretend for this example it's just a college scholarship program.* We observe the students in all these schools as they complete high school (or don't), enter college (or don't), and hopefully earn adult incomes. </p>

<p>The diff-in-diff part is comparing the change in adult earnings across cohorts of students that went to schools that received the intervention, compared to the trend among student cohorts that didn't attend the intervention high schools. </p>

<p>I've created some pretend data that tries to mimic the authors' real data in greatly simplified form. You can find that here, along with my (probably wrong) R script:
<a href=""https://drive.google.com/folderview?id=0B6Sk_VEqK32Gb1M4bVQxYVIzOTQ&amp;usp=sharing"" rel=""nofollow"">https://drive.google.com/folderview?id=0B6Sk_VEqK32Gb1M4bVQxYVIzOTQ&amp;usp=sharing</a></p>

<p>(You'll need to paste the cells into excel, which I used to create the values, since google docs doesn't have the same formulas. Apologies for the inconvenience.)</p>

<p>It's got 8 variables</p>

<ul>
<li>""id"": a row index variable</li>
<li>""student"": indexes students within schools, across cohorts. So the index goes from 1 to 20 within each school. But there are only 5 students per cohort, or high school graduating class, because there are 4 time periods. (not sure if that was smart).</li>
<li>""random_uniform"": just a uniform random variable between 0.01 and 0.99. I just used this to create the next variable.</li>
<li>""test_score"": a covariate, student test score. All are normally distributed with a standard deviation of 4, and a mean that's specific to the school and graduating cohort. For interest, I made it so that some schools started with lower overall means, but each school's mean score improved a little over time (about 10 points). All the scores are around 40-60. </li>
<li>""school"": a factor variable that indicates the school. There are 4 schools.</li>
<li>""treat_indicator"": a factor variable that is 0 before the intervention, and 1 after the intervention at the schools that receive the intervention (schools 1 and 2).</li>
<li>""time_period"": a factor variable that denotes the graduating class cohort. There are 4. </li>
<li>""adult_earnings"": a numeric variable that's a function of the students' high school test score (""test_score""), plus a bunch of noise. For the kids that received the ""college scholarship"" intervention--kids in the latter 2 cohorts at schools 1 &amp; 2--I've also added an additional earnings bump between 1,000 and 2,000 dollars, to simulate a treatment effect. </li>
</ul>

<p><strong>So my question is, how do I find the true effect of the intervention, if I want to use both high school graduating cohort (time) and school (cluster) fixed effects?</strong> </p>

<p>My R script is in that shared folder, but I'm not sure it's correct. The regression equation I gave R was</p>

<pre><code>fixedreg &lt;- lm(adult_earnings ~ treat_indicator + test_score + school + time_period, 
                     data=mydata)
</code></pre>

<p>Does that model the time and cluster fixed effects correctly, and create an unbiased coefficient on the ""treat_indicator"" variable?</p>

<p>Any insight would be much appreciated. Thanks!</p>

<hr>

<p>*Or read the actual paper and laugh at my ridiculous attempt to simplify all this.</p>
"
"0.262612865719445","0.235702260395516","221011","<p>I have two monthly time series: </p>

<ul>
<li>one for house prices expressed in annual change growth rates: $\left( \text{ln}(X_t) - \text{ln}(X_{t-12})\right) - \left( \text{ln}(X_{t-1}) - \text{ln}(X_{t-1-12})\right)$;</li>
<li>the other simply in growth rates: $\text{ln}(X_t) - \text{ln}(X_{t-1})$. </li>
</ul>

<p>Here is the data: </p>

<pre><code>House Prices = [1]  0.009189829  0.022612618  0.003952796 -0.015179184  0.001903336 -0.028779902  0.025668239 -0.011237850
  [9]  0.014782630 -0.018844480 -0.023547458  0.020613233  0.029281069 -0.010539781  0.006707366  0.023693144
 [17] -0.002632498  0.148738752 -0.154539337  0.013908319 -0.002294980  0.013274177  0.010043605 -0.007862785
 [25] -0.018297295 -0.003167249  0.022984841  0.001666694 -0.001310199 -0.131548705  0.114723242 -0.003431495
 [33]  0.000953231 -0.010096108 -0.009434595 -0.037774255  0.030877947 -0.011245971 -0.018800312 -0.012805013
 [41]  0.001326392 -0.012034079 -0.045279346 -0.017308170  0.002490863 -0.007340975  0.005052948 -0.024053201
 [49] -0.004190424 -0.028607790  0.004678486  0.026626293 -0.015166864  0.006988983  0.038257855  0.020798177
 [57]  0.008175391  0.021294030 -0.013331432  0.030969145  0.017065249 -0.002672683  0.019435476 -0.037047871
 [65]  0.001844432  0.007663458  0.034406137 -0.049379845 -0.012527106 -0.012859680  0.012954488 -0.015463951
 [73] -0.025509006  0.006318645  0.012977464  0.019940525 -0.025592828  0.020774198 -0.033613414  0.018338077
 [81]  0.001765807  0.009236604 -0.041413104  0.030227358  0.017180849  0.012593360 -0.039001526 -0.004994992
 [89]  0.037766071 -0.043167230 -0.016613786  0.023199890 -0.016214873 -0.012282560  0.065978520 -0.031465767
 [97]  0.006355108 -0.000449523 -0.005810647  0.016823517 -0.021988463  0.026178014  0.007654339 -0.008356379
[105]  0.013273736  0.031645473 -0.046408064  0.022334664  0.008517194 -0.014892335  0.019147342  0.007955040
[113]  0.014122506 -0.035722162  0.018174284  0.021410306 -0.038943797 -0.014517888  0.032750195  0.022506553
[121] -0.003870785  0.130924075 -0.057934974 -0.174228244  0.016937619  0.010647759  0.015691962 -0.033174094
[129]  0.038263205  0.003456250 -0.013422897

B = [1] -0.0223848461  0.0102749646  0.0913403867 -0.0758207770 -0.0053898407 -0.0204047336  0.0050358986
  [8]  0.0195335195 -0.0200303353 -0.0045390828  0.0056380761 -0.0004492945  0.0040043649  0.0012918928
 [15] -0.0104850394  0.0047110190  0.0049805985 -0.0046957178  0.0095002549  0.0202597343 -0.0183526932
 [22]  0.0237185217 -0.0137022065  0.0133787918 -0.0212629487  0.0070512978  0.0959447868 -0.0801519036
 [29] -0.0362526334 -0.0000278572  0.0269014993  0.0009862920 -0.0329868357  0.0283667004 -0.0135186142
 [36] -0.0004975495  0.0053822189  0.0108219907 -0.0078419784  0.0418340658 -0.0316367599 -0.0092324801
 [43] -0.0192830637  0.0336003682  0.0021479539 -0.0146426306  0.0003717930  0.0216259502 -0.0323127786
 [50]  0.0033077606 -0.0123735085 -0.0014757035  0.0266339779 -0.0228959378  0.0002848944  0.0133572802
 [57] -0.0093035312 -0.0034350607  0.0052349772  0.0115210916 -0.0122443122  0.0435497970 -0.0100099291
 [64]  0.0267252321 -0.0654005679  0.0088385287 -0.0089122237  0.0155299273 -0.0027394997 -0.0126183268
 [71]  0.0090999709  0.0017039487 -0.0144843611  0.0269128625  0.0042663583  0.0220574344 -0.0523831016
 [78] -0.0059331639  0.0171559908  0.0125030653  0.0151902738  0.0471484001 -0.0477394702  0.0888317354
 [85] -0.1044700154  0.0234134906 -0.0215966718  0.0157974035  0.0970094980 -0.1049559862 -0.0290578406
 [92]  0.0617653831 -0.0132202439  0.0022117274  0.0091225692  0.0424813190 -0.0614889434  0.0163745828
 [99] -0.0112793057  0.0666179349 -0.0352838073 -0.0259179501  0.0269557599  0.0127882202 -0.0430512536
[106]  0.0862308560 -0.0633012329  0.0596481270  0.0900367605 -0.0303162498 -0.0153738373 -0.0442218848
[113] -0.0116158350 -0.0531058308  0.2036373944  0.1598602057 -0.3837940703 -0.0069112146 -0.0192015196
[120]  0.0110269191 -0.0351135484  0.0439917033  0.0522746614  0.0036354828 -0.0414276671 -0.0361649669
[127]  0.0080753079  0.0352684982 -0.0282391428 -0.0141622744  0.0045799464
</code></pre>

<p>I am studying if <code>B</code> has an effect on <code>House prices</code>. For this reason first a take a simple liner regression between the two and I get a negative and significant estimate at the 95% confidence interval: (-0.0004189 *). </p>

<p>Wanting to reach a step forward I undertake a Granger causality test as following:</p>

<p>I) Determine the optimal number of lags using the AIC/BIC test using:</p>

<pre><code>select.lags&lt;-function(x,y,max.lag=20) {
  y&lt;-as.numeric(y)
  y.lag&lt;-embed(y,max.lag+1)[,-1,drop=FALSE]
  x.lag&lt;-embed(x,max.lag+1)[,-1,drop=FALSE]

  t&lt;-tail(seq_along(y),nrow(y.lag))

  ms=lapply(1:max.lag,function(i) lm(y[t]~y.lag[,1:i]+x.lag[,1:i]))

  pvals&lt;-mapply(function(i) anova(ms[[i]],ms[[i-1]])[2,""Pr(&gt;F)""],max.lag:2)
  ind&lt;-which(pvals&lt;0.05)[1]
  ftest&lt;-ifelse(is.na(ind),1,max.lag-ind+1)

  aic&lt;-as.numeric(lapply(ms,AIC))
  bic&lt;-as.numeric(lapply(ms,BIC))
  structure(list(ic=cbind(aic=aic,bic=bic),pvals=pvals,
                 selection=list(aic=which.min(aic),bic=which.min(bic),ftest=ftest)))
}

s&lt;-select.lags(Topic.15,House.Prices,20)
t(s$selection)
plot.ts(s$ic)
</code></pre>

<p>As a result I get:     </p>

<pre><code>aic bic ftest
14  12  13   
</code></pre>

<p>Here is when it comes the first doubt: why are they giving me different results? Nevertheless, when I do the Granger causality test for both directions using these numbers as possible lags I get in all high significant results (***) only in the direction that <code>B</code> is causing <code>House prices</code> movements:</p>

<pre><code>lmtest::grangertest(Topic.15,House.Prices,12)
lmtest::grangertest(House.Prices,Topic.15,12)
</code></pre>

<p>I do not seem to see the direction of the cause, is it possitive or negative (an increase in <code>B</code> produces an increase or a drop in <code>House prices</code> at time $t+1$?).<br>
Another question, is the conclusion valid that changes in <code>B</code> produce changes in <code>House prices</code>? What are the weakness in this line of argument?</p>
"
"0.131306432859723","0.117851130197758","231774","<p>I'm trying to understand the difference between XTREG and PLM.  First, I have looked at this answered question:</p>

<p><a href=""http://stats.stackexchange.com/questions/66973/difference-between-fixed-effects-models-in-r-plm-and-stata-xtreg?newreg=2e066243824749e8b789ff8296f47eb6"">Difference between fixed effects models in R (plm) and Stata (xtreg)</a></p>

<p>But when I try the code provided by the answerer, I get different answers from R and Stata.  The STATA results match those of the answerer, but the R results do not.</p>

<p>I have an inkling for why.  When I execute that code in R, R doesn't create lags within the grouping variable, it creates lags overall.  For example, if there are 50 states and 17 years, when including a lag in the regression, I will lose 50 observations: the first year for each state.  In STATA, the sample size reduces accordingly.  In R, the sample size reduces by 1. This is because its not identifying the ""state"" grouping variable.  So, does anyone have an idea of what is going on here?</p>
"
"0.0758098043578903","0.136082763487954","232360","<p>I've got this data frame in which all the 'B_X' columns are the predictors for all the 'A_X' columns. I want to filter out the non-important variables among 'B_X'. The difficulty lies in that we've got only ordered cat./cat and flags as variables.</p>

<p>My thoughts so far: I could try and apply some kind of dimension reduction methods for all 'A_X', but I have no experience in that with flags. (Although I have heard something about CATEGORICAL PCA (in SPSS for example), but I am not sure whether it could work for flags) After that, perhaps I could build a regression for the components which I've got from the previous step, using 'B_X' as predictors...?</p>

<p>Also, I am not sure whether a 'simple' association test between 'B_X' could be considerable?</p>

<pre><code>df &lt;- data.frame(A_1 = as.factor(sample(c(0, 1), 5000, replace = TRUE)),
                 A_2 = as.factor(sample(c(0, 1), 5000, replace = TRUE)),
                 A_3 = as.factor(sample(c(0, 1), 5000, replace = TRUE)),
                 A_4 = as.factor(sample(c(0, 1), 5000, replace = TRUE)),
                 A_5 = as.factor(sample(c(0, 1), 5000, replace = TRUE)),
                 A_6 = as.factor(sample(c(0, 1), 5000, replace = TRUE)),
                 A_7 = as.factor(sample(c(0, 1), 5000, replace = TRUE)),
                 A_8 = as.factor(sample(c(0, 1), 5000, replace = TRUE)),
                 A_9 = as.factor(sample(c(0, 1), 5000, replace = TRUE)),
                 A_10 = as.factor(sample(c(0, 1), 5000, replace = TRUE)),
                 A_11 = as.factor(sample(c(0, 1), 5000, replace = TRUE)),
                 A_12 = as.factor(sample(c(0, 1), 5000, replace = TRUE)),
                 A_13 = as.factor(sample(c(0, 1), 5000, replace = TRUE)),
                 A_14 = as.factor(sample(c(0, 1), 5000, replace = TRUE)),
                 A_15 = as.factor(sample(c(0, 1), 5000, replace = TRUE)),
                 A_16 = as.factor(sample(c(0, 1), 5000, replace = TRUE)),
                 A_17 = as.factor(sample(c(0, 1), 5000, replace = TRUE)),
                 A_18 = as.factor(sample(c(0, 1), 5000, replace = TRUE)),
                 B_1 = ordered(sample(c(0, 1, 2, 3, 4, 5), 5000, replace = TRUE)),
                 B_2 = ordered(sample(c(0, 1, 2, 3, 4), 5000, replace = TRUE)),
                 B_3 = ordered(sample(c(0, 1, 2, 3, 4), 5000, replace = TRUE)),
                 B_4 = as.factor(sample(c(0, 1, 2, 3, 4), 5000, replace = TRUE)),
                 B_5 = as.factor(sample(c(0, 1, 2, 3, 4), 5000, replace = TRUE)),
                 B_6 = as.factor(sample(c(0, 1, 2, 3, 4), 5000, replace = TRUE)),
                 B_7 = as.factor(sample(c(0, 1), 5000, replace = TRUE)),
                 B_8 = as.factor(sample(c(0, 1), 5000, replace = TRUE)),
                 B_9 = as.factor(sample(c(0, 1), 5000, replace = TRUE)),
                 B_10 = as.factor(sample(c(0, 1), 5000, replace = TRUE)),
                 B_11 = as.factor(sample(c(0, 1), 5000, replace = TRUE)))
</code></pre>

<p>What are your thoughts on this?</p>
"
