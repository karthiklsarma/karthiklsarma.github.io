"V1","V2","V3","V4"
"0.0729972383233908","0.0750234484920533","   837","<p>I am creating multiple logistic regression models using lrm from Harrell's Design package in R.  One model I would like to make is the model with no predictors.  For example, I want to predict a constant c such that: </p>

<pre><code>logit(Y) ~ c
</code></pre>

<p>I know I how to compute c (divide the number of ""1""s by the total), what I would like is to use <code>lrm</code> so I can manipulate it as a model in a consistent way with the other models I am making.  Is this possible, and if so how?  </p>

<p>I have tried so far:</p>

<pre><code>library(Design)
data(mtcars)
lrm(am ~ 1, data=mtcars)
</code></pre>

<p>which gives the error:</p>

<pre><code>Error in dimnames(stats) &lt;- list(names(cof), c(""Coef"", ""S.E."", ""Wald Z"",  :
    length of 'dimnames' [1] not equal to array extent
</code></pre>

<p>and I have tried:</p>

<pre><code>lrm(am ~ ., data=mtcars)
</code></pre>

<p>But this uses all the predictors, rather then none of the predictors.</p>
"
"0.0729972383233908","0.0750234484920533","  1413","<p>It seems like the current revision of lmer does not allow for custom link functions.  </p>

<ol>
<li><p>If one needs to fit a logistic
linear mixed effect model with a
custom link function what options
are available in R?</p></li>
<li><p>If none - what options are available in other
statistics/programming packages?</p></li>
<li><p>Are there conceptual reasons lmer
does not have custom link functions,
or are the constraints purely
pragmatic/programmatic?</p></li>
</ol>
"
"NaN","NaN","  3531","<p>I would like to perform reversible jump on a network model, but before arriving there, I'm wondering if there are any R packages which support reversible jump for a user specified generalized linear model or spatial-GLM?</p>

<p>Something as simple as an RJMCMC procedure (in R) for the selection of predictors in a logistic regression would be a nice place for me to start?  Does such a function exist?</p>

<p>Through googling, I've only found <a href=""http://cran.r-project.org/web/packages/RJaCGH/index.html"" rel=""nofollow"">RJaCGH</a> which appears to be a bit more complicated (and application specific) than I was hoping for.</p>
"
"NaN","NaN","  6412","<p>What is your favorite free tool on Linux for multivariate logistic regression?</p>

<p>Possibilities I've seen:</p>

<ul>
<li><a href=""http://www.r-project.org"" rel=""nofollow"">R</a> (see <a href=""http://nlp.stanford.edu/~manning/courses/ling289/logistic.pdf"" rel=""nofollow"">paper</a>).  <a href=""http://stackoverflow.com/questions/3439248/logistic-regression-in-r-sas-like-output"">This question</a> says use <a href=""http://cran.r-project.org/package=Design"" rel=""nofollow"">design</a>.</li>
<li>Can you use <a href=""http://docs.scipy.org/doc/scipy/reference/stats.html#statistical-functions"" rel=""nofollow"">SciPy</a>?</li>
</ul>

<p>Other choices?</p>

<p>Do people have experience with large data?</p>
"
"NaN","NaN","  8303","<p>I am fitting a binomial family glm in R, and I have a whole troupe of explanatory variables, and I need to find the best (R-squared as a measure is fine). Short of writing a script to loop through random different combinations of the explanatory variables and then recording which performs the best, I really dont know what to do. And the <code>leaps</code> function from package <strong><a href=""http://cran.r-project.org/web/packages/leaps/index.html"">leaps</a></strong> does not seem to do logistic regression.</p>

<p>Any help or suggestions would be greatly appreciated
Leendert   </p>
"
"0.127071881385753","0.130599060552097"," 11107","<p>I need to do a logistic regression using R on my data. My response variable (<code>y</code>) is survival at weaning (<code>surv=0</code>; did not <code>surv=1</code>) and I have several independent variables which are binary and categoricals in nature.</p>

<p>I am following some examples on this website <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/dae/logit.htm</a> and trying to run some models.</p>

<p>Running the model: </p>

<pre><code>&gt; mysurv2 &lt;- glm(surv~as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
                 as.factor(pmtone), family=binomial(link=""logit""), data=ap)
&gt; summary(mysurv2)

Call:
glm(formula = surv ~ as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
    as.factor(pmtone), family = binomial(link = ""logit""), data = ap)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.2837  -0.5121  -0.5121  -0.5058   2.0590  

Coefficients:
                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -0.01135    0.23613  -0.048  0.96166    
as.factor(PTEM)2   -0.74642    0.24482  -3.049  0.00230 ** 
as.factor(PTEM)3   -1.95401    0.23259  -8.401  &lt; 2e-16 ***
as.factor(pshiv)2  -0.02638    0.06784  -0.389  0.69738    
as.factor(presp)2   0.74549    0.10532   7.079 1.46e-12 ***
as.factor(presp)3   0.66793    0.66540   1.004  0.31547    
as.factor(pmtone)2  0.54699    0.09678   5.652 1.58e-08 ***
as.factor(pmtone)3  1.82337    0.75409   2.418  0.01561 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 7892.6  on 8791  degrees of freedom
Residual deviance: 7252.8  on 8784  degrees of freedom
  (341 observations deleted due to missingness)
AIC: 7268.8

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Adding the <code>na.action=na.pass</code> at the end of the model gave me an error message. I thought that this would take care NA's in my independent variables.</p>

<pre><code>&gt; mysurv &lt;- glm(surv~as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
                as.factor(pmtone), family=binomial(link=""logit""), data=ap, 
                na.action=na.pass)
Error: NA/NaN/Inf in foreign function call (arg 1)
</code></pre>

<p>Since this is my first time to venture into logistic regression, I am wondering whether there is any package in R that would be more suitable?</p>

<p>I am also tryng to understand the regression coefficients. The independent variables used in the model are:</p>

<ol>
<li><p>rectal temperature: </p>

<ul>
<li><code>(PTEM)1</code> = newborns with rectal temp. below 35.4 0C</li>
<li><code>(PTEM)2</code> = newborns with rectal temp. between 35.4 to 36.9 0C</li>
<li><code>(PTEM)3</code> = newborns with rectal temp. above 37.0 0C</li>
</ul></li>
<li><p>shivering:</p>

<ul>
<li><code>(pshiv)1</code> = newborns that were not shivering</li>
<li><code>(pshiv)2</code> = newborns that were shivering</li>
</ul></li>
<li><p>respiration:</p>

<ul>
<li><code>(presp)1</code> = newborns with normal respiration</li>
<li><code>(presp)2</code> = newborns with slight respiration problem</li>
<li><code>(presp)3</code> = newborns with poor respiration</li>
</ul></li>
<li><p>muscle tone:</p>

<ul>
<li><code>(pmtone)1</code> = newborns with normal muscle tone</li>
<li><code>(pmtone)2</code> = newborns with moderate muscle tone</li>
<li><code>(pmtone)1</code> = newborns with poor muscle tone</li>
</ul></li>
</ol>

<p>Looking at the coefficients, I got the following:</p>

<pre><code>                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -0.01135    0.23613  -0.048  0.96166    
as.factor(PTEM)2   -0.74642    0.24482  -3.049  0.00230 ** 
as.factor(PTEM)3   -1.95401    0.23259  -8.401  &lt; 2e-16 ***
as.factor(pshiv)2  -0.02638    0.06784  -0.389  0.69738    
as.factor(presp)2   0.74549    0.10532   7.079 1.46e-12 ***
as.factor(presp)3   0.66793    0.66540   1.004  0.31547    
as.factor(pmtone)2  0.54699    0.09678   5.652 1.58e-08 ***
as.factor(pmtone)3  1.82337    0.75409   2.418  0.01561 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>In my other analysis, I found that newborns:  </p>

<p>a) with higher rectal temperature<br>
b) do not shiver<br>
c) good respiration and<br>
d) good muscle tone at birth were more likely to survive.  </p>

<p>I am a bit confused with the coefficients I am getting above. I am wondering whether whether I am not interpreting the results correctly or is it something else?</p>
"
"0.086028070377267","0.106099178353461"," 11457","<p>is it possible to do stepwise (direction = both) model selection in nested binary logistic regression in R? I would also appreciate if you can teach me  how to get:</p>

<ul>
<li>Hosmer-Lemeshow statitistic,</li>
<li>Odds ratio of the predictors, </li>
<li>Prediction success of the model.</li>
</ul>

<p>I used lme4 package of R. This is the script I used to get the general model with all the independent variables:</p>

<pre><code>nest.reg &lt;- glmer(decision ~ age + education + children + (1|town), family = binomial, data = fish)
</code></pre>

<p>where:</p>

<ul>
<li>fish -- dataframe</li>
<li>decision -- 1 or 0, whether the respondent exit or stay, respectively.</li>
<li>age, education and children -- independent variables.</li>
<li>town -- random effect (where our respondents are nested)</li>
</ul>

<p>Now my problem is how to get the best model. I know how to do stepwise model selection but only for linear regression. (<code>step( lm(decision ~ age + education + children, data = fish), direction +""both"")</code>). But this could not be used for binary logistic regression right? also when i add <code>(1|town)</code> to the formula to account for the effects of town, I get an error result. </p>

<p>By the way... I'm very much thankful to Manoel Galdino <a href=""http://stackoverflow.com/questions/5906272/step-by-step-procedure-on-how-to-run-nested-logistic-regression-in-r"">who provided me with the script on how to run nested logistic regression</a>. </p>

<p>Thank you very much for your help.</p>
"
"NaN","NaN"," 12743","<p>I would like to know how to estimate a population average model of a hierarchical logistic regression using <code>R</code> package <code>geepack</code>.</p>

<p>The <code>Stata</code> code is: </p>

<pre><code>xtlogit dep ind1 ind2 ind3, i(ind4) pa
</code></pre>

<p>I would like to reproduce this in <code>R</code> using <code>geepack</code> or any other method.</p>
"
"0.126434925588327","0.129944424547263"," 15623","<p>I am completely new to R, just downloaded and installed it today. I am familiar with SAS and Stata; I am using R because I have found out that in survey regression analysis, R is capable of using data that have stratum with one PSU. However, I cannot figure out how to write the code at all.</p>

<p>Here is what I have done so far: read a Stata dataset and save the .RData file. I have also put in the MASS, pscl, and survey (for svyglm) packages.</p>

<p>Here's what I need to do:
1) I am using survey data, so I have a ""weight"" variable, a ""strata"" variable, and a ""PSU"" variable. I need to incorporate those; I know how to use svyset in Stata, but no idea in R.
2) I have stratum with singleton PSUs. I need to use an option called survey.lonely.psu I believe, and I have no idea where to even begin with that. This is the reason why I am using R instead of Stata as I do not want to collapse stratum or delete observations.
3) The types of regression models I have to run: survey negative binomial, survey zero-inflated negative binomial (need to also determine the predictors of zeros), survey logistic, and survey OLS regression.
4)I also really can't make much sense in R of how to write the model in R code. In Stata, I can simply write the model as:</p>

<p>svy: nbreg dependent_var independent_var1 independent_var2 independent_var3</p>

<p>I can't figure out how to do that at all in R.</p>

<p>Any and all help will be greatly appreciated.</p>
"
"0.0486648255489272","0.0750234484920533"," 16346","<p>(Please note the cross-post at <a href=""http://stackoverflow.com/questions/7626347/difference-between-lp-or-simply-in-rs-locfit"">http://stackoverflow.com/questions/7626347/difference-between-lp-or-simply-in-rs-locfit</a>)</p>

<p>I am not sure I see the difference between different examples for local logistic regression in the documentation of the gold standard locfit package for R: <a href=""http://cran.r-project.org/web/packages/locfit/locfit.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/locfit/locfit.pdf</a></p>

<p>I get starkingly different results with</p>

<pre><code>fit2&lt;-scb(closed_rule ~ lp(bl),deg=1,xlim=c(0,1),ev=lfgrid(100), family='binomial',alpha=cbind(0,0.3),kern=""parm"")
</code></pre>

<p>from</p>

<pre><code>fit2&lt;-scb(closed_rule ~ bl,deg=1,xlim=c(0,1),ev=lfgrid(100), family='binomial',alpha=cbind(0,0.3),kern=""parm"")
</code></pre>

<p>.</p>

<p>What is the nature of the difference? Maybe that can help me phrase which I wanted. I had in mind an index linear in bl within a logistic link function predicting the probability of closed_rule. The documentation of lp says that it fits a local polynomial -- which is great, but I thought that would happen even if I leave it out. And in any case, the documentation has examples for ""local logistic regression"" either way...</p>
"
"0.127071881385753","0.143658966607306"," 17480","<p>I've created a few Cox regression models and I would like to see how well these models perform and I thought that perhaps a ROC-curve or a c-statistic might be useful similar to this articles use:</p>

<p><a href=""http://onlinelibrary.wiley.com/doi/10.1002/bjs.6930/abstract"" rel=""nofollow"">J. N. Armitage och J. H. van der Meulen, â€Identifying coâ€morbidity in surgical patients using administrative data with the Royal College of Surgeons Charlson Scoreâ€, British Journal of Surgery, vol. 97, num. 5, ss. 772-781, Maj 2010.</a></p>

<p>Armitage used logistic regression but I wonder if it's possible to use a model from the survival package, the <a href=""http://cran.r-project.org/web/packages/survivalROC/index.html"" rel=""nofollow"">survivalROC</a> gives a hint of this being possible but I can't figure out how to get that to work with a regular Cox regression. </p>

<p>I would be grateful if someone would show me how to do a ROC-analysis on this example:</p>

<pre><code>library(survival)
data(veteran)

attach(veteran)
surv &lt;- Surv(time, status)
fit &lt;- coxph(surv ~ trt + age + prior, data=veteran)
summary(fit)
</code></pre>

<p>If possible I would appreciate both the raw c-statics output and a nice graph</p>

<p>Thanks!</p>

<h2>Update</h2>

<p>Thank you very much for answers. @Dwin: I would just like to be sure that I've understood it right before selecting your answer. </p>

<p>The calculation as I understand it according to DWin's suggestion:</p>

<pre><code>library(survival)
library(rms)
data(veteran)

fit.cph &lt;- cph(surv ~ trt + age + prior, data=veteran, x=TRUE, y=TRUE, surv=TRUE)

# Summary fails!?
#summary(fit.cph)

# Get the Dxy
v &lt;- validate(fit.cph, dxy=TRUE, B=100)
# Is this the correct value?
Dxy = v[rownames(v)==""Dxy"", colnames(v)==""index.corrected""]

# The c-statistic according to the Dxy=2(c-0.5)
Dxy/2+0.5
</code></pre>

<p>I'm unfamiliar with the validate function and bootstrapping but after looking at prof. Frank Harrel's answer <a href=""http://r.789695.n4.nabble.com/Interpreting-the-example-given-by-Prof-Frank-Harrell-in-Design-validate-cph-tt3316820.html#a3324516"" rel=""nofollow"">here on R-help</a> I figured that it's probably the way to get the Dxy. The help for validate states:</p>

<blockquote>
  <p>... Somers' Dxy rank correlation to be computed at each resample (this
  takes a bit longer than the likelihood based statistics). The values
  corresponting to the row Dxy are equal to 2 * (C - 0.5) where C is the
  C-index or concordance probability.</p>
</blockquote>

<p>I guess I'm mostly confused by the columns. I figured that the corrected value is the one I should use but I haven't really understood the validate output:</p>

<pre><code>      index.orig training    test optimism index.corrected   n
Dxy      -0.0137  -0.0715 -0.0071  -0.0644          0.0507 100
R2        0.0079   0.0278  0.0037   0.0242         -0.0162 100
Slope     1.0000   1.0000  0.2939   0.7061          0.2939 100
...
</code></pre>

<p>In the <a href=""http://r.789695.n4.nabble.com/Interpreting-the-example-given-by-Prof-Frank-Harrell-in-Design-validate-cph-tt3316820.html#a3324516"" rel=""nofollow"">R-help question</a> I've understood that I should have ""surv=TRUE"" in the cph if I have strata but I'm uncertain on what the purpose of the ""u=60"" parameter in the validate function is. I would be grateful if you could help me understand these and check that I haven't made any mistakes.</p>
"
"NaN","NaN"," 19082","<p>I have a logistic regression model with several categorical explanatory variables and one interaction term (between two binary variables, named A and B). I know how to calculate the odds ratios for the different levels of A and B (for A=1, e.g., I need to add the coeff for A and coeff for A*B, then exponentiate), but how do I get a confidence interval for this OR? I need to do this in R please, this is the only statistical package I have access to.</p>
"
"0.0729972383233908","0.0750234484920533"," 22392","<p>I am learning logistic regression modeling using the book ""Applied Logistic Regression"" by Hosmer.</p>

<p>In chpaters, he suggested using Fractional Polynomials for fitting continuous variable which does not seems to be related to logit in linear fashion. I tried the <code>mfp</code> package and can give exactly the same verbose as the book. </p>

<p>But I don't know how to write the transformed variable based on the output of fractional polynomials. The book only shows example of the transformed variable when $J=2$ with $p_1=0$ and $p_2=-0.5$ (page 101) and when $J=2$ with $p_1=2$ and $p_2=2$ (page 101), But what about the others? Currently my case is $J=2$ with $p_1=-1$ and $p_2=-1$.</p>

<p>I know little about fractional polynomials and the book seems not giving sufficient hits on this part. Can anyone refer me to some place which I can know how to write the polynomial? Thanks.</p>
"
"0.145994476646782","0.125039080820089"," 24365","<p>I am using the mlogit package in R to run a multinomial logistic regression on pooled discrete choice data collected using two different questionnaire formats. I want to test whether the format had a significant effect on choices. When I run the basic model I get a result. But when I run the same model with a dummy variable indicating which format the respondents saw, I get an error: ""Error in solve.default(H, g[!fixed]) : Lapack routine dgesv: system is exactly singular""</p>

<p>I was able to replicate the error using Train's Electricity dataset in the mlogit package, setting a dummy based on whether the respondent ID was odd or even:</p>

<pre><code>library(mlogit)
data(""Electricity"", package = ""mlogit"")
Electr &lt;- mlogit.data(Electricity, id = ""id"", choice = ""choice"", 
                      varying = 3:26, shape = ""wide"", sep = """")
Electr$odd.dummy &lt;- ifelse(Electr$id %% 2 == 0, 0, 1) # As example, set dummy if ID is odd
summary(mlogit(choice ~ pf + cl + loc + wk + tod + seas | 0, data=Electr)) # Basic model
summary(mlogit(choice ~ pf + cl + loc + wk + tod + seas + odd.dummy | 0, data=Electr)) # Basic + dummy
summary(mlogit(choice ~ odd.dummy | 0, data=Electr)) # Only dummy
</code></pre>

<p>As with my data, the first model runs, but the second two are singular.</p>

<p>I understand that a result will be singular if there is perfect colinearity between variables, but I don't see how this is the case here.  Respondents were randomly assigned to one format or the other, and the underlying experimental design was the same in both formats, so there shouldn't be any colinearity between the dummy and the other variables.</p>

<p>I would be grateful if someone could explain why adding the dummy leads to a singular result, and even more grateful if they could suggest a solution to avoid it.</p>
"
"0.139779069524328","0.143658966607306"," 24442","<p>I'm running into troubles fitting a polytomous logistic regression model using grouped data. The data are of the form (dput at bottom):</p>

<pre><code>&gt; head(alligator)
    lake  sex  size    food count
1 Hancock male small    fish     7
2 Hancock male small  invert     1
3 Hancock male small reptile     0
4 Hancock male small    bird     0
5 Hancock male small   other     5
6 Hancock male large    fish     4
</code></pre>

<p>And I've tried to fit the model with <code>vglm()</code> from package VGAM:</p>

<pre><code>&gt; result &lt;- vglm(food~lake+size+sex, data=alligator, fam=multinomial, weights=count)
Error in if (max(abs(ycounts - round(ycounts))) &gt; smallno) warning(""converting 'ycounts' to integer in @loglikelihood"") : 
  missing value where TRUE/FALSE needed
In addition: Warning messages:
1: In checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon) :
  96 elements replaced by 1.819e-12
</code></pre>

<p>It was also suggested to look at <code>mlogit()</code> from package <code>globaltest</code> (on Bioconductor), but it does not appear to support grouped data. It obviously doesn't support the <code>weights</code> parameter, but I can't find where the equivalent parameter is documented:</p>

<pre><code>source(""http://bioconductor.org/biocLite.R"")
biocLite(""globaltest"")

result &lt;- mlogit(food~lake+size+sex, weights=count, data=alligator)
Error in mlogit(food ~ lake + size + sex, weights = count, data = alligator) : 
  unused argument(s) (weights = count)
</code></pre>

<p>If anyone could put me down the right path, I'd appreciate it!</p>

<pre><code>&gt; dput(alligator)
structure(list(lake = structure(c(2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
4L, 4L, 4L, 4L, 4L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), .Label = c(""George"", ""Hancock"", 
""Oklawaha"", ""Trafford""), class = ""factor""), sex = structure(c(2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), .Label = c(""female"", 
""male""), class = ""factor""), size = structure(c(2L, 2L, 2L, 2L, 
2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 
2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 
1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 
2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 
1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L), .Label = c(""large"", 
""small""), class = ""factor""), food = structure(c(2L, 3L, 5L, 1L, 
4L, 2L, 3L, 5L, 1L, 4L, 2L, 3L, 5L, 1L, 4L, 2L, 3L, 5L, 1L, 4L, 
2L, 3L, 5L, 1L, 4L, 2L, 3L, 5L, 1L, 4L, 2L, 3L, 5L, 1L, 4L, 2L, 
3L, 5L, 1L, 4L, 2L, 3L, 5L, 1L, 4L, 2L, 3L, 5L, 1L, 4L, 2L, 3L, 
5L, 1L, 4L, 2L, 3L, 5L, 1L, 4L, 2L, 3L, 5L, 1L, 4L, 2L, 3L, 5L, 
1L, 4L, 2L, 3L, 5L, 1L, 4L, 2L, 3L, 5L, 1L, 4L), .Label = c(""bird"", 
""fish"", ""invert"", ""other"", ""reptile""), class = ""factor""), count = c(7L, 
1L, 0L, 0L, 5L, 4L, 0L, 0L, 1L, 2L, 16L, 3L, 2L, 2L, 3L, 3L, 
0L, 1L, 2L, 3L, 2L, 2L, 0L, 0L, 1L, 13L, 7L, 6L, 0L, 0L, 3L, 
9L, 1L, 0L, 2L, 0L, 1L, 0L, 1L, 0L, 3L, 7L, 1L, 0L, 1L, 8L, 6L, 
6L, 3L, 5L, 2L, 4L, 1L, 1L, 4L, 0L, 1L, 0L, 0L, 0L, 13L, 10L, 
0L, 2L, 2L, 9L, 0L, 0L, 1L, 2L, 3L, 9L, 1L, 0L, 1L, 8L, 1L, 0L, 
0L, 1L)), .Names = c(""lake"", ""sex"", ""size"", ""food"", ""count""), class = ""data.frame"", row.names = c(NA, 
-80L))
</code></pre>
"
"0.0298009977541075","0.0306281945915844"," 25282","<p>I want to make a nested logistic regression in R with the package <a href=""http://cran.r-project.org/web/packages/mlogit/index.html"" rel=""nofollow"">mlogit</a>.</p>

<p>I would like to test how producer's decision to enter organisations (14 organisations) or not is affected by different factors.</p>

<p>Producer can be in specific organisation and other according to year. I was advised to use year as separate variables in columns. This is OK. But for my different organizations will it be the same? So if my producer appear in my database several times (6 times) in row, isn't it redundant? </p>

<p>Have you an Idea of the struture of database that I can adopt?</p>

<p>Here is how my data looks like in R:</p>

<pre><code>   Year   Organisation   Member AGE.Member ...
1  2005 Organisation 1 Member 1         37
2  2005 Organisation 1 Member 2         32
3  2005 Organisation 3 Member 3         32
4  2005 Organisation 4 Member 4         35
5  2005 Organisation 2 Member 5         33
6  2005 Organisation 3 Member 6         33

'data.frame':   18 obs. of  4 variables:
 $ ANNEE       : int  2005 2005 2005 2005 2005 2005 2005 2005 2005 2006 2006 2006...
     $ Organisation: Factor w/ 4 levels ""Organisation 1"",..: 1 1 3 4 2 3 2 3 3 2 ...
 $ Member      : Factor w/ 9 levels &quot;Member 1&quot;,&quot;Member 2&quot;,..: 1 2 3 4 5 6 7 8 9 1 ...
     $ AGE.Member  : int  37 32 32 35 33 33 32 32 33 37 ...
</code></pre>
"
"0.163226787060855","0.156573695352373"," 25988","<p>An assumption of the ordinal logistic regression is the proportional odds assumption. Using R and the 2 packages mentioned I have 2 ways to check that but I have questions in each one.</p>

<p>1) Using the rms package</p>

<p>Given the next commands</p>

<pre><code>library(rms)
ddist &lt;- datadist(Ki67,Cyclin_E)
options(datadist='ddist')
f &lt;- lrm(grade ~Ki67+Cyclin_E);f
sf &lt;- function(y)
c('Y&gt;=1'=qlogis(mean(y &gt;= 1)),'Y&gt;=2'=qlogis(mean(y &gt;= 2)),'Y&gt;=3'=qlogis(mean(y &gt;= 3)))
s &lt;- summary(grade ~Ki67+Cyclin_E, fun=sf)
plot(s,which=1:3,pch=1:3,xlab='logit',main='',xlim=c(-2.5,2.5))
</code></pre>

<p>I have</p>

<pre><code>lrm(formula = grade ~ Ki67 + Cyclin_E)

Frequencies of Missing Values Due to Each Variable
   grade     Ki67 Cyclin_E 
       0        0        3 


                     Model Likelihood     Discrimination    Rank Discrim.    
                        Ratio Test            Indexes          Indexes       

Obs            42    LR chi2     11.38    R2       0.268    C       0.728    
 1             11    d.f.            2    g        1.279    Dxy     0.456    
 2             15    Pr(&gt; chi2) 0.0034    gr       3.592    gamma   0.458    
 3             16                         gp       0.192    tau-a   0.308    
max |deriv| 1e-07                         Brier    0.166                     


         Coef    S.E.   Wald Z Pr(&gt;|Z|)
y&gt;=2     -0.1895 0.8427 -0.22  0.8221  
y&gt;=3     -2.0690 0.9109 -2.27  0.0231  
Ki67      0.0971 0.0330  2.94  0.0033  
Cyclin_E -0.0076 0.0227 -0.33  0.7387 
</code></pre>

<p>The <code>s</code> table gives: (unfortunately I don't know how to upload a graph made in R)</p>

<pre><code>grade    N=45

+--------+-------+--+----+---------+----------+
|        |       |N |Y&gt;=1|Y&gt;=2     |Y&gt;=3      |
+--------+-------+--+----+---------+----------+
|Ki67    |[ 2, 9)|12|Inf |0.6931472|-1.0986123|
|        |[ 9,16)|12|Inf |0.3364722|-2.3978953|
|        |[16,24)|10|Inf |2.1972246| 0.0000000|
|        |[24,44]|11|Inf |2.3025851| 1.5040774|
+--------+-------+--+----+---------+----------+
|Cyclin_E|[ 3,16)|15|Inf |1.0116009|-0.1335314|
|        |[16,22)| 7|Inf |1.7917595|-0.9162907|
|        |[22,33)|10|Inf |1.3862944|-0.8472979|
|        |[33,80]|10|Inf |0.4054651|-0.4054651|
|        |Missing| 3|Inf |      Inf| 0.6931472|
+--------+-------+--+----+---------+----------+
|Overall |       |45|Inf |1.1284653|-0.4054651|
+--------+-------+--+----+---------+----------+
</code></pre>

<p>Where for the Ki67 I see that 3 out of the 4 differences  <code>logit(P[Y&gt; = 2])-logit(P[Y&gt; = 3])</code> are close to 2. Only the last one is quite lower (around 0.8). But here Ki67 is continuous and not categorical so I don't know if the results of the table are correct and there isn't any p-value to decide. By the way I run the above in SPSS and I didn't reject the assumption.</p>

<p>2) Using the VGAM package</p>

<p>Here using the next commands I have the model under the assumption of proportional odds</p>

<pre><code>library(VGAM)
fit1 &lt;- vglm(grade ~Ki67+Cyclin_E,family=cumulative(parallel=T))
summary(fit1)
</code></pre>

<p>And the results</p>

<pre><code>Coefficients:
                Estimate Std. Error  z value
(Intercept):1  0.1894723   0.820442  0.23094
(Intercept):2  2.0690395   0.886732  2.33333
Ki67          -0.0970972   0.032423 -2.99467
Cyclin_E       0.0075887   0.021521  0.35261

Number of linear predictors:  2 

Names of linear predictors: logit(P[Y&lt; = 1]), logit(P[Y&lt; = 2])

Dispersion Parameter for cumulative family:   1

Residual deviance: 79.86801 on 80 degrees of freedom

Log-likelihood: -39.93401 on 80 degrees of freedom

Number of iterations: 5 
</code></pre>

<p>While using the next commands I have the model without the assumption of proportional odds</p>

<pre><code>fit2 &lt;- vglm(grade ~Ki67+Cyclin_E,family=cumulative(parallel=F))
</code></pre>

<p>where unfortunately i receice the next message </p>

<blockquote>
  <p>Warning message: In vglm.fitter(x = x, y = y, w = w, offset = offset,
  Xm2 = Xm2,  :   convergence not obtained in 30 iterations</p>
</blockquote>

<p>However if I type <code>summary(fit2)</code> I get results but again I don't know if they are correct. My intention was to use the next commands and get the answer but know I doubt if this is correct (by the way if I do it I get <code>p-value=0.6</code>. </p>

<pre><code>pchisq(deviance(fit1)-deviance(fit2),
df=df.residual(fit1)-df.residual(fit2),lower.tail=FALSE)
</code></pre>

<p>So, regarding the methods mentioned above does anyone knows whether the results I get are valid or in the case of the VGAM package is there any way to increase the number of itterations?Is there any other way to check it? </p>
"
"0.059601995508215","0.0612563891831689"," 26178","<p>I want to create a classification table regarding an ordinal response variable with three levels but I don't know how to do it. Searching on the site I fell on the question posted by Brandon Bertelsen that covers only the case of the binary logistic regression (link at the end of the post).Does anyone knows how I can create such that table in my case?</p>

<p>I don't know if it is important but I used the <code>rms</code> package to run the olr and using the <code>predict(fit,type=""fitted.ind"")</code> command I get the next table with probability for each case</p>

<pre><code>      grade=1    grade=2   grade=3
1  0.08042197 0.28380601 0.6357720
2  0.08086877 0.28475584 0.6343754
3  0.41472656 0.40802584 0.1772476
4  0.39680650 0.41484517 0.1883483
5  0.25402385 0.43644283 0.3095333
6  0.13539881 0.37098177 0.4936194
7  0.12591996 0.35959459 0.5144855
8  0.50489952 0.36489760 0.1302029
9          NA         NA        NA
10 0.34757283 0.42969971 0.2227275
11 0.24690054 0.43539812 0.3177013
12 0.17325212 0.40529586 0.4214520
13 0.45795712 0.38900855 0.1530343
14 0.03594015 0.16033637 0.8037235
15         NA         NA        NA
16 0.50188652 0.36653955 0.1315739
17 0.48710163 0.37441720 0.1384812
18 0.38094725 0.42028884 0.1987639
19 0.04134659 0.17894428 0.7797091
20 0.12844729 0.36275605 0.5087967
21 0.23991274 0.43410413 0.3259831
22 0.20506362 0.42316514 0.3717712
23 0.45457929 0.39061326 0.1548075
24         NA         NA        NA
25 0.31269786 0.43606610 0.2512360
26 0.20905830 0.42483513 0.3661066
27 0.05240710 0.21353381 0.7340591
28 0.26569967 0.43759072 0.2967096
29 0.21258621 0.42621415 0.3611996
30 0.11407246 0.34347156 0.5424560
31 0.34656138 0.42993750 0.2235011
32 0.01813256 0.08978609 0.8920813
33 0.44034224 0.39716470 0.1624931
34 0.12213714 0.35468488 0.5231780
35 0.40888783 0.41032190 0.1807903
36 0.33901842 0.43161582 0.2293658
37 0.13275554 0.36793345 0.4993110
38 0.32091057 0.43492411 0.2441653
39 0.45984161 0.38810515 0.1520532
40 0.55550665 0.33564053 0.1088528
41 0.02812293 0.13122652 0.8406505
42 0.46250424 0.38681892 0.1506768
43 0.07352751 0.26852580 0.6579467
44 0.04330967 0.18541327 0.7712771
45 0.45457929 0.39061326 0.1548075
</code></pre>

<p><a href=""http://stats.stackexchange.com/questions/4832/logistic-regression-classification-tables-a-la-spss-in-r"">Logistic Regression: Classification Tables a la SPSS in R</a></p>
"
"0.11150512337989","0.114600210537152"," 26568","<p>I would like to understand how to generate <em>prediction intervals</em> for logistic regression estimates. </p>

<p>I was advised to follow the procedures in Collett's <em>Modelling Binary Data</em>, 2nd Ed p.98-99. After implementing this procedure and comparing it to R's <code>predict.glm</code>, I actually think this book is showing the procedure for computing <em>confidence intervals</em>, not prediction intervals.</p>

<p>Implementation of the procedure from Collett, with a comparison to <code>predict.glm</code>, is shown below.</p>

<p>I would like to know: how do I go from here to producing a prediction interval instead of a confidence interval?</p>

<pre><code>#Derived from Collett 'Modelling Binary Data' 2nd Edition p.98-99
#Need reproducible ""random"" numbers.
seed &lt;- 67

num.students &lt;- 1000
which.student &lt;- 1

#Generate data frame with made-up data from students:
set.seed(seed) #reset seed
v1 &lt;- rbinom(num.students,1,0.7)
v2 &lt;- rnorm(length(v1),0.7,0.3)
v3 &lt;- rpois(length(v1),1)

#Create df representing students
students &lt;- data.frame(
    intercept = rep(1,length(v1)),
    outcome = v1,
    score1 = v2,
    score2 = v3
)
print(head(students))

predict.and.append &lt;- function(input){
    #Create a vanilla logistic model as a function of score1 and score2
    data.model &lt;- glm(outcome ~ score1 + score2, data=input, family=binomial)

    #Calculate predictions and SE.fit with the R package's internal method
    # These are in logits.
    predictions &lt;- as.data.frame(predict(data.model, se.fit=TRUE, type='link'))

    predictions$actual &lt;- input$outcome
    predictions$lower &lt;- plogis(predictions$fit - 1.96 * predictions$se.fit)
    predictions$prediction &lt;- plogis(predictions$fit)
    predictions$upper &lt;- plogis(predictions$fit + 1.96 * predictions$se.fit)


    return (list(data.model, predictions))
}

output &lt;- predict.and.append(students)

data.model &lt;- output[[1]]

#summary(data.model)

#Export vcov matrix 
model.vcov &lt;- vcov(data.model)

# Now our goal is to reproduce 'predictions' and the se.fit manually using the vcov matrix
this.student.predictors &lt;- as.matrix(students[which.student,c(1,3,4)])

#Prediction:
this.student.prediction &lt;- sum(this.student.predictors * coef(data.model))
square.student &lt;- t(this.student.predictors) %*% this.student.predictors
se.student &lt;- sqrt(sum(model.vcov * square.student))

manual.prediction &lt;- data.frame(lower = plogis(this.student.prediction - 1.96*se.student), 
    prediction = plogis(this.student.prediction), 
    upper = plogis(this.student.prediction + 1.96*se.student))

print(""Data preview:"")
print(head(students))
print(paste(""Point estimate of the outcome probability for student"", which.student,""(2.5%, point prediction, 97.5%) by Collett's procedure:""))
manual.prediction
print(paste(""Point estimate of the outcome probability for student"", which.student,""(2.5%, point prediction, 97.5%) by R's predict.glm:""))    
print(output[[2]][which.student,c('lower','prediction','upper')])
</code></pre>
"
"0.0942390294485422","0.0968548555282575"," 27273","<p>I am trying to analyze repeated measures data and am struggling to make it work in <code>R</code>. My data is essentially the following, I have two treatment groups. Every subject in each group is tested everyday and given a score (the percentage correct on a test). The data is in the long format:</p>

<pre><code>Time Percent Subject   Group
   1       0    GK11 Ethanol
   2       0    GK11 Ethanol
   3       0    GK11 Ethanol
   4       0    GK11 Ethanol
   5       0    GK11 Ethanol
   6       0    GK11 Ethanol
</code></pre>

<p>The data resembles a logistic curve, subjects do very poorly for a few days followed by rapid improvement, followed by a plateau. I'd like to know if the treatment has an effect on the test performance curve. My thought was to use <code>nlmer()</code> in the <code>lme4</code> package in <code>R</code>. I can fit lines for each group using the following:</p>

<pre><code>print(nm1 &lt;- nlmer(Percent ~ SSlogis(Time,Asym, xmid, scal) ~ Asym | Subject,
salinedata, start = c(Asym =.60,  xmid = 23, scal = 5)), corr = FALSE)
</code></pre>

<p>I can than compare groups by looking at the estimates for the different parameters and standard deviations of the estimated lines but I'm not sure this is the proper way to do it. Any help would be greatly appreciated.  </p>
"
"NaN","NaN"," 27297","<p>I'm using the <code>logistf</code> package in R to perform Firth logistic regression on an unbalanced dataset. I have a logistf object:</p>

<pre><code>fit = logistf(a~b)
</code></pre>

<p>Is there a <code>predict()</code> function like on that's used in the <code>lm</code> class to predict probabilities for future data points? Or do I have to manually input the estimated parameters from the Firth regression.</p>
"
"0.042144975196109","0.043314808182421"," 29044","<p>R and Statistics newbie here.</p>

<p>Ok, I have a logistic regression and have used the predict function to develop a probability curve based on my estimates. </p>

<pre><code>## LOGIT MODEL:
library(car)
mod1 = glm(factor(won) ~ as.numeric(bid), data=mydat, family=binomial(link=""logit""))

## PROBABILITY CURVE:
all.x &lt;- expand.grid(won=unique(won), bid=unique(bid))
y.hat.new &lt;- predict(mod1, newdata=all.x, type=""response"")
plot(bid&lt;-000:1000,predict(mod1,newdata=data.frame(bid&lt;-c(000:1000)),type=""response""), lwd=5, col=""blue"", type=""l"")
</code></pre>

<p>This is great but I'm curious about plotting the confidence intervals for the probabilities. I've tried plot.ci() but had no luck. Can anyone point me to some ways to get this done, preferably with the car package or base R.</p>

<p>Thanks.</p>
"
"0.126434925588327","0.129944424547263"," 29653","<p>The likelihood ratio (a.k.a. deviance) $G^2$ statistic and lack-of-fit (or goodness-of-fit) test is fairly straightforward to obtain for a logistic regression model (fit using the <code>glm(..., family = binomial)</code> function) in R. However, it can be easy to have some cell counts end up low enough that the test is unreliable. One way to verify the reliability of the likelihood ratio test for lack of fit is to compare its test statistic and <em>P</em>-value to those of Pearson's chi square (or $\chi^2$) lack-of-fit test.</p>

<p>Neither the <code>glm</code> object nor its <code>summary()</code> method report the test statistic for Pearson's chi square test for lack of fit. In my search, the only thing I came up with is the <code>chisq.test()</code> function (in the <code>stats</code> package): its documentation says ""<code>chisq.test</code> performs chi-squared contingency table tests and goodness-of-fit tests."" However, the documentation is sparse on how to perform such tests:</p>

<blockquote>
  <p>If <code>x</code> is a matrix with one row or column, or if <code>x</code> is a vector and <code>y</code> is not given, then a <em>goodness-of-fit</em> test is performed (<code>x</code> is treated as a one-dimensional contingency table). The entries of <code>x</code> must be non-negative integers. In this case, the hypothesis tested is whether the population probabilities equal those in <code>p</code>, or are all equal if <code>p</code> is not given.</p>
</blockquote>

<p>I'd imagine that you could use the <code>y</code> component of the <code>glm</code> object for the <code>x</code> argument of <code>chisq.test</code>. However, you can't use the <code>fitted.values</code> component of the <code>glm</code> object for the <code>p</code> argument of <code>chisq.test</code>, because you'll get an error: ""<code>probabilities must sum to 1.</code>""</p>

<p>How can I (in R) at least calculate the Pearson $\chi^2$ test statistic for lack of fit without having to run through the steps manually?</p>
"
"NaN","NaN"," 31299","<p>I have a dependent variable made up of 3 categories and 14 binary predictor variables.</p>

<p>I have tried using <code>mlogit</code> and <code>nnet/multinom</code> packages in R. </p>

<p><strong>Is there a better approach than multinomial logistic regression for this particular scenario?</strong></p>
"
"NaN","NaN"," 32239","<p>As described in Merlo et al (<a href=""http://www.ncbi.nlm.nih.gov/pubmed/16537344"" rel=""nofollow"">J Epidem Comm Health 2006</a>), the 95% credible interval for MOR is calculated using MCMC. MOR is defined as $\exp(\sqrt{2\sigma^2}\times 0.675)$, where $\sigma$ is the level-2 variance of the random intercept $u$ from a null model of a hierarchical logistic regression.  </p>

<p>Does anyone have an idea of how to write a program for an Markov chain Monte Carlo to calculate the standard error of the  median odds ratio (MOR) using <a href=""http://cran.r-project.org/web/packages/rjags/index.html"" rel=""nofollow"">rjags</a>?<br>
My dependent variable is outcome(alive/dead) and the clustering (level2)variable is Hospital. There are 140 hospitals and would like to see variations in outcome between hospitals. Other risk factors will be included later as independent level1 variables.</p>
"
"0.042144975196109","0.043314808182421"," 34843","<p>I have a logistic model that I've built with the <code>nls</code> function in R. I want to use Bayesian model averaging for variable selection, but I can't find a package for that in R. Are there any suitable packages? If not, is it possible to make a not too complicated script for it? </p>

<p>Data example:</p>

<pre><code>y&lt;-sample(c(1,0),100,replace=T)

var1&lt;-sample(c(1,0),100,replace=T)

var2&lt;-sample(c(1,0),100,replace=T)

var3&lt;-sample(c(1,0),100,replace=T)
</code></pre>

<p>The model:</p>

<pre><code>Sw&lt;- function(y1, N1,N2,N3) {

   SA &lt;- nls(y1~exp(c+(a1*N1)+(a2*N2)+(a3*N3))/(1+exp(c+(a1*N1)+(a2*N2)+(a3*N3)))
    ,start=list(a1=-0.2,a2=-0.2,a3=-0.2,c=0.2))
   SA   
}



model &lt;- Sw(y, var1,var2,var3)
</code></pre>

<p>How would I do Bayesian model averaging on this? I have 190 observations, where about 70 are <code>1</code>s and 120 are <code>0</code>s. I have 13 variables in total.</p>
"
"NaN","NaN"," 35071","<p>Fitting a logistic regression using <a href=""http://cran.r-project.org/web/packages/lme4/index.html"">lme4</a> ends with </p>

<pre><code>Error in mer_finalize(ans) : Downdated X'X is not positive definite. 
</code></pre>

<p>A likely cause of this error is apparently rank deficiency. What is rank deficiency, and how should I address it?</p>
"
"0.11150512337989","0.114600210537152"," 37411","<p>Suppose I'm building a logistic regression classifier that predicts whether someone is married or single. (1 = married, 0 = single) I want to choose a point on the precision-recall curve that gives me at least 75% precision, so I want to choose thresholds $t_1$ and $t_2$, so that:</p>

<ul>
<li>If the output of my classifier is greater than $t_1$, I output ""married"".</li>
<li>If the output is below $t_2$, I output ""single"".</li>
<li>If the output is in between, I output ""I don't know"".</li>
</ul>

<p>A couple questions:</p>

<ol>
<li>I think under the standard definition of precision, precision will be measuring the precision of the married class alone (i.e., precision = # times I correctly predict married / total # times I predict married). However, what I really want to do is measure the overall precision (i.e., the total # times I correctly predict married or single / total # times I predict married or single). Is this an okay thing to do? If not, what should I be doing?</li>
<li>Is there a way to calculate this ""overall"" precision/recall curve in R (e.g., using the ROCR package or some other library)? I'm currently using the ROCR package, but it seems to only give me the single-class-at-a-time precision/recall.</li>
</ol>
"
"NaN","NaN"," 37830","<p>I've been experimenting with the <code>rfe</code> function in the <code>caret</code> package to do logistic regression with feature selection. I used the <code>lmFuncs</code> functions with the following <code>rfeContol</code> :</p>

<p><code>ctrl &lt;- rfeControl(functions = lmFuncs,
                     method = 'cv',
                     rerank=TRUE,
                     saveDetails=TRUE,
                     verbose = TRUE,
                     returnResamp = ""all"",
                     number=100)</code></p>

<p>Below is the structure of the <code>rfe</code> call:</p>

<p><code>fit.rfe=rfe(df.preds,df.depend, metric='RMSE',sizes=c(5,10,15,20), rfeControl=ctrl)</code></p>

<p><code>df.preds</code> is a data frame of inputs to the model. <code>df.depend</code> is a vector of 1 or 0 corresponding to each row in <code>df.preds</code> to indicate response.</p>

<p>The resulting model accessed in from the <code>fit</code> object in the <code>rfe</code> object is of class <code>lm</code> and produces predicted values of less than zero and greater than 1 when I use the following code with the <code>predict</code> function:</p>

<p><code>predict(fit.rfe$fit,df,type='response')</code></p>

<p>Given I'm expecting this to be a logistic, all predicted values should greater than zero and less than one. </p>

<p>Any help will be appreciated.</p>
"
"0.10323368445272","0.0884159819612176"," 38500","<p>I am currently working with a logistic semi-parametric model in R using the mgcv package.  The output from the model gives the standard log-odds coefficients; however, reviewers have requested marginal effects (like the ones in Stata using the margins command).  I would like to do average marginal effects (though, marginal effects at the means--modes for categorical covariates--would at least be a start).</p>

<p>I was wondering if anyone has an implementation for this in R for models that use the gam() function.  I have a rather large data set (1.2 million observations, a large number of discrete and continuous covariates, and fixed effects for 2,000 individuals).  Are these estimates even tractable, given the non-parametric treatment of a couple of the continuous covariates?  Any information would be helpful.</p>

<p>Here is what I am working with, though I get errors when attempting to do the bootstrapped SEs for the effects (comes from this helpful site probitlogit-marginal-effects-in-r-2/).  I am not sure how this treats the non-parametrically estimated smooths (but maybe these don't matter, since it is using the ""predict"" function?):</p>

<pre><code>mfxboot &lt;- function(modform,dist,data,boot=1000,digits=3){ #dist is the distribution choice of logit or probit
      require(mgcv)

x &lt;- gam(modform, family=binomial(link=dist),method=""GCV.Cp"",data)
      # get marginal effects

pdf &lt;- ifelse(dist==""probit"",               
 mean(dnorm(predict(x, type = ""link"")))            
 mean(dlogis(predict(x, type = ""link"")))
 marginal.effects &lt;- pdf*coef(x)


bootvals &lt;- matrix(rep(NA,boot*length(coef(x))), nrow=boot)  
set.seed(1111)  
for(i in 1:boot){    
samp1 &lt;- data[sample(1:dim(data)[1],replace=T,dim(data)[1]),]    
x1 &lt;- gam(modform, family=binomial(link=dist),method=""GCV.Cp"",samp1)    
pdf1 &lt;- ifelse(dist==""probit"",                   
mean(dnorm(predict(x1, type = ""link""))),                   
mean(dlogis(predict(x1, type = ""link""))))       
bootvals[i,] &lt;- pdf1*coef(x1)     
}

res &lt;- cbind(marginal.effects,apply(bootvals,2,sd),marginal.effects/apply(bootvals,2,sd))     
if(names(x$coefficients[1])==""(Intercept)""){        
res1 &lt;- res[2:nrow(res),]    
res2 &lt;- matrix(as.numeric(sprintf(paste(""%."",paste(digits,""f"",sep=""""),sep=""""),res1)),nrow=dim(res1)[1])         
rownames(res2) &lt;- rownames(res1)        
} else {    
res2 &lt;- matrix(as.numeric(sprintf(paste(""%."",paste(digits,""f"",sep=""""),sep="""")),nrow=dim(res)[1]))       
rownames(res2) &lt;- rownames(res)    
}     
colnames(res2) &lt;- c(""marginal.effect"",""standard.error"",""z.ratio"") 
      return(res2)
}
</code></pre>
"
"0.042144975196109","0.043314808182421"," 38541","<p>I used the functions from this <a href=""http://www.math.mcmaster.ca/peter/s4f03/s4f03_0607/rochl.html"" rel=""nofollow"">link</a> for creating ROC curve for logistic regression model. Since the object produced by <code>glmer</code> in <code>lme4</code> package is a S4 object (as far as I know) and the function from the link cannot handle it.</p>

<p>I wonder if there are similar functions for creating ROC curve for multi-level logistic regression model in R.</p>
"
"0.151955869072729","0.156173761888606"," 40499","<p>When using the <code>step.plr()</code> function in the <a href=""http://cran.r-project.org/web/packages/stepPlr/index.html"" rel=""nofollow"">stepPlr</a> package, if my predictors are factors, do I need to encode my predictors as dummy variables manually before passing it to the function? I do know that I can specify ""level"", but how  the ""level"" parameter works is confusing to me. 
My understanding is that I need to tell <code>step.plr()</code> explicitly which factors should be encoded as dummy variables and thus leaving one factor out intentionally. </p>

<p>Let's consider a simple example. Suppose I have 1 categorical predicator with 4 levels and binary response. Normally, if I use <code>glm()</code> to fit a logistic regression model, <code>glm()</code> would automatically convert the categorical predicator into 3 dummy variables. Now in <code>stepPlr()</code>, do I specify the ""level"" parameter for that predictor with 4 levels or 3 levels? The ""Help"" section is vague, and says: </p>

<blockquote>
  <p>If the j-th column of x is discrete, level[[ j ]] is the set of levels for the categorical factor.</p>
</blockquote>

<p>Does it mean I should tell <code>step.plr()</code> about all 4 levels, or I should make an intelligent decision myself and tell <code>step.plr()</code> to use only 3 levels? </p>

<p>==============UPDATE (16 Oct 2012)=============</p>

<p>The following example will demonstrate what is the problem with <code>step.plr()</code>'s automatic dummy variable encoding. It is a slight modification of the code in the function's help section. 
     set.seed(100)</p>

<pre><code>n &lt;- 100
p &lt;- 3
z &lt;- matrix(sample(seq(3),n*p,replace=TRUE),nrow=n)
x &lt;- data.frame(x1=factor(z[ ,1]),x2=factor(z[ ,2]),
                x3=factor(sample(seq(3), n, replace=TRUE, prob=c(0.2, 0.5, 0.3))),
                x4=factor(sample(seq(3), n, replace=TRUE, prob=c(0.1, 0.3, 0.6))))
y &lt;- sample(c(0,1),n,replace=TRUE)
fit &lt;- step.plr(x,y, cp=""aic"")
summary(fit)
</code></pre>

<p>And here's an excerpt of the result:</p>

<pre><code>Call:
plr(x = ix0, y = y, weights = weights, offset.subset = offset.subset, 
    offset.coefficients = offset.coefficients, lambda = lambda, 
    cp = cp)

Coefficients:
      Estimate Std.Error z value Pr(&gt;|z|)
Intercept  0.91386   5.04780   0.181    0.856
x4.1       1.33787   4.61089   0.290    0.772
x4.2      -1.70462   4.91240  -0.347    0.729
x4.3       0.36675   3.18857   0.115    0.908
x3.1:x4.1  7.04901  14.35112   0.491    0.623
x3.1:x4.2 -5.50973  15.53674  -0.355    0.723
x3.1:x4.3 -0.50012   7.95651  -0.063    0.950
</code></pre>

<p>You can see that all levels, that is, (1,2,3), are used to fit the model. But normally you only need two dummy variables to encode a predictor with 3 levels.
On the other hand, if you use <code>glm()</code>: </p>

<pre><code>glm(y~.^2, data=x, family=binomial)
</code></pre>

<p>you will get the correct dummy variable encoding.</p>
"
"0.0729972383233908","0.0500156323280355"," 40739","<p>The <code>bild</code> package appears to be an excellent package for serial binary responses.  But it is for discrete time.  I would like to specify a smooth function of time for the odds ratio connection of the current response Y with binary responses measured at earlier times, or at least a first-order Markov version of this.  I believe this is called alternating logistic regression.  Does anyone know of an R package that handles continuous time, i.e., measurement times can be at any follow-up time?  I don't need random effects in the model. </p>
"
"0.042144975196109","0"," 43551","<p>I am running a multinomial logistic regression using the <code>mlogit</code> package and <code>mlogit</code> function in R.  Now I need to check for outliers for the model.</p>

<p>Is there any approach or function in R for testing outliers in an <code>mlogit</code> model?</p>
"
"0.11150512337989","0.0982287518889871"," 45449","<p>I have a large set of predictors (more than 43,000) for predicting a dependent variable which can take 2 values (0 or 1). The number of observations is more than 45,000. Most of the predictors are unigrams, bigrams and trigrams of words, so there is high degree of collinearity among them. There is a lot of sparsity in my dataset as well. I am using the logistic regression from the glmnet package, which works for the kind of dataset I have. My problem is how can I report p-value significance of the predictors. I do get the beta coefficient, but is there a way to claim that the beta coefficients are statistically significant?</p>

<p>Here is my code:</p>

<pre><code>library('glmnet')
data &lt;- read.csv('datafile.csv', header=T)
mat = as.matrix(data)
X = mat[,1:ncol(mat)-1] 
y = mat[,ncol(mat)]
fit &lt;- cv.glmnet(X,y, family=""binomial"")
</code></pre>

<p>Another question is:
I am using the default alpha=1, lasso penalty which causes the additional problem that if two predictors are collinear the lasso will pick one of them at random and assign zero beta weight to the other. I also tried with ridge penalty (alpha=0) which assigns similar coefficients to highly correlated variables rather than selecting one of them. However, the model with lasso penalty gives me a much lower deviance than the one with ridge penalty. Is there any other way that I can report both predictors which are highly collinear?</p>
"
"0.059601995508215","0.0612563891831689"," 45723","<p>I am a student in biology, currently finishing my master in Behavior, evolution and conservation. I have been in the Swiss national park for my field work and I have some trouble to analyze my data.</p>

<p>So basically what I have is a lot of factors and what I need to do is a multinomial logistic regression :</p>

<p>Factors :</p>

<ul>
<li>Behaviour (4 states - Moving, Feeding, Resting, Runing)</li>
<li>Age (Of the individual)</li>
<li>Temperature</li>
<li>Valley (where the chamois was, 2 different choices)</li>
<li>Year (of the observation)</li>
<li>Month (of the observation)</li>
<li>Kid (if it has a kid or not)</li>
<li>Individual (The number written on the tag he had on his ear)</li>
</ul>

<p>What I want to do is to check what factors influence the Behavior.</p>

<p>I tried several things with R but can't use the <code>mlogit</code> package which is the one I have been told to use.</p>

<p>I have also tried in JMP, now the problem I have here is that I just clicked on ""Fit Y in function of X"" and selected my response variable and my factors and bam, I had results, but to be honest this seems very simple and I was wondering if I am not missing something somewhere.</p>

<p>Edit : Here is what R returns when I try the <code>mlogit</code> function :</p>

<pre><code>mlogit(Behavior~Age+Temp+Valley+Individual+Year+Month, Merge)
Error in `row.names&lt;-.data.frame`(`*tmp*`, value = value) : 
  invalid 'row.names' length
</code></pre>

<p>Can anyone here that can help me either with R or with JMP?</p>
"
"0.178805986524645","0.183769167549507"," 46322","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/28936/how-to-compute-significant-interaction-estimates-when-main-effect-is-not-signifi"">How to compute significant interaction estimates when main effect is not significant?</a>  </p>
</blockquote>



<p>I am an applied linguist and I am modelling responses to a vocabulary test taken by second language learners of English; the aim is to test theoretical hypotheses regarding the relationship between the nature of the word and the likelihood of the learnersâ€™ knowing that word. The models I am using to explore the role of explanatory variables are the random item Item Response Theory model LLTM+e (see de Boeck et al. 2011; de Boeck 2008). These are created using the lmer function in the LME4 package in R and treat item and person responses as random. The estimates shown below are effectively like those from a binary logistic regression model, indicating the log-odds of a correct response on a word, given certain properties. Covariates relate to both item and person characteristics. </p>

<p>The nature of my concern is actually a basic regression issue. I have found a significant interaction between ability grouping of the test taker (GRP; with two levels High and Low) and the length of the word in letters (LEN_L). As far as I can see the estimate of the fixed effects for the first model shows that (a) the lower level learners have an overall lower probability of giving a correct answer (b) that LEN_L does not provide a significant explanation across the pattern of responses for the whole test-taker population, and (c) a significant interaction between GRP and LEN_L indicates that the lower ability learners are less likely to give a correct answer for a longer word. This is in keeping with theory. </p>

<p>However, when I model the data without including the main effect for LEN_L, I am not seeing a significant effect for either high or low groups as shown in Model 2. LEN_L does not show as significant if modelled without interaction with grp low (not shown). I feel that I am missing something obvious, but I cannot quite grasp what is happening. And my references have run dry on this particular issue and I am thinking myself around in circles about it.</p>

<p>(NB: in my full model I have many other significant covariates, but this pattern holds true.) 
My query basically regards whether I should use Model 1, including the non-significant main effect, or whether my findings from Model 2 indicate that the finding is a little unstable. Any advice would be much appreciated! (I can post more details if necessary.)
Karen</p>

<p>Model 1 Fixed effects:</p>

<pre><code>              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)    0.25244    0.83194   0.303  0.76156    
grp low       -2.09126    0.26406  -7.920 2.38e-15 ***
LEN_L          0.02093    0.13062   0.160  0.87272    
grp low:LEN_L -0.09185    0.03146  -2.919  0.00351 **
</code></pre>

<p>Model 2 Fixed effects:</p>

<pre><code>               Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)     0.25243    0.83194   0.303    0.762    
grp low        -2.09126    0.26406  -7.920 2.38e-15 ***
grp high:LEN_L  0.02093    0.13062   0.160    0.873    
grp low:LEN_L  -0.07092    0.13202  -0.537    0.591  
</code></pre>

<blockquote>
  <p>De Boeck, P., Bakker, M., Zwitser, R., Nivard, M., Hofman, A.,
  Tuerlinckx, F. and Partchev, I. (2011) <a href=""http://www.jstatsoft.org/v39/i12/"" rel=""nofollow"">The Estimation of Item
  Response Models with the 'lmer' Function from the lme4 Package in
  R</a>. Journal of Statistical Software (39:12) pp 1-28</p>
</blockquote>
"
"NaN","NaN"," 46661","<p>I'm using the R package <a href=""http://cran.r-project.org/web/packages/ltm/index.html"" rel=""nofollow"">ltm</a> to create a 2-parameter logistic regression.</p>

<p>The input matrix is sparse - many users have taken a small subset of the items in the item bank.</p>

<p>For some of my data sets i'm running into this error:</p>

<pre><code>Error in if (any(ind &lt;- pr == 0)) pr[ind] &lt;- sqrt(.Machine$double.eps) : 
  missing value where TRUE/FALSE needed
</code></pre>

<p>Not sure what the issue is.  Doesn't repro on most of my data sets.  Any thoughts?</p>
"
"0.163546526421265","0.17859152928949"," 48381","<p>I'm trying to estimate power in a logistic regression with a continuous exposure in a cohort study (ie, the ratio of the sampling probabilities is 1). I have population cumulative incidence (probability) and population exposure variability and exposure mean and an expected odds ratio. I also have a total sample size.</p>

<p>I'm using R and it seems like <code>Hmisc::bpower</code> is only for logistic regression with binary exposure and I can't seem to find any packages that estimate binomial power with continuous exposure.</p>

<p>I've attempted the following simulation but it's quite slow given my total sample size and I'm not sure if it's right:</p>

<pre><code>p &lt;- vector()
betahat &lt;- vector()
for(i in 1:1000){
n &lt;- 40000  #total sample size
intercept = log(0.008662265)  #where exp(intercept) = P(D=1)
beta &lt;- log(1.4) #where exp(beta)=OR corresponding to a one unit change in xtest
xtest &lt;- rnorm(n,1.2,.31)  #xtest is vector length 40,000 with mean 1.2 and sd .31
linpred &lt;- intercept + xtest*beta #linear predictor
prob &lt;- exp(linpred)/(1 + exp(linpred)) #link function
runis &lt;- runif(n,0,1) #generate a vector length n from a uniform distribution 0,1
ytest &lt;- ifelse(runis &lt; prob,1,0)  #if a random value from a uniform distribution 0,1 is less than prob, then the outcome is 1.  otherwise the outcome is 0
coefs &lt;- coef(summary(glm(ytest~xtest, family=""binomial"")))  #run a logistic regression
p[i] &lt;- coefs[2,4] #store the p value
betahat[i] &lt;- coefs[2,1] #store the unexponentiated betahat
}

mean(p &lt; .05)
#power

exp(mean(betahat))
#sanity check, should equal 1.4--it does
</code></pre>

<p>Is there anything wrong with this approach?</p>

<p>One concern of mine is that the cumulative incidence (ie, probability of event over the given time period) comes from a population that did not have 0 exposure.  In fact, it's reasonable to assume that the value i'm using for an intercept is actually from a population that has an exposure variability similar to mine. In that case, how would I estimate the unexposed probability given an odds ratio (and other information that I would find in say, a published paper) to use in my power calculation?</p>
"
"0.17905415595223","0.174338739950863"," 48415","<p>I come to you today because I face a huge problem that I cannot explain.</p>

<p>I have run a multinomial logistic regression (using the mlogit package) on behavioral data. I prepare the data by doing</p>

<pre><code>    mlogit &lt;- mlogit.data(Merge, choice = ""Choice"", shape = ""long"", alt.var = ""Comp"", 
                          drop.index = TRUE)
</code></pre>

<p>on my Merge data.</p>

<p>which gives me the following:</p>

<pre><code>                Date     Time ActivityX ActivityY Temp Behavior Valley Age Month Year kid Individual Choice
    1.F   01/05/2012 00:00:00        80        58   10        F  Fuorn   8     5 2012   Y         26   TRUE
    1.R   01/05/2012 00:00:00        80        58   10        F  Fuorn   8     5 2012   Y         26  FALSE
    1.M   01/05/2012 00:00:00        80        58   10        F  Fuorn   8     5 2012   Y         26  FALSE
    1.RUN 01/05/2012 00:00:00        80        58   10        F  Fuorn   8     5 2012   Y         26  FALSE
    2.F   01/05/2012 00:05:00        90        76   10        F  Fuorn   8     5 2012   Y         26   TRUE
    2.R   01/05/2012 00:05:00        90        76   10        F  Fuorn   8     5 2012   Y         26  FALSE
    2.M   01/05/2012 00:05:00        90        76   10        F  Fuorn   8     5 2012   Y         26  FALSE
    2.RUN 01/05/2012 00:05:00        90        76   10        F  Fuorn   8     5 2012   Y         26  FALSE
    3.F   01/05/2012 00:10:00        51        47   10        M  Fuorn   8     5 2012   Y         26  FALSE
    3.R   01/05/2012 00:10:00        51        47   10        M  Fuorn   8     5 2012   Y         26  FALSE
    3.M   01/05/2012 00:10:00        51        47   10        M  Fuorn   8     5 2012   Y         26   TRUE
    3.RUN 01/05/2012 00:10:00        51        47   10        M  Fuorn   8     5 2012   Y         26  FALSE
    4.F   01/05/2012 00:15:00         0         0   10        R  Fuorn   8     5 2012   Y         26  FALSE
    4.R   01/05/2012 00:15:00         0         0   10        R  Fuorn   8     5 2012   Y         26   TRUE
    4.M   01/05/2012 00:15:00         0         0   10        R  Fuorn   8     5 2012   Y         26  FALSE
    4.RUN 01/05/2012 00:15:00         0         0   10        R  Fuorn   8     5 2012   Y         26  FALSE
    5.F   01/05/2012 00:20:00         0         0    9        R  Fuorn   8     5 2012   Y         26  FALSE
    5.R   01/05/2012 00:20:00         0         0    9        R  Fuorn   8     5 2012   Y         26   TRUE
    5.M   01/05/2012 00:20:00         0         0    9        R  Fuorn   8     5 2012   Y         26  FALSE
    5.RUN 01/05/2012 00:20:00         0         0    9        R  Fuorn   8     5 2012   Y         26  FALSE
</code></pre>

<p>then I ran my regression :</p>

<pre><code>m1 &lt;- mlogit(Choice ~ 1 |Temp + Valley + Age + kid + Month , mlogit)
</code></pre>

<p>and it gave me significant results :</p>

<pre><code>                          Estimate  Std. Error  t-value  Pr(&gt;|t|)    
    M:(intercept)      -4.2153e-01  5.7533e-02  -7.3268 2.358e-13 ***
    R:(intercept)       6.2325e-01  3.4958e-02  17.8284 &lt; 2.2e-16 ***
    RUN:(intercept)    -1.2275e+01  4.0526e-01 -30.2895 &lt; 2.2e-16 ***
    M:Temp              1.5371e-02  9.8680e-04  15.5764 &lt; 2.2e-16 ***
    R:Temp             -3.9871e-02  6.7926e-04 -58.6975 &lt; 2.2e-16 ***
    RUN:Temp           -4.4532e-02  6.8696e-03  -6.4825 9.023e-11 ***
    M:ValleyTrupchun   -3.6154e-01  1.6362e-02 -22.0968 &lt; 2.2e-16 ***
    R:ValleyTrupchun   -4.0186e-02  9.7968e-03  -4.1020 4.096e-05 ***
    RUN:ValleyTrupchun  1.2895e+00  8.5357e-02  15.1066 &lt; 2.2e-16 ***
    M:Age              -1.1026e-02  2.6902e-03  -4.0985 4.158e-05 ***
    R:Age               1.9465e-02  1.6479e-03  11.8119 &lt; 2.2e-16 ***
    RUN:Age             5.5473e-02  1.6661e-02   3.3294 0.0008703 ***
    M:kidY              6.0686e-02  2.2638e-02   2.6807 0.0073460 ** 
    R:kidY             -4.1638e-01  1.2391e-02 -33.6024 &lt; 2.2e-16 ***
    RUN:kidY            6.2311e-01  1.0410e-01   5.9854 2.158e-09 ***
    M:Month            -2.0466e-01  8.4448e-03 -24.2346 &lt; 2.2e-16 ***
    R:Month             2.4148e-02  5.2317e-03   4.6157 3.917e-06 ***
    RUN:Month           9.8715e-01  5.6209e-02  17.5622 &lt; 2.2e-16 ***
</code></pre>

<p>those results were in line with what I expected to find in literature so I was quite happy.</p>

<p>My next step was to plot my results and here is when I have some trouble.</p>

<p>First of all when I plot my original data and compare it with the result of my regression I find some huge differences. For example, when I plot the %of time spend in a behavior (M for moving, F for feeding, R for resting and Run for running, in my regression F is the reference) in function of age, I find that the older an individual gets, the more they will rest and the more they will move, but the estimates I got from my regression shows that they should rest more (when they get older) but move less. So to summarize, my graph on the original data shows the opposite as what I got from the regression.</p>

<p>I don't know if it is normal, in the sense that I don't know if I can compare my original data to the result of my regression in a way that my regression shows the probability from switching to a behavior from an other each time my variable grows of one unit.</p>

<p>So I wanted to use the <code>predict()</code> function but I don't know how to do that. I was hoping to get some help here.</p>
"
"0.059601995508215","0.0306281945915844"," 48651","<p>I am looking for a Least Angle Regression (LAR) packages in R or MATLAB which can be used for <strong>classification</strong> problems.</p>

<p>The only package that I currently know which fits this description is <a href=""http://cran.r-project.org/web/packages/glmpath/index.html"" rel=""nofollow"">glmpath</a>. The issue with this package is that it is a little old and somewhat limited in its scope (I am forced to rely on logistic regression for classification problems model).</p>

<p>I am wondering if anyone knows of other packages that allow me to run LAR on different types of classification models, such as Support Vector Machines (see <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.62.391&amp;rep=rep1&amp;type=pdf"" rel=""nofollow"">The Entire Regularization Path for the Support Vector Machine</a>). </p>

<p>The ideal package would allow me to run LAR-type algorithms for different types of classification models and also provide a function that can produce the full regularization path. </p>
"
"0.126434925588327","0.129944424547263"," 49497","<p>I have a dataset I'm working on that has some co-variate shift between the training set and the test set.  I'm trying to build a predictive model to predict an outcome, using the training set.  So far my best model is a random forest.</p>

<p>How can I deal with the shifted distributions in the training vs. test set?  I've come across 2 possible solutions that I've been able to implement myself:</p>

<ol>
<li>Remove the shifted variables.  This is sub-optimal, but helps prevent my model from over fitting the training set.</li>
<li>Use a logistic regression to predict whether a given observation is from the test set (after balancing the classes), predict ""test set probabilities"" for the training set, and then boostrap sample the training set, using the probabilities for sampling.  Then fit the final model on the new training set.</li>
</ol>

<p>Both 1 and 2 are pretty easy to implement, but neither one satisfies me, as #1 omits variables that might be relevant, and #2 uses a logistic regression, when my final model is tree-based.  Furthermore, #2 takes a few paragraphs of custom code, and I worry that my implementation may not be correct.</p>

<p>What are the standard methods for dealing with covariate shift?  Are there any packages in R (or another language) that implement these methods?</p>

<p>/edit: It seems like ""kernel mean matching"" is another approach I could take.  I've found lots of academic papers on the subject, but no one seems to have published any code.  I'm going to try to implement this on my own, and will post the code as an answer to this question when I do.</p>
"
"0.042144975196109","0.043314808182421"," 49714","<p>I have recently begun to read about bayesian statistics and I am playing around with the R2WinBUGS package. I'm trying to fit a logistic regression to the spam data (that can be found on the webpage of the elements of statistical learning) using R and WinBUGS. My approach was to first divide the data into 80% training and 20% testing sets. I can fit the model using the 80% set but I dont know how to write WinBugs code to predict on new observations (say the 20% test set) and I wonder if this approach to study model/classification precisicion make sense in a Bayesian Approach?</p>
"
"0.0842899503922179","0.086629616364842"," 50726","<p>I'm using the <code>lme4</code> package in R to do some logistic mixed-effects modeling.<br>
My understanding was that sum of each random effects should be zero.</p>

<p>When I make toy linear mixed-models using <code>lmer</code>, the random effects are usually &lt; $10^{-10}$ confirming my belief that the <code>colSums(ranef(model)$groups) ~ 0</code>
But in toy binomial models (and in models of my real binomial data) some of the random effect sum to ~0.9. </p>

<p>Should I be concerned?  How do I interpret this?  </p>

<p>Here is a linear toy example
<code><pre>
toylin&lt;-function(n=30,gn=10,doplot=FALSE){
 require(lme4)
 x=runif(n,0,1000)
 y1=matrix(0,gn,n)
 y2=y1
 for (gx in 1:gn)
 {
   y1[gx,]=2*x*(1+(gx-5.5)/10) + gx-5.5  + rnorm(n,sd=10)
   y2[gx,]=3*x*(1+(gx-5.5)/10) * runif(1,1,10)  + rnorm(n,sd=20)
 }
 c1=y1*0;
 c2=y2*0+1;
 y=c(t(y1[c(1:gn),]),t(y2[c(1:gn),]))
 g=rep(1:gn,each=n,times=2)
 x=rep(x,times=gn*2)
 c=c(c1,c2)
 df=data.frame(list(x=x,y=y,c=factor(c),g=factor(g)))
 (m=lmer(y~x*c + (x*c|g),data=df))
 if (doplot==TRUE)
  {require(lattice)
   df$fit=fitted(m)
   plot1=xyplot(fit ~ x|g,data=df,group=c,pch=19,cex=.1)
   plot2=xyplot(y ~ x|g,data=df,group=c)
   print(plot1+plot2)
  }
 print(colMeans(ranef(m)$g))
 m
}
</code></pre></p>

<p>In this case the colMeans always come out $&lt;10^{-6}$ </p>

<p>Here is a binomial toy example (I would share my actual data, but it is being submitted for publication and I am not sure what the journal policy is on posting beforehand):</p>

<p><pre><code>
toybin&lt;-function(n=100,gn=4,doplot=FALSE){
  require(lme4)<br>
  x=runif(n,-16,16)
  y1=matrix(0,gn,n)
  y2=y1
  for (gx in 1:gn)
  { com=runif(1,1,5)
    ucom=runif(1,1,5)
    y1[gx,]=tanh(x/(com+ucom) + rnorm(1)) > runif(x,-1,1)
    y2[gx,]=tanh(2*(x+2)/com + rnorm(1)) > runif(x,-1,1)
  }
  c1=y1*0;
  c2=y2*0+1;
  y=c(t(y1[c(1:gn),]),t(y2[c(1:gn),]))
  g=rep(1:gn,each=n,times=2)
  x=rep(x,times=gn*2)
  c=c(c1,c2)
  df=data.frame(list(x=x,y=y,c=factor(c),g=factor(g)))
  (m=lmer(y~x*c + (x*c|g),data=df,family=binomial))
  if (doplot==TRUE)
   {require(lattice)
    df$fit=fitted(m)
    print(xyplot(fit ~ x|g,data=df,group=c,pch=19,cex=.1))
   }
  print(colMeans(ranef(m)$g))
  m
}
</pre></code></p>

<p>Now the colMeans sometimes come out above 0.3, and definitely higher, on average than the linear example.</p>
"
"0.11150512337989","0.114600210537152"," 51152","<p>I've been trying to use the fastbw function from the rms package in R to perform logistic regression with backward selection, with p-values as exclusion criterion (I am well aware of the arguments against using p-values for this as opposed to e.g. AIC). However, the results are not in agreement with what I would get if I perform the backward selection manually, as fastbw often drops more factors in comparison. The results also seem to depend on the number of factors considered, even with the option </p>

<pre><code>type=""individual"".
</code></pre>

<p>I created some simple example data in order to prove my point, which give the following result:</p>

<pre><code>&gt; fastbw(lrm(y~x1+x2+x3+x4),rule=""p"",type=""individual"")

 Deleted Chi-Sq d.f. P      Residual d.f. P      AIC  
 x3      0.37   1    0.5412 0.37     1    0.5412 -1.63
 x1      1.82   1    0.1771 2.20     2    0.3336 -1.80
 x4      2.58   1    0.1082 4.78     3    0.1889 -1.22
 x2      3.56   1    0.0591 8.34     4    0.0799  0.34

[...]

Factors in Final Model

None
</code></pre>

<p>I.e., x2 is dropped as the last of the factors considered, resulting in a model without factors. However, if I consider x2 only, I get the following result. </p>

<pre><code>&gt; fastbw(lrm(y~x2),rule=""p"",type=""individual"")

No Factors Deleted

Factors in Final Model

[1] x2
</code></pre>

<p>The same is true if I do the backward selection manually, as x2 considered separately has a p-value of 0.045. What might cause this behavior? Since x2 is the last remaining variable in the backward selection, the results shouldn't depend on associations with other model covariates.</p>
"
"0.11150512337989","0.0982287518889871"," 51464","<p>Background: For a project, I am fitting a conditional logit model where I have 5 control cases for every realized case. To do that I use the <code>clogit()</code> function in the package <code>survival</code>. I wanted to graph interactions with the <code>effects</code> package by John Fox et al. It turns out that this package can't handle <code>clogit</code> objects (output of <code>clogit()</code>). </p>

<p>As I believed I remembered that conditional logit were a special case of GLM, I thought the clever/lazy way to get my interaction plots would be to refit the model using a fixed effects glm and then use <code>effect()</code>.
The documentation of <code>clogit</code> seemed to confirm my intuition:</p>

<blockquote>
  <p>It turns out that the logliklihood for a conditional logistic regresson model = loglik from a Cox model with a particular data structure. [...]
  When a well tested Cox model routine is available many packages use this â€˜trickâ€™ rather than writing a new software routine from scratch, and this is what the clogit routine does. </p>
  
  <p>In detail, a stratified Cox model with each case/control group assigned to its own stratum, time set to a constant, status of 1=case 0=control, and using the exact partial likelihood has the same likelihood formula as a conditional logistic regression. The clogit routine creates the necessary dummy variable of times (all 1) and the strata, then calls coxph.</p>
</blockquote>

<p>Based on this description, it seems that I should be able to reproduce the stratification achieved through <code>strata()</code> by using a random intercept for each case/control group with <code>1|group</code> in <code>lmer()</code>. However, when I try, the results of <code>clogit</code> and <code>lmer</code> differ. One thing is that I probably have the wrong likelihood function. I don't really know how to specify this in <code>lmer</code> but more important, I am wondering what else I am missing. </p>

<p>I wonder whether I am completely wrong or somewhat on the right track but missing some pieces? What I would like is to understand what are the difference in terms of how the model is fitted between a conditional logit and a regular one (I understand that might be quite a long answer, so a book reference would be a great start). The my usual references for regression (Gelman and Hill, 2007; Mills 2011) are somewhat silent on the subject.</p>
"
"0.042144975196109","0.043314808182421"," 55240","<p>I'm working on a data set modeling road kills (0 = random point, 1 = road kill) as a function of a number of habitat variables.  Following Hosmer and Lemeshow, I've examined each continuous predictor variable for linearity, and a couple appear nonlinear.  I'd like to try a fractional polynomial transformation for each, also following Hosmer and Lemeshow, and have looked at the R package mfp, but I'm having trouble coming up with (and understanding) the R code that will correctly transform the variable.  Can anyone suggest R code that would help me accomplish the concepts on p. 101 - 102 of Hosmer and Lemeshow's Applied Logistic Regression (2000).  Thanks!</p>
"
"0.0729972383233908","0.0750234484920533"," 56608","<p>I'm doing some clinical database research and in an effort to lessen the burden on our statistical staff, I started to look for different software solutions to get the analyses I need, and that's where BIDS (business intelligence development studio).  BIDS allows queries to be run against a SQL Server and store the results of those queries in a table or view.  That table or view is then consumed by BIDS and logistic and linear regressions as while as CART analyses can be done on them.<br>
BIDS Version</p>

<p>The x axis that is cut off on the lift chart is 'overall population %'
<img src=""http://i.stack.imgur.com/LbpXf.jpg"" alt=""enter image description here""> </p>

<p>A mining accuracy chart of the CART
<img src=""http://i.stack.imgur.com/Vm5Te.jpg"" alt=""enter image description here""></p>

<p>I'm not quite sure how this works in other stats packages, but in BIDS you define a certain percentage of your data set to train the model and the rest of the set is compared against that model and the lift chart shows the improvement in identifying the outcome you desire vs. a guess.  I'm only vaguely familiar with CART analyses in the first place, and don't know the first thing about R, but this same analysis was done in R with very similar results.  The red portion of what looks like a health meter in a video game corresponds nearly identical to the analysis done in R.  However, there are no p values in the BIDS version.  Consider the node Max Total Poly Pharm >=7 and &lt; 13.
BIDS shows me (not present in the picture)
value         cases    probability
not present   2133     91.89</p>

<p>Is there any way to ascertain a p value from that.  And does R use any of it's cases as training data for the model?</p>
"
"0.0729972383233908","0.0750234484920533"," 56871","<p>I have a dataset from a bank with demographic data and one variable telling if the customer is a good customer or not (binary variable). I would like to do prediction on if the customer is good or not based on this demographic data.</p>

<p>I managed to do it with a logistic regression, but would like now to compare the result (classification rate) with neural networks. </p>

<p>I found 2 functions from different packages doing that:
- nnet()
- neuralnet()
But those functions seem to be conceived for numerical dependent variables.</p>

<p>Thus my question: is there a possibility to use these functions for a categorical numerical variable (by estimating a posteriori probabilities for instance) or is there another function doing that?</p>

<p>Thanks a lot!</p>

<p>Robin</p>
"
"0.0729972383233908","0.0750234484920533"," 57448","<p>I am running a model (logistic regression) with 20 independent variables in R. </p>

<p>Before running the model I calculated the correlation between all the variables and finally selected my variables by also checking ""visually"" the histograms of each variable in the case of presence and again in the case of absence. In situations where I don't see any obvious distribution associated to both presence &amp; absence, I discard the variable.</p>

<p>I would like to make ""official"" calculations for the level of relation between Presence/Absence and each variable (how much each variable contributes to the Presence/Absence), for example with <code>Cramer's V index</code>, but the available function I find is from the package <code>vcd</code> and has some limitations: 
doesn't give the <code>Cramer's V</code> (as well as the Phi-Coefficient Contingency Coeff.) for each independent variable, and it doesn't run for one independent variable.</p>

<p>I might be missing some other obvious way to do this. Any help is appreciated.</p>
"
"0.11920399101643","0.122512778366338"," 58315","<p>I want to find the most important predictors for a binomial dependent variable out of a set of more than 43,000 independent variables (These form the columns of my input dataset). The number of observations is more than 45,000 (these form the rows of my input dataset). Most of the independent variables are unigrams, bigrams and trigrams of words, so there is high degree of collinearity among them. There is a lot of sparsity in my dataset as well. I am using the logistic regression from the glmnet package, which works for the kind of dataset I have. Here is some code:</p>

<pre><code>library('glmnet')
data &lt;- read.csv('datafile.csv', header=T)
mat = as.matrix(data)
X = mat[,1:ncol(mat)-1] 
y = mat[,ncol(mat)]
fit &lt;- cv.glmnet(X,y, family=""binomial"", type.measure = ""class"")
betacoeff = as.matrix(fit$glmnet.fit$beta[,ncol(fit$glmnet.fit$beta)])
</code></pre>

<p>betacoeff returns the betas for all the independent variables. I am thinking of showing the predictors corresponding to the top 50 betas as the most important predictors. 
My questions are:</p>

<ol>
<li><p>glmnet picks one good predictor out of a bunch of highly correlated good predictors. So I am not sure how much I can rely on the betas returned by the above model run.</p></li>
<li><p>Should I manually sample the data (say 10 times) and each time run the above model, get the list of predictors with the top betas and then find those which are present in all 10 repetitions? Is there any standard way of doing this? What is the standard way of sampling in this case?</p></li>
<li><p>My other question is about cvm (cross validation error) returned by the above model. Since I use type.measure = ""class"", cvm gives the misclassification error for different values of lambda. How do I report the misclassification error for the entire model? Is it the cvm corresponding to lambda.min?</p></li>
</ol>
"
"0.059601995508215","0.0612563891831689"," 58772","<p>In testing the parallel regression assumption in ordinal logistic regression I find there are several approaches. I've used both the graphical approach (as detailed in HarrellÂ´s book) and the approach detailed using the  <a href=""http://cran.r-project.org/web/packages/ordinal/vignettes/clm_tutorial.pdf"" rel=""nofollow"">ordinal package</a> in R. </p>

<p>However I would also like to run the Brant test (from Stata) for both the individual variables and also for the total model. I've looked around but cannot find it implemented in R. </p>

<p><strong>Is there an implementation of the Brant test in R?</strong></p>
"
"0.0842899503922179","0.086629616364842"," 59311","<p>I'm working on a behavoural scorecard modelling exercise, and many of the decisions taken to date have been based on the experience of a consulting credit analyst (whose experience software-wise is SAS) as I am primarily in BI. So far I have:</p>

<ul>
<li>a linux pc with 32gb of ram and an i7 processor</li>
<li>an observation window  </li>
<li>~90 potential characteristics </li>
<li>a binary outcome</li>
</ul>

<p>In <strong>R</strong>, I have</p>

<ol>
<li>loaded the dataset (225k obs of 88 vars, 1 outcome)</li>
<li>split the dataset up based on the recommendations/examples in the package <strong>caret</strong> i.e. predictors and outcomes split up (150k obs in training sample)</li>
<li>removed any variables showing a high degree of correlation (caret::findCorrelation)</li>
<li>cut all continuous variables into categorical intervals </li>
<li>reduced the number of variables based on near zero variance, missing values, and low information value (IV) (150k obs of 48 vars)</li>
<li>tried bestglm::bestglm, caret::train (with glm and glmnet), FWDselect::selection, FWDselect::qselection but eventually had to interrupt each of these due to not completing after 4 hours of 100% CPU usage</li>
<li>used FactoMineR:MCA to perform a multiple correspondence analysis (on predictors only)</li>
</ol>

<p>What I would like to do is have a selection of logistic regression models for say 4, 8, 12, and 16 variables that are the most predictive models at each point.  I'm not sure if I'm going in the correct direction here with MCA as I've mainly been simply trying to find something that works in a timely fashion for reducing my variables further or going directly to variable selection steps.</p>

<p>I would appreciate any advice on how to do any of step 6 better, whether 7 makes sense and what step 8 should be. </p>

<p>Thanks,</p>

<p>Steph</p>

<p>PS Design decisions up to 6&amp;7 can't be revised so please, no telling me off for them! </p>
"
"NaN","NaN"," 59906","<p>I can't find any packages which allow me to implement bolasso in R, does anyone know of one?</p>

<p>Otherwise, I am interested in model selection techniques which can be implemented in R for logistic models. </p>

<p>My sole concern is predictive ability. The data has many covariates (or features) and I would like to investigate up to 3rd or even 4th level interactions and 2nd order terms, this is several thousand covariates (there are ~200k observations) so it is important that the techniques are fast.</p>
"
"0.0632174627941634","0.086629616364842"," 60087","<p>Is it viable to do several binary logistic regressions instead of doing a multinomial regression? From this question: <a href=""http://stats.stackexchange.com/questions/52104/multinomial-logistic-regression-vs-binary-logistic-regression"">Multinomial logistic regression vs binary logistic regression</a> I see that the multinomial regression might have lower standard errors. </p>

<p>However, the package I would like to utilize has not been generalized to multinomial regression (<code>ncvreg</code>: <a href=""http://cran.r-project.org/web/packages/ncvreg/ncvreg.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/ncvreg/ncvreg.pdf</a>) and so I was wondering if I could simply do several binary logistic regressions instead.</p>
"
"0.151955869072729","0.132147029290359"," 61138","<p>I am trying to calculate the marginal effects of a multinomial logistic regression. To do this I use the <code>mlogit</code> package and the <code>effects()</code> function.</p>

<p>Here is how the procedure works (source : <code>effects()</code> function of <code>mlogit</code> package) :</p>

<pre><code>data(""Fishing"", package = ""mlogit"")
Fish &lt;- mlogit.data(Fishing, varying = c(2:9), shape = ""wide"", choice = ""mode"")
m &lt;- mlogit(mode ~ price | income | catch, data = Fish)
# compute a data.frame containing the mean value of the covariates in the sample
z &lt;- with(Fish, data.frame(price = tapply(price, index(m)$alt, mean), 
	catch = tapply(catch, index(m)$alt, mean), 
income = mean(income)))
# compute the marginal effects (the second one is an elasticity
effects(m, covariate = ""income"", data = z)
effects(m, covariate = ""price"", type = ""rr"", data = z)
effects(m, covariate = ""catch"", type = ""ar"", data = z)
</code></pre>

<p>I have no problem with first step (<code>mlogit.data()</code> function). I think my problem is in the specification of the multinomial regression.</p>

<p>My regression (for example with three variables) is on the form: <code>Y ~ 0 | X1 + X2 + X3</code>. When I try to estimate the marginal effects for a model with 2 variables, there is no problem, however for 3 variables R console returns me the following error: ""Error in if (rhs% in% c (1, 3)) {: argument is of length zero "" (translation from error in R console in french).</p>

<p>To understand what is my problem I tried to perform a multinomial regression of similar shape on the dataset ""Fishing"", i.e.,: <code>mode ~ 0 | income + price + catch</code> (even if this form has no ""economic"" sense.) Again the R console returns me the same error for 3 variables but manages to estimate these effects for a model with two variables.</p>

<p>This leads me to think that my problem really comes from the specification of my multinomial regression.  Do you know how I could find a solution to my problem? Or could you suggest another logit multinomial regression form ?</p>

<p>Thank you for your help :)</p>
"
"0.042144975196109","0.043314808182421"," 61845","<p>I am trying to run a Cox regression on a sample 2,000,000 row dataset as follows using only R. This is a direct translation of a PHREG in SAS. The sample is representative of the structure of the original dataset.</p>

<pre><code>##
library(survival)

### Replace 100000 by 2,000,000

test &lt;- data.frame(start=runif(100000,1,100), stop=runif(100000,101,300), censor=round(runif(100000,0,1)), testfactor=round(runif(100000,1,11)))

test$testfactorf &lt;- as.factor(test$testfactor)
summ &lt;- coxph(Surv(start,stop,censor) ~ relevel(testfactorf, 2), test)

# summary(summ)
##

user  system elapsed 
9.400   0.090   9.481 
</code></pre>

<p>The main challenge is in the compute time for the original dataset (2m rows). As far as I understand, in SAS this could take up to 1 day, ... but at least it finishes.</p>

<ul>
<li><p>Running the example with only 100,000 observations take only 9 seconds. Thereafter the time increases almost quadratically for every 100,000 increment in the number of observations.</p></li>
<li><p>I have not found any means to parallelize the operation (e.g., we can leverage a 48-core machine if this was possible)</p></li>
<li><p>Neither <code>biglm</code> nor any package from Revolution Analytics is available for Cox regression, and so I cannot leverage those.</p></li>
</ul>

<p><strong>Is there a means to represent this in terms of a logistic regression (for which there are packages in Revolution) or if there are any other alternatives to this problem?</strong> I know that they are fundamentally different, but it's the closest I can assume as a possibility given the circumstances. </p>
"
"0.0842899503922179","0.086629616364842"," 61996","<p>The data set which I am trying to analyse is Student Test Data.</p>

<p>I have a data of responses (either 1/correct response or 0/incorrect response) on some questions of a set of students. I have fitted a ""3 parameter logistic model"" for each question and then calculated the goodness of fit estimate and hence the respective p-values.</p>

<p>Now my problem is I don't know how to aggregate these individual goodness of fit estimates to get a total goodness of fit for the whole model, Is there any measure (preferably in the programming language R) which can suggest about the whole model depending on the p-values of multiple tests. The package which I used is ltm inside R.</p>
"
"0.086028070377267","0.106099178353461"," 62307","<p>I used regular logistic regression on my dataset and got a few significant hits. However, since the data is 1:1 case-control matched data I decided to try using conditional logistic regression (<code>clogit()</code> in the <code>survival</code> package of <code>R</code>). </p>

<p>However, none of the results are significant any more. Is this normal? I was hoping to gain power by switching to conditional logistic regression. </p>

<pre><code> m&lt;-clogit(PHENO==2 ~ x + as.factor(COVAR[,1]) + strata(COVAR[,2]) )
</code></pre>

<p>This is a GWAS (genome wide association study). <code>PHENO</code> is either 1 (control) or 2 (case). <code>x</code> is number of copies of the minor allele (0, 1, or 2) in the SNP of interest, <code>COVAR[,1]</code> is a race covariate and <code>COVAR[,2]</code> is the matching covariate. </p>

<p>This test is performed for each SNP (~300,000) and the resulting p-values are adjusted for FDR. </p>

<p>Perhaps the matching of the cases and controls was not very good?</p>

<p>Matching: case and control pairs had: 1) same sex. 2) BMI > threshold. 3) same self-reported ethnicity. 4) lived within 50 miles of procurement site. </p>

<p>Perhaps I should use GLMM?</p>
"
"0.042144975196109","0.043314808182421"," 62483","<p>I've got a conditional logistic regression setup using <code>clogit</code> in <code>R</code> like this: </p>

<pre><code>m&lt;-clogit(PHENO==2 ~ x + as.factor(COVAR[,1]) + strata(COVAR[,2]) )
</code></pre>

<p>I wanted to try doing GLMM analysis in <code>R</code>. I'm a little confused on the syntax for the <code>lme4</code> package in <code>R</code>. <code>COVAR[,2]</code> is the matching variable in my data. Can someone explain to me the difference between some of these statements:</p>

<pre><code>m &lt;-  lmer(PHENO==2 ~ x + as.factor(COVAR[,1]) + (x|COVAR[,2]) )
m &lt;-  lmer(PHENO==2 ~ x + as.factor(COVAR[,1]) + (1|COVAR[,2]) )
</code></pre>

<p>Which one is more appropriate? </p>
"
"NaN","NaN"," 62919","<p>@Dmitrij Celov posted an answer to the question <a href=""http://stats.stackexchange.com/questions/8303/how-to-do-logistic-regression-subset-selection"">How to do logistic regression subset selection</a> saying that <a href=""http://www.jstor.org/discover/10.2307/2531779?uid=3739256&amp;uid=2&amp;uid=4&amp;sid=21102415674961"" rel=""nofollow"">this paper</a> was a good reference. I was wondering if anyone knows if any of these authors ever created an R package based on it.</p>
"
"0.0942390294485422","0.0968548555282575"," 65730","<p>I want to check the probability distribution for my data using R.In some site I found <code>fitdistr</code> function in package <code>MASS</code> can do that. To check that I have generated 105 random Poisson numbers &amp; run the <code>fitdistr</code> function to check whether it is able to identify or not. I used following code</p>

<pre><code>library(MASS)
zpois=rpois(105,0.1)

fitdistr(zpois, 't')$loglik
fitdistr(zpois, 'normal')$loglik
fitdistr(zpois, 'logistic')$loglik
fitdistr(zpois, 'weibull')$loglik
fitdistr(zpois, 'gamma')$loglik
fitdistr(zpois, 'lognormal')$loglik
fitdistr(zpois, 'exponential')$loglik
fitdistr(zpois, 'Poisson')$loglik
fitdistr(zpois, 'negative binomial')$loglik
</code></pre>

<p>I found that it is giving lowest value for normal distribution. I know that the large sample approximation of Poisson distribution is normal distribution but I don't want the large sample approximation. I want to see the exact distribution.</p>

<p>Can you help me to guide the suitable function in R using which I can get the exact distribution fit?</p>
"
"0.042144975196109","0.043314808182421"," 66279","<p>Has anyone written a package in <code>R</code> to calculate diagnostic plots after <code>clogit</code>, conditional logistic regression? e.g. leverage. Or a related question, how do you stratify using <code>glm</code> (perhaps I can stratify using <code>glm</code> and <code>family =binomial</code>, and then use diagnostic packages for <code>glm</code>?)</p>
"
"0.273204544979369","0.274258027159403"," 67873","<p><strong>TLDR</strong>: How can I perform inference for the between group differences in a possibly logistic growth with time in the presence of outliers, unequal measurement times and frequency, bounded measurements and possible random effects on individual and per study level?</p>

<p>I am attempting to analyse a dataset where measurements for individuals were made at different time points. Measurements start low at time 0 and follow (very roughly) a logistic growth pattern with time. I am trying to establish if there are differences between two groups of individuals. The analysis is complicated by the following factors:</p>

<ul>
<li>The effect of time is non-linear, so either a non-linear logistic regression (biologically plausible, but not particularly well fitting) or a non-parametric regression seem appropriate</li>
<li>There are massive outliers, so regression using the sum of squared residuals seems off the table. Quantile regression seems appropriate.</li>
<li>Random effects may be appropriate on a per individual and per study level. Mixed effects models seems appropriate.</li>
<li>Measurement times, number of available measurements and end of monitoring differ between individuals. Survival analysis techniques seem appropriate. Possibly also applying weights equal to 1 / number of observations for individual.</li>
<li>Measurements are bounded below at 0 and while there is no obvious boundary above, arbitrarily high measurements seem biologically implausible. However, quite a few individuals have some measurements of zero (partly due to the measurement accuracy of the device).</li>
<li>A few models I tried so far failed to fit, usually with an unhelpful error related to the numerical procedure. This leads me to believe that I will need a reasonably robust method able to deal with this somewhat ugly dataset.</li>
<li>Finally, I want to produce inference of the form ""group 1 has faster growth than group 2"" or ""group 1 has a higher asymptotic level than group 2"".</li>
</ul>

<p>What I have tried so far (all in R) - I was aware that most of the below are not particularly appropriate for the dataset, but I wanted to see which models could actually be fitted without numerical errors:</p>

<ul>
<li>Non-parametric regression using crs in the crs package. Nicely produces a curve reasonably close to logistic growth for most of the time period with some strange behavious toward the end of the monitoring period (where there are fewer observations). Using individuals as fixed effects reveals some outliers. Using the variable of interest as fixed effects shows some difference. However, I am not sure if there is any way to assess fits and do inference on a model this complex.</li>
<li>Non-linear mixed effects regression using nlme in package nlme and SSlogis. Gradually building up the model with update() works reasonably well. Getting too complex with the fixed effects or the random effects leads to convergence failure. Since there is no reason to believe that residuals are normally distributed, it seems a bad idea to pursue this further. Edit: I have recently become aware that it is possible to specify autocorrelated residuals in nlme. However, at the moment it seems I cannot even get fixed weights to work. Advice on the correct syntax is welcome.</li>
<li>Non-linear mixed effects regression using nlmer in package LME4 and a custom likelihood for the logistic growth model. Works fairly well, but standard errors on the fixed effects get massive, probably due to the outliers. I also have the slight suspicion that some of the models fail to fit without error, as I sometimes get tiny random effects (about 10^10 smaller than with slightly simpler models). Since there is no reason to believe that residuals are normally distributed, it seems a bad idea to pursue this further.</li>
<li>Non-linear quantile regression using nlrq in package quantreg and SSlogis. Fits reliably and quickly, but percentile lines intersect. This means that an area containing 90% of the data is not fully contained in an area containing 95% of the data.</li>
<li>Non-parametric quantile regression using the LMS method with package VGAM. Even trivial models failed with obscure errors using this dataset. I believe the number of zeros in the dataset and / or the large range of the data while also getting close to zero may be an issue.</li>
<li>To complete this list, I should probably also mention the lqmm package for Linear Quantile Mixed Models, which I have not used yet. While the package cannot use non-linear models as far as I know, transforming the time variable may produce something reasonably close.</li>
</ul>

<p>I would appreciate feedback if these or any other method might be used to produce reasonably robust inference in this scenario. Maybe regression is not needed at all and another, possibly simpler method is sufficient. I'd be happy to provide an example dataset, if required, but think this question might also be of interest beyond the current dataset.</p>
"
"0.11150512337989","0.114600210537152"," 68553","<p>I am trying to run a logistic regression analysis in R using the speedglm package. 
The data is CNVs (a type of genetic variant), whether that CNV occurred in a case or control and wether genes in a pathway is hit by the CNV or not (Pathway.hit), and how many genes were hit by the CNV that were not in the pathway (Pathway.out).
I run two models with and without the Pathway.hit covariate and compare to see if a pathway is preferentially hit by cases vs controls.  </p>

<p>the models and comparison of said are as follows:</p>

<pre><code>fit1 = speedglm(status~size+Pathway.out, data=cnvs, family=binomial('logit'))
fit2 = speedglm(status~size+Pathway.out+Pathway.hit, data=cnvs,family=binomial('logit'))
P.anova = 1-pchisq(abs(fit1$deviance - fit2$deviance), abs(fit1$df - fit2$df))
</code></pre>

<p>It seems to work okay for most data I throw at it, but in a few cases I get the error:</p>

<pre><code>Error in solve.default(XTX, XTz, tol = tol.solve) : 
  system is computationally singular: reciprocal condition number = 1.87978e-16
</code></pre>

<p>After some googling around I think I found what's causing the problem:</p>

<pre><code>by(cnvs$Pathway.hit, cnvs$status, summary)
cnvs$status: 1 (controls)
        0     1 
    13333     0 
    ------------------------------------ 
    cnvs$status: 2 (cases)
    0     1 
10258     2 
</code></pre>

<p>So here there no observations in controls and only 2 in cases. </p>

<p>If I use with normal glm method however, then it does not throw an error (but that of course doesn't necessarily mean the results will be meaningful). The reason I am using the speedglm package is that I have approximately 16,000 of these analyses to run, and using the base glm function for all 16,000 takes about 20 hours, where as I think speedglm can reduce it down to 8 or so.</p>

<p>So my question is, should I ignore those analyses which throw an error and list the results as NA as there were too few observations, or when speed glm fails should I retry with normal glm? In the above example there are 2 observations of the covariate in cases, but 0 in controls. Might this not be interesting? Would the analysis also fail if there were 0 in controls and 20 in cases - that would certainly be interesting would it not?</p>

<p>Thanks for the help in advance,
Cheers,
Davy</p>
"
"0.163226787060855","0.156573695352373"," 68786","<p>I measured a binary response for each subject in 5 different conditions. For each subject and condition, I replicated the experiment 36 times. I thus have 36 binary values per condition per subject.</p>

<p>I am trying to build a model for those data. I suppose a logistic regression is what I'm looking for, and I am working with the <code>lmer</code> package. My aim is to check whether the conditions significantly influence the observed values, so I would have two models:</p>

<pre><code>lmH1&lt;-lmer(value~condition, (random effects), data=dataset, family=binomial)
</code></pre>

<p>and</p>

<pre><code>lmH0&lt;-lmer(value~1, (random effects), data=dataset, family=binomial) 
</code></pre>

<p>By looking at the output from <code>anova(lmH0, lmH1)</code>, I would be able to determine the significance of the effect of my condition.</p>

<p>I am just not sure what to specify as random effect; the models I defined so far are:</p>

<pre><code>lmH1 &lt;- lmer( value ~ condition + ( 1 | subject ), data = dataSet, family = binomial )
</code></pre>

<p>and </p>

<pre><code>lmH2 &lt;- lmer( value ~ condition + ( 1 | subject/condition ), data = dataSet, family = binomial )
</code></pre>

<p>However I am not sure about how lmer handles the replicates, so I don't know whether I should include those replicates in my random effects or not. I could modify the proposed models so that the grouping defined by the random effects refers to a specific binary values instead of a group of binaries values. My new models would then be</p>

<pre><code>lmH1a &lt;- lmer( value ~ condition + ( 1 | subject/(condition:replicate) ), data = dataSet, family = binomial )
</code></pre>

<p>and</p>

<pre><code>lmH2a &lt;- lmer( value ~ condition + ( 1 | subject/condition/replicate ), data = dataSet, family = binomial )
</code></pre>

<p>With those models R returns the warning message <code>Number of levels of a grouping factor for the random effects is equal to n, the number of observations</code>. But the model is still computed.</p>

<p>All 4 models return very similar values for the fixed effects and for the random effects that they have in common (e.g. the subject random effects are very similar for all 4 models and the condition within subject random effects are very similar for <code>lmH2</code> and <code>lmH2a</code>).</p>

<p>How can I check which random effect structure is the most appropriate for my design and collected data?</p>
"
"0.180658947792261","0.185673562118557"," 69957","<p>Here's my issue of the day:</p>

<p>At the moment I'm teaching myself Econometrics and making use of logistic regression. I have some SAS code and I want to be sure I understand it well first before trying to convert it to R. (I don't have and I don't know SAS). In this code, I want to model the probability for one person to be an 'unemployed employee'. By this I mean ""age"" between 15 and 64, and ""tact"" = ""jobless"". I want to try to predict this outcome with the following variables: sex, age and idnat (nationality number). (Other things being equal).</p>

<p>SAS code :</p>

<pre><code>/* Unemployment rate : number of unemployment amongst the workforce */
proc logistic data=census;
class sex(ref=""Man"") age idnat(ref=""spanish"") / param=glm;
class tact (ref=first);
model tact = sex age idnat / link=logit;
where 15&lt;=age&lt;=64 and tact in (""Employee"" ""Jobless"");
weight weight;
format age ageC. tact $activity. idnat $nat_dom. inat $nationalty. sex $M_W.;

lsmeans sex / obsmargins ilink;
lsmeans idnat / obsmargins ilink;
lsmeans age / obsmargins ilink;
run;
</code></pre>

<p>This is a sample of what the database should looks like :</p>

<pre><code>      idnat     sex     age  tact      
 [1,] ""english"" ""Woman"" ""42"" ""Employee""
 [2,] ""french""  ""Woman"" ""31"" ""Jobless"" 
 [3,] ""spanish"" ""Woman"" ""19"" ""Employee""
 [4,] ""english"" ""Man""   ""45"" ""Jobless"" 
 [5,] ""english"" ""Man""   ""34"" ""Employee""
 [6,] ""spanish"" ""Woman"" ""25"" ""Employee""
 [7,] ""spanish"" ""Man""   ""39"" ""Jobless"" 
 [8,] ""spanish"" ""Woman"" ""44"" ""Jobless"" 
 [9,] ""spanish"" ""Man""   ""29"" ""Employee""
[10,] ""spanish"" ""Man""   ""62"" ""Retired"" 
[11,] ""spanish"" ""Man""   ""64"" ""Retired"" 
[12,] ""english"" ""Woman"" ""53"" ""Jobless"" 
[13,] ""english"" ""Man""   ""43"" ""Jobless"" 
[14,] ""french""  ""Man""   ""61"" ""Retired"" 
[15,] ""french""  ""Man""   ""50"" ""Employee""
</code></pre>

<p>This is the kind of result I wish to get :</p>

<pre><code>Variable    Modality    Value   ChiSq   Indicator
Sex         Women       56.6%   0.00001 -8.9%
            Men         65.5%       
Nationality 
            1:Spanish   62.6%       
            2:French    51.2%   0.00001 -11.4%
            3:English   48.0%   0.00001 -14.6%
Age 
            &lt;25yo       33.1%   0.00001 -44.9%
        Ref:26&lt;x&lt;54yo   78.0%       
            55yo=&lt;      48.7%   0.00001 -29.3%
</code></pre>

<p>Indicator is P(category)-P(ref)
(I interpret the above as follows: other things being equal, women have -8.9% chance of being employed vs men and those aged less than 25 have a -44.9% chance of being employed than those aged between 26 and 54).</p>

<p>So if I understand well, the best approach would be to use a binary logistic regression (link=logit). This uses references ""male vs female""(sex), ""employee vs jobless""(from 'tact' variable)... I presume 'tact' is automatically converted to a binary (0-1) variable by SAS.</p>

<p>Here is my 1st attempt in R. I haven't check it yet (need my own PC) :</p>

<pre><code>### before using glm function 
### change all predictors to factors and relevel reference
recens$sex &lt;- relevel(factor(recens$sex), ref = ""Man"")
recens$idnat &lt;- relevel(factor(recens$idnat), ref = ""spanish"")  
recens$tact &lt;- relevel(factor(recens$tact), ref = ""Employee"")
recens$ageC &lt;- relevel(factor(recens$ageC), ref = ""Ref : De 26 a 54 ans"")

### Calculations of the probabilities with function glm, 
### formatted variables, and conditions with subset restriction to ""from 15yo to 64""
### and ""employee"" and ""jobless"" only.
glm1 &lt;- glm(activite ~ sex + ageC + idnat, data=recens, weights = weight, 
            subset= recens$age[(15&lt;= recens$age | recens$age &lt;= 64)] 
            &amp; recens$tact %in% c(""Employee"",""Jobless""), 
            family=quasibinomial(""logit""))
</code></pre>

<p>My questions :</p>

<p>For the moment, it seems there are many functions to carry out a logistic regression in R like glm which seems to fit.</p>

<p>However after visiting many forums it seems a lot of people recommend not trying to exactly reproduce SAS PROC LOGISTIC, particularly the function LSMEANS. Dr Franck Harrel, (author of package:rms) for one.</p>

<p>That said, I guess my big issue is LSMEANS and its options Obsmargins and ILINK. Even after reading over its description repeatedly I can hardly understand how it works.</p>

<p>So far, what I understand of Obsmargin is that it respects the structure of the total population of the database (i.e. calculations are done with proportions of the total population). ILINK appears to be used to obtain the predicted probability value (jobless rate, employment rate) for each of the predictors (e.g. female then male) rather than the value found by the (exponential) model?</p>

<p>In short, how could this be done through R, with lrm from rms or lsmeans?</p>

<p>I'm really lost in all of this. If someone could explain it to me better and tell me if I'm on the right track it would make my day.</p>

<p>Thank you for your help and sorry for all the mistakes my English is a bit rusty.</p>

<p>Binh</p>
"
"0.0729972383233908","0.0750234484920533"," 70174","<p>I have three variables, a factor (<code>c</code>) as the dependent variable and two ordinal independent variables (<code>a, b</code>). Each variable has five categories (<code>1,2,3,4,5</code>). Thus, I fitted a multinomial logistic regression (<code>testus</code>, see below) with the <code>car</code> package. Now I would like to get the predicted probabilities. </p>

<pre><code>&gt; a &lt;- sample(5,100,TRUE)
&gt; b &lt;- sample(5,100,TRUE)
&gt; c &lt;- sample(5,100,TRUE)
&gt; required(car)
&gt; a &lt;- as.ordered(a)
&gt; b &lt;- as.ordered(b)
&gt; c &lt;- as.factor(c)
&gt; testus        &lt;- multinom(c~a+b)
&gt; predictors    &lt;- expand.grid(b=c(""1"",""2"",""3"",""4"",""5""), a=c(""1"",""2"",""3"",""4"",""5""))
&gt; p.fit         &lt;- predict(testus, predictors, type='probs')
&gt; probabilities &lt;- data.frame(predictors,p.fit)
</code></pre>

<p>Now I got the predicted probabilities for a under b and c. </p>

<pre><code>&gt; head(probabilities)
  b a         X1         X2         X3         X4        X5
1 1 1 0.10609054 0.22599152 0.20107167 0.21953158 0.2473147
2 2 1 0.20886614 0.27207108 0.08613633 0.18276394 0.2501625
3 3 1 0.17041268 0.24995975 0.16234240 0.13111518 0.2861700
4 4 1 0.23704078 0.21179521 0.08493274 0.03135092 0.4348804
5 5 1 0.09494071 0.09659144 0.24162612 0.21812449 0.3487172
6 1 2 0.14059489 0.17793438 0.29272452 0.26104833 0.1276979`
</code></pre>

<p>The first two columns show the categories of the independent variables <code>a</code> and <code>b</code>. The next five columns show the conditional probabilities (e.g., $P(c=1|b=1~\&amp;~a=1) = 0.10609$. But now I would like to know only the predicted probabilities for <code>c</code> under <code>a</code> or the predicted probabilities for <code>c</code> under <code>b</code>. Is this possible?</p>
"
"0.10323368445272","0.106099178353461"," 71292","<p>I need to compare the goodness of fit of several averaged logistic regression models by calculating the deviance explained. I'm using the <code>MuMIn</code> package in R to average many logistic regression models into a single averaged model. I ultimately want to compare the explanatory power of several averaged models, in part by using the deviance explained.</p>

<p>My questions are:</p>

<ol>
<li><p>Does deviance explained apply to averaged models as a strong measure of the goodness of fit?</p></li>
<li><p>How does one calculate the deviance explained (calculated as the null deviance less the residual deviance as a proportion of the null deviance) from the averaged model output from the <code>model.avg()</code> command in <code>MuMIn</code>? </p>

<p>Examining the structure of the averaged model object indicates that the null and residual deviances are calculated on each individual model that contributes to the averaged model, but I'm not sure how to extract them and then calculate the overall deviance explained by the averaged model.</p></li>
</ol>
"
"0.11150512337989","0.114600210537152"," 71727","<p>In addition to <a href=""http://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_varclus_sect004.htm"">PROC VARCLUS</a>, <a href=""http://cran.r-project.org/web/packages/randomForest/index.html"">randomForest</a>, <a href=""http://cran.r-project.org/web/packages/glmnet/index.html"">glmnet</a>, and assessing multicollinearity among potential predictor variables (without regards to the outcome of interest), I am seeking other methods of variable selection in lieu of using stepwise methods for building more parsimonious binary logistic regression models (containing 8 to 12 variables to predict outcomes such as loan payment/default or current/late payment history) from a wide array of potential predictor variables (500+ variables, 200k+ records). </p>

<p>Below I have included an R script using <a href=""http://cran.r-project.org/web/packages/FSelector/index.html"">FSelector</a> to select the 8 highest ""ranked"" variables:</p>

<pre><code>library(FSelector)
fit &lt;- information.gain(outcome ~ ., dataset)
fit2 &lt;- cutoff.k(fit,8)
reducedmodel &lt;- as.simple.formula(fit2,""outcome"")
print(reducedmodel)
</code></pre>

<p>I have two questions regarding this script and the <code>FSelector</code> algorithm in general:   </p>

<ol>
<li><p>Is the <code>information.gain</code> criteria in the above script synonymous with <a href=""http://en.wikipedia.org/wiki/Kullback-Leibler_divergence"">Kullback-Leibler divergence</a>?
If so, can someone explain this in more layman terms than Wikipedia as I am relatively new to this area of statistics and would like to start off with the right idea of this concept as I may likely use this approach a great deal in the future?</p></li>
<li><p>Is this a valid approach, if there is such a thing as a valid approach, to select a desired number of variables for a binary logistic regression model (e.g., selecting the 8 highest ""ranked"" variables for use in a parsimonious model)? If not, can you provide an alternative approach to do so?</p></li>
</ol>

<p>Any insight or references regarding this topic and/or these questions will be greatly appreciated!</p>
"
"0.042144975196109","0.043314808182421"," 74304","<p>What goodness of fit tests are usually used for quantile regression? Ideally I need something similar to F-test in linear regression, but something like AIC in logistic regression will suite as well. I use quantreg R package, but found only some Khmaladze test in there. To be fair I hardly understand what is does.</p>
"
"0.139779069524328","0.143658966607306"," 76490","<h2>Background</h2>

<p>A laboratory wants to evaluate whether a certain form of <a href=""http://en.wikipedia.org/wiki/Polyacrylamide_gel_electrophoresis"" rel=""nofollow"">gel electrophoresis</a> is suited as a classification method for the quality of a certain substance. Several gels were loaded, each with a clean sample of the substance and with a sample that contains impurities. In addition, a molecular marker was also loaded which serves as a reference. The following picture illustrates the setup (the picture doesn't show the actual experiment, I have taken it from Wikipedia for illustration):</p>

<p><img src=""http://i.stack.imgur.com/vC53q.png"" alt=""Example of a gel electrophoresis""></p>

<p>Two parameters were measured for each gel and each lane:</p>

<ol>
<li>The <strong>molecular weight</strong> (that is how ""high up"" a compound wandered during the electrophoresis)</li>
<li>The <strong>relative quantity.</strong> The total quantity of each lane is normalized to 1 and the density of each band is measured which results in the relative quantity of each band.</li>
</ol>

<p>A scatterplot of the relative quantity vs. molecular weight is then produced which could look something like this (it's artificial data):</p>

<p><img src=""http://i.stack.imgur.com/ndzh9.png"" alt=""Example scatterplot""></p>

<p>This graphic can be read as follows: Both the ""good"" (blue points) and ""impure"" (red points) substance exhibit two bands, one at around a molecular weight of 120 and one at around 165. The bands of the ""impure"" substance at a molecular weight around 120 are considerably less dense than the ""good"" substance and can be well distinguished.</p>

<hr>

<h2>Goal</h2>

<p>The goal is to determine two boxed (see graphic below) which determine a ""good"" substance. These boxes will then be used for classification of the substance in the future into ""good"" and ""impure"". If a substance exhibits lanes that fall within the boxes it is classified as ""good"" and else as ""impure"".</p>

<p>These decision-rules should be <em>simple</em> to apply for someone in the laboratory. That's why it should be boxes instead of curved decision boundaries.</p>

<p>False-negatives (i.e. classify a sample as ""impure"" when it's really ""good"") are considered worse than false-positives. That is, an emphasis should be placed on the <a href=""http://en.wikipedia.org/wiki/Sensitivity_and_specificity#Sensitivity"" rel=""nofollow"">sensitivity</a>, rather than on the <a href=""http://en.wikipedia.org/wiki/Sensitivity_and_specificity#Specificity"" rel=""nofollow"">specificity</a>.</p>

<p><img src=""http://i.stack.imgur.com/vhzEW.png"" alt=""Example decision boxed""></p>

<hr>

<h2>Question</h2>

<p>I'm am no expert in machine learning. I know, however, that there are quite a few machine learning algorithms/techniques that could be helpful: $k$-nearest neighbors (e.g. <code>knn</code> in <code>R</code>), classification trees (e.g. <code>rpart</code> or <code>ctree</code>), support vector machines (<code>ksvm</code>), logistic regression, boosting and bagging methods and many more.</p>

<p>One problem of many of those algorithms is that they don't provide a simple ruleset or linear boundaries. In addition, the <strong>sample size</strong> is around <strong>70.</strong></p>

<p>My questions are:</p>

<ul>
<li>Has anyone an idea of how to proceed here?</li>
<li>Does it make sense to split the dataset into training- and test-set?</li>
<li>What proportion of the data should the training set be (I thought around a 60/40-split).</li>
<li>What, in general, is the workflow for such an analysis? Something like: Splitting dataset -> fit algorithm on the training set -> predict outcome for the test set?</li>
<li>How to avoid overfitting (i.e. boxes that are too small)?</li>
<li>What is a good statistic to assess the predictive performance in this case? AUC? Accurary? Positive predictive value? <a href=""http://en.wikipedia.org/wiki/Matthews_correlation_coefficient"" rel=""nofollow"">Matthews correlation coefficient</a>?</li>
</ul>

<p>Assume that I'm familiar with <code>R</code> and the <code>caret</code> package. Thank you very much for you time and help.</p>

<hr>

<h2>Example data</h2>

<p>Here is an example dataset.</p>

<pre><code>structure(list(mol.wt = c(125.145401455869, 118.210252208676, 
165.048583787746, 126.003687476776, 170.149347112565, 127.761533014759, 
155.523172614798, 120.094514977175, 161.234986765321, 168.471542655269, 
156.522990530521, 154.377948321209, 165.365756398877, 167.965538771316, 
116.132241687833, 115.143539160903, 156.696830822196, 162.578494491556, 
136.830624758899, 123.886594633942, 124.247484227948, 126.257226352824, 
160.684010454816, 166.618872115047, 126.599387146887, 165.690375912529, 
159.786861142652, 114.520735974329, 125.753594471656, 157.551537154148, 
157.320636890647, 171.5759136115, 158.580005438661, 125.647463565197, 
130.404710783509, 127.128218318572, 162.144126888907, 161.804616951055, 
167.917268243627, 168.582197247178), rel.qtd = c(57.68339235957, 
54.0514508510085, 25.0703901938793, 37.6933881305906, 36.6853653723001, 
53.6650555524679, 52.268438087776, 52.8621831466857, 43.1242291166037, 
46.6771236380788, 38.0328239221277, 40.0454611708371, 44.6406366176158, 
40.8238699987682, 51.9464749018547, 54.0302533272953, 37.9792331383524, 
48.3853988095525, 38.2093977349102, 42.2636098418388, 42.9876895407144, 
40.8018728193786, 40.1097096927465, 38.7432550253867, 39.2633283608111, 
43.4673723102812, 53.3740718733815, 49.1067921475768, 52.3002598744634, 
44.9847844953241, 44.3014423068017, 44.0191971364465, 47.0805245356855, 
55.0124134796556, 57.9938440244052, 62.8314454977068, 45.8093815891894, 
43.2300677500964, 39.4801550161538, 51.6253515591173), quality = structure(c(2L, 
2L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 1L), .Label = c(""bad"", ""good""), class = ""factor"")), .Names = c(""mol.wt"", 
""rel.qtd"", ""quality""), row.names = c(10L, 14L, 47L, 16L, 57L, 
54L, 45L, 12L, 43L, 67L, 25L, 21L, 1L, 55L, 20L, 22L, 37L, 15L, 
8L, 38L, 46L, 64L, 51L, 65L, 52L, 61L, 63L, 32L, 50L, 27L, 19L, 
69L, 23L, 42L, 6L, 48L, 11L, 13L, 5L, 71L), class = ""data.frame"")
</code></pre>
"
"0.059601995508215","0.0612563891831689"," 76850","<p>I am trying to do a multinomial logistic regression on some data that I generated. I am using R and the package mlogit. My data looks like the following:</p>

<pre><code>Class X1 X2  X3
V +0.0655197 +0.6418541 +1.8110291
V-0.6713268 -0.0262458 -0.3602958
V +0.2357610 -0.3602958 -0.6943458
M +0.3900129 +0.5583416 -1.7800082
M +0.5714871 -0.2767833 +0.5583416
M +1.0732807 -1.7800082 -0.3602958
S +0.9553640 -0.3602958 +0.6418541
S +0.1139899 +0.1356030 +0.3889280
S +0.4717283 -0.2852090 -1.1229880
</code></pre>

<p>My model is</p>

<pre><code>Class = B1*X1 + B2*X2 + B3*X3
</code></pre>

<p>So far, I have:</p>

<pre><code>library(mlogit);
allData &lt;- read.table(""Features/AllFeatures.dat"", header=TRUE);
allData$Class&lt;-as.factor(allData$Class);
mlData&lt;-mlogit.data(allData, choice=""Class"");
myData&lt;- mlogit(Class~1|X1 + X2 + x3, data = mlData);
print(summary(myData));
</code></pre>

<p>Which gives me:</p>

<pre><code>Coefficients :
                           Estimate Std. Error t-value  Pr(&gt;|t|)    
S:(intercept)                -0.6832392  0.0951834 -7.1781 7.068e-13 ***
V:(intercept)                -0.6696254  0.0943282 -7.0989 1.258e-12 ***
S:X1                         -0.1362492  0.1134039 -1.2015    0.2296    
V:X1                         -0.0052649  0.1128722 -0.0466    0.9628    
S:X2                         -0.0198451  0.0973608 -0.2038    0.8385    
V:X2                          0.0183261  0.0974789  0.1880    0.8509    
S:X3                          0.1728694  0.1110473  1.5567    0.1195    
V:X3                          0.0230260  0.1101147  0.2091    0.8344
</code></pre>

<p>However in the following: <a href=""http://cran.r-project.org/web/packages/mlogit/vignettes/Exercises.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/mlogit/vignettes/Exercises.pdf</a></p>

<p>The author gets:</p>

<pre><code>Coefficients :
Estimate Std. Error t-value Pr(&gt;|t|)
ic -0.00623187 0.00035277 -17.665 &lt; 2.2e-16 ***
oc -0.00458008 0.00032216 -14.217 &lt; 2.2e-16 ***
</code></pre>

<p>How can I change my function call to get the same? I want the actual coefficients for the model, not for the comparisons.</p>

<p>Also: What is the difference between 'wide' and 'long'? What form is my data in?</p>

<p>Also: Do you have some mathematical references for this type of multinomial logistic regression aimed at engineers?</p>

<p>Thanks!</p>
"
"0.11150512337989","0.114600210537152"," 77245","<p>To better ask my question, I have provided some of the outputs from both a 16 variable model (<code>fit</code>) and a 17 variable model (<code>fit2</code>) below (all predictor variables in these models are continuous, where the only difference between these models is that <code>fit</code> does not contain variable 17 (var17)):</p>

<pre><code>fit                    Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
 Obs        102849    LR chi2   13602.84    R2       0.173    C       0.703    
  0          69833    d.f.            17    g        1.150    Dxy     0.407    
  1          33016    Pr(&gt; chi2) &lt;0.0001    gr       3.160    gamma   0.416    
 max |deriv| 3e-05                          gp       0.180    tau-a   0.177    
                                            Brier    0.190       


fit2                 Model Likelihood       Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
 Obs        102849    LR chi2   13639.70    R2       0.174    C       0.703    
  0          69833    d.f.            18    g        1.154    Dxy     0.407    
  1          33016    Pr(&gt; chi2) &lt;0.0001    gr       3.170    gamma   0.412    
 max |deriv| 3e-05                          gp       0.180    tau-a   0.177    
                                            Brier    0.190          
</code></pre>

<p>I used Frank Harrell's <code>rms</code> package to build these <code>lrm</code> models. As you can see, these models do not appear to vary much, if at all, across <em>Discrimination Indexes</em> and <em>Rank Discrim. Indexes</em>; however, using <code>lrtest(fit,fit2)</code>, I was provided with the following results:</p>

<pre><code> L.R. Chisq         d.f.            P 
3.685374e+01     1.000000e+00    1.273315e-09 
</code></pre>

<p>As such, we would reject the null hypothesis of this likelihood ratio test; however, I would assume this is likely due to the large sample size (<em>n</em> = 102849) as these models appear to perform in a similar fashion. Furthermore, I am interested in finding a better way of formally comparing nested binary logistic regression models when <em>n</em> is large.</p>

<p>I greatly appreciate any feedback, R scripts, or documentation that can steer me in the right direction in terms of comparing these types of nested models! Thanks!</p>
"
"0.133274113551006","0.136973450269748"," 81612","<p>Recently I was trying to do logistic regression using the <code>rms::lrm()</code> function. But I had some trouble understanding the model objects from the function. Here is the example from the package:</p>

<pre><code>#dataset
n            &lt;- 1000    # define sample size
set.seed(17)            # so can reproduce the results
treat        &lt;- factor(sample(c('a','b','c'), n,TRUE))
num.diseases &lt;- sample(0:4, n,TRUE)
age          &lt;- rnorm(n, 50, 10)
cholesterol  &lt;- rnorm(n, 200, 25)
weight       &lt;- rnorm(n, 150, 20)
sex          &lt;- factor(sample(c('female','male'), n,TRUE))
L            &lt;- .1*(num.diseases-2) + .045*(age-50) +
                (log(cholesterol - 10)-5.2)*(-2*(treat=='a') +
                3.5*(treat=='b')+2*(treat=='c'))
y            &lt;- ifelse(runif(n) &lt; plogis(L), 1, 0)
#fit model
g            &lt;- lrm(y ~ treat*rcs(age))

&gt; g

Logistic Regression Model

lrm(formula = y ~ treat * rcs(age))

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          1000    LR chi2      76.77    R2       0.099    C       0.656    
 0            478    d.f.            14    g        0.665    Dxy     0.312    
 1            522    Pr(&gt; chi2) &lt;0.0001    gr       1.945    gamma   0.314    
max |deriv| 3e-06                          gp       0.156    tau-a   0.156    
                                           Brier    0.231    
&gt; anova(g)
                Wald Statistics          Response: y 

 Factor                                     Chi-Square d.f. P     
 treat  (Factor+Higher Order Factors)        5.62      10   0.8462
  All Interactions                           1.30       8   0.9956
 age  (Factor+Higher Order Factors)         65.99      12   &lt;.0001
  All Interactions                           1.30       8   0.9956
  Nonlinear (Factor+Higher Order Factors)    2.23       9   0.9872
 treat * age  (Factor+Higher Order Factors)  1.30       8   0.9956
  Nonlinear                                  0.99       6   0.9858
  Nonlinear Interaction : f(A,B) vs. AB      0.99       6   0.9858
 TOTAL NONLINEAR                             2.23       9   0.9872
 TOTAL NONLINEAR + INTERACTION               2.57      11   0.9953
 TOTAL                                      69.06      14   &lt;.0001
</code></pre>

<p><strong>Here are my questions:</strong><br>
For the object <code>g</code>,  </p>

<ul>
<li>What does the <code>max |deriv| 3e-06</code> mean?  </li>
<li>What do the Discrimination and Rand Discrim. Indexes suggest?  </li>
</ul>

<p>For the <code>anova(g)</code> object,  </p>

<ul>
<li>What's the <code>Factor +Higher Order Factors</code> for the treat?  </li>
<li>Why there are two <code>all interactions</code>? How to explain the nonlinear parts?</li>
</ul>
"
"0.0842899503922179","0.086629616364842"," 82981","<p>I am exploring hierarchical logistic regression, using glmer from the lme4 package. To my understanding, one of the first steps in multilevel modeling is to estimate the degree of clustering of level-1 units within level-2 units, given by the intraclass correlation (to ""justify"" the additional cost of estimating parameters to account for the clustering). When I run a fully unconditional model with glmer</p>

<pre><code>fitMLnull &lt;- glmer(outcome ~ 1 + (1|level2.ID), family=binomial)
</code></pre>

<p>the glmer fit gives me the variance in the intercept for outcome at level-2, but the residual level-1 variance in outcome is nowhere to be found. I read somewhere (can't track it down now) that the residual level-1 variance is <em>not</em> estimated in HGLM (or at least in glmer). Is this true? If so, is there an alternative way to approximate the degree of clustering in the data? If not, how can I access this value to calculate the ICC?</p>
"
"0.0942390294485422","0.0968548555282575"," 83260","<p>I used the clogit function (from the survival package) to run a conditional logistic regression in R with a big dataset of 1:M matched pairs with n=300368964 and number of events= 39995.</p>

<pre><code>model &lt;- clogit(Alliance ~ OVB + CVC + BVB + strata(Strata), method=""exact"")    
</code></pre>

<p>I received following results:</p>

<pre><code>                 coef  exp(coef)   se(coef)       z Pr(&gt;|z|)    
OVB        -0.0498174  0.9514031  0.0166275  -2.996  0.00273 ** 
BVB         0.0277405  1.0281289  0.0304956   0.910  0.36300    
CVC         1.1709851  3.2251683  0.1089709  10.746  &lt; 2e-16 ***
EarlyStage -1.3215824  0.2667129  0.0205851 -64.201  &lt; 2e-16 ***
AvgVCSize   0.0087976  1.0088364  0.0002035  43.224  &lt; 2e-16 ***
NumberVC    0.0643579  1.0664740  0.0034502  18.653  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Rsquare= 0   (max possible= 0.001 )
Likelihood ratio test= 6511  on 6 df,   p=0
Wald test            = 6471  on 6 df,   p=0
Score (logrank) test = 6801  on 6 df,   p=0
</code></pre>

<p>Since Rsquare equals 0 and the test ratios seems very high, I tried to plot the results to check whether the model fits. But I wasn't able to plot it properly.</p>

<p>I would online many papers which use the ratio Prob > chi2 = 0 from Stata as test ratio to proof the model fit. </p>

<p>How could I calculate this ratio in R? Are there any other ways I could check the model fit of my clogit results?</p>

<p>I would appreciate any help.</p>

<p>Thanks you very much in advance.</p>
"
"0.042144975196109","0.043314808182421"," 83364","<p>I have been reading a number of papers where researchers have created risk scores based on logistic regression models. Often they refer to ""<a href=""http://www.ncbi.nlm.nih.gov/pubmed/15122742"" rel=""nofollow"">Sullivan's method</a>"" but I have no access to this paper and the explanations provided are far from clear. I noticed that Dr. Harrell's excellent RMS package provides a nomogram function which is in a way similar to creating a risk score (albeit with a very pretty graphical output).</p>

<p>It seems after tinkering around with it that the way it works is by dividing the beta coefficients by the smallest beta coefficient and then multiplying a constant to create points for categorical variables. However I cannot for the life of me figure out what is going on with continuous variables. I've spent hours searching google without much luck, and I would appreciate if someone could shed some light on this for me. Thanks!</p>
"
"0.042144975196109","0.043314808182421"," 83852","<p>I have a dataset for vehicles and trying to predict what will fail. Here is my data set</p>

<pre><code>vId, MileageSincePartLastReplaced,  AgeOfPart, TypeOfVehicle, Failure

x,100000,200days,Truck,Alt Belt

y,200000,600days,PCar,Transmission Belt

z,140000,230days,Van,Fan Belt
</code></pre>

<p>Failure is outcome variable with 20 different types of failure categories. What I am looking for is given mileage driven and age of the part which one is likely to fail?</p>

<p>I am unable to figure out which model/models I should be looking for? I was looking into multinomial regression &amp; ordered logistic regression  but was not sure. Any help around how to go about this and which package I could use?  </p>

<p>Note: I asked same question in Stack Overflow and was suggested to move to this forum.</p>
"
"0.0842899503922179","0.086629616364842"," 85555","<p>I would like to run a lagged random effects regression.</p>

<p>The data is from an experiment in which participants were assigned to groups of five and participated in an interactive game for 20 rounds.</p>

<p>Participants could exchange something during the experiment, which is the dependent variable.</p>

<p>Now I would like to predict/explain, how much participant received from other participants based on the behaviour of previous rounds.</p>

<p>Since the data is clustered on three levels: subject, group and time (rounds), I am a little bit lost how to correctly formulate the model.</p>

<p>I am currently using the lme4 package in R. 
I transformed the dependent variable to a 0/1 (nothing received/something received) variable, due to high skewness, so I would need to specify a multilevel logistic model.</p>

<p>So far, I specified and ran the following models:</p>

<pre><code>glmer(DV ~ predictors* + (1 + round * subject | group), family = binomial)
</code></pre>

<p>and:</p>

<pre><code>glmer(DV ~ predictors* + (1 + round * group | subject), family = binomial)
</code></pre>

<p>*predictors are on subject-level.</p>

<p>I get similar (although not the same) estimates for both models, however in model1, z-values are much higher (and therefore p-values much lower).</p>

<p>Can someone help me on that?</p>

<p>What I want to know is; Can previous behaviour (that is behaviour from round x-1 etc.) predict how much a participant received in round x.
But control/acknowledge that participants are clustered in groups and that behaviour is correlated over time (rounds).</p>
"
"0.188478058897084","0.184024225503689"," 87359","<p><strong>Background</strong></p>

<p>I have a large dataset that contains three binary outcomes for individuals belonging to groups. I am interested in jointly modeling these binary outcomes because I have reason to believe they are positively correlated with one another. Most of my data is at the individual level, however I also have some group-level information.</p>

<p>Because of the structure of my data, I am treating this as a 3-level logistic regression. The first level defines the multivariate structure through dummy variables that indicate for each outcome. Therefore level-1 accounts for the within-individual measurements. Level-2 provides the between-individuals variances and level-3 gives the between-group variance.</p>

<p><em>Hypothetical Example:</em></p>

<p>Suppose I have data for students in classrooms. I want to examine whether certain student level characteristics are important predictors for passing three different pre-tests (math, history, and gym). The pre-tests are constructed such that about half of the students should pass each exam (no floor or ceiling effect). Since some students are better than others, I expect whether or not they pass their history exam to be correlated to their probability of passing their math and gym pre-tests. I also expect that students in the same classroom will perform more similarly than students across classrooms.</p>

<p><strong>Here is my attempt at writing out the model</strong></p>

<p>I use $h$ to index level-1, $i$ to index level-2, and $j$ to index level-3. Recall that level-1 corresponds to within-individual, level-2 corresponds to between-individuals, and level-3 corresponds to between-groups. So I have $h$ measures for the $i^\text{th}$ individual in the $j^\text{th}$ group.</p>

<p>Let 
\begin{align}
\alpha_{1ij} &amp;= 1 \text{ if outcome}_1 = 1 \text{ and 0 otherwise} \\
\alpha_{2ij} &amp;= 1 \text{ if outcome}_2 = 1 \text{ and 0 otherwise} \\
\alpha_{3ij} &amp;= 1 \text{ if outcome}_3 = 1 \text{ and 0 otherwise}
\end{align}</p>

<p>\begin{align}
\text{log}\left( \frac{\pi_{hij}}{1 - \pi_{hij}} \right) &amp;= \alpha_{0hij} + \alpha_{1hij}Z_{ij} + \eta_h \\
Z_{ij} &amp;= \beta_{0ij} + X_{ij}\beta_{ij} + U_{j} + \epsilon_i \\
U_{j} &amp;= \gamma_{0j} + X_{j}\gamma_{j}
 + \rho_j
\end{align}</p>

<p>Please leave suggests about this notation; I am not positive that it is correct.</p>

<p><strong>Trying to specify the model in R</strong></p>

<p>I have been using R 3.0.1, but am open to solutions using other standard software (e.g. SAS, Stata, WinBUGs, etc.). Since I am modeling a binary response I am using the <code>glmer</code> function in the <code>lme4</code> package.</p>

<p>My data is in a long format with one row per outcome per individual per group.</p>

<p>One of my current problems is correctly specifying a 3-level model. Given 5 individual-level measures and one group level measure, I have tried to specify the model as:</p>

<pre><code>&gt; glmer(pi ~ outcome1:(x1 + x2 + x3 + x4 + u5) + 
             outcome2:(x1 + x2 + x3 + x4 + u5) +
             outcome3:(x1 + x2 + x3 + x4 + u5) +
             (1 + x1 + x2 + x3 + x4 | individual) +
             (1 + u5 | group), family=binomial, data=pretest)
</code></pre>

<p>but this often produces warnings and does not converge.</p>

<p><strong>My questions</strong></p>

<ol>
<li><p>Does my approach make sense? (i.e. does it make sense to model a multilevel multivariate model by specifying the multivariate structure in the first level?)</p></li>
<li><p>I have difficult with the proper notation and appreciate suggestions for clarifying my notation.</p></li>
<li><p>Am I using the right tools for this problem? Should I be using other packages or software?</p></li>
</ol>

<p>Thanks in advance for your suggestions and advice.</p>
"
"0.145994476646782","0.150046896984107"," 87650","<p>I ran into (what I think is) an inconsistency when running a random-intercept model (using the <em>lmer</em> function in the <em>lme4</em> package in R).</p>

<p>Here is what I do: I first run a model with a set of covariates; then I run the same model re-scaling (linearly transforming) one of the regressors. To my knowledge, this should change <strong>only the coefficient</strong> of the variable that is linearly transformed. And indeed, this is what happens when I run this ""experiment"" with a simple linear regression model and with a logistic model.</p>

<p>This code replicates the ""normal"" behaviour:</p>

<pre><code># Create three random independent variables
x1 &lt;- rnorm(20)
x2 &lt;- rnorm(20)
x3 &lt;- as.factor(sample(0:2, 20, replace = TRUE))
# Their random coefficients
coef1 &lt;- runif(1, -1, 1)
coef2 &lt;- runif(1, -1, 1)
# Create a continuous dependent variable and a binomial one
y1 &lt;- coef1 * x1 + coef2 * x2 + runif(20)
y2 &lt;- y1
y2[which(y1 &gt; quantile(y1, 0.5))] &lt;- 1
y2[which(y1 &lt;= quantile(y1, 0.5))] &lt;- 0
# Finally, a linear transformation of x1
x1.trans &lt;- x1*3
</code></pre>

<p>So, let us run an OLS model:</p>

<pre><code>lm &lt;- lm(y1 ~ x1 + x2 + x3)
summary(lm)
# OLS model with one variable linearly transformed
lm.bis &lt;- lm(y1 ~ x1.trans + x2 + x3)
summary(lm.bis)
</code></pre>

<p>The coefficients of <em>x1</em> and <em>x1.trans</em> are different, <strong>but the R-square of the two models is the same</strong>:</p>

<pre><code>summary(lm)$r.sq == summary(lm.bis)$r.sq
</code></pre>

<p>The same with a logistic model:</p>

<pre><code>logm &lt;- glm(y2 ~ x1 + x2, family=""binomial"")
summary(logm)
logm.bis &lt;- glm(y2 ~ x1.trans + x2, family=""binomial"")
summary(logm.bis)
</code></pre>

<p>Even in this case, <strong>the log-likelihood of the two models is the same</strong>:</p>

<pre><code>logLik(logm) == logLik(logm.bis)
</code></pre>

<p>So far, so good. However, when I do the same with a hierarchical model, <strong>the log-likelihood (and consequently the AIC and BIC) of the two models are different</strong>, although the coefficient of the transformed variable remains significant with the same z value and the other coefficients are the same.</p>

<pre><code># Multilevel model
mm &lt;- lmer(y1 ~ x1 + x2 + (1 | x3))
summary(mm)
mm.bis &lt;- lmer(y1 ~ x1.trans + x2 + (1 | x3))
summary(mm.bis)
logLik(mm) == logLik(mm.bis) ### FALSE! ###
</code></pre>

<p>Why? Also the ""REML criterion at convergence"" is obviously different. I don't understand this result. This is probably due to my moderate knowledge of the math of hierarchical models. I'd be very happy if some of you could show me what's the trick here.</p>

<p>Since we then use AIC and BIC to compare models, I am puzzled by the fact that a simple transformation that shouldn't change anything makes one model better (or worse) than another.</p>
"
"0.0842899503922179","0.086629616364842"," 87956","<p>I have a repeated-measures experiment where the dependent variable is a percentage, and I have multiple factors as independent variables. I'd like to use <code>glmer</code> from the R package <code>lme4</code> to treat it as a logistic regression problem (by specifying <code>family=binomial</code>) since it seems to accommodate this setup directly.</p>

<p>My data looks like this:</p>

<pre><code> &gt; head(data.xvsy)
   foldnum      featureset noisered pooldur dpoolmode       auc
 1       0         mfcc-ms      nr0       1      mean 0.6760438
 2       1         mfcc-ms      nr0       1      mean 0.6739482
 3       0    melspec-maxp    nr075       1       max 0.8141421
 4       1    melspec-maxp    nr075       1       max 0.7822994
 5       0 chrmpeak-tpor1d    nr075       1       max 0.6547476
 6       1 chrmpeak-tpor1d    nr075       1       max 0.6699825
</code></pre>

<p>and here's the R command that I was hoping would be appropriate:</p>

<pre><code> glmer(auc~1+featureset*noisered*pooldur*dpoolmode+(1|foldnum), data.xvsy, family=binomial)
</code></pre>

<p>The problem with this is that the command complains about my dependent variable not being integers:</p>

<pre><code>In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>and the analysis of this (pilot) data gives weird answers as a result.</p>

<p>I understand why the <code>binomial</code> family expects integers (yes-no counts), but it seems it should be OK to regress percentage data directly. How to do this?</p>
"
"0.042144975196109","0.043314808182421"," 88388","<p>I would like to use cross-validation to test how predictive my mixed-effect logistic regression model is (model run with glmer). Is there an easy way to do this using a package in R? I've only seen cross validation functions in R for use with linear models.</p>
"
"0.042144975196109","0.043314808182421"," 89130","<p>Consider this logistic regression:</p>

<pre><code>mtcars$vs &lt;- as.factor(mtcars$vs)
log_reg_mtcars &lt;- glm(am ~ vs*wt +vs*mpg, family = ""binomial"", mtcars)
</code></pre>

<p>I tried using the effects package to extract some coefficients from the model:</p>

<pre><code>as.data.frame(effect(""vs"", log_reg_mtcars)) 

NOTE: vs is not a high-order term in the model
  vs       fit       se       lower     upper
1  0 0.3957869 2.916877 0.002150238 0.9950031
2  1 0.0336822 2.181009 0.000484832 0.7146704
</code></pre>

<p>Could anyone explain why the coefficients/standard error for <code>vs</code> given by this code are different. Or in other words, what is the effects package doing?</p>

<pre><code>as.data.frame(summary(log_reg_mtcars)$coef)[2,1:2]

     Estimate Std. Error
vs1 -40.70047   40.53197
</code></pre>
"
"0.042144975196109","0.043314808182421"," 93202","<p>I  am using a  decision tree and random forest for a classification problem.
The output is  binary {0,1} and some of the input variables are categorical while the others are continuous. </p>

<p>I would like to know if it is possible to extract odds ratio from one of these models. Odds ratio similar to the ones that are commonly obtained using logistic regression.</p>

<p>I am using R with the package <code>rpart</code> and <code>randomforest</code> so the best answer would be a theoretical explanation and an R code. </p>

<p>Thanks!</p>
"
"0.042144975196109","0.043314808182421"," 93708","<p>Suppose I have a data frame such that:</p>

<pre><code>set.seed(2014)
df&lt;-data.frame(y=rbinom(100,1,0.3),futime=as.integer(rnorm(100,100,10)),
     age=rnorm(100,50,5),sex=rbinom(100,1,0.5))
df[df&gt;100]&lt;-100
</code></pre>

<p>where <code>y</code> is the outcomes and <code>futime&lt;100</code> are right censored of follow up time. I would fit a logistic regression model to predict the outcomes as if they are not censored. I saw there are some options such as <code>tobit</code> model, <code>survival</code> model and <code>censReg</code> package, but seems not very similar situation. Would somebody know the best way for this using <code>age</code> and <code>sex</code> as predictors? </p>
"
"0.133274113551006","0.123276105242774"," 94089","<p>I started to use the function <code>multinom</code> of <code>R</code> package <code>nnet</code> in order to fit several conditional probability distributions with the multinomial logistic model. I need the parameters of the fittings in order to pass them to a Java program, which will compute the probabilities and use them.</p>

<p>My problem is that the probabilities computed with the parameters returned by <code>multinom</code>, following the usual <a href=""http://en.wikipedia.org/wiki/Multinomial_logistic_regression#As_a_set_of_independent_binary_regressions"" rel=""nofollow"">definition</a> of multinomial logistic model, are not the same as those directly computed in <code>R</code>, which are the correct ones. On Stack Overflow I have already asked a <a href=""http://stackoverflow.com/questions/22905807/how-does-the-function-multinom-from-r-package-nnet-compute-the-multinomial-proba"">question</a> about this issue, but I do not still know how the <code>R</code> function <code>multinom</code> computes these probabilities; my guess is that it relies on neural networks, since this function belongs to <code>R</code> package <code>nnet</code>, but I do not have any idea about the details, and an inspection of the code led to nowhere.</p>

<p>Do you know an <code>R</code> package which fits conditional probabilities and returns the corresponding parameters of the model, so that we may easily compute the probabilities in another program? E.g., using the MARS model (<code>R</code> package <code>earth</code>) or Projection Pursuit Regression (<code>R</code> package <code>ppr</code>) is not feasible, since computing the probabilities from the parameters of these models would be a mess. Besides, the function <code>mlogit</code> from the <code>R</code> package with the same name is not applicable as well, since the dataset should be in a certain format (we would also need the predictors corresponding to the alternative, ""non-chosen"" response variable).</p>
"
"0.151955869072729","0.156173761888606"," 94581","<p>I have a ordinal dependendent variable, easiness, that ranges from 1 (not easy) to 5 (very easy).  Increases in the values of the independent factors are associated with an increased easiness rating.</p>

<p>Two of my independent variables (<code>condA</code> and <code>condB</code>) are categorical, each with 2 levels, and 2 (<code>abilityA</code>, <code>abilityB</code>) are continuous.</p>

<p>I'm using the <a href=""http://cran.r-project.org/web/packages/ordinal/index.html"">ordinal</a> package in R, where it uses what I believe to be</p>

<p>$$\text{logit}(p(Y \leqslant g)) = \ln \frac{p(Y \leqslant g)}{p(Y &gt; g)} = \beta_{0_g} - (\beta_{1} X_{1} + \dots + \beta_{p} X_{p}) \quad(g = 1, \ldots, k-1)$$<br>
(from @caracal's answer <a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">here</a>)</p>

<p>I've been learning this independently and would appreciate any help possible as I'm still struggling with it.  In addition to the tutorials accompanying the ordinal package, I've also found the following to be helpful: </p>

<ul>
<li><a href=""http://stats.stackexchange.com/questions/89474/interpretation-of-ordinal-logistic-regression"">Interpretation of ordinal logistic regression</a></li>
<li><a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">Negative coefficient in ordered logistic regression</a></li>
</ul>

<p>But I'm trying to interpret the results, and put the different resources together and am getting stuck. </p>

<ol>
<li><p>I've read many different explanations, both abstract and applied, but am still having a hard time wrapping my mind around what it means to say: </p>

<blockquote>
  <p>With a 1 unit increase in condB (i.e., changing from one level to the next of the categorical predictor), the predicted odds of observing Y = 5 versus Y = 1 to 4 (as well as the predicted odds of observed Y = 4 versus Y = 1 to 3) change by a factor of exp(beta) which, for diagram, is exp(0.457) = 1.58. </p>
</blockquote>

<p>a. Is this different for the categorical vs. continuous independent variables?<br>
b. Part of my difficulty may be with the cumulative odds idea and those comparisons. ... Is it fair to say that going from condA = absent (reference level) to condA = present is 1.58 times more likely to be rated at a higher level of easiness?  I'm pretty sure that is NOT correct, but I'm not sure how to correctly state it.</p></li>
</ol>

<p>Graphically,<br>
1. Implementing the code in <a href=""http://stats.stackexchange.com/questions/89474/interpretation-of-ordinal-logistic-regression"">this post</a>, I'm confused as to why the resulting 'probability' values are so large.<br>
2. The graph of p (Y = g) in <a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">this post</a> makes the most sense to me ... with an interpretation of the probability of observing a particular category of Y at a particular value of X.  The reason I am trying to get the graph in the first place is to get a better understanding of the results overall.</p>

<p>Here's the output from my model:</p>

<pre><code>m1c2 &lt;- clmm (easiness ~ condA + condB + abilityA + abilityB + (1|content) + (1|ID), 
              data = d, na.action = na.omit)
summary(m1c2)
Cumulative Link Mixed Model fitted with the Laplace approximation

formula: 
easiness ~ illus2 + dx2 + abilEM_obli + valueEM_obli + (1 | content) +  (1 | ID)
data:    d

link  threshold nobs logLik  AIC    niter     max.grad
logit flexible  366  -468.44 956.88 729(3615) 4.36e-04
cond.H 
4.5e+01

Random effects:
 Groups  Name        Variance Std.Dev.
 ID      (Intercept) 2.90     1.70    
 content  (Intercept) 0.24     0.49    
Number of groups:  ID 92,  content 4 

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)    
condA              0.681      0.213    3.20   0.0014 ** 
condB              0.457      0.211    2.17   0.0303 *  
abilityA           1.148      0.255    4.51  6.5e-06 ***
abilityB           0.577      0.247    2.34   0.0195 *  

Threshold coefficients:
    Estimate Std. Error z value
1|2   -3.500      0.438   -7.99
2|3   -1.545      0.378   -4.08
3|4    0.193      0.366    0.53
4|5    2.121      0.385    5.50
</code></pre>
"
"NaN","NaN"," 95203","<p>I need to perform stepwise binary logistic regression (The horror! The horror!) on 1.5 million observations.  This takes far too long in SAS, so I'm wondering if I can use R to process it in a multicore environment.  Apparently package gmulti (<a href=""http://www.jstatsoft.org/v34/i12/paper"" rel=""nofollow"">http://www.jstatsoft.org/v34/i12/paper</a>) will do the trick, but it's not clear to me if it will do that outside of its genetic algorithm.  That still might work for me, but I don't have a large number of variables (about 30) so it's not necessary.  As long as the results of the brute force and ga approach could be assured to be similar, then I might try it.  However, I see others have had problems getting the parallel feature to run: <a href=""https://stat.ethz.ch/pipermail/r-help/2013-April/351820.html"" rel=""nofollow"">https://stat.ethz.ch/pipermail/r-help/2013-April/351820.html</a>.  Any other suggestions 
on how to parallelize logistic regression in R?  A web search turned up a couple of papers, but not much that seemed specific to R.  And please spare me a lecture about stepwise regression-I'm very well aware of the pitfalls.  I'm replicating someone else's analysis.  I'm using a Windows 64 bit system.</p>
"
"0.042144975196109","0"," 95425","<p>I am trying to calculate ePCP and Brier scores in R for a mixed-effect binary logistic regression. It cannot seem to find any packages that work for mixed models. I have tried the packages OOmisc and ModelGood, and spent much time searching the web with no success.</p>

<p>I am using the lme4 package to fit the regression.</p>

<p>I would be very grateful for any tips.</p>
"
"0.0942390294485422","0.0968548555282575"," 96999","<p>I'm working on a model that requires me to look for predictors for a rare event (less than 0.5% of the total of my observations). My total sample is a significant part of the total population (50,000 cases). My final objective is to obtain comparable probability values for all the non-events, without the bias of the groups difference in the logistic regression. </p>

<p>I've been reading the info in the following link: </p>

<p><a href=""http://gking.harvard.edu/files/gking/files/0s.pdf"" rel=""nofollow"">http://gking.harvard.edu/files/gking/files/0s.pdf</a></p>

<p>It advises me first to use a sample of my original sample, containing all the events (1) and a random sample of 1-5 times bigger of the non-event (0) sample.</p>

<p>Then it suggests using weights based on the proportion of the sample 1s to 0s. In the section 4.2 of the linked text, he offers a ""easy to implement"" weighted log-likelihood that can be implemented in any logit function.</p>

<p>I wish to implement these weights somehow with R's glm(...,family=binomial(link=""logit"")) or similar function ( the ""weights"" parameter is not for frequency weighting), but I don't really know how to apply this weighting.</p>

<p>Does anybody knows how to make it or any other alternative suggestion?  </p>

<p><strong>Edit1:</strong> As suggested bellow, is Firth's method for bias-correction by penalizing the likelihood in the <code>logistf</code> package a correct approach in this case? I'm not much knowledgeable in statistics, and, while I understand the input and the coefficients/output of the logistic model, what happens in between is still quite a mystery to me, sorry.</p>
"
"0.10323368445272","0.106099178353461"," 99254","<p>I have a large set of time series data, consisting of series from two different conditions, the averages of which are shown below.</p>

<p>I would like to fit a model to this data, to test that a) the peak value is greater in the 'conflict' condition, and b) the peak occurs earlier in this condition.</p>

<pre><code>ggplot(data, aes(x=Time, y=Variable, colour=Condition)) 
    + stat_summary(fun.data=mean_se, geom=""pointrange"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/3NQ8T.png"" alt=""enter image description here""></p>

<p>From my own research on this, I know that:</p>

<ul>
<li>Growth curve analysis is the usual way of modelling time series data like this, but I can't figure out how I would fit a polynomial for this shape of curve.
<ul>
<li>I have some experience fitting GCA models using <code>lme4</code> in R, mostly following <a href=""http://www.danmirman.org/gca"" rel=""nofollow"">Dan Mirman's tutorials</a>, but I'm still learning.</li>
</ul></li>
<li>Curves of this kind are referred to as Hubbert curves, and typically used to model oil production, as a symmetric logistic curve up and down.</li>
<li>R package <code>grofit</code> is maybe useful for analyses of this kind, although I would rather know I'm barking up the right tree before investing time in learning how to use this.</li>
</ul>

<p>Can anyone point me in the right direction here?</p>
"
"0.0729972383233908","0.0750234484920533","101003","<p>I have a data set of around 5000 features. For that data I first used Chi Square test for feature selection; after that, I got around 1500 variables which showed significance relationship with the response variable. </p>

<p>Now I need to fit logistic regression on that. I am using glmulti package for R (glmulti package provides efficient subset selection for vlm) but it can use only 30 features at a time, else its performance goes down as the number of rows in my dataset is around 20000.</p>

<p>Is there any other approach or techniques to solve the above problems? If I go by the above method it will take too much time to fit the model.</p>
"
"0.119946702195905","0.136973450269748","102892","<p>My situation:</p>

<ul>
<li>small sample size: 116</li>
<li>binary outcome variable</li>
<li>long list of explanatory variables: 44</li>
<li>explanatory variables did not come from the top of my head; their choice was based on the literature.</li>
</ul>

<p>Statistical test chosen: logistic regression</p>

<p>I need to find the variables that best explain variations in the outcome variable (I am not interested in making predictions).</p>

<p>The problem: This question is a follow-up on the 2 questions listed below. From them, I got that performing automated stepwise regression has its downsides. Anyway, it seems that my sample size would be too small for that. It seems that my sample is also too small to enter all variables at once (using the SPSS 'Enter' method). This leaves me with my issue unresolved: how can I select a subset of variables from my original long list in order to perform multivariate logistic regression analysis?</p>

<p>UPDATE1: I am not an statistician, so I would appreciate if jargons can be reduced to the minimum. I am working with SPSS and am not familiar with other packages, so options that could be run with that software would be highly preferable.</p>

<p>UPDATE2: It seems that SPSS does not support LASSO for logistic regression. So following one of your suggestions, I am now struggling with R. I have passed through the basics, and managed to run a univariate logistic regression routine successfully using the glm code. But as I tried glmnet with the same dataset, I am receiving an error message. How could I fix it? Below is the code I used, followed by the error message:</p>

<pre><code>data1 &lt;- read.table(""C:\\\data1.csv"",header=TRUE,sep="";"",na.string=99:9999)

y &lt;- data1[,1]

x &lt;- data1[,2:45]

glmnet(x,y,family=""binomial"",alpha=1)  

**in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs,  : 
(list) object cannot be coerced to type 'double'**
</code></pre>

<p>UPDATE3: I got another error message, now related to missing values. My question concerning that matter is <a href=""http://stats.stackexchange.com/questions/104194/how-to-handle-with-missing-values-in-order-to-prepare-data-for-feature-selection"">here</a>. </p>

<ul>
<li><a href=""http://stats.stackexchange.com/questions/88482/can-univariate-linear-regression-be-used-to-identify-useful-variables-for-a-subs"">Can univariate linear regression be used to identify useful variables for a subsequent multiple logistic regression?</a></li>
<li><a href=""http://stats.stackexchange.com/questions/20836/algorithms-for-automatic-model-selection/20856#20856"">Algorithms for automatic model selection</a></li>
</ul>
"
"0.0842899503922179","0.086629616364842","103884","<p>I would like to find if there is a significant difference between two ROC-curves. I've found the roc.test in the pROC package. However, I cannot seem to find any information on how this test is actually performed. Can anyone explain it to me or refer to a webpage that has information on this?</p>

<p>From the <a href=""http://www.inside-r.org/packages/cran/pROC/docs/roc.test"" rel=""nofollow"">documentation</a> for <code>roc.test()</code>, I've found that the used test is DeLong's test (based on the article 'Comparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach'). However, I don't quite understand the article. </p>

<p>My ROC-curves have been found by comparing two logistic regressions, where one has a subset of attributes of the other seeing as I want to examine whether these attributes are useful. The probabilities are found using 5-fold cross-validation.</p>
"
"0.139779069524328","0.117539154496887","104194","<p>My situation:</p>

<ul>
<li>small sample size: 116 </li>
<li>binary outcome variable</li>
<li>long list of explanatory variables: 44</li>
<li>explanatory variables did not come from the top of my head; their choice was based on the literature.</li>
<li>most cases in the sample and most variables have missing values.</li>
</ul>

<p>Approach to feature selection chosen: LASSO</p>

<p>R's glmnet package won't let me run the glmnet routine, apparently due to the existence of missing values in my data set. There seems to be various methods for handling missing data, so I would like to know:</p>

<ul>
<li>Does LASSO impose any restriction in terms of the method of imputation that I can use?</li>
<li>What would be the best bet for imputation method? Ideally, I need a method that I could run on SPSS (preferably) or R.</li>
</ul>

<p>UPDATE1: It became clear from some of the answers below that I have do deal with more basic issues before considering imputation methods. I would like to add here new questions regarding that. On the the answer suggesting the coding as constant value and the creation of a new variable in order to deal with 'not applicable' values and the usage of group lasso:</p>

<ul>
<li>Would you say that if I use group LASSO, I would be able to use the approach suggested to continuous predictors also to categorical predictors? If so, I assume it would be equivalent to creating a new category - I am wary that this may introduce bias.</li>
<li>Does anyone know if R's glmnet package supports group LASSO? If not, would anyone suggest another one that does that in combination with logistic regression? Several options mentioning group LASSO can be found in CRAN repository, any suggestions of the most appropriate for my case? Maybe SGL?</li>
</ul>

<p>This is a follow-up on a previous question of mine (<a href=""http://stats.stackexchange.com/questions/102892/how-to-select-a-subset-of-variables-from-my-original-long-list-in-order-to-perfo"">How to select a subset of variables from my original long list in order to perform logistic regression analysis?</a>).</p>

<p>OBS: I am not a statistician.</p>
"
"0.0842899503922179","0.0649722122736315","105480","<p>I have a survey question where the respondent can check one choice or two choice maximum. Question looks like this:</p>

<blockquote>
  <p>What is the more important characteristic when you buy chocolate?</p>
  
  <ul>
  <li>Sweetness (characteristic 1) </li>
  <li>Packaging (characteristic 2) </li>
  <li>Price (characteristic 3) </li>
  <li>... (characteristic 4)  </li>
  <li>... (characteristic 5) </li>
  <li>... (characteristic 6)</li>
  </ul>
</blockquote>

<p>In my data, I have 6 column (1 for each characteristic) with the value 1 if the respondent checked the ad-hoc column and 0 otherwise.</p>

<p>I would like to perform a regression where the outcome is the question and the predictors the socio demographic criteria (some of them are numerical such as the age and other categorical such as the gender).</p>

<p>Now I wonder how I should do. Should I do a simple logistic regression 6 times with same predictors but each time different outcome?</p>

<blockquote>
  <ul>
  <li>glm(caracteristic1 ~ age+gender)</li>
  <li>glm(caracteristic2 ~ age+gender)</li>
  <li>glm(caracteristic3 ~ age+gender)</li>
  <li>glm(caracteristic4 ~ age+gender)</li>
  <li>glm(caracteristic5 ~ age+gender)</li>
  <li>glm(caracteristic6 ~ age+gender)</li>
  </ul>
</blockquote>

<p>Or is it possible to use another method such as the  multinomial logistic regression. If yes which R packages do you recommend me to use?</p>
"
"0.163546526421265","0.17859152928949","106259","<p>I use SPSS, but am forced to use R for exact logistic regression.  So I'm brand new to R (and hating it so far) and also new to logistic regression.  I've read the original elrm paper and looked at examples of its use. However, I can't find information on the questions below (after the data description).</p>

<p>The fit of two models of cognitive processing was compared for each subject in each of 3 conditions. My binary dependent variable is whether the difference in model fits was significant or not (my ""Success"" variable below). I have three experimental Conditions: 0, 1, and 2.  0 is my reference group.  My question is:  is there an overall effect of Condition? If so, which conditions differ?  The specific alternative hypothesis is that the proportion/probability of ""success"" should be greater in conditions 0 and 1 than in condition 2.  My data look like</p>

<p><img src=""http://i.stack.imgur.com/OAFtP.gif"" alt=""original data""></p>

<p>...and so on. SPSS actually creates the dummy variables for you on the fly but they are easy enough to create explicitly.</p>

<p><strong>Question 1:</strong>  I have read that to use elrm you have to enter the data such that the response variable represents success/number of trials. And as far as I can tell elrm doesn't create dummy variables automatically.  I've seen examples of tables representing this data structure, but can't find any step-by-step examples of getting raw data into that format, espescially given a one-variable 3-levels situation.  Is there an example out there that I'm missing?  If not, is this what the data should look like? </p>

<p><img src=""http://i.stack.imgur.com/aVxOL.gif"" alt=""reformated data""></p>

<p>I'm not sure how I'd enter the dummy variables into the formula...just as separate variables?</p>

<p><strong>Question 2:</strong>  I can see how I can get the tests of the coefficients of the dummy variables. But I can't figure out how to get a test of the overall effect of the independent variable. I need to evaluate the overall effect of Condition before looking at individual conditions.  Is there a way to get that out of elrm? (I found an example of this done for the aod package which runs regular logistic regression but not exact logistic regression.)</p>

<p><strong>Question 3:</strong>  I can't find a description of what the p-value for individual coeffeicients represents in elrm.  Is this is for the Wald test?</p>
"
"0.11920399101643","0.0918845837747533","108745","<p>My situation:</p>

<ul>
<li>small sample size: 116</li>
<li>binary outcome variable</li>
<li>long list of explanatory variables: 44</li>
<li>explanatory variables did not come from the top of my head; their choice was based on the literature.</li>
<li>most cases in the sample and most variables have missing values.</li>
</ul>

<p>Approach to feature selection chosen: LASSO</p>

<p>R's glmnet package won't let me run the glmnet routine, apparently due to the existence of missing values in my data set. It became clear from some of the answers to my <a href=""http://stats.stackexchange.com/questions/104194/how-to-handle-with-missing-values-in-order-to-prepare-data-for-feature-selection"">previous question</a> that I have do deal with more basic issues before considering imputation methods. I would like to add here new questions regarding that. On the the answer suggesting the coding as constant value and the creation of a new variable in order to deal with 'not applicable' values in continuous variables and the usage of group lasso:</p>

<ul>
<li><p>Would you say that if I use group LASSO, I would be able to use the approach suggested to continuous predictors also to categorical predictors? If so, I assume it would be equivalent to creating a new category - I am wary that this may introduce bias. If that is not advisable, what would be in the case of categorical variables?</p></li>
<li><p>Does anyone know if R's glmnet package supports group LASSO? If not, would anyone suggest another one that does that in combination with logistic regression? Several options mentioning group LASSO can be found in CRAN repository, any suggestions of the most appropriate for my case? Maybe SGL?</p></li>
</ul>

<p>OBS: I am not a statistician.</p>
"
"0.086028070377267","0.0884159819612176","109851","<p>I am using logistic regression to predict likelihood of an event occurring. Ultimately, these probabilities are put into a production environment, where we focus as much as possible on hitting our ""Yes"" predictions. It is therefore useful for us to have an idea of what definitive ""hits"" or ""non-hits"" might be <em>a priori</em> (before running in production), in addition to other measures we use for informing this determination.</p>

<p>My question is, what would be the proper way to predict a definitive class (1,0) based on  the predicted probability? Specifically, I use R's <code>glmnet</code> package for my modeling. This package arbitrarily picks .5 probability as threshold for a yes or no. I believe that I need to take the results of a proper scoring rule, based on predicted probabilities, to extrapolate  to a definitive class. An example of my modeling process is below:</p>

<pre><code>mods &lt;- c('glmnet', 'scoring')
lapply(mods, require, character.only = T)

# run cross-validated LASSO regression
fit &lt;- cv.glmnet(x = df1[, c(2:100)]), y = df1[, 1], family = 'binomial', 
type.measure = 'auc')

# generate predicted probabilities across new data
df2$prob &lt;- predict(fit, type=""response"", newx = df2[, c(2:100)], s = 'lambda.min')

# calculate Brier score for each record
df2$propscore &lt;- brierscore(df2[,1] ~ df2$prob, data = df2)
</code></pre>

<p>So I now have a series of Brier scores for each prediction, but then how do I use the Brier score to appropriately weight each likelihood being a yes or no?</p>

<p>I understand that there are other methods to make this determination as well, such as Random Forest.</p>
"
"0.0729972383233908","0.0750234484920533","110308","<p>I am working on fitting a GEE model to a multinomial logistic outcome using the R package <code>geepack</code>. My understanding is that the package uses <code>glm</code> to fit the formula. However, from what I can see, <code>glm</code> doesn't have a family for a multinomial outcome, just binomial. How do you go about fitting a multinomial logistic equation using <code>gee</code> in R? 
Thanks.</p>
"
"0.042144975196109","0.043314808182421","111168","<p>I'm using the function <code>compareGrowthCurves</code> in the R package statmod, but I can't seem to find an explanation of what types of curve this is valid for.  Does anyone know if there are particular families of functions I shouldn't use this for? </p>

<p>Specifically, I have data on bacterial growth and under some conditions I have textbook logistic growth to a stable plateau in stationary phase, but in others I have an obvious death phase after the onset of stationary phase.  Will I get valid results with this type of curve? (I am only making comparisons between curves of the same general shape).</p>

<p>Thanks for any info, I'm sure it's out there but I haven't found it.</p>
"
"0.146428339346645","0.162069171992452","111383","<p>I have been reading several CV posts on binary logistic regression but I am still confused for my current situation.</p>

<p>I am attempting to fit a binary logistic regression to a series of continuous and categorical variables in order to predict the mortality or the survival of animals (<code>qual_status</code>). Please see the <code>str</code> below:</p>

<pre><code>&gt; str(logit)
'data.frame':   136 obs. of  9 variables:
 $ id         : Factor w/ 135 levels ""01001"",""01002"",..: 26 27 28 29 30 31 32 33 34 35 ...
 $ gear       : Factor w/ 2 levels ""j"",""sc"": 2 1 1 2 1 2 1 2 2 1 ...
 $ depth      : num  146 163 179 190 194 172 172 175 240 214 ...
 $ length     : num  37 35 42 38 37 41 37 52 38 37 ...
 $ condition  : Factor w/ 4 levels ""1"",""2"",""3"",""4"": 1 1 4 1 4 2 2 1 2 1 ...
 $ in_water   : num  80 45 114 110 60 121 56 140 93 68 ...
 $ in_air     : num  60 136 128 136 165 118 220 90 177 240 ...
 $ delta_temp : num  8.5 8.4 8.3 8.5 8.5 8.6 8.6 8.7 8.7 8.7 ...
 $ qual_status: Factor w/ 2 levels ""0"",""1"": 1 1 2 1 2 1 2 1 1 1 ...
</code></pre>

<p>I have no issues fitting an the following additive binary logistic regression with the <code>glm</code> function:</p>

<p><code>glm(qual_status ~ gear + depth + length + condition + in_water + in_air + delta_temp, data = logit, family = binomial)</code></p>

<p>...but I am also interested at how these predictor variables interact with one another and possibly influence survival. However, when I attempt the following interactive binary logistic regression:</p>

<p><code>glm(qual_status ~ gear * depth * length * condition * in_water * in_air * delta_temp, data = logit, family = binomial)</code></p>

<p>I receive a warning message <code>""glm.fit: fitted probabilities numerically 0 or 1 occurred""</code>, along with missing coefficients due to singularities (NA or &lt;2e-16 <em>*</em>) when I use <code>summary</code>:</p>

<pre><code>Call:
glm(formula = qual_status ~ gear * depth * length * condition * 
    in_water * in_air * delta_temp, family = binomial, data = logit)

Deviance Residuals: 
  [1]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
 [36]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
 [71]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
[106]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0

Coefficients: (122 not defined because of singularities)
                                                            Estimate Std. Error    z value Pr(&gt;|z|)    
(Intercept)                                                1.419e+30  5.400e+22   26274077   &lt;2e-16 ***
gearsc                                                    -1.419e+30  5.400e+22  -26274077   &lt;2e-16 ***
depth                                                      1.396e+28  4.040e+20   34539471   &lt;2e-16 ***
length                                                     6.807e+28  1.836e+21   37079584   &lt;2e-16 ***
condition2                                                -3.229e+30  8.559e+22  -37727993   &lt;2e-16 ***
condition3                                                 1.747e+31  4.636e+23   37671986   &lt;2e-16 ***
condition4                                                 9.007e+31  2.388e+24   37724167   &lt;2e-16 ***
in_water                                                  -4.540e+28  1.263e+21  -35935748   &lt;2e-16 ***
in_air                                                    -4.429e+28  1.182e+21  -37470809   &lt;2e-16 ***
delta_temp                                                -1.778e+28  3.237e+21   -5492850   &lt;2e-16 ***
gearsc:depth                                              -1.396e+28  4.040e+20  -34539471   &lt;2e-16 ***
gearsc:length                                             -6.807e+28  1.836e+21  -37079584   &lt;2e-16 ***
depth:length                                              -9.293e+26  2.450e+19  -37930778   &lt;2e-16 ***
gearsc:condition2                                          1.348e+30  3.567e+22   37809001   &lt;2e-16 ***
gearsc:condition3                                          2.816e+30  7.495e+22   37575317   &lt;2e-16 ***
gearsc:condition4                                                 NA         NA         NA       NA    
</code></pre>

<p>Fitting only the continuous variables to a binary logistic regression doesn't yield any warnings or singularities but the addition of the ordinal predictor variables causes issues. Along with avoiding these warnings, is there a function/package that can handle dummy variables (I believe that is what I am looking for) in logistic regressions in <code>R</code>?</p>
"
"0.0942390294485422","0.077483884422606","111457","<p>I ran two logistic regression models, one with a dataset including outliers and one without outliers, with multiple predictors.</p>

<p>I checked each model's fit with the le Cessie â€“ van Houwelingen â€“ Copas â€“ Hosmer unweighted sum of squares test for global goodness of fit from the rms package in R (following advice <a href=""http://www.r-bloggers.com/veterinary-epidemiologic-research-glm-evaluating-logistic-regression-models-part-3/"" rel=""nofollow"">here</a>).</p>

<pre><code>model1 &lt;- lrm(y ~ a + b + c + d, data1, method = ""lrm.fit"", model = TRUE, x = TRUE, y = TRUE, linear.predictors = TRUE, se.fit = FALSE)
residuals(model1, type = ""gof"")
</code></pre>

<p>For the model with outliers the p value was close to 0, indicating a lack of fit. For the model without outliers p value was 0.52, indicating that my model was not incorrect.</p>

<p>I then ran 10-fold cross validation for both models with DAAG package and was surprised to get identical (poor) accuracy results for both = 0.56</p>

<pre><code>cv10&lt;-CVbinary(model1,nfolds=10)
</code></pre>

<p>I thought that the model created using the dataset without outliers, having a much better fit, will give me higher accuracy. Am I missing something here? I will be grateful for your help.</p>
"
"0.11150512337989","0.114600210537152","112247","<p>I'm trying to use the Match() function from the Matching package in R to do a propensity score analysis.</p>

<p>My outcome of interest is a binary variable (0/1).  My treatment is also a binary variable (0/1).  In addition, I have a number of other variables that I want to control for in this analysis.</p>

<p>First, I fit a logistic regression to define a propensity score for the treatment:</p>

<pre><code>glm1 = glm(Treatment ~ variable1 + variable2 + variable3 + ..., 
           data=dataset, family=""binomial"")
</code></pre>

<p>Then, I used the Match function to estimate the average treatment effect on the treated:</p>

<pre><code>rr1 = Match(Y = Outcome, Tr = Treatment, X = glm1$fitted)
</code></pre>

<p>Finally, I called for a summary:</p>

<pre><code>summary(rr1)
</code></pre>

<p>My question is how to interpret the output.  I get:</p>

<pre><code>Estimate... -0.349,
AI SE... 0.124,
T-stat... -2.827,
p.val... 0.005
</code></pre>

<p>What does this mean?  In particular, what is Estimate?  The documentation says it's ""The estimated average causal effect.""  But what are the units?  Can I interpret this to mean that the treatment reduced the outcome by a relative 35%?  Or by an absolute 0.35?  Or do I need to exponentiate?</p>

<p>Any help on the interpretation would be much appreciated!</p>
"
"0.0942390294485422","0.077483884422606","112348","<p>I am running a logistic regression model in R using multiply imputed data created using Amelia II, which I am then analyzing using Zelig. I would like to be able to report some measures of goodness-of-fit (e.g. likelihood ratio, pseudo R-squared, Hosmer-Lemeshow), however none are provided in the default Zelig output and I haven't been able to figure out a way to extract any from the <code>zelig()</code> object. </p>

<p>Do measures of goodness-of-fit need to be calculated differently when using multiply imputed datasets? Are there any R packages that are able to do this? I have looked into several packages that provide measures of goodness-of-fit, such as pscl, however they only work on glm objects, not MI objects created when using Amelia and Zelig.</p>

<p>Thanks in advance for your help!</p>
"
"0.151955869072729","0.144160395589483","112481","<p>I am attempting to plot hurdle regression output with an interaction term in R, but I have some trouble doing so with the count portion of the model. </p>

<pre><code>Model &lt;- hurdle(Suicide. ~ Age + gender + bullying*support, dist = ""negbin"", link = ""logit"")
</code></pre>

<p>Since I am unaware of any packages that would allow me to plot the hurdle model in its entirety, I estimated each component separately (binomial logit and negative binomial count), using the <code>MASS</code> package, and I am attempting to plot the results. </p>

<p>So far, I have had the best luck using the <code>visreg</code> package for plotting, but I would like to hear other suggestions. I have been able to reproduce and successfully plot the original logistic output from the hurdle model in <code>MASS</code>, but not the negative binomial count data (i.e., the parameter estimates from <code>MASS</code> are not the same as they are in the hurdle regression output).</p>

<p>I would greatly appreciate any insight regarding how others have plotted hurdle regression results in the past, or how I might be able to reproduce negative binomial coefficients originally obtained from the hurdle model using <code>glm.nb()</code> in <code>MASS</code>. </p>

<p>Here is what I am using to plot my data:</p>

<pre><code>##Logistic 

logistic&lt;-glm(SuicideBinary ~ Age + gender + bullying*support, data = sx, family=""binomial"")

data(""sx"", package = ""MASS"")
##Linear scale
visreg(logistic, ""bullying"", by=""support"", xlab = ""bullying"", ylab = ""Log odds (Suicide yes/no)"")

##Logistic/probability scale
visreg(logistic, ""bullying"", by=""support"", scale = ""response"", xlab = ""bullying"", ylab = ""P(Initial Attempt)"")

##Count model

NegBin&lt;-glm.nb(Suicide. ~ Age + gender + bullying*support, data = sx)

data(""sx"", package = ""MASS"")
##Linear scale
visreg(NegBin, ""bullying"", by=""support"", xlab = ""bullying"", ylab = ""Count model (number of suicide attempts)"")

##Logistic/probability scale
visreg(NegBin, ""bullying"", by=""support"", scale = ""response"", xlab = ""bullying"", ylab = ""P(Subsequent Attempts)"")
</code></pre>
"
"0.152345001256798","0.156573695352373","112760","<p>I have used <code>mlogit</code> package and I am trying to summarize the results I have from my model.  I have a question regarding the reference value and will get to that in a moment.</p>

<pre><code>redata.full &lt;- mlogit(no.C~ 1| WR+age+age2+BP+noC.1yr, data=redata, reflevel=""0"", na.action=na.fail)

no.C = number of offspring    
WR = risk
age+age2 = the non-linear relationship that as an individual ages their production decreases
BP = browsing pressure
noC.1yr = number of offspring produced the year before
</code></pre>

<p>I recognize that my data is ordinal in nature, but Im following other people's methods who have done this and used the reference based approach rather than ordinal logistic regression.  However, I am still shakey on justification other than citing the other person and saying ""he did it too!""  If anyone has a suggestion I would appreciate it.</p>

<p>My results for this model are: </p>

<pre><code>Call:
mlogit(formula = no.C ~ 1 | WR + age + age2 + BP + noC.1yr, data = redata, 
    na.action = na.fail, reflevel = ""0"", method = ""nr"", print.level = 0)

Frequencies of alternatives:
       0        1        2 
0.233766 0.675325 0.090909 

nr method
5 iterations, 0h:0m:0s 
g'(-H)^-1g = 2.16E-07 
gradient close to zero 

Coefficients :
               Estimate Std. Error t-value Pr(&gt;|t|)  
1:(intercept) -0.281226   1.225763 -0.2294  0.81854  
2:(intercept) -0.605312   1.997179 -0.3031  0.76183  
1:WR           0.847273   0.518854  1.6330  0.10248  
2:WR           1.347976   0.689916  1.9538  0.05072 .
1:age          0.314075   0.275486  1.1401  0.25425  
2:age         -0.422368   0.395240 -1.0686  0.28523  
1:age2        -0.018998   0.014446 -1.3151  0.18847  
2:age2         0.022572   0.018949  1.1912  0.23359  
1:BP          -0.143720   0.173585 -0.8280  0.40770  
2:BP          -0.074553   0.331108 -0.2252  0.82185  
1:noC.1yr      0.574304   0.377821  1.5200  0.12850  
2:noC.1yr      1.251673   0.626033  1.9994  0.04557 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Log-Likelihood: -116.6
McFadden R^2:  0.079844 
Likelihood ratio test : chisq = 20.236 (p.value = 0.0271)

exp(cbind(OddsRatio = coef(redata.full), ci))
              OddsRatio      2.5 %    97.5 %
1:(intercept) 0.7548580 0.06831155  8.341351
2:(intercept) 0.5459038 0.01089217 27.360107
1:WR          2.3332750 0.84394900  6.450831
2:WR          3.8496270 0.99577472 14.882511
1:age         1.3689929 0.79782462  2.349065
2:age         0.6554925 0.30209181  1.422317
1:age2        0.9811815 0.95379086  1.009359
2:age2        1.0228284 0.98553735  1.061530
1:BP          0.8661299 0.61634947  1.217136
2:BP          0.9281585 0.48504538  1.776078
1:noC.1yr     1.7758933 0.84686698  3.724076
2:noC.1yr     3.4961862 1.02497823 11.925441
</code></pre>

<p>I would like confirmation of my interpretations:
The model is better than a null - obtained from the likelihood ratio test.</p>

<p>Question: How do I test how well the model is actually working (i.e., goodness of fit)?  Hosmer-Lemshow test? Ive read warnings about using the McFaddin's Pseudo R where they really aren't applicable to multinomial regressions.  Ive found a HL test with <code>ResourceSelection</code> library and it says my model is NOT doing well at all.  Now what?</p>

<p>Interpretation:
WR and noC.1yr are the only variables that are coming out as slightly significant.  But this is only between the reference value of 0 and production of 2 calves.  It is not significantly different between 0 or 1 for these variables.  </p>

<p>Question: Ive been trying to find somewhere in the vignette what the t-value is - it is just a t-test?  How would I refer to the estimate as being significant?  ""The estimated odds for 2-offspring being produced versus 0 were 3.85 (95% CI = 1.0-14.88) which was significant (t= 1.99, P=0.05)""</p>

<p>Referring to my statement regarding setting the reference value.  When I run this exact same model using my other options of 0 or 1 offspring - I get completely different results of which variables are significant.  If I use 2 as the reference value then Age+WR+noC.yr are significant.  If I use 1, then Age only is sig.  So, which one to use?  I have read you want to pick one that is most relevant to your hypothesis, but in this case I could motivate any of the 3 levels.  </p>
"
"0.112386600522957","0.115506155153123","112801","<p>I am seemingly blindly following this <a href=""http://www.cfc.umt.edu/grizzlybearrecovery/pdfs/Schwartz%20et%20al.%202006e.pdf"" rel=""nofollow"">publication</a> that has done work very similar to what I need to accomplish (page 18-21).  My analysis is a multinomial logistic regression where I have 3 possible outcomes 0, 1, or 2 offspring produced.  In the publication, they have recommended a Hosmer-Lemshow and a Persons test for goodness-of-fit.  I have only figured out how to do the Hosmer-Lemshow test and my results are not so good (i.e. P-val is 0.00002).  I have no idea how to do the Pearsons test (suggestions are appreciated).</p>

<p>The paper I am following, of course their tests are ""good"" for model fit (page 21).  But they then go onto suggest that Somers D, the Goodman-Kruskala gamma and the Kendall's tau-a all indicate that their models are a good fit.  But the paper does not report any of the values for these indices or how they calculated them. </p>

<p>I have just found a package <code>ryouready</code> that runs all of these tests.  However, I have been having difficulties finding any help explaining what the values mean, let alone knowing if I have input my variables correctly.</p>

<p>My response variable is number of offspring, most of my explanatory variables are continuous like age or risk.  Do I need to calculate the mean of each explanatory variable within each response variable (get the mean risk for 1 offspring, mean risk for 2 offspring etc...)and then compare those? It also seems that these tests are for 2x2 tables.  If I am just looking at risk, my table would be a 1x3.  However, my complete model will have 4 variables (age, risk, bp, and #offspring year before).  </p>

<p>As you can likely tell, I am in the dark here on where to start. I would appreciate suggested readings, pdf lectures or videos of lectures would even be better!    </p>

<p>EDIT/UPDATE:
I have run the tests over my counts - I have 2 time periods (before/after) and then the count of offspring in each class (0,1,2).  I do not know how to interpret the values - what is ""good"".  What should I be looking for?  Any source that explains these values would be nice to see.  </p>

<pre><code>Kendall's (and Stuart's) Tau statistics
    Tau-b: 0.143
    Tau-c: 0.130
Somers' d:
    Columns dependent: 0.151 
    Rows dependent: 0.136 
    Symmetric: 0.143 
Goodman-Kruskal Gamma: 0.312 
Warning message:
In formatC(x, digits, format = ""f"") : class of 'x' was discarded
</code></pre>
"
"0.0729972383233908","0.0500156323280355","114195","<p>I have a dataset with a lot of missing values and mix of continues and categorical variables.  I want to use something like group lasso to do features selection. Probably the output is binary 0,1 and so grouped lasso logistic regression seems to be the more sensible choice. </p>

<p>My problem is the very large number of missing values. Deleting non complete rows is not an option.</p>

<p>Is there any R implementation that can be used similarly to the lasso and that can handle missing values and categorical variables at the same time?   </p>

<p>A possible solution has been proposed <a href=""http://stats.stackexchange.com/questions/110461/lasso-or-other-regularized-regression-with-censored-missing-data"">here</a> but it does not refer to any R package.</p>
"
"0.059601995508215","0.0612563891831689","114218","<p>After running a gradient boosted model with <code>n</code> data points using multinomial regression where the response variable (a factor, as required by the gbm function) has <code>k</code> levels with R package gbm, I see that the predictions are output as as a vector of length <code>n*k</code>. Predicted responses are from:</p>

<pre><code>probs.var.multinom &lt;- predict.gbm(gbm.model.multinom, test.data, best.iter.gbm, 
                                  type=""response"")
</code></pre>

<p>Note that this is different from the output of a logistic (distribution = ""bernoulli"") model, where the results are a vector the same length as the number of cases.</p>

<p>How should this be interpreted? Specifically, how can I link the response vector back to the input data set to evaluate the classification?</p>
"
"0.0842899503922179","0.086629616364842","114399","<p>I have a theoretical growth function that can be perturbed by events, and I'd like to estimate the growth parameters as well as the perturbation, and the rate of falloff after that perturbation.</p>

<p>I'm thinking of using a logistic function to model the effect of the event and the falloff of that effect (if any).</p>

<p>To ground this, $x$ is time, and $t$ is the time the event occurs. Before time $t$, or if the event never occurs, we have a simple linear regression. After the event occurs, I model the contribution of the event with magnitude controlled by $\beta_2$ and rate of falloff by $\beta_3$.</p>

<p>$y_i=\left\{x_{i}&lt;t:\beta_{0}+\beta_1x_i+\epsilon_i,x_i&gt;t:\beta_0+\beta_1x_i+2\beta_2\frac{1}{\left(1+e^{\beta_3\left(x_i-t\right)}\right)}+\epsilon_i\right\}$</p>

<p>(<em>edited to add the error term</em>)</p>

<p>Here's a <a href=""https://www.desmos.com/calculator/nzmusqqosq"" rel=""nofollow"">Desmos graph</a> if it helps.</p>

<p>I'm really not sure how to estimate parameters for this model in any of the stats packages I'm familiar with in R. Do I need to turn to Bayesian methods?</p>
"
"0.163226787060855","0.145389859970061","115647","<p>I have the following problem: I have a set of English words which I want to translate to Dutch. Of each words I mined a set of possible translations. For example, for the word ""Eighteen"" I obtained only one possible Dutch translation: ""Achttien"", which is correct. However, for other words I obtained multiple translations. For the word ""Good""  I have the translations ""Goed"", ""Braaf"" and ""Eerlijk"", which are technically correct translations but by far the best and most commonly used translation is only the word ""Goed"".</p>

<p>For a training set of English words I manually defined the optimal (correct) translation. Using this set I want to train some model to optimally pick for each English word the optimal Dutch word using some predictors. For example, I assume words that are more frequently used are probably better translations than others, and I assume that words that are noted first in a list of translations are probably better translations than others (e.g., in a dictionary, the first translation is usually the best).</p>

<p>So, my data looks something like this:</p>

<pre><code>English     Dutch       Frequency   Order   Correct
---------------------------------------------------
Eighteen    Achttien    800         1       TRUE
Good        Goed        900         1       TRUE
Good        Braaf       500         2       FALSE
Good        Eerlijk     600         3       FALSE
old         bejaard     300         1       FALSE
old         oud         900         2       TRUE
</code></pre>

<p>I want to predict the classification in the column <code>Correct</code>. At first I thought a logistic regression could do this, but that does not take into account that each row is not independent. e.g., for each unique value of the column <code>English</code> only one is correct and all others are false. Thus, some other classification method is required.</p>

<p>I was hoping you could point me in the right direction as to what method (or even better, an <code>R</code> package) would be suitable to tackle this problem. I guess this problem occurs more often in Machine Learning but I have no experience in that field.</p>
"
"0.0842899503922179","0.0649722122736315","116850","<p>I have a dataset consisting of repeated measures data of graded toxicity scores (0-4) in a large number of patients being treated with a anti-cancer drug. We would like to identify predictors for developing dose limiting toxicity (e.g. score 3 or 4).</p>

<p>Initially the dataset was analyzed using a cumulative link mixed model to identify such predictors (clmm in R package ordinal).</p>

<p>However, there is also informative dropout occuring in the dataset (MNAR), e.g. the risk of dropping out is related to the severity of toxicity. As this therefore may bias parameter estimates, it seems a joint model that includes a survival model for dropout would be best. </p>

<p>My questions are:</p>

<ol>
<li><p>Is a joint model for dropout and outcome the only suitable option to account for dropout?  Or would there be other ways as well ? The only ugly alternative would be to analyze only the first part of the dataset, but I would rather like to use ALL the available data.</p></li>
<li><p>Is there an R package available that would readily allow such an analysis. E.g. longitudinal mixed-effect modelling of ordered categorical data, together with a dropout survival model. Or, alternatively, a mixed effect model logistic regression model with dropout (e.g. in this case we would reduce the dataset to having a dose limiting toxicity yes/no) ? The JM and joinR packages appear to be based on continuous data for the repeated measures.</p></li>
</ol>
"
"0.0842899503922179","0.086629616364842","117340","<p>My situation:</p>

<ul>
<li>small sample size: 116</li>
<li>binary outcome variable</li>
<li>long list of explanatory variables: 50</li>
<li>explanatory variables did not come from the top of my head; their choice was based on the literature.</li>
</ul>

<p>Following a suggestion to a previous <a href=""http://stats.stackexchange.com/questions/102892/how-to-select-a-subset-of-variables-from-my-original-long-list-in-order-to-perfo"">question</a> of mine, I have run LASSO (using R's glmnet package) in order to select the subset of exaplanatory variables that best explain variations in my binary outcome variable.</p>

<p>I have calculated lambda.min through cross-validation (cv.glmnet command) and got the correspondent coefficients for my explanatory variables. For 6 of my total 50 explanatory variables, the coefficients were non-zero. Are those coefficients comparable, i.e. can I say that the variables with the highest ones are the most important? If they are not comparable, can I run logistic regression (using glm) with those 6 variables and then compare them in terms of coefficients and p-values?</p>
"
"0.0486648255489272","0.0750234484920533","118034","<p>I would like to build a logistic regression model in which I will be looking for predictor variables having a significant effect on the breeding success of a raptor bird. </p>

<p>The predictors in the dataset are highly correlated, which led me to consider logistic ridge regression. Furthermore, I investigated different breeding grounds in which one or multiple birds have been breeding. Since this makes the data clustered, I would need to add the breeding ground as a random effect in the model.</p>

<p>Thus, I would need a 'mixed logistic ridge regression' approach if I am getting things right here. This paper suggests this approach too for another problem:</p>

<p><a href=""http://www.ncbi.nlm.nih.gov/pubmed/22049265"" rel=""nofollow"">http://www.ncbi.nlm.nih.gov/pubmed/22049265</a></p>

<p>Are there any people aware of the availability of an R package or something related having implemented a mixed logistic ridge regression approach as the paper and myself just described? I did not succeed in finding one.</p>

<p>Thank you.</p>
"
"0.259799075253674","0.259983820356272","118215","<p>This is my first post on StackExchange, but I have been using it as a resource for quite a while, I will do my best to use the appropriate format and make the appropriate edits. Also, this is a multi-part question. I wasn't sure if I should split the question into several different posts or just one. Since the questions are all from one section in the same text I thought it would be more relevant to post as one question.</p>

<p>I am researching habitat use of a large mammal species for a Master's Thesis. The goal of this project is to provide forest managers (who are most likely not statisticians) with a practical framework to assess the quality of habitat on the lands they manage in regard to this species. This animal is relatively elusive, a habitat specialist, and usually located in remote areas. Relatively few studies have been carried out regarding the distribution of the species, especially seasonally.  Several animals were fitted with GPS collars for a period of one year. One hundred locations (50 summer and 50 winter) were randomly selected from each animal's GPS collar data. In addition, 50 points were randomly generated within each animal's home range to serve as ""available"" or ""pseudo-absence"" locations. The locations from the GPS collars are coded a 1 and the randomly selected available locations are coded as 0.</p>

<p>For each location, several habitat variables were sampled in the field (tree diameters, horizontal cover, coarse woody debris, etc) and several were sampled remotely through GIS (elevation, distance to road, ruggedness, etc). The variables are mostly continuous except for 1 categorical variable that has 7 levels.</p>

<p>My goal is to use regression modelling to build resource selection functions (RSF) to model the relative probability of use of resource units. I would like to build a seasonal (winter and summer) RSF for the population of animals (design type I) as well as each individual animal (design type III).</p>

<p>I am using R to perform the statistical analysis.</p>

<p>The <strong>primary text</strong> I have been using isâ€¦</p>

<ul>
<li>""Hosmer, D. W., Lemeshow, S., &amp; Sturdivant, R. X. 2013. Applied Logistic Regression. Wiley, Chicester"".</li>
</ul>

<p>The majority of the examples in Hosmer et al. use STATA, <strong>I have also been using the following 2 texts for reference with R</strong>.</p>

<ul>
<li>""Crawley, M. J. 2005. Statistics : an introduction using R. J. Wiley,
Chichester, West Sussex, England.""</li>
<li>""Plant, R. E. 2012. Spatial Data Analysis in Ecology and Agriculture 
Using R. CRC Press, London, GBR.""</li>
</ul>

<p>I am currently following the steps in <strong>Chapter 4 of Hosmer et al. for the ""Purposeful Selection of Covariates""</strong> and have a few questions about the process. I have outlined the first few steps in the text below to aid in my questions.</p>

<ol>
<li>Step 1: A univariable analysis of each independent variable (I used a
univariable logistic regression). Any variable whose univariable test
has a p-value of less than 0.25 should be included in the first
multivariable model.</li>
<li>Step 2: Fit a multivariable model containing all covariates
identified for inclusion at step 1 and to assess the importance of
each covariate using the p-value of its Wald statistic. Variables
that do not contribute at traditional levels of significance should
be eliminated and a new model fit. The newer, smaller model should be
compared to the old, larger model using the partial likelihood ratio
test.</li>
<li>Step 3: Compare the values of the estimated coefficients in the
smaller model to their respective values from the large model. Any
variable whose coefficient has changed markedly in magnitude should
be added back into the model as it is important in the sense of
providing a needed adjustment of the effect of the variables that
remain in the model. Cycle through steps 2 and 3 until it appears that all of the important variables are included in the model and those excluded are clinically and/or statistically unimportant. Hosmer et al. use the ""<em>delta-beta-hat-percent</em>""
as a measure of the change in magnitude of the coefficients. They
suggest a significant change as a <em>delta-beta-hat-percent</em> of >20%. Hosmer et al. define the <em>delta-beta-hat-percent</em> as 
$\Delta\hat{\beta}\%=100\frac{\hat{\theta}_{1}-\hat{\beta}_{1}}{\hat{\beta}_{1}}$.
Where $\hat{\theta}_{1}$ is the coefficient from the smaller model and $\hat{\beta}_{1}$ is the coefficient from the larger model.</li>
<li>Step 4: Add each variable not selected in Step 1 to the model
obtained at the end of step 3, one at a time, and check its
significance either by the Wald statistic p-value or the partial
likelihood ratio test if it is a categorical variable with more than
2 levels. This step is vital for identifying variables that, by
themselves, are not significantly related to the outcome but make an
important contribution in the presence of other variables. We refer
to the model at the end of Step 4 as the <em>preliminary main effects
model</em>.</li>
<li>Steps 5-7: I have not progressed to this point so I will leave these
steps out for now, or save them for a different question.</li>
</ol>

<p><strong>My questions:</strong> </p>

<ol>
<li>In step 2, what would be appropriate as a traditional level of
significance, a p-value of &lt;0.05 something larger like &lt;.25?</li>
<li>In step 2 again, I want to make sure the R code I have been using for the partial likelihood test is correct and I want to make sure I am interpreting the results correctly. Here is what I have been doing&hellip;<code>anova(smallmodel,largemodel,test='Chisq')</code> If the p-value is significant (&lt;0.05) I add the variable back to the model, if it is insignificant I proceed with deletion?</li>
<li>In step 3, I have a question regarding the <em>delta-beta-hat-percent</em> and when it is appropriate to add an excluded variable back to the model. For example, I exclude one variable from the model and it changes the $\Delta\hat{\beta}\%$ for a different variable by >20%. However, the variable with the >20% change in $\Delta\hat{\beta}\%$ seems to be insignificant and looks as if it will be excluded from the model in the next few cycles of Steps 2 and 3. How can I make a determination if both variables should be included or excluded from the model? Because I am proceeding by excluding 1 variable at a time by deleting the least significant variables first, I am hesitant to exclude a variable out of order.</li>
<li><p>Finally, I want to make sure the code I am using to calculate $\Delta\hat{\beta}\%$ is correct. I have been using the following code. If there is a package that will do this for me or a more simple way of doing it I am open to suggestions.  </p>

<p><code>100*((smallmodel$coef[2]-largemodel$coef[2])/largemodel$coef[2])</code></p></li>
</ol>
"
"0.11920399101643","0.122512778366338","122039","<p><strong>SOLVED</strong>: an elastic net model, as any other logistic regression model, will not generate more coefficients than input variables. Check Zach's answer to understand how from an (apparent) low number of inputs, more coefficients can be generated. The cause of this question was a code bug, as the users pointed out.</p>

<p>This is a simple question. I've fitted a model with 1334 variables using elastic net to perform feature selection and regularization. I'm now trying to interpret the obtained coefficients in order to find correlations between the input variables and the output. The only problem is that instead of the (expected) 1335 coefficients (intercept+1334), extracting the coefficients through <code>coef(model,s=""lambda.min"")</code> yields around 1390 coefficients. This seems highly counterintuitive and stops me from mapping a single coefficient to a single input variable, so I suppose I'm not understanding some of the insides of the elastic net. Any idea would be very helpful. Thanks in advance.</p>

<p>PS: just in case someone wonders so, I've not included interaction terms nor any synthetic variable, just the original 1334 ones.</p>

<p>PS2: elastic net references:</p>

<ul>
<li>Mathematical paper: <a href=""http://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&amp;%20Hastie.pdf"" rel=""nofollow"">http://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&amp;%20Hastie.pdf</a></li>
<li>R package tutorial: <a href=""http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html"" rel=""nofollow"">http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html</a></li>
</ul>

<p>PS3: about the code used to fit the model:</p>

<p>it is a 250 line script, so unless you specifically need it, I think it'd only clutter the question. Basically, the algorithm takes as an input a data frame of 1393 colums, where the last one is the target variable and the first 1392 are the input variables. So, after separating those into two matrices, input and output, the actual model fitting is done in this call:</p>

<p><code>cv.glmnet(x=input_matrix,y=output_matrix,family=""binomial"",type.measure=""auc"")</code></p>

<p>If you need to, I can actually generate a reproducible file with the data I use and the whole script. </p>
"
"0.146428339346645","0.15049280256442","122212","<p>I have been developing a logistic regression model based on retrospective data from a national trauma database of head injury in the UK. The key outcome is 30 day mortality (denoted as ""Survive"" measure). Other measures with published evidence of significant effect on outcome in previous studies include:</p>

<pre><code>Year - Year of procedure = 1994-2013
Age - Age of patient = 16.0-101.5
ISS - Injury Severity Score = 0-75
Sex - Gender of patient = Male or Female
inctoCran - Time from head injury to craniotomy in minutes = 0-2880 (After 2880 minutes is defined as a separate diagnosis)
</code></pre>

<p>Using these models, given the dichotomous dependent variable, I have built a logistic regression using lrm.</p>

<p>The method of model variable selection was based on existing clinical literature modelling the same diagnosis. All have been modelled with a linear fit with the exception of ISS which has been modelled traditionally through fractional polynomials. No publication has identified known significant interactions between the above variables.</p>

<p>Following advice from Frank Harrell, I have proceeded with the use of regression splines to model ISS (there are advantages to this approach highlighted in the comments below). The model was thus pre-specified as follows:</p>

<pre><code>rcs.ASDH&lt;-lrm(formula = Survive ~ Age + GCS + rcs(ISS) +
    Year + inctoCran + oth, data = ASDH_Paper1.1, x=TRUE, y=TRUE)
</code></pre>

<p>Results of the model were:</p>

<pre><code>&gt; rcs.ASDH

Logistic Regression Model

lrm(formula = Survive ~ Age + GCS + rcs(ISS) + Year + inctoCran + 
    oth, data = ASDH_Paper1.1, x = TRUE, y = TRUE)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          2135    LR chi2     342.48    R2       0.211    C       0.743    
 0            629    d.f.             8    g        1.195    Dxy     0.486    
 1           1506    Pr(&gt; chi2) &lt;0.0001    gr       3.303    gamma   0.487    
max |deriv| 5e-05                          gp       0.202    tau-a   0.202    
                                           Brier    0.176                     

          Coef     S.E.    Wald Z Pr(&gt;|Z|)
Intercept -62.1040 18.8611 -3.29  0.0010  
Age        -0.0266  0.0030 -8.83  &lt;0.0001 
GCS         0.1423  0.0135 10.56  &lt;0.0001 
ISS        -0.2125  0.0393 -5.40  &lt;0.0001 
ISS'        0.3706  0.1948  1.90  0.0572  
ISS''      -0.9544  0.7409 -1.29  0.1976  
Year        0.0339  0.0094  3.60  0.0003  
inctoCran   0.0003  0.0001  2.78  0.0054  
oth=1       0.3577  0.2009  1.78  0.0750  
</code></pre>

<p>I then used the calibrate function in the rms package in order to assess accuracy of the predictions from the model. The following results were obtained:</p>

<pre><code>plot(calibrate(rcs.ASDH, B=1000), main=""rcs.ASDH"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/HYTsp.png"" alt=""Bootstrap calibration curves penalized for overfitting""></p>

<p>Following completion of the model design, I created the following graph to demonstrate the effect of the Year of incident on survival, basing values of the median in continuous variables and the mode in categorical variables:</p>

<pre><code>ASDH &lt;- Predict(rcs.ASDH, Year=seq(1994,2013,by=1),Age=48.7,ISS=25,inctoCran=356,Other=0,GCS=8,Sex=""Male"",neuroYN=1,neuroFirst=1)
Probabilities &lt;- data.frame(cbind(ASDH$yhat,exp(ASDH$yhat)/(1+exp(ASDH$yhat)),exp(ASDH$lower)/(1+exp(ASDH$lower)),exp(ASDH$upper)/(1+exp(ASDH$upper))))
names(Probabilities) &lt;- c(""yhat"",""p.yhat"",""p.lower"",""p.upper"")
ASDH&lt;-merge(ASDH,Probabilities,by=""yhat"")
plot(ASDH$Year,ASDH$p.yhat,xlab=""Year"",ylab=""Probability of Survival"",main=""30 Day Outcome Following Craniotomy for Acute SDH by Year"", ylim=range(c(ASDH$p.lower,ASDH$p.upper)),pch=19)
arrows(ASDH$Year,ASDH$p.lower,ASDH$Year,ASDH$p.upper,length=0.05,angle=90,code=3)
</code></pre>

<p>The code above resulted in the following output:</p>

<p><img src=""http://i.stack.imgur.com/KGYcz.png"" alt=""Year trend with lower and upper""></p>

<p><strong><em>My remaining questions are the following:</em></strong></p>

<p><strong>1. Spline Interpretation</strong> - How can I calculate the p-value for the splines combined for the overall variable?</p>
"
"0.11150512337989","0.114600210537152","123435","<p>I'm working on disease prevalence, something I've never done before, and I'm trying to weight a gam with population. It seems to me that the prevalence rate for China ought to count a bit more than Niue. I am fitting GAM models using the R packages <code>mgcv</code>. BTW, for this disease, as far as we know now, everyone is at risk so it's not a prevalance ratio problem like with communicable diseases.</p>

<p>My problem is that logistic gam weights are counted as the $N$ while the response variable is then supposed to be counts. I really only have counts /million (which I can easily make proportions of course) derived from long term data collection and the data for some countries yield counts of only a couple / million when their populations are in thousands. Therefore, I can't turn it into the actual count and gam can't work out the model. The real $N$ that went into determining the numbers is not the population, since the data is collected and averaged over time, but they are highly correlated. So I still want to use it as a weight.</p>

<p>There's that problem and additionally that when I'm working out a model across 100 countries accounting for the bulk of the human population it seems that my CI's for the GAM should be rather small (nonexistent?). Therefore, I do need a way to get the population in there. Perhaps someone knows of a GAM package that can work with proportions rather than counts? I know there are some for generalized linear modelling but I need nonlinear.</p>
"
"0.139779069524328","0.143658966607306","123498","<p>I would like to generate a confidence interval for predicted vs actual rates.</p>

<p>I am auditing my group of anaesthetists (aka anesthesiologists) to see how we compare on a number of potentially preventable complications (eg post-operative nausea, severe pain, hypothermia).</p>

<p>I have 20000 surgical operation records and I can make a GLM to make a ""case-mix adjusted risk"" (using age, gender, type of surgery, duration of surgery as risk factors) and thus <a href=""http://www.r-tutor.com/elementary-statistics/logistic-regression/estimated-logistic-regression-equation"" rel=""nofollow"">generate a risk</a> for each patient.</p>

<p>I can then aggregate the risk and actual per clinician I can generate an actual and predicted rate for each complication. I can make a confidence interval for my actual - but it seems a bit simplistic to just test to see if the confidence interval on the actual rate includes the rate generated from summing the glm-predicted risks.</p>

<p><a href=""http://stats.stackexchange.com/questions/7344/how-to-graphically-compare-predicted-and-actual-values-from-multivariate-regress"">This question</a> has some pointers to a package but I am hoping for some more specific suggestions.</p>

<p>To clarify what I have already (using ""requirement for pain protocol"" as an example):</p>

<pre><code># make model (dependent variable has values 1/0)
model.pp = glm(
pain_protocol1 ~
age + log_age + age2 + inv_age
+ op_time + log_op_time + op_time2
+ gender
+ category
+ thimble,
family = ""binomial"",
data=d4)
# calculate predicted PACU time and then difference between predicted and actual:
d4$pred_pp = predict(model.pp, newdata=d4, type=""response"", na.action=""na.pass"")

d4$extra_pp = d4$pain_protocol1 - d4$pred_pp
# aggregate deviation from predicted rate
ppr_pa &lt;- aggregate(extra_pp ~ adult_anaesthetist, data=d4, FUN=mean)
barplot(ppr_pa$extra_pp, name=ppr_pa$adult_anaesthetist,las=2) 
</code></pre>

<p>So I can make this plot for my colleagues, showing the variation we have in how much pain our patients experience in the ""post anaesthesia care unit"". These variations are great enough so that they definitely represent a material difference in patient experience, and most of the difference will also be unlikely to be variation due to chance (ie ""statistically significant""). However, as I examine smaller subgroups and other complications that are less frequent it would be good to be able to calculate confidence intervals.</p>

<p><img src=""http://i.stack.imgur.com/FFe8W.png"" alt=""bar plot illustrating difference between predicted and actual rates of &quot;needing pain protocol&quot;""></p>

<p>Note that each clinician has a different number of cases, and each clinician is given a bird code-name for anonymity.</p>
"
"0.126434925588327","0.129944424547263","124512","<p>I have been developing a logistic regression model based on retrospective data from a national trauma database of head injury in the UK. The key outcome is 30 day mortality (denoted as ""Survive"" measure). Other measures across the whole database with published evidence of significant effect on outcome in previous studies include:</p>

<pre><code>Year - Year of procedure = 1994-2013
Age - Age of patient = 16.0-101.5
ISS - Injury Severity Score = 1-75
GCS - Glasgow Coma Scale = 3-15
Sex - Gender of patient = Male or Female
inctoCran - Time from head injury to craniotomy in minutes = 0-2880 (After 2880 minutes is defined as a separate diagnosis)
</code></pre>

<p>There are two additional variables where the data is only available from 2004:</p>

<pre><code>Pupil reactivity (1 is brisk, 2 is sluggish and 3 is unreactive)
Pupil size (continuous variable in mm - 1-10)
</code></pre>

<p>Based on literature these are significant predictors of outcome. However, out of a series of 2140, I am missing 936. Secondly, the measure is not missing at random, having only been collected in recent years.</p>

<p>My questions are the following in order to address the year range 1994-2013:</p>

<p>1) My data is heavily skewed to later years; how can I adapt the logistic regression to reduce the effect of this when assessing the effect of the year of procedure on outcome?</p>

<p>2) Can I exclude pupil reactivity since it was not collected before 2004 in performing this analysis even if it is a strong predictor?</p>

<p>3) If I should include pupil reactivity, can a multivariate regression be built with the variables above with which to perform imputation to create data for 1994-2003 given 43% of the data is missing?</p>

<p>4) If not possible, could imputation be performed based on data since 2009 where ~15% is missing?</p>

<p>I perform all statistical analyses exclusively with R and would be grateful if you could add known packages/formulae to execute your suggestions.</p>
"
"0.042144975196109","0.043314808182421","125211","<p>I am new to machine learning/statistical modelling.</p>

<p>I am trying to run a classification on a highly sparse dataset with 100 features, most of which are categorical (TRUE/FALSE) with the remaining values missing. To handle missing values, I filled the missing spots with the text 'Nothing', thereby creating a new level.</p>

<p>Next, I am trying to run a logistic regression using a penalty (glmnet package). When I check the coefficients, I see dummy variables corresponding to 'Nothing' having the higher coefficients.</p>

<p>How should I remove these coefficients? What would be a better approach to this?</p>

<p>Or should I just use trees? Please suggest the best way forward.</p>

<p>Thanks!</p>
"
"0.206632741273888","0.212368319374705","125453","<p>I have used the â€˜polrâ€™ function in the MASS package to run an ordinal logistic regression for an ordinal categorical response variable with 15 continuous explanatory variables.</p>

<p>I have used the code (shown below) to check that my model meets the proportional odds assumption following advice provided in <a href=""http://www.ats.ucla.edu/stat/r/dae/ologit.htm"">UCLA's guide</a>. However, Iâ€™m a little worried about the output implying that not only are the coefficients across various cutpoints similar, but they are exactly the same (see graphic below). </p>

<pre><code>FGV1b &lt;- data.frame(FG1_val_cat=factor(FGV1b[,""FG1_val_cat""]), 
                    scale(FGV1[,c(""X"",""Y"",""Slope"",""Ele"",""Aspect"",""Prox_to_for_FG"", 
                          ""Prox_to_for_mL"", ""Prox_to_nat_border"", ""Prox_to_village"", 
                          ""Prox_to_roads"", ""Prox_to_rivers"", ""Prox_to_waterFG"", 
                          ""Prox_to_watermL"", ""Prox_to_core"", ""Prox_to_NR"", ""PCA1"", 
                          ""PCA2"", ""PCA3"")]))
b     &lt;- polr(FG1_val_cat ~ X + Y + Slope + Ele + Aspect + Prox_to_for_FG + 
                            Prox_to_for_mL + Prox_to_nat_border + Prox_to_village + 
                            Prox_to_roads + Prox_to_rivers + Prox_to_waterFG + 
                            Prox_to_watermL + Prox_to_core + Prox_to_NR, 
              data=FGV1b, Hess=TRUE)
</code></pre>

<p>View a summary of the model:</p>

<pre><code>summary(b)
(ctableb &lt;- coef(summary(b)))
q        &lt;- pnorm(abs(ctableb[, ""t value""]), lower.tail=FALSE) * 2
(ctableb &lt;- cbind(ctableb, ""p value""=q))
</code></pre>

<p>And now we can look at the confidence intervals for the parameter estimates:</p>

<pre><code>(cib &lt;- confint(b)) 
confint.default(b)
</code></pre>

<p>But these results are still quite hard to interpret, so let's convert the coefficients into odds ratios</p>

<pre><code>exp(cbind(OR=coef(b), cib))
</code></pre>

<p>Checking the assumption. So the following code will estimate the values to be graphed. First it shows us the logit transformations of the probabilities of being greater than or equal to each value of the target variable</p>

<pre><code>FG1_val_cat &lt;- as.numeric(FG1_val_cat)
sf &lt;- function(y) {
  c('VC&gt;=1' = qlogis(mean(FG1_val_cat &gt;= 1)),
    'VC&gt;=2' = qlogis(mean(FG1_val_cat &gt;= 2)),
    'VC&gt;=3' = qlogis(mean(FG1_val_cat &gt;= 3)),
    'VC&gt;=4' = qlogis(mean(FG1_val_cat &gt;= 4)),
    'VC&gt;=5' = qlogis(mean(FG1_val_cat &gt;= 5)),
    'VC&gt;=6' = qlogis(mean(FG1_val_cat &gt;= 6)),
    'VC&gt;=7' = qlogis(mean(FG1_val_cat &gt;= 7)),
    'VC&gt;=8' = qlogis(mean(FG1_val_cat &gt;= 8)))
}
(t &lt;- with(FGV1b, summary(as.numeric(FG1_val_cat) ~ X + Y + Slope + Ele + Aspect + 
                             Prox_to_for_FG + Prox_to_for_mL + Prox_to_nat_border + 
                             Prox_to_village + Prox_to_roads + Prox_to_rivers + 
                             Prox_to_waterFG + Prox_to_watermL + Prox_to_core + 
                             Prox_to_NR, fun=sf)))
</code></pre>

<p>The table above displays the (linear) predicted values we would get if we regressed our dependent variable on our predictor variables one at a time, without the parallel slopes assumption. So now, we can run a series of binary logistic regressions with varying cutpoints on the dependent variable to check the equality of coefficients across cutpoints</p>

<pre><code>par(mfrow=c(1,1))
plot(t, which=1:8, pch=1:8, xlab='logit', main=' ', xlim=range(s[,7:8]))
</code></pre>

<p><img src=""http://i.stack.imgur.com/4Uicq.jpg"" alt=""polr assumption check""></p>

<p>Apologies that I am no statistics expert and perhaps I am missing something obvious here. However, I have spent a long time trying to figure out if there is a problem in how I tested the model assumption and also trying to figure out other ways to run the same kind of model. </p>

<p>For example, I read in many help mailing lists that others use the vglm function (in the VGAM package) and the lrm function (in the rms package) (for example see here:  <a href=""http://stats.stackexchange.com/questions/25988/proportional-odds-assumption-in-ordinal-logistic-regression-in-r-with-the-packag"">Proportional odds assumption in ordinal logistic regression in R with the packages VGAM and rms</a>). I have tried to run the same models but am continuously coming up against warnings and errors.</p>

<p>For example, when I try to fit the vglm model with the â€˜parallel=FALSEâ€™ argument (as the previous link mentions is important for testing the proportional odds assumption), I encounter the following error:</p>

<blockquote>
  <p>Error in lm.fit(X.vlm, y = z.vlm, ...) : NA/NaN/Inf in 'y'<br>
  In addition: Warning message:<br>
  In Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals = residuals,  :
    fitted values close to 0 or 1</p>
</blockquote>

<p>I would like to ask please if there is anyone who might understand and be able to explain to me why the graph I produced above looks as it does. If indeed it means that something isnâ€™t right, could you please help me find a way to test the proportional odds assumption when just using the polr function. Or if that is just not possible, then I will resort to trying to use the vglm function, but would then need some help to explain why I keep getting the error given above.</p>

<p>NOTE: As a background, there are 1000 datapoints here, which are actually location points across a study area. I am looking to see if there are any relationships between the categorical response variable and these 15 explanatory variables. All of those 15 explanatory variables are spatial characteristics (for example, elevation, x-y coordinates, proximity to forest etc.). The 1000 datapoints were randomly allocated using a GIS, but I took a stratified sampling approach. I made sure that 125 points were randomly chosen within each of the 8 different categorical response levels. I hope this information is also helpful.</p>
"
"0.0942390294485422","0.0968548555282575","125465","<p>I'm searching for a built-in function in R for calculating the required sample size (given certain power, alpha,..) of a mixed ANOVA (2 between, 1 within variable, 2x2x2 design). Does this function exist? I didn't find it in the following packages:</p>

<p>â€¢pwr is the oldest power-analysis library; some introductory info can be found on Quick-R</p>

<p>â€¢PoweR: Computation of power and level tables for hypothesis tests</p>

<p>â€¢Power2Stage: Power and Sample size distribution of 2-stage BE studies via simulations</p>

<p>â€¢powerAnalysis: Power analysis in experimental design</p>

<p>â€¢powerGWASinteraction: Power Calculations for Interactions for GWAS</p>

<p>â€¢powerMediation: Power/Sample size calculation for mediation analysis, simple linear 
regression, logistic regression, or longitudinal study</p>

<p>â€¢powerpkg: Power analyses for the affected sib pair and the TDT design</p>

<p>â€¢powerSurvEpi: Power and sample size calculation for survival analysis of epidemiological studies</p>

<p>â€¢PowerTOST: Power and Sample size based on two one-sided t-tests (TOST) for (bio)equivalence studies</p>

<p>â€¢longpower: Power and sample size for linear model of longitudinal data</p>

<p>Thanks!</p>
"
"0.0729972383233908","0.0750234484920533","125603","<p>I am new to the world of <strong>Regression</strong> in statistics and I have been doing a research in which I am building an ordinal logistic regression model (ORM). In order to fit my ORM model, I am using the 'orm' function of 'rms' package from R (<a href=""http://cran.r-project.org/web/packages/rms/rms.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/rms/rms.pdf</a>).</p>

<p>Now I am trying to assess the goodness of fit of my model. By reading the R documentation, I can see the following statement in the 'stat' property of the 'orm' object (pg.98):</p>

<p>""(...)Nagelkerke R2 index, the g-index, gr (the g-index on the odds ratio scale),
and <strong>pdm (the mean absolute difference between 0.5 and the predicted probability
that $Y\geq q$  the marginal median)</strong>(...).""</p>

<p>I don't have enough background to understand the short description of the pdm measure. But when I try to do more research on this measure, I am not able to find related material (e.g. I've been finding ""prescription drug misuse""). In summary, my question is:</p>

<p>Would you know if the 'pdm' measure has some synonym which is more widely used? Or can you provide some references where I can study the pdm metric?</p>
"
"0.0842899503922179","0.086629616364842","125764","<p>I'm trying to understand what factors contribute to the a certain outcome which is a ordered factor variable. In order to just understand which factor is statistically more significant than the others, I would like to build a model and given that my output variable is an ordered factorial variable - I thought I should go for Ordinal Logistic Regression. Given the tradeoff between interpretability and flexibility of models, in my case since I'm only making inferences and not predictions, should I rather go for a easier model to handle like Generalized Linear Models? It kind of boils down to me choosing the <code>polr</code> package in R versus the <code>glm</code> one.</p>
"
"NaN","NaN","125846","<p>Is there any package in R, which help us to plot QQ plot for log logistic distribution and 2 parameter exponential distribution? What methodology can be adopted for the same?</p>
"
"0.11150512337989","0.114600210537152","126338","<p>I'm running a binary prediction using a supervised topic modeling package in R (<code>lda</code> package, using <code>slda.predict</code> function). The result of the prediction returns results in linear space. From Googling around, people say that I need to take a sigmoid  to convert the result to a logical value. I'm not really sure what this means. </p>

<p>Basically I have list of documents, and their corresponding labels. What I am trying to do is set 80% of these documents and their labels, and train them using supervised LDA. The label of the document is 0 or 1. I manage to train the document just fine using this piece of code:</p>

<pre><code>example &lt;- c(""I am the role model"",""I have a major crazy   headache"",""i don't have money"", ""you are money crazy major"")
corpus = lexicalize(example, lower=TRUE)
label = c(1,1,0,0)
params &lt;- sample(c(1, 0), 2, replace=TRUE)
result &lt;- slda.em(documents=corpus$documents,
              K=2,
              vocab=poliblog.vocab,
              num.e.iterations=10,
              num.m.iterations=4,
              alpha=1.0, eta=0.1,
              label,
              params,
              variance=0.25,
              lambda=1.0,
              logistic=TRUE,
              method=""sLDA"")
</code></pre>

<p>for simplicity purpose, i'll try to predict the same document given the model above.</p>

<pre><code>predictions &lt;- slda.predict(corpus$documents,
                            result$topics, 
                        result$model,
                        alpha = 1.0,
                        eta=0.1)
</code></pre>

<p>Now, my problem is, the result of the prediction isn't binary. it's continuous value. I need to convert it back to binary using some sort of sigmoid(according to an <a href=""https://lists.cs.princeton.edu/pipermail/topic-models/2012-June/001912.html"" rel=""nofollow"">article here</a>) </p>

<p>The result i'm getting doesn't seem like a probability. For the 4 documents above, this is the output of the predictions variable</p>

<pre><code>           [,1]
[1,]  44.827420
[2,]  53.895682
[3,] -17.139034
[4,]   1.299764
</code></pre>

<p>How do I do this in R?</p>
"
"0.0753912235588337","0.077483884422606","129260","<p>I'm currently estimating a survival model (accelerated failure time model) with a log-logistic distribution in R using the survival package and the survreg function. I want to simulate expected survival times in line with King et al (2001), but I am unsure of the link function needed to calculate the expected survival time for the log-logistic distribution from the survreg regression output. I have added a minimal working example below.</p>

<pre><code>library(survival)
data(kidney)

survreg(formula = Surv(time, status) ~ 
                  age + 
                  cluster(id), 
                  data = kidney, 
                  dist = ""loglogistic"", 
                  robust = TRUE)

               Value Std. Err (Naive SE)     z        p
(Intercept)  4.38127   0.6783     0.5338  6.46 1.05e-10
age         -0.00298   0.0135     0.0114 -0.22 8.26e-01
Log(scale)  -0.23009   0.0732     0.1038 -3.14 1.67e-03

Scale= 0.794 

Log logistic distribution
Loglik(model)= -342   Loglik(intercept only)= -342
    Chisq= 0.07 on 1 degrees of freedom, p= 0.79 
(Loglikelihood assumes independent observations)
Number of Newton-Raphson Iterations: 3 
n= 76 
</code></pre>

<p>I simply want to know how I can calculate expected survival time from the estimated parameters from the survreg output.</p>

<p>King, G., Tomz, M., &amp; Wittenberg, J. (2000). Making the most of statistical analyses: Improving interpretation and presentation. American journal of political science, 347-361.</p>
"
"0.11150512337989","0.114600210537152","129298","<p>I would like to get the optimal cutoff of an ROC curve relating to a logistic regression.
I am using the roc from the R package pROC. I am assuming same cost of false negative and false positive using youden's J statistics max(sensitivity+specificity).
I have variable status (binary) and primary variable test (continuous).</p>

<p>roc(status, test, print.thres=T, print.auc=T, plot=T)
Gives me a cutoff of 27.150</p>

<p>I searched on this forum for suggestions and they doesn't seem to give me the right cutoff</p>

<p>I used logistic regression, and I get the parameter value 14.25199 and -0.59877.
Using the parameter values:</p>

<p>roc(status, 14.25199-0.59877*test, print.thres=T, print.auc=T, plot=T)</p>

<p>Gives me a cutoff of -2.005</p>

<p>And another suggestion, is to use the probability instead.</p>

<p>prob=predict(glm(status~test, family=binomial),type=c(""response""))</p>

<p>roc(status, prob, print.thres=T, print.auc=T, plot=T)</p>

<p>Gives me a cutoff of 0.119</p>

<p>As you can see none of the method work. Both method gives the correct AUC but not the cutoff/threshold. The correct method should give me cutoff of 27.150.
What is the correct x form to input to get the correct optimal cutoff/threshold from the command roc(status, x,â€¦.)</p>
"
"0.139779069524328","0.143658966607306","130035","<p>I'm trying to train multiple Multi-Layer-Perceptrons using foreach and mlp(...) from the RSNNS package. As I ran into problems reproducing the results generated by mlp() and foreach in a parallel environment, I created a minimal example without a parallel backend:</p>

<pre><code>library(foreach)
set.seed(42)
a &lt;- foreach(i=1:10) %do% { runif(3) }
set.seed(42)
b &lt;- foreach(i=1:10) %do% { runif(3) }
identical(a, b)
TRUE
</code></pre>

<p>This works as intended. a and b are identical. The next example is a modified version of the RSNNS demo.</p>

<pre><code>set.seed(42)
library(foreach)
library(RSNNS)
data(iris)

#shuffle the vector
iris &lt;- iris[sample(1:nrow(iris),length(1:nrow(iris))),1:ncol(iris)]

irisValues &lt;- iris[,1:4]
irisTargets &lt;- decodeClassLabels(iris[,5])

iris &lt;- splitForTrainingAndTest(irisValues, irisTargets, ratio=0.15)
iris &lt;- normTrainingAndTestSet(iris)

hiddenUnits = c(5,10)

set.seed(42)
trainedModels &lt;- foreach(i=hiddenUnits, .packages='RSNNS') %do% {

        mlp(iris$inputsTrain, iris$targetsTrain, 
            size = i,
            maxit = 50,
            initFunc = ""Randomize_Weights"", initFuncParams = c(-0.3, 0.3),
            learnFunc = ""BackpropWeightDecay"",
            learnFuncParams = c(0.4, 0, 0, 0),
            updateFunc = ""Topological_Order"", updateFuncParams = c(0),
            hiddenActFunc = ""Act_Logistic"",
            shufflePatterns = F, linOut = FALSE,
            inputsTest = iris$inputsTest, targetsTest=iris$targetsTest,
            pruneFunc = NULL, pruneFuncParams = NULL)}

set.seed(42)
trainedModels2 &lt;- foreach(i=hiddenUnits, .packages='RSNNS') %do% {

        mlp(iris$inputsTrain, iris$targetsTrain, 
            size = i,
            maxit = 50,
            initFunc = ""Randomize_Weights"", initFuncParams = c(-0.3, 0.3),
            learnFunc = ""BackpropWeightDecay"",
            learnFuncParams = c(0.4, 0, 0, 0),
            updateFunc = ""Topological_Order"", updateFuncParams = c(0),
            hiddenActFunc = ""Act_Logistic"",
            shufflePatterns = F, linOut = FALSE,
            inputsTest = iris$inputsTest, targetsTest=iris$targetsTest,
            pruneFunc = NULL, pruneFuncParams = NULL)}

identical(trainedModels, trainedModels2)
FALSE
</code></pre>

<p>The results aren't identical. Is there a way to make the results reproducible? The goal would be to run the code in parallel using the package doParallel. But I can't even get the same results in sequential mode.</p>

<p><strong>Edit:</strong> As shufflePatterns = F, the only random component in the models should be the random initialisation of the weights. It seems like set seed(x) has no effect on the generation of the random starting weights.</p>

<p><strong>Edit2:</strong> Did some research and found this on the github site of RSNNS: </p>

<p>Changelog for version 0.4-6 (22-12-2014):</p>

<p>...</p>

<ul>
<li>now using R's random number generator instead of the system's one</li>
</ul>

<p>Updated to the new version but I still can't reproduce results, even without foreach loops:</p>

<pre><code>set.seed(42)
trainedModels &lt;- mlp(iris$inputsTrain, iris$targetsTrain, 
                      size = 5,
                      maxit = 50,
                      initFunc = ""Randomize_Weights"",initFuncParams = c(-0.3, 0.3),
                      learnFunc = ""BackpropWeightDecay"",
                      learnFuncParams = c(0.4, 0, 0, 0),
                      updateFunc = ""Topological_Order"", updateFuncParams = c(0),
                      hiddenActFunc = ""Act_Logistic"",
                      shufflePatterns = F, linOut = FALSE,
                      inputsTest = iris$inputsTest, targetsTest=iris$targetsTest,
                      pruneFunc = NULL, pruneFuncParams = NULL)
set.seed(42)
trainedModels2 &lt;- mlp(iris$inputsTrain, iris$targetsTrain, 
                      size = 5,
                      maxit = 50,
                      initFunc = ""Randomize_Weights"",initFuncParams = c(-0.3, 0.3),
                      learnFunc = ""BackpropWeightDecay"",
                      learnFuncParams = c(0.4, 0, 0, 0),
                      updateFunc = ""Topological_Order"", updateFuncParams = c(0),
                      hiddenActFunc = ""Act_Logistic"",
                      shufflePatterns = F, linOut = FALSE,
                      inputsTest = iris$inputsTest, targetsTest=iris$targetsTest,
                      pruneFunc = NULL, pruneFuncParams = NULL)
identical(trainedModels, trainedModels2)
FALSE
</code></pre>
"
"0.042144975196109","0.043314808182421","130414","<p>How to perform a logistic regression and/or SVM on sparse data in R?  I have $ 10^6 $ observations, $ 10^4 $ TRUE/FALSE features, and the data is sparse, i.e.</p>

<pre><code>1,0,0,0,0,0,1,0,0,...
0,0,0,1,0,0,0,0,0,...
0,0,0,0,0,0,1,0,1,...
...
</code></pre>

<p>The sparseness is about 99%.</p>

<p>The package should be able to estimate feature significances and/or perform feature selection of some kind.</p>
"
"0.059601995508215","0.0612563891831689","132787","<p>How do you calculate marginal effects of parameters of logit model in R uging package {glm}?</p>

<p>Are following codes correct?</p>

<pre><code>#### preparation ####
# dependent variable
yseed &lt;- rnorm(100)
y &lt;- ifelse(yseed &gt; 0, 1, 0)

# independent variables
x1 &lt;- rnorm(100, mean=100, sd=20)
x2 &lt;- rnorm(100, mean=50, sd=20)
X &lt;- cbind(1, x1, x2)ã€€
Xmean &lt;- apply(X, 2, mean)

#### analysis ####
# logit model
res &lt;- glm(y ~ x1 + x2, family=binomial(link=""logit""))
summary(res)

# Marginal effects (ME) calculation
LAMBDA &lt;- function(x) { 1 / (1 + exp(-x))}ã€€# cdf of standard logistic distribution

# ME of (intercept)
ME_1  &lt;- coef[1] * LAMBDA(Xmean %*% coef(res)) * (1 - LAMBDA(Xmean %*% coef(res)))
# ME of x1   
ME_x1 &lt;- coef[2] * LAMBDA(Xmean %*% coef(res)) * (1 - LAMBDA(Xmean %*% coef(res))) 
# ME of x2
ME_x2 &lt;- coef[3] * LAMBDA(Xmean %*% coef(res)) * (1 - LAMBDA(Xmean %*% coef(res))) 
</code></pre>
"
"0.11150512337989","0.114600210537152","133248","<p>Iâ€™m analyzing student performance data. In my dataset, each row corresponds to a student and each column contains several performance metrics (continuous) and the student type (categorical, 4 types). The student type was computed in another analysis, using Expectation-Maximization, based on how students were graded over time. My sample is small, with 50 students.</p>

<p>I want to understand what characterizes each student type, regarding the performance features I have. I want to understand things like â€œthe more grade they have the more likely is to belong to a particular clusterâ€ and so on, if they are present at all in my data.</p>

<p>My first question is:
1) I believe that what I need is Multinomial Logistic Regression. Am I right or is there a better way to achieve this?</p>

<p>If yes, Iâ€™ve been exploring Multinomial Logistic Regression in R, using the multinom of the nnet package, but I need help with the following:</p>

<p>2) Understanding if the model is any good. So far I have the percentage of correctly classified instances, but I know this is not a very good goodness of fit measure. </p>

<p>3) How to assess how good each individual predictor is. I know how to look for the exponentiated B, but I donâ€™t know how to assess its significance. I read that using the t-distribution to compute the p-value here is usually a mistake. I found a <a href=""http://stats.stackexchange.com/questions/63222/getting-p-values-for-multinorm-in-r-nnet-package"">similar post here</a>, but a clear answer was not provided.</p>

<p>Thank you in advance for any answer, suggestion or comment.</p>
"
"0.183705687857867","0.178867773119389","133571","<p>I know there are already lots of questions around this topic (especially <a href=""http://stats.stackexchange.com/questions/16390/when-to-use-generalized-estimating-equations-vs-mixed-effects-models/16415#16415"">this one</a> and <a href=""http://stats.stackexchange.com/questions/24689/interpreting-coefficients-of-ordinal-logistic-regression-when-there-is-clusterin/29701#29701"">this one</a>) but I haven't really seen anything that directly helps me (It will be obvious I'm not a great statistician, but I'll do my best to explain). </p>

<p>I am running an ordinal regression in R (<code>clm</code> and <code>clmm</code>). My response variable is a rating between 0 and 4. I have two types of explanatory variables: individual and scenario variables [let's say <code>IVs</code> and <code>SVs</code>]. </p>

<p>Six different scenario variables (all dummies with at most 4 different values) represent potential collaboration scenarios that get rated by the respondent (between 0 and 4) creating the response variable. (Research design is a conjoint analysis; there are a total of 192 different scenarios possible)</p>

<p>On top of that I have a variety of individual characteristics about the respondent (age, gender, work experience, networking skills, ...) all derived from a survey.</p>

<p>Every respondent rates between 3 and 16 different scenarios (average 8.1); every scenario is rated by at least 8 respondents. Every respondent and every scenario have a unique identifier (called <code>IVid</code> and <code>SVid</code>). So they are non nested within each other.</p>

<p>Thus the basic regression looks like this:</p>

<pre><code>clm.base &lt;- clm(rating ~ SVs + IVs, data = dt) 
</code></pre>

<p>The hypothesis I am trying to test is that there are specific individual characteristics, that will influence the rating of the scenarios, independent of the actual content of the scenarios. Basically, some people are more or less favourable to all types of collaboration scenarios. </p>

<p>Now a reviewer of my paper asks me to include individual fixed effects (which in management [my field] basically means dummies for each individual). My assumption originally was that this would result in all individual variables being dropped. This is exactly what happens when I use another model (package <code>lfe</code>)</p>

<pre><code>felm.complete &lt;- felm(rating ~ SVs + IVs | SVid + IVid | 0 | IVid, data = dt) 
</code></pre>

<p>In this regression basically all my variables are perfectly collinear as expected.
However, when I approximate this in the ordinal package, there is no perfect collinearity. I presume this is related that <code>clmm</code> adds so-called 'random effects'. The regression takes a couple of minutes to run but eventually returns results</p>

<pre><code>clmm.complete &lt;- clmm(rating ~ SVs + IVs + (1|SVid) + (1|IVid), data = dt)
</code></pre>

<p>Now, the results here are pretty useless:</p>

<ul>
<li>All but one of my IVs are insignificant</li>
</ul>

<p>I am trying to understand what exactly happens when adding the <code>(1|IVid)</code> term in the <code>clmm</code> model. If it basically adds something like an individual dummy than the fact almost everything is now insignificant is no surprise. The coefficients of the <code>IVid</code> dummies would capture the effect I am looking for (some people rate all scenarios higher or lower, regardless of scenario content) most accurately.</p>

<p>Now I wonder whether this interpretation is correct or whether the results I got from running the simple <code>clm</code> regression are just not reliable? </p>

<p>Concretely, I'd like to find out:</p>

<ul>
<li>What happens when adding a random effect to <code>clmm</code></li>
<li>A laymen explanation of how the Laplace approximation works</li>
<li>How to group errors around individuals when running <code>clm</code></li>
<li>Is it possible to extract the coefficients of these random effects <code>(1|id)</code> for as far as there is such a thing?</li>
</ul>
"
"0.10323368445272","0.106099178353461","134335","<p>I am using <a href=""http://cran.r-project.org/web/packages/twang/vignettes/mnps.pdf"" rel=""nofollow"">mnsp</a> function to estimate propensity scores for multiple treatments. Then, I generate weights using the survey package, but I cannot use svyglm to estimate my treatment effects because my outcome is not binary or numeric. </p>

<p>My outcome variable has 3 categories, therefore I want to estimate relative risk ratios by running a multinomial logistic model. However, survey package does not allow multinomial logit. It seems like mlogit function allows weights. Would it be fine to just plug in the weights I derived from the get.weights function? </p>

<p>I am a novice in R, any recommendations are welcome. Here is my R code:</p>

<pre><code>DAT &lt;-read.delim(""DAT.txt"", header=TRUE)
library(twang)
set.seed(1)
mnps.DAT &lt;- mnps(city_ber ~ age + unemplyd + student + moedum + faedum,
                data = TIES, estimand = ""ATE"", verbose = FALSE,
               stop.method = c(""es.mean"", ""ks.mean""),
                n.trees = 10000)
#Diagnostics
plot(mnps.DAT, plots = 1)
plot(mnps.DAT, plots = 2)
plot(mnps.DAT, plots = 3, pairwiseMax = FALSE, figureRows = 3)
means.table(mnps.DAT, stop.method = ""es.mean"", digits = 3)

#Generating Weights
require(survey)
DAT$w &lt;- get.weights(mnps.DAT, stop.method = ""es.mean"")
    design.mnps &lt;- svydesign(ids=~1, weights=~w, data=DAT)
    summary(DAT$w)

#Outcome analysis?
</code></pre>
"
"NaN","NaN","135424","<p>I have a 2 class classification problem that I'm trying to optimize the log-loss for (not ROC/AUC).  I'm using the <code>glmnet</code> package in <code>R</code> but am unsure how to set the code up.  </p>

<p>I'm assuming that the <code>family</code> argument should be <code>'binomial'</code> but am uncertain if I should use the <code>link = 'logistic'</code> or <code>'log'</code> and whether I need to adjust the <code>type.measure</code> argument.</p>

<p>Any suggestions?</p>
"
"0.119946702195905","0.136973450269748","135967","<p>I am a beginner in R. I am doing logistic regression using around 80 independent variables using <code>glm</code> function in R. The dependent variable is <code>churn</code> which says whether a customer churned or not. I want to know how to identify the right combination of variables to get a good predictive logistic regression model in R.  I also want to know how to identify the same for making good decision tree in R ( I am using the <code>ctree</code> function from the <code>party</code> package).
So far, I had used <code>drop1</code> function  and  <code>anova(LogMdl, test=""Chisq"")</code> where <code>LogMdl</code> is my logistic regression model to drop unwanted variables in the predictive model.  But maximum accuracy I was able to achieve was only 60%. </p>

<p>Also I am not sure if I am using the <code>drop1</code> and <code>anova</code> functions correctly. I dropped the variables with lowest AIC using <code>drop1</code> function.  Using <code>anova</code> function, I dropped variables with p value > 0.05</p>

<p>Kindly help me how to identify the right set of variables for both logistic regression and decision tree models to increase my model's predictive accuracy to close to 90% or more than that if possible.   </p>

<pre><code>library(party)
setwd(""D:/CIS/Project work"")
CellData &lt;- read.csv(""Cell2Cell_SPSS_Data - Orig.csv"")
trainData &lt;- subset(CellData,calibrat==""1"")
testData &lt;- subset(CellData,calibrat==""0"") # validation or test data set
LogMdl = glm(formula=churn ~ revenue  + mou    + recchrge+ directas+ 
               overage + roam    + changem +
               changer  +dropvce + blckvce + unansvce+ 
               custcare+ threeway+ mourec  +
               outcalls +incalls + peakvce + opeakvce+ 
               dropblk + callfwdv+ callwait+
               months  + uniqsubs+ actvsubs+  phones  + models  +
               eqpdays  +customer+ age1    + age2    + 
               children+ credita + creditaa+
               creditb  +creditc + creditde+ creditgy+ creditz + 
               prizmrur+ prizmub +
               prizmtwn +refurb  + webcap  + truck   + 
               rv      + occprof + occcler +
               occcrft  +occstud + occhmkr + occret  + 
               occself + ownrent + marryun +
               marryyes +marryno + mailord + mailres + 
               mailflag+ travel  + pcown   +
               creditcd +retcalls+ retaccpt+ newcelly+ newcelln+ 
               refer   + incmiss +
               income   +mcycle  + creditad+ setprcm + setprc  + retcall, 
               data=trainData, family=binomial(link=""logit""),
               control = list(maxit = 50))
ProbMdl = predict(LogMdl, testData, type = ""response"")
testData$churndep = rep(0,31047)  # replacing all churndep with zero
testData$churndep[ProbMdl&gt;0.5] = 1   # converting records with prob &gt; 0.5 as churned
table(testData$churndep,testData$churn)  # comparing predicted and actual churn
mean(testData$churndep!=testData$churn)    # prints the error %
</code></pre>

<p>Link for documentation of variables: <a href=""https://drive.google.com/file/d/0B9y78DHd3U-DZS05VndFV3A4Ylk/"" rel=""nofollow"">https://drive.google.com/file/d/0B9y78DHd3U-DZS05VndFV3A4Ylk/</a></p>

<p>Link for Dataset (.csv file) : 
<a href=""https://drive.google.com/file/d/0B9y78DHd3U-DYm9FOV9zYW15bHM/"" rel=""nofollow"">https://drive.google.com/file/d/0B9y78DHd3U-DYm9FOV9zYW15bHM/</a></p>

<p>I could not produce the output of <code>dput</code> since the data size is more than 5 MB. So I have zipped the file and placed in the above link. </p>

<p>Description of important variables:
* <code>churn</code> is the variable that says whether a customer churned or not.....
* <code>churndep</code> is the variable that needs to be predicted in the test data (validation data) and has to be compared with the <code>churn</code> variable which is already populated with actual churn.
For both churn and churndep, value of 1 means churned and 0 means not churned.</p>
"
"0.042144975196109","0.043314808182421","137148","<p>I am looking for advice on what package and syntax to use in R for modelling a logistic multilevel analysis with variables as follows:</p>

<ul>
<li>Dependent variable (binary) = 0 or 1</li>
<li>Independent variable 1 (fixed effect, 2 levels) = A or B</li>
<li>Independent variable 2 (10 levels, cluster-randomized based on number of subjects, 6 nested within A and 4 nested within B) = A1, A2, A3, A4, A5, A6 &amp; B1, B2, B3, B4</li>
<li>Units of measure (rows) = individual subjects</li>
</ul>

<p>I am interested in extracting an odds ratio with 95% CI for the fixed effect. Any advice or resources are much appreciated, thanks!</p>
"
"0.10323368445272","0.106099178353461","137424","<p>I am fitting a binomial logistic regression in R using glm. By chance, I have found out that if I change the order of my predictor variables, glm fails to estimate the model. The message I get is  <em>unexpected result from lpSolveAPI for primal test</em>. </p>

<p>I am using the safeBinaryRegression package, so I am confident there are no separation issues between my outcome and predictor variables. However, I am not so confident that there are no quasi-separation issues among my predictor variables. Am I correct that if this is the case, then I might be running into multicolinearity, and this is the source of glm not being able to fit the model? </p>

<p>If so, my question is for advice on how to approach the issue. Should I look for the predictor variables highly correlated and omit one of them? Is there any convenient way of doing so for 11 categorical predictors? </p>

<p>What I see right now: </p>

<pre><code>lModel &lt;- glm(mob_change ~ education + gender + start_age + income + dist_change + lu_change + dou_change + marriage + student2work + wh_change,
              data = regression_data, 
              family = binomial())
# Fine, and I can inspect the model. No predictor has std. error &gt; 1.05

# Now if I move the last variable (or any of the last three, for what I've tested) to
# be the first in predictor... 
lModel.3 &lt;- glm(mob_change ~ wh_change + gender + education + start_age + income + dist_change + lu_change + dou_change + marriage + student2work,
            data = regression_data, 
            family = binomial())

Error in separator(X, Y, purpose = ""find"") : 
  unexpected result from lpSolveAPI for primal test
</code></pre>
"
"0.059601995508215","0.0612563891831689","137901","<p>I am using the package MatchIt in R to perform propensity score matching. I have chosen to use nearest neighbor matching with a caliper of 0.2 and since in my case i have more cases than controls i have to use the replacement=TRUE option, so that a control can be used more than once.</p>

<p>The graphical histogram check is satisfying and the stand.mean differences are all small with a max of 0.03 (btw any other suggestions for testing the matching?)
I want to use the matched dataset to check the treatment effect after all the matching(perform logistic regression with mortality as outcome and treatment as explanatory variable now) and i am wondering if i should take into consideration the weights that were resulted from the matching. Since i used the replacement option not all observations have a weight of 1 anymore. Shall i use this somehow or can i just perform an unweighted final logistic regression on the matched data to estimate the effect of treatment.</p>
"
"0.188692116954597","0.193929710799258","138230","<p>I want to perform propensity score matching of observational data of an Intensive Care Unit in order to find out wheather hydroxyethyl starch is better or worse than colloids in terms of renal replacement therapy (RRT), Akute Kidney Injury (AKI) and mortality. </p>

<p>I use the MatchIt package in R (King et al. 2007 - <a href=""http://gking.harvard.edu/matchit"" rel=""nofollow"">http://gking.harvard.edu/matchit</a>). This package is quite well documented. But there are some things that I dont understand.
First I matched on sociodemographic covariates (as this seems standard protocol with matching): Gender, weight, height and age.
Nearest neighbor matching seems to have worked:</p>

<p>NN matching</p>

<pre><code>m.out.nn

Call: 
matchit(formula = treat1 ~ Geschlecht + Gewicht.kg + Groesse.cm + 
    Alter.bei.ITS.Aufn + BMI, data = hes.vs.kristall.clean, method = ""nearest"")

Sample sizes:
          Control Treated
All           359    3944
Matched       359     359
Unmatched       0    3585
Discarded       0       0

Treatment status treat1 is HES = yes , Colloids otherwise btw. I did a numerical balance check and balance actually WORSENED after matching. Overall as well as some of the covariates drastically:
Call:
matchit(formula = treat1 ~ Geschlecht + Gewicht.kg + Groesse.cm + 
    Alter.bei.ITS.Aufn + BMI, data = hes.vs.kristall.clean, method = ""nearest"")

Summary of balance for all data:
                   Means Treated Means Control SD Control Mean Diff eQQ Med eQQ Mean  eQQ Max
distance                  0.9168        0.9145     0.0131    0.0022  0.0018   0.0023   0.0388
Geschlecht                0.0041        0.0000     0.0000    0.0041  0.0000   0.0056   1.0000
Geschlechtm               0.6463        0.6100     0.4884    0.0363  0.0000   0.0362   1.0000
Geschlechtw               0.3496        0.3900     0.4884   -0.0403  0.0000   0.0390   1.0000
Gewicht.kg               79.1349       77.8930    18.2092    1.2419  2.0000   1.8462  30.0000
Groesse.cm              169.9184      169.9861    11.9693   -0.0677  0.0000   0.7604  30.0000
Alter.bei.ITS.Aufn       64.5950       63.4808    14.4918    1.1142  0.8000   1.2916   6.2000
BMI                      28.4858       27.8005    15.1559    0.6853  0.7080   2.1550 347.8520


Summary of balance for matched data:
                   Means Treated Means Control SD Control Mean Diff eQQ Med eQQ Mean  eQQ Max
distance                  0.9357        0.9145     0.0131    0.0212  0.0172   0.0212   0.0687
Geschlecht                0.0446        0.0000     0.0000    0.0446  0.0000   0.0446   1.0000
Geschlechtm               0.9053        0.6100     0.4884    0.2953  0.0000   0.2953   1.0000
Geschlechtw               0.0501        0.3900     0.4884   -0.3398  0.0000   0.3398   1.0000
Gewicht.kg               98.1744       77.8930    18.2092   20.2813 17.0000  20.2813  62.0000
Groesse.cm              164.7103      169.9861    11.9693   -5.2758  2.0000   5.4540  77.0000
Alter.bei.ITS.Aufn       72.5702       63.4808    14.4918    9.0894  7.0000   9.0894  26.5000
BMI                      44.1753       27.8005    15.1559   16.3748  6.7500  16.3748 258.9020

Percent Balance Improvement:
                   Mean Diff.   eQQ Med  eQQ Mean   eQQ Max
distance            -852.5593 -839.8420 -808.2029  -77.0723
Geschlecht          -998.6072    0.0000 -700.0000    0.0000
Geschlechtm         -714.0668    0.0000 -715.3846    0.0000
Geschlechtw         -742.6908    0.0000 -771.4286    0.0000
Gewicht.kg         -1533.1522 -750.0000 -998.5214 -106.6667
Groesse.cm         -7691.0845      -Inf -617.2161 -156.6667
Alter.bei.ITS.Aufn  -715.7611 -775.0000 -603.7093 -327.4194
BMI                -2289.4307 -853.3898 -659.8482   25.5712

Sample sizes:
          Control Treated
All           359    3944
Matched       359     359
Unmatched       0    3585
Discarded       0       0
</code></pre>

<p>How can this be possible?</p>

<p>I also did genetic matching (Sekhon 2011 - <a href=""http://sekhon.berkeley.edu/matching/"" rel=""nofollow"">http://sekhon.berkeley.edu/matching/</a>). This is a fancy algorithm that automatically optimizes covariate balance. There covariate balance has indeed improved (as it should have):</p>

<pre><code>Genetic matching
load(file=""m.out.genetic.RData"")
Numerical Balance Check 
summary(m.out.genetic)

Call:
matchit(formula = treat1 ~ Geschlecht + Gewicht.kg + Groesse.cm + 
    Alter.bei.ITS.Aufn, data = hes.vs.kristall.clean, method = ""genetic"")

Summary of balance for all data:
                   Means Treated Means Control SD Control Mean Diff eQQ Med eQQ Mean eQQ Max
distance                  0.9167        0.9147     0.0126    0.0021  0.0019   0.0022  0.0374
Geschlecht                0.0041        0.0000     0.0000    0.0041  0.0000   0.0056  1.0000
Geschlechtm               0.6463        0.6100     0.4884    0.0363  0.0000   0.0362  1.0000
Geschlechtw               0.3496        0.3900     0.4884   -0.0403  0.0000   0.0390  1.0000
Gewicht.kg               79.1349       77.8930    18.2092    1.2419  2.0000   1.8462 30.0000
Groesse.cm              169.9184      169.9861    11.9693   -0.0677  0.0000   0.7604 30.0000
Alter.bei.ITS.Aufn       64.5950       63.4808    14.4918    1.1142  0.8000   1.2916  6.2000


Summary of balance for matched data:
                   Means Treated Means Control SD Control Mean Diff eQQ Med eQQ Mean eQQ Max
distance                  0.9167        0.9164     0.0105    0.0003  0.0018   0.0021  0.0374
Geschlecht                0.0041        0.0000     0.0000    0.0041  0.0000   0.0056  1.0000
Geschlechtm               0.6463        0.6481     0.4782   -0.0018  0.0000   0.0364  1.0000
Geschlechtw               0.3496        0.3519     0.4782   -0.0023  0.0000   0.0392  1.0000
Gewicht.kg               79.1349       79.0556    15.9832    0.0793  2.0000   1.7801 30.0000
Groesse.cm              169.9184      170.0479    10.6992   -0.1296  0.0000   0.7703 30.0000
Alter.bei.ITS.Aufn       64.5950       64.7378    13.3160   -0.1428  0.8000   1.2440  6.2000

Percent Balance Improvement:
                   Mean Diff. eQQ Med eQQ Mean eQQ Max
distance              83.7418  3.8423   2.7923       0
Geschlecht             0.0000  0.0000  -0.5602       0
Geschlechtm           95.1066  0.0000  -0.5602       0
Geschlechtw           94.3414  0.0000  -0.5602       0
Gewicht.kg            93.6115  0.0000   3.5817       0
Groesse.cm           -91.3359  0.0000  -1.2969       0
Alter.bei.ITS.Aufn    87.1817  0.0000   3.6903       0

Sample sizes:
          Control Treated
All           359    3944
Matched       357    3944
Unmatched       2       0
Discarded       0       0
</code></pre>

<p>I also checked balance graphically and it did improve (despite being good pre-matching).</p>

<p>Now my questions are:</p>

<ol>
<li><p>Can I use the nearest neighbor matched data? How could I change this so that balance does improve? What kind of distance metric does Nearest neighbor matching use (by default) (Euclidean ?). Because with Euclidean the non-Boolean covariates (Gender) could be made more important than they are.</p></li>
<li><p>How can I perform analysis after matching? - How can I get the Average Treatment Effect (ATE) and the Average Treatment Effect on the Treated (ATET) in terms of HES for AKI, RRT and mortaility and does that make sense for these response variables (AKI, RRT and mortaility)? Or should I get the odds ratio for Akute Kidney Injury, renal replacement therapy and mortaility from the matched observational data? How do I get these values?
I know that MatchIt recommends using Zelig to get these values but that didn't seem to work with my data. 
Can I use logistic regression with the matched data to get the odds ratio of HES vs. Cristalloids of AKI, RRT and mortality ?</p></li>
</ol>
"
"0.126434925588327","0.129944424547263","138908","<p>I am looking at the effects role has an opportunities to collaborate between groups in a social network. At a basic level the data are modeled as:</p>

<pre><code>relRatio~role
</code></pre>

<p>With relative ratio being the percentage of teammates who are part of the subject's normal group. The data I have come from multiple time slices over the years, with some of the subjects being polled two or more times. Not every subject has multiple entries, nor does every subject with multiple entries have the same number of entries. From some advice I received it was suggested that I test the differences between groups using a random effect ANOVA model, which would be modeled (in R) as</p>

<pre><code>relRatio~role+Error(subjectId)
</code></pre>

<p>After trying to read up more on random effects ANOVA, I started to get the impression that linear mixed effects models (with the lmer) package are preferred over random effects ANOVA, although I have yet to see a clear distinction between the two. This leads to my first question: Which approach is best for modeling my data?</p>

<p>If it involves using the random effects ANOVA, I would greatly appreciate it if someone could recommend a resource for the process of interpreting the results.</p>

<p>My second question is, if the better approach is to use the mixed effects models, which I have tried, why do I get striping in my residuals?</p>

<p><img src=""http://i.stack.imgur.com/vqWjn.png"" alt=""enter image description here""></p>

<p>One guide suggested that I am dealing with categorical data, which requires the use of logistic regression. However, the dependent variable for my data is continuous, and the IV is categorical, which I thought LMM are supposed to handle. This leads to my second question - does my residual plot indicate something is wrong with the way I have modeled my data?</p>
"
"NaN","NaN","139756","<p>I'm working on a classification problem with continous and categorical predictors with Random Forests (RF). I'm particularly interested on RF as we avoid the specification of the functional form.</p>

<p>However when it comes to the partial dependance for categorical variables, I'm not sure how to interpret that. For instance, the partial dependence (with the command <code>partialPlot</code> in the <code>R</code> package <code>randomForest</code>) for a binary predictor would give two values, one for each category. My question is: how exactely do you interpret those values? The documentation of <code>partialPlot</code> is quite cryptic in this respect.</p>

<p>My confusion arises, I guess, because I'm used with usual logistic regression where with a dummy coding system you in general obtain the log-odds of the variable of interest against the baseline category. But with RF things are different... Any help is appreciable!</p>
"
"0.179706778051997","0.166225466399364","140600","<p>I'm running a fixed effects logistic regression in R. The model consists of a binary outcome and two binary predictors, with no interaction term. On the log-odds scale, and as an odds-ratio, the coefficient for one of the predictors (<code>carbf</code> in the mocked-up example below) indicates that the expected probability of Y=1 (""success"") is different between the two levels of the factor (i.e., the effect is significant). </p>

<p>When I use the <code>effects</code> package to get marginal predicted probabilities, the 95% CIs for the two levels of <code>carbf</code> overlap considerably, indicating there is no evidence of a difference in the expected probability of Y=1 between the two factor levels.</p>

<p>When I use the <code>mfx</code> package to get average marginal effects for the coefficients (i.e., for the expected <em>difference</em> in the probability of Y=1 between the two factor levels), I do get a significant difference.</p>

<p><strong>I'm confused as to whether this discrepancy is because:</strong> </p>

<p><strong>1) the output from the model and the <code>mfx</code> package is an expected <em>difference</em> in the probability of Y=1 between factor levels, rather than predicted probabilities for each level.</strong></p>

<p><strong>2) of the way the <code>effects</code> package is calculating the marginal effect.</strong> </p>

<p>In an effort to determine this, I modified the source code from the <code>mfx</code> package to give me average marginal effects for each level of the <code>carbf</code> factor. The 95% CIs for these predictions <em>do not</em> overlap, indicating a significant difference. This makes me wonder why I get such different results using the <code>effects</code> package. Or is it that I'm just confused about the difference between marginal effects for coefficients and for predicted probabilities?</p>

<pre><code>#####################################
# packages
library(effects)
library(mfx)
library(ggplot2)

# data
data(mtcars)
carsdat &lt;- mtcars
carsdat$carb &lt;- ifelse(carsdat$carb %in% 1:3, 0, 1)
facvars &lt;- c(""vs"", ""am"", ""carb"")
carsdat[, paste0(facvars, ""f"")] &lt;- lapply(carsdat[, facvars], factor)

# model
m1 &lt;- glm(vsf ~ amf + carbf, 
    family = binomial(link = ""logit""), 
    data = carsdat)
summary(m1)


#####################################
# effects package
eff &lt;- allEffects(m1)
plot(eff, rescale.axis = FALSE)
eff_df &lt;- data.frame(eff[[""carbf""]])
eff_df 

#   carbf   fit    se  lower upper
# 1     0 0.607 0.469 0.3808 0.795
# 2     1 0.156 0.797 0.0375 0.469


#####################################
# mfx package marginal effects (at mean)
mfx1 &lt;-logitmfx(vsf ~ amf + carbf, data = carsdat, atmean = TRUE, robust = FALSE)
mfx1 

#         dF/dx Std. Err.     z  P&gt;|z|
# amf1    0.217     0.197  1.10 0.2697
# carbf1 -0.450     0.155 -2.91 0.0037

# mfx package marginal effects (averaged)
mfx2 &lt;-logitmfx(vsf ~ amf + carbf, data = carsdat, atmean = FALSE, robust = FALSE)
mfx2

#         dF/dx Std. Err.     z  P&gt;|z|
# amf1    0.177     0.158  1.12 0.2623
# carbf1 -0.436     0.150 -2.90 0.0037


#####################################
# mfx source code
fit &lt;- m1
x1 = model.matrix(fit)  
be = as.matrix(na.omit(coef(fit)))
k1 = length(na.omit(coef(fit)))
fxb = mean(plogis(x1 %*% be)*(1-plogis(x1 %*% be))) 
vcv = vcov(fit)

# data frame for predictions
mfx_pred &lt;- data.frame(mfx = rep(NA, 4), se = rep(NA, 4), 
    row.names = c(""amf0"", ""amf1"", ""carbf0"", ""carbf1""))
disc &lt;- rownames(mfx_pred)

# hard coded prediction estimates and SE  
disx0c &lt;- disx1c &lt;- disx0a &lt;- disx1a &lt;- x1 
disx1a[, ""amf1""] &lt;- max(x1[, ""amf1""]) 
disx0a[, ""amf1""] &lt;- min(x1[, ""amf1""]) 
disx1c[, ""carbf1""] &lt;- max(x1[, ""carbf1""]) 
disx0c[, ""carbf1""] &lt;- min(x1[, ""carbf1""])
mfx_pred[""amf0"", 1] &lt;- mean(plogis(disx0a %*% be))
mfx_pred[""amf1"", 1] &lt;- mean(plogis(disx1a %*% be))
mfx_pred[""carbf0"", 1] &lt;- mean(plogis(disx0c %*% be))
mfx_pred[""carbf1"", 1] &lt;- mean(plogis(disx1c %*% be))
# standard errors
gr0a &lt;- as.numeric(dlogis(disx0a %*% be)) * disx0a
gr1a &lt;- as.numeric(dlogis(disx1a %*% be)) * disx1a
gr0c &lt;- as.numeric(dlogis(disx0c %*% be)) * disx0c
gr1c &lt;- as.numeric(dlogis(disx1c %*% be)) * disx1c
avegr0a &lt;- as.matrix(colMeans(gr0a))
avegr1a &lt;- as.matrix(colMeans(gr1a))
avegr0c &lt;- as.matrix(colMeans(gr0c))
avegr1c &lt;- as.matrix(colMeans(gr1c))
mfx_pred[""amf0"", 2] &lt;- sqrt(t(avegr0a) %*% vcv %*% avegr0a)
mfx_pred[""amf1"", 2] &lt;- sqrt(t(avegr1a) %*% vcv %*% avegr1a)
mfx_pred[""carbf0"", 2] &lt;- sqrt(t(avegr0c) %*% vcv %*% avegr0c)
mfx_pred[""carbf1"", 2] &lt;- sqrt(t(avegr1c) %*% vcv %*% avegr1c)  

mfx_pred$pred &lt;- rownames(mfx_pred)
    mfx_pred$lcl &lt;- mfx_pred$mfx - (mfx_pred$se * 1.96)
mfx_pred$ucl &lt;- mfx_pred$mfx + (mfx_pred$se * 1.96)

#          mfx    se   pred     lcl   ucl
# amf0   0.366 0.101   amf0  0.1682 0.563
# amf1   0.543 0.122   amf1  0.3041 0.782
# carbf0 0.601 0.107 carbf0  0.3916 0.811
# carbf1 0.165 0.105 carbf1 -0.0412 0.372

ggplot(mfx_pred, aes(x = pred, y = mfx)) +
    geom_point() +
    geom_errorbar(aes(ymin = lcl, ymax = ucl)) +
    theme_bw()
</code></pre>
"
"0.0753912235588337","0.0968548555282575","140929","<p>I wish to carry out logistic regression analysis using Firth's method, as implemented in R logistf package, to analyse SNP case/control data, for rare variants, whilst controlling for stratification using PCA eigenvectors as covariates. I wish to obtain p-values for each SNP (additive model).</p>

<p>Previously I have performed logistic regression analysis using PLINK:</p>

<pre><code>plink  --bfile snpdata --logistic --ci 0.95 --covar plink2_pca.eigenvec --covar-number 1-2 --out snpout
</code></pre>

<p>I would like to perform similar analysis, but wish to handle quasi-complete separation of the rare variants in my data sets.</p>

<p>I have followed a SNP analysis example provided with logistf and been able to obtain P values:</p>

<p>A very small sample of the snpdata (cases: case 1, control 0; SNP additive allele counts for minor allele: 0, 1, 2):</p>

<pre><code>           PC01         PC02 case exm226_A exm401_A exm4584_A exm146_A
1  -0.003092320 -0.002737810    1            0       0       0       0
2   0.015637300  0.008232330    1            0       0       0       0
3   0.006746730  0.008704400    1            0       1       0       1
4   0.001438270  0.000875751    0            0       0       0       0
5  -0.004161490  0.011407500    0            0       0       2       0

for(i in 1:ncol(snpdata)) snpdata[,i] &lt;-as.factor(snpdata[,i])
snpdata &lt;- snpdata[sapply(snpdata,function(x) length(levels(x))&gt;=2)] 
fitsnp &lt;- logistf(data=snpdata, formula=case~1, pl=FALSE)
add1(fitsnp)
</code></pre>

<p>But I am not clear on how to pass the eigenvectors in as covariates, or whether I can used the eigenvector values as is, or need to convert to these as factors?   </p>

<pre><code>fitsnp &lt;- logistf(data=snpdata,formula=case~(1+PC01+PC02), pl=FALSE)
</code></pre>

<p>I'm not sure if I am on the right track here and can't find a sufficiently similar example on-line to follow.</p>

<p>I would appreciate any assistance, or explanation if I am going completely wrong here.</p>

<p>Thanks in advance.</p>
"
"0.0729972383233908","0.0750234484920533","141203","<p>My question regards the use of the <code>gamlss</code> package. I am using <code>gamlss</code> package to fit a dataset to a logistic function. There is only one predictor, let me denote it with <code>x</code> and because the overall dependence is not exactly sigmoid, a better model is achieved by wrapping the predictor <code>x</code> in a smoother function. I chose the cubic spline smoother (<code>cs</code>). The response is binomial. I'll denote it with <code>y</code>:  </p>

<pre><code>y = cbind(number of successful events, total number - number of successful events). 
</code></pre>

<p>The R code is  the following:</p>

<pre><code>cs15 &lt;- gamlss(y~cs(x, df=15), sigma.fo=~1, family=BI,  data=mydata, trace=FALSE) 
</code></pre>

<p>I want to estimate predicted response for a set of predictors not contained in the original data set. I know this can be achieved with the function: </p>

<pre><code>cs15fit = predict(cs25, newdata=data.frame(x=xnew), type=""response"")
</code></pre>

<p>However, my problem is that I also want to estimate the standard errors, which should be done by adding <code>se.fit=T</code>:</p>

<pre><code>cs15fit=predict(cs25, newdata=data.frame(x=xnew), type=""response"", se.fit=T)
</code></pre>

<p>But the addition of <code>se.fit = T</code> produces the following error:</p>

<blockquote>
  <p>se.fit = TRUE is not supported for new data values at the moment</p>
</blockquote>

<p>Anyone know how I can still find standard errors for the new values? </p>
"
"0.163546526421265","0.17859152928949","141844","<p>I tried to plot the results of an ordered logistic regression analysis by calculating the probabilities of endorsing every answer category of the dependent variable (6-point Likert scale, ranging from ""1"" to ""6""). However, I've received strange probabilities when I calculated the probabilities based on this formula: $\rm{Pr}(y_i \le k|X_i) = \rm{logit}^{-1}(X_i\beta)$.</p>

<p>Below you see how exactly I tried to calculate the probabilities and plot the results of the ordered logistic regression model (<code>m2</code>) that I fitted using the <code>polr</code> function (<code>MASS</code> package). The probabilities (<code>probLALR</code>) that I calculated and used to plot an ""expected mean score"" are puzzling as the expcected mean score in the plot increases along the RIV.st continuum while the coefficient for <code>RIV.st</code> is negative (-0.1636). I would have expected that the expected mean score decreases due to the negative main effect of <code>RIV.st</code> and the irrelevance of the interaction terms for the low admiration and low rivalry condition (LALR) of the current 2 by 2 design (first factor = <code>f.adm</code>; second factor = <code>f.riv</code>; dummy coding 0 and 1).</p>

<p>Any idea of how to make sense of the found pattern? Is this the right way to calculate the probabilities? The way I used the intercepts in the formula to calculate the probabilities might be problematic (cf., <a href=""https://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression"">Negative coefficient in ordered logistic regression</a>).</p>

<pre><code>m2 &lt;- polr(short.f ~ 1 + f.adm*f.riv + f.adm*RIV.st + f.riv*RIV.st, data=sampleNS)

# f.adm  = dummy (first factor of 2 by 2 design);
# f.riv  = dummy (second factor of 2 by 2 design);
# RIV.st = continuous predictor (standardized)
summary(m2)
Coefficients:
                Value Std. Error t value
f.adm1         1.0203    0.14959  6.8203
f.riv1        -0.8611    0.14535 -5.9240
RIV.st        -0.1636    0.09398 -1.7403
f.adm1:f.riv1 -1.2793    0.20759 -6.1625
f.adm1:RIV.st  0.0390    0.10584  0.3685
f.riv1:RIV.st  0.6989    0.10759  6.4953

Intercepts:
    Value    Std. Error t value 
1|2  -2.6563   0.1389   -19.1278
2|3  -1.2139   0.1136   -10.6898
3|4  -0.3598   0.1069    -3.3660
4|5   0.9861   0.1121     8.7967
5|6   3.1997   0.1720    18.6008
</code></pre>

<p>Here you see how I tried to calculate the probabilities (<code>probLALR</code>) for 1 of the 4 conditions of the 2 by 2 design:</p>

<pre><code>inv.logit  &lt;- function(x){ return(exp(x)/(1+exp(x))) }
Pred       &lt;- seq(-3, 3, by=0.01)
b = c(-2.6563,-1.2139,-0.3598,0.9861,3.1997) # intercepts of model m2
a = c(1.0203,-0.8611,-0.1636,-1.2793,0.0390,0.6989) # coefficients of m2
probLALR   &lt;- data.frame(matrix(NA,601,5))
for (k in 1:5){ 
    probLALR[,k] &lt;- inv.logit(b[k] + a[1]*0 + a[2]*0 + 
                               a[3]*Pred  + a[4]*0*0 + 
                               a[5]*Pred*0 + a[6]*Pred*0)
}

plot(Pred,probLALR[,1],type=""l"",ylim=c(0,1))   # p(1)
lines(Pred,probLALR[,2],col=""red"")             # p(1 or 2)
lines(Pred,probLALR[,3],col=""green"")           # P(1 or 2 or 3)
lines(Pred,probLALR[,4],col=""orange"")          # P(1 or 2 or 3 or 4)
lines(Pred,probLALR[,5],col=""orange"")          # P(1 or 2 or 3 or 4 or 5)

# option response functions:

orc = matrix(NA,601,6)
orc[,6] = 1-probLALR[,5]        # prob of 6
orc[,5]= probLALR[,5]-probLALR[,4]  # prob of 5
orc[,4]= probLALR[,4]-probLALR[,3]  # prob of 4
orc[,3]= probLALR[,3]-probLALR[,2]  # prob of 3
orc[,2]= probLALR[,2]-probLALR[,1]  # prob of 2
orc[,1]= probLALR[,1]           # prob of 1


plot(Pred,orc[,1],type=""l"",ylim=c(0,1))   # p(1)
lines(Pred,orc[,2],col=""red"")             # p(2)
lines(Pred,orc[,3],col=""green"")           # P(3)
lines(Pred,orc[,4],col=""orange"")          # P(4)
lines(Pred,orc[,5],col=""purple"")          # P(5)
lines(Pred,orc[,6],col=""purple"")          # P(6)

# mean score

mean = orc[,1]*1+orc[,2]*2+orc[,3]*3+orc[,4]*4+orc[,5]*5+orc[,6]*6
plot(Pred,mean,type=""l"",xlab=""RIV.st"",ylab=""expected mean score"",ylim=c(1,6))  
</code></pre>
"
"NaN","NaN","143328","<p>I am developing a logistic regression model where perfect variable separation occurs. I want to calculate a cutoff from this data. Interestingly, the length of the slot <code>cutoffs</code> of <code>pred.obj</code> is only 5, as well as the slots <code>fp</code>, <code>tp</code>, <code>tn</code>, <code>fn</code>, <code>n.pos.pred</code> and <code>n.neg.pred</code>. I expect it to have the same length as the observations. </p>

<p>Has anybody an explanation for this? (And knows how to solve it?) </p>

<p>MWE:</p>

<pre><code> library(ROCR) # package for prediction/performance functions
 y &lt;- c(0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0)
 x &lt;- c(-5, 5, 3, -2, 4, 3, -8, 2, 5, 3, -5, -3, -2)
 model &lt;- glm(as.factor(y) ~ x, family = ""binomial"")
 preds &lt;- predict(model, type = ""response"")
 (pred.obj &lt;- prediction(preds, y))
 perf &lt;- performance(pred.obj, ""acc"")
 (cutoff &lt;- perf@x.values[[1]][which.max(perf@y.values[[1]])])
</code></pre>
"
"NaN","NaN","148648","<p>I'm planning to work on some credit risk models using logistic regresson in R. Binary response. What all goodness of fit tests are to be known and WHAT PACKAGES are required for the same? Thank you.
I found some help in this post but it is not exhaustive: <a href=""http://stats.stackexchange.com/questions/51275/what-is-the-difference-in-what-aic-and-c-statistic-auc-actually-measure-for-mo"">What is the difference in what AIC and c-statistic (AUC) actually measure for model fit?</a></p>
"
"0.163226787060855","0.167757530734685","148699","<p>For a current piece of work Iâ€™m trying to model the probability of tree death for beech trees in a woodland in the UK. I have records of whether trees were alive or dead for 3 different census periods along with data on their diameter and growth rate. Each tree has an ID number so it can be identified at each time interval. However, the census intervals vary so that for the time between one survey and another is either 4, 12 or 18 years. Obviously the longer the census period the greater the probability a tree will have died by the time it is next surveyed. <strong>I had problems making a realistic reproducible example so you can find the <a href=""https://github.com/PhilAMartin/Denny_mortality/blob/master/Data/Stack_dead.csv"" rel=""nofollow"">data here</a>.</strong></p>

<p>The variables in the dataset are:</p>

<ol>
<li>ID - Unique ID for tree</li>
<li>Block - the ID for the 20x20m plot in which the tree was located</li>
<li>Dead - Status of tree, either dead (1) or alive (0)</li>
<li>GR - Annual growth rate from previous survey</li>
<li>DBH - diameter of tree at breast height</li>
<li>SL - Length of time between censuses in years</li>
</ol>

<p>Once a tree is recorded as dead it disappears from subsequent surveys.</p>

<p>Ideally I would like to be able to estimate the annual probability of mortality of a tree using information on diameter and growth rate. Having searched around for quite a while I have seen that logistic exposure models appear able to account for differences in census periods by using an altered version of logit link for binomial models as detailed by Ben Bolker <a href=""https://rpubs.com/bbolker/logregexp"" rel=""nofollow"">here</a>. This was originally used by Shaffer to determine the daily probability of bird nest survival where the age (and therefore exposure) of the nest differed. I've not seen it used outside of the context of models of nest survival but it seems like I should be able to use it to model survival/mortality where the exposure differs.</p>

<pre><code>require(MASS)
logexp &lt;- function(exposure = 1)
{
  linkfun &lt;- function(mu) qlogis(mu^(1/exposure))
  ## FIXME: is there some trick we can play here to allow
  ##   evaluation in the context of the 'data' argument?
  linkinv &lt;- function(eta)  plogis(eta)^exposure
  logit_mu_eta &lt;- function(eta) {
    ifelse(abs(eta)&gt;30,.Machine$double.eps,
           exp(eta)/(1+exp(eta))^2)
    ## OR .Call(stats:::C_logit_mu_eta, eta, PACKAGE = ""stats"")
  }
  mu.eta &lt;- function(eta) {       
    exposure * plogis(eta)^(exposure-1) *
      logit_mu_eta(eta)
  }
  valideta &lt;- function(eta) TRUE
  link &lt;- paste(""logexp("", deparse(substitute(exposure)), "")"",
                sep="""")
  structure(list(linkfun = linkfun, linkinv = linkinv,
                 mu.eta = mu.eta, valideta = valideta, 
                 name = link),
            class = ""link-glm"")
}
</code></pre>

<p>At the moment my model looks like this, but I will incorporate more variables as I go along:</p>

<pre><code>require(lme4)
Dead&lt;-read.csv(""Stack_dead.csv"",)


M1&lt;-glmer(Dead~DBH+(1|ID),data=Dead,family=binomial(logexp(Dead$SL))) 
#I use (1|ID) here to account for the repeated measurements of the same individuals
    summary(M1)

plot(Dead$DBH,plogis(predict(M1,re.form=NA)))
</code></pre>

<p><strong>Primarily I want to know</strong>:</p>

<ol>
<li><strong>Does the statistical technique I am using to control for the difference in time between census seem sensible? If it isn't, can you think of a better way to deal with the problem?</strong></li>
<li><strong>If the answer to the first question is yes, is using the inverse logit (plogis) the correct way to get predictions expressed as probabilities?</strong></li>
</ol>

<p>Thanks in advance for any help!</p>
"
"0.126434925588327","0.115506155153123","149012","<p><a href=""http://en.wikipedia.org/wiki/Discrete_choice#F._Logit_with_variables_that_vary_over_alternatives_.28also_called_conditional_logit.29"" rel=""nofollow"">Conditional logistic regression</a> is a <a href=""http://en.wikipedia.org/wiki/Fixed_effects_model"" rel=""nofollow"">fixed effects model</a>. If you're modeling the dependent variable $y$, a glm fixed effect model doesn't actually model $y$. Instead, the glm fixed effect models measure $y-mean(y)$ for a particular group. I think that this is <em>not</em> the case for a conditional logistic regression. The coefficients of the regression can be interpreted in the space of $y$. Is that correct?</p>

<p>My particular situation:
I am running a conditional logit with <a href=""https://stat.ethz.ch/R-manual/R-devel/library/survival/html/clogit.html"" rel=""nofollow"">clogit</a> in R, from the <code>survival</code> package. Are the coefficients returned to be interpreted in the space of $y$, or in the space of something like $y-mean(y)$? </p>

<p>Normally the difference isn't very relevant; one would interpret the coefficient roughly the same either way. However, in my case one of the independent variables is fitted as a spline. Specifically, it is a restricted cubic spline, as calculated from <code>rcspline.eval</code> in the <a href=""http://cran.r-project.org/web/packages/Hmisc/Hmisc.pdf"" rel=""nofollow"">Hmisc</a> package. <code>clogit</code> produces a coefficient for each knot of the spline, and in order to interpret the overall effect of the variable one needs to reconstruct the spline from the coefficients (using <code>rcspline.restate</code>). I want to make sure that I should be looking at the shape of this spline in the range of $y$ (which in my case is 0-100) or in the range of something like $y-mean(y)$ (in this case, $mean(y)$ is the same for all groups: 50). If it is the case that the space is shifted this will be particularly weird for a spline, because presumably the knots should also be shifted somehow.</p>
"
"0.0729972383233908","0.0750234484920533","149140","<p>Currently I am working on a large data set with well over 200 variables (238 to be exact) and 290 observations for each variable (in theory). This data set is missing quite a lot of values, with variables ranging from 0-100% 'missingness'. I will eventually be performing logistical regression on this data, so of my 238 columns I will at most only be using ten or so.</p>

<p>However as almost all of my columns are missing some data, I am turning to multiple imputation to fill in the blanks (using the MICE package).</p>

<p>My question is; given that I have a large amount of variation in the missing data, at what percentage missing should I start to exclude variables from the mice() function? </p>

<p>Can mice function well with variables that are missing 50% of their values? What about 60%, 70%, 80%, 90%?</p>
"
"0.042144975196109","0.043314808182421","151600","<p>Has anyone written a package for R that can do a logistic regression over categorical variables (like <code>glm</code>) but with the constraint, and I do realize this is weird, that <em>all the residuals must be nonnegative?</em>  (In response space, not link space.  In other words, the predicted probability in each cell must come out less than or equal to the observed probability in that cell.) Alternatively, is there a straightforward way to transform a <code>glm</code> problem so that it will come out with nonnegative residuals?</p>

<p>I know I can probably persuade <code>optim</code> to do what I want but if a shortcut exists that sure would be nice.</p>
"
"0.0942390294485422","0.0968548555282575","151915","<p>I've performed a logistic regression with L-BFGS on R and noticed that if I changed the initialization, the model retuned was different.</p>

<p>Here is my dataset (390 obs. of 14 variables, Y is the target variable) :</p>

<pre><code>GEST    DILATE    EFFACE    CONSIS    CONTR    MEMBRAN    AGE    STRAT    GRAVID    PARIT    DIAB    TRANSF    GEMEL    Y
31           3       100         3        1         2     26         3         1        0       2         2       1     1
28           8         0         3        1         2     25         3         1        0       2         1       2     1
31           3       100         3        2         2     28         3         2        0       2         1       1     1
...
</code></pre>

<p>This dataset is found here: <a href=""http://tutoriels-data-mining.blogspot.fr/2008/04/rgression-logistique-binaire.html"" rel=""nofollow"">http://tutoriels-data-mining.blogspot.fr/2008/04/rgression-logistique-binaire.html</a> in ""DonnÃ©es : prematures.xls"". Y is a column I created with the column ""PREMATURE"", Y=IF(PREMATURE=""positif"";1;0)</p>

<p>I've used the optimx package like here <a href=""http://stats.stackexchange.com/questions/17436/logistic-regression-with-lbfgs-solver"">Logistic regression with LBFGS solver</a>, here is the code: </p>

<pre><code>install.packages(""optimx"")
  library(optimx)

vY = as.matrix(premature['PREMATURE'])
# Recoding the response variable
vY = ifelse(vY == ""positif"", 1, 0)

mX = as.matrix(premature[c('GEST', 'DILATE', 'EFFACE', 'CONSIS', 'CONTR', 
                           'MEMBRAN', 'AGE', 'STRAT', 'GRAVID', 'PARIT', 
                           'DIAB', 'TRANSF', 'GEMEL')])

#add an intercept to the predictor variables
mX = cbind(rep(1, nrow(mX)), mX)

#the number of variables and observations
iK = ncol(mX)
iN = nrow(mX)

#define the logistic transformation
logit = function(mX, vBeta) {
  return(exp(mX %*% vBeta)/(1+ exp(mX %*% vBeta)) )
}

# stable parametrisation of the log-likelihood function
logLikelihoodLogitStable = function(vBeta, mX, vY) {
  return(-sum(
    vY*(mX %*% vBeta - log(1+exp(mX %*% vBeta)))
    + (1-vY)*(-log(1 + exp(mX %*% vBeta)))
  )  # sum
  )  # return 
}

# score function
likelihoodScore = function(vBeta, mX, vY) {
  return(t(mX) %*% (logit(mX, vBeta) - vY) )
}

# initial set of parameters (arbitrary starting parameters)
vBeta0 = c(10, -0.1, -0.3, 0.001, 0.01, 0.01, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01)

optimLogitLBFGS = optimx(vBeta0, logLikelihoodLogitStable,
                         method = 'L-BFGS-B', gr = likelihoodScore, 
                         mX = mX, vY = vY, hessian=TRUE)
</code></pre>

<p>I get this :</p>

<pre><code> optimLogitLBFGS
                p1         p2       p3         p4         p5         p6
L-BFGS-B 9.720242 -0.1652943 0.525449 0.01681583 0.02781123 -0.3921004
                 p7          p8         p9       p10        p11        p12
L-BFGS-B -1.694412 -0.03461208 0.02759248 0.1993573 -0.6718275 0.02537887
                 p13      p14   value fevals gevals niter convcode  kkt1  kkt2
L-BFGS-B -0.8374338 0.625044 187.581    121    121    NA        1 FALSE FALSE
          xtimes
L-BFGS-B  0.044
</code></pre>

<p>But if I change </p>

<pre><code>vBeta0 = c(10, -0.1, -0.3, 0.001, 0.01, 0.01, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01)
</code></pre>

<p>in</p>

<pre><code>vBeta0 = rep(0.1, iK)
</code></pre>

<p>I get a different result :</p>

<pre><code>optimLogitLBFGS
                 p1             p2             p3              p4               p5
L-BFGS-B 0.372672689046 0.206785276091 0.398104550108 0.0175008380158 -0.0460042719084
                 p6             p7               p8            p9            p10
L-BFGS-B 0.139760396213 -1.43192069477 -0.0207666651106 -1.1396642657 0.212186387416
                 p11             p12             p13            p14         value
L-BFGS-B -0.583698421298 0.0576485672766 -0.802789658686 0.993103617257 185.472518798
         fevals gevals niter convcode  kkt1  kkt2 xtimes
L-BFGS-B    121    121    NA        1 FALSE FALSE   0.05
</code></pre>

<p>How can I choose the initial parameters to get the best model?</p>
"
"0.139779069524328","0.143658966607306","151961","<p>*Please note this question is about the Platt probabilistic output and SVM class assignment, not about the code or the package itself. It just happens to be the code where I stumbled on the issue.</p>

<p>In <a href=""http://stats.stackexchange.com/questions/147260/including-class-probabilities-might-skew-a-model-in-caret"">another question</a> I asked about bad models coming from <code>caret</code> and associated <code>kernlab</code> when <code>prob.model=TRUE</code>. I found the answer myself, in both <a href=""http://stackoverflow.com/questions/29766951/different-results-with-caret-when-classprobs-true"">stackoverflow</a> and <a href=""http://r.789695.n4.nabble.com/Inconsistent-results-between-caret-kernlab-versions-td4680500.html"" rel=""nofollow"">from Max Kuhn himself</a>:</p>

<blockquote>
<pre><code>&gt; predict(newSVM, df[43,-1]) [1] O32078 10 Levels: O27479 O31403 O32057 O32059 O32060 O32078 ... O32676
&gt; predict(newSVM, df[43,-1], type = ""probabilities"")
     O27479     O31403     O32057    O32059    O32060     O32078
[1,] 0.08791826 0.05911645 0.2424997 0.1036943 0.06968587 0.1648394
     O32089     O32663     O32668     O32676
[1,] 0.04890477 0.05210836 0.09838892 0.07284396
</code></pre>
  
  <p>Note that, based on the probability model, the class with the largest
  probability is O32057 (p = 0.24) while the basic SVM model predicts
  O32078 (p = 0.16).</p>
  
  <p><strong>Somebody (maybe me) saw this discrepancy and that led to me to follow
  this rule:</strong></p>

<pre><code>if(prob.model = TRUE) use the class with the maximum probability   
  else use the class prediction from ksvm().
</code></pre>
  
  <p>Therefore:</p>

<pre><code>predict(svm.m1, df[43,-1])
 [1] O32057
 10 Levels: O27479 O31403 O32057 O32059 O32060 O32078 ... O32676
</code></pre>
</blockquote>

<p>Isn't that innacurate? <code>kernlab</code> searches for the optimal probability cutoff that minimizes error, that's why the assigned class and the maximum probability don't match: they don't have to.</p>

<p>Check this reproducible example. I excluded two cherrypicked <code>virginica</code> samples.</p>

<pre><code>require(kernlab);require(caret);
#kernel=polynomial; degree=3; scale=0.1; C=0.31
set.seed(101);SVM&lt;-ksvm(Species~., data=iris[-c(135,150),], kernel='polydot',C=.31, kpar=list( scale=.1, degree=3), prob.model=T)
</code></pre>

<p>Here's the resulting model </p>

<pre><code>&gt; SVM
Support Vector Machine object of class ""ksvm"" 

SV type: C-svc  (classification) 
 parameter : cost C = 0.31 

Polynomial kernel function. 
 Hyperparameters : degree =  3  scale =  0.1  offset =  1 

Number of Support Vectors : 58 

Objective Function Value : -1.4591 -0.7955 -10.2392 
Training error : 0.033784 
Probability model included. 
</code></pre>

<p>Now let's check the predicted class probabilities in those two samples</p>

<pre><code>&gt; predict(SVM, iris[c(135,150),-5], type=""probabilities"")
          setosa versicolor virginica
[1,] 0.008286638  0.4414114  0.550302
[2,] 0.013824451  0.3035556  0.682620
</code></pre>

<p>And the class predictions</p>

<pre><code>&gt; predict(SVM, iris[c(135,150),-5])
[1] versicolor virginica 
Levels: setosa versicolor virginica
</code></pre>

<p>Sample 150 was assigned to <code>virginica</code>, with a class probability of around 0.68. Sample 135 was assigned to <code>versicolor</code> with a probability of around 0.44, yet <code>virginica</code> probability nicely sits around 0.55.
Looking at several CV folds, we perceive that kernlab only assigns <code>virginica</code> when its probability is over a given value (way higher than 0.5). That's the cutoff I mentioned, and it happens thanks to the well known bad clustering in <code>iris</code> between <code>virginica</code> and <code>versicolor</code>.</p>

<p>So, am I right on these suppositions and therefore is <code>caret</code> class assignment model (maximum probability) wrong?</p>

<p>EDIT:
I've been experimenting with pairwise probability coupling of Platt scaling (logistic regression fit), isotononic regression and a model I'm working on. A weakness (?) I perceived in Platt's model is the probability isn't bound to be 0.5 when the binary SVM decision output is 0, which is the expected result as the instance would lie exactly on the separating hyperplane.</p>
"
"0.0942390294485422","0.0968548555282575","152868","<p>I have an administrative database with hospital readmissions (binomial: yes/no) and a couple of predictors. I've fitted a multilevel model with the function <code>glmer</code> from the package <code>lme4</code> to estimate the effect of these predictors on readmissions. 
The model has two levels: <code>hospital</code> and <code>patient</code>.
When I calculate the predicted probabilities (the chance of a readmission), and afterwards calculate the readmission ratio's for each hospital (by dividing the observed readmissions by the predicted readmissions), all my ratio's are around 1 which can't be correct.</p>

<p>Before I've calculated the predicted probabilities with a normal logistic regression, which gives more plausible ratio's (from 0,64 to 1,5)</p>

<p>This is my code to calculate predicted probabilities for the multilevel model:</p>

<pre><code>database$predprob &lt;- fitted(model1)
</code></pre>

<p>I've also tried this one, but it gives exactly the same predicted probabilities:</p>

<pre><code>database$predprob &lt;- predict(model1, newdata = database, type = ""response"", na.action = na.omit)
</code></pre>

<p>Does anybody know how to calculate predicted probabilities for a multilevel analysis? I suppose there must be another way to calculate it as my calculated ratios (observed/predicted) are all around 1.</p>
"
"0.0632174627941634","0.0649722122736315","153527","<p>I am looking at the <code>wine</code> dataset from the R <code>FactoMineR</code>package to find out whether the categorical variable soil type affects the feature set. Basically there are four distinct soil types and 29 different features and there are 20 observations</p>

<pre>
obs&nbsp;soil&nbsp;f1&nbsp;f2&nbsp;...&nbsp;f29
  1&nbsp;   1&nbsp;11&nbsp;87&nbsp;...&nbsp;20
  .&nbsp;   .&nbsp; .&nbsp; .&nbsp;...&nbsp; .
  7&nbsp;   2&nbsp;13&nbsp;10&nbsp;...&nbsp;10
  .&nbsp;   .&nbsp; .&nbsp; .&nbsp;...&nbsp; .
 15&nbsp;   3&nbsp;77&nbsp;53&nbsp;...&nbsp;54
  .&nbsp;   .&nbsp; .&nbsp; .&nbsp;...&nbsp; .
 20&nbsp;   4&nbsp;88&nbsp;81&nbsp;...&nbsp;21
</pre> 

<p>Given that I cannot use ANOVA or logistic regressions what other ways are there to figure out if the soil type affects the features? I am considering using box plots for visual inspection, but 29 box plots seems like a bit much. Are there any succinct plot types that could give me this kind of information?</p>
"
"0.059601995508215","0.0612563891831689","156610","<p>Whenever I run a logistic regression, I need to set the threshold so that it groups probabilities higher than the threshold to my positive group:</p>

<pre><code>table(test$Noshow, logpred&gt;0.5)
</code></pre>

<p>What I sometimes do is to optimize my confusion matrix so that I get a good Sensitivity and Specificity values. Is this a good way to do it?</p>

<p>On the other hand, when I run a randomForest model using the ""randomForest"" package, I can set the cutoffs such as this:</p>

<pre><code>k &lt;- 0.5
rfcutoff &lt;- c(k, 1-k)
rftree &lt;- randomForest(rffrmla_all, data = train1, mtry = 2, keep.forest = TRUE, importance = TRUE, ntree = 500, cutoff = rfcutoff)
</code></pre>

<p>Again I optimize my confusion matrix to get a good cutoff or base the cutoff on a naive baseline proportion. What is a good way to do this? </p>
"
"0.059601995508215","0.0612563891831689","159301","<p>I have a classification problem I am attempting to model using logistic regression (via the <code>glm</code> package in R): </p>

<pre><code>cols &lt;- c(""x"", ""z"", ""a"", ""b"", ""c"") 
formula = paste0(""x ~ "", paste(cols, collapse = ""+""))
formula = as.formula(formula)
</code></pre>

<p>I have a bunch of explanatory variables at the moment. How advisable is it to model this relationship using <code>gbm</code>, see the relative inference strength of each variable, and then remove seemingly meaningless variables before <code>glm</code> regression?</p>

<p>I ask just because I have done this in the past, with <code>glm</code> and <code>gbm</code> giving seemingly contradictory signals on different explanatory variables. </p>
"
"0.042144975196109","0","160495","<p>I working with R on a classification problem. My outcome variable is binary with two levels 1 and 2. 
First of all I tried the logistic regression, which of all methods has the best performance, altough still poor. </p>

<p>I tried nnet package, random forest, the fuzzy package frbs and decision trees. </p>

<p>The nnet function gives me only one class - in this case 2.</p>

<p>I had some hope with frbs package. See my code below:</p>

<pre><code>obj &lt;- frbs.learn(train,method.type=""FRBCS.CHI"",control=list(num.labels=3,type.mf=""GAUSSIAN""))
summary(obj)
#test set without def 
pred&lt;-predict(obj,newdata=test[,1:8])
</code></pre>

<p>But the predictions are wrong, the class 1 is completely missclassified</p>

<pre><code>#percentage error
tdef&lt;-test$def
err = 100*sum(pred!=tdef)/ nrow(pred)
print(err)
[1] 16.93038
</code></pre>

<p>I'm wondering what I could improve to classify the output variable. Is something wrong with my data? 
Are the parameters not right? </p>

<p>Can someone please verifiy?  I'm at the end of my knowledge...</p>

<p>You can find the (normalized) data here:
<a href=""https://drive.google.com/open?id=1xrCXTLqKvGiGeo2X0Y1DvoSKvzbYFnyccLimceDIbZg"" rel=""nofollow"">https://drive.google.com/open?id=1xrCXTLqKvGiGeo2X0Y1DvoSKvzbYFnyccLimceDIbZg</a></p>
"
"0.146428339346645","0.138916433136387","160638","<h1>General question</h1>

<p>When I perform a logistic regression using lrm and specify weights for the observations, I get the following warning message:</p>

<blockquote>
  <p>Warning message:
  In lrm(Tag ~ DLL, weights = W, data = tagdata, x = TRUE, y = TRUE) :
    currently weights are ignored in model validation and bootstrapping lrm fits</p>
</blockquote>

<p>My interpretation is that everything that the rms package will tell me regarding goodness-of-fit, notably using the residuals.lrm tool, is wrong. Is this correct?</p>

<h1>Specific example</h1>

<p>To be more specific, I have working example. All the code and output can be found in this <a href=""https://github.com/jwimberley/crossvalidated-posts/tree/master/lrm_gof"" rel=""nofollow"">GitHub repository</a>. I have two CSV tables of data, <a href=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/toystudy.csv"" rel=""nofollow"">toystudy.csv</a> and <a href=""https://github.com/jwimberley/crossvalidated-posts/raw/master/lrm_gof/realstudy.csv"" rel=""nofollow"">realstudy.csv</a>. There are three columns in each:</p>

<ol>
<li>The binomial response $y$ (0 or 1) [called Tag in code]</li>
<li>The predictor $x$ [called DLL in code]</li>
<li>The weight for the observation [called W in code]</li>
</ol>

<p>The former is simulated data, where all the weights are unity and where a logistic regression $log(\pi) = \theta_0 + \theta_1 x$ should fit the data perfectly. The latter is real data from my analysis, where the validity of this simple model is in question. The real data has weighted observations. (Some of the weights are negative, but there is a well-defined reason for this). The analysis code in contained completely in <a href=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/regressionTest.R"" rel=""nofollow"">regressionTest.R</a>; the meat of the code is</p>

<pre><code>library(rms)
fit &lt;- lrm(Tag ~ DLL, weights = W, data = tagdata, x=TRUE, y=TRUE)
residuals(fit,""gof"")
</code></pre>

<p>Here are the results for the two tables of data.</p>

<h3>Case 1: Toy data</h3>

<p>The goodness-of-fit claimed by lrm (which is something called the le Cessie-van Houwelingen-Copas-Hosmer test, I understand?) is very good:</p>

<p><img src=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/toy/residuals.png"" alt=""enter image description here""></p>

<p>This is confirmed by grouping the data into 20 quantiles of the predictor and overlaying the predicted success rate over the average actual success rate:</p>

<p><img src=""http://i.stack.imgur.com/hOEFs.png"" alt=""enter image description here""></p>

<h3>Case 2: Real data</h3>

<p>In this case, the goodness-of-fit reported by lrm is horrendous:</p>

<p><img src=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/real/residuals.png"" alt=""enter image description here""></p>

<p>However, I don't think it should be that bad. Again grouping the data into quantiles, and taking into account the weights when computing the average values in each bin:</p>

<p><img src=""http://i.stack.imgur.com/mgzhc.png"" alt=""enter image description here""></p>

<p>Comparing the prediction to the observed values and their standard errors, I don't think this is that bad (the error bars here depend on how the standard error on a weighted mean is computed, so they might not be 100% right, but should at least be close). On the other hand, if I produce the same plot while ignoring the weights:</p>

<p><img src=""http://i.stack.imgur.com/dId9F.png"" alt=""enter image description here""></p>

<p>I can definitely imagine this fit being as poor as the goodness-of-fit test says.</p>

<h2>Conclusion</h2>

<p>So, is residuals.rm simply ignoring the weights when it calculates its goodness-of-fit statistic? And if so, is there any R package that will do this correctly?</p>
"
"0.146428339346645","0.162069171992452","162426","<p>I am trying to create a logistic regression model and a random forest model on the same data to predict probability of default. For the logistic regression model, I have created some dummy variables from categorical variables. Finally, for the input of logistic regression, I have 9 dummy variables and 2 numeric variables (age and level, age takes values from 18 to 60, level from 4 to 10). I want to use same input dataset for the random forest model. When I did so, using ""randomForest"" Package, I get following Variable Importance Plot.</p>

<p><img src=""http://i.stack.imgur.com/qscyb.png"" alt=""enter image description here""></p>

<p>Level seems to be a very good variable both by MSE and Node Purity. Also, level is a very important variable in logistic regression (p value ~ 10^-5). 
However, Age is very important by Node purity, but not by MSE. Also, in logistic regression, age is not a very good variable with p value of 0.026. So I want to understand, Does being numeric increases the node purity importance of a variable by overfitting? Is it not suitable to use numeric and dummy variables together in random forest model? Or is there something I am missing.</p>

<p>I had similar doubts about using numeric and dummy variables in logistic regression, but in logistic regression it did not create any problem. Please help.</p>
"
"0.145994476646782","0.150046896984107","162463","<p>I am doing some data analysis on a fairly large health data set of patients with diagnoses and the respective procedures received for each event. I was asked to run a multinomial logistic regression on my data.</p>

<p>The dataset has around 4,000 columns of attributes, of which around 3,000 are unique diagnoses. The diagnosis variables take on the value of 1 if the patient had that diagnosis and 0 if he or she did not.  The remaining approximately 1,000 variables pertain to unique procedures, which also take on the value of 1 if the patient has received it, and 0 if he or she did not.</p>

<p>The dataset contains information on approximately 30,000 patients. I, admittedly naively, ran a the multinom function in the multinom package in R on all 4,000 variables, with the dependent variable being the very last procedure the patient has received (marked as ""Final procedure"" in the matrix), but R isn't able to complete the computation. </p>

<p>I would like some overall advice in perhaps a different package I could use for running regressions on large data sets (cannot use bigmemory however because this is on windows) or even perhaps reformatting my data. </p>

<p>Initially, my data set had around 50 columns, because the maximum number of diagnoses and procedures a patient had was 25 diagnoses and 25 procedures, so each column was marked as ""Diagnosis X"" and ""Procedure x,"" with the corresponding element being the actual diagnosis/procedure identifier. For all the patients who did I have all 25 diagnoses/procedures (so most of them), the values in the data frame would just be NA. Now I am wondering if I could perhaps resort to using this data frame instead and have a nicer, smaller matrix to work with? The only real reason I reformatted my data set into the much larger matrix was because my grad student asked me to do so, but maybe this isn't the way to go.</p>
"
"0.0729972383233908","0.0750234484920533","162867","<p>I'm currently building zero-inflated Poisson &amp; negative binomial predictive models using the zeroinfl() function from the pscl package in R.</p>

<p>Incorporating penalized regressions into my model to account for shrinkage and variable selection is a priority. In addition I'd like to use penalization to avoid convergence issues due to perfect/quasi separation in my data (better than manually removing variables).</p>

<p><strong>Question</strong>: Realizing that zero-inflated models $\neq$ hurdle models, for purposes of variable selection will my models be <strong>seriously biased</strong> if I first run separate run lasso (or elastic net) Poisson and logistic regressions with glmnet to select variables for the zeroinfl()?</p>
"
"0.0942390294485422","0.0968548555282575","163819","<p>I am running a multinomial logistic regression model (with 3 possible outcomes) in R. I am trying to find the best way to assess the predictive power/accuracy of the model, and the best thing I've come up with is using a ROC curve.</p>

<p>For multi-class ROC analysis, I know that there is the one vs. one comparison or the one vs. all comparison. For the one vs. one comparison, would I need three separate ROC curves for each possible combination of outcome comparisons? If so, do I need to make a third model for comparing the two outcomes that were initially being compared to the baseline outcome?</p>

<p>For one vs. all comparison is the threshold for a r ""random model"" now 33% instead of 50%?</p>

<p>And finally, is there a better way to go about doing this/visualizing it?</p>

<p>EDIT: I know the pROC package has a multi class.roc function, but I don't totally get what it does.</p>
"
"0.042144975196109","0.043314808182421","163986","<p>I effectively want to model the probability of a player winning his service point (a point in which he is the server) based on the values of explanatory variables (namely court surface and opponent world ranking)</p>

<p>Can this be done using a binary response logistic regression?</p>

<p>Consider the fact that I can view my response variable as number of successes out of a total number of trials (for which I have the data). Will it work considering I have both categorical and numerical explanatory variables?</p>

<p>Any feedback on why this will/won't work or how I can make it work would be hugely appreciated! I am doing the analysis in R, so pointers on functions or packages would also be welcome! </p>
"
"0.13382827025955","0.150046896984107","166584","<p>I am conducting a regression in order to predict a tennis player's service point win % i.e. the percentage of points he wins when he is the server.
Model 1 If my DV data lies in the range 0.3-0.9, does it make sense to use a logistic regression? If using logistic I would endeavor to build a model with serve win % as my DV and my IV's as:</p>

<p>+average serve win % of last n matches (maybe n=5 or 10) to account for form </p>

<p>+surface </p>

<p>+player ranking </p>

<p>+opposition ranking</p>

<p>..... Would this be a good model to use? Preliminary logistic regressions just involving serve win % regressed on surface + player ranking + opponent ranking ... are showing some strange results so im losing faith in logistic for this data.</p>

<p>An alternative I'm considering is to use raw variables in a linear regression type model with interactions.... Along the lines of Aiken &amp; West 1991
My dependent variable will be number of service points won in match, and my independent variables will be:</p>

<p>+no. service points played in match +the surface the match played on </p>

<p>+the player's ranking points +the opponents ranking points</p>

<p>+an interaction between player and opponent ranking points </p>

<p>+an interaction between surface and no. points played </p>

<p>+average service points won in last n matches</p>

<p>+average % of service points won in last m matches</p>

<p>Do either of these models stand out as smart or appropriate ways to model this data? For context, for each player I have between 100-350 matches worth of data. I would love to hear what you guys think, or if you have any other suggestions on how to predict serve win % using the stated variables I would really appreciate it. I'm conducting this analysis in R so any code/package suggestions would also be great </p>
"
"0.127071881385753","0.143658966607306","166779","<p>Iâ€™ve seen some papers that present the idea of training classifiers such as logistic regression that are really meant to optimize a custom cost model (such as by maximizing profit given expect revenues for predictions depending on whether they are false positives, true negatives, true positives, or true negatives) not by optimizing the typical log-loss function and then looking for the optimal decision cut-off threshold, but by using different loss functions that weight differently the costs of each classification type or of each misclassification type (although I've seen that different authors propose different functions), and these seem to provide better results when evaluating them based on the customly-defined cost function.</p>

<p>I was wondering if there are any implementations of such methods in R. Particularly, I'd like to try fitting a logistic regression treating the cost of misclassifying as false positive to be a multiple of the cost of misclassifying as false negative. I found a package that does just this for decision trees (although in that case it's based on the class proportions on the leaves rather than something like log-loss) and I see that there are some options for observation-specific weights in logistic regression, but not for error type weights.</p>
"
"0.0842899503922179","0.086629616364842","167389","<p>I have a database with more than 500 samples with 22 quantitative features each and I would like to predict a categorical variable (0 or 1).
I am trying to fit a logistic regression model and a neural network in R using glm and the neural net package.</p>

<p>By selecting different features I noted that I get different results in terms of ability of my two models to predict the test set (another 500+ samples database) a +-10% in accuracy depending on how many and what features are used (using the same test set).
Essentially the models seems to get worse using more than 12 features on average but of course I cannot try all the combinations and I'd rather not use random features as a blind guess. Also the neural network seems a little bit picky on features and does not always converge (however this is a minor issue).</p>

<p>What tests could I run in R to select the most ""explicative"" features?</p>

<p>(By the way, is ""explicative"" the right terminology?)</p>
"
"0.157692057757926","0.162069171992452","171325","<p>I am trying to fit a logistic regression model in R to classify a y variable as either 0 or 1. I have a dataset of around 2000 observations and decided to split it in half (training and testing).</p>

<p>After having decided which variables to include in my model, I subset the data and fitted the logistic regression as follows:</p>

<pre><code>clf &lt;- glm(y~.,data=df,family='binomial')
summary(clf)
</code></pre>

<p>Then, I tested the classifier on the testing set (1000 observations) and got 0.75 accuracy score.</p>

<pre><code>results &lt;- ifelse(predict(model,testdf,type='response') &gt; 0.5,1,0)
error &lt;- mean(r_results != results)
print(1-error) #prints out 0.74984
</code></pre>

<p>After this step, I decided to crossvalidate using the boot package</p>

<pre><code>library(boot)

# K-fold CV
error_cv = NULL

# Cost function for binary variable (as suggested by the R documentation)
cost &lt;- function(r, pi = 0) mean(abs(r-pi) &gt; 0.5)


for(i in 1:10)
{
    error_cv[i] &lt;- cv.glm(df,clf,cost,K=10)$delta[1]
}

error_cv
</code></pre>

<p>now, here is where I encounter a problem:</p>

<p>K-fold cross validation as I understand it, does the following (quote from Wikipedia):
""In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k âˆ’ 1 subsamples are used as training data. The cross-validation process is then repeated k times (the folds), with each of the k subsamples used exactly once as the validation data.""</p>

<p>However, how come that cv.glm() gets as argument my already fitted model? I don't understand what it is doing. Furthermore, if the data argument is equal to the training set, I get error rates of arount 0.2 whereas if I set data=testdf I get error rates of around 0.4. Since the two sets, df and testdf, have been splitted randomly, I cannot explain this large difference and I cannot explain why cv.glm() does not (apparently) do the fit and test process it is supposed to do.</p>

<p>What am I missing?</p>
"
"0.059601995508215","0.0306281945915844","172867","<p>I am trying to create a decision tree using the <code>rpart</code> package in R.
To arrive at the optimal depth for the tree, I am using the <code>plotcp</code> function in the package (which provides the cross validated error rate for various complexity parameter thresholds). </p>

<p>Ideally, you would expect the error to be very high for high values of cp,  which will then gradually decrease before increasing again or flattening out (bias-variance trade-off)</p>

<p>However, the plot that I am getting is almost a straight line and as such implies that my optimal tree should have zero nodes. That should not be possible, as I do have a few strong/moderately strong predictors in my data set (and this relationship gets picked up when I try other models like logistic/conditional inference trees (<code>party</code> package) and the resulting models are reasonably accurate). </p>

<p>I am pasting the cp plot that I got below. Any thoughts on what the issue could be?</p>

<p><a href=""http://i.stack.imgur.com/twpRZ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/twpRZ.png"" alt=""enter image description here""></a></p>
"
"0.059601995508215","0.0612563891831689","172889","<p>Recently I'm using R survival package to try to predict the probability of people going to churn. I found some <a href=""http://stackoverflow.com/questions/27408862/how-to-predict-survival-probabilities-in-r"">examples</a> on stack overflow and also tried that to my own data. Here is my prediction model and output:</p>

<pre><code>&gt; Status_by_Time &lt;- Surv(time = Duration, event = Gift_Status_ind)
&gt; model.fit2 &lt;- survreg(Status_by_Time ~ Age
                + Gender_ind 
                + Fundraiser_ind
                + Monthly.Recurring.Amount
                + Frequency_ind
                + Monthly.first.gift.amount
                + Monthly.last.gift.amount
                #+ Duration
                #+ Saved.
                + Upgrade.first.time
                + Upgrade.second.time,
                dist = ""logistic""
)

&gt; summary(model.fit2)
                            Value Std. Error      z         p
(Intercept)                81.525    1.46725  55.56  0.00e+00
Age                         0.156    0.01889   8.27  1.33e-16
Gender_ind2                 2.278    0.55955   4.07  4.68e-05
Gender_ind3                -9.514    1.09689  -8.67  4.18e-18
Fundraiser_ind2            -8.798    0.69303 -12.70  6.25e-37
Fundraiser_ind3             4.028    0.90970   4.43  9.52e-06
Monthly.Recurring.Amount   -1.211    0.04856 -24.95 2.39e-137
Frequency_ind2            257.319    0.00000    Inf  0.00e+00
Frequency_ind3              8.562    2.71423   3.15  1.61e-03
Frequency_ind4            -89.067    1.39379 -63.90  0.00e+00
Monthly.first.gift.amount  -2.538    0.03721 -68.22  0.00e+00
Monthly.last.gift.amount    1.827    0.04981  36.67 2.38e-294
Upgrade.first.time          6.467    0.82381   7.85  4.15e-15
Upgrade.second.time        10.849    2.72927   3.98  7.04e-05
Log(scale)                  2.869    0.00841 341.02  0.00e+00

Scale= 17.6 

Logistic distribution
Loglik(model)= -51841.8   Loglik(intercept only)= -55404
Chisq= 7124.45 on 13 degrees of freedom, p= 0 
Number of Newton-Raphson Iterations: 8 
n= 18097 

&gt; predicted.values &lt;- predict(model.fit2, newdata = churn.df.trim, type = ""quantile"", p = (1:9)/10) # 10 times event???
&gt; head(predicted.values)
            [,1]      [,2]     [,3]     [,4]     [,5]     [,6]     [,7]     [,8]     [,9]
 [1,]   2.219425 16.513993 26.01508 33.80343 40.95072 48.09800 55.88635 65.38744 79.68201
 [2,]  11.088257 25.382825 34.88392 42.67227 49.81955 56.96683 64.75518 74.25627 88.55084
 [3,] -11.996590  2.297977 11.79907 19.58742 26.73470 33.88198 41.67033 51.17143 65.46599
 [4,]   5.456971 19.751539 29.25263 37.04098 44.18826 51.33555 59.12390 68.62499 82.91955
 [5,]  19.690749 33.985316 43.48641 51.27476 58.42204 65.56932 73.35767 82.85876 97.15333
 [6,]  -8.187469  6.107099 15.60819 23.39654 30.54382 37.69111 45.47946 54.98055 69.27511
</code></pre>

<p>I reckon all these numbers are not probabilities. Is there some way to interpret these numbers or turn them into probabilities? Also if I use <code>p = (1:9)/10</code> does that mean I'm calculating the probability for the next 9 or 10 period?</p>

<p>Much appreciate if someone could give me a straight forward explanation (none academic one). </p>
"
"0.0729972383233908","0.0750234484920533","173828","<p>I have a data set shown below, the 1st, 2nd,3rd column are dependent variable(dv), and 2 independent variables (iv1 &amp; iv2) respectively, I expected the regression coefficient of the ""iv1"" shows a positive value, as there is a positive correlation between dv and iv1. However, The result shows a negative regression coefficient for iv1 (beta_iv1 = -0.55), I am wondering why this happened, I appreciate if anyone can help.</p>

<p>dv    iv1     iv2</p>

<p>1     0.00    7.70<br>
1     2.90    0.00<br>
1     0.00    7.70<br>
1     2.90    0.50<br>
1     0.00    9.50<br>
1     1.50    7.70<br>
1     5.70    0.50<br>
1     7.10    2.30<br>
1     5.70    4.10<br>
1     0.00    4.10<br>
1     4.30    4.10<br>
1     0.00    10.00<br>
1     0.00    4.10<br>
1     2.90    0.50<br>
1     0.00    9.50<br>
0     0.00    9.50<br>
1     0.00    5.90<br>
1     0.00    4.10<br>
1     1.50    5.90<br>
1     5.70    2.30<br>
1     1.50    0.00<br>
0     0.00    10.00<br>
1     5.70    0.00<br>
1     5.70    0.50<br>
1     4.30    2.30<br>
0     0.00    10.00<br>
1     2.90    5.90<br>
1     0.00    5.90<br>
1     0.00    5.90<br>
1     2.90    2.30<br>
1     1.50    2.30<br>
1     2.90    0.50<br>
1     5.70    4.10<br>
1     1.50    0.00<br>
1     0.00    7.70  </p>

<p>I run this using R with package ""logistf"" which overcomes separation problem of logistic regression. The code I run this data set is as below:</p>

<blockquote>
  <p>library(logistf);<br>
      tempT=read.table(fileS);<br>
     fit&lt;-logistf(dv ~ iv1+iv2, data=tempT);</p>
</blockquote>

<p>and the result shows below:  </p>

<pre><code>           coef  se(coef) lower 0.95  upper 0.95     Chisq      p
</code></pre>

<blockquote>
  <p>(Intercept)  9.0086382 5.1741382   1.650380 61.61244068 7.6897111 
  0.005553652<br>
  tempT[, 2]  -0.5509122 0.6567110  -6.013208  1.55280975 0.5490404 0.458710039<br>
  tempT[, 3]  -0.9051062 0.5597601  -6.317335 -0.06328166 4.8315401 0.027943657</p>
</blockquote>

<p>Likelihood ratio test=7.213821 on 2 df, p=0.02713555, n=35</p>
"
"0.059601995508215","0.0612563891831689","174396","<p>Hi I'm doing some logistic regression, currently using <code>glmnet</code> package in R.</p>

<p><code>glmnet</code> provides a few measure metrics for cross validation. For classification, we can use type.measure = 'auc' (area under ROC curve) or type.measure = 'class' (misclassification rate).</p>

<p>I'm working with some insurance data with a low rate of positive examples (y = 0 for 95% data, and = 1 for 5% of them. OR 95% people did't buy insurance). </p>

<pre><code>library(ISLR)
data(Caravan)
y = dta$Purchase
x = as.matrix( dta[ , -which(colnames(dta)=='Purchase') ] )
</code></pre>

<p>And I want to predict who are likely to buy insurance.</p>

<p>I think I need to use F1 score or some custom metrics. For example, if I can make 200 by selling an insurance, and the cost to contact one person is 20, then I want to maximize </p>

<p>metric = 200 * N(true positive) - 20 * N(predicted positive)</p>

<p>Is there a way I can do this with glmnet or something else more suitable? Or is <code>AUC</code> a similar metric as F1 score? Any help or discussion's appreciated. Thanks</p>
"
"0.10323368445272","0.106099178353461","174518","<p>consider the following data set:</p>

<pre><code>a &lt;- c(1, 2, 3, 1, 4, 1968, 2, 1)
b &lt;- c(2, 1, 2, 4, 3, 1984, 2, 0)
c &lt;- c(3, 3, 4, 2, 1, 1945, 1, 0)
d &lt;- c(4, 1, 4, 3, 2, 1975, 3, 1)
df &lt;- data.frame(rbind(a,b,c,d))
names(df) &lt;- c(""ID"", ""OptionW"", ""OptionX"", ""OptionY"", ""OptionZ"", ""yearofBirth"", ""education"", ""sex"")


ID OptionW OptionX OptionY OptionZ yearofBirth education sex
1       2       3       1       4        1968         2   1
2       1       2       4       3        1984         2   0
3       3       4       2       1        1945         1   0
4       1       4       3       2        1975         3   1
</code></pre>

<p>Two hundred people where asked to rank Options W to Z from 1 to 4 in their effectiveness to lower crime rates in their community. Their age, highest academic degree and sex are annotated as well.
I want to find out:</p>

<ul>
<li>which options are preferred by the majority of citizens?</li>
<li>are there significant differences in what men or women, old or young, well or less well educated citizens believe?</li>
<li>how likely is the ranking order going to change if the person is older/younger, has had more or less formal education and is male or female? </li>
</ul>

<p>I read that a multinomial logistic regression might be the way to go, but I find it hard to adapt the examples I find to my data set. Often they allow for only one option to be chosen, making each choice (W, X Y Z) a level of one variable (Options). But in my case I have several variables (OptionW, OptionX, OptionY, OptionZ) where the ranking placement appears to be the level (1,2,3,..10). Or am I looking at it the wrong way?</p>

<p>Which function from what package would be suitable? And are there other methods available apart from mlr?</p>

<p>I use R mostly for spatial analysis and am not very fluent in statistics. Hopefully you can help me here.</p>
"
"0.042144975196109","0.043314808182421","174989","<p>Please bear with me, I am very new to R.</p>

<p>My question is regarding the use of the <code>improveProb</code> function in the <code>Hmisc</code> package. I have two logistic models, the only difference being that the second model contains my novel marker of interest. I am trying to calculate NRI and IDI.</p>

<p>I have the PredRisks for both models - PredRisk1 and PredRisk2, and my outcome is disease 0/1. How do I define this in R in order to run</p>

<p><code>improveProb(x1, x2, y)</code>?</p>

<hr>

<p>The data are the same for both models. We are looking at ways to validate our findings. We have performed k-fold cross-validation (MSE=0.08) and bootstrapping with optimism (AUC original = 0.826 After correction =0.791) to check for overfitting. Is this appropriate? The LRT was significant for both logistic regression models, but I need to check this. Also, the AIC for model 2 is lower than model 1. Thanks again for your expert knowledge :)</p>
"
"0.042144975196109","0.043314808182421","175312","<p>I'm using the model ReLogit from the package Zelig in R. ReLogit is a logistic regression for rare events data.
After having estimated the model on the training set, I want to calculate the AUC (Area under the ROC Curve) of this model in the test set. 
How can I do this with this package?</p>
"
"0.059601995508215","0.0612563891831689","175325","<p>I am trying to determine which of several independent variables {A,B,C} best predicts a subject's response R, which can be one of 3 choices, {high, medium, low}.</p>

<p>As the dependent variable is multinomial i.e. not binomial, a binomial logistic regression is not suitable. However, using the lme4 package to fit a glm with <code>family = 'binomial'</code> to the data runs without error or warning, and correctly identifies that independent variable B is the best predictor.</p>

<p>Is this a lucky coincidence, or is the call to <code>glm()</code> automatically fitting multiple binomial regressions to the data (one for each binary combination), or is it something else entirely? Any comments are welcome.</p>
"
"0.0842899503922179","0.086629616364842","176388","<p>The following two question outline how one can plot the results from a survival analysis using R. <a href=""http://stackoverflow.com/questions/9151591/how-to-plot-the-survival-curve-generated-by-survreg-package-survival-of-r"">Q1</a> and <a href=""http://stackoverflow.com/questions/16236939/plot-survival-and-hazard-function-of-survreg-using-curve"">Q2</a></p>

<p>But both of the examples assume, or more directly specify a weibull distribution fitted to the survival model.</p>

<p>Refering to <code>survreg()</code> which is within the <code>survival</code> package in R, the following are possible fitting assumptions:</p>

<ul>
<li>weibull </li>
<li>exponential</li>
<li>gaussian </li>
<li>logistic </li>
<li>lognormal </li>
<li>loglogistic</li>
</ul>

<p>So if we take the example from the <code>survreg()</code> help.</p>

<pre><code>library(survival)

data(ovarian)

head(ovarian)

survival.weibull &lt;- survreg(Surv(time, status) ~ ph.ecog + age + strata(sex), 
                          dist='weibull', lung)

survival.logn &lt;- survreg(Surv(time, status) ~ ph.ecog + age + strata(sex), 
                            dist='lognormal', lung)

survival.logl &lt;- survreg(Surv(time, status) ~ ph.ecog + age + strata(sex), 
                         dist='loglogistic', lung)
</code></pre>

<p>How can one verify the best appropriate distribution to fit. From the summary statistic we get the <code>Loglik(model)</code> value. Is this the best indicator? Or is there a graphical method to visualise the best fit - I was suggested a QQ-plot may be of help.</p>

<p>Thanks in advance for an advice </p>
"
"0.127071881385753","0.143658966607306","177654","<p>When running an ordered logistic regression using the <code>polr</code> function of the <code>MASS</code>package (DV is low, medium, high) and have a look at the summary I get Î²s for every IV and the intercepts for low|medium and medium|high.</p>

<p>The <code>predict</code>function for assessing the probabilities (<code>type='p'</code>) or the classes (<code>type='class'</code>) also works just fine.</p>

<p>However I want to calculate the probabilities myself in order to use them with different data sets.</p>

<p>If I use the following code for a <em>logistic model with a binary (!) dependent variable</em>, I can exactly replicate the <code>predict</code> - outcome:</p>

<p><code>log_pred &lt;- (logit_model$coefficients[1] + logit_model$coefficients[2]*IV_1 + logit_model$coefficients[3]*IV_2)</code></p>

<ul>
<li><code>logit_model</code> is my <code>glm</code>-object</li>
<li><code>logit_model$zeta[1]</code> is the first intercept</li>
<li><code>logit_model$zeta[2]</code> is the second intercept</li>
<li><code>logit_model$coefficients[1]</code> is the Î² of IV_1</li>
<li><code>logit_model$coefficients[2]</code> is the Î² of IV_2</li>
</ul>

<p>the only thing I have to do now, to get the predicted probabilities is:</p>

<p><code>log_pred_probs &lt;- exp(log_pred)/(1+exp(log_pred))</code></p>

<p>If I understand all the posts on ordered logistic regression I read correctly, the only thing I have to change with a <code>polr</code> object with the 3 ""groups"" of low, medium, and high would be to:</p>

<ul>
<li>run the <code>log-pred</code>part for each group using their own intercepts, let's call them <code>log_pred1</code> and <code>log_pred2</code></li>
<li>and to, then, run the following code (similar to the logistic model above):
<code>log_pred_probs1 &lt;- exp(log_pred1)/(1+exp(log_pred1)+exp(log_pred2))</code> for ""low""
<code>log_pred_probs2 &lt;- exp(log_pred2)/(1+exp(log_pred1)+exp(log_pred2))</code> for ""medium""
<code>log_pred_probs3 &lt;- 1/(1+exp(log_pred1)+exp(log_pred2))</code> for ""high""</li>
</ul>

<p>I think there are at least two problems ('cause this doesn't work at all):</p>

<ol>
<li>I need the Î²-coefficients for every level of the dependent variable, and <code>summary(polr-object)</code>does only show the Î²s for the first group (so does <code>$coefficients</code>)</li>
<li>and I am not sure about the computation of the predicted probabilities for group 3, ""high"".</li>
</ol>

<p>So these are the questions in short: <strong>How do I assess the Î²-coefficients for every level of the DV in a <code>polr</code>object?</strong></p>

<p>And</p>

<p><strong>How do I compute the predicted probabilities for every level of the DV myself?</strong></p>
"
"0.0729972383233908","0.0750234484920533","177673","<p>I am conducting survival analysis research and I am trying to decide which distribution (Weibull, lognormal and loglogistic) best fits my data.</p>

<p>One can ""Fit of univariate distributions to non-censored data"" using the <code>fitdist()</code> from the <code>fitdistrplus</code> package in R.</p>

<p>referring to the examples from the help page, we can fit hypothetical distributions by the following:</p>

<pre><code>fitW &lt;- fitdist(serving, ""weibull"")
fitg &lt;- fitdist(serving, ""gamma"")
fitln &lt;- fitdist(serving, ""lnorm"") 
</code></pre>

<p>etc.</p>

<p>But one cannot fit an assumed log-logistic distribution with the package.</p>

<p>From research, I have found <a href=""https://cran.r-project.org/web/packages/fitdistrplus/vignettes/paper2JSS.pdf"" rel=""nofollow"">this paper</a> (see pg 5-6) which attempts to implement a secondary package, <code>actuar</code> which has the capability to fit a loglogis distribution.</p>

<p>Here we find the code:</p>

<pre><code>fendo.ll &lt;- fitdist(ATV, ""llogis"", start = list(shape = 1, scale = 500))
</code></pre>

<p>which then states at the bottom of the page</p>

<p>It can thus help to find correct initial values for the distribution parameters in non trivial cases, by iterative calls if necessary (see the reference manual for examples (<a href=""https://cran.r-project.org/web/packages/fitdistrplus/fitdistrplus.pdf"" rel=""nofollow"">Delignette-Muller et al., 2014</a>))</p>

<p>So can one explain where these values come from?</p>
"
"0.13382827025955","0.125039080820089","179250","<p>I am conducting a multifactorial analyisis involving categorical variables by using R. The response is â€œyesâ€ or â€œnoâ€ (Iâ€™m therefore using binary logistic regression) and the predictors have 2 up to 3 levels. The sample size is quite small (n = 230).</p>

<p>I tried before both the glm and rsm packages, but one predictor (B2) had an extremely high standard error; it was a separation problem since one of its level has zero observation. I have been then suggested to rely on penalized logistic regression and in particular the brglm package.</p>

<p>Here as follows, you can see the new output (brglm) for the model containing all the variables:</p>

<pre><code>Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)  1.64053    0.78655   2.086   0.0370 * 
B1x         -0.07434    0.71502  -0.104   0.9172   
B1y         -0.20966    0.59622  -0.352   0.7251   
B2x          0.09004    0.57168   0.158   0.8748   
B2y         -2.68270    1.67192  -1.605   0.1086   
B3x         -0.53347    0.33882  -1.574   0.1154   
B4x         -0.59495    0.30320  -1.962   0.0497 (*) 
B5x         -0.79962    0.47249  -1.692   0.0906 . 
B5y         -1.42761    0.59344  -2.406   0.0161 (*) 
B6x          1.36948    0.49224   2.782   0.0054 (**)
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 275.58  on 229  degrees of freedom
Residual deviance: 270.93  on 220  degrees of freedom
Penalized deviance: 253.9314 
AIC:  290.93 
</code></pre>

<p>As you can see, the problem regarding the huge standard error has been finally solved but there are still several issues:</p>

<p>1) In the description of the bglrm package, they say:</p>

<p>â€œIt is not advised to use methods associated with model comparison (add1, drop1,anova, etc.) on objects of class ""brglm"". Model comparison when estimation is performed using the modified scores or the penalized likelihood is an on-going research topic and will be implemented as soon as it is concludedâ€</p>

<p><strong>How can I safely remove non significant predictors?</strong> <strong>Have I only to rely on the p-value in the above output?</strong> 
That means that I have to remove from a further model predictors from B1 to B4, is that correct? (that was also case when I used a non penalized logistic regression model (such as lrm), but then I could use drop1).</p>

<p>2) <strong>How can I detect multicollinearity issues in brglm?</strong> </p>

<p>By using a non penalized likelihood model, I know that there is multicollinearity between B5 and B6, but how can I prove it now?</p>

<p>3) <strong>How can I calculate in brglm indexes such as C and Nagelgerkeâ€™s R</strong>? </p>

<p>4) In the description of the package, it is also written: â€œThe use of Akaike's information criterion (AIC) for model selection when method = ""brglm.fit"" is controversial.â€  </p>

<p>Is it safe or not to publish AIC in a paper?</p>

<p>Thank you very much in advance!</p>
"
"0.042144975196109","0.043314808182421","179898","<p>I have a clinical dataset (1400 cases) and I applied 4 data mining techniques (ANN, Decision Tree, SVM, Logistic Regression) to predict the binary outcome (Yes, No).</p>

<p>Now, I want to improve prediction accuracy through ensemble methods.<br>
What are the criteria to choose which model can be combined with another model?
And how can that be done in R? Can I use the ""caret"" package?</p>
"
"0.11150512337989","0.114600210537152","180135","<p>I have fit a generalized additive model (GAM) using the mgcv package in R. My model has a dichotomous response variable and so i've used the binomial family link function. After creating the model I would like to do a little post-estimation inference above and beyond the plot.gam graphs. </p>

<p>I would like to take two x-values, for example, and calculate the risk ratio and 95% confidence intervals for that ratio. Obtaining the risk ratio seems fairly straightforward. I could transform the predictions into probabilities and simply divide the two probabilities corresponding to the x-values of interest in order to get the risk ratio. I am less certain how to get the confidence intervals.</p>

<p>In this link here: <a href=""http://grokbase.com/t/r/r-help/125qbnw21a/r-mgcv-how-to-calculate-a-confidence-interval-of-a-ratio"" rel=""nofollow"">http://grokbase.com/t/r/r-help/125qbnw21a/r-mgcv-how-to-calculate-a-confidence-interval-of-a-ratio</a> Simon Wood, the author of the mgcv package explained how to get the CIs for a log ratio using a poisson model. I'm uncertain how I would need to change the code to get the risk ratios and 95% CIs from my logistic model. </p>

<p>Here is a reproducible example provided by Simon Wood in the link above:</p>

<pre><code>    library(mgcv)

    ## simulate some data
    dat &lt;- gamSim(1, n=1000, dist=""poisson"", scale=.25)

    ## fit log-linear model...
    b &lt;- gam(y~s(x0)+s(x1)+s(x2)+s(x3), family=poisson,
    data=dat, method=""REML"")

    ## data at which predictions to be compared...
    pd &lt;- data.frame(x0=c(.2,.3),x1=c(.5,.5),x2=c(.5,.5),
    x3=c(.5,.5))

    ## log(E(y_1)/E(y_2)) = s(x_1) - s(x_2)
    Xp &lt;- predict(b,newdata=pd,type=""lpmatrix"")

    ## ... Xp%*%coef(b) gives log(E(y_1)) and log(E(y_2)),
    ## so the required difference is computed as...
    diff &lt;- (Xp[1,]-Xp[2,])
    dly &lt;- t(diff)%*%coef(b) ## required log ratio (diff of logs)
    se.dly &lt;- sqrt(t(diff)%*%vcov(b)%*%diff) ## corresponding s.e.
    dly + c(-2,2)*se.dly ## 95%CI
</code></pre>

<p>Any help is greatly appreciated.</p>
"
"0.0942390294485422","0.0968548555282575","180191","<p>We can apply the Hosmer-Lemeshow goodness of fit to logistic regression modelling and to test if an underlying assumption is not applicable.</p>

<p>This <a href=""https://www.youtube.com/watch?v=MYW8gA1EQCQ"" rel=""nofollow"">link</a> shows a video of the application to a standard <code>glm()</code> model</p>

<p>This <a href=""http://stats.stackexchange.com/questions/132652/how-to-determine-which-distribution-fits-my-data-best-r"">detailed question</a>, outlines various simulation-based tests one can run to assess underlying distributions.</p>

<p><strong>But I want to apply the Hosmer-Lemeshow goodness of fit to survival analysis with assumed underlying data distributions</strong>.</p>

<p>Much literature points one towards a cox proportional hazards model, but from what I understand, a cox ph model does not assume an underlying distribution of data.
Therefore lets take some random data from the <code>survreg()</code> function of the <code>survival</code> package</p>

<pre><code>library(survival)

data(ovarian)

head(ovarian)

s &lt;- Surv(ovarian$futime, ovarian$fustat)
sWei &lt;- survreg(s ~ age,dist='weibull',data=ovarian)
</code></pre>

<p>How can we applying a H+L G.O.F statistic test?
I had hoped to follow this <a href=""http://thestatsgeek.com/2014/02/16/the-hosmer-lemeshow-goodness-of-fit-test-for-logistic-regression/"" rel=""nofollow"">link</a>, however the <code>survreg()</code> does not allow a <code>fitted()</code> function. Thus this does not work</p>

<pre><code>library(ResourceSelection)
hl &lt;- hoslem.test(sWei$y, fitted(sWei), g=10))
</code></pre>
"
"NaN","NaN","180337","<p>I always report odds ratios when using logistic regression for predictions. 
I wanted know is it meaningful to report odds ratios when modeling with gradient boosting approach? 
I am using gbm package in R to make the predictions.</p>

<p>Thanks!</p>
"
"NaN","NaN","180580","<p>To understand my logistic regression fit and identify non linear effects, I plan to estimate the conditional density and then calculate the log odds comparing to log odds from logistic regression. To me  this is the equivalent of scatter plot of single  independent variable vs dependent and prediction. </p>

<p>A) Does this seem like the right approach? </p>

<p>B) I am using R, and I am surprised that there is no package already doing this? </p>
"
"NaN","NaN","182467","<p>I'm running a logistic regression (presence/absence response) in R, using glmer (lme4 package). Ben Bolker'sÂ <a href=""http://glmm.wikidot.com/faq"" rel=""nofollow"">overdisp_fun</a> (see link) tells me my model is overdispersed, so I decided to include an individual-level random effect. This is not solving my problem, as I get convergence issues and overdispersion is not reduced. Could anyone recommend an alternative?</p>

<p>Thanks!! </p>
"
"0.157692057757926","0.162069171992452","182582","<p>This link from UCLA.edu (<a href=""http://www.ats.ucla.edu/stat/r/dae/mlogit.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/dae/mlogit.htm</a>) provides a useful example for a multinomial logistic case.
However, I wonder how can I correctly interpret the coefficients in case of grouped data. Here is an example following the ucla.edu exercise:</p>

<pre><code># Load packages
require(foreign)
require(nnet)
require(ggplot2)
require(reshape2)
# Read data from ucla.edu
ml &lt;- read.dta(""http://www.ats.ucla.edu/stat/data/hsbdemo.dta"")
str(ml)
# $ prog   : Factor w/ 3 levels ""general"",""academic"",""vocation"": 3 1 3 3 3 1 3 3 3 3 ...
# $ write  : num  35 33 39 37 31 36 36 31 41 37 ...
# $ ses    : Factor w/ 3 levels ""low"",""middle"",""high"": 1 2 3 1 2 3 2 2 2 2 ...
</code></pre>

<p>NOTE: I wish to predict variable <code>prog</code> using the numeric variable <code>write</code> and the 3 level factor <code>ses</code>. I don't care if the 3 levels, ""low"", ""middle"", ""high"", are ordered! As far as I am concerned this factor can be gender or country or whatever.</p>

<pre><code># Choose the baseline level of 'prog' dependent variable
ml$prog2 &lt;- relevel(ml$prog, ref = ""academic"")
</code></pre>

<p>I run a simple model based only on 'write' independent variable (I.V.)</p>

<pre><code>test1 &lt;- multinom(prog2 ~ write, data = ml)
summary(test1)
# I get the following coefficients
Coefficients:
         (Intercept)       write
general     2.712485 -0.06600782
vocation    5.359000 -0.11780909
</code></pre>

<p>I run the complex model, including 'ses' factor as the second I.V.</p>

<pre><code>test2 &lt;- multinom(prog2 ~ write + ses, data = ml)
summary(test2)
# I get the following coefficients
Coefficients:
         (Intercept)      write  sesmiddle    seshigh
general     2.852198 -0.0579287 -0.5332810 -1.1628226
vocation    5.218260 -0.1136037  0.2913859 -0.9826649
</code></pre>

<p><strong>So, how can I directly get the intercepts for each level of <code>ses</code>?</strong> I am not interested in the change of intercept from one level to another.</p>

<p>I tried this:</p>

<pre><code>test3 &lt;- multinom(prog2 ~ write + ses -1, data = ml)
summary(test3)
</code></pre>

<p>which is interpreted by R as omitting the 'main' intercept</p>

<pre><code>Coefficients:
               write   seslow sesmiddle  seshigh
general  -0.05792787 2.852149  2.318866 1.689318
vocation -0.11360180 5.218146  5.509562 4.235464
</code></pre>

<p>As I see, the intercept of model <code>test2</code> is actually the coefficient of level <code>low</code> of factor <code>ses</code> in model <code>test3</code> (the first level in alphabetical order).</p>

<p>So, now, can I make the interpretations like this:?</p>

<p>The log odds of being in general program vs. in academic program will increase by 2.85 for <code>ses=low</code>, by 2.32 for <code>ses=middle</code> and by 1.69 for <code>ses=high</code>. In a similar way for the log odds of being in vocation program vs. in academic program: the increase will be by 5.22 for <code>ses=low</code>, by 5.51 for <code>ses=middle</code> and by 4.24 for <code>ses=high</code>.</p>

<p>Sorry if my question might look naive.</p>
"
"0.0753912235588337","0.0968548555282575","182656","<p>I have data in longitudinal or clustered format (please see the example below). My response variable is dichotomous. I want to examine which factors explains why a subject in the dataset gets Y=1. In the example below I show only one predictor â€“ X.</p>

<p>Since I have a dichotomous response variable, I am thinking of logistic regression. However, the longitudinal format violates the distributional assumption of ML-theory. <strong>So my question is which logistic model would be appropriate here?</strong> And if possible, which R-package would be relevant (if not covered by the standard stats)? </p>

<p>All subjects are countries and observed (let's say) from 1990-1994. A country can get more than 1 Y per year, from different Z's.  I have been thinking of logistic panel models. Although I am not sure which specific model would be appropriate (assuming that panel models are more appropriate group of models). Perhaps random effects as each observation is not of the same nature (a country can get Y=1 from different groups, see variable Z). The Z variable is not a part of analysis though. Grateful for all suggestions!</p>

<pre><code>COUNTRY      YEAR  Y           X        Z

    A        1990  0           0        K
    A        1991  1           0        K
    A        1992  0           0        K
    A        1993  1           0        L
    B        1994  0           1        L
    B        1990  0           1        L
    B        1991  0           1        L
    B        1992  1           1        L
    C        1990  1           0        K
    C        1991  1           0        K
    C        1992  0           0        L
    C        1993  0           1        K
    C        1994  0           1        L
    D        1990  0           1        L
    D        1991  0           0        K
    D        1992  0           0        K
    D        1993  0           1        K
    D        1994  0           1        K
</code></pre>
"
"0.059601995508215","0.0612563891831689","183846","<p>I created a SEM model in R (lavaan package), but one of myÂ dependent variables is continuous, while the other is binary.</p>

<p>The model isÂ as follows:</p>

<pre><code>modelx &lt;- '
a =~ a1Â + a2Â + a3

bÂ =~ b1Â + b2 +Â b3

c =~Â c1 + c2 + c3

x ~ a + b + c + zÂ +Â w

y ~ a + b + cÂ + zÂ +Â w

'

sem(modelx, data=mydata, estimator=""DWLS"")
</code></pre>

<p>zÂ and wÂ are covariates. x is a scale (0-12), however y is a binary variable (0;1).Â </p>

<p>Thus, the question is, how can I implement a linear regression for predicting x and a logistic regression for y in a single SEM model? I assume that lavaan doesn't automatically account for the different dependent variables. If it does, I would like to see a reference for that. All ideas are welcome.</p>

<p>Edit: Using the latent variable factor scores from the measurement model for a, b, c in a glm (binomial reg for y and linear for x) and lavaan, the results are more closely aligned for x than for y. Does it mean that lavaan ignores/doesn't do good with the dichotomous variable in this particular case, or my question from the start is moot or unnecessary?</p>
"
"0.059601995508215","0.0612563891831689","183976","<p>I created a SEM model in R (<code>lavaan</code> package), but one of my dependent variables is continuous, while the other is binary.</p>

<p>The model is as follows:</p>

<pre><code>modelx &lt;- '
a =~ a1 + a2 + a3
b =~ b1 + b2 + b3
c =~ c1 + c2 + c3
x ~ a + b + c + z + w

y ~ a + b + c + z + w
'
sem(modelx, data=mydata, estimator=""DWLS"")
</code></pre>

<p><code>z</code> and <code>w</code> are covariates. <code>x</code> is a scale (0-12), however <code>y</code> is a binary variable (0;1). </p>

<p>Thus, the question is, how can I implement a linear regression for predicting x and a logistic regression for y in a single SEM model? I assume that lavaan doesn't automatically account for the different dependent variables. If it does, I would like to see a reference for that.</p>
"
"0.042144975196109","0.043314808182421","184391","<p>I am attempting to perform a piecewise/segmented logistic regression on survey data using  <a href=""http://www.asdfree.com/2015/11/statistically-significant-trends-with.html"" rel=""nofollow"">this tutorial</a> as my basis. I have data for the period 2006 to 2013, however 2012 is missing.</p>

<p>The analysis proceeds as expected until the point in step 8 where I add the segmented variable with one breakpoint (the final line of code in the example below).</p>

<pre><code>library(segmented)
df &lt;- data.frame(yr=c(2006:2011,2013),
             mean= c(0.11290830, 0.12814364, 0.11149552, 0.12071058, 0.11776731, 0.10363014, 0.09888132),
             wgt = c(602.2272, 546.2958, 594.1818, 756.0167, 579.1533, 481.9694, 654.3281))
o &lt;- lm( log( mean ) ~ yr , weights = wgt , data = df )
os &lt;- segmented( o , ~yr)
</code></pre>

<p>At this point I get the error message:</p>

<blockquote>
  <p>""Error in segmented.lm(o, ~yr) : only 1 datum in an interval: breakpoint(s) at the boundary or too close each other""</p>
</blockquote>

<p>From my reading, in particular <a href=""http://r.789695.n4.nabble.com/Estimating-and-predicting-using-quot-segmented-quot-Package-td4682541.html"" rel=""nofollow"">here</a>, this is because the breakpoint falls at 2007, thus leaving 2006 on it's own and unable to have a slope calculated for it. I understand that this is likely because I have so few data points.</p>

<p>Does anyone have any tips for getting around this or another package / technique that would be more appropriate? The second link suggests using additional dummy data but I'm a bit wary of this approach.</p>
"
"0.042144975196109","0.043314808182421","185166","<p>I thought I've understood the output of the logistic regression in R (also I learned a lot through stackexchange), but somehow my vizualization tells me something different.
The output of the glm in R is: </p>

<pre><code>Call:
glm(formula = Zustand ~ Temp + Tag + Gehege + Temp:Gehege + Tag:Gehege + 
    Individuum + Gehege:Individuum + Temp:Individuum + Tag:Individuum, 
    family = binomial)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.9257  -0.5462  -0.4408  -0.3106   2.8510  

Coefficients:
                               Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                   -3.124244   1.302784  -2.398 0.016479 *  
Temp                           0.083585   0.068098   1.227 0.219664    
Tag                           -0.005485   0.013584  -0.404 0.686364    
Gehegeneues                    3.646637   1.249182   2.919 0.003509 ** 
IndividuumJoachim             -0.769787   0.927953  -0.830 0.406791    
IndividuumMary                -0.420745   0.797966  -0.527 0.598005    
Temp:Gehegeneues              -0.169516   0.062047  -2.732 0.006294 ** 
Tag:Gehegeneues               -0.057955   0.017154  -3.379 0.000729 ***
Gehegeneues:IndividuumJoachim  0.728588   0.329791   2.209 0.027158 *  
Gehegeneues:IndividuumMary     0.951688   0.396865   2.398 0.016484 *  
Temp:IndividuumJoachim         0.015466   0.049143   0.315 0.752979    
Temp:IndividuumMary           -0.012944   0.044467  -0.291 0.770985    
Tag:IndividuumJoachim         -0.035718   0.019177  -1.863 0.062532 .  
Tag:IndividuumMary             0.016538   0.019597   0.844 0.398724  
</code></pre>

<p>But my vizualization with the visreg-package...</p>

<pre><code>visreg(Ergebnis3, ""Gehege"", by = ""Individuum"", type = ""contrast"", scale = 'response', rug = F, ylim = c(0.0,0.6), main = ""Preening / Moulting"", xlab = ""Enclosure"", ylab = ""Likelihood"")
</code></pre>

<p>...looks like this:</p>

<p><a href=""http://i.stack.imgur.com/YxQc4.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YxQc4.jpg"" alt=""enter image description here""></a></p>

<p>""Individuum = Georg"" and ""Gehege = altes"" is my reference level and I thought the coefficient of ""Gehege = neues"" (3.64) means that the probability of ""Zustand"" in ""Gehege = neues"" is 3.64 times higher than in ""Gehege = altes"" or not? But in the graphic it's lower. Also for changing continuous variables ""Temp"" and ""Tag"" (here it's shown i.e. for 19.5 Â°C) ""Gehege = neues"" keeps to be lower for most of the times. The odds Ratio for ""Gehege = neues"" are also about 38, this high number is a bit weird... </p>

<p>I hope this is enough information to help, I have a real complex question I want to answer.</p>
"
"0.0942390294485422","0.0968548555282575","185800","<p>I try to find a model using logistic regression. More precisely, what I did so far, is using stepwise regression and subset selection (although I know, it is often a bad idea) to find the ""best"" model. Clearly, depending on the information criteria I used, I got different results. </p>

<p>Now, I found an interesting example on page 250 in the book <a href=""http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf"" rel=""nofollow"">""An Introduction to Statistical Learning""</a>. They chose among the models of different sizes using cross-validation, that is they make predictions for each model and compute the test errors. Eventually, the compute the cross validation error and choose the model corresponding to the minimal average cross-validation error. </p>

<p>However, the function <code>regsubsets</code> of the R package ""leaps"" is only working for linear models. How can I implement this for logistic regression or glm models in general? </p>

<p>My idea was, to just estimate the models within a cross-validation using the <code>step</code> function of the ""stats"" package and then kind of take the average number of features (which is determined by minimum AIC, for example). Is this a legitimate approach?</p>
"
"NaN","NaN","186560","<p>I am fitting a multinomial logistic regression using the glmnet package in R:</p>

<pre><code>library(glmnet)
data(MultinomialExample)
cvfit=cv.glmnet(x, y, family=""multinomial"", type.multinomial = ""grouped"")
plot(cvfit)
</code></pre>

<p><a href=""http://i.stack.imgur.com/gF135.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/gF135.png"" alt=""rplot""></a></p>

<p>What is ""Multinomial Deviance"" and how does it relate to ""<a href=""https://www.kaggle.com/wiki/MultiClassLogLoss"" rel=""nofollow"">Multinomial Logloss</a>""?</p>
"
"0.145994476646782","0.150046896984107","186845","<p>I created some data using the following code:</p>

<pre><code>set.seed(1221)
x &lt;- runif(500)
y &lt;- runif(500,0,2)
z &lt;- rep(0,500)
z[-0.8*x + y - 0.75 &gt; 0] &lt;- 1
plot(x,y,col=as.factor(z))
</code></pre>

<p>This produces the following plot</p>

<p><a href=""http://i.stack.imgur.com/ycWdr.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ycWdr.png"" alt=""enter image description here""></a></p>

<p>The data is linearly separable. Then, I applied the glm function to create a logistic regression model.</p>

<pre><code>df &lt;- data.frame(class = z, x = x, y = y)
model &lt;- glm(z ~ x + y, family = binomial, data = df)
</code></pre>

<p>This produces the following output:</p>

<pre><code>summary(model)
Call:
glm(formula = z ~ x + y, family = binomial, data = df)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-8.127e-04  -2.000e-08  -2.000e-08   2.000e-08   7.699e-04  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)    -1062      52666   -0.02    0.984
x              -1163      57197   -0.02    0.984
y               1433      70408    0.02    0.984

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 6.8274e+02  on 499  degrees of freedom
Residual deviance: 1.3345e-06  on 497  degrees of freedom
AIC: 6

Number of Fisher Scoring iterations: 25
</code></pre>

<p>The result surprised me, first because the parameter estimates are huge, and second because I was expecting such estimates to be close to the original decision boundary function, i.e. <code>-0.8x + y - 0.75 = 0</code>.</p>

<p>I then used the <a href=""http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html"" rel=""nofollow"">glmnet</a> package to see if I could solve this issue. This package creates a penalised logistic regression model in order to deal with the large values in the parameter estimates. The code I used is the following:</p>

<pre><code>library(glmnet)
cvfit &lt;- cv.glmnet(as.matrix(df[,-1]), as.factor(df$class), family =   ""binomial"", type.measure = ""class"")
plot(cvfit)
</code></pre>

<p><a href=""http://i.stack.imgur.com/vH4AV.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/vH4AV.png"" alt=""enter image description here""></a></p>

<p>And the coefficients for the optimal penalty strength are:</p>

<pre><code>coef(cvfit, s = ""lambda.min"")
3 x 1 sparse Matrix of class ""dgCMatrix""
                    1
(Intercept) -84.01446
x           -91.40983
y           113.18736
</code></pre>

<p>Such coefficients are smaller than the ones obtained with the <code>glm</code> function. Still they are not the same as the decision boundary function. </p>

<p>Does anybody know why this is happening? Any help is greatly appreciated.</p>
"
"NaN","NaN","187443","<p>I wanna run sample selection on my data. My response is binary and also, I have some continuous and some binary variables. I was wondering how can I use ""sampleSelection"" package? 
It seems that it does not support logistic regression as well as factor variables.
Any thought?</p>

<p>PS. My goal is modeling my data using binomial logistic regression.</p>
"
"0.059601995508215","0.0612563891831689","187679","<p>I'd like to test two classifiers at the same time, that is logistic regression and classification trees. To find a classification threshold, which for example maximises F1-score, I split my data set into train, validation and test set. Because this is all pretty new to me, I wrote my own loop to understand the procedure behind it. I was wondering, how would you find manually (without just using cross-validation from the caret package) the optimal cp-value? Is the optimal cp the average of all the cp picked for each fold? But if so, what do I need the validation set for? In logistic regression, this is more clear to me since I need it to find the threshold which maximises my F1 score. I appreciate your help!</p>
"
"0.0842899503922179","0.086629616364842","189627","<p>I am using the <code>survey</code> package and my model is:  </p>

<pre><code>modelsvy &lt;- svydesign(id =~ 1, data=temp12, weights=~WGT) 
model12s &lt;- svyglm(DEPVAR ~ var1 + var2 +... ,  modelsvy, family= quasibinomial)  
</code></pre>

<p>This has been going well for me. </p>

<p>I have a sample of the US population (N=4,343), with fractional weights (<code>WGT</code>) on each observation. The completed model is then scored as a probability: </p>

<pre><code>mdlg2012$DEPVARSCR &lt;- predict(model12s,mdlg2012)  
mdlg2012$DEPVARSCRP &lt;- (1 / (1 + exp(-1 * mdlg2012$DEPVARSCR)))  
</code></pre>

<p>The probabilities are applied to census block-group data to estimate the number of households interested in buying things like Life Insurance.  </p>

<p>In a variation, I built a couple models that did not use all N. For example, I subset to N=2,339, with DEPVAR = 1 (n=178) and DEPVAR = 0 (n=2161). The entire population was not included. The proportion of N was 178 / 2,339 = 0.076. For N overall, it would have been 178 / 4343 = 0.041 (stated on an unweighted basis, I do need to weight them).  </p>

<p>My question is, would you happen to know how I might calibrate the resulting new model probabilities back to the full US population? I down sampled, removing n = 2,004, which inflated the DEPVAR incidence. The logistic transform in this case overstates the probabilities. Is the calibrate(design,...) command useful here?</p>
"
"0.145994476646782","0.137542988902098","189903","<p>I am conducting logistic regression analysis: The data includes 107 observations, dependent variable is a binary one, there is about 5 covariates which are both continuous, binary and multi-categorical variables. I want to use some cut_off points to predict the outcome.</p>

<p>So basicly, I select one cut_off point (based on the requirement that the sensitivity >70% and specificity > 70%). Then I devide my data into train set (85% data points) and test set (15% data points).</p>

<p>I fit the model with 5 covariates on the train set, and use the model to predict the outcome on the test set. I use the glm() function to fit the model, and glm.predict() function to predict on test sets. </p>

<p>Since there is missing data, I create 40 imputed data sets using MICE package in R. The procedure above is repeated over 40 imputed data sets. For each data set, I obtain the mis-classification errors.</p>

<p>So, to get the overall mis-classification errors, I averaged over 40 mis-classification error rates.</p>

<p>My question is: </p>

<p>How to assess the variability of this overall mis-classification errors?</p>

<p>As I am thinking that we can not use the usual formula to calculate the variance for this number, as the mis-classification errors over different imputed data sets might be correlated to each other.</p>

<p>Does any one have a suggestion or reference to do this?
(I am using R).
Thank you for any inputs.</p>
"
"0.0942390294485422","0.0968548555282575","190389","<p>I built a conditional logistic regression with the function clogit (package survival) in R and in which I included one categorical independent variable (habitat type) with 15 levels. I noted that the sign of parameter estimates changed between models that were built for each level of the categorical independent variable and a model that included the categorical variable (thus, all levels). Contrary to the model including the categorical variable, the results of models for each level of the categorical variable made sense from a biological standpoint. Does sign changes signify a multicollinearity issue? However, in my case, the values of VIFs for each level of the categorical variable were &lt; 3. Should I group some levels of my categorical variable because I noted the levels that were significant, were often those with few observations ?</p>
"
"0.0942390294485422","0.0968548555282575","191506","<p>My dependent variable has 4 categories, but when I run the multinomial logistic regression using the package <code>nnet</code> with function <code>multinom</code> the results only show 3 categories. </p>

<p>I've tried changing the category numbers from 0,2,3,4 to 1,2,3,4, and also tried using names instead of numbers for the categories but it still wont show all 4 categories in the results. </p>

<p>Also, when I changed the categories to names instead of numbers, the resulting p values for each category drastically changed. Why is this? 
The p values were acquired using these commands</p>

<pre><code>z &lt;- summary(siglm)$coefficients/summary(siglm)$standard.errors
p &lt;- (1 - pnorm(abs(z), 0, 1)) * 2
p
</code></pre>
"
"0.059601995508215","0.0612563891831689","191712","<p>I am using KFAS to fit a dynamic logistic model of the form;</p>

<p>$\hat{y} = \bf \beta_t x + \epsilon$ </p>

<p>$\beta_t = \beta_{t-1} + \eta$</p>

<p>So the regression parameters change over time, and act as latent variables to be estimated by the filter.</p>

<p>Can state space models of this form generally accept situations where we have multiple observations per time period? I believe they can, but I can't figure out how to specify this in KFAS (or any other R package for that matter).</p>

<p>I've tried the below code, but KFAS thinks that this means there are 22 time periods - there are actually only ten.</p>

<pre><code>library(KFAS)
y = c(1,0,0,0,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1)
i = seq.Date(from = as.Date(""2014-01-01""), as.Date(""2014-01-10""), length.out = 22)
x = rnorm(n = 22, mean = 1, sd = 2)

a =   model = SSModel(y ~ 
                    SSMregression(~x),
                  distribution = ""binomial"")

fit = fitSSM(a, inits = c(0,0))
</code></pre>
"
"0.157692057757926","0.162069171992452","191916","<p>I have taken plenty of time to try and help myself, but I keep reaching dead ends. </p>

<p>I have a dataset consisting of body measurements collected from a bird species, and the sex of each bird (known by molecular means). I built a logistic regression model (using the AIC information criterion) to assess which measurements explain better the sex of the birds. My ultimate goal is to have an equation which could be used by others under field conditions to predict reliably the sex of the birds by taking as few body measurements as possible. </p>

<p>My final model includes four independent variables, namely ""Culmen"", ""Head-bill"", ""Tarsus length"", and ""Wing length"" (all continuous). I wish my model was a little more parsimonious, but all the variables seem to be important according to AIC criterion. Because the model produced should be used as prediction tool, I decided validate it using a leave-one-out cross validation approach. In my learning process, I first tried to complete the analyses (cross-validation and plotting) by including only one explanatory variable, namely ""Culmen"". </p>

<p>The output of the cross validation (package ""boot"" in R) yields two values (deltas), which are the cross-validated prediction errors where the first number is the raw leave-one-out, or lieu cross-validation result, and the second one is a bias-corrected version of it. </p>

<pre><code>model.full &lt;- glm(Sex ~ Culmen, data = my.data, family = binomial)
summary(model.full.1)

cv.glm(my.data, model.full, K=114)

$call
cv.glm(data = my.data, glmfit = model.full, K = 114)

$K
[1] 114

$delta
[1] 0.05941851 0.05937288
</code></pre>

<p>Q1. Could anyone expalin what do these two values represent and how to interpret them?    </p>

<p>Following is the code as presented by Dr. Markus MÃ¼ller (Calimo) in a similar, albeit not identical, post (<a href=""http://stackoverflow.com/questions/20346568/feature-selection-cross-validation-but-how-to-make-roc-curves-in-r"">http://stackoverflow.com/questions/20346568/feature-selection-cross-validation-but-how-to-make-roc-curves-in-r</a>) which I tried to tweak to meet my data:</p>

<pre><code>library(pROC)
data(my.data)
k &lt;- 114    # Number of observations or rows in dataset
n &lt;- dim(my.data)[1]
indices &lt;- sample(rep(1:k, ceiling(n/k))[1:n])

all.response &lt;- all.predictor &lt;- aucs &lt;- c()
for (i in 1:k) {
test = my.data[indices==i,]
learn = my.data[indices!=i,]
model &lt;- glm(Sex ~ Culmen, data = learn, family=binomial)
model.pred &lt;- predict(model, newdata=test)
aucs &lt;- c(aucs, roc(test$Sex, model.pred)$auc)
all.response &lt;- c(all.response, test$outcome)
all.predictor &lt;- c(all.predictor, model.pred)
}

Error in roc.default(test$Sex, model.pred) : No case observation.

roc(all.response, all.predictor)

Error in roc.default(all.response, all.predictor) : No valid data provided.

mean(aucs)
</code></pre>

<p>Q2. What's the reason for the first error message? I guess the second error is associated with the first one, and that it will be solved once I find a solution to the first one.</p>

<p>I will appreciate very much any help!!</p>

<p>Luciano </p>
"
"NaN","NaN","193419","<p>What is the implication if I don't fix a logistic regression that has complete or quasi separation? can I still read the marginal effects or are they not going to be valid? </p>

<p>My exercise is actually to just find out which independent variables are most predictive of Y. </p>

<p>I read some responses to complete/quasi-separation and I tried using logistf package for R but got this error message ""NA/NaN/Inf in foreign function call logistf"". why does this arises? </p>
"
"0.11920399101643","0.122512778366338","193617","<p>I have a dataset on which there are about 1000 dummy variables indicating location. I do not have access to lat/long.</p>

<p>I am using xgboost to train it. The more that I train it does seem to be lowering the test-set error. However, when I try to look at the branches of the trees using <code>trees = xgb.model.dt.tree(names,model = model2)</code> <strong>all</strong> values of the split (<code>trees$Split</code>) are a negative value (-1.00136e-05).</p>

<p>Considering that these are dummy variables (1 or 0) for a negative split value it will always go to the yes branch. My question is how is it even learning anything with a negative split value? It seems the more rounds that I let it train the better the algorithm gets in terms of test set error. This doesn't make much sense.</p>

<p>If it helps its a multi-class classification problem with 3 classes and the error metric is log-loss.</p>

<p>edit: to answer questions of standardisation and -1/1 see below:</p>

<blockquote>
  <p>unique(as.vector(as.matrix(train_set)))</p>
</blockquote>

<pre><code>[1] 1 0
</code></pre>

<p>edit 2: This number seems to occur even on his basic example <a href=""https://github.com/dmlc/xgboost/blob/master/R-package/demo/basic_walkthrough.R"" rel=""nofollow"">here</a></p>

<p>edit 3: Minimal example included:</p>

<pre><code>require(xgboost)
set.seed(1)
data(agaricus.train, package='xgboost')
train &lt;- agaricus.train
bst &lt;- xgboost(data = as.matrix(train$data), label = train$label, max.depth = 2, eta = 1, nround = 2,
               nthread = 2, objective = ""binary:logistic"")
trees = xgb.model.dt.tree(dimnames(train$data)[[2]],model = bst)
    print(head(trees$Split))
&gt;&gt;&gt; [1] ""-1.00136e-05"" ""-1.00136e-05"" ""-1.00136e-05"" NA NA NA   
</code></pre>

<h1>Solution</h1>

<p>As Vadim points out below I was using a old version of xgboost (the one offered on cran). Use the following to update the version and you will get 0.5 as expected.</p>

<pre><code>install.packages(""drat"", repos=""https://cran.rstudio.com"")
drat:::addRepo(""dmlc"")
install.packages(""xgboost"", repos=""http://dmlc.ml/drat/"", type = ""source"")
</code></pre>
"
"0.0842899503922179","0.086629616364842","194140","<p>I've been using stepAIC to narrow down my logistic regression model.  However, I get the following warning when I run my model:</p>

<p>glm.fit: fitted probabilities numerically 0 or 1 occurred</p>

<p>I know this means I have complete or quasi-complete separation in my data.  On examination of my data, I see the quasi-complete separation and think that it's meaningful.  Reading online, I see recommendations to use a Firth penalized regression (logistf) or exact logistic regression (elrm); but neither of these will work with stepAIC.  I've also tried bayesglm but I still get the same warning. </p>

<p>How should I select a model when my data has complete separation?  How would I do this in R?  Is my mistake in my stats or in my understanding of using the packages in R?  Any help would be much appreciated!</p>
"
"0.0729972383233908","0.0750234484920533","194582","<p>I'm attempting <strong>one-to-one exact</strong> matching in R. That is, I have 60,000 treatment observations and 200,000 control observations but I only want 60,000 control observations and I want those to be matched exactly to the treatment set on a set of variables; normally this would reduce N greatly but it will not in this case. The <strong>one-to-one</strong> aspect of this can be done with nearest-neighbor matching in R's MatchIt package but that method isn't <strong>exact</strong>; it seems to rely on logistic regression propensity scores. MatchIt can also do <strong>exact</strong> matching but then it's not <strong>one-to-one</strong>. Does MatchIt, or another package, implement <strong>exact one-to-one</strong> matching? I realize I can probably create the solution myself with a lot of code but this seems like something that might already exist.</p>
"
"0.0842899503922179","0.086629616364842","196586","<p>I would like to do a gene x environment interaction analysis in a matched (1-1) case control samples. I referred all related previous publications and in most of the papers authors used either STATA or SAS. I got few references for performing conditional logisitic regression in R, for example using survival (clogit) package. But I couldn't find any reference for adding interaction terms in conditional logistic models in R. Can someone help me with references for interaction analysis using conditional logistic regression in R?</p>
"
"NaN","NaN","196734","<p>I have a panel data set with binary dependent variable of 20,000 observations and 11 independent variables.  I ran a logistic regression with fixed effects and the model returns maximum log likelihood value of <code>-7417.845</code> and AIC equals <code>Inf</code>. I am not sure why here the AIC value goes to infinity?</p>

<p>I am using R package ""<a href=""https://cran.r-project.org/web/packages/glmmML/glmmML.pdf"" rel=""nofollow"">glmmML</a>"".</p>
"
"0.11150512337989","0.114600210537152","196829","<p>I have a dataset with 15 binary covariates and a continuous response variable bounded between 0 and 1. The binary variables represent correct or incorrect answers on a short test and the response variable is a measure of the same test takers performance on a related but more advanced and reliable test. I would like to select the best variables and weights to predict the score on the more advanced test. What would be the best way of doing this?</p>

<p>PS. I'm not a statistician but a computer scientist with only basic statistics and machine learning in my portfolio.</p>

<p>(Side note: One idea I had was to use some kind of logistic L1 or L2 regularized regression, however, glmnet does not seem to accept non-binary response variables when fitting a logistic model, which I guess is reasonable for normal use. The built-in glm function does accept a (0,1)-bounded response but does not perform regularization. If this approach seems reasonable, any tips on suitable packages or would I have to implement it myself? Other ideas I had was using ""normal"" regularized regression, or perhaps Principal Component Regression, however, I have tried both these and they give very different results and neither perform very well.)</p>
"
"0.139779069524328","0.143658966607306","198801","<p>Logistic Regression using R's <code>glm</code> package is giving me the following summary (snap of the few first variables). </p>

<p><strong>My Data Set:</strong></p>

<ul>
<li>Dimensions: 1252 rows and 224 columns (after using model.matrix). The Data  has been standardized.</li>
<li>Response variable is binary.</li>
<li>Trying to predict if an employee will leave the company, based on employee attributes </li>
</ul>

<p><a href=""http://i.stack.imgur.com/Ai42v.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Ai42v.png"" alt=""enter image description here""></a></p>

<p><strong>My Understanding:</strong></p>

<p>The model does not give a good fit because:</p>

<ol>
<li>Residual Deviance > Null Deviance. </li>
<li>p.value = 1 - pchisq(3676.5, 817) turns out to be 0.</li>
<li>The first warning about 0 or 1 fitted probability message suggests that due to some predictor(s) the model might be giving perfect predictions </li>
<li>Second warning on â€˜rank deficiencyâ€™ suggests that there might be predictors that are linearly dependent on one another.</li>
</ol>

<p><strong>My Questions:</strong></p>

<ol>
<li>How can I improve the model? I would like to see Residual Deviance &lt; Null Deviance. I will invest time on dropping the linearly dependent variables in the model, but is there anything I should do first to test the â€˜modelâ€™ itself, before revisiting my data? I am asking this because SVM worked quite well on the same data set. </li>
<li>Why do I have such extreme coefficient values?</li>
<li>Many answers to other posts state that â€˜AICâ€™ is used to compare different</li>
<li>The summary parameters (coefficients , std error and p-values) for many dummy factors obtained via model.matrix, like GSS_SEXM,  is shown as 'NA'. Why is it so? </li>
<li>logistic models. What is meant by â€˜differentâ€™ here? Models trained on different data sets that bear different coefficients, like say different set of attributes? </li>
</ol>
"
"0.0942390294485422","0.077483884422606","199912","<p>I have a large dataset with 4000 variables and 15000 observations. I am looking to build a predictive model using logistic regression. I believe that the glmnet package (using elastic net) is the best tool to use with such a large set of variables. Every variable of the 4000 is a moving average. I have split the dataset into two - training and testing.</p>

<p>When I run the code with glmnet I find something unusual happening. As I increase the number of variables for glmnet to select the model probabilities get more and more extreme which causes the misclassification rate to converge to 0%. I realise something is wrong but I cannot figure what it is.</p>

<p>Here is the code I have used:</p>

<pre><code>x &lt;- as.matrix(training[1:4000])
newx &lt;- as.matrix(testing[1:4000])

model &lt;- cv.glmnet(x, y, alpha = 0.5, family = 'binomial')

predict(model, type = ""coefficients"",s = model$lambda.min)
predict(model, newx, type= ""response"",s = model$lambda.min)
</code></pre>

<p>Is this overfitting?
I also read that categorical variables need to worked around with glmnet - none of the 4000 are categorical but they are grouped by external categorical vars.</p>

<p>I'm desperate for some help!</p>
"
"0.11920399101643","0.107198681070546","199978","<p>I've been building a logistic regression model (using the ""glm"" method in caret). The training dataset is extremely imbalanced (99% of the observations in the majority class), so I've been trying to optimize the probability threshold during the resampling process using the train function from the caret package as described in this example of a svm model: <a href=""http://topepo.github.io/caret/custom_models.html#Illustration5"" rel=""nofollow"">Illustrative Example 5: Optimizing probability thresholds for class imbalances.</a></p>

<p>The idea is to get the classification parameters for different values of the probability thershold, like this:</p>

<pre><code>threshold   ROC    Sens   Spec   Dist   ROC SD  Sens SD  Spec SD  Dist SD
 0.0100     0.957  1.000  0.000  1.000  0.0366  0.0000   0.0000   0.0000 
 0.0616     0.957  1.000  0.000  1.000  0.0366  0.0000   0.0000   0.0000 
 0.1132     0.957  1.000  0.000  1.000  0.0366  0.0000   0.0000   0.0000 
 0.1647     0.957  1.000  0.000  1.000  0.0366  0.0000   0.0000   0.0000 
  ...        ...                  ...                      ...      ...
</code></pre>

<p>I noticed that the 'glm' method in caret uses 0.5 as the probability cutoff value as can be seen in the predict function of the model:</p>

<pre><code>code_glm &lt;- getModelInfo(""glm"", regex = FALSE)[[1]]
code_glm$predict
    function(modelFit, newdata, submodels = NULL) {
                    if(!is.data.frame(newdata)) newdata &lt;- as.data.frame(newdata)
                    if(modelFit$problemType == ""Classification"") {
                      probs &lt;-  predict(modelFit, newdata, type = ""response"")
                      out &lt;- ifelse(probs &lt; .5,
                                    modelFit$obsLevel[1],
                                    modelFit$obsLevel[2])
                } else {
                  out &lt;- predict(modelFit, newdata, type = ""response"")
                }
                out
              }
</code></pre>

<p>Any ideas about how to pass a grid of probability cutoff values to the predict function shown above to get the optime cutoff value?</p>

<p>I've been trying to adapt the code from the <a href=""http://topepo.github.io/caret/custom_models.html#Illustration5"" rel=""nofollow"">example shown in the caret website</a>, but I haven't been able to make it work. I think I'm finding difficult to understand how caret uses the model's interfaces... </p>

<p>Any help to make this work would be much appreciated... Thanks in advance.</p>
"
"0.0842899503922179","0.086629616364842","200477","<p>I am conducting multiple imputation by chained equations in R using the MICE package, followed by a logistic regression on the imputed dataset.</p>

<p>I need to compute a 95% confidence interval about the predictions for use in creating a plotâ€”that is, the grey shading in the image at this link.</p>

<p><a href=""http://imgur.com/guLEyTQ"" rel=""nofollow"">http://imgur.com/guLEyTQ</a></p>

<hr>

<p>I followed the approach described in the answer to this question...</p>

<p><a href=""http://stats.stackexchange.com/questions/66946/how-are-the-standard-errors-computed-for-the-fitted-values-from-a-logistic-regre"">How are the standard errors computed for the fitted values from a logistic regression?</a></p>

<p>...which uses the following lines of code to yield the std.er of prediction for any specific value of the predictor:</p>

<pre><code>o &lt;- glm(y ~ x, data = dat)
C &lt;- c(1, 1.5)
std.er &lt;- sqrt(t(C) %*% vcov(o) %*% C)
</code></pre>

<p>But of course <strong>I need to adapt this code to the fact that I am using a model resulting from multiple imputation</strong>.  In that context, I am not sure <strong><em>which</em></strong> variance-covariance matrix (corresponding to â€œvcov(o)â€ in the above example) I should be using in my equation to produce the ""std.er"".</p>

<hr>

<p>Based on the documentation for MICE I see three candidate matrices:</p>

<ul>
<li><p>ubar - The average of the variance-covariance matrix of the complete data estimates.</p></li>
<li><p>b - The between imputation variance-covariance matrix.</p></li>
<li><p>t - The total variance-covariance matrix.</p></li>
</ul>

<p><a href=""http://www.inside-r.org/packages/cran/mice/docs/is.mipo"" rel=""nofollow"">http://www.inside-r.org/packages/cran/mice/docs/is.mipo</a></p>

<p>Based on trying all three, the b matrix seems patently wrong, but both the t and the ubar matrices seem plausible.  Can anybody confirm which one is appropriate?</p>

<p>Thank you.</p>
"
"0.135164620935365","0.15049280256442","200703","<p>I'm using matched pairs logistic regression (1-1 matched case-control; Hosmer and Lemeshow 2000) to model differences between vegetation selected at nest sites vs. paired random sites. To do this, I created a data frame that contained the difference in vegetation measurements between nest and random sites (so nest minus random) and used R to fit a logistic regression model, using a vector of all 1's as the 'Response' and a no-intercept model.</p>

<p>Here's the data frame (I only include 1 of the covariates, grass density, for the example):</p>

<pre><code>nest&lt;-structure(list(VerGR = c(1.380952381, 1.952380953, 2.666666667, 
-3.809523809, 2.428571428, 2.142857143, 0.142857143, 2.095238095, 
1.952380952, 3.333333334, 3.190476191, -2.857142857, 2.857142858, 
-1.666666667, 0.523809524, 4.761904762, 0.571428571, 2.238095238, 
-2.809523809, 0.857142857, 1.523809524, -2.476190476, -0.428571428, 
-5.190476191, 4.142857143, 2.857142858, -2.476190476, 4.095238096, 
1.428571428, 1.714285714, -2.80952381, 3.142857143, 2.809523809, 
7.238095238, 2.523809523, 2.333333333, -0.095238096, -0.095238096, 
-0.142857143, 4.047619048, 4.761904759, -1.285714285, -1.190476191, 
2.523809524, -2.095238095, -2, 4.761904761, 8.952380952, 1.095238096, 
5.666666666, -0.714285714, 0, 2.809523809, -0.238095239, 3.666666667, 
0.904761905, -4.952380952, -3.666666667, 2, -0.619047619, 4.523809524, 
1.523809524, 4.619047619, 6.142857143, 3.19047619, -2.190476191, 
-1.666666667, 2.714285714, -1.285714286, 2.857142857, 2.761904762, 
2.809523809, -7.142857139, -5.952380949, -1.19047619, 1.523809524, 
-0.38095238, 5.571428571, 5.238095239, 2.047619048, 7.857142857, 
0.61904761, 2.523809524, -1.190476191), Response = c(1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L)), .Names = c(""VerGR"", ""Response""), class = ""data.frame"", row.names = c(NA, 
-84L))
</code></pre>

<p>And the no-intercept logistic regression models I am running:</p>

<pre><code>grass.mod &lt;- glm(Response ~ VerGR - 1, data=nest, family=""binomial"")
grass2.mod &lt;- glm(Response ~ VerGR + I(VerGR^2) - 1, data=nest, family=""binomial"")
</code></pre>

<p>For the most part the models run fine, and give the same parameter estimates as models implemented using the 'clogit' function from the survival R package. The data set for the clogit models is slightly different, with Responses = 1 (nest) or = 0 (random point), and includes a column called 'PairID' to indicate nest-random pairs. Here's what the clogit models look like:</p>

<pre><code>library(survival)
grass.mod.clog &lt;- clogit(Response ~ VerGR + strata(PairID), data=full)
grass2.mod.clog &lt;- clogit(Response ~ VerGR + I(VerGR^2) + strata(PairID), data=full)
</code></pre>

<p>But when I run the glm's, I get these 2 warnings if using a quadratic term:</p>

<pre><code>Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
</code></pre>

<p>I'm able to satisfy the first warning if I use more iterations in the glm formula, but I'm not sure what is happening with the second warning. I would be glad to use the 'clogit' function (which works with quadratic terms), but I'm unsure how to create prediction plots to visually display the data when going that route. Any suggestions?</p>

<p>Thanks,
Jay</p>
"
"0.163226787060855","0.156573695352373","202973","<p>Suppose I have a data set of <code>N</code> observations <code>(n = 1...N)</code> for out-of-sample estimation and values of ($y_n$). I have also <code>I</code> statistical models <code>(i= 1...I)</code> which every model has its own estimate on each data point ($\hat{y}^i_n$).</p>

<p>In addition I have a model selection method $\phi$ which would pick a model's estimate among the model set as its own according to its assessment on previous performance of the models ($\hat{y}^\phi_n = \min_i\{\hat{f_i}(y_n), i \in I\} $).</p>

<p>My claim should be ""model selection's performance is better than all models it picks estimates from"". I am trying to find a proper method to describe the statistical power of the model selection method, compared to individual models in the model set.</p>

<p>All individual models follow different assumptions, distributions and dependence structure. Some are iid, some have heteroskedasticity. Actually, there is no restriction on models except it should yield an estimate.</p>

<p>Some The models are employed on time series but what they do is asset pricing on different assets and contracts. But for a broaded audience I will make the following analogy.</p>

<p>Suppose you have a machine that predicts the scores on basketball matches. It does not only predict the final score, it also predicts a distribution of the scores throughout the time. It also predicts which player will score when.</p>

<p>Suppose you have many machines of this sort and all have different predictions. All of them had been right on some occasion (That is what statistics is after all right? No model is perfect.). </p>

<p>I am trying to figure out which machine is better at predicting what and when, using the previous performance of the machines. I can say stuff like 'oh machine A was good at predicting scores occured in the last 10 mins, but for the last 2 months model B became better'. </p>

<p>It turns out my estimates using the machines are better than any machine could do it alone in the long run. I checked for several error terms starting with MAPE and MSE. But I want to show that it is not a coincidence but a statistically significant fact. I have a fair sample size (~100k) over a good enough time period (5 years).</p>

<p>I fiddled with some thoughts about proportion of $\phi$ selecting the model with the lowest error and some logistic regression on that according to the criteria it uses to pick the models. But I lack the comprehensive knowledge on this domain of statistics.</p>

<p>ps. R package suggestions are also appreciated.</p>
"
"0.140266956067134","0.144160395589483","203816","<p>I am trying to duplicate the results from <code>sklearn</code> logistic regression library using <code>glmnet</code> package in R.</p>

<p>From the <code>sklearn</code> logistic regression <a href=""http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"" rel=""nofollow"">documentation</a>, it is trying to minimize the cost function under l2 penalty
$$\min_{w,c} \frac12 w^Tw + C\sum_{i=1}^N \log(\exp(-y_i(X_i^Tw+c)) + 1)$$</p>

<p>From the <a href=""https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet_beta.html#log"" rel=""nofollow"">vignettes</a> of <code>glmnet</code>, its implementation minimizes a slightly different cost function
$$\min_{\beta, \beta_0} -\left[\frac1N \sum_{i=1}^N y_i(\beta_0+x_i^T\beta)-\log(1+e^{(\beta_0+x_i^T\beta)})\right] + \lambda[(\alpha-1)||\beta||_2^2/2+\alpha||\beta||_1]$$</p>

<p>With some tweak in the second equation, and by setting $\alpha=0$, $$\lambda\min_{\beta, \beta_0} \frac1{N\lambda} \sum_{i=1}^N \left[-y_i(\beta_0+x_i^T\beta)+\log(1+e^{(\beta_0+x_i^T\beta)})\right] + ||\beta||_2^2/2$$</p>

<p>which differs from <code>sklearn</code> cost function only by a factor of $\lambda$ if set $\frac1{N\lambda}=C$, so I was expecting the same coefficient estimation from the two packages. But they are different. I am using the dataset from UCLA idre <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">tutorial</a>, predicting <code>admit</code> based on <code>gre</code>, <code>gpa</code> and <code>rank</code>. There are 400 observations, so with $C=1$, $\lambda = 0.0025$.</p>

<pre><code>#python sklearn
df = pd.read_csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
y, X = dmatrices('admit ~ gre + gpa + C(rank)', df, return_type = 'dataframe')
X.head()
&gt;  Intercept  C(rank)[T.2]  C(rank)[T.3]  C(rank)[T.4]  gre   gpa
0          1             0             1             0  380  3.61
1          1             0             1             0  660  3.67
2          1             0             0             0  800  4.00
3          1             0             0             1  640  3.19
4          1             0             0             1  520  2.93

model = LogisticRegression(fit_intercept = False, C = 1)
mdl = model.fit(X, y)
model.coef_
&gt; array([[-1.35417783, -0.71628751, -1.26038726, -1.49762706,  0.00169198,
     0.13992661]]) 
# corresponding to predictors [Intercept, rank_2, rank_3, rank_4, gre, gpa]


&gt; # R glmnet
&gt; df = fread(""http://www.ats.ucla.edu/stat/data/binary.csv"")
&gt; X = as.matrix(model.matrix(admit~gre+gpa+as.factor(rank), data=df))[,2:6]
&gt; y = df[, admit]
&gt; mylogit &lt;- glmnet(X, y, family = ""binomial"", alpha = 0)
&gt; coef(mylogit, s = 0.0025)
6 x 1 sparse Matrix of class ""dgCMatrix""
                    1
(Intercept)      -3.984226893
gre               0.002216795
gpa               0.772048342
as.factor(rank)2 -0.530731081
as.factor(rank)3 -1.164306231
as.factor(rank)4 -1.354160642
</code></pre>

<p>The <code>R</code> output is somehow close to logistic regression without regularization, as can be seen <a href=""http://stats.stackexchange.com/questions/203740/logistic-regression-scikit-learn-vs-statsmodels"">here</a>. Am I missing something or doing something obviously wrong?</p>

<p>Update: I also tried to use <code>LiblineaR</code> package in <code>R</code> to conduct the same process, and yet got another different set of estimates (<code>liblinear</code> is also the solver in <code>sklearn</code>):</p>

<pre><code>&gt; fit = LiblineaR(X, y, type = 0, cost = 1)
&gt; print(fit)
$TypeDetail
    [1] ""L2-regularized logistic regression primal (L2R_LR)""
    $Type
[1] 0
$W
            gre          gpa as.factor(rank)2 as.factor(rank)3 as.factor(rank)4         Bias
[1,] 0.00113215 7.321421e-06     5.354841e-07     1.353818e-06      9.59564e-07 2.395513e-06
</code></pre>

<p>Update 2: turning off standardization in <code>glmnet</code> gives:</p>

<pre><code>&gt; mylogit &lt;- glmnet(X, y, family = ""binomial"", alpha = 0, standardize = F)
&gt; coef(mylogit, s = 0.0025)
6 x 1 sparse Matrix of class ""dgCMatrix""
                     1
(Intercept)      -2.8180677693
gre               0.0034434192
gpa               0.0001882333
as.factor(rank)2  0.0001268816
as.factor(rank)3 -0.0002259491
as.factor(rank)4 -0.0002028832
</code></pre>
"
"0.059601995508215","0.0612563891831689","203863","<p>I'm using R,OpenBUGS and R2OpenBUGS package on Linux Mint and i'm fitting a bayesian multinomial logistic model. After dribbling a lot of usual errors on my way to get my model running, i came upon an error i can't get the grasp of how to solve it. </p>

<p>So after ""model is syntactically correct,data loaded, model compiled,initial values generated, model initialized"" messages,
 i get 
""inference can not be made when sampler is in adaptive phase"".</p>

<p>Does anyone know what exactly this message mean and how could i solve it?
Thanks</p>
"
"0.119946702195905","0.136973450269748","204145","<p><strong>Background and Problem</strong></p>

<p>I have a question concerning a meta-analysis combining effects from between- and within-subject designs using log-odds ratios (OR) as the metric of interest. I am familiar with conducting meta-analyses and will be undertaking my calculations in R (using the <code>metafor</code> and <code>lme4</code> packages). To provide greater context, the studies in question ask research subjects to make a binary decision with respect to a personal preference across one of two conditions. In some cases, each participant is assigned to a single condition (making only a single binary response); in others, each subject takes part in both conditions (making two binary responses). For now, presume I have the raw data in all cases. The issue I face is how best to calculate an OR that is comparable across design and whether I should take the correlation between conditions into account for the within-subject designs.</p>

<p><strong>My Current Approach</strong></p>

<p>I presently use logistic regression to estimate the OR for between-subject designs. The slope represents the OR and the sampling variance can be calculated by squaring the SE of the slope coefficient. Using this approach produces estimates comparable to equations reported in common texts such The Handbook of Research Synthesis and Meta-Analysis, 2nd Edition (p. 243). I then extend this approach to use a multilevel logistic regression model including a random intercept by subject to estimate the OR for within-subject designs while account for the dependency between conditions. The OR and sampling variance are otherwise calculated in the same fashion. </p>

<p><strong>My Questions:</strong></p>

<p>With this in mind, I would like to ask:</p>

<ol>
<li>Is it reasonable to meta-analytically aggregate OR calculated using standard and multilevel logistic regression?</li>
<li>Would it be better to use standard logistic regression for both designs (ignoring the correlation between conditions for the within-subject designs)?</li>
</ol>
"
"0.230837536004046","0.237244975155213","206042","<p>I implement <code>n</code> permutations into a regression analysis, to test the model for stability. Thus I obtain <code>n</code> odds ratios (ORs) and <code>n</code> associated 95% CI intervals. </p>

<p>Each permutation represents a matched-pair study. We pair similar <code>case</code>'s with <code>control</code>'s and then run a conditional logistic regression to obtain a measure of association between the outcome of interest and exposure variable (treatment status).</p>

<p>Taking the following example I have implemented into a <code>R</code> script.
In short what I have done is:</p>

<ol>
<li>Take a portion of the a given population</li>
<li>We assign at dummy variable to the population (1/0) to indicate treatment status</li>
<li>based on a set of parameters we pair those with treatment status==1 to equivalent treatment status==0</li>
<li>we define an outcome of interest that we wish to measure if treatment had an effect on the outcome</li>
<li>We conduct a logistic regression to determine the ORs associated with treatment status</li>
<li>we repeat this n time, each time obtaining an ORs and associated 95% confidence interval</li>
</ol>

<p><strong>But what I am not sure, is how I can report on the spread of my data. I generate a different odds ratio and 95% CI for each permutation.</strong></p>

<p>Taking the following hypothetical example, we run a simulation 100 times. It only takes a minute to simulate.</p>

<p>We take an worked exampled from the <a href=""https://cran.r-project.org/web/packages/Matching/Matching.pdf"" rel=""nofollow"">Matching package</a> in R. </p>

<pre><code>set.seed(123)    
# preamble, prepare the data for the simulation
    #1.
    library(Matching)
    library(survival)
    #2.
    require(doParallel)
    cl&lt;-makeCluster(2)
    registerDoParallel(cl)
    #3.
    clusterEvalQ(cl,library(Matching))
    clusterEvalQ(cl,library(survival))

    m &lt;- 100


    Result = foreach(i=1:m,.combine=cbind) %do%{

      # attach the data
      data(lalonde)

      # we want to assess if treatment is associated with greater odds for the outcome of interest
      # lets create our hypothetical outcome of interest
      lalonde$success &lt;- with(lalonde, ifelse(re78 &gt; 8125, 1, 0))

      # lets take a portion of the original population, say only 395
      n &lt;- sample(1:445,420, replace = F)
      n &lt;- sort(n, decreasing = F)
      lalonde &lt;- lalonde[n,]
      head(lalonde$age)

      # taking from the example from GenMatch (in Matching package)
      #The covariates we want to match on
      # but we only include some of the original variables (we come back to the others later)
      X = cbind(lalonde$age, lalonde$educ, lalonde$black, lalonde$hisp, 
                lalonde$married, lalonde$nodegr)

      #The covariates we want to obtain balance on
      BalanceMat &lt;- X

      # creat our matrix
      genout &lt;- GenMatch(Tr=lalonde$treat, X=X, BalanceMatrix=BalanceMat, 
                         pop.size=16, max.generations=10, wait.generations=1)


      # match our collisions on a 1-1 basis
      mout &lt;- Match(Y=NULL, Tr=lalonde$treat, X=X, Weight.matrix=genout, ties = F, replace = F)
      summary(mout)

      # here we create our case and control populations
      treat &lt;- lalonde[mout$index.treat,]
          control &lt;- lalonde[mout$index.control,]

      # and we want to apply a unique identifier for each pair
      # we call this command during the regression
      treat$Pair_ID &lt;- c(1:length(treat$age))
      control$Pair_ID &lt;- treat$Pair_ID 

      # finally we combine the data
      matched &lt;- rbind(treat, control)

      # now we run a conditional logitic regression on the paired data to determine the Odds Ratio associated with treatment
      # we account for the difference in pairs by the strata() command
      # we account for some of the original matching parameters that we removed from the matching process
      model_1 &lt;- clogit(success ~ treat + strata(Pair_ID) + re74, matched, method = 'efron')
      summary(model_1)

      OR_M1 &lt;- exp(model_1$coefficients[1])
      CI_U1 &lt;- exp(confint(model_1))[1,2]
      CI_L1 &lt;- exp(confint(model_1))[1,1]

      Result &lt;- rbind(OR_M1, CI_U1, CI_L1)

    }
</code></pre>

<p>To summarise the script:</p>

<ul>
<li>we take 420 people from the original population (of 445)</li>
<li>we define the outcome of interest is. That is if the person had <code>re78 &gt; 8125</code> yes or no</li>
<li>for each treat==1, we find an equivalent treat==0 based on age, educ,  black, hisp, married, nodegr. We only want exact 1-1 matching</li>
<li>we assign an unique indicator variable for each pair 1,2,3.....x</li>
<li>We then develop a regression model to determine the OR for our outcome of interest (<code>re78 &gt; 8125</code>) associated with the treatment status (=1 relative to =0). </li>
<li>we save the ORs and 95%CI</li>
</ul>

<p>We can then plot the ORs and shade the 95%CI</p>

<pre><code>plot(Result[1,], ylim=c(0,2.5))
polygon(c(1:m,m:1), c(Result[3,],Result[2,]),col=adjustcolor(""grey"", alpha=0.4), border = NA)
</code></pre>

<p><strong>But how can I summarise the several ORs I obtained, the spread of it and/or an associated confidence level?</strong></p>

<p><strong>EDIT</strong>
Am I able to assess my study as if it was a meta-analysis. If so, one could implement the solution proposed by @Bernd Weiss <a href=""http://stats.stackexchange.com/questions/9483/how-to-calculate-confidence-intervals-for-pooled-odd-ratios-in-meta-analysis?rq=1"">here</a>?</p>

<p>For this we need to obtain the natural log of the ORs and the std. err.?</p>

<p>We update the last part of the command to:</p>

<pre><code>.......    
model_1 &lt;- clogit(success ~ treat + strata(Pair_ID) + re74, matched, method = 'efron')
      summary(model_1)

  OR_M1 &lt;- exp(model_1$coefficients[1])

  l_OR_T2 &lt;- model_1$coefficients[1]
  s_e &lt;- coef(summary(model_1))[1,3]

  CI_U1 &lt;- exp(confint(model_1))[1,2]
  CI_L1 &lt;- exp(confint(model_1))[1,1]

  Result &lt;- rbind(OR_M1, l_OR_T2, s_e, CI_U1, CI_L1)
</code></pre>

<p>Using we can then call upon the <code>metagen()</code>, command</p>

<pre><code>library(meta)
or.fem &lt;- metagen(as.numeric(Result[2,]), as.numeric(Result[3,]), sm = ""OR"")
</code></pre>

<p>Where <code>as.numeric(Result[2,])</code> is the log(OR) and <code>as.numeric(Result[3,])</code> is the std. err. Thus we obtain a 95% CI ...... But have we introduced a bias in the CI by the imputations. We see our 95% range is significant (greater than 1), however for each permutation, we only get a lower 95% CI > 1 </p>

<pre><code>sum(as.numeric(Result[5,])&gt;1.00)
</code></pre>

<p>times. Therefore I think the large <code>n</code> and thus <code>degrees of freedom</code> in the meta-analysis are giving us a significant result </p>
"
"0.11920399101643","0.122512778366338","206075","<p>I'm relatively new to machine learning (started about 5 months ago), and I'm looking at potentially implementing an ensemble classifier as part of my research. </p>

<p>I have built 3 models that I use to classify whether sales data is going to win or lose. Each model produces the probability of the sale winning or losing, and then I apply thresholds to those to classify them as either a ""Win"", ""Loss"" or ""Borderline Loss"". There are 25 variables, all of which are discrete. </p>

<p>The three models are Naive Bayes, Tree Augmented Naive Bayes (TAN) and Logistic Regression. I am using the bnlearn package for the bayesian classifiers, and a simple glm for the Logistic Regression. All models have high accuracy performances when tested on unseen data:</p>

<p>Naive Bayes Accuracy: 88% </p>

<p>TAN Accuracy: 91%</p>

<p>Logistic Regression Accuracy: 92%</p>

<p>I want to try implementing an ensemble classifier to see if I can get the best possible accuracy across all three models. My question is, how do I go about implementing something like this? I can't find too many examples online, at least not with these models for implementing one. From what I have read, one way to do it is to have a voting system, where if the 2 models predict the sale will win, but 1 predicts with will lose, then it is classified as a win. But what happens in this case if all 3 models had different predictions? I have all my prediction data ready, as in I have all the test data and each models prediction for each sale, my question so is, how would I proceed from here? </p>

<p>If someone knows of any available resources or tutorials that may help, I would greatly appreciate it!</p>
"
"0.0842899503922179","0.086629616364842","207177","<p>I am fitting a logistic regression model for the likelihood of patients suffering morbidity after surgery. The most commonly used prediction tool at the moment is POSSUM (Physiological and Operative Severity Score for the enUmeration of Mortality and Morbidity), which I would like to compare my model against.</p>

<p>In terms of discrimination, I have the Area Under the ROC curves calculated for both and would like to compare the two. </p>

<p>It seems in Stata that the command to use is <code>roccomp</code>. This produces a chi2 statistic and a p-value.</p>

<p>The R equivalent seems to require the <code>pROC</code> package and the function to use is <code>roc.test()</code>. However this function returns a z-statistic and p-value.</p>

<p>Looking at the documentation, both seem to be implementations of DeLong et al's methods of comparing AUROCs[1], but I cannot for the life of me understand why one gives a chi2 and the other a z-statistic. Are the tests equivalent?</p>

<p><em>Reference</em>:
1. Elisabeth R. DeLong, David M. DeLong and Daniel L. Clarke-Pearson (1988) â€œComparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approachâ€. Biometrics 44, 837--845.</p>

<p><strong>EDIT</strong>: Does this have anything to do with the explanation: <a href=""http://stats.stackexchange.com/questions/173415/at-what-level-is-a-chi2-test-mathematically-identical-to-a-z-test-of-propo/173483#173483"">At What Level is a $\chi^2$ test Mathematically Identical to a $z$-test of Proportions?</a> ?</p>
"
"0.139779069524328","0.143658966607306","208090","<p>I have a dataset with multiple dependent variables, which are counts of about 53 different categories of debris found on beaches. I also have a variety of independent variables, some of which I am interested in as fixed effects, but others of which are probably best off as random effects. They are a mix of categorical and continuous variables, e.g. State, county, distance from north to south, number of people present, etc. </p>

<p>A very tiny sample data set is as follows, though there are additional factors such as distance to road, date of survey, number of people on beach, and multiple transects per site. </p>

<pre><code>Counts&lt;- as.data.frame(matrix (rpois(100,1), ncol=5))
colnames(Counts)&lt;-c(""Glass"", ""HardPlastic"", ""SoftPlastic"", ""PlasticBag"", ""Fragments"")
State&lt;-rep(c(""CA"",""OR"",""WA""), each=6)
Counts$State&lt;-c(State,""CA"",""OR"")
    County&lt;-rep((1:9), each=2)
    Counts$County&lt;-c(County, 1,4)
Counts$Distance&lt;-c(10, 15, 13, 19, 18, 23, 38, 40, 49, 44, 47, 45, 52, 53, 55, 59, 51, 53, 14, 33)
    Year&lt;-rep(c(""2010"",""2011"",""2012""), times=7)
    Counts$Year&lt;-Year[1:20]
</code></pre>

<p>I would like to know whether the data vary by state, whether they change over time, and ultimately, whether the variability is higher within a site or between sites. </p>

<p>I think the best way to look at the data would be through a multinomial logistic regression model. I have been working in R, so I have looked at nnet (multinom) and VGAM (vglm), but it appears that neither of these support random effects. It could also be useful to have a smooth on some of the geographic data, so I've had a look at mgcv, but I can't find whether that package would support multiple dependent variables. </p>

<p>I have read that MCMCglmm will handle random effects, but I must admit I am a bit daunted by the complexity of how to set up the model structure, especially with respect to the priors. </p>

<p>My specific questions therefore are:</p>

<ol>
<li>can mgcv handle multiple DVs? </li>
<li>Is there another package I have overlooked?</li>
</ol>

<p>UPDATE:</p>

<p>I have found the following site: <a href=""http://search.r-project.org/library/mgcv/html/mvn.html"" rel=""nofollow"">http://search.r-project.org/library/mgcv/html/mvn.html</a>, which describes using the mvn family in mgcv as a way to run multivariate normal additive models. The model structure then for my sample data set would appear to look something like this:</p>

<pre><code>b&lt;-gam(list(Glass~s(State)+s(County),SoftPlastic~s(State)+s(County),
  PlasticBag~s(State)+s(County),family=mvn(d=3),data=Counts))
</code></pre>

<p>If this is correct, however, the model will get enormously long and complex as additional categories are added, and furthermore, I don't believe my data are normally distributed. </p>

<p>In other words, I'm still very much looking for an answer to this question!</p>
"
"0.042144975196109","0.043314808182421","209374","<p>I'm using the <code>multinom</code> package in R to run a multinomial logistic regression model. My dependent variable has 3 levels and as the output, I'm getting the probability for each of the level.</p>

<p>Currently, I have the VIF, AIC, p-values and confusion matrix in the model.</p>

<p>I have the following questions:</p>

<ol>
<li><p>I want a single output based on the probabilities. How do I decide a ""cut-off"" for deciding the ""best event""?</p></li>
<li><p>Does it make sense to get an ROC curve here? If yes, then how do I get one?</p></li>
<li><p>What are the things I should look at for the validation of the model?</p></li>
</ol>
"
"0.0942390294485422","0.077483884422606","212213","<p>I have a problem with this ""-inf"" value in my table and I don't understand where it's coming from. As long as it's there, I cannot run a proportional odds assumption check. Here is my model, I used the MASS package:</p>

<pre><code>    m27 &lt;- polr(typ ~ bek2 + vst2 + verw2 + nwoe2, data = typmed23, method=""logistic"", Hess=T)

    summary(m27)
    Call: polr(formula = typ ~ bek2 + vst2 + verw2 + nwoe2, data = typmed23, Hess = T, method = ""logistic"")

    Coefficients:
        Value Std. Error t value
bek2   0.4620     0.2705  1.7080
vst2   0.1169     0.3217  0.3635
verw2 -0.7230     0.2580 -2.8028
nwoe2  1.8877     0.2791  6.7626

Intercepts:
     Value   Std. Error t value
1|2  2.9883  1.1341     2.6349
2|3  5.2964  1.1764     4.5021

Residual Deviance: 373.3716 
AIC: 385.3716 
</code></pre>

<p>Here is my code for the table:</p>

<pre><code>    sf &lt;- function(y) {
      c('Y&gt;=1' = qlogis(mean(y &gt;= 1)),   #typ outcome
        'Y&gt;=2' = qlogis(mean(y &gt;= 2)),
        'Y&gt;=3' = qlogis(mean(y &gt;= 3)))
    }
    s &lt;- with(typmed23, summary(as.numeric(typ) ~ bek2 + vst2 + verw2 + nwoe2, fun=sf))  
</code></pre>

<p>which produces this table:</p>

<pre><code>    as.numeric(typ)    N=244

    +-------+-+---+----+----------+-----------+
    |       | |N  |Y&gt;=1|Y&gt;=2      |Y&gt;=3       |
    +-------+-+---+----+----------+-----------+
    |bek2   |1|104|Inf | 0.6359888|-0.63598877|
    |       |2|128|Inf | 0.7884574|-0.54430155|
    |       |3| 12|Inf | 0.0000000|-1.60943791|
    +-------+-+---+----+----------+-----------+
    |vst2   |1| 55|Inf | 1.7707061| 0.10919929|
    |       |2|148|Inf | 0.8602013|-0.55431074|
    |       |3| 41|Inf |-1.0033021|-2.97041447|
    +-------+-+---+----+----------+-----------+
    |verw2  |1| 77|Inf | 3.6243409| 0.73236789|
    |       |2| 76|Inf | 1.1700713|-0.89794159|
    |       |3| 91|Inf |-0.7598386|-1.98413136|
    +-------+-+---+----+----------+-----------+
    |nwoe2  |1| 58|Inf |-2.3608540|       -Inf|
    |       |2| 40|Inf |-0.1000835|-0.96940056|
    |       |3|146|Inf | 2.8478121| 0.02739897|
    +-------+-+---+----+----------+-----------+
    |Overall| |244|Inf | 0.6808771|-0.62625295|
    +-------+-+---+----+----------+-----------+
</code></pre>

<p>My dataframe is fine: No NA or huge values, no empty cells, only 5 columns containing values between 1 and 3. Can somebody tell me what to do and how to solve the problem? </p>

<p>Here is the variable (column in a df) </p>

<pre><code>nwoe2: typmed23$nwoe2
 [1] 3 3 3 3 3 3 3 1 1 1 3 3 3 2 3 3 2 2 1 2 1 3 3 3 2 2 1 2 3 3 3 1 3 3 3
 [36] 3 2 3 2 3 3 3 2 3 1 3 3 1 3 1 3 3 3 2 1 1 3 3 2 1 1 1 3 3 2 3 3 3 2 1
 [71] 1 3 3 3 3 1 2 3 1 3 1 1 3 3 3 3 3 2 2 3 3 3 3 3 3 3 3 3 1 3 3 3 3 3 3
 [106] 3 3 3 1 3 3 3 3 3 1 2 2 1 3 3 2 3 3 3 3 1 1 1 1 3 3 3 2 3 2 1 3 3 3 3
 [141] 1 2 3 1 3 3 3 3 3 1 3 3 3 1 2 2 3 1 3 3 1 2 3 2 2 1 3 3 3 3 3 1 1 3 3
 [176] 1 2 3 3 3 2 1 1 2 3 3 3 1 3 3 1 1 3 3 3 2 2 2 3 2 3 3 1 1 3 1 2 3 3 2
 [211] 3 1 3 3 2 3 3 1 3 3 1 1 3 3 3 3 3 3 3 1 1 3 1 3 3 3 2 3 3 2 1 1 1 3
</code></pre>

<p>Just to avoid any confusion: I'm referring to the lonely ""-Inf"" in column ""Y>=3"", not to the column ""Y>=1"".</p>
"
"0.059601995508215","0.0612563891831689","212375","<p>I work most of my time with categorical data (predictors and outcome), I usually do a trees in SPSS to make groups and rank which groups are more predominant to buy / not buy.</p>

<p>But now I'm into R, and I'm finding limitations trying to use other modelling techniques as they require numerical data (SVM, NN, Logistic Regression)... I'm using the package ""caret"" to do the models but I usually get no results.</p>

<p>How do you treat categorical variables to fit in those models?</p>

<p>Most of the data is multinomial... should I create n-1 predictors for a n-level predictor? Does R have a function to do this automatically?</p>
"
"0.145994476646782","0.150046896984107","212611","<p>I have ran these two Logistic Regression models (below) on some small data and I am able to interpret the output - significance and direction - of the regressors, but I do not know for sure how to interpret all the data which is supposed to tell me everything related to <strong>effect (size) etc</strong>. I did select my predictors properly by adding one each time and checking whether the model was still significant (which yielded the same result as an automatic stepAIC from the MASS package) and I also did some diagnostic checks (outliertest, VIF-score).</p>

<p>What (I think) I got from the models is:</p>

<ul>
<li><strong>R2</strong>: model1 only explains 4.8% of all variation and model2 6.6%, so no predictive power?</li>
<li><strong>C</strong>: model1 does not have acceptable discrimination, neither does model2 (&lt;0.7)</li>
</ul>

<p>Is there other <strong>important information that I am ignorant of</strong>? It seems that these models do <strong>not have much 'power'</strong> (according to <strong>R2</strong> and <strong>C</strong>), but how are they then <strong>still significant</strong> (there is also very significant behaviour (***) for regressors)?</p>

<p>*PS: Sorry if am missing obvious things - I do not have that strong of a statistical background. I am also finding it a hard time searching for all the parameters and metrics since they are often denoted by a one letter name (e.g. C, g) - which is not easy to search for if you do not know what you are looking for... So that's why I came to CrossValidated!</p>

<p>I have found <a href=""http://stats.stackexchange.com/questions/104485/logistic-regression-evaluation-metrics"">this question</a>, but it does not really have an answer since it's maybe way too vague? If someone else has a reading suggestion for my problem, that's also welcomed!*</p>

<h2>First model: Agentivity ~ Period + Genre</h2>

<pre><code>(from lrm)      
                     Model Likelihood      Discrimination    Rank Discrim.    
                       Ratio Test            Indexes           Indexes       
Obs          700    LR chi2      25.55    R2       0.048    C       0.602    
 strong      403    d.f.             4    g        0.440    Dxy     0.204    
 weak        297    Pr(&gt; chi2) &lt;0.0001    gr       1.553    gamma   0.240    
max |deriv| 3e-14                         gp       0.105    tau-a   0.100    
                                          Brier    0.236                     

(from glm)
Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.3171  -0.9825  -0.8094   1.1882   1.6090  

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 954.29  on 699  degrees of freedom
Residual deviance: 928.74  on 695  degrees of freedom
AIC: 938.74

Number of Fisher Scoring iterations: 4
</code></pre>

<h2>Second model: Type ~ Period</h2>

<pre><code>(from lrm)
                      Model Likelihood      Discrimination    Rank Discrim.    
                         Ratio Test           Indexes           Indexes       
Obs           872    LR chi2      36.70    R2       0.066    C       0.637    
 mediopassive 701    d.f.             2    g        0.552    Dxy     0.275    
 passive      171    Pr(&gt; chi2) &lt;0.0001    gr       1.736    gamma   0.401    
max |deriv| 9e-10                          gp       0.087    tau-a   0.087    
                                           Brier    0.151                     

(for glm)
Deviance Residuals: 
 Min       1Q   Median       3Q      Max  
-0.8645  -0.6109  -0.4960  -0.4960   2.0767  

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 863.19  on 871  degrees of freedom
Residual deviance: 826.49  on 869  degrees of freedom
AIC: 832.49

Number of Fisher Scoring iterations: 4
</code></pre>
"
"0.042144975196109","0.043314808182421","213910","<p>I'm curious as to how BoxTidwell works in R. The page for the package itself seems to lack descriptions. I have a logistic regression with many numerical and categorical predictors. Every time I use BoxTidwell(y ~ x1+x2...) I get</p>

<blockquote>
  <p>Error in boxTidwell.default(y, X1, X2, max.iter = max.iter, tol = tol,  : 
    the variables to be transformed must have only positive values</p>
</blockquote>

<p>This occurs even when I removed all the negative predictors. Does this mean that I should not take any categorical variables in the test? and because I do have negative predictors how would I incorporate them?</p>

<p>Also, should I specify something like 'family= binomial' in the command as I do in glm?</p>
"
"0.059601995508215","0.0612563891831689","214022","<p>I was trying out the Subselect R package to see how it worked and if it would be useful for a logistic regression problem I'm working on.  <a href=""https://cran.r-project.org/web/packages/subselect/vignettes/subselect.pdf"" rel=""nofollow"">Link</a> to the package.</p>

<p>I decided I would follow Example 4 on page 28 to see if I could perform the Anneal function on the glm I previously fit to my data.  I used the helper function, glmHmat, to extract the required matrices in the same way sa done in example 4.</p>

<pre><code>Fullmodel&lt;-glm(G,family=binomial,data)
Hmat &lt;- glmHmat(Fullmodel)
</code></pre>

<p>I then tried the anneal function and got the following error.</p>

<pre><code>test&lt;-anneal(Hmat$mat,1,10,H=Hmat$H,r=1,nsol=10,criterion = ""Wald"")
</code></pre>

<p>Yet, I got this error.</p>

<pre><code>Error in validmat(mat, p, tolval, tolsym, allowsingular = FALSE, algorithm) : 

 The covariance/total matrix supplied is not symmetric.
  Symmetric entries differ by up to 1.02445483207703e-07.
</code></pre>

<p>So, I thought I would test if this were true:</p>

<pre><code>isSymmetric(Hmat$mat,tol=1e-09)
[1] TRUE
isSymmetric(Hmat$H,tol=1e-09)
[1] TRUE
</code></pre>

<p>So I can't make heads or tails of this error message.  Any ideas?</p>
"
"0.059601995508215","0.0612563891831689","214175","<p>I'm new to xgboost package and here is the <a href=""https://github.com/dmlc/xgboost/blob/master/doc/parameter.md"" rel=""nofollow"">doc</a> on the parameters of this library for your reference.</p>

<p>My question is, logistic regression <strong>doesn't do</strong> binary splitting and build a tree unlike decision trees. If so, why max.depth and eta (learning rate) has been used in the <a href=""http://stats.stackexchange.com/questions/171043/how-to-tune-hyperparameters-of-xgboost-trees"">example</a> where the objective parameter is binary:logistic. (and the answer is accepted) </p>

<p>Isn't it wrong combination? or am I missing anything?</p>

<pre><code># xgboost fitting with arbitrary parameters
xgb_params_1 = list(
  objective = ""binary:logistic"",                                               # binary classification
  eta = 0.01,                                                                  # learning rate
  max.depth = 3,                                                               # max tree depth
  eval_metric = ""auc""                                                          # evaluation/loss metric
)
</code></pre>
"
"NaN","NaN","214512","<p>I'm trying to run a logistic regression with a L2-Penalty on a dataset I have. For the regression coefficient I also want to have the p-values of the signifance or at least the standard errors.
My plan was to do it with Python but unfortunately none of the package fulfilled my needs. </p>

<p>As I need a solution for this problem really quick, I thought to use R in this case. Unfortunately I don't have the time to read all about the package R offers, so I just liked to ask if there is any way to do the above mentioned in R?</p>
"
"0.042144975196109","0.043314808182421","214790","<p>My model is logistic regression. Is there a way to tune the parameter lambda of lasso or ridge based on cross-validated log-loss and brier(eg. proper scores?) in any R packages? </p>

<p>I'm using glmnet right now and the only measure available seems to be deviance, mean absolute error, misclassification error(is this based on 0.5 cut off?) and auc which are not proper scores and are therefore less desirable. </p>

<p>On a related note, is there a score like squared loss but penalize the deviation from one outcome more severely?</p>
"
"0.0842899503922179","0.086629616364842","217643","<p>I'm trying to find out if my numeric predictors have a linear relation to the logit of my logistic regression. I tried to use the lrm fit in the rms package where I have used 3 knot cubic spline on all numeric predictors like this:</p>

<pre><code>&gt; fit &lt;- lrm(y ~ rcs(x1,3)+rcs(x2,3)+.....)
</code></pre>

<p>There after I used anova on lrm fit. The main question is how do I use the result in anova(fit)? </p>

<p>My understanding is that the wald statistics are just the associated coefficients squared and dived by it's se. But what about the statistics for nonlinear terms here? are they the wald statistics for the coefficients for the squared predictors? </p>

<p>If none of the statistics are significant, can I conclude that there are no quadratic effect from my predictors?</p>
"
"0.11920399101643","0.122512778366338","219304","<p>I am examining social interaction data in individuals within two groups. Each social encounter has been coded to one of 4 categories, and these encounters are nested within individual, whom are nested within groups. The number of social encounters per individual is variable and my groups are unequal sample sizes. </p>

<p>I want to examine whether the proportion of social encounters in different categories significantly differ as a function of group. I previously examined a different DV in this data that was continuous, not categorical, and used a multilevel model in R (nlme package) to do so (data nested within individuals within groups). I have done some looking online and as far as I can tell, R should be able to run a multilevel model with categorical dependent variable as well. (i.e., <a href=""http://www.upa.pdx.edu/IOA/newsom/mlrclass/ho_binary.pdf"" rel=""nofollow"">http://www.upa.pdx.edu/IOA/newsom/mlrclass/ho_binary.pdf</a>). However, I am not sure how to implement this and I think that the naming online is inconsistent (some sources referring to this analysis as MLM with categorical variable, others calling it a multinomial logistic regression). </p>

<p>Is it possible to modify my current R script for continuous DV so that it analyzes for a categorical DV instead? Or do I need a different script? Thank you in advance for any help.</p>
"
"0.0729972383233908","0.0750234484920533","219453","<p>The Hosmer-Lemeshow test is a statistical test for goodness of fit for logistic regression models. According to <code>?hoslem.test</code>, it deals only with binary logistic regression. However, I wonder if this test can be used to a ordered logit model which has more than 2 levels for the dependent variable.</p>

<p>The ordered logit model is also known as the proportional odds model, or a cumulative logit model. And I use the ""Ordinal"" package. Thanks a lot.</p>
"
"NaN","NaN","220168","<p>I'm searching for an R package to raster time series smoothing. Currently, I'm using an approach like this one (using the equation suggested by Hamunyella et al., 2013)</p>

<pre><code>for (i in 2:(length(stacklist)-1)){
    r &lt;-  raster(stacklist[i])
    r1 &lt;- raster(stacklist[i-1])
    r3 &lt;- raster(stacklist[i+1])
    r2&lt;-mean(r1,r3)
    r[((r-r1)&lt;(-0.01*r1)) &amp; ((r-r3)&lt;(-0.01*r3))]&lt;-r2[((r-r1)&lt;(-0.01*r1)) &amp; ((r-r3)&lt;(-0.01*r3))]
    writeRaster(r,filename=paste(substr(stacklist[i], p1+1, (p1+7)),""_cropmLname.tif"",sep=""""),format=""GTiff"",overwrite=TRUE)
}
</code></pre>

<p>This is fast, but there ought to be an easier way. For example for implementing another kind of filter (e.g. Savitzky-Golay, Double-Logistic filtering). Does anybody know such?</p>
"
"0.0729972383233908","0.0750234484920533","220364","<p>So, im in a bit of trouble here. I am using R (i'm very new at this), and i'm trying to plot the probability effects of a interaction effect, using the effects package. </p>

<p>This is what the plot shows<a href=""http://i.stack.imgur.com/bBR1O.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bBR1O.jpg"" alt=""enter image description here""></a></p>

<p>However, when looking at the logistic regression model: it shows a b coefficient of B -1.333**, ExpB.27 indicating a negative moderation effect.</p>

<p>My quistion: how do i interpret this plot? and how does this relate to the findings? </p>

<p>Thank you guys in advance</p>

<p>Update:
the code i used is: </p>

<pre><code>data.mod &lt;-glm(outc_bin1~ctr_projsize+ctrfirmage+ctr_avgfirmsize+ctr_unirep+ctr_EPO+ctr_avginv+ctr_funding+ctr_projage+ctr_patent+techdiv+involvement+geolog+tech2+techdiv:involvement+tech2:involvement+geolog:involvement, family=binomial(link = ""logit""), data=data, x=TRUE)

plot(effect(""techdiv:involvement"", data.mod, xlevels=list(involvement=c(1, 2, 3, 4)))
</code></pre>

<p>Regression output:</p>

<p><a href=""http://i.stack.imgur.com/pjQOH.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pjQOH.jpg"" alt=""enter image description here""></a></p>
"
"0.121662063872318","0.125039080820089","220813","<p>I have troubles finding methodology to test a model in R. My question may be long and contain many details, but I will be glad if somebody can help me and give some hints how to tackle it.  </p>

<p>I have multilevel (nested) panel data, where one observation is a frequency of flights (or ratio of use of bio fuel from 0 to 1) of airline <code>i</code> on route <code>j</code> for time period <code>t</code> corresponding to one month.
 I suspect that I need to introduce airline fixed effect since frequency of flights may depend on which airline it is (Lufthansa or Singapore Airlines for example). 
Then I also need to do route fixed effect, since frequency of flights between Frankfurt-New York and Frankfurt-Abidjan will be also specific to each route. And I also need to do time fixed effect, since frequency of flight in the following month most likely to depend on the previous month, since airlines when they schedule flights in advance they choose particular aircraft and competitors behavior. </p>

<p>Below I provided the illustration of data set that I have</p>

<pre><code>A &lt;- data.frame(route=as.factor(rep('Paris-London', 138)), 
                 airline = as.factor(c(rep('Luft', 12), rep('DL', 8), rep('Sin', 72), rep('Turkish', 46))),
                 Date = c(seq(as.Date('2010-01-01'), as.Date('2011-01-01'), by = 'month'), 
                          seq(as.Date('2010-01-01'), as.Date('2010-07-01'), by = 'month'),
                          seq(as.Date('2010-01-01'), as.Date('2015-12-01'), by = 'month'),
                          seq(as.Date('2012-03-01'), as.Date('2015-12-01'), by = 'month')), 
                 ratio_use_bio_fuel= runif(138),
                distance_km=c(rep(600, 138)),
                revenue=sample(10000:25000, 138),
                frequency=sample(100:250, 138, replace=TRUE)
)


B&lt;- data.frame(route=as.factor(rep('Amsterdam-Brussels', 108)), 
               airline = as.factor(c(rep('EEE', 36), rep('FFF', 36), rep('GGG', 36))),
               Date = c(seq(as.Date('2013-01-01'), as.Date('2015-12-01'), by = 'month'), 
                        seq(as.Date('2013-01-01'), as.Date('2015-12-01'), by = 'month'),
                        seq(as.Date('2013-01-01'), as.Date('2015-12-01'), by = 'month')),
               ratio_use_bio_fuel= runif(108),
               distance_km=c(rep(1000, 108)),            
                revenue = sample(10000:25000, 108),
               frequency=sample(50:100, 108, replace=TRUE)



)


C &lt;- data.frame(route=as.factor(rep('Tokyo-Bangkok', 131)), 
                airline = as.factor(c(rep('Chin', 46), rep('Thai', 13), rep('Jap', 72))),
                Date = c(seq(as.Date('2012-03-01'), as.Date('2015-12-01'), by = 'month'),
                         seq(as.Date('2010-01-01'), as.Date('2011-01-01'), by = 'month'), 
                         seq(as.Date('2010-01-01'), as.Date('2015-12-01'), by = 'month')), 
                ratio_use_bio_fuel= runif(131),
                distance_km=c(rep(2500, 131)),
                revenue=sample(10000:25000, 131),
                frequency=sample(32:78, 131, replace=TRUE)

)

d&lt;-rbind(A,B,C)
</code></pre>

<p>Some of the airlines do not have observations for particular time period. In some cases they exited the route and do not provide flights any more or there was a merger. Therefore, I have unbalanced panel data. Does it cause any issues if I run the regression? </p>

<p>I know there are different packages in R such as plm and lme. </p>

<p>But do not know how to do logistic fixed effect regression where my dependent variable is ratio of usage of bio fuel (continuous of 0 to 1) conditional on airline and route. </p>
"
"0.11150512337989","0.114600210537152","221525","<p>I am using the <code>svyglm</code> function in the <code>survey</code> package in <code>R</code> to fit logistic regression models to a stratified, cluster survey. I want to calculate confidence intervals for my regression coefficients. The default method for <code>confint.svyglm</code> says that it creates Wald confidence intervals by adding and subtracting a multiple of the standard error. But the confidence interval this produces is not consistent with the p-value from the model - confidence intervals that do not overlap 0 still have p-values greater than .05.</p>

<p>I tried to replicate the p-value and confidence interval calculations by hand. It appears the p-value is calculated using a t-test, with the df of the t distribution taken from the residual degrees of freedom from the model. So far so good. But the confidence interval provided by <code>confint.svyglm</code> is just coefficient +/- 1.96*standard.error. This seems wrong - for a 95% confidence interval, I think the multiplier for the standard error should be the .975 quantile of a t-distribution with the appropriate degrees of freedom (in my case 10), which can be somewhat different from 1.96 (the .975 quantile of a z-distribution). True? Has anyone else had this problem? I am relatively new to working with survey data. Is there a reason to always use the z-quantile instead of the t-quantile for complex surveys specifically, or is this just a bug in the package?</p>
"
"0.0942390294485422","0.0968548555282575","222426","<p>I would like to analyze a Randomized Response variable as the final response variable in a Structural Equation Model (SEM), with <code>R</code>. However, I found no example about this. To the best of my knowledge <code>R</code> packages enable users to fit multivariate logistic regressions only. 
However, I have seen that Randomized Response can be analyzed with SEM in Mplus (Hox, J., &amp; Lensvelt-Mulders, G. (2004). <a href=""http://www.tandfonline.com/doi/abs/10.1207/s15328007sem1104_6?journalCode=hsem20"" rel=""nofollow"">Randomized response analysis in Mplus</a>. Structural equation modeling, 11(4), 615-620.). </p>

<p>Given that <code>R</code> has good packages for latent variable analysis, like <code>lavaan</code>, could you tell me how could I model a randomized response with SEM in <code>R</code>?</p>
"
"0.145994476646782","0.137542988902098","222479","<p>I'm new in this area, hope my question is understandable.
I need to fit conditional logistic regression model in R and use it for predictions on unseen data (output should be probability).
My datasets are  quite large (over 150k rows) and contains many (~500) noisy features.
I found package called <strong>clogitboost</strong> and tried to use it with relatively small number of boosting iterations (max 30, because with larger values it takes too long to compute and raises an error in the end - perhaps, it's resources limitations) - results are mediocre. I tried to use unconditional approach with regularization - <strong>glmnet</strong> and got better results, however, due nature of data I guess it will be better to use conditional regression with regularization similar to what is used in <strong>glmnet</strong> (tried to remove some features and apply <strong>clogitboost</strong> again and got slightly better results). There is package called <strong>clogitL1</strong> , which seems to do that, I tried to use it and it fits model quickly, but it doesn't provide <strong>predict()</strong> function, Usage described in  paper with attached R code here:
<a href=""https://www.jstatsoft.org/article/view/v058i12"" rel=""nofollow"">https://www.jstatsoft.org/article/view/v058i12</a>, they made some predictions in some way, but I can't understand it. Can I somehow manually predict using  unseen data, just like it's possible with <strong>clogitboost</strong> <strong>predict()</strong> (parameters are Model, X and Strata column) using model that was fitted with <strong>clogitL1</strong>? Note: in description of package <strong>clogitL1</strong> - ""Tools for the fitting and cross validation of <strong><em>exact</em></strong> conditional logistic regression models"" - so I'm not sure about what ""exact"" means here and  if it makes sense to use that package for my purposes. If it's not possible to predict, then, should I manually select features by checking their ""importance"" that can be found in <strong>clogitL1</strong> model? </p>
"
"0.10323368445272","0.106099178353461","222544","<p>I used the package for random forest. It is not clear to me how to use the results. 
In  logistic regression you can have an equation as an output, in standard tree some rules. If you receive a new dataset you can apply the equation on the new data and predict an outcome (like default/no default). Or saying the customers with characteristics a and characteristics b will have a default, so you can predict the outcome before it happens. That is the scoring tecnique.</p>

<p>Is it possible to use random forest in a similar situation, or how would you use the results of a RF? </p>

<p>my python code:</p>

<pre><code>from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score

#creating a test and train dataset

from sklearn.cross_validation import train_test_split

train, test = train_test_split(df, test_size = 0.3)

clf = RandomForestClassifier(max_depth = 30, min_samples_split=2, n_estimators = 200, random_state = 1)

#training the model
clf.fit(train[columns], train[""churn""])

#testing the model
predictions = clf.predict(test[columns])

print(predictions)

print(roc_auc_score(predictions, test[""churn""]))
</code></pre>
"
"0.13382827025955","0.137542988902098","223582","<p>I am trying to tie the odds ratio from a 2x2 cross classification table to the intercepts of a logistic regression on those 2 variables. I have a cross classification table that produces 2 odds ratios and the results of a logistic regression of PLACE3 ~ VIOL should produce intecepts should match the odds ratio of the contingency table. i.e. Odds ratio = exp(intercepts)  BUT the POLR package is not producing the correct intercepts.</p>

<p>Here is the data.  In the logistic regression PLACE3 is the outcome and VIOl is the independent variable.   You can see the PLACE3 vs. VIOL contingency table below and the logistic regression of PLACE3 ~ VIOL.  The odds ratios in the contingency table 1.79 and 3.1 are correct but the polr function seems off. Any thoughts on why  exp(summary(m)$zeta) does not produce 1.79 and 3.1?</p>

<p>For reference this is from Lemeshow's Applied Logisitic Regression book page 274.</p>

<pre><code>library(data.table)
aps &lt;- fread('http://www.umass.edu/statdata/statdata/data/aps.dat')
colnames(aps) = c(""ID"",""PLACE"",""PLACE3"",""AGE"",""RACE"",""GENDER"",""NEURO"",""EMOT"",""DANGER"",""ELOPE"",""LOS"",""BEHAV"",""CUSTD"",
                    ""VIOL"")
head(aps)
</code></pre>

<p>Here is  a cross classification table of PLACE3 vs. VIOl variables</p>

<pre><code>table(aps$PLACE3,aps$VIOL) 
      0   1
  0  80 179
  1  26 104
  2  15 104
</code></pre>

<p>using PLACE3 = 0 as the reference the 2 odds ratios from the contingency table are </p>

<pre><code>(104*80)/(179*26)  #1.79
(104*80)/(179*15)  #3.10
</code></pre>

<p>These odds ratios should be the same as exponentiating the slope coefficients  from 
a logistic model  PLACE3 ~ VIOL which is below</p>

<pre><code>aps$constant = rep(1,dim(aps)[1])
m &lt;- polr(as.factor(PLACE3) ~ constant + as.factor(VIOL), data = aps, Hess=TRUE,model=TRUE,method = c(""logistic""))
summary(m)

&gt; summary(m)
Call:
polr(formula = as.factor(PLACE3) ~ constant + as.factor(VIOL), 
    data = aps, Hess = TRUE, model = TRUE, method = c(""logistic""))

Coefficients:
                  Value Std. Error t value
as.factor(VIOL)1 0.8454     0.2112   4.003

Intercepts:
    Value  Std. Error t value
0|1 0.6869 0.1884     3.6464 
1|2 1.8608 0.2032     9.1557 

Residual Deviance: 1031.75 
AIC: 1037.75 
</code></pre>

<p>But you can see the exponentiation of the zeta vector is not 1.79 and 3.10</p>

<pre><code>exp(summary(m)$zeta)

&gt; exp(summary(m)$zeta)
     0|1      1|2 
1.987495 6.429049 
</code></pre>
"
"NaN","NaN","224265","<p>I'm fitting  a Bayesian logistic regression to model the effect of two covariates on a Randomized Response, with the 'rr' package.
I would like to compare two nested models by using the Bayes factor. Unfortunately, the 'rr'package does not estimate it by default. Is there any way to obtain it from the output of the 'rrreg.bayes' function?</p>

<p>For anybody who would like to help me, here is an example of application. If you were able to explain me how to compute the Bayes factor from the output of the example provided, I will be able to do the same on my data.</p>

<p><a href=""http://www.inside-r.org/node/333540"" rel=""nofollow"">http://www.inside-r.org/node/333540</a></p>

<p>Kind regards,</p>

<p>Jacopo Cerri </p>
"
"0.0942390294485422","0.0968548555282575","224539","<p>I have a dataset with about 30 potential predictors and 115 observations. I'm looking into building a prediction model with the data using logistic regression.</p>

<p>From what I have read - the typical rule of thumb to split the dataset is an 80/20 split, where 80 percent of the dataset will be used for training the model and the remaining 20 percent will be used for validation/testing. </p>

<p>Using the Confusion Matrix from the Caret package in R, the accuracy of the model is 91%, No information Rate of .55, and a significant p value comparing the Accuracy to NIR (P&lt;.0001).</p>

<p>I'm wary to trust these results, since only 92 observations were used for training and 23 observations for testing.</p>

<p>Is the sample size enough to create a generalizable prediction model? If not, how do I determine the required sample size for model training and testing?</p>
"
"0.11150512337989","0.114600210537152","227013","<p>I'm new to propensity score matching (PSM). So, my questions can be bit trivial.</p>

<p>1) Suppose I've 3 treatment levels and want to check the effectiveness of the treatment levels. Treatment levels are taking drug on time, not taking drug on time and not taking drug at regular interval. For this I need to do multinomial logistic regression. </p>

<p>But in PSM we do case-control study. So, how we are going to define which will be the case and which will be the control? Will it be the case that we will use one group as control for each time and other 2 groups as case?</p>

<p>2) Can anyone please tell me which package to use for multilevel group in R. I checked <a href=""http://stats.stackexchange.com/questions/81434/compare-regressions-among-more-than-2-groups"">this</a> link. But this link is old. I also checked <a href=""https://cran.r-project.org/web/packages/twang/"" rel=""nofollow"">this</a> package which seems to do multilevel. But is there any other option for packages?</p>
"
"NaN","NaN","228878","<p>gling with the interpretation of the coefficients of a zero-inflation model and I find no clear answer in the net. Maybe someone can help me and other people in the same situation.</p>

<p>After fitting cancer incidences through a Poisson regression with zero-inflation (zeroinfl package in R), in the logistic component, the coefficient estimate for the age variable is -3.6.</p>

<p>Does that mean that for each additional year of age, the odds of having zero cancer incidences increases by 3.6, or vice versa?</p>

<p>Many thanks, Gion</p>
"
"0.168579900784436","0.173259232729684","229477","<p><br>I am struggling to interpret the results of a binomial logistic regression I did.<br> The experiment has 4 conditions, in each condition all participants receive different version of treatment. <br>DVs (1 per condition)=DE01,DE02,DE03,DE04, <br>all binary (1 - participants take a spec. decision, 0 - don't)
<br>Predictors: FTFinal (continuous, a freedom threat scale)
<br>SRFinal (continuous, situational reactance scale)
<br>TRFinal (continuous, trait reactance scale)
<br>SVO_Type(binary, egoists=1, altruists=0)
<br>After running these binomial (logit) models,<br><br> <code>model_soc_inf&lt;- glm(mydata$DE01~FTFinal+SRFinal+TRFinal+SVO_Type,
                    family=binomial(link='logit'),data=mydata)
model_soc_inf1&lt;- glm(mydata$DE01~FTFinal+SRFinal+TRFinal,
                     family=binomial(link='logit'),data=mydata)
summary(model_soc_inf)
model_pers_inf &lt;- glm(mydata$DE02~FTFinal+SRFinal+TRFinal+SVO_Type,
                      family=binomial(link='logit'),data=mydata)
model_pers_inf1 &lt;- glm(mydata$DE02~FTFinal+SRFinal+TRFinal,
                       family=binomial(link='logit'),data=mydata)
model_pers_inf2 &lt;- glm(mydata$DE02~SRFinal+TRFinal+SVO_Type,
                       family=binomial(link='logit'),data=mydata)
summary(model_pers_inf)
model_soc_uninf&lt;-glm(mydata$DE03~FTFinal+SRFinal+TRFinal+SVO_Type,
                     family=binomial(link='logit'),data=mydata)
model_soc_uninf1&lt;-glm(mydata$DE03~FTFinal+SRFinal+TRFinal,
                      family=binomial(link='logit'),data=mydata)
summary(model_soc_uninf)
model_pers_uninf&lt;-glm(mydata$DE04~FTFinal+SRFinal+TRFinal+SVO_Type,
                      family=binomial(link='logit'),data=mydata)
model_pers_uninf1&lt;-glm(mydata$DE04~FTFinal+SRFinal+TRFinal,
                       family=binomial(link='logit'),data=mydata)
summary(model_pers_uninf)</code><br><br>I ended up with the following<a href=""http://i.stack.imgur.com/4JKLa.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4JKLa.png"" alt=""enter image description here""></a>. Initially I tested 2 models per condition, when condition 2 (DE02 as a DV) got my attention. In model(3)There are two variables, which are significant predictors of DE02 (taking a decision or not) - FTFinal and SVO Type. In context, the values for model (3) would mean that all else equal, being an Egoist (SVO_Type 1) decreases the (log)likelihood of taking a decision in comparison to being an altruist. Also, higher scores on FTFinal(freedom threat) increase the likelihood of taking the decision. So far so good. Removing SVO_Type from the regression (model 4) made the FTFinal coefficient non-significant. Removing FTFinal from the model does not change the significance of SVO_Type.</p>

<p>So I figured:ok, mediaiton, perhaps, or moderation. I tried first to look for mediation in both in R and SPSS. The moderation attempt was in vain: entering an interaction term SVO_Type:FTFinal makes all variables in model(3) non-significant.Here's the code for that:<code>model1&lt;-glm(DE02~FTFinal,family=binomial(link='logit'),data=mydata)
summary(model1)
model2&lt;-glm(DE02~SVO_Type,family=binomial(link='logit'),data=mydata)
summary(model2)
model3&lt;-glm(DE02~FTFinal+SVO_Type,family=binomial(link='logit'),data=mydata)
summary(model3)
interaction&lt;-glm(DE02~SVO_Type+FTFinal+SVO_Type:FTFinal, family =binomial(
  link = ""logit""),data = mydata)</code> <br>As for mediation, I followed  <a href=""http://www.nrhpsych.com/mediation/logmed.html"" rel=""nofollow"">this</a> mediation procedure for logistic regression, but found no mediation. </p>

<p>To sum up:
There is some relationship between SVO_Type and FTFinal, but I have no clue what.
Predicting DE02 from SVO_Type only is not significant.
Predicting DE02 from FTFinal is not significant
Putitng those two in the regression makes them both significant predictors.
Including an interaction between these both in any model, predicting DE02 model makes all variables in the model insignificant.<br>
So I am at a total loss: As far as I know, to test moderation, you need an interaction term. This term is between a categorical var (SVO_Type) and the continuous one(FTFinal), perhaps that goes wrong? And to test mediation outside SPSS, I tried the ""mediate"" package in R, only to discover that there is a ""treatment"" argument in the main funciton, which is to be the treatment variable (exp Vs cntrl). I don't have such, all ppns are subjected to different versions of the same treatment. 
I apologize for <a href=""http://www.filedropper.com/mydata"" rel=""nofollow"">this external way of uploading the dataset</a>, it is way too complicated to reproduce here (I am a noob).
Any help would be greatly appreciated. I have no clue what the relationship between SVO_Final and FTFinal is.</p>
"
"0.133274113551006","0.136973450269748","229624","<p>I have been working to fit a normal distribution to data that is truncated to only be zero or greater. Given my data, which I have at the bottom, I previously used the following code: </p>

<pre><code>library(fitdistrplus)
library(truncnorm)
fitdist(testData, ""truncnorm"", start = list(a = 0, mean = 0.8, sd = 0.9))
</code></pre>

<p>Which, of course, won't work for a number of reasons, not least of which being that the mle estimator provides increasingly negative estimates as <code>a</code> tends towards zero. I previously got some very helpful information about fitting a normal distribution to this data <a href=""http://stackoverflow.com/questions/38838343/fitting-truncnorm-using-fitdistrplus/38839835#38839835"">here</a>, where two basic options were presented:</p>

<p>I might either use a low, negative value for <code>a</code>, and try out a number of different values</p>

<pre><code>fitdist(testData, ""truncnorm"", fix.arg=list(a=-.15),
        start = list(mean = mean(testData), sd = sd(testData)))
</code></pre>

<p>or I might set lower bounds for the parameters</p>

<pre><code>fitdist(testData, ""truncnorm"", fix.arg=list(a=0),
        start = list(mean = mean(testData), sd = sd(testData)),
        optim.method=""L-BFGS-B"", lower=c(0, 0))
</code></pre>

<p>Either way, though, it seems that some information is being lost - in the first case, because <code>a</code> isn't being truncated at zero, and in the second because the parameters have arbitrary lower bounds - I'm only concerned with achieving a good fit of the data, not with having positive a positive mean for the distribution. Given that mle estimators tend negative as <code>a</code> goes to zero, would it be better to use non-mle estimation? Does it make sense to have negative values of a if the data itself can't be negative?</p>

<p>This question applies more generally as well, as I have been using the <code>truncdist</code> package to try to fit Weibull, Log Normal, and Logistic distributions as well (for the Weibull, of course, there is no need to truncate at zero).</p>

<p>Finally, here's the data:</p>

<pre><code>testData &lt;- c(3.2725167726, 0.1501345235, 1.5784128343, 1.218953218, 1.1895520932, 
              2.659871271, 2.8200152609, 0.0497193249, 0.0430677458, 1.6035277181, 
              0.2003910167, 0.4982836845, 0.9867184303, 3.4082793339, 1.6083770189, 
              2.9140912221, 0.6486576911, 0.335227878, 0.5088426851, 2.0395797721, 
              1.5216239237, 2.6116576364, 0.1081283479, 0.4791143698, 0.6388625172, 
              0.261194346, 0.2300098384, 0.6421213993, 0.2671907741, 0.1388568942, 
              0.479645736, 0.0726750815, 0.2058983462, 1.0936704833, 0.2874115077, 
              0.1151566887, 0.0129750118, 0.152288794, 0.1508512023, 0.176000366, 
              0.2499423442, 0.8463027325, 0.0456045486, 0.7689214668, 0.9332181529, 
              0.0290242892, 0.0441181842, 0.0759601229, 0.0767983979, 0.1348839304
)
</code></pre>
"
"0.139779069524328","0.143658966607306","229884","<p>I have a cancer classification problem (type A vs type B) on radiological images from which i have generated 756 texture-based predictive features (wavelet transform followed by texture analysis, i.e., features described by Haralick, Amasadun etc) and 8 semantic features based on subjective assessment by expert radiologist. This is entirely for research and publication to show that these predictive features may be useful in this particular problem. I do not intend to deploy the model for practitioners. </p>

<p>I have 107 cases. 60% cases are type A and 40% type B (in keeping with their natural proportions in population). I have done several iterations of model development with varying results. One particular method is giving me an 80% 80% classification accuracy but I am suspicious that my method is not going to stand critical analysis. I am going to outline my method and a few alternatives. I will be grateful if someone can pick if it is flawed. I have used R for this:</p>

<p>Step 1: Split into 71 training and 36 test cases.<br>
Step 2: remove correlated features from training dataset (766 -> 240) using findcorrelation function in R (caret package)<br>
Step 3: rank training data features using Gini index (Corelearn package)<br>
Step 4: Train multivariate logistic regression models on top 10 ranked features using subsets of sizes 3 , 4, 5 ,and 6 in all possible combination (<sup>10</sup>C<sub>3</sub>=252, <sup>10</sup>C<sub>4</sub>=504, <sup>10</sup>C<sub>5</sub>=630). So <strong>total 1386 multivariate logistic regression models were trained</strong> using 10-fold cross-validation and tested on test dataset.<br>
Step 5: Of these I selected a model which gave the best combination of training and test dataset accuracy, i.e., 3 feature model with 80% 80% accuracy.<p></p>

<p>Somehow running 1300 permutations seems quite dodgy to me and seems to have introduced some false discovery. Just want to confirm if this is a valid ML technique or whether I should skip step 4 and only train on top 5 ranked features without running and permutations.</p>

<p>Thanks. <p> PS I experiemented a bit with naive bayes and random forests but get rubbish test set accuracy so dropped them</p>

<p>====================</p>

<h1>UPDATE</h1>

<p>Following discussion with SO members, i have changed the model drastically and thus moved more recent questions regarding model optimisation into a new post <a href=""http://stats.stackexchange.com/questions/232829/lasso-regularised-classification-highly-variable-choice-of-lambda-min-on-repeate"">LASSO regularised classification highly variable choice of lambda.min on repeated cv</a></p>
"
"0.0298009977541075","0.0612563891831689","230760","<p>I need to do a multinomial logistic regression with a nominal variable, and I've heard about the Gmulti package in r ,and how it provides automatic selection methods models, However all the examples that i found are only applied on binaire logistic regression, so I wonder if it's even working on a multinomial logistic regression and in case is true, is the Gmulti take in consideration the problem of multicolinearity between the independents variables. Please help me thank you </p>
"
"0.042144975196109","0.043314808182421","230776","<p>I'am trying to do a multinomial logistic regression to explain the political orientations in Tunisia but I'am really confused about using the vglm() function from package VGAM or the multinom() function from package nnet or the mlogit function... can someone tell me please the difference and wich one is the best ?
Thank you very much</p>
"
"0.188692116954597","0.18469496266596","231066","<p>*EDIT: I ran test again with data set provided and realized that the cause of problem is definitely rank deficiency, because estimated values of parameters in nonlinear regression showed non existing p values and there was no way to create confidence intervals with this data. </p>

<h2>Thank you all for reading and help! This question is closed.</h2>

<p>I researched seed germination. I took 75 seed replicates and put them in  different ecological parameters (like temperature) and took data about sprouts in different time intervals. </p>

<p>Reading statistical science papers about this topic, I found that I should analyze my data in a time-to-event model (dose response curve), where I can use log-logistic regression or nonlinear regression (Ritz et al., 2013 -<a href=""http://dx.doi.org/10.1016/j.eja.2012.10.003"" rel=""nofollow"">http://dx.doi.org/10.1016/j.eja.2012.10.003</a>). </p>

<p>Two models (nonlinear and log-logistic) lead to quantitatively very similar fitted germination curves, i.e., similar parameter estimates, but qualitatively different statements about the precision of estimates. Nonlinear regression model yields an overly precise estimate of the proportion of seeds that germinated during the experiment, so the precision reported by the nonlinear regression is too high.</p>

<p>Similarly, the 95% confidence intervals of the fitted curves also demonstrate the dramatic difference in precision of the two models: Accurate prediction of germination percentages is not warranted by the data unless very low percentages are of interest.</p>

<p>Because of that I choose log-logistic regression as a model. First few data sets; treatments analyzed in R using analysis of Dose-Response Curves (drc package) went smooth, and I was able to plot and get final graph. Such data, which was successfully analyzed, contained treatments where max seeds germination was for example 50% of total seed number.</p>

<p>Example:</p>

<p><a href=""http://i.stack.imgur.com/yUqOu.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/yUqOu.jpg"" alt=""Example of successful data analysis""></a></p>

<p>The problems arose when I entered the log-logistic model with treatment where all the seeds germinated in a short amount of time (meaning the treatment for this set of seeds is most adequate for their successful sprouting). For example, 100% of seeds germinated in only 5 days, so there are only two or three time intervals and a large number of sprouted seeds. The R program here reported  convergence error:</p>

<pre><code>Error in optim(startVec, opfct, hessian = TRUE, method = optMethod, control = list(maxit = maxIt,  : 
non-finite value supplied by optim
Error in drmOpt(opfct, opdfct1, startVecSc, optMethod, constrained, warnVal,  : 
Convergence failed 
</code></pre>

<p>Since I'm still a student in biology I have a very basic knowledge in statistics, so I tried to solve the problem with literature. </p>

<p>At first I thought that convergence failed because of perfect or complete separation, but through longer research it seems that the problem lies in rank deficiency. </p>

<p>When I analyzed the same data with nonlinear regression I've managed to fit curve and plot a graph without a problem.  </p>

<p>So, is there a way to make log-logistic model work even though I have obviously small data in cases of 100% germination? Should I switch to nonlinear regression  even though the reported precision would be too high. </p>
"
"0.11150512337989","0.114600210537152","232450","<p>We are developping a software that run hierarchical linear model in R with the lme4 package. The model we are trying to fit is of the following shape:</p>

<pre><code>&gt; data
ID | Dummy_1 | Dummy_2 | Dummy_3 | Rating 
1  | 0       | 1       | 1       | 14
1  | 1       | 0       | 1       | 15
1  | 0       | 1       | 0       | 11
2  | 1       | 0       | 1       | 15
2  | 1       | 0       | 0       | 12

x = lmer(formula = Rating ~ Dummy_1 + Dummy_2 + Dummy_3 + (1 + Dummy_1 + Dummy_2 + Dummy_3 | ID), data = data)
</code></pre>

<p>It is important to note that this is the shape that the data will take, however, Rating can have very different range depending on the data te user provide.</p>

<p>The example above illustrate a limit case we are trying to deal with which occurs when the dummy variables perfectly predict the independant variable.</p>

<p>Here we can see that for example with <code>intercept = 10</code>, <code>B1 = 2</code>, <code>B2 = 1</code> and <code>B3 = 3</code> we perfectly predict the Rating variable. It implies first that the <code>ID</code> is useless and that we are in a case of <code>complete separation</code>.</p>

<p><strong>Question:</strong> How do you deal with (quasi-)complete separation when the independant variable is not binomial but continuous as it is the case here ? I could only find explanations for logistic regression. Please, ignore the fact that I use a linear regression for discrete-ordinal data and that I treat them as continuous :)</p>

<p>So that you know the warnings I get from R are the following:</p>

<pre><code>1: In optwrap(optimizer, devfun, getStart(start, rho$lower,  ... :
  convergence code 1 from bobyqa: bobyqa -- maximum number of function evaluations exceeded
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  ... :
  unable to evaluate scaled gradient
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  ... :
  Model failed to converge: degenerate  Hessian with 4 negative eigenvalues
</code></pre>
"
"0.184544530977799","0.180635236963042","232829","<p>I am using CT scans to classify lung cancer into one of two types (Adenocarcinoma vs. Squamous carcinoma; we can abbreviate them A &amp; B). I am applying LASSO penalized logistic regression to a data set containing 756 CT-derived texture features and 12 radiologist identified categorical (yes/no) features. The latter are based on literature for relevance whereas the former are computer generated with no prior proof of being useful. I have 107 cases so my final dataframe (df) is 107 x 768 dimensional:</p>

<p>i)Texture features (mathematical quantities n=756) are continuous variables scaled and centered. Their names are stored in list <code>â€˜texVarsâ€™</code></p>

<p>ii)Semantic features(Qualitative features subjective assessed by experienced radiologist, n=12). These are usually categorical binary inputs of yes / no type. Their names are stored in list <code>â€˜semVarsâ€™</code>.</p>

<p>Following comments from community on my original (very different) model <a href=""http://stats.stackexchange.com/questions/229884/is-my-high-dimensional-data-logistic-regression-workflow-correct"">Is my high dimensional data logistic regression workflow correct?</a>, I performed my LR development in three steps:</p>

<p>1)Feature selection: I used principle components analysis to reduce texture feature-space from 756 to 30. I kept 4 most relevant (from literature) semantic features. This gave me 34 final features. I used the following command:</p>

<pre><code>trans = preProcess(df[,texVars], method=c(""BoxCox"", ""center"",   ""scale"", ""pca""),thresh=.95)  # only column-names matching â€˜texVarsâ€™ are included.
neodf2 &lt;- predict(trans,df[,texVars]).
neodf.sem &lt;- neodf2[,c(""Tumour"",""AirBronchogram"", ""Cavity"", ""GroundglassComponent"",""Shape"")]  # this DF is 107 x 4 dimensional, containing only 4 semantic features (most relevant from prior knowledge).
neodf.tex &lt;- neodf2[,c(""Tumour"",setdiff(names(neodf2),names(neodf.sem)))] # this only has the 30 PCA vectors (labelled PC1 â€“ PC30).
</code></pre>

<p>2) Model development (LASSO) and penalty term tuning (10fold cross-validation) using cv.glmnet command  Deviance was used as determinant of model quality. Using this method, I developed a model incorporating only semantic features, a second model incorporating only texture features, and a third model incorporating both semantic and texture features. Here are the commands:</p>

<pre><code>#Converting to model.matrix for glmnet 
xall &lt;- model.matrix(Tumour~.,neodf2)[,-1]
xtex &lt;- model.matrix(Tumour~.,neodf.tex)[,-1]
xsem &lt;- model.matrix(Tumour~.,neodf.sem)[,-1]
y &lt;- neodf$Tumour
require(glmnet)
grid &lt;- 10^seq(10,-2,length=100)

lasso.all &lt;- cv.glmnet(xall,y,alpha=1,lambda=grid, family=""binomial"", type.measure=""deviance"") 
lasso.tex &lt;- cv.glmnet(xtex,y,alpha=1,lambda=grid, family=""binomial"", type.measure=""deviance"")
lasso.sem &lt;- cv.glmnet(xsem,y,alpha=1,lambda=grid, family=""binomial"", type.measure=""deviance"")
</code></pre>

<p>3) Testing model classification accuracy on entire dataset. The following is the backbone of  bootstrap to generate 95% confidence intervals of predictive accuracy:</p>

<pre><code>pred &lt;- predict(lasso.all, newx = xall, s = ""lambda.min"", ""class"")
tabl &lt;- table(pred,y)
sum(diag(prop.table(tabl)))
</code></pre>

<p>4) As an alternative means to assess model performance than classification accuracy, I used ROC area under curve on entire dataset and compared AUROC curves from different models using DeLong's method (pROC package)</p>

<p>The results are interesting</p>

<pre><code> =================================================
</code></pre>

<p>LR MODEL BASED ON SEMANTIC FEATURES ALONE:
    lasso.sem$lambda.min
     0.01</p>

<p>Plot cv lambda vs. binomial devance <a href=""http://i.stack.imgur.com/6Modw.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6Modw.png"" alt=""cv lambda vs binomia deviance""></a></p>

<pre><code>             Feature          Odds Ratio
1                (Intercept)  0.1292604
2      AirBronchogramPresent  0.1145378
3              CavityPresent 35.4350358
4 GroundglassComponentAbsent  4.3657928
5                 ShapeOvoid  2.4752881

AUC: .84


=================================================    
</code></pre>

<p>LR MODEL BASED ON TEXTURE FEATURES ALONE:</p>

<pre><code>lasso.tex$lambda.min
1e+10   
</code></pre>

<p>Plot  cv lambda vs binomial deviance (texture alone). Note how the 95% CI's are all overlapping! <a href=""http://i.stack.imgur.com/1Mk7M.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1Mk7M.png"" alt="" cv lambda vs binomial deviance ""></a></p>

<pre><code>   Feature OddsRatio
1 (Intercept) 0.6461538



============================================================
</code></pre>

<p>LR MODEL BASED ON TEXTURE + SEMANTIC FEATURES:</p>

<pre><code>lasso.all$lambda.min
0.05 
</code></pre>

<p>Plot  cv lambda vs binomial deviance <a href=""http://i.stack.imgur.com/p1AHX.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/p1AHX.png"" alt=""cv lambda vs binomial deviance""></a></p>

<pre><code>                          Feature   OddsRatio
1                (Intercept)        0.3136489
2                       PC23        0.9404430
3                       PC27        0.8564001
4      AirBronchogramPresent        0.2691959
5              CavityPresent        6.7422427
6 GroundglassComponentAbsent        2.0514275
7                 ShapeOvoid        1.5974378

 AUC : .88
</code></pre>

<p>Plot showing loglambda vs coefficients. The dashed vertical line shows the cross-validated optimum lambda:<a href=""http://i.stack.imgur.com/d6FaO.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/d6FaO.jpg"" alt=""enter image description here""></a></p>

<p>Having rejected the texture only model, which only contains intercept, i was left with two models - semantic and combined texture+semantic. I created ROC curves for both and compared them using DeLong's method:</p>

<pre><code>pred.sem&lt;- predict(lasso.sem, newx = xsem, s = ""lambda.min"")
pred.all&lt;- predict(lasso.all, newx = xall, s = ""lambda.min"")

roc.sem&lt;- roc(y,as.numeric(pred.sem), ci=TRUE, boot.n=1000, ci.alpha=0.95,stratified=FALSE)    

roc.all&lt;- roc(y,as.numeric(pred.all), ci=TRUE, boot.n=1000, ci.alpha=0.95,stratified=FALSE)
</code></pre>

<p>Outputs of ROC analysis are:</p>

<pre><code>data:  roc.sem and roc.all
Z = -2.1212, p-value = 0.0339
alternative hypothesis: true difference in AUC is not equal to 0
sample estimates:
AUC of roc1 AUC of roc2 
  0.8369963   0.8809524 
</code></pre>

<p>Showing that combined model ROC curve is significantly better despite the modest improvement in AUC (83% vs 88%).
Questions are:</p>

<p>a) is my methodology airtight from a publication point of view now? Apologies in advance for any gross errors in my presentation of this problem.</p>

<p>b) what is the formal inference that texture model is intercept only.</p>

<p>c) if texture model is useless, how do its variables become useful once added to semantic features and yield a higher overall accuracy in the combined result? perhaps that means the effect of texture features alone is too small to be detected in this small dataset but becomes apparent when combined with a stronger predictor (i.e., semantic features). </p>

<p>Any further comments are welcome.</p>
"
"0.0942390294485422","0.0968548555282575","233178","<p>I have a database with 1200 observations and 14 variables and I'am trying to do a classification tree for my dependent nominal variable who hase 4 modality</p>

<pre><code>    &gt; table(testarbre2$Q99)

  Autres       Nahdha Ne pas voter Nidaa Tounes 
     248          351          303          298 
</code></pre>

<p>at firt i tried to do a multinom logistic regression but i got the mojority of my predictor variables non significant. it seems that Even with 1200 people I was trying to fit a model for which I don't have sufficient data. 
so i tried to do a classification tree using the package rpart from R 
but the problem is that the error is so high about 65% and more, and the missclassification is about 70% 
this is the code R that i used </p>

<pre><code>   #preparation of the data
   set.seed(26)
   train=sample(1:nrow(testarbre2),nrow(testarbre2)*7/10)
   test=-train
   training_data=testarbre2[train,]
   testing_data=testarbre2[test,]
   testing_vote=vote[test]

   #fitting the model
   library(rpart)
   library(rpart.plot)
   Tree &lt;- rpart(Q99~.,data=training_data)
   rpart.plot(Tree)
   printcp(Tree)
   plotcp(Tree)

    #Construction of the complete tree
  Tree &lt;-rpart(Q99~.,data=training_data,control=rpart.control(minsplit=50,cp=0))

     #Prune the tree
    treeOptimal &lt;- prune(Tree,cp=Tree$cptable[which.min(Tree$cptable[,4]),1])
    rpart.plot(treeOptimal)

   #Prediction
   a=predict(ptitanicOptimal,testing_data2,type = ""class"")
   mc=table(a,testing_vote2)
</code></pre>

<p>I don't know if i missed a step or i used a wrong approach in the construction of my classification tree or the database is causing the problem</p>

<p>Please someone help me to understand what's wrong with my model</p>
"
"0.11150512337989","0.0982287518889871","234537","<p>I've split my data set into a training and test set. I've performed a principal component analysis on the training set and have used the first 3 principal components to generate a logistic regression model for my response.</p>

<p>I now want to use this model to make predictions for my test data set and check if this is true. </p>

<p>I've been trying to use the predict function but obviously the model uses the principal components of the training set as the predictors whereas my test set just has all the original predictors so obviously they're not compatible.</p>

<p><strong>How do I go about 'projecting' my test data onto the principal components I've already generated so I can use my model to make predictions?</strong></p>

<p>Ideally I'd like to do this without using any external packages (it's for university). I am working in R.</p>
"
"0.042144975196109","0.043314808182421","234672","<p>It's well known that there are <a href=""http://rstatistics.net/variable-importance-of-predictors-that-contribute-most-significantly-to-a-response-variable-in-r/"" rel=""nofollow"">a lot of feature selection libraries out there</a> for R. What are the most commonly used (and statistically sound) packages to use when selecting features for use in a logistic regression?</p>
"
"0.0942390294485422","0.077483884422606","234763","<p>I have a fairly large dataset ($\approx 3 \bar{M}$ observations for a dozen candidate predictors) and I would like to perform a logistic regression on that dataset.
I have a problem of separation in that dataset so usual model can't converge. That's why I am using Firth penalization (logistf package for R) to have my model to adjust.</p>

<p>I would like to select the best subset of variables for my final model but I can't find the proper way to do that. I know that stepwise selection is out of question and I usually would use L1 or L2 penalized regression so that some coefficients are reduced to 0.</p>

<p>My problem is : the function I am using to adjust my model doesn't handle extra penalization so no Elasticnet-Firth regression.</p>

<p>Is there, apart from stepwise selection, another way to select my variables ?</p>

<p>Thanks in advance !</p>
"
