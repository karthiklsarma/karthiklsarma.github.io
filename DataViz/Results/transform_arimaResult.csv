"V1","V2","V3","V4"
"0.210042012604201","0.201346816564207","  6330","<p>I have previously used <a href=""http://www.forecastpro.com/"">forecast pro</a> to forecast univariate time series, but am switching my workflow over to R.  The forecast package for R contains a lot of useful functions, but one thing it doesn't do is any kind of data transformation before running auto.arima().  In some cases forecast pro decides to log transform data before doing forecasts, but I haven't yet figured out why.</p>

<p>So my question is: when should I log-transform my time series before trying ARIMA methods on it?</p>

<p>/edit: after reading your answers, I'm going to use something like this, where x is my time series:</p>

<pre><code>library(lmtest)
if ((gqtest(x~1)$p.value &lt; 0.10) {
    x&lt;-log(x)
}
</code></pre>

<p>Does this make sense?</p>
"
"0.216930457818656","0.259937622455018"," 28737","<p>I have time series as </p>

<pre><code>0.4385487 0.7024281 0.9381081 0.8235792 0.7779642 1.1670665 1.1958634 1.1958634 0.8235792 0.8530141 0.8802216 1.1958634 1.1235897 1.3542734 1.3245534 0.9381081 1.1670665 1.1958634 0.8802216 1.3542734 1.1670665 4.9167998 0.9651803 0.8221709 1.1070461 1.2006974 1.3542734 0.9651803 0.9381081 0.9651803 0.8854192 1.3245534 1.1235897 1.2006974 1.1958634 0.4385487 1.3245534 4.9167998 1.2277843 0.8530141 1.0018480 0.3588158 0.8530141 0.8867365 1.3542734 1.1958634 1.1958634 0.9651803 0.8802216 0.8235792 4.9167998 1.1958634 0.9651803 0.8854192 0.8854192 1.2006974 0.8867365 0.9381081 0.8235792 0.9651803 0.4385487 0.9936722 0.8821301 1.3542734 1.1235897 1.6132899 1.3245534 1.3542734 0.8132233 0.8530141 1.1958634 1.2279813 0.8354292 1.3578511 1.1070461 0.8530141 0.9670581 1.1958634 0.7779642 1.2006974 1.1958634 0.8235792 1.3245534 0.5119648 2.3386331 0.8890464 0.8867365 4.9167998 1.2006974 1.2006974 0.6715839 4.9167998 0.7747481 4.9167998 0.8867365 1.2277843 0.8890464 1.2277843 0.8890464 1.0541099 0.8821301 
</code></pre>

<p>I am using package ""itsmr""-autofit(),""forecast""-auto.arima(),""package""--functions</p>

<ol>
<li><p>Autoregressive model</p>

<pre><code>&gt; ar(t)

Call:
    ar(x = t)

    Order selected 0  sigma^2 estimated as  0.9222 
</code></pre></li>
<li><p>ARMA model</p>

<pre><code>&gt; autofit(t)
    $phi
    [1] 0

    $theta
    [1] 0

    $sigma2
    [1] 0.9130698

    $aicc
    [1] 279.4807

    $se.phi
    [1] 0

    $se.theta
    [1] 0
</code></pre></li>
<li><p>ARIMA model</p>

<pre><code>    &gt; auto.arima(t)
    Series: t 
    ARIMA(0,0,0) with non-zero mean 

    Coefficients:
          intercept
             1.2623
    s.e.     0.0951

    sigma^2 estimated as 0.9131:  log likelihood=-138.72
    AIC=281.44   AICc=281.56   BIC=286.67
</code></pre>

<p>The auto.arima function automatically differences time series: we don't have to worry about transformation.</p>

<pre><code>&gt; auto.arima(AirPassengers)
Series: AirPassengers 
ARIMA(0,1,1)(0,1,0)[12]                    

Coefficients:
          ma1
      -0.3184
s.e.   0.0877

sigma^2 estimated as 137.3:  log likelihood=-508.32
AIC=1020.64   AICc=1020.73   BIC=1026.39`
</code></pre></li>
</ol>

<p>Which model should I select to get p,q values &amp; for forecasting purpose?</p>
"
"0.171498585142509","0.164398987305357"," 32657","<p>I was playing with the <a href=""http://cran.r-project.org/web/packages/TSA/index.html"" rel=""nofollow"">TSA</a> package in R and wanted to test the <code>arimax</code> function to the solution provided in Pankratz's <em>Forecasting with Dynamic Regression Models</em>, chapter 8. The savings rate and the function seems to provide similar results as the ones in the book except for the IO weights which are quite different. I bet there is a transformation that I might be missing.</p>

<p>Any help on understanding why IO coefficients are so different would be appreciated...</p>

<p>the solution states </p>

<pre><code>AO @ t=82,43,89
LS @ t=99
IO @ t=62,55
</code></pre>

<p>with Parameters estimates</p>

<pre><code>C = 6.1635
w82 = 2.3346
w99 = -1.5114
w43 = 1.1378
w62 = 1.4574
w55 = -1.4915
w89 = -1.0702
AR1 = 0.7976
MA2 = -0.3762
</code></pre>

<p>To fit the model in R, I used
(<code>saving</code> is the data)</p>

<pre><code>arimax(saving, order = c(1,0,2), fixed=c(NA,0,NA,NA,NA,NA,NA,NA,NA,NA), io=c(55,62), 
       xreg=data.frame(AO82=1*(seq(saving)==82),
                       AO43=1*(seq(saving)==43),
                       AO89=1*(seq(saving)==89),
                       LS99=1*(seq(saving)&gt;=99)),
       method='ML')
</code></pre>

<p>The savings rate data is (100 points)</p>

<p>4.9
5.2
5.7
5.7
6.2
6.7
6.9
7.1
6.6
7
6.9
6.4
6.6
6.4
7
7.3
6
6.3
4.8
5.3
5.4
4.7
4.9
4.4
5.1
5.3
6
5.9
5.9
5.6
5.3
4.5
4.7
4.6
4.3
5
5.2
6.2
5.8
6.7
5.7
6.1
7.2
6.5
6.1
6.3
6.4
7
7.6
7.2
7.5
7.8
7.2
7.5
5.6
5.7
4.9
5.1
6.2
6
6.1
7.5
7.8
8
8
8.1
7.6
7.1
6.6
5.6
5.9
6.6
6.8
7.8
7.9
8.7
7.7
7.3
6.7
7.5
6.4
9.7
7.5
7.1
6.4
6
5.7
5
4.2
5.1
5.4
5.1
5.3
5
4.8
4.7
5
5.4
4.3
3.5</p>

<p>here it is my output</p>

<pre><code>&gt; arimax(saving, order = c(1,0,2),fixed=c(NA,0,NA,NA,NA,NA,NA,NA,NA,NA),io=c(55,62),xreg=data.frame(AO82=1*(seq(saving)==82),
+ AO43=1*(seq(saving)==43),AO89=1*(seq(saving)==89),LS99=1*(seq(saving)&gt;=99)),method='ML')

Call:
arimax(x = saving, order = c(1, 0, 2), xreg = data.frame(AO82 = 1 * (seq(saving) == 
    82), AO43 = 1 * (seq(saving) == 43), AO89 = 1 * (seq(saving) == 
    89), LS99 = 1 * (seq(saving) &gt;= 99)), fixed = c(NA, 0, NA, NA, NA, NA, 
    NA, NA, NA, NA), method = ""ML"", io = c(55, 62))

Coefficients:
         ar1  ma1     ma2  intercept    AO82    AO43     AO89     LS99    IO-55   IO-62
      0.7918    0  0.3406     6.0628  2.3800  1.1297  -1.0466  -1.4885  -0.5958  0.5517
s.e.  0.0674    0  0.1060     0.3209  0.3969  0.3780   0.3835   0.5150   0.4044  0.3772

sigma^2 estimated as 0.2611:  log likelihood = -75.57,  aic = 169.14
</code></pre>
"
"0.247536885744169","0.28474739872575"," 38248","<p>I just want to ask about the <code>Arima</code> function in forecast package. The usage of it is,</p>

<pre><code>Arima(x, order=c(0,0,0), seasonal=list(order=c(0,0,0), period=NA),
    xreg=NULL, include.mean=TRUE, include.drift=FALSE, 
    include.constant, lambda=model$lambda, transform.pars=TRUE, 
    fixed=NULL, init=NULL, method=c(""CSS-ML"",""ML"",""CSS""), n.cond, 
    optim.control=list(), kappa=1e6, model=NULL)
</code></pre>

<p>My data has trend and seasonality, so I applied seasonal differencing using the codes below,</p>

<pre><code>Diff1LogCP &lt;- diff(LogChickenProd, lag = 4, differences = 1)
</code></pre>

<p>Now, I want to have a model SARIMA(1,1,1). Using the <code>Arima</code> function, I'm not sure with the <code>order</code> and <code>seasonal</code> argument. Since my model is SARIMA(1,1,1), so what I did is,</p>

<pre><code>SARIMA111 &lt;- Arima(ChickenProd, seasonal = list(order = c(1,1,1), period = 4))
</code></pre>

<p>Notice that I ignore the <code>order</code> argument before the <code>seasonal</code> argument. This is because I'm using a seasonal ARIMA model, and not the ordinary ARIMA. So I use the <code>seasonal</code> argument only. Is that correct? Or do I need to include the <code>order</code> argument, that makes my new codes be</p>

<pre><code>SARIMA111 &lt;- Arima(ChickenProd, order = c(1,1,1), seasonal = list(order = c(1,1,1), period = 4))
</code></pre>

<p>Or should it be like</p>

<pre><code> SARIMA111 &lt;- Arima(ChickenProd, order = c(1,0,1), seasonal = list(order = c(0,1,0), period = 4))
</code></pre>

<p>Thank you in advance!</p>
"
"0.242535625036333","0.232495277487639"," 43214","<p>The basic idea i'm trying is to model the data with factor analysis, <strong>assuming a latent variable structure</strong> that underlies the observations. Labels for ""real"" anomalies are available and used for validation. Another important note is that the data does not have a ""very"" Gaussian nature. </p>

<p>Then, projecting the observations onto the new subspace found, and <strong>looking for outliers</strong> - i.e. finding the observations that digress from the underlying structure, assuming the anomalies i'm looking for will be more prevalent there. </p>

<p>Using R's <em>factanal</em>, with <em>varimax</em> (or <em>oblimin</em>) rotations, i have encountered the following result, which i'm not sure about :</p>

<ul>
<li>When projecting the <strong>normalized</strong> data (z-score) using the FA model found, classification/detection is poor ; but since factanal scales the data in advance, i believe this is the correct way(?)</li>
<li>When projecting the <strong>original</strong> data (not normalized) using the same FA model, detection is much improved. </li>
</ul>

<p>Could this be due to projecting the observations onto a subspace that assumes a low variance, much more ""normal"" distribution, thereby ""aggravating"" any anomalous points?</p>

<p>Would appreciate help/insight as to what i'm missing!</p>

<p>Thank you.</p>

<p><strong>Update:</strong>
- LX yields much better classification than LX*, where L is the loadings matrix, X is the data matrix, and X* is the standardized data matrix. LX also yields better classification than in the following.
<a href=""https://stat.ethz.ch/pipermail/r-help/2002-April/020278.html"" rel=""nofollow"">https://stat.ethz.ch/pipermail/r-help/2002-April/020278.html</a></p>

<p>Could the LX transformation be related to LDA?</p>
"
"0.27116307227332","0.155962573473011"," 87726","<p>In the Arima package, using a Box-Cox transformation give wrong results when later applied to the forecast method.</p>

<p>For example, consider this data:</p>

<pre><code>library(forecast)
data&lt;-c(2,3,2,3,2,3)
</code></pre>

<p>And for the sake of simplicity, consider an ARIMA(0,0,0) model. (The mean of this series is 2.5.)</p>

<p>The mean forecast made without a Box-Cox transformation is correct:</p>

<pre><code>forecast(Arima(data,order=c(0,0,0)))$mean
 [1] 2.5 2.5 2.5 2.5 2.5 2.5 2.5 2.5 2.5 2.5
</code></pre>

<p>However if we use a Box-Cox transformation, such as a log transformation with lambda=0, the ""mean"" forecast is wrong:</p>

<pre><code>forecast(Arima(data,order=c(0,0,0), lambda=0))$mean
[1] 2.44949 2.44949 2.44949 2.44949 2.44949 2.44949 2.44949 2.44949 2.44949 2.44949
</code></pre>

<p>It seems that to produce the mean forecast of Y=exp(X), it does E(Y)=exp(E(X)).</p>

<p>Is there a way to correct this?
Is there a package with a correct implementation of forecasts with Box-Cox transformations?</p>
"
"0.27116307227332","0.259937622455018","102775","<p>I have a problem on what model class (AR, MA, ARMA, ARIMA, etc.) will I use on my data, i.e., what order (say 1,0,1) will I use, using a Box-Jenkins procedure.</p>

<p>I have already done many transformations on my data but the errors are so large and the correlation is somewhat small.  My data are stationary (ADF test and KPSS test) but not normally distributed (Anderson-Darling, Wilk-Shapiro and Kolmogorov-Smirnov test). So I apply natural log and then test it again but it is still not normally distributed. So I differenced it once and it is now stationary and normally distributed.</p>

<p>I already satisfy the requirements of using a Box-Jenkins process. Then I use <code>auto.arima</code> in R to know what order to use and I also try SPSS using its expert modeler to cross check.</p>

<p>My problem is I still get large errors and small R-squared.  I need to know what to do for determining order? I also have problems in understanding ACFs and PACFs.</p>

<p>Below is my actual data:</p>

<pre><code>Harvest
</code></pre>

<blockquote>
  <p>60477
  29323
  51369
  15800
  58994
  45496
  17227
  92103
  138573
  39181
  51192
  13132
  400
  18258
  54553
  7220
  1418
  6807
  17915
  89015
  122154
  122853
  63398
  27246
  27013
  36317
  65735
  94744
  78763
  39769
  20422
  27398
  33552
  10000
  6500
  5300
  5700
  4800
  5300
  6450
  9300
  5834
  29200
  39975
  65000
  45494
  79000
  7900
  54758
  70581
  31505
  45437
  29691
  110947
  40498
  71238
  42170
  38723
  64813
  122992
  17929
  11652
  134137
  110043
  60153
  7625
  25967
  38918
  1621
  14946
  76610
  84516
  72223
  40399
  63482
  34918
  63098
  105388
  135809
  31345
  66880
  160511
  40238
  35767
  105560
  119276
  154348
  86935
  73728
  167119
  128709
  97040
  21780
  9906
  62213
  99940
  72626
  117783
  58037
  68756
  25721
  19853
  4943
  2027
  20251
  114718
  27801
  80868
  94761
  18914
  119632
  187924
  56950
  52886
  141456
  141507</p>
</blockquote>

<p><img src=""http://i.stack.imgur.com/ASQah.png"" alt=""Harvest graph""></p>

<p>This is the differenced data</p>

<pre><code>d_Harvest
</code></pre>

<blockquote>
  <p>-31154
  22046
  -35569
  43194
  -13498
  -28269
  74876
  46470
  -99392
  12011
  -38060
  -12732
  17858
  36295
  -47333
  -5802
  5389
  11108
  71100
  33139
  699
  -59455
  -36152
  -233
  9304
  29418
  29009
  -15981
  -38994
  -19347
  6976
  6154
  -23552
  -3500
  -1200
  400
  -900
  500
  1150
  2850
  -3466
  23366
  10775
  25025
  -19506
  33506
  -71100
  46858
  15823
  -39076
  13932
  -15746
  81256
  -70449
  30740
  -29068
  -3447
  26090
  58179
  -105063
  -6277
  122485
  -24094
  -49890
  -52528
  18342
  12951
  -37297
  13325
  61664
  7906
  -12293
  -31824
  23083
  -28564
  28180
  42290
  30421
  -104464
  35535
  93631
  -120273
  -4471
  69793
  13716
  35072
  -67413
  -13207
  93391
  -38410
  -31669
  -75260
  -11874
  52307
  37727
  -27314
  45157
  -59746
  10719
  -43035
  -5868
  -14910
  -2916
  18224
  94467
  -86917
  53067
  13893
  -75847
  100718
  68292
  -130974
  -4064
  88570
  51</p>
</blockquote>

<p><img src=""http://i.stack.imgur.com/WzyDi.png"" alt=""enter image description here""></p>
"
"0.121267812518166","0.232495277487639","106038","<p>I know that this is probably a question that's been asked plenty of times, but i haven't seen an answer that's both accurate and simple. How do you estimate the appropriate forecast model for a time series by visual inspection of the ACF and PACF plots? Which one, ACF or PACF, tells the AR or the MA (or do they both?) Which part of the graphs tell you the seasonal and non seasonal part for a seasonal ARIMA?</p>

<p>Take for instance these functions:</p>

<p><img src=""https://i.imgur.com/E64Sd7p.png"" alt=""enter image description here""></p>

<p>They show the ACF and PCF of a log transformed series that's been differenced twice, one simple difference and one seasonal.</p>

<p>How would you caracterize it? What model best fits it?</p>

<p>Thanks in advance!</p>

<p><strong>EDIT:</strong> Added raw data</p>

<p>Original data: <a href=""http://pastebin.com/KRJnXzXp"">here</a></p>

<p>Log transformed data: <a href=""http://pastebin.com/JR3bkctv"">here</a></p>

<p><strong>EDIT:</strong> Corrected ACF and PACF functions (previous ones were overdifferentiated)</p>
"
"0.394120390684041","0.435928645289322","110611","<p>Note that this is a simplified example:</p>

<p>I have some time series that I made stationary by differencing twice. Then I ran <code>arima</code> on it, and set d = 0 to prevent additional differencing (I'm aware that <code>auto.arima</code> could detect the order of integration, but I'm hard-coding this myself for other reasons). Now I want to use <code>fitted</code> data from my <code>arima</code> object to determine what the <em>non-stationary</em> fit would look like.</p>

<p>For example:</p>

<pre><code>library('forecast')
# simulate ARIMA(1,2,0) time series:
rawData &lt;- arima.sim(n = 20, list(order = c(1,2,0), ar = 0.7)) 
# use diff function to make the series stationary:
stationaryData &lt;- diff(diff(rawData))

# fit ARIMA on them appropriately
rawDataFit &lt;- arima(rawData, c(1,2,0)) # include.mean = FALSE by default
stationaryDataFit &lt;- arima(stationaryData, c(1,0,0), include.mean = FALSE) # stationaryData is already twice differenced

# notice that there is very small variance between the AR(1) coefficients:
coef(rawDataFit)
coef(stationaryDataFit)
</code></pre>

<p>In this particular instance, my AR(1) coefficeints are 0.5511049 and 0.5511048. I also forced my ARIMA to exclude an intercept, so these ARIMA objects so be similar.</p>

<pre><code># plot of rawData and the fitted values
plot(rawData, type = ""l"")
lines(fitted(rawDataFit), col = ""slategrey"")
</code></pre>

<p>Here's an example of what that plot could look like:
<img src=""http://i.stack.imgur.com/5R0cM.png"" alt=""Here&#39;s an example of what that plot could look like""></p>

<p>I want to recreate the above plot, <em>without</em> using the rawDataFit object</p>

<pre><code># using the diffinv function, I can easily replicate the rawData:
recoveredRawData &lt;- diffinv(stationaryData, differences = 2, xi = rawData[1:2])

# Now I also want to ""recover"" the non-stationary data from the fitted AR(1) object:
recoveredFit &lt;- diffinv(fitted(stationaryDataFit), differences = 2, xi = c(0,0))

# plot of rawData and the fitted values
plot(recoveredRawData, type = ""l"")    
lines(recoveredFit, col = ""slategrey"")
</code></pre>

<p>Here's the attempt to recreate the above plot, using the results from my stationaryDataFit:</p>

<p><img src=""http://i.stack.imgur.com/xZuKH.png"" alt=""Here&#39;s the attempt to recreate the above plot, using the results from my stationaryDataFit""></p>

<p>The shape looks correct, but the values are clearly off. I am <em>not</em> expecting to recover exactly the same results from both methods of fitting, but I still expect them to be reasonably close. </p>

<p>I strongly suspect the problem is with my choice of xi in the <code>diffinv</code> function, since that's really the only place I'm making any assumptions. But I'm having trouble reconciling the issue.</p>

<p>To integrate the data, <code>diffinv</code> requires the first observations of the integrated data. This is how I can convert the stationaryData back to the rawData, by passing the first two values of rawData to the <code>diffinv</code> xi argument. But I'm unsure what to use as the starting values to integrate <code>fitted(stationaryDataFit)</code>. The first two values of the (integrated) rawData are 0, so that's what I'm trying for now...</p>

<p>Any ideas?</p>

<p><strong>EDIT:</strong> Is this a legitimate work-around? Take the residuals from my stationaryDataFit object, and just subtract those from my rawData? For example:</p>

<pre><code># prefix 2 zeros, so the vectors are the same length (due to secord-differencing):
recoveredFit &lt;- rawData - c(rep(0, 2), stationaryDataFit$residuals)
</code></pre>

<p>My concern is about whether I need to transform my residuals from the stationaryDataFit somehow? In fact, the residuals from both fits are extremely close (within several decimals).</p>

<p>Thank you!</p>
"
"0.171498585142509","0.164398987305357","115271","<p>I have been given data to forecast however it has a negative figure within the data which then, when doing a log transformation to make the series stationary, the ARIMA script i have written won't work.</p>

<pre><code>datan&lt;-c(144627.7451,575166.2487,854245.7137,1230639.153,1160052.421,479928.7072,-261427.4238,1181746.229,168251.621,556741.5149,1840484.518,1704679.404,1878380.278,1865288.502,1849340.253,1965974.112,2093192.242,1912399.391,2633179.421,2134618.008,2070856.492,1238565.331)

freqdata&lt;-4
startdata&lt;-c(9,2)
horiz&lt;-4
datats&lt;-ts(datan,frequency=freqdata,start=startdata)
force.log&lt;-""log""
datadates&lt;-as.character(c(""9q2"",""9q3"",""9q4"",""10q1"",""10q2"",""10q3"",""10q4"",""11q1"",""11q2"",""11q3"",""11q4"",""12q1"",""12q2"",""12q3"",""12q4"",""13q1"",""13q2"",""13q3"",""13q4"",""14q1"",""14q2"",""14q3""))
dataMAT&lt;-matrix(0,ncol=freqdata,nrow=(length(datats)+freqdata),byrow=TRUE)
for (i in 1:freqdata)
  {dataMAT[,i]&lt;-c(rep(0,length=i-1),lag(datats,k=-i+1),rep(0,length=freqdata-i+1))}
dataind&lt;-dataMAT[c(-1:(-freqdata+1),-(length(dataMAT[,1])-freqdata+1):-(length(dataMAT[,1]))),]
dataind2&lt;-data.frame(dataind)
lm1&lt;-lm(X1~.,data=dataind2)
lm2&lt;-lm(X1~X2+dataind2[,length(dataind2[1,])],data=dataind2)
library(lmtest)
library(car)
bptest1&lt;-bptest(lm1)
bptest2&lt;-bptest(lm2)
gqtest1&lt;-gqtest(lm1)
ncvtest1&lt;-ncvTest(lm1)
ncvtest2&lt;-ncvTest(lm2)
if(force.log==""level"") 
  {aslog&lt;-""n""}else
    {{if(force.log==""log"")
       {aslog&lt;-""y""}else
         {if(bptest1$p.value&lt;0.1|bptest2$p.value&lt;0.1|gqtest1$p.value&lt;0.1|ncvtest1$p&lt;0.1|ncvtest2$p&lt;0.1)
           {aslog&lt;-""y""}else
              {aslog&lt;-""n""}}}}
if(aslog==""y"")
  {dataa&lt;-log(datats)}else
    {dataa&lt;-datats}
startLa&lt;-startdata[1]+trunc((1/freqdata)*(length(dataa)-horiz))
startLb&lt;-1+((1/freqdata)*(length(dataa)-horiz)-trunc((1/freqdata)*(length(dataa)-horiz)))*freqdata
startL&lt;-c(startLa,startLb)
K&lt;-ts(rep(dataa,length=length(dataa)-horiz),frequency=freqdata,start=startdata)
L&lt;-ts(dataa[-1:-(length(dataa)-horiz)],frequency=freqdata,start=startL)
library(strucchange)
efp1rc&lt;-efp(lm1,data=dataind2,type=""Rec-CUSUM"")
efp2rc&lt;-efp(lm2,data=dataind2,type=""Rec-CUSUM"")
efp1rm&lt;-efp(lm1,data=dataind2,type=""Rec-MOSUM"")
efp2rm&lt;-efp(lm2,data=dataind2,type=""Rec-MOSUM"")
plot(efp2rc)
lines(efp1rc$process,col =""darkblue"")
plot(efp2rm)
lines(efp1rm$process,col=""darkblue"")
gefp2&lt;-gefp(lm2,data=dataind2)
plot(gefp2)
plot(dataa)
pacf(dataa)
sctest(efp2rc)
cat(""log series,y/n?:"",aslog)
</code></pre>

<p>then i want to run arima to get the forecasts</p>

<pre><code>library(tseries)
library(forecast)
max.sdiff&lt;-3
arima.force.seasonality&lt;-""n""
kpssW&lt;-kpss.test(dataa,null=""Level"")
ppW&lt;-tryCatch({ppW&lt;-pp.test(dataa,alternative=""stationary"")},error=function(ppW){ppW&lt;-list(error=""TRUE"",p.value=0.99)})
adfW&lt;-adf.test(dataa,alternative=""stationary"",k=trunc((length(dataa)-1)^(1/3)))
if(kpssW$p.value&lt;0.05|ppW$p.value&gt;0.05|adfW$p.value&gt;0.05)
  {ndiffsW=1}else
    {ndiffsW=0}
aaW&lt;-auto.arima(dataa,max.D=max.sdiff,d=ndiffsW,seasonal=TRUE,allowdrift=FALSE,stepwise=FALSE,trace=TRUE,seasonal.test=""ch"")
orderWA&lt;-c(aaW$arma[1],aaW$arma[6],aaW$arma[2])
orderWS&lt;-c(aaW$arma[3],aaW$arma[7],aaW$arma[4])
if(sum(aaW$arma[1:2])==0)
  {orderWA[1]&lt;-1}else
    {NULL}
if(arima.force.seasonality==""y"")
  {if(sum(aaW$arma[3:4])==0)
    {orderWS[1]&lt;-1}else
      {NULL}}else
        {NULL}
Arimab&lt;-Arima(dataa,order=orderWA,seasonal=list(order=orderWS),method=""ML"")
fArimab&lt;-forecast(Arimab,h=8,simulate=TRUE,fan=TRUE)
if(aslog==""y"")
  {fArimabF&lt;-exp(fArimab$mean[1:horiz])}else
    {fArimabF&lt;-fArimab$mean[1:horiz]}
plot(fArimab,main=""ARIMA Forecast"",sub=""blue=fitted,red=actual"") 
lines(dataa,col=""red"",lwd=2) #changes colour and size of dataa
lines(ts(append(fitted(Arimab),fArimab$mean[1]),frequency=freqdata,start=startdata),col=""blue"",lwd=2)
if(aslog==""y"")
  {Arimab2f&lt;-exp(fArimab$mean[1:horiz])}else
    {Arimab2f&lt;-fArimab$mean[1:horiz]} 
start(fArimab$mean)-&gt;startARIMA
ArimaALTf&lt;-ts(prettyNum(Arimab2f,big.interval=3L,big.mark="",""),frequency=freqdata,start=startARIMA)
View(ArimaALTf,title=""ARIMA2 final forecast"") #brings up table of the forecasts
summary(Arimab)
</code></pre>

<p>If anyone can help me figure out how to forecast this data with the negative i will be really grateful!!</p>
"
"0.344423360096832","0.450225168890748","116145","<p>I have downloaded the daily stock Adjusted Close price of one stock from sep 2011 to till date. As per my study plan, I have plotted some basic plots to understand the daily stock Adjusted closing price.</p>

<p>Here is the xyplot of the stock closing price by date and the code used to plot(My x axis not visible).</p>

<pre><code>Stock_T=stocks[which(symbol=='Stock_T'),]
xyplot(Adj.Close~Date,type='l',data=Stock_T,main='Adj.Close Price of the Stock_T')
</code></pre>

<p><img src=""http://i.stack.imgur.com/Ivlmk.png"" alt=""Timeseries plot of the raw data- Adjusted Closing price of the Stock""></p>

<p>By seeing this plot, the closing price was stable for period but had sudden huge increase in the stock price, it might had some other indicator which caused this much change in the stock price. Now my objective is to learn some ARIMA modeling concepts using this stock prices and try to do some forecasting of the stock price for few weeks. </p>

<p>As I have basic knowledge in ARIMA modeling, and I learned in the books that we should have stationary series before applying the ARIMA Model.</p>

<p>So, now I have plotted the ACF and PACF of the above raw data timeseries.</p>

<pre><code>acf(Stock_T$Adj.Close)
</code></pre>

<p><img src=""http://i.stack.imgur.com/rpy8S.png"" alt=""Raw data ACF Plot""></p>

<pre><code>pacf(Stock_T$Adj.Close)
</code></pre>

<p><img src=""http://i.stack.imgur.com/QKQge.png"" alt=""Raw data PACF plot""></p>

<p>From the above ACF and PACF plot, the series is not stationary and have huge autocorrelation (please correct me if am wrong), by differencing the series we will have stationary series (please correct me if am wrong). Here is the below plot.</p>

<pre><code>Stock_T_d1=diff(Stock_T$Adj.Close)
</code></pre>

<p><img src=""http://i.stack.imgur.com/sD9Tj.png"" alt=""First difference of the raw series""></p>

<p>Here the differencing series and its ACF AND PACF plots. ACF plot shows that there is no auto correlation and the series is stationary (please correct me if I am wrong) but I am unable to interpret the PACF plots, can someone explain it to me?  </p>

<p><img src=""http://i.stack.imgur.com/cAoVf.png"" alt=""ACF plot of Difference series""></p>

<p><img src=""http://i.stack.imgur.com/U10g8.png"" alt=""PACF plot of Difference series""></p>

<p>The above difference series shows some unequal variance in the series and so I am taking log transformation before differencing and its ACF and PACF.</p>

<pre><code>Stock_T_logd1=diff(log(Stock_T$Adj.Close))
</code></pre>

<p><img src=""http://i.stack.imgur.com/MWovq.png"" alt=""Difference Logged series ""></p>

<p><img src=""http://i.stack.imgur.com/oYd2d.png"" alt=""ACF of Difference logged Series""></p>

<p><img src=""http://i.stack.imgur.com/HQxZw.png"" alt=""PACF of Difference logged Series""></p>

<p>Now I will try to ask my questions.</p>

<ol>
<li>Should we have stationary series before we apply ARIMA?</li>
<li>Could you please explain me the ACF and PACF of the original series, and what we should do if we have this kind of series?</li>
<li>Could you please explain me the ACF and PACF of the difference series, and what will be the next step?</li>
<li>Could you please explain me the ACF and PACF of the difference logged series, and what will be the next step?</li>
<li>Should we use difference series or difference logged series?</li>
<li>What will be the ARIMA orders of this series?</li>
<li>Is there any R code to find the ARIMA order automatically of the original series?</li>
</ol>
"
"0.509324812576299","0.557988665970332","123576","<p>I am trying to test the effect on the heat flux between indoors and outdoors before and after removing insulation.</p>

<p>Briefly, I have 26 sensors on a wall, measuring heat flow between indoors and outdoors over a number of days. The wall was part of a real world experimental setup so that the insulation on the wall was removed halfway through the experiment. Â What I care about is to have a measure of the effect of the removal of the insulation (I am not interested in any form of forecasting). Â I am exploring the use of a SARIMA/ARIMAX models with one regressor because, aside from the removal of the insulation, the heat flow between indoors and outdoors was affected by daily cyclical and random environmental effects (heating on or off, daily temperature changes, wind, etc).  Here I will present that data and analysis of one sensor.  My data has been collected hourly, and I have transformed the variable â€˜insulatedâ€™ â€˜not insulatedâ€™ as a factor of 0s and 1s as indicator.</p>

<pre><code>heat.flux = c(8.677048,6.558642,5.920314,5.583614,5.373176,5.253928,4.938272,7.358305,9.743266,10.46577,11.06201,10.90067,11.49691,13.15236,12.10017,10.60606,10.45875,10.03788,9.588945,9.287318,8.578844,8.024691,10.26936,11.8757,10.20623,8.634961,8.305275,8.101852,8.12991,7.947531,7.814254,10.40264,13.08221,14.3729,14.94809,15.08838,15.20763,15.75477,14.57632,12.79461,11.97391,10.97082,10.33249,9.701178,9.715208,9.083895,10.63412,12.07912,9.736251,7.638889,6.453423,5.983446,5.499439,5.099607,4.70679,6.972503,9.259259,9.981762,10.24832,10.17116,10.27637,10.27637,9.546857,7.568743,7.168911,6.867284,6.705948,6.916386,8.319304,8.424523,11.41274,13.52413,11.70034,9.532828,8.957632,9.07688,9.694164,9.301347,9.048822,12.28255,14.95511,15.22868,15.24972,15.12346,15.08838,15.17256,13.68547,12.18434,12.1633,12.13524,11.81257,11.58109,11.44781,11.27946,13.87486,15.92312,14.07828,11.90376,10.46577,9.518799,8.978676,8.803311,8.684063,11.65123,14.39394,15.69865,16.61756,16.828,16.83502,16.16863,14.23962,12.19837,12.09315,11.5881,11.20932,10.50786,10.59203,10.64815,13.51712,15.71268,13.92396,12.10718,12.2615,11.65123,11.05499,10.31846,9.834456,12.9349,15.41807,15.78283,15.8179,16.11953,15.95118,15.63552,13.1243,11.22334,10.21324,8.705107,7.526655,6.15881,5.30303,5.597643,8.599888,11.17424,9.631033,8.038721,7.638889,7.203984,7.161897,6.76908,6.888328,9.518799,12.40881,13.21549,14.28872,14.43603,14.8078,14.81481,13.60129,12.59119,11.86167,11.91779,11.73541,12.04405,11.51796,11.74242,13.7486,15.85999,14.84989,12.63328,10.68322,9.343434,8.592873,8.333333,8.445567,10.97783,13.82576,15.12346,16.58249,17.61364,18.30808,19.10774,17.97138,16.62458,15.867,16.07744,15.63552,16.0073,15.42508,15.01122,17.10157,18.94641,22.44669,18.94641,16.01431,14.55527,13.88889,12.77357,11.66526,12.46493,15.41807,16.75786,17.27694,17.03143,16.84905,16.828,16.02834,16.35802,16.04237,15.03928,14.00112,14.1344,13.86785,13.99411,15.30584,18.20286,19.49355,16.16162,14.05022,12.05107,12.27553,13.01207,12.5491,13.72054,16.91218,18.62374,18.79209,20.80527,19.50758,20.18799,20.63692,18.49747,17.25589,17.38215,18.40629,18.60269,19.12177,18.66582,21.09989,24.45286,26.71156,23.54798,20.01964,17.98541,14.83586,14.31678,15.15152,15.30584,17.95735,19.71801,20.30724,20.19501,20.2862,20.1459,20.10382,18.20988,16.54742,15.22868,13.96605,12.71044,11.61616,10.71829,12.12121,14.77273,14.04321,12.44388,10.94978,10.2413,9.708193,9.638047,9.322391,11.27245,14.24663,14.77273,14.75168,14.92705,15.47419,15.48822,14.73765,13.68547,12.65432,12.35269,12.34568,12.32464,12.7385,12.84371,14.16947,17.34007,17.09456,15.0954,13.40488,11.70735,10.8165,10.64815,12.01599,13.55219,16.7298,17.45932,17.61364,19.58474,20.02666,19.79517,19.38833,17.32604,16.11953,15.62851,15.01122,14.70258,14.5693,14.35887,16.28086,18.69388,18.92536,16.56846,15.97222,13.34877,12.81566,12.04405,13.23653,14.1835,16.75786,17.55752,17.98541,18.85522,18.8482,19.02357,18.96044,17.31201,15.42508,14.38692,13.57323,12.36672,12.03002,11.41274,13.15236,15.88103,14.66049,12.8858,11.67228,11.03395,9.399551,8.375421,8.073793,10.6271,13.57323,13.61532,14.31678,14.73765,15.08838,15.62149,16.6807,15.28479,14.07127,13.14534,12.61223,12.57015,12.02301,12.17031,14.33782,18.83418,20.45455,18.67985,18.40629,16.51235,14.45006,14.61841,15.20763,15.57941,18.06958,19.88636,20.51066,21.633,23.24635,24.28451,24.70539,24.19332,22.81145,21.97671,21.58389,21.3945,21.21212,20.89646,21.1069,23.86364)

insulation = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)
</code></pre>

<p>First off, the time series plot of the heat flux is this (the red line is when the insulation is removed):</p>

<p><img src=""http://i.stack.imgur.com/SYSQj.jpg"" alt=""Time series plot of heat flux""></p>

<p>Than this are the ACF and PACF plots of the same data:</p>

<p><img src=""http://i.stack.imgur.com/7keT7.jpg"" alt=""ACF and PACF of the data""></p>

<p>For my data, an <code>stl()</code> decomposition, run as <code>stl(ts(heat.flux, frequency = 24), 'period')</code></p>

<p>shows a strong â€˜seasonalâ€™ (i.e daily) component and a trend in the series. Â </p>

<p><img src=""http://i.stack.imgur.com/CUsta.jpg"" alt=""STL of the data""></p>

<p>Firs off I am trying to determine the best parameters for a SARIMA or ARIMAX model so that I can get an estimation of the effect removing the insulation. Despite the fact I can produce the ACF and PACF plots there is no way I can figure out the proper orders, so I load the library <code>forecast</code> and I run:</p>

<pre><code>library(forecast)
auto.arima(ts(heat.flux, frequency = 24), xreg = insulation, max.p = 10, max.q = 10, max.P = 10, max.Q = 10, stationary = F)Â 
</code></pre>

<p>The reason why I do not specify a stationary model is because of the trend I see with <code>stl()</code> and because I assume an effect of removing the insulation.</p>

<p>from <code>auto.arima()</code> I get:</p>

<pre><code>Series: ts(heat.flux, frequency = 24) 
ARIMA(2,0,2)(1,0,1)[24] with non-zero mean 

Coefficients:
         ar1      ar2      ma1      ma2    sar1     sma1  intercept  carp.hour$interv
      1.9414  -0.9495  -0.7423  -0.1793  0.9717  -0.6009    11.2449            4.3338
s.e.  0.0231   0.0221   0.0570   0.0544  0.0104   0.0544     2.1075            0.5548

sigma^2 estimated as 0.4484:  log likelihood=-411.28
AIC=840.55   AICc=841.03   BIC=876.11
</code></pre>

<p>If I try to use the <code>TSA</code> package and use <code>arimax()</code> with those orders I get basically the same stuff:</p>

<pre><code>library(TSA)
arimax(ts(heat.flux, frequency = 24), xreg = insulation, order = c(2,0,2), seasonal = list(order = c(1,0,1), frequency = 24))
Series: x 
ARIMA(2,0,2)(1,0,1)[24] with non-zero mean 

Coefficients:
         ar1      ar2      ma1      ma2    sar1     sma1  intercept    xreg
      1.9414  -0.9495  -0.7423  -0.1793  0.9717  -0.6009    11.2449  4.3338
s.e.  0.0231   0.0221   0.0570   0.0544  0.0104   0.0544     2.1075  0.5548

sigma^2 estimated as 0.4484:  log likelihood=-411.28
AIC=838.55   AICc=839.03   BIC=874.11
</code></pre>

<p>And all is apparently well (Irrespective of the function I choose I get an estimate of the effect of the removal of the insulation and a se with it with is what I want). Â Unfortunately, when I test the fit of this model with the function <code>sarima()</code> from the <code>astsa</code>package I get significant Ljung-Box p-values for all my sensors and for all the lags:</p>

<pre><code>library(astsa)
sarima(ts(heat.flux, frequency = 24), p = 2, d = 0, q = 2, P =1, D = 0, Q = 1, S = 24, xreg = insulation)
$fit

Call:
stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, 
Q), period = S), xreg = xreg, optim.control = list(trace = trc, REPORT = 1, 
reltol = tol))

Coefficients:
         ar1      ar2      ma1      ma2    sar1     sma1  intercept    xreg
      1.9414  -0.9495  -0.7423  -0.1793  0.9717  -0.6009    11.2449  4.3338
s.e.  0.0231   0.0221   0.0570   0.0544  0.0104   0.0544     2.1075  0.5548

sigma^2 estimated as 0.4484:  log likelihood = -411.28,  aic = 840.55
</code></pre>

<p>but the plot that comes with is shows that at every single lag the Ljung-Box statistics is significant:</p>

<p><img src=""http://i.stack.imgur.com/ZMGKN.jpg"" alt=""SARIMA""></p>

<p>What is going on?  To sum it up:</p>

<ol>
<li>which of these models is the most correct to estimate the effect of insulation?</li>
<li>why are the Ljung-Box p-values all significant?  I would have though that the ARIMA/ARIMAX/SARIMA would have sorted that issue</li>
<li>If the orders calculated by <code>auto.arima()</code> are the problem, how could I find them in a different way (which is computationally feasible and does not take days).</li>
</ol>

<p>Finally, two notes.  I also have collected variables such as internal and external temperatures, windspeed, etc, but I would have though that integrating these in the model would be superfluous given the fact it is already an ARIMA model to start with.  Second, I am not at all wedded to this kind of analysis, but I am aware that a straightforward linear model would not be acceptable given the autocorrelation between the data points.</p>
"
"0.30012252399939","0.328797974610715","123723","<p>Can anyone tell me the formula behind the <code>forecast</code> function in R? Preferably in the form easily understood by mathematicians (e.g  x_t, Î¸ etc)</p>

<p>Here is my code in case it helps</p>

<pre><code>suppressMessages(library(lmtest))
suppressMessages(library(car))
suppressMessages(library(tseries))
suppressMessages(library(forecast))
suppressMessages(library(TTR))
suppressMessages(library(geoR))
suppressMessages(library(MASS))
suppressMessages(library(gtools))
#-------------------------------------------------------------------------------
Model &lt;- ""choosing ARIMA""
Series.title &lt;- ""EMEA GAM&lt;250K""
#-------------------------------------------------------------------------------
Input.data &lt;- matrix(c(""08Q1"",""08Q2"",""08Q3"",""08Q4"",""09Q1"",""09Q2"",""09Q3"",""09Q4"",""10Q1"",""10Q2"",""10Q3"",""10Q4"",""11Q1"",""11Q2"",""11Q3"",""11Q4"",""12Q1"",""12Q2"",""12Q3"",""12Q4"",""13Q1"",""13Q2"",""13Q3"",""13Q4"",""14Q1"",""14Q2"",""14Q3"",5403.675741,6773.504993,7231.117289,7835.55156,5236.709983,5526.619467,6555.781711,11464.72728,7210.068674,7501.610403,8670.903486,10872.93518,8209.022658,8153.393088,10196.44775,13244.50201,8356.732878,10188.44157,10601.32205,12617.82102,11786.52641,10044.98676,11006.0051,15101.9456,10992.27282,11421.18922,10731.31198),ncol=2,byrow=FALSE)

#-------------------------------------------------------------------------------
# The frequency of the data. 1/4 for QUARTERLY, 1/12 for MONTHLY

Frequency &lt;- 1/4

#-------------------------------------------------------------------------------
# How many quarters/months to forecast

Forecast.horizon &lt;- 4

#-------------------------------------------------------------------------------
# The first date in the series. Use c(8, 1) to denote 2008 q1

Start.date &lt;- c(8, 1)

#-------------------------------------------------------------------------------
# The dates of the forecasts

Forecast.dates &lt;- c(""14Q4"", ""15Q1"", ""15Q2"", ""15Q3"")

#-------------------------------------------------------------------------------
# Selects the data column from Input.data

Data.col &lt;- as.numeric(Input.data[, 2])

#-------------------------------------------------------------------------------
# Turns the Data.col into a time-series

Data.col.ts &lt;- ts(Data.col, deltat=Frequency, start = Start.date)

#-------------------------------------------------------------------------------
# A character vector of the dates from Input.data

Dates.col &lt;- as.character(Input.data[,1])

#------- Transform ------------------------------------------------------------------------
# Starts the testing to see if the data should be logged

transform.method &lt;- round(BoxCox.lambda(Data.col.ts, method = ""loglik""), 5)

log.values &lt;- seq(0, 0.24999, by = 0.00001)
sqrt.values &lt;- seq(0.25, 0.74999, by = 0.00001)

which.transform.log &lt;- transform.method %in% log.values
which.transform.sqrt &lt;- transform.method %in% sqrt.values

if (which.transform.log == ""TRUE""){
  as.log &lt;- ""log""
  Data.new &lt;- log(Data.col.ts)
} else {
  if (which.transform.sqrt == ""TRUE""){
    as.log &lt;- ""sqrt""
    Data.new &lt;- sqrt(Data.col.ts)
  } else {
    as.log &lt;- ""no""
    Data.new &lt;- Data.col.ts
  }
}

#----- Find best ARIMA model ---------------------------------------------------

a &lt;- permutations(n = 3, r = 6, v = c(0:2), repeats.allowed = TRUE)
a &lt;- a[ifelse((a[, 1] + a[, 4] &gt; 2 | a[, 2] + a[, 5] &gt; 2 | a[, 3] + a[, 6] &gt; 2),
              FALSE, TRUE), ]

Arimafit &lt;- matrix(0,
                   ncol  = length(Data.new),
                   nrow  = length(a[, 1]),
                   byrow = TRUE)

totb &lt;- matrix(0, ncol = 1, nrow = length(a[, 1]))
arimaerror &lt;- matrix(0, ncol = length(Data.new), nrow = 1)

for (i in 1:length(a[, 1])){
  ArimaData.new &lt;- try(Arima(Data.new,
                             order    = a[i, c(1:3)],
                             seasonal = list(order = a[i, c(4:6)]),
                             method   = ""ML""),
                       silent = TRUE)

  if (is(ArimaData.new, ""try-error"")){
    ArimaData.new &lt;- arimaerror
  } else {
    ArimaData.new &lt;- ArimaData.new
  }

  arimafitted &lt;- try(fitted(ArimaData.new), silent = TRUE)

  if (is(arimafitted, ""try-error"")){
    fitarima &lt;- arimaerror
  } else {
    fitarima &lt;- arimafitted
  }

  if (as.log == ""log""){
    Arimafit[i, ] &lt;- c(exp(fitarima))
    Datanew &lt;- c(exp(Data.new))
  } else {
    if (as.log == ""sqrt""){
      Arimafit[i, ] &lt;- c((fitarima)^2)
      Datanew &lt;- c((Data.new)^2)
    } else {
      Arimafit[i, ] &lt;- c(fitarima)
      Datanew &lt;- c(Data.new)
    }
  }

  data &lt;- c(Datanew)

  arima.fits &lt;- c(Arimafit[i, ])

  fullres &lt;- data - arima.fits

  v &lt;- acf(fullres, plot = FALSE)

  w &lt;- pacf(fullres, plot = FALSE)

  if (v$acf[2]&gt;0.4|v$acf[2]&lt;(-0.4)|v$acf[3]&gt;0.4|v$acf[3]&lt;(-0.4)|v$acf[4]&gt;0.4|v$acf[4]&lt;(-0.4)|v$acf[5]&gt;0.4|v$acf[5]&lt;(-0.4)|v$acf[6]&gt;0.4|v$acf[6]&lt;(-0.4)|v$acf[7]&gt;0.4|v$acf[7]&lt;(-0.4)|w$acf[1]&gt;0.4|w$acf[1]&lt;(-0.4)|w$acf[2]&gt;0.4|w$acf[2]&lt;(-0.4)|w$acf[3]&gt;0.4|w$acf[3]&lt;(-0.4)|w$acf[4]&gt;0.4|w$acf[4]&lt;(-0.4)|w$acf[5]&gt;0.4|w$acf[5]&lt;(-0.4)|w$acf[6]&gt;0.4|w$acf[6]&lt;(-0.4)){
    totb[i] &lt;- ""n""
  } else {
    totb[i] &lt;- sum(abs(w$acf[1:4]))
  }

  j &lt;- match(min(totb), totb)

  order.arima &lt;- a[j, c(1:3)]

  order.seasonal.arima &lt;- a[j, c(4:6)]
}

#----- ARIMA -------------------------------------------------------------------
# Fits an ARIMA model with the orders set
Arima.Data.new &lt;- Arima(Data.new,
                        order    = order.arima,
                        seasonal = list(order=order.seasonal.arima),
                        method   = ""ML"")

#-------------------------------------------------------------------------------
# Forecasts from the ARIMA model

suppressWarnings(forecast.Data.new &lt;- forecast(Arima.Data.new,
                                               h        = ifelse(frequency(Arima.Data.new) &gt; 1, 2 * frequency(Arima.Data.new), 10),
                                               simulate = TRUE,
                                               fan      = TRUE))
</code></pre>
"
"0.171498585142509","0.164398987305357","128730","<p>If i understand correctly, the ARIMA function produces an estimate for the mean of the process instead of the intercept. It is possible to transform the mean into the intercept: mean= 1-Sum(AR-Coefficients). Is it also possible to transform the standard error of the mean into standard error of the intercept? </p>

<p>As an example:</p>

<pre><code>arima(x = example, order = c(2, 0, 2), method = ""ML"")

Coefficients:
          ar1      ar2     ma1     ma2  intercept
      -0.7508  -0.0367  1.3156  0.5433    -0.0183
s.e.   0.2591   0.1772  0.2424  0.1958     0.0244

sigma^2 estimated as 0.03748:  log likelihood = 35.67,  aic = -59.33

# mean:
&gt; 1+0.7508+0.0367 
[1] 1.7875
</code></pre>

<p>Thanks a lot</p>

<p>laterstat     </p>
"
"0.27116307227332","0.259937622455018","130104","<p>I have a weekly number of items sold from 2012 to 2014. 2014 not being complete. So bassically two periods. I am looking at seasonality and stationarity of the response variable (# of items sold)</p>

<p>I have first plotted them against time</p>

<p>![response in time][1]</p>

<p>I then had did stl decomposition in suspicion of trend and seasonality</p>

<p>![stl decomposition][2]</p>

<p>However my data is stationary without any transformation</p>

<p>![enter image description here][3]
![enter image description here][4]</p>

<p>All my testing attests to stationarity of course...
so I did </p>

<pre><code>boxplot(Dealer_Traffic~month, data=total, xlab= ""Month"", ylab = ""Traffic to Dealer"")
</code></pre>

<p>to see differences in mean of sales through month. Month was created using dates I had to create the month a given observation belonged to. </p>

<p>![Boxplot][5]</p>

<p>and difference in mean observed are not significant. WHAT????</p>

<p>![ANOVA][6]</p>

<p>and here is the acf and pacf</p>

<p>![acf and pacf][7]</p>

<p>So could I conclude that my raw data is stationary hence needs no transformation?</p>

<p>Can I conclude that an ARIMA would be suitable? Which ARIMA model would you recommend?</p>

<p>It seems like I cannot have photos here. So basically all my tests reject nonstationarity but I observe seasonality and trend in stl decomposition. No difference between monthly means. I tried 
 pp test Box.test kpss and adf.test for first and pairwise.t.test and aov for means tuckey. </p>

<p>Can I conclude that the series is stationary?</p>
"
"0.242535625036333","0.232495277487639","131041","<p>I have fitted a seasonal ARIMA model using R to a log transformed times series which I called lnseries. </p>

<p>I can forecast fine for the transformed time series (<code>lnseries</code>) storing the ARIMA model (which I called <code>fit</code>) then using the command:</p>

<p>$\texttt{plot(forecast(fit))},$</p>

<p>this shows me the forecast and 95% confidence interval. But I'm stuck on how to get the actual original time series forecast plot using this model.</p>

<p>Help anyone!?</p>
"
"0.365636212063565","0.38554980047803","138108","<p>Using the attached data that has been recently updated I am not able to obtain a statistically significant forecast. The data is extremely seasonal. The data is stored here for easy replication: </p>

<p><a href=""http://ge.tt/1uihVfA2/v/0?c"" rel=""nofollow"">http://ge.tt/1uihVfA2/v/0?c</a></p>

<pre><code># 1. Make a R timeseries out of the rawdata: specify frequency &amp; startdate
gIIP &lt;- ts(Data, frequency=12, start=c(2003,11))
print(gIIP)
plot.ts(gIIP, type=""l"", col=""blue"", ylab=""MTD Ships"", lwd=2,
        main=""Full data"")
grid()
</code></pre>

<p>Using the auto.arima function I don't need to factor a Box-Cox because the auto.arima factors that into selecting the best model. </p>

<p>Upon ""selecting the best model"" I  The best model suggested was Arima(order = c(0, 0, 1),
      seasonal = list(order = c(1, 0, 1), period = 12) with non-zero mean </p>

<pre><code># 5. Perform estimation
library(forecast)
library(zoo)
library(stats)
auto.arima(gIIP, d=NA, D=NA, max.p=12, max.q=12,
           max.P=2, max.Q=2, max.order=12, max.d=2, max.D=2,
           start.p=2, start.q=2, start.P=1, start.Q=1,
           stationary=FALSE, seasonal=TRUE,
           ic=c(""aicc"",""aic"", ""bic""), stepwise=FALSE, trace=TRUE,
           approximation=FALSE | frequency(gIIP)&gt;12), xreg=NULL,
           test=c(""kpss"",""adf"",""pp""), seasonal.test=c(""ocsb"",""ch""),
           allowdrift=TRUE, lambda=TRUE, parallel=FALSE, num.cores=4
</code></pre>

<p>)</p>

<p>then proceed to conduct accuracy diagnostics but unable to obtain any output.</p>

<pre><code>#Check standard error etc of ""fitted"" ARIMA
pos.arima &lt;- function(gIIP, order = c(0, 0, 1),
      seasonal = list(order = c(1, 0, 1), period = 12),
      xreg = NULL, include.drift=TRUE, 
      transform.pars = TRUE,
      fixed = NULL, init = NULL,
      method = c(""CSS-ML"", ""ML"", ""CSS""), 
      optim.method = ""BFGS"",
      optim.control = list(), kappa = 1e6)

acf(pos.arima) 
pacf(pos.arima)
</code></pre>

<p>The following step to conduct an ex ante (out of sample forecast) but also unable to obtain a statistically significant forecast---forecast with lowest standard error rate. I tested this by removing the last 5 observations to test the model. </p>

<pre><code># 7. Forecast Out-Of-Sample ---this used to work
fit &lt;- Arima(gIIP, order = c(0, 0, 1), seasonal = list(order = c(1, 0, 1), period = 12),
             xreg = TRUE, include.mean = TRUE, transform.pars = TRUE, 
             fixed = NULL, init = NULL, method = c(""CSS-ML"", ""ML"", ""CSS""), 
             optim.method = ""BFGS"", optim.control = list(), kappa = 1e6)
plot(forecast(fit,h=9))
print(forecast(fit,h=9))
</code></pre>

<p>Used to obtain output here. Can you please help me diagnose why there ARIMA model is not working like it once did for me? Thank you for your time.  </p>
"
"0.365636212063565","0.38554980047803","145251","<p>Sorry in advance if this is too basic of a question - I've been struggling with this data set for almost a month and feel like I'm going in circles, and the more I Google the more confused I get.</p>

<p>I have a time series of hourly activity levels (mean of 7 persons) for a period of about 2 months (1704 observations). There is obviously a strong ""seasonal"" component (freq=24) to this time series, with activity showing regular fluctuations between night and day. I am ultimately hoping to compare my activity time series to three other time series of environmental variables, to see how weather, temperature, etc affect people's activity on an hourly scale, following the methods in <a href=""http://cid.oxfordjournals.org/content/early/2012/05/21/cid.cis509.full"" rel=""nofollow"">this paper</a>. I'm not planning on doing forecasting, just wanting to know if these explanatory variables are affecting activity, and if so, how.</p>

<p>The paper linked above did their analysis in a few steps, if I understand correctly:</p>

<ol>
<li>Use stl to assess trend and seasonality.</li>
<li>Fit time series to ARIMA model.</li>
<li>Transform data into series of independent, identically distributed random variables</li>
<li>Choose best-fitting model by AIC</li>
<li>Use residuals for cross-correlating variables.</li>
</ol>

<p>Okay. Here are my questions:</p>

<ol>
<li><p>I can do step 1, but don't know how to relate that to step 2. Am I using the remainder from stl analysis for ARIMA modeling? If not, what's the point of step 1?</p></li>
<li><p>I understand how to choose some candidate models for ARIMA based on ACF, PACF, and auto.arima. But I can't get past the diagnostics. My Ljung-Box values are ALWAYS significant for ALL lags. Okay, so that means my residuals are correlated (I think). And since I want to use the residuals for cross-correlation, I assume that's bad. But no matter which models I try (I've tried maybe 6-10, is that enough?) I can't get good Ljung-Box p-values. The best fitting ARIMA so far (by AIC) is (1,0,2)x(1,1,2)24.</p></li>
</ol>

<p>Does this mean my time series doesn't fit an ARIMA model? How can I get iid residuals if I can't even get it to fit a model? Arrrghh.</p>

<p>So to be more succinct, my main question is: why do I always have these significant Ljung-Box values, and what can I do to fit a better model to get iid residuals?</p>

<p>Subsample of data (full set <a href=""https://www.dropbox.com/s/lhd9zu0x8r4o8pe/fitbit%20data.txt?dl=0"" rel=""nofollow"">here</a>):</p>

<pre><code>[1] 24 16 40 48 50 38 24  4  4  5  3  6  4  4  4  3 12 63 55 42 56 20 10 26 45 47 66 64 59
[30] 54 24  5  6  2  4  3  6 10  6  2 13 39 26 17 24 13 19 26 17 32 54 68 58 39 20  0  3  2
[59]  8  2  4  1  5 11  5 60 57 54 40 40 53 74 40 42 57 46 46 26  9  8  4  6 14  8  5  3  2
[88]  7 19 47 53 43 53 51 55 64 48 64 57 56 52 34 22  8  5  6  4  6  3  4  7  6 27 40 48 41
[117] 43 51 50 44 56 64 68 46 49 35 16  2 14  3  7  3 13  3  3  2 14 49 62 42 41 57 52 63 32
[146] 54 59 60 68 24 12  2  2  2  2  7  6  5  9 10 26 53 50 59 28 45 47 44 48 55 59 77 86 33
[175] 18 16 10  6  9  9 14  7  9  7  9 46 57 41 33 32 34 29 39 39 27 26  4 10  9  6  6  2  4
[204]  1  2  2  4  4 17 50 47 24 27 34 26 38 20  6 20 15 25  8  2  2  3  6  4  3  3  4  4  2
[233] 18 41 63 52 37 32 32 28 48 20  6 10  9  7  5 10  4  3  4  7  4  3  4 10  8 56 47 50 27
[262] 30 22 38 38 28 33 24 18 12 14  2 10  4 21  4  5  6  4  4 20 41 46 16  8 20 24 21 16 27
[291] 10  6 14  5  6  6 12  2 10  7  6  2  2  3 16 47 56 43 30 35 32 41 20 20 11 34 16  6 10
[320]  2  5 10  3 11  6  5  7  5 14 50 30 26 19 16 10  5 12 12 22 16 16 10  4  5  4  4  8 14
[349]  4  6  4  5 21 47 28 15  8 12 18 18 16 10  5  8 12  3  6  4  5 12 11  8  2  4  6 10 25
[378] 42 20 15  8 18 10 10  6 18 12  4  7  6  6  4  8 14  3 10 11  5 10  9 26 54 41 36 44  9
[407]  4  5  3  8 12 16 11 12 13 26  5 13 13  1  1  5 18  7 39 64 64 65 44 34 42 63 62 54 26
[436] 30 34 25 15  7  1  0  2  1  0  9 13 10 33 65 59 48 44 60 65 44 55 65 67 76 85 63 48  8
[465]  2  0  3  1  1  1  8 12 19 72 67 42 46 70 54 37 41 66 62 54 80 52 22  3  2  2  1  1  5
[494]  2  2  5 37 48 32 29 27 25 21  2 17  3 24  2  7  1  1  4  7  8  7  4  3  6  2  4 26 28
[523] 15  6  2  4  1 12  4  2  4 14 11  2  5  1 13 16 10  5 14  1  2  3 13 24 29 20 12  8  4
[552]  8  1 11  8 10  6  4  6  1  6  8  4  7 18 17 12  3 18 50 25 27 20 14 14  9 14 14 15  5
[581]  8  3  4  3  3 11 12 12  4 19 25  8 33 53 61 49 50 34 38 45 76 65 72 53 84 65 51 19  4
[610]  2 11  7  5  3  6  3 38 85 83 72 58 77 78 63 73 64 56 22  3 10 13 10  2  1  1  0  8  6
[639]  5  2 34 54 56 54 14  5 17 18 21  3 14 14  6  4  1  2  4 10  7  3  3  4 12 17 54 68 49
[668] 51 38 11 29 17  1  2  4  8  9  6  4  3 14  0  1 10  8  4  3  3 25 31  9  9 10  6  8  9
[697]  4 11  4  6  3  9  0  2  4  1 10 20 11  2  8  4 28 35 40 34 36 19 19 15 23 14  6  4  2
[726]  6  5  4  2  4  4  2  8 13 17  4 44 30 23 22 11  5 10 12  6  8 11  1 12 10  1  2  0  6
[755]  6  3  4  9  1  9 13 41  8  6  9 13 28  7  2  8  7  2  3  6  1  2  5  4  4  4  2  5  9
[784]  9 28 53 40 28  6  8  1  7  2 13 20  7  3  8  4  2  2  6  3  5 16  8  2 14 16 41 20 22
[813]  7  8 10 24 23 24 19 14  5  1  1  2  9  0  6  2 15  8  4  5 26 28  9  9 16 30 11 12  7
</code></pre>

<p>ACF/PACF after taking 24th difference: </p>

<p><img src=""http://i.stack.imgur.com/1SWHy.png"" alt=""ACF/PACF of time series after taking 24th difference""></p>

<p>Diagnostics of SARIMA(1,0,2)x(1,1,2)24 model (best model by AIC and as suggested by auto.arima):</p>

<p><img src=""http://i.stack.imgur.com/Tp70f.png"" alt=""enter image description here""></p>
"
"0.171498585142509","0.164398987305357","148371","<p>I'm trying to fit a sarima model on the univariate data with 180 points (periodicity=12). I use the auto.arima function in R. After fitting a model to the data, the only problem is the violation of the normality assumption. Then, I refit models after transforming the data but the residuals are still non-normal. For transformation of the data, I use both BoxCox.lambda (in forecast package) and boxcoxnc (in AID package) functions. Can anybody help me to fix this problem?</p>

<pre><code>ser=c(1.887090e+04, -6.023007e+00,  1.193635e-02, -1.455856e-05,  1.064251e-08, -4.953592e-12,  1.517229e-15, -3.090332e-19,
4.137144e-23, -3.491891e-27,  1.682794e-31, -3.527046e-36,  1.904962e+04, -7.394189e+00,  1.600849e-02, -2.077511e-05,
1.585519e-08,-7.587987e-12,    2.363570e-15, -4.859251e-19,  6.534816e-23, -5.525202e-27,  2.663420e-31, -5.580438e-36,
2.009098e+04, -1.061082e+01,  2.319182e-02, -2.917768e-05,  2.171827e-08, -1.019917e-11,  3.133564e-15, -6.379905e-19,
8.520995e-23, -7.168462e-27,  3.442102e-31, -7.188143e-36,  2.067028e+04, -8.034999e+00,  1.761326e-02, -2.240562e-05,
1.680919e-08, -7.961614e-12,  2.469832e-15, -5.081494e-19,  6.861040e-23, -5.835236e-27,  2.831898e-31, -5.974519e-36,
2.233604e+04, -1.033148e+01,  2.287039e-02, -2.952031e-05,  2.255568e-08, -1.086351e-11,  3.419260e-15, -7.123005e-19,
9.720229e-23, -8.341734e-27,  4.079166e-31, -8.660882e-36,  2.392045e+04, -8.246481e+00,  1.585412e-02, -2.056180e-05,
1.636424e-08, -8.253437e-12,  2.710813e-15, -5.858824e-19,  8.245204e-23, -7.258003e-27,  3.624039e-31, -7.827743e-36,
2.636514e+04, -9.886355e+00,  1.951992e-02, -2.504930e-05,  1.963158e-08, -9.789139e-12,  3.190186e-15, -6.856046e-19,
9.606813e-23, -8.427664e-27,  4.196799e-31, -9.046539e-36,  2.866210e+04, -8.866902e+00,  1.734494e-02, -2.387617e-05,
1.957175e-08, -9.993900e-12,  3.300201e-15, -7.152619e-19,  1.008517e-22, -8.892694e-27,  4.448060e-31, -9.626143e-36,
3.002254e+04, -1.007403e+01,  2.151203e-02, -2.984675e-05,  2.427803e-08, -1.226036e-11,  3.997630e-15, -8.550747e-19,
1.190499e-22, -1.037815e-26,  5.140218e-31, -1.103334e-35,  2.929311e+04, -1.123255e+01,  2.282206e-02, -2.968240e-05,
2.323868e-08, -1.146069e-11,  3.677709e-15, -7.777557e-19,  1.073806e-22, -9.301478e-27,  4.584147e-31, -9.800725e-36,
3.306894e+04, -1.396117e+01,  2.326777e-02, -2.724425e-05,  2.023428e-08, -9.690231e-12,  3.055811e-15, -6.392630e-19,
8.763020e-23, -7.552202e-27,  3.707622e-31, -7.901994e-36,  3.491666e+04, -1.315883e+01,  2.554492e-02, -3.194439e-05,
2.437661e-08, -1.184053e-11,  3.762542e-15, -7.896499e-19,  1.082565e-22, -9.310722e-27,  4.554895e-31, -9.664092e-36,
3.775600e+04, -2.101521e+01,  4.695457e-02, -6.000206e-05,  4.510264e-08, -2.134088e-11,  6.600784e-15, -1.352465e-18,
1.817468e-22, -1.538166e-26,  7.429410e-31, -1.560507e-35,  3.699341e+04, -1.019327e+01,  1.761360e-02, -2.428662e-05,
2.084200e-08, -1.112473e-11,  3.796505e-15, -8.415154e-19,  1.204392e-22, -1.072641e-26,  5.402195e-31, -1.174885e-35,
4.009280e+04, -1.887174e+01,  3.441926e-02, -4.161190e-05,  3.152055e-08, -1.535050e-11,  4.911316e-15, -1.040003e-18,
1.440215e-22, -1.251900e-26,  6.190925e-31, -1.327693e-35)

require(""forecast"")
fit=auto.arima(ser,d = 0,D = 1,max.p = 6, max.q = 6,max.P = 6, max.Q = 6, max.order = 25,start.p=1, start.q=1, start.P=1, start.Q=1,stationary = FALSE,
seasonal=TRUE,stepwise=TRUE,trace=TRUE,approximation=FALSE,allowdrift=FALSE,ic=""aicc"")
</code></pre>
"
"0.27116307227332","0.259937622455018","151652","<p>I fitted a number of SARIMA models using R and chose the ARIMA(0,0,0)(3,1,0)[12] as the best fitted model to the univariate data with 180 points (periodicity=12). This model is chosen as the best model according to the criteria of lowest MAPE among other fitted 624 models.</p>

<p>The residuals of the model violates the assumption of independently distributed residuals (and same for the 2nd best, 3rd best model etc.). Actually the residuals are also non-normally distributed; however the model is fitted with the method of conditional sum of squares in order to bypass the violation of normality assumption. </p>

<p>In the data, the most of the values are close to zero and this does not allow any data transformation. </p>

<p>The data represent the evolution of coefficents of a 11th degree polynomial equation (in total 15 equations representing different years of electricity load duration curves). The purpose is to forecast the coefficients of e.g. the 16th equation and so the corresponding load duration curve.</p>

<p>Can anybody sugggest/provide any solutions to this case?</p>

<pre><code>x=c(1.887090e+04, -6.023007e+00,  1.193635e-02, -1.455856e-05,  1.064251e-08, -4.953592e-12,  1.517229e-15, -3.090332e-19,
4.137144e-23, -3.491891e-27,  1.682794e-31, -3.527046e-36,  1.904962e+04, -7.394189e+00,  1.600849e-02, -2.077511e-05,
1.585519e-08,-7.587987e-12,    2.363570e-15, -4.859251e-19,  6.534816e-23, -5.525202e-27,  2.663420e-31, -5.580438e-36,
2.009098e+04, -1.061082e+01,  2.319182e-02, -2.917768e-05,  2.171827e-08, -1.019917e-11,  3.133564e-15, -6.379905e-19,
8.520995e-23, -7.168462e-27,  3.442102e-31, -7.188143e-36,  2.067028e+04, -8.034999e+00,  1.761326e-02, -2.240562e-05,
1.680919e-08, -7.961614e-12,  2.469832e-15, -5.081494e-19,  6.861040e-23, -5.835236e-27,  2.831898e-31, -5.974519e-36,
2.233604e+04, -1.033148e+01,  2.287039e-02, -2.952031e-05,  2.255568e-08, -1.086351e-11,  3.419260e-15, -7.123005e-19,
9.720229e-23, -8.341734e-27,  4.079166e-31, -8.660882e-36,  2.392045e+04, -8.246481e+00,  1.585412e-02, -2.056180e-05,
1.636424e-08, -8.253437e-12,  2.710813e-15, -5.858824e-19,  8.245204e-23, -7.258003e-27,  3.624039e-31, -7.827743e-36,
2.636514e+04, -9.886355e+00,  1.951992e-02, -2.504930e-05,  1.963158e-08, -9.789139e-12,  3.190186e-15, -6.856046e-19,
9.606813e-23, -8.427664e-27,  4.196799e-31, -9.046539e-36,  2.866210e+04, -8.866902e+00,  1.734494e-02, -2.387617e-05,
1.957175e-08, -9.993900e-12,  3.300201e-15, -7.152619e-19,  1.008517e-22, -8.892694e-27,  4.448060e-31, -9.626143e-36,
3.002254e+04, -1.007403e+01,  2.151203e-02, -2.984675e-05,  2.427803e-08, -1.226036e-11,  3.997630e-15, -8.550747e-19,
1.190499e-22, -1.037815e-26,  5.140218e-31, -1.103334e-35,  2.929311e+04, -1.123255e+01,  2.282206e-02, -2.968240e-05,
2.323868e-08, -1.146069e-11,  3.677709e-15, -7.777557e-19,  1.073806e-22, -9.301478e-27,  4.584147e-31, -9.800725e-36,
3.306894e+04, -1.396117e+01,  2.326777e-02, -2.724425e-05,  2.023428e-08, -9.690231e-12,  3.055811e-15, -6.392630e-19,
8.763020e-23, -7.552202e-27,  3.707622e-31, -7.901994e-36,  3.491666e+04, -1.315883e+01,  2.554492e-02, -3.194439e-05,
2.437661e-08, -1.184053e-11,  3.762542e-15, -7.896499e-19,  1.082565e-22, -9.310722e-27,  4.554895e-31, -9.664092e-36,
3.775600e+04, -2.101521e+01,  4.695457e-02, -6.000206e-05,  4.510264e-08, -2.134088e-11,  6.600784e-15, -1.352465e-18,
1.817468e-22, -1.538166e-26,  7.429410e-31, -1.560507e-35,  3.699341e+04, -1.019327e+01,  1.761360e-02, -2.428662e-05,
2.084200e-08, -1.112473e-11,  3.796505e-15, -8.415154e-19,  1.204392e-22, -1.072641e-26,  5.402195e-31, -1.174885e-35,
4.009280e+04, -1.887174e+01,  3.441926e-02, -4.161190e-05,  3.152055e-08, -1.535050e-11,  4.911316e-15, -1.040003e-18,
1.440215e-22, -1.251900e-26,  6.190925e-31, -1.327693e-35)

fit=arima(x, order = c(0, 0, 0),seasonal = list(order = c(3, 1, 0), period =12),method=c(""CSS""))

par(mfrow=c(1,2)) 
x1&lt;-acf(fit$residuals,180,ylab=""Sample ACF"",main ="""",xaxt=""n"")
axis(1, at=seq(0, 15, by=2), labels = TRUE)
abline(v=(seq(0,15,1)), col=""black"", lty=""dotted"")

x2&lt;-pacf(fit$residuals,180,ylab=""Sample PACF"",main ="""",xaxt=""n"")
axis(1, at=seq(0, 15, by=2), labels = TRUE)
abline(v=(seq(0,15,by=1)), col=""black"", lty=""dotted"")
</code></pre>
"
"0.557831937583566","0.557988665970332","151657","<p>I am running X-13 SEATS on r for monthly data in six years of observations and I think I got a (sufficiently) reasonable fit for the ARIMA model, but the output also shows me that my original series does not have significant seasonality, as it follows:</p>

<pre><code> Call:
seas(x = data_r[, 1], transform.function = ""log"", regression.aictest = NULL, 
    outlier = NULL, arima.model = ""(0 1 1)(1 1 0)"")

Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
AR-Seasonal-12     -0.6194     0.1110  -5.581 2.39e-08 ***
MA-Nonseasonal-01   0.6220     0.1093   5.690 1.27e-08 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

SEATS adj.  ARIMA: (0 1 1)(1 1 0)  Obs.: 60  Transform: log
AICc: 773.4, BIC: 778.4  QS (no seasonality in final):    0  
Box-Ljung (no autocorr.): 20.04   Shapiro (normality): 0.9754
    &gt; 
                qs p-val
    qsori        0     1
    qsorievadj   0     1
    qsrsd        0     1
    qssadj       0     1
    qssadjevadj  0     1
    qsirr        0     1
    qsirrevadj   0     1
</code></pre>

<p>(Still, there is also the fact that the irregular component seems to dominate the SI ratio for some specific months in some years. So maybe there is some dummy variable in the pre-adjustment that I am missing (right?)) </p>

<p>But when I run a regression on Stata for yearly and monthly dummies on the original series -- assuming the seasonality is deterministic --, I cannot reject with an F test that they are all equal to zero. What does this show me? That my ARIMA fit is not correct?</p>

<p>Also, if someone could point me out the difference in interpretation that you should have when running a regression on seasonal dummies and deseasonalizing data with a X-13 SEATS, it would be also very helpful. Maybe that is what I am missing here.</p>

<p>Edit: is it by any chance a common practice, in some particular situations (when you are deseasonalizing a set of series), still deseasonalize a given series even if that series does not show significant seasonality?</p>

<p>Edit2: Adding the results of the automatic adjustment:</p>

<pre><code>Coefficients:
                   Estimate Std. Error z value Pr(&gt;|z|)    
Constant            59.1761    38.0551   1.555  0.11994    
Easter[15]        -903.6151   341.1891  -2.648  0.00809 ** 
MA-Nonseasonal-01    0.4974     0.1138   4.370 1.24e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

SEATS adj.  ARIMA: (0 1 1)  Obs.: 60  Transform: none
AICc: 925.6, BIC: 933.2  QS (no seasonality in final):    0  
Box-Ljung (no autocorr.):  21.9   Shapiro (normality): 0.9498 *

            qs p-val
qsori        0     1
qsorievadj   0     1
qsrsd        0     1
qssadj       0     1
qssadjevadj  0     1
qsirr        0     1
qsirrevadj   0     1 
</code></pre>

<p>I also, I get the following error for the monthplot function with the automatic adjustment: </p>

<pre><code>Error in `[.default`(x$data, , ""seasonal"") : subscript out of bounds
</code></pre>

<p>Following this result from the automatic adjustment, the use of the dummy for easter, with the original specification, does not change that much the first output:</p>

<pre><code>Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
Easter[15]        -0.08307    0.02690  -3.088  0.00202 ** 
AR-Seasonal-12    -0.63353    0.10816  -5.858  4.7e-09 ***
MA-Nonseasonal-01  0.50391    0.12075   4.173  3.0e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

SEATS adj.  ARIMA: (0 1 1)(1 1 0)  Obs.: 60  Transform: log
AICc: 767.9, BIC: 774.3  QS (no seasonality in final):    0  
Box-Ljung (no autocorr.): 29.37   Shapiro (normality): 0.9721  
            qs p-val
qsori        0     1
qsorievadj   0     1
qsrsd        0     1
qssadj       0     1
qssadjevadj  0     1
qsirr        0     1
qsirrevadj   0     1
</code></pre>

<p>Most recent observation: Now I Think I am fairly sure that there is no significant seasonality in this series, but I would be thankful if someone could show me other problems that I might not be considering. Still, I would like a possible canonical/scholarly answer on why I can reject the null hypothesis for the whole set of seasonal dummies being zero (though I had a small result for the F test with my data, ~4, but I still reject the null) and still get a reasonable ARIMA fit with which I cannot reject no seasonality in my original data. Does that have something to do with the difference of the adjustment with ARIMA models and deterministic seasonality? An intuitive answer on this difference would be of some help.</p>
"
"0.121267812518166","0","154639","<p>I present here two examples one with transformed data and the other without any transformation. In the transformed data case, the upper interval gets enormous large, whereas not in the untransformed case. (Function in forecast package in R)</p>

<pre><code>    dmnd=c(8.6,9.8,11.2,12.4,13.5,15.7,18.6,21.1,22.3,23.6,24.6,26.3,28.3,29.6,33.3,36.4,40.5,44.9,48.4,52.6,56.8,60.5,67.2,73.4,77.8,85.6,94.8,105.5,114.0,118.5,128.3,126.9,132.6,141.2,150.0,160.8,174.6,190.0,198.1,194.1,210.4,230.3,242.4,246.4,255.5)
    #with transformation
    fit &lt;- Arima(dmnd[11:45], order=c(1,2,0), lambda=-0.25)
    prg=forecast(fit,h=16,level=c(95),fan=FALSE,lambda=-0.25)
    prg



Point Forecast     Lo 95      Hi 95
36       262.8665 241.34010   286.8475
37       271.4097 231.18466   320.7765
38       279.9162 216.75679   367.8499
39       288.9224 201.26616   429.9376
40       298.2242 184.91488   513.2452
41       307.9328 168.56079   626.1724
42       318.0273 152.59291   782.3793
43       328.5442 137.35984  1003.7047
44       339.4971 123.06285  1326.8115
45       350.9112 109.82630  1815.8213
46       362.8087  97.70297  2589.1252
47       375.2149  86.69722  3879.8347
48       388.1558  76.77777  6185.0627
49       401.6596  67.89002 10677.4213
50       415.7557  59.96484 20513.6022
51       430.4756  52.92545 45881.0816

#without transformation
fit &lt;- Arima(dmnd[11:45], order=c(1,2,0))
prg=forecast(fit,h=16,level=c(95),fan=FALSE)
prg

 36       263.5024 252.64559 274.3592
 37       271.7410 249.52972 293.9523
 38       279.9288 243.87448 315.9832
 39       288.1275 236.23983 340.0153
 40       296.3239 226.81344 365.8344
 41       304.5208 215.76687 393.2747
 42       312.7176 203.22448 422.2107
 43       320.9144 189.28707 452.5417
 44       329.1112 174.03721 484.1851
 45       337.3079 157.54444 517.0715
 46       345.5047 139.86827 551.1412
 47       353.7015 121.06045 586.3426
 48       361.8983 101.16647 622.6302
 49       370.0951  80.22673 659.9635
 50       378.2919  58.27746 698.3063
 51       386.4887  35.35135 737.6260
</code></pre>
"
"0.369970036697972","0.419136822142455","154641","<p>This question is similar to the following <a href=""http://stats.stackexchange.com/questions/32634/difference-time-series-before-arima-or-within-arima"">question</a> in the sense I am currently doing the differencing and mean removal of the time series outside the <code>Arima</code> function in R. And I do not know how to do these steps within <code>Arima</code> function in R. The reason is that I am trying to perform the following procedure (data <code>dowj_ts</code> can be found at the bottom): </p>

<pre><code>dowj_ts_d1 &lt;- diff(dowj_ts) # differencing at lag 1 (1-B)
drift &lt;- mean(diff(dowj_ts))
dowj_ts_d1_demeaned &lt;- dowj_ts_d1 - mean(dowj_ts_d1) # mean removal
# Maximum Likelihood AR(1) for the mean-corrected differences X_t
fit &lt;- Arima(dowj_ts_d1_demeaned, order=c(1,0,0),include.mean=F, transform.pars = T)
</code></pre>

<p>Note that the <code>drift</code> is actually <code>0.1336364</code>. And <code>summary(fit)</code> gives the table below:</p>

<pre><code>Series: dowj_ts_d1_demeaned 
ARIMA(1,0,0) with zero mean     

Coefficients:
         ar1
      0.4471
s.e.  0.1051

sigma^2 estimated as 0.1455:  log likelihood=-35.16
AIC=74.32   AICc=74.48   BIC=79.01

Training set error measures:
                       ME     RMSE       MAE       MPE     MAPE      MASE
Training set -0.004721362 0.381457 0.2982851 -9.337089 209.6878 0.8477813
                    ACF1
Training set -0.04852626
</code></pre>

<p>Ultimately, I want to predict 2-step ahead forecast of <strong>the original series</strong>, and this starts to become ugly: </p>

<pre><code> tail(c(dowj_ts[1], dowj_ts[1] + cumsum(c(dowj_ts_d1_demeaned,forecast.Arima(fit,h=2)$mean) + drift)),2)
</code></pre>

<p>And currently these are all done outside the <code>Arima</code> function from the <code>forecast</code> package. I know I can do differencing within Arima like this: </p>

<pre><code> Arima(dowj_ts, order=c(1,1,0),include.drift=T,transform.pars = F)
</code></pre>

<p>This gives:</p>

<pre><code>Series: dowj_ts 
ARIMA(1,1,0) with drift         

Coefficients:
         ar1   drift
      0.4478  0.1204
s.e.  0.1059  0.0786

sigma^2 estimated as 0.1474:  log likelihood=-34.69
AIC=75.38   AICc=75.71   BIC=82.41
</code></pre>

<p>But the drift term computed by R is different from the <code>drift = 0.1336364</code> that I computed manually.</p>

<p>So <strong>my question is: how can I differenced the series and then remove the mean of the differenced series within the Arima function ?</strong></p>

<p><strong>Second question:</strong> Why is the drift term estimated by <code>Arima</code> different from the drift term I computed ? In fact, what does the <strong>mathematical model</strong> look like when <code>include.drift = T</code> ? This really confuses me. </p>

<p>Data can be found below: </p>

<pre><code>structure(c(110.94, 110.69, 110.43, 110.56, 110.75, 110.84, 110.46, 
110.56, 110.46, 110.05, 109.6, 109.31, 109.31, 109.25, 109.02, 
108.54, 108.77, 109.02, 109.44, 109.38, 109.53, 109.89, 110.56, 
110.56, 110.72, 111.23, 111.48, 111.58, 111.9, 112.19, 112.06, 
111.96, 111.68, 111.36, 111.42, 112, 112.22, 112.7, 113.15, 114.36, 
114.65, 115.06, 115.86, 116.4, 116.44, 116.88, 118.07, 118.51, 
119.28, 119.79, 119.7, 119.28, 119.66, 120.14, 120.97, 121.13, 
121.55, 121.96, 122.26, 123.79, 124.11, 124.14, 123.37, 123.02, 
122.86, 123.02, 123.11, 123.05, 123.05, 122.83, 123.18, 122.67, 
122.73, 122.86, 122.67, 122.09, 122, 121.23), .Tsp = c(1, 78, 
1), class = ""ts"")
</code></pre>
"
"NaN","NaN","165182","<p>According to <a href=""https://stat.ethz.ch/R-manual/R-devel/library/stats/html/arima.html"" rel=""nofollow"">the manual</a> the <code>arima</code> function in R doesn't transform parameters when optimizing using <code>CSS</code>. Is there a particular (statistical) reason for this?  </p>
"
"0.30012252399939","0.287698227784375","169299","<p>I use auto.arima function in R to fit a TS model to a annual data composed of electricity demand. The series is transformed w.r.t Box-Cox lambda due to the prevailing heteroscedasticity and then it is twiced differenced to eliminate the trend in the data. The first ACF/PACF plot (w/o transformation) suggest that an ARIMA model should be fitted to the model; whereas the second ACF/PACF (with transformation) plot suggests that an AR model should be fitted. However both of them depend on the same data. 
In both of the case, the auto.arima function selects the best model as ARIMA(1,2,1) which can be expected according to the first plot but not according to the second plot due to the one spike in PACF.     </p>

<p><a href=""http://i.stack.imgur.com/qoYJH.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qoYJH.jpg"" alt=""untransformed""></a>
<a href=""http://i.stack.imgur.com/FOzNv.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FOzNv.jpg"" alt=""transformed""></a></p>

<pre><code>dmnd=c(8.6,9.8,11.2,12.4,13.5,15.7,18.6,21.1,22.3,23.6,24.6,26.3,28.3,29.6,33.3,36.4,40.5,44.9,48.4,52.6,56.8,60.5,67.2,73.4,77.8,85.6,94.8,105.5,
</code></pre>

<p>114.0,118.5,128.3,126.9,132.6,141.2,150.0,160.8,174.6,190.0,198.1,194.1,210.4,230.3,242.4,246.4,257.2)</p>

<pre><code>x=ts(dmnd,frequency=1)

sdx=diff(x,differences = 2)
x1&lt;-acf(sdx,length(sdx),ylab=""Sample ACF"",main ="""")
x2&lt;-pacf(sdx,length(sdx),ylab=""Sample PACF"",main ="""")

library(FitAR)

#transformation
fit=arima(x,order=c(0,2,0))
BoxCox(fit, interval = c(-1, 1), type = ""BoxCox"")

library(forecast)
tx=BoxCox(x, -0.049)

sdx=diff(tx,differences = 2)
x1&lt;-acf(sdx,length(sdx),ylab=""Sample ACF"",main ="""")
x2&lt;-pacf(sdx,length(sdx),ylab=""Sample PACF"",main ="""")


fit=auto.arima(x,d = 2,D = 0,start.p=0, start.q=0, max.p=5, max.q=5,stationary=FALSE,seasonal=FALSE,stepwise=TRUE,trace=TRUE,approximation=FALSE,allowdrift=TRUE,ic=""aicc"",lambda=-0.049)

par(mfrow=c(1,2)) 
x1&lt;-acf(fit$residuals,length(fit$residuals),ylab=""Sample ACF"",main ="""")
x2&lt;-pacf(fit$residuals,length(fit$residuals),ylab=""Sample PACF"",main ="""")

Box.test(fit$residuals, lag = length(fit$residuals)/5, type = c(""Ljung-Box""), fitdf = length(fit$ coef))

shapiro.test(fit$residuals)

library(TSA)
x.standard=rstandard.Arima(fit)
qqnorm(x.standard,main ="""")
qqline(x.standard)
</code></pre>

<p><strong><em>Results of ARIMA(3,1,0) Model</em></strong> </p>

<p><a href=""http://i.stack.imgur.com/MmgeP.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/MmgeP.jpg"" alt=""Summary""></a></p>

<p><a href=""http://i.stack.imgur.com/ojFY1.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ojFY1.jpg"" alt=""sample ACF/PACf""></a>
<a href=""http://i.stack.imgur.com/Gh94D.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Gh94D.jpg"" alt=""Normality""></a></p>

<pre><code>Shapiro-Wilk normality test

data:  fit$residuals
W = 0.8557, p-value = 5.153e-05

Box-Ljung test

data:  fit$residuals
X-squared = 14.2044, df = 6, p-value = 0.02743
</code></pre>

<p><strong><em>Diagnostics of Residuals ARIMA(1,1,1) transformed</em></strong>
<a href=""http://i.stack.imgur.com/lQGXi.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/lQGXi.jpg"" alt=""ACF/PACF""></a>
<a href=""http://i.stack.imgur.com/twQf2.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/twQf2.jpg"" alt=""Normal""></a></p>

<p><strong><em>auto.arima result for the series without the observations ""32, 40, 41""</em></strong>
<a href=""http://i.stack.imgur.com/9XI2O.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9XI2O.jpg"" alt=""without some observation""></a></p>
"
"0.121267812518166","0.116247638743819","177117","<p>I use auto.arima function to model the below provided time series data. At the end of the analysis, the best model is given as ARIMA(1,2,1). The log- likelihood=93.69 is positive which is unusual. It is clear for me that the log-likehood is not as same as the probability. But how can this originate from the analysis? Does it depend on the data? Or just due to sign convention? </p>

<pre><code>  srs=c(8.6,9.8,11.2,12.4,13.5,15.7,18.6,21.1,22.3,23.6,24.6,26.3,28.3,29.6,33.3,36.4,40.5,44.9,48.4,52.6,56.8,60.5,67.2,73.4,77.8,85.6,94.8,105.5,
114.0,118.5,128.3,126.9,132.6,141.2,150.0,160.8,174.6,190.0,198.1,194.1,210.4,230.3,242.4,246.4,257.2)

x=ts(srs,frequency=1)

fit=auto.arima(x,d = 2,D = 0,start.p=0, start.q=0, max.p=5, max.q=5,stationary=FALSE,seasonal=FALSE,stepwise=TRUE,trace=TRUE,approximation=FALSE,allowdrift=TRUE,ic=""aicc"",lambda=-0.049)

library(forecast)
tx=BoxCox(x, -0.049)
</code></pre>

<h1>result of transformation</h1>

<pre><code> tx=(2.042209,2.159383,2.278396,2.368590,2.443563,2.575967,2.723460,2.832405, 2.879977, 2.928574, 2.964082,3.021106, 3.083437, 3.121522, 3.221002, 3.295802, 3.385064, 3.470876, 3.533058, 3.601728, 3.664871, 3.716566,3.802248, 3.873901, 3.921001, 3.998007, 4.079888, 4.165227, 4.226782, 4.257449, 4.320209, 4.311558, 4.346176,4.395557, 4.442923, 4.497221, 4.561284, 4.626783, 4.659033, 4.643283, 4.705451, 4.774832, 4.814009, 4.826510,4.859228)
</code></pre>

<h1>result of density function given observation</h1>

<pre><code>dnorm(tx ,mean(tx), sd(tx),log=TRUE)
Time Series:
Start = 1 
End = 45 
Frequency = 1 
[1] -2.7168796 -2.4462962 -2.1917838 -2.0125389 -1.8724951 -1.6450191 -1.4214597 -1.2765217 -1.2186144
[10] -1.1628381 -1.1242424 -1.0660746 -1.0078703 -0.9750711 -0.8992890 -0.8517306 -0.8055615 -0.7720362
[19] -0.7543945 -0.7414067 -0.7354801 -0.7349191 -0.7424969 -0.7569828 -0.7705473 -0.7996330 -0.8399630
[28] -0.8923108 -0.9366056 -0.9607175 -1.0143004 -1.0065755 -1.0381350 -1.0861521 -1.1355218 -1.1961059
[37] -1.2730668 -1.3578864 -1.4019282 -1.3802323 -1.4679575 -1.5724597 -1.6345414 -1.6548185 -1.7089570
</code></pre>
"
"NaN","NaN","198494","<p>I am using arimax model to calculate incremental sales due to promotions. Arima code is as below</p>

<pre><code>**out &lt;- try(Arima(unlist(final_data$total_sales), order=c(1,1,0),seasonal=c(0,0,0), xreg=reg_inp, include.mean=TRUE,include.drift=TRUE,transform.pars=TRUE,fixed=NULL, init=NULL, method=""ML"",kappa=1e6))**  
</code></pre>

<p>Co-Efficients of the arima produces negative values for some predictors and I use these coefficients to calculate incremental sales</p>

<p><a href=""http://i.stack.imgur.com/Ak93N.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Ak93N.jpg"" alt=""Co_efficients""></a> </p>

<p>So my question is an is there any way to restrict my coefficients to give me only positive numbers?</p>
"
"0.297044262893002","0.28474739872575","202319","<p>I have daily sales data for a department store for the past 850 days. I have indicators on the major holidays and the days leading up to the major holidays. The number of days before the holidays that are included was chosen by AIC. The issue I'm having is that there are outliers throughout the data that I'm not sure how to handle. Or, at least that's what I think is happening since I don't seem to get accurate forecasts. I'm using a CV to calculate the MAPE of forecasts two weeks out, using the first 450 days as the initial training set and the rest to see how well the model forecasts the data.</p>

<p>I've used tso() from the tsoutliers package and tsoutliers from the forecast package to find outliers. They both give different results.</p>

<pre><code>tsoutliers(data$Sales)

$index
[1] 230 270 271 328 635

$replacements
[1] 2222.160 2088.573 2231.577 1812.380 2138.655

train = 454
trainingdata = data$Sales[1:train]
trainingdata = ts(trainingdata,frequency = 7)
tso(trainingdata,types = c(""AO"", ""LS"", ""TC""))

Series: trainingdata 
ARIMA(2,1,1)(2,0,0)[7]                    

Coefficients:
     ar1     ar2      ma1    sar1    sar2      AO52      TC68       TC80      AO86
  0.2872  0.1331  -0.9717  0.3567  0.4607  885.2061  890.3690  -863.4296  836.8638
s.e.  0.0508  0.0480   0.0107  0.0436  0.0429  169.2521  163.4243   166.0282  169.8535
     AO111     AO121      TC229     AO259      TC270     AO328     AO416
  754.1791  691.0849  1236.8523  711.3954  1790.0292  764.9712  920.1783
s.e.  169.2042  167.7273   163.1458  167.9835   163.9663  170.0103  168.9235

sigma^2 estimated as 44080:  log likelihood=-3064.92
AIC=6152.24   AICc=6153.65   BIC=6222.21

Outliers:
type ind  time coefhat  tstat
1    AO  52  8:03   885.2  5.230
2    TC  68 10:05   890.4  5.448
3    TC  80 12:03  -863.4 -5.200
4    AO  86 13:02   836.9  4.927
5    AO 111 16:06   754.2  4.457
6    AO 121 18:02   691.1  4.120
7    TC 229 33:05  1236.9  7.581
8    AO 259 37:07   711.4  4.235
9    TC 270 39:04  1790.0 10.917
10   AO 328 47:06   765.0  4.500
11   AO 416 60:03   920.2  5.447
</code></pre>

<p>Running BoxCox on the data it recommends a transform of the data</p>

<pre><code>lambda &lt;- BoxCox.lambda(data$Sales)
trainingdata = BoxCox(trainingdata,lambda)
tso(trainingdata,types = c(""AO"", ""LS"", ""TC""))
Series: trainingdata 
ARIMA(3,1,1)(2,0,0)[7]                    

Coefficients:
     ar1     ar2      ar3      ma1    sar1    sar2      LS3    AO52     AO53    TC68
  0.3918  0.0993  -0.0587  -0.9856  0.3632  0.4144  13.5805  5.7218  -7.7957  6.3960
s.e.  0.0383  0.0418   0.0416   0.0142  0.0361  0.0341   1.3201  1.2980   1.3041  1.2763
      AO80   AO121   TC229   TC270   AO416     AO445   TC634   AO780
  -23.3707  5.5352  5.8088  7.0446  7.9304  -23.6372  5.5475  6.7194
s.e.    1.2376  1.2307  1.2594  1.2640  1.2476    1.2393  1.2598  1.2353

sigma^2 estimated as 2.332:  log likelihood=-1482.63
AIC=3003.26   AICc=3004.23   BIC=3092.34

Outliers:
type ind   time coefhat   tstat
1    LS   3   1:03  13.581  10.287
2    AO  52   8:03   5.722   4.408
3    AO  53   8:04  -7.796  -5.978
4    TC  68  10:05   6.396   5.012
5    AO  80  12:03 -23.371 -18.883
6    AO 121  18:02   5.535   4.498
7    TC 229  33:05   5.809   4.612
8    TC 270  39:04   7.045   5.573
9    AO 416  60:03   7.930   6.356
10   AO 445  64:04 -23.637 -19.073
11   TC 634  91:04   5.547   4.404
12   AO 780 112:03   6.719   5.439
</code></pre>

<p>Some of these outliers are already taken care of since they're the holidays. I'm not sure how to handle the rest of the outliers when fitting the model and in the CV.</p>

<p>What is the best way to go about taking care of the outliers? I can reset the values of the training data where it's predicted as an outliers to the recommended value if it's not a holiday for fitting the model and then still calculate the MAPE off of the original data. However, there's a LS at index 3 so I'm not sure that would make sense for that.</p>
"
"0","0.116247638743819","203422","<p>I'm trying to forecast an auto.arima model like the code below.  The time series I'm forecasting needs to have a transformation applied to it.  I was wondering if I also need to apply the same transformation to the xreg predictors, like I have done in the example below?</p>

<p>Code:</p>

<pre><code>series&lt;-BoxCox(OldSeries,0.27)
Train &lt;-series[1:700]
Validation &lt;- series[701:1000]


Xreg&lt;-BoxCox(series2[1:1000],0.27)

##Predictor
xregTrain &lt;- Xreg[1:700]
xregVal &lt;- Xreg[701:1000]


fit &lt;- auto.arima(Train, xreg = xregTrain)

Fcast&lt;-forecast(fit, h=300, xreg = xregVal)
</code></pre>
"
"0.3638034375545","0.348742916231458","213192","<p>I have half-hourly electricity data of several homes for a duration of one month. Also, I have ambient temperature at same sampling rate. Now, I need to make half-hourly forecasts using historical electricity data and forecasted temperature for coming day.</p>

<p>Currently, I am facing a problem with seasonality, i.e., in some homes I observe (via visual inspection) day-wise seasonality and in some other homes no pattern is found. <a href=""http://stats.stackexchange.com/a/212797/60072"">Stephan</a> suggested to check for weekly seasonality as well, but visually I do not find any. So, I thought to try models with different forced seasonalities to find the prediction accuracy. I can think of two options:</p>

<ol>
<li>Model_1 with daily seasonality (frequency = 48 observations)</li>
<li>Model_2 with weekly and daily seasonality (48, 7*48)</li>
</ol>

<p>Keeping the above approach in mind, I am facing the following issue:</p>

<ol>
<li>For Model_1, I can use <code>auto.arima()</code> with <code>xreg</code> option to specify an extra <code>temperature</code> predictor. However, this only works for a single seasonality.</li>
<li>For Model_2, I tried to use the <code>tbats()</code> forecasting function, which models multiple seasonalities, but does not allow extra predictor variables. </li>
</ol>

<p>Is there a forecasting function which allows <em>both</em> multiple predictors and multiple seasonalities?</p>

<p>Here is the electricity data of one month. </p>

<pre><code>    data &lt;- structure(c(1642.8, 1467.1, 165.57, 1630.99, 1618.65, 1629.29, 
        1598.93, 1839.9, 1604.52, 1606.73, 1473.82, 1669.17, 1698.9, 
        2111.21, 2056.41, 3671.29, 2808.01, 1336.15, 794.11, 1212.15, 
        377.36, 888.54, 174.58, 218.54, 420.76, 389.58, 397.77, 395.31, 
        359.11, 364.8, 376.13, 389.37, 929.5, 1702.38, 519.65, 2452.28, 
        1354.45, 1842.96, 725.41, 661.11, 528.44, 733.4, 429.51, 310.47, 
        279.72, 407.83, 1791.1, 1754.53, 1536.73, 1608.37, 1432.23, 1401.72, 
        1582.14, 1558.75, 1536.24, 1745.59, 1375.61, 1556.71, 1671.12, 
        1206.77, 1391.84, 876.23, 1617.3, 1638.99, 1833.61, 1591.42, 
        1455, 183.87, 177.55, 184.36, 332.99, 352.95, 425.1, 945.67, 
        342.3, 348.45, 227.18, 382.15, 268.91, 335.88, 326.94, 233.23, 
        169.71, 179.51, 195.3, 207.23, 1681.9, 1493.32, 941.52, 980.36, 
        924.31, 379.02, 1229.89, 1590.21, 1250.92, 1149.24, 1124.04, 
        993.78, 883.98, 860.69, 934.17, 969.31, 1049.55, 1104.94, 904.3, 
        1220.23, 1183.9, 891.26, 825.33, 787.77, 1060.93, 1029.1, 982.25, 
        193.13, 182.65, 181.75, 167.68, 165.89, 291.02, 300.2, 418.4, 
        297.66, 231.89, 305.66, 701.18, 338.5, 337.24, 332.11, 332.66, 
        187.8, 179.03, 130.22, 177.73, 172.24, 173.45, 334.23, 810.53, 
        359.41, 330.23, 333.29, 568.85, 2462.46, 1660.32, 1156.13, 1136.2, 
        1189.07, 832.73, 181.91, 185.93, 1076.77, 672.78, 1376.71, 1020.17, 
        382.18, 1160.74, 791.36, 1569.4, 817.78, 850.71, 747.21, 826.12, 
        1306.46, 506.23, 140.05, 132.6, 304.19, 308.14, 406.13, 290.73, 
        188.9, 165.59, 174.56, 145.97, 151.83, 142.29, 443.58, 799.9, 
        279.36, 223.88, 221.03, 291.26, 374.97, 431.36, 598.98, 625.82, 
        1052.02, 2036.83, 1230.03, 1429.81, 1099.34, 1646.03, 1668.56, 
        1631.79, 1604.04, 2849.49, 2998.63, 2476.96, 1601.04, 1216.42, 
        2004.2, 1868.51, 1961.91, 1813.35, 1500.22, 1276.94, 1369.29, 
        632.43, 238.15, 488.76, 467.6, 330.27, 144.67, 153.36, 924.12, 
        1348.18, 799.01, 524.3, 420.5, 264.34, 283.86, 198.95, 206.52, 
        217.33, 356.16, 207.83, 197.6, 194.03, 193.01, 249.85, 271.22, 
        244.25, 442.5, 660.49, 245.66, 356.13, 443.32, 336.15, 849.74, 
        1709.21, 1542.09, 1315.69, 2628.6, 2261.8, 1576.42, 1776.48, 
        1239.64, 1401.12, 1106.17, 1378.55, 1315.57, 1141, 1642.63, 2484.13, 
        1968.94, 3059.42, 1317.32, 905.05, 484.42, 486.86, 96.79, 183.45, 
        173.94, 342, 255.96, 320.54, 106.19, 147.88, 150.77, 176.17, 
        344.75, 371.73, 309.46, 237.86, 187.32, 202.61, 292.5, 248.28, 
        259.3, 283.67, 365.09, 230.47, 326.15, 350.92, 335.42, 419.39, 
        345.31, 1093.22, 1392.87, 1298.11, 919.16, 1654.53, 1045.99, 
        558.42, 437.29, 857.9, 758.34, 1220.04, 1390.62, 956.74, 909.93, 
        584.67, 409.87, 387.3, 387.93, 1276.28, 871.06, 413.8, 313.2, 
        199.5, 330.21, 210.9, 358.28, 352.13, 233.62, 259.18, 123.57, 
        255.58, 411.78, 427.65, 318.95, 298.5, 283.23, 279.85, 200.45, 
        205.97, 254.24, 307.98, 1090.53, 289.71, 215.14, 286.63, 328.55, 
        288.96, 1281.19, 1354.8, 1302.05, 1254.46, 261.95, 270.09, 243.28, 
        696.61, 314.27, 241.73, 245.68, 157.74, 222.55, 294.36, 185.46, 
        203.49, 182.14, 246.69, 178.26, 397.5, 330.2, 212.02, 248.72, 
        265.48, 249.37, 130.59, 248.97, 279.94, 319.07, 358.5, 278.98, 
        251.92, 304.66, 455.05, 365.95, 340.93, 287.51, 264.82, 260.18, 
        34.35, 35.11, 184.09, 247.18, 160.9, 139.27, 284.96, 296.31, 
        252.3, 342.65, 353.03, 380.52, 346.19, 350.06, 218.52, 133.94, 
        173.7, 128.26, 167.8, 112.77, 147.8, 129, 170.54, 89.88, 243.08, 
        97.61, 190.31, 193.94, 268.17, 233.5, 205.27, 92.29, 167.43, 
        168.34, 151.99, 193.84, 379.1, 318.69, 327.28, 487.39, 414.01, 
        336.06, 278.02, 168.05, 155.6, 236.4, 264.94, 296.05, 326.46, 
        357.43, 356.31, 340.29, 319.81, 312.79, 341.53, 317.36, 309.62, 
        440.6, 285.5, 282.06, 288.99, 334.48, 196.54, 144.24, 218.55, 
        173.64, 242.29, 251.78, 186.81, 184.36, 141.62, 208.91, 157.53, 
        154.03, 139.44, 137.66, 256.75, 1202.05, 177.36, 177.93, 72.83, 
        252.9, 231.35, 1090.39, 442.91, 363.12, 248.96, 478.75, 249.64, 
        297.29, 227.28, 365.82, 879.7, 488.93, 184.79, 138.13, 151.77, 
        123.18, 175.76, 251.84, 208.06, 126.68, 246.3, 307.34, 319.79, 
        324.3, 379.6, 309.53, 253.17, 221.91, 228.42, 150.24, 148.59, 
        118.79, 86.89, 140.51, 200.43, 212.15, 276.14, 441.81, 125.77, 
        152.42, 329.28, 269.21, 177.35, 1106.29, 128.92, 96.35, 63.53, 
        520.62, 940.25, 1014.34, 314.99, 390.2, 330.1, 377.04, 341.35, 
        342.79, 241.79, 249.9, 391.92, 292.68, 105.02, 179.99, 118.53, 
        154.17, 90.53, 206.7, 345.33, 244.75, 291.68, 820.57, 1777.84, 
        1805.83, 1753.73, 1416.7, 279.2, 262.82, 1345.88, 467.98, 1136.66, 
        170.02, 159.96, 1478.8, 1414.12, 1347.93, 1505.59, 1341.69, 445.53, 
        277.59, 1609.61, 1476.45, 244.08, 192.57, 213.55, 439.02, 112.86, 
        128.54, 376.09, 251.15, 116.27, 254.82, 302.56, 304.6, 198.5, 
        240.05, 219.35, 70.3, 190.96, 211.95, 328.53, 714.62, 3176.56, 
        2604.09, 191.65, 145.25, 93.93, 83.6, 81.13, 146.12, 331.75, 
        250.24, 1144.53, 1616.47, 1008.7, 316.65, 311.46, 1152.99, 1504.86, 
        1543.21, 1081.54, 1428.07, 1358.23, 1349.75, 190.23, 2398.92, 
        2196.11, 1466.94, 2249.77, 2150.2, 2542.25, 618.03, 453.22, 880.99, 
        1497.86, 440.96, 161.85, 324.88, 434.11, 316.33, 444.66, 359.14, 
        277.41, 1237.28, 761.41, 183.53, 309.44, 213.48, 121.64, 346.7, 
        149.86, 2060.39, 1102.13, 347.97, 600.24, 912.6, 590.77, 1805.76, 
        1673.93, 1573.91, 505.74, 446.76, 1033.41, 1668.68, 1293.9, 383.81, 
        1419.99, 1349.4, 711.55, 218.63, 182, 401.93, 1876, 1486.34, 
        1543.11, 2313.8, 478.57, 615.19, 542.68, 971.98, 531.11, 766.21, 
        489.76, 344.47, 319.86, 321.26, 311.41, 288.67, 310.67, 305.15, 
        419.33, 422.84, 950.08, 2188.88, 3454.92, 1989.54, 590.33, 327.05, 
        354.78, 578.41, 1583.29, 2016.66, 1481.03, 293.21, 1864.84, 399.65, 
        366.76, 357.7, 2074.97, 1626.86, 1133, 1624.61, 1506.93, 628.4, 
        1405.68, 217.8, 1223.11, 1356.97, 1171.72, 1182.86, 1642.11, 
        2289.02, 814.39, 595.76, 542.78, 1596.41, 884.97, 235.25, 1540.68, 
        781.95, 115.71, 1204.98, 718.66, 452.09, 305.58, 444.67, 356.76, 
        182.54, 674.47, 153.8, 862.25, 1322.88, 323.33, 1659.64, 496.72, 
        304.74, 246.6, 327.12, 239.31, 246.72, 225.72, 234.7, 324.07, 
        304.27, 171.86, 97.64, 242.69, 295, 324.53, 513.81, 1100.65, 
        1151.77, 231.56, 189.88, 786.3, 1164.87, 676.09, 882.82, 1496.3, 
        1027.91, 872.92, 809.1, 840.31, 1302.18, 2055.87, 677.74, 934.66, 
        263.91, 186.68, 248.5, 214.62, 371.54, 298, 294.52, 304.86, 1295.77, 
        942.5, 305, 265.78, 255.89, 255.63, 151.54, 108.16, 116.81, 100.19, 
        224.75, 84.11, 1143.89, 262.15, 784.21, 1728.29, 1506.79, 434.94, 
        374.29, 265.43, 560.74, 1651.49, 1063.07, 1054.69, 1298.4, 1261.59, 
        1132.75, 692.82, 660.57, 198.25, 97.81, 1258.67, 833.64, 796.35, 
        868.76, 999.86, 2240.02, 885.72, 1317.52, 1267.18, 167.93, 133.22, 
        364.44, 267.17, 406.13, 412.52, 1036.04, 779.34, 655.43, 1901.2, 
        270.18, 266.31, 284.21, 288.66, 135.38, 176.11, 154.86, 160.21, 
        146.28, 163.72, 139.75, 278.12, 253.51, 319.62, 396.39, 1662.69, 
        1577.09, 1059.71, 241.64, 407.54, 290.49, 846.17, 1325.31, 1418.23, 
        1432.5, 1412.6, 1015.87, 1619.88, 1426.58, 1333.32, 1963.35, 
        1638.11, 1081.89, 285.56, 1084.58, 2038.77, 1022.39, 1145.92, 
        513.87, 107.7, 176.19, 143.77, 374.3, 373.99, 221.76, 148.3, 
        331.01, 2323.12, 1502.09, 347.17, 296.7, 306.06, 313.03, 221.69, 
        295.35, 301.95, 250.92, 231.54, 140.6, 717.63, 863.5, 402.15, 
        1337.78, 1575.44, 1738.49, 1675.57, 1617.66, 1365.58, 242.8, 
        286.29, 712.34, 1559.59, 1600.34, 3447.88, 3432.89, 3337.23, 
        1472.21, 1323.76, 1265.31, 1221.65, 1312.63, 2016.09, 2972.13, 
        1451.67, 735.67, 130.13, 379.87, 162.21, 226.48, 417.18, 357.51, 
        346.37, 200.89, 190.15, 276.05, 942.74, 1471.99, 1047.53, 1240.06, 
        742.62, 169.34, 144.28, 220.58, 165.23, 344.8, 227.47, 254.64, 
        742.37, 1809.01, 436.11, 1692.87, 1697.74, 1474.91, 1635.68, 
        1664.2, 1489.85, 1316.4, 1364.07, 1510.02, 1497.89, 1709.17, 
        2846.71, 2736.08, 1015.43, 1017.08, 1195.21, 751.18, 523.89, 
        199.69, 2148.71, 1151.39, 1182.84, 788.91, 259.31, 146.29, 141.09, 
        299.38, 349.53, 404.08, 449.9, 391.77, 251.89, 222.25, 281.04, 
        565.4, 371.24, 219.75, 324.45, 227, 157.67, 212.48, 201.69, 140.84, 
        220.67, 187.64, 399.79, 157.92, 275.49, 326.99, 1340.51, 1578.68, 
        1599.41, 1667.74, 1129.07, 1312.55, 1393.09, 1368.69, 1130.85, 
        968.71, 1130.2, 1223.1, 1124.5, 1077.09, 1052.42, 1255.31, 918.21, 
        1263.93, 706.21, 3080.29, 1620.18, 1122.48, 750.06, 262.89, 110.02, 
        236.83, 413, 227.53, 355, 277.51, 258.03, 368.44, 656.68, 1808.17, 
        1493.41, 1272.21, 330.67, 1674.18, 719.45, 1089.68, 784.01, 275.73, 
        312.69, 345.11, 761.8, 1020.11, 259.16, 345.98, 270.23, 580.15, 
        1303.68, 1659.7, 1732.94, 204.9, 373.58, 373.36, 1381.52, 1437.74, 
        1262.78, 1264.6, 1184.54, 1175.12, 857.09, 1428.34, 841.92, 232.47, 
        223.22, 473.24, 382.7, 189.84, 1737.48, 1689.34, 378.48, 872.56, 
        180.12, 363.03, 301.38, 412.57, 401.17, 387.35, 417.63, 300.61, 
        376.82, 284.31, 232.31, 269.96, 188.41, 203.79, 134.88, 193.66, 
        57.86, 89.71, 167.66, 60.84, 197.03, 703.66, 1638.7, 1467.03, 
        347.22, 1397.23, 1511.92, 1362.25, 1397.18, 1106.19, 826.5, 1033.04, 
        1039.46, 584.98, 706.35, 548.07, 373.39, 681.6, 1231.28, 288.94, 
        649.36, 79.28, 209.23, 290.75, 304.12, 132.57, 91.2, 355.37, 
        197.45, 343.17, 339.2, 284.65, 229, 234.73, 322.36, 323.43, 295.1, 
        197.03, 308.17, 223.88, 235.08, 225.16, 172.83, 236.26, 135.17, 
        394.89, 479.2, 315.34, 280.9, 282.36, 204.78, 367.24, 1712.29, 
        1521, 1686.09, 960.19, 1019.12, 1062.77, 851.88, 1369.98, 689.2, 
        580.5, 751.74, 547.67, 556.64, 493.85, 404.15, 428.07, 716.2, 
        1442.05, 1045.81, 1497.07, 567.59, 155.07, 537.72, 446.03, 282.57, 
        642.88, 409.37, 338.91, 173.89, 358.63, 195.42, 209.95, 186.44, 
        152.34, 105.51, 132.26, 82.14, 122.88, 149.38, 211.42, 350.17, 
        429.72, 336.4, 982.21, 1436.25, 1726.87, 1830.42, 1282.05, 1293.4, 
        1121.01, 946.2, 707.1, 154.53, 767.56, 607.78, 448, 288.23, 270.32, 
        223.93, 166.22, 262.45, 223.74, 159.31, 210.44, 257.94, 183.99, 
        151.38, 206.11, 193.43, 388.95, 577.98, 304.64, 285.13, 256.59, 
        420.26, 289.34, 356.02, 358.08, 325.22, 275.95, 164.46, 213.23, 
        142.99, 221.66, 270.61, 206.56, 213.68, 254.33, 250.15, 267.99, 
        403.95, 671.2, 1574.11, 396.34, 477.88, 631.08, 618.25, 1366.88, 
        298.19, 287.05, 290.38, 332.44, 235.9, 229.79, 831.6, 1320.86, 
        477.31, 944.84, 547.33, 411.21, 705.43, 873.49, 572.81, 585.36, 
        1229.69, 701.01, 653.49, 74.81, 162.47, 179.54, 330.27, 544.51, 
        332.41, 296.66, 130.66, 1055.61, 556.79, 265.43, 383.44, 398.22, 
        362.66, 223.99, 130.35, 193.67, 217.68, 273.3, 247.84, 161.66, 
        320.08, 322.52, 274.61, 811.44, 353.85, 323.41, 383.61, 389.5
        ), .Dim = c(1248L, 1L), .Dimnames = list(NULL, ""power""))
</code></pre>

<p>Temperature predictor values are:</p>

<pre><code>temp&lt;-structure(c(31, 31, 31, 31, 30, 29, 28, 27.5, 27, 26, 26, 26, 
26, 26, 26, 28, 29, 29, 30, 31, 32, 33, 33, 34, 34, 35, 36, 36, 
36.5, 37, 38, 38, 39, 39, 39, 38, 37, 36, 36, 35, 34, 33, 33, 
33, 32.5, 32, 32, 31, 30, 26, 24, 24, 24, 24, 24, 24, 25, 25, 
25, 25, 26, 26, 26, 27, 28.3, 29.7, 31, 32, 33, 33, 34, 34, 34, 
35, 35, 35, 36, 37, 38, 39, 39, 39, 39, 38.5, 38, 37, 36, 35, 
34, 34, 33, 32.5, 32, 30, 30, 30, 28, 28, 27, 25, 25, 24, 24, 
24, 24, 24, 25, 25, 25, 25, 25, 27, 28, 30, 31, 33, 34, 37, 37, 
38, 38, 39, 39.3, 39.7, 40, 40, 40, 40, 41, 40, 40, 38, 38, 36, 
35, 34, 33, 31, 31, 30, 31, 30, 30, 29, 28, 28, 28, 27, 27, 28, 
27, 24, 24, 24, 23, 23, 23, 24, 24, 26, 27, 28, 29.7, 31.3, 33, 
34, 35.3, 36.7, 38, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 
39, 38, 37, 36, 34, 33, 32, 32, 30, 28.5, 27, 27, 27, 27, 27, 
26, 26, 26, 25.3, 24.7, 24, 24, 24, 24, 23, 23, 24.5, 26, 28, 
29.7, 31.3, 33, 34.5, 36, 37, 38, 39, 40, 40, 40.3, 40.7, 41, 
41, 41, 41, 41, 41, 41, 39.5, 38, 37, 37, 35, 34, 34, 33, 33, 
33, 32, 31, 31, 30, 30, 30, 29, 29, 28.5, 28, 28, 28, 29, 29, 
28.5, 28, 29, 29, 30, 31, 31, 32.5, 34, 35, 37, 39.5, 42, 42, 
42, 42, 42, 43, 44, 45, 45, 44, 43, 42, 42, 41, 39, 36.5, 34, 
34, 33, 33, 33, 33, 33, 33, 32, 32, 32, 32, 31.5, 31, 30, 30, 
30, 29, 29, 28, 27, 28, 29, 30, 31, 32.5, 34, 34, 35, 37, 40, 
41, 41, 42, 42, 42.5, 43, 43, 43, 42, 42, 43, 42, 42, 41, 40, 
39, 37.5, 36, 35, 35, 34, 34, 33, 32, 32, 32, 32, 31, 31, 31, 
31, 31, 28, 28, 28, 28, 28, 28.5, 29, 30, 30.5, 31, 32, 33.5, 
35, 36, 37, 38, 39, 38, 39, 40, 41, 42, 42, 43, 42, 42, 42, 41, 
41, 40, 39, 38, 37, 36, 35, 34, 34, 33, 32, 32, 31, 30.5, 30, 
30, 30, 30, 29, 29, 29, 28, 27, 27, 27, 27, 28.5, 30, 31.5, 33, 
34, 35, 36, 37, 38, 38, 39, 40, 41, 42, 42, 42, 42, 42, 42, 42, 
42, 42, 42, 42, 40, 40, 39, 37, 37, 36.5, 36, 35, 34, 34, 33, 
33, 32, 32, 32, 32, 32, 32, 31, 31, 30, 30, 30, 30, 31, 31, 31, 
32, 33.5, 35, 35.5, 36, 37, 37, 38, 39, 40, 41, 41, 42, 42, 42, 
43, 43, 42, 41, 41, 40, 38, 36, 35, 34, 34, 35, 36, 36, 35, 35, 
33, 33, 32, 31, 31, 30, 30, 29, 29, 29, 29, 29, 31, 31.5, 32, 
31, 30, 31, 32, 33, 34, 34, 35, 36, 36, 37, 37, 38, 38, 38, 39, 
39, 39, 39, 39, 39, 39, 38, 37, 37, 37, 37, 35, 33, 31, 31, 31, 
30, 30, 30, 30, 30, 30, 30, 30, 29, 29, 28, 28, 27, 27, 26, 29, 
29, 30, 31, 32, 32, 33, 33, 33, 34, 34, 35, 35, 37, 37, 37, 37, 
37, 38, 38, 38, 38, 37, 37, 36, 35, 34.5, 34, 34, 33, 33, 32, 
32, 32, 32, 32, 31, 31, 31, 31, 31, 31, 31, 30, 30, 29.5, 29, 
29, 30, 30, 31, 32, 32, 33, 33.5, 34, 34, 34, 30.7, 27.3, 24, 
24, 24, 24.7, 25.3, 26, 27, 28, 29, 29, 29, 28, 28, 27, 27, 26, 
25, 25, 25, 24.5, 24, 24, 24, 24, 24, 24, 24, 24, 23, 23, 23, 
23, 23, 23, 24, 24, 24, 25, 25, 26, 28, 29, 30, 31, 31, 32, 33, 
34, 35, 35, 36, 37, 37, 37, 37, 37, 37, 37, 35, 34, 33, 33, 33, 
32, 31.7, 31.3, 31, 31, 31, 31, 30, 30, 30, 30, 28, 28, 27, 27, 
26, 26, 25, 25, 25, 25, 25, 26, 27, 28, 29.7, 31.3, 33, 34, 34, 
35, 36, 37, 36, 36, 35, 35, 34, 32, 32, 33, 31.3, 29.7, 28, 28, 
29, 29, 29, 29, 28, 27.5, 27, 26.5, 26, 26, 26, 26, 26, 26, 25, 
25, 25, 25, 25, 24, 24, 23, 23, 23, 24.5, 26, 27, 28, 29, 30, 
31, 32, 32, 32, 33, 34, 35, 36, 36, 36, 36, 36, 36, 36, 37, 37, 
37, 36, 36, 34, 34, 33, 32, 32, 31, 31, 30, 28, 28, 28, 28, 27, 
27, 27, 27, 27, 27, 27, 27, 26.5, 26, 26, 26, 26, 27, 28.8, 30.5, 
32.2, 34, 34, 34, 35, 36, 37, 38, 38, 38, 39, 40, 39, 39, 39, 
40, 40, 40, 39, 39, 38, 37, 35, 35, 34, 34, 34, 33, 32, 32, 32, 
32, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 33, 34, 
35, 37, 37, 37.3, 37.7, 38, 39, 40, 43, 43, 43, 44, 44, 43, 43, 
43, 43, 43, 43, 42, 41, 40, 39, 38, 37, 36, 35, 35, 34, 34, 34, 
34, 34, 33, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 34, 
35, 36, 37, 38, 38, 40, 40.5, 41, 42, 42, 42, 42, 41, 40, 40, 
39, 39, 37, 32, 27, 26, 26, 27, 28, 27, 27, 27, 26, 25, 25, 25, 
25, 25, 25, 25, 25, 25, 25, 25, 24, 23, 23, 22.5, 22, 22, 23, 
24, 25, 26.3, 27.7, 29, 30, 32, 33, 34, 36, 37, 37, 39, 40, 40.5, 
41, 42, 42, 42, 43, 43, 42, 41.5, 41, 39, 38, 35, 35, 35, 34, 
33, 33, 33, 33, 32, 32, 31, 31, 30, 30, 30, 29, 29, 28, 28, 28, 
28.5, 29, 29, 29, 30, 32, 32, 34, 35, 37, 38, 39, 40, 41, 42, 
42, 43, 44, 44, 44, 45, 45, 45, 44, 43, 43, 42, 40, 38.5, 37, 
35, 35, 34, 34, 33, 34, 33, 33, 33, 33, 32, 32, 31, 32, 31, 30, 
29, 29, 29, 29, 30, 30, 31, 32.7, 34.3, 36, 37, 38, 39, 40, 41, 
41, 42, 42, 43, 44, 44, 44, 44, 44, 43.5, 43, 43, 42, 42, 40, 
38, 38, 37, 36, 36, 36, 36, 35, 35, 34, 34, 34, 34, 32, 32, 31.5, 
31, 31, 30, 30, 30, 30, 31, 33, 34, 35, 37, 38, 39, 40, 40, 42, 
42, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 43, 42, 40, 40, 
40, 39, 38, 38, 38, 37, 37, 36, 36, 35.5, 35, 34.5, 34, 33, 33, 
32, 32, 31, 30, 30, 30, 31, 32, 33.5, 35, 36.5, 38, 38, 39, 40, 
41, 41, 41, 42, 42, 42, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 
42, 40.5, 39, 38, 38, 37, 37, 37, 36, 36, 36, 36, 36, 36, 36, 
35, 35, 34, 34, 34, 32, 32, 31, 31, 33, 34, 35, 35, 36, 38, 39, 
40, 41, 42, 42.5, 43, 43, 44, 44, 45, 45, 45, 45, 45, 45, 45, 
44, 43.5, 43, 43, 42, 41, 40, 40, 39, 39, 38, 38, 37, 36, 36, 
35, 34, 34, 34, 32, 32, 32, 32, 32, 32, 32, 34, 35, 35, 37, 38, 
38, 39, 39, 40, 41, 42, 43, 44, 44, 44, 45, 45, 46, 46, 46, 45, 
45, 45, 44, 42, 40, 38, 37, 37, 37, 37, 37, 36, 35), .Dim = c(1248L, 
1L), .Dimnames = list(NULL, ""temperature""))
</code></pre>

<p>Forecasted temperature values are:</p>

<pre><code>forecast_temp &lt;- structure(c(35, 34, 34, 33, 33, 30, 29.7, 29.3, 29, 28, 28, 27, 
27.3, 27.7, 28, 29, 30.5, 32, 33.5, 35, 36, 36, 37, 39, 39.5, 
40, 40, 41, 42, 42, 43, 43, 43, 44, 44, 44, 43, 42, 40, 39, 39, 
38, 37, 37, 36, 35, 35, 34), .Dim = c(48L, 1L), .Dimnames = list(
    NULL, ""temperature""))
</code></pre>

<h3><em>UPDATE-1</em></h3>

<p>From @Stephan's suggestion, I followed the approach mentioned by Prof. Rob at <a href=""http://robjhyndman.com/hyndsight/dailydata/"" rel=""nofollow"">link1</a>, <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">link2</a> as</p>

<pre><code>library(forecast)
tsob &lt;- ts(train_power,frequency = 48) #training electriciy data at daily frequency
tsob_weekly &lt;- fourier(ts(train_power,frequency = 7*48),K=3) #training data at weekly frequency
tempob &lt;- ts(train_temperature,frequency = 48) #Temperature, another predictor variable
fit &lt;- auto.arima(tsob,xreg=cbind(tsob_weekly,tempob),seasonal=FALSE)
tempob_forecast &lt;- ts(test_temperature,frequency = 48) #forecasted temperauture values
forecast_val &lt;- forecast(fit,xreg=tempob_forecast,h=48*5) #forecast for coming 5 days
</code></pre>

<p>As evident from the code, I have used Fourier transformation suggested at above links to show weekly seasonal affect. Up to <code>auto.arima()</code>, it works properly but at forecast function an error is thrown as:</p>

<pre><code>Error in forecast.Arima(fit, xreg = tempob_forecast, h = 48 * 5) : 
  Number of regressors does not match fitted model
</code></pre>

<p>The error is clear, i.e., I do not provide the forecasted regressor values of <code>tsob_weekly</code>, which I don't have. How should I handle this issue? Prof. Rob has used the same value of regressor for both the <code>auto.arima</code> and <code>forecast</code> functions.</p>
"
"0.121267812518166","0","214969","<p>I refer to this question <a href=""http://stats.stackexchange.com/questions/87726/how-to-get-the-true-mean-forecast-using-the-arima-package-with-a-box-cox-transfo"">How to get the true mean forecast using the Arima package with a Box-Cox transformation</a></p>

<p>Could anyone please tell me why is the variance of the Box-Cox transformed data given by this formula:</p>

<pre><code>fvar &lt;- ((BoxCox(fc$upper,fit$lambda)BoxCox(fc$lower,fit$lambda))/qnorm(0.975)/2)^2
</code></pre>

<p>Any hints or references?
I understand what the BoxCox() function does, and also qnorm(), but where is this formula from? Thanks!</p>
"
"0.121267812518166","0.116247638743819","217474","<p>Currently I'm using the ARIMA provided in R, the training series is a seasonal time series, with some values close to zero in each period, and I find that when the training series have a descending trend, then in the result of the forecast, there will be some negative values.</p>

<p>But the time series should be positive on every timestamps, is there a way to add constraints in ARIMA so as to prevent forecasting negative values?</p>

<p>If adding constraints is not possible, is there a way to transform the predicted results so as to make them all positive and still capture the tendency?</p>
"
"0","0.116247638743819","220973","<p><a href=""http://i.stack.imgur.com/27CVA.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/27CVA.png"" alt=""enter image description here""></a>I'm using the R function <code>auto.arima</code> to fit an arima model for a time series, 
the result is an ARIMA(2,1,1). After that I apply the <code>forecast</code> function to predict some futur values. My question is Should I do the transformation (""un-differentiate"" the predicted values) or is it done by <code>forecast</code> automatically ? 
edit : here is what i get when i execute the code : </p>

<pre><code>arimaf = auto.arima(timeseries)
pred = forecast(arimaf, h = 10)
plot(pred, main = ""PREDICTION USING ARIMA(2,1,1)"")
</code></pre>
"
"0.320844473959874","0.307562342614623","221411","<p>I currently have hourly electricity demand data last for 5 years, where I used:  </p>

<pre><code>demand &lt;- msts(mydata$DEMAND,seasonal.period=c(24,182.5*24,365*24),start=2012)
</code></pre>

<p>The plot of <code>stl</code> shows the data have a clear decreasing trend and seasonality, the data is also not normally distributed. </p>

<p><a href=""http://i.stack.imgur.com/SG417.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/SG417.png"" alt=""Stl plot of original time series""></a></p>

<p>I tried:</p>

<ol>
<li>take seasonal difference, then take first difference</li>
<li>log transform</li>
<li>Box-Cox transform </li>
</ol>

<p>All of them do not work, I still have a time series with seasonality and trend. (Do you know how to deal with this?). e.g. the plot of 40 days data after seasonal and first difference: 
<a href=""http://i.stack.imgur.com/kh3YA.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/kh3YA.png"" alt=""the plot of 40 days data after seasonal and first difference:""></a></p>

<p>Then I use following code to fit the above three times series.</p>

<pre><code>fit &lt;- auto.arima(demand, seasonal=FALSE, xreg=fourier(demand, K=4))
</code></pre>

<p>I get ACF plot with clearly seasonal, and significant PACF plot until lag>200.</p>

<p>I also tried: </p>

<pre><code>fit &lt;- tbats(demand)
</code></pre>

<p>No improvement in residuals.</p>

<p>Can any one help me with this? Many thanks.</p>
"
"0.121267812518166","0.116247638743819","224176","<p>If my series requires a log-transformation to stabilize variability, do I apply the <code>sarima</code> function to the log-transformed series or the original series? Does the same apply to the <code>auto.arima</code> function?</p>
"
"0.121267812518166","0.116247638743819","228576","<p>I carried out a log transformation on some data about patient admission in the hospital, aiming to generate an additive model. I went ahead to forecast the same data using: </p>

<pre><code>MYFORECAST = forecast.Arima(auto.arima(admission, d=3, D=NA, stationary=FALSE, 
                            seasonal=FALSE, ic=""aic"", trace=TRUE, allowdrift=FALSE, 
                            allowmean=TRUE)
</code></pre>

<p>in R. I got the forecasts but I was left wondering:  Should report the forecasts or take exponent of the values? </p>
"
