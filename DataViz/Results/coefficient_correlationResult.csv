"V1","V2","V3","V4"
"0.145782423805283","0.134687005940295","  5087","<p>There are numerous procedures for functional data clustering based on orthonormal basis functions. I have a series of models built with the GAMM models, using the <code>gamm()</code> from the mgcv package in R. For fitting a long-term trend, I use a thin plate regression spline. Next to that, I introduce a CAR1 model in the random component to correct for autocorrelation. For more info, see eg the paper of Simon Wood on <a href=""http://r.789695.n4.nabble.com/attachment/2063352/0/tprs.pdf"">thin plate regression splines</a> or his <a href=""http://rads.stackoverflow.com/amzn/click/1584884746"">book on GAM models</a>.</p>

<p>Now I'm a bit puzzled in how I get the correct coefficients out of the models. And I'm even less confident that the coefficients I can extract, are the ones I should use to cluster different models. </p>

<p>A simple example, using:</p>

<pre><code>#runnable code
require(mgcv)
require(nlme)
library(RLRsim)
library(RColorBrewer)

x1 &lt;- 1:1000
x2 &lt;- runif(1000,10,500)

fx1 &lt;- -4*sin(x1/50)
fx2 &lt;- -10*(x2)^(1/4)
y &lt;- 60+ fx1 + fx2 + rnorm(1000,0,5)

test &lt;- gamm(y~s(x1)+s(x2))
# end runnable code
</code></pre>

<p>Then I can construct the original basis using smoothCon :</p>

<pre><code>#runnable code
um &lt;- smoothCon(s(x1),data=data.frame(x1=x1),
         knots=NULL,absorb.cons=FALSE)
#end runnable code
</code></pre>

<p>Now,when I look at the basis functions I can extract using </p>

<pre><code># runnable code
X &lt;- extract.lmeDesign(test$lme)$X
Z &lt;- extract.lmeDesign(test$lme)$Z

op &lt;- par(mfrow=c(2,5),mar=c(4,4,1,1))
plot(x1,X[,1],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,X[,2],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,8],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,7],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,6],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,5],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,4],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,3],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,2],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,1],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
par(op)
# end runnable code
</code></pre>

<p>they look already quite different. I can get the final coefficients used to build the smoother by</p>

<pre><code>#runnable code
Fcoef &lt;- test$lme$coef$fixed
Rcoef &lt;- unlist(test$lme$coef$random)
#end runnable code
</code></pre>

<p>but I'm far from sure these are the coefficients I look for. I fear I can't just use those coefficients as data in a clustering procedure. I would really like to know which coefficients are used to transform the basis functions from the ones I get with <code>smoothCon()</code> to the ones I extract from the lme-part of the gamm-object. And if possible, where I can find them. I've read the related articles, but somehow I fail to figure it out myself. All help is appreciated.</p>
"
"0.0839181358296689","0.0839921051131616","  5270","<p>I have some data which, after lots of searching, I concluded would probably benefit from using a linear mixed effects model. I think I have an interesting result here, but I am having a little trouble figuring out how to interpret all of the results. This is what I get from the summary() function in R:</p>

<pre><code>&gt; summary(nonzero.lmer)
Linear mixed model fit by REML 
Formula: relative.sents.A ~ relative.sents.B + (1 | id.A) + (1 | abstract) 
   Data: nonzero 
    AIC    BIC logLik deviance REMLdev
 -698.8 -683.9  354.4   -722.6  -708.8
Random effects:
 Groups   Name        Variance   Std.Dev. 
 id.A     (Intercept) 1.0790e-04 0.0103877
 abstract (Intercept) 3.0966e-05 0.0055647
 Residual             2.9675e-04 0.0172263
Number of obs: 146, groups: id.A, 97; abstract, 52

Fixed effects:
                 Estimate Std. Error t value
(Intercept)      0.017260   0.003046   5.667
relative.sents.B 0.428808   0.080050   5.357

Correlation of Fixed Effects:
            (Intr)
rltv.snts.B -0.742
</code></pre>

<p>My question involves the relationship between the dependent variable (""relative.sents.A"") and ""relative.sents.B"" once the random factors are factored out. I gather that the t-value of 5.357 for relative.sents.B should be significant.</p>

<p>But does this show what the direction of the effect is? I am thinking that because the coefficient for the slope is positive that this means that as relative.sents.B increases, so does my dependent variable. Is this correct?</p>

<p>The book I've been using briefly mentions that the correlation reported here is not a normal correlation, but goes into no details. Normally, I'd look there to figure out the direction and magnitude of the effect. Is that wrong?</p>

<p>If I'm wrong on both counts, then what is a good (hopefully reasonably straightforward) way to discover the direction and size of the effect?</p>
"
"0.139162484826546","0.139285149006224","  7357","<p>I know this is a fairly specific <code>R</code> question, but I may be thinking about proportion variance explained, $R^2$, incorrectly. Here goes.</p>

<p>I'm trying to use the <code>R</code> package <code>randomForest</code>. I have some training data and testing data. When I fit a random forest model, the <code>randomForest</code> function allows you to input new testing data to test. It then tells you the percentage of variance explained in this new data. When I look at this, I get one number.</p>

<p>When I use the <code>predict()</code> function to predict the outcome value of the testing data based on the model fit from the training data, and I take the squared correlation coefficient between these values and the <em>actual</em> outcome values for the testing data, I get a different number. <em>These values don't match up</em>. </p>

<p>Here's some <code>R</code> code to demonstrate the problem.</p>

<pre><code># use the built in iris data
data(iris)

#load the randomForest library
library(randomForest)

# split the data into training and testing sets
index &lt;- 1:nrow(iris)
trainindex &lt;- sample(index, trunc(length(index)/2))
trainset &lt;- iris[trainindex, ]
testset &lt;- iris[-trainindex, ]

# fit a model to the training set (column 1, Sepal.Length, will be the outcome)
set.seed(42)
model &lt;- randomForest(x=trainset[ ,-1],y=trainset[ ,1])

# predict values for the testing set (the first column is the outcome, leave it out)
predicted &lt;- predict(model, testset[ ,-1])

# what's the squared correlation coefficient between predicted and actual values?
cor(predicted, testset[, 1])^2

# now, refit the model using built-in x.test and y.test
set.seed(42)
randomForest(x=trainset[ ,-1], y=trainset[ ,1], xtest=testset[ ,-1], ytest=testset[ ,1])
</code></pre>

<p>Thanks for any help you might be willing to lend.</p>
"
"0.10277830647413","0.102868899974728"," 11384","<p>I want to apply a PCA on a dataset, which consists of mixed type variables (continuous and binary). To illustrate the procedure, I paste a minimal reproducible example in R below.</p>

<pre><code># Generate synthetic dataset
set.seed(12345)
n &lt;- 100
x1 &lt;- rnorm(n)
x2 &lt;- runif(n, -2, 2)
x3 &lt;- x1 + x2 + rnorm(n)
x4 &lt;- rbinom(n, 1, 0.5)
x5 &lt;- rbinom(n, 1, 0.6)
data &lt;- data.frame(x1, x2, x3, x4, x5)

# Correlation matrix with appropriate coefficients
# Pearson product-moment: 2 continuous variables
# Point-biserial: 1 continuous and 1 binary variable
# Phi: 2 binary variables
# For testing purposes use hetcor function
library(polycor)
C &lt;- as.matrix(hetcor(data=data))

# Run PCA
pca &lt;- princomp(covmat=C)
L &lt;- loadings(pca)
</code></pre>

<p>Now, I wonder how to calculate component scores (i.e., raw variables weighted by component loadings). When dataset consists of continuous variables, component scores are simply obtained by multiplying (scaled) raw data and eigenvectors stored in loading matrix (L in the example above). Any pointers would be greatly appreciated.</p>
"
"0.173001668965327","0.173154160549773"," 11498","<p>I'm fitting a linear model where the response is a function both of time and of static covariates (i.e. ones that are independent of time). The ultimate goal is to identify significant effects of the static covariates.</p>

<p>Is this the best general strategy for selecting variables (in R, using the <code>nlme</code> package)? Anything I can do better?</p>

<ol>
<li>Break the data up by groups and plot it against time. For continuous covariates, bin it and plot the data in each bin against time. Use the group-specific trends to make an initial guess at what time terms to include-- time, time^n, sin(2*pi*time)+cos(2*pi*time), log(time), exp(time), etc.</li>
<li>Add one term at a time, comparing each model to its predecessor, never adding a higher order in the absence of lower order terms. Sin and cos are never added separately. <strong><em>Is it acceptable to pass over a term that significantly improves the fit of the model if there is no physical interpretation of that term?</em></strong>.</li>
<li>With the full dataset, use forward selection to add static variables to the model and then relevant interaction terms with each other and with the time terms. <strong><em>I've seen some strong criticism of stepwise regression, but doesn't forward selection ignore significant higher order terms if the lower order terms they depend on are not significant? And I've noticed that it's hard to pick a starting model for backward elimination that isn't saturated, or singular, or fails to converge. How do you decide between variable selection algorithms?</em></strong></li>
<li>Add random effects to the model. <strong><em>Is this as simple as doing the variable selection using <code>lm()</code> and then putting the final formula into <code>lme()</code> and specifying the random effects? Or should I include random effects from the very start?</em></strong>. Compare the fits of models using a random intercept only, a random interaction with the linear time term, and random interaction with each successive time term. </li>
<li>Plot a semivariogram to see if an autoregressive error term is needed. <strong><em>What should a semivariogram look like if the answer is 'no'? A horizontal line? How straight, how horizontal? Does including autoregression in the model again require checking potential variables and interactions to make sure they're still relevant?</em></strong></li>
<li>Plot the residuals to see if the variance changes as a function of fitted value, time, or any of the other terms. If it does, weigh the variances appropriately (for <code>lme()</code>, use the <code>weights</code> argument to specify a <code>varFunc()</code>) and compare to the unweighted model to see if this improves the fit. <strong><em>Is this the right sequence in which to do this step, or should it be done before autocorrelation?</em></strong>.</li>
<li>Do <code>summary()</code> of the fitted model to identify significant coefficients for numeric covariates. Do <code>Anova()</code> of the fitted model to identify significant effects for qualitative covariates.</li>
</ol>
"
"0.0750586625040802","0.0563436169819011"," 15145","<p>I have performed the path analysis using the <code>sem</code> function in R. The model which I fitted consists of both direct and indirect paths. I have some trouble in interpreting the estimates of the SEM coefficients. </p>

<ul>
<li>Does R gives the value of total effect = (direct effect + indirect effect) directly or do I have to multiply the coefficients which are on the indirect path and then add them to the coefficients which is on the direct path? This is the usual way of doing path analysis with the raw/absolute correlation coefficients.</li>
</ul>

<p>For example consider X (independent variable), Y (dependent variable) and M (Mediating variable). </p>

<p>The raw/absolute correlation/ standardized regression coefficients between them are X and Y  -0.06; X and M 0.22 and M and Y 0.28 whereas on the path analysis/sem in R, the above coefficients are X and Y -0.13; X and M 0.22 and M and Y 0.31. </p>

<ul>
<li>Thus  is the total effect of X and Y  equal to -0.13?</li>
<li>Alternatively how should I interpret this coefficient considering the effect of variable M into the account?</li>
</ul>
"
"0.0839181358296689","0.0839921051131616"," 15592","<p>I need a bit help for interpreting this results...</p>

<p>I did a correlation analysis with R with the assocstats function.
The result is: </p>

<pre><code>$summary
Call: xtabs(formula = ~MH[, i] + MH[, j], data = MH)
Number of cases in table: 2306 
Number of factors: 2 
Test for independence of all factors:
    Chisq = 2806.6, df = 3318, p-value = 1
    Chi-squared approximation may be incorrect

$object
                    X^2   df P(&gt; X^2)
Likelihood Ratio 1036.1 3318        1
Pearson          2806.6 3318        1

Phi-Coefficient   : 1.103 
Contingency Coeff.: 0.741 
Cramer's V        : 0.637 

$summary
Call: xtabs(formula = ~MH[, i] + MH[, j], data = MH)
Number of cases in table: 2343 
Number of factors: 2 
Test for independence of all factors:
    Chisq = 118.46, df = 73, p-value = 0.000611
    Chi-squared approximation may be incorrect

$object
                    X^2 df   P(&gt; X^2)
Likelihood Ratio 130.83 73 3.8115e-05
Pearson          118.46 73 6.1100e-04

Phi-Coefficient   : 0.225 
Contingency Coeff.: 0.219 
Cramer's V        : 0.225 
</code></pre>

<p>But what does the p-value of 1 in this case mean if Cramer's V is 0.637 and therefor show a strong corelation or with 0.000611 with a Cramer's V of 0.225?</p>

<p>And does anyone of you know a good rule of thumb for interpreting Cramer's V?</p>

<p>Thanks for your help.</p>
"
"0.0726752374667264","0.0727392967453308"," 15900","<p>I plan to do a simulation study where I compare the performance of several robust correlation techniques with different distributions (skewed, with outliers, etc.). With <em>robust</em>, I mean the ideal case of being robust against a) skewed distributions, b) outliers, and c) heavy tails.</p>

<p>Along with the Pearson correlation as a baseline, I was thinking to include following more robust measures:</p>

<ul>
<li>Spearman's $\rho$</li>
<li>Percentage bend correlation (Wilcox, 1994, [1])</li>
<li>Minimum volume ellipsoid, minimum covariance determinant (<code>cov.mve</code>/ <code>cov.mcd</code> with the <code>cor=TRUE</code> option)</li>
<li>Probably, the winsorized correlation</li>
</ul>

<p>Of course there are many more options (especially if you include robust regression techniques as well), but I want to restrict myself to the mostly used/ mostly promising approaches.</p>

<p><strong>Now I have three questions (feel free to answer only single ones):</strong></p>

<ol>
<li><strong>Are there other robust correlational methods I could/ should include?</strong></li>
<li><strong>Which robust correlation techniques are</strong> <em><strong>actually</em></strong>  <strong>used in your field?</strong>
<sub>(Speaking for psychological research: Except Spearman's $\rho$, I have never seen any robust correlation technique outside of a technical paper. Bootstrapping is getting more and more popular, but other robust statistics are more or less non-existent so far).</sub></li>
<li><strong>Are there already systematical comparisons of multiple correlation techniques that you know of?</strong></li>
</ol>

<p>Also feel free to comment the list of methods given above.</p>

<hr>

<p>[1] Wilcox, R. R. (1994). The percentage bend correlation coefficient. <em>Psychometrika</em>, 59, 601-616.</p>
"
"0.0750586625040802","0.0939060283031685"," 16996","<p>I wrote this R function in order to test between matrix (mat1, mat2) correlation by re-sampling the second matrix numR times to obtain correlation coefficient distribution against which to test the observed value. But irrespective of the observed value I always get the distribution with SD 0.3 max. I don`t see how if the original correlation is as high as 0.9 that after 10000 re-samples it still cannot exceed 0.3. Any comments on the code or a possible test with independent data? </p>

<pre><code>resamplerSimAlt &lt;- function(mat1, mat2, numR, graph = FALSE)
{
  statSim &lt;- numeric(numR)
  mat1vcv &lt;- cov(mat1)
  mat2vcvT &lt;- cov(mat2)
  ltM1 &lt;- mat1vcv[col(mat1vcv) &lt;= row(mat1vcv)]
  ltM2T &lt;- mat2vcvT[col(mat2vcvT) &lt;= row(mat2vcvT)]
  statObs &lt;- cor(ltM1, ltM2T)                           
  indice &lt;- c(1:length(mat2))
  resamplesIndices &lt;- lapply(1:numR, function(i) sample(indice, replace = F))
  for (i in 1:numR)
  {
    ss &lt;- mat2[sample(resamplesIndices[[i]])]
    ss &lt;- matrix(ss, nrow = dim(mat2)[[1]], ncol = dim(mat2)[[2]])
    mat2ss &lt;- cov(ss)
    ltM2ss &lt;- mat2ss[col(mat2ss) &lt;= row(mat2ss)]
    statSim[i] &lt;- cor(ltM1, ltM2ss)
  }
  if (graph == TRUE)
  {
    plot(1, main = ""resampled data density distribution"", xlim = c(0, statObs+0.1), ylim = c(0,14))
    points(density(statSim), type=""l"", lwd=2)
    abline(v = statObs)
    text(10, 10, ""observed corelation = "")
  }
  list( obs = statObs , sumFit = sum(statSim &gt; statObs)/numR)
}
</code></pre>
"
"0.0726752374667264","0.0727392967453308"," 17111","<p>I have a classic linear model, with 5 possible regressors. They are uncorrelated with one another, and have quite low correlation with the response. I have arrived at a model where 3 of the regressors have significant coefficients for their t statistic (p&lt;0.05). Adding either or both of the remaining 2 variables gives p values >0.05 for the t statistic, for the added variables. This leads me to believe the 3 variable model is ""best"".</p>

<p>However, using the anova(a,b) command in R where a is the 3 variable model and b is the full model, the p value for the F statistic is &lt; 0.05, which tells me to prefer the full model over the 3 variable model. How can I reconcile these apparent contradictions ?</p>

<p>Thanks
PS
Edit: Some further background. This is homework so I won't post details, but we are not given details of what the regressors represent - they are just numbered 1 to 5. We are asked to ""derive an appropriate model, giving justification"". </p>
"
"0.10277830647413","0.0857240833122733"," 17371","<p>I was wondering, is it possible to have a very strong correlation coefficient (say .9 or higher), with a high p value (say .25 or higher)?</p>

<p>Here's an example of a low correlation coefficient, with a high p value:</p>

<pre><code>set.seed(10)
y &lt;- rnorm(100)
x &lt;- rnorm(100)+.1*y
cor.test(x,y)
</code></pre>

<p>cor = 0.03908927, p=0.6994</p>

<p>High correlation coefficient, low p value:</p>

<pre><code>y &lt;- rnorm(100)
x &lt;- rnorm(100)+2*y
cor.test(x,y)
</code></pre>

<p>cor = 0.8807809, p=2.2e-16</p>

<p>Low correlation coefficient, low p value:</p>

<pre><code>y &lt;- rnorm(100000)
x &lt;- rnorm(100000)+.1*y
cor.test(x,y)
</code></pre>

<p>cor = 0.1035018, p=2.2e-16</p>

<p>High correlation coefficient, high p value:
???</p>
"
"0.0839181358296689","0.0839921051131616"," 17757","<p>I want to calculate correlation coefficient between random vectors and use this for hypothesis testing. The code</p>

<pre><code>bs &lt;- function(qtS, rvNum = 1000)
{
  eb &lt;- qtS * rvNum
  randVect &lt;- matrix(rnorm(eb), nrow=qtS, ncol = rvNum)
  randVect &lt;- randVect/sqrt(sum(randVect * randVect))
  randVectCor &lt;- cor(randVect)
  randVectCor &lt;- randVectCor[col(randVectCor) &lt; row(randVectCor)]
  plot(density(randVectCor), main = ""resampled data density distribution"", type = ""l"", lwd  = 2, xlim = c(min(randVectCor), max(randVectCor)))
  list(min = min(randVectCor), max = max(randVectCor), randVectCor = randVectCor)
}
</code></pre>

<p>works well but with one peculiar thing that I argue a lot with my colleague. Namely, if the qtS is small (i.e. 5) then the max and min are higer than 0.99 which in turn leads to rejection of the vast majority of possible observed correlation values. But with increasing qtS min and max values get lower and with i.e. 50 this value is 0.6. Is this OK, I mean the fact that possible correlations between vectors with fewer elements are orderly larger than between vectors with greater number of elements? </p>
"
"0.118678165819385","0.1187827741833"," 18497","<p>Wikipedia suggests that one way to look at <a href=""http://en.wikipedia.org/wiki/Inter-rater_reliability#Intra-class_correlation_coefficient"" rel=""nofollow"">inter-rater reliability</a> is to use a random effects model to compute <a href=""http://en.wikipedia.org/wiki/Intra-class_correlation_coefficient#.22Modern.22_ICCs"" rel=""nofollow"">intraclass correlation</a>.  The example of intraclass correlation talks about looking at</p>

<p>$$\frac{\sigma_\alpha^2}{\sigma_\alpha^2+\sigma_\epsilon^2}$$</p>

<p>from a model</p>

<p>$$Y_{ij} = \mu + \alpha_i + \epsilon_{ij}$$</p>

<p>""where Y<sub>ij</sub> is the j<sup>th</sup> observation in the i<sup>th</sup> group, Î¼ is an unobserved overall mean, Î±<sub>i</sub> is an unobserved random effect shared by all values in group i, and Îµ<sub>ij</sub> is an unobserved noise term.""</p>

<p>This is an attractive model especially because in my data no rater has rated all things (although most have rated 20+), and things are rated a variable number of times (usually 3-4).</p>

<p>Question #0: Is ""group i"" in that example (""group i"") a grouping of things being rated?</p>

<p>Question #1: If I'm looking for inter-rater-reliability, don't I need a random effects model with two terms, one for the rater, and one for the thing rated?  After all, both have possible variation.</p>

<p>Question #2: How would I best express this model in R?</p>

<p>It looks as if <a href=""http://stats.stackexchange.com/questions/18088/intraclass-correlation-icc-for-an-interaction"">this question</a> has a nice-looking proposal:</p>

<pre><code>lmer(measurement ~ 1 + (1 | subject) + (1 | site), mydata)
</code></pre>

<p>I looked at a <a href=""http://stats.stackexchange.com/questions/10905/how-to-specify-random-effects-in-lme"">couple</a> <a href=""http://stackoverflow.com/questions/1380694/how-to-fit-a-random-effects-model-with-subject-as-random-in-r"">questions</a>, and the syntax of the ""random"" parameter for lme is opaque to me.  I read the <a href=""http://stat.ethz.ch/R-manual/R-patched/library/nlme/html/lme.html"" rel=""nofollow"">help page for lme</a>, but the description for ""random"" is incomprehensible to me without examples.</p>

<p>This question is somewhat similar to a <a href=""http://stats.stackexchange.com/questions/14004/intra-and-inter-rater-reliability-on-the-same-data"">long</a> <a href=""http://stats.stackexchange.com/questions/3539/inter-rater-reliability-for-ordinal-or-interval-data"">list</a> of <a href=""http://stats.stackexchange.com/questions/4009/inter-rater-reliability-using-intra-class-correlation-with-ratings-for-multiple"">questions</a>, with <a href=""http://stats.stackexchange.com/questions/3539/inter-rater-reliability-for-ordinal-or-interval-data"">this</a> the closest.  However, most don't address R in detail.</p>
"
"0.0419590679148345","0.0419960525565808"," 20438","<p>I fitted a GEE model using the function <code>genZcor</code> with user defined
correlation matrix. I want to get the var-cov matrix of the regression
coefficients. But the output provides only limited information.</p>

<p>I would be very much thankful if you could kindly let me know how to get
it since I am struggling lot getting this.</p>
"
"0.10277830647413","0.102868899974728"," 20672","<p>I have two continuous variables, X and Y, that are correlated - they are not independent. To correct for non-independence, I have a known correlation structure, a matrix S.</p>

<p>If one calls <code>gls(Y ~ X, correlation = S)</code>, what I think happens is that, internally, gls() transforms X and Y in some way so that the regression ends up being <code>S^(-1)*Y = S^(-1) * X</code>.</p>

<p>How is this transformation actually performed? From the literature I've consulted, I've seen everything from:</p>

<pre><code>X.transformed &lt;- solve(chol(S)) %*% X 
#The inverse of the Choleski decomposition of S times the vertical vector X, 
#which in my case does nothing to the data
</code></pre>

<p>to</p>

<pre><code>X.transformed &lt;- chol(solve(S)) %*% X 
# which has negative values and gives meaningless values of X
</code></pre>

<p>Another method I've seen is transforming the dependent variable by </p>

<pre><code>chol(solve(S)) %*% Y 
</code></pre>

<p>and the independent variable by </p>

<pre><code>chol(solve(S)) %*% cbind(1,X) 
</code></pre>

<p>and doing the linear model using the transformed intercept terms in the first column of the X matrix: </p>

<pre><code>lm(Y ~ X - 1)
</code></pre>

<p>On a related note, is there any point to manually transforming the data in order to plot it? Do the transformed values have any meaning, or are they simply there to estimate regression coefficients? (In other words, if X is a variable of body mass figures, X values are not necessarily errant if they're negative since they're still linear?) I suppose it would follow from this that an $R^2$ statistic on transformed variables is also meaningless?</p>
"
"0.0593390829096927","0.0593913870916499"," 21789","<p>Teegavarapu and Chandramouli (2005) has mentioned Coefficient of correlation method for spatial interpolation of moisture data that calculates the coefficients between a point and its neighbors and uses it for interpolation. </p>

<p>R has many spatial packages across many levels (sp, geoR, gstat, automap). I have been able to find many methods including geostatistical methods implemented in R especially for cross validation.</p>

<p>Does anyone know a package that implements above method and especially its cross validation version?</p>
"
"0.145350474933453","0.145478593490662"," 22125","<p>I'm trying to replicate <a href=""http://www.mendeley.com/research/averaging-correlation-coefficients-should-fishers-z-transformation-be-used/"">Silver &amp; Dunlap (1987)</a>.  I'm just comparing averaging correlations or averaging z transform correlations and back transforming.  I seem to not be replicating the asymmetry in the bias they find (back transformed zs are not closer to the population value for me than rs).  Any thoughts?  Is it possible that 1987 computing power just didn't explore the space enough?</p>

<pre><code># Fisher's r2z
fr2z &lt;- atanh
# and back
fz2r &lt;- tanh

# a function that generates a matrix of two correlated variables
rcor &lt;- function(n, m1, m2, var1, var2, corr12){
    require(MASS)
    Sigma &lt;- c(var1, sqrt(var1*var2)*corr12, sqrt(var1*var2)*corr12, var2)
    Sigma &lt;- matrix(Sigma, 2, 2)
    return( mvrnorm(n, c(m1,m2), Sigma, empirical=FALSE) )
    }
</code></pre>

<p>With these function it's easy to look at a bunch of correlations (basically replicate silver and dunlap 1987) and see the difference between averaging correlations and averaging z-scores and back transforming.  Here's just one.</p>

<pre><code>r &lt;- 0.9
Y &lt;- replicate(20000, rcor(10, 0, 0, 1, 1, r))
rs &lt;- apply(Y, 3, function(x) cor(x[,1], x[,2]))
mean(rs) - r
zs &lt;- fr2z(rs)
fz2r( mean(zs) ) - r
</code></pre>

<p>Just looking at the sample size of 10 and correlations of 0.1, 0.5, and 0.9 these are the results.</p>

<pre><code>     rho  r bias   z bias
     0.1  -0.006   0.006
     0.5  -0.024   0.021
     0.9  -0.011   0.011
</code></pre>

<p>And these are derived from Table 1 of Silver &amp; Dunlap.</p>

<pre><code>     rho  r bias   z bias
     0.1  -0.007   0.003
     0.5  -0.025   0.001
     0.9  -0.011  -0.007
</code></pre>

<p>These are quite different results.  From my test I'm seeing that it's just a matter of direction of bias, not magnitude.  But, in the published paper they're finding much less magnitude with z.  I couldn't find a published non-replication.</p>
"
"0.0856485887284415","0.102868899974728"," 23173","<p>Here is the head of my data set (<code>tjornres</code>): </p>

<pre><code>       Fish.1 Fish.2  MORPHO      DIET 
1         1      2        0.03768       0.1559250 
2         1      3        0.05609       0.7897060 
3         1      4        0.03934       0.4638010 
4         1      5        0.03363       0.1200480 
5         1      6        0.05629       0.4390760 
6         1      8        0.08366       0.1866750 
7         1      9        0.04892       0.0988235 
8         1     10       0.04427       0.2637140 
</code></pre>

<p><code>MORPHO</code> and <code>DIET</code> refer to the morphological and diet distances between fish 1 and fish 2. My original data set has over 2400 pairs of fish. My goal  is to resample this dataset by selecting only 435. 
I would like to do this 999 times and get a distribution of the correlation coefficients <code>MORPHO~DIET</code>. </p>

<p>I went on and wrote this code: </p>

<pre><code>head(tjornres) 

essayres = tjornres                  # copy of the data             
R = 999                                         # the number of replicates             
cor.values = numeric(R)         # store the data             
for (i in 1:R) {                              # loop 
+ group1 = sample(essayres, size=435, replace=F) 
+ group2 = sample(essayres, size=435, replace=F) 
+ cor.values[i] = cor.test(group1,group2)$cor 
+ } 
</code></pre>

<p>I have a syntax error in this code. </p>

<p>Also if I run one resampling, <code>sample(essayres, size=435, replace=F)</code>, I get this error </p>

<pre><code>message: Error in `[.data.frame`(x, .Internal(sample(length(x), size, replace,  
:cannot take a sample larger than the population when 'replace = FALSE'.
</code></pre>

<p>Does anyone know why this code is not working? Are there any other ways to resample (without replacement) ? 
Thank you for your help, </p>
"
"0.0726752374667264","0.0727392967453308"," 23983","<p>I am very new in partial least squares (PLS) and I try to understand the output of the R function <code>plsr()</code> in the <code>pls</code> package. Let us simulate data and run the PLS:</p>

<pre><code>library(pls)
n &lt;- 50
x1 &lt;- rnorm(n); xx1 &lt;- scale(x1) 
x2 &lt;- rnorm(n); xx2 &lt;- scale(x2)
y &lt;- x1 + x2 + rnorm(n,0,0.1); yy &lt;- scale(y)
p &lt;- plsr(yy ~ xx1+xx2, ncomp=1)
</code></pre>

<p>I was expecting that the following numbers $a$ and $b$</p>

<pre><code>&gt; ( w &lt;- loading.weights(p) )

Loadings:
    Comp 1
xx1 0.723 
xx2 0.690 

               Comp 1
SS loadings       1.0
Proportion Var    0.5
&gt; a &lt;- w[""xx1"",]
&gt; b &lt;- w[""xx2"",]
&gt; a^2+b^2
[1] 1
</code></pre>

<p>are calculated in order to maximize </p>

<pre><code>&gt; cor(y, a*xx1+b*xx2)
          [,1]
[1,] 0.9981291
</code></pre>

<p>but this is not exactly the case:</p>

<pre><code>&gt; f &lt;- function(ab){
+ a &lt;- ab[1]; b &lt;- ab[2]
+ cor(y, a*xx1+b*xx2)
+ }
&gt; optim(c(0.7,0.6), f, control=list(fnscale=-1))
$par
[1] 0.7128259 0.6672870

$value
[1] 0.9981618
</code></pre>

<p>Is it a numerical error, or do I misunderstand the nature of $a$ and $b$ ? </p>

<p>I would also like to know what are these coefficients: </p>

<pre><code>&gt; p$coef
, , 1 comps

           yy
xx1 0.6672848
xx2 0.6368604 
</code></pre>

<p><strong>EDIT</strong>: Now I see what <code>p$coef</code> is:</p>

<pre><code>&gt; x &lt;- a*xx1+b*xx2
&gt; coef(lm(yy~0+x))
        x 
0.9224208 
&gt; coef(lm(yy~0+x))*a
        x 
0.6672848 
&gt; coef(lm(yy~0+x))*b
        x 
0.6368604 
</code></pre>

<p>So I think I'm right about the nature of $a$ and $b$.</p>

<p><strong>EDIT:</strong> In view of the comments given by @chl I feel my question is not clear enough, so let me provide more details.  In my example there is a vector $Y$ of responses and a two-columns matrix $X$ of predictors and I use the normalized version $\tilde Y$ of $Y$ and the normalized version $\tilde X$ of $X$ (centered and divided by standard deviations). The definition of the first PLS component $t_1$ is $t_1 = a \tilde X_1 + b \tilde X_2$ with $a$ and $b$ chosen in order to have a maximal value of the inner product $\langle t_1, \tilde Y \rangle$. <strong>Hence it is equivalent to maximizing the correlation between $t_1$ and $Y$, isn't it ?</strong></p>
"
"0.0839181358296689","0.0839921051131616"," 24980","<p>I have problems in using the <code>cor()</code> and <code>cor.test()</code> functions.</p>

<p>I just have two matrices (only numerical values, and the same number of
row and columns) and I want to have the correlation number and the 
corresponding p-value.</p>

<p>When I use <code>cor(matrix1, matrix2)</code> I get the correlation coefficients for all the cells.
I just want a single number as result of cor.</p>

<p>In additon when I do <code>cor.test(matrix1, matrix2)</code> I get the following error </p>

<pre><code>Error in cor.test.default(matrix1, matrix2) : 'x' must be a numeric vector
</code></pre>

<p>How can I get p-values for matrices?</p>

<p>You find the simple tables I want to correlate here:</p>

<p><a href=""http://dl.dropbox.com/u/3288659/table_exp1_offline_MEANS.csv"" rel=""nofollow"">http://dl.dropbox.com/u/3288659/table_exp1_offline_MEANS.csv</a></p>

<p><a href=""http://dl.dropbox.com/u/3288659/table_exp2_offline_MEANS.csv"" rel=""nofollow"">http://dl.dropbox.com/u/3288659/table_exp2_offline_MEANS.csv</a></p>
"
"0.103843395091962","0.103934927410387"," 29590","<p>I am using R to examine the relationship between two variables in a small data set ($n=16$). </p>

<p>My problem is that I'm not really sure how to handle the analysis (read: I'm in deeper waters than I've traveled before).</p>

<p>Do I use Spearman's Rho to calculate the rank coefficient? Or do I assume normality and use standard parametric tests (Pearson's)? Or do I transform the data, and then do the parametric tests? Or should I do something else?</p>

<p>My initial feeling is that I should use non-parametric tests on this data set for various reasons:</p>

<ol>
<li>I feel strongly that there is an upper and lower bound to the possible values observed for either the dependent or independent variable. </li>
<li>I do not feel that the relationship between the variables is linear. I've provided the QQ Plot of the dependent variable, and the scatterplot of the dep &amp; indep vars below, and neither seems to adhere to the normality assumptions:</li>
</ol>

<p>The thing is, I'm getting hung up on several things:</p>

<ol>
<li>the Pearson's coefficient seems really high 0.94366 which makes me wonder if I'm sacrificing something by using Spearman's instead (for the sake of full disclosure, the Spearman's coefficient of 0.86765 is also significant at $\alpha = .001$).  <strong>Now Answered</strong></li>
<li>If Spearman's is the answer, what is the next step in building a predictive model?  <strong>Now Answered</strong></li>
<li>Since the sample is relatively small, should I use some kind of resampling to calculate the correlation instead of just using the 16 values?</li>
<li>What other major things might I be overlooking?</li>
</ol>

<hr>

<p>Plots</p>

<p>QQ Plot of Indep Var:
<img src=""http://i.stack.imgur.com/qc5wV.png"" alt=""enter image description here"">
<a href=""http://dl.dropbox.com/u/27272488/qqPlotY.png"" rel=""nofollow"">http://dl.dropbox.com/u/27272488/qqPlotY.png</a></p>

<p>Scatterplot of Dep &amp; Indep Vars:
<img src=""http://i.stack.imgur.com/2pGTb.png"" alt=""enter image description here"">
<a href=""http://dl.dropbox.com/u/27272488/XvsY.png"" rel=""nofollow"">http://dl.dropbox.com/u/27272488/XvsY.png</a></p>

<hr>

<p>Data</p>

<pre><code>&gt; df$year
    [1] 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009
    &gt; df$x
[1] 1.39 1.41 1.48 1.46 1.24 1.34 1.70 1.61 1.47 1.69 1.94 2.30 2.51 2.64 3.01 2.14
&gt; df$y
[1] 2320 2227 2161 2116 2294 2483 2897 3197 3270 3714 4028 4576 4837 5174 5312 4462
</code></pre>
"
"0.139162484826546","0.139285149006224"," 30061","<p>What approaches exist to observe the time lag between two variables?</p>

<p>I need to analyze the relationship between blood pressure and some other factor, such as exercise. The data set I am drawing from has around 1800 individuals, with an average of 100 entries a piece. It is generally known that there is a strong relationship between exercise level and blood pressure. However, if a person increases their steps to 8000+ a day, how long will it take for their blood pressure to drop? I am new to this type of analysis, and this is a challenge I have been thinking about for weeks. </p>

<p>I don't know if anyone wants to comment on possible approaches to this challenge or any issues surrounding it.</p>

<p>Some issues I have been dealing with:</p>

<ol>
<li><p>Is it better to treat this as a times series analysis or longitudinal data analysis?</p>

<p>My understanding is that time series usually focuses on one variable with no missing data and is observed at consistent intervals, where as longitudinal is over a longer period and has inconsistent time intervals, dropouts, and missing data.</p>

<p>The data I have seems to fit the longitudinal description more, but it also seems like time series could be used if I averaged the values by week so there would be no missing entries. I'm not sure about the pros and cons of each approach.</p></li>
<li><p>Should I be fitting a causal model, or would some other method like regression be more helpful?</p>

<p>I've been looking at various possible causal models, for example Marginal Structural Models (MSM) or Structural Nested Models (SNM), but there seem to be very little information on their application. I did find one R package that applied inverse probability weights and then used Cox proportional hazards regression model on a survival object (MSM), but that seemed to be focus on weighting for confounding and right censoring. Its result was a correlation coefficient, which I don't think helps me.</p>

<p>So I'm not sure if fitting a causal model is what I want, because that seems to be more focused on the making intellectually satisfying assumptions about relationships within the data and then determining the degree of causality, rather than providing information about time lag.</p>

<p>If anyone knows about MSM, SNM, their use in R, or how they might relate to this problem, that would be awesome to hear.</p></li>
<li><p>What about survival analysis or SEM?</p>

<p>I haven't explored these options very in-depth yet but they sound potentially relevant.</p></li>
</ol>

<p>I've kind of stalled, so any hints about what direction I might want to go would be really appreciated. </p>

<p>Thanks in advance.</p>
"
"0.0726752374667264","0.0727392967453308"," 31159","<p>I fit a simple linear model $y = bX$ to a data set today, and that produced 24 residuals (I have 24 data points, one for each year from 1984-2007). I would like to test the time-independence of the residuals of my model, and I was recommended by my supervisor to use the Ljung-Box test. The <code>Box.test</code> function in R takes 4 arguments: </p>

<ul>
<li><code>x</code>:     a numeric vector or univariate time series.</li>
<li><code>lag</code>:   the statistic will be based on lag autocorrelation coefficients.</li>
<li><code>type</code>:  test to be performed: partial matching is used.</li>
<li><code>fitdf</code>:     number of degrees of freedom to be subtracted if x is a
series of residuals.</li>
</ul>

<p>What does <code>lag</code> mean, and what value would you guys recommend I use for the test? Also, what does <code>fitdf</code> represent, and what would the value for that parameter be in my case? Finally, the value of <code>x</code> is a vector of my 24 residuals, correct?</p>
"
"0.118678165819385","0.1187827741833"," 31204","<p>I want to model two different time variables, some of which are heavily collinear in my data (age + cohort = period). Doing this I ran into some trouble with <code>lmer</code> and and interactions of <code>poly()</code>, but it's probably not limited to <code>lmer</code>, I got the same results with <code>nlme</code> IIRC.</p>

<p>Obviously, my understanding of what the poly() function does is lacking. I understand what <code>poly(x,d,raw=T)</code> does and I thought without <code>raw=T</code> it makes orthogonal polynomials (I can't say I really understand what that means), which makes fitting easier, but doesn't let you interpret the coefficients directly.<br>
I <a href=""http://r.789695.n4.nabble.com/use-of-poly-td847784.html"">read</a> that because I'm using the predict function, the predictions should be the same.</p>

<p>But they aren't, even when the models converge normally. I'm using centered variables and I first thought that maybe the orthogonal polynomial leads to higher fixed effect correlation with the collinear interaction term, but it seems comparable. I've pasted two <a href=""https://gist.github.com/3002722"">model summaries over here</a>.</p>

<p>These plots hopefully illustrate the extent of the difference. I used the predict-function which is only available in the dev. version of lme4 (heard about it <a href=""http://stats.stackexchange.com/a/29749/2795"">here</a>), but the fixed effects are the same in the CRAN version (and they also seem off by themselves, e.g. ~ 5 for the interaction when my DV has a range of 0-4).</p>

<p>The lmer call was</p>

<pre><code>cohort2_age =lmer(churchattendance ~ 
poly(cohort_c,2,raw=T) * age_c + 
ctd_c + dropoutalive + obs_c + (1+ age_c |PERSNR), data=long.kg)
</code></pre>

<p>The prediction was fixed effects only, on fake data (all other predictors=0) where I marked the range present in the original data as extrapolation=F.</p>

<pre><code>predict(cohort2_age,REform=NA,newdata=cohort.moderates.age)
</code></pre>

<p>I can provide more context if need be (I didn't manage to produce a reproducible example easily, but can of course try harder), but I think this is a more basic plea: explain the <code>poly()</code> function to me, pretty please.</p>

<h3>Raw polynomials</h3>

<p><img src=""http://i.stack.imgur.com/2FIIO.jpg"" alt=""Raw polynomials""></p>

<h3>Orthogonal polynomials (clipped, nonclipped at <a href=""http://i.imgur.com/iFmLE.png"">Imgur</a>)</h3>

<p><img src=""http://i.stack.imgur.com/CvVdC.jpg"" alt=""Orthogonal polynomials""></p>
"
"0.0593390829096927","0.0593913870916499"," 31803","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/4040/r-compute-correlation-by-group"">R: compute correlation by group</a>  </p>
</blockquote>



<p>I have a data set with a 6 or so data points for groups of data.</p>

<p>e.g.:</p>

<pre><code>email_id   date_sent  number_sent  number_of_views  number_of_responses
1           5/4         600            25                6
1           5/5         500            22                8
1           5/6         450            23                4
1           5/7         700            34               12
2           5/5         900            30               10
2           5/6         750            28               11
...
</code></pre>

<p>(this is made up data that illustrates the point)</p>

<p>Assuming I have this in a data frame in R, I'd like to write something which will give me stats by group.  I'm most interested in the correlation coefficient between some of the columns.  </p>

<p>I know how to do this with a data frame that contains only one group:</p>

<p>cor(col1, col2)</p>

<p>but I'd like to learn a technique that will allow me to extract data that looks something like this:</p>

<pre><code>email_id    cor(col3, col4)
1             .73
2             .85
3             .98
</code></pre>

<p>and so on.</p>

<p>Thanks,  </p>
"
"0.201403525991205","0.193181841760272"," 32040","<p>I'm trying to analyze effect of Year on variable logInd for particular group of individuals (I have 3 groups). <strong>The simplest model:</strong></p>

<pre><code>&gt; fix1 = lm(logInd ~ 0 + Group + Year:Group, data = mydata)
&gt; summary(fix1)

Call:
lm(formula = logInd ~ 0 + Group + Year:Group, data = mydata)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.5835 -0.3543 -0.0024  0.3944  4.7294 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
Group1       4.6395740  0.0466217  99.515  &lt; 2e-16 ***
Group2       4.8094268  0.0534118  90.044  &lt; 2e-16 ***
Group3       4.5607287  0.0561066  81.287  &lt; 2e-16 ***
Group1:Year -0.0084165  0.0027144  -3.101  0.00195 ** 
Group2:Year  0.0032369  0.0031098   1.041  0.29802    
Group3:Year  0.0006081  0.0032666   0.186  0.85235    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.7926 on 2981 degrees of freedom
Multiple R-squared: 0.9717,     Adjusted R-squared: 0.9716 
F-statistic: 1.705e+04 on 6 and 2981 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>We can see the Group1 is significantly declining, the Groups2 and 3 increasing but not significantly so.</p>

<p><strong>Clearly the individual should be random effect, so I introduce random intercept effect for each individual:</strong></p>

<pre><code>&gt; mix1a = lmer(logInd ~ 0 + Group + Year:Group + (1|Individual), data = mydata)
&gt; summary(mix1a)
Linear mixed model fit by REML 
Formula: logInd ~ 0 + Group + Year:Group + (1 | Individual) 
   Data: mydata 
  AIC  BIC logLik deviance REMLdev
 4727 4775  -2356     4671    4711
Random effects:
 Groups     Name        Variance Std.Dev.
 Individual (Intercept) 0.39357  0.62735 
 Residual               0.24532  0.49530 
Number of obs: 2987, groups: Individual, 103

Fixed effects:
              Estimate Std. Error t value
Group1       4.6395740  0.1010868   45.90
Group2       4.8094268  0.1158095   41.53
Group3       4.5607287  0.1216522   37.49
Group1:Year -0.0084165  0.0016963   -4.96
Group2:Year  0.0032369  0.0019433    1.67
Group3:Year  0.0006081  0.0020414    0.30

Correlation of Fixed Effects:
            Group1 Group2 Group3 Grp1:Y Grp2:Y
Group2       0.000                            
Group3       0.000  0.000                     
Group1:Year -0.252  0.000  0.000              
Group2:Year  0.000 -0.252  0.000  0.000       
Group3:Year  0.000  0.000 -0.252  0.000  0.000
</code></pre>

<p>It had an expected effect - the SE of slopes (coefficients Group1-3:Year) are now lower and the residual SE is also lower.</p>

<p><strong>The individuals are also different in slope so I also introduced the random slope effect:</strong></p>

<pre><code>&gt; mix1c = lmer(logInd ~ 0 + Group + Year:Group + (1 + Year|Individual), data = mydata)
&gt; summary(mix1c)
Linear mixed model fit by REML 
Formula: logInd ~ 0 + Group + Year:Group + (1 + Year | Individual) 
   Data: mydata 
  AIC  BIC logLik deviance REMLdev
 2941 3001  -1461     2885    2921
Random effects:
 Groups     Name        Variance  Std.Dev. Corr   
 Individual (Intercept) 0.1054790 0.324775        
            Year        0.0017447 0.041769 -0.246 
 Residual               0.1223920 0.349846        
Number of obs: 2987, groups: Individual, 103

Fixed effects:
              Estimate Std. Error t value
Group1       4.6395740  0.0541746   85.64
Group2       4.8094268  0.0620648   77.49
Group3       4.5607287  0.0651960   69.95
Group1:Year -0.0084165  0.0065557   -1.28
Group2:Year  0.0032369  0.0075105    0.43
Group3:Year  0.0006081  0.0078894    0.08

Correlation of Fixed Effects:
            Group1 Group2 Group3 Grp1:Y Grp2:Y
Group2       0.000                            
Group3       0.000  0.000                     
Group1:Year -0.285  0.000  0.000              
Group2:Year  0.000 -0.285  0.000  0.000       
Group3:Year  0.000  0.000 -0.285  0.000  0.000
</code></pre>

<h3><strong>But now, contrary to the expectation, the SE of slopes (coefficients Group1-3:Year) are now much higher, even higher than with no random effect at all!</strong></h3>

<p>How is this possible? I would expect that the random effect will ""eat"" the unexplained variability and increase ""sureness"" of the estimate!</p>

<p>However, the residual SE behaves as expected - it is lower than in the random intercept model.</p>

<p><a href=""http://artax.karlin.mff.cuni.cz/~ttel5535/pub/my_so_question_data.R"" rel=""nofollow"">Here is the data</a> if needed.</p>

<h2>Edit</h2>

<p>Now I realized astonishing fact. If I do the linear regression for each individual separately and then run ANOVA on the resultant slopes, <strong>I get exactly the same result as the random slope model!</strong> Would you know why?</p>

<pre><code>indivSlope = c()
for (indiv in 1:103) {
    mod1 = lm(logInd ~ Year, data = mydata[mydata$Individual == indiv,])
    indivSlope[indiv] = coef(mod1)['Year']
}

indivGroup = unique(mydata[,c(""Individual"", ""Group"")])[,""Group""]


anova1 = lm(indivSlope ~ 0 + indivGroup)
summary(anova1)

Call:
lm(formula = indivSlope ~ 0 + indivGroup)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.176288 -0.016502  0.004692  0.020316  0.153086 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
indivGroup1 -0.0084165  0.0065555  -1.284    0.202
indivGroup2  0.0032369  0.0075103   0.431    0.667
indivGroup3  0.0006081  0.0078892   0.077    0.939

Residual standard error: 0.04248 on 100 degrees of freedom
Multiple R-squared: 0.01807,    Adjusted R-squared: -0.01139 
F-statistic: 0.6133 on 3 and 100 DF,  p-value: 0.6079 
</code></pre>

<p><a href=""http://artax.karlin.mff.cuni.cz/~ttel5535/pub/my_so_question_data.R"" rel=""nofollow"">Here is the data</a> if needed.</p>
"
"0.0839181358296689","0.0629940788348712"," 32188","<p>I am doing a one way ANOVA (per species) with custom contrasts.</p>

<pre><code>     [,1] [,2] [,3] [,4]
0.5    -1    0    0    0
5       1   -1    0    0
12.5    0    1   -1    0
25      0    0    1   -1
50      0    0    0    1
</code></pre>

<p>where I compare intensity 0.5 against 5, 5 against 12.5 and so on. These is the data I'm working on</p>

<p><img src=""http://i.stack.imgur.com/L7uVk.png"" alt=""enter image description here""></p>

<p>with the following results</p>

<pre><code>Generalized least squares fit by REML
  Model: dark ~ intensity 
  Data: skofijski.diurnal[skofijski.diurnal$species == ""niphargus"", ] 
       AIC      BIC    logLik
  63.41333 67.66163 -25.70667

Coefficients:
            Value Std.Error  t-value p-value
(Intercept) 16.95 0.2140872 79.17334  0.0000
intensity1   2.20 0.4281744  5.13809  0.0001
intensity2   1.40 0.5244044  2.66970  0.0175
intensity3   2.10 0.5244044  4.00454  0.0011
intensity4   1.80 0.4281744  4.20389  0.0008

 Correlation: 
           (Intr) intns1 intns2 intns3
intensity1 0.000                      
intensity2 0.000  0.612               
intensity3 0.000  0.408  0.667        
intensity4 0.000  0.250  0.408  0.612 

Standardized residuals:
       Min         Q1        Med         Q3        Max 
-2.3500484 -0.7833495  0.2611165  0.7833495  1.3055824 

Residual standard error: 0.9574271 
Degrees of freedom: 20 total; 15 residual
</code></pre>

<p>16.95 is the global mean for ""niphargus"". In intensity1, I'm comparing means for intensity 0.5 against 5.</p>

<p>If I understood this right, the coefficient for intensity1 of 2.2 should be half the difference between means of intensity levels 0.5 and 5. However, my hand calculations don't match those of the summary. Can anyone chip in what am I doing wrong?</p>

<pre><code>ce1 &lt;- skofijski.diurnal$intensity
    levels(ce1) &lt;- c(""0.5"", ""5"", ""0"", ""0"", ""0"")
    ce1 &lt;- as.factor(as.character(ce1))
    tapply(skofijski.diurnal$dark, ce1, mean)
       0    0.5      5 
  14.500 11.875 13.000 
diff(tapply(skofijski.diurnal$dark, ce1, mean))/2
      0.5       5 
  -1.3125  0.5625 
</code></pre>
"
"0.118678165819385","0.1187827741833"," 32735","<p>As a financial institution, we often run into analysis of time series data. A lot of times we end up doing regression using time series variables. As this happens, we often encounter residuals with time series structure that violates basic assumption of independent errors in OLS regression.  Recently we are building another model in which I believe we have regression with autocorrelated errors.The residuals from linear model have <code>lm(object)</code> has clearly a AR(1) structure, as evident from ACF and PACF.  I took two different approaches, the first one was obviously to fit the model using Generalized least squares <code>gls()</code> in R. My expectation was that the residuals from gls(object) would be a white noise (independent errors).  But the residuals from <code>gls(object)</code> still have the same ARIMA structure as in the ordinary regression. Unfortunately there is something wrong in what I am doing that I could not figure out.  Hence I decided to manually adjust the regression coefficients from the linear model (OLS estimates). Surprisingly that seems to be working when I plotted the residuals from adjusted regression (the residuals are white noise). I really want to use <code>gls()</code> in <code>nlme</code> package so that coding will be lot simpler and easier. What would be the approach I should take here? Am I supposed to use REML? or is my expectation of non-correlated residuals (white noise) from gls() object wrong?</p>

<pre><code>gls.bk_ai &lt;- gls(PRNP_BK_actINV ~ PRM_BK_INV_ENDING + NPRM_BK_INV_ENDING, 
                 correlation=corARMA(p=1), method='ML',  data  = fit.cap01A)

gls2.bk_ai  &lt;- update(gls.bk_ai, correlation = corARMA(p=2))

gls3.bk_ai &lt;- update(gls.bk_ai, correlation = corARMA(p=3))

gls0.bk_ai &lt;- update(gls.bk_ai, correlation = NULL)

anova(gls.bk_ai, gls2.bk_ai, gls3.bk_ai, gls0.bk_ai)  
     ## looking at the AIC value, gls model with AR(1) will be the best bet

acf2(residuals(gls.bk_ai)) # residuals are not white noise
</code></pre>

<p>Is there something wrong with what I am doing???????</p>
"
"0.201228620594782","0.201405992705548"," 33105","<p>I'm working on an ongoing data analysis project about a series of live educational seminars. Each of my data points represents one such event, and for each one I have a multitude of categorical variables, as well as a couple quantitative ones that are my desired response variables (total revenue and number of attendees).</p>

<p>One trend I'm interested in looking at is how the frequency of these events affects my two response variables. Over the years, we have increased the frequency of the events and I'd like to determine whether or not it makes sense to continue doing so. I've created a couple of variables to help track this frequency:</p>

<p><code>NEAREST.SEM</code> - the number of days between this event and the nearest one to it chronologically in either direction</p>

<p><code>LAST.SEM</code> - the number of days between this event and the nearest one to it chronologically <em>before</em> it</p>

<p><code>WEEKLY.SEMS</code> - the total number of events held during the 7-day period starting on Monday within which this event falls</p>

<p>Depending on how I do the analysis, these three variables seem to have varying significance, but the one that seems to consistently come out on top is <code>NEAREST.SEM</code>, which I have found to be significant at the 0.01 level in one test and the 0.001 level in another. The other two variables are significant in predicting revenue but not number of attendees, which is not ideal since we are more interested in number of attendees. (The data for revenue is not representative of the total revenue for each event due to certain special offers for repeat customers that aren't taken into account there.)</p>

<p>Increasing the frequency of events seems to decrease each event's individual performance, but has so far increased overall performance. I'd like to determine the ""turning point"" at which overall performance will either dip or level off. Unfortunately, this is going to be tough to predict because my best-fitted variable, <code>NEAREST.SEM</code>, isn't as good a representation of increased frequency. Note, for example, that it would look exactly the same whether 4 or 5 events were held per week--it would always have the value of 1 in such situations. In fact, any time that events are grouped in clusters of consecutive days, we'll always get 1 for them on this variable...</p>

<p>One option would be to just use <code>WEEKLY.SEMS</code> as a predictor of revenue, which it is well correlated with, but as I said, we'd much rather do this analysis based on number of attendees, a better measure of an event's success.</p>

<p>So I really have two questions here:</p>

<ol>
<li><p>Any suggestions on my dilemma of which variable to use and how to deal with the problems I laid out above?</p></li>
<li><p>Once I decide on a predictor factor, how can I go about estimating the average decrease in revenue increasing to various frequencies will have? Should I run a multiple regression using all my variables and use the coefficient on the predictor factor? Or should I run a regression with just the one factor and my response and use that coefficient? Or is there a better test than regression to use?</p></li>
</ol>

<p>(By the way, I'm using R for my analysis and I'd appreciate any advice specifically tailored for that language.)</p>

<p>UPDATE: I have tried creating two new measures, one that's the average distance in days of the nearest event on either side, and one that's the number of events within 3 days in both directions...neither of them had any significant correlation. I'm running out of ideas here...</p>
"
"0.0484501583111509","0.0727392967453308"," 34202","<p>I am trying to make sense of the spearman test I am getting as a result of using <a href=""http://stat.ethz.ch/R-manual/R-patched/library/stats/html/cor.test.html"" rel=""nofollow"">cor.test on R package</a>. <a href=""http://geography.uoregon.edu/geogr/topics/interpstats.htm"" rel=""nofollow"">This page</a> seemed to me to inform that I should care not only for p-values and rho values from the output but as well its relation with the t tests. Sometimes I get as result p value of 0 and rho value of 1 and the value of the statistical test is zero as well.</p>

<p>I saw a similar question <a href=""http://stats.stackexchange.com/questions/17696/significance-test-on-the-difference-of-spearmans-correlation-coefficient"">here</a> but seems too focused on some performed spearman correlation. Since this is not really R related but involves conceptual understanding of spearman test statistic as mentioned on R, I thought this would be more appropriated than stack overflow. </p>

<p>Thank you.</p>
"
"0.0419590679148345","0.0419960525565808"," 35936","<p>I am new to R and trying to practice with some exercises. Given a data set with 40  observations and 5 variables. Spending is the the response and there are 4 predictors. I started with a linear model Residuals:</p>

<pre><code>    Min      1Q  Median      3Q     Max 
-51.082 -11.320  -1.451   9.452  94.252 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  22.55565   17.19680   1.312   0.1968    
sex         -22.11833    8.21111  -2.694   0.0101 *  
status        0.05223    0.28111   0.186   0.8535    
income        4.96198    1.02539   4.839 1.79e-05 ***
verbal       -2.95949    2.17215  -1.362   0.1803    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 22.69 on 42 degrees of freedom
Multiple R-squared: 0.5267, Adjusted R-squared: 0.4816 
F-statistic: 11.69 on 4 and 42 DF,  p-value: 1.815e-06 
</code></pre>

<p>First, is this what they mean by fit regression model and Secondly, how do I compute the correlation of the residuals with the fitted values? </p>
"
"0.139162484826546","0.139285149006224"," 37840","<p>Okay, so I am trying to understand linear regression. I've got a data set and it looks all quite alright, but I am confused. This is my linear model-summary:</p>

<pre><code>Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 0.2068621  0.0247002   8.375 4.13e-09 ***
temp        0.0031074  0.0004779   6.502 4.79e-07 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.04226 on 28 degrees of freedom
Multiple R-squared: 0.6016, Adjusted R-squared: 0.5874 
F-statistic: 42.28 on 1 and 28 DF,  p-value: 4.789e-07 
</code></pre>

<p>so, the p-value is really low, which means it is very unlikely to get the correlation between x,y just by chance.
If I plot it and then draw the regression line it looks like this:
<a href=""http://s14.directupload.net/images/120923/l83eellv.png"" rel=""nofollow"">http://s14.directupload.net/images/120923/l83eellv.png</a>
(Had it in as a picture but I am - as a new user - currently not allowed to post it)
Blue lines = confidence interval
Green lines = prediction interval</p>

<p>Now, a lot of the points do not fall into the confidence interval, why would that happen? I think none of the datapoints falls on the regression line b/c they are just quite far away from each other, but what I am not sure of: Is this a real problem? They still are around the regression line and you can totally see a pattern. But is that enough?
I'm trying to figure it out, but I just keep asking myself the same questions over and over again.</p>

<p>What I thought of so far:
The confidence interval says that if you calculate CI's over and over again, in 95% of the times the true mean falls into the CI.
So: It it is not a problem that the dp do not fall into it, as these are not the means really.
The prediction interval on the other hand says, that if you calculate PI's over and over again, in 95% of the times the true VALUE falls into the interval. So, it is quite important to have the points in it (which I do have).
Then I've read the PI always has to have a wider range than the CI. Why is that?
This is what I have done:</p>

<pre><code>conf&lt;-predict(fm, interval=c(""confidence""))
prd&lt;-predict(fm, interval=c(""prediction""))
</code></pre>

<p>and then I plotted it by:</p>

<pre><code>matlines(temp,conf[,c(""lwr"",""upr"")], col=""red"")
matlines(temp,prd[,c(""lwr"",""upr"")], col=""red"")
</code></pre>

<p>Now, if I calculate CI and PI for additional data, it does not matter how wide I choose the range, I get the exact same lines as above. I cannot understand. What does that mean?
This would then be:</p>

<pre><code>conf&lt;-predict(fm,newdata=data.frame(x=newx), interval=c(""confidence""))
prd&lt;-predict(fm,newdata=data.frame(x=newx), interval=c(""prediction""))
</code></pre>

<p>for new x I chose different sequences.
If the sequence has a different # of observations than the variables in my regression, I am getting a warning. Why would that be?</p>
"
"0.0951542219543327","0.0952380952380952"," 38187","<p>I'm modelling a time series data using ARIMA. Now, I'm trying to test for the serial correlation of my model SARIMA(1,1,1) using the durbin watson test.</p>

<p>My problem is that I don't know what linear model I would put on the formula of the <code>dwtest</code> function in R. Here's the usage of the function,</p>

<pre><code>dwtest(formula, order.by = NULL, alternative = c(""greater"", ""two.sided"", ""less""),
       iterations = 15, exact = NULL, tol = 1e-10, data = list())
</code></pre>

<p>Here's my code below,</p>

<p>Data: <a href=""http://iitstat.weebly.com/uploads/7/3/4/0/7340846/chickenprod.rdata"" rel=""nofollow"">http://iitstat.weebly.com/uploads/7/3/4/0/7340846/chickenprod.rdata</a></p>

<p>To download the data just right click the link and click ""Save Link As...""</p>

<pre><code>library(forecast)
library(lmtest)
ChickenProd &lt;- ts(ChickenProd, start = 1980, frequency = 4)
SARIMA111 &lt;- Arima(ChickenProd, seasonal = list(order = c(1,1,1), period = 4))
</code></pre>

<p>The residuals of my model SARIMA111 is obtain by</p>

<pre><code>SARIMA111[[""residuals""]]
</code></pre>

<p>Now, I want to test the serial correlation of it using the Durbin-Watson test, but I don't know what linear model I would use in the <code>formula</code> argument of <code>dwtest</code> function in R. Is it the SARIMA(1,1,1) model? If so, how will I extract the coefficients of the SARIMA(1,1,1) model, and make a linear model formula in R?</p>

<p>Thank you in Advance!</p>
"
"0.139162484826546","0.139285149006224"," 38491","<p>If we have a spatial autoregressive process, we can estimate a model to control for the autoregression with a spatial lag,
$$y=\rho W y + X\beta + \epsilon$$
Where $\rho$ is the strength of the spatial correlation, and $W$ is a matrix of spatial weights. The <code>spdep</code> package for R contains the <code>lagsarlm</code> command which is designed to estimate precisely this model. The package contains methods for creating the weights. But there seems to be some discrepancy between the model fit between <code>lagsarlm()</code> and <code>lm()</code> fitted to what should be a similar model.</p>

<p>As an example, consider the example given with <code>?lagsarlm</code> in R. </p>

<pre><code>library(spdep)
data(oldcol)
COL.lag &lt;- lagsarlm(CRIME ~ INC + HOVAL, data=COL.OLD,
                nb2listw(COL.nb, style=""W""), method=""eigen"", quiet=TRUE)
summary(COL.lag)
Residuals:
      Min        1Q    Median        3Q       Max 
-37.68585  -5.35636   0.05421   6.02013  23.20555 

Type: lag 
Coefficients: (asymptotic standard errors) 
             Estimate Std. Error z value  Pr(&gt;|z|)
(Intercept) 45.079251   7.177347  6.2808 3.369e-10
INC         -1.031616   0.305143 -3.3808 0.0007229
HOVAL       -0.265926   0.088499 -3.0049 0.0026570

Rho: 0.43102, LR test value: 9.9736, p-value: 0.001588
Asymptotic standard error: 0.11768
    z-value: 3.6626, p-value: 0.00024962
Wald statistic: 13.415, p-value: 0.00024962
</code></pre>

<p>We can estimate what (I think) should be the same model by computing the actual spatial lag variable,</p>

<pre><code>crime.lag &lt;- lag.listw(nb2listw(COL.nb, style=""W""), COL.OLD$CRIME)
linearlag &lt;- lm(CRIME ~ crime.lag + INC + HOVAL, data=COL.OLD)
Residuals:
    Min      1Q  Median      3Q     Max 
-38.644  -6.103   0.266   6.563  21.610 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 38.18099    9.21531   4.143 0.000149 ***
crime.lag    0.55733    0.15029   3.709 0.000570 ***
INC         -0.86584    0.35541  -2.436 0.018864 *  
HOVAL       -0.26358    0.09136  -2.885 0.005986 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 10.12 on 45 degrees of freedom
Multiple R-squared: 0.6572, Adjusted R-squared: 0.6343 
F-statistic: 28.75 on 3 and 45 DF,  p-value: 1.543e-10 
</code></pre>

<p>The two models, which I think should be identical, are in fact significantly different from each other in every parameter and in model fit (with the <code>linearlag</code> model providing significantly lower AIC). Are there reasons why this should be? Why should I just not use the second model and abandon the special methods?</p>
"
"0.125877203744503","0.125988157669742"," 38524","<p>I got completely different results from lmer() and lme()! Just look at the coefficients' std.errors. Completely different in both cases. Why is that and which model is correct?</p>

<pre><code>&gt; mix1c = lmer(logInd ~ 0 + crit_i + Year:crit_i + (1 + Year|Taxon), data = datai)
&gt; mix1d = lme(logInd ~ 0 + crit_i + Year:crit_i, random = ~ 1 + Year|Taxon, data = datai)
&gt; 
&gt; summary(mix1d)
Linear mixed-effects model fit by REML
 Data: datai 
       AIC      BIC    logLik
  4727.606 4799.598 -2351.803

Random effects:
 Formula: ~1 + Year | Taxon
 Structure: General positive-definite, Log-Cholesky parametrization
            StdDev       Corr  
(Intercept) 9.829727e-08 (Intr)
Year        3.248182e-04 0.619 
Residual    4.933979e-01       

Fixed effects: logInd ~ 0 + crit_i + Year:crit_i 
                 Value Std.Error   DF   t-value p-value
crit_iA      29.053940  4.660176   99  6.234515  0.0000
crit_iF       0.184840  3.188341   99  0.057974  0.9539
crit_iU      12.340580  5.464541   99  2.258301  0.0261
crit_iW       5.324854  5.152019   99  1.033547  0.3039
crit_iA:Year -0.012272  0.002336 2881 -5.253846  0.0000
crit_iF:Year  0.002237  0.001598 2881  1.399542  0.1618
crit_iU:Year -0.003870  0.002739 2881 -1.412988  0.1578
crit_iW:Year -0.000305  0.002582 2881 -0.118278  0.9059
 Correlation: 
             crit_A crit_F crit_U crit_W cr_A:Y cr_F:Y cr_U:Y
crit_iF       0                                              
crit_iU       0      0                                       
crit_iW       0      0      0                                
crit_iA:Year -1      0      0      0                         
crit_iF:Year  0     -1      0      0      0                  
crit_iU:Year  0      0     -1      0      0      0           
crit_iW:Year  0      0      0     -1      0      0      0    

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-6.98370498 -0.39653580  0.02349353  0.43356564  5.15742550 

Number of Observations: 2987
Number of Groups: 103 
&gt; summary(mix1c)
Linear mixed model fit by REML 
Formula: logInd ~ 0 + crit_i + Year:crit_i + (1 + Year | Taxon) 
   Data: datai 
  AIC  BIC logLik deviance REMLdev
 2961 3033  -1469     2893    2937
Random effects:
 Groups   Name        Variance   Std.Dev. Corr   
 Taxon    (Intercept) 6.9112e+03 83.13360        
          Year        1.7582e-03  0.04193 -1.000 
 Residual             1.2239e-01  0.34985        
Number of obs: 2987, groups: Taxon, 103

Fixed effects:
               Estimate Std. Error t value
crit_iA      29.0539403 18.0295239   1.611
crit_iF       0.1848404 12.3352135   0.015
crit_iU      12.3405800 21.1414908   0.584
crit_iW       5.3248537 19.9323887   0.267
crit_iA:Year -0.0122717  0.0090916  -1.350
crit_iF:Year  0.0022365  0.0062202   0.360
crit_iU:Year -0.0038701  0.0106608  -0.363
crit_iW:Year -0.0003054  0.0100511  -0.030

Correlation of Fixed Effects:
            crit_A crit_F crit_U crit_W cr_A:Y cr_F:Y cr_U:Y
crit_iF      0.000                                          
crit_iU      0.000  0.000                                   
crit_iW      0.000  0.000  0.000                            
crit_iA:Yer -1.000  0.000  0.000  0.000                     
crit_iF:Yer  0.000 -1.000  0.000  0.000  0.000              
crit_iU:Yer  0.000  0.000 -1.000  0.000  0.000  0.000       
crit_iW:Yer  0.000  0.000  0.000 -1.000  0.000  0.000  0.000
&gt; 
</code></pre>
"
"0.10277830647413","0.102868899974728"," 39000","<p>I assessed the internal reliability of a self-created scale with eight items ($N = 150$) by calculating Cronbachâ€™s $\alpha$. It appears that one item correlates low with the overall score of the scale (item 4 in the example below). The corrected item-total correlation, i.e. the correlation of this item with the scale total excluding that item, is only $r= .046$. </p>

<pre><code>library(psych)
scale&lt;-mydata[,c(24,25,26,27,28,29,30,31)]
alpha(scale)

Reliability analysis   
Call: alpha(x = scale)

          0.62      0.64    0.66      0.18  4.3 0.79

 Reliability if an item is dropped:
      raw_alpha std.alpha G6(smc) average_r
item1      0.56      0.59    0.62      0.17
item2      0.53      0.57    0.58      0.16
item3      0.54      0.56    0.58      0.16
item4      0.66      0.67    0.67      0.23
item5      0.60      0.62    0.63      0.19
item6      0.55      0.59    0.62      0.17
item7      0.58      0.61    0.63      0.18
item8      0.63      0.65    0.67      0.21

 Item statistics 
        n    r r.cor r.drop mean   sd
item1 144 0.60  0.51  0.395  4.5 0.71
item2 145 0.65  0.62  0.499  4.6 0.71
item3 142 0.67  0.64  0.484  4.5 0.72
item4 146 0.33  0.15  0.046  4.6 0.81
item5 147 0.51  0.41  0.298  4.9 0.41
item6 139 0.59  0.50  0.404  4.4 0.82
item7 136 0.53  0.43  0.339  4.2 1.03
item8 135 0.39  0.21  0.190  4.3 0.94

Non missing response frequency for each item
         1    2    3    4    5 miss
item1 0.01 0.01 0.04 0.34 0.60 0.04
item2 0.01 0.01 0.03 0.24 0.71 0.03
item3 0.00 0.01 0.11 0.28 0.60 0.05
item4 0.01 0.03 0.05 0.14 0.77 0.03
item5 0.00 0.00 0.02 0.11 0.87 0.02
item6 0.01 0.02 0.10 0.29 0.58 0.07
item7 0.04 0.02 0.15 0.25 0.54 0.09
item8 0.02 0.03 0.11 0.32 0.52 0.10
</code></pre>

<p><strong>PROBLEM</strong>: I would like to report this low correlation with the degrees of freedom in parentheses and the significance level in the main text. Yet, I am not sure whether I calculated the correct p-value. What I did is a simple regression with item 4 as the dependent variable:</p>

<pre><code>scale &lt;- as.data.frame(scale)
summary(lm(item4 ~ item1+item2+item3+item5+item6+item7+item8, data=scale))

Call:
lm(formula = item4 ~ item1 + item2 + item3 + item5 + item6 + 
    item7 + item8, data = scale)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.4256 -0.0465  0.2869  0.3500  1.3405 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  1.65098    1.00288   1.646 0.102492    
item1        0.12916    0.11560   1.117 0.266262    
item2        0.02387    0.12921   0.185 0.853760    
item3       -0.07323    0.12718  -0.576 0.565921    
item5        0.64204    0.18636   3.445 0.000802 ***
item6       -0.04596    0.10230  -0.449 0.654120    
item7       -0.13217    0.08030  -1.646 0.102545    
item8        0.05609    0.08758   0.641 0.523136    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.8235 on 113 degrees of freedom
  (29 observations deleted due to missingness)
Multiple R-squared: 0.1385,     Adjusted R-squared: 0.08518 
F-statistic: 2.596 on 7 and 113 DF,  p-value: 0.01604
</code></pre>

<p><strong>QUESTION:</strong> Is it correct if I report something like ""Item 4 correlates only weakly with the overall score of the scale $(r(113)= .046, p= .02)$"" - or did I make a rather large error in reasoning here? </p>
"
"0.251846406179265","0.231630417138765"," 41362","<p>I have no formal training in statistics so please correct me if I use the wrong terms as I try to explain my problem.</p>

<p>I have a set of data (less than 80 points) with essentially 1 single outcome (a float we will call <code>dcl</code>) that can potentially depends on 10 of other variables, most of them floats maybe one or two boolean.</p>

<p>While I might ask some multi-variate regression question later, let's start with something simple. </p>

<p>Historically, people in my field have focused on the strongest correlation between <code>dcl</code> and variable <code>J</code> and some of my data suggests some other dependence on a float <code>Re</code> which is I'm sure at least weakly correlated with 'J' as they share some variables in their respective expressions. So my first question is how do I test the correlation and/or the independence of 'Re' and 'J' on the outcome 'dlc'? Intuitively and physically, I expect 'dlc' to depend strongly on 'J' and weakly on 'Re'. How do I prove this with a statistical analysis?</p>

<p>Here are a few graphs to illustrate the data:</p>

<p><img src=""http://i.stack.imgur.com/dVRMa.png"" alt=""Re vs J"">   </p>

<p><img src=""http://i.stack.imgur.com/hNPvb.png"" alt=""dcl vs J""></p>

<p><img src=""http://i.stack.imgur.com/D3YJe.png"" alt=""dcl vs Re""></p>

<p>Final point, in terms of software, I have python and R installed, I'm fairly proficient in python but I just installed R and know pretty much nothing about it.</p>

<p>EDIT 1: </p>

<p>Following gung's suggestion, I ran my dataset through R:</p>

<pre><code>Call:
lm(formula = dcl ~ J + I(J^2) + Re + I(Re^2), data = df)

Residuals:
    Min      1Q  Median      3Q     Max 
-9.0078 -3.7930 -0.4458  2.0869 21.2538 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  1.648e+01  1.232e+00  13.380  &lt; 2e-16 ***
J           -2.662e+00  3.773e-01  -7.054 6.58e-10 ***
I(J^2)       1.096e-01  2.071e-02   5.293 1.10e-06 ***
Re           1.966e-06  1.621e-05   0.121    0.904    
I(Re^2)      2.191e-11  3.441e-11   0.637    0.526    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 5.369 on 77 degrees of freedom
Multiple R-squared: 0.4831, Adjusted R-squared: 0.4562 
F-statistic: 17.99 on 4 and 77 DF,  p-value: 1.818e-10
</code></pre>

<p>So now I need some help to decipher this (but I will look into R documentation too). I don't know if it's relevant but on physical grounds only, it's probable the dependency in J is $dcl \sim \frac{1}{\sqrt{J}}$. Can I put this directly into the model? Does that already tell me something about the dependency on $J$ vs $Re$?</p>

<pre><code>Call:
lm(formula = dcl ~ J + I(J^(-0.5)) + Re + I(Re^(-0.5)), data = df)

Residuals:
    Min      1Q  Median      3Q     Max 
-7.8119 -3.0097 -0.8504  1.8506 12.1439 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   8.175e-01  1.634e+00   0.500   0.6184    
J            -2.946e-01  1.258e-01  -2.343   0.0217 *  
I(J^(-0.5))   4.516e+00  7.053e-01   6.403 1.09e-08 ***
Re            3.332e-05  6.684e-06   4.985 3.72e-06 ***
I(Re^(-0.5))  6.009e+02  1.354e+02   4.438 2.98e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 4.426 on 77 degrees of freedom
Multiple R-squared: 0.6487, Adjusted R-squared: 0.6305 
F-statistic: 35.55 on 4 and 77 DF,  p-value: &lt; 2.2e-16

&gt; model = lm(dcl ~ J+I(J^(-0.5)) + Re+I(Re^(-0.5)), data=df)
&gt; summary(model)
</code></pre>

<p><strong>EDIT 2</strong>: OK I'm starting to understand things better. Also, again based on physical grounds, I would think that the dependency is more something like $dcl ~ \frac{1}{\sqrt{J}} Re^{n}$ with possibly other variables in that product that I ignore. So when I enter such model in R (can I still use <code>lm</code> for something non-linear?), here is what I get:</p>

<pre><code>Call:
lm(formula = dcl ~ I(J^(-0.5)) * I(Re^(-0.1)), data = df)

Residuals:
     Min       1Q   Median       3Q      Max 
-10.5363  -3.0192  -0.2556   1.4373  17.1494 

Coefficients:
                         Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)               -43.220      9.164  -4.716 1.03e-05 ***
I(J^(-0.5))                63.813     11.088   5.755 1.62e-07 ***
I(Re^(-0.1))              124.245     24.038   5.169 1.77e-06 ***
I(J^(-0.5)):I(Re^(-0.1)) -142.744     27.269  -5.235 1.36e-06 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 4.668 on 78 degrees of freedom
Multiple R-squared: 0.6042, Adjusted R-squared: 0.5889 
F-statistic: 39.68 on 3 and 78 DF,  p-value: 1.122e-15
</code></pre>

<p>Does that 4th line tell me something about the dependence between $J$ and $Re$? What kind of tools could I use to get an estimate on the exponent on Re? Because right now I'm just trying a few different numbers empirically to see how the errors evolve. Next for me is to plot the dcl vs the new model and see how well the data collapses visually...</p>

<p>EDIT 3:</p>

<p>In the end, I used <code>nls</code> to explore the possible exponents of my fit. I also removed some outliers in my data that used a different experimental method. I settled on a fit that gave me decent Pr(>|t|) and residuals and which visually produce a decent fit. The last outlier is actually another experiment with a different setup, but one that I trust. So in a sense it's good that it shows up as an outlier as it hints at other parameters that need to be explored. Thank you gung, I accept your answer as I believe it guided me in the right direction.</p>

<pre><code>&gt; model = nls(L.D ~ C*I(J^(c1))*I(Re_s^(c2)), start=list(C=10,c1=-0.25,c2=-0.25),data=df)
&gt; summary(model)
Formula: L.D ~ C * I(J^(c1)) * I(Re_s^(c2))
Parameters:
   Estimate Std. Error t value Pr(&gt;|t|)    
C  57.20389   26.40011   2.167   0.0337 *  
c1 -0.27721    0.05901  -4.698 1.27e-05 ***
c2 -0.16424    0.04936  -3.327   0.0014 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
Residual standard error: 4 on 70 degrees of freedom
Number of iterations to convergence: 8 
Achieved convergence tolerance: 2.91e-06 
</code></pre>

<p><img src=""http://i.stack.imgur.com/dfgWX.png"" alt=""enter image description here""></p>
"
"0.132686223108569","0.132803178814933"," 43109","<p><strong>My goal is to generate two variable which are correlated and one of them is heteroscedastic with regards to an grouping variable.</strong></p>

<p>To create two variables with a desired correlation the common way to go is to use the cholesky decomposition. I additionally used the residuals of the orthogonal projection and standardized the variables up front to make sure the correlation stays at the desired value.</p>

<p>To introduce heteroscedasticity to one of the variables i tried the following:</p>

<ol>
<li><p>I did the cholesky decomposition for $u=(u_1,u_2), \ q[i] =
    c(1,1.01,1.02,1.03,...)$</p>

<p>$Var(u) = \begin{pmatrix} 1 &amp; \rho\cdot q[i] \\ \rho\cdot q[i] &amp;
    q[i]^2 \end{pmatrix}$</p>

<p>The reason why I choose this covariance matrix is that while the
variance of $u_2$ increases with $q[i]$ the correlation stays at the
desired value $\rho$.</p>

<p>But this does obviously not introduce heteroscedasticity to $u_2$....</p></li>
<li><p>A second attempt was to decomposed both variables $u_1,u_2$ with sample size n into two
parts each of length $\frac{1}{2}\cdot n$: $\ u_1 = (u_{1.1},u_{1.2}), u_2 =
    (u_{2.1},u_{2.2})$.     Now i did the cholesky decomposition for</p>

<p>$Var(u) = \begin{pmatrix} 1 &amp; 0 &amp; \rho &amp; \rho\sqrt{q[i]} \\ 0 &amp; 1 &amp;
    \rho &amp; \rho\sqrt{q[i]} \\ \rho &amp; \rho &amp; 1 &amp; 0 \\ \rho\sqrt{q[i]} &amp;
    \rho\sqrt{q[i]} &amp; 0 &amp; q[i] \end{pmatrix}$</p>

<p>After I generated the four variables I
merged $u_{1.1},u_{1.2}$ back to one $n\times 1$ vector. Analogous
for $u_2$. Testing the outcome yields that the heteroscedasticity is now where it   should be as the variance of $u_{2.2}$ increases with $q[i]$. But the correlation vanishes as
$q[i]$ increases because the variance of $u_2$ increases more
rapidly then the covariance between $u_1$ and $u_2$</p></li>
<li><p>A third attempt which i end up doing was to just multiply $u_2$ with
a dummy-variable</p>

<p>$z_2 =\begin{cases}   1\\   q[i] \end{cases}$ </p>

<p>This approach does work for
small values of $q[i]$ (as my graphic shows) but as soon as $q[i]$ is
large enough it dominates the nominator and denominator of the
correlation coefficient such that the conditional variance
(depending on the value of $z_2$) does no longer increase in $q[i]$.</p></li>
</ol>

<p>Here is my code for case (3):</p>

<pre><code>q     &lt;- seq(1.01,10,0.1)
n         &lt;- 100
rho   &lt;- 0.5
sd_u1     &lt;- numeric(0)
sd_u2.1   &lt;- numeric(0)
sd_u2.2   &lt;- numeric(0)
cor_u1_u2 &lt;- numeric(0)


for(i in 1:length(q)){
u1      &lt;- rnorm(n,0,1)
u1      &lt;- ( u1 - mean(u1) )/sd(u1)
u2      &lt;- rnorm(n,0,1)
z2      &lt;- c(rep(1,0.5*n),rep(q[i],0.5*n))
u2      &lt;- u2*z2
u2      &lt;- as.vector( ( diag(n) - u1%*%solve(t(u1)%*%u1)%*%t(u1) ) %*% u2 )
u2      &lt;- ( u2 - mean(u2) )/sd(u2)
z       &lt;- cbind(u1, rho*u1+sqrt(1-rho^2)*u2) 
sd_u1[i]      &lt;- sd(z[,1])
sd_u2.1[i]    &lt;- sd(z[,2][1:(0.5*n)])
sd_u2.2[i]    &lt;- sd(z[,2][(0.5*n+1):n])
cor_u1_u2[i]  &lt;- cor(z[,1],z[,2])
}
par(mfrow=c(3,1))
plot(q,sd_u1, type=""l"")
plot(q,sd_u2.1, type=""l"", ylim=c(0,2))
lines(q,sd_u2.2,col=""red"")
plot(q,cor_u1_u2, type=""l"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/5kDJn.jpg"" alt=""enter image description here"">    </p>
"
"0.0839181358296689","0.0839921051131616"," 43458","<p>I am trying to fit some variables which are high collinear using nnls(non negative least squared routine) in R. </p>

<p>I have about 1 million observations and 200 highly collinear variables for the coefficients to be found.</p>

<p>My problem is that in the final result, the algorithm seems to select variables that have pretty low correlation with the independent variable, it seems to throw the variables that have a high correlation in favor for the ones with a low correlation. I dont seem to understand why. Has anyone run into something similar, any way I can modify to pick variables in order of correlation, I am using the standard NNLS package in R.</p>
"
"0.0726752374667264","0.0484928644968872"," 47308","<p>I have created 5 imputations of a dataset and have fit a survival model to them all in R. I want to combine the estimates of the coefficients and the standard errors of the coefficients. To do this I have taken the mean of the coefficients and combined the standard errors using Rubins formula, i.e. 
$$(1+1/m) \times \text{between imputation variance} + \text{within imputation variance}$$</p>

<p>I was thinking about it and I'm having some doubts as to whether this makes sense? Wouldn't the variance be over estimated in this case? I performed a t-test on the coefficients and it results in a relatively high p-value when I know the data should show a good correlation. I use Rubins df formula for the t-test. Is there an alternative method of producing the combine variance, one that doesn't result in over estimation?</p>
"
"0.0839181358296689","0.0629940788348712"," 47987","<p>Going through the <a href=""http://en.wikipedia.org/wiki/Phi_coefficient"" rel=""nofollow"">Wiki article</a> on the Phi coefficient, I've noticed that for paired binary data ""a Pearson correlation coefficient estimated for two binary variables will return the phi coefficient"".</p>

<p>Upon running a quick simulation I found this to not be the case.  However, it appears that the phi coefficient does approximate the pearson's correlation coefficient.</p>

<pre><code>x &lt;- c(1,   1,  0,  0,  1,  0,  1,  1,  1)
y &lt;- c(1,   1,  0,  0,  0,  0,  1,  1,  1)
cor(x,y)
sqrt(chisq.test(table(x,y))$statistic/length(x)) # phi

x &lt;- rep(x, 1000)
y &lt;- rep(y, 1000)
sqrt(chisq.test(table(x,y))$statistic/length(x)) # phi
# it now DOES approximates the pearsons correlation.
cor(x,y)
</code></pre>

<p><strong>But it is not apparent to me why (mathematically) this is the case.</strong></p>
"
"0.125877203744503","0.125988157669742"," 50167","<p>I would like to know if there is a way to get p-values when using the GLS function (part of the nlme package) for the ML estimate of the error-autoregressive parameters. I looked through the package information on nlme and have not found a clear example of how to do this.</p>

<p>Here is an example of some code that illustrates my question:</p>

<pre><code>x &lt;- rnorm(100)
y &lt;- rnorm(100)
z &lt;- rnorm(100)
df &lt;- data.frame(x,y,z)

test.reg &lt;- gls(x ~ y+z,data=df,correlation=corARMA(p=1),method=""ML"")

summary(test.reg)

Generalized least squares fit by maximum likelihood
  Model: x ~ y + z 
  Data: df 
   AIC      BIC    logLik
  289.3276 302.3534 -139.6638

Correlation Structure: AR(1)
 Formula: ~1 
 Parameter estimate(s):
   Phi 
0.01887445 

Coefficients:
                 Value  Std.Error   t-value p-value
(Intercept) 0.08418063 0.10214246 0.8241492  0.4119
y           0.00236772 0.09410912 0.0251593  0.9800
z           0.07689263 0.09480738 0.8110405  0.4193

 Correlation: 
  (Intr) y     
y  0.135       
z -0.013  0.074

Standardized residuals:
        Min          Q1         Med          Q3         Max 
-1.94366502 -0.66310726  0.09546043  0.64881478  2.78384648 

Residual standard error: 0.9781187 
Degrees of freedom: 100 total; 97 residual
</code></pre>

<p>What I would like to know more about is the Parameter estimate Phi. I have searched through the nlme package to get more information about p-values for Phi but have come up short. I am unsure if the gls function computes the p-values for Phi. </p>

<p>I do know this gives the coefficients of phi:</p>

<pre><code>coeff &lt;- test.reg$modelStruct$corStruct

Correlation structure of class corAR1 representing
   Phi 
0.01887445 
</code></pre>

<p>But that is all I have really found. Any help would be great!</p>

<p>Thanks so much,
John</p>
"
"0.0726752374667264","0.0727392967453308"," 50561","<p>I am trying to calculate the Intraclass Correlation for a rater study using R and the library lme4 and the function lmer. The data has the following design: The same 6 raters (at least 4) are rating 25 horses live and all raters are rating a subset of 10 horses on video. </p>

<p>The two way random model applied:</p>

<pre><code>m1 &lt;- lmer(Score ~ -1 + (1|HorseID) + (1|RaterID) + Time, data=mydata)
</code></pre>

<p>The absolute agreement ICC is calculated using the estimated coefficients:</p>

<pre><code>xVars &lt;- function(model) {
exvars = lme4::VarCorr(model)
vars = c(exvars$HorseID[1,1], exvars$RaterID[1,1], attr(exvars,""sc"")^2) 
names(vars) &lt;- c('item var', 'judge var', 'residual var')
vars }

# helper function for ICC(k) variations

icck &lt;- function(variances, k=1) {
icc = variances[1] / (variances[1] + (variances[2] + variances[3]) / k)
names(icc) = c(paste('ICC', k, sep=''))
icc }
</code></pre>

<p>And the ICC as:</p>

<pre><code>ICC.m1 &lt;- icck(xVars(m1))
</code></pre>

<p>I would like to add:</p>

<ul>
<li>Confidence Intervals for the ICC</li>
<li>Cronbach's alpha</li>
</ul>

<p>But I can't figure out at smart way of doing so? Help would be greatly appreciated!!</p>
"
"0.0938233281301002","0.0751248226425348"," 51155","<p>@John recently pointed out to me that R's <code>poly</code> function produces less correlated values (more orthogonal) to fit polynomial predictors, i.e. the transformed predictors have a lower correlation with each other than if I just mean centered the 1st order predictor prior to generating the nth order predictor(s).  R's poly accomplishes this using an algorithm from Kennedy, W. J. Jr and Gentle, J. E. (1980) Statistical Computing Marcel Dekker, pg 343-344.  I checked the book out from the library, but I have to say that I am not quite following what is going on.  I've also made some plots comparing these values to the raw values and I sort of can see what is going on, but I still don't I fully understand it.  It seems like the data is being processed and subjected to some transformation that involves the mean of the values and some prediction coefficients.  I can always use <code>predict.poly</code> to turn the values from real values to <code>poly</code> values.  My question is how should I report the results of a model fit with <code>poly</code>?  The raw slope coefficients refer to a transformed value, if I report them, then I assume I should also report some of the coefficients from <code>poly</code> itself as well.  Which ones?  In any particular format?  Is anybody going to really understand them?  Is this a compelling reason to stick to raw mean-centered coefficients?</p>
"
"0.132686223108569","0.132803178814933"," 56374","<p>as I am stepping into forecasting with ARIMA models, I am trying to understand how I can improve a forecast based on ARIMA fit with seasonality and drift. </p>

<p>My data is the following time series ( over 3 years, with clear trend upwards and visible seasonality, which seems to be not supported by autocorrelation at lags 12, 24, 36??). </p>

<pre><code>    &gt; bal2sum3years.ts
             Jan     Feb     Mar     Apr     May     Jun     Jul     Aug          
    2010 2540346 2139440 2218652 2176167 2287778 1861061 2000102 2560729 
    2011 3119573 2704986 2594432 2362869 2509506 2434504 2680088 2689888 
    2012 3619060 3204588 2800260 2973428 2737696 2744716 3043868 2867416 
             Sep     Oct     Nov     Dec
    2010 2232261 2394644 2468479 2816287
    2011 2480940 2699780 2760268 3206372
    2012 2951516 3119176 3032960 3738256
</code></pre>

<p>The model that was suggested by <code>auto.arima(bal2sum3years.ts)</code> gave me the following model:</p>

<pre><code>    Series: bal2sum3years.ts 
    ARIMA(0,0,0)(0,1,0)[12] with drift         

    Coefficients:
              drift
          31725.567
    s.e.   2651.693

    sigma^2 estimated as 2.43e+10:  log likelihood=-321.02
    AIC=646.04   AICc=646.61   BIC=648.39
</code></pre>

<p>However, the <code>acf(bal2sum3years.ts,max.lag=35)</code> does not show acf coefficients higher than 0.3. The seasonality of the data is, however, pretty obvious - spike at the beginning of every year. This is what the series looks like on the graph:
<img src=""http://i.stack.imgur.com/kQi5N.png"" alt=""Original Time Series""></p>

<p>The forecast using <code>fit=Arima(bal2sum3years.ts,seasonal=list(order=c(0,1,0),period=12),include.drift=TRUE)</code> , called by function <code>forecast(fit)</code>, results in the next 12months's means being equal to the last 12 months of the data plus constant. This can be seen by calling <code>plot(forecast(fit))</code>, </p>

<p><img src=""http://i.stack.imgur.com/GJqcG.png"" alt=""Actual and Forecasted Data""></p>

<p>I have also checked the residuals, which are not autocorrelated but have positive mean ( non zero). </p>

<p>The fit does not model the original time series precisely, in my opinion ( blue the original time series, red is the <code>fitted(fit)</code>:</p>

<p><img src=""http://i.stack.imgur.com/ux3i7.png"" alt=""Original vs fit""></p>

<p>The guestion is, is the model incorrect? Am I missing something? How can I improve the model? It seems that the model literally takes the last 12 months and adds a constant to achieve the next 12 months. </p>

<p>I am a relative beginner in time series forecasting models and statistics. </p>

<p>Thank you very much for your answers!</p>
"
"0.196805473415241","0.19697894676176"," 56695","<p>I understand that we use random effects (or mixed effects) models when we believe that some model parameter(s) vary randomly across some grouping factor. I have a desire to fit a model where the response has been normalized and centered (not perfectly, but pretty close) across a grouping factor, but an independent variable <code>x</code> has not been adjusted in any way. This led me to the following test (using <em>fabricated</em> data) to ensure that I'd find the effect I was looking for if it was indeed there. I ran one <em>mixed</em> effects model with a random intercept (across groups defined by <code>f</code>) and a second <em>fixed</em> effect model with the factor f as a fixed effect predictor. I used the R package <code>lmer</code> for the mixed effect model, and the base function <code>lm()</code> for the fixed effect model. Following is the data and the results. </p>

<p>Notice that <code>y</code>, regardless of group, varies around 0. And that <code>x</code> varies consistently with <code>y</code> within group, but varies much more across groups than <code>y</code></p>

<pre><code>&gt; data
      y   x f
1  -0.5   2 1
2   0.0   3 1
3   0.5   4 1
4  -0.6  -4 2
5   0.0  -3 2
6   0.6  -2 2
7  -0.2  13 3
8   0.1  14 3
9   0.4  15 3
10 -0.5 -15 4
11 -0.1 -14 4
12  0.4 -13 4
</code></pre>

<p>If you're interested in working with the data, here is <code>dput()</code> output:</p>

<pre><code>data&lt;-structure(list(y = c(-0.5, 0, 0.5, -0.6, 0, 0.6, -0.2, 0.1, 0.4, 
-0.5, -0.1, 0.4), x = c(2, 3, 4, -4, -3, -2, 13, 14, 15, -15, 
-14, -13), f = structure(c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 
4L, 4L, 4L), .Label = c(""1"", ""2"", ""3"", ""4""), class = ""factor"")), 
.Names = c(""y"",""x"",""f""), row.names = c(NA, -12L), class = ""data.frame"")
</code></pre>

<p>Fitting the mixed effects model:</p>

<pre><code>&gt; summary(lmer(y~ x + (1|f),data=data))
Linear mixed model fit by REML 
Formula: y ~ x + (1 | f) 
   Data: data 
   AIC   BIC logLik deviance REMLdev
 28.59 30.53  -10.3       11   20.59
Random effects:
 Groups   Name        Variance Std.Dev.
 f        (Intercept) 0.00000  0.00000 
 Residual             0.17567  0.41913 
Number of obs: 12, groups: f, 4

Fixed effects:
            Estimate Std. Error t value
(Intercept) 0.008333   0.120992   0.069
x           0.008643   0.011912   0.726

Correlation of Fixed Effects:
  (Intr)
x 0.000 
</code></pre>

<p>I note that the intercept variance component is estimated 0, and importantly to me, <code>x</code> is not a significant predictor of <code>y</code>.</p>

<p>Next I fit the fixed effect model with <code>f</code> as a predictor instead of a grouping factor for a random intercept:</p>

<pre><code>&gt; summary(lm(y~ x + f,data=data))

Call:
lm(formula = y ~ x + f, data = data)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.16250 -0.03438  0.00000  0.03125  0.16250 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -1.38750    0.14099  -9.841 2.38e-05 ***
x            0.46250    0.04128  11.205 1.01e-05 ***
f2           2.77500    0.26538  10.457 1.59e-05 ***
f3          -4.98750    0.46396 -10.750 1.33e-05 ***
f4           7.79583    0.70817  11.008 1.13e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.1168 on 7 degrees of freedom
Multiple R-squared: 0.9484, Adjusted R-squared: 0.9189 
F-statistic: 32.16 on 4 and 7 DF,  p-value: 0.0001348 
</code></pre>

<p>Now I notice that, as expected, <code>x</code> is a significant predictor of <code>y</code>.</p>

<p><strong>What I am looking for</strong> is intuition regarding this difference. In what way is my thinking wrong here? Why do I incorrectly expect to find a significant parameter for <code>x</code> in both of these models but only actually see it in the fixed effect model?</p>
"
"0.0726752374667264","0.0727392967453308"," 57448","<p>I am running a model (logistic regression) with 20 independent variables in R. </p>

<p>Before running the model I calculated the correlation between all the variables and finally selected my variables by also checking ""visually"" the histograms of each variable in the case of presence and again in the case of absence. In situations where I don't see any obvious distribution associated to both presence &amp; absence, I discard the variable.</p>

<p>I would like to make ""official"" calculations for the level of relation between Presence/Absence and each variable (how much each variable contributes to the Presence/Absence), for example with <code>Cramer's V index</code>, but the available function I find is from the package <code>vcd</code> and has some limitations: 
doesn't give the <code>Cramer's V</code> (as well as the Phi-Coefficient Contingency Coeff.) for each independent variable, and it doesn't run for one independent variable.</p>

<p>I might be missing some other obvious way to do this. Any help is appreciated.</p>
"
"NaN","NaN"," 58540","<p>I'm going through my copy of Analysis of Financial Time Series, 2nd Edition, and I'm at the ARMA portion. One of the techniques for model selection is computation of the extended auto-correlation function, which indicates that, in a table of EACF's with MA coefficients listed across the top and AR coefficients listed down the side, the top-leftmost EACF that is less than the absolute value of 2 times the standard error of the EACF is at the position that indicates a good choice for the orders of the model. Can someone please explain to me exactly what the EACF is?</p>
"
"0.0839181358296689","0.0839921051131616"," 58811","<p>1. Which one is NOT a linear regression models? Please give a 1-2 sentences brief
explanation to your choice.<br>
(a)  $y_i = Î²_0 +\exp(Î²_1x_i)+E_i, i = 1, 2, \ldots, n$<br>
(b)  $y_i = Î²_0 + Î²_1x_i + Î²_2 x_{ii} + E_i , i = 1, 2, \ldots, n$<br>
(c)  $y_i =Î²_0\exp(x_i)+Î²_2x_i^7 +E_i, i=1, 2,\ldots, n$  </p>

<p>2. Suppose $X$ and $Y$ has linear correlation coefficient $r = 0.5$, and there are 77 observations, what is the test statistic for the hypothesis test  </p>

<p>$$H0:Î²_1=0 \quad\text{vs.}\quad Ha:Î²_1\neq0 $$</p>

<p>where $Î²_1$ comes from the simple linear regression model below? Please give a 1-2
sentences brief explanation to your choice.   $\quad Y = Î²_0 + Î²_1X + E$  </p>

<p>(a). Not enough information<br>
(b). 5<br>
(c). 0.25  </p>

<p>3. Which model is more possible to have smaller $R^2$? Please give a 1-2 sentences brief explanation to your choice.<br>
A: $Y=Î²_0+Î²_1X_1+E$<br>
B: $Y=Î²_0^*+Î²_1^*X_1+Î²_2^*X_2+E^*$<br>
where $Y$ and $X_1$ in model A and B are the same.</p>

<p>(a). Not enough information<br>
(b). Model A<br>
(c). Model B  </p>
"
"0.145350474933453","0.145478593490662"," 58900","<p>I have very recently started learning about Generalised Linear Mixed Models and was using R to explore what difference it makes to treat group membership as either fixed or random effect. In particular, I am looking at the example dataset discussed here:</p>

<p><a href=""http://www.ats.ucla.edu/stat/mult_pkg/glmm.htm"">http://www.ats.ucla.edu/stat/mult_pkg/glmm.htm</a></p>

<p><a href=""http://www.ats.ucla.edu/stat/r/dae/melogit.htm"">http://www.ats.ucla.edu/stat/r/dae/melogit.htm</a></p>

<p>As outlined in this tutorial, the effect of Doctor ID is appreciable and I was expecting the mixed model with a random intercept to give better results. However, comparing AIC values for the two methods suggest that this model is worse:</p>

<pre><code>&gt; require(lme4) ; hdp = read.csv(""http://www.ats.ucla.edu/stat/data/hdp.csv"")
&gt; hdp$DID = factor(hdp$DID) ; hdp$Married = factor(hdp$Married)
&gt; GLM = glm(remission~Age+Married+IL6+DID,data=hdp,family=binomial);summary(GLM)

Call:
glm(formula = remission ~ Age + Married + IL6 + DID, family = binomial, 
data = hdp)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.5265  -0.6278  -0.2272   0.5492   2.7329  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -1.560e+01  1.219e+03  -0.013    0.990    
Age         -5.869e-02  5.272e-03 -11.133  &lt; 2e-16 ***
Married1     2.688e-01  6.646e-02   4.044 5.26e-05 ***
IL6         -5.550e-02  1.153e-02  -4.815 1.47e-06 ***
DID2         1.805e+01  1.219e+03   0.015    0.988    
DID3         1.932e+01  1.219e+03   0.016    0.987   

[...]

DID405       1.566e+01  1.219e+03   0.013    0.990    
DID405       1.566e+01  1.219e+03   0.013    0.990    
DID406      -2.885e-01  3.929e+03   0.000    1.000    
DID407       2.012e+01  1.219e+03   0.017    0.987    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 10353  on 8524  degrees of freedom
Residual deviance:  6436  on 8115  degrees of freedom
AIC: 7256

Number of Fisher Scoring iterations: 17


&gt; GLMM = glmer(remission~Age+Married+IL6+(1|DID),data=hdp,family=binomial) ; m

Generalized linear mixed model fit by the Laplace approximation 
Formula: remission ~ Age + Married + IL6 + (1 | DID) 
Data: hdp 
AIC  BIC logLik deviance
7743 7778  -3867     7733
Random effects:
Groups Name        Variance Std.Dev.
DID    (Intercept) 3.8401   1.9596  
Number of obs: 8525, groups: DID, 407

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  1.461438   0.272709   5.359 8.37e-08 ***
Age         -0.055969   0.005038 -11.109  &lt; 2e-16 ***
Married1     0.260065   0.063736   4.080 4.50e-05 ***
IL6         -0.053288   0.011058  -4.819 1.44e-06 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
         (Intr) Age    Marrd1
Age      -0.898              
Married1  0.070 -0.224       
IL6      -0.162  0.012 -0.033


&gt; extractAIC(GLM) ; extractAIC(GLMM)

[1]  410.000 7255.962
[1]    5.000 7743.188
</code></pre>

<p>Thus, my questions are:</p>

<p>(1) Is it appropriate to compare the AIC values provided by the two functions? If so, why does the fixed effect model do better?</p>

<p>(2) What is the best way to identify if fixed or random effects are more important (ie to quantify that the variability due to the doctor is more important than patient characteristics?</p>
"
"0.10277830647413","0.0857240833122733"," 59561","<p>When I run this code:</p>

<pre><code>require(nlme)

a &lt;- matrix(c(1,3,5,7,4,5,6,4,7,8,9))

b &lt;- matrix(c(3,5,6,2,4,6,7,8,7,8,9))

res &lt;- lm(a ~ b)

print(summary(res))

res_gls &lt;- gls(a ~ b)

print(summary(res_gls))
</code></pre>

<p>I get the same coefficients and the same statistical significance on the coefficients:</p>

<pre><code>Loading required package: nlme

Call:
lm(formula = a ~ b)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.7361 -1.1348 -0.2955  1.2463  3.8234 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)   2.0576     1.8732   1.098   0.3005  
b             0.5595     0.2986   1.874   0.0937 .
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 2.088 on 9 degrees of freedom
Multiple R-squared: 0.2807, Adjusted R-squared: 0.2007 
F-statistic: 3.512 on 1 and 9 DF,  p-value: 0.09371 

Generalized least squares fit by REML
  Model: a ~ b 
  Data: NULL 
      AIC      BIC    logLik
  51.0801 51.67177 -22.54005

Coefficients:
                Value Std.Error  t-value p-value
(Intercept) 2.0576208 1.8731573 1.098477  0.3005
b           0.5594796 0.2985566 1.873948  0.0937

 Correlation: 
  (Intr)
b -0.942

Standardized residuals:
       Min         Q1        Med         Q3        Max 
-1.3104006 -0.5434780 -0.1415446  0.5968911  1.8311781 

Residual standard error: 2.087956 
Degrees of freedom: 11 total; 9 residual
</code></pre>

<p>Why is this happening? In what cases do OLS estimates are the same as GLS estimates? </p>
"
"0.201228620594782","0.183892428122457"," 59912","<p>So I often do little self-experiments where I blind &amp; randomize things; these can be formulated as your normal <em>t</em>-tests, but sometimes the measured metrics have extensive baselines which seem like they could be used for more accurate answers. A bunch of reading upon <em>n</em>-of-1 and single-subject designs suggested that people have been moving to mixed/hierarchical/multilevel models for analyzing such setups (eg. Nelson 2012 <a href=""http://etd.lsu.edu/docs/available/etd-04252012-152015/unrestricted/NelsonDiss.pdf"" rel=""nofollow"">""Hierarchical linear modeling versus visual analysis of single subject design data""</a> or <a href=""http://www.eric.ed.gov/PDFS/EJ800974.pdf"" rel=""nofollow"" title=""Van den Noortgate et al 2007"">""The Aggregation of Single-Case Results using Hierarchical Linear Models""</a>).</p>

<p>As I understand it, the idea is to split the subject's data into experiment vs baseline, and treat those as the groups. I'm trying to understand how sensible this is with a recent experiment, so hopefully someone can point out if I go wrong in using <code>lmer</code> here.</p>

<hr>

<p>We start with a regular linear model which examines purely the experimental data (the numeric <code>Response</code> vs the binary <code>Intervention</code> variables) and ignores the extensive baseline phase before, during, and after the experiment:</p>

<pre><code>R&gt; experiment &lt;- read.csv(""http://dl.dropboxusercontent.com/u/85192141/data.csv"")
R&gt; summary(lm(Response ~ Intervention, data=experiment))

...
Residuals:
    Min      1Q  Median      3Q     Max
-1.0156 -0.8889 -0.0156  0.1111  1.1111

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)    3.0156     0.0889    33.9   &lt;2e-16
Intervention  -0.1267     0.1262    -1.0     0.32

Residual standard error: 0.711 on 125 degrees of freedom
  (145 observations deleted due to missingness)
Multiple R-squared:  0.008, Adjusted R-squared:  6.73e-05
F-statistic: 1.01 on 1 and 125 DF,  p-value: 0.317

R&gt; confint(lm(Response ~ Intervention, data=experiment))
               2.5 % 97.5 %
(Intercept)   2.8397  3.192
Intervention -0.3765  0.123
</code></pre>

<p>The estimated coefficient is not statistically-significant: -0.38-0.12. But it's definitely slanted towards being negative. So this is the 'conservative' case, where we ignore the baseline entirely. What's the optimistic case? Well, it seems to me that the optimistic case is when we take the entire baseline and assume it is exactly the same as the 'off'/0 intervention in the experiment, in which case we get a narrower CI (because our estimate of the intercept has halved its standard error):</p>

<pre><code>R&gt; experiment$Intervention[is.na(experiment$Intervention)] &lt;- 0
R&gt; summary(lm(Response ~ Intervention, data=experiment))

...
Residuals:
    Min      1Q  Median      3Q     Max 
-1.9924 -0.8889  0.0076  1.0076  1.1111 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)    2.9924     0.0375   79.88   &lt;2e-16
Intervention  -0.1036     0.1012   -1.02     0.31

Residual standard error: 0.746 on 458 degrees of freedom
Multiple R-squared:  0.00228,   Adjusted R-squared:  0.000101 
F-statistic: 1.05 on 1 and 458 DF,  p-value: 0.307

R&gt; confint(lm(Response ~ Intervention, data=experiment))
               2.5 %  97.5 %
(Intercept)   2.9188 3.06607
Intervention -0.3025 0.09538
</code></pre>

<p>It's narrowed to -0.30-0.10; still not statistically-significant, but closer.</p>

<p>It seems to me that a hierarchical model ought to produce a CI intermediate between the pessimistic and optimistic cases: it loses some power because it's estimating how different the two phases are before it does any combining.</p>

<p>Here is my multilevel model, split between baseline and experimental phases:</p>

<pre><code>library(lme4)
experiment &lt;- read.csv(""http://dl.dropboxusercontent.com/u/85192141/data.csv"")
experiment$Phase &lt;- ifelse(is.na(experiment$Intervention), TRUE, FALSE)
model &lt;- lmer(Response ~ Intervention + (1|Phase), data=experiment); summary(model)

...
 AIC BIC logLik deviance REMLdev
 286 297   -139      273     278
Random effects:
 Groups   Name        Variance Std.Dev.
 Phase    (Intercept) 0.0106   0.103   
 Residual             0.5057   0.711   
Number of obs: 127, groups: Phase, 1

Fixed effects:
             Estimate Std. Error t value
(Intercept)     3.016      0.136    22.2
Intervention   -0.127      0.126    -1.0

Correlation of Fixed Effects:
            (Intr)
Interventin -0.461

m &lt;- mcmcsamp((lmer(Response ~ Intervention + (1|Phase), data=experiment)), n = 100000)
HPDinterval(m, prob=0.95)$fixef

                lower   upper
(Intercept)  -45.3107 56.6558
Intervention  -0.3742  0.1191
</code></pre>

<p>The estimated CI comes out exactly in the middle, as expected:</p>

<ol>
<li>pessimistic   -0.38 0.12</li>
<li>hierarchical  -0.37 0.11</li>
<li>optimistic    -0.30 0.10</li>
</ol>

<p>So, my basic question is: is this a sane approach to take? It's spitting out answers that seem intuitively correct, but that might just be a coincidence.</p>

<hr>

<p>Incidentally, one might be worried about time trends. The randomization/blocking would fix that in the experimental period but not the baseline. Fortunately, that doesn't seem to be an issue:</p>

<pre><code>experiment$Time &lt;- 1:nrow(experiment)
summary(lmer(Response ~ Intervention + Time + (1|Phase), data=experiment))

...
 AIC BIC logLik deviance REMLdev
 298 312   -144      272     288
Random effects:
 Groups   Name        Variance Std.Dev.
 Phase    (Intercept) 0.0106   0.103   
 Residual             0.5055   0.711   
Number of obs: 127, groups: Phase, 1

Fixed effects:
             Estimate Std. Error t value
(Intercept)   3.42517    0.42325    8.09
Intervention -0.12398    0.12621   -0.98
Time         -0.00132    0.00129   -1.02

Correlation of Fixed Effects:
            (Intr) Intrvn
Interventin -0.128       
Time        -0.947 -0.021
</code></pre>
"
"0.132686223108569","0.132803178814933"," 60403","<p>I am a bit stumped on the behavior of $R^2$ in non-linear models.</p>

<p>Below is some data and two hyperbolic fits. One in which two parameters are estimated (Model $m_1$), and another in which one parameter is fixed at $100$, and only the other parameter is estimated (Model $m_2$). Model $m_1$ has much smaller residual standard error ($8.01$ on $2$ df) than $m_2$ ($13.7$ on $3$ df) and just by looking at it (see graph below), a better fit than $m_2$ (even though $m_1$ itself could be improved). The residual sums of squares as reported by anova() are $128.35$ for $m_1$ and $565.80$ for $m_2$. With one df difference that yields a p-value for the difference in SS residual of $.12$. </p>

<p>The difference in residual sums of squares and the eyeball fit is not surprising. </p>

<p>Yet, the $R^2$ of $m_2$ (the constrained model) is much better ($R^2$ is computed (probably incorrectly) by squaring the correlation of predicted values and observed values of $Y$). $m_1$ has a squared correlation of observed and predicted Y values of $.921$, whereas $m_2$ has $.995$.   </p>

<p>Is $R^2$ useful at all for these models? Can one distinguish these models based on $R^2$ (e.g., an exponential model could be a competing model and we may check $R^2$ of that model). </p>

<p>I am curious about this, because in my corner of the literature these types of models are favored or discarded based on $R^2$ evidence and I also don't understand how a constraint can improve $R^2$ in this situation.</p>

<pre><code>test &lt;- data.frame(x=c(1,10,50,100),y=c(57.7,28.0,17.8,14.8))

m1 &lt;- nls(y ~ a / (1+b*x),test,start=list(a=200,b=.07))
m2 &lt;- nls(y ~ 100 / (1+b*x),test,start=list(b=.07))
coeffm1 &lt;- coefficients(m1)
coeffm2 &lt;- coefficients(m2)
summary(m1)
summary(m2)

anova(m1,m2)

test$m1pred &lt;- fitted(m1)
test$m2pred &lt;- fitted(m2)

cor(test$y,test$m1pred)^2
cor(test$y,test$m2pred)^2

plot(test$x,test$y,ylim=c(0,60))
curve((y=coeffm1[""a""] / (1+coeffm1[""b""]*x)),add=T)
curve((y=100 / (1+coeffm2[""b""]*x)),add=T,lty=""dashed"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/XEOHw.jpg"" alt=""Solid line is model m1, dashed line is model m2. Eyeball fit of m1 is better than m2.""></p>

<p><em>Solid line is model m1, dashed line is model m2. Eyeball fit of m1 is better than m2.</em></p>
"
"0.111013258946721","0.0952380952380952"," 61008","<p>I'm a psycholinguistics student with few knowledge in statistics and I have some doubts about a Correlation of Fixed Effects in lmer function (lme4 package). So, if my question is stupidâ€¦ hummmâ€¦ I'm sorry!</p>

<p>My response variable is RT (Reaction Time in a self-paced reading experiment) and my independent variables are Ant (PP, NP) and Verbo (SG, PL). </p>

<p>I have modeled the data with intercepts for Sujeitos (the people who are doing the task) and Item (the sentences i've used), asking for principal effects and interactions between the variables. Here is the model <code>lmer(RT~Ant*Verbo+(1|Sujeitos)+(1|Item))</code> and that's the coefficients for fixed effects:</p>

<p><img src=""http://i.stack.imgur.com/MM27e.jpg"" alt=""Table Fixed Effects""></p>

<p>So, I have made a table of the coefficients for the interactions.</p>

<p>My problem is: that -0.705 and -0.716 correlation effects are a problem for my interaction terms? I'm saying this because the coefficients for the condition NP:SG came from the coefficients of SG only:</p>

<p><img src=""http://i.stack.imgur.com/vsYZo.jpg"" alt=""NP:SG""></p>

<p>and the coefficients for PP:PL came from the PP only:</p>

<p><img src=""http://i.stack.imgur.com/t0f8G.jpg"" alt=""PP:PL""></p>

<p>So, to me, there is no problem here, because I'm not contrasting PP:SG x PP (correlation = -0.705) and I'm not contrasting PP:SG x SG ((correlation = -0.716)). But I do that when getting the coefficients for PP:SG:</p>

<p>ï¿¼<img src=""http://i.stack.imgur.com/xBlx7.jpg"" alt=""PP:SG""></p>

<p>In the last case, there is a contrast between PP:SG x PP and SG. So, the correlation could be a problem. Is this correct? And, if so, how can I deal with this? I've read some thinks in <a href=""http://hlplab.wordpress.com/2011/02/24/diagnosing-collinearity-in-lme4/"" rel=""nofollow"">Jaeger's blog</a> and in this book: <code>Howell, 2010. Statistical Methods for Psychology</code>, but it doesn't help much.</p>

<p>Thank you.</p>
"
"0.125877203744503","0.125988157669742"," 61444","<p>I want to fit a linear model of $y$ on $x$ with respect to a grouping factor and with respect to a covariance structure for the dependent variable within groups. I have a dataset similar to the following:</p>

<pre><code>y1 &lt;- rnorm(25)
y2 &lt;- y1 + rnorm(25)
y3 &lt;- y2 + rnorm(25)
y4 &lt;- y3 + rnorm(25)
y &lt;- c(rbind(y1, y2, y3, y4)) # this is vector of the form: y1[1], y2[1], .., y4[1], y1[2], .., y4[2], ..., y1[25], .., y4[25]
x &lt;- runif(100)
# Dataset:
df &lt;- data.frame(y = y, x = x, group = factor(rep(1:25, rep(4,25)))) 
</code></pre>

<p>I need to estimate the coefficients for the $x$-variable in each <em>group</em> (these effects are nested in <em>group</em> and they are not random) and to do statistical inference for these coefficients. 
Of course, variables in each <em>group</em> are dependent:</p>

<pre><code>cor(cbind(y1, y2, y3, y4))
</code></pre>

<p>so I should use a method that considers the latter (unknown) covariance structure of the dependent variable.</p>

<p>I found the function <em>gls</em> in the <em>nlme</em> library which is supposed to do that:</p>

<pre><code>library(nlme)
# covariance structure:
cs &lt;- corSymm(form = ~ 1 | group) # or maybe something different than group?
cs &lt;- Initialize(cs, data = df)
# variance structure:
df$group2 &lt;- factor(rep(1:4, 25)) # We make a new grouping factor for our data. Is that righr?
vs &lt;- varIdent(form = ~ 1 | group2 ) # or maybe something different instead of factor(rep(1:4, 25))?
vs &lt;- Initialize(vs, data = df)
</code></pre>

<p>But I am not really sure if the grouping factors for <em>cs</em> and <em>vs</em> is right.</p>

<pre><code># Model:
gls(y ~ x %in% group, correlation = cs, weights = vs, data = df) 
</code></pre>

<p>The help for <em>gls</em> is very poor and I don't have the book from Pinheiro, Bates. So I am not really sure if this is doing what I want. Also I don't really understand why the estimated covariance structure by <em>gls</em> is so different from <em>cor(cbind(y1, y2, y3, y4))</em>.</p>
"
"NaN","NaN"," 63364","<p>I have two questions regarding the R script.</p>

<p>If I want to know the size of the sampling when I do a t-test with R, I can use this R script :</p>

<pre><code>power.t.test(delta=1,sd=1.5,sig.level=0.05,power=0.8)
</code></pre>

<p>If I want to know the size of the sampling when I do the test of Pearson correlation coefficient, what is the R script?</p>

<p>And for a chi-square test what is the R script?</p>
"
"0.0593390829096927","0.0593913870916499"," 64152","<p>I am aware of this <a href=""http://stats.stackexchange.com/questions/15157/significance-testing-of-three-or-more-correlations-using-fishers-transformation"">question here</a>, which mine was listed as a duplicate of, but it does not fully answer my question. It did however help me progress a little further so thanks, I couldn't find it before. The online calculator on the above answer also disagrees (though marginally) with the <a href=""http://vassarstats.net/rdiff.html"" rel=""nofollow"">vassarstats.net</a> calculator so I think that further backs up my reasoning for not using a black box.</p>

<p>So I'll re-explain my problem and hope that this gets opened to answers:</p>

<p>I have two Pearson correlation coefficients which I would like to compare. Each comes from 2 sets of 40 genetically identical lines and are correlations between male and female trait values.</p>

<p>I have managed to do $z$-transformations on my correlation coefficients in <code>R</code> using the function <code>atanh()</code> and replicated that with a home-made function <code>RtoZ &lt;- function (r) 0.5*log((1+r)/(1-r))</code>. </p>

<p>The problem is what to do next: how do I actually test, in <code>R</code>,  whether the two correlations are different?</p>
"
"0.167836271659338","0.167984210226323"," 65489","<p>My problem can be summarized very simply: I'm using a linear mixed-effects model and I am trying to get p-values using pvals.fnc(). The problem is that this function seems to have some trouble estimating p-values directly from the t-values associated with model coefficients (Baayen et al., 2008), and I don't know what is going wrong with the way I do it (i.e. according to what I have read, it should work). So, I'm explaining my model below, and if you can point out what I am doing wrong and suggest changes I would really appreciate it!</p>

<p><strong>DESCRIPTION</strong>: I have a 2 by 2 within subjects design, fully crossing two <em>categorical</em> factors, ""Gram"" and ""Number"", each with two levels. This is the command I used to run the model:</p>

<pre><code>&gt;m &lt;- lmer(RT ~ Gram*Number + (1|Subject) + (0+Gram+Number|Subject) + (1|Item),data= data)
</code></pre>

<p>If I understand this code, I am getting coefficients for the two fixed effects (Gram and Number) and their interaction, and I am fitting a model that has by-subject intercepts and slopes for the two fixed effects, and a by-item intercept for them. Following Barr et al. (2013), I thought that this code gets rid of the correlation parameters. I don't want estimate the correlations because I want to get the p-values using pvals.fnc (), and I read that this function doesn't work if there are correlations in the model.</p>

<p>The command seems to work:</p>

<pre><code>&gt;m
Linear mixed model fit by REML 
Formula: RT ~ Gram * Number + (1 | Subject) + (0 + Gram + Number | Subject) + (1 |Item) 
   Data: mverb[mverb$Region == ""06v1"", ] 
   AIC   BIC logLik deviance REMLdev
 20134 20204 -10054    20138   20108
Random effects:
 Groups      Name        Variance  Std.Dev. Corr          
 Item       (Intercept)   273.508  16.5381               
 Subject     Gramgram        0.000   0.0000               
             Gramungram   3717.213  60.9689    NaN        
             Number1        59.361   7.7046    NaN -1.000 
 Subject     (Intercept) 14075.240 118.6391               
 Residual                35758.311 189.0987               
Number of obs: 1502, groups: Item, 48; Subject, 32

Fixed effects:
             Estimate Std. Error  t value
(Intercept)    402.520     22.321  18.033
Gram1          -57.788     14.545  -3.973
Number1         -4.191      9.858  -0.425
Gram1:Number1   15.693     19.527   0.804

Correlation of Fixed Effects:
            (Intr) Gram1  Numbr1
Gram1       -0.181              
Number1     -0.034  0.104       
Gram1:Nmbr1  0.000 -0.002 -0.011
</code></pre>

<p>However, when I try to calculate the p-values I still get an error message:</p>

<pre><code>&gt;pvals.fnc(m, withMCMC=T)$fixed
Error in pvals.fnc(m, withMCMC = T) : 
MCMC sampling is not implemented in recent versions of lme4
  for models with random correlation parameters
</code></pre>

<p>Am I making a mistake when I specify my model? Shouldn't pvals.fnc() work if I removed the correlations?</p>

<p>Thanks for your help!</p>
"
"0.0839181358296689","0.0629940788348712"," 65880","<p>I have two data series containing 132 log-returns. One is for EURUSD, the other is for NZDUSD. The <code>head()</code> function shows you how some of the data looks. The correlation coefficient between the two, as calculated by <code>cor()</code> is $0.5178912$.</p>

<p>To get a better sense of the correlation coefficient I bootstrap it by running <code>cor()</code> 1000 times on different 132 long samples. I run this in a loop and update <code>euro.nzd.corr</code> on every iteration. This is the R code I'm using:</p>

<pre><code>head(euro)
[1] -0.001257862 -0.011637970  0.002428757  0.003602590 -0.003457319 -0.002012728
head(nzd)
[1]  0.008773255 -0.007744927  0.005498693  0.005642524 -0.000896363  0.003449576
cor(euro,nzd)
[1] 0.5178912
euro.nzd.corr &lt;- numeric(1000)
for(i in 1:1000){
euro.nzd.corr[i] = cor(euro[sample(132,132,replace=TRUE)],nzd[sample(132,132,replace=TRUE)])
}
plot(density(euro.nzd.corr), lwd=3, col=""steelblue"")
</code></pre>

<p>Once I have the data, I plot the density chart, and get this:</p>

<p><img src=""http://i.imgur.com/7IRIENi.png"" alt=""density""></p>

<p>Bootstrapped data has mean $\approx 0$ and mostly spreads between $-0.3$ and $0.3$. Where has the initial <code>cor()</code> result of $0.5178912$ gone? What am I to make of this? That it is better to conclude the two variables are uncorrelated versus correlated with a coefficient of $\approx 0.52$? Have I made any coding mistakes, or is the applied methodology simply flawed?</p>
"
"0.103843395091962","0.1187827741833"," 66369","<p>I've found two definitions in the literature for the autocorrelation time of a weakly stationary time series:</p>

<p>$$
\tau_a = 1+2\sum_{k=1}^\infty \rho_k \quad \text{versus} \quad \tau_b = 1+2\sum_{k=1}^\infty \left|\rho_k\right|
$$</p>

<p>where $\rho_k = \frac{\text{Cov}[X_t,X_{t+h}]}{\text{Var}[X_t]}$ is the autocorrelation at lag $k$.  </p>

<p>One application of the autocorrelation time is to find the ""effective sample size"": if you have $n$ observations of a time series, and you know its autocorrelation time $\tau$, then you can pretend that you have</p>

<p>$$
n_\text{eff} = \frac{n}{\tau}
$$</p>

<p>independent samples instead of $n$ correlated ones for the purposes of finding the mean.  Estimating $\tau$ from data is non-trivial, but there are a few ways of doing it (see <a href=""http://arxiv.org/abs/1011.0175"">Thompson 2010</a>).</p>

<p>The definition without absolute values, $\tau_a$, seems more common in the literature; but it admits the possibility of $\tau_a&lt;1$.  Using R and the ""coda"" package:</p>

<pre><code>require(coda)
ts.uncorr &lt;- arima.sim(model=list(),n=10000)         # white noise 
ts.corr &lt;- arima.sim(model=list(ar=-0.5),n=10000)    # AR(1)
effectiveSize(ts.uncorr)                             # Sanity check
    # result should be close to 10000
effectiveSize(ts.corr)
    # result is in the neighborhood of 30000... ???
</code></pre>

<p>The ""effectiveSize"" function in ""coda"" uses a definition of the autocorrelation time equivalent to $\tau_a$, above.  There are some other R packages out there that compute effective sample size or autocorrelation time, and all the ones I've tried give results consistent with this:  that an AR(1) process with a negative AR coefficient has <em>more</em> effective samples than the correlated time series.  This seems strange.  </p>

<p>Obviously, this can never happen in the $\tau_b$ definition of autocorrelation time.</p>

<p>What is the correct definition of autocorrelation time?  Is there something wrong with my understanding of effective sample sizes?  The $n_\text{eff} &gt; n$ result shown above seems like it must be wrong... what's going on?</p>
"
"0.19247955013414","0.192649210414002"," 68363","<p>A coworker and I are trying to analyze agreement between two measurement methods.  I apologize in advance for needing some extra explanation due to the fact I'm an engineer whose statistics background is mostly geared toward the relationship between signal-to-noise ratio and bit error rates, and other analysis of random processes.</p>

<p>For method comparison, it is natural to create a Bland-Altman plot (and we've done so).  However our data has some additional characteristics that Bland-Altman style analysis doesn't account for.  Furthermore, we're trying to compare our results to an earlier study that published a correlation coefficient resulting from mixed-effect analysis, unfortunately this publication didn't say whether they were reporting Pearson correlation coefficient or Intra-class correlation (maybe there are others too?).</p>

<p>The characteristics of our data set are:</p>

<ul>
<li>Multiple test subjects</li>
<li>Multiple observation instants for each test subject, sequentially ordered and equally spaced in time</li>
<li>The subjects are time varying, but receiving treatment so that the time dependent changes are not monotonic</li>
<li>At each observation instant, one measurement is made using each methods</li>
</ul>

<p>A statistician here at our university warned us that a simple paired analysis wasn't appropriate because there's a subject-specific effect, and pointed us to mixed-effects analysis but couldn't help further.</p>

<p>I read several articles on mixed-effect analysis, but most of them are a comparison of groups, rather than a group of comparisons, if that makes sense.  The information I found on intra-class correlation said it treats the measurements within the class interchangeably, and that seems suspect here.</p>

<p>This article uses mixed-effect analysis for method comparison, but has repeated measurements instead of a series of time-separated measurements.  It also doesn't cover correlation coefficients on grouped data.</p>

<ul>
<li><a href=""http://www.degruyter.com/view/j/ijb.2008.4.1/ijb.2008.4.1.1107/ijb.2008.4.1.1107.xml"" rel=""nofollow"">Statistical Models for Assessing Agreement in Method Comparison Studies with Replicate Measurements</a></li>
</ul>

<p>Here's what I've done so far, using R:</p>

<p>Load the data</p>

<pre><code>data &lt;- read.table(filename, header=TRUE, sep="","");
</code></pre>

<p>Convert variables to cases, adding factors (is it correct to create a factor for the encounter, since the time indicators are independent for each test subject?):</p>

<pre><code>library(""reshape"")
mdata &lt;- within(melt(data, variable_name=""method"", id=c(""subject"", ""time"")), {
  subject &lt;- factor(subject)
  time &lt;- factor(interaction(subject, time))
  method &lt;- factor(method)
})
</code></pre>

<p>Run linear mixed-effects model.  I've chosen an autocorrelation structure for the random subject/time covariance matrices, because of the nice periodic measurements.</p>

<pre><code>library(nlme)
lm2 &lt;- lme(value ~ method, random = list( ~1|subject, ~1|time ), corr = corAR1(), data = mdata)
</code></pre>

<p>I'd like to know whether I've assigned the right factors to fixed and random effects.  Also, from my research I guess there should be a random effect on method*subject, but it shouldn't have AR(1) structure and I don't know how to give different structure to different random effects.</p>

<p>Finally, I did calculate a correlation coefficient, using intra-class correlation and encounter as the class to get measurements paired properly.  But I don't think this is using the subject grouping, and as I said earlier, I don't feel like treating class members interchangeably is right.</p>

<pre><code>library(psychometric)
r2 &lt;- ICC1.lme(value, time, mdata)
</code></pre>

<p>What would you do differently?  It seems like the <code>lmer</code> function was a bit easier to describe random effect nested groups, but I didn't find a way to control the correlation structure.</p>
"
"0.0593390829096927","0.0593913870916499"," 68599","<p>I have two discrete random variables with PMF of the form</p>

<p>\begin{align*}
    P(X) =
    \begin{cases}
        p_0, &amp; \mbox{if } X=0 \\
        p_1, &amp; \mbox{if } X=1 \\
        p_2, &amp; \mbox{if } X=2
    \end{cases}
\end{align*}
\begin{align*}
    P(Y) =
    \begin{cases}
        q_0, &amp; \mbox{if } Y=0 \\
        q_1, &amp; \mbox{if } Y=1 \\
        q_2, &amp; \mbox{if } Y=2
    \end{cases}
\end{align*}</p>

<p>Expectations can be calculated easily, 
$E(X) = p_1 + 2 p_2$ , 
$E(Y) = q_1 + 2 q_2$ .</p>

<p>Now, 
assuming $X$ and $Y$ are independent,
define a third RV (collapsed form) as follows:</p>

<p>\begin{align*}
    Z =
    \begin{cases}
        1, \mbox{if } X+Y&gt;1 \\
        0, \mbox{otherwise}
    \end{cases}
\end{align*}</p>

<p>I am wondering if the correlation coefficients 
$\rho_{XZ}$ and $\rho_{YZ}$ are determined
by the parameters $p_1, \; p_2, \; q_1, \; q_2$,
if they are, what is the formula for computing
them.</p>

<p>A simulation in R suggests 
both $r_{XZ}$ and $r_{YZ}$ 
are distributed quite symmetric around the mean of 0.5
instead of 0
(<strong>note these are sample correlation coefficients</strong>):</p>

<pre><code>require(foreach)
snp1 = foreach(i=1:5000, .combine='cbind') %do% {
    sample(0:2, 1000, replace=T, prob=c(.81, .18, .01))
}
snp2 = foreach(i=1:5000, .combine='cbind') %do% {
    sample(0:2, 1000, replace=T, prob=c(.64, .32, .04))
}

collsnp = (snp1 + snp2 &gt; 1) + 0

r1c = foreach(i=1:5000, .combine='c') %do% {
    cor(snp1[, i], collsnp[, i])
}
r2c = foreach(i=1:5000, .combine='c') %do% {
    cor(snp2[, i], collsnp[, i])
}

hist(r1c)
hist(r2c)
</code></pre>

<p><img src=""http://i.stack.imgur.com/MNIEg.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/M6HpN.png"" alt=""enter image description here""></p>
"
"0.0419590679148345","0.0419960525565808"," 68966","<p>I am trying to manually estimate the non-seasonal components of an SARIMA (p,d,q)x(P,D,Q)[s]. I thought the estimation is going the same way like in ARIMA, but the output says somehow something different. </p>

<p>I have an autocorrelation in the acf correlogram and one significance bound at lag 1 in the pacf. That means I have an autocorrelation first order.</p>

<p>I'm confused now, why <code>auto.arima</code> is giving me the result (0,1,1)x(0,0,1)[12] instead of (1,1,0)x(0,0,1)[12]</p>

<p>Here is my code example:</p>

<pre><code>timeseries &lt;- ts(daten, start=c(1955,1), freq=12)

&gt; timeseries
      Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec
1955  1.8  1.7  1.5  1.2  1.5  1.5  1.6  1.8  1.5  1.5  1.6  1.3
1956  0.7  0.6  0.4  0.9  0.9  0.8  0.8  0.6  0.6  0.4  0.4  0.2
1957  0.2  0.1  0.6  0.8  0.3  0.4  0.5  0.7  0.8  0.9  1.0  1.3
1958  1.7  1.7  1.4  1.0  0.9  1.3  1.3  1.0  1.5  1.4  1.4  2.2
1959  1.3  1.7  1.7  2.2  2.8  2.5  2.2  2.3  1.8  1.6  1.3  1.4
1960  2.2  1.8  1.9  1.6  1.1  0.8  1.1  1.1  1.1  1.4  1.2  1.2
1961  0.9  1.2  1.3  0.9  0.7  0.8  0.8  1.2  1.0  1.0  1.4  1.0
1962  1.1  0.8  1.1  1.7  2.1  2.0  2.1  2.1  2.0  2.3  2.0  2.3
1963  1.6  1.9  1.6  1.4  1.6  1.8  1.8  1.9  2.5  2.3  2.2  2.1
1964  2.1  2.1  1.9  2.3  2.1  2.0  2.1  1.8  1.0  1.1  1.5  1.4
1965  1.8  1.9  2.0  2.0  2.0  2.0  2.0  2.0  2.7  2.7  3.3  3.1
1966  2.9  3.0  3.3  2.6  3.1  3.4  3.5  3.3  3.0  2.5  1.4  1.1
1967  0.9  1.0  0.4  0.8  0.0  0.0 -0.7 -0.1 -0.5 -0.1  0.3  0.8
1968  0.8  0.5  1.2  1.0  1.2  0.8  1.2  1.0  1.3  1.3  1.6  1.9
1969  2.0  2.2  2.3  2.7  2.4  2.4  2.6  2.5  2.9  2.9  2.8  2.3
1970  2.3  2.5  2.3  2.2  2.2  2.0  1.9  2.2  2.1  2.1  1.9  2.0
1971  1.9  1.8  1.8  1.1  1.6  1.9  1.9   NA 

diffts &lt;- diff(timeseries,12)
tsdisplay(diffts, lag.max=36)
</code></pre>

<p><img src=""http://i.stack.imgur.com/2MgzU.jpg"" alt=""enter image description here""></p>

<p>But <code>auto.arima</code> is giving me the following output:</p>

<pre><code>auto.arima(timeseries)

Series: timeseries 
ARIMA(0,1,1)(0,0,1)[12]                    

Coefficients:
          ma1     sma1
      -0.1280  -0.7260
s.e.   0.0684   0.0584

sigma^2 estimated as 0.07113:  log likelihood=-23.77
AIC=53.54   AICc=53.66   BIC=63.42
</code></pre>
"
"0.0419590679148345","0.0419960525565808"," 69098","<p>To get the power of the t-test I use this R code:</p>

<pre><code>pwr.t.test(n=25,d=0.35,sig.level=0.05,type=""one.sample"",alternative=""greater"") 
</code></pre>

<p>and for the test of the Pearson coefficient of correlation I use this R code : </p>

<pre><code>pwr.r.test(r=0.3,n=36,sig.level=0.05,alternative=""greater"")
</code></pre>

<p>I would like to know if it is possible to get the R code/script to get the power of the 3 nonparametric test (Wilcoxon, Wilcoxon/Mann-Whitney and the Kruskal-Wallis)? </p>
"
"0.111890847772892","0.125988157669742"," 69524","<p>I am trying to fit a nonlinear regression model in R using <code>nls()</code>. I have a form of the equation I want to fit to:</p>

<p>$$y = (a \times x_{1}^c +b \times x_{2}^d) (x_{3}^e)$$</p>

<p>where the coefficients to be found in regression are a,b,c,d, and e. My data is output from a simulation model where $x_{1}$, $x_{2}$, and $x_{3}$ are all integers from $0$ to $10$, with the condition that $x_{1} + x_{2} + x_{3} \le 10$. $y$ is also integer valued and ranges from $0$ to roughly $1000$. The objective is to fit these data to a rate function that will be used in a Markov Chain.</p>

<p>When I try to fit this regression model directly using <code>nls()</code>, my <code>nlsResiduals</code> plot looks like this:</p>

<p><img src=""http://i.stack.imgur.com/6scJ3.png"" alt=""nls residuals""></p>

<p>I know that autocorrelated residuals are problematic, and that non-normal residuals can also be problematic. How can I fix this problem? I was thinking of using transforms on the data like</p>

<p>$$\log(y) = \log((a \times x_{1}^c +b \times x_{2}^d) (x_{3}^e))$$</p>

<p>or</p>

<p>$$y^{1/n} = ((a \times x_{1}^c +b \times x_{2}^d) (x_{3}^e))^{1/n}$$  where $n &gt; 1$. I've noticed if $n$ increases, my autocorrelation graph and QQ-plot look ""better"" (i.e., more scattered and more normal, respectively). </p>

<p>Both of these seem to correct a lot (but not all) of the autocorrelated residuals, and help to make the residuals more normally distributed. Am I on the right track here, or am I committing some cardinal sin in statistics? Once I settle on a transformation, how can I tell which is best?</p>

<p>Any help, suggestions, or comments are very appreciated.</p>
"
"0.0839181358296689","0.0839921051131616"," 69835","<p>For each round of bootstrapping (say $n=1000$), I would like to select the genotypes (the columns) at random without replacement from the original dataset. With this I would like to form two groups with 10 and 10 genotypes each to match the genotypes in $X$ and $Y$ subsets respectively.</p>

<p>With the original dataset (correlation between $X$ and $Y$) and the bootstrapped datasets (correlation between $X'$ and $Y'$) draw a histogram as follows to see how many percent of the 1000 pairs derived from random permuting exhibit lower correlations than the original dataset.</p>

<p>A sample histogram is shown below.</p>

<p><img src=""http://i.stack.imgur.com/YE06t.png"" alt=""enter image description here""></p>

<p>Maybe for such a sample data</p>

<pre><code> set.seed(1) 
  x &lt;- matrix(rnorm(1000), nrow=100, ncol=10)
  y &lt;- matrix(rnorm(1000), nrow=100, ncol=10).
</code></pre>

<p>I found a useful MATLAB code which I have added <a href=""http://courses.washington.edu/matlab1/Bootstrap_examples.html"" rel=""nofollow"">here</a> to bootstrap correlation coefficients. Is there something similar in R to obtain a similar plot?</p>
"
"0.10277830647413","0.0857240833122733"," 70866","<p>I have a modelling dilemma. I am creating a model that attempts to predict demand (leads not sales) based upon the correlation to advertising spend. We know that without advertising spend, demand is driven by seasonality. So our models include seasonal factors like month of the year and even day of the week. 
If I were building a regular linear regression model, I would fit a linear regression model to a training dataset, to get estimates of the coefficients of the seasonal factors and advertising spend to demand. In order to get an estimate of future baseline demand, I would forecast demand using all the coefficients from the model and then I would estimate a baseline by setting adspend equal to zero. 
For ARIMA models, there are additional factors such as AR and MA terms. Would I estimate my baseline the same way by just setting the coefficient on advertising spend equal to zero?
Thanks for any thoughts.</p>
"
"0.0593390829096927","0.0593913870916499"," 71654","<p>I am interested in generating correlated, skewed data in order to evaluate the application of a statistical approach to a data set I am analyzing. Specifically, I have two correlated variables, and I can determine the correlation coefficient ($R^2$) between the two, as well as the shape, scale and location of each skewed distribution using the sn.em command within the sn package for R.</p>

<p>I can generate a skewed distribution using the rsnorm function within the <a href=""http://cran.r-project.org/web/packages/VGAM/index.html"" rel=""nofollow"">VGAM</a> package. However, I cannot tell it to generate a second vector that is correlated to the first, also skewed, and has its own specific set of shape, scale and location parameters as calculated from the data.</p>

<p>Any advice on how to do this would be greatly appreciated, and thank you in advance!</p>
"
"0.111013258946721","0.111111111111111"," 71924","<p><em><a href=""http://stackoverflow.com/q/19186966/1414455"">Cross-posted from SO.</a></em></p>

<p>I am trying to replicate the results of <code>bgtest</code> from the <a href=""http://cran.r-project.org/web/packages/lmtest/lmtest.pdf"" rel=""nofollow""><code>lmtest</code></a> R package.</p>

<p>I am using the following dataset:</p>

<pre><code>           rs   month   r20
1    2.365042  1952m3  4.33
2    2.317500  1952m4  4.23
3    2.350833  1952m5  4.36
4    2.451833  1952m6  4.57
5    2.466167  1952m7  4.36
6    2.468417  1952m8  4.11
7    2.485583  1952m9  4.20
8    2.415125 1952m10  4.19
9    2.389875 1952m11  4.15
10   2.418167 1952m12  4.22
11   2.396042  1953m1  4.13
12   2.401042  1953m2  4.10
13   2.400833  1953m3  4.04
14   2.383500  1953m4  3.94
15   2.366708  1953m5  3.95
16   2.365625  1953m6  4.02
17   2.348583  1953m7  3.98
18   2.334375  1953m8  3.94
19   2.133542  1953m9  3.78
20   2.097375 1953m10  3.80
21   2.097708 1953m11  3.78
22   2.130583 1953m12  3.83
23   2.096000  1954m1  3.79
24   2.064042  1954m2  3.79
25   2.115083  1954m3  3.76
26   2.047333  1954m4  3.71
27   1.713875  1954m5  3.65
28   1.606167  1954m6  3.61
29   1.561667  1954m7  3.35
30   1.613292  1954m8  3.36
31   1.621083  1954m9  3.35
32   1.587667 1954m10  3.35
33   1.637792 1954m11  3.38
34   1.865917 1954m12  3.51
35   2.356417  1955m1  3.64
36   3.810000  1955m2  3.85
37   3.797000  1955m3  3.83
38   3.906000  1955m4  4.15
39   3.937000  1955m5  4.21
40   3.969000  1955m6  4.33
41   3.971000  1955m7  4.47
42   4.005000  1955m8  4.84
43   4.072000  1955m9  4.68
44   4.071000 1955m10  4.50
45   4.104000 1955m11  4.64
46   4.072000 1955m12  4.70
47   4.071000  1956m1  4.84
48   5.218000  1956m2  4.87
49   5.165000  1956m3  5.02
50   5.008000  1956m4  4.85
51   4.955000  1956m5  5.12
52   5.136000  1956m6  5.25
53   4.977000  1956m7  5.27
54   4.027000  1956m8  5.20
55   5.091000  1956m9  5.35
56   4.991000 1956m10  5.33
57   5.020000 1956m11  5.50
58   4.858000 1956m12  5.29
59   4.553000  1957m1  4.91
60   4.148000  1957m2  4.93
61   4.099000  1957m3  5.08
62   3.914000  1957m4  5.11
63   3.921000  1957m5  5.43
64   3.854000  1957m6  5.55
65   3.845000  1957m7  5.60
66   4.121000  1957m8  5.75
67   6.605000  1957m9  5.98
68   6.603000 1957m10  5.84
69   6.459000 1957m11  5.89
70   6.375000 1957m12  5.81
71   6.127000  1958m1  5.66
72   6.014000  1958m2  5.65
73   5.523000  1958m3  5.64
74   5.179000  1958m4  5.45
75   4.816000  1958m5  5.46
76   4.294000  1958m6  5.45
77   4.159000  1958m7  5.46
78   3.760000  1958m8  5.49
79   3.625000  1958m9  5.36
80   3.584000 1958m10  5.35
81   3.305000 1958m11  5.36
82   3.152000 1958m12  5.36
83   3.107000  1959m1  5.20
84   3.276000  1959m2  5.20
85   3.287000  1959m3  5.24
86   3.283000  1959m4  5.22
87   3.382000  1959m5  5.28
88   3.452000  1959m6  5.18
89   3.484000  1959m7  5.13
90   3.488000  1959m8  5.21
91   3.472000  1959m9  5.33
92   3.386000 1959m10  5.06
93   3.400000 1959m11  5.04
94   3.687000 1959m12  5.21
95   4.538000  1960m1  5.34
96   4.554000  1960m2  5.43
97   4.621000  1960m3  5.53
98   4.652000  1960m4  5.59
99   4.556000  1960m5  5.62
100  5.681000  1960m6  5.92
101  5.546000  1960m7  5.97
102  5.588000  1960m8  5.95
103  5.565000  1960m9  5.97
104  5.090000 1960m10  5.97
105  4.639000 1960m11  5.97
106  4.349000 1960m12  6.01
107  4.165000  1961m1  6.01
108  4.399000  1961m2  6.04
109  4.485000  1961m3  6.05
110  4.407000  1961m4  6.01
111  4.436000  1961m5  6.08
112  4.537000  1961m6  6.33
113  6.688000  1961m7  6.52
114  6.700000  1961m8  6.63
115  6.552000  1961m9  6.65
116  5.727000 1961m10  6.33
117  5.389000 1961m11  6.34
118  5.403000 1961m12  6.41
119  5.242000  1962m1  6.35
120  5.531000  1962m2  6.26
121  4.405000  1962m3  6.25
122  4.052000  1962m4  6.24
123  3.816000  1962m5  6.25
124  3.921000  1962m6  6.24
125  3.887000  1962m7  5.98
126  3.752000  1962m8  5.77
127  3.635000  1962m9  5.27
128  3.858000 1962m10  5.37
129  3.689000 1962m11  5.42
130  3.717000 1962m12  5.36
131  3.491000  1963m1  5.54
132  3.426000  1963m2  5.74
133  3.756000  1963m3  5.69
134  3.709000  1963m4  5.50
135  3.635000  1963m5  5.31
136  3.702000  1963m6  5.28
137  3.761000  1963m7  5.20
138  3.723000  1963m8  5.22
139  3.674000  1963m9  5.21
140  3.745000 1963m10  5.26
141  3.739000 1963m11  5.51
142  3.721000 1963m12  5.63
143  3.758000  1964m1  5.64
144  4.307000  1964m2  5.85
145  4.302000  1964m3  5.76
146  4.302000  1964m4  5.93
147  4.384000  1964m5  5.90
148  4.464000  1964m6  5.97
149  4.654000  1964m7  6.02
150  4.656000  1964m8  6.00
151  4.703000  1964m9  6.00
152  4.698000 1964m10  6.06
153  6.630000 1964m11  6.23
154  6.627000 1964m12  6.41
155  6.543000  1965m1  6.41
156  6.442000  1965m2  6.43
157  6.549000  1965m3  6.53
158  6.375000  1965m4  6.61
159  6.364000  1965m5  6.76
160  5.542000  1965m6  6.78
161  5.630000  1965m7  6.80
162  5.559000  1965m8  6.65
163  5.559000  1965m9  6.35
164  5.440000 1965m10  6.37
165  5.395000 1965m11  6.40
166  5.521000 1965m12  6.59
167  5.483000  1966m1  6.52
168  5.620000  1966m2  6.61
169  5.604000  1966m3  6.77
170  5.638000  1966m4  6.78
171  5.659000  1966m5  6.82
172  5.728000  1966m6  7.03
173  6.679000  1966m7  7.29
174  6.726000  1966m8  7.41
175  6.747000  1966m9  7.29
176  6.513000 1966m10  6.96
177  6.738000 1966m11  6.97
178  6.527000 1966m12  6.78
179  6.080000  1967m1  6.58
180  6.035000  1967m2  6.49
181  5.495000  1967m3  6.50
182  5.412000  1967m4  6.46
183  5.248000  1967m5  6.65
184  5.275000  1967m6  6.86
185  5.345000  1967m7  6.92
186  5.291000  1967m8  6.90
187  5.475000  1967m9  6.98
188  5.726000 1967m10  7.00
189  7.553000 1967m11  7.22
190  7.484000 1967m12  7.20
191  7.520000  1968m1  7.28
192  7.374000  1968m2  7.28
193  7.108000  1968m3  7.29
194  7.080000  1968m4  7.34
195  7.241000  1968m5  7.50
196  7.242000  1968m6  7.87
197  7.059000  1968m7  7.63
198  6.945000  1968m8  7.63
199  6.577000  1968m9  7.64
200  6.493000 1968m10  7.70
201  6.789000 1968m11  7.93
202  6.777000 1968m12  8.17
203  6.728000  1969m1  8.47
204  7.711000  1969m2  8.61
205  7.782000  1969m3  8.81
206  7.798000  1969m4  8.90
207  7.850000  1969m5  9.46
208  7.880000  1969m6  9.31
209  7.830000  1969m7  9.19
210  7.790000  1969m8  9.49
211  7.811000  1969m9  9.21
212  7.743000 1969m10  8.95
213  7.738000 1969m11  9.29
214  7.650000 1969m12  9.04
215  7.550000  1970m1  9.03
216  7.600000  1970m2  8.79
217  7.270000  1970m3  8.75
218  6.940000  1970m4  8.94
219  6.190000  1970m5  9.40
220  6.870000  1970m6  9.58
221  6.850000  1970m7  9.33
222  6.820000  1970m8  9.19
223  6.820000  1970m9  9.28
224  6.810000 1970m10  9.15
225  6.810000 1970m11  9.51
226  6.820000 1970m12  9.62
227  6.790000  1971m1  9.51
228  6.750000  1971m2  9.35
229  6.660000  1971m3  9.07
230  5.920000  1971m4  9.07
231  5.650000  1971m5  9.03
232  5.590000  1971m6  9.08
233  5.570000  1971m7  9.22
234  5.750000  1971m8  8.96
235  4.830000  1971m9  8.50
236  4.630000 1971m10  8.51
237  4.480000 1971m11  7.79
238  4.360000 1971m12  8.10
239  4.360000  1972m1  7.93
240  4.370000  1972m2  7.90
241  4.340000  1972m3  8.16
242  4.300000  1972m4  8.26
243  4.270000  1972m5  8.60
244  5.210000  1972m6  9.32
245  5.600000  1972m7  9.23
246  5.790000  1972m8  9.36
247  6.440000  1972m9  9.54
248  6.740000 1972m10  9.46
249  6.880000 1972m11  9.45
250  7.760000 1972m12  9.62
251  8.210000  1973m1  9.56
252  8.080000  1973m2  9.65
253  8.070000  1973m3 10.01
254  7.670000  1973m4  9.93
255  7.330000  1973m5 10.02
256  7.060000  1973m6 10.15
257  8.270000  1973m7 10.60
258 10.910000  1973m8 11.30
259 10.970000  1973m9 11.55
260 10.770000 1973m10 11.28
261 11.730000 1973m11 12.00
262 12.460000 1973m12 12.50
263 12.090000  1974m1 12.89
264 11.920000  1974m2 13.50
265 11.950000  1974m3 13.68
266 11.520000  1974m4 14.21
267 11.360000  1974m5 13.80
268 11.230000  1974m6 14.38
269 11.200000  1974m7 14.88
270 11.240000  1974m8 15.29
271 11.060000  1974m9 14.95
272 10.930000 1974m10 15.68
273 10.980000 1974m11 16.75
274 10.990000 1974m12 17.18
275 10.590000  1975m1 16.02
276  9.880000  1975m2 14.58
277  9.500000  1975m3 13.43
278  9.260000  1975m4 13.89
279  9.470000  1975m5 14.53
280  9.430000  1975m6 14.41
281  9.710000  1975m7 13.93
282 10.430000  1975m8 13.87
283 10.360000  1975m9 13.79
284 11.420000 1975m10 14.66
285 11.100000 1975m11 14.81
286 10.820000 1975m12 14.79
287  9.990000  1976m1 13.79
288  8.760000  1976m2 13.46
289  8.460000  1976m3 13.88
290  9.060000  1976m4 13.77
291 10.440000  1976m5 13.59
292 10.960000  1976m6 14.09
293 10.870000  1976m7 14.16
294 10.880000  1976m8 14.33
295 12.050000  1976m9 14.79
296 14.000000 1976m10 16.03
297 14.140000 1976m11 15.79
298 13.780000 1976m12 15.48
299 12.730000  1977m1 14.48
300 11.020000  1977m2 13.93
301  9.920000  1977m3 13.25
302  8.240000  1977m4 13.05
303  7.400000  1977m5 12.69
304  7.450000  1977m6 13.26
305  7.430000  1977m7 13.62
306  6.540000  1977m8 13.12
307  5.680000  1977m9 11.88
308  4.530000 1977m10 10.98
309  4.960000 1977m11 11.28
310  6.370000 1977m12 11.16
311  5.810000  1978m1 11.06
312  5.960000  1978m2 11.75
313  5.930000  1978m3 11.72
314  6.730000  1978m4 12.39
315  8.400000  1978m5 12.72
316  9.170000  1978m6 12.79
317  9.220000  1978m7 12.72
318  8.900000  1978m8 12.55
319  8.980000  1978m9 12.64
320  9.860000 1978m10 12.91
321 11.510000 1978m11 13.16
322 11.570000 1978m12 13.22
323 11.860000  1979m1 13.68
324 12.630000  1979m2 13.94
325 11.350000  1979m3 12.35
326 11.320000  1979m4 11.68
327 11.350000  1979m5 11.94
328 12.570000  1979m6 12.69
329 13.320000  1979m7 12.25
330 13.320000  1979m8 12.30
331 13.380000  1979m9 12.60
332 13.380000 1979m10 13.16
333 15.330000 1979m11 14.54
334 15.900000 1979m12 14.72
335 15.790000  1980m1 14.17
336 16.140000  1980m2 14.45
337 16.180000  1980m3 14.70
338 16.170000  1980m4 14.27
339 16.090000  1980m5 14.01
340 15.800000  1980m6 13.78
341 14.550000  1980m7 13.07
342 14.860000  1980m8 13.58
343 14.400000  1980m9 13.38
344 14.290000 1980m10 13.12
345 13.950000 1980m11 13.22
346 13.070000 1980m12 13.67
347 12.820000  1981m1 13.96
348 12.090000  1981m2 13.89
349 11.530000  1981m3 13.68
350 11.330000  1981m4 13.64
351 11.350000  1981m5 14.31
352 12.090000  1981m6 14.57
353 13.150000  1981m7 15.14
354 13.420000  1981m8 15.09
355 13.960000  1981m9 15.59
356 15.550000 1981m10 15.95
357 14.080000 1981m11 15.44
358 14.510000 1981m12 15.65
359 14.160000  1982m1 15.58
360 13.300000  1982m2 14.74
361 12.480000  1982m3 13.72
362 12.890000  1982m4 13.96
363 12.530000  1982m5 13.69
364 12.230000  1982m6 13.56
365 11.280000  1982m7 13.20
366 10.080000  1982m8 12.23
367  9.910000  1982m9 11.40
368  8.910000 1982m10 10.50
369  9.220000 1982m11 10.64
370  9.960000 1982m12 11.34
371 10.590000  1983m1 11.60
372 10.740000  1983m2 11.50
373 10.470000  1983m3 10.97
374  9.840000  1983m4 10.56
375  9.700000  1983m5 10.65
376  9.470000  1983m6 10.39
377  9.370000  1983m7 10.95
378  9.340000  1983m8 11.07
379  9.160000  1983m9 10.67
380  8.840000 1983m10 10.61
381  8.840000 1983m11 10.29
382  8.870000 1983m12 10.35
383  8.870000  1984m1 10.28
384  8.850000  1984m2 10.42
385  8.430000  1984m3 10.23
386  8.380000  1984m4 10.40
387  8.820000  1984m5 10.93
388  8.860000  1984m6 11.15
389 10.970000  1984m7 11.67
390 10.210000  1984m8 10.98
391 10.020000  1984m9 10.78
392  9.850000 1984m10 10.69
393  9.230000 1984m11 10.32
394  9.100000 1984m12 10.46
395 10.550000  1985m1 10.96
396 12.690000  1985m2 11.06
397 12.930000  1985m3 10.90
398 11.930000  1985m4 10.68
399 11.940000  1985m5 10.88
400 11.890000  1985m6 10.70
401 11.390000  1985m7 10.44
402 10.960000  1985m8 10.37
403 11.060000  1985m9 10.39
404 11.050000 1985m10 10.22
405 11.110000 1985m11 10.37
406 11.150000 1985m12 10.45
407 11.980000  1986m1 10.80
408 12.020000  1986m2 10.40
409 11.060000  1986m3  9.39
410  9.990000  1986m4  8.76
411  9.700000  1986m5  9.00
412  9.320000  1986m6  9.23
413  9.450000  1986m7  9.37
414  9.390000  1986m8  9.41
415  9.610000  1986m9  9.97
416 10.250000 1986m10 10.62
417 10.630000 1986m11 10.80
418 10.660000 1986m12 10.69
419 10.520000  1987m1 10.09
420 10.290000  1987m2  9.83
421  9.350000  1987m3  9.16
422  9.430000  1987m4  9.12
423  8.460000  1987m5  8.82
424  8.540000  1987m6  8.90
425  8.840000  1987m7  9.23
426  9.790000  1987m8  9.20
427  9.690000  1987m9  9.98
428  9.450000 1987m10  9.88
429  8.430000 1987m11  9.20
430  8.190000 1987m12  9.57
431  8.370000  1988m1  9.57
432  8.790000  1988m2  9.38
433  8.270000  1988m3  9.12
434  7.740000  1988m4  9.12
435  7.540000  1988m5  9.27
436  8.880000  1988m6  9.32
437 10.050000  1988m7  9.51
438 11.130000  1988m8  9.47
439 11.530000  1988m9  9.60
440 11.540000 1988m10  9.23
441 12.070000 1988m11  9.30
442 12.540000 1988m12  9.46
443 12.450000  1989m1  9.35
444 12.390000  1989m2  9.15
445 12.410000  1989m3  9.26
446 12.470000  1989m4  9.52
447 12.540000  1989m5  9.52
448 13.590000  1989m6  9.88
449 13.290000  1989m7  9.53
450 13.320000  1989m8  9.37
451 13.440000  1989m9  9.62
452 14.460000 1989m10  9.81
453 14.450000 1989m11  9.99
454 14.500000 1989m12  9.96
455 14.500000  1990m1 10.28
456 14.450000  1990m2 10.72
457 14.570000  1990m3 11.46
458 14.590000  1990m4 11.77
459 14.500000  1990m5 11.49
460 14.380000  1990m6 11.01
461 14.320000  1990m7 11.03
462 14.310000  1990m8 11.41
463 14.260000  1990m9 11.32
464 13.370000 1990m10 11.12
465 12.920000 1990m11 10.94
466 12.960000 1990m12 10.40
467 13.000000  1991m1 10.22
468 12.390000  1991m2  9.89
469 11.640000  1991m3 10.06
470 11.250000  1991m4  9.99
471 10.840000  1991m5 10.15
472 10.720000  1991m6 10.34
473 10.520000  1991m7 10.10
474 10.200000  1991m8  9.89
475  9.660000  1991m9  9.54
476  9.860000 1991m10  9.62
477  9.980000 1991m11  9.68
478 10.100000 1991m12  9.56
479  9.970000  1992m1  9.34
480  9.800000  1992m2  9.21
481 10.100000  1992m3  9.54
482  9.970000  1992m4  9.33
483  9.430000  1992m5  8.99
484  9.420000  1992m6  9.02
485  9.430000  1992m7  8.90
486  9.650000  1992m8  9.13
487  9.160000  1992m9  9.12
488  7.470000 1992m10  9.24
489  6.490000 1992m11  8.84
490  6.390000 1992m12  8.84
491  6.050000  1993m1  8.92
492  5.370000  1993m2  8.63
493  5.380000  1993m3  8.33
494  5.330000  1993m4  8.39
495  5.300000  1993m5  8.60
496  5.190000  1993m6  8.39
497  5.130000  1993m7  7.96
498  5.060000  1993m8  7.39
499  5.170000  1993m9  7.18
500  5.150000 1993m10  7.09
501  4.950000 1993m11  7.06
502  4.870000 1993m12  6.46
503  4.890000  1994m1  6.41
504  4.760000  1994m2  6.83
505  4.830000  1994m3  7.47
506  4.880000  1994m4  7.83
507  4.810000  1994m5  8.24
508  4.880000  1994m6  8.55
509  5.090000  1994m7  8.41
510  5.340000  1994m8  8.52
511  5.390000  1994m9  8.72
512  5.440000 1994m10  8.63
513  5.630000 1994m11  8.53
514  5.870000 1994m12  8.44
515  5.930000  1995m1  8.61
516  6.160000  1995m2  8.52
517  6.090000  1995m3  8.50
518  6.300000  1995m4  8.39
519  6.200000  1995m5  8.18
520  6.370000  1995m6  8.16
521  6.620000  1995m7  8.36
522  6.590000  1995m8  8.24
523  6.520000  1995m9  8.09
524  6.530000 1995m10  8.34
525  6.380000 1995m11  8.01
526  6.220000 1995m12  7.94
</code></pre>

<p>which is saved as <code>ukrates.csv</code>.</p>

<p>Here is the code to attempt to reproduce the <code>bgtest</code> module.</p>

<pre><code>rm(list = ls())

library(zoo)
library(lmtest)
library(dynlm)

# read in the data
dfUK = read.csv('./data/ukrates.csv', header = TRUE)
summary(dfUK)

# run the time series regression
zooUK = zoo(dfUK[, c('rs', 'r20')], order.by = as.yearmon(dfUK$month, 
                                                              '%Ym%m'))
    zooUKAug = merge(zooUK, 
                     'drs' = diff(zooUK$rs, 1), 
                 'ldr20' = lag(diff(zooUK$r20, 1), -1))
lmUK2 = dynlm(drs ~ ldr20, data = zooUKAug)

# Breusch-Godfrey regression
zooUKBG = merge(zooUKAug, 'resid' = resid(lmUK2))
lmBG = dynlm(as.formula(paste('resid',  
                              '~', 
                              attr(lmUK2$terms, 'term.labels'),
                              ' + L(resid, 1)')),
             data = zooUKBG) 

# BG test using lmtest package
bgtest(lmUK2, order = 1, type = 'Chisq') # 14.5614

# attempt to recreate BG-test 
length(lmBG$residuals)*
      sum(lmBG$fitted^2)/sum(lmBG$residuals^2)
</code></pre>

<p>This is based on the following code for computing the chi-squared statistic directly from the <code>bgtest</code> function code:</p>

<pre><code>&gt; bgtest
function (formula, order = 1, order.by = NULL, type = c(""Chisq"", 
    ""F""), data = list(), fill = 0) 
{
    dname &lt;- paste(deparse(substitute(formula)))
    if (!inherits(formula, ""formula"")) {
        X &lt;- if (is.matrix(formula$x)) 
                formula$x
        else model.matrix(terms(formula), model.frame(formula))
        y &lt;- if (is.vector(formula$y)) 
                formula$y
        else model.response(model.frame(formula))
    }
    else {
        mf &lt;- model.frame(formula, data = data)
        y &lt;- model.response(mf)
        X &lt;- model.matrix(formula, data = data)
    }
    if (!is.null(order.by)) {
        if (inherits(order.by, ""formula"")) {
            z &lt;- model.matrix(order.by, data = data)
            z &lt;- as.vector(z[, ncol(z)])
        }
        else {
            z &lt;- order.by
        }
        X &lt;- as.matrix(X[order(z), ])
        y &lt;- y[order(z)]
    }
    n &lt;- nrow(X)
    k &lt;- ncol(X)
    order &lt;- 1:order
    m &lt;- length(order)
    resi &lt;- lm.fit(X, y)$residuals
        Z &lt;- sapply(order, function(x) c(rep(fill, length.out = x), 
            resi[1:(n - x)]))
        if (any(na &lt;- !complete.cases(Z))) {
            X &lt;- X[!na, , drop = FALSE]
            Z &lt;- Z[!na, , drop = FALSE]
            y &lt;- y[!na]
            resi &lt;- resi[!na]
            n &lt;- nrow(X)
        }
        auxfit &lt;- lm.fit(cbind(X, Z), resi)
        cf &lt;- auxfit$coefficients
    vc &lt;- chol2inv(auxfit$qr$qr) * sum(auxfit$residuals^2)/auxfit$df.residual
    names(cf) &lt;- colnames(vc) &lt;- rownames(vc) &lt;- c(colnames(X), 
        paste(""lag(resid)"", order, sep = ""_""))
    switch(match.arg(type), Chisq = {
        bg &lt;- n * sum(auxfit$fitted^2)/sum(resi^2)
            p.val &lt;- pchisq(bg, m, lower.tail = FALSE)
            df &lt;- m
            names(df) &lt;- ""df""
        }, F = {
            uresi &lt;- auxfit$residuals
        bg &lt;- ((sum(resi^2) - sum(uresi^2))/m)/(sum(uresi^2)/(n - 
            k - m))
        df &lt;- c(m, n - k - m)
        names(df) &lt;- c(""df1"", ""df2"")
        p.val &lt;- pf(bg, df1 = df[1], df2 = df[2], lower.tail = FALSE)
    })
    names(bg) &lt;- ""LM test""
    RVAL &lt;- list(statistic = bg, parameter = df, method = paste(""Breusch-Godfrey test for serial correlation of order up to"", 
        max(order)), p.value = p.val, data.name = dname, coefficients = cf, 
        vcov = vc)
    class(RVAL) &lt;- c(""bgtest"", ""htest"")
    return(RVAL)
}
&lt;environment: namespace:lmtest&gt;
</code></pre>

<p>I am wondering why I am getting the different results.</p>
"
"0.151285570837612","0.151418920859833"," 72421","<p>I have data for a network of weather stations across the United States. This gives me a data frame that contains date, latitude, longitude, and some measured value. Assume that data are collected once per day and driven by regional-scale weather (no, we are not going to get into that discussion). </p>

<p>I'd like to show graphically how simultaneously-measured values are correlated across time and space. My goal is to show the regional homogeneity (or lack thereof) of the value that is being investigated. </p>

<h2>Data set</h2>

<p>To start with, I took a group of stations in the region of Massachusetts and Maine. I selected sites by latitude and longitude from an index file that is available on NOAA's FTP site.</p>

<p><img src=""http://i.stack.imgur.com/aZm4N.jpg"" alt=""enter image description here""></p>

<p>Straight away you see one problem: there are lots of sites that have similar identifiers or are very close. FWIW, I identify them using both the USAF and WBAN codes. Looking deeper in to the metadata I saw that they have different coordinates and elevations, and data stop at one site then start at another. So, because I don't know any better, I have to treat them as separate stations. This means the data contains pairs of stations that are very close to each other.</p>

<h2>Preliminary Analysis</h2>

<p>I tried grouping the data by calendar month and then calculating the ordinary least squares regression between different pairs of data. I then plot the correlation between all pairs as a line connecting the stations (below). The line color shows the value of R2 from the OLS fit. The figure then shows how the 30+ data points from January, February, etc. are correlated between different stations in the area of interest. </p>

<p><img src=""http://i.stack.imgur.com/X4YZI.jpg"" alt=""correlation between daily data during each calendar month""></p>

<p>I've written the underlying codes so that the daily mean is only calculated if there are data points every 6-hour period, so data should be comparable across sites.</p>

<h3>Problems</h3>

<p>Unfortunately, there is simply too much data to make sense of on one plot. That can't be fixed by reducing the size of the lines. </p>

<p>I've tried plotting the correlations between the nearest neighbors in the region, but that turns into a mess very quickly. The facets below show the network without correlation values, using $k$ nearest neighbors from a subset of the stations. This figure was just to test the concept.
<img src=""http://i.stack.imgur.com/NWzm2.jpg"" alt=""enter image description here""></p>

<p>The network appears to be too complex, so I think I need to figure out a way to reduce the complexity, or apply some kind of spatial kernel.</p>

<p>I am also not sure what is the most appropriate metric to show correlation, but for the intended (non-technical) audience, the correlation coefficient from OLS might just be the simplest to explain. I may need to present some other information like the gradient or standard error as well.</p>

<h3>Questions</h3>

<p>I'm learning my way into this field and R at the same time, and would appreciate suggestions on:</p>

<ol>
<li>What's the more formal name for what I'm trying to do? Are there some helpful terms that would let me find more literature? My searches are drawing blanks for what must be a common application.</li>
<li>Are there more appropriate methods to show the correlation between multiple data sets separated in space?</li>
<li>... in particular, methods that are easy to show results from visually?</li>
<li>Are any of these implemented in R?</li>
<li>Do any of these approaches lend themselves to automation?</li>
</ol>
"
"0.118678165819385","0.1187827741833"," 72569","<p>What does it mean when two random effects are highly or perfectly correlated?<br>
That is, in R when you call summary on a mixed model object, under ""Random effects"" ""corr"" is 1 or -1.</p>

<pre><code>summary(model.lmer) 
Random effects:
Groups   Name                    Variance   Std.Dev.  Corr                 
popu     (Intercept)             2.5714e-01 0.5070912                      
          amdclipped              4.2505e-04 0.0206167  1.000               
          nutrientHigh            7.5078e-02 0.2740042  1.000  1.000        
          amdclipped:nutrientHigh 6.5322e-06 0.0025558 -1.000 -1.000 -1.000
</code></pre>

<p>I know this is bad and indicates that the random effects part of the model is too complex, but I'm trying to understand</p>

<ul>
<li>1)what is doing on statistically  </li>
<li>2)what is going on practically with
the structure of the response variables.</li>
</ul>

<p><strong>Example</strong></p>

<p>Here is an example based on ""<a href=""http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;cad=rja&amp;ved=0CDYQFjAC&amp;url=http://glmm.wdfiles.com/local--files/examples/Banta_ex.pdf&amp;ei=hTNYUpuzBu7J4APN5YHYBg&amp;usg=AFQjCNG65VjvqOLeYLFxJZnzmlMevgEbuA&amp;bvm=bv.53899372,d.dmg"">GLMMs in action: gene-by-environment interaction in total fruit production of wild populations of Arabidopsis thaliana</a>""
by Bolker et al</p>

<p>Download data</p>

<pre><code>download.file(url = ""http://glmm.wdfiles.com/local--files/trondheim/Banta_TotalFruits.csv"", destfile = ""Banta_TotalFruits.csv"")
dat.tf &lt;- read.csv(""Banta_TotalFruits.csv"", header = TRUE)
</code></pre>

<p>Set up factors</p>

<pre><code>dat.tf &lt;- transform(dat.tf,X=factor(X),gen=factor(gen),rack=factor(rack),amd=factor(amd,levels=c(""unclipped"",""clipped"")),nutrient=factor(nutrient,label=c(""Low"",""High"")))
</code></pre>

<p>Modeling log(total.fruits+1) with ""population"" (popu) as random effect</p>

<pre><code>model.lmer &lt;- lmer(log(total.fruits+1) ~ nutrient*amd + (amd*nutrient|popu), data= dat.tf)
</code></pre>

<p>Accessing the Correlation matrix of the random effects show that everything is perfectly correlated</p>

<pre><code>attr(VarCorr(model.lmer)$popu,""correlation"")

                         (Intercept) amdclipped nutrientHigh amdclipped:nutrientHigh
(Intercept)                       1          1            1                      -1
amdclipped                        1          1            1                      -1
nutrientHigh                      1          1            1                      -1
amdclipped:nutrientHigh          -1         -1           -1                       1
</code></pre>

<p>I understand that these are the correlation coefficients of two vectors of random effects coefficients, such as</p>

<pre><code>cor(ranef(model.lmer)$popu$amdclipped, ranef(model.lmer)$popu$nutrientHigh)
</code></pre>

<p>Does a high correlation mean that the two random effects contain redundant information?  Is this analogous to multicollinearity in multiple regression when a model with highly correlated predictors should be simplified?</p>
"
"0.0839181358296689","0.0839921051131616"," 72750","<p>How to estimate maximum observable correlation between two correlation matrices? </p>

<p>I need this in order to correct the observed correlation coefficient between two correlation matrices I have calculated based on different sample sizes.</p>

<p>This is the methodology I need and the way I tried to solve this. </p>

<p>In order to find this value I use the notion of matrix repeatability where this quantity is calculated as</p>

<pre><code>t = (Vo - Ve)/Vo
</code></pre>

<p>where Vo is the variance in the observed correlation values, and Ve is the error variance. Then I use this to calculate Rmax (maximum achievable correlation) as $\sqrt{t1*t2}$ where t1 and t2 are matrix repeatabilites. </p>

<p>This is my solution in R</p>

<pre><code>repeatCor &lt;- function(mat1, mat2)
{
mmc1 &lt;- cor(mat1)
mmc2 &lt;- cor(mat2)

a &lt;- cor(mat1)[lower.tri(cor(mat1))]
b &lt;- cor(mat2)[lower.tri(cor(mat2))]

statObs &lt;- cor(a,b)

mmce1 &lt;- numeric(length(a))
mmce2 &lt;- numeric(length(a))
for(i in 1:length(a)) {
    mmce1[i] &lt;- sqrt((1-a[i]^2)/(length(a)-1))  #errors for every correlation coefficient
    mmce2[i] &lt;- sqrt((1-b[i]^2)/(length(b)-1))
}

Vt1 &lt;- var(a) - var(mmce1)
Vt2 &lt;- var(b) - var(mmce2)

t1 &lt;- Vt1/var(a)
t2 &lt;- Vt2/var(b)

Rmax &lt;- sqrt(t1*t2)

corCor &lt;- statObs/Rmax

list(rep1 = t1, rep2 = t2, Rmax = Rmax, corRaw = statObs, corCor = corCor, m1 = var(mmce1), m2 = mean(mmce2))
}
</code></pre>

<p>but the values I obtain for Rmax are very near 1 and they should be lower. Maybe I am doing something obviously wrong? </p>
"
"0.0938233281301002","0.0939060283031685"," 74068","<p>I am going to estimate False Discovery Rate using permutation tests. </p>

<p>To my knowledge, several R packages are applicable for multiple testing. </p>

<p>I have several independent datasets for meta-analysis. </p>

<p>I computed a Pearson correlation coefficient, converted it into a Fisher-Z score and then calculated the mean effect size of each gene pair from several independent datasets. </p>

<p>Actually, in each dataset, there are two different subgroups - healthy subjects and patients. In this co-expression network, I analyzed the gene expression profiles of the <em>patient</em> group only.</p>

<p>Here is my question.</p>

<blockquote>
  <p>What should I do to estimate FDR using permutation test with creating random and independent shuffling gene expression values of all genes in each dataset
  to break the inter-gene relationships while keeping intact the expression mean and standard deviation of the genes in every dataset?</p>
</blockquote>

<p>Unfortunately, I have no idea how to do it. I am still new to statistics and as well as R. </p>

<p>I would appreciate it if you could answer the question with corresponding code I could run. </p>
"
"0.0593390829096927","0.0593913870916499"," 76131","<p>In <code>R</code>, I have a dataset of many variables and based on correlation matrix I see that some of the variable are correlate with the others. For simplicity, let's assume that there are tree variables X, Y, Z that have high pairwise correlation coefficients. But it seems that information about the variable Y is already included in the variable Z. How can I check (in R or in Python) if X is dependent on Y given Z?</p>
"
"0.119417600797712","0.119522860933439"," 76490","<h2>Background</h2>

<p>A laboratory wants to evaluate whether a certain form of <a href=""http://en.wikipedia.org/wiki/Polyacrylamide_gel_electrophoresis"" rel=""nofollow"">gel electrophoresis</a> is suited as a classification method for the quality of a certain substance. Several gels were loaded, each with a clean sample of the substance and with a sample that contains impurities. In addition, a molecular marker was also loaded which serves as a reference. The following picture illustrates the setup (the picture doesn't show the actual experiment, I have taken it from Wikipedia for illustration):</p>

<p><img src=""http://i.stack.imgur.com/vC53q.png"" alt=""Example of a gel electrophoresis""></p>

<p>Two parameters were measured for each gel and each lane:</p>

<ol>
<li>The <strong>molecular weight</strong> (that is how ""high up"" a compound wandered during the electrophoresis)</li>
<li>The <strong>relative quantity.</strong> The total quantity of each lane is normalized to 1 and the density of each band is measured which results in the relative quantity of each band.</li>
</ol>

<p>A scatterplot of the relative quantity vs. molecular weight is then produced which could look something like this (it's artificial data):</p>

<p><img src=""http://i.stack.imgur.com/ndzh9.png"" alt=""Example scatterplot""></p>

<p>This graphic can be read as follows: Both the ""good"" (blue points) and ""impure"" (red points) substance exhibit two bands, one at around a molecular weight of 120 and one at around 165. The bands of the ""impure"" substance at a molecular weight around 120 are considerably less dense than the ""good"" substance and can be well distinguished.</p>

<hr>

<h2>Goal</h2>

<p>The goal is to determine two boxed (see graphic below) which determine a ""good"" substance. These boxes will then be used for classification of the substance in the future into ""good"" and ""impure"". If a substance exhibits lanes that fall within the boxes it is classified as ""good"" and else as ""impure"".</p>

<p>These decision-rules should be <em>simple</em> to apply for someone in the laboratory. That's why it should be boxes instead of curved decision boundaries.</p>

<p>False-negatives (i.e. classify a sample as ""impure"" when it's really ""good"") are considered worse than false-positives. That is, an emphasis should be placed on the <a href=""http://en.wikipedia.org/wiki/Sensitivity_and_specificity#Sensitivity"" rel=""nofollow"">sensitivity</a>, rather than on the <a href=""http://en.wikipedia.org/wiki/Sensitivity_and_specificity#Specificity"" rel=""nofollow"">specificity</a>.</p>

<p><img src=""http://i.stack.imgur.com/vhzEW.png"" alt=""Example decision boxed""></p>

<hr>

<h2>Question</h2>

<p>I'm am no expert in machine learning. I know, however, that there are quite a few machine learning algorithms/techniques that could be helpful: $k$-nearest neighbors (e.g. <code>knn</code> in <code>R</code>), classification trees (e.g. <code>rpart</code> or <code>ctree</code>), support vector machines (<code>ksvm</code>), logistic regression, boosting and bagging methods and many more.</p>

<p>One problem of many of those algorithms is that they don't provide a simple ruleset or linear boundaries. In addition, the <strong>sample size</strong> is around <strong>70.</strong></p>

<p>My questions are:</p>

<ul>
<li>Has anyone an idea of how to proceed here?</li>
<li>Does it make sense to split the dataset into training- and test-set?</li>
<li>What proportion of the data should the training set be (I thought around a 60/40-split).</li>
<li>What, in general, is the workflow for such an analysis? Something like: Splitting dataset -> fit algorithm on the training set -> predict outcome for the test set?</li>
<li>How to avoid overfitting (i.e. boxes that are too small)?</li>
<li>What is a good statistic to assess the predictive performance in this case? AUC? Accurary? Positive predictive value? <a href=""http://en.wikipedia.org/wiki/Matthews_correlation_coefficient"" rel=""nofollow"">Matthews correlation coefficient</a>?</li>
</ul>

<p>Assume that I'm familiar with <code>R</code> and the <code>caret</code> package. Thank you very much for you time and help.</p>

<hr>

<h2>Example data</h2>

<p>Here is an example dataset.</p>

<pre><code>structure(list(mol.wt = c(125.145401455869, 118.210252208676, 
165.048583787746, 126.003687476776, 170.149347112565, 127.761533014759, 
155.523172614798, 120.094514977175, 161.234986765321, 168.471542655269, 
156.522990530521, 154.377948321209, 165.365756398877, 167.965538771316, 
116.132241687833, 115.143539160903, 156.696830822196, 162.578494491556, 
136.830624758899, 123.886594633942, 124.247484227948, 126.257226352824, 
160.684010454816, 166.618872115047, 126.599387146887, 165.690375912529, 
159.786861142652, 114.520735974329, 125.753594471656, 157.551537154148, 
157.320636890647, 171.5759136115, 158.580005438661, 125.647463565197, 
130.404710783509, 127.128218318572, 162.144126888907, 161.804616951055, 
167.917268243627, 168.582197247178), rel.qtd = c(57.68339235957, 
54.0514508510085, 25.0703901938793, 37.6933881305906, 36.6853653723001, 
53.6650555524679, 52.268438087776, 52.8621831466857, 43.1242291166037, 
46.6771236380788, 38.0328239221277, 40.0454611708371, 44.6406366176158, 
40.8238699987682, 51.9464749018547, 54.0302533272953, 37.9792331383524, 
48.3853988095525, 38.2093977349102, 42.2636098418388, 42.9876895407144, 
40.8018728193786, 40.1097096927465, 38.7432550253867, 39.2633283608111, 
43.4673723102812, 53.3740718733815, 49.1067921475768, 52.3002598744634, 
44.9847844953241, 44.3014423068017, 44.0191971364465, 47.0805245356855, 
55.0124134796556, 57.9938440244052, 62.8314454977068, 45.8093815891894, 
43.2300677500964, 39.4801550161538, 51.6253515591173), quality = structure(c(2L, 
2L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 1L), .Label = c(""bad"", ""good""), class = ""factor"")), .Names = c(""mol.wt"", 
""rel.qtd"", ""quality""), row.names = c(10L, 14L, 47L, 16L, 57L, 
54L, 45L, 12L, 43L, 67L, 25L, 21L, 1L, 55L, 20L, 22L, 37L, 15L, 
8L, 38L, 46L, 64L, 51L, 65L, 52L, 61L, 63L, 32L, 50L, 27L, 19L, 
69L, 23L, 42L, 6L, 48L, 11L, 13L, 5L, 71L), class = ""data.frame"")
</code></pre>
"
"0.10277830647413","0.102868899974728"," 77355","<pre><code>a &lt;- c(10,11)
b &lt;- c(2,3)
</code></pre>

<p>The covariance (<code>C</code>) between<code>a</code> and <code>b</code> = </p>

<pre><code>sum ( (a-mean(a)) * (b-mean(b)) ) # equal to  cov(a,b) in this case as n=2
</code></pre>

<p>The sample covariance (<code>Csamp</code>) between<code>a</code> and <code>b</code> = </p>

<pre><code>sum ( (a-mean(a)) * (b-mean(b)) ) / n-1 # equal to  cov(a,b)
</code></pre>

<p>Now lets say the sum of square differences for <code>a</code> = <code>SSa</code>. This means the Variance(<code>V</code>) for <code>a</code> = <code>SSa/n</code> and the sample variance(<code>Vsamp</code>) is <code>SSa/n-1</code>. The sample standard deviation (<code>SDsamp</code>) for <code>a</code> would be <code>sqrt(SSa/n-1)</code></p>

<p>The correlation coeffictent equations as I have found it are: </p>

<pre><code>(1) C  /  sqrt(SSa) * sqrt(SSb)

# or

(2)  C  /  SDsamp_a * SDsamp_b

# (1) uses sqrt(SSa), not V or V samp and (2) uses SDsamp  not V and does not use Csamp
</code></pre>

<p>I am confused over how to calculate the correlation coefficient (<code>CF</code>) and the sample correlation coefficient (<code>CFsamp</code>). The two formulas above do not seem the same to me. can someone explain how to calculate <code>CF</code> and <code>CFsamp</code>?</p>
"
"0.118678165819385","0.1187827741833"," 77820","<p>I have read up on Bayesian methods enough now to feel that I would rather use Bayesian analysis over Frequentist, the trouble now is finding the correct tools...</p>

<p>I have data from an experiment with two groups of inbred fly lines and the measured variable is a continuous numeric value. Each group contains 40 genetic lines, within each line the response (longevity) of 200 males in 4 blocks (4*50) and 200 females in 4 blocks was measured. </p>

<p>I would like to compare the correlation in group A (gA) to the correlation in group B (gB) because gB is predicted to have a lower cross-sex genetic correlation than gA.  The main aim is to show that the cross-sex genetic correlation for gB is significantly lower than gA - biologically speaking it still supports the theory if the correlation for gB was 0.8, just as long as it is less than the gA correlation. However, it is predicted from a similar previous study and theory that gB should have a correlation around 0, and gA a correlation of around 0.5.</p>

<p>What Bayesian analysis tools (ideally in R) can be used to assess this?</p>

<p>I'm thinking maybe I could measure the correlation in one and test the other using that correlation as a prior to see if it is significantly different from the prior.</p>

<p>Here is some dummy data but you should note I have not set a correlation between the males and females of the A group which is what is expected under the theory tested - it's quite complex to do this so it will take me some time and I'm waist deep in marking student reports. From this current data the correlations for both groups should be 0 (randomly generated numbers).</p>

<pre><code># lines groups sexes focals blocks
l = 40
g = 2
s = 2
f = 50
b = 4

dummy.df = data.frame(c(rep(""M"",l*g*f*b),(rep(""F"",l*g*f*b))))
colnames(dummy.df) = ""gender""

dummy.df$line  = rep(rep(1:l, each = f*b),each = s)
    dummy.df$group = rep( c(rep(""A"",f*b),rep(""B"",f*b)), each = 1)
dummy.df$block = rep(1:b, each = f)
    dummy.df$fly   = 1:(l*g*s*f*b)

# with random uncorrelated lifespans
# note: according to the hypothesis the sexes would correlate in group A (~0.5) 
# such that lines with long life males also have long life females, and in the B group the sexes would be independent of each other
dummy.df$life  = rnorm(length(dummy.df$group),50,5)
head(dummy.df)
</code></pre>

<hr>

<p><em>(Note: I have done this analysis previously using Fisher's z transformation and would like to see the difference of Bayesian methods: <a href=""http://stats.stackexchange.com/questions/64152/are-two-pearson-correlation-coefficients-different"">Are two Pearson correlation coefficients different?</a>)</em></p>
"
"0.16250677125654","0.162650012158089"," 78539","<p>I would like to determine the variance explained by random factors and slopes in a mixed model but am unsure if the analysis I use and my interpretation are correct. Furthermore, comparing models and analysing a mixed model with random slopes seem to give opposite conclusions, therefore I would like to know when to include random slopes? Below I give an overview of the analysis.</p>

<p>I tested three groups of 10 individuals twice in the same task. As I expect individuals to differ in their response across the two tasks, I also include random slopes. The analysis I run is:</p>

<pre><code>lme(behaviour ~ stage * group, random = ~ stage|ID, data=data)
</code></pre>

<p>Part of the output I get is the following:</p>

<pre><code>Linear mixed-effects model fit by maximum likelihood
Data: data 
    AIC       BIC   logLik
   -72.07494 -48.50785 46.03747

 Random effects:
  Formula: ~stage | ID
  Structure: General positive-definite, Log-Cholesky parametrization
             StdDev     Corr  
 (Intercept) 0.12646601 (Intr)
 stage2      0.12662159 -0.455
 Residual    0.05714907       
</code></pre>

<p>To calculate the variance I extract the SD of ID, slopes, and the residual variance as follows:</p>

<pre><code>SD.ID &lt;- (fm2$sigma * attr(corMatrix(fm2$modelStruct[[1]])[[1]],""stdDev""))[[1]]
SD.slope &lt;- (fm2$sigma * attr(corMatrix(fm2$modelStruct[[1]])[[1]],""stdDev""))[[2]]
SD.residual &lt;- fm2$sigma
</code></pre>

<p>And then calculate the percentage of variance explained:</p>

<pre><code>(SD.ID/(SD.ID+SD.slope+SD.residual))*100
(SD.slope/(SD.ID+SD.slope+SD.residual))*100
</code></pre>

<p>In this case this seems to suggest: ""individual ID and random slopes explained 40.8% and 40.8% repectively of the variance of behaviour"".</p>

<p>Although this seems to suggest the random slopes explain a large part of the variance, it seems perhaps a more simple model without slopes is more appropriate:</p>

<pre><code>fm1 &lt;- lme(behaviour ~ stage * group, random = ~ stage|ID, data=data, method=""ML"")
fm0 &lt;- lme(behaviour ~ stage * group, random = ~ 1|ID, data=data, method=""ML"")
anova(fm0,fm1)
</code></pre>

<p>since I get the following output:</p>

<pre><code>    Model df       AIC       BIC   logLik   Test    L.Ratio p-value
fm0     1  8 -76.00947 -57.15580 46.00473                          
fm1     2 10 -72.07494 -48.50785 46.03747 1 vs 2 0.06547599  0.9678
</code></pre>

<p>Which to me seems to suggest the model with the random slope does not significantly better fit the data. This seems contrasting to the 40% of the variance that it seems to explain, as shown with the data above.</p>

<p>Furthermore, if I correlate the coefficients from model fm1, thus the intercept with the slope:</p>

<pre><code>cor.test(fm1$coefficients[[2]][[1]][,1],fm1$coefficients[[2]][[1]][,2]) 
</code></pre>

<p>I get the following output:</p>

<pre><code>Pearson's product-moment correlation

data:  fm1$coefficients[[2]][[1]][, 1] and fm1$coefficients[[2]][[1]][, 2]
t = -2.6802, df = 37, p-value = 0.01092
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 -0.6376166 -0.1004855
sample estimates:
       cor 
-0.4032186 
</code></pre>

<p>Which thus seems to suggest that individuals with lower initial behaviour scores change more in their behaviour over time. Therefore again I would think running the model with the slopes would make the most sense. </p>

<p>Thus, to repeat my question: how can I determine the variance explained by random factors and slopes in a mixed model and when do I know when to include random slopes?</p>
"
"0.0938233281301002","0.0751248226425348"," 79107","<p>I'm trying my hand at resampling techniques with a dataset I have, and I think either I'm missing a conceptual point with bootstrapping, or I'm doing something incorrectly in <code>R</code>. Basically, I'm trying to use it in a correlation/regression framework, and I'm able to get the original coefficients, the bootstrap bias, and the bootstrap coefficients but I can't find a way to have <code>R</code> easily display the bootstrap model $R^2$ (when I'm working with several predictors), the Pearson $r$, or the $p$-values for individual regression coefficients. (I'm using the <code>Boot</code> function in the <code>car</code> package).</p>

<p>A secondary question...the more general function <code>boot</code> in the <code>boot</code> package requires defining a function to use as an argument. The function must include an argument for the original data set, and a second argument which is a set of indices, frequencies, or weights for the bootstrap sample. I'm a little confused by this. What conceptually are these indices I am specifying, and how do I specify them syntactically within my function?</p>
"
"0.0938233281301002","0.0939060283031685"," 79777","<p>Below is the graph of two variables, X and Y, each representing count data. N=348. Note the scales of the axes:<br>
<img src=""http://i.stack.imgur.com/ih1yU.jpg"" alt=""http://i.imgur.com/tNGyTX5.jpg"">  </p>

<p>Y is very approximately lognormal, but X has no decent fit (including Poisson, negative binomial, lognormal and gamma of the log transform).<br>
Spearman coefficient between X and Y is close to 0, and p-value to reject no correlation is very high.  </p>

<p>From the plot, there appears to be no combinations of extreme values of both x and y.</p>

<p>When I log transform both X and Y, the following plot results:<br>
<img src=""http://i.stack.imgur.com/DVVHU.jpg"" alt=""enter image description here""><br>
Clearly any appearance of pattern has disappeared.  </p>

<p>My questions are: </p>

<ul>
<li>Why is there a lack of combinations of ""extreme"" values on the linear scale, but not on the log scale?</li>
<li>Is there any significance to the lack of combination of extreme values on the linear scale, and is there anyway to investigate further?  </li>
</ul>

<p>The purpose of this study is exploratory.  </p>
"
"0.0938233281301002","0.0939060283031685"," 79830","<p>I'm using R to develop regression models, and I need to compare two different models' performance. The question that arises is, ""Is Model 1 statistically better than model 2?"" and I don't seem to have a way to answer that question.</p>

<p>Background: Model 1 consists of Variable A regressed on the endpoint. Model 2 consists of Variables B, C, and D regressed on the endpoint. Both models are developed using lm - ordinary least squares, nothing too fancy here.</p>

<p>Given that these are not nested models, I cannot compare them using ANOVA.</p>

<p>I can look at the R2 of actual vs predicted for each model, and I see that Model 2 is better, but how do I determine if it is statistically significantly better?</p>

<p>I've also used the Concordance Correlation Coefficient, but again, I can't find a way to prove significance. The best I've come up with is that the rho for Model 2 is better than Model 1, but that rho value is within the 95% confidence limits of the rho for Model 1.</p>

<p>I should throw in there that my assessment of predicted vs actual has been on a 60 observation hold-out set (240 observations in the training set).</p>
"
"NaN","NaN"," 79942","<p>I have elemental concentrations for 18 elements in 36 samples, determined by neutron activation analysis. I would like to calculate correlation coefficients, etc. by regression analysis according to the York method.</p>
"
"0.0419590679148345","0.0419960525565808"," 83103","<p>I have been working in R and have finally got some code working regarding intraclass correlation coefficients. In the output I get something called ""subjects"". I thought it was to do with the number of columns of data but for a file with 20 columns I get 3 or 4 subjects depending on the data set.</p>

<p>I have very little knowledge of R and I'm not great at statistics talk so would someone be able to explain in simple terms what a subject is when it is given in the output in R having done a single score intraclass correlation please?</p>

<p>Thanks!</p>
"
"0.118678165819385","0.103934927410387"," 84054","<p>I encountered a real-world problem where I want to model the effectiveness of various advertising media of a brand (measured in terms of sales). Basically, the Y in this case is weekly sales, and the X's are media investments in newspaper, magazine, display boards, tv, radio and online, as well as incentive, which is a percentage (like 10% off the original).</p>

<p>There are a few problems with the modelling work:</p>

<ul>
<li><p>all variables should have positive coefficients. Typically, more advertising or incentive is at least as good as less advertising/incentive (maybe this is not true if you buy all of the advertising spots in the world, as then your consumer will start to hate your brand, but this is not going to happen here). However, when I fit a typical regression (e.g. lm, glm, gls etc), some coefficients turn out to be negative (as data may be a bit noiser than expected, hence causing this problem?). I wonder if this can be controlled (I know in nonlinear regressions you can set constraints for parameters)</p></li>
<li><p>there should be some sort of diminishing marginal return of advertising spendings, but I am not exactly sure how to model that. Some ideas include using a log or square root transformation, another idea may be to use a nonlinear regression and estimator something like a*newspaper^b, where a is some coefficient, and b is an exponent between 0 and 1.</p></li>
<li><p>this is serial correlation, but this may not be exactly important here as the goal is only to estimate the parameters (if I use a regression I think I still get the unbiased estimators right? Autocorrelation only screws up the p-values, which is ignored here). Also, how to deal with seasonalities? I don't have much data (2 years) so maybe there is nothing we can do about it, but I have seen adding cos(0.0172*time) + sin(0.0172*time) to the regression equation to adjust for seasonal changes.</p></li>
</ul>

<p>Thanks.</p>
"
"0.225956495875133","0.218357193084674"," 86032","<p>I'm currently working with a data set that has numerous samples collected over time at different sites in a study area, and I'm interested in detecting a trend over time for that area.  I know that in an ideal experimental or balanced situation, using a random slope and intercept model is a great way to get at the overall trend within the study area.  With our data, however, many of the sites are missing samples and a handful of the sites only have one data point.</p>

<p>I'm curious if there's a way to intuitively understand how the sample imbalance will affect the estimate of the overall slope?  To put it differently, are there ways to know if sample imbalances are causing problems,  or are there things I can look for in my model output that would indicate I shouldn't trust what the model is estimating?</p>

<p>I created a contrived example with 20 data points to look at this. I put 10 data points with a slope of 1 into one site (a), and put the other 10 data points with a slope of -1 into unique sites (b through l).  I had assumed that when I looked at both a random intercept and random slope and intercept model that they would be somewhat similar, or that at least the latter would give more weight to the site with good data over time.</p>

<pre><code>&gt; library(lme4)
&gt; set.seed(9999)

&gt; x = c(0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9) + rnorm(20,mean=0,sd=0.1)
&gt; y = c(0,1,2,3,4,5,6,7,8,9,9,8,7,6,5,4,3,2,1,0) + rnorm(20,mean=0,sd=0.1)
&gt; z = c(rep('a',10),'b','c','d','e','f','h','i','j','k','l')
&gt; z = factor(z)

&gt; m0 = lm(y~x)
&gt; m1 = lmer(y~x+(1|z))
&gt; m2 = lmer(y~x+(1+x|z))

&gt; summary(m0)
&gt; summary(m1)
&gt; summary(m2)
&gt; anova(m1,m2)
</code></pre>

<p>As expected, the slope of the linear model was near zero, but the results for the two mixed effects models were nearly opposite.  Even though sites b through l only have one data point, it seems like they contribute more towards the slope because the trend is occurring over so many sites.  The random slope and intercept model was also preferred to using model selection criteria.</p>

<pre><code> &gt; summary(m0)$coefficients
                Estimate Std. Error    t value    Pr(&gt;|t|)
 (Intercept)  4.53784796  1.2586990  3.6051890 0.002023703
 x           -0.01178748  0.2335094 -0.0504797 0.960296079

 &gt; summary(m1)
 Linear mixed model fit by REML ['lmerMod']
 Formula: y ~ x + (1 | z) 

 REML criterion at convergence: 62.0877 

 Random effects:
  Groups   Name        Variance Std.Dev.
  z        (Intercept) 33.30788 5.7713  
  Residual              0.01583 0.1258  
 Number of obs: 20, groups: z, 11

 Fixed effects:
             Estimate Std. Error t value
 (Intercept) -0.03597    1.74163   -0.02
 x            0.99332    0.01386   71.66

 Correlation of Fixed Effects:
   (Intr)
 x -0.036

 &gt; summary(m2)
 Linear mixed model fit by REML ['lmerMod']
 Formula: y ~ x + (1 + x | z) 

 REML criterion at convergence: 31.0386 

 Random effects:
  Groups   Name        Variance Std.Dev. Corr 
  z        (Intercept) 7.78818  2.7907        
      x           0.37691  0.6139   -1.00
  Residual             0.01524  0.1234        
 Number of obs: 20, groups: z, 11

 Fixed effects:
             Estimate Std. Error t value
 (Intercept)   8.2121     0.8566   9.587
 x            -0.8201     0.1882  -4.358

 Correlation of Fixed Effects:
   (Intr)
 x -0.999

 &gt; anova(m1,m2)
 Data: 
 Models:
 m1: y ~ x + (1 | z)
 m2: y ~ x + (1 + x | z)
    Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
 m1  4 66.206 70.189 -29.103   58.206                             
 m2  6 36.745 42.719 -12.372   24.745 33.462      2  5.419e-08 ***
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I see that under this extreme example, the random slope and intercept have an almost perfect correlation.  Is what I can pull from this is that, in a sense, the model gives more value to the sites with only one data point because the overall trend is so strong but across multiple sites, but that I should view the slope estimate this model produces as suspect with such a high correlation?  Is there anything else that should look for?  For my specific study, I could also set some sort of criteria for what level of replication I thought was necessary to make proper inferences, e.g. eliminate all the sites that less than five samples.</p>

<p>Many thanks for your thoughts.</p>
"
"0.16250677125654","0.162650012158089"," 86273","<p>I'm trying to calculate the log-likelihood for a generalized nonlinear least squares regression for the function $f(x)=\frac{\beta_1}{(1+\frac x\beta_2)^{\beta_3}}$ optimized by the <code>gnls</code> function in the R package <code>nlme</code>, using the variance covariance matrix generated by distances on a a phylogenetic tree assuming Brownian motion (<code>corBrownian(phy=tree)</code> from the <code>ape</code> package). The following reproducible R code fits the gnls model using x,y data and a random tree with 9 taxa:</p>

<pre><code>require(ape)
require(nlme)
require(expm)
tree &lt;- rtree(9)
x &lt;- c(0,14.51,32.9,44.41,86.18,136.28,178.21,262.3,521.94)
y &lt;- c(100,93.69,82.09,62.24,32.71,48.4,35.98,15.73,9.71)
data &lt;- data.frame(x,y,row.names=tree$tip.label)
model &lt;- y~beta1/((1+(x/beta2))^beta3)
f=function(beta,x) beta[1]/((1+(x/beta[2]))^beta[3])
start &lt;- c(beta1=103.651004,beta2=119.55067,beta3=1.370105)
correlation &lt;- corBrownian(phy=tree)
fit &lt;- gnls(model=model,data=data,start=start,correlation=correlation)
logLik(fit) 
</code></pre>

<p>I would like to calculate the log-likelihood ""by hand"" (in R, but without use of the <code>logLik</code> function) based on the estimated parameters obtained from <code>gnls</code> so it matches the output from <code>logLik(fit)</code>. NOTE: I am not trying to estimate parameters; I just want to calculate log-likelihood of the parameters estimated by the <code>gnls</code> function (although if someone has a reproducible example of how to estimate parameters without <code>gnls</code>, I would be very interested in seeing it!). </p>

<p>I'm not really sure how to go about doing this in R. The linear algebra notation described in Mixed-Effects Models in S and S-Plus (Pinheiro and Bates) is very much over my head and none of my attempts have matched <code>logLik(fit)</code>. Here are the details described by Pinheiro and Bates:</p>

<p>The log-likelihood for the generalized nonlinear least squares model  $y_i=f_i(\phi_i,v_i)+\epsilon_i$ where $\phi_i=A_i\beta$ is calculated as follows:</p>

<p>$l(\beta,\sigma^2,\delta|y)=-\frac 12 \Bigl\{ N\log(2\pi\sigma^2)+\sum\limits_{i=1}^M{\Bigl[\frac{||y_i^*-f_i^*(\beta)||^2}{\sigma^2}+\log|\Lambda_i|\Bigl]\Bigl\}}$</p>

<p>where $N$ is the number of observations, and $f_i^*(\beta)=f_i^*(\phi_i,v_i)$.</p>

<p>$\Lambda_i$ is positive-definite, $y_i^*=\Lambda_i^{-T/2}y_i$ and $f_i^*(\phi_i,v_i)=\Lambda_i^{-T/2}f_i(\phi_i,v_i)$</p>

<p>For fixed $\beta$ and $\lambda$, the ML estimator of $\sigma^2$ is </p>

<p>$\hat\sigma(\beta,\lambda)=\sum\limits_{i=1}^M||y_i^*-f_i^*(\beta)||^2 / N$</p>

<p>and the profiled log-likelihood is</p>

<p>$l(\beta,\lambda|y)=-\frac12\Bigl\{N[\log(2\pi/N)+1]+\log\Bigl(\sum\limits_{i=1}^M||y_i^*-f_i^*(\beta)||^2\Bigl)+\sum\limits_{i=1}^M\log|\Lambda_i|\Bigl\}$</p>

<p>which is used with a Gauss-Seidel algorithm to find the ML estimates of $\beta$ and $\lambda$. A less biased estimate of $\sigma^2$ is used:</p>

<p>$\sigma^2=\sum\limits_{i=1}^M\Bigl|\Bigl|\hat\Lambda_i^{-T/2}[y_i-f_i(\hat\beta)]\Bigl|\Bigl|^2/(N-p)$</p>

<p>where $p$ represents the length of $\beta$.</p>

<p>I have compiled a list of specific questions that I am facing:</p>

<ol>
<li>What is $\Lambda_i$? Is it the distance matrix produced by <code>big_lambda &lt;- vcv.phylo(tree)</code> in <code>ape</code>, or does it need to be somehow transformed or parameterized by $\lambda$, or something else entirely?</li>
<li>Would $\sigma^2$ be <code>fit$sigma^2</code>, or the equation for the less biased estimate (the last equation in this post)?</li>
<li>Is it necessary to use $\lambda$ to calculate log-likelihood, or is that just an intermediate step for parameter estimation? Also, how is $\lambda$ used? Is it a single value or a vector, and is it multiplied by all of $\Lambda_i$ or just off-diagonal elements, etc.?</li>
<li>What is $||y-f(\beta)||$? Would that be <code>norm(y-f(fit$coefficients,x),""F"")</code> in the package <code>Matrix</code>? If so, I'm confused about how to calculate the sum $\sum\limits_{i=1}^M||y_i^*-f_i^*(\beta)||^2$, because <code>norm()</code> returns a single value, not a vector.</li>
<li>How does one calculate $\log|\Lambda_i|$? Is it <code>log(diag(abs(big_lambda)))</code> where <code>big_lambda</code> is $\Lambda_i$, or is it <code>logm(abs(big_lambda))</code> from the package <code>expm</code>? If it is <code>logm()</code>, how does one take the sum of a matrix (or is it implied that it is just the diagonal elements)?</li>
<li>Just to confirm, is $\Lambda_i^{-T/2}$ calculated like this: <code>t(solve(sqrtm(big_lambda)))</code>?</li>
<li>How are $y_i^*$ and $f_i^*(\beta)$ calculated? Is it either of the following:</li>
</ol>

<p><code>y_star &lt;- t(solve(sqrtm(big_lambda))) %*% y</code></p>

<p>and</p>

<p><code>f_star &lt;- t(solve(sqrtm(big_lambda))) %*% f(fit$coefficients,x)</code></p>

<p>or would it be</p>

<p><code>y_star &lt;- t(solve(sqrtm(big_lambda))) * y</code></p>

<p>and</p>

<p><code>f_star &lt;- t(solve(sqrtm(big_lambda))) * f(fit$coefficients,x)</code> ?</p>

<p>If all of these questions are answered, in theory, I think the log-likelihood should be calculable to match the output from <code>logLik(fit)</code>. Any help on any of these questions would be greatly appreciated. If anything needs clarification, please let me know. Thanks!</p>

<p><strong>UPDATE</strong>: I have been experimenting with various possibilities for the calculation of the log-likelihood, and here is the best I have come up with so far. <code>logLik_calc</code> is consistently about 1 to 3 off from the value returned by <code>logLik(fit)</code>. Either I'm close to the actual solution, or this is purely by coincidence. Any thoughts?</p>

<pre><code>  C &lt;- vcv.phylo(tree) # variance-covariance matrix
  tC &lt;- t(solve(sqrtm(C))) # C^(-T/2)
  log_C &lt;- log(diag(abs(C))) # log|C|
  N &lt;- length(y)
  y_star &lt;- tC%*%y 
  f_star &lt;- tC%*%f(fit$coefficients,x)
  dif &lt;- y_star-f_star  
  sigma_squared &lt;-  sum(abs(y_star-f_star)^2)/N
  # using fit$sigma^2 also produces a slightly different answer than logLik(fit)
  logLik_calc &lt;- -((N*log(2*pi*(sigma_squared)))+
       sum(((abs(dif)^2)/(sigma_squared))+log_C))/2
</code></pre>
"
"0.0593390829096927","0.0296956935458249"," 86803","<p>My main goal is to compute cepstral coefficients in R from an ACF vector (Auto-Coorelation function vector with discrete time-step). Correct me if I am wrong in terminology or whatever as I am not a statistician/signal-processor. My first question will be</p>

<ul>
<li>Are the elements of an ACF vector i.e. the auto-correlation-coefficients (ACC) linear-predictive-coefficients (LPC)? I think they are since an AR(p) process is a linear predictive model.</li>
</ul>

<p>If my above hypothesis is true, then I have found MatLab code for converting LPC coefficients to cepstral coefficients (CC): <a href=""http://www.mathworks.com/help/dsp/ref/lpctofromcepstralcoefficients.html"" rel=""nofollow"">MatLab Code</a>. Then my question would reduce to:</p>

<ul>
<li>Is there an R library which will convert LPCs to CCs?</li>
</ul>

<p>Thanks for all the help, and please correct my terminology if I am wrong!</p>
"
"0.118678165819385","0.1187827741833"," 86952","<p>I'm trying to regress some simple pooled data. My data has 60 observations and three columns: Weight, Height, and Sex (female=1, male=0).</p>

<p>If I regress thus, Weight ~ Height + Sex, my model is fairly satisfactory, but the residuals are not homoscedastic (green errors are male, blue female):</p>

<p><img src=""http://i1267.photobucket.com/albums/jj541/nbahmanyar/Rplot_zps69001b34.png"" alt=""plot""></p>

<p>I tried regressing on the log of Weight and/or Height, but that didn't do much. What should I do to make the residuals homescedastic and/or make my model more accurate? Any help would be appreciated.</p>

<p><strong>Edit</strong></p>

<p>Doing a generalized regression model gives the following.</p>

<pre><code>Generalized least squares fit by REML
  Model: Weight ~ h + s 
  Data: P149 
       AIC      BIC    logLik
  514.2221 524.4374 -252.1111

Variance function:
 Structure: Different standard deviations per stratum
 Formula: ~1 | Sex 
 Parameter estimates:
        0         1 
1.0000000 0.6685307 

Coefficients:
                 Value Std.Error   t-value p-value
(Intercept)  27.197499  51.88129  0.524226  0.6022
h             1.852382   0.75634  2.449128  0.0174
s           -25.284478   5.53300 -4.569755  0.0000

 Correlation: 
  (Intr) h     
h -0.997       
s -0.524  0.466

Standardized residuals:
       Min         Q1        Med         Q3        Max 
-1.6655243 -0.6879858 -0.1839396  0.5628971  3.9857544 

Residual standard error: 22.13369 
Degrees of freedom: 60 total; 57 residual
</code></pre>

<p>With this s. residual plot:</p>

<p><img src=""http://i1267.photobucket.com/albums/jj541/nbahmanyar/Rplot1_zps5ee264a0.png"" alt=""""></p>

<p>Could someone please explain how precisely this model is different from a standard multiple regression model? Thanks.</p>
"
"0.0419590679148345","0.0419960525565808"," 87445","<p>I have fitted random coefficient Poisson analysis in R. I have obtained the following results: </p>

<p>Generalized linear mixed model fit by maximum likelihood ['glmerMod']
Family: poisson ( log )</p>

<p>Formula: frequency ~ 1 + cc + ageveh + make + (1 | AREA) </p>

<p>Data: x </p>

<pre><code>  AIC       BIC    logLik  deviance 
</code></pre>

<p>1359.1477 1389.7370 -672.5739 1345.1477 </p>

<p>Random effects:</p>

<p>Groups Name        Variance Std.Dev.</p>

<p>AREA   (Intercept) 1.323    1.15 </p>

<p>Number of obs: 584, groups: AREA, 8</p>

<p>Fixed effects:
            Estimate Std. Error z value Pr(>|z|) </p>

<p>(Intercept) -0.12902    0.44432  -0.290   0.7715 </p>

<p>ccL          0.05656    0.12371   0.457   0.6475</p>

<p>agevehO      0.02136    0.09264   0.231   0.8177</p>

<p>make2       -0.45454    0.20632  -2.203   0.0276 *</p>

<p>make3       -0.31799    0.21422  -1.484   0.1377 </p>

<h2>make4       -0.29708    0.14469  -2.053   0.0401 *</h2>

<p>Signif. codes:  0 â€˜<strong><em>â€™ 0.001 â€˜</strong>â€™ 0.01 â€˜</em>â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1</p>

<p>Correlation of Fixed Effects:</p>

<pre><code>    (Intr) ccL    agevhO make2  make3
</code></pre>

<p>ccL      0.052    </p>

<p>agevehO -0.179 -0.232   </p>

<p>make2   -0.171 -0.007 -0.001 </p>

<p>make3   -0.156  0.022 -0.078  0.366 </p>

<p>make4   -0.300 -0.235  0.167  0.544  0.522</p>

<p>However I am unable to interpret the results. </p>
"
"0.0839181358296689","0.0839921051131616"," 87510","<p>I have obtained the following results in R from a random coefficient Poisson analysis.</p>

<p>Generalized linear mixed model fit by maximum likelihood ['glmerMod']
Family: poisson ( log )
Formula: frequency ~ 1 + insgen + ageveh + make + area + (1 | ID) 
   Data: Panel </p>

<pre><code>  AIC       BIC    logLik   deviance 
</code></pre>

<p>1099.9670 1134.9262  -541.9835  1083.9670 </p>

<p>Random effects:</p>

<p>Groups Name          Variance   Std.Dev.</p>

<p>ID  (Intercept)     1.551e-11     3.939e-06</p>

<p>Number of obs: 584, groups: ID, 584</p>

<p>Fixed effects:</p>

<pre><code>          Estimate    Std. Error   z value   Pr(&gt;|z|)  
</code></pre>

<p>(Intercept)    -22.98292   8432.07738    -0.003     0.9978  </p>

<p>insgenM          0.02616     0.08806      0.297     0.7664  </p>

<p>ageveho          0.05889     0.08586      0.686     0.4928 </p>

<p>make             -0.10447    0.04126     -2.532    0.0113 *</p>

<p>area1             23.68571  8432.07738   0.003    0.9978  </p>

<p>area2             23.85969  8432.07738   0.003    0.9977 </p>

<p>area3             23.77374  8432.07738   0.003   0.9978  </p>

<hr>

<p>Signif. codes:  0 â€˜<strong><em>â€™ 0.001 â€˜</strong>â€™ 0.01 â€˜</em>â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1</p>

<p>Correlation of Fixed Effects:</p>

<pre><code>    (Intr)  insgnM  ageveh  make    area1   area2 
</code></pre>

<p>insgenM   0.000  </p>

<p>ageveho   0.000  -0.037   </p>

<p>make      0.000   0.071 0.108 </p>

<p>area1    -1.000   0.000 0.000  0.000   </p>

<p>area2    -1.000  0.000 0.000  0.000  1.000</p>

<p>area3   -1.000  0.000  0.000  0.000  1.000  1.000       </p>

<p>I have included predictors:gender (Male,female), area which has 4 levels (area 1,2,3,4) and vehicle age (New and old) and make of car.</p>
"
"0.15699645640569","0.157134840263677"," 88513","<p>I'm quite new to the area of spatial statistics, but I'm very interest in some general principles. The last two weeks I've created an example dataset, which contains three datsets.</p>

<ol>
<li>A dataset of ill persons. </li>
<li>A dataset of cities with the the overall population.</li>
<li>A dataset of points, which visualizes waterfeatures.</li>
</ol>

<p>The whole situation looks like this:<br>
Blue: Water features, yellow: cities, red: persons. 
Please note that the persons are located on the cities coordinates.
<img src=""http://i.stack.imgur.com/iFi21.png"" alt=""PLot""></p>

<p>I've already performed some basic analyses:</p>

<p>Person dataset:</p>

<ul>
<li>I've calculated the distance between each person and the nearest waterfeature.</li>
</ul>

<p>City dataset:</p>

<ul>
<li>I've calculated the number of ill and not_ill people per city.</li>
<li>I've calculated the rate of ill and not_ill people</li>
<li>Because persons and cities share the same location, I've also infer the distance between each city and the nearest water feature.</li>
</ul>

<p>Now I want to check a possible correlation between the number of ill persons/rate of ill persons and the proximity to water features. I know, that the datasets are possibly not representative or suitable for my hyptothesis: I hold that there are more ill persons, where the distance to water features are lower than somewhere else.</p>

<p>I've already looked for some suitable methods, but there are so many possible ways, so I don't know, which of these could be useful for my notional use case. I've read about semivariogram, variogram, Ripley's K function, G-Function, correlation coefficient. As you can see, I have a broad range of <em>possible</em> methods, but unfortunately not this necessary expertise.<br>
My questions:</p>

<ol>
<li>Do you have any tip, which methods could be the most suitable?  </li>
<li>And another question: A risk/cluster analysis would be cool. Something, which shows the ""areas"" with a high risk of becoming ill based on the number of ill persons in a city. But I think there are two problems: I have to interpolate my dataset!? And a cluster analysis is only possible by using polygons, right? Interesting R packages (from my point of view) could be <code>SpatialEpi</code> or <code>DCluster</code> as well as <code>spatsat</code>.</li>
<li>And one last question: is there any ""general"" problem of having many persons at one location? I know that this is not the best dataset, because ideally each point has a own position...</li>
</ol>

<p>To give you a better overview, I've prepared some code in R, which loads my dataset and which plots the picture, which I've included here.</p>

<pre><code>library(RgoogleMaps)
library(ggplot2)
library(ggmap)
library(sp)
library(fossil)

persons = read.csv(""http://pastebin.com/raw.php?i=AuAQNqVt"", header = TRUE, stringsAsFactors=FALSE)
city= read.csv(""http://pastebin.com/raw.php?i=ZfPDFYCK"", header = TRUE, stringsAsFactors=FALSE)
water= read.csv(""http://pastebin.com/raw.php?i=hQRvMZwE"", header = TRUE, stringsAsFactors=FALSE)

# plot data
gc &lt;- geocode('new york, usa')
center &lt;- as.numeric(gc)  
G &lt;- ggmap(get_googlemap(center = center, color = 'bw', scale = 1, zoom = 11, maptype = ""terrain"", frame=T), extent=""device"")
G1 &lt;- G + geom_point(aes(x=POINT_X, y=POINT_Y ),data=city_all_parameters, shape = 22, color=""black"", fill = ""yellow"", size = 4) + geom_point(aes(x=POINT_X, y=POINT_Y ),data=persons, shape = 8, color=""red"", size=2.5) + geom_point(aes(x=POINT_X, y=POINT_Y ),data=water, color=""blue"", size=1)
plot(G1)
</code></pre>

<p>I'm excited to hear from you :)</p>
"
"0.0419590679148345","0.0419960525565808"," 88722","<p>I am building a regression model of time series data in R, where my primary interest is the coefficients of the independent variables. The data exhibit strong seasonality with a trend.</p>

<p><img src=""http://i.stack.imgur.com/GYxaU.png"" alt=""Original data""></p>

<p>The model looks good, with four of the six regressors significant:
<img src=""http://i.stack.imgur.com/ZmoSd.png"" alt=""Model""></p>

<p>Here are the OLS residuals:
<img src=""http://i.stack.imgur.com/EIybo.png"" alt=""Residuals""></p>

<p>I used auto.arima to select the sARIMA structure, and it returns the model (0,1,1)(1,1,0)[12].</p>

<pre><code>fit.ar &lt;- auto.arima(at.ts, xreg = xreg1, stepwise=FALSE, approximation=FALSE)
summary(fit.ar)

Series: at.ts 
ARIMA(0,1,1)(1,1,0)[12]                    

Coefficients:
          ma1    sar1      v1       v2      v3       v4         v5
      -0.7058  0.3974  0.0342  -0.0160  0.0349  -0.0042  -113.4196
s.e.   0.1298  0.2043  0.0239   0.0567  0.0555   0.0333   117.1205

sigma^2 estimated as 3.86e+10:  log likelihood=-458.13
AIC=932.26   AICc=936.05   BIC=947.06

Training set error measures:
                   ME     RMSE      MAE       MPE     MAPE      MASE
Training set 7906.896 147920.3 103060.4 0.1590107 3.048322 0.1150526
</code></pre>

<p>My question is this: based on the parameter estimates and s.e. of the regressors, I believe that none of them are significant - is this correct, and if so, what does it imply if my goal is to interpret the relative importance of these predictors as opposed to forecasting?</p>

<p>Any other advice relative to the process of building this model is welcome and appreciated.</p>

<p>Here are the ACF and PACF for the residuals:</p>

<p><img src=""http://i.stack.imgur.com/a3Gvy.png"" alt=""ACF-PACF""></p>

<pre><code>&gt; durbinWatsonTest(mod.ols, max.lag=12)
 lag Autocorrelation D-W Statistic p-value
   1     0.120522674     1.6705144   0.106
   2     0.212723044     1.4816530   0.024
   3     0.159828108     1.5814771   0.114
   4     0.031083831     1.8352377   0.744
   5     0.081081308     1.6787808   0.418
   6    -0.024202465     1.8587561   0.954
   7    -0.008399949     1.7720761   0.944
   8     0.040751905     1.6022835   0.512
   9     0.129788310     1.4214391   0.178
  10    -0.015442379     1.6611922   0.822
  11     0.004506292     1.6133994   0.770
  12     0.376037337     0.7191359   0.000
 Alternative hypothesis: rho[lag] != 0
</code></pre>
"
"NaN","NaN"," 92625","<p>R doesn't return the correlation coefficient's variance (or standard error) when coding <code>summary(linmod)</code>, <code>linmod</code> being a linear model with one stochastic variable. Wouldn't it be reasonable to first check this variance when reflecting on how reliable <code>linmod</code> is in terms of correlation, even before dealing with, say, the standard error of the slope which is returned by the <code>summary</code> code?</p>
"
"0.125877203744503","0.111989473484215"," 93417","<p>This question is a prolongation of this question: <a href=""http://stats.stackexchange.com/questions/11096/how-to-interpret-coefficients-in-a-poisson-regression"">How to interpret coefficients in a Poisson regression?</a></p>

<p>If we follow the (almost) exact same routine, but we add correlation between the variablese treatment and improved (just for the sake of my question, which is interpreting the output), we get:</p>

<pre><code>treatment     &lt;- factor(rep(c(1, 2), c(43, 41)), 
                        levels = c(1, 2),
                        labels = c(""placebo"", ""treated""))
improved      &lt;- factor(rep(c(1, 2, 3, 1, 2, 3), c(29, 7, 7, 13, 7, 21)),
                        levels = c(1, 2, 3),
                        labels = c(""none"", ""some"", ""marked""))    
numberofdrugs &lt;- rpois(84, 10) + 1    
healthvalue   &lt;- rpois(84, 5)   
y             &lt;- data.frame(healthvalue, numberofdrugs, treatment, improved)
test          &lt;- glm(healthvalue~numberofdrugs+treatment+improved + treatment:improved, y, family=poisson)
summary(test)
</code></pre>

<p>Note the $\textbf{ treatment:improved}$ term I added inside the glm function. </p>

<p>Now, we get the following output:</p>

<pre><code>    Call:
glm(formula = healthvalue ~ numberofdrugs + treatment + improved + 
    treatment:improved, family = poisson, data = y)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.9261  -0.8733  -0.0296   0.5473   2.3358  

Coefficients:
                                 Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                      1.553051   0.184229   8.430   &lt;2e-16 ***
numberofdrugs                    0.004298   0.014242   0.302   0.7628    
treatmenttreated                 0.007399   0.149440   0.050   0.9605    
improvedsome                     0.358897   0.164891   2.177   0.0295 *  
improvedmarked                  -0.178360   0.203756  -0.875   0.3814    
treatmenttreated:improvedsome   -0.330336   0.265310  -1.245   0.2131    
treatmenttreated:improvedmarked  0.050617   0.260203   0.195   0.8458    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 97.805  on 83  degrees of freedom
Residual deviance: 89.276  on 77  degrees of freedom
AIC: 383.29

Number of Fisher Scoring iterations: 5
</code></pre>

<p>If we ignore what seems to be insignificant coefficients, I can ask my question:</p>

<p>I understand that, as in the original post, treatment=placebo and improved=none is the base level for those variables, and thus are set to zero. My question is, why does it not exist any interaction terms with the base lavels for treatment=placebo and improved=none?</p>

<p>I thought setting the base levels to zero was just a construct, and in my mind there should still exist correlation between them...(?)</p>
"
"0.10277830647413","0.102868899974728"," 93423","<p>please help a sociologist struggling to get to grips with R and statistics in general..!</p>

<p>I've got a data set with 11 variables. I need to investigate the possible relationships between one of the variables (percentage of smokers in a population) and the rest of them- things like unemployment rate, education level, and so on. I'm working purely with linear regression.</p>

<p>I'm getting there with correlation, but having looked around at numerous examples on the web, I'm still unsure with regards to method. Apologies if it's completely obvious, but I can't seem to find an answer anywhere.</p>

<p>As I see it, it seems sensible to look for correlation between smoking and all the other variables firstly with scatterplots, and then by calculating the correlation coefficient if there is an identifiable linear relationship (Pearson's r or Spearman, depending on normality). If we then continue and do separate simple linear regressions between smoking rate and our identified variable, that all seems well and good.</p>

<p>But where does multiple regression fit in here? Does it make any sense, statistically speaking, to do simple linear regression first and then multiple linear regression also? Or would it be best to just jump straight from testing correlation to multiple regression?</p>

<p>Any help much appreciated!</p>
"
"0.132686223108569","0.119522860933439"," 95484","<p>I have a decision support system that recommends Treatments (<code>T1</code> to <code>T8</code>) to patients, where <code>T</code>'s are nominal (i.e. no ordering or rating between them). I compare the system recommendations with the recorded treatments in a confusion matrix, where the columns list computer recommendations and the rows list recorded treatments:</p>

<pre><code>cmg &lt;- matrix(c(1639, 116, 49, 35, 138, 0, 0, 236,
                 150, 274, 27, 21,  28, 0, 0,  73,
                  22,  24, 58,  9,  94, 0, 0,  30,
                  33,  27, 31, 21, 146, 0, 0,  49,
                  14,   9,  5,  1,  49, 0, 0,  22,
                   1,   0,  1,  1,   7, 0, 0,   6,
                  11,   0,  0,  1,  14, 0, 0,  21,
                 201,  11,  8,  5,  49, 0, 0, 253), 
              ncol=8,dimnames = rep(list(c(""T1"",""T2"",""T3"",""T4"",""T5"", ""T6"",""T7"", ""T8"")),2))
</code></pre>

<p>Based on <code>cmg</code>, I need to provide some <strong>agreement statistics</strong> and report how much the system agrees with recorded treatments. </p>

<p>I checked this <a href=""http://www.john-uebersax.com/stat/agree.htm#recs"" rel=""nofollow"">link</a>, which suggests finding <a href=""http://www.john-uebersax.com/stat/raw.htm"" rel=""nofollow"">overall and specific agreements</a> for nominal data. Here, specific agreements are calculated by collapsing the matrix to a binary confusion matrix [2*2] for each <code>Ti</code>. Also, they suggest calculating Cohen's Kappa both for <code>cmg</code> and for each collapsed matrix separately. 
I do these as below:</p>

<pre><code>require(psych)
# Overall agreement
overall_agg &lt;- sum(diag(cmg))/sum(cmg)

# Overall Cohen's Kappa for cmg
unweighted_kappa &lt;- cohen.kappa( cmg, n.obs=sum(cmg) )

# initialise containers
spec_agr_guideline &lt;- list()        
collapsed_mat_guideline &lt;- list() 
unweighted_kappa_psych &lt;- list()

# loop through all treatments    
for (i in seq(1,nrow(cmg)) ) {
  # Specific agreements
  spec_agr_guideline [i] &lt;- 2*cmg[i,i] / (sum(cmg[i,]) + sum (cmg[,i]) )
  # Collapsed positive agreement confusion matrices per treatment
  collapsed_mat_guideline[[i]] &lt;- matrix(c(cmg[i,i],             sum(cmg[i,])-cmg[i,i],
                                         sum(cmg[,i])-cmg[i,i],  sum(cmg)-sum(cmg[i,])-sum(cmg[,i])+cmg[i,i]), 
                                       ncol=2)
  # Calculate unweighted Cohen's Kappa per collapsed (binary) confusion amtrix
  unweighted_kappa_psych[[i]] &lt;- cohen.kappa( collapsed_mat_guideline[[i]], n.obs=sum(collapsed_mat_guideline[[i]]) )
}
</code></pre>

<p><strong>Questions:</strong></p>

<p><strong>1)</strong> Do I report the magnitudes of the Cohen's Kappas, or their p-values or something else? How can I interpret the Kappa results?E.g. for <code>cmg</code>, I get the output below:</p>

<pre><code>unweighted_kappa
Cohen Kappa and Weighted Kappa correlation coefficients and confidence boundaries 
                 lower estimate upper
unweighted kappa  0.34     0.36  0.38
</code></pre>

<p><strong>2)</strong> It is said that the usefulness of the Kappa values are <a href=""http://www.john-uebersax.com/stat/kappa.htm"" rel=""nofollow"">open to debate</a>. Are there any other agreement stats that I need to report in my (nominal data) case? For instance, in the same link, they suggest <a href=""http://www.john-uebersax.com/stat/mcnemar.htm#stuart"" rel=""nofollow"">Stuart-Maxwell, McNemar tests</a>, etc..</p>
"
"0.125877203744503","0.125988157669742"," 95844","<p>Given the following data frame:</p>

<pre><code>df &lt;- data.frame(x1 = c(26, 28, 19, 27, 23, 31, 22, 1, 2, 1, 1, 1),
                 x2 = c(5, 5, 7, 5, 7, 4, 2, 0, 0, 0, 0, 1),
                 x3 = c(8, 6, 5, 7, 5, 9, 5, 1, 0, 1, 0, 1),
                 x4 = c(8, 5, 3, 8, 1, 3, 4, 0, 0, 1, 0, 0),
                 x5 = c(1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0),
                 x6 = c(2, 3, 1, 0, 1, 1, 3, 37, 49, 39, 28, 30))
</code></pre>

<p>Such that</p>

<pre><code>&gt; df
   x1 x2 x3 x4 x5 x6
1  26  5  8  8  1  2
2  28  5  6  5  1  3
3  19  7  5  3  1  1
4  27  5  7  8  1  0
5  23  7  5  1  1  1
6  31  4  9  3  0  1
7  22  2  5  4  1  3
8   1  0  1  0  0 37
9   2  0  0  0  0 49
10  1  0  1  1  0 39
11  1  0  0  0  0 28
12  1  1  1  0  0 30
</code></pre>

<p>I would like to group these 12 individuals using hierarchical clusters, and using the correlation as the distance measure. So this is what I did:</p>

<pre><code>clus &lt;- hcluster(df, method = 'corr')
</code></pre>

<p>And this is the plot of <code>clus</code>:</p>

<p><img src=""http://i.stack.imgur.com/ALfbr.png"" alt=""dendogram""></p>

<p>This <code>df</code> is actually one of 69 cases I'm doing cluster analysis on. To come up with a cutoff point, I have looked at several dendograms and played around with the <code>h</code> parameter in <code>cutree</code> until I was satisfied with a result that made sense for most cases. That number was <code>k = .5</code>. So this is the grouping we've ended up with afterwards:</p>

<pre><code>&gt; data.frame(df, cluster = cutree(clus, h = .5))
   x1 x2 x3 x4 x5 x6 cluster
1  26  5  8  8  1  2       1
2  28  5  6  5  1  3       1
3  19  7  5  3  1  1       1
4  27  5  7  8  1  0       1
5  23  7  5  1  1  1       1
6  31  4  9  3  0  1       1
7  22  2  5  4  1  3       1
8   1  0  1  0  0 37       2
9   2  0  0  0  0 49       2
10  1  0  1  1  0 39       2
11  1  0  0  0  0 28       2
12  1  1  1  0  0 30       2
</code></pre>

<p>However, I am having trouble interpreting the .5 cutoff in this case. I've taken a look around the Internet, including the help pages <code>?hcluster</code>, <code>?hclust</code> and <code>?cutree</code>, but with no success. The farthest I've become to understanding the process is by doing this:</p>

<p>First, I take a look at how the merging was made:</p>

<pre><code>&gt; clus$merge
      [,1] [,2]
 [1,]   -9  -11
 [2,]   -8  -10
 [3,]    1    2
 [4,]  -12    3
 [5,]   -1   -4
 [6,]   -3   -5
 [7,]   -2   -7
 [8,]   -6    7
 [9,]    5    8
[10,]    6    9
[11,]    4   10
</code></pre>

<p>Which means everything started by joining observations 9 and 11, then observations 8 and 10, then steps 1 and 2 (i.e., joining 9, 11, 8 and 10), etc. Reading about the <code>merge</code> value of <code>hcluster</code> helps understand the matrix above.</p>

<p>Now I take a look at each step's height:</p>

<pre><code>&gt; clus$height
[1] 1.284794e-05 3.423587e-04 7.856873e-04 1.107160e-03 3.186764e-03 6.463286e-03 
    6.746793e-03 1.539053e-02 3.060367e-02 6.125852e-02 1.381041e+00
&gt; clus$height &gt; .5
[1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE
</code></pre>

<p>Which means that clustering stopped only in the final step, when the height finally goes above .5 (as the Dendogram had already pointed, BTW).</p>

<p>Now, here is my question: <strong>how do I interpret the heights?</strong> Is it the ""remainder of the correlation coefficient"" (please don't have a heart attack)? I can reproduce the height of the first step (joining of observations 9 and 11) like so:</p>

<pre><code>&gt; 1 - cor(as.numeric(df[9, ]), as.numeric(df[11, ]))
[1] 1.284794e-05
</code></pre>

<p>And also for the following step, that joins observations 8 and 10:</p>

<pre><code>&gt; 1 - cor(as.numeric(df[8, ]), as.numeric(df[10, ]))
[1] 0.0003423587
</code></pre>

<p>But the next step involves joining those 4 observations, and I don't know:</p>

<ol>
<li>The correct way of calculating this step's height</li>
<li>What each of those heights actually means.</li>
</ol>
"
"0.10277830647413","0.102868899974728"," 95891","<p>I'm running a logistic regression model where anecdotally I expected age to be a very large factor. If you see from the charts I made in Excel before running the model through R, this is how the support lines up by age:</p>

<p><img src=""http://i.stack.imgur.com/oEVZQ.jpg"" alt=""enter image description here""></p>

<p>Looks pretty significant.</p>

<p>Though when I run the model, as you can see below, age is the <em>only</em> thing that's not significant -- which was very surprising:</p>

<pre><code>&gt; attach(mydata) 
&gt; 
&gt; # Define variables 
&gt; 
&gt; Y &lt;- cbind(support)
&gt; X &lt;- cbind(sex, region, age, supportscore1, supportscore2, county)
&gt;
&gt; # Logit model coefficients 
&gt; 
&gt; logit &lt;- glm(Y ~ X, family=binomial (link = ""logit""), na.action = na.exclude) 
&gt; 
&gt; summary(logit) 

Call:
glm(formula = Y ~ X, family = binomial(link = ""logit""), na.action = na.exclude)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.1019  -0.7609   0.5231   0.7101   2.3965  

Coefficients:
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)            4.013446   0.440962   9.102  &lt; 2e-16 ***
Xsex                  -0.229256   0.104859  -2.186 0.028792 *  
Xregion               -1.103308   0.091497 -12.058  &lt; 2e-16 ***
Xage                   0.004569   0.003209   1.424 0.154512    
Xsupportscore1        -0.019262   0.005732  -3.360 0.000778 ***
Xsupportscore2         0.019810   0.005264   3.764 0.000168 ***
Xcounty               -0.047581   0.011161  -4.263 2.02e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2871.5  on 2072  degrees of freedom
Residual deviance: 2245.5  on 2066  degrees of freedom
  (66 observations deleted due to missingness)
AIC: 2259.5

Number of Fisher Scoring iterations: 4
</code></pre>

<p>My only guess on this is that the previous support scores (both 0-100 numerical values) I'm using may have already taken age into account, and the model doesn't want to count it twice. Though, to compare, region and county are just two different ways of cutting up the geography -- and those both seem significant.</p>

<p>Can somebody let me know what you would think if your model told you that age wasn't significant when in clearly is? Trying to figure out if there's a way of thinking about it that I'm missing or if something in my code is wrong.</p>

<p>Thanks!</p>

<p>--
<strong>EDIT</strong></p>

<p>Pairs plot added to show correlation (despite some factors being categorical):</p>

<pre><code>pairs(~sex + region +  age + supportscore1 + supportscore2 + county, data=mydata)
</code></pre>

<p><img src=""http://i.stack.imgur.com/N2IG4.jpg"" alt=""enter image description here""></p>
"
"0.0593390829096927","0.0593913870916499"," 97051","<p>According to some articles (e.g. <a href=""http://brenocon.com/blog/2012/03/cosine-similarity-pearson-correlation-and-ols-coefficients/"" rel=""nofollow"">here</a>) correlation is just a centered version of cosine similarity. I use the following code to calculate the cosine similarity matrix of the column vectors of a matrix <code>X</code> (slightly modified from <a href=""http://stats.stackexchange.com/questions/31565/is-there-an-r-function-that-will-compute-the-cosine-dissimilarity-matrix"">here</a>):</p>

<pre><code>cos.sim &lt;- function(ix) 
{
  A = X[,ix[1]]
  B = X[,ix[2]]
  return(t(A)%*%B/sqrt(sum(A^2)*sum(B^2)))
}   
n &lt;- ncol(X) 
cmb &lt;- expand.grid(i=1:n, j=1:n) 
C &lt;- matrix(apply(cmb,1,cos.sim),n,n)
</code></pre>

<p><strong>My question</strong> <br>
Which modifications of the code above are needed to get the correlation matrix <code>cor(X)</code> instead of the cosine similarity matrix. I guess the changes are minimal but I can't see them at the moment.</p>
"
"0.157346504680629","0.167984210226323","100365","<p>We have route-level data (that I cannot share) on monthly bus ridership in New York City, creating a panel $N= 185$, $T=36$. We estimate a fixed effects model and random effects model with R's <code>plm</code> package. (For this MWE, I use the <code>Grunfeld</code> investment data, which illustrates the problems I am seeing fairly well).</p>

<pre><code>library(plm)
data(""Grunfeld"")
model_1A &lt;- lm(inv ~ value + capital, data= Grunfeld)
fe.plm &lt;- plm(formula(model_1A), model=""within"", index=c(""firm"", ""year""),
              data=Grunfeld)
re.plm &lt;- plm(formula(model_1A), model=""random"", index=c(""firm"", ""year""),
              data=Grunfeld)
</code></pre>

<p>A Hausman test indicates that the FE model is preferred, because the estimates differ (note that in the MWE, we fail to reject).</p>

<pre><code>phtest(fe.plm, re.plm)

##  Hausman Test

##data:  formula(model_1A)
##chisq = 2.3304, df = 2, p-value = 0.3119
##alternative hypothesis: one model is inconsistent

pbgtest(fe.plm)

##  Breusch-Godfrey/Wooldridge test for serial correlation in panel models

##data:  formula(model_1A)
##chisq = 65.0632, df = 20, p-value = 1.14e-06
##alternative hypothesis: serial correlation in idiosyncratic errors

pbgtest(re.plm)

##  Breusch-Godfrey/Wooldridge test for serial correlation in panel models

##data:  formula(model_1A)
##chisq = 69.9495, df = 20, p-value = 1.856e-07
##alternative hypothesis: serial correlation in idiosyncratic errors
</code></pre>

<p>This is where what I think should happen diverges from what many people try to do. My understanding of serial correlation is that it affects the standard errors but not the coefficients. This would suggest to me a serial correlation-robust standard error. For instance (<a href=""http://stats.stackexchange.com/a/60262/10026"">as in this answer</a>),</p>

<pre><code>fe.rse &lt;- sqrt(diag(vcovHC(fe.plm, type=""HC1"", cluster=""group"")))
re.rse &lt;- sqrt(diag(vcovHC(re.plm, type=""HC1"", cluster=""group"")))
</code></pre>

<p>** Why not just use sc-robust standard errors?**</p>

<p>But what many authors do instead is include specific AR(1) or ARMA disturbances because Stata makes this easy. For the FE models, we can use <code>gls</code> from the <code>nlme</code> package on demeaned data (note: <code>fe.plm</code> and <code>fe.gls</code> are virtually identical),</p>

<pre><code># within estimator is demeaned
demean &lt;- numcolwise(function(x) x - mean(x))
Grunfeld.dm &lt;- ddply(Grunfeld, .(firm), demean)
Grunfeld.dm$year &lt;- Grunfeld$year

fe.gls &lt;- gls(update(formula(model_1A), .~.-1), method=""ML"",  data=Grunfeld.dm)
fear.gls &lt;- update(fe.gls,  correlation = corAR1(form = ~ year | firm))
fearma.gls &lt;- update(fe.gls, correlation = corARMA(form = ~ year | firm, 
                                                   p=1,q=1))
</code></pre>

<p>The RE models can be estimated in with <code>lme</code> in the <code>nlme</code> package (again, <code>re.plm</code> and <code>re.lme</code> are identical).</p>

<pre><code>re.lme &lt;- lme(fixed = formula(model_1A), random = ~ 1|firm, data = mta)
rear.lme &lt;- update(re.lme,  correlation = corAR1(form = ~ year | firm))
rearma.lme &lt;- update(re.lme,  correlation = corARMA(form = ~ year | firm, 
                                                    p=1,q=1))
</code></pre>

<p>There are a few things I don't understand about this:</p>

<ul>
<li><p><strong>Why do the coefficients change when serial correlation doesn't (shouldn't?) affect estimates?</strong></p></li>
<li><p><strong>Can we still use the Hausman test to select between FE and RE models with autoregressive errors?</strong></p></li>
<li><p><strong>How can we test for residual autocorrelation? And if it exists, wouldn't we <em>still</em> need a robust standard error?</strong></p></li>
</ul>
"
"0.118678165819385","0.1187827741833","101016","<p>My question is strongly related to this one: <a href=""https://stats.stackexchange.com/questions/11384/pca-and-component-scores-based-on-a-mix-of-continuous-and-binary-variables"">PCA and component scores based on a mix of continuous and binary variables</a>.
I will basically use the same code, but add a new nominal feature (x6) to the data set.</p>

<p>I want to apply a PCA on a dataset consisting of continuous, binary and categorical variables.</p>

<pre><code># Generate synthetic dataset
set.seed(12345)
n &lt;- 100
x1 &lt;- rnorm(n)
x2 &lt;- runif(n, -2, 2)
x3 &lt;- x1 + x2 + rnorm(n)
x4 &lt;- rbinom(n, 1, 0.5)
x5 &lt;- rbinom(n, 1, 0.6)
x6 &lt;- c(rep('A', 25), rep('B', 25), rep('C', 25), rep('D', 25))
data &lt;- data.frame(x1, x2, x3, x4, x5, x6)

# Correlation matrix with appropriate coefficients
# Pearson product-moment: 2 continuous variables
# Point-biserial: 1 continuous and 1 binary variable
# Phi: 2 binary variables
# For testing purposes use hetcor function
library(polycor)
C &lt;- as.matrix(hetcor(data=data))

# Run PCA
pca &lt;- princomp(covmat=C)
L &lt;- loadings(pca)
</code></pre>

<p>Now in order to calculate the component scores, it was suggested to multiply the data set with the loadings L, which works fine for numerical and binary variables, but not on categorical data. The following computation causes the categorical feature to be a vector of NAÂ´s.</p>

<pre><code>scores &lt;- data * L
</code></pre>

<p>How can I obtain the scores for this feature? Do I have to split it up into dummy variables to make this work?</p>
"
"0.119417600797712","0.132803178814933","108088","<p>I have some elementary problems understanding the consequences of using/adding a lagged  dependent variable in my  predictive model.  Iâ€™m trying to predict values $Y_{i,t+\tau}$  for  $\tau=1-3$ with:</p>

<p>$Y_{i,t+1}=a+bY_{i,t}+cX_{i,t}+e_{i,t+1}$</p>

<p>$Y_{i,t+2}=a+bY_{i,t}+cX_{i,t}+e_{i,t+2}$</p>

<p>$Y_{i,t+3}=a+bY_{i,t}+cX_{i,t}+e_{i,t+3}$ </p>

<p>I already performed a pooled regression where you basically ignore individual firm effects and time-effects and treat every subject equally. As I am trying to forecast different levels (in USD) and my data appears to be extremely tailed as it covers a few extremely large subjects (with extremely high values) but also many small subjects the predictions of the model perform rather poor as the intercept $a$ that is equal for all subject seems largely responsible for this. A fixed model however with individual intercepts is not valid with Lagged dependent variables as the LDV is correlated with the within errors. To account for the heavy tailed errors I already estimated the pooled model with the rlm package (robust lm) that produced slightly better results but overall they appear still very unsatisfactory.</p>

<p>I further read that adding LDVâ€™s results in biased and inconsistent estimators as there is severe correlation between the predictor variables and the model errors and that regular procedures for autocorrelation are not valid anymore. One solution I came across is the use of Instrumental Variables with an Anderson-Hsiao Estimator (i.e using a lag -2 that is not correlated with the error term (with non-autocorrelation assumed but how can you assume no autocorrelation if you incorporate a lag?) Another one is the Arellano Bond GMM estimator, however applying GMM you have to set up moment conditions and I have no idea how to do that and I donâ€™t know exactly how this methods work. What I care about is to obtain an unbiased estimator with valid coefficients and not about standard errors as I donâ€™t do inference. Are there any other strategies to cope with LDVâ€™s I am currently unaware of and what is the best/ideal/easiest way to deal with such matter? Do you best take care of some issues while you ignore others (e.g. autocorrelation)? Iâ€™m a little bit lost here.</p>
"
"0.0839181358296689","0.0839921051131616","108156","<p>My question is generally on Singular Value Decomposition (SVD), and particularly on  Latent Semantic Indexing (LSI).</p>

<p>Say, I have $ A_{word \times document} $ that contains frequencies of 5 words for 7 documents.</p>

<pre><code>A =  matrix(data=c(2,0,8,6,0,3,1,
                   1,6,0,1,7,0,1,
                   5,0,7,4,0,5,6,
                   7,0,8,5,0,8,5,
                   0,10,0,0,7,0,0), ncol=7, byrow=TRUE)
rownames(A) &lt;- c('doctor','car','nurse','hospital','wheel')
</code></pre>

<p>I get the matrix factorization for $A$ by using SVD: $A = U \cdot D \cdot V^T $. </p>

<pre><code>s = svd(A)
D = diag(s$d) # singular value matrix
S = diag(s$d^0.5 ) # diag matrix with square roots of singular values.
</code></pre>

<p>In <a href=""http://www.ling.ohio-state.edu/~kbaker/pubs/Singular_Value_Decomposition_Tutorial.pdf"">1</a> and <a href=""http://files.grouplens.org/papers/webKDD00.pdf"">2</a>, it is stated that: </p>

<p>$WordSim = U \cdot S$ gives the  <strong>word similarity matrix</strong>, where the rows of $WordSim $ represent different words.  </p>

<p><code>WordSim =  s$u %*% S</code></p>

<p>$DocSim= S \cdot V^T$ gives the  <strong>document similarity matrix</strong> where the columns of $DocSim$ represent different documents.</p>

<p><code>DocSim = S %*% t(s$v)</code></p>

<p><strong>Questions:</strong></p>

<ol>
<li>Algebraically, why are $WordSim$ and $DocSimS$ word/document similarity matrices? Is there an intuitive explanation?</li>
<li>Based on the R example given, can we make any intuitive word count / similarity observations by just looking at $WordSim$ and $DocSim$ (without using cosine similarity or correlation coefficient between rows / columns)? </li>
</ol>

<p><img src=""http://i.stack.imgur.com/3KJuT.png"" alt=""enter image description here""></p>
"
"0.111013258946721","0.111111111111111","109534","<p>I would like to display correlation coefficients in a table (ideally - with p-value).
However, my code produces exactly the same values for each period (so something is obviously wrong). Could you give me any advice:</p>

<pre><code>#first of all, I read my data table from CSV file:
imported &lt;- read.table (file=""/home/someone/data_for_R.csv"", header=TRUE, sep='\t', quote='""\'', dec=',', fill=FALSE, comment.char=""#"",  na.strings = ""NA"", nrows = -1, skip = 0, check.names = TRUE, strip.white = FALSE, blank.lines.skip = TRUE)

# Typing: class(imported[[""Period""]]) produces:
# [1] ""factor""

#Typing: levels(imported[[""Period""]]) produces:
# [1] ""Summer 2010"" ""Summer 2011"" ""Winter 2010"" ""Winter 2011"" ""Winter 2012""

xx &lt;- imported[c(""Period"",""Data1.MEAN"",""Data2.MEAN"")]
result &lt;- by(xx, xx$Period, function(x) {cor(xx$Data1.MEAN, xx$Data2.MEAN)})
    result.dataframe &lt;- as.data.frame(as.matrix(result))
    result.dataframe$C &lt;- rownames(result)
</code></pre>

<p><strong>EDIT:</strong></p>

<p>Code which reads file from Github:</p>

<pre><code>library(RCurl)
x &lt;- getURL(""https://raw.githubusercontent.com/kedziorm/testowe/master/data_for_R.csv"")
imported &lt;- read.csv (text=x, header=TRUE, sep='\t', quote='""\'', dec=',', fill=FALSE, comment.char=""#"",  na.strings = ""NA"", nrows = -1, skip = 0, check.names = TRUE, strip.white = FALSE, blank.lines.skip = TRUE)
xx &lt;- imported[c(""Period"",""Data1.MEAN"",""Data2.MEAN"")]
result &lt;- by(xx, xx$Period, function(x) {cor(xx$Data1.MEAN, xx$Data2.MEAN)})
    result.dataframe &lt;- as.data.frame(as.matrix(result))
    result.dataframe$C &lt;- rownames(result)
</code></pre>

<p><strong>EDIT:</strong>
This should finally work:</p>

<pre><code>x &lt;- ""Period\tDate\tData1.MEAN\tData1.MEDIAN\tData2.MEAN\tData2.MEDIAN\tData3.MEAN\tData3.MEDIAN\nWinter 2010\t26-03-2010\t0,3580917\t0,307479\t0,551191\t0,612853\t0,3476462\t0,3996462\nWinter 2010\t26-04-2010\t0,3016958\t0,2643808\t0,417791\t0,393714\t0,2811050286\t0,3061050286\nSummer 2010\t03-07-2010\t0,1916181\t0,1816603\t0,390925\t0,37385\t0,2183438286\t0,2923438286\nSummer 2010\t04-07-2010\t0,2548711\t0,1738567\t0,4349834\t0,4957131\t0,2467746286\t0,3437746286\nWinter 2011\t01-11-2010\t0,3393042\t0,2870481\t0,497295\t0,538132\t0,3210420857\t0,3690420857\nSummer 2011\t04-06-2011\t0,222748\t0,2218226\t0,363823\t0,275725\t0,2309696\t0,2809696\nSummer 2011\t05-06-2011\t0,241889\t0,1918457\t0,373566\t0,292997\t0,2306573429\t0,2966573429\nWinter 2012\t07-11-2011\t0,2264874\t0,2601413\t0,373048\t0,274139\t0,2456219143\t0,2756219143\nWinter 2012\t08-11-2011\t0,2414665\t0,2662565\t0,314382\t0,279857\t0,2348871429\t0,2598871429\nWinter 2012\t09-11-2011\t0,2817838\t0,2325952\t0,376063\t0,468148\t0,254412\t0,287412\nWinter 2012\t10-11-2011\t0,2476841\t0,2667485\t0,406902\t0,476582\t0,2632384571\t0,3632384571\n""
imported &lt;- read.csv (text=x, header=TRUE, sep='\t', quote='""\'', dec=',', fill=FALSE, comment.char=""#"",  na.strings = ""NA"", nrows = -1, skip = 0, check.names = TRUE, strip.white = FALSE, blank.lines.skip = TRUE)
xx &lt;- imported[c(""Period"",""Data1.MEAN"",""Data2.MEAN"")]
result &lt;- by(xx, xx$Period, function(x) {cor(xx$Data1.MEAN, xx$Data2.MEAN)})
    result.dataframe &lt;- as.data.frame(as.matrix(result))
    result.dataframe$C &lt;- rownames(result)
</code></pre>
"
"0.111890847772892","0.111989473484215","110278","<p>I fit two gee models with two different correlation structures, exchangeable vs ar(1), which resulted in very different p.values. I'm wondering what reasons have led that. Would somebody offer an explanation?</p>

<pre><code>&gt; summary(fit1)

Call:
geeglm(formula = LAU ~ SAMPLENO, data = bd, id = MAGE, corstr = ""exchangeable"")

 Coefficients:
             Estimate   Std.err   Wald Pr(&gt;|W|)    
(Intercept)  0.067349  0.004537 220.32   &lt;2e-16 ***
SAMPLENO    -0.002800  0.000947   8.74   0.0031 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Estimated Scale Parameters:
            Estimate  Std.err
(Intercept) 0.000425 0.000166

Correlation: Structure = exchangeable  Link = identity 

Estimated Correlation Parameters:
      Estimate Std.err
alpha    0.206   0.249
Number of clusters:   9   Maximum cluster size: 8 




&gt; summary(fit2)

Call:
geeglm(formula = LAU ~ SAMPLENO, data = bd, id = MAGE, corstr = ""ar1"")

 Coefficients:
            Estimate  Std.err   Wald Pr(&gt;|W|)    
(Intercept)  0.06392  0.00588 118.25   &lt;2e-16 ***
SAMPLENO    -0.00142  0.00219   0.42     0.52    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Estimated Scale Parameters:
            Estimate  Std.err
(Intercept) 0.000427 0.000172

Correlation: Structure = ar1  Link = identity 

Estimated Correlation Parameters:
      Estimate Std.err
alpha    0.659   0.221
Number of clusters:   9   Maximum cluster size: 8
</code></pre>
"
"NaN","NaN","110597","<p>My question is simple: is there a function in <code>R</code> which estimates the linear regresion model in a similar fashion as <code>lm</code>, but only using the means, variances, and covariance (correlations), i.e. the sufficient statistics? I am looking for a function to which I can input these statistics (plus sample size) and it returns regression coefficients and tests.</p>
"
"0.0593390829096927","0.0593913870916499","110719","<p>I want to estimate a covariance matrix from data with some missing values. Ideally I'd like an R package but python could be ok. </p>

<p>R has some built in ways of doing this. You can use </p>

<pre><code>cov.mat=cov(X,use='pairwise')
</code></pre>

<p>Or the same using cor (correlation). The trouble is that if you do this with cov, the matrix will not be guaranteed to be positive definite. If you do cov2cor(cor.mat), you will find correlation coefficients outside of [-1,1]. Using pairwise with cor seems to handle this. Then I could use the diagonal variances to go from cor.mat to cov.mat. Still, this is probably not optimal. </p>

<p>There appears to be a few packages that claim to do this (mvnmle, rsem) but neither appear to work. rsem fails to run for me. mvnmle can only handle up to 50 variables. I need to handle roughly 1500 variables. Would like it to run in a few seconds.  </p>

<p>Anyone know of a good package for this?</p>
"
"0.173968166256234","0.174121509755601","112670","<p>I'm working on a dataset with the following variables:</p>

<pre><code>Y:  a boolean telling whether the subject has experienced a seizure
ID:  the id of the subject
Sess:  the subject's session number (a subject has been observed multiple times)
X3:  numeric measurements of the subject's behavior during the session 
X4:  ""
X6:  ""
</code></pre>

<p>The idea is to see if the behavior measurements(X3,X4,X6) can be used predict the Seizure status (Y) in the population.  If there were just one session per subject I'd model the data with a logistic regression, but since each subject has multiple sessions the observations cannot be assumed independent.</p>

<p>It seems a GEE logistic model makes the most sense for this dataset, but I'm having trouble understanding the results when compared to a regular glm.  When introducing correlation structure to the GEE equation the signs of the coefficients are reversed, and the predicted values are opposite of what they would be with an independent correlation structure.</p>

<pre><code>library(""geepack"")
#regular logistic model
mod0 = glm(Y~X3+X4+X6, family=binomial(""logit""), data=mice)
#gee logistic model, independent correlation structure
mod1 = geeglm(Y~X3+X4+X6, id=ID, family=binomial(""logit""), corstr=""indep"", scale.fix=T, waves=Sess, data=mice)
#gee logistic model, exchangeable correlation structure
mod2 = geeglm(Y~X3+X4+X6, id=ID, family=binomial(""logit""), corstr=""exchangeable"", scale.fix=T, waves=Sess, data=mice)
</code></pre>

<p>As expected, the parameter estimates of the glm model(mod0) and the independent correlation gee model(mod1) are the same.  But when an exchangeable correlation structure(mod2) is introduced, the estimates are completely different and change sign.</p>

<pre><code>&gt; 
&gt; summary(mod1)                    

Call:
geeglm(formula = Y ~ X3 + X4 + X6, family = binomial(""logit""), 
    data = mice, id = ID, waves = Sess, corstr = ""indep"", scale.fix = T)

 Coefficients:
            Estimate Std.err  Wald Pr(&gt;|W|)    
(Intercept)   -1.494   0.459 10.59  0.00114 ** 
X3            -2.377   0.626 14.42  0.00015 ***
X4             1.090   0.398  7.50  0.00619 ** 
X6             1.233   0.403  9.36  0.00222 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Scale is fixed.

Correlation: Structure = independenceNumber of clusters:   28   Maximum cluster size: 4 
&gt; summary(mod2)

Call:
geeglm(formula = Y ~ X3 + X4 + X6, family = binomial(""logit""), 
    data = mice, id = ID, waves = Sess, corstr = ""exchangeable"", 
    scale.fix = T)

 Coefficients:
            Estimate Std.err Wald Pr(&gt;|W|)   
(Intercept)  -0.8928  0.4255 4.40   0.0359 * 
X3            0.1059  0.0383 7.64   0.0057 **
X4           -0.0427  0.0299 2.04   0.1535   
X6           -0.0528  0.0213 6.16   0.0131 * 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Scale is fixed.

Correlation: Structure = exchangeable  Link = identity 

Estimated Correlation Parameters:
      Estimate Std.err
alpha     1.09   0.133
Number of clusters:   28   Maximum cluster size: 4 
</code></pre>

<p>My biggest concern is that the predicted Y's of the two GEE models show opposite trends, i.e. behaviors that would predict a positive seizure status in mod1, predict a negative seizure status in mod2.    </p>

<pre><code>plot(mod1$fitted, mod2$fitted)
</code></pre>

<p><img src=""http://i.stack.imgur.com/nAogH.png"" alt=""scatter plot of fitted values from mod1, mod2""></p>

<p>Also, what's with the estimated correlation parameter of alpha = 1.09 in mod2?  Unless I'm interpreting this wrong, shouldn't this always fall between -1 and 1?</p>

<pre><code>Correlation: Structure = exchangeable  Link = identity 

Estimated Correlation Parameters:
      Estimate Std.err
alpha     1.09   0.133
</code></pre>

<p>It seems odd to me that the results are completely flipped depending on whether or not the observations are assumed to have dependence structure.  Can anyone else offer insight on this behavior?</p>

<p>Here is the data:</p>

<pre><code>&gt; dput(mice)
structure(list(Y = c(0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 1L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 
0L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 
0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L
), ID = c(1L, 1L, 1L, 2L, 2L, 2L, 3L, 3L, 3L, 4L, 4L, 4L, 5L, 
5L, 5L, 6L, 6L, 6L, 7L, 7L, 8L, 8L, 9L, 9L, 10L, 10L, 11L, 11L, 
11L, 12L, 12L, 12L, 13L, 13L, 13L, 13L, 14L, 14L, 14L, 14L, 15L, 
15L, 15L, 15L, 16L, 16L, 16L, 16L, 17L, 17L, 17L, 18L, 18L, 18L, 
19L, 19L, 19L, 20L, 20L, 20L, 21L, 21L, 21L, 22L, 22L, 22L, 23L, 
23L, 23L, 23L, 24L, 24L, 24L, 24L, 25L, 25L, 25L, 25L, 26L, 26L, 
26L, 26L, 27L, 27L, 27L, 27L, 28L, 28L, 28L, 28L), Sess = c(2L, 
3L, 4L, 2L, 3L, 4L, 2L, 3L, 4L, 2L, 3L, 4L, 2L, 3L, 4L, 2L, 3L, 
4L, 3L, 4L, 3L, 4L, 3L, 4L, 3L, 4L, 1L, 3L, 4L, 1L, 3L, 4L, 1L, 
2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 2L, 
3L, 4L, 2L, 3L, 4L, 2L, 3L, 4L, 2L, 3L, 4L, 1L, 2L, 3L, 1L, 2L, 
3L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 
4L, 1L, 2L, 3L, 4L, 1L, 2L, 3L, 4L), X3 = c(0.511015060794264, 
0.898356533693696, 0.798280430157052, 1.31144372617517, 0.829923189452201, 
0.289089506643144, -0.763028944257538, -0.944459588217789, -1.16474609928919, 
-0.182524267014845, -0.338967193889711, -0.896037509887988, 0.00426073081308205, 
0.0576592603165749, -1.4984737260339, 1.34752684212464, 0.106461438449047, 
-0.108424579472268, -2.85991432039569, -0.230115838261355, -1.54479536845993, 
-1.23693649938367, -1.53704616612456, -1.04825100254239, 0.142768659484482, 
0.28358135516745, -0.236302896321009, 0.708743856986942, -0.507503006972081, 
0.401550711842527, -0.16928449007327, 0.867816722958898, 0.487459858572122, 
1.35172112260613, 0.14742652989871, 0.742155288774287, 0.348552056119878, 
-0.82489952485408, 0.0366834636917457, -0.731010479377091, 0.979544093857171, 
1.4161996712129, 0.661035838980077, 0.600235250313596, -1.10872641912335, 
-0.212101744145196, -0.919575135240643, -0.813993077336991, -0.547068540188791, 
-0.0260198210967738, -0.0962240349391501, -0.251025721625606, 
0.894913664382802, -0.21993004239326, 0.0628839847717805, 1.77763503559622, 
0.718459471596243, 0.984412886705251, 0.77603470471174, 0.486187732642953, 
1.78012655684609, 1.31622243756713, 1.29635178661133, 0.427995111986702, 
0.993748401511881, 0.387623239882247, 0.42006794384777, -0.815889182132972, 
-0.897540332229183, -1.041943103505, 0.379425827374942, -1.00707718576756, 
-0.889182530787803, 0.148432805676879, -0.287928359114935, -0.747152636892815, 
-1.41003790431546, -0.611571256991109, -1.02569548477235, -1.02700056733181, 
-1.45808867127733, -1.47973458605138, 2.23643966561508, 2.69397876103083, 
0.81841473415516, 2.12167589051282, 0.267133799544379, -0.326215175076418, 
-1.08788244901967, -1.18733017947214), X4 = c(0.050598970482242, 
-0.0279694583060402, 0.999225143631274, 0.199872317584803, 0.779316284168575, 
-0.3552692229881, -0.232161792808608, -0.333479851296274, -0.748169603107953, 
-0.57785843363913, -0.480747933235349, -0.740466500603612, -0.618559437949564, 
-0.591541699294345, -0.538855647639331, 0.431376763414175, -0.327931008191724, 
-0.469416282917978, -0.659224551441466, -0.55285236403596, -0.637082867133913, 
-0.780321541069982, -0.40539035027884, -0.54024676972473, -0.185562290173831, 
0.054439450703482, 0.624097793456316, 0.24018937319873, -0.264194638773171, 
-0.389590537012038, -0.42771343162755, -0.738790918078674, -0.122411831542788, 
0.600119921164627, 0.0442597161778152, -0.0955011351192086, -0.521259643827527, 
-0.550050365103255, -0.504566887441653, -0.506571005423286, 0.523650149759566, 
0.341920916685254, -0.396343801985993, -0.366532239883921, -0.739276449002057, 
-0.56054127343218, -0.587601788901296, -0.56798329186843, -0.454937006653748, 
-0.672730639942183, -0.564864467446687, -0.678853515419629, 0.573072971483937, 
0.596973680548765, 0.0403978228634349, 1.93617633381248, 2.54301964691615, 
0.363075891004736, 0.0205658396444095, 0.560923287570261, 1.24212005971229, 
2.32518793880728, 2.69979166871713, 0.626868716830008, 0.219463581391793, 
0.236477261174534, -0.115429539698909, -0.49754151674106, -0.40827433350644, 
-0.0433283703798658, -0.578451015506926, -0.714208713291922, 
-0.802387726290423, -0.836794085697031, -0.471800405613954, -0.668030208971065, 
-0.610945789491312, -0.780838257914176, -0.411360572155088, -0.494388869332376, 
-0.63231547268951, -0.743853022088574, 4.90627675753856, 3.38455016460328, 
0.859445571488139, 2.42212262705776, -0.324759764820016, -0.541581784452693, 
-0.485324968098865, -0.770539730529603), X6 = c(0.0150287583709043, 
-0.151984283645294, -0.347950002037732, 0.379891135882966, 0.129107019894704, 
-0.314047917638528, -0.516381047940779, -0.751192211830495, -0.884460389494645, 
-0.462363867892961, -0.397583161539858, -0.559528880497725, -0.842987555132397, 
-0.922797893301111, -1.01175722882932, 0.32346425626624, -0.610909601293237, 
-0.605155952259822, -1.29840867980623, 0.0793710626694382, -0.806959976634144, 
-0.674523251142452, -0.960113466801064, -0.783836535852452, -0.0665321645536412, 
0.482235339656537, -0.319499220427413, -0.115345965733089, -0.30806448545927, 
0.251747727063608, -0.305013811851957, -0.931916656036151, 0.415032839884745, 
0.337184728843034, 0.0584335852357015, -0.0712185313438638, 0.78632612201797, 
0.490831043388539, 0.8902425262631, 0.160088439571744, 0.90343086944952, 
0.928495373121098, -0.389259569427933, -0.304578433259833, -0.593364448723133, 
-0.411333868741105, -0.882691663964141, -0.91208274239495, -0.708633954450382, 
-0.339396626779965, -0.420927315080057, -0.421383857909298, 0.407474183771483, 
0.629710767351175, -0.438726438495567, 2.40977730548689, 2.47250810430208, 
0.783562677342961, 0.781304150896319, 0.563221804716475, 1.85514126067038, 
1.30723846671955, 1.94869625911545, 0.876751836832149, 0.626629859119409, 
0.067113945916172, 3.54280776301513, 0.0082773667305384, -0.311414481848668, 
-0.732779325538588, -0.594477082903005, -1.0385239418576, -1.04141739541776, 
-0.99472304141247, -0.599659297534257, -0.804801224448196, -1.13096932958525, 
-0.641957537144073, -0.722959119516237, -0.671146043591047, -0.714432955420477, 
-0.766750949574034, 1.20993739830475, 2.79011376379402, 2.64532317075082, 
2.54251033029822, -0.539516582572252, -0.6419726544563, -0.663768795224503, 
-0.644829826467113)), .Names = c(""Y"", ""ID"", ""Sess"", ""X3"", ""X4"", 
""X6""), row.names = c(NA, -90L), class = ""data.frame"")
</code></pre>
"
"0.0938233281301002","0.0751248226425348","113502","<p>I am trying to run a regression using about 80 independent variables. The problem is that the last 20+ coefficients return NA. If I condense the range of data to within 60, I get coefficients for everything just fine. Why am I getting these NAs and how do I resolve it? I need to reproduce this code using all of these variables.</p>

<pre><code>composite &lt;- read.csv(""file.csv"", header = T, stringsAsFactors = FALSE)
composite &lt;- subset(composite, select = -ncol(composite))
composite &lt;- subset(composite, select = -Date)
model1 &lt;- lm(indepvariable ~., data = composite, na.action = na.exclude)
</code></pre>

<p>composite is a data frame with 82 variables.</p>

<p>UPDATE:</p>

<p>What I have done is found a way to create an object that contains only the significantly correlated variables, to narrow the number of independent variables down. </p>

<p>I have a variable now: sigvars, which is the names of an object that sorted a correlation matrix and picked out only the variables with correlation coefficients >0.5 and &lt;-0.5. Here is the code:</p>

<pre><code>sortedcor &lt;- sort(cor(composite)[,1])
regvar = NULL

k = 1
for(i in 1:length(sortedcor)){
  if(sortedcor[i] &gt; .5 | sortedcor[i] &lt; -.5){
    regvar[k] = i
  k = k+1
 }
}
regvar

sigvars &lt;- names(sortedcor[regvar])
</code></pre>

<p>However, it is not working in my lm() function:</p>

<pre><code>model1 &lt;- lm(data.matrix(composite[1]) ~ sigvars, data = composite)
</code></pre>

<p>Error: Error in model.frame.default(formula = data.matrix(composite[1]) ~ sigvars,  : 
  variable lengths differ (found for 'sigvars')</p>
"
"0.0726752374667264","0.0727392967453308","114221","<p>I plotted normal probability plot in R using <code>qqnorm</code> and <code>qqline</code>. I want some help on:</p>

<ol>
<li>How to estimate ""probability"" that a data has normal distribution? (I read in a paper that a probability of 0.10 is required to assume that a data is normally distributed). </li>
<li>Also, how to calculate correlation coefficient for a normal probability plot in R?</li>
<li>Is the normality test valid for nonlinear regression too? (This might be a silly question, excuse me for that!)</li>
</ol>

<p>Thanks!</p>
"
"0.151285570837612","0.151418920859833","114728","<p>I'm trying to run a QAP logistic regression to predict the odds of a tie in a social network (represented as a binary adjacency matrix) given two independent variables (also binary matrices) but am getting opposite results depending on whether I run the analysis in R or UCINET.</p>

<p>All three matrices are rectangular (30 x 75). The 30 rows are people I've interviewed and the 75 columns are the entire population (including the 30 interviewees). All matrices include the person IDs as row and column names.</p>

<p>Running the analysis in R (see code at the bottom of the question), I get the following output:</p>

<pre><code>            Estimate  Exp(b)       Pr(&lt;=b) Pr(&gt;=b) Pr(&gt;=|b|)
(intercept) -5.298525  0.004998961 0.0001  0.9999  0.0001   
indep1       1.797138  6.032358591 0.9693  0.0307  0.2393   
indep2       3.194162 24.389724184 1.0000  0.0000  0.0030   
</code></pre>

<p>But after exporting the variables to .csv files and re-running it in UCINET, I get:</p>

<pre><code>                  1       2       3       4       5       6       7       8       9 
               Coef OddsRat     Sig      SD     Avg     Min     Max   P(ge)   P(le) 
            ------- ------- ------- ------- ------- ------- ------- ------- ------- 
1 Intercept  -3.931   0.020   0.000   0.540  -2.785  -3.931  -1.783       1   0.000 
2    indep1   2.391  10.925   0.003   1.508  -0.239  -5.089  15.437   0.003   0.998 
3    indep2   1.458   4.296   0.000   0.504  -0.026 -15.991   1.458   0.000       1 
</code></pre>

<p>Any ideas why this might be happening?</p>

<p>In case it's important, the (QAP) correlation coefficient between the two independent variables is 0.382</p>

<p>I've only included the 30 interviewees in the matrix rows because they are the only people from whom there might be a tie. The networks and QAP regressions are directed.</p>

<p>Incidentally, if I run the QAP logit using full 75x75 adjacency matrices (all people in the columns also appear as rows), I get the same output in both programs.</p>

<p>I also have a related question... A colleague suggested I could run the analysis using the 75x75 matrices but replace the rows of people I haven't interviewed with NAs. This gives me the same results in R and UCINET. Does this seem like a sensible approach, rather than using rectangular matrices?</p>

<p>Thanks!</p>

<p>Reproducible example in R:</p>

<pre><code>library(sna)

# row and column labels
rowIDs &lt;- c(""1"",  ""2"",  ""3"",  ""5"",  ""9"",  ""16"", ""18"", ""19"", ""26"", ""27"", ""34"", ""35"", ""36"", ""40"", ""46"", ""49"", ""60"", ""64"", ""70"", ""72"", ""73"", ""74"", ""75"", ""76"", ""77"", ""82"", ""85"", ""86"", ""97"", ""100"")
colIDs &lt;- c(""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", ""12"", ""13"", ""14"", ""15"", ""16"", ""17"", ""18"", ""19"", ""20"", ""21"", ""23"", ""26"", ""27"", ""34"", ""35"", ""36"", ""38"", ""40"", ""41"", ""43"", ""45"", ""46"", ""47"", ""49"", ""51"", ""52"", ""53"", ""57"", ""59"", ""60"", ""61"", ""62"", ""63"", ""64"", ""65"", ""66"", ""67"", ""68"", ""70"", ""72"", ""73"", ""74"", ""75"", ""76"", ""77"", ""78"", ""79"", ""81"", ""82"", ""83"", ""84"", ""85"", ""86"", ""87"", ""88"", ""89"", ""91"", ""92"", ""93"", ""94"", ""95"", ""96"", ""97"", ""100"", ""101"")

# create matrices
adj.dep &lt;- as.matrix( read.csv(header=F, text=
               ""0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0""))

adj.indep1 &lt;- as.matrix( read.csv(header=F, text=
               ""0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1""))

adj.indep2 &lt;- as.matrix( read.csv(header=F, text=
               ""0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1
                1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0
                1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0
                0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1""))

# assign row/column names
rownames(adj.dep) &lt;- rowIDs
colnames(adj.dep) &lt;- colIDs
rownames(adj.indep1) &lt;- rowIDs
colnames(adj.indep1) &lt;- colIDs
rownames(adj.indep2) &lt;- rowIDs
colnames(adj.indep2) &lt;- colIDs

# set up independent variables
g.indeps &lt;- array(dim=c(2, nrow(adj.indep1), ncol(adj.indep1)))
g.indeps[1,,] &lt;- adj.indep1
g.indeps[2,,] &lt;- adj.indep2

# run the analysis
# (warning, this command takes a bit of time to run with 10,000 reps)
nl &lt;- netlogit(adj.dep, g.indeps, reps=10000, nullhyp=""qap"")
# print output
summary(nl)
</code></pre>
"
"0.24103649422711","0.23393838043014","114895","<p>I am a user more familiar with R, and have been trying to estimate random slopes (selection coefficients) for about 35 individuals over 5 years for four habitat variables. The response variable is whether a location was ""used"" (1) or ""available"" (0) habitat (""use"" below).</p>

<p>I am using a Windows 64-bit computer.</p>

<p>In R version 3.1.0, I use the data and expression below. PS, TH, RS, and HW are fixed effects (standardized, measured distance to habitat types). lme4 V 1.1-7. </p>

<pre><code>str(dat)
'data.frame':   359756 obs. of  7 variables:
 $ use     : num  1 1 1 1 1 1 1 1 1 1 ...
 $ Year    : Factor w/ 5 levels ""1"",""2"",""3"",""4"",..: 4 4 4 4 4 4 4 4 3 4 ...
 $ ID      : num  306 306 306 306 306 306 306 306 162 306 ...
 $ PS: num  -0.32 -0.317 -0.317 -0.318 -0.317 ...
 $ TH: num  -0.211 -0.211 -0.211 -0.213 -0.22 ...
 $ RS: num  -0.337 -0.337 -0.337 -0.337 -0.337 ...
 $ HW: num  -0.0258 -0.19 -0.19 -0.19 -0.4561 ...

glmer(use ~  PS + TH + RS + HW +
     (1 + PS + TH + RS + HW |ID/Year),
     family = binomial, data = dat, control=glmerControl(optimizer=""bobyqa""))
</code></pre>

<p>glmer gives me parameter estimates for the fixed effects that make sense to me, and the random slopes (which I interpret as selection coefficients to each habitat type) also make sense when I investigate the data qualitatively. The log-likelihood for the model is -3050.8.</p>

<p>However, most research in animal ecology do not use R because with animal location data, spatial autocorrelation can make the standard errors prone to type I error. While R uses model-based standard errors, empirical (also Huber-white or sandwich) standard errors are preferred. </p>

<p>While R does not currently offer this option (to my knowledge - PLEASE, correct me if I am wrong), SAS does - although I do not have access to SAS, a colleague agreed to let me borrow his computer to determine if the standard errors change significantly when the empirical method is used.</p>

<p>First, we wished to ensure that when using model-based standard errors, SAS would produce similar estimates to R - to be certain that the model is specified the same way in both programs. I don't care if they are exactly the same - just similar.
I tried (SAS V 9.2):</p>

<pre><code>proc glimmix data=dat method=laplace;
   class year id;
   model use =  PS TH RS HW / dist=bin solution ddfm=betwithin;
   random intercept PS TH RS HW / subject = year(id) solution type=UN;
run;title;
</code></pre>

<p>I also tried various other forms, such as adding lines</p>

<pre><code>random intercept / subject = year(id) solution type=UN;
random intercept PS TH RS HW / subject = id solution type=UN;
</code></pre>

<p>I tried without specifying the </p>

<pre><code>solution type = UN,
</code></pre>

<p>or commenting out</p>

<pre><code>ddfm=betwithin;
</code></pre>

<p>No matter how we specify the model (and we have tried many ways), I cannot get the random slopes in SAS to remotely resemble those output from R - even though the fixed effects are similar enough. And when I mean different, I mean that not even the signs are the same. The -2 Log Likelihood in SAS was 71344.94. </p>

<p>I can't upload my full dataset; so I made a toy dataset with only the records from three individuals. SAS gives me output in a few minutes; in R it takes over an hour. Weird. With this toy dataset I'm now getting different estimates for the fixed effects. </p>

<p>My question:
Can anyone shed light on why the random slopes estimates might be so different between R and SAS? Is there anything I can do in R, or SAS, to modify my code so that the calls produce similar results? I'd rather change the code in SAS, since I ""believe"" my R estimates more. </p>

<p>I'm really concerned with these differences and want to get to the bottom of this problem!</p>

<p>My output from a toy dataset that uses only three of the 35 individuals in the full dataset for R and SAS are included as jpegs.</p>

<p><img src=""http://i.stack.imgur.com/ucNnh.jpg"" alt=""R output"">
<img src=""http://i.stack.imgur.com/jUC0K.jpg"" alt=""SAS output 1"">
<img src=""http://i.stack.imgur.com/IfCJm.jpg"" alt=""SAS output 2"">
<img src=""http://i.stack.imgur.com/7XJdA.jpg"" alt=""SAS output 3""></p>

<hr>

<p>EDIT AND UPDATE:</p>

<p>As @JakeWestfall helped discover, the slopes in SAS do not include the fixed effects. When I add the fixed effects, here is the result - comparing R slopes to SAS slopes for one fixed effect, ""PS"", between programs: (Selection coefficient = random slope). Note the increased variation in SAS. </p>

<p><img src=""http://i.stack.imgur.com/JozTd.jpg"" alt=""R vs SAS for PS""></p>
"
"0.0839181358296689","0.0839921051131616","117867","<p>I have a basic linear regression model I fitted to a time series. Unfortunately I have to account for autocorrelation and heteroskedasicity in the model and I have done so with the NeweyWest function from the sandwich package in R while analyzing the coefficients. </p>

<p>Now I would like to create prediction intervals using the predict() function (or any other function) while utilizing the NeweyWest matrix/SEs.</p>

<p>As this is the first quesiton I post on here and my experinece in R is very limited here is some information:</p>

<pre><code>LMModel = lm(Return~Sentiment, data=Time Series)
</code></pre>

<p><strong>This is the function I used for my coefficient testing:</strong></p>

<pre><code>coeftest(LMModel , vcov=NeweyWest(LMModel , lag=27, ar.method=""ols""))
</code></pre>

<p><strong>I would like thsi function to use NeweyWest in some way:</strong></p>

<pre><code>predict(LMModel, newdata, interval = ""prediction"", level = 0.95) 
</code></pre>

<p>Thanks a lot in advance!</p>
"
"0.0839181358296689","0.0839921051131616","117910","<p>According to Wikipedia (source of all truth and knowledge...),
<a href=""http://en.wikipedia.org/wiki/Generalized_least_squares#Properties"" rel=""nofollow"">http://en.wikipedia.org/wiki/Generalized_least_squares#Properties</a></p>

<p>a weighted least square regression is equivalent to a standard least square regression, if the variables have been previously ""decorrelated"" using a Cholesky decomposition.</p>

<p>I made up then a very simple example with the function pgls from the package CAPER to test it, where the correlation arises from a phylogeny tree:</p>

<pre><code>tree.mod:
((A:0.2,(B:0.1,C:0.1):0.1):0.1,((E:0.1,F:0.1):0.1,D:0.2):0.1);
</code></pre>

<p>The two approaches are compared here:</p>

<pre><code>library(caper)

## Data
species = c(""A"",""B"",""C"",""D"",""E"",""F"")
gene = c(0.1,0.2,0.3,0.5,0.6,0.7)
pheno = c( 0,0,0,1,1,1)
data=data.frame(species,gene,pheno)

## Phylogeny
tree = read.tree( ""small/tree_small.mod"" )

## GLS regression
cat(""\n     ===&gt; GLS\n"")
cdata   = comparative.data( phy = tree, data = data, names.col = ""species"" )
res = pgls( pheno~gene, cdata )
print(summary(res))

## Cholesky
cat(""\n     ===&gt; Cholesky\n"")
corr = vcv( tree )
cholesky = chol( corr )
invCho = solve( cholesky )
data.gene =  invCho %*% as.vector( data$gene )
data.pheno =  invCho %*% as.vector( data$pheno )
res=lm( data.pheno ~ data.gene )
print(summary(res))
</code></pre>

<p>and yield the outputs:</p>

<pre><code>====&gt; GLS
Coefficients:
        Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept) -0.13000    0.27261 -0.4769  0.65834  
gene         1.63333    0.59489  2.7456  0.05161 .


=====&gt;Cholesky
Coefficients:
        Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)  0.02214    0.28551   0.078    0.942  
data.gene    1.29188    0.35006   3.690    0.021 *
</code></pre>

<p>as you can see the results are different...</p>

<p>Does anyone have a clue why?</p>
"
"0.151285570837612","0.151418920859833","118394","<p>I have a dataset in which individuals were assessed at two time points during the study on a cognitive test, as such I was wondering which statistical model would be more appropriate for my data, either linear regression or mixed effects models? </p>

<p>The average length of follow up for my data is 59 months with a standard deviation of 43.03 (range is 0.63-167 months) with 88 (33%) of people having data for only one time point. </p>

<p>For linear regression, the approach I was thinking utilising was taking the delta of the test score between the two time points and regressing that against time (months between test scores). </p>

<p>If I used mixed effects models, the main issue I have is how to handle individuals who have only wave of data? While I know mixed effects models are especially robust in regards to the analysis of unbalanced data, would 33% missingnes cause issues?</p>

<p>Just sample R code highlighting the output using either linear regression or mixed models.</p>

<pre><code>fm1 &lt;- lm(mmse_difference ~ mmse_months_between*ORgrs_apoe, data = dat.wide)
summary(fm1)

Call:
lm(formula = mmse_difference ~ mmse_months_between * ORgrs_apoe, 
    data = newdat)

Residuals:
    Min      1Q  Median      3Q     Max 
-20.960  -3.957   1.854   5.200  12.550 

Coefficients:
                               Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)                    -2.74185    2.20667  -1.243    0.216
mmse_months_between            -0.01768    0.03051  -0.579    0.563
ORgrs_apoe                      0.35163    1.17782   0.299    0.766
mmse_months_between:ORgrs_apoe -0.01973    0.01748  -1.129    0.261

Residual standard error: 7.3 on 170 degrees of freedom
  (88 observations deleted due to missingness)
Multiple R-squared:  0.08481,   Adjusted R-squared:  0.06866 
F-statistic: 5.251 on 3 and 170 DF,  p-value: 0.001725
Num. obs. 174

fm2 &lt;- lme(mmse ~ mmse_months*ORgrs_apoe, random = ~mmse_months|patientid, data = dat.long, method = ""ML"", na.action = na.exclude)
summary(fm2)
Linear mixed-effects model fit by maximum likelihood
 Data: dat.long 
       AIC      BIC    logLik
  2797.467 2829.537 -1390.733

Random effects:
 Formula: ~mmse_months | patientid
 Structure: General positive-definite, Log-Cholesky parametrization
            StdDev    Corr  
(Intercept) 7.2972822 (Intr)
mmse_months 0.1132399 0.85  
Residual    2.9431616       

Fixed effects: mmse ~ mmse_months * ORgrs_apoe 
                           Value Std.Error  DF   t-value p-value
(Intercept)            24.635821 1.0959420 231 22.479130  0.0000
mmse_months            -0.069918 0.0223198 172 -3.132544  0.0020
ORgrs_apoe             -1.283348 0.6062892 231 -2.116726  0.0354
mmse_months:ORgrs_apoe -0.024952 0.0130561 172 -1.911103  0.0577
 Correlation: 
                       (Intr) mms_mn ORgrs_
mmse_months             0.438              
ORgrs_apoe             -0.882 -0.377       
mmse_months:ORgrs_apoe -0.357 -0.891  0.397

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-3.48643949 -0.31734164  0.07636708  0.26575764  2.49901891 

Number of Observations: 407
Number of Groups: 233 
</code></pre>

<p>Thanks.     </p>
"
"0.178017248729078","0.17817416127495","120421","<p>I am new to gam, and most of my knowledge comes from this document <a href=""http://www3.nd.edu/~mclark19/learn/GAMS.pdf"">http://www3.nd.edu/~mclark19/learn/GAMS.pdf</a>. Now I am using generalized addictive model with random effects to model some data, where I want to see how ""speedChange"" correlates with ""response"" in my dataset, with consideration of random effects ""user.id""</p>

<p>The code I run is shown as follows:</p>

<pre><code>speed.gammer &lt;- gamm4(response ~ s(speedChange) , data= t, random=~(1|user.id))
</code></pre>

<p>The gam can be plotted as follows:
<img src=""http://i.stack.imgur.com/xWkaU.jpg"" alt=""enter image description here""></p>

<p>I then try to interpret the gam:</p>

<pre><code>summary(speed.gammer$gam)
</code></pre>

<p>which gives the following :</p>

<pre><code>Family: gaussian 
Link function: identity 

Formula:
response ~ s(speedChange)

Parametric coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  2.30618    0.01482   155.6   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Approximate significance of smooth terms:
                 edf Ref.df     F p-value    
s(speedChange) 5.875  5.875 28.61  &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

R-sq.(adj) =  0.0263   
lmer.REML =  14688  Scale est. = 0.57643   n = 5619
</code></pre>

<p>From what I understand from the output, I learned that speedChange is significantly correlates with response, and the non-linear relationship is as shown in the plot. I know the R-squared is small, but that's not what I want to ask. I actually don't understand the mer model.</p>

<p>If I run:</p>

<pre><code>summary(speed.gammer$mer)
</code></pre>

<p>I got the following results:</p>

<pre><code>Linear mixed model fit by REML ['lmerMod']

REML criterion at convergence: 14687.7

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.5908 -0.6500 -0.0454  0.5880  3.7110 

Random effects:
 Groups   Name           Variance Std.Dev.
 user.id  (Intercept)     0.2853  0.5342  
 Xr       s(speedChange) 56.4011  7.5101  
 Residual                 0.5764  0.7592  
 Number of obs: 5619, groups:  user.id, 3042; Xr, 8

Fixed effects:
                    Estimate Std. Error t value
X(Intercept)        2.306181   0.014823  155.58
Xs(speedChange)Fx1 -0.008977   0.115045   -0.08

Correlation of Fixed Effects:
X(Int)
Xs(spdCh)F1 0.004 
</code></pre>

<p>I understand this is an lmerMod. I understand the output for lmer function, but not here. I don't understand what ""X"" means in the fixed effects. From the t-value it seems that the Intercept is significant but not the speedChange. I want to report the result of my analysis, but what is the relationship between the gam results and this mer result? How can I interpret the mer result of  </p>

<pre><code>Xs(speedChange)Fx1 -0.008977   0.115045   -0.08
</code></pre>

<p>together with the gam result:</p>

<pre><code>s(speedChange) 5.875  5.875 28.61  &lt;2e-16 ***
</code></pre>

<p>I don't see any documents that help me to understand the output in order to report the result. Could someone help?</p>
"
"0.0839181358296689","0.0839921051131616","120552","<p>I am trying to determine the correlation between an observed and theoretical set of data. Both sets of information are in the form of a table of deflection against time. Both samples have the same start and end time.</p>

<p>The problem i am encountering is with uneven samples sizes. for my theoretical result i have 300 results with a step size of 0.133ms and for my actual i have 400 results with a step size of 0.1ms. is there a simple way to get a coefficient bewteen these 2 sets without altering the sample size to a common value for both ?</p>
"
"0.0938233281301002","0.0939060283031685","121038","<p>Suppose I have 1000 draws each of two random variables X and Y.</p>

<p>If I wanted to sample the sum of these variables, I would simply calculate 1000 samples, i.e.</p>

<p>$$ S_{i}=X_{i}+Y_{i}, i=1,2,â€¦,1000 $$</p>

<p>And that would give me draws from the pdf of the sum of these two variables. </p>

<p>Now suppose I know that the correlation between these two variables is Ï=0.8</p>

<p>A higher value of X is now more likely to coincide with a higher value of Y, and the standard deviation of the sums will be higher.</p>

<p>Can I resample the sum of the two variables without adding a specific assumption regarding the specific destribution of X and Y?</p>

<p>For example.</p>

<p>Assume X and Y are population forecasts for male population in Washington and Oregon in 2050. I have two separate models for population in Oregon and Washington that give me pdf for population in Washington/Oregon in 2050. Because the models were run independently of each other, X+Y=S will underestimate the standard deviance of the sum, simply because of the fact that coefficients and/or the future development of independent variables in the model for Washington and Oregon are likely to be correlated.</p>

<p>Let us also assume that I only have those draws and I cannot account for the fact that X and Y are related by designing a smarter model. I can however observe the correlation of population numbers in Washington and Oregon.</p>
"
"0.118678165819385","0.103934927410387","122039","<p><strong>SOLVED</strong>: an elastic net model, as any other logistic regression model, will not generate more coefficients than input variables. Check Zach's answer to understand how from an (apparent) low number of inputs, more coefficients can be generated. The cause of this question was a code bug, as the users pointed out.</p>

<p>This is a simple question. I've fitted a model with 1334 variables using elastic net to perform feature selection and regularization. I'm now trying to interpret the obtained coefficients in order to find correlations between the input variables and the output. The only problem is that instead of the (expected) 1335 coefficients (intercept+1334), extracting the coefficients through <code>coef(model,s=""lambda.min"")</code> yields around 1390 coefficients. This seems highly counterintuitive and stops me from mapping a single coefficient to a single input variable, so I suppose I'm not understanding some of the insides of the elastic net. Any idea would be very helpful. Thanks in advance.</p>

<p>PS: just in case someone wonders so, I've not included interaction terms nor any synthetic variable, just the original 1334 ones.</p>

<p>PS2: elastic net references:</p>

<ul>
<li>Mathematical paper: <a href=""http://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&amp;%20Hastie.pdf"" rel=""nofollow"">http://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&amp;%20Hastie.pdf</a></li>
<li>R package tutorial: <a href=""http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html"" rel=""nofollow"">http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html</a></li>
</ul>

<p>PS3: about the code used to fit the model:</p>

<p>it is a 250 line script, so unless you specifically need it, I think it'd only clutter the question. Basically, the algorithm takes as an input a data frame of 1393 colums, where the last one is the target variable and the first 1392 are the input variables. So, after separating those into two matrices, input and output, the actual model fitting is done in this call:</p>

<p><code>cv.glmnet(x=input_matrix,y=output_matrix,family=""binomial"",type.measure=""auc"")</code></p>

<p>If you need to, I can actually generate a reproducible file with the data I use and the whole script. </p>
"
"0.205721255836448","0.19766648444296","123576","<p>I am trying to test the effect on the heat flux between indoors and outdoors before and after removing insulation.</p>

<p>Briefly, I have 26 sensors on a wall, measuring heat flow between indoors and outdoors over a number of days. The wall was part of a real world experimental setup so that the insulation on the wall was removed halfway through the experiment. Â What I care about is to have a measure of the effect of the removal of the insulation (I am not interested in any form of forecasting). Â I am exploring the use of a SARIMA/ARIMAX models with one regressor because, aside from the removal of the insulation, the heat flow between indoors and outdoors was affected by daily cyclical and random environmental effects (heating on or off, daily temperature changes, wind, etc).  Here I will present that data and analysis of one sensor.  My data has been collected hourly, and I have transformed the variable â€˜insulatedâ€™ â€˜not insulatedâ€™ as a factor of 0s and 1s as indicator.</p>

<pre><code>heat.flux = c(8.677048,6.558642,5.920314,5.583614,5.373176,5.253928,4.938272,7.358305,9.743266,10.46577,11.06201,10.90067,11.49691,13.15236,12.10017,10.60606,10.45875,10.03788,9.588945,9.287318,8.578844,8.024691,10.26936,11.8757,10.20623,8.634961,8.305275,8.101852,8.12991,7.947531,7.814254,10.40264,13.08221,14.3729,14.94809,15.08838,15.20763,15.75477,14.57632,12.79461,11.97391,10.97082,10.33249,9.701178,9.715208,9.083895,10.63412,12.07912,9.736251,7.638889,6.453423,5.983446,5.499439,5.099607,4.70679,6.972503,9.259259,9.981762,10.24832,10.17116,10.27637,10.27637,9.546857,7.568743,7.168911,6.867284,6.705948,6.916386,8.319304,8.424523,11.41274,13.52413,11.70034,9.532828,8.957632,9.07688,9.694164,9.301347,9.048822,12.28255,14.95511,15.22868,15.24972,15.12346,15.08838,15.17256,13.68547,12.18434,12.1633,12.13524,11.81257,11.58109,11.44781,11.27946,13.87486,15.92312,14.07828,11.90376,10.46577,9.518799,8.978676,8.803311,8.684063,11.65123,14.39394,15.69865,16.61756,16.828,16.83502,16.16863,14.23962,12.19837,12.09315,11.5881,11.20932,10.50786,10.59203,10.64815,13.51712,15.71268,13.92396,12.10718,12.2615,11.65123,11.05499,10.31846,9.834456,12.9349,15.41807,15.78283,15.8179,16.11953,15.95118,15.63552,13.1243,11.22334,10.21324,8.705107,7.526655,6.15881,5.30303,5.597643,8.599888,11.17424,9.631033,8.038721,7.638889,7.203984,7.161897,6.76908,6.888328,9.518799,12.40881,13.21549,14.28872,14.43603,14.8078,14.81481,13.60129,12.59119,11.86167,11.91779,11.73541,12.04405,11.51796,11.74242,13.7486,15.85999,14.84989,12.63328,10.68322,9.343434,8.592873,8.333333,8.445567,10.97783,13.82576,15.12346,16.58249,17.61364,18.30808,19.10774,17.97138,16.62458,15.867,16.07744,15.63552,16.0073,15.42508,15.01122,17.10157,18.94641,22.44669,18.94641,16.01431,14.55527,13.88889,12.77357,11.66526,12.46493,15.41807,16.75786,17.27694,17.03143,16.84905,16.828,16.02834,16.35802,16.04237,15.03928,14.00112,14.1344,13.86785,13.99411,15.30584,18.20286,19.49355,16.16162,14.05022,12.05107,12.27553,13.01207,12.5491,13.72054,16.91218,18.62374,18.79209,20.80527,19.50758,20.18799,20.63692,18.49747,17.25589,17.38215,18.40629,18.60269,19.12177,18.66582,21.09989,24.45286,26.71156,23.54798,20.01964,17.98541,14.83586,14.31678,15.15152,15.30584,17.95735,19.71801,20.30724,20.19501,20.2862,20.1459,20.10382,18.20988,16.54742,15.22868,13.96605,12.71044,11.61616,10.71829,12.12121,14.77273,14.04321,12.44388,10.94978,10.2413,9.708193,9.638047,9.322391,11.27245,14.24663,14.77273,14.75168,14.92705,15.47419,15.48822,14.73765,13.68547,12.65432,12.35269,12.34568,12.32464,12.7385,12.84371,14.16947,17.34007,17.09456,15.0954,13.40488,11.70735,10.8165,10.64815,12.01599,13.55219,16.7298,17.45932,17.61364,19.58474,20.02666,19.79517,19.38833,17.32604,16.11953,15.62851,15.01122,14.70258,14.5693,14.35887,16.28086,18.69388,18.92536,16.56846,15.97222,13.34877,12.81566,12.04405,13.23653,14.1835,16.75786,17.55752,17.98541,18.85522,18.8482,19.02357,18.96044,17.31201,15.42508,14.38692,13.57323,12.36672,12.03002,11.41274,13.15236,15.88103,14.66049,12.8858,11.67228,11.03395,9.399551,8.375421,8.073793,10.6271,13.57323,13.61532,14.31678,14.73765,15.08838,15.62149,16.6807,15.28479,14.07127,13.14534,12.61223,12.57015,12.02301,12.17031,14.33782,18.83418,20.45455,18.67985,18.40629,16.51235,14.45006,14.61841,15.20763,15.57941,18.06958,19.88636,20.51066,21.633,23.24635,24.28451,24.70539,24.19332,22.81145,21.97671,21.58389,21.3945,21.21212,20.89646,21.1069,23.86364)

insulation = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)
</code></pre>

<p>First off, the time series plot of the heat flux is this (the red line is when the insulation is removed):</p>

<p><img src=""http://i.stack.imgur.com/SYSQj.jpg"" alt=""Time series plot of heat flux""></p>

<p>Than this are the ACF and PACF plots of the same data:</p>

<p><img src=""http://i.stack.imgur.com/7keT7.jpg"" alt=""ACF and PACF of the data""></p>

<p>For my data, an <code>stl()</code> decomposition, run as <code>stl(ts(heat.flux, frequency = 24), 'period')</code></p>

<p>shows a strong â€˜seasonalâ€™ (i.e daily) component and a trend in the series. Â </p>

<p><img src=""http://i.stack.imgur.com/CUsta.jpg"" alt=""STL of the data""></p>

<p>Firs off I am trying to determine the best parameters for a SARIMA or ARIMAX model so that I can get an estimation of the effect removing the insulation. Despite the fact I can produce the ACF and PACF plots there is no way I can figure out the proper orders, so I load the library <code>forecast</code> and I run:</p>

<pre><code>library(forecast)
auto.arima(ts(heat.flux, frequency = 24), xreg = insulation, max.p = 10, max.q = 10, max.P = 10, max.Q = 10, stationary = F)Â 
</code></pre>

<p>The reason why I do not specify a stationary model is because of the trend I see with <code>stl()</code> and because I assume an effect of removing the insulation.</p>

<p>from <code>auto.arima()</code> I get:</p>

<pre><code>Series: ts(heat.flux, frequency = 24) 
ARIMA(2,0,2)(1,0,1)[24] with non-zero mean 

Coefficients:
         ar1      ar2      ma1      ma2    sar1     sma1  intercept  carp.hour$interv
      1.9414  -0.9495  -0.7423  -0.1793  0.9717  -0.6009    11.2449            4.3338
s.e.  0.0231   0.0221   0.0570   0.0544  0.0104   0.0544     2.1075            0.5548

sigma^2 estimated as 0.4484:  log likelihood=-411.28
AIC=840.55   AICc=841.03   BIC=876.11
</code></pre>

<p>If I try to use the <code>TSA</code> package and use <code>arimax()</code> with those orders I get basically the same stuff:</p>

<pre><code>library(TSA)
arimax(ts(heat.flux, frequency = 24), xreg = insulation, order = c(2,0,2), seasonal = list(order = c(1,0,1), frequency = 24))
Series: x 
ARIMA(2,0,2)(1,0,1)[24] with non-zero mean 

Coefficients:
         ar1      ar2      ma1      ma2    sar1     sma1  intercept    xreg
      1.9414  -0.9495  -0.7423  -0.1793  0.9717  -0.6009    11.2449  4.3338
s.e.  0.0231   0.0221   0.0570   0.0544  0.0104   0.0544     2.1075  0.5548

sigma^2 estimated as 0.4484:  log likelihood=-411.28
AIC=838.55   AICc=839.03   BIC=874.11
</code></pre>

<p>And all is apparently well (Irrespective of the function I choose I get an estimate of the effect of the removal of the insulation and a se with it with is what I want). Â Unfortunately, when I test the fit of this model with the function <code>sarima()</code> from the <code>astsa</code>package I get significant Ljung-Box p-values for all my sensors and for all the lags:</p>

<pre><code>library(astsa)
sarima(ts(heat.flux, frequency = 24), p = 2, d = 0, q = 2, P =1, D = 0, Q = 1, S = 24, xreg = insulation)
$fit

Call:
stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, 
Q), period = S), xreg = xreg, optim.control = list(trace = trc, REPORT = 1, 
reltol = tol))

Coefficients:
         ar1      ar2      ma1      ma2    sar1     sma1  intercept    xreg
      1.9414  -0.9495  -0.7423  -0.1793  0.9717  -0.6009    11.2449  4.3338
s.e.  0.0231   0.0221   0.0570   0.0544  0.0104   0.0544     2.1075  0.5548

sigma^2 estimated as 0.4484:  log likelihood = -411.28,  aic = 840.55
</code></pre>

<p>but the plot that comes with is shows that at every single lag the Ljung-Box statistics is significant:</p>

<p><img src=""http://i.stack.imgur.com/ZMGKN.jpg"" alt=""SARIMA""></p>

<p>What is going on?  To sum it up:</p>

<ol>
<li>which of these models is the most correct to estimate the effect of insulation?</li>
<li>why are the Ljung-Box p-values all significant?  I would have though that the ARIMA/ARIMAX/SARIMA would have sorted that issue</li>
<li>If the orders calculated by <code>auto.arima()</code> are the problem, how could I find them in a different way (which is computationally feasible and does not take days).</li>
</ol>

<p>Finally, two notes.  I also have collected variables such as internal and external temperatures, windspeed, etc, but I would have though that integrating these in the model would be superfluous given the fact it is already an ARIMA model to start with.  Second, I am not at all wedded to this kind of analysis, but I am aware that a straightforward linear model would not be acceptable given the autocorrelation between the data points.</p>
"
"0.182895336805923","0.173421993904824","125414","<p>I have two variables:</p>

<ul>
<li>urban areas</li>
<li>protected areas.</li>
</ul>

<p>My observations are urban areas and protected areas in each year. But these observations are the cumulative ones, so observations in each variable have auto-correlation.</p>

<p>Can I use the general correlation such as yielded by <code>cor()</code> in R to measure the correlation between these two variables? If not, which indicator or method can I use?</p>

<p>I have the scatter plot: the horizontal variable is urban area in a specific year, and the vertical variable is another one in that specific year. And these two variables are increasing as years pass. I can see these two variables present a linear relationship. And my purpose is to find a indicator which can measure this linear relationship. I actually have tested the linear regression: the urban area as independent variable, the protected area as dependent variable, and I put 14 pairs of each year into the regression model, and the coefficients can pass the t-test, and model can pass the t-test, the $R^2$ can reach more than 0.9. </p>

<p>I want to research the relationship between urban development and protected area development. And the scatter plot below is urban and protected area pairs on global scale for 1950-2014 with 5 year intervals (except for 2010 and 2014).</p>

<p>I want to test two questions: First, are these two areas (urban and protected areas) both increasing over the research period? Second, does urbanization (here I mean the development of urban area) cause the development of protected areas?</p>

<p>I want to use some correlation analysis to solve the first question, such as correlation, linear regression or MIC value. However, because my data are time series, I'm not sure it can be used in the calculation of correlation? So I raise this question. In addition, I don't know other methods that could be used to measure strength of linear relationship between two time series. </p>

<p>And for the second question, I want to use Granger causality test to test the causality relationship between these two areas statistically. I know the result of Granger causality can't be sure to determine the causality relationship. And in my opinion, the reasons to improve the development of urban areas or protected areas are both complex, and some of them may be shared. At this level, I simply want to test the causality relationship between these two variables.</p>

<p><img src=""http://i.stack.imgur.com/tHlOm.jpg"" alt=""scatter plot between urban and farm land, the point is a variable pair in a specific year""></p>
"
"0.151672986506104","0.151806678014216","125699","<p>I am trying to do an exploratory factor analysis (EFA) in R with oblique (promax) rotation. </p>

<p>From <a href=""http://en.wikipedia.org/wiki/Factor_analysis#Terminology"" rel=""nofollow"">Wikipedia</a>, </p>

<blockquote>
  <p><i>In oblique rotation, one gets both a pattern matrix and a structure
  matrix. The structure matrix is simply the factor loading matrix as in
  orthogonal rotation, representing the variance in a measured variable
  explained by a factor on both a unique and common contributions basis.
  The pattern matrix, in contrast, contains coefficients which just
  represent unique contributions. The more factors, the lower the
  pattern coefficients as a rule since there will be more common
  contributions to variance explained. For oblique rotation, the
  researcher looks at both the structure and pattern coefficients when
  attributing a label to a factor. Principles of oblique rotation can be
  derived from both cross entropy and its dual entropy.</i></p>
</blockquote>

<p>In R,  <code>factanal()</code> function gives me one loading matrix, while SPSS gives me two matrices namely, (i) pattern matrix and (ii) structure matrix. But interestingly, the entries of the loading matrix in R differ from the pattern matrix and the structure matrix in SPSS. </p>

<p>So, here are my questions:</p>

<ol>
<li><p>What is the loading matrix given by <code>factanal()</code>? Is it the pattern matrix or the structure matrix or the factor loading matrix (the $l_{ik}$'s  in $x_i - \mu_i = l_{i1}F_1 + ... + l_{ik}F_k + e_i$, where $x_i$, $F_k$'s and $e_i$ are the observed measurement, latent factors and the error respectively)?</p></li>
<li><p>If the loading matrix given by <code>factanal()</code> is either the pattern or structure matrix, why are the entries are so different from that given by SPSS?</p></li>
<li><p>If  the loading matrix given by <code>factanal()</code> is not the pattern nor structure matrix, how can I get/calculate them in R?</p></li>
</ol>

<p>Thanks.</p>

<hr>

<p>Below are the outputs in R and SPSS of the same data set:</p>

<p>R</p>

<p><code>factanal</code>: </p>

<pre><code>Call:
factanal(x = X, factors = 2, rotation = ""promax"")

Uniquenesses:
   B2    B3    B5    B6    B7    B9   B10 
0.326 0.559 0.487 0.800 0.715 0.733 0.631 

Loadings:
    Factor1 Factor2
B2   0.835         
B3   0.425   0.375 
B5   0.754  -0.126 
B6   0.137   0.377 
B7           0.521 
B9           0.539 
B10 -0.197   0.653 

               Factor1 Factor2
SS loadings      1.510   1.289
Proportion Var   0.216   0.184
Cumulative Var   0.216   0.400

Factor Correlations:
        Factor1 Factor2
Factor1   1.000  -0.375
Factor2  -0.375   1.000

Test of the hypothesis that 2 factors are sufficient.
The chi square statistic is 9.34 on 8 degrees of freedom.
The p-value is 0.314 
</code></pre>

<p><code>factanal</code> &amp; <code>promax</code></p>

<pre><code>&gt; promax(factanal(X, factors=2, rotation=""none"")$loadings, m=4) #m: The power used the target for promax
    $loadings

Loadings:
    Factor1 Factor2
B2   0.835         
B3   0.425   0.375 
B5   0.754  -0.126 
B6   0.137   0.377 
B7           0.521 
B9           0.539 
B10 -0.197   0.653 

               Factor1 Factor2
SS loadings      1.510   1.289
Proportion Var   0.216   0.184
Cumulative Var   0.216   0.400

$rotmat
           [,1]      [,2]
[1,]  0.8926099 0.2266316
[2,] -0.6060523 1.0548412
</code></pre>

<p><code>fa</code> in <code>psych</code>:</p>

<pre><code>&gt; fa(cov(X), nfactors=2, rotate=""promax"", fm=""ml"")$loading

Loadings:
    ML1    ML2   
B2   0.835       
B3   0.425  0.375
B5   0.754 -0.126
B6   0.137  0.377
B7          0.521
B9          0.539
B10 -0.197  0.653

                 ML1   ML2
SS loadings    1.510 1.289
Proportion Var 0.216 0.184
Cumulative Var 0.216 0.400
</code></pre>

<hr>

<p>SPSS</p>

<p>syntax:</p>

<pre><code>FACTOR
  /VARIABLES B2 B3 B5 B6 B7 B9 B10 
  /MISSING LISTWISE 
  /ANALYSIS B2 B3 B5 B6 B7 B9 B10 
  /PRINT INITIAL CORRELATION EXTRACTION ROTATION FSCORE
  /CRITERIA FACTORS(2) ITERATE(50)
  /EXTRACTION ML
  /CRITERIA ITERATE(50)
  /ROTATION PROMAX(4).
</code></pre>

<p>output:</p>

<pre><code>Pattern Matrixa     
    Factor  
    1   2
B2  .830    -.029
B3  .437    .376
B5  .746    -.114
B6  .150    .374
B7  .049    .515
B9  -.052   .531
B10 -.173   .643
Extraction Method: Maximum Likelihood. 
 Rotation Method: Promax with Kaiser Normalization.     
a Rotation converged in 3 iterations.       

Structure Matrix        
    Factor  
    1   2
B2  .820    .246
B3  .561    .521
B5  .708    .133
B6  .274    .424
B7  .220    .531
B9  .125    .514
B10 .041    .585
Extraction Method: Maximum Likelihood. 
 Rotation Method: Promax with Kaiser Normalization.     
</code></pre>

<hr>

<p>Data can be downloaded <a href=""https://www.dropbox.com/s/3yx97dv4svb71o3/X.csv?dl=0"" rel=""nofollow"">here</a>.</p>
"
"0.244661306563947","0.237674698635794","127479","<p>I'm using a mixed effects model with logistic link function (using lme4 version 1.1-7 in R).  However, I noticed that the estimates of significance for fixed effects change depending on the order of the rows in the dataset.  </p>

<p>That is, if I run a model on a dataset, I get certain estimate for my fixed effect and it has a certain p-value.  I run the model again, and I get the same estimate and p-value.  Now, I shuffle the order of rows (the data is not mixed, just the rows are in a different order).  Running the model a third time, the p-value is very different.</p>

<p>For the data I have, the estimated p-value for the fixed effect can be between p=0.001 and p=0.08.  Obviously, these are crucial differences given conventional significance levels. </p>

<p>I understand that the estimates are just estimated, and there will be differences between values for a number of reasons.  However, the magnitude of the differences for my data seem large to me, and I wouldn't expect the order of my dataframe to have this effect (we discovered this problem by chance when a colleague ran the same model but got different results.  It turned out they had ordered their data frame.).  </p>

<p>Here is the output of my script:
(X and Y are binary variables which are contrast-coded and centred, Group and SubGroup are categorical variables)</p>

<pre><code>&gt; # Fit model
&gt; m1 = glmer(X ~Y+(1+Y|Group)+(1+Y|SubGroup),family=binomial(link='logit'),data=d)
&gt; # Shuffle order of rows
&gt; d = d[sample(1:nrow(d)),]
&gt; # Fit model again
&gt; m2 = glmer(X ~Y+(1+Y|Group)+(1+Y|SubGroup),family=binomial(link='logit'),data=d)
&gt; summary(m1)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
      ['glmerMod']
 Family: binomial  ( logit )
Formula: X ~ Y + (1 + Y | Group) + (1 + Y | SubGroup)
   Data: d

      AIC       BIC    logLik  deviance  df.resid 
 200692.0  200773.2 -100338.0  200676.0    189910 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.1368 -0.5852 -0.4873 -0.1599  6.2540 

Random effects:
 Groups       Name        Variance Std.Dev. Corr 
 SubGroup     (Intercept) 0.2939   0.5421        
              Y1          0.1847   0.4298   -0.79
 Group        (Intercept) 0.2829   0.5319        
              Y1          0.4640   0.6812   -0.07
Number of obs: 189918, groups:  SubGroup, 15; Group, 12

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.0886     0.1325  -8.214   &lt;2e-16 ***
Y1            0.3772     0.2123   1.777   0.0756 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
     (Intr)
Y1 0.112 
&gt;
&gt; # -----------------
&gt; summary(m2)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
      ['glmerMod']
 Family: binomial  ( logit )
Formula: X ~ Y + (1 + Y | Group) + (1 + Y | SubGroup)
   Data: d

      AIC       BIC    logLik  deviance  df.resid 
 200692.0  200773.2 -100338.0  200676.0    189910 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.1368 -0.5852 -0.4873 -0.1599  6.2540 

Random effects:
 Groups       Name        Variance Std.Dev. Corr 
 SubGroup     (Intercept) 0.2939   0.5422        
              Y1          0.1846   0.4296   -0.79
 Group        (Intercept) 0.2829   0.5318        
              Y1          0.4641   0.6813   -0.07
Number of obs: 189918, groups:  SubGroup, 15; Group, 12

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.0886     0.1166  -9.334  &lt; 2e-16 ***
Y1            0.3773     0.1130   3.339 0.000841 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
     (Intr)
Y1 0.074 
</code></pre>

<p>I'm afraid that I can't attach the data due to privacy reasons. </p>

<p>Both models converge.  The difference appears to be in the standard errors, while the differences in coefficient estimates are smaller.  The model fit (AIC etc.) are the same, so maybe there are multiple optimal convergences, and the order of the data pushes the optimiser into different ones.  However, I get slightly different estimates every time I shuffle the data frame (not just two or three unique estimates).  In one case (not shown above), the model did not converge simply because of a shuffling of the rows.</p>

<p>I suspect that the problem lies with the structure of my particular data.  It's reasonably large (nearly 200,000 cases), and has nested random effects.  I have tried centering the data, using contrast coding and feeding starting values to lmer based on a previous fit.  This seems to help somewhat, but I still get reasonably large differences in p-values.  I also tried using different ways of calculating p-values, but I got the same problem.</p>

<p>Below, I've tried to replicate this problem with synthesised data.  The differences here aren't as big as with my real data, but it gives an idea of the problem.</p>

<pre><code>library(lme4)
set.seed(999)

# make a somewhat complex data frame
x = c(rnorm(10000),rnorm(10000,0.1))
x = sample(x)
y = jitter(x,amount=10)
a = rep(1:20,length.out=length(x))
y[a==1] = jitter(y[a==1],amount=3)
y[a==2] = jitter(x[a==2],amount=1)
y[a&gt;3 &amp; a&lt;6] = rnorm(sum(a&gt;3 &amp; a&lt;6))
# convert to binary variables
y = y &gt;0
x = x &gt;0
# make a data frame
d = data.frame(x1=x,y1=y,a1=a)

# run model 
m1 = glmer(x1~y1+(1+y1|a1),data=d,family=binomial(link='logit'))

# shuffle order of rows
d = d[sample(nrow(d)),]

# run model again
m2 = glmer(x1~y1+(1+y1|a1),data=d,family=binomial(link='logit'))

# show output
summary(m1)
summary(m2)
</code></pre>

<p>One solution to this is to run the model multiple times with different row orders, and report the range of p-values.  However, this seems inelegant and potentially quite confusing.</p>

<p>The problem does not affect model comparison estimates (using anova), since these are based on differences in model fit.  The fixed effect coefficient estimates are also reasonably robust.  Therefore, I could just report the effect size, confidence intervals and the p-value from a model comparison with a null model, rather than the p-values from within the main model.</p>

<p>Anyway, has anyone else had this problem?  Any advice on how to proceed?</p>
"
"0.16250677125654","0.162650012158089","127502","<p>I've performed PCA on face images dataset and I'm not sure how can I use the most informative principal components to show the ""reduced"" image.</p>

<p>The original image is 96*96 pixels (96*96 = 9216) and I use a sample of 70 images here (70 rows and 9216 column). We get 70 principal components (min{num of samples, num of features}=70).  </p>

<p>How can I re-construct a 96x96 image in order to show the eigenfaces? I want to show my students how the eigenvectors ""predict"" the real data.</p>

<p>The dataset I'm using <a href=""https://kaggle2.blob.core.windows.net/competitions-data/kaggle/3486/training.zip?sv=2012-02-12&amp;se=2014-12-13T06%3A39%3A32Z&amp;sr=b&amp;sp=r&amp;sig=sgyAAFGa1XfYW51UX%2BFDdNMMrWDJe8KkqPOT1QvyXE8%3D"" rel=""nofollow"">can be downloaded here</a>.</p>

<p>The code:</p>

<pre><code>install.packages(""foreach"")

file ='C:\\I\\Love\\Data Science\\face.training.csv'
data_all = read.csv(file , stringsAsFactors=F)
dim(data_all) #7049   31

# use only 70 first images
data = data_all[1:70,]
names(data)
str(data)

# extract the images data
im.train &lt;- data$Image
    data$Image = NULL

# each image is a vector of 96*96 pixels (96*96 = 9216).
library(foreach)
im.train &lt;- foreach(im = im.train, .combine=rbind) %dopar% {
  as.integer(unlist(strsplit(im, "" "")))
}

# im.train is a matrix of pixels 70x9216

# show picture number 2
im &lt;- matrix(data=rev(im.train[2,]), nrow=96, ncol=96)
image(1:96, 1:96, im, col=gray((0:255)/255))

# Apply PCA
pca &lt;- prcomp(im.train,
                 center = TRUE,
                 scale. = TRUE) ## using correlation matrix

# There are in general min(n âˆ’ 1, p) informative principal components in a data set with n observations and p variables. Hence, pca$x is 70x70

# Standard deviation of each component
pca$sdev

# A numeric matrix which provides the data for the principal components analysis
pca$x
    dim(pca$x)


# The print method returns the standard deviation of each of the PCs, 
# and their rotation (or loadings), which are the coefficients of the linear combinations of the continuous variables.
print(pca)

#The summary method describe the importance of the PCs.
summary(pca)
#The first row describe again the standard deviation associated with each PC. 
#The second row shows the proportion of the variance in the data explained by each component 
#while the third row describe the cumulative proportion of explained variance. 


# plot method returns a plot of the variances (y-axis) associated with the PCs (x-axis). 
# useful to decide how many PCs to retain for further analysis. 
plot(pca, type = ""l"")
</code></pre>
"
"NaN","NaN","130763","<p>I have a series of actual price and predicted price. Values of which are given below. Now when I try to find the correlation coefficient using the R function                cor(actual price,predicted price) I get 0.6769881. But when I try to find the mean square error the value comes 3.634382e-05, which I believe is a bit low for the corresponding correlation coefficient. Is there something I am doing wrong </p>

<pre><code>     **predicted Price**                            **Actual Price** 
        4.760972                                      4.780 
        4.767296                                      4.779
        4.776825                                      4.775
        4.776761                                      4.776
        4.775952                                      4.779
        4.776835                                      4.777
        4.772703                                      4.776
        4.774033                                      4.778
        4.774343                                      4.779
        4.778052                                      4.783
        4.779570                                      4.779
        4.774922                                      4.781
        4.777020                                      4.779
        4.780514                                      4.786
        4.784221                                      4.784
        4.780591                                      4.780 
        4.782547                                      4.784
        4.783528                                      4.785
        4.784811                                      4.783
        4.787684                                      4.787
        4.790639                                      4.787
</code></pre>
"
"0.0726752374667264","0.0727392967453308","131386","<p>I run a community website with ~1600 users who made 750Â 000 votes on 100Â 000 posts.  The votes are made on the <a href=""http://en.wikipedia.org/wiki/Likert_scale"" rel=""nofollow"">Likert scale</a>, i.e. from 1 to 5. I want to help users find like-minded people.</p>

<p>After some googling, I found <a href=""http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient"" rel=""nofollow"">Pearson product-moment correlation coefficient</a>, which is apparently very easy to calculate in R.</p>

<p>For each pair of users, I selected votes that they made on same posts, obtaining as a result a bunch of tables like that:</p>

<pre><code>user1 user2
    1     1
    5     5
    5     5
    5     1
    5     1
    1     1
</code></pre>

<p>Now, I can read each table as</p>

<pre><code>mydata = read.table(""tablename"")
</code></pre>

<p>and run </p>

<pre><code>cor(mydata[[1]],mydata[[2]])
cor.test(mydata[[1]],mydata[[2]])$p.value
</code></pre>

<p>to get the correlation <em>r</em> and significance <em>p</em>.</p>

<p>Then, I am stuck with two questions:</p>

<ol>
<li>How to properly order a list of users given <em>r</em> and <em>p</em>? Should I choose a cutoff value of <em>p</em> arbitrarily, then order by descending <em>r</em>?</li>
<li>Did I choose the  best algorithm? What are the alternatives to Pearson's <em>r</em> in this case?</li>
</ol>
"
"0.0839181358296689","0.0839921051131616","133488","<p>I am trying to run regression on financial data in R. I am new to regression analysis so I am finding it to difficult to interpret certain scenarios. I have the code as follows:</p>

<pre><code>#regression analysis
fit &lt;- lm(fiveMinReturns~RegressionData, data=maindata)
summary(fit) # show results
#correlation
cor(maindata$fiveMinReturns,maindata$RegressionData,use=""everything"")
</code></pre>

<p>My output is: </p>

<pre><code>Call:
lm(formula = fiveMinReturns ~ RegressionData, data = maindata)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.205790 -0.001144 -0.000062  0.001117  0.156418 

Coefficients:
                Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)    6.346e-05  8.785e-06   7.223 5.09e-13 ***
RegressionData 1.597e-07  1.432e-08  11.155  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.004035 on 210912 degrees of freedom
Multiple R-squared:  0.0005896, Adjusted R-squared:  0.0005849 
F-statistic: 124.4 on 1 and 210912 DF,  p-value: &lt; 2.2e-16

cor(maindata$fiveMinReturns,maindata$RegressionData,use=""everything"")
[1] 0.02428219
</code></pre>

<p>p-value is very small that means two variables are tightly coupled, but correlation is small too.
My question is how do I evaluate this situation?
Can we say that this equation will give correct results almost every time?
Which scenario suggests both p-value and correlation both to be really small?
What measures should i take to improve the result? </p>
"
"0.145350474933453","0.145478593490662","134499","<h2>Can anyone explain How I can interpret my result from the below model :</h2>

<p>I am trying to build the linear regression model for finding the transaction behavior of the customer in bank accounts. I have created the below model and done the nvcTest for the same. I need some one help to summary the result of this model.</p>

<pre><code>&gt; str(trans_data)
'data.frame':   8597 obs. of  20 variables:
 $ cust_id          : int  1 1 1 1 1 1 1 1 1 1 ...
 $ acc_type_id      : int  3 3 3 3 3 3 3 3 3 3 ...
 $ loc_id           : int  260 144 362 321 114 331 343 17 345 284 ...
 $ tx_type          : Factor w/ 2 levels ""CR"",""DB"": 1 1 1 2 1 1 1 2 2 2 ...
 $ total_bal        : num  8.36e+08 8.70e+08 9.69e+08 8.93e+08 9.60e+08 ...
 $ age              : int  30 30 30 30 30 30 30 30 30 30 ...
 $ gender           : Factor w/ 2 levels ""F"",""M"": 2 2 2 2 2 2 2 2 2 2 ...
 $ city             : chr  ""New Orleans"" ""New Orleans"" ""New Orleans"" ""New Orleans"" ...
 $ county           : chr  ""Orleans"" ""Orleans"" ""Orleans"" ""Orleans"" ...
 $ state            : chr  ""LA"" ""LA"" ""LA"" ""LA"" ...
 $ category         : Factor w/ 6 levels ""HNI"",""LNI"",""MNI"",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ employment_status: Factor w/ 3 levels ""SAL"",""SEL"",""UNE"": 1 1 1 1 1 1 1 1 1 1 ...
 $ marital_status   : Factor w/ 2 levels ""MARRIED"",""UNMARRIED"": 1 1 1 1 1 1 1 1 1 1 ...
 $ branch_num       : int  1 1 1 1 1 1 1 1 1 1 ...
 $ acc_type         : Factor w/ 3 levels ""current"",""savings"",..: 3 3 3 3 3 3 3 3 3 3 ...
 $ acc_sub_type     : Factor w/ 6 levels ""eqty"",""fd"",""gold"",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ yyyy             : int  2013 2013 2013 2013 2013 2013 2013 2013 2013 2013 ...
 $ mm               : int  1 1 1 1 1 1 1 1 1 1 ...
 $ dd               : int  3 4 8 9 11 12 13 17 21 22 ...
 $ tx_amt           : num  78626273 33171055 98937915 75726140 67162109 ...


model1&lt;-lm(formula = tx_amt ~ tx_type + total_bal +   age + gender + category + employment_status + marital_status + acc_type + 
acc_sub_type + yyyy + mm + dd , data=trans_data)

&gt; summary(model1)

Call:
lm(formula = tx_amt ~ tx_type + total_bal + age + gender + category + 
    employment_status + marital_status + acc_type + acc_sub_type + 
    yyyy + mm + dd, data = trans_data)

Residuals:
      Min        1Q    Median        3Q       Max 
-54030777 -24870155    230953  24933123  52907029 

Coefficients: (2 not defined because of singularities)
                          Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)              1.901e+09  1.161e+09   1.638   0.1015  
tx_typeDB               -6.713e+05  6.245e+05  -1.075   0.2824  
total_bal                1.118e-05  3.877e-04   0.029   0.9770  
age                     -7.315e+03  8.957e+04  -0.082   0.9349  
genderM                  2.784e+05  3.258e+06   0.085   0.9319  
categoryLNI             -1.648e+04  3.142e+06  -0.005   0.9958  
categoryMNI             -3.674e+05  2.095e+06  -0.175   0.8608  
categoryNNI              8.522e+05  5.419e+06   0.157   0.8751  
categoryXXX              2.184e+06  2.425e+06   0.901   0.3677  
categoryYYY             -1.949e+06  1.469e+06  -1.327   0.1847  
employment_statusSEL    -1.219e+06  2.755e+06  -0.443   0.6581  
employment_statusUNE     1.606e+06  1.551e+06   1.035   0.3006  
marital_statusUNMARRIED         NA         NA      NA       NA  
acc_typesavings         -1.882e+06  1.192e+06  -1.580   0.1143  
acc_typesecurity         1.909e+05  5.429e+06   0.035   0.9719  
acc_sub_typefd          -1.019e+07  1.288e+07  -0.791   0.4292  
acc_sub_typegold         9.310e+05  1.522e+06   0.612   0.5406  
acc_sub_typemutual_fnd  -1.245e+06  1.041e+06  -1.196   0.2317  
acc_sub_typenormal       1.446e+05  5.311e+06   0.027   0.9783  
acc_sub_typerd                  NA         NA      NA       NA  
yyyy                    -9.185e+05  5.763e+05  -1.594   0.1110  
mm                          -1.904e+05  8.972e+04  -2.123   0.0338 *
dd                       2.334e+04  3.547e+04   0.658   0.5105  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 28850000 on 8576 degrees of freedom
Multiple R-squared:  0.003322,  Adjusted R-squared:  0.0009977 
F-statistic: 1.429 on 20 and 8576 DF,  p-value: 0.09664

library(car)

durbinWatsonTest(model1)
 lag Autocorrelation D-W Statistic p-value
   1   -0.0008975802      2.001674   0.916
 Alternative hypothesis: rho != 0

ncvTest(model1)
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.132947    Df = 1     p = 0.7153958

model4&lt;-lm(formula = tx_amt ~ cust_id + acc_type_id + loc_id + tx_type + total_bal +   age + gender + category + employment_status + marital_status + acc_type + 
acc_sub_type + yyyy + mm + dd , data=trans_data)

 summary(model4)

Call:
lm(formula = tx_amt ~ cust_id + acc_type_id + loc_id + tx_type + 
    total_bal + age + gender + category + employment_status + 
    marital_status + acc_type + acc_sub_type + yyyy + mm + dd, 
    data = trans_data)

Residuals:
      Min        1Q    Median        3Q       Max 
-54109589 -24870576    214972  24958230  52899612 

Coefficients: (4 not defined because of singularities)
                          Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)              1.891e+09  1.161e+09   1.629   0.1033  
cust_id                  1.915e+05  1.822e+05   1.051   0.2933  
acc_type_id             -4.634e+04  2.656e+06  -0.017   0.9861  
loc_id                  -1.941e+03  2.173e+03  -0.893   0.3718  
tx_typeDB               -6.715e+05  6.245e+05  -1.075   0.2824  
total_bal                1.602e-05  3.877e-04   0.041   0.9670  
age                      9.159e+04  9.163e+04   1.000   0.3175  
genderM                 -4.463e+06  4.155e+06  -1.074   0.2828  
categoryLNI             -2.748e+06  3.170e+06  -0.867   0.3860  
categoryMNI             -2.091e+06  2.305e+06  -0.907   0.3644  
categoryNNI             -6.203e+06  6.973e+06  -0.890   0.3737  
categoryXXX             -1.115e+06  2.649e+06  -0.421   0.6738  
categoryYYY             -1.290e+06  1.242e+06  -1.039   0.2989  
employment_statusSEL    -4.898e+06  2.978e+06  -1.645   0.1001  
employment_statusUNE            NA         NA      NA       NA  
marital_statusUNMARRIED         NA         NA      NA       NA  
acc_typesavings         -1.708e+06  1.084e+07  -0.158   0.8747  
acc_typesecurity         1.317e+05  5.429e+06   0.024   0.9807  
acc_sub_typefd          -1.049e+07  1.209e+07  -0.868   0.3856  
acc_sub_typegold         9.841e+05  3.047e+06   0.323   0.7467  
acc_sub_typemutual_fnd  -1.282e+06  2.874e+06  -0.446   0.6557  
acc_sub_typenormal              NA         NA      NA       NA  
acc_sub_typerd                  NA         NA      NA       NA  
yyyy                    -9.124e+05  5.764e+05  -1.583   0.1134  
mm                      -1.905e+05  8.972e+04  -2.124   0.0337 *
dd                       2.302e+04  3.548e+04   0.649   0.5164  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 28850000 on 8575 degrees of freedom
Multiple R-squared:  0.003415,  Adjusted R-squared:  0.0009741 
F-statistic: 1.399 on 21 and 8575 DF,  p-value: 0.1055

ncvTest(model4)
Non-constant Variance Score Test 
Variance formula: ~ fitted.values 
Chisquare = 0.08904766    Df = 1     p = 0.7653914
</code></pre>
"
"0.16250677125654","0.162650012158089","134523","<p>I present using dummy data for an example, see below for R code to replicate data. </p>

<p>Imagine that I have collected data on how much mail 500 people receive. In my survey there are 250 men and 250 women, so $n$ = 500. I asked each respondent to categorise &amp; count the mail they received, over the course of one week, in to one of three classes; junk, bills, and personal. </p>

<p>What I want to test is the amount of variation in mail received as explained by each of the three classes. Assuming that all mail received falls in to one of the three classes, I would think that I could generate a variance-covariance matrix (using MCMCglmm in R) and then derive correlations. The correlations would then act as estimates of how much variance is explained by each class of mail.</p>

<p>$\rho_{ij} = \frac{cov_{total, class_i}}{\sqrt{\sigma^2_{total} * \sigma^2_{class_i}}}$</p>

<p>$Percentage$ $variance$ $explained = 100 * \rho_{ij}$</p>

<p>Where $\rho_{ij}$ is the correlation between total received and the category of interest. For example, I might show that the correlation between total mail received and junk mail received, as defined above, is 0.7, thus conclude that the amount of junk mail one receives explains 70% of the variation in mail received. Further, I might sum together junk and bills in to a new class called ""businesses"" which might explain 85% of the variance is the correlation, as defined above, is 0.85.</p>

<p><strong>Would this be an appropriate approach or is it a terribly flawed idea?</strong> I've read that using correlation coefficients as a way to measure ""explained variation"" can be controversial, but I think in this case it is appropriate because the value total can only be determined by the 3 constituents, thus there are no more variables that can explain variation in the total.</p>

<hr>

<pre><code># Clear workspace
rm(list = ls())
set.seed(1123)

# Number of people surveyed
m = 250 # males
f = 250 # females
n = m + f

# Distributions
junk = rpois(n, 3)
personal = rpois(n, 2)
bills = rpois(n, 3)

# Dataframe
mail = data.frame(as.factor(1:n), as.factor(rep(c(""m"",""f""), each = m)), 
            sample(junk, replace=TRUE, size = n), sample(bills, replace=TRUE, size = n), sample(personal, replace=TRUE, size = n))
colnames(mail) = c(""person"", ""sex"", ""junk"", ""bills"", ""personal"")
mail$total = apply(mail[,3:5],1,sum)
    mail$junk = as.factor(mail$junk); mail$bills = as.factor(mail$bills); mail$personal = as.factor(mail$personal); mail$total = as.factor(mail$total)
</code></pre>
"
"0.0419590679148345","0.0419960525565808","136071","<p>I am hoping someone can check this code to ensure that I have interpreted the various pieces of PCA correctly. I am trying to figure out a way to identify the leading contributors to the performance of multiple securities. For example, one idea I had was to run a multivariate regression using the securities returns as dependent variables and include things like oil, the dollar, the euro, treasury yields, etc.
E.g.,
SBUX + AAPL + MCD + BAC + TWTR = intercept + oil + dollar + euro + steel + gold + e</p>

<p>I then thought that PCA would probably be better suited for this type of exercise. Here is my code from R. The csv file consists of a matrix of 900 securities and 30 rows (30 daily returns for 900 securities)</p>

<pre><code>FD &lt;- read.csv(""U:/Personal Projects/R/Data Files/FD Securities Jan 2015.csv"")

#Removes columns with any na values
FD1 &lt;- FD[, sapply(FD, function(x) !any(is.na(x)))]

#removes ""zero/constant variance"" columns, which I think are NaNs I couldnt erase using is.nan
FD2 &lt;- FD1[,apply(FD1, 2, var, na.rm=TRUE) != 0]

#Calc PCs using singe value decomposition (prcomp). Should data be a correlation matrix of the variables? I get reasonable looking results both ways, i.e., PC1 explains 30-50% of variance, PC2 ~10%-15%, etc.
FD2.pca &lt;- prcomp(cor(FD2), retx = TRUE, scale = TRUE)
summary(FD2.pca)
plot(FD2.pca)

#These are the 'loadings', i.e., coefficients used for each linear combination?
as.matrix(FD2.pca$rotation[,1])

#I think these ""scores"" are the coefficients of interest, as they incorporate the factor     weightings because the output is pca$rotation * scale (stddev of each factor)
as.matrix(FD2.pca$x[,1]) as.matrix(FD2.pca$x[,2]) as.matrix(FD2.pca$x[,3])     as.matrix(FD2.pca$x[,4])

#Scatterplot of the first two principal components. Not sure if this is right of if $x should be used.
plot(x = FD2.pca$rotation[,1], FD2.pca$rotation[,2], xlab = ""PC1"", ylab = ""PC2"", main=""Principal Component Analysis:"")
</code></pre>
"
"0.111013258946721","0.111111111111111","136925","<p>I have some short grouped time series data. I would like to fit a dynamic multilevel regression model in R, with random coefficients for the mean and first order auto-correlation in each group, and with no cross correlation between the two variance parameters; i.e. this model:</p>

<p>$y_{i,t} - \mu_{i} = \rho_{i} (y_{i,t-1} - \mu_{i}) + \epsilon_{i,t}$</p>

<p>$\mu_{i} = \mu + v_{\mu}$ </p>

<p>$\rho_{i} = \rho + v_{\rho}$ </p>

<p>$\epsilon_{i,t} \sim N(0,\sigma_\epsilon^2), v_{\mu} \sim N(0,\sigma_\mu^2), v_{\rho} \sim N(0,\sigma_\rho^2)$ </p>

<p>The best I can do so far in R is:</p>

<pre><code>library(nlme)
library(dplyr)

#create toy data set
df0 &lt;- Orthodont %&gt;% 
  group_by(Subject) %&gt;% 
  mutate(lag1=lag(distance)) %&gt;% 
  filter(!is.na(lag1))

#multilevel model, mean (not Subject specific mean) centered
m1 &lt;- lme(fixed = distance ~ I(lag1 - mean(distance)), data=df0, 
          random= list(Subject = pdDiag(~ + I(lag1 - mean(distance)))) )
m1
# Linear mixed-effects model fit by REML
#   Data: df0 
#   Log-restricted-likelihood: -164.8976
#   Fixed: distance ~ I(lag1 - mean(distance)) 
#              (Intercept) I(lag1 - mean(distance)) 
#               25.7907622                0.8289975 
# 
# Random effects:
#  Formula: ~+I(lag1 - mean(distance)) | Subject
#  Structure: Diagonal
#          (Intercept) I(lag1 - mean(distance)) Residual
# StdDev: 7.892818e-05                0.3031237 1.675277
# 
# Number of Observations: 81
# Number of Groups: 27 
</code></pre>

<p>This 1) does not estimate ($\mu$) and 2) centres the distance lag ($y_{i,t-1}$) on the population mean ($\mu$) rather than the subject ($i$) specific mean ($\mu_i$) that I desire. </p>
"
"0.0839181358296689","0.0839921051131616","137475","<p>I had thought that if I call acf() on a vector of integers, and examine the value correlation coefficient at K (lag) = 1, I would expect:</p>

<ul>
<li>A value close to +/-1 would show that the integers are not random at
all</li>
<li>A value close to 0 would show that the sequence is pretty much random</li>
</ul>

<p>I have called the acf function on sequences that are <strong><em>very</em></strong> <strong><em>un-random</em></strong>, and the above premise does NOT hold.</p>

<p>I am learning about acf, and it is interesting, but I donâ€™t necessarily see how it can prove show that a sequence of integers is close to random.</p>

<p>Is it important to choose the correct lag value? </p>

<p>Any and all advice/pointers very much appreciated.</p>

<p>Thank you,</p>

<p>Richard Rogers</p>
"
"0.0726752374667264","0.0727392967453308","139040","<p>Citation:</p>

<blockquote>
  <p>As the data represent repeated measurements from individual plots,
  within-plot correlation may result in inefficient estimates and
  underestimation of standard error. Therefore the error term (o) was
  explicitly modeled using a first order autoregressive model that
  accommodates for the irregular spacing of measurements.</p>
</blockquote>

<p><img src=""http://i.stack.imgur.com/tKOSU.jpg"" alt=""enter image description here""></p>

<blockquote>
  <p>where e(i, T) is the error at the ith sample plot at age T, a is the time since the previous measurement, p is a coefficient of correlation between measurements one year apart and u(i)s are normally and independently distributed random errors.</p>
</blockquote>

<p>Authors wrote that they used ordinary nonlinear last squares procedure.
How do I incorporate such error statement into nonlinear last squares procedure (<code>nls</code>) in R?</p>
"
"0.0726752374667264","0.0727392967453308","139928","<p>how to simulate data (in R) to generate , sample values
1) variables with specific correlation values for a particular model AND 
2) with predefined regression coefficients? 
3) Can we also set the mean and SD in the same process? 
4) Also how does one simulate the p value/significance of the variable. </p>

<p>This is for imitating existing models for analysis and teaching purposes</p>

<p>Sorry for not being specific : this is for multiple regression, sample values. I would like to specify the mean and SD if possible (apparently not, I can specify only one in order to specify the regression coefficients?)</p>

<p>Thanks for the help.</p>
"
"0.0938233281301002","0.0751248226425348","141051","<p>I am interested in testing if the correlation coefficient between 2 dependent variables is significantly affected by an independent variable (which has two levels). I don't think calculating a partial correlation is what I want to do because I don't want to exclude the effect of the independent variable. What I did was to calculate the correlation coefficients independently for each subset of data corresponding to each level of the independent variable and then compared the correlation coefficients using paired.r. I don't know if this is the better way, it seems like there should be a way to estimate the effect of the independent variable within one statistic. Note I am doing this test within each subject.</p>

<p>Any input would be much appreciated!</p>
"
"0.0726752374667264","0.0727392967453308","141928","<p>Say I have two groups of observations <code>A</code> and <code>B</code>. Each group contains the SAME two set of observations <code>a_1,a_2</code> and <code>b_1,b_2</code>. For group <code>A</code>, I estimate how well <code>a_1</code> and <code>a_2</code> are correlated by computing Spearman's Correlation Coefficient between <code>a_1</code> and <code>a_2</code> and likewise for group <code>B</code>. How do I assess the magnitude of the difference?</p>

<p>E.g. think of it this way: Group <code>A</code> is women and group <code>B</code> is men. <code>a_1</code> is weight and so is <code>b_1</code>. Likewise <code>a_2</code> and <code>b_2</code> are heights. How do I asses sif there is a significant difference in how weight and height are connected in men and women?</p>

<p>I guess it comes down to assuming linearity and then constructing and comparing two linear models, one for men and one for women?</p>
"
"0.111890847772892","0.0979907892986885","142693","<p><strong><em>Is the following a reasonable illustration of the OVB problem?</em></strong></p>

<p>We build up fictional data around the regression line:</p>

<p>$$y = 7.2 + 2.3 \, x_1 + 0.1 \, x_2 + 1.5 \, x_3 + 0.013 \, x_4 + eps$$</p>

<p>by using this function:</p>

<pre><code>correlatedValue = function(x, r){
  r2 = r**2
  ve = 1 - r2
  SD = sqrt(ve)
  e  = rnorm(length(x), mean = 0, sd = SD)
  y  = r * x + e
}
</code></pre>

<p>-thank you, @gung for this post:
<a href=""http://stats.stackexchange.com/questions/38856/how-to-generate-correlated-random-numbers-given-means-variances-and-degree-of"">How to generate correlated random numbers (given means, variances and degree of correlation)?</a></p>

<p>And the following function, which generates four variables (<strong><em>x1</em></strong>, <strong><em>x2</em></strong>, <strong><em>x3</em></strong> and <strong><em>x4</em></strong>) as well as noise (<strong><em>eps</em></strong>). <strong><em>x1</em></strong> and <strong><em>x3</em></strong> are sample from normal distributions; <strong><em>x2</em></strong> is extracted from a uniform; and <strong><em>x4</em></strong> from a Poisson.</p>

<pre><code>variables &lt;- function(){
x &lt;- rnorm(1000)
x1 &lt;- 50 + 15 * x
x3 &lt;- 28 + 11 * correlatedValue(x = x, r = 0.6)
x2 &lt;- runif(1000, 0, 100)
x4 &lt;- rpois(1000,50)
eps &lt;- rnorm(1000,5, 7)
y = 7.2 + 2.3 * x1 + 0.001 * x2 + 1.5 * x3 + 0.013 * x4 + eps
dat &lt;- as.data.frame(cbind(y, x1, x2, x3, x4))
c &lt;- as.numeric(coef(lm(y ~ x2 + x3 + x4, dat))[3])
d &lt;- as.numeric(coef(lm(y ~ x1 + x2 + x3 + x4, dat))[4])
c(c,d)
}
</code></pre>

<p><strong><em>x1</em></strong> and <strong><em>x3</em></strong> are highly influential on <strong><em>y</em></strong> and are correlated with each other, setting the values up to observe <strong><em>OVB</em></strong>. <strong><em>x2</em></strong> and <strong><em>x4</em></strong> are less influential.</p>

<p>Here is the plotting of <strong><em>y</em></strong> against <strong><em>x1</em></strong>, <strong><em>x2</em></strong>, <strong><em>x3</em></strong>  and <strong><em>x4</em></strong>, and <strong><em>x1</em></strong> over <strong><em>x3</em></strong> with added regression lines:</p>

<p><img src=""http://i.stack.imgur.com/I4u0S.png"" alt=""enter image description here""></p>

<p>And following is the variance-covariance matrix:</p>

<pre><code>             y           x1           x2         x3          x4
y   1.00000000  0.944410945  0.014421682 0.77571067 -0.01463981
x1  0.94441094  1.000000000 -0.001726526 0.56504020 -0.03562991
x2  0.01442168 -0.001726526  1.000000000 0.03537959  0.02253922
x3  0.77571067  0.565040198  0.035379590 1.00000000  0.02573827
x4 -0.01463981 -0.035629906  0.022539218 0.02573827  1.00000000
</code></pre>

<p>Predictably, the regression including all variables shows similar coefficients to the initial equation:</p>

<pre><code>coef(lm(y~.,dat))[2:5]
         x1          x2          x3          x4 
2.253353226 0.004899445 1.547915198 0.017710038 
</code></pre>

<p>Wrapping up, a quick simulation is carried out to obtain the mean of the <strong><em>x3</em></strong> coefficient in 1,000 simulations <em>WITHOUT</em> including <strong><em>x1</em></strong> (""coef_x3"") and then <em>WITH</em> <strong><em>x1</em></strong> (""coef_x3_full""):</p>

<pre><code>coef_x3 &lt;- NULL
coef_x3_full &lt;- NULL
for (i in 1:1000){
  coef_x3[i] = variables()[1]
  coef_x3_full[i] = variables()[2]
}
mean(coef_x3)
mean(coef_x3_full)
</code></pre>

<p>obtaining a coefficient for <strong><em>x3</em></strong> of <strong>3.383</strong> when <strong><em>x1</em></strong> is excluded versus a coefficient for <strong><em>x3</em></strong> of <strong>1.502</strong> when included. So when <strong><em>x1</em></strong> is included we have an unbiased estimation of the true <strong><em>x3</em></strong> coefficient (<strong><em>1.5</em></strong>), whereas the estimation is biased when we exclude <strong><em>x1</em></strong>.</p>
"
"0.0839181358296689","0.0629940788348712","145514","<p>Since a few days I do not get ahead when trying to compare two <strong><em>Pearson</em></strong> correlation coefficients. Imagine that I've got two datasets where on each I do a correlation between Land Surface Temperature and an urban metric. The datasets are different in their length, so the first one has round about 160.000 observables and the second one has about 2400 observables. For the correlation on the first dataset I get a <strong><em>Pearson</em></strong> of -0.74 and for the second I get -0.885. Now I want to find out whether these coefficients are significantly different from each other. Is there any appropriate method you could suggest?</p>

<p>I already played around with the <em>Fisher-Z-Transformation</em>, but from my point of view with no purposeful results. When I calculate the <em>Fisher-Z</em> on my coefficients, it results in 20.95 (<em>Fisher-Z</em>).</p>

<p>I would be very happy about suitable advices.</p>
"
"0.201875659629796","0.210135746153178","145849","<p>Iâ€™ve got a question concerning the R package <em>strucchange</em> that I use for testing and dating structural breaks in my PhD thesis.  To be specific, I use the generalized fluctuation test framework with CUSUM/MOSUM and in particular Moving Estimates (<strong>ME</strong>) tests for my analysis. Thus, the following description focuses on the ME test, but in principle is more general to all fluctuation tests.</p>

<p><strong>The problem:</strong> I am testing time series data for structural breaks with the ME test that draws on the function <strong>efp</strong> provided by strucchange. Given the nature of time series data, I want to tackle potential heteroskedasticity and autocorrelation in the data. Strucchange provides some functionality with respect to calculating <em>heteroskedasticity</em> (<strong>HC</strong>) and <em>autocorrelation</em> (<strong>HAC</strong>) consistent covariance matrices,  e.g., the approaches suggested by Newey-West (1987) or Andrews (1991). </p>

<p>However, this functionality in strucchange is limited to the function <strong>gefp</strong> that calculates Generalized Empirical M-Fluctuation Processes that as far as I know does not allow to perform estimates-based tests such as the ME test. Thus, I cannot use <strong>efp</strong> to estimate ME tests (or other tests that are available in this function) using HAC covariance matrices. </p>

<p><strong>The question:</strong> Does anybody know how I could make use of the <strong>efp</strong> function in <em>strucchange</em> for testing and dating structural changes but use HAC covariance matrices to take heteroskedasticity and autocorrelation into account? Maybe there is some way to use the sandwich package for this?</p>

<p><strong>Many thanks for any help!</strong></p>

<p>Here is a minimal working example to show the problem</p>

<pre><code>library(foreign)
library(strucchange)

data(""Nile"")

#using the function efp to perform a moving estimates test
#assuming sperical disturbances
ocus.nile &lt;- efp(Nile ~ 1, type = ""ME"")
sctest(ocus.nile)

#applying the vcov function with the kernHAC option to take heteroskedasticity and autocorrelation does not work, i.e., the option is not used and the result is the same
sctest(ocus.nile, vcov=kernHAC)

#using the function gefp to perform a generalized M-fluctuation process however works with vcov
#assuming spherical disturbances
ocus.nile2 &lt;- gefp(Nile ~ 1, fit = lm)
sctest(ocus.nile2)

#controlling for heteroskedasticity and autocorrelation using an appropriate covariance matrix changes the result, i.e. works
ocus.nile2 &lt;- gefp(Nile ~ 1, fit = lm, vcov= kernHAC)
sctest(ocus.nile2)
</code></pre>

<p><strong>Some background</strong></p>

<p>Though probably not necessary, here is some more in-depth background about the problem for the interested reader (and the archive). The formulas are taken from Zeileis et al., 2005, â€Monitoring structural change in dynamic econometric modelsâ€. </p>

<p>The ME test is used to detect structural breaks in the standard linear regression model over time. What it does it in essence partitioning the data and rather than estimating the regression based on the whole sample, it sequentially moves â€œthroughâ€ time in a fixed-width windows containing only a sub-sample of the observations and in each window it estimates the model. These estimates are used to the computation of empirical fluctuation processes that capture fluctuations in regression coefficients and residuals over time. Significant fluctuations of the coefficients are signs of a structural break in the regression. The test statistic of the Moving estimates test is</p>

<p><img src=""http://i.stack.imgur.com/qQ0FY.png"" alt=""Moving estimates test statistic""></p>

<p>where <em>n</em> is the number of observations, <em>h</em> is the bandwith (how many percent of the total number of observations are used for the window), <em>nh</em> is thus the size of the window, Q_(n)=X_(n)^T X_(N)/n, i=[k+t(n-k)], and sigma^2 is an estimate of the variance. The way I understand the above statistic is that it compares the difference between the sub-sample estimate of beta with the whole sample (the window) estimate and how this difference develops over time. A zero difference would indicate a sub-sample estimate that perfectly equals the whole-sample estimate, which would indicate perfect stability of the coefficient. In my understanding, the efp function in strucchange calculates sigma^2 based on the standard OLS residuals u^ i.e., sigma^2=(1/n-k)âˆ‘_(i=1)^n u_i^2 . Thus, in the presence of heteroskedasticity or autocorrelation, the OLS assumption of spherical disturbances will be violated. Thus, ideally, sigma^2 should be estimated based on a HAC covariance matrix to avoid wrong inference.</p>

<p>The question that comes to my mind is whether there is a way to use the ME test based on a HAC estimate. If not, it seems to me that it is limited to spherical disturbances of residuals, which seems to be violated in most applications.</p>
"
"0.16250677125654","0.151806678014216","146919","<p>Please, be kind, as I'm totally noob in stats and R...</p>

<p>I'm the owner of a small restaurant in a commercial center, and I managedd to collect two main dataset, commercial-center (cc) and restaurant (rest).</p>

<p>cc <a href=""https://www.dropbox.com/s/7k6k1rjimrcfqlq/cc.csv?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/7k6k1rjimrcfqlq/cc.csv?dl=0</a> <br/>
DAY 10:00-12.00 12:00-14:00 14:00-16:00 16:00-18:00 18:00-20:00 20:00-22:00 SUB-TOTAL<br/>
01/01/2012  0   825,55  534,85  879,7   964,725 161,975 3366,8</p>

<p>till today</p>

<p>rest <a href=""https://www.dropbox.com/s/rtoqwg64tu4poxs/rest.csv?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/rtoqwg64tu4poxs/rest.csv?dl=0</a> <br/>
is a database of all the restaurant sales from 04 dec 2014<br/>
I have managed to make it in a similar fashion as the above format<br/>
It contains many NA values (periods we were closed)</p>

<p>Also, I gathered other data: <br/>
events <br/>
is a dataset containing holidays and other events (it has to be categorized, as some increase sales, some the opposite)</p>

<p>weather <br/>
a collection of calculated observation for a spot 2 km away from the restaurant, each observation (wind speed, temp, cloudness and rain) have been turned in a value on a scale of 4 values</p>

<p>My objectives are:</p>

<ol>
<li>understand 'rest' seasonalities (daily/weekly, yearly)</li>
<li>understand how weather and other independet variables (not fixed events i.e. Easter or concerts) modify 'rest' and find a coefficient to apply to the model </li>
<li>forecast next days, weeks, months sales movements</li>
<li>verificate the restaurant trend, net of seasonalities and other variables</li>
</ol>

<p>An SD up to 30-40% on model/observed is still a good point to me as long as error distribution is not well spread but with a solid peak on 0</p>

<p>After many testing and try, I have found that the best model to choose is TBATS (BATS is deadly slow to me), expecially for managing multiple seasonality (which is a main point in my study).</p>

<p>The method I was thinking was to:</p>

<ol>
<li>Find indipendent variables coefficient for 'rest' and 'cc' and apply them to the data</li>
<li>Load 'rest' and 'c'c, with modifications at point 1, in an msts object and throw them to TBATS</li>
<li>Find correlations or other kind of link between the seasonalities of 'rest' and 'cc', get coefficients and use them for modeling future 'rest' data</li>
</ol>

<p>Not sure whether I can post multiple questions in here, if not just answer to this please: am I choosing the proper work method, or is anything better out there?</p>

<p>I'm struggling myself with many questions:</p>

<p><ol>
<li>How can I properly describe the data I have in R for using in TBATS, with sampling every 2 hours, starting at 10 and finishing at 22? Should I create a model for each column?</li>
<li>TBATS is not allowing for independent variables (events), is it? How to manage them?</li>
<li>TBATS does not accept NA values? How to manage the closing periods or days (NA) in the 'rest' dataset? Some weeks we were closed on Mondays, some Tuesday, some others we were always opening; we have some afternoon openings.... big mess... Maybe use regressions..?</li>
<li>If I do:</p>

<blockquote>
  <p>export &lt;- data.frame(DAYS=date,tbats.components(cc-SUBTOTAL.msts.tbats),errors=resid(cc-SUBTOTAL.msts.tbats))</li>
  </ol>
  how should I interpretate data? <br/>
  i.e. If I do level+season1+season2+errors=cObserved, I get figures which are slightly different from the ""real"" observed: sd(cObserved/observed)=3% which is fine for my objectives... is it correct?</p>
</blockquote>

<ol start=""5"">
<li>In TBATS, is trend some kind of SMA? at which window?</li>
</ol>
"
"0.0419590679148345","0.0419960525565808","147987","<p>Given a pair of random variables $(X,Y)$ over a product space $\mathcal{X}\times \mathcal{Y}$, the <em>maximal correlation</em> coefficient is defined as
$$\rho_2(X;Y):=\sup\frac{\mathbb{E}[f(X)g(Y)]}{||f||_2||g||_2},$$ where supremum is taken over all pair of functions $(f,g)$ such that $\mathbb{E}[f(X)]=\mathbb{E}[g(Y)]=0$ and $f\in L^2(\mathcal{X})$ and $g\in L^2(\mathcal{Y})$.</p>

<p>Renyi showed that that $\rho_2(X;Y)=0$ if and only if $X$ and $Y$ are independent and  $\rho_2(X;Y)=1$ if there exists a pair of functions $f$ and $g$ such that $f(X)=g(Y)$ with probability one.</p>

<p>I am looking for an algorithm in R or MATLAB to estimate the maximal correlation for a (discrete or continuous) given joint distribution $P_{XY}$.</p>
"
"0.1876466562602","0.187812056606337","149275","<p>I am analyzing some data where the variables of interests are circular (angles). I use R and the <code>circular</code> package.</p>

<p>In my dataset, every observation consists in a 2D Euclidean vector representing a movement on a plane (the length is constant, only the angle varies), together with a measure of a response to that movement, expressed as displacement in cartesian coordinates (i.e. the displacement along the x and y axis). I would like to fit a model where the initial movement (either the x-y cartesian representation or only the angle, since the length is constant) is the response variable. Ultimately, I would like to use the fitted model to estimate the initial movement vector in a related dataset where I only know the response to the movement.</p>

<p>My measurement of the response to the movement are affected by noise, independent over x and y dimensions, with possibly different variances and different additive biases. 
To represent graphically the problem, what I want to do is estimate the direction of the original vectors (gray thick arrows in the figure below) starting from the noisy measurements (the thin black arrows).
<img src=""http://i.stack.imgur.com/g7GWJ.png"" alt=""enter image description here""></p>

<p>I have tried fitting different models:</p>

<ol>
<li>one model with both predictor and dependent variable being circular
(the angle of the initial movement vector, and the angle computed
from the (x,y) displacement)</li>
<li>one multivariate, linear model, with the cartesian (x,y) components of the initial movement as dependent variables, and the
measured x,y displacement as linear predictors; </li>
<li>one model with circular dependent variable, and linear predictor (this last one with no success)</li>
</ol>

<p>First, I wasn't able to fit the 3rd model for some reason that I don't understand. I report here a reproducible example</p>

<pre><code>require(circular)
theta &lt;- circular(runif(500,0,2*pi),units=""radians"",type=""angles"",zero=0,rotation=""counter"")
rho &lt;- rep(8,500)

# add measurement noise (different for x and y)
mX &lt;- as.numeric(rnorm(500,0,2) + rho * cos(theta))
mY &lt;- as.numeric(rnorm(500,-1,1) + rho * sin(theta))
linearPred &lt;- cbind(mX,mY)

# fit model
mcl &lt;- lm.circular(y=theta, x=linearPred,init=c(1,1), type=""c-l"",verbose=T)
</code></pre>

<p>Here is the output:</p>

<pre><code>&gt; mcl &lt;- lm.circular(y=theta, x=linearPred,init=c(1,1), type=""c-l"",verbose=T)
Iteration  1 :    Log-Likelihood =  2.392981 
Iteration  2 :    Log-Likelihood =  1.082084 
Iteration  3 :    Log-Likelihood =  0.8013503 
Iteration  4 :    Log-Likelihood =  NA 
Error in while (diff &gt; tol) { : 
  valore mancante dove Ã¨ richiesto TRUE/FALSE
</code></pre>

<p>(the last line says: missing value where a TRUE/FALSE was required).
Can anyone shed light on this? I don't understand where this error comes from.</p>

<p>Second question, which model would you suggest to use among the first two? Here is the code that I used for the two models</p>

<pre><code># fit sin(theta) and cos(theta) with a multivariate linear model
mmv &lt;- lm(cbind(sin(theta),cos(theta)) ~ mX+mY)

# ""circular-circular"" model
angularPred &lt;- circular(atan2(mY,mX),units=""radians"",type=""angles"",zero=0,rotation=""counter"")
mcc &lt;- lm.circular(y=theta, x=angularPred, type=""c-c"")
</code></pre>

<p>I can compute the angle from the multivariate fitted values as <code>atan2(mmv$fitted[,1],mmv$fitted[,2])</code>. Both seems to perform similarly in terms of mean angular error. To compare the two I computed the correlation between the predicted angles and the initial angle <code>theta</code> (Jammalamadaka - Sarma correlation coefficient), and the multivariate model seems to perform slightly better:</p>

<pre><code>fitted.mmv &lt;- circular(atan2(mmv$fitted[,1],mmv$fitted[,2]),units=""radians"",type=""angles"",zero=0,rotation=""counter"")
&gt; cor.circular(fitted.mmv,theta) # multivariate
[1] 0.9851422
&gt; cor.circular(mcc$fitted,theta) # ""circular-circular""
[1] 0.7262862
</code></pre>

<p>However, the distribution of residuals of the multivariate model shows a strange pattern (figure below). 
<img src=""http://i.stack.imgur.com/ROHA0.png"" alt=""enter image description here""></p>

<p>Is this pattern in the residuals a problem? Can anyone gives some advice on this? Which model would you use in this case? Is there any other possible approach that you would recommend?
Any advice is appreciated, thanks!</p>
"
"0.10277830647413","0.102868899974728","149415","<p>I've run <code>lagsarlm</code> on my dataset, using a mixed model and using a row-standardized adjacency matrix. I have results that I think are good, but would am not sure how to interpret them.</p>

<ul>
<li>Does the p-value of 0.12 on rho mean I cannot count on spatial autocorrelation of the response?</li>
<li>Does the low p-value for the LM test mean that the error term is not spatially correlated to the response?</li>
<li>What about the various p-values of the coefficients: Should I remove predictors that have high p-values and run it again?</li>
</ul>

<p>.</p>

<pre><code>&gt; summary(lm.lag)

Call:lagsarlm(formula = Y.scaled ~ Narcotics.Crime.Rate + Assault..Homicide. + 
    Infant.Mortality.Rate + Below.Poverty.Level + Per.Capita.Income, 
    data = X.scaled, listw = W.mat, type = ""mixed"")

Residuals:
     Min       1Q   Median       3Q      Max 
-0.96641 -0.33183 -0.13579  0.15113  3.00270 

Type: mixed 
Coefficients: (asymptotic standard errors) 
                           Estimate Std. Error z value  Pr(&gt;|z|)
(Intercept)                0.007063   0.069365  0.1018 0.9188960
Narcotics.Crime.Rate       0.465759   0.176160  2.6439 0.0081945
Assault..Homicide.         0.202034   0.156141  1.2939 0.1956925
Infant.Mortality.Rate      0.121582   0.130806  0.9295 0.3526420
Below.Poverty.Level        0.051494   0.129330  0.3982 0.6905098
Per.Capita.Income         -0.119833   0.171509 -0.6987 0.4847382
lag.Narcotics.Crime.Rate  -0.673492   0.284876 -2.3642 0.0180710
lag.Assault..Homicide.     0.366021   0.295266  1.2396 0.2151117
lag.Infant.Mortality.Rate  0.010755   0.240319  0.0448 0.9643038
lag.Below.Poverty.Level    0.232895   0.202924  1.1477 0.2510930
lag.Per.Capita.Income      0.885463   0.256441  3.4529 0.0005546

Rho: 0.26724, LR test value: 2.3413, p-value: 0.12598
Asymptotic standard error: 0.14187
    z-value: 1.8838, p-value: 0.059597
Wald statistic: 3.5486, p-value: 0.059597

Log likelihood: -70.92512 for mixed model
ML residual variance (sigma squared): 0.36337, (sigma: 0.6028)
Number of observations: 77 
Number of parameters estimated: 13 
AIC: 167.85, (AIC for lm: 168.19)
LM test for residual autocorrelation
test value: 14.516, p-value: 0.00013896
</code></pre>
"
"0.0593390829096927","0.0593913870916499","149660","<p>I am wondering if the computed <code>R-squared</code> from <code>lm</code> would be the same if you compute the predictions and then calculate <code>R and square it</code> between them and the original values (to be predicted). In my case here they are different.
the model is:</p>

<pre><code>log(y) = alog(1+x1)+blog(1+x2)+c
x1 &lt;- c(1,2,3,4,NA,5,5,6);x1 &lt;- log(x1) 
x2 &lt;- c(0.9,0.4,0.8,4,NA,3,4,6);x2 &lt;- log(x2) 
y  &lt;- c(1.6,4.4,5.5,8.3,3,NA,7,3) 
y2 = log(y)
fm1 &lt;- lm(y2 ~ x1+x2)
&gt; summary(fm1)
Call:
lm(formula = y2 ~ x1 + x2)

Residuals:
        1         2         3         4         7         8 
-0.147261 -0.029340  0.008704  0.612128  0.229587 -0.673819 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)   0.5868     0.5551   1.057    0.368
x1            0.9509     0.5871   1.620    0.204
x2           -0.2892     0.3517  -0.822    0.471

Residual standard error: 0.549 on 3 degrees of freedom
(2 observations deleted due to missingness)
Multiple R-squared:  0.5079,    Adjusted R-squared:  0.1799 
F-statistic: 1.548 on 2 and 3 DF,  p-value: 0.3452  
</code></pre>

<p>Now lets compare the correlations:  </p>

<pre><code>&gt; y1= exp((0.9509*x1)+ (-0.2892 * x2) +(0.5868))
&gt; cor(y1,y, use=""na.or.complete"", method=""pearson"")
[1] 0.4813884
&gt; (0.4813884)^2
[1] 0.2317348
</code></pre>

<p>You can see here the values are different!</p>

<pre><code>&gt; y1
[1] 1.647246 2.600102 2.838444 2.381542       NA 2.792149 2.639814 2.655218
&gt; fitted(fm2)
       1        2        3        4        7        8 
0.661362 1.471144 1.751985 1.450974 1.728920 1.752749
</code></pre>
"
"0.196991754075415","0.197165391618229","149732","<p>I'm trying to analyze the data from an experiment I conducted, and could use some guidance in relation to fixed vs. random effects.</p>

<p>The experiment was related to risk-seeking behavior in the context of hypothetical gambles, and implemented a 3 (Response Scale: Control vs. RI vs. ABR) x 3 (Stakes) X 5 (Endowment) factorial design. Response Scale was a between-subjects manipulation, and the levels of Stakes and Endowment were combined factorially to produce 15 different gamble scenarios, all of which were evaluated by each participant (i.e. gamble evaluation was within-subjects). The DV of interest for the particular analysis I'm working on is a binary indicator variable called ""Would.Play"" that describes whether a participant would choose to play the gamble if they were to encounter it in real life.</p>

<p>As a preliminary analysis, I'd like to be able to claim that there were no [or, as the data seem to indicate, <em>were</em>] meaningful differences in Would.Play as a result of random assignment to a particular Response Scale condition (designated by the factor variable ""Response.Scale"", ref=""Control"").</p>

<p>I can obviously do this with a binary logit for each of the 15 gambles (designated by the variable ""Gamble.Num""), but I'd like to avoid issues with multiple testing. My preference, therefore, is to fit a single model that accounts for the heterogeneity in gambles by fitting a separate intercept for each gamble.</p>

<p>I've come across two ways to do this, each of which seems to give different results: Dummy ""Fixed Effects"" modeling in glm() and ""random effects"" modeling in glmer() (see output below).</p>

<p>It seems possible that the difference in the estimated coefficients could be the result of the Dummy ""Fixed Effects"" approach taking Gamble.Num==1 as a reference level, but I don't have a very deep understanding of the math underlying these two techniques. I was hoping someone would be able to give me a quick explanation of (a) why the these two models appear to give different results; and (b) whether one of these approaches is better suited to answering my question of interest: is there a unique effect of Response.Scale on Would.Play, taking heterogeneity in gambles into account?</p>

<p>Below is a quick look at the data I'm using, and the output of the two models:</p>

<pre><code>## Data ##
head(analysis.0.data)
 Local.ID Condition Response.Scale RS.Code Gambles.First Gamble.Num Endowment Stakes
1        8         4             RI       1             0          1      -150     10
2        8         4             RI       1             0          2      -150     50
3        8         4             RI       1             0          3      -150    200
4        8         4             RI       1             0          4       -25     10
5        8         4             RI       1             0          5       -25     50
6        8         4             RI       1             0          6       -25    200
  Would.Play Perc.Risk
1          0         4
2          0         6
3          0         5
4          0         3
5          0         5
6          0         7


## Dummy ""Fixed Effects"" Model ##
summary(glm(Would.Play ~ Response.Scale + factor(Gamble.Num), family=""binomial"",     
data=analysis.0.data))

Call:
glm(formula = Would.Play ~ Response.Scale + factor(Gamble.Num), 
    family = ""binomial"", data = analysis.0.data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.7766  -0.7204  -0.4678   0.7006   2.5394  

Coefficients:
                     Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)          -1.14906    0.21987  -5.226 1.73e-07 ***
Response.ScaleRI     -0.06749    0.12815  -0.527  0.59844    
Response.ScaleABR    -0.91035    0.13843  -6.576 4.82e-11 ***
factor(Gamble.Num)2  -0.94090    0.35886  -2.622  0.00874 ** 
factor(Gamble.Num)3  -1.12416    0.37769  -2.976  0.00292 ** 
factor(Gamble.Num)4   0.31966    0.28379   1.126  0.25999    
factor(Gamble.Num)5  -0.63953    0.33303  -1.920  0.05482 .  
factor(Gamble.Num)6  -0.85860    0.35120  -2.445  0.01449 *  
factor(Gamble.Num)7   1.42100    0.26770   5.308 1.11e-07 ***
factor(Gamble.Num)8   0.35620    0.28268   1.260  0.20765    
factor(Gamble.Num)9  -0.51138    0.32379  -1.579  0.11425    
factor(Gamble.Num)10  2.10754    0.27298   7.720 1.16e-14 ***
factor(Gamble.Num)11  0.28248    0.28496   0.991  0.32154    
factor(Gamble.Num)12 -1.02908    0.36760  -2.799  0.00512 ** 
factor(Gamble.Num)13  2.49612    0.28133   8.873  &lt; 2e-16 ***
factor(Gamble.Num)14  1.72839    0.26867   6.433 1.25e-10 ***
factor(Gamble.Num)15  0.08524    0.29204   0.292  0.77039    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2649.2  on 2249  degrees of freedom
Residual deviance: 2096.4  on 2233  degrees of freedom
AIC: 2130.4

Number of Fisher Scoring iterations: 5


## GLMER ""Random-Effects"" Model##
summary(glmer(Would.Play ~ Response.Scale + (1|Gamble.Num), family=""binomial"", 
data=analysis.0.data))
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
[glmerMod]
 Family: binomial  ( logit )
Formula: Would.Play ~ Response.Scale + (1 | Gamble.Num)
   Data: analysis.0.data

     AIC      BIC   logLik deviance df.resid 
  2169.3   2192.1  -1080.6   2161.3     2246 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.9011 -0.5461 -0.3522  0.5439  4.6708 

Random effects:
 Groups     Name        Variance Std.Dev.
 Gamble.Num (Intercept) 1.291    1.136   
Number of obs: 2250, groups:  Gamble.Num, 15

Fixed effects:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)       -0.90254    0.30722  -2.938  0.00331 ** 
Response.ScaleRI  -0.06682    0.12707  -0.526  0.59897    
Response.ScaleABR -0.90170    0.13727  -6.569 5.07e-11 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Rs.SRI
Rspns.SclRI -0.202       
Rspns.ScABR -0.183  0.456
</code></pre>

<p>Thanks!</p>
"
"0.168127401577463","0.1583770322444","149799","<p>I want to code for Detrended Cross Correlation in R for time-series data but I'm still stuck. I don't know why the coefficient is not in range -1 : 1. I try to write following these equation below</p>

<p><a href=""http://arxiv.org/pdf/1310.3984.pdf"" rel=""nofollow"">Measuring correlations between non-stationary series with DCCA coefficient</a></p>

<p>Detrened cross-correlation coefficient is calculated as detrended covariance of two dataset over detrened variance of two integrated series </p>

<p><img src=""http://i.stack.imgur.com/7EjJX.png"" alt=""enter image description here"">  (Equation 1)</p>

<p>For time-series {xt}, use integrated series profile</p>

<p><img src=""http://i.stack.imgur.com/JNdJv.png"" alt=""enter image description here"">   (Equation 2)</p>

<p>where the data must be detrended by local trend in box of size s</p>

<p><img src=""http://i.stack.imgur.com/eMg8Z.png"" alt=""enter image description here"">  (Equation 3)</p>

<p><img src=""http://i.stack.imgur.com/SfhD3.png"" alt=""enter image description here"">(Equation 4)</p>

<p>The X_hat is linear fit value evaluated by least square method</p>

<p>Detrended covariance of two profiles</p>

<p><img src=""http://i.stack.imgur.com/aiHyX.png"" alt=""enter image description here""> (Equation 5)</p>

<p>Average the covariance over all boxes</p>

<p><img src=""http://i.stack.imgur.com/ixtwd.png"" alt=""enter image description here"">  (Equation 6)</p>

<pre><code>## data_1
    x= c(-1.042061,-0.669056,-0.685977,-0.067925,0.808380,1.385235,1.455245,0.540762 ,0.139570,-1.038133,0.080121,-0.102159,-0.068675,0.515445,0.600459,0.655325,0.610604,0.482337,0.079108,-0.118951,-0.050178,0.007500,-0.200622)
    ## data_2
    y= c(-2.368030,-2.607095,-1.277660,0.301499,1.346982,1.885968,1.765950,1.242890,-0.464786,0.186658,-0.036450,-0.396513,-0.157115,-0.012962,0.378752,-0.151658,0.774253,0.646541,0.311877,-0.694177,-0.412918,-0.338630,0.276635)
    ## window size = 6
    k=6
    DCCA_CC=function(x,y,k){
      ## calculate cumulative sum profile of all t
    xx&lt;- cumsum(x - mean(x))  ## Equation 2
    yy&lt;- cumsum(y - mean(y))  ## Equation 2

      ## Divide in to overlapping boxes of size k

  slide_win_xx = mat_sliding_window(xx,k)
  slide_win_yy = mat_sliding_window(yy,k)
  ## calculate linear fit value in each box 
  x_hat = t(apply(slide_win_xx,1,function(n) (lm(n~seq(1:length(n)))$fitted.values)))
  y_hat = t(apply(slide_win_yy,1,function(n) (lm(n~seq(1:length(n)))$fitted.values)))

##  Get detrend variance in each box with linear fit value (detrend by local trend).
  F2_dfa_x = c()
  F2_dfa_y = c()
  for(i in 1:nrow(x_hat)){
 ## Equation 4
    F2_dfa_x = c(F2_dfa_x,mean((xx[i:(i+k-1)]-x_hat[i,])^2))
  }
  for(i in 1:nrow(y_hat)){
## Equation 4
    F2_dfa_y = c(F2_dfa_y,mean((yy[i:(i+k-1)]-y_hat[i,])^2))
  }
  ## Average detrend variance over all boxes to obtain fluctuation
  F2_dfa_x = mean(F2_dfa_x) ## Equation 3
  F2_dfa_y = mean(F2_dfa_y) ## Equation 3

  ## Get detrended covariance of two profile
  F2_dcca = c()
  for(i in 1:nrow(x_hat)){
  ## Equation 5
    F2_dcca = c(F2_dcca,mean((xx[i:(i+k-1)]-x_hat[i,]) * (yy[i:(i+k-1)]-y_hat[i,]) ))
  }

## Equation 6
  F2_dcca = mean(F2_dcca)

## Calculate correlation coefficient 
  rho = F2_dcca / (F2_dfa_x * F2_dfa_y) ## Equation 1
  return(rho)
}

mat_sliding_window = function(xx,k){
## Function to generate boxes given dataset(xx) and box size (k)
  slide_mat=c()
  for (i in 1:(length(xx)-k+1)){
    slide_mat = rbind(slide_mat,xx[i:(i+k-1)] )
  }
  return(slide_mat)
}

print(DCCA_CC(x,y,k)) ##This give me 3.392302
</code></pre>

<p>I'm not sure if something wrong in integrated profile.</p>
"
"0.0419590679148345","0.0419960525565808","149852","<p>I want to calculate the cophenetic correlation coefficient.
reading previous posts  </p>

<p><a href=""http://stats.stackexchange.com/questions/92546/comparison-of-cophenetic-correlation-coefficients-on-different-data-sets"">Comparison of cophenetic correlation coefficients on different data sets</a></p>

<p><a href=""http://stats.stackexchange.com/questions/33066/on-cophenetic-correlation-for-dendrogram-clustering"">On cophenetic correlation for dendrogram clustering</a></p>

<p><a href=""http://stackoverflow.com/questions/5639794/in-r-how-can-i-plot-a-similarity-matrix-like-a-block-graph-after-clustering-d"">http://stackoverflow.com/questions/5639794/in-r-how-can-i-plot-a-similarity-matrix-like-a-block-graph-after-clustering-d</a></p>

<p>I used the <code>cophenetic</code> function in the package <code>stats</code>. 
As far as I understand the results are cophenetic distances for the hierarchical clusteringis, in a new object of class <code>dis</code>. </p>

<pre><code>coph&lt;-cophenetic(hclsut_result)
</code></pre>

<p>To have an overview I clustered the cophenetic matrix, and I obtained the same clustering  as the one performed on my dataset.</p>

<p>However, I wanted to have an unique value that indicate the fidelity with wich my clsutering represent my distance matrix. Therefore, I correlated the <code>dis_matrix_for_my_dataset</code> with the <code>coph</code>.</p>

<pre><code>cor(euclidian_dist, coph)
</code></pre>

<p>Am I understanding right that the value I obtain indicates the <strong>cophenetic correlation coefficient</strong>?</p>
"
"0.0593390829096927","0.0593913870916499","151409","<p>I would like to fit a non-linear model that looks like the following: 
$V(g)=a*A(g)/(b*B(g)+c*C(g))$, where $g$ represents a gene, $a$, $b$ and $c$ are coefficients of $A(g)$, $B(g)$, $C(g)$, which are simple functions of $g$ that don't need to be optimized.  </p>

<p>As I have many genes ($&gt;1000$), my goal is to set up the coefficients $a$, $b$, and $c$, such that correlation between $V(g)$ and another given value $DE(g)$ is maximized. </p>

<p>I can do simple iteration for this; e.g.</p>

<pre><code>for i in range(a):
    for j in range(b):
        calculate V(g) for all genes with a=i and b=j
        calculate spearman correlation between all V(g) and all DE(g). 
take i,j pair that make the maximum spearman correlation value. 
</code></pre>

<ol>
<li>But are there R packages that people usually use for this purpose?</li>
<li>Since number of parameters are already set, I don't think I need to do cross-validation, do I?</li>
</ol>
"
"0.0726752374667264","0.0484928644968872","153133","<p>I want to calculate the weighted Kappa value with R. I compared two functions kappa2 and cohen.kappa but the two kappa values disagree. Is there an explanation for this or am I doing sth wrong?</p>

<p><strong>My dataset (Kappa_Step):</strong></p>

<pre><code>str(Kappa_Step)
'data.frame':   410 obs. of  2 variables:
$ BDM_a_EPT: int  1 20 5 4 7 7 30 17 7 9 ...
$ Step_fit : int  -2 14 8 7 16 3 18 10 3 9 ...
dim(Kappa_Step)
[1] 410   2
</code></pre>

<p><strong>kappa2 function :</strong></p>

<pre><code>&gt; kappa2(Kappa_Step, weight = c(""squared""), sort.levels = FALSE)
Cohen's Kappa for 2 Raters (Weights: squared)    
Subjects = 409, 
Raters = 2, 
Kappa = 0.126, 
z = 2.64, 
p-value = 0.0084 
</code></pre>

<p><strong>cohen.kappa function:</strong> </p>

<pre><code>&gt; cohen.kappa(Kappa_Step, w=NULL,n.obs=0,alpha=.05)
Cohen Kappa and Weighted Kappa correlation coefficients and confidence boundaries 
                   lower estimate upper
unweighted kappa -0.0042     0.02 0.044
weighted kappa    0.3316     0.41 0.492

Number of subjects = 409 
</code></pre>

<p><strong>Kappa functions:</strong> </p>

<ul>
<li><p>kappa2 from irr library (<a href=""http://www.inside-r.org/packages/cran/irr/docs/kappa2"" rel=""nofollow"">http://www.inside-r.org/packages/cran/irr/docs/kappa2</a>)</p></li>
<li><p>cohen.kappa from psych library ( <a href=""http://www.inside-r.org/packages/cran/psych/docs/wkappa"" rel=""nofollow"">http://www.inside-r.org/packages/cran/psych/docs/wkappa</a>)</p></li>
</ul>
"
"0.218025712400179","0.218217890235992","155524","<p>I am new to both mixed effect and Additive models so I'm sorry if the answer here is trivial.</p>

<p>I have data collected on several metabolic chemicals (M1,M2...), covariates (time,Race,Gender...) and disease state (D,D.binary).  I'm trying to generate a GAMM based on variables selected from a GEE variable selection.</p>

<p>Data: </p>

<ul>
<li>8 cases, 51 matched controls</li>
<li>approximately 10 time points from each subject</li>
<li>~630 observations</li>
<li>M1,M2...M3 are metabolites many of which are formed from common parts, Metabolite levels are correlated in that they are competing for the same component parts</li>
<li>Covariates stratify the subjects into subgroups</li>
</ul>

<p>Here is my model as it is now:</p>

<pre><code>&gt; b = gamm(D.binary ~ Time  + s(M1)  , 
      random = list(ParticipantID = ~ 1 + Time),  niterPQL=50,
      data = NEC_data, family=binomial(link=""logit"")) 

 Maximum number of PQL iterations:  50 
 iteration 1
 iteration 2
 ...
 iteration 49
 iteration 50
 Warning message:
 In gammPQL(y ~ X - 1, random = rand, data = strip.offset(mf), family = family,  :
  gamm not converged, try increasing niterPQL

&gt; plot(b$gam,pages=1)
</code></pre>

<p><img src=""http://i.stack.imgur.com/KhLNe.png"" alt=""enter image description here""></p>

<pre><code>&gt; summary(b$lme) # details of underlying lme fit
Linear mixed-effects model fit by maximum likelihood
 Data: data 
   AIC  BIC logLik
  -160 -124     88

Random effects:
 Formula: ~Xr - 1 | g
 Structure: pdIdnot
          Xr1   Xr2   Xr3   Xr4   Xr5   Xr6   Xr7   Xr8
StdDev: 0.812 0.812 0.812 0.812 0.812 0.812 0.812 0.812

 Formula: ~1 + Time | ParticipantID %in% g
 Structure: General positive-definite, Log-Cholesky parametrization
            StdDev  Corr  
(Intercept) 5.68324 (Intr)
Time        0.50739 -0.92 
Residual    0.00691       

Variance function:
 Structure: fixed weights
 Formula: ~invwt 
Fixed effects: list(fixed) 
                   Value Std.Error  DF t-value p-value
X(Intercept)       -2.81     0.729 573   -3.86  0.0001
XTime              -0.15     0.065 573   -2.30  0.0220
Xs(M1)Fx1 -1.60     0.066 573  -24.29  0.0000
 Correlation: 
                   X(Int) XTime  
XTime              -0.920       
Xs(M1)Fx1  0.004  0.000

Standardized Within-Group Residuals:
    Min      Q1     Med      Q3     Max 
-2.3472 -0.0692 -0.0117  0.0305 20.7271 

Number of Observations: 636
Number of Groups: 
                   g ParticipantID %in% g 
                   1                   61 
&gt; summary(b$gam) # gam style summary of fitted model

Family: binomial 
Link function: logit 

Formula:
NEC.binary ~ Time + s(M1)

Parametric coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  -2.8135     0.7289   -3.86  0.00013 ***
Time         -0.1495     0.0651   -2.30  0.02188 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Approximate significance of smooth terms:
               edf Ref.df     F p-value    
s(M1) 4.1    4.1 14913  &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

R-sq.(adj) =  0.0872   
  Scale est. = 4.7815e-05  n = 636
&gt; anova(b$gam) 

Family: binomial 
Link function: logit 

Formula:
NEC.binary ~ Time + s(M1)

Parametric Terms:
    df    F p-value
Time  1 5.28   0.022

Approximate significance of smooth terms:
               edf Ref.df     F p-value
s(M1) 4.1    4.1 14913  &lt;2e-16
&gt; gam.check(b$gam)
</code></pre>

<p><img src=""http://i.stack.imgur.com/IA4hi.png"" alt=""enter image description here""></p>

<p>I suspect I may have messed up something fairly basic since M1 is the most obvious discriminator of the disease state.  It is significant (as it should be) but the correlation is very low. Also, obviously, the model didn't converge (even when I increased iterations from 20->50). And finally the check plots look pretty outrageous</p>

<h2>Question</h2>

<p>Have I made a basic syntax error? Is there some malicious component in my model I'm over looking? Any help would be greatly appreciated. </p>

<h3>Further work</h3>

<p>I would like to add another metabolite (M2) to the model and 2 covariates (Birthweight and Race).  When I add M2 to the model I get an non-convergence error:</p>

<pre><code>&gt; b = gamm(D.binary ~ Time  + s(M1) + s(M2) , 
      random = list(ParticipantID = ~ 1 + Time),  niterPQL=20, correlation = corLin(),
      data = NEC_data, family=binomial(link=""logit""))

 Maximum number of PQL iterations:  20 
iteration 1
iteration 2
Error in lme.formula(fixed = fixed, random = random, data = data, correlation = correlation,  : 
  nlminb problem, convergence error code = 1
  message = false convergence (8)
</code></pre>

<p>Any advice about moving into the multidimensional space would also be appreciated.</p>

<h1>Addition</h1>

<p>I also tried this model with the discrete disease classification (control: 0,1  disease: 2,3) and poisson noise.</p>

<pre><code>&gt; b = gamm(NEC ~ DPP  + s(DSLNT_ug.mL)  , 
+      random = list(ParticipantID = ~ 1 + DPP),  niterPQL=20,
+      data = NEC_data, family=poisson) 

 Maximum number of PQL iterations:  20 
iteration 1
iteration 2
...
iteration 19
iteration 20
Error in solve.default(pdMatrix(a, factor = TRUE)) : 
  system is computationally singular: reciprocal condition number = 3.13906e-19
</code></pre>
"
"0.0726752374667264","0.0484928644968872","156780","<p>I am running a logistic regression with 5 continuous independent variables (IV). The problem is that IV4 when taken alone has a positive correlation with outcome (coeff > 0), and when taken with the other variables has a negative correlation (coeff &lt; 0). I evaluated correlation between IV4 and the other variables, and the results are: 
IV4 vs. IV1 (-0.51), IV4 vs. IV2 (-0.48), IV4 vs. IV3 (0.61) and IV4 vs. IV5 (0.73).</p>

<p>I ran other logistic regressions <em>eliminating one at a time all the other variables</em> to look if one of them was responsible for the sign change, and I noticed that when eliminating IV1, the sign of V4 coefficient became positive.</p>

<p>Thus, it seems that IV1 changes the sign of the coefficient of IV4. 
Is there someone who knows what might be the cause and (possibly) the solution?</p>

<p>Practically, do I have to eliminate the IV4 (or IV1) from the model and explain why?</p>

<p>Thanks a lot for answering</p>

<p>Leonardo Frazzoni, MD</p>
"
"0.0839181358296689","0.0839921051131616","157694","<p>I have a formula (not mine):</p>

<pre><code>NY/A = (Passing Yards - Sack Yards) / (Passes Attempted + Times Sacked)
</code></pre>

<p>and this formula correlates with wins at a correlation coefficient of 0.50. </p>

<p>Edit: I have the data for wins and passing yards, sack yards, passes attempted, and times sacked. NY/A stands for net yards per attempt, and I would like to correlate this with wins at a coefficient of at least 0.52. I can put some weight on times sacked or sack yards. </p>

<p>My goal is to increase the correlation to at least 0.52. How can I do this? Is there a regression I can run in R?</p>
"
"0.15699645640569","0.145910923101986","158366","<p>I'm trying to fit a multiple regression model with pairwise deletion in the context of missing data.  <code>lm()</code> uses listwise deletion, which I'd prefer not to use in my case.  I'd also prefer not to use multiple imputation or FIML.  How can I do multiple regression with pairwise deletion in R?</p>

<p>I have tried the <code>mat.regress()</code> function of the <code>psych</code> package, which fits regression models to correlation/covariance matrices (which can be obtained from pairwise deletion), but the regression model does not appear to include an intercept parameter.</p>

<p>Here's what I've tried (small example):</p>

<pre><code>set.seed(33333)
y &lt;- rnorm(1000)
x1 &lt;- y*2 + rnorm(1000, sd=.2)
x2 &lt;- y*5 + rnorm(1000, sd=.5)

y[sample(1:1000, 10)] &lt;- NA
x1[sample(1:1000, 10)] &lt;- NA
x2[sample(1:1000, 10)] &lt;- NA

mydata &lt;- data.frame(y, x1, x2)
covMatrix &lt;- cov(mydata, use=""pairwise.complete.obs"")

#Listwise Deletion
listwiseDeletion &lt;- lm(y ~ x1 + x2, data=mydata)
observations &lt;- length(listwiseDeletion$na.action) #30 rows deleted due to listwise deletion

coef(listwiseDeletion)
(Intercept)          x1          x2 
0.001995527 0.245372245 0.100001989

#Pairwise Deletion --- but missing intercept
pairwiseDeletion &lt;- mat.regress(y=""y"", x=c(""x1"",""x2""), data=covMatrix, n.obs=observations)
pairwiseDeletion$beta
       y
x1 0.1861277
x2 0.1251995

#Pairwise Deletion --- tried to add intercept, but received error when fitting model
mydata$intercept &lt;- 0
covMatrixWithIntercept &lt;- cov(mydata, use=""pairwise.complete.obs"")

pairwiseDeletionWithIntercept &lt;- mat.regress(y=""y"", x=c(""intercept"",""x1"",""x2""), data=covMatrixWithIntercept, n.obs=observations)
Something is seriously wrong the correlation matrix.
In smc, smcs were set to 1.0
Warning messages:
1: In cov2cor(C) :
  diag(.) had 0 or NA entries; non-finite result is doubtful
2: In cor.smooth(R) :
  I am sorry, there is something seriously wrong with the correlation matrix,
cor.smooth failed to  smooth it because some of the eigen values are NA.  
Are you sure you specified the data correctly?
</code></pre>

<p>So, how can I obtain an intercept parameter using <code>mat.regress</code>, or how can I obtain parameter estimates from pairwise deletion using another method or package in R?  I've seen <a href=""https://stats.stackexchange.com/questions/107597/is-there-a-way-to-use-the-covariance-matrix-to-find-coefficients-for-multiple-re"">matrix calculations</a> to do this, but, ideally, there'd be a package that also outputs regression diagnostics, fit stats, etc.  Also, preferably, the method would be able to fit interaction terms.</p>
"
"0.0419590679148345","0.0419960525565808","159605","<p>I'm new on time series. I'm trying to solve an exercise on the simulation of an ARMA process. </p>

<p>The problem is the following: 
Generate 100 simulations, each with n=60 elements of an ARMA(1,2) process with mean $\mu=1.25$ and parameters $\phi_1=-0.5$, $\theta_1=0.5$, $\theta_2=-0.7$ and $\sigma^2=0.5$
For each simulation estimate the mean and the first two correlation coefficients and find in how many simulations they are contained in the theoretical confidence intervals. </p>

<p>Ok so, I think that for one simulation of the ARMA(1,2) I should do something like: </p>

<pre><code>x &lt;- arima.sim(list(order = c(1,0,2), ar = -0.5 , ma=c(0.5, -0.7)), sd=sqrt(0.5), n = 60)
</code></pre>

<p>And this is one simulation, right? But then, for generate the other 99 simulation what should I do? Can I construct a kind of a loop? </p>

<p>Thank you in advance ! Cheers</p>
"
"0.0726752374667264","0.0727392967453308","159741","<p>I have 100 simulations of an ARMA(1,2) process, created with R is such a way: </p>

<pre><code>M &lt;- replicate(100, arima.sim(list(order=c(1,0,2),ar=-0.5,ma=c(0.5,-0.7)), mean=1.25,sd=sqrt(0.5),n=60))
M &lt;- data.matrix(M)
</code></pre>

<p>Thus each column represents a time series.</p>

<p>Now, my next step is to compute the first two correlation coefficients of each simulation. 
This is the point in which I'm stuck. 
My idea is first do a loop over the columns of M, in order to compute the correlation coefficient for each time series and allocate the result in a matrix. (that should be 3x100) </p>

<p>What I have tryed to do in R is the following:</p>

<pre><code>CorrCoeff&lt;- list()
length(CorrCoeff) &lt;- 300
dim(CorrCoeff) &lt;- c(3,100)
CorrCoeff &lt;- data.matrix(CorrCoeff) #empty matrix that I will fill with the loop 

for(i in 1:ncol(M) #loop over the colums
  { CorrCoeff[,i] &lt;- cbind(acf(M[,i],2)) } 
CorrCoeff
</code></pre>

<p>But unfortunately this code doesn't work. </p>

<p>Then I have tried also: </p>

<pre><code>a &lt;- vector(mode=""numeric"")
for(i in 1:ncol(M))
  { a[i] &lt;- cbind(acf(M[,i],2)) } 
</code></pre>

<p>Here I get the acf for each time series but the output is presented is a strange way and I don't know how to put these results in a matrix. </p>

<p>Can someone tell me where I'm wrong or give me some suggestions? 
Thank you! Cheers</p>
"
"0.10277830647413","0.0857240833122733","160066","<p>I am an R beginner, so sorry if I missing something basic. I did a linear fit with 2 input variables and 1 output. Plots of the output vs each input show a clear positive slope for each. However, the data output shows a negative coefficient for one variable. Can someone help me understand why the plots appear to show positive correlation slope for myoutput vs d3 while the model shows a negative coefficient? Thanks!</p>

<p><em>Input variables</em>: d2, d3</p>

<p><em>Output variable</em>: myoutput</p>

<pre><code>&gt; mydata = read.table(""tdata.csv"", header=TRUE, sep="","")
&gt; lm.fit= lm(myoutput ~ d2 + d3, data = mydata)
&gt; summary(lm.fit)

Call:
lm(formula = myoutput ~ d2 + d3, data = mydata)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.011531 -0.001770  0.000019  0.001800  0.008234 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -7.601e-06  1.335e-04  -0.057    0.955    
d2           1.329e+00  5.776e-02  23.012   &lt;2e-16 ***
d3          -4.931e-01  5.197e-02  -9.488   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.00293 on 486 degrees of freedom
Multiple R-squared:  0.8265,    Adjusted R-squared:  0.8258 
F-statistic:  1158 on 2 and 486 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p><img src=""http://i.stack.imgur.com/N4ATP.jpg"" alt=""myoutput vs. d2""></p>

<p><img src=""http://i.stack.imgur.com/oQq8G.jpg"" alt=""myoutput vs. d3""></p>
"
"0.132686223108569","0.132803178814933","160316","<p>I have a dataset consisting of about 600 observations. Each observation has around 100 attributes. One of the attributes I want to predict. Since the attribute that I want to predict can only have non-negative integer values, I was looking for ways to predict count data and found that there are various options, such as Poisson regression or negative binomial regression.</p>

<p>For my first try I used negative binomial regression in <code>R</code>:</p>

<pre><code>#First load the data into a dataset
dataset &lt;- test_observations[, c(5:8, 54)]

#Create the model
fm_nbin &lt;- glm.nb(NumberOfIncidents ~ ., data = dataset[10:600, ] )
</code></pre>

<p>I then wanted to see how to predicted values look like:</p>

<pre><code>#Create data to test prediction
newdata &lt;- dataset[1:10, ]

#Do the prediction
predict(fm_nbin, newdata, type=""response"")
</code></pre>

<p>Now the problem is the output looks like this:</p>

<pre><code>     1         2         3         4         5         6         7         8         9        10 
0.2247337 0.2642789 0.2205408 0.2161833 0.1794224 0.2081522 0.2412996 0.2074992 0.2213011 0.2100026 
</code></pre>

<p>The problem with this is that I expected that the predicted values are integers, since that is the whole purpose of using a negative binomial regression. What am I missing here?</p>

<p>Furthermore, I would like to evaluate my predictions in terms of mean squared error and mean absolute error, as well as a correlation coefficient. However, I couldn't find a way to get these easily, without doing all the calculations manually. Is there any built-in function for this?</p>
"
"0.151672986506104","0.140963343870344","160675","<p>I'm interested in determining both the slope regression coefficient and plotting regression lines for autocorrelated time-series datasets of rainfall.  Specifically, I'd like to identify the best approach in R that would allow me to visualize the regression line on the original (undifferenced) time-series plot when I need to difference the data to remove stationarity (i.e, when d>0 in an arima model).</p>

<p>As a start, I'm exploring the use of auto.arima (from the forecast package) and sarima (from the astsa package) which can output regression coefficients in the presence of autocorrelation.</p>

<p>For example:</p>

<ol>
<li><p>Using auto.arima. The 'drift' of -5.009 represents the slope (see <a href=""http://robjhyndman.com/hyndsight/arima-trends/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/arima-trends/</a>) </p>

<pre><code>&gt; min.ar &lt;- auto.arima(dec.yr.mmin$min_prcp)
&gt; summary(min.ar)

Series: dec.yr.mmin$min_prcp 
ARIMA(1,1,0) with drift         

Coefficients:
          ar1    drift
      -0.5138  -5.0089
s.e.   0.2465   5.7986

sigma^2 estimated as 949:  log likelihood=-57.82
AIC=121.64   AICc=124.31   BIC=123.34

Training set error measures:
                     ME     RMSE      MAE       MPE     MAPE      MASE       ACF1
Training set -0.9479987 28.52129 23.83494 -2.484233 16.12547 0.7957998 -0.2617352
</code></pre></li>
<li><p>Using sarima to fit the model and output the slope</p>

<pre><code>  &gt; fit.min &lt;- sarima(dec.yr.mmin$min_prcp, 1,1,0,                       reg=dec.yr.mmin$decade)
initial  value 3.542448 
  iter   2 value 3.488927
  iter   3 value 3.386967
  iter   4 value 3.383464
  iter   5 value 3.382408
  iter   6 value 3.382051
  iter   7 value 3.382024
  iter   8 value 3.382020
  iter   9 value 3.381925
  iter   9 value 3.381925
  iter   9 value 3.381925
  final  value 3.381925 
  converged
  initial  value 3.400729 
  iter   2 value 3.399523
  iter   3 value 3.399490
  iter   4 value 3.399488
  iter   4 value 3.399488
  iter   4 value 3.399488
  final  value 3.399488 
  converged
</code></pre></li>
</ol>

<p>3.Output coefficients</p>

<pre><code>      &gt; fit.min$fit$coef
             ar1       xreg 
      -0.5137696 -0.5009045 
</code></pre>

<ol start=""4"">
<li><p>For comparison, this is the output from an OLS regression which may give an incorrect slope due to autocorrelation.</p>

<pre><code>  &gt; m3 &lt;- lm(dec.yr.mmin$min_prcp ~ dec.yr.mmin$decade)
  &gt; summary(m3)

  Call:
  lm(formula = dec.yr.mmin$min_prcp ~ dec.yr.mmin$decade)

  Residuals:
      Min      1Q  Median      3Q     Max 
  -45.504  -8.048   1.892  13.650  38.357 

  Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)   
  (Intercept)        1014.1570   319.9461   3.170  0.00807 **
  dec.yr.mmin$decade   -0.4222     0.1580  -2.672  0.02032 * 
  ---
  Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

  Residual standard error: 23.83 on 12 degrees of freedom
  Multiple R-squared:  0.3731,  Adjusted R-squared:  0.3209 
  F-statistic: 7.142 on 1 and 12 DF,  p-value: 0.02032
</code></pre>

<p>`</p></li>
</ol>

<p>The output from sarima identifies the slope coefficient and also intercept when d=0. When differencing is required (e.g., ARIMA (1,1,0) as output above), sarima only outputs the slope. </p>

<p>My question: when d = 1 or more, what approaches in R would allow me to add/visualize the regression line onto the original undifferenced time-series plot. Is it possible to derive the fitted values of the regression line, or derive intercept values from sarima/auto.arima or other package?</p>

<p>Many thanks in advance for your suggestions.</p>
"
"0.10277830647413","0.102868899974728","160943","<p>Reading this <a href=""http://stats.stackexchange.com/a/78830/67822"">post</a> by @gung brought me to try to reproduce his superb illustrations, and led ultimately to question something I had read or heard, but that I'd like to understand more intuitively: Why is an OLS <code>lm</code> controlling for a correlated variable better (can I say 'better' tentatively?) than a mixed-effects model with different intersects and slopes?</p>

<p>Here's the toy example, again trying to parallel the post quoted above:</p>

<p>First the data:</p>

<pre><code>set.seed(0)    
x1 &lt;- c(rnorm(10,3,1),rnorm(10,5,1),rnorm(10,7,1))
x2 &lt;- rep(c(1:3),each=10)
    y1 &lt;- 2  -0.8 * x1[1:10] + 8 * x2[1:10] +rnorm(10)
    y2 &lt;- 6  -0.8 * x1[11:20] + 8 * x2[11:20] +rnorm(10)
    y3 &lt;- 8  -0.8 * x1[21:30] + 8 * x2[21:30] +rnorm(10)
y &lt;- c(y1, y2, y3)
</code></pre>

<p>And the different models:</p>

<pre><code>library(lme4)

fit1 &lt;- lm(y ~ x1)
fit2 &lt;- lm(y ~ x1 + x2)
fit3 &lt;- lmer(y ~ x1 + (1|x2))
fit4 &lt;- lmer(y ~ x1|x2, REML=F)
</code></pre>

<p>Comparing  Akaike information criterion (AIC) between models:</p>

<pre><code>AIC(fit1, fit2, fit3, fit4)

     df      AIC
     df      AIC
fit1  3 184.5330
fit2  4  97.6568
fit3  4 112.0120
fit4  5 114.8401
</code></pre>

<p>So it seems that the best model is <code>lm(y ~ x1 + x2)</code>, which I guess make sense given the strong correlation between <code>x1</code> and <code>x2</code> <code>cor(x1,x2) [1] 0.8619565</code>.</p>

<p>But the question is, What is the intuition behind this behavior, when the mixed model with varying intercepts and slopes seems to result in coefficients that fit the data beautifully?</p>

<pre><code>coef(lmer(y ~ x1|x2))
$x2

      x1       (Intercept)
1 -1.1595730    11.37746
2 -0.2586303    19.38601
3  0.2829754    24.20038

library(lattice)    
xyplot(y ~ x1, groups = x2, pch=19,
           panel = function(x, y,...) {
             panel.xyplot(x, y,...);
             panel.abline(a=coef(fit4)$x2[1,2], b=coef(fit4)$x2[1,1],lty=2,col='blue');
             panel.abline(a=coef(fit4)$x2[2,2], b=coef(fit4)$x2[2,1],lty=2,col='magenta');
             panel.abline(a=coef(fit4)$x2[3,2], b=coef(fit4)$x2[3,1],lty=2,col='green')
           })
</code></pre>

<p><img src=""http://i.stack.imgur.com/NYweu.png"" alt=""enter image description here""></p>

<p>I do realize that the graphical fit of the bivariate OLS can look pretty good as well:</p>

<pre><code>library(scatterplot3d)
plot1 &lt;- scatterplot3d(x1,x2,y, type='h', pch=16,
              highlight.3d=T)
plot1$plane3d(fit2)
</code></pre>

<p><img src=""http://i.stack.imgur.com/Jt2bz.png"" alt=""enter image description here""></p>

<p>... and I don't know if this invalidates the question. </p>
"
"0.0839181358296689","0.0839921051131616","161797","<p>Say I have some predictors, and I know how they affect some dependent variable:</p>

<pre><code>#predictors
x1&lt;- seq(0,10,0.1)
x2&lt;-runif(101,0,1)
#specify how predictors affect dependent variable y
y&lt;- 15*x1 + 10*x1*x2
#introduce random error
y.err&lt;- rnorm(101,0.01)
y&lt;- y + y.err
</code></pre>

<p>I can then model <code>y</code> as a function of <code>x1</code> and <code>x2</code> like this:</p>

<pre><code>fit&lt;- lm(y ~ x1 + x1*x2)
</code></pre>

<p>which yields this output:</p>

<pre><code>summary(fit)

Call:
lm(formula = y ~ x1 + x1 * x2)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.76646 -0.59886 -0.09115  0.70549  2.85311 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.38875    0.41630   0.934    0.353    
x1          14.93601    0.07409 201.585   &lt;2e-16 ***
x2          -0.74815    0.79518  -0.941    0.349    
x1:x2       10.10469    0.13698  73.768   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 1.025 on 97 degrees of freedom
Multiple R-squared:  0.9997,    Adjusted R-squared:  0.9997 
F-statistic: 1.184e+05 on 3 and 97 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>So, total model $R^2$ is 0.997. </p>

<p>I would like to know what percentage of that $R^2$ value can be attributed to x1, x2, and the interaction of x1 and x2. I am also aware of a previous post on this topic linked <a href=""http://stats.stackexchange.com/questions/60872/how-to-split-r-squared-between-predictor-variables-in-multiple-regression"">here</a>, where the user has an identical question. However, the solution proposed (as I read it) was to run the correlations individually, square them, and they will sum to the full model $R^2$. This is not the case here. </p>
"
"0.0726752374667264","0.0727392967453308","161853","<p>I'm trying on attendance data to find relationships between where people go. For example, say I have information on users A, B, C &amp; D and what kinds of places they've visited in the past few months:</p>

<pre><code>a = c(2,1,3,4)
b = c(1,0,2,5)
c = c(3,2,3,1)
d = c(0,4,1,2)
counts = cbind(a,b,c,d)
row.names(counts) = c(""Concerts"", ""Restaurants"", ""Sporting Events"", ""Movies"")
#                 a b c d
# Concerts        2 1 3 0
# Restaurants     1 0 2 4
# Sporting Events 3 2 3 1
# Movies          4 5 1 2
</code></pre>

<p>My main question is if using the Kendall correlation coefficient is appropriate here (even though the data are counts instead of ranks)? In reality this data is very sparse, which seems like it will also pose a problem. So additionally, does anyone know any methods similar to this for sparse count data?</p>

<p>As an example for the Kendall correlation, this is what I was thinking:</p>

<pre><code>cor(counts[1,], counts[4,], method=""Kendall"")
</code></pre>

<p>Would yield -.33, presumably meaning that there's a relatively weak negative correlation in people going to movies vs. concerts (in this fictitious example).</p>
"
"0.282021346298503","0.294031180013868","162804","<p>When searching for correlations between between a dependent variable and a factor or a combination of factors in a repeated measure design with lme() I noticed that I can encounter two types of results, and I am wondering which is the best way to report each of them in a journal publication. It is not clear to me when I should report the values of the beta coefficient together with the t-test value and p-value, or the beta coefficient with F value and p-value.</p>

<p>Letâ€™s have as a reference the following two models:</p>

<p>MODEL TYPE 1: fixed effects only </p>

<pre><code>lme_Weigth &lt;- lme(Sound_Feature ~ Weight, data = My_Data, random = ~1 | Subject)
summary(lme_Weigth)

lme_Height &lt;- lme(Sound_Feature ~ Height, data = My_Data, random = ~1 | Subject)
summary(lme_Height)
</code></pre>

<p>MODEL TYPE 2: Fixed and interaction effects together</p>

<pre><code>lme_Interaction &lt;- lme(Sound_Feature ~ Weight*Height, data = My_Data, random = ~1 | Subject)

summary(lme_Interaction)  
anova.lme(lme_Interaction, type = ""marginal"").
</code></pre>

<p>RESULTS CASE 1: Applying model type 2 I do not get any significant p-value so there is no interaction effect. Therefore I check
the simplified model type 1, and I get for both Height and Weight significant p-values.</p>

<p>RESULTS CASE 2: Applying model type 2 I get a significant p-value so there is an interaction effect. Therefore I do not check
the simplified model type 1 for the two factors separately. Moreover, in the results of model type 2 I can also see that the fixed effects of both factors are significant.</p>

<p>I am not sure if in presence of an interaction it is correct to report the significant interactions of the separate factors, since I read somewhere that it does not make too much sense. Am I wrong?</p>

<p>My attempt in reporting the results for the two cases is the following. Can you please tell me it I am right?</p>

<p>â€œWe performed a linear mixed effects analysis of the relationship between Sound_Feature and Height and Weight. As fixed effects, we entered Height and Weight (without interaction term) into a first model, and we included the interaction effect into a second model. As random effects, we had intercepts for subjects.â€</p>

<p>RESULTS CASE 1: â€œResults showed that Sound_Feature was linearly related to Height (beta = value, t(df)= value, p &lt; 0.05) and Weight (beta = value, t(df)= value, p &lt; 0.05), but no to their interaction effect.â€</p>

<p>RESULTS CASE 2:  â€œResults showed that Sound_Feature was linearly related to Height (beta = value, F(df)= value, p &lt; 0.05) and Weight (beta = value, F(df)= value, p &lt; 0.05), and to their interaction effect (beta = value, F(df)= value, p &lt; 0.05).â€</p>

<p>Basically I used for reporting the beta value in the 2 cases I use the output of summary(). In the case 1, I report the value of the t-test, still taken from summary. But for case 2 I do not report the t-test, I report the F value as result of anova.lme(lme_Interaction, type = ""marginal"").</p>

<p>Is this the correct way of proceeding in the results reporting?</p>

<p>I give an example of the outputs I get using the two models for the three cases:</p>

<p>RESULTS CASE 1:</p>

<pre><code>&gt; ############### Sound_Level_Peak vs Weight*Height ###############
&gt; 
&gt;
&gt; 
&gt; library(nlme)
&gt; lme_Sound_Level_Peak &lt;- lme(Sound_Level_Peak ~ Weight*Height, data = My_Data1, random = ~1 | Subject)
&gt; 
&gt; summary(lme_Sound_Level_Peak)
Linear mixed-effects model fit by REML
 Data: My_Data1 
       AIC      BIC    logLik
  716.2123 732.4152 -352.1061

Random effects:
 Formula: ~1 | Subject
        (Intercept) Residual
StdDev:    5.470027 4.246533

Fixed effects: Sound_Level_Peak ~ Weight * Height 
                  Value Std.Error DF    t-value p-value
(Intercept)   -7.185833  97.56924 95 -0.0736485  0.9414
Weight         0.993543   1.63151 15  0.6089715  0.5517
Height        -0.076300   0.55955 15 -0.1363592  0.8934
Weight:Height -0.005403   0.00898 15 -0.6017421  0.5563
 Correlation: 
              (Intr) Weight Height
Weight        -0.927              
Height        -0.994  0.886       
Weight:Height  0.951 -0.996 -0.919

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-2.95289464 -0.51041805 -0.06414148  0.48562230  2.95415889 

Number of Observations: 114
Number of Groups: 19 


&gt; anova.lme(lme_Sound_Level_Peak,type = ""marginal"")
              numDF denDF   F-value p-value
(Intercept)       1    95 0.0054241  0.9414
Weight            1    15 0.3708463  0.5517
Height            1    15 0.0185938  0.8934
Weight:Height     1    15 0.3620936  0.5563
&gt; 
&gt; 





&gt; ############### Sound_Level_Peak vs Weight ###############
&gt; 
&gt; library(nlme)
&gt; summary(lme(Sound_Level_Peak ~ Weight, data = My_Data1, random = ~1 | Subject))
Linear mixed-effects model fit by REML
 Data: My_Data1 
       AIC      BIC    logLik
  706.8101 717.6841 -349.4051

Random effects:
 Formula: ~1 | Subject
        (Intercept) Residual
StdDev:    5.717712 4.246533

Fixed effects: Sound_Level_Peak ~ Weight 
                Value Std.Error DF    t-value p-value
(Intercept) -3.393843  6.291036 95 -0.5394728  0.5908
Weight      -0.196214  0.087647 17 -2.2386822  0.0388
 Correlation: 
       (Intr)
Weight -0.976

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-2.90606493 -0.51419643 -0.05659565  0.56770327  3.00098859 

Number of Observations: 114
Number of Groups: 19 
&gt; 
&gt; 
&gt; 
&gt; 
&gt; 
&gt; 
&gt; ############### Sound_Level_Peak vs Height ###############
&gt; 
&gt; library(nlme)
&gt; summary(lme(Sound_Level_Peak ~ Height, data = My_Data1, random = ~1 | Subject))
Linear mixed-effects model fit by REML
 Data: My_Data1 
       AIC      BIC   logLik
  702.9241 713.7981 -347.462

Random effects:
 Formula: ~1 | Subject
        (Intercept) Residual
StdDev:    5.174077 4.246533

Fixed effects: Sound_Level_Peak ~ Height 
               Value Std.Error DF   t-value p-value
(Intercept) 46.36896 20.764187 95  2.233122  0.0279
Height      -0.36643  0.119588 17 -3.064113  0.0070
 Correlation: 
       (Intr)
Height -0.998

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-2.93697776 -0.50963502 -0.06774953  0.50428597  2.97007576 

Number of Observations: 114
Number of Groups: 19 
&gt; 
&gt; 
</code></pre>

<p>So, I will report the results in this way: â€œResults showed that Sound_Level_Peak was linearly related to Height (beta = -0.36643, t(17)= -3.064113, p = 0.007) and Weight (beta = -0.196214, t(17)= -2.2386822, p &lt; 0.0388), but no to their interaction effect.â€</p>

<p>RESULTS CASE 2:</p>

<pre><code>&gt; ############### Centroid vs Weight*Height ###############
&gt; 
&gt; 
&gt; 
&gt; library(nlme)
&gt; lme_Centroid &lt;- lme(Centroid ~ Weight*Height, data = My_Data2, random = ~1 | Subject)
&gt; 
&gt; summary(lme_Centroid)
Linear mixed-effects model fit by REML
 Data: My_Data2 
       AIC      BIC    logLik
  1904.563 1920.766 -946.2817

Random effects:
 Formula: ~1 | Subject
        (Intercept) Residual
StdDev:    1180.301 945.3498

Fixed effects: Centroid ~ Weight * Height 
                  Value Std.Error DF   t-value p-value
(Intercept)   -45019.39 21114.912 95 -2.132113  0.0356
Weight           710.53   353.074 15  2.012414  0.0625
Height           330.61   121.092 15  2.730246  0.0155
Weight:Height     -4.34     1.943 15 -2.233779  0.0411
 Correlation: 
              (Intr) Weight Height
Weight        -0.927              
Height        -0.994  0.886       
Weight:Height  0.951 -0.996 -0.919

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-2.16255520 -0.60084449 -0.02651629  0.54377042  1.92638924 

Number of Observations: 114
Number of Groups: 19 


&gt; anova.lme(lme_Centroid,type = ""marginal"")
              numDF denDF  F-value p-value
(Intercept)       1    95 4.545908  0.0356
Weight            1    15 4.049810  0.0625
Height            1    15 7.454243  0.0155
Weight:Height     1    15 4.989769  0.0411
&gt; 
&gt; 
&gt; 
</code></pre>

<p>So, I will report the results in this way:  â€œResults showed that Centroid was linearly related to the interaction effect of Weight and Height (beta = -4.34, F(1,15)= 4.989769, p = 0.0411), and to Height (beta = 330.61, F(1,15)= 7.454243, p = 0.0155). </p>
"
"0.0726752374667264","0.0484928644968872","168784","<p>I am trying choose best $k$ from the consensus clustering using the Cophenetic Correlation Coefficient (CCC). I tried as follows. The correlation coefficients values are poor, i.e., <code>k=2 (0.2110048)</code>, <code>k=3 (0.1934558)</code>, <code>k=4 (0.175295)</code>. Please suggest whether am following correct method. </p>

<pre><code># consensus clustering
library(Biobase)
data(geneData)
d = geneData
# median center genes
dc = sweep(d, 1, apply(d,1,median))

rcc = ConsensusClusterPlus(dc, maxK=4, reps=100, pItem=0.8, pFeature=1, title=""example"", 
                           distance=""pearson"", innerLinkage=""ward.D"", 
                           finalLinkage=""ward.D"", clusterAlg=""hc"")

# Cophenetic Correlation Coefficient
k2 &lt;- rcc[[2]]$consensusMatrix
d1 &lt;- as.dist(t(k2))
hc &lt;- hclust(d1, ""ward.D"")
d2 &lt;- cophenetic(hc)
cor(d1, d2)
# 0.2110048

k3 &lt;- rcc[[3]]$consensusMatrix
d1 &lt;- as.dist(t(k3))
hc &lt;- hclust(d1, ""ward.D"")
d2 &lt;- cophenetic(hc)
cor(d1, d2)
# 0.1934558

k4 &lt;- rcc[[4]]$consensusMatrix
d1 &lt;- as.dist(t(k4))
hc &lt;- hclust(d1, ""ward.D"")
d2 &lt;- cophenetic(hc)
cor(d1, d2)
# 0.175295
</code></pre>
"
"0.103843395091962","0.103934927410387","169139","<p>I work with a very small questionnaire dataset, in which perception of 10 respondents to the size of, say, an unseen population is quantified: The population is divided into eight sub-populations, and each respondent is asked to guess the minimum and maximum number of people living in each sub-population. Some social characteristics of respondents, that might affect their perception, is collected (both continuous and categorical).</p>

<p>I anticipate that respondents can be grouped into [two] different social groups of believers based on their perception of the population size. Indeed, this grouping of variables might reflect that, overall, the sociological profile of respondents that perceived a large size for this population differs from that of the respondents who had expressed smaller, and more realistic here, guesses.</p>

<p>The variables are:   </p>

<pre><code>ID   Age   Group   Education   Job   Experience   Experience_B
 1    32       L           2     P          6.0              1
 2    31      NL           3     F          0.7              1
 4    35      NL           3     F          4.5              4
 9    31      NL           3     F          4.0              2
13    34      NL           3     F          3.8              3
19    30      NL           2     P          5.0              4
20    35       L           3     P         10.0              8
21    45       L           1     F         26.0              4
22    29      NL           3     F          1.0              2
24    30       L           3     F          4.0              3
</code></pre>

<p>And, the respondents' guess of the population size in each sub-population (a= minimum, b= maximum):</p>

<pre><code>        1a  1b  2a  2b  3a  3b  4a  4b  5a  5b  6a  6b  7a  7b  8a  8b
WAR001  20  40  30  50  50  70  10  20  20  30  50  70  15  30  5   10
WAR002  35  70  20  40  30  60  20  40  5   15  3   10  30  60  30  70
WAR004  8   13  9   12  10  17  3   7   8   11  11  21  7   14  10  20
WAR009  15  30  18  32  25  45  21  40  17  38  12  28  23  40  11  24
WAR013  20  25  25  30  35  40  8   10  10  15  45  50  25  30  0   0
WAR019  13  15  11  15  20  30  3   5   1   3   1   2   3   5   0   0
WAR020  15  30  10  18  30  50  10  15  7   10  10  15  30  35  25  35
WAR021  25  30  15  20  25  30  3   5   15  20  5   7   25  30  0   0
WAR022  7   9   8   13  10  17  4   8   6   11  9   18  6   13  9   14
WAR024  15  30  10  20  20  40  10  20  10  20  10  20  20  40  20  40
</code></pre>

<p>I was thinking of constructing a data matrix to calculate descriptive statistics and the Pearson correlation coefficients among the variables. But not sure about the best approach for grouping variables given the data. Also, I would like to visualize the social profile of the respondents in a radar-like graph, similar to this:
<a href=""http://i.stack.imgur.com/com8O.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/com8O.png"" alt=""enter image description here""></a> </p>

<p>I guess considering my sample size, no modeling approaches could be considered. Anyhow, any suggestion is highly appreciated. Meanwhile, I would like to hear your suggestions about the best approaches available for visualization of this data in R.</p>
"
"0.0938233281301002","0.0939060283031685","171151","<p>I am trying to build a multiple regression model using R. I have a number of predictor variables. I have some basic domain knowledge for which I am trying to build the model. To start with, I included a few predictor variables based on domain knowledge and high correlation coefficients with the response variable, while excluding some other predictors due to multicollinearity. I would like to figure out if I should include some interaction terms. But, due to large number of predictors, I am having a hard time trying to figure out which all interaction terms I should include in the model. Based on what I have read on this site about automated model selection (thanks, @gung et. al), I am trying to avoid using it. </p>
"
"0.1876466562602","0.17842145377602","172226","<p>Let's assume an analytical model predicts an epidemic trend over time, i.e. number of infections over time. We also have a computer simulation results over time to verify the performance of the model. The goal is to prove the simulation results and predicted values of the analytical model (which are both a time series) are statistically close or similar. By similarity I mean the model predicts the values close to what simulation is providing.</p>

<p><strong>Background</strong>:
Researching around this topic, I came across the following posts:</p>

<ol>
<li><p><a href=""http://stackoverflow.com/questions/13835924/similarity-of-trends-in-time-series-analysis"">http://stackoverflow.com/questions/13835924/similarity-of-trends-in-time-series-analysis</a></p></li>
<li><p><a href=""http://stats.stackexchange.com/questions/19103/how-to-statistically-compare-two-time-series"">How to statistically compare two time series?</a></p></li>
</ol>

<p>Both discussions suggest three approaches, where I am interested in two of them basically:</p>

<p>(1). Use of ARIMA; 
 (2). Use of Granger test</p>

<p>For the first suggested solution, this is what has been written there in regards to ARIMA, in (1):</p>

<blockquote>
  <p>Run ARIMA on both data sets. (The basic idea here is to see if the same set of parameters (which make up the ARIMA model) can describe both your temp time series. If you run auto.arima() in forecast (R), then it will select the parameters p,d,q for your data, a great convenience.</p>
</blockquote>

<p>I ran auto.arima on the simulation values and then ran forecast, here are the results:</p>

<pre><code>ARIMA(2,0,0) with zero mean     

Coefficients:
         ar1      ar2
      1.4848  -0.5619
s.e.  0.1876   0.1873

sigma^2 estimated as 121434:  log likelihood=-110.64
AIC=227.27   AICc=229.46   BIC=229.4
</code></pre>

<p>I ran auto.arima on predicted model values and then forecast. This is the result of the predicted model:</p>

<pre><code>ARIMA(2,0,0) with non-zero mean 

Coefficients:
         ar1      ar2  intercept
      1.5170  -0.7996  1478.8843
s.e.  0.1329   0.1412   290.4144

sigma^2 estimated as 85627:  log likelihood=-108.11
AIC=224.21   AICc=228.21   BIC=227.05
</code></pre>

<p><strong>Question 1</strong> What are the values that need to be compared to prove that the two series are similar especially the trend over time?</p>

<p>Regarding the second suggested option, I have read about it and found that Granger test is usually used to see if the values of series <em>A</em> at time <em>t</em> can predict the values of Series <em>B</em> at time <em>t+1</em>. </p>

<p><strong>Question 2</strong> Basically, in my case I want to compare the values of time series A and B at the same time, how this one is relevant to my case then?</p>

<p><strong>Question 3</strong> Is there any available method can be used to prove that the trend of two time-series over time is similar?</p>

<p>FYI. I saw another method which is using Pearson Correlation Coefficient and I could follow the reasoning there. Moreover, verifying analytical models with simulations has been widely used in the literature. see:</p>

<ol>
<li><a href=""http://users.ece.gatech.edu/~jic/tnn05.pdf"" rel=""nofollow"">Spatial-Temporal Modeling of Malware Propagation in Networks Modeling</a></li>
<li><a href=""http://cs.ucf.edu/~czou/research/emailWorm-TDSC.pdf"" rel=""nofollow"">Modeling and Simulation Study of the Propagation and Defense of Internet Email Worm</a></li>
</ol>
"
"0.0593390829096927","0.0593913870916499","173630","<p>I have fitted a model using <code>glm</code>. 
In the <code>summary(model)</code> I get a correlation matrix, but I don't understand how the correlations are calculated and what the interpretation is. 
The documentation says ""The estimated correlations of the estimated coefficients"", but the estimated coefficients are real numbers (one-dimensional) so the correlation does not make sense.  </p>
"
"0.0726752374667264","0.0727392967453308","173828","<p>I have a data set shown below, the 1st, 2nd,3rd column are dependent variable(dv), and 2 independent variables (iv1 &amp; iv2) respectively, I expected the regression coefficient of the ""iv1"" shows a positive value, as there is a positive correlation between dv and iv1. However, The result shows a negative regression coefficient for iv1 (beta_iv1 = -0.55), I am wondering why this happened, I appreciate if anyone can help.</p>

<p>dv    iv1     iv2</p>

<p>1     0.00    7.70<br>
1     2.90    0.00<br>
1     0.00    7.70<br>
1     2.90    0.50<br>
1     0.00    9.50<br>
1     1.50    7.70<br>
1     5.70    0.50<br>
1     7.10    2.30<br>
1     5.70    4.10<br>
1     0.00    4.10<br>
1     4.30    4.10<br>
1     0.00    10.00<br>
1     0.00    4.10<br>
1     2.90    0.50<br>
1     0.00    9.50<br>
0     0.00    9.50<br>
1     0.00    5.90<br>
1     0.00    4.10<br>
1     1.50    5.90<br>
1     5.70    2.30<br>
1     1.50    0.00<br>
0     0.00    10.00<br>
1     5.70    0.00<br>
1     5.70    0.50<br>
1     4.30    2.30<br>
0     0.00    10.00<br>
1     2.90    5.90<br>
1     0.00    5.90<br>
1     0.00    5.90<br>
1     2.90    2.30<br>
1     1.50    2.30<br>
1     2.90    0.50<br>
1     5.70    4.10<br>
1     1.50    0.00<br>
1     0.00    7.70  </p>

<p>I run this using R with package ""logistf"" which overcomes separation problem of logistic regression. The code I run this data set is as below:</p>

<blockquote>
  <p>library(logistf);<br>
      tempT=read.table(fileS);<br>
     fit&lt;-logistf(dv ~ iv1+iv2, data=tempT);</p>
</blockquote>

<p>and the result shows below:  </p>

<pre><code>           coef  se(coef) lower 0.95  upper 0.95     Chisq      p
</code></pre>

<blockquote>
  <p>(Intercept)  9.0086382 5.1741382   1.650380 61.61244068 7.6897111 
  0.005553652<br>
  tempT[, 2]  -0.5509122 0.6567110  -6.013208  1.55280975 0.5490404 0.458710039<br>
  tempT[, 3]  -0.9051062 0.5597601  -6.317335 -0.06328166 4.8315401 0.027943657</p>
</blockquote>

<p>Likelihood ratio test=7.213821 on 2 df, p=0.02713555, n=35</p>
"
"0.125877203744503","0.125988157669742","174057","<p>This is probably an embarrassingly easy question, but where else can I turn to... </p>

<p>I'm trying to put together examples of regression with mixed effects using <code>lmer</code> {lme4}, so that I can present [R] code that automatically downloads toy datasets in Google Drive and run every instance in <a href=""http://stats.stackexchange.com/a/13173/67822"">this blockbuster post</a>. </p>

<p>And starting with the first case (i.e. <code>V1 ~ (1|V2) + V3</code>, where <code>V3</code> is a continuous variable acting as a fixed effect, and <code>V2</code> is <code>Subjects</code>, both trying to account for <code>V1</code>, a continuous DV), I was expecting to retrieve different intercepts for each one of the <code>Subjects</code> and a single slope for all of them. Yet, this was not the case consistently.</p>

<p>I don't want to bore you with the origin or meaning of the datasets below, because I'm sure most of you get the idea without much explaining. So let me show you what I get... If you're so inclined you can just copy and paste in [R]... it should work if you have {lme4} in your Environment:</p>

<h1>Expected Output:</h1>

<pre><code>politeness &lt;- read.csv(""http://www.bodowinter.com/tutorial/politeness_data.csv"")
head(politeness)

  subject   gender scenario  attitude frequency
1      F1      F        1      pol     213.3
2      F1      F        1      inf     204.5
3      F1      F        2      pol     285.1
4      F1      F        2      inf     259.7    


library(lme4)

fit &lt;- lmer(frequency ~ (1|subject) + attitude, data = politeness)

coefficients(fit)
            $subject
               (Intercept) attitudepol
            F1    241.1352   -19.37584
            F2    266.8920   -19.37584
            F3    259.5540   -19.37584
            M3    179.0262   -19.37584
            M4    155.6906   -19.37584
            M7    113.2306   -19.37584
</code></pre>

<h1>Surprising Output:</h1>

<pre><code>library(gsheet)
recall &lt;- read.csv(text = 
    gsheet2text('https://drive.google.com/open?id=1iVDJ_g3MjhxLhyyLHGd4PhYhsYW7Ob0JmaJP8MarWXU',
              format ='csv'))
head(recall)

 Subject Time Emtl_Value Recall_Rate Caffeine_Intake
1     Jim    0   Negative          54              95
2     Jim    0    Neutral          56              86
3     Jim    0   Positive          90             180
4     Jim    1   Negative          26             200

fit &lt;- lmer(Recall_Rate ~ (1|Subject) + Caffeine_Intake, data = recall)

coefficients(fit)
        $Subject
               (Intercept) Caffeine_Intake
        Jason     51.51206        0.013369
        Jim       51.51206        0.013369
        Ron       51.51206        0.013369
        Tina      51.51206        0.013369
        Victor    51.51206        0.013369
</code></pre>

<p>Here is the output of (<code>summary(fit)</code>):</p>

<pre><code>Linear mixed model fit by REML ['lmerMod']
Formula: Recall_Rate ~ (1 | Subject) + Caffeine_Intake
   Data: recall

REML criterion at convergence: 413.9

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-1.54125 -0.98422  0.04967  0.81465  1.83317 

Random effects:
 Groups   Name        Variance Std.Dev.
 Subject  (Intercept)   0.0     0.00   
 Residual             601.2    24.52   
Number of obs: 45, groups:  Subject, 5

Fixed effects:
                Estimate Std. Error t value
(Intercept)     51.51206    5.92408   8.695
Caffeine_Intake  0.01337    0.03792   0.353

Correlation of Fixed Effects:
            (Intr)
Caffen_Intk -0.787
</code></pre>

<h1>Question:</h1>

<p><strong>Why are all the Intercepts for the different subjects the same in the second example? The structure of the datasets and the <code>lmer</code> syntax appear very similar... and the boxplots don't seem to support the result:</strong></p>

<p><a href=""http://i.stack.imgur.com/xXYdS.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xXYdS.png"" alt=""enter image description here""></a></p>

<p>Thank you in advance!</p>
"
"0.119417600797712","0.119522860933439","175770","<p>This question is more of theoretical. I am not sure if this is the right place, but still giving it a try. </p>

<p>I have two variables &mdash; direct cost and indirect cost. When sales persons go for a sales pitch to a customer they know about direct cost that they are going to incur for this service, but they don't know much about indirect cost (they will come to know about it in latter stages). An estimate of indirect cost at this stage will be valuable for sales persons. </p>

<p>I am trying to predict indirect cost as a function of direct cost. I am doing this via a simple linear regression. I plotted scatter plot between direct cost and indirect cost and see a <strong>good linear relationship</strong> between them. I also see that direct cost and indirect cost are <strong>highly corelated</strong> to each other with correlation coefficient as 0.98, so I expected a very good prediction accuracy. But surprisingly, my prediction accuracy is not so good. I have around 200,000 points in my training data and average prediction error on training data is 17 %. Though adjusted R-Square value is 0.97. I am using <code>lm()</code> function from R.       </p>

<p>My question is that in case of simple linear regression, in general, should we expect better prediction accuracy if dependent and independent variables are highly correlated or is it my misconception? If we expect good accuracy, am I missing something here. Please note that I have also tried centering these variables around mean. </p>
"
"0.0951542219543327","0.111111111111111","175996","<p>I have been studying a few simple statistical models for (univariate) time series. From my understanding,</p>

<ul>
<li><p>ARIMA and its siblings are used to model the <em>mean</em> of a time series. Rather than a static measure like <code>mean()</code>, the result is a series estimating the mean.</p></li>
<li><p>ARCH and its brothers are used to model the <em>volatility</em> of a time series. Rather than the usual <code>sd()</code>, the result is a series estimating the variance. </p></li>
</ul>

<h2>Question</h2>

<p>What would be a credible model for the correlation of two time series?</p>

<h2>Notes</h2>

<p>While mean models explore the idea of regressing lagged values of the time series, volatility models (eg. ARCH model) explore the idea of regressing lagged residuals where residuals are the difference of a mean model to its original time series.</p>

<p>In its general sense and for a variety of reasons, ARIMA and ARCH are <em>superior</em> models than rolling windows with <code>mean()</code> (popularly known as moving averages outside statistics world) and <code>sd()</code>.</p>

<p>However, there is no such a thing for the <em>correlation</em> of two time series X and Y to my knowledge.</p>

<p>The closest thing would be rolling a sad, straight window with <code>cor()</code>, Pearson's coefficient function in R, and work around the resulting series.</p>

<h2>A poor solution</h2>

<p>Trying to replicate Pearson's correlation model,</p>

<pre><code>p_(X,Y) = cov(X,Y) / (sd(X) sd(Y))
        = E((X-mean(X))(Y-mean(Y))) / (sd(X) sd(Y)),
</code></pre>

<p>to the time series world, I had the above without the intended success.</p>

<pre><code>library('forecast')
library('fGarch')

X &lt;- 1:200 + rnorm(200, sd=10)
Y &lt;- 50 + (1:200)/100 + rnorm(200, sd=5)

plot(1:200, X, t='l', main=""What would be a resulting ts correlation of X and Y?"")
lines(1:200, Y, t='l', col='blue')

# Mimic Pearson correlation, cov(X,Y)/(sd(X)*sd(Y)).
Xm &lt;- as.vector(X) - as.vector(fitted(Arima(X, order=c(2,0,1))))
Ym &lt;- as.vector(Y) - as.vector(fitted(Arima(Y, order=c(2,0,1))))

Xv &lt;- garchFit(formula=~arma(2,1) + garch(2,1), data=X)@sigma.t
Yv &lt;- garchFit(formula=~arma(2,1) + garch(2,1), data=Y)@sigma.t

correlation &lt;- Xm * Ym / (Xv * Yv)    # this can be forecast

plot(correlation, t='l', col='blue', ylim=c(-2, 2), main='Correlation models')
abline(h=c(-1, 1))
abline(h=cor(X, Y), col='red', lwd=5)

# Correlation rolling window of size 10.
df &lt;- data.frame(X, Y)
crw &lt;- rep(NA, 10)
for (i in 11:nrow(df))
  crw &lt;- c(crw, cor(df[(i-10):i, 1], df[(i-10):i, 2]))

lines(crw, col='darkgreen', lwd=5)

legend('topright',
  c('pearson mimic', 'static cor()', 'rolling cor() like moving averages'),
  col=c('blue', 'red', 'darkgreen'), lwd=c(1, 5, 5))
</code></pre>
"
"0.0726752374667264","0.0727392967453308","178014","<p>As the title states, I want to generate a time series that follows an AR(1) proces and thus has a certain overall level of autocorrelation.</p>

<p>I'm using the <code>arima.sim</code> function (which is implemented as standard in R).</p>

<p>I thought that for example the following command:</p>

<pre><code>arima.sim(model=list(Ar=-0.5),n=400)
</code></pre>

<p>would generate a time series of length 400 and an autocorrelation of -0.5.
However, I've noticed that the values you can give to the <code>Ar</code> parameter are not limited to [-1; 1]. For example, you could input <code>10 000</code>.</p>

<p>Can anyone explain to me what the <code>Ar</code> parameter actually represents? Because it apparently is not a correlation coefficient...</p>

<p>After reading on the internet it seems to me that there's not a lot of information there for people who want to simulate time series data using a model as opposed to people who want to fit data to a model...</p>
"
"0.121125395777877","0.145478593490662","179115","<p>i checked the presence of spatial autocorrelation in the abundance of species living in 10x10m plots, 60 samples per plot.
I did vegan's correlog function with default bins (12 distance classes ~ each encompassing about 1.30m in my plot). I detrended my data for x,y coordinates and calculated the correlograms with spearman and pearson correlation procedures, adjusted for multiple testing with FDR.</p>

<p>I have difficulties interpreting the outcomes.</p>

<p>Here are two plots, same site, pearson vs. spearman. Each correlogram represents a species (sequence is the same), filled boxes represent a lag bin at which autocorrelation is significant.</p>

<p><a href=""http://i.stack.imgur.com/LBOki.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/LBOki.jpg"" alt=""Pearson""></a></p>

<p><a href=""http://i.stack.imgur.com/cZjYO.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/cZjYO.jpg"" alt=""Spearman""></a></p>

<ol>
<li><p>I understand that Spearman is more relaxed in terms of data normality requirements. Most of my species data seem to follow Poisson/negative binomial distributions though, when modelling species outcomes, so i wonder if Spearman really is the best thing here, since a) the correlograms are similar, but the p-values are very different between the two.</p></li>
<li><p>Given the really low Mantel coefficients, even if there is significant autocorrelation at some scales, does it affect my hypothesis test, since the correlation seems really weak? How do i interpret a significant non-random spatial process with a mantel coefficient of 0.08?!</p></li>
<li><p>Here is another plot, another site, same species.
<a href=""http://i.stack.imgur.com/vIQqF.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/vIQqF.jpg"" alt=""site2""></a></p></li>
</ol>

<p>Here the spatial organisation is less pronounced, and partially inverted (species 1, top left). Now, if i want to test for significant differences between the two sites, i guess i need to correct for reduced degree of freedoms, but are there any tests that takes into account different degrees of spatial autocorrelation for the two populations to check?</p>

<p>Â´Thank you very much!</p>
"
"0.0938233281301002","0.0939060283031685","181489","<p>I have to estimate a number of regressions where a lot of autcorrelation is present. Now, for historical reasons, this autocorrelation is resolved using an iterative Prais-Winsten estimation (a modification of the Cochraneâ€“Orcutt estimation). I have found some R-code which performs this procedure:</p>

<pre><code>    prais.winsten.lm &lt;- function(mod){
    X &lt;- model.matrix(mod)
    y &lt;- model.response(model.frame(mod))
    e &lt;- residuals(mod)
 n &lt;- length(e)
 names &lt;- colnames(X)
 rho &lt;- sum(e[1:(n-1)]*e[2:n])/sum(e^2)
 y &lt;- c(y[1] * (1 - rho^2)^0.5, y[2:n] - rho * y[1:(n-1)])
 X &lt;- rbind(X[1,] * (1 - rho^2)^0.5, X[2:n,] - rho * X[1:(n-1),])
 mod &lt;- lm(y ~ X - 1)
 result &lt;- list()
 result$coefficients &lt;- coef(mod)
     names(result$coefficients) &lt;- names
 summary &lt;- summary(mod, corr = F)
 result$cov &lt;- (summary$sigma^2) * summary$cov.unscaled
     dimnames(result$cov) &lt;- list(names, names)
 result$sigma &lt;- summary$sigma
 result$rho &lt;- rho
 class(result) &lt;- 'prais.winsten'
 result
 }
</code></pre>

<p>Now, this code works fine when all the other regressors are exogeneous. But, in my case, a part of X is endogeneous turning the standard ols regression performed in the above code not usable.</p>

<p>I was thinking about modifying the above code into the following:</p>

<pre><code>  prais.winsten.plm &lt;- function(mod){
  X &lt;- model.matrix(mod,component=""projected"")
  Z &lt;- model.matrix(mod,component=""instruments"")
  y &lt;- model.response(model.frame(mod))
  e &lt;- residuals(mod)
  n &lt;- length(e)
  names &lt;- colnames(X)
  rho &lt;- sum(e[1:(n-1)]*e[2:n])/sum(e^2)
  y &lt;- c(y[1] * (1 - rho^2)^0.5, y[2:n] - rho * y[1:(n-1)])
  X &lt;- rbind(X[1,] * (1 - rho^2)^0.5, X[2:n,] - rho * X[1:(n-1),])
  Z &lt;- rbind(Z[1,] * (1 - rho^2)^0.5, Z[2:n,] - rho * Z[1:(n-1),])
  mod &lt;- ivreg(y ~ X -1|Z)
  result &lt;- list()
  result$coefficients &lt;- coef(mod)
      names(result$coefficients) &lt;- names
  summary &lt;- summary(mod, corr = F)
  cov &lt;- (summary$sigma^2) * summary$cov.unscaled
  result$se&lt;- sqrt(diag(cov))
      dimnames(cov) &lt;- list(names, names)
      result$sigma &lt;- summary$sigma
      result$rho &lt;- rho
  class(result) &lt;- 'prais.winsten'
  result
  }
</code></pre>

<p>I have however no idea if such an approach is correct, especially since I found no similar ways in dealing with this problem.</p>
"
"0.145350474933453","0.13335537736644","181665","<p>My question relates to calculating the uncertainty associated with the estimation of slopes in a varying intercept, varying slope hierarchical model.</p>

<p>I would like to calculate the effect of a treatment in different districts. If I ran a simple linear regression in which there was no pooling at district level, my model would look something like:</p>

<pre><code>    fm1 &lt;- lm(response ~ treatment*district)
</code></pre>

<p>I could then find the effect of treatment in district j by summing the coefficient for treatment with that of the interaction term treatment:districtj, and the standard error for that estimate could be found with:</p>

<pre><code>    sqrt(vcov(fm1)[2,2] + vcov(fm1)[4,4] + 2*vcov(fm1)[2,4])
</code></pre>

<p>I get stuck when moving to a multilevel model with random slopes along the lines of:</p>

<pre><code>    fm2 &lt;- lmer(response ~ treatment + (1 + treatment | district))
</code></pre>

<p>I can again find the effect of treatment in district j easily by summing the coefficient for treatment with the random slope estimate for district j. I would like to account for the uncertainty of both coefficients and I'm not sure how to do that. I can use the arm package to extract the standard errors associated with the slope estimates for each district and I know the standard error of the treatment estimate but I don't know how to estimate their correlation.  </p>

<p>It seems like this should be an easy question to answer but I haven't been able to find it. Hoping someone can point me in the right direction.</p>
"
"0.0419590679148345","0.0419960525565808","183655","<p>I know how to calculate the correlation coefficient for a continuous data, I need help calculating correlation coefficient Ï for a count data as shown below</p>

<pre><code>           Response     Questions   Freq
            Yes         A           5
            No          A           25
            Yes         B           12
            No          B           34
            Yes         C           9
            No          C           43
</code></pre>
"
"0.0938233281301002","0.0939060283031685","184341","<p>I am using the Deming function provided by Terry T. on <a href=""http://www.mail-archive.com/r-help@r-project.org/msg85070.html"">this archived r-help thread</a>.  I am comparing two methods, so I have data that look like this:</p>

<pre><code>y  x     stdy   stdx
1  1.2   0.23   0.67
2  1.8   0.05   0.89
4  7.5   1.13   0.44
... ...  ...   ...
</code></pre>

<p>I have done my Deming regression (also called ""total least squares regression"") and I get a slope and intercept. I would like to get a correlation coefficient so I've start calculating the $R^2$. I have manually entered the formula: </p>

<pre><code>R2 &lt;- function(coef,i,x,y,sdty){
    predy    &lt;- (coef*x)+i
    stdyl    &lt;- sum((y-predy)^2)   ### The calculated std like if it was a lm (SSres)
    Reelstdy &lt;- sum(stdy)          ### the real stdy from the data  (SSres real)
    disty    &lt;- sum((y-mean(y))^2) ### SS tot
    R2       &lt;- 1-(stdyl/disty)    ### R2 formula
    R2avecstdyconnu &lt;- 1-(Reelstdy/disty) ### R2 with the known stdy
    return(data.frame(R2, R2avecstdyconnu, stdy, Reelstdy))
}
</code></pre>

<p>This formula works and gives me output.</p>

<ul>
<li>Which of the two $R^2$s makes more sense? (I personally think of both of them as kind of biased.)  </li>
<li>Is there a way to get a correlation coefficient from a total least squared regression?</li>
</ul>

<p>OUTPUT FROM THE DEMING REGRESSION:</p>

<pre><code>Call:
deming(x = Data$DS, y = Data$DM, xstd = Data$SES, ystd = Data$SEM,     dfbeta = T)

               Coef  se(coef)         z            p
Intercept 0.3874572 0.2249302 3.1004680 2.806415e-10
Slope     1.2546922 0.1140142 0.8450883 4.549709e-02

   Scale= 0.7906686 
&gt; 
</code></pre>
"
"0.19247955013414","0.192649210414002","184654","<p>There is a prior question <a href=""http://stats.stackexchange.com/questions/72468/simulating-data-to-fit-a-mediation-model"">here</a>, but it doesn't quite cover what I need.</p>

<p>I am trying to simulate a scenario with 3 variables in a mediation setup and with measurement error. All variables must be standardized (mean â‰ˆ 0, sd â‰ˆ 1).</p>

<p><a href=""http://i.stack.imgur.com/HqtgW.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/HqtgW.png"" alt=""enter image description here""></a></p>

<p>The arrows from nowhere are residuals/errors, round corners = latent variables, rectangles = manifest (observed) variables.</p>

<p>Normally when simulating data with measurement error, I do the following:</p>

<pre><code>n = 10e4
x_measure_beta = .8
x_true = rnorm(n)
x_orb = x_true * x_measure_beta + rnorm(n) * get_error(x_measure_beta)
d = data.frame(x_true, x_orb)
</code></pre>

<p><code>get_error()</code> is:</p>

<pre><code>get_error = function(x) {
  x_sq = x^2
  remain_var = 1 - x_sq
  error_beta = remain_var %&gt;% sqrt

  return(error_beta)
}
</code></pre>

<p>In other words, get error takes a (standardized) beta, squares it to calculate the proportion of variance explained (R2), then subjects that from 1 to get the remaining variance, then takes the square root to get back to a beta coefficient.</p>

<p>Note that the variance of the predictor betas sum to 1:</p>

<pre><code>&gt; c(x_measure_beta, get_error(x_measure_beta))^2 %&gt;% sum %&gt;% sqrt
[1] 1
</code></pre>

<p>In the above case, the output is as expected:</p>

<pre><code>&gt; cor(d)
          x_true     x_orb
x_true 1.0000000 0.7993681
x_orb  0.7993681 1.0000000
&gt; sapply(d, sd)
   x_true     x_orb 
0.9994189 0.9987206 
</code></pre>

<p>However, in the mediation scenario, this doesn't work. First some settings:</p>

<pre><code># mediation scenario ------------------------------------------------------
library(stringr)

n = 10e4

#mediation
med_xym = .5
beta_xy_total = .8

#measurement error -- score x true score beta
beta_xT = .9
beta_mT = .9
beta_yT = .9

#betas
beta_xy = (beta_xy_total^2 * (1-med_xym)) %&gt;% sqrt
beta_xm = (beta_xy_total^2 * med_xym) %&gt;% sqrt %&gt;% sqrt
beta_my = beta_xm
beta_ey = get_error(beta_xy_total)
</code></pre>

<p>So, the total effect of x on y is .80, and it is 50% mediated by m. For measurement error, I use a score x true score of .90.</p>

<p>Betas are then: xâ†’y is half of the .80, which I calculated by first converting to proportion variance, multiply by .50 (<code>1-med_xym</code>) because mediation is 50%, and then square root to get the beta, .57.</p>

<p>I stipulate that paths xâ†’m = mâ†’y, thus betas xâ†’m and mâ†’y are the square root of the indirect beta xâ†’mâ†’y (.57) giving them both .75. This is because the link goes thru both of them to get to y.</p>

<p>The residual for y is then the square of the total xâ†’y path, subtracted from 1, and then square rooted, .60.</p>

<p>Based on my understanding, the number should thus be:</p>

<p><a href=""http://i.stack.imgur.com/oCwuZ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/oCwuZ.png"" alt=""enter image description here""></a></p>

<p>Then generate:</p>

<h1>generate</h1>

<pre><code>#generate
x_true = rnorm(n)
x_orb = x_true * beta_xT + rnorm(n) * get_error(beta_xT)

m_true = x_true * beta_xm + rnorm(n) * get_error(beta_xm)
m_orb = m_true * beta_mT + rnorm(n) * get_error(beta_mT)

y_true = x_true * beta_xy + m_true * beta_my + rnorm(n) * beta_ey
y_orb = y_true * beta_yT + rnorm(n) * get_error(beta_yT)

d_t = data.frame(x_true, m_true, y_true)
</code></pre>

<p>However, result is incorrect:</p>

<pre><code>&gt; cor(d_t) %&gt;% round(2)
       x_true m_true y_true
x_true   1.00   0.75   0.82
m_true   0.75   1.00   0.86
y_true   0.82   0.86   1.00
&gt; sapply(d_t, sd)
  x_true   m_true   y_true 
0.998325 1.000613 1.371431 
</code></pre>

<p>The correlation is a bit too large (should be .80). It is not a coincidence because I have tried with larger samples. Upon examining the variables, I see that the sd of the y is too large. This baffled me because betas seem to be right in terms of variance:</p>

<pre><code>&gt; c(beta_xy, beta_xm*beta_my)^2 %&gt;% sum %&gt;% sqrt
[1] 0.8
&gt; c(beta_xy, beta_xm*beta_my, beta_ey)^2 %&gt;% sum
[1] 1
</code></pre>

<p>Thinking it may be due to the non-zero correlation between x and m, I tried with some random variables:</p>

<pre><code># how does sd increase when adding uncorrelated variables ------------------------------
&gt; t = rnorm(n) + rnorm(n)
&gt; sd(t)
[1] 1.411864
&gt; t = rnorm(n) + rnorm(n) + rnorm(n)
&gt; sd(t)
[1] 1.732708
&gt; t = rnorm(n) + rnorm(n) + rnorm(n) + rnorm(n)
&gt; sd(t)
[1] 2.000496
</code></pre>

<p>We see that for random variables, the effect of summing them is that the new sd is the square root of the sum of the former, i.e. sqrt(1+1)=1.41, sqrt(1+1+1)=1.73 etc.</p>

<p>However, what happens with correlated variables:</p>

<pre><code># how does sd increase when adding correlated variables -------------------
#same as above but with correlated variables
library(MASS)

#simulate
mat_length = 5
r_mat = rep(.5, mat_length*mat_length) %&gt;% matrix(nrow=mat_length, ncol=mat_length)
diag(r_mat) = 1
t = mvrnorm(n = n, mu = rep(0, mat_length), Sigma = r_mat)
t2 = t[, 1] + t[, 2]
t3 = t[, 1] + t[, 2] + t[, 3]
t4 = t[, 1] + t[, 2] + t[, 3] + t[, 4]

&gt; data.frame(t, t2, t3, t4) %&gt;% sapply(sd)
       X1        X2        X3        X4        X5        t2        t3        t4 
0.9960690 0.9998767 1.0001114 0.9996555 0.9979987 1.7251341 2.4412936 3.1531223 
</code></pre>

<p>They have higher variances than before (1.41 vs. 1.73, 1.71 vs. 2.44, 2 vs. 2.44). The intuitive explanation is that because they correlate, cases that are extreme in the same direction get more extreme while cases that are extreme in opposite directions are reduced. Thus, more inequality among cases is created by the summing across variables.</p>

<hr>

<p>How do I simulate a mediation scenario as specified above? I'd like to do it manually so that I can understand what is going on, not use software that deals with the complexity for me.</p>
"
"0.0938233281301002","0.0939060283031685","185646","<p>I have a dataset of about n = 100,000 observations and p = 247 predictors with one binomial dependent variable (values are 0, 1)</p>

<p>I run the following code in R:</p>

<pre><code>cvfit.retro = cv.glmnet(retro.x, y=as.factor(retro.y), family='binomial')
</code></pre>

<p>where
retro.x is the matrix of 247 predictors
retro.y is the vector of the binomial variable</p>

<p>The idea behind running the code was to reduce the number of variables to a more manageable number for further analysis for their predictive power in relation to the binomial dependent variable.  </p>

<p>However, the cvfit.retro output at a relatively high lambda of exp(-6),</p>

<pre><code>coef(cvfit.retro, s=exp(-6))
</code></pre>

<p>shows highly correlated variables being INCLUDED in the model.  For example, there are two variables called v708 and v709 that have non-zero coefficients in the CV.GLMNET output and these two variables have a correlation of 0.9995 using the cor() function.  These variables are obviously powerful predictors in the model but one of them should have been zeroed out in the LASSO in CV.GLMNET given that alpha defaults to 1 (LASSO) in cv.glmnet </p>

<p>Am I missing something here?  Can someone explain how I can fix this issue?  Obviously, I could just eliminate one of the correlated variables manually but that kind of partially defeats the purpose of LASSO.  Any advice is appreciated.  Thank you!</p>
"
"0.0726752374667264","0.0727392967453308","186324","<p>This is the part of my exam preparation. Similar tasks will be provided in my exam paper with extremely limited amount of time. So I need to learn techniques for solving these tasks quickly.</p>

<p>Here is given table showing correlation coefficients for given variables:
<a href=""http://i.stack.imgur.com/hYXu4.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/hYXu4.png"" alt=""Table of correlation coefficients""></a></p>

<p>So I need to undertake this simple calculation:</p>

<p><a href=""http://i.stack.imgur.com/YSMwL.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YSMwL.png"" alt=""What I need to calculate""></a></p>

<ol>
<li>Regarding (a), do I need to calculate the <em>expectation value</em> (?) of (1+ 2*(0.06+-0.11+0.28+1)+3)? And then run calculation? </li>
<li>Regarding (b), could I just easily put the command cov() with the given numbers to calculate it?</li>
<li>Doing c, how can I implement it in R? Can I load the whole table in a data frame and calculate all the things I need?</li>
</ol>

<p>Thank you very much in advance.</p>

<p>UPD: </p>

<blockquote>
  <p>X, Y, Z: standardised to <em>0</em> mean and <em>1</em> standard deviation.</p>
  
  <p>Mean of W is <em>10</em> and standard deviation of W is <em>5</em>.</p>
</blockquote>
"
"0.0419590679148345","0.0419960525565808","188753","<p>I'm attempting to predict vegetation productivity based on climatic and land use variables (the latter are categorical). I found that there is a multicollinearity problem between the predictors (especially land use) as seen from the Variance Inflation Factor (VIF of the Ordinary Least Squares Regression). </p>

<p>Although my knowledge of lasso regression is basic, I assume lasso regression might solve the multicollinearity problem and also select variables that are driving the system. I appreciate an R code for estimating the standardized beta coefficients for the predictors or approaches on how to proceed.</p>

<pre><code>Variable           Coeff.  Std Coeff.  VIF    Std Error    t      P  Value 
Constant          -0.228   0            0      0.086       -2.644  0.008  
Precipitation      &lt;.001   0.151       2.688   &lt;.001        8.541  0.0  
Solar Rad          0.002   0.343       2.836   &lt;.001        18.939 &lt;.001  
Temp              -0.116  -1.604       28.12   0.004       -28.11  0.0  
Water Stress       0.881   0.391       2.352   0.037        23.7   &lt;.001  
Vapor Pressure     0.135   1.382       30.49   0.006        23.259 0.0    
  1               -0.103   -0.109      52.086  0.074       -1.398  0.162    
  2               -0.14    -0.048      6.49    0.079       -1.761  0.078   
  3               -0.11    -0.048      10.007  0.077       -1.42   0.156    
  4               -0.104   -0.234      236.288 0.073       -1.416  0.157    
  5               -0.097   -0.242      285.244 0.073       -1.331  0.183    
  6               -0.104   -0.09       35.067  0.074       -1.406  0.16    
  8               -0.119   -0.261      221.361 0.073       -1.629  0.103 
ELEVATION          &lt;.001   -0.115      3.917   &lt;.001       -5.381  &lt;.001
Condition Number: 59.833 
Mean of Correlation Matrix: 0.221 1st    
Eigenvalue divided by m: 0.328
</code></pre>
"
"0.0726752374667264","0.0727392967453308","188835","<p>I work a lot with noisy time traces with usually 2.5e6 data points and sampling frequency ~ MHz. I usually apply some ""traditional"" digital signal processing utilities like low/high-pass filters to extract interesting frequency components. Or create spectrograms/scalograms to analyze the evolution of different frequency bands. Usually they are not stationary signals, coming from discharge pulses. But often they are at least locally stationary.</p>

<p>But more and more I realize that in the end I need to perform some averaging and the end goal is usually time evolution of some statistical property, like coherence (something like frequency dependent correlation coefficient) or covariance of some properties.</p>

<p>I'm wondering whether I should therefore ditch my usual DSP style (mostly in Python) and go for the statistical style from the beginning in R. I'm quite lured by the huge number of packages in CRAN and the possibility of not writing so much boiler-plate code. </p>

<p>However, all the different complicated names of various models and methods make it seem quite impenetrable to me as an outsider so possibly I'm expecting miracles because I don't fully understand all the limitations of the models and such. So I seek guidance here.</p>
"
"0.205721255836448","0.214138691479874","189933","<p>I am seeking advice on how to effectively eliminate autocorrelation from a linear mixed model. My experimental design and explanation of fixed and random factors can be found here from an earlier question I asked: </p>

<p><a href=""http://stats.stackexchange.com/questions/188929/crossed-fixed-effects-model-specification-including-nesting-and-repeated-measure"">Crossed fixed effects model specification including nesting and repeated measures using glmm in R</a></p>

<p>I have treated day as numeric even though I only have four sampling time points (so I could treat it as a categorical predictor as well). Aside: Although four sample points is very few, I donâ€™t think that this is the root of the problem as this same dataset is giving me this residual autocorrelation issues using a different response variable that has 24 time points.</p>

<p>My issue is that I have tried a number of different autocorrelation structures and canâ€™t seem to achieve the random, non-significant residuals needed to confirm a lack of autocorrelation. I am using the function <code>lme</code> in the R package <code>nlme</code> to deal with autocorrelation. </p>

<p>I have tried the various autocorrelation classes  with variations to form</p>

<p>1) <code>corAR1</code> (autoregressive process of order 1).</p>

<p>2) <code>corARMA</code> (autoregressive moving average process)</p>

<p>3) <code>corCAR1</code> (continuous autoregressive process)</p>

<p>4) <code>corGaus</code> (Gaussian spatial correlation)</p>

<p>With form varying in the following ways with these different autocorrelation classes:</p>

<pre><code>form=~1
form=~1| TankNumb/RecruitID2
form=~Day| TankNumb/RecruitID2
</code></pre>

<p>If we look at a model without the time factor ""Day"" added, the ACF and PACF plots look like this. </p>

<pre><code>lme4_lognormal_notime&lt;-lmer(Arealog~Temperature*Culture+(1|TankNumb/RecruitID2), data=growthSR_noNA)

acf(residuals(lme4_lognormal_notime, retype=""normalized""))
pacf(residuals(lme4_lognormal_notime, retype=""normalized""))
</code></pre>

<p><a href=""http://i.stack.imgur.com/xVdrb.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xVdrb.jpg"" alt=""enter image description here""></a></p>

<p>Also, if I look at the residuals of the model without â€œDayâ€ included, I do not see any strong pattern in the residuals that would make me think there is a temporal autocorrelation problem.</p>

<pre><code>plot(residuals(lme4_lognormal_Ben_notime, retype=""normalized"")~growthSR_noNA$Day)
</code></pre>

<p><a href=""http://i.stack.imgur.com/wU1ZC.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/wU1ZC.jpg"" alt=""enter image description here""></a></p>

<p>Now for two different models with autocorrelation structure to hopefully eliminate autocorrelation:</p>

<pre><code>nlme_lognormal_mult_cor&lt;-lme(Arealog~Temperature*Culture*Day, random=~1|TankNumb/RecruitID2,correlation=corAR1(form=~1), data=growthSR_noNA)
</code></pre>

<p><a href=""http://i.stack.imgur.com/Oe3et.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Oe3et.jpg"" alt=""enter image description here""></a></p>

<pre><code>nlme_lognormal_mult_cortime&lt;-lme(Arealog~Temperature*Culture*Day, random=~1|TankNumb/RecruitID2,correlation=corAR1(form=~Day|TankNumb/RecruitID2), data=growthSR_noNA)
</code></pre>

<p><a href=""http://i.stack.imgur.com/pUgA1.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pUgA1.jpg"" alt=""enter image description here""></a></p>

<pre><code>ARMA_nlme_lognormal_mult_cor&lt;-lme(Arealog~Temperature*Culture*Day, random=~1|TankNumb/RecruitID2,correlation=corARMA(form=~1, p=0, q=1), data=growthSR_noNA)
</code></pre>

<p><a href=""http://i.stack.imgur.com/6TMeL.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6TMeL.jpg"" alt=""enter image description here""></a></p>

<p>The AIC suggests that the simplest correlation structure is the best. </p>

<pre><code>AIC(nlme_lognormal_mult,nlme_lognormal_mult_cor, nlme_lognormal_mult_cortime,ARMA_nlme_lognormal_Ben_mult_cor)

                               df      AIC
nlme_lognormal_mult              15 1233.997
nlme_lognormal_mult_cor          16 1184.389
nlme_lognormal_mult_cortime      16 1235.997
ARMA_nlme_lognormal_Ben_mult_cor 16 1198.451
</code></pre>

<p>As I mentioned above, I have tried a number of different <code>cor</code> functions (the four listed above) and different <code>form</code> specifications. They all end up with ACF/PCF plots like the last two models with a first lag at below 0.2 in the ACF plot and a PCF plot with the first three lags around 0.10.</p>

<p>I have also read a number of sites describing how to specify corARMA models based on diagnosing the ACF plots and have tried a number of variations of p and q parameters. </p>

<p>Questions: </p>

<ol>
<li>Does anyone have some advice on which type of correlation structure that might elimate this autocorrelation problem based on the patterns in my ACF/PCF plots? Should I be diagnosing based on a model with or without Day included?</li>
</ol>

<p>2.Is there ever an acceptable level of autocorrelation? 
This post (<a href=""http://stats.stackexchange.com/questions/80823/do-autocorrelated-residual-patterns-remain-even-in-models-with-appropriate-corre"">Do autocorrelated residual patterns remain even in models with appropriate correlation structures, &amp; how to select the best models?</a>) states that small amounts of autocorrelation probably won't impact the model coefficients very much. ""The estimate is slightly larger than zero so will have negligible effect on the model fit and hence you might wish to leave it in the model if there is a strong a priori reason to assume residual autocorrelation."" Potentially there is some autocorrelation that is not being caused by temporal autocorrelation, like outliers? Is there a cut-off, for example, autocorrelation below 0.1? I have extremely small 95% confidence intervals, so it doesn't take a lot of autocorrelation in my models to be significantly too much.</p>

<p>Any advice is appreciated! </p>
"
"0.0726752374667264","0.0727392967453308","190586","<p>I am using cross correlation to demonstrate a potential link between two time series (ext &amp; co). Both series are strongly autocorrelated, so it is difficult to assess the dependence between the two series. For a quick preliminary analysis, the cross correlation shows a clear (somehow delayed) link between the two time series, although it might spurious. <a href=""http://i.stack.imgur.com/eHUnj.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/eHUnj.jpg"" alt=""CCF""></a>. Prewhitening seems to be the best option; I will prewhiten my x variable by fitting an ARIMA process and then use the coefficients to filter my variable y. My question is if I should estimate the coefficients of the ARIMA process (for example using <code>auto.arima</code>) using my series x or by using the residuals of the OLS regression of x on y.</p>
"
"0.133237935355665","0.145478593490662","191937","<p>I want to compare <em>observed</em> bivariate (Pearson's $\rho$ and Spearman's $\rho$) correlations coefficients with what would be expected from random data.</p>

<p>Assume that we measure, say, 36, cases across <em>very</em> many variables (1000).
(I know this is odd, it's called <a href=""https://en.wikipedia.org/wiki/Q_methodology"" rel=""nofollow"">Q methodology</a>.
Assume further that <em>each of the variables</em> is (strictly) normally distributed  <em>across the cases</em>.
(Again, very odd, but true because people as <em>people-variables</em> rank order <em>item-cases</em> under a normal distribution.)</p>

<p>So, <em>if people sorted randomly</em>, we should get:</p>

<pre><code>m &lt;- sapply(X = 1:1000, FUN = function(x) rnorm(36))
</code></pre>

<p>Now -- because this is Q methodology -- we correlate <em>all people-variables</em>:</p>

<pre><code>cors &lt;- cor(x = m, method = ""pearson"")
</code></pre>

<p>Then we try to plot that, and superimpose the <em>distribution</em> of Pearson's correlation coefficient in random data, which should actually be quite close to the observed correlations in our fake data:</p>

<pre><code>library(ggplot2)
cor.data &lt;- cors[upper.tri(cors, diag = FALSE)]  # we're only interested in one of the off-diagonals, otherwise there'd be duplicates
cor.data &lt;- as.data.frame(cor.data)  # that's how ggplot likes it
colnames(cor.data) &lt;- ""pearson""
g &lt;- ggplot(data = cor.data, mapping = aes(x = pearson))
g &lt;- g + xlim(-1,1)  # actual limits of pearsons r
g &lt;- g + geom_histogram(mapping = aes(y = ..density..))
g &lt;- g + stat_function(fun = dt, colour = ""red"", args = list(df = 36-1))
g
</code></pre>

<p>This gives:</p>

<p><a href=""http://i.stack.imgur.com/4l5q0.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4l5q0.png"" alt=""density plot""></a></p>

<p>The superimposed curve is clearly wrong.
(Also notice that while odd, the y-axis densities are <em>actually correct</em>: because the x-values are so small, this is how the area sums to one).</p>

<p>I remember (vaguely) that the t-distribution is relevant in this context, but I can't wrap my head around how to parametrize it properly.
In particular, are the degrees of freedom given by the number of correlations (1000^2/2-500), or the number of observations on which these correlations are based (36)?</p>

<p>Either way, the superimposed curve in the above is clearly wrong.</p>

<p>I'm also confused because, the probability distribution of Pearson's r would need to be bounded (there are <em>no</em> values beyond (-) 1) -- but the t-distribution is <em>not</em> bounded.</p>

<p>Which distribution describes Pearson's $\rho$ in this case?</p>

<hr>

<p>Bonus:</p>

<p>The above data are actually idealized: in my real Q-study, people-variables actually have very few columns under a normal distribution to sort their item-cases into, like so:</p>

<p><a href=""http://i.stack.imgur.com/A4FUM.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/A4FUM.jpg"" alt=""q-sort""></a></p>

<p>In effect, people-variables are actually <em>rank-ordering</em> item-cases, so Pearson's is not applicable.
As a rough-and-dirty fix, I've opted for Spearman's $\rho$, instead.
Is the probability distribution the same for Spearman's $\rho$?</p>

<hr>

<p><strong>Update</strong>: If anyone is interested, here is the R code to implement @amoeba's fantastic response below:</p>

<pre class=""lang-r prettyprint-override""><code>library(ggplot2)
cor.data &lt;- cors[upper.tri(cors, diag = FALSE)]  # we're only interested in one of the off-diagonals, otherwise there'd be duplicates
cor.data &lt;- as.data.frame(cor.data)  # that's how ggplot likes it
summary(cor.data)
colnames(cor.data) &lt;- ""pearson""
pearson.p &lt;- function(r, n) {
  pofr &lt;- ((1-r^2)^((n-4)/2))/beta(a = 1/2, b = (n-2)/2)
  return(pofr)
}
g &lt;- NULL
g &lt;- ggplot(data = cor.data, mapping = aes(x = pearson))
g &lt;- g + xlim(-1,1)  # actual limits of pearsons r
g &lt;- g + geom_histogram(mapping = aes(y = ..density..))
g &lt;- g + stat_function(fun = pearson.p, colour = ""red"", args = list(n = nrow(m)))
g
</code></pre>

<p>Crucial are the <code>pearson.p</code> function and the last ggplot2 addition.</p>

<p>Here's the outcome; matches perfectly, as one would expect:</p>

<p><a href=""http://i.stack.imgur.com/fsMAY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fsMAY.png"" alt=""enter image description here""></a></p>
"
"0.157346504680629","0.167984210226323","192130","<p>I have some data <em>with more variables than observations</em>, that I'd like to subject to a principal components analysis.
For didactic reasons (to give an intuition for factor retention criteria under parallel analysis), I am here interested in the <em>distribution</em> of the residual correlations.</p>

<p>Let this be my data (sampled from real data):
</p>

<pre><code>data &lt;- c(1,-1,-3,-1,1,1,-2,-2,1,3,-3,0,2,4,0,0,-1,0,-1,-4,-2,3,-2,2,0,4,-3,-1,0,2,2,-4,0,3,0,1,-2,-1,-3,2,-1,4,-4,0,3,2,-3,-2,4,-1,3,0,1,0,-3,2,1,-2,1,-1,1,1,-4,3,2,0,-2,0,0,0,-1,0,-3,3,-4,2,0,-1,0,0,1,1,0,-3,1,-3,4,-2,0,-1,-4,-1,-2,2,2,-2,0,1,0,2,-1,3,3,-1,4,0,-2,1,-4,0,-3,-2,-2,-1,-1,-3,1,3,1,-3,2,-2,2,3,0,-1,-4,4,-1,1,0,0,3,2,-1,0,0,0,2,-2,4,1,0,1,-1,-2,-3,-1,1,-2,-2,-1,0,-1,-4,1,2,2,0,0,1,3,4,0,-4,-1,4,0,-2,3,0,3,-3,0,2,1,2,-3,1,0,-3,0,-4,1,1,2,0,3,0,-1,1,-1,2,-2,1,3,0,0,-3,-3,-4,-1,4,-2,3,2,-2,-1,-1,2,0,1,0,0,-2,4,1,4,0,0,1,-1,-4,-1,2,0,-3,0,3,-1,1,-1,-4,4,-3,-2,1,-2,0,-3,3,-1,-2,2,0,0,-2,3,2,0,1,2,1,-2,-2,1,-3,1,-4,0,3,3,-1,-1,3,1,0,2,2,-2,-4,-3,0,-1,2,0,2,0,-2,-1,0,-1,1,0,0,4,-3,4,0,4,-1,-2,3,-4,0,1,-2,-2,0,-1,1,0,-4,-2,-3,0,-1,1,-3,2,0,0,-1,3,0,2,2,-3,3,2,4,-1,1,1,-1,3,-2,-2,1,0,-3,-2,0,-1,0,-4,4,-3,3,1,2,2,-2,0,0,0,-1,0,1,-1,-4,3,-3,2,1,1,4,0,-1,2,-2,-1,0,2,0,3,-3,-4,0,-1,0,-3,2,2,-1,-1,0,2,-4,0,0,1,-2,1,3,0,-3,4,-2,4,-1,1,3,-2,1,1,-1,1,-4,-1,-2,1,-3,-3,2,2,-4,-3,-1,-2,0,2,2,1,0,1,3,0,0,-2,4,1,-2,-1,0,0,4,3,0,0,-1,3,2,2,1,4,1,3,-3,3,-2,-1,0,0,0,-1,1,0,2,-1,-4,-2,-4,1,-1,4,0,-2,-3,3,0,1,-2,-1,0,2,-3,0,2,2,-4,-2,-2,2,-1,0,1,1,-3,-3,4,1,1,3,3,0,-4,-1,-3,-2,3,1,4,2,-2,0,0,0,0,0,0,-1,-1,-1,0,0,-1,4,-1,2,-2,-2,3,-1,-4,-2,1,-3,-2,2,2,4,-4,-1,0,-1,1,3,1,1,-3,0,-3,2,0,0,1,0,0,3,4,2,0,1,1,0,-4,1,4,0,0,-3,2,-3,-1,3,0,0,3,-1,-4,-1,-1,2,3,-1,-2,-2,0,2,-3,-2,1,0,-2,1,4,0,-1,-2,0,0,-3,-1,0,-3,-2,0,3,-4,0,1,0,3,1,-2,-1,1,-1,3,4,-2,-4,2,-3,2,2,-1,1,1,0,2,-3,0,-2,-2,0,0,-3,0,1,3,-1,-1,2,0,1,0,-1,3,-4,-3,-2,3,-2,2,4,0,-4,4,-1,2,1,2,0,1,-1,1,3,3,-2,3,0,1,-3,4,2,-2,0,-4,1,-1,0,2,-1,2,0,0,-4,0,4,-1,-2,1,-3,1,0,2,-1,-1,-3,0,-2,1,-1,0,-1,0,0,3,-4,-3,0,1,-3,-2,4,-4,1,2,4,0,-2,-2,-1,0,-1,1,3,0,-3,0,-1,1,2,2,1,2,-2,3,-4,1,0,-3,-1,-2,-1,-2,4,1,0,2,3,-2,-3,0,-1,3,0,0,1,4,-4,0,2,-3,0,0,-2,-1,1,-1,3,2,2,1,-2,1,-2,0,-3,1,-4,0,1,3,-1,-3,0,-2,0,-1,1,0,-2,0,0,-1,-1,-4,0,4,-3,1,-1,2,2,3,4,2,3,2,4,3,-2,4,0,3,-4,-3,3,-1,-1,-3,2,-2,1,2,1,2,-4,-1,-2,-1,-3,2,1,1,-2,0,-1,1,0,0,0,0,0,0,-3,4,-4,3,-1,0,-3)
data &lt;- c(data, 2,3,-1,0,-2,3,1,-1,1,1,2,-1,1,-4,0,-1,0,2,2,-3,1,-2,0,-2,0,4,0,-2,0,-2,4,-3,0,-2,0,-1,-4,0,3,-1,-3,1,2,4,0,1,0,-1,-2,-1,3,-3,0,1,3,-4,2,-2,0,1,-1,2,2,0,1,-4,0,-3,0,4,-2,-3,-1,3,-2,-2,3,2,2,2,3,1,1,-4,1,-2,2,0,1,1,0,-3,0,0,-1,-1,-1,-1,0,0,4,-1,-2,-2,-1,-2,0,-3,-1,4,-1,-2,0,4,-3,0,0,-1,3,-4,1,0,0,1,0,2,1,-4,3,3,2,2,2,1,0,-3,1,-1,3,-2,0,0,2,-4,0,0,2,2,0,0,-1,-1,1,-2,3,-4,-1,1,-1,-3,2,-2,4,-3,0,1,3,0,4,1,1,-3,-2,-2,2,-2,0,-2,1,-3,-3,0,2,-4,0,4,1,0,0,4,3,-4,3,0,-1,2,-1,1,1,-3,0,-2,0,-1,2,1,-1,3,-1,0,2,-4,1,0,3,-2,-1,0,2,1,-3,0,-4,3,-1,0,0,-3,-2,-2,1,3,-1,0,2,-3,2,0,4,-1,-2,4,1,-1,1,0,3,0,-1,-1,2,-3,-1,-2,0,-2,-2,0,-3,3,0,0,3,-4,-1,0,2,1,-2,-1,1,-4,4,-3,2,1,0,4,1,2,1,1,-2,-1,-3,-1,1,0,3,4,0,-2,-4,-2,-2,0,0,1,4,-4,0,2,3,-3,2,2,-1,0,-1,2,1,-1,3,0,1,-3,0,-2,4,-1,-1,-2,-2,-3,1,3,0,-1,-4,4,0,0,3,-1,2,-2,-1,0,-3,2,-3,3,1,-4,1,0,1,0,1,2,0,0,2,2,0,-3,-2,1,1,-3,-1,-1,2,0,-1,2,0,0,0,2,4,-4,-2,0,-2,0,1,3,3,-4,1,-1,3,-3,0,1,-1,-2,4,-4,-3,-3,2,1,-1,-1,-2,4,1,0,-2,-2,1,3,-1,0,4,-4,0,0,0,3,3,-1,0,-3,2,0,-1,1,-2,2,2,0,1,3,3,-3,-2,-1,0,-3,-1,-2,2,-4,-2,3,-2,4,0,0,2,-1,-1,0,2,0,0,2,1,-4,4,-1,1,0,1,1,0,-3,1,-2,1,-3,-1,-1,1,-2,1,-1,2,-2,-3,2,-4,-1,0,3,3,3,0,-4,0,4,-3,0,4,-2,2,2,0,0,0,0,1,-1,1,-2,1,-3,-1,-1,1,-3,-1,2,-1,-2,-2,2,-4,-1,1,2,1,-3,3,0,0,4,-2,0,1,-4,0,3,0,4,2,3,0,0,0,3,4,-3,4,-3,-2,-3,3,-2,2,0,2,3,0,0,2,2,1,-4,0,0,0,1,-1,1,1,-4,-1,-2,1,0,0,-1,-1,-2,-1,-3,1,0,3,-3,1,-1,3,2,-1,-4,-2,3,4,-1,0,-2,4,0,0,-1,1,2,1,2,2,-4,-1,-2,1,0,0,0,-2,-3,0,4,3,0,-1,0,3,-4,1,1,-1,0,-3,2,4,2,3,0,2,-2,-3,-1,1,2,-4,0,1,-3,1,-2,-1,-2,0,-1,0,-2,0,-2,2,-2,-3,0,2,-4,0,-1,1,0,-3,1,-3,1,0,2,4,-4,-2,-1,1,-2,0,1,3,0,3,0,0,-1,-1,3,4,-1,2,-1,1,-1,-3,0,2,-1,-2,2,4,-1,-4,3,-2,3,0,3,-1,-3,-2,0,2,0,-4,-3,4,-2,2,1,1,1,0,1,0,0,0,0,3,-3,-2,-1,1,-1,1,3,-1,-1,-4,1,0,0,2,-2,-3,-4,0,-3,4,1,0,1,0,-1,3,-2,0,-2,0,4,2,2,2,0,1,-2,-4,1,0,-4,0,4,-1,-3,-3,2,-3,0,1,2,2,-2,-1,4,-1,3,-1,3,0,-2,0,0,1,-2,1,2,-1,0,3,-2,-1,-1,0,0,3,0,0,1,2,-2,-2,2,0,1,3,2,4,-3,0,-1,1,-3,-4,0,3,-4,-2,0,2,-3,1,-1,1,-1,4,-4,-3,-3,-1,0,-1,-2,0,1,-1,1)
data &lt;- c(data, -2,0,0,1,3,2,4,-2,-3,-1,0,0,3,0,2,-1,-2,2,2,0,1,-4,4,3,1,0,2,-4,0,0,-2,-3,-1,1,0,-2,-1,3,1,-2,-1,3,2,-4,-1,-1,2,4,0,3,0,-3,2,-2,1,1,1,4,0,-3,0,0,4,-2,-2,0,1,-4,4,0,0,-4,-2,1,-2,2,2,3,-1,-3,-1,0,0,3,0,2,-1,-3,1,-3,1,-1,2,3,-1,0,1,0,4,-2,-2,0,0,-2,3,-1,-1,3,2,0,-2,4,-1,-1,0,-4,-1,0,1,0,-3,1,1,-4,2,-3,0,-3,1,3,2,2,1)
data &lt;- matrix(data = data, nrow = 36)
</code></pre>

<p>(sorry about the clunky input).</p>

<p>Now, let's look at the <em>initial</em> correlation and then the residual correlations.
(I'm doing this here with <code>psych::principal</code> for the sake of convenience, but I've also calculated the residuals by hand with <code>prcomp</code>, with same results).</p>

<pre class=""lang-r prettyprint-override""><code>cor &lt;- cor(data, method=""pearson"")  # figure out initial cors
library(psych)
pca &lt;- principal(r = data, nfactors = 8, residuals = TRUE, rotate = ""none"")  # I know 8 factors is ridiculous, but that's the didactiv point I'm trying to make.
#&gt; Warning in cor.smooth(r): Matrix was not positive definite, smoothing was
#&gt; done
#&gt; In factor.scores, the correlation matrix is singular, an approximation is used
#&gt; Warning in cor.smooth(r): Matrix was not positive definite, smoothing was
#&gt; done

# Calculate residuals ===
cor &lt;- as.matrix(cor)  # just to be sure
loa &lt;- pca$loadings  # take just the loas
res &lt;- NULL
res &lt;- array(data = NA, dim = c(nrow(cor), ncol(cor), ncol(loa)), dimnames = list(cor = NULL, cor = NULL, PC = 1:ncol(loa)))  # this is what the residuals array should look lile
for (i in 1:ncol(loa)) {
  if (i == 1) {
    res[,, i] &lt;- cor - loa[, i] %*% t(loa[, i])
  } else {
    res[,, i] &lt;- res[,, i-1] - loa[, i] %*% t(loa[, i])
  }
}
# we now have an array with var, var, PC as dimensions

# Take only upper triangle (without diagonal)
take.tri &lt;- function(cors) {  # make this little helper function
  cors &lt;- cors[upper.tri(x = cors, diag = FALSE)]
  return(cors)
}
res.df &lt;- apply(X = res, MARGIN = 3, FUN = take.tri)

# add the original correlation as 0th PC =======
res.df &lt;- cbind(`0` = take.tri(cor), res.df)

# make plot =====
library(reshape2)
library(ggplot2)
res.df &lt;- melt(data = res.df)
colnames(res.df) &lt;- c(""Obs"", ""PC"", ""Cor"")
g &lt;- NULL
g &lt;- ggplot(data = res.df, mapping = aes(x = Cor, color = PC, group = PC))
g &lt;- g + geom_freqpoly(binwidth = 0.05) 
g &lt;- g + xlim(-1,1)
g
#&gt; Warning: Removed 18 rows containing missing values (geom_path).
</code></pre>

<p></p>

<p><a href=""http://i.stack.imgur.com/Jh3zS.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Jh3zS.png"" alt=""above output residuals""></a></p>

<p><a href=""http://i.stack.imgur.com/KwjBe.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/KwjBe.png"" alt=""real deal residuals""></a></p>

<p>The above plots shows the distribution of correlation coefficients (denoted as PC 0), as well as the residual correlations from the first 8 principal component, all in one frequency polygon plot.</p>

<p>The smoother plot uses the full dataset, the rough one just the above sample (can't share the full dataset).</p>

<p>I get why the distribution becomes more <em>leptopkurtic</em> (aka: steep) as more components are extracted; that's the whole point of PCA (and this illustration): the remaining correlation matrices approximate a singular matrix with all zeros (and 1s on the diagonal).</p>

<p>I also understand why the original correlations are <em>asymmetric</em> -- given my kind of data (<a href=""https://en.wikipedia.org/wiki/Q_methodology"" rel=""nofollow"">Q-sorts</a>), that frequently happens.</p>

<p><strong>What I don't understand is why the asymmetry seems to disappear <em>completely</em> after the first PC is extracted</strong>.
Shouldn't the asymmetry dissipate slowly as more PCs are extracted?</p>

<p>My questions are:</p>

<ul>
<li>Is this to be expected?</li>
<li>Is this in the logic of PCA, or a computational artefact?</li>
<li>Is this approach meaningful to illustrate parallel analysis / why you <em>shouldn't overretain</em> components?</li>
</ul>

<p>Also, as a <strong>bonus</strong>, I'd be really curious what the <em>expected</em> distribution of correlation coefficients from random data would be, but that's <a href=""http://stats.stackexchange.com/questions/191937/what-is-the-probability-distribution-for-pearsons-r-correlation-coefficient-fro"">another question</a>.</p>
"
"0.0938233281301002","0.0939060283031685","194768","<p>I am trying to understand how to interpret the results I get from LDA.</p>

<p>Running from the iris dataset in R, I can see the discriminant coefficients are in the model and then I can plot the model to see the clustering.  However none of this appears interpretable to me.</p>

<pre><code>library(MASS)
mdl&lt;-lda(Species~.,data=iris)
mdl
</code></pre>

<p>Mdl Output Below</p>

<pre><code>    Call:
lda(Species ~ ., data = iris)

Prior probabilities of groups:
    setosa versicolor  virginica 
 0.3333333  0.3333333  0.3333333 

Group means:
           Sepal.Length Sepal.Width Petal.Length Petal.Width
setosa            5.006       3.428        1.462       0.246
versicolor        5.936       2.770        4.260       1.326
virginica         6.588       2.974        5.552       2.026

Coefficients of linear discriminants:
                    LD1         LD2
Sepal.Length  0.8293776  0.02410215
Sepal.Width   1.5344731  2.16452123
Petal.Length -2.2012117 -0.93192121
Petal.Width  -2.8104603  2.83918785

Proportion of trace:
   LD1    LD2  
0.9912 0.0088 
</code></pre>

<p><strong>What I would like to be able to say is if sepal.length is y and petal.width is x then this flower will be z.</strong>  Clearly I wish to apply this to other datasets.  Alternatively I could get that from a tree structure.</p>

<p><strong>As a secondary question, how do you know how well the model fits?</strong> Looking at the correlations in this dataset, it appears to me that Petal.Length &amp; Petal.Width are strongly correlated so I would think the appropriate thing to do would be to drop one of them.  I know I can look at the accuracy on a test dataset, but is there anything here that tells me in the model how good this model is, an equivalent R^2?</p>

<p><strong>Finally, why are their only 2 discriminant variables given that there are 3 variables?</strong></p>

<p>I've seen some questions on this but never giving a full answer as to if and how this is possible to interpret.  To me the value in these models is being able to say, if you do x then you get y, you can then take that knowledge and apply it to the appropriate business context.  If this is not possible then where is the value in it?</p>
"
"0.15699645640569","0.157134840263677","195443","<p>I am looking at two time series, from 01/01/2000 to the present: <br></p>

<ul>
<li>The <a href=""https://research.stlouisfed.org/fred2/series/NAPMNOI/"" rel=""nofollow"" title=""ISM Manufacturing: New Orders Index"">ISM Manufacturing: New Orders Index</a>, only available seasonally adjusted</li>
<li>The manufacturing industry unemployment rate, only available unadjusted (<a href=""https://research.stlouisfed.org/fred2/series/LNU04032232"" rel=""nofollow"">https://research.stlouisfed.org/fred2/series/LNU04032232</a>)</li>
</ul>

<p>I was <em>hoping</em> to construct a multivariate ts model, and use the <strong>New Orders Index</strong> to forecast the <strong>manufacturing industry unemployment rate</strong>. However, am I correct in assuming it is not 'ideal' to use seasonally adjusted data to predict another time series? Because doesn't SA cause (ideally) all the seasonal time series structure to be removed from the data?</p>

<h3>EDIT:</h3>

<p>Sorry, it just now hit me to link to the data I was using by putting it on Google Drive. It's in .csv files, for easy viewing with any program.</p>

<ul>
<li>Manufacturing new orders index data, in <strong>OrdersIndex.csv</strong><br><a href=""https://drive.google.com/file/d/0B2Y54SySHrVwZXczR1N4LXZMdXc/view?usp=sharing"" rel=""nofollow"">https://drive.google.com/file/d/0B2Y54SySHrVwZXczR1N4LXZMdXc/view?usp=sharing</a></li>
<li>Manufacturing industry unemployment rate, in <strong>Unem.csv</strong>
<br><a href=""https://drive.google.com/file/d/0B2Y54SySHrVweFVpRjJFanAwQmc/view?usp=sharing"" rel=""nofollow"">https://drive.google.com/file/d/0B2Y54SySHrVweFVpRjJFanAwQmc/view?usp=sharing</a></li>
</ul>

<p>Below is the New Orders Index time series, with the dashed line indicating the mean of 54.61. It looks fairly stationary to me; a decent spike in 2008, but definitely reverts to the mean.</p>

<pre><code>&gt; plot.ts(OrdersIndex[,2])
&gt; mean(OrdersIndex[,2])
[1] 54.60829
&gt; abline(h=c(54.61), lty=2)
&gt; 
</code></pre>

<p><a href=""http://i.stack.imgur.com/C61sm.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/C61sm.png"" alt=""New Orders Index""></a></p>

<p>The ACF and PACF of the series are below. ACF displays dampened sine-wave behavior, PACF has a sharp cut-off after lag 1. This suggests an AR(1) model, as the ACF's slow dying off (at lags > 1) is due to the auto correlation at lag 1.</p>

<pre><code>&gt; Acf(OrdersIndex[,2], plot=T)   #the Acf() function is part of 'forecast' package
&gt; Acf(OrdersIndex[,2], plot=T, type=c('partial'))
&gt;
</code></pre>

<p><a href=""http://i.stack.imgur.com/Dg2Es.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Dg2Es.png"" alt=""ACF plot""></a>
<a href=""http://i.stack.imgur.com/0PqBR.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/0PqBR.png"" alt=""PACF plot""></a></p>

<p>After running an arima(1,0,0) model with a mean, the ACF and PACF of the residuals do not show significant spikes at any lags.</p>

<pre><code>&gt; OrdersIndex100 &lt;- arima(OrdersIndex[,2], order=c(1,0,0))
&gt; OrdersIndex100

Call:
arima(x = OrdersIndex[, 2], order = c(1, 0, 0))

Coefficients:
         ar1  intercept
      0.8738    54.6979
s.e.  0.0341     1.9399

sigma^2 estimated as 12.39:  log likelihood = -517.44,  aic = 1040.88
&gt;
</code></pre>

<p>Running an Ljung-Box test on the residuals indicates there is not any time series structure left in the data.</p>

<pre><code>&gt; LBQPlot(OrdersIndex100$residuals, k=1)   # LBQPlot() is part of 'FitAR' package
&gt;
</code></pre>

<p><a href=""http://i.stack.imgur.com/xXQKc.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xXQKc.png"" alt=""Ljung-Box Test""></a></p>

<h3>Conclusion</h3>

<p>The conclusion I arrive at is that the seasonally adjusting done to the data by the ISM (Institute of Supply Management) effectively removed all the seasonality from the data. So, this SA data would be less useful in modeling than non-SA data (this is assuming that I would be using this data series as the Input, and the unemployment data series as the Output). Is this a valid conclusion? You all see any glaring problems with my analysis?</p>
"
"0.118678165819385","0.103934927410387","197024","<p>In moran scatter plot, I checked</p>

<ol>
<li><p>Slope of <code>lm((spatial lagged variable) ~ (variable))</code>: moran coefficient,</p></li>
<li><p>Slope of <code>lm((spatial lagged standardized variable) ~ (standardized
variable))</code>: moran coefficient, and</p></li>
<li><p>Slope of <code>lm((standardized spatial lagged variable) ~ (standardized
variable))</code>: correlation coefficient.</p></li>
</ol>

<p>However, I think Anselin (1999) remark case [3] is moran coefficient. Is
it true?</p>

<pre><code>####################
library(spdep)
data(baltimore)
# head(baltimore)
# plot(baltimore$X, baltimore$Y)
nb &lt;- knn2nb(knearneigh(cbind(baltimore$X, baltimore$Y), k=4))
listW &lt;- nb2listw(nb, style=""W"")
W &lt;- listw2mat(listW)

y &lt;- baltimore$PRICE    # as variable
Wy &lt;- W %*% y       # as spatially lagged variable
Z_y &lt;- scale(y)     # as standardizedã€€variable
Z_Wy &lt;- scale(Wy)       # as standardized spatially lagged variable
WZ_y &lt;- W %*% Z_y       # as spatially lagged standardized variable

## Correration coefficient
cor(y, Wy)
## (Global) Moran's I
moran.test(y, listW, randomisation=F, alternative=""two.sided"")

par(mfrow=c(1,3))
## Moran scatterplot
# Case [1]: 
# lm((spatially lagged variable) ~ (variable))
# equal to moran.plot(y, listW)
plot(y, Wy, xlab=""variable"", ylab=""spatially lagged variable"",
    main=paste(""[1] slope of line:"", round(coef(lm(Wy~y))[2], 3)))
abline(lm(Wy~y))
abline(v=mean(y), lty=2); abline(h=mean(Wy), lty=2)

# Case [2]: 
# lm((spatially lagged standardized variable) ~ (standardized variable)) 
plot(Z_y, WZ_y, xlab=""standardizedã€€variable"", 
ylab=""spatially lagged standardized variable"", 
main=paste(""[2] slope of line:"", round(coef(lm(WZ_y~Z_y))[2], 3)))
abline(lm(WZ_y~Z_y))
abline(v=mean(Z_y), lty=2); abline(h=mean(WZ_y), lty=2)

# Case [3]: 
# lm((standardized spatially lagged variable) ~ (standardized variable))
plot(Z_y, Z_Wy, xlab=""standardizedã€€variable"", 
ylab=""standardized spatially lagged variable"",
main=paste(""[3] slope of line:"", round(coef(lm(Z_Wy~Z_y))[2], 3)))
abline(lm(Z_Wy~Z_y))
abline(v=mean(Z_y), lty=2); abline(h=mean(Z_Wy), lty=2)

par(mfrow=c(1,1))
####################
</code></pre>

<p>Anselin, L. (1999) Interactive techniques and exploratory spatial data
analysis. In: Longley P.A.</p>

<p>Goodchild M.F., Maguire D.J., Rhind D.W. (eds) Geographic information system: Principles, techniques, management and applications, 253â€“266, Wiley, New York.</p>
"
"0.245039746552799","0.245255735793986","197435","<p>This is my first time posting. I hope I've included an appropriate amount of info. I have many questions, but try to highlight the obstacles I've been facing.</p>

<p>I am trying to run three GLMMs in R (3.2.2) on three separate response variables (various measures of sociality between pairs of animals), but with the same set of fixed effects and interactions. Two of the response variables are integers, but the other is a continuous numeric. The response variables are:</p>

<ol>
<li>traveling together- out of all the times I followed my subject, how many of those times were the other individuals present (<strong>Travel</strong>)</li>
<li>proximity- out of all the instantaneous scans to see who was within 5m of my subject, how often was that other individual within 5m (<strong>Within_Five</strong>) </li>
<li>touching- out of all the hours/minutes I watched my subject how many minutes spent touching (<strong>total_touch</strong>)</li>
</ol>

<p>The fixed effects are features of the pair, such as age difference (<strong>Age_Diff</strong>) and whether the partner is the <strong>mother</strong>, <strong>brother</strong>, or <strong>cousin</strong>. I want to have age of the subject (<strong>subject_age</strong>) as an interaction as the social behavior may change with age.</p>

<p>Given Subjects (a subset of the population that I observed) and Partners (anyone in the population who they may be interacting with, including ) are repeated, I am treating both as random effects, e.g. (1 | Partner). </p>

<pre><code>&gt; str(dat)
'data.frame':   954 obs. of  29 variables:
 $ Pair          : Factor w/ 954 levels ""Abrams_Barron"",..: 159 268 269 378     700 334 601 920 179 75 ...
 $ Subject       : Factor w/ 18 levels ""Abrams"",""Barron"",..: 3 6 6 8 14 7 12 18 4 2 ...
  $ Partner       : Factor w/ 54 levels ""Abrams"",""Barron"",..: 54 3 4 7 11 16    18 19 21 23 ...
 $ mother        : Factor w/ 2 levels ""NoMom"",""Mom"": 1 1 1 1 1 1 1 1 1 1 ...
 $ brother       : Factor w/ 2 levels ""NoBro"",""bro"": 2 2 2 2 2 2 2 2 2 2 ...
 $ cousin        : Factor w/ 2 levels ""NoCuz"",""cuz"": 2 1 1 1 1 1 1 1 1 1 ...
 $ subject_age   : num  14.3 17.7 17.7 18.7 19.7 ...
 $ partner_age   : num  8.67 41.69 31.69 12.39 24.68 ...
 $ Age_Diff      : num  -5.59 24.01 14.01 -6.29 5 ...
 $ Total_Scans   : int  314 309 309 313 289 314 310 321 305 283 ...
 $ Total_Hours   : num  43.2 44.3 44.3 45.1 40.9 ...
 $ Total_Minutes : num  2593 2656 2656 2707 2456 ...
 $ Total_Follows : int  45 46 46 47 43 48 46 48 46 45 ...
 $ Within_Five   : int  9 13 12 20 26 10 6 4 30 9 ...
 $ total_touch   : num  0 0 1.77 6.19 31.07 ...
 $ Touch_Rate_Min: num  0 0 0.000667 0.002287 0.012649 ...
 $ Touch_Rate    : num  0 0 0.04 0.137 0.759 ...
 $ Travel        : int  25 28 27 24 30 21 23 5 26 17 ...
 $ Travel_Rate   : num  0.556 0.609 0.587 0.511 0.698 ...
</code></pre>

<p>I have used lme4 in the past, but since some of my response variables have lots of zeros, it seems appropriate to run a zero-inflation model, and I have been trying glmmADMB. With some of the models, I have been getting warning messages.</p>

<p>For the Travel model, I don't think I need a zero-inflation model, but I get a warning when I run a poisson model with glmer</p>

<pre><code>&gt; travel.poisson.FULL2 &lt;- glmer(Travel ~ subject_age*brother + subject_age*Age_Diff + subject_age*cousin + subject_age*mother + log(Total_Follows) + (1|Subject) + (1|Partner), dat, family = poisson)
Warning messages:
1: Some predictor variables are on very different scales: consider rescaling 
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.0120444 (tol = 0.001, component 1)
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: very large eigenvalue
 - Rescale variables?;Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
&gt; summary(travel.poisson.FULL2)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: poisson  ( log )
Formula: Travel ~ subject_age * brother + subject_age * Age_Diff + subject_age *      cousin + subject_age * mother + log(Total_Follows) + (1 |  
    Subject) + (1 | Partner)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
  8989.2   9052.4  -4481.6   8963.2      941 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.5702 -1.7767 -0.3958  0.9003 16.4475 

Random effects:
 Groups  Name        Variance Std.Dev.
 Partner (Intercept) 0.25078  0.5008  
 Subject (Intercept) 0.01283  0.1133  
Number of obs: 954, groups:  Partner, 54; Subject, 18

Fixed effects:
                         Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)            -4.9997116  1.7689865  -2.826  0.00471 ** 
subject_age             0.0246246  0.0138692   1.775  0.07582 .  
brotherbro              1.2107468  0.4163344   2.908  0.00364 ** 
Age_Diff                0.0226314  0.0086427   2.619  0.00883 ** 
cousincuz               1.0076641  0.3756843   2.682  0.00731 ** 
motherMom               1.9488415  0.6762811   2.882  0.00396 ** 
log(Total_Follows)      1.7389194  0.4464744   3.895 9.83e-05 ***
subject_age:brotherbro -0.0362557  0.0260557  -1.391  0.16408    
subject_age:Age_Diff   -0.0002216  0.0003799  -0.583  0.55972    
subject_age:cousincuz  -0.0635028  0.0236412  -2.686  0.00723 ** 
subject_age:motherMom  -0.1105007  0.0413857  -2.670  0.00758 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) sbjct_ brthrb Ag_Dff cosncz mthrMm l(T_F) sbjct_g:b s_:A_D sbjct_g:c
subject_age -0.354                                                                     
brotherbro  -0.014  0.058                                                              
Age_Diff    -0.071  0.443 -0.001                                                       
cousincuz   -0.007  0.095  0.011  0.109                                                
motherMom   -0.001  0.000  0.023 -0.126 -0.004                                         
lg(Ttl_Fll) -0.990  0.227  0.006  0.001 -0.006  0.002                                  
sbjct_g:brt  0.015 -0.056 -0.988  0.005 -0.012 -0.025 -0.008                           
sbjct_g:A_D  0.027 -0.192  0.008 -0.714 -0.150  0.168 -0.002 -0.013                    
sbjct_g:csn  0.006 -0.092 -0.012 -0.106 -0.988  0.004  0.006  0.013     0.147          
sbjct_g:mtM  0.004 -0.002 -0.024  0.123  0.003 -0.991 -0.004  0.028    -0.169 -0.003   
fit warnings:
Some predictor variables are on very different scales: consider rescaling
convergence code: 0
Model failed to converge with max|grad| = 0.0120444 (tol = 0.001, component 1)
Model is nearly unidentifiable: very large eigenvalue
 - Rescale variables?
Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p>Whereas, when I run with glmmadmb, I don't get a warning</p>

<pre><code>&gt; travel.poisson.FULL &lt;- glmmadmb(Travel ~ subject_age*brother + subject_age*Age_Diff + subject_age*cousin + subject_age*mother + log(Total_Follows) + (1|Subject) + (1|Partner), dat, family = ""poisson"", zeroInflation = F)
&gt; summary(travel.poisson.FULL)

Call:
glmmadmb(formula = Travel ~ subject_age * brother + subject_age * 
    Age_Diff + subject_age * cousin + subject_age * mother + 
    log(Total_Follows) + (1 | Subject) + (1 | Partner), data = dat, 
    family = ""poisson"", zeroInflation = F)

AIC: 7430.9 

Coefficients:
                        Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)            -9.52e+00   6.32e+00   -1.50    0.132  
subject_age             3.28e-02   4.56e-02    0.72    0.471  
brotherbro              9.49e-01   4.29e-01    2.21    0.027 *
Age_Diff                2.70e-02   1.28e-02    2.11    0.035 *
cousincuz               5.36e-02   4.27e-01    0.13    0.900  
motherMom              -1.21e+00   8.39e-01   -1.45    0.148  
log(Total_Follows)      2.75e+00   1.60e+00    1.72    0.086 .
subject_age:brotherbro -2.85e-02   2.66e-02   -1.07    0.284  
subject_age:Age_Diff    2.64e-05   4.11e-04    0.06    0.949  
subject_age:cousincuz  -3.11e-03   2.70e-02   -0.12    0.908  
subject_age:motherMom   7.76e-02   4.99e-02    1.56    0.120  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Number of observations: total=954, Subject=18, Partner=54 
Random effect variance(s):
Error in VarCorr(x) : 
  could not find symbol ""rdig"" in environment of the generic function
</code></pre>

<p>For the touching model (which has lots of 0s, so it seemed appropriate to run a zero inflation model), I also got a warning.</p>

<pre><code>touch.poisson.FULL &lt;- glmmadmb(total_touch ~ subject_age*brother + subject_age*Age_Diff + subject_age*cousin + subject_age*mother + log(Total_Scans) +  (1|Subject) + (1|Partner), dat, family = ""poisson"", zeroInflation = T)

&gt;Warning messages:
1: In glmmadmb(total_touch ~ subject_age * brother + subject_age *  :
  non-integer response values in discrete family
2: In glmmadmb(total_touch ~ subject_age * brother + subject_age *  :
  Convergence failed:log-likelihood of gradient= -1.15888
</code></pre>

<p>What accounts for the differences in warnings produced by glmmadmb and glmer?</p>

<p>Some related questions:</p>

<p>Is it appropriate to put all of these effects and interactions into one model? Do these warnings and differences between packages suggest my model is unstable?</p>

<p>I have been trying to learn as much as possible about GLMMs and using them in R. I know there are a lot of different statistical approaches, but I want to ensure I am modeling my data appropriately and producing reliable results. Any and all help or advice is appreciated, and I am happy to provide more information.</p>
"
"0.0726752374667264","0.0727392967453308","197634","<p>What is the correct way to compare correlation between 2 dependent variables in R?</p>

<p>Thanks</p>

<p>Edit:
Here is the edited question and apologize for not asking this correctly before:</p>

<p>I acquired 2 measures from 2 different experiments and I want to know whether these 2 measures are correlated. </p>

<p>The problem is Measure 1 is confounded by some other covariates. So, I went ahead and did multiple regression and found the coefficient of my main effect. Since this is an estimate it has a mean with a deviation.</p>

<p>I could do cor(mean(parameter_estimate, Measure_2)) but I need to know if this correlation is significant. Is this right as I dont incorporate the spread of the estimate (variance)? My guess is the mean may be significant but with the standard error of the estimate, the correlation may become insignificant.</p>

<p>Thank you for your help</p>

<p>Regards</p>
"
"0.125877203744503","0.111989473484215","197682","<p>I'm trying to replicate Table 13.8 from Fitzmaurice, Laird, &amp; Ware (2011) using R for teaching purposes. This is a GEE count model of the number of bacteria on 30 patients at two waves. In their SAS-estimated results, they get a correlation between the two waves of 0.797. My ""geepack"" results in R, however, give me a 0 correlation. (And I get a 0 correlation when I try other count data.) </p>

<p>Any ideas as to how I can get a result more like their SAS results? Effectively I'm just looking for a positive estimated correlation.</p>

<p>My coefficients are similar in scale to theirs. My full script is posted <a href=""http://spia.uga.edu/faculty_pages/monogan/teaching/pd/exampleGEE.R"" rel=""nofollow"">here</a>, with the COUNT EXAMPLE halfway down being of interest. Here's what I get when I plug the relevant lines of code into R. Notice the ""alpha"" param:</p>

<pre><code> library(geepack)
 library(reshape)
 library(nlme)
 library(car)
 library(MuMIn)
 leprosy&lt;-read.table(""http://spia.uga.edu/faculty_pages/monogan/teaching/pd/leprosy.txt"", header=TRUE, sep="""")
 leprosy$id&lt;-c(1:nrow(leprosy))
     leprosy$drug&lt;-relevel(leprosy$drug,""C"")
     leprosy$antibiotic&lt;-1-as.numeric(leprosy$drug==""C"")
     m.leprosy&lt;-melt.data.frame(data=leprosy, measure.vars=c(""pre"",""post""), id=c(""id"",""drug"",""antibiotic""))
     m.leprosy$time&lt;-as.numeric(m.leprosy$variable==""post"")
     m.leprosy$a&lt;-as.numeric(m.leprosy$time==1 &amp; m.leprosy$drug==""A"")
 m.leprosy$b&lt;-as.numeric(m.leprosy$time==1 &amp; m.leprosy$drug==""B"")
     m.leprosy$treat&lt;-as.numeric(m.leprosy$time==1 &amp; m.leprosy$antibiotic==1)
 mod.3&lt;-geeglm(value~time+a+b, id=id, waves=time, family=poisson(link=""log""), data=m.leprosy,corstr=""exchangeable"")
 summary(mod.3)

Call:
geeglm(formula = value ~ time + a + b, family = poisson(link = ""log""), 
    data = m.leprosy, id = id, waves = time, corstr = ""exchangeable"")

 Coefficients:
            Estimate Std.err   Wald Pr(&gt;|W|)    
(Intercept)   2.3734  0.0801 877.10   &lt;2e-16 ***
time          0.1362  0.1919   0.50   0.4778    
a            -0.8419  0.3155   7.12   0.0076 ** 
b            -0.7013  0.3493   4.03   0.0447 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Estimated Scale Parameters:
            Estimate Std.err
(Intercept)      3.2   0.454

Correlation: Structure = exchangeable  Link = identity 

Estimated Correlation Parameters:
      Estimate Std.err
alpha        0       0
Number of clusters:   60   Maximum cluster size: 1 
</code></pre>

<p>I tend to believe Fitzmaurice, Laird, &amp; Ware's results of a positive correlation. Any help would be very much appreciated!</p>
"
"0.222521560911347","0.230141624980143","198181","<p><strong>Scientific question:</strong>
I want to know if temperature is changing across time (specifically, if it is increasing or decreasing). </p>

<p><strong>Data:</strong> My data consists of monthly temp averages across 90 years from a single weather station. I have no NA values. The temp data clearly oscillates annually due to monthly/seasonal trends. The temp data also appears to have approx 20-30-yr cycles when graphically viewing annual trends (by plotting annual avg temps across year):</p>

<p><a href=""http://i.stack.imgur.com/MapTs.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/MapTs.png"" alt=""NC Temp deviation""></a> </p>

<p><strong>Analyses done in R using nlme() package</strong></p>

<p><strong>Models:</strong> I tried a number of <code>gls</code> models and selected models that had lower AICs to move forward with. I also checked the significance of adding predictors based on ANOVA. It turns out that including time (centered around 1950), month (as a factor), and PDO (Pacific Decadal Oscillation) trend data create the 'best' model (i.e., the one with the lowest AIC and in which each predictor improves the model significantly). Interestingly, using season (as a factor) performed worse than using month; additionally, no interactions were significant or improved the model. The best model is shown below:</p>

<pre><code>mod1 &lt;- gls(temp.avg ~ I(year-1950) + factor(month) + pdo, data = df)

&gt; anova(mod1)
Denom. DF: 1102 
               numDF  F-value p-value
(Intercept)        1 87333.28  &lt;.0001
I(year - 1950)     1    21.71  &lt;.0001
pdo                1   236.39  &lt;.0001
factor(month)     11  2036.10  &lt;.0001

&gt; AIC(mpdo7,mod.2.1)
        df      AIC
mod1    15 4393.008
</code></pre>

<p>I decided to check the residuals for temporal autocorrelation (using Bonferroni adjusted CI's), and found there to be significant lags in both the ACF and pACF. I ran numerous updates of the otherwise best model (mod1) using various corARMA parameter values. The best corARMA gls model removed any lingering autocorrelation and resulted in an improved AIC. But time (centered around 1950) becomes non-significant. This corARMA model is shown below:</p>

<pre><code>mod2 &lt;- gls(temp.avg ~ I(year-1950) + factor(month) + pdo , data = df, correlation = corARMA(p = 2, q = 1)

&gt;   anova(mod2)
Denom. DF: 1102 
               numDF   F-value p-value
(Intercept)        1 2813.3151  &lt;.0001
I(year - 1950)     1    2.8226  0.0932
factor(month)     11 1714.1792  &lt;.0001
pdo                1   17.2564  &lt;.0001

&gt; AIC(mpdo7,mod.2.1)
        df      AIC
mod2    18 4300.847

______________________________________________________________________

&gt;   summary(mod2)
Generalized least squares fit by REML
  Model: temp.avg ~ I(year - 1950) + factor(month) + pdo 
  Data: df 
       AIC      BIC    logLik
  4300.847 4390.935 -2132.423

Correlation Structure: ARMA(2,1)
 Formula: ~1 
 Parameter estimate(s):
      Phi1       Phi2     Theta1 
 1.1547490 -0.1617395 -0.9562998 

Coefficients:
                    Value Std.Error  t-value p-value
(Intercept)      4.259341 0.3611524 11.79375  0.0000
I(year - 1950)  -0.005929 0.0089268 -0.66423  0.5067
factor(month)2   1.274701 0.2169314  5.87606  0.0000
factor(month)3   5.289981 0.2341412 22.59313  0.0000
factor(month)4  10.488766 0.2369501 44.26571  0.0000
factor(month)5  15.107012 0.2373788 63.64094  0.0000
factor(month)6  19.442830 0.2373898 81.90256  0.0000
factor(month)7  21.183097 0.2378432 89.06329  0.0000
factor(month)8  20.459759 0.2383149 85.85178  0.0000
factor(month)9  17.116882 0.2380955 71.89083  0.0000
factor(month)10 10.994331 0.2371708 46.35618  0.0000
factor(month)11  5.516954 0.2342594 23.55062  0.0000
factor(month)12  1.127587 0.2172498  5.19028  0.0000
pdo             -0.237958 0.0572830 -4.15408  0.0000

 Correlation: 
                (Intr) I(-195 fct()2 fct()3 fct()4 fct()5 fct()6 fct()7 fct()8  fct()9 fc()10 fc()11 fc()12
I(year - 1950)  -0.454                                                        
factor(month)2  -0.301  0.004                                                 
factor(month)3  -0.325  0.006  0.540                                          
factor(month)4  -0.330  0.009  0.471  0.576                                   
factor(month)5  -0.332  0.011  0.460  0.507  0.582                            
factor(month)6  -0.334  0.013  0.457  0.495  0.512  0.582                     
factor(month)7  -0.333  0.017  0.457  0.494  0.502  0.515  0.582              
factor(month)8  -0.333  0.019  0.456  0.494  0.500  0.503  0.512  0.585       
factor(month)9  -0.334  0.022  0.456  0.493  0.500  0.501  0.501  0.516  0.585
factor(month)10 -0.336  0.024  0.456  0.492  0.498  0.499  0.499  0.503  0.515  0.583  
factor(month)11 -0.334  0.026  0.451  0.486  0.492  0.493  0.493  0.494  0.496  0.508  0.576  
factor(month)12 -0.315  0.031  0.418  0.450  0.455  0.457  0.457  0.456  0.456  0.458  0.470  0.540
pdo              0.022  0.020  0.018  0.033  0.039  0.030  0.002  0.059  0.087  0.080  0.052  0.030 -0.009


Standardized residuals:
        Min          Q1         Med          Q3         Max 
-3.58980730 -0.58818160  0.04577038  0.65586932  3.87365176 

Residual standard error: 1.739869 
Degrees of freedom: 1116 total; 1102 residual
</code></pre>

<p><strong>My Questions:</strong></p>

<ol>
<li><p>Is it even appropriate to use an ARMA correlation here?</p>

<ul>
<li>I assume that any inferences from a simple linear model (e.g., <code>lm(temp ~ year)</code>) are inappropriate b/c of other underlying correlation structure (even though this simple linear trend <em>is</em> what I'm most interested in.</li>
<li><p>I assume by removing affects of time lags (i.e. autocorrelation), I can better 'see' if there is in fact a long term temporal trend (incline/decline)?</p>

<ul>
<li>Is this the correct way to think about this?</li>
</ul></li>
</ul></li>
<li><p>Concerning year becoming non-significant in the model...</p>

<ul>
<li>Would this have occurred because <em>all</em> of the temporal trend turned out to be due to autocorrealtion and therefore is now otherwise being accounted for in the model?</li>
<li><p>Do I remove time from my model now (since it's no longer a significant predictor)??</p>

<ul>
<li><p><strong>UPDATE:</strong> I did do this, and the resulting model had a lower AIC (4291 vs 4300 of mod2 above). </p></li>
<li><p>Though this isn't really a useful step for me, because I'm actually concerned about a trend in temp due to <em>time</em> (i.e., year) itself. </p></li>
</ul></li>
</ul></li>
<li><p>Interpretation -- Am I interpreting the results correctly??:</p>

<ul>
<li>So based on the <code>summary</code> output above for mod2, is it correct to assume the answer to my original scientific question is: ""temperature has declined at a rate of -0.005929, but this decline is not significant (p = 0.5067)."" ??</li>
</ul></li>
<li><p>Next steps...</p>

<ul>
<li>I ultimately want to see if temperature will have an impact on tree-community time-series data. My motivation behind the procedure mentioned here was to determine if there was a trend in temperature before bothering to start including it in subsequent analyses.</li>
<li>So as performed, I assume I can now say that there is not a significant linear change (increase/decline) in temp. This would suggest that perhaps temp is not important to include in subsequent analyses?</li>
<li>However...perhaps the cyclic nature of the temp <em>is</em> important and drives cyclic patterns in the plant data. How would I approach this? (i.e., how do I 'correlate' the cyclic trend in temp with potential cyclic trend in plants' -- vs. simply <em>removing</em> cyclic (seasonal) trends based on the ACF results)? </li>
</ul></li>
</ol>
"
"0.0419590679148345","0.0419960525565808","198294","<p>I want to perform a <code>polychoric correlation</code> on non-normal distributed underlying variables (which are in my case likert scaled). According to Joakim EkstrÃ¶m (<a href=""https://www.google.de/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0ahUKEwj54LulvpDLAhXJwxQKHWT4CAAQFggdMAA&amp;url=http%3A%2F%2Fstatistics.ucla.edu%2Fsystem%2Fresources%2FBAhbBlsHOgZmSSJiMjAxMi8wNS8xNi8xNV8xNF8yOV8yMF9BX0dlbmVyYWxpemVkX0RlZmluaXRpb25fb2ZfdGhlX1BvbHljaG9yaWNfQ29ycmVsYXRpb25fQ29lZmZpY2llbnQucGRmBjoGRVQ%2FA%2520Generalized%2520Definition%2520of%2520the%2520Polychoric%2520Correlation%2520Coefficient.pdf&amp;usg=AFQjCNE4zf_x18xQNP7t-wsP869r9H4USQ&amp;cad=rja"" rel=""nofollow"">""A Generalized Definition of the Polychoric Correlation Coeficient""</a>) you can perform the correlation with a large class of continuous bivariate distributions. I just do not know how to do the calculation. I checked all <code>R</code> packages with <code>Polychoric Correlation</code> I could find but non provides something like an argument for distribution moment parameters or something else. 
It would be nice, if anyone could tell me how I can calculate what I want and if the results are going to be robust. </p>
"
"0.132686223108569","0.132803178814933","198510","<p>I am currently studying the effect of organic farming on honeybee colonies. I have calculated the percentage of organic land in several buffer areas around the hives (from 100m to 3000m in 100m steps) and I would like to study how an increase in this percentage changes honeybee performance. Unfortunately we do not know much about the foraging behavior of bees, but we know that they forage at variable scales, depending e.g. on the flower availability. I don't know how to select an appropriate scale. </p>

<p>I have seen that other authors compared via AIC linear models that differed only in the scale of the (explanatory) landscape variable but contained the same response variable. A colleague recommended me to plot the Pearson correlation coefficients of the relation between the response variable and the percentage of organic farming at different scales. From this I should identify a turning point, which would then be the optimal scale. </p>

<p>In some cases I did not really find a turning point within the regarded range of scales, but rather an increase of the correlation coefficient with increasing buffer area. (I do not want to look at larger scales because (i) if the scale is very large the difference in the percentage of organic farming between different hives is small (ii) the hives are not too far from each other, at very large scales there will be a large overlap of buffers from different hives (iii) I regard as unlikely that fields that far from the hive have a strong effect on the colony development.)</p>

<p>Both approaches appear to me like a bit of data mining, as we search a scale where the effect is the largest, based on the correlation without correcting p-values via post-hoc tests. </p>

<p>I tried to see at which scale I have the nicest gradient of organic farming, but this is very subjective and I do not really know which to choose under this aspect, as the percentage of buffers that contain some organic farming increases with a larger buffer area, but the maximum percentage of organic land decreases. </p>

<p>Sorry for this long text. I hope to get some suggestions. â˜º Best, bee guy</p>
"
"0.0419590679148345","0.0419960525565808","200198","<p>I'm trying to build a product recommender system. I'm collecting users data from social media like number of mutual friends,age,gender,career for users u1..u50. u1 is target user and I want to apply regression and find correlation coefficient between u1-u2,u1-u3 and so on(and give more weightage to user's rating with higher correlation coefficient). Is it possible to do like this and any ideas how to implement it? </p>
"
"NaN","NaN","202963","<p>I am learning R and how to do regressions in a biological context, so please forgive me.</p>

<p>I am stumped on how to test if a slope parameter is less than a certain number at the alpha = 0.05 level. These are my two correlation(?) coefficients:</p>

<blockquote>
  <p>reg.fit &lt;- lm(gro3 ~ gro2) # fit linear model
  reg.fit</p>
  
  <p>Call:
  lm(formula = gro3 ~ gro2)</p>
  
  <p>Coefficients:</p>
  
  <p>(Intercept),         gro2<br>
     -0.8426,       0.3582    </p>
</blockquote>

<p>I <em>think</em> I should use a t-test to test these somehow. But I'm not sure where to start.</p>
"
"0.151672986506104","0.162650012158089","203290","<h2><strong>Data:</strong></h2>

<p>I have 92 years of monthly climate data. One of my variables is a drought index (<a href=""https://climatedataguide.ucar.edu/climate-data/standardized-precipitation-evapotranspiration-index-spei"" rel=""nofollow"">SPEI</a>) ranging from -2 (dry) to 2 (wet). 
All the data can be found <a href=""http://theforestecologist.web.unc.edu/files/2015/07/df.clim_.csv"" rel=""nofollow""><strong>here</strong></a>.</p>

<p><strong><em>Data Structure:</em></strong></p>

<pre><code>year month  order.ID  season  temp.avg  ppt.avg    GDD    pdo     spei
1923     0        13       1    6.1612  0.23516  73.54   0.27   0.6544
1923     1        14       1    5.0967  0.24161  40.66   0.01   0.4837
1923     2        15       1     3.425  0.24428  47.19  -0.95   0.5207
1923     3        16       2    9.7870  0.46612  161.3  -1.23   0.2631
1923     4        17       2    12.753    0.304  238.1  -0.64   0.2476
</code></pre>

<p><em>note: PDO = Pacific Decadal Oscillation Index | order.ID = (monthly) time series order</em></p>

<p><strong><em>Graph of SPEI across time:</em></strong></p>

<p><a href=""http://i.stack.imgur.com/OGVEQ.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/OGVEQ.jpg"" alt=""SPEI""></a></p>

<p>I am trying to determine the long-term (92yr) trends of the SPEI data. I am doing so with a general least squares approach using <code>gls</code> function from the <code>nlme</code> package in <code>R</code>. </p>

<hr>

<h2><strong>Issue:</strong></h2>

<p>After examining my best (lowest AIC) model ( <code>gls(spei ~ I(year - 1950) + pdo)</code> ) using ACF and PACF, it was very clear that there was underlying temporal autocorrelation and likely periodicity. </p>

<p><a href=""http://i.stack.imgur.com/agvIw.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/agvIw.jpg"" alt=""ACF &amp; PACF for NC""></a> </p>

<p>I've attempted to account for these issues in two ways:</p>

<ol>
<li><p><strong>Accounting for periodicity</strong> by including <strong>sin + cos</strong> parameters to the model. I did so by examining how including the following parameters, </p>

<pre><code>I(cos(2*pi/P*(order.ID))) + I(sin(2*pi/P*(order.ID)))
</code></pre>

<p>in my model using various period lengths (P) affected the AIC of the model: </p>

<p><a href=""http://i.stack.imgur.com/HM8GF.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/HM8GF.jpg"" alt=""sincos_AIC""></a></p>

<ul>
<li>Though including the sin/cos parameters using the period (630) that created the model with the lowest AIC improved the AIC of my model, it did not correct for the issues I was seeing in my ACF/PACF plots. In fact, even including upwards of 4 combinations of SIN/COS parameters using numerous ""strong"" periods (e.g., 630, 238, 183 and 49) failed to eliminate the ACF/PACF issues:</li>
</ul>

<p><a href=""http://i.stack.imgur.com/2j9f6.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/2j9f6.jpg"" alt=""acf of periods 630 + 238""></a></p></li>
<li><p><strong>Accounting for autocorrelation</strong> using <strong>corARMA</strong> argument in <code>gls</code> function. Specifically, I tried ARMA models using all combinations of p = {1:4} &amp; q = {0:3} with no luck in eliminating the autocorrelation. </p>

<ul>
<li><p>The best performing (lowest AIC) model was </p>

<pre><code>gls(spei ~ I(year - 1950) + pdo, correlation = corARMA(p=4,q=2), method = ML)
</code></pre></li>
<li><p>But the resulting ACF/PACF still had issues:</p></li>
</ul>

<p><a href=""http://i.stack.imgur.com/4m65R.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4m65R.jpg"" alt=""acf3""></a></p></li>
</ol>

<p>Note: I did try a combination of 1 and 2 (using sin+cos parameters <em>and</em> a corARMA argument in my model), but even the combination of approaches failed to ""fix"" the problem.</p>

<p>Additional note:</p>

<p>I tried adding months and seasons (separately and together) to the model in two ways: 1st, as periods (12 and 4 respectively) in SIN/COS parameters and, second, as categorical dummy variables in the model. Both approaches resulted in models with higher AICs - essentially suggesting the periodicity is not due to months or seasons??</p>

<hr>

<h2><strong>Question</strong>:</h2>

<p>What do I do now? How do I go about ""fixing"" this data - i.e., how do I properly account for the periodicity and autocorrelation so that I can accurately view the long-term annual (linear) trend (and correct p value of the coefficient)?</p>

<p>Obviously nothing I've tried so far has worked...</p>
"
"0.132686223108569","0.132803178814933","204145","<p><strong>Background and Problem</strong></p>

<p>I have a question concerning a meta-analysis combining effects from between- and within-subject designs using log-odds ratios (OR) as the metric of interest. I am familiar with conducting meta-analyses and will be undertaking my calculations in R (using the <code>metafor</code> and <code>lme4</code> packages). To provide greater context, the studies in question ask research subjects to make a binary decision with respect to a personal preference across one of two conditions. In some cases, each participant is assigned to a single condition (making only a single binary response); in others, each subject takes part in both conditions (making two binary responses). For now, presume I have the raw data in all cases. The issue I face is how best to calculate an OR that is comparable across design and whether I should take the correlation between conditions into account for the within-subject designs.</p>

<p><strong>My Current Approach</strong></p>

<p>I presently use logistic regression to estimate the OR for between-subject designs. The slope represents the OR and the sampling variance can be calculated by squaring the SE of the slope coefficient. Using this approach produces estimates comparable to equations reported in common texts such The Handbook of Research Synthesis and Meta-Analysis, 2nd Edition (p. 243). I then extend this approach to use a multilevel logistic regression model including a random intercept by subject to estimate the OR for within-subject designs while account for the dependency between conditions. The OR and sampling variance are otherwise calculated in the same fashion. </p>

<p><strong>My Questions:</strong></p>

<p>With this in mind, I would like to ask:</p>

<ol>
<li>Is it reasonable to meta-analytically aggregate OR calculated using standard and multilevel logistic regression?</li>
<li>Would it be better to use standard logistic regression for both designs (ignoring the correlation between conditions for the within-subject designs)?</li>
</ol>
"
"0.0938233281301002","0.0939060283031685","204357","<p>I have carried out a PCA, and have used the first principal component (called <code>m_pca1</code> later on) in a general linear model. Here's the PCA output:</p>

<pre><code>Area -0.521
Hue -0.4668
Saturation -0.487
Brightness -0.854
Red -0.855
Green -0.982
Blue -0.899
</code></pre>

<p>So the first component shows all colour variables to be negatively correlated.</p>

<p>My minimum adequate model from the <code>glm</code> is:</p>

<p>Call:</p>

<pre><code>lm(formula = m_pca1 ~ Pop + Diplostomum)
</code></pre>

<p>Residuals:</p>

<pre><code>    Min      1Q  Median      3Q     Max 
-5.2140 -0.9200  0.0688  0.9148  3.2798 
</code></pre>

<p>Coefficients:</p>

<pre><code>            Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)  0.27409    0.70233   0.390  0.69720   
Diplostomum -0.05059    0.02480  -2.040  0.04405 * 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 1.57 on 97 degrees of freedom
Multiple R-squared:  0.5128,    Adjusted R-squared:  0.3822 
F-statistic: 3.927 on 26 and 97 DF,  p-value: 4.743e-07
</code></pre>

<p>I'm trying to interpret the effect of Diplostomum on <code>m_pca1</code> (1st principal component). The coefficient for Diplostomum is negative, implying a negative correlation with <code>m_pca1</code>.</p>

<p>But, <code>m_pca1</code> is about negative correlations between colour variables, so does this mean Diplostomum increases as these colour variable increase or decrease? </p>
"
"0.0938233281301002","0.0939060283031685","206039","<p>I am looking at a logistic regression model for predicting hospital acquired infection likelihood (HAI) from predictors of whether germs are found on the  x number of patients (Patient), x number of environmental spots (Env), x number of air samples (Air) or x number of nurses' hands (Hand).</p>

<pre><code>   Month Patient Env Air Hand HAI HAIcat BedOccupancy
      1       4   0   0    1   1    yes            9
      2       2   0   2    0   0     no            9
      3       2   1   0    1   0     no            5
      4       1   2   0    2   2    yes            7
      5       2   3   0    1   1    yes            6
      6       1   2   0    0   1    yes            5
      7       4   0   0    2   1    yes            7
      8       2   0   0    1   3    yes            7
      9       3   2   2    0   1    yes            8
     10       3   0   0    1   1    yes            8
</code></pre>

<p>For example for Month 1, the percentage of HAI would be HAI/BedOccupancy=1/9.
So I'd like to know if bed occupancy or other contamination is significant in predicting HAI. I run a Logistic regression, but it says it's junk. What does a statistician do now?</p>

<pre><code>model&lt;-glm(cbind(MR$HAI,MR$BedOccupancy)~MR$Patient+MR$Env+MR$Air+MR$Hand,family = ""binomial"")
</code></pre>

<p>But I get a bad fit and non-significant correlation:</p>

<pre><code>Call:
glm(formula = cbind(MR$HAI, MR$BedOccupancy) ~ MR$Patient + MR$Env + MR$Air + 
        MR$Hand, family = ""binomial"")

Deviance Residuals: 
       1         2         3         4         5         6         7         8         9        10  
-0.12882  -1.08046  -1.33787   0.01400  -0.10685  -0.02229  -0.04008   1.03688   0.75723  -0.23824  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -1.30758    1.34049  -0.975    0.329
MR$Patient  -0.22920    0.39350  -0.582    0.560
    MR$Env      -0.02415    0.37672  -0.064    0.949
MR$Air      -0.46851    0.64611  -0.725    0.468
    MR$Hand      0.16054    0.58277   0.275    0.783

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 6.6594  on 9  degrees of freedom
Residual deviance: 4.6929  on 5  degrees of freedom
AIC: 30.911

Number of Fisher Scoring iterations: 5
</code></pre>
"
"0.0593390829096927","0.0593913870916499","207347","<p>I'm trying to calculate time-weighted Pearson correlation as described in <a href=""http://goo.gl/HoqwI7"" rel=""nofollow"">http://goo.gl/HoqwI7</a> The coefficient is given by</p>

<p>$$\rho_t(X,Y) = \left ( \frac{1-r}{1-r^N} \right ) \sum_{i=0}^{N} r^{i-1} \frac{(x_{-i} - \mu_X)(y_{-i} - \mu_Y)}{\sigma_X\sigma_Y},$$</p>

<p>where $N+1$ is the number of observations, $t=0,1,...,N$ indicates time period when the coefficient is calculated ($t = N$ in my case, i.e. I'm calculating the correlation based on full sample), $\sigma_X$, $\mu_X$, $x_{-i}$ denote standard deviation, expectation, and the $i$th latest observation in $X$ respectively, $r$ is the decay constant, equal to a real number less than 1.</p>

<p>My problem is that $\rho_t(X,X) \neq 1$ when calculating it based on the formula above. Below is an example of how I calculate it in R. </p>

<pre><code>set.seed(314)
N &lt;- 1000
x &lt;- cumsum(rnorm(N+1)) # random walk

cor(x, x) # Pearson correlation
# Result is 1

r &lt;- 0.998
sigma &lt;- sd(x)
mu &lt;- mean(x)

(1 - r)/(1 - r^N) * sum( r^(0:N-1) * (x - mu)^2/sigma^2 ) # rho_t(x, x)
# Result is 1.171462
</code></pre>

<p>Please correct me if I have a bug in my code or suggest how I should modify the formula if it contains a mistake (my gut feeling says that the normalisation might be wrong).</p>
"
"NaN","NaN","208621","<p>I am trying to perform a clustering analysis in R according to Spearman correlation coefficient.</p>

<p>Could it happen that the analysis identifies only one big cluster as in the figure attached (the code is below the figure)? How would you interpret that? Please not that the same code generates a more meaningful dendogram if I change the matrix I use (in this case <code>mat_settings_old</code>).</p>

<p><a href=""http://i.stack.imgur.com/XIisq.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/XIisq.png"" alt=""enter image description here""></a></p>
"
"0.0419590679148345","0.0419960525565808","208768","<p>I'm doing a GLMM with quasi-Poisson to check for a spatial correlation between some predator bugs and their prey (count data of predator and prey + added distance of plots). I've added everything into the formula using ""<code>glmmPQL</code>"" and looking at the result with a ""<code>summary</code>"" of model. I then get the different values called ""<code>Value</code>"", ""<code>Std.Error</code>"", ""<code>DF</code>"", ""<code>t-value</code>"" and ""<code>p-value</code>"". My question is, what does the value simply called ""<code>value</code>"" tell me? I think the number tells me how one factor x (predator bug) reacts to one increased value of y (prey) (Beta coefficient). Is this correct?</p>

<p>edit:
Added output</p>

<p><a href=""http://i.stack.imgur.com/Eenrb.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Eenrb.png"" alt=""enter image description here""></a></p>
"
"0.19247955013414","0.183892428122457","210757","<p>I have repeated measures of <code>happiness</code> for a sample of participants, and a single measure of <code>satisfaction</code> for each of the participants.</p>

<p>I want to predict <code>satisfaction</code> from the repeated measures of <code>happiness</code>. To do so, I want to create a new variable, called <code>happiness.change</code>, which measures the degree of change/trend in <code>happiness</code> from the first measurement to the last, for each participant (for example, a negative <code>happiness.change</code> if there is a decrease in <code>happiness</code> over time). Then I want to predict <code>satisfaction</code> from <code>happiness.change</code>.</p>

<p>Below (using R) is an excerpt from my data (a sample of 9 participants):</p>

<pre><code>ids &lt;- c(rep(seq(1:5), each = 2), rep(6:9, each = 5))
time &lt;- c(rep(1:2, 5), rep(1:5, 4))
happiness &lt;- c(0.80,0.00,0.75,0.00,0.80,0.00,2.75,2.50,0.40,0.20,
               3.80,0.40,0.00,0.20,3.40,3.00,2.60,3.40,3.80,0.00,
               3.60,3.60,0.20,0.40,1.00,0.40,0.20,1.20,1.20,0.00)
satisfaction &lt;- c(6,6,2,2,3,3,2,2,2,2,5,5,5,5,5,7,7,7,7,7,1,1,1,1,1,2,2,2,2,2)
data &lt;- as.data.frame(matrix(c(ids, time, happiness, satisfaction),
                         nrow = 30,
                         ncol = 4,
                         dimnames = list(c(),c(""id"", ""time"",
                                               ""happiness"", ""satisfaction""))))
print(data)

#    id time happiness satisfaction
# 1   1    1      0.80            6
# 2   1    2      0.00            6
# 3   2    1      0.75            2
# 4   2    2      0.00            2
# 5   3    1      0.80            3
# 6   3    2      0.00            3
# 7   4    1      2.75            2
# 8   4    2      2.50            2
# 9   5    1      0.40            2
# 10  5    2      0.20            2
# 11  6    1      3.80            5
# 12  6    2      0.40            5
# 13  6    3      0.00            5
# 14  6    4      0.20            5
# 15  6    5      3.40            5
# 16  7    1      3.00            7
# 17  7    2      2.60            7
# 18  7    3      3.40            7
# 19  7    4      3.80            7
# 20  7    5      0.00            7
# 21  8    1      3.60            1
# 22  8    2      3.60            1
# 23  8    3      0.20            1
# 24  8    4      0.40            1
# 25  8    5      1.00            1
# 26  9    1      0.40            2
# 27  9    2      0.20            2
# 28  9    3      1.20            2
# 29  9    4      1.20            2
# 30  9    5      0.00            2
</code></pre>

<p>To create <code>happiness.change</code> I was advised to use the coefficients produced by either of these equations:</p>

<pre><code>require(lme4)
model1 &lt;- lmer(happiness ~ time + (time | id), data = data)

require(nlme)
model2 &lt;- lme(happiness ~ time, random = ~1 + time | id, data = data)
</code></pre>

<p>For example, running <code>coef(model1)</code> produces the following coefficients (column <code>time</code>):</p>

<pre><code># $id
#   (Intercept)        time
# 1   0.9936158 -0.05991770
# 2   0.9739595 -0.05674569
# 3   0.9936158 -0.05991770
# 4   2.5539086 -0.31170766
# 5   0.8998610 -0.04478815
# 6   2.0446747 -0.22953078
# 7   3.2906564 -0.43059926
# 8   2.7120530 -0.33722798
# 9   1.0027053 -0.06138450
# 
# attr(,""class"")
# [1] ""coef.mer""
</code></pre>

<p>And then connecting between <code>satisfaction</code> and the coefficients:</p>

<pre><code>coefs1 &lt;- as.data.frame(unlist(coef(model1))[10:18])
satisfactionData &lt;- reshape(data,
                            direction = ""wide"",
                            idvar = ""id"",
                            timevar = ""time"")[c(1,3)]
newData &lt;- cbind(satisfactionData, coefs1)
colnames(newData) &lt;- c(""id"", ""satisfaction"", ""happiness.change"")
print(newData)

#    id satisfaction happiness.change
# 1   1            6      -0.05991770
# 3   2            2      -0.05674569
# 5   3            3      -0.05991770
# 7   4            2      -0.31170766
# 9   5            2      -0.04478815
# 11  6            5      -0.22953078
# 16  7            7      -0.43059926
# 21  8            1      -0.33722798
# 26  9            2      -0.06138450
</code></pre>

<hr>

<p><strong>I have several questions:</strong></p>

<ol>
<li><p>Running <code>model2</code> generates the following error:</p>

<pre><code>Error in lme.formula(happiness ~ time, random = ~1 + time | id, data = data) : 
  nlminb problem, convergence error code = 1
  message = iteration limit reached without convergence (10)
</code></pre>

<p>I don't remember where, but I read that running <code>lme(happiness ~ time, random = ~1 + time | id, control = list(opt = ""optim""), data = data)</code> bypasses this error, and indeed it does. But what exactly does it do? Do the coefficients produced by using this model still represent the change in <code>happiness</code> over time?</p></li>
<li><p>Running <code>model2</code> (with <code>control = list(opt = ""optim"")</code>) produces slightly (<em>very</em> slightly) different coefficients than <code>model1</code>; why? What is the difference between the models?</p></li>
<li><p>What would be the suitable method for testing the relationship between <code>happiness.change</code> and <code>satisfaction</code>? I tried <code>cor.test(newData$satisfaction, newData$happiness.change)</code>, which produced the following results, but I'm not sure how to interpret them (they are significant in the complete data set):</p>

<pre><code>    Pearson's product-moment correlation

data:  newData$satisfaction and newData$happiness.change
t = -0.72735, df = 7, p-value = 0.4906
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 -0.7901064  0.4843018
sample estimates:
       cor 
-0.2650786 
</code></pre></li>
<li><p>As you can see, <code>model1</code> produces negative coefficients for all the participants (<code>model2</code> as well). Even for participants whose change over time is purely positive (for example, a participant with 2 <code>happiness</code> measures: <code>time1 = 0</code> and <code>time2 = 0.8</code>; no such example in the sample above, but it is in my data).</p>

<p>This may be a problem, because I want to be able to distinguish between participants whose change in <code>happiness</code> is positive (<code>happiness</code> increases over time) and participants whose change in <code>happiness</code> is negative (<code>happiness</code> decreases over time); and then see whether there's a difference between these participants in their relationship between <code>happiness.change</code> and <code>satisfaction</code>. </p>

<p>So my question is this: Is it statistically ""legit"" to divide my participants beforehand into groups based on their ""raw"" change in <code>happiness</code> (for example, I would label a participant with <code>time1 = 0</code> and <code>time2 = 0.8</code> as having a positive change), and then model the coefficient for these groups separately?</p>

<p>However, creating sub-groups this way may be difficult with participants with more than 2 measures of <code>happiness</code>, which brings me to my final question:</p></li>
<li><p>If I understand correctly, <code>model1</code> and <code>model2</code> assume there is a linear change over time. However, I checked the whole sample (424 participants), and a cubic model (<code>happiness ~ time</code>) actually explains more of the variance in <code>happiness</code>. So I suppose a cubic change over time is more appropriate. How can I create a new variable reflecting such a change in <code>happiness</code> over time?</p></li>
</ol>

<p>I realize this is quite long and these are a lot of questions, and so any help will be greatly appreciated. If anyone can answer even one of these questions, I will be very grateful.</p>

<p>Thanks!</p>
"
"0.139162484826546","0.126622862732931","210988","<p>I am addressing reviewer comments on a meta-analysis paper I performed on mean gastric pH in fed and fasted states.</p>

<p>Some literature studies included in the meta-analysis provided data for multiple comparisons (e.g under fed and fasted condition) as <code>study2</code> in the example data below while other studies provided data under only fed (<code>study1</code>) or fasted conditions (<code>study3</code>). The <code>metaphor</code> package in R treats the effect sizes obtained from studies provided data from multiple comparisons as an independent investigations (i.e. number of studies (k=4) and treats the same as studies with data obtained only under a single set of conditions (for example, study2 fed and fasted data may be correlated; however, the analysis ignores this correlation and treat data fed from <code>study2</code> the same way it does for data fed in <code>study1</code>).</p>

<p>My question: What potential impact ignoring this correlation might have on the meta-estimate of mean gastric pH?   </p>

<pre><code>   df &lt;- 
   study   nsub   status   pH   vi
   study1    5    fed      5.0  0.10
   study2   10    fasted   2.0  0.02
   study2    8    fed      4.0  0.20
   study3    6    fasted   1.5  0.15


rma(yi = pH, vi = vi, data=df, method=""REML"", knha = TRUE, mods = ~factor(status)-1)

#Result
Mixed-Effects Model (k = 4; tau^2 estimator: REML)

tau^2 (estimated amount of residual heterogeneity):     0.1591 (SE = 0.2709)
tau (square root of estimated tau^2 value):             0.3988
I^2 (residual heterogeneity / unaccounted variability): 59.45%
H^2 (unaccounted variability / sampling variability):   2.47

Test for Residual Heterogeneity: 
QE(df = 2) = 4.8039, p-val = 0.0905

Test of Moderators (coefficient(s) 1,2): 
F(df1 = 2, df2 = 2) = 79.1333, p-val = 0.0125

Model Results:

                      estimate      se     tval    pval   ci.lb   ci.ub    
factor(status)fasted    1.8166  0.3475   5.2278  0.0347  0.3215  3.3117   *
factor(status)fed       4.5809  0.4003  11.4427  0.0076  2.8584  6.3034  **

---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>
"
"0.0726752374667264","0.0727392967453308","211435","<p>In statistics, there is the $\mathrm{R}$ value for the product moment correlation coefficient and the $\mathrm{R}^2$ value for the coefficient of determination.</p>

<p>In both cases they are described as a scale of correlation, where $0$ is no correlation and $1$ is perfect correlation. However, for a given data set these values are different - for example when $\mathrm{R}=0.8$ the $\mathrm{R}^2=0.64$. How can this be so?</p>

<p>Another layer of confusion is that for the $\mathrm{R}^2$ value, it is said that the value represents the proportion of change in $y$ caused by changes in $x$. For instance, an $\mathrm{R}^2$ value of 0.7 means that 70% of the change in the dependent variable is explained by changes in the independent variable. Is this also true of the $\mathrm{R}$ value? Why or why not?</p>
"
"0.111013258946721","0.111111111111111","211851","<p>I have this bivariate data:</p>

<pre><code>x=(7.1,7.1,7.2,8.3,9.4,10.5,11.4)

y=(2.8,2.9,2.8,2.6,3.5,4.6,5.0)
</code></pre>

<p>I want to examine the relationship between x and y by using Spearmans correlation coefficient (R computes: r=0.7). And I want to test the value of the correlation coefficient for significance. The null hypothesis is: rho = 0 (two sided test), which means no relationship. The following R-Code computes an approximate p-value of 0.07992:</p>

<pre><code>cor.test(x,y,method=""spearman"")
</code></pre>

<p>But R gives me the following warning: No exact p-value because of ties.</p>

<p>Now I want to compute a permutation test to get the exact p-value (for alpha=0.05, two sided). I looked it up on the Internet and I think it should be possible with the package ""coin"". But I have no idea how to do this. </p>

<p>I found already the following solution:</p>

<pre><code>library(coin)

spearman_test(y~x,distribution=approximate(B=9999)) 
</code></pre>

<p>R computes a p-value of 0.08641. But I am not sure, if this is correct. I want to find an exact p-value and not an approximate one. </p>

<p>I would be really grateful, if anybody could help me. </p>
"
"0.0938233281301002","0.0751248226425348","212453","<p>In my research project I have to do a regression of the financial risk on the business risk of the year before.
As a reference, I have a paper showing the results for several countries. The paper states that, for the country I am interested in, the coefficient of the regression is negative. their year span :1995-2008</p>

<p>I have to perform the same analysis at region level for the country I am interested in. I find a negative lagged correlation coefficient. However, the coefficient in my regression is positive and significant. My year span:2000-20014.</p>

<p>Should I not have a negative coefficient as in the country-level regression (paper).
Thank you.</p>
"
"0.126511349842314","0.126622862732931","213470","<p><strong>Background</strong></p>

<p>Although my data should have a multinomial dependent variable, I have settled for a binary as I could not understand too much of MCMCglmm. The data is a time series cross sectional, so am looking at each individual outcome vis-a-vis the others. I don't have much experience with statistics, so I really need to know if am on the right path of actually coming up with values for a dynamic linear model or way off. Since i need effects from the independent variables.</p>

<p><strong><em>The data sample</em></strong></p>

<p>The data is for students who applied for university courses and were admitted within a period of 3 years. The data mainly has the grades in the subjects done in their pre-entry level exams. Each student can do a maximum of 4, but in the model below <code>NA</code> values are filled with <code>0</code> (Not sure if a correct assumption). </p>

<p><strong>The problem</strong></p>

<ol>
<li>How do I get time varying effects?</li>
<li>How do i extract the effect of time? </li>
<li>what does it imply when time is expressed as a random effect?</li>
</ol>

<p>Every input is highly appreciated, Thank you.</p>

<p>Below is a result from the <code>glmer</code> function with formula</p>

<p><code>glmllb2 &lt;- glmer(logi ~ history + c.r.e + economics + geography + literature + f.art + entrepreneurship + luganda + kiswahili + french + i.r.e + historyc + historycsq + (1 | called), family = binomial(""logit""), control = glmerControl(optimizer = ""bobyqa""), nAGQ = 100, data = data.apriori.llb2)</code></p>

<p>where <code>history+c.r.e + ... + i.r.e</code> are subject grades that predict student admission into a course <code>logi</code> (as a binary) while <code>historyc</code> is a grand mean centered variable for history and <code>historycsq</code> is the squared variable for <code>historyc</code>. <code>called</code> is time in years re-scaled to <code>1,2 and 3</code></p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Adaptive Gauss-Hermite Quadrature, nAGQ = 100) [glmerMod]
 Family: binomial  ( logit )
Formula: logi ~ history + c.r.e + economics + geography + literature +  
    f.art + entrepreneurship + luganda + kiswahili + french +      i.r.e + historyc + historycsq + (1 | called)
   Data: data.apriori.llb2
Control: glmerControl(optimizer = ""bobyqa"")

     AIC      BIC   logLik deviance df.resid 
  1778.2   1874.8   -875.1   1750.2     7317 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
 -3.843  -0.123  -0.054  -0.025 213.612 

Random effects:
 Groups Name        Variance Std.Dev.
 called (Intercept) 0.09975  0.3158  
Number of obs: 7331, groups:  called, 3

Fixed effects:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)      -13.27747    0.52128 -25.471  &lt; 2e-16 ***
history            0.55942    0.04656  12.016  &lt; 2e-16 ***
c.r.e              0.45941    0.03652  12.580  &lt; 2e-16 ***
economics          0.69835    0.04509  15.489  &lt; 2e-16 ***
geography          0.49442    0.04137  11.950  &lt; 2e-16 ***
literature         0.77936    0.04129  18.877  &lt; 2e-16 ***
f.art              0.50219    0.04387  11.447  &lt; 2e-16 ***
entrepreneurship   0.46377    0.04504  10.297  &lt; 2e-16 ***
luganda            0.49340    0.07643   6.456 1.08e-10 ***
kiswahili          0.52691    0.10498   5.019 5.20e-07 ***
french             0.65225    0.09133   7.142 9.22e-13 ***
i.r.e              0.59269    0.08265   7.171 7.44e-13 ***
historycsq         0.03721    0.01794   2.075    0.038 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) histry c.r.e  ecnmcs ggrphy litrtr f.art  entrpr lugand kiswhl french i.r.e 
history     -0.559                                                                             
c.r.e       -0.608  0.143                                                                      
economics   -0.340 -0.083 -0.002                                                               
geography   -0.569  0.187  0.624 -0.059                                                        
literature  -0.625  0.169  0.540  0.086  0.709                                                 
f.art       -0.614  0.231  0.588  0.118  0.525  0.585                                          
entrprnrshp -0.554  0.191  0.564 -0.031  0.583  0.654  0.596                                   
luganda     -0.305  0.086  0.289  0.065  0.305  0.337  0.283  0.281                            
kiswahili   -0.242  0.158  0.173  0.073  0.161  0.195  0.195  0.186  0.099                     
french      -0.265  0.079  0.314 -0.002  0.257  0.268  0.290  0.305  0.144  0.094              
i.r.e       -0.256  0.038  0.349  0.041  0.253  0.251  0.231  0.242  0.049  0.083  0.137       
historycsq   0.153 -0.253 -0.235 -0.107 -0.275 -0.204 -0.202 -0.231 -0.106 -0.112 -0.119 -0.102
fit warnings:
fixed-effect model matrix is rank deficient so dropping 1 column / coefficient
</code></pre>
"
"0.0593390829096927","0.0593913870916499","213628","<p>I am analyzing a multilevel dataset with an AR(1) error structure and random intercept and slope.  I fit what I believe is the exact same model in SPSS and R- my coefficients and standard errors are identical to the third decimal point, but my degrees of freedom are dramatically different.  I have been able to find some information on how nlme calculates df, but not how SPSS does it.  Given that these analyses are for a scientific paper and thus p-values matter (yes I know), I need to figure out what is going on and which DF to trust.</p>

<p>Here is my code for SPSS:</p>

<pre><code>MIXED BF_lnLP_incr BY   moduleJustDone WITH 
    BF_RSPcl_incr timePoint
  /CRITERIA=CIN(95) MXITER(1000) MXSTEP(10) SCORING(1) SINGULAR(0.000000000001) HCONVERGE(0, 
    ABSOLUTE) LCONVERGE(0, ABSOLUTE) PCONVERGE(0.000001, ABSOLUTE)
  /FIXED=BF_RSPcl_incr moduleJustDone timePoint | SSTYPE(3)
  /METHOD=REML
  /PRINT=CORB COVB G  LMATRIX R SOLUTION TESTCOV
  /RANDOM=INTERCEPT timePoint  | SUBJECT(subjID) COVTYPE(VC)
  /REPEATED=timePoint | SUBJECT(subjID) COVTYPE(AR1).
</code></pre>

<p>Results for SPSS:</p>

<pre><code>Type III Tests of Fixed Effects             
Source          Numerator df    Denominator df    F      Sig.
Intercept       1               725.420         15.262  .000
BF_RSPcl_incr   1               989.998         490.641 .000
moduleJustDone  3               876.414         1.334   .262
timePoint       1               413.480          .013   .909
</code></pre>

<p>Code for R:</p>

<pre><code>lme(BF_lnLP_incr~as.factor(moduleJustDone)+timePoint+BF_RSPcl_incr,random=~timePoint|subjID,
          correlation = corAR1(,form=~timePoint|subjID),
          data=data,na.action=na.omit,method=""REML"")
</code></pre>

<p>Results for R:</p>

<pre><code>                          numDF denDF  F-value p-value
(Intercept)                   1   712 299.8204  &lt;.0001
as.factor(moduleJustDone)     3   712   6.9441  0.0001
timePoint                     1   712   5.2299  0.0225
BF_RSPcl_incr                 1   712 490.5761  &lt;.0001
</code></pre>
"
"0.118678165819385","0.1187827741833","215532","<p>I am a clinicians (limited statistical knowledge) who is trying to use mlogit pkg in R to analyze a clinical dataset, running logistic regression on it. I am trying to ascertain if there is any correlation seen in patients with many vars and heart block (var = block (0s and 1s))</p>

<p>I have a dataset with these variables</p>

<pre><code> [1] ""Age""                    ""Sex""                    ""Race""                       ""Obesity""               
 [5] ""CAD""                    ""HTN""                    ""DM""                     ""HLD""                   
 [9] ""CHF""                    ""COPD""                   ""Asthma""                 ""Thyroid.disorder""      
[13] ""Smoking""                ""Illicit.drug.use""       ""Alcohol""                ""INR""                   
[17] ""TB""                     ""AST""                    ""ALT""                    ""Cirrhosis""             
[21] ""Adenosine""              ""Amiodarone""             ""Beta.blocker""           ""CCB""                   
[25] ""Digoxin""                ""TCA""                    ""SSRI""                   ""Antipsychotic""         
[29] ""AV.block""               ""Bundle.branch.block""    ""PAC.PVC""                ""Afib""                  
[33] ""Other.Arrythmia""        ""Nonspecific.ST.wave""    ""Anterioseptal..ST.wave"" ""Anteriolateral.ST.wave""
[37] ""Inferior.ST.wave""       ""Posterior.ST.wave""      ""Axis.deviation""         ""Low.voltage""           
[41] ""Qt.prolongation""        ""Hypertrophy""            ""block""
</code></pre>

<p>Now I have used the mlogit pkg in R</p>

<p>My Code is</p>

<pre><code># Reshaping data
mydata &lt;- mlogit.data(data=mydata, shape=""wide"", choice=""block"")

# Creating Model with all Vars
model &lt;- mlogit(data=mydata, formula=block~0|Age+Sex+Race+Obesity+CAD+HTN+DM+HLD+CHF+COPD+Asthma+Thyroid.disorder+Smoking+Illicit.drug.use+Beta.blocker+CCB+Digoxin+TCA+SSRI+Antipsychotic+Hypertrophy+INR+TB+AST+ALT)
</code></pre>

<p>Generates this error :</p>

<pre><code> Error in solve.default(H, g[!fixed]) : 
 Lapack routine dgesv: system is exactly singular: U[43,43] = 0
</code></pre>

<p>Now if I break the variables into different variables like..</p>

<pre><code>model1 &lt;- mlogit(data=mydata, formula=block~0|Age+Sex+Race+Obesity+CAD+HTN+DM+HLD+CHF+COPD+Asthma+Thyroid.disorder+Smoking)
model2 &lt;- mlogit(data=mydata, formula=block~0|Illicit.drug.use+Beta.blocker+CCB+Digoxin+TCA+SSRI+Antipsychotic+Hypertrophy)
model3 &lt;- mlogit(data=mydata, formula=block~0|INR+TB+AST+ALT)
</code></pre>

<p>IT WORKS without any errors.</p>

<p>But here I would like to know,
how breaking into different models would change my Coefficients?
What should I do to avoid the error and try to incorporate all variables in one model?
How should I interpret my results if break into 3 different models as opposed to one?</p>

<p>Any help is highly appreciated.</p>
"
"0.0726752374667264","0.0484928644968872","217926","<p><em>Will the signs of coef (Estimate) of lm and glm always be the same?</em> <strong>^</strong></p>

<p>According to below toy example, it seems yes. Can you provide a case where they might be different? (If it matters in my real data the outcome is binary, hence used <code>mpg &gt; 20</code>)</p>

<pre><code># dummy data
d &lt;- mtcars

# fit lm, glm, glm_bi
fit_lm &lt;- lm(mpg &gt; 20 ~ cyl + disp, data = d)
fit_glm &lt;- glm(mpg &gt; 20 ~ cyl + disp, data = d)
fit_glm_bi &lt;- glm(mpg &gt; 20 ~ cyl + disp, family = binomial, data = d)

# Signs are always same?
# lm compared to glm
all.equal(sign(coef(fit_lm)),
          sign(coef(fit_glm)))
# output
# [1] TRUE

# lm compared to glm(family = binomial)
all.equal(sign(coef(fit_lm)),
          sign(coef(fit_glm_bi)))

# output
# [1] TRUE
</code></pre>

<p><strong>^</strong> Very much sounds like a dupe, found this similar post: <a href=""http://stats.stackexchange.com/questions/91666/sign-of-coefficients-in-linear-regression-vs-the-sign-of-correlation"">Sign of coefficients in linear regression vs. the sign of correlation</a>. Let me know if this is a dupe.</p>
"
"0.111013258946721","0.111111111111111","218970","<p>For a specific analysis I want to calculate the <em>variance partition coefficient</em> (VPC). I am using the following formula:</p>

<pre><code> test &lt;- glmer(SocEenz ~ Herkomst + OuderPersoon + statusscore14 + M_SpoWeek + 
             (1|POSCODN), data = dataScaled, family = binomial)

&gt; summary(test)
     Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
       Family: binomial  ( logit )
       Formula: SocEenz ~ Herkomst + OuderPersoon + statusscore14 + M_SpoWeek +  (1 | POSCODN)
       Data: dataScaled

   AIC      BIC   logLik deviance df.resid 
  43707.5  43757.9 -21847.8  43695.5    32684 

Scaled residuals: 
   Min      1Q  Median      3Q     Max 
 -1.3263 -0.8378 -0.7164  1.1263  2.4788 

  Random effects:
 Groups  Name        Variance Std.Dev.
 POSCODN (Intercept) 0.007148 0.08455 
 Number of obs: 32690, groups:  POSCODN, 173

Fixed effects:
          Estimate Std. Error z value Pr(&gt;|z|)    
 (Intercept)   -0.56437    0.01798 -31.387  &lt; 2e-16 ***
 Herkomst1      0.49571    0.02980  16.633  &lt; 2e-16 ***
 OuderPersoon1  0.29911    0.02433  12.295  &lt; 2e-16 ***
 statusscore14 -0.09900    0.01353  -7.316 2.56e-13 ***
 M_SpoWeek     -0.08658    0.01225  -7.067 1.58e-12 ***

 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

   Correlation of Fixed Effects:
        (Intr) Hrkms1 OdrPr1 stts14
Herkomst1   -0.436                     
OuderPersn1 -0.553  0.168              
statusscr14 -0.068  0.233  0.013       
M_SpoWeek   -0.044 -0.029  0.138 -0.034
</code></pre>

<p>Because I only have information in the outcome about POSCODN at random effects I am not sure how to calculate the VPC. How can I get an extra row there with information about residuals. For example: </p>

<pre><code>  Random effects:
 Groups  Name        Variance Std.Dev.
 POSCODN (Intercept) 0.007148 0.08455 
 Residual             258.357 16.0735 
 Number of obs: 32690, groups:  POSCODN, 173
</code></pre>

<p>Or do you have other suggestions how to calculate the VPC?</p>

<p>Thanks!</p>
"
"0.178017248729078","0.168275596759675","220001","<p>I'm trying to model a logistic regression in R between two simple variables:</p>

<ul>
<li>Rating: An independent ordered categorical one, ranging from 1 to 99 (1, 2, 3, 4, 5, 99 in particular, 1 is the best)</li>
<li>Result: A dependent binary variable (0-1, not accepted/accepted)</li>
</ul>

<p>The formula I use is </p>

<pre><code>glm(formula = result_dummy ~ best_rating, family = binomial(link = ""logit""), 
    data = cd[1:10000, ])
</code></pre>

<p>result_dummy is a 0/1 numerical variable (original result column was a factor) and scaled_rating is the rating column after use the R <code>scale</code> function.</p>

<p>My thought here was to find a negative correlation (low rating -> more probability to accept) but the more samples I use the more odd results I find:</p>

<pre><code>10 samples:

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)     0.6484     0.7413   0.875    0.382
scaled_rating  -5.9403     5.8179  -1.021    0.307
</code></pre>

<hr>

<pre><code>100 samples:
Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)   -0.09593    0.27492  -0.349  0.72714   
scaled_rating -5.06251    1.76645  -2.866  0.00416 **
</code></pre>

<hr>

<pre><code>1000 samples:

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -0.03539    0.09335  -0.379    0.705    
scaled_rating -6.81964    0.62003 -10.999   &lt;2e-16 ***
</code></pre>

<hr>

<pre><code>10000 samples:
Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)     0.2489     0.0291   8.553   &lt;2e-16 ***
scaled_rating  -7.2319     0.2004 -36.094   &lt;2e-16 ***
</code></pre>

<hr>

<p>Notes:
I know that after the fit I should check residual plot, normality assumptions, etc. etc. but nonetheless I find really strange this behaviour.</p>

<p>I also have similar results using simply the rating column instead of the scaled one.</p>

<p>Edit:
The <code>rating</code> variable is not really an ordinal one, so as pointed out by @Scortchi maybe it would be better to treat it as a categorical one.
I have surely better results and model stability, obviously the model is a simple one and the residual error would be always high (because some variables as not been included in the model).
Indeed, including the frequency table as requested shows that the rating variable IS NOT sufficient for having a clear separation between the result outcome.</p>

<pre><code>          0      1
  1    2881  42564
  2   13878 129292
  3   36839 179500
  4   43511  97148
  5   37330  47002
  6   31801  21228
  7   19096   6034
  99  10008      3
</code></pre>
"
"0.0593390829096927","0.0593913870916499","223176","<p>I have been teaching myself how to use RWeka, specifically so that I may implement the M5P model. I have been able to use apply to my data, but do not understand what the percentage represents. For example, the beginning of the sample output from RWeka's manual is: </p>

<pre><code>M5 pruned model tree:
(using smoothed linear models)

CHMIN &lt;= 7.5 : LM1 (165/12.903%)
</code></pre>

<p>The other LMs have other ""scores"" like this, like (6/18.551%) and (23/48.302%). What exactly do these percentages and numbers represent?</p>

<p>Also, if they do not represent the accuracy or success of the predictions, then how can I access such a value? I'm thinking about the correlation coefficient classically shown in machine learning papers.</p>
"
"0.0419590679148345","0.0419960525565808","223264","<p>I am studying the relationship of two variables while controlling for the effect of another two variables. The correlation coefficient without controlling anything was 0.441, but the partial correlation coefficient (after controlling) became negative, with a value of -0.22. I feel it is a bit strange to get such a result.</p>

<p>Has anyone encountered something like this situation? How could it be explained?</p>
"
"0.173001668965327","0.162968621693904","223439","<p>I am running an analysis on a national sample of 20,000, representative at the province level (34 provinces)</p>

<p>After checking for linearity and normality of my dependent variable I have run a preliminary OLS in order to see how the covariates perform in explaining the variation of the variable of interest. 
I have selected the relevant independent variables following accreditee literature in my field of analysis, explored the covariance matrix in order to avoid problems of multicollinearity etc..</p>

<p>The result from the OLS is good in term of significance level of the coefficients, the sign and the magnitute of the latter fullfil my expectations and match the results find by other analysis.
However, the value of <code>R^2</code> is quite low: only <code>0.09</code> . Thus, knowing that some variation could be explained by the differences between provinces I have first estimate the OLS adding first provincial dummies and after distric dummies (398 districts).</p>

<p>The <code>R^2</code> improved much, reaching respectively the <code>36%</code> and <code>41%</code>.
However, what I would like to see are the underling cause of the regional differences: why do they perform differently? </p>

<p>Among the variables I have some take a unique value according to each observation's province. I cannot use them in the OLS while using the province dummies because there would be perfect collinearity.</p>

<p>In my view using a mixed linear model would help.</p>

<p>I have run a random intercept null.model in which only the dependent variable is regressed against an intercept. For the estimation I have used the command <code>lmer</code> from the <code>{lme4}</code>package in <code>R</code>.</p>

<p>The InterCorrelation Coeffient equals <code>0.30</code>, suggesting that the 30% of the variation happens between groups, the values of the group-mean reliance are quite high too (not less than <code>0.9</code>)- I repeat myself: the sample is province representative.</p>

<p>I finally run a set of random mixed intercept model with 2 levels:</p>

<p>where: <code>i</code> indicates the household and <code>j</code> indicates the province.</p>

<ol>
<li><code>Y_i = beta0j + beta1 X_i + e_ij</code> </li>
<li><code>Y_i = beta0j + beta1 X_i + beta2 Z_j + e_ij</code></li>
<li><code>Y_i = beta0j + beta1 X_i + beta2 W_j + e_ij</code></li>
</ol>

<p>The <strong>first question</strong> is: am I allowed to include a variable which varies only at the provincial level (as Z and W do) given that their coefficient is not computed by taking in cosideration the groups?
As far as I have understood it would be a mistake to use those variables in the random part of the model in order to get random coefficients.</p>

<p>The <strong>second question</strong>: given that by running the command <code>anova()</code>, also in <code>lme4</code>, model 1 is statistically different from model 2, and model 2 is  statistically not different from model 3, can I say that Z and W have the same power in explaing the variation in Y despite the fact that Z's coefficient is significant and W's is not significant?</p>

<p>Think as if Z and W were proxies for the same dimension. They are in fact statistically and conceptually high correlated.</p>

<p>Sorry but I cannot give more details on the actual problem I am woking on.</p>

<p>Thanks in advance. </p>
"
"0.173001668965327","0.173154160549773","223918","<p>First time doing a meta-analysis, and I'd prefer not to botch this.</p>

<p>Out of a set of studies, I have six studies that each have used a pair of measures (on the same participants) that are commonly averaged to one value. However, there are theoretical reasons that it might be better to use these measures separately. I am testing this by comparing whether the effect sizes from measure 1 are different from the effect sizes from measure 2. </p>

<p>So I have 12 correlations from 6 studies, which introduces dependency between correlations from the same study.</p>

<p>Using the <code>metafor</code> package, I use <code>rma.mv</code> and the <code>random</code> argument to account for the dependency:</p>

<pre><code>pd &lt;- escalc(measure=""COR"", ri=r_Pur, ni=n, data=d)
rp &lt;- rma.mv(yi, vi, data=pd, mods= ~ compare, random= ~ compare | source)
</code></pre>

<p>where the <code>source</code> denotes the study, with six levels, and the <code>compare</code> denotes the measure, with two levels.</p>

<p>The results I get are:</p>

<pre><code>Multivariate Meta-Analysis Model (k = 12; method: REML)

Variance Components: 

outer factor: source  (nlvls = 6)
inner factor: compare (nlvls = 2)

            estim    sqrt  fixed
tau^2      0.0040  0.0634     no
rho        1.0000             no

Test for Residual Heterogeneity: 
QE(df = 10) = 22.9376, p-val = 0.0110

Test of Moderators (coefficient(s) 2): 
QM(df = 1) = 69.7315, p-val &lt; .0001

Model Results:

               estimate      se     zval    pval    ci.lb    ci.ub     
intrcpt          0.4341  0.0331  13.0975  &lt;.0001   0.3691   0.4990  ***
compareConsEc   -0.2374  0.0284  -8.3505  &lt;.0001  -0.2931  -0.1817  ***

---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 


Warning messages:
1: In rma.mv(yi, vi, data = pd, mods = ~compare, random = ~compare |  :
  Redundant predictors dropped from the model.
</code></pre>

<p>I'm interpreting that there is a difference between the two measures, as the model results row for <code>compareConsEc</code> is much smaller than the intercept (it read somewhere that the use of Q statistic is not recommended). However, the warning indicates that a predictor has been dropped, and since the <code>compare</code> is there but the estimate for the study-level is not, I assume the study-level was dropped. Yet, when I run the same analysis in <code>rma</code> without the random argument, the results are not (exactly) the same, which suggests that it is not <em>simply</em> dropping the random argument but doing something else differently as well.</p>

<p>My questions are:</p>

<ul>
<li>Is the analysis and syntax correct in principle?</li>
<li>Reading <a href=""http://stats.stackexchange.com/questions/158546/metafor-rma-mv-function-missing-estimates-for-two-levels-of-a-categorical-moder"">this</a>, I interpret that I have too few datapoints to estimate a two-level model. Does that mean that it is simply not possible to take the study-level dependency into account in the analysis? </li>
<li>If so, does that mean I should run a <code>rma</code> without the <code>random</code> argument instead, acknowledging in the report that this dependency might confuse the results? (and if so, to which direction?) Or is there a better option, such as an alternative analysis?</li>
<li>Why are the results from <code>rma</code> and <code>rma.mv</code> different?</li>
<li>Is the <code>tau^2</code> useful in that table, with a predictor being dropped and the dependency being unaccounted for? Why is the <code>rho</code> 1?</li>
<li>I used REML method, as a default. What would be a good source to the respective pros and cons of different methods?</li>
</ul>
"
"0.0839181358296689","0.0839921051131616","224078","<p>I have three related questions on the package CausalImpact in R. The package can be found <a href=""https://github.com/google/CausalImpact"" rel=""nofollow"">here</a> and a reproducible example is below.</p>

<ol>
<li>Do I basically understand correctly, that the model makes ""1-step ahead""
predictions? I assume it works like a simple lm model that makes
lots of regressions for t+1 with predictors values from t-1 and then looks for the most contributory predictors?</li>
<li>When talking about ""coefficients"", does that mean they are (Pearson)
correlation coefficients?</li>
<li>The function <code>plot(impact$model$bsts.model,""coefficients"")</code> produces
a plot with inclusion probabilities ranging from 0 to 1. Is there
any way to access the actual values in a table? I found
that <code>colMeans(impact$model$bsts.model$coefficients)</code> provides some
values but I'd like to have a confirmation for this.</li>
<li>In my code, I changed <code>bsts.model &lt;- bsts(y ~ x1, ss, niter = 1000)</code> to <code>bsts.model &lt;- bsts(y ~ x1 + x2 + x3 + x4 + x5, ss, niter = 1000)</code> in order to get values for different variables that I defined before. I did not change the cbind functions for that, as I weren't sure if that was necessary. Now I wonder what to do when I do have a table with -let's say- 200 predictors? Do I have to enter x1 to x200 after <code>bsts(y ~</code> to find the best predictors?</li>
</ol>

<p>R code below:</p>

<pre><code>install.packages(""devtools"")
library(devtools)
devtools::install_github(""google/CausalImpact"")
#Download the tar from the git and then install the package in RStudio.

library(CausalImpact)

set.seed(1)
x1 &lt;- 100 + arima.sim(model = list(ar = 0.999), n = 100)
x2 &lt;- 50 + arima.sim(model = list(ar = 0.899), n = 100)
x3 &lt;- 80 + arima.sim(model = list(ar = 0.799), n = 100)
x4 &lt;- 1.25 * x1 + rnorm(100)
x5 &lt;- 101 + arima.sim(model = list(ar = 0.999), n = 100)
y &lt;- 1.2 * x1 + rnorm(100)
y[71:100] &lt;- y[71:100] + 10
data &lt;- cbind(y, x1)

dim(data)
head(data)
data
matplot(data, type = ""l"")

time.points &lt;- seq.Date(as.Date(""2014-01-01""), by = 1, length.out = 100)
data &lt;- zoo(cbind(y, x1), time.points)
head(data)

pre.period &lt;- as.Date(c(""2014-01-01"", ""2014-03-11""))
post.period &lt;- as.Date(c(""2014-03-12"", ""2014-04-10""))

impact &lt;- CausalImpact(data, pre.period, post.period)
plot(impact)

summary(impact)
summary(impact, ""report"")
impact$summary

post.period &lt;- c(71, 100)
post.period.response &lt;- y[post.period[1] : post.period[2]]
y[post.period[1] : post.period[2]] &lt;- NA

ss &lt;- AddLocalLevel(list(), y)

bsts.model &lt;- bsts(y ~ x1 + x2 + x3 + x4 + x5, ss, niter = 1000)

impact &lt;- CausalImpact(bsts.model = bsts.model,post.period.response =     post.period.response)

plot(impact)
summary(impact)
summary(impact, ""report"")

plot(impact$model$bsts.model,""coefficients"")
plot(impact$model$bsts.model, ""coef"", inc = .1)
plot(impact$model$bsts.model, ""coef"", inc = .05)

colMeans(impact$model$bsts.model$coefficients)
</code></pre>
"
"0.15699645640569","0.157134840263677","225283","<p>Iâ€™m analyzing crowdsourced Twitter data, where workers labeled tweets. Within my dataset (N=2,400), I have one IV (call it â€˜dsâ€™) with 2 levels that differentiates which dataset the workers labeled. I have four factors of interest (what workers labeled) -- these are my DVs (let's call them f1, f2, f3, f4). Three of those factors are binomial &lt;0,1>, and one multinomial &lt;0, 1, 2>. Even though the latter can be treated as ordinal, I'm working under the assumption it is nominal. Finally, my datasets are of unequal lengths.</p>

<p>My goal is to analyze the relationship between each of the labeled factors for each level of the IV. More specifically, <strong>I want to tease out the different contributions of each of those factors on each dataset quantitatively, i.e., show amount of variance explained</strong> (e.g., ds1 influenced f1 more than f2, while the inverse for ds2). The end game is to model each factor into a scoring function, which allows me to compute a unified score. Hence, I need to back up the parameter weights for this function.</p>

<p>A snippet of my data frame looks like this:</p>

<pre><code>   f1 f2 f3 f4 ds
1   1  0  1  0  1
2   0  0  0  2  1
3   0  0  1  1  2
4   1  1  0  2  2
</code></pre>

<p>What I initially did was to compute correlations between each factor, and used the strengths of those correlations to back up my scoring function. However, given the many posts and tutorials I've been reading, it seems I need to make use of a mix of logistic and multinomial regression. What I have done so far is run binomial logit (using ?glm with class â€˜binomial') on the first 3 factors, and multinomial regression (using ?nnet) on f4. However, it seems I can only assess one outcome variable at a time.</p>

<p>For f1-f3, I have run the following R code:</p>

<pre><code>fit &lt;- glm(f1 ~ ds, data = xx, family = ""binomial"")
summary(fit)
confint.default(fit)
wald.test(b = coef(fit), Sigma = vcov(fit), Terms = 2)
</code></pre>

<p>For f4:</p>

<pre><code>fit &lt;- multinom(f4 ~ ds, data = xx)
summary(fit)
z &lt;- summary(fit)$coefficients/summary(fit)$standard.errors
p &lt;- (1 - pnorm(abs(z), 0, 1))*2
</code></pre>

<p>My questions:</p>

<p><strong>1.</strong> Is running such logistic regression analyses appropriate for what I want to do, namely to tease out contributions of each factor? If so, is it meaningful to compare the coefficients of each factor with the other, when computed separately? Or is simply showing a correlation matrix sufficient in my case?</p>

<p><strong>2.</strong> Are there alternative techniques to assess all outcome variables/DVs at once, with respect to each level of my IV? If so, could you please provide me with some pointers (ideally for R)? I'm now looking into hierarchical multinomial marginal (HMM) models... </p>

<p>If something is unclear above, Iâ€™d be happy to clarify.</p>
"
"NaN","NaN","225645","<p>I am trying to perform a logistic regression with the following code </p>

<blockquote>
  <p><code>Y ~ x1+x2+x3,data=data, family=binomial(link=""logit"")</code>. </p>
</blockquote>

<p>However on inspection of both the outcome and predictors i noticed that they are characterized by spatial auto-correlation. My question is, how do I account for the spatial auto-correlation, to get better coefficients? </p>
"
"0.10277830647413","0.102868899974728","225985","<p>I am a teacher in a language program at a university, and I was interested in investigating whether or not there is a correlation between the number of sessions a student spends in our program and their performance on the TOEFL (an English proficiency test). I have test scores from students who took this test before entering our program, and test scores from the same students who took the test again after spending 1 - 4 sessions in our program. </p>

<p>To be clear, these students did not take the test again after each session, so for any given student, I only have their pre-program test score and an additional score from when they took the test again (either after 1, 2, 3, or 4 sessions in our program).</p>

<p>Is there a way to analyze this data to learn whether or not the time spent in our program correlates to an improve in performance on the TOEFL test? My experience with statistics of this kind is limited, but I thought I could use the Kruskal-Wallis test to analyze the gain scores between pre and post tests between the four groups of students (students who spent either 1, 2, 3, or 4 sessions in our program) to see if there is a meaningful connection. A co-worker thought I should use Kendall's tau coefficient instead, which I have not used before.  </p>

<p>Would either of these statistics be appropriate for this situation? This is not data from an experiment, and the students are all from different populations and took this test in different locations after studying with different teachers in our program, so I realize that there are many confounding variables. Any guidance would be greatly appreciated! </p>
"
"0.0839181358296689","0.0839921051131616","226069","<p>I just need some clarification regarding the interpretation of the Spearman's Rank Correlation Coefficient output in R. I am currently determining correlations over a tri-nominal temporal scale in an ecological setting. My basic code is as below: </p>

<blockquote>
  <p>cor.test(v1,v2,method=""spearman"")</p>
</blockquote>

<p>and the example output is as follows:</p>

<pre><code>Spearman's rank correlation rho
</code></pre>

<blockquote>
  <p>data:  v1 and v2
  S = 466770, p-value = 0.4601
  alternative hypothesis: true rho is not equal to 0
  sample estimates:
        rho 
  0.06203443 </p>
</blockquote>

<p>I understand the output for the rho and p values however i cannot find a definitive answer for what 'S' is? Am i required to report this value? Any clarification will be much welcomed. </p>

<p>Thanks</p>
"
"0.0419590679148345","0.0419960525565808","226747","<p>I am trying to wrap my head around the notation for this three-level model. </p>

<p>Level 1: Repeated observations
Level 2: Client
Level 3: Therapist</p>

<p>I am using a baseline intercept model to calculate intra-class correlation coefficients. In order to partition the variance at both the client and therapist level, I have random effects listed for both. </p>

<p>lme4 code:</p>

<pre><code>mod01 &lt;- lmer(var ~ 1 + (1 | client) + (1 | therapist), data = dat10)
</code></pre>

<p>And my notation thus far is:</p>

<p>(ð‘‰ð‘Žð‘Ÿð‘–ð‘Žð‘ð‘™ð‘’)time,client,therapist = ð›¾00 + ð‘¢client + ð‘¢therapist + ð‘’client,therapist</p>

<p>But is that error term correct? I'm trying to keep this as simple as possible for my audience by using the combined equation. Any feedback is greatly appreciated. </p>
"
"0.139162484826546","0.139285149006224","228641","<p>Consider the following dataset I want to use as the independent variables to conduct linear regression on:</p>

<pre><code>set.seed(42)
sa = runif(10)
sb = runif(10)
sc = sb+sa
sd = sb-sa
df = data.frame(sa,sb,sc,sd)
</code></pre>

<p>Now I want to perform tests for multicollinearity. I'm aware of the <code>ppcor</code> package, which calculates the partial correlation between the variables. In this case:</p>

<pre><code>&gt; pcor(df)
$estimate
            [,1]        [,2]       [,3]       [,4]
[1,]  1.00000000  0.06649968 -0.7325597  0.7706902
[2,]  0.06649968  1.00000000 -0.6304810 -0.6870502
[3,] -0.73255975 -0.63048097  1.0000000  0.1308260
[4,]  0.77069021 -0.68705016  0.1308260  1.0000000
</code></pre>

<p>As far as I know, there is no way of telling that <code>sc</code> and <code>sd</code> are linear combinations of <code>sa</code> and <code>sb</code>, just by looking at the estimates (or the other outputs of <code>pcor</code>, for that matter).</p>

<p>The only method that comes to my mind, is applying linear regression on each of the independent variables like so:</p>

<pre><code>summary(lm(sc~sa+sb,df))

Coefficients:
             Estimate Std. Error   t value Pr(&gt;|t|)    
(Intercept) 1.404e-16  3.915e-17 3.587e+00   0.0089 ** 
sa          1.000e+00  4.716e-17 2.121e+16   &lt;2e-16 ***
sb          1.000e+00  4.135e-17 2.418e+16   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 3.672e-17 on 7 degrees of freedom
Multiple R-squared:      1, Adjusted R-squared:      1 

summary(lm(sd~sa+sb,df))

Coefficients:
             Estimate Std. Error   t value Pr(&gt;|t|)    
(Intercept) 1.404e-16  3.915e-17 3.587e+00   0.0089 ** 
sa          1.000e+00  4.716e-17 2.121e+16   &lt;2e-16 ***
sb          1.000e+00  4.135e-17 2.418e+16   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 3.672e-17 on 7 degrees of freedom
Multiple R-squared:      1, Adjusted R-squared:      1 
</code></pre>

<p>I'm wondering two things: </p>

<ol>
<li><p>Is my approach with linear regression reasonable? I think the downside is, that it can only detect linear correlation. But non-linear correlation shouldn't be a problem with linear regression, right?</p></li>
<li><p>Is there an R function/package that automatically checks for multiple correlation?</p></li>
</ol>
"
"0.151285570837612","0.151418920859833","229598","<p>This isn't a problem with correlation between predictors - I have two models, each considers only one of the variables. That is the only difference between the models.  </p>

<p>I'm estimating the probability of an diagnosis given some confounders and a measure of monthly temperature. I have two possible temperature definitions I'm considering: monthly average temperature and monthly average <em>high</em> temperature. I don't expect the response to temperature to be linear, so I broke average temperature into 5 degree bins with bottom and top coding at &lt; 40 and > 90. I did the same with average high temperature but shifted the bins slightly with bottom and top coding &lt; 50 and > 100. </p>

<p>I estimate the first logistic model </p>

<pre><code>event ~ age + sex + ... + mean_temp_group
</code></pre>

<p>and get the response I'd expect from my theorized process. However, I'd prefer to report the results using mean high temperature since average temperature is misleadingly low (average temp of 70, for instance, is pretty warm with highs in the 80s but people think ""70 degrees? That's wonderful!""). So I estimate the same model but instead replace <code>mean_temp_group</code> with <code>mean_high_group</code>:</p>

<pre><code>event ~ age + sex + ... + mean_high_group
</code></pre>

<p>and the results don't match either my theory or what I saw with <code>mean_temp_group</code>. </p>

<p>That seems weird given how similar the two variables are. The average and average high variables have a correlation coefficient of 0.9939. In essence the average high is the average plus a constant (on average, 9.4 degrees with a standard deviation of 2.1). </p>

<p>At first I assumed this was a problem with the code, so I re-pulled the data (still have the same problem and the data extraction seems to be accurate). I also took the model with <code>mean_temp_group</code> and edited the formula in place to read <code>mean_high_group</code> lest I omitted/included a different variable between the models (I didn't). </p>

<p>I assume it has something to do with the binning or something along those lines - any ideas? I'm very confused by two variables that basically appear to be an additive shift of each other giving very different results. </p>
"
"0.0419590679148345","0.0419960525565808","230372","<p>I was wondering if someone on here could help</p>

<p>I recently ran a Spatial Durbin Regression model in R which came back with two of my three independent variables had significant beta coefficients. A colleague then advised me that I should run a sensitivity test using a negative binomial model to see if I get the same results. However the results are different as all my beta coefficients become significant.</p>

<p>What I am trying to understand is would this likely be due to the incorporation of the spatially lagged independent variable (which has a significant rho value in the spatial Durbin regression)? And does it make sense to use the negative binomial as a 'sensitivity test' as I would think the assumptions would be different, particularly around spatial autocorrelation. </p>
"
"0.118678165819385","0.1187827741833","230532","<p>Trying to get the Bayes Factor for a correlation between two variables in my data, I tried three different functions. All implement the Jeffreysâ€“Zellnerâ€“Siow (JZS) prior, but I get quite different results with the three approaches. Two questions:</p>

<ol>
<li><p>Is this suspicious, or is it reasonable that they produce different values, as the implementations are slightly different?</p></li>
<li><p>Is there a consensus on the best measure to use?  </p></li>
</ol>

<p>My data:</p>

<pre><code>a=rnorm(100,1,2)
b=rnorm(100,.8,1.5)
myData &lt;- data.frame(a=a, b=b)
</code></pre>

<p>I try the <code>jzs_corbf</code> function, described and implemented <a href=""http://www.ncbi.nlm.nih.gov/pubmed/22798023"" rel=""nofollow"">here</a> (<a href=""http://dsquintana.com/post/98962697485/how-to-calculate-a-bayes-factor-for-correlations"" rel=""nofollow"">shorter version</a>)</p>

<pre><code>cor.resu.a_b &lt;- cor.test(myData$a, myData$b, method=c(""pearson""))
cor.resu.a_b$estimate
n = 100
r = cor.resu.a_b$estimate
jzs_corbf(r,n)
[1] 0.08206358
</code></pre>

<p>I also tried the convenience function from the <code>BayesFactor</code> package:</p>

<pre><code>require(BayesFactor)
regressionBF(b ~ a, data = myData, progress=FALSE)

Bayes factor analysis
--------------
[1] a : 0.2181081 Â±0%

Against denominator:
  Intercept only 
---
Bayes factor type: BFlinearModel, JZS
</code></pre>

<p>And I also tried the a function described <a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4891395/"" rel=""nofollow"">recently</a> (<a href=""https://osf.io/9d4ip/"" rel=""nofollow"">code</a>)</p>

<pre><code>bf10JeffreysIntegrate(n=100, r=r)

      cor 
0.1297927
</code></pre>

<p>While in this case the differences are only numerical, in my real data I get quite big differences that make it more difficult to decide on an interpretation. </p>

<p><a href=""http://stats.stackexchange.com/questions/184950/calculating-bayes-factor-from-a-correlation-coefficient"">Related</a></p>
"
"0.133237935355665","0.13335537736644","231829","<p>I'm trying to see if there is a correlation between the height of grass and the height under branches available for grass to grow. I have 227 paired observations:</p>

<pre><code>GrassHeight HeightUnderDebris
0            0
0            0
0            0
8            16
0            0
0            0
0            0
2            2
6            6
0            0
0            0
1            1
0            0
0            0
0            0
8            15
0            0
7            7
15           15
</code></pre>

<p>My data is not normally distributed and it fails at the assumption of bivariate normality:</p>

<pre><code>result&lt;-hzTest(data,cov = TRUE,qqplot = FALSE)
result&lt;-mardiaTest(data,cov = TRUE,qqplot = FALSE)
result&lt;-roystonTest(data,qqplot = FALSE)
</code></pre>

<p>Therefore, I need to use a Spearman's rho or Kendall's tau. Firstly, Spearman's rho results in a warning message:</p>

<pre><code>cor.test(GrassHeight, HeightUnderDebris, method=""spearman"")

Spearman's rank correlation rho

data:  GrassHeight and HeightUnderDebris
S = 123090, p-value &lt; 2.2e-16
alternative hypothesis: true rho is not equal to 0
sample estimates:
rho 
0.9368622 

Warning message:
In cor.test.default(GrassHeight, HeightUnderDebris, method = ""spearman"") :
Cannot compute exact p-value with ties
</code></pre>

<p>So I then decided to use Kendall's tau as it can deal with ties:</p>

<pre><code>cor.test(GrassHeight, HeightUnderDebris, method=""kendall"")

Kendall's rank correlation tau

data:  GrassHeight and HeightUnderDebris
z = 17.202, p-value &lt; 2.2e-16
alternative hypothesis: true tau is not equal to 0
sample estimates:
tau 
0.858494
</code></pre>

<p>Firstly, should I be concerned that my data has many zeros? They are important as they reflect that if there is no space under branches, then there is no space for grass growth hence why the grass height is 0.</p>

<p>Secondly, how would you interpret Kendall's results? Is it right that the two variables are uncorrelated at 0.05 significance level if their correlation coefficient is zero? In this case, tau is 0.858. That is not zero and will be rounded up to 1. Can I say that the two variables are correlated based on this?</p>

<p>Should I rather look at <code>rpudplus</code> and the function <code>rpucor</code>, which now uses Kendallâ€™s tau-b to compute the correlation coefficient?</p>

<p>What post-hoc test can I do to find out the nature of the correlation, i.e: as the height between the ground and branch increases, grass height increases?</p>
"
"0.251754407489007","0.237977631153958","231872","<p>For a better understanding of how r is conducting a logistic regression I created the following test-data (the two predictors and the criterion are binary variables):</p>

<pre><code>   UV1 UV2 AV
1    1   1  1
2    1   1  1
3    1   1  1
4    1   1  1
5    1   1  1
6    1   1  1
7    1   1  1
8    0   0  1
9    0   0  1
10   0   0  1
11   1   1  0
12   1   1  0
13   1   0  0
14   1   0  0
15   1   0  0
16   1   0  0
17   1   0  0
18   0   0  0
19   0   0  0
20   0   0  0
</code></pre>

<p>AV = dependent variable/criterion</p>

<p>UV1 / UV2 = both independant variables/predictors</p>

<p>For measuring the UVs effect on the AV a logistic regression is necessary, as the AV is a binary variable. Hence i used the following code</p>

<pre><code>&gt; lrmodel &lt;- glm(AV ~ UV1 + UV2, data = lrdata, family = ""binomial"")
</code></pre>

<p>including <strong>""family = ""binomial""""</strong>. Is this correct ( I think so :-))?</p>

<p>Regarding my test-data, I was wondering about the whole model, especially
the estimators and sigificance:</p>

<pre><code>&gt; summary(lrmodel)


Call:
glm(formula = AV ~ UV1 + UV2, family = ""binomial"", data = lrdata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.7344  -0.2944   0.3544   0.7090   1.1774  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -4.065e-15  8.165e-01   0.000    1.000
UV1         -1.857e+01  2.917e+03  -0.006    0.995
UV2          1.982e+01  2.917e+03   0.007    0.995

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 27.726  on 19  degrees of freedom
Residual deviance: 17.852  on 17  degrees of freedom
AIC: 23.852

Number of Fisher Scoring iterations: 17
</code></pre>

<ol>
<li><p>Why is UV2 not significant. See therefore that for group AV = 1 there are 7 cases with UV2 = 1, and for group AV = 0 there are only 3 cases with UV2 = 1. 
I was expecting that UV2 is a significant discriminator.</p></li>
<li><p>Despite the not-significance of the UVs, the estimators are - in my opinion- very high (e.g. for UV2 = 1.982e+01). How is this possible?</p></li>
<li><p>Why isn't the intercept 0,5?? We have 5 cases with AV = 1 and 5 cases with AV = 0.</p></li>
</ol>

<p>Further: I created UV1 as a predictor I expected not to be significant:  for group AV = 1 there are 5 cases withe UV1 = 1, and for group AV = 0 there are 5 cases withe UV1 = 1 as well.</p>

<p>The whole ""picture"" I gained from the logistic is confusing me...</p>

<p>What was consuming me more:
When I run a ""NOT-logistic"" regression (by omitting <strong>""family = ""binomial""</strong>)</p>

<pre><code>&gt; lrmodel &lt;- glm(AV ~ UV1 + UV2, data = lrdata,)
</code></pre>

<p>I get the expected results</p>

<pre><code>Call:
glm(formula = AV ~ UV1 + UV2, data = lrdata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.7778  -0.1250   0.1111   0.2222   0.5000  

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)   0.5000     0.1731   2.889  0.01020 * 
UV1          -0.5000     0.2567  -1.948  0.06816 . 
UV2           0.7778     0.2365   3.289  0.00433 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for gaussian family taken to be 0.1797386)

    Null deviance: 5.0000  on 19  degrees of freedom
Residual deviance: 3.0556  on 17  degrees of freedom
AIC: 27.182

Number of Fisher Scoring iterations: 2
</code></pre>

<ol>
<li>UV1 is not significant! :-)</li>
<li>UV2 has an positive effect on AV = 1! :-)</li>
<li>The intercept is 0.5! :-)</li>
</ol>

<p>My overall question: Why isn't logistic regression (including ""family = ""binomial"") producing results as expected, but a ""NOT-logistic"" regression (not including ""family = ""binomial"") does?</p>

<p>Update:
are the observations described above because of the correlation of UV1 and UV 2. Corr = 0.56
After manipulating the UV2's data </p>

<p>AV: 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0</p>

<p>UV1: 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0</p>

<p>UV2: <strong>0, 0, 0,</strong> 1, 1, 1, 1, <strong>1, 1, 1</strong>, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0</p>

<p>(I changed the positions of the three 0s with the three 1s in UV2 to gain a correlation &lt; 0.1 between UV1 and UV2) hence:</p>

<pre><code>UV1 UV2 AV
1    1   0  1
2    1   0  1
3    1   0  1
4    1   1  1
5    1   1  1
6    1   1  1
7    1   1  1
8    0   1  1
9    0   1  1
10   0   1  1
11   1   1  0
12   1   1  0
13   1   0  0
14   1   0  0
15   1   0  0
16   1   0  0
17   1   0  0
18   0   0  0
19   0   0  0
20   0   0  0
</code></pre>

<p>to avoid correlation, my results come closer to my expectations:</p>

<pre><code>Call:
glm(formula = AV ~ UV1 + UV2, family = ""binomial"", data = lrdata)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.76465  -0.81583  -0.03095   0.74994   1.58873  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)  -1.1248     1.0862  -1.036   0.3004  
UV1           0.1955     1.1393   0.172   0.8637  
UV2           2.2495     1.0566   2.129   0.0333 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 27.726  on 19  degrees of freedom
Residual deviance: 22.396  on 17  degrees of freedom
AIC: 28.396

Number of Fisher Scoring iterations: 4
</code></pre>

<p>But why does the correlation influence the results of the logistic regression and not the results of the ""not-logistic"" regression? </p>
"
"0.0839181358296689","0.0629940788348712","232433","<p>I'm doing a liner regression fit using R. I used <code>lm()</code> to do the regression. Then I standardize my data using <code>scale()</code> and again do the regression on standardize data using <code>lm()</code>.</p>

<p>Surprisingly the regression coefficient of one variable was positive before standardization and after standardization I found it is showing negative coefficient. I checked the correlation between that variable and my predictor. It has positive significant correlation.  </p>

<pre><code>data_bd2=data_2[,c(1:3,5:7)] 
str(data_bd2) 
fit_bd=lm(data_bd2) 
vif(fit_bd) 
summary(fit_bd) 
scale_data_bd2=data.frame(scale(data_bd2))
colnames(scale_data_bd2)=colnames(data_bd2) 
fit_bd_std=lm(scale_data_bd2) 
summary(fit_bd_std) 
</code></pre>

<p>Can you please help me understand why sign of regression coefficient differ before and after standardization?</p>
"
"0.0726752374667264","0.0727392967453308","232812","<p>There are 2 sides to this question. On the first side, we are trying to run PCA.</p>

<p>While running PCA on a dataset having 62 independent variables (IVs), I have found that first component (PC1) explains ~90% of variance and that of variables or parameters contributing to this component, a variable say V1 has highest contribution. This means that V1 plays a big role in capturing major chunk of variance in the whole dataset. And hence, this V1 is important.</p>

<p>On the second side, we have found correlations of all IVs with the dependent variable (DV) Y, so that we can drop IVs with the correlation coefficients (with Y) less than a particular threshold, say 0.2. This exercise is done so that we have less number of IVs for building a model. Here, we find that if we consider this threshold, we have to drop V1.</p>

<p>This to me is surprising because V1 is very important when you look at it from PCA point of view but is very less important when you look at it from point of view of explaining DV. </p>

<p>Looking at these 2 sides, should we drop V1 now and why? Please share your inputs. If you need any other input from my end, please do ask.</p>

<p>PS - we are doing this exercise in R.</p>
"
