"V1","V2","V3","V4"
"NaN","NaN","  2234","<p>I would like as many algorithms that perform the same task as logistic regression.  That is  algorithms/models that can give a prediction to a binary response (Y) with some explanatory variable (X).</p>

<p>I would be glad if after you name the algorithm, if you would also show how to implement it in R.  Here is a code that can be updated with other models:</p>

<pre><code>set.seed(55)
n &lt;- 100
x &lt;- c(rnorm(n), 1+rnorm(n))
y &lt;- c(rep(0,n), rep(1,n))
r &lt;- glm(y~x, family=binomial)
plot(y~x)
abline(lm(y~x),col='red',lty=2)
xx &lt;- seq(min(x), max(x), length=100)
yy &lt;- predict(r, data.frame(x=xx), type='response')
lines(xx,yy, col='blue', lwd=5, lty=2)
title(main='Logistic regression with the ""glm"" function')
</code></pre>
"
"0.0693931503088838","0.0676252226000574","  4830","<p>Full Disclosure: This is homework. I've included a link to the dataset ( <a href=""http://www.bertelsen.ca/R/logistic-regression.sav"">http://www.bertelsen.ca/R/logistic-regression.sav</a> )</p>

<p>My goal is to maximize the prediction of loan defaulters in this data set.  </p>

<p>Every model that I have come up with so far, predicts >90% of non-defaulters, but &lt;40% of defaulters making the classification efficiency overall ~80%. So, I wonder if there are interaction effects between the variables? Within a logistic regression, other than testing each possible combination is there a way to identify potential interaction effects? Or alternatively a way to boost the efficiency of classification of defaulters. </p>

<p>I'm stuck, any recommendations would be helpful in your choice of words, R-code or SPSS syntax. </p>

<p>My primary variables are outlined in the following histogram and scatterplot (with the exception of the dichotomous variable)</p>

<p>A description of the primary variables: </p>

<pre><code>age: Age in years
employ: Years with current employer
address: Years at current address
income: Household income in thousands
debtinc: Debt to income ratio (x100)
creddebt: Credit card debt in thousands
othdebt: Other debt in thousands
default: Previously defaulted (dichotomous, yes/no, 0/1)
ed: Level of education (No HS, HS, Some College, College, Post-grad)
</code></pre>

<p>Additional variables are just transformations of the above. I also tried converting a few of the continuous variables into categorical variables and implementing them in the model, no luck there. </p>

<p>If you'd like to pop it into R, quickly, here it is: </p>

<pre><code>## R Code
df &lt;- read.spss(file=""http://www.bertelsen.ca/R/logistic-regression.sav"", use.value.labels=T, to.data.frame=T)
</code></pre>

<p><img src=""http://i.stack.imgur.com/aVqtZ.jpg"" alt=""alt text"">
<img src=""http://i.stack.imgur.com/VQJDg.jpg"" alt=""alt text""></p>
"
"0.0400641540107502","0.0390434404721515","  9131","<p>Let's take the following example:</p>

<pre><code>set.seed(342)
x1 &lt;- runif(100)
x2 &lt;- runif(100)
y &lt;- x1+x2 + 2*x1*x2 + rnorm(100)
fit &lt;- lm(y~x1*x2)
</code></pre>

<p>This creates a model of y based on x1 and x2, using a OLS regression.  If we wish to predict y for a given x_vec we could simply use the formula we get from the <code>summary(fit)</code>.</p>

<p>However, what if we want to predict the lower and upper predictions of y? (for a given confidence level).</p>

<p>How then would we build the formula?</p>

<p>Thanks.</p>
"
"0.0716689374632466","0.0873037869711973"," 11457","<p>is it possible to do stepwise (direction = both) model selection in nested binary logistic regression in R? I would also appreciate if you can teach me  how to get:</p>

<ul>
<li>Hosmer-Lemeshow statitistic,</li>
<li>Odds ratio of the predictors, </li>
<li>Prediction success of the model.</li>
</ul>

<p>I used lme4 package of R. This is the script I used to get the general model with all the independent variables:</p>

<pre><code>nest.reg &lt;- glmer(decision ~ age + education + children + (1|town), family = binomial, data = fish)
</code></pre>

<p>where:</p>

<ul>
<li>fish -- dataframe</li>
<li>decision -- 1 or 0, whether the respondent exit or stay, respectively.</li>
<li>age, education and children -- independent variables.</li>
<li>town -- random effect (where our respondents are nested)</li>
</ul>

<p>Now my problem is how to get the best model. I know how to do stepwise model selection but only for linear regression. (<code>step( lm(decision ~ age + education + children, data = fish), direction +""both"")</code>). But this could not be used for binary logistic regression right? also when i add <code>(1|town)</code> to the formula to account for the effects of town, I get an error result. </p>

<p>By the way... I'm very much thankful to Manoel Galdino <a href=""http://stackoverflow.com/questions/5906272/step-by-step-procedure-on-how-to-run-nested-logistic-regression-in-r"">who provided me with the script on how to run nested logistic regression</a>. </p>

<p>Thank you very much for your help.</p>
"
"0.113318539934015","0.110431526074847"," 12223","<p>I am trying to figure out how to control the smoothing parameters in an mgcv:gam model.</p>

<p>I have a binomial variable I am trying to model as primarily a function of x and y coordinates on a fixed grid, plus some other variables with more minor influences.  In the past I have constructed a reasonably good local regression model using package locfit and just the (x,y) values.  </p>

<p>However, I want to try incorporating the other variables into the model, and it looked like generalized additive models (GAM) were a good possibility.  After looking at packages gam and mgcv, both of which have a GAM function, I opted for the latter since a number of comments in mailing list threads seem to recommend it.  One downside is that it doesn't seem to support a local regression smoother like loess or locfit.</p>

<p>To start, I just wanted to try to replicate approximately the locfit model, using just (x,y) coordinates.  I tried with both regular and tensor product smooths:</p>

<pre><code>my.gam.te &lt;- gam(z ~ te(x, y), family=binomial(logit), data=my.data, scale = -1)

my.gam.s  &lt;- gam(z ~  s(x, y), family=binomial(logit), data=my.data, scale = -1)
</code></pre>

<p>However, plotting the predictions from the model, they are much much more smoothed compared to the locfit model.  So I've been trying to tune the model to not oversmooth as much.  I've tried adjusting the parameters sp and k, but it's not clear to me how they affect the smoothing.  In locfit, the nn parameter controls the span of the neighborhood used, with smaller values allowing for less smoothing and more ""wiggling"", which helps to capture some areas on the grid where the probability of the binomial outcomes changes rapidly.  How would I go about setting up the gam model to enable it to behave similarly?</p>
"
"0.120797969451519","0.129492442570703"," 13172","<p>I would like to use a binary logistic regression model in the context of streaming data (multidimensional time series) in order to predict the value of the dependent variable of the data (i.e. row) that just arrived, given the past observations. As far as I know, logistic regression is traditionally used for postmortem analysis, where each dependent variable has already been set (either by inspection, or by the nature of the study). </p>

<p>What happens in the case of time series though,  where we want to make prediction (on the fly) about the dependent variable in terms of historical data (for example in a time window of the last $t$ seconds) and, of course, the previous estimates of the dependent variable?</p>

<p>And if you see the above system over time, how it should be constructed in order for the regression to work? Do we have to train it first by labeling, let's say, the first 50 rows of our data (i.e. setting the dependent variable to 0 or 1) and then use the current estimate of vector ${\beta}$ to estimate the new probability of the dependent variable being 0 or 1 for the data that just arrived (i.e. the new row that was just added to the system)?</p>

<p>To make my problem more clear, I am trying to build a system that parses a dataset row by row and tries to make prediction of a binary outcome (dependent variable) , given the knowledge (observation or estimation) of all the previous dependent or explanatory variables that have arrived in a fixed time window. My system is in Rerl and uses R for the inference. </p>
"
"0.0566592699670073","0.0552157630374233"," 13469","<p>Tools such as random forests or adaboost are powerful at solving cross-sectional binary logistic problems or prediction problems where there are many weak learners. But can these tools be adapted to solve panel regression problems? </p>

<p>One could naively introduce a time index as an independent variable but all this does is to provide an additional degree of freedom to the fitting algorithm. What we would like is a solution that allows information from period T-1 to have bearing on period T. </p>

<p>If there is not a straightforward way to do this using these algorithms, is there an alternative algorithm that can perform a panel regression making use of the information in both the cross-section and time-series?</p>
"
"0.114024581281567","0.111119579623079"," 14206","<p>I am using SVM to predict diabetes. I am using the <a href=""http://www.cdc.gov/BRFSS/"">BRFSS</a> data set for this purpose. The data set has the dimensions of $432607 \times 136$ and is skewed. The percentage of <code>Y</code>s in the target variable is $11\%$ while the <code>N</code>s constitute the remaining $89\%$.</p>

<p>I am using only <code>15</code> out of <code>136</code> independent variables from the data set. One of the reasons for reducing the data set was to have more training samples when rows containing <code>NA</code>s are omitted.</p>

<p>These <code>15</code> variables were selected after running statistical methods such as random trees, logistic regression and finding out which variables are significant from the resulting models. For example, after running logistic regression we used <code>p-value</code> to order the most significant variables.</p>

<p>Is my method of doing variable selection correct? Any suggestions to is greatly welcome. </p>

<p>The following is my <code>R</code> implementation. </p>

<pre><code>library(e1071) # Support Vector Machines

#--------------------------------------------------------------------
# read brfss file (huge 135 MB file)
#--------------------------------------------------------------------
y &lt;- read.csv(""http://www.hofroe.net/stat579/brfss%2009/brfss-2009-clean.csv"")
indicator &lt;- c(""DIABETE2"", ""GENHLTH"", ""PERSDOC2"", ""SEX"", ""FLUSHOT3"", ""PNEUVAC3"", 
    ""X_RFHYPE5"", ""X_RFCHOL"", ""RACE2"", ""X_SMOKER3"", ""X_AGE_G"", ""X_BMI4CAT"", 
    ""X_INCOMG"", ""X_RFDRHV3"", ""X_RFDRHV3"", ""X_STATE"");
target &lt;- ""DIABETE2"";
diabetes &lt;- y[, indicator];

#--------------------------------------------------------------------
# recode DIABETE2
#--------------------------------------------------------------------
x &lt;- diabetes$DIABETE2;
x[x &gt; 1]  &lt;- 'N';
x[x != 'N']  &lt;- 'Y';
diabetes$DIABETE2 &lt;- x; 
rm(x);

#--------------------------------------------------------------------
# remove NA
#--------------------------------------------------------------------
x &lt;- na.omit(diabetes);
diabetes &lt;- x;
rm(x);

#--------------------------------------------------------------------
# reproducible research 
#--------------------------------------------------------------------
set.seed(1612);
nsamples &lt;- 1000; 
sample.diabetes &lt;- diabetes[sample(nrow(diabetes), nsamples), ]; 

#--------------------------------------------------------------------
# split the dataset into training and test
#--------------------------------------------------------------------
ratio &lt;- 0.7;
train.samples &lt;- ratio*nsamples;
train.rows &lt;- c(sample(nrow(sample.diabetes), trunc(train.samples)));

train.set  &lt;- sample.diabetes[train.rows, ];
test.set   &lt;- sample.diabetes[-train.rows, ];

train.result &lt;- train.set[ , which(names(train.set) == target)];
test.result  &lt;- test.set[ , which(names(test.set) == target)];

#--------------------------------------------------------------------
# SVM 
#--------------------------------------------------------------------
formula &lt;- as.formula(factor(DIABETE2) ~ . );
svm.tune &lt;- tune.svm(formula, data = train.set, 
    gamma = 10^(-3:0), cost = 10^(-1:1));
svm.model &lt;- svm(formula, data = train.set, 
    kernel = ""linear"", 
    gamma = svm.tune$best.parameters$gamma, 
    cost  = svm.tune$best.parameters$cost);

#--------------------------------------------------------------------
# Confusion matrix
#--------------------------------------------------------------------
train.pred &lt;- predict(svm.model, train.set);
test.pred  &lt;- predict(svm.model, test.set);
svm.table &lt;- table(pred = test.pred, true = test.result);
print(svm.table);
</code></pre>

<p>I ran with $1000$ (training = $700$ and test = $300$) samples since it is faster in my laptop. The confusion matrix for the test data ($300$ samples)  I get is quite bad.</p>

<pre><code>    true
pred   N   Y
   N 262  38
   Y   0   0
</code></pre>

<p>I need to improve my prediction for the <code>Y</code> class. In fact, I need to be as accurate as possible with <code>Y</code> even if I perform poorly with <code>N</code>. Any suggestions to improve the accuracy of classification would be greatly appreciated.</p>
"
"0.120192462032251","0.117130321416455"," 14399","<p>For my microsimulation, I want to use R to predict values and draw a random sample based on this prediction.</p>

<p>To clarify my point: I want to simulate the number of chronic conditions people suffer from ($y_t$) at a certain point in time. I have a few waves of panel data available to estimate a relation between age, sex, number of chronic conditions in the previous observation period (plus some others that I might include in later stages).</p>

<p>Suppose my regression model is $y_t = Î²_0 + Î²_1 age + Î²_2 sex + Î²_3y_{t-1} +u $.   </p>

<p>Since R provides me with coefficients for the betas, it is easy to predict $y_t$ given the independent variables. However, this is not what I want to do. Instead, I want my population of about 1300 individuals to resemble the variance in the possible outcomes of $y_t$ (otherwise after a few steps my simulated population wonâ€™t include those unlucky ones with much more chronic conditions than the average).   </p>

<p>I believe what I have to do is to draw a random sample from the distribution of the predicted value $y_t$, conditional on the independent variables. I further believe this can be done by drawing random numbers with mean $Î²$ and variance $var(Î²)$, multiplied by the actual values of the independent variables.  </p>

<p>So my question is: Is this the correct approach? Will this produce reliable values? Or do I need to take possible covariation of the independent variables etc. into consideration?</p>

<p><strong>Edit</strong>: Another point came to my mind. Does it make a difference whether $y_t$ or $y_t - y_{t-1}$ is my left hand side variable?</p>

<p>Thank you for your ideas.</p>
"
"0.0400641540107502","0.0390434404721515"," 18233","<p>After running a regression of the form <code>reg &lt;- lm(y ~ x1 + x2, data=example)</code> on a dataset, I can get predicted values using </p>

<pre><code>predict(reg, example, interval=""prediction"", level=0.95)
</code></pre>

<p>I'm wondering what the predicted values actually refer to when I'm using the regression to predict the actual dataset. Shouldn't I obtain the original values?</p>
"
"0.0716689374632466","0.0873037869711973"," 18470","<p>Basically all I want to do is predict a scalar response using some curves.
I've got as far as doing a regression (using fRegress from the fda package) but have no idea how to apply the results to a NEW set of curves (for prediction).</p>

<p>I have N=536 curves, and 536 scalar responses. Here's what I've done so far:</p>

<ul>
<li>I've created a basis for the curves.</li>
<li>I've created a fdPar object to introduce a penalty</li>
<li>I've created the fd object using smooth.basis to smooth the curves with the chosen penalty on the specified basis.</li>
<li>I've ran a regression using fRegress(), regressing the curves on the scalar response.</li>
</ul>

<p>Now, all I'd like to do, is use that regression to produce predictions for a new set of data that I have. I can't seem to find an easy way to do this.</p>

<p>Cheers</p>
"
"0.0981367343026181","0.0956365069595007"," 20001","<p>I am trying to cross validate a logistic regression model with probability sampling weights (weights representing number of subjects in the population).  I am not sure how to handle the weights in each of the 'folds' (cross-validation steps).  I don't think it is as simple as leaving out the observations, I believe the weights need to be rescaled at each step.</p>

<p>SAS has an option in proc surveylogistic to get cross validated (leave one out) prediction probabilities.  Unfortunately I cannot find in the documentation any details on how these were calculated.  I would like to reproduce those probabilities in R.  So far I have not had success and am not sure if my approach is correct.  </p>

<p>I hope someone can recommend an appropriate method to do the cross validation with the sampling weights.  If they could match the SAS results that would be great too.</p>

<p>R code for leave-one-out cross validated probabilities (produces error):</p>

<pre><code>library(bootstrap)
library(survey)
fitLogistic = function(x,y){
  tmp=as.data.frame(cbind(y,x))
  dsn=svydesign(ids=~0,weights=wt,data=tmp)
  svyglm(y~x1+x2, 
         data=tmp,family = quasibinomial,design=dsn)
} 
predict.logistic = function(fitLog,x){
  pred.logistic=predict(fitLog,newdata=x,type='response')
  print(pred.logistic)
  ifelse(pred.logistic&gt;=.5,1,0)
} 
CV_Res= crossval(x=data1[,-1], y=data1[,1], fitLogistic, predict.logistic, ngroup = 13)
</code></pre>

<p>Sample Data Set:</p>

<pre><code>y   x1  x2  wt
0   0   1   2479.223
1   0   1   374.7355
1   0   2   1953.4025
1   1   2   1914.0136
0   0   2   2162.8524
1   0   2   491.0571
0   0   1   1842.1192
0   0   1   400.8098
0   1   1   995.5307
0   0   1   955.6634
1   0   2   2260.7749
0   1   1   1707.6085
0   0   2   1969.9993
</code></pre>

<p>SAS proc surveylogistic leave-one-out cross validated probabilities for sample data set:</p>

<p>.0072, 1 .884, .954, ...</p>

<p>SAS Code:</p>

<pre><code>proc surveylogistic;
model y=x1 x2;
weight wt;
output out=a2 predprobs=x;
run;
</code></pre>
"
"0.0400641540107502","0.0390434404721515"," 20157","<p>I try to use the <code>gbm.fit()</code> function for a boosted regression tree model implemented in the R package <a href=""http://cran.r-project.org/web/packages/gbm/index.html"" rel=""nofollow"">gbm</a>. To investigate e.g., the bootstrapped prediction error and all other functionalities I want to use the <code>errorest()</code> from the <a href=""http://cran.r-project.org/web/packages/ipred/index.html"" rel=""nofollow"">ipred</a> package. I think <code>errorest()</code> does not accept the <code>gbm</code> output. Is there a workaround? </p>

<p>Sorry, for the missing example. Please, see below</p>

<pre><code>library(ipred)
library(gbm)
data(BostonHousing)
test &lt;- gbm(medv ~ ., distribution = ""gaussian"",  data=BostonHousing)
</code></pre>

<p>I am not sure how to use the result in <code>errorest()</code>. Can someone give me a helping hand? Thanks!</p>
"
"0.0566592699670073","0.0552157630374233"," 22902","<p>When comparing results obtained with different models in R, what should I look for to select the best one?</p>

<p>If I use for example the following 4 models applied to the same presence/absence sample taken from a species dataset, with the same variables:</p>

<ul>
<li><p>Generalized linear model</p></li>
<li><p>Generalized additive models Classification</p></li>
<li><p>Regression Tree</p></li>
<li><p>Artificial Neural Networks</p></li>
</ul>

<p>Should I compare all methods by AIC, Kappa, or cross-validation?</p>

<p>Will I ever be certain of selecting the best model?</p>

<p>What happens if I compare those 4 models prediction with a Bayes factor? Can I compare them?</p>
"
"0.139198742242089","0.146087177447694"," 23042","<p>Can someone explain my Cox model to me in plain English? </p>

<p>I fitted the following Cox regression model to <strong>all</strong> of my data using the <code>cph</code> function. My data are saved in an object called <code>Data</code>. The variables <code>w</code>, <code>x</code>, and <code>y</code> are continuous; <code>z</code> is a factor of two levels. Time is measured in months. Some of my patients are missing data for variable <code>z</code> (<em>NB</em>: I have duly noted Dr. Harrell's suggestion, below, that I impute these values so as to avoid biasing my model, and will do so in the future).</p>

<pre><code>&gt; fit &lt;- cph(formula = Surv(time, event) ~ w + x + y + z, data = Data, x = T, y = T, surv = T, time.inc = 12)

Cox Proportional Hazards Model
Frequencies of Missing Values Due to Each Variable
Surv(time, event)    w    x    y    z 
                0    0    0    0   14 

                Model Tests          Discrimination 
                                            Indexes        
Obs       152   LR chi2      8.33    R2       0.054    
Events     64   d.f.            4    g        0.437    
Center 0.7261   Pr(&gt; chi2) 0.0803    gr       1.548    
                Score chi2   8.07                      
                Pr(&gt; chi2) 0.0891                      

                   Coef    S.E.   Wald Z   Pr(&gt;|Z|)
         w      -0.0133  0.0503    -0.26     0.7914  
         x      -0.0388  0.0351    -1.11     0.2679  
         y      -0.0363  0.0491    -0.74     0.4600  
         z=1     0.3208  0.2540     1.26     0.2067
</code></pre>

<p>I also tried to test the assumption of proportional hazards by using the <code>cox.zph</code> command, below, but do not know how to interpret its results. Putting <code>plot()</code> around the command gives an error message.</p>

<pre><code> cox.zph(fit, transform=""km"", global=TRUE)
            rho chisq      p
 w      -0.1125 1.312 0.2520
 x       0.0402 0.179 0.6725
 y       0.2349 4.527 0.0334
 z=1     0.0906 0.512 0.4742
 GLOBAL      NA 5.558 0.2347
</code></pre>

<hr>

<h3>First Problem</h3>

<ul>
<li>Can someone explain the results of the above output to me in plain English? I have a medical background and no formal training in statistics.</li>
</ul>

<h3>Second Problem</h3>

<ul>
<li><p>As suggested by Dr. Harrell, I would like to internally validate my model by performing 100 iterations of 10-fold cross-validation using the <code>rms</code> package (from what I understand, this would entail building <code>100 * 10 = 1000</code> different models and then asking them to predict the survival times of patients that they had never seen).</p>

<p>I tried using the <code>validate</code> function, as shown.</p>

<pre><code>&gt; v1 &lt;- validate(fit, method=""crossvalidation"", B = 10, dxy=T)
&gt; v1
      index.orig training    test optimism index.corrected  n
Dxy      -0.2542  -0.2578 -0.1356  -0.1223         -0.1320 10
R2        0.0543   0.0565  0.1372  -0.0806          0.1350 10
Slope     1.0000   1.0000  0.9107   0.0893          0.9107 10
D         0.0122   0.0128  0.0404  -0.0276          0.0397 10
U        -0.0033  -0.0038  0.0873  -0.0911          0.0878 10
Q         0.0155   0.0166 -0.0470   0.0636         -0.0481 10
g         0.4369   0.4424  0.6754  -0.2331          0.6700 10
</code></pre>

<p>How do you perform the 100x resampling? I think my above code only performs the cross-validation once.</p></li>
<li><p>I then wanted to know how good my model was at prediction. I tried the following:</p>

<pre><code>&gt; c_index &lt;- abs(v1[1,5])/2 + 0.5
&gt; c_index
[1] 0.565984
</code></pre>

<p>Does this mean that my model is only very slightly better than flipping a coin?</p></li>
</ul>

<h3>Third Problem</h3>

<p>Dr. Harrell points out that I have assumed linearity for the covariate effects, and that the number of events in my sample is just barely large enough to fit a reliable model if all covariate effects happen to be linear.</p>

<ul>
<li>Does this mean that I should include some sort of interaction term in my model? If so, any advice as to what to put?</li>
</ul>
"
"0.0801283080215004","0.078086880944303"," 23248","<p>I have a problem of the form ""what is the probability that a user will 'like' a certain movie?"" For a bunch of users, I know the movies each has watched historically, and the movies each has liked. Additionally, for each movie I know the name of the director.</p>

<p>I calibrated a logistic regression for each user of the form:</p>

<p><code>glm(liked_by_user_1 ~ liked_by_user_2 + ... + liked_by_user_k + factor(director), family=binomial, data = subset(MovieWatchings, user_id == 1))</code></p>

<p>But my problem is: say that in the past, user 1 has watched movies from directors <code>D1</code> through <code>DM</code>, but next month <code>U1</code> watches a movie directed by <code>DN</code>? In that case the R <code>predict()</code> function will give an error, because the glm model for user 1 doesn't have an estimated parameter for the case of <code>director = DN</code>. But I must know something about <code>U1's</code> probability of liking the new movie, because I still know which other users have seen and liked this movie, and that has some predictive power.</p>

<p>How can I set up my model so that I can take into account other users' liking behavior, AND user 1's director preferences, but still have sensible predictions when user 1 sees his first movie from a new director? Is logistic regression even the right type of model for this case?</p>
"
"0.149906337799172","0.146087177447694"," 23795","<p>I am using a relevance vector machine as implemented in the kernlab-package in R, trained on a dataset with 360 continuous variables (features) and 60 examples (also continuous, so it's a relevance vector regression).</p>

<p>I have several datasets with equivalent dimensions from different subjects. Now it works fine for most of the subjects, but with one particular dataset, I get this strange results:</p>

<p>When using leave-one-out cross validation (so I train the RVM and try to subsequently predict one observation that was left out of the training), most of the predicted values are just around the mean of the example-values.
So I really don't get good predictions, but just a slightly different value than the mean.</p>

<p>It seems like the SVM is not working at all;
When I plot the fitted values against the actual values, I see the same pattern; predictions around the mean. So the RVM is not even able to predict the values it was trained on (for the other datasets I get correlations of around .9 between fitted and actual values).</p>

<p>It seems like, that I can at least improve the fitting (so that the RVM is at least able to predict the values it was trained on) by transforming the dependent variable (the example-values), for example by taking the square root of the dependent variable.</p>

<p>so this is the output for the untransformed dependent variable:</p>

<p>Relevance Vector Machine object of class ""rvm"" 
Problem type: regression </p>

<pre><code>Linear (vanilla) kernel function. 

Number of Relevance Vectors : 5 
Variance :  1407.006
Training error : 1383.534902093 
</code></pre>

<p>this, if I first transform the dependent variable by taking the square root:</p>

<p>Relevance Vector Machine object of class ""rvm"" 
Problem type: regression </p>

<pre><code>Linear (vanilla) kernel function. 

Number of Relevance Vectors : 55 
Variance :  1.711355
Training error : 0.89601609 
</code></pre>

<p>How is it, that the RVM-results change so dramatically, just by transforming the dependent variable? And what is going wrong, when an SVM just predicts values around the mean of the dependent variable (even for the values and observations it was trained on)?</p>
"
"0.0991537224422629","0.110431526074847"," 24706","<p>I have some regression equations, take from various published studies, that predict an individual's mass from its length. Typically only an equation's parameter values, R-squared, standard error of the estimate, and sample size are reported in a manuscript. I'm then using those equations to predict the masses of individuals from a new set of length observations. I'm mainly interested in total mass per unit area, so I'm summing the masses of all individuals and dividing by the area they occupy. Here's a brief example using R code:</p>

<pre><code># regression equation 1 information
eq1 &lt;- list(b0= 0.9, b1= 3.2, n= 10, r2= 0.984, see= 1.28)

# regression equation 2 information
eq2 &lt;- list(b0= 1.1, b1= 2.8, n= 16, r2= 0.971, see= 1.65)

# new observations
length &lt;- rgamma(100, 4)
area &lt;- 1000

# equation 1 prediction
mass.eq1 &lt;- eq1$b0 + eq1$b1 * length
massPerArea.eq1 &lt;- sum(mass.eq1) / area

# equation 2 prediction
mass.eq2 &lt;- eq2$b0 + eq2$b1 * length
massPerArea.eq2 &lt;- sum(mass.eq2) / area

# compare the two massPerArea predictions...?
</code></pre>

<p>Each regression equation of course results in a different final estimate, but how can I determine the uncertainty in those estimates and to what degree they differ statistically? Since I'm summing up individuals, is propagation of error part of that uncertainty? If a direct computation of that uncertainty is possible, that'd would be great, but if there's a solution that requires numerical simulations, that'd be fine also (and actually, I've tried some simulating, but I thought I'd ask first if a direct approach exists). Thanks! </p>
"
"NaN","NaN"," 26084","<p>If i do a modelLookup('glmnet') it says TRUE for probModel (and in fact, I'd expect it to be usable as a model to predict probabilities in a binary outcome prediction problem as glmnet has a 'binomial' family argument).</p>

<p>However, following the instructions from the caret package I say:</p>

<pre><code>trainControl = trainControl(classProbs=TRUE)

modelFit = train(X, y, method='glmnet', trControl=trainControl)
</code></pre>

<p>and I get:</p>

<pre><code>cannot compute class probabilities for regression
</code></pre>

<p>Am I doing something wrong?</p>
"
"0.149906337799172","0.146087177447694"," 26500","<p>Hello after struggling with using R for the last couple of days I was hoping someone could help me with a statistical analysis I am completing for an environmental science honours project. Using R statistics is not something we have been taught and I am worried that I may have bitten of more then I can chew, however my whole project is based around the <strong>hierarchical partitioning method and the exhaustive search multiple regression analysis method.</strong></p>

<p>The <a href=""http://cran.r-project.org/web/packages/hier.part/index.html"" rel=""nofollow"">hier.part</a> package was installed along with <a href=""http://cran.r-project.org/web/packages/gtools/index.html"" rel=""nofollow"">gtools</a>.</p>

<p>I have converted my dataset to a .csv file with seven independent variables and one dependant variable with around 400 replicates (my intention is to do this analysis on eight datasets in total with different amounts of replicates and another dependant variable, but I am starting with this one). The dependant variable is GPP, the independent variables are, NDVI, Temperature, Precipitation, Solar Radiation, Nutrient Availability and Soil Available Water Capacity.</p>

<p>Secondly I imported the .csv file into R using the script</p>

<pre><code>GPPANDDRIVER &lt;- read.table(""C:\\etc, header=T, sep="","")
</code></pre>

<p>This works fine and I can edit the table using </p>

<pre><code>edit(GPPANDDRIVER)
</code></pre>

<p>After looking at the <code>hier.part</code> package documentation available <a href=""http://cran.r-project.org/web/packages/hier.part/hier.part.pdf"" rel=""nofollow"">here</a> it seems like I need to define Y which in the script below is the dependent variable and define <code>scan</code> which is the independent variables (mentioned before).</p>

<pre><code>hier.part(y, xcan, family = ""gaussian"", gof = ""RMSPE"", barplot = TRUE)
</code></pre>

<p>I was defining the dependant <code>y</code> vector as </p>

<pre><code>y &lt;- as.vector(GPPANDDRIVER[""GPP""])
</code></pre>

<p>This also works fine and I have my y vector. However I am not sure how to load independent variables onto the xcan dataframe part of the script. I have tried typing in two scripts but they have not worked.</p>

<pre><code>xcan &lt;- as.vector(GPPANDDRIVER[-GPP])
## AND
xcan &lt;- data.frame(GPPANDDRIVER[-GPP])
</code></pre>

<p>If anyone could help me find the right script for representing my independant variables as xcan that would be greatly appreciated. Also once defined if I entered in the hier.part script mentioned above would R then show me results of the analysis after processing? I will be moving onto to the regression analysis after this if anyone can shed some light on this first problem.</p>

<pre><code>*information on hier.part arguments.*

**Arguments**

y a vector containing the dependent variables

xcan a dataframe containing the n independent variables

family family argument of glm

gof Goodness-of-fit measure. Currently ""RMSPE"", Root-mean-square â€™predictionâ€™

error, ""logLik"", Log-Likelihood or ""Rsqu"", R-squared

print.vars if FALSE, the function returns a vector of goodness-of-fit measures. If TRUE, a data frame is returned with first column listing variable combinations and the
second column listing goodness-of-fit measures.
</code></pre>
"
"0.0908569611434023","0.103299233817667"," 26568","<p>I would like to understand how to generate <em>prediction intervals</em> for logistic regression estimates. </p>

<p>I was advised to follow the procedures in Collett's <em>Modelling Binary Data</em>, 2nd Ed p.98-99. After implementing this procedure and comparing it to R's <code>predict.glm</code>, I actually think this book is showing the procedure for computing <em>confidence intervals</em>, not prediction intervals.</p>

<p>Implementation of the procedure from Collett, with a comparison to <code>predict.glm</code>, is shown below.</p>

<p>I would like to know: how do I go from here to producing a prediction interval instead of a confidence interval?</p>

<pre><code>#Derived from Collett 'Modelling Binary Data' 2nd Edition p.98-99
#Need reproducible ""random"" numbers.
seed &lt;- 67

num.students &lt;- 1000
which.student &lt;- 1

#Generate data frame with made-up data from students:
set.seed(seed) #reset seed
v1 &lt;- rbinom(num.students,1,0.7)
v2 &lt;- rnorm(length(v1),0.7,0.3)
v3 &lt;- rpois(length(v1),1)

#Create df representing students
students &lt;- data.frame(
    intercept = rep(1,length(v1)),
    outcome = v1,
    score1 = v2,
    score2 = v3
)
print(head(students))

predict.and.append &lt;- function(input){
    #Create a vanilla logistic model as a function of score1 and score2
    data.model &lt;- glm(outcome ~ score1 + score2, data=input, family=binomial)

    #Calculate predictions and SE.fit with the R package's internal method
    # These are in logits.
    predictions &lt;- as.data.frame(predict(data.model, se.fit=TRUE, type='link'))

    predictions$actual &lt;- input$outcome
    predictions$lower &lt;- plogis(predictions$fit - 1.96 * predictions$se.fit)
    predictions$prediction &lt;- plogis(predictions$fit)
    predictions$upper &lt;- plogis(predictions$fit + 1.96 * predictions$se.fit)


    return (list(data.model, predictions))
}

output &lt;- predict.and.append(students)

data.model &lt;- output[[1]]

#summary(data.model)

#Export vcov matrix 
model.vcov &lt;- vcov(data.model)

# Now our goal is to reproduce 'predictions' and the se.fit manually using the vcov matrix
this.student.predictors &lt;- as.matrix(students[which.student,c(1,3,4)])

#Prediction:
this.student.prediction &lt;- sum(this.student.predictors * coef(data.model))
square.student &lt;- t(this.student.predictors) %*% this.student.predictors
se.student &lt;- sqrt(sum(model.vcov * square.student))

manual.prediction &lt;- data.frame(lower = plogis(this.student.prediction - 1.96*se.student), 
    prediction = plogis(this.student.prediction), 
    upper = plogis(this.student.prediction + 1.96*se.student))

print(""Data preview:"")
print(head(students))
print(paste(""Point estimate of the outcome probability for student"", which.student,""(2.5%, point prediction, 97.5%) by Collett's procedure:""))
manual.prediction
print(paste(""Point estimate of the outcome probability for student"", which.student,""(2.5%, point prediction, 97.5%) by R's predict.glm:""))    
print(output[[2]][which.student,c('lower','prediction','upper')])
</code></pre>
"
"0.0566592699670073","0.0552157630374233"," 28438","<p>I'd like to do rank prediction (something very similar with regression) based on historical data, is there any package I can use in R ? 
Here's my problems:</p>

<p>I have a historical data of sports games, and all the rank of each team and some statistic of these teams, I'd like to use these data to predict these teams' next year rank.</p>

<p>Thanks</p>
"
"0.155471754152787","0.160980229054196"," 28688","<p>I ran <code>lm()</code> on my data with models selected by individual <code>lm</code>'s of each characteristic and then combined the top $R^2$ based on $p$-value. For instance, the first few characteristics are taken, then the rest are evaluated if they have $p&lt;.005$. My characteristics contain some duplication: for instance, I have a characteristic and its normalized variant in test P. My $p$-values are all very small but my diagrams do not look correct for R and T. (Referring to this blog post: <a href=""http://www.findnwrite.com/musings/evaluating-linear-regression-model-in-r/"" rel=""nofollow"">Evaluating Linear Regression Model in R</a>.)</p>

<p>In test P (and T) there is one outlier according to Cooks Distance. How do I find and eliminate that instance?</p>

<p>According to this tutorial on <a href=""http://www.montefiore.ulg.ac.be/~kvansteen/GBIO0009-1/ac20092010/Class8/Using%20R%20for%20linear%20regression.pdf"" rel=""nofollow"">Using R for Linear Regression</a>,</p>

<blockquote>
  <p>The plot in the upper left shows the residual errors plotted versus
  their fitted values.  The residuals should be randomly distributed
  around the horizontal line representing a residual error of zero; that
  is, there should not be a distinct trend in the distribution of
  points.</p>
</blockquote>

<p>Test P looks ok in the residual error but test R and T have a grouping what does that mean and how do I account for it?</p>

<blockquote>
  <p>The plot in the lower left is a standard Q-Q plot, which should
  suggest that the  residual errors are normally distributed.  The
  scale-location plot in the upper right shows the square root of the
  standardized residuals (sort of a square root of relative error) as a
  function of the fitted values.  Again, there should be no obvious
  trend in this plot.</p>
</blockquote>

<p>Again Test P looks ok in the standard Q-Q plot but test R and T have a grouping what does that mean and how do I account for it?</p>

<p>Also what is the coefficients on the output. I notice it lists the characteristics and a p value but i don't understand what it means.</p>

<p>And finally how do I make predictions using the model I created? </p>

<p><strong>Test P</strong>
F-statistic: 2.684 on 280 and 2221 DF,  p-value: &lt; 2.2e-16 
<img src=""http://i.stack.imgur.com/10A8r.png"" alt=""enter image description here""></p>

<p><strong>Test R</strong>
F-statistic: 3.691 on 258 and 2243 DF,  p-value: &lt; 2.2e-16 
<img src=""http://i.stack.imgur.com/jy6IR.png"" alt=""enter image description here""></p>

<p><strong>Test T</strong>
F-statistic: 4.029 on 268 and 2233 DF,  p-value: &lt; 2.2e-16 
<img src=""http://i.stack.imgur.com/bs69P.png"" alt=""enter image description here""></p>

<p>edit after running gls my p looks like this</p>

<p><img src=""http://i.stack.imgur.com/cUBGB.png"" alt=""""></p>
"
"0.120192462032251","0.117130321416455"," 28732","<p>I am using the randomForest package in R (R version 2.13.1, randomForest version 4.6-2) for regression and noticed a significant bias in my results: the prediction error is dependent on the value of the response variable. High values are under-predicted and low values are over-predicted. At first I suspected this was a consequence of my data but the following simple example suggests that this is inherent to the random forest algorithm:</p>

<pre><code>n = 1000; 
x1 = rnorm(n, mean = 0, sd = 1)
response = x1
predictors = data.frame(x1=x1) 
rf = randomForest(x=predictors, y=response)
error = response-predict(rf, predictors)
plot(x1, error)
</code></pre>

<p>I suspect the bias is dependent on the distribution of the response, for example, if <code>x1</code> is uniformly-distributed, there is no bias; if <code>x1</code> is exponentially distributed, the bias is one-sided. Essentially, the values of the response at the tails of a normal distribution are outliers. It is no surprise that a model would have difficulty predicting outliers. In the case of randomForest, a response value of extreme magnitude from the tail of a distribution is less likely to end up in a terminal leaf and its effect will be washed out in the ensemble average.</p>

<p>Note that I tried to capture this effect in a previous example, ""RandomForest in R linear regression tails mtry"". This was a bad example. If the bias in the above example is truly inherent to the algorithm, it follows that a bias correction could be formulated given the response distribution one is trying to predict, resulting in more accurate predictions.  </p>

<p>Are tree-based methods, such as random forest, subject to response distribution bias? If so, is this previously known to the statistics community and how is it usually corrected (e.g. a second model that uses the residuals of the biased model as input)?</p>

<p>Correction of a response-dependent bias is difficult because, by nature, the response is not known. Unfortunately, the estimate/predicted response does not often share the same relationship to the bias.</p>
"
"0.160256616043001","0.156173761888606"," 28756","<p>I have two datasets a training and a test dataset. The dependent variable is a proportion and there are 54 predictors which are positive and negative real numbers and another 7 predictors that are text. </p>

<p>There are three response variables. Total the normalized total number of hits. Treatment the normalized total number during treatment and a percent which is a ratio of the other two responses.</p>

<p>At the moment using lm on the percent prediction data I have a corolation of .4. 85% of the varibles are within 20% of their target. For the treatment response variable using glm in poisson mode i have a correlation of .6 percent but the variables do not match the target data at all.</p>

<p>I have two main issues I need advice on: </p>

<p>(1) it rejected the text predictors because it said factor has new level(s)
I would like it to ignore the information for those that have new level but not disregard it for those that have the correct information how do i do that? </p>

<p>(2) To make my dependent variable a real number, rather than a proportion bounded between 0 and 1, I was advised to transform the response using, for example, the logit transform or the Normal quantile function (<code>qnorm</code> in R). The problem is that these transformations (and others like it) will map 0 and 1 to non-finite values. How can I model these data in a regression setting when the response is a proportion that can be 0 or 1? </p>

<p>Using linear regression with outlier removal I am able to get 2239 of 2583 testing data within 20% of their actual value I would like to have that many within 10%. </p>

<p>Using the posson distribution glm the amount of treatment correlates with 69%.</p>

<p>Ignoring this second issue for the moment, I transform the y~x1+x2 such that y=log(y/(1-y)) the correlation of my predictions to actual data drops from 6% to 2%
This is what the data looks like after the logit transform</p>

<p><img src=""http://i.stack.imgur.com/rqkaD.png"" alt=""log distribution""></p>

<p>This is what the data looks like before the log distribution
<img src=""http://i.stack.imgur.com/Mx8sh.png"" alt=""normal percentages""></p>
"
"0.0693931503088838","0.0676252226000574"," 28819","<p>I have created a multiple linear regression model with R using <code>lm</code> and <code>glm</code>. I am using <code>lm</code> on a training set and <code>predict</code> on a testing set to validate the model. In one test my results are within 80% of what they should be for 80% of the cases. It correlates with 40% for one response variable and with 63% for another response variable (but the response variable with 63% correlation isn't near the actual values of the prediction). I have 53 predicates. What is the probability of that occurring randomly?
I've tried to build an multi-class svm off of the features using the predicates but so far the svm has been unable to properly predict the results.</p>
"
"0.0895861718290583","0.0873037869711973"," 28879","<p>I am looking for some libraries in R that can do incremental learning (also called online or sequential learning). The use case of such learning in comparison to traditional batch methods would be to process large amounts of data. Such practices include streams and data from sensors, where it is not feasible to use always the same model or to rebuild the model from scratch every time. Any machine learning algorithm that can use only single new example to change the model would suffice. However, the model itself must not hold on to old data (as you can imagine it would soon get too big), instead just calculating some statistics about data. </p>

<p>For multivariate regression, online approach like <a href=""http://en.wikipedia.org/wiki/Stochastic_gradient_descent"" rel=""nofollow"">Stochastic gradient descent</a> would be a good option. For regression / model trees something like <a href=""http://www.liaad.up.pt/~kdus/DAMI10.pdf"" rel=""nofollow"">this article</a> comes to mind. I am looking for such library where relatively good <strong>prediction</strong> accuracy (with respect to traditional batch methods) could be achieved based on the evolving model.</p>
"
"0.204428162572655","0.206598467635334"," 29336","<p><strong>Introduction</strong> </p>

<p>I am aiming to forecast the annual growth rates for a number of macroeconomic indicators (denote one by $Y_t$).
One of the tasks is to test the forecasting performance of rival time series models with and without exogenous variables ($X_t$, a $T\times k$ matrix). The list of rival models include:</p>

<ol>
<li>AR(I)MA model (annual growth rates are unlikely to have ""unit Roo"", though the latter is either assumed or tested) $$A(L)Y_t =\mu+ B(L)\varepsilon_t$$</li>
<li>linear regression model with ARMA errors $$Y_t = X_t\beta + \eta_t, \ \ A(L)\eta_t =  B(L)\varepsilon_t $$</li>
<li>lagged dependent variable model (autoregressive model with exogenous variables)
$$A(L)Y_t = X_t\beta + \varepsilon_t$$ </li>
<li>linear regression model
$$Y_t = X_t\beta + \varepsilon_t$$</li>
</ol>

<p>Where $\varepsilon_t$ is assumed to be a strong white noise, zero-mean constant variance i.i.d. process; $A(L)$ and $B(L)$ are autoregressive (of order $p$) and moving average (of order $q$) polynomials with $L$ - a back-shift (lag) operator.</p>

<p>Note that the primary and the only goal is forecasting performance, thus any ""good"" properties of parameter estimates are of the secondary concern. All I need is to test for the most parsimonious, robust to starting conditions forecaster. Decision will be made with one of the <code>accuracy()</code> options, but first I need to obtain the material for the comparison.</p>

<p>Models 1. and 2. are estimated by <code>auto.arima()</code> with default <code>""CSS-ML""</code> estimation method. Models 3. and 4. are estimated by ordinary least squares (<code>lm()</code>). $T$ is about $40$ quarters.</p>

<p><strong>Approaches tried so far</strong></p>

<p>To make the jack-knifed residuals the first approach denoted by ""rolling"" has been implemented. Starting from feasibly large sub-sample of time series data, parameters are estimated and an $h$ ahead forecast is done by the <code>predict()</code> function (EDIT: it is the same suggestion as in the first part Rob's answer to the second question). After that one point is added and the estimation\prediction steps are repeated. </p>

<p>A weak point of such experiments is that the number of time ticks (sample size) used to estimate the parameters is different. While I would like to test the robustness to the starting conditions, keeping the sample size for estimation fixed.</p>

<p>Bearing this in mind I tried to set the several subsequent values (EDIT: for the interval $k+p+q&lt;t_0&lt;t_1&lt;T-h+1$) in $Y_t$ being missing values (NA). In models 2.-4. this also implies dropping the corresponding subsequent rows in data matrix $X_t$. Prediction for 3. and 4. is straightforward (the same <code>predict()</code> with omitted $X_t$ data rows works well). All my concerns are about models 1. and 2.</p>

<p>With just the AR($p$) part the predictions are done sequentially $Y_{t+1|t} = \hat A(L)Y_t$. But with the presence of MA($q$) one could not (?) use the estimated parameters directly. From the Brockwell and Davis ""Introduction to Time Series and Forecasting"" Chapter 3.3 it follows that one needs an innovation algorithm to recursively estimate the $\theta_{n,j}$ from the specific system of equations that involves estimated autoregressive and moving average parameters. EDIT: these $\theta_{n,j}$ parameters are used to make the ARMA prediction, not the originally estimated parameters $\theta_{j}$. However it is remarked in the same chapter that $\theta_{n,j}$ asymptotically approaches $\theta_{j}$ if the process is invertible. It is not evident that 30-40 points is enough for the asymptotic result to be used even if is invertible. </p>

<p>Notes: I don't want to restrict $q$ to zero, since I am not doing so in true out-of-sample forecasting. EDIT: also not that it is not missing value imputation problem, but forecasting experiment, that the trajectory is not supposed to bridge two sub-samples by this imputing the missing values.</p>

<p><strong>Questions</strong></p>

<ol>
<li>Does <code>auto.arima()</code> performs correctly with the presence of missing values inside of the sample? [Already answered by Rob.]</li>
<li>(The actually crucial part of this post) How to correctly forecast (NOT impute) these missed points from the ARMA model when both $p&gt;0$ and $q&gt;0$? (I hope there are the ways already implemented in R language, but I simply is missing something.)</li>
</ol>

<p>EDIT: since the parameters for ARMA parts are estimated correctly could I legally rearrange the arima object to include the estimated parameters and the data only for the first subsample and then use a predict function? </p>

<p>EDIT2: I have tried to modify the estimated <code>mod</code> structure - the resulting forecast from <code>predict.Arima</code> is identical (double precision differences) to the forecast where I use the estimated MA and AR coefficients predicting $Y_{t+1|t}$ directly as $\hat A(L)(Y_t-X_t\hat \beta)+ X_t\hat \beta+\hat B(L)\hat \varepsilon_t $, without <code>KalmanForecast()</code>. This was expected since state space representation is supplied with the same estimated $\theta_j$, not $\theta_{n,j}$. So the only question remains is the difference between $\theta_j$ and $\theta_{n,j}$ significant to influence the point forecasts? I hope the answer is negative.</p>
"
"0.0817806119188484","0.0956365069595007"," 32313","<p>I have a linear regression model that is used to forecast the 'afluent natural energy' (ANE) of some region.</p>

<p>The predictors for this model are:</p>

<ul>
<li>the previous month ANE (<code>ANE0</code>)</li>
<li>the previous month rain volume (<code>PREC0</code>)</li>
<li>the current month forecast for rain volume (<code>PREC1</code>)</li>
</ul>

<p>We have 7 years of historical data for all of these variables, for each month. The current model just runs a OLS linear regression. I feel there's a lot of improvements to be done, but i'm not a time series specialist.</p>

<p>The first thing I notice is that the predictors are highly correlated (multicollinearity).
I'm not certain of the impacts of multicollinearity on prediction confidence.</p>

<p>I decided to try a time series approach, so I ran a ACF and PACF on the historic data:
The ACF shows a sine wave pattern, and the PACF has a spike at 1 and 2. So I tried both <code>ARIMA (2, 0, 0)</code> and <code>ARIMA(2,0,1)</code> to predict 20 periods ahead.</p>

<p>The ARIMA(2,0,1) shows good results, but I'm not certain as to how to compare it to the linear regression model.</p>

<p>What's the best way to test the performance of these model?  I'm using R as analysis tool (together with the <code>forecast</code> package). </p>
"
"0.179172343658117","0.174607573942395"," 33712","<p>I have a question about which prediction variance to use to calculate prediction intervals from a fitted <code>lm</code> object in R. </p>

<p>For a certain multiple linear regression model I have obtained an error variance with leave-one-out-cross-validation (LOOCV) by taking the mean of the squared difference between observed and predicted values (i.e., mean squared prediction error). I am aware of some of the drawbacks of LOOCV (e.g., <a href=""http://stats.stackexchange.com/questions/2352/when-are-shaos-results-on-leave-one-out-cross-validation-applicable"">When are Shao&#39;s results on leave-one-out cross-validation applicable?</a>), but for my specific application this was the easiest (and probably the only realistically) implementable CV method. The final fitted linear model (<code>fitted_lm</code>) is fitted with all observations and with this model I would like to make predictions for new observations (<code>new_observations</code>). For this I am using the <code>predict.lm</code>  function in R.</p>

<pre><code>predict(fitted_lm, new_observations, interval = ""prediction"", pred.var = ???)
</code></pre>

<p>My questions are:  </p>

<ul>
<li>What value do I use for <code>pred.var</code> (i.e., â€œthe variance(s) for future observations to be assumed for prediction intervalsâ€) in order to obtain realistic prediction intervals for my new_observations?  </li>
<li>Do I use the error variance obtained from the LOOCV, or do I use the functionâ€™s default (i.e., â€œthe default is to assume that future observations have the same error variance as those used for fittingâ€)?  </li>
<li>Is the mean squared prediction error not appropriate in this case?</li>
</ul>

<p>Following up on Michael Chernick's answer hereunder, I had a look in the Draper &amp; Smith (1998) book  (â€œApplied regression analysis. 3rd Editionâ€). In this book <em>s<sup>2</sup></em> is defined as â€œvariance about the regressionâ€ (p 32). This is, I presume, what we describe below as the model estimate of residual variance. Furthermore, this book mentions: </p>

<blockquote>
  <p>â€œSince the actual observed value of <em>Y</em> varies about the true mean value <em>Ïƒ<sup>2</sup></em> [independent of the <em>V(Å¶)</em>], a predicted value of an individual observation will still be given with <em>Å¶</em> but will have variance</p>
  
  <p><img src=""http://i.stack.imgur.com/uUPXs.jpg"" alt=""formula""></p>
  
  <p>With corresponding estimated value obtained by inserting <em>s<sup>2</sup></em> for <em>Ïƒ<sup>2</sup></em>â€ (pp 82-81).</p>
</blockquote>

<p>Thus, as far as I understand, in the D &amp; S book they only use the model estimate of residual variance to calculate confidence intervals. This would be the default setting in the <code>predict</code> function (function help: â€œthe default is to assume that future observations have the same error variance as those used for fittingâ€). However, as fosgen states below, â€œalthough LOOCV mean squared prediction error is not equal to the real mean squared prediction error, it is much more close to real than error variance of fitted modelâ€.</p>

<p>To make this more concrete; in my dataset I get a model estimate of residual variance of <code>0.005998</code> and a LOOCV mean squared prediction error of <code>0.007293</code>. What should I then fill in as <code>pred.var</code> in the <code>predict.lm</code> function:</p>

<ul>
<li>Nothing (i.e. use the default, which would equal to the model estimate of residual variance)</li>
<li><code>0.007293</code> (i.e. the LOOCV mean squared prediction error) </li>
<li><code>0.005998 + 0.007293</code> (Michael Chernick: â€œThe model estimate of residual variance gets added to the error variance due to estimating the parameters to get the prediction error variance for a new observationâ€).</li>
</ul>
"
"0.145754769518238","0.160980229054196"," 33981","<p>I would like to use <a href=""http://en.wikipedia.org/wiki/Exponential_smoothing#Double_exponential_smoothing"">double exponential smoothing</a> to predict prevalence rates of care dependency in Austrian federal states. </p>

<p>My data is very detailed, thus I would like to make use of that in order to refine my predictions. I have the percentage of people in care dependency levels 1â€“7 aged 50â€“99 in 9 Austrian federal states.</p>

<pre><code> str(daten[1:12][daten$jahr&gt;1996,])
'data.frame':   39600 obs. of  12 variables:
 $ age       : num  50 51 52 53 54 55 56 57 58 59 ...
 $ gender    : Factor w/ 2 levels ""male"",""female"": 1 1 1 1 1 1 1 1 1 1 ...
 $ bundesland: Factor w/ 9 levels ""Bgld"",""Ktn"",""Noe"",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ jahr      : num  1997 1997 1997 1997 1997 ...
 $ PfSt0     : num  0.992 0.989 0.985 0.985 0.985 ...
 $ PfSt1     : num  0.001458 0.000967 0.001459 0 0.002199 ...
 $ PfSt2     : num  0.00437 0.00193 0.00802 0.00793 0.00587 ...
 $ PfSt3     : num  0.00146 0.0058 0.00073 0.00433 0.0044 ...
 $ PfSt4     : num  0.000729 0 0.002188 0.002163 0.000733 ...
 $ PfSt5     : num  0 0.000967 0.002188 0.000721 0.002199 ...
 $ PfSt6     : num  0 0.000967 0 0 0 ...
 $ PfSt7     : num  0 0 0.00073 0 0 ...
</code></pre>

<p>DES is a time series analysis method. Time series analysis explains a data series by its past values only. While it is true that I use only past data of care dependency, one could regard age, gender and federal state as explanatory variables. Instead of computing individual double exponential smoothing forecasts for each age, gender, federal state combination, I could assume structural uniformity within these time series. Thus, my data might be regarded a multilevel panel dataset, with 50 observations per year (age groups) nested in 9 federal states each. (I plan to do separate regressions for males and females.) </p>

<p>I would like to use federal state, age and age squared as explanatory variables apart from previous value and previous trend, as done in double exponential smoothing. </p>

<p>However, in panel data analysis, time trends are typically covered by including the year variable in the regression, and rarely ever by including lags. <strong>How could I realize a forcasting method similar to double exponential smoothing in a panel dataset, i.e. including also other explanatory variables?</strong> (Preferably in R)</p>

<p>(Matters are complicated further by the fact that I have 7 instead of 1 dependent variables.)</p>
"
"0.161255109292305","0.174607573942395"," 35489","<p>I have real daily market data which I'm looking at to create a model for forecasting. The model that I created (below) used autoregressive terms within a linear regression.</p>

<p>I was sharing this with a colleague and he said ""autoregressive variables are correlated with the other variables in multiple linear setting which creates multicollinarity problem, creating unreliable result.""</p>

<p>So I'm turning to the group for help. Here is the data and the analysis that I performed in R.</p>

<pre><code>#Read in Data
MarketData = read.table('http://sharp-waterfall-3397.herokuapp.com/MarketCategories6.txt', header=TRUE,na.strings = ""NA"", sep="","")
MarketData$Month &lt;- as.factor(MarketData$Month)
MarketData$Weekday &lt;- as.factor(MarketData$Weekday)

str(MarketData)
</code></pre>

<p>I'm using the <a href=""http://cran.r-project.org/web/packages/PERregress/index.html"" rel=""nofollow"">PERregress</a> library to help with the autoregression using the <code>back()</code> function and to help with the residual diagnostics:</p>

<pre><code>library(PERregress)
descStat(MarketData)
</code></pre>

<p>Subsetting the data for model building and prediction purposes:</p>

<pre><code>Total = MarketData
MarketData = MarketData[1:268,]
attach(MarketData)
</code></pre>

<p>Here is a regression with everything that I can think of. Note you can have higher autoregressive terms but this will start to mask events since R will ignore the first several rows. Also just an FYI for some reason the residual analysis is breaking which I liked to look for points with undue leverage.</p>

<pre><code>#Market1Category1 Regression for the markets with everything that I can think of it
Market1Category1Output=lm(Market1Category1 ~ Trend+Month2+Month3+Month4+
                          Month5+Month6+Month7+Month8+Month9+Monday+
                          Tuesday+Wednesday+Thursday+Friday+Saturday+
                          Holiday1+Holiday2+Holiday3+Holiday4+
                          Event1+Event2+Event3+Event4+Event5+Event6+Event7+
                          Event8+Event9+Event10+Event11+Event12+Event13+
                          Event14+Event15+Event16+Event17+Event18+Event19+
                          Event20+Event21+Event22+Event23+Event24+Event25+
                          Event26+Event27+Event28+
                          back(Market1Category1)+back(Market1Category1, 2))
summary(Market1Category1Output)
acf(Market1Category1Output$residuals)
residualPlots(Market1Category1Output)
</code></pre>

<p>Here is the final equation. I'd like to say that I reduced the variables using partial f-test but I couldn't find an easy way to do this so if you know a function please let me know. Basically I looked at the change in adjusted $R^2$.</p>

<pre><code>#Final regression equation 
Market1Category1Output=lm(Market1Category1 ~ Month5+Month6+Month7+
                          Tuesday+Wednesday+Thursday+Friday+Saturday+
                          Holiday1+Holiday3+Event2+Event7+Event10+
                          Event13+Event16+Event25+Event28+
                          back(Market1Category1)+back(Market1Category1, 6))
summary(Market1Category1Output)
acf(Market1Category1Output$residuals)
residualPlots(Market1Category1Output)
</code></pre>

<p>Here is a plot of the actuals in green vs the predictions in blue but there's a problem:</p>

<pre><code>plot(Time, Market1Category1, col='green')
points(Time, predict(Market1Category1Output, MarketData), col='blue', pch=20)
</code></pre>

<p>The issue is that predict will use the data values instead of it's predicted values for the autoregressive terms. In order to make it use predicted terms I created this loop. If you know a better way let me know.</p>

<pre><code>dataSet2 &lt;- Total
dataSet2[8:length(dataSet2$Time),""Market1Category1""] &lt;- NA
    for (i in (1:(length(dataSet2$Time)-7))) {
  dataSet2[6+i+1,""Market1Category1""] &lt;- 1
  dataSet2[6+i+1,""Market1Category1""] &lt;- predict(Market1Category1Output, 
                                                dataSet2[0:6+i+1,])[6+1] 
}
</code></pre>

<p>Here is the plot again with the results in blue using the predicted results for the autoregressive terms (with the exception of the first 7 since the model needs those to <code>predict</code>):</p>

<pre><code>plot(Total$Time, Total$Market1Category1, col='green')
points(dataSet2$Time, dataSet2$Market1Category1, col='blue', pch=20)
</code></pre>

<p>So here are my questions in order of importance:</p>

<ol>
<li>Does using autoregressive and linear terms violate any fundamental assumptions?</li>
<li>What issues can this cause and what analysis/steps should I do take to avoid these problems?</li>
<li>Is there a better approach to modeling this timeseries?</li>
<li>Is there a more efficient approach?</li>
<li>Given the residuals what steps would you take?</li>
</ol>

<p>Finally two questions which is just causing me more work than possibly necessary:</p>

<ol>
<li>As you can see instead of using the factors for weekday and month I'm using separate conditional variables. I'm doing this because if I use the factor and a level turns out to be insignificant (e.g., Monday for days of the week). I can't remove it. Perhaps there's a way?</li>
<li>Is there a quick way to run a partial F-statistic to understand whether removing a variable makes sense?</li>
</ol>
"
"0.120192462032251","0.117130321416455"," 35719","<p>I am just learning R. I have developed a regression model with six predictor variables. While developing it, I found the relationships are not very linear. So, maybe because of this the predictions of my model are not exact.</p>

<p>Here is Headers of my data set:</p>

<pre><code>1.bouncerate(To be predicted)
2.avgServerResponseTime
3.avgServerConnectionTime
4.avgRedirectionTime
5.avgPageDownloadTime
6.avgDomainLookupTime
7.avgPageLoadTime
</code></pre>

<p>Sample datasets:</p>

<pre><code>28.57142857,4.132,0.234,0,0.505,0,14.168
42.85714286,3.356777778,0.090777778,0.077333333,0.459,0.105444444,14.78644444
0,3.372,0.1105,0.0015,0.425,0.1305,34.3425
33.33333333,3.583,0.218,0,0.385,0.649,11.816
66.66666667,2.438,0.234,0,0.3405,0,8.645
100,2.805,0.179666667,3.203666667,0.000333333,0.11,13.47066667
66.66666667,0.977,0,0.003,0,0,12.847
0,2.776,0,7.888,0,0,14.393
100,2.59,0.261,0,0.517,0,6.216
</code></pre>

<p>Here is the summary of my model:</p>

<pre><code>Call:
lm(formula = y ~ x_1 + x_2 + x_3 + x_4 + x_5 + x_6)

Residuals:
     Min       1Q   Median       3Q      Max 
-125.302  -26.210    0.702   26.261  111.511 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 48.62944    0.27999 173.684  &lt; 2e-16 ***
x_1         -0.67831    0.08053  -8.423  &lt; 2e-16 ***
x_2          0.07476    0.49578   0.151 0.880143    
x_3         -0.22981    0.06489  -3.541 0.000399 ***
x_4          0.01845    0.09070   0.203 0.838814    
x_5          3.76952    0.67006   5.626 1.87e-08 ***
x_6          0.07698    0.01565   4.919 8.75e-07 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 33.76 on 19710 degrees of freedom
Multiple R-squared: 0.006298,   Adjusted R-squared: 0.005995 
F-statistic: 20.82 on 6 and 19710 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>plot with all single variable are below:
<img src=""http://i.stack.imgur.com/jsW0j.png"" alt=""bouncerate vs avgServerConnectionTime"">
<img src=""http://i.stack.imgur.com/uhrya.png"" alt=""bouncerate vs ServerResponseTime"">
<img src=""http://i.stack.imgur.com/iuROe.png"" alt=""bouncerate vs avgRedirectionTime"">
<img src=""http://i.stack.imgur.com/wbfsP.png"" alt=""bouncerate vs avgDomainLookupTime"">
<img src=""http://i.stack.imgur.com/arTC6.png"" alt=""bouncerate vs avgPageLoadTime""></p>

<p>I have certain questions about this model:  </p>

<ol>
<li>Is there any way to improve the accuracy of this model?  </li>
<li>Which of the values is most useful: residual standard error, degrees of freedom, multiple R-squared, adjusted R-squared, F-statistics, or p-values for choosing best model?  </li>
<li>Is it appropriate to use polynomial transformations with these data?  </li>
<li>In case I do use polynomial terms in my model, which degree is most appropriate?  </li>
</ol>
"
"0.120192462032251","0.117130321416455"," 36221","<p>After development of recommendation engine with the R, before removal of outliers from data-set value of residual standard error was 1351 and after removal of outlier its 656. Still there is no accurate prediction which gives 10% correct(near) prediction. For more fitting i also have tried polynomial model with two ,three and four degree but still no improvement. Is there any most important thing to consider without R-squared or adjusted R-squared.   </p>

<p>Where i am using dataset with linear regression model for prediction of product purchase revenue on the base of total numbers of time product added to cart, removed from cart, total numbers of page views of product page. For checking model prediction accuracy i am considering only minimum residual standard error.</p>

<p>Here is Model summary</p>

<pre><code>&gt; summary(model_out)

Call:
lm(formula = yitemrevenue_out ~ xcartaddtotalrs_out + xcartremove_out + 
    xproductviews_out + xuniqprodview_out + xprodviewinrs_out, 
    data = as)

Residuals:
    Min      1Q  Median      3Q     Max 
-2671.1  -173.6   -83.4   -42.9 14288.6 

Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)          3.992e+01  1.254e+01   3.183  0.00147 ** 
xcartaddtotalrs_out -7.888e-03  2.570e-03  -3.070  0.00216 ** 
xcartremove_out     -3.410e+01  2.431e+01  -1.403  0.16076    
xproductviews_out    1.248e+01  1.222e+00  10.215  &lt; 2e-16 ***
xuniqprodview_out   -1.350e+01  1.487e+00  -9.076  &lt; 2e-16 ***
xprodviewinrs_out    3.705e-04  5.151e-05   7.193 7.62e-13 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 656.4 on 3721 degrees of freedom
Multiple R-squared: 0.1398, Adjusted R-squared: 0.1386 
F-statistic: 120.9 on 5 and 3721 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>Thanks</p>
"
"0.113318539934015","0.110431526074847"," 36303","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/31690/how-to-test-the-statistical-significance-for-categorical-variable-in-linear-regr"">How to test the statistical significance for categorical variable in linear regression?</a>  </p>
</blockquote>



<p>As we know we can use linear models for numeric dataset(independent variables are numerical only), but what type model is applicable here when I have numeric + categorical dataset(independent variables are combination of numeric and categorical).</p>

<p>for example I have two datasets</p>

<p>1.numeric dataset
2.numeric dataset + categorical dataset</p>

<pre>
1.numeric dataset (Prediction of price of home)

Independent variables
x1 =  numbers of bedrooms
x2 =  size of home in sq. feet

dependent variable
x3 =  price of home

here
dependent variable is numerical
independent variable is with numerical values


2.numeric dataset + categorical dataset(prediction of web visits)

Independent variables
x1 =  search time
x2 =  search query
x3 =  browser
x4 = country

dependent variable
x3 =  visits

here 
dependent variable is numerical
independent variable is with combination of numerical and categorical values
</pre>

<p>I assume here that for dataset 1 linear model with lm() is applicable, but its not possible for second dataset. can any one suggest best technique for dataset 2 to be implemented with model for prediction.</p>
"
"0.0566592699670073","0.0552157630374233"," 37372","<p>For a regression problem, I used gradient boosting machines and assessed RMSE. My dataset is comprised of 34 features and 10,000 records. Only 2 predictors were considered 'important' (importance for other predictors happened to be zero): they are both factor features with 300 levels or more, so lots of predictions on new data set gets the same result. When I delete these two features, I get nearly the same RMSE score even if only 3 or 4 predictors highlight some relative influence (again, with zero influence for other predictors).</p>

<p>What could explain this result? Should I be concerned with levels of factor predictors?</p>
"
"0.0693931503088838","0.0676252226000574"," 37714","<p>I would like to make a prediction for a (new) subject to have a certain outcome given the historical data and the model:</p>

<pre><code>glm(outcome ~ age + treatment + history, family=binomial, ...) 
</code></pre>

<p>however in the historical data that will be fitted by the model, I have some sort of repeated measurements on some of the subjects (and I don't know if repeated measures is the appropriate term to be used here, hence using lmer etc is doubtful); example:<br></p>

<pre><code>subject_ID    age    treatment    history    outcome
S_1           33      T_1         H_1        0
S_2           27      T_2         H_2        1
S_2           27      T_3         H_2        1
S_3           56      T_1         H_11       0
etc...
</code></pre>

<p>In this example subject_2 (S_2) has two rows because he had simultaneously two different treatments at the same time. could a logistic regression still be used or should cases like subject_2 be removed from the analysis?</p>
"
"0.120797969451519","0.129492442570703"," 37840","<p>Okay, so I am trying to understand linear regression. I've got a data set and it looks all quite alright, but I am confused. This is my linear model-summary:</p>

<pre><code>Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 0.2068621  0.0247002   8.375 4.13e-09 ***
temp        0.0031074  0.0004779   6.502 4.79e-07 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.04226 on 28 degrees of freedom
Multiple R-squared: 0.6016, Adjusted R-squared: 0.5874 
F-statistic: 42.28 on 1 and 28 DF,  p-value: 4.789e-07 
</code></pre>

<p>so, the p-value is really low, which means it is very unlikely to get the correlation between x,y just by chance.
If I plot it and then draw the regression line it looks like this:
<a href=""http://s14.directupload.net/images/120923/l83eellv.png"" rel=""nofollow"">http://s14.directupload.net/images/120923/l83eellv.png</a>
(Had it in as a picture but I am - as a new user - currently not allowed to post it)
Blue lines = confidence interval
Green lines = prediction interval</p>

<p>Now, a lot of the points do not fall into the confidence interval, why would that happen? I think none of the datapoints falls on the regression line b/c they are just quite far away from each other, but what I am not sure of: Is this a real problem? They still are around the regression line and you can totally see a pattern. But is that enough?
I'm trying to figure it out, but I just keep asking myself the same questions over and over again.</p>

<p>What I thought of so far:
The confidence interval says that if you calculate CI's over and over again, in 95% of the times the true mean falls into the CI.
So: It it is not a problem that the dp do not fall into it, as these are not the means really.
The prediction interval on the other hand says, that if you calculate PI's over and over again, in 95% of the times the true VALUE falls into the interval. So, it is quite important to have the points in it (which I do have).
Then I've read the PI always has to have a wider range than the CI. Why is that?
This is what I have done:</p>

<pre><code>conf&lt;-predict(fm, interval=c(""confidence""))
prd&lt;-predict(fm, interval=c(""prediction""))
</code></pre>

<p>and then I plotted it by:</p>

<pre><code>matlines(temp,conf[,c(""lwr"",""upr"")], col=""red"")
matlines(temp,prd[,c(""lwr"",""upr"")], col=""red"")
</code></pre>

<p>Now, if I calculate CI and PI for additional data, it does not matter how wide I choose the range, I get the exact same lines as above. I cannot understand. What does that mean?
This would then be:</p>

<pre><code>conf&lt;-predict(fm,newdata=data.frame(x=newx), interval=c(""confidence""))
prd&lt;-predict(fm,newdata=data.frame(x=newx), interval=c(""prediction""))
</code></pre>

<p>for new x I chose different sequences.
If the sequence has a different # of observations than the variables in my regression, I am getting a warning. Why would that be?</p>
"
"0.0566592699670073","0.0552157630374233"," 37973","<p>I am fitting a simple linear regression model with 4 predictors:</p>

<p><code>lm(Outcome ~ Predictor1 + Predictor2 + Predictor3 + Predictor4, data=dat.s)</code></p>

<p>I'm finding that the model predictions are consistently off as shown in this graph:
<img src=""http://i.stack.imgur.com/CNLJz.png"" alt=""scatterplot of predictions and residuals""></p>

<p>The model clearly overestimates the low values and underestimates the high values, but the miss-estimation is very linear -- it seems like the model should be able to just adjust the slope and fit the data better. Why is that not happening? In case it helps, here are scatterplots of the the Outcome against each of the four Predictors:
<img src=""http://i.stack.imgur.com/uc55e.png"" alt=""enter image description here""></p>

<p>Using the <code>car</code> package <code>outlierTest</code> function did not identify any outliers.</p>
"
"0.0566592699670073","0.0552157630374233"," 41006","<p>Pardon my naÃ¯vetÃ© if this is a dumb question, but I'm new to R.  I'm trying to do an ordered logit regression.  I'm running the model like so (just a dumb little model estimating number of firms in a market from income and population measures).  My question is about predictions.</p>

<pre><code>nfirm.opr&lt;-polr(y~pop0+inc0, Hess = TRUE)
pr_out&lt;-predict(nfirm.opr)
</code></pre>

<p>When I run predict (which I'm trying to use to get the predicted y), the outputs are either 0, 3, or 27, which in no way reflects what should seem to be the prediction based upon my manual predictions from the coefficient estimates and intercepts.  Does anyone know how get ""accurate"" predictions for my ordered logit model?</p>

<p><strong>EDIT</strong></p>

<p>To clarify my concern, my response data has observations across all the levels</p>

<pre><code>&gt;head(table(y))
y
0  1  2  3  4  5 
29 21 19 27 15 16 
</code></pre>

<p>where as my predict variable seems to be bunching up</p>

<pre><code>&gt; head(table(pr_out))
pr_out
0     1   2   3   4   5 
117   0   0 114   0   0 
</code></pre>
"
"0.0566592699670073","0.0552157630374233"," 41540","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/41697/prediction-results-for-two-response-variable-from-random-forest"">Prediction results for two response variable from random forest</a>  </p>
</blockquote>



<p>I use <a href=""http://cran.r-project.org/web/packages/randomForest/index.html"" rel=""nofollow"">randomForest</a> R package to do regression <code>rf = randomForest(A + B ~ C + D, data, ..)</code>. I want to know what are prediction values for A and B, and use <code>predict(rf)</code>. The output is an aggregate value instead of two values for A and B. Do you have any suggestion to implement a <code>predict</code> function to generate two predictions? </p>
"
"0.0693931503088838","0.0676252226000574"," 41697","<p>Would any R expert explain the predic function in the randomforest package to me?</p>

<p>I want to get two prediction results for numberic response variable A and B seperately from following regression</p>

<pre><code>result &lt;-randomforest(A + B ~ C + D + E, data = dataset)
predict(result)
</code></pre>

<p>I can get one prediction result. But prediction is neither A nor B. I can get the prediction results for both A and B from Mvpart and party package. </p>

<p>Thanks in advance!</p>
"
"0.105999788000636","0.103299233817667"," 43699","<p>I am R-tool beginner. I have a question regarding how to know the performance of a linear regression model by using validation data.
My approach was</p>

<ol>
<li><p>Create training and validation data sets from original data set.
""train"" is name of my training data set and ""valid"" is name of my validation data set. ""category"" will be my target variable and ""date_time"" is my independent variable.</p></li>
<li><p>Use training data set to create a regression model</p>

<blockquote>
  <p>attach(train)</p>
  
  <p>lreg=lm(category~date_time)</p>
</blockquote></li>
<li><p>Now do predictions for validation data set using model created with training data set</p>

<blockquote>
  <p>p=predict(lreg,valid)</p>
</blockquote></li>
<li><p>Now check the accuracy by finding the values of ACC, AUC.</p>

<blockquote>
  <p>mmetric(valid$category,p,""AUC"")</p>
  
  <p>mmetric(valid$category,p,""ACC"")</p>
</blockquote></li>
</ol>

<p>Now if AUC and ACC have small values then it means that model created by training data set is not good in making predictions.</p>

<p>Is my approach correct ?</p>

<p>Thanks and regards!</p>
"
"NaN","NaN"," 43785","<p>If I have a set of continuous predictors $X$ and a binary outcome $Y$ and I wanted to build a predictive model of $P(Y|X)$, I would start with a logistic regression model.</p>

<p>However, in my particular case, my $Y$ isn't binary, it's continuous between 0 and 1.  Is there a similar Generalized Linear Model that can be applied in this case?  My optimistic/naive attempt in R reveals that </p>

<pre><code>set.seed(123)
df &lt;- data.frame(y=runif(8), x1=rnorm(8), x2=rnorm(8))
mod &lt;- glm(y ~ ., data=df, family=binomial('logit'))

# Warning message:
# In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!

rbind(yhat=predict(mod, newdata=df), y=df$y)
#              1         2          3         4         5          6         7           8
# yhat 0.7461449 0.4869853 -0.1092115 1.9854276 0.8328304 -1.3708688 1.0150934 -0.03496334
# y    0.2875775 0.7883051  0.4089769 0.8830174 0.9404673  0.0455565 0.5281055  0.89241904
</code></pre>

<p>Note that some of the predictions are outside of $(0,1)$.  Any suggestions?</p>
"
"0.0693931503088838","0.0676252226000574"," 44308","<p>I'm building a supervised learning model where the target variable is a uniformly-distributed continuous value ranging from 0-1 (originally a rank value from 1-38000, then scaled down to 0-1). The 20 predictor variables are a mix of continuous and categorical variables, with no more than 6 levels for the latter. But when I apply various modeling techniques to the data (including regression, random forest, MARS, neural nets) using R, the predicted values for every model are shaped like a bell curve. </p>

<p>What could cause the fitted values (bell shape) to be so consistently different from the actual ones (uniform shape)? And is there any way to force these models to produce a more uniformly-shaped prediction?</p>
"
"0.0566592699670073","0.0552157630374233"," 45696","<p>I'm not sure if I used the concept ""extreme values"" right. Anyhow, I'm trying to produce a model that estimates maximum tree heights / $\text{km}^2$. I have a database of around 24000 points ($\text{km}^2$), each has the max tree height value and 33 predictors. After playing around with random forest I manage to achieve a correlation of 0.67 between the real height and the estimated height on the test sample (20%). A MSE of around 1.6 meters. But Maximum errors of up to 33 meters. What I can see is that patches with very tall trees or very short trees (50 meters - 1 meters) are out of the scope of the model. Thinking in linear regression it is analogous to losing prediction power as you move away from the center of gravity of the observations. Right? How can I cope with this if at all?</p>

<p>p.s. this was implemented in R</p>
"
"0.149906337799172","0.146087177447694"," 46075","<p>Is it possible to add standard error or confidence interval to a plot of a predicted vs observed values derived from a multiple regression model? I believe that I have seen such plots as an output in Statistica, but am unsure how to create them in R.</p>

<p>I believe I have a solution (below), but am unsure that I have done this correctly. Basically, I have created a new <code>dataframe</code> with predictor variable in the range of their possible values. My worry with such an approach is that the prediction is based on the rows of data, and does not really address situations where the variables are randomly selected. </p>

<p>Many thanks for your help.</p>

<p><strong>Example:</strong></p>

<pre><code>set.seed(1)
n &lt;- 200
x1 &lt;- rnorm(n, mean=10, sd=3)
x2 &lt;- rnorm(n, mean=20, sd=5)
e &lt;- rnorm(n, mean=10, sd=3)

y &lt;- 5 + 2*x1 + 0.5*x2 + e

fit &lt;- lm(y ~ x1 + x2)
summary(fit)

#plot of predicted vs observed
pred1 &lt;- predict(fit, se.fit=TRUE)
plot(pred1$fit ~ y)
abline(0,1, col=8, lwd=2)

#new dataframe sequence of each predictor variable in their range
df.new &lt;- data.frame(x1=seq(min(x1), max(x1),,100), x2=seq(min(x2), max(x2),,100))
pred2 &lt;- predict(fit, df.new, se.fit=TRUE)

#plot of predicted vs observed w/ standard error interval?
png(""pred_vs_obs.png"", width=6, height=6, units=""in"", res=200)
plot(pred1$fit ~ y)
    abline(0,1, col=8, lwd=2)
    lines(pred2$fit+1.96*pred2$se.fit ~ pred2$fit, col=2, lty=2, lwd=2)
lines(pred2$fit-1.96*pred2$se.fit ~ pred2$fit, col=2, lty=2, lwd=2)
dev.off()
</code></pre>

<p><img src=""http://i.stack.imgur.com/AgUkO.png"" alt=""enter image description here""></p>

<p><strong>Edit:</strong> </p>

<p>The following code elaborates on my hesitation with the method that I used. The relationship between Standard Error (SE) and y is not a precise; i.e. various values of y that are relatively close together, have widely differing SE (black symbols in figure below, <code>pred1</code>), while the above method predicts a single SE for each predicted y (red symbols, <code>pred2</code>). Furthermore, using several different combinations of x1 and x2 that always result in the same y-value, I get a single (but different!) SE (green symbol, <code>pred3</code>). What is going on here? Is there a more correct way of doing this with some sort of permutation method?</p>

<pre><code>#? Do different solutions to a given predicted value always give the same standard error?
y.tmp &lt;- rep(40,20)
x1.tmp &lt;- seq(0,10, length(y.tmp))
x2.tmp &lt;- (y.tmp - fit$coeff[1] - fit$coeff[2]*x1.tmp) / fit$coeff[3]

df3 &lt;- data.frame(x1=x1.tmp, x2=x2.tmp)
pred3 &lt;- predict(fit, df3, se.fit=TRUE)

YLIM &lt;- range(pred1$se.fit, pred2$se.fit, pred3$se.fit)
    png(""fit.se_vs_fit.png"", width=6, height=6, units=""in"", res=200)
    plot(pred1$se.fit ~ pred1$fit, ylim=YLIM, lwd=2)
    points(pred2$se.fit ~ pred2$fit, col=2, lwd=2)
    points(pred3$se.fit ~ pred3$fit, col=3, lwd=2)
legend(""topright"", legend=c(""orig. data"", ""range of x1 &amp; x2"", ""various comb. of x1 &amp; x2 \nto acheive y=40""), col=1:3, pch=1, lwd=2, lty=0)
dev.off()
</code></pre>

<p><img src=""http://i.stack.imgur.com/4TcVs.png"" alt=""enter image description here""></p>
"
"0.0693931503088838","0.0676252226000574"," 47348","<p>I am trying to run a logistic regression in R on my data where my independent variables are 13 continuous variables and my dependent variable is binary.  I want to segment my data so that I train on the first 80% and test on the last 20%.  I have a total of 3750 rows of data so I utilize the first 3000 for training.  I have written the following:  </p>

<pre><code>mydata&lt;-totaldata[1:3000,2:15]
mylogit&lt;-glm(mydata$TARGET ~ mydata$VAR1+mydata$VAR2+mydata$VAR3+mydata$VAR4+ #$
                             mydata$VAR5+mydata$VAR6+mydata$VAR7+mydata$VAR8+
                             mydata$VAR9+mydata$VAR10+mydata$VAR11+mydata$VAR12+
                             mydata$VAR13, family=""binomial"")

predictdata=totaldata[3001:3751,3:15]
in_frame&lt;-data.frame(predictdata)
predictions=predict(mylogit,in_frame,type=""response"")
</code></pre>

<p>However I get the following warning message: 
Warning message:
'newdata' had 751 rows but variable(s) found have 3000 rows </p>

<p>Then when I look at predictions there are 3000 predictions not the 751 that I wanted.  What can I do to fix this?</p>
"
"0.0817806119188484","0.0956365069595007"," 47774","<p>I am carrying a linear regression on some data. One of my variables is a factor (categorical). Using regression with an intercept leads to difficult interpretation, since one of the factor levels is taken as the intercept, and the remaining levels are given relative to that. Removing the intercept give me an effect of each level of the factor, which I what I want.</p>

<p>As far as I know, both models are precisely equivalent. They produce identical predictions (on the training set) - to within ~3e-15. However, their RÂ² scores vary wildly.</p>

<pre><code># MWE
library(car)
int &lt;- lm(fscore ~ 1 + partner.status + conformity + fcategory,
          data = Moore)  #with intercept
nint &lt;- lm(fscore ~ 0 + partner.status + conformity + fcategory,
           data = Moore) #w/o intercept
summary(int)$r.squared
summary(nint)$r.squared  #RÂ² values are not remotely the same
max(predict(int)-predict(nint)) #Predictions are essentially identical
</code></pre>

<p>Why are the models not identical? Is it because RÂ² is a comparision of the model to ""no model"", and that ""no model"" corresponds to ""y=0"" and ""y=mean(fscore)"", for nint and int, respectively?</p>
"
"0.0400641540107502","0.0390434404721515"," 48694","<p>I'm working on a regression problem involving multiple independent learning tasks. I'm using conditional random forest as the learner (see Hothorn et al. 2006) as they are quite robust. The problem I'm having is saving the models. Multiple learning tasks means many big models.</p>

<p>How can I save a subset of each model without taking too much disk/memory space without compromising the usability of my models for predictions?</p>

<p>Thanks!</p>

<p>PK
^_^</p>
"
"0.0693931503088838","0.0676252226000574"," 48811","<p>For count data that I have collected, I use Poisson regression to build models. I do this using the <code>glm</code> function in R, where I use <code>family = ""poisson""</code>. To evaluate possible models (I have several predictors) I use the AIC. So far so good. Now I want to perform cross-validation. I already succeeded in doing this using the <code>cv.glm</code> function from the <code>boot</code> package. From <a href=""http://stat.ethz.ch/R-manual/R-patched/library/boot/html/cv.glm.html"">the documentation</a> of <code>cv.glm</code> I see that e.g. for binomial data you need to use a specific cost function to get a meaningful prediction error. However, I have no idea yet what cost function is appropriate for <code>family = poisson</code>, and an extensive Google search did not yield any specific results. My question is anybody has some light to shed on which cost function is appropriate for <code>cv.glm</code> in case of poisson glm's.</p>
"
"0.0716689374632466","0.0873037869711973"," 49141","<p>My predictions coming from a logistic regression model (glm in R) are not bounded between 0 and 1 like I would expected. My understanding of logistic regression is that your input and model parameters are combined linearly and the response is transformed into a probability using the logit link function. Since the logit function is bounded between 0 and 1, I expected my predictions to be bounded between 0 and 1.</p>

<p>However that's not what I see when I implement logistic regression in R:</p>

<pre><code>data(iris)
iris.sub &lt;- subset(iris, Species%in%c(""versicolor"",""virginica""))
model    &lt;- glm(Species ~ Sepal.Length + Sepal.Width, data = iris.sub, 
                family = binomial(link = ""logit""))
hist(predict(model))
</code></pre>

<p><img src=""http://i.stack.imgur.com/0BHU5.png"" alt=""enter image description here""></p>

<p>If anything the output of predict(model) looks normal to me. Can anyone explain to me why the values I get are not probabilities?</p>
"
"0.18504840082369","0.195361754177944"," 50086","<p>Assume for example a trivariate Gaussian model:
$$
{\boldsymbol Y}_1, \ldots, {\boldsymbol Y}_n \sim_{\text{iid}} {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right) \quad (*)
$$
with ${\boldsymbol \mu} = (\mu_1,\mu_2,\mu_3)'$. </p>

<p>The Bayesian conjugate theory of this model is well known. 
This model is the most simple case of a  multivariate linear regression model. 
And more generally, there is a well known Bayesian conjugate theory of multivariate linear regression, which is  the extension to the case when  the multivariate mean  ${\boldsymbol \mu}= {\boldsymbol \mu}(x_i)$ is allowed to depend on the covariates $x_i$ of individual $i \in \{1, \ldots, n \}$, with linear constraints about the multivariate means ${\boldsymbol \mu}(x_i)$. See for instance <a href=""http://books.google.be/books?id=GL8VS9i_B2AC&amp;dq=bayesian%20econometrics%20bayesm&amp;hl=fr&amp;source=gbs_navlinks_s"" rel=""nofollow"">Rossi &amp; al's book</a> accompanied by the crantastic <a href=""http://cran.r-project.org/web/packages/bayesm/index.html"" rel=""nofollow""><code>bayesm</code></a> package for <code>R</code>. 
We know in addition that the Jeffreys prior is a limit form of the conjugate prior  distributions.</p>

<p>Let us come back to the simple multivariate Gaussian model $(*)$. Instead of generalizing this model to the case of linearly dependent multivariate means ${\boldsymbol \mu}(x_i)$ depending on individuals $i=1,\ldots,n$, we can consider a more restrictive model by assuming linear constraints about the components $\mu_1$, $\mu_2$, $\mu_3$ of the multivariate mean ${\boldsymbol \mu}$. </p>

<h3>Example</h3>

<p>Consider some concentrations $x_{i,t}$ of $4$ blood samples $i=1,2,3,4$ measured at $3$ timepoints $t=t_1,t_2,t_3$. 
Assume that the samples are independent and that the series of the three measurements 
$(x_{i,t_1},x_{i,t_2},x_{i,t_3}) \sim {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right)$ for each sample $i$ with a mean 
${\boldsymbol \mu} = (\mu_1,\mu_2,\mu_3)'$ whose three components are 
linearly related to the timepoints: $\mu_j = \alpha + \beta t_j$.</p>

<p>This example falls into the context of <em>Multivariate linear regression with a within-design structure</em>. 
See for instance <a href=""http://wweb.uta.edu/management/Dr.Casper/Fall10/BSAD6314/Coursematerial/O%27Brien%20&amp;%20Kaiser%201985%20-%20MANOVA%20-%20RM%20-%20Psy%20Bull%2085.pdf"" rel=""nofollow"">O'Brien &amp; Kaiser 1985</a> and <a href=""http://www.stat.ualberta.ca/~wiens/stat575/misc%20resources/Appendix-Multivariate-Linear-Models.pdf"" rel=""nofollow"">Fox &amp; Weisberg's appendix</a></p>

<p>So my example is a simple example of this situation because there are only some predictors (the timepoints) for the components of the mean, but there are no predictors for individuals. This example could be written as follows:
$$
(**) \left\{\begin{matrix} 
{\boldsymbol Y}_1, \ldots, {\boldsymbol Y}_n \sim_{\text{iid}} {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right) \\ 
{\boldsymbol \mu} = X {\boldsymbol \beta}
\end{matrix}\right.
$$
with ${\boldsymbol \beta}=(\alpha, \beta)'$ and $X=\begin{pmatrix} 1 &amp; t_1 \\ 1 &amp; t_2 \\ 1 &amp; t_3 \end{pmatrix}$ is the matrix of covariates for the components of the multivariate mean ${\boldsymbol \mu}$. The second line of $(**)$ could be termed as the <em>within design</em>, or the <em>repeated measures design</em>, or the <em>structural design</em> (I would appreciate if a specialist had some comments about this vocabulary).</p>

<p>I think such a model can be fitted as a generalized least-squares model, as follows in  <code>R</code> :</p>

<pre><code>gls(response ~ ""between covariates"" , data=dat, 
  correlation=corSymm(form=  ~ ""within covariates"" | individual ))
</code></pre>

<p>(after stacking the data in long format).</p>

<p><strong>My first question</strong> is Bayesian: what about the Bayesian analysis of model $(**)$ and more generally the Bayesian analysis of the multivariate linear regression model with a structural design ? 
Is there a conjugate family ? What about the Jeffreys prior ? Is there an appropriate R package to perform this Bayesian analysis ?</p>

<p><strong>My second question</strong> is not Bayesian: I have recently discovered some possibilities of John Fox's great <code>car</code> package to analyse 
such models with ordinary least squares theory (the <code>Anova()</code> function with the <code>idesign</code> argument --- see <a href=""http://www.stat.ualberta.ca/~wiens/stat575/misc%20resources/Appendix-Multivariate-Linear-Models.pdf"" rel=""nofollow"">Fox &amp; Weisberg's appendix</a>). Perhaps I'm wrong, but I am under the impression that this package only allows to get the MANOVA table (sum of squares analysis) with an orthogonal matrix $X$, and I'd like to get (exact) confidence intervals about the within-design parameters for an arbitrary matrix $X$, as well as prediction intervals. Is there a way to do so with <code>R</code> using ordinary least squares ?  </p>
"
"0.120797969451519","0.129492442570703"," 55393","<p>I have a PDF (Probability Density Function) generated from a vector of 1,000,000 empirical values. This empirical PDF is heavily skewed to the right.</p>

<p>In this form, I can't make accurate predictions using a linear regression.</p>

<p>To fix this, is there some method to find the function F(x) to transform (i.e. ""squash"") the values in the vector into a standard normal distribution, so I can feed said transformed vector into a linear regression?</p>

<p>Of course, this would also involve finding the inverse of F(x) that transforms (i.e. ""de-squashes"") any predictions back into the original empirical PDF.</p>

<p><strong>What I have tried</strong></p>

<p>So far, I have managed to generate the density function from the empirical data:</p>

<p><img src=""http://i.stack.imgur.com/HIBUP.png"" alt=""enter image description here""></p>

<p>Here is the R code:</p>

<pre><code>par(mfrow=c(2,1))

install.packages(""bootstrap"")
library(bootstrap)
data(stamp)
nobs &lt;- dim(stamp)[1]
hist(stamp$Thickness,col=""grey"",breaks=100,freq=F)
	dens &lt;- density(stamp$Thickness)
lines(dens,col=""blue"",lwd=3)

plot(density(stamp$Thickness),col=""black"",lwd=3, main=""Simulation to choose density plot"")
	for(i in 1:10)
	{
		newThick &lt;- rnorm(nobs,mean=stamp$Thickness,sd=dens$bw*1.5)
		lines(density(newThick,bw=dens$bw),col=""grey"",lwd=3)
}

# If I wanted to do a linear regression to predict stamp thickness,
# what is the function F(x) to ""squash"" (i.e. transform) the ""stamp""
# vector into a normal distribution, and the corresponding inverse 
# function Finv(x) to ""desquash"" (i.e. untransform) any predictions back 
# into the original prediction?
</code></pre>

<p><strong>Update 1</strong></p>

<p>@Andre Silva sugggested that:</p>

<blockquote>
  <p>What need to have normal distribution are the residuals (predicted
  versus observed) derived from your (multiple) linear regression model.</p>
</blockquote>

<p>According to <a href=""http://www.stat.yale.edu/Courses/1997-98/101/linmult.htm"" rel=""nofollow"">post on Multiple Linear Regression</a>:</p>

<blockquote>
  <p>After fitting the regression line, it is important to investigate the
  residuals to determine whether or not they appear to fit the
  assumption of a normal distribution. A normal quantile plot of the
  standardized residuals y -  is shown to the left. Despite two large
  values which may be outliers in the data, the residuals do not seem to
  deviate from a random sample from a normal distribution in any
  systematic manner.</p>
</blockquote>

<p><img src=""http://i.stack.imgur.com/3ybm0.gif"" alt=""enter image description here""></p>

<p><strong>Update 2</strong></p>

<p>See <a href=""http://stats.stackexchange.com/questions/11351/left-skewed-vs-symmetric-distribution-observed/11352#11352"">Left skewed vs. symmetric distribution observed</a> for R code that illustrates that the only relevant concern is if the residuals are normally distributed.</p>
"
"0.0801283080215004","0.078086880944303"," 56440","<p>So I'm working with logistic regression models in R. Though I'm still new to statistics I feel like I got a bit of an understanding for regression models by now, but there's still something that bothers me:</p>

<p>Looking at the linked picture, you see the summary R prints for an example model I created. The model is trying to predict, if an email in the dataset will be refound or not (binary variable <code>isRefound</code>) and the dataset contains two variables closely related to <code>isRefound</code> , namely <code>next24</code> and <code>next7days</code> - these are also binary and tell if a mail will be clicked in the next 24hrs / next 7 days from the current point in the logs.</p>

<p>The high p-value should indicate, that the impact this variable has on the model prediction is pretty random, isn't it? 
Based on this I don't understand why the precision of the models predictions drops below 10% when these two variables are left out of the calculation formula. If these variables show such a low significance, why does removing them from the model have such a big impact?</p>

<p>Best regards and thanks in advance,
Rickyfox</p>

<p><img src=""http://i.stack.imgur.com/oiCrN.png"" alt=""enter image description here""></p>

<hr>

<h2>EDIT:</h2>

<p>First I removed only next24, which should yield a low impact because it's coef is pretty small. As expected, little changed - not gonna upload a pic for that. </p>

<p>Removing next7days tho had a big impact on the model: AIC 200k up, precision down to 16% and recall down to 73%</p>

<p><img src=""http://i.stack.imgur.com/583nx.png"" alt=""enter image description here""></p>
"
"0.0934830260250838","0.117130321416455"," 56521","<p>I need to calculate the regression variance ($\sigma^2$) in order to estimate both the confidence intervals and the prediction intervals in a gls regression analysis.  For the analysis, the covariance matrix ($V$) of the response variable ($y$) is known in advance, and so I use it directly as the weighting matrix (=$V^{-1}$) in the gls regression analysis.</p>

<p>The regression variance is a weighted sum of the residual error:
$\sigma^2 = \frac{ (Y â€“ X\beta)^T C^{-1} (Y â€“ X\beta)}{n â€“ p}$</p>

<p>My question/problem is how to determine the weighting matrix $C^{-1}$?  $C$ cannot be set equal to $V$ since (according to the above equation) $C$ must be dimensionless while $V$ has the same units as $\sigma^2$.</p>

<p>Based on my reading of the literature and available texts, it seems that $C$ is the correlation matrix and is a scaled or normalized form of the covariance matrix $V$.  i.e., $V = Var(\epsilon^2) = \sigma^2 C$.  But my problem is that $\sigma^2$ is not yet known, and so I need another way find $C$ from $V$.</p>

<p>R functions such as gls() will compute the regression variance (if I knew how gls() does this, it would answer my question).  However I cannot use gls() in this case since I am specifying a user-defined covariance (weighting) matrix, and gls() only accepts a limited set of specific correlation structures.</p>

<p>In fact a possible solution can be found in this <a href=""http://stats.stackexchange.com/questions/14426/prediction-with-gls"">earlier post</a> where an equation for the SEE (or sigma2) for a GLS regression was cited :</p>

<p>GLS calc of SEE: sqrt( sum( ( residuals from linear model) ^ 2 * glsWeight ) ) / sum( glsWeight ) * length( glsWeight ) / residualDegreeFreedom )</p>

<p>However I am unable to ascertain the validity of this equation and cannot find its source reference.</p>
"
"0.0693931503088838","0.0676252226000574"," 56871","<p>I have a dataset from a bank with demographic data and one variable telling if the customer is a good customer or not (binary variable). I would like to do prediction on if the customer is good or not based on this demographic data.</p>

<p>I managed to do it with a logistic regression, but would like now to compare the result (classification rate) with neural networks. </p>

<p>I found 2 functions from different packages doing that:
- nnet()
- neuralnet()
But those functions seem to be conceived for numerical dependent variables.</p>

<p>Thus my question: is there a possibility to use these functions for a categorical numerical variable (by estimating a posteriori probabilities for instance) or is there another function doing that?</p>

<p>Thanks a lot!</p>

<p>Robin</p>
"
"0.105999788000636","0.103299233817667"," 57811","<p>I'm trying to do LASSO in R with the package glmpath. However, I'm not sure if I am using the accompanying prediction function <em>predict.glmpath()</em> correctly. Suppose I fit some regularized binomial regression model like so:</p>

<pre><code>fit &lt;- glmpath(x = data$x, y=data$y, family=binomial)
</code></pre>

<p>Then I can use predict.glmpath() to estimate the value of the response variable $y$ at $x$ for varying values of $\lambda$ through</p>

<pre><code>pred &lt;- predict.glmpath(fit, newx = x, mode=""lambda"", s=seq(0,10,1),type=""response"")
</code></pre>

<p>However, in the help file it can be seen that there is also an option <em>newy</em>. How should one interpret the result when calling <em>predict.glmpath()</em> with <em>newy = some.y</em>? </p>

<p><strong>[Edit]</strong> An additional question came to mind:</p>

<p>The option <em>type</em> can have the following values, according to the help file:</p>

<pre><code>                      description in help file

""response""            the estimated responses are returned
""loglik""              the log-likelihoods are returned
""coefficients""        the coefficients are returned. The coefficients for the initial input variables are returned (rather than the standardized coefficients)
""link""(default)       the linear predictors are returned
</code></pre>

<p>However, to which linear predictors and coefficients are they referring to? Surely not those of the original model?</p>
"
"0.126693979201741","0.123466199581199"," 58101","<p>I am doing predictions on monthly temperature data for 100 years, from 1901 to 2000 (i.e 1200 data points). I want to know if the method I follow is correct because in my output, I do not see the requisite ""randomness"" of temperature being reproduced in the prediction.  </p>

<p>Here is a link to the plot of the prediction (in red)<br>
<a href=""https://docs.google.com/file/d/0B1Lm03a_91xiYks5TVJDYU05VUE/edit?usp=sharing"" rel=""nofollow"">https://docs.google.com/file/d/0B1Lm03a_91xiYks5TVJDYU05VUE/edit?usp=sharing</a>  </p>

<p>EDIT: added the ACF and PACF of the detrended and de-seasonalised time series:
<a href=""https://docs.google.com/file/d/0B1Lm03a_91xia2RTOHZrajJtZXM/edit?usp=sharing"" rel=""nofollow"">https://docs.google.com/file/d/0B1Lm03a_91xia2RTOHZrajJtZXM/edit?usp=sharing</a></p>

<p>Below is the dput() of my data:</p>

<pre><code>&gt; dput(fr.monthly.temp.ts)
structure(c(2.7, 0.4, 4.7, 10, 13, 16.9, 19.2, 18.3, 15.7, 10.6,   
4.9, 3.5, 4.1, 3.2, 7.5, 10.3, 10, 15.1, 18.2, 17.4, 15, 10.2, 
6.3, 3.5, 3.8, 5.9, 7.6, 7.1, 12.9, 14.9, 17.6, 17.3, 15.5, 12.1, 
6.9, 2.7, 3, 4.6, 5.5, 10.3, 13.6, 16.3, 20.2, 18.5, 13.9, 11.2, 
5.4, 4.8, 1.7, 4, 7.4, 9.3, 11.9, 16.5, 20, 17.6, 14.7, 8.4, 
5.5, 3.8, 4.3, 3.1, 5.6, 8.5, 12.6, 16.1, 18.2, 18.9, 16, 12.7, 
7.4, 2.3, 2.5, 2.1, 6.3, 8.4, 12.7, 15.1, 16.5, 17.9, 16.2, 11.6, 
7.6, 5.6, 1.7, 4.8, 5, 7.7, 14.2, 16.8, 17.9, 17.1, 14.8, 12.1, 
6.5, 3.6, 2.2, 2, 4.7, 10.4, 12.8, 14.2, 16.3, 18, 14.2, 12.2, 
5, 4.9, 4, 5.4, 6.6, 8.5, 11.9, 16.1, 16.4, 17.3, 14.2, 11.9, 
5.9, 6, 1.6, 4.5, 6.4, 8.3, 13.6, 16.1, 20.8, 20.7, 17.5, 11.3, 
7.3, 6.6, 4.6, 6.8, 8.4, 9.2, 13.8, 15.5, 17.9, 15.5, 12.5, 10, 
5.5, 5.8, 5.4, 4.7, 7.9, 9.1, 13, 15.8, 16.5, 17.6, 15.4, 12.3, 
9.2, 4, 0.7, 6.5, 7.4, 11.2, 12.2, 15.3, 17.3, 18.2, 15.3, 10.6, 
6.3, 5.7, 3.5, 4.3, 5.7, 8.5, 14.2, 17, 17.2, 17.5, 14.7, 9.6, 
4.6, 7, 6.4, 4.8, 5.9, 9.5, 13.8, 14, 17.4, 18.4, 14.5, 11.5, 
7, 4.3, 1.1, 1.4, 4.4, 6.7, 15.1, 17.6, 18.3, 17.2, 16.4, 9.4, 
7.3, 1.4, 3.7, 5.4, 6.5, 8.4, 14.2, 15, 18, 18.1, 15.4, 9.7, 
6.4, 6.9, 3.3, 3.7, 6.2, 7.8, 13.8, 16.3, 15.9, 18.9, 16.2, 8.8, 
4.6, 5.5, 5, 6.4, 8.2, 9.9, 14.4, 16, 17.4, 16.5, 15.2, 11.5, 
6, 4, 6.4, 4.2, 7.2, 8.9, 13.7, 16.9, 20.6, 18, 17, 14.1, 4.7, 
4.5, 3.4, 4.7, 6.6, 8, 14.8, 16.3, 16.7, 16.9, 13.7, 9.2, 5.4, 
4.5, 3.7, 6.3, 7.6, 9.4, 12.2, 14.1, 19.9, 18.8, 15.1, 12.3, 
5.3, 3.8, 3.8, 2.4, 6.4, 9.2, 14.1, 16.2, 18, 15.9, 15.2, 11.7, 
7.1, 4.5, 4.8, 5.6, 4.3, 9.1, 12.9, 17, 18, 17.6, 13.3, 11.8, 
4.9, 3.9, 4.1, 8.3, 7.2, 10.3, 11.6, 14.5, 18.2, 18.7, 17.3, 
11.5, 8.3, 2.5, 4.3, 4.5, 7.7, 9.8, 13.7, 15.7, 18, 17.8, 15.2, 
11.3, 6.7, 2.9, 5, 6.4, 7.1, 9.3, 11.8, 16.1, 20.5, 19.3, 15.8, 
11.5, 8.2, 3.7, 0.3, -0.2, 6.7, 7.8, 13.2, 16.3, 19.1, 18.1, 
18.4, 11.4, 7.3, 6.4, 5.8, 3.3, 7, 9.7, 12.1, 17.7, 17.3, 18.2, 
15.9, 11.9, 8.6, 4.5, 3.7, 3.3, 5.8, 8.8, 13.8, 17.5, 17.7, 17, 
12.8, 10.6, 8.2, 3.2, 4.8, 1.4, 5.5, 8, 12.1, 15.8, 17.4, 20.4, 
17.2, 11, 7.4, 5, 1.8, 4.3, 7.8, 10.1, 13.1, 15.4, 19.5, 20.1, 
16.7, 12, 5.5, 0.3, 3.3, 3.1, 6.3, 10.4, 13.8, 17.2, 20, 17.5, 
17.1, 11.9, 5.8, 7.6, 2.6, 5.1, 6.2, 9.1, 11.6, 17.2, 19.5, 18.1, 
16.1, 10.7, 7, 3.9, 6.5, 4.6, 7.9, 8.3, 13.4, 16.1, 17.2, 18, 
16, 9.1, 6.6, 4.2, 5.3, 6.9, 5.6, 9.9, 14.2, 16.6, 18.6, 19.1, 
15.5, 11.7, 6.3, 3.2, 4.4, 3.9, 8.8, 7.7, 11.7, 16.8, 17.5, 18.2, 
15.6, 11.3, 9.3, 2.5, 5.3, 4.7, 5.4, 10.2, 11.5, 16.4, 17.3, 
18.1, 15.2, 10.3, 8.7, 2.6, -0.9, 4.5, 7.1, 9.6, 13.5, 17.1, 
17.1, 17.5, 15.6, 10.6, 7.6, 1.1, 0.7, 4.5, 7.3, 8.2, 10.3, 16.8, 
19.3, 16.9, 15.5, 10.8, 6.6, 3.7, -0.2, -0.1, 7.7, 10.6, 13.1, 
16.7, 18.1, 18.7, 16.7, 13.2, 5.5, 4.8, 4.8, 5.3, 8, 11.5, 14.2, 
16.4, 19.2, 19.2, 16, 12.4, 5.9, 3.4, 5.1, 2.2, 5.1, 11.1, 13.4, 
16, 18.6, 20.6, 15.2, 10.1, 7.1, 3.4, -1, 7.1, 8.4, 11.9, 14.8, 
17.8, 20, 18.1, 16.7, 12.3, 6.5, 4.8, 1.7, 6.4, 6.7, 11.2, 13.1, 
15.7, 18.9, 17.9, 16.2, 11.3, 7.1, 2.1, 1, 1.3, 7.3, 11.3, 14.8, 
17.9, 20.4, 20.9, 17.6, 12.1, 8.3, 3.8, 5.7, 4.5, 9.5, 10.4, 
14, 15.8, 17, 17.8, 15.5, 11.4, 7.2, 4.6, 4.5, 5.4, 5.7, 11.7, 
12.2, 16.8, 20.6, 19.8, 18.6, 13.4, 6.4, 5.1, 3, 6.4, 8, 8.7, 
14.2, 18.3, 20.2, 18.6, 15.2, 11.4, 7.4, 1.1, 4.6, 4.7, 5.8, 
9.1, 11.8, 16.1, 18.7, 17.5, 16.5, 10.5, 8.7, 4.9, 2.7, 2.8, 
8.1, 11.2, 14.5, 17.9, 20.2, 18.9, 13.1, 10.9, 5.5, 3.5, 1.1, 
3, 7.5, 10.1, 14.8, 15.4, 18, 18.8, 16.2, 12.1, 7, 6.8, 1.7, 
2.3, 7.5, 8.6, 12.6, 16, 16.4, 16.9, 15.5, 12.4, 8, 6.2, 4.4, 
3.6, 4.6, 10.3, 12.5, 16.4, 19.1, 19.2, 15.7, 10.4, 6.7, 6.4, 
4.4, -1.8, 6.7, 8.1, 13.8, 14.4, 17.8, 16.4, 16.4, 10.6, 5.3, 
5.2, 3.1, 6.9, 9.8, 9.6, 11.5, 17, 18.5, 17.6, 15.1, 11.8, 6.8, 
3.6, 3.7, 6.2, 4.9, 7.9, 13.9, 15.6, 17.9, 18.4, 17.3, 11.4, 
6.7, 5.1, 3.4, 4.5, 8.6, 10.2, 13.8, 17, 20.3, 18.9, 17.2, 12.2, 
6.8, 5.7, 3.5, 5, 8, 9.6, 14.5, 17.6, 16.8, 17.3, 14.5, 11.1, 
8.4, 3.5, 3.6, 7.6, 8.3, 11.7, 12.5, 16.6, 17.7, 18, 18.5, 12.3, 
6.4, 4.5, 4.8, 3.7, 3.9, 9.1, 11.5, 15.8, 17.6, 18.6, 15.5, 11.9, 
5.4, 1.3, -1.6, -0.3, 6.5, 9.6, 12.2, 15.8, 18.5, 16.5, 15.2, 
11.5, 9.3, 1.3, 1.5, 5.2, 5.6, 9.6, 14.5, 16.8, 19.6, 18.2, 16.7, 
9.6, 7.2, 3.2, 3.6, 1.7, 6.6, 8.7, 12.7, 16.1, 16.7, 17.1, 13.7, 
12.2, 6.3, 5.7, 2.6, 7.9, 6.2, 10.5, 13.2, 17, 16.8, 17.2, 16.6, 
12.7, 5, 5.3, 3.5, 5.5, 7.7, 8.8, 12.5, 15.6, 19.8, 18.1, 15.3, 
13.2, 7.1, 3, 3.3, 4.3, 6.8, 9.9, 11.8, 15.9, 17.8, 17.2, 15.1, 
13.5, 6.8, 3, 4.8, 2.1, 6.2, 9.2, 13.2, 15, 19.1, 18.1, 15.9, 
13.1, 7.1, 1.4, 4.1, 4.3, 4.4, 7.6, 12.8, 17.6, 17.8, 18.3, 16.6, 
11.3, 8.7, 2.6, 3.1, 4.2, 3.8, 10.5, 13.7, 14.8, 19.7, 18.7, 
15.7, 12.3, 5.8, 4.9, 3.2, 5.5, 7.9, 8.9, 11.7, 14.3, 18, 17.1, 
13.3, 10.9, 7.3, 4.5, 3, 3.4, 6.1, 7.6, 13.5, 17, 18.1, 19.9, 
16.7, 10.6, 6.8, 3.7, 6.2, 5.5, 7.3, 9.4, 12.5, 15.9, 17.7, 18.6, 
14.5, 8.2, 7.4, 6.8, 6.4, 5.5, 5.3, 9, 12.1, 15.9, 19.1, 19.8, 
16.1, 10.4, 6.7, 3.1, 4.1, 4.8, 6, 8.9, 14, 18.8, 20.1, 19, 14.8, 
11.8, 6.6, 3.1, 3.7, 6.6, 8.3, 8.3, 12.1, 14.8, 17.8, 16.9, 14.7, 
12.9, 7, 5.3, 3.3, 4, 7.2, 7.8, 12.3, 15.2, 17.3, 17.2, 15.6, 
11.8, 6.7, 5.1, 1.3, 4, 6.6, 8.2, 12.3, 16.5, 18.5, 17.1, 15.7, 
12.4, 6.7, 5.7, 2.2, 6.3, 6.2, 8.4, 11.9, 15, 16.4, 18.6, 16.5, 
10.8, 5.8, 3.1, 3.3, 2.9, 9.2, 10, 12.6, 16, 17.5, 18.8, 16.2, 
11.2, 7.2, 3.8, 4.6, 5, 6.3, 9.3, 13.4, 17.4, 20.1, 18, 17.4, 
11.4, 8.3, 4.9, 5.5, 2.5, 7, 8.9, 11.5, 17.1, 22.2, 19.3, 16.3, 
11.9, 7.6, 4.5, 4.2, 3.5, 5.2, 9.6, 10.4, 15.8, 18.8, 18.4, 14.7, 
11.9, 9, 4.5, -1, 3.8, 5.2, 9.7, 12.5, 15.3, 19.4, 17.6, 17.3, 
12.3, 4.4, 5.6, 3.9, -0.6, 5.9, 6.9, 13.7, 16.9, 18.7, 17.6, 
14.9, 13.1, 7.9, 5, -0.8, 3.7, 4.8, 10.9, 11.4, 15, 18.6, 18.6, 
17.8, 12.4, 7.1, 5.2, 6.4, 4.9, 6.5, 10.1, 13.8, 16.2, 17.8, 
18.7, 15.7, 12.9, 6.3, 6, 4.2, 5.6, 9.3, 8.2, 15.3, 16.9, 20.2, 
19.5, 16.5, 13.2, 7, 5.6, 4.8, 8.8, 8.7, 8.9, 15.3, 16, 19.7, 
20.4, 15.9, 13.3, 7.2, 3.1, 3.9, 1.9, 9, 8.7, 11.7, 14.9, 19.6, 
20.7, 17.9, 10.9, 6.9, 3.6, 2.8, 4.9, 7.6, 9.5, 15.3, 16.1, 19.1, 
19.9, 15.5, 9.6, 9, 4.8, 5.9, 3.5, 7, 10.4, 14.1, 17.3, 17.8, 
18.7, 14.7, 10.4, 4.8, 6.2, 5.2, 5.1, 9.4, 8.7, 13.6, 17.1, 21.4, 
19.9, 15, 12, 10.2, 6.5, 4.5, 7.5, 6.5, 9.9, 13.6, 16.1, 21.1, 
20.2, 14.5, 14.6, 7.5, 3.8, 5, 2.9, 6, 10, 12.2, 17.5, 18.7, 
18.2, 14.2, 11.9, 6.9, 3.4, 2.3, 6.9, 9.3, 10, 14.2, 16.3, 18.6, 
21, 17, 12.4, 8.4, 5.5, 5, 5.9, 8.1, 9, 14.9, 17, 18.5, 19.4, 
16.1, 11.6, 5.2, 4.5, 5.3, 4.3, 8, 10, 15.2, 16.3, 20.2, 19.4, 
17.9, 12.2, 6.4, 5, 3.7, 6.6, 7.5, 9.9, 15, 17.8, 17.5, 19.6, 
16.9, 12.2, 8.2, 7.1), .Tsp = c(1901, 2000.91666666667, 12), class = ""ts"")  
</code></pre>

<p>I run <code>stl()</code> on it to remove the seasonality:  </p>

<pre><code># calculate and remove the seasonality  
fr.monthly.temp.ts.stl &lt;- stl(fr.monthly.temp.ts, s.window=""periodic"")    # get the    components  
fr.monthly.temp.seas &lt;- fr.monthly.temp.ts.stl$time.series[,""seasonal""]  
#plot(fr.monthly.temp.seas)  

fr.monthly.temp.ts.noseas &lt;- fr.monthly.temp.ts - fr.monthly.temp.seas  
#plot(fr.monthly.temp.ts.noseas)  
</code></pre>

<p>Then remove the trend with a regression:</p>

<pre><code>fr.mtrend.noseas &lt;- lm(fr.monthly.temp.ts.noseas~t)  
summary(fr.mtrend.noseas)  
</code></pre>

<p>and then use the residuals of this model to fit an ARIMA model (after checking the ACF and PACF for which one is appropriate):</p>

<pre><code># create time series of residuals..this is our ""detrended"" series..for now use only linear trend result  
fr.monthly.temp.ts.new &lt;- ts(fr.mtrend.noseas$resid, start=c(1901,1), frequency=12)
#plot.ts(fr.monthly.temp.ts.new, main=""Detrended and de-seasonalized time series"")

# ARIMA 1,1,1  
fit6 &lt;- arima(fr.monthly.temp.ts.new,order=c(1,1,1))  
fit6  
tsdiag(fit6)  
</code></pre>

<p>I then make a prediction on the stationary time series:</p>

<pre><code>#forecast for the stationary TS, for next 50 yrs months  
forecast &lt;- predict(fit6,n.ahead=600)  
</code></pre>

<p>And then add back the trend and seasonality:</p>

<pre><code>t.new &lt;- (n+1):(n+600)  

#initial time series = stationaryTS + seasonality + trend  
fr.monthly.temp.ts.init &lt;- fr.monthly.temp.ts.new + fr.monthly.temp.seas +
                            fr.mtrend.noseas$coefficients[1] + t * fr.mtrend.noseas$coefficients[2]  

#same for the prediction: we need to add seasonality and trend  
pred.Xt &lt;- forecast$pred + fr.monthly.temp.seas[1:(1+50*12 - 1)] + 
                                fr.mtrend.noseas$coefficients[1] + t.new * fr.mtrend.noseas$coefficients[2]  

plot(fr.monthly.temp.ts.init,type=""l"",xlim=c(1940,2060))  
lines(pred.Xt,col=""red"",lwd=2)  
</code></pre>

<p>So going back to my question: Do I need to add some white noise to the prediction to be able to realistically predict temperature? And more generally, is my method correct?</p>
"
"0.105999788000636","0.103299233817667"," 59074","<p>I am trying to get a grasp on how to use machine learning to predict financial timeseries 1 or more steps into the future.</p>

<p>I have a financial timeseries with some descriptive data and I would like to form a model and then use the model to predict n-steps ahead.</p>

<p>What I have been doing so far is:</p>

<pre><code>getSymbols(""GOOG"")

GOOG$sma &lt;- SMA(Cl(GOOG))
    GOOG$range &lt;- GOOG$GOOG.High-GOOG$GOOG.Low

tail(GOOG)


           GOOG.Open GOOG.High GOOG.Low GOOG.Close GOOG.Volume GOOG.Adjusted     sma range
2013-05-07    863.01    863.87   850.67     857.23     1959000        857.23 828.214 13.20
2013-05-08    857.00    873.88   852.91     873.63     2468300        873.63 834.232 20.97
2013-05-09    870.84    879.66   868.23     871.48     2200600        871.48 840.470 11.43
2013-05-10    875.31    880.54   872.16     880.23     1897700        880.23 848.351  8.38
2013-05-13    878.89    882.47   873.38     877.53     1448500        877.53 854.198  9.09
2013-05-14    877.50    888.69   877.14     887.10     1579300        887.10 860.451 11.55
</code></pre>

<p>Then I have fitted a randomForest model to this data.</p>

<pre><code>fit &lt;- randomForest(GOOG$GOOG.Close ~ GOOG$sma + GOOG$range, GOOG)
</code></pre>

<p>Which seems to fit surprisingly well:</p>

<pre><code>&gt; fit

Call:
 randomForest(formula = GOOG$GOOG.Close ~ GOOG$sma + GOOG$range,      data = GOOG) 
               Type of random forest: regression
                     Number of trees: 500
No. of variables tried at each split: 1

          Mean of squared residuals: 353.9844
                    % Var explained: 97.28
</code></pre>

<p>And tried to use it to predict:</p>

<pre><code>predict(fit, GOOG, n.ahead=2)
</code></pre>

<p>But this prediction ofc did not work.</p>

<p>I try to predict the Close, should I lag the other variables by as many steps as I want the prediction, before fitting the model?</p>

<p>Probably a lot of other stuff I should take into account as well but these are really my first steps trying out machine-learning.</p>

<p>Thankful for any tips!</p>
"
"0.114024581281567","0.123466199581199"," 59530","<p>With a colleague, we are working on a dataset containing ~5000 continuous variables for 120 individuals belonging to 8 classes.</p>

<p>We want to <strong>estimate the relative importance of each variable</strong> to explain the classes.
We have used a random forest approach with some success.
Now, we could like to go deeper by considering the fact that <strong>the 8 classes we fit are unequally distant from each other</strong>.
In fact, in our case <strong>we can <em>a priori</em> generate a distance matrix (<em>i.e.</em> cost matrix) for all possible pairs of classes</strong>.</p>

<p>My (very limited) understanding of random forest is that, for regression problems,
the error $E$ is computed by the mean square difference between the OOB sample and the prediction for the same sample:</p>

<p>$E = n^{-1}\sum\limits_{i=1}^n{{(y_i-\hat{y}_i)}^2}$</p>

<p>Where $y_i$ is the predicted value and $\hat{y}_i$ the real value of an out-of bag-sample $i$.</p>

<p>Ultimately, the calculation of the variable importance depends on how the error is computed (right?).</p>

<p>In our case, I would like to use a modified loss function, for instance:</p>

<p>$E = n^{-1}\sum\limits_{i=1}^n{M_{y_i,\hat{y}_i}}$</p>

<p>Where $M$ is predefined a distance matrix; so $M_{a,b}$ is the distance between class $a$ and $b$.
In this way the misclassification error would be more important if $y_i$ and $\hat{y}_i$ represent distant classes and, ultimately, <strong>the variable importance should be more relevant</strong>.</p>

<p>My questions are:</p>

<ol>
<li>Does this approach make sense to you, or am I missing something?</li>
<li>Can you think of any study that has used something similar.</li>
<li>We have so far used the <code>randomForest</code> package in R. It does not seem possible to use it in combination with an a priori distance matrix between classes. Do you know if this is already implemented somewhere?</li>
</ol>

<p><strong>EDIT</strong></p>

<p>I believe this is a very frequent problem in my field, biology, because we deal with classes for which relations can be represented and quantified by trees (dendrograms), often because of there lineage.</p>

<p>After some research, it appears that my question is about using a <strong>cost-sensitive</strong> version of random forest. In this respect, it is very similar to <a href=""http://stats.stackexchange.com/questions/46963/how-to-control-the-cost-of-misclassification-in-random-forests"">this question</a>.
I specificity want to use a <strong>cost matrix</strong> rather than a cost vector though.
It there any ontological reason why it is not possible or is it simply not implemented?</p>
"
"0.0801283080215004","0.078086880944303"," 59741","<p>say I have a sensor that measures temperature, pressure ++, and want to use this data to predict some quantity ""A"". If I use multivariate regression, I can simply implement a model of the form A=a0+a1x1+a2x2+..., and whenever I have new measurements I can use the model to make predictions.</p>

<p>If I on the other hand make a predictive model using random forests, I'm not really sure how to use it. I've used the caret package to split my data into training and test sets, and do automatic feature selection using random forest and cross-validation. I get good predictions on the test set, but have no idea how to implement these trees to use in say a digital signal processor. In R I just use the predict() function, but this is obviously not available outside of R.</p>

<p>This is probably a stupid questing, but it's the best I can do.</p>

<p>Any suggestions are welcome.</p>
"
"0.0600962310161253","0.078086880944303"," 60003","<p>Here I perform a GLS regression in R and the degrees of freedom is reported as ""Degrees of freedom: 60 total; 58 residual"". In this regression I see five parameters that are being estimated: the slope of the regression line, the intercept of the regression line, the residual standard deviation, the constant of the variance function, and the power of the variance function. When I go to generate prediction intervals for the regression line what degrees of freedom should I use? Anticipating the answer is not 55, why aren't the degrees of freedom 55?</p>

<pre><code>library(nlme)

X &lt;- c(1,1,1,1,1,1,1,1,1,1,4,4,4,4,4,4,4,4,4,4,
  10,10,10,10,10,10,10,10,10,10,20,20,20,20,20,20,
  20,20,20,20,30,30,30,30,30,30,30,30,30,30,40,40,   
  40,40,40,40,40,40,40,40)

Y &lt;- c(1.07,1.01,0.99,1.09,0.94,1.00,1.01,0.98,1.00,
  1.03,3.66,3.75,3.77,3.92,4.08,3.99,3.95,4.10,
  3.88,4.04,10.13,10.2,9.77,10.28,8.71,9.79,9.82,
  9.85,10.07,9.63,20.22,19.46,19.02,20.06,20.94,
  19.92,19.96,20.04,19.67,19.96,31.04,31.4,31.84,
  30.77,32.13,31.17,30.36,29.95,30.74,30.67,41.14,
  40.29,42.77,38.36,39.17,39.61,40.73,39.42,40.72,
  40.24)

m &lt;- data.frame(X,Y)

fit &lt;- gls(Y ~ X,weights=varConstPower(form = ~ X),data=m)
summary(fit)
</code></pre>
"
"0.126693979201741","0.123466199581199"," 60274","<p>I have a dataset that I'm trying to classify into 2 groups, A and B, using a random forest model. I know the true grouping and I'm trying to see how well I can model it using the other available variables. I've tried 2 different approaches that I thought would be equivalent, but which are actually giving me quite different results:</p>

<ol>
<li>Reading in the grouping as a (non-numeric) factor in R, growing a classification forest, and taking the proportion of trees that vote for group A as my prediction.</li>
<li>Constructing an indicator variable for membership of group A, growing a regression forest, and taking the ensemble prediction as usual.</li>
</ol>

<p>The split between the 2 groups is roughly 90-10 A vs. B. I'm growing 240 trees from ~200k observations of the same variables. I've left most of the settings at the defaults for the R randomForest package, but to keep the processing time down to a manageable level I've increased the node size to 200. The results are as follows:</p>

<ol>
<li>In the vast majority of cases, all 240 trees vote for A. The average predicted chance of any one observation being in A is about 99.9%. Worse still, not a single member of group B gets a majority of votes for group B!</li>
<li>I get a wide range of predictions, with the mean prediction lying close to the observed mean of ~90%.</li>
</ol>

<p>How can two apparently similar methods give such different results?</p>

<p>As for how I ended up trying this - I was initially trying to classify my dataset into a larger number of groups, of which B was one, but I noticed that B was being classified almost 100% incorrectly. The other groups are all much better behaved, even though most of them make up a far smaller proportion of my data.</p>
"
"0.126693979201741","0.123466199581199"," 60476","<p>I've run a regression on U.S. counties, and am checking for collinearity in my 'independent' variables.  Belsley, Kuh, and Welsch's <em>Regression Diagnostics</em> suggests looking at the Condition Index and Variance Decomposition Proportions:</p>

<pre><code>library(perturb)
## colldiag(, scale=TRUE) for model with interaction
Condition
Index   Variance Decomposition Proportions
           (Intercept) inc09_10k unins09 sqmi_log pop10_perSqmi_log phys_per100k nppa_per100k black10_pct hisp10_pct elderly09_pct inc09_10k:unins09
1    1.000 0.000       0.000     0.000   0.000    0.001             0.002        0.003        0.002       0.002      0.001         0.000            
2    3.130 0.000       0.000     0.000   0.000    0.002             0.053        0.011        0.148       0.231      0.000         0.000            
3    3.305 0.000       0.000     0.000   0.000    0.000             0.095        0.072        0.351       0.003      0.000         0.000            
4    3.839 0.000       0.000     0.000   0.001    0.000             0.143        0.002        0.105       0.280      0.009         0.000            
5    5.547 0.000       0.002     0.000   0.000    0.050             0.093        0.592        0.084       0.005      0.002         0.000            
6    7.981 0.000       0.005     0.006   0.001    0.150             0.560        0.256        0.002       0.040      0.026         0.001            
7   11.170 0.000       0.009     0.003   0.000    0.046             0.000        0.018        0.003       0.250      0.272         0.035            
8   12.766 0.000       0.050     0.029   0.015    0.309             0.023        0.043        0.220       0.094      0.005         0.002            
9   18.800 0.009       0.017     0.003   0.209    0.001             0.002        0.001        0.047       0.006      0.430         0.041            
10  40.827 0.134       0.159     0.163   0.555    0.283             0.015        0.001        0.035       0.008      0.186         0.238            
11  76.709 0.855       0.759     0.796   0.219    0.157             0.013        0.002        0.004       0.080      0.069         0.683            

## colldiag(, scale=TRUE) for model without interaction
Condition
Index   Variance Decomposition Proportions
           (Intercept) inc09_10k unins09 sqmi_log pop10_perSqmi_log phys_per100k nppa_per100k black10_pct hisp10_pct elderly09_pct
1    1.000 0.000       0.001     0.001   0.000    0.001             0.003        0.004        0.003       0.003      0.001        
2    2.988 0.000       0.000     0.001   0.000    0.002             0.030        0.003        0.216       0.253      0.000        
3    3.128 0.000       0.000     0.002   0.000    0.000             0.112        0.076        0.294       0.027      0.000        
4    3.630 0.000       0.002     0.001   0.001    0.000             0.160        0.003        0.105       0.248      0.009        
5    5.234 0.000       0.008     0.002   0.000    0.053             0.087        0.594        0.086       0.004      0.001        
6    7.556 0.000       0.024     0.039   0.001    0.143             0.557        0.275        0.002       0.025      0.035        
7   11.898 0.000       0.278     0.080   0.017    0.371             0.026        0.023        0.147       0.005      0.038        
8   13.242 0.000       0.001     0.343   0.006    0.000             0.000        0.017        0.129       0.328      0.553        
9   21.558 0.010       0.540     0.332   0.355    0.037             0.000        0.003        0.003       0.020      0.083        
10  50.506 0.989       0.148     0.199   0.620    0.393             0.026        0.004        0.016       0.087      0.279        
</code></pre>

<p><code>?HH::vif</code> suggests that VIFs >5 are problematic:</p>

<pre><code>library(HH)
## vif() for model with interaction
        inc09_10k           unins09          sqmi_log pop10_perSqmi_log      phys_per100k      nppa_per100k       black10_pct        hisp10_pct 
         8.378646         16.329881          1.653584          2.744314          1.885095          1.471123          1.436229          1.789454 
    elderly09_pct inc09_10k:unins09 
         1.547234         11.590162 

## vif() for model without interaction
        inc09_10k           unins09          sqmi_log pop10_perSqmi_log      phys_per100k      nppa_per100k       black10_pct        hisp10_pct 
         1.859426          2.378138          1.628817          2.716702          1.882828          1.471102          1.404482          1.772352 
    elderly09_pct 
         1.545867 
</code></pre>

<p>Whereas John Fox's <em>Regression Diagnostics</em> suggests looking at the square root of the VIF:</p>

<pre><code>library(car)
## sqrt(vif) for model with interaction
        inc09_10k           unins09          sqmi_log pop10_perSqmi_log      phys_per100k      nppa_per100k       black10_pct        hisp10_pct 
         2.894589          4.041025          1.285917          1.656597          1.372987          1.212898          1.198428          1.337705 
    elderly09_pct inc09_10k:unins09 
         1.243879          3.404433 
## sqrt(vif) for model without interaction
        inc09_10k           unins09          sqmi_log pop10_perSqmi_log      phys_per100k      nppa_per100k       black10_pct        hisp10_pct 
         1.363608          1.542121          1.276251          1.648242          1.372162          1.212890          1.185108          1.331297 
    elderly09_pct 
         1.243329 
</code></pre>

<p>In the first two cases (where a clear cutoff is suggested), the model is problematic only when the interaction term is included.</p>

<p>The model with the interaction term has until this point been my preferred specification.</p>

<p>I have two questions given this quirk of the data:</p>

<ol>
<li>Does an interaction term always worsen the collinearity of the data?</li>
<li>Since the two variables without the interaction term are not above the threshold, am I ok using the model with the interaction term.  Specifically, the reason I think this might be ok is that I'm using the King, Tomz, and Wittenberg (2000) method to interpret the coefficients (negative binomial model), where I generally hold the other coefficients at the mean, and then interpret what happens to predictions of my dependent variable when I move <code>inc09_10k</code> and <code>unins09</code> around independently and jointly.</li>
</ol>
"
"0.0895861718290583","0.0873037869711973"," 61344","<p>In a paper by <a href=""http://www.ncbi.nlm.nih.gov/pubmed/23628224"" rel=""nofollow"">Faraklas et al</a>, the researchers create a Necrotizing Soft-Tissue Infection Mortality Risk Calculator. They use logistic regression to create a model with mortality from necrotizing soft-tissue infection as the main outcome and then calculate the area under the curve (AUC). They use the bootstrap method to find the ""bootstrap optimism-corrected ROC area.""</p>

<p>If I were to do this in <code>R</code>, how would it look like? The code I have been toying with looks something like below:</p>

<pre><code>library(boot)
library(ROCR)

auc_calc &lt;- function(data, indices, outcomes) {
  d &lt;- data[indices,]
  # Using glm for logistic regression
  # Do I recreate the glm model for each dataset?
  fit &lt;- glm(outcomes[indices,] ~ X1 + X2 + X3, data=d, family=binomial)
  fit.predict &lt;- predict(fit, type=""response"")

  # Using ROCR to calculate AUC
  pred &lt;- prediction(fit.predict, outcomes[indices,])
  perf &lt;- performance(pred, ""auc"")

  # Returning the AUC
  return(perf@y.values[[1]])
}

boot.results &lt;- boot(data=my.data, statistic=auc_calc, R=10000, outcomes=my.outcomes)
</code></pre>

<p>Is this correct? Or am I doing something wrong - namely should I be passing in a glm model rather than recalculating it each time? As always thanks for the help.</p>
"
"0.165188738787336","0.160980229054196"," 62852","<p>I have implemented a Gibbs Sampler for the <strong>Bayesian Elastic Net</strong> (BEN) according to this paper on <a href=""http://www.stat.ufl.edu/~casella/Papers/BL-Final.pdf"" rel=""nofollow"">Penalized Regression by Kyung et al.</a><br>
In this paper, they execute a simulation study that has been used in other papers on Penalized Regression (LASSO, Bridge, Ridge) to compare the performance of the proposed models.</p>

<p>Here are details of the simulation taken from the above mentioned paper:  </p>

<blockquote>
  <p>We simulate data from the true model
  $$
y=X\beta+\sigma\epsilon \quad\epsilon_i\,{\raise.17ex\hbox{$\scriptstyle\sim$}}\,\text{iid}\,N(0,1)
$$
  We simulate data sets with $n=20$ to fit models and $n=200$ to compare prediction errors of proposed models with eight predictors. We let $\beta=(3,1.5,0,0,2,0,0,0)$ and $\sigma=3$. The pairwise correlation between $x_i$ and $x_j$ was set to be $corr(i,j)=0.5^{|i-j|}$.<br>
  Later on they say, that for the prediction error, they calculate the average mean squared error based on 50 replications. By average they mean the median in this case.</p>
</blockquote>

<p>To simulate this data and calculate the MSE I've used following code in R: </p>

<pre><code># Number of observations
n.train &lt;- 20
n.test  &lt;- 200
# Error variance
sigma &lt;- 3
# Pairwise correlation of X
cor &lt;- 0.5
# Number of predictors
p &lt;- 8
# Create training and test data set (package QRM and mvtnorm required)
Z &lt;- equicorr(p, rho=cor)
X.train &lt;- rmvnorm(n.train,sigma=Z)
X.test  &lt;- rmvnorm(n.test,sigma=Z)
# Create error 
error.train &lt;- rnorm(n.train,mean=0,sd=1)
error.test  &lt;- rnorm(n.test,mean=0,sd=1)
# Create beta
beta.true &lt;- c(3,1.5,0,0,2,0,0,0)
# Create both responses
Y.train &lt;- X.train %*% beta.true + sigma*error.train
Y.test  &lt;- X.test %*% beta.true + sigma*error.test

# Fit the training data set with the BEN Gibbs Sampler
beta.ben &lt;- BEN(X.train,Y.train, iter=11000, burn = 1000)
# Calculate the predicted response
Y.pred   &lt;- X.test %*% beta.ben
# Calculate the mean squared error (MSE)
MSE      &lt;- sum((Y.train - Y.pred)^2)/n.train
</code></pre>

<p>My problem is that my results are not even close to comparable to the ones in the paper which makes me doubt my simulation study ""setup"".<br>
As one of the authors of the paper has uploaded the Gibbs Sampler code and I could check if I did something wrong, I know that the problem doesn't lie there.</p>

<p>So my questions are:</p>

<ol>
<li>Does anybody have experience with this kind of simulation study and can check if I did something wrong?</li>
<li>Is the MSE I calculate the same as the one used in the paper? In researching on this topic I found many different ways to calculate the MSE and it was also sometimes used but actually the mean squared prediction error was meant. For example the Wikipedia article on MSE alone lists three variations.</li>
</ol>

<p>I don't need help with coding, rather more information on how this simulation is typically excecuted so I can figure out what I'm doing wrong.</p>
"
"0.183786979161368","0.187245762594928"," 63233","<h2>Background</h2>

<p>In a paper from Epstein (1991): <a href=""http://journals.ametsoc.org/doi/pdf/10.1175/1520-0442%281991%29004%3C0365%3AOODCVF%3E2.0.CO%3B2"" rel=""nofollow"">On obtaining daily climatological values from monthly means</a>, the formulation and an algorithm for calculating Fourier interpolation for periodical and even-spaced values are given.</p>

<p>In the paper, the goal is to <strong>obtain daily values from monthly means</strong> by interpolation.</p>

<p>In short, it is assumed that unknown daily values can be represented by the sum of harmonic components:
$$
y(t) = a_{0} + \sum_{j}\left[a_{j}\,\cos(2\pi jt/12)+b_{j}\,\sin(2\pi jt/12)\right]
$$
In the paper $t$ (time) is expressed in months.</p>

<p>After some derviation, it is shown that the terms can be calculated by:
$$
\begin{align}
a_{0} &amp;= \sum_{T}Y_{T}/12 \\
a_{j} &amp;= \left[ (\pi j/12)/\sin(\pi j/12)\right] \times \sum_{T}\left[Y_{T}\,\cos(2\pi jT/12)/6 \right]~~~~~~~j=1,\ldots, 5 \\
b_{j} &amp;= \left[ (\pi j/12)/\sin(\pi j/12)\right] \times \sum_{T}\left[Y_{T}\,\sin(2\pi jT/12)/6 \right]~~~~~~~j=1,\ldots, 5 \\
a_{6} &amp;= \left[ (\pi j/12)/\sin(\pi j/12)\right]\times \sum_{T}\left[Y_{T}\cos(\pi T)/12\right] \\
b_{6} &amp;= 0
\end{align}
$$
Where $Y_{T}$ denote the monthly means and $T$ the month.</p>

<p><a href=""http://journals.ametsoc.org/doi/pdf/10.1175/1520-0493%281995%29123%3C2251%3ATIODSU%3E2.0.CO%3B2"" rel=""nofollow"">Harzallah (1995)</a> summarizes this aproach as follows: ""The interpolation is carried out by adding zeros to the spectral coefficients of data and by performing an inverse Fourier transform to the resulting extended coefficients. The method is equivalent to applying a rectangular filter to Fourier coefficients.""</p>

<hr>

<h2>Questions</h2>

<p>My goal is to use the above methodology for interpolation of <strong>weekly means to obtain daily data</strong> (see <a href=""http://stats.stackexchange.com/questions/59418/interpolation-of-influenza-data-that-conserves-weekly-mean/63135#63135"">my previous question</a>). In summary, I have 835 weekly means of count data (see the example dataset at the bottom of the question). There are quite a few things that I don't understand before I can apply the approach outlined above:</p>

<ol>
<li>How would the formulas have to be changed for my situation (weekly instead of monthly values)?</li>
<li>How could the time $t$ be expressed? I assumed $t/835$ (or $t/n$ with $n$ data points in general), is that correct?</li>
<li>Why does the author calculate 7 terms (i.e. $0\leq j \leq 6$)? How many terms would I have to consider?</li>
<li>I understand that the question can probably be solved by using a <a href=""http://stats.stackexchange.com/questions/60500/how-to-find-a-good-fit-for-semi-sinusoidal-model-in-r?lq=1"">regression approach</a> and using the predictions for interpolation (thanks to Nick). Still, some things are unclear to me: How many terms of harmonics should be included in the regression? And what period should I take? How can the regression be done to ensure that the weekly means are preserved (as I don't want an exact harmonic fit to the data)?</li>
</ol>

<p>Using the <a href=""http://stats.stackexchange.com/questions/60500/how-to-find-a-good-fit-for-semi-sinusoidal-model-in-r?lq=1"">regression approach</a> (which is also explained in <a href=""http://www.stata-journal.com/sjpdf.html?articlenum=st0116"" rel=""nofollow"">this paper</a>), I managed to get an exact harmonic fit to the data (the $j$ in my example would run through $1, \ldots, 417$, so I fitted 417 terms). <strong>How can this approach be modified -$~$if possible$~$- to achieve the conservation of the weekly means?</strong> Maybe by applying correction factors to each regression term?</p>

<p>The plot of the exact harmonic fit is:</p>

<p><img src=""http://i.stack.imgur.com/7XuxU.png"" alt=""Exact harmonic fit""></p>

<p><strong>EDIT</strong></p>

<p>Using the <a href=""http://cran.r-project.org/web/packages/signal/"" rel=""nofollow"">signal package</a> and the <code>interp1</code> function, here's what I've managed to do using the example data set from below (many thanks to @noumenal). I use <code>q=7</code> as we have weekly data:</p>

<pre><code># Set up the time scale

daily.ts &lt;- seq(from=as.Date(""1995-01-01""), to=as.Date(""2010-12-31""), by=""day"")

# Set up data frame 

ts.frame &lt;- data.frame(daily.ts=daily.ts, wdayno=as.POSIXlt(daily.ts)$wday,
                       yearday = 1:5844,
                       no.influ.cases=NA)

# Add the data from the example dataset called ""my.dat""

ts.frame$no.influ.cases[ts.frame$wdayno==3] &lt;- my.dat$case

# Interpolation

case.interp1 &lt;- interp1(x=ts.frame$yearday[!is.na(ts.frame$no.influ.case)],y=(ts.frame$no.influ.cases[!is.na(ts.frame$no.influ.case)]),xi=ts.frame$yearday, method = c(""cubic""))

# Plot subset for better interpretation
par(bg=""white"", cex=1.2, las=1)
plot((ts.frame$no.influ.cases)~ts.frame$yearday, pch=20,
     col=grey(0.4),
     cex=1, las=1,xlim=c(0,400), xlab=""Day"", ylab=""Influenza cases"")
lines(case.interp1, col=""steelblue"", lwd=1)
</code></pre>

<p><img src=""http://i.stack.imgur.com/R1FE8.png"" alt=""Cubicinterpo""></p>

<p>There are two issues here:</p>

<ol>
<li>The curve seem to fit ""too good"": it goes through every point </li>
<li>The weekly means are not conserved</li>
</ol>

<p><strong>Example dataset</strong></p>

<pre><code>structure(list(date = structure(c(9134, 9141, 9148, 9155, 9162, 
9169, 9176, 9183, 9190, 9197, 9204, 9211, 9218, 9225, 9232, 9239, 
9246, 9253, 9260, 9267, 9274, 9281, 9288, 9295, 9302, 9309, 9316, 
9323, 9330, 9337, 9344, 9351, 9358, 9365, 9372, 9379, 9386, 9393, 
9400, 9407, 9414, 9421, 9428, 9435, 9442, 9449, 9456, 9463, 9470, 
9477, 9484, 9491, 9498, 9505, 9512, 9519, 9526, 9533, 9540, 9547, 
9554, 9561, 9568, 9575, 9582, 9589, 9596, 9603, 9610, 9617, 9624, 
9631, 9638, 9645, 9652, 9659, 9666, 9673, 9680, 9687, 9694, 9701, 
9708, 9715, 9722, 9729, 9736, 9743, 9750, 9757, 9764, 9771, 9778, 
9785, 9792, 9799, 9806, 9813, 9820, 9827, 9834, 9841, 9848, 9855, 
9862, 9869, 9876, 9883, 9890, 9897, 9904, 9911, 9918, 9925, 9932, 
9939, 9946, 9953, 9960, 9967, 9974, 9981, 9988, 9995, 10002, 
10009, 10016, 10023, 10030, 10037, 10044, 10051, 10058, 10065, 
10072, 10079, 10086, 10093, 10100, 10107, 10114, 10121, 10128, 
10135, 10142, 10149, 10156, 10163, 10170, 10177, 10184, 10191, 
10198, 10205, 10212, 10219, 10226, 10233, 10240, 10247, 10254, 
10261, 10268, 10275, 10282, 10289, 10296, 10303, 10310, 10317, 
10324, 10331, 10338, 10345, 10352, 10359, 10366, 10373, 10380, 
10387, 10394, 10401, 10408, 10415, 10422, 10429, 10436, 10443, 
10450, 10457, 10464, 10471, 10478, 10485, 10492, 10499, 10506, 
10513, 10520, 10527, 10534, 10541, 10548, 10555, 10562, 10569, 
10576, 10583, 10590, 10597, 10604, 10611, 10618, 10625, 10632, 
10639, 10646, 10653, 10660, 10667, 10674, 10681, 10688, 10695, 
10702, 10709, 10716, 10723, 10730, 10737, 10744, 10751, 10758, 
10765, 10772, 10779, 10786, 10793, 10800, 10807, 10814, 10821, 
10828, 10835, 10842, 10849, 10856, 10863, 10870, 10877, 10884, 
10891, 10898, 10905, 10912, 10919, 10926, 10933, 10940, 10947, 
10954, 10961, 10968, 10975, 10982, 10989, 10996, 11003, 11010, 
11017, 11024, 11031, 11038, 11045, 11052, 11059, 11066, 11073, 
11080, 11087, 11094, 11101, 11108, 11115, 11122, 11129, 11136, 
11143, 11150, 11157, 11164, 11171, 11178, 11185, 11192, 11199, 
11206, 11213, 11220, 11227, 11234, 11241, 11248, 11255, 11262, 
11269, 11276, 11283, 11290, 11297, 11304, 11311, 11318, 11325, 
11332, 11339, 11346, 11353, 11360, 11367, 11374, 11381, 11388, 
11395, 11402, 11409, 11416, 11423, 11430, 11437, 11444, 11451, 
11458, 11465, 11472, 11479, 11486, 11493, 11500, 11507, 11514, 
11521, 11528, 11535, 11542, 11549, 11556, 11563, 11570, 11577, 
11584, 11591, 11598, 11605, 11612, 11619, 11626, 11633, 11640, 
11647, 11654, 11661, 11668, 11675, 11682, 11689, 11696, 11703, 
11710, 11717, 11724, 11731, 11738, 11745, 11752, 11759, 11766, 
11773, 11780, 11787, 11794, 11801, 11808, 11815, 11822, 11829, 
11836, 11843, 11850, 11857, 11864, 11871, 11878, 11885, 11892, 
11899, 11906, 11913, 11920, 11927, 11934, 11941, 11948, 11955, 
11962, 11969, 11976, 11983, 11990, 11997, 12004, 12011, 12018, 
12025, 12032, 12039, 12046, 12053, 12060, 12067, 12074, 12081, 
12088, 12095, 12102, 12109, 12116, 12123, 12130, 12137, 12144, 
12151, 12158, 12165, 12172, 12179, 12186, 12193, 12200, 12207, 
12214, 12221, 12228, 12235, 12242, 12249, 12256, 12263, 12270, 
12277, 12284, 12291, 12298, 12305, 12312, 12319, 12326, 12333, 
12340, 12347, 12354, 12361, 12368, 12375, 12382, 12389, 12396, 
12403, 12410, 12417, 12424, 12431, 12438, 12445, 12452, 12459, 
12466, 12473, 12480, 12487, 12494, 12501, 12508, 12515, 12522, 
12529, 12536, 12543, 12550, 12557, 12564, 12571, 12578, 12585, 
12592, 12599, 12606, 12613, 12620, 12627, 12634, 12641, 12648, 
12655, 12662, 12669, 12676, 12683, 12690, 12697, 12704, 12711, 
12718, 12725, 12732, 12739, 12746, 12753, 12760, 12767, 12774, 
12781, 12788, 12795, 12802, 12809, 12816, 12823, 12830, 12837, 
12844, 12851, 12858, 12865, 12872, 12879, 12886, 12893, 12900, 
12907, 12914, 12921, 12928, 12935, 12942, 12949, 12956, 12963, 
12970, 12977, 12984, 12991, 12998, 13005, 13012, 13019, 13026, 
13033, 13040, 13047, 13054, 13061, 13068, 13075, 13082, 13089, 
13096, 13103, 13110, 13117, 13124, 13131, 13138, 13145, 13152, 
13159, 13166, 13173, 13180, 13187, 13194, 13201, 13208, 13215, 
13222, 13229, 13236, 13243, 13250, 13257, 13264, 13271, 13278, 
13285, 13292, 13299, 13306, 13313, 13320, 13327, 13334, 13341, 
13348, 13355, 13362, 13369, 13376, 13383, 13390, 13397, 13404, 
13411, 13418, 13425, 13432, 13439, 13446, 13453, 13460, 13467, 
13474, 13481, 13488, 13495, 13502, 13509, 13516, 13523, 13530, 
13537, 13544, 13551, 13558, 13565, 13572, 13579, 13586, 13593, 
13600, 13607, 13614, 13621, 13628, 13635, 13642, 13649, 13656, 
13663, 13670, 13677, 13684, 13691, 13698, 13705, 13712, 13719, 
13726, 13733, 13740, 13747, 13754, 13761, 13768, 13775, 13782, 
13789, 13796, 13803, 13810, 13817, 13824, 13831, 13838, 13845, 
13852, 13859, 13866, 13873, 13880, 13887, 13894, 13901, 13908, 
13915, 13922, 13929, 13936, 13943, 13950, 13957, 13964, 13971, 
13978, 13985, 13992, 13999, 14006, 14013, 14020, 14027, 14034, 
14041, 14048, 14055, 14062, 14069, 14076, 14083, 14090, 14097, 
14104, 14111, 14118, 14125, 14132, 14139, 14146, 14153, 14160, 
14167, 14174, 14181, 14188, 14195, 14202, 14209, 14216, 14223, 
14230, 14237, 14244, 14251, 14258, 14265, 14272, 14279, 14286, 
14293, 14300, 14307, 14314, 14321, 14328, 14335, 14342, 14349, 
14356, 14363, 14370, 14377, 14384, 14391, 14398, 14405, 14412, 
14419, 14426, 14433, 14440, 14447, 14454, 14461, 14468, 14475, 
14482, 14489, 14496, 14503, 14510, 14517, 14524, 14531, 14538, 
14545, 14552, 14559, 14566, 14573, 14580, 14587, 14594, 14601, 
14608, 14615, 14622, 14629, 14636, 14643, 14650, 14657, 14664, 
14671, 14678, 14685, 14692, 14699, 14706, 14713, 14720, 14727, 
14734, 14741, 14748, 14755, 14762, 14769, 14776, 14783, 14790, 
14797, 14804, 14811, 14818, 14825, 14832, 14839, 14846, 14853, 
14860, 14867, 14874, 14881, 14888, 14895, 14902, 14909, 14916, 
14923, 14930, 14937, 14944, 14951, 14958, 14965, 14972), class = ""Date""), 
    cases = c(168L, 199L, 214L, 230L, 267L, 373L, 387L, 443L, 
    579L, 821L, 1229L, 1014L, 831L, 648L, 257L, 203L, 137L, 78L, 
    82L, 69L, 45L, 51L, 45L, 63L, 55L, 54L, 52L, 27L, 24L, 12L, 
    10L, 22L, 42L, 32L, 52L, 82L, 95L, 91L, 104L, 143L, 114L, 
    100L, 83L, 113L, 145L, 175L, 222L, 258L, 384L, 755L, 976L, 
    879L, 846L, 1004L, 801L, 799L, 680L, 530L, 410L, 302L, 288L, 
    234L, 269L, 245L, 240L, 176L, 188L, 128L, 96L, 59L, 63L, 
    44L, 52L, 39L, 50L, 36L, 40L, 48L, 32L, 39L, 28L, 29L, 16L, 
    20L, 25L, 25L, 48L, 57L, 76L, 117L, 107L, 91L, 90L, 83L, 
    76L, 86L, 104L, 101L, 116L, 120L, 185L, 290L, 537L, 485L, 
    561L, 1142L, 1213L, 1235L, 1085L, 1052L, 987L, 918L, 746L, 
    620L, 396L, 280L, 214L, 148L, 148L, 94L, 107L, 69L, 55L, 
    69L, 47L, 43L, 49L, 30L, 42L, 51L, 41L, 39L, 40L, 38L, 22L, 
    37L, 26L, 40L, 56L, 54L, 74L, 99L, 114L, 114L, 120L, 114L, 
    123L, 131L, 170L, 147L, 163L, 163L, 160L, 158L, 163L, 124L, 
    115L, 176L, 171L, 214L, 320L, 507L, 902L, 1190L, 1272L, 1282L, 
    1146L, 896L, 597L, 434L, 216L, 141L, 101L, 86L, 65L, 55L, 
    35L, 49L, 29L, 55L, 53L, 57L, 34L, 43L, 42L, 13L, 17L, 20L, 
    27L, 36L, 47L, 64L, 77L, 82L, 82L, 95L, 107L, 96L, 106L, 
    93L, 114L, 102L, 116L, 128L, 123L, 212L, 203L, 165L, 267L, 
    550L, 761L, 998L, 1308L, 1613L, 1704L, 1669L, 1296L, 975L, 
    600L, 337L, 259L, 145L, 91L, 70L, 79L, 63L, 58L, 51L, 53L, 
    39L, 49L, 33L, 47L, 56L, 32L, 43L, 47L, 19L, 32L, 18L, 34L, 
    39L, 63L, 57L, 55L, 69L, 76L, 103L, 99L, 108L, 131L, 113L, 
    106L, 122L, 138L, 136L, 175L, 207L, 324L, 499L, 985L, 1674L, 
    1753L, 1419L, 1105L, 821L, 466L, 274L, 180L, 143L, 82L, 101L, 
    72L, 55L, 71L, 50L, 33L, 26L, 25L, 27L, 21L, 24L, 24L, 20L, 
    18L, 18L, 25L, 23L, 13L, 10L, 16L, 9L, 12L, 16L, 25L, 31L, 
    36L, 40L, 36L, 47L, 32L, 46L, 75L, 63L, 49L, 90L, 83L, 101L, 
    78L, 79L, 98L, 131L, 83L, 122L, 179L, 334L, 544L, 656L, 718L, 
    570L, 323L, 220L, 194L, 125L, 95L, 77L, 46L, 42L, 29L, 35L, 
    21L, 29L, 16L, 14L, 19L, 15L, 19L, 18L, 21L, 10L, 14L, 7L, 
    7L, 5L, 9L, 14L, 11L, 18L, 22L, 39L, 36L, 46L, 44L, 37L, 
    30L, 39L, 37L, 45L, 71L, 59L, 57L, 80L, 68L, 88L, 72L, 74L, 
    208L, 357L, 621L, 839L, 964L, 835L, 735L, 651L, 400L, 292L, 
    198L, 85L, 64L, 41L, 40L, 23L, 18L, 14L, 22L, 9L, 19L, 8L, 
    14L, 12L, 15L, 14L, 4L, 6L, 7L, 7L, 8L, 13L, 10L, 19L, 17L, 
    20L, 22L, 40L, 37L, 45L, 34L, 26L, 35L, 67L, 49L, 77L, 82L, 
    80L, 104L, 88L, 49L, 73L, 113L, 142L, 152L, 206L, 293L, 513L, 
    657L, 919L, 930L, 793L, 603L, 323L, 202L, 112L, 55L, 31L, 
    27L, 15L, 15L, 6L, 13L, 21L, 10L, 11L, 9L, 8L, 11L, 7L, 5L, 
    1L, 4L, 7L, 2L, 6L, 12L, 14L, 21L, 29L, 32L, 26L, 22L, 44L, 
    39L, 47L, 44L, 93L, 145L, 289L, 456L, 685L, 548L, 687L, 773L, 
    575L, 355L, 248L, 179L, 129L, 122L, 103L, 72L, 72L, 36L, 
    26L, 31L, 12L, 14L, 14L, 14L, 7L, 8L, 2L, 7L, 8L, 9L, 26L, 
    10L, 13L, 13L, 5L, 5L, 3L, 6L, 1L, 10L, 6L, 7L, 17L, 12L, 
    21L, 32L, 29L, 18L, 22L, 24L, 38L, 52L, 53L, 73L, 49L, 52L, 
    70L, 77L, 95L, 135L, 163L, 303L, 473L, 823L, 1126L, 1052L, 
    794L, 459L, 314L, 252L, 111L, 55L, 35L, 14L, 30L, 21L, 16L, 
    9L, 11L, 6L, 6L, 8L, 9L, 9L, 10L, 15L, 15L, 11L, 6L, 3L, 
    8L, 4L, 7L, 7L, 13L, 10L, 23L, 24L, 36L, 25L, 34L, 37L, 46L, 
    39L, 37L, 55L, 65L, 54L, 60L, 82L, 55L, 53L, 61L, 52L, 75L, 
    92L, 121L, 170L, 199L, 231L, 259L, 331L, 357L, 262L, 154L, 
    77L, 34L, 41L, 21L, 17L, 16L, 7L, 15L, 11L, 7L, 5L, 6L, 13L, 
    7L, 6L, 8L, 7L, 1L, 11L, 9L, 3L, 9L, 9L, 8L, 15L, 19L, 16L, 
    10L, 12L, 26L, 35L, 35L, 41L, 34L, 30L, 36L, 43L, 23L, 55L, 
    107L, 141L, 217L, 381L, 736L, 782L, 663L, 398L, 182L, 137L, 
    79L, 28L, 26L, 16L, 14L, 8L, 4L, 4L, 6L, 6L, 11L, 4L, 5L, 
    7L, 7L, 6L, 8L, 2L, 3L, 3L, 1L, 1L, 3L, 3L, 2L, 8L, 8L, 11L, 
    10L, 11L, 8L, 24L, 25L, 25L, 33L, 36L, 51L, 61L, 74L, 92L, 
    89L, 123L, 402L, 602L, 524L, 494L, 406L, 344L, 329L, 225L, 
    136L, 136L, 84L, 55L, 55L, 42L, 19L, 28L, 8L, 7L, 2L, 7L, 
    6L, 4L, 3L, 5L, 3L, 3L, 0L, 1L, 2L, 3L, 2L, 1L, 2L, 2L, 9L, 
    4L, 9L, 10L, 18L, 15L, 13L, 12L, 10L, 19L, 15L, 22L, 23L, 
    34L, 43L, 53L, 47L, 57L, 328L, 552L, 787L, 736L, 578L, 374L, 
    228L, 161L, 121L, 96L, 58L, 50L, 37L, 14L, 9L, 6L, 15L, 12L, 
    9L, 1L, 6L, 4L, 7L, 7L, 3L, 6L, 9L, 15L, 22L, 28L, 34L, 62L, 
    54L, 75L, 65L, 58L, 57L, 60L, 37L, 47L, 60L, 89L, 90L, 193L, 
    364L, 553L, 543L, 676L, 550L, 403L, 252L, 140L, 125L, 99L, 
    63L, 63L, 76L, 85L, 68L, 67L, 38L, 25L, 24L, 11L, 9L, 9L, 
    4L, 8L, 4L, 6L, 5L, 2L, 6L, 4L, 4L, 1L, 5L, 4L, 1L, 2L, 2L, 
    2L, 2L, 3L, 4L, 4L, 7L, 5L, 2L, 10L, 11L, 17L, 11L, 16L, 
    15L, 11L, 12L, 21L, 20L, 25L, 46L, 51L, 90L, 123L)), .Names = c(""date"", 
""cases""), row.names = c(NA, -835L), class = ""data.frame"")
</code></pre>
"
"0.120192462032251","0.117130321416455"," 63681","<p>I have been adamantly searching the web to learn how to successfully implement a dynamic regression time series in the forecast package for R. The time series data that I am using is weekly data (frequency=52) of incoming call volume and prediction variables are mailers sent out every now and then. They are a significant predictor of the data for the week that they hit, the following week, and the week after that. I have created lagged variables and use these three as the predictors. </p>

<p>My main concern is that the arima model is not taking into account the time series frequency. When I tell it to recognize the ts with a frequency of 52 it has an error. 
I have looked at the <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">fortrain function</a> but do not understand it. I also have looked at the tbats suggested but found that those will not work with prediction variables. </p>

<p>The Zoo function recognizes 52 frequency but it is not advised to use with the <a href=""http://stackoverflow.com/questions/16050684/using-the-combination-forecastauto-arima"">forecast package</a>.</p>

<p>Here is the basic code. The problem is that the time series calwater[,5] is not recognized as such. It is imputed as a simple vector as an integer...</p>

<pre><code>#this works without taking into acount the ts
fit2 &lt;- auto.arima(calwater[6:96,5], xreg=calwater[6:96,6:8], d=0)
fccal &lt;- forecast(fit2, xreg=calwater[97:106,6:8], h=10)
fccal
plot(fccal, main=""Forecast Cal Water"", ylab=""Calls"")

#to form a ts object
calincall&lt;-ts(calwater[1:106,5],start=c(2011,23),frequency=52)

#once the ts is added to the model this dispalys
#Error in `[.default`(calincall, 2:100, 1) : incorrect number of dimensions
</code></pre>

<p>Maybe the error is because there is just a little over two years of data. </p>

<pre><code>#Time Series: Start = c(2011, 23), End = c(2013, 24),Frequency = 52 
</code></pre>

<p>I would be very grateful for any guidance in for this particular issue. I am using the forecast package and prefer to continue within the package but I am open to suggestions. </p>
"
"0.120192462032251","0.117130321416455"," 65690","<p>I fit a logistic on three numeric continuous variables, followed by a categorical factor [Y, N].</p>

<pre><code>logit2A &lt;- glm(DisclosedDriver ~ VehDrvr_Dif+POL_SEQ_NUM+PRMTOTAL+SAFE_DRVR_PLEDGE_FLG, data = DF, family = ""binomial"") 
</code></pre>

<p>Fit looks wonderful.</p>

<pre><code>Coefficients:
                    Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)           -2.204e+00  2.253e-01  -9.782  &lt; 2e-16 ***
VehDrvr_Dif            2.918e-01  1.026e-01   2.845 0.004440 ** 
POL_SEQ_NUM           -1.893e-01  5.617e-02  -3.370 0.000751 ***
PRMTOTAL               1.109e-04  5.526e-05   2.006 0.044804 *  
SAFE_DRVR_PLEDGE_FLGY -7.220e-01  1.633e-01  -4.422 9.76e-06 ***
</code></pre>

<p>So obviously R took the Safe_Drvr_Pledge_Flg categorical factor variable and placed all 'N' values in reference or intercept as opposed to the listed 'Y'.</p>

<p>Now I want to take my fit and calculate the probabilities that my model determines. And here comes the error:</p>

<pre><code>&gt; DF$P_GLM&lt;- predict.glm(logit2A, DF, type=""response"", se.fit=FALSE)
    Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) : 
factor SAFE_DRVR_PLEDGE_FLG has new levels 
</code></pre>

<p>Umm... no it doesn't, because I just fit the model with the exact same data I'm trying to use for the prediction. What's the problem?</p>

<p>Trying to respond to first comment:
Don't know what you mean. I've got 3500 rows of data... It's a logistic regression on 4 continuous variables and one categorical. The categorical has two values, Y or N. My glm fit give the numbers given. I just want to plug it all back in with the predict function and it gives me that error. Here's the categorical variable:</p>

<pre><code> &gt; DF$SAFE_DRVR_PLEDGE_FLG
 [1] Y Y Y Y Y Y Y N Y Y Y Y Y Y Y Y Y Y Y N Y Y N Y Y N Y Y N Y Y Y Y Y Y Y Y Y Y N Y Y N Y Y Y N Y Y Y Y Y N Y Y Y Y Y Y
 [60] Y Y Y Y N Y Y Y Y Y Y Y Y N Y Y Y N N Y N Y Y Y Y Y N Y Y N Y N N Y Y Y N Y Y Y Y N Y Y Y Y Y N Y N Y N Y Y Y Y Y N Y
 [119] N Y Y Y Y Y Y Y Y N Y Y Y Y Y Y N Y Y Y N Y Y Y N Y Y Y N N Y N N N Y N Y Y Y N N Y Y N Y Y Y Y N N Y Y Y Y N N Y N N
 Levels:  N Y
</code></pre>

<p>What do you mean by a working example? The fit works. The probability output of the predict function doesn't...</p>
"
"0.113318539934015","0.110431526074847"," 66250","<p>I have the following data frame</p>

<pre><code>structure(list(Chi = structure(c(1L, 1L, 5L, 5L, 6L, 9L, 9L, 
12L, 13L, 14L, 14L, 16L, 16L, 19L, 19L, 20L, 20L, 23L, 24L, 24L, 
26L, 26L, 31L, 31L, 33L, 33L, 36L, 37L, 37L, 40L, 40L, 43L, 43L, 
44L, 44L, 45L, 46L, 47L, 47L, 48L, 48L, 52L, 52L, 54L, 54L, 55L, 
55L, 56L, 59L, 59L, 61L, 61L, 63L, 63L, 64L, 64L, 65L, 65L, 69L, 
69L, 70L, 70L, 71L, 71L, 72L, 72L, 75L, 75L, 76L, 76L, 77L, 77L, 
79L, 79L, 86L, 86L, 87L, 87L, 88L, 88L, 91L, 91L, 92L, 92L, 93L, 
95L, 96L, 96L, 97L, 97L, 98L, 98L, 99L, 99L, 100L, 100L, 101L, 
101L, 103L, 103L, 104L, 104L, 107L, 108L, 108L, 112L, 112L, 113L, 
116L, 116L, 117L, 120L, 125L, 125L, 127L, 127L, 129L, 129L, 130L, 
131L, 131L, 132L, 132L, 134L, 134L, 135L, 135L, 136L, 136L, 139L, 
141L, 141L, 143L, 144L, 144L, 145L, 145L, 146L, 150L, 150L, 151L, 
151L, 153L, 153L, 155L, 155L, 157L, 162L, 162L, 163L, 163L, 164L, 
164L, 167L, 167L, 168L, 169L, 169L, 171L, 171L, 172L, 172L, 174L, 
174L, 175L, 175L, 177L, 177L, 180L, 180L, 183L, 187L, 27L, 83L, 
83L, 165L, 165L, 85L, 85L, 156L, 156L, 17L, 17L, 123L, 123L, 
124L, 124L, 57L, 57L, 42L, 42L, 159L, 159L, 38L, 38L, 82L, 82L, 
41L, 41L, 142L, 142L), .Label = c(""0106610856"", ""0107470802"", 
""0108490513"", ""0108590534"", ""0109480651"", ""0111290260"", ""0111410339"", 
""0201390418"", ""0207570604"", ""0208360352"", ""0212323105"", ""0212380362"", 
""0301310432"", ""0302705635"", ""0303450495"", ""0304260266"", ""0304440574"", 
""0305280546"", ""0305380338"", ""0305381393"", ""0305510576"", ""0305542214"", 
""0308610733"", ""0309370345"", ""0309665035"", ""0310380545"", ""0403320259"", 
""0403360374"", ""0404360343"", ""0406270198"", ""0501451137"", ""0504460676"", 
""0511310366"", ""0605270511"", ""0605340560"", ""0605410461"", ""0605410585"", 
""0606260684"", ""0606270353"", ""0609360507"", ""0702520535"", ""0702570818"", 
""0705430421"", ""0710380364"", ""0801330378"", ""0801430275"", ""0802320430"", 
""0803510802"", ""0805390383"", ""0806560533"", ""0809430460"", ""0902380354"", 
""0904340252"", ""0904370445"", ""0906340403"", ""0907380379"", ""0909415420"", 
""0910300100"", ""0911430253"", ""1001270460"", ""1001360389"", ""1002455294"", 
""1005280487"", ""1006330445"", ""1009350447"", ""1010375156"", ""1011270447"", 
""1012350312"", ""1012400441"", ""1102570648"", ""1105450589"", ""1106230566"", 
""1106330587"", ""1204530475"", ""1206350342"", ""1208330373"", ""1209280345"", 
""1209400502"", ""1209400561"", ""1210380536"", ""1302240455"", ""1305751256"", 
""1306370353"", ""1307260470"", ""1310340250"", ""1312430613"", ""1312440597"", 
""1312690593"", ""1404430512"", ""1404530479"", ""1405330376"", ""1406310360"", 
""1406350419"", ""1406430439"", ""1408460602"", ""1412360366"", ""1502385236"", 
""1503370488"", ""1503470628"", ""1503660400"", ""1506390447"", ""1508340196"", 
""1510340688"", ""1510440453"", ""1603310622"", ""1604440376"", ""1606370014"", 
""1609650549"", ""1610345304"", ""1610345304x"", ""1612300367"", ""1702330397"", 
""1704330181"", ""1706330316"", ""1712560522"", ""1802340270"", ""1804310336"", 
""1808430417"", ""1810400244"", ""1902340299"", ""1902610679"", ""1905360355"", 
""1906320438"", ""1906390525"", ""1909310514"", ""1912460408"", ""2002440204"", 
""2004350288"", ""2007350203"", ""2009400364"", ""2009460669"", ""2011410428"", 
""2011500524"", ""2103335236"", ""2109370262"", ""2112290355"", ""2201330484"", 
""2201600686"", ""2203290471"", ""2203406259"", ""2205430513"", ""2207340473"", 
""2208340396"", ""2303430410"", ""2303530717"", ""2308290390"", ""2309420506"", 
""2310370398"", ""2310370398.0"", ""2312280310"", ""2404436295"", ""2406640663"", 
""2411420404"", ""2501520858"", ""2505330239"", ""2505380376"", ""2511320428"", 
""2511320436"", ""2511360306"", ""2601490470"", ""2601520566"", ""2608450598"", 
""2611400237"", ""2701470625"", ""2702230407"", ""2702340342"", ""2703470916"", 
""2704380538"", ""2709250586"", ""2712350545"", ""2712541146"", ""2805310438"", 
""2805350472"", ""2807360475"", ""2807480594"", ""2809325316"", ""2809470634"", 
""2902400411"", ""2903350442"", ""2905330376"", ""2906450480"", ""2910240363"", 
""3004510529"", ""3007230195"", ""3012410333"", ""3107440299"", ""3108350420""
), class = ""factor""), Sex = structure(c(2L, 2L, 2L, 2L, 1L, 1L, 
1L, 1L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 
2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 
2L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 
1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 1L, 
1L, 1L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
2L, 1L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 
2L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 
2L, 2L, 2L), .Label = c(""F"", ""M""), class = ""factor""), Age = c(50L, 
50L, 63L, 63L, 83L, 55L, 55L, 72L, 81L, 42L, 42L, 86L, 86L, 74L, 
74L, 74L, 74L, 50L, 74L, 74L, 73L, 73L, 67L, 67L, 79L, 79L, 71L, 
70L, 70L, 75L, 75L, 68L, 68L, 73L, 73L, 79L, 69L, 79L, 79L, 61L, 
61L, 74L, 74L, 74L, 74L, 77L, 77L, 73L, 68L, 68L, 76L, 76L, 84L, 
84L, 78L, 78L, 77L, 77L, 71L, 71L, 55L, 55L, 67L, 67L, 88L, 88L, 
77L, 77L, 78L, 78L, 84L, 84L, 71L, 71L, 69L, 69L, 67L, 67L, 41L, 
41L, 78L, 78L, 80L, 80L, 76L, 66L, 76L, 76L, 73L, 73L, 74L, 74L, 
64L, 64L, 46L, 46L, 72L, 72L, 78L, 78L, 67L, 67L, 74L, 47L, 47L, 
79L, 79L, 79L, 78L, 78L, 81L, 77L, 79L, 79L, 67L, 67L, 76L, 76L, 
70L, 64L, 64L, 70L, 70L, 79L, 79L, 74L, 74L, 82L, 82L, 83L, 69L, 
69L, 76L, 69L, 69L, 58L, 58L, 83L, 83L, 83L, 68L, 68L, 69L, 69L, 
79L, 79L, 79L, 66L, 66L, 70L, 70L, 65L, 65L, 65L, 65L, 72L, 87L, 
87L, 57L, 57L, 80L, 80L, 76L, 76L, 63L, 63L, 64L, 64L, 78L, 78L, 
60L, 76L, 80L, 75L, 75L, 90L, 90L, 78L, 78L, 74L, 74L, 69L, 69L, 
80L, 80L, 73L, 73L, 71L, 71L, 56L, 56L, 76L, 76L, 87L, 87L, 38L, 
38L, 61L, 61L, 78L, 78L), SBR = c(12.061, 11.447, 9.403, 9.136, 
9.747, 8.648, 7.934, 7.914, 9.349, 11.224, 10.433, 4.897, 5.823, 
8.683, 8.692, 13.018, 13.386, 7.817, 7.384, 7.518, 11.091, 11.028, 
8.372, 8.497, 10.751, 10.488, 4.347, 2.593, 2.203, 6.461, 7.272, 
4.581, 4.593, 10.31, 9.004, 10.362, 10.307, 9.266, 10.163, 9.24, 
8.732, 8.449, 7.823, 10.427, 10.669, 8.695, 8.729, 8.653, 12.299, 
12.158, 11.748, 11.19, 8.431, 8.717, 8.253, 8.412, 6.911, 6.805, 
9.468, 11.413, 6.603, 7.697, 7.762, 7.097, 10.607, 8.162, 5.419, 
5.575, 7.007, 6.974, 8.708, 8.419, 9.47, 8.42, 8.229, 8.027, 
5.294, 4.628, 11.475, 10.328, 7.905, 8.491, 10.724, 9.02, 9.095, 
5.754, 9.805, 7.332, 6.669, 5.118, 12.443, 11.972, 13.309, 13.906, 
14.963, 15.119, 6.465, 6.38, 6.949, 6.064, 6.541, 6.648, 3.542, 
11.148, 11.918, 9.743, 9.795, 6.103, 6.025, 3.917, 7.304, 7.628, 
8.092, 7.347, 9.051, 8.206, 10.697, 10.286, 4.564, 10.62, 9.84, 
9.105, 7.998, 6.437, 5.707, 6.949, 6.315, 6.165, 6.68, 8.86, 
8.326, 8.6, 7.776, 5.193, 5.456, 11.864, 11.381, 6.385, 10.972, 
9.87, 9.645, 7.738, 10.096, 9.667, 9.687, 8.255, 4.606, 8.738, 
8.519, 7.002, 6.288, 10.425, 10.303, 8.278, 8.342, 6.657, 6.111, 
5.928, 13.06, 12.747, 5.545, 5.845, 9.338, 9.534, 9.635, 8.716, 
7.765, 7.254, 7.517, 7.317, 7.335, 5.628, 4.864, 7.1, 7.02, 6.734, 
5.622, 7.167, 7.391, 6.443, 6.874, 8.373, 7.573, 5.701, 6.355, 
6.884, 6.296, 9.097, 9.645, 7.068, 7.252, 6, 5.794, 8.074, 9.267, 
12.584, 10.723, 9.39, 9.165, 9.635, 8.814), Diagnosis = structure(c(2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), .Label = c(""A"", ""N""), class = ""factor""), 
    fit = c(10.1654358296645, 10.1654358296645, 9.07109655193284, 
    9.07109655193284, 7.38749766311491, 9.74453610746002, 9.74453610746002, 
    8.31347705196477, 7.5558575519967, 10.8388753851917, 10.8388753851917, 
    7.13495782979222, 7.13495782979222, 8.14511716308298, 8.14511716308298, 
    8.14511716308298, 8.14511716308298, 10.1654358296645, 8.14511716308298, 
    8.14511716308298, 8.22929710752388, 8.22929710752388, 8.73437677416926, 
    8.73437677416926, 7.7242174408785, 7.7242174408785, 8.39765699640567, 
    8.48183694084657, 8.48183694084657, 8.06093721864208, 8.06093721864208, 
    8.65019682972836, 8.65019682972836, 8.22929710752388, 8.22929710752388, 
    7.7242174408785, 8.56601688528746, 7.7242174408785, 7.7242174408785, 
    9.23945644081464, 9.23945644081464, 8.14511716308298, 8.14511716308298, 
    8.14511716308298, 8.14511716308298, 7.89257732976029, 7.89257732976029, 
    8.22929710752388, 8.65019682972836, 8.65019682972836, 7.97675727420119, 
    7.97675727420119, 7.30331771867401, 7.30331771867401, 7.80839738531939, 
    7.80839738531939, 7.89257732976029, 7.89257732976029, 8.39765699640567, 
    8.39765699640567, 9.74453610746002, 9.74453610746002, 8.73437677416926, 
    8.73437677416926, 6.96659794091043, 6.96659794091043, 7.89257732976029, 
    7.89257732976029, 7.80839738531939, 7.80839738531939, 7.30331771867401, 
    7.30331771867401, 8.39765699640567, 8.39765699640567, 8.56601688528746, 
    8.56601688528746, 8.73437677416926, 8.73437677416926, 10.9230553296326, 
    10.9230553296326, 7.80839738531939, 7.80839738531939, 7.6400374964376, 
    7.6400374964376, 7.97675727420119, 8.81855671861015, 7.97675727420119, 
    7.97675727420119, 8.22929710752388, 8.22929710752388, 8.14511716308298, 
    8.14511716308298, 8.98691660749195, 8.98691660749195, 10.5021556074281, 
    10.5021556074281, 8.31347705196477, 8.31347705196477, 7.80839738531939, 
    7.80839738531939, 8.73437677416926, 8.73437677416926, 8.14511716308298, 
    10.4179756629872, 10.4179756629872, 7.7242174408785, 7.7242174408785, 
    7.7242174408785, 7.80839738531939, 7.80839738531939, 7.5558575519967, 
    7.89257732976029, 7.7242174408785, 7.7242174408785, 8.73437677416926, 
    8.73437677416926, 7.97675727420119, 7.97675727420119, 8.48183694084657, 
    8.98691660749195, 8.98691660749195, 8.48183694084657, 8.48183694084657, 
    7.7242174408785, 7.7242174408785, 8.14511716308298, 8.14511716308298, 
    7.47167760755581, 7.47167760755581, 7.38749766311491, 8.56601688528746, 
    8.56601688528746, 7.97675727420119, 8.56601688528746, 8.56601688528746, 
    9.49199627413733, 9.49199627413733, 7.38749766311491, 7.38749766311491, 
    7.38749766311491, 8.65019682972836, 8.65019682972836, 8.56601688528746, 
    8.56601688528746, 7.7242174408785, 7.7242174408785, 7.7242174408785, 
    8.81855671861015, 8.81855671861015, 8.48183694084657, 8.48183694084657, 
    8.90273666305105, 8.90273666305105, 8.90273666305105, 8.90273666305105, 
    8.31347705196477, 7.05077788535132, 7.05077788535132, 9.57617621857822, 
    9.57617621857822, 7.6400374964376, 7.6400374964376, 7.97675727420119, 
    7.97675727420119, 9.07109655193284, 9.07109655193284, 8.98691660749195, 
    8.98691660749195, 7.80839738531939, 7.80839738531939, 9.32363638525553, 
    7.97675727420119, 7.6400374964376, 8.06093721864208, 8.06093721864208, 
    6.79823805202863, 6.79823805202863, 7.80839738531939, 7.80839738531939, 
    8.14511716308298, 8.14511716308298, 8.56601688528746, 8.56601688528746, 
    7.6400374964376, 7.6400374964376, 8.22929710752388, 8.22929710752388, 
    8.39765699640567, 8.39765699640567, 9.66035616301912, 9.66035616301912, 
    7.97675727420119, 7.97675727420119, 7.05077788535132, 7.05077788535132, 
    11.1755951629553, 11.1755951629553, 9.23945644081464, 9.23945644081464, 
    7.80839738531939, 7.80839738531939), lwr = c(5.90999794584117, 
    5.90999794584117, 4.85411038352648, 4.85411038352648, 3.16383967274129, 
    5.5078318233643, 5.5078318233643, 4.10340735387365, 3.33646831235876, 
    6.54339594841324, 6.54339594841324, 2.90340734886211, 2.90340734886211, 
    3.93437871118446, 3.93437871118446, 3.93437871118446, 3.93437871118446, 
    5.90999794584117, 3.93437871118446, 3.93437871118446, 4.01899333351728, 
    4.01899333351728, 4.52246825860982, 4.52246825860982, 3.50829974380329, 
    3.50829974380329, 4.18762073879345, 4.27163348349591, 4.27163348349591, 
    3.84956354899072, 3.84956354899072, 4.43905717652239, 4.43905717652239, 
    4.01899333351728, 4.01899333351728, 3.50829974380329, 4.35544561188301, 
    3.50829974380329, 3.50829974380329, 5.0187327527967, 5.0187327527967, 
    3.93437871118446, 3.93437871118446, 3.93437871118446, 3.93437871118446, 
    3.67933199647321, 3.67933199647321, 4.01899333351728, 4.43905717652239, 
    4.43905717652239, 3.76454793766266, 3.76454793766266, 3.07722713717148, 
    3.07722713717148, 3.59391587315661, 3.59391587315661, 3.67933199647321, 
    3.67933199647321, 4.18762073879345, 4.18762073879345, 5.5078318233643, 
    5.5078318233643, 4.52246825860982, 4.52246825860982, 2.72879720476565, 
    2.72879720476565, 3.67933199647321, 3.67933199647321, 3.59391587315661, 
    3.59391587315661, 3.07722713717148, 3.07722713717148, 4.18762073879345, 
    4.18762073879345, 4.35544561188301, 4.35544561188301, 4.52246825860982, 
    4.52246825860982, 6.62171309179384, 6.62171309179384, 3.59391587315661, 
    3.59391587315661, 3.42248381273475, 3.42248381273475, 3.76454793766266, 
    4.60567896791123, 3.76454793766266, 3.76454793766266, 4.01899333351728, 
    4.01899333351728, 3.93437871118446, 3.93437871118446, 4.77149984958079, 
    4.77149984958079, 6.22823093408572, 6.22823093408572, 4.10340735387365, 
    4.10340735387365, 3.59391587315661, 3.59391587315661, 4.52246825860982, 
    4.52246825860982, 3.93437871118446, 6.14896197954192, 6.14896197954192, 
    3.50829974380329, 3.50829974380329, 3.50829974380329, 3.59391587315661, 
    3.59391587315661, 3.33646831235876, 3.67933199647321, 3.50829974380329, 
    3.50829974380329, 4.52246825860982, 4.52246825860982, 3.76454793766266, 
    3.76454793766266, 4.27163348349591, 4.77149984958079, 4.77149984958079, 
    4.27163348349591, 4.27163348349591, 3.50829974380329, 3.50829974380329, 
    3.93437871118446, 3.93437871118446, 3.25025350300496, 3.25025350300496, 
    3.16383967274129, 4.35544561188301, 4.35544561188301, 3.76454793766266, 
    4.35544561188301, 4.35544561188301, 5.26417374160422, 5.26417374160422, 
    3.16383967274129, 3.16383967274129, 3.16383967274129, 4.43905717652239, 
    4.43905717652239, 4.35544561188301, 4.35544561188301, 3.50829974380329, 
    3.50829974380329, 3.50829974380329, 4.60567896791123, 4.60567896791123, 
    4.27163348349591, 4.27163348349591, 4.68868944268447, 4.68868944268447, 
    4.68868944268447, 4.68868944268447, 4.10340735387365, 2.81620086292774, 
    2.81620086292774, 5.34559069482765, 5.34559069482765, 3.42248381273475, 
    3.42248381273475, 3.76454793766266, 3.76454793766266, 4.85411038352648, 
    4.85411038352648, 4.77149984958079, 4.77149984958079, 3.59391587315661, 
    3.59391587315661, 5.10074511800645, 3.76454793766266, 3.42248381273475, 
    3.84956354899072, 3.84956354899072, 2.55340019612735, 2.55340019612735, 
    3.59391587315661, 3.59391587315661, 3.93437871118446, 3.93437871118446, 
    4.35544561188301, 4.35544561188301, 3.42248381273475, 3.42248381273475, 
    4.01899333351728, 4.01899333351728, 4.18762073879345, 4.18762073879345, 
    5.42680991723477, 5.42680991723477, 3.76454793766266, 3.76454793766266, 
    2.81620086292774, 2.81620086292774, 6.85553891121225, 6.85553891121225, 
    5.0187327527967, 5.0187327527967, 3.59391587315661, 3.59391587315661
    ), upr = c(14.4208737134878, 14.4208737134878, 13.2880827203392, 
    13.2880827203392, 11.6111556534885, 13.9812403915557, 13.9812403915557, 
    12.5235467500559, 11.7752467916346, 15.1343548219701, 15.1343548219701, 
    11.3665083107223, 11.3665083107223, 12.3558556149815, 12.3558556149815, 
    12.3558556149815, 12.3558556149815, 14.4208737134878, 12.3558556149815, 
    12.3558556149815, 12.4396008815305, 12.4396008815305, 12.9462852897287, 
    12.9462852897287, 11.9401351379537, 11.9401351379537, 12.6076932540179, 
    12.6920403981972, 12.6920403981972, 12.2723108882934, 12.2723108882934, 
    12.8613364829343, 12.8613364829343, 12.4396008815305, 12.4396008815305, 
    11.9401351379537, 12.7765881586919, 11.9401351379537, 11.9401351379537, 
    13.4601801288326, 13.4601801288326, 12.3558556149815, 12.3558556149815, 
    12.3558556149815, 12.3558556149815, 12.1058226630474, 12.1058226630474, 
    12.4396008815305, 12.8613364829343, 12.8613364829343, 12.1889666107397, 
    12.1889666107397, 11.5294083001765, 11.5294083001765, 12.0228788974822, 
    12.0228788974822, 12.1058226630474, 12.1058226630474, 12.6076932540179, 
    12.6076932540179, 13.9812403915557, 13.9812403915557, 12.9462852897287, 
    12.9462852897287, 11.2043986770552, 11.2043986770552, 12.1058226630474, 
    12.1058226630474, 12.0228788974822, 12.0228788974822, 11.5294083001765, 
    11.5294083001765, 12.6076932540179, 12.6076932540179, 12.7765881586919, 
    12.7765881586919, 12.9462852897287, 12.9462852897287, 15.2243975674713, 
    15.2243975674713, 12.0228788974822, 12.0228788974822, 11.8575911801404, 
    11.8575911801404, 12.1889666107397, 13.0314344693091, 12.1889666107397, 
    12.1889666107397, 12.4396008815305, 12.4396008815305, 12.3558556149815, 
    12.3558556149815, 13.2023333654031, 13.2023333654031, 14.7760802807705, 
    14.7760802807705, 12.5235467500559, 12.5235467500559, 12.0228788974822, 
    12.0228788974822, 12.9462852897287, 12.9462852897287, 12.3558556149815, 
    14.6869893464325, 14.6869893464325, 11.9401351379537, 11.9401351379537, 
    11.9401351379537, 12.0228788974822, 12.0228788974822, 11.7752467916346, 
    12.1058226630474, 11.9401351379537, 11.9401351379537, 12.9462852897287, 
    12.9462852897287, 12.1889666107397, 12.1889666107397, 12.6920403981972, 
    13.2023333654031, 13.2023333654031, 12.6920403981972, 12.6920403981972, 
    11.9401351379537, 11.9401351379537, 12.3558556149815, 12.3558556149815, 
    11.6931017121067, 11.6931017121067, 11.6111556534885, 12.7765881586919, 
    12.7765881586919, 12.1889666107397, 12.7765881586919, 12.7765881586919, 
    13.7198188066704, 13.7198188066704, 11.6111556534885, 11.6111556534885, 
    11.6111556534885, 12.8613364829343, 12.8613364829343, 12.7765881586919, 
    12.7765881586919, 11.9401351379537, 11.9401351379537, 11.9401351379537, 
    13.0314344693091, 13.0314344693091, 12.6920403981972, 12.6920403981972, 
    13.1167838834176, 13.1167838834176, 13.1167838834176, 13.1167838834176, 
    12.5235467500559, 11.2853549077749, 11.2853549077749, 13.8067617423288, 
    13.8067617423288, 11.8575911801404, 11.8575911801404, 12.1889666107397, 
    12.1889666107397, 13.2880827203392, 13.2880827203392, 13.2023333654031, 
    13.2023333654031, 12.0228788974822, 12.0228788974822, 13.5465276525046, 
    12.1889666107397, 11.8575911801404, 12.2723108882934, 12.2723108882934, 
    11.0430759079299, 11.0430759079299, 12.0228788974822, 12.0228788974822, 
    12.3558556149815, 12.3558556149815, 12.7765881586919, 12.7765881586919, 
    11.8575911801404, 11.8575911801404, 12.4396008815305, 12.4396008815305, 
    12.6076932540179, 12.6076932540179, 13.8939024088035, 13.8939024088035, 
    12.1889666107397, 12.1889666107397, 11.2853549077749, 11.2853549077749, 
    15.4956514146983, 15.4956514146983, 13.4601801288326, 13.4601801288326, 
    12.0228788974822, 12.0228788974822)), .Names = c(""Chi"", ""Sex"", 
""Age"", ""SBR"", ""Diagnosis"", ""fit"", ""lwr"", ""upr""), row.names = c(NA, 
201L), class = ""data.frame"")
</code></pre>

<p>I plot SBR v Age for each Sex using <code>ggplot2</code></p>

<pre><code>p &lt;- ggplot(sbr_with_pred, aes(x=Age, y=SBR)) + geom_point(aes(col=Sex), 
                                                           shape=19, alpha=0.4) + 
            geom_smooth(aes(col=Sex),method = 'lm', se=FALSE,linetype=2) + 
            geom_ribbon(aes(y = fit, ymin = lwr, ymax = upr, fill = 'prediction'), 
                        linetype =2,alpha = 0.1) + 
            scale_fill_manual('Interval', values = c('blue')) + theme_bw() + 
            theme(legend.position = ""right"") + 
            scale_y_continuous(limits = c(-3,15.5),breaks = c(0,5,10,15)) + 
            scale_color_manual(""Sex"", values = c('red','blue'))
</code></pre>

<p>which gives the following</p>

<p><img src=""http://i.stack.imgur.com/j3DxP.png"" alt=""enter image description here""></p>

<p>I can get the equation of each regression fit easy enough</p>

<pre><code>lm(formula = SBR ~ Age, data = subset(sbr_with_pred, Sex == ""F""))
lm(formula = SBR ~ Age, data = subset(sbr_with_pred, Sex == ""M""))
</code></pre>

<p>However how do I test whether or not they are significantly different (which they are not). I think analysis of covariance is the appropriate test but I do not know how to implement this in R</p>
"
"NaN","NaN"," 66946","<p>When you predict a fitted value from a logistic regression model, how are standard errors computed?  I mean for the <em>fitted values</em>, not for the coefficients (which involves Fishers information matrix).</p>

<p>I only found out how to get the numbers with <code>R</code> (e.g., <a href=""https://stat.ethz.ch/pipermail/r-help/2010-August/248241.html"">here</a> on r-help, or <a href=""http://stackoverflow.com/questions/14423325/confidence-intervals-for-predictions-from-logistic-regression"">here</a> on Stack Overflow), but I cannot find the formula.</p>

<pre><code>pred &lt;- predict(y.glm, newdata= something, se.fit=TRUE)
</code></pre>

<p>If you could provide online source (preferably on a university website), that would be fantastic.</p>
"
"0.0693931503088838","0.0676252226000574"," 67470","<p>I want to predict a categorical variable using also categorical predictors. Currently, I am looking at classification and regression trees (CART).</p>

<p>The prediction quality is ""good enough"", except for the presence of impossible combinations. In the following minimal example, the combination <code>a==2, b==2</code> is impossible, yet the estimation decides not to use <code>b</code> for splitting.</p>

<pre><code>&gt; library(rpart)
&gt; d &lt;- data.frame(a=rep(factor(c(1,1,2)), 100000), b=factor(c(1,2,1)))
&gt; xtabs(~., d)
   b
a       1     2
  1 1e+05 1e+05
  2 1e+05 0e+00
&gt; (tr &lt;- rpart(a~b, d))
n= 300000 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

1) root 300000 1e+05 1 (0.6666667 0.3333333) *
</code></pre>

<p>When simulating stochastically from this model (by choosing the leaf value by sampling using the annotated probability vector, here $(2/3, 1/3)$, as weights), the combination <code>2, 2</code> will occur:</p>

<pre><code>&gt; prob.m &lt;- predict(tr, d, type=""prob"")
&gt; d$a.sim &lt;- apply(prob.m, 1, function(x) sample.int(length(x), size=1, prob=x))
&gt; xtabs(~a.sim+b, d)
     b
a.sim      1      2
    1 133041  66615
    2  66959  33385
</code></pre>

<p>Is there a way to avoid this, perhaps using another method?</p>

<p>This is just a small example for a more general case. I have around 10 predictors, and I want to exclude all combinations of two (or perhaps three) attributes that have no observation in the sample.</p>

<p>I am aware of the ""loss matrix"" that can be specified as a parameter to <code>rpart</code>, but this is prohibitive if many predictors are used.</p>
"
"NaN","NaN"," 67680","<p>I am using a <a href=""http://en.wikipedia.org/wiki/Relevance_vector_machine"" rel=""nofollow"">relevance vector machine</a> in R, <a href=""http://rss.acs.unt.edu/Rdoc/library/kernlab/html/rvm.html"" rel=""nofollow"">rvm()</a>, to solve a regression problem. I need to know the variance of the fitted values for each identified RVs. Does anyone know how to program to compute the prediction variances with R's <a href=""http://cran.r-project.org/web/packages/kernlab/index.html"" rel=""nofollow"">Kernlab package</a>?</p>
"
"0.200469100892331","0.202875667800172"," 68812","<p>I'm really new to ARIMA methods and am trying to forecast electricity load. I've integrated: electricity load, temperature, weekday (dummy), public holidays, and school holidays. My model tries to perform a non seasonal ARIMA with linear regression for each hour of the day.</p>

<p>Here is my code for an example of one of the 24 hours (6 AM):</p>

<pre><code># ElecLoad contains hourly loads and other data for 2005 and 2006 (=2*365*24 entries):
# 1. Electricity load in MW
# 2. day of weak: sunday=0, monday=1, etc 
# 3. Hour of the day 0 -&gt; 23
# 4. Public Holiday: 1 if Public Holiday, 0 otherwise
# 5. Scool vacation: 1 if no scool
# 6. Temperature in Â°F

# Create the weak matrix = dumy variables for the weakdays
weakmatrix&lt;-model.matrix(~as.factor(ElecLoad[,2]))
#Remove intercept
weakmatrix&lt;-weakmatrix[,-1]

#Generate FullTable
FullTable&lt;-cbind(load=ElecLoad[,1], weakmatrix, ElecLoad[,4],
                 ElecLoad[,3],ElecLoad[,5],ElecLoad[,5]^2, ElecLoad[,6])
colnames(FullTable)&lt;-c(""Load"",""mon"",""tue"",""wed"",""thu"",""fri"",""sat"",
                       ""ScoolHol"",""PubHol"",""Temp"",""Temp2"",""Hour"")

#Create the xreg = substed for a specific hour of the day (column 12 = Hour)
xreg&lt;-subset(FullTable[,2:11], FullTable[,12] == 7)

#Create the Load time serie, also a subset of the full table
LoadTs&lt;-ts(subset(FullTable[,1], FullTable[,12] == 7),start=1,frequency=1)

#Launch of auto.arima
ArimaLoad&lt;-auto.arima(LoadTs, xreg=xreg, lambda=0)
</code></pre>

<p>When I try to forecast with the same 2 years data as <code>xreg</code>, here is my output</p>

<pre><code>plot(forecast(ArimaLoad,xreg=xreg), include=0)
</code></pre>

<p><img src=""http://i.stack.imgur.com/MpNeH.png"" alt=""enter image description here""></p>

<p>While when I try to plot the fitted it looks identical to my original Load</p>

<pre><code>plot(fitted(ArimaLoad))
</code></pre>

<p><img src=""http://i.stack.imgur.com/zsw81.png"" alt=""enter image description here""></p>

<p>I don't understand why the <code>prediction()</code> is so much different than the <code>fitted()</code> with the same <code>xreg</code> matrix. Is this a normal behaviour, how can I improve my model to better fit with the real situation?</p>

<hr>

<p>Thank you so much for your support.</p>

<p>I'm not sure I understood everything from what you propose.</p>

<p>You mean that I should build a first model to forecast the daily average load (I prefer the average than the sum because due to DST, some days don't have 24 hours...). This model would be deterministic, but I don't see what kind of model you're thinking off? Is a multilinear regression ok? I prefer to consider the log(load) to make the different parameters multiplicative which I think is better fit to the reality.
Then I should have 24 hourly models, taking the daily average then split with a sort seasonal effect?
Should I use somewhere an ARIMA model?
I'm not convince of considering the month as having an effect, in my opinion there is no reason that consumption is more important in January than August except if we consider the Temperature and Holidays effects. The hour of the day is related to the activity that's the reason why I'm considering the specific model for each hour. The same way each day of the weak is different.</p>

<p>I've tried a multilinear regression for the same hour (7:00 AM) and the result looks not so bad.</p>

<pre><code>#Create the frame.data
Load&lt;-subset(FullTable[,1], FullTable[,12] == 7)
FullData&lt;-cbind(LogLoad=log(Load), xreg)
FrameData&lt;-data.frame(FullData)

# multilinear regression
mlin&lt;-lm(LogLoad ~ mon+tue+wed+thu+fri+sat+ScoolHol+PubHol+Temp+`Temp2`, FrameData)
plot(exp(mlin$model$LogLoad), type=""l"",col=""blue"")
lines(exp(fitted(mlin)), col=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/MpNeH.png"" alt=""enter image description here""></p>

<p>fitted() in red which is now exactly the same as predict() if I re-use the same data entry (2005-2006) and looks not so far from the original load in blue (no so bad for a simple model). I still don't fully understand why it did not work with ARIMA as it also takes into consideration multilinear regression.</p>

<p>Now my ""simple"" model already takes into account several parameters, like the temperature, the holiday, the school vacations the day of the weak and the hour of the day (local time, not UCT).
How can I improve my model further more? How can I make sure that the parameters are invariant? Is there a specific method?</p>
"
"0.113318539934015","0.110431526074847"," 70249","<p>I would like to use GLM and Elastic Net to select those relevant features + build a linear regression model (i.e., both prediction and understanding, so it would be better to be left with relatively few parameters). The output is continuous. It's $20000$ genes per $50$ cases. I've been reading about the <code>glmnet</code> package, but I'm not 100% sure about the steps to follow:</p>

<ol>
<li><p>Perform CV to choose lambda:<br>
<code>cv &lt;- cv.glmnet(x,y,alpha=0.5)</code><br>
<strong>(Q1)</strong> given the input data, would you choose a different alpha value?<br>
<strong>(Q2)</strong> do I need to do something else before build the model?</p></li>
<li><p>Fit the model:<br>
<code>model=glmnet(x,y,type.gaussian=""covariance"",lambda=cv$lambda.min)</code><br>
<strong>(Q3)</strong> anything better than ""covariance""?<br>
<strong>(Q4)</strong> If lambda was chosen by CV, why does this step need <code>nlambda=</code>?<br>
<strong>(Q5)</strong> is it better to use <code>lambda.min</code> or <code>lambda.1se</code>?</p></li>
<li><p>Obtain the coefficients, to see which parameters have fallen out ("".""):<br>
<code>predict(model, type=""coefficients"")</code></p>

<p>In the help page there are many <code>predict</code> methods (e.g., <code>predict.fishnet</code>, <code>predict.glmnet</code>, <code>predict.lognet</code>, etc). But any ""plain"" predict as I saw on an example.<br>
<strong>(Q6)</strong> Should I use <code>predict</code> or <code>predict.glmnet</code> or other?</p></li>
</ol>

<p>Despite what I've read about regularization methods, I'm quite new in R and in these statistical packages, so it's difficult to be sure if I'm adapting my problem to the code. Any suggestions will be welcomed.</p>

<p><strong>UPDATE</strong><br>
<a href=""http://www.jstatsoft.org/v28/i05/paper"">Based on</a> ""As previously noted, an object of class train contains an element called <code>finalModel</code>, which is the fitted model with the tuning parameter values selected by resampling. This object can be used in the traditional way to generate predictions for new samples, using that model's
predict function.""  </p>

<p>Using <code>caret</code> to tune both alpha and lambda:    </p>

<pre><code>  trc = trainControl(method=cv, number=10)  
  fitM = train(x, y, trControl = trC, method=""glmnet"")  
</code></pre>

<p>Does <code>fitM</code> replace previous step 2? If so, how to specify the glmnet options (<code>type.gaussian=""naive"",lambda=cv$lambda.min/1se</code>) now?<br>
And the following <code>predict</code> step, can I replace <code>model</code> to <code>fitM</code>?</p>

<p>If I do  </p>

<pre><code>  trc = trainControl(method=cv, number=10)  
  fitM = train(x, y, trControl = trC, method=""glmnet"")  
  predict(fitM$finalModel, type=""coefficients"")
</code></pre>

<p>does it make sense at all or am I incorrectly mixing both package vocabulary?</p>
"
"0.0400641540107502","0.0390434404721515"," 71599","<p>Suppose that one has data on a proportion $P$ measured on a continuous scale.  Further suppose that this data has a grouped structure -- some proportions are clustered according to a certain variable $g$ (location, say).  Further say that you've got a covariate $X$ that your think is correlated with $P$.  You want to know the effect of $X$ on $P$ in order to get conditional distributions of $P$ given $X$.  </p>

<p>One could simply run a linear model with random effects, but predictions may go beyond the 0,1 bounds, and intervals will be wrong.  </p>

<p>One could fit a binomial(logit) GLM, but this may be a poor model for the data, as it would lead to overly upward (downward) skewed intervals near the boundaries, and a perhaps-artificial sigmoidal shape to the predictions.</p>

<p>Could a Beta regression work for this sort of problem?  If so, how would one incorporate the grouped structure of the data?  Any ideas for implementation in R?</p>
"
"0.0934830260250838","0.117130321416455"," 71948","<p>I'm fitting a natural spline fit to some data points. I'd like to estimate the prediction error for the predicted value. In linear regression (I agree that natural spline is also a linear regression with a specific type of design matrix), we know:</p>

<p>$\hat{\beta} = (X^T X)^{-1}X^TY \rightarrow \text{ assuming var(Y) = } \sigma^2 I \text{ then : }var(\hat{\beta}) = (X^TX)^{-1} \sigma^2 $ </p>

<p>Now consider $\hat{Y} = {X_i}^T \hat{\beta} + \epsilon_i$. We can then write:</p>

<p>$Var(\hat{Y}) = {X_i}^T ((X^TX)^{-1} \sigma^2) (X_i) + \sigma^2$</p>

<p>This is easy to calculate for linear regression. How should I do it with natural spline? I can get the design matrix for natural spline. I can get  $(X^TX)^{-1} \sigma^2$ but how can I get the rest of it:</p>

<p>Here is an example in R:</p>

<pre><code>set.seed(12345)
x &lt;- c(1:100)
y &lt;- sin(pi*x/50)
epsilon &lt;- rnorm(100, 0, 3)
knots &lt;- c(10, 20, 30, 40, 50, 60, 70, 80, 90)
myFit &lt;- lm(y ~ ns(x, knots = knots))
</code></pre>

<p>Now consider x = 32.5 . How can I get the variance for the $\hat{Y}$ corresponding to x = 32.5 ? I know we can use the predict function. however, what I do really want is to get calculate it similar to linear regression by getting the design matrix and multiplying them together.</p>

<p>I really appreciate your help.</p>
"
"0.139198742242089","0.146087177447694"," 73165","<p>I have a logistic regression model (fit via glmnet in R with elastic net regularization), and I would like to maximize the difference between true positives and false positives.  In order to do this, the following procedure came to mind:</p>

<ol>
<li>Fit standard logistic regression model</li>
<li>Using prediction threshold as 0.5, identify all positive predictions</li>
<li>Assign weight 1 for positively predicted observations, 0 for all others</li>
<li>Fit weighted logistic regression model</li>
</ol>

<p>What would be the flaws with this approach?  What would be the correct way to proceed with this problem?</p>

<p>The reason for wanting to maximize the difference between the number of true positives and false negatives is due to the design of my application.  As part of a class project, I am building a autonomous participant in an online marketplace - if my model predicts it can buy something and sell it later at a higher price, it places a bid.  I would like to stick to logistic regression and output binary outcomes (win, lose) based on fixed costs and unit price increments (I gain or lose the same amount on every transaction).  A false positive hurts me because it means that I buy something and am unable to sell it for a higher price.  However, a false negative doesn't hurt me (only in terms of opportunity cost) because it just means if I didn't buy, but if I had, I would have made money.  Similarly, a true positive benefits me because I buy and then sell for a higher price, but a true negative doesn't benefit me because I didn't take any action.</p>

<p>I agree that the 0.5 cut-off is completely arbitrary, and when I optimized the model from step 1 on the prediction threshold which yields the highest difference between true/false positives, it turns out to be closer to 0.4.  I think this is due to the skewed nature of my data - the ratio between negatives and positives is about 1:3.</p>

<p>Right now, I am following the following steps:</p>

<ol>
<li>Split data intto training/test</li>
<li>Fit model on training, make predictions in test set and compute difference between true/false positives</li>
<li>Fit model on full, make predictions in test set and compute difference between true/false positives</li>
</ol>

<p>The difference between true/false positives is smaller in step #3 than in step #2, despite the training set being a subset of the full set.  Since I don't care whether the model in #3 has more true negatives and less false negatives, is there anything I can do without altering the likelihood function itself?</p>
"
"0.0400641540107502","0.0390434404721515"," 76155","<p>I'm dealing with some data where there are some infrequently occuring categorical variables related to a binary prediction target. </p>

<p>For example marketing partners... some send 1000s of leads but many others send less than 10 a minuscule number in a data set with 20K+ examples.</p>

<p>Typically, we deal with this by grouping all the infrequently occurring partners under the variable ""Else"" and using this as the reference category in logistic regression.</p>

<p>I'm wondering what the pros and cons are of handling the issue like this and if there is a better way to deal with it in pre-processing phase.</p>
"
"0.0693931503088838","0.0676252226000574"," 76714","<p>I'm using the ANES dataset, which is a repeated cross-sectional study of public opinion and knowledge.  I am trying to predict knowledge by individual level characteristics.  The data is organized by year, and each respondent is grouped by age into an age cohort.  </p>

<p>Essentially, I am trying to run multiple multinomial regression models (the knowledge question has 3 levels: correct, incorrect, and don't know), but I want predictions specific to year the questions were asked and the cohort the respondent belongs to.  </p>

<p>Basically, for each year and each <code>cohort = multinom(knowledge ~ gender + education +...)</code></p>

<p>I have 8 years and 8 cohorts.  I tried to do a for loop within a for loop, but for whatever reason it is not working.  I tried:</p>

<pre><code>for(i in 1:length(year)){
   for(j in 1:length(cohort)){
      model &lt;- multinom(knowledge ~ gender + education + income)
  }
}
</code></pre>

<p>Can someone help me figure out what I'm doing wrong, please?  Right now, all this does is give me coefficients for the same year and cohort 100 times...</p>
"
"0.127220775566287","0.135250445200115"," 77546","<p>I am trying to fit a multivariate linear regression model with approximately 60 predictor variables and 30 observations, so I am using the <strong>glmnet</strong> package for regularized regression because p>n.</p>

<p>I have been going through documentation and other questions but I still can't interpret the results, here's a sample code (with 20 predictors and 10 observations to simplify):</p>

<p>I create a matrix x with num rows = num observations and num cols = num predictors and a vector y which represents the response variable</p>

<pre><code>&gt; x=matrix(rnorm(10*20),10,20)
&gt; y=rnorm(10)
</code></pre>

<p>I fit a glmnet model leaving alpha as default (= 1 for lasso penalty)</p>

<pre><code>&gt; fit1=glmnet(x,y)
&gt; print(fit1)
</code></pre>

<p>I understand I get different predictions with decreasing values of lambda (i.e. penalty)</p>

<pre><code>Call:  glmnet(x = x, y = y) 

        Df    %Dev   Lambda
  [1,]  0 0.00000 0.890700
  [2,]  1 0.06159 0.850200
  [3,]  1 0.11770 0.811500
  [4,]  1 0.16880 0.774600
   .
   .
   .
  [96,] 10 0.99740 0.010730
  [97,] 10 0.99760 0.010240
  [98,] 10 0.99780 0.009775
  [99,] 10 0.99800 0.009331
 [100,] 10 0.99820 0.008907
</code></pre>

<p>Now I predict my Beta values choosing, for example, the smallest lambda value given from <code>glmnet</code></p>

<pre><code>&gt; predict(fit1,type=""coef"", s = 0.008907)

21 x 1 sparse Matrix of class ""dgCMatrix""
                  1
(Intercept) -0.08872364
V1           0.23734885
V2          -0.35472137
V3          -0.08088463
V4           .         
V5           .         
V6           .         
V7           0.31127123
V8           .         
V9           .         
V10          .         
V11          0.10636867
V12          .         
V13         -0.20328200
V14         -0.77717745
V15          .         
V16         -0.25924281
V17          .         
V18          .         
V19         -0.57989929
V20         -0.22522859
</code></pre>

<p>If instead I choose lambda with </p>

<pre><code>cv &lt;- cv.glmnet(x,y)
model=glmnet(x,y,lambda=cv$lambda.min)
</code></pre>

<p>All of the variables would be (.).</p>

<p>Doubts and questions:</p>

<ol>
<li>I am not sure about how to choose lambda.</li>
<li>Should I use the non (.) variables to fit another model? In my case I would like to keep as much variables as possible.</li>
<li>How do I know the p-value, i.e. which variables significantly predict the response?</li>
</ol>

<p>I apologize for my poor statistical knowledge! And thank you for any help.</p>
"
"0.0981367343026181","0.0956365069595007"," 78809","<p>I haven't found any literature on the application of Random Forests to MNIST, CIFAR, STL-10, etc. so I thought I'd try them with the <em>permutation-invariant</em> MNIST myself.</p>

<p>In <strong>R</strong>, I tried:</p>

<pre><code>randomForest(train$x, factor(train$y), test$x, factor(test$y), ntree=500)
</code></pre>

<p>This ran for 2 hours and got a 2.8% test error.</p>

<p>I also tried <strong>scikit-learn</strong>, with</p>

<pre><code>RandomForestClassifier(n_estimators=2000,
                       max_features=""auto"", 
                       max_depth=None)
</code></pre>

<p>After 70 minutes, I got a 2.9% test error, but with n_estimators=200 instead, I got a 2.8% test error after just 7 minutes.</p>

<p>With <strong>OpenCV</strong>, I tried</p>

<pre><code>rf.train(images.reshape(-1, 28**2), 
         cv2.CV_ROW_SAMPLE, 
         labels.astype('int'))
</code></pre>

<p>This ran for 6.5 minutes, and using <code>rf</code> for prediction gave a test error of 15%. I don't know how many trees it trained, as their Python binding for Random Forests seems to ignore the <code>params</code> argument, at least in version 2.3.1. I also couldn't figure out how to make it clear to OpenCV that I want to solve a classification problem, rather than regression -- I have my doubts, because replacing <code>astype('int')</code> with <code>astype('float32')</code> gives the same result.</p>

<p>In <strong>neural networks</strong>, for the <em>permutation-invariant</em> MNIST benchmark, the state of the art is 0.8% test error, although training would probably take more than 2 hours on one CPU.</p>

<p>Is it possible to do much better than the 2.8% test error on MNIST using Random Forests? I thought that the general consensus was that Random Forests are usually at least as good as kernel SVMs, which I believe can get a 1.4% test error.</p>
"
"0.16544425129703","0.170186411426254"," 79216","<p><strong>Problem</strong>: When trying to calculate the variance of timeseries sums I get a negative variance, mostly due to autocovariances at large lag steps. Does not seem realistic.</p>

<p>I have a timeseries which is calculated from another timeseries using a regression equation.
I would like to propagate the uncertainty in the regression to the final timeseries. Then I want to sum (or take mean values) different segments of the timeseries over different timeperiods, and get the uncertainty of the sums. The timeseries is originally in 1 hour frequency and I want to sum over periods of 1 day (resampling to daily frequency) up to several years. The timeseries is strongly autocorrelated at short lag times.</p>

<p>For getting the variance of the sum (in the case of 3 elements being summed):
$$Var(a+b+c)= \\ Var(a)+Var(b)+Var(c) + 2 \times (Cov(a,b) + Cov(a,c)+Cov(b,c))$$</p>

<p>I use <code>r</code> for the calculations. I get the variances for each timeseries element as $SE^2$, where $SE$ is the standard error (<code>se.fit</code>) returned from r's <code>predict()</code> function using the regression model. The covariances I get from the autocovariance function <code>acf()</code>.</p>

<p>Here is some code and a selection of the data (excuse clumsy R code, I'm very new to R):</p>

<pre><code>#tsY is the predicted timeseries from the regression
tsY=c(81.4,  79.0,  83.4,   81.7,   75.7,   68.3,   62.3,   57.2,   52.6,   48.8,   45.4,   42.6,   39.9,   37.6,   35.6,   33.8,   32.2,   30.8,   29.6,   28.4,   27.3,   26.2,   25.0,   23.9)
#tsSE is the standard error from the prediction (se.fit)
tsSE=c(1.55,  1.49, 1.60,   1.56,   1.41,   1.23,   1.09,   0.97,   0.87,   0.78,   0.71,   0.65,   0.60,   0.55,   0.51,   0.48,   0.45,   0.42,   0.40,   0.38,   0.36,   0.34,   0.32,   0.30)

tsVar=tsSE^2

#create a matrix of the autocovariances at different lag times, diagonal is lag=0
#rows and columns are indicies in timeseries
covmat&lt;-matrix(numeric(0), length(tsY),length(tsY)) 
for ( i in (1:(length(tsY)) ) ) {
  if (i == 1) {
    autocov&lt;-acf(tsY, type='covariance', lag.max= length(tsY))
    autocovvec&lt;-autocov$acf[1:nrow(autocov$acf)]
    covmat[i:length(tsY),i]=autocovvec
  }  else {
    autocov&lt;-acf(tsY[-(1:i-1)], type='covariance', lag.max= length(tsY))
    autocovvec&lt;-autocov$acf[1:nrow(autocov$acf)]
    covmat[i:length(tsY),i]=autocovvec
  }

}

# sum the matrix columns, but not the diagonal
sumofColumns &lt;- rep(NA, ncol(covmat))
for (i in (1:ncol(covmat))) {
  if (i == 1) {
    sumofColumns[i]=sum(covmat[-(1),i])  
  } else{ 
    sumofColumns[i]=sum(covmat[-(1:i),i])  
  }
}

sumofCov=sum(sumofColumns) # sum of the covariance (Cov(a,b) + Cov(a,c)+...)
sumofVar=sum(tsVar) # sum of the variances of each timeseries element
varofSum=sumofVar+2*sumofCov # variance of the sum of the timeseries

# from the covmat the negative variance occurs at larger lag times.
acf(tsY, type='covariance', lag.max= length(tsY))

&gt; sumofCov
[1] -1151.529
&gt; varofSum
[1] -2283.246
</code></pre>

<p><strong>So I have the following questions:</strong></p>

<blockquote>
  <ol>
  <li><p>Did I completely misunderstand how to calculate variance of sums?</p></li>
  <li><p>Is it better to use a cutoff from the max lags to be considered in the autocovariance? If so how would one determine this? This would especially be important with the complete data where the length is several thousand. </p></li>
  </ol>
  
  <p><strike>3. Why is the covariance negative in this sample data at large? When plotting tsY  <code>plot(tsY)</code> it looks like the covariance/correlation should remain positive.</strike> Because it is the variation in direction from their means.</p>
</blockquote>

<p><strong>EDIT:</strong></p>

<blockquote>
  <p>Comment on <strong>question 2</strong> above:
  I have realized that using n-1 lags, as above in the code, does not make a lot of sense. There appear to be few different ways to determine the maximum lags to consider.  Box &amp; Jenkins (1970) suggest n/4 and R by default 10*log10(n). This does not answer the question however, of how to determine an appropriate cutoff for summing the covariances.</p>
  
  <p>Does it make sense to look at the partial autocorrelation (function pacf()), in order not to overestimate the effect of the auto covariance in the summation term? The partial autocorrelation for my data is significantly different from zero only at 1 or 2 lags. Similarly, fitting an AR model using ar() function, I also get an order of 1 or 2.</p>
</blockquote>

<p>Cheers</p>

<p>Related post <a href=""http://stats.stackexchange.com/questions/10943/variance-on-the-sum-of-predicted-values-from-a-mixed-effect-model-on-a-timeserie"">Variance on the sum of predicted values from a mixed effect model on a timeseries</a></p>
"
"0.0566592699670073","0.0552157630374233"," 82963","<p>A colleague of mine sent me this problem apparently making the rounds on the internet:</p>

<pre><code>If $3 = 18, 4 = 32, 5 = 50, 6 = 72, 7 = 98$, Then, $10 =$ ?
</code></pre>

<p>The answer seems to be 200.</p>

<pre><code>3*6  
4*8  
5*10  
6*12  
7*14  
8*16  
9*18  
10*20=200  
</code></pre>

<p>When I do a linear regression in R:</p>

<pre><code>data     &lt;- data.frame(a=c(3,4,5,6,7), b=c(18,32,50,72,98))  
lm1      &lt;- lm(b~a, data=data)  
new.data &lt;- data.frame(a=c(10,20,30))  
predict  &lt;- predict(lm1, newdata=new.data, interval='prediction')  
</code></pre>

<p>I get:   </p>

<pre><code>  fit      lwr      upr  
1 154 127.5518 180.4482  
2 354 287.0626 420.9374  
3 554 444.2602 663.7398  
</code></pre>

<p>So my linear model is predicting $10 = 154$.</p>

<p>When I plot the data it looks linear... but obviously I assumed something that is not correct.</p>

<p>I'm trying to learn how to best use linear models in R. What is the proper way to analyze this series? Where did I go wrong?</p>
"
"NaN","NaN"," 83401","<p>Is there a way to get the variance of prediction for a linear regression model in R? The variance that I need is $s_f^2=s^2\left(1+\frac{1}{n}+\frac{(x_{n+1}-\bar{x})^2}{\sum_{i=1}^n (x_i-\bar{x})^2}\right)$.</p>
"
"0.0566592699670073","0.0552157630374233"," 83433","<p>I would like to ask how the long-term (multiple step ahead) prediction intervals are calculated by function <code>predict.Arima</code> in R. I am particularly interested in ARIMA models, SARIMA models and in ARIMA models with external regressors (include argument xreg => regression with ARIMA errors) </p>
"
"0.0801283080215004","0.078086880944303"," 83576","<p>I have a feature x, that I use to predict a probability y.</p>

<hr>

<p><strong>Some background on (x,y)</strong></p>

<p>I can't go into too much details, but hopefully the following should be enough to explain what x and y are, at least conceptually <em>[square and circles are NOT the actual label I am working with]</em>:</p>

<p><strong>y</strong></p>

<p>y is the probability of an image being of Class 0 or 1, with: </p>

<ul>
<li>Class 0 means that the image contains a <em>square</em>.</li>
<li>Class 1 means that the image contains a <em>circle</em>.</li>
</ul>

<p>100 people watched the training images, and classified them.
y is the result probability, so y=0 means there is definitely a square, y=1 means there is definitely a round.</p>

<p><strong>x</strong></p>

<p>x is a feature derived from the images, by <em>trying to fit them to a model of a circle</em>, and calculating the error.
So for example when x is very low, the probability of the image having a circle is high (relatively).</p>

<hr>

<p>plot(x,y)</p>

<p><img src=""http://i.stack.imgur.com/05230.png"" alt=""enter image description here""></p>

<p>x,y (1000 values for each) pasted here:
<a href=""http://tny.cz/c320180d"" rel=""nofollow"">http://tny.cz/c320180d</a></p>

<p>Using mean(y) as a predictor, I get <strong>RMSE = 0.285204</strong>:</p>

<pre><code>N = length(x)
average = mean(y)
RMSE = sqrt( 1/N * sum( (average-y)^2 ) )
RMSE
[1] 0.285204
</code></pre>

<p>Then using a linear regression on log(x), I could improve a little bit the <strong>RMSE = 0.2694513</strong>:</p>

<pre><code>log_x = log(x)
plot(log_x,y)
lm.result = lm(formula = y ~ log_x)
abline(lm.result, col=""blue"") # not working very well
linear_prediction = predict( lm.result, new, se.fit = TRUE)
prediction_linear_regression = matrix(0,N,1)
prediction_linear_regression = linear_prediction$fit
RMSE_linear_regression = sqrt( 1/N * sum( (prediction_linear_regression-y)^2 ) )
RMSE_linear_regression
[1] 0.2694513
</code></pre>

<p><img src=""http://i.stack.imgur.com/59Etc.png"" alt=""enter image description here""></p>

<p>Can the RMSE be further improved? What should I try?</p>
"
"0","0.0390434404721515"," 83861","<p>Let's say I have the following data on leads, monthly media spend, and clicks</p>

<pre><code>Month  Leads   Media     Clicks
Jan     150    1000       500
Feb     200    1000       550 
March   300    1200       800
...
</code></pre>

<p>Let's say I run a linear regression where y is leads and the predictors are media and clicks. That's good, I know the relationships between these variable and can generate some lags to produce predictions. But what if I had spent 500 (or 2000 or 0) on media, what would have occurred. How do I perform this type of 'counter-factual' analysis where I attempt to find the results of a model if the actual value from one or two of the predictors was lower or higher? What is the standard approach (aka statistically proper approach)? Is it just a matter to ""adjusting"" the data to the 'new' number and rerunning the regression? or maybe simulating a regression 100+ times with 100+ different values for media?</p>
"
"0.0981367343026181","0.0956365069595007"," 84319","<p>I am working on an age estimation method using 4 types of biological measurements as age predictors. I am using RStudio. 
So far, I have good results when I use linear regression (<code>lm(age~predictor)</code>), but I am encountering heteroskedasticity, and therefore cannot build prediction intervals for my models.<br> 
I have tried transformations to normalize the predictors using ln, inverse, and square root, but to no avail.<br>
I have found a paper explaining the <code>wls</code> function, and I have used it in my models with the weight: $$\frac 1 {1+\frac{\text{predictor}^2} 2}$$ 
This has given me better age predictions, but does not solve the heteroskedasticity problem. </p>

<p>I have done some research, and apparently, one of my options is to create homoscedastic groups in my data by finding the data points where the residual variances change. 
For that, I have used the breakpoints function of strucchange, which gave me 5 breakpoints by default. 
I now want to give 6 different weights (weights are $\frac 1 {\text{var(age)}}$ of each interval) to my 6 intervals of data, but I cannot find a function to do that. I would greatly appreciate any help on the subject. 
Thanks.</p>
"
"0.0801283080215004","0.078086880944303"," 85719","<p>I was used R and mongodb for finding predictions of next date outcome for that I write R code as below</p>

<pre><code>library('RMongo')
mg1 &lt;- mongoDbConnect('demo','localhost',27017)
query &lt;- dbGetQuery(mg1,'hosts',""{'hostId' : '101.10.202.10'}"") 
date &lt;- query$runtimeMillis
    memory &lt;- query$memoryUtilization
plot(date,memory)
lm1 &lt;- lm(memory~date)
</code></pre>

<p>when I plot this graph it looks like below 
<img src=""http://i.stack.imgur.com/57OkC.jpg"" alt=""enter image description here""></p>

<p>Now I want to predict given date memory utilization so I add following code for finding prediction</p>

<pre><code>  new &lt;- data.frame(date=1377843220)
  prediction &lt;- predict(lm1, newdata=new, interval=""predict"")
</code></pre>

<p>but given prediction not match actual values I also tried interval=""confidence"" it also shows me wrong results. How should I find out prediction of given data is linear regression algorithm fitted here or any other algorithm helps?</p>

<p>Data excel file here <a href=""https://fs01n4.sendspace.com/dl/598e001c9c3a45696e48e8030d0f3a6d/52f49628461e5785/3mi7kr/memory_file.xls"" rel=""nofollow"">Download</a></p>
"
"NaN","NaN"," 86432","<p>I'm using 'betareg' package in R to perform beta regression. predict() function with se.fit=T is supposed to return standard errors along with the prediction but it doesn't. Is there any other way I can get the standard error outputs?</p>

<p>I'm open to using other packages that can perform beta regression too.</p>
"
"0.0801283080215004","0.0585651607082273"," 86624","<p>I would like to get the standard error on a prediction. Using R <code>glm</code>, I can get the SE of the fit for a specific prediction:</p>

<pre><code>mod &lt;- glm(y~wa_WSI, data=mydata, family=gaussian(link=""identity""))
predict.glm(mod,newdata=newdata, type=""response"", se.fit=T)
</code></pre>

<p>But when I compare the predictions with the actual values, this number seems way too small. I found a formula for ""standard error of the estimate"" which is $\sqrt{s/(n-p)}$ where $s$ is the sum of the squared residuals, $n$ is the number of data points, and $p$ is the number of terms in the regression. This gives me a much larger result, but is not for a single prediction.</p>

<p>My question is, is the SE formula above the formula I should use and is there some way to get it from the value R gives me for <code>se.fit</code> so that it is specific for a particular prediction?</p>
"
"0.0908569611434023","0.103299233817667"," 87963","<p>As the title to my question says, I am confused as to when the $R^2$ of a model fit does not equal the slope of the regression between observed and predicted values. </p>

<p>I am trying to present model prediction statistics in a similar way to those presented in the summary figures of the Globcolor validation report (<a href=""http://www.globcolour.info/validation/report/GlobCOLOUR_FVR_v1.1.pdf"">link</a>) - (e.g. figure from page 53 of the .pdf):</p>

<p><img src=""http://i.stack.imgur.com/ToRun.png"" alt=""enter image description here""></p>

<p>Here we see that they present the plot of observed versus predicted Chlorophyll concentrations, as well as statistics relating to its regression (e.g. the dashed line: $R^2$, $RMS$, $\alpha$ - intercept, and $\beta$ - slope). </p>

<p>My issue is that in my comparisons, I always get exactly the same value for the overall model fit $R^2$ and $\beta$-slope of the observed versus predicted regression.</p>

<p>Basic question: When (if ever) can these be different?</p>

<p>I have included a basic example of my problem in the following R script:</p>

<pre><code>set.seed(1)
n &lt;- 100
x &lt;- runif(n)
e &lt;- rnorm(n)
a &lt;- 3
b &lt;- 5
y &lt;- a + x*b + e

#fit model
fit &lt;- lm( y ~ x )

#plot regression
plot(x,y)
abline(fit)

#plot predicted versus observed
png(""plot.png"", units=""in"", width=5, height=5, res=400)
par(mar=c(5,5,1,1))
pred &lt;- predict(fit)
plot(y, pred, xlim=range(c(y,pred)), ylim=range(c(y,pred)), xlab=""observed"", ylab=""predicted"")
abline(0,1, lwd=2, col=8)

#add regression
fit2 &lt;- lm(pred ~ y)
lgd &lt;- c(
    paste(""R^2 ="", round(summary(fit2)$r.squared,3)),
    paste(""Offset ="", round(coef(fit2)[1],3)),
    paste(""Slope ="", round(coef(fit2)[2],3))
)
legend(""topleft"", legend=lgd)
abline(fit2, lwd=2)
legend(""bottomright"", legend=c(""predicted ~ observed"", ""1:1""), col=c(1,8), lty=1, lwd=2)

dev.off()

cor(pred, y)^2 # also the same
</code></pre>

<p><img src=""http://i.stack.imgur.com/xyJZm.png"" alt=""enter image description here""></p>
"
"NaN","NaN"," 88240","<p>I used R to estimate a regression with both numeric and categorical variables, and obtained coefficient estimates. </p>

<p>However, when I try to make predictions using new data, there appear to be some problems with dimensions. </p>

<p>Is there anything that must be done to the code so that R handles both types of variables? Is there a command other than <code>predict(...)</code> to combine my coefficients with the new data?</p>
"
"0.127220775566287","0.135250445200115"," 90906","<p>I'm new to R and logistic regression and have to admit that I don't really know how to interpret the result. I'm trying to compute a pretty simple model with 2 predictors (A and B). When I first try to compute models with the predictors one by one they are both significant. When I put them together and add an interaction term they lose their significance (but the interaction term is weakly significant). I interpret this as A and B are overlapping and no longer significant when the oter parameter is hold constant. Right?</p>

<p>But now to the part I don't know how to interpret. I make predictions from my models (see code below) and then run t-tests for the predictions vs. the depending variable. I think this should give a hint on how good the model is (is there a better way?). When I do it this way I get a much lower p-value for the model with both A and B. I think this is contradictory. The first part tells me that A doesn't provide any significant information to the model when combined with B, but on the other hand I get much better predictions. I guess something is really wrong, but I can't figure out what. Can you help me?</p>

<pre><code>model1=glm(f~A, , family=binomial(link=""logit""))
model2=glm(f~B,   family=binomial(link=""logit""))
model3=glm(f~A*B, family=binomial(link=""logit""))
summary(model1)
summary(model2)
summary(model3)
p1=predict(model1, newdata=data, type=""response"", na.rm=TRUE)
p2=predict(model2, newdata=data, type=""response"", na.rm=TRUE)
p3=predict(model3, newdata=data, type=""response"", na.rm=TRUE)
t.test(p1~f)
t.test(p2~f)
t.test(p3~f)
</code></pre>

<p>Part of the output:  </p>

<pre><code>&gt; summary(model1)
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.9756     0.3499  -5.647 1.64e-08 ***
A            -0.5898     0.2119  -2.784  0.00537 ** 

&gt; summary(model2)
              Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)  8.354e-01  1.309e+00   0.638   0.5234  
B           -1.028e-04  5.122e-05  -2.007   0.0447 *

&gt; summary(model3)
Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)  1.254e+00  1.705e+00   0.735    0.462  
A            1.589e+00  9.743e-01   1.631    0.103  
B           -1.324e-04  7.333e-05  -1.805    0.071 .
A:B         -9.418e-05  4.632e-05  -2.033    0.042 *

&gt; t.test(p1~f)
t = -2.614, df = 11.83, p-value = 0.02286

&gt; t.test(p2~f)
t = -1.8702, df = 15.679, p-value = 0.08024

&gt; t.test(p3~f)
t = -4.9777, df = 17.344, p-value = 0.0001084
</code></pre>
"
"NaN","NaN"," 91847","<p>Suppose I have a logistic regression model such like this:</p>

<pre><code>set.seed(123)
df&lt;-data.frame(
y=rbinom(100,1,0.5),
x1=rnorm(100,10,2),
x2=rbinom(100,20,0.6))

fit&lt;-glm(y~x1*x2,data=df,family=""binomial"")
coef(summary(fit))
               Estimate Std. Error    z value  Pr(&gt;|z|)
(Intercept)  5.08314564 6.43692399  0.7896855 0.4297115
x1          -0.66691041 0.64071095 -1.0408912 0.2979260
x2          -0.28338654 0.51254819 -0.5528974 0.5803337
x1:x2        0.04037126 0.05100223  0.7915588 0.4286180
</code></pre>

<p>Does somebody know how to get the prediction matrix in a format like this:</p>

<pre><code>    intercept x1       x2  x1:x2
1   1        10.506637 10  105.06637
2   1        9.942906  17  169.02941
3   1        9.914259  10  99.14259
4   1        12.737205 11  140.10925
</code></pre>
"
"0.0991537224422629","0.110431526074847"," 91903","<p>Is the probability calculated by a logistic regression model (the one that is logit transformed) the fit of cumulative distribution function of successes of original data (ordered by the X variable)?</p>

<p><strong>EDIT:</strong> In other words - how to plot the probability distribution of the original data that you get when you fit a logistic regression model?</p>

<p>The motivation for the question was Jeff Leak's example of regression on the Raven's score in a game and whether they won or not (from Coursera's Data Analysis course). Admittedly, the problem is artificial (see @FrankHarrell's comment below). Here is his data with a mix of his and my code:</p>

<pre><code>download.file(""http://dl.dropbox.com/u/7710864/data/ravensData.rda"", 
              destfile=""ravensData.rda"", method=""internal"")
load(""ravensData.rda"")

plot(ravenWinNum~ravenScore, data=ravensData)
</code></pre>

<p><img src=""http://i.stack.imgur.com/Cr5ka.png"" alt=""enter image description here"">  </p>

<p>It doesn't seem like good material for logistic regression, but let's try anyway:</p>

<pre><code>logRegRavens &lt;- glm(ravenWinNum ~ ravenScore, data=ravensData, family=binomial)
summary(logRegRavens)
# the beta is not significant

# sort table by ravenScore (X)
rav2 = ravensData[order(ravensData$ravenScore), ]

# plot CDF
plot(sort(ravensData$ravenScore), cumsum(rav2$ravenWinNum)/sum(rav2$ravenWinNum), 
         pch=19, col=""blue"", xlab=""Score"", ylab=""Prob Ravens Win"", ylim=c(0,1), 
         xlim=c(-10,50))
# overplot fitted values (Jeff's)
points(ravensData$ravenScore, logRegRavens$fitted, pch=19, col=""red"")
# overplot regression curve
curve(1/(1+exp(-(logRegRavens$coef[1]+logRegRavens$coef[2]*x))), -10, 50, add=T)
</code></pre>

<p>If I understand logistic regression correctly, R does a pretty bad job at finding the right coefficients in this case.  </p>

<p><img src=""http://i.stack.imgur.com/Cb6o8.png"" alt=""enter image description here""></p>

<ul>
<li>blue = original data to be fitted, I believe (CDF)  </li>
<li>red = prediction from the model (fitted data = projection of original data onto regression curve)</li>
</ul>

<p><strong>SOLVED</strong><br>
 - lowess seems to be a good non-parametric estimator of the original data = what is being fitted (thanks @gung). Seeing it allows us to choose the right model, which in this case would be adding squared term to the previous model (@gung)<br>
 - Of course, the problem is pretty artificial and modelling it rather pointless in general (@FrankHarrell)<br>
 - in regular logistic regression it's not CDF, but point probabilities - first pointed out by @FrankHarrell; also my embarrassing inability to calculate CDF pointed out by @gung.</p>
"
"0.187917539376016","0.183129968508307"," 92892","<p>I'm working on a meta-analysis of prevalence data. The aim is to get estimates of prevalence at the country level. The main issue is that the disease is highly correlated with age, and the sample ages of included studies are highly heterogeneous. Only median age is available for most studies, so I can't use SMR-like tricks. I figured I could use meta-regression to solve this, including age as a fixed-effect and introducing study-level and country-level random-effects.</p>

<p>The idea (that I took from <a href=""http://www.thelancet.com/journals/lancet/article/PIIS0140-6736%2813%2961249-0/abstract"" rel=""nofollow"">Fowkes et al</a>) was to use this model to make country-specific predictions of prevalence for each 5-year age group from 15 to 60 (using the median age of the group), and to apply these predictions to the actual population size of each of those groups in the selected country, in order to obtain total infected population and to calculate age-adjusted prevalence in the 15-60 population from that.</p>

<p>I tried several ways to do this using R with packages <code>meta</code> and <code>mgcv</code>. I got some satisfying results, but I'm not that confident with my results and would appreciate some feedback.</p>

<p>First is some simulated data, then the description of my different approaches:</p>

<pre><code>data&lt;-data.frame(id_study=c(""UK1"",""UK2"",""UK3"",""FRA1"",""FRA2"",""BEL1"",""GER1"",""GER2"",""GER3""),
                 country=c(""UK"",""UK"",""UK"",""FRANCE"",""FRANCE"",""BELGIUM"",""GERMANY"",""GERMANY"",""GERMANY""),
                 n_events=c(91,49,18,10,50,6,9,10,22),
                 n_total=c(3041,580,252,480,887,256,400,206,300),
                 study_median_age=c(25,50,58,30,42,26,27,28,36))
</code></pre>

<p><strong>Standard random-effect meta-analysis</strong> with package <code>meta</code>.</p>

<p>I used <code>metaprop()</code> to get a first estimate of the prevalence in each country without taking age into account, and to obtain weights. As expected, heterogeneity was very high, so I used weights from the random-effects model.</p>

<pre><code> meta &lt;- metaprop(event=n_events,n=n_total,byvar=country,sm=""PLOGIT"",method.tau=""REML"",data=data)
 summary(meta)
 data$weight&lt;-meta$w.random
</code></pre>

<p>I used meta to get a first estimate of the prevalence without taking age into account, and to obtain weights. As expected, heterogeneity was very high, so I used weights from the random-effects model.</p>

<p><strong>Generalized additive model</strong> to include age with package <code>mgcv</code>.</p>

<p>The <code>gam()</code> model parameters (k and sp) were chosen using BIC and GCV number (not shown here).</p>

<pre><code> model &lt;- gam( cbind(n_events,n_total-n_events) ~ s(study_median_age,bs=""cr"",k=4,sp=2) + s(country,bs=""re""), weights=weight, data=data, family=""binomial""(link=logit), method=""REML"")
 plot(model,pages=1,residuals=T, all.terms=T, shade=T)
</code></pre>

<p>Predictions for each age group were obtained from this model as explained earlier. CI were obtained directly using <code>predict.gam()</code>, that uses the  Bayesian posterior covariance matrix of the parameters. For exemple considering UK:</p>

<pre><code> newdat&lt;-data.frame(country=""UK"",study_median_age=seq(17,57,5))
 link&lt;-predict(model,newdat,type=""link"",se.fit=T)$fit
 linkse&lt;-predict(model,newdat,type=""link"",se.fit=T)$se
 newdat$prev&lt;-model$family$linkinv(link)
 newdat$CIinf&lt;-model$family$linkinv(link-1.96*linkse)
 newdat$CIsup&lt;-model$family$linkinv(link+1.96*linkse)
 plot(newdat$prev~newdat$study_median_age, type=""l"",ylim=c(0,.12))
 lines(newdat$CIinf~newdat$study_median_age, lty=2)
 lines(newdat$CIsup~newdat$study_median_age, lty=2)
</code></pre>

<p>The results were satisfying, representing the augmentation of the prevalence with advanced age, with coherent confidence intervals. I obtained a total prevalence for the country using the country population structure (not shown, I hope it is clear enough).</p>

<p>However, I figured I needed to include study-level random-effects since there was a high heterogeneity (even though I did not calculate heterogeneity after the meta-regression).</p>

<p><strong>Introducing study-level random-effect</strong> with package <code>gamm4</code>.</p>

<p>Since <code>mgcv</code> models can't handle that much random-effect parameters, I had to switch to <code>gamm4</code>.</p>

<pre><code> model2 &lt;- gamm4(cbind(n_events,n_total-n_events) ~ s(study_median_age,bs=""cr"",k=4) + s(country,bs=""re""), random=~(1|id_study), data=data, weights=weight, family=""binomial""(link=logit))
 plot(model2$gam,pages=1,residuals=T, all.terms=T, shade=T)

 link&lt;-predict(model2$gam,newdat,type=""link"",se.fit=T)$fit
 linkse&lt;-predict(model2$gam,newdat,type=""link"",se.fit=T)$se
 newdat$prev2&lt;-model$family$linkinv(link)
 newdat$CIinf2&lt;-model$family$linkinv(link-1.96*linkse)
 newdat$CIsup2&lt;-model$family$linkinv(link+1.96*linkse)
 plot(newdat$prev2~newdat$study_median_age, type=""l"",col=""red"",ylim=c(0,0.11))
 lines(newdat$CIinf2~newdat$study_median_age, lty=2,col=""red"")
 lines(newdat$CIsup2~newdat$study_median_age, lty=2,col=""red"")
 lines(newdat$prev~newdat$study_median_age, type=""l"",ylim=c(0,.12))
 lines(newdat$CIinf~newdat$study_median_age, lty=2)
 lines(newdat$CIsup~newdat$study_median_age, lty=2)
</code></pre>

<p>Since the study-level random effect was in the mer part of the fit, I didn't have to handle it. </p>

<p>As you can see, I obtain rather different results, with a much smoother relation between age and prevalence, and quite different confidence intervals. It is even more different in the full-data analysis, where the CI are much wider in the model including study-level RE, to the point it is sometimes almost uninformative (prevalence between 0 and 15%, but if it is the way it is...). Moreover, the study-level RE model seems to be more stable when outliers are excluded.</p>

<p><strong>So, my questions are:</strong></p>

<ul>
<li>Did I properly extract the weights from the metaprop() function and used them further?</li>
<li>Did I properly built my <code>gam()</code> and <code>gamm4()</code> models? I read a lot about this, but I'm not used to this king of models.</li>
<li>Which of these models should I use?</li>
</ul>

<p>I would really appreciate some help, since neither my teachers nor my colleagues could. It was a really harsh to conduct the systematic review, and very frustrating to struggle with the analysis... Thank you in advance!</p>
"
"0.160534598239854","0.16564728911227"," 95378","<p>I am doing statistics for the first time in my life and I am not quite sure what to include and how to interpret the results. I am doing a logistic regression in R. Here is what I have so far:</p>

<ol>
<li><p><code>GLM</code> with family = binomial (dependent ~ indep1 + indep2 + ...+ indep7  +0)
If I dont include the 0 I get NA for my last independent variable in the summary output..</p></li>
<li><p><code>Update</code> the model (indep2 has a p-value > 0.05 and is left out)</p></li>
<li><p>I am applying anova</p>

<pre><code>anova(original_model,updated_model, test=""Chisq"")

   Resid.Df  Resid.Dev Df Deviance Pr(&gt;Chi)
1     34067      18078                     
2     34066      18075  1   2.4137   0.1203
</code></pre>

<p>Here I am not sure how to interpret it. What tells me if the simplification of the model is significant? the p-value is with 0.12 bigger than 0.05, does this mean that the simplification is not significant? </p></li>
<li><p>make a cross-table (compare predicted (probability >0.5) - observed)</p>

<pre><code>fit
      FALSE  TRUE
  No  30572    68
  yes  3407    31
</code></pre>

<p>I'd say that 31 values are predicted correctly (yes-true), resp 68 (no-true) but that most values are classified wrong, which means that the model is really bad?</p></li>
<li><p>then I make a wald test for each independent variable for the first independent variable it would look like this:</p>

<pre><code>&gt; wald.test(b = coef(model_updated), Sigma = vcov(model_updated), Terms
&gt; = 1:1)
</code></pre>

<p>here I only look if the p-values are significant and if they are it means that all variables contribute significantly to the predictive ability of the model</p></li>
<li><p>I calculate the odds with their confidence intervals (this is basically exp(estimate)</p>

<pre><code>oddsCI &lt;- exp(cbind(OR = coef(model_updated), confint(model_updated)))
</code></pre>

<p>For all odds smaller than 1 i do 1/odd</p>

<pre><code>Estimate        Odds Ratio      Inverse Odds
-0.000203       0.999801041     1.000198999
 0.000332       1.000326571     odd bigger than 1
-0.000133       0.999846418     1.000153605
-3.48       0.008696665     114.9866056
-4.85       0.029747223     33.61658319
-2.37       0.000438382     2281.113996
-8.16       0.110348634     9.062187402
-2.93       0.062668509     15.95697759
-3.65       0.020156889     49.61083057
-5.45       0.033996464     29.41482359
-4.02       0.004837987     206.6975334
</code></pre>

<p>This O would interpret like that for the ""odd bigger than 1""  the case is over 1 times more likely to occur. (Is is incorrect to say that, or not?) Or for the last row you could say that t for every subtraction of a unit, the odds for the case to appear decreases by a factor of 206.</p></li>
<li><p>Then I look at </p>

<pre><code>with(model_updated, null.deviance - deviance) #deviance
with(model_updated, df.null - df.residsual) #degrees of freedom
 # pvalue
with(Amodel_updated, pchisq(null.deviance - deviance, df.null - df.residual, 
lower.tail = FALSE))
logLik(model_updated)
</code></pre>

<p>But I don't really know what this tells me.</p></li>
<li><p>In a last step I do</p>

<pre><code>stepAIC(model_updated, direction=""both"")
</code></pre>

<p>but also here I don't know how to interpret the outcome. I see that it looks at all interactions between my independent variables but I don't know what it tells me.</p></li>
</ol>

<p>After this, I can make a prediction by using the updated model and by separating it into training data and validation data I suppose?</p>
"
"0.105999788000636","0.103299233817667"," 95383","<p>I have a group of binary tasks performed by multiple subjects. Every task can be either performed right or wrong (i.e.,1/0). My goal is to predict the accuracy of future task given the performance on previous tasks. As tasks considered to be independent, I thought that it makes sense to average the accuracy by making predictions for every task given the rest. </p>

<p>Could you please advise me what is a suitable regression or any predictive model for this kind of data</p>

<p>Additionally, I am interested in cross validation between variables: Lets say I have 10 variables. I'm interested in averaged accuracy of prediction when first 9 variables are predictors and the last one is the response variable, then variables 1-8,10 are predictors and 9th is the response variable, and so on such that every variable calculated once as a response variable.</p>

<p>Thank you for your help</p>
"
"0.0693931503088838","0.0676252226000574"," 95494","<p>I am doing a regression analysis with multiple variables and comparing it to a one-variable null hypothesis. The goal is to see which model provides a better explanation. The topic is information diffusion, hence the names DIFfusion, INDustry, etc. Also I must say that DIF is many times zero (has a long tail) and all variables go between 0 and 1. Anyways, the hypothesis can be translated unto:</p>

<p>H_0: DIF ~ REL</p>

<p>H_1: DIF ~ REL+COM*REL+IND*REL</p>

<p>H_2: DIF ~ REL+COM*REL+IND*REL+SIZE1+SIZE2</p>

<p>REL multiplies the other two variables as theoretically they are related and together they 'should' give a better prediction of DIF.</p>

<p>Now, the tricky things for me are two.</p>

<p>First, (and most importantly), although R values are over 0.6 -good enough for this topic- when I see the residuals they tend to follow a pattern that is similar for all three hypotheses. I really don't know why or how to address it.</p>

<p><img src=""http://i.stack.imgur.com/XRh2q.png"" alt=""Residuals""></p>

<p><img src=""http://i.stack.imgur.com/JXeCI.png"" alt=""QQplot""></p>

<p><img src=""http://i.stack.imgur.com/ZCvJe.png"" alt=""histogram""></p>

<p>Second, I have the intuition that the dependent variable (DIF) behaves like an iceberg and sea-level. Meaning that the lower the sea level (represented by SIZE1-2), I could see more of the shape of the iceberg, where the shape of the iceberg is given by the other variables. Have you encountered situations like this? How would you model/test it?</p>

<p>Any advice?</p>

<p>Using R by the way.</p>
"
"0.0566592699670073","0.0552157630374233"," 95832","<p>I have two data sets </p>

<ol>
<li>Train data  </li>
<li>Test data (with no dependent variable values but I
have data on independent variable or you can say I need to
forecast).</li>
</ol>

<p>Using the training data (which has some <code>NA</code>s in the cell) I performed ordinary least square regression (OLS) using <code>lm()</code> in R and fitted the model &amp; I got the $\beta $ coefficients of the regression model. (All is good so far!)</p>

<p>Now, in the process of prediction for the fitted values, I have some missing values for some cells in the test dataset. I used function <code>predict()</code> as follows: </p>

<pre><code> predict(ols, test_data.df, interval= ""prediction"", na.action=na.pass)
</code></pre>

<p>for the cell (or cells) with <code>NA</code> value the entire row is discarded in generating the output (<code>yhat</code>). Is there any function that could generate the <code>yhat</code> values (other than <code>NA</code>s) for the test data without discarding any rows with missing value in the cell. </p>
"
"0.126693979201741","0.123466199581199"," 95974","<p>This is a follow-up question from this post, here:
<a href=""http://stackoverflow.com/questions/14423325/confidence-intervals-for-predictions-from-logistic-regression"">Confidence intervals for predictions from logistic regression</a></p>

<p>The answer from @Gavin is excellent, but I have some additional questions which I think would be useful for others. I am working with a Poisson model, so basically it is the same approach described in the other post, only <code>family=poisson</code> instead of <code>family=binomial</code>.</p>

<p>To my first question:
@Gavin writes:</p>

<pre><code>mod &lt;- glm(y ~ x, data = foo, family = binomial)
preddat &lt;- with(foo, data.frame(x = seq(min(x), max(x), length = 100))
preds &lt;- predict(mod, newdata = preddata, type = ""link"", se.fit = TRUE)
</code></pre>

<p>What is the point of the second line there? Is it necessary to create a data.frame with minimum and maximum of the explanatory variable? Could I not, for some explanatory variable(s) <code>x</code> (stored in some data frame <code>data</code>), just go from the first line and directly to the third?</p>

<p>To my second question:
In the beginning of his answer @Gavin writes:</p>

<blockquote>
  <p>The usual way is to compute a confidence interval on the scale of the
  linear predictor, where things will be more normal (Gaussian) and then
  apply the inverse of the link function to map the confidence interval
  from the linear predictor scale to the response scale.</p>
</blockquote>

<p>Why are ""things"" more normal on the scale of the linear predictor(s)? Is this also the case when I do my Poisson regression?
I assume the reason for using critical value 1.96 when constructing the CI's, is because of the assumptions that ""things"" are normal. Can somebody explain this further?</p>

<p>My third question:</p>

<p>Is there a relationship between the standard deviation which we get by using <code>se.fit=TRUE</code>  in predict() and the standard deviations of the coefficients of the explanatory variables, which we simply get from <code>summary(mod)</code>? (<code>mod</code> is some glm object)</p>
"
"0.0801283080215004","0.078086880944303"," 97347","<p>How can I improve the accuracy of my logistic regression code, which tests the accuracy using the 10-fold cross-validation technique? I have implemented this code using <code>glmfit</code> and <code>glmval</code>. The desired accuracy is somewhat higher and it requires the parameters to be found using maximum likelihood estimator. Also, when I run this code in MATLAB, I get the following error</p>

<blockquote>
  <p>Warning: X is ill conditioned, or the model is overparameterized, and some coefficients are not identifiable. You should use caution in making predictions. In glmfit at 245 In LR at 8</p>
</blockquote>

<p>The code is:</p>

<pre><code>function LR( X,y)
y(y==-1)=0;
X=[ones(size(X,1),1) X];
disp(size(X,2));
indices = crossvalind('Kfold',y,10);
for i = 1:10
    test = (indices == i); train = ~test;
    b = glmfit(X(train,:),y(train),'binomial','logit');
    y_hat= glmval(b,X(test,:),'logit');
    y_true=y(test,:);
    error(i)=mean(abs(y_true-y_hat));
end
accuracy=(1-error)*100;
fprintf('accuracy= %f +- %f\n',mean(accuracy),std(accuracy));
end
</code></pre>
"
"0.0981367343026181","0.0956365069595007"," 97811","<p>I am using leave-one-out cross-validation to evaluate a linear regression model. In subsequent analysis, I need three specific values for each observation: observed value, predicted value, prediction standard error. Prediction standard error values can be retrieved from function <code>predict.lm</code> setting argument <code>se.fit = TRUE</code>. The following code (adapted from <a href=""http://www.analyticbridge.com/profiles/blogs/cross-validation-in-r-a-do-it-yourself-and-a-black-box-approach"" rel=""nofollow"">here</a>) can be used to do what I currently need:</p>

<pre><code>library(faraway)
gala[1:3, ]
c1 &lt;- c(1:30)
gala2 &lt;- cbind(gala, c1)
gala2[1:3, ]
obs  &lt;- numeric(30)
pred &lt;- numeric(30)
se   &lt;- numeric(30)
for (i in 1:30) {
     model1  &lt;- lm(Species ~ Endemics + Area + Elevation,
                   subset = (c1 != i), data = gala2)
     specpr  &lt;- predict(model1, gala2[i, ], se.fit = TRUE)
     obs[i]  &lt;- gala2[i, 1]
     pred[i] &lt;- specpr$fit
     se[i]   &lt;- specpr$se.fit
}
res &lt;- data.frame(obs, pred, se)
head(res)
  obs       pred       se
1  58  70.185063 5.524249
2  31  72.942732 6.509655
3   3  -8.303608 7.055163
4  25  20.948932 6.998093
5   2 -15.953141 7.403062
6  18  27.274440 6.220029
</code></pre>

<p>I searched through the documentation of some of the packages that offer functions for cross-validation, but did not find any that saves prediction standard errors. Is there any package that already offers such functionality?</p>
"
"0.113318539934015","0.110431526074847"," 99862","<p>Here's my situation.  </p>

<p>I have a multiple linear regression which I've used to come up with a prediction interval to predict a value y for a given (x1,x2,x3,x4,x5,x6).   It reads something like lower: 30, upper:48.  </p>

<p>I also have the same exact thing to predict a value y* at another given (x1*,x2*,x3*,x4*,x5*,x6*).  It reads something like lower:35, upper:51. </p>

<p>I want to answer this question:<br>
What is the probability that the value y* is greater than the value y?</p>

<p>I think it's a basic question, but I'm not sure. 
I could likely come up with this probability if I knew the formula for how the prediction interval is calculated in a multi-variable situation.<br>
Here's what I think should be done, but I wanted to run it by you guys first. </p>

<p>Prediction Intervals are based on a t-distribution with (n-6) degrees of freedom (I have a forced 0 y-int).  So I believe the margin of error calculated is then some constant multiplied by the corresponding value from the t-distribution (t_.05/2 with n-6 degrees of freedom).  The ""some constant"" would be the standard error of this particular estimate. </p>

<p>I then just do a basic 2 sample t-test using the point estimate prediction as the means and these constants as the standard errors with my n-6 degrees of freedom.   Is this accurate? </p>

<p>Is there a better way?</p>

<p>Thanks</p>
"
"0.169977809901022","0.16564728911227","100101","<p>I am currently working at work on a project that attempts to predict an environmental change variable. I am personally not a huge fan of the project, but I still want to do the best job possible. Anyhow, let me first describe the properties of the data, and then state my question. </p>

<p>The environmental variable we are trying to model is continuous, and ranges from 0 to 25 (it can have values such as 0.1345 or 1.2335 or 5.674). The environmental variable also has a significant mass at zero (around 30% of the data are zero values), and most of the data is in the range between > 0 and &lt; 1. To complicate things further, around 3% of the data have extreme values of greater than 2. In my opinion, the distribution of the data resembles a Tweedie distribution. </p>

<p>We have around 5 million observations in the dataset. We are going to predict the environmental variable using a set of eight explanatory variables.</p>

<p>We have been modeling the prediction of the environmental for a week, and the predictive power of our results has been meager. Here are the different modeling approaches I have used:</p>

<ol>
<li><p>GLM model with a tweedie distribution (glm function in r). This model approach overestimate the environmental change when the variables has low values. Most values between 0 - 1 are significantly overestimated, and the model does not predict high values very well either. The GLM approach produces a very bad fit for our data.</p></li>
<li><p>Generalized Additive Model with a tweedie distribution (bam function from mgcv package in r). This approaches fits the data slightly better than the model above, but still significantly overestimates values in the low range.</p></li>
<li><p>Regression tree model (rpart model in r). The regression tree model most accurately predicts values in the lower range of the distribution, but fails to predict zero values, and performs also poorly for values greater than 2. </p></li>
<li><p>Boosted regression model (dismo package in r using gbm.step). The model significantly overestimate values in the lower range of the distribution. For example, if values are 0.23 it predicts values to be 1.67 and so forth. I believe the gbm.step model to be inadequate for our data, since the family of distribution in the package only models the bernoulli (=binomial), poisson, laplace or gaussian family. None of which accurately describe our distribution.</p></li>
</ol>

<p>Since our dataset is very large, I split the data 50/50 into a test and training dataset, and evaluated model fit on a variety of test statistics for the test data set. </p>

<p>Knowing the structure of our data, can anyone think of our modeling approaches that would possibly produce better predictive results? I am still new to machine learning techniques, and I hope that someone might know of other techniques that could be employed.   </p>

<p>My strongest program language is R, but I can also do this analysis in Python or Stata. </p>
"
"0.204287903096402","0.191426216197403","101077","<p>I have very big data and low number of observations. So I decided to use PCA to reduce dimension of the data. The following is R example (just an dummy example - for workout):</p>

<pre><code>xmat &lt;- matrix(sample(-1:1, 100000, replace = TRUE), ncol = 1000)
colnames(xmat) &lt;- paste (""V"", 1:1000, sep ="""")
rownames(xmat) &lt;- paste(""S"", 1:100, sep = """")
</code></pre>

<p>In this example dataset I have <code>1000</code> variables and <code>100</code> observations / subjects. </p>

<p>I am doing PCA. Lets say.</p>

<pre><code>out &lt;- princomp(xmat)
Error in princomp.default(xmat) : 
  'princomp' can only be used with more units than variables
</code></pre>

<p>Q1: is there a way to reduce dimensionality with <code>p &gt; n</code> ? I would like to use all variables information as opposed to representative ones. Without having proper solution I went anyway to use cluster analysis of variables to categorize the variables and pick the randomly from the clusters. </p>

<p>To create a list of representative variables I tried to cluster the variables.</p>

<pre><code># cluster variables 
d &lt;- dist(t(xmat), method = ""euclidean"") # distance matrix
fit &lt;- hclust(d, method=""ward"")
plot(fit)
groups = cutree(fit,40)
groupd &lt;- data.frame(var = names(groups), group = groups)
</code></pre>

<p>What I am thinking is randomly pick one variable from each group above and use this in PCA. Assume that I have the following y variable.</p>

<pre><code>set.seed(1234)
yvar.d &lt;- data.frame (subject = c(paste(""S"", 1:100, sep = """")), yvar = rnorm (100, 50,10))
</code></pre>

<p><strong>Here is my question</strong>: </p>

<ol>
<li>What could be statistical challenge of using cluster analysis ?</li>
<li><p>Can we use PCA scores in predictions of y. How ? Just multiple
regression or we can introduce something such as variance explained
by each components in the model ?</p>

<p><strong>Edits:</strong></p>

<p>Based on the discussions (see the comments below), I am using different function to do PC analysis.</p></li>
</ol>

<p>""The calculation is done by a singular value decomposition of the (centered and possibly scaled) data matrix, not by using eigen on the covariance matrix. This is generally the preferred method for numerical accuracy. The print method for these objects prints the results in a nice format and the plot method produces a scree plot."" - from function help. </p>

<pre><code>     out1 &lt;- prcomp(xmat)
      out1$x[1:3,1:3]
                      PC1        PC2       PC3
S1  2.940862 -2.7379835  6.527103
S2 -1.081124 -0.5294796 -0.276591
S3  2.375710  0.4505205 -4.236289

   out1$sdev
 screeplot(out1,npcs=30, type=""lines"",col=3) # 30 PCA plotted
</code></pre>

<p><img src=""http://i.stack.imgur.com/gMJys.jpg"" alt=""enter image description here""></p>

<pre><code> out1$rotation
</code></pre>

<p>I also come to see an example in SO <a href=""http://stackoverflow.com/questions/10876040/principal-component-analysis-in-r"">how to use PCA in prediction</a>. Here is my workout: </p>

<pre><code>## take our training and test sets
YY &lt;-  yvar.d$yvar 
prop &lt;- 0.5
train = sample(1:length(YY), round(length(YY)*prop,0))


# data for testing model purpose 
testid = setdiff (1:length(YY), train)
YY1 &lt;- YY
newXPCA &lt;- data.frame(out1$x)
test.data &lt;- data.frame (y = YY1[testid],newXPCA[testid,]) 
test.data[1:10,1:10]

train.data &lt;- data.frame(y= YY1[train],newXPCA [train,])
train.data[1:10,1:10]

## fit the PCA
pc &lt;- prcomp(train.data[, -1])
trainwPC &lt;- data.frame (y = train.data$y, pc$x)

model1 &lt;- lm(y ~ ., data = trainwPC)

#predict() method for class ""prcomp""
test.p &lt;- predict(pc, newdata = test.data)
pred &lt;- predict(model1, newdata = data.frame(test.p), type = ""response"")
pred 
Warning message:
In predict.lm(model1, newdata = data.frame(test.p), type = ""response"") :
  prediction from a rank-deficient fit may be misleading
</code></pre>

<p>I just adopted this script from the SO link, I am not sure about accuracy of the script. </p>

<p>I still have technical questions remaining such as clarification to <strong>remaining question 2</strong> above: </p>

<p>(1) If I want to split data into training and test set by sampling <code>50% of data</code> (as show in the script). Should I do just multiple regression with y and the <code>out1$x</code> ? how many components to use ? is variance of each component play role in good model selection such as avoid over-fitting ? How ? </p>

<p>(2) Clustering (using x clusters) vs PCA analysis (with subset of x components vs all ) what would be statistically favorite for predictions in the situations where have <code>p &gt; n</code> ? As I said to my mind the PCA analysis can use all information but I do not know if there is downside of such information such as <code>over-fitting</code> and ""error consumption"". </p>

<p>Worked example appreciated.   </p>
"
"0.0693931503088838","0.0676252226000574","102667","<p>I've created a regression model on my data using random forests in R. The output is quite large, I'm wondering if there's any way to reduce this to only the necessary pieces to make a prediction?</p>

<p>The training data set contains 20 variables and ~45,000 rows, which is also large. My code is listed below.</p>

<pre><code>data &lt;- readRDS(""data.Rds"")

require(""data.table"")
require(""doParallel"")
require(""randomForest"")

train &lt;- data[ which(set == ""train"")]
test &lt;- data[ which(set == ""test"")]
rm(data)

x &lt;- data.table(train[, 2:21, with=FALSE])
y &lt;- as.vector(as.matrix(train[, 23, with=FALSE]))

cl &lt;- makeCluster(detectCores())
registerDoParallel(cl, cores=4)
time &lt;- system.time({rf.fit &lt;- foreach(ntree=rep(500, 6),
                               .combine=combine,
                               .multicombine=TRUE,
                               .packages=""randomForest"") %dopar% 
                   {randomForest(x, y, ntree=ntree)}})
stopCluster(cl)

saveRDS(rf.fit, ""rf.fit.Rds"")
</code></pre>

<p>The output of this is ~230 MB. Once I have the model, is it possible to reduce the size to make it easier to work with? My goals with this are to identify the important variables, and make a prediction on new data. </p>
"
"0.0895861718290583","0.0873037869711973","102884","<p>I've just began looking into survival analyses, and I'm having some difficulty interpreting the results. </p>

<p>I have a model linear regression model where time ~ x1 * x2 * x3... x9, this model seems to fit well (adjusted R${^2}$ of ~0.9) however it was suggested that survival analyses may be a better direction.</p>

<p>I have tried using the R tool for this, but I can't interpret the data. I do have some trails where they timed out (after 1 hour) so I set up my censor column for that. I then tried plotting the full data set (~200 trials), and got a nice downward stepped curve. <img src=""http://i.stack.imgur.com/JVEiV.png"" alt=""Full data set"">, however this doesnt seem to tell me anything, the model used was just survfit(my.surv ~ 1). If I replace the ~ 1 with one of the variables, for example survfit(mysurv ~ x1), I get a total mess. Lines all over the place (mostly vertical).</p>

<p>So, my questions are these:</p>

<p>1) Is it possible to gain useful information, other than saying that 80% of the trials finished before 2000 seconds, which is what I think this plot shows</p>

<p>2) Is it possible to include factors in the model, for example time ~ x1 + x2, etc. If not how does one ascertain which variables are important by this method. </p>

<p>3) How can I use this to make predictions about new data, not included in this model?</p>
"
"0.106837744028667","0.117130321416455","102892","<p>My situation:</p>

<ul>
<li>small sample size: 116</li>
<li>binary outcome variable</li>
<li>long list of explanatory variables: 44</li>
<li>explanatory variables did not come from the top of my head; their choice was based on the literature.</li>
</ul>

<p>Statistical test chosen: logistic regression</p>

<p>I need to find the variables that best explain variations in the outcome variable (I am not interested in making predictions).</p>

<p>The problem: This question is a follow-up on the 2 questions listed below. From them, I got that performing automated stepwise regression has its downsides. Anyway, it seems that my sample size would be too small for that. It seems that my sample is also too small to enter all variables at once (using the SPSS 'Enter' method). This leaves me with my issue unresolved: how can I select a subset of variables from my original long list in order to perform multivariate logistic regression analysis?</p>

<p>UPDATE1: I am not an statistician, so I would appreciate if jargons can be reduced to the minimum. I am working with SPSS and am not familiar with other packages, so options that could be run with that software would be highly preferable.</p>

<p>UPDATE2: It seems that SPSS does not support LASSO for logistic regression. So following one of your suggestions, I am now struggling with R. I have passed through the basics, and managed to run a univariate logistic regression routine successfully using the glm code. But as I tried glmnet with the same dataset, I am receiving an error message. How could I fix it? Below is the code I used, followed by the error message:</p>

<pre><code>data1 &lt;- read.table(""C:\\\data1.csv"",header=TRUE,sep="";"",na.string=99:9999)

y &lt;- data1[,1]

x &lt;- data1[,2:45]

glmnet(x,y,family=""binomial"",alpha=1)  

**in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs,  : 
(list) object cannot be coerced to type 'double'**
</code></pre>

<p>UPDATE3: I got another error message, now related to missing values. My question concerning that matter is <a href=""http://stats.stackexchange.com/questions/104194/how-to-handle-with-missing-values-in-order-to-prepare-data-for-feature-selection"">here</a>. </p>

<ul>
<li><a href=""http://stats.stackexchange.com/questions/88482/can-univariate-linear-regression-be-used-to-identify-useful-variables-for-a-subs"">Can univariate linear regression be used to identify useful variables for a subsequent multiple logistic regression?</a></li>
<li><a href=""http://stats.stackexchange.com/questions/20836/algorithms-for-automatic-model-selection/20856#20856"">Algorithms for automatic model selection</a></li>
</ul>
"
"0.0801283080215004","0.117130321416455","102973","<p>I am using logistic regression to benchmark the performance of some students in different years. I created a scenario as below:</p>

<pre><code>mydata &lt;- read.csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
benchmark.data &lt;- mydata[1:300,] # students form year 1990-1995 as benchmark
compare.data &lt;- mydata[301:400,] # students from year 1996

# logistic regression model created using benchmark student result
temp.glm &lt;- glm(admit~gre+gpa+rank,data=benchmark.data,family=""binomial"")

# using the regression model to predict how students in 1996 perform
compare.data[,""predict""] &lt;- predict(temp.glm,newdata=compare.data,type=""response"")

# making a threshold that if the predicted chance of admit &gt; 0.5, then it is asssumed that the student will get admitted
compare.data[,""predict_admit""] &lt;- ifelse(compare.data[,""predict""]&gt;0.5,1,0)
table(compare.data[,c(""admit"",""predict_admit"")])

#      predict_admit
# admit  0  1
#     0 59  6
#     1 26  9
</code></pre>

<p>From the table, it is seen that 15 students predicted to get admitted and actual number of students get admitted is 35, so the observed/expected ratio is <code>35/15=2.33</code>, as it is larger than <code>1</code>, so I will say that students in year 1996 is performing better than benchmark.</p>

<p>Can I draw my conclusion using the method mentioned above?</p>

<p>Besides, how should I set the threshold? Or should I <code>sum(compare.data[,""predict""])</code> and treat it as expected value?</p>

<h3>Update 1</h3>

<p>I tried and used ROC curve to determine the threshold:</p>

<pre><code>library(ROCR)
benchmark.data[,""predict""] &lt;- predict(temp.glm,newdata=benchmark.data,type=""response"")
preds &lt;- prediction(benchmark.data[,""predict""],as.numeric(benchmark.data[,""admit""]))
plot(performance(preds,""tpr"",""fpr""),print.cutoffs.at=seq(0,1,by=0.05))
</code></pre>

<p>And the charts suggests that threshold at 0.35 seems to give maximized sensitivity and specificity.</p>
"
"0.120797969451519","0.129492442570703","103077","<p>I am having a lot of fun with regression analysis at the moment, and by fun I mean bashing myself repeatedly over the head. I have a set of 200 data points, by filtering on a property of interest, I end up with 153 points of use. </p>

<p>I initially used these 153 points to generate a linear regression, with an excellent R${^2}$ and a plot of fitted vs actual variables of almost a perfect diagonal. Great! However, it was suggested that this might only be an internally predictive model (which as I understand it means the model fits the data, rather than the opposite). So, I then tried this: I randomly selected a sample of 100 of the 153 results, and built the same model, it still gave a relatively good fit. I then used the predict function in R to try to predict the outcome of the other 53 records. It did not go well. What I got was one of 2 things.</p>

<ol>
<li>the predictions made no sense at all, not even on the same scale as the actual values.</li>
<li>most of the predictions made sense (although weren't very accurate) and one or two, were on an entirely different scale (orders of magnitude larger, or smaller).</li>
</ol>

<p>Since the model I am fitting has time as the response variable, it was suggested I use a Gamma fit regression instead of a plain old linear regression. I tried this and ended up essentially with the result.</p>

<p>So, am I using R correctly, was Gamma a good choice for this? I'm pretty sure my data is good (non biased) so if I am unable to predict, despite the good model - does this mean my model is useless? I've been working on this for some weeks now, and it would be great if I could salvage something.</p>

<p>The R commands I have used:</p>

<pre><code>modelSet&lt;-sample(1:nrow(myData),100)
modelData&lt;-myData[modelSet,]
predictData&lt;-myData[-modelSet,]

fit&lt;-lm(""time~(x1+x2+x3+x4+x5+x6)^3"", data=modelData)
pred&lt;-predict(fit, predictData)
plot(predictData$time, pred) &lt;- gives a really not useful plot


fit2&lt;-glm(""time~(x1+x2+x3+x4+x5+x6)^3"", data=modelData, family=Gamma) # tried with link=log too
pred2&lt;-predict(fit2, predictData)
plot(predictData$time, pred2) &lt;- gives an even less useful plot
</code></pre>
"
"0.128491146685005","0.135652379058573","103666","<p>I have trained my random forest model on a 74,000 training examples where each example consists of two proteins Amino Acids sequence (20 characters) and some numeric values representing the similarity between each individual pair of sequences, and finally a numeric value representing the overall similarity between the two proteins, this is a regression model, so I wish for testing I can use just protein sequence and use my trained model to predict the distance between my test case and each of my training protein sequences. A sample of my test case is:</p>

<pre><code>   test= G,Y,L,P,P,S, A,N,L,F,S,N, 1,-2,16,-4,-1,11, 21
</code></pre>

<p>where ""G,Y,L,P,P,S,"" represent a 6 character fragment of the first protein (my testing) and ""A,N,L,F,S,N,"" represent a 6 character fragment of the second protein (my training database) and the numbers ""1,-2,16,-4,-1,11,"" each number represent the similarity between individual pairs of the 6 Amino Acids, e.g., the similarity between ""G and A"" is 1 and the similarity between ""Y and N"" is -2 and the similarity between ""L and L"" is 16, and so on offcourse the higher the number means the higher the similarity between the pairs of characters, finally the last number ""21"" represent the sum of the previous 6 numbers which represents the overall similarity between the two sequences.</p>

<p>when I trained my model on 74,000 of such training datasets the correlation between the predicted distance and actual distance was as high as 0.86, however, when I used the trained model in for testing the correlation was very low 0.17, I strongly believe that this over-fitting problem, however, I'm not sure is it due to the may be not good training datasets or that my features aren't strong enough to give a good prediction especially since the ranking of the features according to their importance was very high for all the features? the following is my features importance according to each node purity. any help on how to recognize the source of the overfitting is highly appreciated:</p>

<pre><code>       IncNodePurity
 V3      24564.326
 V4      22503.744
 V5      25030.450
 V6      24583.235
 V7      24661.309
 V8      20757.662
 V9      22985.824
 V10     22189.759
 V11     23875.170
 V12     23674.853
 V13     23339.595
 V14     19576.762
 V15     10169.309
 V16     19527.972
 V17      5430.600
 V18      4415.307
 V19     12897.114
 V20      3963.717
 V21     62614.692
</code></pre>
"
"0.0801283080215004","0.078086880944303","104733","<p>I have a vector of data with their standard errors:</p>

<pre><code>Estimate &lt;- c(0.254719513441046, 0.130492717014416, 0.0386710035855823, 0.14118562325405, 0.160649388742147, 0.60363287936294, 0.173485345603584, 0.425817607348994, 0.128802795868366, 0.104136474748465)
SE &lt;- c(0.126815201703205, 0.240179692822184, 0.248612907189712, 0.379800224602374, 0.0799874163236805, 0.170568135051654, 0.108163615496468, 0.0585237357996271, 0.16702614577514, 0.124308993809982)
</code></pre>

<p>I need to form a prediction interval for new elements that will be drawn from the same parent population. At first I thought I would do a linear regression to estimate the mean and the error on the mean:</p>

<pre><code>summary(lm(Estimate ~ 1, weights = 1/SE^2))
Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.28014    0.04996   5.607 0.000331 ***
</code></pre>

<p>But when I try naively to call <code>predict()</code> here is what I get:</p>

<pre><code>predict(lm(Estimate ~ 1, weights = 1/SE^2), newdata = data.frame(x = 0), interval = ""prediction"")
        fit       lwr      upr
1 0.2801405 -2.860802 3.421083
Warning message:
In predict.lm(lm(Estimate ~ 1, weights = 1/SE^2), newdata = data.frame(x = 0),  :
  Assuming constant prediction variance even though model fit is weighted
</code></pre>

<p>Did I do this right? Is the 95% confidence interval really [-2.86 3.42]? Do I need to worry about the warning?</p>
"
"0.126693979201741","0.123466199581199","104889","<p>I am working on cross-validation of prediction of my data with 200 subjects and 1000 variables. I am interested ridge regression as number of variables (I want to use) is greater than number of sample. So I want to use shrinkage estimators.  The following is made up example data:</p>

<pre><code> #random population of 200 subjects with 1000 variables 
    M &lt;- matrix(rep(0,200*100),200,1000)
    for (i in 1:200) {
    set.seed(i)
      M[i,] &lt;- ifelse(runif(1000)&lt;0.5,-1,1)
    }
    rownames(M) &lt;- 1:200

    #random yvars 
    set.seed(1234)
    u &lt;- rnorm(1000)
    g &lt;- as.vector(crossprod(t(M),u))
    h2 &lt;- 0.5 
    set.seed(234)
    y &lt;- g + rnorm(200,mean=0,sd=sqrt((1-h2)/h2*var(g)))

    myd &lt;- data.frame(y=y, M)
myd[1:10,1:10]

y X1 X2 X3 X4 X5 X6 X7 X8 X9
1   -7.443403 -1 -1  1  1 -1  1  1  1  1
2  -63.731438 -1  1  1 -1  1  1 -1  1 -1
3  -48.705165 -1  1 -1 -1  1  1 -1 -1  1
4   15.883502  1 -1 -1 -1  1 -1  1  1  1
5   19.087484 -1  1  1 -1 -1  1  1  1  1
6   44.066119  1  1 -1 -1  1  1  1  1  1
7  -26.871182  1 -1 -1 -1 -1  1 -1  1 -1
8  -63.120595 -1 -1  1  1 -1  1 -1  1  1
9   48.330940 -1 -1 -1 -1 -1 -1 -1 -1  1
10 -18.433047  1 -1 -1  1 -1 -1 -1 -1  1
</code></pre>

<p>I would like to do following for cross validation - </p>

<p>(1) split data into two halts - use first half as training and second half as test </p>

<p>(2) K-fold cross validation (say 10 fold or suggestion on any other appropriate fold for my case are welcome)  </p>

<p>I can simply sample the data into two (gaining and test) and use them: </p>

<pre><code># using holdout (50% of the data) cross validation 
training.id &lt;- sample(1:nrow(myd), round(nrow(myd)/2,0), replace = FALSE)
test.id &lt;- setdiff(1:nrow(myd), training.id)

 myd_train &lt;- myd[training.id,]
 myd_test  &lt;- myd[test.id,]   
</code></pre>

<p>I am using <code>lm.ridge</code> from <code>MASS</code> R package. </p>

<pre><code>library(MASS)
out.ridge=lm.ridge(y~., data=myd_train, lambda=seq(0, 100,0.001))
plot(out.ridge)
select(out.ridge)

lam=0.001
abline(v=lam)

out.ridge1 =lm.ridge(y~., data=myd_train, lambda=lam)
hist(out.ridge1$coef)
    out.ridge1$ym
hist(out.ridge1$xm)
</code></pre>

<p>I have two questions - </p>

<p>(1) How can I predict the test set and calculate accuracy (as correlation of predicted vs actual)?</p>

<p>(2) How can I perform K-fold validation? say 10-fold?</p>
"
"0.0566592699670073","0.0552157630374233","105457","<p>So, I had a weighted dynamic graph having info about 10 consecutive timesteps ( basically 10 files ). Now, I had to mine out patterns in the weight and structure of the complete graph. I did that. The output file (having around 10,000 rows) was something like,</p>

<pre><code>Node 1 Node 2 Pattern
 191    570    ""00""  
 21     570    ""00"" 
 378    570    ""00"" 
 459    570    ""00"" 
 552    570    ""00"" 
 223    570    ""00""
 197    570    ""00"" 
 570    689    ""00"" 
 ...................
</code></pre>

<p>Basically gives 2 nodes, and the pattern associated. What I wish to ask is, what kind of model like (linear regression, or bar plots, .... ) can I use here, in order to gather some meaningful info, i.e. let's say, I am able to come up with a prediction model or I can say, that these are 2 graphs which are similar in nature.</p>
"
"0.0400641540107502","0.0390434404721515","105796","<p>Im calculating a Structural Equation model with Partial Least Squares (with R).</p>

<p>Lets say a simple example:</p>

<ul>
<li>two Response values (R1, R2) are combined to a latent variable RespLV = weight1*R1 + weight2*R2</li>
<li>And a few covariates are also combined into latent variables (CoefLV1, CoefLV2, ...)</li>
<li>All latent Variables are standardized to with mean=0 and variance=1</li>
<li>Now a regression is performed with the result RespLV = beta1 * CoefLV1 + beta2 * CoefLV2 + ...</li>
</ul>

<p>It is now possible to do a prediction on the standardized RespLV. Is there a possibility to to a prediction on the unstandardized RespLV?</p>
"
"0.126693979201741","0.123466199581199","108088","<p>I have some elementary problems understanding the consequences of using/adding a lagged  dependent variable in my  predictive model.  Iâ€™m trying to predict values $Y_{i,t+\tau}$  for  $\tau=1-3$ with:</p>

<p>$Y_{i,t+1}=a+bY_{i,t}+cX_{i,t}+e_{i,t+1}$</p>

<p>$Y_{i,t+2}=a+bY_{i,t}+cX_{i,t}+e_{i,t+2}$</p>

<p>$Y_{i,t+3}=a+bY_{i,t}+cX_{i,t}+e_{i,t+3}$ </p>

<p>I already performed a pooled regression where you basically ignore individual firm effects and time-effects and treat every subject equally. As I am trying to forecast different levels (in USD) and my data appears to be extremely tailed as it covers a few extremely large subjects (with extremely high values) but also many small subjects the predictions of the model perform rather poor as the intercept $a$ that is equal for all subject seems largely responsible for this. A fixed model however with individual intercepts is not valid with Lagged dependent variables as the LDV is correlated with the within errors. To account for the heavy tailed errors I already estimated the pooled model with the rlm package (robust lm) that produced slightly better results but overall they appear still very unsatisfactory.</p>

<p>I further read that adding LDVâ€™s results in biased and inconsistent estimators as there is severe correlation between the predictor variables and the model errors and that regular procedures for autocorrelation are not valid anymore. One solution I came across is the use of Instrumental Variables with an Anderson-Hsiao Estimator (i.e using a lag -2 that is not correlated with the error term (with non-autocorrelation assumed but how can you assume no autocorrelation if you incorporate a lag?) Another one is the Arellano Bond GMM estimator, however applying GMM you have to set up moment conditions and I have no idea how to do that and I donâ€™t know exactly how this methods work. What I care about is to obtain an unbiased estimator with valid coefficients and not about standard errors as I donâ€™t do inference. Are there any other strategies to cope with LDVâ€™s I am currently unaware of and what is the best/ideal/easiest way to deal with such matter? Do you best take care of some issues while you ignore others (e.g. autocorrelation)? Iâ€™m a little bit lost here.</p>
"
"0.212125728690807","0.213849730692075","109464","<p>I am new to regression and having problem in solving Heteroscedasticity in OLS. Have done lots of homework and test before seeking your advice. Sharing the background and what I have done to solve the problem. Hope you can share your thoughts if my approach was correct.</p>

<p><strong>Objectives:</strong></p>

<ol>
<li>To find the relationship (model) between an explanatory variable (x) and an explained variable (y) using OLS regression.</li>
<li>if a model (relationship) is found, its usefulness and accuracy of prediction will be studied.</li>
</ol>

<p><strong>Dataset (Cross-sectional):</strong></p>

<ol>
<li>Have 4 datasets, with each 350 sample size.</li>
<li>Each dataset obtained using different intensity of experiment and this is already captured by the explanatory variable in x.</li>
<li>Due to the heterogenity of data, not possible to lump all into a single dataset.</li>
</ol>

<p><strong>Requirement:</strong></p>

<p>One common and statistically acceptable model for all the 4 datasets using OLS</p>

<p><strong>Steps Followed:</strong></p>

<ol>
<li><p>Explanatory Analysis: Found Non-linear relationship </p></li>
<li><p>As intending to use OLS, did 3 transformations of variables in attempt to have linearity:
a) ln(x) ~ ln(y);
b) ln(x) ~ y;
c) x ~ ln(y).
<strong>Note:</strong> Kept d) x ~ y as benchmark</p></li>
<li><p>Did heteroscedasticity test using Breusch-Pagan (BP) test in R for 2(a)-(d) for all the datasets in attempt to find valid model(s).
On the best case i.e 2b), only 2 out of 4 datasets passed the BP test (p-value>0.05)</p></li>
<li><p>As the aim is to have one common model for all the 4 datasets, another variable transformation is done using Tukey's Ladder of Transformation in attempt to have homoscedasticity:
a) ? ? {-2,-1,-0.5, 0.5, 1, 2} is used for x/y/x and y for each of the models in 2(a)-(d). Have total of 64 models (16 x 4) to consider. X and Y refer to the transformed x and y;
b) Now have 2 models passed BP test for 3 out of 4 datasets in the best case;
c) The one that failed has p-value &lt;2.20E-16.</p></li>
<li><p>[deadlock unable to find one valid model that passes all the 4 datasets]</p></li>
<li><p>Proceeded to take the two valid models in Step 4 and done inference Test:
a) the p-values for t-test and F-test are below 0.05 for all the 4 datasets;
b) R-square are above 0.9402 for all the 4 datasets.</p></li>
<li><p>Did cross validation and selected the best model using the smallest mean square error against the two ""valid"" models. Did back transformation on the original scale first before the selection is done so that its apple to apple data comparison. The mean average percentage error for the best model is below 10%</p></li>
<li><p>Now tried to use the best model for prediction:
a) Selected 20 random x values which were not part of the dataset;
b) Predicted y and compared it against Measured y;
c) the  mean average percentage error is below 8% and within the model's mean average percentage error i.e below 10%.</p></li>
</ol>

<p><strong>The problem:</strong></p>

<p>With the steps above I am unable to get a model that passes the heteroscedasticity test all the 4 datasets. Have I done anything incorrectly or is there anything more can be done in Step 4? </p>

<p>Believe mis-specification issue has duly been attended. Not intending to use GLS as I need to use .OLS</p>

<p>I have used heteroscedasticity robust standard errors as a remedy of heteroscedasticity on the one dataset that failed BP test per the Youtube below.
Refer - <a href=""https://www.youtube.com/watch?v=hFoDDwTF4KY"" rel=""nofollow"">https://www.youtube.com/watch?v=hFoDDwTF4KY</a></p>

<p>The standard error increased and t-value decreased for Y for the HC3 corrected dataset. 
But the Y= a  + b X model remain the same.</p>

<p>Is it sufficient to show the p-value for t-test and F-test for the corrected dataset are still below 0.05 hence its ok to use the same Y= a+bX though it failed the BP test earlier?</p>

<p>Hope you can share your thoughts as I am new to regression. </p>

<p>Using many reference books to learn such as </p>

<ol>
<li>Introduction to Econometrics by Wooldridge</li>
<li>Basic Econometrics by Gujerati</li>
<li>Regression Analysis by Example by Chatterjee</li>
</ol>

<p><strong>Original:</strong></p>

<pre><code>Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)          -0.612116   0.009006  -68.76   &lt;2e-16 ***
Y                     5.955984   0.039653  145.65   &lt;2e-16 ***
---

Residual standard error: 0.04138 on 348 degrees of freedom
Multiple R-squared:  0.9832,    Adjusted R-squared:  0.9831 
F-statistic: 2.092e+04 on 1 and 348 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p><strong>Heteroskedasticity Robust Standard Errors corrected using HC3:</strong></p>

<pre><code>Coefficients:
                     Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)          -0.61212    0.01767  -33.77   &lt;2e-16 ***
Y                     5.95598    0.08432   69.12   &lt;2e-16 ***
---

Residual standard error: 0.04138 on 348 degrees of freedom
Multiple R-squared:  0.9832,    Adjusted R-squared:  0.9831 
F-statistic:  4640 on 1 and 348 DF,  p-value: &lt; 2.2e-16

Note: Heteroscedasticity-consistent standard errors using adjustment hc3 
</code></pre>

<p>Thanks</p>
"
"0.0895861718290583","0.0873037869711973","109851","<p>I am using logistic regression to predict likelihood of an event occurring. Ultimately, these probabilities are put into a production environment, where we focus as much as possible on hitting our ""Yes"" predictions. It is therefore useful for us to have an idea of what definitive ""hits"" or ""non-hits"" might be <em>a priori</em> (before running in production), in addition to other measures we use for informing this determination.</p>

<p>My question is, what would be the proper way to predict a definitive class (1,0) based on  the predicted probability? Specifically, I use R's <code>glmnet</code> package for my modeling. This package arbitrarily picks .5 probability as threshold for a yes or no. I believe that I need to take the results of a proper scoring rule, based on predicted probabilities, to extrapolate  to a definitive class. An example of my modeling process is below:</p>

<pre><code>mods &lt;- c('glmnet', 'scoring')
lapply(mods, require, character.only = T)

# run cross-validated LASSO regression
fit &lt;- cv.glmnet(x = df1[, c(2:100)]), y = df1[, 1], family = 'binomial', 
type.measure = 'auc')

# generate predicted probabilities across new data
df2$prob &lt;- predict(fit, type=""response"", newx = df2[, c(2:100)], s = 'lambda.min')

# calculate Brier score for each record
df2$propscore &lt;- brierscore(df2[,1] ~ df2$prob, data = df2)
</code></pre>

<p>So I now have a series of Brier scores for each prediction, but then how do I use the Brier score to appropriately weight each likelihood being a yes or no?</p>

<p>I understand that there are other methods to make this determination as well, such as Random Forest.</p>
"
"0.165188738787336","0.160980229054196","110033","<p>I am running a post-hoc analysis on the data collected during an experiment in which 15 unique stimuli were presented to participants. Having run a least squares regression using the lm() function in R I have found significant results for a subset of the data including 90 observations from 6 participants with two continuous variables and their interaction.</p>

<p>Taking advice from an article by Judd, Westfall &amp; Kenny (2012) I attempted to use a combination of the lmer() function found in the lme4 package in combination with a Kenward-Roger approximation through the KRmodcomp() function in the pbkrtest package (see the appendix in the article) in order to control for random effects:</p>

<pre><code>lmer(Prediction_Difference_Scale~Diff_AWD_LRTI_End_Scale*Diff_AWD_BD_End_Scale + (1|Unique_ID) + (Diff_AWD_LRTI_End_Scale*Diff_AWD_BD_End_Scale|Block),data=Data)
</code></pre>

<p>The first variable after the DV is the fixed effect, the second variable in parentheses indicates that the intercept is random with respect the unique stimuli (Unique_ID) and the third variable in parentheses indicates that both the intercept and the Condition slopes are random with respect to participant (Block) and that a covariance between the effects should be estimated. </p>

<p>When running the lmer() function I get the following error message:</p>

<pre><code>Error in checkNlevels(reTrms$flist, n = n, control) : 
  number of levels of each grouping factor must be &lt; number of observations
</code></pre>

<p>This is obviously because the number of observations equal the number of unique stimuli.</p>

<p>The function works when excluding the (1|Unique_ID) random  effect, which if I understand correctly is the same as carrying out a 'by stimulus' analysis. However, the authors warn against this by stating: ""Conceptually, a significant by-participant result suggests that experimental results would be likely to replicate for a new set of participants, but only using the same sample of stimuli. A significant by-stimulus result, on the other hand, suggests that experimental results would be likely to replicate for a new set of stimuli, but only using the same sample of participants. However, it is a fallacy to assume that the conjunction of these two results implies that a result would be likely to replicate with simultaneously new samples of both participants and stimuli.""</p>

<p>I would like to control for the random effects of both stimuli and participants, but I am unsure how to proceed?</p>

<p>The article can be accessed here: <a href=""http://jakewestfall.org/publications/JWK.pdf"" rel=""nofollow"">http://jakewestfall.org/publications/JWK.pdf</a></p>

<hr>

<p>To clarify the question regarding the 15 unique stimuli, this is 15 unique stimuli per participant, meaning the sample of 90 observations consists of 6 participants. The stimuli for all of the 90 observations are unique however.</p>

<p>I suppose what my question boils down to is whether there is even a need to include the (1|Unique_ID) 'variable' in the function formula as there is no error dependence between any of the stimuli?</p>
"
"0.126693979201741","0.123466199581199","110136","<p>I'm working on a prediction model for a continuous variable (amount of medicine injected) .I use R for modeling.My project flow is to multiply the prediction of a glm (logistic regression) model that is used to predict 0/1 if a medicine was injected at all with an lm (linear regression) model that is used predict amount of medicine injected - this model works rather good In R .My problem is that when I move this model to MSSQL I get different values for the prediction (i.e. for a random row the value in the R is 400 and in SQL the value for the same row is 640.The model in SQL is made by attaching the models coefficiants from the glm model to produce the glm prediction values and then multiplying it with the lm model prediction values. I don't understand why there is a difference if I use the same coefficients?</p>

<p>Here is the code for the lm and glm models in r:</p>

<pre><code>d7_lm&lt;-lm(Ttl_Inject~UserSource+IsNewIndividual+IsCross,data=train)
d7_glm&lt;-glm(Is_Injected~UserSource+IsNewIndividual+IsCross,data=train)
</code></pre>

<p>Here is a part of the r code for the prediction:</p>

<pre><code>demo$d7_lm_pred&lt;-predict(d7_lm,newdata=demo,type='response')
    demo$d7_glm_pred_response&lt;-ifelse(predict(d7_glm,newdata=demo,type='response')&gt;0.5,1,0)
demo$glm01_lm_response&lt;-demo$d7_lm_pred*demo$d7_glm_pred_response # this is used for a container of the prediction model's values.
</code></pre>

<p>Here is a part of the SQL code : </p>

<pre><code>select TOP 1000*, InjectionAmount_pred= (-2.213e -1.180e+00*(case when User='IAF' then 1 else 0 end)-1.665e+00*(case when UserSource='Viral' then 1 else 0 end)
+IsNewIndividual  *  1.167e+00+IsCross )

* IIF((1 / (1 + EXP(-(-1.346e-03+1.140e-02*(case when UserSource='IAF' then 1 else 0 end) -2.975e-03*(case when UserSource='Viral' then 1 else 0 end)
-IsNewIndividual  * 1.503e-04 +IsCross ))))&gt;0.5,1,0) 
</code></pre>
"
"0.0693931503088838","0.0676252226000574","110932","<p>I am working on some <strong>non-parametric bayesian based predictive analysis</strong> using <strong>R</strong>. I have a set of data which denotes various parameters of an online transaction. Based on these parameters I want to develop a model which will provide predictions for future online transactions.</p>

<p>The training data consist of records in this format:</p>

<pre><code>transaction_id (numeric)| duration (integer)| amount | is_holiday (boolean) | status(1 or 0)
                        |                   |        |                      |
                        |                   |        |                      | 
</code></pre>

<p>The problem that I am facing is that I do not know how to proceed ahead. I am do know know what are the steps that I need to follow. I looked up and found that there are few packages in R like <code>DPpackage</code> which have some functions for non-parametric bayesian modeling but there is no concrete example available about how to use it in order to perform various steps of training and testing.</p>

<p>It would be helpful for me if someone could provide me some guidance as in which process will be better for such kind of predictive/regression analysis and how to proceed ahead, like what steps should I perform to get the training and testing done.</p>

<p>Thanks in advance!  </p>
"
"0.105999788000636","0.103299233817667","111540","<p>I have two data sets whose structure is like this:</p>

<p><strong>DATA SET 1:</strong> </p>

<pre><code>        month_year  sales
 [1,]  ""Jan 2000""  ""30000""
 [2,]  ""Feb 2000""  ""12364""
 [3,]  ""Mar 2000""  ""37485""
 [4,]  ""Apr 2000""  ""2000""
 [5,]  ""Jun 2000""  ""7573""
          .     .      .
          .     .      .
</code></pre>

<p><strong>DATA SET 2:</strong></p>

<pre><code>          month_year    profit
     [1,] ""Jan 2000"" ""84737476""
     [2,] ""Jan 2000"" ""39450334""
     [3,] ""Jan 2000"" ""48384943""
     [4,] ""Feb 2000"" ""12345678""
     [5,] ""Feb 2000"" ""49595340""
     [6,] ""Jan 2001"" ""36769493""
              .     .      .
              .     .      .
</code></pre>

<p>As it can be seen the <strong>first data set has one sales value for each month of each year while in second data set I have <code>n</code>(say 100 but not constant all the time) number of values for each month of each year.</strong></p>

<p>Now I want to develop <strong>a predictive model</strong> which can provide <strong>predictions for profit for future coming months</strong>. What is the best approach to develop such a predictive model? I searched online and found that we can form <strong>a regression</strong> or correlation <strong>fit</strong> model and based on that can do predictions. Which way is better- Regression or correlation? And <strong>how to form a regression fit for such data sets</strong> where one data set has multiple data points for each month of each year while the other has a single data point for each month of each year?</p>

<p><strong>NOTE:</strong> I don't know whether it is the right approach or not but if we can get the regression/correlation value for like the month of Jan,Feb,Mar...and so on and then based on sales value we can do prediction of profit in general. I am new to this and hence I can lack in having certain concepts clear and thus request for some guidance for this issue.  </p>
"
"0.114024581281567","0.123466199581199","114211","<p>Out of curiosity, I want to understand how to model this problem. I've been hearing people suggest the use of linear regression but <strong>I am not sure how to encode this problem</strong> (included my attempt below) in R as I am a complete beginner in this area. </p>

<p>I have a task that can be done any number of times (each individual instance is a task instance). Everytime the task completes 1%, I recorded the time elapsed since the task's start time. Therefore, for each task, I will have 100 points (100 1% increments) at which I recorded the time elapsed. </p>

<p>Given that I have this data for many instances, is it possible to predict the finish time for this task when a new task instance is given? </p>

<pre><code>      TaskID Percent TimeElapsed
   1:      1       0   0.2035333
   2:      1       1   0.2062833
   3:      1       2   0.2137167
   4:      1       3   0.2180833
   5:      1       4   0.2490833
  ---                           
3127:     31      96   4.9391667
3128:     31      97   4.9970500
3129:     31      98   5.5644500
3130:     31      99   5.6532667
3131:     31     100   5.8359833
</code></pre>

<p>A quick look at the task behavior (below) tells me there is a bit of a variance in how the task behaves so its hinting that the output should not just be a time prediction but rather a time prediction with some confidence? </p>

<p>In addition, I'm thinking just using the information about the current progress of the task might not be sufficient - the task may have slowed down in some its previous progress points so the finsh time would be affected. Therefore, this information should somehow be encoded into the model?</p>

<p><img src=""http://i.stack.imgur.com/eiEKh.png"" alt=""enter image description here""></p>

<p>I am particularly interested in how to do this using R. I included my initial attempt at using linear regression here but the result does not look good to me. Any suggestions on how to improve this or use some other methods? </p>

<p>I have given the output of dput (on a data table: <code>install.packages(""data.table"")</code>) on <a href=""http://pastebin.com/zX5GdKP2"" rel=""nofollow"">pastebin</a>. If you want a data.frame instead, please see this <a href=""http://pastebin.com/JwVhTCTU"" rel=""nofollow"">paste</a> instead.</p>

<p><strong>EDIT:</strong> Attempt at using linear regression</p>

<p>The thick black line is the median at every point. The thick red line is the regression line fit to the median line. </p>

<p><img src=""http://i.stack.imgur.com/mZFZd.png"" alt=""enter image description here""></p>
"
"0.0566592699670073","0.0552157630374233","114218","<p>After running a gradient boosted model with <code>n</code> data points using multinomial regression where the response variable (a factor, as required by the gbm function) has <code>k</code> levels with R package gbm, I see that the predictions are output as as a vector of length <code>n*k</code>. Predicted responses are from:</p>

<pre><code>probs.var.multinom &lt;- predict.gbm(gbm.model.multinom, test.data, best.iter.gbm, 
                                  type=""response"")
</code></pre>

<p>Note that this is different from the output of a logistic (distribution = ""bernoulli"") model, where the results are a vector the same length as the number of cases.</p>

<p>How should this be interpreted? Specifically, how can I link the response vector back to the input data set to evaluate the classification?</p>
"
"0.120192462032251","0.117130321416455","114675","<p>I really want to understand how the math is working here. I am trying to get the standard error of the fitted values for a time series regression model. In the non-time series regression, I know I can take the transpose of the data multiplied by the variance - covariance matrix of the model coefficients and then multiply by the data values again to get the standard errors of the fitted values.</p>

<p>But I'm not sure how to do this when I am including an autoregressive term.</p>

<pre><code>require(forecast)
require(tserieS)
</code></pre>

<p>Response variable</p>

<pre><code>Sablects &lt;- rnorm(10)
</code></pre>

<p>Covariates</p>

<pre><code>my.xreg &lt;- cbind(rnorm(10),rbinom(10,1,0.5))
</code></pre>

<p>In my actual data, values are normalized so I set the intercept equal to zero here.</p>

<pre><code>m4&lt;-arima(Sablects, order=c(2,0,0),fixed=c(0,NA,0,NA,NA),xreg=my.xreg) 
</code></pre>

<p>The predict function will give me standard errors on my in-sample prediction (the fitted values of my model).</p>

<pre><code>my.se &lt;- predict(m4, newxreg = my.xreg, n.ahead = 10)$se         

my.se
</code></pre>

<p>Now to compare the output of my.se, I want to do this mathematically but I don't know what to use for the values of the ar2 term. I use 1's as a placeholder to demonstrate that my output does not equal the values from <code>my.se</code> above</p>

<pre><code>C &lt;- cbind(rep(1, nrow(my.xreg)), my.xreg[, 1], my.xreg[, 2])

C
</code></pre>

<p>I think this value should equal the first value in my.se, but is not producing the same value as my.se</p>

<pre><code>sqrt(t(C[1, ]) %*% vcov(m4) %*% C[1, ])
</code></pre>

<p>Also, I'm not so great with matrix multiplication but here is my work around for getting all of the se values.</p>

<pre><code>se.output &lt;- matrix(nrow=nrow(C))
</code></pre>

<p>Specify that the max number of i is equal to number of rows of <code>C</code>.</p>

<pre><code>  for(i in 1:nrow(C)){

    # Loop through your multiplication for each row (i) of `C`. For each iteration, save the new data into the new row of se.output

    se.output[i] &lt;- sqrt(t(C[i, ]) %*% vcov(m4) %*% C[i, ])  
    }

se.output
</code></pre>
"
"0.0400641540107502","0.0390434404721515","116175","<p>I try to get into in Cox Regression and read the example chapter from <a href=""http://www.clinicalpredictionmodels.org/"" rel=""nofollow"">Steyerberg's book</a>.</p>

<p>Afterwards I tried to plot log relative hazards against continuous variables using the rms package:</p>

<pre><code>d  &lt;- read.spss('~/R-Test/SMARTst.sav',use.value.labels=F, to.data.frame=T)
dd &lt;- datadist(d)
options(datadist=""dd"")
fit &lt;- cph(Surv(TEVENT,EVENT) ~  rcs(IMT,4), data=d)
plot(Predict(fit), lty=2, lwd=2)
</code></pre>

<p>What are knots? Well, I see what happens when I Change 4 to 3 or numbers > 4 ... but I do not know what I do and why? ;-) Maybe that will be explained in earlier chapters I currrently do not have.</p>

<p>Can someone explain it too me or recommand free websites/articles explaining this in an easy fashin (i am not a statistician).</p>

<p>My final goal is to learn how to create adequate cox models for exploratory medical analyses.</p>
"
"0.0566592699670073","0.0552157630374233","116852","<p>I need help in understanding the <code>pmodel.response</code> function from the R package <code>plm</code>. So far I have interpreted this as a way to get predicted values from a panel data regression.</p>

<p>In the code below I run a least squares dummy variables regression using the standard <code>lm</code>-function and a fixed effects model using <code>plm</code> and then try to compare predictions and model response.</p>

<pre><code>library(plm)

data(Grunfeld)
Grunfeld &lt;- pdata.frame(Grunfeld, index = c(""firm"", ""year""))

grun.lm &lt;- lm(inv ~ value + capital + factor(firm), data=Grunfeld)
grun.fe &lt;- plm(inv ~ value + capital, data=Grunfeld, effect=""individual"",
               model=""within"")

Grunfeld$predict.lm &lt;- predict(grun.lm)
Grunfeld$predict.plm &lt;- pmodel.response(grun.fe)
</code></pre>

<p>Now, if I take a look at the outcome:</p>

<pre><code>&gt; head(Grunfeld)
       firm year   inv  value capital predict.lm predict.plm
1-1935    1 1935 317.6 3078.5     2.8   269.5876     -290.42
1-1936    1 1936 391.8 4661.7    52.6   459.3769     -216.22
1-1937    1 1937 410.6 5387.1   156.9   571.6005     -197.42
1-1938    1 1938 257.7 2792.2   209.2   302.0566     -350.32
1-1939    1 1939 330.8 4313.2   203.4   467.7566     -277.22
1-1940    1 1940 461.2 4643.9   207.2   505.3528     -146.82
</code></pre>

<p>It seems like the output of <code>pmodel.response</code> hardly has anything to do with predicted values. So, what does this function actually do? How to interpret the values in column <code>Grundfeld$predict.plm</code>? This does not get clear for me from the documentation.</p>

<p>Thanks for any help!</p>
"
"0.0801283080215004","0.078086880944303","117867","<p>I have a basic linear regression model I fitted to a time series. Unfortunately I have to account for autocorrelation and heteroskedasicity in the model and I have done so with the NeweyWest function from the sandwich package in R while analyzing the coefficients. </p>

<p>Now I would like to create prediction intervals using the predict() function (or any other function) while utilizing the NeweyWest matrix/SEs.</p>

<p>As this is the first quesiton I post on here and my experinece in R is very limited here is some information:</p>

<pre><code>LMModel = lm(Return~Sentiment, data=Time Series)
</code></pre>

<p><strong>This is the function I used for my coefficient testing:</strong></p>

<pre><code>coeftest(LMModel , vcov=NeweyWest(LMModel , lag=27, ar.method=""ols""))
</code></pre>

<p><strong>I would like thsi function to use NeweyWest in some way:</strong></p>

<pre><code>predict(LMModel, newdata, interval = ""prediction"", level = 0.95) 
</code></pre>

<p>Thanks a lot in advance!</p>
"
"0.0981367343026181","0.0956365069595007","119716","<p>At the moment I am dealing with some kind of supervised regression problem, which differs from the stuff I have found on the internet. </p>

<p><strong>So here's what the data looks like</strong>. (I am working with R)<br>
I have a list with many data.frames. Each data.frame contains ~10 rows with some columns, where one columns says, if this row is <code>winner</code> or not. If one row has <code>winner==TRUE</code> depends on the other variables for this row ( mostly there is just one winner, but there can be also two, maybe even three or none winners in each data.frame ), but more on the values other rows in this set have. And I guess this is the point where it differs from other regression problems I have seen so far, because the training set is not one row containing all information needed to make later predictions, but a data.frame.</p>

<p>One <strong>example data.frame</strong> (they do have more columns)  </p>

<pre><code>      id sub_condition rating rating_count winner
AVLDTQFT           new     90         4469  false
ARUVJVQM           new     98         4751  false
A2MUQS6A           new     98         5306  false
A38QR7HW           new     98        11494   true
A7AAZ62N           new     99         8000  false
A1OS85DC           new     99         2385   true
</code></pre>

<p><strong>So what do I want</strong>?<br>
I would like to feed these data.frames to an algorithm, so I can later on, pass new data.frames and get a prediction for every row in the frame, for how I should change one variable, so this row (if all other stay the same) is <code>winner</code>.</p>

<p>What algorithms and principles do I need to use to solve this type of problem?</p>
"
"0.0566592699670073","0.0552157630374233","119946","<p>I have some time series data where I'm modelling temperature as a function of various predictors. On physical grounds, I can expect that</p>

<p>$$\frac{dT}{dt} \propto T_a - T$$</p>

<p>where $T_a$ is the ambient temperature (which can vary over time, but whose values are known). I thus fit models of the form</p>

<p>$$\Delta T(t) \sim \alpha + \beta \left[ T_a(t) -T(t) \right] + \gamma X(t)$$</p>

<p>with $X$ being the other covariates, and $\alpha$, $\beta$ and $\gamma$ are the regression parameters. I can fit these easily enough in R:</p>

<pre><code>lm(diff(T) ~ I(Ta - T) + x, data=df)
</code></pre>

<p>and I can get predictions for the change in $T$. However, what I really want are predictions for $T$ itself. At the moment I'm calculating these via a loop, where I plug $\hat{T}(t)$ into the regression equation to obtain $\hat{\Delta T}(t+1)$.</p>

<p>Is there any R package, probably time series-related, that will do these calculations automatically?</p>

<p>Also, if there are any issues with this approach, I'd be happy to know about them.</p>
"
"0.0981367343026181","0.0956365069595007","121037","<p>I have a dataset with approximately <strong>4000 rows and 150 columns</strong>. I want to predict the values of a single column (= target).</p>

<p>The data is on cities (demography, social, economic, ... indicators). A lot of these are highly correlated, so I want to do a PCA - Principal Component Analysis. </p>

<p>The problem is, that <strong>~40% of the values are missing</strong>.</p>

<p>My current approach is:
Remove target indicator and do <strong>PCA with mean/median imputation of missing values</strong>.
Select x principal components (PC).
Append target indicator to these PC.
Use PC as predictors for the target variable and try common regression techniques, e.g. knn, linear regression, random forest etc.</p>

<p>With this approach, I'm getting quite good results. My metric is RMSE% - root mean squared relative prediction error. I tried this for all columns in the dataset, the RMSE% is between 0.5% and 8% (depending on the column). These errors are for values I actually know, NOT imputed values.</p>

<p>So, here's my problem: <strong>I'm not sure how much my data is distorted by replacing the missing values with the column mean/median</strong>. Is there any other way of imputing the missing values with minimal effect on the PCA results?</p>
"
"0.138786300617768","0.135250445200115","121192","<p>I have some data I need to fit a model to that can be used for prediction (interpolation). The data is summarized by the plot below. The black line is x=y.</p>

<p><img src=""http://i.stack.imgur.com/x0FqM.png"" alt=""enter image description here""></p>

<p>I want to be able to fit a model so as I can use it to predict any value of the y axis as a function of x axis, as well as get the uncertainty in that estimate.</p>

<p>However in my data, the variance of the y axis variable increases as the x axis variable increases.
In addition, there is another continuous explanatory variable called SequenceSize (plotted as factor to clearly see the colours) which I think I have to take into account, as it is also correlated (negatively) with the variance of the y axis variable, whilst not really affecting the mean so much. As can be seen in the two plots below.</p>

<p><img src=""http://i.stack.imgur.com/7KdHZ.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/d43W1.png"" alt=""enter image description here""></p>

<p>So from a model fit to the data I would like to be able to use it to do.</p>

<ol>
<li>Plugin the value of SequenceSize and the x axis variable.</li>
<li>Get out an estimate of the y axis variable along with some measure of uncertainty in the y estimate, given how the variance and uncertainty is affected by the x axis variable and by SequenceSize.</li>
</ol>

<p>However I'm reading the massive R book by Crawley, and I'm having trouble deciding which model would be best to do this. I'm thinking maybe a multiple regression if I linearized the data by taking log of x and y, but I'm unsure if that's right because of how the variance of the data acts.  </p>

<p>Thanks,
Ben W.</p>
"
"0.0895861718290583","0.0873037869711973","121408","<p>I have average life expectancy at birth data for an 8 year period and I would like to use that 8 year period to predict the trend for average life expectancy for the next 5 years. I would then like to ask whether this deviates significantly from the actual average life expectancy over the next 5 years.</p>

<ol>
<li>What's the best regression model to fit to the observation base data in order to get predictions for next 5 years?</li>
<li>How can I assess whether the difference between the predicted and observed trend is significant?</li>
<li>How can I implement #1 and #2 in R?</li>
</ol>
"
"0.128491146685005","0.146087177447694","122212","<p>I have been developing a logistic regression model based on retrospective data from a national trauma database of head injury in the UK. The key outcome is 30 day mortality (denoted as ""Survive"" measure). Other measures with published evidence of significant effect on outcome in previous studies include:</p>

<pre><code>Year - Year of procedure = 1994-2013
Age - Age of patient = 16.0-101.5
ISS - Injury Severity Score = 0-75
Sex - Gender of patient = Male or Female
inctoCran - Time from head injury to craniotomy in minutes = 0-2880 (After 2880 minutes is defined as a separate diagnosis)
</code></pre>

<p>Using these models, given the dichotomous dependent variable, I have built a logistic regression using lrm.</p>

<p>The method of model variable selection was based on existing clinical literature modelling the same diagnosis. All have been modelled with a linear fit with the exception of ISS which has been modelled traditionally through fractional polynomials. No publication has identified known significant interactions between the above variables.</p>

<p>Following advice from Frank Harrell, I have proceeded with the use of regression splines to model ISS (there are advantages to this approach highlighted in the comments below). The model was thus pre-specified as follows:</p>

<pre><code>rcs.ASDH&lt;-lrm(formula = Survive ~ Age + GCS + rcs(ISS) +
    Year + inctoCran + oth, data = ASDH_Paper1.1, x=TRUE, y=TRUE)
</code></pre>

<p>Results of the model were:</p>

<pre><code>&gt; rcs.ASDH

Logistic Regression Model

lrm(formula = Survive ~ Age + GCS + rcs(ISS) + Year + inctoCran + 
    oth, data = ASDH_Paper1.1, x = TRUE, y = TRUE)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          2135    LR chi2     342.48    R2       0.211    C       0.743    
 0            629    d.f.             8    g        1.195    Dxy     0.486    
 1           1506    Pr(&gt; chi2) &lt;0.0001    gr       3.303    gamma   0.487    
max |deriv| 5e-05                          gp       0.202    tau-a   0.202    
                                           Brier    0.176                     

          Coef     S.E.    Wald Z Pr(&gt;|Z|)
Intercept -62.1040 18.8611 -3.29  0.0010  
Age        -0.0266  0.0030 -8.83  &lt;0.0001 
GCS         0.1423  0.0135 10.56  &lt;0.0001 
ISS        -0.2125  0.0393 -5.40  &lt;0.0001 
ISS'        0.3706  0.1948  1.90  0.0572  
ISS''      -0.9544  0.7409 -1.29  0.1976  
Year        0.0339  0.0094  3.60  0.0003  
inctoCran   0.0003  0.0001  2.78  0.0054  
oth=1       0.3577  0.2009  1.78  0.0750  
</code></pre>

<p>I then used the calibrate function in the rms package in order to assess accuracy of the predictions from the model. The following results were obtained:</p>

<pre><code>plot(calibrate(rcs.ASDH, B=1000), main=""rcs.ASDH"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/HYTsp.png"" alt=""Bootstrap calibration curves penalized for overfitting""></p>

<p>Following completion of the model design, I created the following graph to demonstrate the effect of the Year of incident on survival, basing values of the median in continuous variables and the mode in categorical variables:</p>

<pre><code>ASDH &lt;- Predict(rcs.ASDH, Year=seq(1994,2013,by=1),Age=48.7,ISS=25,inctoCran=356,Other=0,GCS=8,Sex=""Male"",neuroYN=1,neuroFirst=1)
Probabilities &lt;- data.frame(cbind(ASDH$yhat,exp(ASDH$yhat)/(1+exp(ASDH$yhat)),exp(ASDH$lower)/(1+exp(ASDH$lower)),exp(ASDH$upper)/(1+exp(ASDH$upper))))
names(Probabilities) &lt;- c(""yhat"",""p.yhat"",""p.lower"",""p.upper"")
ASDH&lt;-merge(ASDH,Probabilities,by=""yhat"")
plot(ASDH$Year,ASDH$p.yhat,xlab=""Year"",ylab=""Probability of Survival"",main=""30 Day Outcome Following Craniotomy for Acute SDH by Year"", ylim=range(c(ASDH$p.lower,ASDH$p.upper)),pch=19)
arrows(ASDH$Year,ASDH$p.lower,ASDH$Year,ASDH$p.upper,length=0.05,angle=90,code=3)
</code></pre>

<p>The code above resulted in the following output:</p>

<p><img src=""http://i.stack.imgur.com/KGYcz.png"" alt=""Year trend with lower and upper""></p>

<p><strong><em>My remaining questions are the following:</em></strong></p>

<p><strong>1. Spline Interpretation</strong> - How can I calculate the p-value for the splines combined for the overall variable?</p>
"
"0.0991537224422629","0.110431526074847","124214","<p>I build a prediction modeling using both regression and random forest.</p>

<pre><code>testmodel2&lt;-lm(y~as.matrix(xtest))
summary(testmodel2)

rf2&lt;-randomForest(y~.,data=df,importance=TRUE)
varImpPlot(rf2)
</code></pre>

<p>The regression model result shows that <code>t1, t10 and t11</code> are not significant. However, the <code>varImpPlot</code> show that they are pretty important. On the other side, <code>t3,t5 and t6</code> are significant in terms of P-value in the regression result, but they are not important in the Random forest result. </p>

<p>Is there any reason that linear regression result is different with random forest? Which one should be more reliable? The correlation matrix is also attached for the reference. The result of <code>backward step-wise variable selection</code> is also attached.</p>

<p><img src=""http://i.stack.imgur.com/G0h3k.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/lRnDt.png"" alt=""enter image description here""> </p>

<p><img src=""http://i.stack.imgur.com/8BzXW.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/I3zGF.png"" alt=""enter image description here""></p>
"
"NaN","NaN","124373","<p>I'm using the earth package (using caret train function) MARS spline implementation in order to perform non - linear regression modeling. I would like to obtain a measure of prediction uncertainty (not only the expected value). Is there any way to obtain it? Thanks in advance for any help.</p>
"
"0.0981367343026181","0.0956365069595007","124500","<p>I would like to run by you an algorithm for predicting one of two values from a testing data set, based on a linear model applied to a training set. Please let me know whether this algorithm makes sense, whether it can be improved and whether there is already a well established algorithm to accomplish the same result, possibly already encapsulated in some R routine.</p>

<p>Given: Two dataframes, <code>training</code> and <code>testing</code>, comprising two columns: <code>Y</code> and <code>X</code>, in this order. The <code>Y</code> columns of both dataframes take values in the set {2,3}.</p>

<p>Assignment: Predict <code>testing$Y</code> from <code>testing$X</code> based on a linear model with coefficients obtained from the linear regression <code>training$Y ~ training$X</code>.</p>

<p>Suggested solution (R based pseudo-code):</p>

<ol>
<li><code>m &lt;- lm(Y ~ X, data = training)</code></li>
<li><code>p &lt;- predict(training, new_data = testing, interval = ""prediction"")</code></li>
<li>for every row of <code>p</code> do as follows:</li>
<li><code>if p$upr &lt;= 2 or (p$lwr &lt;= 2 &lt; p$upr) or p@fit &lt;= 2, then set p$fit &lt;- 2</code></li>
<li><code>else if p$lwr &gt;= 3 or (p$lwr &lt; 3 &lt;= p$upr) or p$fit &gt;= 3, then set pfit &lt;- 3</code></li>
<li><code>else set p$fit &lt;- round(p$fit)</code></li>
</ol>
"
"0.0566592699670073","0.0552157630374233","125728","<p>In linear regression model, the <code>predict</code> in <code>R</code> is able to calculate the <code>confidence band</code> and <code>prediction band</code>. But for the general predictive models, what are the normal approaches to calculate <code>confidence band</code> and <code>prediction band</code>? I am asking this is because I am experimenting different models for a given data set. The users would like to get <code>confidence band</code> and <code>prediction band</code> as what <code>predict.lm</code> can give. However, not all those model packages provide these two bands-related information. I may have to write my algorithm to calculate them.</p>
"
"0.0801283080215004","0.078086880944303","125764","<p>I'm trying to understand what factors contribute to the a certain outcome which is a ordered factor variable. In order to just understand which factor is statistically more significant than the others, I would like to build a model and given that my output variable is an ordered factorial variable - I thought I should go for Ordinal Logistic Regression. Given the tradeoff between interpretability and flexibility of models, in my case since I'm only making inferences and not predictions, should I rather go for a easier model to handle like Generalized Linear Models? It kind of boils down to me choosing the <code>polr</code> package in R versus the <code>glm</code> one.</p>
"
"0.084988904950511","0.110431526074847","126356","<p>For linear and parametric regression there are multiple tests where variables and residuals are used by means of performing a linear regression function to test serial correlation of regression errors and homocedasticity of regression errors. </p>

<p>My question is about non linear and non parametric regression for prediction or classification such us SVM, NeuralNets, knn, Recursive Partitioning, Adaptive Regression Spline, etc. </p>

<p>In this regard my questions are:</p>

<ol>
<li><p>As is not linear regression what is the equivalent of OLS assumptions for non linear non parametric regression. Are the consequences of OLS violation in the context of nonlinear and non-parametric regression still valid? </p></li>
<li><p>How could I test or what tests exist for serial correlation of errors for non linear and non parametric regression which are not derived from visual inspection of a graph and by using error residuals only. (something comes in mind like testing for significant acf or pacf on the residual errors - Unsure if this is OK).</p></li>
<li><p>How could I test or what tests exist for homocedasticity for non linear and non parametric regression which are not derived from visual inspection of a graph and by using error residuals only. (something comes too mind like homogenity of distances between the residual errors across time).</p></li>
<li><p>Would it be better to transfor the data into linear by seeking some adequate transformation as to avoid all the non linearity issues mentioned above? </p></li>
</ol>

<p>Thank you</p>
"
"0.0566592699670073","0.0552157630374233","126976","<p>I am playing a data without any background information. First, I try multiple linear regression. The model fits well, since the $r^2$ is larger than 90%. I deleted several variables by AIC. The fits improves a little bit. I am interesting what are possible directions that I should look into, if the model fits well at the very beginning.</p>

<p>The target of the analysis is find the model that make the most accurate prediction.</p>

<p>What I can come up with:</p>

<ol>
<li><p>Try other models like regression tree, random forest, SVM or whatever data mining models. The prediction may or may not become better.</p></li>
<li><p>Perform cross validation.</p></li>
<li><p>Check overfitting.</p></li>
<li><p>Diagnostic residual.</p></li>
</ol>
"
"0.0981367343026181","0.0956365069595007","129657","<p>What is the fastest algorithm for fitting a simple logistic 'random effects' type model, with only one level of categorical predictors? </p>

<p>Another way of putting it might be a logistic regression with a Gaussian prior on the coefficients, or ""with shrinkage"".</p>

<p>I'm looking for a very fast and reliable implementation to use in a production environment. This means that the algorithm would need to have a low risk of 'hanging', and a not-drastically-variable time to converge.</p>

<p>There would be between 1 and 5000 data points per 'cell', and 5-100 groups/categories. It would need to exploit sufficient statistics (take counts of group data). Second-level nesting a bonus, but not essential.</p>

<p>This could be done via <code>lme4</code> in <code>R</code>. However, is there a library (e.g. stand-alone C++) which is more efficient for this narrowly-defined type of model?</p>

<p>EDIT: Goal is inference over prediction - specifically, comparison of group estimates (with standard errors), construction of confidence intervals etc.</p>

<p>EDIT: Just to make it clear, I wouldn't be fitting a 'mixed model' so to speak - there would be no fixed effect. The data would be a very long two-column ('successes', 'failures') contingency table, with highly variable n counts.</p>

<p>EDIT: I need the degree of 'shrinkage' in the individual estimates to be informed by the group level variance (as opposed to banging a Jeffery's prior on each individual estimate, or using an Agresti-Coull (1998) type interval).</p>
"
"0.226741209016583","0.227660423202372","131312","<p>I have some R code (which I did not write) and which performs some state space analysis on some time-series. The data itself is shown as dots (scatter plot) and the Kalman filtered and smoothed state is the solid line.</p>

<p><img src=""http://i.stack.imgur.com/41jaI.png"" alt=""Plot""></p>

<p>My question is regarding the confidence intervals shown in this plot. I calculate <em>my own</em> confidence intervals using the standard method (my C# code is below)</p>

<pre><code>public static double ConfidenceInterval(
    IEnumerable&lt;double&gt; samples, double interval)
{
    Contract.Requires(interval &gt; 0 &amp;&amp; interval &lt; 1.0);

    double theta = (interval + 1.0) / 2;
    int sampleSize = samples.Count();
    double alpha = 1.0 - interval;
    double mean = samples.Mean();
    double sd = samples.StandardDeviation();

    var student = new StudentT(0, 1, samples.Count() - 1);
    double T = student.InverseCumulativeDistribution(theta);
    return T * (sd / Math.Sqrt(samples.Count()));
}
</code></pre>

<p>Now this will return a single interval (and it does it correctly) which I will add/subtract from each point on the series I have applied the calculation to to give me my confidence interval. But this is a constant and the R implementation seems to change over the time-series.</p>

<p>My question is why is <strong>the confidence interval changing for the R implementation? Should I be implementing my confidence levels/intervals differently?</strong></p>

<p>Thanks for your time.</p>

<hr>

<p>For reference the R code that produces this plot is below:</p>

<pre><code>install.packages('KFAS')
require(KFAS)

# Example of local level model for Nile series
modelNile&lt;-SSModel(Nile~SSMtrend(1,Q=list(matrix(NA))),H=matrix(NA))
modelNile
modelNile&lt;-fitSSM(inits=c(log(var(Nile)),log(var(Nile))),model=modelNile,
method='BFGS',control=list(REPORT=1,trace=1))$model

# Can use different optimisation: 
# should be one of â€œNelder-Meadâ€, â€œBFGSâ€, â€œCGâ€, â€œL-BFGS-Bâ€, â€œSANNâ€, â€œBrentâ€
modelNile&lt;-SSModel(Nile~SSMtrend(1,Q=list(matrix(NA))),H=matrix(NA))
modelNile
modelNile&lt;-fitSSM(inits=c(log(var(Nile)),log(var(Nile))),model=modelNile,
method='L-BFGS-B',control=list(REPORT=1,trace=1))$model

# Filtering and state smoothing
out&lt;-KFS(modelNile,filtering='state',smoothing='state')
out$model$H
out$model$Q
out

# Confidence and prediction intervals for the expected value and the observations.
# Note that predict uses original model object, not the output from KFS.
conf&lt;-predict(modelNile,interval='confidence')
pred&lt;-predict(modelNile,interval='prediction')
ts.plot(cbind(Nile,pred,conf[,-1]),col=c(1:2,3,3,4,4),
ylab='Predicted Annual flow', main='River Nile')
KFAS 13

# Missing observations, using same parameter estimates
y&lt;-Nile
y[c(21:40,61:80)]&lt;-NA
modelNile&lt;-SSModel(y~SSMtrend(1,Q=list(modelNile$Q)),H=modelNile$H)
out&lt;-KFS(modelNile,filtering='mean',smoothing='mean')

# Filtered and smoothed states
plot.ts(cbind(y,fitted(out,filtered=TRUE),fitted(out)), plot.type='single',
col=1:3, ylab='Predicted Annual flow', main='River Nile')

# Example of multivariate local level model with only one state
# Two series of average global temperature deviations for years 1880-1987
# See Shumway and Stoffer (2006), p. 327 for details
data(GlobalTemp)
model&lt;-SSModel(GlobalTemp~SSMtrend(1,Q=NA,type='common'),H=matrix(NA,2,2))

# Estimating the variance parameters
inits&lt;-chol(cov(GlobalTemp))[c(1,4,3)]
inits[1:2]&lt;-log(inits[1:2])
fit&lt;-fitSSM(inits=c(0.5*log(.1),inits),model=model,method='BFGS')
out&lt;-KFS(fit$model)
    ts.plot(cbind(model$y,coef(out)),col=1:3)
legend('bottomright',legend=c(colnames(GlobalTemp), 'Smoothed signal'), col=1:3, lty=1)

# Seatbelts data
## Not run:
model&lt;-SSModel(log(drivers)~SSMtrend(1,Q=list(NA))+
SSMseasonal(period=12,sea.type='trigonometric',Q=NA)+
log(PetrolPrice)+law,data=Seatbelts,H=NA)

# As trigonometric seasonal contains several disturbances which are all
# identically distributed, default behaviour of fitSSM is not enough,
# as we have constrained Q. We can either provide our own
# model updating function with fitSSM, or just use optim directly:
# option 1:
ownupdatefn&lt;-function(pars,model,...){
model$H[]&lt;-exp(pars[1])
    diag(model$Q[,,1])&lt;-exp(c(pars[2],rep(pars[3],11)))
model #for option 2, replace this with -logLik(model) and call optim directly
}
14 KFAS
fit&lt;-fitSSM(inits=log(c(var(log(Seatbelts[,'drivers'])),0.001,0.0001)),
model=model,updatefn=ownupdatefn,method='BFGS')
out&lt;-KFS(fit$model,smoothing=c('state','mean'))
    out
    ts.plot(cbind(out$model$y,fitted(out)),lty=1:2,col=1:2,
    main='Observations and smoothed signal with and without seasonal component')
    lines(signal(out,states=c(""regression"",""trend""))$signal,col=4,lty=1)
legend('bottomleft',
legend=c('Observations', 'Smoothed signal','Smoothed level'),
col=c(1,2,4), lty=c(1,2,1))

# Multivariate model with constant seasonal pattern,
# using the the seat belt law dummy only for the front seat passangers,
# and restricting the rank of the level component by using custom component
# note the small inconvinience in regression component,
# you must remove the intercept from the additional regression parts manually
model&lt;-SSModel(log(cbind(front,rear))~ -1 + log(PetrolPrice) + log(kms)
+ SSMregression(~-1+law,data=Seatbelts,index=1)
+ SSMcustom(Z=diag(2),T=diag(2),R=matrix(1,2,1),
Q=matrix(1),P1inf=diag(2))
+ SSMseasonal(period=12,sea.type='trigonometric'),
data=Seatbelts,H=matrix(NA,2,2))
likfn&lt;-function(pars,model,estimate=TRUE){
model$H[,,1]&lt;-exp(0.5*pars[1:2])
    model$H[1,2,1]&lt;-model$H[2,1,1]&lt;-tanh(pars[3])*prod(sqrt(exp(0.5*pars[1:2])))
    model$R[28:29]&lt;-exp(pars[4:5])
if(estimate) return(-logLik(model))
model
}
fit&lt;-optim(f=likfn,p=c(-7,-7,1,-1,-3),method='BFGS',model=model)
model&lt;-likfn(fit$p,model,estimate=FALSE)
    model$R[28:29,,1]%*%t(model$R[28:29,,1])
    model$H
out&lt;-KFS(model)
out
ts.plot(cbind(signal(out,states=c('custom','regression'))$signal,model$y),col=1:4)

# For confidence or prediction intervals, use predict on the original model
pred &lt;- predict(model,states=c('custom','regression'),interval='prediction')
ts.plot(pred$front,pred$rear,model$y,col=c(1,2,2,3,4,4,5,6),lty=c(1,2,2,1,2,2,1,1))

## End(Not run)
## Not run:
# Poisson model
model&lt;-SSModel(VanKilled~law+SSMtrend(1,Q=list(matrix(NA)))+
SSMseasonal(period=12,sea.type='dummy',Q=NA),
KFAS 15
data=Seatbelts, distribution='poisson')

# Estimate variance parameters
fit&lt;-fitSSM(inits=c(-4,-7,2), model=model,method='BFGS')
model&lt;-fit$model

# use approximating model, gives posterior mode of the signal and the linear predictor
out_nosim&lt;-KFS(model,smoothing=c('signal','mean'),nsim=0)

# State smoothing via importance sampling
out_sim&lt;-KFS(model,smoothing=c('signal','mean'),nsim=1000)
out_nosim
out_sim

## End(Not run)
# Example of generalized linear modelling with KFS
# Same example as in ?glm
counts &lt;- c(18,17,15,20,10,20,25,13,12)
outcome &lt;- gl(3,1,9)
treatment &lt;- gl(3,3)
print(d.AD &lt;- data.frame(treatment, outcome, counts))
glm.D93 &lt;- glm(counts ~ outcome + treatment, family = poisson())
model&lt;-SSModel(counts ~ outcome + treatment, data=d.AD,
distribution = 'poisson')
out&lt;-KFS(model)
coef(out,start=1,end=1)
coef(glm.D93)
summary(glm.D93)$cov.s
    out$V[,,1]
outnosim&lt;-KFS(model,smoothing=c('state','signal','mean'))
set.seed(1)
outsim&lt;-KFS(model,smoothing=c('state','signal','mean'),nsim=1000)

## linear
# GLM
glm.D93$linear.predictor

# approximate model, this is the posterior mode of p(theta|y)
c(outnosim$thetahat)

# importance sampling on theta, gives E(theta|y)
c(outsim$thetahat)

## predictions on response scale
16 KFAS

# GLM
fitted(glm.D93)

# approximate model with backtransform, equals GLM
c(fitted(outnosim))

# importance sampling on exp(theta)
fitted(outsim)

# prediction variances on link scale
# GLM
as.numeric(predict(glm.D93,type='link',se.fit=TRUE)$se.fit^2)

# approx, equals to GLM results
c(outnosim$V_theta)
    # importance sampling on theta
    c(outsim$V_theta)
# prediction variances on response scale
# GLM
as.numeric(predict(glm.D93,type='response',se.fit=TRUE)$se.fit^2)
    # approx, equals to GLM results
    c(outnosim$V_mu)
# importance sampling on theta
c(outsim$V_mu)
    ## Not run:
    data(sexratio)
    model&lt;-SSModel(Male~SSMtrend(1,Q=list(NA)),u=sexratio[,'Total'],data=sexratio,
    distribution='binomial')
    fit&lt;-fitSSM(model,inits=-15,method='BFGS',control=list(trace=1,REPORT=1))
    fit$model$Q #1.107652e-06

# Computing confidence intervals in response scale
# Uses importance sampling on response scale (4000 samples with antithetics)
pred&lt;-predict(fit$model,type='response',interval='conf',nsim=1000)
    ts.plot(cbind(model$y/model$u,pred),col=c(1,2,3,3),lty=c(1,1,2,2))

# Now with sex ratio instead of the probabilities:
imp&lt;-importanceSSM(fit$model,nsim=1000,antithetics=TRUE)
    sexratio.smooth&lt;-numeric(length(model$y))
sexratio.ci&lt;-matrix(0,length(model$y),2)
    w&lt;-imp$w/sum(imp$w)
    for(i in 1:length(model$y)){
sexr&lt;-exp(imp$sample[i,1,])
sexratio.smooth[i]&lt;-sum(sexr*w)
oo&lt;-order(sexr)
sexratio.ci[i,]&lt;-c(sexr[oo][which.min(abs(cumsum(w[oo]) - 0.05))],
+ sexr[oo][which.min(abs(cumsum(w[oo]) - 0.95))])
}

# Same by direct transformation:
out&lt;-KFS(fit$model,smoothing='signal',nsim=1000)
    KFS 17
    sexratio.smooth2 &lt;- exp(out$thetahat)
sexratio.ci2&lt;-exp(c(out$thetahat)
    + qnorm(0.025) * sqrt(drop(out$V_theta))%o%c(1, -1))
ts.plot(cbind(sexratio.smooth,sexratio.ci,sexratio.smooth2,sexratio.ci2),
col=c(1,1,1,2,2,2),lty=c(1,2,2,1,2,2))

## End(Not run)
# Example of Cubic spline smoothing
## Not run:
require(MASS)
data(mcycle)
model&lt;-SSModel(accel~-1+SSMcustom(Z=matrix(c(1,0),1,2),
T=array(diag(2),c(2,2,nrow(mcycle))),
Q=array(0,c(2,2,nrow(mcycle))),
P1inf=diag(2),P1=diag(0,2)),data=mcycle)
model$T[1,2,]&lt;-c(diff(mcycle$times),1)
model$Q[1,1,]&lt;-c(diff(mcycle$times),1)^3/3
model$Q[1,2,]&lt;-model$Q[2,1,]&lt;-c(diff(mcycle$times),1)^2/2
    model$Q[2,2,]&lt;-c(diff(mcycle$times),1)
    updatefn&lt;-function(pars,model,...){
    model$H[]&lt;-exp(pars[1])
    model$Q[]&lt;-model$Q[]*exp(pars[2])
    model
    }
    fit&lt;-fitSSM(model,inits=c(4,4),updatefn=updatefn,method=""BFGS"")
    pred&lt;-predict(fit$model,interval=""conf"",level=0.95)
plot(x=mcycle$times,y=mcycle$accel,pch=19)
lines(x=mcycle$times,y=pred[,1])
    lines(x=mcycle$times,y=pred[,2],lty=2)
lines(x=mcycle$times,y=pred[,3],lty=2)
## End(Not run)
</code></pre>

<p>The time-series data is:</p>

<pre><code>Time, 2.4, 2.6, 3.2, 3.6, 4, 6.2, 6.6, 6.8, 7.8, 8.2, 8.8, 8.8, 9.6, 10, 10.2, 10.6, 11, 11.4, 13.2, 13.6, 13.8, 14.6, 14.6, 14.6, 14.6, 14.6, 14.6, 14.8, 15.4, 15.4, 15.4, 15.4, 15.6, 15.6, 15.8, 15.8, 16, 16, 16.2, 16.2, 16.2, 16.4, 16.4, 16.6, 16.8, 16.8, 16.8, 17.6, 17.6, 17.6, 17.6, 17.8, 17.8, 18.6, 18.6, 19.2, 19.4, 19.4, 19.6, 20.2, 20.4, 21.2, 21.4, 21.8, 22, 23.2, 23.4, 24, 24.2, 24.2, 24.6, 25, 25, 25.4, 25.4, 25.6, 26, 26.2, 26.2, 26.4, 27, 27.2, 27.2, 27.2, 27.6, 28.2, 28.4, 28.4, 28.6, 29.4, 30.2, 31, 31.2, 32, 32, 32.8, 33.4, 33.8, 34.4, 34.8, 35.2, 35.2, 35.4, 35.6, 35.6, 36.2, 36.2, 38, 38, 39.2, 39.4, 40, 40.4, 41.6, 41.6, 42.4, 42.8, 42.8, 43, 44, 44.4, 45, 46.6, 47.8, 47.8, 48.8, 50.6, 52, 53.2, 55, 55, 55.4, 57.6                                                                                                
mcycle, 0, -1.3, -2.7, 0, -2.7, -2.7, -2.7, -1.3, -2.7, -2.7, -1.3, -2.7, -2.7, -2.7, -5.4, -2.7, -5.4, 0, -2.7, -2.7, 0, -13.3, -5.4, -5.4, -9.3, -16, -22.8, -2.7, -22.8, -32.1, -53.5, -54.9, -40.2, -21.5, -21.5, -50.8, -42.9, -26.8, -21.5, -50.8, -61.7, -5.4, -80.4, -59, -71, -91.1, -77.7, -37.5, -85.6, -123.1, -101.9, -99.1, -104.4, -112.5, -50.8, -123.1, -85.6, -72.3, -127.2, -123.1, -117.9, -134, -101.9, -108.4, -123.1, -123.1, -128.5, -112.5, -95.1, -81.8, -53.5, -64.4, -57.6, -72.3, -44.3, -26.8, -5.4, -107.1, -21.5, -65.6, -16, -45.6, -24.2, 9.5, 4, 12, -21.5, 37.5, 46.9, -17.4, 36.2, 75, 8.1, 54.9, 48.2, 46.9, 16, 45.6, 1.3, 75, -16, -54.9, 69.6, 34.8, 32.1, -37.5, 22.8, 46.9, 10.7, 5.4, -1.3, -21.5, -13.3, 30.8, -10.7, 29.4, 0, -10.7, 14.7, -1.3, 0, 10.7, 10.7, -26.8, -14.7, -13.3, 0, 10.7, -14.7, -2.7, 10.7, -2.7, 10.7
</code></pre>
"
"0.0400641540107502","0.0390434404721515","133320","<p>I am using logistic regression to solve the classification problem.</p>

<pre><code>g = glm(target ~ ., data=trainData, family = binomial(""logit""))
</code></pre>

<p>There are two classes (target): 0 and 1 </p>

<p>When I run the prediction function, it returns probabilities.</p>

<pre><code>p = predict(g, testData, type = ""response"")
</code></pre>

<p>However, it is not clear to me how to understand which class has been assigned?</p>

<pre><code>Real  p 

1   0.17568578
1   0.41698474
1   0.19151927
1   0.25587242
1   0.25604452
0   0.39976069
0   0.39910282
0   0.16879320
</code></pre>

<p>I appreciate if someone can explain me how this works based on the above example. Thanks</p>
"
"0.0895861718290583","0.0873037869711973","133387","<p>I'm trying to create a prediction model for estimation of continuous variable based on about 35 Independent variables.My data set has circa 27k observartions.
Here is the summary of the the targeted continuous variable:</p>

<pre><code>              Frequency Percent
(0,5]              2706  10.053
(5,10]             5226  19.415
(10,25]            4397  16.335
(25,100]           7142  26.533
(100,1e+03]        6465  24.018
(1e+03,1e+05]       981   3.645
Total             26917 100.000
</code></pre>

<p>I tried (by using R) Random Forest (RandomForest package),Linear regression, Conditional Inference Trees (ctree function in party package) but all of them have results that have a significant overestimation.
Here are the results of the prediction where I counted number of observations by thier distance from the actual values:
Any idea how can i balance the results?</p>

<p><img src=""http://i.stack.imgur.com/y70OM.png"" alt=""enter image description here""></p>

<p>Here are some views on the data:
The target variable is LTV for a user, I would like to predict LTV value after 180 days  based on users behavior of the first 7 days.
Here Is a summary fot the target variavle:</p>

<pre><code>  vars     n   mean     sd median trimmed   mad  min      max    range skew kurtosis   se
1    1 26917 178.35 622.29  33.49   66.63 39.28 0.03 22103.73 22103.71 14.1   325.08 3.79
</code></pre>

<p>UPDATE:
Here are the distributions of the targeted variable (first)and the prediction (secound)results:
<img src=""http://i.stack.imgur.com/b3MBs.png"" alt=""targeted variable"">
<img src=""http://i.stack.imgur.com/3N7d1.png"" alt=""prediction results based on the linear regression model that was the best""></p>
"
"0.132877766396671","0.129492442570703","134141","<p>Recently I am reading a paper where the authors use the GAM to make predictions. In brief, the data looks like following:</p>

<pre><code>  y    i    j     x    weekend
5.6    1    1   4.6    Mon.
6.5    1    2   5.6    Mon.
...
4.6    2    1   6.7    Sta.
2.4    2    2   1.2    Sta.
...
</code></pre>

<p>where <code>y</code>, <code>x1</code>, <code>x2</code> are continuous numbers, <code>weekend</code> is the day of the week. In the paper, the authors use the following formula:  </p>

<p>$$y_{ij} = \beta_0 + b_{0i} + \beta_1{\rm weekend}_i + f_1(x_{ij}, {\rm weekend}_i) + \varepsilon_{ij}$$</p>

<p>In the formula, $\beta_0$ is the overall mean, $b_{0i}$ is the random intercept, ${\rm weekend}_i$ determines whether it is weekday or weekend. Ans so I transform ${\rm weekend}$ from {Mon., Thu., .., Sun.} into {0, 1}. And $f_1$ is cubic regression function with 17 spline knots, and in fact will generate two smooth functions one for weekday, another for weekend.</p>

<p>I want to use following code:  </p>

<pre><code>gam(y~ s(i,bs=""re"") + weekend + s(x, by=weekend, bs=""cr"", k=17))
</code></pre>

<p>But I'm not sure whether it fits the formula or not. My questions are:</p>

<ol>
<li><code>gam</code> will automatically generate the mean of the model, so there is no need to specify a $\beta_0$ in the code?  </li>
<li>Is it right that by using <code>s(i,bs=""re"")</code>, the <code>gam</code> will calculate different random effect with distribution $N(0, \delta_i)$ for every $i$ specifically?</li>
<li>Is it good to transform weekend into 0-1 value? and in the code <code>s(x, by=weekend, bs=""cr"", k=17)</code>, does the <code>by</code> keyword mean that it will generate different smooth functions of <code>x</code> for different <code>weekend</code> value?</li>
<li>The last question is that without specifying <code>knots=list()</code>, as in the above code, the default behaviour of the model is to put knot points evenly of the range of value?</li>
</ol>
"
"0.132877766396671","0.129492442570703","134803","<p>Suppose we have a system that essentially evolves as follows:</p>

<pre><code>stock_t+1  = stock_t + inflows_t - outflows_t 
inflows_t  = a1*predictor11_t + a2*predictor12_t+.... error1_t
outflows_t = b1*predictor21_t + b2*predictor22_t+.... error2_t
</code></pre>

<p>I have observations for each of these variables, i.e. I can observe the stock, the inflows, outflows, and the sum of the flows (they add up correctly), as well as each of the predictors. All variables are time series, and simple time series analysis goes a long way. That said, while the stock is not overly volatile and relatively easy to predict, the flow variables are much more volatile and more challenging to model.</p>

<p>Using regression analysis, in the beginning I've only attempted to model the evolution of the stock using a combination of the flow predictors. I found that the best predictors of the stock are close to the best predictors of the flows (though not identical, i.e. some additional transformation is required). Recently, I have also attempted to model the flows. </p>

<p>The trouble is - not unexpectedly, I should say - that using the best regression models for each of the series, the system does not follow the add-up constraint; or put differently: if I calculate the evolution of the stock using the first equation, based on a starting value and equations 2-3, then my prediction of the stock is quite a bit different than my direct forecast of the stock. At the same time, since the stock model is the model that I have the most faith in, I'd rather not move away from these predictions.</p>

<p>So I was thinking that there surely must be a way to model the entire system directly, rather than estimate each equation separately. Right now a state-space approach comes to mind; before I go off into that direction, though, I am wondering whether I am missing something and whether anyone has a different suggestion.</p>

<p>PS I'm using R</p>
"
"0.120192462032251","0.117130321416455","135258","<p>I'm doing a linear regression using the h2o deep learning interface with R.  I'm comparing the predictions to the ones I'm getting from the randomForest R module.  The predictions from randomForest seem to roughly match up with what I'd expect given the distribution of the variable that I'm trying to predict.  However, the predictions from the h2o deep learning module don't seem to match up with what I'd expect.  Here's the summary from the variable I'm trying to predict:</p>

<pre><code>Profit            
Min.   :-1438.56  
1st Qu.: -133.80  
Median :   -0.59  
Mean   :   19.54  
3rd Qu.:  127.39  
Max.   :  508.41 
</code></pre>

<p>and here are the predictions from h2o deep learning:</p>

<pre><code>preddp             
Min.   :-0.079954  
1st Qu.:-0.017919  
Median :-0.010088  
Mean   :-0.011903  
3rd Qu.:-0.003921  
Max.   : 0.060259
</code></pre>

<p>and here are the predictions from randomForest:</p>

<pre><code>Min.       1st Qu.  Median   Mean     3rd Qu. Max. 
-212.500   -6.346   20.530   24.690   50.600  244.700
</code></pre>

<p>which is roughly what I'd expect.  Why are the h2o deep learning predictions so strange?  Also the correction between the predictions and the target variable are strongly positive with randomForest and even more strongly negative with h2o deep learning so I figure I have to be doing something wrong.</p>

<p>Here's the command I'm using to train the deep learning model:</p>

<pre><code>sdmodel.deep &lt;- h2o.deeplearning(columnIndices, 7, df, classification=FALSE)
</code></pre>
"
"0.0400641540107502","0.0390434404721515","136537","<p>I'm working on classifying models for a few different projects.  Several papers on the subject of calibration all suggest using isotonic regression (using PAV) to adjust the model probabilities.</p>

<p>I like the proposed calibration step, but am unsure how to apply it to NEW predictions from the model It appears as if the tools in both R an Python will happily calibrate probabilities if you also provide the true labels.</p>

<p>How can I then apply this to new data where the true labels are unknown.</p>

<p>Thanks!</p>
"
"0.0895861718290583","0.0873037869711973","136763","<p><img src=""http://i.stack.imgur.com/enCPt.png"" alt=""Data is a subset of my original DF. Snip of it is attached""></p>

<p>I use R, Party package in order to fit prediction model (""classifier"") for </p>

<p>""Converted.clicks"" as response variable.</p>

<p>The rest of vars are used as explaining variables in the model.</p>

<p>Here is the relevant part of my code:</p>

<pre><code>table(DF$Converted.clicks)

""0"" = 31456              
""1"" =  39  
""2"" =  6


Formula&lt;-Converted.clicks ~ Day.of.week 
                          + Device
                          + Keyword 
                          + Quality.score
                          + Network..with.search.partners. 
                          + Ad.group
                          + Match.type

ct&lt;-ctree(Formula,data=DF) 

####################################### 
</code></pre>

<h3>Issue:</h3>

<p>The Converted.clicks variable is highly imbalanced.The majority of the observations has </p>

<p>class ""zero"". So after ctree function is applied,all the predictions are ""zero"",there are </p>

<p>no classes ""1"" and ""2"" predicted.</p>

<h3>My questions are:</h3>

<ol>
<li><p>Is the classifier Decision Tree model is appropriate model to predict </p>

<p>as.factor(DF$Converted.clicks)?</p></li>
<li><p>If so, how can I balance the response var (i.e.to give the chance the two rest classes</p>

<p>""1"" and ""2"" to be predicted?) - if I need to use weights, I need an        </p>

<p>example,please.</p></li>
<li><p>Is there any other appropriate model to predict # of Converted.clicks? I understand </p>

<p>that Regression Decision Tree is only for continuous response variable, but in my case   </p>

<p>I have an integer response var, please advise.</p></li>
</ol>
"
"0.155167801263525","0.151214594727283","138424","<p>My data is binary with two linear independent variables.  For both predictors, as they get bigger, there are more positive responses.  I have plotted the data in a heatplot showing density of positive responses along the two variables.  There are the most positive responses in the top right corner and negative responses in the bottom left, with a gradient change visible along both axes.</p>

<p>I would like to plot a line on the heatplot showing where a logistic regression model predicts that positive and negative responses are equally likely.  (My model is of the form <code>response~predictor1*predictor2+(1|participant)</code>.)</p>

<p>My question: How can I figure out the line based on this model at which the positive response rate is 0.5?</p>

<p>I tried using predict(), but that works the opposite way; I have to give it values for the factor rather than giving the response rate I want.  I also tried using a function that I used before when I had only one predictor (<code>function(x) ((log(x/(1-x)))-fixef(fit)[1])/fixef(fit)[2]</code>), but I can only get single values out of that, not a line, and I can only get values for one predictor at a time.</p>

<p>I am using R.</p>

<p>Edit: I have added a contour plot over the heat plot (using geom_contour in ggplot2), which produces this:</p>

<p><img src=""http://i.stack.imgur.com/qObZc.png"" alt=""Each cell represents the frequency of positive responses for a single stimulus.  I added the numbers for clarity.""></p>

<p>I'd like to have a line that actually predicts the cutoff point in a fine-grained way; right now for the independent variables I have stimuli at points 40, 45, 50, etc. but I would like to see a line that predicts, e.g., that when x=32 and y=36 that's the threshold for 50% positive responses.  It could be a curve or it could even be a straight line (whose slope might help visualise the relative contributions of the two factors), but I'm not looking for a pure description of the cells which are >50 vs &lt;50, which is what I think this is doing, I'm looking for a way to plot the regression's predictions.</p>
"
"0.128491146685005","0.146087177447694","138506","<p>I have created regression models using robust regression - in particular, LTS and MM-estimators (using the R package robustbase).  I am now looking to creation prediction intervals.</p>

<p>The standard formula for prediction intervals for linear regression is:
$$
\hat{y_0} \pm t_{\alpha/2, n-p} \sqrt{\hat{\sigma^2}(1+x_0'(X'X)^{-1}x_0)}
$$
(see Montgomery and Peck, Introduction to Linear Regression Analysis, 1992)</p>

<p>For robust regression, obviously, the term $x_0'(X'X)^{-1}x_0$ cannot be used.  The Hat Matrix is different due to the weights.  It would seem to me that we can instead use the Hat Matrix modified with the weights:</p>

<p>$$
x_0'(X'WX)^{-1}x_0
$$</p>

<p>(see page 44 of the PhD thesis by Christopher Assaid at Virigina Tech)(<a href=""http://scholar.lib.vt.edu/theses/available/etd-3649212139711101/unrestricted/Ch6.PDF"" rel=""nofollow"">http://scholar.lib.vt.edu/theses/available/etd-3649212139711101/unrestricted/Ch6.PDF</a>)</p>

<p>We can approximate $\hat{\sigma^2}$ from the data as
$$
\hat{\sigma^2} = \frac{1}{df}\sum e_i^2
$$
where $df$ are the number of degrees of freedom.</p>

<p>I have three questions on this formulation for prediction intervals:</p>

<p>Is my formula correct?  Is the simple adjustment by factoring in the Weight Matrix enough to adapt the OLS formula for prediction intervals to robust regression.</p>

<p>If it is correct, does it apply to <em>all</em> types of robust regression, or just a subset?</p>

<p>Is it correct to estimate the variance as above?  If so, it would seem to me that  robust regression will always have a larger variance, and thus larger prediction intervals, than OLS.  The reason is is that OLS, by definition, is set up to minimize the residual sum of squares.  Robust regression, on the other hand, by definition of down-weighting potential outliers, even though it may give an overall better fit, will see a larger net residual sum of squares because of the contribution of the squared residuals from the outlier points.  Consequently, since the length of the interval is $\sqrt{\hat{\sigma}^2(1+\delta)}$ (where granted $\delta$ is not necessarily small, but the interval is always $\sigma$ plus something), if $\hat{\sigma}_{RR} &gt; \hat{\sigma}_{OLS}$, in general, the length of the interval for RR will be larger.  It seems counter-intuitive to me that if data is fit with both OLS and robust regression and prediction intervals are made, those from OLS will be by definition narrower and may even be contained within the robust ones.  It seems to thus minimize the power of robust regression.</p>

<p>Any answers to these questions or other suggestion/advice on creating prediction intervals for LTS and MM regression would be appreciated.</p>
"
"0.0693931503088838","0.0676252226000574","139653","<p>Original post on stackoverflow:
<a href=""http://stackoverflow.com/questions/28773153/how-to-do-regression-model-selection-if-dummy-variables-are-involved"">http://stackoverflow.com/questions/28773153/how-to-do-regression-model-selection-if-dummy-variables-are-involved</a></p>

<p>I am trying to do a logistic regression analysis in R with two continuous explanatory variables and six other explanatory categorical variables, and find a good regression model to do predictions. When I do step-wise model selections, there are always some levels of certain categorical variables identified as insignificant. I am just wondering how should I deal with this situation. Should I simply drop these levels, or I should force the program to keep all levels of the categorical variables and try to drop the relatively insignificant variables?</p>

<p>Thanks a lot!</p>
"
"0.166111588084","0.161879566940362","140600","<p>I'm running a fixed effects logistic regression in R. The model consists of a binary outcome and two binary predictors, with no interaction term. On the log-odds scale, and as an odds-ratio, the coefficient for one of the predictors (<code>carbf</code> in the mocked-up example below) indicates that the expected probability of Y=1 (""success"") is different between the two levels of the factor (i.e., the effect is significant). </p>

<p>When I use the <code>effects</code> package to get marginal predicted probabilities, the 95% CIs for the two levels of <code>carbf</code> overlap considerably, indicating there is no evidence of a difference in the expected probability of Y=1 between the two factor levels.</p>

<p>When I use the <code>mfx</code> package to get average marginal effects for the coefficients (i.e., for the expected <em>difference</em> in the probability of Y=1 between the two factor levels), I do get a significant difference.</p>

<p><strong>I'm confused as to whether this discrepancy is because:</strong> </p>

<p><strong>1) the output from the model and the <code>mfx</code> package is an expected <em>difference</em> in the probability of Y=1 between factor levels, rather than predicted probabilities for each level.</strong></p>

<p><strong>2) of the way the <code>effects</code> package is calculating the marginal effect.</strong> </p>

<p>In an effort to determine this, I modified the source code from the <code>mfx</code> package to give me average marginal effects for each level of the <code>carbf</code> factor. The 95% CIs for these predictions <em>do not</em> overlap, indicating a significant difference. This makes me wonder why I get such different results using the <code>effects</code> package. Or is it that I'm just confused about the difference between marginal effects for coefficients and for predicted probabilities?</p>

<pre><code>#####################################
# packages
library(effects)
library(mfx)
library(ggplot2)

# data
data(mtcars)
carsdat &lt;- mtcars
carsdat$carb &lt;- ifelse(carsdat$carb %in% 1:3, 0, 1)
facvars &lt;- c(""vs"", ""am"", ""carb"")
carsdat[, paste0(facvars, ""f"")] &lt;- lapply(carsdat[, facvars], factor)

# model
m1 &lt;- glm(vsf ~ amf + carbf, 
    family = binomial(link = ""logit""), 
    data = carsdat)
summary(m1)


#####################################
# effects package
eff &lt;- allEffects(m1)
plot(eff, rescale.axis = FALSE)
eff_df &lt;- data.frame(eff[[""carbf""]])
eff_df 

#   carbf   fit    se  lower upper
# 1     0 0.607 0.469 0.3808 0.795
# 2     1 0.156 0.797 0.0375 0.469


#####################################
# mfx package marginal effects (at mean)
mfx1 &lt;-logitmfx(vsf ~ amf + carbf, data = carsdat, atmean = TRUE, robust = FALSE)
mfx1 

#         dF/dx Std. Err.     z  P&gt;|z|
# amf1    0.217     0.197  1.10 0.2697
# carbf1 -0.450     0.155 -2.91 0.0037

# mfx package marginal effects (averaged)
mfx2 &lt;-logitmfx(vsf ~ amf + carbf, data = carsdat, atmean = FALSE, robust = FALSE)
mfx2

#         dF/dx Std. Err.     z  P&gt;|z|
# amf1    0.177     0.158  1.12 0.2623
# carbf1 -0.436     0.150 -2.90 0.0037


#####################################
# mfx source code
fit &lt;- m1
x1 = model.matrix(fit)  
be = as.matrix(na.omit(coef(fit)))
k1 = length(na.omit(coef(fit)))
fxb = mean(plogis(x1 %*% be)*(1-plogis(x1 %*% be))) 
vcv = vcov(fit)

# data frame for predictions
mfx_pred &lt;- data.frame(mfx = rep(NA, 4), se = rep(NA, 4), 
    row.names = c(""amf0"", ""amf1"", ""carbf0"", ""carbf1""))
disc &lt;- rownames(mfx_pred)

# hard coded prediction estimates and SE  
disx0c &lt;- disx1c &lt;- disx0a &lt;- disx1a &lt;- x1 
disx1a[, ""amf1""] &lt;- max(x1[, ""amf1""]) 
disx0a[, ""amf1""] &lt;- min(x1[, ""amf1""]) 
disx1c[, ""carbf1""] &lt;- max(x1[, ""carbf1""]) 
disx0c[, ""carbf1""] &lt;- min(x1[, ""carbf1""])
mfx_pred[""amf0"", 1] &lt;- mean(plogis(disx0a %*% be))
mfx_pred[""amf1"", 1] &lt;- mean(plogis(disx1a %*% be))
mfx_pred[""carbf0"", 1] &lt;- mean(plogis(disx0c %*% be))
mfx_pred[""carbf1"", 1] &lt;- mean(plogis(disx1c %*% be))
# standard errors
gr0a &lt;- as.numeric(dlogis(disx0a %*% be)) * disx0a
gr1a &lt;- as.numeric(dlogis(disx1a %*% be)) * disx1a
gr0c &lt;- as.numeric(dlogis(disx0c %*% be)) * disx0c
gr1c &lt;- as.numeric(dlogis(disx1c %*% be)) * disx1c
avegr0a &lt;- as.matrix(colMeans(gr0a))
avegr1a &lt;- as.matrix(colMeans(gr1a))
avegr0c &lt;- as.matrix(colMeans(gr0c))
avegr1c &lt;- as.matrix(colMeans(gr1c))
mfx_pred[""amf0"", 2] &lt;- sqrt(t(avegr0a) %*% vcv %*% avegr0a)
mfx_pred[""amf1"", 2] &lt;- sqrt(t(avegr1a) %*% vcv %*% avegr1a)
mfx_pred[""carbf0"", 2] &lt;- sqrt(t(avegr0c) %*% vcv %*% avegr0c)
mfx_pred[""carbf1"", 2] &lt;- sqrt(t(avegr1c) %*% vcv %*% avegr1c)  

mfx_pred$pred &lt;- rownames(mfx_pred)
    mfx_pred$lcl &lt;- mfx_pred$mfx - (mfx_pred$se * 1.96)
mfx_pred$ucl &lt;- mfx_pred$mfx + (mfx_pred$se * 1.96)

#          mfx    se   pred     lcl   ucl
# amf0   0.366 0.101   amf0  0.1682 0.563
# amf1   0.543 0.122   amf1  0.3041 0.782
# carbf0 0.601 0.107 carbf0  0.3916 0.811
# carbf1 0.165 0.105 carbf1 -0.0412 0.372

ggplot(mfx_pred, aes(x = pred, y = mfx)) +
    geom_point() +
    geom_errorbar(aes(ymin = lcl, ymax = ucl)) +
    theme_bw()
</code></pre>
"
"0.105999788000636","0.103299233817667","141552","<p>I have some data that I fitted using a LOESS model in R, giving me this:</p>

<p><img src=""http://i.stack.imgur.com/JfYFZ.png"" alt=""enter image description here""></p>

<p>The data has one predictor and one response, and it is heteroscedastic.</p>

<p>I also added confidence intervals. The problem is that the intervals are confidence intervals for the line, whereas I am interested in the prediction intervals. For example, the bottom panel is more variable then the top panel, but this is not captured in the intervals.</p>

<p>This question is slightly related:
<a href=""http://stats.stackexchange.com/questions/82603/understanding-the-confidence-band-from-a-polynomial-regression"">Understanding the confidence band from a polynomial regression</a>, especially the answer by @AndyW, however in his example he uses the relatively straightforward <code>interval=""predict""</code> argument that exists in <code>predict.lm</code>, but it is absent from <code>predict.loess</code>.</p>

<p>So I have two very related questions:</p>

<ol>
<li>How do I get the pointwise prediction intervals for LOESS?</li>
<li>How can I predict values that will capture that interval, i.e. generate a bunch of random numbers that will eventually look somewhat like the original data?</li>
</ol>

<p>It is possible that I don't need LOESS and should use something else, but I am unfamiliar with my options. Basically it should fit the line using local regression or multiple linear regression, giving me error estimates for the lines, and in addition also different variances for different explanatory variables, so I can predict the distribution of the response variable (y) at certain x values.</p>
"
"0.0908569611434023","0.103299233817667","141603","<p>So I'm playing around with logistic regression in R, using the mtcars dataset, and I decide to create a logistic regression model on the 'am' parameter (that is manual or automatic transmission for those of you familiar with the mtcars-dataset).</p>

<pre><code>Call:
glm(formula = am ~ mpg + qsec + wt, family = binomial, data = mtcars)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-4.484e-05  -2.100e-08  -2.100e-08   2.100e-08   5.163e-05  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)    924.89  883764.07   0.001    0.999
mpg             20.65   18004.32   0.001    0.999
qsec           -55.75   32172.52  -0.002    0.999
wt            -111.33  103183.48  -0.001    0.999

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 4.3230e+01  on 31  degrees of freedom
Residual deviance: 6.2903e-09  on 28  degrees of freedom
AIC: 8

Number of Fisher Scoring iterations: 25
</code></pre>

<p>Now, at first sight this looks like a terrible regression, right? The standard errors are HUGE, the z-values are all close to zero and the corresponding probabilities are all close to one. HOWEVER, the residual deviance is extremely small! </p>

<p>I decide to check how well the model does as a classification model by running:</p>

<pre><code>pred &lt;- predict(logit_fit, data.frame(qsec = mtcars$qsec, wt = mtcars$wt, mpg = mtcars$mpg), type = ""response"") # Make a prediction of the probabilities on our data
mtcars$pred_r &lt;- round(pred, 0) # Round probabilities to closest 0 or 1
table(mtcars$am, mtcars$pred_r) # Check if results of classification is any good.
</code></pre>

<p>Indeed, the model perfectly predicts the data:</p>

<pre><code>     0  1
  0 19  0
  1  0 13
</code></pre>

<p>Have I completely misunderstood how to interpret model data? Am I overfitting massively or what's going on here? What's going on?</p>
"
"0.0566592699670073","0.0552157630374233","141719","<p>I am using the package <code>caret</code> and GBM method for my predictions.</p>

<pre><code>fitControl &lt;- trainControl(## 10-fold CV
        method = ""repeatedcv"",
        number = 10,
        ## repeated ten times
        repeats = 10)

gbmGrid &lt;-  expand.grid(interaction.depth = c(1, 5, 9),
                        n.trees = (1:30)*50,
                        shrinkage = 0.1)

gbmFit &lt;- train(target ~ ., data = traindf,
                method = ""gbm"",
                trControl = fitControl,
                verbose = FALSE,
                ## Now specify the exact models 
                ## to evaludate:
                tuneGrid = gbmGrid,
                metric = ""ROC"")
</code></pre>

<p>There is one concept that I misunderstand. User guides of <code>caret</code> say that ""<strong>By default, the train function chooses the model with the largest performance value (or smallest, for mean squared error in regression models).</strong>"" So, when I run <code>ggplot(gbmFit)</code> I get this graphic:</p>

<p><img src=""http://i.stack.imgur.com/mETe8.png"" alt=""enter image description here""></p>

<p>When I type <code>gbmFit</code> in the console, I see that ""<strong>The final values used for the model were n.trees = 300, interaction.depth = 9 and shrinkage = 0.1</strong>""  How can I manually change these settings in order to make my predictions with different number of boosting iterations and trees?:</p>

<pre><code>predictions_gbm &lt;- predict(gbmFit, newdata = testdf, type = ""raw"")
</code></pre>
"
"NaN","NaN","142248","<p>When Performing a linear regression in <code>r</code> I came across the following terms. </p>

<pre><code> NBA_test =read.csv(""NBA_test.csv"")
 PointsPredictions  = predict(PointsReg4, newdata =  NBA_test)
 SSE = sum((PointsPredictions - NBA_test$PTS)^2)
     SST = sum((mean(NBA$PTS) - NBA_test$PTS) ^ 2)
 R2 = 1- SSE/SST
</code></pre>

<p>In this case I am predicting the number of points. I understood what is meant by SSE(sum of squared errors), but what actually is SST and R square? Also what is the difference between R2 and RMSE?</p>
"
"0.0400641540107502","0.0390434404721515","142312","<p>I'm having an interesting dilemma with the <code>neuralnet</code> and <code>nnet</code> packages in <code>R</code>.  I recently tried a series of feed-forward neural networks giving each the same data sets and every single time, no matter how I tweak the algorithms, hidden layers, neuron sizes, maximum iterations or error thresholds, both functions keep converging their predictions to approximately the mean of whatever they are training on.</p>

<p>A linear regression does way better for each series in terms of fit, and both of these packages seem to do a better job fitting random data from the <code>rnorm</code> function than real data.  In regards to the mathematics of the problem, what could be causing this and how should I resolve?  I have sample code below and can paste a sample dataset below if requested.  Thanks!</p>

<pre><code>model6 &lt;- neuralnet(
    target ~ 1 + majorholiday + mon + sat + sun + thu + tue + wed + tickets + l1_target + l7_target, data = data_nn
    ,algorithm = ""rprop+"", hidden = c(8), stepmax = 500000
    ,err.fct = ""sse"", threshold = 0.01, lifesign = ""full"", lifesign.step = 100
    , linear.output= T)
</code></pre>

<p><strong>EDIT</strong></p>

<p>A user requested I paste some data.  Here is one set below and I just tried the same code again prior to uploading and the same thing happens, converges to the mean of <code>target</code> at about 17.45</p>

<pre><code>    row.names   target  majorholiday    mon sat sun thu tue wed backtickets l1_target   l7_target
1   8   18.976573088    0   0   0   0   0   0   0   13806   18.114001584    36.521334684
2   9   20.701716096    0   1   0   0   0   0   0   15308   18.976573088    35.477867979
3   10  25.014573616    0   0   1   0   0   0   0   13439   20.701716096    28.173601042
4   11  15.706877377    1   0   0   0   0   0   0   11283   25.014573616    27.602288128
5   12  19.633596721    0   0   0   0   1   0   0   12272   15.706877377    13.801144064
6   13  20.049395337    0   0   0   0   0   1   0   9528    19.633596721    32.777717152
7   14  21.720178282    0   0   0   1   0   0   0   13747   20.049395337    18.114001584
8   15  23.390961226    0   0   0   0   0   0   0   15277   21.720178282    18.976573088
9   16  16.707829447    0   1   0   0   0   0   0   16058   23.390961226    20.701716096
10  17  15.872437975    0   0   1   0   0   0   0   14218   16.707829447    25.014573616
11  18  23.295531996    1   0   0   0   0   0   0   11249   15.872437975    15.706877377
12  19  22.363710716    0   0   0   0   1   0   0   13993   23.295531996    19.633596721
13  20  24.227353276    0   0   0   0   0   1   0   13402   22.363710716    20.049395337
14  21  20.500068156    0   0   0   1   0   0   0   14244   24.227353276    21.720178282
15  22  26.090995836    0   0   0   0   0   0   0   14502   20.500068156    23.390961226
16  23  18.636425597    0   1   0   0   0   0   0   16296   26.090995836    16.707829447
17  24  15.840961757    0   0   1   0   0   0   0   13694   18.636425597    15.872437975
18  25  20.650050308    1   0   0   0   0   0   0   10774   15.840961757    23.295531996
19  26  13.467424114    0   0   0   0   1   0   0   12348   20.650050308    22.363710716
20  27  19.752222033    0   0   0   0   0   1   0   12936   13.467424114    24.227353276
21  28  27.832676502    0   0   0   1   0   0   0   14342   19.752222033    20.500068156
22  29  18.854393759    0   0   0   0   0   0   0   14390   27.832676502    26.090995836
23  30  10.773939291    0   1   0   0   0   0   0   16724   18.854393759    18.636425597
24  31  12.569595839    0   0   1   0   0   0   0   14091   10.773939291    15.840961757
25  32  28.153882107    1   0   0   0   0   0   0   11250   12.569595839    20.650050308
26  33  24.400031160    0   0   0   0   1   0   0   12803   28.153882107    13.467424114
27  34  21.584642949    0   0   0   0   0   1   0   13318   24.400031160    19.752222033
28  35  27.215419370    0   0   0   1   0   0   0   14193   21.584642949    27.832676502
29  36  21.584642949    0   0   0   0   0   0   0   14312   27.215419370    18.854393759
30  37  15.015403791    0   1   0   0   0   0   0   16445   21.584642949    10.773939291
31  38  26.276956633    0   0   1   0   0   0   0   13753   15.015403791    12.569595839
32  39  15.139500902    1   0   0   0   0   0   0   11619   26.276956633    28.153882107
33  40  12.467824272    0   0   0   0   1   0   0   14006   15.139500902    24.400031160
34  41  21.373413039    0   0   0   0   0   1   0   14098   12.467824272    21.584642949
35  42  8.015029889 0   0   0   1   0   0   0   14462   21.373413039    27.215419370
36  43  16.030059779    0   0   0   0   0   0   0   15367   8.015029889 21.584642949
37  44  19.592295285    0   1   0   0   0   0   0   17868   16.030059779    15.015403791
38  45  18.701736409    0   0   1   0   0   0   0   15052   19.592295285    26.276956633
39  46  16.002499062    1   0   0   0   0   0   0   10035   18.701736409    15.139500902
40  47  16.943822536    0   0   0   0   1   0   0   13708   16.002499062    12.467824272
41  48  11.295881691    0   0   0   0   0   1   0   13463   16.943822536    21.373413039
42  49  19.767792959    0   0   0   1   0   0   0   13998   11.295881691    8.015029889
43  50  19.767792959    0   0   0   0   0   0   0   14745   19.767792959    16.030059779
44  51  16.943822536    0   1   0   0   0   0   0   16156   19.767792959    19.592295285
45  52  14.119852113    0   0   1   0   0   0   0   13552   16.943822536    18.701736409
46  53  22.869570079    1   0   0   0   0   0   0   11554   14.119852113    16.002499062
47  54  10.481886286    0   0   0   0   1   0   0   13437   22.869570079    16.943822536
48  55  19.057975066    0   0   0   0   0   1   0   14076   10.481886286    11.295881691
49  56  20.010873819    0   0   0   1   0   0   0   14567   19.057975066    19.767792959
50  57  9.528987533 0   0   0   0   0   0   0   14277   20.010873819    19.767792959
51  58  21.916671326    0   1   0   0   0   0   0   16545   9.528987533 16.943822536
52  59  11.000000000    1   0   0   0   0   0   1   15599   21.916671326    14.119852113
53  60  17.000000000    0   0   0   0   1   0   1   17463   11.000000000    22.869570079
54  61  10.000000000    0   0   0   0   0   1   1   17935   17.000000000    10.481886286
55  62  20.000000000    0   0   0   1   0   0   1   18357   10.000000000    19.057975066
56  63  19.000000000    0   0   0   0   0   0   1   19246   20.000000000    20.010873819
57  64  17.000000000    0   1   0   0   0   0   1   21234   19.000000000    9.528987533
58  65  11.000000000    0   0   1   0   0   0   1   18493   17.000000000    21.916671326
59  66  9.000000000 1   0   0   0   0   0   1   15315   11.000000000    11.000000000
60  67  22.000000000    0   0   0   0   1   0   1   17841   9.000000000 17.000000000
61  68  9.000000000 0   0   0   0   0   1   1   18312   22.000000000    10.000000000
62  69  11.000000000    0   0   0   1   0   0   1   17880   9.000000000 20.000000000
63  70  5.000000000 0   0   0   0   0   0   1   19371   11.000000000    19.000000000
64  71  15.000000000    0   1   0   0   0   0   1   21696   5.000000000 17.000000000
65  72  12.000000000    0   0   1   0   0   0   1   18829   15.000000000    11.000000000
66  73  10.000000000    1   0   0   0   0   0   1   14749   12.000000000    9.000000000
67  74  15.000000000    0   0   0   0   1   0   1   17928   10.000000000    22.000000000
68  75  7.000000000 0   0   0   0   0   1   1   18254   15.000000000    9.000000000
</code></pre>
"
"0.0981367343026181","0.0956365069595007","142594","<p>I'm executing a few test runs of a lasso regression with the glmnet package in R using the diabetes dataset (<a href=""http://www4.stat.ncsu.edu/~boos/var.select/diabetes.tab.txt"" rel=""nofollow"">http://www4.stat.ncsu.edu/~boos/var.select/diabetes.tab.txt</a>). Iâ€™m choosing a single lambda value = 0.1 (not a sequence of lambda values) and alpha=1:</p>

<pre><code>&gt; set.seed(1);
  elasticnet_fit &lt;- glmnet(data_matrix, y, family=""gaussian"", lambda=.1, alpha=1); 
  coef(elasticnet_fit);
11 x 1 sparse Matrix of class ""dgCMatrix""
                       s0
(Intercept) -299.82915923
x1            -0.02102016
x2             5.63508548
x3             1.10288671
x4            -0.73514322
x5             0.42465094
x6            -0.03356997
x7             5.39641052
x8            59.76829212
x9             0.27508521
x10          -22.34877815
</code></pre>

<p>I noticed the glmnet documentation reads as follows when selecting lambda values:</p>

<blockquote>
  <p>WARNING: use with care. Do not supply a single value for lambda (for
  predictions after CV use predict() instead). Supply instead a
  decreasing sequence of lambda values. glmnet relies on its warms
  starts for speed, and its often faster to fit a whole path than
  compute a single fit.</p>
</blockquote>

<p>Are the coefficient estimates invalid when supplying only a single lambda value rather than a sequence? (Cross-posting from Stack Overflow.)</p>
"
"NaN","NaN","143328","<p>I am developing a logistic regression model where perfect variable separation occurs. I want to calculate a cutoff from this data. Interestingly, the length of the slot <code>cutoffs</code> of <code>pred.obj</code> is only 5, as well as the slots <code>fp</code>, <code>tp</code>, <code>tn</code>, <code>fn</code>, <code>n.pos.pred</code> and <code>n.neg.pred</code>. I expect it to have the same length as the observations. </p>

<p>Has anybody an explanation for this? (And knows how to solve it?) </p>

<p>MWE:</p>

<pre><code> library(ROCR) # package for prediction/performance functions
 y &lt;- c(0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0)
 x &lt;- c(-5, 5, 3, -2, 4, 3, -8, 2, 5, 3, -5, -3, -2)
 model &lt;- glm(as.factor(y) ~ x, family = ""binomial"")
 preds &lt;- predict(model, type = ""response"")
 (pred.obj &lt;- prediction(preds, y))
 perf &lt;- performance(pred.obj, ""acc"")
 (cutoff &lt;- perf@x.values[[1]][which.max(perf@y.values[[1]])])
</code></pre>
"
"0.120192462032251","0.117130321416455","143559","<p>I am working on a problem where my objective is to predict y given some features x1,x2,x3,...x8,x9 I solved this problem using some statistical and machine learning techniques like regression, trees, random forests &amp; svm. Now that I have a prediction for y, at a given x1,x2,x3..x6 I would like to achieve an optimal value of y, by changing some values of xn which are in my control. Let us say that y was predicted to be 5, however I need a value of 10. Can I put three features aside say x1,x2,x3 and get like a range or values for the aforementioned aside features such that the value of y is 10?</p>

<p>Basically, it is sort of like an inverse problem, where assuming I know the predictor I need to manipulate the features to increase the value of the predictor.</p>

<p>Reproducible example:</p>

<pre><code>    y&lt;- rnorm(100)
x1&lt;- sin(rpois(100))
x2&lt;- cos(rnorm(100))
x3&lt;- sin(rnorm(100))+ rnorm(100)* 3cos(rnorm(100))
x4&lt;- rnorm(100)
y.fit&lt;- lm(y~x1+x2+x3+x4)
library(caret)
y.rf&lt;- train(ROP~ .,data=training,method=""rf"",prox=TRUE)
</code></pre>

<p>So now that I have y.rf and y.fit, lets say i have control over the  values of x1 &amp; x2, hence I would like a given value of y say 0.5, and to achieve this value of y (0.5) at a fixed value of x3,x4 I would like a range for x1 and x2 or possible values for x1 &amp; x2.</p>

<p>How should I proceed?</p>
"
"0.0801283080215004","0.078086880944303","143943","<p>I have a need to do realtime predictions for individual rows of data based on a previously computed randomForest algorithm.  How can I run the ""predict"" command without recomputing ""fit"" on the entire training data set each time?  </p>

<p>I am using R and here's the line of code that computes ""fit"" by applying the randomForest algorithm on the training set.</p>

<pre><code>fit &lt;- randomForest(formula2, data=training, importance=TRUE, ntree=2000, na.action = na.omit)
</code></pre>

<p>And here's the predict command - I want to be able to run this without having to recompute fit every time.  Is this possible?</p>

<pre><code>outp_rf &lt;- predict(fit, testing)
</code></pre>

<p>For LogisticRegression, I know the coefficients so I can rerun the logistic function to compute the outcome.  However not sure how I can do it for RandomForest.</p>
"
"0.139198742242089","0.146087177447694","144348","<p><strong>Note</strong><br>
I've edited the example to be more intuitive and closer to my real data</p>

<p><strong>Intro</strong><br>
I've got data on customers purchases and with it am trying to predict which customers are more likely to make next purchase at some time in the future. Data consist of customers' features like sex, age etc., and their prior purchase behavior like total spendings and number of orders, one row for every customer. The last two columns are the indicator of wether he have made next purchase or not, and number of days till purchase or till today, in case of no purchase.  </p>

<p><strong>Problem</strong><br>
I am building a Cox regression and then want to predict probability of next purchase for individual observations in, say, 30 days from last purchase.</p>

<p>Reproducible example:</p>

<pre><code>library(survival)
library(rms)
library(pec)
library(ggplot2)

data(cost)

# split into train and test sets
set.seed(1)
ind &lt;- sample(1:nrow(cost), 100)
test.set &lt;- cost[ind, ]
train.set &lt;- cost[-ind, ]
</code></pre>

<p>For Cox regression I use <code>cph</code> from <code>rms</code> package, for prediction - <code>predictSurvProb</code> from <code>pec</code> package as suggested in <a href=""http://stats.stackexchange.com/a/36016/72401"">this</a> discussion.</p>

<pre><code># fit Cox model
fit &lt;- cph(Surv(time, status) ~ ., data = train.set, surv = TRUE)

# predict pobability of event in 30 days
test.set$predicted.probs &lt;- 1 - predictSurvProb(fit, newdata = test.set, times = 1000)[, 1]
</code></pre>

<p>Thus, for every customer we have his probability of making a purchase in 1000 units of time. I want to validate prediction against real data.  </p>

<p><strong>Now to the question:</strong> what is the best/valid way to do it?  </p>

<p>Here's what I've tried:<br>
I expect that valid model would predict higher probabilities for customers who made their purchase earlier so correlation between probabilities and number of days to event' would be negative and strong (e.g. for customer who actualy made next purchase in 2 days, probability of buying in 30 days would be very high).</p>

<pre><code>with(test.set, cor(predicted.probs, time))
# [1] -0.5221604
</code></pre>

<p>Also, probability for those who made purchase (status = 1) would be higher than for those who didn't.</p>

<pre><code>with(test.set, by(predicted.probs, status, mean))
# status: 0
# [1] 0.2371247
# --------------
#   status: 1
# [1] 0.4083586
</code></pre>

<p>And a graph to eyeball my assumptions:</p>

<pre><code>qplot(data = test.set, x = time, y = predicted.probs, color = time)
</code></pre>

<p>Am I correct in my reasoning?</p>
"
"0.151091386578686","0.16564728911227","144515","<p>My first question here, hope it works :) I have experimental data $(x_i,y_i),i=1\ldots n$, $n=O(10)$, which describe a function $y=f(x)+\epsilon$ defined in $I=[0,1]$. $y_i$ is a ratio between measured positive quantities: $y_i=z_i/w_i$, where $0 &lt; z_i \leq w_i \forall i$. For these reasons, $f$ satisfies the following constraints:</p>

<ol>
<li>$f(0)=1$</li>
<li>$f(1)=1$</li>
<li>$ 0 &lt; f(x) \leq 1 \forall x \in I $ </li>
</ol>

<p>EDIT1: Experimental data should satisfy constraints, but in a few cases $y_i$ > 1 (by 3% in one case, and by much less in few other cases). That's weird because I was told this couldn't happen, because of the way measures are taken...I could follow this up with the data reduction team, but I don't think it makes a huge difference.</p>

<p>The true $f$ should look like this:</p>

<p><img src=""http://i.stack.imgur.com/ccVgy.png"" alt=""enter image description here""></p>

<p>I would like to fit my data with a model which ideally satisfies the constraints. The goal is to make predictions for values of $x \in I$ (it doesn't make any physical sense to consider values $x&lt;0$ or $x&gt;1$). </p>

<p>I started with a least squares polynomial regression. Of course, that doesn't satisfy the constraints, in particular constraint 3, which makes using the curve to make predictions quite risky. I thought of 3 possible strategies:</p>

<ol>
<li>fit a model such as $1-x(1-x)g(x)+\epsilon$. For $g(x)$ is continuous in $I$, this model satisfy constraints 1 and 2: I have no idea how to satisfy constraint 3, though, and also I have no idea which model to assume for $g(x)$.</li>
<li>as $y_i$= $y_i=z_i/w_i$, with $0 &lt; z_i \leq w_i \forall i$, it may make sense to fit a rational function to my data, i.e., assume model
$y=\frac{1+\sum\limits_{i=1}^n \alpha_ix^i}{1+\sum\limits_{i=1}^m \beta_i(1-x)^i} + \epsilon $ which satisfies constraints 1 and 2. Again, no idea how to satisfy constraint 3, which values to use for $n$ and $m$, etc. (very low, of course, as I have very few experimental data.).</li>
<li>a variation on strategy 2 may be to fit two separate functions $g(x)$ and $h(x)$ respectively to $(x_i,z_i)$ and $(x_i,w_i)$, and then just compute the ratio. Maybe each function is better behaved, and polynomial regression may be sufficient.</li>
</ol>

<p>What do you suggest me to do? Do you have any better ideas? Thanks!</p>

<p>EDIT2: I provide a picture of one of my fits (here the model is a second degree polynomial). I cannot show the $y=0$ line in the picture. The bounds are 95% pointwise prediction bounds for the fit.</p>

<p><img src=""http://i.stack.imgur.com/Suaau.png"" alt=""enter image description here""></p>

<p>EDIT3: The first drawing is taken from a book. Similar drawings are shown in other books. They are based partly on theory (the behavior for $x \to 0$ and $x \to 1$), and partly on experimental data which the books don't report. I don't have access to the original papers. My data sets can be considered in broad agreement. Some are closer than others, but since my case is not exactly the same than that reported in books, I believe the agreement is fair. However, my fit doesn't look at all like the ""expected"" $f(x)$, and that's my problem.</p>

<p>As I stated already, the goal of the fit is to make predictions of $f(x)$ for  $0&lt;x&lt;1$.        </p>
"
"0.149906337799172","0.146087177447694","145684","<p>My problem (question at the end) is to calculate confidence interval (CI) (NOT prediction interval) of the response of a nonlinear model.</p>

<p>I am working with R but this question is not R-specific.</p>

<p>I want to model some data after the following equation (model):</p>

<p>Y ~ a * X^b/b</p>

<p>First, I estimate the parameters a and b through nonlinear regression (using R's ""nls()""), which yields estimates and error on the corresponding estimate.</p>

<pre><code> Nonlinear regression model
 model: Y ~ (A * X^B/B)
  data: data.frame(X = X, Y = Y)
    A      B 
  7.4154 0.6041 
   residual sum-of-squares: 88983
</code></pre>

<p>Then I calculate 95% CI for a and b (using confint(nlm &lt;- nls(Y ~  A * X^B/B, start=list(A=1,B=1)))</p>

<pre><code> &gt; confint(nlm)
 Waiting for profiling to be done...
         2.5%     97.5%
 A 1.21719414 11.549562
 B 0.08583486  1.482389
</code></pre>

<p>In order to calculate 95%CI for Y, given some fixed, certain value of X, my first idea was to propagate uncertainties on a and b to Y through the model equation. This yields some value for 95% CI of Y, given X.</p>

<p>I then came accross the ""propagate"" package that proposes to calculate 95%CI of Y, given X, ""based on asymptotic normality"" (citation from ""<a href=""http://127.0.0.1:22638/library/propagate/html/predictNLS.html"" rel=""nofollow"">http://127.0.0.1:22638/library/propagate/html/predictNLS.html</a>""). However this method yields a VERY different 95%CI. </p>

<p><strong>My question is: Why aren't these two CI equal ?</strong></p>

<p>A worked example (with some random equation that just crossed my mind):</p>

<p>Values needed for error propagation : A, CI(A), B, CI(B), X, CI(X) :</p>

<p>Parameters (A &amp; B)' estimates and 95%CI were calculated from 
     confint(nlm &lt;- nls(Y ~  A * X^B/B, start=list(A=1,B=1))</p>

<p>X was then fixed at 30 for the sake of the argument, and considered error-free.</p>

<pre><code>                A         B  X
 value   7.415380 0.6041404 30
 95% CI  5.166184 0.6982769  0
</code></pre>

<p>The general formula for uncertainties propagation (works for sd, se, ci95%) is :</p>

<p>Y=f(Ai | i = 1 to n)
=> delta(Y) = sqrt( sum( ( dY/dAi * delta(Ai) )^2 ) )</p>

<p>The equation being        Y =  A * X^B/B </p>

<p>Partial derivatives are then: </p>

<pre><code> dF/dA  =  X^B/B
 dF/dB  =  A * (X^B * log(X))/B - A * X^B/B^2
 dF/dX  =  A * (X^(B - 1) * B)/B
</code></pre>

<p>Then</p>

<pre><code> dF/dA = 12.9196498927581
 dF/dB = 167.269472901412
 dF/dX = 1.92930443474376
</code></pre>

<p>This yields</p>

<pre><code> Y = 95.8041099173585 +- 134.526084150286
</code></pre>

<p>However, when using the predictNLS() function from ""propagate"" R package:</p>

<pre><code> predictNLS(nlm, newdata=data.frame(X=30), interval = ""confidence"")$summary

 Propagating predictor value #1 ...
   Prop.Mean.1 Prop.Mean.2 Prop.sd.1 Prop.sd.2 Prop.2.5%
      95.80411    102.8339  20.89399  24.86949  51.89104
   Prop.97.5% Sim.Mean   Sim.sd Sim.Median  Sim.MAD  Sim.2.5%
     153.7767  93.5643 1712.894   97.85209 21.98703 -117.3541
   Sim.97.5%
    210.3916
</code></pre>

<p>Which yields</p>

<pre><code>    Y = 95.80411 +- (153.7767-51.89104)/2
 =&gt; Y = 95.80411 +- 50.94283
</code></pre>

<p>Obviously I must have missed / misunderstood some essential information about CI of response variable, because I believe the person who coded the predictNLS() function must be way more knowledgeable than me about it.</p>

<p>Thanks in advance for your explanations.</p>
"
"0.0693931503088838","0.0676252226000574","145799","<p>I was wondering why do I get linear model when I'm using exponential model,
<code>y = a * exp(-b*-x)</code>, to fit my data.</p>

<p>Here is my code:</p>

<pre><code>ff &lt;- function(x,a,b){a * exp(-b*-x)}
fit2 &lt;- nls(y ~ ff(x,a,b) , data = newdat, start =c(a=107.4623,b=-0.0037)
</code></pre>

<p>The graph below is mydata with the exponential fit (prediction of <code>fit2</code>) in purple curve. The green curve is what I though it would be, it is <code>Smooth.splines</code> fit.
<img src=""http://i.stack.imgur.com/qrUkj.png"" alt=""enter image description here""></p>

<p>Result from <code>fit2</code>:</p>

<pre><code>Nonlinear regression model
  model: dif2 ~ ff(age, a, b)
   data: newdat
         a          b 
109.743680  -0.003793 
 residual sum-of-squares: 2585

Number of iterations to convergence: 2 
Achieved convergence tolerance: 1.446e-06
</code></pre>

<p><img src=""http://i.stack.imgur.com/pdCrx.png"" alt=""enter image description here"">
Here is my data:</p>

<pre><code>   ID  x   y
    1 18 106.47
    1 19 100.35
    1 20 97.4
    1 21 101.03
    1 22 100.3
    1 23 99.06
    1 24 100.81
    2 18 101.95
    2 19 100.69
    2 20 100.89
    3 14 105.87
    3 15 107.44
    3 16 103.05
    3 17 104.86
    3 18 101.86
    3 19 101.48
    3 20 102.77
    3 21 99.63
    3 22 100.21
    3 23 101.28
    3 24 98.77
    3 25 99.91
    4 17 102.42
    4 18 101.85
    4 19 101.31
    5 18 101.24
    5 19 102.27
    5 20 100.03
    5 21 101.53
    6 20 98.08
    6 21 101.2
    6 22 103.16
    6 23 98.3
    6 24 102.21
    6 25 100.18
    6 27 95.28
    6 28 102.05
    6 29 100.72
    6 30 101.4
    7 13 111.3
    7 14 106.55
    7 15 103.23
    7 16 102.31
    7 17 101.11
    7 18 101.52
    7 19 100.14
    8 18 101.05
    8 19 98.15
    8 20 100.55
    8 21 101.62
    8 22 101.04
    8 23 98.22
    9 18 102.87
    9 19 101.46
    9 20 101.07
    9 21 101.32
    10 20 101.93
    10 21 101.73
    10 22 100.24
    11 19 99.75
    11 20 101.35
    11 21 99.34
    11 22 100.12
    12 18 103.34
    12 19 109.52
    12 20 106.98
    12 21 105.21
    12 22 98.87
    12 23 103.81
    12 24 100.38
    12 25 100.12
    12 26 99.7
    12 27 101.16
    12 28 99.02
    12 29 100.15
    12 30 97.32
    13 13 116.43
    13 14 111.75
    13 15 107.42
    13 16 103.5
    13 17 103.37
    13 18 100.66
    13 19 100.73
    13 20 100.84
    13 21 100.05
    14 18 101.66
    14 19 99.9
    14 20 101.4
    14 21 99.86
    14 22 100.82
    15 15 101.27
    15 16 100.01
    15 17 104.27
    16 19 100.26
    16 20 104.13
    17 18 106.12
    18 21 101.18
    18 22 99.51
    18 23 100.59
    19 18 100
    19 19 100.81
    19 20 99.37
    19 21 102.6
    20 22 102.18
    20 23 104.5
    20 24 100.74
    21 22 103.74
    21 23 98.66
    21 24 100.65
    21 25 99.63
    22 24 102.59
    22 25 94.62
    22 26 103.85
    23 20 100.7
    23 21 101.38
    23 22 102.36
    23 23 99.56
    23 24 100
    24 18 101.16
    24 19 99.64
    25 21 96.9
    25 22 109.3
    25 23 101.4
    25 24 98.04
    25 25 99.28
    25 26 99.63
    25 27 101.29
    25 28 100.08
    26 14 109
    26 15 112.37
    26 16 102.4
    26 17 102.15
    26 18 100.82
    27 18 101.14
    27 19 101.38
    28 17 105.09
    28 18 101.74
    28 19 100.2
    29 19 102.11
    29 20 100.57
    29 21 100.91
    29 22 99.61
    29 23 99.99
    30 18 99.81
    30 19 102.07
    31 19 100.75
    31 21 95.43
    32 23 99.73
    32 24 100.8
    32 25 100.1
    32 26 100.88
    32 27 97.73
    32 28 100.36
    33 22 99.4
    33 24 101.46
    33 18 97.65
    33 25 102.75
    33 26 97.7
    33 27 100.67
    34 21 98.27
    34 22 100.42
    34 23 101.16
    34 24 100.13
    34 25 98.55
    35 17 107.46
    35 18 100.22
    35 19 102.03
    35 20 101.52
    35 21 102.05
    35 22 102.46
    35 23 101.56
    35 24 96.88
    35 25 98.97
    35 26 101.68
    35 28 94.12
    36 20 98.63
    36 21 101.59
    36 22 98.76
    37 19 101.9
    37 20 98.66
    37 21 100.19
    37 22 100.03
    37 23 99.97
    38 15 104.32
    38 16 102.98
    38 17 103.4
    38 18 102.78
    38 19 101.73
    38 20 95.57
    39 22 101.5
    39 23 98.37
    39 24 100.4
    39 25 100.79
    40 19 102.93
    40 20 100.88
    40 21 99
    40 22 99.66
    41 21 107.08
    41 22 93.08
    41 24 100.91
    41 25 107.24
    41 26 99.8
    42 14 109.82
    42 15 106.09
    42 16 106.32
    42 17 102.8
    42 18 100.21
    42 19 102.08
    42 21 99.22
    42 22 100.13
    42 23 101.63
    43 16 100.95
    43 17 100.6
    43 18 101.81
    43 19 102.78
    43 20 98.43
    43 23 101.4
    43 24 103.12
    43 25 99.31
    43 26 100.47
    43 27 99.67
    43 28 98.75
    43 29 95.68
    44 23 103.78
    44 24 100.38
    44 25 99.39
    44 26 100.87
    44 27 99.64
    44 28 98.39
    44 29 97.62
    45 18 100.47
    45 19 101.41
    45 20 99.33
    45 21 101.08
    45 22 100.08
    45 23 100.22
    45 24 99.67
    45 25 100.45
    45 26 102.4
    45 27 95.7
    46 20 101.35
    46 21 98.73
    46 22 109.29
    46 23 100.04
    46 24 95.74
    46 25 100.44
    46 26 98.72
    47 19 100.51
    47 20 99.88
    47 21 101.7
    47 22 101.94
    47 23 100.72
    47 24 98.73
    47 25 102.16
    47 26 100.25
    47 27 95.1
    47 28 103.08
    48 25 105.21
    48 26 100.48
    48 27 98.07
    48 28 99.88
    48 29 95.61
    49 16 111.35
    49 17 92.43
    49 18 112.04
    49 19 100.8
    49 20 95.36
    49 21 103.13
    49 22 102.16
    49 23 98.81
    49 25 98.86
    49 26 99.93
    49 27 95.26
    50 23 98.15
    50 24 105.93
    50 25 99.01
    50 26 99.34
    50 27 93.68
    50 28 105.35
    51 24 100.96
    51 25 100.53
    51 26 99.2
    51 27 100.52
    51 28 100.86
    52 25 101.38
    52 26 98.45
    52 27 100.32
    52 28 99.24
    52 29 102.74
    53 24 101.37
    53 25 99.75
    53 27 96.31
    53 28 100.67
    54 22 98.09
    54 23 100.55
    54 24 100.25
    54 25 101.54
    54 26 98.48
    54 27 102.76
    54 28 98.5
    54 30 99.85
    55 22 103.87
    55 23 94.37
    55 24 105.12
    56 18 101.23
    56 19 99.26
    56 20 102.63
    56 21 100.75
    56 23 101.5
    56 24 99.14
    56 27 95.11
    57 16 107.57
    57 17 101.75
    57 18 107.18
    57 19 100.23
    57 20 105.48
    57 21 103.1
    57 22 100.45
    57 23 99.28
    57 24 100.52
    57 25 98.69
    58 27 103.13
    58 28 97.86
    58 29 101.33
    58 30 98.33
    58 32 102.14
    58 34 94.47
    58 35 98.29
    59 19 97.6
    59 20 98.93
    59 22 101.35
    59 23 93.88
    60 20 99.62
    60 22 97.36
    60 23 102.94
    60 24 98.98
    60 25 99.47
    61 18 100.15
    61 19 101.92
    61 20 101.34
    61 21 98.87
    61 22 97.68
    61 23 99.92
    61 24 100.78
    61 25 98.21
    62 20 102.7
    62 21 99.7
    62 22 100.17
    62 23 99.62
    62 24 100.59
</code></pre>
"
"0.0400641540107502","0.0390434404721515","146075","<p>I have a dataframe with one continuous response variable and hundreds of predictor variables (hundreds of additional columns in my dataframe). I'd like to run a regression for the single Response variable against all of the predictor variables, not univariately but all at once - what technique is best suited to do this, is the high dimsionality an issues..? I had been looking at Support Vector Regression using svm from the e1071 package, however my predictions come out to be a static number - I have many 0's in some of the columns..?</p>

<p>P.</p>
"
"0.0895861718290583","0.0873037869711973","147554","<p>I am currently stuck with a problem regarding predictions from linear regressions. I estimated a simple (multivariate) regression model y = b0 + b1 * x + b2 * X, where x is my variable of interest and X is a matrix of controls, using the <code>lm()</code> fct in R.</p>

<p>Now I want to predict y for two different values of x. Finally, I want to know whether those two predictions of y are statistically different from each other. </p>

<p>So far, I used <code>predict(model, se.fit = TRUE, interval = ""prediction"")</code> and got a point prediction as well as the corresponding prediction interval. Using the prediction intervals of the two points, I decided whether there are statistically different based on the overlapping of the prediction intervals.</p>

<p>I got almost no significant differences using this technique even when the estimated coefficients are significant. Is this the right track, or are there different techniques one can use?</p>

<p>Thanks for your help!</p>
"
"0.0991537224422629","0.0966275853154907","147619","<p>I'm trying to build a model that can predict streamflow for an alpine (snowmelt-fed) watershed using snow albedo (roughly, the energy reflectance of the snow) data. Albedo controls the melt of the snowpack, and higher albedo means slower melt, and vice versa. I have daily time-series data for both the snow albedo and streamflow, for 12 years from 2002-2013. The albedo time-series was obtained by spatially-averaging albedo data (raster files) from NASA's MODIS satellite.</p>

<p>I have tried various methods (simple regression, GLMs, GAMs, decision trees and random forests) to build the flow prediction model, but all of them fail because of the autocorrelated relationship between albedo and flow. Since the albedo is a snowpack property, there is a lag between it and the flow (related to snowmelt).</p>

<p>The Cross correlation function (CCF) between albedo and flow is shown below:</p>

<p><a href=""http://imgur.com/PpW1Kpy"" rel=""nofollow""><img src=""http://i.imgur.com/PpW1Kpy.png"" title=""source: imgur.com"" /></a></p>

<p>I have tried to include albedo lags of various days into the models, but I'm not able to mimic the distributed lag relationship between albedo and flow. I have tried to add precipitation, temperature and other climatic data to the predictors, but they don't seem to help. There are similar lagged and cross-correlation problems between these other predictors and flow.</p>

<p>The albedo, flow, precipitation and air temperature time-series are shown below:</p>

<p><a href=""http://imgur.com/Kb8Ta6q"" rel=""nofollow""><img src=""http://i.imgur.com/Kb8Ta6q.png"" title=""source: imgur.com"" /></a></p>

<p>Is there a statistical or machine learning technique in R that I can explore to build the albedo-streamflow model?</p>

<p>Thank you.</p>
"
"NaN","NaN","147793","<p>I would like to know the reason why we ignore those variables in logistic regression whose information value is more than 0.5 though it might carry high information about the prediction.</p>
"
"0.139198742242089","0.135652379058573","151961","<p>*Please note this question is about the Platt probabilistic output and SVM class assignment, not about the code or the package itself. It just happens to be the code where I stumbled on the issue.</p>

<p>In <a href=""http://stats.stackexchange.com/questions/147260/including-class-probabilities-might-skew-a-model-in-caret"">another question</a> I asked about bad models coming from <code>caret</code> and associated <code>kernlab</code> when <code>prob.model=TRUE</code>. I found the answer myself, in both <a href=""http://stackoverflow.com/questions/29766951/different-results-with-caret-when-classprobs-true"">stackoverflow</a> and <a href=""http://r.789695.n4.nabble.com/Inconsistent-results-between-caret-kernlab-versions-td4680500.html"" rel=""nofollow"">from Max Kuhn himself</a>:</p>

<blockquote>
<pre><code>&gt; predict(newSVM, df[43,-1]) [1] O32078 10 Levels: O27479 O31403 O32057 O32059 O32060 O32078 ... O32676
&gt; predict(newSVM, df[43,-1], type = ""probabilities"")
     O27479     O31403     O32057    O32059    O32060     O32078
[1,] 0.08791826 0.05911645 0.2424997 0.1036943 0.06968587 0.1648394
     O32089     O32663     O32668     O32676
[1,] 0.04890477 0.05210836 0.09838892 0.07284396
</code></pre>
  
  <p>Note that, based on the probability model, the class with the largest
  probability is O32057 (p = 0.24) while the basic SVM model predicts
  O32078 (p = 0.16).</p>
  
  <p><strong>Somebody (maybe me) saw this discrepancy and that led to me to follow
  this rule:</strong></p>

<pre><code>if(prob.model = TRUE) use the class with the maximum probability   
  else use the class prediction from ksvm().
</code></pre>
  
  <p>Therefore:</p>

<pre><code>predict(svm.m1, df[43,-1])
 [1] O32057
 10 Levels: O27479 O31403 O32057 O32059 O32060 O32078 ... O32676
</code></pre>
</blockquote>

<p>Isn't that innacurate? <code>kernlab</code> searches for the optimal probability cutoff that minimizes error, that's why the assigned class and the maximum probability don't match: they don't have to.</p>

<p>Check this reproducible example. I excluded two cherrypicked <code>virginica</code> samples.</p>

<pre><code>require(kernlab);require(caret);
#kernel=polynomial; degree=3; scale=0.1; C=0.31
set.seed(101);SVM&lt;-ksvm(Species~., data=iris[-c(135,150),], kernel='polydot',C=.31, kpar=list( scale=.1, degree=3), prob.model=T)
</code></pre>

<p>Here's the resulting model </p>

<pre><code>&gt; SVM
Support Vector Machine object of class ""ksvm"" 

SV type: C-svc  (classification) 
 parameter : cost C = 0.31 

Polynomial kernel function. 
 Hyperparameters : degree =  3  scale =  0.1  offset =  1 

Number of Support Vectors : 58 

Objective Function Value : -1.4591 -0.7955 -10.2392 
Training error : 0.033784 
Probability model included. 
</code></pre>

<p>Now let's check the predicted class probabilities in those two samples</p>

<pre><code>&gt; predict(SVM, iris[c(135,150),-5], type=""probabilities"")
          setosa versicolor virginica
[1,] 0.008286638  0.4414114  0.550302
[2,] 0.013824451  0.3035556  0.682620
</code></pre>

<p>And the class predictions</p>

<pre><code>&gt; predict(SVM, iris[c(135,150),-5])
[1] versicolor virginica 
Levels: setosa versicolor virginica
</code></pre>

<p>Sample 150 was assigned to <code>virginica</code>, with a class probability of around 0.68. Sample 135 was assigned to <code>versicolor</code> with a probability of around 0.44, yet <code>virginica</code> probability nicely sits around 0.55.
Looking at several CV folds, we perceive that kernlab only assigns <code>virginica</code> when its probability is over a given value (way higher than 0.5). That's the cutoff I mentioned, and it happens thanks to the well known bad clustering in <code>iris</code> between <code>virginica</code> and <code>versicolor</code>.</p>

<p>So, am I right on these suppositions and therefore is <code>caret</code> class assignment model (maximum probability) wrong?</p>

<p>EDIT:
I've been experimenting with pairwise probability coupling of Platt scaling (logistic regression fit), isotononic regression and a model I'm working on. A weakness (?) I perceived in Platt's model is the probability isn't bound to be 0.5 when the binary SVM decision output is 0, which is the expected result as the instance would lie exactly on the separating hyperplane.</p>
"
"0.0693931503088838","0.0676252226000574","152203","<p>I've found <a href=""http://cbio.ensmp.fr/~jvert/svn/tutorials/practical/linearregression/linearregression.R"" rel=""nofollow"">this line</a> of code to calculate predicted values from a ridge.lm model:</p>

<pre><code># Predict is not implemented so we need to do it ourselves
y.pred.ridge = scale(data.test[,1:8],center = F, scale = m.ridge$scales)%*% m.ridge$coef[,which.min(m.ridge$GCV)] + m.ridge$ym
</code></pre>

<p>Why center is set to FALSE? And why do I need to add the mean of $Y$ to the predicted values?</p>

<p>I thought $X$ values should be scaled before running a ridge regression, which implies the out of samples predictors should be centered and scaled?
And scaling the predictors doesn't imply centering the outcomes of the regression, so why to add the mean of $Y$ to the predictions?</p>
"
"0.176282277647301","0.195217202360758","152958","<p>I have following simple X and Y vectors: </p>

<pre><code>&gt; X
[1] 1.000 0.063 0.031 0.012 0.005 0.000
&gt; Y
[1] 1.000 1.000 1.000 0.961 0.884 0.000
&gt; 
&gt; plot(X,Y)
</code></pre>

<p><img src=""http://i.stack.imgur.com/xTk8u.png"" alt=""enter image description here""></p>

<p>I want to do regression using log of X. To avoid getting log(0), I try to put +1 or +0.1 or +0.00001 or +0.000000000000001 : </p>

<pre><code>&gt; summary(lm(Y~log(X)))
Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : 
  NA/NaN/Inf in 'x'
&gt; summary(lm(Y~log(1+X)))

Call:
lm(formula = Y ~ log(1 + X))

Residuals:
       1        2        3        4        5        6 
-0.03429  0.22189  0.23428  0.20282  0.12864 -0.75334 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)   0.7533     0.1976   3.812   0.0189 *
log(1 + X)    0.4053     0.6949   0.583   0.5910  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.4273 on 4 degrees of freedom
Multiple R-squared:  0.07838,   Adjusted R-squared:  -0.152 
F-statistic: 0.3402 on 1 and 4 DF,  p-value: 0.591

&gt; summary(lm(Y~log(0.1+X)))

Call:
lm(formula = Y ~ log(0.1 + X))

Residuals:
       1        2        3        4        5        6 
-0.08099  0.20207  0.23447  0.21870  0.15126 -0.72550 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)    1.0669     0.3941   2.707   0.0537 .
log(0.1 + X)   0.1482     0.2030   0.730   0.5058  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.4182 on 4 degrees of freedom
Multiple R-squared:  0.1176,    Adjusted R-squared:  -0.103 
F-statistic: 0.5331 on 1 and 4 DF,  p-value: 0.5058

&gt; summary(lm(Y~log(0.00001+X)))

Call:
lm(formula = Y ~ log(1e-05 + X))

Residuals:
       1        2        3        4        5        6 
-0.24072  0.02087  0.08796  0.13872  0.14445 -0.15128 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)     1.24072    0.12046  10.300 0.000501 ***
log(1e-05 + X)  0.09463    0.02087   4.534 0.010547 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.1797 on 4 degrees of freedom
Multiple R-squared:  0.8371,    Adjusted R-squared:  0.7964 
F-statistic: 20.56 on 1 and 4 DF,  p-value: 0.01055

&gt; 
&gt; summary(lm(Y~log(0.000000000000001+X)))

Call:
lm(formula = Y ~ log(1e-15 + X))

Residuals:
        1         2         3         4         5         6 
-0.065506  0.019244  0.040983  0.031077 -0.019085 -0.006714 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)     1.06551    0.02202   48.38 1.09e-06 ***
log(1e-15 + X)  0.03066    0.00152   20.17 3.57e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.04392 on 4 degrees of freedom
Multiple R-squared:  0.9903,    Adjusted R-squared:  0.9878 
F-statistic: 406.9 on 1 and 4 DF,  p-value: 3.565e-05
</code></pre>

<p>The output is different in all cases. What is the correct value to put to avoid log(0) in regression? What is the correct method for such situations.</p>

<p>Edit: my main aim is to improve prediction of the regression model by adding log term, i.e.: lm(Y ~ X + log(X))</p>
"
"0.0801283080215004","0.078086880944303","154043","<p>In polynomial regression, it is recommended to center predictor input variables to break multi colinear relationships of x to x^2.</p>

<p>From Wikipedia: The underlying monomials can be highly correlated ""For example, x and x2 have correlation around 0.97 when x is uniformly distributed on the interval (0, 1). ""</p>

<p>When a variable x is between -1 and 1, x^2 makes the magnitude smaller while when x is outside of that range, x^2 makes x's magnitude larger.</p>

<p>Making the variable into an integer variable could change the behavior.</p>

<p>E.g.</p>

<pre><code>df$x=round((df$x - mean(df$))*100)
</code></pre>

<p>Any opinions on the scale especially in regards to interval [-1,1] vs [-100,100]</p>

<p>It is common to normalize predictors subtracting the mean and dividing by the standard deviation when doing inference analysis but this question pertains to regression prediction.</p>

<p>Asking a similar question in regards to natural log, a variable that has a range (0,1] has a dramatically different transformed value than [1,100].</p>

<pre><code>log(seq(0.1,1,.1)) #mostly negative
log(seq(0.1,1,.1)*100) #rather positive
</code></pre>

<p>If the predictor variable in the case of log happened to be sometimes less than 1 and others greater than 1, that could make the transformation act a little ""wild"". Would it be best to transform the variable to be within (0,1] or [1,] but not both?</p>
"
"0.0693931503088838","0.0676252226000574","154416","<p>I have climate data for 240 predictors and precipitation flux (as the target variable) for 3000+ days. I want to use <strong>Gaussian kernel regression</strong> to predict the precipitation flux for the next 2000+ days.</p>

<p>I have gone through some of the available packages in both <code>R</code> and <code>MatLab</code>. </p>

<p><code>R</code> has the <code>np</code> package which provides the <code>npreg()</code> to perform kernel regression. However, the documentation for this package does not tell me how I can use the model derived to predict new data.</p>

<p>Similarly, MatLab has the codes provided by <strong>Yi Cao</strong> (<a href=""http://www.mathworks.com/matlabcentral/fileexchange/19279-multivariant-kernel-regression-and-smoothing"" rel=""nofollow"">ksrmv.m</a>) and <strong>Youngmok Yun</strong> (<a href=""http://youngmok.com/gaussian-kernel-regression-for-multidimensional-feature-with-matlab-code/"" rel=""nofollow"">gaussian_kern_reg.m</a>). Here as well, I am unable to understand how to use the model obtained to predict the new data.</p>

<p>I would be very grateful if someone can explain to me how to perform prediction using any one of these methods.</p>
"
"0.0400641540107502","0.0390434404721515","154944","<p>I could not find many information on how to plot a confidence interval and a prediction interval for a Poisson regression (for example with <code>glm()</code>).
What are some ways to calculate such intervals (in general and especially for poisson regression)? Would Bootstrap be a good idea?</p>

<p>Thank you for the help.</p>

<p>And sorry about the edit. It was maybe more of a <code>R</code> question before.</p>
"
"0.0566592699670073","0.0552157630374233","156098","<p>The R function cv.glm (library: boot) calculates the estimated K-fold cross-validation prediction error for generalized linear models and returns delta. Does it make sense to use this function for a lasso regression (library: glmnet) and if so, how can it be carried out? The glmnet library uses a cross-validation to get the best turning parameter, but I did not find any example that cross-validates the final glmnet equation.</p>
"
"0.0981367343026181","0.0956365069595007","156202","<p>I am working on a project where I am to do the intervention analysis and forecasting based on the time series. The problem is something like:</p>

<p><em>I have a normal time series entries but in between them some known event like natural calamities (storm, tornado) happens. I have the data for that and it affects the normal time series. Now my objective is to forecast the value of time series both in normal mode and also when I have a prediction of storm coming.</em></p>

<p>I have been reading <a href=""http://rads.stackoverflow.com/amzn/click/0471615285"" rel=""nofollow"">Forecasting with dynamic regression</a> chapter 7 about intervention analysis. I am also reading about the transfer function modeling. Can you please help me as in which model is good for this kind of time series analysis? Or may be some link which can guide me as how to do it? I will appreciate a link with some example in R or some examples.</p>

<p>EDIT: I guess I was not correct in description but I know the exact time information of all the previous storm events and I sort of want to find out the effect of storm intervention on the time series and I can forecast more closely if I know that there is a storm happening right now.</p>
"
"0.0895861718290583","0.0873037869711973","158492","<p>I have fitted a (Cragg's) truncated normal hurdle model over a dataset in which the dependent variable is either zero or positive. The model consists of two parts: a probit which estimates the probability of the value being zero and a truncated regression which is estimated over the subsample of positive values of the dependent variable.</p>

<p>The output of the estimation (using package <code>mhurdle</code> in R) consists of two columns: one gives the probability of y being 0 and the other gives the estimated value for an uncensored observation (let's call this y*).</p>

<p>Now I would like to use these results for prediction, but instead of probabilities and estimated values of the uncensored y I would like to have the estimated values of the dependent variable, including some zeros (or almost zeros). Should I multiply the probability of y NOT being zero by the value of y*? Or should I take all observations for which P(y=0) > 0.5 to be zero and all the others to be equal to y*?</p>

<p>Sorry if the question is trivial but I'm fairly new to statistics and I have not been able to find the answer so far.</p>
"
"0.0693931503088838","0.0676252226000574","158701","<p>I use the svm function (for regression) to make forecast like I would with for exemple the arima function:<br>
<code>fit&lt;-auto.arima(ts)</code><br>
<code>prediction&lt;-forecast(fit,h=20)</code><br>
which returns different attributes : </p>

<blockquote>
  <ol>
  <li><code>prediction$mean</code> which is the actual prediction  </li>
  <li><code>prediction$lower</code> and <code>prediction$upper</code> which are the   <strong>boundaries of the confidence intervals</strong> on each points of the   <code>prediction$mean</code>.  </li>
  </ol>
</blockquote>

<p>I would like the <code>svm</code> function (from <em>e1071</em> package) to return a more detailed answer than just the value (like the <code>forecast()</code> would).<br>
 But I guess it is not implemented in the function yet.
Is there another function to do it ? Or should I use <strong>bootstrap</strong> methods to try to estimate those boundaries? And if I should use this are they pre-implemented version of them instead of using sample over a for loop which is very time-consuming ?</p>
"
"0.0801283080215004","0.078086880944303","159284","<p>Assuming the data is tidy and it has a mix of columns of type numeric, character and Factor. What is the data type that would give best results when using different prediction techniques in R?</p>

<p>I am guessing the preferred data type would differ based on prediction method like:</p>

<ol>
<li><p>A regression model would require columns to be in numeric type. But what about the columns which are non-numeric ? What should they be to get the best out of the algorithm ? </p></li>
<li><p>Similarly what type of columns would give best results in a classification problem ?</p></li>
</ol>
"
"NaN","NaN","159346","<p>I am fairly new to survival analysis and am playing around in R. I have a fairly simple cox model </p>

<pre><code>library(survival)
data(kidney)
cox&lt;-coxph(Surv(time, type)~delta, data=kidney)
baseline &lt;- basehaz(cox , centered=FALSE)
cox.survfit&lt;- survfit(cox)
plot(cox.survfit)
</code></pre>

<p><img src=""http://i.stack.imgur.com/FIdvJ.png"" alt=""enter image description here""></p>

<p>My question is how do i calculate the survival rates myself without calling the survfit function. I tried to look at <a href=""http://stats.stackexchange.com/questions/36015/prediction-in-cox-regression"">this stackoverflow post</a> which i kind of understand but am not exactly able to turn it into code.  </p>

<p><img src=""http://i.stack.imgur.com/ei0To.png"" alt=""enter image description here""></p>

<p>I have the h0 from basehaz but i am not sure about rest of the calculations . Any ideas ?</p>

<p>EDIT:-
Added the kidney dataset in csv format(in case you don't have the same columns in your kidney datset ) :- </p>

<pre><code>""time"",""delta"",""type""
1.5,1,1
3.5,1,1
4.5,1,1
4.5,1,1
5.5,1,1
8.5,1,1
8.5,1,1
9.5,1,1
10.5,1,1
11.5,1,1
15.5,1,1
16.5,1,1
18.5,1,1
23.5,1,1
26.5,1,1
2.5,0,1
2.5,0,1
3.5,0,1
3.5,0,1
3.5,0,1
4.5,0,1
5.5,0,1
6.5,0,1
6.5,0,1
7.5,0,1
7.5,0,1
7.5,0,1
7.5,0,1
8.5,0,1
9.5,0,1
10.5,0,1
11.5,0,1
12.5,0,1
12.5,0,1
13.5,0,1
14.5,0,1
14.5,0,1
21.5,0,1
21.5,0,1
22.5,0,1
22.5,0,1
25.5,0,1
27.5,0,1
0.5,1,2
0.5,1,2
0.5,1,2
0.5,1,2
0.5,1,2
0.5,1,2
2.5,1,2
2.5,1,2
3.5,1,2
6.5,1,2
15.5,1,2
0.5,0,2
0.5,0,2
0.5,0,2
0.5,0,2
0.5,0,2
0.5,0,2
0.5,0,2
0.5,0,2
0.5,0,2
0.5,0,2
1.5,0,2
1.5,0,2
1.5,0,2
1.5,0,2
2.5,0,2
2.5,0,2
2.5,0,2
2.5,0,2
2.5,0,2
3.5,0,2
3.5,0,2
3.5,0,2
3.5,0,2
3.5,0,2
4.5,0,2
4.5,0,2
4.5,0,2
5.5,0,2
5.5,0,2
5.5,0,2
5.5,0,2
5.5,0,2
6.5,0,2
7.5,0,2
7.5,0,2
7.5,0,2
8.5,0,2
8.5,0,2
8.5,0,2
9.5,0,2
9.5,0,2
10.5,0,2
10.5,0,2
10.5,0,2
11.5,0,2
11.5,0,2
12.5,0,2
12.5,0,2
12.5,0,2
12.5,0,2
14.5,0,2
14.5,0,2
16.5,0,2
16.5,0,2
18.5,0,2
19.5,0,2
19.5,0,2
19.5,0,2
20.5,0,2
22.5,0,2
24.5,0,2
25.5,0,2
26.5,0,2
26.5,0,2
28.5,0,2
</code></pre>
"
"0.150240577540313","0.13665204165253","159647","<p>I've been studying (and applying) SVMs for some time now, mostly through <code>kernlab</code> in <code>R</code>.</p>

<p><code>kernlab</code> allows probabilistic estimation of the outcomes through Platt Scaling, but the same could be achieved with a Pool Adjacent Violators (PAV) isotonic regression (Zadrozny and Elkan, 2002).</p>

<p>I've been wrapping my head over this and came with a (clunky, but it works, or yet I think it does) code to try the PAV algorithm.</p>

<p>I divided the task into three pairwise binary classification task, estimated the probabilities on the training data and coupled the pairwise probabilities to get class probabilities (Wu, Lin, and Weng, 2004).</p>

<p>Predictions were made on the training set. I set the Cost really low <code>C=0.001</code> to try to get some misclassifications. </p>

<p>The Brier Score is defined as:</p>

<p>$$BS=\frac{1}N\sum_{t=1}^N\sum_{i=1}^R(f_{ti}-o_{ti})^2 $$</p>

<p>Where $R$ is the number of classes, $N$ is the number of instances, $f_{ti}$ is the forecast probability of the $t$-th instance belonging to the $i$-th class, and $o_{ti}$ is $1$, if the actual class $y_t$ is equal to $i$ and $0$, if the class $y_t$ is different from $i$.</p>

<pre><code>require(isotone)
require(kernlab)

##PAVA SET/VER
data1   &lt;-  iris[1:100,]        #only setosa and versicolor
MR1 &lt;-  c(rep(0,50),rep(1,100)) #target probabilities
KSVM1   &lt;-  ksvm(Species~., data=data1, type=""C-svc"", kernel=""rbfdot"", C=.001)
PRED1   &lt;-  predict(KSVM1,iris, type=""decision"")    #SVM decision function
PAVA1   &lt;-  gpava(PRED1, MR1)               #generalized pool adjacent violators algorithm 

##PAVA SET/VIR
data2   &lt;-  iris[c(1:50,101:150),]      #only setosa and virginica
MR2 &lt;-  c(rep(0,50),rep(1,50),rep(0,50))    #target probabilities
KSVM2   &lt;-  ksvm(Species~., data=data2, type=""C-svc"", kernel=""rbfdot"", C=.001)
PRED2   &lt;-  predict(KSVM2,iris, type=""decision"")
PAVA2   &lt;-  gpava(PRED2, MR2)

##PAVA VER/VIR
data3   &lt;-  iris[51:150,]   #only versicolor and virginica
MR3 &lt;-  c(rep(0,100),rep(1,50)) #target probabilities
KSVM3   &lt;-  ksvm(Species~., data=data3, type=""C-svc"", kernel=""rbfdot"", C=.001)
PRED3   &lt;-  predict(KSVM3,iris, type=""decision"")
PAVA3   &lt;-  gpava(PRED3, MR3)

#Usual pairwise binary SVM
KSVM    &lt;-  ksvm(Species~.,data=iris, type=""C-svc"", kernel=""rbfdot"", C=.001,prob.model=TRUE)

#probabilities on the training data through Platt scaling and pairwise coupling
PRED    &lt;-  predict(KSVM,iris,type=""probabilities"")

#The usual KSVM response based on the sign of the decision function
RES &lt;-  predict(KSVM,iris)

#pairwise probabilities coupling algorithm on kernlab
PROBS   &lt;-  kernlab::couple(cbind(1-PAVA1$x,1-PAVA2$x,1-PAVA3$x))
colnames(PROBS) &lt;- c(""setosa"",""versicolor"",""virginica"")

#Brier score multiclass definition
BRIER.PAVA  &lt;-  sum(
(cbind(rep(1,50),rep(0,50),rep(0,50))-PROBS[1:50,])^2,
(cbind(rep(0,50),rep(1,50),rep(0,50))-PROBS[51:100,])^2,
(cbind(rep(0,50),rep(0,50),rep(1,50))-PROBS[101:150,])^2)/150

#Brier score multiclass definition
BRIER.PLATT &lt;-  sum(
(cbind(rep(1,50),rep(0,50),rep(0,50))-PRED[1:50,])^2,
(cbind(rep(0,50),rep(1,50),rep(0,50))-PRED[51:100,])^2,
(cbind(rep(0,50),rep(0,50),rep(1,50))-PRED[101:150,])^2)/150

BRIER.PAVA

BRIER.PLATT
</code></pre>

<p>Soon I'll clean up a bit and write a proper wrapper function to do it all, but this result's really worrisome for me.</p>

<pre><code>BRIER.PAVA 
[1] 0.09801759
BRIER.PLATT 
[1] 0.6710232
</code></pre>

<p>The Brier Score I got from the probabilities estimated through PAVA is way better than the one we get on Platt Scaling.</p>

<p>If you check <code>PRED</code> you will see all probabilites fall on the ~0.33 range, while on <code>PROB</code> more extreme values (1 or 0) are expected, which was quite unexpected to me as I'm using a really low <code>C</code>.</p>

<p>References:</p>

<p><a href=""http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf"" rel=""nofollow"">Zadrozny, B., and Elkan, C. ""Transforming classifier scores into accurate multiclass probability estimates."" Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2002.</a></p>

<p><a href=""http://papers.nips.cc/paper/2454-probability-estimates-for-multi-class-classification-by-pairwise-coupling.pdf"" rel=""nofollow"">T.-F. Wu, C.-J. Lin, and Weng, R.C. ""Probability estimates for multi-class classification by pairwise coupling."" The Journal of Machine Learning Research 5 (2004): 975-1005.</a></p>

<p>EDIT:</p>

<p>Also, if you check the AUC of the different probabilities, they are quite high.</p>

<pre><code>requires(caTools)

AUC.PAVA&lt;-caTools::colAUC(PROBS,iris$Species)

AUC.PLATT&lt;-caTools::colAUC(PRED,iris$Species)

colMeans(AUC.PAVA)
colMeans(AUC.PLATT)
</code></pre>

<p>And here's the result</p>

<pre><code>&gt; colMeans(AUC.PAVA)
    setosa versicolor  virginica 
 0.9988667  0.9988667  0.8455333 
&gt; colMeans(AUC.PLATT)
    setosa versicolor  virginica 
 0.8913333  0.8626667  0.9656000 
</code></pre>

<p>Looking at these AUC, I would say Platt Scaling is a really underconfident technique.</p>
"
"0.0693931503088838","0.0676252226000574","160281","<p>Here is what i am doing. I am building a logarithmic model in linear form based on the correlation between two variables shown in the graph!</p>

<p>lm(y~logx,data=logdata) -- i have only one predictor and one response variable<img src=""http://i.stack.imgur.com/6bqBB.png"" alt=""enter image description here""></p>

<p>cor(x,y) = -0.57(bit low for the data i have but they are negatively correlated in general sense and cor should be about atleast -0.80 in normal cases)</p>

<p>Multiple R-square = 35.78%</p>

<p>Data set doesn't contain any null/missing values. But from the graph you can see the concentration of data points is more at the center!</p>

<p>What are the different factors that i should think of in order to improve my model? here is one what i think off -</p>

<ol>
<li>Remove the outliers -- how should i identify outliers?</li>
<li>Should I remove all the data points that are sparsely distributed? -- I am concerned like that is going to effect my model?</li>
</ol>

<p>Do i have to go with regression modeling in this case which is not giving me the best prediction? If not can anyone please suggest me the best algorithm that should be used in this case?</p>
"
"0.114024581281567","0.123466199581199","160316","<p>I have a dataset consisting of about 600 observations. Each observation has around 100 attributes. One of the attributes I want to predict. Since the attribute that I want to predict can only have non-negative integer values, I was looking for ways to predict count data and found that there are various options, such as Poisson regression or negative binomial regression.</p>

<p>For my first try I used negative binomial regression in <code>R</code>:</p>

<pre><code>#First load the data into a dataset
dataset &lt;- test_observations[, c(5:8, 54)]

#Create the model
fm_nbin &lt;- glm.nb(NumberOfIncidents ~ ., data = dataset[10:600, ] )
</code></pre>

<p>I then wanted to see how to predicted values look like:</p>

<pre><code>#Create data to test prediction
newdata &lt;- dataset[1:10, ]

#Do the prediction
predict(fm_nbin, newdata, type=""response"")
</code></pre>

<p>Now the problem is the output looks like this:</p>

<pre><code>     1         2         3         4         5         6         7         8         9        10 
0.2247337 0.2642789 0.2205408 0.2161833 0.1794224 0.2081522 0.2412996 0.2074992 0.2213011 0.2100026 
</code></pre>

<p>The problem with this is that I expected that the predicted values are integers, since that is the whole purpose of using a negative binomial regression. What am I missing here?</p>

<p>Furthermore, I would like to evaluate my predictions in terms of mean squared error and mean absolute error, as well as a correlation coefficient. However, I couldn't find a way to get these easily, without doing all the calculations manually. Is there any built-in function for this?</p>
"
"0.0400641540107502","0.0390434404721515","160495","<p>I working with R on a classification problem. My outcome variable is binary with two levels 1 and 2. 
First of all I tried the logistic regression, which of all methods has the best performance, altough still poor. </p>

<p>I tried nnet package, random forest, the fuzzy package frbs and decision trees. </p>

<p>The nnet function gives me only one class - in this case 2.</p>

<p>I had some hope with frbs package. See my code below:</p>

<pre><code>obj &lt;- frbs.learn(train,method.type=""FRBCS.CHI"",control=list(num.labels=3,type.mf=""GAUSSIAN""))
summary(obj)
#test set without def 
pred&lt;-predict(obj,newdata=test[,1:8])
</code></pre>

<p>But the predictions are wrong, the class 1 is completely missclassified</p>

<pre><code>#percentage error
tdef&lt;-test$def
err = 100*sum(pred!=tdef)/ nrow(pred)
print(err)
[1] 16.93038
</code></pre>

<p>I'm wondering what I could improve to classify the output variable. Is something wrong with my data? 
Are the parameters not right? </p>

<p>Can someone please verifiy?  I'm at the end of my knowledge...</p>

<p>You can find the (normalized) data here:
<a href=""https://drive.google.com/open?id=1xrCXTLqKvGiGeo2X0Y1DvoSKvzbYFnyccLimceDIbZg"" rel=""nofollow"">https://drive.google.com/open?id=1xrCXTLqKvGiGeo2X0Y1DvoSKvzbYFnyccLimceDIbZg</a></p>
"
"0.149906337799172","0.146087177447694","160638","<h1>General question</h1>

<p>When I perform a logistic regression using lrm and specify weights for the observations, I get the following warning message:</p>

<blockquote>
  <p>Warning message:
  In lrm(Tag ~ DLL, weights = W, data = tagdata, x = TRUE, y = TRUE) :
    currently weights are ignored in model validation and bootstrapping lrm fits</p>
</blockquote>

<p>My interpretation is that everything that the rms package will tell me regarding goodness-of-fit, notably using the residuals.lrm tool, is wrong. Is this correct?</p>

<h1>Specific example</h1>

<p>To be more specific, I have working example. All the code and output can be found in this <a href=""https://github.com/jwimberley/crossvalidated-posts/tree/master/lrm_gof"" rel=""nofollow"">GitHub repository</a>. I have two CSV tables of data, <a href=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/toystudy.csv"" rel=""nofollow"">toystudy.csv</a> and <a href=""https://github.com/jwimberley/crossvalidated-posts/raw/master/lrm_gof/realstudy.csv"" rel=""nofollow"">realstudy.csv</a>. There are three columns in each:</p>

<ol>
<li>The binomial response $y$ (0 or 1) [called Tag in code]</li>
<li>The predictor $x$ [called DLL in code]</li>
<li>The weight for the observation [called W in code]</li>
</ol>

<p>The former is simulated data, where all the weights are unity and where a logistic regression $log(\pi) = \theta_0 + \theta_1 x$ should fit the data perfectly. The latter is real data from my analysis, where the validity of this simple model is in question. The real data has weighted observations. (Some of the weights are negative, but there is a well-defined reason for this). The analysis code in contained completely in <a href=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/regressionTest.R"" rel=""nofollow"">regressionTest.R</a>; the meat of the code is</p>

<pre><code>library(rms)
fit &lt;- lrm(Tag ~ DLL, weights = W, data = tagdata, x=TRUE, y=TRUE)
residuals(fit,""gof"")
</code></pre>

<p>Here are the results for the two tables of data.</p>

<h3>Case 1: Toy data</h3>

<p>The goodness-of-fit claimed by lrm (which is something called the le Cessie-van Houwelingen-Copas-Hosmer test, I understand?) is very good:</p>

<p><img src=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/toy/residuals.png"" alt=""enter image description here""></p>

<p>This is confirmed by grouping the data into 20 quantiles of the predictor and overlaying the predicted success rate over the average actual success rate:</p>

<p><img src=""http://i.stack.imgur.com/hOEFs.png"" alt=""enter image description here""></p>

<h3>Case 2: Real data</h3>

<p>In this case, the goodness-of-fit reported by lrm is horrendous:</p>

<p><img src=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/real/residuals.png"" alt=""enter image description here""></p>

<p>However, I don't think it should be that bad. Again grouping the data into quantiles, and taking into account the weights when computing the average values in each bin:</p>

<p><img src=""http://i.stack.imgur.com/mgzhc.png"" alt=""enter image description here""></p>

<p>Comparing the prediction to the observed values and their standard errors, I don't think this is that bad (the error bars here depend on how the standard error on a weighted mean is computed, so they might not be 100% right, but should at least be close). On the other hand, if I produce the same plot while ignoring the weights:</p>

<p><img src=""http://i.stack.imgur.com/dId9F.png"" alt=""enter image description here""></p>

<p>I can definitely imagine this fit being as poor as the goodness-of-fit test says.</p>

<h2>Conclusion</h2>

<p>So, is residuals.rm simply ignoring the weights when it calculates its goodness-of-fit statistic? And if so, is there any R package that will do this correctly?</p>
"
"0.0693931503088838","0.0676252226000574","160696","<p>As a pet project, I have been learning some data analysis and machine learning skills (mainly text analytics) with the Analytics Edge course on edX. I decided to put some of my new skills at use analysing a dataset from UCI Machine Learning: <a href=""https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection"" rel=""nofollow"">https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection</a></p>

<p>I did some analysis already (can be seen at <a href=""https://github.com/Khaltar/Portfolio/blob/master/R/Machine%20Learning/SMS.R"" rel=""nofollow"">https://github.com/Khaltar/Portfolio/blob/master/R/Machine%20Learning/SMS.R</a>) and computed a LogRegression Model, a RF Model and a CART Model. The Random Forest Model seems to be getting the best results regarding AUC and accuracy but I'm still not happy with it.</p>

<p>A friend of mine suggested using a bagging approach joining the three models and using some kind of ""voting"" system to classify predictions and achieve better results but I am completely at a loss on how to do that.  My doubt is how to actually implement a bootstrapping model in R using RF to raise accuracy of the model. I tried using bagRboostR package (sample code in my sms.R file in github) but I can't figure out how to use it or if there is a simpler solution to implement it.</p>

<p>Thanks in advance</p>
"
"0.120192462032251","0.117130321416455","160709","<p>I am doing binary logistic regression in R and I need to calculate the Count R squared for various model specifications. Count R2 is the number of correctly predicted observations using the model divided by the total number of observations. It measures how well the model predicts the correct value of the dependent variable, using known values. I'm planning to use the model for prediction, so the percent of observations that are predicted correctly would be really useful for me. Creating a classification table is difficult because I have missing data, so the fitted table and the original table have different numbers of records. I'm pretty inexperienced with R, so I don't know if there's a straightforward way to get around that.</p>

<p>My specific question is:</p>

<p>Is there a command in R to get the Count R2, (and better yet, the adjusted count R2)?</p>

<p>If not, is there an easy way to get R to put the predicted probabilities and the original dependent variable in a table together, when there are different numbers of records? This would allow me to calculate the Count R2 myself.</p>
"
"0.105999788000636","0.103299233817667","160721","<p>I am trying to replicate the <a href=""http://www.pnas.org/content/110/15/5802.full"" rel=""nofollow"">Kosinski, Stillwell, &amp; Graepel (2013) study</a> about predicting private traits and attributes from Facebook like data for study purposes. First I have admit, however, that I am quite a newbie to data science and building prediction models in R.</p>

<p>My Data:</p>

<ol>
<li>A sparse matrix (dim 237 x 43232) that contains 237 users and if they liked one of the 43232 fb-pages or not (indicated by 1 for liked and 0 for not-liked). The row-names contain user.ids.</li>
<li>A data.frame with columns containing the user.ids, age, gender, and several survey scores, for this example lets take the SOP2 score (optimism-pessimism score).</li>
</ol>

<p>My goal is to predict the SOP2 score with the user likes. Kosinski et al. describe their model building in <a href=""http://www.pnas.org/content/110/15/5802/F1.expansion.html"" rel=""nofollow"">this graphic</a>. So far I have done a SVD using the irlab R package:</p>

<pre><code>Comps.likes &lt;- irlba(Likes.matrix, nu = 100, nv = 100)
</code></pre>

<p>And this is where I am stuck .. how to go on from here?</p>

<p>From what I assume I should get to something like this:</p>

<pre><code>fit &lt;- lm(SOP2 ~ Comps.likes$d, data = someDataFrame)
</code></pre>

<p>or an equivalent using the caret package.</p>

<p>What I am trying to figure out:</p>

<ol>
<li>The step missing is how to get from the Comps.likes (SVD step) to
the regression formula with a coefficient for each Facebook like to
predict SOP2.</li>
<li>The step to actually predict SOP2 from a user vector
with the likes per user.id.</li>
</ol>

<p>Any help or hints to further resources?</p>
"
"0.0566592699670073","0.0552157630374233","164333","<p>I have been looking at some tutorials and articles and couldn't get a scenario where two variables are in different scales and used in modeling.</p>

<p>So, firstly lets assume I have one metric of numeric type, other in percentages, and other in decimals. </p>

<ol>
<li>If I want to use those variables in a regression model for
prediction then do I need to do some standardization before fitting a<br>
model to the variables? If so how do we it in R or Python?</li>
<li>Moreover, if I want to use these features in k-means
clustering, do I need to follow the same steps as mentioned above?</li>
</ol>
"
"0.0600962310161253","0.078086880944303","164912","<p>I am modelling invertebrate.biomass ~ habitat.type * calendar.day + habitat.type * calendar.day ^ 2, with a random intercept of transect.id (50 transects were repeated 5 times)</p>

<p>My response is zero-heavy - about 25% are 0s - and the non-zeroes are strongly right-skewed. </p>

<p>I understand a possible way of dealing with this is to construct 2 models - one modelling a binary response in a logistic regression and the other modelling the non-zero response in a (e.g.) Gamma regression. I'm working in R and following the ideas in <a href=""http://seananderson.ca/2014/05/18/gamma-hurdle.html"" rel=""nofollow"">this post</a>.</p>

<p>I want to check the method of combining the results of these 2 models, in order to generate quantitative predictions (ultimately with CI). Am I correct in multiplying the predicted probabilities from the logistic regression with the predicted (non-zero) biomass from the Gamma regression? Thus, the predicted (non-zero) biomass gets down-weighted according to the probability of there actually being an invertebrate present at all. This makes sense in my head, but feels too easy to be true. </p>

<p>See plots below which demonstrate my method in it's current form.
<a href=""http://i.stack.imgur.com/MVmJc.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/MVmJc.png"" alt=""Gamma hurdle model""></a></p>

<p>Assuming I'm right so far, how would I then go about generating a SE / CI for the predictions combining two models? </p>
"
"0.0801283080215004","0.078086880944303","166461","<p>I have created an example in R to illustrate the problem:</p>

<pre><code>&gt; set.seed(10)
&gt; Ydata&lt;-rnorm(200,15,5)*rep(1:200)^3
&gt; Xdata&lt;-rep(1:200)

&gt; lm.test&lt;-lm(log(Ydata)~Xdata)
&gt; summary(lm.test)$r.squared 

[1] 0.7665965

&gt; Yfit&lt;-fitted.values(lm.test)
&gt; lm.test2&lt;-lm(Yfit1~log(Ydata))
&gt; summary(lm.test2)$r.squared 

[1] 0.7665965

&gt; ExpYfit&lt;-exp(fitted.values(lm.test))
&gt; lm.test3&lt;-lm(ExpYfit~Ydata)
&gt; summary(lm.test3)$r.squared

[1] 0.6088178
</code></pre>

<p>When calculating the r-squared of some exponential model, fitted values for log(Y) run against observed log(Y) give the same r-squared as the original regression as expected:</p>

<p>log(Y) = fitted values = a + bX</p>

<p>but when we want to estimate the level of Y, exponentials of both sides are taken:</p>

<p>Y= exp(a + bX) = exp(fitted values)</p>

<p>but when running level Y against exponential fitted values, the R-squared is calculated incorrectly. </p>

<p>Why is this? and does this mean my predictions of Y are wrong?</p>
"
"0.120797969451519","0.129492442570703","166779","<p>Iâ€™ve seen some papers that present the idea of training classifiers such as logistic regression that are really meant to optimize a custom cost model (such as by maximizing profit given expect revenues for predictions depending on whether they are false positives, true negatives, true positives, or true negatives) not by optimizing the typical log-loss function and then looking for the optimal decision cut-off threshold, but by using different loss functions that weight differently the costs of each classification type or of each misclassification type (although I've seen that different authors propose different functions), and these seem to provide better results when evaluating them based on the customly-defined cost function.</p>

<p>I was wondering if there are any implementations of such methods in R. Particularly, I'd like to try fitting a logistic regression treating the cost of misclassifying as false positive to be a multiple of the cost of misclassifying as false negative. I found a package that does just this for decision trees (although in that case it's based on the class proportions on the leaves rather than something like log-loss) and I see that there are some options for observation-specific weights in logistic regression, but not for error type weights.</p>
"
"0.0981367343026181","0.0956365069595007","166953","<p><strong>Issue</strong>: Cannot forecast sales accurately using quantile regression in R. I am using rq function from ""quantreg"" package which is giving me warning ""Result might have Non unique solutions""</p>

<p><strong>Aim</strong>: I am trying to forecast hourly sales of a store using quantile regression. </p>

<p>Below are the columns in my source table for forecasting.</p>

<ul>
<li><em>transaction_date</em> : sales date (input)</li>
<li><em>hr1 to hr24</em> : column with hourly sales info. (24 columns) (input)</li>
<li><em>totala</em> : total of 24 column hr1 to hr24 (not using currently)</li>
<li><em>location, department, sales_type</em>: forecasting will be done for each location, sales_type and department. (used to select data)</li>
<li><em>f1 to f24 :</em> columns I want to forecast for each hour (24 columns) (output)</li>
</ul>

<p>Packages Used: forecast, quantreg, Metrics</p>

<p><strong>Code</strong>: 
I have extracted date features from transaction_date eg. weekend, week of month and also holidays (1 if it is holiday 0 for regular days).</p>

<pre><code>attach(train_data) 
Y &lt;- cbind(hr) 
X &lt;- cbind(transation_date, Years, Months, Days, WeekDay, WeekofYear, Weekend, WeekofMonth, holidays) 

quantreg.all &lt;- rq(Y ~ X, tau = seq(0.05, 0.95, by = 0.05))
prediction_train &lt;- data.frame(predict(quantreg.all))
</code></pre>

<p>I have 19 models in prediction_train for each tau from 0.05 to 0.95, I select best model based on rmse value and than forecast using that tau.</p>

<pre><code>rmse(actual, predicted)
</code></pre>

<p>transaction_date is Date type, quantreg.all is rqs class and rest are numeric.</p>

<p><strong>Note:</strong> Stores are not open 24 hours, hence many hour columns will be 0 (time when store was close). Currently for most of such hours rq is predicting 0 or some negative values.</p>

<p>Weather  does not have major impact on sales.</p>
"
"0.0817806119188484","0.0956365069595007","167089","<p>I am new to cross-validation and I have a data-set called LDA.scores for 12 measured call-type parameters. I am trying to run a k-fold repeated cross validation with 10 folds and associated naive Bayes method. The grouping factor is Family, since I am trying to assimilate if call-type parameters between between both families are different. I am trying to run this code</p>

<pre><code> library(caret)
 train_control&lt;-trainControl(method=""repeatedcv"", number=10, repeats=3)
 model&lt;-train(Family~., data=LDA.scores, trControl=train_control,method=""nb"")
 predictions &lt;- predict(model, LDA.scores[,2:13])
 confusionMatrix(predictions,LDA.scores$Family)
</code></pre>

<p>I keep on getting these error messages:</p>

<pre><code> Error in train.default(x, y, weights = w, ...) : 
 wrong model type for regression
</code></pre>

<p>I do not understand what I am doing wrong.  How can I run this code to produce a naive Bayes matrix.  Any advice would be deeply appreciated. I have tried everything possible with my novel capabilities. Words cannot describe my gratitude if anyone has a solution.  Here is a portion of my dataframe: </p>

<pre><code>      Family SBI.max.Part.1 SBI.max.Part.2 SBI.min.Part.1 SBI.min.Part.2
1         G8    -0.48055680   -0.086292700   -0.157157188   -0.438809944
2         G8     0.12600625   -0.074481895    0.057316151   -0.539013927
3         G8     0.06823834   -0.056765686    0.064711783   -0.539013927
4         G8     0.67480139   -0.050860283    0.153459372   -0.539013927
5         G8     0.64591744   -0.050860283    0.072107416   -0.472211271
6         G8     0.21265812   -0.068576492    0.057316151   -0.071395338
7         G8    -0.01841352   -0.068576492   -0.053618335   -0.071395338
8         G8     0.12600625    0.055436970    0.012942357    0.296019267
9         G8    -0.22060120    0.114491000   -0.038827070    0.563229889
10        G8     0.27042603   -0.021333268    0.049920519   -0.037994010
11        G8     0.03935439   -0.044954880    0.012942357    0.195815284
12        G8    -0.45167284    0.008193747   -0.075805232   -0.171599321
13        G8    -0.04729748   -0.056765686    0.035129254   -0.305204632
14        G8    -0.10506539    0.008193747   -0.046222702    0.062209973
15        G8     0.09712230    0.037720761    0.109085578   -0.104796666
16        G8    -0.07618143    0.014099150   -0.038827070    0.095611301
17        G8     0.29930998    0.108585597    0.057316151    0.028808645
18        G8     0.01047043   -0.074481895    0.020337989   -0.071395338
19        G8    -0.24948516    0.002288344    0.035129254    0.329420595
20        G8    -0.04729748    0.049531567    0.057316151    0.296019267
21        G8    -0.01841352    0.043626164    0.005546724   -0.171599321
22        G8    -0.19171725    0.049531567   -0.016640173   -0.071395338
23        G8    -0.48055680    0.020004552   -0.142365923    0.596631217
24        G8     0.01047043    0.008193747    0.220020063    0.062209973
25        G8    -0.42278889    0.025909955   -0.149761556    0.028808645
26        G8    -0.45167284    0.031815358   -0.134970291   -0.138197994
27        G8    -0.30725307    0.049531567    0.042524886    0.095611301
28        G8     0.24154207   -0.039049477    0.072107416   -0.104796666
29        G8     1.45466817   -0.003617059    0.064711783    0.296019267
30        G8    -0.01841352    0.002288344    0.020337989    0.028808645
31        G8     0.38596185    0.084963985    0.049920519   -0.037994010
32        G8     0.15489021   -0.080387298    0.020337989   -0.338605960
33        G8    -0.04729748    0.067247776    0.138668107    0.129012629
34        V4     0.27042603    0.031815358    0.049920519    0.195815284
35        V4    -0.07618143    0.037720761    0.020337989   -0.037994010
36        V4    -0.10506539    0.025909955   -0.083200864    0.396223251
37        V4    -0.01841352    0.126301805   -0.024035805    0.362821923
38        V4     0.01047043    0.031815358   -0.016640173   -0.138197994
39        V4     0.06823834    0.037720761   -0.038827070    0.262617940
40        V4    -0.16283329   -0.050860283   -0.038827070   -0.405408616
41        V4    -0.01841352   -0.039049477    0.005546724   -0.205000649
42        V4    -0.39390493   -0.003617059   -0.090596497    0.129012629
43        V4    -0.04729748    0.008193747   -0.009244540    0.195815284
44        V4     0.01047043   -0.039049477   -0.016640173   -0.205000649
45        V4     0.01047043   -0.003617059   -0.075805232   -0.004592683
46        V4     0.06823834    0.008193747   -0.090596497   -0.205000649
47        V4    -0.04729748    0.014099150    0.012942357   -0.071395338
48        V4    -0.22060120   -0.015427865   -0.075805232   -0.171599321
49        V4    -0.16283329    0.020004552   -0.061013967   -0.104796666
50        V4    -0.07618143    0.031815358   -0.038827070   -0.138197994
51        V4    -0.22060120    0.020004552   -0.112783394   -0.104796666
52        V4    -0.19171725   -0.033144074   -0.068409599   -0.071395338
53        V4    -0.16283329   -0.039049477   -0.090596497   -0.104796666
54        V4    -0.22060120   -0.009522462   -0.053618335   -0.037994010
55        V4    -0.13394934   -0.003617059   -0.075805232   -0.004592683
56        V4    -0.27836911   -0.044954880   -0.090596497   -0.238401977
57        V4    -0.04729748   -0.050860283    0.064711783    0.028808645
58        V4     0.01047043   -0.044954880    0.012942357   -0.305204632
59        V4     0.12600625   -0.068576492    0.042524886   -0.305204632
60        V4     0.06823834   -0.033144074   -0.061013967   -0.271803305
61        V4     0.06823834   -0.027238671   -0.061013967   -0.037994010
62        V4     0.32819394   -0.068576492    0.064711783   -0.372007288
63        V4     0.32819394    0.014099150    0.175646269    0.095611301
64        V4    -0.27836911    0.002288344   -0.068409599    0.195815284
65        V4     0.18377416    0.025909955    0.027733621    0.162413956
66        V4     0.55926557   -0.009522462    0.042524886    0.229216612
67        V4    -0.19171725   -0.009522462   -0.038827070    0.229216612
68        V4    -0.19171725    0.025909955   -0.009244540    0.396223251
69        V4     0.01047043    0.155828820    0.027733621    0.630032545
70        V4    -0.19171725    0.002288344   -0.031431438    0.463025906
71        V4    -0.01841352   -0.044954880   -0.046222702    0.496427234
72        V4    -0.07618143   -0.015427865   -0.031431438    0.062209973
73        V4    -0.13394934    0.008193747   -0.068409599   -0.071395338
74        V4    -0.39390493    0.037720761   -0.120179026    0.229216612
75        V4    -0.04729748    0.008193747    0.035129254   -0.071395338
76        V4    -0.27836911   -0.015427865   -0.061013967   -0.071395338
77        V4     0.70368535   -0.056765686    0.397515240   -0.205000649
78        V4     0.29930998    0.079058582    0.138668107    0.229216612
79        V4    -0.13394934   -0.056765686    0.020337989   -0.305204632
80        V4     0.21265812    0.025909955    0.035129254    0.396223251

   'data.frame':    80 obs. of  13 variables:
 $ Family           : Factor w/ 2 levels ""G8"",""V4"": 1 1 1 1 1 1 1 1 1 1 .                  
     $ SBI.max.Part.1   : num  -0.4806 0.126 0.0682 0.6748 0.6459 ...
 $ SBI.max.Part.2   : num  -0.0863 -0.0745 -0.0568 -0.0509 -0.0509 ...
     $ SBI.min.Part.1   : num  -0.1572 0.0573 0.0647 0.1535 0.0721 ...
 $ SBI.min.Part.2   : num  -0.439 -0.539 -0.539 -0.539 -0.472 ...
</code></pre>
"
"0.132877766396671","0.129492442570703","167363","<p>I have no training in Bayesian data analysis, so I can't wrap my head around how to start solving the following problem and am hoping you can help:</p>

<p>I am using linear regression to forecast the net scores (home - visitor) of (American) pro-football games from differences in team-strength scores (home - visitor). Those strength scores fall on a 0-100 scale, and they represent the percent chance that the team in question would beat another team selected at random from the 31 others in the league. The differences between those strength scores and the net game scores are both normally distributed.</p>

<p>Right now, I am using team-strength scores that are fixed for the entire season in a mixed-effects model that also includes random intercepts for each team as the home team. The strength scores are fixed because they come from a preseason survey. I would like to see if I can make the predictions more accurate by using Bayesian updating to allow that team-strength score to vary over the course of the season, as we learn more about how teams are performing relative to preseason expectations.</p>

<p>The single piece of information that strikes me as most useful in that regard is the cumulative sum of each team's prediction errors --- in other words, the cumulative sum of the differences between the team's predicted game performance (based on the preseason strength scores and where each game is played) and its actual game performance. </p>

<p>How might I go about doing that? In R, I have gotten as far as computing those cumulative errors, which turn out to be normally distributed for the season with a mean of ~0 and sd of ~50. I have tinkered with algebraic ways to adjust the strength scores as a function of that cumulative error. The forecasts based on those algebraic adjustments are slightly more accurate, but the approach seems clunky, and I'd like to use this problem as an opportunity to learn about Bayesian updating if I can. Any suggestions on how to do that in the context of this problem --- and, ideally, in R --- would be much appreciated.</p>
"
"0.17463559859131","0.170186411426254","168068","<p>I am using R for this analysis, and so examples and graphics will be produced in this language. I am willing to provide equivalent examples in similar languages if it will help someone, and am willing to accept answers in terms of other languages.</p>

<p>In this question, I intend to display graphs produced in order to verify assumptions, and ask for help in getting a better model. I understand that this may be considered too specific. However, it is my opinion that it would be helpful to have more examples of bad models and how to correct them on this site. If a moderator finds this not to be the case, I will happily delete this post.</p>

<p>I have conducted an initial linear model (lm) in R. It is multiple categorical regression with approx 100,000 cases, two categorical regressors and a continuous regressand. The goal of this regression is prediction: specifically, I would like to estimate prediction intervals. Find below some diagnostics of the initial model:</p>

<p>Residuals histogram (full) below. It may be difficult (impossible) to see, but there exist (sparse) values between 300 and 2000, as well as -50 and -500. Between -50 and 300, values are very dense. This indicates, to my understanding, heavy tails.</p>

<p><a href=""http://i.stack.imgur.com/FoGN7.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FoGN7.png"" alt=""Residuals Historgram""></a></p>

<p>Residuals histogram (partial) below. Same image as above, but zoomed to the dense area.</p>

<p><a href=""http://i.stack.imgur.com/Q9bBl.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Q9bBl.png"" alt=""enter image description here""></a></p>

<p>A normal Quantile Quantile (normal QQ plot) is found below. Again, according to the <a href=""http://stats.stackexchange.com/questions/101274/how-to-interpret-a-qq-plot"">holy grail of qqplots</a>, (super) heavy tails are indicated.</p>

<p><a href=""http://i.stack.imgur.com/TrATp.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/TrATp.png"" alt=""Initial QQPlot""></a></p>

<p>Below is predicted vs residuals. Clearly, funky stuff is going on, suggesting heteroscedasticity:</p>

<p><a href=""http://i.stack.imgur.com/oOMRU.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/oOMRU.png"" alt=""Resid Vs Predicted""></a></p>

<p>I first tried some transformations. BoxCox yields a value very close to zero. So I will try to take the log of the regressand (in accordance with <a href=""https://en.wikipedia.org/wiki/Power_transform#Box.E2.80.93Cox_transformation"" rel=""nofollow"">the Wikipedia page</a>). </p>

<p><a href=""http://i.stack.imgur.com/3IbMv.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/3IbMv.png"" alt=""boxcox""></a></p>

<p><strong>Log Transform:</strong></p>

<p>Log transformed histogram, looks a lot better, but we still have some skew:</p>

<p><a href=""http://i.stack.imgur.com/exSgd.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/exSgd.png"" alt=""Histogram of Log transform""></a></p>

<p>And the NormalQQ Plot. Still seems that the residuals are not normally distributed.</p>

<p><a href=""http://i.stack.imgur.com/JosvR.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/JosvR.png"" alt=""log QQPlot""></a></p>

<p>Logarithm transformed Residual vs Predicted. Seems we have some decreasing variance now, but I would be willing to accept this assumption.</p>

<p><a href=""http://i.stack.imgur.com/Sg4B9.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Sg4B9.png"" alt=""Log Resid Vs Predicted""></a></p>

<p>Other transformations I tried: raising regressand to powers 1/2, 1/3 and -1. None of these had satisfactory results; I choose not to include information about these transformations in order to save space, but will happily provide such information should it be requested.</p>

<p><strong>Here lie my questions:</strong></p>

<p>1) Is the solution to this problem simply to keep trying increasingly wacky transformations (ex: $1/log(x^{\pi/3})$)?</p>

<p>2) I have been looking (intermittently over a period of weeks) at Generalized Linear Models, which seem to allow a non-normal distribution of residuals. Unfortunately, I have not been able to understand them, and non of my (undergraduate statistics) peers have knowledge of them. If GLM's present a solution to this issue, I would be grateful if someone could explain them in this context. (Even if they are not a solution, I would be grateful for a simple explanation, or a reference to one).</p>

<p>2i) If GLM's are a good fit, I believe I would still need a distribution to model error by. What ways are there of detecting which (family) of distribution is the best fit for the residuals, after which I assume I can perform MLE to get the parameters? I've been having issues trying to evaluate heavy tailed distributions with respect to skew, because they tend not to have any moments, and so have $\infty$ or indeterminate skew.</p>

<p>3) Is there another class of models not aforementioned I should look into?</p>

<p>4) Is my current model sufficient for prediction intervals, despite the non-normality of residuals?</p>

<p>Some more information about the model: I am predicting a cost, thus the log transform is appealing in that my predicted values are positive reals.</p>

<p>I will be hanging around my computer all day, and have R gui open on my other monitor, so should be able to fulfill most requests for additional information.</p>
"
"0.155471754152787","0.160980229054196","168725","<p>This question relates to whether it is a good starting point for a cut point in binary classification with logistic regression to the use the mean of the binary response variable as the initial cut point rather than simply 0.5.</p>

<p>Traditionally when people use logistic regression, people with use 0.5 as the threshold to determine when the model predicts YES/positive versus NO/negative.</p>

<p>People may run into trouble when the model only predicts one ""answer"" when using an imbalanced training set.</p>

<p>One way of dealing with this is to balance the training set via oversampling or under-sampling and keeping the test holdout set with the original balance.</p>

<p>However, I suspect that a good starting point for a cut point appears to be the mean of the binary response variable.  Is this usually true?</p>

<p>I created two models, one on a balanced training set and another on the original imbalanced training set.
<code>print(table(actual=test$y, predicted=test$fit&gt;0.5))</code></p>

<pre><code>       predicted
 actual FALSE TRUE
      0  2359  500
      1    11  130
</code></pre>

<p>With the imbalanced training, I used the mean of the binary response variable:</p>

<pre><code>print(table(actual=test$y, predicted=test$fit&gt;0.0496))

       predicted
 actual FALSE TRUE
      0  2317  542
      1     7  134
</code></pre>

<p>If one just uses 0.5, it looks like the model is a complete failure:</p>

<pre><code>`print(table(actual=test$y, predicted=test$fit&gt;0.5))`

       predicted
 actual FALSE
      0  2848
      1   152
</code></pre>

<p>They both had a KS of 0.76, so it seems like sound advice.</p>

<p>Example R code:</p>

<pre><code>require(ROCR)
require(lattice)
#
x=1:10000/10000;
y=ifelse(runif(10000)-0.7&gt;jitter(x),1,0)
#y=ifelse(rnorm(10000)-0.99&gt;x,1,0)
mean(y)

s=sample(length(x),length(x)*0.7);

df=data.frame(x=x,y=y)


##undersample
train=df[s,]
train=rbind(train[train$y==1,],train[sample(which(train$y==0),sum(train$y==1)),])
    ##oversample
    train=df[s,]
    train=rbind(train[train$y==0,],train[sample(which(train$y==1),sum(train$y==0),replace = T),])
mean(train$y) #now balanced
    threshold=0.5
    test=df[-s,] #unbalanced
    mean(test$y)
#

ex=glm(y~x,train, family = ""binomial"")
summary(ex)
nrow(test)
test$fit=predict(ex,newdata = test,type=""response"")
    message(""threshold="",threshold)
    print(table(actual=test$y, predicted=test$fit&gt;threshold)) 

#+results
pred&lt;-prediction(test$fit,test$y)
perf &lt;- performance(pred,""tpr"",""fpr"")
ks.sc=max(attr(perf,'y.values')[[1]]-attr(perf,'x.values')[[1]])
plot(perf)
print(ks.sc); #ks.score


levelplot(fit~y+x,test,col.regions =  terrain.colors(100)[1:95]) 

#+ imbalanced approach
#############imbalance approach

train=df[s,]
threshold=mean(y)
message(""threshold="",threshold)
ex=glm(y~x,train, family = ""binomial"")
summary(ex)
test$fit=predict(ex,test,type = ""response"")
    summary(test$fit)
print(table(actual=test$y, predicted=test$fit&gt;threshold)) 

print(table(actual=test$y, predicted=test$fit&gt;0.5)) 

pred&lt;-prediction(test$fit,test$y)
perf &lt;- performance(pred,""tpr"",""fpr"")
ks.sc=max(attr(perf,'y.values')[[1]]-attr(perf,'x.values')[[1]])
plot(perf)
print(ks.sc); #ks.score


levelplot(fit~y+x,test,col.regions =  terrain.colors(100)[1:95]) 
</code></pre>

<p>I noticed a similar question asked <a href=""http://stats.stackexchange.com/questions/91305/how-to-choose-the-cutoff-probability-for-a-rare-event-logistic-regression"">How to choose the cutoff probability for a rare event Logistic Regression</a></p>

<p>I like the answer given here which states to maximize the specificity or sensitivity:
<a href=""http://stats.stackexchange.com/questions/25389/obtaining-predicted-values-y-1-or-0-from-a-logistic-regression-model-fit/25398#25398"">Obtaining predicted values (Y=1 or 0) from a logistic regression model fit</a></p>

<p>But I also suspect that the usual starting cut off of 0.5 is bad advice.</p>

<p>Comments?</p>
"
"0.0895861718290583","0.0873037869711973","173410","<p>I am working with a lasso regression with the glmnet package. I read these threads: <a href=""http://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia"">When conducting multiple regression, when should you center your predictor variables &amp; when should you standardize them?</a>, <a href=""http://stats.stackexchange.com/questions/19523/need-for-centering-and-standardizing-data-in-regression"">Need for centering and standardizing data in regression</a> and <a href=""http://stats.stackexchange.com/questions/86434/is-standardisation-before-lasso-really-necessary"">Is standardisation before Lasso really necessary?</a>.</p>

<p>Based on the responses I decided that I need to standardize my data before using it. I do have some questions however:</p>

<ul>
<li>Do I need to standardize the predictors and the responses or only the predictors?</li>
<li>I am using the function scale(myData, center = TRUE, scale = TRUE) for building the model, but I am wondering what do I do when I want to do predictions with a test data set. I think I should also standardize and center the test data, but how to I do that? Substracting the mean from the initial (training) dataset and the dividing it by the standard deviation of the initial dataset? </li>
<li>When I get a result do I need to ""backscale"" it (using the original mean and standard deviation) or do I already get the ""final"" result? </li>
</ul>
"
"0.0981367343026181","0.0956365069595007","173717","<p>I am an undergrad student and I'm super new to R! I have a data set that I have split into a training and test set. I obtained a multiple regression model from my training set, and now I want to use it to predict my test data. My dependent variable is Plant Species Richness (PSR), and my original data set had 4 independent variables (Area, AdjacentWetlands, Roads, and Forest) but my model is only using Area and Forest: <code>LM&lt;-lm(PSR~Area+Forest, data=Wetlands)</code>. How do I use this model to predict PSR in my test set? And then how do I assess whether it is a good prediction or not?</p>
"
"0.0462621002059225","0.0676252226000574","174110","<p>I'm growing a regression tree with the <code>rpart</code> function in R (package of the same name). I would like to be able to choose myself the number of nodes (not the depth of the tree, but the actual number of nodes), either by growing or pruning afterwards.</p>

<p>The problem is that in the cptable the number of nodes jumps and skips some numbers (from 1 to 5 here). I would like, for instance a tree giving 3, 4 or 5 distinct prediction values. </p>

<pre><code>set.seed(1)
df=data.frame(x=rnorm(100), y=rnorm(100))
tree=rpart(data=df, y~x)
tree$cptable
####           CP nsplit rel error   xerror      xstd
#### 1 0.03357572      0 1.0000000 1.013942 0.1337594
#### 2 0.02899422      1 0.9664243 1.187690 0.1620186
#### 3 0.01488440      5 0.8504474 1.188779 0.1632158
</code></pre>

<p>And I can't find the control parameter to set this, if it is even possible (growing or pruning).
Can someone help here please?</p>
"
"0.160256616043001","0.156173761888606","174136","<p>I have a dataset of a metric predictor variable $X$, and an ordered categorical predicted value $Y$ for several individuals. The dataset are from two groups $G_1$ and $G_2$. I want to estimate $Y$ from $X$, and I want to be able to compare the forecast accuracy of models, in group and individual level. For example, I want to know if these models helps to estimate $Y$ from a $X$ for a new user of a category, or a new experiment from the same user of a known category?</p>

<p>In <a href=""https://en.wikipedia.org/wiki/Ordered_probit"" rel=""nofollow"">ordered probit</a>, we suppose that $Y^*$ is the exact but unobserved dependent variable, and $X$ is the vector of independent variables, and $\beta$ is the a regression coefficient which we wish to estimate.</p>

<p>$Y^* = \mathbf{x}' \beta + \epsilon$</p>

<p>We can not observer $y*$ directly, but we instead can only observe the categories of response:</p>

<p>$
Y= \begin{cases}
0~~ \text{if}~~y^* \le 0, \\
1~~ \text{if}~~0&lt;y^* \le \mu_1, \\
2~~ \text{if}~~\mu_1 &lt;y^* \le \mu_2 \\
\vdots \\
N~~ \text{if}~~ \mu_{N-1} &lt; y^*.
\end{cases}
$</p>

<p>I came across this article from Gelman et al. that describes Bayesian Hierarchical Model: <a href=""https://en.wikipedia.org/wiki/Ordered_probit"" rel=""nofollow"">Multilevel (Hierarchical) Modeling: What It Can and Cannot Do</a>, which has been implemented in Python <a href=""http://nbviewer.ipython.org/github/fonnesbeck/multilevel_modeling/blob/master/multilevel_modeling.ipynb"" rel=""nofollow"">here</a>.</p>

<p>I am processing data in R, and I have selected a <strong>thresholded Bayesian hierarchical model</strong> to use with the <strong>generalized linear model</strong>. I have calculated the parameters of it using MCMC. My question is that how should I compare accuracy of ordered probit, and the equivalent Bayesian hierarchical model in R?</p>

<p>Gelman has used <a href=""https://en.wikipedia.org/wiki/Root-mean-square_deviation"" rel=""nofollow"">RMSE</a> for comparison using cross-validation. First he <em>removed single data points and checked the prediction from the model fit to the rest of the data, then removed single counties and performed the same procedure. For each cross-validation step, we compare complete-pooling, no-pooling, and multilevel estimates.</em></p>

<p>I have done MCMC simulation using RJags, which gave me the posterior distribution of the parameters, but how can I compare posterior distribution with a single point estimate of <strong>ordered probit</strong> to compare accuracy? Should I do as Gleman did and use RMSE? How? Or should I compare posterior distribution with results of several experiments with ordered probit? Is <a href=""http://www.stat.columbia.edu/~gelman/presentations/ggr.pdf"" rel=""nofollow"">posterior predictive check</a> usable here? I usually prefer cross-validation, but I don't know how to do this here.</p>

<p>PS: The notion of <strong>Goodness of fit</strong> in Bayesian analysis is ambigious to me. <a href=""http://people.stat.sfu.ca/~tim/papers/survey.pdf"" rel=""nofollow"">This paper</a> states:</p>

<blockquote>
  <p>GOODNESS-OF-FIT:</p>
  
  <p>In Bayesian statistics, there is no consensus on the
  correct"" approach to the assessment of goodness-of fit. When Bayesian
  model assessment is considered, it appears that the prominent modern
  approaches are based on the posterior predictive distribution (Gelman,
  Meng and Stern 1996).</p>
</blockquote>
"
"0.0566592699670073","0.0552157630374233","174970","<p>I just started learning churn analysis, and I'm trying to analyze the churn data found here:
<a href=""http://www.sgi.com/tech/mlc/db/churn.data"" rel=""nofollow"">http://www.sgi.com/tech/mlc/db/churn.data</a>
in R, and there are a few concepts I don't entirely get and I would be very grateful if I could get them answered. </p>

<p>I used Kaplan-Meier Estimators since the data is right-censored, and obtained the churn rate.</p>

<p>Now when I tried analyzing different trends (I focused on international plan and voicemail plan), I got a p-value of 0 for the international plan! Is this possible? </p>

<p>I further performed a Cox regression on the data, again, just with those two variables, and ran coxzph, and it said that both p values were below 0.05. So they were basically useless? Should I throw them out when making predictions?</p>

<p>And how does one get to building a predictive model with something like this?</p>

<p>Thank you very much! Here's a copy of my code:</p>

<pre><code>library(survival)
dat &lt;- read.csv('/Users/priyanksmehta/Desktop/churn.csv')
dat$survival &lt;- Surv(dat$AccountLength, dat$Churn == 1)
plotting &lt;- survfit(survival ~ intlact, data = dat)
plotting2 &lt;- survfit(survival ~ vmailact, data = dat)
plot(plotting2, conf.int = FALSE, mark.time = FALSE, xlab = 'Subscribed since', ylab = 'Churn rate', col = c('blue', 'red'))
lnames &lt;- c('Voice Mail Activation', 'No Voice Mail')
legend('topright', lnames, col = c('blue', 'red'), lty = 1)
logrank &lt;- survdiff(formula = survival ~ intlact, data = dat)
logrank2 &lt;- survdiff(formula = survival ~ vmailact, data = dat)
print(logrank)
print(logrank2)
results &lt;- coxph(survival ~ intlact + vmailact, data = dat)
print(results)
coxsig &lt;- cox.zph(results)
print(coxsig)
</code></pre>
"
"NaN","NaN","175127","<p>I am using R to perform linear regression.  I have seen ways to calculate prediction intervals, but these depend on homoscedastic data.  Is there a way to calculate prediction intervals with heteroscedastic data?</p>
"
"0.132877766396671","0.129492442570703","175770","<p>This question is more of theoretical. I am not sure if this is the right place, but still giving it a try. </p>

<p>I have two variables &mdash; direct cost and indirect cost. When sales persons go for a sales pitch to a customer they know about direct cost that they are going to incur for this service, but they don't know much about indirect cost (they will come to know about it in latter stages). An estimate of indirect cost at this stage will be valuable for sales persons. </p>

<p>I am trying to predict indirect cost as a function of direct cost. I am doing this via a simple linear regression. I plotted scatter plot between direct cost and indirect cost and see a <strong>good linear relationship</strong> between them. I also see that direct cost and indirect cost are <strong>highly corelated</strong> to each other with correlation coefficient as 0.98, so I expected a very good prediction accuracy. But surprisingly, my prediction accuracy is not so good. I have around 200,000 points in my training data and average prediction error on training data is 17 %. Though adjusted R-Square value is 0.97. I am using <code>lm()</code> function from R.       </p>

<p>My question is that in case of simple linear regression, in general, should we expect better prediction accuracy if dependent and independent variables are highly correlated or is it my misconception? If we expect good accuracy, am I missing something here. Please note that I have also tried centering these variables around mean. </p>
"
"0.140224539037626","0.156173761888606","178861","<p>This is my first question in Cross Validated.  I was redirected here from StackOverflow, so I hope this is the right place and I get the question right...  I've searched for this topic in the forums and didn't find any similar question, so I hope somebody can help me.</p>

<p>I am trying to fit a loess model with two predictors, one of them is locally weighted while the other is parametric.  Once the model is fitted, I want to get the slope of the regression line on the parametric predictor, conditioning on a certain value of the non-parametric predictor.  When computing this slope with whichever two values from the regression line, I understand I should get a constant value corresponding to the slope.  However, if I compute the slope with every two successive values, I get a curve resembling much of a parabola.  Here you have some code for showcasing this issue:</p>

<pre><code>## DATA

criterion &lt;- c(
  -1.8914741214789, -0.864604956700496, 2.43013372852099, -1.32227168040904, 
  0.861585875211724, -1.25506145955845, 1.1201893940246, -1.18560159611826, 
  1.24871681364416, -1.24362634687504, -2.22058652097061, -0.67920682239687, 
  -1.14096010679208, 0.228758533000768, -1.3607742780652, 0.473165865126464, 
  0.0438948075908679, -1.14355404117161, 2.60406860120487, 0.539583593348819, 
  -0.599388766026817, -1.14918693916554, -0.17334788506616, -0.836478866743926, 
  2.88908329995278, -0.401016464464891, -0.292311619619775, 1.12804091879547, 
  3.6733647991105, -3.31190363769332, -0.672558641084861, 0.498844902537884, 
  -0.037062115762172, -1.02017987978252, -0.854805324374525, 1.34168735130207, 
  -0.242996720017973, 1.14871933640721, -0.736312690622741, -2.51965992948912, 
  -3.16327863554555, -0.269020543067839, 0.552150179356176, 0.320523449469915, 
  1.14736382025547, 0.891590469554733, -0.717678520852477, 3.15631635301073, 
  -0.225648864790169, -1.35189421566795, -0.558073572773821, -1.33356547103824, 
  -2.01215450549957, 1.63719762873429, -2.0218275161466, 0.513289109719022, 
  -1.30263029019454, 1.85949704911488, -1.22544429584118, 0.336732253617053, 
  -2.45126282746359, -4.94155955259729, 0.743231639698684, 1.04320562270113, 
  3.99232225357353, -1.07752259470057, -0.353671379249384, -0.748973055922694, 
  0.467443998802691, 0.966013090920868, 1.32739645813748, 1.07159468103619, 
  -1.8542024758483, 0.360922743179635, -4.99642432601298, 0.596072047320551, 
  -1.48256500350222, -0.251689130094422, -0.104867519535428, -3.23675187957067, 
  1.15657910171856, -0.640772355492231, 2.21198640279181, -0.229386564567888, 
  -2.8014148535931, 0.325261825780768, 1.65431768179619, -0.701353356393564, 
  1.56301740126489, -2.91989037858617, 0.560634128846807, -3.40972988669857, 
  0.519955616184439, -0.673752119923202, -0.126511467211613, -1.49156456253545, 
  2.68041989003066, -3.18246878051744, -1.05338046600476, -0.122679130411665, 
  0.619202563903638, -2.80132012240656, -1.50106228060585, -1.78428153598023, 
  -0.17959372353835, -3.7657930817963, 1.74830598714522, 0.199267717912346, 
  -0.187088254090319, -0.431926901631399, -2.50168001668916, -0.715294537723936, 
  4.8050892573889, 3.48017935641437, -2.29413209640673, 1.88045620792631, 
  -0.125724128270772, -0.514660621563394, 1.28920199656138, -0.888250921411933, 
  -1.53336797414911, 0.566890809767711, 2.18492239723917, 1.45986142278563, 
  2.29475550546227, -2.91360806155925, -1.28474245565384, -1.15236199251384, 
  -2.68344935749574, -1.0406761060411, 0.236606541573282, -1.0577344636865
)

lw.predictor &lt;- c(
  3.97543828892376, 3.74367045733871, 2.8031293667213, 3.12721154621725, 
  3.57809163186571, 3.85490258924953, 3.04509486547235, 3.06527167000886, 
  3.42172751371031, 3.48342454710053, 3.03382754767655, 3.29840143930276, 
  3.42532870135535, 3.3466401061363, 4.19719410513314, 4.27624851591474, 
  3.90521253346176, 3.66434131328865, 3.78008480009251, 3.26961939052607, 
  3.37557706887951, 3.07887188021758, 3.20615845753053, 3.25681582455448, 
  3.07575583761175, 3.4678563115072, 3.63412290863538, 2.7341072520575, 
  3.04486992771651, 4.0244118093156, 2.75978954925269, 2.94469571886678, 
  3.64916954335701, 3.23529338509991, 3.09993371559714, 3.92201374051927, 
  2.63693470862178, 3.19438720086359, 4.10395732841177, 4.14695795420843, 
  3.37963279191973, 2.75059146814963, 3.05430305033533, 4.07380539679535, 
  3.41070032578776, 3.52175236580135, 3.9922870844146, 2.83689005875791, 
  3.59280102122038, 3.744585131656, 4.07464596327428, 2.94632345777042, 
  3.79563556141373, 2.92157773101387, 3.60631105869347, 3.90380916892684, 
  3.4349134104059, 2.86428168814471, 2.80459505537921, 3.52738795051305, 
  3.90100092529829, 3.95557522270349, 2.88144750533953, 3.4177217437656, 
  2.72281079587565, 3.44307922077723, 3.58191805968554, 2.85374063580388, 
  3.61825659978916, 3.35154840518957, 2.78055872970075, 3.31559205647068, 
  3.72845408487401, 3.45439961676827, 3.47673283886995, 3.38348122582045, 
  2.88524826215121, 3.37314129436951, 4.17608980116535, 2.78621853030773, 
  4.23989532049638, 2.65428059546312, 2.75954135557898, 3.90836826127649, 
  3.70911502202012, 3.9502037401938, 2.72934334016319, 3.56908337873796, 
  3.53107535330031, 4.10445798400541, 3.37029733251965, 2.77784779211612, 
  4.00205426701893, 3.390559011573, 3.75061638769833, 3.67591196400612, 
  4.00034245109432, 4.19507212536165, 2.64290209936905, 2.84965751366817, 
  2.93584367468566, 3.5792399897338, 3.87103752330681, 4.11112756588916, 
  3.69820393282565, 3.47909608792875, 3.021611653787, 3.38833615773409, 
  3.53688974115618, 3.86802841711593, 3.04014239032067, 3.83441517392514, 
  2.78055872970075, 2.95676609717858, 3.46963343371941, 2.69652236718191, 
  4.21169279083643, 3.59508797308864, 2.90229007358549, 3.74055889165213, 
  4.22533130742836, 3.10942006661772, 2.81532012340698, 3.39540382330109, 
  3.11580153059886, 2.73435775434875, 3.80464748245529, 3.63431137604942, 
  4.09744323888573, 2.77908036429911, 3.30047734198123, 3.7238586557053
)

par.predictor &lt;- c(
  94.3333333333333, 105.333333333333, 135.666666666667, 113.666666666667, 
  116.333333333333, 112.666666666667, 117.333333333333, 100.333333333333, 
  118, 118.333333333333, 119.666666666667, 109.666666666667, 116.333333333333, 
  107.666666666667, 110.666666666667, 97, 99.6666666666667, 108.666666666667, 
  118.333333333333, 119, 106.333333333333, 121.666666666667, 103, 
  132, 99, 121.666666666667, 99.6666666666667, 104.666666666667, 
  124.333333333333, 106.666666666667, 118.666666666667, 111, 125, 
  101.666666666667, 101, 103, 102, 108.333333333333, 119.333333333333, 
  131.333333333333, 105.666666666667, 118.666666666667, 124.333333333333, 
  111.333333333333, 108.333333333333, 126.666666666667, 111, 107.333333333333, 
  118.666666666667, 120.333333333333, 114, 117.666666666667, 118, 
  104.666666666667, 115.333333333333, 117.666666666667, 101.666666666667, 
  118, 103.333333333333, 120, 105, 106.666666666667, 104.333333333333, 
  112.333333333333, 109.333333333333, 103.333333333333, 114, 112, 
  122.333333333333, 115, 135.666666666667, 102.666666666667, 116, 
  115.333333333333, 118, 107, 104, 113.666666666667, 130, 128.333333333333, 
  110.333333333333, 127.666666666667, 129.333333333333, 107, 110.666666666667, 
  97.3333333333333, 120.333333333333, 90.6666666666667, 122.333333333333, 
  104, 93.6666666666667, 102.333333333333, 111.333333333333, 121.666666666667, 
  127.666666666667, 115, 103, 91.6666666666667, 150.666666666667, 
  127, 126.333333333333, 111.666666666667, 117.666666666667, 109, 
  103, 104, 103.666666666667, 109, 122.666666666667, 122, 116.333333333333, 
  117.333333333333, 102, 156, 105.666666666667, 106.333333333333, 
  124.333333333333, 105.333333333333, 121.333333333333, 92.6666666666667, 
  117.666666666667, 122.666666666667, 88.3333333333333, 119, 121.333333333333, 
  97, 107, 109.333333333333, 113.666666666667, 103.666666666667, 
  117, 112.666666666667
)

# Predictor variables are standardized prior to fitting the loess model
data &lt;- data.frame(criterion = criterion, lw.predictor = scale(lw.predictor), par.predictor = scale(par.predictor))


## MODEL FIT

lwr.fit &lt;- loess(
  criterion ~ lw.predictor * par.predictor, data,
  parametric = ""par.predictor"",
  span = .7,
  family = ""gaussian"", degree = 2, drop.square = ""par.predictor"",
  normalize = FALSE
)


## PREDICTION ON GRID

# Statistics to standardize axes (based on the sample)
par.mean &lt;- mean(par.predictor)
par.sd &lt;- sd(par.predictor)
lw.mean &lt;- mean(lw.predictor)
lw.sd &lt;- sd(lw.predictor)

# Axes (standardized with the sample parameters)
pred.lw.axis &lt;- (sqrt(seq(7, 18.25, by = 1/12)) - lw.mean) / lw.sd
pred.par.axis &lt;- (89:156 - par.mean) / par.sd

# Grid generation
std.prediction.values &lt;- expand.grid(
  lw.predictor = pred.lw.axis,
  par.predictor = pred.par.axis
)

# Prediction
lwr.prediction &lt;- predict(lwr.fit, std.prediction.values, se = TRUE)


## GRAPHIC REPRESENTATION OF THE PARAMETRIC REGRESSION

# Slope computing and plotting:

# Values for the parametric predictor, conditioned on the first value of the locally weighted predictor
test.values &lt;- lwr.prediction$fit[1, ]

# Slope computed with every two successive values of the parametric predictor
slopes &lt;- (test.values[-1] - test.values[-length(test.values)]) / (pred.par.axis[-1] - pred.par.axis[-length(pred.par.axis)])

# Plotting the results
plot(slopes, type = ""l"", ylab = ""Slopes"")
</code></pre>

<p>Well, if you run this code, you will get the plot of a (near) parabolic curve as I said.  I may be getting it wrong, but I think the plot should show a constant line.  That is very weird in my opinion; one could think it is due to quantification error, but I doubt this as it would be somehow random and the values much smaller.  On the other hand, I have tried also changing the parameters of the loess model, but the problem seems to be persistent, and happens also with many different data sets.</p>

<p>Thank you so much in advance for your help.</p>
"
"0.0400641540107502","0.0390434404721515","179541","<p>As the complexity parameter is calculated? What is the meaning of it?</p>

<p>From what I read, the cp is a value at which the tree makes divisions in the nodes until the reduction in the relative error is less than a certain value.</p>

<p>There are places I read that say the CP affects only the growth of the tree and others say that interferes with pruning too. For min appears that it interferes only in growth but not sure.</p>

<p>I am using rpart () package to create trees, in the case of the classification tree exists missclassification rate to evaluate the ratings, but in the case of regression is not anything to evaluate the predictions beyond the MSE?</p>
"
"0.0801283080215004","0.078086880944303","179748","<p>I have a dataset, say $A$:</p>

<pre><code>x    y
20   3.4
30   3.3
35   4.5 
</code></pre>

<p>I am fitting a regression model, a mixed model of R's <code>lme4</code> family to be exact, to predict $y$ given $x$. 
I have a set of <code>newdata</code> which don't have an observed $y$. 
Let $\hat{y}$ be the predicted value for this set $B$:</p>

<pre><code>x   yhat
20   3.3
100  6.6 
</code></pre>

<p>I need to report whether the predicted value is expected to be reasonably accurate. 
As you can see in dataset $B$, <code>x[2]</code> is somewhat of an outlier, and therefore $\hat{y}$ is also a value which is rare.
It happens to fall outside the 95% confidence interval of predicted values. $\hat{y}$ is very close to a real observation , in real life. </p>

<p>What kind of metric is used to report that $\hat{y}$ is actually quite a good prediction? 
I hope my question is clear even though I've struggled to explain it...</p>

<h3>Edited to add in response to comment below:</h3>

<p>The model has only one fixed parameter ($x$, continuous) while also having a random effects group parameter . 
Model looks like this </p>

<pre><code>LMER2&lt;-lmer(y~x + (1 |group), training_data)
</code></pre>

<p><code>lme4</code> does give me the coefficients and standard errors , and I have used the <code>bootMer</code> function to calculate $\hat{y}$ confidence interval following <a href=""http://www.r-bloggers.com/confidence-intervals-for-prediction-in-glmms/"" rel=""nofollow"">this article</a>.</p>
"
"0.0400641540107502","0.0390434404721515","179898","<p>I have a clinical dataset (1400 cases) and I applied 4 data mining techniques (ANN, Decision Tree, SVM, Logistic Regression) to predict the binary outcome (Yes, No).</p>

<p>Now, I want to improve prediction accuracy through ensemble methods.<br>
What are the criteria to choose which model can be combined with another model?
And how can that be done in R? Can I use the ""caret"" package?</p>
"
"NaN","NaN","180337","<p>I always report odds ratios when using logistic regression for predictions. 
I wanted know is it meaningful to report odds ratios when modeling with gradient boosting approach? 
I am using gbm package in R to make the predictions.</p>

<p>Thanks!</p>
"
"0.0981367343026181","0.0956365069595007","180546","<p>I am trying to use Random Forest regression. I have a response variable:</p>

<pre><code>y = rnorm(10000, mean=0, sd=3)
</code></pre>

<p>And a few predictor variables (which are just the response with added noise):</p>

<pre><code>x = data.frame(v1=y + rnorm(10000, mean=0, sd=3), v2=y + rnorm(10000, mean=0, sd=3), v3=y + rnorm(10000, mean=0, sd=3))
</code></pre>

<p>I build the random forest:</p>

<pre><code>r = randomForest(x, y)
</code></pre>

<p>The model is good, explaining ~73% of the variance. However, when I look at the residuals:</p>

<pre><code>plot(y, y - r$predicted)
</code></pre>

<p><a href=""http://i.stack.imgur.com/3OF4z.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/3OF4z.png"" alt=""Instead of being centered around zero, the residuals are correlated with the response variable""></a></p>

<p>Instead of being centered around zero, they are correlated with the response variable. It seems that the model should correct this. Maybe, since each OOB prediction is an average, this behavior is some kind of ""regression to the mean""? Does anyone know why this happens? Is anything I can do about it?</p>

<p>I am trying to build a model and use the residuals to estimate something. Right now, they are useless because they only reflect the value I'm trying to predict. If anyone can help, I'd really appreciate it!</p>
"
"NaN","NaN","180580","<p>To understand my logistic regression fit and identify non linear effects, I plan to estimate the conditional density and then calculate the log odds comparing to log odds from logistic regression. To me  this is the equivalent of scatter plot of single  independent variable vs dependent and prediction. </p>

<p>A) Does this seem like the right approach? </p>

<p>B) I am using R, and I am surprised that there is no package already doing this? </p>
"
"0.0693931503088838","0.0676252226000574","180813","<p>I know linear regression is the workhorse of machine learning. I understand the internals of it and I am playing with some real data samples.</p>

<p>Obviously using a simple line (polynomial degree = 1) is not very useful for most of the datasets, my understanding is that as I increase the polynomial degree I will</p>

<ul>
<li>Get a more accurate prediction</li>
<li>Eventually will face the danger of overfiting</li>
</ul>

<p>Now, I have been playing with R and some datasets and this is what I got...</p>

<pre><code>stock &lt;- EuStockMarkets[, 'DAX']
plot(stock)
model &lt;- lm(stock ~ lm(poly(time(stock), 1, raw=TRUE)))
points(time(stock), predict(model), type=""l"", col=""blue"", lwd=2)
model10 &lt;- lm(stock ~ poly(time(stock), 10, raw=TRUE))
points(time(stock), predict(model10), type=""l"", col=""red"", lwd=2)
text(1994, 5000, paste(""Degree, blue=1, red=10""), pos=2)
</code></pre>

<p><a href=""http://i.stack.imgur.com/0BLlF.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/0BLlF.png"" alt=""linear regression output""></a></p>

<p>Now, the red line is definately a muuuuch better fit thant the blue one, that said, two questions come to my mind</p>

<ol>
<li>The red line is almost always (if not always) ascending, meaning that technically any time would be good to buy shares, that does not represent the truth (from 1994 to 1995 there are ups and downs, not to mention 1997 to 1998)</li>
<li>Does R automatically apply regularization to prevent overfiting.</li>
</ol>

<p>I am fully aware that this is a very simplistic example (normally I would use more features, not just the date, in order to predict the price.</p>

<p>Would a more sophisticated tech such as neuronal network provide a better output here?</p>
"
"0.113318539934015","0.110431526074847","180992","<p>I'm building a machine learning (random forest) regression model to predict flow in a river, using rainfall, relative humidity, air temperature and certain other climatic variables. Since flow on a particular day (<code>flow_t</code>) is highly correlated with flow on previous day (<code>flow_t_1</code>), I want to include lagged flow in the model formulation.</p>

<p>In case I build the model this way:</p>

<pre><code>require(randomForest)
flow.rf=randomForest(flow_t~flow_t_1+temp+humidity..........)
</code></pre>

<p>How can I use the above model for predictions? 
Since the input dataset for prediction will not have the flow variable, I cannot include its lagged version in the prediction call. I know that the <code>dynlm</code> package can be used to perform 'autoregressive distributed lag modeling' to include lagged dependent variables, but how can this be done for machine learning models? Or even for other statistical modeling techniques, like GLMs and GAMs?</p>
"
"0.114024581281567","0.123466199581199","181065","<p>In some previous asked questions, I was told to not delete the outliers, because they contain valuable information.</p>

<p>After testing different regression, I came to the conclusion that until now, the <code>MARS</code> regression delivers the ""best responses"".</p>

<p>I know that <code>MARS</code> is very <em>robust</em> and there is no <em>a priori</em> knowledge about the data distribution needed.</p>

<p>But there are some question which I have about the parameters.</p>

<p>I'm using the <code>earth</code> function implemented in <code>R</code></p>

<p><strong>data set:</strong>
   <a href=""http://www.filedropper.com/data_8"" rel=""nofollow"">file</a></p>

<p>So I've got 5 variables, <em>price, livingArea, area, discrete, dummy</em> and I'm trying to explain <code>price</code> using the other ones.</p>

<p><a href=""http://i.stack.imgur.com/gGf4G.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/gGf4G.png"" alt=""enter image description here""></a></p>

<p>as you can see, there are some outliers and a <code>log</code> doesn't really solve the problem.
Due to the fact that <code>area</code> can be <code>null</code>, a <code>log</code> won't be a good transformation idea. </p>

<p><strong>what I do:</strong></p>

<p>Because the answers from other questions suggested to use the raw data, 
I'm running now the regression through my data without doing any changes to it.</p>

<p>so my regression formula looks like this:</p>

<pre><code>earth(price ~ ., data = data[,-1], weights = weights, penalty = -1)
</code></pre>

<p>I'm setting <code>penalty = -1</code> because I saw that doing this, the method defines more knots and also the results look better.</p>

<p>Also I tried to define the variables <code>discrete</code> and <code>dummy</code> as <code>factors</code> and use them as follows in the regression:</p>

<ol>
<li>independent</li>
<li>livingArea * discrete or livingArea : discrete and <code>dummy</code> as independent</li>
<li>the same as at <code>3.</code> but changing <code>discrete</code> with <code>dummy</code></li>
<li>livingArea * discrete * dummy </li>
</ol>

<p>I must say that I didn't expect, that a regression with this variables as factors, will return such ""bad"" results.</p>

<p><strong>what I want:</strong></p>

<p>I want to use the model in order to predict the value of new data.</p>

<pre><code>    livingArea area discrete dummy
1         87    0        7    0.5
</code></pre>

<p>The prediction of this observation should be <code>~ 330000</code>, but with what I'm doing now, I ain't coming not even close to this value.</p>

<p>I think that, having more knots increases the precision of the result.</p>

<p><strong>questions:</strong></p>

<ul>
<li>I don't really understand the parameters</li>
<li>I created my model with different values for the <code>pmethod</code>, but the result was always the same. what's the point in choosing a method when the result will be the same?</li>
<li>how could I determine if I have to set/change the values of different parameters like <code>thresh</code>, <code>minspan</code>, <code>nk</code>, etc.</li>
</ul>
"
"0.0566592699670073","0.0552157630374233","182509","<p>I am using logistic regression (with R) for detecting fraudulent transactions. So far I am achieving a relatively good ratio of success (f-score).</p>

<p>However I have noticed something, once I have my model built, the threshold that gives me the best f-score is to consider something as fraudulent if the logit function is bigger than 0.059 (I started with 0.5). </p>

<p>For the record, I am using 7099 observations as training examples and 3042 as testing data, in total I am using 6 features/columns for prediction (planning to add a couple more)</p>

<p>Now my questions are the following</p>

<ol>
<li>Am I doing something terribly wrong so that I have to use such a low limit to start labeling transactions as fraudulent?</li>
<li>That said, the vast majority of transaction are legitimate, does it justify the low threshold for the labeling (again, 0.059) ?</li>
<li>Would it be worth to explore other algs such random forest or neuronal networks?</li>
</ol>
"
"0.0801283080215004","0.078086880944303","182595","<p>I'm trying to use sparse linear model for my data,input x(29*50),output y(29*1). In R, the package of <strong><em>glmnet</em></strong> can be used. </p>

<p>Firstly, cv.glmnet() choose lambda and coefficients(at min error), here with leave-one-out cv method,and then plot it. </p>

<pre><code>cv.fit = cv.glmnet(x,y,family=""gaussian"",nfolds=29)

plot(cv.fit)
</code></pre>

<p><a href=""http://i.stack.imgur.com/PEEeb.png"" rel=""nofollow"">the plot of mse aganist log(lambda) in cv model</a></p>

<p>Next, print the coefficients</p>

<pre><code>coef(cv.fit,s=""lambda.min"")
</code></pre>

<blockquote>
  <p>51 x 1 sparse Matrix of class ""dgCMatrix""               </p>

<pre><code>              1
</code></pre>
  
  <p>(Intercept)   267.7241</p>
  
  <p>cluster_0  .<br>
  cluster_1     .<br>
  cluster_2     .<br>
  cluster_3     .<br>
  cluster_4     .<br>
  ...</p>
  
  <p>cluster_47    .<br>
  cluster_48    .<br>
  cluster_49    .  </p>
</blockquote>

<p>Finally, to measure the model's ability for prediction, accuracy is calculated(defined as 1 minus average absolute error divided by numeric range of y)</p>

<pre><code>py &lt;- predict(cv.fit,newx=x,s=""lambda.min"")
py
</code></pre>

<blockquote>
  <p>V1     267.7241</p>
  
  <p>V2     267.7241</p>
  
  <p>...</p>
  
  <p>v29    267.7241  </p>
</blockquote>

<pre><code>ave_abs_error &lt;- mean(abs(py-y))
n_range &lt;- max(y)-min(y)
acc &lt;- 1-ave_abs_error/n_range
acc
</code></pre>

<blockquote>
  <blockquote>
    <p>0.918365</p>
  </blockquote>
</blockquote>

<p>Although the acc(0.918365) is very high, there is a serious problem. As seen from the plot above, the lambda.min is very large(73.03439),and all coefficients  are zero(only with intercept value 267.7241), all predicted py are the same as intercept.
That's really weird! </p>

<p>I searched lots of threads in forum, here<a href=""http://stats.stackexchange.com/questions/72251/an-example-lasso-regression-using-glmnet-for-binary-outcome"">http://stats.stackexchange.com/questions/72251/an-example-lasso-regression-using-glmnet-for-binary-outcome</a> explains that there is no local min for too few observations and all coefficients were shrunk to zero with the shrinkage penalties.</p>

<p>Does anybody has other interpretations?</p>

<p>Thanks in advance!</p>
"
"NaN","NaN","182610","<p>Let's assume my dataset</p>

<pre><code>D1 has the variables x1, x2, x3, x4, x5, x6, x7, x8, x9, x10
</code></pre>

<p>I have used Gradient boosting regression on this and want to score on a new data set having same set of variable(of-course) but the <strong>sequence are different</strong>.</p>

<pre><code>D_scoring has the sequence like x1, x5, x8, x3, x4, x7, x9, x2, x6, x10
</code></pre>

<p><strong>Does this difference in sequence will hamper my prediction(accuracy</strong>).</p>

<p>If yes, what is the reason behind that ?</p>

<p><em>P.S: The reason behind the question is i have done a GBM regression the socring was good with the Validation and CV but while scoring with new data the learning is very bad.</em></p>
"
"0.113318539934015","0.110431526074847","183337","<p>I am trying to test the predictive accuracy of regression using training sets of varying sizes.</p>

<pre><code>Y &lt;- rnorm(100)
X &lt;- replicate(5, Y+rnorm(100) )   
data &lt;- as.data.frame(cbind(Y,X))
</code></pre>

<p>Let's say the training set is 2% of the data:</p>

<pre><code>train &lt;- nrow(data) * 0.02
test &lt;- nrow(data) - train 
</code></pre>

<p>I repeat the process for 1000 times:</p>

<pre><code>MSE &lt;- vector()
for( i in 1:1000){

train.elements &lt;- sample(1:nrow(data),train)
train.set &lt;- data[train.elements,]
test.set &lt;- data[setdiff(1:nrow(data), train.elements),]

# then I fit a regression model:

    model &lt;- lm(train.set[,1]~ train.set[,2]+train.set[,3]+train.set[,4]+train.set[,5])

#I now use this model to predict the values in the test set:
predictions &lt;- predict.lm(model,data=test.set)

MSE[i] &lt;- mean((test.set[,1] - predictions)^2)

}
</code></pre>

<p>My problem is that due to the small sample size the MSE sometimes is extremely huge.</p>

<p>Is this normal? I am unable to plot a curve of the MSE as a function of training set size because the MSE for small sample sizes are so large.</p>
"
"0.105999788000636","0.103299233817667","184699","<p>I have a question regarding p-values in the linear regression.The purpose of using the linear regression model is mainly to predict future values with accuracy. As I read, when the purpose is prediction, I can somehow not may too much attention to multicollinearity and the assumption of the linear model.
Some of my predictors are categorical variables: can I include the levels having non-significant p-values when I am only interested in prediction?</p>

<p>The output is as follows:</p>

<pre><code>lm(formula = log(cost1) ~ log(cost2) + program + location+ month + type, data=data)
Residuals:
Min       1Q   Median       3Q      Max
-0.88768 -0.10647  0.00169  0.09248  0.91612
Coefficients:
                          Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)              0.1526869  0.1186113   1.287  0.19858
log(cost2)               0.9812236  0.0072015 136.253  &lt; 2e-16 ***
program1                -0.0055709  0.0475793  -0.117  0.90684 
program2                -0.0007374  0.0593048  -0.012  0.99008  
program3                 0.0531385  0.0734250   0.724  0.46958 
program4                 0.0712944  0.0472402   1.509  0.13188
locationA                0.0210172  0.0319844   0.657  0.51141
locationB                0.0415091  0.0298623   1.390  0.16514
locationC                0.0898111  0.0316606   2.837  0.00474 ** 
month02                 -0.0631733  0.0454815  -1.389  0.16545  
month03                  0.0195483  0.0449924   0.434  0.66412 
month04                  0.0037596  0.0446384   0.084  0.93291
month05                  0.0387446  0.0422586   0.917  0.35966    
month06                  0.0899078  0.0494497   1.818  0.06963 .
month07                  0.0974763  0.0459993   2.119  0.03457 * 
month08                  0.0351214  0.0472294   0.744  0.45744 
month09                  0.0652653  0.0629235   1.037  0.30013
month10                 -0.5510485  0.1986461  -2.774  0.00574 ** 
TypeI                   -0.0815081  0.0821450  -0.992  0.32155
TypeII                   0.0340512  0.0436612   0.780  0.43582 
TypeIII                  0.0703337  0.0268013   2.624  0.00895 **
TypeIV                  -0.0658808  0.0411735  -1.600  0.11021   
TypeV                    0.1327603  0.0331560   4.004 7.16e-05 ***
TypeVI                   0.0994576  0.0264572   3.759  0.00019 ***
</code></pre>

<p>This model gave me the best prediction among other combination of predictors. Can I still use this model even though not all p-values are significant or no?</p>

<p>I would greatly appreciate your help!
Thank you</p>
"
"0.114024581281567","0.123466199581199","184712","<p>I am trying to </p>

<p>1) classify a bunch of [0,1] ratios into two groups  Group 0: Ratio = 0, Group 1: Ratio != 0.</p>

<p>2) predict the actual response with multiple predictors in R.</p>

<p>My question would then be:</p>

<p>Q1: Can I use the scaled predicted probability as the predicted response? </p>

<p>Q2: Should I classify the group before the regression before running the regression to solve the warning message? Would the data structure/predicted be affected?</p>

<p>I thought of achieving Goal 1 and Goal 2 separately but I can't seem to find a way to fit a unbalanced [0,1] non-censored data with good prediction.</p>

<hr>

<p>Basically my response is something like this</p>

<pre><code>y&lt;-c(rep(0,100),0.3,0.4,0.8,1.0)
x&lt;-cbind(rnorm(104,20,2),as.factor(c(rep(0,90),rep(1,5),rep(0,8),rep(1,1)))
,as.factor(sample(c(1:3),104,TRUE,prob = c(0.6,0.3,0.1))))

data&lt;-data.frame(cbind(y,x))
</code></pre>

<p>and y is strictly between 0 to 1.</p>

<p>I then fit it with a logistic regression and get the predicted probability:</p>

<pre><code>fit&lt;-glm(y~.,data=data, family = ""binomial"")  
fit.prob&lt;-predict(fit,type=""response"")
</code></pre>

<p>I used the probability to make classification model (Goal 1)</p>

<pre><code>class&lt;-y;class[y==0]=""0"";class[y!=0]=""1""

cutoff&lt;-0.06
fit.pred=rep(0,length(fit.prob)); fit.pred[fit.prob &gt;=cutoff]=1
table(fit.pred,class)
</code></pre>

<p>However, I also want to predict y from new data set, this is probably wrong, but here's what I did</p>

<pre><code>se&lt;-fit.prob&lt;-predict(fit,type=""response"",se=T)$se.fit
scaled.fit&lt;-fit.prob/max(fit.prob)
scale.fit.UL&lt;-scaled.fit+1.96*se
scale.fit.LL&lt;-scaled.fit-1.96*se
</code></pre>

<p>and I used this to be the prediction interval for y. Is there any other way to do it other than this?</p>
"
"0.0716689374632466","0.0698430295769578","184944","<p>My model produces some results, but seems to drop several factors for some reason. This then leads (I believe) to the error I get when trying to run prediction with the model on the test data-set.</p>

<p>Note that SUBJECT should contain around 18 unique departments, but the regression only seems to look at 8 of them.</p>

<p>Data pulled from <a href=""https://data.cityofboston.gov/City-Services/311-Service-Requests/awu8-dc52"" rel=""nofollow"">here</a> with the datediffs created as the difference between CLOSED_DT and TARGET_DT and categorical variables created for time of day submitted using OPEN_DT and categorical month variables created out of OPEN_DT.</p>

<hr>

<pre><code>### TRAINING DATASET: running regression on close date compared to target
LM.train = lm(train$datediffs ~ as.factor(train$SUBJECT)+
              as.factor(train$daytime) + as.factor(train$month))
summary(LM.train)

Call:
lm(formula = train$datediffs ~ as.factor(train$SUBJECT) + as.factor(train$daytime) + 
        as.factor(train$month))

Residuals:
     Min       1Q   Median       3Q      Max 
-17934.2     -1.4     13.5     25.2   1497.3 

Coefficients:
                                                           Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)                                                136.1612    12.2502  11.115  &lt; 2e-16 ***
as.factor(train$SUBJECT)Boston Water &amp; Sewer Commission   -114.4284   121.0248  -0.945  0.34441    
as.factor(train$SUBJECT)Civil Rights                        39.9356    59.0766   0.676  0.49904    
as.factor(train$SUBJECT)Inspectional Services             -134.6127    12.0019 -11.216  &lt; 2e-16 ***
as.factor(train$SUBJECT)Mayor's 24 Hour Hotline           -139.2901    13.8814 -10.034  &lt; 2e-16 ***
as.factor(train$SUBJECT)Parks &amp; Recreation Department     -404.6200    12.3268 -32.824  &lt; 2e-16 ***
as.factor(train$SUBJECT)Property Management               -174.4930    12.6617 -13.781  &lt; 2e-16 ***
as.factor(train$SUBJECT)Public Works Department           -153.2878    11.9246 -12.855  &lt; 2e-16 ***
as.factor(train$SUBJECT)Transportation - Traffic Division -142.7941    12.1351 -11.767  &lt; 2e-16 ***
as.factor(train$daytime)3pm to 7pm                           5.2794     2.1009   2.513  0.01197 *  
as.factor(train$daytime)7pm to midnight                     -4.5244     2.8398  -1.593  0.11111    
as.factor(train$daytime)Midnight to 10am                    -0.5208     1.9152  -0.272  0.78569    
as.factor(train$month)2                                     10.4747     3.4356   3.049  0.00230 ** 
as.factor(train$month)3                                     -6.1202     3.9623  -1.545  0.12244    
as.factor(train$month)4                                     -9.2142     4.1532  -2.219  0.02652 *  
as.factor(train$month)5                                    -11.4799     4.0921  -2.805  0.00503 ** 
as.factor(train$month)6                                     -9.6011     4.1127  -2.335  0.01957 *  
as.factor(train$month)7                                      0.4905     3.8696   0.127  0.89914    
as.factor(train$month)8                                      1.6532     3.7653   0.439  0.66061    
as.factor(train$month)9                                    -15.2938     3.7509  -4.077 4.56e-05 ***
as.factor(train$month)10                                   -23.4418     4.0240  -5.826 5.70e-09 ***
as.factor(train$month)11                                    -4.3002     4.3591  -0.986  0.32389    
as.factor(train$month)12                                    -9.6494     4.3658  -2.210  0.02709 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 361.3 on 214420 degrees of freedom
  (85557 observations deleted due to missingness)
Multiple R-squared:  0.02863,   Adjusted R-squared:  0.02853 
F-statistic: 287.3 on 22 and 214420 DF,  p-value: &lt; 2.2e-16


###################
############### PREDICTION ##################
###################

predict(LM.train, newdata=test)
Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) : 
      factor as.factor(train$SUBJECT) has new levels Animal Control, Boston Police Department, City Hall Truck, Consumer Affairs &amp; Licensing, CRM Application, Disability Department, Neighborhood Services, Veterans, Women's Commission, Youthline
</code></pre>
"
"0.0693931503088838","0.0676252226000574","185033","<p>I have a regression (time series) problem </p>

<p>$$y_t = w_1x_t+w_2z_t$$</p>

<p>Now, when I include a time-1 lag in this problem, so that the equation becomes </p>

<p>$$y_t = w_1x_t+w_2z_t+w_3x_{t-1}+w_4z_{t-1}$$</p>

<p>my $R^2$ values go up a little bit, but further including a time-2 lag:</p>

<p>$$y_t = w_1x_t+w_2z_t+w_3x_{t-1}+w_4z_{t-1}+w_5x_{t-2}+w_6z_{t-2}$$</p>

<p>completely ruins the model: Multiple $R^2$ is equal to $1$, but Adjusted $R^2$ is NaN (in R), all Std. Errors, etc. are NA. Also, when I use the learned models for prediction, the first model is slightly better than the lag-1 model, which is immensely better than the lag-2 model (the lag-2 model predicts nonsense).</p>

<p>What could be the reason for this (other than some implementation error on my side)? Too few data points to estimate the model (I have only 11 observations, but quite a few variables: The lag-0 model has about 20 variables, then there are 40 for the lag-1 model, and 60 for the lag-2 model)? Or is this an instance of multicollinearity? Does this indicate that the lag-0 model is most suitable? </p>
"
"0.120192462032251","0.117130321416455","185449","<p>I've implemented a comparison between the performance of 80%-forecast intervals is in the forecast package - see 1st part of the code below providing a number of hits
This number states, how many times the forcast interval was right for the left-out data entries. Btw regarding variable names: the German ""preis"" means ""price"" and ""absatz"" means ""sales"", i.e.
""preise"" means ""prices"" and ""absaetze"" is the plural for ""sales"".</p>

<p>So, I compared the formula-based prediction interval to what I think bootstrapping is - see 2nd part of the code. But the number of hits in the 2nd case by no means resembles the 80% of the first case.
The following actions did not help to reproduce the 80% : using less data in the given data frame, using median formulas for bootstrapping instead of the upper/lower computation in the loop,
more samples resampling in the resampling.</p>

<p>I cannot imagine the bootstrapping approach performing so bad - what did I do wrong?  </p>

<pre><code>#given

# data frame

preis&lt;-c(1:100)
absatz&lt;-(-2*preis)+1000+rnorm(100)


jeansData&lt;-data.frame(absaetze=absatz,preise=preis)

#### implementation ###


#leave-one-out cross-validation for formula, i.e. with the borders         given     above  
###### (1ST PART) ########

numberOfHits&lt;-0

for(i in (1:100)){

preisCandidateToBeChecked&lt;-preis[i]
absatzCandidateToBeChecked&lt;-absatz[i]

absatzWithoutCandidate&lt;-absatz[-i]
preisWithoutCandidate&lt;-preis[-i]

jeansData&lt;-data.frame            (absaetze=absatzWithoutCandidate,preise=preisWithoutCandidate)
fit&lt;-lm((absaetze~preise), data=jeansData)

#check, if in interval and count as hit, if value is in interval

if(absatzCandidateToBeChecked &lt;= (forecast(fit,     newdata=preisCandidateToBeChecked)$upper[1]) &amp; (absatzCandidateToBeChecked &gt;= (forecast(fit, newdata=preisCandidateToBeChecked)$lower[1])) )
{numberOfHits&lt;-numberOfHits+1}

}

#execute code until here and inspect numberOfHits; the hit rate pretty much resembles the 80% assumed

#then execute the rest

#leave-one-ot cross-validation for bootstrapping (not using the bootstrap function)  ###### (2ND PART) ########


numberOfHits&lt;-0

for(i in (1:100)){

preisCandidateToBeChecked&lt;-preis[i]
absatzCandidateToBeChecked&lt;-absatz[i]

absatzWithoutCandidate&lt;-absatz[-i]
preisWithoutCandidate&lt;-preis[-i]

jeansData&lt;-data.frame(absaetze=absatzWithoutCandidate,preise=preisWithoutCandidate)

#ten or hundred or thousand regressions by bootstrapping

allPredictions&lt;-c()

for(j in (1:10)){

fit&lt;-lm((absaetze~preise), data=jeansData[sample(nrow(jeansData),10,replace=TRUE),])

allPredictions&lt;-c(allPredictions,forecast(fit,     newdata=preisCandidateToBeChecked)$mean)

}

#build and name bootstrapped forecast interval from regressions

upper&lt;-sort(allPredictions)[9]
lower&lt;-sort(allPredictions)[2]

if((absatzCandidateToBeChecked &lt;= upper) &amp; (absatzCandidateToBeChecked &gt;= lower) )
{numberOfHits&lt;-numberOfHits+1}

} #inspect numberOfHitsAgain - it's around 40%. What is foul here?!
</code></pre>
"
"0.0716689374632466","0.0873037869711973","185800","<p>I try to find a model using logistic regression. More precisely, what I did so far, is using stepwise regression and subset selection (although I know, it is often a bad idea) to find the ""best"" model. Clearly, depending on the information criteria I used, I got different results. </p>

<p>Now, I found an interesting example on page 250 in the book <a href=""http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf"" rel=""nofollow"">""An Introduction to Statistical Learning""</a>. They chose among the models of different sizes using cross-validation, that is they make predictions for each model and compute the test errors. Eventually, the compute the cross validation error and choose the model corresponding to the minimal average cross-validation error. </p>

<p>However, the function <code>regsubsets</code> of the R package ""leaps"" is only working for linear models. How can I implement this for logistic regression or glm models in general? </p>

<p>My idea was, to just estimate the models within a cross-validation using the <code>step</code> function of the ""stats"" package and then kind of take the average number of features (which is determined by minimum AIC, for example). Is this a legitimate approach?</p>
"
"0.156252904002751","0.170186411426254","186393","<p>I built a multivariate regression tree using the <code>party</code> package in R. The depth of the tree (max. number of splits) is 13. For the first 3/4 splits the tree is relatively easy to interpret which is useful in our case. However with an increase in the number of splits interpretation becomes impossible. The idea is to get a measure of variable importance from this tree, similar to the idea of variable importance in random forests. For random forests there is function <code>varimp</code> but for regression trees it does not seem to exist. I'm aware of the <code>caret</code> package but it is built for CART of the <code>rpart</code> package.</p>

<p>Now, I have an idea of how to measure variable importance in CART but i'm a little lost on how to implement it using the <code>party</code>/<code>partykit</code> package. From <em>Ishwaran (2007)</em>:</p>

<blockquote>
  <p>We define the VIMP for a variable x<sub>v</sub> as the difference between prediction error when x<sub>v</sub> is â€œnoised upâ€ versus the prediction error otherwise. To noise up x<sub>v</sub> we adopt the following convention. To assign a terminal value to a case x, drop x down T [which is your tree] and follow its path until either a terminal node is reached or a node with a split depending upon x<sub>v</sub> is reached. In the latter case choose the right or left daughter of the node with equal probability. Now continue down the tree, randomly choosing right and left daughter nodes whenever a split is encountered (whether the split depends upon x<sub>v</sub> or not) until reaching a terminal node. Assign x the node membership of this terminal node.</p>
</blockquote>

<p>However:</p>

<blockquote>
  <p>This type of scenario shows that a non-informative variable can appear informative over a single tree under our noising up process...Moreover, for a single tree, this kind of problem can be resolved by slightly modifying the noising up process. Rather than using random left-right assignments on all nodes beneath x<sub>v</sub>, use random assignments for only those nodes that split on x<sub>v</sub>. This will impact prediction only when x<sub>v</sub> is informative and not affect prediction for non-informative variables</p>
</blockquote>

<p>How do I go about implementing this procedure? It seems that the <code>fitted_node()</code> function from the <code>partykit</code> package should do the trick. <code>fitted_node()</code> takes the following arguments:</p>

<pre><code>fitted_node(node, data, vmatch = 1:ncol(data), obs = 1:nrow(data), perm = NULL)
</code></pre>

<p>where</p>

<blockquote>
  <p><strong>node</strong>:  an object of class partynode<br>
   <strong>data</strong>: a list or data.frame<br>
   <strong>vmatch</strong>: a permutation of the variable numbers in data<br>
   <strong>obs</strong>: a logical or integer vector indicating a subset of the           observations in  data<br>
  <strong>perm</strong>: a vector of integers specifying the variables to be permuted
  prior before splitting (i.e., for computing permutation variable
  importances).  The default NULL doesnâ€™t alter the data.</p>
</blockquote>

<p>I can recursively partition the <code>data</code> using the tree specified in <code>node</code>. However how do I ""noise up"" one of the splitting variables in my tree? It is not clear to me whether i should use the <code>vmatch</code> and/or <code>perm</code> arguments and how i should specify them (for example do <code>perm</code> and <code>vmatch</code> refer to the column number of the covariate or do they refer to the cells in <code>data</code>?)</p>

<h2>References</h2>

<ol>
<li>Ishwaran, H. (2007). Variable importance in binary regression trees
and forests. Electronic Journal of Statistics, 1, 519â€“537.
<a href=""http://doi.org/10.1214/07-EJS039"" rel=""nofollow"">http://doi.org/10.1214/07-EJS039</a></li>
</ol>
"
"0.0895861718290583","0.0873037869711973","186396","<p>I have a dataset that I divide into two equal partitions A and B.</p>

<p>I estimate a regression model on partition A.</p>

<p>I want to calculate the cross-validated $R^2$ when predicting the values in partition B.</p>

<p>I would like to know if the following approach is correct and also what other ways there could be:</p>

<pre><code>#generate data:

data &lt;- replicate(10, rnorm(100))
data &lt;- as.data.frame(data)

#divide into training and test set:

train &lt;- data[1:50,]
test &lt;- data[51:100,]

#fit model and get predictions for unseen data:

model &lt;- lm(train[,1] ~., data = train)
predictions &lt;- predict(model, test)

#obtain cross-validated R squared:

cor(predictions,test[,1])^2
</code></pre>
"
"0.0981367343026181","0.0956365069595007","186620","<p>I've used the gbm in R to generate a model. Although I can use predict.gbm to fit the model on new data set, I want to know the detailed step of gbm to calculate the prediction, beacuse I need to write such code in C++ for other application. </p>

<p>I used</p>

<pre><code>    tree &lt;- pretty.gbm.tree(model, i.tree=1)
</code></pre>

<p>and it shows:</p>

<pre><code>    SplitVar SplitCodePred LeftNode RightNode MissingNode ErrorReduction Weight    Prediction
    0        22  1.443225e-04        1        26          30      529.36379 132010 -5.171610e-06
    1        48 -7.033773e-04        2        18          25      351.30874  91916 -4.234042e-04
    2        32 -1.581934e-03        3         4          17      294.59416   4239 -3.235046e-03
    3        -1  4.159611e-03       -1        -1          -1        0.00000    478  4.159611e-03
    4        47  2.370844e-02        5        12          16      168.32051   3761 -4.174862e-03
    5        30 -6.295779e-01        6         7          11      105.65495   3525 -3.627476e-03
    6        -1  4.671729e-03       -1        -1          -1        0.00000    147  4.671729e-03
    7        16 -5.710531e-01        8         9          10       98.05341   3378 -3.988632e-03
    8        -1 -2.225353e-03       -1        -1          -1        0.00000   1631 -2.225353e-03
    9        -1 -5.634830e-03       -1        -1          -1        0.00000   1747 -5.634830e-03
    10       -1 -3.988632e-03       -1        -1          -1        0.00000   3378 -3.988632e-03
    11       -1 -3.627476e-03       -1        -1          -1        0.00000   3525 -3.627476e-03
    12       43 -2.381094e-02       13        14          15      135.69243    236 -1.235085e-02
    13       -1 -6.450770e-03       -1        -1          -1        0.00000    147 -6.450770e-03
    14       -1 -2.209593e-02       -1        -1          -1        0.00000     89 -2.209593e-02
    15       -1 -1.235085e-02       -1        -1          -1        0.00000    236 -1.235085e-02
    16       -1 -4.174862e-03       -1        -1          -1        0.00000   3761 -4.174862e-03
    17       -1 -3.235046e-03       -1        -1          -1        0.00000   4239 -3.235046e-03
    18        0  8.715281e-02       19        23          24      128.75576  87677 -2.874671e-04
    19       36  3.360935e-01       20        21          22      106.12050  51342 -6.098461e-04
    20       -1 -8.775861e-04       -1        -1          -1        0.00000  38121 -8.775861e-04
    21       -1  1.621467e-04       -1        -1          -1        0.00000  13221  1.621467e-04
    22       -1 -6.098461e-04       -1        -1          -1        0.00000  51342 -6.098461e-04
    23       -1  1.680601e-04       -1        -1          -1        0.00000  36335  1.680601e-04
    24       -1 -2.874671e-04       -1        -1          -1        0.00000  87677 -2.874671e-04
    25       -1 -4.234042e-04       -1        -1          -1        0.00000  91916 -4.234042e-04
    26       48  1.212169e-04       27        28          29      118.94817  40094  9.536318e-04
    27       -1  4.416651e-04       -1        -1          -1        0.00000  21287  4.416651e-04
    28       -1  1.533109e-03       -1        -1          -1        0.00000  18807  1.533109e-03
    29       -1  9.536318e-04       -1        -1          -1        0.00000  40094  9.536318e-04
    30       -1 -5.171610e-06       -1        -1          -1        0.00000 132010 -5.171610e-06
</code></pre>

<p>Then I use the first 10 samples of the original data set to calculate the prediction:</p>

<pre><code>    pred &lt;- predict.gbm(model, newdata=train.sample[1:10,],n.trees=1)
    pred
</code></pre>

<p>it shows:</p>

<pre><code>    [1] -0.01897030 -0.01897030 -0.01897030 -0.01896438 -0.01896438 -0.02001003 -0.02001003 -0.02001003 -0.02001003 -0.02001003
</code></pre>

<p>My understanding of gbm is that the predicted values would be one of the values of a leaf. But these fitted values are not shown in the original tree. I check the code of predict.gbm, it shows the core part of it is to call a compiled function gbm_pred, whose detailed is hidden.Does anybody know how to reconstruct a gbm regression step-by-step in R? </p>

<p>Thank you very much.</p>
"
"0.0817806119188484","0.0637576713063338","186667","<p>Let's say I have a small dataset:</p>

<pre><code>data &lt;- replicate(4,rnorm(13))
</code></pre>

<p>I want to test the out-of-sample predictions of a regression model as a function of increasing training set size (increasing by 10% in each increment).</p>

<p>I use the following procedure:</p>

<pre><code>test.set &lt;- 3

#for each iteration increase the test set by 10%
train.set &lt;- (nrow(data)-test.set) * seq(0.1,0.9,0.1)
train.set &lt;- train.set[train.set&gt;1]
results &lt;- vector()

  y=0
  for (t in train.set){
    y=y+1
    trainSize &lt;- t
    train &lt;- sample(1:nrow(data),trainSize)
    test &lt;- sample(1:nrow(data),test.set) 

    test.data &lt;- data[test,]
    train.data &lt;- data[train,]



    #fit a linear regression:
    train.data &lt;- as.data.frame(train.data)
    model &lt;- lm(train.data[,1] ~., data=train.data[,2:4])

    #get predictions:
    test.data &lt;- as.data.frame(test.data)
    predictions &lt;- predict(model,test.data)

    #calculate out of sample R squared (1-SSE/TSS): 

    error &lt;- 1 - sum( (test.data[,1] - predictions)^2 ) / ((nrow(test.data)-1) * var(test.data[,1]))

    results[y] &lt;- error
  }
</code></pre>

<p>I repeat this procedure several times and take the average of the repetitions.</p>

<p>My problem is that I get unreliable results. I am assuming this is because the dataset is really small. What could I do to get more reasonable estimates?</p>
"
"0.0400641540107502","0.0390434404721515","187053","<p>I am regressing actual counts of traffic against predictions using ridge regression (<code>cv.glmnet</code> in R).  The data (both predicted and actual) has a roughly exponential distribution, i.e. a few large values (which are important to predict) and many small ones.  Residuals in the model are usually proportional to the size of the target variable.</p>

<p>What is the best approach to fit such a model correctly?</p>

<p>Transform both predicted and target data beforehand (cube root, log, Box-Cox)?</p>

<p>Or is there something I can do with the estimating process that negates the need to do this - by treating errors in large values as less bad than errors in small ones?</p>
"
"0.0981367343026181","0.0956365069595007","188115","<p>I currently have 3 models to predict Y from a linear combination of independent variables:</p>

<p>Model 1: Y ~ A + B</p>

<p>Model 2: Y ~ A + C</p>

<p>Model 3: Y ~ A + D</p>

<p>Now, I want to compare their in-sample fitting and out-of-sample prediction performances. I could split the data into training and testing sets, run the linear regression (lm) on the training set, and predict Y of the testing set with the result. However, the results vary depends on seeds used to split the data set.</p>

<p>I came across ""Cross Validation"" concept, but got confused on how to use it both in in-sample fitting and out-of-sample prediction. Across folds, the coefficients obtained from linear regression on training set will be different. This will affect the prediction part on the testing set as well. (Also that for every fold, the training and testing sets keep changing.)</p>

<p>Could someone help me with how to actually use cross-validation in this setting? Thank you!</p>
"
"0.0693931503088838","0.0676252226000574","190403","<p>I am working on a dataset that has 300+ predictors and the dependent variables is very imbalanced (99:1). I need to have a prediction accuracy to show to my client.Here is my analytical process. </p>

<ol>
<li>clean data: remove incomplete columns and rows, then I have 80% of rows remaining and 100+ predictors. </li>
<li>use LASSO: use LASSO with logistic regression to generate the model (by setting up train and testing sets).
Then I have problem finding the best cut points. Below is the accuracy stats for the prediction in testing set if I set cut point as 50%:</li>
</ol>

<p><code>
pred   0   1
    0 825  36
    1  23  43
</code></p>

<p>The prediction accuracy is too low and I am wondering if it could be improved by choosing different cut points.</p>

<p>Appreciate any helps and suggestions.
Thanks.</p>
"
"0.0981367343026181","0.0956365069595007","191222","<p>I'm using R to fit a linear regression model and then I use this model to predict values but it does not predict very well boundary values. Do you know how to fix it?</p>

<p>ZLFPS is:</p>

<pre><code>ZLFPS&lt;-c(27.06,25.31,24.1,23.34,22.35,21.66,21.23,21.02,20.77,20.11,20.07,19.7,19.64,19.08,18.77,18.44,18.24,18.02,17.61,17.58,16.98,19.43,18.29,17.35,16.57,15.98,15.5,15.33,14.87,14.84,14.46,14.25,14.17,14.09,13.82,13.77,13.76,13.71,13.35,13.34,13.14,13.05,25.11,23.49,22.51,21.53,20.53,19.61,19.17,18.72,18.08,17.95,17.77,17.74,17.7,17.62,17.45,17.17,17.06,16.9,16.68,16.65,16.25,19.49,18.17,17.17,16.35,15.68,15.07,14.53,14.01,13.6,13.18,13.11,12.97,12.96,12.95,12.94,12.9,12.84,12.83,12.79,12.7,12.68,27.41,25.39,23.98,22.71,21.39,20.76,19.74,19.49,19.12,18.67,18.35,18.15,17.84,17.67,17.65,17.48,17.44,17.05,16.72,16.46,16.13,23.07,21.33,20.09,18.96,17.74,17.16,16.43,15.78,15.27,15.06,14.75,14.69,14.69,14.6,14.55,14.53,14.5,14.25,14.23,14.07,14.05,29.89,27.18,25.75,24.23,23.23,21.94,21.32,20.69,20.35,19.62,19.49,19.45,19,18.86,18.82,18.19,18.06,17.93,17.56,17.48,17.11,23.66,21.65,19.99,18.52,17.22,16.29,15.53,14.95,14.32,14.04,13.85,13.82,13.72,13.64,13.5,13.5,13.43,13.39,13.28,13.25,13.21,26.32,24.97,23.27,22.86,21.12,20.74,20.4,19.93,19.71,19.35,19.25,18.99,18.99,18.88,18.84,18.53,18.29,18.27,17.93,17.79,17.34,20.83,19.76,18.62,17.38,16.66,15.79,15.51,15.11,14.84,14.69,14.64,14.55,14.44,14.29,14.23,14.19,14.17,14.03,13.91,13.8,13.58,32.91,30.21,28.17,25.99,24.38,23.23,22.55,20.74,20.35,19.75,19.28,19.15,18.25,18.2,18.12,17.89,17.68,17.33,17.23,17.07,16.78,25.9,23.56,21.39,20.11,18.66,17.3,16.76,16.07,15.52,15.07,14.6,14.29,14.12,13.95,13.89,13.66,13.63,13.42,13.28,13.27,13.13,24.21,22.89,21.17,20.06,19.1,18.44,17.68,17.18,16.74,16.07,15.93,15.5,15.41,15.11,14.84,14.74,14.68,14.37,14.29,14.29,14.27,18.97,17.59,16.05,15.49,14.51,13.91,13.45,12.81,12.6,12,11.98,11.6,11.42,11.33,11.27,11.13,11.12,11.11,10.92,10.87,10.87,28.61,26.4,24.22,23.04,21.8,20.71,20.47,19.76,19.38,19.18,18.55,17.99,17.95,17.74,17.62,17.47,17.25,16.63,16.54,16.39,16.12,21.98,20.32,19.49,18.2,17.1,16.47,15.87,15.37,14.89,14.52,14.37,13.96,13.95,13.72,13.54,13.41,13.39,13.24,13.07,12.96,12.95,27.6,25.68,24.56,23.52,22.41,21.69,20.88,20.35,20.26,19.66,19.19,19.13,19.11,18.89,18.53,18.13,17.67,17.3,17.26,17.26,16.71,19.13,17.76,17.01,16.18,15.43,14.8,14.42,14,13.8,13.67,13.33,13.23,12.86,12.85,12.82,12.75,12.61,12.59,12.59,12.45,12.32)

QPZL&lt;-c(36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16,36,35,34,33,32,31,30,29,28,27,26,25,24,23,22,21,20,19,18,17,16)

ZLDBFSAO&lt;-c(1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2)    
</code></pre>

<p>My model is:</p>

<pre><code>fit32=lm(log(ZLFPS) ~ poly(QPZL,2,raw=T) + ZLDBFSAO)

results3 &lt;- coef(summary(fit32))

first3&lt;-as.numeric(results3[1])
second3&lt;-as.numeric(results3[2])
third3&lt;-as.numeric(results3[3])
fourth3&lt;-as.numeric(results3[4])
fifth3&lt;-as.numeric(results3[5])

#inverse model used for prediction of FPS
f1 &lt;- function(x) {first3 +second3*x +third3*x^2 + fourth3*1}
</code></pre>

<p>You can see my dataset <a href=""https://docs.google.com/spreadsheets/d/1vlc5c6qO973vgIKZaadL5J5nz5hpFFFSua4P2Qb1Kig/edit?usp=sharing"" rel=""nofollow"">here</a>. This dataset contains the values that I have to predict.  The FPS variation per QP is heterogenous. See dataset. I added a new column.
The fitted dataset is a different one.</p>

<p>To test the model just write <code>exp(f1(selected_QP))</code> where selected QP varies from 16 to 36. See the given dataset for QP values and the FPS value that the model should predict.</p>

<p>You can run the model online <a href=""http://www.r-fiddle.org/#/fiddle?id=E5J5usMi&amp;version=1"" rel=""nofollow"">here</a>.</p>

<p>When I'm using QP values in the middle, let's say between 23 and 32 the model predicts the FPS value pretty well. Otherwise, the prediction has big error value.</p>
"
"0.149906337799172","0.146087177447694","191916","<p>I have taken plenty of time to try and help myself, but I keep reaching dead ends. </p>

<p>I have a dataset consisting of body measurements collected from a bird species, and the sex of each bird (known by molecular means). I built a logistic regression model (using the AIC information criterion) to assess which measurements explain better the sex of the birds. My ultimate goal is to have an equation which could be used by others under field conditions to predict reliably the sex of the birds by taking as few body measurements as possible. </p>

<p>My final model includes four independent variables, namely ""Culmen"", ""Head-bill"", ""Tarsus length"", and ""Wing length"" (all continuous). I wish my model was a little more parsimonious, but all the variables seem to be important according to AIC criterion. Because the model produced should be used as prediction tool, I decided validate it using a leave-one-out cross validation approach. In my learning process, I first tried to complete the analyses (cross-validation and plotting) by including only one explanatory variable, namely ""Culmen"". </p>

<p>The output of the cross validation (package ""boot"" in R) yields two values (deltas), which are the cross-validated prediction errors where the first number is the raw leave-one-out, or lieu cross-validation result, and the second one is a bias-corrected version of it. </p>

<pre><code>model.full &lt;- glm(Sex ~ Culmen, data = my.data, family = binomial)
summary(model.full.1)

cv.glm(my.data, model.full, K=114)

$call
cv.glm(data = my.data, glmfit = model.full, K = 114)

$K
[1] 114

$delta
[1] 0.05941851 0.05937288
</code></pre>

<p>Q1. Could anyone expalin what do these two values represent and how to interpret them?    </p>

<p>Following is the code as presented by Dr. Markus MÃ¼ller (Calimo) in a similar, albeit not identical, post (<a href=""http://stackoverflow.com/questions/20346568/feature-selection-cross-validation-but-how-to-make-roc-curves-in-r"">http://stackoverflow.com/questions/20346568/feature-selection-cross-validation-but-how-to-make-roc-curves-in-r</a>) which I tried to tweak to meet my data:</p>

<pre><code>library(pROC)
data(my.data)
k &lt;- 114    # Number of observations or rows in dataset
n &lt;- dim(my.data)[1]
indices &lt;- sample(rep(1:k, ceiling(n/k))[1:n])

all.response &lt;- all.predictor &lt;- aucs &lt;- c()
for (i in 1:k) {
test = my.data[indices==i,]
learn = my.data[indices!=i,]
model &lt;- glm(Sex ~ Culmen, data = learn, family=binomial)
model.pred &lt;- predict(model, newdata=test)
aucs &lt;- c(aucs, roc(test$Sex, model.pred)$auc)
all.response &lt;- c(all.response, test$outcome)
all.predictor &lt;- c(all.predictor, model.pred)
}

Error in roc.default(test$Sex, model.pred) : No case observation.

roc(all.response, all.predictor)

Error in roc.default(all.response, all.predictor) : No valid data provided.

mean(aucs)
</code></pre>

<p>Q2. What's the reason for the first error message? I guess the second error is associated with the first one, and that it will be solved once I find a solution to the first one.</p>

<p>I will appreciate very much any help!!</p>

<p>Luciano </p>
"
"0.0400641540107502","0.0390434404721515","193166","<p>I have limited statistic knowledge but I am trying to conduct logistic regression by using a data with 300+ predictors. So I decided to use glmnet and LASSO. Below please see my code:</p>

<pre><code>fit.lasso = glmnet(x, y,family=""binomial"",alpha = 1)
    plot(fit.lasso, xvar = ""lambda"", label = TRUE)
    cv.lasso = cv.glmnet(x,y,family=""binomial"",alpha = 1)
    plot(cv.lasso)
    coef(cv.lasso)
    cv.lasso$lambda.min
        bestlam = cv.lasso$lambda.min
    lasso.pred=predict(fit.lasso,s=bestlam,newx = x,type = ""response"")
</code></pre>

<p>I have two questions and appreciate any helps.</p>

<ol>
<li>(removed since it was more related to purely programming question)</li>
<li>I have used CV to select lambda but I didn't partition the data into training and testing. Is it necessary since I have already used CV? I will need the <code>lasso.pred</code> to compare with actual to calculate the prediction accuracy.</li>
</ol>

<p>Thank you in advance!</p>
"
"0.0981367343026181","0.0956365069595007","193417","<p>I have an experiment where we measure the energy used by a building and want to regress this energy linearly against so-called degree-days, calculated with two different methods. The data looks like this:</p>

<p><a href=""http://i.stack.imgur.com/eR1yF.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/eR1yF.png"" alt=""enter image description here""></a></p>

<p>A regression line has been added to each group, that has been forced to go through the origin.</p>

<p>I want to compute the slope of these lines (with std. error), but I'm not sure what is the right way. My data looks like this:</p>

<pre><code>&gt; alvDegreeDays[sample(nrow(alvDegreeDays), 4),]
          Energy  BaseTemp DegreeDays
Feb 2014   984.7 Estimated   365.9771
Mar 2014   864.7 Estimated   307.2246
Apr 20151  512.8       SIA    50.0000
Sep 2015   239.2 Estimated    95.4787
</code></pre>

<p>I've tried this first:</p>

<blockquote>
  <p>lm(Energy ~ DegreeDays * BaseTemp + 0, alvDegreeDays)</p>
</blockquote>

<pre><code>Call:
lm(formula = Energy ~ DegreeDays * BaseTemp + 0, data = alvDegreeDays)

Coefficients:
            DegreeDays       BaseTempEstimated             BaseTempSIA  
                 2.436                  23.094                 174.390  
DegreeDays:BaseTempSIA  
                 1.181  
</code></pre>

<p>But this yields <code>BaseTempEstimated</code> and <code>BaseTempSIA</code> terms which are, in effect, intercept terms.</p>

<p>Next I tried the following:</p>

<blockquote>
  <p>(foo &lt;- lm(Energy ~ DegreeDays + DegreeDays:BaseTemp + 0, alvDegreeDays))</p>
</blockquote>

<pre><code>Call:
lm(formula = Energy ~ DegreeDays + DegreeDays:BaseTemp + 0, data = alvDegreeDays)

Coefficients:
                  DegreeDays  DegreeDays:BaseTempEstimated        DegreeDays:BaseTempSIA  
                       4.401                        -1.897                            NA  
</code></pre>

<p>This looks better, but when I try to call <code>predict</code> on this model I get weird error messages:</p>

<pre><code>&gt; predict(foo, list(DegreeDays = 1, BaseTemp = ""Estimated""))
       1 
2.504507 
Warning message:
In predict.lm(foo, list(DegreeDays = 1, BaseTemp = ""Estimated"")) :
  prediction from a rank-deficient fit may be misleading
</code></pre>

<p>Any idea what I may be doing wrong (or right) here?</p>
"
"0.0400641540107502","0.0390434404721515","194847","<p>I am building a regression model where I want to score/optimize/train 'over-predictions' to be twice costly as under predictions.  I am attempting to do this in R and hopefully with caret package.   </p>

<p>I don't even really know where to start or if this is feasible?  The models I would want to test this out on would be the <code>gbm</code> and <code>rf</code> method within caret.</p>

<p>I am not sure how to go about doing this.  I don't have an specific example handy, but if anyone can provide an example using the <code>Boston</code> dataset in the <code>MASS</code> package that would be helpful.  It's a lot easier for me to pick things up once I have a somewhat working example. (plus a simple statistical explanation of key things I should keep in mind, I would appreciate it)</p>

<p>I have tried to search this extensively on Internet, but I can't find a good practical example to get me started.   </p>

<pre><code>library(caret)
library(MASS)
rf1 = train(medv ~.-chas, method='rf', data=Boston)
# or any other example..
</code></pre>
"
"0.179172343658117","0.174607573942395","195359","<p>I have a set of complex survey data with sampling weights. I am using the <code>svyglm()</code> function from the <code>survey</code> package in R to describe the relationship between 2 variables in a GLM. I am using the quasipoisson family because both variables are over-dispersed. </p>

<p>The GLM output is as follows:</p>

<pre><code>hlsereg &lt;- svyglm(formula = HLSEPALLACRESFIX ~ HLSE_ACRE, sbdiv, family = quasipoisson)

Survey design:
svydesign(id = ~1, weights = ~spwgtdividedby3, data = sportsbind)

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  5.489465   0.414979  13.228   &lt;2e-16 ***
HLSE_ACRE   -0.002744   0.001118  -2.454   0.0144 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for quasipoisson family taken to be 2.601914e+15)

Number of Fisher Scoring iterations: 12
</code></pre>

<p>I have used the <code>predict()</code> and <code>lines()</code> function to plot this model output:</p>

<pre><code>acreaxis &lt;- seq(0,2000,.1)
hlse = predict(hlsereg, list(HLSE_ACRE = acreaxis))
    plot(jitter(sportsbind$HLSE_ACRE,  amount = 2.5), jitter(sportsbind$HLSEPALLACRESFIX),pch = 16,  xlab = ""Acres"", ylab = ""Price per person per acre"",  xlim = c(0, 350), ylim = c(0,35), col=alpha(""red"",.35), font = 2, font.lab = 2)
    lines(acreaxis, hlse, lwd=4, col = ""red"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/3EUZ6.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/3EUZ6.png"" alt=""enter image description here""></a></p>

<p>This plots a line given by the regression output of an intercept at 5.5 and a very slow negative slope of -.003, but I'm uncertain if this is a correct representation of the line.</p>

<p>I have found others using the <code>predict(..., type = ""response"")</code> option, which is shown in various plots of quasipoisson models, including the one found by @Glen_b at <a href=""http://stats.stackexchange.com/a/177926/45582"">this question</a> and for <a href=""http://stats.stackexchange.com/questions/38201/problems-plotting-glm-data-of-binomial-proportional-data?rq=1"">binomial GLMs here</a>. The <code>predict.glm()</code> help page notes for the <code>type</code> argument that: ""The default is on the scale of the linear predictors; the alternative ""response"" is on the scale of the response variable."" I just don't understand what that means.  The ""response"" type yields a very different prediction line, which is curved and at a much higher value (note the scale of the y-axis, with an intercept at ~250):</p>

<pre><code>hlse = predict(hlsereg, list(HLSE_ACRE = acreaxis), type = ""response"")
plot(jitter(sportsbind$HLSE_ACRE,  amount = 2.5), jitter(sportsbind$HLSEPALLACRESFIX),pch = 16,  xlab = ""Acres"", ylab = ""Price per person per acre"",  xlim = c(0, 350), ylim = c(0,400), col=alpha(""red""),     font = 2, font.lab = 2)
lines(acreaxis, hlse, lwd=4, col = ""black"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/jnY9T.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/jnY9T.png"" alt=""enter image description here""></a></p>

<p>I have also tried to run a GLM using the negative binomial distribution, but despite inputting the quasipoisson coefficient values for starting values, the model can't find valid coefficients (I have purged all zeros from the data):</p>

<pre><code> hlsereg.nb &lt;- glm.nb(HLSEPALLACRESFIX~HLSE_ACRE,data = model.frame(sbdiv.scaledweights), start = c(5.45, -.003))
Error: no valid set of coefficients has been found: please supply starting values
In addition: Warning message:
glm.fit: fitted rates numerically 0 occurred 
</code></pre>

<p>My questions:</p>

<p>1) What is the most appropriate illustration of the GLM output from a quasipoisson family?<br>
2) If the negative binomial is more appropriate to describe this relationship, why can't it find a coefficient? If I figure out how to get it to find a coefficient, how would I visualize that output?</p>
"
"0.0716689374632466","0.0873037869711973","196839","<p>From a data stream i'm receiving a pair of measurements consisting of a current consumption and a current percentage every second. By accumulating the consumption over time it will represent eventually the maximum capacity when the percentage reaches from 100% to 0%.</p>

<p>I want to predict the maximum capacity in (almost) real time using linear regression with a small sample size window of two percent. However, when i compare the models of these local regressions of every two percent with the model of the whole data regression, i get very different results due to perhaps local fluctuation. (see figure)</p>

<p>Is there a way to bring the local regression models closer to the whole data model? (in a way that i can see the differences due to fluctuation but overall closer predictions to the whole data model)</p>

<p><a href=""http://i.stack.imgur.com/e1c8w.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/e1c8w.png"" alt=""enter image description here""></a></p>
"
"0.106837744028667","0.117130321416455","198268","<p>I'm am trying to predict disease states in a medical setting where I have three subject groups (1,2,3). I have cross-validated a multinomial logistic regression model using the following</p>

<pre><code>cvfit=cv.glmnet(Xtrain, ytrain, family=""multinomial"", type.multinomial = ""grouped"", parallel = TRUE, standardize=TRUE)
</code></pre>

<p>where Xtrain is a 42x20 matrix with 42 observations and 20 predictors.</p>

<p>If I run the following to get the coefficients of the model</p>

<pre><code>coef(cvfit)
</code></pre>

<p>I get the following output</p>

<pre><code>$`1`
21 x 1 sparse Matrix of class ""dgCMatrix""
                    1
(Intercept)  2.519025
V1           2.955347
V2           .       
V3           .       
V4          -3.508274
V5           .       
V6           .       
V7           .       
V8           .       
V9           .       
V10          .       
V11          .       
V12          .       
V13          .       
V14          .       
V15          .       
V16          .       
V17          .       
V18          .       
V19          .       
V20         -2.108070

$`2`
21 x 1 sparse Matrix of class ""dgCMatrix""
                     1
(Intercept)  1.5460376
V1          -5.2882709
V2           .        
V3           .        
V4           0.4144632
V5           .        
V6           .        
V7           .        
V8           .        
V9           .        
V10          .        
V11          .        
V12          .        
V13          .        
V14          .        
V15          .        
V16          .        
V17          .        
V18          .        
V19          .        
V20          1.4674672

$`3`
21 x 1 sparse Matrix of class ""dgCMatrix""
                     1
(Intercept) -4.0650622
V1           2.3329236
V2           .        
V3           .        
V4           3.0938106
V5           .        
V6           .        
V7           .        
V8           .        
V9           .        
V10          .        
V11          .        
V12          .        
V13          .        
V14          .        
V15          .        
V16          .        
V17          .        
V18          .        
V19          .        
V20          0.6406032
</code></pre>

<p>I would like to be able to say something concerning the risk of being in one group compared to another based on increments in the predictors with non-zero coefficients, however, I cannot seem to find any information as to which class the cvglmnet() function uses as base in order to calculate the risks. </p>

<p>Does anyone know this, or have an idea on how to interpret the results for use in a model?</p>

<p><strong>EDIT:</strong></p>

<p>I realize now that I may have overlooked a crucial detail. In ""The Elements of Statistical Learning: Data Mining, Inference, and Prediction"" by Hastie, T et al (2009), it is stated on page 657 that a multiclass logistic model can be described as</p>

<p>$P(Y=k|X=x) = \frac{\exp{(\beta_{k0}+x^{T}\beta_{k})}}{\sum_{l=1}^{K}\exp{(\beta_{l0}+x^{T}\beta_{l})}}$</p>

<p>where I can see that the denominator is just a normalization factor. I guess this means that I can interpret the obtained coefficients above directly for each subject group. Or is this wrongly interpreted?</p>
"
"0.14482328117929","0.151214594727283","198315","<p>I am currently getting slightly confused with how a rolling forecast should be setup in R, as in how the data should be organised in order to <em>train</em> and <em>test</em> my model. I feel there is a large gap in my understanding somewhere.</p>

<p>I have seen many examples of forecasting methods, but not many on time-series (using lagged variables) that go into detail, i.e. perform everything manually using <code>predict()</code>. Instead it normally just points to a built in R package. <strong>My question has to do with the training of the model - the alignment of the data for the training.</strong></p>

<p>I know that the regression equation (assuming I am performing a linear regression) looks like this:</p>

<p>$$ y_t = \beta_1 y_{t-1} + \beta_2 y_{t-2} + \beta_3 x_{t-1} + \beta_4 x_{t-2} + \varepsilon_t $$ </p>

<p>So I have my outcome variable, $y$, being explained by two of its own lagged values, plus two lagged values of a second variable, $x$. Each variable has its own coefficient, all of which my model is estimating.</p>

<p>Let's say I have a <em>data.table</em> (or data.frame), where each row consists of the data aligned according to the equation above. Each row is one day, and represents the given equation. I have five columns, and for this example say 200 rows/days.</p>

<blockquote>
  <p>Day  |  $y_t$  |  $y_{t-1}$  |  $y_{t-2}$  |  $x_{t-1}$  |  $x_{t-2}$</p>
  
  <p>001  | <em>Values</em> --></p>
  
  <p>002  | <em>Values</em> --></p>
  
  <p>003  | <em>Values</em> --></p>
</blockquote>

<p>I use the above equation as the formula to fit my model to obtain estimates for the four coefficients (neglecting $\varepsilon$) using a fixed 40-day time frame.</p>

<pre><code>model &lt;- lm(y ~ y_1 + y_2 + x_1, x_2,            ## my regression formula
            data = input_data[1:40])             ## my data.table 
</code></pre>

<p>Now I want to make a prediction using the fitted <code>model</code>.
I do this by using <code>predict()</code> in <strong>R</strong> as follows:
I take the next (41st) row of data, minus the outcome variable</p>

<pre><code>my_pred &lt;- predict(model, newdata = input_data[41][, outcome := NULL])
</code></pre>

<p>And then calculate my error:</p>

<pre><code>my_error &lt;- input_data[41, outcome] - my_pred
</code></pre>

<p>I then shift everything forward one row, so still a 40-day frame <code>input_data[2:41]</code> updating the coefficients for all variables and predicting the following outcome variable, <code>input_data[42]</code>. This is yielding terrible results for my model, with overall accuracies not much better than a naÃ¯ve forecast, i.e. random guessing.</p>

<p>Should I realign the data for the training segment, so that each row rather represents the data I had on that day? This would mean adding one more column, $x_t$.</p>

<p>Any other suggestions or comments?</p>

<p>Thanks.</p>
"
"0.132877766396671","0.129492442570703","198801","<p>Logistic Regression using R's <code>glm</code> package is giving me the following summary (snap of the few first variables). </p>

<p><strong>My Data Set:</strong></p>

<ul>
<li>Dimensions: 1252 rows and 224 columns (after using model.matrix). The Data  has been standardized.</li>
<li>Response variable is binary.</li>
<li>Trying to predict if an employee will leave the company, based on employee attributes </li>
</ul>

<p><a href=""http://i.stack.imgur.com/Ai42v.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Ai42v.png"" alt=""enter image description here""></a></p>

<p><strong>My Understanding:</strong></p>

<p>The model does not give a good fit because:</p>

<ol>
<li>Residual Deviance > Null Deviance. </li>
<li>p.value = 1 - pchisq(3676.5, 817) turns out to be 0.</li>
<li>The first warning about 0 or 1 fitted probability message suggests that due to some predictor(s) the model might be giving perfect predictions </li>
<li>Second warning on â€˜rank deficiencyâ€™ suggests that there might be predictors that are linearly dependent on one another.</li>
</ol>

<p><strong>My Questions:</strong></p>

<ol>
<li>How can I improve the model? I would like to see Residual Deviance &lt; Null Deviance. I will invest time on dropping the linearly dependent variables in the model, but is there anything I should do first to test the â€˜modelâ€™ itself, before revisiting my data? I am asking this because SVM worked quite well on the same data set. </li>
<li>Why do I have such extreme coefficient values?</li>
<li>Many answers to other posts state that â€˜AICâ€™ is used to compare different</li>
<li>The summary parameters (coefficients , std error and p-values) for many dummy factors obtained via model.matrix, like GSS_SEXM,  is shown as 'NA'. Why is it so? </li>
<li>logistic models. What is meant by â€˜differentâ€™ here? Models trained on different data sets that bear different coefficients, like say different set of attributes? </li>
</ol>
"
"0.133341564548166","0.140773126592868","198844","<p>I'm trying to understand how <code>auto.arima</code> with covariates in the xreg parameter works. I'm familiar with regression and I'm starting to work on forecasting.</p>

<p>My understanding of forecasting is that you look for patterns in the past time series and then project those paterns onto the future.  </p>

<p>My uderstanding of regression is that you use predictors to try to generate an output value and minimize the difference between your created value and the real value.  </p>

<p>So how does forecasting <code>auto.arima</code> with <code>xreg</code> work? Do you create a forecast for a timeseries based on past data and regression model based on the input time series and input <code>xreg</code>, and then forecast each data point in the time series and for each forecasted data point use the regression model you built and future <code>xreg</code> values to adjust the forecasted values?</p>

<p>I'm a former physics grad student, so I'm not allergic to math but I'm just looking for a high level overview of the process here to understand how forecasting <code>auto.arima</code> works.  </p>

<p>For example like, </p>

<ul>
<li><p>step 1: build forecast model on input time series, and regression model on input time series and input <code>xreg</code> values</p></li>
<li><p>step 2: forecast model into future one step, and predict value with regression model and future <code>xreg</code> values</p></li>
<li><p>step 3: algorithm combines forecasted value and regression model prediction to get combined value</p></li>
</ul>

<p>This is just a guess at how it works, but it's an example of the kind of high level explanation I'm looking for.</p>

<p>I've included some code below that I've been working on trying to forecast time in to out <code>TiTo</code> for customers at a restaurant with predictor count of customers in the restaurant <code>CustCount</code>.</p>

<pre><code>OV&lt;-zoo(SampleData$TiTo, 
    order.by=SampleData$DateTime)


eDate &lt;- ts(OV, frequency = 24)

Train &lt;-eDate[1:15000]
Test &lt;- eDate[15001:22773]

xregTrain &lt;- SampleData[1:15000,]$CustCount
    xregTest &lt;- SampleData[15001:22773,]$CustCount

Arima.fit &lt;- auto.arima(Train, xreg = xregTrain)

Acast&lt;-forecast(Arima.fit, h=7772, xreg = xregTest)

accuracy(Acast$mean,Test)
</code></pre>
"
"0.0801283080215004","0.078086880944303","200477","<p>I am conducting multiple imputation by chained equations in R using the MICE package, followed by a logistic regression on the imputed dataset.</p>

<p>I need to compute a 95% confidence interval about the predictions for use in creating a plotâ€”that is, the grey shading in the image at this link.</p>

<p><a href=""http://imgur.com/guLEyTQ"" rel=""nofollow"">http://imgur.com/guLEyTQ</a></p>

<hr>

<p>I followed the approach described in the answer to this question...</p>

<p><a href=""http://stats.stackexchange.com/questions/66946/how-are-the-standard-errors-computed-for-the-fitted-values-from-a-logistic-regre"">How are the standard errors computed for the fitted values from a logistic regression?</a></p>

<p>...which uses the following lines of code to yield the std.er of prediction for any specific value of the predictor:</p>

<pre><code>o &lt;- glm(y ~ x, data = dat)
C &lt;- c(1, 1.5)
std.er &lt;- sqrt(t(C) %*% vcov(o) %*% C)
</code></pre>

<p>But of course <strong>I need to adapt this code to the fact that I am using a model resulting from multiple imputation</strong>.  In that context, I am not sure <strong><em>which</em></strong> variance-covariance matrix (corresponding to â€œvcov(o)â€ in the above example) I should be using in my equation to produce the ""std.er"".</p>

<hr>

<p>Based on the documentation for MICE I see three candidate matrices:</p>

<ul>
<li><p>ubar - The average of the variance-covariance matrix of the complete data estimates.</p></li>
<li><p>b - The between imputation variance-covariance matrix.</p></li>
<li><p>t - The total variance-covariance matrix.</p></li>
</ul>

<p><a href=""http://www.inside-r.org/packages/cran/mice/docs/is.mipo"" rel=""nofollow"">http://www.inside-r.org/packages/cran/mice/docs/is.mipo</a></p>

<p>Based on trying all three, the b matrix seems patently wrong, but both the t and the ubar matrices seem plausible.  Can anybody confirm which one is appropriate?</p>

<p>Thank you.</p>
"
"0.114024581281567","0.123466199581199","200703","<p>I'm using matched pairs logistic regression (1-1 matched case-control; Hosmer and Lemeshow 2000) to model differences between vegetation selected at nest sites vs. paired random sites. To do this, I created a data frame that contained the difference in vegetation measurements between nest and random sites (so nest minus random) and used R to fit a logistic regression model, using a vector of all 1's as the 'Response' and a no-intercept model.</p>

<p>Here's the data frame (I only include 1 of the covariates, grass density, for the example):</p>

<pre><code>nest&lt;-structure(list(VerGR = c(1.380952381, 1.952380953, 2.666666667, 
-3.809523809, 2.428571428, 2.142857143, 0.142857143, 2.095238095, 
1.952380952, 3.333333334, 3.190476191, -2.857142857, 2.857142858, 
-1.666666667, 0.523809524, 4.761904762, 0.571428571, 2.238095238, 
-2.809523809, 0.857142857, 1.523809524, -2.476190476, -0.428571428, 
-5.190476191, 4.142857143, 2.857142858, -2.476190476, 4.095238096, 
1.428571428, 1.714285714, -2.80952381, 3.142857143, 2.809523809, 
7.238095238, 2.523809523, 2.333333333, -0.095238096, -0.095238096, 
-0.142857143, 4.047619048, 4.761904759, -1.285714285, -1.190476191, 
2.523809524, -2.095238095, -2, 4.761904761, 8.952380952, 1.095238096, 
5.666666666, -0.714285714, 0, 2.809523809, -0.238095239, 3.666666667, 
0.904761905, -4.952380952, -3.666666667, 2, -0.619047619, 4.523809524, 
1.523809524, 4.619047619, 6.142857143, 3.19047619, -2.190476191, 
-1.666666667, 2.714285714, -1.285714286, 2.857142857, 2.761904762, 
2.809523809, -7.142857139, -5.952380949, -1.19047619, 1.523809524, 
-0.38095238, 5.571428571, 5.238095239, 2.047619048, 7.857142857, 
0.61904761, 2.523809524, -1.190476191), Response = c(1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L)), .Names = c(""VerGR"", ""Response""), class = ""data.frame"", row.names = c(NA, 
-84L))
</code></pre>

<p>And the no-intercept logistic regression models I am running:</p>

<pre><code>grass.mod &lt;- glm(Response ~ VerGR - 1, data=nest, family=""binomial"")
grass2.mod &lt;- glm(Response ~ VerGR + I(VerGR^2) - 1, data=nest, family=""binomial"")
</code></pre>

<p>For the most part the models run fine, and give the same parameter estimates as models implemented using the 'clogit' function from the survival R package. The data set for the clogit models is slightly different, with Responses = 1 (nest) or = 0 (random point), and includes a column called 'PairID' to indicate nest-random pairs. Here's what the clogit models look like:</p>

<pre><code>library(survival)
grass.mod.clog &lt;- clogit(Response ~ VerGR + strata(PairID), data=full)
grass2.mod.clog &lt;- clogit(Response ~ VerGR + I(VerGR^2) + strata(PairID), data=full)
</code></pre>

<p>But when I run the glm's, I get these 2 warnings if using a quadratic term:</p>

<pre><code>Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
</code></pre>

<p>I'm able to satisfy the first warning if I use more iterations in the glm formula, but I'm not sure what is happening with the second warning. I would be glad to use the 'clogit' function (which works with quadratic terms), but I'm unsure how to create prediction plots to visually display the data when going that route. Any suggestions?</p>

<p>Thanks,
Jay</p>
"
"0.0716689374632466","0.0873037869711973","202028","<p>I have been provided a sample logistic regression as follows:</p>

<p><code>glm(formula = output ~ X1 + X2 + X3 + X4 + X5 + X1:term + term:X5 - 1, family=""binomial"", data=mydata)</code></p>

<p>I am not too familiar with logistic regression, so I have a few questions about how to properly predict on a new test set using this model:</p>

<p>1) Unlike a regular regression, I cannot simply 'plug-in' the variables and get a meaningful numeric output. Instead, I must first set a threshold probability above which values will be 1 and below which values will be 0. Is this correct?</p>

<p>2) I cannot make use of this sample model or get the same results as the person who provided it until I have the probability threshold that was used for prediction. Is this correct?</p>

<p>3) If I wanted to split the outputs into tiers, would I use the probabilities for that and map them to some other value? How would that process work (feel free to let me know if this is out of scope).</p>

<p>Thanks!</p>
"
"0.0981367343026181","0.0956365069595007","202215","<p>I am working on a daily response variable.  As part of weekly prediction methods, multiple linear regression is also used.  We also have monthly prediction on the same response variable.  In the monthly prediction, I used 4 months, 5 months, and 12 months lag of the response variable as the predictors.  For the weekly prediction, should I use 4 weeks, 5 weeks, and 12 weeks as lag or is it something else?</p>
"
"0.149906337799172","0.146087177447694","202973","<p>Suppose I have a data set of <code>N</code> observations <code>(n = 1...N)</code> for out-of-sample estimation and values of ($y_n$). I have also <code>I</code> statistical models <code>(i= 1...I)</code> which every model has its own estimate on each data point ($\hat{y}^i_n$).</p>

<p>In addition I have a model selection method $\phi$ which would pick a model's estimate among the model set as its own according to its assessment on previous performance of the models ($\hat{y}^\phi_n = \min_i\{\hat{f_i}(y_n), i \in I\} $).</p>

<p>My claim should be ""model selection's performance is better than all models it picks estimates from"". I am trying to find a proper method to describe the statistical power of the model selection method, compared to individual models in the model set.</p>

<p>All individual models follow different assumptions, distributions and dependence structure. Some are iid, some have heteroskedasticity. Actually, there is no restriction on models except it should yield an estimate.</p>

<p>Some The models are employed on time series but what they do is asset pricing on different assets and contracts. But for a broaded audience I will make the following analogy.</p>

<p>Suppose you have a machine that predicts the scores on basketball matches. It does not only predict the final score, it also predicts a distribution of the scores throughout the time. It also predicts which player will score when.</p>

<p>Suppose you have many machines of this sort and all have different predictions. All of them had been right on some occasion (That is what statistics is after all right? No model is perfect.). </p>

<p>I am trying to figure out which machine is better at predicting what and when, using the previous performance of the machines. I can say stuff like 'oh machine A was good at predicting scores occured in the last 10 mins, but for the last 2 months model B became better'. </p>

<p>It turns out my estimates using the machines are better than any machine could do it alone in the long run. I checked for several error terms starting with MAPE and MSE. But I want to show that it is not a coincidence but a statistically significant fact. I have a fair sample size (~100k) over a good enough time period (5 years).</p>

<p>I fiddled with some thoughts about proportion of $\phi$ selecting the model with the lowest error and some logistic regression on that according to the criteria it uses to pick the models. But I lack the comprehensive knowledge on this domain of statistics.</p>

<p>ps. R package suggestions are also appreciated.</p>
"
"0.160534598239854","0.16564728911227","203454","<p>Although I have visited this site several times, this is the first time I make a question, so be kind if it is not in a appropriate form.</p>

<p>My problem is part statistical and part R. I am trying to build a Cox PH model in order to make prediction of unemployment. I have a big dataset, N=32538 with covariates p=37 . I split this sample in 3 parts according to Hastie &amp; Tibshirani, train =50%, test=25% and validation = 25%. So, I now have a training set of N=16270 cases. I would like to reduce the number of predictors, but from what I know and have read, it is not wise to do any kind of stepwise elimination. Therefore, I am trying to perform a penalized cox regression, especially with LASSO, using the R package 'penalized'. </p>

<pre><code>library( penalized )
</code></pre>

<p>However, it seems that it cannot run...I am not sure why, but I suppose that either my laptop is not very powerful, or that the respective functions are not very efficient for such a bog dataset. </p>

<pre><code>optL1( Surv ( time, status ) ~ . , minlambda=5, fold=3, data=mydata )
optL1( Surv ( time, status ) ~ . , minlambda=5,maxlamda=15, fold=3, data=mydata )
</code></pre>

<p>As you can see, I specify minlambda in the first case and both min and max lambda in the second. If I leave it unspecified, it just crushes my whole OS. Now, my pc runs veeeeeeery slow, and after 3 hours ( the most I left it running ), although it seems still running, nothing at all was produced. Those familiar with this function, know that while it is running, it produces in the console ""what is going on"". That is , for every lambda that it checks, it shows it in the consore along with the according cvl( log-lik ). </p>

<p>In some cases, but not always,  it produces the well-known irritating message of memory error....</p>

<pre><code> Error: cannot allocate vector of size xxx Mb
</code></pre>

<p>My details :</p>

<pre><code>session(info)
R version 3.2.4 Revised  ( 2016-03-16 r70336)
Platform: x86_64-w64-mingw32/x64 ( 64 bit)
Running under: Windows &gt;= 8 x64 ( build 9200)
</code></pre>

<p>For now, I tried to run the function in subsets of the full dataset, and I ""managed"""" to make it run until  N=8000( the half sample).</p>

<p><strong>Question</strong> 1:</p>

<p>Do you now if I am doing something wrong in running the specific function, or it is an unsolved problem and I have to find another way to proceed ?  </p>

<p><strong>Question 2</strong></p>

<p>Do you know if there are any other packages in R, that can accommodate more efficient the penalized cox regression, and also be capable of making predictions ?</p>

<p>Many thanks!!
Giannis</p>

<p><strong>EDIT</strong></p>

<p>actually, as you can see, I used the classic formula for regression. Meaning, I used the 'dot' in order to include all the predictors in the model. Moreover, as you maybe have guessed, I have many categorical predictors. Do you think that it is better to add the variables all by name in the model, and specify the factors with :</p>

<pre><code>factor(var1) 
</code></pre>

<p>????</p>

<p>Because by reading the vignette( penalized) , I realized that they use only continuous predictors, leaving out of the model the categorical ones! </p>
"
"0.196430676054233","0.199083264845299","203785","<p>I'm using the <code>tgp</code> package in R for fully Bayesian Gaussian Process Regression, and it's great! I'm currently performing regression for experimental data coming from turbomachinery testing, and I'm using the <code>bgp</code> function. This function uses a GP prior with either a <code>linear</code> mean or a <code>constant</code> mean (respectively, option <code>meanfn=""linear""</code> or <code>meanfn=""constant""</code>, which is the default). Note that <code>tgp</code> allows the use of treed Gaussian priors, but for now I'm staying simple, so I'm using the <code>bgp</code> function which doesn't use regression trees, just ordinary Gaussian Processes.</p>

<p>I would like my posterior predictive mean to go to zero away from the training set data, for physical reasons. How can I impose that? I was thinking to set the prior over $\beta_0$ to a Normal distribution centered at 0 and with an extremely small variance, but I'm not sure how to do that. From <code>help(btgp)</code></p>

<pre><code>bprior Linear (beta) prior, default is ""bflat""; alternates include ""b0"" hierarchical Normal
prior, ""bmle"" empirical Bayes Normal prior, ""b0not"" Bayesian treed LMstyle
prior from Chipman et al. (same as ""b0"" but without tau2), ""bmzt"" a independent
Normal prior (mean zero) with inverse-gamma variance (tau2), and
""bmznot"" is the same as ""bmznot"" without tau2. The default ""bflat"" gives
an â€œimproperâ€ prior which can perform badly when the signal-to-noise ratio is
low. In these cases the â€œproperâ€ hierarchical specification ""b0"" or independent
""bmzt"" or ""bmznot"" priors may perform better
</code></pre>

<p>Default is the improper prior <code>""bflat""</code>, which is not what I want. If I use the <code>""b0""</code> hierarchical Normal prior, I guess I cannot set the mean and the variance because they should become additional hyperparameters to be determined in the Bayesian paradigm. Thus, I may go for <code>""bmzt""</code>, the independent Normal prior with zero mean. However, with this prior I cannot set the variance, which is again an hyperparameter. Basically, I want my prior mean function to be zero, so that away from the data, also the posterior predictive mean will be zero. Is there a way to achieve that?</p>

<p>EDIT: nobody wants to have a try? :) As my actual case is quite complicated, I wrote a small test case which illustrates the main problem, with the help of the <code>tgp</code> package author. NOTE: unless you have an optimized version of R, you may want to set <code>BTE = c(1000,10000,2)</code> in the call to <code>bgp</code>, or you may have to wait for a very long time to get an answer.</p>

<pre><code># clear the workspace
rm(list=ls())
gc()
graphics.off()

# set seed for reproducibility
set.seed(825)

# load required packages
library(tgp)
library(ggplot2)

# simulated data
x &lt;- seq(-1,1,len=100)
eps &lt;- rnorm(n=100,mean=0,sd=0.5)
y &lt;- -5*x^2+eps
ymean &lt;- mean(y)

# prediction points
xpred &lt;- seq(-20,20,len=100)

# fit GP
GPModel &lt;- bgp(X=x,Z=y,XX=xpred,meanfn = ""constant"", bprior=""bmzt"", 
BTE = c(2000,52000,2), tau2.p=c(1,10000), tau2.lam=""fixed"")    
ypred &lt;- GPModel$ZZ.mean 

# plots
ymean_vector &lt;- rep(ymean,100)
df &lt;- data.frame(x,y,xpred,ypred,ymean_vector)
p &lt;- ggplot(data=df)
p &lt;- p + geom_point(aes(x=x,y=y)) + 
    geom_line(aes(x=xpred,y=ypred),col=""blue"") +
    geom_line(aes(x=xpred,y=ymean_vector),col=""red"") +
    geom_line(aes(x=xpred,y=GPModel$ZZ.q1), col=""green"") + 
        geom_line(aes(x=xpred,y=GPModel$ZZ.q2), col=""green"")
p
</code></pre>

<p>The resulting plot is</p>

<p><a href=""http://i.stack.imgur.com/VjePX.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/VjePX.png"" alt=""enter image description here""></a></p>

<p>The mean response is the red line: the blue line is the GP posterior predictive mean, and the green lines give the 90% credible interval.Thus, outside the training data range, the data mean is indeed included in the 90% credible interval, but I would like the predictive mean to converge to it...I think that if I could find a way to set the standard deviation of the prior for $\beta_0$ to some  extremely small value, I would achieve what I want, but I don't know how to do it.</p>

<p>EDIT2: I can use either a multiplicative (separable) squared exponential kernel
or an additive squared exponential kernel.</p>

<pre><code>sep_Gaussian_Kernel &lt;- function(x,y,sigma,l) {
    prod(sigma*exp(-0.5*(abs(x-y)/l)^2))
}    

add_Gaussian_Kernel &lt;- function(x,y,sigma,l) {
    sum(sigma*exp(-0.5*(abs(x-y)/l)^2))/length(x)
} 
</code></pre>
"
"0","0.0390434404721515","203957","<p>I am learning the ropes of total least squares regression and I found this thread <a href=""https://stats.stackexchange.com/questions/13152"">How to perform orthogonal regression (total least squares) via PCA?</a> where the answer by @amoeba, together with some R code, is just spectacular.</p>

<p>However, unlike a vanilla linear regression, I am unsure about how to calculate the confidence interval of my prediction.</p>

<p>Can anyone help and, if possible, provide some R code?</p>
"
"0.120192462032251","0.117130321416455","204763","<p>Using linear regression as an equation for prediction is straightforward with,</p>

<p>$$ Y_i = \beta_0 + \beta_1 X_i. $$</p>

<p>Once the betas are estimated I can insert different values of $X$ to use as a what-if analysis for different scenarios. </p>

<p>But trying to do the same with ARIMA models is proving difficult to translate. For example with an ARIMA(2,1,1) model, how do I create an equation where I can try out different scenarios to see how the projection changes? </p>

<p>Below I have the output for a projection of sales based on past sales and extra regressors. I see that a unit change in <code>poc0_3_PER</code> results in a <code>135.2229</code> change in sales. But how do I account for the moving average and auto-regression components?</p>

<pre><code>arima(ts.count, order=c(2,1,1), xreg=df.back[3:4])

Call:
arima(x = ts.count, order = c(2, 1, 1), xreg = df.back[3:4])

Coefficients:
          ar1     ar2     ma1  poc0_3_PER
      -0.4569  0.2458  0.9455    135.2229
</code></pre>

<p>I have <code>ar1</code> and <code>ar2</code> estimates along with <code>ma1</code> and the extra regressors. How do I convert this into a working equation wherein I can try out different scenarios for the extra regressors to see how the prediction is affected?</p>

<p>I'm hoping that the solution is not an equation like <a href=""http://stats.stackexchange.com/questions/69407/how-do-i-write-a-mathematical-equation-for-arima-2-1-0-x-0-2-2-period-12?rq=1"">this post here</a>. I do have SARIMA models at times with orders like <code>SARIMA(2,0,1)(1,0,1)[12]</code>.</p>
"
"0.0716689374632466","0.0873037869711973","204839","<p>Further to <a href=""http://stats.stackexchange.com/questions/200460/multiple-imputation-for-predictive-analysis-using-mice-package-in-r"">my prior question</a> on multivariable adjustment in regression models, using covariates which are available only for some cases, I have researched in some detail the main methods for limited dependent variables, including Heckman correction or tobit models. However, I fear that they do not apply to my issue, which has more to do with <strong>limited independent variables</strong>.</p>

<p>In particular, I am giving below an example of the dataset and the possible analysis in R (disregard the overfitting, it's just to make an example, my actual dataset has at least 10,000 cases):</p>

<pre><code>dep &lt;- c(8, 9, 21, -3, 4, 6, 9, 10, 8, 9, 11, 39, 91, 51, 38, 28, 21)
cov1 &lt;- c(68, 58, 42, 19, 39, 49, 29, 38, 25, 22, 19, 36, 39,90, 105, 73, 25)
cov2 &lt;- c(0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0)
cov3 &lt;- c(0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1)
cov4 &lt;- c(NA, NA, NA, NA, NA, NA, 56, 33, 45, 44, 56, 49, 36, 39, 40, 41, 59)
cov5 &lt;- c(NA, NA, NA, NA, NA, NA, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0)
mydata &lt;- data.frame(cbind(dep, cov1, cov2, cov3, cov4, cov5)) 
mydata

reg1 &lt;- lm(dep ~ cov1 + cov2, data = mydata, na.action = na.omit)
anova(reg1)
summary(reg1)

reg2 &lt;- lm(dep ~ cov1 + cov2 + cov3 + cov4 + cov5, data = mydata, na.action = na.omit)
anova(reg2)
summary(reg2)
</code></pre>

<p>What should I do to best adjust for covariates cov1, cov2, cov3, cov4 and cov5, having dep as dependent variable, given that cov4 and cov5 are available only for patients with cov3 = 1? </p>

<p>Should I discard all cases with cov3 = 0, or should I conduct two separate analyses and then pool the regression coefficients according to their standard error? Or is there any other more reasonable approach?</p>

<p>Unfortunately I did not find anything meaningful searching Google, Google Scholar, or PubMed:</p>

<p><a href=""https://www.google.it/search?q=limited+independent+variable&amp;nirf=limited+dependent+variable"" rel=""nofollow"">https://www.google.it/search?q=limited+independent+variable&amp;nirf=limited+dependent+variable</a></p>

<p><a href=""https://scholar.google.it/scholar?hl=en&amp;q=limited+independent+variable"" rel=""nofollow"">https://scholar.google.it/scholar?hl=en&amp;q=limited+independent+variable</a></p>

<p><a href=""http://www.ncbi.nlm.nih.gov/pubmed/?term=limited+independent+variable"" rel=""nofollow"">http://www.ncbi.nlm.nih.gov/pubmed/?term=limited+independent+variable</a>*</p>

<p>To further clarify what is at stake, this is my real problem: I want to create a clinical prediction score (to predict prognosis and future quality of life) for patients undergoing myocardial perfusion imaging (a non-invasive cardiac test used in subjects with or at risk for coronary artery disease). The imaging test follows immediately an exercise stress test in fit patients, and a pharmacologic stress test in those who are not fit. The latter test is worse than the former, and does not provide several important prognostic features (eg maximum heart rate, or workload), so I must include exercise test variables in the multivariable model. But if I do so, I lose more than 1000 patients who only underwent a pharmacologic stress test.</p>
"
"0.0991537224422629","0.110431526074847","206075","<p>I'm relatively new to machine learning (started about 5 months ago), and I'm looking at potentially implementing an ensemble classifier as part of my research. </p>

<p>I have built 3 models that I use to classify whether sales data is going to win or lose. Each model produces the probability of the sale winning or losing, and then I apply thresholds to those to classify them as either a ""Win"", ""Loss"" or ""Borderline Loss"". There are 25 variables, all of which are discrete. </p>

<p>The three models are Naive Bayes, Tree Augmented Naive Bayes (TAN) and Logistic Regression. I am using the bnlearn package for the bayesian classifiers, and a simple glm for the Logistic Regression. All models have high accuracy performances when tested on unseen data:</p>

<p>Naive Bayes Accuracy: 88% </p>

<p>TAN Accuracy: 91%</p>

<p>Logistic Regression Accuracy: 92%</p>

<p>I want to try implementing an ensemble classifier to see if I can get the best possible accuracy across all three models. My question is, how do I go about implementing something like this? I can't find too many examples online, at least not with these models for implementing one. From what I have read, one way to do it is to have a voting system, where if the 2 models predict the sale will win, but 1 predicts with will lose, then it is classified as a win. But what happens in this case if all 3 models had different predictions? I have all my prediction data ready, as in I have all the test data and each models prediction for each sale, my question so is, how would I proceed from here? </p>

<p>If someone knows of any available resources or tutorials that may help, I would greatly appreciate it!</p>
"
"0.0801283080215004","0.078086880944303","206862","<p>I have a regression Random Forest model (generated using H2O and R). After tuning and building the model, I plot the predicted value vs. the labeled value of both the train and test datasets. In a model with zero errors I'd expect it to align with the 45-deg line. In my plot a get a line which is paralleled to the 45-deg line, but not aligned to it.
From the plot I concluded that the model pick up the general trend, the slope, correctly, but it's biased in terms of absolute values.
I'm a little confused on how to address it. I tried both parameter tuning and enlarging the train dataset, but in vain. 
Does anyone know how to solve it? Thanks!</p>

<p><a href=""http://i.stack.imgur.com/ziSnK.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ziSnK.png"" alt=""The prediction vs. labeled plot""></a></p>
"
"0.0600962310161253","0.078086880944303","207177","<p>I am fitting a logistic regression model for the likelihood of patients suffering morbidity after surgery. The most commonly used prediction tool at the moment is POSSUM (Physiological and Operative Severity Score for the enUmeration of Mortality and Morbidity), which I would like to compare my model against.</p>

<p>In terms of discrimination, I have the Area Under the ROC curves calculated for both and would like to compare the two. </p>

<p>It seems in Stata that the command to use is <code>roccomp</code>. This produces a chi2 statistic and a p-value.</p>

<p>The R equivalent seems to require the <code>pROC</code> package and the function to use is <code>roc.test()</code>. However this function returns a z-statistic and p-value.</p>

<p>Looking at the documentation, both seem to be implementations of DeLong et al's methods of comparing AUROCs[1], but I cannot for the life of me understand why one gives a chi2 and the other a z-statistic. Are the tests equivalent?</p>

<p><em>Reference</em>:
1. Elisabeth R. DeLong, David M. DeLong and Daniel L. Clarke-Pearson (1988) â€œComparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approachâ€. Biometrics 44, 837--845.</p>

<p><strong>EDIT</strong>: Does this have anything to do with the explanation: <a href=""http://stats.stackexchange.com/questions/173415/at-what-level-is-a-chi2-test-mathematically-identical-to-a-z-test-of-propo/173483#173483"">At What Level is a $\chi^2$ test Mathematically Identical to a $z$-test of Proportions?</a> ?</p>
"
"0.0400641540107502","0.0390434404721515","208277","<p>I find it difficult to connect the coefficients of a regression model that includes splines to the actual prediction equation. For example, how could that be done with the following model?</p>

<pre><code>&gt; library(rms)
&gt; x &lt;- 1:11
&gt; y &lt;- c(0.2,0.40, 0.6, 0.75, 0.88, 0.99, 1.1, 1.15, 1.16, 1.16, 1.16 )
&gt; dd &lt;- datadist(x); options(datadist='dd')
&gt;  f &lt;- ols(y ~ rcs(x, c(3, 5, 7, 9)))
&gt; f  


  Linear Regression Model

ols(formula = y ~ rcs(x, c(3, 5, 7, 9)))

            Model Likelihood     Discrimination    
               Ratio Test           Indexes        
Obs       11    LR chi2     66.08    R2       0.998    
sigma 0.0201    d.f.            3    R2 adj   0.996    
d.f.       7    Pr(&gt; chi2) 0.0000    g        0.383    

Residuals

  Min        1Q    Median        3Q       Max 
-0.027360 -0.011739  0.001227  0.009892  0.031166 

           Coef    S.E.   t     Pr(&gt;|t|)
Intercept  0.0465 0.0224  2.08 0.0762  
x          0.1741 0.0072 24.18 &lt;0.0001 
x'        -0.1004 0.0311 -3.23 0.0144  
x''        0.0542 0.0913  0.59 0.5715  


&gt; Function(f)
function(x = 6){
  0.046475489 + 0.17411942*x - 0.002790266*pmax(x-3,0)^3 + 
  0.0015048699*pmax(x-5,0)^3 + 0.0053610582*pmax(x-7,0)^3 - 
  0.0040756621*pmax(x-9,0)^3 
}
</code></pre>
"
"0.0566592699670073","0.0552157630374233","209030","<p>I fitted a Cox PH model in R with the survival package and the <code>coxph</code> function.
I get the beta estimates from this model.
How can I use these coefficients to manually predict on new data, like the predict function does.</p>

<p>In a linear regression this is just the matrix multiplication <code>X %*% beta</code> if $X$ is the data and $beta$ is the vector of coefficients.</p>

<p>How is this in the Cox model? I also see that predict has several options for types of predictions.</p>

<p>here is a minimal example:</p>

<pre><code>library(survival)
data(""ovarian"")
m &lt;- coxph(formula = Surv(futime, fustat) ~., data=ovarian)
</code></pre>

<p>these two give different results:</p>

<pre><code>head(as.matrix(ovarian[, -c(1:2)]) %*% m$coefficients)

      [,1]
1 10.102002
2 10.371810
3  9.706097
4  6.820160
5  7.357138
6  7.627324

head(predict(m, ovarian))
          1           2           3           4           5           6 
 2.66935119  2.93915962  2.27344680 -0.61249088 -0.07551308  0.19467374 
</code></pre>
"
"0.0566592699670073","0.0552157630374233","209747","<p>I am building CTR(<a href=""https://en.wikipedia.org/wiki/Click-through_rate"" rel=""nofollow"">https://en.wikipedia.org/wiki/Click-through_rate</a>)
Click prediction model with different (61) variables.Dependent variable is weather 0/1( click).I have build logistic regression model and getting probabilities of click for different combination of independent variable.</p>

<p>I am confused about model validation-</p>

<p>1)  What are the parameters should I use for model validation?</p>

<p>2) I am not classifying anything but using classification model for click through rate prediction so using Pseudo R square/ likelihood ration would work?</p>

<p>3) Is there any strategy that I can use for model validation?</p>
"
"0.120797969451519","0.129492442570703","209999","<p>Let's say I fit a regression model to standardized data and then use it to predict out-of-sample data. I evaluate model performance using mean squared error.</p>

<p>I compare this model to another model that simply selects a single predictor and 
calculates mean squared error as: <code>mean((DV - single predictor)^2)</code></p>

<p>The weird thing is that this single predictor model performs better than regression.</p>

<p>What am I missing?</p>

<pre><code>    #load data
    data(mtcars)

    #standardize data
    data &lt;- scale(mtcars)

    #matrix for storing results of a 100 replications
    MSE &lt;- matrix(0,ncol=2,nrow=100)

    #replicate procedure 100 times
    for(reps in 1:100){

    #divide data into training and test set
    train &lt;- sample(1:nrow(data),nrow(data)*0.5)
    test &lt;- setdiff(1:nrow(data),train)

    train.data &lt;- as.data.frame(data[train,])
    test.data &lt;- as.data.frame(data[test,])

    #fit linear regression and get predictions for test data
    fit &lt;- lm(train.data[,1]~.,data=train.data[,2:ncol(train.data)])
    preds &lt;- suppressWarnings(predict(fit,test.data))

    #calculate mean squared error for regression  
    MSE[reps,1] &lt;- mean((test.data[,1] - preds)^2)

    #calculate mean squared error for the single variable prediction
    MSE[reps,2] &lt;- mean((test.data[,1] - test.data[,5])^2)

    }

    #compare MSE for regression and single variable prediction.    
    colMeans(MSE)
    [1] 1.3268521 0.6020453
</code></pre>
"
"NaN","NaN","210651","<p>I would like to use the OOB cases from a random forest fit to estimate the mean squared prediction error so I don't have to cross-validate. I am using the randomForest package in R. It is clear from the documentation that OOB error is reported for classification, but I can't figure out how to get OOB MSPE for regression. Am I missing it or is it truly not reported, which seems odd?</p>
"
"0.184295108449451","0.187408514266327","211518","<p>I am trying to understand how KRR works for drug-protein-interaction and many aspects of it seem very confusing.</p>

<p>Supposing I have a data set as follows of Drug-Protein interactions; values show how tightly a drug binds to a target, some of the interactions are missing (NaN), and those are the ones I am trying to predict.
Numbers I am giving here are only and only made-up numbers for the sake of explanation, since I cannot copy the entire data set as it contains 100 drugs and 100 proteins. So every number you see here is just a random number!</p>

<pre><code>           [,Protein1] [,Protein2] [,Protein3] [,Protein4] [,Protein5] [,Protein6]
[Drug1,]  6.763232 8.97455 5.655 3.3245454 NaN 3.9232321
[Drug2,]  1.211123 2.34343 9.344 NaN 5.6445 4.343
[Drug3,]  1.3429286  2.8805642 6.1998635 Nan 2.328635 9.34343
[Drug4,]  6.5210577  7.1228635 NaN 4.1228635 4.9998635 6.002805
[Drug5,]  NaN  0.9230754 8.34343 9.09098 7.66575 3.9900
[Drug6,] 1.2167197 0.6700215 0.999 NaN 5.553 1.34343
</code></pre>

<p>The approach used in drug discovery is then to compute similarities between proteins and similarities between drugs.</p>

<p>Therefore, there is a <strong>Drug Kernel</strong> computed to show similarities between all drugs (e.g. from online databases).</p>

<pre><code>           [,Drug1] [,Drug2] [,Drug3] [,Drug4] [,Drug5] [,Drug6]
[Drug1,]  6.454 8.788 5.655 3.3245454 3.32233 3.9232321
[Drug2,]  6.211123 7.34343 9.344 1.2121 5.6445 4.343
[Drug3,]  5.3429286  2.8805642 6.1998635 6.7765 2.328635 9.34343
[Drug4,]  4.5210577  1.1228635 7.34 2.1228635 3.9998635 5.002805
[Drug5,]  9.34  0.9230754 1.34343 9.09098 7.66575 3.9900
[Drug6,]  1.2167197 0.6700215 1.999 1.23 5.553 1.34343
</code></pre>

<p>And then protein similarities are computed based on some approach. This matrix will be the <strong>Protein Kernel</strong>.</p>

<pre><code>           [,Protein1] [,Protein2] [,Protein3] [,Protein4] [,Protein5] [,Protein6]
[Protein1,]  50 80 90 10 20 30
[Protein2,]  60 70 10 10 35 75
[Protein3,]  99 89 51 69 48 10
[Protein4,]  10 54 68 97 64 17
[Protein5,]  60 58 95 64 10 16
[Protein6,]  88 14 97 63 63 10
</code></pre>

<p>Then the Kronecker Product is computed for Drug Kernel and Protein Kernel, which directly relates protein-drug pairs.</p>

<p><a href=""http://i.stack.imgur.com/1CRW4.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1CRW4.png"" alt=""enter image description here""></a></p>

<p>Here K is the matrix containing Kronecker Products. So basically, it's a bigger matrix, for this case where we have 6 Proteins and 6 Drugs, the K matrix becomes a 36 x 36 matrix.</p>

<p>Now alpha coefficients are computed for Kernel Ridge Regression with the following formula.</p>

<p><a href=""http://i.stack.imgur.com/iJppt.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/iJppt.png"" alt=""enter image description here""></a></p>

<p>K is the kernel matrix that relates drug-target pairs [therefore, Kronecker Products]
y is the vector with the labels (binding affinities) [So I assume it is just the vector version of the very first matrix in this post, that is the Drug-Protein interaction matrix, <strong>is this correct?</strong>]
I is the identity matrix (of the same size as the kernel matrix),
lambda is the regularization parameter, set preferably to 0.1.</p>

<p>Up to here, I have been able to do everything in R. But my problem starts when I have to do the actual prediction. I do not understand the idea behind KRR, and how to predict those NaN values based on the Kronecker Product K matrix values..</p>

<p>The formula for KRR is:
To compute the prediction for the test point using the equation for g(x) this is the formula</p>

<p><a href=""http://i.stack.imgur.com/HE2zZ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/HE2zZ.png"" alt=""enter image description here""></a>
where x is a test point and x_iâ€™s are training points</p>

<p>My biggest confusion here is, <strong><em>WHAT should I actually put instead of X and X_i?</em></strong> Out of all the matrices I have, which is X for the formula above and which one contains the X_i values?
And how actually can the values in the K matrix be the basis for predicting the values in the very first matrix here?!</p>

<p><strong>Any help and guidance</strong> will be extremely appreciated as I am very confused understanding how KRR works, especially understanding how it works for Drug-Target interaction when having Kronecker Products. So any input here will be really welcome</p>

<p>(<a href=""http://arxiv.org/pdf/1601.01507.pdf"" rel=""nofollow"">http://arxiv.org/pdf/1601.01507.pdf</a>  A paper analyzing what I am trying to do, i.e. relating drugs to proteins by Kronecker Products and then applying KRR, reading the whole paper didn't really clear up anything for me.)</p>
"
"0.0693931503088838","0.0676252226000574","212027","<p>I am using <code>gbm</code> to predict an imbalanced binary outcome, with the intent of obtaining a ranking by class probability estimation that produces a strong class separation on out-of-sample data.  (I am combining this class probability with other predictions, including from logistic regression, in an ensemble model.)</p>

<p>According to <a href=""http://bioconductor.wustl.edu/extra/vignettes/gbm/inst/doc/gbm.pdf"" rel=""nofollow"">this gbm vignette</a> (Ridgeway, 2007), under ""common user options"" for loss functions:</p>

<blockquote>
  <p>This should be easily dictated by the application.  For most
  classification problems either <code>bernoulli</code> or <code>adaboost</code> will be
  appropriate, the former being recommended. (p. 5)</p>
</blockquote>

<p>There's no explanation provided for favoring bernoulli over <a href=""http://stats.stackexchange.com/questions/37497/how-to-use-r-gbm-with-distribution-adaboost"">adaboost</a> nor any mention of the option for <a href=""https://en.wikipedia.org/wiki/Huber_loss"" rel=""nofollow""><code>huberized</code> loss function</a>, although this function may have been added at a later date.</p>

<p>Related question, but broader than mine:  <a href=""http://stats.stackexchange.com/questions/112359/choosing-between-loss-functions-for-binary-classification"">Choosing between loss functions for binary classification</a>.  This answer references <a href=""http://www.eecs.berkeley.edu/~wainwrig/stat241b/bartlettetal.pdf"" rel=""nofollow"">Bartlett (2006)</a> which is a challenging read for me.</p>

<p>Although performance is satisfactory under the bernoulli loss function, I am having a hard time understanding the justification for selecting one over another.  I'm trying all of them, but are there any theoretical justifications that are at least somewhat intuitive?</p>
"
"0.0981367343026181","0.0956365069595007","213253","<p>In general, my question is how to estimate some prediction intervals in the case of penalized linear models (in particular, I think about the glmnet R package). I understood that the introduction of a penalization in the objective function generates a shrinkage effect, which is a bias on the estimated coefficients. 
I understand that in this case the calculation of the uncertainties is troublesome</p>

<p><a href=""https://air.unimi.it/retrieve/handle/2434/153099/133417/phd_unimi_R07738.pdf"" rel=""nofollow"">https://air.unimi.it/retrieve/handle/2434/153099/133417/phd_unimi_R07738.pdf</a></p>

<p>(see sections 3.2 and 3.3 the quoted papers)</p>

<p>Two bootstrap methods (random x vs fixed x) are discussed in the context of standard linear models here</p>

<p><a href=""http://stats.stackexchange.com/questions/64813/two-ways-of-using-bootstrap-to-estimate-the-confidence-interval-of-coefficients?rq=1"">Two ways of using bootstrap to estimate the confidence interval of coefficients in regression</a></p>

<p>but again the focus is on the beta coefficients.
However, I am not interested in the estimate of the confidence intervals on the beta coefficients, but only on the predicted values.
For instance, consider the following R code</p>

<pre><code>library(glmnet)


# Generate data
set.seed(19875)  # Set seed for reproducibility
n &lt;- 1000  # Number of observations
p &lt;- 5000  # Number of predictors included in model
real_p &lt;- 15  # Number of true predictors
x &lt;- matrix(rnorm(n*p), nrow=n, ncol=p)
y &lt;- apply(x[,1:real_p], 1, sum) + rnorm(n)

# Split data into train (2/3) and test (1/3) sets
train_rows &lt;- sample(1:n, .66*n)
x.train &lt;- x[train_rows, ]
x.test &lt;- x[-train_rows, ]

y.train &lt;- y[train_rows]
y.test &lt;- y[-train_rows]



fit.elnet &lt;- glmnet(x.train, y.train, family=""gaussian"", alpha=.5)

yhat &lt;- predict(fit.elnet, s=fit.elnet$lambda, newx=x.test)
</code></pre>

<p>Does anybody know how to calculate a meaningful confidence interval for yhat?</p>

<p>Thanks!</p>
"
"0.0981367343026181","0.0956365069595007","213571","<p>I am trying to do L2-regularized MLR on a data set using caret. Following is what I have done so far to achieve this:</p>

<pre><code>r_squared &lt;-  function ( pred, actual){
    mean_actual = mean (actual)
    ss_e = sum ((pred - actual )^2)
    ss_total = sum ((actual-mean_actual)^2 )
    r_squared = 1 - (ss_e/ss_total)
}

df = as.data.frame(matrix(rnorm(10000, 10, 3), 1000))
colnames(df)[1] = ""response""
set.seed(753)
inTraining &lt;- createDataPartition(df[[""response""]], p = .75, list = FALSE)
training &lt;- df[inTraining,]
testing  &lt;- df[-inTraining,]
testing_response &lt;- base::subset(testing,
                                 select = c(paste (""response"")))
gridsearch_for_lambda =  data.frame (alpha = 0,
                                      lambda = c (2^c(-15:15), 3^c(-15:15)))
regression_formula = as.formula (paste (""response"", ""~ "", "" ."", sep = "" ""))
train_control = trainControl (method=""cv"", number =10,
                              savePredictions =TRUE , allowParallel = FALSE )
model = train (regression_formula,
                           data = training,
                           trControl = train_control,       
                           method = ""glmnet"",
                           tuneGrid =gridsearch_for_lambda,
                           preProcess = NULL
            )
prediction = predict (model, newdata = testing)
testing_response[[""predicted""]] = prediction
r_sq = round (r_squared(testing_response[[""predicted""]],
              testing_response[[""response""]] ),3)
</code></pre>

<p>Here I am concerned about assurance that the model I am using for prediction is the best one (the optimal tuned lambda value).</p>

<p>P.S.: The data is sampled from random normal distribution, which is not giving a good R^2 value, but I want to get the idea correctly</p>
"
"0.0400641540107502","0.0585651607082273","214665","<p>I have 15-minute streamflow observations for a small stream, but the dataset has some gaps in it. I want to fill the gaps with a regression using observations from a nearby stream (and quantify the uncertainty caused by these gaps). </p>

<p>If I use a linear relationship between the two streams, I get negative predictions for my target stream when the other has low flow, because my target stream dries out before the predictor stream. If I force the regression through zero (it has just a slope), then the model doesnâ€™t fit very well, which influences my results for the uncertainty analysis.</p>

<p>Is a truncated or a linear regression a good solution?  If so, how do I fit the two parameters?</p>

<p>Is there a better model?  It should be zero when my predictor stream has low flow, and then increase roughly in proportion to flow at the predictor stream. I'm working in R if anyone has suggestions for functions.</p>
"
"NaN","NaN","214886","<p>Suppose I'm doing regression with training, validation, and test sets. I can find RMSE and R squared (R^2, the coefficient of determination) from the output of my software (such as R's lm() function).</p>

<p>My understanding is that the test RMSE (or MSE) is the measure of goodness of predicting the validation/test values, while R^2 is a measure of goodness of fit in capturing the variance in the training set.</p>

<p>In the real world, what I really care about is generalized prediction accuracy on data I haven't seen. So then what is the utility of the R^2 value compared to RMSE?</p>
"
"0.0693931503088838","0.0676252226000574","214892","<p>I'm trying to construct a univariate prediction model using logistic regression in order to predict credit default likelihood from overdue level in telecommunication companies:</p>

<p><a href=""https://drive.google.com/open?id=0BzdYGYN6vfqBeDdKRDFpelFKbTg"" rel=""nofollow"">https://drive.google.com/open?id=0BzdYGYN6vfqBeDdKRDFpelFKbTg</a></p>

<p>For this, I used the function glm and found two problematic ranks:
        RANK_OVERDUE between S/. 3,000 and S/. 5,000 &amp; RANK_OVERDUE More than S/. 5,000.</p>

<p>which have p-values of 0.946 and 0.473:</p>

<pre><code>Call:
glm(formula = impago ~ MONTO_VENCIDO_DOC_IMPAGOS, family = binomial, 
    data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.1355  -0.0569  -0.0569  -0.0569   3.5855  

Coefficients:
                                                  Estimate
(Intercept)                                       -6.42627
RANK_OVERDUE&lt;S/. 0 - S/. 500]         0.69763
RANK_OVERDUE&lt;S/. 1,500 - S/. 3,000]   1.73952
RANK_OVERDUE&lt;S/. 3,000 - S/. 5,000] -10.13980
RANK_OVERDUE&lt;S/. 500 - 1,500]         1.13854
RANK_OVERDUEMÃ¡s de S/. 5,000          0.71916
</code></pre>

<p></p>

<pre><code>                                                 Pr(&gt;|z|)    
(Intercept)                                       &lt; 2e-16 ***
RANK_OVERDUE&lt;S/. 0 - S/. 500]       1.78e-15 ***
RANK_OVERDUE&lt;S/. 1,500 - S/. 3,000] 2.51e-05 ***
RANK_OVERDUE&lt;S/. 3,000 - S/. 5,000]    0.946    
RANK_OVERDUE&lt;S/. 500 - 1,500]       1.23e-06 ***
RANK_OVERDUEMÃ¡s de S/. 5,000           0.473    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 9683.9  on 345828  degrees of freedom
Residual deviance: 9603.5  on 345823  degrees of freedom
AIC: 9615.5

Number of Fisher Scoring iterations: 15
</code></pre>

<p>I would need to know what options I have on order to deal with this situation. Should these ranks be included in the model? I tried to join them into one (overdue over S/. 3,000) but when applying again the model, it continued to be not significant (I obtained a p-value of 0.919).</p>
"
"0.0908569611434023","0.103299233817667","217529","<p>I'm on my project to predict the amount of demand of products in a store. we have lot of 0 on the amount of sales of products so when I did multiple regression, predictions of the demand had negative number and I got weird residual plot.
<a href=""http://i.stack.imgur.com/hSqAE.png"" rel=""nofollow"">enter image description here</a></p>

<p>1.It has big difference between under 500 and over 500 in my plot. I want to use weight to make nice residual plot. but How can I decide weight?</p>

<p>And I don't want negative predicted amount of demand. so I used Negative binomial regression and poisson regression to prevent making negative number in prediction. and I got some crazy residual plot. 
<a href=""http://i.stack.imgur.com/0EswV.png"" rel=""nofollow"">enter image description here</a></p>

<p>2.Can I keep using this binomial regression for prediction?
3.Could you recommend any other useful regression model? 
4.If variance of data is bigger than mean of data, should I have to use quasi-poisson regression? </p>
"
"0.0400641540107502","0.0390434404721515","218842","<p>I'm using a random forest in R (randomForest) to predict a binary output (1,0) for a dataset that is heavily unbalanced. In this example let's assume the population has 1% 1's and 99% 0's.</p>

<p>Building the random forest on such unbalanced data is difficult and I get much better results when building it on a 50:50 sample.  When predicting a validation set, I obtain the % of trees that predicted that data point to be a 1.  For example, customer A has a 75% probability of being a 1 (based on the # of trees that predicted 1)</p>

<p>If I want to re-scale these predictions back to the original population ratio of 1:99, is there a good way to do this?</p>

<p>In the past I've used logistic regression, and I can adjust the intercept accordingly to down-scale the predicted probability.</p>

<p>Is there a good way to think about this from the RF point of view?  Can I simply just down-weight the predictions from the 50:50 sample by 50 (50% down to 1%)?</p>

<p>Thanks in advance for any thoughts and help</p>
"
"0.155167801263525","0.151214594727283","221161","<p>I have been having trouble with the predict function underestimating (or overestimating) the predictions from an lmer model with some polynomials. Hopefully my edits make it clearer. I have scaled data that looks like this:</p>

<pre><code>Terr      Date     Year            Age  
T.092     123      0.548425     -0.86392            
T.104     102      1.2072       -0.48185            
T.104     105      1.075445     -0.86392            
T.104     112      0.94369      -1.24599            
T.040     116     -0.2421        2.192652           
T.040     114     -0.37386       1.810581           
T.040     119     -0.50561       1.428509           
T.040     128      0.15316      -0.09978            
T.040     113      0.021405     -0.48185
</code></pre>

<p>Iâ€™m trying to determine how Year affects lay date after controlling for Age, with Terr (territory) as a random variable. I usually include polynomials and do model averaging, but whether I use a single model or do model averaging, the predict function gives predictions that are a bit lower or higher than they should be. I realize that the model below would not be a good model for this data, Iâ€™m just trying to provide a simplified example.  </p>

<p>Below is my code  </p>

<pre><code>library(lme4
m1 &lt;- lmer(Date ~ (1|Terr) + Year + Age + I(Age^2), data=data)
new.dat &lt;- data.frame(Year = c(-2,-1,0,1),
                  Age=mean(data$Age, na.rm=TRUE))
Predictions=predict(m1, newdata = new.dat, re.form=NA)
pred.l&lt;-cbind(new.dat, Predictions)
pred.l  

      Year          Age Predictions
    1   -2 2.265676e-16    124.4439
    2   -1 2.265676e-16    123.2124
    3    0 2.265676e-16    121.9810
    4    1 2.265676e-16    120.7496
</code></pre>

<p>When plotted with the means, the graph looks like this:</p>

<p><a href=""http://i.stack.imgur.com/mwIpJ.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/mwIpJ.jpg"" alt=""graph1""></a></p>

<p>When I use effects, I get a much better fit  </p>

<pre><code>library(effects)
ef.1c=effect(c(""Year""), m1, xlevels=list(Year=-2:1))
pred.lc=data.frame(ef.1c)
pred.lc

      Year      fit        se    lower    upper
    1   -2 126.0226 0.6186425 124.8089 127.2363
    2   -1 124.7911 0.4291211 123.9493 125.6330
    3    0 123.5597 0.3298340 122.9126 124.2068
    4    1 122.3283 0.3957970 121.5518 123.1048
</code></pre>

<p><a href=""http://i.stack.imgur.com/SvI3f.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/SvI3f.jpg"" alt=""graph2""></a></p>

<p>After much trial and error, I have discovered that the problem is with the Age polynomial, because when the Age polynomial is not included, the predicted and fitted are equal and both fit well. Below is the same  model but with Age as a linear term.  </p>

<pre><code>m2 &lt;- lmer(Date ~ (1|Terr) + Year + Age, data=data)
new.dat &lt;- data.frame(Year = c(-2,-1,0,1),
                  Age=mean(data$Age, na.rm=TRUE))
Predictionsd=predict(m2, newdata = new.dat, re.form=NA)  
pred.ld&lt;-cbind(new.dat, Predictionsd)
pred.ld

      Year          Age Predictionsd
    1   -2 2.265676e-16     125.9551
    2   -1 2.265676e-16     124.7653
    3    0 2.265676e-16     123.5755
    4    1 2.265676e-16     122.3857

library(effects)
ef.1e=effect(c(""Year""), m2, xlevels=list(Year=-2:1))
pred.le=data.frame(ef.1e)
pred.le

      Year      fit        se    lower    upper
    1   -2 125.9551 0.6401008 124.6993 127.2109
    2   -1 124.7653 0.4436129 123.8950 125.6356
    3    0 123.5755 0.3406741 122.9072 124.2439
    4    1 122.3857 0.4093021 121.5827 123.1887
</code></pre>

<p>I do many similar analyses, and this issue with the predictions being slightly lower (or higher) than they should be often happens when Age is included as a polynomial. When I include a polynomial for Year, there is no problem and the predicted and fitted are equal, so I know the problem is not with all polynomials.</p>

<pre><code>m3 &lt;- lmer(Date ~ (1|Terr) + Year + I(Year^2) + Age, data=data)

new.dat &lt;- data.frame(Year = c(-2,-1,0,1),
                  Age=mean(data$Age, na.rm=TRUE))
Predictionsf=predict(m3, newdata = new.dat, re.form=NA)  
pred.lf&lt;-cbind(new.dat, Predictionsf)
pred.lf

      Year          Age Predictionsf
    1   -2 2.265676e-16     125.6103
    2   -1 2.265676e-16     124.8494
    3    0 2.265676e-16     123.7483
    4    1 2.265676e-16     122.3070

library(effects)
ef.1g=effect(c(""Year""), m3, xlevels=list(Year=-2:1))
pred.lg=data.frame(ef.1g)
pred.lg

      Year      fit        se    lower    upper
    1   -2 125.6103 0.8206625 124.0003 127.2203
    2   -1 124.8494 0.4615719 123.9438 125.7549
    3    0 123.7483 0.4275858 122.9094 124.5871
    4    1 122.3070 0.4262110 121.4708 123.1431
</code></pre>

<p>I've looked for answers (e.g., <a href=""http://stats.stackexchange.com/questions/180010/overestimated-and-underestimated-predictions-in-regression"">here</a>) but haven't found anything that is directly helpful. I can provide the whole data set if needed. Does anyone have any insight?</p>
"
"0.140224539037626","0.156173761888606","221510","<p>I'm new to logistic regression analysis, and was unable to find an answer elsewhere in Cross Validated or Stack Overflow. </p>

<p>Consider a standard logistic regression analysis of a binary outcome (admission to college) based on continuous covariates gre score and high school gpa, and ordinal categorical rank prestige of the undergraduate institution (data from the nice UCLA stats dept. logistic regression in R tutorial: <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/dae/logit.htm</a>)</p>

<pre><code>&gt; admissions.data &lt;- read.csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
&gt; admissions.data$rank &lt;- as.factor(admissions.data$rank)
&gt; summary(admissions.data)
     admit             gre             gpa        rank
 Min.   :0.0000   Min.   :220.0   Min.   :2.260   1: 61
 1st Qu.:0.0000   1st Qu.:520.0   1st Qu.:3.130   2:151
 Median :0.0000   Median :580.0   Median :3.395   3:121
 Mean   :0.3175   Mean   :587.7   Mean   :3.390   4: 67
 3rd Qu.:1.0000   3rd Qu.:660.0   3rd Qu.:3.670
 Max.   :1.0000   Max.   :800.0   Max.   :4.000

&gt; fit1 &lt;- glm(admit ~ gre + gpa + rank, data = admissions.data, family=""binomial"")
&gt; summary(fit1)

Call:
glm(formula = admit ~ gre + gpa + rank, family = ""binomial"",
    data = admissions.data)

Deviance Residuals:
    Min       1Q   Median       3Q      Max
-1.6268  -0.8662  -0.6388   1.1490   2.0790

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -3.989979   1.139951  -3.500 0.000465 ***
gre          0.002264   0.001094   2.070 0.038465 *
gpa          0.804038   0.331819   2.423 0.015388 *
rank2       -0.675443   0.316490  -2.134 0.032829 *
rank3       -1.340204   0.345306  -3.881 0.000104 ***
rank4       -1.551464   0.417832  -3.713 0.000205 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 499.98  on 399  degrees of freedom
Residual deviance: 458.52  on 394  degrees of freedom
AIC: 470.52

Number of Fisher Scoring iterations: 4

# Odds Ratios
&gt; exp(coef(fit1))
(Intercept)         gre         gpa       rank2       rank3       rank4
  0.0185001   1.0022670   2.2345448   0.5089310   0.2617923   0.2119375

# 95% confidence intervals
&gt; exp(confint(fit1))
Waiting for profiling to be done...
                  2.5 %    97.5 %
(Intercept) 0.001889165 0.1665354
gre         1.000137602 1.0044457
gpa         1.173858216 4.3238349
rank2       0.272289674 0.9448343
rank3       0.131641717 0.5115181
rank4       0.090715546 0.4706961
</code></pre>

<p>My questions are:</p>

<p>1) In R, is there a straight-forward way to determine ORs with 95% CIs for specific values of the covariates? E.g., based on this model, what are the odds of college acceptance for students applying to a rank 2 schools with a gpa of 3 and a gre score of 750, compared with a student applying to a rank 3 school with the same gpa and gre score? I could calculate ORs by hand given the model coefficient estimates and these specific covariate values, but am unsure how to correctly propagate SEs to calculate 95% CIs.</p>

<p>2) Would this particular example be considered a case-control study design, and therefore odds ratios could be estimated, but not predictions? (See: <a href=""http://stats.stackexchange.com/questions/69561/case-control-study-and-logistic-regression"">Case-control study and Logistic regression</a>)</p>
"
"0.127220775566287","0.135250445200115","222479","<p>I'm new in this area, hope my question is understandable.
I need to fit conditional logistic regression model in R and use it for predictions on unseen data (output should be probability).
My datasets are  quite large (over 150k rows) and contains many (~500) noisy features.
I found package called <strong>clogitboost</strong> and tried to use it with relatively small number of boosting iterations (max 30, because with larger values it takes too long to compute and raises an error in the end - perhaps, it's resources limitations) - results are mediocre. I tried to use unconditional approach with regularization - <strong>glmnet</strong> and got better results, however, due nature of data I guess it will be better to use conditional regression with regularization similar to what is used in <strong>glmnet</strong> (tried to remove some features and apply <strong>clogitboost</strong> again and got slightly better results). There is package called <strong>clogitL1</strong> , which seems to do that, I tried to use it and it fits model quickly, but it doesn't provide <strong>predict()</strong> function, Usage described in  paper with attached R code here:
<a href=""https://www.jstatsoft.org/article/view/v058i12"" rel=""nofollow"">https://www.jstatsoft.org/article/view/v058i12</a>, they made some predictions in some way, but I can't understand it. Can I somehow manually predict using  unseen data, just like it's possible with <strong>clogitboost</strong> <strong>predict()</strong> (parameters are Model, X and Strata column) using model that was fitted with <strong>clogitL1</strong>? Note: in description of package <strong>clogitL1</strong> - ""Tools for the fitting and cross validation of <strong><em>exact</em></strong> conditional logistic regression models"" - so I'm not sure about what ""exact"" means here and  if it makes sense to use that package for my purposes. If it's not possible to predict, then, should I manually select features by checking their ""importance"" that can be found in <strong>clogitL1</strong> model? </p>
"
"0.0981367343026181","0.0956365069595007","222544","<p>I used the package for random forest. It is not clear to me how to use the results. 
In  logistic regression you can have an equation as an output, in standard tree some rules. If you receive a new dataset you can apply the equation on the new data and predict an outcome (like default/no default). Or saying the customers with characteristics a and characteristics b will have a default, so you can predict the outcome before it happens. That is the scoring tecnique.</p>

<p>Is it possible to use random forest in a similar situation, or how would you use the results of a RF? </p>

<p>my python code:</p>

<pre><code>from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score

#creating a test and train dataset

from sklearn.cross_validation import train_test_split

train, test = train_test_split(df, test_size = 0.3)

clf = RandomForestClassifier(max_depth = 30, min_samples_split=2, n_estimators = 200, random_state = 1)

#training the model
clf.fit(train[columns], train[""churn""])

#testing the model
predictions = clf.predict(test[columns])

print(predictions)

print(roc_auc_score(predictions, test[""churn""]))
</code></pre>
"
"0.0981367343026181","0.0956365069595007","222803","<p>I am using elastic net regression on a dataset with quite a small number of observations (clinical risk scores) and large number (1000+) of potential predictor variables (gene expression values). The ultimate aim is to identify variables (genes) that could be explored further experimentally. </p>

<p>However, I noticed that the variables being selected (e.g. coefficients not equal to zero) vary when I leave out a single observation from the dataset (some are maintained, some drop, some are added in), and I would like get some more confidence regarding which variables are relatively robust to such changes.</p>

<ol>
<li><p>Would it be methodologically acceptable to generate either jackknife or bootstrap datasets from my original dataset, and repeat the whole model selection procedure (e.g. repeated selection of tuning parameters based on cross validation and the associated model) for each of these datasets, and then for each of those models determine how often a variable was selected?</p></li>
<li><p>Would it be acceptable to re-run a linear regression analysis with sets of selected genes of decreasing frequency of selection (e.g. only use the top 3 genes selected most often, or the top 4 etc), and select the cut-off of frequency of selection of genes based on the cross validation metrics I get for each of these models? If I do this I see a sort of leveling of cross validation prediction error metrics after a certain point (e.g. adding further variables does not help to reduce prediction error metrics). This second approach feels like a ""wrong"" thing to do. However, it is just to determine a threshold of which subset of selected genes I want to focus on further.</p></li>
</ol>
"
"0.0895861718290583","0.0873037869711973","223447","<p>Let's suppose I have <em>p</em> predictor variables. For those predictors, there exists a weight vector <em>w</em> of length <em>p</em> that, if multiplied by the predictors, will minimize an error function. This is not any different than what linear regression performs when the error metric is RMSE. The problem is that I am not using RMSE to determine performance. Instead, I must multiply my weights by my predictors, then plug them into a complex function that takes .5 seconds to compute, and only then do I know if my error improved or worsened. </p>

<p>Pseudo R Code:</p>

<pre><code>vec=rnorm(150,0,1)
p=matrix(unlist(split(vec, ceiling(seq_along(vec)/15))),ncol=10)
response=rnorm(15,0,1)
w=rnorm(15,0,1)

for(i in 1:500){
  #multiply predictors by weights to get predictions
  preds=colSums(t(p)*w)

  #complex error function that takes .5 seconds, e.g.:
  #this isn't the true error function, just an example:
  preds=ifelse(preds&gt;1,preds,ifelse(preds&lt;=1&amp;preds&gt;0,0,-1)) 
  error=mean(abs(response-preds))

  #update weight vector w to move in the most optimal pattern to minimize error
  w= ???
}
</code></pre>

<p>How to update <em>w</em> in the most efficient manner?</p>
"
"0.0801283080215004","0.078086880944303","224078","<p>I have three related questions on the package CausalImpact in R. The package can be found <a href=""https://github.com/google/CausalImpact"" rel=""nofollow"">here</a> and a reproducible example is below.</p>

<ol>
<li>Do I basically understand correctly, that the model makes ""1-step ahead""
predictions? I assume it works like a simple lm model that makes
lots of regressions for t+1 with predictors values from t-1 and then looks for the most contributory predictors?</li>
<li>When talking about ""coefficients"", does that mean they are (Pearson)
correlation coefficients?</li>
<li>The function <code>plot(impact$model$bsts.model,""coefficients"")</code> produces
a plot with inclusion probabilities ranging from 0 to 1. Is there
any way to access the actual values in a table? I found
that <code>colMeans(impact$model$bsts.model$coefficients)</code> provides some
values but I'd like to have a confirmation for this.</li>
<li>In my code, I changed <code>bsts.model &lt;- bsts(y ~ x1, ss, niter = 1000)</code> to <code>bsts.model &lt;- bsts(y ~ x1 + x2 + x3 + x4 + x5, ss, niter = 1000)</code> in order to get values for different variables that I defined before. I did not change the cbind functions for that, as I weren't sure if that was necessary. Now I wonder what to do when I do have a table with -let's say- 200 predictors? Do I have to enter x1 to x200 after <code>bsts(y ~</code> to find the best predictors?</li>
</ol>

<p>R code below:</p>

<pre><code>install.packages(""devtools"")
library(devtools)
devtools::install_github(""google/CausalImpact"")
#Download the tar from the git and then install the package in RStudio.

library(CausalImpact)

set.seed(1)
x1 &lt;- 100 + arima.sim(model = list(ar = 0.999), n = 100)
x2 &lt;- 50 + arima.sim(model = list(ar = 0.899), n = 100)
x3 &lt;- 80 + arima.sim(model = list(ar = 0.799), n = 100)
x4 &lt;- 1.25 * x1 + rnorm(100)
x5 &lt;- 101 + arima.sim(model = list(ar = 0.999), n = 100)
y &lt;- 1.2 * x1 + rnorm(100)
y[71:100] &lt;- y[71:100] + 10
data &lt;- cbind(y, x1)

dim(data)
head(data)
data
matplot(data, type = ""l"")

time.points &lt;- seq.Date(as.Date(""2014-01-01""), by = 1, length.out = 100)
data &lt;- zoo(cbind(y, x1), time.points)
head(data)

pre.period &lt;- as.Date(c(""2014-01-01"", ""2014-03-11""))
post.period &lt;- as.Date(c(""2014-03-12"", ""2014-04-10""))

impact &lt;- CausalImpact(data, pre.period, post.period)
plot(impact)

summary(impact)
summary(impact, ""report"")
impact$summary

post.period &lt;- c(71, 100)
post.period.response &lt;- y[post.period[1] : post.period[2]]
y[post.period[1] : post.period[2]] &lt;- NA

ss &lt;- AddLocalLevel(list(), y)

bsts.model &lt;- bsts(y ~ x1 + x2 + x3 + x4 + x5, ss, niter = 1000)

impact &lt;- CausalImpact(bsts.model = bsts.model,post.period.response =     post.period.response)

plot(impact)
summary(impact)
summary(impact, ""report"")

plot(impact$model$bsts.model,""coefficients"")
plot(impact$model$bsts.model, ""coef"", inc = .1)
plot(impact$model$bsts.model, ""coef"", inc = .05)

colMeans(impact$model$bsts.model$coefficients)
</code></pre>
"
"0.0895861718290583","0.0873037869711973","224539","<p>I have a dataset with about 30 potential predictors and 115 observations. I'm looking into building a prediction model with the data using logistic regression.</p>

<p>From what I have read - the typical rule of thumb to split the dataset is an 80/20 split, where 80 percent of the dataset will be used for training the model and the remaining 20 percent will be used for validation/testing. </p>

<p>Using the Confusion Matrix from the Caret package in R, the accuracy of the model is 91%, No information Rate of .55, and a significant p value comparing the Accuracy to NIR (P&lt;.0001).</p>

<p>I'm wary to trust these results, since only 92 observations were used for training and 23 observations for testing.</p>

<p>Is the sample size enough to create a generalizable prediction model? If not, how do I determine the required sample size for model training and testing?</p>
"
"0.105999788000636","0.103299233817667","224947","<p>What are some of the best practices and steps to building models for prediction and or inferences? </p>

<p>What have been taught to me during my classes was the steps outlined in Chapter 4 of Hosmer et al. for the ""Purposeful Selection of Covariates"". The method to screen a large dataset with many potential predictors is to use a algorithmic approach such as Stepwise, best subset regression, etc. Then verify the model after the fact for potential collinearity, confounders, etc.  However, I have read much criticism on this site in regards to those said steps and methods.</p>

<p>For example - if I was provided a dataset with ~100 potential predictors, what would be the best practice to selecting those said predictors for inclusion or exclusion of the model for prediction/inference ? </p>

<p>According to Hosmer et al., the steps would be to perform univariate analysis to screen for all of those potential predictors (p &lt; .25), then move to inclusion of those said predictors to a multivariate model. Take a stepwise approach to removing insignificant predictors, then add back and verify the significance of each non significant predictor. </p>

<p>However - the more I've read on this site the more confused I've gotten about what is considered best practices, and I've come to question more and more of what was taught during my classes.</p>

<p>Once again just to reiterate - </p>

<ol>
<li><p>What would be the best practices for building a model for obtaining unbiased measure of association for each individual predictors?</p></li>
<li><p>What would be the best practices for building a model strictly for prediction?</p></li>
</ol>

<p>I'm still learning much about the world of data science and appreciate any help that is provided!</p>

<p>""Hosmer, D. W., Lemeshow, S., &amp; Sturdivant, R. X. 2013. Applied Logistic Regression. Wiley, Chicester"".</p>
"
"0.0400641540107502","0.0390434404721515","226226","<p>I'm new to predictive analytics. I have data variables which are highly skewed, I want to normalize those for better predictions. I've used normalization,standardization. but they gave same data distributions as before. how can I bring my data to Normality,and what techniques should I use.
Is normalizing data variables necessary in every case (clustering, regression, classification) ? 
please help with an example if possible.
Thank you.</p>
"
"0.149906337799172","0.146087177447694","228316","<p>I want to predict a binary response variable <code>y</code> using logistic regression. <code>x1</code> to <code>x4</code> are the log  of continuous variables and <code>x5</code> to <code>x7</code> are binary variables. </p>

<pre><code>Call:
glm(formula = y ~ x1 + x2 + x3 + x4 + x5 + 
    x6 + x7, family = binomial(), data = df)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.6604  -0.5712   0.4691   0.6242   2.4095  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -2.84633    0.31609  -9.005  &lt; 2e-16 ***
x1             0.14196    0.04828   2.940  0.00328 ** 
x2             4.05937    0.22702  17.881  &lt; 2e-16 ***
x3            -0.83492    0.08330 -10.023  &lt; 2e-16 ***
x4             0.05679    0.02109   2.693  0.00709 ** 
x5             0.08741    0.18955   0.461  0.64467    
x6            -2.21632    0.53202  -4.166  3.1e-05 ***
x7             0.25282    0.15716   1.609  0.10769    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1749.5  on 1329  degrees of freedom
Residual deviance: 1110.5  on 1322  degrees of freedom
AIC: 1126.5

Number of Fisher Scoring iterations: 5
</code></pre>

<p>The output of the GLM shows that most of my variables are significant for my model, but the various goodness of fit test I have done:</p>

<pre><code>anova &lt;- anova(model, test = ""Chisq"")   # Anova
1 - pchisq(sum(anova$Deviance, na.rm = TRUE),df = 7) # Null Model vs Most Complex Model
1 - pchisq(model$null.deviance - model$deviance, 
           df = (model$df.null - model$df.residual )) # Null Deviance - Residual Deviance ~ X^2
hoslem.test(model$y, model$fitted.values, g = 8)     # Homer Lemeshow test
pR2(model)                                            # Pseudo-R^2
</code></pre>

<p>tell me that there is a lack of evidence to support my model.</p>

<p>More over, I have a bimodal deviance plot. I suspect the bimodal distribution is caused by the sparsity of my binary variables.
 <a href=""http://i.stack.imgur.com/J27fL.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/J27fL.png"" alt=""enter image description here""></a></p>

<p>So I calculated the absolute error <code>abs(y - y_hat)</code>, and obtained the following:</p>

<ul>
<li>77% of my absolute errors were in [0;0.25], which I think is very good!</li>
</ul>

<p>On the following plot, Y=1 is red, and Y=0 is green. This model is better at predicting when Y will be 1 than 0.</p>

<p><a href=""http://i.stack.imgur.com/ZEGuv.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZEGuv.png"" alt=""enter image description here""></a></p>

<p>My question is thus the following:</p>

<p>The goodness of fit tests all assume that my null hypothesis follows a Chi square distribution of some sort. Is it correct to conclude that based on my absolute error, my model's prediction is OK, it's just that it doesn't follow a Chi square distribution and thus perform poorly with these tests? </p>
"
"0.0600962310161253","0.0585651607082273","228679","<p>I understand that ""glmnet"" package has alpha and lambda regularization parameters which can be optimized by ""caret"" package's train function. Optimal lambda value and lambda values of trained model are in the image. </p>

<p><strong>Can some one please help me understand what these lambda values mean, do they mean while minimizing the criterion function of multinomial regression, the aforementioned lambda values represent values at each iteration?</strong></p>

<pre><code>library(caret)
library(nnet)
ctrl &lt;- trainControl(method = ""repeatedcv"", number = 10, savePredictions = TRUE)
model_train_glmnet &lt;- train(Class2 ~ ZCR + Energy + EntropyE + SpectralC + SpectralS + SpectralE + SpectralF + SpectralR + MFCC1 + MFCC2 + MFCC3 + MFCC4 + MFCC5 + MFCC6 + MFCC7 + MFCC8 + MFCC9 + MFCC10 + MFCC11 + MFCC12 + MFCC13, data = training, method=""glmnet"", trControl = ctrl, tuneLength = 5)

print(model_train_glmnet$finalModel$lambdaOpt)
[1] 0.007676627
&gt; 
&gt; print(model_train_glmnet$finalModel$lambda)
[1] 3.838314e-01 3.497328e-01 3.186635e-01 2.903543e-01 2.645601e-01
[6] 2.410573e-01 2.196424e-01 2.001300e-01 1.823510e-01 1.661514e-01
[11] 1.513910e-01 1.379418e-01 1.256875e-01 1.145217e-01 1.043479e-01
[16] 9.507796e-02 8.663150e-02 7.893539e-02 7.192299e-02 6.553355e-02
[21] 5.971173e-02 5.440710e-02 4.957373e-02 4.516973e-02 4.115698e-02
[26] 3.750071e-02 3.416925e-02 3.113375e-02 2.836791e-02 2.584778e-02
[31] 2.355154e-02 2.145928e-02 1.955290e-02 1.781587e-02 1.623316e-02
[36] 1.479105e-02 1.347706e-02 1.227979e-02 1.118889e-02 1.019490e-02
[41] 9.289211e-03 8.463983e-03 7.712066e-03 7.026948e-03 6.402693e-03
[46] 5.833895e-03 5.315628e-03 4.843402e-03 4.413128e-03 4.021078e-03
[51] 3.663856e-03 3.338369e-03 3.041798e-03 2.771573e-03 2.525354e-03
[56] 2.301009e-03 2.096593e-03 1.910338e-03 1.740629e-03 1.585996e-03
[61] 1.445100e-03 1.316722e-03 1.199748e-03 1.093165e-03 9.960517e-04
[66] 9.075652e-04 8.269396e-04 7.534766e-04 6.865398e-04 6.255495e-04
[71] 5.699774e-04 5.193422e-04 4.732052e-04 4.311670e-04 3.928633e-04
[76] 3.579624e-04 3.261620e-04 2.971867e-04 2.707854e-04 2.467296e-04
[81] 2.248108e-04 2.048393e-04 1.866419e-04 1.700611e-04 1.549534e-04
[86] 1.411878e-04 1.286450e-04 1.172166e-04 1.068034e-04 9.731524e-05
[91] 8.867002e-05 8.079282e-05 7.361541e-05 6.707562e-05 6.111681e-05
[96] 5.568736e-05 5.074025e-05 4.623262e-05 4.212544e-05 3.838314e-05
</code></pre>
"
"0.105999788000636","0.103299233817667","230022","<p>First off, here is my calculation for the confidence intervals:</p>

<pre><code>BPLpredictions[""upr""] = BPLpredictions$fit + (z * BPLpredictions$se.fit)
BPLpredictions[""lwr""] = BPLpredictions$fit - (z * BPLpredictions$se.fit)
</code></pre>

<p>After calculating the running total for all of my predictions, I end up with the following data frame:</p>

<pre><code>&gt; BPLSeason
        upr       lwr       fit Running fit Running lwr Running upr
1  1.046068 0.6191719 0.8326201   0.8326201   0.6191719    1.046068
2  1.066816 0.6655935 0.8662049   1.6988250   1.2847654    2.112885
3  1.088620 0.7136692 0.9011444   2.5999694   1.9984346    3.201504
4  1.167051 0.8622466 1.0146486   3.6146180   2.8606811    4.368555
5  1.112002 0.7629848 0.9374932   4.5521112   3.6236659    5.480557
6  1.112002 0.7629848 0.9374932   5.4896044   4.3866506    6.592558
7  1.088620 0.7136692 0.9011444   6.3907488   5.1003198    7.681178
8  1.242889 0.9534183 1.0981538   7.4889026   6.0537381    8.924067
9  1.201472 0.9096795 1.0555757   8.5444783   6.9634176   10.125539
10 1.201472 0.9096795 1.0555757   9.6000541   7.8730970   11.327011
11 1.292990 0.9919085 1.1424492  10.7425033   8.8650055   12.620001
12 1.201472 0.9096795 1.0555757  11.7980790   9.7746850   13.821473
13 1.242889 0.9534183 1.0981538  12.8962328  10.7281032   15.064362
14 1.242889 0.9534183 1.0981538  13.9943865  11.6815215   16.307252
15 1.292990 0.9919085 1.1424492  15.1368357  12.6734300   17.600241
16 1.292990 0.9919085 1.1424492  16.2792849  13.6653384   18.893231
17 1.352706 1.0243564 1.1885314  17.4678163  14.6896948   20.245938
18 1.292990 0.9919085 1.1424492  18.6102655  15.6816033   21.538928
19 1.352706 1.0243564 1.1885314  19.7987969  16.7059597   22.891634
20 1.422037 1.0509079 1.2364723  21.0352693  17.7568676   24.313671
</code></pre>

<p>So you understand the context of this data frame, fit is the predicted number of goals in a specific game and upr and lwr and the upper and lower bounds of the 95% CI. </p>

<p>The three additional columns are essentially the running total for each. This may be a dumb question but is it ok for me to calculate the running total for the upr and lwr bounds or should I, for example, use <code>upr[20] - fit[20]</code> and add that to <code>running fit[20]</code></p>

<p>Sorry if this is trivial. I'm brand new to R and Stats. </p>

<p>[Additional Information]</p>

<p>The following negative binomial regression model was used:</p>

<pre><code>mod 1 = glm.nb(Goals ~ Defense, data = Messi.Liga)
</code></pre>

<p>Here is the summary of the model:</p>

<pre><code>Coefficients:
 Estimate Std. Error z value Pr(&gt;|z|) 
(Intercept) 3.05943 1.11817 2.736 0.00622 **
Defense -0.03954 0.01498 -2.639 0.00831 **
</code></pre>

<p>This command was used to predict BPL values:</p>

<pre><code>BPLpredictions = data.frame(predict(mod1, bpl.df, type = ""response"", se.fit = TRUE))
</code></pre>

<p>And finally, this is how I calculated the CI (z = 1.96):</p>

<pre><code>BPLpredictions[""upr""] = BPLpredictions$fit + (z * BPLpredictions$se.fit)
BPLpredictions[""lwr""] = BPLpredictions$fit - (z * BPLpredictions$se.fit)
</code></pre>
"
"0.108718172506367","0.129492442570703","230201","<p>I'm using the glmnet package in R to do ridge regression. When I have a full set of dummy variables (if you took a horizontal sum of all these dummy variables you would get the constant), ridge regression with lambda = 0 is NOT dropping any of the dummy variables. In contrast, OLS gives the expected result by dropping at least 1 of the dummies to prevent perfect multi-collinearity. I'd like to know why the discrepancy exists. </p>

<pre><code> library(glmnet)
 set.seed(1)
make_dummies_out_of_factors&lt;- function(your_df, names_of_factor_variables) {
  indices&lt;- which(names(your_df) %in% names_of_factor_variables) #Finds columns corresponding to factor variables
  model_matrices_list&lt;- lapply(indices, function(x) {
    model.matrix(~your_df[,x] - 1, your_df)
  })
  #create a model matrix for each factor variable, and stores each one as a list
  model_matrices_together&lt;- do.call(cbind, model_matrices_list)
  #Column bind all model matrices which are stored as lists
  final&lt;- cbind(your_df, model_matrices_together)
  #Column bind all the model matrices to the original data
  final&lt;- final[,-indices]
  #Get rid of the original factor variables

  names(final)&lt;- gsub(""your_df.*\\]"", ""dummy_"", names(final))
  #Give appropriate names to the dummies

  return(final)
}
test_df&lt;- data.frame(numeric1 = rnorm(1000), numeric2 = rnorm(1000), 
                     state = rep(letters[1:4], 250), year = rep(c(""yr1"", ""yr2""), 500)) #This data frame has 2 factor variables
test_df&lt;- make_dummies_out_of_factors(test_df, names_of_factor_variables = c(""state"", ""year""))

linear_alldum&lt;- lm(test_df$numeric2 ~ test_df$numeric1 + test_df$dummy_yr1 + test_df$dummy_yr2 + test_df$dummy_a + 
                     test_df$dummy_b + test_df$dummy_c + test_df$dummy_d)


X_test&lt;- as.matrix(test_df[,-1]) #Remove dependent variable out of X matrix
y_test&lt;- test_df[,1] #This is the dependent variable

ridge_alldum&lt;- glmnet(x = X_test, y = y_test, lambda = seq(200, 0, by = -1), alpha = 0)


comparison = data.frame(as.matrix(coef(ridge_alldum))[,201], coefficients(linear_alldum))
names(comparison)[1]&lt;- ""coefficients_ridge_l0""
names(comparison)[2]&lt;- ""coefficients_linear_reg""
#Note that coefficients aren't identical, and that ridge regression doesn't drop coefficients. 

prediction_linear&lt;- predict(linear_alldum)
prediction_ridge&lt;- predict(ridge_alldum, newx = X_test, s = 0)
predictions&lt;- data.frame(prediction_linear, prediction_ridge = prediction_ridge)
names(predictions)[2]&lt;- ""prediction_ridge""

#Note that the predictions using linear regression and ridge regression aren't the same. 

sapply(predictions, mean) #Means of predictions using linear and ridge.
sapply(predictions, sd) #SDs of predictions using linear and ridge. 
</code></pre>
"
"0.0895861718290583","0.0873037869711973","230581","<p>My problem is the following, my data has a lot of branch off points and the tree grows very rapidly. The end result is not readable, the end nodes are overlapped and even conversion to rules is more or less useless. 
I am using the rpart package. </p>

<pre><code>#Scoring model
d = sort(sample(nrow(Memmbers),nrow(Memmbers)* .6))
#select training sample
train&lt;-Memmbers[d, ]
test&lt;-Memmbers[-d, ]


s&lt;-glm(verifikation ~ . - userId,data = Memmbers,family = binomial())
summary(s)

library(ROCR)

#score test data set 
test$score &lt;- predict(s,type='response',test)
pred&lt;-prediction(test$score,test$verifikation)
perf&lt;- performance(pred,""tpr"",""fpr"")
plot(perf)

max(attr(perf,'y.values')[[1]]-attr(perf,'x.values')[[1]])

#get results of terms in regression 
g&lt;-predict(s,type='terms',test)
#function to pick top 3 reasons
#works by sorting coefficient terms in equation
# and selecting top 3 in sort for each loan scored 
ftopk&lt;- function(x,top=3){
  res=names(x)[order(x, decreasing = TRUE)][1:top]
  paste(res,collapse="";"",sep="""")
}
# Application of the function using the top 3 rows
topk=apply(g,1,ftopk,top=3)
#add reason list to scored tets sample
test&lt;-cbind(test, topk)

library(rpart)
library(rattle)

fit1 &lt;- rpart(verifikation ~ . - userId, data = train)
fancyRpartPlot(fit1);
test$t&lt;-predict(fit1,type='class',test)

################## PLot tree with priors 
#score test data 
test$score1 &lt;- predict(fit2,type = 'prob',test)
pred5&lt;-prediction(test$score1[,2],test$verifikation)
perf5&lt;- performance(pred5,""tpr"",""fpr"")

#90-10 priors with smaller complexity parameter to allow more complex trees
fit2 &lt;- rpart(verifikation ~ . - userId , data = train,method = ""class"",parms = list(prior=c(.9,.1)),cp=.0002)
plot(fit2);text(fit2,pos=2,cex=0.1,col=""blue"");

#compare complexity
printcp(fit1)
printcp(fit2)
plotcp(fit2)

#convert trees to rules 
amess&lt;-asRules(fit2)
t.b&lt;-rpart.rules.table(fit2)
library(rattle)
library(rpart.plot)
library(RColorBrewer)

fancyRpartPlot(fit2)
</code></pre>

<p>And here is the output of fancyRpartPlot(fit2) </p>

<p><a href=""http://i.stack.imgur.com/otsP3.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/otsP3.png"" alt=""enter image description here""></a></p>

<p>My goal is to extract some useful rules from the entire process to implement in a score card. </p>
"
"0.0566592699670073","0.0552157630374233","230851","<p>When we specify the â€œfamily=â€ argument inside glm() in R, how is the distribution being used to regress between the dependent and independent.?</p>

<p>In simple linear regression( lm() in R ) ,we simply calculate the mean of Y for each X and that becomes the predicted value. How different is this when we mention a family? </p>

<p>I do know that each distribution has few paramters that describes it ( mean, shape, scale etc). So how are they used to get predictions?</p>

<p>Bascially I would like to learn what does it mean to fit a distribution to a data.</p>

<p>Edit : to clarify the question</p>

<p>Lets say I trained my model on a dataset of 1lac obs with 2 independent variables and the coefficients are 1,2. i.e. beta1 = 1 and beta2 = 2</p>

<pre><code>          Y    x1   x2
1st obs.  2    .5    1
2nd obs.  1    .25  .25
</code></pre>

<p>So if I choose Poisson distribution, then</p>

<pre><code>1st obs. mean(mu) = exp(.5*1+1*2) = 12.18
</code></pre>

<p>similarily we get a mean for the second obs and so on. 
Now how is this mean related to what you gonna predict? I am not able to connect this. </p>

<p>Another major concern is also how the shape/scale i.e.(sigma, nu, tau for gamlss) being modeled. However they are secondary and for now wanna focus on glm My questions may be stupid but even any resources/links will be helpful and I shall sincerely read them </p>
"
"0.148626157759368","0.178919521355137","231066","<p>*EDIT: I ran test again with data set provided and realized that the cause of problem is definitely rank deficiency, because estimated values of parameters in nonlinear regression showed non existing p values and there was no way to create confidence intervals with this data. </p>

<h2>Thank you all for reading and help! This question is closed.</h2>

<p>I researched seed germination. I took 75 seed replicates and put them in  different ecological parameters (like temperature) and took data about sprouts in different time intervals. </p>

<p>Reading statistical science papers about this topic, I found that I should analyze my data in a time-to-event model (dose response curve), where I can use log-logistic regression or nonlinear regression (Ritz et al., 2013 -<a href=""http://dx.doi.org/10.1016/j.eja.2012.10.003"" rel=""nofollow"">http://dx.doi.org/10.1016/j.eja.2012.10.003</a>). </p>

<p>Two models (nonlinear and log-logistic) lead to quantitatively very similar fitted germination curves, i.e., similar parameter estimates, but qualitatively different statements about the precision of estimates. Nonlinear regression model yields an overly precise estimate of the proportion of seeds that germinated during the experiment, so the precision reported by the nonlinear regression is too high.</p>

<p>Similarly, the 95% confidence intervals of the fitted curves also demonstrate the dramatic difference in precision of the two models: Accurate prediction of germination percentages is not warranted by the data unless very low percentages are of interest.</p>

<p>Because of that I choose log-logistic regression as a model. First few data sets; treatments analyzed in R using analysis of Dose-Response Curves (drc package) went smooth, and I was able to plot and get final graph. Such data, which was successfully analyzed, contained treatments where max seeds germination was for example 50% of total seed number.</p>

<p>Example:</p>

<p><a href=""http://i.stack.imgur.com/yUqOu.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/yUqOu.jpg"" alt=""Example of successful data analysis""></a></p>

<p>The problems arose when I entered the log-logistic model with treatment where all the seeds germinated in a short amount of time (meaning the treatment for this set of seeds is most adequate for their successful sprouting). For example, 100% of seeds germinated in only 5 days, so there are only two or three time intervals and a large number of sprouted seeds. The R program here reported  convergence error:</p>

<pre><code>Error in optim(startVec, opfct, hessian = TRUE, method = optMethod, control = list(maxit = maxIt,  : 
non-finite value supplied by optim
Error in drmOpt(opfct, opdfct1, startVecSc, optMethod, constrained, warnVal,  : 
Convergence failed 
</code></pre>

<p>Since I'm still a student in biology I have a very basic knowledge in statistics, so I tried to solve the problem with literature. </p>

<p>At first I thought that convergence failed because of perfect or complete separation, but through longer research it seems that the problem lies in rank deficiency. </p>

<p>When I analyzed the same data with nonlinear regression I've managed to fit curve and plot a graph without a problem.  </p>

<p>So, is there a way to make log-logistic model work even though I have obviously small data in cases of 100% germination? Should I switch to nonlinear regression  even though the reported precision would be too high. </p>
"
"0.0886857854412185","0.098772959664959","231632","<p>After answering to question <a href=""http://stats.stackexchange.com/q/231059/58675"">Compare the statistical significance of the difference between two polynomial regressions in R</a>, I realized that I have always assumed that <code>ggplot2</code> plots <a href=""https://en.wikipedia.org/wiki/Confidence_and_prediction_bands"" rel=""nofollow"">simultaneous confidence bands</a>, not pointwise confidence bands, without actually knowing that for sure. I asked on SO: <a href=""http://stackoverflow.com/q/39110516/1711271"">http://stackoverflow.com/q/39110516/1711271</a>. I got an interesting answer, which I tried to apply. Results however can be weird:</p>

<pre><code>library(dplyr)
# sample datasets
setosa &lt;- iris %&gt;% filter(Species == ""setosa"") %&gt;% select(Sepal.Length, Sepal.Width, Species)
virginica &lt;- iris %&gt;% filter(Species == ""virginica"") %&gt;% select(Sepal.Length, Sepal.Width, Species)

# compute simultaneous confidence bands
setosa &lt;- setosa %&gt;% arrange(Sepal.Length)
virginica &lt;- virginica %&gt;% arrange(Sepal.Length)

# 1. compute linear models
Model &lt;- as.formula(Sepal.Width ~ poly(Sepal.Length,2))
fit1  &lt;- lm(Model, data = setosa)
fit2  &lt;- lm(Model, data = virginica)
# 2. compute design matrices
X1  &lt;- model.matrix(fit1)
X2  &lt;- model.matrix(fit2)
# 3. general linear hypotheses
cht1 &lt;- multcomp::glht(fit1, linfct = X1) 
cht2 &lt;- multcomp::glht(fit2, linfct = X2) 
# 4. simultaneous confidence bands (finally!)
cc1 &lt;- confint(cht1); cc1 &lt;- as.data.frame(cc1$confint)
cc2 &lt;- confint(cht2); cc2 &lt;- as.data.frame(cc2$confint)
setosa$LowerBound &lt;- cc1$lwr
setosa$UpperBound &lt;- cc1$upr
virginica$LowerBound &lt;- cc2$lwr
virginica$UpperBound &lt;- cc1$upr

# combine datasets
mydata &lt;- rbind(setosa, virginica)

# plot both simultaneous confidence bands and pointwise confidence
# bands, to show the difference
library(ggplot2)
# prepare a plot using dataframe mydata, mapping sepal Length to x,
# sepal width to y, and grouping the data by species
ggplot(data = mydata, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) + 
# add data points
geom_point() +
# add quadratic regression with orthogonal polynomials and 95% pointwise
# confidence intervals
geom_smooth(method =""lm"", formula = y ~ poly(x,2)) +
# # add 95% simultaneous confidence bands
geom_ribbon(aes(ymin = LowerBound, ymax = UpperBound),alpha = 0.5, fill = ""grey70"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/giSOG.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/giSOG.png"" alt=""enter image description here""></a></p>

<p>Questions:</p>

<ol>
<li>How would you plot the simultaneous confidence bands? Using ribbons with some transparency sort of does the job, but I'd rather not have the colored contours around the ribbons.</li>
<li>Why both the upper and lower boundary of the <code>setosa</code> ribbon are so smooth, while the upper bound of the <code>virginca</code> ribbon is so jagged? I would expect simultaneous confidence bands to be ""hyperbolic"" bands around the regression curve, thus very smooth. Am I computing the right thing here?</li>
</ol>

<p>PS just for the sake for clarity, I'm not interested in <strong>prediction</strong> bands. Here the focus is on <strong>simultaneous confidence bands</strong> and <strong>pointwise confidence bands</strong>.</p>
"
"0.0895861718290583","0.0873037869711973","233178","<p>I have a database with 1200 observations and 14 variables and I'am trying to do a classification tree for my dependent nominal variable who hase 4 modality</p>

<pre><code>    &gt; table(testarbre2$Q99)

  Autres       Nahdha Ne pas voter Nidaa Tounes 
     248          351          303          298 
</code></pre>

<p>at firt i tried to do a multinom logistic regression but i got the mojority of my predictor variables non significant. it seems that Even with 1200 people I was trying to fit a model for which I don't have sufficient data. 
so i tried to do a classification tree using the package rpart from R 
but the problem is that the error is so high about 65% and more, and the missclassification is about 70% 
this is the code R that i used </p>

<pre><code>   #preparation of the data
   set.seed(26)
   train=sample(1:nrow(testarbre2),nrow(testarbre2)*7/10)
   test=-train
   training_data=testarbre2[train,]
   testing_data=testarbre2[test,]
   testing_vote=vote[test]

   #fitting the model
   library(rpart)
   library(rpart.plot)
   Tree &lt;- rpart(Q99~.,data=training_data)
   rpart.plot(Tree)
   printcp(Tree)
   plotcp(Tree)

    #Construction of the complete tree
  Tree &lt;-rpart(Q99~.,data=training_data,control=rpart.control(minsplit=50,cp=0))

     #Prune the tree
    treeOptimal &lt;- prune(Tree,cp=Tree$cptable[which.min(Tree$cptable[,4]),1])
    rpart.plot(treeOptimal)

   #Prediction
   a=predict(ptitanicOptimal,testing_data2,type = ""class"")
   mc=table(a,testing_vote2)
</code></pre>

<p>I don't know if i missed a step or i used a wrong approach in the construction of my classification tree or the database is causing the problem</p>

<p>Please someone help me to understand what's wrong with my model</p>
"
"0.0981367343026181","0.0956365069595007","233366","<p>I am trying to use <code>lme4::glmer()</code> to fit a binomial GLMM with dependent variable that is not binary, but a continuous variable between zero and one. One can think of this variable as a probability; in fact it <em>is</em> probability as reported by human subjects (in an experiment that I help analyzing). The <code>glmer()</code> yields a model that is clearly off, and very far from the one I get with <code>glm()</code>, so something goes wrong. Why? What can I do? </p>

<hr>

<p><strong>More details</strong></p>

<p>Apparently it is possible to use logistic regression not only for binary DV but also for continuous DV between zero and one. Indeed, when I run </p>

<pre><code>glm(reportedProbability ~ a + b + c, myData, family=""binomial"")
</code></pre>

<p>I get a warning message</p>

<pre class=""lang-none prettyprint-override""><code>Warning message:
In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>but a very reasonable fit (all factors are categorical, so I can easily check whether model predictions are close to the across-subjects-means, and they are). </p>

<p>However, what I actually want to use is</p>

<pre><code>glmer(reportedProbability ~ a + b + c + (1 | subject), myData, family=""binomial"")
</code></pre>

<p>It gives me the identical warning, returns a model, but this model is clearly very much off; the estimates of the fixed effects are very far from the <code>glm()</code> ones and from the across-subject-means. (And I need to include <code>glmerControl(optimizer=""bobyqa"")</code> into the <code>glmer</code> call, otherwise it does not converge at all.)</p>
"
"0.0400641540107502","0.0390434404721515","233490","<p>This is the exact error:  </p>

<blockquote>
  <p>Error in randomForest.default(m, y, ...) : 
    sampsize has too many elements.</p>
</blockquote>

<p>Here is part of my code for classification (not regression) using compiled 3rd party data for prediction:</p>

<pre><code>library(randomForest)
library(ROCR)

table(train_hc$RESPONSE)
     0      1 
243697   6303

table(test_hc$RESPONSE)
     0      1 
243566   6434 
</code></pre>

<p>Train &amp; test were both created from the same randomly sorted file using different records.</p>

<pre><code>rfm_hc &lt;- randomForest(RESPONSE ~ ., data=train_hc[!names(train_hc)%in%exclude_cols], 
                       nodesize=1, strata=train_hc$response, sampsize=c(6000,6000), 
                       ntree=501, mtry=5, importance=TRUE, type=""prob"", 
                       keep.forest=TRUE, test=test_hc, cutoff=c(0.7,0.3))

Error in randomForest.default(m, y, ...) : 
  sampsize has too many elements.
</code></pre>

<p>From what I found so far it was suggested that the issue is the dependent variable levels but what I included above shows there is only two levels and response is a factor. I also was using 500k each for train &amp; test then changed this to 250k each. This didn't help.</p>

<p>Any ideas?</p>
"
"0.0566592699670073","0.0552157630374233","233802","<p>I've been given a data set containing 155 training examples and 108 features.I removed the features with more than 79 NA values and brought then them down to 99. I trained using the first 140 examples and used the rest for testing the prediction.The target variable is a discrete value i.e 0,1,2,3,4 though not restricted to those. I tried using multinomial logistic regression and random forest and got poor accuracy. Is there an algorithm that performs well on small data sets? Decision trees and regression don't seem to be working well for this. I am using R for the data analysis</p>
"
"0.0462621002059225","0.0676252226000574","234084","<p>I am new to multinomial logit regression, though I have done work with simple logisitc regression. I am attempting to build a model looking like the following:</p>

<pre><code>log(Pi/1-Pi) ~ X1 + X2 + X3
</code></pre>

<p>Where my dependent variable I am attempting to model for is in 4 levels: -2, 0, 3, or 7. I am running the model using the <code>nnet</code> package in <code>R</code>. While my model works, it doesn't show any prediction values of -2 or 3. I would expect this for the -2 because very little of the data resulted in -2 but a fair amount did show a 3 as the result. The model yielded a misclassification error of 34%. </p>

<p><strong>Would the reason for a binary response when I hoped for a 4-way response be something within my R code or is there an understanding of multinomial regression I am not understanding?</strong> I know this question is very general as I would like to understand not only the math behind what is happening but also I am working in R properly. Below is my R code.</p>

<pre><code>Drive$Points.2 &lt;- factor(Drive$Points)

library(nnet)
model.100 &lt;- multinom(Points.2 ~ X1 + X2 + X3, data = Drive)
summary(model.100)

cm &lt;- table(predict(model.100, Drive), Drive$Points.2)
cm
</code></pre>
"
"0.0400641540107502","0.0390434404721515","234446","<p>I have a time series $Y_t$ that is stationary, and several explanatory variables $X$ .. $Z$ (stationary as well). </p>

<p>Is there an R package (or Python one) that can automatically fit all the possible predictive regressions of the form</p>

<p>$Y_t$ = a + b * L(X) + c * L(Y) + d * L(Z) + epsilon</p>

<p>where indicates the vector of lagged X valuesm ie L(X) = ($X_{t-1}$, $X_{t-2}$, ..). The package should tell me automatically (for a given max lag of course) which specification has the best in-sample prediction accuracy (by eventually dropping some of the predictive variables)?</p>

<p>Thanks!</p>
"
"NaN","NaN","234462","<p>I am a complete newbie in this area and I am probably doing something wrong. I am trying to use <code>randomForest</code> in <code>R</code> for a regression and I am using the <code>iris</code> dataset. I want predict the <code>Sepal.Length</code> from the <code>Petal.Width</code> and the <code>Sepal.Length</code>. I want to test it with cross validation and I got confused. Here is my example. </p>

<pre><code>data &lt;- iris
k = 5
n=floor(nrow(iris)/k)
error=rep(NA,k)

for (i in 1:k){
  s1=((i-1)*n+1)
  s2=(n*i)
  sub=s1:s2
  train=iris[-sub,]
  test=iris[sub,]
  model=randomForest(Sepal.Length~Sepal.Width+Petal.Length,data=train,mtry=1,ntree=501,importance=TRUE)
  prediction=predict(model,newdata=test[,-1])
  error[i]= roc.area(test[,1],prediction)
}
</code></pre>

<p>Questions</p>

<ol>
<li>Is the ROC a good choice for a regression model?</li>
<li>How can I get the accuracies for every fold and for the entire model?</li>
</ol>
"
"0.105999788000636","0.103299233817667","234537","<p>I've split my data set into a training and test set. I've performed a principal component analysis on the training set and have used the first 3 principal components to generate a logistic regression model for my response.</p>

<p>I now want to use this model to make predictions for my test data set and check if this is true. </p>

<p>I've been trying to use the predict function but obviously the model uses the principal components of the training set as the predictors whereas my test set just has all the original predictors so obviously they're not compatible.</p>

<p><strong>How do I go about 'projecting' my test data onto the principal components I've already generated so I can use my model to make predictions?</strong></p>

<p>Ideally I'd like to do this without using any external packages (it's for university). I am working in R.</p>
"
"0.0566592699670073","0.0552157630374233","235422","<p>I am migrating from Weka+Pentaho forecasting and I am trying to get a regression model working in R.</p>

<p>As for my data, it is a time series of network utilization (second column).  How to make sure that I make use of timestamp?</p>

<p>I know that discretization becomes inconsistent at one point but this should not be related to the problem. In Weka I realigned discretization to appear linear. Need to fix this in R if this will cause issues.</p>

<p>Here is two samples from a set of 22k+ records sitting in a list of two columns:</p>

<p><code>2015-12-21 04:11:56          87
 2015-12-21 04:16:56          82
 2015-12-21 04:21:56          76
 2015-12-21 04:26:56          88
 2015-12-21 04:31:56          83</code></p>

<p><code>21999 2016-03-03 23:03:16          59
22000 2016-03-03 23:08:16          51
22001 2016-03-03 23:13:16          58
22002 2016-03-03 23:18:16          42
22003 2016-03-03 23:23:16          56
22004 2016-03-03 23:28:16          53
22005 2016-03-03 23:33:16          61</code>
I suspect I may confuse dimensions here... </p>

<p>I succeeded with SVM and KNN models in Weka, So far, neither is working in R. Below is my attempt to predict with KKNN library which fails.</p>

<p><code>library(kknn)
 fit = train.kknn(utilization~., d)
 predict(fit)
</code></p>

<p>The snippet above fails with the following message:
<code>Error: C stack usage  7969804 is too close to the limit</code></p>

<p>I need some guidelines on R pipeline to get at least a rough model and produce a prediction of N points, and calculate prediction performance metrics.</p>
"
