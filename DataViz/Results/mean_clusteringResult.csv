"V1","V2","V3","V4"
"0.155286242846124","0.193022859120546","  3271","<p>I have seen a few queries on clustering in time series and specifically on clustering, but I don't think they answer my question. </p>

<p><strong>Background:</strong> I want to cluster genes in a time course experiment in yeast. There are four time points say: <em>t1</em>  <em>t2</em>  <em>t3</em>  and  <em>t4</em> and total number of genes <em>G</em>. I have the data in form a matrix <em>M</em> in which the columns represent the treatments (or time points)  <em>t1</em>  <em>t2</em>  <em>t3</em>  and  <em>t4</em>  and the rows represent the genes. Therefore, <em>M</em> is a Gx4 matrix. </p>

<p><strong>Problem:</strong> I want to cluster the genes which behave the same across all time points <em>t1</em>  <em>t2</em>  <em>t3</em>  and  <em>t4</em>  as well as within a particular time point <em>ti</em> , where i is in {1, 2, 3, 4} (In case we cannot do both the clusterings together, the clustering within a time point is more important than clustering across time points). In addition to this, I also want to draw a heatmap.</p>

<p><strong>My Solution:</strong> 
I use the R code below to obtain a heatmap as well as the clusters using <code>hclust</code> function in R (performs hierarchical clustering with euclidean distance)</p>

<pre><code>    row.scaled.expr &lt;- (expr.diff - rowMeans(expr.diff)) / rowSds(expr.diff)

    breaks.expr &lt;- c(quantile(row.scaled.expr[row.scaled.expr &lt; 0],
                               seq(0,1,length=10)[-9]), 0,
                               quantile(row.scaled.expr[row.scaled.expr &gt; 0],
                               seq(0,1,length=10))[-1] )


    blue.red.expr &lt;- maPalette(low = ""blue"", high = ""red"", mid = ""white"",
                     k=length(breaks.expr) - 1)

    pdf(""images/clust.pdf"",
         height=30,width=20,pointsize=20)
    ht1 &lt;- heatmap.2(row.scaled.expr, col = blue.red.expr, Colv = FALSE, key = FALSE, 
      dendrogram = ""row"", scale = ""none"", trace = ""none"",
      cex=1.5, cexRow=1, cexCol=2,
      density.info = ""none"", breaks = breaks.expr, 
      labCol = colnames(row.scaled.expr),
      labRow="""",
      lmat=rbind( c(0, 3), c(2,1), c(0,4) ), lhei=c(0.25, 4, 0.25 ),
      main=expression(""Heat Map""),
      ylab=""Genes in the Microarray"",
      xlab=""Treatments""
      )
    dev.off()
</code></pre>

<p>I recently discovered <code>hopach</code> package in <em>Bioconductor</em> which can be used to estimate the number of clusters. Previously, I was randomly assigning the number of bins for the heatmap and cutting the tree at an appropriate height to get a pre-specified number of clusters. </p>

<p><strong>Possible Problems in my solution:</strong></p>

<ol>
<li>I may be not clustering the genes within a particular treatment and clustering genes only across treatments or vice versa.</li>
<li>There may be better ways of obtaining a heatmap for the pattern I want to see (similar genes within a treatment and across treatments).</li>
<li>There may be better visualization methods which I am not aware of.</li>
</ol>

<p><strong>Note:</strong></p>

<ol>
<li><p><em>csgillespie</em> (moderator) has a more general document on his website in which he discusses all the aspects of time course analysis (including heatmaps and clustering). I would appreciate if you can point me to an articles which describe heatmaps and clustering in detail.</p></li>
<li><p>I have tried the <code>pvclust</code> package, but it complains that <em>M</em> is singular and then it crashes.</p></li>
</ol>
"
"0.0515026202624605","0.064018439966448","  5366","<p>I need to cluster units into $k$ clusters to minimize within-group sum of squares (WSS), but I need to ensure that the clusters each contain at least $m$ units.  Any idea if any of R's clustering functions allow for clustering into $k$ clusters subject to a minimum cluster size constraint?  kmeans() does not seem to offer a size constraint option.</p>
"
"0.170814867130737","0.2123251450326","  7175","<p>I'm experimenting with classifying data into groups. I'm quite new to this topic, and trying to understand the output of some of the analysis.</p>

<p>Using examples from <a href=""http://www.statmethods.net/advstats/cluster.html"">Quick-R</a>, several <code>R</code> packages are suggested. I have tried using two of these packages (<code>fpc</code> using the <code>kmeans</code> function,  and <code>mclust</code>). One aspect of this analysis that I do not understand is the comparison of the results.</p>

<pre><code># comparing 2 cluster solutions
library(fpc)
cluster.stats(d, fit1$cluster, fit2$cluster)
</code></pre>

<p>I've read through the relevant parts of the <code>fpc</code> <a href=""http://cran.r-project.org/web/packages/fpc/fpc.pdf"">manual</a> and am still not clear on what I should be aiming for. For example, this is the output of comparing two different clustering approaches:</p>

<pre><code>$n
[1] 521

$cluster.number
[1] 4

$cluster.size
[1] 250 119  78  74

$diameter
[1]  5.278162  9.773658 16.460074  7.328020

$average.distance
[1] 1.632656 2.106422 3.461598 2.622574

$median.distance
[1] 1.562625 1.788113 2.763217 2.463826

$separation
[1] 0.2797048 0.3754188 0.2797048 0.3557264

$average.toother
[1] 3.442575 3.929158 4.068230 4.425910

$separation.matrix
          [,1]      [,2]      [,3]      [,4]
[1,] 0.0000000 0.3754188 0.2797048 0.3557264
[2,] 0.3754188 0.0000000 0.6299734 2.9020383
[3,] 0.2797048 0.6299734 0.0000000 0.6803704
[4,] 0.3557264 2.9020383 0.6803704 0.0000000

$average.between
[1] 3.865142

$average.within
[1] 1.894740

$n.between
[1] 91610

$n.within
[1] 43850

$within.cluster.ss
[1] 1785.935

$clus.avg.silwidths
         1          2          3          4 
0.42072895 0.31672350 0.01810699 0.23728253 

$avg.silwidth
[1] 0.3106403

$g2
NULL

$g3
NULL

$pearsongamma
[1] 0.4869491

$dunn
[1] 0.01699292

$entropy
[1] 1.251134

$wb.ratio
[1] 0.4902123

$ch
[1] 178.9074

$corrected.rand
[1] 0.2046704

$vi
[1] 1.56189
</code></pre>

<hr>

<p>My primary question here is to better understand how to interpret the results of this cluster comparison.</p>

<hr>

<p>Previously, I had asked more about the effect of scaling data, and calculating a distance matrix. However that was answered clearly by mariana soffer, and I'm just reorganizing my question to emphasize that I am interested in the intrepretation of my output which is a comparison of two different clustering algorithms.</p>

<p><em>Previous part of question</em>: 
If I am doing any type of clustering, should I always scale data? For example, I am using the function <code>dist()</code> on my scaled dataset as input to the <code>cluster.stats()</code> function, however I don't fully understand what is going on. I read about <code>dist()</code> <a href=""http://stat.ethz.ch/R-manual/R-patched/library/stats/html/dist.html"">here</a> and it states that:</p>

<blockquote>
  <p>this function computes and returns the distance matrix computed by using the specified distance measure to compute the distances between the rows of a data matrix.</p>
</blockquote>
"
"0.154507860787381","0.170715839910528","  7250","<p>I'm having difficulty understanding one or two aspects of the cluster package. I'm following the example from <a href=""http://www.statmethods.net/advstats/cluster.html"">Quick-R</a> closely, but don't understand one or two aspects of the analysis. I've included the code that I am using for this particular example.</p>

<pre><code>## Libraries
library(stats)
library(fpc) 

## Data
mydata = structure(list(a = c(461.4210925, 1549.524107, 936.42856, 0, 
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 131.4349206, 0, 762.6110846, 
3837.850406), b = c(19578.64174, 2233.308842, 4714.514274, 0, 
2760.510002, 1225.392118, 3706.428246, 2693.353714, 2674.126613, 
592.7384164, 1820.976961, 1318.654162, 1075.854792, 1211.248996, 
1851.363623, 3245.540062, 1711.817955, 2127.285272, 2186.671242
), c = c(1101.899095, 3.166506463, 0, 0, 0, 1130.890295, 0, 654.5054857, 
100.9491289, 0, 0, 0, 0, 0, 789.091922, 0, 0, 0, 0), d = c(33184.53871, 
11777.47447, 15961.71874, 10951.32402, 12840.14983, 13305.26424, 
12193.16597, 14873.26461, 11129.10269, 11642.93146, 9684.238583, 
15946.48195, 11025.08607, 11686.32213, 10608.82649, 8635.844964, 
10837.96219, 10772.53223, 14844.76478), e = c(13252.50358, 2509.5037, 
1418.364947, 2217.952853, 166.92007, 3585.488983, 1776.410835, 
3445.14319, 1675.722506, 1902.396338, 945.5376228, 1205.456943, 
2048.880329, 2883.497101, 1253.020175, 1507.442736, 0, 1686.548559, 
5662.704559), f = c(44.24828759, 0, 485.9617601, 372.108855, 
0, 509.4916263, 0, 0, 0, 212.9541122, 80.62920455, 0, 0, 30.16525587, 
135.0501384, 68.38023073, 0, 21.9317122, 65.09052886), g = c(415.8909649, 
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 637.2629479, 0, 0, 
0), h = c(583.2213618, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
0, 0, 0, 0, 0), i = c(68206.47387, 18072.97762, 23516.98828, 
13541.38572, 15767.5799, 19756.52726, 17676.00505, 21666.267, 
15579.90094, 14351.02033, 12531.38237, 18470.59306, 14149.82119, 
15811.23348, 14637.35235, 13588.64291, 12549.78014, 15370.90886, 
26597.08152)), .Names = c(""a"", ""b"", ""c"", ""d"", ""e"", ""f"", ""g"", 
""h"", ""i""), row.names = c(NA, -19L), class = ""data.frame"")
</code></pre>

<p>Then I standardize the variables:</p>

<pre><code># standardize variables
mydata &lt;- scale(mydata) 

## K-means Clustering 

# Determine number of clusters
wss &lt;- (nrow(mydata)-1)*sum(apply(mydata,2,var))
for (i in 2:15) wss[i] &lt;- sum(kmeans(mydata, centers=i)$withinss)
# Q1
plot(1:15, wss, type=""b"", xlab=""Number of Clusters"",  ylab=""Within groups sum of squares"") 

# K-Means Cluster Analysis
fit &lt;- kmeans(mydata, 3) # number of values in cluster solution

# get cluster means 
aggregate(mydata,by=list(fit$cluster),FUN=mean)

# append cluster assignment
mydata &lt;- data.frame(mydata, cluster = fit$cluster)

# Cluster Plot against 1st 2 principal components - vary parameters for most readable graph
clusplot(mydata, fit$cluster, color=TRUE, shade=TRUE, labels=0, lines=0) # Q2

# Centroid Plot against 1st 2 discriminant functions
plotcluster(mydata, fit$cluster)
</code></pre>

<p>My question is, how can the plot which shows the number of clusters (marked <code>Q1</code> in my code) be related to the actual values (cluster number and variable name) ? </p>

<p>Update: I now understand that the <code>clusplot()</code> function is a bivariate plot, with PCA1 and PCA2. However, I don't understand the link between the PCA components and the cluster groups. What is the relationship between the PCA values and the clustering groups? I've read elsewhere about the link between kmeans and PCA, but I still don't understand how they can be displayed on the same bivariate graph. </p>
"
"0.230588306217099","0.232029213433861","  7903","<p>QUESTION:</p>

<p>I have binary data on exam questions (correct/incorrect). Some individuals might have had prior access to a subset of questions and their correct answers. I donâ€™t know who, how many, or which. If there were no cheating, suppose I would model the probability of a correct response for item $i$ as $logit((p_i = 1 | z)) = \beta_i + z$, where $\beta_i$ represents question difficulty and $z$ is the individualâ€™s latent ability. This is a very simple item response model that can be estimated with functions like ltmâ€™s rasch() in R. In addition to the estimates $\hat{z}_j$ (where $j$ indexes individuals) of the latent variable, I have access to separate estimates $\hat{q}_j$ of the same latent variable which were derived from another dataset in which cheating was not possible. </p>

<p>The goal is to identify individuals who likely cheated and the items they cheated on. What are some approaches you might take? In addition to the raw data, $\hat{\beta}_i$, $\hat{z}_j$, and $\hat{q}_j$ are all available, although the first two will have some bias due to cheating. Ideally, the solution would come in the form of probabilistic clustering/classification, although this is not necessary. Practical ideas are highly welcomed as are formal approaches. </p>

<p>So far, I have compared the correlation of question scores for pairs of individuals with higher vs. lower $\hat{q}_j -\hat{z}_j $ scores (where $\hat{q}_j - \hat{z}_j $ is a rough index of the probability that they cheated). For example, I sorted individuals by $\hat{q}_j - \hat{z}_j $ and then plotted the correlation of successive pairs of individualsâ€™ question scores. I also tried plotting the mean correlation of scores for individuals whose $\hat{q}_j - \hat{z}_j $ values were greater than the $n^{th}$ quantile of $\hat{q}_j - \hat{z}_j $, as a function of $n$. No obvious patterns for either approach.</p>

<hr>

<p>UPDATE:</p>

<p>I ended up combining ideas from @SheldonCooper and the helpful <a href=""http://pricetheory.uchicago.edu/levitt/Papers/JacobLevitt2003.pdf"">Freakonomics paper</a> that @whuber pointed me toward. <em>Other ideas/comments/criticisms welcome.</em></p>

<p>Let $X_{ij}$ be person $j$â€™s binary score on question $i$. Estimate the item response model $$logit(Pr(X_{ij} = 1 | z_j) = \beta_i + z_j,$$ where $\beta_i$ is the itemâ€™s easiness parameter and $z_j$ is a latent ability variable. (A more complicated model can be substituted; Iâ€™m using a 2PL in my application). As I mentioned in my original post, I have estimates $\hat{q_j } $ of the ability variable from a separate dataset $\{y_{ij}\}$ (different items, same persons) on which cheating was not possible. Specifically, $\hat{q_j} $ are empirical Bayes estimates from the same item response model as above. </p>

<p>The probability of the observed score $x_{ij}$, conditional on item easiness and person ability, can be written $$p_{ij} = Pr(X_{ij} = x_{ij} | \hat{\beta_i }, \hat{q_j }) = P_{ij}(\hat{\beta_i }, \hat{q_j })^{x_{ij}} (1 - P_{ij}(\hat{\beta_i }, \hat{q_j }))^{1-x_{ij}},$$ where $P_{ij}(\hat{\beta_i }, \hat{q_j }) = ilogit(\hat{\beta_i} + \hat{q_j})$ is the predicted probability of a correct response, and $ilogit$ is the inverse logit. Then, conditional on item and person characteristics, the joint probability that person $j$ has the observations $x_j$ is $$p_j = \prod_i p_{ij},$$ and similarly, the joint probability that item $i$ has the observations $x_i$ is $$p_i = \prod_j p_{ij}.$$ Persons with the lowest $p_j$ values are those whose observed scores are conditionally least likely -- they are possibly cheaters. Items with the lowest $p_j$ values are those which are conditionally least likely -- they are the possible leaked/shared items. This approach relies on the assumptions that the models are correct and that person $j$â€™s scores are uncorrelated conditional on person and item characteristics. A violation of the second assumption isnâ€™t problematic though, as long as the degree of correlation does not vary across persons, and the model for $p_{ij}$ could easily be improved (e.g., by adding additional person or item characteristics).</p>

<p>An additional step I tried is to take r% of the least likely persons (i.e. persons with the lowest r% of sorted p_j values), compute the mean distance between their observed scores x_j (which should be correlated for persons with low r, who are possible cheaters), and plot it for r = 0.001, 0.002, ..., 1.000. The mean distance increases for r = 0.001 to r = 0.025, reaches a maximum, and then declines slowly to a minimum at r = 1. Not exactly what I was hoping for. </p>
"
"0.126155140059354","0.130677093372329","  8152","<p>I have a few questions regarding multiple imputation for nested data. 
Context: I have repeated measures (4 times) from a survey and these are clustered in workplaces (205 workplaces). There are about 180 items on this survey.</p>

<p>q1. Is it possible to take both the repeated measures and the workplace clustering into consideration or do i have to decide for one of the two?</p>

<p>q2. If i can only take into consideration one of the two clusterings (repeated measures vs workplace) which one would you recommend</p>

<p>q3. I have about 10000 observations and about 400 of them have missing values for the workplace. What would you recommend to do in this case? (also i should mention that the 205 workplaces are nested in 17 Organizations - For the moment i use general categories based on the organization: e.g. Organization1-Unclassified). Is there a meaningful way to actually impute these categories? </p>

<p>q4. Would you recommend to use all 180 items for imputation or the items that i intend to use in each of my models? </p>

<p>I use R for analysis and it would be greatly appreciated if you can recommend any packages for multiple imputation for clustered data.</p>

<p>Thanks in advance</p>
"
"0.0515026202624605","0.064018439966448","  8729","<p>I'm trying to gain a better understanding of kmeans clustering and am still unclear about colinearity and scaling of data. To explore colinearity, I made a plot of all five variables that I am considering shown in the figure below, along with a correlation calculation.
<img src=""http://i.stack.imgur.com/W5MZJ.jpg"" alt=""colinearity""></p>

<p>I started off with a larger number of parameters, and excluded any that had a correlation higher than 0.6 (an assumption I made). The five I choose to include are shown in this diagram.</p>

<p>Then, I scaled the date using the <code>R</code> function <code>scale(x)</code> before applying the <code>kmeans()</code> function. However, I'm not sure whether <code>center = TRUE</code> and <code>scale = TRUE</code> should also be included as I don't understand the differences that these arguments make. (The <code>scale()</code> description is given as <code>scale(x, center = TRUE, scale = TRUE)</code>).</p>

<p>Is the process that I describe an appropriate way of identifying clusters?</p>
"
"0.192705159543249","0.222425421019475"," 10017","<p>I am trying to understand standard error ""clustering"" and how to execute in R (it is trivial in Stata). In R I have been unsuccessful using either <code>plm</code> or writing my own function. I'll use the <code>diamonds</code> data from the <code>ggplot2</code> package.</p>

<p>I can do fixed effects with either dummy variables</p>

<pre><code>&gt; library(plyr)
&gt; library(ggplot2)
&gt; library(lmtest)
&gt; library(sandwich)
&gt; # with dummies to create fixed effects
&gt; fe.lsdv &lt;- lm(price ~ carat + factor(cut) + 0, data = diamonds)
&gt; ct.lsdv &lt;- coeftest(fe.lsdv, vcov. = vcovHC)
&gt; ct.lsdv

t test of coefficients:

                      Estimate Std. Error  t value  Pr(&gt;|t|)    
carat                 7871.082     24.892  316.207 &lt; 2.2e-16 ***
factor(cut)Fair      -3875.470     51.190  -75.707 &lt; 2.2e-16 ***
factor(cut)Good      -2755.138     26.570 -103.692 &lt; 2.2e-16 ***
factor(cut)Very Good -2365.334     20.548 -115.111 &lt; 2.2e-16 ***
factor(cut)Premium   -2436.393     21.172 -115.075 &lt; 2.2e-16 ***
factor(cut)Ideal     -2074.546     16.092 -128.920 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>or by de-meaning both left- and right-hand sides (no time invariant regressors here) and correcting degrees of freedom.</p>

<pre><code>&gt; # by demeaning with degrees of freedom correction
&gt; diamonds &lt;- ddply(diamonds, .(cut), transform, price.dm = price - mean(price), carat.dm = carat  .... [TRUNCATED] 
&gt; fe.dm &lt;- lm(price.dm ~ carat.dm + 0, data = diamonds)
&gt; ct.dm &lt;- coeftest(fe.dm, vcov. = vcovHC, df = nrow(diamonds) - 1 - 5)
&gt; ct.dm

t test of coefficients:

         Estimate Std. Error t value  Pr(&gt;|t|)    
carat.dm 7871.082     24.888  316.26 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>I can't replicate these results with <code>plm</code>, because I don't have a ""time"" index (i.e., this isn't really a panel, just clusters that could have a common bias in their error terms).</p>

<pre><code>&gt; plm.temp &lt;- plm(price ~ carat, data = diamonds, index = ""cut"")
duplicate couples (time-id)
Error in pdim.default(index[[1]], index[[2]]) : 
</code></pre>

<p>I also tried to code my own covariance matrix with clustered standard error using Stata's explanation of their <code>cluster</code> option (<a href=""http://www.stata.com/support/faqs/stat/cluster.html"">explained here</a>), which is to solve $$\hat V_{cluster} = (X&#39;X)^{-1} \left( \sum_{j=1}^{n_c} u_j&#39;u_j \right) (X&#39;X)^{-1}$$ where $u_j = \sum_{cluster~j} e_i * x_i$, $n_c$ si the number of clusters, $e_i$ is the residual for the $i^{th}$ observation and $x_i$ is the row vector of predictors, including the constant (this also appears as equation (7.22) in Wooldridge's <em>Cross Section and Panel Data</em>). But the following code gives very large covariance matrices. Are these very large values given the small number of clusters I have? Given that I can't get <code>plm</code> to do clusters on one factor, I'm not sure how to benchmark my code.</p>

<pre><code>&gt; # with cluster robust se
&gt; lm.temp &lt;- lm(price ~ carat + factor(cut) + 0, data = diamonds)
&gt; 
&gt; # using the model that Stata uses
&gt; stata.clustering &lt;- function(x, clu, res) {
+     x &lt;- as.matrix(x)
+     clu &lt;- as.vector(clu)
+     res &lt;- as.vector(res)
+     fac &lt;- unique(clu)
+     num.fac &lt;- length(fac)
+     num.reg &lt;- ncol(x)
+     u &lt;- matrix(NA, nrow = num.fac, ncol = num.reg)
+     meat &lt;- matrix(NA, nrow = num.reg, ncol = num.reg)
+     
+     # outer terms (X'X)^-1
+     outer &lt;- solve(t(x) %*% x)
+ 
+     # inner term sum_j u_j'u_j where u_j = sum_i e_i * x_i
+     for (i in seq(num.fac)) {
+         index.loop &lt;- clu == fac[i]
+         res.loop &lt;- res[index.loop]
+         x.loop &lt;- x[clu == fac[i], ]
+         u[i, ] &lt;- as.vector(colSums(res.loop * x.loop))
+     }
+     inner &lt;- t(u) %*% u
+ 
+     # 
+     V &lt;- outer %*% inner %*% outer
+     return(V)
+ }
&gt; x.temp &lt;- data.frame(const = 1, diamonds[, ""carat""])
&gt; summary(lm.temp)

Call:
lm(formula = price ~ carat + factor(cut) + 0, data = diamonds)

Residuals:
     Min       1Q   Median       3Q      Max 
-17540.7   -791.6    -37.6    522.1  12721.4 

Coefficients:
                     Estimate Std. Error t value Pr(&gt;|t|)    
carat                 7871.08      13.98   563.0   &lt;2e-16 ***
factor(cut)Fair      -3875.47      40.41   -95.9   &lt;2e-16 ***
factor(cut)Good      -2755.14      24.63  -111.9   &lt;2e-16 ***
factor(cut)Very Good -2365.33      17.78  -133.0   &lt;2e-16 ***
factor(cut)Premium   -2436.39      17.92  -136.0   &lt;2e-16 ***
factor(cut)Ideal     -2074.55      14.23  -145.8   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 1511 on 53934 degrees of freedom
Multiple R-squared: 0.9272, Adjusted R-squared: 0.9272 
F-statistic: 1.145e+05 on 6 and 53934 DF,  p-value: &lt; 2.2e-16 

&gt; stata.clustering(x = x.temp, clu = diamonds$cut, res = lm.temp$residuals)
                        const diamonds....carat..
const                11352.64           -14227.44
diamonds....carat.. -14227.44            17830.22
</code></pre>

<p>Can this be done in R? It is a fairly common technique in econometrics (there's a brief tutorial in <a href=""http://sekhon.berkeley.edu/causalinf/sp2010/section/week7.pdf"">this lecture</a>), but I can't figure it out in R. Thanks!</p>
"
"0.154507860787381","0.128036879932896"," 10411","<p>I have a few datasets of ""interactions"" between pairs of elements like so:</p>

<pre><code>element1 element2 1
element2 element3 1
element4 element5 1
...
element505535 element4 2
</code></pre>

<p>where the value in the 3rd column is the ""strength"" of interaction. Almost all of these strengths are ""1."" A strength of 1 means that this interaction was observed one time. A strength of 2 means 2x, etc. I have actually gone one step further and normalized all of my datasets by the total # of interactions observed in the dataset so that datasets interaction values can be compared.</p>

<p>There are 5-6 million interactions listed in each file and each dataset is obviously under-sampled since there are ~500k elements (making a square matrix of ~250 trillion positions).</p>

<p>I would like to cluster these datasets so that I can make statements about which types of elements tend to cluster with which other types elements. Obviously, robusticity of clustering will be a factorâ€”but this is partially ameliorated by the fact that I will make biological replicates of the data.</p>

<p>I have tried a few different ""naive"" clustering approaches just to see what I could do easily with the data. I fully realize that these are problematic ways of clustering, either because they are not robust or because they rely on the data being very undersampled, but here is what I've done:</p>

<ol>
<li><p>Clustering elements together as long as there is at least one interaction between each element in the cluster and at least one other element in the cluster. When I do this, all the elements end up in a single cluster. This was important to do because it tells me that there are no pairs of elements that are totally isolated from the rest of the group.</p></li>
<li><p>Finding ""superclusters""â€”that is, clusters where every member of the cluster interacts with every other member of the cluster (e.g. a triangle for a cluster of 3 and a box with an X in the middle for a cluster of 4, etc). This yields almost exclusively clusters with 2 and 3 elements after about 10% of the data has been analyzed (this is still running).</p></li>
</ol>

<p>I would love to be able to do some sort of hierarchical clustering using my ""interaction strength"" values as the distance measure between each pair of elements (unobserved interactions have a strength of 0). Does anyone know of a way to do HC on this sort of large, sparse dataâ€”or know of a clustering method that might be more appropriate? I've used R up until now.</p>
"
"0.11516335992622","0.143149583578467"," 12580","<p>I used the following example code from <strong>latticeExtra</strong> to understand two-way clustering in <em>R</em></p>

<pre><code>library(latticeExtra)
data(mtcars)
x  &lt;- t(as.matrix(scale(mtcars)))
dd.row &lt;- as.dendrogram(hclust(dist(x)))
row.ord &lt;- order.dendrogram(dd.row)

dd.col &lt;- as.dendrogram(hclust(dist(t(x))))
col.ord &lt;- order.dendrogram(dd.col)

library(lattice)

levelplot(x[row.ord, col.ord],
      aspect = ""fill"",
      scales = list(x = list(rot = 90)),
      colorkey = list(space = ""left""),
      legend =
      list(right =
           list(fun = dendrogramGrob,
                args =
                list(x = dd.col, ord = col.ord,
                     side = ""right"",
                     size = 10)),
           top =
           list(fun = dendrogramGrob,
                args =
                list(x = dd.row,
                     side = ""top"",
                     size = 10))))
</code></pre>

<p>and this is what I got</p>

<p><img src=""http://i.stack.imgur.com/zxWV1.png"" alt=""enter image description here""></p>

<p>Joining of both row and column entities make sense to me but I'm confused with different color shades of heatmap.</p>

<p><strong>Questions</strong></p>

<ul>
<li>Do the Joining of row variables also take into account the column variables and vice versa</li>
<li>What does mean the different colors in heatmap for different row variables clustering as well as for column variables clustering. Specifically focus on <strong>cyl</strong> and <strong>disp</strong> row variables.</li>
</ul>
"
"0.136263125082667","0.145180175548748"," 13857","<p>I have a huge matrix (individuals X features with row.names as individuals numbers) and the corresponding segment in another vector of 1D (row.names are the same as in my huge matrix and the vector represent the segments associated).
I.E. :</p>

<pre><code>row.names VAR1 VAR2 VAR3 VAR4 â€¦ VAR3000
    12     4    12    5   18      8
    58     6    13    19   3     10
</code></pre>

<p>for the huge matrix.
and:</p>

<pre><code>row.names  x
    12     4
    58     2
</code></pre>

<p>for the segment representation (where x represent the individual' segment).</p>

<p>I have no a priori model and I want to select a subset of variables (variable/feature selection) in order to predict the segment using a minimal subset of variables. I didn't use biclustering technique to detect my classes but a simple-way one. Which technique would you recommend to :</p>

<ol>
<li>select the most discriminative variables (lasso, elastic net) and why?</li>
<li>predict the segment from these variables.</li>
<li>predict multiple values in another similar matrix (same individuals, few predictors that have been selected). Is it possible in this case to use correlation matrix (or cov) to infer directly the values of predictors that are not known in the other matrix (not using the following method: predict the class, then fill the missing values with medoid values or cluster-mean values)?</li>
</ol>

<p>Thanks in advance.</p>
"
"0.186170871191717","0.181824391349504"," 14051","<p>Thanks for reading my question.</p>

<p>I have several thousand data points scattered on an (x,y) grid that I am trying to cluster.  The data points are not uniformly distributed across the grid, but are concentrated in certain areas.  I am most interested in identifying the centers of the clusters as representing starting points that minimize the average (Euclidean) distance from a point to the nearest cluster center.</p>

<p>Depending on the specific model and data set, there are between 3 and 7 clusters.  The number of clusters is known beforehand in each instance, and does not need to be determined by an algorithm.  In each situation, some of the centers of the clusters are known (0 to 4 known starting points), but the rest are unknown.  The goal is to identify the centers of the unknown clusters that minimize the average distance to a center across all the data points.</p>

<p>How can I run a clustering algorithm where I can specify a certain number of cluster centers, and solve for the others?  I am using R, and have looked primarily at package mclust.  My thought was that specifying priors for mean and scale to Mclust with very small scale variances for the known centers and very large scale variances (uninformative prior) for the unknowns would be a good approach, but I am having trouble coding it.  The available examples for specifying priors in the package documentation aren't terribly helpful (to me), and might be used for a completely different purpose than what I'm trying to do.</p>

<p>My attempt to code this in R looks something like:</p>

<pre><code># create data matrix of points
x &lt;- rnorm(100, 50, 25)
y &lt;- rnorm(100, 50, 25)
my.data &lt;- cbind(x,y)

# Two known centers, rest are unknown so provide mean of x,y as default starting point
#
# known centers are (25, 10) and (90, 65), assume midpoints of grid for others
x.prior.mean &lt;- c(25, 90, 50, 50, 50)
y.prior.mean &lt;- c(10, 65, 50, 50, 50)

# Provide small scale (variance) for known centers, large scale for unknown centers (uninformative prior)
x.prior.scale &lt;- c( 0.1, 0.1, 100, 100, 100)
y.prior.scale &lt;- c( 0.1, 0.1, 100, 100, 100)

# Create a cluster model with no prior specified
my.clust.noprior &lt;- Mclust(data=my.data, G=5)

# Now add a prior for mean
my.clust.prior &lt;-   Mclust(data=my.data, G=5, prior=priorControl(mean=cbind(x=x.prior.mean, y=y.prior.mean)))

# Compare what I think are the centers of the clusters (mean of parameters).
# The centers in the prior-specified case don't seem to reflect the known centers
my.clust.noprior$parameters$mean
my.clust.prior$parameters$mean

# Commented out, but attempting the following statement that adds scale parameter yields an error:
#    Error in chol.default(priorParams$scale) : non-square matrix in 'chol'
#
# my.clust &lt;-   Mclust(data=my.data, G=5, prior=priorControl(mean=cbind(x=x.prior.mean, y=y.prior.mean), scale=cbind(x.prior.scale, y.prior.scale)))
</code></pre>

<p>Is there a way to accomplish what I'm trying to do?  I am open to using other R packages besides mclust if there's one better suited for this problem.</p>

<p>Thank you</p>
"
"0.072835704072923","0.0905357460425185"," 15548","<p>I have a data which  contains several columns which I later reduced using a PCA algorithms to two different components. I then applied the k-means algorithms to the data.<br>
Now, how can I verify that my data  clustered  well into each group? Or how do I determine misclassification rate?</p>

<p>For instance, using R, if I check the cluster vector say k$cluster against the labels of the data I had previously before clustering  can I just draw a confusion matrix from that and assume that 1 in the clustered vector is equivalent to 1 in my labels?</p>

<pre><code>col3    col2     Col1   lables                                           
123     2.32      2.50    0           
124    2.81      3.10     1     
125    2.72      3.09     2     
126    2.92      3.03     3     
127    2.32      2.95     4     
</code></pre>

<p>Please note this is a hypothetical data; my data is way bigger than this.</p>
"
"0.136263125082667","0.120983479623957"," 16137","<p>Are there any tools in R that could be used to optimize the allocation of customers amongst possible offers, given constraints? Can anyone give hints/examples on their use? Hope my setup makes sense...</p>

<p>Here is the problem setup:</p>

<p><strong>There are the following:</strong></p>

<ul>
<li>$N$ customers ($N$ is large)</li>
<li>$F$ offers (the offers that can be made to a customer;  $F$ is relatively small)</li>
<li>$P_{nf}$ -- the probability of acceptance of offer $f$ by customer $n$</li>
<li>$D_{nf}$ -- the expected monetary value if customer $n$ accepts offer $f$</li>
<li>$C_f$ -- the cost of offering offer $f$ to any customer</li>
<li>$E_{nf}$ -- the expected profit of offering offer $f$ to customer $n$ ($P_{nf} D_{nf} - C_{f}$)</li>
</ul>

<p><strong>Constraints:</strong></p>

<ul>
<li>Each customer can be allocated to only 1 offer (not every customer need receive anything).</li>
<li>The total number of offers made (call it $T$) between a and b. </li>
<li>The total cost $TC&lt;c$.</li>
</ul>

<p>The percentage of $T$ comprised by each offer $f$ is $\geq d$. This means that sometimes an offer has to be made at least $d$ times. There is one of these rules for each offer.</p>

<p><strong>Goal:</strong></p>

<ul>
<li>Maximize profit.</li>
</ul>

<p>Anything in R?</p>

<p><strong>EDIT:</strong></p>

<ul>
<li><p>I wonder about using something along the lines of <a href=""http://cran.r-project.org/web/packages/Rglpk/index.html"" rel=""nofollow"">http://cran.r-project.org/web/packages/Rglpk/index.html</a>
which appears to support ""large"" problems and the types of constraints I have.
Of course, ""large"" in the context appears to be much less than the millions for N I have. </p></li>
<li><p>One thought I had was to caculate the expected profit for each customer and each offer. Then, run a clustering algorithm (row = customers and columns = expected profit for each promotion) like k-means with k large (e.g. 1,000). Then assign each of the customers into a cluster and use the cluster centroid as the value of the expected profit for the optimizer.</p></li>
</ul>

<p><strong>EDIT AGAIN</strong></p>

<p>For the sake of helping others, the conclusion I came to was to indeed cluster the customers and then use a standard linear solver (I got lpSolve in R to work well). </p>

<p>The other option is to use a non linear approximation. Robert Agnew helped me tremendously on this question - using his dual formulation. See this <a href=""http://www.r-bloggers.com/marketing-optimization-using-the-nonlinear-minimization-function-nlm/"" rel=""nofollow"">post</a>.  His R script is also linked and works great - changing from equality constraints for the offer quantity to inequality constraints requires use of nlminb(). </p>
"
"0.126155140059354","0.104541674697863"," 18969","<p>I want to use model-based clustering to classify 1,225 time series (24 periods each). I have decomposed these time series using the fast Fourier transform and selected the harmonics that explain at least a threshold percentage of time series variance for all time series in the sample. I want to do model-based clustering on the real and imaginary parts for each transform element of a give time series because it would potentially save me from having to account for temporal autocorrelation in model based clustering across periods of a time series. I know that each complex element of the fast Fourier transform is independent from other elements, but I do not know if the imaginary and real parts of the output for a given output element are independent. I would like to know because if they were, it would allow me to maintain the default assumption of the Mclust package in R for model-based clustering that the variables analyzed have a multivariate Gaussian distribution.</p>

<p>NOTE: The input is real-valued, and I have converted from a two-sided to a one-sided spectrum by removing redundant frequency elements and multiplying the positive frequencies (other than the mean component) by two per the advice I got from another StackOverflow answer here: <a href=""http://stackoverflow.com/questions/8264530/how-do-i-calculate-amplitude-and-phase-angle-of-fft-output-from-real-valued-in"">http://stackoverflow.com/questions/8264530/how-do-i-calculate-amplitude-and-phase-angle-of-fft-output-from-real-valued-in</a></p>
"
"0.126155140059354","0.0784062560233975"," 20673","<p>We ask a set of users to independently detect and annotate all the buildings on a map.
We do not have a priori knowledge about the location or event the existence of buildings on this map.</p>

<p>I would like to aggregate/cluster their annotations in a way that only 'consensual' annotations are taking into account. That means that:</p>

<ul>
<li>I do not know a priori the number of clusters /object on the map (unsupervised approach)</li>
<li>each point in a cluster should belong to a different user (democratic clustering constraint) i.e. a cluster is formed only if the candidate points, spatially enough close, are from several users. with for instance a threshold (if 60% of the users annotate the same area, we consider it as a building)</li>
</ul>

<p>Which (modified) clustering algorithm will be appropriated for such task? (I work with R)?</p>

<p>bonus  </p>

<ul>
<li>The density of the objects on the map is changing. one map could have a dense area of buildings + sparse subareas of buildings). how to choose dynamically the distance between 2 points to consider they are enough close?</li>
</ul>

<hr>

<p>Tried algorithm:</p>

<p>A density based algorithm (DBSCAN)
but:
- it does not take into account the democratic constraint in the formation of the cluster. - there is a problem to handle the change of spatial density on a map ( dense area  of building + sparse area of building on the same map)</p>

<p>Thanks!</p>
"
"0.0515026202624605","0.064018439966448"," 21421","<p>I have a set of about ten questions that I would like to use to create groupings from. The responses are all dichotomous (responses are in the form of 1 or 2 where 1 and 2 represent differences in preference discovered through qualitative research).  </p>

<p>The questions, were provided in the form: </p>

<pre><code>Which describes you best:
1. I prefer apples
2. I prefer pears
</code></pre>

<p>So far, I've looked at latent class analysis (challenging to interpret and not consistently reproducible), linear discriminant analysis, kmeans clustering (not consistently reproducible), and multiple correspondence analysis (MCA provided the most interpret-able results, but I'm unclear IF or how one could classify respondents using the results). </p>

<p>What would be the most reasonable clustering method to use?</p>

<p>If you want to play around with my data feel free to do so (n=799): </p>

<pre><code>dat &lt;- read.csv(""http://www.bertelsen.ca/media/stackoverflow/cluster.csv"")
</code></pre>
"
"0.0892051550175079","0.110883190643186"," 21685","<p>I am trying to get to grips with Clustering and Visualisation.</p>

<p>I have a set of data (a matrix) that I want to cluster (using R) and then visualise (HTML5 Canvas).  </p>

<p>So, I can use MDS to get the coordinates of a matrix, for example:</p>

<pre><code>cells &lt;- c(1, 1, 2, 1, 4, 3, 5, 4)
rnames &lt;- c(""A"", ""B"", ""C"", ""D"")
cnames &lt;- c(""X"", ""Y"")
x &lt;- matrix(cells, nrow=4, ncol=2, byrow=TRUE, dimnames=list(rnames, cnames))
d &lt;-dist(x)
m &lt;- cmdscale(d,eig=TRUE, k=3)
m &lt;- cmdscale(d,eig=TRUE, k=2)
print(m)
</code></pre>

<p>But, should I first cluster this data (using something like k-means) or should I cluster the output of MDS?  Or can I just cluster (and get the coordinates using some other method) and ignore MDS?</p>

<p>What is the relationship between MDS and K-means, if any?</p>

<p>I am unsure what is the best way to approach?</p>

<p>Any suggestions? </p>

<p>Thanks,</p>

<p>s</p>
"
"0.072835704072923","0.0905357460425185"," 21718","<p>Given a data matrix like this:</p>

<pre><code>        [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]
 [1,]  9.520 11.137 16.576 18.225 20.576 25.861     NA
 [2,]  9.005  9.491 11.106 16.530 18.184 20.495 25.773
 [3,]  9.437 11.050 20.393 25.711     NA     NA     NA
 [4,]  9.442 11.058 20.411 25.711     NA     NA     NA
 [5,]  9.431 11.045 20.421 25.707     NA     NA     NA
 [6,]  9.461 11.052 20.319 25.657     NA     NA     NA
 [7,]  9.245 10.819 20.253 25.628     NA     NA     NA
 [8,]  9.229 10.801 20.216 25.594     NA     NA     NA
 [9,]  9.234 10.805 20.258 25.619     NA     NA     NA
[10,]  9.241 10.814 20.264 25.626     NA     NA     NA
[11,]  9.248 10.819 20.281 25.649     NA     NA     NA
[12,]  9.231 10.800 20.219 25.567     NA     NA     NA
</code></pre>

<p>How do I get a data frame like this?:</p>

<pre><code> S     p1     p2     p3     p4     p5     p6     p7
 1     NA  9.520 11.137 16.576 18.225 20.576 25.861     
 2  9.005  9.491 11.106 16.530 18.184 20.495 25.773
 3     NA  9.437 11.050     NA     NA 20.393 25.711
 4     NA  9.442 11.058     NA     NA 20.411 25.711
 5     NA  9.431 11.045     NA     NA 20.421 25.707
 6     NA  9.461 11.052     NA     NA 20.319 25.657
 7     NA  9.245 10.819     NA     NA 20.253 25.628
 8     NA  9.229 10.801     NA     NA 20.216 25.594
 9     NA  9.234 10.805     NA     NA 20.258 25.619
10     NA  9.241 10.814     NA     NA 20.264 25.626 
11     NA  9.248 10.819     NA     NA 20.281 25.649 
12     NA  9.231 10.800     NA     NA 20.219 25.567 
</code></pre>

<p>In the matrix, each row represent the timepoints from 1 sample. The matrix is balanced with an arbitrary number of NAs as needed. In the dataframe, each column represents all the timepoints that are functionally equivalent. Timepoints in a single sample cannot be functionally equivalent. </p>

<p>I think I should be able to solve this with some form of kmeans clustering that restricts each cluster from having more than one member from each sample.  Any ideas?</p>
"
"0.11516335992622","0.143149583578467"," 21791","<p>I wrote a script to do some analysis and it was working fine until I tried to impliment a while loop to find the number clusters appropriate for k-means. For some reason it keeps saying that an argument is of length zero, but there shouldn't be any. I'm running this remotely, and it works fine locally.</p>

<pre><code>freq_1 &lt;- NULL
freq_alignment &lt;- NULL
for (res in point_reference) {
    point &lt;- paste(as.character(res), ""_output.txt"", sep = """")
    point_file &lt;- file(point, ""r"")
    point2 &lt;- read.table(point_file)
    point2 &lt;- as.data.frame(point2)
    k &lt;- 2
    check &lt;- 0.5
    while (check &lt; 0.75) {
            k &lt;- k + 1
            kcluster &lt;- kmeans(point2, k)
            check &lt;- kcluster$betweenss/(kcluster$tot.withins+kcluster$betweenss)
}
    config &lt;- kcluster$cluster
    frames &lt;- length(config)
    freq &lt;- as.data.frame(table(conformations))
    freq_1 &lt;- cbind(freq_1, freq)))
    freq_alignment &lt;- cbind(freq_alignment, kcluster$cluster)
    close(residue_file)
} 
</code></pre>

<p>point_reference is a list of numbers (2, 3, 4, etc.) corresponding to which file to load and the files themselves load fine. My goal is to find the k that corresponds to 75% of the total SS coming from between clusters. Only the loops is wrong...if I replace it with just clustering with k = 5, it works fine. The exact error is:</p>

<p>Error in while (check &lt; 0.75) { : argument is of length zero</p>

<p>Again, I'm doing this remotely on a cluster, and it works on my desktop R64. All files were produced on the cluster. I hope you guys can help! Thank in advance.</p>
"
"0.126155140059354","0.156812512046795"," 21955","<p>I am doing an unsupervised clustering analysis for a genomics project. This means that I do not know when a particular clustering analysis is good or not.</p>

<p>I am running different clustering algorithms and different 'sets of features'. What I mean with different 'sets of features' is that given a data frame, I choose different combination of columns depending on its biological importance. For instance, some variables measure things at the sequence level, while others are measuring a particular cellular process or some other feature that cannot be measured at the sequence level. I am playing around with the different outputs of these sets of features, running the algorithms with <em>all</em> the features, or ignoring some, etc .</p>

<p>What I want is to compare the different clusters of these different runs and see if some of my objects are being clustered similarly despite lacking some sets of features. Does this make sense?</p>

<p>Is there any recommendation on how can I do this?</p>
"
"0.072835704072923","0.0905357460425185"," 27323","<p>I have 114 vectors with 6 boolean attributes. I saw that might be several distinct clusters in a simple visualization. K-means clustering on the transformed vectors (true = 1, false = 0) results in roughly the clusters that I had seen in the visualization.</p>

<p>However, I am not sure what the most appropriate clustering method for this kind of data is, and how to determine the confidence in those factors (the k-means results change every time due to randomization). Should I treat the data as nominal or as numerical data?</p>

<p>What would be the best way to do a cluster analysis on this kind of data in R?</p>
"
"0.192705159543249","0.222425421019475"," 28492","<p>For fun, I tried to replicate the results of <a href=""http://rpproxy.iii.com:9797/MuseSessionID=248c435aa056d82d70d390e949c628fb/MuseHost=rfs.oxfordjournals.org/MusePath/content/22/1/435.abstract"" rel=""nofollow"">Petersen (2009)</a> who deals with the correct estimation of standard errors in finance panel data sets. </p>

<p>In a nutshell, he estimates the following standard regression for a panel data set:</p>

<p>$$
Y_{it} = X_{it} \beta + \epsilon_{it}
$$ </p>

<p>where $\epsilon_{it} = \gamma_i + \eta_{it}$ and $x_{it} = \mu_{i} + \nu_{it}$. Hence, both the residual and the independent variable have a firm-specific component. Petersen goes on to show that this results in biased standard errors when applying the standard OLS. For example, he shows in table 1 of his paper that if both the residual volatility and the variable volatility are driven by 50% by a firm-specific component, the true standard errors are nearly twice as large as the ones given by OLS.</p>

<p>He shows that in a MCS and I reproduced those results in R, as you can see from the code below. Naturally, I asked myself how I would compute the correct standard errors in R and the package of choice seemed to be <code>plm</code>. However, I just don't get the correct results out of it and I don't know what I miss.</p>

<p>Here is my code:</p>

<pre><code>library(plm)
runMCS &lt;- function(runs, nrN, nrT, fracFirmX, fracFirmEps, sd_X, sd_eps, beta) {

  betas    &lt;- numeric(runs)
  se_betas &lt;- numeric(runs)
  panel_betas    &lt;- numeric(runs)
  se_panel_betas &lt;- numeric(runs)

  for (i in 1:runs) {

    #Model epsilon, X, and Y
    eps &lt;- rep(rnorm(nrN, 
                     mean=0, 
                     sd = sd_eps * sqrt(fracFirmEps)), 
               each=nrT) + 
                 rnorm(nrN * nrT, mean=0, sd = sd_eps * sqrt(1-fracFirmEps))
    X   &lt;- rep(rnorm(nrN, 
                     mean=0, 
                     sd = sd_X   * sqrt(fracFirmX)),   
               each=nrT) + 
                 rnorm(nrN * nrT, mean=0, sd = sd_X   * sqrt(1-fracFirmX))
    Y   &lt;- beta * X + eps

    #Compute regression (OLS)
    reg &lt;- summary(lm(Y ~ X))

    #Save results
    betas[i]    &lt;- reg$coef[2, 1]
    se_betas[i] &lt;- reg$coef[2, 2]

    #Try plm
    df &lt;- data.frame(Firm = rep(1:nrN, each=nrT),
                     Time = rep(1:nrT, times=nrN),
                     Y = Y,
                     X = X)
    preg &lt;- summary(plm(Y ~ X, data=df, index=c(""Firm"", ""Time""), effect=""individual"", model=""within"")) #within is fixed effects
    panel_betas[i]    &lt;- preg$coef[1, 1]
    se_panel_betas[i] &lt;- preg$coef[1, 2]
  }

  return(c(avg_beta = mean(betas), 
           true_se = sd(betas), 
           avg_se = mean(se_betas), 
           avg_clustered = mean(panel_betas),
           se_clustered = mean(se_panel_betas)))

}
MCS_50_50 &lt;- runMCS(50, 500, 10, 0.5, 0.5, 1, 2, 1)
MCS_50_50
     avg_beta       true_se        avg_se avg_clustered  se_clustered 
   1.00503955    0.06020203    0.02825567    1.00433092    0.02985546
</code></pre>

<p>Note that I only run the simulation 50 times here because the plm function slows it down considerably. So basically, it makes virtually no difference if I call <code>lm</code> or <code>plm</code>. I'm pretty confident that I set the <code>index</code> and <code>model</code> option correct after reading the vignette of the package. However, I must miss something here! Interestingly, the package also has the <code>fixef</code> function and if I call that on one run, I get something like  this:</p>

<pre><code>summary(fixef(plm(Y ~ X, data=df, index=c(""Firm"", ""Time""), effect=""individual"", model=""within"")))
1      13.60377     0.44112    30.8391 &lt; 2.2e-16 ***
2    -830.74707     0.44136 -1882.2236 &lt; 2.2e-16 ***
3    -326.96042     0.44137  -740.7840 &lt; 2.2e-16 ***
4     169.16463     0.44246   382.3287 &lt; 2.2e-16 ***
...
</code></pre>

<p>I'm not quite sure how to interpret those results, but here, I get considerably larger standard errors for each firm separately. If I would average those, I would end up with something above 0.44 which is considerably closer to the true standard errors, but still not right.</p>

<p>So, again a very long question from me, sorry for that ;-) Note that I did check answers before and I found this interesting <a href=""http://stats.stackexchange.com/questions/10017/standard-error-clustering-in-r-either-manually-or-in-plm"">link</a>. The white paper that is referred to in the answer is interestingly the same person that implemented the solution on Petersen's <a href=""http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/se_programming.htm"" rel=""nofollow"">webpage</a>. So I'm pretty sure that I could get the correct standard errors by implementing Mahmood Arai's solution. But I'm looking for an already implemented and therefore safe option and I just wonder why that plm function does not work.</p>
"
"0.0515026202624605","0.064018439966448"," 28620","<p>I recently read a <a href=""http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/"" rel=""nofollow"">fascinating article</a> describing methods for clustering data without assuming a fixed number of clusters.</p>

<p>The article even includes some sample code, in a mix of Ruby, Python, and R.  However, the meat of the analysis is performed using <a href=""http://scikit-learn.sourceforge.net/dev/index.html"" rel=""nofollow"">scikit-learn</a>'s <a href=""http://scikit-learn.sourceforge.net/dev/modules/mixture.html"" rel=""nofollow"">Dirichlet Process Gaussian Mixture Model</a> to actually find clusters in some sample data taken from McDonald's menu.</p>

<p>Obviously, this a a great excuse to learn some more python, but I'm lazy and would like to find a ready-made R package that can take a dataframe and return clusters, in a manner similar to the <a href=""http://stat.ethz.ch/R-manual/R-devel/library/stats/html/kmeans.html"" rel=""nofollow"">kmeans</a> function.  <a href=""http://cran.r-project.org/web/views/Cluster.html"" rel=""nofollow"">A quick search on CRAN</a> reveals the packages <a href=""http://cran.r-project.org/web/packages/dpmixsim/index.html"" rel=""nofollow"">dpmixsim</a> and <a href=""http://cran.r-project.org/web/packages/profdpm/index.html"" rel=""nofollow"">profdpm</a>.  Any suggestions for the best place to start?</p>
"
"0.0515026202624605","0.064018439966448"," 31906","<p>I'm trying to do fuzzy k-means clustering on a dataset using the cmeans function (R) . The problem Im facing is that the sizes of clusters are not as I would like them to be. This is done by calculating the cluster to which the observations are ""closest"". </p>

<pre><code>cl$size
 [1]   108    31   192    51   722 18460    67  1584   419 17270
</code></pre>

<p>Here we see that for 10 clusters we have two huge clusters and a lot of very small ones. Does this imply that two clusters are optimal in any way? If I do regular K-means 10 segments look very well, with good sizes and their intepretation makes a lot of sense but I would like to try fuzzy correctly. I just started exploring this fuzzy clustering so any help and pointers are overly welcome. </p>
"
"0.0515026202624605","0.064018439966448"," 33210","<p>I have a database where each observation is a person. They were questioned on their attitude towards the consumption of X category of product. I have being using K-means to segment this data. </p>

<p>I have noticed that people under 19 years old tend to be quite different in their responses to those over 19.  I was thinking of dividing the data in <code>&lt;19</code> and <code>&gt;=19</code> and producing two clusterings and then merging them so as to produce a single report. </p>

<p>Does this make any sense from a data mining point-of-view? Is there precedent for doing so?</p>
"
"0.11516335992622","0.143149583578467"," 34238","<h2>Data Structure</h2>

<pre><code>&gt; str(data)
 'data.frame':   6138 obs. of  10 variables:
 $ RT     : int  484 391 422 516 563 531 406 500 516 578 ...
 $ ASCORE : num  5.1 4 3.8 2.6 2.7 6.5 4.9 2.9 2.6 7.2 ...
 $ HSCORE : num  6 2.1 7.9 1 6.9 8.9 8.2 3.6 1.7 8.6 ...
 $ MVMNT  : Factor w/ 2 levels ""_Withd"",""Appr"": 2 2 1 1 2 1 2 1 1 2 ...
 $ STIM   : Factor w/ 123 levels "" arti"","" cele"",..: 16 23 82 42 105 4 93 9 34 25 ...
 $ DRUG   : Factor w/ 2 levels ""Inactive"",""Pharm"": 1 1 1 1 1 1 1 1 1 1 ...
 $ FULLNSS: Factor w/ 2 levels ""Fasted"",""Fed"": 2 2 2 2 2 2 2 2 2 2 ...
 $ PATIENT: Factor w/ 25 levels ""Subj01"",""Subj02"",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ SESSION: Factor w/ 4 levels ""Sess1"",""Sess2"",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ TRIAL  : Factor w/ 6138 levels ""T0001"",""T0002"",..: 1 2 3 4 5 6 7 8 9 10 ...
</code></pre>

<h2>Full Model Candidate</h2>

<pre><code>model.loaded.fit &lt;- lmer(RT ~ ASCORE*HSCORE*MVMNT*DRUG*FULLNSS
                              + (1|PATIENT) + (1|SESSION), data, REML = TRUE)
</code></pre>

<ul>
<li>Reaction times from trials are clustered within sessions, which in turn are clustered within patients</li>
<li>Each trial can be characterized by two continuous covariates of ASCORE and HSCORE (ranging between 1-9) and by a movement response (withdraw or approach)</li>
<li>Sessions are characterized by drug intake (placebo or active pharmacon) and by fullness (fasted or pre-fed)</li>
</ul>

<h2>Modeling and R Syntax?</h2>

<p>I'm trying to specify an appropriate <em>full model</em> with a loaded mean structure that can be used as a starting point in a top-down model selection strategy. </p>

<p>Specific issues:</p>

<ul>
<li>Is the syntax correctly specifying the clustering and random effects?</li>
<li>Beyond syntax, is this model appropriate for the above within-subject design?</li>
<li>Should the full model specify all interactions of fixed effects, or only the ones that I am really interested in?</li>
<li>I have not included the STIM factor in the model, which characterizes the specific stimulus type used in a trial, but which I am not interested to estimate in any way - should I specify that as a random factor given it has 123 levels and very few data points per stimulus type?</li>
</ul>
"
"0.126155140059354","0.156812512046795"," 35047","<p>I have a data set of 40,000 individuals which I clustered using k-means. I used 30 variables, each ordinal from <code>1=minimum</code> to <code>5=maximum</code>. I reduced these 30 variables to 10 factors and ran K-means on these new variables. I kept a clustering of 12 clusters. </p>

<p>I now have 7000 new observations, I want to classify these using a discriminant function. I built one in SPSS using the 30 previous variables. I get a good clasification on the original dataset (40,000), but when I run it on the new observations, one segment doesn't show at all. 0 cases are of this type. This makes no sense to me. </p>

<p>The original clusters seem healthy and apart, I did them in R with multiple different initial center methods and ran them many many times, and stuck with the best one.  The discriminant function has no problem finding this ""disappearing"" cluster in the original dataset. </p>

<p>What could be causing this? </p>
"
"0.126155140059354","0.130677093372329"," 38117","<p>I want to visualize the results of a clustering (produced with <code>protoclust{protoclust}</code>) by creating scater plots for each pair of variables used for classifying my data, colouring by classes and overlapping the ellipses for the 95% confidence interval for each of the classes (to inspect which elipses-classes overlap under each pair of variables).</p>

<p>I have implemented the drawing of the ellipses in two different ways and the resulting ellipses are different! (bigger ellipses for first implementation!) 
A priori they differ only in size (some diferent scaling?), as the  centers and angle of axes, seem to be the similar in both.
I guess I must be doing something wrong by using one of them (hope not with both!), or with the arguments.</p>

<p>Can anyone tell me what I am doing wrong?</p>

<p>Here the code for the two implementations; both are based on the answers to <a href=""http://stackoverflow.com/questions/2397097/how-can-a-data-ellipse-be-superimposed-on-a-ggplot2-scatterplot"">How can a data ellipse be superimposed on a ggplot2 scatterplot?</a></p>

<pre><code>### 1st implementation 
### using ellipse{ellipse}
library(ellipse)
library(ggplot2) 
library(RColorBrewer)
colorpal &lt;- brewer.pal(10, ""Paired"")

x &lt;- data$x
    y &lt;- data$y
group &lt;- data$group
df &lt;- data.frame(x=x, y=y, group=factor(group))

df_ell &lt;- data.frame() 
for(g in levels(df$group)){df_ell &lt;- rbind(df_ell, cbind(as.data.frame(with(df[df$group==g,], ellipse(cor(x, y),scale=c(sd(x),sd(y)),centre=c(mean(x),mean(y))))),group=g))} 

p1 &lt;- ggplot(data=df, aes(x=x, y=y,colour=group)) + geom_point() + 
  geom_path(data=df_ell, aes(x=x, y=y,colour=group))+scale_colour_manual(values=colorpal)

### 2nd implementation 
###using function ellipse_stat() 
###code by Josef Fruehwald available in: https://github.com/JoFrhwld/FAAV/blob/master/r/stat-ellipse.R

p2 &lt;-qplot(data=df, x=x,y=y,colour=group)+stat_ellipse(level=0.95)+scale_colour_manual(values=colorpal)
</code></pre>

<p>Here is the two plots together (left graph is <code>p1</code> implementation (<code>ellipse()</code>):</p>

<p><img src=""http://i.stack.imgur.com/Qf49A.jpg"" alt=""enter image description here""></p>

<p>The data are available here: <a href=""https://www.dropbox.com/sh/xa8xrisa4sfxyj0/l5zaGQmXJt"">https://www.dropbox.com/sh/xa8xrisa4sfxyj0/l5zaGQmXJt</a></p>
"
"0.170814867130737","0.2123251450326"," 38460","<p>I am new here - and relatively new to statistics, data mining and R. I am trying to understand why my data is not clustering correctly - or if I am reading it wrong. Shortly about the project:</p>

<p>My data points are behavior of online gaming users over time (one observation for each user). I am trying to cluster a matrix containing 5000 observations. So far I have 4 dimensions that correspond to average games/day, interval between days, score, friends - all of these happened in the first 2 months of user's online presence. So it looks approximately like:</p>

<pre><code>  avg_g_day avg_interval  score friends
1       8.5            6   6050       0
2   48.1304       1.8636  90530       0
3   70.0702       1.0714 293520       2
4        25            1   5710       4
5      3.75           10  10900       0
</code></pre>

<p>I am trying cluster this using all possible methods in R. So far I have not had success. I first scale my data using scale() and then fit the clusters. When I try kmeans (which would mean Euclidian distance, in my understanding), I get intersecting clusters upon visualization with clusplot. When I am trying to find the best k, I get that the best k is 2, which seems quite un-commonsense with 5000 points and 4 dimensions. I have tried other methods and other distance metrics, so far to no avail - I have used Mclust() and hclust. In each dimension the distribution is not uniform and I do not get warnings that best number of clusters is 1. Then why does the kmeans or pam method give me clusters that intersect on the 2 first principal components graph (clusplot)? Is there a way to separate them? </p>

<p>Since I am not sure which details could provide more insight into my problem, I will post them if questions arise. Thank you for any help. </p>
"
"0.0515026202624605","0.064018439966448"," 41575","<p>I have a dataset of about a million companies containing their names, total employees and annual sales. I want to come up with a function that when given the company returns the 5 most similar companies in terms of their distance in total employees and annual sales.</p>

<p>I thought of doing k-means clustering on the dataset and find clusters. Then return all the companies in that cluster. The problem with this approach is that I don't know the number of clusters I should form beforehand.</p>

<p>Also, on a separate note, if I were to obtain a list of specialties (e.g. marketing, software, etc.) for each of the companies - how can I transform this qualitative value in to a number which can later help me calculate similarity.</p>
"
"0.11516335992622","0.114519666862774"," 46798","<p>I am looking for a clustering algorithm. My idealized dataset looks like this:<br>
<img src=""http://i.stack.imgur.com/pqEpC.png"" alt=""http://i.stack.imgur.com/bSlNU.png""></p>

<p>The clustering result should look like the Rapidminer density plot:<br>
<img src=""http://i.stack.imgur.com/uMRQ4.png"" alt=""http://i.stack.imgur.com/Sk3xK.png""> </p>

<p>Means 3 or 4 clusters should be the clustering result: One Cluster (1|1) to (5|6), one or two for the points on Z = 5 and one for the overlaying area (6|1) to (11|6).</p>

<p>Based on ""density"" I gave the <code>DBSCAN</code> R package a shot. The best result I could get was the one in the above picture. So not exactly what I expected, as the overlaying area was not recognized as a separate cluster. Any ideas which algorithm would provide the results I expect?
<img src=""http://i.stack.imgur.com/pqEpC.png"" alt=""enter image description here""></p>
"
"0.154507860787381","0.192055319899344"," 46978","<p>I am fitting a <em>Fixed-Effects</em> model, with intercepts at <code>cluster</code> level.</p>

<p>One of the most direct ways is probably to use the <code>-plm-</code> package. Another well-known possibility is to apply OLS (i.e. to adopt <code>-lm-</code>) to the <em>demeaned data</em>, where the means are taken at the clustering level.</p>

<p>This second approach is usually referred to as the <strong>within transformation</strong>. It is quite convenient from a computational standpoint, because we are still controlling unobserved heterogeneity at clustering level, but we do not need to estimate all the time-fixed intercepts.</p>

<p>I have tried both of these approaches, and I came to a strange result. In practice, the coefficient of the regressor of interest, <code>x</code>, is the same in both cases. However, its standard error (and actually all the other relevant quantities of the regression: R squared, F test, etc.) is different.</p>

<p>Please, notice that I have carefully read both the <em>R documentation</em> about <code>-plm-</code> and the <a href=""http://www.google.it/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;ved=0CD4QFjAB&amp;url=http://www.jstatsoft.org/v27/i02/paper&amp;ei=7f3mUP_0DYrXtAaD7oDADw&amp;usg=AFQjCNFu_xrsnFYsC8j8DDh9mRQnoyQ6jg&amp;bvm=bv.1355534169,d.bGE"" rel=""nofollow"">related paper of the authors</a>, where it is stated that the package apply the <em>within transformation</em> and then apply OLS, as I did...</p>

<p>The R script is:</p>

<pre><code># set seed, load packages, create fake sample

set.seed(999)
library(plyr)
library(plm)

dat &lt;- expand.grid(id=factor(1:3), cluster=factor(1:6))
dat &lt;- cbind(dat, x=runif(18), y=runif(18, 2, 5))


############################
#   FE model using -plm-   #
############################

# model fit  
fe.1 &lt;- plm(y ~ x, data=dat, index=""cluster"", model=""within"")

# estimated coefficient and standard error of x
b.1 &lt;- summary(fe.1)$coefficients[,1]
    se.1 &lt;- summary(fe.1)$coefficients[,2]


######################################
#   OLS on within-transformed data   #
######################################

# augmenting data frame with cluster-mean centered variables 
dat.2 &lt;- ddply(dat, .(cluster), transform, dem_x=x-mean(x), dem_y=y-mean(y))

# model fit
fe.2 &lt;- lm(dem_y ~ dem_x - 1, data=dat.2)

# estimated coefficient and standard error of x
b.2 &lt;- summary(fe.2)$coefficients[1,1]
    se.2 &lt;- summary(fe.2)$coefficients[1,2]


#########################
#   models comparison   #
#########################

b.1; b.2
se.1; se.2

summary(fe.1)
summary(fe.2)
</code></pre>

<p>Notice that in the second model it is necessary to manually eliminate the intercept from the model. </p>
"
"0.11516335992622","0.0858897501470802"," 49313","<p>I have a k-means clustering result with 35 clusters, there are 5000 documents that each belong to one of the 35 cluster. I would like to visualize the results of the clustering algorithm on a scatter plot (or something similar) where each document is colored based on which cluster they belong to, and their distance on the visualization is proportional to their distance in similarity (i.e. the more similar they are, the closer they appear on the visualization). Ideally, it would also be nice to see the top 10 words that belong to the clusters. I am attaching my code for the clustering algorithm, it deals with data from a database. </p>

<pre><code>myCorpus &lt;- Corpus(VectorSource(userbios$bio))
    docs &lt;- userbios$twitter_id
# convert to lower case
myCorpus &lt;- tm_map(myCorpus, tolower)
# remove punctuation
myCorpus &lt;- tm_map(myCorpus, removePunctuation)
# remove numbers
myCorpus &lt;- tm_map(myCorpus, removeNumbers)
# remove URLs
removeURL &lt;- function(x) gsub(""http[[:alnum:]]*"", """", x)
myCorpus &lt;- tm_map(myCorpus, removeURL)
# add one extra stop words:  ""via""
myStopwords &lt;- c(stopwords('english'), ""twitter"", ""tweets"", ""tweet"", ""tweeting"", ""account"")


# remove stopwords from corpus
myCorpus &lt;- tm_map(myCorpus, removeWords, myStopwords)


myTdm &lt;- TermDocumentMatrix(myCorpus, control = list(wordLengths=c(1,Inf), weighting=weightTfIdf))
# remove sparse terms
myTdm2 &lt;- removeSparseTerms(myTdm, sparse=0.90)

m2 &lt;- as.matrix(myTdm2)
#cluster terms
distMatrix &lt;- dist(scale(m2))
fit &lt;- hclust(distMatrix, method=""ward"")
# transpose the matrix to cluster documents (tweets)
 m3 &lt;- t(m2)

# k-means clustering
 k &lt;- 35
kmeansResult &lt;- kmeans(m3, k)
#cluster centers
round(kmeansResult$centers, digits=3)
    for (i in 1:k) {
      cat(paste(""cluster "", i, "": "", sep=""""))
      s &lt;- sort(kmeansResult$centers[i,], decreasing=T)
      cat(names(s)[1:15], ""\n"")
      # print the tweets of every cluster + # 
      print(docs[which(kmeansResult$cluster==i)])
}
</code></pre>
"
"0.072835704072923","0.0905357460425185"," 51556","<p>Does anyone know a good method to determine if clustering using kmeans is even appropriate? That is, what if your sample is actually homogenous? I know something like a mixture model (via mclust in R) will provide fit statistics for the 1:k cluster case, but it seems like all of the techniques to evaluate kmeans requires at least 2 clusters.</p>

<p>Does anyone know of a technique to compare the 1 and 2 cluster cases for kmeans?</p>
"
"0.145671408145846","0.135803619063778"," 51856","<p>I performed and plotted a kmeans analysis in R with the following commands:</p>

<pre><code> km = kmeans(t(mat2), centers = 4)
 plotcluster(t(mat2), km$cluster)      #from library(fpc)
</code></pre>

<p>Here is the result from the plot: <img src=""http://i.stack.imgur.com/p3ds0.png"" alt=""kmeans clustering""></p>

<p>This question is related to a previous question: <a href=""http://stats.stackexchange.com/questions/51707/reading-kmeans-data-and-chart-from-r"">Previous Question</a></p>

<p>My data matrix has dimensions $291 \times 31$ (after taking the transpose by <code>t(mat2)</code>)
What I want to know, is <strong>how can I create a mapping from each row in the matrix to a 2D point in the plot?</strong> My idea is to get the $31$ dimensional coordinates for each point in the plot and then map and compute the 2D coordinates with <code>discrproj()</code>.For example, I see that I should be able to find the 2D center points of all clusters by calling <code>discrproj()</code> on the matrix given by <code>km$centers</code> (which has dimensions $4 \times 31$ and hence contains the coordinates for each cluster in $31$ dimensional space). </p>

<p>However, where is the data for the coordinates in $31$ dimensional space for every 2D point in the plot? Is this data just my $291 \times 31$ data matrix? In summary:</p>

<ol>
<li>How can I create a mapping from each row in the $291 \times 31$ data matrix to a 2D point in the plot?</li>
<li>Where/what is the data for the coordinates in $31$ dimensional space for every 2D point in the plot</li>
</ol>
"
"NaN","NaN"," 55147","<p>I'm wondering if there is a good way to calculate the clustering criterion based on BIC formula, for a k-means output in R? I'm a bit confused as to how to calculate that BIC so that I can compare it with other clustering models. Currently I'm using the stats package implementation of k-means.</p>
"
"0.072835704072923","0.0905357460425185"," 56479","<p>I want to do clustering of my data in R, using kmeans or hclust (I am a new R user).</p>

<p>My data is ordinal, Likert scale, to measure the causes of cost escalation. I have 41 causes ""variables"" that scaled from 1 to 5 (1: no effect, 5: major effect). I have about 160 observations ""who rank the causes"".</p>

<p>I would like to cluster the variables (the columns) in terms of similarity of occurrence in observations, but I don't know how to start.</p>

<p>Do I have to convert the scale to percentage or z-score before clustering?</p>

<p>My data is available and shared as a <a href=""https://docs.google.com/spreadsheet/ccc?key=0AlrR2eXjV8nXdGtLdlYzVk01cE96Rzg2NzRpbEZjUFE&amp;usp=sharing"" rel=""nofollow"">Google Drive spreadsheet</a>.</p>
"
"0.105129283382795","0.104541674697863"," 58071","<p>I have multiple <code>kmeans</code> plots that I have generated in R. Specifically I have $5$ weeks and I generate $1$ <code>kmeans</code> plot per week. I am clustering on vectors. Most vectors in the $5$ <code>kmeans</code> plots will occur in each plot. What I am interested in determining is which vectors have changed cluster membership. To make this clear suppose I have a vector identified by the word ""soccer"" then I would like to see which cluster it belongs to in week1, week2, and so forth.</p>

<p>Testing for membership change should be a simple task, for each <code>kmeans</code> clustering, each point is given an ID as to which cluster it belongs. I have 4 clusters so each week our example vector with name ""soccer"" could be tagged $1$, $2$, $3$, or $4$. The naive solution would be to just check the tag for a particular vector each week. However, this is not the case because R randomly selects the tags for each cluster. I know this is the case because each cluster represents some class of curves. You can visibly see that the kmeans algorithm has partitioned the vectors into 4 classes of curves. </p>

<p>Are there ways to make the tag IDs for each cluster stay constant? That is if the cluster tagged in week1 with ID $2$ is the linear curve, then the clusters tagged $2$ in all remaining weeks will always be the linear cluster.</p>

<p>Are there any initial conditions I can pass to <code>kmeans</code> to make this happen? I posted this question here instead of stackoverflow because I believe this questions requires more understanding of the <code>kmeans</code> algorithm.</p>
"
"0.170814867130737","0.173720573208491"," 59080","<p>I've got a set of (continuous) values from a measurement, where each object should be either positive or negative, and I know that the values of the ""negative"" objects should be approximately normally distributed.</p>

<p>I've been using a <code>k-means</code>-based algorithm in <code>R</code> to cluster the data and thus to classify the objects. Here is a typical example of the clustering result, shown as a density plot:</p>

<p><img src=""http://i.stack.imgur.com/aB0Ob.png"" alt=""enter image description here""></p>

<p>As you can see, although the algorithm does distinguish two populations, it includes a part of the positive objects into the negative cluster (the blue one). You can also see that there are some extremely negative outliers, causing the blue curve to be skewed to the right. These outliers are artefacts, resulting from the method the values are measured and calculated, and I'd like to find a cut-off to exclude them in future.</p>

<p>Now, I'd like to fit a normal distribution curve the the negative population and to define a cut-off for those artefacts (outliers) based on this fit as well as to estimate some characteristics of the distribution. I was trying to do this by extracting only those objects that fall into the blue cluster and then fitting a normal distribution on them, but obviously this doesn't work out, mainly due to the bump on the right of the blue population. So I need another classification algorithm that would a priori assume the existence of two Gauss distributions and fit them (in the best case taking into account the possibility of the existence of outliers). However, so far I haven't been able to find an algorithm that would be reliable and fast enough (I've got around 20,000-50,000 objects), although I'm sure there must be a simple way in <code>R</code> to do so.</p>

<p>So in summary, I'm looking for a classiciation algorithm implemented in <code>R</code> to fit a normal distribution to the ""negative"" objects of my measurement, ideally taking into account that outliers do exist.</p>

<p>Does anyone has a suggestion how to accomplish this?</p>
"
"0.11516335992622","0.114519666862774"," 59554","<p>I'm running an analysis on a few data sets that each typically have 100-200 cases measured across 120-160 variables - something similar to looking at gene expressions. Each variable is a non-centered score for expression of a particular attribute frequency for each case. In many cases though, any given attribute is likely to be 0 (i.e. very sparse for most). </p>

<p>The cases typically fall into 2-4 natural groups, and I'm trying to figure out how to find out which high-expression attributes are most representative for each group and/or which ones are driving the distinctions between group memberships. </p>

<p>I've been experimenting with using correlation clustering and the resulting groups <em>do</em> appear to match the natural groups assigned by other means, so now I'm just looking for a way to ""unpack"" those clusters in terms of their attribute expressions to find out which attributes/variables are the most influential ones in each of my samples. </p>

<p>Given the number of variables involved, it seems like the usual approaches like PCA or discriminate or factor analysis would be very cumbersome. So far, I haven't really found much information on how to deal with variable influence that would fit situations when there are scores or hundreds of variables.</p>

<p>Any suggestions?</p>
"
"0.170814867130737","0.193022859120546"," 62622","<p>I have as an input a number of points that I need to partition into clusters. Each point has a number of features that are ideally to be used to find the similarity between each point and the others. Some of these features are scalar values (one number) and others are vectors.</p>

<p>For example, assume that each point has the following features:</p>

<ol>
<li><p><strong>S1</strong>: scalar value</p></li>
<li><p><strong>V1</strong>: 48 $\times$ 1 vector</p></li>
<li><p><strong>V2</strong>: 48 $\times$ 1 vector</p></li>
</ol>

<p>For example one point may have (S1,V1, V2) as (100, {0, 100, 20, 30}, {75,0,10, 5})</p>

<p>My hypothesis is to use <a href=""http://en.wikipedia.org/wiki/Cosine_similarity"" rel=""nofollow"">cosine similarity</a> to find how similar the vector V1 or V2 of one point is to the vector V1 or V2 of another point. I have already computed the similarity matrices between all points in terms of V1 and V2 similarities.</p>

<p>By exploring the standard clustering algorithms in R, I have found that k-means turns to  use the Euclidean distance, which might be suitable for clustering points according to their scalar values, because [subject unclear] doesn't work for the situation where I have hybrid types of features (scalars and vectors). Also the K-medoid clustering seems to be supporting only the Euclidean and the Manhattan distances.</p>

<p>I think what should be done is to generate one more distance/similarity matrix between all points based on the scalar value, so that we end with three similarity matrices that show the similarity between each point and the other points according to each feature regardless of it being a scalar or a vector, and use those matrices for finding the neighbourhood of points while clustering.</p>

<p>I wonder if there is an implementation for a clustering algorithm that accepts as an input the similarity matrices (or alternatively the dissimilarity/distance matrices) between vector features of multiple points and uses them for clustering?</p>
"
"0.262793169802285","0.278262003135101"," 65411","<p>I have carried out a clustering of coordinate points (longitude, latitude) and found surprising, adverse results from clustering criteria for the optimal number of clusters. The criteria are taken from the <code>clusterCrit()</code> package. The points which I am trying to cluster on a plot (the geographic characteristics of the data set is clearly visible) :</p>

<blockquote>
  <p><img src=""http://i.stack.imgur.com/EAgVj.jpg"" alt=""Plot of all observations""></p>
</blockquote>

<p>The full procedure was the following :</p>

<ol>
<li>Carried out hierarchical clustering on 10k points and saved
medoids for 2 : 150 clusters.</li>
<li>Took the medoids from (1) as seeds for kmeans clustering of 163k observations. </li>
<li>Checked 6 different clustering criteria for the optimal number of clusters.</li>
</ol>

<p>Only 2 clustering criteria gave results that make sense for me â€“ the Silhouette and Davies-Bouldin criteria. For both of them one should look for the maximum on the plot. It seems both give the answer â€œ22 Clusters is a good numberâ€. For the graphs below: on the x axis is the number of clusters and on the y axis the value of the criterion, sorry for the wrong descriptions on the image. Silhouette and Davies-Bouldin respectively :</p>

<blockquote>
  <p><img src=""http://i.stack.imgur.com/9tlDB.jpg"" alt=""Silhoette Criterion Plot"">
  <img src=""http://i.stack.imgur.com/USELa.jpg"" alt=""Davies-Bouldin Criterion Plot""></p>
</blockquote>

<p>Now letâ€™s look at Calinski-Harabasz and Log_SS values. The maximum is to be found on the plot. The graph indicates that the higher the value the better the clustering. Such a steady growth is quite surprising, I think 150 clusters is already a quite high number. Below the plots for Calinski-Harabasz and Log_SS values respectively.</p>

<blockquote>
  <p><img src=""http://i.stack.imgur.com/toHAM.jpg"" alt=""Calinski-Harabasz Criterion Plot"">
  <img src=""http://i.stack.imgur.com/yJiG0.jpg"" alt=""Log_SS Criterion Plot""></p>
</blockquote>

<p>Now for the most surprising part the last two criteria. For the Ball-Hall the biggest difference between two clusterings is desired and for Ratkowsky-Lance the maximum. Ball-Hall and Ratkowsky-Lance plots respectively :</p>

<blockquote>
  <p><img src=""http://i.stack.imgur.com/09zWT.jpg"" alt=""Ball-Hall Criterion Plot"">
  <img src=""http://i.stack.imgur.com/UpE3b.jpg"" alt=""Ratkowsky-Lance Criterion Plot""></p>
</blockquote>

<p>The last two criteria give completely adverse answers (the smaller the number of clusters the better) than the 3rd and 4th criteria. How is that possible? For me it seems like only the first two criteria were able to make any sense of the clustering. A Silhouette width of around 0.6 is not that bad. Should I just skip the indicators that give strange answers and believe in those that give reasonable answers? </p>

<p><em>Edit: Plot for 22 clusters<img src=""http://i.stack.imgur.com/gNbON.jpg"" alt=""22 cluster solution""></em></p>

<hr>

<p><strong>Edit</strong></p>

<p>You can see that the data is quite nicely clustered in 22 groups so criteria indicating that you should choose 2 clusters seem to have weaknesses, the heuristic isn't working properly. It is ok when I can plot the data or when the data can be packed in less than 4 principal components and then plotted. But if not? How should I choose the number of clusters other than by using a criterion? I have seen tests which indicated Calinski and Ratkowsky as very good criteria and still they give adverse results for an seemingly easy data set. So maybe the question shouldn't be ""why are the results differing"" but ""how much can we trust those criteria?"".</p>

<p>Why is an euclidian metric not good? I am not really interested in the actual, exact distance between them. I understand the true distance is spheric but for all points A,B,C,D if Spheric(A,B) > Spheric(C,D) than also Euclidian(A,B) > Euclidian(C,D) which should be sufficient for for a clustering metric. </p>

<p>Why I want to cluster those points? I want to build a predictive model and there is a lot of information contained in the location of each observation. For each observation I also have cities and regions. But there are too many different cities and I don't want to make for example 5000 factor variables; therefore I thought about clustering them by coordinates. It worked pretty well as the densities in different regions are different and the algorithm found it, 22 factor variables would be all right. I could also judge the goodness of the clustering by the results of the predictive model but I am not sure if this would be wise computationally. Thanks for the new algorithms, I will definitely try them if they work fast on huge data sets.</p>
"
"0.136263125082667","0.16937687147354"," 65589","<p>I am currently implementing a Kmeans clustering algorithm in R. I am not using any packages and I wrote it from scratch. I am using only one set of initial guesses, and my action upon finding an empty cluster is to select a new data point randomly and use that as the new mean for the empty cluster.</p>

<p>I have gathered from reading online that the solution does not always converge, and it is highly sensitive to the initial means, so when I see that behavior I am not surprised. But I am finding that sometimes my solution is actually cycling between two or more different solutions. So I have two questions associated with this observation:</p>

<p>1) Within a solution cycle, one solution is always better than the others as measured by the total sum of squared distances of all points to their nearest clusters. So this implies that not only does the algorithm not necessarily find the global optimum, but also it sometimes does not even improve the total sum of squared distances from one iteration to the next? I thought the solution was at least always improving... </p>

<p>2) What is the best way to get around this problem? Do I have to program it to recognize cycles and then select the iteration in the cycle with the lowest total distance? Or is there an easier way? </p>

<p>Any help would be greatly appreciated.<br>
Thanks.</p>
"
"0.136263125082667","0.145180175548748"," 66728","<p>I'm trying to write my own code for cluster-robust (AKA panel-robust, AKA heteroskedasticity and serial-correlation-consistent) standard errors, so that I can make a couple of small extensions.  But I can't get my results to match Stata's (in which this procedure is routine), so I am probably missing some detail.  Would appreciate pointers on the code, below.</p>

<p>The model is 
$$
y_{ig} = \alpha_g + X'_{ig}\beta + \epsilon_{ig}
$$
The groupwise mean is the subtracted from every term above, getting rid of the $\alpha$, and leaving de-meaned group-varying variables:
$$
ydm_{ig} = Xdm'_{ig}\beta + \epsilon dm_{ig}
$$</p>

<p>The variance of $\beta$ is estimated as </p>

<p>$$
\left(\displaystyle\sum_G Xdm_g'Xdm_g\right)^{-1} \displaystyle\sum_G Xdm_g' \epsilon dm_g \epsilon dm_g' Xdm_g \left(\displaystyle\sum_G Xdm_g'Xdm_g\right)^{-1}
$$</p>

<p>Standard errors are the square root of the diagonal of this matrix, inflated by $G/(G-1) \times (N-1)/(N-K)$.</p>

<p>This is all textbook econometrics.  Standard in stata, and code exists to do it in R.  (<a href=""http://people.su.se/~ma/clustering.pdf%E2%80%8E"" rel=""nofollow"">this</a>, for example)</p>

<p>I want my code to not rely on <code>plm</code>, <code>lmtest</code> or <code>sandwich</code>.</p>

<p>The following script is supposed to implement the math above, simulating a panel dataset in which outcomes are autocorrelated and groups have different forms of heteroskedasticity.  It gives the right point estimates but the standard errors are bigger than those given by stata:</p>

<pre><code>rm(list=ls()) 
set.seed(999)
N = 1000
G = 200
x = rnorm(N)
z = rexp(N)
obs = N/G
fe = data.frame(ID=1:G,fe = rnorm(G)+5)
fe = data.frame(ID = rep(fe$ID,obs),fe = rep(fe$fe,obs))
t = rep(1,G); for (i in 2:obs) {t = c(t,rep(i,G))}
data = data.frame(y=x+z+fe$fe+x*rnorm(N,mean=0,sd=fe$fe*runif(N)), ID=fe$ID,x,z,t)
write.csv(data,""testdata.csv"")

demean = function(var,ID){
    dat = data.frame(var,ID)
    library(doBy)
    means = summaryBy(var~ID,data=dat,fun=mean)
    d = data.frame(var,ID)
    a = merge(d,means,by=""ID"")
    adm = a[,2]-a[,3]
    adm
    }
xdm = demean(data$x,data$ID)
ydm = demean(data$y,data$ID)
zdm = demean(data$z,data$ID)

mdm = lm(ydm~xdm+zdm-1)
summary(mdm)

e = mdm$resid

lpm = cbind(xdm,zdm)

bread = matrix(0,ncol(lpm),ncol(lpm))
tofu = matrix(0,ncol(lpm),ncol(lpm))
K = mdm$rank
    for (i in 1:G){
    	X = lpm[data$ID==unique(data$ID)[i],]
    bread = t(X)%*%X +bread

    r = e[data$ID==unique(data$ID)[i]]
    tofu = t(X) %*% r %*% t(r) %*% X + tofu
    }
bread = solve(bread)
vcv = bread%*%tofu%*%bread
se = G/(G-1)*(N-1)/(N-K)*sqrt(diag(vcv))
summary(mdm)
se
</code></pre>

<p>Using that <code>testdata.csv</code> file, I can compare to Stata:</p>

<pre><code>clear all
insheet using ""testdata.csv""
xtset id t
xtreg y x z,fe cluster(id)
</code></pre>

<p>My R-code:</p>

<pre><code>1&gt; se
       xdm        zdm 
0.16946120 0.08793485 
</code></pre>

<p>Stata's output:</p>

<pre><code>             |               Robust
           y |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
           x |   1.223739   .1992449     6.14   0.000     .8308371    1.616642
           z |    .876592   .0960943     9.12   0.000     .6870981    1.066086
</code></pre>

<p>Mine are consistently more optimistic than Stata's.  Anybody help me figure out why?  I feel like it is probably some small bug, but I can't pinpoint where.</p>

<p><strong>EDIT</strong>:  The answer to the question is <a href=""http://www.stata.com/support/faqs/statistics/intercept-in-fixed-effects-model/"" rel=""nofollow"">here</a> and <a href=""http://www.stata.com/manuals13/xtxtreg.pdf"" rel=""nofollow"">here</a>.  Stata doesn't run a textbook ``within'' estimator, rather it adds the averages back into each variable, which is what allows for the estimation of a constant term (which has the interpretation of the mean of the fixed effects, and which allows for prediction.  When I alter my code to copy those procedures, my SE's are equivalent up to the 3rd or 4th decimal point.</p>
"
"0.116796964356571","0.0725900877743741"," 67380","<p>My question is probably elementary, and I apologize for that. I am reading Kogan's ""Introduction to Clustering Large and High-Dimensional Data""; I am interested in understanding batch K-means and K-means and use it in $\operatorname{R}$. In the textbook it is stated that both algorithms need an initial choice of</p>

<ol>
<li>the number of clusters $K$</li>
<li>An initial partition of the given dataset</li>
</ol>

<p>Using such entries, the algorithms can perform the learning exercise. Kogan states that the initial partition is usually found using a Principal Direction Divisive Partitioning (PDDP) algorithm. </p>

<p>Looking at the K-means function <code>kmeans</code> in $\operatorname{R}$ I have noticed the absence of the initial partition as argument of the function itself. One can specify the number of clusters or a set of initial centers.</p>

<p>Moreover, the default K-mean algorithm used by <code>kmeans</code> is the one by  Hartigan and Wong (1979). Unfortunately I have no access to the original paper, and I could not run through the original code, searching for the initial partition.</p>

<p>My questions are:</p>

<ul>
<li>is there an initial partition choice hidden somewhere in <code>kmeans</code>? If yes, how is it chosen?</li>
<li>In absence of initial partition choice, how does <code>kmeans</code> begins to run (a high level overview would be great!)?</li>
</ul>

<p>I thank you all.</p>
"
"0.162865585496114","0.14171085778131"," 68345","<p>This question is related to my quest of clustering the sequences using mixture Markov modeling. </p>

<p>I have trouble understanding Dirichlet priors in the context of MAP-estimate (Mixture Markov Models). Namely, my priors end up being (much) larger than one.<br>
I have non-informative priors defined as following:</p>

<p>$$
p(\theta_n^{j}|a_n^{j})=\frac{\Gamma(\sum_{m=1}^{M}(a_{nm}^{j}+1))}{\prod_{m=1}^{M}\Gamma(a_{nm}^{j}+1)}*\prod_{m=1}^{M}(\theta_{nm}^{j})^{a_{nm}^{j}},
$$</p>

<p>where each $a_n^{j}$ is a <em>M</em>-vector with components $a_{nm}^{j}&gt;0$. <em>j</em> denotes the jth component in the mixture and <em>n</em> is the number of the row of the TPM ( transition probability matrix). Then I use the sum of log(Dirichlet Priors) across each row and each component. </p>

<p>The most confusing aspects are:</p>

<p>1) In all literature the Dirichlet prior formula is given as:</p>

<p>$$
p(\theta_n^{j}|a_n^{j})=\frac{\Gamma(\sum_{m=1}^{M}(a_{nm}^{j}))}{\prod_{m=1}^{M}\Gamma(a_{nm}^{j})}*\prod_{m=1}^{M}(\theta_{nm}^{j})^{a_{nm}^{j}-1},
$$</p>

<p>(notice the -1 in the exponent term). Is there possibly a typo in the article or can the -1 term be skipped? </p>

<p>2) The article sets $a_n^{j}$ equal to 10% of the corresponding relative frequencies of the TPM of the original counts across all sequences ( if our model has just 1 component). Then, let us consider such example:</p>

<p>there are 3 possible states in the Markov Chain, and the <em>n</em>-th row transition probabilities are 0.1, 0.8, 0.1. Let $a_n^j$ be equal to (0.01,0.03,0.06). Then, following the formula, my prior will be ( calculated in R) :</p>

<p>according to the first version of formula: <strong>1.961152</strong></p>

<p>according to the second version of the formula: <strong>245.144</strong></p>

<p>This has been computed by defining a function in R:</p>

<pre><code>    &gt; dirichletPrior&lt;-function(matrix_row,alpha_row){
        dirichl&lt;-((gamma(sum(alpha_row+1)))/(prod(gamma(alpha_row+1))))*prod(matrix_row^(alpha_row))
        dirichl
     }
</code></pre>

<p>The results seem a bit like a nonsense to me, and I do not understand where is my logic faulty. </p>

<p>3) Is ""Dirichlet prior"" a probability density function or a likelihood function? What would it mean if the result for each row is larger than 1? Are alpha multinomial parameters like I gave in the example, meaningful at all? What do they have in common with concentration parameter? </p>

<p>The article I have been referring to is located under the following link: http://www.cs.uoi.gr/~kblekas/papers/C19.pdfâ€Ž. </p>
"
"0.126155140059354","0.156812512046795"," 69046","<p>I have calculated log-likelihood distances between 50 sequences according to the Formula (1): </p>

<p>$$
D(X_i,X_j)= 1/2(\log p(X_i|Mod_j)+\log p(X_j|Mod_i)),  
$$
where $
p(X_i|Mod_j)
$ is the likelihood of sequence $X_i$ being produced by model $Mod_j$, where $Mod_j$ is a corresponding Markov model of the given $Seq_j$, defined by its Transition Probability Matrix and Start Probabilities Vector.  The measure is symmetrical as seen from the definition. To make the measure more ""legible"" and similar to the traditional measures, I compute distance$=(1-D)$ from formula (1). Thus, $D(X_i,X_i) = 0$ and the distance increases if the likelihood decreases. </p>

<p>Now, I have a 50x50 Distance Matrix.I have run a ""meaningfullness"" check, and it seemed ok for me - i.e. more similar sequences had smaller distance and very different ones had very large distance. The distances seemed to satisfy the triangle inequality. However, I have noticed that:</p>

<p>1)  the shorter sequences seem to be ""closer"" to all other sequences than longer ones. It seems that this distance measure is biased to favor short distances. </p>

<p>2) I have tried PAM-clustering with the distance matrix by converting my distance matrix to dist object in <code>R</code> by using as.dist(), and my results were very bad, even for 2 clusters or 49 ( max avg.silhouette width produced by <code>R</code> function pam was 0.28). With some numbers of clusters the avg.silhouette widths were even negative. </p>

<p>I am coming to conclusion that my way of computing medoids is invalid/conceptually wrong. What could be the problem? Can log-likelihood distance matrix be used with medoids clustering at all? </p>

<p>edit: I am including the heatmap of the distance matrix, where x- and y-axis represent sequences (1 through 50th). It looks strange to me but I cannot pinpoint what exactly doesn't feel right. </p>

<p><img src=""http://i.stack.imgur.com/RcSBc.png"" alt=""heatmap""></p>
"
"0.072835704072923","0.0452678730212593"," 70844","<p>I have a data set with 20,000 discussion forum threads, many only one or two posts, some up to 400-posts. I have 5,000 individuals who participated in these threads. I want to calculate the strength of the relationship between two people based on how many threads they have participated in for export to Gephi and clustering to see if I can find any clear groups - are there any pre-existing algorithms to do this?</p>

<p>I hacked up my own function in R, which looks at the length of the thread (smaller thread means stronger connection to other participants - too large threads are excluded), number of times posting (two people posting twice each in a thread are more strongly related), etc. It kind of works, but I'm sure I'm reinventing the wheel, there must be some existing algorithms/approaches etc? (I'm using R, but even a general algorithm is welcome). </p>
"
"0.0892051550175079","0.110883190643186"," 74495","<p>I am trying to use R to do Kmeans clustering and as most people I ran into the challenge of determining when to finish. I have 10,000 items and potentially 10 times of that down the road. My goal is to create a series of clusters with minimal size (e.g. 50 items per cluster) OR reasonably similar items. In other words, I don't want any of my output clusters to be too small (even if the items are quite different from each other), but I also don't mind if the clusters are too big as long as the items are similar enough.</p>

<p>I imagine I can use some kind of divisive hierarchical approach. I can start by building a small number of clusters and examine each cluster to determine if it needs to be split into more clusters. I can keep doing this till all clusters meet my stopping criteria.</p>

<p>I wonder if anyone knows good information on how other people do this? </p>
"
"0.162865585496114","0.182199674290256"," 77027","<p>For non-statisticians like me, it is very difficult to capture the idea of <code>VI</code> metric (variation of information) even after reading the relevant paper by Marina Melia ""<a href=""http://www.sciencedirect.com/science/article/pii/S0047259X06002016"">Comparing clusterings - An information based distance</a>"" (Journal of Multivariate Analysis, 2007). In fact, I am not familiar with many of the clusterings' terms out there.  </p>

<p>Below is a MWE and I would like to know what does the output mean in the different metrics used.  I have these two clusters in R and in the same order of id:  </p>

<pre><code>&gt; dput(a)
structure(c(4L, 3L, 4L, 4L, 4L, 4L, 4L, 3L, 4L, 4L, 3L, 3L, 3L, 
4L, 4L, 4L, 4L, 4L, 3L, 4L, 4L, 4L, 3L, 4L, 4L, 3L, 4L, 4L, 4L, 
1L, 1L, 4L, 4L, 4L, 4L, 4L, 4L, 3L, 4L, 4L, 3L, 4L, 4L, 2L, 2L, 
4L, 3L, 3L, 2L, 2L, 2L, 4L, 3L, 4L, 4L, 3L, 1L, 4L, 3L, 4L, 4L, 
4L, 3L, 4L, 4L, 4L, 4L, 2L, 2L, 2L, 4L, 3L, 4L, 4L, 4L, 4L, 4L, 
4L, 4L, 4L, 4L, 3L, 4L, 4L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 3L, 4L, 
4L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 3L, 4L, 4L, 4L, 4L, 2L, 2L, 4L
), .Label = c(""1"", ""2"", ""3"", ""4""), class = ""factor"")
&gt; dput(b)
structure(c(4L, 3L, 4L, 4L, 4L, 4L, 4L, 3L, 4L, 4L, 3L, 3L, 3L, 
4L, 4L, 3L, 4L, 4L, 3L, 4L, 4L, 4L, 3L, 4L, 4L, 3L, 4L, 4L, 4L, 
1L, 1L, 4L, 4L, 4L, 4L, 4L, 4L, 3L, 4L, 4L, 3L, 4L, 4L, 2L, 2L, 
4L, 3L, 3L, 2L, 2L, 2L, 4L, 3L, 4L, 4L, 3L, 1L, 4L, 3L, 4L, 4L, 
3L, 3L, 4L, 4L, 4L, 4L, 2L, 2L, 2L, 4L, 3L, 3L, 4L, 4L, 4L, 4L, 
4L, 4L, 4L, 4L, 3L, 4L, 4L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 3L, 4L, 
4L, 3L, 4L, 4L, 4L, 4L, 4L, 3L, 3L, 4L, 4L, 4L, 4L, 2L, 2L, 4L
), .Label = c(""1"", ""2"", ""3"", ""4""), class = ""factor"")
</code></pre>

<p>Now doing comparisons based on the <code>VI</code> as well as other metrics / indices and in chronological order of their appearance in literature.  </p>

<pre><code>library(igraph)
  # Normalized Mutual Information (NMI) measure 2005:
compare(a, b, method = c(""nmi"")) 
[1] 0.8673525
  # Variation of Information (VI) metric 2003:
compare(a, b, method = c(""vi"")) 
[1] 0.2451685
  # Jaccard Index 2002:
clusteval::cluster_similarity(a, b, similarity = c(""jaccard""), method = ""independence"") 
[1] 0.8800522
  # van Dongen S metric 2000:
compare(a, b, method = c(""split.join"")) 
[1] 8
  # Adjusted Rand Index 1985:
compare(a, b, method = c(""adjusted.rand"")) 
[1] 0.8750403
  # Rand Index 1971:
compare(a, b, method = c(""rand"")) 
[1] 0.9374788
</code></pre>

<p>As you can see, the <code>VI</code> value was different from all the others.  </p>

<ul>
<li>What does this value tell (and how is it related to the figure below)? </li>
<li>What are the guidelines for considering this value low or high? </li>
<li>Are there any guidelines defined?  </li>
</ul>

<p>Maybe experts in the field can provide some sensible descriptions for laymen like me when trying to report such results. I would really appreciate if someone would provide also guidelines for other metrics as well (when to consider the value is large or small, i.e., in relation to a similarity between two clusters).  </p>

<p>I have read related CV threads <a href=""http://stats.stackexchange.com/questions/24961/comparing-clusterings-rand-index-vs-variation-of-information"">here</a> and <a href=""http://stats.stackexchange.com/questions/15548/validation-of-clustering-results/15600?noredirect=1#comment150263_15600"">here</a>, but still couldn't grasp the intuition behind <code>VI</code>. Can someone explain this in plain English?  </p>

<p>The below figure is figure 2 from the above mentioned paper about <code>VI</code>.  </p>

<p><img src=""http://i.stack.imgur.com/OnCzM.png"" alt=""enter image description here""></p>
"
"0.126155140059354","0.104541674697863"," 77660","<p>I have 11 scale parameters for each of 218 observations belonging to subjects, I did standardized PCA to reduce dimensionality of the data and found two meaningful components. Using Euclidean distances this was followed by cluster analysis of these two components (explaining about 75% of the variance) with bottom-up approach using the hierarchical agglomerative clustering (HAC) by <code>FactoMineR</code> R package and Ward's linkage method.
The optimal number of clusters was 4 as suggested by the package based on minimizing the ratio of two successive partition inter-clusters inertia gains.<br>
This is just the number of observations per cluster:  </p>

<pre><code>&gt; table(df$clust)

  1   2   3   4 
  6  21  46 145
</code></pre>

<p>These 4 clusters turned out to be clinically important and subjects with cluster 1 were severely affected by disease. Cluster 4 were non-reactive subjects, Cluster 3 showed some reaction, and finally cluster 2 was like a special entity protected from disease. I don't know if these clusters can assume some kind of ordinal ranking or not. It is difficult to judge from the theoretical point of view related to the field, but I can say that cluster 4->3->1 is somehow showing some direction, and hence could be regarded as ordinal, on the other hand, cluster 2 is a little bit different but very important as subjects with this clusters were protected from disease. So, I am really confused as whether to consider these 4 clusters ordinal or not.  </p>

<p>Suppose that I have another set of 11 new readings of the scale parameters for one subject as new data, what statistical analysis would be useful to predict the membership of this subject to those 4 clusters? Could you please refer to a similar example with R code if possible? that would be greatly appreciated.  </p>

<p>Providing a professional answer would be highly esteemed, but also recommending some books using R code would also be encouraged, as I am searching for such a book that covers this topic thoroughly, many books are out there but it is difficult to judge which one would do the job. May be someone, has more experience with this kind of problems and can give a word of advise here.  </p>
"
"0.126155140059354","0.130677093372329"," 77672","<p>Suppose I have 50 scale parameters, these are all genes measured for one sample from a subject at the clinic, after data reduction by PCA, two meaningful components were extracted. This was followed by cluster analysis and turned out to be 4 meaningful clusters of subjects based on the two components of these 50 genes.  </p>

<p>Since investigating 50 genes for one subject would be costy, one would like to reduce that number so that the same clustering pattern can still be obtained but with minimal costs possible ( there should be some measures here to say acceptable clustering or not, I wonder what kind of measures would fit this case though).  </p>

<p>Of course, the more genes investigated, the more information gained, but there should be some measure to tell when to stop wasting more money when the same result is <em>satisfactorily</em> achievable will less number of genes.  </p>

<p>Is there any R package that already implemented this approach? what would be the statistical approach in this case to select the most important genes that would preserve the clustering pattern? what criteria to be used in order to reach the minimum clustering pattern?  </p>
"
"0.162865585496114","0.161955266035783"," 78148","<p>I have biological time series (9 years long) of the biomass of species which logically exhibit a seasonal pattern. I would like to cluster them into a few groups based on their typical seasonal evolution (e.g. spring vs. summer species). 
To do so, I was advised to use Fourier transform in order to decompose their signal into <code>N</code> harmonics (e.g. 3: annual, bi-annual and tri-annual seasonal cycles) and use the amplitudes and phases of these in a Principal Components Analysis (PCA; which would work as the harmonics are orthogonal/uncorrelated).</p>

<p>I know there are already some similar subjects in this Forum, yet some aspects remain unclear to me. My questions are:</p>

<p>(1) When I reconstruct the time evolution from the <code>N</code> first harmonics computed from the Discrete Fourier Transform (DFT), the explained variability of the original signal (the RÂ² of the linear model between recomposed signal and the original data) is sometimes only 0.40 (<code>N=3</code>) or 0.60 (<code>N=5</code>). In your experience, does it mean the data are not suited for this approach, does that invalidate the approach? Is there more pre-processing I could do to fix that (e.g., smoothing the signals, â€¦)? Some species exhibit sudden increases spaced by total absence, and I wonder if this doesnâ€™t call for the need of higher frequency harmonics; should I expect difficulties there and how to tackle them?</p>

<p>(2) Beside DFT which appears limited here, I considered using continuous Fourier Transform through a Fast Fourier Transform (FFT) algorithm and working on the power spectrum of each time series. I wonder if this could allow me to select <code>N'</code> so-called â€œharmonicsâ€ by selecting the <code>N'</code> highest peaks in the periodogram and then calculating the corresponding amplitude and phase to be used in a following PCA... Does that make sense? 
How to concretely use the info given by a FFT algorithm in R (such as <code>fft()</code> or <code>spec.pgram()</code>) in order to run a subsequent PCA (or any other clustering method)? [any R code snippet would be very welcome]</p>

<p>(3) How to reconstruct the signal from selected harmonics in the continuous case (FFT)? I can easily do this in the DFT case, but I am stupidly blocked in the continuous caseâ€¦ Any R code snippet is of course very welcome.</p>

<p>Any help regarding these questions would be very appreciated.
Links toward concrete examples, especially with associated R code, would be very helpful too (as well as method name or keywords).
Thank you. </p>

<p>PS: in case it is useful: The time series are of equal length and pre-processed to have uniform sampling intervals; stationarity may be assumed; no long-term trend is in the way.
I divided the time series in 52 equally-spaced observations per year (i.e., 468 observations over the 9 years).</p>
"
"0.0892051550175079","0.110883190643186"," 78322","<p>I have two parts of a multidimensional data set, let's call them <code>train</code> and <code>test</code>. And I want to built a model based on the train data set and then validate it on the test data set.
The number of clusters is known.</p>

<p>I tried to apply k-means clustering in R and I got an object that contains the centers of clusters:</p>

<pre><code>kClust &lt;- kmeans(train, centers=N, nstart=M)
</code></pre>

<p>Is there a function in R that takes the centers of clusters that were found and assigns clusters to my test data set?</p>

<p>What are the other methods/algorithms that I can try?</p>
"
"0.072835704072923","0.0905357460425185"," 80004","<p>I'm trying to assess the uncertainty in hierarchical cluster analysis. It is a dataset composed of 409 observations and 27 variables (with a value ranging form 0 to 100). The dataset represents immunohistochemical scores in a gastrointestinal cancers.</p>

<p>A meaninful clustering of observations and markers is observed with Pearson uncentered distance and average linkage.</p>

<pre><code>hc &lt;- hclust(Dist(t(imputedMatrix), method=""pearson""), method=""average"")
hr &lt;- hclust(Dist(imputedMatrix, method=""pearson""), method=""average"")
heatmap.2(imputedMatrix, Rowv=as.dendrogram(hr), Colv=as.dendrogram(hc),   col=greenred(100), scale=""none"", ColSideColors=patientcolors, density.info=""none"", trace=""none"")

pv2 &lt;- pvclust(imputedMatrix, method.dist=""uncentered"", method.hclust=""average"", nboot=10000)
plot(pv2, hang=-1)
pvrect(pv2, alpha=0.95)

clsig &lt;- unlist(pvpick(pv2, alpha=0.90, pv=""au"", type=""geq"", max.only=TRUE)$clusters) 
    dend_colored &lt;- dendrapply(as.dendrogram(pv2$hclust), dendroCol, keys=clsig,     xPar=""edgePar"", bgr=""black"", fgr=""red"", pch=20)
heatmap.2(imputedMatrix, Rowv=as.dendrogram(hr), Colv=dend_colored, col=greenred(1
</code></pre>

<p>However, when using pvclust to assess their uncertainty, many small low-levels subclusters are highlighted as significant, but not any higher level one. Also, a group of tumours (to the right in the plots) is indeed a control group that should be clearly distinguished at the highest level from the other ones. pvclust shows even there the same kind of pattern. </p>

<p>Indeed the clusters of interest are the magenta, green and red in the color bar. Does this pvclust results support their existance (versus a clustering artefact by chance)? </p>

<p>How could these pvclust results be interpreted? Maybe I am using the tool in a wrong way? Or the wrong tool for this kind of data?</p>

<p>Thank you very much in advance.</p>

<p><img src=""http://i.stack.imgur.com/nedQr.png"" alt=""pvclust dendrogram"">
<img src=""http://i.stack.imgur.com/KoaHW.png"" alt=""hclust and pvclust, the latter in red in the dendrogram""></p>
"
"0.154507860787381","0.149376359921712"," 81396","<p>I'm trying to compile a list of clustering algorithms that are:</p>

<ol>
<li>Implemented in R</li>
<li>Operate on sparse <em>data matrices</em> (not (dis)similarity matrices), such as those created by the <a href=""http://www.rdocumentation.org/packages/Matrix/functions/sparseMatrix"" rel=""nofollow"">sparseMatrix</a> function.</li>
</ol>

<p>There are several other questions on CV that discuss this concept, but none of them link to R packages that can operate directly on sparse matrices:</p>

<ol>
<li><a href=""http://stats.stackexchange.com/questions/10411/clustering-large-and-sparse-datasets"">Clustering large and sparse datasets</a></li>
<li><a href=""http://stats.stackexchange.com/questions/44640/clustering-high-dimensional-sparse-binary-data"">Clustering high-dimensional sparse binary data</a></li>
<li><a href=""http://stats.stackexchange.com/questions/10122/looking-for-sparse-and-high-dimensional-clustering-implementation"">Looking for sparse and high-dimensional clustering implementation</a></li>
<li><a href=""http://stats.stackexchange.com/questions/9778/space-efficient-clustering/"">Space-efficient clustering</a></li>
</ol>

<p>So far, I've found exactly one function in R that can cluster sparse matrices:</p>

<h1><a href=""http://www.rdocumentation.org/packages/skmeans/functions/skmeans"" rel=""nofollow"">skmeans</a>: spherical kmeans</h1>

<p>From the <a href=""http://cran.r-project.org/web/packages/skmeans/index.html"" rel=""nofollow"">skmeans package</a>. kmeans using <a href=""http://en.wikipedia.org/wiki/Cosine_similarity"" rel=""nofollow"">cosine distance</a>.  Operates on dgTMatrix objects. Provides an interface to a genetic k-means algorithm, pclust, CLUTO, gmeans, and kmndirs.</p>

<p>Example:</p>

<pre><code>library(Matrix)
set.seed(42)

nrow &lt;- 1000
ncol &lt;- 10000
i &lt;- rep(1:nrow, sample(5:100, nrow, replace=TRUE))
nnz &lt;- length(i)
M1 &lt;- sparseMatrix(i = i,
                   j = sample(ncol, nnz, replace = TRUE),
                   x = sample(0:1 , nnz, replace = TRUE), 
                   dims = c(nrow, ncol))
M1 &lt;- M1[rowSums(M1) != 0, colSums(M1) != 0]

library(skmeans)
library(cluster)
clust_sk &lt;- skmeans(M1, 10, method='pclust', control=list(verbose=TRUE))
summary(silhouette(clust_sk))
</code></pre>

<hr>

<p>The following algorithms get honerable mentions: they're not quite clustering algorithms, but operate on sparse matrices.</p>

<h1><a href=""http://www.rdocumentation.org/packages/arules/functions/apriori"" rel=""nofollow"">apriori</a>: association rules mining</h1>

<p>From the <a href=""http://cran.r-project.org/web/packages/arules/index.html"" rel=""nofollow"">arules package</a>. Operates on ""transactions"" objects, which can be coerced from ngCMatrix objects.  Can be used to make recommendations.</p>

<p>example:</p>

<pre><code>library(arules)
M1_trans &lt;- as(as(t(M1), 'ngCMatrix'), 'transactions')
rules &lt;- apriori(M1_trans, parameter = 
list(supp = 0.01, conf = 0.01, target = ""rules""))
summary(rules)
</code></pre>

<h1><a href=""http://www.rdocumentation.org/packages/irlba/functions/irlba"" rel=""nofollow"">irlba</a>:  sparse SVD</h1>

<p>From the <a href=""http://cran.r-project.org/web/packages/irlba/index.html"" rel=""nofollow"">irlba package</a>.  Does SVD on sparse matrices.  Can be used to reduced the dimensionality of sparse matrices prior to clustering with traditional R packages.</p>

<p>example:</p>

<pre><code>library(irlba)
s &lt;- irlba(M1, nu = 0, nv=10)
M1_reduced &lt;- as.matrix(M1 %*% s$v)
    clust_kmeans &lt;- kmeans(M1, 10)
    summary(silhouette(clust_kmeans$cluster, dist(M1_reduced)))
</code></pre>

<h1><a href=""http://cran.r-project.org/web/packages/apcluster/index.html"" rel=""nofollow"">apcluster</a>:  Affinity Propagation Clustering</h1>

<pre><code>library(apcluster)
sim &lt;- crossprod(M1)
sim &lt;- sim / sqrt(sim)
clust_ap &lt;- apcluster(sim) #Takes a while
</code></pre>

<p>What other functions are out there?</p>
"
"0.353160824656872","0.356674165527353"," 81727","<p>I have 4 clusters (see plot below) extracted from data of medical samples <code>N=218</code> measured for 11 genes/predictors <code>P=11</code> by this method: first PCA analysis validated to have 2 important PCs that explained 75% of the data, then different clustering algorithms, distances, linkages (in hierarchical approach only) were compared: the majority support the presence of 4 distinct clusters. Taking the scientific hypothesis into consideration, clusters out of $K$-means algorithm were found the most plausible and were the most balanced clusters too: class #1 <code>n=12</code>, class #2 <code>n=21</code>, class #3 <code>n=79</code>, and class #4 <code>n=106</code>.  </p>

<p>Projecting the observations on plane 1-2 component scores, revealed the below scatter plot with each cluster color coded.
<img src=""http://i.stack.imgur.com/09sTF.png"" alt=""enter image description here""></p>

<p><strong>The aim is to find a global optimum classifier using R after doing PLS to the data.</strong>  </p>

<p>Knowing that these 4 clusters were actually the product of latent PCA components, it was natural to think of PCR as a next step to predict classes, but that approach turned out to be sub-optimal for two reasons: first, results do not related to probibilities (0-1), second, it does not relate well with the classes as the outcome variable. As many know, this would be better solved with PLS-DA method + softmax to find probabilities of class (0-1).  </p>

<p>However, many reports confirm the superiority of using LDA as a second step using the <em>scores</em> of PLS, given that same standardization parameters (mean, sd) be used of the training set on the holdout-test set, even using the PLS projections out of the training set on the test set in order to get the <em>scores</em> which would be the <em>actual</em> holdout-test set to validate the classifier in question.  </p>

<p>From the methodology point of view, this path is potentially encompassed with many dangers and subtle errors when one is un/misinformed about the tools used in context.  </p>

<p>The <code>caret</code> package which is unique of its kind given the consistent infrastructure it provides to train and validate an array of different models making use of <em>de facto</em> standard respective R-packages, and hence <code>caret</code> promotes itself as a road map to a validated modeling leveraging off R rich libraries. As heart to blood vessels, so <code>caret</code> to other packages in my opinion. That being said, unwatchful playing with the heart could cost you dearly, and might lead also to a stand-still or a <em>model-arrest</em> of your data. R is free, many free books out there, but buying <code>caret</code> only book paid off, i.e. <code>Applied Predictive Modeling</code>. The help files, companion website (very appealing btw), are great resources but they won't substitute the text inside the book IMHO. However, in the book, I couldn't find a direct answer to the PLS-Classifier two step method amid others. The potential with <code>caret</code> is immense, thanks to Max Kuhn and his colleagues, that primarily encouraged me to post this question.  </p>

<p>Back to the example above and the methodology of wish:<br>
<strong>Data splitting:</strong>  </p>

<p>Training set (77% <code>n=168</code>) for 10K-cross-validation: tuning (model-specific parameters, feature selection <code>P=11</code>, and cost to deal with imbalanced clusters). For CV this would be roughly <code>n=150</code> for fitting the model using differnt parameters of wish and 'n=17<code>for evaluation of parameters (I would call the</code>n=17' the CV-test to avoid confusion later on). Repetition = 5, so this will make 10 folds x 5 times = 50 training folds (<code>n=150</code> each) and 50 CV-test folds (<code>n=17</code> each). Holdout-test set (23% <code>n=50</code>).  </p>

<p><strong>Q1</strong> I know that one can do parameters' tuning along with feature selection at the same time (i.e., parallel), but how to evaluate the cost/weights if one would like to evaluate cost-sensitive models (SVM, CART, C5.0) using the PLS scores to counteract class imbalance?    </p>

<p><strong>Q2</strong> What is the alternative approach when reserving a separate data set for cost evaluation (i.e., <code>evaluation set</code>), as recommended, is not possible given the small sample size in this case? can one do tuning of model parameter, feature selection, and cost for imbalance all three at the same time? if not what is the best practice in this case?  </p>

<p><strong>Q3</strong> Given the small sample size, is bootstrapping preferred to CV? if yes how would it be implemented to do exhaustive tuning like above for the PLS scores?    </p>

<p><strong>Q4</strong> Given the imbalance above, is there a way to ensure that each CV training fold would include the minimum number of <code>hard</code> class(es) in order to have good estimation on the CV-test fold? is there any argument to pass to ensure presence of the small classes each time fold would be generated?  </p>

<p><strong>PLS special notes</strong>  </p>

<p>This is the approach in my mind (please correct me if I am missing something somewhere during the course):  </p>

<p>In each CV iteration on the many CV-traning folds, there should be a unique PLS projection matrix for each iteration that would be used in the next second step of getting PLS scores for the the respective CV-test set inheriting the same standardization parameters (<code>mean</code>, <code>sd</code>), this means that two things would be inherited; the PLS projection and the standardization parameters (mean and sd) in order to apply them to the CV-test folds, this way, given the example here, 50 values would be returned hoping to reach the best parameter in question. One complication though, there should be an argument to specify the desired number of PLS components to retain and to be used in calculation of scores out of each CV-test fold (better to be pre-defined in a previous tuning step may be). My expectation, is that after deciding on the best model, there should be a way to get the PLS projection matrix for the whole training set (i.e.<code>n=168</code>) along with (<code>mean</code>, <code>sd</code>) to apply them on the holdout-test to validate the best model. So in total, there would be 50 different PLS projection matrices, means, sds from CV step and 1 extra frothe whole training set, am I right?<br>
Feature selection in this method would entail two things: predictors space and the PLS components space.</p>

<p><strong>Q5</strong> How to perform these two selections (predictor and PLS component) in <code>caret</code>? this is because feature selection here is different than otherwise since here we deal with scores rather than the observations themselves to determine best predictors that to construct the PLS components.</p>

<p><strong>Note:</strong> When one is happy with the best final model, it would be recommended to fit the model on the whole data set <code>n=218</code> to get the correct estimates withe the least uncertainty.  </p>

<p>A similar procedure is implemented in <code>caret::train()</code> function that can be fed with <code>preProc</code> argument to specify the type of desired pre-processing of data (most are mentioned in the help system but I couldn't find <strong>PLS</strong> among them, better if with an argument to specify the desired components similar to PCA pre-processing). I am aware of the fact, that inheriting pre-processing parameters to holdout test set and to CV-test, can only be performed using the <code>predict.train()</code> function, as opposed to calling the generic <code>predict()</code> function to the <code>$finalModel</code> that won't inherit pre-processing parameters.     </p>

<p><strong>Q6</strong> How to implement this strategy (if correctly described) to train and validate the two-step PLS-[classifier] methodology using PLS scores subspace instead of observations making use of <code>caret</code> infrastructure?  </p>

<p>Thanks in advance.</p>
"
"0.103005240524921","0.128036879932896"," 86318","<p>I have a semi-small matrix of <strong>binary features</strong> of dimension 250k x 100. Each row is a user and the columns are binary ""tags"" of some user behavior e.g. ""likes_cats"".</p>

<pre><code>user  1   2   3   4   5  ...
-------------------------
A     1   0   1   0   1
B     0   1   0   1   0
C     1   0   0   1   0
</code></pre>

<p>I would like to fit the users into 5-10 clusters and analyze the loadings to see if I can interpret groups of user behavior. There appears to be quite a few approaches to fitting clusters on binary data - what do we think might be the best strategy for this data?</p>

<ul>
<li><p>PCA</p></li>
<li><p>Making a <em>Jaccard Similarity</em> matrix, fitting a hierarchical cluster and then using the top ""nodes"".</p></li>
<li><p>K-medians</p></li>
<li><p>K-medoids</p></li>
<li><p><a href=""http://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Clustering/Proximus"">Proximus</a>?</p></li>
<li><p>Agnes</p></li>
</ul>

<p>So far I've had some success with using hierarchical clustering but I'm really not sure it's the best way to go..</p>

<pre><code>tags = read.csv(""~/tags.csv"")
d = dist(tags, method = ""binary"")
hc = hclust(d, method=""ward"")
plot(hc)
cluster.means = aggregate(tags,by=list(cutree(hc, k = 6)), mean)
</code></pre>

<p><img src=""http://i.stack.imgur.com/cyT6t.png"" alt=""enter image description here""></p>
"
"0.170814867130737","0.193022859120546"," 91134","<p>consider the following example data:</p>

<pre><code>df1 &lt;- data.frame(customer=c(rep(""customer1"",5),rep(""customer2"",10),rep(""customer3"",7)),
                  money_spent=sample(22))

df2 &lt;- data.frame(customer=c(""customer1"",""customer2"",""customer3""),
                  origin=c(""US"",""US"",""UK""),
                  industry_sector=c(""IS1"",""IS2"",""IS3""),
                  currency=c(""USD"",""USD"",""GBP""))
</code></pre>

<p>My actual data consists of about 200000 rows and I would like to examine it in terms of, for instance, do customers from the US spent more money compared to customers from other countries. I would also like to see whether the amount of money spent depends on the industry sector and so on. I have some more explanatory variables apart from origin, industry sector and currency which I would like to look into. Also the number of records for the customers differ so that it might make sense to average the money spent for each customer.</p>

<p>I am not sure about how to best analyse this data. I first thought of cluster analysis, in particular, hierarchical clustering but am not sure whether it can be applied to such data and, in particular, how to structure the data to be put into the function. The R function <code>hclust</code> takes a matrix as the input but how would I structure such a matrix in terms of my data? Could k-means clustering be a better alternative? </p>

<p>Another approach would probably to analyse this data using boxplots and an one-way ANOVA approach to see whether ""money spent"" differs between different countries or industry sectors. However, this approach does not test whether the variables are dependend on each other. To look into this, I have been advised to apply a decision tree first and then do some statistical significance analysis. However, from what I have read so far I cannot see how decision trees can help me to detect variable dependencies.</p>

<p>So, I am wondering whether there any other/better techniques/functions out there which are more suitable for such data? Maybe a time series analysis is more appropriate since we have also recorded the dates when customers spent money.</p>
"
"0.163542784198764","0.166324785964779"," 91348","<p>I am working on data analysis.</p>

<p>Given a group of data vectors, each of them has the same dimension. Each element in a vector is a floating point number. </p>

<pre><code>V1 [  ,   ,   , â€¦ ] 
V2[  ,   ,   , â€¦ ] 
...
Vn [  ,   ,   , â€¦ ] 
</code></pre>

<p>Suppose that each vector has M numbers. M can be 10000.</p>

<p>n can be 200. </p>

<p>I need to find out how to partition the n vectors into sub-groups such that each vector in one subgroup can be represented by a basic vector in the subgroup. </p>

<p>For example, </p>

<p>W = union of V1, V2, V3 â€¦ Vn</p>

<p>Find subgroup i, j, â€¦ t :</p>

<pre><code>Gi = [  V1, V6, V3, V5, â€¦ , Vx ]
Gj = [V22, V11, V56, V45, â€¦ , Vy]
â€¦
Gt = [V78, V90, V9, V12, â€¦ , Vz]
</code></pre>

<p>Such that :</p>

<p>Union of Gi , Gj, â€¦ , Gt is equal to W and there is no overlap among  all Gi , Gj, â€¦ , Gt. </p>

<p>Also , each subgroup has a basic vector that has strong correlation with all other element vector in the subgroup. For example, in Gi, we may have vector Vx as the basic vector such that all other vectors have <strong>strong (linear) correlation</strong>  with Vx. <strong>Here, we measure the linear correlation betwwen two vectors not two data points.</strong> </p>

<p>Moreover, we need to minimize the number of the subgroups, here, it is  "" t "" . It means that given 200 vectors ( n = 200), we prefer a subgroup G1, G2, â€¦, Gt, and t is minimized. For example, we prefer t = 5 over t = 6. if t is more than 10, it may not be useful. </p>

<p>My questions:
What kind of knowledge domain this problem belongs to ? </p>

<p>Is it a clustering analysis ? But, in cluster analysis, one data point is a number, but, here one data point  is a vector.</p>

<p>Are there some statistics models or algorithm can be used to do this kind of analysis ?  Are there some software tools or packages that solve this problem ? </p>

<p>If my questions are not a good fit for this forum, please tell me where I should post it. </p>

<p>R packages do the clustering for data points not for data vector by correlation.</p>

<p>Any help would be appreciated. </p>
"
"0.126155140059354","0.156812512046795"," 92329","<p>I'm a little new to data mining and would definitely appreciate some tips.
I'm using clustering algorithms looking for possible grouping in some variables described below.
I've been using the Excel data mining add-in which connects to SSAS and uses the EM algorithm by default.  I'm also using R, so far with the Kmeans algorithm.</p>

<p>I have two independent variables X, and G which are integers,
and three dependent variables A, B, C which are related to X and G by the equations below:</p>

<p>$$A=100\left(\frac{2G\cdot.06+K}{2G\cdot.4+2\cdot.06+K+X}\right)$$</p>

<p>$$B=100\left(\frac X{2G\cdot.4+2G\cdot0.06+K+X}\right)$$</p>

<p>C=100-((((2xG)x(0.06)+K)/((2xG)x(.4)+(2xG)x(0.06)+K+X)))x100 + (X/((2xG)x(.4)+(2xG)x(0.06)+K+X))X100)</p>

<p>where K is an arbitrary constant which varies.
Most of the data follows the formulas above with some variation.</p>

<p>I'm using clustering algorithms to look for groupings in the A,B, C variables.
Any advice on strategies for looking for clusters, and how to tell when I've been successful would be greatly appreciated.</p>
"
"0.11516335992622","0.143149583578467"," 92985","<p>Recently I have come across usage of cluster plot, which combines k-mean clustering along with PCA. The plot shows different clusters plotted using first two PCs. I have checked some of the threads (<a href=""http://stats.stackexchange.com/questions/31083/how-to-produce-a-pretty-plot-of-the-results-of-k-means-cluster-analysis"">here</a> and <a href=""http://www.dataminingblog.com/combining-pca-and-k-means/"" rel=""nofollow"">here</a>) regarding the usage. </p>

<p><strong>I want to know, during generating a cluster plot, does the data is clustered first and then PCA is done, or the reverse way (PCA followed by k-mean clustering)?</strong></p>

<p>Because the second link says PCA is done followed clustering. But in the first link where an example is shown to generate a cluster plot, data is clustered first and then the cluster plot is generated. </p>

<p>Regarding interpretation, does the plot has to be interpreted as the number of clusters generated or are there any extra points to interpret?</p>
"
"0.072835704072923","0.0905357460425185"," 93815","<p>I have some experiences with time series modelling, in the form of simple ARIMA models and so on. Now I have some data that exhibits volatility clustering, and I would like to try to start with fitting a GARCH (1,1) model on the data. </p>

<p>I have a data series and a number of variables I think influence it. So in basic regression terms, it looks like: </p>

<p>$$
y_t = \alpha + \beta_1 x_{t1} + \beta_2 x_{t2} + \epsilon_t .
$$</p>

<p>But I am at a complete loss at how to implement this into a GARCH (1,1) - model? I've looked at the <code>rugarch</code>-package and the <code>fGarch</code>-package in <code>R</code>, but I haven't been able to do anything meaningful besides the examples one can find on the internet. </p>
"
"0.145671408145846","0.158437555574407"," 95844","<p>Given the following data frame:</p>

<pre><code>df &lt;- data.frame(x1 = c(26, 28, 19, 27, 23, 31, 22, 1, 2, 1, 1, 1),
                 x2 = c(5, 5, 7, 5, 7, 4, 2, 0, 0, 0, 0, 1),
                 x3 = c(8, 6, 5, 7, 5, 9, 5, 1, 0, 1, 0, 1),
                 x4 = c(8, 5, 3, 8, 1, 3, 4, 0, 0, 1, 0, 0),
                 x5 = c(1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0),
                 x6 = c(2, 3, 1, 0, 1, 1, 3, 37, 49, 39, 28, 30))
</code></pre>

<p>Such that</p>

<pre><code>&gt; df
   x1 x2 x3 x4 x5 x6
1  26  5  8  8  1  2
2  28  5  6  5  1  3
3  19  7  5  3  1  1
4  27  5  7  8  1  0
5  23  7  5  1  1  1
6  31  4  9  3  0  1
7  22  2  5  4  1  3
8   1  0  1  0  0 37
9   2  0  0  0  0 49
10  1  0  1  1  0 39
11  1  0  0  0  0 28
12  1  1  1  0  0 30
</code></pre>

<p>I would like to group these 12 individuals using hierarchical clusters, and using the correlation as the distance measure. So this is what I did:</p>

<pre><code>clus &lt;- hcluster(df, method = 'corr')
</code></pre>

<p>And this is the plot of <code>clus</code>:</p>

<p><img src=""http://i.stack.imgur.com/ALfbr.png"" alt=""dendogram""></p>

<p>This <code>df</code> is actually one of 69 cases I'm doing cluster analysis on. To come up with a cutoff point, I have looked at several dendograms and played around with the <code>h</code> parameter in <code>cutree</code> until I was satisfied with a result that made sense for most cases. That number was <code>k = .5</code>. So this is the grouping we've ended up with afterwards:</p>

<pre><code>&gt; data.frame(df, cluster = cutree(clus, h = .5))
   x1 x2 x3 x4 x5 x6 cluster
1  26  5  8  8  1  2       1
2  28  5  6  5  1  3       1
3  19  7  5  3  1  1       1
4  27  5  7  8  1  0       1
5  23  7  5  1  1  1       1
6  31  4  9  3  0  1       1
7  22  2  5  4  1  3       1
8   1  0  1  0  0 37       2
9   2  0  0  0  0 49       2
10  1  0  1  1  0 39       2
11  1  0  0  0  0 28       2
12  1  1  1  0  0 30       2
</code></pre>

<p>However, I am having trouble interpreting the .5 cutoff in this case. I've taken a look around the Internet, including the help pages <code>?hcluster</code>, <code>?hclust</code> and <code>?cutree</code>, but with no success. The farthest I've become to understanding the process is by doing this:</p>

<p>First, I take a look at how the merging was made:</p>

<pre><code>&gt; clus$merge
      [,1] [,2]
 [1,]   -9  -11
 [2,]   -8  -10
 [3,]    1    2
 [4,]  -12    3
 [5,]   -1   -4
 [6,]   -3   -5
 [7,]   -2   -7
 [8,]   -6    7
 [9,]    5    8
[10,]    6    9
[11,]    4   10
</code></pre>

<p>Which means everything started by joining observations 9 and 11, then observations 8 and 10, then steps 1 and 2 (i.e., joining 9, 11, 8 and 10), etc. Reading about the <code>merge</code> value of <code>hcluster</code> helps understand the matrix above.</p>

<p>Now I take a look at each step's height:</p>

<pre><code>&gt; clus$height
[1] 1.284794e-05 3.423587e-04 7.856873e-04 1.107160e-03 3.186764e-03 6.463286e-03 
    6.746793e-03 1.539053e-02 3.060367e-02 6.125852e-02 1.381041e+00
&gt; clus$height &gt; .5
[1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE
</code></pre>

<p>Which means that clustering stopped only in the final step, when the height finally goes above .5 (as the Dendogram had already pointed, BTW).</p>

<p>Now, here is my question: <strong>how do I interpret the heights?</strong> Is it the ""remainder of the correlation coefficient"" (please don't have a heart attack)? I can reproduce the height of the first step (joining of observations 9 and 11) like so:</p>

<pre><code>&gt; 1 - cor(as.numeric(df[9, ]), as.numeric(df[11, ]))
[1] 1.284794e-05
</code></pre>

<p>And also for the following step, that joins observations 8 and 10:</p>

<pre><code>&gt; 1 - cor(as.numeric(df[8, ]), as.numeric(df[10, ]))
[1] 0.0003423587
</code></pre>

<p>But the next step involves joining those 4 observations, and I don't know:</p>

<ol>
<li>The correct way of calculating this step's height</li>
<li>What each of those heights actually means.</li>
</ol>
"
"0.11516335992622","0.0858897501470802"," 96946","<p>I am reading a code for a Bayesian clustering method. Both prior and the likelihood are normally distributed. If I understood correctly, such cases are called ""conjugate priors""</p>

<p>My question is about calculating the posterior mean and variance. So it is implemented in the code as following</p>

<pre><code>d = nrow (data.i)##number of attributes
n = ncol (data.i)##number of replication for each attribute
Smatrix = matrix (1, nrow=d, ncol=d)##correlation of attributes
Imatrix = Smatrix - 1
diag (Imatrix) = 1

prior.precision = solve (sdWICluster^2 * Smatrix + sdTSampling^2 * Imatrix) #inverting the prior correlation matrix (prior.precision)
prior.mean = cluster.mean # mean of each clusters

sample.precision = sdResidual^(-2) * Imatrix
sample.mean = apply (data.i, 1, mean)#mean for each cluster

post.cov = solve (as.matrix(prior.precision + n*sample.precision)) # posterior covariance matrix
post.mean = as.vector (post.cov %*% (prior.precision %*% prior.mean + n*sample.precision %*% sample.mean)) # posterior of the mean
</code></pre>

<p>it seems the code has take the following formula, </p>

<p>$\mu_{po} = C_{po}\times((\mu_{pr} \times \tau_{pr})+(n\times\tau_{li}\times\mu_{li}))$</p>

<p>$C_{po} = \tau_{pr} + n\times\tau_{li}$</p>

<p>As I said, this is the case of conjugates of normal distributions with unknown mean but variance known; however, it does not fit to the formula I have from <a href=""http://www.people.fas.harvard.edu/~plam/teaching/methods/conjugacy/conjugacy_print.pdf"" rel=""nofollow"">here</a>. (or may be it does by my eye is not able to catch). I appreciate if someone make some comments about the code and the formula</p>
"
"NaN","NaN"," 99794","<p>I used <code>kmeans</code> command on my data-frame (as suggested in ""R and Data Mining: Examples and Case studies""). Now my data is clustered into x number of cluster.<br>
What they don't tell you in this book is what to do next.<br>
How can I get <code>characteristics</code> specific for <code>cluster 1</code>.<br>
Or how does <code>cluster 2</code> differ from <code>cluster 3</code>?</p>

<p>Example of my data-frame after clustering.  </p>

<pre><code>Cluster   Char1   Char2   Char3   Char4   Char4
1         0.00    0.02    1.23    3.21    2.34
1         2.12    12.1    1.42    1.31    2.04
2         5.35    59.2    0.01    9.32    9.33
3         5.23    10.3    8.13    0.72    0.91   
...    
</code></pre>

<p>Is there more informative guide on the internet? How can I investigate these things?</p>
"
"0.206010481049842","0.192055319899344","100322","<p>I hope this is not too much to read, but I tried to give you a specific overview over my problem.</p>

<p>I am currently trying to model the German electricity market, with a special focus on <a href=""http://www.tennettso.de/site/en/Transparency/publications/network-figures/use-of-balancing-power"" rel=""nofollow"">balancing power</a>.</p>

<p>Considering this setting I have a data set consisting of hourly observations of balancing power in Megawatts for the year of 2013. As the application of this data set would not represent any uncertainty in the data I wanted to use a little trick to ""assume"" uncertainty. While this is really only a basic approximation my model has to have some sort of ""uncertainty"", else there will be a big mistake. In order to do this, I divided my data set into five blocks (too many more would be to much for my calculation) of varying Megawatt-steps calculating the ""probability"" of a specific value to be in one of these blocks using the <a href=""http://en.wikipedia.org/wiki/Cumulative_distribution_function#Complementary_cumulative_distribution_function_.28tail_distribution.29"" rel=""nofollow"">Complementary cumulative distribution function</a>.</p>

<p>What I mean by that is that I took the aforementioned function and tried to best approximate it in a (5-)piece-wise fashion. Unfortunately I had to do this in my example by a ""sense of proportion"". I hope the picture makes it a little more clear (On the x-axis the values are in Megawatts). </p>

<p><strong>EDIT:</strong> To be clear, I am not trying to best-possibly approximate the whole-function in a sense of a non parametric boundary estimation as user603 kindly described in his answer. (Or maybe this can be done using non parametric boundary estimation...?) I just want to fit the five (or whatever number) blocks best-possible to the function in a fashion that, e.g. the Sum of squared Errors is at a minimum. Also: i would love the knot location to be estimated.</p>

<p><img src=""http://i.stack.imgur.com/xh9Q2.png"" alt=""(5-)Piece-wise approximation of the CCDF""></p>

<p>The following code example should further explain what I do:</p>

<pre><code># This function divides the input data into 5 different blocks.
# v, w, x and y are all Megawatt values dividing the data set
getCall_block &lt;- function(v, w, x, y, data){

  n &lt;- nrow(data)
  #' parameters for function
  data_v &lt;- subset(data, data[,2] &lt;= v, select = c(colnames(data)[1],
                                                   colnames(data)[2]))
  data_w &lt;- subset(data, data[,2] &gt; v &amp; data[,2] &lt;= w, select = c(colnames(data)[1],
                                                                  colnames(data)[2]))
  data_x &lt;- subset(data, data[,2] &gt; w &amp; data[,2] &lt;= x, select = c(colnames(data)[1],
                                                                  colnames(data)[2]))
  data_y &lt;- subset(data, data[,2] &gt; x &amp; data[,2] &lt;= y, select = c(colnames(data)[1],
                                                                  colnames(data)[2]))
  data_r &lt;- subset(data, data[,2] &gt; y, select = c(colnames(data)[1],
                                                  colnames(data)[2]))
  #' 
  df_v &lt;- getAverage(data, data_v, 0, v)
  df_w &lt;- getAverage(data, data_w, v, w)
  df_x &lt;- getAverage(data, data_x, w, x)
  df_y &lt;- getAverage(data, data_y, x, y)
  df_r &lt;- getAverage(data, data_r, y, max(data[,2]))

  return(rbind(df_v, df_w, df_x, df_y, df_r))
}
</code></pre>

<p>And the getAverage function...</p>

<pre><code>#This function is called in the above mentioned function. 
#It calculates the mean average of the points of the CCDF lying in one block. 
#v and w are the input parameters in Megawatts. 
#They can be understood as starting point and end point of a given block.

getAverage &lt;- function(data_c, data, v, w){
  #' Imposing greater equal on v if v &gt; 0
  if(w &gt; v){
    if (v &gt; 0){
      v &lt;- v + 1
    } else {
      v &lt;- 0
    }
  #' Initializing div, df, mean, n, sum
    div &lt;- 0
    mean &lt;- 0
    n  &lt;- nrow(data)
    sum &lt;- 0
    df &lt;- data.frame(ID = data[,1], call_freq = numeric(n))
  #' Creating inverse ECDF based on dataset
    F1 &lt;- ecdf(data_c[,2])
  #' Building sum over every data point in the interval [v;w]/(v;w]
    for (i in seq(v, w, 1)){
      sum &lt;- sum + (1 - F1(i))
      div &lt;- div + 1
    }
    mean &lt;- sum / div
    df$call_freq = mapply(function(x) return (x),  mean)
    return (merge(data, df, by = ""ID""))
    } else {
      cat(""Check input parameters. v is greater than w."")
  }
}
</code></pre>

<p>My questions are the following:</p>

<p>I) Does this way of approximating a CCDF sound okay for you guys? What did I overlook or completely mess up?</p>

<p>II) Is there a way to automatically approximate the block sizes, in order to not rely on a sense of proportion. E.g. minimizing the sum of squared errors between the piece-wise function and the original function. Maybe this could be done by using a clustering algorithm like k-means? But then how to cluster this CCDF...?</p>

<p>III) Is there a far more easier way to approximate any function in a piece-wise fashion?</p>

<p>Thanks in advance!</p>
"
"0.11516335992622","0.143149583578467","102984","<p>I have a dataset consists of 5 features : A, B, C, D, E. They are all numeric values. Instead of doing a density-based clustering, what I want to do is to cluster the data in a decision-tree-like manner.</p>

<p>The approach I mean is something like this:</p>

<p>The algorithm may divide the data into X initial clusters based on feature C, i.e. the X clusters may have small C, medium C, large C and very large C values etc. Next, under each of the X cluster nodes, the algorithm further divide the data into Y clusters based on feature A. The algorithm continues until all the features are used.</p>

<p>The algorithm that I described above is like a decision-tree algorithm. But I need it for unsupervised clustering, instead of supervised classification. </p>

<p>My questions are the following:</p>

<ol>
<li>Do such algorithms already exists? What is the correct name for such algorithm</li>
<li>Is there a R/python package/library which has an implementation of this kind of algorithms?</li>
</ol>
"
"0.0892051550175079","0.110883190643186","103244","<p>I'm running a hierarchical clustering on a sample of data using the steps below:</p>

<pre><code>library(RODBC)

setwd('D:/r/cluster2')
channel &lt;- odbcConnectExcel('cluster.xls')
data &lt;- sqlFetch(channel, 'clust9')

y9 &lt;- data.frame(inf=data$infest, faible=data$faible, moyen=data$moyen, fort=data$fort, lon=data$Lon, lat=data$Lat)

y9 &lt;- na.omit(y9) # listwise deletion of missing
y9.use &lt;- y9
y9 &lt;- scale(y9) # standardize variables

wss &lt;- (nrow(y9)-1)*sum(apply(y9,2,var))
for (i in 2:15) wss[i] &lt;- sum(kmeans(y9, centers=i)$withinss)

plot(1:15, wss, type=""b"", xlab=""Number of Clusters"", ylab=""Within groups sum of squares"")

# K-Means Cluster Analysis
fit &lt;- kmeans(y9, 5) # 5 cluster solution

aggregate(y9,by=list(fit$cluster),FUN=mean)

y9 &lt;- data.frame(y9, fit$cluster)

# Ward Hierarchical Clustering

d &lt;- dist(y9, method = ""euclidean"") # distance matrix

fit &lt;- hclust(d, method=""ward"") 

plot(fit) # display dendogram

rect.hclust(fit, k=5, border=""red"")
</code></pre>

<p>and i got this results:</p>

<p><img src=""http://i.stack.imgur.com/65YdS.png"" alt=""enter image description here""></p>

<p>But when I did the same steps the next day I got different results: 
<img src=""http://i.stack.imgur.com/clS5h.png"" alt=""enter image description here""></p>

<p>They are not different in everything, but there are individual that they now belong to another cluster!</p>

<p>so I don't know why  this behavior? i'm interested in interpreting and explaning the results, so when i get different results each time, that will make my previous interpretation wrong, what can I do for now ?</p>
"
"0.136263125082667","0.120983479623957","107448","<p>I'd like to cluster points based on a distance criteria. As
I want to cluster spatial points I am using euclidean distance 
and a hierachical cluster approach. In a final step I'd like to 
cut the dendrogramm at a specific distance to make sure that all 
cluster means are spatially separated by a minimum distance specified. </p>

<p>Here the R code so far:</p>

<pre><code># Creating the spatial points
set.seed(1)
x &lt;- runif(100,0,150)
set.seed(5)
y &lt;- runif(100,0,150)
df &lt;- data.frame(x,y)
plot(df)

# Clustering using a hierachical approach based on euclidean
hc &lt;- hclust(dist(df,method = ""euclidean""), method=""average"")

# Specify a distance of minimum 60 units between cluster means?
df$memb &lt;- cutree(hc, h = 60)

# plot clusters in colors
plot(df$x,df$y,col=df$memb)

# calculate cluster means
cent &lt;- NULL
for(k in 1:length(unique(df$memb))){
      cent &lt;- rbind(cent, colMeans(df[df$memb == k, , drop = FALSE]))
}
cent &lt;- as.data.frame(cent)

# plot points on top of clusters
points(cent$x,cent$y,pch=15)
</code></pre>

<p>However when I am cross-checking the distance between the cluster means, I observe
minimum distances that are slightly below my threshold (e.g. 60):</p>

<pre><code># Summary of distance between cluster means
summary(dist(cent))
</code></pre>

<p>I guess that is related to the clustering method ""average"" which is different
from the cluster means I am calculating thereafter. However, how can I achieve,
that the distance between cluster means (or centroids, etc) is at least 60 units?
What should I use as a measure of cluster mean, and what can I use as a clustering method?</p>
"
"0.126155140059354","0.130677093372329","109754","<p><img src=""http://i.stack.imgur.com/QbZ5T.png"" alt=""Consider the following plot.""></p>

<p>I want to identify the regions that are considerably higher than the highest cluster. (The obvious regions which should be identified as their own clusters, notably at the x coordinate ~10 e+07. How would I be able to identify that using a clustering algorithm?</p>

<p>I am using R algorithm kmeans in the picture above. with 6 centers:</p>

<p><code>kmeans(numbers_vector, centers=6, nstart=10)</code></p>

<p>What can I do to alleviate the inadequacy of this algorithm? Use a different clustering algorithm? Have more centers? But If I have more centers, it identifies many more regions in the center (namely clusters 3,5, and 6). Any ideas? </p>

<p>Here is histogram and density. It is important to note that the histogram and density plot DO NOT show the spike ~x=10e+07, because the number of points involved in the spike ~20, perhaps are completely overshadowed by the ~107,350 points plotted.</p>

<p><img src=""http://i.stack.imgur.com/1zrGE.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/Od8v7.png"" alt=""enter image description here""></p>

<p>Data: x (88289 obs.);   Bandwidth 'bw' = 0.7574</p>

<pre><code>   x                 y            
</code></pre>

<p>Min.   : -1.272   Min.   :0.000e+00<br>
 1st Qu.: 40.114   1st Qu.:4.110e-06<br>
 Median : 81.500   Median :3.167e-05<br>
 Mean   : 81.500   Mean   :6.035e-03<br>
 3rd Qu.:122.886   3rd Qu.:2.886e-03<br>
 Max.   :164.272   Max.   :4.827e-02  </p>
"
"0.0892051550175079","0.0739221270954573","110622","<p>I have some prior knowledge of grouping, but this may be incorrect or is not sufficient as I need larger number of groups (i.e. subgroups). For example in the following data I have 3 groups in addition to two variables. I would like to use the group information (as prior knowledge) (here 3 groups) to create meaningful groups (here 9 groups/clusters). Is there a correct way to perform such analysis.</p>

<pre><code># Dummy data 
group &lt;- rep(1:3, each =3000)
X &lt;- c(rnorm(1000, 0.1, 0.04), rnorm(1000,0.2, 0.04), rnorm(1000, 0.4, 0.02),
       rnorm(1000, 0.4, 0.04), rnorm(1000,0.5, 0.08), rnorm(1000, 0.6, 0.12), 
       rnorm(1000, 0.7, 0.08), rnorm(1000,0.8, 0.1), rnorm(1000, 0.9, 0.06)
)

Y &lt;-  c(rnorm(1000, 0.5, 0.04), rnorm(1000,0.6, 0.04), rnorm(1000, 0.7, 0.04),
       rnorm(1000, 0.35, 0.12), rnorm(1000,0.45, 0.04), rnorm(1000, 0.3, 0.02), 
       rnorm(1000, 0.55, 0.09), rnorm(1000,0.65, 0.12), rnorm(1000, 0.65, 0.04)
)
</code></pre>

<p>Prior information of 3 clusters:</p>

<pre><code>col = c(""red"", ""cyan"", ""green"")
plot(cbind(X,Y), col = col[group], pch = ""."")
</code></pre>

<p><img src=""http://i.stack.imgur.com/v5xUI.jpg"" alt=""enter image description here""></p>

<p>Clustering analysis assuming 9 clusters.</p>

<pre><code>cl &lt;- kmeans(cbind(X,Y), 9)

colrs &lt;- c(""red"",""purple"", ""yellow"", ""tan"", ""pink"", ""cyan"", ""blue"", ""green"", ""black"")
plot(cbind(X,Y), col = colrs[cl$cluster], pch = ""."")
</code></pre>

<p><img src=""http://i.stack.imgur.com/tecHn.jpg"" alt=""enter image description here""></p>
"
"0.0892051550175079","0.0369610635477286","110722","<p>I have a large data set that I would like to cluster using spherical K means algorithm. However, I am relatively new to this subject and R in general. Most of my knowledge is self taught and I am still in the beginning stages- I have read all about K means clustering in the past few days and I would like to apply it to my own project (184 rows of 4000+ columns containing measurements in decimal values). How and where do I start?</p>

<p>I am trying to teach myself how to go about this in R but I can't seem to find examples online on how to do this, or at least they don't apply to my case. I was wondering if anyone here would know about a step by step tutorial or has access to a script in which a spherical k means clustering has been conducted in R, and if you could share it with me. I have found a few papers but they are very advanced, and most are for text clustering and not numeric values.</p>

<p>I hope this question is not too vague. Does anyone have experience in this subject and could guide me on which steps I have to take to get started? Absolute beginner here, so I apologize if I am completely wrong to ask such a question on this platform.</p>

<p>Thank you for taking time out of your day to read this!</p>
"
"0.072835704072923","0.0905357460425185","112749","<p>I have a dataset of items and I want to measure how my clustering method works. I'm using R and simple k-means clustering. I have a lets say gold standard of my clusters and I want to see how good is the result of my clustering algorithm based on the features I used. Is there any straightforward way to do that in R? I was thinking about something like Jaccard similarity per item. I mean the intersection/union of clusters containing each item and somehow mixing them together!</p>
"
"0.146579026946503","0.101222041272365","113504","<p>I have some code that looks for clusters in x,y data. To check the number of clusters I use, I want to get the BIC. This is not possible (easily) using <code>kmeans()</code>, and so I've switched to the <a href=""http://cran.r-project.org/web/packages/mclust/index.html"" rel=""nofollow"">mclust package</a>. Specifically, I'm trying to replace <code>kmeans()</code> from the R stats package, with <code>Mclust()</code> from the mclust package. </p>

<p>Using <code>Mclust()</code> requires me to specify which model should be used for the clustering. According to <code>?Mclust</code>, the following models can be used in <code>Mclust()</code>:</p>

<pre><code>univariate mixture      
""E""  =   equal variance (one-dimensional)
""V""  =   variable variance (one-dimensional)
multivariate mixture        
""EII""    =   spherical, equal volume
""VII""    =   spherical, unequal volume
""EEI""    =   diagonal, equal volume and shape
""VEI""    =   diagonal, varying volume, equal shape
""EVI""    =   diagonal, equal volume, varying shape
""VVI""    =   diagonal, varying volume and shape
""EEE""    =   ellipsoidal, equal volume, shape, and orientation
""EEV""    =   ellipsoidal, equal volume and equal shape
""VEV""    =   ellipsoidal, equal shape
""VVV""    =   ellipsoidal, varying volume, shape, and orientation
single component        
""X""  =   univariate normal
""XII""    =   spherical multivariate normal
""XXI""    =   diagonal multivariate normal
""XXX""    =   ellipsoidal multivariate normal
</code></pre>

<p>I'm presuming that k-means in stats is a ""spherical, unequal volume"" model, ie. to get <code>k-means(x = data, centers = 6)</code> to match <code>mclust()</code>, I should use <code>mclust(data, G = 6, modelNames = c(""VII""))</code>. </p>

<p>However, in the limited tests I've done, this gives different cluster centroids. The example below uses 6 clusters with some test data. The centroids obtained through each method are shown.</p>

<p><img src=""http://i.stack.imgur.com/NJrQ0.png"" alt=""enter image description here""></p>

<p>Can anyone confirm which <code>mclust()</code> model is equivalent to <code>kmeans()</code>?</p>
"
"0.0892051550175079","0.110883190643186","116136","<p>Say you've $N$ functions $f_N(x)$ defined on a regular grid $x$.  You don't know the form of $f(x)$, you've only got several realizations of it.  The different functions are related to each other somehow, but you don't know exactly how.  You want to pick $n$ representative ones, or equivalently, group them into $n$ clusters.</p>

<p>Here is an example:</p>

<pre><code>N = 1000
library(MASS)
B = mvrnorm(N,mu=c(0,0,.0001),Sigma = matrix(c(1,.5,-.3,.5,.5,-.2,-.3,-.2,1),3))
X = cbind(1,x)
dim(B)
dim(X)
f = B[,1:2]%*%t(X)+as.matrix(rexp(N,rate = (abs(B[,3]))^.1))%*%t(as.matrix(sin(x)))
plot(1:10,cex=0,xlim=c(0,10),ylim=c(-10,10))
for (i in 1:nrow(f)){
    lines(x,f[i,],col=rgb(1,0,0,.2))
    }
</code></pre>

<p>(A plot will pop up)</p>

<p>I want to group them somehow.  I'm not sure how.  Intuitively, I want them organized by where they start, whether they go up or down, and how wiggly they are.  But we're supposing that I didn't know the data generating process, I've only got that matrix <code>f</code>.</p>

<p>I was thinking I could do a taylor expansion on each row of <code>f</code>, out to some arbitrary order $t$.  This would give me an approximation $f(x) = X'\beta$, where $X$ is $1,x,x^2,...,x^t$ and $\beta$ is the transformed Taylor approximation coefficients.  I was thinking of then k-means clustering by those $\beta$'s.  </p>

<p>My question:  does that approach make sense?  I just came up with it, but I don't know if any better methods have already been figured out.  </p>

<p>And if it does make sense, how do you do a Taylor series in R, getting what I'm conceptualizing as $\beta$?</p>
"
"0.072835704072923","0.0905357460425185","118272","<p>I am trying to implement K means clustering in R, 
Here is what my data look like:</p>

<pre><code> Seq                RegionNames(Zip) X%year(PercentChange)
4002                   53147      -1.683282e-02
4003                   28504      -1.807185e-02
4004                   10591      -5.432917e-03
4005                   96761       1.151578e-02
4006                   32750       5.905045e-03
4007                   54904      -1.193602e-04
4008                   97140       2.667454e-02
4009                   33774       1.932240e-02
4010                   43616      -1.159712e-03
4011                   89011       3.021237e-02
</code></pre>

<p>I am trying to cluster zip codes (<code>RegionNames</code>) based on percentage change in 5 years(<code>X5Year</code>)</p>

<p>Here is my code</p>

<pre><code>    newallHomesZip &lt;- data.frame(allHomesZip$RegionName,allHomesZip$X5Year) #Making new data frame with only zip and 5 yearly percentage increase
    (cl &lt;- kmeans(na.omit(newallHomesZip), 2))  #omiting Null values and trying to form two clusters
    plot(x, col = cl$cluster) #plotting
</code></pre>

<p>here is the plot I get :</p>

<p><img src=""http://i.stack.imgur.com/7CJwL.png"" alt=""enter image description here""></p>

<p>Clearly the clusters are not good.</p>

<p>I don't really understand the Kmean method, I want to form clusters based on percent change as magnitude and zip code as identifier</p>
"
"0.185695338177052","0.21306624726853","123040","<p>I have a matrix of 336x256 floating point numbers (336 bacterial genomes (columns) x 256 normalized tetranucleotide frequencies (rows), e.g. every column adds up to 1).</p>

<p>I get nice results when I run my analysis using principle component analysis. First I calculate the kmeans clusters on the data, then run a PCA and colorize the data points based on the initial kmeans clustering in 2D and 3D:</p>

<pre><code>library(tsne)
library(rgl)
library(FactoMineR)
library(vegan)
# read input data
mydata &lt;-t(read.csv(""freq.out"", header = T, stringsAsFactors = F, sep = ""\t"", row.names = 1))
# Kmeans Cluster with 5 centers and iterations =10000
km &lt;- kmeans(mydata,5,10000)
# run principle component analysis
pc&lt;-prcomp(mydata)
# plot dots
plot(pc$x[,1], pc$x[,2],col=km$cluster,pch=16)
    # plot spiderweb and connect outliners with dotted line
    pc&lt;-cbind(pc$x[,1], pc$x[,2])
    ordispider(pc, factor(km$cluster), label = TRUE)
ordihull(pc, factor(km$cluster), lty = ""dotted"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/QviDR.png"" alt=""enter image description here""></p>

<pre><code># plot the third dimension
pc3d&lt;-cbind(pc$x[,1], pc$x[,2], pc$x[,3])
    plot3d(pc3d, col = km$cluster,type=""s"",size=1,scale=0.2)
</code></pre>

<p><img src=""http://i.stack.imgur.com/fNiri.png"" alt=""enter image description here""></p>

<p>But when I try to swap the PCA with the t-SNE method, the results look very unexpected:</p>

<pre><code>tsne_data &lt;- tsne(mydata, k=3, max_iter=500, epoch=500)
plot(tsne_data[,1], tsne_data[,2], col=km$cluster, pch=16)
    ordispider(tsne_data, factor(km$cluster), label = TRUE)
ordihull(tsne_data, factor(km$cluster), lty = ""dotted"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/6WKYl.png"" alt=""enter image description here""></p>

<pre><code>plot3d(tsne_data, main=""T-SNE"", col = km$cluster,type=""s"",size=1,scale=0.2)
</code></pre>

<p><img src=""http://i.stack.imgur.com/Hqqpf.png"" alt=""enter image description here""></p>

<p>My question here is why the kmeans clustering is so different from what t-SNE calculates. I would have expected an even better separation between the clusters than what the PCA does but it looks almost random to me. Do you know why this is? Am I missing a scaling step or some sort of normalization?</p>

<p>Thanks for your advice.</p>
"
"NaN","NaN","123787","<p>I have a sample data below that is from a large data set, where each participant is given multiple condition for scoring.</p>

<pre><code> Participant&lt;-c(""p1"",""p1"",""p2"",""p2"",""p3"",""p3"")
 Condition&lt;-c( ""c1"",""c2"",""c1"",""c2"",""c1"",""c2"")
 Score&lt;-c(4,5, 5,7,8,2)
 T&lt;-data.frame(Participant, Condition, Score)
</code></pre>

<p>I am trying to use K-mean clustering to split participants in different groups, is there any good way to do it, considering the condition is not numeric?</p>

<p>thanks!</p>
"
"0.103005240524921","0.096027659949672","130974","<p>I need to use binary variables (values 0 &amp; 1) in k-means. But k-means only works with continuous variables. I know some people still use these binary variables in k-means ignoring the fact that k-means is only designed for continuous variables. This is unacceptable to me.</p>

<p>Questions:  </p>

<ol>
<li>So what is the statistically / mathematically correct way of using binary variables in k-means / hierarchical clustering?</li>
<li>How to implement the solution in SAS / R?</li>
</ol>
"
"0.185695338177052","0.21306624726853","132629","<p>I was trying to implement some clustering tendency tools in R, namely the Hopkin's index and the Coxâ€“Lewis index.</p>

<p>Here is the <a href=""https://books.google.com.sg/books?id=QgD-3Tcj8DkC&amp;pg=PA899&amp;lpg=PA899&amp;dq=clustering+tendency+%5BSergios_Theodoridis,_Konstantinos_Koutroumbas&amp;source=bl&amp;ots=lUUv2H7uh4&amp;sig=oq2Ax1wtX2MnHSrMvuOqLSSj9cU&amp;hl=en&amp;sa=X&amp;ei=VuitVO6kBoySuATXo4KYCA&amp;ved=0CCMQ6AEwAQ#v=onepage&amp;q=clustering%20tendency%20%5BSergios_Theodoridis%2C_Konstantinos_Koutroumbas&amp;f=false"" rel=""nofollow"">link</a> at page 901 to show what they are</p>

<p>This is what I managed to come up with in R. For the distance, I use the Euclidean distance. I also use additional R packages: data.table and RANN</p>

<pre><code>library(data.table)
library(RANN)
hopkins=function (data)
{
   #Number of samples
   m = round(0.2 * nrow(data))

   #Index of my data to be choosen as data samples
   sample = round(runif(m, 0, nrow(data)))

   #Get distance to nearest neigbour for each sample (W)
   #Get rid of first column as it is 0
   nearestW = nn2(data,data[sample,],k=2)$nn.dists[,-1]

   #Get the random sample from uniform distribution for each column
   samp = function(data,no.of.samples,min,max)
   {
    #Get the first and last quantile for every column
    quantile = quantile(x=data,probs=c(0.25,0.75))
    points = runif(n=no.of.samples,min=quantile[1],max=quantile[2])
    return(points)
   }

   uniform.sample = data[, lapply(.SD,samp,no.of.samples=m), .SDcols=names(data)]

  #Get distance to nearest data for each uniform sample (U)
  nearestU = nn2(data,uniform.sample,k=1)$nn.dists[,1]

  #Apply Hopkins index
  return(sum(nearestU)/(sum(nearestW)+sum(nearestU)))
}

CoxLouis=function (data)
{
  #Number of samples
  m = round(0.2 * nrow(data))

  #Get the random sample from uniform distribution for each column
  samp = function(data,no.of.samples,min,max)
  {
    #Get the first and last quantile for every column
    quantile = quantile(x=data,probs=c(0.25,0.75))
    points = runif(n=no.of.samples,min=quantile[1],max=quantile[2])
    return(points)
  }

  uniform.sample = data[, lapply(.SD,samp,no.of.samples=m), .SDcols=names(data)]

  #Get distance to nearest data for each uniform sample (U)
  nearest = nn2(data,uniform.sample,k=1)
  nearestU = nearest$nn.dists[,1]

  #Get the index of the nearest data
  nearest.dataindex = nearest$nn.idx[,1]

  #Get distance to nearest neigbour for each data (W)
  #Get rid of first column as it is 0
  nearestW = nn2(data,data[nearest.dataindex,],k=2)$nn.dists[,-1]

  #Apply Coxâ€“Lewis index
  return(mean(nearestU/nearestW))
}
</code></pre>

<p>Running hopkins(data.table(iris[,1:4])) gives 0.756792 and Coxâ€“Lewis(data) gives 2.709349.</p>

<p>On the other hand, the lecture notes from this <a href=""http://www.cs.rpi.edu/~zaki/www-new/pmwiki.php/Dmcourse/Main?action=download&amp;upname=chap18.pdf"" rel=""nofollow"">link</a> gives a much higher value of 0.935 for the Hopkin's index. I suspect that it has something to do with the way I am creating samples from a uniform distribution. </p>

<p>What I did was to generate uniform samples at random along each dimension. As for the min and max values, I use the first and last quantile respectively instead of the data min and max value. I thought that it will be more robust.</p>

<p>Can someone enlighten me if I am indeed going on the right direction for that ?  </p>
"
"0.103005240524921","0.128036879932896","134842","<p><strong>Scenario</strong></p>

<p>I have a project about fraud detection where i need to find outliers by kmeans. </p>

<ol>
<li><p>I have a dataset about bank credits length of 1000.  There are 21</p></li>
<li><p>columns (14 categorical, 7 numeric columns).</p></li>
</ol>

<p><strong>Issue</strong></p>

<p>I want to find outliers by clustering data and I need to put all outliers inside the same cluster. How can I achieve this with R.</p>

<p><strong>my tries</strong></p>

<p>I have tried by <code>""lofactor""</code>, but categorical columns caused me an error. </p>

<p>I deleted categorical columns, then it worked. </p>

<p><strong>results</strong></p>

<p>But I shouldn't delete categorical columns since they are also important for determining outliers.</p>

<p>So how can I achieve to find outlier pattern in R?</p>
"
"NaN","NaN","134862","<p>Can you please tell me when to use the K-mean clustering and hierarchical clustering algorithm and what is the different between them...</p>

<p>Regards,</p>

<p>Rahul</p>
"
"0.236912053207318","0.243270071872502","136594","<p>I'm trying to plot all the steps of a k-means algorithm with r, but I can't. </p>

<p>The k-means algorithm works in this way:</p>

<ul>
<li>Step 1. Initialize the center of the clusters</li>
<li>Step 2. Assign the closest initial centers to each data point</li>
<li>Step 3. Set the position of each cluster to the mean of all data points belonging to that cluster</li>
<li>Step 4. Assign the closest cluster to each data point </li>
<li>Step 5. Repeat steps 3-4 until convergence</li>
</ul>

<p>I plot the dataset and initial centers of clusters (Step 1). Too, I can plot the new cluster centers and show which point belongs to each cluster (Step 3 and 4). But I don't know how to plot the Step 2. I need <strong>the very first initial centers membership of each point</strong>, before the first iteration, but <code>kmeans()</code> doesn't give it you. How could I calculate this?</p>

<p>Here is my code:</p>

<pre><code>set.seed(2009)
points1 &lt;- data.frame(x=rnorm( 50,1,0.1), y=rnorm(50,5,0.1))
points2 &lt;- data.frame(x=rnorm( 50,5,0.1),  y=rnorm(50,5,0.1))
points3 &lt;- data.frame(x=rnorm(200,3,0.8),y=rnorm(200,3,0.8))
df &lt;- rbind(points1, points2, points3)

p &lt;- ggplot(df, aes(x, y))
p + geom_point(size=7, color=""grey"") + labs(title=""Initial configuration"")

y &lt;- c(4.88871745,4.88099143,3.69713723)
x &lt;- c(0.75606015,1.26736958,3.04961545)
kcenters &lt;- data.frame(x,y)

p + geom_point(size=7, color=""grey"") + 
    geom_point(data=kcenters, aes(x, y), size=7, color=""black"", shape=""x"") + 
    labs(title=""Initial centers"")

dfCluster &lt;- kmeans(df, centers=kcenters, iter.max=1)

p + geom_point(size=7, aes(colour=as.factor(dfCluster$cluster))) + 
        geom_point(data=data.frame(dfCluster$center), aes(x, y), size=7, 
               color=""black"", shape=""x"") + 
    theme(legend.position=""none"") + 
    labs(title=""First iteration"")
</code></pre>

<p>My goal would be to show the initial center membership of each point in ""Initial centers"" plot.</p>

<hr>

<p>Edit:</p>

<p>I think I did not explain myself properly.</p>

<p>On this website there is a simulation showing what I would like to get:</p>

<p><a href=""http://www.onmyphd.com/?p=k-means.clustering"" rel=""nofollow"">http://www.onmyphd.com/?p=k-means.clustering</a></p>

<p>When you click the ""Iteration"" button the first time (click1), the initial centers are placed. Pressing a second time (click 2), points are assigned to closer center, and painted with different colors. When you click the third time (click3), new centers are calculated, and when you press for the fourth time (click4), points are assigned to closer center again.</p>

<p>When you run <code>kmeans()</code> and stop it at the first iteration, you get the new centers of the clusters ( click3 ), <code>dfCluster$center</code>, and cluster membership of each point (click4), <code>dfCluster$cluster</code>, but you do not get the initial center membership of each point (click 2), which is exactly what I'm looking for.</p>

<hr>

<p>I finally accomplished what I wanted: a step-by-step k-means. Sorry if the code it's not perfect, I'm a newbie with R.</p>

<pre><code>#How does k-means work

library(ggplot2)

set.seed(2009)
points1&lt;-data.frame(x=rnorm(50,1,0.1),y=rnorm(50,5,0.1))
points2&lt;-data.frame(x=rnorm(50,5,0.1),y=rnorm(50,5,0.1))
points3&lt;-data.frame(x=rnorm(200,3,0.8),y=rnorm(200,3,0.8))
df&lt;-rbind(points1,points2,points3)

#plot initial points
p &lt;- ggplot(df, aes(x, y))
p + geom_point(size=7, color=""grey"")

#set initial centers
kcenters&lt;-df[c(49,26,297),]

#plot centers
p + geom_point(size=7, color=""grey"") + geom_point(data=kcenters, aes(x, y), size=7, color=""black"", shape=""x"")

#assignment (to calculate distances to initial centers and to allocate points to the cluster to which they are closest)
library(reshape)
distances &lt;- melt(as.matrix(dist(df,diag=T,upper = T)), varnames = c(""row"", ""col""))
dist_center1&lt;-subset(distances,col==49,select = value)
dist_center2&lt;-subset(distances,col==26,select = value)
dist_center3&lt;-subset(distances,col==297,select = value)
dist_centers&lt;-data.frame(dist_center1,dist_center2,dist_center3)
colnames(dist_centers)&lt;-c(""dist_center1"",""dist_center2"",""dist_center3"")
dist_centers$cluster&lt;-apply(dist_centers, 1, which.min)
df&lt;-cbind(df,dist_centers)

#plot assignment
p + geom_point(size=7, aes(colour=as.factor(df$cluster))) + geom_point(data=kcenters, aes(x, y), size=7, color=""black"", shape=""x"") + theme(legend.position=""none"")

#calculate new centers
x&lt;-tapply(df$x,df$cluster,mean)
y&lt;-tapply(df$y,df$cluster,mean)
kcenters&lt;-data.frame(x,y)

#plot new centers
p + geom_point(size=7, aes(colour=as.factor(df$cluster))) + geom_point(data=kcenters, aes(x, y), size=7, color=""black"", shape=""x"") + theme(legend.position=""none"")
</code></pre>

<p>And then, you can continue the procedure slightly adjusting the code above:</p>

<pre><code>#assignment
df&lt;-rbind(df[,1:2],kcenters)
row.names(df) &lt;- NULL
distances &lt;- melt(as.matrix(dist(df,diag=T,upper = T)), varnames = c(""row"", ""col""))
dist_center1&lt;-subset(distances,col==301,select = value)
dist_center2&lt;-subset(distances,col==302,select = value)
dist_center3&lt;-subset(distances,col==303,select = value)
dist_centers&lt;-data.frame(dist_center1,dist_center2,dist_center3)
colnames(dist_centers)&lt;-c(""dist_center1"",""dist_center2"",""dist_center3"")
dist_centers$cluster&lt;-apply(dist_centers, 1, which.min)
df&lt;-cbind(df[1:300,],dist_centers[1:300,])

#plot assignment
p + geom_point(size=7, aes(colour=as.factor(df$cluster))) + geom_point(data=kcenters, aes(x, y), size=7, color=""black"", shape=""x"") + theme(legend.position=""none"")

#calculate new centers
x&lt;-tapply(df$x,df$cluster,mean)
y&lt;-tapply(df$y,df$cluster,mean)
kcenters&lt;-data.frame(x,y)

#plot new centers
p + geom_point(size=7, aes(colour=as.factor(df$cluster))) + geom_point(data=kcenters, aes(x, y), size=7, color=""black"", shape=""x"") + theme(legend.position=""none"")
</code></pre>

<p>If you run <code>kmeans()</code> with the same initial centers and stop it on first iteration, <code>dfCluster&lt;-kmeans(df,centers=kcenters, iter.max = 1)</code>, you get the follow centers:</p>

<pre><code>&gt; dfCluster$centers
         x        y
1 1.129419 4.905327
2 2.928011 2.880839
3 4.715513 4.766608
</code></pre>

<p>These centers do not match with the ones I get in the first iteration of my procedure (#calculate new centers). I have to run it for 14 times (#assigment and #calculate new centers) to obtain them. I don't know the meaning of ""iteration"" in the <code>kmeans()</code> procedure. Does anybody know?</p>
"
"0.11516335992622","0.143149583578467","137291","<p>My goal is to cluster time series based on their DTW distance. Therefore I've calculated full distance matrices as input for several clustering algorithms. I first had a look at hierarchical methods, since the number of clusters don't have to be specified at the beginning (moreover k-means is problematic because of the problem of averaging time series under DTW and k-medoids is expensive). </p>

<p>Single linkage (not really useful), complete linkage and average linkage (UPGMA/WPGMA) are unproblematic methods, another criterion which seems to be often used is the Ward method (in R: <code>ward.D2</code> for the <code>hclust</code>-function). I've seen at least <a href=""http://www.prasa.org/proceedings/2012/prasa2012-14.pdf"" rel=""nofollow"">one paper</a> which uses the Ward method with DTW distances, however I am bit skeptical about the usage of Ward in this context. The ward distance of two clusters $A,B$ is defined, according to that paper, as: </p>

<p>$$D_{AB}=\frac{||c_A-c_B||^2}{1/|A|+1/|B|}$$</p>

<p>Where $c_A,c_B$ is the centroid of A, B. </p>

<p>My questions are:  </p>

<ol>
<li><p>How the centroids are calculated if only a distance matrix is given? (I'm guessing this calculation can't be applied when DTW distances are used.) </p></li>
<li><p>Since the application of the Ward method with DTW distances seems to be questionable, are there any other alternative (hierarchical) clustering methods, which can be used with a DTW distance matrix?  </p></li>
</ol>
"
"0.11516335992622","0.143149583578467","138591","<p>I have a dataset in CSV format that looks as follows:</p>

<pre><code>guid,eventA.location,eventA.time,eventB.location,eventB.time,...
a12b,server3,1424474828.1804667,server7,1424474828.1804668,...
a12c,server3,1424474829.4444667,server2,1424474838.3334668,...
</code></pre>

<p>Each row has a unique <code>guid</code>, and the columns come in pairs of location and time.  The locations are one of a small set of 10 possible values, <code>server1</code> through <code>server10</code>.  The times are in seconds since epoch.  There are 400 <code>guid</code>s, and about 40 events (so about 80 columns).  Some cells may have NA values, but not too many, so I'm happy to get rid of the rows that have them.</p>

<p><strong>How do I perform a k-means clustering on this data, and then create a nice plot of it?</strong>  Not sure how to go about handling the non-numeric data, the N/A data, the fact that the time scale is very tight (within 30s, so relative to the absolute values of these since-epoch times, the differences look negligible but really aren't), etc.</p>

<p>Here's what I've tried so far, but not gotten very far, and the error messages don't make much sense to me:</p>

<pre><code>&gt; x &lt;- read.csv('/path/to/file')
&gt; km &lt;- kmeans(x, 3) 
Error in do_one(nmeth) : NA/NaN/Inf in foreign function call (arg 1)
In addition: Warning messages:
1: In do_one(nmeth) : NAs introduced by coercion
2: In do_one(nmeth) : NAs introduced by coercion
&gt; km &lt;- kmeans(na.omit(x), 3)
Error in sample.int(m, k) : invalid first argument
&gt; km &lt;- kmeans(factor(na.omit(x)), 3)
Error in sort.list(y) : 'x' must be atomic for 'sort.list'
Have you called 'sort' on a list?
</code></pre>

<p>I've also run <code>daisy(na.omit(x))</code> but I'm not sure what to make of the output:</p>

<pre><code>Dissimilarities :
dissimilarity(0)

Metric :  mixed ;  Types = N, I, N, I, N, I, N, I, N, I, N, I, N, I, N, I, N, I, N, I, N, I, N, I, A, I, N, I, N, I, N, I, N, I, N, I, N, I, A, I, N, I, N, I, N, I, N, I, A, N, I, N, I, N, I, N, I, N, I, N, I, N, A, N, I, N, I, N, I, N, I, N, I, N, I, N, I, N, I, A, I, N, I, N, I, N, I, N, I, N, I, N
Number of objects : 0
There were 50 or more warnings (use warnings() to see the first 50)
</code></pre>
"
"0.145671408145846","0.158437555574407","140839","<p>Example of data set</p>

<pre><code>PRODUCT_ID household_key   BASKET_ID DAY QUANTITY SALES_VALUE STORE_ID RETAIL_DISC TRANS_TIME WEEK_NO COUPON_DISC
1     1082185          2375 26984851472   1        1        1.21      364        0.00          6       1           0
2     1036325          2375 26984851472   1        1        0.99      364       -0.30          6       1           0
3     1033142          2375 26984851472   1        1        0.82      364        0.00          6       1           0
4     8160430          2375 26984851472   1        1        1.50      364       -0.39          6       1           0
5     1004906          2375 26984851472   1        1        1.39      364       -0.60          6       1           0
6     6423775          2375 26984851516   1        1        2.00      364       -0.79          6       1           0
7     9487839          2375 26984851516   1        1        2.00      364       -0.79          6       1           0
8      826249          2375 26984851516   1        2        1.98      364       -0.60          6       1           0
9     1102651          2375 26984851516   1        1        1.89      364        0.00          6       1           0
10    1043142          2375 26984851516   1        1        1.57      364       -0.68          6       1           0
11    1085983          2375 26984851516   1        1        2.99      364       -0.40          6       1           0
12     981760          1364 26984896261   1        1        0.60    31742       -0.79          6       1           0
13     842930          1364 26984896261   1        1        2.19    31742        0.00          6       1           0
14     920955          1364 26984896261   1        1        3.09    31742        0.00          6       1           0
15     937406          1364 26984896261   1        1        2.50    31742       -0.99          6       1           0
16     897044          1364 26984896261   1        1        2.99    31742       -0.40          6       1           0
17    1048462          1130 26984905972   1        1        1.19    31642       -0.80          5       1           0
18     833715          1130 26984905972   1        2        0.34    31642       -0.32          5       1           0
19     866950          1130 26984905972   1        2        0.34    31642       -0.32          5       1           0
20    1022843          1130 26984905972   1        2        0.34    31642       -0.32          5       1           0
21    1071333          1130 26984905972   1        2        0.34    31642       -0.32          5       1           0
22     923972          1173 26984945254   1        1        0.67      412        0.00          7       1           0
23    1131351          1173 26984945254   1        1        0.88      412        0.00          7       1           0
24     824399          1173 26984945254   1        2        1.98      412        0.00          7       1           0
25    1082185            98 26984951769   1        1        0.39      337        0.00          7       1           0
26     965138            98 26984951769   1        2        3.00      337       -0.08          7       1           0
27     878302            98 26984951769   1        3        0.90      337        0.00          7       1           0
28    1087347            98 26984951769   1        1        0.25      337        0.00          7       1           0
29     985911            98 26984951769   1        1        1.25      337       -0.34          7       1           0
30     877180          1172 26985025264   1        1        2.29      396        0.00          4       1           0
   COUPON_MATCH_DISC MANUFACTURER DEPARTMENT BRAND COMMODITY_DESC SUB_COMMODITY_DESC
1                  0            2         34     2            293                166
2                  0           69         34     1            300                391
3                  0            2         34     2            213               1499
4                  0           69         34     1            215               1513
5                  0           69         34     1            235               1695
6                  0          586         18     2             74                941
7                  0          586         18     2             74                941
8                  0           69         18     1             16               1005
9                  0         1266         18     2            231               1593
10                 0          321         11     2             39               2097
11                 0          586         18     2             74               2251
12                 0           69         18     1            104                697
13                 0           69         18     1             72                944
14                 0         3664         22     2             26                970
15                 0         2209         23     2             36               1197
16                 0         1075         18     2             81               2034
17                 0         1273         18     2             16               1233
18                 0         1002         18     2            275               1790
19                 0         1002         18     2            275               1790
20                 0         1002         18     2            275               1790
21                 0         1002         18     2            275               1790
22                 0          353         11     2             44                420
23                 0           69         11     1            105               1100
24                 0         1573         11     2             45               1958
25                 0            2         34     2            293                166
26                 0           69         18     1             74               1900
27                 0          544         18     2             15               1985
28                 0          544         18     2             15               1985
29                 0           69         18     1             17               2167
30                 0         2296         11     2            273                237
</code></pre>

<p>As you can see from above this data is category data. I want to cluster this data by using K MEANS to find similar behaviour within households. There are 2500 unique households and each row represents each transaction made for each unique product. there are in total 93000 unique products.</p>

<p>I understand I need to pre process this data one step more before I can use the K means clustering. </p>

<p>Something I thought of was </p>

<pre><code>library(dplyr)
randomtest &lt;- mydata %&gt;% group_by(household_key) %&gt;% 
  summarise_each(funs(sum),QUANTITY, SALES_VALUE, RETAIL_DISC, COUPON_DISC)
</code></pre>

<p>the code above gives me the sum of quantity, sales_value, retail_disc and coupon_disc for each household. I used this data frame to do my clustering by removing the household key.</p>

<p>due to lack of reputations I can not plot the results I got.</p>

<p>However looking at this result I don't think there is anything meaningful I can find which describes the behaviour for the households.</p>

<p>I am in need of some serious help where someone can suggest me some more ideas of pre processing this data so when I do the clustering it outputs me something meaningful.</p>

<p>I have tried many other things aswell but each time I got results which didn't make sense. I am sure I am close to the answer just need some help with silly mistakes.</p>

<p>Any help would be much appreciated.</p>

<p>Thank you</p>
"
"0.0515026202624605","0.064018439966448","141280","<p>I have run k-means clustering. I have also plotted the results using the following code in R: </p>

<pre><code>library(cluster)
library(fpc)
km &lt;- kmeans(Mydata,3)
clusplot(data, km$cluster, color=TRUE, shade=T,   lines=0)
</code></pre>

<p><img src=""http://i.stack.imgur.com/2cZFZ.png"" alt=""enter image description here""></p>

<p>I do not understand what the ""component 1"" and ""component 2"" in the graph are. I also have no clue about what is meant by ""These two components explain 46.78% of the point variability"". </p>

<p>What are the components? How are they helpful in understanding the clustered data? </p>
"
"0.0515026202624605","0.064018439966448","145691","<p>I am working on a data clustering and don't know how I can achieve it with R !
I am working on a data set of 50 observations each of 8 variables. What i want is to have clusters gathering the observations with high homogeneity. 
Kmeans isn't giving me that in terms of data visualization !
How can I do that ?</p>
"
"0.072835704072923","0.0905357460425185","148417","<p>Is it OK to use <code>kmeans</code> with binary variables? I mean Euclidean distance? I guess the binary variables will be the ones that get the most power to determine the result.</p>

<p>Look at the following example:</p>

<pre><code>data= data.frame(a=c(1,0,1,1), b=c(0.1,.2,.6,.8))
plot(data)
kmeans(data,2)
## Clustering vector: [1] 1 2 1 1
</code></pre>

<p>So the result is determined by the binary variable.</p>

<p>Is there a way to treat binary variables differently? Should I use Manhattan distance for all variables?</p>
"
"NaN","NaN","148597","<p>I am working on a clustering model with the kmeans() function in the package stats and I have a question about the output. </p>

<p>My data is a sample from several tech companies and AAPL._UP is a variable equal to ""1"" if apple was up on that particular day. </p>

<p>I ran a kmeans algorithm with a k=16 and it gave me some output. I can interpret most of it but I'm just not sure what I'm looking at here. Can some one let me know what these numbers mean?</p>

<p>There is a picture of what I am looking at <img src=""http://imgur.com/OnKHCLX.jpg"" alt=""picture""></p>

<p>If it helps, here are the cluster assignments <img src=""http://imgur.com/jom3h05.jpg"" alt=""picture""></p>
"
"0.103005240524921","0.128036879932896","149254","<p>I have to separate 425 observations based on certain variables numbering 32.</p>

<p>1)I used PCA to reduce the dimensionality of Data, which gave me 32 components out of which 5 components accounted for 75% of the variance.</p>

<p><img src=""http://i.stack.imgur.com/F5pPi.png"" alt=""Spider plot of clusters![][1]"">2)I used Elbow plot to determine no of clusters that is 6. I further used these 5 components as variables for kmeans clustering. when plotted using clusplot() Clusters are not well separated.</p>

<p><img src=""http://i.stack.imgur.com/R11XN.png"" alt=""Clusters""> </p>

<p>Does this mean that clustering was not successful.(between_SS / total_SS =  67.9 %). Is there any other way to determine effective ness of clustering? Can somebody suggest a better way to obtain better clusters may be PAM, Hierarchical clustering ...?</p>
"
"0.11516335992622","0.143149583578467","149707","<p>I have data that refer to the number of occurrences of specific variable in samples:</p>

<pre><code>       V1  V2  V3 ...
sample1 0   2   1
sample2 7   1   0
sample3 1   4   1  
....
</code></pre>

<p>The data refers to the occurrence of genes(V1...) in different genomes (sample1..).</p>

<p>I want to perform a cluster analysis combined with an heat map.
I used the function <code>heatmap.2</code> in the <code>gplot</code> package in R.
I used Euclidian distances for calculating the distance among the samples. 
The clustering algorithm is the default one for the function <code>hclust</code> in R (<code>hclust(d, method = ""complete"", members = NULL)</code>).
However, I am not completely sure it is the right method. 
Any suggestion on how to choose the right method to calculate the distances among my samples?</p>

<p><strong>EDIT</strong>
The aim is to describe the distribution of the variables (genes) among the samples (genome), and cluster the samples(genomes) according with the values that each variables assume (meaning, how many specific genes are present)</p>
"
"0.218507112218769","0.21125007409921","152359","<p>I am trying to construct (undirected) <strong>social network based on co-occurence of individuals</strong>. Clustering algorithm will be later applied on this network to find some distinct subgroups. Issue is that studied animal species has <strong>very short longevity</strong> (or rather very high mortality due to predators). <strong>It causes that not all of the relationships in my network may have existed at the same time.</strong> If you look on the diagram below, the <em>""red""</em> individuals are almost extincted after 3-4 years*, but they have the <em>""longest""</em> time to <em>""meet""</em> other individuals, whereas <em>""blue""</em> individuals have only two years to <em>""meet""</em> others.</p>

<p><img src=""http://i.stack.imgur.com/J9nhk.png"" alt=""enter image description here""></p>

<p><em>Theoretically I can assume that each individual has expected longevity less than 10 years. Therefore not catching of ""red"" individual 5 or 6 years after tagging does not necessarily means that is dead.</em></p>

<p><strong>How to include this time effect into social network?</strong></p>

<p><strong>Specific questions I want to answer:</strong>
<strong>First question: Are observed social connections distinct from a connections explained solely by shared space use? i.e., How to test if associations are random or preferred?</strong></p>

<p>If answer to first question will be that associations between individuals are <strong>NOT</strong> random, then I have a second qeustion...</p>

<p><strong>Does social structure correlates with genetic relatedness? i.e., are closely related individuals more often together?</strong> (DNA profiles of all inividuals are bolow)</p>

<p>Here I created some data structurally similar to my database:</p>

<pre><code>data &lt;- data.frame(obs_date = c(""C1"",""C2"",""C3"",""C4"",""C5"",""C6"",""C1"",""C2"",
                                ""C3"",""C4"",""C1"",""C2"",""C3"",""C1"",""C2"",""C3"",
                                ""C4"",""C5"",""C6"",""C7"",""C1"",""C3"",""C4"",""C5"",
                                ""C6"",""C7"",""C8"",""C3"",""C4"",""C5"",""C6"",""C7"",
                                ""C3"",""C4"",""C5"",""C6"",""C3"",""C4"",""C5"",""C3"",
                                ""C4"",""C5"",""C6"",""C5"",""C6"",""C7"",""C8"",""C5"",
                                ""C5"",""C6"",""C7"",""C8"",""C5"",""C6"",""C7"",""C7"",
                                ""C7"",""C8"",""C7"",""C8"",""C7"",""C8"",""C7"",""C8""),
                   ind_id = rep(LETTERS[1:20], times = c(6,4,3,7,1,6,5,4,
                                               3,2,2,4,1,4,3,1,2,2,2,2)),
                   obs = rep(c(""seen"",""not_seen"",""seen"",""not_seen"",""seen"",
                               ""not_seen"",""seen"",""not_seen"",""seen""),
                               times = c(3,1,4,1,9,1,9,3,33)))
</code></pre>

<p>Here I added genetic structure. Data are completely fabricated, but they should reflect close genetic relatedness between same collor individuals. Aditionally <em>""violet""</em> individuals are offsprings of <em>""blue""</em>, <em>""blue""</em> are offsprings of <em>""green""</em>, <em>""green""</em> are offsprings of <em>""red""</em>. </p>

<pre><code>gen.raw &lt;- matrix(c(""a"",""g"",""g"",""g"",""c"",""g"",""a"",""a"",""g"",""g"",""g"",""g"",""t"",""c"",""t"",""c"",""t"",""t"",""a"",""a"",""t"",""t"",""a"",""a"",
                    ""a"",""g"",""g"",""g"",""c"",""g"",""a"",""a"",""g"",""g"",""g"",""g"",""c"",""c"",""t"",""c"",""t"",""t"",""a"",""a"",""t"",""c"",""a"",""a"",
                    ""a"",""g"",""g"",""g"",""c"",""g"",""g"",""a"",""g"",""g"",""g"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""a"",""a"",""t"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""t"",""t"",""g"",""g"",""a"",""g"",""g"",""g"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""a"",""a"",""a"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""t"",""t"",""g"",""g"",""a"",""g"",""g"",""g"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""a"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""t"",""t"",""g"",""g"",""a"",""g"",""g"",""g"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""a"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""t"",""t"",""g"",""g"",""g"",""g"",""g"",""c"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""a"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""t"",""t"",""g"",""a"",""c"",""g"",""t"",""c"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""a"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""t"",""t"",""g"",""a"",""c"",""g"",""t"",""c"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""a"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""t"",""t"",""g"",""a"",""c"",""g"",""t"",""c"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""a"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""t"",""t"",""g"",""a"",""c"",""g"",""t"",""c"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""a"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""t"",""t"",""g"",""a"",""c"",""g"",""t"",""c"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""a"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""t"",""t"",""g"",""a"",""c"",""g"",""t"",""c"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""a"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""t"",""t"",""g"",""a"",""c"",""g"",""t"",""c"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""t"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""t"",""t"",""g"",""a"",""c"",""g"",""t"",""c"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""t"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""t"",""t"",""g"",""a"",""c"",""g"",""t"",""c"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""t"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""c"",""t"",""g"",""a"",""c"",""g"",""g"",""c"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""t"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""c"",""t"",""g"",""a"",""c"",""g"",""g"",""c"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""t"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""c"",""t"",""g"",""a"",""c"",""g"",""g"",""c"",""g"",""c"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""t"",""c"",""a"",""a"",
                    ""a"",""g"",""t"",""c"",""t"",""g"",""a"",""c"",""g"",""c"",""c"",""g"",""t"",""c"",""t"",""t"",""c"",""t"",""t"",""a"",""t"",""c"",""a"",""a""),
                    byrow = TRUE, ncol = 24)
rownames(gen.raw) &lt;- LETTERS[1:20]
</code></pre>

<p>Ok, source data are given above. Now I will create <strong>two distance matrices</strong>. First is <strong>association matrix</strong> derived from co-occurence data represented by <strong>OR-SP index</strong>. Observed  Roost-Sharing Proportion is calculated for each pair of individuals by <strong>dividing the number of days two individuals were found together by the number of all possible days they could be together</strong> (overlap bewteen first and last recordngs of both individuals).</p>

<pre><code># matrix of days roosting together
EG &lt;- expand.grid(unique(data$ind_id), unique(data$ind_id))

data_seen &lt;- subset(data, obs == ""seen"")

my.length.dt &lt;- numeric(nrow(EG))
for (i in 1:nrow(EG)) {
my.length.dt[i] &lt;- length(intersect(as.vector(data_seen$obs_date[data_seen$ind_id == EG[i, 1]]),
                                    as.vector(data_seen$obs_date[data_seen$ind_id == EG[i, 2]])))
days.together &lt;- matrix(my.length.dt, byrow = TRUE, ncol = length(unique(data$ind_id)))
    colnames(days.together) &lt;- rownames(days.together) &lt;- unique(data$ind_id)
}
days.together

# matrix of all possible potentional roosting days
EG &lt;- expand.grid(unique(data$ind_id), unique(data$ind_id))
my.length.rdp &lt;- numeric(nrow(EG))
for (i in 1:nrow(EG)) {
my.length.rdp[i] &lt;- length(intersect(as.vector(data$obs_date[data$ind_id == EG[i, 1]]),
                                     as.vector(data$obs_date[data$ind_id == EG[i, 2]])))
roosting_days_possible &lt;- matrix(my.length.rdp, byrow = TRUE, ncol = length(unique(data$ind_id)))
    colnames(roosting_days_possible) &lt;- rownames(roosting_days_possible) &lt;- unique(data$ind_id)
}
roosting_days_possible

# OBSERVED ROOST-SHARING PROPORTION
OSP &lt;- days.together/roosting_days_possible
OSP[ is.nan(OSP) ] &lt;- 0
diag(OSP) &lt;- 0

# So here is association matrix derived from co-occurence data
round(OSP,2)
# social distance matrix
soc_dist &lt;- as.dist(OSP)
</code></pre>

<p>Next step is to take DNA sequences and make genetic relatedness matrix</p>

<pre><code># creating matrix of relatedness
library(ape)
gen.str &lt;- as.DNAbin(gen.raw)
my.gen.dist &lt;- dist.dna(gen.str)
fit &lt;- hclust(my.gen.dist, method=""ward"")
plot(fit) # display dendogram 
</code></pre>

<p>Finally, here I <strong>compare social distance with genetic distance by Mantel test</strong>.</p>

<pre><code>library(ade4)
mantel.rtest(soc_dist, my.gen.dist, nrepet = 9999)
</code></pre>

<p><strong>Does its result (p > 0.05) mean that there is no correlation between social and genetic structure?</strong></p>

<p><strong>Is this appropriate solution to answer my question? Any ideas?</strong></p>

<p>I also found that for social structure might be better this type of graph instead of dendrogram. Good for finding distinct social group.</p>

<pre><code># Show social structure
library(igraph)
g &lt;- graph.adjacency(OSP, weighted=TRUE, mode =""undirected"")
g &lt;- simplify(g)
# set labels and degrees of vertices
V(g)$label &lt;- V(g)$name
V(g)$degree &lt;- degree(g)
wc &lt;- walktrap.community(g)
plot(wc, g)
</code></pre>
"
"0.145671408145846","0.158437555574407","155989","<p>I'm a Software Engineer trying to learn how to do a <a href=""http://en.wikipedia.org/wiki/Principal_component_analysis"" rel=""nofollow"">Principal Components Analysis</a> in Python or R.</p>

<p>I've found a few <a href=""https://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/"" rel=""nofollow"">links</a> which do a good job of explaining the concept from a high-level.  However, I haven't seen any examples which walk you through all of the steps from start to finish.</p>

<p>For example, letâ€™s say you have a 50-dimensioned dataset, which has 50 columns of varying data types (boolean, float, integer, varchar etc.)â€¦ Do those values need to be scaled or normalized to something like 0.0..1.0? Or can the PCA algorithm handle those disparate data types?</p>

<p>Ideally, I want to see something which does a walkthrough of each step and explains it on the way. <strong>Especially starting with disparate data which needs to be scaled or normalized.</strong> All examples I've seen online, including ones which use well-known example data sets (such as the <a href=""http://archive.ics.uci.edu/ml/datasets/Iris"" rel=""nofollow"">Iris dataset</a>), start with pristine data where all of the columns are the same data type. I'm starting with a large dataset with many columns of varying data types.  What do I do?</p>

<p>Incidentally, after applying PCA to my dataset, I plan on running it through clustering (k-means probably).</p>

<p><strong>Update 9/10/2015</strong></p>

<p>Since this question has been marked as off-topic, I'm not able to submit or select an answer.  In any case, I found two links from Sebastian Raschka to be very helpful:</p>

<ul>
<li><a href=""http://sebastianraschka.com/Articles/2014_pca_step_by_step.html"" rel=""nofollow"">Implementing a Principal Component Analysis (PCA) in Python step by step</a></li>
<li><a href=""http://nbviewer.ipython.org/github/rasbt/pattern_classification/blob/master/preprocessing/feature_encoding.ipynb"" rel=""nofollow"">Tips and Tricks for Encoding Categorical Features in Classification Tasks</a></li>
</ul>
"
"0.072835704072923","0.0905357460425185","157849","<p>I have a df containing samples of a time varying quantity (namely, exposure to electromagnetic field generated by a GSM station), and I use the mixtools package to find a fitting mixture model (tipically, a 2- or 3-component guassian mixture) because I assume that (or rather, want to test if) samples come from different pdf's based on the hour of the day (e.g., with larger mean during peak hours and lower ones during off-peak hours).</p>

<p>It seems to me that normalmixEM function included in the package does not return any threshold values that can be used to assign samples to the various distributions: how can I achieve that?</p>

<p>Should that function be unavailable in the mixtools package, could you please point me to a more general tool that would help me clustering values, also considering that it should make its own initial guess of the threshold values?</p>

<p>Many thanks!
Nicola</p>
"
"0.136263125082667","0.145180175548748","158716","<p>I am using FactomineR to explore a set of continuous variables in a large set of sites (ecological data). I did a PCA and found the relevant principal components and their scores and such. Afterwards I do a hierarchical clustering on the resulting PCA using HCPC with K-means clustering of the sites. The result comes up with 3 clusters, which confirms what I expected when seeing the PCA plot.
The data I am using (just for learning this stuff) can be found on <a href=""http://datadryad.org/resource/doi:10.5061/dryad.rg832/1"" rel=""nofollow"">http://datadryad.org/resource/doi:10.5061/dryad.rg832/1</a></p>

<p>The code I am using is the following:</p>

<pre><code>pca &lt;- PCA(pca_data_jouffray, graph=FALSE, scale.unit = TRUE)
hcpc &lt;- HCPC(pca, min = 3, max=10, iter.max=10, graph=FALSE)
</code></pre>

<p>My question is the following: I can see the significance of each variable for each cluster with (in this case for cluster 1)</p>

<pre><code>hcpc$desc.var$quanti$`1`

              v.test Mean in category Overall mean sd in category Overall sd          p.value
Macroalgae 11.646270        38.144928    15.142384      23.438186 18.6474238 2.397240e-31
Sand        9.303437        21.561594    11.087748      13.893514 10.6290048 1.359779e-20
CCA        -4.386715         3.094203     6.719785       3.940017  7.8031164 1.150752e-05
Hard.coral -5.755929         6.797101    18.303808       7.952790 18.8740653 8.616669e-09
Complexity -5.934810         1.702899     2.283113       0.737548  0.9230203 2.941863e-09
Turf.algae -7.446795        30.673913    48.649007      14.649733 22.7893314 9.563492e-14
</code></pre>

<p>But what I would like to know is if I can find out if the complete cluster is significantly different from the overall mean. So a p-value for each cluster as a whole, not split up by variable. I would think that there could be a test to see if the mean euclidean distances between the individuals of a cluster is significantly different of the overall mean euclidean distance between all individuals?</p>

<p>Is this possible? </p>
"
"NaN","NaN","159427","<p>I have a time series data in R, and I am using functional clustering. I would like to interpret a figure that is output below the code. Furthermore, I would like to control line colors and thickness in the figure.</p>

<pre><code>library(fda.usc)
data(phoneme)
mlearn&lt;-phoneme$learn[c(1:50,101:150,201:250),]
out.fd1=kmeans.fd(mlearn,ncl=3,draw=TRUE)
</code></pre>
"
"0.0892051550175079","0.110883190643186","160132","<p>I want to do a k-means clustering on a dataset containing 22 numerical variables between 0 and 100 and 75 observations using R. I read this post 
<a href=""http://stats.stackexchange.com/questions/133656/how-to-understand-the-drawbacks-of-k-means"">How to understand the drawbacks of K-means</a> on k-means clustering assumptions.  My question are: </p>

<p>1- How can I check that my clusters are Spherical or not? having 22 variables, I cannot visualize them.</p>

<p>2- For checking if ""all variables have the same variance"" do I need to test them statistically?</p>

<p>3- For checking ""the prior probability for all k clusters are the same, i.e. each cluster has roughly equal number of observations"" what should I do if I do not expect my data to have clusters of almost the same size? In other words, it is natural for my data to have clusters of different sizes.</p>
"
"0.126155140059354","0.130677093372329","161073","<p>I am working with GPS data for density based clustering in R.</p>

<p>Let's suppose, I have produced a path out of the following dataset. Now, where the density of plot is high enough (as shown in graph) over any particular area, it should produce a cluster.</p>

<p>Suppose this is my ggplot produced from a dataset as:</p>

<pre><code>   Lat          Long
92.14894444 50.01011111
92.14894444 50.01011111
92.14825    50.01491667
92.15875    50.01502778
92.15708333 49.98458333
92.16005556 49.98566667
92.16266667 49.99105556
92.16119444 50.00330556
92.16475    50.01558333
....
</code></pre>

<p>**I don't have to predefine numbers of clusters. So i think it's good to use density based clustering algorithm in this. If you have any other solution to produce similar results, you are welcome.</p>

<p><img src=""http://i.stack.imgur.com/YBtBU.png"" alt=""enter image description here""></p>

<p>Now, I want to produce these black circles over my ggplot.</p>

<p>I tried using density based clustering but it's not producing very good results.</p>

<p>Now, when I look at the clusters using density based, they are not meaningful. Some clusters have points which are too far. I want dense clusters but not that big in size (lets suppose within a range of 1 km radius). The output I want to produce is shown in ggplot.</p>

<p>This is what I have produced till now..</p>

<pre><code>library(ggplot2)
sp &lt;- ggplot(df, aes(x=Lat, y=Long )) +geom_point()
sp + geom_density2d()
</code></pre>

<p><img src=""http://i.stack.imgur.com/2YJ4y.png"" alt=""enter image description here""></p>
"
"0.072835704072923","0.0452678730212593","161675","<p>I am trying to perform a clustering analysis for a csv file with 50k+ rows, 10 columns. I tried k-mean, hierarchical and model based clustering methods. Only k-mean works because of the large data set. However, k-mean does not show obvious differentiations between clusters. So I am wondering is there any other way to better perform clustering analysis? Thanks in advanced!</p>

<p>The data looks like this</p>

<pre><code>Revenue  Employee  Longitude Latitude  LocalEmployee BooleanQuestions ...
1000     100       xxxx      xxxx      10
...                                                                   ...
</code></pre>

<p>Here is part of my code:</p>

<pre><code>mydata &lt;- scale(mydata)
wss &lt;- (nrow(mydata)-1)*sum(apply(mydata,2,var))
for(i in 2:15)wss[i]&lt;- sum(fit=kmeans(mydata,centers=i,15)$withinss)
plot(1:15,wss,type=""b"",main=""15 clusters"",xlab=""no. of cluster"",ylab=""with clsuter sum of squares"")

fit &lt;- kmeans(mydata,7)
clusplot(mydata, fit$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)
</code></pre>

<p><img src=""http://i.stack.imgur.com/YWUTR.png"" alt=""enter image description here""></p>
"
"0.072835704072923","0.0452678730212593","162018","<p>I am trying to perform a clustering analysis for a csv file with 50k+ rows, 10 columns. I tried k-mean, hierarchical and model based clustering methods. Only k-mean works because of the large data set. However, k-mean does not show obvious differentiations between clusters. So I am wondering is there any other way to better perform clustering analysis?</p>

<p>The data looks like this</p>

<pre><code>Revenue  Employee  Longitude Latitude  LocalEmployee BooleanQuestions ...
1000     100       xxxx      xxxx      10
...                                                                   ...
</code></pre>

<p>Here is part of my code:</p>

<pre><code>mydata &lt;- scale(mydata)
wss &lt;- (nrow(mydata)-1)*sum(apply(mydata,2,var))
for(i in 2:15)wss[i]&lt;- sum(fit=kmeans(mydata,centers=i,15)$withinss)
plot(1:15,wss,type=""b"",main=""15 clusters"",xlab=""no. of cluster"",ylab=""with clsuter sum of squares"")

fit &lt;- kmeans(mydata,7)
clusplot(mydata, fit$cluster, color=TRUE, shade=TRUE, labels=2, lines=0)
</code></pre>
"
"0.105129283382795","0.104541674697863","163162","<p>I am trying to do clustering on a distance matrix which contains numeric data. But I am not sure how to decide upon the number of clusters or value k for clara function in R. But after running it with some random number of clusters, I ran silhouette function on it and summary gives me like this:</p>

<p>Cluster sizes and average silhouette widths:  </p>

<pre><code>           7            3            4            5            7            4 
 0.222273330 -0.001592881  0.117937463  0.121326365  0.137911639  0.161932689 
Individual silhouette widths:
    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
-0.10410  0.08961  0.12500  0.14140  0.19840  0.30580 
</code></pre>

<p>This is the result for value of k=6. If I change it to say 5 or 4, I obtain silhouette for each cluster and also mean value. How do I decide upon the number of clusters? Do I need to plot like mean silhouette vs k? How do we do something like this in a large dataset with around million observations?</p>
"
"0.0892051550175079","0.110883190643186","163477","<p>I am looking for a little guidance as to the correct approach to this problem. We have a list of IDs and roughly 8 different numerical variables such as quantity and revenue. Each ID is unique to the dataset.</p>

<p>We are looking to group these customers together based on the 8 different variables. </p>

<p>At first we looked into k-means clustering, however that only accounted for one variable and the groups were unhelpful (ie there were 2 groups with just one customer in them and then rest were all grouped together, when we tried more clusters it became even more convoluted.)</p>

<p>We are attempting to do this in R and were curious if anyone had any recommendations as to which approach we should take? </p>

<p>Also, this is not homework for a stats class, this is a real business application. </p>
"
"0.072835704072923","0.0905357460425185","164333","<p>I have been looking at some tutorials and articles and couldn't get a scenario where two variables are in different scales and used in modeling.</p>

<p>So, firstly lets assume I have one metric of numeric type, other in percentages, and other in decimals. </p>

<ol>
<li>If I want to use those variables in a regression model for
prediction then do I need to do some standardization before fitting a<br>
model to the variables? If so how do we it in R or Python?</li>
<li>Moreover, if I want to use these features in k-means
clustering, do I need to follow the same steps as mentioned above?</li>
</ol>
"
"0.072835704072923","0.0905357460425185","166536","<p>I have a dataset with 3.205 observations and 6 variables about viewing habits from Rio and Sao Paulo in Brazil. They're supposed to have different habits so I need these data to form natural clusters (e.g., popular in both cities, one or the other, or neither). Here's what the 20 first rows look like:</p>

<pre><code>Program Shr_RIO Shr_SP  Rat_RIO Rat_SP  Fid_RIO Fid_SP
000903  0,475   0,383   10,835  13,451  18,671  18,636
000891  0,188   0,392   4,782   15,069  19,919  27,576
000891  0,255   0,628   6,146   24,163  18,871  28,396
000891  0,467   1,073   11,671  40,993  27,096  54,080
005831  1,677   0,465   34,284  15,517  21,914  12,847
005831  1,296   0,341   28,660  11,704  22,661  17,542
005831  0,443   0,215   9,014   7,142   13,920  11,603
005831  2,209   0,492   47,295  16,633  36,377  18,491
005831  1,049   0,194   23,343  6,613   32,202  7,857
000021  14,568  14,478  220,870 376,907 48,186  42,395
000021  13,244  13,085  209,847 299,642 40,093  37,626
000021  11,369  10,633  184,305 265,684 37,995  37,035
001055  1,459   0,359   31,598  12,559  38,428  29,078
001055  0,499   0,984   12,330  36,874  45,344  44,331
001055  0,351   0,509   7,867   17,797  17,732  41,494
000400  8,726   7,288   148,966 200,682 51,861  36,761
000400  6,840   6,762   139,347 196,219 35,954  37,387
000400  6,192   6,789   116,733 192,109 36,731  33,938
000400  8,281   6,795   160,616 198,423 41,236  34,247
</code></pre>

<p>Each row refers to different days the program was aired, that's why you see multiple rows for the same <code>Program</code>. I know that I could summarize one <code>Program</code> per row, so I could try some hierarchical clustering, but I also know that those means have a lot of variance. Is there any way that I could try some model-based clustering or another pattern recognition algorithm to a data like that? Is there any other way to summarize and have those variance well represented into the clusters? </p>
"
"0.137340320699895","0.149376359921712","168202","<p>I'm currently checking some clustering evaluation indexes in R, and now I'm using Silhouette and its respective function in R, ""silhouette"" (in ""cluster"" package). To test the method, I used the following code:</p>

<pre><code>data &lt;- matrix(c(1,2, 2,1, 1,1, 2,2, 8,9, 9,8, 9,9, 8,8, 1,15, 2,15, 1,14, 2,14),12,2,byrow=T)
clust &lt;- c(1,1,1,1, 2,2,2,2, 3,3,3,3)
diss &lt;- as.matrix(dist(data))
sil &lt;- mean(silhouette(clust,dmatrix=diss)[,3])
</code></pre>

<p>Using this data and with ""clust"" being the obtained configuration (from k-means), I would evaluate the silhouette of this configuration by the mean of the silhouettes for each datum. The point is that I searched for its use with k-means and found this page:</p>

<p><a href=""https://stat.ethz.ch/pipermail/r-help/2008-March/155939.html"" rel=""nofollow"">https://stat.ethz.ch/pipermail/r-help/2008-March/155939.html</a></p>

<p>And it's recommended to use the squared distance matrix instead, making <code>sil &lt;- mean(silhouette(clust,dmatrix=diss^2)[,3])</code>. This use changes the result from 0.8793842 to 0.9850074.</p>

<p>The point for me is the evaluation of the configuration itself, and as I created the data to clearly show three groups, the higher silhouette for this configuration makes more sense to me than the lower one.</p>

<p>I'm not sure if I understood it right, but the use of the squared distance matrix on a k-means clustering evaluation is because of the squared distance of its cost functions. But is its use needed? I mean, the evaluation using the distance matrix would be enough to evaluate two different configurations (both resulting from k-means) and point which one is better.</p>

<p>So, should I use the squared distance for a k-means clustering evaluation? And as I'm evaluating the configuration, shouldn't the same distance matrix be used to evaluate many different methods?</p>

<p>Thanks in advance!</p>
"
"0.162865585496114","0.182199674290256","168367","<p>I have the following dataset: consider a dataset $X$ of $1400 \times 600$. The rows represent households at time $1 \leq t \leq 14$. So I have $100$ households. The columns represent the programs that they have watched on that day and for how long (in minutes).</p>

<p>My question is how to ""aggregate"" the data. The following few ideas came to mind:</p>

<ul>
<li>Take the mean over the days and then aggregate to household level.</li>
<li>Take the sum over the days and then aggregate the data.</li>
<li>(*) Cluster the household days and take the largest cluster as the most representative viewing statement of that household.</li>
</ul>

<p>I would like to do (*) method, however the question is which algorithm is the most suitable to do this?</p>

<p>I have thought up the following trivial algorithm:</p>

<ol>
<li>Take the first vector of the household and then calculate its euler distance with each of the 13 remaining vectors.</li>
<li>Aggregate those vectors whose distance is less than $\epsilon$ with the first vector and repeat this process with the remaining vectors.</li>
<li>Take the vector that has been aggregated with the most as the representative vector.</li>
</ol>

<p>The problem is choosing an appropriate $\epsilon$. Not sure whether this is a real clustering algorithm (maybe someone can give its name?). My question is now: <strong>what existing algorithm can I use for the cluster step?</strong></p>

<p>Note: I cannot normalize the data, as far as I know, because even though this data set is rather small. For big data sets, the zero values will become non-zero and we need to have a special way to store it.</p>

<p>Note: with sparse I mean lots of zeroes. </p>

<p>I looked into the ideas of BellKor's Pragmatic Chaos of the Netflix prize and it seems that they first removed the individual effects and then the program specific effects. I was wondering how they did without having the sparse matrix of netflix scores transformed into a dense matrix.</p>
"
"0.0515026202624605","0.064018439966448","168717","<p>this is my first post.  I have an irregular time series that exhibits large shifts in both mean and in the direction of the trend.  It looks something like this (though this is far cleaner than reality):</p>

<pre><code>set.seed(1)
x = sort(rep(seq(as.POSIXct('2015/01/01'),as.POSIXct('2015/01/10'),by='day'),100) + runif(1000,0,80000))
y = c(seq(300),seq(90,70,-.2),seq(20,50,.1),seq(238.5,90,-.5)) + runif(1000,50,80)
plot(x,y)
</code></pre>

<p>The task is to:
 1. accurately partition/segment the data
 2. extract the change point indices
 3. fit regression separately for each segment</p>

<p>I have tried several routes, including:
a) hierarchical clustering based on dissimilarity matrix
b) function cpt.mean/var/meanvar from package changepoint (does not seem to work well)
c) function 'breakpoints' from package strucchange (slow and often inaccurate)
d) various types of kmeans (inappropriate, I know)</p>

<p>I have also explored some other packages, such as TSclust, urca, and DTW, but these seem better suited to clustering <em>sets</em> of time-series, not individual values within a time series.</p>

<p>Can someone point me in the correct direction? Perhaps I am not considering the appropriate data model?</p>

<p><strong>UPDATE</strong>
Thank you all for your very considered responses.  I went back to the strucchange package, and after some fiddling, have gotten it to work quite well.  I had not initially appreciated the h 'minimal segment size' argument.
Finished product:
<a href=""http://i.stack.imgur.com/qBgeY.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qBgeY.jpg"" alt=""enter image description here""></a></p>
"
"0.162865585496114","0.202444082544729","172617","<p><strong>Problem:</strong></p>

<p>I am figuring out the best way to find clusters for a dataset with observations that are densely packed together. The dataset is retail stores with three numeric variables based on operations metrics.</p>

<p>I do not know how to create a simulated dataset for an example like this. I have densely clustered data and outliers, but under 4k observations. </p>

<p><strong>Business objective:</strong></p>

<p>We need to separate the dataset into groups based on several variables.</p>

<p>The goal is to narrow down the stores with greater priority. Later on, we will use inference statistics for determining the cause of the operation metrics stated. Segmenting the stores based on priority makes sense through the three operations variables included.</p>

<p>I tried two different types of partitioning clustering methods, k-values, and different variables, but all yeilded poor validation results. Hereâ€™s the steps I took:</p>

<p><strong>Clustering with 2/3 variables:</strong></p>

<ol>
<li><p>Standardize in daisy dissimilarity matrix with euclidean distance <code>daisy()</code> function from <code>cluster</code> package in CRAN.</p></li>
<li><p>Chose k for k-means by looking at SSE chart <code>kmeans()</code> function.</p></li>
<li><p>Chose k for k-medoid by <code>pamk()</code> function in <code>fpc</code> package in CRAN for highest average silhouette width among clusters - resulted in a 0.23 average silhouette width. K-medoid was used with the <code>pam()</code> function from <code>cluster</code> package in CRAN.</p></li>
<li><p>Choose clustering algorithm by dunn-index - highest clustering result was k-medoids with 0.002. I used the <code>cluster-stats()</code> function in <code>fpc</code>.</p></li>
</ol>

<p><strong>Clustering with all three variables:</strong>
-same procedure as above.</p>

<p><strong>Result:</strong>
K-medoids with 2 clusters using two variables represented the algorithm with the highest dunn-indes. </p>

<p><strong>Overview:</strong>
After selecting the optimal number of clusters for each clustering method and comparing the best one using dunn-index, the results have overlap. </p>

<p>What is the recommended method for performing cluster analysis on densely clustered datasets? Do I need to perform clustering multiple times in order to segment the data further? </p>

<p><strong>EDIT: Added scatterplot showing clustering with 3 variables</strong></p>

<p><a href=""http://i.stack.imgur.com/ZoVyj.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZoVyj.png"" alt=""Scatterplot with cluster labels color-coded""></a></p>
"
"0.0515026202624605","0.064018439966448","173217","<p>I have high dimensional ($m \approx 2k$), high sample (n=140,000) dataset in R that I load into memory run PCA on it (returns $m \approx 400$ components to cover 95% of variance) then I run k means clustering on this dataset. However, even with wildly different number of clusters (from 1 to 1000) I always get the same total sum of squares. Though the cluster assigned to the datapoints I have inspected seem reasonable at first sight (they change seemingly appropriately with the number of clusters.) </p>

<p>So codewise:</p>

<pre><code>trans = preProcess(train, method=c('BoxCox','center','scale','pca'))
train_pc = predict(trans, train)
kNumbers = c(1,5,10,15,20,25,100,1000)
for (i in kNumbers) {
    model = kmeans(train_pc, centers=i, nstart=10)
    cat(model$totss + '\n')
}
</code></pre>

<p>Things I have tried:</p>

<ul>
<li>Playing with <code>iter_max</code> from 10 to 100 (then usually no warning messages about lack of convergence)</li>
<li>Increasing the <code>n_start</code> (1 to 10)</li>
<li>Preprocessing the data <code>BoxCox</code>, <code>center</code>, <code>scale</code></li>
</ul>

<p>Ultimately, this is just a part of feature engineering so if there is a way to get some generic way of informing the subsequent algorithm of some sort of idea of ""big picture"" closeness, then I am all up for that as well.</p>

<p>Any ideas?</p>
"
"0.11516335992622","0.143149583578467","173723","<p>I am developing a semi-supervised method for identifying anomalies in a time series with multiple states. Let's consider this example time series in which there are two states e.g. state 1 and 2 with mean power 0 and 100, respectively. </p>

<p><a href=""http://i.stack.imgur.com/C8KM0.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/C8KM0.png"" alt=""enter image description here""></a></p>

<p>As we can see, whenever the system enters into a particular state, it stays there for a fixed amount of time (with some variance) before transitioning into another state during the normal operation. e.g. 20 seconds in state 1, and 40 seconds in state 2, approximately. I am looking for an semi-supervised method to detect the anomalous state in which system enters into that state but stays there for a long/short period of time than the usual duration, as marked in read in the above figure. </p>

<p>So there are two obvious questions to ask here:  </p>

<ol>
<li>Identifying the number of states from the give time series. I use a clustering method (such as k-means) to find it.</li>
<li>Learning the state properties such as power-level and state duration. </li>
</ol>

<p>I am able to detect the states and their corresponding power level (or fitting to a distribution) using a HMM from the given normal time series. But how to learn the state duration? After learning these parameters, I am planning to use a finite state machine to model the states and their properties to detect the anomalies in an online manner. Any suggestions??</p>
"
"0.0892051550175079","0.110883190643186","173951","<p>My data has 192 variables in total (30k rows) and most of the variables have missing values. I want to do a classification model on this data e.g.</p>

<p>Only 3 dependent variables (excluding the customer ID) have 100% fill rate. Overall only 38 variables have less than 25% fill rate.</p>

<pre><code>Code used:

for(i in colnames(model_data)) {
    NA_count[i] = sum(is.na(model_data[,i]))
}

per_NA = data.frame(NA_count/nrow(model_data))
</code></pre>

<p>I have never worked this sparse data before. In my earlier experiences, we used to do the imputation mostly by the mean/median. I have also heard people using clustering for imputation but this data is so sparse (for almost all variables) that i am not sure if clustering will help (neither will knn impute). Please advice what method should be used for the same.</p>

<p>Also the predictor variable has 7% event rate</p>

<pre><code>prop.table(table(model_data_final$dependent))
         0          1 
0.92813333 0.07186667 
</code></pre>

<p>Again this is a new territory for me as i have never worked with low event rate data. I read a lot of articles wherein people have mentioned various techniques such as weighted RF, oversampling etc. Can someone please give some intuition on the same?</p>

<p>And why can't we just create a model on the data as is and then while validation decide the threshold of probability based on the performance on validation data?</p>

<p>Can't post the data for privacy reasons.</p>

<p>Thanks in advance</p>
"
"0.136263125082667","0.120983479623957","174556","<p>I want to use k-means to cluster my data.  I have broken one column into 4 dummy variables and I have normalized all of the data to mean=0 and sd=1.  Will k-means work with these dummy variables?</p>

<p>I have run the k-means in R and the results look pretty good, but are much more dependent on the value of these dummy variables than the rest of the data.  My 'between_SS / total_ss' = 58%</p>

<p>Data Sample:</p>

<pre><code>num_months, sales, dummy_a, dummy_b, dummy_c, dummy_d
10, 102.33, 1, 0, 0, 0
5.7, 57.5, 0, 0, 0, 1
21.3, 152.88, 0, 1, 0, 0
</code></pre>

<p>Code:</p>

<pre><code>library(""ggplot2"")
library(""scatterplot3d"")

mydata &lt;- read.csv(""data.csv"", stringsAsFactors = FALSE) 
data &lt;- scale(data)

km &lt;- kmeans(data, 4)     #Break into 4 clusters

##...combine the dummy variables into 1 field so I can use it as the 3rd dimension to graph it

results$color[results$cluster1==1] &lt;- ""red""
results$color[results$cluster2==1] &lt;- ""blue""
results$color[results$cluster3==1] &lt;- ""green""
results$color[results$cluster4==1] &lt;- ""orange""
with(results, {
    s3d &lt;- scatterplot3d(num_months, sales, dummy_combined,       
                  color=color, pch=19)       
     s3d.coords &lt;- s3d$xyz.convert(num_months, sales, dummy_combined)
})
</code></pre>

<p>edit:
Here is some code for my comment below.  It uses kmeans to cluster 3-dimensional data, 2 of which are binary data. It looks like it does a fine job clustering.</p>

<pre><code>seed(2015)
v1 &lt;- c(runif(500, min = -10, -5), runif(500, min = 5, 10))
v2 &lt;- round(runif(1000, min=0, max=1))
v3 &lt;- round(runif(1000, min=0, max=1))
v1 &lt;- scale(v1)
v2 &lt;- scale(v2)
v3 &lt;- scale(v3)

mat &lt;- matrix(c(v1,v2),nrow=length(v1))

k &lt;- kmeans(mat,4)

plot3d(v1, v2, v3,  size=7, col = k$cluster)        
</code></pre>

<p><a href=""http://i.stack.imgur.com/d7OtI.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/d7OtI.png"" alt=""enter image description here""></a></p>
"
"0.072835704072923","0.0905357460425185","174570","<p>I have some data that should be randomly assigned to treatment $T$, and am running some tests on observables to give evidence that this is indeed the case.</p>

<p>Let's focus on an outcome I'll call $X$, which is discrete, so the most natural test of if $X \perp T$ is Pearson's $\chi^2$. The Pearson test (via <code>chisq.test</code> in R) gives a $p$-value of .008, which at first glance indicates a fair amount of correlation between $X$ and $T$. Code was specifically:</p>

<pre><code>library(data.table)
analysis_data[ , chisq.test(x, treatment)$p.value]
</code></pre>

<p>However, there is a fair amount of meaningful clustering in the data, which is correlated with $X$ (e.g., as cluster size increases, average $X$ decreases).</p>

<p>To deal with this, I tried to implement a blocked bootstrap as follows:</p>

<ol>
<li><p>Assign cluster ID number</p></li>
<li><p>For each iteration $b=1,\ldots,B$:</p>

<p>a. draw $n$ cluster IDs at random (with replacement, where $n$ is the total number of clusters and create a sample consisting of those $n$ clusters of observations.</p>

<p>b. Calculate the Pearson's $\chi^2$ statistic (via <a href=""https://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test#Test_of_independence"" rel=""nofollow"">Wikipedia</a>) within each sample via <code>chisq.test(x, treatment)$statistic</code></p></li>
<li><p>Calculate $\tau_0$, the $\chi^2$ value in the original sample.</p></li>
<li><p>Calculate the $p$-value as the proportion of $\tau_b$ which exceed $\tau_0$, i.e.</p></li>
</ol>

<p>$$p = \frac{1}{B}\sum_{b=1}^B \mathbb{1}[\tau_b &gt; \tau_0]$$</p>

<p>(in code:</p>

<pre><code>analysis_data[ , cluster_id := .GRP, by = cluster_vars]
setkey(analysis_data, cluster_id)
test_dist &lt;- replicate(BB, analysis_data[
  .(sample(unique(cluster_id), rep = TRUE)),
  chisq.test(x, treatment)$statistic])
    t0 &lt;- analysis_data[, chisq.test(x, treatment)$statistic]
mean(test_dist &gt; t0)
</code></pre>

<p>)</p>

<p>When I do this, the $p$ value I get out (with <code>BB</code> = 10000) is .81. That strikes me as an almost unbelievably large change.</p>

<p>What's more, when I restrict attention to the subsample of size-1 clusters, I am met again with a large (though partially mediated) difference between the bootstrapped and the parametric result-- .28 from <code>chisq.test</code> vs. .88 from my bootstrap procedure.</p>

<p>Am I doing something wrong? What might be causing such large-magnitude changes in $p$-value between the procedures?</p>
"
"NaN","NaN","175547","<p>I have 200 15x15 matrices containing correlation values between 15 nodes at 200 different time points. I want to cluster the 200 matrices using k-means clustering. Is there a way to do this, probably in Matlab or R?</p>

<p>Update: I was able to solve it in matlab by unwraping each 15x15 matrix into vectors and apply kmeans().</p>
"
"0.145671408145846","0.158437555574407","179373","<p>I want to cluster high dimensional sparse data (100k rows and 2k columns, 10-20  non-zero values per row). Each row represents a person and each column an attribute this person does or does not posess (the columns may not be  independent from each other):</p>

<pre><code>person attr1 attr2 attr3 ...
pers1      1    0     1
pers2      0    0     1
pers3      0    0     1
pers4      0    1     0
</code></pre>

<p>To me this sounds like a subspace clustering issue so I wanted to apply CLIQUE and Entropy weighted kmeans (EWKM) both implemented in R. But CLIQUE does not seem to converge after 5 hours of calculation and EWKM finishes after 1 Iteration.</p>

<p>EWKM quits because after initialization it has a solution with a ridiculous small delta (below 1.0e-20) no matter what k or lambda i choose. The cluster results from different runs share no similarities. Therefore I do not believe that the results are more than arbitrary. Is EWKM even suited for this kind of data? The entropy in each column is VERY low (only 0's and 1's but almost always 0, even for attributes that are comparatively often shared).</p>

<p>T-SNE produces a plot like this on a random subset of the data:
<a href=""http://i.stack.imgur.com/Qng3T.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Qng3T.png"" alt=""T-SNE Plot""></a>
To me it looks like there are a handful of obvious cluster centers but not really strong partitions. Do you have an idea what algorithms may produce good results? Is EWKM even suited for this kind of data?</p>
"
"0.171411081394202","0.19531072666282","182232","<p>I have time-series data containing 1440 observations and the plot of the data is
<a href=""http://i.stack.imgur.com/LWkw7.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/LWkw7.png"" alt=""enter image description here""></a></p>

<p>I want to fit the Gaussian Mixture Models (GMM) to the above plot, and for the same I am using Mclust function of <a href=""https://cran.fhcrc.org/web/packages/mclust/index.html"" rel=""nofollow"">mclust</a> package. Finally, I want a fit somewhat like this:
<a href=""http://i.stack.imgur.com/zTtjJ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/zTtjJ.png"" alt=""enter image description here""></a></p>

<p>On using Mclust function, I do get following statistics</p>

<pre><code>   mclus_data &lt;- Mclust(givendataseries)
   &gt; summary(mclus_data)
----------------------------------------------------
Gaussian finite mixture model fitted by EM algorithm 
----------------------------------------------------

Mclust E (univariate, equal variance) model with 8 components:

 log.likelihood    n df      BIC      ICL
       9525.438 1440 16 18934.52 18183.67

Clustering table:
   1    2    3    4    5    6    7    8 
1262    0    0    0    0   13  114   51 
</code></pre>

<p>In the above statistic, I can not understand following:</p>

<ol>
<li>Significance of <code>log.likelihood</code>, <code>BIC</code> and <code>ICL</code>. I can understand what each of them is, but what their magnitude/value refers to?</li>
<li>It shows there are 8 clusters, but why cluster no. <code>2,3,4,5</code> has <code>0</code> values? What does this mean?</li>
<li>From the plot it is clear that there must be two Guassians, but why <code>Mclust</code> function shows there are 8 Guassians?</li>
</ol>

<p><strong>Update:</strong>
Actually, I want to do model based clustering of time series data. But currently  I want to fit the distribution to my raw data, as shown in Figure 1 on page no. 3 of <a href=""https://www.dropbox.com/s/q50e9q168lt27si/VerstileClusteringMethod.pdf?dl=0"" rel=""nofollow"">this</a> paper. For your quick reference, mentioned figure in said paper is
<a href=""http://i.stack.imgur.com/8Jq1B.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/8Jq1B.png"" alt=""enter image description here""></a></p>
"
"0.0515026202624605","0.064018439966448","187595","<p>I frequently come across data sets that have both categorical and numeric data.  I think this is just a fact of life where the data is not all in one category.  I'm basically trying to find some practical approaches or strategies on how to go about solving this problem.</p>

<p>In many books and initial searches on google I tend to get some sort of Kmeans clustering and a lot of phd-looking papers.  I think this is a common problem so I'm wondering if anyone can guide me to some practical suggestions or methods that I can further investigate and implement in R or Python?</p>

<p>I'm basically lost and just looking for some direction!</p>
"
"0.192705159543249","0.188206125478018","188235","<p>Recently I tried to cluster the <code>semeion.data</code> file available in <a href=""http://archive.ics.uci.edu/ml/datasets/Semeion+Handwritten+Digit"" rel=""nofollow"">UCI repository</a> using <code>kmeans</code> clustering as well as using the <code>neuralgas</code> method available in R's <code>flexcust</code> package. This file contains handwritten digits and pixel values are either 0 or 1 in an image of 16 X 16 pixels. </p>

<p>I have slightly modified the <code>semeion.data</code> file in that it contains just 257 columns with 256 pixel-values, as usual, and the last column is the digit label for that image: (0,1,2..9). Data set size is 1593 X 257.</p>

<p>I removed the 257th column and then tried to cluster the remaining dataset first, using k-means and then using <code>neuralgas</code> from the <code>flexclust</code> package. The results from <code>neuralgas</code> method are quite superior. The R-code and the results in both cases are as follows:</p>

<pre><code># R-code using kmeans without neuralgas
data &lt;- read.csv(""mod_semeion.csv"", header=FALSE)   # Data has no headers   
df   &lt;- data[,-257]                                 # df Data frame without digits labels
cl   &lt;- kmeans(df, 10 , iter.max = 500, nstart=200) # create 10 clusters
table(cl$cluster,data[,257])                         # Which cluster has which digits

       0   1   2   3   4   5   6   7   8   9
  1    0   0   1   4   9  69   1   5   8  75    # Cluster 1 has both digits 5 and 9
  2   85   0   0   0   0   1   1   0   0   1    # Cluster 2 has mainly digit 2
  3   70   0   0   0   0   2  38   0   1   1
  4    0  55   6   0  19   6   5 131   5   8    # Cluster 4 has mainlt digit 7
  5    0   2   0  96   0  63   1   0  19  44
  6    0  99  14   7   9   0   0  20   1   3
  7    0   4  96  43   2   6   0   0  17   6
  8    3   1   3   0 119   2   1   1   0   1    # Has mostly digit 4
  9    2   0  35   9   0   3   0   1 103  19
  10   1   1   4   0   3   7 114   0   1   0    # has mostly digit 6
</code></pre>

<p>Columnwise, for example, digit 7 is mostly clustered in cluster 4,
and digit 6 is mostly clustered in cluster 10. </p>

<pre><code># Distribution of digits in the data-set are as:
table(data[,257])   # Also, the column totals in each of the above 10 columns   (0-9)   
  0   1   2   3   4   5   6   7   8   9 
161 162 159 159 161 159 161 158 155 158 

# Cluster-wise points distribution are as:
table(cl$cluster)    # row-population/per-cluster
  1   2   3   4   5   6   7   8   9  10 
 172  88 112 235 225 153 174 131 172 131 

# Average relative cluster purity with kmeans: 
#  (max-of-a-digit)/cluster-population = (max-per-row/row-population)
((75/172) + (85/88) + (70/112) + (131/235) + (96/225) + (99/153) + (96/174) + 
 (119/131) + (103/172) + (114/131))/10
[1] 0.6587315  
</code></pre>

<p>Using the neural gas code, the average relative cluster purity is around 70% as against 65.8% as above. The code and the results are as below:</p>

<pre><code># R-code with neuralgas method
library(flexclust)  
obj &lt;- new(""cclustControl"")
obj@iter.max &lt;- 500
cl1 &lt;- cclust(df, 10, dist=""manhattan"", method=""neuralgas"", control=obj)
table(cl1@cluster, data[,257])  # which cluster has which digits

       0   1   2   3   4   5   6   7   8   9
  1  150   0   0   0   2   2  38   0   1   3    # Has mostly digit 0
  2    2   0  22   7   0   0   0   0  94  11    # has mostly digit 8
  3    0 105  24  11   9   0   0  25   2   6
  4    6   3   4   1   7  11 113   1   2   1
  5    2   1   4   0 118   1   0   0   0   0
  6    0  45   0   0  17   2   4 129   3   4
  7    1   2   1   1   4  71   1   3   8  29
  8    0   2   0 135   0  69   5   0  25  37
  9    0   4 104   3   1   3   0   0   8   1
  10   0   0   0   1   3   0   0   0  12  66


# Cluster-wise points distribution is as:
table(cl1@cluster)      # row-wise-total / per-cluster

  1   2   3   4   5   6   7   8   9  10 
196 136 182 149 126 204 121 273 124  82 

# Average relative cluster purity with neuralgas
((150/196) + (94/136) + (105/182) + (113/149) + (118/126) + (129/204) +  
 (71/121) + (135/273) + (104/124) + (66/82))/10
[1] 0.7085526
</code></pre>

<p>As can be seen from the above two cross-tables, neuralgas improves 
cluster purity and some digits are recognized much better using
neuralgas. For example, most 0's are now clustered together in cluster 1.</p>

<p>My question is: What is the neural gas method? Is it possible to explain how it works conceptually / in simple terms?</p>
"
"0.0515026202624605","0","188816","<p>I am trying to do k-means clustering in R using the cclust package.
In k-means clustering, the initial centroid assignment greatly affects the final allocation. The kmeans package has an nstart option, which guarantees that your results are based on 'nstart' number of initial configurations. Is there an equivalent option for the same in the cclust package?</p>

<p>Thanks.</p>

<p>TAK</p>
"
"0.0892051550175079","0.110883190643186","189163","<p>I want to use the <code>blockcluster</code> package in <code>R</code> to perform co-clustering on my data. But the function requires the number of row and column clusters to be prespecified. How do you decide the numbers of row and column clusters? Shall I do k-means clustering on the rows and columns separately prior to the use of <code>blockcluster</code>? </p>
"
"0.136263125082667","0.120983479623957","190168","<p>i'm not really experienced in spatial stats yet, but i'm growing into it. </p>

<p>I basically want to ascertain if certain values in a raster are a) autocorrelated and b) are more likely to exist in a certain spatial area (k-means? i'm unsure)</p>

<p>the data is a single raster showing land use change from 1 year to another (sorry I can't post the data) and there are about 50 possible changes (some far more prevalent than others). Quickly viewing the raster, it is clear that some changes are more prevalent in northern extremes, some in areas of upland farming etc etc, patterns do exist. But I want to prove this with stats. </p>

<p>For a) Local Moran's I (using a simple binary queen's spatial-weights matrix) gives us indication of spatial autocorrelation - this is useful for finding 'clumps' of similar data, correct?</p>

<p>For b) I'd like to explore whether each change combination is more likely to exist in a certain part of the UK (in Scotland, in western extremes etc). Would this be some sort of k-means clustering?</p>

<p>I'm doing all this in R,</p>

<p>thanks for any help 
(<a href=""http://gis.stackexchange.com/questions/90691/r-raster-package-morans-i-interpretation/108558#108558"">this question</a> had some good info re Moran's I),
(this <a href=""http://stats.stackexchange.com/questions/9739/clustering-spatial-data-in-r"">question</a> seems to start out with a similar goal, finding regional patterns in surface sea temps but fizzled out).</p>
"
"0.145671408145846","0.113169682553148","190938","<p>based on customer data I want to perform a clustering using different clustering algorithms (K-Means, Expectation Maximization, etc.) in R. The most attributes were engineered pursuing the goal to be basically meaningful for a customer segmentation.</p>

<p>Without feature selection, the results are very poor regarding evaluation criteria like ASW, BSS, WSS, etc.</p>

<p>My question now is whether I need to do a feature selection technique (wrapper/filter) or just select the features I think are most valuable for segmenting the customers. I found very different sources regarding this issue. The most authors say the features have to be selected concerning the business objective. Other sources propose feature selection methods for unsupervised learning. Is that really useful for a customer segmentation or is it only needed for image segmentation for instance?</p>

<p>My opinion is: Attributes might be economic valuable even so not useful for the clustering process and vice versa. This would mean I select manually the features.</p>

<p>I performed already a PCA which resulted also in poor results regarding clustering evaluation criteria. Therefore I obviously have to select only a few attributes in order to obtain a clear and stable clustering.</p>
"
"0.126155140059354","0.104541674697863","194768","<p>I am trying to understand how to interpret the results I get from LDA.</p>

<p>Running from the iris dataset in R, I can see the discriminant coefficients are in the model and then I can plot the model to see the clustering.  However none of this appears interpretable to me.</p>

<pre><code>library(MASS)
mdl&lt;-lda(Species~.,data=iris)
mdl
</code></pre>

<p>Mdl Output Below</p>

<pre><code>    Call:
lda(Species ~ ., data = iris)

Prior probabilities of groups:
    setosa versicolor  virginica 
 0.3333333  0.3333333  0.3333333 

Group means:
           Sepal.Length Sepal.Width Petal.Length Petal.Width
setosa            5.006       3.428        1.462       0.246
versicolor        5.936       2.770        4.260       1.326
virginica         6.588       2.974        5.552       2.026

Coefficients of linear discriminants:
                    LD1         LD2
Sepal.Length  0.8293776  0.02410215
Sepal.Width   1.5344731  2.16452123
Petal.Length -2.2012117 -0.93192121
Petal.Width  -2.8104603  2.83918785

Proportion of trace:
   LD1    LD2  
0.9912 0.0088 
</code></pre>

<p><strong>What I would like to be able to say is if sepal.length is y and petal.width is x then this flower will be z.</strong>  Clearly I wish to apply this to other datasets.  Alternatively I could get that from a tree structure.</p>

<p><strong>As a secondary question, how do you know how well the model fits?</strong> Looking at the correlations in this dataset, it appears to me that Petal.Length &amp; Petal.Width are strongly correlated so I would think the appropriate thing to do would be to drop one of them.  I know I can look at the accuracy on a test dataset, but is there anything here that tells me in the model how good this model is, an equivalent R^2?</p>

<p><strong>Finally, why are their only 2 discriminant variables given that there are 3 variables?</strong></p>

<p>I've seen some questions on this but never giving a full answer as to if and how this is possible to interpret.  To me the value in these models is being able to say, if you do x then you get y, you can then take that knowledge and apply it to the appropriate business context.  If this is not possible then where is the value in it?</p>
"
"0.072835704072923","0.0452678730212593","195096","<p>Probably a simple question but I'm trying to interpret BIC for k-means.</p>

<p>I have some k-means clustering and calculating BIC gives me a negative value, with a plot something like this:</p>

<pre><code>-75000 |                 xxxxxxxxxxx
       |            xxxxx           xxxxx
(BIC)  |        xxxx
       |     xxx
       |   xx
-80000 | x
       ------------------------------------
         2           (k)  25             50
</code></pre>

<p>I've searched around but I can't find any results that show a plot like this, apart from on another unanswered question (<a href=""http://datascience.stackexchange.com/questions/6508/k-means-incoherent-behaviour-choosing-k-with-elbow-method-bic-variance-explain"" title=""here"">here</a>).</p>

<p>Does a ""smaller"" BIC mean that my best number of clusters is ""2"" (most negative), or ""25"" (closer to zero), or is my plot just broken?</p>
"
"0.154507860787381","0.10669739994408","199821","<p>I am doing a project on the Gap Statistic from Tibshirani etc <a href=""http://www.web.stanford.edu/%7Ehastie/Papers/gap.pdf"" rel=""nofollow"">http://www.web.stanford.edu/%7Ehastie/Papers/gap.pdf</a></p>

<p>On page 4 of the pdf in section 4 on ""The computational implementation of the gap statistic"" two possible reference distributions to select Monte-Carlo samples from:</p>

<p>a) generate each reference feature uniformly overthe range of observed values for that feature</p>

<p>b) generate reference features from a uniform distribution over a box aligned with the principal components of the data. (The details involve assuming the columns have mean zero and computing the singular value decomposition, transforming the data, picking uniform random samples over the ranges of the columns in the transformed matrix, then back transforming the sample.)</p>

<p>The comments say the latter takes account of the shape of the distribution, and makes the procedure rotationally invariant, as long as the clustering method is invariant.</p>

<p>I can't tell from the help which method is used in the R function <code>clusGap</code> in the cluster library. I have tried looking at the code for the function (see below) and I think it doesn't do the singular value decomposition b) method - but just does the a) method. (I was looking at the part of the code following <code>for (b in 1:B)</code>). My question is am I right?</p>

<pre><code>&gt; clusGap
function (x, FUNcluster, K.max, B = 100, verbose = interactive(), 
    ...) 
{
    stopifnot(is.function(FUNcluster), length(dim(x)) == 2, K.max &gt;= 
        2, (n &lt;- nrow(x)) &gt;= 1, ncol(x) &gt;= 1)
    if (B != (B. &lt;- as.integer(B)) || (B &lt;- B.) &lt;= 0) 
        stop(""'B' has to be a positive integer"")
    if (is.data.frame(x)) 
        x &lt;- as.matrix(x)
    ii &lt;- seq_len(n)
    W.k &lt;- function(X, kk) {
        clus &lt;- if (kk &gt; 1) 
            FUNcluster(X, kk, ...)$cluster
            else rep.int(1L, nrow(X))
            0.5 * sum(vapply(split(ii, clus), function(I) {
                xs &lt;- X[I, , drop = FALSE]
                sum(dist(xs)/nrow(xs))
            }, 0))
        }
        logW &lt;- E.logW &lt;- SE.sim &lt;- numeric(K.max)
        if (verbose) 
            cat(""Clustering k = 1,2,..., K.max (= "", K.max, ""): .. "", 
                sep = """")
        for (k in 1:K.max) logW[k] &lt;- log(W.k(x, k))
        if (verbose) 
            cat(""done\n"")
        xs &lt;- scale(x, center = TRUE, scale = FALSE)
        m.x &lt;- rep(attr(xs, ""scaled:center""), each = n)
        V.sx &lt;- svd(xs)$v
    rng.x1 &lt;- apply(xs %*% V.sx, 2, range)
    logWks &lt;- matrix(0, B, K.max)
    if (verbose) 
        cat(""Bootstrapping, b = 1,2,..., B (= "", B, "")  [one \"".\"" per sample]:\n"", 
            sep = """")
    for (b in 1:B) {
        z1 &lt;- apply(rng.x1, 2, function(M, nn) runif(nn, min = M[1], 
            max = M[2]), nn = n)
        z &lt;- tcrossprod(z1, V.sx) + m.x
        for (k in 1:K.max) {
            logWks[b, k] &lt;- log(W.k(z, k))
        }
        if (verbose) 
            cat(""."", if (b%%50 == 0) 
                paste(b, ""\n""))
    }
    if (verbose &amp;&amp; (B%%50 != 0)) 
        cat("""", B, ""\n"")
    E.logW &lt;- colMeans(logWks)
    SE.sim &lt;- sqrt((1 + 1/B) * apply(logWks, 2, var))
    structure(class = ""clusGap"", list(Tab = cbind(logW, E.logW, 
        gap = E.logW - logW, SE.sim), n = n, B = B, FUNcluster = FUNcluster))
}
&lt;bytecode: 0x0000000010248578&gt;
&lt;environment: namespace:cluster&gt;
Warning messages:
1: In .HTMLsearch(query) : Unrecognized search field: title
2: In .HTMLsearch(query) : Unrecognized search field: keyword
3: In .HTMLsearch(query) : Unrecognized search field: alias
</code></pre>
"
"0.103005240524921","0.128036879932896","200254","<p>I have a dataset of two columns (we can call them x and y).
I understand that for cross-validation I need to split my data into k partitions, and for that the general consensus is that I use createFolds. However, I am a little confused about the output. It seems like I can't use createFolds on my entire dataset, it seems like I have to specify something like <strong>createFolds(data$x, k=5)</strong>. Then I get an output of 5 folds with a row of numbers, and I haven't been able to find documentation on what the output means exactly. </p>

<p>My second question is how to apply cross-validation in R to my test dataset? As I understand it, in R all I need to do to perform k-means clustering on is to specify kmeans(data, centres,...). So if I apply that command to my held-out dataset, how can I use that for cross validation on my test dataset? Thank you! </p>
"
"0.072835704072923","0.0452678730212593","203063","<p>I'm trying to perform a spatial clustering assignment by minimizing spatial distance while maximizing total weight within each cluster. </p>

<h3>My Data</h3>

<p>My data contains 3 columns and approximately 170 rows (example shown below)</p>

<pre><code>AvgWeight   lat          long
87.799      33.888102   -84.29321
165.258     31.459666   -83.51083
148.733     44.916657   -97.11346
484.038     43.020762   -88.26852
74.175      39.849156   -75.18159
83.861      42.02933    -93.60966
235.524     36.022863   -79.77895
</code></pre>

<h3>Goal</h3>

<p>I'd like to be able to cluster all of my spatial locations together while also maximizing the sum of my <code>AvgWeight</code> column within each cluster up to some adjustable constraint (Say 10,000 lbs). </p>

<h3>Additional Notes</h3>

<p>My previous clustering experience has been limited to <code>kmeans</code> in <code>R</code> so I was hoping that there would be some sort of variation I could implement to redefine my problem to fit with that methodology. However, I'm not sure how to go about minimizing geographic distance while maximizing my weight variable. I'm open to any suggestions on alternative methodologies preferably ones which can be implemented with <code>R</code>.</p>
"
"0.0515026202624605","0.064018439966448","204195","<p>Which clustering algorithm I should be using for the below type of dataset.
Say I have dataset with two variables (one is Age and another is say var). 
I converted them to the binary format as shown below. Kindly help me with the following questions</p>

<p>1.Should I use k-means directly on the data ?</p>

<p>2.Can I use hierarchical clustering ? if so which distance measuring algorithm</p>

<p>3.Or is there any better approach for this feature transformation and algorithm</p>

<pre><code>Age var
24  x1
54  x2
18  x1
45  x3
30  x2
</code></pre>

<p>I converted the categorical feature to this way</p>

<pre><code>Age var.x1 var.x2 var.x3
24  1       0      0
54  0       1      0
18  1       0      0
45  0       0      1
30  0       1      0
</code></pre>
"
"0.185695338177052","0.230821767874241","204760","<p>I would like to know how I can use clustering methods in R (in this case, Kmeans) if I have an ""unkind"" input matrix (I get this error log: </p>

<blockquote>
  <p>The TSS matrix is indefinite. There must be too many missing values. The index cannot be calculated.)</p>
</blockquote>

<p>I could see that I might get this error if my matrix produces negative eigenvalues (like, here: <a href=""http://stackoverflow.com/questions/20669596/nbclust-package-error"">http://stackoverflow.com/questions/20669596/nbclust-package-error</a>), but what I'm missing is the ""next step"" part. I could see a suggestion was to ""go back to the Data"", but what should I do then? Is there any transformation or something that might help? (I'm pretty new to R and clustering in general...)</p>

<p>The Data I'm using are the result of a survey (which I briefly transformed and scaled via the <code>scale</code> function in R) so I was wondering if there were some algorithms or methods I could use in order to go on with my analysis (from literature I couldn't find great help). Or, if you think this is unfixable or simply non the best solution, do you have any other suggestion for clustering my data? What I'm willing to do is to identify some clusters of possible users/customers of some services, depending on their usual habits (e.g.: if they use many social networks they will be more likely to use chat/whatsapp/app to ask for bank account information - I have both the information of their social network usage and their ways of communicating with a ""bank assistant"").</p>

<p>The Dataset consists of 994 rows and 103 columns. Don't know if it may help, but the code is simply this:</p>

<pre><code>Data2&lt;- read.csv(...)
bDataScale &lt;- scale(Data2)
nc &lt;- NbClust(bDataScale, min.nc=2, max.nc=993, method=""kmeans"")
</code></pre>

<p>And I get:</p>

<blockquote>
  <p>Error in NbClust(bDataScale, min.nc = 2, max.nc = 993, method = ""kmeans"") : 
    The TSS matrix is indefinite. There must be too many missing values. The index cannot be calculated.</p>
</blockquote>

<p>Thank you in advance for your help or any corrections,</p>

<p>Julia</p>

<p>P.S.: as it would be logical to expect, I get the same error also with the unscaled matrix.</p>
"
"0.136263125082667","0.145180175548748","204876","<p>This question related to this <a href=""http://stats.stackexchange.com/questions/204455/reducing-number-of-labels-in-a-dataset"">other one</a>, for which I have devised a strategy and now want some feedback on it.</p>

<p>My data consists of 434042 rows, each corresponding to an observation tagged with 1 of 20 labels. The variables for each observation are:</p>

<ul>
<li>5 are binary (T/F).</li>
<li>4 are categorical.</li>
<li>1 is ordinal (ranking).</li>
<li>3 is continuous.</li>
<li>1 is the time component.</li>
<li>1 label, corresponding to one of 20 each observation can take.</li>
</ul>

<p>My current logic is that to measure similarity, one must calculate the distance between observations, and then the distance between the center (or mean) of each category. Categories that are close together can be then grouped into a broader one.</p>

<p>To compute this, what I am doing is:</p>

<ul>
<li>Calculate the ratio of T/F observations within each label.</li>
<li>Calculate the % occurrence of each categorical value per categorical variable, for each label.</li>
<li>Treat the ordinal value as a continuous one.</li>
<li>Take the mean and standard deviation of each continuous variable.</li>
<li>Remove the time component from the analysis.</li>
</ul>

<p>I then pass on these label-specific summary variables to the <code>hclust</code>function in R, so the clustering algorithm can suggest me which categories I can group together.</p>

<p>Does this approach seem valid? Any recommendation or suggestion would be greatly appreciated.</p>
"
"0.136263125082667","0.145180175548748","206867","<p>I want to do cluster analysis of a product monthly sales during 5 years in 30 stores (my data are time series). I want to cluster the stores according to its seasonality.
This is an example of my data:</p>

<blockquote>
  <p>Month    Year   Shop1   Shop2   Shop3  ...</p>
  
  <p>12       2008   3000    5000     700 ...</p>
  
  <p>1        2009   2000    4000     500 ...</p>
  
  <p>2        2009   6000    5000     300 ...</p>
  
  <p>3        2009   7000    7000     600 ...</p>
  
  <p>4        2009   5000    4000     900 ...</p>
  
  <p>5        2009    5000    8000     1000 ...
  ...</p>
</blockquote>

<p>I have read several questions about this topic but I still do not understand the procedure or how to deal with this problem.</p>

<ol>
<li><p>I have found the package TSclust and I am considering using the dissimilarity index CORT. It covers both conventional measures for the proximity on observations and temporal correlation for the behavior proximity estimation. Do you think that is a good approach to use this measure?</p></li>
<li><p>I have also found the following procedure in: (<a href=""http://stats.stackexchange.com/questions/9475/time-series-clustering/19042#19042"">Time series clustering</a>), that consists in:</p></li>
</ol>

<p>Step 1</p>

<p>Perform a fast Fourier transform on the time series data. This decomposes your time series data into mean and frequency components and allows you to use variables for clustering that do not show heavy autocorrelation like many raw time series.</p>

<p>Step 2</p>

<p>If time series is real-valued, discard the second half of the fast Fourier transform elements because they are redundant.</p>

<p>Step 3</p>

<p>Separate the real and imaginary parts of each fast Fourier transform element.</p>

<p>Step 4</p>

<p>Perform model-based clustering on the real and imaginary parts of each frequency element.</p>

<p>Step 5</p>

<p>Plot the percentiles of the time series by cluster to examine their shape.</p>

<p>Have you ever done something like that? If so, could you provide an example code to carry out these steps?
Or do you know other steps?</p>

<ol start=""3"">
<li>I have also read the paper of Kumar, Patel and Woo: ""Clustering seasonality patterns in the presence of errors"", but i do not know how to reproduce their procedure in R.</li>
</ol>

<p>Any help would be helpful!</p>
"
"0.103005240524921","0.096027659949672","208235","<p>I have 3 sets of data that will cluster into 3 distinct groups. Each group is unbalanced meaning that there are different number of points in each cluster (cluster1 = 300, cluster2=50, cluster3=900). I'd like to know the best way to determine how and where to add a fourth cluster to these groups (i.e. which group can best accommodate for another cluster?). </p>

<p>What I've done to this point is preformed k-means clustering on each group. I've also computed the sum of squared error (SSE) for each group for k=2..15:</p>

<pre><code>wss = (nrow(data)-1)*sum(apply(data,2,var))
for (i in 2:15) wss[i] = sum(kmeans(data,centers=i)$withinss)
</code></pre>

<p>Now I have a table that looks something like this: </p>

<pre><code>""m""                ""d""                  ""e""
2813569.28725861    472355.029297394    5784658.19776383
1904107.38583311    318157.708967953    3506296.79084521
1648681.74129464    276421.573303097    2925602.69727677
1524909.05220068    259298.616113487    2685211.28641095
1465477.09738752    249822.780393088    2568056.73068996
1436268.67341786    244545.97418366 2492074.85372952
1418250.72662858    241004.184066676    2446687.20606313
1405885.99063985    237817.978851278    2411366.20542939
1395267.90044412    235054.310901762    2398095.04077986
1386557.42023853    232570.376337123    2374974.99416473
1379531.10300039    231250.918255804    2371436.20025802
1366720.2571021 229001.055906548    2352940.58741131
1360624.70769575    227234.173112882    2339408.89209271
1350787.58878321    226286.438463845    2333231.7286477
1345737.53064247    224540.981656591    2325617.52627248
</code></pre>

<p>Each line corresponds to the SSE for a particular k (row) in a particular group (column). I'm under the assumption that it would be incorrect to simply compare the row for each k and take the maximum value and use that corresponding group to add the cluster to. Am I correct in thinking this? Is there a better way to go about this? </p>

<p>Clarification Edit:
The three clusters in the table provided above (m, d, e) are the three distinct, initial datasets. By themselves they form 3 distinct cluster. Now, say I'd like to split one of these clusters into 2. Which do I choose? How do I determine which one to split?</p>
"
"NaN","NaN","208621","<p>I am trying to perform a clustering analysis in R according to Spearman correlation coefficient.</p>

<p>Could it happen that the analysis identifies only one big cluster as in the figure attached (the code is below the figure)? How would you interpret that? Please not that the same code generates a more meaningful dendogram if I change the matrix I use (in this case <code>mat_settings_old</code>).</p>

<p><a href=""http://i.stack.imgur.com/XIisq.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/XIisq.png"" alt=""enter image description here""></a></p>
"
"0.072835704072923","0.0905357460425185","209364","<p>I'm working on segmentation/clustering and trying to use Gaussian Mixture Modelling for Model-Based Clustering. I'm using the R package Mclust in order to come up with the best fit for my data.</p>

<p>All data is transformed to a uniform distribution with mean zero, standard deviation one (I know, not Gaussian) and the variables included are chosen based on earlier attempts using k-means, where the given variables seemed to be discriminating. Of course, k-means comes with some drawbacks (lack of statistical foundation, no control of cross-correlation etc,), and that's the reason I want to use Model-Based Clustering (or latent class analysis, with the package poLCA).</p>

<p>When using mclustBIC, many of the possible BICs are actually NA. I tried to reduce the dimension of the data, but this didn't improve the output. For example the VEV is only calculated for nr clusters 1:3, while it looks like it could improve for more clusters (see plot below).</p>

<p>Someone who experienced similar problems? And can someone help me into the right direction for finding the best model, using mclust? I would like to calculate other BICs with a higher number of clusters.</p>

<p>Help would be appreciated!</p>

<p><a href=""http://i.stack.imgur.com/YboGv.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YboGv.png"" alt=""enter image description here""></a></p>
"
"0.11516335992622","0.114519666862774","209446","<p>I have a very large data set with 9000 observations and 25 categorical variables, which I've transformed into binary data and preformed hierarchical clustering and K-modes clustering in R. </p>

<pre><code>library(klaR)
cluster &lt;- list()
for(k in 1:8) 
{
cluster[[paste0(""k."", k)]] &lt;- kmodes(data, k,iter.max=100)
}
</code></pre>

<p>I would like to know </p>

<p>1) if it's better to specify the number of modes <code>k</code> (where the algorithm chooses a random set of distinct rows from the data as the initial modes) or to specify the initial starting values/modes myself (give it a set of initial distinct cluster modes in replace of <code>k</code>). If the later, how do you decide on meaningful initial modes? For example for <code>k=4</code>, can I specify the initial modes to be 4 rows from the hierarchical binary clustering output where I cut the tree at <code>k=4</code>?</p>

<p>2) how many times I should run the algorithm and </p>

<p>3) if 100 iterations is adequate.</p>
"
"0.154507860787381","0.192055319899344","211102","<p>I have two data sets which contains information about subsystems in a bacterial metabolic model<br>
DataSet1: Behavior data of the subsystems<br>
DataSet2: Structural data of the same subsystems<br>
Then perform hierarchical clustering on the two data sets and obtain two dendrograms. My goal is to find how similar these dendrograms are.
I used FM index as suggested in <a href=""https://cran.r-project.org/web/packages/dendextend/vignettes/introduction.html"" rel=""nofollow"">this article</a> as it gives some sort of confidence interval on the final answer.</p>

<p>This is the code I used:</p>

<pre><code>#Hierarchical clustering for dataset 1
dInit &lt;- dist(initDataSource)
hcInit = hclust(dInit)

#Hierarchical clustering for dataset 2
dFinal &lt;- dist(finalDataSource)
hcFinal = hclust(dFinal)

#calculating the FM index
FM_index(cutree(hcInit, k=2), cutree(hcFinal , k=3))
</code></pre>

<p>Output:</p>

<pre><code>[1] 0.7462025
attr(,""E_FM"")
[1] 0.6253888
attr(,""V_FM"")
[1] 0.007626263
</code></pre>

<p>I need some help interpreting the output of FM index function. As stated in <a href=""https://cran.r-project.org/web/packages/dendextend/vignettes/introduction.html"" rel=""nofollow"">this article</a> there are three values in the output; FM index, expected FM index and the variance.  </p>

<p><strong>Question:</strong>
Is the variance value some sort of a p-value in which we could accept the or reject the H0 (null hypothesis -  the two trees are not similar). For example in the below scenario as variance is 0.007 (which is less than 0.01) we could reject H0 and the two dendrograms are similar by a value of 0.7462. </p>

<p>In a case where we get a very high FM index but a fairly high variance does that mean we are unable to reject the H0 and hence the dendrograms are not similar? </p>

<pre><code>&gt; FM_index(cutree(hcInit, k=3), cutree(hcFinal, k=3))
[1] 0.8461141
attr(,""E_FM"")
[1] 0.5515411
attr(,""V_FM"")
[1] 0.01347924
</code></pre>
"
"0.116796964356571","0.145180175548748","212293","<p>I have seen other users ask about recreating SAS's CCC output in other programs. This question, <a href=""http://stats.stackexchange.com/questions/29114/cubic-clustering-criterion-in-r"">Cubic clustering criterion in R</a>, has an answer that says to use <code>NbClust</code> to calculate, but that function does not handle large datasets well. It makes a call to <code>dist</code> that must allocate a 50 gig object. I have tried replacing the function with <code>cluster::daisy</code>, and <code>proxy::dist</code> from this SO question with the same memory problems.</p>

<p>Avoiding the <code>dist</code> call altogether may be the best option. I am looking to other options to recreate it. In this question <a href=""http://stats.stackexchange.com/questions/9016/how-to-define-number-of-clusters-in-k-means-clustering/9019#9019"">How to define number of clusters in K-means clustering?</a>, a user goes through the math provided by SAS. But I do not have the stats chops to translate that into R code. </p>

<p>Keeping it simple, I have <code>kmeans</code> output that provides total sum of squares (tot.ss), within.ss, between.ss, and I also calculated the $R^2$. </p>

<pre><code>kmeans(x = mydata, centers = 23, iter.max = ITER)
Within cluster sum of squares by cluster:
 [1]  91248.77  72122.06  78680.32  90402.25  86341.35 153533.51  73988.63  64903.32
 [9]  38334.98  84125.14  92366.93  74721.24 110313.76  96859.55  84516.37  56068.08
[17]  76201.69  86194.35  59526.00  53709.75  72503.21  50767.36  80531.94
 (between_SS / total_SS =  36.5 %)

Available components:

[1] ""cluster""      ""centers""      ""totss""        ""withinss""     ""tot.withinss"" ""betweenss""   
[7] ""size""         ""iter""         ""ifault""
</code></pre>

<p><strong><em>Can I calculate the CCC using these measures?</em></strong></p>

<hr>

<p>The second question has a long description from the SAS pdf. But I saw a <a href=""http://www.palgrave-journals.com/jibs/journal/v37/n4/fig_tab/8400206t2.html"" rel=""nofollow"">simplified equation here</a>.</p>

<p><a href=""http://i.stack.imgur.com/wKiHu.gif"" rel=""nofollow""><img src=""http://i.stack.imgur.com/wKiHu.gif"" alt=""enter image description here""></a> </p>

<p>where $E(R^2)$ is the expected $R^2$, and $R^2$ is the observed $R^2$, and $K$ is the variance-stabilizing transformation.</p>

<p>*Can this equation be completed by R's <code>kmeans</code> output and a calculated $R^2$</p>

<p><strong><em>Edit</em></strong></p>

<p>One reason why I am focusing on <code>kmeans</code> is that SAS users utilize <code>PROC FASTCLUS</code> when running large datasets. It is equivalent to R's kmeans function. The package <code>NbClust</code> calculates the CCC that I'm looking for, but it does it on the full data with euclidean distance, which is impossible for most computers. That is equivalent to SAS's <code>PROC CLUSTER</code>.</p>
"
"0.0892051550175079","0.0739221270954573","213313","<p>How can I calculate the probability of membership with R's <code>kmeans</code> output?</p>

<p>The output of <code>kmeans</code> is as follows:</p>

<pre><code>k &lt;- kmeans(iris[-5], 3)
str(k)
# List of 9
# $ cluster     : int [1:150] 2 3 3 3 2 2 2 2 3 3 ...
# $ centers     : num [1:3, 1:4] 6.31 5.18 4.74 2.9 3.62 ...
# ..- attr(*, ""dimnames"")=List of 2
# .. ..$ : chr [1:3] ""1"" ""2"" ""3""
# .. ..$ : chr [1:4] ""Sepal.Length"" ""Sepal.Width"" ""Petal.Length"" ""Petal.Width""
# $ totss       : num 681
# $ withinss    : num [1:3] 118.65 6.43 17.67
# $ tot.withinss: num 143
# $ betweenss   : num 539
# $ size        : int [1:3] 96 33 21
# $ iter        : int 2
# $ ifault      : int 0
# - attr(*, ""class"")= chr ""kmeans""
</code></pre>

<p>Is this enough information to copy what we get from <code>Mclust</code>?</p>

<pre><code>library(mclust)
m &lt;- Mclust(iris[-5])
head(m$z)
#           [,1]         [,2]
# [1,] 1.0000000 2.513157e-11
# [2,] 0.9999999 5.556411e-08
# [3,] 1.0000000 3.635438e-09
# [4,] 0.9999999 8.611811e-08
# [5,] 1.0000000 8.504494e-12
# [6,] 1.0000000 1.400364e-12
</code></pre>

<p>Obvious question is ""Why not use <code>mclust</code>?"". My data is too large to computationally do hierarchical clustering. I have tried with <code>Mclust</code>, <code>NbClust</code>, <code>vegan</code>, and many others. The call to <code>dist</code> that all of the functions use max out after a few hundred thousand rows.</p>

<p>I have seen some talk about probabilistic-D, and ""soft"" clustering, but I do not know how to implement it without changing the output of the original clusters from <code>kmeans</code>.</p>

<p><strong>Edit</strong>
I know that SAS is able to export probabilities with <code>PROC FASTCLUS</code>, but from what I hear it is taking a sample of the data to get the probabilities. That might be one route to take if I could figure out how it's doing the subsetting.</p>
"
"0.162865585496114","0.14171085778131","215961","<p>I have a set of distributions corresponding to predictions for how each of hundreds of players will perform.  I am looking to identify the distinct distributions of players.  In other words, I'm looking to identify the <a href=""https://stats.stackexchange.com/questions/57921/how-to-prove-the-number-of-distinct-distributions-in-a-group-of-distributions"">distinct distributions in a group of distributions</a>.</p>

<p>I know <code>Mclust()</code> can perform clustering on a vector, e.g.:</p>

<pre><code>library(""mclust"")

mydata &lt;- c(1,1,2,2,3,3,5,7,8,9,10)

summary(Mclust(mydata), parameters=TRUE)
Mclust(mydata)$classification
</code></pre>

<p>However, my data are a series of vectors (i.e., distributions)---one vector for each player, e.g.:</p>

<pre><code>set.seed(12345)
playerA &lt;- rnorm(10, mean=1, sd=.1)
playerB &lt;- rnorm(100, mean=1, sd=1)
playerC &lt;- rnorm(10, mean=2, sd=1)
playerD &lt;- rnorm(5, mean=2, sd=2)
playerE &lt;- rnorm(2, mean=3, sd=1)
playerF &lt;- rnorm(20, mean=5, sd=1)
playerG &lt;- rnorm(100, mean=7, sd=.5)
playerH &lt;- rnorm(10, mean=8, sd=2)
playerI &lt;- rnorm(5, mean=9, sd=1)
playerJ &lt;- rnorm(10, mean=10, sd=.5)
</code></pre>

<p>How can I perform clustering to identify the distinct clusters of players based on their distributions, focusing on differences in their means, rather than their variances.  I don't want to just cluster the mean values, though, because I want to take into account the variances to know whether their means are in the same or in a different cluster (e.g., high variability in two players' distributions may indicate that two players with different means are in the same cluster).  Ideally, I'd like two players with the same mean and different variability distributions to be in the same cluster.  Is there a way to do this using the <code>mclust</code> or another package in R?  I've considered doing pairwise t-tests, but this seems that it would be heavily dependent on the sample size in each distribution (which I'd rather it not be <em>too</em> dependent on sample size, if possible).  I've also considered comparisons based on effect size (Cohen's d).  I'm not sure what other options there are (e.g., Tukey's HSD, hierarchical clustering, etc.)</p>
"
"0.072835704072923","0.0905357460425185","216046","<p>I am trying to find a clustering solution with the help of <code>flexclust</code> package in R. The following code has been adapted from the vignette for the <code>flexclust</code> package:</p>

<pre><code>library(flexclust)
library(ISLR)

Auto &lt;- read.table(Auto)
AutoMinus &lt;- Auto[ -c(8:9)]
AutoMinus.mat &lt;- as.matrix(AutoMinus)

# Setting the parameters

fc_cont &lt;- new(""flexclustControl"")  
fc_cont@tolerance &lt;- 0.01   
fc_cont@iter.max &lt;- 25
fc_cont@verbose &lt;- 1
fc_family &lt;- ""kmeans""             

seed1 &lt;- 12345
fc_seed &lt;- seed1
num_clusters &lt;- 3
set.seed(fc_seed)

AutoMinus.cl &lt;- kcca(AutoMinus.mat, k = num_clusters, save.data = TRUE, control = fc_cont, family = kccaFamily(fc_family))
summary(AutoMinus.cl)

cluster info:
  size  av_dist max_dist separation
1  122 263.0698 525.3528   480.3347
2  180 217.1523 610.9503   478.7658
3   90 290.9777 905.5731   551.2422
</code></pre>

<p>Every time I change the seed, the output changes. I am evaluating different outputs based on lowest <code>av_dist</code>, lowest <code>max_dist</code> and minimum <code>separation</code>. My understanding is <code>separation</code> is within a given cluster. I tried to find the definition of <code>separation</code> in the documentation, but couldn't find it. My questions are:</p>

<ol>
<li>How do I set the seed that will give me the best/optimal(?) solution?</li>
<li>Are there any general good practices for initializing a seed value?</li>
<li>Is my understanding of <code>separation</code> correct?</li>
</ol>

<p>Thank you!</p>
"
"0.162865585496114","0.161955266035783","217467","<p>I'm working on a project that requires some clustering analysis.  In performing the analysis, I noticed something that seemed odd to me.  I understand that in k-means the total sum of squares (total distance of all observations from the global center) equals the between sum of squares (distance between the centroids) plus the total within sum of squares (sum of the distances of each observation to its centroid).  But I also see that total sum of squares is not exactly equal to the total variance of the distribution, which I don't understand.  What I've noticed is that the two numbers (total sum of squares from k-means analysis vs. total variance) get closer to each other as the sample sizes get larger.  Here's a quick simulation in R that shows what I'm talking about:</p>

<pre><code>require(dplyr)
compare_sd_km &lt;- function(numObs){
    set.seed(3)
    x &lt;- rnorm(numObs, 0, 1)
    y &lt;- rnorm(numObs, 0, 1)

    km &lt;- kmeans(data.frame(x,y), centers = 5)

    kmeansMeasure &lt;- (km$betweenss + km$tot.withinss) / numObs
    varianceMeasure &lt;- var(x) + var(y)
    return(c(kmeansMeasure, varianceMeasure))
}

numObsMultipliers &lt;- c(1:5)
numObsMultipliers &lt;- sapply(numObsMultipliers, function(x) 10^x)
comparisons &lt;- lapply(numObsMultipliers, function(x)  compare_sd_km(x))
comparisons &lt;- as.data.frame(do.call(rbind, comparisons))
comparisons$observations &lt;- numObsMultipliers
colnames(comparisons) &lt;- c('kmeansMeasure', 'varianceMeasure', 'observations')
comparisons &lt;- mutate(comparisons, difference = kmeansMeasure - varianceMeasure)
comparisons &lt;- comparisons[, c(3,1,2,4)]

#### RESULT ####
  observations kmeansMeasure varianceMeasure     difference
1           10      1.142764        1.269738 -0.12697379740
2          100      1.920365        1.939762 -0.01939762318
3         1000      1.988562        1.990553 -0.00199055288
4        10000      2.031035        2.031238 -0.00020312381
5       100000      2.007437        2.007457 -0.00002007457
</code></pre>

<p>Any ideas what's going on here?  Rounding issues?  Something having to do with how the algorithms are implemented?  Am I calculating something incorrectly? Or do total sum of squares and total variance actually mean something substantively different?  Thanks for any help anyone can provide.</p>
"
"NaN","NaN","218913","<p>I am new to clustering.  My apology if this has been asked before.</p>

<p>I'd like to differentiate two distinct linear populations within sample matrix, and tag them differently.  Apparently k-means couldn't do a good job, which is shown in the plot below.  Does anybody have any good suggestion?</p>

<p>Example code:</p>

<pre><code>x &lt;- c(1:1000)
y &lt;- x
y2 &lt;- 10 * x
sample &lt;- matrix(c(x, x, y, y2), nrow = 2000)
</code></pre>

<p><a href=""http://i.stack.imgur.com/O3OGC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/O3OGC.png"" alt=""enter image description here""></a></p>

<p>Thanks a lot!</p>
"
"0.103005240524921","0.096027659949672","220449","<p>I have the following dataframe </p>

<pre><code>&gt; str(kmeans)
'data.frame':   29271 obs. of  3 variables:
 $ x: num  -4.5 -1.514 -0.203 -2.016 -3.525 ...
 $ y: num  -1.127 1.195 0.622 3.138 -1.382 ...
 $ z: num  -0.607 -2.354 1.121 0.318 1.958 ...
</code></pre>

<p>x, y and z are basically PC1, PC2 and PC3 components after PCA of the original dataset. The original dataset consisted of 10 features. </p>

<p>I have performed K-means on kmeans dataset. Below is the graph
<a href=""http://i.stack.imgur.com/6LyBz.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6LyBz.png"" alt=""K means clustering""></a> </p>

<p>How do I extract the original features from these clusters. For example if I identify cluster 1 then what are the values of corresponding attributes of the original 10 features not of the principal components.</p>
"
"0.291848181487276","0.234734279876976","221144","<p>(This post is a repost of a question I posted yesterday (now deleted), but I've tried to scale back volume of words and simplify what I'm asking)</p>

<p>I'm hoping to get some help interpreting a kmeans script and output I have created. This is in the context of text analysis. I created this script after reading several articles online on text analysis. I have linked to some of them them below.</p>

<p>Sample r script and corpus of text data I will refer to throughout this post:</p>

<pre><code>library(tm) # for text mining

## make a example corpus
# make a df of documents a to i
a &lt;- ""dog dog cat carrot""
b &lt;- ""phone cat dog""
c &lt;- ""phone book dog""
d &lt;- ""cat book trees""
e &lt;- ""phone orange""
f &lt;- ""phone circles dog""
g &lt;- ""dog cat square""
h &lt;- ""dog trees cat""
i &lt;- ""phone carrot cat""
j &lt;- c(a,b,c,d,e,f,g,h,i)
x &lt;- data.frame(j)    

# turn x into a document term matrix (dtm)
docs &lt;- Corpus(DataframeSource(x))
dtm &lt;- DocumentTermMatrix(docs)

# create distance matrix for clustering
m &lt;- as.matrix(dtm)
d &lt;- dist(m, method = ""euclidean"")

# kmeans clustering
kfit &lt;- kmeans(d, 2)
#plot â€“ need library cluster
library(cluster)
clusplot(m, kfit$cluster)
</code></pre>

<p>That's it for the script. Below are the output of some of the variables in the script:</p>

<p>Here's x, the data frame x that was transformed into a corpus:</p>

<pre><code> x
                       j
    1 dog dog cat carrot
    2      phone cat dog
    3     phone book dog
    4     cat book trees
    5       phone orange
    6  phone circles dog
    7     dog cat square
    8      dog trees cat
    9   phone carrot cat
</code></pre>

<p>An here's the resulting document term matrix dtm:</p>

<pre><code>    &gt; inspect(dtm)
&lt;&lt;DocumentTermMatrix (documents: 9, terms: 9)&gt;&gt;
Non-/sparse entries: 26/55
Sparsity           : 68%
Maximal term length: 7
Weighting          : term frequency (tf)

    Terms
Docs book carrot cat circles dog orange phone square trees
   1    0      1   1       0   2      0     0      0     0
   2    0      0   1       0   1      0     1      0     0
   3    1      0   0       0   1      0     1      0     0
   4    1      0   1       0   0      0     0      0     1
   5    0      0   0       0   0      1     1      0     0
   6    0      0   0       1   1      0     1      0     0
   7    0      0   1       0   1      0     0      1     0
   8    0      0   1       0   1      0     0      0     1
   9    0      1   1       0   0      0     1      0     0
</code></pre>

<p>And here is the distance matrix d</p>

<pre><code>&gt; d
         1        2        3        4        5        6        7        8
2 1.732051                                                               
3 2.236068 1.414214                                                      
4 2.645751 2.000000 2.000000                                             
5 2.828427 1.732051 1.732051 2.236068                                    
6 2.236068 1.414214 1.414214 2.449490 1.732051                           
7 1.732051 1.414214 2.000000 2.000000 2.236068 2.000000                  
8 1.732051 1.414214 2.000000 1.414214 2.236068 2.000000 1.414214         
9 2.236068 1.414214 2.000000 2.000000 1.732051 2.000000 2.000000 2.000000
</code></pre>

<p>Here is the result, kfit:</p>

<pre><code>&gt; kfit
K-means clustering with 2 clusters of sizes 5, 4

Cluster means:
         1        2        3        4        5        6        7        8        9
1 2.253736 1.194938 1.312096 2.137112 1.385641 1.312096 1.930056 1.930056 1.429253
2 1.527463 1.640119 2.059017 1.514991 2.384158 2.171389 1.286566 1.140119 2.059017

Clustering vector:
1 2 3 4 5 6 7 8 9 
2 1 1 2 1 1 2 2 1 

Within cluster sum of squares by cluster:
[1] 13.3468 12.3932
 (between_SS / total_SS =  29.5 %)

Available components:

[1] ""cluster""      ""centers""      ""totss""        ""withinss""     ""tot.withinss"" ""betweenss""    ""size""         ""iter""        
[9] ""ifault""      
</code></pre>

<p>Here is the resulting plot:
<a href=""http://i.stack.imgur.com/vBvTa.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/vBvTa.png"" alt=""enter image description here""></a></p>

<p>I have several questions about this:</p>

<ol>
<li>In calculating my distance matrix d (a parameter used in kfit calculation) I did this: <code>d &lt;- dist(m, method = ""euclidean"")</code>. Another article I encountered did this: <code>d &lt;- dist(t(m), method = ""euclidean"")</code>. Then, separately on a <a href=""http://stackoverflow.com/questions/38003622/within-the-context-of-a-document-term-matrix-what-exactly-are-x-and-y-axis-in-k"">SO question</a> I posted recently someone commented ""kmeans should be run on the data matrix, not on the distance matrix!"". Presumably they mean <code>kmeans()</code> should take m instead of d as input. Of these 3 variations which/who is ""right"". Or, assuming all are valid in one way or another, which would be the conventional way to go in setting up an initial baseline model?</li>
<li>As I understand it, when kmeans function is called on d, what happens is that 2 random centroids are chosen (in this case k=2). Then r will look at each row in d and determine which documents are closest to which centroid. Based on the matrix d above, what would that actually look like? For example if the first random centroid was 1.5 and the second was 2, then how would document 4 be assigned? In the matrix d doc4 is 2.645751 2.000000 2.000000 so (in r) mean(c(2.645751,2.000000,2.000000)) = 2.2 so in the first iteration of kmeans in this example doc4 is assigned to the cluster with value 2 since it's closer to that than to 1.5. After this the mean of the cluster is reclauculated as a new centroid and the docs reassigned where appropriate. Is this right or have I completely missed the point?</li>
<li>In the kfit output above what is ""cluster means""? E.g., Doc3 cluster 1 has a value of 1.312096. What is this number in this context? [edit, since looking at this again a few days after posting I can see that it's the distance of each document to the final cluster centers. So the lowest number (closest) is what determines which cluster each doc is assigned].</li>
<li>In the kfit output above, ""clustering vector"" looks like it's just what cluster each doc was assigned to. OK.</li>
<li>In the kfit output above, ""Within cluster sum of squares by cluster"". What is that? <code>13.3468 12.3932 (between_SS / total_SS =  29.5 %)</code>. A measure of the variance within each cluster, presumably meaning a lower number implies a stronger grouping as opposed to a more sparse one. Is that a fair statement? What about the percentage  given 29.5%. What's that? Is 29.5% ""good"". Would a lower or higher number be preferred in any instance of kmeans? If I experimented with different numbers of k, what would I be looking for to determine if the increasing/decreasing number of clusters has helped or hindered the analysis?</li>
<li>The screenshot of the plot goes from -1 to 3. What is being measured here? As opposed to education and earnings, height and weight, what is the number 3 at the top of the scale in this context?</li>
<li>In the plot the message ""These two components explain 50.96% of the point variability"" I already found some detailed info <a href=""http://stats.stackexchange.com/questions/141280/understanding-cluster-plot-and-component-variability"">here</a> (in case anyone else comes across this post - just for completeness of understanding kmeans output wanted to add here.).</li>
</ol>

<p>Here's some of the articles I read that helped me to create this script: </p>

<ul>
<li><a href=""https://rstudio-pubs-static.s3.amazonaws.com/31867_8236987cf0a8444e962ccd2aec46d9c3.html"" rel=""nofollow"">Basic Text Mining in R</a></li>
<li><a href=""https://eight2late.wordpress.com/2015/07/22/a-gentle-introduction-to-cluster-analysis-using-r/"" rel=""nofollow"">A Gentle Introduction into Cluster Analysis using R (text mining based article)</a></li>
</ul>
"
"0.0892051550175079","0.110883190643186","221832","<p>I have a dataset like df and I want to cluster the data in R. These variables are binary showing that a person uses a programming language or not. I have these questions:</p>

<p>1- How can I visualize my data? Up to now, I tried heatmap (with clusters on both rows and columns)</p>

<p>2- What is the best distance calculation method for this data?</p>

<p>3- What is the best clustering method for this data?</p>

<p>4- Should I consider normalizing the variables i.e. (value-mean)/sd before clustering? All the variables are either zero or one but the variables have different standard deviations.</p>

<pre><code>  language &lt;- c(
  ""R, Matlab"",
  ""Assembly, R, Go, Rust"",
  ""Java, Javascript, Ruby, SQL"",
  ""Java, Ruby"",
  ""C, C++"",
  ""PHP, Javascript, Ruby, Assembly, Swift, R, Matlab, Go, Haskell"",
  ""R"",
  ""Perl, Javascript, R"",
  ""Javascript, Ruby, Bash"",
  ""Python, PHP, Javascript"",
  ""Java"",
  ""Java, C""
)

df &lt;-as.data.frame(language,stringsAsFactors = FALSE)


df &lt;- reshape2::recast(
  data =  setNames(strsplit(language, "", "", T), language), 
  formula = L1~value, 
  fun.aggregate = length
)

str(df)
</code></pre>
"
"0.11516335992622","0.0858897501470802","221850","<p>I'm reading up on kmeans and following a blog post to do some text analysis.</p>

<p>I watched a <a href=""https://www.youtube.com/watch?v=Ao2vnhelKhI"" rel=""nofollow"">helpful video</a> by Andrew Ng fro Coursera which really helped my understanding of what is going on. Here is a screen shot from the video:</p>

<p><a href=""http://i.stack.imgur.com/XdHhG.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/XdHhG.png"" alt=""kmeans clustering""></a></p>

<p>So far kmeans makes sense, start with K cluster centroids, assign each point to a cluster based on distance (Euclidean?), recalculate mean, repeat. </p>

<p>But I'm also following <a href=""https://rstudio-pubs-static.s3.amazonaws.com/31867_8236987cf0a8444e962ccd2aec46d9c3.html"" rel=""nofollow"">this</a> blog post on text analysis in R. Following the article I make a document term matrix.</p>

<p>Context is online survey results. Let's say there are e.g. 10k survey results and a total of 15k ""tokens"", so a dtm of 10k*15k.</p>

<p>Further down the article we are shown an example of kmeans clustering on the dtm:</p>

<pre><code>library(fpc)   
d &lt;- dist(t(dtmss), method=""euclidian"")   
kfit &lt;- kmeans(d, 2)   
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)  
</code></pre>

<p>And it works.</p>

<p>But I'm trying to understand how. An example in kmeans clustering I found online was weight and height on the x,y axis for determining how to split a population for clothing sizes of small, medium and large. That makes sense.</p>

<p>But in the context of a DTM, what would be on the x and y axis in the screen shot above? It's just not clicking. </p>
"
"0.185695338177052","0.142044164845687","223035","<p>I am an ecology graduate with a decent practical familiarity with statistics in R, but limited experience of approaches such as PCA, and Cluster Analysis. 
I am currently faced with the challenge of trying to apply my skills to an entirely unfamiliar problem:  my dad is writing a book on archaeological finds of blades, has collected data on 176 finds and has tasked me with analysing it.  </p>

<p>The data selected for analysis is structured thus:   </p>

<pre><code> Blade.length     Max.width     Max.thickness     Shape     Broken.back        Type   

 Min.   :165.0   Min.   :20.00   Min.   : 3.500   A   :70   Min.   :0.0000   Cs   :39  
 1st Qu.:220.0   1st Qu.:28.75   1st Qu.: 5.000   B   : 8   1st Qu.:0.0000   Hbs  :15  
 Median :270.0   Median :34.00   Median : 6.000   C   :14   Median :0.0000   Lbs  :17  
 Mean   :311.5   Mean   :35.20   Mean   : 6.464   D   :14   Mean   :0.2686   Ls   :23  
 3rd Qu.:353.0   3rd Qu.:39.00   3rd Qu.: 7.875   E   :30   3rd Qu.:0.5000   Ns   :43  
 Max.   :760.0   Max.   :62.00   Max.   :11.000   F   :12   Max.   :1.0000   Small:35  
 NA's   :9       NA's   :4       NA's   :86       NA's:28   NA's   :1        NA's : 4 
</code></pre>

<p>Shape is a variable of categories pertaining to the shape of the tip of the blades - these categories are in no particular order.  Broken.back is a different way of looking at ""shape"", effectively binary, although some cases are ""in between"" and have been entered as 0.5.   ""Type"" is a supplementary variable referring to what each blade has been identified as, using a pre-existing typology. Part of the exercise is to examine if this pre-existing typology is fit for purpose. </p>

<p>The dataset is, necessarily, incomplete, with NAs in all variables, although blades with lots of missing data have been excluded from the analysis.  Within the sample remaining, the most incomplete column is blade thickness, with 48% NAs.  </p>

<p>So far I have attempted to visualise the data by means of factorial analysis of mixed data, with imputation, using packages MissMDA and FactoMineR.   However I've found myself bewildered by the number of options and what approach is appropriate for the sort of data I have. </p>

<p>More importantly, I am looking to conduct hierarchical cluster analysis of the data to examine the relatedness of different finds and try and statistically define types (<a href=""http://www.r-bloggers.com/hierarchical-clustering-in-r-2/"" rel=""nofollow"">http://www.r-bloggers.com/hierarchical-clustering-in-r-2/</a>), so far using HCLUST, Dist, and vegdist (package: Vegan).   However, I am not clear as to;  </p>

<ul>
<li>How to manage, prepare or transform the types of data I have for this type of analysis.</li>
<li>What dissimilarity index method would be most appropriate in this context.</li>
<li>What type of clustering / linkage method would be most appropriate in this context. </li>
</ul>

<p>Sorry for the long question. As you can see I am quite bewildered and out of my depth.  Thanks in advance. </p>
"
"0.072835704072923","0.0905357460425185","224445","<p>I have a matrix of data (215 rows, 618 cols) the data is xy positional data from a square surface. Most of the data is 0, and very few are 1. When I plot this data I see that the 1's form 2 small clusters...I'd like to use a clustering technique to automatically colour the clusters and to know how many 1's (cells) make up each cluster..?
Can I use kmeans or DBSCAN for this..? the examples i've seen answered seem to be on xy numbers data (if that makes sense) and not xy positional data with only 1's &amp; 0's.<a href=""http://i.stack.imgur.com/bEMBW.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bEMBW.jpg"" alt=""enter image description here""></a></p>

<p>Any help would be appreciated.
Paul.</p>
"
"0.116796964356571","0.120983479623957","224449","<p>I am conducting clustering analysis in which I am using three clustering algorithms <code>K-means</code>, <code>Spectral Clustering</code>, and <code>Hierarchical clustering</code> on 3 datasets in UCI repository. </p>

<p>I have used <code>R</code> packages to conduct clustering analysis and got the results such as Size of clusters, cluster vector, cluster means, Within cluster sum of squares, and grouping of cluster by Class. </p>

<p>Following is an example of my <code>K-means</code> on the Pima Indian diabetes data in UCI repository:</p>

<pre><code>diabetes &lt;- read.csv(url(""http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data""), header = FALSE)

names(diabetes)&lt;- c(""No.ofTimesPregnant"", ""GlucoseConcentration"", ""BloodPressure"", ""TricepSkinThickness"", ""insulin"", ""BMI"", ""PedigreeFunction"", ""Age"", ""Class"") 

set.seed(20)

KmeansCluster &lt;- kmeans(diabetes[, 1:8], 4, nstart = 20, iter.max=10)

pcol &lt;- as.character(diabetes$Class)
pairs(diabetes[1:8], pch = pcol, col = c(""green"", ""red"") KmeansCluster$cluster])
KmeansCluster
table(KmeansCluster$cluster, diabetes$Class)
</code></pre>

<p>I wish to know how I can compare the results of each clustering algorithm? So that I can say that particular clustering algorithm is best for this dataset. More specifically to say, what metric should I choose and how I can get that metric in <code>R</code> (For example, it would be helpful if you could tell me how to get those metric on my above <code>R</code> code for <code>K-means</code>)?.  </p>

<p>As I know the diameter of the cluster and average distance of each cluster is used as a measure to compare clustering algorithm in general. </p>
"
"0.154507860787381","0.170715839910528","224509","<p>I'm conducting a meta-analysis on standardised mean difference scores. Some studies provide multiple effect sizes, thereby violating the assumption of independence. An example is given below (all effect sizes were calculated with regard to a pre-test). In study A, all participants received the same treatment (watching a video), and were tested repeatedly. In study B, there were two different treatment groups (one group watched a video, the other group listened to an audio book), and everyone was tested once. Study C provided only one effect size.</p>

<pre><code>study        treatment          testing_moment         effect_size

A            video              immediately            0.6
A            video              delayed                0.5
B            video              immediately            0.9
B            audio_book         immediately            0.7
C            audio_book         delayed                0.4
</code></pre>

<p>I'm using the <em>metafor</em> package in <em>R</em>, in which you can fit a multilevel model to account for non-independent sampling errors. </p>

<p>What I've done:</p>

<pre><code>rma.mv(effect_size_vector, variance_vector, mods = ~ testing_moment, 
  random = ~ 1 | treatment/study, data = rev)
</code></pre>

<p>Could anyone please have a look whether this approach is correct? I'm especially unsure about whether I've correctly indicated the clustering using slash (/) (this decision was based on <a href=""http://conjugateprior.org/2013/01/formulae-in-r-anova/"" rel=""nofollow"">this page</a>), and whether the model as a result indeed takes into account the non-independence of effect sizes. </p>

<p>I'm also wondering whether somehow it should be corrected that the samples in study A are dependent and in study B they are independent. Or is that already accounted for by virtue of the treatment variable being the same for both samples in study A?</p>
"
"0.154507860787381","0.170715839910528","230717","<p>I have question about output of <code>PROXIMUS</code> algorithm. In the example of
use <a href=""https://en.wikibooks.org/wiki/Data_Mining_Algorithms_In_R/Clustering/Proximus#Algorithm"" rel=""nofollow"">here</a>.
In Case Study section is all clear, but I do not know what way they use
for get clustered words to groups: </p>

<pre><code>Group 1 (computers) = {intel, computer, software, linux, windows, Firefox, explorer, programming}
Group 2 (authors) = {kuth, shakespeare, grisham, asimov, book} 
Group 3 (noise) = {love}
</code></pre>

<p>How can I get this output for my data? </p>

<p>Next in
the example section is graph. How do I read this graph?</p>

<p>My data are:</p>

<pre><code>objects cat1    cat2    cat3    cat4 ...
A       TRUE    FALSE   FALSE   FALSE
B       TRUE    FALSE   TRUE    FALSE
C       TRUE    FALSE   FALSE   FALSE
D       FALSE   TRUE    TRUE    TRUE
E       TRUE    TRUE    TRUE    TRUE
F       TRUE    FALSE   TRUE    FALSE
</code></pre>

<p>After apply of <code>Proximus</code> algorithm I get this output:</p>

<pre><code>pr &lt;- proximus(x, max.radius=8, debug=TRUE)
#Non-Zero: 55
#Sparsity: 0.48
#  0 [6,3,5] 1 &gt;
#  1 [3,3,5] 1 * 1
#  1 [3,1,0] 1 &gt;
#  2 [1,1,0] 1 * 2
#  2 [2,1,0] 1 &gt;
#  3 [1,1,0] 1 * 3
#  3 [1,1,0] 1 * 4


summary(pr)
#Size Length Radius Error Fnorm Jsim Valid
#1    3     16      5  0.16     3 0.81  TRUE
#2    1      9      0  0.00     0 1.00  TRUE
#3    1      4      0  0.00     0 1.00  TRUE
#4    1      2      0  0.00     0 1.00  TRUE
</code></pre>

<p>So it means that 3 objects are in one cluster and all other objects have
own cluster. What way I can use for get list of objects in cluster? And I get this: <a href=""http://i.stack.imgur.com/uQAxz.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/uQAxz.png"" alt=""graph""></a> </p>

<p>How do I read this graph? </p>

<p>Many thanks for your help!</p>
"
"0.072835704072923","0.0905357460425185","232269","<p><strong>Clustering sea waves data in R</strong></p>

<p>I have proceed in clustering of storm's energy data using different clustering  methods (kmeans, hclust, agnes, funny) in R (always in 5 clusters) but even if it is easy to choose the best method for my work, I need a computational (and not theoretical) method or a package to compare and evaluate the methods via their results. Do you believe that there is something?</p>

<p>Thanks in advance,</p>
"
"0.103005240524921","0.128036879932896","232590","<p>I have a number of groups with monthly data from 2010 to 2016. It's over 80 groups. I succesfully ran an ARIMA model with the montly data but with the sales data summed up (without groups). </p>

<p>Now I'd like to compare the performance with a per group model that runs an ARIMA model for each group and maybe later consider another type of grouping (geographical location, clustering, etc.)</p>

<p>I ran my original model with the following code:</p>

<pre><code>        Datos &lt;- read.csv(""C:/Users/borja.sanz/Desktop/Borja/Forecasting/V`enter code here`entas/Datos para Forecast.csv"")
        options(scipen=999)
        library(lubridate)
        Datos$Fecha = dmy(Datos$Fecha)

        #Declare time series
        tsDatos&lt;-ts(Datos$VentaLocal,start = c(2010,1),frequency = 12)
        plot(tsDatos)
        library(forecast)
        library(dplyr)

        #AutoArima Model
        m_aa = auto.arima(tsDatos)
        f_aa = forecast(m_aa, h=36)
        plot(f_aa)

#Create the forecasts along with the lower and upper bound
    forecast_df = data.frame(prediction=f_aa$mean,
                             abajo=f_aa$lower[,2],
                             arriba=f_aa$upper[,2],
                             date=last_date + seq(1/12, 3, by=1/12))
    forecast_df
</code></pre>

<p>This is how my data looks like:</p>

<pre><code>       Group    Year    Month   Date    Sales
1   2010    1   1/01/2010   134536.625
1   2010    2   1/02/2010   117506.625
1   2010    3   1/03/2010   132153.75
1   2010    4   1/04/2010   129723.125
1   2010    5   1/05/2010   135834.5
1   2010    6   1/06/2010   130115.375
1   2010    7   1/07/2010   144716
1   2010    8   1/08/2010   137195
1   2010    9   1/09/2010   137522.875
1   2010    10  1/10/2010   187063
1   2010    11  1/11/2010   162002.75
1   2010    12  1/12/2010   262297.375
1   2011    1   1/01/2011   177291.25
1   2011    2   1/02/2011   154816
1   2011    3   1/03/2011   171231.125
1   2011    4   1/04/2011   217717
1   2011    5   1/05/2011   178767.75
1   2011    6   1/06/2011   180817.75
1   2011    7   1/07/2011   216927.125
1   2011    8   1/08/2011   204509.125
1   2011    9   1/09/2011   199449.5
1   2011    10  1/10/2011   243812.125
1   2011    11  1/11/2011   232135.875
1   2011    12  1/12/2011   330854.75
1   2012    1   1/01/2012   217123.875
1   2012    2   1/02/2012   200558
1   2012    3   1/03/2012   215689.5
1   2012    4   1/04/2012   245500.25
1   2012    5   1/05/2012   219687.25
1   2012    6   1/06/2012   243345.625
1   2012    7   1/07/2012   249042
1   2012    8   1/08/2012   198443.75
1   2012    9   1/09/2012   209157.375
1   2012    10  1/10/2012   234089
1   2012    11  1/11/2012   237531
1   2012    12  1/12/2012   365301.25
1   2013    1   1/01/2013   211129.375
1   2013    2   1/02/2013   185249.625
1   2013    3   1/03/2013   256565.625
1   2013    4   1/04/2013   183549.5
1   2013    5   1/05/2013   189698.25
1   2013    6   1/06/2013   207955.625
1   2013    7   1/07/2013   230764.125
1   2013    8   1/08/2013   212551.625
1   2013    9   1/09/2013   201329.5
1   2013    10  1/10/2013   242745.125
1   2013    11  1/11/2013   261893.375
1   2013    12  1/12/2013   418313.25
1   2014    1   1/01/2014   205532.75
1   2014    2   1/02/2014   170487.75
1   2014    3   1/03/2014   196077
1   2014    4   1/04/2014   221760.875
1   2014    5   1/05/2014   198185
1   2014    6   1/06/2014   204919.25
1   2014    7   1/07/2014   218972.75
1   2014    8   1/08/2014   221439.875
1   2014    9   1/09/2014   195888.375
1   2014    10  1/10/2014   234595.75
1   2014    11  1/11/2014   259712.875
1   2014    12  1/12/2014   355691.875
1   2015    1   1/01/2015   205156.25
1   2015    2   1/02/2015   185358.875
1   2015    3   1/03/2015   218555.75
1   2015    4   1/04/2015   204233.625
1   2015    5   1/05/2015   212160.625
1   2015    6   1/06/2015   207217.25
1   2015    7   1/07/2015   225723.75
1   2015    8   1/08/2015   205902.625
1   2015    9   1/09/2015   196940.625
1   2015    10  1/10/2015   250916
1   2015    11  1/11/2015   236835.125
1   2015    12  1/12/2015   358327.625
2   2010    1   1/01/2010   227175.875
2   2010    2   1/02/2010   205042
2   2010    3   1/03/2010   239206.375
2   2010    4   1/04/2010   212059.875
2   2010    5   1/05/2010   232789
2   2010    6   1/06/2010   247876.125
2   2010    7   1/07/2010   278557
2   2010    8   1/08/2010   270410.125
2   2010    9   1/09/2010   251060.375
2   2010    10  1/10/2010   302738.625
2   2010    11  1/11/2010   266869.75
2   2010    12  1/12/2010   272978.75
2   2011    1   1/01/2011   238614.5
2   2011    2   1/02/2011   224240.375
2   2011    3   1/03/2011   245457.375
2   2011    4   1/04/2011   238583.5
2   2011    5   1/05/2011   252392.75
2   2011    6   1/06/2011   256749.5
2   2011    7   1/07/2011   264736.125
2   2011    8   1/08/2011   256414
2   2011    9   1/09/2011   242335.125
2   2011    10  1/10/2011   305224.75
2   2011    11  1/11/2011   289199.875
2   2011    12  1/12/2011   281807.75
2   2012    1   1/01/2012   244886.125
2   2012    2   1/02/2012   232062.375
2   2012    3   1/03/2012   264991.75
2   2012    4   1/04/2012   232750.5
2   2012    5   1/05/2012   248498.375
2   2012    6   1/06/2012   264290.875
2   2012    7   1/07/2012   272689.75
2   2012    8   1/08/2012   260441.25
2   2012    9   1/09/2012   251852.375
2   2012    10  1/10/2012   305929.625
2   2012    11  1/11/2012   276711.625
2   2012    12  1/12/2012   278672.875
2   2013    1   1/01/2013   242613.875
2   2013    2   1/02/2013   227575.75
2   2013    3   1/03/2013   250318.875
2   2013    4   1/04/2013   250150.375
2   2013    5   1/05/2013   258467.25
2   2013    6   1/06/2013   261359.25
2   2013    7   1/07/2013   279113.75
2   2013    8   1/08/2013   258699
2   2013    9   1/09/2013   244841.375
2   2013    10  1/10/2013   308197.25
2   2013    11  1/11/2013   284195.5
2   2013    12  1/12/2013   287718.75
2   2014    1   1/01/2014   239510.375
2   2014    2   1/02/2014   216338.125
2   2014    3   1/03/2014   245626.75
2   2014    4   1/04/2014   230619.875
2   2014    5   1/05/2014   251758.875
2   2014    6   1/06/2014   254946.75
2   2014    7   1/07/2014   276268.75
2   2014    8   1/08/2014   266151.75
2   2014    9   1/09/2014   245859.375
2   2014    10  1/10/2014   317797.5
2   2014    11  1/11/2014   283786.625
2   2014    12  1/12/2014   289767.875
2   2015    1   1/01/2015   244008
2   2015    2   1/02/2015   228638
2   2015    3   1/03/2015   260056
2   2015    4   1/04/2015   232560.875
2   2015    5   1/05/2015   252642.125
2   2015    6   1/06/2015   249018.5
2   2015    7   1/07/2015   278113.125
2   2015    8   1/08/2015   255851
2   2015    9   1/09/2015   263046.625
2   2015    10  1/10/2015   344240.75
2   2015    11  1/11/2015   295486.125
2   2015    12  1/12/2015   293499.375
</code></pre>

<p>I only included two groups in the sample. I would like to use a function like one of the apply (tapply, lapply, sapply, etc.) that can run an AUTO.ARIMA model per group. Then I would like to obtain the forecast for each group for x number of months and also if I could visualize the model coefficients.</p>
"
"0.103005240524921","0.128036879932896","232631","<p>I was searching for appliction of unsupervised learning in trading and came across this <a href=""https://www.r-bloggers.com/artificial-intelligence-in-trading-k-means-clustering/"" rel=""nofollow"">site</a>. I have understood most part of the code but some I have no idea whats happening.</p>

<p>This code is used for optimizing cluster number</p>

<pre><code>#optimal number of clusters
wss = (nrow(nasa)-1)*sum(apply(nasa,2,var))
for (i in 2:15) wss[i] = sum(kmeans(nasa, centers=i)$withinss)
wss=(data.frame(number=1:15,value=as.numeric(wss)))
</code></pre>

<p>what is the principle behind this optimization code?</p>

<p>The second code which I didnt undertand is of autocorrelation code?</p>

<pre><code>autocorrelation=head(cbind(kmeanObject$cluster,lag(as.xts(kmeanObject$cluster),-1)),-1)
xtabs(~autocorrelation[,1]+(autocorrelation[,2]))

y=apply(xtabs(~autocorrelation[,1]+(autocorrelation[,2])),1,sum)
x=xtabs(~autocorrelation[,1]+(autocorrelation[,2]))
</code></pre>

<p><code>autocorrelation</code> variable is the columns binded of cluster and one day lagged cluster value. From this why they used <code>xtabs</code> instead of <code>cov</code> function to calculate correlation.</p>

<p>The last doubt is how to get and read percentage table?</p>

<pre><code>    1         2     3         4     5
1   0.11    0.25    0.30    0.22    0.12
2   0.01    0.59    0.17    0.21    0.02
3   0.02    0.49    0.21    0.24    0.04
4   0.03    0.40    0.26    0.27    0.04
5   0.19    0.18    0.24    0.23    0.17
</code></pre>

<p>here 2x2 is 0.59 and 3x2 is 0.49 what this means?</p>

<p>lets say i did a kmeans clustering using new data</p>

<pre><code>newkmeanObject=kmeans(new_nasa,5,iter.max=10)
newkmeanObject$cluster
</code></pre>

<p>and obtained last cluster values as 2 so what is this means?</p>
"
