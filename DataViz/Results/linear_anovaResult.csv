"V1","V2","V3","V4"
"NaN","NaN","  1866","<p>Following to the recent questions we had <a href=""http://stats.stackexchange.com/questions/1818/how-to-determine-the-sample-size-needed-for-repeated-measurement-anova/1823#1823"">here</a>.</p>

<p>I was hopping to know if anyone had come across or can share <strong>R code for performing a custom power analysis based on simulation for a linear model?</strong></p>

<p>Later I would obviously like to extend it to more complex models, but lm seems to right place to start. Thanks.</p>
"
"0.119713032670143","0.121566134770966","  3412","<p>I have an experiment that I'll try to abstract here.  Imagine I toss three white stones in front of you and ask you to make a judgment about their position.  I record a variety of properties of the stones and your response.   I do this over a number of subjects.  I generate two models.  One is that the nearest stone to you predicts your response, and the other is that the geometric center of the stones predicts your response.  So, using lmer in R I could write.</p>

<pre><code>mNear   &lt;- lmer(resp ~ nearest + (1|subject), REML = FALSE)
mCenter &lt;- lmer(resp ~ center  + (1|subject), REML = FALSE)
</code></pre>

<p><strong>UPDATE AND CHANGE - more direct version that incorporates several helpful comments</strong></p>

<p>I could try</p>

<pre><code>anova(mNear, mCenter)
</code></pre>

<p>Which is incorrect, of course, because they're not nested and I can't really compare them that way.  I was expecting anova.mer to throw an error but it didn't.  But the possible nesting that I could try here isn't natural and still leaves me with somewhat less analytical statements.  When models are nested naturally (e.g. quadratic on linear) the test is only one way.  But in this case what would it mean to have asymmetric findings?</p>

<p>For example, I could make a model three:</p>

<pre><code>mBoth &lt;- lmer(resp ~ center + nearest + (1|subject), REML = FALSE)
</code></pre>

<p>Then I can anova.</p>

<pre><code>anova(mCenter, mBoth)
anova(mNearest, mBoth)
</code></pre>

<p>This is fair to do and now I find that the center adds to the nearest effect (the second command) but BIC actually goes up when nearest is added to center (correction for the lower parsimony).  This confirms what was suspected.</p>

<p>But is finding this sufficient?  And is this fair when center and nearest are so highly correlated?</p>

<p>Is there a better way to analytically compare the models when it's not about adding and subtracting explanatory variables (degrees of freedom)?</p>
"
"0.05643326479831","0.0573068255061253","  4544","<p>Please provide R code which allows one to conduct a between-subjects ANOVA with -3, -1, 1, 3 contrasts.  I understand there is a debate regarding the appropriate Sum of Squares (SS) type for such an analysis.  However, as the default type of SS used in SAS and SPSS (Type III) is considered the standard in my area.  Thus I would like the results of this analysis to match perfectly what is generated by those statistics programs.  To be accepted an answer must directly call aov(), but other answers may be voted up (espeically if they are easy to understand/use).</p>

<pre><code>sample.data &lt;- data.frame(IV=rep(1:4,each=20),DV=rep(c(-3,-3,1,3),each=20)+rnorm(80))
</code></pre>

<p><strong>Edit:</strong> Please note, the contrast I am requesting is not a simple linear or polynomial contrast but is a contrast derived by a theoretical prediction, i.e. the type of contrasts discussed by Rosenthal and Rosnow.</p>
"
"0.05643326479831","0.0573068255061253","  5525","<p>Is there a way to get the number of parameters of a linear model like that?</p>

<pre><code>model &lt;- lm(Y~X1+X2)
</code></pre>

<p>I would like to get the number 3 somehow (intercept + X1 + X2). I looked for something like this in the structures that <code>lm</code>, <code>summary(model)</code> and <code>anova(model)</code> return, but I didn't figure it out. In case I don't get an answer, I'll stick on</p>

<pre><code>dim(model.matrix(model))[2]
</code></pre>

<p>Thank you</p>
"
"0.0892288262810312","0.0906100470365937","  7775","<p>Does anyone have suggestions or packages that will calculate the coefficient of partial determination?</p>

<p>The coefficient of partial determination can be defined as the percent of variation that cannot be explained in a reduced model, but can be explained by the predictors specified in a full(er) model. This coefficient is used to provide insight into whether or not one or more additional predictors may be useful in a more fully specified regression model.</p>

<p>The calculation for the partial r^2 is relatively straight forward after estimating your two models and generating the ANOVA tables for them. The calculation for the partial r^2 is:</p>

<p>(SSEreduced - SSEfull) / SSEreduced</p>

<p>I've written this relatively simple function that will calculate this for a multiple linear regression model. I'm unfamiliar with other model structures in R where this function may not perform as well:</p>

<pre><code>partialR2 &lt;- function(model.full, model.reduced){
    anova.full &lt;- anova(model.full)
    anova.reduced &lt;- anova(model.reduced)

    sse.full &lt;- tail(anova.full$""Sum Sq"", 1)
    sse.reduced &lt;- tail(anova.reduced$""Sum Sq"", 1)

    pR2 &lt;- (sse.reduced - sse.full) / sse.reduced
    return(pR2)

    }
</code></pre>

<p>Any suggestions or tips on more robust functions to accomplish this task and/or more efficient implementations of the above code would be much appreciated.</p>
"
"NaN","NaN","  8513","<p>Let's say $y$ is a linear function of $x$ and a dummy $d$. My hypothesis is that $d$ itself is like a hedonistic index of a vector of other variables, $Z$. I have support for this in a $MANOVA$ of $Z$ (i.e. $z_1$, $z_2$, ..., $z_n$) on $d$. Is there any way to test the <em>equivalence</em> of these two models:</p>

<p>Model 1: $y = b_0 + b_1 \cdot x + b_2\cdot d + e_1$</p>

<p>Model 2: $y = g_0 + Z\cdot G + e_2$</p>

<p>where $G$ is the column vector of parameters.</p>
"
"0.0892288262810312","0.072488037629275"," 10429","<p>I'm wondering how to fit multivariate linear mixed model and finding multivariate BLUP in R. I'd appreciate if someone come up with example and R code. Thanks</p>

<p><strong>Edit</strong></p>

<p>I wonder how to fit multivariate linear mixed model with <code>lme4</code>. I fitted univariate linear mixed models with the following code:</p>

<pre><code>library(lme4)
lmer.m1 &lt;- lmer(Y1~A*B+(1|Block)+(1|Block:A), data=Data)
summary(lmer.m1)
anova(lmer.m1)

lmer.m2 &lt;- lmer(Y2~A*B+(1|Block)+(1|Block:A), data=Data)
summary(lmer.m2)
anova(lmer.m2)
</code></pre>

<p>I'd like to know how to fit multivariate linear mixed model with <code>lme4</code>. The data is below:</p>

<pre><code>Block A B    Y1    Y2
 1 1 1 135.8 121.6
 1 1 2 149.4 142.5
 1 1 3 155.4 145.0
 1 2 1 105.9 106.6
 1 2 2 112.9 119.2
 1 2 3 121.6 126.7
 2 1 1 121.9 133.5
 2 1 2 136.5 146.1
 2 1 3 145.8 154.0
 2 2 1 102.1 116.0
 2 2 2 112.0 121.3
 2 2 3 114.6 137.3
 3 1 1 133.4 132.4
 3 1 2 139.1 141.8
 3 1 3 157.3 156.1
 3 2 1 101.2  89.0
 3 2 2 109.8 104.6
 3 2 3 111.0 107.7
 4 1 1 124.9 133.4
 4 1 2 140.3 147.7
 4 1 3 147.1 157.7
 4 2 1 110.5  99.1
 4 2 2 117.7 100.9
 4 2 3 129.5 116.2
</code></pre>

<p>Thank in advance for your time and cooperation.</p>
"
"0.208393641145664","0.232781425629767"," 11079","<p>I need an help because I donÂ´t know if the command for the ANOVA analysis I am 
performing in R is correct. Indeed using the function aov I get the following error: <code>In aov (......) Error() model is singular</code></p>

<p>The structure of my table is the following: subject, stimulus, condition, sex, response</p>

<p>Example:</p>

<pre><code>subject  stimulus condition sex    response
subject1    gravel  EXP1    M      59.8060
subject2    gravel  EXP1    M      49.9880
subject3    gravel  EXP1    M      73.7420
subject4    gravel  EXP1    M      45.5190
subject5    gravel  EXP1    M      51.6770
subject6    gravel  EXP1    M      42.1760
subject7    gravel  EXP1    M      56.1110
subject8    gravel  EXP1    M      54.9500
subject9    gravel  EXP1    M      62.6920
subject10   gravel  EXP1    M      50.7270
subject1    gravel  EXP2    M      70.9270
subject2    gravel  EXP2    M      61.3200
subject3    gravel  EXP2    M      70.2930
subject4    gravel  EXP2    M      49.9880
subject5    gravel  EXP2    M      69.1670
subject6    gravel  EXP2    M      62.2700
subject7    gravel  EXP2    M      70.9270
subject8    gravel  EXP2    M      63.6770
subject9    gravel  EXP2    M      72.4400
subject10   gravel  EXP2    M      58.8560
subject11   gravel  EXP1    F      46.5750
subject12   gravel  EXP1    F      58.1520
subject13   gravel  EXP1    F      57.4490
subject14   gravel  EXP1    F      59.8770
subject15   gravel  EXP1    F      55.5480
subject16   gravel  EXP1    F      46.2230
subject17   gravel  EXP1    F      63.3260
subject18   gravel  EXP1    F      60.6860
subject19   gravel  EXP1    F      59.4900
subject20   gravel  EXP1    F      52.6630
subject11   gravel  EXP2    F      55.7240
subject12   gravel  EXP2    F      66.4220
subject13   gravel  EXP2    F      65.9300
subject14   gravel  EXP2    F      61.8120
subject15   gravel  EXP2    F      62.5160
subject16   gravel  EXP2    F      65.5780
subject17   gravel  EXP2    F      59.5600
subject18   gravel  EXP2    F      63.8180
subject19   gravel  EXP2    F      61.4250
.....
.....
.....
.....
</code></pre>

<p>As you can notice each subject repeated the evaluation in 2 conditions (EXP1 and EXP2).</p>

<p>What I am interested in is to know if there are significant differences between 
the evaluations of the males and the females.</p>

<p>This is the command I used to perform the ANOVA with repeated measures:</p>

<pre><code>aov1 = aov(response ~ stimulus*sex + Error(subject/(stimulus*sex)), data=scrd)
summary(aov1)
</code></pre>

<p>I get the following error:</p>

<pre><code>&gt; aov1 = aov(response ~ stimulus*sex + Error(subject/(stimulus*sex)), data=scrd)
Warning message:
In aov(response ~ stimulus * sex + Error(subject/(stimulus * sex)),  :
Error() model is singular
&gt; summary(aov1)

Error: subject
          Df  Sum Sq Mean Sq F value Pr(&gt;F)
sex        1  166.71  166.72   1.273  0.274
Residuals 18 2357.29  130.96               

Error: subject:stimulus
              Df Sum Sq Mean Sq F value Pr(&gt;F)    
stimulus       6 7547.9 1257.98 35.9633 &lt;2e-16 ***
stimulus:sex   6   94.2   15.70  0.4487 0.8445    
Residuals    108 3777.8   34.98                   
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Error: Within
           Df Sum Sq Mean Sq F value Pr(&gt;F)
Residuals 420 9620.6  22.906               
&gt; 
</code></pre>

<p>The thing is that looking at the data it is evident for me that there is a 
difference between male and females, because for each stimulus I always get
a mean higher for the males rather than the females. 
Therefore the ANOVA should indicate significant differences....</p>

<p>Is there anyone who can suggest me where I am wrong?</p>

<p>Finally, I know that in R there are two libraries on linear mixed models called 
nlme and lme4, but I have never used it so far and I donÂ´t know if I have to utilize it for my case.
Is it the case to utilize it? If yes, could you please provide a quick R example
of a command which could solve my problem?</p>

<p>Thanks in advance!</p>

<p>Best regards</p>

<hr>

<p>Dear all, 
I am stuck now ;-( Indeed I understood everything you suggested me but still I donÂ´t get significance in the ANOVA results, and definitively there is an error, because results cannot be non-significant. Indeed looking at the means for each stimulus, it is possible to notice that males gave always higher evaluations than females.</p>

<p>To prove this I discarded for a moment the effect of the repeated measures, and I performed an ANOVA separately on both the two conditions (EXP1 and EXP2) during which the evaluations were given.
What I get is significant differences between males and female, in both EXP1 and EXP2.</p>

<p>Now, why when I perform the ANOVA with repeated measures I donÂ´t get the same behavior?</p>

<p>My design is the following:
-sex is a between-subjects factor (with two levels)
-stimulus is a within-subjects factor (with 3 assumed levels)
-condition is a within-subjects factor (with 2 levels)
-all factors are fully crossed</p>

<p>I tried, both the ways suggested but without achieving significance: </p>

<pre><code>mDf &lt;- aggregate(response ~ subject + sex, data=scrd, FUN=mean)
summary(aov(response ~ sex, data=mDf))     # ANOVA with just the between-effect
</code></pre>

<p>and</p>

<pre><code>aov1 = aov(response ~ sex*stimulus*condition + Error(subject/(stimulus*condition)), data=scrd)
summary(aov1)
</code></pre>

<p>Instead if I perform the ANOVA on the two subtables of EXP 1 and 2 I get significant differences. </p>

<pre><code>table_EXP1 &lt;- subset(scrd, condition == ""EXP1"")
table_EXP2 &lt;- subset(scrd, condition == ""EXP2"")


fit_table_EXP1 &lt;- lm(response ~ stimulus*sex, data=table_EXP1) 
summary(fit_table_EXP1 )
anova(fit_table_EXP1 )


fit_table_EXP2 &lt;- lm(response ~ stimulus*sex, data=table_EXP2) 
summary(fit_table_EXP2)
anova(fit_table_EXP2)
</code></pre>

<p>....how can this be possible?...it is a contraddiction....</p>

<p>HELP!</p>

<p>Please enlighten me!</p>

<p>Thanks in advance</p>

<p>Cheers</p>
"
"0.159617376893524","0.162088179694622"," 11498","<p>I'm fitting a linear model where the response is a function both of time and of static covariates (i.e. ones that are independent of time). The ultimate goal is to identify significant effects of the static covariates.</p>

<p>Is this the best general strategy for selecting variables (in R, using the <code>nlme</code> package)? Anything I can do better?</p>

<ol>
<li>Break the data up by groups and plot it against time. For continuous covariates, bin it and plot the data in each bin against time. Use the group-specific trends to make an initial guess at what time terms to include-- time, time^n, sin(2*pi*time)+cos(2*pi*time), log(time), exp(time), etc.</li>
<li>Add one term at a time, comparing each model to its predecessor, never adding a higher order in the absence of lower order terms. Sin and cos are never added separately. <strong><em>Is it acceptable to pass over a term that significantly improves the fit of the model if there is no physical interpretation of that term?</em></strong>.</li>
<li>With the full dataset, use forward selection to add static variables to the model and then relevant interaction terms with each other and with the time terms. <strong><em>I've seen some strong criticism of stepwise regression, but doesn't forward selection ignore significant higher order terms if the lower order terms they depend on are not significant? And I've noticed that it's hard to pick a starting model for backward elimination that isn't saturated, or singular, or fails to converge. How do you decide between variable selection algorithms?</em></strong></li>
<li>Add random effects to the model. <strong><em>Is this as simple as doing the variable selection using <code>lm()</code> and then putting the final formula into <code>lme()</code> and specifying the random effects? Or should I include random effects from the very start?</em></strong>. Compare the fits of models using a random intercept only, a random interaction with the linear time term, and random interaction with each successive time term. </li>
<li>Plot a semivariogram to see if an autoregressive error term is needed. <strong><em>What should a semivariogram look like if the answer is 'no'? A horizontal line? How straight, how horizontal? Does including autoregression in the model again require checking potential variables and interactions to make sure they're still relevant?</em></strong></li>
<li>Plot the residuals to see if the variance changes as a function of fitted value, time, or any of the other terms. If it does, weigh the variances appropriately (for <code>lme()</code>, use the <code>weights</code> argument to specify a <code>varFunc()</code>) and compare to the unweighted model to see if this improves the fit. <strong><em>Is this the right sequence in which to do this step, or should it be done before autocorrelation?</em></strong>.</li>
<li>Do <code>summary()</code> of the fitted model to identify significant coefficients for numeric covariates. Do <code>Anova()</code> of the fitted model to identify significant effects for qualitative covariates.</li>
</ol>
"
"0.0892288262810312","0.0906100470365937"," 13091","<p>I have this model:</p>

<pre><code>model &lt;- zelig(dv~(product*intervention), model = ""negbin"", data = data)
</code></pre>

<p>intervention has <strong>two levels</strong>: neutral(=0), treatment(=1)<br />
product has <strong>two levels</strong>: product1(=0), product2(=1)</p>

<p>I build f_all to just have one factor with 4 groups for comparison analysis.</p>

<p>Thus I have <strong>4 groups</strong> in f_all<br />
1. product1-neutral<br />
2. product1-treatment<br />
3. product2-neutral<br />
4. product2-treament<br /></p>

<p><strong>My interaction hypothesis is that treatment only works for product2.</strong></p>

<p>Zelig gives me my predicted significant interaction. <br /></p>

<p>Yet, I need planned contrasts to test my specific hypothesis: c(-1,1,0,0) and c(0,0,1,-1)</p>

<p>I researched and found a description of doing this with multcomp on this page: <a href=""http://stats.stackexchange.com/questions/12993/how-to-setup-and-interpret-anova-contrasts-with-the-car-package-in-r"">post comparisons</a></p>

<p>The regression output shows my predicted interaction</p>

<pre><code>(Intercept)  1.34223    0.08024  16.728   &lt;2e-16 ***
product      0.08747    0.08025   1.090   0.2757
intervention 0.07437    0.07731   0.962   0.3361
interaction  0.45645    0.22263   2.050   0.0403 * 
</code></pre>

<p>However, it said multcomp and the glht function is for linear models, but I am using a negbin model.</p>

<p><strong>3 Questions regarding this problem:</strong><br />
1. Can I do planned comparisons on my negbin model using multcomp?<br />
2. If not what appropriate method is there to do this for my negbin model?<br />
3. Based on R using treatment contrasts per default could I just interpret the interaction coefficient as the contrast comparing product2-neutral versus product2-treatment? Can I then interpret the intervention coefficient as contrast comparing product1-neutral versus product1-treament?</p>
"
"0.0399043442233811","0.0405220449236554"," 13465","<p>When doing a GLM and you get the ""not defined because of singularities"" error in the anova output, how does one counteract this error from happening? </p>

<p>Some have suggested that it is due to collinearity between covariates or that one of the levels is not present in the dataset (see: <a href=""http://r.789695.n4.nabble.com/interpreting-quot-not-defined-because-of-singularities-quot-in-lm-td882827.html"">interpreting ""not defined because of singularities"" in lm</a>)</p>

<p>If I wanted to see which ""particular treatment"" is driving the model and I have 4 levels of treatment: <code>Treat 1</code>, <code>Treat 2</code>, <code>Treat 3</code> &amp; <code>Treat 4</code>, which are recorded in my spreadsheet as: when <code>Treat 1</code> is 1 the rest are zero, when <code>Treat 2</code> is 1 the rest are zero, etc., what would I have to do?</p>
"
"0.143877159211166","0.146104310758895"," 14978","<p>When running a repeated measures ANOVA in SPSS, it's possible to 'Save' the residuals as new variables in the data editor.</p>

<p>But the values output do not match the residuals given in R, and seem to be residuals for a between-subjects model. Unless I am missing something? Is SPSS giving the wrong residuals?</p>

<p>Example in R:</p>

<pre><code>set.seed(1)  # hopefully this keeps things the same every time!

  # create a data frame with each line representing one subject,
  # and create first and second observations for some experiment

DF &lt;- data.frame(participant=factor(1:5), first=rnorm(5, 10, 5), second=rnorm(5, 20, 5))

DF
</code></pre>

<p>-</p>

<pre><code>  participant     first   second
1           1  6.867731 15.89766
2           2 10.918217 22.43715
3           3  5.821857 23.69162
4           4 17.976404 22.87891
5           5 11.647539 18.47306
</code></pre>

<p>-</p>

<pre><code>  # reshape it for an ANOVA in R
DFlong &lt;- reshape(DF, direction=""long"", varying=c(""first"", ""second""), v.names=""value"", idvar=""participant"", times=c(1, 2), timevar=""group"")

DFlong
</code></pre>

<p>-</p>

<pre><code>    participant group     value
1.1           1     1  6.867731
2.1           2     1 10.918217
3.1           3     1  5.821857
4.1           4     1 17.976404
5.1           5     1 11.647539
1.2           1     2 15.897658
2.2           2     2 22.437145
3.2           3     2 23.691624
4.2           4     2 22.878907
5.2           5     2 18.473058
</code></pre>

<p>-</p>

<pre><code>my.aov &lt;- aov(value ~ group + Error( participant / group ), DFlong)
summary(my.aov)
</code></pre>

<p>-</p>

<pre><code>Error: participant
          Df Sum Sq Mean Sq F value Pr(&gt;F)
Residuals  4 86.474  21.619               

Error: participant:group
          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
group      1 251.469 251.469  19.871 0.01118 *
Residuals  4  50.619  12.655                  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>-</p>

<pre><code>my.aov$""participant:group""$residuals
</code></pre>

<p>-</p>

<pre><code>        6          7          8          9         10 
0.7066837 -1.0533061 -5.5440267  3.6252135 -2.2654355 
</code></pre>

<p>-</p>

<pre><code># import into SPSS:
write.table(DF, ""C:/test.txt"", row.names=FALSE)
</code></pre>

<p>Then load SPSS, and run:</p>

<pre><code>GET DATA  /TYPE = TXT
 /FILE = 'C:\test.txt'
 /DELCASE = LINE
 /DELIMITERS = "" ""
 /QUALIFIER = '""'
 /ARRANGEMENT = DELIMITED
 /FIRSTCASE = 2
 /IMPORTCASE = ALL
 /VARIABLES =
 participant F1.0
 first F16.14
 second F16.13
 .
CACHE.
EXECUTE.
DATASET NAME DataSet1 WINDOW=FRONT.
</code></pre>

<p>Now change the variable types to scale (in the 'variables' tab - I don't know the syntax for this). Then run:</p>

<pre><code>GLM
  first second
  /WSFACTOR = factor1 2 Polynomial
  /METHOD = SSTYPE(3)
  /SAVE = RESID
  /CRITERIA = ALPHA(.05)
  /WSDESIGN = factor1 .
</code></pre>

<p>Or, do the above SPSS commands using the GUI: File->Read text data... find C:\test.txt, import it, remember to specify that the file has variable names as the first case, and run:</p>

<ol>
<li><p>Analyze->General Linear Model->Repeated Measures...</p></li>
<li><p>Set number of levels to 2</p></li>
<li><p>Put variables into analysis, 'first' and 'second'.</p></li>
<li><p>Open 'Save...' dialog box, check 'Residuals->Unstandardized'</p></li>
<li><p>Run analysis, SPSS creates two variables of residuals:</p>

<pre><code>RES_1    RES_2
-3.78    -4.78
  .27     1.76
-4.82     3.02
 7.33     2.20
 1.00    -2.20
</code></pre></li>
</ol>

<p>Note these values are different to R. So has SPSS got it wrong?</p>
"
"0.0987582133970426","0.114613651012251"," 15062","<p>The title says it all, and I'm confused. The following runs a repeated measures aov() in R, and runs what I thought was an equivalent lm() call, but they return different error residuals (although the sums of squares are the same).</p>

<p>Clearly the residuals and fitted values from aov() are the ones used in the model, because their sums of squares add up to each of the model/residual sums of squares reported in summary(my.aov). So what are the actual linear models that are applied to a repeated measures design?</p>

<pre><code>set.seed(1)
# make data frame,
# 5 participants, with 2 experimental factors, each with 2 levels
# factor1 is A, B
# factor2 is 1, 2
DF &lt;- data.frame(participant=factor(1:5), A.1=rnorm(5, 50, 20), A.2=rnorm(5, 100, 20), B.1=rnorm(5, 20, 20), B.2=rnorm(5, 50, 20))

# get our experimental conditions
conditions &lt;- names(DF)[ names(DF) != ""participant"" ]

# reshape it for aov
DFlong &lt;- reshape(DF, direction=""long"", varying=conditions, v.names=""value"", idvar=""participant"", times=conditions, timevar=""group"")

# make the conditions separate variables called factor1 and factor2
DFlong$factor1 &lt;- factor( rep(c(""A"", ""B""), each=10) )
DFlong$factor2 &lt;- factor( rep(c(1, 2), each=5) )

# call aov
my.aov &lt;- aov(value ~ factor1*factor2 + Error(participant / (factor1*factor2)), DFlong)

# similar for an lm() call
fit &lt;- lm(value ~ factor1*factor2 + participant, DFlong)

# what's aov telling us?
summary(my.aov)

# check SS residuals
sum(residuals(fit)^2)       # == 5945.668

# check they add up to the residuals from summary(my.aov)
2406.1 + 1744.1 + 1795.46   # == 5945.66

# all good so far, but how are the residuals in the aov calculated?
my.aov$""participant:factor1""$residuals

#clearly these are the ones used in the ANOVA:
sum(my.aov$""participant:factor1""$residuals ^ 2)

# this corresponds to the factor1 residuals here:
summary(my.aov)


# but they are different to the residuals reported from lm()
residuals(fit)
my.aov$""participant""$residuals
my.aov$""participant:factor1""$residuals
my.aov$""participant:factor1:factor2""$residuals
</code></pre>
"
"0.0691163516376137","0.0701862406343596"," 17111","<p>I have a classic linear model, with 5 possible regressors. They are uncorrelated with one another, and have quite low correlation with the response. I have arrived at a model where 3 of the regressors have significant coefficients for their t statistic (p&lt;0.05). Adding either or both of the remaining 2 variables gives p values >0.05 for the t statistic, for the added variables. This leads me to believe the 3 variable model is ""best"".</p>

<p>However, using the anova(a,b) command in R where a is the 3 variable model and b is the full model, the p value for the F statistic is &lt; 0.05, which tells me to prefer the full model over the 3 variable model. How can I reconcile these apparent contradictions ?</p>

<p>Thanks
PS
Edit: Some further background. This is homework so I won't post details, but we are not given details of what the regressors represent - they are just numbered 1 to 5. We are asked to ""derive an appropriate model, giving justification"". </p>
"
"0.0598565163350717","0.0607830673854831"," 18084","<p>There's a lot about collinearity with respect to continuous predictors but not so much that I can find on categorical predictors. I have data of this type illustrated below.   </p>

<p>The first factor is a genetic variable (allele count), the second factor is a disease category. Clearly the genes precede the disease and are a factor in showing symptoms that lead to a diagnosis. However, a regular analysis using type II or III sums of squares, as would be commonly done in psych with SPSS, misses the effect. A type I sums of squares analysis picks it up, when the appropriate order is entered because it is order dependent. Further, there are likely to be extra components to the disease process which are not related to the gene that are not well identified with type II or III, see <strong>anova(lm1)</strong> below vs lm2 or Anova.</p>

<p><em>Example data:</em>  </p>

<pre><code>set.seed(69)
iv1 &lt;- sample(c(0,1,2), 150, replace=T)
iv2 &lt;- round(iv1 + rnorm(150, 0, 1), 0)
iv2 &lt;- ifelse(iv2&lt;0, 0, iv2)
iv2 &lt;- ifelse(iv2&gt;2, 2, iv2)
dv  &lt;- iv2 + rnorm(150, 0, 2)
iv2 &lt;- factor(iv2, labels=c(""a"", ""b"", ""c""))
df1 &lt;- data.frame(dv, iv1, iv2)

library(car)
chisq.test(table(iv1, iv2))          # quick gene &amp; disease relations
lm1 &lt;- lm(dv~iv1*iv2, df1);    lm2 &lt;- lm(dv~iv2*iv1, df1)
anova(lm1);                    anova(lm2)
Anova(lm1, type=""II"");         Anova(lm2, type=""II"")
</code></pre>

<ol>
<li><strong>lm1</strong> with type I SS to me seems the appropriate way to analyse the data given the background theory. Is my assumption correct?  </li>
<li>I'm used to explicitly manipulated orthogonal designs, where these problems don't usually pop up. Is it difficult to convince reviewers that this is the best process (assuming point 1 is correct) in the context of an SPSS centric field?  </li>
<li>And what to report in the stats section? Any extra analysis, or comments that should go in?</li>
</ol>
"
"0.0977452818676612","0.099258333397093"," 19469","<p>Here is an example: I have a set of observations of different individuals from lots of different families of grasses:</p>

<pre><code>individual#, Fam, Genus, Factor1(3 levels), Factor2(7 levels), Factor3(5 levels), Response1(3 levels), Response2(3 levels)
</code></pre>

<p>What I am hoping to discover is whether the frequency of occurrences of Response1 and 2 are linked to family groups, and whether Factors 1 - 3 (things like soil type, sun exposure etc) have an impact. </p>

<p>Example: </p>

<pre><code>family,  resp1a,  resp1b,   resp1c 
1,       14%(20), 16%(24),  67%(98),  Total N = 147  
2,       38%(98), 86%(220), 48%(123), Total N = 256
...
</code></pre>

<p>First, I need to see whether these differences in responses between families is significant (chi-squared?). Secondly, I need to see if one of the 3 factors has an effect on the response.</p>

<p>Now it seems in my basic understanding, that if the response(s) were continuous measurement, ANOVA/MANOVA would work. Easy-peasy. However, since everything is discreet categories (including the independent and dependent variables) I can't do this. Additionally, since the responses are not mutually exclusive, this seems to violate an assumption of the log-linear model.</p>

<p>I've scoured, and keep bouncing around between Multinomial Logistic Regression, or just independent Chi-Square tests, or... hell I don't know anymore.</p>

<p>And yes, I am trying to swim before learning to float.</p>

<p>Oh, and this is all happening in R.</p>
"
"0.174733250035473","0.17743803952666"," 20452","<p>My primary question is how to interpret the output (coefficients, F, P) when conducting a Type I (sequential) ANOVA? </p>

<p>My specific research problem is a bit more complex, so I will break my example into parts. First, if I am interested in the effect of spider density (X1) on say plant growth (Y1) and I planted seedlings in enclosures and manipulated spider density, then I can analyze the data with a simple ANOVA or linear regression. Then it wouldn't matter if I used Type I, II, or III Sum of Squares (SS) for my ANOVA. In my case, I have 4 replicates of 5 density levels, so I can use density as a factor or as a continuous variable. In this case, I prefer to interpret it as a continuous independent (predictor) variable. In R I might run the following:</p>

<pre><code>lm1 &lt;- lm(y1 ~ density, data = Ena)
summary(lm1)
anova(lm1)
</code></pre>

<p>Running the anova function will make sense for comparison later hopefully, so please ignore the oddness of it here. The output is:</p>

<pre><code>Response: y1
          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
density    1 0.48357 0.48357  3.4279 0.08058 .
Residuals 18 2.53920 0.14107 
</code></pre>

<p>Now, let's say I suspect that the starting level of inorganic nitrogen in the soil, which I couldn't control, may have also significantly affected the plant growth. I'm not particularly interested in this effect but would like to potentially account for the variation it causes. Really, my primary interest is in the effects of spider density (hypothesis: increased spider density causes increased plant growth - presumably through reduction of herbivorous insects but I'm only testing the effect not the mechanism). I could add the effect of inorganic N to my analysis. </p>

<p>For the sake of my question, let's pretend that I test the interaction density*inorganicN and it's non-significant so I remove it from the analysis and run the following main effects:</p>

<pre><code>&gt; lm2 &lt;- lm(y1 ~ density + inorganicN, data = Ena)
&gt; anova(lm2)
Analysis of Variance Table

Response: y1
           Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
density     1 0.48357 0.48357  3.4113 0.08223 .
inorganicN  1 0.12936 0.12936  0.9126 0.35282  
Residuals  17 2.40983 0.14175 
</code></pre>

<p>Now, it makes a difference whether I use Type I or Type II SS (I know some people object to the terms Type I &amp; II etc. but given the popularity of SAS it's easy short-hand). R anova{stats} uses Type I by default. I can calculate the type II SS, F, and P for density by reversing the order of my main effects or I can use Dr. John Fox's ""car"" package (companion to applied regression). I prefer the latter method since it is easier for more complex problems.</p>

<pre><code>library(car)
Anova(lm2)
            Sum Sq Df F value  Pr(&gt;F)  
density    0.58425  1  4.1216 0.05829 .
inorganicN 0.12936  1  0.9126 0.35282  
Residuals  2.40983 17  
</code></pre>

<p>My understanding is that type II hypotheses would be, ""There is no linear effect of x1 on y1 given the effect of (holding constant?) x2"" and the same for x2 given x1. I guess this is where I get confused. <strong>What is the hypothesis being tested by the ANOVA using the type I (sequential) method above compared to the hypothesis using the type II method?</strong></p>

<p>In reality, my data is a bit more complex because I measured numerous metrics of plant growth as well as nutrient dynamics and litter decomposition. My actual analysis is something like:</p>

<pre><code>Y &lt;- cbind(y1 + y2 + y3 + y4 + y5)
# Type II
mlm1 &lt;- lm(Y ~ density + nitrate + Npred, data = Ena)
Manova(mlm1)

Type II MANOVA Tests: Pillai test statistic
        Df test stat approx F num Df den Df  Pr(&gt;F)    
density  1   0.34397        1      5     12 0.34269    
nitrate  1   0.99994    40337      5     12 &lt; 2e-16 ***
Npred    1   0.65582        5      5     12 0.01445 * 


# Type I
maov1 &lt;- manova(Y ~ density + nitrate + Npred, data = Ena)
summary(maov1)

          Df  Pillai approx F num Df den Df  Pr(&gt;F)    
density    1 0.99950     4762      5     12 &lt; 2e-16 ***
nitrate    1 0.99995    46248      5     12 &lt; 2e-16 ***
Npred      1 0.65582        5      5     12 0.01445 *  
Residuals 16                                           
</code></pre>
"
"0.132347737294141","0.122178562499719"," 22211","<p>I have some multivariate data and want to investigate the effect of some environmental gradient. I want to use capscale but I donÂ´t know how to deal with the permutation scheme. I have made up some artificial data, with 20 sites along a gradient (""env""):</p>

<pre><code>######### create some species data along a gradient
df &lt;- structure(list(site = 1:20, 
                     sp1 = c(7L, 4L, 2L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), 
                     sp2 = c(1L, 2L, 4L, 7L, 8L, 7L, 4L, 2L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), 
                     sp3 = c(0L, 0L, 0L, 0L, 0L, 1L, 2L, 4L, 7L, 8L, 7L, 4L, 2L, 1L, 0L, 0L, 0L, 0L, 0L, 0L), 
                     sp4 = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 2L, 4L, 7L, 8L, 7L, 4L, 2L, 1L, 0L), 
                     sp5 = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 2L, 4L, 7L, 8L), 
                     sp6 = c(0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), 
                     sp7 = c(0L, 0L, 0L, 0L, 0L, 0L, 2L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), 
                     sp8 = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 3L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), 
                     sp9 = c(0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 4L, 1L, 0L, 0L)), 
                .Names = c(""site"", ""sp1"", ""sp2"", ""sp3"", ""sp4"", ""sp5"", ""sp6"", ""sp7"", ""sp8"", ""sp9""), 
                class = ""data.frame"", row.names = c(NA, -20L))

# add some linear responses
df$sp6 &lt;- round(seq(1, 8, 7/19), digits = 2) # linear response
df$sp7 &lt;- round(seq(1, 4, 3/19), digits = 2) # no so strong linear response
df$sp9 &lt;- round(seq(1, 6, 5/19), digits = 2)

# gradient
df$env &lt;- 1:20
</code></pre>

<p>If I sampled only once I would do something like this:</p>

<pre><code># db-RDA sampled at one time
require(vegan)
mod &lt;- capscale(df[ ,-c(1, 10)] ~ env, data = df, distance = ""bray"")
anova(mod, by = ""terms"", step = 999)  # assess the ""significance"" of contraining variable
plot(mod)
</code></pre>

<p>Now imagine I sampled the same data trice, but in three different months:</p>

<pre><code># now we replate exactly the same data 2 more times 
repdf &lt;- rbind(df, df, df)  
repdf$time &lt;- rep(1:3, each = nrow(df))
repdf$site &lt;- factor(repdf$site)
</code></pre>

<p>If I would use unrestricted permutations, then this won't capture the repeated measures and the p-values would be to low.</p>

<p>I could restrict the permutations within each sites (using strata = site in vegan), but this destroys only the temporal effect and yields to a p of 1 (because every permuation is the same):</p>

<pre><code>repmod &lt;- capscale(repdf[ ,-c(1, 11, 12)] ~ env, data = repdf, distance = ""bray"") # db-RDA
anova(repmod, by = ""terms"", strata = repdf$site, step = 999)  
</code></pre>

<p><strong>My question:</strong>
How should I restrict the permutations assessing the effect of the gradient taking this temporal correlation into account? What permutationscheme should I use?</p>

<p><strong>Some ideas:</strong>
a) Permute the strata ( = sites), but not within the strata. This will destroy the env gradient, so the p-value is only determined by this.</p>

<p>b) Include ""time"" into the model (with interactions) and the run for time-effect a different permutation-scheme (permute within sites) than for env (permute sites, but not within sites).</p>

<p>I know about the permute-package and can incorporate it into permutest.cca, so my question is more theoretical.</p>
"
"0.149641290837679","0.162088179694622"," 23276","<p>We know that a paired <i>t</i>-test is just a special case of one-way repeated-measures (or within-subject) ANOVA as well as linear mixed-effect model, which can be demonstrated with lme() function the nlme package in R as shown below.</p>

<pre><code>#response data from 10 subjects under two conditions
x1&lt;-rnorm(10)
x2&lt;-1+rnorm(10)

# Now create a dataframe for lme
myDat &lt;- data.frame(c(x1,x2), c(rep(""x1"", 10), rep(""x2"", 10)), rep(paste(""S"", seq(1,10), sep=""""), 2))
names(myDat) &lt;- c(""y"", ""x"", ""subj"")
</code></pre>

<p>When I run the following paired t-test:</p>

<pre><code>t.test(x1, x2, paired = TRUE)
</code></pre>

<p>I got this result (you will get a different result because of the random generator):</p>

<pre><code>t = -2.3056, df = 9, p-value = 0.04657
</code></pre>

<p>With the ANOVA approach we can get the same result:</p>

<pre><code>summary(aov(y ~ x + Error(subj/x), myDat))

# the F-value below is just the square of the t-value from paired t-test:
          Df  F value Pr(&gt;F)
x          1  5.3158  0.04657
</code></pre>

<p>Now I can obtain the same result in lme with the following model, assuming a positive-definite symmetrical correlation matrix for the two conditions:</p>

<pre><code>summary(fm1 &lt;- lme(y ~ x, random=list(subj=pdSymm(form=~x-1)), data=myDat))

# the 2nd row in the following agrees with the paired t-test
# (Intercept) -0.2488202 0.3142115  9 -0.7918878  0.4488
# xx2          1.3325786 0.5779727  9  2.3056084  0.0466
</code></pre>

<p>Or another model, assuming a compound symmetry for the correlation matrix of the two conditions:</p>

<pre><code>summary(fm2 &lt;- lme(y ~ x, random=list(subj=pdCompSymm(form=~x-1)), data=myDat))

# the 2nd row in the following agrees with the paired t-test
# (Intercept) -0.2488202 0.4023431  9 -0.618428  0.5516
# xx2          1.3325786 0.5779727  9  2.305608  0.0466
</code></pre>

<p>With the paired t-test and one-way repeated-measures ANOVA, I can write down the traditional cell mean model as</p>

<pre><code>Yij = Î¼ + Î±i + Î²j + Îµij, i = 1, 2; j = 1, ..., 10
</code></pre>

<p>where i indexes condition, j indexes subject, Y<sub>ij</sub> is the response variable, Î¼ is constant for the fixed effect for overall mean, Î±<sub>i</sub> is the fixed effect for condition, Î²<sub>j</sub> is the random effect for subject following N(0, Ïƒ<sub>p</sub><sup>2</sup>) (Ïƒ<sub>p</sub><sup>2</sup> is population variance), and Îµ<sub>ij</sub> is residual following N(0, Ïƒ<sup>2</sup>) (Ïƒ<sup>2</sup> is within-subject variance).</p>

<p>I thought that the cell mean model above would not be appropriate for the lme models, but the trouble is that I can't come up with a reasonable model for the two lme() approaches with the correlation structure assumption. The reason is that the lme model seems to have more parameters for the random components than the cell mean model above offers. At least the lme model provides exactly the same F-value, degrees of freedom, and p-value as well, which gls cannot. More specifically gls gives incorrect DFs due to the fact that it does not account for the fact that each subject has two observations, leading to much inflated DFs. The lme model most likely is overparameterized in specifying the random effects, but I don't know what the model is and what the parameters are. So the issue is still unresolved for me. </p>
"
"0.132809685425692","0.146104310758895"," 24844","<p>I am running 3 models on 3 subsets of the same data.  The set up is as follows:</p>

<ol>
<li>Outcome (DV) is binary categorical</li>
<li>Time (IV) is repeated twice (pre and post)</li>
<li>Treatement (IV of interest) is binary categorical</li>
</ol>

<p>I am interested to know if at time 2 treatment has had an effect on outcome.  I used the lme4 package and used the following R code:</p>

<pre><code>tot.null&lt;-lmer(as.factor(outcome)~Time+(1|id), family=binomial(link='logit'),
             data=df.total)
tot.mod&lt;-lmer(as.factor(outcome)~trt*Time+(Time|id), 
             family=binomial(link='logit'), data=df.total)
anova(tot.null,tot.mod)
summary(tot.mod)
</code></pre>

<p><strong>Data head</strong></p>

<pre><code>   id             trt Time outcome
1   1 peer discussion   -1       1
2   2 peer discussion   -1       1
3   3 peer discussion   -1       0
4   4 peer discussion   -1       1
5   5 peer discussion   -1       1
</code></pre>

<p><strong>str of data</strong></p>

<pre><code>&gt; str(df.total)
'data.frame':   872 obs. of  4 variables:
 $ id     : int  1 2 3 4 5 6 7 8 9 10 ...
     $ trt    : Factor w/ 2 levels ""peer discussion"",..: 1 1 1 1 1 1 1 1 1 1 ...
 $ Time   : num  -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 ...
     $ outcome: num  1 1 1 1 1 1 1 0 1 0 ...
</code></pre>

<p>The problem is I get an error messoge on the <code>tot.mod</code>:</p>

<pre><code>&gt; tot.mod&lt;-glmer(as.factor(outcome)~trt*Time+(Time|id), family=binomial(link='logit'),
               data=df.total)
Warning message:
In mer_finalize(ans) : false convergence (8)
</code></pre>

<p>I think this is the reason the model is significant but none of the predictors are.  look at the inflated SEs.</p>

<p><strong>Comparison to the null model and the summary of full model</strong></p>

<pre><code>&gt; anova(tot.null,tot.mod)
Data: df.total
Models:
tot.null: as.factor(outcome) ~ Time + (1 | id)
tot.mod: as.factor(outcome) ~ trt * Time + (Time | id)
         Df    AIC    BIC  logLik  Chisq Chi Df            Pr(&gt;Chisq)    
tot.null  3 689.54 703.85 -341.77                                        
tot.mod   7 410.67 444.07 -198.34 286.86      4 &lt; 0.00000000000000022 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
&gt; summary(tot.mod)
Generalized linear mixed model fit by the Laplace approximation 
Formula: as.factor(outcome) ~ trt2 * Time + (Time | id) 
   Data: df.total 
   AIC   BIC logLik deviance
 410.7 444.1 -198.3    396.7
Random effects:
 Groups Name        Variance Std.Dev. Corr  
 id     (Intercept)  396.46  19.911         
        Time        1441.98  37.973   0.470 
Number of obs: 872, groups: id, 436

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept) 10.09866    3.33921   3.024  0.00249 **
trt21        0.01792    5.10796   0.004  0.99720   
Time        -0.93753    5.79560  -0.162  0.87149   
trt21:Time  -0.84882   10.41073  -0.082  0.93502   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Correlation of Fixed Effects:
           (Intr) trt21  Time  
trt21      -0.654              
Time        0.558 -0.365       
trt21:Time -0.311  0.473 -0.557
</code></pre>

<p>What's going on?  Why is the model significant but none of the betas?  In OLS I know this is an indicator of multi-colinearity among predictors.  I don't think that's the reason here.  Please help with understanding this problem as well as the error message (I think they may be connected).  What are some things I should check for?</p>

<p>The other two  models from the same data set (<code>split</code> on a different grouping variable) had no apparent problems.</p>

<p>Thank you in advance.</p>

<p><em>Using R 2.14.2, lme4 v. 0.999375-42 on a win 7 machine</em> </p>
"
"0.120316124812856","0.109960706249747"," 26461","<p>We have a data set with two covariates and a categorical grouping variable and want to know if there are significant differences between the slope or intercept among the covariates associated with the different grouping variables.  We've used anova() and lm() to compare the fits of three different models: 1) with a single slope and intercept, 2) with different intercepts for each group, and 3) with a slope and an intercept for each group.  According to the anova() general linear test, the second model is the most appropriate of the three, there is a significant improvement to the model by including a separate intercept for each group.  However, when we look at the 95% confidence intervals for these intercepts -- they all overlap, suggesting there aren't significant differences between the intercepts.  How can these two results be reconciled?  We thought another way of interpreting the results of the model-selection method was that there has to be at least one significant difference among the intercepts... but perhaps this is not correct?</p>

<p>Below is the R code to replicate this analysis.  We've used the dput() function so you can work with exactly the same data we're grappling with.</p>

<pre><code># Begin R Script
# &gt; dput(data)
structure(list(Head = c(1.92, 1.93, 1.79, 1.94, 1.91, 1.88, 1.91, 
1.9, 1.97, 1.97, 1.95, 1.93, 1.95, 2, 1.87, 1.88, 1.97, 1.88, 
1.89, 1.86, 1.86, 1.97, 2.02, 2.04, 1.9, 1.83, 1.95, 1.87, 1.93, 
1.94, 1.91, 1.96, 1.89, 1.87, 1.95, 1.86, 2.03, 1.88, 1.98, 1.97, 
1.86, 2.04, 1.86, 1.92, 1.98, 1.86, 1.83, 1.93, 1.9, 1.97, 1.92, 
2.04, 1.92, 1.9, 1.93, 1.96, 1.91, 2.01, 1.97, 1.96, 1.76, 1.84, 
1.92, 1.96, 1.87, 2.1, 2.17, 2.1, 2.11, 2.17, 2.12, 2.06, 2.06, 
2.1, 2.05, 2.07, 2.2, 2.14, 2.02, 2.08, 2.16, 2.11, 2.29, 2.08, 
2.04, 2.12, 2.02, 2.22, 2.22, 2.2, 2.26, 2.15, 2, 2.24, 2.18, 
2.07, 2.06, 2.18, 2.14, 2.13, 2.2, 2.1, 2.13, 2.15, 2.25, 2.14, 
2.07, 1.98, 2.16, 2.11, 2.21, 2.18, 2.13, 2.06, 2.21, 2.08, 1.88, 
1.81, 1.87, 1.88, 1.87, 1.79, 1.99, 1.87, 1.95, 1.91, 1.99, 1.85, 
2.03, 1.88, 1.88, 1.87, 1.85, 1.94, 1.98, 2.01, 1.82, 1.85, 1.75, 
1.95, 1.92, 1.91, 1.98, 1.92, 1.96, 1.9, 1.86, 1.97, 2.06, 1.86, 
1.91, 2.01, 1.73, 1.97, 1.94, 1.81, 1.86, 1.99, 1.96, 1.94, 1.85, 
1.91, 1.96, 1.9, 1.98, 1.89, 1.88, 1.95, 1.9, 1.94, NA, 1.84, 
1.83, 1.84, 1.96, 1.74, 1.91, 1.84, 1.88, 1.83, 1.93, 1.78, 1.88, 
1.93, 2.15, 2.16, 2.23, 2.09, 2.36, 2.31, 2.25, 2.29, 2.3, 2.04, 
2.22, 2.19, 2.25, 2.31, 2.3, 2.28, 2.25, 2.15, 2.29, 2.24, 2.34, 
2.2, 2.24, 2.17, 2.26, 2.18, 2.17, 2.34, 2.23, 2.36, 2.31, 2.13, 
2.2, 2.27, 2.27, 2.2, 2.34, 2.12, 2.26, 2.18, 2.31, 2.24, 2.26, 
2.15, 2.29, 2.14, 2.25, 2.31, 2.13, 2.09, 2.24, 2.26, 2.26, 2.21, 
2.25, 2.29, 2.15, 2.2, 2.18, 2.16, 2.14, 2.26, 2.22, 2.12, 2.12, 
2.16, 2.27, 2.17, 2.27, 2.17, 2.3, 2.25, 2.17, 2.27, 2.06, 2.13, 
2.11, 2.11, 1.97, 2.09, 2.06, 2.11, 2.09, 2.08, 2.17, 2.12, 2.13, 
1.99, 2.08, 2.01, 1.97, 1.97, 2.09, 1.94, 2.06, 2.09, 2.04, 2, 
2.14, 2.07, 1.98, 2, 2.19, 2.12, 2.06, 2, 2.02, 2.16, 2.1, 1.97, 
1.97, 2.1, 2.02, 1.99, 2.13, 2.05, 2.05, 2.16, 2.02, 2.02, 2.08, 
1.98, 2.04, 2.02, 2.07, 2.02, 2.02, 2.02), Site = structure(c(2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 5L, 
5L, 5L, 5L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
4L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 6L, 
6L, 6L, 6L, 6L, 6L, 6L, 6L), .Label = c(""ANZ"", ""BC"", ""DV"", ""MC"", 
""RB"", ""WW""), class = ""factor""), Leg = c(2.38, 2.45, 2.22, 2.23, 
2.26, 2.32, 2.28, 2.17, 2.39, 2.27, 2.42, 2.33, 2.31, 2.32, 2.25, 
2.27, 2.38, 2.28, 2.33, 2.24, 2.21, 2.22, 2.42, 2.23, 2.36, 2.2, 
2.28, 2.23, 2.33, 2.35, 2.36, 2.26, 2.26, 2.3, 2.23, 2.31, 2.27, 
2.23, 2.37, 2.27, 2.26, 2.3, 2.33, 2.34, 2.27, 2.4, 2.22, 2.25, 
2.28, 2.33, 2.26, 2.32, 2.29, 2.31, 2.37, 2.24, 2.26, 2.36, 2.32, 
2.32, 2.15, 2.2, 2.29, 2.37, 2.26, 2.24, 2.23, 2.24, 2.26, 2.18, 
2.11, 2.23, 2.31, 2.25, 2.15, 2.3, 2.33, 2.35, 2.21, 2.36, 2.27, 
2.24, 2.35, 2.24, 2.33, 2.32, 2.24, 2.35, 2.36, 2.39, 2.28, 2.36, 
2.19, 2.27, 2.39, 2.23, 2.29, 2.32, 2.3, 2.32, NA, 2.25, 2.24, 
2.21, 2.37, 2.21, 2.21, 2.27, 2.27, 2.26, 2.19, 2.2, 2.25, 2.25, 
2.25, NA, 2.24, 2.17, 2.2, 2.2, 2.18, 2.14, 2.17, 2.27, 2.28, 
2.27, 2.29, 2.23, 2.25, 2.33, 2.22, 2.29, 2.19, 2.15, 2.24, 2.24, 
2.26, 2.25, 2.09, 2.27, 2.18, 2.2, 2.25, 2.24, 2.18, 2.3, 2.26, 
2.18, 2.27, 2.12, 2.18, 2.33, 2.13, 2.28, 2.23, 2.16, 2.2, 2.3, 
2.31, 2.18, 2.33, 2.29, 2.26, 2.21, 2.22, 2.27, 2.32, 2.24, 2.25, 
2.17, 2.2, 2.26, 2.27, 2.24, 2.25, 2.09, 2.25, 2.21, 2.24, 2.21, 
2.22, 2.13, 2.24, 2.21, 2.3, 2.34, 2.35, 2.32, 2.46, 2.43, 2.42, 
2.41, 2.32, 2.25, 2.33, 2.19, 2.45, 2.32, 2.4, 2.38, 2.35, 2.39, 
2.29, 2.35, 2.43, 2.29, 2.33, 2.31, 2.28, 2.38, 2.32, 2.43, 2.27, 
2.4, 2.37, 2.27, 2.41, 2.32, 2.38, 2.23, 2.33, 2.21, 2.34, 2.19, 
2.34, 2.35, 2.35, 2.31, 2.33, 2.41, 2.53, 2.39, 2.17, 2.16, 2.38, 
2.34, 2.33, 2.33, 2.29, 2.43, 2.28, 2.34, 2.38, 2.3, 2.29, 2.43, 
2.36, 2.24, 2.35, 2.38, 2.4, 2.36, 2.42, 2.28, 2.45, 2.33, 2.32, 
2.33, 2.31, 2.44, 2.37, 2.4, 2.35, 2.33, 2.31, 2.36, 2.43, 2.38, 
2.4, 2.38, 2.46, 2.33, 2.38, 2.23, 2.24, 2.39, 2.36, 2.19, 2.32, 
2.37, 2.39, 2.34, 2.39, 2.23, 2.25, 2.29, 2.39, 2.35, NA, 2.28, 
2.35, 2.38, 2.34, 2.17, 2.29, NA, 2.26, NA, NA, NA, 2.24, 2.33, 
2.23, 2.28, 2.29, 2.23, 2.2, 2.27, 2.31, 2.31, 2.26, 2.28)), .Names = c(""Head"", 
""Site"", ""Leg""), class = ""data.frame"", row.names = c(NA, -312L
)) 

# plot graph
library(ggplot2)

qplot(Head, Leg, 
    color=Site, 
    data=data) + 
        stat_smooth(method=""lm"", alpha=0.2) + 
        theme_bw()
</code></pre>

<p><img src=""http://i.stack.imgur.com/QMIBf.jpg"" alt=""enter image description here""></p>

<pre><code># create linear models
lm.1 &lt;- lm(Leg ~ Head, data)
lm.2 &lt;- lm(Leg ~ Head + Site, data)
lm.3 &lt;- lm(Leg ~ Head*Site, data)

# evaluate linear models
anova(lm.1, lm.2, lm.3)
anova(lm.1, lm.2)

# &gt; anova(lm.1, lm.2)
# Analysis of Variance Table
# Model 1: Leg.3.1 ~ Head.W1
# Model 2: Leg.3.1 ~ Head.W1 + Site
  # Res.Df     RSS Df Sum of Sq     F    Pr(&gt;F)    
# 1    302 1.25589                                 
# 2    297 0.91332  5   0.34257 22.28 &lt; 2.2e-16 ***


# examining the multiple-intercepts model (lm.2)
summary(lm.2)
coef(lm.2)
confint(lm.2)

# extracting the intercepts
intercepts &lt;- coef(lm.2)[c(1, 3:7)]
intercepts.1 &lt;- intercepts[1]
intercepts &lt;- intercepts.1 + intercepts
intercepts[1] &lt;- intercepts.1
intercepts

# extracting the confidence intervals
ci &lt;- confint(lm.2)[c(1, 3:7),]
ci[2:6,] &lt;- ci[2:6,] + confint(lm.2)[1,]
ci[,1]

# putting everything together in a dataframe
labels &lt;- c(""ANZ"", ""BC"", ""DV"", ""MC"", ""RB"", ""WW"")
ci.dataframe &lt;- data.frame(Site=labels, Intercept=intercepts, CI.low = ci[,1], CI.high = ci[,2])
ci.dataframe

# plotting intercepts and 95% CI
qplot(Site, Intercept, geom=c(""point"", ""errorbar""), ymin=CI.low, ymax=CI.high, data=ci.dataframe, ylab=""Intercept &amp; 95% CI"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/40PNp.jpg"" alt=""ancova intercepts""></p>

<p>Just to summarize -- the problem is that the 95% CIs for the intercepts all overlap, but the model selection method suggests that the best model is one that fits different intercepts.  So I'm inclined to think either our model selection method is flawed or the 95% CIs for the intercept estimates were calculated incorrectly.  Any thoughts would be greatly appreciated!</p>
"
"0.164529826154006","0.167076671386255"," 29280","<p>I can't get the same results in R as in GraphPad Prism for repeated measures anova.</p>

<p>The experiment was a stimulation time course, so I have as DV=response and as factor ""time"" within groups, also I add a factor sample for each experiment</p>

<pre><code>data &lt;- read.csv(""http://dl.dropbox.com/u/4828275/datos.csv"")
options(contrasts=c(""contr.sum"",""contr.poly""))

## Convert variables to factor
data &lt;- within(data, {
sample &lt;- factor(sample)
time &lt;- factor(time)
})
aov &lt;- aov(response~time+sample, data=data)
summary(glht(aov, linfct=mcp(time=""Dunnett"")))

     Simultaneous Tests for General Linear Hypotheses

Multiple Comparisons of Means: Dunnett Contrasts


Fit: lme.formula(fixed = response ~ time, data = data, random = ~1 | 
sample)

Linear Hypotheses:
            Estimate Std. Error z value Pr(&gt;|z|)
2 - 0 == 0    1.1789     2.0800   0.567        1
5 - 0 == 0    1.2966     2.0800   0.623        1
10 - 0 == 0   1.0555     2.0800   0.507        1
15 - 0 == 0   0.4317     2.0800   0.208        1
30 - 0 == 0   0.2148     2.0800   0.103        1
(Adjusted p values reported -- bonferroni method)
</code></pre>

<p>For repeated measures I have this code</p>

<pre><code>aov.repeated &lt;- ezANOVA(
  data
  , dv = .(response)
  , wid = .(time)
  , within = .(sample)
  , type = 1
  , return_aov = TRUE
)$aov
</code></pre>

<p>The GraphPad Prism results for the same data was</p>

<pre><code>Table Analyzed  Data 1              

Repeated Measures ANOVA                 
  P value   0.0415              
  P value summary   *               
  Are means signif. different? (P &lt; 0.05)   Yes             
  Number of groups  6               
  F 2.863               
  R square  0.4172              

 Was the pairing significantly effective?                   
  R square  0.1980              
  F 2.119               
  P value   0.1162              
  P value summary   ns              
  Is there significant matching? (P &lt; 0.05) No              

ANOVA Table SS  df  MS      
  Treatment (between columns)   130.6   5   26.12       
  Individual (between rows) 77.30   4   19.32       
  Residual (random) 182.4   20  9.121       
  Total 390.3   29          

Dunnett's Multiple Comparison Test  Mean Diff.  q   Significant? P &lt; 0.05?  Summary 95% CI of diff
  0 vs 2    -2.861  1.498   No  ns  -8.085 to 2.362
  0 vs 5    -5.777  3.024   Yes *   -11.00 to -0.5531
  0 vs 10   -6.009  3.146   Yes *   -11.23 to -0.7855
  0 vs 15   -4.621  2.419   No  ns  -9.844 to 0.6029
  0 vs 30   -2.581  1.351   No  ns  -7.805 to 2.642
</code></pre>

<p>How can I get the same results as above in R?
Is there a way to get Dunnett's Multiple Comparison Test in aov.repeated? </p>
"
"0.119713032670143","0.121566134770966"," 29981","<p>Let's have some linear model, for example just simple ANOVA:</p>

<pre><code># data generation
set.seed(1.234)                      
Ng &lt;- c(41, 37, 42)                    
data &lt;- rnorm(sum(Ng), mean = rep(c(-1, 0, 1), Ng), sd = 1)      
fact &lt;- as.factor(rep(LETTERS[1:3], Ng)) 

m1 = lm(data ~ 0 + fact)
summary(m1)
</code></pre>

<p>Result is as follows:</p>

<pre><code>Call:
lm(formula = data ~ 0 + fact)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.30047 -0.60414 -0.04078  0.54316  2.25323 

Coefficients:
      Estimate Std. Error t value Pr(&gt;|t|)    
factA  -0.9142     0.1388  -6.588 1.34e-09 ***
factB   0.1484     0.1461   1.016    0.312    
factC   1.0990     0.1371   8.015 9.25e-13 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.8886 on 117 degrees of freedom
Multiple R-squared: 0.4816,     Adjusted R-squared: 0.4683 
F-statistic: 36.23 on 3 and 117 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>Now I try two different methods to estimate confidence interval of these parameters</p>

<pre><code>c = coef(summary(m1))

# 1st method: CI limits from SE, assuming normal distribution
cbind(low = c[,1] - qnorm(p = 0.975) * c[,2], 
    high = c[,1] + qnorm(p = 0.975) * c[,2])

# 2nd method
confint(m1)
</code></pre>

<h2>Questions:</h2>

<ol>
<li>What is the distribution of estimated linear regression coefficients? Normal or $t$?</li>
<li>Why do both methods yield different results? Assuming normal distribution and correct SE, I'd expect both methods to have the same result.</li>
</ol>

<p>Thank you very much!</p>

<p>data ~ 0 + fact</p>

<p><strong>EDIT after an answer</strong>:</p>

<p>The answer is exact, this will give exactly the same result as <code>confint(m1)</code>!</p>

<pre><code># 3rd method
cbind(low = c[,1] - qt(p = 0.975, df = sum(Ng) - 3) * c[,2], 
    high = c[,1] + qt(p = 0.975, df = sum(Ng) - 3) * c[,2])
</code></pre>
"
"0.199521721116906","0.194505815633546"," 31118","<p>I performed an experiment where I raised different families coming from two different source populations, where each family was split up into a different treatments. After the experiment I measured several traits on each individual. 
To test for an effect of either treatment or source as well as their interaction, I used a linear mixed effect model with family as random factor, i.e.</p>

<pre><code>lme(fixed=Trait~Treatment*Source,random=~1|Family,method=""ML"")
</code></pre>

<p>so far so good,
Now I have to calculate the relative variance components, i.e. the percentage of variation that is explained by either treatment or source as well as the interaction.</p>

<p>Without a random effect, I could easily use the sums of squares (SS) to calculate the variance explained by each factor. But for a mixed model (with ML estimation), there are no SS, hence I thought I could use Treatment and Source as random effects too to estimate the variance, i.e.</p>

<pre><code>lme(fixed=Trait~1,random=~(Treatment*Source)|Family, method=""REML"")
</code></pre>

<p>However, in some cases, lme does not converge, hence I used lmer from the lme4 package:</p>

<pre><code>lmer(Trait~1+(Treatment*Source|Family),data=DATA)
</code></pre>

<p>Where I extract the variances from the model using the summary function:</p>

<pre><code>model&lt;-lmer(Trait~1+(Treatment*Source|Family),data=regrexpdat)
results&lt;-model@REmat
variances&lt;-results[,3]
</code></pre>

<p>I get the same values as with the VarCorr function. I use then these values to calculate the actual percentage of variation taking the sum as the total variation.</p>

<p>Where I am struggling is with the interpretation of the results from the initial lme model (with treatment and source as fixed effects) and the random model to estimate the variance components (with treatment and source as random effect). I find in most cases that the percentage of variance explained by each factor does not correspond to the significance of the fixed effect.</p>

<p>For example for the trait HD,
The initial lme suggests a tendency for the interaction as well as a significance for Treatment. Using a backward procedure, I find that Treatment has a close to significant tendency. However, estimating variance components, I find that Source has the highest variance, making up to 26.7% of the total variance.</p>

<p>The lme:</p>

<pre><code>anova(lme(fixed=HD~as.factor(Treatment)*as.factor(Source),random=~1|as.factor(Family),method=""ML"",data=test),type=""m"")
                                      numDF denDF  F-value p-value
(Intercept)                                1   426 0.044523  0.8330
as.factor(Treatment)                       1   426 5.935189  0.0153
as.factor(Source)                          1    11 0.042662  0.8401
as.factor(Treatment):as.factor(Source)     1   426 3.754112  0.0533
</code></pre>

<p>And the lmer:</p>

<pre><code>summary(lmer(HD~1+(as.factor(Treatment)*as.factor(Source)|Family),data=regrexpdat))
Linear mixed model fit by REML 
Formula: HD ~ 1 + (as.factor(Treatment) * as.factor(Source) | Family) 
   Data: regrexpdat 
    AIC    BIC logLik deviance REMLdev
 -103.5 -54.43  63.75   -132.5  -127.5
Random effects:
 Groups   Name                                      Variance  Std.Dev. Corr                 
 Family   (Intercept)                               0.0113276 0.106431                      
          as.factor(Treatment)                      0.0063710 0.079819  0.405               
          as.factor(Source)                         0.0235294 0.153393 -0.134 -0.157        
          as.factor(Treatment)L:as.factor(Source)   0.0076353 0.087380 -0.578 -0.589 -0.585 
 Residual                                           0.0394610 0.198648                      
Number of obs: 441, groups: Family, 13

Fixed effects:
            Estimate Std. Error t value
(Intercept) -0.02740    0.03237  -0.846
</code></pre>

<p>Hence my question is, is it correct what I am doing? Or should I use another way to estimate the amount of variance explained by each factor (i.e. Treatment, Source and their interaction). For example, would the effect sizes be a more appropriate way to go?</p>

<p>Thanks!</p>

<p>Kay Lucek</p>
"
"0.0904945466110142","0.107211253483779"," 31494","<p>I'm very new to all this, and I am testing different ways to perform a two-way type III ANOVA on my data.</p>

<ul>
<li>I have tried <code>anova()</code> from the <code>stats</code> package, after fitting a linear regression with <code>lm()</code>;</li>
<li>I have tried <code>Anova()</code> from the <code>car</code> package, using the same linear regression (and this gives me the same result as <code>anova()</code> when I use <code>type=""II""</code> - I thought <code>anova()</code> used type I SS by default?).</li>
<li>And I am now trying to use <code>ezANOVA()</code> from the <code>ez</code> package.</li>
</ul>

<p>With this last one, I can't understand what the <code>wid=.()</code> argument is (even reading the help), and as it is not optional, I can't leave it blank. What I am trying to use is as follows, with its result:</p>

<pre><code>&gt; attach(data)
&gt; library(""ez"")
&gt; ezANOVA(data=data, dv=.(AG.DW), wid=.(), within=.(Genotype, Treatment), type=3)
Warning: Converting """" to factor for ANOVA.
Error in sort.list(y) : 'x' must be atomic for 'sort.list'
Have you called 'sort' on a list?
</code></pre>

<p>Is this the right script? What is <code>wid</code> and what should I fill it with?</p>

<p>Concerning my data, the columns <code>Genotype</code> and <code>Treatment</code> are my two factors, and I want to see if there is an interaction when looking at the aboveground dry weight of my plants (column <code>AG.DW</code>). My data is balanced.</p>

<p>I am sorry if information is missing or inaccurate here: this is my first contribution here, and I am only discovering statistics at the moment (and I can't see how to join my data file).</p>
"
"0.0754121221758452","0.0765794667741282"," 31629","<p>I need to conduct a type II anova. Therefore, I use the <code>Anova</code> function from the <code>car</code> package. In addition, for <code>Factor1</code> I want to use a polynomial contrast matrix to see linear and quadratic effects.</p>

<p>However, using the <code>Anova</code> function from the <code>car</code> package does not display the linear and quadratic effects compared to the normal <code>summary.lm(lm(...))</code> functions. Before I apply the <code>Anova</code> function from the <code>car</code> package I test for non significant interaction as that is required for doing a type II anova.</p>

<p>I was wondering how I can extract the linear and the quadratic effects from the <code>Anova</code> output.</p>

<p>Here is a working example in R:</p>

<p><a href=""http://pastebin.com/mfk5PEq1"" rel=""nofollow"">http://pastebin.com/mfk5PEq1</a></p>

<p>Cheers</p>
"
"0.191540852272229","0.186401406648815"," 32040","<p>I'm trying to analyze effect of Year on variable logInd for particular group of individuals (I have 3 groups). <strong>The simplest model:</strong></p>

<pre><code>&gt; fix1 = lm(logInd ~ 0 + Group + Year:Group, data = mydata)
&gt; summary(fix1)

Call:
lm(formula = logInd ~ 0 + Group + Year:Group, data = mydata)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.5835 -0.3543 -0.0024  0.3944  4.7294 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
Group1       4.6395740  0.0466217  99.515  &lt; 2e-16 ***
Group2       4.8094268  0.0534118  90.044  &lt; 2e-16 ***
Group3       4.5607287  0.0561066  81.287  &lt; 2e-16 ***
Group1:Year -0.0084165  0.0027144  -3.101  0.00195 ** 
Group2:Year  0.0032369  0.0031098   1.041  0.29802    
Group3:Year  0.0006081  0.0032666   0.186  0.85235    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 0.7926 on 2981 degrees of freedom
Multiple R-squared: 0.9717,     Adjusted R-squared: 0.9716 
F-statistic: 1.705e+04 on 6 and 2981 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>We can see the Group1 is significantly declining, the Groups2 and 3 increasing but not significantly so.</p>

<p><strong>Clearly the individual should be random effect, so I introduce random intercept effect for each individual:</strong></p>

<pre><code>&gt; mix1a = lmer(logInd ~ 0 + Group + Year:Group + (1|Individual), data = mydata)
&gt; summary(mix1a)
Linear mixed model fit by REML 
Formula: logInd ~ 0 + Group + Year:Group + (1 | Individual) 
   Data: mydata 
  AIC  BIC logLik deviance REMLdev
 4727 4775  -2356     4671    4711
Random effects:
 Groups     Name        Variance Std.Dev.
 Individual (Intercept) 0.39357  0.62735 
 Residual               0.24532  0.49530 
Number of obs: 2987, groups: Individual, 103

Fixed effects:
              Estimate Std. Error t value
Group1       4.6395740  0.1010868   45.90
Group2       4.8094268  0.1158095   41.53
Group3       4.5607287  0.1216522   37.49
Group1:Year -0.0084165  0.0016963   -4.96
Group2:Year  0.0032369  0.0019433    1.67
Group3:Year  0.0006081  0.0020414    0.30

Correlation of Fixed Effects:
            Group1 Group2 Group3 Grp1:Y Grp2:Y
Group2       0.000                            
Group3       0.000  0.000                     
Group1:Year -0.252  0.000  0.000              
Group2:Year  0.000 -0.252  0.000  0.000       
Group3:Year  0.000  0.000 -0.252  0.000  0.000
</code></pre>

<p>It had an expected effect - the SE of slopes (coefficients Group1-3:Year) are now lower and the residual SE is also lower.</p>

<p><strong>The individuals are also different in slope so I also introduced the random slope effect:</strong></p>

<pre><code>&gt; mix1c = lmer(logInd ~ 0 + Group + Year:Group + (1 + Year|Individual), data = mydata)
&gt; summary(mix1c)
Linear mixed model fit by REML 
Formula: logInd ~ 0 + Group + Year:Group + (1 + Year | Individual) 
   Data: mydata 
  AIC  BIC logLik deviance REMLdev
 2941 3001  -1461     2885    2921
Random effects:
 Groups     Name        Variance  Std.Dev. Corr   
 Individual (Intercept) 0.1054790 0.324775        
            Year        0.0017447 0.041769 -0.246 
 Residual               0.1223920 0.349846        
Number of obs: 2987, groups: Individual, 103

Fixed effects:
              Estimate Std. Error t value
Group1       4.6395740  0.0541746   85.64
Group2       4.8094268  0.0620648   77.49
Group3       4.5607287  0.0651960   69.95
Group1:Year -0.0084165  0.0065557   -1.28
Group2:Year  0.0032369  0.0075105    0.43
Group3:Year  0.0006081  0.0078894    0.08

Correlation of Fixed Effects:
            Group1 Group2 Group3 Grp1:Y Grp2:Y
Group2       0.000                            
Group3       0.000  0.000                     
Group1:Year -0.285  0.000  0.000              
Group2:Year  0.000 -0.285  0.000  0.000       
Group3:Year  0.000  0.000 -0.285  0.000  0.000
</code></pre>

<h3><strong>But now, contrary to the expectation, the SE of slopes (coefficients Group1-3:Year) are now much higher, even higher than with no random effect at all!</strong></h3>

<p>How is this possible? I would expect that the random effect will ""eat"" the unexplained variability and increase ""sureness"" of the estimate!</p>

<p>However, the residual SE behaves as expected - it is lower than in the random intercept model.</p>

<p><a href=""http://artax.karlin.mff.cuni.cz/~ttel5535/pub/my_so_question_data.R"" rel=""nofollow"">Here is the data</a> if needed.</p>

<h2>Edit</h2>

<p>Now I realized astonishing fact. If I do the linear regression for each individual separately and then run ANOVA on the resultant slopes, <strong>I get exactly the same result as the random slope model!</strong> Would you know why?</p>

<pre><code>indivSlope = c()
for (indiv in 1:103) {
    mod1 = lm(logInd ~ Year, data = mydata[mydata$Individual == indiv,])
    indivSlope[indiv] = coef(mod1)['Year']
}

indivGroup = unique(mydata[,c(""Individual"", ""Group"")])[,""Group""]


anova1 = lm(indivSlope ~ 0 + indivGroup)
summary(anova1)

Call:
lm(formula = indivSlope ~ 0 + indivGroup)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.176288 -0.016502  0.004692  0.020316  0.153086 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
indivGroup1 -0.0084165  0.0065555  -1.284    0.202
indivGroup2  0.0032369  0.0075103   0.431    0.667
indivGroup3  0.0006081  0.0078892   0.077    0.939

Residual standard error: 0.04248 on 100 degrees of freedom
Multiple R-squared: 0.01807,    Adjusted R-squared: -0.01139 
F-statistic: 0.6133 on 3 and 100 DF,  p-value: 0.6079 
</code></pre>

<p><a href=""http://artax.karlin.mff.cuni.cz/~ttel5535/pub/my_so_question_data.R"" rel=""nofollow"">Here is the data</a> if needed.</p>
"
"0.071383061024825","0.0906100470365937"," 32087","<p>My study design involves a control and 2 test groups plus some covariates. Each group consists of around 20 observations. In total I look at around 1,000 variables.</p>

<p>I created a linear model using the <code>lm()</code> function in R including 2 covariates. After that I thought I would include another covariate because doing a PCA plot earlier showed a slight effect on that covariate. However, after adding this covariate to the model 50% of the significant hits are now different. I was actually assuming that it would pretty much identical as the effect was hardly seen in the PCA.</p>

<p>Could it be that I have overfitted the model? Or is the effect simple just not shown in the PCA plot but is there?</p>

<p>I just compared the two models using <code>anova(lm1, lm2)</code> and the p-value is significant which I think means that the third covariate adds significant information to the model?</p>

<pre><code>lm1 &lt;- lm(var ~ factor_of_interest + cov1 + cov2)
lm2 &lt;- lm(var ~ factor_of_interest + cov1 + cov2 + cov3)
anova(lm1, lm2)
</code></pre>
"
"0.071383061024825","0.0906100470365937"," 32094","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/32087/linear-model-overfitting-due-to-too-many-covariates"">Linear model overfitting  due to too many covariates</a>  </p>
</blockquote>



<p>My study design involves a control and 2 test groups plus some covariates.
Each group consists of around 20 observations. In total I look at around a 1000 variables.</p>

<p>I created a linear model using the <code>lm</code> function in R including 2 covariates. After that I thought I include another covariate because doing a PCA plot earlier showed a slight effect on that covariate. However, after adding this covariate to the model 50% of the significant hits are now different. I was actually assuming that it would pretty much identical as the effect was hardly seen in the PCA. </p>

<p>Could it be that I have overfitted the model? Or is the effect simple just not shown in the PCA plot but is there? </p>

<p>I just compared the two models using <code>anova(lm1, lm2)</code> and the p-value is significant which I think means that the third covariate adds significant information to the model?</p>

<pre><code>lm1 &lt;- lm(var ~ factor_of_interest + cov1 + cov2)
lm2 &lt;- lm(var ~ factor_of_interest + cov1 + cov2 + cov3)

anova(lm1, lm2)
</code></pre>
"
"0.0977452818676612","0.099258333397093"," 32248","<p>I'm trying to report an effect size for a Linear Mixed-Model we've fitted in R. Right now I'm looking at reporting partial eta squared or eta squared. However, to do so I need to calculate the Sums of Squared Error. We're using the <code>lme()</code> function, which does not report MSE or effect sizes. I am not the one doing the primary analysis, so I don't know if we can switch to using the <code>ez</code> package, as described in <a href=""http://stats.stackexchange.com/questions/2962/omega-squared-for-measure-of-effect-in-r"">Omega squared for measure of effect in R?</a>.</p>

<p>The <code>lmeObject</code> returned by the <code>lme()</code> function has some information, but I am not sure which is the most appropriate. When using <code>anova()</code> on the <code>lmeObject</code>, it reports the denominator degrees of freedom for the F-test as 56683, so with a value for MSE I can calculate SSE and reverse-engineer the F-tests to get the SStreatments I need for partial eta squared. I have 2 fixed effects and one random effect (for a repeated-measures design).</p>

<p>I've looked at <code>lmeObject$sigma</code> and calculated the sums of the squares of <code>lmeObject$residuals[,1]</code>, but they don't agree (I'm squaring the <code>sigma</code> and dividing the SS by the degrees of freedom).</p>

<p>Any R masters out there that can tell how to calculate MSE or SSE from an <code>lmeObject</code>?</p>
"
"0.11286652959662","0.114613651012251"," 32735","<p>As a financial institution, we often run into analysis of time series data. A lot of times we end up doing regression using time series variables. As this happens, we often encounter residuals with time series structure that violates basic assumption of independent errors in OLS regression.  Recently we are building another model in which I believe we have regression with autocorrelated errors.The residuals from linear model have <code>lm(object)</code> has clearly a AR(1) structure, as evident from ACF and PACF.  I took two different approaches, the first one was obviously to fit the model using Generalized least squares <code>gls()</code> in R. My expectation was that the residuals from gls(object) would be a white noise (independent errors).  But the residuals from <code>gls(object)</code> still have the same ARIMA structure as in the ordinary regression. Unfortunately there is something wrong in what I am doing that I could not figure out.  Hence I decided to manually adjust the regression coefficients from the linear model (OLS estimates). Surprisingly that seems to be working when I plotted the residuals from adjusted regression (the residuals are white noise). I really want to use <code>gls()</code> in <code>nlme</code> package so that coding will be lot simpler and easier. What would be the approach I should take here? Am I supposed to use REML? or is my expectation of non-correlated residuals (white noise) from gls() object wrong?</p>

<pre><code>gls.bk_ai &lt;- gls(PRNP_BK_actINV ~ PRM_BK_INV_ENDING + NPRM_BK_INV_ENDING, 
                 correlation=corARMA(p=1), method='ML',  data  = fit.cap01A)

gls2.bk_ai  &lt;- update(gls.bk_ai, correlation = corARMA(p=2))

gls3.bk_ai &lt;- update(gls.bk_ai, correlation = corARMA(p=3))

gls0.bk_ai &lt;- update(gls.bk_ai, correlation = NULL)

anova(gls.bk_ai, gls2.bk_ai, gls3.bk_ai, gls0.bk_ai)  
     ## looking at the AIC value, gls model with AR(1) will be the best bet

acf2(residuals(gls.bk_ai)) # residuals are not white noise
</code></pre>

<p>Is there something wrong with what I am doing???????</p>
"
"NaN","NaN"," 34069","<p>I need to do some simple mean comparisons between groups (basic ANOVA F-tests) on data with missing values. I use the <a href=""http://cran.r-project.org/web/packages/mice/index.html"">mice</a> package in R for multiple imputation, but I can only pool results for the linear model coefficients, or the $R^2$.</p>

<p>Does anyone know how to combine to pool multiple F-statistics from each linear model fit? Or, how can I compute the standard errors for the F-test?</p>
"
"0.0977452818676612","0.0827152778309109"," 37805","<p>I have a GLMM of the form: </p>

<pre><code>lmer(present? ~ factor1 + factor2 + continuous + factor1*continuous + 
                (1 | factor3), family=binomial)
</code></pre>

<p>When I use <code>drop1(model, test=""Chi"")</code>, I get different results than if I use <code>Anova(model, type=""III"")</code> from the car package or <code>summary(model)</code>. These latter two give the same answers. </p>

<p>Using a bunch of fabricated data, I have found that these two methods normally do not differ. They give the same answer for balanced linear models, unbalanced linear models (where unequal n in different groups), and for balanced generalised linear models, but not for balanced generalised linear mixed models. So it appears that only in cases where random factors are included does this discord manifest.</p>

<ul>
<li>Why is there a discrepancy between these two methods?  </li>
<li>When using GLMM should <code>Anova()</code> or <code>drop1()</code> be used?  </li>
<li>The difference between these two is rather slight, at least for my data. Does it even matter which is used?</li>
</ul>
"
"0.138232703275227","0.140372481268719"," 37944","<p>I am currently using the R package <a href=""http://cran.r-project.org/web/packages/lme4/lme4.pdf"">lme4</a>.</p>

<p>I am using a linear mixed effects models with random effects:</p>

<pre><code>library(lme4)
mod1 &lt;- lmer(r1 ~ (1 | site), data = sample_set) #Only random effects
mod2 &lt;- lmer(r1 ~ p1 + (1 | site), data = sample_set) #One fixed effect + 
            # random effects
mod3 &lt;- lmer(r1 ~ p1 + p2 + (1 | site), data = sample_set) #Two fixed effects + 
            # random effects
</code></pre>

<p>To compare models, I am using the <code>anova</code> function and looking at differences in AIC relative to the lowest AIC model:</p>

<pre><code>anova(mod1, mod2, mod3)
</code></pre>

<p>The above is fine for comparing models. </p>

<p>However, I also need some simple way to interpret goodness of fit measures for each model. Does anyone have experience with such measures? I have done some research, and there are journal papers on R squared for the fixed effects of mixed effects models:</p>

<ul>
<li>Cheng, J., Edwards, L. J., Maldonado-Molina, M. M., Komro, K. A., &amp; Muller, K. E. (2010). Real longitudinal data analysis for real people: Building a good enough mixed model. Statistics in Medicine, 29(4), 504-520. doi: 10.1002/sim.3775  </li>
<li>Edwards, L. J., Muller, K. E., Wolfinger, R. D., Qaqish, B. F., &amp; Schabenberger, O. (2008). An R2 statistic for fixed effects in the linear mixed model. Statistics in Medicine, 27(29), 6137-6157. doi: 10.1002/sim.3429  </li>
</ul>

<p>It seems however, that there is some criticism surrounding the use of measures such as those proposed in the above papers.</p>

<p>Could someone please suggest a few easy to interpret, goodness of fit measures that could apply to my models?  </p>
"
"NaN","NaN"," 40884","<p>How do I test for Lack Of Fit (F-test) using R? I've seen a similar question, but that was for SPSS and it was just said that is can be easily done in R, but not how. </p>

<p>I know in simple linear regression I would use <code>anova(fm1,fm2)</code>, <code>fm1</code> being my model, <code>fm2</code> being the same model with <code>x</code> as a factor (if there are several <code>y</code> for <code>x</code>).
How do I do it in multiple linear regression? </p>
"
"0.0987582133970426","0.0859602382591879"," 41510","<p>How do you explain that ? There's only one operator but the mixed model returns an estimate for the <code>operator</code> random effect. Furthermore the <code>sample</code> effect is confounded with the interaction <code>sample:operator</code>. Below is the R code.</p>

<pre><code>&gt; dd
   sample operator         y
9      10      SCF 0.9153188
10     10      SCF 0.9884982
19    100      SCF 2.0798781
20    100      SCF 2.0464027
29   1000      SCF 3.0401590
30   1000      SCF 3.0114448
39  10000      SCF 4.1348324
40  10000      SCF 4.0840063
49  1e+05      SCF 5.1235795
50  1e+05      SCF 5.1106381
59  1e+06      SCF 6.0803404
60  1e+06      SCF 6.2353263
&gt; str(dd)
'data.frame':   12 obs. of  3 variables:
 $ sample  : Factor w/ 6 levels ""10"",""100"",""1000"",..: 1 1 2 2 3 3 4 4 5 5 ...
     $ operator: Factor w/ 1 level ""SCF"": 1 1 1 1 1 1 1 1 1 1 ...
 $ y       : num  0.915 0.988 2.08 2.046 3.04 ...
&gt; lmer(y ~ (1|sample)+(1|operator)+(1|sample:operator), data=dd) 
Linear mixed model fit by REML 
Formula: y ~ (1 | sample) + (1 | operator) + (1 | sample:operator) 
   Data: dd 
  AIC   BIC logLik deviance REMLdev
 18.6 21.03 -4.302    9.932   8.605
Random effects:
 Groups          Name        Variance   Std.Dev.
 sample:operator (Intercept) 1.87954740 1.370966
 sample          (Intercept) 1.87954925 1.370967
 operator        (Intercept) 0.00063096 0.025119
 Residual                    0.00283931 0.053285
Number of obs: 12, groups: sample:operator, 6; sample, 6; operator, 1

Fixed effects:
            Estimate Std. Error t value
(Intercept)   3.5709     0.7921   4.508
</code></pre>

<p>For those who are more familiar with SAS the corresponding code is:</p>

<pre><code>PROC MIXED DATA=dd;
CLASS sample operator;
MODEL y=;
RANDOM sample operator sample*operator;
RUN;
</code></pre>

<p>This is nothing but the crossed 2-way ANOVA with random effects.</p>
"
"0.0977452818676612","0.0827152778309109"," 41654","<p>I am a refugee from SPSS in the process of re-learning how to do everything in R. Mostly it's been fun, as R is great for a lot of things but I've run into a snag that I can't seem to find a solution for; any help appreciated.</p>

<p>I want to run repeated-measures ANOVA with contrasts. I know linear mixed models are generally the better way to go but I work in a community where that is only just starting to catch on and my students need to understand what they're reading when they encounter older techniques. SPSS makes contrasts on repeated measures easy, but R does weird things I don't understand. Here's a sample:</p>

<pre><code>df.wide = data.frame(Subj = 1:8, Day1 = c(6,5,5,6,7,4,4,5), 
                     Day2 = c(5,5,6,5,3,2,4,7), Day3 = c(2,4,3,4,3,1,1,2))
attach(df.wide)

## contrasts as t-tests
Linear = -Day1 + 0*Day2 + Day3
Day1vs23 = -2*Day1 + Day2 + Day3
t.test(Linear)
t.test(Day1vs23)

## repeated-measures with long-format data
library(reshape2)
df.long = melt(df.wide, id.vars=""Subj"", variable.name=""Trial"", value.name=""DV"")
contrasts(df.long$Trial) = contr.poly(3)
df.long$Subj = factor(df.long$Subj)
Anv = aov(DV ~ Trial + Error(Subj/Trial), data=df.long)
summary(Anv, split=list(Trial=list(""Linear""=1, ""Quad""=2)))
</code></pre>

<p>The t-tests show t-values of 7.51 for Linear and 1.27 for quadratic, but this does NOT match the <code>aov()</code> output, which provides F-values of 25.28 and 2.51, and of course substantially different p-values.  SPSS contrasts on the repeated measures yield t-values and F-values that match up. </p>

<p>It looks like R isn't partitioning the error term like SPSS does for contrasts, and both contrasts are being tested against a 14 d.f. error term, which is why it doesn't match the t-test.  That seems wrong to me.</p>

<p>So, what am I doing wrong in R and how do I fix it?  </p>
"
"0.119713032670143","0.0945514381551959"," 43361","<p>As a follow-up to <a href=""http://stats.stackexchange.com/questions/41390/test-for-effects-of-two-categorical-variables-on-a-binary-response-variable"">this question</a>, I have the following data:</p>

<pre><code>   Site Treatment Survival
1   BED        DN      1.0
2   BED        DN      1.0
3   BED        DN      1.0
4   BED        MB      1.0
5   BED        MB      1.0
6   BED        MB      0.9
7   BED    Forest      0.4
8   BED    Forest      0.5
9   BED    Forest      0.4
10  BRO        DN      0.9
11  BRO        DN      1.0
12  BRO        DN      1.0
13  BRO        MB      1.0
14  BRO        MB      1.0
15  BRO        MB      1.0
16  BRO    Forest      1.0
17  BRO    Forest      1.0
18  BRO    Forest      1.0
19  LAP        DN      0.8
20  LAP        DN      0.4
21  LAP        DN      0.6
22  LAP        MB      0.5
23  LAP        MB      1.0
24  LAP        MB      0.7
25  LAP    Forest      0.2
26  LAP    Forest      0.2
27  LAP    Forest      0.4
</code></pre>

<p>on which I ran a binomial glm :</p>

<pre><code>&gt; glm.out &lt;- glm(Survival~Site*Treatment, data=surv,
    family=""binomial"", weights=rep(10, nrow(surv)))
&gt; anova(glm.out, test=""Chisq"")

Analysis of Deviance Table
Model: binomial, link: logit
Response: Survival
Terms added sequentially (first to last)

               Df Deviance Resid. Df Resid. Dev P(&gt;|Chi|)    
NULL                              26    138.254              
Site            2   63.098        24     75.155 1.988e-14 ***
Treatment       2   42.991        22     32.164 4.620e-10 ***
Site:Treatment  4   13.874        18     18.290  0.007707 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>All effects are significant, so now I want to do post-hoc comparisons.  I have discovered the <code>glht</code> function in the multcomp package, which seems to do what I want.  I read somewhere that with 2 independent variables, you should set <code>interaction_average=TRUE</code> to get a result equivalent to a TukeyHSD for a linear model.</p>

<pre><code>&gt; Treat.comp &lt;- glht(glm.out, mcp(Treatment=""Tukey"", interaction_average=TRUE))
&gt; summary(Treat.comp)
</code></pre>

<p>which gives me these strange results:</p>

<pre><code>     Simultaneous Tests for General Linear Hypotheses
Multiple Comparisons of Means: Tukey Contrasts
Fit: glm(formula = Survival ~ Site * Treatment, family = ""binomial"", 
    data = surv.san, weights = rep(10, nrow(surv.san)))

Linear Hypotheses:
                 Estimate Std. Error z value Pr(&gt;|z|)
DN - MB == 0       -0.202   3316.127   0.000        1
Forest - MB == 0   -1.886   3316.127  -0.001        1
Forest - DN == 0   -1.684   3316.127  -0.001        1
(Adjusted p values reported -- single-step method)
</code></pre>

<p>Why are my P-values = 1 for all comparisons even though the Treatment effect was highly significant?  </p>

<p>If I try using <code>glht</code> with <code>interaction_average=FALSE</code> I get more reasonable results but with a warning message:</p>

<pre><code>Warning message:
In mcp2matrix(model, linfct = linfct) :
  covariate interactions found -- default contrast might be inappropriate
</code></pre>

<p>Can someone help me to understand what I am doing wrong?  Thank-you!!!</p>
"
"NaN","NaN"," 44581","<p>I have two generalized linear models <code>mod1</code> and <code>mod2</code>. </p>

<pre><code>mod1 &lt;- glm(Score ~ Height + Gene, data=mydata, family=binomial)

mod2 &lt;- glm(Score ~ Height * Gene, data=mydata, family=binomial)
</code></pre>

<p>I want to perform an analysis of deviance to test the significance of the interaction term.</p>

<p>At first I did <code>anova(mod1,mod2)</code>, and I used the function <code>1 - pchisq()</code> to obtain a p-value for the deviance result I got from the anova table.</p>

<p>I also did another test: <code>anova(mod2, test=""Chisq"")</code>. This gave a table for all the terms added sequentially first to last, and I obtained a very different p-value  value for the interaction term <code>Height:Gene</code>...</p>

<p><strong>Which one should I use?</strong></p>
"
"0.138643499732942","0.151619608715781"," 45377","<p>My question is, how to get effect sizes for a linear mixed model?</p>

<p>I am using the following model in SPSS:</p>

<pre><code>MIXED
  transfer  BY distance training rotation sequence  WITH pretest
  /CRITERIA = CIN(95) MXITER(100) MXSTEP(5) SCORING(1) SINGULAR(0.000000000001) 
     HCONVERGE(0, ABSOLUTE) LCONVERGE(0, ABSOLUTE) PCONVERGE(0.000001, ABSOLUTE)
  /FIXED = pretest rotation sequence distance training distance*training  | SSTYPE(3)
  /METHOD = REML
  /REPEATED = distance | SUBJECT(ResponseID) COVTYPE(CS) .
</code></pre>

<p>I've also done it in R, where it looks like this:</p>

<pre><code>library( nlme )
options(contrasts=c(""contr.sum"",""contr.poly""))
lm1 &lt;- lme(transfer ~ training * distance + rotation + sequence + pretest, 
           random=~1|ResponseID, method=""REML"", data=wide.data )
fit &lt;- anova.lme(lm1,type='marginal')
print( fit )
</code></pre>

<p>The dv, transfer, measures pretest to posttest improvement for a given level of distance. Distance is a categorical within-subjects variable with either 2 or 3 levels depending on the dataset. Training is a categorical between-subjects variable with either 2 or 4 levels depending on the dataset. Rotation and sequence are binary categorical between-subjects variables. Pretest is a covariate with different values for each level of distance.</p>

<p>I've already submitted the results of the analysis for publication, and got a reviewer comment asking for odds ratios. Ideally I'd like to get odds ratios for each effect in the model. If I can't get odds ratios, some other measure of effect sizes might be OK. Most important, I need to get it for the training * distance interaction and the covariate (pretest) because these are the only significant effects.</p>

<p>Ideally I'd get it straight from SPSS but as far as I can see, SPSS can't do it. I couldn't figure out how to do it in R either. Second best would be to put the output of one of these into some other software that could calculate it for me. I found some free software that could compute eta-squared for regular mixed ANOVA but not for linear mixed model.</p>

<p>Other questions about effect sizes seem to relate to other models, not LMMs. I did see another similar question about LMMs which is currently unanswered.</p>

<p>Any ideas?</p>
"
"0.0892288262810312","0.0906100470365937"," 45939","<p>When I try to use the data and example for HLR from <a href=""http://dl.dropbox.com/u/10246536/Web/RTutorialSeries/dataset_hlr.csv"" rel=""nofollow"">this example dataset</a> taken from this post in the R Tutorial Series on <a href=""http://rtutorialseries.blogspot.com/2010/01/r-tutorial-series-basic-hierarchical.html"" rel=""nofollow"">hierarchical linear regression</a> the results don't match when I try to use the same method in SPSS.  Is it because SPSS is using a different type of sum of squares (III)?</p>

<p>The F values match for the final model, but not the 2nd one and some of the sum of squares seem off.</p>

<pre><code>#R method
url &lt;- ""http://dl.dropbox.com/u/10246536/Web/RTutorialSeries/dataset_hlr.csv""
datavar &lt;- read.csv(url, header=T)

#create three linear models using lm(FORMULA, DATAVAR)
#one predictor model
onePredictorModel &lt;- lm(ROLL ~ UNEM, datavar)
#two predictor model
twoPredictorModel &lt;- lm(ROLL ~ UNEM + HGRAD, datavar)
#three predictor model
threePredictorModel &lt;- lm(ROLL ~ UNEM + HGRAD + INC, datavar)

#get summary data for each model using summary(OBJECT)
summary(onePredictorModel)
summary(twoPredictorModel)
summary(threePredictorModel)

#compare successive models using anova(MODEL1, MODEL2, MODELi)
test&lt;- anova(onePredictorModel, twoPredictorModel, threePredictorModel)
</code></pre>

<p>Below here is the code for SPSS.</p>

<pre><code>*SPSS method
data list free /YEAR ROLL UNEM HGRAD INC.
begin data
1   5501    8.1 9552    1923
2   5945    7   9680    1961
3   6629    7.3 9731    1979
4   7556    7.5 11666   2030
5   8716    7   14675   2112
6   9369    6.4 15265   2192
7   9920    6.5 15484   2235
8   10167   6.4 15723   2351
9   11084   6.3 16501   2411
10  12504   7.7 16890   2475
11  13746   8.2 17203   2524
12  13656   7.5 17707   2674
13  13850   7.4 18108   2833
14  14145   8.2 18266   2863
15  14888   10.1    19308   2839
16  14991   9.2 18224   2898
17  14836   7.7 18997   3123
18  14478   5.7 19505   3195
19  14539   6.5 19800   3239
20  14395   7.5 19546   3129
21  14599   7.3 19117   3100
22  14969   9.2 18774   3008
23  15107   10.1    17813   2983
24  14831   7.5 17304   3069
25  15081   8.8 16756   3151
26  15127   9.1 16749   3127
27  15856   8.8 16925   3179
28  15938   7.8 17231   3207
29  16081   7   16816   3345
end data.


REGRESSION /MISSING LISTWISE 
/STATISTICS COEFF OUTS R ANOVA CHANGE 
/CRITERIA=PIN (.05) POUT(.10) 
/NOORIGIN /DEPENDENT ROLL 
/METHOD=ENTER UNEM 
/METHOD=ENTER HGRAD 
/METHOD=ENTER INC.
</code></pre>

<p>Or did I mess something up in the procedure for SPSS?</p>
"
"0.11356975465314","0.115327761665773"," 46473","<p>The partial SS for two factor (N and P factors) factorial experiment with interaction  can be calculated as:</p>

<p>\begin{eqnarray*}
\textrm{SS}\left(N_{i}|\mu,P_{j},\left(NP\right)_{ij}\right) &amp; = &amp; \textrm{SS}\left(\mu,N_{i},P_{j},\left(NP\right)_{ij}\right)-\textrm{SS}\left(\mu,P_{j},\left(NP\right)_{ij}\right)\\
\textrm{SS}\left(P_{j}|\mu,N_{i},\left(NP\right)_{ij}\right) &amp; = &amp; \textrm{SS}\left(\mu,N_{i},P_{j},\left(NP\right)_{ij}\right)-\textrm{SS}\left(\mu,N_{i},\left(NP\right)_{ij}\right)\\
\textrm{SS}\left(\left(NP\right)_{ij}|\mu,N_{i},P_{j}\right) &amp; = &amp; \textrm{SS}\left(\mu,N_{i},P_{j},\left(NP\right)_{ij}\right)-\textrm{SS}\left(\mu,N_{i},P_{j}\right)
\end{eqnarray*}</p>

<p>I coded these partial SS for two factor factorial experiment with interaction in R. I'm not getting the right answer and would appreciate if you point out my mistake. Thanks</p>

<pre><code># Partial SS
y &lt;- c(55, 56, 57, 53, 54, 55, 51, 52, 53, 61, 62, 63)
Trt &lt;- gl(  n = 4, k = 3, length = 4 * 3
          , labels = c(""N0P0"", ""N0P1"", ""N1P0"", ""N1P1"")
          , ordered = FALSE)
N &lt;- gl(n = 2, k = 6, length = 2 * 6
        , labels = c(""Low"", ""High"")
        , ordered = FALSE)
P &lt;- gl(n = 2, k = 3, length = 2 * 6
        , labels = c(""Low"", ""High"")
        , ordered = FALSE)
Data &lt;- data.frame(y, Trt, N, P)
Fit1 &lt;- aov(formula = y ~ N * P, data = Data)
ANOVASummary1 &lt;- summary(Fit1)
print(ANOVASummary1)

library(MASS)

X &lt;- 
  cbind(
      model.matrix(object = y ~ 1,        data = Data)
    , model.matrix(object = y ~ -1 + N,   data = Data)
    , model.matrix(object = y ~ -1 + P,   data = Data)
    , model.matrix(object = y ~ -1 + N:P, data = Data)
    )

H  &lt;- X  %*% ginv(t(X) %*% X) %*% t(X)

X1 &lt;- 
  cbind(
      model.matrix(object = y ~ -1 + N ,  data = Data)
    , model.matrix(object = y ~ -1 + P,   data = Data)
    , model.matrix(object = y ~ -1 + N:P, data = Data)
    )
H1  &lt;- X1  %*% ginv(t(X1) %*% X1)  %*% t(X1)
SSM1  &lt;- t(y) %*% (H - H1) %*% y
SSM1



X2 &lt;- 
  cbind(
      model.matrix(object = y ~  1,       data = Data)
    , model.matrix(object = y ~ -1 + P,   data = Data)
    , model.matrix(object = y ~ -1 + N:P, data = Data)
    )
H2  &lt;- X2  %*% ginv(t(X2) %*% X2)  %*% t(X2)
SSM2  &lt;- t(y) %*% (H - H2) %*% y
SSM2

X3 &lt;- 
  cbind(
      model.matrix(object = y ~  1,       data = Data)
    , model.matrix(object = y ~ -1 + N,   data = Data)
    , model.matrix(object = y ~ -1 + N:P, data = Data)
    )

H3  &lt;- X3  %*% ginv(t(X3) %*% X3)  %*% t(X3)
SSM3  &lt;- t(y) %*% (H - H3)  %*% y
SSM3

X4 &lt;- 
  cbind(
      model.matrix(object = y ~  1,     data = Data)
    , model.matrix(object = y ~ -1 + N, data = Data)
    , model.matrix(object = y ~ -1 + P, data = Data)
    )

H4  &lt;- X4  %*% ginv(t(X4) %*% X4)  %*% t(X4)
SSM4  &lt;- t(y) %*% (H - H4)  %*% y
SSM4

print(ANOVASummary1)
</code></pre>

<p><strong>Edit</strong></p>

<p>The <code>print(ANOVASummary1)</code> gives the ANOVA. I want to get the SS in ANOVA using Linear Model approach. Using the Linear Model approach I got the correct Sequential SS but could not figure out how to get the Partial, Adjusted or Type-III SS ( See  <a href=""http://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_introglmest_sect009.htm"" rel=""nofollow"">here</a> and <a href=""http://stats.stackexchange.com/q/23197/3903"">here</a>).</p>
"
"0.292021959937506","0.31162074740129"," 48040","<p>I'm trying to test the significance of the ""component"" effect in a multivariate regression model. I'm not sure what is the right way. Using R, I have tried a way with <code>lm()</code> and another way with <code>gls()</code>, and they don't yield compatible results. </p>

<p><strong>Please note that this is not a question about which methodology is the right one to use to analyze my data. By the way these are simulated data. My question is about the understanding in mathematical terms of the R procedures I use.</strong></p>

<p>The dataset:</p>

<pre><code>&gt; str(dat)
'data.frame':   31 obs. of  5 variables:
 $ group: Factor w/ 5 levels ""1"",""5"",""2"",""3"",..: 1 1 1 1 1 1 1 1 3 3 ...
     $ id   : Factor w/ 8 levels ""1"",""2"",""3"",""4"",..: 1 2 3 4 5 6 7 8 1 3 ...
 $ x    : num  2.5 3 3 4 1.2 3.8 3.9 4 2.5 2.9 ...
     $ y    : num  2.6 3.8 3.9 3.8 1.6 5.2 1.3 3.6 4 3.2 ...
 $ z    : num  3.1 3.6 4.9 3.8 2.1 6 2.1 2.9 4.2 2.9 ...
&gt; head(dat,10)
   group id   x   y   z
1      1  1 2.5 2.6 3.1
2      1  2 3.0 3.8 3.6
3      1  3 3.0 3.9 4.9
4      1  4 4.0 3.8 3.8
5      1  5 1.2 1.6 2.1
6      1  6 3.8 5.2 6.0
7      1  7 3.9 1.3 2.1
8      1  8 4.0 3.6 2.9
9      2  1 2.5 4.0 4.2
10     2  3 2.9 3.2 2.9
</code></pre>

<p>I convert this dataset into ""long format"" for graphics (and later for <code>gls()</code>):</p>

<pre><code>dat$subject &lt;- dat$group : dat$id
    dat.long &lt;- reshape(dat, direction=""long"", varying=list(3:5), 
    	idvar=""subject"", v.names=""value"", timevar=""component"", times=c(""x"",""y"",""z""))
    dat.long$component &lt;- factor(dat.long$component)

xyplot(value ~ component | group, data=dat.long, 
    pch=16, 
    strip = strip.custom(strip.names=TRUE,var.name=""group"" ), layout=c(5,1))
</code></pre>

<p><img src=""http://i.stack.imgur.com/14KtA.png"" alt=""enter image description here""></p>

<p>Each individual of each group has $3$ repeated measures $x$,$y$,$z$ (I should join the points in the graphic to see the repeated measures).</p>

<p>I want to fit a MANOVA model using group as factor and $(x,y,z)$ is the multivariate response:
$$
\begin{pmatrix} x_{ij} \\ y_{ij} \\ z_{ij} \end{pmatrix} 
\sim {\cal N}_3\left( 
\begin{matrix} \mu_{i1} \\ \mu_{i2} \\ \mu_{i3} \end{matrix}, 
\Sigma \right), \quad i=1,\ldots,5
$$
(of course we could use the default R parameterization $\mu_{ik}=\mu_{1k} + \alpha_{ik}$ by considering <code>group1</code>as the ""intercept"" for each response but I prefer ""my"" parameterization).</p>

<p>This model is fitted as follows using <code>lm()</code>:</p>

<pre><code>###  multivariate least-squares fitting  ###
mfit &lt;- lm( cbind(x,y,z)~group, data=dat )
</code></pre>

<p>I think the model can also  be fitted with <code>gls()</code> as follows (but with a different fitting procedure) :</p>

<pre><code>### generalized least-squares fitting  ###
library(nlme)
gfit &lt;- gls(value ~ group*component, data=dat.long, correlation=corSymm(form= ~ 1 | subject))
</code></pre>

<p>Recall that <code>subject = group:id</code> is the identifier of the individuals. The <code>correlation=corSymm(form= ~ 1 | subject)</code> argument means that the responses $x$, $y$, $z$ for each individual are correlated. Here <code>corSymm</code> means a general, ""unrestricted"",  covariance structure (termed as ""unstructured"" in SAS language).</p>

<p>To check that <code>mfit</code> and <code>gfit</code> are equivalent, we can check for instance that we can deduce the estimated parameters of <code>mfit</code> from the estimated parameters of <code>gfit</code>and vice-versa (so the ""mean"" parameters have exactly the same fitted values):</p>

<pre><code>&gt; coef(mfit)
                  x          y          z
(Intercept)  3.1750  3.2250000  3.5625000
group5      -0.9500 -0.4750000  0.1125000
group2      -1.0750 -0.5678571 -0.2339286
group3      -0.7875 -0.1000000  0.1875000
group4      -0.3750  0.4000000 -0.0125000
&gt; coef(gfit)
      (Intercept)            group5            group2            group3 
        3.1750000        -0.9500000        -1.0750000        -0.7875000 
           group4        componenty        componentz group5:componenty 
       -0.3750000         0.0500000         0.3875000         0.4750000 
group2:componenty group3:componenty group4:componenty group5:componentz 
        0.5071429         0.6875000         0.7750000         1.0625000 
group2:componentz group3:componentz group4:componentz 
        0.8410714         0.9750000         0.3625000 
</code></pre>

<p>Now I want to test the ""component effect"". Rigorously speaking, writing the model as 
$$
\begin{pmatrix} x_{ij} \\ y_{ij} \\ z_{ij} \end{pmatrix} 
\sim {\cal N}_3\left( 
\begin{matrix} \mu_{i1} \\ \mu_{i2} \\ \mu_{i3} \end{matrix}, 
\Sigma \right),
$$
I want to test the hypothesis $\boxed{H_0\colon \{\mu_{1i}=\mu_{2i}=\mu_{3i} \quad \forall i=1,2,3,4,5 \}}$.</p>

<p>Below are my attempts, one attempt with <code>gfit</code> and two attempts with <code>mfit()</code>:</p>

<pre><code>###########################################
## testing significance of the component ##
###########################################

&gt; ### with gfit  ###
&gt; anova(gfit)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 504.5226  &lt;.0001
group               4   0.7797  0.5418
component           2  11.7073  &lt;.0001
group:component     8   0.7978  0.6063
&gt; 
&gt; ### with mfit ###
&gt; library(car)
&gt; 
&gt; # first attempt : 
&gt; idata &lt;- data.frame(component=c(""x"",""y"",""z""))
&gt; ( av.ok &lt;- Anova(mfit, idata=idata, idesign=~component, type=""III"") )

Type III Repeated Measures MANOVA Tests: Pillai test statistic
                Df test stat approx F num Df den Df    Pr(&gt;F)    
(Intercept)      1   0.84396  140.625      1     26 5.449e-12 ***
group            4   0.10369    0.752      4     26    0.5658    
component        1   0.04913    0.646      2     25    0.5328    
group:component  4   0.22360    0.818      8     52    0.5901    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
&gt; 
&gt; # second attempt :
&gt; linearHypothesis(mfit, ""(Intercept) = 0"", idata=idata, idesign=~component, iterms=""component"")

 Response transformation matrix:
  component1 component2
x          1          0
y          0          1
z         -1         -1

Sum of squares and products for the hypothesis:
           component1 component2
component1    1.20125    1.04625
component2    1.04625    0.91125

Sum of squares and products for error:
           component1 component2
component1   31.46179   14.67696
component2   14.67696   21.42304

Multivariate Tests: 
                 Df test stat  approx F num Df den Df  Pr(&gt;F)
Pillai            1 0.0491253 0.6457903      2     25 0.53277
Wilks             1 0.9508747 0.6457903      2     25 0.53277
Hotelling-Lawley  1 0.0516632 0.6457903      2     25 0.53277
Roy               1 0.0516632 0.6457903      2     25 0.53277
</code></pre>

<p>With <code>anova(gfit)</code> the component is significant, but not with my two attempts using <code>mfit</code> and the <code>car</code> package. </p>

<p>I know that <code>gls()</code> use a different fitting method than <code>lm()</code> but this is surely not the cause of the difference. </p>

<p>So my questions are :</p>

<ul>
<li>did I do something wrong ?</li>
<li>which method tests my $H_0$ hypothesis ?</li>
<li>what is the $H_0$ hypothesis of the other methods ?</li>
</ul>

<p>And I have an auxiliary question: how to get $\hat\Sigma$ with <code>mfit</code> and <code>gfit</code> ?</p>

<h2>Update 1</h2>

<p>Below is a reproducible example which simulates the dataset. 
Now I think I understand : both ANOVA methods are correct (the first one with <code>anova(gfit)</code> and the second one with <code>Anova(mfit, ...)</code>, <strong>and they yield very close results when using the type II sum of squares in <code>Anova(mfit, ...)</code></strong>.  For the above example: </p>

<pre><code>&gt; Anova(mfit, idata=idata, idesign=~component, type=""II"")

Type II Repeated Measures MANOVA Tests: Pillai test statistic
                Df test stat approx F num Df den Df    Pr(&gt;F)    
(Intercept)      1   0.94691   463.72      1     26 &lt; 2.2e-16 ***
group            4   0.10369     0.75      4     26 0.5657691    
component        1   0.50344    12.67      2     25 0.0001584 ***
group:component  4   0.22360     0.82      8     52 0.5900848    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>is very close to </p>

<pre><code>&gt; anova(gfit)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 504.5226  &lt;.0001
group               4   0.7797  0.5418
component           2  11.7073  &lt;.0001
group:component     8   0.7978  0.6063
</code></pre>

<p>Below is the reproducible code with the data sampler (I simulate uncorrelated repeated measures but it suffices to include a covariance matrix in the <code>rmvnorm()</code> function to simulate correlated repeated measures) :</p>

<pre><code>library(mvtnorm)
library(nlme)
library(car)

# set data parameters 
I &lt;- 5 # number of groups
J &lt;- 16 # number of individuals per group
dat &lt;- data.frame(
    group = gl(I,J),
    id = gl(J,1,I*J),
    x=NA, 
    y=NA, 
    z=NA
)
Mu &lt;- c(1:I) # group means of components (assuming E(x)=E(y)=E(z) in each group)

# simulates data: 
for(i in 1:I){
    which.group.i &lt;- which(dat$group==i)
    dat[which.group.i,c(""x"",""y"",""z"")] &lt;- round(rmvnorm(n=J, mean=rep(Mu[i],3)), 1)
}

dat$subject &lt;- droplevels( dat$group : dat$id )
    dat.long &lt;- reshape(dat, direction=""long"", varying=list(3:5), 
    	idvar=""subject"", v.names=""value"", timevar=""component"", times=c(""x"",""y"",""z""))
    dat.long$component &lt;- factor(dat.long$component)

# multivariate least-squares fitting 
mfit &lt;- lm( cbind(x,y,z)~group, data=dat )

# gls fitting
dat.long$order.xyz &lt;- as.numeric(dat.long$component)
gfit &lt;- gls(value ~ group*component , data=dat.long, correlation=corSymm(form=  ~ order.xyz | subject)) 

# compares ANOVA : 
anova(gfit)
idata &lt;- data.frame(component=c(""x"",""y"",""z""))
Anova(mfit, idata=idata, idesign=~component, type=""II"")
Anova(mfit, idata=idata, idesign=~component, type=""III"")
</code></pre>

<p>So now I wonder which type of sum of squares is the more appropriate one for my real study... but this is another question</p>

<h2>Update 2</h2>

<p>About my question <em>""how to get $\hat\Sigma$""</em>, here is the answer for <code>gls()</code>:</p>

<pre><code>&gt; getVarCov(gfit)
Marginal variance covariance matrix
        [,1]    [,2]    [,3]
[1,] 0.92909 0.47739 0.24628
[2,] 0.47739 0.92909 0.53369
[3,] 0.24628 0.53369 0.92909
  Standard Deviations: 0.96389 0.96389 0.96389 
</code></pre>

<p>That shows that <strong><code>mfit</code>and <code>gfit</code> were not equivalent models</strong>: <code>gfit</code>assumes the same variance for the three components.</p>

<p>In order to fit a fully unrestricted covariance matrix for the repeated measures, we have to type:</p>

<pre><code>gfit2 &lt;- gls(value ~ group*component , data=dat.long, 
    correlation=corSymm(form=  ~ 1 | subject), 
    weights=varIdent(form = ~1 | component))

&gt; summary(gfit2)
Generalized least squares fit by REML
  Model: value ~ group * component 
  Data: dat.long 
       AIC      BIC    logLik
  264.0077 313.4986 -111.0038

Correlation Structure: General
 Formula: ~1 | subject 
 Parameter estimate(s):
 Correlation: 
  1     2    
2 0.529      
3 0.300 0.616
Variance function:
 Structure: Different standard deviations per stratum
 Formula: ~1 | component 
 Parameter estimates:
       x        y        z 
1.000000 1.253534 1.169335 

....

Residual standard error: 0.8523997 
</code></pre>

<p>But yet I don't understand the extracted covariance matrix given by <code>getVarCov()</code> (but this is not important since we get this matrix with <code>summary(gfit2)</code>): </p>

<pre><code>   &gt; getVarCov(gfit2)
    Error in t(S * sqrt(vars)) : 
      dims [product 9] do not match the length of object [0]
    &gt; getVarCov(gfit2, individual=""1:1"")
    Marginal variance covariance matrix
            [,1]    [,2]    [,3]
    [1,] 0.72659 0.48164 0.25500
    [2,] 0.48164 1.14170 0.65562
    [3,] 0.25500 0.65562 0.99349
      Standard Deviations: 0.8524 1.0685 0.99674 
    &gt; getVarCov(gfit2, individual=""1:2"")
    Marginal variance covariance matrix
            [,1]    [,2]    [,3]
    [1,] 1.14170 0.56319 0.27337
    [2,] 0.56319 0.99349 0.52302
    [3,] 0.27337 0.52302 0.72659
      Standard Deviations: 1.0685 0.99674 0.8524 
</code></pre>

<p>Unfortunately, the <code>anova(gfit2)</code> table is not as close to <code>Anova(mfit, ..., type=""II"")</code> as <code>anova(gfit)</code>:</p>

<pre><code>&gt; anova(gfit2)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 498.1744  &lt;.0001
group               4   1.0514  0.3864
component           2  13.1801  &lt;.0001
group:component     8   0.8310  0.5780

&gt; Anova(mfit, idata=idata, idesign=~component, type=""II"")

Type II Repeated Measures MANOVA Tests: Pillai test statistic
                Df test stat approx F num Df den Df    Pr(&gt;F)    
(Intercept)      1   0.94691   463.72      1     26 &lt; 2.2e-16 ***
group            4   0.10369     0.75      4     26 0.5657691    
component        1   0.50344    12.67      2     25 0.0001584 ***
group:component  4   0.22360     0.82      8     52 0.5900848    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

&gt; anova(gfit)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 504.5226  &lt;.0001
group               4   0.7797  0.5418
component           2  11.7073  &lt;.0001
group:component     8   0.7978  0.6063
</code></pre>
"
"0.195647144107831","0.17483459044311"," 50086","<p>Assume for example a trivariate Gaussian model:
$$
{\boldsymbol Y}_1, \ldots, {\boldsymbol Y}_n \sim_{\text{iid}} {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right) \quad (*)
$$
with ${\boldsymbol \mu} = (\mu_1,\mu_2,\mu_3)'$. </p>

<p>The Bayesian conjugate theory of this model is well known. 
This model is the most simple case of a  multivariate linear regression model. 
And more generally, there is a well known Bayesian conjugate theory of multivariate linear regression, which is  the extension to the case when  the multivariate mean  ${\boldsymbol \mu}= {\boldsymbol \mu}(x_i)$ is allowed to depend on the covariates $x_i$ of individual $i \in \{1, \ldots, n \}$, with linear constraints about the multivariate means ${\boldsymbol \mu}(x_i)$. See for instance <a href=""http://books.google.be/books?id=GL8VS9i_B2AC&amp;dq=bayesian%20econometrics%20bayesm&amp;hl=fr&amp;source=gbs_navlinks_s"" rel=""nofollow"">Rossi &amp; al's book</a> accompanied by the crantastic <a href=""http://cran.r-project.org/web/packages/bayesm/index.html"" rel=""nofollow""><code>bayesm</code></a> package for <code>R</code>. 
We know in addition that the Jeffreys prior is a limit form of the conjugate prior  distributions.</p>

<p>Let us come back to the simple multivariate Gaussian model $(*)$. Instead of generalizing this model to the case of linearly dependent multivariate means ${\boldsymbol \mu}(x_i)$ depending on individuals $i=1,\ldots,n$, we can consider a more restrictive model by assuming linear constraints about the components $\mu_1$, $\mu_2$, $\mu_3$ of the multivariate mean ${\boldsymbol \mu}$. </p>

<h3>Example</h3>

<p>Consider some concentrations $x_{i,t}$ of $4$ blood samples $i=1,2,3,4$ measured at $3$ timepoints $t=t_1,t_2,t_3$. 
Assume that the samples are independent and that the series of the three measurements 
$(x_{i,t_1},x_{i,t_2},x_{i,t_3}) \sim {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right)$ for each sample $i$ with a mean 
${\boldsymbol \mu} = (\mu_1,\mu_2,\mu_3)'$ whose three components are 
linearly related to the timepoints: $\mu_j = \alpha + \beta t_j$.</p>

<p>This example falls into the context of <em>Multivariate linear regression with a within-design structure</em>. 
See for instance <a href=""http://wweb.uta.edu/management/Dr.Casper/Fall10/BSAD6314/Coursematerial/O%27Brien%20&amp;%20Kaiser%201985%20-%20MANOVA%20-%20RM%20-%20Psy%20Bull%2085.pdf"" rel=""nofollow"">O'Brien &amp; Kaiser 1985</a> and <a href=""http://www.stat.ualberta.ca/~wiens/stat575/misc%20resources/Appendix-Multivariate-Linear-Models.pdf"" rel=""nofollow"">Fox &amp; Weisberg's appendix</a></p>

<p>So my example is a simple example of this situation because there are only some predictors (the timepoints) for the components of the mean, but there are no predictors for individuals. This example could be written as follows:
$$
(**) \left\{\begin{matrix} 
{\boldsymbol Y}_1, \ldots, {\boldsymbol Y}_n \sim_{\text{iid}} {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right) \\ 
{\boldsymbol \mu} = X {\boldsymbol \beta}
\end{matrix}\right.
$$
with ${\boldsymbol \beta}=(\alpha, \beta)'$ and $X=\begin{pmatrix} 1 &amp; t_1 \\ 1 &amp; t_2 \\ 1 &amp; t_3 \end{pmatrix}$ is the matrix of covariates for the components of the multivariate mean ${\boldsymbol \mu}$. The second line of $(**)$ could be termed as the <em>within design</em>, or the <em>repeated measures design</em>, or the <em>structural design</em> (I would appreciate if a specialist had some comments about this vocabulary).</p>

<p>I think such a model can be fitted as a generalized least-squares model, as follows in  <code>R</code> :</p>

<pre><code>gls(response ~ ""between covariates"" , data=dat, 
  correlation=corSymm(form=  ~ ""within covariates"" | individual ))
</code></pre>

<p>(after stacking the data in long format).</p>

<p><strong>My first question</strong> is Bayesian: what about the Bayesian analysis of model $(**)$ and more generally the Bayesian analysis of the multivariate linear regression model with a structural design ? 
Is there a conjugate family ? What about the Jeffreys prior ? Is there an appropriate R package to perform this Bayesian analysis ?</p>

<p><strong>My second question</strong> is not Bayesian: I have recently discovered some possibilities of John Fox's great <code>car</code> package to analyse 
such models with ordinary least squares theory (the <code>Anova()</code> function with the <code>idesign</code> argument --- see <a href=""http://www.stat.ualberta.ca/~wiens/stat575/misc%20resources/Appendix-Multivariate-Linear-Models.pdf"" rel=""nofollow"">Fox &amp; Weisberg's appendix</a>). Perhaps I'm wrong, but I am under the impression that this package only allows to get the MANOVA table (sum of squares analysis) with an orthogonal matrix $X$, and I'd like to get (exact) confidence intervals about the within-design parameters for an arbitrary matrix $X$, as well as prediction intervals. Is there a way to do so with <code>R</code> using ordinary least squares ?  </p>
"
"0.028216632399155","0.0573068255061253"," 50180","<p>In my research I have performed a series of measurements on 5 different brands of blocks. Each block has been inspected for deformation under incremental forces (20, 30, 40, 50, 60, 70, 80, 90, 100, 110 and 120 N). The deformation for each force was measured 3 times and the mean values were assigned to each brand for a specific amount of force. I was successful in creating linear regression graphs for these 5 different brands.</p>

<p>Now my wish is to see whether a brand makes a significant difference in deformation values and to perform a post-hoc analysis to compare brands among themselves. In other words to compare the linear regression lines. Sorry if what I am saying makes no sense.</p>

<p>So far, I have tried the following commands:</p>

<pre><code>anova(lm(Deformation~Force*Brand, data=Data))
lm(Deformation~Force, data=Data))

# and
aov.data = aov(Deformation~Force*Brand, Data)
</code></pre>

<p>I have gotten suspiciously low p-values (<em>*</em>) which clearly indicates that I might be doing something wrong. I would be grateful if you could help me with this issue.</p>

<pre><code>Force   Brand   Deformation  
20  Brand1  0.65  
30  Brand1  1.23  
40  Brand1  1.25  
50  Brand1  2.39  
60  Brand1  2.45  
70  Brand1  2.93  
80  Brand1  3.13  
90  Brand1  3.57  
100 Brand1  4.68  
110 Brand1  4.84  
120 Brand1  5.33  
20  Brand2  1.24  
30  Brand2  1.11  
40  Brand2  1.6  
50  Brand2  2.13  
60  Brand2  2.69  
70  Brand2  3.60  
80  Brand2  3.90  
90  Brand2  3.99  
100 Brand2  4.51  
110 Brand2  4.74  
120 Brand2  5.98  
20  Brand3  1.21  
30  Brand3  1.37  
40  Brand3  2.56  
50  Brand3  2.49  
60  Brand3  3.17  
70  Brand3  3.33  
80  Brand3  3.38  
90  Brand3  4.2  
100 Brand3  4.22  
110 Brand3  5.22  
120 Brand3  6.28  
20  Brand4  0.92  
30  Brand4  0.89  
40  Brand4  1.2  
50  Brand4  1.67  
60  Brand4  1.98  
70  Brand4  2.25  
80  Brand4  3.8  
90  Brand4  4.17  
100 Brand4  4.94  
110 Brand4  5.4  
120 Brand4  5.76  
20  Brand5  0.69  
30  Brand5  1.26  
40  Brand5  1.61  
50  Brand5  2.17  
60  Brand5  2.07  
70  Brand5  3.35  
80  Brand5  3.27  
90  Brand5  4.13  
100 Brand5  4.25  
110 Brand5  4.59  
120 Brand5  5  
</code></pre>
"
"0.0892288262810312","0.0906100470365937"," 52516","<p>I have a data set with the following:</p>

<p>N = 60;
x = developmental stage (range 25 to 44);
y = proportion of 10 minute trial performing a behavior (range 0 to 0.81; 30 zeros)</p>

<p>A scatterplot produces a quadratic looking curve where those in mid-development clearly performed the behavior for more time. Most of the zeros are in the youngest and oldest individuals. If I break up the data into 5 groups according to developmental stage, an ANOVA/Tukey strongly supports this pattern. However, I would like to analyze this data continuously without breaking it into groups.</p>

<p>I have considered arcsine square root transformed proportion data in a linear regression, but I am unsure if that can incorporate a quadratic term, and this analysis results in a very small R squared value (less than 0.1). I have also considered arcsine square root transformed proportion data in a GLM containing a quadratic term or a beta regression (zeros??), but am not sure where to go from here.</p>

<p>I am planning to say in the paper that the individuals in mid-development perform the behavior more than those in early or late development, but am struggling to interpret the data in a way that supports that statement.</p>

<p>I appreciate any suggestions, thank you!</p>
"
"0.105576971046183","0.107211253483779"," 53427","<p>Lets take as an example a repeated measures design with 10 subjects that are all reading the same letter strings and pressing a button as soon as they determine whether the string is valid English word, producing reaction times (RT). I wish to determine whether word length has a significant effect on the produced RT (it should), using a linear mixed effect model. Using R and the lme4 package I construct the following model:</p>

<pre><code>m = lmer(RT ~ 1 + word.length + (1 + word.length|subject), data=rt.data)
</code></pre>

<p>As you can see, I allow both the intercept and the slope to vary randomly across subjects, as I suspect that the effect of word length might be larger for slow readers than fast readers. </p>

<p>Understanding that p-values are not trivial in these types of models, my approach is to construct a NULL model, containing only the random effects. But I'm not sure whether this should be:</p>

<pre><code>m.null = lmer(RT ~ (1 + word.length|subject), data=rt.data)
</code></pre>

<p>or:</p>

<pre><code>m.null = lmer(RT ~ (1 | subject), data=rt.data)
</code></pre>

<p>In the end, I wish to perform an anova between the model with word length and the NULL model like so:</p>

<pre><code>anova(m, m.null)
</code></pre>

<p>which should give me a p-value whether the addition of word length actually makes the model fit better and thus whether word length actually influences the RT.</p>
"
"0.154851601086124","0.137592552906328"," 55856","<p>I have a data which is longitudinal. I have same subject reading at 3 different treatment one after the other and I ran the samples in 2 batches:</p>

<ul>
<li>Batch one has subject id 1:7 and the three time point for each</li>
<li>Batch two has id 8:13 and three time point reading for each</li>
</ul>

<p>I am trying to fit a mixed effect model with two random effects. <code>X</code> is my level of substance in blood. <code>Visit1</code> is a factor with 3 levels.</p>

<pre class=""lang-r prettyprint-override""><code>lme1 &lt;- lme(fixed = x~visit1, random = ~ visit1|sampleid/batch,data=newx)
</code></pre>

<p>Since batch is a higher level. Would this be correct? The reason of my confusion is the output:</p>

<pre><code>Linear mixed-effects model fit by REML
 Data: newx
       AIC      BIC    logLik
    98.60629 123.9426 -33.30314

Random effects:

 Formula: ~visit1 | sampleid

 Structure: General positive-definite, Log-Cholesky parametrization

            StdDev    Corr         
(Intercept) 0.4653918 (Intr) vst1V4
 visit1V4    0.5036920 -0.475       
 visit1V7    0.2531549 -0.466  0.003

Formula: ~visit1 | batch %in% sampleid
 Structure: General positive-definite, Log-Cholesky parametrization
            StdDev    Corr         
(Intercept) 0.4653914 (Intr) vst1V4
visit1V4    0.5036924 -0.475       
visit1V7    0.2531550 -0.466  0.003
Residual    0.1674964              

Fixed effects: x ~ visit1
                Value Std.Error DF  t-value p-value
(Intercept) 10.952459 0.1883601 24 58.14640  0.0000
visit1V4     0.031497 0.2082014 24  0.15128  0.8810
visit1V7    -0.346842 0.1190620 24 -2.91312  0.0076

Correlation:
         (Intr) vst1V4
visit1V4 -0.492       
visit1V7 -0.473  0.090

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-1.12819289 -0.19660884 -0.06543492  0.17633675  1.01718946 

Number of Observations: 39
Number of Groups: 
           sampleid batch %in% sampleid 
                 13                  13 
</code></pre>

<p>Only intercept model gives a better fit<br/></p>

<pre><code>    lme2&lt;-update(lme1,random=~1|sampleid/batch)
     &gt; summary(lme2)
     Linear mixed-effects model fit by REML
     Data: newx 
     AIC      BIC    logLik
   53.50052 63.00163 -20.75026

   Random effects:
   Formula: ~1 | sampleid
     (Intercept)
    StdDev: 1.636268e-05

    Formula: ~1 | batch %in% sampleid
     (Intercept)  Residual
    StdDev: 1.632057e-05 0.3869672

   Fixed effects: x ~ visit1 
         Value Std.Error DF  t-value p-value
  (Intercept)  9.923162 0.1073254 24 92.45866  0.0000
  visit1V4    -0.333890 0.1517810 24 -2.19982  0.0377
  visit1V7    -0.246486 0.1517810 24 -1.62396  0.1174
 Correlation: 
         (Intr) vst1V4
visit1V4 -0.707       
visit1V7 -0.707  0.500

 Standardized Within-Group Residuals:
       Min         Q1        Med         Q3        Max 
-2.5208757 -0.5757388  0.2255019  0.7457944  1.2827374 

Number of Observations: 39
Number of Groups: 
           sampleid batch %in% sampleid 
             13                  13 




anova(lme1,lme2)
     Model df      AIC      BIC    logLik   Test   L.Ratio p-value
lme1     1 16 73.15228 98.48859 -20.57614                         
lme2     2  6 53.50052 63.00163 -20.75026 1 vs 2 0.3482341       1
</code></pre>

<p>My question is about the batch effect.There seems to be no batch effect.Am i correct?</p>
"
"0.0977452818676612","0.099258333397093"," 56380","<p>The <code>lme4</code> package in R includes the <code>cake</code> dataset. </p>

<pre><code>library(lme4)
head(cake[,2:4], 20)
   recipe temperature angle
1       A         175    42
2       A         185    46
3       A         195    47
4       A         205    39
5       A         215    53
6       A         225    42
7       B         175    39
8       B         185    46
9       B         195    51
10      B         205    49
11      B         215    55
12      B         225    42
13      C         175    46
14      C         185    44
15      C         195    45
16      C         205    46
17      C         215    48
18      C         225    63
19      A         175    47
20      A         185    29
</code></pre>

<p>I've analysed the <code>cake</code> dataset using two different models below. The first model is a 2 factor ANOVA:</p>

<pre><code>summary(aov(angle ~ temperature + recipe, cake))
             Df Sum Sq Mean Sq F value   Pr(&gt;F)    
temperature   5   2100   420.1   6.918 4.37e-06 ***
recipe        2    135    67.5   1.112     0.33    
Residuals   262  15908    60.7                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>...and the second is a mixed effects model, with <code>temperature</code> as a random effect:</p>

<pre><code>lmer(angle ~ recipe + (1| temperature), data=cake, REML=F)
Linear mixed model fit by maximum likelihood 
Formula: angle ~ recipe + (1 | temperature) 
   Data: cake 
  AIC  BIC logLik deviance REMLdev
 1893 1911 -941.7     1883    1877
Random effects:
 Groups      Name        Variance Std.Dev.
 temperature (Intercept)  6.4399  2.5377  
 Residual                60.2560  7.7625  
Number of obs: 270, groups: temperature, 6

Fixed effects:
            Estimate Std. Error t value
(Intercept)   33.122      1.320  25.093
recipeB       -1.478      1.157  -1.277
recipeC       -1.522      1.157  -1.315

Correlation of Fixed Effects:
        (Intr) recipB
recipeB -0.438       
recipeC -0.438  0.500
</code></pre>

<p>Is someone able to provide a summary of what the mixed effect model has done differently to the ANOVA?</p>
"
"0.143877159211166","0.134865517623595"," 57517","<p>I have done rANOVA in SPSS and then in R and got two different p-values from the same model. While SPSS gives 0.032, R gives 0.0162, which when rounded is a half. Other data are exactly the same.</p>

<p><strong>1. Why?</strong></p>

<p>Intuitively, one of them does a one-tailed, the other a two-tailed test. Other values are: Sum Sq <code>19.071</code> (SPSS) v. <code>24.14</code> (R) or F value <code>4.863</code> (SPSS) v. <code>6.156</code> (R).</p>

<p>Other parts of the table are the same though, both <code>IV1 * IV2</code> and <code>residuals</code>. </p>

<p><strong>2. Why? Which is <em>â€œmore correctâ€</em>?</strong></p>

<p><strong>3. How to get the SPSS-reported values from R?</strong></p>

<p>SPSS approach:</p>

<pre><code>Analyze â€“&gt; General Linear Model â€“&gt; Repeated Measuresâ€¦
</code></pre>

<p>R approach:</p>

<pre><code>summary(aov(DV ~ IV1 * IV2 + Error(subject/(IV1 * IV2)), data))
</code></pre>

<p><strong>Update to answer @PeterFlom and give additional details:</strong></p>

<p>I am evaluating <em>the effect of an intervention (treatment: a lecture) on the ability to spot behavioral clues of emotions using visual stimuli</em> using <strong>pretest-posttest plus control group</strong> experimental design.</p>

<p>The experimental group is given the first set of stimuli, then there is intervention and the second set of stimuli follows. Naturally, the same without the intervention occurs in the case of the control group. <strong>Two groups, two measurements each.</strong></p>

<p>The <strong>hypothesis</strong> is that the intervention significantly raises the ability to spot behavioral clues of emotions.</p>

<p>SPSS calculates the same interaction too.</p>

<p>The data:</p>

<pre><code>    subject   group   phase     value
1        A1     exp     pre        13
2        A2     exp     pre         7
.
35       B1    cont     pre         9
36       B2    cont     pre        14
.
57       A1     exp    post        11
58       A2     exp    post        12
.
91       B1    cont    post        13
92       B2    cont    post        12
</code></pre>

<p>Honestly, I am using it as is because a tutorial said so. I have got a very shallow understanding of the correct process in R.</p>

<p><strong>Update in response to @ttnphns:</strong></p>

<p>SPSS data:</p>

<pre><code>group    pre    post
  exp     13      11
  exp      7      12
 cont      9      13
 cont     14      12
</code></pre>

<p>SPSS commands:</p>

<pre><code>GET DATA 
  /TYPE=TXT 
  /FILE=""F:\file.csv"" 
  /DELCASE=LINE 
  /DELIMITERS="";"" 
  /ARRANGEMENT=DELIMITED 
  /FIRSTCASE=2 
  /IMPORTCASE=ALL 
  /VARIABLES= 
  group A4 
  pre F2.0 
  post F2.0. 
CACHE. 
EXECUTE. 
DATASET NAME DataSet1 WINDOW=FRONT. 
GLM pre post BY group 
  /WSFACTOR=phase 2 Polynomial 
  /METHOD=SSTYPE(3) 
  /PLOT=PROFILE(phase*group) 
  /PRINT=DESCRIPTIVE ETASQ 
  /CRITERIA=ALPHA(.05) 
  /WSDESIGN=phase 
  /DESIGN=group.
</code></pre>
"
"0.178457652562062","0.181220094073187"," 58321","<p>I need some help with the statistical analysis of a study of a particular surgery to remove a particular cancer. I am using the statistical program R to conduct my analysis. My data are saved in the object <code>study_data</code>.</p>

<h3>Data</h3>

<pre><code># Create reproducible example data
set.seed(50)

study_data &lt;- data.frame(
              Patient_ID = 1:500,
              Institution = sample(c(""New York"",""San Francisco"",""Houston"",""Chicago""),500,T),
              Gender = sample(c(""Male"",""Female""),500,T),
              Race = sample(c(""White"",""Black"",""Hispanic"",""Asian""),500,T),
              Tumor_grade = sample(c(""One"",""Two"",""Three"",""Four""),500,T),
              Pathologic_stage = sample(c(""P0"",""Pa"",""Pis"",""P1"",""P2a"",""P2b"",""P3a"",""P3b"",""P4a"",""P4b""),500,T),
              Treatment_arm = sample(c(""One"",""Two"",""Three"",""Four""),500,T),
              Surgery_age = round(runif(500,20,100)),
              Nodes_removed = round(runif(500,1,130)))
</code></pre>

<p>Here is what the data look like:</p>

<pre><code># Peak at the first six lines of the data
head(study_data)

  Patient_ID   Institution Gender     Race Tumor_grade Pathologic_stage Treatment_arm Surgery_age Nodes_removed
1          1       Houston   Male Hispanic         One              P2b           Two          77           130
2          2 San Francisco Female Hispanic       Three               Pa           Two          38           112
3          3      New York Female    Black        Four               P0          Four          90            90
4          4       Chicago   Male Hispanic         Two              Pis          Four          46             4
5          5       Houston Female    Black        Four              P2a          Four          96           114
6          6      New York   Male    Black       Three              P3b          Four          92             7
</code></pre>

<h3>My interest</h3>

<p>I am interested in learning more about what variables are associated with the number of lymph nodes removed during the surgery. My first thought was to simply stratify the data by a particular variable and then calculate the median number of nodes removed.</p>

<p>For example, to see if the institution at which the surgery was performed mattered, I could write:</p>

<pre><code>cbind(do.call(rbind, by(study_data$Nodes_removed, study_data$Institution, summary)))

              Min. 1st Qu. Median  Mean 3rd Qu. Max.
Chicago          1   25.50   65.5 64.48   98.75  129
Houston          1   40.00   71.0 69.26  100.00  130
New York         4   36.00   67.0 67.96  100.00  129
San Francisco    3   36.75   61.0 65.76   99.00  127
</code></pre>

<p>This lets me compare the median nodes removed in each institutional city.</p>

<h3>My question</h3>

<p>I would like to fully examine the association between all of my variables and the outcome <code>Nodes_removed</code>.</p>

<ol>
<li>Should I just do these simple summary statistics for all of my variables?</li>
<li>Do I need to perform some sort of hypothesis test for all of the associations to say whether or not the summary statistics differ? For example, should I calculate a median and a confidence interval for each comparison?</li>
<li>Or should I be using t-tests to compare one group to another?</li>
<li>In the case of a multi-level variable, should I use ANOVA?</li>
<li>Is there any role for linear regression analysis here? </li>
<li>If I wanted to build a single model that includes every possible predictor variable, what method should I use?</li>
</ol>

<p>For example, say that I am most interested in the association between the age at which the surgery was performed, <code>Surgery_age</code>, and <code>Nodes_removed</code>. However, I would like to adjust this association for potential confounders like gender, race, tumor grade, treatment arm, etc. What is the best way for me to do this?</p>

<p>Thanks for any advice you can give!</p>
"
"0.159617376893524","0.162088179694622"," 58745","<p>EDIT 2: I originally thought I needed to run a two-factor ANOVA with repeated measures on one factor, but I now think a linear mixed-effect model will work better for my data. I think I nearly know what needs to happen, but am still confused by few points.</p>

<p>The experiments I need to analyze look like this: </p>

<ul>
<li>Subjects were assigned to one of several treatment groups</li>
<li>Measurements of each subject were taken on multiple days</li>
<li>So:
<ul>
<li>Subject is nested within treatment</li>
<li>Treatment is crossed with day</li>
</ul></li>
</ul>

<p>(each subject is assigned to only one treatment, and measurements are taken on each subject on each day)</p>

<p>My dataset contains the following information:</p>

<ul>
<li>Subject = blocking factor (random factor)</li>
<li>Day = within subject or repeated measures factor (fixed factor)</li>
<li>Treatment = between subject factor (fixed factor)</li>
<li>Obs = measured (dependent) variable</li>
</ul>

<p><strong>UPDATE</strong>
OK, so I went and talked to a statistician, but he's an SAS user.  He thinks that the model should be:</p>

<p><strong>Treatment + Day + Subject(Treatment) + Day*Subject(Treatment)</strong></p>

<p>Obviously his notation is different from the R syntax, but this model is supposed to account for:</p>

<ul>
<li>Treatment   (fixed)</li>
<li>Day   (fixed)</li>
<li>the Treatment*Day interaction</li>
<li>Subject nested within Treatment  (random)</li>
<li>Day crossed with ""Subject within Treatment""   (random)</li>
</ul>

<p>So, is this the correct syntax to use? </p>

<pre><code>m4 &lt;- lmer(Obs~Treatment*Day + (1+Treatment/Subject) + (1+Day*Treatment/Subject), mydata)
</code></pre>

<p>I'm particularly concerned about whether the Day crossed with ""Subject within Treatment"" part is right.  Is anyone familiar with SAS, or confident that they understand what's going on in his model, able to comment on whether my sad attempt at R syntax matches?</p>

<p>Here are my previous attempts at building a model and writing syntax (discussed in answers &amp; comments):</p>

<pre><code>m1 &lt;- lmer(Obs ~ Treatment * Day + (1 | Subject), mydata)
</code></pre>

<p>How do I deal with the fact that subject is nested within treatment?  How does <code>m1</code> differ from: </p>

<pre><code>m2 &lt;- lmer(Obs ~ Treatment * Day + (Treatment|Subject), mydata)
m3 &lt;- lmer(Obs ~ Treatment * Day + (Treatment:Subject), mydata)
</code></pre>

<p>and are <code>m2</code> and <code>m3</code> equivalent (and if not, why)?</p>

<p>Also, do I need to be using nlme instead of lme4 if I want to specify the correlation structure (like <code>correlation = corAR1</code>)?  According to <a href=""http://circ.ahajournals.org/content/117/9/1238.full"">Repeated Measures</a>, for a repeated-measures analysis with repeated measures on one factor, the covariance structure (the nature of the correlations between measurements of the same subject) is important. </p>

<p>When I was trying to do a repeated-measures ANOVA, I'd decided to use a Type II SS; is this still relevant, and if so, how do I go about specifying that?</p>

<p>Here's an example of what the data look like:</p>

<pre><code>mydata &lt;- data.frame(
  Subject  = c(13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 
               34, 35, 36, 37, 38, 39, 40, 62, 63, 64, 65, 13, 14, 15, 16, 17, 18, 
               19, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 
               40, 62, 63, 64, 65, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 
               29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 62, 63, 64, 65), 
  Day       = c(rep(c(""Day1"", ""Day3"", ""Day6""), each=28)), 
  Treatment = c(rep(c(""B"", ""A"", ""C"", ""B"", ""C"", ""A"", ""A"", ""B"", ""A"", ""C"", ""B"", ""C"", 
                      ""A"", ""A"", ""B"", ""A"", ""C"", ""B"", ""C"", ""A"", ""A""), each = 4)), 
  Obs       = c(6.472687, 7.017110, 6.200715, 6.613928, 6.829968, 7.387583, 7.367293, 
                8.018853, 7.527408, 6.746739, 7.296910, 6.983360, 6.816621, 6.571689, 
                5.911261, 6.954988, 7.624122, 7.669865, 7.676225, 7.263593, 7.704737, 
                7.328716, 7.295610, 5.964180, 6.880814, 6.926342, 6.926342, 7.562293, 
                6.677607, 7.023526, 6.441864, 7.020875, 7.478931, 7.495336, 7.427709, 
                7.633020, 7.382091, 7.359731, 7.285889, 7.496863, 6.632403, 6.171196, 
                6.306012, 7.253833, 7.594852, 6.915225, 7.220147, 7.298227, 7.573612, 
                7.366550, 7.560513, 7.289078, 7.287802, 7.155336, 7.394452, 7.465383, 
                6.976048, 7.222966, 6.584153, 7.013223, 7.569905, 7.459185, 7.504068, 
                7.801867, 7.598728, 7.475841, 7.511873, 7.518384, 6.618589, 5.854754, 
                6.125749, 6.962720, 7.540600, 7.379861, 7.344189, 7.362815, 7.805802, 
                7.764172, 7.789844, 7.616437, NA, NA, NA, NA))
</code></pre>
"
"0.11356975465314","0.128141957406415"," 58874","<p>As the title says, what I'd like to do is stepwise introduction of predictor variables to a mixed-effects model. I'm going to first say what I'd be doing if it were stepwise linear regression, just to make sure I've got that part right, and then describe the full model to which I want to apply an analogous approach.</p>

<p>I have a student population who took a pretest, then a tutorial, then a posttest. The tutorial involved doing problems from several categories with feedback, and the users could control which category the next problem would come from and when to stop the tutorial.</p>

<p>I want to create a model that will account for posttest performance using pretest score and some measures of behavior during the tutorial, including total number of problems done, accuracy, and probability of switching category. The last of these is of greatest theoretical interest. There are other variables I'm not mentioning for simplicity.</p>

<p>For the linear regression approach, I first did a simple regression using posttest score as the DV and including the main effects (only) of pretest score, tutorial accuracy, and number of problems as predictors. Then, I added probability of switching as an additional predictor, and compared the resulting model to the previous one to see if it had significantly better explanatory power (it did). The R code I used is below.</p>

<pre><code>lm1 &lt;- lm( posttestScore ~ pretestScore + practiceAccuracy + practiceNumTrials, data=subj.data )
lm2 &lt;- lm( posttestScore ~ pretestScore + practiceAccuracy + practiceNumTrials + probCategorySame, data=subj.data )
anova( lm1, lm2 )
</code></pre>

<p>So far so good? OK, next, I switched to a mixed model in order to include a binary within-subjects factor, 'test question type'. Both pretest and posttest have values for each level of this factor for every subject. (It's unrelated to the 'problem category' I mentioned for the tutorial.) The other predictors, however, only have one value for each participant. My models then became:</p>

<pre><code>library( nlme )
lm1 &lt;- lme( posttestScore ~ pretestScore + questionType + practiceAccuracy + practiceNumTrials, random=~1|sid, method=""REML"", data=D )
lm2 &lt;- lme( posttestScore ~ pretestScore + questionType + practiceAccuracy + practiceNumTrials + probCategorySame, random=~1|sid, method=""REML"", data=D )
</code></pre>

<p>However, I don't know how to test whether the second model resulted in a significant improvement over the first model. Is that the right question I should be asking and, if so, how should I do it?</p>
"
"0.143877159211166","0.146104310758895"," 59608","<p>I performed an experiment on coral colonies (10 colonies, randomly chosen in repeated measurements); manipulating aragonite/carbon chemistry and temperature (fixed effect) to analyse effects on the respiration rate (response variable). </p>

<p>I used the standard ""aov"" argument to test for significances: </p>

<pre><code>&gt; aovARAGT&lt;-aov(RR~HiC*HiT+SIZE, data=neu) #HiC = param. ARAG / HiT = param. T
&gt; summary(aovARAGT)
        Df  Sum Sq Mean Sq F value  Pr(&gt;F)   
HiC          1 0.02391 0.02391   2.088 0.17411   
HiT          1 0.19208 0.19208  16.769 0.00149 **
SIZE         1 0.02423 0.02423   2.116 0.17145   
HiC:HiT      1 0.00191 0.00191   0.167 0.69043   
Residuals   12 0.13746 0.01145                    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Now I want to test for random effects on the corals respiration rate using a glm, size of the colonies is another variable I included.</p>

<p>I am unsure how to interpret the results and to say whether the individual animal has a significant effect on the response variable or not. My best guess is to compare a model with random effect with a model without random effects, but I don't have a clue how this comparison is made.</p>

<pre><code>&gt; coralglm&lt;-lme(RR~TEMP*ARAG+SIZE, random= ~ 1 | ANIMAL, data=neu, method=""ML"")
&gt; summary(coralglm)
Linear mixed-effects model fit by maximum likelihood
Data: neu 
AIC       BIC   logLik
-15.18325 -9.350752 14.59162

Random effects:
Formula: ~1 | ANIMAL
 (Intercept)  Residual
StdDev: 2.142837e-06 0.1025639

Fixed effects: RR ~ TEMP * ARAG + SIZE 
         Value Std.Error DF    t-value p-value
(Intercept)  2.2967729 2.5158883  7  0.9129073  0.3916
TEMP        -0.0906237 0.0865013  5 -1.0476570  0.3428
ARAG        -0.2221899 1.6720758  5 -0.1328827  0.8995
SIZE         0.0017142 0.0014612  7  1.1731477  0.2791
TEMP:ARAG    0.0087810 0.0572029  5  0.1535061  0.8840
Correlation: 
  (Intr) TEMP   ARAG   SIZE  
TEMP      -0.996                     
ARAG      -0.963  0.959              
SIZE       0.073 -0.155 -0.051       
TEMP:ARAG  0.958 -0.955 -0.999  0.046

Standardized Within-Group Residuals:
   Min         Q1        Med         Q3        Max 
-2.3657936 -0.2605501  0.2769216  0.7377278  1.1827805 

Number of Observations: 17
Number of Groups: 9 
</code></pre>

<p>From these two outputs a couple of questions arise:</p>

<ol>
<li>From the ANOVA output I read that only temperature has a significant effect on the response variable - correct?</li>
<li>Why is Temp not significant in the glm and are the outputs of these methodically very different approaches even comparable - do I use both for my statistics?</li>
<li>How do I find out if the individuality of the corals has a significant random impact on my response variable?</li>
</ol>
"
"0.132347737294141","0.134396418749691"," 59861","<p><strong>Data structure:</strong>
I have two datasets from two protected areas that differ in protection status. Both areas contain 43 and 37 sites each. </p>

<p><strong>Question:</strong>
I would like to know which test would be the best for testing whether the PA status has had an effect on:  </p>

<ol>
<li>the first axis of a PCoA (principal coordinates analysis) - i.e. species composition turnover (derived by constructing a bray curtis dissimilarity matrix) and </li>
<li>species richness per site (a continuous variable). </li>
</ol>

<p><strong>Problem:</strong>
I understand that there is pseudoreplication present in this as I only have two areas. From what I have read, it seems that I either have to use an ANCOVA / GLM / mixed-effect model, where I define PA status as both a random effect and a fixed effect. I intended to nest sites within PA, but it seems that as there is only one datapoint per site it will not work as a nested object. </p>

<p>For those familiar with R, here are some codes I have tried:</p>

<pre><code>pcoaPAanovadata1 &lt;- read.csv(""PCoA\\data\\
                              combined data PCoA axis 1 with distance variables.csv"", 
                              header=T)

str(pcoaPAanovadata1)
'data.frame': 80 obs. of 7 variables:
PCOA:    num -0.2215 -0.3521 -0.0611 0.3434 -0.3624 ...
PA.stat: Factor w/ 2 levels ""N"",""P"": 1 1 1 1 1 1 1 1 1 1 ...
village: num 33.6 33.7 39.9 37.9 34 ...
road:    num 4.18 3.8 0.89 0.1 3.43 5.49 1.86 5.04 0.79 0.88 ...
track:   num 8.11 6.48 3.11 2.71 4.49 5.35 1.25 4.03 7.62 6.77 ...
site:    Factor w/ 80 levels ""M1_11"",""M1_17"",..: 1 2 3 4 5 6 7 8 9 10 ...
rich:    num 3.27 1.79 7.31 0.82 1.79 1.82 2.45 0.82 5.47 2.79 ...
</code></pre>

<p>compare community composition turnover at different PAs:
below specifies a null model where the slope deviates as a result of the random effect </p>

<pre><code>z0 &lt;- lmer(rich ~ 1, random = ~ 1 | pastat/site, data = pcoaPAanovadata1)
summary(z0)
z1 &lt;- lme(rich ~ pastat, random = ~ 1 | pastat/site, data = pcoaPAanovadata1)
summary(z1)
anova(z0,z1)
</code></pre>

<p>impacts of distance variables:</p>

<pre><code>zz &lt;- lme(pcoa ~ road, random = ~ 1 | pastat/site, data = pcoaPAanovadata1)
summary(zz)
</code></pre>

<p>The errors I get from the lme(linear mixed effect model):</p>

<pre><code>Warning message:
In pt(-abs(tTable[, ""t-value""]), tTable[, ""DF""]) : NaNs produced
</code></pre>

<p>The error I get from the ANOVA:</p>

<pre><code>Warning message:
In anova.lme(z0, z1) :
fitted objects with different fixed effects. REML comparisons are not meaningful.
</code></pre>

<p>Firstly, I was hoping to just clarify whether the test I am running is correct. Secondly, it'd be great if someone could tell me what the errors mean. I apologise if my question is poorly phrased, I am relatively new to R and the statistics I am using. </p>
"
"0.126188616281267","0.128141957406415"," 60403","<p>I am a bit stumped on the behavior of $R^2$ in non-linear models.</p>

<p>Below is some data and two hyperbolic fits. One in which two parameters are estimated (Model $m_1$), and another in which one parameter is fixed at $100$, and only the other parameter is estimated (Model $m_2$). Model $m_1$ has much smaller residual standard error ($8.01$ on $2$ df) than $m_2$ ($13.7$ on $3$ df) and just by looking at it (see graph below), a better fit than $m_2$ (even though $m_1$ itself could be improved). The residual sums of squares as reported by anova() are $128.35$ for $m_1$ and $565.80$ for $m_2$. With one df difference that yields a p-value for the difference in SS residual of $.12$. </p>

<p>The difference in residual sums of squares and the eyeball fit is not surprising. </p>

<p>Yet, the $R^2$ of $m_2$ (the constrained model) is much better ($R^2$ is computed (probably incorrectly) by squaring the correlation of predicted values and observed values of $Y$). $m_1$ has a squared correlation of observed and predicted Y values of $.921$, whereas $m_2$ has $.995$.   </p>

<p>Is $R^2$ useful at all for these models? Can one distinguish these models based on $R^2$ (e.g., an exponential model could be a competing model and we may check $R^2$ of that model). </p>

<p>I am curious about this, because in my corner of the literature these types of models are favored or discarded based on $R^2$ evidence and I also don't understand how a constraint can improve $R^2$ in this situation.</p>

<pre><code>test &lt;- data.frame(x=c(1,10,50,100),y=c(57.7,28.0,17.8,14.8))

m1 &lt;- nls(y ~ a / (1+b*x),test,start=list(a=200,b=.07))
m2 &lt;- nls(y ~ 100 / (1+b*x),test,start=list(b=.07))
coeffm1 &lt;- coefficients(m1)
coeffm2 &lt;- coefficients(m2)
summary(m1)
summary(m2)

anova(m1,m2)

test$m1pred &lt;- fitted(m1)
test$m2pred &lt;- fitted(m2)

cor(test$y,test$m1pred)^2
cor(test$y,test$m2pred)^2

plot(test$x,test$y,ylim=c(0,60))
curve((y=coeffm1[""a""] / (1+coeffm1[""b""]*x)),add=T)
curve((y=100 / (1+coeffm2[""b""]*x)),add=T,lty=""dashed"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/XEOHw.jpg"" alt=""Solid line is model m1, dashed line is model m2. Eyeball fit of m1 is better than m2.""></p>

<p><em>Solid line is model m1, dashed line is model m2. Eyeball fit of m1 is better than m2.</em></p>
"
"0.05643326479831","0.0573068255061253"," 62513","<p>I understand that the output of, for example, <code>summary(lm(Fertility ~ . , data = swiss))</code> is a table of $\hat\beta$s, standard errors, t-statistics, and p-values for two-sided tests of the null hypothesis that each parameter is equal to 0.</p>

<p>But, what should I actually call this method of evaluating a linear model when presenting it to non-statisticians? If I say I used a t-test, it will confuse the hell out of them.</p>

<p>We have the term ANOVA to describe testing hypotheses about coefficients by using F-statistics when we don't care about the direction of change. Is there a corresponding concise, unambiguous term for testing hypotheses about linear coefficients by using their t-statistics when we <em>do</em> care about the direction of change?</p>

<p>Thanks.</p>
"
"0.138232703275227","0.128674774496326"," 63872","<p>I am think that it is possible to analyse <strong>a model with just random effects</strong> but I am not sure as I have never done it. I am looking for guidance on whether it is appropriate, what assumptions I need to be aware of, and how to do it properly.</p>

<p>From my study of an insect; </p>

<ul>
<li>I have a response variable (age at death, ""age"")  </li>
<li>Two treatments
(""Treat1"" and ""Treat2"") both of which have two levels (Treat1 has
""A"" and ""B"", and Treat2 has ""P"" and ""Q"")  </li>
<li>There is also 40 genotypes
(1-40)  </li>
<li>With four replicates (w,x,y,z) of each combination of
Genotype/Treat1/Treat2 </li>
<li>Each replicate contains 50 individuals</li>
</ul>

<p>Put simply, my data looks like 32000 rows of this:</p>

<pre><code>Treat1  Treat2  Genotype  Block  Individual   Age   
A       P       1         w      1            23
A       P       1         w      2            35
A       P       1         w      3            44
.       .       .         .      .            .
.       .       .         .      .            .
.       .       .         .      .            .
B       Q       40        z      50           76     
</code></pre>

<p>I would like to know if each combination of Treat1 and Treat2 (AP,AQ,BP,BQ) have genetic genetic variation - i.e. is there variation between my 40 genotypes within each treatment combination?</p>

<p>I think I need a model for each of AP, AQ, BP, and BQ, along the lines of </p>

<pre><code>Age ~ Genotype [ Treat1 == ""A"" &amp; Treat2 == ""P""] * Block [ Treat1 == ""A"" &amp; Treat2 == ""P""]
</code></pre>

<p>Where  Genotype and Block are random effects. I hear Gamma distribtions are better to use in lifespan (time to death) models.</p>

<p><strong>My questions are:</strong></p>

<p>a. Is this an appropriate way to show whether or not my genotypes have variation?</p>

<p>b. Can I build the four models as defined above or is that a really poor way of doing it?</p>

<p>c. If possible, what functions should I be using in R (lm, glm, lmer... &amp; summary, summary.lm, aov, anova...)?</p>

<p>d. What should I expect, if gamma is more suitable than gaussian, to see when I compare <code>plot(model)</code> for gamma compared to gaussian?</p>

<hr>

<p>This is currently my model...</p>

<pre><code>AP= df$Treat1==""A"" &amp; df$Treat2==""P""
apmodel&lt;- lmer(df$Age[AP]~(1|df$Genotype[AP])+(1|df$Block[AP]))
summary(apmodel)
</code></pre>

<p>Which I think is right but I'm not sure what to do with the output..</p>

<pre><code>&gt; summary(apmodel)
Linear mixed model fit by REML 
Formula: df$Age[AP] ~ (1 | df$Genotype[AP]) + (1 | df$Block[AP]) 
       AIC   BIC logLik deviance REMLdev
     57343 57371 -28667    57336   57335
    Random effects:
     Groups           Name        Variance Std.Dev.
     df$Genotype[AP]  (Intercept) 17.23798 4.15186 
     df$Block[AP]     (Intercept)  0.15416 0.39263 
     Residual                     93.18777 9.65338 
    Number of obs: 7757, groups: df$line[AP], 40; df$Block[AP], 4

Fixed effects:
            Estimate Std. Error t value
(Intercept)  49.9948     0.6939   72.05
</code></pre>

<p><strong>Is there genetic variance??</strong></p>
"
"0.0987582133970426","0.114613651012251"," 63913","<p>I conducted an experiment in a factorial design: I measured light (PAR) in three herbivore treatments as well as six nutrient treatments. The experiment was blocked.</p>

<p>I've run the linear model as follows (you can download the data from my website to replicate)</p>

<pre><code>dat &lt;- read.csv('http://www.natelemoine.com/testDat.csv')
mod1 &lt;- lm(light ~ Nutrient*Herbivore + BlockID, dat)
</code></pre>

<p>The residual plots look pretty good</p>

<pre><code>par(mfrow=c(2,2))
plot(mod1)
</code></pre>

<p>When I look at the ANOVA table, I see main effects of Nutrient and Herbivore. </p>

<pre><code>anova(mod1)

Analysis of Variance Table 

Response: light 
                    Df  Sum Sq Mean Sq F value    Pr(&gt;F)     
Nutrient             5  4.5603 0.91206  7.1198 5.152e-06 *** 
Herbivore            2  2.1358 1.06791  8.3364 0.0003661 *** 
BlockID              9  5.6186 0.62429  4.8734 9.663e-06 *** 
Nutrient:Herbivore  10  1.7372 0.17372  1.3561 0.2058882     
Residuals          153 19.5996 0.12810                       
--- 
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
</code></pre>

<p>However, the regression table shows non-significant main effects and significant interactions.</p>

<pre><code>summary(mod1)

Call: 
lm(formula = light ~ Nutrient * Herbivore + BlockID, data = dat) 

Residuals: 
     Min       1Q   Median       3Q      Max  
-0.96084 -0.19573  0.01328  0.24176  0.74200  

Coefficients: 
                           Estimate Std. Error t value Pr(&gt;|t|)     
(Intercept)                1.351669   0.138619   9.751  &lt; 2e-16 *** 
Nutrientb                  0.170548   0.160064   1.066  0.28833     
Nutrientc                 -0.002172   0.160064  -0.014  0.98919     
Nutrientd                 -0.163537   0.160064  -1.022  0.30854     
Nutriente                 -0.392894   0.160064  -2.455  0.01522 *   
Nutrientf                  0.137610   0.160064   0.860  0.39129     
HerbivorePaired           -0.074901   0.160064  -0.468  0.64049     
HerbivoreZebra            -0.036931   0.160064  -0.231  0.81784     
... 
Nutrientb:HerbivorePaired  0.040539   0.226364   0.179  0.85811     
Nutrientc:HerbivorePaired  0.323127   0.226364   1.427  0.15548     
Nutrientd:HerbivorePaired  0.642734   0.226364   2.839  0.00513 **  
Nutriente:HerbivorePaired  0.454013   0.226364   2.006  0.04665 *   
Nutrientf:HerbivorePaired  0.384195   0.226364   1.697  0.09168 .   
Nutrientb:HerbivoreZebra   0.064540   0.226364   0.285  0.77594     
Nutrientc:HerbivoreZebra   0.279311   0.226364   1.234  0.21913     
Nutrientd:HerbivoreZebra   0.536160   0.226364   2.369  0.01911 *   
Nutriente:HerbivoreZebra   0.394504   0.226364   1.743  0.08338 .   
Nutrientf:HerbivoreZebra   0.324598   0.226364   1.434  0.15362     
--- 
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.3579 on 153 degrees of freedom 
Multiple R-squared:  0.4176,    Adjusted R-squared:  0.3186  
F-statistic: 4.219 on 26 and 153 DF,  p-value: 8.643e-09 
</code></pre>

<p>I know that this question has been previously <a href=""http://stats.stackexchange.com/questions/20002/regression-vs-anova-discrepancy"">asked and answered</a> in <a href=""http://stats.stackexchange.com/questions/28938/why-do-linear-regression-and-anova-give-different-p-value-in-case-of-consideri"">multiple posts</a>. In the earlier posts, the issue revolved around the different types of SS used in anova() and lm(). However, I don't think that is the issue here. First of all, the design is balanced:</p>

<pre><code>with(dat, tapply(light, list(Nutrient, Herbivore), length))
</code></pre>

<p>Second, using the Anova() option doesn't change the anova table. This isn't a surprise because the design is balanced.</p>

<pre><code>Anova(mod1, type=2)
Anova(mod1, type=3)
</code></pre>

<p>Changing the contrast doesn't change the results (qualitatively). I still get pretty much backwards intepretations from anova() vs. summary().</p>

<pre><code>options(contrasts=c(""contr.sum"",""contr.poly""))
mod2 &lt;- lm(light ~ Nutrient*Herbivore + BlockID, dat)
anova(mod2)
summary(mod2)
</code></pre>

<p>I'm confused because everything I've read on regression not agreeing with ANOVA implicates differences in the way R uses SS for summary() and anova() functions. However, in the balanced design, the SS types are equivalent, and the results here don't change. How can I have completely opposite interpretations depending on which output I use?</p>
"
"0.195647144107831","0.190728644119756"," 64010","<p>I am wondering what the exact relationship between partial $R^2$ and coefficients in a linear model is and whether I should use only one or both to illustrate the importance and influence of factors.</p>

<p>As far as I know, with <code>summary</code> I get estimates of the coefficients, and with <code>anova</code> the sum of squares for each factor - the proportion of the sum of squares of one factor divided by the sum of the sum of squares plus residuals is partial $R^2$ (the following code is in <code>R</code>).</p>

<pre><code>library(car)
mod&lt;-lm(education~income+young+urban,data=Anscombe)
    summary(mod)

Call:
lm(formula = education ~ income + young + urban, data = Anscombe)

Residuals:
    Min      1Q  Median      3Q     Max 
-60.240 -15.738  -1.156  15.883  51.380 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -2.868e+02  6.492e+01  -4.418 5.82e-05 ***
income       8.065e-02  9.299e-03   8.674 2.56e-11 ***
young        8.173e-01  1.598e-01   5.115 5.69e-06 ***
urban       -1.058e-01  3.428e-02  -3.086  0.00339 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 26.69 on 47 degrees of freedom
Multiple R-squared:  0.6896,    Adjusted R-squared:  0.6698 
F-statistic: 34.81 on 3 and 47 DF,  p-value: 5.337e-12

anova(mod)
Analysis of Variance Table

Response: education
          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
income     1  48087   48087 67.4869 1.219e-10 ***
young      1  19537   19537 27.4192 3.767e-06 ***
urban      1   6787    6787  9.5255  0.003393 ** 
Residuals 47  33489     713                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>The size of the coefficients for 'young' (0.8) and 'urban' (-0.1, about 1/8 of the former, ignoring '-') does not match the explained variance ('young' ~19500 and 'urban' ~6790, i.e. around 1/3).</p>

<p>So I thought I would need to scale my data because I assumed that if a factor's range is much wider than another factor's range their coefficients would be hard to compare:</p>

<pre><code>Anscombe.sc&lt;-data.frame(scale(Anscombe))
mod&lt;-lm(education~income+young+urban,data=Anscombe.sc)
summary(mod)

Call:
lm(formula = education ~ income + young + urban, data = Anscombe.sc)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.29675 -0.33879 -0.02489  0.34191  1.10602 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  2.084e-16  8.046e-02   0.000  1.00000    
income       9.723e-01  1.121e-01   8.674 2.56e-11 ***
young        4.216e-01  8.242e-02   5.115 5.69e-06 ***
urban       -3.447e-01  1.117e-01  -3.086  0.00339 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.5746 on 47 degrees of freedom
Multiple R-squared:  0.6896,    Adjusted R-squared:  0.6698 
F-statistic: 34.81 on 3 and 47 DF,  p-value: 5.337e-12

anova(mod)
Analysis of Variance Table

Response: education
          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
income     1 22.2830 22.2830 67.4869 1.219e-10 ***
young      1  9.0533  9.0533 27.4192 3.767e-06 ***
urban      1  3.1451  3.1451  9.5255  0.003393 ** 
Residuals 47 15.5186  0.3302                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1    
</code></pre>

<p>But that doesn't really make a difference, partial $R^2$ and the size of the coefficients (these are now <em>standardized coefficients</em>) still do not match:</p>

<pre><code>22.3/(22.3+9.1+3.1+15.5)
# income: partial R2 0.446, Coeff 0.97
9.1/(22.3+9.1+3.1+15.5)
# young:  partial R2 0.182, Coeff 0.42
3.1/(22.3+9.1+3.1+15.5)
# urban:  partial R2 0.062, Coeff -0.34
</code></pre>

<p><strong>So is it fair to say that 'young' explains three times as much variance as 'urban' because partial $R^2$ for 'young' is three times that of 'urban'?</strong> Why is the coefficient of 'young' then not three times that of 'urban' (ignoring the sign)?</p>

<p>I suppose the answer for this question will then also tell me the answer to my initial query: Should I use partial $R^2$ or coefficients to illustrate the relative importance of factors? (Ignoring direction of influence - sign - for the time being.)</p>

<p><strong>Edit:</strong></p>

<p>Partial eta-squared appears to be another name for what I called partial $R^2$. <a href=""http://www.inside-r.org/packages/cran/heplots/docs/etasq"">etasq {heplots}</a> is a useful function that produces similar results:</p>

<pre><code>etasq(mod)
          Partial eta^2
income        0.6154918
young         0.3576083
urban         0.1685162
Residuals            NA
</code></pre>
"
"0.0691163516376137","0.0701862406343596"," 67840","<p>I have collected data on gas fluxes from plots of soil subjected to 5 different treatments (""D2"", ""K2"", ""M"", ""N"", and ""O2""), which also possessed variable clay contents.  The experiment was laid out in a randomized complete block design, with 4 replications.  Within each plot, two separate measurements of flux were performed.  The resulting data.frame resembles the following:</p>

<pre><code>block   treatment   subsample       flux        clay
1           D2          1           112068.6003 14.8
1           D2          2           129223.1641 14.8
1           K2          1           256712.4712 15.5
1           K2          2           113343.9756 15.5
1           M2          1           85794.47834 16.4
1           M2          2           -33620.6990 16.4
1           N           1           70283.98133 18.2
1           N           2           49569.84621 18.2
1           O2          1           100553.1116 13.4
1           O2          2           38885.99674 13.4
2           D2          1           96968.58451 15.8
</code></pre>

<p>I want to build a linear mixed effect model that takes account of this subsampling, and have come up with:</p>

<pre><code>flux.lme &lt;- lme(flux ~ treatment + block, random = ~1|subsample)
</code></pre>

<p>Which produces an ANOVA table:</p>

<pre><code>&gt; anova(flux.lme)
               numDF denDF   F-value p-value
(Intercept)        1    26 158.15781  &lt;.0001
treatment          4    26   8.88691  0.0001
clay               1    26   1.72640  0.2003
block              3    26   1.59188  0.2153
treatment:clay     4    26   1.73011  0.1736
</code></pre>

<p>This output seems a bit strange to me, as the denominator degrees of freedom should not be 26, which is taking each repeated sampling as independent experimental unit. It should instead be based on the number of â€œmain plotsâ€, which is 20. In my case the denominator d.f. should be 20-1-3-1-4-4=7. Is is possible to instruct the lme() function to use this value?</p>
"
"0.119713032670143","0.121566134770966"," 73191","<p>For ordinary linear regression with Gaussian noise, it is easy to interpret the significance of a variable.  This is consistent with a partial F test.  The square of the t-test for the second variable equals to the partial F-test statistic, and their p-values are the same.</p>

<p>I wrote simple R codes to confirm this.</p>

<p>Is there something like this for logistic regression?  I thought/hoped that the likelihood ratio test would correspond to this, but no.  What should I do if the variable and the likelihood ratio test (of adding that particular variable) do not have the same (in)significant effect?</p>

<p>I appreciate your time and help,</p>

<pre><code>rm(list=ls(all=TRUE)) 
n = 100   ;       x1 = runif(n,-4,4)   ;       x2 = runif(n,6,10)
y = 3*x1 + 8*x2 + rnorm(n,2,4)
l1 = lm(y~x1)  ;  l2 = lm(y~x1+x2)  ;  a = anova(l1,l2)

summary(l1)$coeff
             Estimate Std. Error   t value     Pr(&gt;|t|)
(Intercept) 66.093853  1.0123131 65.289929 1.385202e-82
x1           3.199212  0.4292828  7.452458 3.664499e-11

summary(l2)$coeff
            Estimate Std. Error    t value     Pr(&gt;|t|)
(Intercept) 2.767750  2.7871368  0.9930441 3.231592e-01
x1          2.870897  0.1707022 16.8181610 1.648852e-30
x2          7.871545  0.3428392 22.9598753 5.370614e-41

(summary(l2)$coeff[3,3])^2
527.1559
&gt;     a 
    Analysis of Variance Table

Model 1: y ~ x1
Model 2: y ~ x1 + x2
  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    
1     98 9899.1                                  
2     97 1538.4  1    8360.6 527.16 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

&gt;     a$F ; a$Pr
   [1]       NA 527.1559
[1]           NA 5.370614e-41
&gt; 
&gt; 
&gt; 
&gt; rm(list=ls(all=TRUE)) 
&gt; n = 100
&gt; x1 = runif(n,-4,4)
&gt; x2 = runif(n,6,10)
&gt; 
&gt; y = rbinom(n,1,1/(1+exp(-3*x1 - 2*x2 + 20)))
&gt; 
&gt; l1 = glm(y~x1,family=binomial)
&gt; l2 = glm(y~x1+x2,family=binomial)
&gt; 
&gt; a = anova(l1,l2)
&gt; 
&gt; summary(l1)$coeff
                 Estimate Std. Error   z value     Pr(&gt;|z|)
    (Intercept) -2.988069   0.812041 -3.679702 2.335068e-04
    x1           2.115333   0.498431  4.243984 2.195858e-05
    &gt; summary(l2)$coeff
              Estimate Std. Error   z value     Pr(&gt;|z|)
(Intercept) -17.215960  5.5710699 -3.090243 0.0019999276
x1            3.048657  0.8618367  3.537395 0.0004040949
x2            1.675323  0.5976386  2.803238 0.0050592272
&gt; 
&gt; (summary(l2)$coeff[3,3])^2
    [1] 7.858145
    &gt; 
    &gt; l1$deviance -  l2$deviance
    [1] 13.65371
    &gt; pchisq(l1$deviance -  l2$deviance,df=1)
[1] 0.9997802
&gt; 
&gt; a
Analysis of Deviance Table

Model 1: y ~ x1
Model 2: y ~ x1 + x2
  Resid. Df Resid. Dev Df Deviance
1        98     45.534            
2        97     31.880  1   13.654
&gt; a$F
    NULL
    &gt; a$Pr
    NULL
</code></pre>
"
"0.143877159211166","0.146104310758895"," 74971","<p>When you collect data from participants in an experiment, sometimes you can collect repeated responses for <em>the same condition</em>, e.g., in R:</p>

<pre><code>set.seed(2012) # keep the example the same each time.

data.full &lt;- data.frame(id=gl(10, 4),
                        condition=gl(2, 40),
                        response=c(rnorm(40), rnorm(40, 1)))
head(data.full)

# Output:
#   id condition    response
# 1  1         1 -0.77791825
# 2  1         1 -0.57787590
# 3  1         1  0.66325605
# 4  1         1  0.08802235
# 5  2         1  1.25707865
# 6  2         1 -0.62977450
</code></pre>

<p>To analyse this (i.e. does condition predict response) I would normally take the mean response for each participant, for each condition. I would do this on the basis that we are supposed to be generalizing from a sample to a population, i.e. there should be one 'estimate' response from each participant for each condition, and the collection of these single responses (for each condition) is our sample, then we do an analysis which generalizes to the population.</p>

<p>I would transform the data e.g. like this:</p>

<pre><code>library(plyr)
data.means &lt;- ddply(data.full, .(id, condition),
                    summarize,
                    mean.response=mean(response))
head(data.means)

# Output:
#   id condition mean.response
# 1  1         1    -0.1511289
# 2  1         2     0.8658770
# 3  2         1     0.1510842
# 4  2         2     0.0129323
# 5  3         1     0.1857577
# 6  3         2     0.9859697
</code></pre>

<p>And then proceed with the within-subjects analysis (note the same process would apply if there were more conditions or a 2x2 design etc.), e.g.:</p>

<pre><code>aov1 &lt;- aov(mean.response ~ condition + Error(id/condition), data=data.means)
summary(aov1) # F = 4.2, p = .07, not significant
</code></pre>

<p>However, I've been told that with linear mixed-effects models, you can include all the underlying data on the basis that the lme models can include correlated data. My understanding was that they could include correlated data meant they could include responses from the same participants (within-subjects effects modelled as random effects), not that you could include the underlying data that gives the participant response estimate.</p>

<p>My question is, can you include the underlying data collected from the multiple responses of each participant in the <em>same condition</em>, i.e. can you do this:</p>

<pre><code>library(nlme)
lme1 &lt;- lme(response ~ 1, random= ~ 1|id/condition, data=data.full, method=""ML"")
lme2 &lt;- update(lme1, .~. + condition)
anova(lme1, lme2)

# X(1) = 3.19, p = .07, not significant
</code></pre>

<p>Or should you do this:</p>

<pre><code>lme1 &lt;- lme(mean.response ~ 1, random= ~ 1|id/condition, data=data.means, method=""ML"")
lme2 &lt;- update(lme1, .~. + condition)
anova(lme1, lme2)

# X(1) = 5.25, p = .02, significant
</code></pre>

<p>Which is the correct approach?</p>
"
"0.138643499732942","0.140789636664653"," 76250","<p>I am new to statistics and I am trying to understand the difference between ANOVA and linear regression. I am using R to explore this. I read various articles about why ANOVA and regression are different but still the same and how the can be visualised etc. I think I am pretty there but one bit is still missing.</p>

<p>I understand that ANOVA compares the variance within groups with the variance between groups to determine whether there is or is not a difference between any of the groups tested. (<a href=""https://controls.engin.umich.edu/wiki/index.php/Factor_analysis_and_ANOVA"" rel=""nofollow"">https://controls.engin.umich.edu/wiki/index.php/Factor_analysis_and_ANOVA</a>)</p>

<p>For linear regression, I found a post in this forum which says that the same can be tested when we test whether b (slope) = 0.
(<a href=""http://stats.stackexchange.com/questions/555/why-is-anova-taught-used-as-if-it-is-a-different-research-methodology-compared"">Why is ANOVA taught / used as if it is a different research methodology compared to linear regression?</a>)</p>

<p>For more than two groups I found a website stating:</p>

<p>The null hypothesis is: $\text{H}_0: Âµ_1 = Âµ_2 = Âµ_3$</p>

<p>The linear regression model is: $y = b_0 + b_1X_1 + b_2X_2 + e$</p>

<p>The output of the linear regression is, however, then the intercept for one group and the difference to this intercept for the other two groups. 
(<a href=""http://www.real-statistics.com/multiple-regression/anova-using-regression/"" rel=""nofollow"">http://www.real-statistics.com/multiple-regression/anova-using-regression/</a>)</p>

<p>for me, this looks like that actually the intercepts are compared and not the slopes?</p>

<p>Another example where they compare intercepts rather than the slopes can be found here:
(<a href=""http://www.theanalysisfactor.com/why-anova-and-linear-regression-are-the-same-analysis/"" rel=""nofollow"">http://www.theanalysisfactor.com/why-anova-and-linear-regression-are-the-same-analysis/</a>)</p>

<p>I am now struggling to understand what is actually compared in the linear regression? the slopes, the intercepts or both? </p>
"
"0.05643326479831","0.0573068255061253"," 76609","<p>I am running an experiment testing reaction times under different conditions. I have a data sample located <a href=""http://chymera.eu/data/test/ER_aov.csv"" rel=""nofollow"">here</a> and I have added a graphical plot of my data below in order to ease your understanding:</p>

<p><img src=""http://i.stack.imgur.com/owWXn.png"" alt=""enter image description here""></p>

<p>I would like to check whether weak emotion recognition has significantly higher error rates than all other conditions. I was told that the proper way to do this was a repeated measurement ANOVA. I have found out that this can be done via R's <code>stats::aov()</code> function.</p>

<p>I am interfacing with R via RPy and you may see my exact code under <a href=""http://nbviewer.ipython.org/urls/gist.github.com/TheChymera/7474588/raw/a8244233365b7cce81e58457a6571325daecc1cf/ER-aov"" rel=""nofollow"">this notebook</a>.</p>

<p>I am getting the following resulting summary:</p>

<pre><code>Error: ID
          Df Sum Sq  Mean Sq F value Pr(&gt;F)
Residuals  6  0.022 0.003666               

Error: Within
          Df  Sum Sq  Mean Sq F value Pr(&gt;F)   
COI        6 0.02628 0.004379   3.468 0.0083 **
Residuals 36 0.04547 0.001263                  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>How does this help me address my issue?</p>

<p>Additionally, in a discussion resulting from <a href=""http://stats.stackexchange.com/questions/74967/data-in-one-condition-significantly-different-from-data-in-other-conditions-test"">this other question</a> I have been told that while anova is acceptable in some cases (such as this one) linear models such as the one generated by <code>nlme::lme()</code> are preferable.</p>

<p>I have used that function in <a href=""http://nbviewer.ipython.org/urls/gist.github.com/TheChymera/7396334/raw/187786dc7375b6a4255b2b54fc4d269bd846f32d/nlme-aov"" rel=""nofollow"">this other notebook</a>, and the output reads as follows:</p>

<pre><code>Fixed effects: ER ~ COI 
                 Value  Std.Error DF  t-value p-value
(Intercept) 0.01928571 0.01514819 36 1.273137  0.2111
COIem-hard  0.07028571 0.01899579 36 3.700069  0.0007
COIsc-11    0.00403687 0.01899579 36 0.212514  0.8329
COIsc-15    0.01700000 0.01899579 36 0.894935  0.3768
COIsc-19    0.00417857 0.01899579 36 0.219974  0.8271
COIsc-23    0.00432488 0.01899579 36 0.227676  0.8212
COIsc-27    0.00417857 0.01899579 36 0.219974  0.8271
</code></pre>

<p>how am I to interpret those p-values in the context of the point I'm trying to make?
Also, why is my first COI (COIem-easy) absent from the list?</p>

<p>As a general point I would also be very happy to hear which of these 2 approaches you advise I should use. </p>
"
"0.028216632399155","0.0573068255061253"," 76625","<p>I'm looking for a way to run a repeated-measures multiple regression in R, which would take care of sphericity - either by applying some corrections (such as Huynh-Feldt), or by avoiding the problem in some other way.</p>

<p>I have 2 factorial repeated measure variables: 3- and 2-level (<code>roi_ant</code>, <code>roi_lat</code>), and a quantitative between-subject variable (<code>pred</code>), and one dependent quantitative variable (<code>mv</code>). I want to test for a full model including all possible interactions between the two within-subject and the between-subject variables (i.e., <code>mv ~ pred * roi_ant * roi_lat</code>). I am most interested in the slope of <code>pred</code> - whether it is different from 0, and whether it changes depending on <code>roi_ant</code> and <code>roi_lat</code>.</p>

<p>I have tried to do repeated-measures MANCOVA (that would remove the sphericity assumption) using car::Anova package, but if I got <a href=""http://r.789695.n4.nabble.com/car-Anova-Can-it-be-used-for-ANCOVA-with-repeated-measures-factors-td4637324.html"" rel=""nofollow"">this</a> discussion right, <code>car::Anova()</code> is not able to handle such a design.</p>

<p><code>ezANOVA()</code> performs ANCOVA with applying corrections for sphericity, but reports only the repeated-measure variables (after removing the influence of the covariate), and does not report anything on the covariate <code>pred</code>.</p>

<p>Mixed linear models (<code>lme4</code>) should handle this design well, but since <code>mcmsamp</code> is still not implemented, it is difficult to get p-values out of them.</p>

<p>Do you have any other suggestions? </p>
"
"0.0598565163350717","0.0607830673854831"," 76733","<p>I am trying to determine whether the data depicted in the following figure shows a plateau phase for higher scramblings.</p>

<p><img src=""http://i.stack.imgur.com/QaBpZ.png"" alt=""enter image description here""></p>

<p>As I have often been encouraged to use linear models instead of anova - I have decided to tackle this issue with lme4.</p>

<p>The output I get for my latex document from the <a href=""http://cran.r-project.org/web/packages/texreg/index.html"" rel=""nofollow"">texreg</a> <a href=""http://cran.r-project.org/web/packages/lme4/index.html"" rel=""nofollow"">lme4</a> parser is as follows:</p>

<pre><code>                     Model 1 
</code></pre>

<p>(Intercept)              : 2.03 &nbsp;&nbsp; [1.78; 2.28]<sup>*</sup>


      <br></p>

<p>COIsc-14                 : -0.23 &nbsp;&nbsp; [-0.38; -0.08]<sup>*</sup>


   <br></p>

<p>COIsc-18                 : -0.23 &nbsp;&nbsp; [-0.38; -0.08]<sup>*</sup>


   <br></p>

<p>COIsc-22                 : -0.19 &nbsp;&nbsp; [-0.34; -0.04]<sup>*</sup>


   <br></p>

<p>COIsc-26                 : -0.32 &nbsp;&nbsp; [-0.47; -0.17]<sup>*</sup>


   <br></p>

<p>COIsc-6                  : 0.45 &nbsp;&nbsp; [0.30; 0.60]<sup>*</sup>


      <br></p>

<p>AIC                      : 1697.17                        <br></p>

<p>BIC                      : 1735.04                        <br></p>

<p>Log Likelihood           : -840.59                        <br></p>

<p>Deviance                 : 1681.17                        <br></p>

<p>Num. obs.                : 840                            <br></p>

<p>Num. groups: ID          : 7                              <br></p>

<p>Variance: ID.(Intercept) : 0.08                           <br></p>

<p>Variance: Residual       : 0.41                           <br></p>

<p><sup>*</sup>0 outside the confidence interval</p>

<pre><code>                     Model 1 
</code></pre>

<p>My confidence interval is 0.95 . Can I make the point I am trying to make by the fact that the 14, 18, 22, and 26 values are all in each other's confidence intervals?</p>

<p>Also, what information do I get from </p>

<pre><code>Variance: ID.(Intercept) : 0.08 

Variance: Residual : 0.41 
</code></pre>

<p>? Does that mean that 0.51 of my variance is explained by my categories, 0.08 by my IDs (which I define as random) and 0.41 by unidentified sources? 0.41 sounds like a lot, is this a good model then?</p>
"
"0.0691163516376137","0.0701862406343596"," 76814","<p>I want to use both anova and linear models to test the assumption that my some of my categories have different means than the rest.</p>

<p>I am using stats::aov for anova and nlme::lme for linear modelling. The full code is available in <a href=""http://nbviewer.ipython.org/urls/gist.github.com/TheChymera/7516431/raw/e9a80f5a2f21c6097b182bf4d6b086a5e7e76048/ER-nlm-aov"" rel=""nofollow"">this notebook</a>.</p>

<p>Basically I end up getting:</p>

<pre><code>Error: ID
          Df  Sum Sq  Mean Sq F value Pr(&gt;F)
Residuals  6 0.01198 0.001997               

Error: Within
          Df  Sum Sq  Mean Sq F value Pr(&gt;F)  
COI        7 0.01926 0.002752    1.92 0.0903 .
Residuals 42 0.06020 0.001433 
</code></pre>

<p>and</p>

<pre><code>Fixed effects: ER ~ COI 
                      Value  Std.Error DF   t-value p-value
(Intercept)      0.00200000 0.01465712 42 0.1364525  0.8921
COIemotion-hard  0.05600000 0.02023699 42 2.7672098  0.0084
COIscrambling-06 0.04218045 0.02023699 42 2.0843242  0.0433
COIscrambling-10 0.02094737 0.02023699 42 1.0351029  0.3065
COIscrambling-14 0.00685714 0.02023699 42 0.3388420  0.7364
COIscrambling-18 0.02085714 0.02023699 42 1.0306445  0.3086
COIscrambling-22 0.04885714 0.02023699 42 2.4142493  0.0202
COIscrambling-26 0.02742857 0.02023699 42 1.3553681  0.1825
</code></pre>

<p>Which means that lme is telling me 3 groups may significantly lie outside the range of the others, while aov is telling me the probability of ANY GROUP AT ALL being different from the others is not significant (using a p=0.05 cutoff).</p>

<p>What am I to make of this? 
Do I understand these results correctly?</p>
"
"0.173939003877979","0.167335104133733"," 76980","<p>I'm trying to analyse some data I've recently gotten my hands on, but I'm not entirely sure which model to use. One suggestion has been a Mixed Model, Repeated Measurements ANOVA, but I'm not sure if that such kind of model can answer the questions of interest.</p>

<p><strong>The data</strong>: 
Two individual persons (A and B) have had a lot of different values (V1, V2, V3, ..., Vn) measured four times (At T0, T1, T2 and T3) - The spacing between times differs.</p>

<p>The different values have been grouped into categories (C1, C2, C3, ..., Cn). One value may belong to none, one or multiple categories. Each of the categories have a continuos value (Response_C1,Response_C1, ..., Response_Cn), which is the sum of the measured values belonging to that category. </p>

<p>In addition to this, person B was given a drug at T1.</p>

<p>What I would like to investigate now, is:</p>

<ol>
<li>Is there any observable effect after administering the drug</li>
<li>On which categories did the drug have an effect</li>
<li>If there is an effct on a category, what is the effect size</li>
<li>How does the effect vary over time</li>
<li>If there is an effect, is the effect observed from the drug at T1 still persistant at T3</li>
</ol>

<p>I realise one of the major pitfalls is the lack of both time points and samples, but it would be appreciated if you could suggest any articles/methods for this type of analysis.</p>

<p><strong>What I have tried so far</strong> is just Repeated Measurements ANOVA, using R:</p>

<pre><code>test.aov &lt;- aov(Response_C ~ Category * Timepoint * Treatment + Error(Sample), data=df)
</code></pre>

<p>But I am not sure that the model is correct, neither am I sure that it actually answers my questions, even if I try to model it as a mixed model. </p>

<p>Any help is much appreciated. Please let me know if any additional information is needed</p>

<p><strong>Edit 1:</strong> After doing some more reading, it seems a Generalised Linear Model with a negative binomial distribution (since this kind of data is usually over-dispersed) might be better suited for this kind of data, but I'm still not sure if such a model would answer the questions. Potentially I could fit a model to each individual category, but that would inflate the Type-I error I guess, and so we would need to correct for multiple testing.</p>

<p><strong>Edit 2:</strong> Some more reading, and I thought the <code>lme4</code> R package would be a good way to fit a Linear mixed model to my data, and just do individual comparisons of each category. Here's the model I tried to fit:</p>

<pre><code>lm1 &lt;- lmer(Response ~ Treatment * Timepoint + (1|Subject), data=my_data)
</code></pre>

<p>First off, I'm not sure whether Timepoint should be a factorial or a numerical value. As I mentioned, timepoints are not evenly distributed (To be precise, I have for time 0, 2days, 14 days, 90days), however, the design is balanced. If I enter the Timepoints as a numerical value, I don't get any estimate of what the value is at any given Timepoint, but just some numbers for Correlation of fixed effects, which I can't really use for anything. On the other hand, if I enter the Timepoints as factors, I do get an estimated value for the effect at each timepoint, but I'm not too sure how certain or reliable this value is.</p>
"
"0.0892288262810312","0.072488037629275"," 77891","<p>I ran a repeated design whereby I tested 30 males and 30 females across three different tasks. I want to understand how the behaviour of males and females is different and how that depends on the task. I used both the lmer and lme4 package to investigate this, however, I am stuck with trying to check assumptions for either method. The code I run is</p>

<pre><code>lm.full &lt;- lmer(behaviour ~ task*sex + (1|ID/task), REML=FALSE, data=dat)
lm.full2 &lt;-lme(behaviour ~ task*sex, random = ~ 1|ID/task, method=""ML"", data=dat)
</code></pre>

<p>I checked if the interaction was the best model by comparing it with the simpler model without the interaction and running an anova:</p>

<pre><code>lm.base1 &lt;- lmer(behaviour ~ task+sex+(1|ID/task), REML=FALSE, data=dat)
lm.base2 &lt;- lme(behaviour ~ task+sex, random= ~1|ID/task), method=""ML"", data=dat)
anova(lm.base1, lm.full)
anova(lm.base2, lm.full2)
</code></pre>

<p>Q1: Is it ok to use these categorical predictors in a linear mixed model?<br/>
Q2: Do I understand correctly it is fine the outcome variable (""behaviour"") does not need to be normally distributed itself (across sex/tasks)?<br/>
Q3: How can I check homogeneity of variance? For a simple linear model I use <code>plot(LM$fitted.values,rstandard(LM))</code>. Is using <code>plot(reside(lm.base1))</code> sufficient?<br/>
Q4: To check for normality is using the following code ok?</p>

<pre><code>hist((resid(lm.base1) - mean(resid(lm.base1))) / sd(resid(lm.base1)), freq = FALSE); curve(dnorm, add = TRUE)
</code></pre>
"
"0.164529826154006","0.167076671386255"," 78539","<p>I would like to determine the variance explained by random factors and slopes in a mixed model but am unsure if the analysis I use and my interpretation are correct. Furthermore, comparing models and analysing a mixed model with random slopes seem to give opposite conclusions, therefore I would like to know when to include random slopes? Below I give an overview of the analysis.</p>

<p>I tested three groups of 10 individuals twice in the same task. As I expect individuals to differ in their response across the two tasks, I also include random slopes. The analysis I run is:</p>

<pre><code>lme(behaviour ~ stage * group, random = ~ stage|ID, data=data)
</code></pre>

<p>Part of the output I get is the following:</p>

<pre><code>Linear mixed-effects model fit by maximum likelihood
Data: data 
    AIC       BIC   logLik
   -72.07494 -48.50785 46.03747

 Random effects:
  Formula: ~stage | ID
  Structure: General positive-definite, Log-Cholesky parametrization
             StdDev     Corr  
 (Intercept) 0.12646601 (Intr)
 stage2      0.12662159 -0.455
 Residual    0.05714907       
</code></pre>

<p>To calculate the variance I extract the SD of ID, slopes, and the residual variance as follows:</p>

<pre><code>SD.ID &lt;- (fm2$sigma * attr(corMatrix(fm2$modelStruct[[1]])[[1]],""stdDev""))[[1]]
SD.slope &lt;- (fm2$sigma * attr(corMatrix(fm2$modelStruct[[1]])[[1]],""stdDev""))[[2]]
SD.residual &lt;- fm2$sigma
</code></pre>

<p>And then calculate the percentage of variance explained:</p>

<pre><code>(SD.ID/(SD.ID+SD.slope+SD.residual))*100
(SD.slope/(SD.ID+SD.slope+SD.residual))*100
</code></pre>

<p>In this case this seems to suggest: ""individual ID and random slopes explained 40.8% and 40.8% repectively of the variance of behaviour"".</p>

<p>Although this seems to suggest the random slopes explain a large part of the variance, it seems perhaps a more simple model without slopes is more appropriate:</p>

<pre><code>fm1 &lt;- lme(behaviour ~ stage * group, random = ~ stage|ID, data=data, method=""ML"")
fm0 &lt;- lme(behaviour ~ stage * group, random = ~ 1|ID, data=data, method=""ML"")
anova(fm0,fm1)
</code></pre>

<p>since I get the following output:</p>

<pre><code>    Model df       AIC       BIC   logLik   Test    L.Ratio p-value
fm0     1  8 -76.00947 -57.15580 46.00473                          
fm1     2 10 -72.07494 -48.50785 46.03747 1 vs 2 0.06547599  0.9678
</code></pre>

<p>Which to me seems to suggest the model with the random slope does not significantly better fit the data. This seems contrasting to the 40% of the variance that it seems to explain, as shown with the data above.</p>

<p>Furthermore, if I correlate the coefficients from model fm1, thus the intercept with the slope:</p>

<pre><code>cor.test(fm1$coefficients[[2]][[1]][,1],fm1$coefficients[[2]][[1]][,2]) 
</code></pre>

<p>I get the following output:</p>

<pre><code>Pearson's product-moment correlation

data:  fm1$coefficients[[2]][[1]][, 1] and fm1$coefficients[[2]][[1]][, 2]
t = -2.6802, df = 37, p-value = 0.01092
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 -0.6376166 -0.1004855
sample estimates:
       cor 
-0.4032186 
</code></pre>

<p>Which thus seems to suggest that individuals with lower initial behaviour scores change more in their behaviour over time. Therefore again I would think running the model with the slopes would make the most sense. </p>

<p>Thus, to repeat my question: how can I determine the variance explained by random factors and slopes in a mixed model and when do I know when to include random slopes?</p>
"
"0.11286652959662","0.114613651012251"," 79029","<p>I work with habitat use data (summarized binomial data: visited / did not visit) which I fitted with a mixed model of the binomial family with a random factor accounting for repeated sampling of the same individual. I simply want basic descriptions of the data - effects of sex and year on the proportion of time spent in each habitat.</p>

<p>The code is as follow (for a basic model without interaction):</p>

<pre><code>Colony.glmmSexYear &lt;- glmmPQL(cbind(ColoYes.allnoF24, ColoNo.allnoF24) ~ 
                              year.coded + sex, random= ~1| BirdID, 
                              family=binomial(), data=mydata)
summary(Colony.glmmSexYear)
</code></pre>

<p>Here is the fixed effects section of my output: </p>

<pre><code>Fixed effects: cbind(ColoYes.allnoF24, ColoNo.allnoF24) ~ year.coded + sex

                     Value Std.Error DF   t-value p-value
(Intercept)      1.4442270 0.1824173 74  7.917161  0.0000
year.codedY2011 -0.1733713 0.1864744 74 -0.929732  0.3555
year.codedY2012 -0.3004284 0.2027411 74 -1.481833  0.1426
sexM             0.4403108 0.1507471 74  2.920857  0.0046
</code></pre>

<p>You were supposed to see that 2011-2012 is also not significant (not shown). </p>

<p>In traditional linear models I would report the p value for the whole treatment ""year"" from an ANOVA associated with a F test (i.e, just one p value), not the individual p values of the various year comparisons (3 p-values). But my understanding is that producing an ANOVA from <code>glmmPQL</code> is not possible (?). Reporting those three individual p values of my multiple year comparisons really seems clumsy and out of place...</p>

<p>What is the proper way to do this? Should I not be using <code>glmmPQL</code>?</p>

<p>Specification: I cut into the background description of the study for simplicity, but this is not a resource selection study - please do not suggest using resource selection models.</p>
"
"0.0399043442233811","0.0405220449236554"," 80172","<p>I performed a multivariate linear regression such that:</p>

<pre><code>fit&lt;-lm(as.matrix(y)~mwtkg+mbmi+mage,data=x)
</code></pre>

<p>where $y$ is a $500 \times 26$ multivariate outcomes. Then, I am wondering how to explain the <code>anova(fit)</code>:</p>

<pre><code>&gt; anova(fit)
Analysis of Variance Table

             Df  Pillai approx F num Df den Df    Pr(&gt;F)    
(Intercept)   1 0.99959    63064     25    651 &lt; 2.2e-16 ***
mwtkg         1 0.03506        1     25    651    0.5403    
mbmi          1 0.20862        7     25    651 &lt; 2.2e-16 ***
mage          1 0.09016        3     25    651 4.567e-05 ***
Residuals   675                                             
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>What do the three Dfs, Pillai, and P values mean for the model?</p>
"
"0.0977452818676612","0.099258333397093"," 81368","<p>I'm reproducing an example from <a href=""http://rads.stackoverflow.com/amzn/click/0470073713"">Generalized, Linear, and Mixed Models</a>. My MWE is below:</p>

<pre><code>Dilution &lt;- c(1/128, 1/64, 1/32, 1/16, 1/8, 1/4, 1/2, 1, 2, 4)
NoofPlates &lt;- rep(x=5, times=10)
NoPositive &lt;- c(0, 0, 2, 2, 3, 4, 5, 5, 5, 5)
Data &lt;- data.frame(Dilution,  NoofPlates, NoPositive)

fm1 &lt;- glm(formula=NoPositive/NoofPlates~log(Dilution), family=binomial(""logit""), data=Data)
summary(object=fm1)
</code></pre>

<hr>

<p><strong>Output</strong></p>

<hr>

<pre><code>Call:
glm(formula = NoPositive/NoofPlates ~ log(Dilution), family = binomial(""logit""), 
    data = Data)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-0.38326  -0.20019   0.00871   0.15607   0.48505  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)      4.174      2.800   1.491    0.136
log(Dilution)    1.623      1.022   1.587    0.112

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 8.24241  on 9  degrees of freedom
Residual deviance: 0.64658  on 8  degrees of freedom
AIC: 6.8563

Number of Fisher Scoring iterations: 6
</code></pre>

<hr>

<p><strong>Code</strong></p>

<hr>

<pre><code>anova(object=fm1, test=""Chisq"")
</code></pre>

<hr>

<p><strong>Output</strong></p>

<hr>

<pre><code>Analysis of Deviance Table

Model: binomial, link: logit

Response: NoPositive/NoofPlates

Terms added sequentially (first to last)


              Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)   
NULL                              9     8.2424            
log(Dilution)  1   7.5958         8     0.6466  0.00585 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<hr>

<p><strong>Code</strong></p>

<hr>

<pre><code>library(aod)
wald.test(b=coef(object=fm1), Sigma=vcov(object=fm1), Terms=2)
</code></pre>

<hr>

<p><strong>Output</strong></p>

<hr>

<pre><code>Wald test:
----------

Chi-squared test:
X2 = 2.5, df = 1, P(&gt; X2) = 0.11
</code></pre>

<p>Estimated coefficients are perfectly matching with the results given in the book but SE's are far apart. Based on LRT test the slope  is significant but based on Wald and Z-test slope coefficient is insignificant. I wonder if I miss something basic. Thanks in advance for your help.</p>
"
"0.126713311335625","0.128674774496326"," 81430","<p>I have a mixed model and the data looks like this:</p>

<pre><code>&gt; head(pce.ddply)
  subject Condition errorType     errors
1    j202         G         O 0.00000000
2    j202         G         P 0.00000000
3    j203         G         O 0.08333333
4    j203         G         P 0.00000000
5    j205         G         O 0.16666667
6    j205         G         P 0.00000000
</code></pre>

<p>Each subject provides two datapoints for errorType (O or P) and each subject is in either Condition G (N=30) or N (N=33).  errorType is a repeated variable and Condition is a between variable.  I'm interested in both main effects and the interactions.  So, first an anova:</p>

<pre><code>&gt; summary(aov(errors ~ Condition * errorType + Error(subject/(errorType)),
                 data = pce.ddply))

Error: subject
          Df  Sum Sq  Mean Sq F value Pr(&gt;F)
Condition  1 0.00507 0.005065   2.465  0.122
Residuals 61 0.12534 0.002055               

Error: subject:errorType
                    Df  Sum Sq Mean Sq F value   Pr(&gt;F)    
errorType            1 0.03199 0.03199   10.52 0.001919 ** 
Condition:errorType  1 0.04010 0.04010   13.19 0.000579 ***
Residuals           61 0.18552 0.00304                     
</code></pre>

<p>Condition is not significant, but errorType is, as well as the interaction.</p>

<p>However, when I use lmer, I get a totally different set of results:</p>

<pre><code>&gt; lmer(errors ~ Condition * errorType + (1 | subject),
                    data = pce.ddply)
Linear mixed model fit by REML 
Formula: errors ~ Condition * errorType + (1 | subject) 
   Data: pce.ddply 
    AIC    BIC logLik deviance REMLdev
 -356.6 -339.6  184.3     -399  -368.6
Random effects:
 Groups   Name        Variance Std.Dev.
 subject  (Intercept) 0.000000 0.000000
 Residual             0.002548 0.050477
Number of obs: 126, groups: subject, 63

Fixed effects:
                       Estimate Std. Error t value
(Intercept)            0.028030   0.009216   3.042
ConditionN             0.048416   0.012734   3.802
errorTypeP             0.005556   0.013033   0.426
ConditionN:errorTypeP -0.071442   0.018008  -3.967

Correlation of Fixed Effects:
            (Intr) CndtnN errrTP
ConditionN  -0.724              
errorTypeP  -0.707  0.512       
CndtnN:rrTP  0.512 -0.707 -0.724
</code></pre>

<p>So for lmer, Condition and the interaction are significant, but errorType is not.</p>

<p>Also, the lmer result is exactly the same as a glm result, leading me to believe something is wrong.</p>

<p>Can someone please help me understand why they are so different?  I suspect I am using lmer incorrectly (though I've tried many other versions like (errorType | subject) with similar results.</p>

<p>(I have seen researchers use both approaches in the literature with similar data.)</p>
"
"0.126713311335625","0.105279360951539"," 81612","<p>Recently I was trying to do logistic regression using the <code>rms::lrm()</code> function. But I had some trouble understanding the model objects from the function. Here is the example from the package:</p>

<pre><code>#dataset
n            &lt;- 1000    # define sample size
set.seed(17)            # so can reproduce the results
treat        &lt;- factor(sample(c('a','b','c'), n,TRUE))
num.diseases &lt;- sample(0:4, n,TRUE)
age          &lt;- rnorm(n, 50, 10)
cholesterol  &lt;- rnorm(n, 200, 25)
weight       &lt;- rnorm(n, 150, 20)
sex          &lt;- factor(sample(c('female','male'), n,TRUE))
L            &lt;- .1*(num.diseases-2) + .045*(age-50) +
                (log(cholesterol - 10)-5.2)*(-2*(treat=='a') +
                3.5*(treat=='b')+2*(treat=='c'))
y            &lt;- ifelse(runif(n) &lt; plogis(L), 1, 0)
#fit model
g            &lt;- lrm(y ~ treat*rcs(age))

&gt; g

Logistic Regression Model

lrm(formula = y ~ treat * rcs(age))

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          1000    LR chi2      76.77    R2       0.099    C       0.656    
 0            478    d.f.            14    g        0.665    Dxy     0.312    
 1            522    Pr(&gt; chi2) &lt;0.0001    gr       1.945    gamma   0.314    
max |deriv| 3e-06                          gp       0.156    tau-a   0.156    
                                           Brier    0.231    
&gt; anova(g)
                Wald Statistics          Response: y 

 Factor                                     Chi-Square d.f. P     
 treat  (Factor+Higher Order Factors)        5.62      10   0.8462
  All Interactions                           1.30       8   0.9956
 age  (Factor+Higher Order Factors)         65.99      12   &lt;.0001
  All Interactions                           1.30       8   0.9956
  Nonlinear (Factor+Higher Order Factors)    2.23       9   0.9872
 treat * age  (Factor+Higher Order Factors)  1.30       8   0.9956
  Nonlinear                                  0.99       6   0.9858
  Nonlinear Interaction : f(A,B) vs. AB      0.99       6   0.9858
 TOTAL NONLINEAR                             2.23       9   0.9872
 TOTAL NONLINEAR + INTERACTION               2.57      11   0.9953
 TOTAL                                      69.06      14   &lt;.0001
</code></pre>

<p><strong>Here are my questions:</strong><br>
For the object <code>g</code>,  </p>

<ul>
<li>What does the <code>max |deriv| 3e-06</code> mean?  </li>
<li>What do the Discrimination and Rand Discrim. Indexes suggest?  </li>
</ul>

<p>For the <code>anova(g)</code> object,  </p>

<ul>
<li>What's the <code>Factor +Higher Order Factors</code> for the treat?  </li>
<li>Why there are two <code>all interactions</code>? How to explain the nonlinear parts?</li>
</ul>
"
"0.287951884060726","0.27074930127818"," 82102","<p>I hope this is an appropriate forum to post this question. I recently upgraded my R software from 2.15.0 to 3.0.2. I also upgraded the lme4 package from .999999-0 to 1.1-2. After doing so, the results from one of my linear mixed models analyses have changed a bit unexpectedly. In some respects, I was expecting some change, as the lme4 developers very clearly stated that they had made some significant changes to some fundamental components in the package. However, the changes that I am seeing (described below) make me think that something else is awry. I will start by explaining the experimental design, which is quite simple and then the issue at hand.</p>

<p>My experiment is a basic repeated measures design. I used 24 ""Items"" that each appeared in three different ""Conditions"" (SmallClause_Som, NoSmallClause, SmallClause_NoSom). Levels of Condition were rotated across three presentation lists such that each Subject (45 total, each assigned to a particular list) only saw one level of each item.</p>

<p>I used lmer() for the analysis. Condition was entered in as a Fixed effect and ""Subject"" and ""Item"" were entered as Random effects.</p>

<p>The problem:
Using the current version of R 3.0.2 and lme4 1.1-2 with NoSmallClause as the reference level (and no weighting on any of the contrasts), the ConditionSmallClause_Som/NoSmallClause contrast produces a t value of 1.680. </p>

<p>But, when I change reference level to SmallClause_Som (to observe the one remaining contrast) I get not only a change in the polarity of the effect (plus to minus, as expected), but the values change as well.</p>

<p>When I use R 2.15.0 and lme4 .999999-0 (on another computer), I do not experience this issue. I get slightly different values, but they do not change (apart from the polarity) when I change reference level.</p>

<p>My colleague also tried my analysis for me using R 3.0.2 and a version of lme4 (pre version 1.0) (I don't know exactly which version, but it was before the major changes) and he also does not experience the issue.</p>

<p>R 2.15.0 lme4 1.1-2 (older) output:</p>

<pre><code>&gt; #set ref level
&gt; test$Condition &lt;- relevel(test$Condition, ref=""NoSmallClause"")

&gt; #Model 4: Random slopes by Subject and Item
&gt; test.lmer4=lmer(Rating~Condition+(1+Condition|Subject)+(1+Condition|Item),test, REML=FALSE)
&gt; summary(test.lmer4)

Linear mixed model fit by maximum likelihood 
Formula: Rating ~ Condition + (1 + Condition | Subject) + (1 + Condition |      Item) 
   Data: test 
 AIC  BIC logLik deviance REMLdev
 3930 4010  -1949     3898    3902
Random effects:
 Groups   Name                       Variance   Std.Dev. Corr          
 Subject  (Intercept)                0.98998765 0.994981               
          ConditionSmallClause_Som   0.00203374 0.045097 -1.000        
          ConditionSmallClause_NoSom 0.00019873 0.014097  1.000 -1.000 
 Item     (Intercept)                0.96231875 0.980978               
          ConditionSmallClause_Som   0.89924400 0.948285 -0.020        
          ConditionSmallClause_NoSom 0.62128577 0.788217 -0.256  0.361 
 Residual                            1.68810777 1.299272               
Number of obs: 1080, groups: Subject, 45; Item, 24

Fixed effects:
                       Estimate Std. Error t value
(Intercept)                  2.9583     0.2584  11.447
ConditionSmallClause_Som     0.3639     0.2165   1.680
ConditionSmallClause_NoSom   0.1472     0.1878   0.784

Correlation of Fixed Effects:
            (Intr) CnSC_S
CndtnSmlC_S -0.116       
CndtnSmC_NS -0.260  0.392

&gt; #set ref level
&gt; test$Condition &lt;- relevel(test$Condition, ref=""SmallClause_Som"")
&gt; 
&gt; #Model 4: Random slopes by Subject and Item
&gt; test.lmer4=lmer(Rating~Condition+(1+Condition|Subject)+(1+Condition|Item),test, REML=FALSE)
&gt; summary(test.lmer4)

Linear mixed model fit by maximum likelihood 
Formula: Rating ~ Condition + (1 + Condition | Subject) + (1 + Condition |      Item) 
   Data: test 
  AIC  BIC logLik deviance REMLdev
 3930 4010  -1949     3898    3902
Random effects:
 Groups   Name                       Variance  Std.Dev. Corr          
 Subject  (Intercept)                0.9023239 0.949907               
          ConditionNoSmallClause     0.0020340 0.045099 1.000         
          ConditionSmallClause_NoSom 0.0035039 0.059194 1.000  1.000  
 Item     (Intercept)                1.8238288 1.350492               
          ConditionNoSmallClause     0.8992237 0.948274 -0.687        
          ConditionSmallClause_NoSom 0.9804329 0.990168 -0.604  0.670 
 Residual                            1.6881050 1.299271               
Number of obs: 1080, groups: Subject, 45; Item, 24

Fixed effects:
                           Estimate Std. Error t value
(Intercept)                  3.3222     0.3174  10.468
ConditionNoSmallClause      -0.3639     0.2165  -1.680
ConditionSmallClause_NoSom  -0.2167     0.2243  -0.966

Correlation of Fixed Effects:
            (Intr) CndNSC
CndtnNSmllC -0.588       
CndtnSmC_NS -0.521  0.638
</code></pre>

<p>R 3.0.2 and lme4 1.1-2 (newer) output:</p>

<pre><code>&gt; #set ref level
&gt; test$Condition &lt;- relevel(test$Condition, ref=""NoSmallClause"")

&gt; #Model 4: Random slopes by Subject and Item
&gt; summary(test.lmer4)

Linear mixed model fit by maximum likelihood ['lmerMod']
Formula: Rating ~ Condition + (1 + Condition | Subject) + (1 + Condition |      Item) 
   Data: test 

      AIC       BIC    logLik  deviance 
 3942.557  4022.312 -1955.278  3910.557 

Random effects:
 Groups   Name                       Variance Std.Dev. Corr       
 Subject  (Intercept)                0.9522   0.9758              
          ConditionSmallClause_NoSom 0.1767   0.4204    0.03      
          ConditionSmallClause_Som   0.1760   0.4196   -0.15  0.92
 Item     (Intercept)                1.2830   1.1327              
          ConditionSmallClause_NoSom 0.7782   0.8822   -0.41      
          ConditionSmallClause_Som   1.4901   1.2207    0.09  0.41
 Residual                            1.6466   1.2832              
Number of obs: 1080, groups: Subject, 45; Item, 24

Fixed effects:
                           Estimate Std. Error t value
(Intercept)                  2.9583     0.2814  10.512
ConditionSmallClause_NoSom   0.1472     0.2133   0.690
ConditionSmallClause_Som     0.3639     0.2741   1.327

Correlation of Fixed Effects:
            (Intr) CSC_NS
CndtnSmC_NS -0.357       
CndtnSmlC_S -0.007  0.451
&gt; #anova (test.lmer3, test.lmer4)
&gt; 
&gt; #set ref level
&gt; test$Condition &lt;- relevel(test$Condition, ref=""SmallClause_Som"")
&gt; 
&gt; #Model 4: Random slopes by Subject and Item
&gt; test.lmer4=lmer(Rating~Condition+(1+Condition|Subject)+(1+Condition|Item),test, REML=FALSE)
&gt; summary(test.lmer4)
Linear mixed model fit by maximum likelihood ['lmerMod']
Formula: Rating ~ Condition + (1 + Condition | Subject) + (1 + Condition |      Item) 
   Data: test 

      AIC       BIC    logLik  deviance 
 3951.357  4031.113 -1959.679  3919.357 

Random effects:
 Groups   Name                       Variance Std.Dev. Corr       
 Subject  (Intercept)                0.88980  0.9433              
          ConditionNoSmallClause     0.04299  0.2073   0.83       
          ConditionSmallClause_NoSom 0.01562  0.1250   0.90  0.67 
 Item     (Intercept)                2.39736  1.5483              
          ConditionNoSmallClause     0.72053  0.8488   -0.04      
          ConditionSmallClause_NoSom 1.87804  1.3704   -0.16  0.53
 Residual                            1.65166  1.2852              
Number of obs: 1080, groups: Subject, 45; Item, 24

Fixed effects:
                           Estimate Std. Error t value
(Intercept)                  3.3222     0.3525   9.425
ConditionNoSmallClause      -0.3639     0.2004  -1.816
ConditionSmallClause_NoSom  -0.2167     0.2963  -0.731

Correlation of Fixed Effects:
            (Intr) CndNSC
CndtnNSmllC -0.045       
CndtnSmC_NS -0.160  0.514
</code></pre>

<p>My question:
What is going on here? Why is changing the reference level producing a shift from 1.327 to -1.816 in the t scores for the new version of lme4 whereas it produces the same (disregarding sign) value of 1.680/-1.680 in the old version's t scores? Only the older version seems to make sense to me.</p>

<p>1) Am I specifying my model incorrectly for the new version of lme4?</p>

<p>2) Am I missing some basic fundamental fact about how contrasts work? That is, is it possible to get different values just from changing the reference level? (the correlation values look a bit odd in the newer output).</p>

<p>3) Is this a bug in lme4?</p>

<p>4) Some other explanation...?</p>

<p>I have had some other odd issues as well with this same analysis using lme4 1.1-2. For example, if I don't clear the workspace and re-run an analysis, the values also will change between analyses (and also within the analysis as I change the reference level). This never happened to me on the earlier version (and it still does not happen when I run it on the earlier version now).</p>

<p>I hope someone can help with this. I found two other similar questions online (after much searching) but neither had any informative responses.</p>

<p>Thanks DT</p>
"
"0.05643326479831","0.0573068255061253"," 82391","<p>I got a warning message when I was trying to do anova for two nlme::gls objects. Here is an example:</p>

<pre><code>require(nlme)
set.seed(123)
y&lt;-rnorm(100,10,2)
x1&lt;-rnorm(100)
x2&lt;-sample(1:5,100,T)
x3&lt;-rt(100,20)
x4&lt;-rbinom(100,1,0.3)
fit1&lt;-gls(y~x1+x2+x3+x4,correlation=corAR1(form=~1|x4))
fit2&lt;-gls(y~x3+x4,correlation=corAR1(form=~1|x4))
anova(fit1,fit2)
     Model df      AIC      BIC    logLik   Test  L.Ratio p-value
fit1     1  7 421.5587 439.4359 -203.7794                        
fit2     2  5 413.9855 426.8590 -201.9928 1 vs 2 3.573242  0.1675
Warning message:
In nlme::anova.lme(object = fit1, fit2) :
  fitted objects with different fixed effects. REML comparisons are not meaningful.
</code></pre>

<p>Dose anybody know what the message means? Should I do the following rather than above?</p>

<pre><code>&gt; anova(fit1, L=c(-1, -1, 1, 1))
Denom. DF: 95 
 F-test for linear combination(s)
(Intercept)          x1          x2          x3 
         -1          -1           1           1 
  numDF  F-value p-value
1     1 281.9844  &lt;.0001
</code></pre>

<p>Why the numDF is one here? It seems 2?</p>
"
"0.173939003877979","0.167335104133733"," 83458","<p>My question is about the best way to estimate the effect of a predictor on a dependent variable, while accounting for several other predictors that may correlate with the predictor of interest. I'm using a linear mixed-effects model, using the <code>lmer</code> function from the R <code>lme4</code> package. (Warning: I'm fairly new at this, so their may be some misunderstandings woven through my question.)</p>

<h2>The problem</h2>

<p>To make things a bit more specific, I'll just explain the actual data that I'm working with. I have eye-movement data of participants freely viewing natural scenes. I want to determine whether pupil size predicts the 'visual saliency' (i.e. the conspicuity) of the locations in the image that participants are looking at. But there are many other things that correlate with pupil size, such as luminosity, and this makes the analysis tricky (or does it?).</p>

<h2>Option 1 (simple): Looking at fixed effects</h2>

<p>One option would be to simply create a linear mixed-effects model that has all predictors of saliency that I can think of, including the predictor of interest (<code>pupil_size</code>), as fixed effects and <code>subject</code> and <code>scene</code> as random effects. (To keep things manageable, I'm using a purely additive model, although I suppose that this is a whole topic in itself.)</p>

<pre><code>my_lmer = lmer(saliency ~ brightness + (.. lots of predictors ...)
    + pupil_size + (1|subject) + (1|scene))
</code></pre>

<p>This will give me a t-value for the fixed effect <code>pupil_size</code>. From what I understand, this fixed effect will already be partial, so it's the unique predictive power of pupil size, with any correlations between fixed effects already taken into account. Is my understanding correct?</p>

<h2>Option 2 (complex): Using model comparison</h2>

<p>An alternative approach, which I have from <a href=""http://www.sciencedirect.com/science/article/pii/S0749596X07001398"">Baayen et al. (2008)</a>, is to compare a model without pupil size as fixed effect (<code>simple_model</code>) to a model with pupil size as fixed effect (<code>complex_model</code>).</p>

<pre><code>simple_model = lmer(saliency ~ brightness + (.. lots of predictors ...)
    + (1|subject) + (1|scene))
complex_model = lmer(saliency ~ brightness + (.. lots of predictors ...)
    pupil_size + (1|subject) + (1|scene))
</code></pre>

<p>Now I can use the <code>anova</code> function to compare these two models (see Baayen's paper for an example). This will give me a <code>Chisq Chi</code> value, and I can use this to determine whether adding <code>pupil_size</code> as fixed effect is a justified addition to the model.</p>

<p>Clearly, this model comparison approach is more complex than simply looking at the t-values for fixed effects in a single model. And it seems to me that if <code>pupil_size</code> is a significant predictor (per Option 1), then it must also be a significant addition to the model (per Option 2).</p>

<p>In sum, my question is: <em>Is there any reason to do a model comparison (Option 2), or am I better off just creating a single linear mixed-effects model and seeing whether the t-value associated with <code>pupil_size</code> as fixed effect is sufficiently high (Option 1)?</em></p>
"
"0.211153942092367","0.206764560290146"," 86032","<p>I'm currently working with a data set that has numerous samples collected over time at different sites in a study area, and I'm interested in detecting a trend over time for that area.  I know that in an ideal experimental or balanced situation, using a random slope and intercept model is a great way to get at the overall trend within the study area.  With our data, however, many of the sites are missing samples and a handful of the sites only have one data point.</p>

<p>I'm curious if there's a way to intuitively understand how the sample imbalance will affect the estimate of the overall slope?  To put it differently, are there ways to know if sample imbalances are causing problems,  or are there things I can look for in my model output that would indicate I shouldn't trust what the model is estimating?</p>

<p>I created a contrived example with 20 data points to look at this. I put 10 data points with a slope of 1 into one site (a), and put the other 10 data points with a slope of -1 into unique sites (b through l).  I had assumed that when I looked at both a random intercept and random slope and intercept model that they would be somewhat similar, or that at least the latter would give more weight to the site with good data over time.</p>

<pre><code>&gt; library(lme4)
&gt; set.seed(9999)

&gt; x = c(0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9) + rnorm(20,mean=0,sd=0.1)
&gt; y = c(0,1,2,3,4,5,6,7,8,9,9,8,7,6,5,4,3,2,1,0) + rnorm(20,mean=0,sd=0.1)
&gt; z = c(rep('a',10),'b','c','d','e','f','h','i','j','k','l')
&gt; z = factor(z)

&gt; m0 = lm(y~x)
&gt; m1 = lmer(y~x+(1|z))
&gt; m2 = lmer(y~x+(1+x|z))

&gt; summary(m0)
&gt; summary(m1)
&gt; summary(m2)
&gt; anova(m1,m2)
</code></pre>

<p>As expected, the slope of the linear model was near zero, but the results for the two mixed effects models were nearly opposite.  Even though sites b through l only have one data point, it seems like they contribute more towards the slope because the trend is occurring over so many sites.  The random slope and intercept model was also preferred to using model selection criteria.</p>

<pre><code> &gt; summary(m0)$coefficients
                Estimate Std. Error    t value    Pr(&gt;|t|)
 (Intercept)  4.53784796  1.2586990  3.6051890 0.002023703
 x           -0.01178748  0.2335094 -0.0504797 0.960296079

 &gt; summary(m1)
 Linear mixed model fit by REML ['lmerMod']
 Formula: y ~ x + (1 | z) 

 REML criterion at convergence: 62.0877 

 Random effects:
  Groups   Name        Variance Std.Dev.
  z        (Intercept) 33.30788 5.7713  
  Residual              0.01583 0.1258  
 Number of obs: 20, groups: z, 11

 Fixed effects:
             Estimate Std. Error t value
 (Intercept) -0.03597    1.74163   -0.02
 x            0.99332    0.01386   71.66

 Correlation of Fixed Effects:
   (Intr)
 x -0.036

 &gt; summary(m2)
 Linear mixed model fit by REML ['lmerMod']
 Formula: y ~ x + (1 + x | z) 

 REML criterion at convergence: 31.0386 

 Random effects:
  Groups   Name        Variance Std.Dev. Corr 
  z        (Intercept) 7.78818  2.7907        
      x           0.37691  0.6139   -1.00
  Residual             0.01524  0.1234        
 Number of obs: 20, groups: z, 11

 Fixed effects:
             Estimate Std. Error t value
 (Intercept)   8.2121     0.8566   9.587
 x            -0.8201     0.1882  -4.358

 Correlation of Fixed Effects:
   (Intr)
 x -0.999

 &gt; anova(m1,m2)
 Data: 
 Models:
 m1: y ~ x + (1 | z)
 m2: y ~ x + (1 + x | z)
    Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
 m1  4 66.206 70.189 -29.103   58.206                             
 m2  6 36.745 42.719 -12.372   24.745 33.462      2  5.419e-08 ***
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I see that under this extreme example, the random slope and intercept have an almost perfect correlation.  Is what I can pull from this is that, in a sense, the model gives more value to the sites with only one data point because the overall trend is so strong but across multiple sites, but that I should view the slope estimate this model produces as suspect with such a high correlation?  Is there anything else that should look for?  For my specific study, I could also set some sort of criteria for what level of replication I thought was necessary to make proper inferences, e.g. eliminate all the sites that less than five samples.</p>

<p>Many thanks for your thoughts.</p>
"
"0.164529826154006","0.157248631892946"," 87487","<p><strong>Short version</strong></p>

<p>Is there a difference <strong>per treatment</strong> given time and this dataset?</p>

<p><strong>Or</strong> if the difference we're trying to demonstrate is important, what's the best method we have for teasing this out?</p>

<p><strong>Long version</strong></p>

<p>Ok, sorry if a bit <em>biology 101</em> but this appears to be an edge case where the data and the model need to line up in the right way in order to draw some conclusions. </p>

<p>Seems like a common issue... Would be nice to demonstrate an intuition rather than repeating this experiment with larger sample sizes. </p>

<p>Let's say I have this graph, showing mean +- std. error:</p>

<p><img src=""http://i.stack.imgur.com/eIKeF.png"" alt=""p1""></p>

<p>Now, it looks like there's a difference here. Can this be justified (avoiding Bayesian approaches)?</p>

<p>The simpleminded man's  approach would be to take Day 4 and apply a <em>t-test</em> (as usual: 2-sided, unpaired, unequal variance), but this doesn't work in this case. It appears the variance is too high as we only had 3x measurements per time-point (err.. mostly my design, p = 0.22).</p>

<p><strong>Edit</strong> On reflection the next obvious approach would be ANOVA on a linear regression. Overlooked this on first draft. This also doesn't seem like the right approach as the usual linear model is impaired from heteroskedasticity (<em>exaggerated variance over time</em>). <strong>End Edit</strong></p>

<p>I'm guessing there's a way to include <strong>all</strong> the data which would fit a simple (1-2 parameter) model of growth over time per predictor variable then compare these models using some formal test. </p>

<p>This method should be justifiable yet accessible to a relatively unsophisticated audience.</p>

<p>I have looked at <code>compareGrowthCurves</code> in <a href=""http://cran.r-project.org/web/packages/statmod/statmod.pdf"" rel=""nofollow"">statmod</a>, read about <a href=""http://www.jstatsoft.org/v33/i07/paper"" rel=""nofollow"">grofit</a> and tried a linear mixed-effects model adapted from <a href=""http://stats.stackexchange.com/questions/61153/nlme-regression-curve-comparison-in-r-anova-p-value"">this question on SE</a>. This latter is closest to the bill, although in my case the measurements are not from the <strong>same subject</strong> over time so I'm not sure mixed-effects/multilevel models are appropriate. </p>

<p>One sensible approach would be to model the rate of growth per time as linear and fixed and have the random effect be <strong>Tx</strong> then <a href=""http://www.statistik.uni-dortmund.de/useR-2008/slides/Scheipl+Greven+Kuechenhoff.pdf"" rel=""nofollow"">test it's significance</a>, although I gather there's <a href=""http://glmm.wikidot.com/faq"" rel=""nofollow"">some debate</a> about the merits of such an approach.</p>

<p>(Also this method specifies a linear model which would not appear to be the best way to model a comparison of growth which in the case of one predictor has not yet hit an upper boundary and in the other appears basically static. I'm guessing there's a generalized mixed-effects model approach to this difficulty which would be more appropriate.)</p>

<p>Now the code:</p>

<pre><code>df1 &lt;- data.frame(Day = rep(rep(0:4, each=3), 2),
              Tx = rep(c(""Control"", ""BHB""), each=15),
              y = c(rep(16e3, 3),
              32e3, 56e3, 6e3,
              36e3, 14e3, 24e3,
              90e3, 22e3, 18e3,
              246e3, 38e3, 82e3,
              rep(16e3, 3),
              16e3, 34e3, 16e3,
              20e3, 20e3, 24e3,
              4e3, 12e3, 16e3,
              20e3, 5e3, 12e3))
### standard error
stdErr &lt;- function(x) sqrt(var(x)) / sqrt(length(x))
library(plyr)
### summarise as mean and standard error to allow for plotting
df2 &lt;- ddply(df1, c(""Day"", ""Tx""), summarise,
             m1 = mean(y),
             se = stdErr(y) )
library(ggplot2)
### plot with position dodge
pd &lt;- position_dodge(.1)
ggplot(df2, aes(x=Day, y=m1, color=Tx)) +
 geom_errorbar(aes(ymin=m1-se, ymax=m1+se), width=.1, position=pd) +
 geom_line(position=pd) +
 geom_point(position=pd, size=3) +
 ylab(""No. cells / ml"")
</code></pre>

<p>Some formal tests:</p>

<pre><code>### t-test day 4
with(df1[df1$Day==4, ], t.test(y ~ Tx))
### anova
anova(lm(y ~ Tx + Day, df1))
### mixed effects model
library(nlme)
f1 &lt;- lme(y ~ Day, random = ~1|Tx, data=df1[df1$Day!=0, ])
library(RLRsim)
exactRLRT(f1)
</code></pre>

<p>this last giving</p>

<pre><code>    simulated finite sample distribution of RLRT.  (p-value based on 10000
    simulated values)

data:  
RLRT = 1.6722, p-value = 0.0465
</code></pre>

<p>By which I conclude that the probability of this data (or something more extreme), <em>given the null hypothesis that there is no influence of <strong>treatment</strong> on <strong>change over time</em></strong> is close to the elusive 0.05. </p>

<p>Again, sorry if this appears a bit basic but I feel a case like this could be used to illustrate the importance of modelling in avoiding further needless experimental repetition. </p>
"
"0.0977452818676612","0.099258333397093"," 87834","<p>I am doing linear mixed models using lme4 and this is the results of model comparison:</p>

<pre><code>&gt; anova(lmer5,lmer6,lmer32)

       Df   AIC   BIC logLik   Chisq Chi Df Pr(&gt;Chisq)    
lmer32  9 43172 43226 -21577                              
lmer6  21 43190 43315 -21574  6.3081     12     0.8998    
lmer5  26 43162 43317 -21555 37.9971      5  3.778e-07 ***
</code></pre>

<p>As you can see, the results show that one model is significantly better than the others and normally I will choose model with smallest logLik. However in this result, the logLik is negative. Do you think it is a good idea to choose model from logLik in this case, or should I choose it from AIC or BIC instead.</p>

<p>As no conclusion whether AIC is better than BIC, I am confused which one I should choose. What do you think?</p>
"
"0.106411584595683","0.121566134770966"," 87920","<p>I am doing linear mixed model using lme4. According to Winter (2013, <a href=""http://www.bodowinter.com/tutorial/bw_LME_tutorial2.pdf"" rel=""nofollow"">http://www.bodowinter.com/tutorial/bw_LME_tutorial2.pdf</a>), as the new version of R does not give p-values due to inconclusion of degree of freedom, p-values of mixed models can be derived from model comparison. From the examples on page 12, he suggested to construct the null model:</p>

<pre><code>politeness.null=lmer (frequency ~ gender + (1|subject) + (1|scenario), data=politeness, REML=FALSE)
</code></pre>

<p>Then add the fixed effect that we are interested in:</p>

<pre><code>politeness.null=lmer (frequency ~ attitude + gender + (1|subject) + (1|scenario), data=politeness, REML=FALSE)
</code></pre>

<p>And then the p-values of attitude can be given from </p>

<pre><code>anova(politeness.null,politeness.model)
</code></pre>

<p>However, in my case, I have 3-way interaction: color*sex*food, and when I run the model I have 17 layers of fixed effects, such as white (compare to red), blue (compared to red), white:male(compared to female), etc.</p>

<p>Then my question is how I can get p-values for all these fixed effects? I am not sure if I should have some fixed effects that I am not interested in first:</p>

<pre><code>lmer1=lmer (duration ~ action + (1|subject) + (1|repetition), data=data.frame, REML=FALSE) 
</code></pre>

<p>Then add:</p>

<pre><code>lmer1=lmer (duration ~ action + color + (1|subject) + (1|repetition), data=data.frame, REML=FALSE)  
</code></pre>

<p>Then add:</p>

<pre><code>lmer1=lmer (duration ~ action + color + sex + (1|subject) + (1|repetition), data=data.frame, REML=FALSE) 
</code></pre>

<p>or </p>

<pre><code>lmer1=lmer (duration ~ action + color*sex + (1|subject) + (1|repetition), data=data.frame, REML=FALSE)
</code></pre>

<p>or</p>

<pre><code>lmer1=lmer (duration ~ action + sex + (1|subject) + (1|repetition), data=data.frame, REML=FALSE)
</code></pre>

<p>Anyone can help me about this? or is there any other ways to get p-values easier than model comparison?</p>
"
"0.0892288262810312","0.0906100470365937"," 88252","<p>I'm trying to analyse some temperature data, collected at 49 sites over a three month period. The data are in the form of a maximum reading at each site on each day. I'm interested in how temperature differs between sites, and want to generate predicted temperature values for days when I do not have readings at a given site.</p>

<p>I've run a two-way ANOVA with Date and Site as explanatory factors;</p>

<pre><code>Max.Daily.T &lt;- aov(MaxDailyT~Site+Date, data=iB)
</code></pre>

<p>but the data have non-constant variance and non-normal errors, so I've switched to GLM;</p>

<pre><code>Max.Daily.T.glm &lt;- glm(MaxDailyT~ Site+Date, data=iB)
</code></pre>

<p>If I leave date in the form ""dd/mm/yy"", R outputs the levels in alphabetical order. If I code the dates so R prints them in order, the estimates differ (it produces an estimate for my first day, 25/3/13, rather than the first alphabetical day, 1/4/13), but by the same amount (6.27 degrees).</p>

<p>This suggests to me that R is treating 'Date' (or coded Date) as a factor with completely independent effects of each level. Of course, in reality the temperature at a site on one day will go a long way to predicting the temperature there the following day. Does anyone know how to incorporate this into the model? My only thought is to use coded Date as a continuous variable, but I wonder whether this causes problems as the temperatures fluctuate rather than being a linear relationship.</p>

<p>Thanks in advance,
Andrew</p>
"
"0.0892288262810312","0.072488037629275"," 91700","<p>I am trying to understand the steps behind the linear regression process. I already have a linear model like:</p>

<p><code>lmodel1 &lt;- lm(y~x1+x2+x3, data=dataset)</code></p>

<p>for which R calculates several different things (<code>Coefficients, Intercept, Residuals, F-statistic</code> and <code>p-value</code>) among  others.</p>

<p>At this point, i am mostly interested in <code>F-statistic</code> and <code>p-value</code>.
So far, i have concluded to the following:</p>

<p>The process is iterative and begins taking under consideration every variable. In order to achieve an optimal <code>y</code> some <code>x</code> variables have to be ""taken out"". This comes as a result of calculating F-statistic, which quantifies the importance of each <code>xi</code> and the dependent variable <code>y</code>.
When <code>F value</code> is smaller than <code>p-value</code>(?) that variable is removed.
Next step of the process is to compare that <code>F-statistic</code> of a <code>xi</code> independent variable, with an <code>F-to-enter</code> and <code>F-to-remove</code> in order see if the removed variable will be re-inserted to the equation.(?)</p>

<p>Now, please do correct me if i am wrong regarding the steps desribed above.
Is that what happens under <code>lm()</code>'s hood. Are those the right variables.?</p>

<p>R-wise speaking how does these values can be shown, inserted or calculated in a multiple linear regression model.? How is <code>ANOVA</code> related to the above?</p>

<p>I am afraid R's <code>summary</code> and <code>help</code>  take too much for granted.</p>

<p>Thanks in advance for any suggestions.</p>
"
"0.106411584595683","0.108058786463081"," 91848","<p>I need a little bit of help and confirmation that I have the right idea. 
I have some fake data of 8 tribes; within each tribe members work hard to gain food for their own tribe. No one can speak to these tribes, but people suspect that each tribe has one of the two strategies presented below for gaining food:</p>

<ol>
<li><p>Members of a tribe who travel farther away from the tribe's main location are given more food, so they face less of a chance of starving before coming back.</p></li>
<li><p>Members of a tribe who travel far are given less food; that way if they are lost, there is less of a food loss to the tribe as a whole.</p></li>
</ol>

<p>The variables (columns) I am working with are <code>Tribe number</code>, <code>Distance from the tribe location</code> when sample was taken (10, 20, 30, or 40 miles), <code>Weight of each member</code> that we are studying (related to the amount of food is given), <code>height</code> (taller people use energy more efficiently, and there is a strong positive correlation between weight and height in arbitrary units and inches), finally I have each observation categorized by height (group <code>1</code>: 56â€“62in, group <code>2</code>: 62â€“64...).</p>

<p>I want to find out if the tribes use different strategies, and also if there is a difference among the classes <code>pf</code> height. In addition I want to find out the strategies that are in use. I am having a hard time with how to classify each tribe as using either strategy <code>1</code> or <code>2</code>. I was thinking of doing a one-way ANOVA to check if there is a difference in <code>mean</code> within each group based on <code>distance</code>. (In a particular tribe is there a difference in the mean of weight between those who were 10, 20 , 30, or 40 miles?) I don't know how to figure out if each colony uses a different strategy.</p>

<p>Finally, I want to build a linear model of mass on colony, distance, and height. I know how to build a model and run diagnostics. My concern here is, can I use distance as a categorical variable since its values are 10, 20, 30, or 40 miles? </p>
"
"0.05643326479831","0.0573068255061253"," 93007","<p>I ran an experiment with an eye tracker and my data frame has this look:</p>

<pre><code>               Condition   DWellsAOI1 DwellsAOI2  TotalDwells 
Participant1       1             12         13            25
Participant2       2            100         11           111
Participant3       1             50         50           100
</code></pre>

<p>and so on. <code>DWellsAOI1</code> counts the number of dwells on AOI1. Each participant belongs to one condition only, and the duration of the experiment is not fixed, so different <code>TotalDwells</code>.</p>

<p>I was trying to check if there is a significant difference among the between conditions in terms of <code>DwellsAOI1</code>, and my first approach was to compute the percentages (<code>DwellsAOI1/TotalDwells</code>) and run anova. However, data violates both the assumptions.</p>

<p>I searched on Google and I found that I can use a generalized linear model (GLM) with binomial family.</p>

<p>Currently I'm running it like this:</p>

<pre><code>mod &lt;- glm(cbind(DwellsAOI1,TotalDwells-DwellsAOI1) ~ Condition,
               data=df, family=binomial(""logit""))
summary(aov(mod))
</code></pre>

<p>Is it the correct way?</p>

<p>Thanks!</p>
"
"0.19966946028644","0.202760250721483"," 93601","<p>I am a complete novice and dummy when it comes to statistics so I apologise in advance...</p>

<p>I have been asked to report the results of my GLMMs (I ran two) in a table. This table must state: effect, standard error, test statistic, and P value, for all fixed effects. </p>

<p>Unfortunately I am struggling to read my output. </p>

<p>The out put is as follows, if anyone would be kind enough to help I would be very grateful and will know for future reference which bit equates to what (also I have been told my degrees of freedom are different for both the tests, could someone explain why this is?).</p>

<pre><code>GLMM 1-run for predictors of step length. 
Response variable = step length. 
fixed effects = depth and direction threshold. 
random factor = individual

Models:
m2: step ~ (1 | ind)
m1: step ~ Depth * threshold + (1 | ind)
Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) 
m2 3 373235 373259 -186615 373229 
m1 8 373225 373290 -186605 373209 19.767 5 0.001382 **
---
Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>.</p>

<pre><code>GLMM 2 -run to investigate potential predictors of PDBA.
response variables = depth and step length. 
fixed effect = direction threshold.
random factor = Individual

Models:
m3: PDBA ~ Depth + (1 | ind) + thresholdepth
m2: PDBA ~ step * threshold + Depth * threshold + (1 | ind)
Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) 
m3 6 -48205 -48157 24109 -48217 
m2 11 -48430 -48341 24226 -48452 235.1 5 &lt; 2.2e-16 ***
---
Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Models:
m4: PDBA ~ step + (1 | ind) + step:threshold
m2: PDBA ~ step * threshold + Depth * threshold + (1 | ind)
Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) 
m4 6 -48206 -48158 24109 -48218 
m2 11 -48430 -48341 24226 -48452 233.81 5 &lt; 2.2e-16 ***
---
Signif. codes: 0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Hi, I think the package I used was was lme4? </p>

<p>I have run a summary for the first GLMM and this is what I got, I have no idea which parts are relevant though, I assume it doesn't all go in a table?! </p>

<pre><code>&gt; summary(m1)
Linear mixed model fit by REML ['lmerMod']
Formula: step ~ Depth * threshold + (1 | ind) 

REML criterion at convergence: 373184 

Random effects:
 Groups   Name        Variance Std.Dev.
 ind      (Intercept) 196519   443.3   
 Residual             469370   685.1   
Number of obs: 23473, groups: ind, 11

Fixed effects:
                  Estimate Std. Error t value
(Intercept)      160.95895  134.80279   1.194
Depth              0.06438    0.44777   0.144
threshold2        51.18065   17.62222   2.904
threshold3         1.47733   21.43879   0.069
Depth:threshold2  -1.23654    0.60029  -2.060
Depth:threshold3  -0.09587    0.65088  -0.147

Correlation of Fixed Effects:
            (Intr) Depth  thrsh2 thrsh3 Dpth:2
Depth       -0.094                            
threshold2  -0.090  0.712                     
threshold3  -0.075  0.588  0.567              
Dpth:thrsh2  0.071 -0.737 -0.745 -0.435       
Dpth:thrsh3  0.064 -0.674 -0.490 -0.857  0.502
</code></pre>

<p>For GLMM 1 I ran this code -</p>

<pre><code>m1&lt;-lmer(step~Depth*threshold+(1|ind))
m2&lt;-lmer(step~(1|ind))
anova(m1,m2)
</code></pre>

<p>For GLMM 2 I ran this code -</p>

<pre><code>m2&lt;-lmer(PDBA~step*threshold+Depth*threshold+(1|ind))
m3&lt;-update(m2,~.-step*threshold)
anova(m2,m3)
</code></pre>

<p>and this one:</p>

<pre><code>m2&lt;-lmer(PDBA~step*threshold+Depth*threshold+(1|ind))
m4&lt;-update(m2,~.-Depth*threshold)
anova(m2,m4)
</code></pre>

<p>The output from the Anova only gives me one p value for each GLMM and I think I need a p value for each of the fixed effects within the models?</p>

<p>Does anyone know what code I can run to get this?
Thank you</p>
"
"0.143877159211166","0.134865517623595"," 93892","<p>I need to get p values for the fixed effects in the following GLMM's I ran. Does anyone know of code that I can run that will give me the p values I need? At the moment the output from the ANOVA only gives me one p value and I believe I need a separate p value for each of the fixed effects in the models. </p>

<p>Thanks in advance.
Code is as follows -</p>

<p>For GLMM 1 I ran this code -</p>

<pre><code>m1&lt;-lmer(step~Depth*threshold+(1|ind))
m2&lt;-lmer(step~(1|ind))
anova(m1,m2)
</code></pre>

<p>For GLMM 2 I ran this code -</p>

<pre><code>m2&lt;-lmer(PDBA~step*threshold+Depth*threshold+(1|ind))
m3&lt;-update(m2,~.-step*threshold)
anova(m2,m3)
</code></pre>

<p>and this one:</p>

<pre><code>m2&lt;-lmer(PDBA~step*threshold+Depth*threshold+(1|ind))
m4&lt;-update(m2,~.-Depth*threshold)
anova(m2,m4)
</code></pre>

<p>When I ran GLMM 1 code this is what I got:</p>

<pre><code>m2: step ~ (1 | ind)
m1: step ~ Depth * threshold + (1 | ind)
Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)   
m2  3 373235 373259 -186615   373229                            
m1  8 373225 373290 -186605   373209 19.767      5   0.001382 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>summary</p>

<pre><code>&gt; summary(m1)
Linear mixed model fit by REML ['lmerMod']
Formula: step ~ Depth * threshold + (1 | ind) 

REML criterion at convergence: 373184 

Random effects:
 Groups   Name        Variance Std.Dev.
 ind      (Intercept) 196519   443.3   
 Residual             469370   685.1   
Number of obs: 23473, groups: ind, 11

Fixed effects:
                 Estimate Std. Error t value
  (Intercept)      160.95895  134.80279   1.194
Depth              0.06438    0.44777   0.144
threshold2        51.18065   17.62222   2.904
threshold3         1.47733   21.43879   0.069
Depth:threshold2  -1.23654    0.60029  -2.060
Depth:threshold3  -0.09587    0.65088  -0.147

Correlation of Fixed Effects:
            (Intr) Depth  thrsh2 thrsh3 Dpth:2
Depth       -0.094                            
threshold2  -0.090  0.712                     
threshold3  -0.075  0.588  0.567              
Dpth:thrsh2  0.071 -0.737 -0.745 -0.435       
Dpth:thrsh3  0.064 -0.674 -0.490 -0.857  0.502
</code></pre>

<p>OUTPUT FROM SUGGESTED CODE BY SETH (IN COMMENTS)</p>

<pre><code>Models:
m6: step ~ Depth + threshold + (1 | ind)
m5: step ~ Depth + threshold + Depth:threshold + (1 | ind)
   Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)  
m6  6 373227 373275 -186607   373215                           
m5  8 373225 373290 -186605   373209 5.2901      2      0.071 .
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.211279591578102","0.221948380809238"," 94057","<p>I have an agricultural field experiment (testing a plant protection agent):</p>

<p><strong>Split plot design</strong> with: </p>

<pre><code>2 whole plot treatments ""infestation"": ""high"" &amp; ""low"" 
8 split-plot treatments (""treat""): 

1. Untreated Control (""Ctrl1"")
2. Reference Product (""Ctrl2"")
3. 1 x Test-Product 1
4. 2 x Test-Product 1
5. 3 x Test-Product 1
6. 1 x Test-Product 2
7. 2 x Test-Product 2
8. 3 x Test-Product 2

and 4 replicates (""block""):
</code></pre>

<p>The parameter of interest in this example is grain (<strong>yield</strong>):</p>

<p>First, I could model this:</p>

<pre><code>lme(yield ~ infestation * treat, random = ~ 1 | block/infestation, data)
</code></pre>

<p>or</p>

<pre><code> lmer(yield ~ infestation * treat + (1 | block/infestation), data)
</code></pre>

<p>But as can be seen treatments 3-8 can and have to be recoded as 2 products (""prod"") being tested 1-3 times (""times""), so I have a 2x3 subdesign.</p>

<p>One possibility would be subsetting the data: </p>

<pre><code>  data2 &lt;- subset(data, !data$treat == ""Ctrl1"" &amp;  !data$treat == ""Ctrl2"")
</code></pre>

<p>and recode the resting treatments to ""prod"" = 1,2 and ""times"" = 1:3
then run:</p>

<pre><code>lme(yield ~ infestation * form * times, random = ~ 1 | block/infestation, data)
</code></pre>

<p>Afterwards I could still do contrasts to compare the control treatments with the treated ones. </p>

<p>But (here my actual problem starts): I read an article of</p>

<p><strong>H.P. Piepho</strong>: ""<em>A Note on the Analysis of Designed Experiments with Complex Treatment Structure</em>"", 
HortScience 41(2):446--452. 2006</p>

<p>The author wants to show ""<em>how a meaningful analysis can be obtained based on a linear model by appropiate coidng of factors. (...) Our main objective is to demonstrate that the introduction of dummy variables can conveniently solve a wide variety of inferential problems that would otherwise either require ... multiple linear contrasts... or not make fully eficient use of the data, e.g when only data from orthogonal subdesigns are analysed.</em>""  </p>

<p>A very similar example (Example 1 in the article) is discussed within, and an alternative analysis in SAS is proposed - which I wanted to try to realise in R. </p>

<p>The author adds a dummy variable (<strong>ctrl_vs_trt</strong>) to the data and codes it: ""control"", ""trt"" (in my case <strong>trt</strong>, <strong>Ctrl1</strong>, <strong>Ctrl2</strong>"". </p>

<p>The he uses: 
(in his case <strong>prod</strong> is <strong>form</strong> ulation, and <strong>times</strong> is <strong>conc</strong> entration)</p>

<pre><code>PROC GLM;
CLASS block contr_vs_trt form conc;   ## 
MODEL set = block contr_vs_trt
        contr_vs_trt * form
    contr_vs_trt * conc
    contr_vs_trt * form * conc;
RUN.                    
</code></pre>

<p>I cite a further paragraph: 
""<em>Of course, a test for contr_vs_trt is not produced with this model, and one cannot compute simple means or marginal means. Also, the Type I SS for <strong>form</strong>, <strong>conc</strong>, and <strong>form x conc</strong> are not the same as with Type III SS. With Type III SS, the test for form is adjusted for <strong>conc</strong>, as fitting <strong>conc</strong> takes out the control when coding factors as in Table ""xy"" (as I did here). Similarly, the test for <strong>conc</strong> is adjusted for <strong>form</strong>, because fitting of <strong>form</strong> takes out the control. As a result, the Type III ANOVA for the model <strong>form x conc</strong> turns out to be that for the 3x2 factorial subdesign. (...)
It seems much more stringent and transparent to use the nested model <strong>contr_vs_trt/(form x conc)</strong>, as this properly reflects all nesting and crossing features of the design.</em>""</p>

<p>Now, how to do that in <code>lme</code> or <code>lmer</code>?</p>

<p>lme does not run at all, even if I simplify to: </p>

<pre><code> lme(yield ~ prod * times, random = ~1|block, data), 
 I get
 Error in MEEM(object, conLin, control$niterEM) : 
 Singularity in backsolve at level 0, block 1
</code></pre>

<p>The term <strong>prod * times</strong> cannot be run (<strong>prod + times</strong> logically can). Eliminating both controls from the data set resolves this problem. </p>

<p><code>lmer</code> runs with <strong>prod * times</strong>, but always given the message:</p>

<pre><code> fixed-effect model matrix is rank deficient so dropping ""x"" columns / coefficients
</code></pre>

<p>I understand that the subdesign is not orthogonal and therefor dropping is occuring, but I cannot say if the analysis after dropping can still be right. </p>

<p>Also, I do not know how to specify the full model (leaving out the ""infestation"" whole plot for a second):</p>

<pre><code>lmer(yield ~ prod * times + (1|block/ctr_vs_trt), data)
</code></pre>

<p><strong>prod * times</strong> is nested inside <strong>ctr_vs_trt</strong> but both are nested inside the same block (or whole plot).
Is nesting of fixed effects possible in <code>lme</code> or <code>lmer</code> - does it work as I proposed?
Does it even make sense to run the full model?</p>

<p>With <code>aov()</code> I get the model running, even the partitioning of Df's is right. But due to strong non-orthogonality it is not possible to assume that the results are right.</p>

<p>I can get meaningful results subsetting and using contrasts, but I found the authors approach interesting and it would help in the analysis of some of my other trials. 
Thanks in advance for any help; I hope this question is not too long...</p>
"
"0.16929979439493","0.162369338934022"," 94888","<p>I'm analysing some behavioural data using <code>lme4</code> in <code>R</code>, mostly following <a href=""http://www.bodowinter.com/tutorials.html"" rel=""nofollow"">Bodo Winter's excellent tutorials</a>, but I don't understand if I'm handling interactions properly. Worse, no-one else involved in this research uses mixed models, so I'm a bit adrift when it comes to making sure things are right.</p>

<p>Rather than just post a cry for help, I thought I should make my best effort at interpreting the problem, and then beg your collective corrections. A few other asides are:</p>

<ul>
<li>While writing, I've found <a href=""http://stackoverflow.com/questions/17794729/test-for-significance-of-interaction-in-linear-mixed-models-in-nlme-in-r"">this question</a>, showing that <code>nlme</code> more directly give p values for interaction terms, but I think it's still valid to ask with relation to <code>lme4</code>.</li>
<li><code>Livius'</code> answer to <a href=""http://stats.stackexchange.com/questions/63464/is-this-an-acceptable-way-to-analyse-mixed-effect-models-with-lme4-in-r"">this question</a> provided links to a lot of additional reading, which I'll be trying to get through in the next few days, so I'll comment with any progress that brings.</li>
</ul>

<hr>

<p>In my data, I have a dependent variable <code>dv</code>, a <code>condition</code> manipulation (0 = control, 1 = experimental condition, which should result in a higher <code>dv</code>), and also a prerequisite, labelled <code>appropriate</code>: trials coded <code>1</code> for this should show the effect, but trials coded <code>0</code> might not, because a crucial factor is missing.</p>

<p>I have also included two random intercepts, for <code>subject</code>, and for <code>target</code>, reflecting correlated <code>dv</code> values within each subject, and within each of the 14 problems solved (each participant solved both a control and an experimental version of each problem).</p>

<pre><code>library(lme4)
data = read.csv(""data.csv"")

null_model        = lmer(dv ~ (1 | subject) + (1 | target), data = data)
mainfx_model      = lmer(dv ~ condition + appropriate + (1 | subject) + (1 | target),
                         data = data)
interaction_model = lmer(dv ~ condition + appropriate + condition*appropriate +
                              (1 | subject) + (1 | target), data = data)
summary(interaction_model)
</code></pre>

<p>Output:</p>

<pre><code>## Linear mixed model fit by REML ['lmerMod']
## ...excluded for brevity....
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  subject  (Intercept) 0.006594 0.0812  
##  target   (Intercept) 0.000557 0.0236  
##  Residual             0.210172 0.4584  
## Number of obs: 690, groups: subject, 38; target, 14
## 
## Fixed effects:
##                                Estimate Std. Error t value
## (Intercept)                    0.2518     0.0501    5.03
## conditioncontrol               0.0579     0.0588    0.98
## appropriate                   -0.0358     0.0595   -0.60
## conditioncontrol:appropriate  -0.1553     0.0740   -2.10
## 
## Correlation of Fixed Effects:
## ...excluded for brevity.
</code></pre>

<p>ANOVA then shows <code>interaction_model</code> to be a significantly better fit than <code>mainfx_model</code>, from which I conclude that there's a significant interaction present (p = .035).</p>

<pre><code>anova(mainfx_model, interaction_model)
</code></pre>

<p>Output:</p>

<pre><code>## ...excluded for brevity....
##                   Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq)  
## mainfx_model       6 913 940   -450      901                          
## interaction_model  7 910 942   -448      896  4.44      1      0.035 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</code></pre>

<p>From there, I isolate a subset of the data for which the <code>appropriate</code> requirement is met (i.e., <code>appropriate = 1</code>), and for it fit a null model, and a model including <code>condition</code> as an effect, compare the two models using ANOVA again, and lo, find that <code>condition</code> is a significant predictor.</p>

<pre><code>good_data = data[data$appropriate == 1, ]
good_null_model   = lmer(dv ~ (1 | subject) + (1 | target), data = good_data)
good_mainfx_model = lmer(dv ~ condition + (1 | subject) + (1 | target), data = good_data)

anova(good_null_model, good_mainfx_model)
</code></pre>

<p>Output:</p>

<pre><code>## Data: good_data
## models:
## good_null_model: dv ~ (1 | subject) + (1 | target)
## good_mainfx_model: dv ~ condition + (1 | subject) + (1 | target)
##                   Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq)  
## good_null_model    4 491 507   -241      483                          
## good_mainfx_model  5 487 507   -238      477  5.55      1      0.018 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</code></pre>
"
"0.10828451233157","0.134396418749691"," 96195","<p>I have run a split splot model in SPSS (via repeated measures function) and I would like to reproduce my results using R. To do so I used ezANOVA function from ez package to obtain sphericity tests and correction and type III SS. I have read that aov function does not give type III SS. So far the results are identical with those from SPSS but I cannot tell so when I run tests of within subjects contrasts. I use polynomial contrasts and my factor has six levels. What I manage to get is t-values instead of f-values or f-values but not the same as in SPSS. Here is my data and code
<code>id    subj  treat  m1    m2    m3    m4    m5   m6
1       1     1   455   460   510   504   436   466
2       2     1   467   565   610   596   542   587
3       3     1   445   530   580   597   582   619
4       4     1   485   542   594   583   611   612
5       5     1   480   500   550   528   562   576
6       6     2   514   560   565   524   552   597
7       7     2   440   480   536   484   567   569
8       8     2   495   570   569   585   576   677
9       9     2   520   590   610   637   671   702
10     10     2   503   555   591   605   649   675
11     11     3   496   560   622   622   632   670
12     12     3   498   540   589   557   568   609
13     13     3   478   510   568   555   576   605
14     14     3   545   565   580   601   633   649
15     15     3   472   498   540   524   532   583</code></p>

<p>m1-m6 represent measurements in 6 different time points. My code for the contrasts.</p>

<p><code>long.df&lt;- melt(data, id=c('subj','treat'))
 long.df&lt;- long.df[order(long.df$subj)]
 names(long.df)&lt;- c('subj','treat','time','meas')
 mod&lt;- lm(meas~time + time^2 + time^3 + time^4 + time^5, long.df)
 summary(mod) # this is how I obtain t-values for the polynomial contrasts #
 mod1&lt;- aov(meas~time + time^2 + time^3 + time^4 + time^5, long.df)
 summary(mod1, split=list(time=list(""Linear""=1, ""Quad""=2,'q'=3,'f'=4,'fif'=5))) # this is how i obtain f-values #</code></p>

<p>I think that the F-values correspond to type 1 SS (as I use aov function). How am I wrong? Is there a way to conduct the Tests of within subjects contrasts and obtain type III SS so I will have identical results to SPSS? Here are the SPSS results. Many thanks in advance!</p>

<p>Source  time    Type III SS/    df/      Mean Square/       F/   Sig.<br>
time    Linear  123662.881/ 1/  123662.881/ 83.591/ .000<br>
     Quadratic  5928.007/   1/  5928.007/   18.968/ .001<br>
    Cubic   10462.676/  1/  10462.676/  28.075/ .000<br>
    Order 4 798.193  /       1/       798.193/  4.010/  .068<br>
    Order 5 1702.743/   1/  1702.743/   4.878/  .047    </p>
"
"0.115193919396023","0.140372481268719"," 96393","<p>I have run a split splot model in SPSS (via repeated measures function) and I would like to reproduce my results using R. To do so I used ezANOVA function from ez package to obtain sphericity tests and correction and type III SS. I have read that aov function does not give type III SS. So far the results are identical with those from SPSS but I cannot tell so when I run tests of within subjects contrasts. I use polynomial contrasts and my factor has six levels. What I manage to get is t-values instead of f-values or f-values but not the same as in SPSS. Here is my data and code</p>

<pre><code>id    subj  treat  m1    m2    m3    m4    m5   m6
1       1     1   455   460   510   504   436   466
2       2     1   467   565   610   596   542   587
3       3     1   445   530   580   597   582   619
4       4     1   485   542   594   583   611   612
5       5     1   480   500   550   528   562   576
6       6     2   514   560   565   524   552   597
7       7     2   440   480   536   484   567   569
8       8     2   495   570   569   585   576   677
9       9     2   520   590   610   637   671   702
10     10     2   503   555   591   605   649   675
11     11     3   496   560   622   622   632   670
12     12     3   498   540   589   557   568   609
13     13     3   478   510   568   555   576   605
14     14     3   545   565   580   601   633   649
15     15     3   472   498   540   524   532   583
</code></pre>

<p>m1-m6 represent measurements in 6 different time points. My code for the contrasts.</p>

<pre><code>long.df&lt;- melt(data, id=c('subj','treat'))
long.df&lt;- long.df[order(long.df$subj)]
names(long.df)&lt;- c('subj','treat','time','meas')
mod&lt;- lm(meas~time + time^2 + time^3 + time^4 + time^5, long.df)
summary(mod) # this is how I obtain t-values for the polynomial contrasts #
mod1&lt;- aov(meas~time + time^2 + time^3 + time^4 + time^5, long.df)
summary(mod1, split=list(time=list(""Linear""=1, ""Quad""=2,'q'=3,'f'=4,'fif'=5))) 
# this is how i obtain f-values #
</code></pre>

<p>I think that the F-values correspond to type 1 SS (as I use aov function). How am I wrong? Is there a way to conduct the Tests of within subjects contrasts and obtain type III SS so I will have identical results to SPSS? Here are the SPSS results. Many thanks in advance!</p>

<pre><code>Source     time    Type III SS/   df/    Mean Square/        F/   Sig.  
time     Linear     123662.881/    1/     123662.881/   83.591/   .000  
      Quadratic       5928.007/    1/       5928.007/   18.968/   .001  
          Cubic      10462.676/    1/      10462.676/   28.075/   .000  
        Order 4        798.193/    1/        798.193/    4.010/   .068  
        Order 5       1702.743/    1/       1702.743/    4.878/   .047
</code></pre>
"
"0.159617376893524","0.151957668463708"," 99118","<p>Having read through a few posts, I still couldn't find an answer to my question. </p>

<p>I'm trying to investigate for the effect of covariate C on a longitudinal dataset. I have two linear mixed effect models given below:</p>

<p><code>A.lme &lt;- lme( A ~ B + C, data = data1, random = ~ 1 | id)</code></p>

<p><code>B.lme &lt;- lme( A ~ B*C, data = data1, random = ~ 1 | id)</code></p>

<p>I just want to be sure that I'm interpreting these two the right way. I believe in order to investigate for covariate C, I should be analysing <code>B.lme</code>. </p>

<p>B represents time whilst A represents immune cells in the body whilst C represents a viral infection status.</p>

<p>The summary and anova for B.lme suggests that C has no significant effect on both intercept and slope as given below:</p>

<pre><code>&gt;summary(B.lme)
Linear mixed-effects model fit by REML
Data: data1 
   AIC      BIC    logLik
4238.806 4270.106 -2113.403

Random effects:
Formula: ~1 | id
         (Intercept)  Residual
StdDev:   0.9242001 0.9692625

Fixed effects: A ~ C + B + C:B 
              Value     Std.Error   DF   t-value p-value
(Intercept)  -3.0675750 0.6212136 1118 -4.938036  0.0000
C             0.7364624 0.6264595  244  1.175595  0.2409
B             0.2200117 0.1988966 1118  1.106161  0.2689
C:B           0.0131436 0.2000672 1118  0.065696  0.9476
Correlation: 
             (Intr)    C       B   
C            -0.992              
B            -0.849  0.842       
C:B           0.844 -0.844 -0.994

Standardized Within-Group Residuals:
    Min          Q1         Med          Q3         Max 
-8.51192452 -0.38169972  0.05365992  0.47695927  7.43457534 

Number of Observations: 1366
Number of Groups: 246 
anova(B.lme)
              numDF denDF  F-value p-value
(Intercept)     1  1118 811.5700  &lt;.0001
C               1   244   3.7171  0.0550
B               1  1118 117.6260  &lt;.0001
B:C             1  1118   0.0043  0.9476
</code></pre>

<p>When I had a closer look at A.lme, the summary/anova suggests that variable C is significant.</p>

<pre><code>&gt;summary(A.lme)
Linear mixed-effects model fit by REML
Data: data1 
   AIC      BIC    logLik
4235.429 4261.517 -2112.715

Random effects:
Formula: ~1 | id
         (Intercept)  Residual
StdDev:   0.9228998 0.9690801

Fixed effects: A ~ B + C 
             Value      Std.Error  DF   t-value   p-value
(Intercept) -3.1021904 0.3332309  1119  -9.309431 0.0000
B            0.2330059 0.0214786  1119  10.848303 0.0000
C            0.7713974 0.3352298  244   2.301100  0.0222
Correlation: 
          (Intr)   B   
B         -0.171       
C         -0.971  0.034

Standardized Within-Group Residuals:
    Min          Q1         Med          Q3         Max 
-8.51328019 -0.38179254  0.05385169  0.47724088  7.43658227 

Number of Observations: 1366
Number of Groups: 246 

anova(A.lme)
              numDF denDF F-value p-value
(Intercept)     1  1119 813.4873   &lt;.0001
B               1  1119 116.1162   &lt;.0001
C               1   244   5.2951   0.0222
</code></pre>

<p>My question is which of the two models is more suitable for investigating C as a covariate? My second question is how important is the significance of the p-value of C in A.lme-this seems to suggest to me that C has a significant impact on the slope and intercept but not when combined with B (C:B). Can I safely conclude that C is not significant in B.lme? I'm using the nlme package in R. </p>

<p>Any help would be highly appreciated. </p>
"
"0.196071517657198","0.191448666935321"," 99952","<p><strong>Please read edit 3 first</strong></p>

<p>I am trying to find out the significant factors in a dataset of percentages, a sample of which are below. The difficulty is that the data violates the assumptions of ANOVA, and most of the data are fairly close to 100%.</p>

<p>Please note that glm + binary would not work: the samples used to calculate each proportion are not independent. I do have access to the denominator, if that helps.</p>

<p>Any direction where to start? I've read quite a few things here and elsewhere (notably trying to use some transformation such as arcsin, etc...) and also some other methods I never heard of (""contingency table approaches""). As in a ""textbook ANOVA"" I would like to know which factors are significant, and how much of the variability they explain.</p>

<pre><code>data = c(0.79,0.98,0.95,0.95,1,0.98,0.99,0.97,0.99,0.99,0.98,0.99,0.99,0.94,0.94,  
0.86,0.84,0.86,0.97,0.96,0.53,0.87,0.97,0.81,0.99,1,0.99,0.87,0.98,0.97,0.93,0.8,  
0.7,0.94,0.89,0.98,0.89,0.98,0.96,0.98)
</code></pre>

<p><strong>Edit:</strong> sorry for the lack of clarity. Here's how my percentages are calculated: I basically throw a number of particles (known only a posteriori) in a funnel and count how many make it through/how many remain stuck in the funnel. The percentage is the ratio of the particles having made it through divided by the total number of particles.If a particle which comes at the beginning of the trial remains stuck, the odds of a subsequent particle to remain stuck are higher. As such, I don't think I can apply a generalized linear model, specifying binomial as familly (in R I mean). But again, my statistical insights may be utterly wrong.</p>

<p><strong>Edit2:</strong> regarding independance, I guess my comment was misleading. Each sample in the vector above is independant from the others. However, as I explain in the edit above, the samples used to calculate each percentage are not. </p>

<p><strong>Edit3:</strong>
Well, I reckon I probably made a mess of my question, think it may help if I rephrase the problem and show some data (a fraction of my whole dataset). Besides, I have progressed a bit, hopefully in the right direction. I do not know if I should do a full edit of my original question, or even abandon and start anew, I'll add this as an edit for the interest of history (let me know if I should do differently).</p>

<p>My response variable is the percentage of particles having made it through a funnel (the total number of particles is different for each percentage). If a particle at the beginning of the trial remains stuck, the odds of a subsequent particle to remain stuck are higher.</p>

<p>The (potential) explanatory variables are 1) the type of particles, 2) the funnel type, 3) the funnel position and 4) the total number of particles. In a first stage, I want to find which of these actually impact the response variable and by how much.
In a second stage, I would like to use the current dataset as a reference for the analysis of future samples.  Precisely, I would like to know if the percentage of particles having made through the funnel is  significantly different from the reference dataset and by how much. </p>

<p>Plotting the data indicates me that each population may have a different mean and a different variance.</p>

<pre><code># Libraries ####
library(ggplot2)
library(betareg)
library(lmtest)

#Create data####
df5 = structure(list(type = c(""Type1"", ""Type1"", ""Type1"", ""Type2"", ""Type2"", 
                              ""Type2"", ""Type2"", ""Type2"", ""Type2"", ""Type3"", ""Type3"", ""Type3"", 
                              ""Type1"", ""Type1"", ""Type1"", ""Type3"", ""Type3"", ""Type3"", ""Type2"", 
                              ""Type2"", ""Type2"", ""Type1"", ""Type1"", ""Type1"", ""Type1"", ""Type1"", 
                              ""Type1"", ""Type3"", ""Type3"", ""Type3"", ""Type2"", ""Type2"", ""Type2"", 
                              ""Type1"", ""Type1"", ""Type1"", ""Type3"", ""Type3"", ""Type3"", ""Type1"", 
                              ""Type1"", ""Type1"", ""Type3"", ""Type3"", ""Type3"", ""Type2"", ""Type2"", 
                              ""Type2"", ""Type3"", ""Type3"", ""Type3"", ""Type2"", ""Type2"", ""Type2""), 
                     funnelType = c(""fType1"", ""fType1"", ""fType1"", ""fType1"", ""fType1"", 
                                    ""fType1"", ""fType2"", ""fType2"", ""fType2"", ""fType1"", ""fType1"", ""fType1"", 
                                    ""fType2"", ""fType2"", ""fType2"", ""fType1"", ""fType1"", ""fType1"", ""fType1"", 
                                    ""fType1"", ""fType1"", ""fType1"", ""fType1"", ""fType1"", ""fType1"", ""fType1"", 
                                    ""fType1"", ""fType2"", ""fType2"", ""fType2"", ""fType1"", ""fType1"", ""fType1"", 
                                    ""fType2"", ""fType2"", ""fType2"", ""fType1"", ""fType1"", ""fType1"", ""fType2"", 
                                    ""fType2"", ""fType2"", ""fType2"", ""fType2"", ""fType2"", ""fType2"", ""fType2"", 
                                    ""fType2"", ""fType2"", ""fType2"", ""fType2"", ""fType2"", ""fType2"", ""fType2""), 
                     position = c(""a"", ""b"", ""c"", ""a"", ""b"", ""c"", ""a"", ""b"", ""c"", 
                                  ""a"", ""b"", ""c"", ""a"", ""b"", ""c"", ""a"", ""b"", ""c"", ""a"", ""b"", ""c"", ""a"", 
                                  ""b"", ""c"", ""a"", ""b"", ""c"", ""a"", ""b"", ""c"", ""a"", ""b"", ""c"", ""a"", ""b"", 
                                  ""c"", ""a"", ""b"", ""c"", ""a"", ""b"", ""c"", ""a"", ""b"", ""c"", ""a"", ""b"", ""c"", 
                                  ""a"", ""b"", ""c"", ""a"", ""b"", ""c""), 
                     total = c(420L, 293L, 324L, 549L, 
                               527L, 603L, 533L, 571L, 438L, 496L, 534L, 604L, 489L, 360L, 383L, 
                               524L, 560L, 606L, 493L, 513L, 572L, 530L, 527L, 543L, 616L, 471L, 
                               554L, 392L, 530L, 443L, 561L, 529L, 599L, 529L, 481L, 521L, 621L, 
                               567L, 609L, 447L, 398L, 462L, 528L, 574L, 522L, 654L, 531L, 556L, 
                               642L, 569L, 684L, 372L, 540L, 345L), 
                     percentage = c(0.98808, 0.95569, 0.9784, 0.57741, 0.81017, 0.82919, 0.93809, 0.94041, 
                                    0.94744, 0.93352, 0.98129, 0.99006, 0.97339, 0.94728, 0.98695, 
                                    0.9542, 0.84989, 0.92574, 0.79116, 0.92782, 0.98077, 0.96605, 
                                    0.97155, 0.96503, 0.97076, 0.95756, 0.99097, 0.96429, 0.9434, 
                                    0.99097, 0.78253, 0.94518, 0.97664, 0.99056, 0.96261, 0.98273, 
                                    0.88402, 0.96824, 0.92118, 0.95524, 0.97991, 0.9762, 0.83144, 
                                    0.95643, 0.98085, 0.95107, 0.94162, 0.98741, 0.83635, 0.94372, 
                                    0.98099, 0.88447, 0.94817, 0.95365)), 
                .Names = c(""type"", ""funnelType"",""position"", ""total"", ""percentage""), class = ""data.frame"", 
                row.names = c(2L, 
                              3L, 4L, 18L, 19L, 20L, 50L, 51L, 52L, 66L, 67L, 68L, 98L, 99L, 
                              100L, 114L, 115L, 116L, 130L, 131L, 132L, 146L, 147L, 148L, 162L, 
                              163L, 164L, 194L, 195L, 196L, 210L, 211L, 212L, 226L, 227L, 228L, 
                              258L, 259L, 260L, 306L, 307L, 308L, 322L, 323L, 324L, 354L, 355L, 
                              356L, 370L, 371L, 372L, 386L, 387L, 388L))

ggplot(df5,aes(x=total,y=percentage))+geom_boxplot(outlier.shape = NA)+geom_jitter(aes(col=position))+facet_grid(type~funnelType)
</code></pre>

<p>It does seem that I am in a situation comparable the dyslexic example in <a href=""http://psycnet.apa.org/journals/met/11/1/54/"" rel=""nofollow"">http://psycnet.apa.org/journals/met/11/1/54/</a>. I consequently think that I should be able to apply the same method to analyse my data. To do so I use the package betareg in R (no interactions for the sake of the example).</p>

<pre><code>full &lt;- betareg(percentage ~ type+funnelType+position+total|type+funnelType+position+total,data = df5)
summary(full)
</code></pre>

<p>This yields the results that all explanatory variables but ""total"" have a significant effect on the mean. From what I know explanatory variables are likely to be significant in large data sets (I have a relatively large number of observastions in my full dataset). How do I judge the magnitude of the effect of each factor? What should I do if two of the explanatory variables are correlated?</p>

<p>In my second stage I want to compare a new sample to the subpopulation in my dataset that has the same predictor values. How should I perform this? Unequal variance t-test?</p>
"
"0.0399043442233811","0.0405220449236554","100751","<p>In one of the slides of a statistics course that I followed the following about using drop1 or ANCOVA is stated.</p>

<blockquote>
  <p>Using drop1 the p-values shown are p-values for deleting one variable at a time from the full model, whereas the p-values in the output of anova are sequential, as in a step-up strategy. This problem does not arise in ANOVA or linear regression, only in ANCOVA and mixed models.)</p>
</blockquote>

<p>I looked at it for 20 minutes, and cannot understand that if this is true why someone would still use the command <code>anova()</code> in R anyway.</p>

<p>Does anyone have an idea?</p>

<p>The reason why I ask this question here is because the subject was given a few months ago.</p>
"
"0.159617376893524","0.162088179694622","101265","<p>I was trying to use the nlme package in r to do a multilevel linear model.</p>

<p>I have yield as response variable and rainfall as predictor variable for 60 years for 6 different locations (State). I am trying to see whether rainfall has same level of effect on yield in all locations or different effects. In principle, I am trying to see if slope of yield vs rainfall significantly varies between locations. Therefore rainfall is my random effect. I built my model like this: </p>

<pre><code> # baseline model which only includes intercept
mdl1&lt;-gls(yield ~ 1,data = data, method=""ML"")

#intercept as random effect
mdl2&lt;-lme(yield ~ 1,data=data,random = ~1|state,method=""ML"")  

 # slope as random effect
mdl3&lt;-lme(yield ~ rain, data = data, random = ~rain|state,method=""ML"")

##compare the three model
anova(mdl1,mdl2,mdl3)
##this shows me when I add slope as random effect, my model shows better fit compared to baseline model (mdl1)
</code></pre>

<p>this is all working fine. The problem starts when I do the same analysis using an another predictor variable (a count data)</p>

<pre><code> # baseline model which only includes intercept: Works fine
mdl4&lt;-gls(yield ~ 1,data = data, method=""ML"")

#intercept as random effect - works fine
mdl5&lt;-lme(yield ~ 1,data=data,random = ~1|state,method=""ML"")  

 # include different predictor (break) this time instead of rain
mdl6&lt;-lme(yield ~ break, data = data, random = ~break|state,method=""ML"")
</code></pre>

<p>when i run the mdl 6, this gives me the error</p>

<pre><code>Error in lme.formula(res_yld ~ brk, data = data, random = ~brk | state,  : 
nlminb problem, convergence error code = 1
message = iteration limit reached without convergence (10)
</code></pre>

<p>I have absolutely no clue why is this happening. Everything worked fine for my first predictor but this does not work on another predictor. What am I doing wrong here? I tried reading about this online but the posts are not very clear to me. I would really appreciate of anyone could me out on this.
Thanks</p>
"
"0.0798086884467622","0.0810440898473108","102689","<p>I have a problem with some analysis I need to do.</p>

<p>I have a series of regressions. Some of the predictors of these regression are categorical with multiple levels. I performed regressions, both linear and logistic, choosing a baseline for these category according to various factors.</p>

<p>The problem is that my colleagues asked not only for a confrontation of the factors to a baseline but also a pairwise confrontation. Like you it's used to do with a post-hoc test for ANOVA (they are pretty new to regressions and their benefits).</p>

<p>How should I approach this?
I thought of some solutions:</p>

<ul>
<li>Subsetting: That is subset the data to include two factors at time, and therefore repeating the regression once per every subset.</li>
<li>Splitting: Splitting the category column in a column for every factor and put 0 and 1 as levels. This approach can furthermore be conducted in two ways:
<ul>
<li>Putting all the new columns in the regression (minding that they are mutually exclusive).</li>
<li>Putting one column at time, multiplying the regressions.</li>
</ul></li>
</ul>

<p>Which approach would you suggest, minding statistical correctness and workload?</p>

<p>Especially, what's the conceptual difference between the three methods?</p>

<p>Thanks a lot!</p>
"
"0.0691163516376137","0.0701862406343596","104548","<p>I followed <a href=""http://rtutorialseries.blogspot.hk/2010/01/r-tutorial-series-basic-hierarchical.html"" rel=""nofollow"">this tutorial</a> to learn Hierarchical Linear Regression (HLR) in R, but couldn't understand how to interpret its sample output of <code>&gt;anova(model1,model2,model3)</code></p>

<p><img src=""http://i.stack.imgur.com/MxXIM.png"" alt=""enter image description here""></p>

<p>The tutorial simply says </p>

<blockquote>
  <p>each predictor added along the way is making an important contribution to the overall model.</p>
</blockquote>

<p>But I would like some more details to <strong>quantify</strong> the contribution of each explanatory variable, like:</p>

<ol>
<li><p>""UNEM"" explains <code>X</code> (or <code>X%</code>) variance</p></li>
<li><p>Adding the ""HGRAD"" variable explains <code>Y</code> (or <code>Y%</code>) more variance</p></li>
<li><p>Adding the ""INC"" variable further explains <code>Z</code> (or <code>Z%</code>) more variance</p></li>
</ol>

<p>So, can I get the value of <code>X</code>, <code>Y</code>, and <code>Z</code> using the above ANOVA table? How? Specifically, what do <code>Res.Df</code>, <code>RSS</code>, <code>Sum of Sq</code> mean in this ANOVA table?</p>
"
"0.191374511943613","0.194336900433961","105906","<blockquote>
  <p>The bounty I placed on this question expires in the next 24 hours.</p>
</blockquote>

<p>I have a psychological data set which, traditionally, would be analysed using a paired samples t test.
The design of the experiment is $39 (subjects) \times 7 (targets) \times 2 (conditions)$, and I'm interested in the difference in a given variable between the conditions.</p>

<p>The traditional approach has been to average across targets so that I have 2 observations per participant, and then compare these averages using a paired t test.</p>

<p>I wanted to use a mixed models approach, as has become increasingly popular in this field (i.e. <a href=""http://www.sfs.uni-tuebingen.de/~hbaayen/publications/baayenDavidsonBates.pdf"" rel=""nofollow"">Baayen, Davidson &amp; Bates, 2008</a>), and so the first model I fit, which I thought should approximate the results of the t test, was one with $condition$ as a fixed effect, and random intercepts for $subjects$ (i.e. $var = \alpha + \beta*condition + Intercept(subject) + \epsilon$. Obviously, the full model would also include random intercepts for $targets$.</p>

<p>However, I'm struggling to understand why I achieve pretty divergent results between the two approaches.
Can anyone explain what's going on here?
I've also seen (what I understand to be) a similar question asked <a href=""http://stats.stackexchange.com/questions/23276/paired-t-test-as-a-special-case-of-linear-mixed-effect-modeling-still-unresolve"">here</a>, with an answer about correlation structure which I'm not equipped to understand. If this is also what's at issue here, I would appreciate if anyone could suggest some resources to read up on this.</p>

<p><strong>Edit:</strong> I've posted <a href=""https://gist.github.com/EoinTravers/ce86c93fb42fba284464"" rel=""nofollow"">the example data, and R script, here</a>.</p>

<p><strong>Edit #2 - Bounty added</strong></p>

<p>Some additional points:</p>

<ul>
<li>I'm only analysing the correct responses (think of it as analogous to reaction time), so there are <strong>missing cases</strong> - not every participant provides 7 data points per condition.
<ul>
<li>When I analyse all responsees, rather than just the correct ones, the difference between the two results is reduced, but not eliminated. This suggests to me that the missing cases are a factor here.</li>
</ul></li>
<li>The variable isn't normally distributed. In my final model, I scale it using a Box-Cox transformation, but I omit that here for consistency with the t test.</li>
<li>As pointed out by @PeterFlom, the $df$s differ hugely between the two approaches, but I assume this to be because the t test is being applied to the aggregate data (2 observations per participant, 1 per condition), while the mixed model is applied to raw scores ($&lt;14$ observations per participant, $&lt;7$ per condition).</li>
<li>@BenBolker notes that the t values also differ pretty considerably.</li>
</ul>

<p>My analysis code is below.</p>

<pre><code>&gt;library(dplyr)
&gt;subject_means = group_by(data, subject, condition) %&gt;% summarise(var=mean(var))
&gt;t.test(var ~ condition, data=subject_means, paired=T)

    Paired t-test

data:  var by condition
t = -1.3394, df = 37, p-value = 0.1886
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
 -0.14596388  0.02978745
sample estimates:
mean of the differences 
            -0.05808822 

&gt;library(lme4)
&gt;lm.0 = lmer(var ~ (1|subject), data=data)
&gt;lm.1 = lmer(var ~ condition + (1|subject), data=data)
&gt;anova(lm.0, lm.1)

Data: data
Models:
object: var ~ (1 | subject)
..1: var ~ condition + (1 | subject)
       Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)  
object  3 489.09 501.23 -241.55   483.09                           
..1     4 485.81 502.00 -238.90   477.81 5.2859      1     0.0215 *

&gt;library(lmerTest)
&gt;summary(lm.1)$coef

              Estimate Std. Error        df  t value     Pr(&gt;|t|)
(Intercept) 0.11862462 0.02878027  98.60659 4.121734 7.842075e-05
condition   0.09580546 0.04161237 400.27441 2.302331 2.182890e-02
</code></pre>

<p>Notice, specifically, the jump in the p value from $p = .188$ in the t test, to $p = .021$ from either <code>lmer</code> method.</p>

<hr>

<p>I've tried, and failed to provide a reproducible example of this, using the <code>anorexia</code> dataset in the <code>MASS</code> package, so I would assume the problem is something idiosyncratic to my data, but I don't understand what.</p>

<pre><code># Borrowing from http://ww2.coastal.edu/kingw/statistics/R-tutorials/dependent-t.html
&gt;data(anorexia, package=""MASS"")
&gt;ft = subset(anorexia, subset=(Treat==""FT""))
&gt;wgt = c(ft$Prewt, ft$Postwt)
&gt;pre.post = rep(c(""pre"",""post""),c(17,17))
&gt;subject = rep(LETTERS[1:17],2)
&gt;t.test(wgt~pre.post, data=ft.new, paired=T)

    Paired t-test

data:  wgt by pre.post
t = 4.1849, df = 16, p-value = 0.0007003
alternative hypothesis: true difference in means is not equal to 0
95 percent confidence interval:
  3.58470 10.94471
sample estimates:
mean of the differences 
               7.264706 

&gt;m = lmer(wgt ~ pre.post + (1|subject), data=ft.new)
&gt;summary(m)$coef

             Estimate Std. Error       df   t value     Pr(&gt;|t|)
(Intercept) 90.494118   1.689013 26.17129 53.578096 0.0000000000
pre.postpre -7.264706   1.735930 15.99968 -4.184908 0.0007002806
</code></pre>
"
"0.119713032670143","0.121566134770966","106079","<p>I am estimating a random intercept and a random slope model using the following R code. My dependent and independent variable are both continuous.</p>

<pre><code>randominterceptfixedslope&lt;-lmer(y ~ x + (1|state),data=data,method=""ML"") # model with fixed slope but random intercept
randominterceptrandomslope&lt;-lmer(y ~ x + (1+x|state),data=data,method=""ML"") # model with random slope and random intercept

anova(randominterceptfixedslope,randominterceptrandomslope)
</code></pre>

<p>Anova tells me that my randominterceptrandomslope model is a better fit on the data. So far good, please correct me if I am wrong. </p>

<p>My question is: If I have another independent variable $x_1$, can I put two independent variables in the above model i.e. can I have a randominterceptfixedslope and randominterceptrandomslope model with two independent variables. If yes, how do I do that? As in what the code should look like?</p>

<p>Thanks for your response. I got a second query. lets say my full model is this: </p>

<pre><code>     randominterceptrandomslope&lt;-lmer(y ~ x1 + x2 + x3 + x4 (1+x1+x2+x3+x4|state),data=data,method=""ML"")
</code></pre>

<p>If some of my independent variables are correlated, what is the procedure of reducing the collinearity issue in a linear mixed effect model?  I could spot collinerity  using VIF and retain the most significant independent variables but I can do this for each factor level (levels of state) individually. But won't it result in retaining some independent variables in one factor level while deleting the same in other factor level? I guess the main question is how to spot collinearity in a mixed effect model and what to do with it when you have 5 or 6 independent variables?</p>
"
"NaN","NaN","108161","<p>The <code>lmerTest</code> package provides an ANOVA function for linear mixed effects models with optionally Satterthwaite's (default) or Kenward-Roger's approximation of the degrees of freedom. What is the difference between these two approaches? When to choose which?</p>
"
"0.0892288262810312","0.0906100470365937","110531","<p>Non-parametric ANOVA â€“ a hot topic that is unanswered. </p>

<p>There are many questions on this topic online. However, they all seem to end in a debate and no definite answer or clear explanation (that I can relate to my data set). The only good answers I can find seem to relate to one treatment, that was repeated over time. How do I handle two treatments? More importantly, should this be handled as a linear model? </p>

<p>I cannot meet assumptions of homogeneity, normality, equal sample size, and transformations were not helpful.</p>

<p>Data setup: </p>

<ul>
<li>Soil cores growing plants.</li>
<li>Subject â€“ about 33 samples. A core will consist of a treatment combo (<em>n</em> = 5 to 6; repeats)</li>
<li><code>Treatment01</code> - clay, sand, loamy sand</li>
<li><code>Treatment02</code> - water, fertiliser</li>
<li>Time - an sample is analysed from each core sample, every week, for 3 months.</li>
<li>Variable - Nutrients (mg/kg soil), moisture, pH, mass of soil etc. For simplicity, let's do carbon.</li>
</ul>

<p>Here are my failed attempts on R. This problem rests in the statistical theory. </p>

<p><code>Anova(lm(Carbon ~ Treatment01 * Treatment02, data), type = ""3"")</code><br>
or
<code>model1 &lt;- lm(Carbon ~ Treatment01 * Treatment02)</code></p>

<p>Then something like...</p>

<pre><code>print(lsmeans(model1, list(pairwise ~ Treatment01)), adjust = c(""tukey""))
print(lsmeans(model1, list(pairwise ~ Treatment02)), adjust = c(""tukey""))
print(lsmeans(model1, list(pairwise ~ Treatment01 | Treatment02)), adjust = c(""tukey""))
print(lsmeans(model1, list(pairwise ~ Treatment02 | Treatment01)), adjust = c(""tukey""))
</code></pre>

<p>Any suggestions? What are the disadvantages in pulling the data set apart to do a series of Friedman or Kruskalâ€“Wallis tests (depending on how much I pull this data set apart?)</p>
"
"0.0977452818676612","0.099258333397093","110632","<p>If there are 2 nlme models with same non-linear mean function, model 1 and model 2, how do you compare them ? Which R function does this for us ?</p>

<p>And when there are random effects or fixed effects, I don't know how a nested model is defined ? </p>

<p>For example model 1, </p>

<pre><code>`model1  &lt;- nlme(df_measures ~ meanfunc(w, time, b0,b1,b2) , 
            fixed = list(b0 ~1, b1 ~ 1, b2 ~ 1),
            random = b0 + b1 + b2 ~ 1,
            groups = ~ subject ,
            data = dataa,
            start = list(fixed = c(b0 = 3,1, b1 = 5,1,b2 = 1,1)),
            verbose = T
            )`
</code></pre>

<p>and model 2: </p>

<pre><code>`model2 &lt;- nlme(df_measures ~ meanfunc(w, time, b0,b1,b2) , 
            fixed = list(b0 ~w, b1 ~ w, b2 ~ w),
            random = b0 + b1 + b2 ~ 1,
            groups = ~ subject ,
            data = dataa,
            start = list(fixed = c(b0 = 3,1, b1 = 5,1,b2 = 1,1)),
            verbose = T
            )`
</code></pre>

<p>Is model2 a nested model of model 1 ? This is confusing me and how do I compare them ? </p>

<p>If model 2 is a nested model of model 1, can I still use <code>ANOVA</code>  function in R to compare them ? 
What if model 2 is not a nested model of model 1 ? How should I compare between these 2 models? Based on what criteria ? </p>

<p>Thanks.</p>
"
"0.119713032670143","0.121566134770966","110917","<p>Suppose I create a dummy scenario as such:</p>

<pre><code>&gt; A &lt;- rnorm(10000) 
&gt; B &lt;- rnorm(10000) 
&gt; C &lt;- rnorm(10000) 
&gt; Y &lt;- A*B + rnorm(10000,sd=0.1)
</code></pre>

<p>Doing a simple ANOVA correctly identifies that none of the variables are significantly predictive of the outcome:</p>

<pre><code>&gt; anova(lm(Y~A+B+C))
Analysis of Variance Table

Response: Y
            Df  Sum Sq Mean Sq F value Pr(&gt;F)
A            1     1.5 1.54411  1.4209 0.2333
B            1     0.3 0.28909  0.2660 0.6060
C            1     1.6 1.62425  1.4946 0.2215
Residuals 9996 10862.8 1.08672    
</code></pre>

<p>But not let's say I decide to include the interaction terms:</p>

<pre><code>&gt; anova(lm(Y~A*B*C))
Analysis of Variance Table

Response: Y
           Df  Sum Sq Mean Sq    F value    Pr(&gt;F)    
A            1     1.5     1.5 1.5281e+02 &lt; 2.2e-16 ***
B            1     0.3     0.3 2.8610e+01  9.05e-08 ***
C            1     1.6     1.6 1.6074e+02 &lt; 2.2e-16 ***
A:B          1 10761.8 10761.8 1.0650e+06 &lt; 2.2e-16 ***
A:C          1     0.0     0.0 9.8700e-02    0.7534    
B:C          1     0.0     0.0 1.5062e+00    0.2197    
A:B:C        1     0.0     0.0 1.6790e-01    0.6820    
Residuals 9992   101.0     0.0                         
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>It has correctly identified the interaction between A and B as being the most significant, but now for some reason the individual terms A and B have also gained significance... and C which had nothing at all to do with creating the model is significant as well?  Either I have not written the test correctly or I am completely misunderstanding how a Two-Way ANOVA with interaction terms works</p>

<p>Using a simple linear model gives expected results:</p>

<pre><code>&gt; summary(lm(Y~A*B*C))

Call:
lm(formula = Y ~ A * B * C)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.29566 -0.06667 -0.00092  0.06665  0.33620 

Coefficients:
              Estimate Std. Error  t value Pr(&gt;|t|)    
(Intercept) -0.0003212  0.0009707   -0.331    0.741    
A            0.0003483  0.0009613    0.362    0.717    
B            0.0003184  0.0009619    0.331    0.741    
C           -0.0003213  0.0009702   -0.331    0.741    
A:B          1.0008711  0.0009370 1068.214   &lt;2e-16 ***
A:C         -0.0014855  0.0009588   -1.549    0.121    
B:C          0.0008860  0.0009561    0.927    0.354    
A:B:C       -0.0002489  0.0009085   -0.274    0.784    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.09705 on 9992 degrees of freedom
Multiple R-squared:  0.9913,    Adjusted R-squared:  0.9913 
F-statistic: 1.634e+05 on 7 and 9992 DF,  p-value: &lt; 2.2e-16
</code></pre>
"
"0.159617376893524","0.151957668463708","111535","<p>This is my first post, so sorry if it not optimally written. I have a paired samples at two time points in two groups, undergoing the same intervention. I want to test the effect of my intervention on weight.</p>

<p>I'm using R. Here's some data to illustrate: my data frame called <code>df</code>:</p>

<pre><code>      patients   timepoint        group          Weight
        102            1            1            107.30
        104            1            1             94.10
        117            1            1            110.80
        121            1            1            108.90
        153            1            1             95.40
        155            1            1            105.10
        161            1            1             97.70
        162            1            1             83.60
        167            1            1             82.40
        173            1            1             86.40
        176            1            1             81.90
        177            1            1             90.90
        179            1            1             95.30
        101            1            2             81.30
        108            1            2             72.30
        113            1            2             68.50
        170            1            2             89.20
        171            1            2             77.50
        172            1            2             94.50
        175            1            2             78.30
        181            1            2             71.40
        182            1            2             72.80
        183            1            2             73.50
        186            1            2             87.90
        187            1            2             83.50
        188            1            2             70.10
        102            2            1            102.70
        104            2            1             90.40
        117            2            1            107.30
        121            2            1            107.50
        153            2            1             95.00
        155            2            1            102.80
        161            2            1             95.40
        162            2            1             78.30
        167            2            1             81.90
        173            2            1             85.30
        176            2            1             83.10
        177            2            1             90.50
        179            2            1             97.50
        101            2            2             78.40
        108            2            2             72.00
        113            2            2             66.80
        170            2            2             90.20
        171            2            2             77.60
        172            2            2             93.40
        175            2            2             80.30
        181            2            2             72.60
        182            2            2             71.40
        183            2            2             74.20
        186            2            2             88.70
        187            2            2             80.50
        188            2            2             71.20
</code></pre>

<p>Since this is paired data (between time points) and unpaired (between groups), I guess I must use a mixed linear model. Im going for the lme4 package in R.</p>

<p>""timepoints"" and ""group"" will be my fixed effects (I exhaust both). ""patients"" will be my random effect, which also picks up that I have multiple responses per patient. </p>

<p>I will use a random intercept model, but I actually expect that my patients differ with how they react to my experimental manipulation, so a random slopes model would be nice also. However, it seams I over-parametrize the model if doing so.</p>

<p>I will use the likelighood ratio using anova() for a full model and a reduced model.</p>

<pre><code>full = lmer(Weight ~ timepoint + group + (1|patients), data=df,
        REML=FALSE)

reduced = lmer(Weight ~ group + (1|patients), data=df,
                   REML=FALSE)
</code></pre>

<p>""timepoint"" is the main factor in question. I now test using anova():</p>

<pre><code>&gt; anova(reduced,full)
Data: df
Models:
reduced: Weight ~ group + (1 | patients)
full: Weight ~ timepoint + group + (1 | patients)
        Df    AIC    BIC  logLik deviance Chisq Chi Df Pr(&gt;Chisq)  
reduced  4 309.72 317.52 -150.86   301.72                          
full     5 306.02 315.78 -148.01   296.02 5.695      1    0.01701 *
</code></pre>

<p>Question is, Im I doing it correctly? And how do I interpret the result?</p>

<p>BUT, what I really want is to test if the effect of time is different on the two groups. How do I do this?</p>

<p>Thank you.</p>
"
"0.10828451233157","0.109960706249747","112220","<p>I have measurements from 12 mice, grouped in two conditions. I each mouse I have measurements from 4 tissues. The design is not balanced, 5 mice in condition1 and 7 in condition2.</p>

<p>After reading the bioconductor edgeR manual I have set up the following model:</p>

<pre><code>design &lt;- model.matrix(~ Condition + Tissue + Condition:Mouse + Condition:Tissue)
</code></pre>

<p>I have then manually removed terms with no observations.</p>

<pre><code>design &lt;- design[, -which(colnames(design) %in% c(""Condition1:Mouse6"", ""Condition1:Mouse7""))]
</code></pre>

<p>I can fit the model, but when I set up contrasts I get confused. I want to get the main effect of being in condition2, the main effect of each tissue and the interaction effect for each tissue. </p>

<pre><code>colnames(design)
 [1] ""(Intercept)""  ""Condition2""    ""Tissue2""              
 [4] ""Tissue3""  ""Tissue4""   ""Condition1:Mouse2""            
 [7] ""Condition2:Mouse2""    ""Condition1:Mouse3"" ""Condition2:Mouse3""         
 [10] ""Condition1:Mouse4""    ""Condition2:Mouse4"" ""Condition1:Mouse5""            
 [13] ""Condition2:Mouse5""    ""Condition2:Mouse6"" ""Condition2:Mouse7""         
 [16] ""Condition2:Tissue2""   ""Condition2:Tissue3""  ""Condition2:Tissue4"" 
</code></pre>

<p>To get the main effect of condition I use the contrast</p>

<pre><code>c(0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
</code></pre>

<p>Comparing e.g. Tissue 1 and 2 or 2 and 3 I do like this</p>

<pre><code>c(0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
c(0,0,-1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
</code></pre>

<p>But say I want to get the difference between condition 1 and 2 in tissue 1. Given how the model matrix is built tissue 1 is not present. Can I do a comparison like:</p>

<pre><code>c(0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,-1,-1)
</code></pre>

<p>And how about an anova like test for changes across all the tissue:condition interaction terms. The contrast</p>

<pre><code>c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1)
</code></pre>

<p>Would compare only tissue2, 3 and 4, and with what?</p>

<p>Many questions here. Quick recap. 
1. Is it OK to remove empty terms in an unbalanced design?
2. How do I find the effect of the level in the tissues category that is missing in the model matrix. 
3. How do I find changes happening across all levels of the interaction terms (related to the missing tissue1 problem).</p>

<p>A link to a page with a thorough R model matrix/linear model guide would also be appreciated.</p>
"
"0.222286550555375","0.225727443034925","115065","<p>I am working with data from a computer task which has 288 total trials, each of which can be categorically classified according to <strong>Trial Type</strong>, <strong>Number of Stimuli</strong>, and <strong>Probe Location</strong>.  Because I want to also examine a continuous variable, the total Cartesian <strong>Distance</strong> between stimuli per trial (divided by number of stimuli to control for varying numbers), I have opted to use a mixed linear model with repeated measures.  In addition to each of these task variables, I am also interested in whether folks in various diagnostic groups perform differently on the task, as well as whether or not there is a <strong>Dx</strong> interaction with any of the above variables.  Thus (if I'm not mistaken), I have the following effects in my model:</p>

<p><strong>Trial Type</strong>, a fixed effect
<strong>Number of Stimuli</strong>, a fixed effect
<strong>Probe Location</strong>, a fixed effect
<strong>Dist</strong>(ance), a fixed effect
<strong>Dx</strong>, a fixed effect
<strong>Dx*Trial Type</strong>, a fixed effect
<strong>Dx*Number of Stimuli</strong>, a fixed effect
<strong>Dx*Probe Location</strong>, a fixed effect
<strong>Dx*Dist</strong>, a fixed effect
<strong>Trial</strong>, a random effect, nested within
<strong>SubID</strong>, a random effect</p>

<p>Based on my examination of documentation, it seems that the nesting of random effects does not seem to be important to lme4, and so I specify my model as follows:</p>

<p><code>tab.lmer &lt;- lmer(Correct ~  Dx+No_of_Stim+Trial_Type+Probe_Loc+Dist+Dx*No_of_Stim+Dx*Trial_Type+Dx*Probe_Loc+Dx*Dist+(1|Trial)+(1|SubID),data=bigdf)</code></p>

<p>This would be my first question: <strong>1) Is the above model specification correct?</strong></p>

<p>Assuming so, I am a bit troubled by my results, but as I read and recall my instruction on such models, I am wondering if interpretation of particular coefficients is bad practice in this case:</p>

<pre><code>Linear mixed model fit by REML ['merModLmerTest']
Formula: Correct ~ Dx + No_of_Stim + Trial_Type + Probe_Loc + Dist + Dx *  
    No_of_Stim + Dx * Trial_Type + Dx * Probe_Loc + Dx * Dist +  
    (1 | Trial) + (1 | SubID)
   Data: bigdf

REML criterion at convergence: 13600.4

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-2.89810 -0.03306  0.27004  0.55363  2.81656 

Random effects:
 Groups   Name        Variance Std.Dev.
 Trial    (Intercept) 0.013256 0.11513 
 SubID    (Intercept) 0.006299 0.07937 
 Residual             0.131522 0.36266 
Number of obs: 15840, groups:  Trial, 288; SubID, 55

Fixed effects:
                         Estimate Std. Error         df t value Pr(&gt;|t|)    
(Intercept)             4.196e-01  4.229e-02  4.570e+02   9.922  &lt; 2e-16 ***
DxPROBAND               8.662e-02  4.330e-02  2.920e+02   2.000  0.04640 *  
DxRELATIVE              9.917e-02  4.009e-02  2.920e+02   2.474  0.01394 *  
No_of_Stim3            -9.281e-02  1.999e-02  4.520e+02  -4.642 4.53e-06 ***
Trial_Type1             3.656e-02  2.020e-02  4.520e+02   1.810  0.07097 .  
Probe_Loc1              3.502e-01  2.266e-02  4.520e+02  15.456  &lt; 2e-16 ***
Probe_Loc2              3.535e-01  3.110e-02  4.520e+02  11.369  &lt; 2e-16 ***
Dist                    1.817e-01  2.794e-02  4.520e+02   6.505 2.06e-10 ***
DxPROBAND:No_of_Stim3  -1.744e-02  1.759e-02  1.548e+04  -0.992  0.32144    
DxRELATIVE:No_of_Stim3 -2.886e-02  1.628e-02  1.548e+04  -1.773  0.07628 .  
DxPROBAND:Trial_Type1  -9.250e-03  1.777e-02  1.548e+04  -0.521  0.60267    
DxRELATIVE:Trial_Type1  1.336e-02  1.645e-02  1.548e+04   0.812  0.41682    
DxPROBAND:Probe_Loc1   -8.696e-02  1.993e-02  1.548e+04  -4.363 1.29e-05 ***
DxRELATIVE:Probe_Loc1  -4.287e-02  1.845e-02  1.548e+04  -2.323  0.02018 *  
DxPROBAND:Probe_Loc2   -1.389e-01  2.735e-02  1.548e+04  -5.079 3.83e-07 ***
DxRELATIVE:Probe_Loc2  -8.036e-02  2.532e-02  1.548e+04  -3.173  0.00151 ** 
DxPROBAND:Dist         -3.920e-02  2.457e-02  1.548e+04  -1.595  0.11066    
DxRELATIVE:Dist        -1.485e-02  2.275e-02  1.548e+04  -0.653  0.51390    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>In general, these results make sense to me.  The troubling portion, however, comes in the positive, significant (yes, I am using LmerTest) p-value for DxProband, particularly in light of the fact that in terms of performance means, Probands are performing worse than Controls.  So, this mismatch concerns me.  Examining the corresponding ANOVA:</p>

<pre><code>&gt; anova(tab.lmer)
Analysis of Variance Table of type 3  with  Satterthwaite 
approximation for degrees of freedom
               Sum Sq Mean Sq NumDF   DenDF F.value    Pr(&gt;F)    
Dx             0.8615  0.4308     2   159.0   1.412   0.24662    
No_of_Stim     0.6984  0.6984     1   283.5  37.043 3.741e-09 ***
Trial_Type     8.3413  8.3413     1   283.5   4.456   0.03565 *  
Probe_Loc     25.7223 12.8612     2   283.5 116.405 &lt; 2.2e-16 ***
Dist           5.8596  5.8596     1   283.5  43.399 2.166e-10 ***
Dx:No_of_Stim  1.4103  0.7051     2 15483.7   1.590   0.20395    
Dx:Trial_Type  2.0323  1.0162     2 15483.7   0.841   0.43128    
Dx:Probe_Loc   3.5740  0.8935     4 15483.7   7.299 7.224e-06 ***
Dx:Dist        0.3360  0.1680     2 15483.7   1.277   0.27885    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>...the results seem to more or less line up with the regression, with the exception of the <strong>Dx</strong> variable.  So, my second question is <strong>2) Can anyone clarify what is going on with the Dx variable? Is interpreting individual coefficients from the regression model bad practice in this case?</strong></p>

<p>Finally, as a basic (and somewhat embarrassing) afterthought, <strong>3) If I attempt to reduce the model, I should favor the model with the <em>lower</em> REML, yes?</strong></p>

<p>In summation,
<strong>1) Is the above model specification correct?</strong>
<strong>2) Can anyone clarify what is going on with the Dx variable? Is interpreting individual coefficients from the regression model bad practice in this case?</strong>
<strong>3) If I attempt to reduce the model, I should favor the model with the <em>lower</em> REML, yes?</strong></p>

<p><strong>ADDENDUM</strong></p>

<p>By request, I'll describe the task and data a little further.  The data come from a computer task in which participants are presented a number of stimuli, either two or three, in various locations about the screen.  These stimuli can either be ""targets"" or ""distractors"".  After these stimuli, a probe stimulus is presented; if it appears in a position where a previous target has appeared, participants should respond ""yes""; if it appears in the position of a previous distractor or elsewhere, the correct answer is ""no.""  There are 288 trials of this nature; some have two stimuli and some have three, and some lack distractors entirely.  The variables in my model, then, can be elaborated as follows:</p>

<p><strong>Number of Stimuli:</strong> 2 or 3 (2 levels)</p>

<p><strong>Trial Type:</strong> No Distractor (0) or Distractor (1) (2 levels)</p>

<p><strong>Probe Location:</strong> Probe at Target (1), Probe at Distractor (2), or Probe Elsewhere (0) (3 levels)</p>

<p><strong>Distance:</strong> Total Cartesian distance between stimuli, divided by number of stimuli per trial (Continuous)</p>

<p><strong>Dx:</strong> Participant's clinical categorization</p>

<p><strong>Sub ID:</strong> Unique subject identifier (random effect)</p>

<p><strong>Trial:</strong> Trial number (1:288) (random effect)</p>

<p><strong>Correct:</strong> Response classification, either incorrect (0) or correct (1) per trial</p>

<p>Note that the task design makes it inherently imbalanced, as trials without distractors cannot have Probe Location ""Probe at Distractor""; this makes R mad when I try to run RM ANOVAs, and it is another reason I opted for a regression.</p>

<p>Below is a sample of my data (with SubID altered, just in case anyone might get mad):</p>

<pre><code>     SubID      Dx Correct No_of_Stim Trial_Type Probe_Loc      Dist Trial
1 99999999 PROBAND       1          3          0         1 0.9217487     1
2 99999999 PROBAND       0          3          0         0 1.2808184     2
3 99999999 PROBAND       1          3          0         0 1.0645292     3
4 99999999 PROBAND       1          3          1         2 0.7838786     4
5 99999999 PROBAND       0          3          0         0 1.0968788     5
6 99999999 PROBAND       1          3          1         1 1.3076598     6
</code></pre>

<p>Hopefully, with the above variable descriptions these data should be self-explanatory.</p>

<p>Any assistance that people can provide in this matter is very much appreciated.</p>

<p>Sincerely,
peteralynn</p>
"
"0.132347737294141","0.109960706249747","115154","<p>Relatively new to stats. I use linear regression  and get R^2, which is quite low.</p>

<p><strong>MODEL 1</strong></p>

<pre><code>    lmoutar=lm(formula = ts_y ~ ts_y_lag + ts_x)
</code></pre>

<p>So switched to arima with external regressor. Using ""auto.arima"", I formulate arimax model</p>

<p><strong>MODEL 2</strong></p>

<pre><code>    fitarima &lt;- auto.arima(ts_y, xreg=ts_x)
    arimaout&lt;-arima(ts_y,order=c(2,0,5),xreg=ts_x)
</code></pre>

<p>How can I compare the explanability of AR model with arima model. From the thread <a href=""http://stats.stackexchange.com/questions/8750/how-can-i-calculate-the-r-squared-of-a-regression-with-arima-errors-using-r"">How can I calculate the R-squared of a regression with arima errors using R?</a>, I understand R^2 is not an option for ARIMA.</p>

<p>From the thread <a href=""http://stats.stackexchange.com/questions/11850/model-comparison-between-an-arima-model-and-a-regression-model"">Model comparison between an ARIMA model and a regression model</a>, AIC/BIC is not the right criteria and MSE from forcast/predict can be possible criteria for comparison across AR and ARIMA model. Is MSE the best option for model comparison, if so how would I generate MSE for AR and ARIMA?</p>

<p>I tried to compare the above ar and arima model using anova, but I get following error message</p>

<pre><code>anova.lm(lmoutar,arimaout)
   Warning message:
    In anova.lmlist(object, ...) :
            models with response â€˜""NULL""â€™ removed because response differs from model 1
</code></pre>

<p>What does this error message mean? </p>

<p><strong><em>EDIT</em></strong></p>

<p>Thanks for the response so far and insight that AR is nested within ARIMA. How would one answer this question, if I rephrase  as ""How to compare AR, ARIMA and General Linear Models?"". The first model I listed has AR(1) and independent variable; it is a general linear model. So how would I compare a GLM versus ARIMAX model? Any thing else besides MSE that I could use to judge between GLM and ARIMAX</p>
"
"0.11286652959662","0.114613651012251","115239","<p>How can I (1) compare two linear models between years and (2) Can I compare 2 models with different response variables?</p>

<p>My data have 4 variables: y_meas, x, year, y_calc.  ""y_meas"" is a lab measured response varibale, and ""y_calc"" is an estimate of the same variable, using a standard calculation.  ""x "" is a dosage, similar(ish) between two years:</p>

<pre><code>#create dataset
set.seed(100)
dat &lt;- within(data.frame(x = rep(1:10, times=2)),
                 {
                   year &lt;- rep(1990:1991, each = 10)
                   y_meas &lt;- 0.5 * x* (1:20) + rnorm(20)
         y_calc &lt;- 0.3 * x* (1:20) + rnorm(20)
                   year &lt;- factor(year)                 # convert to a factor
                 }
                 )
</code></pre>

<p>I have two related questions:
(1) Is there any difference between slope/intercept of models for 1990 and 1991?</p>

<pre><code>m.1990&lt;-lm(y_meas~x, data=subset(dat, year==1990))
m.1991&lt;-lm(y_meas~x, data=subset(dat, year==1991))
anova(m.1990)
anova(m.1991) 
# both models are significant
</code></pre>

<p>I can't run <code>anova(m.1990,m.1991)</code> because the models are not nested?  Do I need to use year as a dummy variable and run ANCOVA? What does this look like (roughly)?</p>

<p>(2) Assuming I can combine 1990 and 1991, can I compare the slope/intercept of 'y_meas~x' and 'y_calc~x'?   Yes, two different response variables, but are on the same scale.  </p>
"
"0.132809685425692","0.146104310758895","115304","<p>I am learning about building linear regression models by looking over someone elses R code.  Here is the example data I am using:</p>

<pre><code>v1  v2  v3  response
0.417655013 -0.012026453    -0.528416414    48.55555556
-0.018445979    -0.460809371    0.054017873 47.76666667
-0.246110341    0.092230159 0.057435968 49.14444444
-0.521980295    -0.428499038    0.119640369 51.08888889
0.633310578 -0.224215856    -0.153917427    48.97777778
0.41522316  0.050609412 -0.642394965    48.5
-0.07349941 0.547128578 -0.539018121    53.95555556
-0.313950353    0.207853678 0.713903994 48.16666667
0.404643796 -0.326782199    -0.785848428    47.7
0.028246796 -0.424323318    0.289313911 49.34444444
0.720822953 -0.166712488    0.323246062 50.78888889
-0.430825851    -0.308119827    0.543823856 52.65555556
-0.964175294    0.661700584 -0.11905972 51.03333333
-0.178955757    -0.11148414 -0.151179885    48.28888889
0.488388035 0.515903257 -0.087738159    48.68888889
-0.097527627    0.188292773 0.207321867 49.86666667
0.481853599 0.21142728  -0.226700254    48.38888889
1.139561277 -0.293574756    0.574855693 54.55555556
0.104077762 0.16075114  -0.131124443    48.61111111
</code></pre>

<p>I read in the data and use a call to <code>lm()</code> to build a model:</p>

<pre><code>&gt; my_data&lt;- read.table(""data.csv"", header = T, sep = "","")
&gt; my_lm &lt;- lm(response~v1 + v2 + v3 + v1:v2 + v1:v3 + v2:v3, data=my_data)
&gt; summary(my_lm)

Call:
lm(formula = response ~ v1 + v2 + v3 + v1:v2 + v1:v3 + v2:v3, 
data = my_data)

Residuals:
    Min      1Q  Median      3Q     Max 
-2.0603 -0.6615 -0.1891  1.0395  1.8280 

Coefficients:
         Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  49.33944    0.42089 117.226  &lt; 2e-16 ***
v1            0.06611    0.82320   0.080  0.93732    
v2           -0.36725    1.06359  -0.345  0.73585    
v3            0.72741    1.00973   0.720  0.48508    
v1:v2        -2.54544    2.21663  -1.148  0.27321    
v1:v3         0.80641    2.77603   0.290  0.77640    
v2:v3       -12.16017    3.62473  -3.355  0.00573 ** 
--- 
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 1.375 on 12 degrees of freedom
Multiple R-squared:  0.697, Adjusted R-squared:  0.5455 
F-statistic:   4.6 on 6 and 12 DF,  p-value: 0.01191
</code></pre>

<p>Following along with their code I then use a call to <code>anova()</code>:</p>

<pre><code>&gt; my_lm_anova &lt;- anova(my_lm)
&gt; my_lm_anova
Analysis of Variance Table

Response: response
          Df  Sum Sq Mean Sq F value   Pr(&gt;F)   
v1         1  0.0010  0.0010  0.0005 0.982400   
v2         1  0.2842  0.2842  0.1503 0.705036   
v3         1  9.8059  9.8059  5.1856 0.041891 * 
v1:v2      1  4.3653  4.3653  2.3084 0.154573   
v1:v3      1 16.4582 16.4582  8.7034 0.012141 * 
v2:v3      1 21.2824 21.2824 11.2545 0.005729 **
Residuals 12 22.6921  1.8910                    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>However, I am not sure:</p>

<ol>
<li>Why I would use the call to ANOVA in this situation, and</li>
<li>What the ANOVA table is telling me about the predictor variables.</li>
</ol>

<p>From the code they appear to use the ANOVA table as follows.  For predictor variable v1, the result of </p>

<ul>
<li>Adding the 'Sum Sq' entry for v1 together with half of the 'Sum Sq' entry for v1:v2 and half of the 'Sum Sq' entry for v1:v3, </li>
<li>Dividing by the sum of the entire 'Sum Sq' column, and</li>
<li>Multiplying by 100</li>
</ul>

<p>gives the percent of variance of the response variable that is explained by predictor variable v1 in the <code>lm()</code> model.  I don't see why this is nor why half of the 'Sum Sq' entry for v1:v2 is attributed to v1 and half to v2.  Is this just convenience?</p>
"
"0.11356975465314","0.128141957406415","115556","<p>I am sorry for stating such a question. Unfortunatelly, it's very-very hard to go through all statistics.</p>

<p>I have a crossover study design with 3 treatments, 3 periods, and a baseline (covariate).
The data table looks like this:</p>

<pre><code>SUBJECT     BASELINE   PERIOD1    PERIOD2    PERIOD3

subj_1      baseline   diet3      diet1      diet2   
subj_2      baseline   diet2      diet3      diet1
subj_3      baseline   diet1      diet2      diet3
subj_4      baseline   diet1      diet3      diet2
...         ...        ...        ...        ...
subj_27     baseline   diet2      diet1      diet3
</code></pre>

<p>PERIOD1, PERIOD2, PERIOD3 correspond to the time (week5, week11, week17). In between there is wash-out period.</p>

<p>My question is how to perform repeated measures ANOVA to such a experimental design?</p>

<p>I was trying to search for some examples and code but what I found out at the end is that I can apply mixed effect linear model to such data. Unfortunatelly, I did not find enough examples describing similar studies and the theory behind is very hard (I need time to understand).</p>

<p>I'm trying to find solution in R but SAS is also good. As I understood, in R there is a package <strong>nlme</strong> and I can apply a function <em>lme()</em> to fit an ANOVA model.</p>

<p>In order to do so, I found out that I have to divide my data according to treatment sequences (I will have 6 sequences: diet1-diet2-diet3, diet1-diet3-diet2, diet2-diet1-diet3, diet2-diet3-diet1, diet3-diet1-diet2, diet3-diet2-diet1).</p>

<p>Then I have to create a mixed effect linear model:</p>

<p>I assume (according to what I read and found) that probably it should look like this:</p>

<pre><code>output_value ~ period(fixed effect) + diet(fixed effect) + sequence(fixed_effect)
                   + subject(random_effect) + baseline_value(random_effect)
</code></pre>

<p>In R it probably looks something like:</p>

<pre><code>lme( output_value ~ period + diet + sequence, random ~ 1 | subject + baseline_value )
</code></pre>

<p>Is it right?</p>
"
"0.0691163516376137","0.0701862406343596","116487","<p>I need to predict payment day of the month (1-31) for each client (I have at most 9 month of payments and on average is 5). I have both categorical variables and numerical. I tried to use rpart to do a regression tree (method='anova') but I'm not sure if it's using the nominal variables. </p>

<p>I also tried a regression (linear to start) and doesn't work good either, but it's better then the regression tree.</p>

<p>If I use a Weibull for this, will it mean that each client is going to have a parameter of shape and scale? what about the other variables? How can I insert them into the distribution?</p>

<p>So, what model would you recommend?</p>

<p>Thanks</p>
"
"0.0399043442233811","0.0405220449236554","117489","<p>From my limited statistical knowledge, I could use MANOVA if I had multiple independent variables (x1, x2...xn). What can I do (specifically in R) with one ""x"" variable and multiple ""y"" groups? I'm trying to see if there is any relationship between the y's with respect to their regression with x. I've already set up a loop that computes bivariate, piecewise linear regressions between each pair (x-y1, x-y2, ... x-yn), but that does not include any analysis of variation between the y variables. Does anybody know how I might do this (in a statistically sound manner, of course) in R? My data looks like this:</p>

<pre><code>x         y1       y2      y3      y4      y5
4.19    5.51    19.76   50.00   19.36   54.07
8.60    10.16   33.01   82.99   38.48   44.95
8.03    7.82    31.29   79.05   40.12   59.18
6.64    8.99    27.13   69.13   30.44   59.02
7.03    8.22    25.29   74.45   36.02   50.88
1.50    5.90    10.69   22.88   10.34   34.50
4.36    7.61    19.27   44.47   20.06   24.62
7.17    8.30    26.72   68.68   31.61   20.16
2.68    5.61    14.25   37.07   15.20   67.75
7.91    7.75    30.93   82.01   38.62   65.36
3.74    5.24    16.42   40.17   17.54   15.19
</code></pre>
"
"0.0977452818676612","0.0827152778309109","117637","<p>I have problem with the diagnostic of the one way analysis of variance model (fitted in R).
I've checked all the assumptions of the analysis of variance</p>

<p>1) ""For each level of the within-subjects factor, the dependent variable must have a normal distribution."" (shapiro-wilk test) <a href=""http://en.wikipedia.org/wiki/Repeated_measures_design#Repeated_measures_ANOVA"" rel=""nofollow"">http://en.wikipedia.org/wiki/Repeated_measures_design#Repeated_measures_ANOVA</a>
Ok, so after there shapiro-wilk failure to reject, I bury my head in the sand and silently assume that this assumption is met.</p>

<p>2) there is homogeneity of the variance of the dependent variable in groups, there are only 2 groups (bartlett test)</p>

<p>3) all observations are independent</p>

<p>Then I fitted model with <code>aov()</code> function and after checking whether the p-value is greater or less than 0.05 (doesn't really matter now) I would like to check the <strong>quality of the fit</strong>.</p>

<p>So after I check if residuals have normal distribution and the answer is no, is there any solution? Because that means the quality is weak. For example box-cox transformation for dependent variable?</p>

<p>Should I also check the homogeneity of the variance for residuals ( motivation for this question is that ANOVA is some kind of a linear model )? Can I perform bartlett test again? And what if there is heterogeneity? Should then I use the weighted least squares method <strong>WLSM</strong> (the same as I would be a linear model)?</p>

<p>Are diagnostic plots for <code>aov()</code> function, which is similar to <code>lm()</code> (as I would perform <code>plot(lm(  formula ) )</code> are valid? Or only they are proper for linear model where explanatory variable are continuous?</p>

<p>Thanks for help! </p>
"
"0.0798086884467622","0.0810440898473108","118391","<p>I have a data set where I measured the number of molecules (M) present in cells as a function of drug (with or without) and days of treatment (5 timepoints). I repeated the experiment 3 times, with cells from a separate donor each time. I am currently trying to compare the means between groups. However, the data are not normal and heteroskedastic and I'm in the process of figuring out how to best deal with this.</p>

<p>Transforming the data makes the dataset normal, but the heteroskedacity remains. I'm a stats novice, but my reading over the past several days suggests that a linear mixed model should be able to deal with this. Based primarily on the Pinhiero/Bates book, I have cobbled together the following models using lme in R; they are the same except for the varPower statement. </p>

<pre><code>model1&lt;-lme(sqrt(M) ~ drug + Days + drug*Days, 
            random = ~ 1+drug+Days+drug*Days|Donor, data=D)

model2&lt;-lme(sqrt(M) ~ drug + Days + drug*Days,
            random = ~ 1+drug+Days+drug*Days|Donor, data=D, varPower(form = ~fitted(.)) )
</code></pre>

<p>When I compare these two models using anova(), model2 has a significantly increased log likelihood. However, when I examine the standardized residuals plotted against either fitted values or the independent variables, the graphs for the two models have identical shape. Note that the magnitude of the residuals is slightly greater with varPower() included:</p>

<pre><code>plot(model1, resid(., type=""p"") ~ fitted(.), abline=0)
plot(model1, resid(., type=""p"") ~ Days|drug, abline=0)
</code></pre>

<p><img src=""http://i.imgur.com/vcFwduT.png"" alt=""residual plots""></p>

<p>Does the similarity between these plots mean that I have failed to sufficiently account for the unequal variances between groups?</p>

<p>If so, what approaches might yield sufficient correction?  Additionally, if you have any general comments about the suitability of lme here, or the structure of the model, those would be welcome as well!</p>
"
"0.119713032670143","0.108058786463081","118475","<p>I have an unbalanced linear mixed effects model with three fixed factors of various levels and one random factor for my repeated measures data (<a href=""http://stats.stackexchange.com/questions/99742/how-to-analyze-interdependent-interaction-terms-of-lmer-model"">for details see here</a>).
Thanks to your help I managed to do post-hoc tests on the significant interaction terms using <code>lsm</code> from the <strong>lsmeans</strong> package. However, I need to report the F statistic (F value and degrees of freedom) for these post-hoc tests and wonder how???</p>

<p>Here is what I do:</p>

<ol>
<li><p>Model comparison using <code>anova()</code> resulting into the final model
<code>model_final</code>, which reads:</p>

<p><code>sc ~ time + cond + place + time:cond + cond:place + (1|ID), data)</code> . </p></li>
<li><p>I analyze the significant interaction time:cond using <code>lsmeans</code>:</p>

<p><code>posthoc_1 &lt;- glht(model_final, lsm(pairwise ~ cond|time)</code></p>

<p><code>summary(posthoc_1)</code></p></li>
</ol>

<p>and get sth like below for each level of <code>time</code>, here is the example for <code>time1</code>.</p>

<pre><code>&gt; Note: df set to 268 
&gt;
&gt; $`time = time1`
&gt; 
&gt;    Simultaneous Tests for General Linear Hypotheses
&gt; 
&gt; Fit: lme4::lmer(formula = sc ~ time + cond + place + time:cond + cond:place + (1|ID), data)
&gt; 
&gt; Linear Hypotheses:
&gt;                    Estimate Std. Error t value Pr(&gt;|t|) 
&gt; cond1 - cond2 == 0   3.1867     0.6797   4.688 4.39e-06 ***
</code></pre>

<p>This gives me t-values for the various levels of the interaction terms and their corresponding p-value, but no F stats!</p>

<p>My questions: </p>

<ol>
<li>Is there any way of obtaining the F stats? (F value and degree of
freedom) </li>
<li>Or am I stuck with the t-values? If so, is t(0.095;268) =
4.588, p &lt; 0.001 reporting the correct degrees of freedom?</li>
</ol>
"
"0.169534769933959","0.163098084665869","120650","<p>Field explains how to analyse repeated-measures data using linear mixed-effect models (LME). See Field et al., Discovering Statistics Using R, 2012, p. 573.</p>

<p>However, the way he specifies the model, there appears to be <strong>only one observation per level of the grouping factors</strong>. Is this a mistake in the textbook? If not, why not? It seems to me the random effects specify a full model and fit the data (almost) exactly.</p>

<p>The code is as follows:</p>

<pre><code>library(reshape2)
library(nlme)
# Load dataset:
dat.wide &lt;- read.delim(""http://www.sagepub.com/dsur/study/DSUR%20Data%20Files/Chapter%2013/Bushtucker.dat"")

dat.wide
#   participant stick_insect kangaroo_testicle fish_eye witchetty_grub
# 1          P1            8                 7        1              6
# 2          P2            9                 5        2              5
# 3          P3            6                 2        3              8
# 4          P4            5                 3        1              9
# 5          P5            8                 4        5              8
# 6          P6            7                 5        6              7
# 7          P7           10                 2        7              2
# 8          P8           12                 6        8              1


dat &lt;- melt(dat.wide, variable.name=""animal"", value.name=""retch"")
head(dat)
#   participant       animal retch
# 1          P1 stick_insect     8
# 2          P2 stick_insect     9
# ...

# set contrasts, not relevant to question but keeps example same as in book:
PartvsWhole &lt;- c(1, -1, -1, 1)
TesticlevsEye &lt;- c(0, -1, 1, 0)
StickvsGrub &lt;- c(-1, 0, 0, 1)
contrasts(dat$animal) &lt;- cbind(PartvsWhole, TesticlevsEye, StickvsGrub)

# Fit intercept term, then add ""animal"" term.
# NOTE: random effects are animal nested within participant, as in textbook.
# This would presumably give one observation per group?    
lme1 &lt;- lme(retch ~ 1, random=~1|participant/animal, data=dat, method=""ML"")
lme2 &lt;- lme(retch ~ 1 + animal, random=~1|participant/animal, data=dat, method=""ML"")

# I have checked these are the same results as in textbook
anova(lme1, lme2)    
summary(lme2)

# residuals are near-zero (e-05)
resid(lme2)

ran1 &lt;- random.effects(lme1)
ran2 &lt;- random.effects(lme2)
# same number of random effects as observations:
nrow(ran1$animal)
    nrow(ran2$animal)
nrow(dat)
</code></pre>

<p>The concern of one observation per level seems to be confirmed by using package lme4 instead:</p>

<pre><code>library(lme4)
# Using lme4 produces an error:
# ""Error in checkNlevels(reTrms$flist, n = n, control) : 
# number of levels of each grouping factor must be &lt; number of observations""    
lmer1 &lt;- lmer(retch ~ 1 + (1|participant/animal), data=dat, REML=F)
lmer2 &lt;- lmer(retch ~ 1 + animal + (1|participant/animal), data=dat, REML=F)
anova(lmer1, lmer2)

# see http://stackoverflow.com/questions/19713228/lmer-returning-error-grouping-factor-must-be-number-of-observations
# can force fit with lme4:
# control=lmerControl(check.nobs.vs.nlev = ""ignore"",
#                     check.nobs.vs.rankZ = ""ignore"",
#                     check.nobs.vs.nRE=""ignore""))

lmer1 &lt;- lmer(retch ~ 1 + (1|participant/animal), data=dat, REML=F,
              control=lmerControl(check.nobs.vs.nlev=""ignore"",
                                  check.nobs.vs.rankZ=""ignore"",
                                  check.nobs.vs.nRE=""ignore""))
lmer2 &lt;- lmer(retch ~ 1 + animal + (1|participant/animal), data=dat, REML=F,
              control=lmerControl(check.nobs.vs.nlev=""ignore"",
                                  check.nobs.vs.rankZ=""ignore"",
                                  check.nobs.vs.nRE=""ignore""))
# ignoring errors, we get the same results as with lme:
anova(lmer1, lmer2)
anova(lme1, lme2) # same
</code></pre>

<p>Should the random term should be 1|participant instead?</p>
"
"0.132347737294141","0.134396418749691","120768","<p>I'm using <code>glmer()</code> with a binomial response variable. My optimal model has two fixed effects (flow and DNA) which in summary() show a non-significant p value but when I remove each fixed effect in turn from the model the likelihood ratio test comparing the two models shows a significant p value. I'm struggling to understand (1) if this is normal, and (2) how to report the results if the explanatory variables ""flow"" and ""DNA"" are important but their p values in the model are well above 0.05?</p>

<p>Optimal model:</p>

<pre><code>a25 &lt;- glmer(Status_qpcr~(1|Root)+Flow+DNA,
             family=binomial, data=spore)
summary(a25)

Generalized linear mixed model fit by maximum likelihood (Laplace
Approximation) ['glmerMod']  
Family: binomial  ( logit ) 
Formula: Status_qpcr ~ (1 | Root) + Flow + DNA   
Data: spore
      AIC      BIC   logLik deviance df.resid 
     72.9     81.0    -32.4     64.9       52 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.9318 -0.8163  0.4435  0.6848  1.6133 

Random effects:  
  Groups Name        Variance Std.Dev.  
  Root   (Intercept) 0.3842   0.6199   
  Number of obs: 56, groups:  Root, 9

Fixed effects:
Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept) -0.97752    0.79252  -1.233    0.217   
Flow         3.82779    2.27165   1.685    0.092 . 
DNA          0.01616    0.01039   1.556    0.120  
--- 
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
     (Intr) Flow   Flow -0.775        
     DNA    -0.576  0.227
</code></pre>

<p>Likelihood ratio test:</p>

<pre><code>a26 &lt;- update(a25,~.-DNA)
anova(a25,a26)

Data: spore 
Models: 
    a26: Status_qpcr ~ (1 | Root) + Flow 
    a25: Status_qpcr ~ (1 | Root) + Flow + DNA
    Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)   
a26  3 74.802 80.878 -34.401   68.802                            
a25  4 72.897 80.998 -32.448   64.897 3.9049      1    0.04815 *

a27 &lt;- update(a25,~.-Flow)
anova(a25,a27)

Data: spore 
Models: 
    a27: Status_qpcr ~ (1 | Root) + DNA 
    a25: Status_qpcr ~ (1 | Root) + Flow + DNA
    Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)
a27  3 78.440 84.723 -36.220   72.440                             
a25  4 72.897 80.998 -32.448   64.897 7.5427      1   0.006025 **
</code></pre>
"
"0.187554277031098","0.183403547465877","122717","<p>I have some trouble obtaining equivalent results between an <code>aov</code> between-within repeated measures model and an <code>lmer</code> mixed model.</p>

<p>My data and script look as follows</p>

<pre><code>data=read.csv(""https://www.dropbox.com/s/zgle45tpyv5t781/fitness.csv?dl=1"")
data$id=factor(data$id)
data
   id  FITNESS      TEST PULSE
1   1  pilates   CYCLING    91
2   2  pilates   CYCLING    82
3   3  pilates   CYCLING    65
4   4  pilates   CYCLING    90
5   5  pilates   CYCLING    79
6   6  pilates   CYCLING    84
7   7 aerobics   CYCLING    84
8   8 aerobics   CYCLING    77
9   9 aerobics   CYCLING    71
10 10 aerobics   CYCLING    91
11 11 aerobics   CYCLING    72
12 12 aerobics   CYCLING    93
13 13    zumba   CYCLING    63
14 14    zumba   CYCLING    87
15 15    zumba   CYCLING    67
16 16    zumba   CYCLING    98
17 17    zumba   CYCLING    63
18 18    zumba   CYCLING    72
19  1  pilates   JOGGING   136
20  2  pilates   JOGGING   119
21  3  pilates   JOGGING   126
22  4  pilates   JOGGING   108
23  5  pilates   JOGGING   122
24  6  pilates   JOGGING   101
25  7 aerobics   JOGGING   116
26  8 aerobics   JOGGING   142
27  9 aerobics   JOGGING   137
28 10 aerobics   JOGGING   134
29 11 aerobics   JOGGING   131
30 12 aerobics   JOGGING   120
31 13    zumba   JOGGING    99
32 14    zumba   JOGGING    99
33 15    zumba   JOGGING    98
34 16    zumba   JOGGING    99
35 17    zumba   JOGGING    87
36 18    zumba   JOGGING    89
37  1  pilates SPRINTING   179
38  2  pilates SPRINTING   195
39  3  pilates SPRINTING   188
40  4  pilates SPRINTING   189
41  5  pilates SPRINTING   173
42  6  pilates SPRINTING   193
43  7 aerobics SPRINTING   184
44  8 aerobics SPRINTING   179
45  9 aerobics SPRINTING   179
46 10 aerobics SPRINTING   174
47 11 aerobics SPRINTING   164
48 12 aerobics SPRINTING   182
49 13    zumba SPRINTING   111
50 14    zumba SPRINTING   103
51 15    zumba SPRINTING   113
52 16    zumba SPRINTING   118
53 17    zumba SPRINTING   127
54 18    zumba SPRINTING   113
</code></pre>

<p>Basically, 3 x 6 subjects (<code>id</code>) were subjected to three different <code>FITNESS</code> workout schemes each and their <code>PULSE</code> was measured after carrying out three different types of endurance <code>TEST</code>s.</p>

<p>I then fitted the following <code>aov</code> model :</p>

<pre><code>library(afex)
library(car)
set_sum_contrasts()
fit1 = aov(PULSE ~ FITNESS*TEST + Error(id/TEST),data=data)
summary(fit1)
Error: id
          Df Sum Sq Mean Sq F value   Pr(&gt;F)    
FITNESS    2  14194    7097   115.1 7.92e-10 ***
Residuals 15    925      62                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Error: id:TEST
             Df Sum Sq Mean Sq F value   Pr(&gt;F)    
TEST          2  57459   28729   253.7  &lt; 2e-16 ***
FITNESS:TEST  4   8200    2050    18.1 1.16e-07 ***
Residuals    30   3397     113                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>The result I obtain using</p>

<pre><code>set_sum_contrasts()
fit2=aov.car(PULSE ~ FITNESS*TEST+Error(id/TEST),data=data,type=3,return=""Anova"")
summary(fit2)
</code></pre>

<p>is identical to this.</p>

<p>A mixed model run using <code>nlme</code> gives a directly equivalent result, e.g. using <code>lme</code> :</p>

<pre><code>library(lmerTest)    
lme1=lme(PULSE ~ FITNESS*TEST, random=~1|id, correlation=corCompSymm(form=~1|id),data=data)
anova(lme1)
             numDF denDF   F-value p-value
(Intercept)      1    30 12136.126  &lt;.0001
FITNESS          2    15   115.127  &lt;.0001
TEST             2    30   253.694  &lt;.0001
FITNESS:TEST     4    30    18.103  &lt;.0001


summary(lme1)
Linear mixed-effects model fit by REML
 Data: data 
       AIC      BIC    logLik
  371.5375 393.2175 -173.7688

Random effects:
 Formula: ~1 | id
        (Intercept) Residual
StdDev:    1.699959 9.651662

Correlation Structure: Compound symmetry
 Formula: ~1 | id 
 Parameter estimate(s):
       Rho 
-0.2156615 
Fixed effects: PULSE ~ FITNESS * TEST 
                                 Value Std.Error DF   t-value p-value
(Intercept)                   81.33333  4.000926 30 20.328628  0.0000
FITNESSpilates                 0.50000  5.658164 15  0.088368  0.9308
FITNESSzumba                  -6.33333  5.658164 15 -1.119327  0.2806
TESTJOGGING                   48.66667  6.143952 30  7.921069  0.0000
TESTSPRINTING                 95.66667  6.143952 30 15.570868  0.0000
FITNESSpilates:TESTJOGGING   -11.83333  8.688861 30 -1.361897  0.1834
FITNESSzumba:TESTJOGGING     -28.50000  8.688861 30 -3.280062  0.0026
FITNESSpilates:TESTSPRINTING   8.66667  8.688861 30  0.997446  0.3265
FITNESSzumba:TESTSPRINTING   -56.50000  8.688861 30 -6.502579  0.0000
</code></pre>

<p>Or using <code>gls</code> :</p>

<pre><code>library(lmerTest)    
gls1=gls(PULSE ~ FITNESS*TEST, correlation=corCompSymm(form=~1|id),data=data)
anova(gls1)
</code></pre>

<p>However, the result I obtain using <code>lme4</code>'s <code>lmer</code> is different :</p>

<pre><code>set_sum_contrasts()
fit3=lmer(PULSE ~ FITNESS*TEST+(1|id),data=data)
summary(fit3)
Linear mixed model fit by REML ['lmerMod']
Formula: PULSE ~ FITNESS * TEST + (1 | id)
   Data: data

REML criterion at convergence: 362.4

Random effects:
 Groups   Name        Variance Std.Dev.
 id       (Intercept)  0.00    0.0     
 Residual             96.04    9.8     
...

Anova(fit3,test.statistic=""F"",type=3)
Analysis of Deviance Table (Type III Wald F tests with Kenward-Roger df)

Response: PULSE
                    F Df Df.res    Pr(&gt;F)    
(Intercept)  7789.360  1     15 &lt; 2.2e-16 ***
FITNESS        73.892  2     15 1.712e-08 ***
TEST          299.127  2     30 &lt; 2.2e-16 ***
FITNESS:TEST   21.345  4     30 2.030e-08 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Anybody any thoughts what I am doing wrong with the <code>lmer</code> model? Or where the difference comes from? Could it have to do anything with <code>lmer</code> not allowing negative intraclass corellations or something like that? Given that <code>nlme</code>'s <code>gls</code> and <code>lme</code> do return the correct result, though, I am wondering how this is different in <code>gls</code> and <code>lme</code>? Is it that the option <code>correlation=corCompSymm(form=~1|id)</code> causes them to  directly estimate the intraclass correlation, which can be either positive or negative, whereas <code>lmer</code> estimates a variance component, which cannot be negative (and ends up being estimated as zero in this case)?</p>
"
"0.105576971046183","0.107211253483779","123999","<p>I'm analysing count data with a generalised linear model in R. I started with a Poisson family distribution, but then realized that data was clearly overdispersed. I then took the option of applying a glm with negative binomial distribution (I'm using the function <code>glm.nb()</code> from MASS package). Interestingly, I get the same best-selected model with a forward and a backward stepwise selection approach, which is: </p>

<pre><code>m.step2 &lt;- glm.nb(round(N.FLOWERS) ~ Hs_obs+RELATEDNESS+CLONALITY+PRODUCTION, data = flower[c(-12, -17), ])
</code></pre>

<p>Then to test for fixed effects I use the anova() function, which gives:</p>

<pre><code>anova(m.step2, test = ""Chi"")
Analysis of Deviance Table
Model: Negative Binomial(1.143), link: log
Response: round(N.FLOWERS)
Terms added sequentially (first to last)
              Df Deviance Resid. Df  Resid. Dev   Pr(&gt;F)   

 NULL                           15     40.674                   
 Hs_obs       1   9.5978        14     31.076    0.001948 **
 RELATEDNESS  1   9.4956        13     21.581    0.002060 **
 CLONALITY    1   3.0411        12     18.540    0.081181 . 
 PRODUCTION   1   3.7857        11     14.754    0.051693 .
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
 Warning messages: 
 1: In anova.negbin(m.step2, test = ""F"") : tests made without re-estimating 'theta'
</code></pre>

<p>However, if there were overdispersion (even with the negative binomial) these p-values should be corrected, shouldn't they? In my case, the residual deviance (obtained from the <code>summary(m.step2)</code>) is 14.754 and residual degrees of freedom 11. Thus, overdispersion is 14.754/11 = 1.34. </p>

<p>How do I correct the p-values to account for the small amount of overdispersion detected in this negative binomial model?</p>
"
"0.145173376018241","0.167076671386255","124581","<p>I was doing some log-linear models to test for interactions/associations in multiway contingency tables (based on the tutorial here, <a href=""http://ww2.coastal.edu/kingw/statistics/R-tutorials/loglin.html"" rel=""nofollow"">http://ww2.coastal.edu/kingw/statistics/R-tutorials/loglin.html</a>). I was doing this using a Poisson <code>glm</code> on the observed frequencies as well as with <code>MASS</code>'s <code>loglm</code>. I was just wondering though what type of hypothesis test would make most sense here, sequential type I using <code>anova()</code> (not good since p values there depend on the order of the factors in the model), type III test using <code>Anova()</code> in <code>car</code> (independent of the order of the factors in the model) or using <code>drop1</code> starting from the most complex model? </p>

<p>E.g. using the Titanic passenger survival data</p>

<pre><code>library(COUNT)
data(titanic)
titanic=droplevels(titanic)
head(titanic)
mytable=xtabs(~class+age+sex+survived, data=titanic)
ftable(mytable)
                       survived  no yes
class     age    sex                   
1st class child  women            0   1
                 man              0   5
          adults women            4 140
                 man            118  57
2nd class child  women            0  13
                 man              0  11
          adults women           13  80
                 man            154  14
3rd class child  women           17  14
                 man             35  13
          adults women           89  76
                 man            387  75
freqdata=data.frame(mytable)
fullmodel=glm(Freq~SITE*SEX*MORTALITY,family=poisson,data=freqdata)
</code></pre>

<p>Would the most sensible test for interactions between the different categorical factors then be given by type I SS as in</p>

<pre><code>anova(fullmodel, test=""Chisq"")
Analysis of Deviance Table

Model: poisson, link: log

Response: Freq

Terms added sequentially (first to last)


                       Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                                      23    2173.33              
class                   2   231.18        21    1942.15 &lt; 2.2e-16 ***
age                     1  1072.61        20     869.54 &lt; 2.2e-16 ***
sex                     1   137.74        19     731.80 &lt; 2.2e-16 ***
survived                1    77.61        18     654.19 &lt; 2.2e-16 ***
class:age               2    32.41        16     621.78 9.178e-08 ***
class:sex               2    29.61        14     592.17 3.719e-07 ***
age:sex                 1     6.09        13     586.09   0.01363 *  
class:survived          2   132.69        11     453.40 &lt; 2.2e-16 ***
age:survived            1    25.58        10     427.81 4.237e-07 ***
sex:survived            1   312.93         9     114.89 &lt; 2.2e-16 ***
class:age:sex           2     4.04         7     110.84   0.13250    
class:age:survived      2    35.45         5      75.39 2.002e-08 ***
class:sex:survived      2    73.71         3       1.69 &lt; 2.2e-16 ***
age:sex:survived        1     1.69         2       0.00   0.19421    
class:age:sex:survived  2     0.00         0       0.00   1.00000 
</code></pre>

<p>or using type III SS using <code>car</code>'s <code>Anova</code> :</p>

<pre><code>library(car)
library(afex)
set_sum_contrasts()
Anova(fullmodel, test=""LR"", type=""III"")
Analysis of Deviance Table (Type III tests)

Response: Freq
                       LR Chisq Df Pr(&gt;Chisq)    
class                    37.353  2  7.744e-09 ***
age                       5.545  1  0.0185317 *  
sex                       0.000  1  0.9999999    
survived                  1.386  1  0.2390319    
class:age                 5.476  2  0.0646851 .  
class:sex                 0.000  2  1.0000000    
age:sex                   0.000  1  0.9999888    
class:survived           16.983  2  0.0002052 ***
age:survived              0.056  1  0.8126973    
sex:survived              0.000  1  0.9999953    
class:age:sex             0.000  2  1.0000000    
class:age:survived        3.461  2  0.1771673    
class:sex:survived        0.000  2  1.0000000    
age:sex:survived          0.000  1  0.9999905    
class:age:sex:survived    0.000  2  1.0000000    
</code></pre>

<p>or using single term deletions and LRTs with <code>drop1</code> :</p>

<pre><code>fullmodel=glm(Freq~class+age+sex+survived+class:age+class:sex+class:survived+age:sex+age:survived+sex:survived, family=poisson, data=freqdata)
drop1(fullmodel,test=""Chisq"")
Single term deletions

Model:
Freq ~ class + age + sex + survived + class:age + class:sex + 
    class:survived + age:sex + age:survived + sex:survived
               Df Deviance    AIC     LRT  Pr(&gt;Chi)    
&lt;none&gt;              114.89 249.01                      
class:age       2   162.76 292.89  47.877 4.016e-11 ***
class:sex       2   115.74 245.86   0.850    0.6537    
class:survived  2   230.95 361.08 116.067 &lt; 2.2e-16 ***
age:sex         1   114.89 247.02   0.008    0.9294    
age:survived    1   134.39 266.52  19.505 1.003e-05 ***
sex:survived    1   427.81 559.94 312.927 &lt; 2.2e-16 ***
</code></pre>

<p>?</p>

<p>[This last result appears to match that of <code>MASS</code>'s <code>loglm</code>, as should be the case :</p>

<pre><code>fullmodel=loglm(~class+age+sex+survived+class:age+class:sex+class:survived+age:sex+age:survived+sex:survived, mytable)
stepAIC(fullmodel) 
drop1(fullmodel,test=""Chisq"")
Single term deletions

Model:
~class + age + sex + survived + class:age + class:sex + class:survived + 
    age:sex + age:survived + sex:survived
               Df    AIC     LRT  Pr(&gt;Chi)    
&lt;none&gt;            144.89                      
class:age       2 188.76  47.877 4.016e-11 ***
class:sex       2 141.74   0.850    0.6537    
class:survived  2 256.95 116.067 &lt; 2.2e-16 ***
age:sex         1 142.89   0.008    0.9294    
age:survived    1 162.39  19.505 1.003e-05 ***
sex:survived    1 455.81 312.927 &lt; 2.2e-16 ***
</code></pre>

<p>]</p>

<p>(Any other more elegant ways btw to specify a model with main effects + all first order interaction effects?)</p>

<p>Any thoughts what would be the best way to analyse such multiway contingency tables, and adequately test for associations for unbalanced data sets?</p>

<p>EDIT: based on the answer below I went for the <code>drop1</code> solution :</p>

<pre><code>fullmodel=glm(Freq~class+age+sex+survived+class:age+class:sex+class:survived+age:sex+age:survived+sex:survived, family=poisson, data=freqdata)
drop1(fullmodel,test=""Chisq"")
</code></pre>

<p>which is equivalent to the log-linear model in <code>MASS</code> :</p>

<pre><code>fullmodel=loglm(~class+age+sex+survived+class:age+class:sex+class:survived+age:sex+age:survived+sex:survived, mytable)
stepAIC(fullmodel) 
drop1(fullmodel,test=""Chisq"")
</code></pre>
"
"0.0892288262810312","0.0906100470365937","125465","<p>I'm searching for a built-in function in R for calculating the required sample size (given certain power, alpha,..) of a mixed ANOVA (2 between, 1 within variable, 2x2x2 design). Does this function exist? I didn't find it in the following packages:</p>

<p>â€¢pwr is the oldest power-analysis library; some introductory info can be found on Quick-R</p>

<p>â€¢PoweR: Computation of power and level tables for hypothesis tests</p>

<p>â€¢Power2Stage: Power and Sample size distribution of 2-stage BE studies via simulations</p>

<p>â€¢powerAnalysis: Power analysis in experimental design</p>

<p>â€¢powerGWASinteraction: Power Calculations for Interactions for GWAS</p>

<p>â€¢powerMediation: Power/Sample size calculation for mediation analysis, simple linear 
regression, logistic regression, or longitudinal study</p>

<p>â€¢powerpkg: Power analyses for the affected sib pair and the TDT design</p>

<p>â€¢powerSurvEpi: Power and sample size calculation for survival analysis of epidemiological studies</p>

<p>â€¢PowerTOST: Power and Sample size based on two one-sided t-tests (TOST) for (bio)equivalence studies</p>

<p>â€¢longpower: Power and sample size for linear model of longitudinal data</p>

<p>Thanks!</p>
"
"0","0","125723","<p>I am trying to run a Post Hoc test (<code>glht</code>) after a linear mixed model (<code>lme</code>) in <code>R</code>.
I was searching for a answer but couldn't find a helpful one.</p>

<p>Maybe I have some expectations to the model which my data doesn't support?</p>

<p>Here is my code:</p>

<pre><code>library(nlme)
library(multcomp)

anova(mod &lt;- lme(Yield~Crop, random=~1|ID, data=DFField, method=""ML"",na.action=na.omit))

            numDF denDF  F-value p-value
(Intercept)     1    53 948.9091  &lt;.0001
Crop            2    15  14.7678   3e-04

glht(mod, linfct=mcp(Crop=""Tukey""))
</code></pre>

<blockquote>
  <p>Error in glht.matrix(model = list(modelStruct = list(reStruct = list(ID = -9.6584044661227)),  : 
    â€˜ncol(linfct)â€™ is not equal to â€˜length(coef(model))â€™</p>
</blockquote>

<p>DF:</p>

<pre><code>ID  Crop    Yield
1   RMix    2294.36
1   RMix    4585.23
1   RMix    5979
1   RMix    6243.64
8   RMix    4396.24
8   RMix    5324.89
8   RMix    7266.89
27  MMix    7709.27
27  MMix    9342.2
27  MMix    5871.42
27  MMix    6244.97
35  MMix    9689.6
35  MMix    9449.34
35  MMix    7226.16
35  MMix    6090.7
2   RWet    6466.69
2   RWet    6301.73
2   RWet    6228.92
2   RWet    6093.93
9   RWet    7638.95
9   RWet    5047.91
9   RWet    6069.68
9   RWet    4827.53
21  MMix    6674.9
21  MMix    8106.99
21  MMix    6904.2
21  MMix    5708.12
22  MMix    6385.65
22  MMix    11056.43
22  MMix    5356.61
22  MMix    5631.58
3   RMix    3571.98
3   RMix    5293.26
3   RMix    5861.77
3   RMix    6483.02
11  RMix    4342.29
11  RMix    5470.5
11  RMix    5735.8
11  RMix    6083.33
23  RWet    6609.08
23  RWet    8229.66
23  RWet    6774.49
23  RWet    5917.72
33  RWet    7070.02
33  RWet    7309.89
33  RWet    5948.05
33  RWet    5784.07
4   MMix    15195.57
4   MMix    9117.13
4   MMix    5881.72
4   MMix    5431.84
14  MMix    11432.33
14  MMix    11052.94
14  MMix    5013.5
14  MMix    6682.77
25  RMix    2791.01
25  RMix    3825.83
25  RMix    6710.05
25  RMix    5285.8
26  RMix    2051.71
26  RMix    3115.92
26  RMix    6755.08
26  RMix    5568.8
15  RWet    7584.63
15  RWet    7213.5
15  RWet    5729.61
15  RWet    7140.97
16  RWet    6637.91
16  RWet    7791.15
16  RWet    5199.39
16  RWet    6801.6
</code></pre>
"
"0.143877159211166","0.134865517623595","125787","<p>I would like to learn what is the correct way to approach analysis of this data. I have done some reading on the subject, but I still feel uncertain. Perhaps many approaches are valid, but simply  that some are more conservative than others?</p>

<p>My study: </p>

<p>I have 5 grasslands, and in each grassland I have 30 spiders. For each spider I have an estimate of what proportion of herbivores it consumes ""Diet"" (so 5 x 30, n = 150). For each grassland I also have an estimate of the overall biomass of herbivores that exist there ""Biomass"". Thus I have 5 values of ""Biomass"" (one for each grassland) and 150 of ""Diet"" (30 spiders per grassland). Both Diet and Biomass are continous variables. </p>

<p>I would like to run an anlysis that tests how Diet changes across Biomass and derive a slope value, thus keeping Biomass as a continous variable:</p>

<p>Diet ~ Biomass</p>

<p>As I understand it, if I use raw data for Diet (n=150) then using anova is more approrpiate, and grassland becomes a factor with 5 levels.</p>

<p>Or I could run it as a linear regression and thus keep Biomass as a continuous variable and derive a slope value. However, as a linear regression, should I use the raw data (n=150) or mean values (so 5 means - one for each grassland based on 30 samples). Which of the 2 linear regression approaches is correct? (means or raw data). </p>

<p>While I am familiar with the notion that both anova and regression have the same underlying mathematics and are now regarded as general linear modelling, I still don't know how this affects the data that I should be using when running a linear model of the form:  Diet ~ Biomass</p>

<p>Using raw data seems better because it captures the variability in the dataset, but if i use it with Biomass as a continous variable to get a slope value (i.e regression analysis) I am concerned that it inflates the degrees of freedom (df=1,149) and is psuedo-replicated, so inaccurately increases my chances of a significant result? Therefore, is it incorrect to model the raw data (n=150) against only 5 values of ""Biomass"" in a linear form (and not as factors as required in an anova)?</p>
"
"0.028216632399155","0.0573068255061253","126510","<p>How do we do two-way ANOVA (one observation per mean), as testing H_A in Section 8.5 in Seber and Lee's Linear Regression Analysis, in R?
Note that the linear model for this case doesn't have interaction between the row and column factors.</p>

<p>For example, I want to test in the following 3 x 2 table, if the mean of each row is the same. </p>

<p>5 | 4<br>
7 | 6<br>
4 | 7  </p>

<p>Note that I used <code>lm</code> for one-way ANOVA, but couldn't find out which function and arguments to do two-way ANOVA (one observation per mean). I am not trying to implement it in R.</p>

<p>Thanks.</p>
"
"0.140790963210591","0.127920832207306","127134","<p><strong>Updated</strong></p>

<p>I have been developing a logistic regression model based on retrospective data from a national trauma database of head injury in the UK. The key outcome is 30 day mortality (denoted as ""Survive"" measure). Other measures with published evidence of significant effect on outcome in previous studies include:</p>

<pre><code>Yeardecimal - Date of procedure (expressed as decimal of year) = 1994-2013
Age - Age of patient = 16.0-101.5
ISS - Injury Severity Score = 16-75
Sex - Gender of patient = Male or Female
inctoCran - Time from head injury to craniotomy in minutes = 0-2880 (After 2880 minutes is defined as a separate diagnosis)
Mechanism - Mechanism of injury = Fall &lt;2m, Fall &gt;2m, Shooting/stabbing, RTC (Road Traffic Collision), Other
neuroFirst - Location of first admission (Neurosurgical Unit) = NSU vs. Non-NSU
rcteye - Pupil reactivity = NA / Both unreactive = O, 1 reactive = 1, both reactive = 2
rcteyeYN - dummy = 0 or 1 for presence or absence of data
GCS - Glasgow Coma Scale = 3-15
GCSYN - dummy = 0 or 1 for presence or absence of data
</code></pre>

<p>Dummy variables were included to enable a larger sample size where the majority of cases were excluding  <code>GCS</code> and <code>rcteye</code> variables (missing not at random).</p>

<p>In order to test for interactions, initially I ran the following:</p>

<pre><code>rcs.ASDH&lt;-lrm(formula = Survive ~ (rcs(Age) + rcs(ISS) + 
     neuroFirst + Mechanism + rcteye + rcteyeYN + GCS + GCSYN + rcs(Yeardecimal))^2, data = ASDH_Paper1.1)
</code></pre>

<p>but when I did I got the following error:</p>

<pre><code>singular information matrix in lrm.fit (rank= 151 ).  Offending variable(s):
GCSYN * Yeardecimal''' GCSYN * Yeardecimal' GCSYN * Yeardecimal GCS * Yeardecimal''' GCS * Yeardecimal GCS * GCSYN rcteyeYN * Yeardecimal''' rcteyeYN * Yeardecimal'' rcteyeYN * Yeardecimal rcteyeYN * GCSYN rcteye * Yeardecimal''' rcteye * Yeardecimal rcteye * rcteyeYN Mechanism=RTC * Yeardecimal''' Mechanism=Other * Yeardecimal''' Mechanism=Fall &gt; 2m * Yeardecimal''' Mechanism=Shooting / Stabbing * Yeardecimal Mechanism=RTC * Yeardecimal Mechanism=Other * Yeardecimal Mechanism=Fall &gt; 2m * Yeardecimal neuroFirst * Yeardecimal ISS'' * Yeardecimal''' ISS * Yeardecimal''' ISS'' * Yeardecimal'' ISS'' * Yeardecimal ISS' * Yeardecimal ISS * Yeardecimal ISS'' * GCSYN ISS'' * rcteyeYN ISS'' * Mechanism=RTC Age'' * Yeardecimal''' Age'' * Yeardecimal'' Age''' * Yeardecimal' Age''' * Yeardecimal Age'' * Yeardecimal Age' * Yeardecimal Age * Yeardecimal Age'' * GCSYN Age''' * rcteyeYN 
Error in lrm(formula = Survive ~ (rcs(Age) + rcs(ISS) + neuroFirst + Mechanism +  : 
  Unable to fit model using â€œlrm.fitâ€
</code></pre>

<p>The only way I could run the model is with an adjustment. <code>Yeardecimal</code> is excluded from any interaction as is the interaction of <code>GCS:GCSYN</code> and <code>rcteye:rcteyeYN</code> which produced the same error as written above. It made sense to exclude the interactions between a variable and its missing dummy but I am not sure what to do about <code>Yeardecimal</code>:</p>

<pre><code>rcs.ASDH&lt;-lrm(formula = Survive ~ (rcs(Age) + rcs(ISS) + 
     neuroFirst + Mechanism + rcteye + rcteyeYN) * (rcs(Age) + rcs(ISS) + 
     neuroFirst + Mechanism + GCS + GCSYN) + rcs(Yeardecimal), data = ASDH_Paper1.1)
</code></pre>

<p>From this model the following interactions were identified with an <code>anova</code> output:</p>

<pre><code>&gt; anova(rcs.ASDH)
                Wald Statistics          Response: Survive 

 Factor                                                Chi-Square d.f. P     
 Age  (Factor+Higher Order Factors)                    130.42      52  &lt;.0001
  All Interactions                                      78.68      48  0.0034
  Nonlinear (Factor+Higher Order Factors)               46.53      39  0.1901
 ISS  (Factor+Higher Order Factors)                    181.65      42  &lt;.0001
  All Interactions                                      52.43      39  0.0738
  Nonlinear (Factor+Higher Order Factors)               55.01      28  0.0017
 neuroFirst  (Factor+Higher Order Factors)              37.68      16  0.0017
  All Interactions                                      11.54      15  0.7136
 Mechanism  (Factor+Higher Order Factors)               63.72      52  0.1277
  All Interactions                                      58.35      48  0.1455
 rcteye  (Factor+Higher Order Factors)                 242.07      15  &lt;.0001
  All Interactions                                      19.39      14  0.1507
 rcteyeYN  (Factor+Higher Order Factors)               204.58      15  &lt;.0001
  All Interactions                                      29.88      14  0.0079
 GCS  (Factor+Higher Order Factors)                    162.81      15  &lt;.0001
  All Interactions                                      11.62      14  0.6365
 GCSYN  (Factor+Higher Order Factors)                   94.50      15  &lt;.0001
  All Interactions                                      41.74      14  0.0001
 Yeardecimal                                            51.96       4  &lt;.0001
  Nonlinear                                             10.27       3  0.0164
 Age * ISS  (Factor+Higher Order Factors)               11.90      12  0.4534
  Nonlinear                                              9.40      11  0.5851
  Nonlinear Interaction : f(A,B) vs. AB                  9.40      11  0.5851
  f(A,B) vs. Af(B) + Bg(A)                               7.96       6  0.2411
  Nonlinear Interaction in Age vs. Af(B)                 8.75       9  0.4605
  Nonlinear Interaction in ISS vs. Bg(A)                 8.58       8  0.3790
 Age * neuroFirst  (Factor+Higher Order Factors)         2.66       4  0.6166
  Nonlinear                                              2.05       3  0.5624
  Nonlinear Interaction : f(A,B) vs. AB                  2.05       3  0.5624
 Age * Mechanism  (Factor+Higher Order Factors)         17.58      16  0.3493
  Nonlinear                                             13.82      12  0.3127
  Nonlinear Interaction : f(A,B) vs. AB                 13.82      12  0.3127
 Age * GCS  (Factor+Higher Order Factors)                6.24       4  0.1819
  Nonlinear                                              3.89       3  0.2741
  Nonlinear Interaction : f(A,B) vs. AB                  3.89       3  0.2741
 Age * GCSYN  (Factor+Higher Order Factors)             20.11       4  0.0005
  Nonlinear                                              8.86       3  0.0312
  Nonlinear Interaction : f(A,B) vs. AB                  8.86       3  0.0312
 ISS * neuroFirst  (Factor+Higher Order Factors)         3.23       3  0.3571
  Nonlinear                                              0.87       2  0.6480
  Nonlinear Interaction : f(A,B) vs. AB                  0.87       2  0.6480
 ISS * Mechanism  (Factor+Higher Order Factors)         23.95      12  0.0206
  Nonlinear                                             20.66       8  0.0081
  Nonlinear Interaction : f(A,B) vs. AB                 20.66       8  0.0081
 ISS * GCS  (Factor+Higher Order Factors)                0.77       3  0.8570
  Nonlinear                                              0.42       2  0.8102
  Nonlinear Interaction : f(A,B) vs. AB                  0.42       2  0.8102
 ISS * GCSYN  (Factor+Higher Order Factors)              6.53       3  0.0886
  Nonlinear                                              2.35       2  0.3085
  Nonlinear Interaction : f(A,B) vs. AB                  2.35       2  0.3085
 neuroFirst * Mechanism  (Factor+Higher Order Factors)   2.45       4  0.6533
 neuroFirst * GCS  (Factor+Higher Order Factors)         0.00       1  0.9726
 neuroFirst * GCSYN  (Factor+Higher Order Factors)       1.39       1  0.2382
 Mechanism * GCS  (Factor+Higher Order Factors)          0.10       4  0.9987
 Mechanism * GCSYN  (Factor+Higher Order Factors)        1.74       4  0.7828
 Age * rcteye  (Factor+Higher Order Factors)             8.66       4  0.0702
  Nonlinear                                              7.29       3  0.0633
  Nonlinear Interaction : f(A,B) vs. AB                  7.29       3  0.0633
 ISS * rcteye  (Factor+Higher Order Factors)             4.18       3  0.2424
  Nonlinear                                              1.49       2  0.4744
  Nonlinear Interaction : f(A,B) vs. AB                  1.49       2  0.4744
 neuroFirst * rcteye  (Factor+Higher Order Factors)      0.10       1  0.7460
 Mechanism * rcteye  (Factor+Higher Order Factors)       3.44       4  0.4867
 rcteye * GCS  (Factor+Higher Order Factors)             2.30       1  0.1297
 rcteye * GCSYN  (Factor+Higher Order Factors)           2.57       1  0.1090
 Age * rcteyeYN  (Factor+Higher Order Factors)           7.23       4  0.1242
  Nonlinear                                              7.23       3  0.0649
  Nonlinear Interaction : f(A,B) vs. AB                  7.23       3  0.0649
 ISS * rcteyeYN  (Factor+Higher Order Factors)           2.47       3  0.4814
  Nonlinear                                              0.11       2  0.9462
  Nonlinear Interaction : f(A,B) vs. AB                  0.11       2  0.9462
 neuroFirst * rcteyeYN  (Factor+Higher Order Factors)    0.12       1  0.7280
 Mechanism * rcteyeYN  (Factor+Higher Order Factors)     1.81       4  0.7701
 rcteyeYN * GCS  (Factor+Higher Order Factors)           3.70       1  0.0543
 rcteyeYN * GCSYN  (Factor+Higher Order Factors)         8.74       1  0.0031
 TOTAL NONLINEAR                                       102.74      64  0.0015
 TOTAL INTERACTION                                     178.52     103  &lt;.0001
 TOTAL NONLINEAR + INTERACTION                         241.87     111  &lt;.0001
 TOTAL                                                 889.91     123  &lt;.0001
</code></pre>

<p>The <code>summary</code> function revealed the following results:</p>

<pre><code>             Effects              Response : Survive 

 Factor                                    Low    High   Diff. Effect       S.E.   Lower 0.95 Upper 0.95    
 Age                                         37.6   72.0 34.40         0.15   0.38   -0.58      8.900000e-01
  Odds Ratio                                 37.6   72.0 34.40         1.16     NA    0.56      2.430000e+00
 ISS                                         20.0   26.0  6.00        -1.34   0.31   -1.95     -7.400000e-01
  Odds Ratio                                 20.0   26.0  6.00         0.26     NA    0.14      4.800000e-01
 neuroFirst                                   0.0    1.0  1.00        -0.23   0.37   -0.95      5.000000e-01
  Odds Ratio                                  0.0    1.0  1.00         0.80     NA    0.39      1.650000e+00
 rcteye                                       0.0    2.0  2.00         3.20   0.50    2.22      4.170000e+00
  Odds Ratio                                  0.0    2.0  2.00        24.41     NA    9.24      6.452000e+01
 rcteyeYN                                     0.0    1.0  1.00        -3.34   0.44   -4.21     -2.480000e+00
  Odds Ratio                                  0.0    1.0  1.00         0.04     NA    0.01      8.000000e-02
 GCS                                          0.0   12.0 12.00         1.94   0.49    0.98      2.890000e+00
  Odds Ratio                                  0.0   12.0 12.00         6.94     NA    2.67      1.799000e+01
 GCSYN                                        0.0    1.0  1.00        -1.32   0.45   -2.20     -4.400000e-01
  Odds Ratio                                  0.0    1.0  1.00         0.27     NA    0.11      6.400000e-01
 Yeardecimal                               2005.5 2012.4  6.85         0.20   0.12   -0.03      4.400000e-01
  Odds Ratio                               2005.5 2012.4  6.85         1.22     NA    0.97      1.550000e+00
 Mechanism - Fall &gt; 2m:Fall &lt; 2m              1.0    2.0    NA        -0.89   0.35   -1.58     -2.000000e-01
  Odds Ratio                                  1.0    2.0    NA         0.41     NA    0.21      8.200000e-01
 Mechanism - Other:Fall &lt; 2m                  1.0    3.0    NA         0.25   0.42   -0.58      1.080000e+00
  Odds Ratio                                  1.0    3.0    NA         1.28     NA    0.56      2.930000e+00
 Mechanism - RTC:Fall &lt; 2m                    1.0    4.0    NA        -0.68   0.43   -1.52      1.700000e-01
  Odds Ratio                                  1.0    4.0    NA         0.51     NA    0.22      1.190000e+00
 Mechanism - Shooting / Stabbing:Fall &lt; 2m    1.0    5.0    NA        18.97 116.63 -209.63      2.475600e+02
  Odds Ratio                                  1.0    5.0    NA 172906690.96     NA    0.00     3.272814e+107

Adjusted to: Age=54.2 ISS=25 neuroFirst=0 Mechanism=Fall &lt; 2m rcteye=1 rcteyeYN=0 GCS=3 GCSYN=0 
</code></pre>

<p>Remaining questions are:</p>

<p><strong>1</strong> - Is my dummy variable treatment for variables missing not at random appropriate, including the exclusion of interactions with the main term?</p>

<p><strong>2</strong> - Can I resolve the issues with assessing interaction of the Yeardecimal term?</p>

<p><strong>3</strong> - Should I exclude non-significant interaction terms? I read that exclusion only of a ""chunk"" is advised - <a href=""http://stats.stackexchange.com/questions/11009/including-the-interaction-but-not-the-main-effects-in-a-model"">Including the interaction but not the main effects in a model</a></p>

<p><strong>4</strong> - Is the odds ratio for each variable the ""Effect"" column? If so, is this the OR between the lowest and highest value of each variable?</p>
"
"0.232680311561883","0.236282094630595","127479","<p>I'm using a mixed effects model with logistic link function (using lme4 version 1.1-7 in R).  However, I noticed that the estimates of significance for fixed effects change depending on the order of the rows in the dataset.  </p>

<p>That is, if I run a model on a dataset, I get certain estimate for my fixed effect and it has a certain p-value.  I run the model again, and I get the same estimate and p-value.  Now, I shuffle the order of rows (the data is not mixed, just the rows are in a different order).  Running the model a third time, the p-value is very different.</p>

<p>For the data I have, the estimated p-value for the fixed effect can be between p=0.001 and p=0.08.  Obviously, these are crucial differences given conventional significance levels. </p>

<p>I understand that the estimates are just estimated, and there will be differences between values for a number of reasons.  However, the magnitude of the differences for my data seem large to me, and I wouldn't expect the order of my dataframe to have this effect (we discovered this problem by chance when a colleague ran the same model but got different results.  It turned out they had ordered their data frame.).  </p>

<p>Here is the output of my script:
(X and Y are binary variables which are contrast-coded and centred, Group and SubGroup are categorical variables)</p>

<pre><code>&gt; # Fit model
&gt; m1 = glmer(X ~Y+(1+Y|Group)+(1+Y|SubGroup),family=binomial(link='logit'),data=d)
&gt; # Shuffle order of rows
&gt; d = d[sample(1:nrow(d)),]
&gt; # Fit model again
&gt; m2 = glmer(X ~Y+(1+Y|Group)+(1+Y|SubGroup),family=binomial(link='logit'),data=d)
&gt; summary(m1)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
      ['glmerMod']
 Family: binomial  ( logit )
Formula: X ~ Y + (1 + Y | Group) + (1 + Y | SubGroup)
   Data: d

      AIC       BIC    logLik  deviance  df.resid 
 200692.0  200773.2 -100338.0  200676.0    189910 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.1368 -0.5852 -0.4873 -0.1599  6.2540 

Random effects:
 Groups       Name        Variance Std.Dev. Corr 
 SubGroup     (Intercept) 0.2939   0.5421        
              Y1          0.1847   0.4298   -0.79
 Group        (Intercept) 0.2829   0.5319        
              Y1          0.4640   0.6812   -0.07
Number of obs: 189918, groups:  SubGroup, 15; Group, 12

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.0886     0.1325  -8.214   &lt;2e-16 ***
Y1            0.3772     0.2123   1.777   0.0756 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
     (Intr)
Y1 0.112 
&gt;
&gt; # -----------------
&gt; summary(m2)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
      ['glmerMod']
 Family: binomial  ( logit )
Formula: X ~ Y + (1 + Y | Group) + (1 + Y | SubGroup)
   Data: d

      AIC       BIC    logLik  deviance  df.resid 
 200692.0  200773.2 -100338.0  200676.0    189910 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.1368 -0.5852 -0.4873 -0.1599  6.2540 

Random effects:
 Groups       Name        Variance Std.Dev. Corr 
 SubGroup     (Intercept) 0.2939   0.5422        
              Y1          0.1846   0.4296   -0.79
 Group        (Intercept) 0.2829   0.5318        
              Y1          0.4641   0.6813   -0.07
Number of obs: 189918, groups:  SubGroup, 15; Group, 12

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.0886     0.1166  -9.334  &lt; 2e-16 ***
Y1            0.3773     0.1130   3.339 0.000841 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
     (Intr)
Y1 0.074 
</code></pre>

<p>I'm afraid that I can't attach the data due to privacy reasons. </p>

<p>Both models converge.  The difference appears to be in the standard errors, while the differences in coefficient estimates are smaller.  The model fit (AIC etc.) are the same, so maybe there are multiple optimal convergences, and the order of the data pushes the optimiser into different ones.  However, I get slightly different estimates every time I shuffle the data frame (not just two or three unique estimates).  In one case (not shown above), the model did not converge simply because of a shuffling of the rows.</p>

<p>I suspect that the problem lies with the structure of my particular data.  It's reasonably large (nearly 200,000 cases), and has nested random effects.  I have tried centering the data, using contrast coding and feeding starting values to lmer based on a previous fit.  This seems to help somewhat, but I still get reasonably large differences in p-values.  I also tried using different ways of calculating p-values, but I got the same problem.</p>

<p>Below, I've tried to replicate this problem with synthesised data.  The differences here aren't as big as with my real data, but it gives an idea of the problem.</p>

<pre><code>library(lme4)
set.seed(999)

# make a somewhat complex data frame
x = c(rnorm(10000),rnorm(10000,0.1))
x = sample(x)
y = jitter(x,amount=10)
a = rep(1:20,length.out=length(x))
y[a==1] = jitter(y[a==1],amount=3)
y[a==2] = jitter(x[a==2],amount=1)
y[a&gt;3 &amp; a&lt;6] = rnorm(sum(a&gt;3 &amp; a&lt;6))
# convert to binary variables
y = y &gt;0
x = x &gt;0
# make a data frame
d = data.frame(x1=x,y1=y,a1=a)

# run model 
m1 = glmer(x1~y1+(1+y1|a1),data=d,family=binomial(link='logit'))

# shuffle order of rows
d = d[sample(nrow(d)),]

# run model again
m2 = glmer(x1~y1+(1+y1|a1),data=d,family=binomial(link='logit'))

# show output
summary(m1)
summary(m2)
</code></pre>

<p>One solution to this is to run the model multiple times with different row orders, and report the range of p-values.  However, this seems inelegant and potentially quite confusing.</p>

<p>The problem does not affect model comparison estimates (using anova), since these are based on differences in model fit.  The fixed effect coefficient estimates are also reasonably robust.  Therefore, I could just report the effect size, confidence intervals and the p-value from a model comparison with a null model, rather than the p-values from within the main model.</p>

<p>Anyway, has anyone else had this problem?  Any advice on how to proceed?</p>
"
"0.197035967734661","0.15631718197586","129761","<p>These multiple imputation results relate to data I have previously described and shown here - <a href=""http://stats.stackexchange.com/questions/129739/skewed-distributions-for-logistic-regression"">Skewed Distributions for Logistic Regression</a></p>

<p>Three variables I am using have missing data. Their names, descriptions and % missing are shown below.</p>

<pre><code>inctoCran - Time from head injury to craniotomy in minutes = 0-2880 (After 2880 minutes is defined as a separate diagnosis) - 58% missing
GCS - Glasgow Coma Scale = 3-15 - 37% missing
rcteyemi - Pupil reactivity (1 = neither, 2 = one, 3 = both) - 56% missing
</code></pre>

<p>I have been using mutliple imputation to model the missing data above following advice in a previous post here - <a href=""http://stats.stackexchange.com/questions/127134/describing-results-from-logistic-regression-with-restricted-cubic-splines-using"">Describing Results from Logistic Regression with Restricted Cubic Splines Using rms in R</a></p>

<p>Given this is a longitudinal analysis, a key variable of importance is the year of the treatment so we can investigate how our patient management has improved. The variable in question, <code>Yeardecimal</code> is highly significant in univariate analysis:</p>

<pre><code>&gt; rcs.ASDH&lt;-lrm(formula = Survive ~ Yeardecimalc, data = ASDH_Paper1.1)
&gt; 
&gt; rcs.ASDH

Logistic Regression Model

lrm(formula = Survive ~ Yeardecimalc, data = ASDH_Paper1.1)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          5998    LR chi2      91.47    R2       0.023    C       0.572    
 0           1281    d.f.             1    g        0.309    Dxy     0.143    
 1           4717    Pr(&gt; chi2) &lt;0.0001    gr       1.362    gamma   0.146    
max |deriv| 3e-12                          gp       0.054    tau-a   0.048    
                                           Brier    0.165                     

             Coef   S.E.   Wald Z Pr(&gt;|Z|)
Intercept    0.8696 0.0530 16.42  &lt;0.0001 
Yeardecimalc 0.0551 0.0057  9.70  &lt;0.0001 
</code></pre>

<p>To deal with missingness, I used <code>aregImpute</code> and <code>fit.mult.impute</code> to conduct multiple imputation prior to multivariate logisic regression. When including Yeardecimal, the results were as follows:</p>

<pre><code>&gt; a &lt;- aregImpute(~ I(Outcome30) + Age + GCS + I(Other) + ISS + inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + LOS + Yeardecimalc, nk=4, data = ASDH_Paper1.1, n.impute=10)
Iteration 13 
&gt; 
&gt; a

Multiple Imputation using Bootstrap and PMM

aregImpute(formula = ~I(Outcome30) + Age + GCS + I(Other) + ISS + 
    inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + 
    LOS + Yeardecimalc, data = ASDH_Paper1.1, n.impute = 10, 
    nk = 4)

n: 5998     p: 12   Imputations: 10     nk: 4 

Number of NAs:
   Outcome30          Age          GCS        Other          ISS    inctoCran     rcteyemi   neuroFirst      neuroYN 
           0            0         2242            0            0         3500         3376            0            0 
   Mechanism          LOS Yeardecimalc 
           0            0            0 

             type d.f.
Outcome30       c    1
Age             s    3
GCS             s    3
Other           c    1
ISS             s    3
inctoCran       s    3
rcteyemi        l    1
neuroFirst      l    1
neuroYN         l    1
Mechanism       c    4
LOS             s    3
Yeardecimalc    s    3

Transformation of Target Variables Forced to be Linear

R-squares for Predicting Non-Missing Values for Each Variable
Using Last Imputations of Predictors
      GCS inctoCran  rcteyemi 
    0.421     0.181     0.358 

&gt; rcs.ASDH &lt;- fit.mult.impute(Survive ~ rcs(Age) + GCS + Mechanism + rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + Sex + Other + rcteyemi,lrm,a,data=ASDH_Paper1.1)

&gt; rcs.ASDH

Logistic Regression Model

fit.mult.impute(formula = Survive ~ rcs(Age) + GCS + Mechanism + 
    rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + 
    Sex + Other + rcteyemi, fitter = lrm, xtrans = a, data = ASDH_Paper1.1)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          5998    LR chi2    1609.98    R2       0.365    C       0.836    
 0           1281    d.f.            25    g        1.584    Dxy     0.672    
 1           4717    Pr(&gt; chi2) &lt;0.0001    gr       4.875    gamma   0.674    
max |deriv| 0.001                          gp       0.222    tau-a   0.226    
                                           Brier    0.121                     

                              Coef    S.E.    Wald Z Pr(&gt;|Z|)
Intercept                     21.3339 67.4400  0.32  0.7517  
Age                           -0.0088  0.0132 -0.67  0.5052  
Age'                          -0.0294  0.0643 -0.46  0.6471  
Age''                         -0.0134  0.2479 -0.05  0.9570  
Age'''                         0.2588  0.3534  0.73  0.4639  
GCS                            0.1100  0.0145  7.61  &lt;0.0001 
Mechanism=Fall &gt; 2m           -0.0651  0.1162 -0.56  0.5754  
Mechanism=Other                0.2285  0.1338  1.71  0.0876  
Mechanism=RTC                  0.0449  0.1332  0.34  0.7360  
Mechanism=Shooting / Stabbing  2.1150  1.1142  1.90  0.0577  
ISS                           -0.1069  0.0318 -3.36  0.0008  
ISS'                          -0.0359  0.1306 -0.27  0.7835  
ISS''                          1.8296  1.9259  0.95  0.3421  
neuroFirst                    -0.3483  0.0973 -3.58  0.0003  
inctoCrand                     0.0001  0.0053  0.02  0.9872  
inctoCrand'                   -0.0745  0.3060 -0.24  0.8077  
inctoCrand''                   0.1696  0.5901  0.29  0.7738  
inctoCrand'''                 -0.1167  0.3150 -0.37  0.7110  
inctoCranYN                   -0.2814  0.6165 -0.46  0.6480  
Yeardecimalc                  -0.0101  0.0337 -0.30  0.7641  
Yeardecimalc'                  0.0386  0.0651  0.59  0.5536  
Yeardecimalc''                -0.7417  0.8210 -0.90  0.3663  
Yeardecimalc'''                7.0367  4.9344  1.43  0.1539  
Sex=Male                       0.0668  0.0891  0.75  0.4534  
Other=1                        0.3238  0.1611  2.01  0.0445  
rcteyemi                       1.1589  0.1050 11.04  &lt;0.0001 


&gt; anova(rcs.ASDH)
                Wald Statistics          Response: Survive 

 Factor          Chi-Square d.f. P     
 Age              83.07      4   &lt;.0001
  Nonlinear        5.97      3   0.1131
 GCS              57.89      1   &lt;.0001
 Mechanism         8.14      4   0.0867
 ISS              77.31      3   &lt;.0001
  Nonlinear       35.04      2   &lt;.0001
 neuroFirst       12.81      1   0.0003
 inctoCrand        2.32      4   0.6777
  Nonlinear        2.29      3   0.5149
 inctoCranYN       0.21      1   0.6480
 Yeardecimalc      4.19      4   0.3807
  Nonlinear        3.77      3   0.2874
 Sex               0.56      1   0.4534
 Other             4.04      1   0.0445
 rcteyemi        121.80      1   &lt;.0001
 TOTAL NONLINEAR  47.27     11   &lt;.0001
 TOTAL           679.09     25   &lt;.0001
&gt; 
</code></pre>

<p>Yeardecimal is no longer significant. However, if I exclude Yeardecimal from aregImpute only, I have the alternative result below:</p>

<pre><code>&gt; a &lt;- aregImpute(~ I(Outcome30) + Age + GCS + I(Other) + ISS + inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + LOS, nk=4, data = ASDH_Paper1.1, n.impute=10)
Iteration 13 
&gt; 
&gt; a

Multiple Imputation using Bootstrap and PMM

aregImpute(formula = ~I(Outcome30) + Age + GCS + I(Other) + ISS + 
    inctoCran + I(rcteyemi) + I(neuroFirst) + I(neuroYN) + Mechanism + 
    LOS, data = ASDH_Paper1.1, n.impute = 10, nk = 4)

n: 5998     p: 11   Imputations: 10     nk: 4 

Number of NAs:
 Outcome30        Age        GCS      Other        ISS  inctoCran   rcteyemi neuroFirst    neuroYN  Mechanism        LOS 
         0          0       2242          0          0       3500       3376          0          0          0          0 

           type d.f.
Outcome30     c    1
Age           s    3
GCS           s    3
Other         c    1
ISS           s    3
inctoCran     s    3
rcteyemi      l    1
neuroFirst    l    1
neuroYN       l    1
Mechanism     c    4
LOS           s    3

Transformation of Target Variables Forced to be Linear

R-squares for Predicting Non-Missing Values for Each Variable
Using Last Imputations of Predictors
      GCS inctoCran  rcteyemi 
    0.407     0.194     0.320 
&gt; 

&gt; rcs.ASDH &lt;- fit.mult.impute(Survive ~ rcs(Age) + GCS + Mechanism + rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + Sex + Other + rcteyemi,lrm,a,data=ASDH_Paper1.1)
&gt; rcs.ASDH

Logistic Regression Model

fit.mult.impute(formula = Survive ~ rcs(Age) + GCS + Mechanism + 
    rcs(ISS) + neuroFirst + rcs(inctoCrand) + inctoCranYN + rcs(Yeardecimalc) + 
    Sex + Other + rcteyemi, fitter = lrm, xtrans = a, data = ASDH_Paper1.1)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          5998    LR chi2    1607.92    R2       0.364    C       0.834    
 0           1281    d.f.            25    g        1.578    Dxy     0.667    
 1           4717    Pr(&gt; chi2) &lt;0.0001    gr       4.846    gamma   0.669    
max |deriv| 0.003                          gp       0.221    tau-a   0.224    
                                           Brier    0.120                     

                              Coef     S.E.    Wald Z Pr(&gt;|Z|)
Intercept                     -55.6574 58.3464 -0.95  0.3401  
Age                            -0.0084  0.0128 -0.66  0.5105  
Age'                           -0.0335  0.0612 -0.55  0.5838  
Age''                           0.0050  0.2365  0.02  0.9830  
Age'''                          0.2321  0.3387  0.69  0.4930  
GCS                             0.1099  0.0124  8.88  &lt;0.0001 
Mechanism=Fall &gt; 2m            -0.0631  0.1138 -0.55  0.5793  
Mechanism=Other                 0.2354  0.1381  1.70  0.0883  
Mechanism=RTC                   0.0315  0.1319  0.24  0.8114  
Mechanism=Shooting / Stabbing   1.9297  1.0930  1.77  0.0775  
ISS                            -0.1012  0.0335 -3.02  0.0025  
ISS'                           -0.0599  0.1366 -0.44  0.6613  
ISS''                           2.1581  2.0120  1.07  0.2834  
neuroFirst                     -0.3753  0.0888 -4.23  &lt;0.0001 
inctoCrand                     -0.0007  0.0054 -0.13  0.9002  
inctoCrand'                    -0.0496  0.3116 -0.16  0.8734  
inctoCrand''                    0.1316  0.6021  0.22  0.8270  
inctoCrand'''                  -0.1078  0.3224 -0.33  0.7381  
inctoCranYN                    -0.1697  0.6172 -0.27  0.7834  
Yeardecimalc                    0.0281  0.0291  0.96  0.3349  
Yeardecimalc'                   0.0682  0.0600  1.14  0.2553  
Yeardecimalc''                 -1.4037  0.7685 -1.83  0.0678  
Yeardecimalc'''                10.2513  4.8156  2.13  0.0333  
Sex=Male                        0.0595  0.0890  0.67  0.5037  
Other=1                         0.3579  0.1641  2.18  0.0292  
rcteyemi                        1.1862  0.0799 14.85  &lt;0.0001 


&gt; anova(rcs.ASDH)
                Wald Statistics          Response: Survive 

 Factor          Chi-Square d.f. P     
 Age              78.39      4   &lt;.0001
  Nonlinear        6.23      3   0.1011
 GCS              78.86      1   &lt;.0001
 Mechanism         7.53      4   0.1104
 ISS              76.46      3   &lt;.0001
  Nonlinear       31.16      2   &lt;.0001
 neuroFirst       17.87      1   &lt;.0001
 inctoCrand        3.22      4   0.5214
  Nonlinear        3.19      3   0.3630
 inctoCranYN       0.08      1   0.7834
 Yeardecimalc     44.83      4   &lt;.0001
  Nonlinear        4.67      3   0.1979
 Sex               0.45      1   0.5037
 Other             4.76      1   0.0292
 rcteyemi        220.51      1   &lt;.0001
 TOTAL NONLINEAR  45.39     11   &lt;.0001
 TOTAL           715.22     25   &lt;.0001
&gt; 
</code></pre>

<p>Can anyone help me understand why the statistical results for Yeardecimal are so starkly different?</p>
"
"0.0399043442233811","0.0405220449236554","130476","<p>This model is a simple linear regression:</p>

<pre><code>mtcars_lm &lt;- lm(mpg ~ wt, mtcars)
</code></pre>

<p>And this model adds <code>cyl</code> as a random effect:</p>

<pre><code>library(lme4)
mtcars_mixed_effects &lt;- lmer(mpg ~ wt + (1 | cyl), mtcars)
</code></pre>

<p>Is there a way to test whether adding <code>cyl</code> as random effect is worthwhile? I tried this but it threw an error:</p>

<pre><code>anova(mtcars_mixed_effects, mtcars_lm)
</code></pre>

<p>(please disregard the fact that <code>cyl</code> only has three groups, I'm just using one of R's built in datasets to make question reprodicible).</p>
"
"0.0892288262810312","0.0906100470365937","131093","<p>I hope this is not a duplicate but I cannot find the answer to this question. In a linear model
$$Y_i = \beta_1 X_{i,1} + \dots + \beta_{p-1} X_{i,p-1} + \varepsilon_i, \qquad i = 1, \ldots, n$$
with the usual assumptions, is the regression sum of squares, $SSR$, still</p>

<p>$$ SSR = \sum_{i=1}^n (\hat{y_i} - \overline{y})^2 \text{ ?}$$
where $\hat{y_i} = X \hat{\beta}$ is the $i$-th fitted value, $\hat{\beta} = (X^TX)^{-1}X^T y$ and $X$ is the design matrix without the column of ones that it would have if we couldn't assume $\beta_0 = 0$.</p>

<p>Now, I'm asking this question because using the ""anova"" function in R, you can obtain the $SSR$ by simply adding the corresponding $SSR$'s of each variable (I believe this is called a type I decomposition), but this doesn't match the $SSR$ as calculated above for a model with $\beta_0 = 0$.</p>

<p>Am I missing something or did I just screw up calculating it? </p>

<p>I had a sample of 2 variables, $X_1$ and $X_2$ with $n=11$ observations, as follows:
$x_1 = (1,4,9,11,3,8,5,10,2,7,6)^T$, $x_2 = (8,2,-8,-10,6,-6,0,-12,4,-2,-4)^T$ and $y=(6,8,1,0,5,3,2,-4,10,-3,5)^T$.</p>

<p>I introduced them in R as y, x1 and x2. Then using anova(lm(y~0+x1+x2)) I got Sum Sq of 14.279 for x1 and 161.846 for x2. Their sum is 176.154.</p>

<p>However, using the design matrix with $x_1$ and $x_2$ as its columns, I got $\beta = (\beta_1, \beta_2)^T = (0.7211, 0.8089)^T$ (which matches the ones obtained in R) so $SSR = 96.37352$, which is obviously different from the one obtained in R.</p>
"
"0.132347737294141","0.134396418749691","131152","<p>Let say I've ran this linear regression:</p>

<pre><code>lm_mtcars &lt;- lm(mpg ~ wt + vs, mtcars)
</code></pre>

<p>I can use <code>anova()</code> to see the amount of variance in the dependent variable accounted for by the two predictors:</p>

<pre><code>anova(lm_mtcars)

Analysis of Variance Table

Response: mpg
          Df Sum Sq Mean Sq  F value    Pr(&gt;F)    
wt         1 847.73  847.73 109.7042 2.284e-11 ***
vs         1  54.23   54.23   7.0177   0.01293 *  
Residuals 29 224.09    7.73                       
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Lets say I now add a random intercept for <code>cyl</code>:</p>

<pre><code>library(lme4)
lmer_mtcars &lt;- lmer(mpg ~ wt + vs + (1 | cyl), mtcars)
summary(lmer_mtcars)

Linear mixed model fit by REML ['lmerMod']
Formula: mpg ~ wt + vs + (1 | cyl)
   Data: mtcars

REML criterion at convergence: 148.8

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-1.67088 -0.68589 -0.08363  0.48294  2.16959 

Random effects:
 Groups   Name        Variance Std.Dev.
 cyl      (Intercept) 3.624    1.904   
 Residual             6.784    2.605   
Number of obs: 32, groups:  cyl, 3

Fixed effects:
            Estimate Std. Error t value
(Intercept)  31.4788     2.6007  12.104
wt           -3.8054     0.6989  -5.445
vs            1.9500     1.4315   1.362

Correlation of Fixed Effects:
   (Intr) wt    
wt -0.846       
vs -0.272  0.006
</code></pre>

<p>The variance accounted for by each fixed effect now drops because the random intercept for <code>cyl</code> is now accounting for some of the variance in <code>mpg</code>:</p>

<pre><code>anova(lmer_mtcars)

Analysis of Variance Table
   Df  Sum Sq Mean Sq F value
wt  1 201.707 201.707 29.7345
vs  1  12.587  12.587  1.8555
</code></pre>

<p>But in <code>lmer_mtcars</code>, how can I tell what proportion of the variance is being accounted for by <code>wt</code>, <code>vs</code> and the random intecept for <code>cyl</code>?</p>
"
"0.0798086884467622","0.0810440898473108","134258","<p>I have an experimental design in which I am comparing two or three types of soil (depending on how I group them) across two elevation transects. My overall goal is to compare different soil types, but since elevation is involved some of the parameters I am testing seem to have a linear trend. Within this design there are two different slopes, different elevation points where the soil was collected, and the parameter that I am testing. I also have two different seasons that I repeated this experiment in. I was instructed that this is a split plot ANOVA design, but I am having difficulty finding an r code that works with what I have. Any suggestions?</p>
"
"0.199521721116906","0.202610224618277","136464","<p>I have only a very basic background in statistics, and I have a possibly simple question, but I'm having a bit of trouble with my model. I suppose this is also an R question, but also statistical!</p>

<p>I'm looking for some advice concerning the interpretation my <code>nlme</code> results.</p>

<p>I am modelling photosynthesis-irradiance -relationship and have fitted a nonlinear mixed model in <code>nlme</code> such as this with 3 parameters to estimate:</p>

<pre><code>nlme(Foto~picurve(PAR, Amax, Aqe, LCP), fixed = list(Amax ~ Place, Aqe + LCP ~ 1), random = pdDiag(Amax ~ 1), weight = varPower(), correlation = corARMA(p = 1, q = 1), method = ""ML"",  start = rep(c(15, 0.0054, 20), 2), data = data1)
</code></pre>

<p>The summary is as follows</p>

<pre><code>...

    Fixed effects: list(Amax ~ Place, Aqe + LCP ~ 1) 
                                           Value Std.Error  DF  t-value p-value
    Amax.(Intercept)                   16.427779 0.6567488 959 25.01380  0.0000
    Amax.Place 2                       -1.328056 0.9169505 959 -1.44834  0.1478
    Amax.Place 3                       -1.063690 0.8996467 959 -1.18234  0.2374
    Amax.Place 4                       -3.207345 0.9171032 959 -3.49726  0.0005
    Aqe                                 0.057579 0.0015047 959 38.26518  0.0000
    LCP                                21.388703 0.7486608 959 28.56928  0.0000

...
</code></pre>

<p>I need to make inference on the fixed effects at the 4 places. Each place contains 3 genotypes, and each genotype contain several individuals, but they're not modelled here. Some genotypes contain more individuals than others (unbalanced?), and some individuals may have a few more measurements in them than the others. But I suppose that's beyond the point.</p>

<p>Now my questions is, what is the current recommended way of making inferences about the fixed effects parameter estimates? If I understood correctly, MCMCs etc. are the ""most correct"" approach, but they are probably beyond my comprehension at the moment and probably not what my superiors would want anyway. I know I can obtain the conditional F-test statistics and compare the places with <code>anova()</code>, even if that probably is a bad approach due to the denominator df problems I've been hearing about. But other than that, I don't know any way of doing this (recommendations are welcome).</p>

<p>Next, and more importantly, how does one make pairwise/multiple comparisons between the fixed effects? I need to compare all places to each others, as so:</p>

<pre><code>    Place 1 - Place 2
    Place 1 - Place 3
    Place 1 - Place 4
    Place 2 - Place 3
...
</code></pre>

<p>etc.</p>

<p>Now, again, MCMCs might not be my cup of tea here. The conditional t-test values from the summary can probably be used, at least in some (balanced?) instances, but would still leave me wanting to compare other groups besides the first.</p>

<p>What I would have to do, if I am correct (am I?), is set my contrasts so that I can compare the other groups as well. I tried approaching this with the <code>multcomp</code>-package:</p>

<pre><code>summary(glht(psn7, linfct =  matrix(c(0, 0, 1, -1, 0, 0), 1)))

Linear Hypotheses:
       Estimate Std. Error z value Pr(&gt;|z|)  
1 == 0   2.1437     0.8862   2.419   0.0156 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
(Adjusted p values reported -- single-step method)
</code></pre>

<p>This is where the troubles begin.</p>

<p>1) Is glht unsuited for nlme? It reports z-values (df problems?) - I thought it always reported t-values...</p>

<p>2) I still cannot compare the first group and the others, do I need to manipulate the order of my data or is there a more elegant way of getting all the comparisons?</p>

<p>I know this a very simple question, and I apologize. I'm quite new to this all...</p>

<p>p.s. also, bonus question, how about likelihood ratio tests? Can they be used for inference about fixed effects?</p>
"
"0.246067801748471","0.237062621201868","136495","<p><b>Background:</b><br>
I am using linear mixed-effects models (LMMs) in order to determine how the interaction between two fixed effects influences measures of a response variable.  Since I am working with a dataset in which there are multiple samples from multiple individuals that could violate the assumption of independence of data points, I am treating ""individual"" as a random effect.  Thus, the generic model I am working with is:  </p>

<pre><code>lmer(y ~ Factor1*ContinuousVariable1 + (1|Ind), dataset, REML=T)
</code></pre>

<p>Note: for my actual dataset, I used a likelihood ratio test to determine whether I needed to also nest the multiple trials within individual [i.e., lmer(y ~ Factor1*ContinuousVariable1 + (1|Ind/Trial), dataset)], and failed to reject the null hypothesis that this ""fuller"" model contributed significantly to accounting for additional variation in the data.   </p>

<p><b>Problem to solve:</b><br>
Determine whether the results from my Tukey's post-hoc comparisons are reliable, given the interactions included in my LMM model.</p>

<p><b>Loading data and libraries:</b><br>
library(car) # for Soils dataset<br>
data(Soils)<br>
library(lme4) # for lmer()<br>
library(lsmeans) # for remaining functions  </p>

<p><b>Example code:</b><br>
     ## Create the LMM<br>
     ## ""Na"" is a numeric continuous response variable<br>
     ## ""Contour"" is a factor, with character categories, and is treated as a fixed effect<br>
     ## ""P"" is an integer variable, is treated as a fixed effect, and differs across the Contour groups<br>
     ## ""Group"" is a a numerical factor and is treated as a random effect  </p>

<pre><code>Na.LMER &lt;- lmer(Na ~ Contour*P + (1|Group), Soils, REML=T)
Na.LMER  

Linear mixed model fit by REML ['lmerMod']
Formula: Na ~ Contour * P + (1 | Group)
   Data: Soils
REML criterion at convergence: 190.4919
Random effects:
 Groups   Name        Std.Dev.
 Group    (Intercept) 2.514   
 Residual             1.063   
Number of obs: 48, groups:  Group, 12
Fixed Effects:
   (Intercept)    ContourSlope      ContourTop               P  ContourSlope:P    ContourTop:P  
    7.104951        4.381251       -0.260527       -0.006811       -0.026952       -0.006258  

### Conduct Tukey's post-hoc comparisons
Na.Tukey &lt;- lsmeans(Na.LMER, pairwise~Contour, adjust=""tukey"")
</code></pre>

<blockquote>
  <p>NOTE: Results may be misleading due to involvement in interactions  </p>
</blockquote>

<pre><code>Na.Tukey  

$lsmeans
 Contour      lsmean       SE   df lower.CL upper.CL
 Depression 5.973118 1.289466 8.15 3.008857 8.937380
 Slope      5.875929 1.286895 8.08 2.913697 8.838160
 Top        4.672639 1.294933 8.24 1.701416 7.643863

Confidence level used: 0.95 

$contrasts
 contrast             estimate       SE   df t.ratio p.value
 Depression - Slope 0.09718976 1.821763 8.11   0.053  0.9984
 Depression - Top   1.30047917 1.827450 8.19   0.712  0.7635
 Slope - Top        1.20328941 1.825636 8.16   0.659  0.7925

P value adjustment: tukey method for a family of 3 means 
</code></pre>

<p><b>So this is where the question comes in.</b><br>
Since I received the warning message (""NOTE: Results may be misleading due to involvement in interactions""), I want to verify whether I can reliably use the p-values output from lsmeans() to determine which contrasts were different from each other.  So how can I tell whether the interactions from this particular dataset could be problematic for interpreting the results from the Tukey's post-hoc comparisons.  </p>

<p><b>Here is what I have tried to investigate this issue.</b><br>
Based on the recommendations by Professor Russell Lenth (developer of the lsmeans R package), I used additional functions from the lsmeans R package to investigate what's going on with the data.</p>

<pre><code>### First, here are the F-tests of the fixed effects of the LMM.
anova(Na.LMER)   

Analysis of Variance Table
          Df  Sum Sq Mean Sq F value
Contour    2  0.5696  0.2848  0.2520
P          1 10.4083 10.4083  9.2093
Contour:P  2  6.7070  3.3535  2.9672  
</code></pre>

<p>Does the Contour:P interaction seem relatively strong?  </p>

<p>Next, I'm going to evaluate whether this interaction is important by determining to what extent the values of P varies across the Contour groups, using lsmip().    </p>

<pre><code>Na.lsm &lt;- lsmeans(Na.LMER, ~Contour|P, at=list(P = c(75, 100, 200, 300, 400)))  
Na.lsm    

P =  75:
 Contour       lsmean       SE    df   lower.CL  upper.CL
 Depression  6.594094 1.399580 10.70  3.5029413  9.685246
 Slope       8.953983 1.562754 13.53  5.5913341 12.316631
 Top         5.864180 1.511863 12.76  2.5917619  9.136598

P = 100:
 Contour       lsmean       SE    df   lower.CL  upper.CL
 Depression  6.423808 1.355688  9.64  3.3876590  9.459957
 Slope       8.109909 1.429365 10.79  4.9562943 11.263524
 Top         5.537432 1.391548 10.16  2.4433848  8.631479

P = 200:
 Contour       lsmean       SE    df   lower.CL  upper.CL
 Depression  5.742665 1.286244  8.08  2.7814923  8.703838
 Slope       4.733616 1.354120  9.32  1.6863856  7.780847
 Top         4.230440 1.384598 10.01  1.1459415  7.314939

P = 300:
 Contour       lsmean       SE    df   lower.CL  upper.CL
 Depression  5.061522 1.396923 10.63  1.9738402  8.149204
 Slope       1.357323 1.960472 21.77 -2.7109112  5.425557
 Top         2.923449 2.025495 24.22 -1.2549312  7.101829

P = 400:
 Contour       lsmean       SE    df   lower.CL  upper.CL
 Depression  4.380379 1.651907 17.57  0.9037052  7.857053
 Slope      -2.018970 2.841216 33.67 -7.7950921  3.757152
 Top         1.616457 2.914268 36.01 -4.2938885  7.526803

Confidence level used: 0.95  
</code></pre>

<blockquote>
  <h3>Plotting the interactions</h3>
  
  <p>Na.lsmip &lt;- lsmip(Na.lsm, Contour~P)</p>
</blockquote>

<p><img src=""http://i.stack.imgur.com/9uDul.jpg"" alt=""Interaction of Contour and P""></p>

<blockquote>
  <h3>It seems like the levels of Contour vary at different values of P (especially for Slope), but I'm going to use pairs() to verify this using pairwise comparison at each value of P.</h3>
</blockquote>

<pre><code>pairs(Na.lsm)  
P =  75:
 contrast             estimate       SE    df t.ratio p.value
 Depression - Slope -2.3598888 2.097862 12.15  -1.125  0.5175
 Depression - Top    0.7299139 2.060232 11.74   0.354  0.9335
 Slope - Top         3.0898026 2.174381 13.15   1.421  0.3589

P = 100:
 contrast             estimate       SE    df t.ratio p.value
 Depression - Slope -1.6861012 1.970019 10.22  -0.856  0.6784
 Depression - Top    0.8863760 1.942755  9.90   0.456  0.8928
 Slope - Top         2.5724773 1.994865 10.47   1.290  0.4308

P = 200:
 contrast             estimate       SE    df t.ratio p.value
 Depression - Slope  1.0090489 1.867637  8.70   0.540  0.8539
 Depression - Top    1.5122246 1.889851  9.04   0.800  0.7122
 Slope - Top         0.5031757 1.936686  9.67   0.260  0.9636

P = 300:
 contrast             estimate       SE    df t.ratio p.value
 Depression - Slope  3.7041990 2.407248 16.78   1.539  0.2988
 Depression - Top    2.1380732 2.460493 18.21   0.869  0.6660
 Slope - Top        -1.5661258 2.818879 23.01  -0.556  0.8447

P = 400:
 contrast             estimate       SE    df t.ratio p.value
 Depression - Slope  6.3993492 3.286534 28.79   1.947  0.1439
 Depression - Top    2.7639218 3.349889 30.81   0.825  0.6905
 Slope - Top        -3.6354273 4.070070 34.89  -0.893  0.6481

P value adjustment: tukey method for a family of 3 means  
</code></pre>

<blockquote>
  <h3>Based on the pairs() output, it doesn't seem like Contour groups vary at these incremental values of P.</h3>
  
  <p><b>Since the Contour groups do not seem to vary at different levels of P, does that mean that the interaction strength is not that strong?  and thus, I am okay to ignore the warning message that ""NOTE: Results may be misleading due to involvement in interactions""?</b>  </p>
</blockquote>

<p>I would appreciate any feedback about interpreting these results, and whether there are additional analyses that I should be conducting in order to address my concern.  If there is any additional information that would be helpful in tackling this problem, please let me know.  </p>

<p>Thank you for your time!</p>

<p>UPDATE (2/6/15): I had a minor typo at the beginning, in which the first line of code read ""Dens.LMER &lt;- lmer(...)"".  The lmer product should have been named ""Na.LMER"", which was used in the remaining code.  Thus, the Dens.LMER product that rvl mentions is equivalent to Na.LMER.  I apologize for the inconvenience.  </p>
"
"0.22573305919324","0.222063948836235","136899","<p>My <strong>question</strong> is simple: </p>

<p>How do you determine the overall significance of an interaction (i.e. the marginal effect of $X$ on $Y$ for different values of $Z$)? </p>

<hr>

<p>But the background is a bit long-winded, so please bear with me. Consider this example (<a href=""http://homepages.nyu.edu/~mrg217/marginal_effect_plot.zip"" rel=""nofollow"">data available here</a> along with some <a href=""http://homepages.nyu.edu/~mrg217/marginal_effect_plot.pdf"" rel=""nofollow"">details</a>, taken from <a href=""https://files.nyu.edu/mrg217/public/jop2.pdf"" rel=""nofollow"">Berry, Golder and Milton 2012</a>): </p>

<pre><code>library(""foreign"")
library(""lmtest"")
dta &lt;- read.dta(""alexseev.dta"")
slvote.mod  &lt;- lm(xenovote ~ slavicshare * changenonslav  +
    inc9903 + eduhi02 + unemp02 + apt9200 + vsall03 + brdcont, data=dta)
coeftest(slvote.mod)
</code></pre>

<p>Which will output: </p>

<pre><code>&gt; coeftest(slvote.mod)  ##same as in Berry et al. 2012, but without clustered SEs

t test of coefficients:

                             Estimate  Std. Error t value  Pr(&gt;|t|)    
(Intercept)                8.94287844  2.71084994  3.2989 0.0016110 ** 
slavicshare                0.03148617  0.02089537  1.5068 0.1369279    
changenonslav             -0.85110769  0.35210573 -2.4172 0.0185955 *  
[..]
slavicshare:changenonslav  0.00822591  0.00527380  1.5598 0.1239045    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>However <a href=""https://files.nyu.edu/mrg217/public/pa_final.pdf"" rel=""nofollow"">Brambor, Clark and Golder (2006)</a> (which comes with an <a href=""https://files.nyu.edu/mrg217/public/interaction.html"" rel=""nofollow"">internet appendix</a>) caution that: </p>

<blockquote>
  <p>Although we still want to know about the marginal effect of some independent variable X on Y in an interaction model ($\beta_1 + \beta_3Z$), typical results tables will report only the marginal effect of X when the conditioning variable is zero, i.e., $\beta_1$. Similarly, these tables report only the standard error for this particular effect. As a result, the only inference that can be drawn in this situation is whether $X$ has a significant effect on $Y$ for the unique case in which $Z = 0$. [..] There is simply no way of knowing from the typical results table if $X$ has a significant effect on $Y$ when the conditioning variable is not zero.</p>
  
  <p>The analyst cannot even infer whether $X$ has a meaningful conditional effect on $Y$ from the magnitude and significance of the coefficient on the interaction term either. As we showed earlier, it is perfectly possible for the marginal effect of $X$ on $Y$ to be significant for substantively relevant values of the modifying variable $Z$ even if the coefficient on the interaction term is insignificant. [..] It means that one cannot determine whether a model should include an interaction term simply by looking at the significance of the coefficient on the interaction term. [..]</p>
  
  <p>The point here is that the typical results table often conveys very little information of interest because the analyst is not concerned with model parameters per se; he or she is primarily interested in the marginal effect of $X$ on $Y$ ($\beta_1 + \beta_3Z$) for substantively meaningful values of the conditioning variable $Z$.</p>
</blockquote>

<p>So what they propose is a <strong>marginal-effect plot</strong>. Following <a href=""http://www.ats.ucla.edu/stat/r/faq/concon.htm"" rel=""nofollow"">How can I explain a continuous by continuous interaction</a>, I can roughly replicate it in R as follows: </p>

<pre><code>##marginal effect plot
at.changenonslav &lt;- seq(min(dta$changenonslav, na.rm=T), 
        max(dta$changenonslav, na.rm=T), 1)
sl_slopes &lt;- coef(slvote.mod)['slavicshare'] + 
    coef(slvote.mod)['slavicshare:changenonslav'] * at.changenonslav

library(msm)
sl_estmean&lt;-coef(slvote.mod)
sl_var&lt;-vcov(slvote.mod)
SEs &lt;- rep(NA, length(at.changenonslav))
for (i in 1:length(at.changenonslav)){
    j &lt;- at.changenonslav[i]
    SEs[i] &lt;- deltamethod(~ (x2) + (x10)*j, sl_estmean, sl_var)
}
sl_upper &lt;- sl_slopes + 1.96*SEs
sl_lower &lt;- sl_slopes - 1.96*SEs

plot(at.changenonslav, sl_slopes, ylim=c(-.05, .25), type='l', 
     ylab='Marginal effect of X (""slavicshare"") on Y', 
     xlab='Level of Z (""changenonslav"")')
points(at.changenonslav, sl_upper, type = ""l"", lty = 2)
points(at.changenonslav, sl_lower, type = ""l"", lty = 2)
abline(h=0, col=""grey"")
</code></pre>

<p>Which outputs this (not unlike the <a href=""http://homepages.nyu.edu/~mrg217/fig_jop2.pdf"" rel=""nofollow"">graph in Stata</a> by the authors of the paper): </p>

<p><img src=""http://i.stack.imgur.com/WgzqM.png"" alt=""enter image description here""></p>

<p>Brambor, Clark and Golder (2006) go on to suggest that (slightly paraphrased): </p>

<blockquote>
  <p>95% confidence intervals around the line allow us to determine the conditions under which $X$ has a statistically significant effect on $Y$â€”they have a statistically significant effect whenever the upper and lower bounds of the confidence interval are both above (or below) the zero line.</p>
</blockquote>

<p>There are also the <strong>effect displays</strong> from <code>effects</code> package in R: </p>

<pre><code>##effects display
require(effects)
plot(effect(""slavicshare:changenonslav"", slvote.mod, xlevels=4))
</code></pre>

<p>Which outputs the following graph, although it seems to me that this is a different (more detailed) presentation of the marginal-effect plot above, and that it doesn't help directly wrt statistical significance (i.e. the interpretation is less intuitive and more subjective): </p>

<p><img src=""http://i.stack.imgur.com/fMdQx.png"" alt=""enter image description here""></p>

<p>Another way to determine significance of interactions seems to be suggested (although not explicitly) in <a href=""http://www.jstatsoft.org/v08/i15"" rel=""nofollow"">Fox 2003</a>. There each linear model is followed by an <strong>Anova</strong> table which, if my understanding is correct, indicates if the interaction effect is statistically significant overall. </p>

<pre><code>##Anova
require(car)
Anova(slvote.mod)
</code></pre>

<p>Which outputs: </p>

<pre><code>&gt; Anova(slvote.mod)
Anova Table (Type II tests)

Response: xenovote
                           Sum Sq Df F value    Pr(&gt;F)    
slavicshare                41.374  1  8.8471 0.0041787 ** 
changenonslav              34.098  1  7.2913 0.0089209 ** 
[..]
slavicshare:changenonslav  11.377  1  2.4329 0.1239045    
Residuals                 289.945 62                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</code></pre>

<p>But the two methods seemingly contradict each other in this case, the marginal-effect plot indicating significance, while the Anova indicating lack of significance for the interaction...</p>

<hr>

<p>So to return to my original <strong>question</strong>, how can I determine if the marginal effect of $X$ on $Y$ (for different values of $Z$) is overall statistically significant? </p>

<p>Are both the <em>marginal-effect plot</em> and the <em>Anova</em> (as presented above) valid tools for determining the significance of an interaction? Is one preferred over the other? Are there other statistical tools that can assist in this case? I'm interested in statistical approaches more generally, although solutions in R are very much appreciated...</p>
"
"0.0691163516376137","0.0701862406343596","136927","<p>I want to compare the following two linear models:</p>

<pre><code>model 1: y = mean + A + B  
model 2: y = mean + A + A*B
</code></pre>

<p>Is model 2 equivalent to y = mean + A + B + A*B? Can I use <code>anova(model1, model2)</code> in R to compare the two nested models? </p>

<p>If not, how can I compare them in R?</p>
"
"0.0691163516376137","0.0701862406343596","138132","<p>I have a model based on a dataset that respects all linear model assumptions except for homoscedasticity. When I just ignore the problem of heteroscedasticity, the p-value, for the interaction with group, in my model is &lt;.00001. I definitely know that there is something as per my previous studies and the literature in this field. However, I would like to be honest regarding my analyses and assumptions. Is this assumption really needed if the other 3 main ones are respected (independence, linearity, absence of collinearity) for the interpretation of the p-value in the mixed effects models? </p>

<p>When I run the following on my lmer model called mod:</p>

<pre><code>plot(fitted(mod),residuals(mod))
</code></pre>

<p><img src=""http://i.stack.imgur.com/yYkUm.png"" alt=""enter image description here""></p>

<p>I get a cone shape distribution. I then try to log transform it, and recheck the model, for the interaction with group in my model the p value goes to .40. Quite a jump! My data comes brain activity from patients and healthy individual, just to clarify.</p>

<p>This is my model: </p>

<blockquote>
  <p>lmer(value ~ dist*group + (1|patientnumber), dat1)</p>
</blockquote>

<p>This is how I obtained the p-value:</p>

<blockquote>
  <p>Anova(mod)</p>
</blockquote>

<p>Kindly advise.</p>
"
"0.11356975465314","0.128141957406415","138908","<p>I am looking at the effects role has an opportunities to collaborate between groups in a social network. At a basic level the data are modeled as:</p>

<pre><code>relRatio~role
</code></pre>

<p>With relative ratio being the percentage of teammates who are part of the subject's normal group. The data I have come from multiple time slices over the years, with some of the subjects being polled two or more times. Not every subject has multiple entries, nor does every subject with multiple entries have the same number of entries. From some advice I received it was suggested that I test the differences between groups using a random effect ANOVA model, which would be modeled (in R) as</p>

<pre><code>relRatio~role+Error(subjectId)
</code></pre>

<p>After trying to read up more on random effects ANOVA, I started to get the impression that linear mixed effects models (with the lmer) package are preferred over random effects ANOVA, although I have yet to see a clear distinction between the two. This leads to my first question: Which approach is best for modeling my data?</p>

<p>If it involves using the random effects ANOVA, I would greatly appreciate it if someone could recommend a resource for the process of interpreting the results.</p>

<p>My second question is, if the better approach is to use the mixed effects models, which I have tried, why do I get striping in my residuals?</p>

<p><img src=""http://i.stack.imgur.com/vqWjn.png"" alt=""enter image description here""></p>

<p>One guide suggested that I am dealing with categorical data, which requires the use of logistic regression. However, the dependent variable for my data is continuous, and the IV is categorical, which I thought LMM are supposed to handle. This leads to my second question - does my residual plot indicate something is wrong with the way I have modeled my data?</p>
"
"0.11286652959662","0.100286944635719","140055","<p>I have a dataset with thousands of observations pre-assigned to 18 groups and with measures for 8 different variables. I am using canonical discriminant analysis to see how separable my 18 groups are. What I am actually most interested in is which individual variable separates the groups most (and least). </p>

<p>I have tried running canonical discriminant analysis in R using the ldm() function from the MASS library. </p>

<pre><code>mydata.lda &lt;- lda(group ~ x1 + x2 + x3 .... + x8, data=mydata)
</code></pre>

<p>If I understand correctly, the output has coefficients of linear discriminant which indicates how strongly each variable is associated with each individual discriminant function, and I could standardize the coefficients to help interpret the meanings of the resultant discriminant functions. </p>

<p>I think what I want however is the partial F-square of each individual variable, or the relative ability of each variable to separate groups across all discriminant functions, not one at a time. 
In SPSS, the discriminant analysis function allows one to ask for ""univariate ANOVAs"" which seem to produce what I want: a table showing the Wilks' Lambda statistic and F statistics for each of my 8 variables. How would I get this kind of output in R? Do I need to run a (M)ANOVA based on the output of my lda()? </p>
"
"0.0651635212451075","0.0827152778309109","140140","<p>This paper (<a href=""http://psycnet.apa.org/journals/med/6/4/147/"" rel=""nofollow"">http://psycnet.apa.org/journals/med/6/4/147/</a>) states that departures from normality can be tolerated for one-way ANOVA. </p>

<blockquote>
  <p>""The results give strong support for the robustness of the ANOVA under
  application of non-normally distributed data.""</p>
</blockquote>

<p>I have a factorial design and some of my data is not normal distributed. Although it is only a minor fraction of the total data set (28 out of 340 samples) I wonder if it is legitimate to proceed with a factorial design ANOVA.</p>

<p>My dependant variable is the relative absorption in an IR range (defined by DRIFT analysis) and my independent variables are timepoint, treatment, exposition, depth. Per sampling condition (example: timepoint=0, treatment=x, exposition=north, depth= 0-5cm) I have 3 replicates.</p>

<p>I have 4 treatment, 3 timepoints, 2 exposition and 2 depth. As some replicates are missing, my design is unbalanced and I used type III Anova (from car package) in R . I assumed a linear model with interactions. (linear Model = Absorption ~ Treatment * Timepoint * Depth * Exposition)</p>

<p><img src=""http://i.stack.imgur.com/BkO0p.png"" alt=""Type 3 Anova for unbalanced design""></p>

<p><img src=""http://i.stack.imgur.com/ogoIl.png"" alt=""Type 2 Anova for unbalanced design""></p>
"
"0","0","140605","<p>I am trying to compare a linear model and other non linear models(Asymptotic, Logistic and Ricker) by means of an F test or a likelihood ratio test. I have tried anova(Linear, Logistic,Ricker, Asymptote) but this generates an error. Is there a way to do this in R?</p>

<p>I used the following models:</p>

<pre><code>Linear&lt;-lm(mean~age,Lmaxl)
Logistic&lt;- nlsLM(mean ~ k/(1+((k- Bo)/Bo)*exp(-r*age)), 
    data=Lmaxl, start=list(k=50,Bo=20,r=0.1), 
    control=liâ€Œst(maxiter=200),
    na.action=""na.exclude"")
Asymptote&lt;-nlsLM(mean ~k+(Bo-)*exp(-r*age), 
    data=Lmaxl,
    start=list(k=50,Bo=20,r=0.1),
    control=list(maxitâ€Œâ€‹er=200),
    na.action=""na.exclude"")
 Ricker&lt;- nlsLM( mean~ Bo+(a*age)*exp(-b*age), 
    data=Lmaxl, 
    start=list(Bo=10, a=5, b=0.01),
    control=list(maxiter=200),
    na.action=""na.exclude"")
</code></pre>
"
"0.0892288262810312","0.0906100470365937","141279","<p>For a <code>MANOVA</code> with $n$ variables, I would like to do pairwise comparisons between $k$ levels for one of the variables. </p>

<p>What is the suitable method to adopt for this while adjusting $\alpha$ for the $k(k-1)$ multiple comparisons?</p>

<ol>
<li><p>Is multiple <code>Hotelling</code> $T^2$ tests along with <code>Bonferroni</code> correction or FDR/pFDR appropriate? FDR/pFDR q values would be preferable as the $\beta$ value is important here.</p></li>
<li><p>Any suggestions for <code>R</code> packages to do the same? (Particularly for MANOVA post-hoc multiple comparisons}</p></li>
<li><p>How to test the null hypothesis $H_0^j:|\mu_1^j-\mu_2^j|\ge\delta$ instead of  $H_0^j:\mu_1^j=\mu_2^j$ as in an equivalence test for the multiple comparisons?</p></li>
</ol>

<h1>Edit</h1>

<p>Based on the answer and further comment by <a href=""http://stats.stackexchange.com/users/52554/rvl"">rvl</a>, I was able to explore and come up with the following.</p>

<pre><code>library(lsmeans)
# Use the `oranges` dataset in `lsmeans` package.
# multivariate linear model
oranges.mlm &lt;- lm(cbind(sales1,sales2) ~ price1 + price2 + day + store,
                  data = oranges)
# Get the least square means
oranges.Vlsm &lt;- lsmeans(oranges.mlm, ""store"")
# Multiple comparisons with fdr p value adjustment
test(contrast(oranges.Vlsm, ""pairwise""), side = ""="",  adjust = ""fdr"")
# With threshold spcified
test(contrast(oranges.Vlsm, ""pairwise""), side = ""="",  adjust = ""fdr"", delta = 0.25)
</code></pre>
"
"0.17866033025664","0.190065238130186","141746","<p>Given three variables, <code>y</code> and <code>x</code>, which are positive continuous, and <code>z</code>, which is categorical, I have two candidate models given by:</p>

<pre><code> fit.me &lt;- lmer( y ~ 1 + x + ( 1 + x | factor(z) ) )
</code></pre>

<p>and</p>

<pre><code> fit.fe &lt;- lm( y ~ 1 + x )
</code></pre>

<p>I hope to compare these models to determine which model is more appropriate. It seems to me that in some sense <code>fit.fe</code> is nested within <code>fit.me</code>. Typically, when this general scenario holds, a chi-squared test can be performed. In <code>R</code>, we can perform this test with the following command,</p>

<pre><code> anova(fit.fe,fit.me)
</code></pre>

<p>When both models contain random-effects (generated by <code>lmer</code> from the <code>lme4</code> package), the <code>anova()</code> command works fine. Owing to boundary parameters, it is normally advisable to test the resulting Chi-Square statistic via simulation, nonetheless, we can still <em>use</em> the statistic in the simulation procedure.</p>

<p>When both models contain <em>only</em> fixed-effects, this approach---and, the associated <code>anova()</code> command---work fine.</p>

<p>However, when one model contains random effects and the reduced model contains <em>only</em> fixed-effects, as in the above scenario, the <code>anova()</code> command doesn't work.</p>

<p>More specifically, I get the following error:</p>

<pre><code> &gt; anova(fit.fe, fit.me)
 Error: $ operator not defined for this S4 class
</code></pre>

<p>Is there anything wrong with using the Chi-Square approach from above (with simulation)? Or is this simply a problem of <code>anova()</code> not knowing how to deal with linear models generated by different functions?</p>

<p>In other words, would it be appropriate to manually generate the Chi-Square statistic derived from the models? If so, what are the appropriate degrees of freedom for comparing these models? By my reckoning:</p>

<p>$$ F = \frac{\left((SSE_{reduced}-SSE_{full})/(p-k)\right)}{\left((SSE_{full})/(n-p-1)\right)} \sim F_{p-k,n-p-1} $$</p>

<p>We are estimating two parameters in the fixed effects model (slope and intercept) and two more parameters (variance parameters for the random slope and random intercept) in the mixed-effects model. Typically, the intercept parameter isn't counted in the degrees of freedom computation, so that implies that $k=1$ and $p=k+2=3$; having said that I'm not sure if the variance parameters for the random-effects parameters should be included in the degrees of freedom computation; the variance estimates for fixed-effect parameters are <em>not considered</em>, but I believe that to be because the parameter estimates for fixed effects are assumed to be <em>unknown constants</em> whilst they are considered to be <em>unknowable random variables</em> for mixed effects. I would appreciate some assistance on this issue.</p>

<p>Finally, does anybody have a more appropriate (<code>R</code>-based) solution to comparing these models?</p>
"
"0.154548860618483","0.136015711124441","142317","<p>I want to do <strong>multivariate</strong> (with more than 1 response variables) <strong>multiple</strong> (with more than 1 predictor variables) <strong>nonlinear regression</strong> in <strong>R</strong>.</p>

<p>The data I am concerned with are 3D-coordinates, thus they interact with each other, i.e. the x,y,z-coordinates are not independent. So I cannot just call the <em>nls</em> separately for each response variable (which I tried at first). </p>

<p>A subset of the data-frame with 3D-coordinates where x,y,z are the predictive variables and a,b,c the response variables:</p>

<pre><code>              x           y         z           a            b         c
1  -2.26470e-03 -0.05081670 0.0811701 -0.00671079 -0.045721600 0.0705679
2  -9.13106e-05 -0.00670734 0.0724838 -0.00676299 -0.001638430 0.0588486
3   3.81399e-04  0.03556000 0.0782059 -0.00783726  0.038503800 0.0641364
4   1.42293e-03  0.06133920 0.0708688 -0.00820760  0.062697100 0.0572740
5  -5.06043e-02  0.04759040 0.0418189 -0.05949350  0.040427800 0.0266159
6   5.92963e-02  0.04183450 0.0431029  0.05124780  0.038396500 0.0327903
7  -4.44213e-02 -0.00909717 0.0459059 -0.05021130 -0.005634520 0.0329833
8  -3.75400e-02 -0.00625770 0.0567296 -0.04255200 -0.000666089 0.0436465
9  -2.37768e-02 -0.00707318 0.0581552 -0.03048950 -0.001260670 0.0457355
10 -1.56645e-02 -0.01326670 0.0540247 -0.02101350 -0.009021990 0.0413755
</code></pre>

<p><strong>My question:</strong> Is it possible to call the <em>nls</em> function with more than 1 (in my case 3) response variables? In other words is it possible to substitute <em>y</em> in <code>nls(y ~ f(x,y,z, parameters), data)</code> with something like <em>c(a,b,c)</em> or <em>cbind(a,b,c)</em>, such that <code>nls(cbind(a,b,c) ~ f(x,y,z, parameters), data)</code> ?</p>

<p>In the post <a href=""http://stackoverflow.com/questions/12161659/how-to-write-r-formula-for-multivariate-response"">How to write R formula for multivariate response?</a> it is shown that one can combine several response variables with <em>cbind</em> in the case of linear modeling with the <em>lm</em> function.
This doesn't seem to work for nonlinear modeling with <em>nls</em> .., because the <em>nls</em> call in the code sample at the bottom of my question throws the following error:</p>

<p><code>Error in parse(text = x) : &lt;text&gt;:2:0: unexpected end of input
1: ~ 
   ^</code></p>

<p>which I could not find a solution for online concerning my case of a multivariate regression..</p>

<hr>

<p>My web-searches to my main question only gave me results concerning <em>multivariate <strong>linear</strong> regression</em>, which for example included <a href=""http://stats.stackexchange.com/questions/11127/multivariate-multiple-regression-in-r/11132#11132"">solutions with the manova function</a>..</p>

<p>Therefore, <strong>my question asked in a more general way:</strong> How do you in general solve such a non-linear multivariate multiple regression problem in R which takes into account interactions/dependencies between variables?</p>

<p>Here is my code where </p>

<ul>
<li>function <em>f</em> computes the rotations of coordinates about three axes
in the order x-axis, y-axis, and then z-axis (unfortunately I cannot
include the pic of the equation I wrote in LaTeX here since I haven't
got 10 reputation points yet);</li>
<li><em>rot_data_all</em> is structured as the data-subset above, just with more rows;</li>
<li>alpha1, alpha2 and so on are the parameters which nonlinear
regression should approximate:</li>
</ul>

<p>The code:</p>

<pre><code>f &lt;-function(x, y, z, alpha1, alpha2, alpha3, gamma, theta, phi, s) { 
      a &lt;- alpha1 + s*(cos(theta)*cos(phi)*x - cos(theta)*sin(phi)*y + sin(theta)*z)
      b &lt;- alpha2 + s*((sin(gamma)*sin(theta)*cos(phi) + cos(gamma)*sin(phi))*x 
                          + (-sin(gamma)*sin(theta)*sin(phi) + cos(gamma)*cos(phi))*y
                          - sin(gamma)*cos(phi)*z)
      c &lt;- alpha3 + s*((cos(gamma)*sin(theta)*cos(phi) + sin(gamma)*sin(phi))*x 
                          + (cos(gamma)*sin(theta)*sin(phi) + sin(gamma)*cos(phi))*y
                          + cos(gamma)*cos(phi)*z)
      return(c(a,b,c))
    }

    rot.nls &lt;- nls(cbind(a, b, c) ~ f(x, y, z, alpha1, alpha2, alpha3, gamma, theta, phi, s), 
                   data = rot_data_all, 
                   start = c(alpha1 = 0, alpha2 = 0, alpha3 = 0, gamma = 0.1, theta = 0.1, phi = 0.1, s = 0.1), trace = TRUE)
</code></pre>

<hr>

<p>I hope to find a solution which is general enough to also solve other transformations which cannot be easily linearized like the set of equations for <strong>projective transformation</strong>, i.e. something like the following function:</p>

<pre><code>f.proj &lt;-function(x, y, z, betas) {
  a &lt;- (betas[1,1]*x + betas[1,2]*y + betas[1,3]*z + betas[1,4]) / (betas[4,1]*x + betas[4,2]*y + betas[4,3]*z + betas[4,4])
  b &lt;- (betas[2,1]*x + betas[2,2]*y + betas[2,3]*z + betas[2,4]) / (betas[4,1]*x + betas[4,2]*y + betas[4,3]*z + betas[4,4])
  c &lt;- (betas[3,1]*x + betas[3,2]*y + betas[3,3]*z + betas[3,4]) / (betas[4,1]*x + betas[4,2]*y + betas[4,3]*z + betas[4,4])
  return(c(a,b,c))
}
</code></pre>

<hr>

<p>I am happy to provide more information if needed! Thank you so much!</p>
"
"0.218678901093451","0.222063948836235","142489","<p>I'm analysing PAM fluorescence data from an experimental set-up that I duplicated from an earlier experiment with a missing control. That's why I haven't given the statistics of the experiment much (if any) thought in advance.</p>

<p>The set-up consisted of 8 containers with peat moss (<em>Sphagnum magellanicum</em>), divided over 4 treatments, so that each treatment was performed in duplicate. At regular (weekly) intervals, over the course of 3 months, I performed life PAM fluorescence measurements on a number of capitula (growth tops) in each container to determine a kinetic response curve for each of these capitula.</p>

<p>To minimize intraleaf (in my case, intra<em>capitula</em>) variance, ideally, PAM fluorescence measurements would have been repeated for the same leaf every week in the 3-month time series, but for practical reasons, my AOIs (areas of interest) for the fluorescence meter where located on different capitula every week. This is also my first subquestion: can I consider measurements at different time points in the same container as <em>repeated measures</em>, or would this only be valid if I had been measuring the same AOIs every week? And does this depend on whether I aggregate the measured values of the different AOIs per container before further analysis?</p>

<p>After nightfall, once every week, for 5â€“7 AOIs in each container, I determined a kinetic curve, for which the PAM software performs 20 measurements. The first measurement represents the dark-adapted fluorescence values, after which an actinic light source (at a wavelength that can facilitate photosynthesis) is started for the 19 remaining measurements. From the start of the kinetic curve (the dark adapted $\phi_{PSII}$ values), I determine $F_v/F_m$ and from the end of the curve (the flat part), I determine $\text{mean}(\phi_{PSII})$. $\phi_{PSII}$ and $F_v/F_m$ measure the quantum yield of photosystem II and the max. efficiency of photosystem II, respectively; $F_v/F_m = \phi_{PSII}$ in a dark-adapted state.</p>

<p>I'm interested in building two models, one in which the response (dependent) variable is $\phi_{PSII}$ and one in which it is $F_v/F_m$. The (independent) predictor variables are:</p>

<ul>
<li><code>AOI</code> (factor): a number between 1â€“6; </li>
<li><code>Container</code> (factor): a number between 1â€“8; </li>
<li><code>Treatment</code>: (factor): a number between 1â€“4; and</li>
<li><code>DaysTreated</code> (integer): the number of days since the treatments began.</li>
</ul>

<p>My guess is that I should treat <code>AOI</code> and <code>Container</code> as random effects variables, with <code>AOI</code> nested in <code>Container</code> and <code>Container</code> nested in the fixed effect variable <code>Treatment</code>. <code>DaysTreated</code>, then, would be my continuous predictor (covariate). For $\phi_{PSII}$, I would model this in R like this:</p>

<pre><code>library(nlme)
YII_m1 &lt;- lme(mean_YII ~ DaysTreated * Treatment,
              random = ~1 | Container / AOI,
              method = ""ML"",
              data = fluor_aoi)
# fluor_aoi is a data-frame in which each AOI kinetic curve is
# aggregated into one row, where mean_YII = mean( YII[15:19] )
# and FvFm = YII[1]
</code></pre>

<p>I'm not sure if this is the most parsimious model. To find out, I want to try different models with different fixed effects but all with the same random effects. <code>anova.lme()</code> warned me that comparing between these models is a <a href=""http://stats.stackexchange.com/questions/116770/"">no-go</a> when using the default method (<code>method = ""REML""</code>), which is why I use <code>method = ""ML""</code>.</p>

<pre><code>anova(YII_m1, # ~ DaysTreated * Treatment
      YII_m2, # ~ DaysTreated:Treatment + Treatment
      YII_m3, # ~ DaysTreated:Treatment + DaysTreated
      YII_m4, # ~ DaysTreated:Treatment
      YII_m5, # ~ DaysTreated + Treatment
      YII_m6, # ~ DaysTreated
      YII_m7  # ~ Treatment
     )

       Model df       AIC       BIC   logLik   Test  L.Ratio p-value
YII_m1     1 11 -2390.337 -2340.578 1206.168                        
YII_m2     2 11 -2390.337 -2340.578 1206.168                        
YII_m3     3  8 -2390.347 -2354.158 1203.173 2 vs 3  5.99019  0.1121
YII_m4     4  8 -2390.347 -2354.158 1203.173                        
YII_m5     5  8 -2366.481 -2330.293 1191.241                        
YII_m6     6  5 -2363.842 -2341.224 1186.921 5 vs 6  8.63915  0.0345
YII_m7     7  7 -2264.868 -2233.203 1139.434 6 vs 7 94.97389  &lt;.0001
</code></pre>

<p>I would have liked it if the <a href=""http://stats.stackexchange.com/questions/63464/is-this-an-acceptable-way-to-analyse-mixed-effect-models-with-lme4-in-r"">best fit</a> was model 2 with the fixed effects formula <code>~ DaysTreated:Treatment + Treatment</code>, because my expectation at the onset of my experiment was to see a decline in <em>Sphagnum</em> vitality, but only for some of the treatments and hopefully not in the controls. (The acclimatization period was very long, hoping that any effects on the mosses of the new (greenhouse) environment would have flattened out by the onset of the treatments.)</p>

<p><strong>Edit 2015-May-1:</strong> First I compared only 6 models; model 4 was missing from my initial question. Also, I forgot to factorize treatment, so that instead of model 2, now, different models give the â€˜best fitâ€™.</p>

<p>Anyway, so far (unless you tell me otherwise), I feel I can continue to use model 2, which also best fits the visual observation that 4 of the 8 containers where doing very badly at the end of the experiment while the other 4 seemed to do ok.</p>

<pre><code>anova(YII_m2)
                  numDF denDF  F-value p-value
(Intercept)           1   620 526.9698  &lt;.0001
Treatment             3     4   5.0769  0.0753
DaysTreated:Treatment 4   620  36.4539  &lt;.0001
</code></pre>

<p>An ANCOVA test on model 2 reveals that only the interaction between <code>DaysTreated</code> and <code>Treatment</code> is significant, which makes sense to me, given that the containers started out in roughly the same condition after acclimatization. There was visible difference between containers in the same treatments, but that should have been taken care of by correcting for the random error effect.</p>

<p>Mean $\phi_{PSII}$ plotted per container shows that there have been differences between containers within the same treatment from the onset of the treatments:</p>

<p><img src=""http://i.stack.imgur.com/GgGhG.png"" alt=""Mean Y_II plotted per container shows that there have been differences between containers within the same treatment from the onset of the treatments.""></p>

<p>Now that I've made an <em>attempt</em> at constructing and testing a somewhat decent model (which I'd love to receive criticism on), I'd like to perform a multiple pairwise comparison to find out which treatments diverge significantly from each other over time, but I have no idea what is the proper way to approach this.</p>

<p>Also, I want to try a linear correlation, but again, I'm clueless as to how. Is there an appropriate way to integrate this in my model or should I try to model a regression per treatment? </p>

<p>Please forgive the ignorance in my approach and my questions. I'm a BSc student whose statistical background mainly consists of a brief entry-level course, followed by a recipe-level R course. RTFM comments are definitely welcome, as long as they include a link to TFM.</p>
"
"0.11286652959662","0.114613651012251","143556","<p>I would like to model a treatment effect in two different groups, controlled for some co-variates (like age and education), and I assume that a two-way repeated-measure Anova would be the right approach - if yes, I have some questions on how to model this design.</p>

<p>I'm a bit confused on how to do this with R (and the  <code>lme4</code> package), because I found different approaches for the same design. Let's say, I have following variables:</p>

<ul>
<li>subject</li>
<li>group (control vs treatment group)</li>
<li>time (t0 vs t1, i.e. two measures for each subject)</li>
<li>age (co-variate)</li>
<li>education (co-variate)</li>
</ul>

<p>Am I right, that, according to <a href=""http://stats.stackexchange.com/questions/58745/using-lmer-for-repeated-measures-linear-mixed-effect-model"">this posting on Cross Validated</a>, my model would look like this?</p>

<ol>
<li>model: <code>lmer(DV ~ group * time + age + education + (1+time|subject), mydata)</code> </li>
</ol>

<p>Then I found <a href=""http://www.uni-kiel.de/psychologie/rexrepos/posts/anovaMixed.html#mixed-effects-analysis-1"">this tutorial</a>. Following these instructions, my model would look like this?</p>

<ol start=""2"">
<li>model: <code>lmer(DV ~ group * time + age + education + (1|subject) + (1|group:subject) + (1|time:subject), data=mydata)</code></li>
</ol>

<p>Now I have two questions:</p>

<p>a) which of the two above models is correct? or do both work?</p>

<p>b) my data is in long format, how should my variable <code>subject</code> look like? the same value for each measured person, i.e. a value appears twice in this variable (for <em>person A in group X</em> at <strong>t0</strong> and <em>person A in group X</em> at <strong>t1</strong> the same value), or should each row/observation be indicated by a new, unique ID?</p>
"
"0.132809685425692","0.146104310758895","144349","<p>I'm trying to replicate SPSS output in R for a mixed ANOVA with a polynomial contrast to test a linear trend.</p>

<p>I fitted a mixed ANOVA in R (see code below), but I can't figure out how to get the results of the polynomial contrast for the within subjects variable and how to produce type III Sums of Squares (since that is the type SPSS uses).</p>

<p>I found several posts related to this question (see e.g., <a href=""http://stats.stackexchange.com/questions/4544/how-does-one-do-a-type-iii-ss-anova-in-r-with-contrast-codes"">How does one do a Type-III SS ANOVA in R with contrast codes?</a> and <a href=""http://stats.stackexchange.com/questions/140183/mixed-model-type-iii-sums-of-squares-r-vs-spss"">Mixed Model Type-III Sums of Squares- R vs SPSS</a>), but I couldn't find the answer to my question.</p>

<p>In the code below, the data set is downloaded directly from Open Science Framework. </p>

<p>My dependent variable is perc_causal_words, my between subjects variable is AffCoh, and my within subjects variable is story.</p>

<p>The SPSS result of the polynomial contrast for story*AffCoh is  F(1, 111) = .99, p=.322.</p>

<p>Can someone help me in getting a mixed ANOVA with type III Sums of Squares and a polynomial contrast?</p>

<p>Here is the SPSS syntax I am trying to replicate:</p>

<pre><code>GLM cause.1.00 cause.2.00 cause.3.00 BY AffCoh
/WSFACTOR=story 3 Polynomial 
/METHOD=SSTYPE(3)
/CRITERIA=ALPHA(.05)
/WSDESIGN=story 
/DESIGN=AffCoh.
</code></pre>

<p>The data and the code in R:</p>

<pre><code>library(""httr"")
library(""RCurl"")
source(""http://sachaepskamp.com/files/OSF/getOSFfile.R"") # the getOSFfile function
library(""foreign"")
library(""tidyr"")

##@@ DATA LOADING @@##
file &lt;- getOSFfile(""https://osf.io/nwbpd/"")
data &lt;- read.spss(file)

##@@ DATA MANIPULATION @@##
# select only the variables relevant for the main analysis
# that is: participant number, number of causal words in story 1, 2, and 3, 
# and whether participants were in the coherent or incoherent condition
data_mod &lt;- data.frame(data$Participant,
                           data$cause.1.00,
                           data$cause.2.00,
                           data$cause.3.00,
                           data$AffCoh)

colnames(data_mod) &lt;- c(""subject"",""cause1"",""cause2"",""cause3"",""AffCoh"")
data_mod$subject &lt;- as.factor(data_mod$subject)

# gather data into a long format
data_long &lt;- gather(data=data_mod, 
                    key=story, 
                    perc_causal_words, 
                    cause1:cause3)

# fit mixed ANOVA
aov &lt;- aov(perc_causal_words ~ AffCoh * story + Error(subject/story), data=data_long)
</code></pre>
"
"NaN","NaN","148794","<p>I try to compare samples in function of a different treatment (x).<br>
The design possess inner-replicate (b) nested in replicate (a).<br>
Thus, i want to take account  of the inner-replicate as random effect.<br>
I had performed Mixed Effect Model and General Linear Mixed Model, but the Normal distribution of the residuals and the homoscedasticity of the residuals are not respected.<br>
What test can i use in this situation ?<br>
I'm specially interested in median comparisons.  </p>

<p>Here a R reproducible example:</p>

<pre><code>data &lt;- read.table(text = ""y,x,a,b
3.8535461,1,1,1
3.7672284,1,1,2
4.3958063,1,1,3
2.6762155,1,2,1
4.5604866,1,2,2
1.5892352,1,2,3
2.4078456,1,3,1
3.0846585,1,3,2
3.8501476,1,3,3
1.2837078,2,1,1
1.4770487,2,1,2
0.6881346,2,1,3
3.4812401,2,2,1
4.2177414,2,2,2
3.6936182,2,2,3
1.3323660,2,3,1
0.5364934,2,3,2
2.7027026,2,3,3
2.7258901,3,1,1
2.2834023,3,1,2
3.1254439,3,1,3
2.8741295,3,2,1
2.4544474,3,2,2
3.2790297,3,2,3
2.2481289,3,3,1
2.6108048,3,3,2
1.6789640,3,3,3
2.0489823,4,1,1
3.6704609,4,1,2
2.0028304,4,1,3
1.4445633,4,2,1
0.0000000,4,2,2
2.1329823,4,2,3
1.7065646,4,3,1
0.0000000,4,3,2
0.9242589,4,3,3
2.4239174,5,1,1
1.0919233,5,1,2
0.0000000,5,1,3
2.4501427,5,2,1
2.2731563,5,2,2
1.8855533,5,2,3
0.3576744,5,3,1
1.3190856,5,3,2
1.7817091,5,3,3"",
sep = ',', header = TRUE)

data$x=as.factor(data$x)
data$a=as.factor(data$a)
data$b=as.factor(data$b)

library(nlme)

lme2 = lme(fixed= y ~ x,random= ~ 1|a/b,data=data)
summary(lme2)
anova(lme2)
shapiro.test(residuals(lme2))
bartlett.test(residuals(lme2), data$x)
</code></pre>
"
"0.149308384327784","0.151619608715781","151200","<p>I have been trying to figure out how to do a fairly basic repeated measures analysis using linear mixed effects in R, and then analysing it using post-hoc tests. The problem is that I'm not sure whether the output I get is statistically sound?</p>

<p>The response variable: <code>weighted</code>- an index of habitat preference (prop. individuals on habitatA / prop. of total habitat that is A). A value above 1 indicates the habitat is being used more than what you would expect from its availability. this was repeatedly  measured on the same colony through time over several weeks</p>

<p>Fixed variables: <code>Type</code> - habitat type (live/dead), <code>weeks</code> - the time variable</p>

<p>Random variables: <code>colony</code> - because each measurement of colony violates independence assumption.</p>

<p>Here's what the data loss like plotted over time (orange=live habitat, blue=dead habitat):</p>

<p><img src=""http://i.stack.imgur.com/atUVm.jpg"" alt=""enter image description here""> </p>

<p>i run the analysis using the <code>lmer()</code> function from the <code>lme4</code> package:</p>

<pre><code>results_full=lmer(weighted~type*weeks+(weeks|colony), data=Pos, REML=F)
</code></pre>

<p>My reasoning is that i have no reason to expect a random intercept, they should all start on 1 at time 0, and then individuals will start avoiding the dead habitat and favouring the live habitat. The <code>(weeks|colony)</code> term allows the slope of each colony to be random across time?</p>

<p>So to my question:</p>

<p>I compare the likelihood of two models with each other, in a likelihood ratio test to get p-values of the fixed effects using a reduced model:</p>

<pre><code>results_null=lmer(weighted~type+weeks+ (weeks|colony), data=Pos, REML=F)
anova(results_null, results_full)
</code></pre>

<p>But what I'm really interested in is at what time point (week) do the individuals start avoiding the dead habitat. as you can see from the figure this happens at week 1 so comparing live-dead habitat week by week ""should"" generate a n/s result at week 0 and sig result from then on (I'm not trying to force a statistically significant result, but the fig is pretty clear...)</p>

<p>I tried converting the weeks into a factor, and then performing </p>

<pre><code>lsmeans(results_full, pairwise~type+weeks)
</code></pre>

<p>But it didn't generate anything that seemed meaningful, the output didn't make sense in relation to the data. </p>

<p>Does anyone have any thoughts on A) whether my model and test is appropriate to this data, and B) how I can perform a post hoc test to compare habitat type over time?</p>

<p>Would it be appropriate to use a Dunetts post hoc test to compare preferences to a reference value (=1) rather than to each other?</p>

<p>Grateful for any ideas or pointers!</p>
"
"0.11356975465314","0.128141957406415","153547","<p>I have a very large data set with repeated measurements of same blood value (co) (1 to 7 measurements per patient). Each measurement is coupled with time which is the time interval between surgical operation and blood level measurement. </p>

<p>My aim is to show that this blood value correlates positively with time.</p>

<p>Blood level measurements are highly skewed to <em>right</em> and hence I am using a log-transformation and linear mixed effect regression model (lmer in lme4 package).</p>

<p>I have constructed a null model: </p>

<pre><code>fit1&lt;-(lmer(lgco~(1|id),data=ASR))
</code></pre>

<p>Model 2 includes time as independent variable:</p>

<pre><code>fit2&lt;-(lmer(lgco~time+(1|id),data=ASR))
</code></pre>

<p>Id is the patient number in th dataset.</p>

<p>By using the anova() function I see that fit2 is significantly better than fit1:</p>

<pre><code>&gt; anova(fit1,fit2)
refitting model(s) with ML (instead of REML)

Data: ASR
Models:
fit1: lgco ~ (1 | id)
fit2: lgco ~ time + (1 | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit1  3 342.77 357.50 -168.39   336.77                             
fit2  4 320.64 340.27 -156.32   312.64 24.135      1  8.983e-07 ***
</code></pre>

<p>However I have other data which suggests that the correlation between time and blood value might even more profound, for example quadratic. This would be Model 3.</p>

<p>I tried the following: first I took the square root of the blood value and after that I made the transformation using log.</p>

<pre><code>fit3&lt;-(lmer(lgsqrtco~time+(1|id),data=ASR))
</code></pre>

<p>My question is that can I compare models 2 and 3 in anyway now after the dependent variable has two different transformations in these models. In fit1 and fit2 the transformation is identical, only the independent is added. I assume that with different dependent variable transformation the use of anova() is not allowed: </p>

<pre><code>anova(fit2,fit3)
refitting model(s) with ML (instead of REML)
Data: ASR
Models:
fit2: lgco ~ time + (1 | id)
fit3: lgsqrtco ~ time + (1 | id)
     Df      AIC      BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit2  4   320.64   340.27 -156.32   312.64                             
fit3  4 -1065.66 -1046.03  536.83 -1073.66 1386.3      0  &lt; 2.2e-16 ***
</code></pre>
"
"0.132809685425692","0.146104310758895","153802","<p>I have a large data set with repeated measurements of same blood value (co) (2 to 7 measurements per patient). Each measurement is coupled with time which is the time interval between surgical operation and blood level measurement.</p>

<p>My aim is to show that this blood value correlates positively with time.</p>

<p>Blood level measurements are highly skewed to right and hence I am using a log-transformation and linear mixed effect regression model (lmer in lme4 package).</p>

<p>At first I assumed random intercepts among patients. I constructed a null model and model with time as independent.</p>

<pre><code>fit0&lt;-(lmer(lgco~(1|id),data=mebhr))
fit1&lt;-(lmer(lgco~time+(1|id),data=mebhr))
anova(fit0,fit1)
refitting model(s) with ML (instead of REML)
Data: mebhr
Models:
fit0: lgco ~ (1 | id)
fit1: lgco ~ time + (1 | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit0  3 200.44 213.16 -97.219   194.44                             
fit1  4 189.62 206.59 -90.811   181.62 12.815      1  0.0003438 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Ok, so I have an empty model and model with independent variable.
<img src=""http://i.stack.imgur.com/SFIFL.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/8RbgF.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/phYJ1.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/G4HNH.png"" alt=""enter image description here""></p>

<p>Adding covariate time in my model improves it significantly and also the graphical explanation is clear.</p>

<p>Fixed slopes, however, are not reasonable in my data, so I should use random slopes.</p>

<pre><code>fit0&lt;-(lmer(lgco~(time|id),data=mebhr))
fit1&lt;-(lmer(lgco~time+(time|id),data=mebhr))
anova(fit0,fit1)
refitting model(s) with ML (instead of REML)
Data: mebhr
Models:
fit0: lgco ~ (time| id)
fit1: lgco ~ time + (time | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)   
fit0  5 190.15 211.36 -90.076   180.15                            
fit1  6 182.06 207.51 -85.029   170.06 10.094      1   0.001487 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>At this point I dont understand my model equations. Graphical outputs for fit0 and fit1 are as follows:
<img src=""http://i.stack.imgur.com/E4w5D.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/XyeTJ.png"" alt=""enter image description here""></p>

<p>For the fit1 the model equation is:
<img src=""http://i.stack.imgur.com/Z6WLa.png"" alt=""enter image description here""></p>

<p>Why the lines in fit0 have non-zero slopes? What are they and what is the equation in that case? Also I dont understand how should I clarify the change in model fit? In the case of only random intercepts I can state that ""adding fixed factor <em>beta1</em> to model improves it significantly"". What would be the equal statement in the case of random slopes?</p>
"
"0.138232703275227","0.140372481268719","154488","<p>We measured temperatures of a pond repeatedly every day at each hour for a month at two different depths (i.e., top and bottom). We want to see if the temperatures at the top of the pond are significantly different from the bottom and if so at what hours. Initially I did a two-way anova in R:</p>

<pre><code>aov.result &lt;- aov(temp ~ depth * hour, data = pondtemp)
</code></pre>

<p>followed by post hoc tukey hsd test:</p>

<pre><code>tukey.result &lt;- TukeyHSD(aov.result)
</code></pre>

<p>The pairwise comparison of the depth*hour interaction term is what I need to see which hours have significantly different temperatures between top and bottom. This worked out well but someone pointed out that since it is a repeated measure it does not satisfy the assumption of independence. Therefore I tried using a linear mixed model. I took the depth as the fixed variable and figured the hour (since it has multiple observations in a month) should be the random variable:</p>

<pre><code>pondmdl &lt;- lmer(temp ~ depth + (1+variable|hour), data = pondtemp)
</code></pre>

<p>And used glht package for post-hoc:</p>

<pre><code>summary(glht(pondmdl, mcp(depth = ""Tukey"")))
</code></pre>

<p>However, this does not allow me to do the pair wise comparison I want to do (i.e., comparing Top Hour 1 to Bottom Hour 0 -23)</p>

<p>I found one way by introducing an interaction factor:</p>

<pre><code>pondtemp$depth.hour &lt;- interaction (pondtemp$depth, pondtemp$hour)
</code></pre>

<p>and then using this in my model and glht function:</p>

<pre><code>pondmdl &lt;- lmer(temp~depth.hour + (1+depth|hour), data = pondtemp)
summary(glht(pondmdl, mcp(depth.hour = ""Tukey"")))
</code></pre>

<p>However, I'm not sure if I can allow fixed and random variables to interact like that and still use the same random variable in the random variable error term.</p>

<p>Please advise what is my best option.</p>
"
"0.207349054912841","0.202760250721483","155524","<p>I am new to both mixed effect and Additive models so I'm sorry if the answer here is trivial.</p>

<p>I have data collected on several metabolic chemicals (M1,M2...), covariates (time,Race,Gender...) and disease state (D,D.binary).  I'm trying to generate a GAMM based on variables selected from a GEE variable selection.</p>

<p>Data: </p>

<ul>
<li>8 cases, 51 matched controls</li>
<li>approximately 10 time points from each subject</li>
<li>~630 observations</li>
<li>M1,M2...M3 are metabolites many of which are formed from common parts, Metabolite levels are correlated in that they are competing for the same component parts</li>
<li>Covariates stratify the subjects into subgroups</li>
</ul>

<p>Here is my model as it is now:</p>

<pre><code>&gt; b = gamm(D.binary ~ Time  + s(M1)  , 
      random = list(ParticipantID = ~ 1 + Time),  niterPQL=50,
      data = NEC_data, family=binomial(link=""logit"")) 

 Maximum number of PQL iterations:  50 
 iteration 1
 iteration 2
 ...
 iteration 49
 iteration 50
 Warning message:
 In gammPQL(y ~ X - 1, random = rand, data = strip.offset(mf), family = family,  :
  gamm not converged, try increasing niterPQL

&gt; plot(b$gam,pages=1)
</code></pre>

<p><img src=""http://i.stack.imgur.com/KhLNe.png"" alt=""enter image description here""></p>

<pre><code>&gt; summary(b$lme) # details of underlying lme fit
Linear mixed-effects model fit by maximum likelihood
 Data: data 
   AIC  BIC logLik
  -160 -124     88

Random effects:
 Formula: ~Xr - 1 | g
 Structure: pdIdnot
          Xr1   Xr2   Xr3   Xr4   Xr5   Xr6   Xr7   Xr8
StdDev: 0.812 0.812 0.812 0.812 0.812 0.812 0.812 0.812

 Formula: ~1 + Time | ParticipantID %in% g
 Structure: General positive-definite, Log-Cholesky parametrization
            StdDev  Corr  
(Intercept) 5.68324 (Intr)
Time        0.50739 -0.92 
Residual    0.00691       

Variance function:
 Structure: fixed weights
 Formula: ~invwt 
Fixed effects: list(fixed) 
                   Value Std.Error  DF t-value p-value
X(Intercept)       -2.81     0.729 573   -3.86  0.0001
XTime              -0.15     0.065 573   -2.30  0.0220
Xs(M1)Fx1 -1.60     0.066 573  -24.29  0.0000
 Correlation: 
                   X(Int) XTime  
XTime              -0.920       
Xs(M1)Fx1  0.004  0.000

Standardized Within-Group Residuals:
    Min      Q1     Med      Q3     Max 
-2.3472 -0.0692 -0.0117  0.0305 20.7271 

Number of Observations: 636
Number of Groups: 
                   g ParticipantID %in% g 
                   1                   61 
&gt; summary(b$gam) # gam style summary of fitted model

Family: binomial 
Link function: logit 

Formula:
NEC.binary ~ Time + s(M1)

Parametric coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  -2.8135     0.7289   -3.86  0.00013 ***
Time         -0.1495     0.0651   -2.30  0.02188 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Approximate significance of smooth terms:
               edf Ref.df     F p-value    
s(M1) 4.1    4.1 14913  &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

R-sq.(adj) =  0.0872   
  Scale est. = 4.7815e-05  n = 636
&gt; anova(b$gam) 

Family: binomial 
Link function: logit 

Formula:
NEC.binary ~ Time + s(M1)

Parametric Terms:
    df    F p-value
Time  1 5.28   0.022

Approximate significance of smooth terms:
               edf Ref.df     F p-value
s(M1) 4.1    4.1 14913  &lt;2e-16
&gt; gam.check(b$gam)
</code></pre>

<p><img src=""http://i.stack.imgur.com/IA4hi.png"" alt=""enter image description here""></p>

<p>I suspect I may have messed up something fairly basic since M1 is the most obvious discriminator of the disease state.  It is significant (as it should be) but the correlation is very low. Also, obviously, the model didn't converge (even when I increased iterations from 20->50). And finally the check plots look pretty outrageous</p>

<h2>Question</h2>

<p>Have I made a basic syntax error? Is there some malicious component in my model I'm over looking? Any help would be greatly appreciated. </p>

<h3>Further work</h3>

<p>I would like to add another metabolite (M2) to the model and 2 covariates (Birthweight and Race).  When I add M2 to the model I get an non-convergence error:</p>

<pre><code>&gt; b = gamm(D.binary ~ Time  + s(M1) + s(M2) , 
      random = list(ParticipantID = ~ 1 + Time),  niterPQL=20, correlation = corLin(),
      data = NEC_data, family=binomial(link=""logit""))

 Maximum number of PQL iterations:  20 
iteration 1
iteration 2
Error in lme.formula(fixed = fixed, random = random, data = data, correlation = correlation,  : 
  nlminb problem, convergence error code = 1
  message = false convergence (8)
</code></pre>

<p>Any advice about moving into the multidimensional space would also be appreciated.</p>

<h1>Addition</h1>

<p>I also tried this model with the discrete disease classification (control: 0,1  disease: 2,3) and poisson noise.</p>

<pre><code>&gt; b = gamm(NEC ~ DPP  + s(DSLNT_ug.mL)  , 
+      random = list(ParticipantID = ~ 1 + DPP),  niterPQL=20,
+      data = NEC_data, family=poisson) 

 Maximum number of PQL iterations:  20 
iteration 1
iteration 2
...
iteration 19
iteration 20
Error in solve.default(pdMatrix(a, factor = TRUE)) : 
  system is computationally singular: reciprocal condition number = 3.13906e-19
</code></pre>
"
"0.191374511943613","0.17743803952666","158051","<p>I am analyzing a multiply imputed dataset produced from the MICE package in R. To assess the overall significance of my linear model, I am using pool.compare() to compare my ""full"" model to an intercept only ""restricted"" model. However, the degrees of freedom (residual) returned by pool.compare() seem very highly inflated (I have set m = 50 imputations). I'm aware that 50 imputations is high, but it's needed for my dataset. I've given an example below of the same issue using the nhanes2 dataset from the MICE package. I have two questions:</p>

<p>1) Why are the degrees of freedom returned by pool.compare() so high?  </p>

<p>2) Is it appropriate to use the adjustment to the degrees of freedom suggested by Barnard and Rubin (1999) and described in section 2.3.6 of Stef van Burren's <em>Flexible Imputation of Missing Data</em> textbook?</p>

<p>The R code below shows the issue I'm asking about using the nhanes2 dataset. This dataset has 25 observations and the example fits a linear model with one categorical predictor (age) with three levels and one continuous predictor (chl).</p>

<pre><code># load package and data  
library(""mice"")  
data(nhanes2)  

# impute missing values, m = 50
imp &lt;- mice(nhanes2, m = 50, seed = 1, print = FALSE)

# produce the models to compare, a full model and
# an intercept only restricted model  
fit.imputed.full &lt;- with(imp, lm(bmi ~ age + chl))
fit.imputed.res &lt;- with(imp, lm(bmi ~ 1))  

# compare models using pool.compare()
pooled.comparison &lt;- pool.compare(fit.imputed.full, fit.imputed.res)

# given that the original dataset had 25 observations, and we have a 
# linear model with three predictors (age is a factor with three levels)
# I'd expect the degrees of freedom (residual) for the comparison to be at  
# most 24. The df for the numerator comes as expected:

pooled.comparison$df1
[1] 3

# the df for the denominator comes out a much larger than the 
# maximum of 24:

pooled.comparison$df2
[1] 1374.457

# by way of comparison, the same analysis conducted on a single
# hypothetically complete dataset gives the expected degrees of freedom

nhanes2CCA &lt;- complete(imp, 1)
attach(nhanes2CCA)
fit.CCA.full &lt;- lm(bmi ~ age + chl)
fit.CCA.res &lt;- lm(bmi ~ 1)
detach(nhanes2CCA)
anova(fit.CCA.full, fit.CCA.res)

Model 1: bmi ~ age + chl
Model 2: bmi ~ 1
  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  
1     21 293.60                              
2     24 477.23 -3   -183.62 4.3778 0.01525 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>In the end, it seems strange that an analysis conducted on a hypothetically complete dataset returns 24 degrees of freedom, while an analysis conducted on 50 multiply imputed datasets returns over 1000 degrees of freedom. Why is there this large difference in degrees of freedom?</p>

<p>My second question relates to the correction proposed by Barnard and Rubin(1999). Is it appropriate to use that correction here? Because this is a multi-parameter test, doing so requires, I guess, an estimate of lambda which is averaged across the parameters being estimated. </p>

<p>The figures I've used in this example are:<br>
v_old = 1374.457<br>
v_com = 25-1 = 24<br>
average lambda = 0.329<br>
v_obs = 14.91<br>
v (adjusted degrees of freedom) = 14.75  </p>

<p>Applying this correction in this instance returns a corrected degrees of freedom of 14.75, which is more than the df that would be returned by analyzing only complete cases (12) and less than the df that would be returned by analyzing a hypothetically complete dataset (24). Which seems reasonable. </p>

<p>Thank you all for your assistance. </p>

<p>Matt. </p>
"
"0.143877159211166","0.146104310758895","158319","<p>I have used a repeated-measures ANOVA in SPSS to analyse some of my data. It's the typical approach in my area, but I think it might be more appropriate to use a mixed effect model. However, I struggle with both building the model as well as interpreting it.</p>

<p><strong>Experimental design</strong></p>

<p>300+ participants from two different samples have rated on a continuous scale a stimulus at seven different manipulation levels. I want to test whether individual differences in the participants (recorded as ordinal or binary variables) interact with that rating score. In particular, I'm interested in whether the rating score changes as a function of stimulus level differently in people that, for example, feel mainly attracted to men or women.</p>

<p>Thus,  I have a within-subjects factor (stimulus level), a between-subjects factor (such as being attracted to men or women), and a random effect of participant nested in sample.</p>

<p>I've been using <code>lmer()</code> from the lme4 package and lmerTest and have come up with the following model</p>

<pre><code>model &lt;- lmer (rating.score ~ stim.level + factor + stim.level*factor +
                                                     (1|participant) + (1|sample), mydata)
</code></pre>

<p><strong>Analysis</strong></p>

<ol>
<li>Is lmer() the right package to work with?</li>
<li>Am I appropriately accounting for the random effects of participant and sample, or do I need something like <code>(1|sample/participant)</code>? I followed the <a href=""http://lme4.r-forge.r-project.org/lMMwR/lrgprt.pdf"" rel=""nofollow"">Pastes data example</a>, but am not sure that's the right thing to do in this context.</li>
<li>Based on previous literature, I expect the relationship of <code>rating.score</code> and <code>stim.level</code> to be quadratic - should/could I enter <code>stim.level</code> as squared term?</li>
</ol>

<p><strong>Interpretation</strong></p>

<p>In SPSS, I find a significant interaction of <code>stim.level</code> x <code>factor</code>. By visualizing the interaction and running post-hoc tests I can then interpret the nature of that interaction. In R, I get estimates of the interaction at each level of <code>stim.level</code>, some of which are significant, some of which are not. Can I still make the conclusion that <code>factor</code> affects the relationship of <code>rating.score</code> and <code>stim.level</code> (even though not necessarily to the same extent at each level)?</p>

<p><strong>EDIT:</strong> I just realized I had entered <code>stim.level</code> as a factor. I think it is appropriate to enter it as a linear variable - the different levels correspond to the same manipulation applied with increasing extent (the steps between each level are the same). This also resolves one of my earlier questions regarding an error message when trying to model random slopes which I have thus now removed.</p>
"
"0.132347737294141","0.122178562499719","160253","<p>I'm trying to compare two linear models, one calculated with full dataset and one calculated on a subset of the same data.<br>
The reason why I need/want to do that is, I suspect a part of the data to cause a shift in the slope.<br>
So here is my dummy dataset.</p>

<pre><code>set.seed(5)
x1 &lt;- runif(20, 0, 115)
x2 &lt;- runif(10, 85, 150)
x &lt;- c(x1, x2)

# dependent variable y has two parts with different slopes
y &lt;- c(6*x1 + rnorm(20, 0, 15), 1.5*x2 + 500 + rnorm(10, 0, 15))

# the grouping variables A and B correspond to the first part of y
# grouping variable C to the second part of y
groups &lt;- c(rep(c(""A"", ""B""), each = 10), rep(""C"", 10))

# joining everything together
df &lt;- data.frame(x, y, groups)
</code></pre>

<p>Plotting the dataframe <code>plot(df$x, df$y, col = df$groups)</code> shows that the C group in entirely responsible for a shift in the slope.<br>
I'm trying to see, if leaving out the C group changes the slope significantly.<br>
I tried this by subsetting the dataframe, calculating the linear models, and compare the two models.</p>

<pre><code># first linear model
lm1 &lt;- lm(y ~ x, data = df)
# subset without the C group
df2 &lt;- subset(df, groups != ""C"")
# second linear model
lm2 &lt;- lm(y ~ x, data = df2)

# comparison of the two models
anova(lm1, lm2)
</code></pre>

<p>However, running anova gives an error saying that the ""models were not all fitted to the same size of dataset""</p>

<p>Is there anyway to compare the two models?</p>
"
"0.0892288262810312","0.0906100470365937","160428","<p>I am building a really simple linear model. I want to test if the frass I got over 3 days from butterfly larvae depend upon the food they ate (diet), the butterfly family (the mother line) and subsequent survival (called ""survived"", obviously larvae which may latter die are likely to show e.g. problems to eat at larval stage). I'm also interested in the two way interactions: diet:family and survived:family. The interaction diet:survived could not be included because there is only one individual in one of the two diet that died.</p>

<p>Model:</p>

<pre><code>mod=lm(log(frass.weight)~diet*family+survived+family:survived,data=dat)
Anova(mod)   # all the variables are significant.
summary(mod)  #the R2adj is of 0.81
shapiro.test(resid(mod)) # p-value = 0.2389
</code></pre>

<p>I have not looked at the variance as I have a small sample size. Only 3 individuals of seven family have been recorded for their frass on both the 2 diets.</p>

<p>Problem:</p>

<p>All looks nice, except that when I plot the model <code>plot(mod)</code> I get the following warning:</p>

<pre><code>""Warning messages: 1: not plotting observations with leverage one:  
 3, 20, 30, 35 ""
</code></pre>

<p>Is it just a warning or I have a real issue that these points clearly influence the variance?</p>

<p>When I remove these points, the final model I get is simplier:</p>

<pre><code>mod1=lm(log(frass.weight)~diet*family+survived,data=datout)
</code></pre>

<p>The residuals are good and the plot works now fine.</p>

<p>Therefore, is the warning about the leverage something to not really consider and my first model should be kept or not? Are my points real outliers?</p>
"
"0.126713311335625","0.105279360951539","162553","<p>I would like do create a mixed linear model for an unbalanced dataset (different number of events per subject and a few missing values for some time points).  I am using <code>R version 3.2.1 (2015-06-18)</code>, <code>package: nlme_3.1-120</code>.  </p>

<p>Here comes simulated data:</p>

<pre><code>library(nlme)
set.seed(1)
subject    &lt;- factor(rep(c(1, 1, 2, 3, 4, 4, 4, 5, 6, 7, 7, 8, 9, 9, 10, 
                           11, 11, 11, 12, 13), 10))
event      &lt;- factor(rep(1:20, 10))
timepoint  &lt;- rep(1:10, each = 20)
measure    &lt;- rnorm(length(timepoint)) + timepoint*0.3
timepoint  &lt;- factor(timepoint)
measure[sample(1:length(measure), rpois(5,4))] &lt;- NA
data       &lt;- data.frame(subject=subject, event=event, timepoint=timepoint, 
                         measure=measure)
str(data)
</code></pre>

<p>The model should predict the variable â€œmeasureâ€ over different time points as fixed effect and for subjects and events as random effects.</p>

<pre><code>base      &lt;- lme(measure ~ 1,         data=data, random= ~ 1|subject, 
                 na.action=na.exclude, method=""ML"")
intercept &lt;- lme(measure ~ timepoint, data=data, random= ~ 1|subject, 
                 na.action=na.exclude, method=""ML"")
nested    &lt;- lme(measure ~ timepoint, data=data, random= ~ 1|subject/event, 
                 na.action=na.exclude, method=""ML"")
anova(base, intercept, nested)
</code></pre>

<p>I would like to fit random intercept and slope, because intercept and slope can vary among subjects and events. However when I add the random slope effect, the model does not converge. It does not through any error message, but it runs to infinity. What can I do create a model with random slope that converges?</p>

<p><strong>cave</strong> model runs endless</p>

<pre><code>slope &lt;- lme(measure ~ timepoint, data=data, random= ~ timepoint|subject, 
             na.action=na.exclude, method=""ML"")
</code></pre>

<p>I tried also this</p>

<p><strong>cave</strong> model runs endless</p>

<pre><code>slope2 &lt;- lme(measure ~ timepoint, data=data, random= ~ timepoint|subject, 
              na.action=na.exclude, method=""ML"", control=list(opt=""optim""))
</code></pre>

<p><strong>cave</strong> some models may run endless</p>

<pre><code>slope3      &lt;- lme(measure ~ timepoint, data=data, random= ~ timepoint|subject/event, 
                   na.action=na.exclude, method=""ML"", control = list(opt=""optim""))
covariance  &lt;- lme(measure ~ timepoint, data=data, random= ~ timepoint|subject, 
                   correlation=corAR1(),na.action = na.exclude, method=""ML"")
covariance2 &lt;- lme(measure ~ timepoint, data=data, random= ~ timepoint|subject, 
                   correlation=corAR1(0), na.action=na.exclude, method=""ML"", 
                   control=list(opt=""optim""))
covariance3 &lt;- lme(measure ~ timepoint, data=data, random= ~ timepoint|subject, 
                   correlation=corAR1(0), na.action=na.exclude, method=""ML"", 
                   control=list(maxlter=1000))
</code></pre>
"
"NaN","NaN","162562","<ol>
<li><p>What does the P value in the following example mean?</p>

<pre><code>library(rms)

data(pbc)
d &lt;- pbc
rm(pbc)
d$status &lt;- ifelse(d$status != 0, 1, 0)

ddist &lt;- datadist(d)
options(datadist='ddist')

fit &lt;- lrm(status ~ rcs(age, 4), data=d)
(an &lt;- anova(fit))
plot(Predict(fit), anova=an, pval=TRUE)
</code></pre>

<p><a href=""http://i.stack.imgur.com/WGfjA.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/WGfjA.png"" alt=""P value in the top center of the figure""></a></p></li>
<li><p>How can I interpret the following R output in the upper example (<code>an &lt;- anova(fit)</code>) in the upper example)?</p>

<pre><code>                Wald Statistics          Response: status 

 Factor     Chi-Square d.f. P     
 age        9.18       3    0.0269
 Nonlinear  2.52       2    0.2832
 TOTAL      9.18       3    0.0269
</code></pre></li>
</ol>
"
"0.285026727779364","0.256042023381101","162804","<p>When searching for correlations between between a dependent variable and a factor or a combination of factors in a repeated measure design with lme() I noticed that I can encounter two types of results, and I am wondering which is the best way to report each of them in a journal publication. It is not clear to me when I should report the values of the beta coefficient together with the t-test value and p-value, or the beta coefficient with F value and p-value.</p>

<p>Letâ€™s have as a reference the following two models:</p>

<p>MODEL TYPE 1: fixed effects only </p>

<pre><code>lme_Weigth &lt;- lme(Sound_Feature ~ Weight, data = My_Data, random = ~1 | Subject)
summary(lme_Weigth)

lme_Height &lt;- lme(Sound_Feature ~ Height, data = My_Data, random = ~1 | Subject)
summary(lme_Height)
</code></pre>

<p>MODEL TYPE 2: Fixed and interaction effects together</p>

<pre><code>lme_Interaction &lt;- lme(Sound_Feature ~ Weight*Height, data = My_Data, random = ~1 | Subject)

summary(lme_Interaction)  
anova.lme(lme_Interaction, type = ""marginal"").
</code></pre>

<p>RESULTS CASE 1: Applying model type 2 I do not get any significant p-value so there is no interaction effect. Therefore I check
the simplified model type 1, and I get for both Height and Weight significant p-values.</p>

<p>RESULTS CASE 2: Applying model type 2 I get a significant p-value so there is an interaction effect. Therefore I do not check
the simplified model type 1 for the two factors separately. Moreover, in the results of model type 2 I can also see that the fixed effects of both factors are significant.</p>

<p>I am not sure if in presence of an interaction it is correct to report the significant interactions of the separate factors, since I read somewhere that it does not make too much sense. Am I wrong?</p>

<p>My attempt in reporting the results for the two cases is the following. Can you please tell me it I am right?</p>

<p>â€œWe performed a linear mixed effects analysis of the relationship between Sound_Feature and Height and Weight. As fixed effects, we entered Height and Weight (without interaction term) into a first model, and we included the interaction effect into a second model. As random effects, we had intercepts for subjects.â€</p>

<p>RESULTS CASE 1: â€œResults showed that Sound_Feature was linearly related to Height (beta = value, t(df)= value, p &lt; 0.05) and Weight (beta = value, t(df)= value, p &lt; 0.05), but no to their interaction effect.â€</p>

<p>RESULTS CASE 2:  â€œResults showed that Sound_Feature was linearly related to Height (beta = value, F(df)= value, p &lt; 0.05) and Weight (beta = value, F(df)= value, p &lt; 0.05), and to their interaction effect (beta = value, F(df)= value, p &lt; 0.05).â€</p>

<p>Basically I used for reporting the beta value in the 2 cases I use the output of summary(). In the case 1, I report the value of the t-test, still taken from summary. But for case 2 I do not report the t-test, I report the F value as result of anova.lme(lme_Interaction, type = ""marginal"").</p>

<p>Is this the correct way of proceeding in the results reporting?</p>

<p>I give an example of the outputs I get using the two models for the three cases:</p>

<p>RESULTS CASE 1:</p>

<pre><code>&gt; ############### Sound_Level_Peak vs Weight*Height ###############
&gt; 
&gt;
&gt; 
&gt; library(nlme)
&gt; lme_Sound_Level_Peak &lt;- lme(Sound_Level_Peak ~ Weight*Height, data = My_Data1, random = ~1 | Subject)
&gt; 
&gt; summary(lme_Sound_Level_Peak)
Linear mixed-effects model fit by REML
 Data: My_Data1 
       AIC      BIC    logLik
  716.2123 732.4152 -352.1061

Random effects:
 Formula: ~1 | Subject
        (Intercept) Residual
StdDev:    5.470027 4.246533

Fixed effects: Sound_Level_Peak ~ Weight * Height 
                  Value Std.Error DF    t-value p-value
(Intercept)   -7.185833  97.56924 95 -0.0736485  0.9414
Weight         0.993543   1.63151 15  0.6089715  0.5517
Height        -0.076300   0.55955 15 -0.1363592  0.8934
Weight:Height -0.005403   0.00898 15 -0.6017421  0.5563
 Correlation: 
              (Intr) Weight Height
Weight        -0.927              
Height        -0.994  0.886       
Weight:Height  0.951 -0.996 -0.919

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-2.95289464 -0.51041805 -0.06414148  0.48562230  2.95415889 

Number of Observations: 114
Number of Groups: 19 


&gt; anova.lme(lme_Sound_Level_Peak,type = ""marginal"")
              numDF denDF   F-value p-value
(Intercept)       1    95 0.0054241  0.9414
Weight            1    15 0.3708463  0.5517
Height            1    15 0.0185938  0.8934
Weight:Height     1    15 0.3620936  0.5563
&gt; 
&gt; 





&gt; ############### Sound_Level_Peak vs Weight ###############
&gt; 
&gt; library(nlme)
&gt; summary(lme(Sound_Level_Peak ~ Weight, data = My_Data1, random = ~1 | Subject))
Linear mixed-effects model fit by REML
 Data: My_Data1 
       AIC      BIC    logLik
  706.8101 717.6841 -349.4051

Random effects:
 Formula: ~1 | Subject
        (Intercept) Residual
StdDev:    5.717712 4.246533

Fixed effects: Sound_Level_Peak ~ Weight 
                Value Std.Error DF    t-value p-value
(Intercept) -3.393843  6.291036 95 -0.5394728  0.5908
Weight      -0.196214  0.087647 17 -2.2386822  0.0388
 Correlation: 
       (Intr)
Weight -0.976

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-2.90606493 -0.51419643 -0.05659565  0.56770327  3.00098859 

Number of Observations: 114
Number of Groups: 19 
&gt; 
&gt; 
&gt; 
&gt; 
&gt; 
&gt; 
&gt; ############### Sound_Level_Peak vs Height ###############
&gt; 
&gt; library(nlme)
&gt; summary(lme(Sound_Level_Peak ~ Height, data = My_Data1, random = ~1 | Subject))
Linear mixed-effects model fit by REML
 Data: My_Data1 
       AIC      BIC   logLik
  702.9241 713.7981 -347.462

Random effects:
 Formula: ~1 | Subject
        (Intercept) Residual
StdDev:    5.174077 4.246533

Fixed effects: Sound_Level_Peak ~ Height 
               Value Std.Error DF   t-value p-value
(Intercept) 46.36896 20.764187 95  2.233122  0.0279
Height      -0.36643  0.119588 17 -3.064113  0.0070
 Correlation: 
       (Intr)
Height -0.998

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-2.93697776 -0.50963502 -0.06774953  0.50428597  2.97007576 

Number of Observations: 114
Number of Groups: 19 
&gt; 
&gt; 
</code></pre>

<p>So, I will report the results in this way: â€œResults showed that Sound_Level_Peak was linearly related to Height (beta = -0.36643, t(17)= -3.064113, p = 0.007) and Weight (beta = -0.196214, t(17)= -2.2386822, p &lt; 0.0388), but no to their interaction effect.â€</p>

<p>RESULTS CASE 2:</p>

<pre><code>&gt; ############### Centroid vs Weight*Height ###############
&gt; 
&gt; 
&gt; 
&gt; library(nlme)
&gt; lme_Centroid &lt;- lme(Centroid ~ Weight*Height, data = My_Data2, random = ~1 | Subject)
&gt; 
&gt; summary(lme_Centroid)
Linear mixed-effects model fit by REML
 Data: My_Data2 
       AIC      BIC    logLik
  1904.563 1920.766 -946.2817

Random effects:
 Formula: ~1 | Subject
        (Intercept) Residual
StdDev:    1180.301 945.3498

Fixed effects: Centroid ~ Weight * Height 
                  Value Std.Error DF   t-value p-value
(Intercept)   -45019.39 21114.912 95 -2.132113  0.0356
Weight           710.53   353.074 15  2.012414  0.0625
Height           330.61   121.092 15  2.730246  0.0155
Weight:Height     -4.34     1.943 15 -2.233779  0.0411
 Correlation: 
              (Intr) Weight Height
Weight        -0.927              
Height        -0.994  0.886       
Weight:Height  0.951 -0.996 -0.919

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-2.16255520 -0.60084449 -0.02651629  0.54377042  1.92638924 

Number of Observations: 114
Number of Groups: 19 


&gt; anova.lme(lme_Centroid,type = ""marginal"")
              numDF denDF  F-value p-value
(Intercept)       1    95 4.545908  0.0356
Weight            1    15 4.049810  0.0625
Height            1    15 7.454243  0.0155
Weight:Height     1    15 4.989769  0.0411
&gt; 
&gt; 
&gt; 
</code></pre>

<p>So, I will report the results in this way:  â€œResults showed that Centroid was linearly related to the interaction effect of Weight and Height (beta = -4.34, F(1,15)= 4.989769, p = 0.0411), and to Height (beta = 330.61, F(1,15)= 7.454243, p = 0.0155). </p>
"
"0.21534009585052","0.211619477845242","163161","<p>I try to figure out how to describe my continuous variable. Unfortunately, I did not understand all of the statistics. I would really appreciate I you guys would help me out here. To better illustrate my problem, I wrote the following example.</p>

<pre><code>library(rms)
library(survival)

data(pbc)
d &lt;- pbc
rm(pbc, pbcseq)
d$status &lt;- ifelse(d$status != 0, 1, 0)

dd = datadist(d)
options(datadist='dd')

# linear model
f1 &lt;- cph(Surv(time, status) ~  albumin, data=d)
p1 &lt;- Predict(f1, fun=exp)
(a1 &lt;- anova(f1))
Function(f1)
plot(p1, anova=a1, pval=TRUE, ylab=""Hazard Ratio"")

# rcs model
f2 &lt;- cph(Surv(time, status) ~  rcs(albumin, 4), data=d)
p2 &lt;- Predict(f2, fun=exp)
(a2 &lt;- anova(f2))
Function(f2)
plot(p2, anova=a2, pval=TRUE, ylab=""Hazard Ratio"")

# minimal CI width
p1$diff &lt;- p1$upper-p1$lower
    min(p1$diff) # = 0.002321521
p1[which(p1$diff==min(p1$diff)),]$albumin # = 3.494002
    describe(d$albumin) # mean = 3.497

p2$diff &lt;- p2$upper-p2$lower
    min(p2$diff) # = 0.2039817
p2[which(p2$diff==min(p2$diff)),]$albumin # = 3.502447
    describe(d$albumin) # mean = 3.497

# both models in a single figure
p &lt;- rbind(linear.model=p1, rcs.model=p2)
library(ggplot2)
df &lt;- data.frame(albumin=p$albumin, yhat=p$yhat, lower=p$lower, upper=p$upper, predictor=p$.set.)
(g &lt;- ggplot(data=df, aes(x=albumin, y=yhat, group=predictor, color=predictor)) + geom_line(size=1))
(g &lt;- g + geom_ribbon(data=df, aes(ymin=lower, ymax=upper), alpha=0.2, linetype=0))
(g &lt;- g + theme_bw())
(g &lt;- g + xlab(""Albumin""))
(g &lt;- g + ylab(""Hazard Ratio""))
(g &lt;- g + theme(axis.line = element_line(color='black', size=1)))
(g &lt;- g + theme(axis.ticks = element_line(color='black', size=1)))
(g &lt;- g + theme( plot.background = element_blank() ))
(g &lt;- g + theme( panel.grid.minor = element_blank() ))
(g &lt;- g + theme( panel.border = element_blank() ))
</code></pre>

<ol>
<li>Why shows the plot of the linear model (p1) not a straight line? </li>
<li>How can I plot the models f1 and f2 in the same figure? </li>
<li>How can I compare the models f1 and f2 to investigate which models fits the data better? ... like anova() for coxph in the survival package</li>
<li>Why is the minimal CI width near the mean of albumin more pronounce in the
linear (f1) model? </li>
<li>What does the P value in the plots mean? How do I have to interpret the output of anova(...)</li>
</ol>

<p><a href=""http://i.stack.imgur.com/kbGuv.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/kbGuv.png"" alt=""pkot of f1""></a></p>

<p><a href=""http://i.stack.imgur.com/1UVJd.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1UVJd.png"" alt=""plot of f2""></a></p>

<p><a href=""http://i.stack.imgur.com/WG3ui.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/WG3ui.png"" alt=""combined plot""></a>  </p>

<p><strong>Update #1</strong>
Following the answer from Harrell, I updated the code above showing how to combine spline plots of two predictors in a single figure. One last question: How can I compare the two rms models like <code>anova(m1, m2)</code> of the survival package as shown below?</p>

<pre><code>&gt; m1 &lt;- coxph(Surv(time, status) ~ albumin, data=d)
&gt; m2 &lt;- coxph(Surv(time, status) ~ pspline(albumin), data=d)
&gt; anova(m1, m2) # compare models
Analysis of Deviance Table
 Cox model: response is  Surv(time, status)
 Model 1: ~ albumin
 Model 2: ~ pspline(albumin)
   loglik  Chisq Df P(&gt;|Chi|)
1 -975.61                    
2 -973.26 4.6983 11    0.9449
&gt; summary(m1)
Call:
coxph(formula = Surv(time, status) ~ albumin, data = d)

  n= 418, number of events= 186 

           coef exp(coef) se(coef)      z Pr(&gt;|z|)    
albumin -1.4695    0.2300   0.1714 -8.574   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

        exp(coef) exp(-coef) lower .95 upper .95
albumin      0.23      4.347    0.1644    0.3219

Concordance= 0.688  (se = 0.023 )
Rsquare= 0.147   (max possible= 0.992 )
Likelihood ratio test= 66.6  on 1 df,   p=3.331e-16
Wald test            = 73.51  on 1 df,   p=0
Score (logrank) test = 72.38  on 1 df,   p=0
</code></pre>

<p><strong>UPDATE #2</strong>
I think I just answered my ""one last question"" by myself (see below). I hope this does not show correct accidentally. I would think that I can compare models from <code>cph</code> and <code>coxph</code> that way, can't I? Is the way of calculating the degrees of freedom <code>df</code> correct?</p>

<pre><code>&gt; # using coxph from survival
&gt; m1 &lt;- coxph(Surv(time, status) ~  albumin, data=d)
&gt; m2 &lt;- coxph(Surv(time, status) ~  albumin + age, data=d)
&gt; # loglik = a vector of length 2 containing the log-likelihood with the initial values and with the final values of the coefficients.
&gt; m1$loglik[2]
    [1] -975.6126
    &gt; m2$loglik[2]
[1] -973.2272
&gt; (df &lt;- abs(length(m1$coefficients) - length(m2$coefficients)))
[1] 1
&gt; (LR &lt;- 2 * (m2$loglik[2] - m1$loglik[2]))
[1] 4.770787
&gt; pchisq(LR, df, lower=FALSE)
[1] 0.02894659
&gt; anova(m2, m1)
Analysis of Deviance Table
 Cox model: response is  Surv(time, status)
 Model 1: ~ albumin + age
 Model 2: ~ albumin
   loglik  Chisq Df P(&gt;|Chi|)  
1 -973.23                      
2 -975.61 4.7708  1   0.02895 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; m1 &lt;- cph(Surv(time, status) ~  albumin, data=d)
&gt; m2 &lt;- cph(Surv(time, status) ~  albumin + age, data=d)
&gt; # loglik = a vector of length 2 containing the log-likelihood with the initial values and with the final values of the coefficients.
&gt; m1$loglik[2]
    [1] -975.6126
    &gt; m2$loglik[2]
[1] -973.2272
&gt; (df &lt;- abs(length(m1$coefficients) - length(m2$coefficients)))
[1] 1
&gt; (LR &lt;- 2 * (m2$loglik[2] - m1$loglik[2]))
[1] 4.770787
&gt; pchisq(LR, df, lower=FALSE)
[1] 0.02894659
</code></pre>

<p><strong>UPDATE #3</strong>
I changed the example following the kind answer from DWin as follows. This way the degrees of freedom should be calculated properly:</p>

<pre><code>library(Hmisc)
library(rms)
library(ggplot2)
library(gridExtra)

data(pbc)
d &lt;- pbc
rm(pbc, pbcseq)
d$status &lt;- ifelse(d$status != 0, 1, 0)

### log likelihood test using a coxph model
m1 &lt;- coxph(Surv(time, status) ~  albumin, data=d)
m2 &lt;- coxph(Surv(time, status) ~  albumin + age, data=d)
# loglik = a vector of length 2 containing the log-likelihood with the initial values and with the final values of the coefficients.
m1$loglik[2]
    m2$loglik[2]
(df &lt;- abs(sum(anova(m1)$Df, na.rm=TRUE) - sum(anova(m2)$Df, na.rm=TRUE)))
(LR &lt;- 2 * (m2$loglik[2] - m1$loglik[2])) # the most parsimonious models have to be first
pchisq(LR, df, lower=FALSE)
anova(m2, m1)

### log likelihood test using a cph model
dd = datadist(d)
options(datadist='dd')
m3 &lt;- cph(Surv(time, status) ~  albumin, data=d)
m4 &lt;- cph(Surv(time, status) ~  albumin + age, data=d)
# loglik = a vector of length 2 containing the log-likelihood with the initial values and with the final values of the coefficients.
m3$loglik[2]
    m4$loglik[2]
(df &lt;- abs(print(anova(m3)[, ""d.f.""])[['TOTAL']] - print(anova(m4)[, ""d.f.""])[['TOTAL']]))
(LR &lt;- 2 * (m4$loglik[2] - m3$loglik[2])) # the most parsimonious models have to be first
pchisq(LR, df, lower=FALSE)
</code></pre>
"
"0.0904945466110142","0.0765794667741282","163410","<p>How can I determine whether one coding of a linear predictor leads to a better fit of the corresponding regression model than the other?
In the following example, the restricted cubic spline coding of albumin leads to a higher chi-square value of the resulting model compared to the linear coding. However, it has also more degrees of freedom. As I understand it, I cannot use the log likelihood test in this case, since both models are not nested.</p>

<p>What should I do?</p>

<pre><code>&gt; library(rms)
&gt; 
&gt; data(pbc)
&gt; d &lt;- pbc
&gt; rm(pbc, pbcseq)
&gt; d$status &lt;- ifelse(d$status != 0, 1, 0)
&gt; 
&gt; dd = datadist(d)
&gt; options(datadist='dd')
&gt; 
&gt; # linear model
&gt; m1 &lt;- cph(Surv(time, status) ~  albumin, data=d)
&gt; anova(m1)
                Wald Statistics          Response: Surv(time, status) 

 Factor     Chi-Square d.f. P     
 albumin    73.51      1    &lt;.0001
 TOTAL      73.51      1    &lt;.0001
&gt; 
&gt; # rcs model
&gt; m2 &lt;- cph(Surv(time, status) ~  rcs(albumin, 4), data=d)
&gt; anova(m2)
                Wald Statistics          Response: Surv(time, status) 

 Factor     Chi-Square d.f. P     
 albumin    82.80      3    &lt;.0001
  Nonlinear  4.73      2    0.094 
 TOTAL      82.80      3    &lt;.0001
</code></pre>

<h2>UPDATE #1</h2>

<p>I thought plotting both models would be a good way to decide whether a linear coding or a restricted cubic spline coding would be best. In this case (see below), I would think that the more complex coding is not better. However, the core of my question aimed to reinforce the eyeballing by a statistical test. But as I understand you correct, this is prone to over-fitting?</p>

<p><a href=""http://i.stack.imgur.com/UD5wK.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/UD5wK.png"" alt=""enter image description here""></a></p>
"
"0.0977452818676612","0.0661722222647287","163922","<p>I am trying to find any evidence of warming in monthly times series data of water temperature over a 21-year period that is serially correlated. Essentially I am looking to determine a global trend, like what can be done with OLS regression with data that is from independent observations. I am at a crossroads in trying to determine whether a seasonal ARIMA model or a linear mixed model with a trend component as detailed by Crawley on page 799 of ""The R Book"" (2nd ed.) is the most appropriate method to use. I therefore explored both techniques, but got very contradicting answers!</p>

<p>ARIMA modelling gave me a seasonal ARIMA of form (2,0,2)(0,0,1)[12], indicating that no differencing is required and therefore that the series is stationary with NO trend.</p>

<p>However, the linear mixed affects modelling, comparing two models with and without a trend component using ANOVA and maximum likelihood indicated a highly significant trend (R notation):</p>

<pre><code>model2: ave ~ sin(time * 2 * pi) + cos(time * 2 * pi) + (1 | factor(yr))

model1: ave ~ index + sin(time * 2 * pi) + cos(time * 2 * pi) + (1 | factor(yr))

ANOVA(model2,model1)

      Df  AIC     BIC     logLik deviance Chisq Chi Df Pr(&gt;Chisq)   
model2 5 346.82   364.49    -168.41   336.82                           
model1 6 338.54   359.74    -163.27   326.54 10.28      1   0.001345 **
</code></pre>

<p>How can this be? What am I missing? Is it about assuming whether the trend is a parametric form (appropriate for linear mixed model) or whatever weird shape (appropriate for ARIMA)? If so how do I go about choosing which approach to adopt?</p>

<p>Thank you kindly for any advice.</p>
"
"0.05643326479831","0.0573068255061253","164314","<p>I have a data set with performance and training data that looks something like (this is not the exact data, but gives a general idea):</p>

<pre><code>&gt;dat
Performance Training
1           1
0           1
1           2
0           2
1           3
1           3
</code></pre>

<p>I want to find if there is are any significant differences between performance means for the respective levels of performance in R.  I have tried linear regression and anova, such as: <code>summary(lm(performance~training))</code> or <code>summary(aov(performance~training))</code> both of which yield non-significant results.  However, when I do a T-test to compare some of the means manually it is telling be significant differences exist.  Any thoughts on how to code what I am looking for or what might be going on here?</p>
"
"0.126188616281267","0.128141957406415","164705","<p>I'm working on analyzing a time series of physical variables in many lakes in Florida for an associate, and I've run into an issue. I'm attempting to run a regression for each time series of physical variables in each lake. I can get regression results in R easily, but they don't match up with my coworker's JMP results. Anyway, here's a sample from the data:</p>

<pre><code>Year = seq(1987,2015)
TP = c(14, 12, 14, 14, 17, 16, 15, 12, 18, 14, 15, 18, 18, 21, 21, 17, 17, 20, 19, 17, 18, 18, 26, 20, 18, 21, 21, 20, 18)
summary(lm(TP~Year))
</code></pre>

<p>gives </p>

<pre><code>Call:
lm(formula = TP ~ Year)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.7310 -1.3724 -0.4305  0.9685  6.3675 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -502.90542   98.13981  -5.124 2.18e-05 ***
Year           0.26010    0.04904   5.303 1.35e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 2.21 on 27 degrees of freedom
Multiple R-squared:  0.5102,    Adjusted R-squared:  0.4921 
F-statistic: 28.12 on 1 and 27 DF,  p-value: 1.35e-05
</code></pre>

<p>His JMP analysis spits out the following:</p>

<pre><code>Parameter Estimates
Term        Estimate    Std Error   t Ratio Prob&gt;|t|
Intercept   -500.4634   96.74332    -5.17   &lt;.0001*
Year        0.2588707   0.048347    5.35    &lt;.0001*
</code></pre>

<p>For all lakes and all parameters of interest, the SS, slope estimates, etc. are all slightly off. I have looked into different types of Sum of Squares for ANOVA, but changing to different types (e.g. Type III using Anova()) still doesn't get the results to match up. What am I missing? Any assistance would be appreciated.</p>

<p>Edit: Thanks for y'all's help. Sorry for the belated response, I had to meet up with my colleague. To address the questions:</p>

<ul>
<li>I have hardcoded the data in my question, but it's merely a subset of a much larger dataset from Excel. We are using the same data and the remainder of my code is working properly. <a href=""https://www.dropbox.com/s/k21v38sfdbm0ola/LWFormatted.csv?dl=0"" rel=""nofollow"">Here's what the actual data look like.</a></li>
<li>I know OLS isn't great, but it's being used for some really basic trend descriptions for informing stakeholders. I may pursue a better option in the future.</li>
<li>The JMP model is setup using Y by X with the Bivariate option, then applying a linear regression. Below is a screenshot.</li>
</ul>

<p>Thanks again for your help!</p>

<p><a href=""http://i.stack.imgur.com/ndtBr.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ndtBr.jpg"" alt=""JMP Input""></a></p>
"
"0.126188616281267","0.102513565925132","164948","<p>In a model aimed to assess the influence of land use measures on ecosystem functioning, I have one log-transformed dependent variable (the ecosystem function), and 5 fixed-effects independent variables (1 continuous, 3 binary, 1 categorical).</p>

<p>I've fitted a mixed effects model (with two random effects, spatial and temporal, that remain the same) and now I am trying to find the best model. Initially I wanted to do this by manual stepwise elimination of insignificant variables using the <code>anova(model)</code> command and then comparing likelihood-ratio tests between models. The first model looks like this:</p>

<pre><code>&gt; model0 &lt;- lme(dep_var ~ bin_1 + bin_2 + bin_3 + log(cont_1) + cat_1, random=~1|season/site, method=""ML"", data=abvpp2)
&gt; anova(model0)
                numDF denDF  F-value p-value
(Intercept)         1   319 9384.037  &lt;.0001
bin_1               1    72    2.972  0.0890
bin_2               1    72    0.007  0.9338
bin_3               1    72   12.423  0.0007
log(cont_1 + 1)     1    72    1.655  0.2023
cat_1               2    72   29.382  &lt;.0001
</code></pre>

<p>After dropping the least significant variable <em>bin_2</em>:</p>

<pre><code>&gt; model1 &lt;- lme(dep_var ~ bin_1 + bin_3 + log(cont_1) + cat_1, random=~1|season/site, method=""ML"", data=abvpp2)   
&gt; anova(model1)
                numDF denDF  F-value p-value
(Intercept)         1   319 9257.012  &lt;.0001
bin_1               1    73    2.931  0.0911
bin_3               1    73   12.260  0.0008
log(cont_1 + 1)     1    73    1.616  0.2077
cat_1               2    73   28.361  &lt;.0001
&gt; anova(model0,model1)
       Model df      AIC      BIC    logLik   Test L.Ratio p-value
model0     1 10 517.3610 557.2506 -248.6805                       
model1     2  9 516.6558 552.5564 -249.3279 1 vs 2 1.29476  0.2552
</code></pre>

<p>Pretty straightforward up to here. But after dropping <em>cont_1</em> next, I get this:</p>

<pre><code>&gt; model2 &lt;- lme(dep_var ~ bin_1 + bin_3 + cat_1, random=~1|season/site, method=""ML"", data=abvpp2)
&gt; anova(model1,model2)
       Model df      AIC      BIC    logLik   Test L.Ratio p-value
model1     1  9 516.6558 552.5564 -249.3279                       
model2     2  8 522.7970 554.7087 -253.3985 1 vs 2 8.14122  0.0043
</code></pre>

<p><em>cont_1</em> does seem to have some explanatory power, in spite of it's insignificance in the <code>anova</code>. Apparently, this is due to multicollinearity between the variables, since the order of fitting them in the model changes significances in the anova output drastically (sometimes insignificant variables even become significant when fittet AFTER another variable with usually more explanatory power).
I am now a little unsure about how to select my model. Even though some variables are correlated (<em>r=-0.33</em> and <em>r=0.62</em>), they explain different measures and dropping one of them beforehand would be hard to justify. I would be very grateful for some ideas on model selection while dealing with collinearity in <code>LME</code> models and avoiding <code>stepwise</code> commands.</p>
"
"0.105576971046183","0.107211253483779","165110","<p>I'm struggling with the interpretation of a regression model where a categorial variable (5 levels) is dummy coded. Here is the result of my calculation in R:</p>

<pre><code>Call:
lm(formula = DV ~ Age + Gender + factor(Categorial) + 
Continuous 1 + Continuous 2 + Continuous 3, 
data = dat)

Residuals:
 Min       1Q   Median       3Q      Max 
-1.30058 -0.25326  0.00349  0.28123  1.49877 

Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)           -0.42367    0.30694  -1.380  0.16842   
Age                   -0.05949    0.02026  -2.936  0.00356 **
Gender                -0.01800    0.04828  -0.373  0.70952   
factor(Categorial)2   -0.30625    0.12645  -2.422  0.01596 * 
factor(Categorial)3   -0.03441    0.07752  -0.444  0.65736   
factor(Categorial)4   -0.12603    0.09914  -1.271  0.20453   
factor(Categorial)5   -0.08417    0.13269  -0.634  0.52630    
Continuous 1           0.12080    0.04346   2.779  0.00575 **
Continuous 2          -0.06592    0.04383  -1.504  0.13354   
Continuous 3          -0.06230    0.03475  -1.793  0.07392 . 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.4259 on 336 degrees of freedom
  (6 observations deleted due to missingness)
Multiple R-squared:  0.1315,    Adjusted R-squared:  0.1057 
F-statistic: 5.089 on 10 and 336 DF,  p-value: 6.353e-07
</code></pre>

<p>Ok. Age, Factor 2 of the categorial variable and the first continuous variable are significant predictors of the dependent variable. so far so good. </p>

<p>What I'm not understanding is:</p>

<ol>
<li><p>The reference category of the dummy coded categorial variable is the intercept and the first category of the categorial variable. right? How do I interpret this? </p></li>
<li><p>When doing an anova with the categorial variable as a independent variable, this factor is a significant predictor. With the results of the linear model, one could conclude that this is only due to category 2, right?</p></li>
<li><p>Can I test contrasts with this linear regression model (e.g. Category1 vs. Category2)?</p></li>
<li><p>Should I include interactions?</p></li>
</ol>

<p>I'd be glad for any help :-)</p>
"
"0.0399043442233811","0.0405220449236554","171403","<p>I want to compare these two linear models by anova:</p>

<pre><code>a&lt;-c(-10:-1 , 1:10)
b&lt;-sample(1000, 20, replace = FALSE)
ID&lt;-rep(c(""no"",""bo""), each=10)
dataf&lt;-data.frame(a, b, ID)

x&lt;-lm(dataf[1:10,]$a~dataf[1:10,]$b)
y&lt;-lm(dataf[11:20,]$a~dataf[11:20,]$b)
anova(x,y)
</code></pre>

<p>Unfortunately when I run the test I got this warning where it says:</p>

<pre><code>Warning message:In anova.lmlist(object, ...) :
  models with response â€˜""dataf[11:20, ]$a""â€™ removed because response differs from model 1
</code></pre>

<p>May be the I should use another test.
Can somebody help me to better understand and solve this? Thank you</p>
"
"0.11356975465314","0.115327761665773","173026","<p>Suppose I have a linear regression model Y ~ b1*x1 + b2*x2 with the sample data:</p>

<pre><code>        x1      x2        y
0  -0.7251 -0.6033  10.2508
1  -1.3148  3.7067  10.3054
2  -1.2818 -5.8136   7.2010
3   0.6154  3.6336  11.2807
4   1.4887 -2.8337  10.6299
5  -2.0224 -0.3931   7.4502
6   0.5419 -1.7807  10.7339
7  -1.0745 -5.0296   5.8734
8   3.2116 -0.4681  11.7184
9   1.6767 -4.6040   9.0145
10 -1.3355 -2.6874   6.5014
11  0.8296  2.6793  11.8295
12 -2.8647  4.2916   7.8414
13  1.8445 -3.3924   9.6520
14 -0.1036  2.4273  11.9766
15  1.0886 -3.5636  10.6023
16 -3.3026 -6.9837   4.9962
17  1.4050 -3.8453  10.4630
18 -0.3111 -1.4278   9.2283
19 -3.5454  2.4960   7.3720
</code></pre>

<p>And using the following R code to get the Analysis of Variance Table:</p>

<pre><code>&gt; df &lt;- read.csv('f:/data.csv', header=T, sep="","")
&gt; model_All &lt;- lm(y ~ x1 + x2, data=df)
&gt; anova(model_All)
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
x1         1 44.440  44.440  58.277 6.865e-07 ***
x2         1 26.640  26.640  34.934 1.711e-05 ***
Residuals 17 12.964   0.763                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>And I can calculate the F value of hypothesis test H0ï¼š b2=0 via:</p>

<pre><code>&gt; model_Drop2 &lt;- lm(y ~ x1, data=df)
&gt; SSR_All = sum(resid(model_All)^2)
&gt; SSR_Dp2 = sum(resid(model_Drop2)^2)
&gt; F_x2 = ((SSR_Dp2 - SSR_All) / 1) / (SSR_All / (20 - 3))
&gt; F_x2
[1] 34.93434
</code></pre>

<p>We can see that <code>F_x2</code> <strong>is equal to</strong> F value of <code>x2</code> in <code>Analysis of Variance Table</code>.
However, the value <code>F_x1</code> calculated as follow <strong>is not equal to</strong> F value of <code>x1</code>, I can't understand why:</p>

<pre><code>&gt; model_Drop1 &lt;- lm(y ~ x2, data=df)
&gt; SSR_Dp1 = sum(resid(model_Drop1)^2)
&gt; F_x1 = ((SSR_Dp1 - SSR_All) / 1) / (SSR_All / (20 - 3))
&gt; F_x1
[1] 70.30392
</code></pre>

<p>So, what is the mathematics definition of the F value in table?</p>

<p><strong>EDIT</strong> The result of the features reversed as commented by @Michael M:</p>

<pre><code>&gt; model_All2 &lt;- lm(y ~ x2 + x1, data=df)
&gt; anova(model_All2)
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
x2         1 17.468  17.468  22.907 0.0001718 ***
x1         1 53.612  53.612  70.304 1.914e-07 ***
Residuals 17 12.964   0.763                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.0987582133970426","0.100286944635719","173047","<p>Suppose I have a linear regression model Y ~ b1*x1 + b2*x2 with the sample data:</p>

<pre><code>        x1      x2        y
0  -0.7251 -0.6033  10.2508
1  -1.3148  3.7067  10.3054
2  -1.2818 -5.8136   7.2010
3   0.6154  3.6336  11.2807
4   1.4887 -2.8337  10.6299
5  -2.0224 -0.3931   7.4502
6   0.5419 -1.7807  10.7339
7  -1.0745 -5.0296   5.8734
8   3.2116 -0.4681  11.7184
9   1.6767 -4.6040   9.0145
10 -1.3355 -2.6874   6.5014
11  0.8296  2.6793  11.8295
12 -2.8647  4.2916   7.8414
13  1.8445 -3.3924   9.6520
14 -0.1036  2.4273  11.9766
15  1.0886 -3.5636  10.6023
16 -3.3026 -6.9837   4.9962
17  1.4050 -3.8453  10.4630
18 -0.3111 -1.4278   9.2283
19 -3.5454  2.4960   7.3720
</code></pre>

<p>And using the following R code to get the Analysis of Variance Table:</p>

<pre><code>&gt; df &lt;- read.csv('f:/data.csv', header=T, sep="","")
&gt; model_All &lt;- lm(y ~ x1 + x2, data=df)
&gt; anova(model_All)
Analysis of Variance Table

Response: y
          Df Sum Sq Mean Sq F value    Pr(&gt;F)    
x1         1 44.440  44.440  58.277 6.865e-07 ***
x2         1 26.640  26.640  34.934 1.711e-05 ***
Residuals 17 12.964   0.763                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>And I can calculate the F value of hypothesis test H0ï¼š b2=0 via:</p>

<pre><code>&gt; model_Drop2 &lt;- lm(y ~ x1, data=df)
&gt; SSR_All = sum(resid(model_All)^2)
&gt; SSR_Dp2 = sum(resid(model_Drop2)^2)
&gt; F_x2 = ((SSR_Dp2 - SSR_All) / 1) / (SSR_All / (20 - 3))
&gt; F_x2
[1] 34.93434
</code></pre>

<p>We can see that <code>F_x2</code> <strong>is equal to</strong> F value of <code>x2</code> in <code>Analysis of Variance Table</code>.
However, the value <code>F_x1</code> calculated as follow <strong>is not equal to</strong> F value of <code>x1</code>, I can't understand why:</p>

<pre><code>&gt; model_Drop1 &lt;- lm(y ~ x2, data=df)
&gt; SSR_Dp1 = sum(resid(model_Drop1)^2)
&gt; F_x1 = ((SSR_Dp1 - SSR_All) / 1) / (SSR_All / (20 - 3))
&gt; F_x1
[1] 70.30392
</code></pre>

<p>So, what is the mathematics definition of the F value in table?</p>
"
"0.0798086884467622","0.0810440898473108","174248","<p>Given a simple linear model:</p>

<pre><code>N &lt;- 10
x &lt;- rnorm(N)
y &lt;- x + rnorm(N)
firstData &lt;- data.frame(x, y)

interceptOnly &lt;- lm(y ~ 1, firstData)
linearModel &lt;- lm(y ~ x, firstData)

anova(interceptOnly, linearModel)
summary(linearModel)
</code></pre>

<p>It is possible to predict new values from the same model:</p>

<pre><code>newX &lt;- rnorm(N)
newY &lt;- newX + rnorm(N)
newData &lt;- data.frame(x=newX, y=newY)
newData$predictedY &lt;- predict(linearModel, newData)
</code></pre>

<p>But how do you then evaluate the predicted values? Of course, you can put it in a new lm and say: <code>newLinearModel &lt;- lm(y ~ x, newData)</code> But I would like to use the intercept and coefficient from the original model!</p>

<p>Does anyone know how to produce some kind of summary / anova based on the original intercept+coefficient but calculating the error from the new data?</p>
"
"0.119713032670143","0.121566134770966","174861","<p>Here is <a href=""https://onlinecourses.science.psu.edu/stat501/sites/onlinecourses.science.psu.edu.stat501/files/data/iqsize.txt"" rel=""nofollow"">sample data</a>:</p>

<pre><code>    brainIQ &lt;- 
  read.table (file= ""https://onlinecourses.science.psu.edu/stat501/sites/onlinecourses.science.psu.edu.stat501/files/data/iqsize.txt"",
 head = TRUE)
</code></pre>

<p>I am trying to fit multiple linear regression.</p>

<pre><code>mylm &lt;- lm(PIQ ~  Brain + Height + Weight, data = brainIQ)
anova(mylm)
</code></pre>

<p>Default function anova in R provides sequential sum of squares (type I) sum of square. </p>

<pre><code>Analysis of Variance Table

Response: PIQ
          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
Brain      1  2697.1 2697.09  6.8835 0.01293 *
Height     1  2875.6 2875.65  7.3392 0.01049 *
Weight     1     0.0    0.00  0.0000 0.99775  
Residuals 34 13321.8  391.82                  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I belief, thus the SS are Brain, Height | Brain, Weight | (Brain, Weight) and residuals respectively.</p>

<p>Using package car we can also get type II sum of square. </p>

<pre><code>library(car)
Anova(mylm, type=""II"")
Anova Table (Type II tests)

Response: PIQ
           Sum Sq Df F value    Pr(&gt;F)    
Brain      5239.2  1 13.3716 0.0008556 ***
Height     1934.7  1  4.9378 0.0330338 *  
Weight        0.0  1  0.0000 0.9977495    
Residuals 13321.8 34                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Here sum of squares are like: Brian | (Height, Weight), Height | (Brain, Weight), Weight | (Brain, Height).</p>

<p>Which look pretty like Mintab output:</p>

<p><a href=""http://i.stack.imgur.com/0iXgH.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/0iXgH.png"" alt=""enter image description here""></a></p>

<p>My question is how can I calculate the regression row in the above table in R ? </p>
"
"0.191540852272229","0.186401406648815","175975","<p>Say, I wanted to compare the effect of $t=3$ (8 hours, 12 hours, 16 hours) lengths of exposures to sun on plant growth. I randomly applied these 3 lengths of exposures to $r=3$ pots but let us say that it is too costly or tedious to measure plant growth for all plants in the entire pot, so I randomly selected $s=4$ plants within each pot for my measurement. The variables would be: treatment (a factor of 3 different hours of exposure to sunlight at which each measurement of growth is taken), pot (a factor of three per treatment), plant (a factor of four plants per pot), growth (dependent variable). The following are the null hypotheses to be tested in order (according to our manual): (1) The mean plant growth within pots are the same. (2) There are no differences in mean plant growth among the different treatments.</p>

<p>The linear model for the experiment is</p>

<p>$$Y_{ijk}=\mu+\tau_i+\delta_{ij}+\varepsilon_{ijk}$$</p>

<p>where $Y_{ijk}$ is the $k$th response on the $j$th pot applied with the $i$th treatment, $\tau_i$ is the effect of the length of exposure to sunlight on plant growth, $\delta_{ij}$ is the error associated to the $j$th pot in the $i$th treatment on the growth of the plant, and $\varepsilon_{ijk}$ is the error attributed to the $k$th plant on the $j$th pot applied with treatment $i$.</p>

<p>In the corresponding ANOVA table, I have $SSTot=SSTrt+SSPE+SSSSE$, where SSTot is Total Sum of Squares, SSTrt is Treatment Sum of Square, SSPE is Sum of Squares for the Pots, and SSSSE is the sum of squares for the subsamples,  with degrees of freedom, $t-1$, $t(r-1)$, $tr(s-1)$, respectively, where $t=$ number of treatments, $r=$ number of experimental units and $s=$ number of sampling units,</p>

<p>Our school manual suggests a sequential tests of hypotheses. For tests of variability of the experimental units, it says to test</p>

<p>$$ \frac{MSPE}{MSSSE}\sim F_{(\alpha,t(r-1),tr(s-1))}$$</p>

<p>Then it suggests to consider the following cases for the test of differences among treatment means</p>

<ul>
<li>When $H_0:\sigma^2_\varepsilon=0$ is rejected</li>
</ul>

<p>$$\frac{MSTrt}{MSPE}\sim F_{(t-1,t(r-1))}$$</p>

<ul>
<li>When $H_0: \sigma^2_{\varepsilon}=0$ is accepted</li>
</ul>

<p>$$\frac{MSTrt}{MSE_{pooled}}\sim F_{(t-1,tr(s-1))}$$</p>

<p>where </p>

<p>$$MSE_{pooled}=\frac{SSPE+SSSSE}{t(rs-1)}$$.</p>

<p>Here, MS stands for mean squares for the corresponding sum of squares previously defined.</p>

<p><strong>Question: How do I perform this sequential tests of hypotheses in R</strong>?</p>

<pre><code>model &lt;- lm(response ~ trt/pot, data)
anova(model)
</code></pre>

<p>I can't get it to display the correct $F$ for the treatment differences in both cases. Below is an example when $H_0:\sigma^2_\varepsilon=0$ is rejected.</p>

<p>I actually don't know if all of these even make sense. I did not come from a stats background and I finally decided to start learning it after a while. My school uses SAS and this procedure seems to be built-in. I know of the University Edition of SAS but I couldn't run it on my old laptop. So I am trying to find a way to get the same output in R.</p>

<p><strong>Example</strong></p>

<p><a href=""https://dl.dropboxusercontent.com/u/28713619/crossvalidated/example.csv"" rel=""nofollow"">Here</a> is a the data set for the following.</p>

<pre><code>example &lt;- read.csv(""example.csv"", header=T)
example$pot &lt;- factor(example$pot)
example$hours &lt;- factor(example$hours)
model &lt;- lm(growth ~ hours/pot, example)
anova(model)
</code></pre>

<p>which gives the following output</p>

<pre><code>## Analysis of Variance Table
## 
## Response: growth
##           Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
## hours      2 15.0417  7.5208 15.3255 3.569e-05 ***
## hours:pot  6  8.2083  1.3681  2.7877   0.03054 *  
## Residuals 27 13.2500  0.4907                      
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</code></pre>

<p>In this particular example where there is significant variance among experimental units, our school manual prescribes that the F value for hours should be <code>anova(model)$""Sum Sq""[1]/anova(model)$""Sum Sq""[2]</code>. Is there a way to compute it automatically? In this case, the $F$ for the test of treatment differences should be 5.4973.</p>

<p>This is based on the example given in our manual. I don't know if the manual is even correct in its prescribed procedures. </p>
"
"0.0892288262810312","0.0906100470365937","176294","<p>I have tried to read documentation with no luck. </p>

<p>Let suppose i have a hundred mice and 50 of those mice has a condition X. I am studying the deliveries of the animals and want to know if an incidence of the event Y is more common among the animals with the condition X during the delivery. Some individuals of the animals has only one delivery but some of the animals can have more than one delivery. I want also standardise other factors(nominal and linear). </p>

<p>I have the following variables: 
ID (Same ID can occur on several rows if there is many deliveries), 
Condition.X, 
Color, 
Age, 
Weight, 
Event.Y(0/1)</p>

<p>I am using the following R-code:</p>

<blockquote>
  <p>model &lt;- glmer(Event.Y ~ Condition.X + Color + Age + Weight + (1 | ID), family = binomial)
  nullmodel &lt;- glmer(Event.Y ~  Color + Age + Weight + (1 | ID), family = binomial)
  anova(nullmodel, model)</p>
</blockquote>

<p>If I got it right the method for approximation above is Laplace. </p>

<p>I have also analysed data with SAS and by using RSPL or MMPL methods the significance is better. </p>

<p><strong>My question:</strong> Is it possible to use similar methods to RSPL/MMPL with glmer-function in R? </p>
"
"0.121742211640217","0.146104310758895","176869","<p>I am quite newbie of ANOVA, and just learnt how to run the function.</p>

<p>I am using the function 'Anova' from the package 'car' in R, and when I summary the analysis, I see the text:</p>

<pre><code>Univariate Type II Repeated-Measures ANOVA Assuming Sphericity
</code></pre>

<p>My professor told me that, ""assuming sphericity"" is not the thing she want, and she want some kind of ""real"" thing. Unfortunately, she does not know R well so she cannot tell me how to do that.</p>

<p>Could you explain me what is ""assuming sphericity"", and how could I satisfy the requirement of my prof?</p>

<p><strong>Updated:</strong></p>

<p>Thanks for Rolan and Peter, I chose to run ""lme"" (in package ""nlme"") and I have the output:</p>

<pre><code>    lme3 &lt;- lme (Z_score ~ WTH1 + WTH2, random = ~1|BTW, data = spf)
    summary (lme3)

&gt; Linear mixed-effects model fit by REML  Data: spf 
&gt;        AIC      BIC    logLik
&gt;   327.9795 341.7903 -158.9897
&gt; 
&gt; Random effects:  Formula: ~1 | BTW
&gt;          (Intercept)  Residual StdDev: 2.310226e-05 0.8962092
&gt; 
&gt; Fixed effects: Z_score ~ WTH1 + WTH2 
&gt;                  Value Std.Error  DF   t-value p-value  
&gt;(Intercept) -0.6584315 0.1417031 113 -4.646557   0e+00  
&gt;WTH11        0.6741030 0.1636247 113  4.119813   1e-04  
&gt;WTH21        0.6427601 0.1636247 113  3.928259   1e-04  
&gt;Correlation: 
&gt;       (Intr) WTH11  
&gt; WTH11 -0.577        
&gt; WTH21 -0.577  0.000
&gt; 
&gt; Standardized Within-Group Residuals:
&gt;        Min         Q1        Med         Q3        Max 
&gt; -2.1274596 -0.7449203 -0.1466509  0.6385587  3.2199382 
&gt; 
&gt; Number of Observations: 120 Number of Groups: 5
</code></pre>

<p>So, you can see, my dataset have 5 columns:</p>

<pre><code>&gt; str (spf)
&gt; 'data.frame': 120 obs. of  5 variables:
&gt;  $ id     : Factor w/ 30 levels ""1"",""2"",""3"",""4"",..: 1 1 1 1 2 2 2 2 3 3 ... 
    &gt;  $ BTW    : Factor w/ 5 levels ""1"",""2"",""3"",""4"",..: 1 1 1 1 1 1 1 1 1 1 ...
&gt;  $ WTH1   : Factor w/ 2 levels ""0"",""1"": 1 1 2 2 1 1 2 2 1 1 ...
    &gt;  $ WTH2   : Factor w/ 2 levels ""0"",""1"": 1 2 1 2 1 2 1 2 1 2 ...
&gt;  $ Z_score: num  -1.06 -0.678 1.194 1.94 -1.06 ...
</code></pre>

<p>BTW (between) is actually a Group ID (we run an experiment 5 times, each time with a different group), and WTH1 and WTH2 (within) are within variables. Z_score is the measurement.</p>

<p>I want to know, is there any effect of BTW on WTH1 and WTH2 (I hope not), but I do not know how to interpret the data of lme?</p>

<p>(Using Anova function I mentioned above, the result is quite straightforward, with the notion '<strong>*' and '</strong>' etc to determine the significant level, but there is a problem related to 'Assuming Sphericity"")</p>
"
"0.132347737294141","0.134396418749691","179250","<p>I am conducting a multifactorial analyisis involving categorical variables by using R. The response is â€œyesâ€ or â€œnoâ€ (Iâ€™m therefore using binary logistic regression) and the predictors have 2 up to 3 levels. The sample size is quite small (n = 230).</p>

<p>I tried before both the glm and rsm packages, but one predictor (B2) had an extremely high standard error; it was a separation problem since one of its level has zero observation. I have been then suggested to rely on penalized logistic regression and in particular the brglm package.</p>

<p>Here as follows, you can see the new output (brglm) for the model containing all the variables:</p>

<pre><code>Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)  1.64053    0.78655   2.086   0.0370 * 
B1x         -0.07434    0.71502  -0.104   0.9172   
B1y         -0.20966    0.59622  -0.352   0.7251   
B2x          0.09004    0.57168   0.158   0.8748   
B2y         -2.68270    1.67192  -1.605   0.1086   
B3x         -0.53347    0.33882  -1.574   0.1154   
B4x         -0.59495    0.30320  -1.962   0.0497 (*) 
B5x         -0.79962    0.47249  -1.692   0.0906 . 
B5y         -1.42761    0.59344  -2.406   0.0161 (*) 
B6x          1.36948    0.49224   2.782   0.0054 (**)
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 275.58  on 229  degrees of freedom
Residual deviance: 270.93  on 220  degrees of freedom
Penalized deviance: 253.9314 
AIC:  290.93 
</code></pre>

<p>As you can see, the problem regarding the huge standard error has been finally solved but there are still several issues:</p>

<p>1) In the description of the bglrm package, they say:</p>

<p>â€œIt is not advised to use methods associated with model comparison (add1, drop1,anova, etc.) on objects of class ""brglm"". Model comparison when estimation is performed using the modified scores or the penalized likelihood is an on-going research topic and will be implemented as soon as it is concludedâ€</p>

<p><strong>How can I safely remove non significant predictors?</strong> <strong>Have I only to rely on the p-value in the above output?</strong> 
That means that I have to remove from a further model predictors from B1 to B4, is that correct? (that was also case when I used a non penalized logistic regression model (such as lrm), but then I could use drop1).</p>

<p>2) <strong>How can I detect multicollinearity issues in brglm?</strong> </p>

<p>By using a non penalized likelihood model, I know that there is multicollinearity between B5 and B6, but how can I prove it now?</p>

<p>3) <strong>How can I calculate in brglm indexes such as C and Nagelgerkeâ€™s R</strong>? </p>

<p>4) In the description of the package, it is also written: â€œThe use of Akaike's information criterion (AIC) for model selection when method = ""brglm.fit"" is controversial.â€  </p>

<p>Is it safe or not to publish AIC in a paper?</p>

<p>Thank you very much in advance!</p>
"
"0.132347737294141","0.109960706249747","179498","<p>I'm working on a project with R and I don't think I'm using the appropriate linear regression or plot, I've made both but they don't seem to match.  The study is an ANOVA comparing $CO_2$ emissions per capita with 5 groups of income levels and a relevant linear regression.  For the linear regression I want use $CO_2$ as the dependent variable and $GDP$ as the independent variable and the 5 $income$ levels as dummy variables.</p>

<p>Begin by ordering the variables and remove the intercept:</p>

<pre><code>income_factor = factor(Data01$income, levels=c(""Low income"", 
""Lower middle income"", ""Upper middle income"", ""High income: OECD"", ""High
income: nonOECD"")) 

lm.r = lm(CO2 ~ income_factor -1, data=Data01)
</code></pre>

<p>Gives</p>

<pre><code>summary(lm.r)
Coefficients:
                              Estimate Std. Error t value Pr(&gt;|t|)    
income_factorLow income             0.2318     0.6943   0.334  0.73902    
income_factorLower middle income    1.7727     0.6355   2.789  0.00603 ** 
income_factorUpper middle income    4.7685     0.6271   7.604 4.12e-12 ***
income_factorHigh income: OECD      8.7926     0.7305  12.036  &lt; 2e-16 ***
income_factorHigh income: nonOECD  19.4642     1.3667  14.242  &lt; 2e-16 ***
</code></pre>

<p>So that we may write the linear regression in the form:</p>

<p>$$ CO_2 = \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \beta_4 X_4 + \beta_5 X_5 $$</p>

<p>Where $X_i$ is a dummy variable 1 at the level of income and 0 otherwise</p>

<p>For the corresponding plot I used:</p>

<pre><code> plot &lt;- ggplot(data=Data01, aes(x=GDP, y=CO2, colour=factor(income)))
 plot + stat_smooth(method=lm, fullrange=FALSE) + geom_point()
</code></pre>

<p>Which gives the graph</p>

<p><a href=""http://i.stack.imgur.com/5Iw53.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5Iw53.png"" alt=""CO2 ~ GDP""></a></p>

<p>But here is my confusion, it looks like there is the <em>lm</em> term in the plot, but I don't think it is using the same values taken from the previous linear regression.  As Looking at summary from the linear regression, High income: OECD the estimate is 8.79, but the line for it is pretty much flat.</p>

<p>While I was typing this I realized that the graph has $GDP$ as the X-axis, but is not included in the linear regression.  Would multiplying by $income$_$factor*GDP$ help?</p>
"
"0.0798086884467622","0.0810440898473108","180033","<p>I got a dataset, and I fit the data by linear model first and get a fitted model. and then I introduced a random effect, the subject or id in R arguments. and get a fitted gee model, so how to compare these two models? If I use linear mixed model such as the function lme in R, may I use anova to compare the lmm model and lm model. The difference between the two models is just the random effect. I used the anova function in R and get the answer, I was wondering is it reliable?</p>

<pre><code>m1&lt;-lme(A ~ B ,data=data)
m2&lt;-lme(A ~ B ,data=data,random=~1|Name)
</code></pre>
"
"0.145173376018241","0.157248631892946","180288","<p>I am trying to understand the effect of a covariate (COVAR) in a linear mixed effects model with 2 categorical IVs (IV1, IV2). In order to illustrate where I am struggling, I had to paste the rather long <code>dput()</code> here:</p>

<pre><code>df &lt;- structure(list(ID=c(1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L,1L,2L,3L,4L,5L,6L,7L,8L,9L,10L,11L,12L,13L,14L,15L),
IV1=structure(c(1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,5L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L,4L),.Label=c(""412A"",""415D"",""512A"",""515A"",""615A""),class=""factor""),
IV2=structure(c(1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,1L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,3L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L,2L),.Label=c(""24"",""27"",""2403""),class=""factor""),
DV=c(NA,NA,NA,17,19,27,14,21,21,31,34,NA,22,29,32,16,18,NA,NA,NA,39,33,27.5,28,27,NA,18,NA,24,38,27,15,NA,NA,22,27,17,52,NA,19,35,37,38,30,29,44,74,60,31,54,66,61,60,35,49,NA,52,53,30,36.5,46,57,54,59,NA,41,45,53.5,39,48,43,58.5,50,31,46,23,46,44,25,51,49,32.5,51,37,53,34,52,56,50.5,10,33,31,35,39,27,22,36,21,39,26,35,24,NA,28,39,28,35,21,39,34,30,NA,25,13,NA,31,28,29,32,NA,21,18,32,34,33.5,55,46,26.5,57,29,37,NA,23,52,31,32,41,25,29.5,47,37.5,30,NA,NA,NA,NA,43,43,43,29,42,31,NA,36,16,55,11,30,50,49,38,33,42,45,43.5,35,28,NA,44,36.5,34,41,35,17,38.5,24,49,42,40.5,37.5,15,37.5,32,30,44,25,38,39.5,37.5,43,25,28.5,26,32,43,NA,19,35,19,40.5,33,13,39,39,32,39,44,7,39,40,16,35,52,33,NA,54,24,52,37,31,27,24,31,18,50,16,31,NA,43,NA,42,39,NA,NA,51,36,38,28,NA,30,27,30,31,31,19,NA,38,35,38,21,29,31,8,32,19,23,18,NA,22,30,31,44,31,14,NA,28,25,34,32,39,30,27,33,44,47,16,46.5,12,24,17,40,29,21,47,6,19.5,39,32,28,43,51,42,44,36,48,37,32,37,43,41,10,5,37,28,10,35,45.5,51,22,35,38,39,45,44,46,24,41,37.5,30,NA,33,21,24,NA,25,27,18,NA,22,42,19,30,31,36,19,18,42,25,12,30,32,36.5,27,36,39,37,36,43,35,30.5,11,36,15,43,37,38,23,34,NA,14,39,35,42,38,45,31,41,37,36,37,33,12,44,42,45,39.5,36,44.5,38,14,14,36.5,36,32,43,39,35,38,51,43,48,35,25,49,46,26,46,51.5,35,45.5,NA,53,38.5,45,53,34,51,31,13,36,NA,32,37,43,43,19,35.5,45,41,28,42,44,43,44,34,30,46,43,45,37,33.5,47,23,19,36,38.5,26,41,NA,34,35.5,25,11,38,34,47,9,47,16,20,31,9,9,35,32,NA,34.5,31,NA,32,39,NA,NA,NA,NA,32,26,10,11,NA,37,44,25,15,37,25,10,NA,15,32,NA,24,27,NA,25,31,23,41.5,27,40,31,32,11,NA,14,25,29,36,37,31.5,37,27,21,NA,27,38,NA,NA,25,23,25,40,NA,47,35,33,39,35,38,43,27,35.5,33,28,NA,40,30,48,39,11,35,42.5,42.5,42,42,38,48,46,41,NA,32.5,43.5,34,29,35,NA,38,NA,NA,31,36,31,28.5,15,25,34,30,36,26,35,39,19,NA,NA,31,22,NA,NA,35,35,15,23,38.5,38,NA,36,16,18,26,30,28,NA,25,27,26,25,5,41,29,37,28,34,43,38,29,45,NA,41,32,37,50,31,NA,35,40,41,36,25,34,38,32,38,42,33,34,39,34,39,31,46,8,NA,36,48,25,32,37,NA,40,32,17,37,29,NA,37.5,NA,38,39,NA,44,48,40,NA,20,NA,36.5,20,33,31,41,32.5,28,43,39,29,23,37,32,39,26,36,15,37,31,11,38,29,42,38.5,32,30,37,38,32,33),
COVAR=c(5.2,5.2,5.87,5.68,5.49,7.67,6.3,8.34,7.01,5.51,5.8,4.35,3.95,5.23,6.32,4.01,3.16,3.61,4.67,3.44,5.27,4.59,4.18,4.64,3.97,4.11,3.68,7.57,3.97,5.9,6.02,4.79,5.14,5.84,7.61,4.99,4.18,7.25,3.92,6.3,6.04,5.02,8.01,4.14,8.24,6.21,7.44,5.69,6.31,5.9,6.7,4.96,5.08,4.93,6.4,7.2,7.38,9.59,6.37,8.24,5.6,5.87,4.99,3.64,3.44,5.72,4.52,6.5,4.78,5.18,5.92,8.79,7.65,4.5,4.3,5.76,8.53,4.38,4.46,8.7,8.26,8.89,5.85,6.98,6.65,7.27,8.92,7.43,5.91,5.49,7.64,7.15,6.8,5.74,4.63,4.62,7.02,5.43,9.59,5.42,6.13,8.9,4.66,6.87,6.83,8.38,8.96,5.25,5.54,6.95,8.03,4.33,7.76,6.35,4.99,7.41,6.13,4.67,4.1,4.51,4.6,3.71,6.72,5.37,8.21,6.5,5.46,5.6,7.83,5.08,5.42,3.9,4.88,6.63,4.21,5.3,4.57,8.56,3.84,7.07,4.84,6.19,5.15,3.73,5.32,8.32,7.09,6.06,5.42,7,6.65,5.28,6.08,4.84,4.73,5.15,5.44,6.38,7.4,6.28,4.96,5.14,5.53,8.46,6.93,5.34,5.03,4.4,6.68,7.31,6.17,5.5,9.65,4.36,4.64,6.77,6.95,7.56,8.47,4.68,3.9,4.33,4.77,3.65,5.17,4.44,6.37,4.35,4.55,7.09,4.06,7.78,4.49,6.37,9.03,2.67,3.89,4.38,5.56,6.77,4.48,4.69,4.94,6.17,4.32,4.25,8.11,3.79,5.62,3.99,5.19,4.47,7.07,8.32,8.79,4.27,4.55,4.5,4.15,5.12,10.11,7.68,4.01,6.53,5.66,6.52,5.99,6.62,9.44,5.44,11.1,8.62,5.85,3.82,9.46,8.69,10.36,6.95,6.27,8.37,6.35,7.12,3.71,8.21,5.98,5.49,7.62,6.31,7.98,8.26,6.93,7.03,3.4,3.35,4.74,5.84,7.99,5.07,7.35,7.88,7.44,9.32,7.22,6.47,5.32,5.98,6.61,8.26,7.79,8.19,7.05,3.24,6.5,3.94,7.33,4.4,6.22,5.95,3.56,6.13,6.98,5.2,5.67,5.29,3.6,4.71,5.88,4.27,4.52,5.44,5.39,6.07,6.51,3.24,7.55,4.52,4.19,6.41,5.43,5.48,4.08,5.26,6.99,3.66,5.4,6.13,7.24,10.57,5.92,6.78,6.47,7.78,12.14,8.49,8.77,4.74,8.49,8.03,9.02,5.42,8.22,4.95,5.77,7.49,4.52,4.8,4.62,7,9.01,9.36,4.73,5.14,6.63,7.44,6.91,5.47,7.24,7.46,4.52,6.35,9.13,9.56,8.11,8.97,12.03,8.16,10.79,7.8,6.39,5.8,3.97,7.44,5.03,8.35,6.94,8.44,4.04,6.6,6.04,4.61,5.9,7.72,7.57,6.25,6.96,5.55,9.01,7.44,5.09,5.56,9.17,8.97,7.99,10.16,11.04,6.33,6.96,7,5.08,5.37,4.4,5.49,6.17,6.97,7.65,6.48,5.54,7.79,8.42,7,8.11,5.02,3.9,5.09,4.4,4.63,7.92,9.47,7.05,9.63,4.93,8.36,7.83,10.81,11.58,5.68,11.66,8.01,4.35,5.43,9.3,6.01,5.7,7.64,8.03,7.8,5.9,9.05,6.9,6.36,9.57,6.58,7.66,7.14,5.75,3.58,10.36,6.4,6.09,7.46,7.16,8.78,5.12,4.66,4.61,4.48,4.66,8.11,4.18,5.93,5.97,6.36,6.07,7.4,4.78,8.51,5.21,8.44,5.25,4.68,4.1,3.92,3.57,4.7,5.54,4.5,5.88,5.42,4.45,4.86,6.48,4.71,4.67,4.29,4.71,3.71,5.23,5.64,4.67,3.93,4.79,4.21,4.39,3.4,4.41,4.81,3.85,4.72,4.58,3.09,5.58,4.84,5.19,6.39,3.82,3.89,4.04,4.53,5.8,4.6,4.49,4.35,5.85,4.67,5.44,3.83,5.28,4.33,5.14,3.92,4.37,6.03,6.1,6.38,6.04,5.98,5.26,5.44,3.76,5.37,5.36,6.33,5.52,4.56,4.6,5.58,5.1,4.21,5.03,4.85,4.56,5.79,4.22,3.77,3.34,4.03,6.53,6.97,4.49,6.4,4.49,5.98,5.41,5.03,5.28,4.92,6.92,4.91,4.7,6.6,4.98,6.81,4.8,4.1,4.09,4.87,4.83,4.77,4.4,4.89,4.55,4.55,4.65,5.12,4.85,5.78,5.49,4.58,5.25,5.09,4.93,4.9,5.42,5.33,4.81,4.61,6.67,4.46,5.33,8.05,5.99,4.35,5.06,5.31,4.29,4.29,3.48,4.32,3.86,4.64,4.03,4.18,5.39,4.35,3.54,4.22,3.65,4.63,4.61,4.14,3.4,4.28,5.98,3.48,3.68,5.54,4.22,4.78,3.49,5.84,6.52,6.1,3.9,4.77,4.59,5.31,4.45,4.44,3.97,4.24,3.75,3.84,5.66,4.15,4.35,5.62,5.09,5.65,4.57,4.97,3.53,3.64,3.87,5.49,5.33,4.66,5.85,3.69,6.43,4.73,4.67,4.76,4.7,5.05,8.12,4.53,9.82,3.97,5.24,11.78,5.09,4.94,4.33,5,6.49,7.02,5.1,5.98,4.56,4.06,5.76,4.51,6.56,5.41,4.35,3.76,3.91,3.77,4.69,3.97,4.83,4.78,4.75,4.39,3.46,8.21,3.85,3.48,9.49,3.91,5.19,4.52,4.2,4.7,4.95)),.Names=c(""ID"",""IV1"",""IV2"",""DV"",""COVAR""),class=""data.frame"",row.names=c(NA,675L))
</code></pre>

<p>Model fit with the covariate:</p>

<pre><code>require(lmerTest)
require(car)

m1&lt;-lmer(DV ~ COVAR*IV1*IV2 + (1|IV1:ID), data=df)
</code></pre>

<p>Then I wanted to test whether COVAR is significant and whether an interaction between COVAR and the IVs exists. I used the <code>anova()</code> function provided by <code>lmerTest</code>. Here the covariate is significant as well as the interaction between COVAR and IV2:</p>

<pre><code>anova(m1)
Analysis of Variance Table of type III  with  Satterthwaite 
approximation for degrees of freedom
          Sum Sq Mean Sq NumDF  DenDF F.value    Pr(&gt;F)    
COVAR         4589.8  4589.8     1 567.74  56.506 2.197e-13
IV1            610.4   152.6     4 548.49   1.879  0.112731    
IV2           1223.2   611.6     2 562.64   7.529  0.000593
COVAR:IV1      208.7    52.2     4 560.52   0.642  0.632594    
COVAR:IV2      703.4   351.7     2 563.35   4.330  0.013613  
IV1:IV2        776.8    97.1     8 561.48   1.195  0.299305    
COVAR:IV1:IV2  680.6    85.1     8 561.47   1.047  0.399018
</code></pre>

<p>However when I use <code>Anova(m1, type=3)</code>, just to check and compare different outputs, it comes out like this:</p>

<pre><code>Analysis of Deviance Table (Type III Wald chisquare tests)

Response: DV
           Chisq Df Pr(&gt;Chisq)   
(Intercept)   7.7308  1   0.005429
COVAR         1.9850  1   0.158866   
IV1           6.5038  4   0.164549   
IV2           2.0069  2   0.366610   
COVAR:IV1     0.3739  4   0.984554   
COVAR:IV2     1.6527  2   0.437654   
IV1:IV2       9.5635  8   0.297007   
COVAR:IV1:IV2 8.3786  8   0.397383 
</code></pre>

<p>When I run the <code>Anova(m1)</code> it looks again closer to what <code>anova(m1)</code> produced, however, IV1 is now ""highly"" significant (which is what I would have expected a priori given the nature of IV1), plus there is also an interaction between IV1 and IV2. That being said and also given the discussions regarding type 2 and type 3 SS, I would opt for going ahead with type 2 SS:</p>

<pre><code>Analysis of Deviance Table (Type II Wald chisquare tests)

Response: DV
             Chisq Df Pr(&gt;Chisq)    
COVAR          97.1301  1  &lt; 2.2e-16 
IV1           104.2557  4  &lt; 2.2e-16 
IV2            20.0292  2  4.474e-05 
COVAR:IV1       0.2244  4  0.9941594    
COVAR:IV2       9.1881  2  0.0101119   
IV1:IV2        28.5092  8  0.0003865 
COVAR:IV1:IV2   8.3786  8  0.3973834 
</code></pre>

<p><strong>Question 1:</strong> What is the explanation for these substantial variations between these outputs (especially <code>anova(m1)</code> vs. <code>Anova(m1, type=3)</code> which are both <code>type=3</code> calculations)?</p>

<p>Given the fact that COVAR interacts with IV2 and also that <code>m2 &lt;- lmer(COVAR ~ IV1*IV2 + (1|IV1:ID), data=df); Anova(m2)</code> turns out to be significant (again for IV2), I cannot sell this anlysis as ANCOVA since both additional assumptions for ANCOVA are violated. </p>

<pre><code>Analysis of Deviance Table (Type II Wald chisquare tests)

Response: COVAR
      Chisq Df Pr(&gt;Chisq)    
IV1       3.093  4     0.5424    
IV2     160.317  2  &lt; 2.2e-16
IV1:IV2  34.734  8  2.989e-05
</code></pre>

<p>However, COVAR seems to play an important role and therefore should be kept in the model nonetheless.</p>

<p><strong>Question 2:</strong> Is this reasonable? And if yes, how do I go on and interpret the output of such a model, especially the interaction between COVAR and IV2?</p>

<p>What I would do is plot the interactions for IV1:IV2 and for COVAR:IV2 first:</p>

<pre><code>with(na.omit(df), interaction.plot(IV1,IV2,DV))
require(ggplot2)
ggplot(df,aes(x=COVAR, y=DV))+geom_point(aes(colour=IV2))+
geom_smooth(aes(colour=IV2), method=lm)
</code></pre>

<p>and then start discussing.</p>

<pre><code>&gt; sessionInfo()
R version 3.2.2 (2015-08-14)
Platform: x86_64-w64-mingw32/x64 (64-bit)
Running under: Windows 8 x64 (build 9200)

locale:
[1] LC_COLLATE=English_United States.1252 
[2] LC_CTYPE=English_United States.1252   
[3] LC_MONETARY=English_United States.1252
[4] LC_NUMERIC=C                          
[5] LC_TIME=English_United States.1252    

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods  
[7] base     

other attached packages:
[1] ggplot2_1.0.1   car_2.1-0       lmerTest_2.0-29
[4] lme4_1.1-10     Matrix_1.2-2
</code></pre>
"
"0.159617376893524","0.141827157232794","180531","<p>I am currently running multiple comparisons following ANOVA. For this I want to set my own contrasts in TukeyHSD to exclude comparisons not of interest. So the first thing I did is test whether my manual contrasts give the same results as the automatic contrasts set by <code>multcomp</code>: </p>

<pre><code>library(multcomp)
set.seed(123)

Group &lt;- paste0(rep('R',24), rep(1:6, each= 4))
meanList &lt;- list(n = 24, mean = rep(c(10:16), each=4), sd = 1)
Value &lt;- do.call('rnorm', meanList)
df &lt;- data.frame(Group,Value)

mod1 &lt;- aov(Value ~ Group, data= df)

tuk1 &lt;- glht(mod1, mcp(Group = 'Tukey'))

summary(tuk1)
Linear Hypotheses:
             Estimate Std. Error t value Pr(&gt;|t|)    
R2 - R1 == 0   1.0504     0.6911   1.520  0.65678    
R3 - R1 == 0   1.9032     0.6911   2.754  0.11231    
R4 - R1 == 0   3.2260     0.6911   4.668  0.00223 ** 
R5 - R1 == 0   3.4803     0.6911   5.036  &lt; 0.001 ***
R6 - R1 == 0   4.0302     0.6911   5.831  &lt; 0.001 ***
R3 - R2 == 0   0.8528     0.6911   1.234  0.81492    
R4 - R2 == 0   2.1756     0.6911   3.148  0.05321 .  
R5 - R2 == 0   2.4299     0.6911   3.516  0.02529 *  
R6 - R2 == 0   2.9798     0.6911   4.311  0.00486 ** 
R4 - R3 == 0   1.3228     0.6911   1.914  0.42536    
R5 - R3 == 0   1.5771     0.6911   2.282  0.25116    
R6 - R3 == 0   2.1270     0.6911   3.078  0.06095 .  
R5 - R4 == 0   0.2543     0.6911   0.368  0.99896    
R6 - R4 == 0   0.8042     0.6911   1.164  0.84794    
R6 - R5 == 0   0.5499     0.6911   0.796  0.96465 
</code></pre>

<p>If I now insert the contrast manualy I get a different result: 
I created my contrast matrix using the contrMat function for the example but this could also be done by hand.</p>

<pre><code>cM1 &lt;- contrMat(n=table(df$Group), type='Tukey') 
tuk2 &lt;- glht(mod1, linfct = cM1)

summary(tuk2)
Linear Hypotheses:
             Estimate Std. Error t value Pr(&gt;|t|)    
R2 - R1 == 0  -9.1592     1.0928  -8.382  &lt; 0.001 ***
R3 - R1 == 0  -8.3064     1.0928  -7.601  &lt; 0.001 ***
R4 - R1 == 0  -6.9837     1.0928  -6.391  &lt; 0.001 ***
R5 - R1 == 0  -6.7293     1.0928  -6.158  &lt; 0.001 ***
R6 - R1 == 0  -6.1795     1.0928  -5.655  &lt; 0.001 ***
R3 - R2 == 0   0.8528     0.6911   1.234  0.80911    
R4 - R2 == 0   2.1756     0.6911   3.148  0.05094 .  
R5 - R2 == 0   2.4299     0.6911   3.516  0.02412 *  
R6 - R2 == 0   2.9798     0.6911   4.311  0.00458 ** 
R4 - R3 == 0   1.3228     0.6911   1.914  0.41675    
R5 - R3 == 0   1.5771     0.6911   2.282  0.24407    
R6 - R3 == 0   2.1270     0.6911   3.078  0.05845 .  
R5 - R4 == 0   0.2543     0.6911   0.368  0.99890    
R6 - R4 == 0   0.8042     0.6911   1.164  0.84291    
R6 - R5 == 0   0.5499     0.6911   0.796  0.96318  
</code></pre>

<p>The p-values are very different why is this? I later found that using <code>mcp</code> lead to better results but they are still not exactely the same:</p>

<pre><code>tuk3 &lt;- glht(mod1, linfct = mcp(Group = cM1))  
summary(tuk3)

Linear Hypotheses:
             Estimate Std. Error t value Pr(&gt;|t|)    
R2 - R1 == 0   1.0504     0.6911   1.520  0.65684    
R3 - R1 == 0   1.9032     0.6911   2.754  0.11235    
R4 - R1 == 0   3.2260     0.6911   4.668  0.00218 ** 
R5 - R1 == 0   3.4803     0.6911   5.036  0.00105 ** 
R6 - R1 == 0   4.0302     0.6911   5.831  &lt; 0.001 ***
R3 - R2 == 0   0.8528     0.6911   1.234  0.81497    
R4 - R2 == 0   2.1756     0.6911   3.148  0.05298 .  
R5 - R2 == 0   2.4299     0.6911   3.516  0.02534 *  
R6 - R2 == 0   2.9798     0.6911   4.311  0.00474 ** 
R4 - R3 == 0   1.3228     0.6911   1.914  0.42571    
R5 - R3 == 0   1.5771     0.6911   2.282  0.25134    
R6 - R3 == 0   2.1270     0.6911   3.078  0.06076 .  
R5 - R4 == 0   0.2543     0.6911   0.368  0.99896    
R6 - R4 == 0   0.8042     0.6911   1.164  0.84789    
R6 - R5 == 0   0.5499     0.6911   0.796  0.96468    
</code></pre>

<p>Any help on understanding the particulars of the contrasts in glht would be greatly appreciated. </p>

<p><strong>EDIT</strong> to adress rvl's question:
The contrast matrix between the two tests indeed look differently. So that is the source of the difference. The first matrix is</p>

<pre><code>cM1
Multiple Comparisons of Means: Tukey Contrasts

        R1 R2 R3 R4 R5 R6
R2 - R1 -1  1  0  0  0  0
R3 - R1 -1  0  1  0  0  0
R4 - R1 -1  0  0  1  0  0
R5 - R1 -1  0  0  0  1  0
R6 - R1 -1  0  0  0  0  1
R3 - R2  0 -1  1  0  0  0
R4 - R2  0 -1  0  1  0  0
R5 - R2  0 -1  0  0  1  0
R6 - R2  0 -1  0  0  0  1
R4 - R3  0  0 -1  1  0  0
R5 - R3  0  0 -1  0  1  0
R6 - R3  0  0 -1  0  0  1
R5 - R4  0  0  0 -1  1  0
R6 - R4  0  0  0 -1  0  1
R6 - R5  0  0  0  0 -1  1
</code></pre>

<p>and the one automatically generated by glht is:</p>

<pre><code>tuk1$linfct

           (Intercept) GroupR2 GroupR3 GroupR4 GroupR5 GroupR6
R2 - R1           0       1       0       0       0       0
R3 - R1           0       0       1       0       0       0
R4 - R1           0       0       0       1       0       0
R5 - R1           0       0       0       0       1       0
R6 - R1           0       0       0       0       0       1
R3 - R2           0      -1       1       0       0       0
R4 - R2           0      -1       0       1       0       0
R5 - R2           0      -1       0       0       1       0
R6 - R2           0      -1       0       0       0       1
R4 - R3           0       0      -1       1       0       0
R5 - R3           0       0      -1       0       1       0
R6 - R3           0       0      -1       0       0       1
R5 - R4           0       0       0      -1       1       0
R6 - R4           0       0       0      -1       0       1
R6 - R5           0       0       0       0      -1       1
</code></pre>

<p>To me it seems that the first matrix should be the correct one, because it does only the pairwise comparisons. The first 5 lines of the second matrix compare every group to the mean of all the groups(?). </p>
"
"0.173939003877979","0.176631498807829","181687","<p>I am just dipping my toe into the ocean that is linear effects models and am working through Barr et al.'s 'Keeping it Maximal' paper, trying to figure out the best way to fit a lmem for my experiment. Say you have three groups given three different types of drug over three days: 100mg on first day, 50mg on second day, 10mg on last day. The outcome measure is how they feel that next day on some scale (e.g. mood), before they are given their daily dose (i.e. so we are measuring the effects of the previous day's dose). However participants don't come in at exactly the same time each day, thus as each time of measurement the drug will have had less time to take effect. </p>

<p>I would like to know how best to include that random effect of 'time elapsed since dose' into this model, and just how best to fit the model really.</p>

<p>This is a toy dataset. I have not built any trends into it. </p>

<pre><code>dose100mg &lt;- c(6,2,9,4,6,5,2,4,6,7,3,2)
dose50mg &lt;- c(1,2,4,3,6,1,3,3,2,1,4,1)
dose10mg &lt;- c(8,9,7,9,6,7,8,9,8,7,1,3)
timeD1 &lt;- c(24.2,20.5,26,30,22,26,19,23,29,30,24,16)
timeD2 &lt;- c(24,16,28,20,19,28,30,20,18,15,27,32)
timeD3 &lt;- c(21,28,29,30,29,17,23,18,24,16,28,21)
subject &lt;- c(1,2,3,4,5,6,7,8,9,10,11,12)
group &lt;- factor(c(0,1,0,2,1,2,0,2,1,2,1,0))

df &lt;- data.frame(subject, group, dose100mg, dose50mg, dose10mg, cov)
</code></pre>

<p>Turn it from wide to long</p>

<pre><code>require(tidyr)

df &lt;- gather(df, dose, score, dose100mg:dose50mg:dose10mg)
</code></pre>

<p>Now add the 'hours elapsed since last dose' variable to the dataframe (btw: if anyone knows how to build this into the gather function above I'd appreciate it) </p>

<pre><code>df$hrsElapsed &lt;- c(timeD1, timeD2, timeD3) 
</code></pre>

<p>Now fit a model. First group*dose plus with random intercepts for subject. </p>

<p>require(lme4)</p>

<pre><code># random intercepts

anDf_randomintercepts &lt;- lmer(score ~ group*dose + (1|subject), data = df)

anova(anDf_randomintercepts)
</code></pre>

<p>Next random slopes, and my first question. Is it better to include hrsElasped as a covariate, like this?</p>

<pre><code>anDf_randomSlopes &lt;- lmer(score ~ group*dose + hrsElapsed + (1|subject) + (1+hrsElapsed|subject), data = df)
</code></pre>

<p>Or to include it as a random effect? like this</p>

<pre><code>nDf_randomSlopes &lt;- lmer(score ~ group*dose + (1|subject) + (1+hrsElapsed|subject), data = df)
</code></pre>

<p>I know it's not the latter because I get an error message. </p>

<pre><code>Warning messages:
1: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 0.0450795 (tol = 0.002, component 1)
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p>But I don't know WHY this doesn't work. I would have thought time elapsed would be exactly the sort of variable you'd want to assign to random effects.</p>

<p>What am I doing wrong?</p>

<p>An ancillary question pertains to fitting random slopes for the group-by-subject effect </p>

<pre><code>anDf_randomSlopes &lt;- lmer(score ~ group*dose + (1|subject) + (1+group|subject), data = df)
</code></pre>

<p>When I run this i get the error message</p>

<pre><code>Error: number of observations (=36) &lt;= number of random effects (=36) for term (1 + group | subject); the random-effects parameters and the residual variance (or scale parameter) are probably unidentifiable
</code></pre>

<p>Why doesn't this work? What does it mean?</p>
"
"0.05643326479831","0.0573068255061253","182100","<p>I created an algorithm and I tested it against a current algorithm.</p>

<p>The results are in this form:</p>

<pre><code>Power   Processes   Method  Time(s)
1          3          1     19,94
1          4          1     20,04
1          5          1     20,06
1          6          1     19,95
1          7          1     20,1
1          8          1     20,03
1          3          0     30,3 
...
</code></pre>

<p>for each method where my method is ""1"" and the other message is represented by ""0"". 
Process indicates the available processing power (I only have 4 servers therefore 3,4 processes may run each on a single server, 5-8 servers have to share resources - not indicated in the example table)</p>

<p>I've made 10 replications each test.</p>

<p>I wanted to create a linear regression comparing both models in order to show that even with more processes my algorithm runs faster. But the graphs and statics I could generate with ANOVA didn't really help me?</p>

<p>Which methods do I have to use?
And how may I generate graphs explaining the differences between the regressions?</p>
"
"0.194500731735953","0.232781425629767","183441","<ul>
<li>Note: This question is heavy on R programming, but it was recommended that I post it here after I posted an almost identical question in StackOverflow</li>
</ul>

<h2>Main Question</h2>

<p>I'm looking for help correctly setting up a one-way within subjects MANOVA in R for a data-set that has no between-subject factors.</p>

<h2>Detailed Question</h2>

<p>I'm trying to figure out how to setup a one-way within-subjects MANOVA in R, where my design has a single within-subjects IV (with 2 levels), and 3 DVs. It has come down to a question of whether or not this is best done with the standard <code>manova()</code> function, or using <code>Anova()</code> from the <code>car</code> package. Using a toy example (replicated below), I have done both but get different results, and these differences seem to be associated with how each function is figuring out the appropriate degrees of freedom for the ultimate F-test.</p>

<h2>Example</h2>

<p>To demonstrate the problem, I'll use a subset of the OBrienKaiser data set, and I'll assume that each of the levels of the <code>Hours</code> within-subjects factor instead represents the measurement of a different dependent variable. I'll then take the <code>pre</code> and <code>post</code> conditions to be the two levels of my single within-subjects independent variable. To keep things concise, I'll only look at the first three levels from <code>Hours</code>. </p>

<p>So what I have for my data set is 16 subjects measured in two different conditions (<code>pre</code> and <code>post</code>) on 3 different dependent variables (<code>1</code>,<code>2</code>, and <code>3</code>).</p>

<pre><code>data &lt;- subset(OBrienKaiser,select=c(pre.1,pre.2,pre.3,post.1,post.2,post.3))
</code></pre>

<p><strong>car::Anova( )</strong></p>

<p>To perform this analysis with <code>Anova()</code>, I have primarily relied on a combination of the documentation provided with <code>car</code>, and the slightly more detailed examples found here...</p>

<p><a href=""http://socserv.mcmaster.ca/jfox/Books/Companion/appendix/Appendix-Multivariate-Linear-Models.pdf"" rel=""nofollow"">http://socserv.mcmaster.ca/jfox/Books/Companion/appendix/Appendix-Multivariate-Linear-Models.pdf</a></p>

<p>First, define the within-subjects factor and create the data structure for the linear model.</p>

<pre><code>condition &lt;- as.factor(rep(c('pre','post'),each=3))
idata &lt;- data.frame(condition)
data.model &lt;- with(data,cbind(pre.1,pre.2,pre.3,post.1,post.2,post.3))
</code></pre>

<p>Next, define the multivariate-linear model.</p>

<pre><code>mod.mlm &lt;- lm(data.model ~ 1)
</code></pre>

<p>Finally, perform the MANOVA using a call to <code>Anova()</code> and print the results</p>

<pre><code>mav.car &lt;- Anova(mod.mlm,idata=idata,idesign=~condition,type=3)
print(mav.car)
</code></pre>

<p>The output is...</p>

<pre><code>Type III Repeated Measures MANOVA Tests: Pillai test statistic
            Df test stat approx F num Df den Df   Pr(&gt;F)    
(Intercept)  1   0.91438  160.189      1     15 2.08e-09 ***
condition    1   0.37062    8.833      1     15 0.009498 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>My issue here is that I don't think the DF have been properly calculated. I remember learning something about MANOVAs losing DF for each DV included in the analysis, but the DF here seem to be typical for a univariate-ANOVA of the same design (i.e., if I didn't have multiple DVs). However, in trying to answer this question myself, I came across a pdf of a user manual for STATA (<a href=""http://www.stata.com/manuals13/mvmanova.pdf"" rel=""nofollow"">http://www.stata.com/manuals13/mvmanova.pdf</a>). It presents a problem of measuring 4 DVs for each of 8 trees from 6 different root stocks (i.e., N=48, one between-factor with 6 levels, &amp; DVs=4). They state that for the one-way MANOVA, the DF...</p>

<blockquote>
  <p>are just as they would be for an ANOVA. Because there are
  six rootstocks, we have 5 degrees of freedom for the hypothesis. There > are 42 residual degrees of
  freedom and 47 total degrees of freedom.</p>
</blockquote>

<p><strong>stats::manova( )</strong></p>

<p>This method actually comes from the answer to this posted question...</p>

<p><a href=""http://stats.stackexchange.com/questions/141468/what-is-the-best-approach-for-this-set-up-rm-anova-manova-mixed-models"">What is the best approach for this set-up: RM ANOVA / MANOVA / Mixed-Models?</a></p>

<p>...given by @Chris Novak. For demonstration, I'll use the same dataset, but cast it to a long-format to accommodate the requirements of the <code>stats::manova()</code> function and rename it <code>data2</code>. I'll omit the actual casting, but the result looks like this...</p>

<pre><code>&gt;some(data2,4)
   Subject Condition V1 V2 V3
3        3       pre  5  6  5
16      16       pre  4  5  7
23       7      post  7  7  8
25       9      post  4  5  6
</code></pre>

<p>Setting up the MANOVA using <code>stats::manova()</code> is very similar to setting up a typical repeated-measures anova with that function.</p>

<pre><code>mav.stat &lt;- with(data2,manova(cbind(V1,V2,V3) ~ Condition + Error(Subject/Condition)))
</code></pre>

<p>The output looks like this:</p>

<pre><code>Error: Subject
           Df Pillai approx F num Df den Df Pr(&gt;F)
Residuals 15                                     

Error: Subject:Condition
          Df  Pillai approx F num Df den Df  Pr(&gt;F)  
Condition  1 0.40717   2.9762      3     13 0.07066 .
Residuals 15                                         
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>The P-Values are clearly different, as are the numDf and denDf used in the calculations. While I'm inclined to think that this is the correct way of performing the within-subjects MANOVA, I'd like to know what I'm doing wrong in <code>car::Anova()</code> and how to correctly perform the MANOVA with <code>car::Anova()</code>. I'd also like to understand how the DF get treated/calculated in the computation of a within-subjects MANOVA. Thanks so much for the guidance.</p>
"
"0.167598245738201","0.194505815633546","184491","<p>My question pertains to excluding the interaction term (once it's deemed insignificant) in a two-way repeated measures ANOVA using the <code>Anova()</code> function in the <code>car</code> package. This question is motivated by:</p>

<ol>
<li>Trying to better understand how the <code>Anova()</code> function works</li>
<li>Curiosity</li>
<li>A desire to be consistent with how I have taught other types of ANOVAs (I tell my students to remove an insignificant interaction term and refit the model to assess main effects)</li>
</ol>

<p><strong>Note</strong>: I understand the <code>Anova()</code> function has a <code>type=</code> option where one may request either the type II or III SS, and thus we could simply run the model with <code>type=2</code> and assess the main effect <em>p</em>-values, even if the interaction isn't significant. However, for the reasons listed above, I'm still interested to know if there's any way to actually <em>remove</em> the interaction term and fit a main effects-only model.</p>

<p><strong>Data description</strong>: The following example is from the UCLA website and is a repeated measures two-way ANOVA with one within-subject and one between-subject factor (<a href=""http://www.ats.ucla.edu/stat/r/seminars/Repeated_Measures/repeated_measures.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/seminars/Repeated_Measures/repeated_measures.htm</a>): The data called <code>exer</code> consists of people who were randomly assigned to two different diets: low-fat and not low-fat and three different types of exercise: at rest, walking leisurely and running. Their pulse rate was measured at three different time points during their assigned exercise: at 1 minute, 15 minutes and 30 minutes.</p>

<p>Here, I'm considering only <em>time</em> and <em>diet</em> as predictors (ignoring <em>exercise</em> for simplicity). Note that <em>time</em> is a within-subjects factor and <em>diet</em> and is a between-.</p>

<p><strong>Data to recreate example</strong>:</p>

<pre><code>exer &lt;- read.csv(""http://www.ats.ucla.edu/stat/data/exer.csv"")

# Convert variables to factor
   exer &lt;- within(exer, {diet &lt;- factor(diet)
                         exertype &lt;- factor(exertype)
                         time &lt;- factor(time)
                         id &lt;- factor(id)
                         }
                  )

# Convert data to wide format for sake of Anova() function
  exer_wide &lt;- reshape(exer, 
                       v.names=""pulse"", # Outcome variable
                       timevar=""time"", # Repeated measures
                       idvar=c(""id"", ""diet""), # ID variable and non-time-varying predictors
                       direction=""wide"")
</code></pre>

<p>Snapshot of the data at this point:</p>

<pre><code>exer_wide
#    id diet exertype pulse.1 pulse.2 pulse.3
# 1   1    1        1      85      85      88
# 4   2    1        1      90      92      93
# 7   3    1        1      97      97      94
# 10  4    1        1      80      82      83
# 13  5    1        1      91      92      91
# 16  6    2        1      83      83      84
# 19  7    2        1      87      88      90
# 22  8    2        1      92      94      95
# 25  9    2        1      97      99      96
# 28 10    2        1     100      97     100
</code></pre>

<p><strong>Fitting the repeated measures two-way ANOVA</strong>:</p>

<p><strong>Step 1</strong>: Create linear model object (note between-subjects factor on the right-hand side):</p>

<pre><code>exer_lm &lt;- lm(cbind(pulse.1, pulse.2, pulse.3) ~ diet, data=exer_wide)
</code></pre>

<p><strong>Step 2</strong>: Create time factor:</p>

<pre><code>time_fac &lt;- factor(c(""1"",""2"",""3""), ordered=F) 
</code></pre>

<p><strong>Step 3</strong>: Run ANOVA (using type II SS):</p>

<pre><code>library(car)
exer_aov &lt;- Anova(exer_lm, idata=data.frame(time_fac), idesign=~time_fac, type=2)
summary(exer_aov)

# Univariate Type II Repeated-Measures ANOVA Assuming Sphericity

#                   SS num Df Error SS den Df         F    Pr(&gt;F)    
# (Intercept)   894608      1  11227.0     28 2231.1372 &lt; 2.2e-16 ***
# diet            1262      1  11227.0     28    3.1471   0.08694 .  
# time_fac        2067      2   4900.6     56   11.8078 5.264e-05 ***
# diet:time_fac    193      2   4900.6     56    1.1017   0.33940    
</code></pre>

<p>Note both the univariate and multivariate results indicate the interaction is not significant.</p>

<p>Now, my question is whether there's a way to specify that we don't want the interaction term fit in the model, or if there's no way around this given how the <code>Anova()</code> function is set-up.</p>
"
"0.126188616281267","0.128141957406415","185936","<p><strong>When we use R code for a linear mixed model like this: <code>model=lmer(y~x1+x2+(1|x3),data)</code>, how can we calculate the variance explained by each fixed variable?</strong></p>

<p>To solve this problem, I tried my best to search on the internet.
My understanding is the following : </p>

<ol>
<li><p>After we run the code : <code>model=lmer(y~x1+x2+(1|x3),data)</code>, we run the function summary(model), then we get the variance explained by the random effect ï¼ˆnamely <code>rvariance</code>ï¼‰. </p></li>
<li><p>we run function  <code>anova(model)</code>, we get the sum of square for each fixed variable; Variance of each fixed variable can be obtained by dividing sum of square into n (the number of observation)  ï¼ˆ<code>fvariance1</code>, <code>fvariance2</code>ï¼‰.</p></li>
<li><p>We use function <code>resquredLR</code> in the MunMIn package to get the whole variance explained by the whole model (whole variance)</p></li>
<li><p>To calculate variance explained by each fixed vaiable,  we use this function</p>

<p><code>fixedvaiance1= whole variance*fvariance1/(rvariance+fvariance1+fvariance2)</code></p></li>
</ol>

<p>This is my current understanding. Can anyone give some comments about my understanding? If there are problems with this approach, I'd be happy if you could point them out.</p>
"
"0.0892288262810312","0.0906100470365937","185990","<p>I am running a linear multi level model in R. 
The predictor variable is called ""OAI"", and the response variable is called ""Ens"", I am allowing the intercepts and slopes to vary with ""ID"".</p>

<p>Here is a visual plot of the data: <a href=""http://i.stack.imgur.com/nGXgc.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/nGXgc.png"" alt=""OAI vs Ens, with ID""></a></p>

<p>I have built my model up, to see if including slopes and intercepts significantly improves the model.</p>

<p>First a base line model:</p>

<pre><code>interceptOnly &lt;-gls(Ens~1, data = d, method = ""ML"", na.action=na.exclude)
</code></pre>

<p>Then I allowed intercepts to vary with ID: </p>

<pre><code>randomInterceptOnly &lt;-lme(Ens~1, data = d, random = ~1|ID, method = ""ML"", na.action=na.exclude)
</code></pre>

<p>Next I added OAI as a predictor</p>

<pre><code>randomInterceptOAI &lt;-lme(Ens~OAI, data = d, random = ~1|MusID, method = ""ML"",na.action=na.exclude)
</code></pre>

<p>Next I wanted to compare the ""randomInterceptOnly"" and the ""randomInterceptOAI"" models to see if the fit has improved now I've added the predictor variable:</p>

<pre><code>anova(randomInterceptOnly, randomInterceptOAI)
</code></pre>

<p>Unfortunately, I get this error: 
<a href=""http://i.stack.imgur.com/kX5Rc.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/kX5Rc.png"" alt=""error in R""></a></p>

<p>This is because I have some missing data points for both ""Ens"" and ""OAI"" - this means there are different numbers of observations for the ""randomInterceptOnly"" and  ""randomInterceptOAI"" models. </p>

<p>Is there a good work around for this issue? </p>
"
"0.0691163516376137","0.0701862406343596","186464","<p>I am working on a data set (n= 230) with a categorical dependent variable (outcome: 0/1) and six categorical independent variables (mostly, with only two levels). </p>

<p>There is a certain degree of multicollinearity between two variables (X1 and X6. Anova model comparison shows that a model with X1 performs slightly better than one containing X6) and <strong>a quasi-complete separation issue regarding X4</strong> (due to an empty cell).</p>

<p>I first ran a Random Forest model (all variables were included. Ntree = 5000, mtry = 3). The result was that X1, X2 and X3 are by far the most significant predictors. X4, X5 and X6 seem to have almost no discriminative power (especially X4 whose value  in vimp() is 0.00).The model seems to be reliable (C = 0.73).  </p>

<p><strong>Question 1</strong>: does it make sense at this point to fit Binary Logistic Regression only on the most important predictors obtained through the Random Forest model (X1, X2, X3) without even considering the other three?</p>

<p><strong>Question 2:</strong> In order to avoid the separation problem with Binary Logistic Regression would it make sense to get rid of X4? 
I am quite sure that the empty cell is a bias of my data set. Moreover, this category as a whole represents only 3% of the data (The contingency table is a: 140 <strong>b:0</strong> c:86 <strong>d:6</strong>).</p>
"
"0.0962528998502844","0.122178562499719","186836","<p>Based on a <a href=""http://stats.stackexchange.com/questions/182988/plotting-to-check-homoskedasticity-assumption-for-repeated-measures-anova-in-r"">previous question</a> that I asked about checking assumptions of repeated-measures ANOVAs in R (which turns out to be not so trivial), I'm wondering about the relationship between a repeated-measures ANOVA and a linear mixed model on the same data.</p>

<p>In an excellent exploration of my data, it was suggested to me that I switch to linear mixed models for my full analysis, which I have already done. However, since for reasons of completeness I still need to run a respeated-measures ANOVA, I'm specifically wondering <strong>whether assumption checks on a linear mixed model can be used to infer assumption violations of assuptions for a repeated-measures ANOVA using the same data</strong>.</p>

<p>For example, using the example data below:</p>

<pre><code>set.seed(12)

#Generate variables and data frame
subj &lt;- sort(factor(rep(1:20,8)))
x1 &lt;- rep(c('A','B'),80)
x2 &lt;- rep(c('A','B'),20,each=2)
x3 &lt;- rep(c('A','B'),10, each=4)
outcome &lt;- rnorm(80,10,2)

d3 &lt;- data.frame(outcome,subj,x1,x2,x3)

#Repeated measures ANOVA
m.aov &lt;- aov(outcome ~ x1*x2*x3 + Error(subj/(x1*x2*x3)), d3)

#Linear mixed model assumption checks
require(lme4)
#`subj` as random term to account for the repeated measurements on subject.
m.lmer&lt;-lmer(outcome ~ x1*x2*x3 + (1|subj), data = d3)

# Check for heteroscedasticity
plot(m.lmer)
# or
boxplot(residuals(m.lmer) ~ d3$x1 + d3$x2 + d3$x3)
# Check for normality
qqnorm(residuals(m.lmer))
</code></pre>

<p><strong>Can the assumption plots on m.lmer be used to test assumption violations for m.aov?</strong> For example, if m.lmer displays heteroskedasticity, would that suggest that m.aov is afflicted with heteroskedasticity as well?</p>

<p>Thanks for any insight!</p>
"
"0.05643326479831","0.0573068255061253","187509","<p>With a small book-exercise with four metric variables on 10 cases (one dependent/outcome, three independent/predictor) I ran <em>linear regression</em> in <code>SPSS</code> and <code>R</code>, and <em>ANOVA</em> (in <code>SPSS</code> declaring the predictors as ""covariates"").<br>
I found the output of the <em>SSQ</em> (Sum-of-Squares) different - and obviously from this also the F-test statistic and the p-values. Except from the last predictor the displayed values are different (the predictors in <code>R</code>may be reordered and the analysis be rerun to find all <code>SPSS</code>- coefficients).                   </p>

<p>By reengineering the computations in matrix-formulae I could reproduce the SPSS-values as well as the R-values and found, that <code>SPSS</code> uses the (partial) SSQ based on the logic of the ""usefulness""-coefficients for each predictor (which is sort of semipartial coefficient), while <code>R</code> simply uses the (hierarchically) partial SSQ. <em>(Unfortunately I'm not sure how to express that two approaches correctly so this toy-characterizing might be improved)</em> .             </p>

<p><strong><em>Q:</em></strong> Has that property of different focuses/philosophies been discussed anywhere? Is there some advantage of one over the other?   </p>

<p><hr>
Data: (taken from M. Backhaus et al., multivariate Verfahren)             </p>

<pre><code>predictors                   outcome-item
---------------------------+-------------
Preis   VerkFoer  Vertreter  Absatzmenge
12.50      2000      109      2298
10.00       550      107      1814
 9.95      1000       99      1647
11.50       800       70      1496
12.00         0       81       969
10.00      1500      102      1918
 8.00       800      110      1810
 9.00      1200       92      1896
 9.50      1100       87      1715
12.50      1300       79      1699
</code></pre>

<p>The comparision of the output:
<a href=""http://i.stack.imgur.com/zNJDB.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/zNJDB.png"" alt=""bild""></a></p>
"
"0.232680311561883","0.208484201144642","188361","<p>I'm pretty new in using <code>lmer</code> and be confused about different p-values in Tukey post hoc tests associated with exactly the same estimates. I built a linear mixed model with monetary contributions of human subjects as response variable and their wealth and number of children as explanatory variables. The experiment was designed in a way to contribute for future generations. I don't have repeated measurements of the same individual but some individuals played within the same group. There are several subsets and additional random factors but here I only want to consider the following model where <code>totalcontSubject</code> means contribution of a subject over the entire game, <code>poverty</code> is a factor with 2 levels (rich and poor), and <code>children</code> is a factor with 2 levels (child or noChild). Particularly I'm interested in understanding the fixed effects part of the model.</p>

<pre><code> &gt; summary(TC1)
Linear mixed model fit by REML ['lmerMod']
Formula: totalcontSubject ~ poverty * children + (1 | group_2)
   Data: data

REML criterion at convergence: 414.6

Scaled residuals: 
     Min       1Q   Median       3Q      Max 
-2.42955 -0.45554 -0.09361  0.45228  2.33159 

Random effects:
 Groups   Name        Variance  Std.Dev. 
 group_2  (Intercept) 8.611e-15 9.280e-08
 Residual             1.042e+02 1.021e+01
Number of obs: 58, groups:  group_2, 10

Fixed effects:
                             Estimate Std. Error t value
(Intercept)                    16.200      3.228   5.019
povertyrich                     8.600      3.953   2.175
childrennoChild                 2.800      4.565   0.613
povertyrich:childrennoChild    -4.489      5.642  -0.796

Correlation of Fixed Effects:
            (Intr) pvrty chldrC
povertyrch  -0.816              
chldrnnChld -0.707  0.577       
pvrtyrch:C   0.572 -0.701 -0.809
</code></pre>

<p>If I interpret fixed effects of the summary table in the right way, my intercept denotes poor people with children. The estimate also corresponds to the mean value of this combination in my data. According to my calculations the difference to rich people (shown as <code>povertyrich</code>) actually shows the difference of the intercept to rich people with children, even if not explicitly mentioned by <code>povertyrich</code>. This is the first issue I'm a bit confused. A reduced model only with fixed factor poverty is significant better by <code>anova()</code> but it seems data including children are used for this evaluation.</p>

<p>If I run a Tukey post hoc test by means of my TC1 model, I get a significant difference between rich and poor. But the estimates in the summary actually include children. Estimates of intercept and slope are the means of poor people with children and the difference to rich people with children. They don't correspond to the means of poor or rich data irrespective of parenthood. </p>

<pre><code>summary(glht(TC1, linfct=mcp(povertry=""Tukey"")))

         Simultaneous Tests for General Linear Hypotheses

Multiple Comparisons of Means: Tukey Contrasts


Fit: lmer(formula = totalcontSubject ~ poverty * children + (1 | 
    group_2), data = data)

Linear Hypotheses:
                 Estimate Std. Error z value Pr(&gt;|z|)  
rich - poor == 0    8.600      3.953   2.175   0.0296 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
(Adjusted p values reported -- single-step method)
</code></pre>

<p>I get even more confused when I run a Tukey post hoc test for a subset where I coded interactions in a column such as poor people with children and rich people with children. In this output I have exactly the same estimates and parameters for these categories (like in the summary shown before for rich poor people exclusively) but the p-values are different. A visual check indicates that there is a significant difference between <code>richChild</code> and <code>poorChild</code> but outputs of <code>glht</code> <code>Interak</code> shows me it is not. Also, a comparison between models <code>anova()</code> with fixed factor poverty vs. fixed factors poverty and children indicates that I can get rid of the variable children in my model. Before I do so, I would like to understand the outputs better. I also worry about the high value for Residual and the correlations in the summary table. </p>

<pre><code>&gt; summary(glht(TC1_2, linfct=mcp(Interak=""Tukey"")))

         Simultaneous Tests for General Linear Hypotheses

Multiple Comparisons of Means: Tukey Contrasts


Fit: lmer(formula = totalcontSubject ~ Interak + (1 | group_2), data = data)

Linear Hypotheses:
                               Estimate Std. Error z value Pr(&gt;|z|)
poorNoChild - poorChild == 0      2.800      4.565   0.613    0.927
richChild - poorChild == 0        8.600      3.953   2.175    0.129
richNoChild - poorChild == 0      6.911      4.026   1.717    0.312
richChild - poorNoChild == 0      5.800      3.953   1.467    0.454
richNoChild - poorNoChild == 0    4.111      4.026   1.021    0.735
richNoChild - richChild == 0     -1.689      3.316  -0.509    0.956
(Adjusted p values reported -- single-step method)
</code></pre>
"
"0.0892288262810312","0.072488037629275","189835","<p>I would appreciate some assistance with setting the statistical analyses of my experiment.</p>

<p>During my experiment 14 participant's motor responses (<code>ptp</code>) were tested once before and twice (<code>time</code>: <code>pre</code>, <code>post15</code>, <code>post60</code>) after a certain intervention (<code>condition</code>). During each test 7 different intensities were applied, and the stimulus was replied 10 times at each <code>intensity</code>.</p>

<p>This gives me the following data <code>io</code>:</p>

<pre><code>&gt; io
       subject condition   time intensity        ptp
1         Sbj1        MI    pre       90%  33.006978
...
10        Sbj1        MI    pre       90%         NA
11        Sbj2        MI    pre       90%  44.005610
...
11760    Sbj14       ERS post60      150%   415.1405
</code></pre>

<p>My variable of interest was the motor responses (<code>ptp</code>) and I would like to know whether I have an effect of intervention (<code>condition</code>) on response amplitude over time and a difference across interventions.
It is also noteworthy, that I had to rejects some of the motor responses due to preactivation. Thus the number of stimuli per <code>intensity</code> varies from 0 to 10.</p>

<p>After a lot of reading by my own, I reached the conclusion that the best approach is a linear mixed model with:</p>

<ul>
<li>dependent variable: <code>ptp</code></li>
<li>fixed factors: <code>condition</code>, <code>time</code> and <code>intensity</code></li>
<li>random effect: <code>subject</code></li>
</ul>

<p>and worked with the following code:</p>

<pre><code>io.model = lmer(ptp ~ condition + time + intensity + (1 | subject), data=io, REML=FALSE)
io.null = lmer(ptp ~ time + intensity + (1|subject), data=io, REML = FALSE)
anova(io.null, io.model)
</code></pre>

<p>However, I am not fully convinced with my setup, also as I don't include any interaction which might happen due to some of the variables.</p>
"
"0.178457652562062","0.181220094073187","197710","<p><strong>Experiment:</strong></p>

<p>I have 2 groups and both groups undergo 2 set of evaluations, one with MRI scanner and the other in the lab to test for their behavior. Both these evaluations are known to have statistically significant relationship with age and gender. </p>

<p>Statistical questions:</p>

<p>Whether there is: </p>

<p>1) statistically significant difference between the 2 groups on each evaluation? </p>

<p>2) any relationship between and within the 2 groups between each evaluation?  </p>

<p><strong>Model:</strong></p>

<p>I model the problem as </p>

<p>$\text{MRI_measure} = \beta_{0} + \beta_{1} \text{Age} + \beta_{2} \text{Gender} + \beta_{3} \text{Group} $</p>

<p>$\text{Lab_measure} = \beta_{0}+ \beta_{1}  \text{Age} + \beta_{2}  \text{Gender} +\beta_{3}  \text{Group}$</p>

<p>[Age is continuous and Gender, Group are factors/categorical] </p>

<p>In R: </p>

<pre><code>MRI_model&lt;-lm(cbind(MRI_measure, Lab_measure) ~ age+gender+group, data=data) 
</code></pre>

<p><strong>Result of R:</strong> </p>

<p><code>manova(MRI_model)</code> suggests that yes indeed all the slopes are significantly different than 0 suggesting a relationship between my measures. </p>

<p><strong>Questions</strong> </p>

<p>1) In order to test whether the difference in the MRI_measure is statistically significant between the 2 groups, I use MRI_model$fitted.values for each dependent measure and do a statistical test (either t-test or Wilcox) and claim that the difference is significant. </p>

<p>In the paper I write, multivariate multiple linear regression was performed for the groups while controlling for age and gender. The regressed out MRI_measure was statistically compared to see if the difference is different. </p>

<p>I am assuming that the predicted/fitted.values in model are the regressed out variables. Can I show this and use this result? Is this right? </p>

<p>If no, what is the correct way to statistically compare whether my 2 groups differ in their MRI measure and lab measure when controlled for age and gender. Any R library, literature, possibly a script will be greatly appreciated. </p>

<p>2) I also want to see if there is any relationship between MRI_measure and Lab_measure within the group after they are controlled for age and gender. What is the correct way to do this in R? </p>

<p>Further, I also want to see if there is any significantly different association between the 2 groups for my set of dependent variables. I am thinking of first finding the correlation between 2 dependent variable in each group and test if this correlation is statistically different between the 2 groups? Is this logic right? And if it is, how do I compare the correlation? If not, what is the right way to do this? Any R library, literature, possibly a script will be greatly appreciated. </p>
"
"0.214891470163533","0.210693135400268","198124","<p>I am fitting some generalized additive models using the <code>mgcv</code> package in R, and I am wanting to test between two
models; whether I can remove a term or not. I am, however, getting conflicting results (as far as I can tell).</p>

<p>A model, <code>m1</code>, with a smooth term for <code>x</code> added, appears to give a better fit in terms of $R^{2}_{adj}$, AIC,
deviance explained, and when comparing the models using an F-test. However, the significance of the 
smooth term is not significant (nor is it when I added to the model as a linear covariate, instead of a spline). </p>

<p>Is my interpretation of the smooth terms tests in correct? As much as I could understand the help page, was that the tests are approximate, but there is quite a large difference here.</p>

<p>The model outputs</p>

<pre><code>m1 &lt;- gam(out ~ s(x) + s(y) + s(z), data=dat)
&gt; summary(m1)
# 
# Family: gaussian 
# Link function: identity 
# 
# Formula:
# out ~ s(x) + s(y) + s(z)
# 
# Parametric coefficients:
#               Estimate Std. Error t value Pr(&gt;|t|)
# (Intercept) -7.502e-16  1.209e-01       0        1
# 
# Approximate significance of smooth terms:
#        edf Ref.df     F  p-value    
# s(x) 4.005  4.716 1.810    0.136    
# s(y) 8.799  8.951 4.032 4.01e-05 ***
# s(z) 7.612  8.526 5.649 4.83e-07 ***
# ---
# Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
# 
# R-sq.(adj) =  0.213   Deviance explained = 24.8%
# GCV = 6.9741  Scale est. = 6.6459    n = 455

&gt; AIC(m1)
#[1] 2175.898

&gt; m2 &lt;- gam(out ~ s(y) + s(z), data=dat)
&gt; summary(m2)
# 
# Family: gaussian 
# Link function: identity 
# 
# Formula:
# out ~ s(y) + s(z)
# 
# Parametric coefficients:
#              Estimate Std. Error t value Pr(&gt;|t|)
# (Intercept) 1.705e-15  1.228e-01       0        1
# 
# Approximate significance of smooth terms:
#        edf Ref.df     F  p-value    
# s(y) 8.726  8.968 5.137 6.78e-07 ***
# s(z) 8.110  8.793 5.827 1.55e-07 ***
# ---
# Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
# 
# R-sq.(adj) =  0.187   Deviance explained = 21.7%
# GCV =  7.144  Scale est. = 6.8639    n = 455

&gt; AIC(m2)
#[1] 2187.168

&gt; anova(m1, m2, test=""F"")
# Analysis of Deviance Table
# 
# Model 1: out ~ s(x) + s(y) + s(z)
# Model 2: out ~ s(y) + s(z)
#   Resid. Df Resid. Dev      Df Deviance      F    Pr(&gt;F)    
# 1    433.58     2881.6                                      
# 2    437.16     3000.7 -3.5791   -119.1 5.0073 0.0009864 ***
</code></pre>

<hr>

<p><strong>EDIT</strong>: added model from comments</p>

<pre><code>&gt; summary(m3 &lt;- gam(out ~ s(x) + s(y) + s(z), data=dat, select=TRUE))

#Family: gaussian 
#Link function: identity 

#Formula:
#out ~ s(x) + s(y) + s(z)

#Parametric coefficients:
#              Estimate Std. Error t value Pr(&gt;|t|)
#(Intercept) -1.588e-14  1.209e-01       0        1

#Approximate significance of smooth terms:
#       edf Ref.df     F  p-value    
#s(x) 4.424      9 1.750  0.00161 ** 
#s(y) 8.260      9 3.623 5.56e-06 ***
#s(z) 7.150      9 5.329 4.19e-09 ***
#---
#Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

#R-sq.(adj) =  0.212   Deviance explained = 24.7%
#GCV = 6.9694  Scale est. = 6.6502    n = 455
</code></pre>
"
"0.215010953892623","0.211061249400414","198181","<p><strong>Scientific question:</strong>
I want to know if temperature is changing across time (specifically, if it is increasing or decreasing). </p>

<p><strong>Data:</strong> My data consists of monthly temp averages across 90 years from a single weather station. I have no NA values. The temp data clearly oscillates annually due to monthly/seasonal trends. The temp data also appears to have approx 20-30-yr cycles when graphically viewing annual trends (by plotting annual avg temps across year):</p>

<p><a href=""http://i.stack.imgur.com/MapTs.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/MapTs.png"" alt=""NC Temp deviation""></a> </p>

<p><strong>Analyses done in R using nlme() package</strong></p>

<p><strong>Models:</strong> I tried a number of <code>gls</code> models and selected models that had lower AICs to move forward with. I also checked the significance of adding predictors based on ANOVA. It turns out that including time (centered around 1950), month (as a factor), and PDO (Pacific Decadal Oscillation) trend data create the 'best' model (i.e., the one with the lowest AIC and in which each predictor improves the model significantly). Interestingly, using season (as a factor) performed worse than using month; additionally, no interactions were significant or improved the model. The best model is shown below:</p>

<pre><code>mod1 &lt;- gls(temp.avg ~ I(year-1950) + factor(month) + pdo, data = df)

&gt; anova(mod1)
Denom. DF: 1102 
               numDF  F-value p-value
(Intercept)        1 87333.28  &lt;.0001
I(year - 1950)     1    21.71  &lt;.0001
pdo                1   236.39  &lt;.0001
factor(month)     11  2036.10  &lt;.0001

&gt; AIC(mpdo7,mod.2.1)
        df      AIC
mod1    15 4393.008
</code></pre>

<p>I decided to check the residuals for temporal autocorrelation (using Bonferroni adjusted CI's), and found there to be significant lags in both the ACF and pACF. I ran numerous updates of the otherwise best model (mod1) using various corARMA parameter values. The best corARMA gls model removed any lingering autocorrelation and resulted in an improved AIC. But time (centered around 1950) becomes non-significant. This corARMA model is shown below:</p>

<pre><code>mod2 &lt;- gls(temp.avg ~ I(year-1950) + factor(month) + pdo , data = df, correlation = corARMA(p = 2, q = 1)

&gt;   anova(mod2)
Denom. DF: 1102 
               numDF   F-value p-value
(Intercept)        1 2813.3151  &lt;.0001
I(year - 1950)     1    2.8226  0.0932
factor(month)     11 1714.1792  &lt;.0001
pdo                1   17.2564  &lt;.0001

&gt; AIC(mpdo7,mod.2.1)
        df      AIC
mod2    18 4300.847

______________________________________________________________________

&gt;   summary(mod2)
Generalized least squares fit by REML
  Model: temp.avg ~ I(year - 1950) + factor(month) + pdo 
  Data: df 
       AIC      BIC    logLik
  4300.847 4390.935 -2132.423

Correlation Structure: ARMA(2,1)
 Formula: ~1 
 Parameter estimate(s):
      Phi1       Phi2     Theta1 
 1.1547490 -0.1617395 -0.9562998 

Coefficients:
                    Value Std.Error  t-value p-value
(Intercept)      4.259341 0.3611524 11.79375  0.0000
I(year - 1950)  -0.005929 0.0089268 -0.66423  0.5067
factor(month)2   1.274701 0.2169314  5.87606  0.0000
factor(month)3   5.289981 0.2341412 22.59313  0.0000
factor(month)4  10.488766 0.2369501 44.26571  0.0000
factor(month)5  15.107012 0.2373788 63.64094  0.0000
factor(month)6  19.442830 0.2373898 81.90256  0.0000
factor(month)7  21.183097 0.2378432 89.06329  0.0000
factor(month)8  20.459759 0.2383149 85.85178  0.0000
factor(month)9  17.116882 0.2380955 71.89083  0.0000
factor(month)10 10.994331 0.2371708 46.35618  0.0000
factor(month)11  5.516954 0.2342594 23.55062  0.0000
factor(month)12  1.127587 0.2172498  5.19028  0.0000
pdo             -0.237958 0.0572830 -4.15408  0.0000

 Correlation: 
                (Intr) I(-195 fct()2 fct()3 fct()4 fct()5 fct()6 fct()7 fct()8  fct()9 fc()10 fc()11 fc()12
I(year - 1950)  -0.454                                                        
factor(month)2  -0.301  0.004                                                 
factor(month)3  -0.325  0.006  0.540                                          
factor(month)4  -0.330  0.009  0.471  0.576                                   
factor(month)5  -0.332  0.011  0.460  0.507  0.582                            
factor(month)6  -0.334  0.013  0.457  0.495  0.512  0.582                     
factor(month)7  -0.333  0.017  0.457  0.494  0.502  0.515  0.582              
factor(month)8  -0.333  0.019  0.456  0.494  0.500  0.503  0.512  0.585       
factor(month)9  -0.334  0.022  0.456  0.493  0.500  0.501  0.501  0.516  0.585
factor(month)10 -0.336  0.024  0.456  0.492  0.498  0.499  0.499  0.503  0.515  0.583  
factor(month)11 -0.334  0.026  0.451  0.486  0.492  0.493  0.493  0.494  0.496  0.508  0.576  
factor(month)12 -0.315  0.031  0.418  0.450  0.455  0.457  0.457  0.456  0.456  0.458  0.470  0.540
pdo              0.022  0.020  0.018  0.033  0.039  0.030  0.002  0.059  0.087  0.080  0.052  0.030 -0.009


Standardized residuals:
        Min          Q1         Med          Q3         Max 
-3.58980730 -0.58818160  0.04577038  0.65586932  3.87365176 

Residual standard error: 1.739869 
Degrees of freedom: 1116 total; 1102 residual
</code></pre>

<p><strong>My Questions:</strong></p>

<ol>
<li><p>Is it even appropriate to use an ARMA correlation here?</p>

<ul>
<li>I assume that any inferences from a simple linear model (e.g., <code>lm(temp ~ year)</code>) are inappropriate b/c of other underlying correlation structure (even though this simple linear trend <em>is</em> what I'm most interested in.</li>
<li><p>I assume by removing affects of time lags (i.e. autocorrelation), I can better 'see' if there is in fact a long term temporal trend (incline/decline)?</p>

<ul>
<li>Is this the correct way to think about this?</li>
</ul></li>
</ul></li>
<li><p>Concerning year becoming non-significant in the model...</p>

<ul>
<li>Would this have occurred because <em>all</em> of the temporal trend turned out to be due to autocorrealtion and therefore is now otherwise being accounted for in the model?</li>
<li><p>Do I remove time from my model now (since it's no longer a significant predictor)??</p>

<ul>
<li><p><strong>UPDATE:</strong> I did do this, and the resulting model had a lower AIC (4291 vs 4300 of mod2 above). </p></li>
<li><p>Though this isn't really a useful step for me, because I'm actually concerned about a trend in temp due to <em>time</em> (i.e., year) itself. </p></li>
</ul></li>
</ul></li>
<li><p>Interpretation -- Am I interpreting the results correctly??:</p>

<ul>
<li>So based on the <code>summary</code> output above for mod2, is it correct to assume the answer to my original scientific question is: ""temperature has declined at a rate of -0.005929, but this decline is not significant (p = 0.5067)."" ??</li>
</ul></li>
<li><p>Next steps...</p>

<ul>
<li>I ultimately want to see if temperature will have an impact on tree-community time-series data. My motivation behind the procedure mentioned here was to determine if there was a trend in temperature before bothering to start including it in subsequent analyses.</li>
<li>So as performed, I assume I can now say that there is not a significant linear change (increase/decline) in temp. This would suggest that perhaps temp is not important to include in subsequent analyses?</li>
<li>However...perhaps the cyclic nature of the temp <em>is</em> important and drives cyclic patterns in the plant data. How would I approach this? (i.e., how do I 'correlate' the cyclic trend in temp with potential cyclic trend in plants' -- vs. simply <em>removing</em> cyclic (seasonal) trends based on the ACF results)? </li>
</ul></li>
</ol>
"
"0.169534769933959","0.163098084665869","198484","<p>Consider this example:</p>

<pre><code>foo &lt;-data.frame(x=c(0.010355057,0.013228936,0.016313905,0.019261687,0.021710159,0.023973474,0.025968176,0.027767232,0.029459730,0.030213807,0.023582566,0.008689883,0.006558429,0.005144958),
                 y=c(971.3800,1025.2271,1104.1505,1034.2607,902.6324,713.9053,621.4824,521.7672,428.9838,381.4685,741.7900, 979.7046,1065.5245,1118.0616))
Model3 &lt;- lm(y~poly(x,3),data=foo)
Model4 &lt;- lm(y~poly(x,4),data=foo)
</code></pre>

<p>For <code>Model3</code>, the <code>poly(x,3)</code> term is not significant:</p>

<pre><code>&gt; summary(Model3)

Call:
lm(formula = y ~ poly(x, 3), data = foo)

Residuals:
   Min     1Q Median     3Q    Max 
-76.47 -51.61  -0.55  38.22 100.57 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   829.31      17.85  46.463 5.14e-13 ***
poly(x, 3)1  -819.37      66.78 -12.269 2.37e-07 ***
poly(x, 3)2  -373.05      66.78  -5.586 0.000232 ***
poly(x, 3)3   -87.85      66.78  -1.315 0.217740    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 66.78 on 10 degrees of freedom
Multiple R-squared:  0.9483,    Adjusted R-squared:  0.9328 
F-statistic: 61.15 on 3 and 10 DF,  p-value: 9.771e-07
</code></pre>

<p>However, for <code>Model4</code> it is:</p>

<pre><code>&gt; summary(Model4)

Call:
lm(formula = y ~ poly(x, 4), data = foo)

Residuals:
    Min      1Q  Median      3Q     Max 
-34.344 -19.982   1.229  18.499  33.116 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  829.310      7.924 104.655 3.37e-15 ***
poly(x, 4)1 -819.372     29.650 -27.635 5.16e-10 ***
poly(x, 4)2 -373.052     29.650 -12.582 5.14e-07 ***
poly(x, 4)3  -87.846     29.650  -2.963 0.015887 *  
poly(x, 4)4  191.543     29.650   6.460 0.000117 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 29.65 on 9 degrees of freedom
Multiple R-squared:  0.9908,    Adjusted R-squared:  0.9868 
F-statistic: 243.1 on 4 and 9 DF,  p-value: 3.695e-09
</code></pre>

<p>Why does this happen? Note that the estimate of all coefficients is the same in both cases, since the polynomials are orthogonal. However, the significance is not. This seems to me difficult to understand: if I performed a degree 3 regression, it looks like I could drop the <code>poly(x, 4)3</code> term, thus reverting to a degree 2 orthogonal regression. However, if I performed a degree 4 regression, I shouldn't, even though the coefficients of the common terms have exactly the same estimate. What do I conclude? Probably that one should never trust subset selection :) An <code>anova</code> analysis says that the difference among the degree 2, degree 3 and degree 4 models is significant:</p>

<pre><code>&gt; Model2 &lt;- lm(y~poly(x,2),data=foo)     
&gt; anova(Model2,Model3,Model4)
Analysis of Variance Table

Model 1: y ~ poly(x, 2)
Model 2: y ~ poly(x, 3)
Model 3: y ~ poly(x, 4)
  Res.Df   RSS Df Sum of Sq       F    Pr(&gt;F)    
1     11 52318                                   
2     10 44601  1      7717  8.7782 0.0158868 *  
3      9  7912  1     36689 41.7341 0.0001167 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>EDIT: following a suggestion in comments, I add the residual vs fitted plots for <code>Model2</code>, <code>Model3</code> and <code>Model4</code></p>

<p><a href=""http://i.stack.imgur.com/9ZU8h.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9ZU8h.png"" alt=""enter image description here""></a>` </p>

<p>It's true that the maximum residual error is more or less the same for <code>Model2</code> and <code>Model3</code>, and it becomes nearly one third going from <code>Model3</code> to <code>Model4</code>. There seems to be still some kind of trend in the residuals, though it is less evident than for <code>Model2</code> and <code>Model3</code>. However, why does this invalidate the <em>p</em>-values? Which hypothesis of the linear model paradigm is violated here? I seem to remember that the residuals only had to be uncorrelated with the predictor. However, if they also have to uncorrelated among themselves, then clearly this assumption is violated and the <em>p</em>-values based on the t-test are invalid.</p>
"
"0.0892288262810312","0.0906100470365937","199042","<p>I am using R to find optimal values of Lambda in Box-Cox transformations.
you can find the data I am using here: </p>

<pre>
https://uwyo-files.instructure.com/courses/449832/files/36678098/course%20files/CH03PR15.txt?download=1&inline=1&sf_verifier=8db02990e8a79f78c9ff4418589ec229&ts=1456693144&user_id=569842
</pre>

<p>I have named the data ""C15""</p>

<p>I used typical code to find the SSE for values of lambda in increments of 1 from -2 to positive 2.</p>

<pre><code>C15$CONCENTRATION &lt;- C15$V1
C15$HOURS &lt;- C15$V2
C15 &lt;- C15[,(3:4), drop=F]
attach(C15)
require(MASS)
library(MASS)
hourfit &lt;- lm(CONCENTRATION~HOURS)
bchourfit &lt;- boxcox(hourfit)
#chart indicates somewhere around 0 is the best value for lambda.
C15$concneg2 &lt;- CONCENTRATION^(-2)
    C15$concneg1 &lt;- CONCENTRATION^(-1)
C15$conczero &lt;- CONCENTRATION^0
    C15$concplus1 &lt;- CONCENTRATION^1
C15$concplus2 &lt;- CONCENTRATION^2
attach(C15)
concfitneg2 &lt;- lm(concneg2~HOURS)
concfitneg1 &lt;- lm(concneg1~HOURS)
concfitzero &lt;- lm(conczero~HOURS)
concfitplus1 &lt;- lm(concplus1~HOURS)
concfitplus2 &lt;- lm(concplus2~HOURS)
aov(concfitneg2)
aov(concfitneg1)
aov(concfitzero)
aov(concfitplus1)
aov(concfitplus2)
install.packages(""AID"")
library(""AID"")
boxcoxfr(CONCENTRATION,HOURS)
</code></pre>

<p>From the ANOVA tables you can see that the lowest Sum of Squared error in the residuals comes from a lambda value of 0. However, I now use a function which finds the optimal value of lambda. It now indicates that the optimal value is .14, which is consistent with the log-likelihood graph before. However, when an ANOVA table is generated for the linear model with a transformation of .14, the Sum of Squared error is larger than the model that uses a transformation of zero. </p>

<pre><code>install.packages(""AID"")

library(""AID"")
attach(C15)
boxcoxfr(CONCENTRATION,HOURS)
C15$concopti &lt;- CONCENTRATION^(.14)
attach(C15)
concoptifit &lt;- lm(concopti~HOURS)
aov(concoptifit)
aov(concfitzero)
</code></pre>

<p>Why does the optimal value of Lambda not also give the lowest SSE?</p>
"
"0.138643499732942","0.140789636664653","201105","<p>I'm about to have data from intercept surveys conducted in parks. The goal of the survey is to determine which characteristics of parks users find most important to park quality (do they care a lot about safety, a little about the facilities, and not at all about who else is there?).</p>

<p>We've designed a survey with open-ended questions to answer this question. The current plan is to take down the responses, and then, once we have them, group them into categories (safety, facilities, social environment, accessibility, etc). </p>

<p>For example, one question on the survey asks the user why they came to park. </p>

<p>Each user's response (we're allowing them to list as many reasons as they like, but are asking for primary reasons first, then secondary reasons and so on) will then be associated with some field coding. For one user it might be, say, facilities and park aesthetics, for another it might be easy access. We'll also have some demographic data (age, sex, ethnicity, activity at the park) for each user.</p>

<p><strong>Question 1:</strong> We want to determine which of the categories is most important to users, and if possible, by how much. I've never done any categorical data analysis, and I have no idea what to do here. For some questions we're just going to have counts: 16 people came for facilities, 10 for open spaces, etc.</p>

<p><strong>Question 2:</strong> A separate series of questions asks users to categorize park quality on a Likert-like scale (low to high quality), and also to rate sub-components of park quality in the same way (quality of facilities, from low to high, and so on). We want to determine which predictors have the largest effect on perceived park quality here as well.</p>

<p><strong>I want to know what type of models to fit to our data, and why.</strong></p>

<p>I'm presuming we want some categorical analogue of regression. I want to pick up theoretical underpinnings, learn how to fit models in R, and also how to perform diagnostics on them. </p>

<p>Once I've decided on the appropriate analysis and have picked up the necessary background, I'd like to pre-register my data analysis plan. I've never done this before and am curious what the convention is for this.</p>

<p>Some details about the sample of parks: the city Parks and Recreation department has selected 10 parks for us to visit. Their park selection criteria is not entirely known, but I think they want to visit some well developed and some under developed parks. There are five pairs of parks that the Parks department thinks are comparable. In each pair of parks, one has recently undergone renovation, and the other hasn't.</p>

<p>My background:</p>

<p>I have taken a first course in math-stat, a course on linear regression, and am halfway through a course on experimental design/ANOVA/EM/Bootstrap. I have some pure math, multi, lin-alg and optimization background as well. I have some limited experience in R as well.</p>
"
"0.19966946028644","0.210558721903079","203295","<p>I want to use linear mixed effects models (<code>lme4::lmer</code>) for the selection of features (dependent variables, >1000) on significant differences between specific groups (combinations of independent variables). Therefore I would make a model for each feature (gaining over 1000 models). I then look for significant differences by using contrasts in a posthoc test (<code>multcomp::glht</code>). My question is: given my data does it make sense to use LME models or is a repeated measure ANOVA more appropriate from statistical point of view? And if appropriate which of the models below (if any) should I use? Am I using the right grouping variables and reference levels? </p>

<p>My example data (included at the end of this post) with a representative feature (out of >1000) (<em>Mrkr</em>) looks as follows: The dependent variable (<em>Mrkr</em>) is a continuous variable.</p>

<p>As for the independent variables I have two different species (<em>Spcs</em>: C450, DX20) that undergo two different challenges (<em>Chal</em>: mck, inf). A sample is taken from each subject (<em>Subj</em>: B1 ... B10, D1 ... D10) at different time points (<em>Day</em>: day 0 ... day 30, but differs between species) after the challenge. This let me to believe that I have a random effect for the species since we are following the effect of the challenge trough time on each individual subject.</p>

<p>Because I am firstly interested in the differences between challenges for the same time points for each individual species I constructed a new group (Grp) which is a combination of <em>Chal</em>, <em>Spcs</em> and <em>Day</em> (<em>Grp</em>: I_C_00...M_D_05). </p>

<p>The model I initially used to select features is as follows:</p>

<pre><code>library(""lme4"")
library(""multcomp"")
library(""ggplot2"")

#mydata and K are defined at the end of this post for clarity
mydata$Grp &lt;- relevel(mydata$Grp, ref = ""M_C_03"")   #Set reference level.
mod01 &lt;- lmer(Mrkr ~ Grp + (1|Subj), data = mydata) #Make model with random intercept for Subj.
summary(glht(mod01, linfct = K))                    #Gives the p vals for my specified contrasts (yes, I am aware of the dangers of using p values).

#plots time courses for each individual subject for visualization
ggplot(mydata, aes(x = Day, y = Mrkr)) + 
  geom_point() + 
  geom_line(aes(group = 1)) + 
  facet_wrap(Spcs  + Chal ~  Subj , nrow = 4, ncol = 5) +
  theme(axis.text.x = element_text(angle = 90))
</code></pre>

<p>First I set the benchmark level at the C450/mck/day03 because this is the only time point that all <em>Spcs</em> and <em>Chal</em> combinations have in common. I don't know if this is necessary but intuitively it seemed the right thing to do. If I understand the literature correctly <code>mod01</code> allows for a random intercept for each individual subject. The contrasts were checked with <code>glht()</code> and contrast matrix <code>K</code> (included at the end of this post).</p>

<p>However, after some contemplation and wrestling trough the literature I figured that slopes of the effects might well be dependent on species and challenge and so I came up with the next model:</p>

<pre><code>mod02 &lt;-  lmer(Mrkr ~ Grp + (1 + Spcs|Subj) + (1 + Chal|Subj), data = mydata)
summary(glht(mod02, linfct = K))
</code></pre>

<p>Finally I realized that the subjects are nested in species so my final model would be:</p>

<pre><code>mod03 &lt;-  lmer(Mrkr ~ Grp + (1 + Spcs|Subj/Spcs) + (1 + Chal|Subj/Spcs), data = mydata)
summary(glht(mod03, linfct = K))
</code></pre>

<p>It turns out that the p values for the posthoc tests for all three models are very similar and by using <code>mod01</code> I found already a set of interesting features. However I want to know which model, if any, is the one that describes my random effects structure most accurate. </p>

<p>Kind regards</p>

<pre><code>#Data frame with exampel data
mydata &lt;- structure(list(Subj = structure(c(1L, 2L, 3L, 4L, 5L, 1L, 2L, 
3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 3L, 
4L, 5L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 6L, 7L, 8L, 
9L, 10L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 11L, 12L, 
13L, 14L, 15L, 11L, 12L, 13L, 14L, 15L, 11L, 12L, 13L, 14L, 15L, 
11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 16L, 17L, 18L, 
19L, 20L), .Label = c(""B1"", ""B2"", ""B3"", ""B4"", ""B5"", ""D1"", ""D2"", 
""D3"", ""D4"", ""D5"", ""B6"", ""B7"", ""B8"", ""B9"", ""B10"", ""D6"", ""D7"", 
""D8"", ""D9"", ""D10""), class = c(""ordered"", ""factor"")), Chal = structure(c(2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), .Label = c(""mck"", 
""inf""), class = ""factor""), Day = structure(c(1L, 1L, 1L, 
1L, 1L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 
4L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 1L, 1L, 1L, 1L, 1L, 
2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 2L, 2L, 2L, 2L, 2L, 3L, 
3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 
6L, 6L, 6L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L), .Label = c(""day00"", 
""day03"", ""day05"", ""day08"", ""day18"", ""day30""), class = ""factor""), 
Spcs = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), .Label = c(""C450"", 
""DX20""), class = ""factor""), Grp = structure(c(1L, 1L, 1L, 
1L, 1L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 
4L, 4L, 5L, 5L, 5L, 5L, 5L, 6L, 6L, 6L, 6L, 6L, 7L, 7L, 7L, 
7L, 7L, 8L, 8L, 8L, 8L, 8L, 9L, 9L, 9L, 9L, 9L, 10L, 10L, 
10L, 10L, 10L, 11L, 11L, 11L, 11L, 11L, 12L, 12L, 12L, 12L, 
12L, 13L, 13L, 13L, 13L, 13L, 14L, 14L, 14L, 14L, 14L, 15L, 
15L, 15L, 15L, 15L, 16L, 16L, 16L, 16L, 16L), .Label = c(""I_C_00"", 
""I_C_03"", ""I_C_05"", ""I_C_08"", ""I_C_18"", ""I_C_30"", ""I_D_00"", 
""I_D_03"", ""I_D_05"", ""M_C_03"", ""M_C_05"", ""M_C_08"", ""M_C_18"", 
""M_C_30"", ""M_D_03"", ""M_D_05""), class = ""factor""), Mrkr = c(2399.849218, 
1762.777866, 1774.939084, 1461.419699, 1368.804546, 1126.699114, 
1557.100579, 1369.699809, 2146.155143, 1006.337489, 856.6567507, 
856.775057, 683.6396713, 459.4223325, 651.9368177, 276.29906, 
559.5347751, 294.9815688, 304.0486838, 325.3924639, 814.1927642, 
1424.429248, 949.7589963, 1469.905312, 1319.214754, 1268.595709, 
1184.70564, 870.8718067, 682.4456494, 1177.223394, 512.4325239, 
360.1808537, 525.5669889, 488.9804713, 541.2128606, 1475.036591, 
1132.173062, 1256.048921, 1843.616592, 1892.594627, NA, NA, 
NA, NA, 1100.36921, 1566.125524, 720.1838491, 930.9203894, 
1069.445235, 866.415662, 1021.757551, 1310.491871, 1459.588906, 
1081.572941, 871.4666637, 511.329317, 1010.567794, 513.5011174, 
1005.356367, 734.6804492, 1144.873026, 1467.333437, 1496.635963, 
1519.662963, 1105.464233, 916.0012586, 1248.81632, 591.8699979, 
887.1439846, 610.6604304, 376.610192, 317.2069945, 479.5381028, 
279.0847122, 410.3471923, 491.626902, 331.8743751, 632.6303274, 
588.0827988, 513.1653612)), .Names = c(""Subj"", ""Chal"", ""Day"", 
""Spcs"", ""Grp"", ""Mrkr""), row.names = c(NA, -80L), class = ""data.frame"")

#Contrast matrix for glht()
K &lt;- structure(c(0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 
0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 
0, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 
0, 1, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 
0, 0, 0, 0, 0, 0, 0, 0, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 
0, 0, -1, 0, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1), .Dim = c(11L, 
16L), .Dimnames = list(c(""I_C_00 vs I_C_03"", ""M_C_03 vs I_C_03"", 
""M_C_05 vs I_C_05"", ""M_C_08 vs I_C_08"", ""M_C_18 vs I_C_18"", ""M_C_30 vs I_C_30"", 
""I_D_00 vs I_D_03"", ""M_D_03 vs I_D_03"", ""I_C_00 vs I_D_00"", ""M_C_03 vs M_D_03"", 
""M_C_05 vs M_D_05""), c(""M_C_03"", ""I_C_00"", ""I_C_03"", ""I_C_05"", 
""I_C_08"", ""I_C_18"", ""I_C_30"", ""I_D_00"", ""I_D_03"", ""I_D_05"", ""M_C_05"", 
""M_C_08"", ""M_C_18"", ""M_C_30"", ""M_D_03"", ""M_D_05"")))
</code></pre>
"
"0.0691163516376137","0.0701862406343596","203717","<p>I am trying to do model simplification looking at how different factors may affect distance. So I have snails kept in several habitats and I wanted to see if that affects how closely another snail may follow that snail. So I start off with this model: </p>

<pre><code>  model1 &lt;- lmer(sqrt(dist+6)~  (1|snail)+food+stress+food:stress+
       weight+OriginalL+FollowedL)
summary(model1)
</code></pre>

<p>and the summary is this: </p>

<pre><code>  Linear mixed model fit by REML ['lmerMod']
  Formula: sqrt(dist + 6) ~ (1 | snail) + food + stress + food:stress +  
weight + OriginalL + FollowedL

REML criterion at convergence: 561.1

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.2941 -0.7698 -0.3347  0.7515  1.9564 

Random effects:
 Groups   Name        Variance Std.Dev.
 snail    (Intercept) 0.000    0.000   
 Residual             2.334    1.528   
Number of obs: 148, groups:  snail, 37

Fixed effects:
                               Estimate Std. Error t value
(Intercept)                    4.960927   0.662947   7.483
foodSweetPotato               -0.219039   0.357768  -0.612
stressshelter                 -0.246649   0.355999  -0.693
weight                         0.002520   0.063259   0.040
OriginalL                      0.015549   0.013072   1.189
FollowedL                     -0.008044   0.005972  -1.347
foodSweetPotato:stressshelter -0.300143   0.503215  -0.596

Correlation of Fixed Effects:
            (Intr) fdSwtP strsss weight OrgnlL FllwdL
foodSwetPtt -0.309                                   
stressshltr -0.315  0.502                            
weight      -0.615  0.008  0.009                     
OriginalL   -0.617 -0.021  0.032  0.123              
FollowedL   -0.470  0.118  0.059  0.087 -0.004       
fdSwtPtt:st  0.230 -0.707 -0.708 -0.008 -0.024 -0.055
</code></pre>

<p>Should I remove the least significant factor or remove the interactions first?</p>

<p>And after this is it a simple anova between my first model and most simplified model?</p>
"
"0.126188616281267","0.102513565925132","205151","<p>I am using a generalized Linear Mixed-Effects model to look at the effects of different treatments on a density of trichomes.</p>

<p>The model is :</p>

<pre><code>fitPoisson = glmer(Count_trichomes ~ Treatment1*Treatment2*Treatment3 + 
                         (1 | Block/Code) + offset(log(Length)), family=poisson(), data=dataset)
</code></pre>

<p>Treatment 1 and 2 has 2 levels (0 and 1) and Treatment 3 has 3 levels (0,1,2). Block accounts for the replicates and Code, for each individual. Length is in cm.</p>

<p>An anova(fitPoisson) told me that treatments 1 and 3 are significant and that there is no interactions. What I want now is to know what the density is for each level of treatments.</p>

<p>So I used a lsmeans to look at the differences : </p>

<pre><code>    &gt; lsmeans(fitPoisson, ~ Treatment1)

     Treatment1   lsmean         SE df asymp.LCL asymp.UCL
     0           5.309106 0.06113705 NA  5.189280  5.428933
     1           5.471452 0.06114033 NA  5.351619  5.591285

     Results are averaged over the levels of: Treatment2, Treatment3
     Results are given on the log (not the response) scale. 
     Confidence level used: 0.95
</code></pre>

<p>I can see that the density of level 0 is lower than the density of level 1, but I dont understand what are the units used. It doesn't seems like it is for trichomes/cm, since the mean for level 0 is 107 trichomes/cm and the mean for level 1 is 131 trichomes/cm (calculated in excel).</p>

<p>When I transform back from the log scale, it gives me : </p>

<pre><code>    &gt; summary(lsmeans(fitPoisson, ~ Treatment1), type = ""response"")

     Treatment1   rate       SE df asymp.LCL asymp.UCL
     0           202.1694 12.36004 NA  179.3393  227.9058
     1           237.8053 14.53949 NA  210.9496  268.0799

    Results are averaged over the levels of: Treatment2, Treatment3 
    Confidence level used: 0.95 
    Intervals are back-transformed from the log scale 
</code></pre>

<p>Which is still far from the means I found in excel.</p>

<p>Maybe I just don't understand the information lsmeans is giving me, or I am not using the right function.</p>
"
"NaN","NaN","205227","<p>I'm a little new to R and I haven't done stats in a while. I know a one way ANOVA is the same as a linear regression, but is there a difference between a two way ANOVA and a linear regression with two covariates? And if they are different I'm not sure which one I performed. Below is my sample code:</p>

<pre><code>data.frame[[""Acute""]] = factor(data.frame[[""Acute""]])
data.frame[[""Frequency""]] = factor(data.frame[[""Frequency""]])
DishMortalityVsTime.Total.Acute.Freq = aov(Dish.Mortality ~ Time * Acute * Frequency, data=data.frame)
summary(DishMortalityVsTime.Total.Acute.Freq)
</code></pre>

<p>and the output</p>

<pre><code>                      Df Sum Sq Mean Sq F value               Pr(&gt;F)    
Days                   1  1.352  1.3524  65.189  0.00000000000000429 ***
Acute                  2  5.885  2.9423 141.822 &lt; 0.0000000000000002 ***
Frequency              3  0.539  0.1795   8.653  0.00001279126504853 ***
Days:Acute             2  1.672  0.8361  40.302 &lt; 0.0000000000000002 ***
Days:Frequency         3  0.050  0.0165   0.796                0.496    
Acute:Frequency        6  0.787  0.1311   6.320  0.00000192315201011 ***
Days:Acute:Frequency   6  0.038  0.0064   0.309                0.932    
Residuals            552 11.452  0.0207 
</code></pre>

<p>Any help would be appreciated, Thanks!</p>
"
"0.105576971046183","0.0765794667741282","209939","<p>I would be extremely grateful for some advice on how to correctly fit linear mixed effects models with my repeated measures design!</p>

<p>In my experiment, subjects completed a task with 3 difficulty conditions: easy, medium, and hard. In addition, I have assessed subjects' depressive symptoms on a continuous scale. The outcome measure is accuracy.</p>

<p>""Medium"" here serves as a comparison condition. I hypothesized that depressive symptom severity would moderate the impact of difficulty on accuracy, such that for individuals who are low in depressive symptoms, difficulty would have little impact on accuracy. By contrast, I hypothesized that individuals who are high in depressive symptoms would perform worse during hard rounds and better during easy rounds. Thus, I planned to examine the interaction between difficulty and depressive symptoms.</p>

<p>To accomplish this analysis, my thought was to fit linear mixed effects models using lme4 package in R -- a full model and a reduced model. Then I would implement a likelihood ratio test. I planned to model both random slopes and random intercepts for subjects.</p>

<p>Here's how I would have thought to examine the interaction of a 2-level within-subjects factor (dummy coded) and a centered continuous predictor:</p>

<pre><code>full.model &lt;- lmer(accuracy ~ dummy_difficulty * depression + 
  (1 + dummy_difficulty|subject), REML=FALSE)
reduced.model &lt;- lmer(accuracy ~ dummy_difficulty + depression + 
  (1 + dummy_difficulty|subject), REML=FALSE)
anova(reduced.model, full.model)
</code></pre>

<p>However, my difficulty factor actually has 3 levels.  Since the ""medium"" condition is the comparison condition, I created two dummy variables as follows:</p>

<blockquote>
  <p>dummy_1: easy = 1, medium = -1, hard = 0</p>
  
  <p>dummy_2: easy = 0, medium = -1, hard = 1</p>
</blockquote>

<p>But now, with the two dummy variables, I'm at a loss as to how to model random slopes and random intercepts.  Can anyone help me out?  I would really, really appreciate any advice you might have to offer!</p>
"
"0.126188616281267","0.128141957406415","210001","<p>I have two data sets (<code>base</code> and <code>to_match</code>), each with 10 individuals, grouped in 2 classes. Each individual is described by a set of 4 variables. </p>

<p>What I want to do is: </p>

<ol>
<li>test wether the groups in the first dataset (<code>base</code>) are identical, based on all the describing variables</li>
<li>for each group in the second dataset (<code>to_match</code>) find the best matching group in the first dataset (<code>base</code>). </li>
</ol>

<p>So far, what I did, was to perform a MANOVA within the first dataset, to determine if the groups were identical or not. </p>

<p>Then, I did a Linear Discriminant Analysis on the first group and used the resulting model to predict, for each individual of the second dataset, what was the closest group in the first dataset. </p>

<p>The <code>base</code> dataset looks like this</p>

<pre><code>Group   A   B   C   D
1   0.457713143 -0.961504141    0.569530865 -0.467462304
1   -0.636764605    -0.695107438    0.210832138 -0.602475976
1   -1.216053575    0.647085589 0.42723523  0.024371887
1   -1.143872508    -0.771171997    1.610054266 0.862983524
1   -0.947740051    -0.96552701 -0.528481972    -0.157774001
2   -0.446452415    -0.555949371    -0.392508973    -0.465565853
2   1.143621911 -0.083821489    -1.174028532    -0.307616562
2   -0.118523439    2.250822002 -0.423022806    -0.342627702
2   -0.119453796    2.251860651 -0.460992853    -0.412183789
2   -1.119923882    0.945486343 1.269202026 -1.005019157
</code></pre>

<p>The <code>to_match</code>dataset looks like this:</p>

<pre><code>Group   A   B   C   D
1   0.778450123 -1.245864489    -0.688726943    -0.365538752
1   -1.177318015    0.059801545 0.259885094 0.453012798
1   -1.442516109    -0.422214798    -0.563490254    0.12831251
1   -0.639054602    -0.290063747    -1.249974299    1.473130636
1   -0.334179518    -1.006135106    0.30382184  2.093512163
2   -0.441086171    -0.494222266    -0.346210044    -0.394250031
2   -0.426666213    -0.444327313    -0.350570961    -0.437023047
2   0.382495524 -1.716667725    -1.040363139    0.544599656
2   -0.51828116 -0.757302352    -0.2163689  0.776728601
2   -0.497205151    -0.364979901    -0.632382926    0.222393228
</code></pre>

<p>What I have done so far:</p>

<pre><code>library(MASS)

# Load Datasets
base  &lt;- read.table(""base.txt"", header = T)
to_match  &lt;- read.table(""to_match.txt"", header = T)

# MANOVA analysis
fit &lt;- manova(cbind(A, B, C, D) ~ Group,  base) # Stele was removed
summary(fit)

# LDA analysis
fit &lt;- lda(Group ~ ., data=base, CV=F)

# Predict the class of each individual from the to_match dataset
fit.p &lt;- predict(fit, newdata = to_match[,-1])
</code></pre>

<p>So what I have been able to do so far is to match individuals from the <code>to_match</code>dataset to group in the <code>base</code> dataset using the <code>lda</code> and <code>predict</code> functions from the <code>MASS</code>package. </p>

<p>What I would like to do is <strong>not the match individuals, but the groups directly</strong>. Is this possible? </p>
"
"0.11286652959662","0.114613651012251","210460","<p>the following is the command I used and the results I got for my question, whether the visitation frequency of my bee is different in different experiment types in different locations. I used the lme function of R. I used experiment type as the fixed effect and location as the random effect. I used ANOVA after this to get the F value. Is this right? WIthout ANOVA how I can interpret the results? I had 5 experiment types, but in results it is showing only 4 experiment types. The top most one (expt.antless) is missing.</p>

<pre><code>&gt; names(acera.freq1)
[1] ""expt.type""  ""visit.freq"" ""location""  
&gt; model&lt;-lme(visit.freq~expt.type,random=~1|location,method=""ML"")
&gt; summary(model)
Linear mixed-effects model fit by maximum likelihood
 Data: NULL 
       AIC      BIC    logLik
  1065.928 1087.919 -525.9638

Random effects:
 Formula: ~1 | location
        (Intercept) Residual
StdDev:    4.617241 4.682169

Fixed effects: visit.freq ~ expt.type 
                            Value Std.Error  DF   t-value p-value
(Intercept)             10.192564  1.216852 148  8.376177  0.0000
expt.typeblack.ant      -3.579023  1.074261 148 -3.331615  0.0011
expt.typecrazy.ant      -5.804671  1.740132 148 -3.335765  0.0011
expt.typeother.ants     -5.352438  1.936756 148 -2.763610  0.0064
expt.typered.biting.ant -2.680195  2.048081 148 -1.308637  0.1927
 Correlation: 
                        (Intr) expt.typb. expt.typc. expt.typt.
expt.typeblack.ant      -0.173                                 
expt.typecrazy.ant      -0.128  0.126                          
expt.typeother.ants     -0.081  0.146      0.027               
expt.typered.biting.ant -0.132  0.117      0.143      0.046    

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-1.65727163 -0.70522770 -0.02959964  0.53356588  3.28792891 

Number of Observations: 171
Number of Groups: 19 
&gt; anova(model)
            numDF denDF  F-value p-value
(Intercept)     1   148 56.51744  &lt;.0001
expt.type       4   148  6.28705   1e-04
</code></pre>
"
"0.154548860618483","0.156941205143586","212397","<p>I would like to test the effect of a treatment (""crop"") on species richness. I would rather use a glm for richness as it is a kind of count data.</p>

<p>Besides, I have a nested sampling design (5 values per plot, 5 plot per treatment). Thus I should use a GLMM.</p>

<p>So I write my model :</p>

<pre><code>&gt; GLMM_ric = glmer(richness ~ Crop + (1| Plot),  family=poisson)
&gt; summary(GLMM_ric)

Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [
 glmerMod]
 Family: poisson ( log )
 Formula: richness ~ Crop + (1 | Plot)
 Data: Com_agg

 AIC      BIC   logLik deviance df.resid 
433.8    446.9   -211.9    423.8       95 

Scaled residuals: 
   Min       1Q   Median       3Q      Max 
-1.33174 -0.41445 -0.08382  0.39853  1.73324 

Random effects:
 Groups Name        Variance Std.Dev.
  Plot   (Intercept) 0.08432  0.2904  
 Number of obs: 100, groups: Plot, 20

 Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
 (Intercept)   1.9621     0.1503  13.056   &lt;2e-16 ***
 CropM        -0.5351     0.2211  -2.420   0.0155 *  
 CropYR       -0.3814     0.2181  -1.748   0.0804 .  
 CropOR       -0.3393     0.2175  -1.560   0.1188    
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

 Correlation of Fixed Effects:
        (Intr) CropM  CropYR
 CropM  -0.678              
 CropYR -0.686  0.467       
 CropOR -0.687  0.468  0.475
</code></pre>

<p>and then a simpler model to compare with :</p>

<pre><code> &gt; GLMM_ric0 = glmer(richness ~ (1| Plot), data=Com_agg, family=poisson,    glmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun = 100000)))

 &gt;summary(GLMM_ric0)

 Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) [ glmerMod]
  Family: poisson ( log )
 Formula: richness ~ (1 | Plot)
    Data: Com_agg
 Control: glmerControl(optimizer = ""bobyqa"", optCtrl = list(maxfun = 1e+05))

 AIC      BIC   logLik deviance df.resid 
 433.3    438.5   -214.7    429.3       98 

 Scaled residuals: 
 Min       1Q   Median       3Q      Max 
 -1.27211 -0.39830 -0.03309  0.38204  1.66734 

 Random effects:
  Groups Name        Variance Std.Dev.
  Plot   (Intercept) 0.1251   0.3537  
 Number of obs: 100, groups: Plot, 20

 Fixed effects:
        Estimate Std. Error z value Pr(&gt;|z|)    
 (Intercept)  1.64739    0.09114   18.07   &lt;2e-16 ***
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>And then I compare both models :</p>

<pre><code>&gt; anova(GLMM_ric0, GLMM_ric)
Data: Com_agg
Models:
GLMM_ric0: richness ~ (1 | Plot)
GLMM_ric: richness ~ Crop + (1 | Plot)
              Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)
GLMM_ric0  2 433.32 438.53 -214.66   429.32                         
GLMM_ric   5 433.84 446.86 -211.92   423.84 5.4851      3     0.1395
</code></pre>

<p>So according to my anova, the factor ""crop"" is not significant. Yet in the summary of my model some of the modalities appear to be significant. How should I interpret this ?</p>

<p>I have looked around for a while (e.g. <a href=""http://stats.stackexchange.com/questions/9587/glmm-test-of-significance"">here</a> or <a href=""http://glmm.wikidot.com/faq"" rel=""nofollow"">here</a>) but I could not find much for this precise situation.</p>
"
"0.105576971046183","0.107211253483779","213187","<p>I'm very new to R and stats in general but I think I need to do a repeated measures analysis perhaps using a linear mixed model on my data.</p>

<p>Data are behavioural measurements e.g. distance swum for 21 fish of 2 different genotypes. Each fish was tested for 10 minutes, and data taken for each minute. I've made graphs to show this (B and C).</p>

<p><img src=""http://i.stack.imgur.com/8oKg9.png"" alt=""""></p>

<p>My issue is that I can't do just a normal regression as each of the points is linked over time by the same fish. I've been taught to use linear mixed models such as:</p>

<pre><code>model &lt;- lmer(swimdurbot~1+start+(1+start|file), data=tdautodisc)
model2 &lt;- lmer(swimdurbot~Genotype+start+(1+start|file), data=tdautodisc)
model3 &lt;- lmer(swimdurbot~Genotype+(1+start|file), data=tdautodisc)
model4 &lt;- lmer(swimdurbot~Genotype+start+ Genotype*start +(1+start|file), data=tdautodisc)
model5 &lt;- lmer(swimdurbot~Genotype+start+(1+start|file), data=tdautodisc)
</code></pre>

<p>and to then perform anovas on them but I honestly can't work out what each model is showing, how they are different from each other and which part of the analysis that R comes up with tells me what I want to know.</p>

<p>I want to find out from my data: Does fish behaviour change over 10 mintutes (gradient)? Is the behaviour different between genotypes? and Does genotype affect the change in behaviour over time?</p>

<p>I would really appreciate some help on this as I can't get my head around it, and the graphs don't show any obvious trend so aren't useful for me to guess which part of the analysis relates to which aspect of the graphs.</p>
"
"0.0651635212451075","0.0496291666985465","213244","<p>I would like to check for differences in growth rate between groups. I have three main groups <code>miRs</code> and for each group I have a <code>treatment</code> and a <code>Neg</code>. I want to compare treatment vs Neg for all groups. 
Could someone have a look at my setup?
Why is the variable  <code>mir-135b-5p</code> nor showing up in the results?</p>

<pre><code>&gt; lm &lt;- lme(weight~time*cond, random=~time|miRs, data=testDose)
&gt; anova(lm)
            numDF denDF   F-value p-value
(Intercept)     1    38 233748.85  &lt;.0001
time            1    38    398.12  &lt;.0001
cond            3    38      7.14  0.0006
time:cond       3    38      2.34  0.0887
&gt; summary(lm)
Linear mixed-effects model fit by REML
 Data: testDose 
       AIC      BIC    logLik
  51.94759 72.21414 -13.97379

Random effects:
 Formula: ~time | miRs
 Structure: General positive-definite, Log-Cholesky parametrization
            StdDev       Corr  
(Intercept) 3.279719e-06 (Intr)
time        9.458478e-10 0     
Residual    1.951541e-01       

Fixed effects: weight ~ time * cond 
                        Value  Std.Error DF  t-value p-value
(Intercept)         12.322245 0.16900842 38 72.90906  0.0000
time                 0.021713 0.00257138 38  8.44398  0.0000
condmir-21-3p        0.048565 0.23901400 38  0.20319  0.8401
condmir-584-5p       0.125029 0.23901400 38  0.52310  0.6039
condNeg              0.021297 0.19515412 38  0.10913  0.9137
time:condmir-21-3p  -0.002460 0.00363648 38 -0.67658  0.5028
time:condmir-584-5p -0.006198 0.00363648 38 -1.70431  0.0965
time:condNeg         0.001352 0.00296918 38  0.45549  0.6513
 Correlation: 
                    (Intr) time   c-21-3 c-584- condNg t:-21- t:-584
time                -0.913                                          
condmir-21-3p       -0.707  0.645                                   
condmir-584-5p      -0.707  0.645  0.500                            
condNeg             -0.866  0.791  0.612  0.612                     
time:condmir-21-3p   0.645 -0.707 -0.913 -0.456 -0.559              
time:condmir-584-5p  0.645 -0.707 -0.456 -0.913 -0.559  0.500       
time:condNeg         0.791 -0.866 -0.559 -0.559 -0.913  0.612  0.612

Standardized Within-Group Residuals:
       Min         Q1        Med         Q3        Max 
-1.4935997 -0.5794501 -0.2101814  0.4779404  1.4872837 

Number of Observations: 48
Number of Groups: 3  
</code></pre>

<p>myData</p>

<pre><code>&gt; testDose
     weight time        cond        miRs
1  13.10760   24   mir-21-3p   mir-21-3p
2  13.21659   48   mir-21-3p   mir-21-3p
3  13.59320   72   mir-21-3p   mir-21-3p
4  14.18753   96   mir-21-3p   mir-21-3p
5  13.16919   24         Neg   mir-21-3p
6  13.51846   48         Neg   mir-21-3p
7  14.05866   72         Neg   mir-21-3p
8  14.84804   96         Neg   mir-21-3p
9  12.79820   24   mir-21-3p   mir-21-3p
10 13.12730   48   mir-21-3p   mir-21-3p
11 13.69247   72   mir-21-3p   mir-21-3p
12 14.48472   96   mir-21-3p   mir-21-3p
13 12.81832   24         Neg   mir-21-3p
14 13.17344   48         Neg   mir-21-3p
15 13.78877   72         Neg   mir-21-3p
16 14.44471   96         Neg   mir-21-3p
17 13.11672   24 mir-135b-5p mir-135b-5p
18 13.31708   48 mir-135b-5p mir-135b-5p
19 13.78559   72 mir-135b-5p mir-135b-5p
20 14.54087   96 mir-135b-5p mir-135b-5p
21 13.16919   24         Neg mir-135b-5p
22 13.51846   48         Neg mir-135b-5p
23 14.05866   72         Neg mir-135b-5p
24 14.84804   96         Neg mir-135b-5p
25 12.79218   24 mir-135b-5p mir-135b-5p
26 13.18126   48 mir-135b-5p mir-135b-5p
27 13.78006   72 mir-135b-5p mir-135b-5p
28 14.48629   96 mir-135b-5p mir-135b-5p
29 12.81832   24         Neg mir-135b-5p
30 13.17344   48         Neg mir-135b-5p
31 13.78877   72         Neg mir-135b-5p
32 14.44471   96         Neg mir-135b-5p
33 13.06603   24  mir-584-5p  mir-584-5p
34 13.10624   48  mir-584-5p  mir-584-5p
35 13.27287   72  mir-584-5p  mir-584-5p
36 13.96981   96  mir-584-5p  mir-584-5p
37 13.16919   24         Neg  mir-584-5p
38 13.51846   48         Neg  mir-584-5p
39 14.05866   72         Neg  mir-584-5p
40 14.84804   96         Neg  mir-584-5p
41 12.81925   24  mir-584-5p  mir-584-5p
42 13.03732   48  mir-584-5p  mir-584-5p
43 13.59867   72  mir-584-5p  mir-584-5p
44 14.15521   96  mir-584-5p  mir-584-5p
45 12.81832   24         Neg  mir-584-5p
46 13.17344   48         Neg  mir-584-5p
47 13.78877   72         Neg  mir-584-5p
48 14.44471   96         Neg  mir-584-5p
</code></pre>
"
"0.0814544015563843","0.0827152778309109","213804","<p>I am running some linear regressions in R. I am dealing with a linear dependent and linear as well as categorical independent variables using <a href=""https://stat.ethz.ch/R-manual/R-devel/library/stats/html/lm.html"" rel=""nofollow"">lm</a>. So far, I have looked at the output that <code>summary(model)</code> gives me. </p>

<p>Other studies instead run <a href=""http://www.inside-r.org/packages/cran/car/docs/Anova"" rel=""nofollow"">Anova()</a> from the <a href=""https://cran.r-project.org/web/packages/car/index.html"" rel=""nofollow"">car</a> package on their linear model, which returns a similar table. The docs for <code>Anova()</code> state that it</p>

<blockquote>
  <p>Calculates type-II or type-III analysis-of-variance tables for model objects. </p>
</blockquote>

<p>I am under the impression that this <code>Anova()</code> returns an F instead of the t-statistic but is ~ equivalent in what its tell me. (sample output below). So I was wondering</p>

<ul>
<li><p>Are standard R <code>summary(lm)</code> and car <code>Anova(lm)</code> indeed doing pretty much the same calculations here? If not, what is the difference?</p></li>
<li><p>They both report the same p-value, however the F-statistic at the bottom of the standard output is different from the <code>Anova()</code> one. Why is that?</p></li>
<li><p>What are applications where one would choose one over the other?</p></li>
</ul>

<p>Any help is much appreciated!</p>

<p>Sample output:</p>

<p>Standard R</p>

<pre><code>summary(linreg)
...
         Estimate    t value    Pr(&gt;|t|)
Age      -18.016     -3.917     0.000107
Gender   -45.4912    -4.916     1.35e-06
---
Residual standard error: 85.81 on 359 degrees of freedom
F-statistic: 16.71 on 2 and 359 DF, p-value: 1.147e-07
</code></pre>

<p>Anova() output</p>

<pre><code>Anova(linreg)

Anova Table (Type II tests)

           Sum Sq    F value   Pr (&gt;F)
Age        112997    15.345    0.0001072
Gender     1777936   24.164    1.348e-06
</code></pre>
"
"0.0904945466110142","0.107211253483779","214099","<p>I am a bit confused with a two-way ANOVA that I want to perform in R. I have a mixed linear model that I want to perform an ANOVA for:</p>

<pre><code>fit = lme(response ~ Factor1 * Factor2, data = my_data);
anova(fit);
</code></pre>

<p>Then, I could perform a post-hoc test with Tukey correction as follows:</p>

<pre><code>d = lsmeans(fit, pairwise~Factor1 *  Factor2,
            adjust = ""Tukey"");
</code></pre>

<p>In statistics class, you check that you have to check for normal distribution of <code>response</code> (in each <code>interaction(Factor1, Factor2)</code> and for variance homogeneity for a ""classic"" ANOVA.</p>

<p>Say, I have already checked for normality using q-q plots etc. and am now interested in the homogeneity of variance, e.g. as follows:</p>

<pre><code>bartlett.test(response ~ interaction(Factor1, Factor2),
              data = my_data);
</code></pre>

<p>This gives me something like this:</p>

<pre><code>    Bartlett test of homogeneity of variances

Bartlett's K-squared = 32.186, df = 8, p-value = 8.626e-05
</code></pre>

<p>I have few (10-14) points per <code>interaction(Factor1, Factor2)</code>.</p>

<p>If I understand correctly what I read around ANOVA, this I cannot perform an ANOVA for my data. Is this right? Is there a way around this?</p>

<p>Sorry if this is an easy duplicate of something else, but I could not deduce the answer from the responses I have found here so far.</p>
"
"0.0691163516376137","0.0701862406343596","214613","<p>I am trying to make a simple linear regression to see if my variable ""totalssq"" has an influence on my variable ""hadsa"". (my data is ""dstatss"") Both are quantitative.
I made a model with lm() and tested it with an ANOVA.
Here are the outputs :</p>

<pre><code>    Analysis of Variance Table

    Response: dstatss$hadsa
             Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
      dstatss$totalssq  1  88.272  88.272  5.6848 0.03623 *
     Residuals        11 170.805  15.528                  
     ---
    Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>The p-value is significant, but i don't know what it should mean to me ?
Does it means that there is a significant relationship between my variables ? I don't really know how to interpret this.</p>
"
"0.159617376893524","0.151957668463708","215847","<p>I have no idea how to analyze this dataset.</p>

<p>I am asking if two genotypes, T and M, respond differently to a treatment, E2 (I also have a control, CON). All 36 animals were given both E2 and CON in a counterbalanced order and then their behavior was measured. The behavior was scored once every 30 seconds for 10 minutes as ""yes"" or ""no"" (coded ""1"" or ""0"").</p>

<p>I am interested in knowing if the treatment affects the genotypes differently and whether this effect changes over time.</p>

<p>I have tried running the ANOVA (see below) as a non-parametric test on my count data, but the data are not normally distributed and the results do not make sense.</p>

<pre><code>model&lt;-aov(behavior~genotype*treatment*time+Error(animal/(time*treatment)), data=dataset)
</code></pre>

<p>Therefore, I think that a mixed effects model is right. In this analysis, I think my fixed effects should be genotype and treatment. The random effects should be animal, treatment, and time. I also noted sex and age, but I'm not sure that matters for this analysis. The datafile is set up such that each line is a separate observation for each animal, every 30 seconds. See below:</p>

<pre><code>&gt;animal genotype sex age    treatment   time    behavior                                                                                
&gt;1403   T   F   AHY CON 0   1                                                                               
&gt;1404   T   F   AHY CON 0   1                                                                               
&gt;1406   T   F   HY  CON 0   1                                                                               
&gt;1407   T   F   AHY CON 0   1                                                                               
&gt;1423   T   F   AHY CON 0   1                                                                               
&gt;1425   T   F   HY  CON 0   1                                                                               
&gt;1428   T   F   AHY CON 0   1                                                                               
&gt;1431   T   F   AHY CON 0   1   
</code></pre>

<p>I have tried modeling the data using lme in R but I am not sure that I am nesting the random factors properly because the df for ""genotype"" is 28, but I only have 2 genotypes (so it should be df=1). This is my model:</p>

<pre><code>mixed.model1 &lt;- lme(fixed=behavior~genotype * treatment * time, 
random= ~ 1|animal/time/treatment, data=dataset)
summary(mixed.model1)

Linear mixed-effects model fit by REML
 Data: dataset 
       AIC      BIC    logLik
  2727.064 2779.666 -1351.532

Random effects:
 Formula: ~1 | animal
        (Intercept)
StdDev:    1.345537

 Formula: ~1 | time %in% animal
        (Intercept)
StdDev:   0.5004338

 Formula: ~1 | treatment %in% time %in% animal
        (Intercept)  Residual
StdDev:    2.030743 0.5704242

Fixed effects: behavior ~ genotype * treatment * time 
                           Value Std.Error  DF   t-value p-value
(Intercept)            2.1647059 0.4852943 296  4.460604  0.0000
genotypeT              0.3737557 0.7372150  28  0.506983  0.6161
treatmentE2           -0.3098039 0.4942422 296 -0.626826  0.5313
time                  -0.1465241 0.0578876 268 -2.531184  0.0119
genotypeT:treatmentE2  0.7969834 0.7508078 296  1.061501  0.2893
genotypeT:time        -0.1541752 0.0879375 268 -1.753236  0.0807
treatmentE2:time       0.0124777 0.0796543 296  0.156648  0.8756
genotypeT:treatmentE2:time -0.0367201 0.1210036 296 -0.303463  0.7618
 Correlation: 
                      (Intr) genoT treatE2 time gnW:E2 genoW: trtE2:
genoT                 -0.658                                          
treatE2               -0.509  0.335                                   
time                  -0.656  0.432  0.610                            
genoT:treatE2          0.335 -0.509 -0.658 -0.401                     
genoT:time             0.432 -0.656 -0.401 -0.658  0.610              
treatE2:time           0.451 -0.297 -0.886 -0.688  0.584  0.453       
genoT:treatE2:time    -0.297  0.451  0.584  0.453 -0.886 -0.688 -0.658

Standardized Within-Group Residuals:
        Min          Q1         Med          Q3         Max 
-0.53883607 -0.11769447 -0.03528952  0.04858721  1.83383169 

Number of Observations: 600
Number of Groups: 
 animal        time %in% animal treat %in% time %in% animal
 30                         300                         600 
</code></pre>

<p>I am also concerned that the count data are not normally distributed. Should I use a GLM instead? If so, does that change the random effects? Please help. I have not seen any examples like this in any of the R books I have seen or on this blog.</p>

<p>Thanks in advance!!!</p>
"
"0.0798086884467622","0.0810440898473108","217643","<p>I'm trying to find out if my numeric predictors have a linear relation to the logit of my logistic regression. I tried to use the lrm fit in the rms package where I have used 3 knot cubic spline on all numeric predictors like this:</p>

<pre><code>&gt; fit &lt;- lrm(y ~ rcs(x1,3)+rcs(x2,3)+.....)
</code></pre>

<p>There after I used anova on lrm fit. The main question is how do I use the result in anova(fit)? </p>

<p>My understanding is that the wald statistics are just the associated coefficients squared and dived by it's se. But what about the statistics for nonlinear terms here? are they the wald statistics for the coefficients for the squared predictors? </p>

<p>If none of the statistics are significant, can I conclude that there are no quadratic effect from my predictors?</p>
"
"0.0892288262810312","0.072488037629275","217855","<p>I'm been trying out the anova function in rms. I fit some restricted cubic spline on my continuous IVs in my mode and when I use the anova function on my lrm object I get a list of wald tests. I'm curious as to how the total nonlinear test is obtained?</p>

<p>Example taken from the package itself:</p>

<pre><code>n &lt;- 1000    # define sample size
treat &lt;- factor(sample(c('a','b','c'), n,TRUE))
num.diseases &lt;- sample(0:4, n,TRUE)
age &lt;- rnorm(n, 50, 10)
cholesterol &lt;- rnorm(n, 200, 25)
weight &lt;- rnorm(n, 150, 20)
sex &lt;- factor(sample(c('female','male'), n,TRUE))
label(age) &lt;- 'Age'      # label is in Hmisc
label(num.diseases) &lt;- 'Number of Comorbid Diseases'
label(cholesterol) &lt;- 'Total Cholesterol'
label(weight) &lt;- 'Weight, lbs.'
label(sex) &lt;- 'Sex'
units(cholesterol) &lt;- 'mg/dl'   # uses units.default in Hmisc


# Specify population model for log odds that Y=1
L &lt;- .1*(num.diseases-2) + .045*(age-50) +
 (log(cholesterol - 10)-5.2)*(-2*(treat=='a') +
 3.5*(treat=='b')+2*(treat=='c'))
# Simulate binary y to have Prob(y=1) = 1/[1+exp(-L)]
y &lt;- ifelse(runif(n) &lt; plogis(L), 1, 0)


fit &lt;- lrm(y ~ treat + rcs(age) + rcs(cholesterol))
a &lt;- anova(fit)               
            Wald Statistics          Response: y 
Factor          Chi-Square d.f. P     
treat            4.27       2   0.1185
age             22.30       4   0.0002
Nonlinear       0.64       3   0.8861
cholesterol     12.44       4   0.0144
Nonlinear       6.72       3   0.0814
TOTAL NONLINEAR  7.46       6   0.2804
TOTAL           36.97      10   0.0001
</code></pre>

<p>What is the statistics for the total nonlinear term here? </p>
"
"0.0798086884467622","0.0810440898473108","218251","<p>I am trying to detect evidence of warming in a monthly temperature time series over a 20-year period by testing for a trend. I have precisely followed the method of Crawley (2013) The R Book, 2nd Edition pgs 798-799. In his linear mixed effects model for monthly temperatures he treats the explanatory variables <em>time</em> and linear <em>trend</em> as fixed effects, and <em>year</em> as a categorical random effect allowing for different intercepts for the different years. He then uses ANOVA to compare the full model (with trend explanatory variable) with a reduced version (i.e. without the trend explanatory variable).</p>

<p>A reviewer has questioned why <em>year</em> has been treated as a random effect and suggested that by doing so this would essentially remove a long-term trend. Can anyone clarify why it is correct to include <em>year</em> as a random effect and if by doing so this does or does not remove a trend?     </p>
"
"0.071383061024825","0.0906100470365937","220191","<p>I have run the same two-way Repeated Measure ANOVA in R and in SPSS. Following that I ran a Fisher's Least Significant Difference (LSD).</p>

<p><strong>R script</strong></p>

<pre><code>#The main ANOVA
library(ezANOVA)
anova &lt;- ezANOVA(data = Data, 
                  dv = .(Var),
                  wid = .(Subject), 
                  within = .(Days), 
                  between = .(Group), 
                  detailed = FALSE, 
                  type = 3);
#The post-hoc
attach(Data)
LSD &lt;- pairwise.t.test(Var, Group, p.adj=""none"")
</code></pre>

<p>For SPSS, I used the General Linear Model then Repeated Measure and compared the main effects using the LSD option. </p>

<p><strong>Results</strong>:
The results of the two-way Repeated measure ANOVA is exactly the same for R and SPSS. However, the post-hoc p-value slightly differ. </p>

<p><strong>Questions:</strong> What causes this difference? Does it have to do with how SPSS runs the LSD test. Which LSD test is most reliable?</p>
"
"0.165448994345864","0.176852703025764","220551","<p>I am working on Two-Way ANOVA for an unbalanced design.</p>

<p>Will Tukey multiple comparison be the same for different types (I, II &amp; III) of  Sum Of Squares of ANOVA for an unbalanced design.</p>

<p>I am doing the ANOVA using the <code>car</code> package but the output of the Anova function <code>car</code> package does not work with the <code>TukeyHSD</code> function. The <code>TukeyHSD</code> expects the stats <code>aov</code> output.</p>

<p>I have seen posts suggesting the use of <code>HSD.test</code> from the <code>agricolae</code> package using the linear model <code>lm</code>. But the <code>lm</code> does not take a <code>type</code> argument for the Sum of Squares.</p>

<p>So, does the Tukey comparison be the same for different types of Sum of Squares used for ANOVA?</p>

<p>Below is the sample</p>

<p><strong>code</strong>:: (warpbreaks is a dataset available in R, I am removing 3 rows to make it unbalanced)</p>

<pre><code>library(car)
df &lt;- warpbreaks[-c(1:3), ,]
summary.aov(aov(breaks~wool*tension, df)) # wool: p=.058
car::Anova(aov(breaks~wool*tension, df), type=""II"") # wool: p=.058
car::Anova(aov(breaks~wool*tension, df), type=""III"") # wool: p&lt;.05

TukeyHSD(aov(breaks~wool*tension, df))
</code></pre>

<p><strong>output</strong>::</p>

<pre><code>&gt; summary.aov(aov(breaks~wool*tension, df)) # wool: p=.058
             Df Sum Sq Mean Sq F value   Pr(&gt;F)    
wool          1    327   327.1   2.940 0.093269 .  
tension       2   1954   976.8   8.780 0.000603 ***
wool:tension  2   1257   628.3   5.647 0.006484 ** 
Residuals    45   5006   111.3                     
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; car::Anova(aov(breaks~wool*tension, df), type=""II"") # wool: p=.058
Anova Table (Type II tests)

Response: breaks
             Sum Sq Df F value    Pr(&gt;F)    
wool          476.7  1  4.2847 0.0442255 *  
tension      1953.6  2  8.7800 0.0006034 ***
wool:tension 1256.5  2  5.6472 0.0064836 ** 
Residuals    5006.4 45                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; car::Anova(aov(breaks~wool*tension, df), type=""III"") # wool: p&lt;.05
Anova Table (Type III tests)

Response: breaks
              Sum Sq Df  F value    Pr(&gt;F)    
(Intercept)  14113.5  1 126.8594 1.100e-14 ***
wool          1480.3  1  13.3055 0.0006846 ***
tension       2641.6  2  11.8721 7.236e-05 ***
wool:tension  1256.5  2   5.6472 0.0064836 ** 
Residuals     5006.4 45                       
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
&gt; 
&gt; TukeyHSD(aov(breaks~wool*tension, df))
  Tukey multiple comparisons of means
    95% family-wise confidence level

Fit: aov(formula = breaks ~ wool * tension, data = df)

$wool
         diff       lwr       upr     p adj
B-A -5.074074 -11.03392 0.8857766 0.0932694

$tension
          diff       lwr       upr     p adj
M-L -10.451852 -19.38891 -1.514795 0.0184375
H-L -15.174074 -24.11113 -6.237018 0.0004701
H-M  -4.722222 -13.24337  3.798927 0.3792099

$`wool:tension`
               diff       lwr        upr     p adj
B:L-A:L -20.2777778 -36.82155  -3.734005 0.0084280
A:M-A:L -24.5000000 -41.04377  -7.956227 0.0008625
B:M-A:L -19.7222222 -36.26600  -3.178449 0.0111663
A:H-A:L -23.9444444 -40.48822  -7.400671 0.0011784
B:H-A:L -29.7222222 -46.26600 -13.178449 0.0000407
A:M-B:L  -4.2222222 -19.01942  10.574978 0.9563376
B:M-B:L   0.5555556 -14.24164  15.352756 0.9999974
A:H-B:L  -3.6666667 -18.46387  11.130534 0.9761080
B:H-B:L  -9.4444444 -24.24164   5.352756 0.4157897
B:M-A:M   4.7777778 -10.01942  19.574978 0.9277630
A:H-A:M   0.5555556 -14.24164  15.352756 0.9999974
B:H-A:M  -5.2222222 -20.01942   9.574978 0.8980170
A:H-B:M  -4.2222222 -19.01942  10.574978 0.9563376
B:H-B:M -10.0000000 -24.79720   4.797200 0.3522966
B:H-A:H  -5.7777778 -20.57498   9.019423 0.8521900
</code></pre>

<p>Since for all three types of Sum of Squares (types I, II &amp; III) the Residuals are the same, will the Tukeys comparisons done by using type I ANOVA model with 
<code>TukeyHSD(aov(breaks~wool*tension, df))</code> 
still be valid for type II &amp; III ANOVA's.</p>

<p><strong>Further observation</strong>:
Wool has only two levels A and B so in effect the Tukeys comparison p-value for wool should agree with the ANOVA result.</p>

<p><a href=""http://i.stack.imgur.com/YGmEv.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YGmEv.png"" alt=""p-value for wool from different tests""></a></p>

<p>As can be seen it matches type I and does not match the other two.</p>

<p>So if I have a P-threshold of 0.05, Type II &amp; Type III ANOVA suggest that wool is significant and it do not agree with the Type I ANOVA and the TukeyHSD.</p>

<p>Can it be concluded that Tukey test cannot be done using R for type II and III Sum of Squares?</p>
"
"0.249202549802416","0.253060089439238","220603","<p>I have some measurements (concentration) made in 4 groups (W, X, Y, Z) and time is my covariate. I make a linear model:</p>

<pre><code>fit &lt;- lm(concentration~group*year, data=data)
</code></pre>

<p>The results are as follows: ANOVA table:</p>

<pre><code>anova(fit)

Analysis of Variance Table

Response: concentration
           Df Sum Sq Mean Sq F value    Pr(&gt;F)    
group       3 3600.7 1200.22 32.6132 4.081e-10 *** #!
year        1  559.7  559.71 15.2087 0.0004311 ***
group:year  3   97.3   32.42  0.8809 0.4607155    
Residuals  34 1251.3   36.80                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>and pairwise comparison:</p>

<pre><code>summary(fit)
Call:
lm(formula = concentration ~ group * year, data = data)

Residuals:
   Min     1Q Median     3Q    Max 
-8.818 -4.019 -0.276  4.181 13.097 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  -433.0108   828.4293  -0.523    0.605
groupX      -1574.0090  1170.3741  -1.345    0.188 #!
groupY      -1666.3673  1170.3741  -1.424    0.164 #!
groupZ      -1201.2766  1170.3891  -1.026    0.312 #!
year            0.2418     0.4128   0.586    0.562
groupX:year     0.7937     0.5831   1.361    0.182
groupY:year     0.8409     0.5831   1.442    0.158
groupZ:year     0.6104     0.5831   1.047    0.303

Residual standard error: 6.066 on 34 degrees of freedom
Multiple R-squared:  0.7729,    Adjusted R-squared:  0.7261 
F-statistic: 16.53 on 7 and 34 DF,  p-value: 2.852e-09
</code></pre>

<p>Now I have a problem in the interpretation of this data. As far as I understand, since the interaction in the ANOVA table is nonsignificant, I can check the group effect, and it is significant. This means that the intercept in different groups should be [significantly] different. But when I look to the summary table, there is no significant difference, at least â€“ between group W and others (groupX, groupY and groupZ are nonsignificant). If I change the compared group from W to X or Y or Z the comparison results are still nonsignificant:</p>

<pre><code>data2 &lt;- data
data2$group[data2$group==""X""] &lt;-""A""
fit &lt;- lm(concentration~group*year, data=data2)
summary(fit)

Call:
lm(formula = concentration ~ group * year, data = data2)

Residuals:
   Min     1Q Median     3Q    Max 
-8.818 -4.019 -0.276  4.181 13.097 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept) -2.007e+03  8.267e+02  -2.428   0.0206 *
groupW       1.574e+03  1.170e+03   1.345   0.1876 #! 
groupY      -9.236e+01  1.169e+03  -0.079   0.9375 #! 
groupZ       3.727e+02  1.169e+03   0.319   0.7518 #!
year         1.035e+00  4.119e-01   2.514   0.0168 *
groupW:year -7.937e-01  5.831e-01  -1.361   0.1824  
groupY:year  4.717e-02  5.825e-01   0.081   0.9359  
groupZ:year -1.834e-01  5.825e-01  -0.315   0.7549  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 6.066 on 34 degrees of freedom
Multiple R-squared:  0.7729,    Adjusted R-squared:  0.7261 
F-statistic: 16.53 on 7 and 34 DF,  p-value: 2.852e-09

data2 &lt;- data
data2$group[data2$group==""Y""] &lt;-""A""
fit &lt;- lm(concentration~group*year, data=data2)
summary(fit)

Call:
lm(formula = concentration ~ group * year, data = data2)

Residuals:
   Min     1Q Median     3Q    Max 
-8.818 -4.019 -0.276  4.181 13.097 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept) -2.099e+03  8.267e+02  -2.539   0.0158 *
groupW       1.666e+03  1.170e+03   1.424   0.1636 #! 
groupX       9.236e+01  1.169e+03   0.079   0.9375 #! 
groupZ       4.651e+02  1.169e+03   0.398   0.6933 #! 
year         1.083e+00  4.119e-01   2.628   0.0128 *
groupW:year -8.409e-01  5.831e-01  -1.442   0.1584  
groupX:year -4.717e-02  5.825e-01  -0.081   0.9359  
groupZ:year -2.305e-01  5.825e-01  -0.396   0.6948  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 6.066 on 34 degrees of freedom
Multiple R-squared:  0.7729,    Adjusted R-squared:  0.7261 
F-statistic: 16.53 on 7 and 34 DF,  p-value: 2.852e-09

data2 &lt;- data
data2$group[data2$group==""Z""] &lt;-""A""
fit &lt;- lm(concentration~group*year, data=data2)
summary(fit)

Call:
lm(formula = concentration ~ group * year, data = data2)

Residuals:
   Min     1Q Median     3Q    Max 
-8.818 -4.019 -0.276  4.181 13.097 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)  -433.0108   828.4293  -0.523    0.605
groupX      -1574.0090  1170.3741  -1.345    0.188 #!
groupY      -1666.3673  1170.3741  -1.424    0.164 #!
groupZ      -1201.2766  1170.3891  -1.026    0.312 #!
year            0.2418     0.4128   0.586    0.562
groupX:year     0.7937     0.5831   1.361    0.182
groupY:year     0.8409     0.5831   1.442    0.158
groupZ:year     0.6104     0.5831   1.047    0.303

Residual standard error: 6.066 on 34 degrees of freedom
Multiple R-squared:  0.7729,    Adjusted R-squared:  0.7261 
F-statistic: 16.53 on 7 and 34 DF,  p-value: 2.852e-09
</code></pre>

<p>How is it possible that there are no significant difference between any two groups when there is a significant group effect? Apparently my interpretation that significant group effect means that at least one group differ significantly from other in the intercept value is incorrect. So what is the correct interpretation of the significant group effect?  </p>
"
"0.138232703275227","0.140372481268719","221489","<p>I need to analyse data from an experiment with two within-subjects factors, namely <code>block</code> (<code>1</code>, <code>2</code>, <code>3</code>, <code>4</code>) and <code>IA_LABEL</code> (<code>Label</code> and <code>Ideo</code>). There is one observation per subject for each combination of <code>block</code> by <code>IA_LABEL</code>, and the dependent variable is <code>DWELL_TIME</code>:</p>

<pre><code>&gt; str(data_gr)
'data.frame':   192 obs. of  4 variables:
 $ sbj       : Factor w/ 24 levels ""aggfyt95"",""agkxri94"",..: 1 1 1 1 1 1 1 1 2 2 ...
 $ block     : Factor w/ 4 levels ""1"",""2"",""3"",""4"": 1 1 2 2 3 3 4 4 1 1 ...
 $ IA_LABEL  : Factor w/ 2 levels ""ideog"",""label"": 1 2 1 2 1 2 1 2 1 2 ...
 $ DWELL_TIME: num  781 769 608 757 796 ...
</code></pre>

<p>There is an interaction of the two factors: </p>

<pre><code>&gt; ezANOVA(data=data_gr, dv=.(DWELL_TIME), wid=.(sbj), within=.(IA_LABEL,block), type=3)
$ANOVA
      Effect DFn DFd        F            p p&lt;.05          ges
2       IA_LABEL   1  23 0.310731 0.5826170174       0.0006009409
3          block   3  69 1.054737 0.3740832150       0.0087791282
4 IA_LABEL:block   3  69 7.269528 0.0002626766     * 0.0238067489
</code></pre>

<p>I need to test for each of the four blocks of my experiment if the difference between <code>Label</code> and <code>Ideo</code> is significant (which is, I think, not identical to either <a href=""http://stats.stackexchange.com/questions/49108/post-hoc-test-after-2-factor-repeated-measures-anova-in-r"">this</a>,  nor <a href=""http://stats.stackexchange.com/questions/14078/post-hoc-test-after-anova-with-repeated-measures-using-r/14142#14142"">this</a>).</p>

<p>I re-run the anova with <code>lme</code>, and following the examples provided by the authors of the <a href=""https://cran.r-project.org/web/packages/multcomp/vignettes/multcomp-examples.pdf"" rel=""nofollow""><code>multcomp</code></a>(paragraph 3) package , I created the comparisons of interest.</p>

<p>Here is the code:</p>

<pre><code>mod&lt;-lme(DWELL_TIME ~ IA_LABEL* block, random=list(sbj=pdBlocked(list(~1, pdIdent(~IA_LABEL-1), pdIdent(~block-1)))), data=data_gr)

tmp &lt;- expand.grid(IA_LABEL = unique(data_gr$IA_LABEL), block = unique(data_gr$block))
X &lt;- model.matrix(~ block * IA_LABEL, data =tmp)

Tukey &lt;- contrMat(table(data_gr$IA_LABEL), ""Tukey"")
mat&lt;-matrix(0, nrow = nrow(Tukey), ncol = ncol(Tukey))
K1 &lt;- cbind(Tukey, mat,  mat,  mat)
rownames(K1) &lt;- paste(levels(data_gr$block)[1], rownames(K1), sep = "":"")
K2 &lt;- cbind(mat, Tukey, mat, mat)
rownames(K2) &lt;- paste(levels(data_gr$block)[2], rownames(K2), sep = "":"")
K3 &lt;- cbind(mat,mat, Tukey,mat)
rownames(K3) &lt;- paste(levels(data_gr$block)[3], rownames(K3), sep = "":"")
K4 &lt;- cbind(mat, mat, mat, Tukey)
rownames(K4) &lt;- paste(levels(data_gr$block)[4], rownames(K4), sep = "":"")
K &lt;- rbind(K1, K2, K3, K4)
colnames(K) &lt;- c(colnames(Tukey), colnames(Tukey), colnames(Tukey), colnames(Tukey))

summary(glht(mod, linfct = K %*% X))
</code></pre>

<p>The summary suggests that the tests performed are the required ones:</p>

<pre><code>     Simultaneous Tests for General Linear Hypotheses

Fit: lme.formula(fixed = DWELL_TIME ~ IA_LABEL * block, data = data_gr, 
random = list(sbj = pdBlocked(list(~1, pdIdent(~IA_LABEL - 
    1), pdIdent(~block - 1)))))

Linear Hypotheses:
                     Estimate Std. Error z value Pr(&gt;|z|)  
1:label - ideog == 0   118.27      44.07   2.684   0.0247 *
2:label - ideog == 0   -45.58      55.24  -0.825   0.8073  
3:label - ideog == 0   -83.12      55.24  -1.505   0.3484  
4:label - ideog == 0   -44.26      44.07  -1.004   0.6856  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
(Adjusted p values reported -- single-step method)
</code></pre>

<p>But this method is taken from an example with two between-subjects factors. </p>

<p>So, my question is: Is the method also appropriate/valid for two within-subjects factors?</p>

<p>Thank you in advance for your time,
Fotis</p>
"
"0.16929979439493","0.171920476518376","223439","<p>I am running an analysis on a national sample of 20,000, representative at the province level (34 provinces)</p>

<p>After checking for linearity and normality of my dependent variable I have run a preliminary OLS in order to see how the covariates perform in explaining the variation of the variable of interest. 
I have selected the relevant independent variables following accreditee literature in my field of analysis, explored the covariance matrix in order to avoid problems of multicollinearity etc..</p>

<p>The result from the OLS is good in term of significance level of the coefficients, the sign and the magnitute of the latter fullfil my expectations and match the results find by other analysis.
However, the value of <code>R^2</code> is quite low: only <code>0.09</code> . Thus, knowing that some variation could be explained by the differences between provinces I have first estimate the OLS adding first provincial dummies and after distric dummies (398 districts).</p>

<p>The <code>R^2</code> improved much, reaching respectively the <code>36%</code> and <code>41%</code>.
However, what I would like to see are the underling cause of the regional differences: why do they perform differently? </p>

<p>Among the variables I have some take a unique value according to each observation's province. I cannot use them in the OLS while using the province dummies because there would be perfect collinearity.</p>

<p>In my view using a mixed linear model would help.</p>

<p>I have run a random intercept null.model in which only the dependent variable is regressed against an intercept. For the estimation I have used the command <code>lmer</code> from the <code>{lme4}</code>package in <code>R</code>.</p>

<p>The InterCorrelation Coeffient equals <code>0.30</code>, suggesting that the 30% of the variation happens between groups, the values of the group-mean reliance are quite high too (not less than <code>0.9</code>)- I repeat myself: the sample is province representative.</p>

<p>I finally run a set of random mixed intercept model with 2 levels:</p>

<p>where: <code>i</code> indicates the household and <code>j</code> indicates the province.</p>

<ol>
<li><code>Y_i = beta0j + beta1 X_i + e_ij</code> </li>
<li><code>Y_i = beta0j + beta1 X_i + beta2 Z_j + e_ij</code></li>
<li><code>Y_i = beta0j + beta1 X_i + beta2 W_j + e_ij</code></li>
</ol>

<p>The <strong>first question</strong> is: am I allowed to include a variable which varies only at the provincial level (as Z and W do) given that their coefficient is not computed by taking in cosideration the groups?
As far as I have understood it would be a mistake to use those variables in the random part of the model in order to get random coefficients.</p>

<p>The <strong>second question</strong>: given that by running the command <code>anova()</code>, also in <code>lme4</code>, model 1 is statistically different from model 2, and model 2 is  statistically not different from model 3, can I say that Z and W have the same power in explaing the variation in Y despite the fact that Z's coefficient is significant and W's is not significant?</p>

<p>Think as if Z and W were proxies for the same dimension. They are in fact statistically and conceptually high correlated.</p>

<p>Sorry but I cannot give more details on the actual problem I am woking on.</p>

<p>Thanks in advance. </p>
"
"0.231243506876779","0.247182156712441","223626","<p>In R, I'm wondering how the functions <code>anova()</code> (<code>stats</code> package) and <code>Anova()</code> (<code>car</code> package) differ when being used to compare nested models fit using the <code>glmer()</code> (generalized linear mixed effects model; <code>lme4</code> package) and <code>glm.nb</code> (negative binomial; <code>MASS</code> package) functions. </p>

<p>I've found the two ANOVA functions do not produce the same results for tests of fixed effects in a Poisson mixed model, or a negative binomial fixed effects model (no random effects). Results from both are shown below.</p>

<p><em>My goal</em>: Correctly test the overall significance of a multi-level categorical predictor (fixed; <em>Species</em>). I'm looking for a type III SS-type <em>p</em>-value.</p>

<hr>

<p><em>First</em>: If one fits a <strong>fixed effects</strong> generalized linear model (Poisson here) using <code>glm()</code>, then these two functions <strong>do produce the same results</strong> given the arguments as in the following dummy example:</p>

<pre><code>mod01 &lt;- glm(Count ~ Species + offset(log(Area)), data=data01, family=poisson)

####################
# Anova() function #
####################

library(car)
Anova(mod01, type=3)

# Analysis of Deviance Table (Type III Wald chisquare tests)

# Response: Count
#         LR Chisq Df Pr(&gt;Chisq)    
# Species   255.44  8  &lt; 2.2e-16 ***

####################
# anova() function #
####################

mod01x &lt;- update(mod01, . ~ . - Species)
anova(mod01x, mod01, test=""Chisq"")

# Model 1: Count ~ offset(log(Area))
# Model 2: Count ~ Species + offset(log(Area))

#   Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    
# 1      1063     1456.4                          
# 2      1055     1201.0  8   255.44 &lt; 2.2e-16 ***

# Test statistics are the SAME (255.44) for the fixed effects model
</code></pre>

<hr>

<p><em>However</em>: For a generalized linear <strong>mixed effects</strong> model (using <code>glmer()</code> with random effect for <em>Group</em>), analogous code <strong>gives a different test statistic across the two functions</strong>:</p>

<pre><code>library(lme4)
mod02 &lt;- glmer(Count ~ 1 + Species + (1 | Group) + offset(log(Area)), data=data01, 
               family=poisson(link=""log""), nAGQ=100)

####################
# Anova() function #
####################

Anova(mod02, type=3)

# Analysis of Deviance Table (Type III Wald chisquare tests)

# Response: Count
#                Chisq Df Pr(&gt;Chisq)    
# (Intercept)   4.0029  1    0.04542 *  
# Species     197.9012  8    &lt; 2e-16 ***

####################
# anova() function #
####################

mod02x &lt;- update(mod02, . ~ . - Species)
anova(mod02x, mod02, test=""Chisq"")

# mod02x: Count ~ (1 | Group) + offset(log(Area))
# mod02: Count ~ 1 + Species + (1 | Group) + offset(log(Area))

#        Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
# mod02x  2 1423.9 1433.8 -709.95   1419.9                             
# mod02  10 1191.7 1241.4 -585.85   1171.7 248.21      8  &lt; 2.2e-16 ***

# Now the test statistics are DIFFERENT (197.9012 vs. 248.21)

#####################################################################

# Not a matter of type I vs. III SS since whether the fixed or random
# effect is fit first in the model does not affect results:

# List random effect (Group) before fixed (Species):

mod03 &lt;- glmer(Count ~ 1 + (1 | Group) + Species + offset(log(Area)), data=data01, 
               family=poisson(link=""log""), nAGQ=100)

####################
# Anova() function #
####################

Anova(mod03, type=3)

# Analysis of Deviance Table (Type III Wald chisquare tests)

# Response: Count
#                Chisq Df Pr(&gt;Chisq)    
# (Intercept)   4.0029  1    0.04542 *  
# Species     197.9012  8    &lt; 2e-16 ***

####################
# anova() function #
####################

mod03x &lt;- update(mod03, . ~ . - Species)
anova(mod03x, mod03, test=""Chisq"")

# mod03x: Count ~ (1 | Group) + offset(log(Area))
# mod03: Count ~ 1 + (1 | Group) + Species + offset(log(Area))

#        Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
# mod03x  2 1423.9 1433.8 -709.95   1419.9                             
# mod03  10 1191.7 1241.4 -585.85   1171.7 248.21      8  &lt; 2.2e-16 ***

# Respective test statistics are the same as above case where order of fixed
# and random effects was reversed
</code></pre>

<hr>

<p>Another example of inconsistent test statistics: <strong>Fixed effects negative binomial model</strong>:</p>

<pre><code>library(MASS)
mod04 &lt;- glm.nb(Count ~ Species + offset(log(Area)), data=data01)

####################
# Anova() function #
####################

Anova(mod04, type=3)

# Analysis of Deviance Table (Type III tests)

# Response: Spiders_Tree
#         LR Chisq Df Pr(&gt;Chisq)    
# Species   101.08  8  &lt; 2.2e-16 ***

####################
# anova() function #
####################

mod04x &lt;- update(mod04, . ~ . - Species)
anova(mod04x, mod04)

# Likelihood ratio tests of Negative Binomial Models

# Response: Count
#                            Model     theta Resid. df  2 x log-lik.   Test df LR stat.       Pr(Chi)
# 1           offset(log(Area_M2)) 0.2164382      1063     -1500.688                      
# 2 Species + offset(log(Area_M2)) 0.3488095      1055     -1413.651 1 vs 2  8 87.03677  1.887379e-15 

# Test statistics are also DIFFERENT here (101.08 vs. 87.03677)
</code></pre>

<hr>

<p><em>In summary</em>: The problem:</p>

<ol>
<li>Isn't restricted to only mixed or only fixed effects models</li>
<li>Isn't a matter of type I or III SS, since an example with only one predictor (negative binomial fixed effects model) showed the same problem, and even in the case of more than one predictor (mixed model example), the test is only for the removal of one predictor (<em>Species</em>), so I believe the two types of SS should be equivalent in this case.</li>
</ol>

<p>Could it have to do with the offset? Maybe the functions were written to ""behave well"" with the <code>glm()</code> function, but process others (such as <code>glmer()</code> and <code>glm.nb()</code>) inconsistently? Something else I'm not thinking of?</p>

<hr>

<p>I'm not providing data for my example code above, as I'm assuming someone can comment on the differing theories of each function without a minimal working example. However, if you would like to verify the results really do differ (as shown above), I will add a dummy dataset.</p>
"
"0.0892288262810312","0.0906100470365937","224434","<p>Our experimental design is as follows:</p>

<p>For each of two genotypes (wt and ko), we perform two different gene expression assays (Assay1 and Assay2), and do 4 replicates of each assay. We are interested in knowing if the true proportion of geneA.ko/geneA.wt significantly deviates from 1. However, the catch is that the true abundance we are seeking requires us to normalize the ratio of <em>geneA.ko.Assay1/geneA.wt.Assay1</em> by the ratio <em>geneA.ko.Assay2/geneA.wt.Assay2</em>. In other words, what we want to know is if the following ratio
deviates significantly from 1: <em>(geneA.ko.Assay1/geneA.wt.Assay1) / (geneA.ko.Assay2/geneA.wt.Assay2)</em>. </p>

<p>Here are three sample datasets in R that I hope illustrate the structure of the data:</p>

<pre><code>set.seed(555)
#Not significant, because the ratio of ratios is ~ (500/50)/(10000/1000) = 1
NS = data.frame(geneID=rep(c(""geneA.Rep1"",""geneA.Rep2"",""geneA.Rep3"",""geneA.Rep4""),4),
geno = c(rep(""wt"",4),rep(""ko"",4),rep(""wt"",4),rep(""ko"",4)),
assay = c(rep(""Assay1"",8),rep(""Assay2"",8)),
 intensity = 
   c(rnorm(4,50,5),
    rnorm(4,500,5),
    rnorm(4,1000,5),
    rnorm(4,10000,5)
  )
)

#Significant, because the ratio of ratios is ~ 6
S = data.frame(geneID=rep(c(""geneA.Rep1"",""geneA.Rep2"",""geneA.Rep3"",""geneA.Rep4""),4),
 geno = c(rep(""wt"",4),rep(""ko"",4),rep(""wt"",4),rep(""ko"",4)),
 assay = c(rep(""Assay1"",8),rep(""Assay2"",8)),
 intensity = 
   c(rnorm(4,25,5),
    rnorm(4,150,5),
    rnorm(4,1000,5),
    rnorm(4,1000,5)
  )
)
#Also significant, because the ratio of ratios is ~ 60
S2 = data.frame(geneID=rep(c(""geneA.Rep1"",""geneA.Rep2"",""geneA.Rep3"",""geneA.Rep4""),4),
 geno = c(rep(""wt"",4),rep(""ko"",4),rep(""wt"",4),rep(""ko"",4)),
 assay = c(rep(""Assay1"",8),rep(""Assay2"",8)),
 intensity = 
   c(rnorm(4,25,5),
    rnorm(4,150,5),
    rnorm(4,1000,5),
    rnorm(4,100,5)
  )
)
</code></pre>

<p>Our collaborator recommended using log-linear modeling to determine if the fit to the data is improved by incorporating the Assay2 terms. Unfortunately, I have very little experience with linear modeling beyond basic differential expression analysis. So far I have tried doing the following:</p>

<pre><code>model1 = lm(intensity ~ geno, data = NS, na.action = na.omit)
model2 = lm(intensity ~ geno * assay, data = NS, na.action = na.omit)
anova(model1, model2)
</code></pre>

<p>But the fact that the p-value returned from the anova here is still &lt;&lt;&lt;&lt;&lt; .05 suggests that my model is asking the wrong question. Any insight would be hugely appreciated. I'd be happy to go into more detail on other approaches I've tried if it's useful.</p>
"
"0.179199683424046","0.173702083444913","225241","<p>Consider a mixed model as follows.</p>

<pre><code>library(lme4)
# Load data
data &lt;- structure(list(blk = c(1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3L),
                       gent = c(1, 2, 3, 4, 7, 11, 12, 1, 2, 3, 4, 5, 9, 1, 2, 3, 4, 8, 6, 10L),
                       yld = c(83, 77, 78, 78, 70, 75, 74, 79, 81, 81, 91, 79, 78, 92, 79, 87, 81, 96, 89, 82L),
                       syld = c(250, 240, 268, 287, 226, 395, 450, 260, 220, 237, 227, 281, 311, 258, 224, 238, 278, 347, 300, 289L)),
                  .Names = c(""blk"", ""gent"", ""yld"", ""syld""), class = ""data.frame"", row.names = c(NA, -20L))
data$blk &lt;- as.factor(data$blk)
data$gent &lt;- as.factor(data$gent)
</code></pre>

<p>The data is unbalanced.</p>

<pre><code># Mixed effect model
frmla &lt;- ""syld ~ 1 + gent + (1|blk)""
library(lme4)
model &lt;- lmer(formula(frmla), data = data)

model
Linear mixed model fit by REML ['merModLmerTest']
Formula: syld ~ 1 + gent + (1 | blk)
   Data: data
REML criterion at convergence: 73.9572
Random effects:
 Groups   Name        Std.Dev.
 blk      (Intercept)  9.385  
 Residual             16.919  
Number of obs: 20, groups:  blk, 3
Fixed Effects:
(Intercept)        gent2        gent3        gent4        gent5        gent6        gent7        gent8        gent9  
    256.000      -28.000       -8.333        8.000       32.127       43.678      -36.805       90.678       62.127  
     gent10       gent11       gent12  
     32.678      132.195      187.195  
</code></pre>

<p>Primarily I want to compare the <code>gent</code> levels by LS means.</p>

<pre><code>library(""lmerTest"")
lsmeans(model)
Least Squares Means table:
         gent Estimate Standard Error   DF t-value Lower CI Upper CI p-value    
gent  1   1.0    256.0           11.2  6.9    22.9      229      283  &lt;2e-16 ***
gent  2   5.0    228.0           11.2  6.9    20.4      201      255  &lt;2e-16 ***
gent  3   6.0    247.7           11.2  6.9    22.2      221      274  &lt;2e-16 ***
gent  4   7.0    264.0           11.2  6.9    23.6      237      291  &lt;2e-16 ***
gent  5   8.0    288.1           18.5  8.0    15.6      245      331  &lt;2e-16 ***
gent  6   9.0    299.7           18.5  8.0    16.2      257      342  &lt;2e-16 ***
gent  7  10.0    219.2           18.5  8.0    11.8      177      262  &lt;2e-16 ***
gent  8  11.0    346.7           18.5  8.0    18.8      304      389  &lt;2e-16 ***
gent  9  12.0    318.1           18.5  8.0    17.2      275      361  &lt;2e-16 ***
gent  10  2.0    288.7           18.5  8.0    15.6      246      331  &lt;2e-16 ***
gent  11  3.0    388.2           18.5  8.0    21.0      346      431  &lt;2e-16 ***
gent  12  4.0    443.2           18.5  8.0    24.0      401      486  &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>In addition I am interested in variance partitioning.</p>

<p>The variance component due to random effect and residual can be estimated as follows.</p>

<pre><code>VCrandom &lt;- VarCorr(model)
print(VCrandom, comp = ""Variance"")
 Groups   Name        Variance
 blk      (Intercept)  88.083 
 Residual             286.250
</code></pre>

<p>How to partition the total variance into components due to each of the factors <code>gent</code> and <code>blk</code> along with the residual ? Something similar to the output given by <code>PROC MIXED</code> of <code>SAS</code>, where MSE is computed even when estimation is by ML or REML instead of least squares.</p>

<p>Should I treat the fixed effect as random just for the purpouse of getting variance component ?</p>

<pre><code>frmla2 &lt;- ""syld ~ 1 + (1|gent) + (1|blk)""
model2 &lt;- lmer(formula(frmla2), data = data)
model2

VCrandom2 &lt;- VarCorr(model2)
print(VCrandom2, comp = ""Variance"")
 Groups   Name        Variance
 gent     (Intercept) 4152.08 
 blk      (Intercept)  116.11 
 Residual              274.92 
</code></pre>

<p>If there is no random effect, variance components can be estimated using the least squares approach (ANOVA, Sum of squares, MSE).</p>

<p>The package <code>mixlm</code> has provision for variance partitioning using SS in case of mixed models.</p>

<pre><code>library(mixlm)

mixlm &lt;- lm(syld ~ 1 + r(gent) + r(blk), data)

Anova(mixlm, type=""III"")

Analysis of variance (unrestricted model)
Response: syld
          Mean Sq   Sum Sq Df F value Pr(&gt;F)
gent      5360.49 58965.36 11   18.73 0.0009
blk        638.58  1277.17  2    2.23 0.1886
Residuals  286.25  1717.50  6       -      -

            Err.term(s) Err.df VC(SS)
1 gent              (3)      6 3044.5
2 blk               (3)      6   52.8
3 Residuals           -      -  286.3
(VC = variance component)

               Expected mean squares
gent      (3) + 1.66666666666667 (1)
blk       (3) + 6.66666666666667 (2)
Residuals (3)                       

WARNING: Unbalanced data may lead to poor estimates
</code></pre>

<p>The estimates are different</p>

<pre><code># Total variance
var(data$syld)

|source   |  model1|  model2|  mixlm|
|:--------|-------:|-------:|------:|
|gent     |      NA| 4152.08| 3044.5|
|blk      |  88.083|  116.11|   52.8|
|Residual | 286.250|  274.92|  286.3|
</code></pre>

<p>Can fixed effect variance be extracted using <code>predict</code> function as suggested here <a href=""https://sites.google.com/site/alexandrecourtiol/what-did-i-learn-today/inrhowtoextractthedifferentcomponentsofvarianceinalinearmixedmodel"" rel=""nofollow"">In R: How to extract the different components of variance in a linear mixed model!</a> ?</p>

<pre><code>var(predict(model))
</code></pre>

<p>Which is the most appropriate method compatible with <code>(RE)ML</code> estimates in lme4 ?</p>
"
"0.05643326479831","0.0573068255061253","227073","<p>I have a small perplexity some of you might be able to help me with. 
I have fitted a linear model in R of the form</p>

<pre><code>fullmodel = lm(Y ~ X1 + X2)
</code></pre>

<p>and I want to obtain Likelihood Ratio Tests on the regression coefficients for <code>X1</code> and <code>X2</code>. 
One way to get them is using:</p>

<pre><code>anova(fullmodel, test=""LRT"")
</code></pre>

<p>But, in my understanding, if I use <code>anova</code> on the full model it removes covariates and performs LRT sequentially, indeed results differed depending on ordering of predictors.
<code>drop1</code>, on the other hand, drops one covariate at a time and leaves the rest untouched; thus I could use:</p>

<pre><code>drop1(fullmodel, test=""Chisq"")
</code></pre>

<p>This should work. Yet, out of curiosity, I also tried the following:</p>

<pre><code>fullmodel = lm(Y ~ X1 + X2)
reducedmodel1 = lm(Y ~ X1)
reducedmodel2 = lm(Y ~ X2)

anova(fullmodel, reducedmodel1, test=""LRT"")
anova(fullmodel, reducedmodel2, test=""LRT"")
</code></pre>

<p>In my understanding, the two procedures (<code>drop1</code> and the two separate <code>anova</code>) have identical meaning and should give exactly the same p-values. That's not the case, though; they differ already at the 3rd decimal number. 
Can anyone explain to me why this happens? Am I doing something wrong?</p>
"
"0.138643499732942","0.151619608715781","229722","<p>Thank anyone who look my question. I'm doing a linguistic experiment. I let people in two second language proficiency levels (inter and advanced) and living in two places (city A and B) do a same rating test. The rating test have 6 types of questions, each type have 6 tokens, in total 36 test items for each subject. I also have a native speaker group as control (L1). The picture shows how I code data.
<a href=""http://i.stack.imgur.com/1a2sM.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1a2sM.png"" alt=""enter image description here""></a></p>

<p>The followings are my code.</p>

<pre><code>library(plyr)

# Read data
data = read.csv(""Ba_rang_bei"", header = TRUE)
# Summarise data for table
sum = ddply(.data=data, c(""type"", ""level""), summarise, mean =mean(rating,na.rm=TRUE), sd = sd(rating, na.rm=TRUE))
sum
# Summarise data for analysis
agg = ddply(.data=data, c(""ID"", ""type"", ""level""), summarise, mean = mean(rating, na.rm=TRUE))
# Run anova with 'rating' as the dependent factor, 'type'as a with-subject factor and 'level'as a between-subject factor.
# Include interaction
anova1= aov(mean ~ type*level+Error(ID/type), data = agg)
summary(anova1)
</code></pre>

<p>The result shows:
<a href=""http://i.stack.imgur.com/6kGKI.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6kGKI.png"" alt=""enter image description here""></a><br>
I want to do multiple ANOVA comparisons on subjects' mean rating scores between types and between levels. Like I want to know ""whether A-inter group behave significantly different with the native group on Type A"", ""whether inter group (both in city A and city B) behave significantly different from advaned group (both in city A and city B), ""whether A-inter group's ratings on Type A significantly different on Type B, C, D, E and F.The Tukey HSD test doesn't work in my case, so I have to find an appropriate linear model to do multiple ANOVA comparisons. Can anybody give me any suggestions on building linear models and do multiple ANOVA comparisons? Please help me.</p>
"
"0.183053880989543","0.18588746998031","230734","<p>I've been running GLMMs in the R package ""glmmadmb"" looking at the effects of different sizes of pan trap on the abundance of their catch, using the following code: <code>glmm5 &lt;- glmmadmb(ab$Totalpolls ~ ab$Pan_size+ab$Treatment+log(ab$Nectar+1)+log(ab$Mean.nectar+1)+ab$Max_temp+ab$Season+(1|Year)+(1|Transect), data = ab, zeroInflation = FALSE, family = ""nbinom"")</code></p>

<p>The basic output look like this:</p>

<pre><code>Anova(glmm5)

Analysis of Deviance Table (Type II tests)
Response: ab$Total_polls  Df Chisq Pr(&gt;Chisq)    
ab$Pan_size               3 41.6487  4.763e-09 ***
ab$Treatment              2 14.8347  0.0006007 ***
log(ab$Nectar + 1)        1  8.0988  0.0044295 ** 
log(ab$Mean.nectar + 1)   1  5.0591  0.0244971 *  
ab$Max_temp               1  8.5233  0.0035062 ** 
ab$Season                 4 46.4576  1.978e-09 ***
Residuals               212                       
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>and...</p>

<blockquote>
  <p>summary(glmm5)</p>
</blockquote>

<pre><code>Call:
glmmadmb(formula = ab$Total_polls ~ ab$Pan_size + ab$Treatment + 
log(ab$Nectar + 1) + log(ab$Mean.nectar + 1) + ab$Max_temp + 
ab$Season + (1 | ab$Year) + (1 | ab$Transect), data = ab, 
family = ""nbinom"", zeroInflation = FALSE)

AIC: 855.6 

Coefficients:
                        Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)              -1.1797     0.5312   -2.22  0.02638 *  
ab$Pan_size2              0.5272     0.1587    3.32  0.00089 ***
ab$Pan_size5.5            0.0926     0.1668    0.56  0.57882    
ab$Pan_size12             0.9026     0.1538    5.87  4.4e-09 ***
ab$Treatment24            0.0540     0.1368    0.39  0.69286    
ab$Treatment48            0.4973     0.1291    3.85  0.00012 ***
log(ab$Nectar + 1)        0.0739     0.0260    2.85  0.00443 ** 
log(ab$Mean.nectar + 1)   0.0934     0.0415    2.25  0.02450 *  
ab$Max_temp              -0.0513     0.0176   -2.92  0.00351 ** 
ab$Season5                0.6176     0.2051    3.01  0.00260 ** 
ab$Season6                1.2434     0.2229    5.58  2.4e-08 ***
ab$Season7                0.7909     0.2338    3.38  0.00072 ***
ab$Season8                0.6476     0.3554    1.82  0.06840 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Number of observations: total=228, ab$Year=2, ab$Transect=19 
Random effect variance(s):
Group=ab$Year
             Variance   StdDev
(Intercept) 1.142e-07 0.000338
Group=ab$Transect
            Variance StdDev
(Intercept)  0.01727 0.1314

Negative binomial dispersion parameter: 5.6367 (std. err.: 2.0044)

Log-likelihood: -411.816
</code></pre>

<p>Which shows that pan sizes 12 and 2 are significantly different to size 1, and size 5.5 isn't significantly different at all. However, when I put this model through post hoc tests using the following code: <code>summary(glht(glmm5, lsm(pairwise ~ ab$Pan_size)))</code> (using the glht interface in R package ""Lsmeans"") it gives me this:</p>

<pre><code>     Simultaneous Tests for General Linear Hypotheses

Fit: glmmadmb(formula = ab$Total_polls ~ ab$Treatment + ab$Pan_size + 
log(ab$Nectar + 1) + log(ab$Mean.nectar + 1) + ab$Max_temp + 
ab$Season + (1 | ab$Year) + (1 | ab$Transect), data = ab, 
family = ""nbinom"", zeroInflation = FALSE)

Linear Hypotheses:
              Estimate Std. Error z value Pr(&gt;|z|)    
1 - 2 == 0     -0.1465     0.1239  -1.182  0.62507    
1 - 5.5 == 0   -0.3086     0.1252  -2.464  0.06256 .  
1 - 12 == 0    -0.6975     0.1421  -4.907  &lt; 0.001 ***
2 - 5.5 == 0   -0.1621     0.1063  -1.525  0.40911    
2 - 12 == 0    -0.5510     0.1666  -3.308  0.00498 ** 
5.5 - 12 == 0  -0.3889     0.1327  -2.931  0.01685 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
(Adjusted p values reported -- single-step method)
</code></pre>

<p>Which suggests that size 12 is not better than size 1 and that size two is also not better than size 1; it also indicates that size 5.5 is better than size 1. This all seems to contradict what's in the model summary, which has got me slightly puzzled. All of the other post hoc tests I've run for the other categorical variables in the model have run fine and present results as expected. I've tried changing the position of <code>ab$pan_size</code> in the model, but that doesn't improve things.</p>

<p>Here's a list of the basic code that I'm using:</p>

<pre><code>#pollinator abundance vs. pan trap size and time left active

ab&lt;-read.csv(""Total_polls.csv"")

ab
names(ab)
str(ab)
summary(ab)

# create factors from numerical variables

ab$Year&lt;-as.factor(ab$Year)
ab$Transect&lt;-as.factor(ab$Transect)
ab$Treatment&lt;-as.factor(ab$Treatment)
ab$Pan_size&lt;-as.factor(ab$Pan_size)
ab$Season&lt;-as.factor(ab$Season)

library(glmmADMB)
library(RVAideMemoire)
library(car)

glmm5 &lt;- glmmadmb(ab$Total_polls ~ ab$Treatment+ab$Pan_size+log(ab$Nectar+1)+log(ab$Mean_nectar+1)+ab$Max_temp+ab$Season+(1|ab$Year)+(1|ab$Transect), data = ab, zeroInflation = FALSE, family = ""nbinom"")

Anova(glmm5) #(Package: car)
summary(glmm5) 

# pairwise multiple comparisons tests between multi-level fixed effects (pan_size, treatment (time), and season)

library(multcomp)
library(lsmeans)

glht1 &lt;- (glht(glmm5, lsm(pairwise ~ ab$Pan_size)))
glht2 &lt;- (glht(glmm5, lsm(pairwise ~ ab$Treatment)))
glht3 &lt;- (glht(glmm5, lsm(pairwise ~ ab$Season)))

summary(glht1)
</code></pre>

<p>I'm using R version 3.2.3 (2015-12-10) -- ""Wooden Christmas-Tree"". Could anyone help me to sort this out? I'm not exactly stats savvy, so you may have to be kind with the mathematical language.</p>

<p>Many thanks,
Tom</p>
"
"0.132347737294141","0.134396418749691","230911","<p>I'm not entirely sure of fitting the model for experiment we've made. The variables and relevant description are as follows:</p>

<ul>
<li>ID - participant ID </li>
<li>Trial - 60 for each participant</li>
<li>Memory - between subject binary factor</li>
<li>State - within subject binary factor  </li>
</ul>

<hr>

<ul>
<li>Correct - whether classification a participant made was correct or not</li>
<li>Rating - the judgement made after each trial on four point Likert scale</li>
</ul>

<p>Procedure brief: each participant (N=60) was randomly assigned to experimental or control group (Memory) and had 120 Trials (60 for State = 0 and 60 for State = 1). Each trial composed of perceptual classification (Correct) and judgment of how easy it was (Rating). The classification problem was randomly selected from two groups each trial (State).</p>

<p>I would like to calculate what impacts the performance (Correct) most - is it memory, state, a specific rating on a scale or any combination of above? I'm not interested in between subject variance, on the oposite, it is a random factor here. Also, it appears that there is bias in responses on Likert scales, so that part of variance should be excluded too. </p>

<p>The way I was thinking to approach this is generalized mixed linear model, but I'm not sure I'm doing it right; there is what I've got so far:</p>

<pre><code>model = glmer(Correct ~ (1|ID/Rating) + Memory * State * Rating, data, family=binomial, 
              control = glmerControl(optimizer=""bobyqa"", optCtrl = list(maxfun=100000)))
</code></pre>

<p>Is this approach correct? I'll appreciate your input.</p>

<p>Relevant resources I used: </p>

<ul>
<li><a href=""http://conjugateprior.org/2013/01/formulae-in-r-anova/"" rel=""nofollow"">Formulae in R: ANOVA and other models, mixed and fixed</a></li>
<li><a href=""http://www.theanalysisfactor.com/the-difference-between-crossed-and-nested-factors/"" rel=""nofollow"">The Difference Between Crossed and Nested Factors</a> </li>
<li><a href=""http://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-a-linear-regression-model"">When is it ok to remove the intercept in a linear regression model?</a></li>
<li><a href=""http://stats.stackexchange.com/questions/225198/nested-random-factor-with-confounding-random-variable"">Nested random factor with confounding (random?) variable</a></li>
</ul>
"
"0.119713032670143","0.121566134770966","231258","<p>After using weight of evidence &amp; Information value mechanism, of the 40 odd   variables I am left with 8 variables which are highly or moderately significant.<br>
One of the independent variable which is categorical has 60+ categories. This is a very highly predictable variable hence please suggest as to how should I<br>
use this variable in the model.<br>
When I add this variable in the model my null deviance and AIC decreases   and makes other predictors loose their predictive power.<br>
Then another model without this variable my null deviance and AIC improves.<br>
What could be the reason. Is this variable collinear with some other predictor.   </p>

<p><em>Please see the syntax: &lt; Without that Categorical Var></em>  </p>

<pre><code>m1.logit&lt;- glm(survey ~ region+ know + repS+ und+ case_status, family = binomial(logit), data = a1 )
m1.logit  
summary(m1.logit)

Call:  
glm(formula = survey ~ region + know + repS + und + case_status, 
    family = binomial(logit), data = a1)  

Deviance Residuals:   
     ` Min       1Q   Median    3Q     Max`
    -2.579    0.271   0.290   0.336   2.895    

Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)
    Null deviance: 2553.5  on 2540  degrees of freedom
Residual deviance: 1287.7  on 2526  degrees of freedom
AIC: 1318    
Number of Fisher Scoring iterations: 13
</code></pre>

<p>Also ran an anova test to analyze the table of deviance  </p>

<pre><code>anova(m1.logit, test=""Chisq"")   
Analysis of Deviance Table  

Model: binomial, link: logit  
Response: survey  

Terms added sequentially (first to last)  

             Df Deviance Resid. Df Resid. Dev             Pr(&gt;Chi)     
 NULL                         2540       2554                           
 region       5       13      2535       2540                0.022 *    
 know         1      507      2534       2033 &lt; 0.0000000000000002 ***  
 repS         1      715      2533       1319 &lt; 0.0000000000000002 ***  
 und          1        3      2532       1316                0.109        
 case_status  6       28      2526       1288             0.000078 ***    

 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1  
</code></pre>

<p>Please suggest as to how to deal with this predictor variable with 50+ categories  </p>
"
"0.0399043442233811","0","231770","<p>When modelling and deciding on correct transformation/accounting for non-linearity, how can I decide whether or not for it to be performed?</p>

<p>For instance, say I'm using a restricted cubic spline to model age against mortality (as binary outcome, in log reg). Age should be definitely non-linear, but in my data, non-linearity is insignificant when tested in anova (in R, the rms package's version which is very convenient) although it does add some $\chi$2. I can further try to adjust number of the spline knots or position - but in general - including or excluding a transformation or a version of it based on significance testing feels a bit of overfitting or fishing.</p>

<p>To tl;dr-
For a term I assume to be non-linear, should I always use a non-linear transformation such as rcs(), even if non-significant (and ""wastes"" some df)</p>
"
"0.0892288262810312","0.0906100470365937","234809","<p>The <code>mgcv</code> package for <code>R</code> has two functions for fitting tensor product interactions: <code>te()</code> and <code>ti()</code>. I understand the basic division of labour between the two (fitting a non-linear interaction vs. decomposing this interaction into main effects and an interaction). What I don't understand is why <code>te(x1, x2)</code> and <code>ti(x1) + ti(x2) + ti(x1, x2)</code> may produce (slightly) different results.</p>

<p>MWE (adapted from <code>?ti</code>):</p>

<pre><code>require(mgcv)
test1 &lt;- function(x,z,sx=0.3,sz=0.4) { 
  x &lt;- x*20
 (pi**sx*sz)*(1.2*exp(-(x-0.2)^2/sx^2-(z-0.3)^2/sz^2)+
             0.8*exp(-(x-0.7)^2/sx^2-(z-0.8)^2/sz^2))
}
n &lt;- 500

x &lt;- runif(n)/20;z &lt;- runif(n);
xs &lt;- seq(0,1,length=30)/20;zs &lt;- seq(0,1,length=30)
pr &lt;- data.frame(x=rep(xs,30),z=rep(zs,rep(30,30)))
truth &lt;- matrix(test1(pr$x,pr$z),30,30)
f &lt;- test1(x,z)
y &lt;- f + rnorm(n)*0.2

par(mfrow = c(2,2))

# Model with te()
b2 &lt;- gam(y~te(x,z))
vis.gam(b2, plot.type = ""contour"", color = ""terrain"", main = ""tensor product"")

# Model with ti(a) + ti(b) + ti(a,b)
b3 &lt;- gam(y~ ti(x) + ti(z) + ti(x,z))
vis.gam(b3, plot.type = ""contour"", color = ""terrain"", main = ""tensor anova"")

# Scatterplot of prediction b2/b3
plot(predict(b2), predict(b3))
</code></pre>

<p>The differences aren't very large in this example, but I'm just wondering why there should be differences at all.</p>

<p>Session info:</p>

<pre><code> &gt; devtools::session_info(""mgcv"")
 Session info
 -----------------------------------------------------------------------------------
 setting  value                       
 version  R version 3.3.1 (2016-06-21)
 system   x86_64, linux-gnu           
 ui       RStudio (0.99.491)          
 language en_US                       
 collate  en_US.UTF-8                 
 tz       &lt;NA&gt;                        
 date     2016-09-13                  

 Packages      ---------------------------------------------------------------------------------------
 package * version date       source        
 lattice   0.20-33 2015-07-14 CRAN (R 3.2.1)
 Matrix    1.2-6   2016-05-02 CRAN (R 3.3.0)
 mgcv    * 1.8-12  2016-03-03 CRAN (R 3.2.3)
 nlme    * 3.1-128 2016-05-10 CRAN (R 3.3.1)
</code></pre>
"
"0.187167965030766","0.181425909124269","235168","<p>I'm studying Design and Analysis of Experiments, 8th Edition. Douglas C. Montgomery is the author. I'm trying to replicate the first example he gives in Chapter 13, Experiments with Random Factors.</p>

<p>In this example, there are measurements in a critical dimension on a part. 20 parts are randomly selected and measured by 3 operators, also selected at random. I want to fit two models to this data. The first one I call full model and it is given by</p>

<p>$$y_{ijk} = \mu + \tau_i + \beta_j + (\tau\beta)_{ij} + \varepsilon_{ijk}$$</p>

<p>The other model I call reduced model ant it is given by</p>

<p>$$y_{ijk} = \mu + \tau_i + \beta_j + \varepsilon_{ijk}$$</p>

<p>Both $\tau_i, i=1, \cdots, 20$ and $\beta_j, j=1, 2, 3$ are random effects. The code I'm using to analyze my problem is below:</p>

<pre><code>gauge &lt;- structure(list(part = c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 
11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 1L, 2L, 3L, 
4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 
18L, 19L, 20L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 
12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 1L, 2L, 3L, 4L, 
5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 
19L, 20L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L, 
13L, 14L, 15L, 16L, 17L, 18L, 19L, 20L, 1L, 2L, 3L, 4L, 5L, 6L, 
7L, 8L, 9L, 10L, 11L, 12L, 13L, 14L, 15L, 16L, 17L, 18L, 19L, 
20L), operator = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L), replication = c(1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L), measurement = c(21L, 24L, 20L, 27L, 
19L, 23L, 22L, 19L, 24L, 25L, 21L, 18L, 23L, 24L, 29L, 26L, 20L, 
19L, 25L, 19L, 20L, 23L, 21L, 27L, 18L, 21L, 21L, 17L, 23L, 23L, 
20L, 19L, 25L, 24L, 30L, 26L, 20L, 21L, 26L, 19L, 20L, 24L, 19L, 
28L, 19L, 24L, 22L, 18L, 25L, 26L, 20L, 17L, 25L, 23L, 30L, 25L, 
19L, 19L, 25L, 18L, 20L, 24L, 21L, 26L, 18L, 21L, 24L, 20L, 23L, 
25L, 20L, 19L, 25L, 25L, 28L, 26L, 20L, 19L, 24L, 17L, 19L, 23L, 
20L, 27L, 18L, 23L, 22L, 19L, 24L, 24L, 21L, 18L, 25L, 24L, 31L, 
25L, 20L, 21L, 25L, 19L, 21L, 24L, 22L, 28L, 21L, 22L, 20L, 18L, 
24L, 25L, 20L, 19L, 25L, 25L, 30L, 27L, 20L, 23L, 25L, 17L)), .Names = c(""part"", 
""operator"", ""replication"", ""measurement""), class = ""data.frame"", row.names = c(NA, 
-120L))

###############
# full model
fit.full &lt;- lmer(measurement ~ (1|part) + (1|operator) + (1|part:operator), data=montgomery)
summary(fit.full)
Linear mixed model fit by REML ['lmerMod']
Formula: measurement ~ (1 | part) + (1 | operator) + (1 | part:operator)
   Data: montgomery

REML criterion at convergence: 409.4

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.0313 -0.6595  0.1270  0.5374  2.7345 

Random effects:
 Groups        Name        Variance Std.Dev.
 part:operator (Intercept)  0.00000 0.0000  
 part          (Intercept) 10.25127 3.2018  
 operator      (Intercept)  0.01063 0.1031  
 Residual                   0.88316 0.9398  
Number of obs: 120, groups:  part:operator, 60; part, 20; operator, 3

Fixed effects:
            Estimate Std. Error t value
(Intercept)  22.3917     0.7235   30.95

###############
# reduced model
fit.reduced &lt;- lmer(measurement ~ (1|part) + (1|operator), data=montgomery)
summary(fit.reduced)
Linear mixed model fit by REML ['lmerMod']
Formula: measurement ~ (1 | part) + (1 | operator)
   Data: montgomery

REML criterion at convergence: 409.4

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.0313 -0.6595  0.1270  0.5374  2.7345 

Random effects:
 Groups   Name        Variance Std.Dev.
 part     (Intercept) 10.25127 3.2018  
 operator (Intercept)  0.01063 0.1031  
 Residual              0.88316 0.9398  
Number of obs: 120, groups:  part, 20; operator, 3

Fixed effects:
            Estimate Std. Error t value
(Intercept)  22.3917     0.7235   30.95    
</code></pre>

<p>However, I'm getting different estimates from the ones in the book. Montgomery used Minitab to fit its model and here are his results for the full model:</p>

<p><a href=""http://i.stack.imgur.com/1aeGL.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1aeGL.png"" alt=""Anova Table for the Full Model""></a></p>

<p>They are different from mine. Notice how his <code>part*operator</code> has a negative estimation, while mine is zero. However, his estimates for the reduced model are the same as mine:</p>

<p><a href=""http://i.stack.imgur.com/SaGVu.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/SaGVu.png"" alt=""Anova Table for the Reduced Model""></a></p>

<p>So, my question about his problem are:</p>

<ol>
<li><p>Why our estimates differ for the full model? I understand that I can't have a negative variance like the one he got, but why does Minitab doesn't set it to zero? </p></li>
<li><p>Using R, where (or how) can I get an ANOVA table like the one Minitab presents? I couldn't test my hypothesis in this problem because I can't find the p-values associated with the parameters I'm testing.</p></li>
</ol>
"
