"V1","V2","V3","V4"
"0.0644008186111638","0.102923371199077","  2002","<p>I am running LOESS regression models in R, and I want to compare the outputs of 12 different models with varying sample sizes. I can describe the actual models in more details if it helps with answering the question. </p>

<p>Here are the sample sizes: </p>

<pre><code>Fastballs vs RHH 2008-09: 2002
Fastballs vs LHH 2008-09: 2209
Fastballs vs RHH 2010: 527 
Fastballs vs LHH 2010: 449

Changeups vs RHH 2008-09: 365
Changeups vs LHH 2008-09: 824
Changeups vs RHH 2010: 201
Changeups vs LHH 2010: 330

Curveballs vs RHH 2008-09: 488
Curveballs vs LHH 2008-09: 483
Curveballs vs RHH 2010: 213
Curveballs vs LHH 2010: 162
</code></pre>

<p>The LOESS regression model is a surface fit, where the X location and the Y location of each baseball pitch is used to predict sw, swinging strike probability. However, I'd like to compare between all 12 of these models, but setting the same span (i.e. span = 0.5) will bear different results since there is such a wide range of sample sizes.</p>

<p>My basic question is how do you determine the span of your model? A higher span smooths out the fit more, while a lower span captures more trends but introduces statistical noise if there is too little data. I use a higher span for smaller sample sizes and a lower span for larger sample sizes. </p>

<p>What should I do? What's a good rule of thumb when setting span for LOESS regression models in R? Thanks in advance!</p>
"
"0.135768846660426","0.130188910980824","  3841","<p>I have two years of data which looks basically like this:</p>

<p>Date   <strong><em>_</em>__<em></strong>    Violence Y/N? _</em>  Number of patients</p>

<p>1/1/2008    <strong><em>_</em>___<em></strong>    0  <strong></em>__<em>_</em>__<em>_</em>____</strong> 11</p>

<p>2/1/2008 <strong><em>_</em>__<em>_</em></strong>       0  <strong><em>_</em>__<em>_</em>__<em>_</em>__</strong> 11</p>

<p>3/1/2008 <strong><em>_</em>____</strong><em>1  <strong></em>__<em>_</em>__<em>_</em>____</strong> 12</p>

<p>4/1/2008 <strong><em>_</em>____</strong><em>0  <strong></em>__<em>_</em>__<em>_</em>____</strong> 12</p>

<p>...</p>

<p>31/12/2009_<strong><em>_</em>__</strong>      0_<strong><em>_</em>__<em>_</em>__<em>_</em>__</strong>                 14</p>

<p>i.e. two years of observations, one per day, of a psychiatric ward, which indicate whether there was a violence incident on that day (1 is yes, 0 no) as well as the number of patients on the ward. The hypothesis that we wish to test is that more patients on the ward is associated with an increased probability of violence on the ward.</p>

<p>We realise, of course, that we will have to adjust for the fact that when there are more patients on the ward, violence is more likely because there are just more of them- we are interested in whether each individualâ€™s probability of violence goes up when there are more patients on the ward.</p>

<p>I've seen several papers which just use logistic regression, but I think that is wrong because there is an autoregressive structure (although, looking at the autocorrelation function, it doesnâ€™t get above .1 at any lag, although this is above the â€œsignificantâ€ blue dashed line that R draws for me).</p>

<p>Just to make things more complicated, I can if I wish to break down the results into individual patients, so the data would look just as it does above, except I would have the data for each patient, 1/1/2008, 2/1/2008 etc. and an ID code going down the side so the data would show the whole history of incidents for each patient separately (although not all patients are present for all days, not sure whether that matters).</p>

<p>I would like to use lme4 in R to model the autoregressive structure within each patient, but some Googling comes up with the quotation â€œlme4 is not set up to deal with autoregressive structuresâ€. Even if it were, Iâ€™m not sure I grasp how to write the code anyway.</p>

<p>Just in case anyone notices, I asked a question like this a while ago, they are different datasets with different problems, although actually solving this problem will help with that one (someone suggested I use mixed methods previously, but this autoregression thing has made me unsure how to do this).</p>

<p>So Iâ€™m a bit stuck and lost to be honest. Any help gratefully received!</p>
"
"0.0979827252087026","0.0939557535003536","  5354","<p>I've got some data about airline flights (in a data frame called <code>flights</code>) and I would like to see if the flight time has any effect on the probability of a significantly delayed arrival (meaning 10 or more minutes). I figured I'd use logistic regression, with the flight time as the predictor and whether or not each flight was significantly delayed (a bunch of Bernoullis) as the response. I used the following code...</p>

<pre><code>flights$BigDelay &lt;- flights$ArrDelay &gt;= 10
delay.model &lt;- glm(BigDelay ~ ArrDelay, data=flights, family=binomial(link=""logit""))
summary(delay.model)
</code></pre>

<p>...but got the following output.</p>

<pre><code>&gt; flights$BigDelay &lt;- flights$ArrDelay &gt;= 10
&gt; delay.model &lt;- glm(BigDelay ~ ArrDelay, data=flights, family=binomial(link=""logit""))
Warning messages:
1: In glm.fit(x = X, y = Y, weights = weights, start = start, etastart = etastart,  :
  algorithm did not converge
2: In glm.fit(x = X, y = Y, weights = weights, start = start, etastart = etastart,  :
  fitted probabilities numerically 0 or 1 occurred
&gt; summary(delay.model)

Call:
glm(formula = BigDelay ~ ArrDelay, family = binomial(link = ""logit""),
    data = flights)

Deviance Residuals:
       Min          1Q      Median          3Q         Max
-3.843e-04  -2.107e-08  -2.107e-08   2.107e-08   3.814e-04

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  -312.14     170.26  -1.833   0.0668 .
ArrDelay       32.86      17.92   1.833   0.0668 .
---
Signif. codes:  0 Ã¢***Ã¢ 0.001 Ã¢**Ã¢ 0.01 Ã¢*Ã¢ 0.05 Ã¢.Ã¢ 0.1 Ã¢ Ã¢ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.8375e+06  on 2291292  degrees of freedom
Residual deviance: 9.1675e-03  on 2291291  degrees of freedom
AIC: 4.0092

Number of Fisher Scoring iterations: 25
</code></pre>

<p>What does it mean that the algorithm did not converge? I thought it be because the <code>BigDelay</code> values were <code>TRUE</code> and <code>FALSE</code> instead of <code>0</code> and <code>1</code>, but I got the same error after I converted everything. Any ideas?</p>
"
"0.0678844233302131","0.0650944554904119","  6776","<p>I would appreciate some help getting some EM stuff straight. So, say I generate data in R as follows:</p>

<pre><code>N       &lt;- 100
epsilon &lt;- rnorm(N)
X       &lt;-  10*runif(N)
beta.0  &lt;- 10
beta.1  &lt;-  3
sigma   &lt;- 2
Y       &lt;-  beta.0 + beta.1 * X + sigma * epsilon
epsilon2 &lt;- rnorm(N)
X2 &lt;- 10*runif(N)
Y2 &lt;-  3 - X2 + 0.25 * epsilon2
Y.mix &lt;- c(Y, Y2)
X.mix &lt;- c(X, X2)
</code></pre>

<p>Now, in expectation maximization, in the first step, I have some prior probability, say 0.5, of the data being from either one or the other distribution. So, using EM I know I can estimate the mean and variance of the two mixtures. From looking at a density plot, it seems like the means are at about -2 and 30 for the data I simulated. But, at what stage in EM do I back out the betas? I want to recover the slope, intercept, and sd deviation parameters for the 2 regression-type equations.</p>

<p>Thanks for an clarification.</p>
"
"0.127000127000191","0.121780575111864","  9237","<p>I have several dependent variables that are measures of racial disproportionality; I've calculated them as:</p>

<p>% of events caused by racial minority group / % of events caused by racial majority group</p>

<p>I have a dependent variable for each racial minority group in my sample. I am running longitudinal Generalized Estimating Equations (GEE) on these models, however I am somewhat stumped as to which family is appropriate for these dependent variables. The probability range for my ratios are truncated at 0, as it's not possible to have negative values in my DVs. This makes me question the validity of using a Gaussian family for my models.</p>

<p>The idea behind these variables is that a ratio greater than 1 indicates some level of greater burden of events that a given racial minority is bearing compared to the racial majority, and a ratio less than 1 indicates the opposite.</p>

<ul>
<li>What would be the most appropriate family to use for my GEE regressions?</li>
</ul>

<p>EDIT:</p>

<p>I misspoke about the racial disproportionality measure I was using. The correct formula is:</p>

<p>% events by minority / % of total enrollment that is minority OVER
% events by non-minority / % of total enrollment that is non-minority</p>

<p>Because they are ratios, the number of observations with value less than 1 is comparable to the number of observations greater than 1, with the lower bound being 0 and the upper bound being non-bounded. Looking at the histograms of my response variables, they definitely seem to fit a negative binomial distribution better than the normal. The QIC (GEE adjustment to AIC) confirms this suspicion. My questions now are:</p>

<ul>
<li>Can I trust this evidence to move forward with the negative binomial family?</li>
<li>If so, how do I possibly interpret the exponentiated coefficients from the resulting models? They don't see to be Incidence Rate Ratios, as one would interpret them to be from count variables...</li>
</ul>
"
"0.127000127000191","0.121780575111864"," 11679","<p>I have a nested-case control study that I have been using for analysis. At the end of my work I have deduced a set of variables that I use later to to classify new cases. One example of a simple classifier I am using is a naive Bayes, which will output simply a probability. </p>

<p>So here is my question:</p>

<p>Could I make my probabilities reflect the real world? In my specific example, the condition that I am testing for has a prevalence of 33% in my study, but a it has a population prevalence of only 10%.  Bayes factors have been suggested to me as a way to achieve this, however I am little unsure how to set up the problem. </p>

<p>As an example I have seen a Bayes factor as a logit between the true vs. study prevalence of the outcome. The classifier however was a logistic regression, and in that case the Bayes factor was just added to the linear predictors. I think the example there was very specific, and perhaps an inappropriate method for probabilities of a naive Bayes. Instead what I did was add the logit Bayes factor to the logged probabilities, but I am also not convinced this is right either. I also think a simpler solution would be to use Bayes theorem directly, but there I am not sure how to represented my study vs.population prevalences. The method below isn't quite right, but gets at what I want:</p>

<pre><code>        p_final = classier_posterior*(population_prev)/(study_prev)
</code></pre>

<p>I should contextualize that I use the probabilities to establish a threshold for classification down stream.</p>
"
"0.118797740827873","0.130188910980824"," 12223","<p>I am trying to figure out how to control the smoothing parameters in an mgcv:gam model.</p>

<p>I have a binomial variable I am trying to model as primarily a function of x and y coordinates on a fixed grid, plus some other variables with more minor influences.  In the past I have constructed a reasonably good local regression model using package locfit and just the (x,y) values.  </p>

<p>However, I want to try incorporating the other variables into the model, and it looked like generalized additive models (GAM) were a good possibility.  After looking at packages gam and mgcv, both of which have a GAM function, I opted for the latter since a number of comments in mailing list threads seem to recommend it.  One downside is that it doesn't seem to support a local regression smoother like loess or locfit.</p>

<p>To start, I just wanted to try to replicate approximately the locfit model, using just (x,y) coordinates.  I tried with both regular and tensor product smooths:</p>

<pre><code>my.gam.te &lt;- gam(z ~ te(x, y), family=binomial(logit), data=my.data, scale = -1)

my.gam.s  &lt;- gam(z ~  s(x, y), family=binomial(logit), data=my.data, scale = -1)
</code></pre>

<p>However, plotting the predictions from the model, they are much much more smoothed compared to the locfit model.  So I've been trying to tune the model to not oversmooth as much.  I've tried adjusting the parameters sp and k, but it's not clear to me how they affect the smoothing.  In locfit, the nn parameter controls the span of the neighborhood used, with smaller values allowing for less smoothing and more ""wiggling"", which helps to capture some areas on the grid where the probability of the binomial outcomes changes rapidly.  How would I go about setting up the gam model to enable it to behave similarly?</p>
"
"0.144730076833889","0.152660029954676"," 13172","<p>I would like to use a binary logistic regression model in the context of streaming data (multidimensional time series) in order to predict the value of the dependent variable of the data (i.e. row) that just arrived, given the past observations. As far as I know, logistic regression is traditionally used for postmortem analysis, where each dependent variable has already been set (either by inspection, or by the nature of the study). </p>

<p>What happens in the case of time series though,  where we want to make prediction (on the fly) about the dependent variable in terms of historical data (for example in a time window of the last $t$ seconds) and, of course, the previous estimates of the dependent variable?</p>

<p>And if you see the above system over time, how it should be constructed in order for the regression to work? Do we have to train it first by labeling, let's say, the first 50 rows of our data (i.e. setting the dependent variable to 0 or 1) and then use the current estimate of vector ${\beta}$ to estimate the new probability of the dependent variable being 0 or 1 for the data that just arrived (i.e. the new row that was just added to the system)?</p>

<p>To make my problem more clear, I am trying to build a system that parses a dataset row by row and tries to make prediction of a binary outcome (dependent variable) , given the knowledge (observation or estimation) of all the previous dependent or explanatory variables that have arrived in a fixed time window. My system is in Rerl and uses R for the inference. </p>
"
"0.0678844233302131","0.0650944554904119"," 16346","<p>(Please note the cross-post at <a href=""http://stackoverflow.com/questions/7626347/difference-between-lp-or-simply-in-rs-locfit"">http://stackoverflow.com/questions/7626347/difference-between-lp-or-simply-in-rs-locfit</a>)</p>

<p>I am not sure I see the difference between different examples for local logistic regression in the documentation of the gold standard locfit package for R: <a href=""http://cran.r-project.org/web/packages/locfit/locfit.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/locfit/locfit.pdf</a></p>

<p>I get starkingly different results with</p>

<pre><code>fit2&lt;-scb(closed_rule ~ lp(bl),deg=1,xlim=c(0,1),ev=lfgrid(100), family='binomial',alpha=cbind(0,0.3),kern=""parm"")
</code></pre>

<p>from</p>

<pre><code>fit2&lt;-scb(closed_rule ~ bl,deg=1,xlim=c(0,1),ev=lfgrid(100), family='binomial',alpha=cbind(0,0.3),kern=""parm"")
</code></pre>

<p>.</p>

<p>What is the nature of the difference? Maybe that can help me phrase which I wanted. I had in mind an index linear in bl within a logistic link function predicting the probability of closed_rule. The documentation of lp says that it fits a local polynomial -- which is great, but I thought that would happen even if I leave it out. And in any case, the documentation has examples for ""local logistic regression"" either way...</p>
"
"0.121435348143783","0.145555627434896"," 17480","<p>I've created a few Cox regression models and I would like to see how well these models perform and I thought that perhaps a ROC-curve or a c-statistic might be useful similar to this articles use:</p>

<p><a href=""http://onlinelibrary.wiley.com/doi/10.1002/bjs.6930/abstract"" rel=""nofollow"">J. N. Armitage och J. H. van der Meulen, â€Identifying coâ€morbidity in surgical patients using administrative data with the Royal College of Surgeons Charlson Scoreâ€, British Journal of Surgery, vol. 97, num. 5, ss. 772-781, Maj 2010.</a></p>

<p>Armitage used logistic regression but I wonder if it's possible to use a model from the survival package, the <a href=""http://cran.r-project.org/web/packages/survivalROC/index.html"" rel=""nofollow"">survivalROC</a> gives a hint of this being possible but I can't figure out how to get that to work with a regular Cox regression. </p>

<p>I would be grateful if someone would show me how to do a ROC-analysis on this example:</p>

<pre><code>library(survival)
data(veteran)

attach(veteran)
surv &lt;- Surv(time, status)
fit &lt;- coxph(surv ~ trt + age + prior, data=veteran)
summary(fit)
</code></pre>

<p>If possible I would appreciate both the raw c-statics output and a nice graph</p>

<p>Thanks!</p>

<h2>Update</h2>

<p>Thank you very much for answers. @Dwin: I would just like to be sure that I've understood it right before selecting your answer. </p>

<p>The calculation as I understand it according to DWin's suggestion:</p>

<pre><code>library(survival)
library(rms)
data(veteran)

fit.cph &lt;- cph(surv ~ trt + age + prior, data=veteran, x=TRUE, y=TRUE, surv=TRUE)

# Summary fails!?
#summary(fit.cph)

# Get the Dxy
v &lt;- validate(fit.cph, dxy=TRUE, B=100)
# Is this the correct value?
Dxy = v[rownames(v)==""Dxy"", colnames(v)==""index.corrected""]

# The c-statistic according to the Dxy=2(c-0.5)
Dxy/2+0.5
</code></pre>

<p>I'm unfamiliar with the validate function and bootstrapping but after looking at prof. Frank Harrel's answer <a href=""http://r.789695.n4.nabble.com/Interpreting-the-example-given-by-Prof-Frank-Harrell-in-Design-validate-cph-tt3316820.html#a3324516"" rel=""nofollow"">here on R-help</a> I figured that it's probably the way to get the Dxy. The help for validate states:</p>

<blockquote>
  <p>... Somers' Dxy rank correlation to be computed at each resample (this
  takes a bit longer than the likelihood based statistics). The values
  corresponting to the row Dxy are equal to 2 * (C - 0.5) where C is the
  C-index or concordance probability.</p>
</blockquote>

<p>I guess I'm mostly confused by the columns. I figured that the corrected value is the one I should use but I haven't really understood the validate output:</p>

<pre><code>      index.orig training    test optimism index.corrected   n
Dxy      -0.0137  -0.0715 -0.0071  -0.0644          0.0507 100
R2        0.0079   0.0278  0.0037   0.0242         -0.0162 100
Slope     1.0000   1.0000  0.2939   0.7061          0.2939 100
...
</code></pre>

<p>In the <a href=""http://r.789695.n4.nabble.com/Interpreting-the-example-given-by-Prof-Frank-Harrell-in-Design-validate-cph-tt3316820.html#a3324516"" rel=""nofollow"">R-help question</a> I've understood that I should have ""surv=TRUE"" in the cph if I have strata but I'm uncertain on what the purpose of the ""u=60"" parameter in the validate function is. I would be grateful if you could help me understand these and check that I haven't made any mistakes.</p>
"
"0.118797740827873","0.113915297108221"," 19869","<p>I'm running a predictive model to predict the probability of winning a certain item based on the price that I bid (other factors also). After running the model (ols) in R, I wanted to account for all the variables in my model and develop a graph highlighting the 'predicted probabilities' regarding the primary variables I'm concerned about. So want to have a line graph showing the probability of winning on the y axis, and the bid on the x axis. The following data would result in a graph which shows that the probability of winning decreases as the bid increases.</p>

<pre><code>Bid                  8  6      4
Probability Winning 30% 22%    18%
</code></pre>

<ol>
<li>Are predicted probabilities only relevant for logistic regression models or can be equally relevant for linear regression models?</li>
<li>What is the reasoning and logic behind going from a model to a probability curve which would show the 'trend' in one variable as predicted by another, while accounting for all other factors.</li>
</ol>

<p>Sorry for the elementary question, I'm just a little clueless.
Thanks for the help!</p>
"
"0.107334697685273","0.102923371199077"," 20001","<p>I am trying to cross validate a logistic regression model with probability sampling weights (weights representing number of subjects in the population).  I am not sure how to handle the weights in each of the 'folds' (cross-validation steps).  I don't think it is as simple as leaving out the observations, I believe the weights need to be rescaled at each step.</p>

<p>SAS has an option in proc surveylogistic to get cross validated (leave one out) prediction probabilities.  Unfortunately I cannot find in the documentation any details on how these were calculated.  I would like to reproduce those probabilities in R.  So far I have not had success and am not sure if my approach is correct.  </p>

<p>I hope someone can recommend an appropriate method to do the cross validation with the sampling weights.  If they could match the SAS results that would be great too.</p>

<p>R code for leave-one-out cross validated probabilities (produces error):</p>

<pre><code>library(bootstrap)
library(survey)
fitLogistic = function(x,y){
  tmp=as.data.frame(cbind(y,x))
  dsn=svydesign(ids=~0,weights=wt,data=tmp)
  svyglm(y~x1+x2, 
         data=tmp,family = quasibinomial,design=dsn)
} 
predict.logistic = function(fitLog,x){
  pred.logistic=predict(fitLog,newdata=x,type='response')
  print(pred.logistic)
  ifelse(pred.logistic&gt;=.5,1,0)
} 
CV_Res= crossval(x=data1[,-1], y=data1[,1], fitLogistic, predict.logistic, ngroup = 13)
</code></pre>

<p>Sample Data Set:</p>

<pre><code>y   x1  x2  wt
0   0   1   2479.223
1   0   1   374.7355
1   0   2   1953.4025
1   1   2   1914.0136
0   0   2   2162.8524
1   0   2   491.0571
0   0   1   1842.1192
0   0   1   400.8098
0   1   1   995.5307
0   0   1   955.6634
1   0   2   2260.7749
0   1   1   1707.6085
0   0   2   1969.9993
</code></pre>

<p>SAS proc surveylogistic leave-one-out cross validated probabilities for sample data set:</p>

<p>.0072, 1 .884, .954, ...</p>

<p>SAS Code:</p>

<pre><code>proc surveylogistic;
model y=x1 x2;
weight wt;
output out=a2 predprobs=x;
run;
</code></pre>
"
"0.0960030721474639","0.0920574617898323"," 20645","<p>Possible warning: basic question ahead.</p>

<p>Let's say that I model whether I wear red shoes depending on the weather. Red shoes, which is my dependent variable, is a dichotomous variable as I either wear them or don't. Weather is a variable with five 'levels' and I'm trying to find the probability that I will wear read shoes.</p>

<p>Let's say I model out this relationship with a logistic regression model in R:</p>

<pre><code>mod = glm(shoes ~ weather, data=mydat, family=binomial(link=""logit""))
</code></pre>

<p>Now, what I am interested in is finding what are the probabilities for wearing red shoes for each of the five 'levels' in weather. So perhaps I might have a 20% probability of wearing red shoes when cold, 30% when mild, 40% when warm, and so forth.</p>

<p>I'm wondering if modeling is a requirement for finding this information?
If so, how does one go from somewhat meaningful regression coefficients to meaningful probabilities in R?</p>
"
"0.0720023041105979","0.0920574617898323"," 21067","<p>It's been a while since I've thought about or used a robust logistic regression model. However, I ran a few logits yesterday and realized that my probability curve was being affected by some 'extreme' values, and particularly low ones. However, when I went to run a robust logit model, I got the same results as I did in my logit model.</p>

<p>Under what circumstances should a robust logit produce different results from a traditional logit model? (in terms of coefficients)</p>

<p>R Code:</p>

<pre><code>&gt; library(Design)
&gt; ddist&lt;- datadist(dlmydat)
&gt; options(datadist='ddist')
&gt; me = lrm(factor(dlstatus) ~ dlour_bid, data=dlmydat)
&gt; me

Logistic Regression Model

lrm(formula = factor(dlstatus) ~ dlour_bid, data = dlmydat)


Frequencies of Responses
  1   2 
906 154 

       Obs  Max Deriv Model L.R.       d.f.          P          C        Dxy      Gamma      Tau-a         R2      Brier 
      1060      3e-05     170.11          1          0       0.81      0.619      0.621      0.154      0.263      0.105 

          Coef      S.E.      Wald Z P
Intercept -5.233549 0.3731235 -14.03 0
dlour_bid  0.005367 0.0004925  10.90 0

&gt; library(car)
&gt; dlmod = glm(factor(dlstatus) ~ dlour_bid, data=dlmydat, family=binomial(link=""logit""))
&gt; summary(dlmod)

Call:
glm(formula = factor(dlstatus) ~ dlour_bid, family = binomial(link = ""logit""), 
    data = dlmydat)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.2345  -0.5687  -0.3059  -0.1739   2.6999  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -5.2335492  0.3731235  -14.03   &lt;2e-16 ***
dlour_bid    0.0053667  0.0004925   10.90   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 878.61  on 1059  degrees of freedom
Residual deviance: 708.50  on 1058  degrees of freedom
AIC: 712.5

Number of Fisher Scoring iterations: 6
</code></pre>
"
"0.0858677581482184","0.0823386969592618"," 23248","<p>I have a problem of the form ""what is the probability that a user will 'like' a certain movie?"" For a bunch of users, I know the movies each has watched historically, and the movies each has liked. Additionally, for each movie I know the name of the director.</p>

<p>I calibrated a logistic regression for each user of the form:</p>

<p><code>glm(liked_by_user_1 ~ liked_by_user_2 + ... + liked_by_user_k + factor(director), family=binomial, data = subset(MovieWatchings, user_id == 1))</code></p>

<p>But my problem is: say that in the past, user 1 has watched movies from directors <code>D1</code> through <code>DM</code>, but next month <code>U1</code> watches a movie directed by <code>DN</code>? In that case the R <code>predict()</code> function will give an error, because the glm model for user 1 doesn't have an estimated parameter for the case of <code>director = DN</code>. But I must know something about <code>U1's</code> probability of liking the new movie, because I still know which other users have seen and liked this movie, and that has some predictive power.</p>

<p>How can I set up my model so that I can take into account other users' liking behavior, AND user 1's director preferences, but still have sensible predictions when user 1 sees his first movie from a new director? Is logistic regression even the right type of model for this case?</p>
"
"0.0678844233302131","0.0650944554904119"," 24975","<p>I am working on an assignment involving a logistic regression model, where I need to plot the pearson standardized residuals against one of the predictors. Here's the basic setup:</p>

<pre><code>model &lt;- glm(outcome ~ predictor1 + predictor2, family=binomial(logit))
res &lt;- residuals(model, ""pearson"")
</code></pre>

<p>When looking at the residuals' distribution, I see something totally different than my colleagues who use Stata (using predict and rstandard). Their residuals are more or less normal, whereas in mine there is a gap in the values (not a singe residual is between -0.05 and 1.15). That does make sense in the context of logistic regression, especially that the maximum predicted probability is not so high (38%). </p>

<p>I'd like to understand what's happening here... What is Stata doing that R isn't, with those residuals? </p>
"
"0.0678844233302131","0.0650944554904119"," 26178","<p>I want to create a classification table regarding an ordinal response variable with three levels but I don't know how to do it. Searching on the site I fell on the question posted by Brandon Bertelsen that covers only the case of the binary logistic regression (link at the end of the post).Does anyone knows how I can create such that table in my case?</p>

<p>I don't know if it is important but I used the <code>rms</code> package to run the olr and using the <code>predict(fit,type=""fitted.ind"")</code> command I get the next table with probability for each case</p>

<pre><code>      grade=1    grade=2   grade=3
1  0.08042197 0.28380601 0.6357720
2  0.08086877 0.28475584 0.6343754
3  0.41472656 0.40802584 0.1772476
4  0.39680650 0.41484517 0.1883483
5  0.25402385 0.43644283 0.3095333
6  0.13539881 0.37098177 0.4936194
7  0.12591996 0.35959459 0.5144855
8  0.50489952 0.36489760 0.1302029
9          NA         NA        NA
10 0.34757283 0.42969971 0.2227275
11 0.24690054 0.43539812 0.3177013
12 0.17325212 0.40529586 0.4214520
13 0.45795712 0.38900855 0.1530343
14 0.03594015 0.16033637 0.8037235
15         NA         NA        NA
16 0.50188652 0.36653955 0.1315739
17 0.48710163 0.37441720 0.1384812
18 0.38094725 0.42028884 0.1987639
19 0.04134659 0.17894428 0.7797091
20 0.12844729 0.36275605 0.5087967
21 0.23991274 0.43410413 0.3259831
22 0.20506362 0.42316514 0.3717712
23 0.45457929 0.39061326 0.1548075
24         NA         NA        NA
25 0.31269786 0.43606610 0.2512360
26 0.20905830 0.42483513 0.3661066
27 0.05240710 0.21353381 0.7340591
28 0.26569967 0.43759072 0.2967096
29 0.21258621 0.42621415 0.3611996
30 0.11407246 0.34347156 0.5424560
31 0.34656138 0.42993750 0.2235011
32 0.01813256 0.08978609 0.8920813
33 0.44034224 0.39716470 0.1624931
34 0.12213714 0.35468488 0.5231780
35 0.40888783 0.41032190 0.1807903
36 0.33901842 0.43161582 0.2293658
37 0.13275554 0.36793345 0.4993110
38 0.32091057 0.43492411 0.2441653
39 0.45984161 0.38810515 0.1520532
40 0.55550665 0.33564053 0.1088528
41 0.02812293 0.13122652 0.8406505
42 0.46250424 0.38681892 0.1506768
43 0.07352751 0.26852580 0.6579467
44 0.04330967 0.18541327 0.7712771
45 0.45457929 0.39061326 0.1548075
</code></pre>

<p><a href=""http://stats.stackexchange.com/questions/4832/logistic-regression-classification-tables-a-la-spss-in-r"">Logistic Regression: Classification Tables a la SPSS in R</a></p>
"
"0.0644008186111638","0.0823386969592618"," 26288","<p>I'm trying to understand how to interpret log odds ratios in logistic regression. Let's say I have the following output:</p>

<pre><code>&gt; mod1 = glm(factor(won) ~ bid, data=mydat, family=binomial(link=""logit""))
&gt; summary(mod1)

Call:
glm(formula = factor(won) ~ bid, family = binomial(link = ""logit""), 
    data = mydat)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.5464  -0.6990  -0.6392  -0.5321   2.0124  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -2.133e+00  1.947e-02 -109.53   &lt;2e-16 ***
bid          2.494e-03  5.058e-05   49.32   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 83081  on 80337  degrees of freedom
Residual deviance: 80645  on 80336  degrees of freedom
AIC: 80649

Number of Fisher Scoring iterations: 4
</code></pre>

<p>So my equation would look like:
$$\Pr(Y=1) = \frac{1}{1 + \exp\left(-[-2.13 + 0.002\times(\text{bid})]\right)}$$</p>

<p>From here I calculated probabilities from all bid levels. 
<img src=""http://i.stack.imgur.com/5mLa9.png"" alt=""enter image description here""></p>

<p>I have been using this graph to say that at a 1000 bid, the probability of winning is x. At any given bid level, the probability of winning is x.</p>

<p>I have a feeling that my interpretation is wrong because I'm not considering that these are log-odds. How should I really be interpreting this plot/these results?</p>
"
"0.127000127000191","0.121780575111864"," 26568","<p>I would like to understand how to generate <em>prediction intervals</em> for logistic regression estimates. </p>

<p>I was advised to follow the procedures in Collett's <em>Modelling Binary Data</em>, 2nd Ed p.98-99. After implementing this procedure and comparing it to R's <code>predict.glm</code>, I actually think this book is showing the procedure for computing <em>confidence intervals</em>, not prediction intervals.</p>

<p>Implementation of the procedure from Collett, with a comparison to <code>predict.glm</code>, is shown below.</p>

<p>I would like to know: how do I go from here to producing a prediction interval instead of a confidence interval?</p>

<pre><code>#Derived from Collett 'Modelling Binary Data' 2nd Edition p.98-99
#Need reproducible ""random"" numbers.
seed &lt;- 67

num.students &lt;- 1000
which.student &lt;- 1

#Generate data frame with made-up data from students:
set.seed(seed) #reset seed
v1 &lt;- rbinom(num.students,1,0.7)
v2 &lt;- rnorm(length(v1),0.7,0.3)
v3 &lt;- rpois(length(v1),1)

#Create df representing students
students &lt;- data.frame(
    intercept = rep(1,length(v1)),
    outcome = v1,
    score1 = v2,
    score2 = v3
)
print(head(students))

predict.and.append &lt;- function(input){
    #Create a vanilla logistic model as a function of score1 and score2
    data.model &lt;- glm(outcome ~ score1 + score2, data=input, family=binomial)

    #Calculate predictions and SE.fit with the R package's internal method
    # These are in logits.
    predictions &lt;- as.data.frame(predict(data.model, se.fit=TRUE, type='link'))

    predictions$actual &lt;- input$outcome
    predictions$lower &lt;- plogis(predictions$fit - 1.96 * predictions$se.fit)
    predictions$prediction &lt;- plogis(predictions$fit)
    predictions$upper &lt;- plogis(predictions$fit + 1.96 * predictions$se.fit)


    return (list(data.model, predictions))
}

output &lt;- predict.and.append(students)

data.model &lt;- output[[1]]

#summary(data.model)

#Export vcov matrix 
model.vcov &lt;- vcov(data.model)

# Now our goal is to reproduce 'predictions' and the se.fit manually using the vcov matrix
this.student.predictors &lt;- as.matrix(students[which.student,c(1,3,4)])

#Prediction:
this.student.prediction &lt;- sum(this.student.predictors * coef(data.model))
square.student &lt;- t(this.student.predictors) %*% this.student.predictors
se.student &lt;- sqrt(sum(model.vcov * square.student))

manual.prediction &lt;- data.frame(lower = plogis(this.student.prediction - 1.96*se.student), 
    prediction = plogis(this.student.prediction), 
    upper = plogis(this.student.prediction + 1.96*se.student))

print(""Data preview:"")
print(head(students))
print(paste(""Point estimate of the outcome probability for student"", which.student,""(2.5%, point prediction, 97.5%) by Collett's procedure:""))
manual.prediction
print(paste(""Point estimate of the outcome probability for student"", which.student,""(2.5%, point prediction, 97.5%) by R's predict.glm:""))    
print(output[[2]][which.student,c('lower','prediction','upper')])
</code></pre>
"
"0.083141099321054","0.0797241005179101"," 28819","<p>I have created a multiple linear regression model with R using <code>lm</code> and <code>glm</code>. I am using <code>lm</code> on a training set and <code>predict</code> on a testing set to validate the model. In one test my results are within 80% of what they should be for 80% of the cases. It correlates with 40% for one response variable and with 63% for another response variable (but the response variable with 63% correlation isn't near the actual values of the prediction). I have 53 predicates. What is the probability of that occurring randomly?
I've tried to build an multi-class svm off of the features using the predicates but so far the svm has been unable to properly predict the results.</p>
"
"0.0480015360737319","0.0460287308949162"," 29044","<p>R and Statistics newbie here.</p>

<p>Ok, I have a logistic regression and have used the predict function to develop a probability curve based on my estimates. </p>

<pre><code>## LOGIT MODEL:
library(car)
mod1 = glm(factor(won) ~ as.numeric(bid), data=mydat, family=binomial(link=""logit""))

## PROBABILITY CURVE:
all.x &lt;- expand.grid(won=unique(won), bid=unique(bid))
y.hat.new &lt;- predict(mod1, newdata=all.x, type=""response"")
plot(bid&lt;-000:1000,predict(mod1,newdata=data.frame(bid&lt;-c(000:1000)),type=""response""), lwd=5, col=""blue"", type=""l"")
</code></pre>

<p>This is great but I'm curious about plotting the confidence intervals for the probabilities. I've tried plot.ci() but had no luck. Can anyone point me to some ways to get this done, preferably with the car package or base R.</p>

<p>Thanks.</p>
"
"0.144004608221196","0.138086192684749"," 30061","<p>What approaches exist to observe the time lag between two variables?</p>

<p>I need to analyze the relationship between blood pressure and some other factor, such as exercise. The data set I am drawing from has around 1800 individuals, with an average of 100 entries a piece. It is generally known that there is a strong relationship between exercise level and blood pressure. However, if a person increases their steps to 8000+ a day, how long will it take for their blood pressure to drop? I am new to this type of analysis, and this is a challenge I have been thinking about for weeks. </p>

<p>I don't know if anyone wants to comment on possible approaches to this challenge or any issues surrounding it.</p>

<p>Some issues I have been dealing with:</p>

<ol>
<li><p>Is it better to treat this as a times series analysis or longitudinal data analysis?</p>

<p>My understanding is that time series usually focuses on one variable with no missing data and is observed at consistent intervals, where as longitudinal is over a longer period and has inconsistent time intervals, dropouts, and missing data.</p>

<p>The data I have seems to fit the longitudinal description more, but it also seems like time series could be used if I averaged the values by week so there would be no missing entries. I'm not sure about the pros and cons of each approach.</p></li>
<li><p>Should I be fitting a causal model, or would some other method like regression be more helpful?</p>

<p>I've been looking at various possible causal models, for example Marginal Structural Models (MSM) or Structural Nested Models (SNM), but there seem to be very little information on their application. I did find one R package that applied inverse probability weights and then used Cox proportional hazards regression model on a survival object (MSM), but that seemed to be focus on weighting for confounding and right censoring. Its result was a correlation coefficient, which I don't think helps me.</p>

<p>So I'm not sure if fitting a causal model is what I want, because that seems to be more focused on the making intellectually satisfying assumptions about relationships within the data and then determining the degree of causality, rather than providing information about time lag.</p>

<p>If anyone knows about MSM, SNM, their use in R, or how they might relate to this problem, that would be awesome to hear.</p></li>
<li><p>What about survival analysis or SEM?</p>

<p>I haven't explored these options very in-depth yet but they sound potentially relevant.</p></li>
</ol>

<p>I've kind of stalled, so any hints about what direction I might want to go would be really appreciated. </p>

<p>Thanks in advance.</p>
"
"0.144004608221196","0.138086192684749"," 30451","<p>I'm trying to build a model that would describe some process of payment and distribution of payments in time. I believe that time of payment has <a href=""http://en.wikipedia.org/wiki/L%C3%A9vy_distribution"" rel=""nofollow"">Levy distribution</a> with probability density function:</p>

<p>$ f(x,c)=\sqrt{\frac{c}{2\pi}}~~\frac{e^{ -\frac{c}{2x}}} {x^{3/2}} $</p>

<p>This distribution depends on parameter <em>c</em> which actually defines the shape of distribution. My task is to build model that explains dependency of this parameter on some explaining variables. I'm trying linear dependency $c = \sum_i \beta_i x_i$ </p>

<p>This is example of generalized linear model and is implemented in the <a href=""http://cran.r-project.org/web/packages/VGAM/index.html"" rel=""nofollow"">VGAM package</a> in R. Problem is that in a sample for building this model I have data only from some period at the beginning and this period is different for different groups of cases. And because of that I can not just to run the model in VGAM package on this data as the result will be incorrect significantly exaggerating probability of early payments.</p>

<p>One possible solution I can think about is to change the likelihood function from which parameter is estimated. If we have information only from time up to <em>t</em> and as cumulative distribution function of Levy distribution is:</p>

<p>$ F(x,c)=\textrm{erfc}\left(\sqrt{c/2x}\right) $</p>

<p>the density of distribution up to time <em>t</em>  is  $ f_1(x,c,t)= \frac{f(x,c)}{F(t,c)} $ (where $f(x,c), F(t,c)$ defined as above). This new density functions can be used in estimating regression parameters with maximum likelyhood method. But can it be done in R using methods from VGAM package or usual <strong>glm</strong> function or some other packages? Or there are some better approaches to my problem? I'm interested in implementation in R.</p>

<p>Thank you in advance for any help!</p>
"
"0.0960030721474639","0.0920574617898323"," 31597","<p>I have the following probability function:</p>

<p>$$\text{Prob} = \frac{1}{1 + e^{-z}}$$</p>

<p>where</p>

<p>$$z = B_0 + B_1X_1 + \dots + B_nX_n.$$</p>

<p>My model looks like</p>

<p>$$\Pr(Y=1) = \frac{1}{1 + \exp\left(-[-3.92 + 0.014\times(\text{bid})]\right)}$$</p>

<p>This is visualized via a probability curve which looks like the one below.</p>

<p><img src=""http://i.stack.imgur.com/JjQzf.png"" alt=""enter image description here""></p>

<p>I am considering adding a couple variables to my original regression equation. Let's say I add gender (categorical: F and M) and age (categorical: &lt; 25 and > 26) into the model, I end up with:</p>

<p>$$\Pr(Y=1) = \frac{1}{1 + \exp\left(-[-3.92 + 0.014\times(\text{bid}) + 0.25\times(\text{gender}) + 0.15\times(\text{age})]\right)}$$</p>

<p>In R I can generate a similar probability curve which will tell me the probability of Y=1 when accounting for all three predictors. Where I'm lost is I want to find the probabilities for every possible permutation of these variations.</p>

<p>So when bid = 1,  gender = M, and age is >= 26, what is the probability that Y = 1? Similarly, when bid = 2, gender = F, and age is >= 26, what is the probability that Y = 1?</p>

<p>I want to generate a probability curve which will allow me to visualize this. </p>

<p>Can anyone help? I may be completely misunderstanding what kind of information one can glean from a logit model, but please tell me if I am also misunderstanding the theory.</p>
"
"0.0277136997736847","0.0531494003452734"," 31724","<p>Let's say I have a logistic regression model which predicts whether a consumer will buy an item based on about 10 consumer characteristics. </p>

<p>$$\begin{array}{rcl}Buy &amp;=&amp; B_0 + B_1\times Gender + B_2\times CreditType + B_3\times Education + B_4\times OwnsHome \\\phantom{Buy} &amp;&amp; + B_5\times CarMake + B_6\times CarYear + B_7\times State + B_8\times Income + B_9\times Insurance \\ \phantom{Buy} &amp;&amp;+ B_{10}\times CarAccidents\end{array} $$</p>

<ol>
<li><p>Is there ever an issue with including too many predictors in a logistic regression model? I'm not talking about insignificant variables or ones that may be related, but just the sheer number of variables included in a model. </p></li>
<li><p>With a larger number of predictors, how should one present the regression results in a meaningful manner? Is it just a matter of plotting the probability curve for $Y=1$, or are there ""better"" ways of doing this. I'd be doing this in R, so any help on that end would be appreciated.</p></li>
</ol>
"
"0.083141099321054","0.0797241005179101"," 31735","<p>Related my earlier question <a href=""http://stats.stackexchange.com/questions/31597/graphing-a-probability-curve-for-a-logit-model-with-multiple-predictors"">here</a>. Having ""mastered"" linear regression, I'm trying to learn everything I can about logistic regression and am having issues turning largely ""useless"" coefficients into meaningful information.</p>

<p>I asked in my previous question about graphing the probability curve for every permutation of a logit model. However, I was working on just plotting the main curve and was having some issues. </p>

<p>If I was running a logit with just one predictor, I'd run the following:</p>

<pre><code>mod1 = glm(factor(won) ~ as.numeric(bid), data=mydat, family=binomial(link=""logit""))

all.x &lt;- expand.grid(won=unique(won), bid=unique(bid))
y.hat.new &lt;- predict(mod1, newdata=all.x, type=""response"")
plot(bid&lt;-000:250,predict(mod1,newdata=data.frame(bid&lt;-c(000:250)),type=""response""), lwd=5, col=""blue"", type=""l"")
</code></pre>

<p>However, what about with multiple predictors? I tried to use the mtcars data set to mess around and couldn't get it.</p>

<p>Any suggestion on how to plot the main probability curve for the logit model.</p>

<pre><code>head(mtcars)

m1 = glm(vs ~ disp + wt, data=mtcars, family=binomial(link=""logit""))
summary(m1)

all.x &lt;- expand.grid(vs=unique(mtcars$vs), disp=unique(mtcars$disp), wt=unique(mtcars$wt))

y.hat.new &lt;- predict(m1, newdata=all.x, type=""response"")
plot(disp&lt;-000:250,predict(m1,newdata=data.frame(disp&lt;-c(000:250), wt&lt;-c(0,250)),type=""response""), lwd=5, col=""blue"", type=""l"")
</code></pre>

<p>EDIT = OR is my previous question what I need to do. Since Y = B0 + B1X1 + B2X2, a one unit change in X1 is associated with a exp(B1) change in Y, regardless of the value of B2. Then would it be possible that my original question is really all that should be done. </p>

<p>My appologies on my stupidity if that is indeed the case.</p>
"
"0.138568498868423","0.146160850949502"," 34080","<p>EDIT: I have solved this problem myself. The problem with the simulation below is that the omitted variable should not be included in the 'true model'. I have written a blog post with a more detailed analysis <a href=""http://diffuseprior.wordpress.com/2012/08/15/probit-models-with-endogeneity/"" rel=""nofollow"">here</a>.</p>

<p>I am trying to calculate the Average Structural Function (ASF) for a binary response regression model with an endogenous variable. The ASF is known as the policy relevant result obtained from these models because it shows how the conditional probability of the outcome (one or zero) changes in response to changes in any of the explanatory variables.</p>

<p>To estimate the regression model, I have used a two-step control function approach, wherein the first stage regression residuals ($\textbf{v}_{i}$) are included as a right-hand-side variable in the second stage probit regression Ã  la Rivers and Vuong (1988). </p>

<p>Based on my reading of a paper by Blundell and Powell (2004) (and also <a href=""http://www.cemfi.es/~arellano/binary-endogeneity.pdf"" rel=""nofollow"">these lecture notes</a>) the ASF can be calculated as follows:</p>

<p>$P(y|\bar{\textbf{X}},v)=\widehat{ASF}=\frac{1}{N}\sum^{N}_{i} \Phi(\bar{\textbf{X}}\boldsymbol{\hat{\beta}}+\rho \hat{\textbf{v}_{i}}) $</p>

<p>where the $\textbf{X}$ values are held at a constant level (say their mean), and we average over all of the first-stage residuals (multiplied by the second stage coefficient $\rho$). In effect, this formalization will allow one to calculate how the probability of the outcome varies as the one of the x-variables changes, while all of the other values are (typically) held at their means.</p>

<p>Or so you would think. However, I have attempted this calculation on a simple simulation with R and have not been able to replicate the ASF. My R code is below. Basically, this is a simple setup where we want to measure the effect of y1 on y2 (the binary outcome). There is one omitted variable (x1) that renders y1 endogenous the regression equation of interest.</p>

<p>A picture of my attempt is:</p>

<p><img src=""http://i.stack.imgur.com/OZBA8.jpg"" alt=""enter image description here""></p>

<p>When $x_1$ is available, everything should be fine. Just estimate a standard probit of $y_2$  on $x_1$ and $y_1$. The ASF for this is just the normal CDF for changes in $y_1$. When $x_1$ is not observed, it becomes necessary to instrument $y_1$. </p>

<p>From the IV regression I have calculated the ASF as in the above, and plotted this with comparisons to the model where $x_1$ is observed (the blue line in the picture), and also where $x_1$ is not observed and $y_1$ is not instrumented (the green line).</p>

<p>The red line is my attempt to construct the ASF from the method described in the above. It is clear that this line is not matching the blue line as it should. I have gone wrong somewhere here but I am not sure where. Would somebody be able to help me with this please? </p>

<pre><code>rm(list=ls())
x1 &lt;- rnorm(10000)
x2 &lt;- rnorm(10000)
y1 &lt;- 1 + 0.5*x1 + x2 + rnorm(10000)
y2 &lt;- ifelse(0.5 + 0.5*y1 - 1.5*x1 + rnorm(10000) &gt; 0, 1, 0)

# true
r1 &lt;- glm(y2~y1+x1,binomial(link=""probit""))
data &lt;- data.frame(cbind(seq(-4,6,0.2),mean(x1)))
names(data) &lt;- c(""y1"",""x1"")
asf1 &lt;- cbind(data$y1,pnorm(predict(r1,data)))
plot(asf1,type=""l"",col=""blue"",xlab=""y1"",ylab=""P(y2)"")

# no endog correction
r2 &lt;- glm(y2~y1,binomial(link=""probit""))
data &lt;- data.frame(cbind(seq(-4,6,0.2)))
names(data) &lt;- c(""y1"")
asf2 &lt;- cbind(data$y1,pnorm(predict(r2,data)))
lines(asf2,type=""l"",col=""green"")

# control function approach
v1 &lt;- (residuals(lm(y1~x2)))/sd(residuals(lm(y1~x2)))
r3 &lt;- glm(y2~y1+v1,binomial(link=""probit""))
# proceedure to get asf
asf3 &lt;- cbind(seq(-4,6,0.2),NA)
for(i in 1:dim(asf3)[1]){
    dat2 &lt;- data.frame(cbind(asf3[i,1],v1))
    names(dat2) &lt;- c(""y1"",""v1"")
    asf3[i,2] &lt;- mean(pnorm(predict(r3,dat2)))
}
lines(asf3,type=""l"",col=""red"")
</code></pre>
"
"0.117579270250443","0.112746904200424"," 34263","<p>Last month I asked this question <a href=""http://stats.stackexchange.com/questions/31597/graphing-a-probability-curve-for-a-logit-model-with-multiple-predictors"">here</a>.</p>

<p>After thinking about it recently, I was wondering if it makes sense to think about logit probabilities in that regards. Since the predictor of a coefficient shows the log odds change in the response variable independent of all other predictors, we would expect that plotting bid vs pr(outcome), with the curve representing a different predictor is simply not useful. So if the coefficient for variable x is 0.5, that would be the log odds change regardless of the values for y, z, or f. Therefore, I'm wondering if it makes sense to make such a graph.</p>

<ol>
<li><p>Am I thinking about logistic regression correctly? Since logit coefficients are independent of the other predictors, wouldn't a plot like that be largely ""useless.""</p></li>
<li><p>If that is the case, what should be the main use for predicted probabilities when using logit models?</p></li>
</ol>

<p>Just some sample code if you wish: </p>

<pre><code>df=data.frame(income=c(5,5,3,3,6,5),
              won=c(0,0,1,1,1,0),
              age=c(18,18,23,50,19,39),
              home=c(0,0,1,0,0,1))
str(df)

md1 = glm(factor(won) ~ income + age + home, 
          data=df, family=binomial(link=""logit""))
</code></pre>

<p>Thanks!</p>
"
"0.0678844233302131","0.0650944554904119"," 41660","<p>I'm modeling a set of outcome data the depends on two parameters:</p>

<ol>
<li>time, T</li>
<li>-100 &lt; A &lt; 100</li>
</ol>

<p>I've done logistic regression using R with the command:</p>

<pre><code>model &lt;- glm(Outcome ~ A + T, family = ""binomial"", data = myData)
</code></pre>

<p>My expectation (the only thing that makes sense) is that when A &lt; 0, the fit probability should be an increasing function of time approaching 0.5, while when A > 0 it should be a decreasing function of time approaching 0.5.</p>

<p>However, the fit I get is that A &lt; 0, A > 0, and A = 0 all are increasing functions of time.  They in fact appear to be the same curve just shifted (ie same ""shape"").</p>

<p>What am I doing incorrectly?  Any suggestions?</p>
"
"0.0720023041105979","0.0920574617898323"," 43315","<p>The model that I created in R is:</p>

<blockquote>
  <p>fit &lt;- lm(hired ~ educ + exper + sex, data=data)</p>
</blockquote>

<p>what I am unsure of is how to fit to model to predict probability of interest where p = pr(hiring = 1).</p>

<p>Any help would be appreciated thanks,
Clay </p>

<p><strong>Edit:</strong>
This is the computer output for what I have computed so far. I am unsure if this is even a step in the right direction to find the answer to this question.</p>

<p>What I am trying to do is, Fit a logistic regression model to predict the probability of being hired using years of education, years of experience and sex of job applicants.</p>

<pre><code> &gt; test&lt;-glm(hired ~ educ + exper + sex, data=data, family=binomial(link=""logit""))
 &gt; summary(test)

 Call:
 glm(formula = hired ~ educ + exper + sex, family = binomial(link = ""logit""), 
     data = data)

 Deviance Residuals: 
     Min       1Q   Median       3Q      Max  
 -1.4380  -0.4573  -0.1009   0.1294   2.1804  

 Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)  
 (Intercept) -14.2483     6.0805  -2.343   0.0191 *
 educ          1.1549     0.6023   1.917   0.0552 .
 exper         0.9098     0.4293   2.119   0.0341 *
 sex           5.6037     2.6028   2.153   0.0313 *
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

 (Dispersion parameter for binomial family taken to be 1)

     Null deviance: 35.165  on 27  degrees of freedom
 Residual deviance: 14.735  on 24  degrees of freedom
 AIC: 22.735

 Number of Fisher Scoring iterations: 7
</code></pre>
"
"0.0339422116651065","0.032547227745206"," 45754","<p>I have the following output from a logistic regression model.</p>

<pre><code>Coefficients:
                             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                -2.6023448  0.0694696 -37.460  &lt; 2e-16 ***
our_bid                     0.0039520  0.0007646   5.169 2.35e-07 ***
our_bid:zipcode10000:14849  0.0019334  0.0009006   2.147 0.031807 *  
our_bid:zipcode14850:19699  0.0022905  0.0009514   2.407 0.016064 *  
our_bid:zipcode19700:29999 -0.0009483  0.0008583  -1.105 0.269231    
our_bid:zipcode30000:31999 -0.0016309  0.0011028  -1.479 0.139161    
our_bid:zipcode32000:34999  0.0016241  0.0007856   2.067 0.038688 *  
our_bid:zipcode35000:42999  0.0023549  0.0008541   2.757 0.005831 ** 
our_bid:zipcode43000:49999  0.0007096  0.0008104   0.876 0.381286    
our_bid:zipcode50000:59999  0.0006533  0.0009269   0.705 0.480942    
our_bid:zipcode60000:69999  0.0030564  0.0008169   3.742 0.000183 ***
our_bid:zipcode7000:9999   -0.0027419  0.0012699  -2.159 0.030847 *  
our_bid:zipcode70000:79999  0.0013243  0.0007809   1.696 0.089921 .  
our_bid:zipcode80000:89999  0.0038726  0.0008006   4.837 1.32e-06 ***
our_bid:zipcode90000:96999  0.0038746  0.0007817   4.957 7.18e-07 ***
our_bid:zipcode97000:99820  0.0009085  0.0010044   0.905 0.365726    
---
</code></pre>

<p>I am using these coefficients to draw the predicted probabilities such that.</p>

<p>$$\text{Prob} = \frac{1}{1 + e^{-z}}$$</p>

<p>where</p>

<p>$$z = B_0 + B_1X_1 + \dots + B_nX_n.$$</p>

<p>I realize that interpreting these interaction terms can be challenging. However, I generate the main regression equation and use that to formulate the probability curve. However, I'm not sure how to make sense of any of the ""our_bid:zipcode"" variables? </p>

<p>What about if my model output was: (instead saving zipcode as a factor, I make it a continuous variable)</p>

<pre><code>Coefficients:
                             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                -2.6023448  0.0694696 -37.460  &lt; 2e-16 ***
our_bid                     0.0039520  0.0007646   5.169 2.35e-07 ***
our_bid:zipcode             0.0019334  0.0009006   2.147 0.031807 *  
</code></pre>

<p>Would interpretation being easier with this approach? Keeping with the log-odds, how can I make sense of the log-odds effect that this model expresses for the interaction term?</p>
"
"0.197915403423893","0.18978131929287"," 46322","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/28936/how-to-compute-significant-interaction-estimates-when-main-effect-is-not-signifi"">How to compute significant interaction estimates when main effect is not significant?</a>  </p>
</blockquote>



<p>I am an applied linguist and I am modelling responses to a vocabulary test taken by second language learners of English; the aim is to test theoretical hypotheses regarding the relationship between the nature of the word and the likelihood of the learnersâ€™ knowing that word. The models I am using to explore the role of explanatory variables are the random item Item Response Theory model LLTM+e (see de Boeck et al. 2011; de Boeck 2008). These are created using the lmer function in the LME4 package in R and treat item and person responses as random. The estimates shown below are effectively like those from a binary logistic regression model, indicating the log-odds of a correct response on a word, given certain properties. Covariates relate to both item and person characteristics. </p>

<p>The nature of my concern is actually a basic regression issue. I have found a significant interaction between ability grouping of the test taker (GRP; with two levels High and Low) and the length of the word in letters (LEN_L). As far as I can see the estimate of the fixed effects for the first model shows that (a) the lower level learners have an overall lower probability of giving a correct answer (b) that LEN_L does not provide a significant explanation across the pattern of responses for the whole test-taker population, and (c) a significant interaction between GRP and LEN_L indicates that the lower ability learners are less likely to give a correct answer for a longer word. This is in keeping with theory. </p>

<p>However, when I model the data without including the main effect for LEN_L, I am not seeing a significant effect for either high or low groups as shown in Model 2. LEN_L does not show as significant if modelled without interaction with grp low (not shown). I feel that I am missing something obvious, but I cannot quite grasp what is happening. And my references have run dry on this particular issue and I am thinking myself around in circles about it.</p>

<p>(NB: in my full model I have many other significant covariates, but this pattern holds true.) 
My query basically regards whether I should use Model 1, including the non-significant main effect, or whether my findings from Model 2 indicate that the finding is a little unstable. Any advice would be much appreciated! (I can post more details if necessary.)
Karen</p>

<p>Model 1 Fixed effects:</p>

<pre><code>              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)    0.25244    0.83194   0.303  0.76156    
grp low       -2.09126    0.26406  -7.920 2.38e-15 ***
LEN_L          0.02093    0.13062   0.160  0.87272    
grp low:LEN_L -0.09185    0.03146  -2.919  0.00351 **
</code></pre>

<p>Model 2 Fixed effects:</p>

<pre><code>               Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)     0.25243    0.83194   0.303    0.762    
grp low        -2.09126    0.26406  -7.920 2.38e-15 ***
grp high:LEN_L  0.02093    0.13062   0.160    0.873    
grp low:LEN_L  -0.07092    0.13202  -0.537    0.591  
</code></pre>

<blockquote>
  <p>De Boeck, P., Bakker, M., Zwitser, R., Nivard, M., Hofman, A.,
  Tuerlinckx, F. and Partchev, I. (2011) <a href=""http://www.jstatsoft.org/v39/i12/"" rel=""nofollow"">The Estimation of Item
  Response Models with the 'lmer' Function from the lme4 Package in
  R</a>. Journal of Statistical Software (39:12) pp 1-28</p>
</blockquote>
"
"0.0678844233302131","0.0650944554904119"," 46523","<p>I know I'm missing something in my understanding of logistic regression, and would really appreciate any help.</p>

<p>As far as I understand it, the logistic regression assumes that the probability of a '1' outcome given the inputs, is a linear combination of the inputs, passed through an inverse-logistic function. This is exemplified in the following R code:</p>

<pre><code>#create data:
x1 = rnorm(1000)           # some continuous variables 
x2 = rnorm(1000)
z = 1 + 2*x1 + 3*x2        # linear combination with a bias
pr = 1/(1+exp(-z))         # pass through an inv-logit function
y = pr &gt; 0.5               # take as '1' if probability &gt; 0.5

#now feed it to glm:
df = data.frame(y=y,x1=x1,x2=x2)
glm =glm( y~x1+x2,data=df,family=""binomial"")
</code></pre>

<p>and I get the following error message: </p>

<blockquote>
  <p>Warning messages:
  1: glm.fit: algorithm did not converge 
  2: glm.fit: fitted probabilities numerically 0 or 1 occurred </p>
</blockquote>

<p>I've worked with R for some time now; enough to know that probably I'm the one to blame..
what is happening here?</p>
"
"0.168005376258062","0.184114923579665"," 48381","<p>I'm trying to estimate power in a logistic regression with a continuous exposure in a cohort study (ie, the ratio of the sampling probabilities is 1). I have population cumulative incidence (probability) and population exposure variability and exposure mean and an expected odds ratio. I also have a total sample size.</p>

<p>I'm using R and it seems like <code>Hmisc::bpower</code> is only for logistic regression with binary exposure and I can't seem to find any packages that estimate binomial power with continuous exposure.</p>

<p>I've attempted the following simulation but it's quite slow given my total sample size and I'm not sure if it's right:</p>

<pre><code>p &lt;- vector()
betahat &lt;- vector()
for(i in 1:1000){
n &lt;- 40000  #total sample size
intercept = log(0.008662265)  #where exp(intercept) = P(D=1)
beta &lt;- log(1.4) #where exp(beta)=OR corresponding to a one unit change in xtest
xtest &lt;- rnorm(n,1.2,.31)  #xtest is vector length 40,000 with mean 1.2 and sd .31
linpred &lt;- intercept + xtest*beta #linear predictor
prob &lt;- exp(linpred)/(1 + exp(linpred)) #link function
runis &lt;- runif(n,0,1) #generate a vector length n from a uniform distribution 0,1
ytest &lt;- ifelse(runis &lt; prob,1,0)  #if a random value from a uniform distribution 0,1 is less than prob, then the outcome is 1.  otherwise the outcome is 0
coefs &lt;- coef(summary(glm(ytest~xtest, family=""binomial"")))  #run a logistic regression
p[i] &lt;- coefs[2,4] #store the p value
betahat[i] &lt;- coefs[2,1] #store the unexponentiated betahat
}

mean(p &lt; .05)
#power

exp(mean(betahat))
#sanity check, should equal 1.4--it does
</code></pre>

<p>Is there anything wrong with this approach?</p>

<p>One concern of mine is that the cumulative incidence (ie, probability of event over the given time period) comes from a population that did not have 0 exposure.  In fact, it's reasonable to assume that the value i'm using for an intercept is actually from a population that has an exposure variability similar to mine. In that case, how would I estimate the unexposed probability given an odds ratio (and other information that I would find in say, a published paper) to use in my power calculation?</p>
"
"0.176196922004311","0.19007487139298"," 48415","<p>I come to you today because I face a huge problem that I cannot explain.</p>

<p>I have run a multinomial logistic regression (using the mlogit package) on behavioral data. I prepare the data by doing</p>

<pre><code>    mlogit &lt;- mlogit.data(Merge, choice = ""Choice"", shape = ""long"", alt.var = ""Comp"", 
                          drop.index = TRUE)
</code></pre>

<p>on my Merge data.</p>

<p>which gives me the following:</p>

<pre><code>                Date     Time ActivityX ActivityY Temp Behavior Valley Age Month Year kid Individual Choice
    1.F   01/05/2012 00:00:00        80        58   10        F  Fuorn   8     5 2012   Y         26   TRUE
    1.R   01/05/2012 00:00:00        80        58   10        F  Fuorn   8     5 2012   Y         26  FALSE
    1.M   01/05/2012 00:00:00        80        58   10        F  Fuorn   8     5 2012   Y         26  FALSE
    1.RUN 01/05/2012 00:00:00        80        58   10        F  Fuorn   8     5 2012   Y         26  FALSE
    2.F   01/05/2012 00:05:00        90        76   10        F  Fuorn   8     5 2012   Y         26   TRUE
    2.R   01/05/2012 00:05:00        90        76   10        F  Fuorn   8     5 2012   Y         26  FALSE
    2.M   01/05/2012 00:05:00        90        76   10        F  Fuorn   8     5 2012   Y         26  FALSE
    2.RUN 01/05/2012 00:05:00        90        76   10        F  Fuorn   8     5 2012   Y         26  FALSE
    3.F   01/05/2012 00:10:00        51        47   10        M  Fuorn   8     5 2012   Y         26  FALSE
    3.R   01/05/2012 00:10:00        51        47   10        M  Fuorn   8     5 2012   Y         26  FALSE
    3.M   01/05/2012 00:10:00        51        47   10        M  Fuorn   8     5 2012   Y         26   TRUE
    3.RUN 01/05/2012 00:10:00        51        47   10        M  Fuorn   8     5 2012   Y         26  FALSE
    4.F   01/05/2012 00:15:00         0         0   10        R  Fuorn   8     5 2012   Y         26  FALSE
    4.R   01/05/2012 00:15:00         0         0   10        R  Fuorn   8     5 2012   Y         26   TRUE
    4.M   01/05/2012 00:15:00         0         0   10        R  Fuorn   8     5 2012   Y         26  FALSE
    4.RUN 01/05/2012 00:15:00         0         0   10        R  Fuorn   8     5 2012   Y         26  FALSE
    5.F   01/05/2012 00:20:00         0         0    9        R  Fuorn   8     5 2012   Y         26  FALSE
    5.R   01/05/2012 00:20:00         0         0    9        R  Fuorn   8     5 2012   Y         26   TRUE
    5.M   01/05/2012 00:20:00         0         0    9        R  Fuorn   8     5 2012   Y         26  FALSE
    5.RUN 01/05/2012 00:20:00         0         0    9        R  Fuorn   8     5 2012   Y         26  FALSE
</code></pre>

<p>then I ran my regression :</p>

<pre><code>m1 &lt;- mlogit(Choice ~ 1 |Temp + Valley + Age + kid + Month , mlogit)
</code></pre>

<p>and it gave me significant results :</p>

<pre><code>                          Estimate  Std. Error  t-value  Pr(&gt;|t|)    
    M:(intercept)      -4.2153e-01  5.7533e-02  -7.3268 2.358e-13 ***
    R:(intercept)       6.2325e-01  3.4958e-02  17.8284 &lt; 2.2e-16 ***
    RUN:(intercept)    -1.2275e+01  4.0526e-01 -30.2895 &lt; 2.2e-16 ***
    M:Temp              1.5371e-02  9.8680e-04  15.5764 &lt; 2.2e-16 ***
    R:Temp             -3.9871e-02  6.7926e-04 -58.6975 &lt; 2.2e-16 ***
    RUN:Temp           -4.4532e-02  6.8696e-03  -6.4825 9.023e-11 ***
    M:ValleyTrupchun   -3.6154e-01  1.6362e-02 -22.0968 &lt; 2.2e-16 ***
    R:ValleyTrupchun   -4.0186e-02  9.7968e-03  -4.1020 4.096e-05 ***
    RUN:ValleyTrupchun  1.2895e+00  8.5357e-02  15.1066 &lt; 2.2e-16 ***
    M:Age              -1.1026e-02  2.6902e-03  -4.0985 4.158e-05 ***
    R:Age               1.9465e-02  1.6479e-03  11.8119 &lt; 2.2e-16 ***
    RUN:Age             5.5473e-02  1.6661e-02   3.3294 0.0008703 ***
    M:kidY              6.0686e-02  2.2638e-02   2.6807 0.0073460 ** 
    R:kidY             -4.1638e-01  1.2391e-02 -33.6024 &lt; 2.2e-16 ***
    RUN:kidY            6.2311e-01  1.0410e-01   5.9854 2.158e-09 ***
    M:Month            -2.0466e-01  8.4448e-03 -24.2346 &lt; 2.2e-16 ***
    R:Month             2.4148e-02  5.2317e-03   4.6157 3.917e-06 ***
    RUN:Month           9.8715e-01  5.6209e-02  17.5622 &lt; 2.2e-16 ***
</code></pre>

<p>those results were in line with what I expected to find in literature so I was quite happy.</p>

<p>My next step was to plot my results and here is when I have some trouble.</p>

<p>First of all when I plot my original data and compare it with the result of my regression I find some huge differences. For example, when I plot the %of time spend in a behavior (M for moving, F for feeding, R for resting and Run for running, in my regression F is the reference) in function of age, I find that the older an individual gets, the more they will rest and the more they will move, but the estimates I got from my regression shows that they should rest more (when they get older) but move less. So to summarize, my graph on the original data shows the opposite as what I got from the regression.</p>

<p>I don't know if it is normal, in the sense that I don't know if I can compare my original data to the result of my regression in a way that my regression shows the probability from switching to a behavior from an other each time my variable grows of one unit.</p>

<p>So I wanted to use the <code>predict()</code> function but I don't know how to do that. I was hoping to get some help here.</p>
"
"0.10182663499532","0.130188910980824"," 49078","<p>I have a list of observations of females and their responses, categorized into 5 behaviors, to a potential threat. I'm wondering if, for each response type, whether the presence of an infant makes it more or less likely a female would perform that response. (E.g., we might hypothesize that females with infants are more likely to hide and less likely to go looking for food.) </p>

<p>The following was suggested:</p>

<ol>
<li>Bin females by social group (i.e., each bin would only contain
females who associated with each other).</li>
<li>Calculate the proportion of each response type observed in each
group (all these, for each group, would add to 1).</li>
<li>Arcsine-transform the proportion data.</li>
<li>Perform a t-test, for each behavior, to see whether the proportions
of females responding that way differs depending on the presence of
an infant.</li>
</ol>

<p>This sounded sketchy to me, and it <a href=""http://udel.edu/~mcdonald/stattransform.html"" rel=""nofollow"">seems</a> that <a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2613284/"" rel=""nofollow"">others</a> agree. The consensus seems to be that, in such cases, it's better to use multinomial regression (in this case, BEHAVIOR ~ INFANT_STATUS) to determine whether infant presence has an effect. However, I was wondering whether I could use those results determine whether (and in what direction) the presence of an infant affects the probability of each response behavior. Also, would that be possible if I were to include additional independent variables?</p>

<p>Any advice you can give, on analytical design or actual coding, is much appreciated. I've been doing this in R, but I'm more comfortable in MATLAB if that works as well. Thanks in advance.</p>
"
"0.0720023041105979","0.0920574617898323"," 49141","<p>My predictions coming from a logistic regression model (glm in R) are not bounded between 0 and 1 like I would expected. My understanding of logistic regression is that your input and model parameters are combined linearly and the response is transformed into a probability using the logit link function. Since the logit function is bounded between 0 and 1, I expected my predictions to be bounded between 0 and 1.</p>

<p>However that's not what I see when I implement logistic regression in R:</p>

<pre><code>data(iris)
iris.sub &lt;- subset(iris, Species%in%c(""versicolor"",""virginica""))
model    &lt;- glm(Species ~ Sepal.Length + Sepal.Width, data = iris.sub, 
                family = binomial(link = ""logit""))
hist(predict(model))
</code></pre>

<p><img src=""http://i.stack.imgur.com/0BHU5.png"" alt=""enter image description here""></p>

<p>If anything the output of predict(model) looks normal to me. Can anyone explain to me why the values I get are not probabilities?</p>
"
"0.083141099321054","0.0797241005179101"," 51208","<p>Suppose I want to test whether or not code patches created on weekends have a greater bug rate than those created on weekdays. (We might guess that this is so because people who are at work on weekends are more hurried etc.)</p>

<p>If we follow the standard and model bug creation as a Poisson process, this means (I think) that we want to tell the probability that both data sets came from the same underlying distribution, i.e. $\lambda_1 = \lambda_2$. </p>

<p>How can I do this? One way I thought of is use MLE to find the parameter which best fits one distribution, and then test the likelihood that parameter generates the second distribution. Alternatively, I could do two regressions and then use a likelihood ratio test. The problem with both methods is they are not testing whether one single $\lambda$ underlies both sets, but rather if the best-fit for one is the best-fit for the other.</p>
"
"0.144730076833889","0.152660029954676"," 55393","<p>I have a PDF (Probability Density Function) generated from a vector of 1,000,000 empirical values. This empirical PDF is heavily skewed to the right.</p>

<p>In this form, I can't make accurate predictions using a linear regression.</p>

<p>To fix this, is there some method to find the function F(x) to transform (i.e. ""squash"") the values in the vector into a standard normal distribution, so I can feed said transformed vector into a linear regression?</p>

<p>Of course, this would also involve finding the inverse of F(x) that transforms (i.e. ""de-squashes"") any predictions back into the original empirical PDF.</p>

<p><strong>What I have tried</strong></p>

<p>So far, I have managed to generate the density function from the empirical data:</p>

<p><img src=""http://i.stack.imgur.com/HIBUP.png"" alt=""enter image description here""></p>

<p>Here is the R code:</p>

<pre><code>par(mfrow=c(2,1))

install.packages(""bootstrap"")
library(bootstrap)
data(stamp)
nobs &lt;- dim(stamp)[1]
hist(stamp$Thickness,col=""grey"",breaks=100,freq=F)
	dens &lt;- density(stamp$Thickness)
lines(dens,col=""blue"",lwd=3)

plot(density(stamp$Thickness),col=""black"",lwd=3, main=""Simulation to choose density plot"")
	for(i in 1:10)
	{
		newThick &lt;- rnorm(nobs,mean=stamp$Thickness,sd=dens$bw*1.5)
		lines(density(newThick,bw=dens$bw),col=""grey"",lwd=3)
}

# If I wanted to do a linear regression to predict stamp thickness,
# what is the function F(x) to ""squash"" (i.e. transform) the ""stamp""
# vector into a normal distribution, and the corresponding inverse 
# function Finv(x) to ""desquash"" (i.e. untransform) any predictions back 
# into the original prediction?
</code></pre>

<p><strong>Update 1</strong></p>

<p>@Andre Silva sugggested that:</p>

<blockquote>
  <p>What need to have normal distribution are the residuals (predicted
  versus observed) derived from your (multiple) linear regression model.</p>
</blockquote>

<p>According to <a href=""http://www.stat.yale.edu/Courses/1997-98/101/linmult.htm"" rel=""nofollow"">post on Multiple Linear Regression</a>:</p>

<blockquote>
  <p>After fitting the regression line, it is important to investigate the
  residuals to determine whether or not they appear to fit the
  assumption of a normal distribution. A normal quantile plot of the
  standardized residuals y -  is shown to the left. Despite two large
  values which may be outliers in the data, the residuals do not seem to
  deviate from a random sample from a normal distribution in any
  systematic manner.</p>
</blockquote>

<p><img src=""http://i.stack.imgur.com/3ybm0.gif"" alt=""enter image description here""></p>

<p><strong>Update 2</strong></p>

<p>See <a href=""http://stats.stackexchange.com/questions/11351/left-skewed-vs-symmetric-distribution-observed/11352#11352"">Left skewed vs. symmetric distribution observed</a> for R code that illustrates that the only relevant concern is if the residuals are normally distributed.</p>
"
"0.117579270250443","0.112746904200424"," 56534","<p>I am trying to find a more aesthetic way to present an interaction with a quadratic term in a logistic regression (categorisation of continuous variable is not appropriate).</p>

<p>For a simpler example I use a linear term.</p>

<pre><code>set.seed(1)

df&lt;-data.frame(y=factor(rbinom(50,1,0.5)),var1=rnorm(50),var2=factor(rbinom(50,1,0.5)))
mod&lt;-glm(y ~ var2*var1  , family=""binomial"" , df)

 #plot of predicted probabilities of two levels

new.df&lt;-with(df,data.frame(expand.grid(var1=seq(-2,3,by=0.01),var2=levels(var2))))
pred&lt;-predict(mod,new.df,se.fit=T,type=""r"")

with(new.df,plot(var1,pred$fit))

 #plot the difference in predicted probabilities

trans.logit&lt;-function(x) exp(x)/(1+exp(x))

pp&lt;-trans.logit(coef(mod)[1] + seq(-2,3,by=0.01) * coef(mod)[3]) -trans.logit((coef(mod)[1]+coef(mod)[2]) + seq(-2,3,by=0.01) * (coef(mod)[3]+coef(mod)[4]))

plot(seq(-2,3,by=0.01),pp)
</code></pre>

<h3>Questions</h3>

<ul>
<li>How can I plot the predicted probability difference between the two levels of var2 (rather than the 2 levels separately)  at different values of var1?</li>
<li>Is there a way to define contrasts so I can use these in the glm so I can then pass this to predict? - I need a CI for the difference in probabilities</li>
</ul>
"
"0.0554273995473693","0.0531494003452734"," 56608","<p>I'm doing some clinical database research and in an effort to lessen the burden on our statistical staff, I started to look for different software solutions to get the analyses I need, and that's where BIDS (business intelligence development studio).  BIDS allows queries to be run against a SQL Server and store the results of those queries in a table or view.  That table or view is then consumed by BIDS and logistic and linear regressions as while as CART analyses can be done on them.<br>
BIDS Version</p>

<p>The x axis that is cut off on the lift chart is 'overall population %'
<img src=""http://i.stack.imgur.com/LbpXf.jpg"" alt=""enter image description here""> </p>

<p>A mining accuracy chart of the CART
<img src=""http://i.stack.imgur.com/Vm5Te.jpg"" alt=""enter image description here""></p>

<p>I'm not quite sure how this works in other stats packages, but in BIDS you define a certain percentage of your data set to train the model and the rest of the set is compared against that model and the lift chart shows the improvement in identifying the outcome you desire vs. a guess.  I'm only vaguely familiar with CART analyses in the first place, and don't know the first thing about R, but this same analysis was done in R with very similar results.  The red portion of what looks like a health meter in a video game corresponds nearly identical to the analysis done in R.  However, there are no p values in the BIDS version.  Consider the node Max Total Poly Pharm >=7 and &lt; 13.
BIDS shows me (not present in the picture)
value         cases    probability
not present   2133     91.89</p>

<p>Is there any way to ascertain a p value from that.  And does R use any of it's cases as training data for the model?</p>
"
"0.0480015360737319","0.0460287308949162"," 57551","<p>Was wondering if anyone knows of an R package to estimate the Cauchy-M estimator of regression (see for example the end of this <a href=""http://www.brnt.eu/phd/node13.html#SECTION00712200000000000000"" rel=""nofollow"">section</a>, but with simultaneous estimation of the scale parameter as in section 2 of (1)). </p>

<blockquote>
  <p>(1) Mizera, I. MÃ¼ller, C. H. (2002).  Breakdown points of Cauchy
  regression-scale estimators, Statistics &amp; Probability Letters, Volume
  57, Issue 1, Pages 79-89.</p>
</blockquote>
"
"0.128004096196618","0.138086192684749"," 58448","<p>I'm stuck with a regression modeling problem. I have panel data where the dependent variable is a probability. Below is an excerpt from my data. The complete panel covers more countries and years, however it is unbalanced. What I can observe is the number of events and the number of trials. The event probability was derived from those values (estimation of this probability should be quite good, given the large number of trials). All independent variables are county-year specific.</p>

<pre><code>     country  year  event_prob  events trials    x    x_lag2 ... more variables
  1   Cyprus  2008  0.03902140  11342  290661   4.60   4.13  ...
  2   Cyprus  2009  0.04586650  13482  293940   4.60   4.48  ...
  3   Cyprus  2010  0.05188398  15206  293077   4.60   4.60  ...
  4   Cyprus  2011  0.06433411  18505  287639   5.79   4.60  ...
  5  Estonia  2008  0.07872978  21686  275449   6.02   4.11  ...
  6  Estonia  2009  0.09516270  33599  353069  13.18   4.91  ...
  7  Estonia  2010  0.08645905  36180  418464   7.95   6.03  ...
  8  Estonia  2011  0.07731997  31590  408562   5.53  13.18  ...
  ...
165  USA  2011  0.06100000  9192822  150702000   2.73  3.27  ...
</code></pre>

<p>My goal is to use regression analysis to find out which variables are significant for the event probability. In R-terminology, I'm looking for a model of the form <code>event_prob ~ x + x_lag2 + ...</code> .</p>

<p>The problem is as follows: <code>event_prob</code> has to be between 0 and 1, hence using <code>event_prob ~ x + x_lag2 + ...</code> might not be the best idea. So I was thinking of using the logit transform of <code>event_prob</code> such that <code>logit(event_prob)</code> ranges from $-\infty$ to $\infty$. The first idea was to use the R's <code>plm</code> package, i.e. <code>plm(logit(event_prob)~x+x_lag2,data,index=c(""country"",""year""),model=""random"")</code> or <code>model=""within""</code> (see below). Is that a reasonable approach or am I violating some essential assumptions?</p>

<p>I was also thinking of using panel generalized linear models from the package <code>pglm</code> (with the logit link function), however since I don't know the outcome of the binary events (only the total number of events and trials) is known, I got stuck there. Maybe someone can help me how to proceed here.</p>

<p>Since I have panel data, I'd like to compute both fixed-effects models and random-effects model and then apply the Hausman (1978) test to decide which model is more appropriate.</p>

<p>Do my first attempts at modeling make sense? I'm really not sure how to correctly address this problem. I hope the description of my problem is detailed enough. If not, I'm happy to provide more details</p>

<p>In terms of software, I'd prefer R. SAS and SPSS are also ok since my university has licences for them. I just don't have much experience with them.</p>
"
"0.0480015360737319","0.0460287308949162"," 58620","<p>I have a data set $Y$, each element $y_i$ in $Y$ has the  covariate $\lambda_i$. Here, I want to use Poisson regression to fit a model: <code>glm(Y ~ $\lambda$, family = ""poisson"", data = ..)</code>. Then given a new observe $y_k$ and the covariate $\lambda_k$, suppose the new data follow the fitted model, how can I use R to calculate the probability of observing $y_k$ and $\lambda_k$.</p>
"
"0.128004096196618","0.138086192684749"," 58874","<p>As the title says, what I'd like to do is stepwise introduction of predictor variables to a mixed-effects model. I'm going to first say what I'd be doing if it were stepwise linear regression, just to make sure I've got that part right, and then describe the full model to which I want to apply an analogous approach.</p>

<p>I have a student population who took a pretest, then a tutorial, then a posttest. The tutorial involved doing problems from several categories with feedback, and the users could control which category the next problem would come from and when to stop the tutorial.</p>

<p>I want to create a model that will account for posttest performance using pretest score and some measures of behavior during the tutorial, including total number of problems done, accuracy, and probability of switching category. The last of these is of greatest theoretical interest. There are other variables I'm not mentioning for simplicity.</p>

<p>For the linear regression approach, I first did a simple regression using posttest score as the DV and including the main effects (only) of pretest score, tutorial accuracy, and number of problems as predictors. Then, I added probability of switching as an additional predictor, and compared the resulting model to the previous one to see if it had significantly better explanatory power (it did). The R code I used is below.</p>

<pre><code>lm1 &lt;- lm( posttestScore ~ pretestScore + practiceAccuracy + practiceNumTrials, data=subj.data )
lm2 &lt;- lm( posttestScore ~ pretestScore + practiceAccuracy + practiceNumTrials + probCategorySame, data=subj.data )
anova( lm1, lm2 )
</code></pre>

<p>So far so good? OK, next, I switched to a mixed model in order to include a binary within-subjects factor, 'test question type'. Both pretest and posttest have values for each level of this factor for every subject. (It's unrelated to the 'problem category' I mentioned for the tutorial.) The other predictors, however, only have one value for each participant. My models then became:</p>

<pre><code>library( nlme )
lm1 &lt;- lme( posttestScore ~ pretestScore + questionType + practiceAccuracy + practiceNumTrials, random=~1|sid, method=""REML"", data=D )
lm2 &lt;- lme( posttestScore ~ pretestScore + questionType + practiceAccuracy + practiceNumTrials + probCategorySame, random=~1|sid, method=""REML"", data=D )
</code></pre>

<p>However, I don't know how to test whether the second model resulted in a significant improvement over the first model. Is that the right question I should be asking and, if so, how should I do it?</p>
"
"0.181025128880568","0.195283366471236"," 62070","<p>Let's say you have a response variable and an independent variable. Your data is measured across several levels of a categorical independent variable. One approach in analysing these data would be to use linear regression to estimate a slope at each level of the categorical independent variable. This is the approach I've used here, using <code>sleepstudy</code> dataset from the <code>R</code> <code>lme4</code> package (I've stored the betas from each model in <code>lmBetas</code>):</p>

<pre><code>library(lme4); library(plyr); library(ggplot2)
lmBetas &lt;- daply(sleepstudy, .(Subject), function(x) coef(lm(Reaction ~ Days, data=x))[""Days""])
</code></pre>

<p>Another approach in analysing these data would be to use a mixed effects model to estimate slopes for each level of the categorical independent variable, which in this case is <code>Subject</code>. This is the approach I've taken here (I've stored the betas from the model in <code>lmerBetas</code>):</p>

<pre><code>lmerBetas &lt;- coef(lmer(Reaction ~ Days + (Days | Subject), data=sleepstudy))$Subject[,""Days""]
</code></pre>

<p>I have learned that a single mixed effects model, as implemented through the <code>lmer</code> function in R, is more accurate at estimating slopes than a multiple linear regression model applied to multilevel data. This can be demonstrated with this plot of betas from the above models. </p>

<pre><code>betas &lt;- data.frame(method.betas = c(lmerBetas, lmBetas))
betas$method &lt;- c(rep(""lmer"", 18), rep(""lm"", 18))

ggplot(betas, aes(method.betas)) + 
  geom_histogram() +
  facet_grid(method ~ .)
</code></pre>

<p>The top histogram shows betas estimated using linear regression, and the bottom histogram shows betas estimated using mixed effects. You can see betas estimated using linear regression are more widely spread than those estimated through the mixed effects model.</p>

<p><img src=""http://i.stack.imgur.com/YtB22.jpg"" alt=""enter image description here""></p>

<p>So finally, my questions:</p>

<ol>
<li><p>Is a mixed effects model's higher accuracy in betas estimation connected with the fact that it models intercepts and slopes for each level of the categorical independent variable under a joint probability distribution?</p></li>
<li><p>Generally speaking, why is a mixed effects model more accurate in its betas estimation?</p></li>
</ol>
"
"0.0678844233302131","0.0650944554904119"," 65258","<p>Consider the Challenger-Disaster:</p>

<pre><code>Temp &lt;- c(66,67,68,70,72,75,76,79,53,58,70,75,67,67,69,70,73,76,78,81,57,63,70)
Fail &lt;- factor(c(0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,1,1,1))
shuttle &lt;- data.frame(Temp, Fail)
colnames(shuttle) &lt;- c(""Temp"", ""Fail"")
</code></pre>

<p>Now I can fit a logistic model which will explain the ""Fail"" of O-ring seals by Temperature:</p>

<pre><code>fit &lt;- glm(Fail~Temp,data=shuttle, family=binomial); fit
</code></pre>

<p>The R output looks like this:</p>

<pre><code> Call:  glm(formula = Ausfall ~ Temp, family = binomial, data =
 shuttle)

 Coefficients: (Intercept)         Temp  
     15.0429      -0.2322  

 Degrees of Freedom: 22 Total (i.e. Null);  21 Residual Null Deviance:  
 28.27  Residual Deviance: 20.32    AIC: 24.32
</code></pre>

<h3>Questions</h3>

<ul>
<li><strong>In general, how do you predict probabilities for specific data in logistic regressions using R?</strong></li>
<li><strong>Or specifically, what is the command to calculate the probability of a ""Fail"" if temperature is at 37Â°?</strong> (which it was in the night before the Challenger disaster).</li>
</ul>

<p>I thought it would be something like this:</p>

<pre><code>predict(fit, Temp=37)
</code></pre>

<p>but it won't give me ""0.9984243"" (which I calculated myself with:  </p>

<pre><code>exp(15.0429 + (37*(-0.2322))) / 1+ exp(15.0429 + (37*(-0.2322)))
</code></pre>

<p>The method <code>predict</code> returns a matrix of numbers that makes no sense to me.</p>
"
"0.0339422116651065","0.0650944554904119"," 65375","<p>I have done an ordinal regression using <code>lrm</code> from the <code>rms</code> package.</p>

<pre><code>load(url(""http://www.produnis.de/R/NEXT-Q0-OR.RData""))
require(rms)
mydata &lt;- NEXT.or
fit &lt;- lrm(QuitJob~QD,data=mydata);fit
</code></pre>

<p>I would like to predict the rank in ""QuitJob"" (1=won't quit ... 5=will quit very soon) by giving a value for ""QD"" (how much work to do). For example, if ""QD"" is 78, what rank of ""QuitJob"" is likely?</p>

<p>If I type in </p>

<pre><code>predict.lrm(fit, data.frame(QD=78))
</code></pre>

<p>I get</p>

<pre><code>        1 
0.3191933 
</code></pre>

<p>Do I understand it right, that the probability of being in rank 1 is 31,92% ?!
If so, how can I get the probabilities for ranks 2-5 ?</p>
"
"0.083141099321054","0.0797241005179101"," 65384","<p>I'm analyzing users' in-game data in order to model whether they're going to be paid user or not. </p>

<p>Here's my model:</p>

<pre><code>Logistic Regression Model

lrm(formula = becomePaid ~ x1 + x2 + 
    x3 + x4 + x5 + x6, data = sn, x = TRUE, 
    y = TRUE)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs         1e+05    LR chi2    1488.63    R2       0.147    C       0.774    
 0          99065    d.f.             6    g        1.141    Dxy     0.547    
 1            935    Pr(&gt; chi2) &lt;0.0001    gr       3.130    gamma   0.586    
max |deriv| 8e-09                          gp       0.011    tau-a   0.010    
                                           Brier    0.009                     

               Coef    S.E.   Wald Z Pr(&gt;|Z|)
Intercept      -6.7910 0.0938 -72.36 &lt;0.0001 
x1              0.0756 0.0193   3.92 &lt;0.0001 
x2              0.0698 0.0091   7.64 &lt;0.0001 
x3              0.0020 0.0002  11.05 &lt;0.0001 
x4              0.0172 0.0057   3.03 0.0024  
x5              0.0304 0.0045   6.82 &lt;0.0001 
x6             -0.0132 0.0042  -3.17 0.0015  
</code></pre>

<p>And in my model, I created couple of use cases such as:</p>

<pre><code>    test1   test2       
x1  8           9
x2  10          10
x3  250        250
x4  6           6
x5  2           2
x6  0           1
</code></pre>

<p>Then the probability of user test1 is to turn out to be a paid user is %.07 and % 0.84 for test2.</p>

<p>However I want to calculate the cumulative probabilities such as users whose' x1 values are greater than 8, x2 values are between 10 and 20 and so on.</p>

<p>Is there any way to calculate this ?</p>

<p>Thanks ! </p>
"
"0.144004608221196","0.138086192684749"," 65690","<p>I fit a logistic on three numeric continuous variables, followed by a categorical factor [Y, N].</p>

<pre><code>logit2A &lt;- glm(DisclosedDriver ~ VehDrvr_Dif+POL_SEQ_NUM+PRMTOTAL+SAFE_DRVR_PLEDGE_FLG, data = DF, family = ""binomial"") 
</code></pre>

<p>Fit looks wonderful.</p>

<pre><code>Coefficients:
                    Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)           -2.204e+00  2.253e-01  -9.782  &lt; 2e-16 ***
VehDrvr_Dif            2.918e-01  1.026e-01   2.845 0.004440 ** 
POL_SEQ_NUM           -1.893e-01  5.617e-02  -3.370 0.000751 ***
PRMTOTAL               1.109e-04  5.526e-05   2.006 0.044804 *  
SAFE_DRVR_PLEDGE_FLGY -7.220e-01  1.633e-01  -4.422 9.76e-06 ***
</code></pre>

<p>So obviously R took the Safe_Drvr_Pledge_Flg categorical factor variable and placed all 'N' values in reference or intercept as opposed to the listed 'Y'.</p>

<p>Now I want to take my fit and calculate the probabilities that my model determines. And here comes the error:</p>

<pre><code>&gt; DF$P_GLM&lt;- predict.glm(logit2A, DF, type=""response"", se.fit=FALSE)
    Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) : 
factor SAFE_DRVR_PLEDGE_FLG has new levels 
</code></pre>

<p>Umm... no it doesn't, because I just fit the model with the exact same data I'm trying to use for the prediction. What's the problem?</p>

<p>Trying to respond to first comment:
Don't know what you mean. I've got 3500 rows of data... It's a logistic regression on 4 continuous variables and one categorical. The categorical has two values, Y or N. My glm fit give the numbers given. I just want to plug it all back in with the predict function and it gives me that error. Here's the categorical variable:</p>

<pre><code> &gt; DF$SAFE_DRVR_PLEDGE_FLG
 [1] Y Y Y Y Y Y Y N Y Y Y Y Y Y Y Y Y Y Y N Y Y N Y Y N Y Y N Y Y Y Y Y Y Y Y Y Y N Y Y N Y Y Y N Y Y Y Y Y N Y Y Y Y Y Y
 [60] Y Y Y Y N Y Y Y Y Y Y Y Y N Y Y Y N N Y N Y Y Y Y Y N Y Y N Y N N Y Y Y N Y Y Y Y N Y Y Y Y Y N Y N Y N Y Y Y Y Y N Y
 [119] N Y Y Y Y Y Y Y Y N Y Y Y Y Y Y N Y Y Y N Y Y Y N Y Y Y N N Y N N N Y N Y Y Y N N Y Y N Y Y Y Y N N Y Y Y Y N N Y N N
 Levels:  N Y
</code></pre>

<p>What do you mean by a working example? The fit works. The probability output of the predict function doesn't...</p>
"
"0.180005760276495","0.172607740855936"," 67243","<p>This is a fairly complex question so I will attempt to ask it in a fairly basic manner. </p>

<p>I have data on the abundance of 99 different species of estuarine macroinvertebrate species and the sediment mud content (0 - 100 %) in which each observation was obtained. I have a total of 1402 observations for each species (i.e. a massive dataset). </p>

<p>Here is a subset of the raw data for one species to give you an idea of the data I'm working with (if I had 10 reputation points I'd upload a plot of real raw data):</p>

<pre><code>Abundance: 10,14,10,3,3,3,3,4,5,5,0,0,0,0,0,0,0,0,0,0,0,0,0,6,6,6,0,0,0,0,12,0,0,0,34,0,0
Mud %:     0.9,4,2,10,13,14,6,5,5,7,22,27,34,37,47,58,54,70,54,80,90,65,56,7,8,34,67,54,32,1,57,45,49,4,78,65,45,35
</code></pre>

<p>The primary aim of my research is to determine an ""optimum mud % range"" (e.g. 15 - 45 %) and ""distribution mud % range"" (e.g. 0 - 80 %) for each of the 99 invertebrate species.</p>

<p>As you can see the abundance data for the above species contains a significant number of zero values. Although this significantly skews any sort of model that I run on the data (i.e. GLM, GAM), even if I model the non-zero data only, the model for certain species does not fit the data at all well.</p>

<p>So, my question is: what would be the best, most robust way to determine an ""optimum"" and ""distribution"" mud range for each species, given that responses vary significantly between species? </p>

<hr>

<p>Just to clarify - the above data is a hypothetical example to give you an idea of how messy the abundance (that is count) data can be for a given species.</p>

<p>Regarding the poisson regression approach: I'm considering conducting a two-step GLM or GAM approach for each species; Step (1) uses logistic regression to model the ""probability of presence""  for a given species over the mud gradient - using presence/absence data. This obviously takes into account the zero counts; and Step (2) models the ""maximum abundance"" over the mud gradient - using presence only count data. This step gives me an idea of the species typical response to mud where they DO occur. What are your thoughts on this approach?</p>

<p>I have R code for both steps for one particular species. Heres the code:</p>

<pre><code>     ## BINARY

aa1&lt;-glm(Bin~Mud,dist=binomial,data=Antho)
xmin &lt;- ceiling(min(Antho$Mud))
    xmax &lt;- floor(max(Antho$Mud))
Mudnew &lt;- seq(from=xmin, to=xmax, by=0.1)
pred.dat &lt;- data.frame(Mudnew)
names(pred.dat) &lt;- ""Mud""
pred.aa1 &lt;- data.frame(predict.glm(aa1, pred.dat, se.fit=TRUE, type=""response""))
pred.aa1.comb &lt;- data.frame(pred.dat, pred.aa1)
names(pred.aa1.comb)
plot(fit ~ Mud, data=pred.aa1.comb, type=""l"", lwd=2, col=1, ylab=""Probability of presence"", xlab=""Mud content (%)"", ylim=c(0,1))


## Maximum abundance

 aa2&lt;-glm(Maxabund~Mud,family=Gamma,data=antho)
 xmin &lt;- ceiling(min(antho$Mud))
     xmax &lt;- floor(max(antho$Mud))
 Mudnew &lt;- seq(from=xmin, to=xmax, by=0.1)
 pred.dat &lt;- data.frame(Mudnew)
 names(pred.dat) &lt;- ""Mud""
 pred.aa2 &lt;- data.frame(predict.glm(aa2, pred.dat, se.fit=TRUE, type=""response""))
 pred.aa2.comb &lt;- data.frame(pred.dat, pred.aa2)
 names(pred.aa2.comb)
 plot(fit ~ Mud, data=pred.aa2.comb, type=""l"", lwd=2, col=1, ylab=""Maximum abundance per 0.0132 m2"", xlab=""Mud content (%)"")
 AIC(aa2)
</code></pre>

<p>My question is: for step (2); will the model code need to be altered (i.e. family=) depending on the shape of each species abundance data, if so, would I just need to inspect a scatter plot of the raw presence only abundance data to confirm the use of a certain function? and how would the code be written for a certain species exhibiting a certain response/functional form? </p>
"
"0.083141099321054","0.0797241005179101"," 67470","<p>I want to predict a categorical variable using also categorical predictors. Currently, I am looking at classification and regression trees (CART).</p>

<p>The prediction quality is ""good enough"", except for the presence of impossible combinations. In the following minimal example, the combination <code>a==2, b==2</code> is impossible, yet the estimation decides not to use <code>b</code> for splitting.</p>

<pre><code>&gt; library(rpart)
&gt; d &lt;- data.frame(a=rep(factor(c(1,1,2)), 100000), b=factor(c(1,2,1)))
&gt; xtabs(~., d)
   b
a       1     2
  1 1e+05 1e+05
  2 1e+05 0e+00
&gt; (tr &lt;- rpart(a~b, d))
n= 300000 

node), split, n, loss, yval, (yprob)
      * denotes terminal node

1) root 300000 1e+05 1 (0.6666667 0.3333333) *
</code></pre>

<p>When simulating stochastically from this model (by choosing the leaf value by sampling using the annotated probability vector, here $(2/3, 1/3)$, as weights), the combination <code>2, 2</code> will occur:</p>

<pre><code>&gt; prob.m &lt;- predict(tr, d, type=""prob"")
&gt; d$a.sim &lt;- apply(prob.m, 1, function(x) sample.int(length(x), size=1, prob=x))
&gt; xtabs(~a.sim+b, d)
     b
a.sim      1      2
    1 133041  66615
    2  66959  33385
</code></pre>

<p>Is there a way to avoid this, perhaps using another method?</p>

<p>This is just a small example for a more general case. I have around 10 predictors, and I want to exclude all combinations of two (or perhaps three) attributes that have no observation in the sample.</p>

<p>I am aware of the ""loss matrix"" that can be specified as a parameter to <code>rpart</code>, but this is prohibitive if many predictors are used.</p>
"
"0.184211313787608","0.196267167994715"," 69957","<p>Here's my issue of the day:</p>

<p>At the moment I'm teaching myself Econometrics and making use of logistic regression. I have some SAS code and I want to be sure I understand it well first before trying to convert it to R. (I don't have and I don't know SAS). In this code, I want to model the probability for one person to be an 'unemployed employee'. By this I mean ""age"" between 15 and 64, and ""tact"" = ""jobless"". I want to try to predict this outcome with the following variables: sex, age and idnat (nationality number). (Other things being equal).</p>

<p>SAS code :</p>

<pre><code>/* Unemployment rate : number of unemployment amongst the workforce */
proc logistic data=census;
class sex(ref=""Man"") age idnat(ref=""spanish"") / param=glm;
class tact (ref=first);
model tact = sex age idnat / link=logit;
where 15&lt;=age&lt;=64 and tact in (""Employee"" ""Jobless"");
weight weight;
format age ageC. tact $activity. idnat $nat_dom. inat $nationalty. sex $M_W.;

lsmeans sex / obsmargins ilink;
lsmeans idnat / obsmargins ilink;
lsmeans age / obsmargins ilink;
run;
</code></pre>

<p>This is a sample of what the database should looks like :</p>

<pre><code>      idnat     sex     age  tact      
 [1,] ""english"" ""Woman"" ""42"" ""Employee""
 [2,] ""french""  ""Woman"" ""31"" ""Jobless"" 
 [3,] ""spanish"" ""Woman"" ""19"" ""Employee""
 [4,] ""english"" ""Man""   ""45"" ""Jobless"" 
 [5,] ""english"" ""Man""   ""34"" ""Employee""
 [6,] ""spanish"" ""Woman"" ""25"" ""Employee""
 [7,] ""spanish"" ""Man""   ""39"" ""Jobless"" 
 [8,] ""spanish"" ""Woman"" ""44"" ""Jobless"" 
 [9,] ""spanish"" ""Man""   ""29"" ""Employee""
[10,] ""spanish"" ""Man""   ""62"" ""Retired"" 
[11,] ""spanish"" ""Man""   ""64"" ""Retired"" 
[12,] ""english"" ""Woman"" ""53"" ""Jobless"" 
[13,] ""english"" ""Man""   ""43"" ""Jobless"" 
[14,] ""french""  ""Man""   ""61"" ""Retired"" 
[15,] ""french""  ""Man""   ""50"" ""Employee""
</code></pre>

<p>This is the kind of result I wish to get :</p>

<pre><code>Variable    Modality    Value   ChiSq   Indicator
Sex         Women       56.6%   0.00001 -8.9%
            Men         65.5%       
Nationality 
            1:Spanish   62.6%       
            2:French    51.2%   0.00001 -11.4%
            3:English   48.0%   0.00001 -14.6%
Age 
            &lt;25yo       33.1%   0.00001 -44.9%
        Ref:26&lt;x&lt;54yo   78.0%       
            55yo=&lt;      48.7%   0.00001 -29.3%
</code></pre>

<p>Indicator is P(category)-P(ref)
(I interpret the above as follows: other things being equal, women have -8.9% chance of being employed vs men and those aged less than 25 have a -44.9% chance of being employed than those aged between 26 and 54).</p>

<p>So if I understand well, the best approach would be to use a binary logistic regression (link=logit). This uses references ""male vs female""(sex), ""employee vs jobless""(from 'tact' variable)... I presume 'tact' is automatically converted to a binary (0-1) variable by SAS.</p>

<p>Here is my 1st attempt in R. I haven't check it yet (need my own PC) :</p>

<pre><code>### before using glm function 
### change all predictors to factors and relevel reference
recens$sex &lt;- relevel(factor(recens$sex), ref = ""Man"")
recens$idnat &lt;- relevel(factor(recens$idnat), ref = ""spanish"")  
recens$tact &lt;- relevel(factor(recens$tact), ref = ""Employee"")
recens$ageC &lt;- relevel(factor(recens$ageC), ref = ""Ref : De 26 a 54 ans"")

### Calculations of the probabilities with function glm, 
### formatted variables, and conditions with subset restriction to ""from 15yo to 64""
### and ""employee"" and ""jobless"" only.
glm1 &lt;- glm(activite ~ sex + ageC + idnat, data=recens, weights = weight, 
            subset= recens$age[(15&lt;= recens$age | recens$age &lt;= 64)] 
            &amp; recens$tact %in% c(""Employee"",""Jobless""), 
            family=quasibinomial(""logit""))
</code></pre>

<p>My questions :</p>

<p>For the moment, it seems there are many functions to carry out a logistic regression in R like glm which seems to fit.</p>

<p>However after visiting many forums it seems a lot of people recommend not trying to exactly reproduce SAS PROC LOGISTIC, particularly the function LSMEANS. Dr Franck Harrel, (author of package:rms) for one.</p>

<p>That said, I guess my big issue is LSMEANS and its options Obsmargins and ILINK. Even after reading over its description repeatedly I can hardly understand how it works.</p>

<p>So far, what I understand of Obsmargin is that it respects the structure of the total population of the database (i.e. calculations are done with proportions of the total population). ILINK appears to be used to obtain the predicted probability value (jobless rate, employment rate) for each of the predictors (e.g. female then male) rather than the value found by the (exponential) model?</p>

<p>In short, how could this be done through R, with lrm from rms or lsmeans?</p>

<p>I'm really lost in all of this. If someone could explain it to me better and tell me if I'm on the right track it would make my day.</p>

<p>Thank you for your help and sorry for all the mistakes my English is a bit rusty.</p>

<p>Binh</p>
"
"0.230407373153913","0.230143654474581"," 71414","<p>I'm trying to use a zero-inflated gamma model (or a gamma 'hurdle' model). The model is a mixture of logistic regression and generalized linear modeling. I can do this analysis in two steps: 1) do a logistic regression against presence/absence data and then 2) Use a generalized linear model with a gamma distribution on the positive values. I'm trying to set the model up where, if y = 0, then E(y) = p, but if y > 0, then E(y) is gamma distributed.</p>

<p>I'm trying to set this up in BUGS/JAGS, because I've seen these models <a href=""http://stats.stackexchange.com/questions/15967/how-can-i-set-up-a-zero-inflated-poisson-in-jags"">worked before for poisson-distributions</a>. I tried to adapt the code from that link into a gamma distribution, but I can't quite seem to get it to work. I know the code won't work because my data have zeroes, and the likelihood function can't be evaluated with zeroes. However, even when restricted to positive values, I get errors for invalid parent values. Even so, I'm not even sure the model is specified correctly.</p>

<p>Here is the model:</p>

<pre><code># For the ones trick
C &lt;- 10000

# for every observation
for(i in 1:N){
    # log-likelihood of the observation from the gamma likelihood
    LogPos[i] &lt;- (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - N*shape[i]*log(scale[i]) - N*loggam(scale[i])
    #likelihood
    Lpos[i] &lt;- exp(LogPos[i])

    # redefine the shape and rate parameters as a function of the mean and sd
    shape[i] &lt;- pow(mu[i], 2) / pow(sd, 2)
    scale[i] &lt;- mu[i] / pow(sd, 2)

    # mu is a function of MTD: use the inverse link
    #mu[i] &lt;- 1/eta[i]
    mu[i] &lt;- beta0 + beta1*MTD[i]


    # zero-inflated part, where w[i] is the probability of being zero
    w[i] &lt;- exp(zeta[i]) / (1 + exp(zeta[i]))
    zeta[i] &lt;- gamma0 + gamma1*MPD[i]

    # ones trick
    p[i] &lt;- Lpos[i] / C
    ones[i] ~ dbern(p[i])

    # Full likelihood
    Lik[i] &lt;- Lpos[i]*(1 - w[i]) + equals(y[i], 0)*w[i]
  } 

# PRIORS
beta0 ~ dnorm(0, 0.001)
beta1 ~ dnorm(0, 0.001)

gamma0 ~ dnorm(0, 0.001)
gamma1 ~ dnorm(0, 0.001)

sd ~ dunif(0, 100)
</code></pre>

<p>Has anyone set a model up like this or have any advice on how to set it up correctly?</p>

<p><strong>UPDATE</strong></p>

<p>I've tried a new set of code that's similar, but slight different. I still have not gotten it to work</p>

<pre><code>model{

  # For the ones trick
  C &lt;- 10000

  # for every observation
  for(i in 1:N){

    # make a dummy variable that is 0 if y is &lt; 0.0001 and 1 if y &gt; 0.0001. This is essentially a presence
    # absence dummy variable
    z[i] &lt;- step(y[i] - 0.0001)

    # define the logistic regression model, where w is the probability of occurance.
    # use the logistic transformation exp(z)/(1 + exp(z)), where z is a linear function
    w[i] &lt;- exp(zeta[i]) / (1 + exp(zeta[i]))
    zeta[i] &lt;- gamma0 + gamma1*MPD[i]

    # define the gamma regression model for the mean. use the log link the ensure positive, non-zero mu
    mu[i] &lt;- exp(eta[i])
    eta[i] &lt;- beta0 + beta1*MTD[i]

    # redefine the mu and sd of the continuous part into the shape and scale parameters
    shape[i] &lt;- pow(mu[i], 2) / pow(sd, 2)
    scale[i] &lt;- mu[i] / pow(sd, 2)

    # for readability, define the log-likelihood of the gamma here
    logGamma[i] &lt;- (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - shape[i]*log(scale[i]) - loggam(scale[i])

    # define the total likelihood, where the likelihood is (1 - w) if y &lt; 0.0001 (z = 0) or
    # the likelihood is w * gammalik if y &gt;= 0.0001 (z = 1). So if z = 1, then the first bit must be
    # 0 and the second bit 1. Use 1 - z, which is 0 if y &gt; 0.0001 and 1 if y &lt; 0.0001
    logLik[i] &lt;- (1 - z[i]) * log(1 - w[i]) + z[i] * ( log(w[i]) + logGamma[i] )

    # Use the ones trick
    p[i] &lt;- logLik[i] / C
    ones[i] ~ dbern(p[i])
  } 

  # PRIORS
  beta0 ~ dnorm(0, 0.001)
  beta1 ~ dnorm(0, 0.001)

  gamma0 ~ dnorm(0, 0.001)
  gamma1 ~ dnorm(0, 0.001)

  sd ~ dgamma(1, 2)

}
</code></pre>

<p><strong>UPDATE 2</strong></p>

<p>I've gotten it to run by deleting the definition of logGamma[i] and putting it directly into the likelihood function, which now reads:</p>

<pre><code>logLik[i] &lt;- (1 - z[i]) * log(1 - w[i]) + z[i] * ( log(w[i]) + (shape[i] - 1)*log(y[i]) - y[i]/scale[i] - shape[i]*log(scale[i]) - loggam(scale[i]) )
</code></pre>

<p>The problem was that JAGS was trying to evaluate the log likelihood at all observations first, resulting in NA's for the 0's. In the new way, it only evaluates it if z = 1 (I think). I'm still having trouble getting the parameter estimates to line up. For example, the gamma's are almost identical to a logistic regression of the same form (hooray!). But the betas are pretty far off from a gamma GLM of the positive values. I don't know if this is normal or not, but I suspect there are still problems with my model specification.</p>
"
"0.108857251714449","0.121780575111864"," 73567","<p>I have a logistic regression model with several variables and one of those variables (called x3 in my example below) is not significant. However, x3 should remain in the model because it is scientifically important.</p>

<p>Now, x3 is continuous and I want to create a plot of the predicted probability vs x3. Even though x3 is not statistically significant, it has an effect on my outcome and therefore it has an effect on the predicted probability. This means that I can see from the graph, that the probability changes with increasing x3. However, how should I interpret the graph and the change in the predicted probability, given that x3 is indeed not statistically significant?</p>

<p>Below is a simulated data in R set to illustrate my question. The graph also contains a 95% confidence interval for the predicted probability (dashed lines):</p>

<pre><code>&gt; set.seed(314)
&gt; n &lt;- 300
&gt; x1 &lt;- rbinom(n,1,0.5)
&gt; x2 &lt;- rbinom(n,1,0.5)
&gt; x3 &lt;- rexp(n)
&gt; logit &lt;- 0.5+0.9*x1-0.5*x2
&gt; prob &lt;- exp(logit)/(1+exp(logit))
&gt; y &lt;- rbinom(n,1,prob)
&gt; 
&gt; model &lt;- glm(y~x1+x2+x3, family=""binomial"")
&gt; summary(model)

Call:
glm(formula = y ~ x1 + x2 + x3, family = ""binomial"")

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.0394  -1.1254   0.5604   0.8554   1.4457  

Coefficients:
            Estimate Std. Error z value Pr(    &gt;|z|)    
(Intercept)   1.1402     0.2638   4.323 1.54e-05 ***
x1            0.8256     0.2653   3.112  0.00186 ** 
x2           -1.1338     0.2658  -4.266 1.99e-05 ***
x3           -0.1478     0.1249  -1.183  0.23681    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 373.05  on 299  degrees of freedom
Residual deviance: 341.21  on 296  degrees of freedom
AIC: 349.21

Number of Fisher Scoring iterations: 3

&gt; 
&gt; dat &lt;- data.frame(x1=1, x2=1, x3=seq(0,5,0.1))
&gt; preds &lt;- predict(model, dat,type = ""link"", se.fit = TRUE )
&gt; critval &lt;- 1.96
&gt; upr &lt;- preds$fit + (critval * preds$se.fit)
&gt; lwr &lt;- preds$fit - (critval * preds$se.fit)
&gt; fit &lt;- preds$fit
    &gt; 
    &gt; fit2 &lt;- mod$family$linkinv(fit)
    &gt; upr2 &lt;- mod$family$linkinv(upr)
    &gt; lwr2 &lt;- mod$family$linkinv(lwr)
    &gt; 
    &gt; plot(dat$x3, fit2, lwd=2, type=""l"", main=""Predicted Probability"", ylab=""Probability"", xlab=""x3"", ylim=c(0,1.00))
&gt; lines(dat$x3, upr2, lty=2)
    &gt; lines(dat$x3, lwr2, lty=2)
</code></pre>

<p><img src=""http://i.stack.imgur.com/ljW7W.png"" alt=""enter image description here""></p>

<p>Thanks!</p>

<p>Emilia</p>
"
"0.0960030721474639","0.0920574617898323"," 73950","<p>My question is similar to this link <a href=""http://stats.stackexchange.com/questions/12425/creating-a-certainty-score-from-the-votes-in-random-forests"">Creating a &quot;certainty score&quot; from the votes in random forests?</a></p>

<p>I am trying to build a random forest for a binary response (1 &amp; 0). Let's say we have 10,000 different records and I am building 500 trees. Is there a way to score the records in terms of the certainty / confidence / likelihood of being categorized in category 1 (for example)? The link above suggests using the number of votes among all 500 trees, but this way can only give me up to 500 different scores, how can I differentiate further for these 10,000 records? (Like regression, the scores can be easily obtained). </p>

<p>One solution is to average the score of each tree in the forest. the tree is the probability of 1s in the final node. Anyone know how to produce that average in R? I couldnt find this in the randomForest package. I think if I write my own codes to do that it , the run time may not be as fast as a built-in function. </p>
"
"0.118797740827873","0.130188910980824"," 77057","<p>I have a data set consisting of six independent environmental variables (all binomial: present / absent) and one dependent variable (binomial: disease present / absent). 
In order to determine the combination of factors that have the highest probability of leading to disease, I first need to conduct an Expert Opinion poll where I will have several experts rank all possible combinations of variables according to their probability of leading to the occurrence of disease. Then, I will obtain regression parameters for each variable using a conjoint analysis approach where each expert conforms a level (hierarchical design), the six environmental variables are independent variables, and the rankings are the dependent variable. There being six factor variables, there exist a total of 64 possible orthogonal combinations. 
I reduced this overwhelming number of possible combinations (while retaining orthogonality) using the <code>AlgDesign</code> package of R. Here is the code followed only by relevant pieces of output:</p>

<pre><code>levels.design = c(2,2,2,2,2,2)
full.design &lt;- gen.factorial(levels.design)

   X1 X2 X3 X4 X5 X6
1  -1 -1 -1 -1 -1 -1
2   1 -1 -1 -1 -1 -1
3  -1  1 -1 -1 -1 -1
   .................
63 -1  1  1  1  1  1
64  1  1  1  1  1  1

set.seed(69)
fractional &lt;- optFederov(~., data=full.design, approximate=FALSE, criterion=""D"")
fractional
</code></pre>

<p>The result is a subset of 12 combinations to be included in the conjoint analysis:</p>

<pre><code>$design
    X1 X2 X3 X4 X5 X6
4   1  1 -1 -1 -1 -1
5  -1 -1  1 -1 -1 -1
    ................
57 -1 -1 -1  1  1  1 
</code></pre>

<p>From what I understand, doing a regression analysis on all 64 combinations should lead to the same regression parameters as those obtained if I use only the reduced set (i.e. 12 combinations from the fractional factorial design).  </p>

<p>Questions: </p>

<ol>
<li>Do the code and the resulting output make sense?</li>
<li>Could anyone point me to a good and simple reference on how this fractional design works? I am afraid I might be doing things wrong by selecting a subset that produces different results from those obtained if a full factorial design was employed.</li>
</ol>
"
"0.135768846660426","0.130188910980824"," 81120","<p>I frequently use this model to test catch efficiency and size selection properties of a given trawl fishing gear:</p>

<p>\begin{equation}
\theta(l)=\frac{s\times r(l)}{(1-s)+s\times r(l)}
\end{equation}</p>

<p>where $\theta(l)$ denotes the expected catch rate in the test gear ($T$), which has been fishing in parallel with a non-selective gear (the control gear with blind meshes,$C$). The parameters affecting $\theta(l)$ are:</p>

<ul>
<li><p><strong>Split parameter</strong> $(s)$: It defines the probability of a fish to enter in $T$ ($s$) or in $C$ ($1-s$), $s\in\{0,1\}$</p></li>
<li><p><strong>Fish size selection</strong> ($r(l)$): It defines the likelihood of fish retention in $T$. This likelihood is conditioned to fish body length, therefore it describes the size selection in $T$. Fish size selection is used to be defined using the logit function:</p></li>
</ul>

<p>\begin{equation}
r(l)=\frac{exp(\beta_1+\beta_2\times l )}{1+exp(\beta_1+\beta_2\times l )} 
\end{equation}</p>

<p>Overall, there is a total of 3 parameters to be estimated ($s$, $\beta_1$, $\beta_2$). We use nonlinear regression techniques to estimate such parameters by maximizing the binomial log-likelihood function:</p>

<p>\begin{equation}
\sum_l(N_{l}^T\times \log\theta(l)+N_{l}^C\times \log(1-\theta(l))) 
\end{equation}</p>

<p>where $N_{l}^T$ is the number of fishes per length-class caught in $T$, and $N_{l}^C$ is the numbers caught in $C$.</p>

<p>During a normal experiment, we deploy the pair of gears ($T$ and $C$) several times to perform parallel fishing. To account for the between-haul variation, we use the bootstrap (using a resampling scheme based on resampling between hauls and fishes within hauls) to estimate the errors of $s$, $\beta_1$ and $\beta_2$.</p>

<p>IÂ´m wondering if itÂ´s possible to shift towards a nonlinear mixed modeling approach, where the hauls are considered as a random component. At the moment I only could find such approach by using least squares as minimization criteria. But I could not find a way to keep using the log-likelihood binomial mass function as target criteria.</p>

<p>Thank you beforehand for any comment or guidance.</p>
"
"0.083141099321054","0.0797241005179101"," 82139","<p>I want to predict if a customer is interested in a new product  and I use the randomForest package for that.</p>

<p>Target variable : factor (Yes or No) <em>so I use the randomForest for classification</em> :</p>

<pre><code>randomForest(x=train,y=labels_train,xtest=test, ytest=labels_test,  ntree=100)
</code></pre>

<p>Variables : </p>

<ul>
<li>the city (A,B,C)</li>
<li>Gender : Male / Female</li>
<li>Age classes : 18-25, 25-59 ,60 and +</li>
</ul>

<p><strong>Problem :</strong> only 40% of the residents of the city A are interested (same proportion for both gender and all age classes).</p>

<p>In this case, ALL the tree of the forest assign ""Not interested"" for the residents of this city.</p>

<p>According to the forest these residents have a probability of 0% to be interested by the product.</p>

<p><strong>Proposition :</strong> Using regression instead of classification.
In this way, each tree will declare 40%, and the vote is replace by an average wich give a probability
of 40%</p>

<p>Could you confirm it's correct to do that when the goal is not really to classify but to have a probability of interest,
in order to only contact people who have a probability higher than 30% for example.</p>
"
"0.083141099321054","0.0797241005179101"," 83576","<p>I have a feature x, that I use to predict a probability y.</p>

<hr>

<p><strong>Some background on (x,y)</strong></p>

<p>I can't go into too much details, but hopefully the following should be enough to explain what x and y are, at least conceptually <em>[square and circles are NOT the actual label I am working with]</em>:</p>

<p><strong>y</strong></p>

<p>y is the probability of an image being of Class 0 or 1, with: </p>

<ul>
<li>Class 0 means that the image contains a <em>square</em>.</li>
<li>Class 1 means that the image contains a <em>circle</em>.</li>
</ul>

<p>100 people watched the training images, and classified them.
y is the result probability, so y=0 means there is definitely a square, y=1 means there is definitely a round.</p>

<p><strong>x</strong></p>

<p>x is a feature derived from the images, by <em>trying to fit them to a model of a circle</em>, and calculating the error.
So for example when x is very low, the probability of the image having a circle is high (relatively).</p>

<hr>

<p>plot(x,y)</p>

<p><img src=""http://i.stack.imgur.com/05230.png"" alt=""enter image description here""></p>

<p>x,y (1000 values for each) pasted here:
<a href=""http://tny.cz/c320180d"" rel=""nofollow"">http://tny.cz/c320180d</a></p>

<p>Using mean(y) as a predictor, I get <strong>RMSE = 0.285204</strong>:</p>

<pre><code>N = length(x)
average = mean(y)
RMSE = sqrt( 1/N * sum( (average-y)^2 ) )
RMSE
[1] 0.285204
</code></pre>

<p>Then using a linear regression on log(x), I could improve a little bit the <strong>RMSE = 0.2694513</strong>:</p>

<pre><code>log_x = log(x)
plot(log_x,y)
lm.result = lm(formula = y ~ log_x)
abline(lm.result, col=""blue"") # not working very well
linear_prediction = predict( lm.result, new, se.fit = TRUE)
prediction_linear_regression = matrix(0,N,1)
prediction_linear_regression = linear_prediction$fit
RMSE_linear_regression = sqrt( 1/N * sum( (prediction_linear_regression-y)^2 ) )
RMSE_linear_regression
[1] 0.2694513
</code></pre>

<p><img src=""http://i.stack.imgur.com/59Etc.png"" alt=""enter image description here""></p>

<p>Can the RMSE be further improved? What should I try?</p>
"
"NaN","NaN"," 83777","<p>I computed a feature <code>x</code> that I use to predict <code>y</code> which is a <strong>probability</strong> of being a certain class.</p>

<p><strong>Raw data in R format for (x,y) is pasted here:</strong> 
<a href=""http://tny.cz/a97b3fd0"" rel=""nofollow"">http://tny.cz/a97b3fd0</a>
(500 samples)</p>

<p>plot(x,y) looks like this:</p>

<p><img src=""http://i.stack.imgur.com/sKoJo.png"" alt=""enter image description here""></p>

<p>It seems similar to a <a href=""http://stats.stackexchange.com/questions/83554/linear-regression-not-fitting-well"">previous question</a> I asked, so I am tempted to try <a href=""http://stats.stackexchange.com/a/83613/21720"">this advice</a>, which I understand is the ""Least Squared Fit""on log(y) (because y is a probability).</p>

<p>Am I correct?</p>

<p>(My objective is to minimize the MSE.)</p>
"
"0.0783861801669621","0.0751646028002829"," 83908","<p>I am working in program R. I am modeling the incidence of flight in a seabird in relation to distance to the nearest ship (potential disturbance, range = 0 to 74 km from the bird). 1= flight during observation, 0 = no flight. The bird does fly with some unknown probability when no ships are present or really ""far"" away. I am trying to find this really far distance and associated probability of flight using binary logistic regression.</p>

<p>Model = Flight ~ ship distance. Other variables were explored but fell out with stepwise selection.</p>

<p>During exploratory analysis I truncated the data down only looking at smaller distances from the ship (20, 15, 10 km). These models are highly significant and predict that as the ship gets closer the probability of flight increases. However when I include all the data (out to 74 km) the intercept is significant (and predicts the true % of observed flight events) but the slope term is non-significant. </p>

<p>Can I use a weighting scheme to give more weight to observations when the ship was closer?</p>

<p>Thanks.</p>

<p>Edit: I am working through the suggestions made by @Scortchi and @Underminer. Here is a plot of a loess smooth on the observed data to better help visualize the pattern. </p>

<p><img src=""http://i.stack.imgur.com/ZabQh.jpg"" alt=""Loess smooth of probability of flight as a function of distance to nearest ship""></p>

<p>The distance to the ship data does not discriminate between approaching ships and departing ships it is just a straight line measure to the nearest ship. The dip in the probability of flight at 8.5 I believe can be attributed to ""unaffected"" birds that did not fly as the ship passed by them. So as the ship departs and gets further from the observation site we were more like to be observing birds that for whatever reason did not fly when the ship passed and are less likely to fly for ""naturally occurring"" reason. As additional birds fill back into the observation area the ""baseline"" flushing rate is resumed and birds start to fly at ""normal"" probabilities. </p>
"
"0.0678844233302131","0.0650944554904119"," 83945","<p>How do you solve the following problem?</p>

<blockquote>
  <p>A Simulation Study (Probit Regression).</p>
  
  <p>Assume $y|x\sim {\rm Binary}(p)$, where $p= E(y|x)$, and $Î¦^{-1}(\pi)=-1+5.1x_{1i}-0.3x_{2i}$
  Generate data with $x_{1i}\sim{\rm Unif}(0,1)$, $x_{2i}=1$ for $i$ odd and $x_{2i}=0$ for $i$ even, and sample size $n=500$. Try generalized linear model (GLM) with logistic and probit links.</p>
</blockquote>

<p>Here is what I did, I know there is a problem, but I don't know what:</p>

<pre><code>n         &lt;- 500
beta0     &lt;- -1
beta1     &lt;- 5.1
beta2     &lt;- -0.3
x1        &lt;- runif(n=n, min=0, max=1)
x2        &lt;- (1:n)%%2
y         &lt;- pnorm(beta0 + beta1*x1 + beta2*x2)
prob.glm  &lt;- glm(y~x1+x2, family=binomial(link=probit))
logit.glm &lt;- glm(y~x1+x2, family=binomial)
</code></pre>

<p>I know <code>y</code> is a probability here, but how do you simulate a binary variable from the probability? </p>
"
"0.20923384488012","0.200634586470368"," 87359","<p><strong>Background</strong></p>

<p>I have a large dataset that contains three binary outcomes for individuals belonging to groups. I am interested in jointly modeling these binary outcomes because I have reason to believe they are positively correlated with one another. Most of my data is at the individual level, however I also have some group-level information.</p>

<p>Because of the structure of my data, I am treating this as a 3-level logistic regression. The first level defines the multivariate structure through dummy variables that indicate for each outcome. Therefore level-1 accounts for the within-individual measurements. Level-2 provides the between-individuals variances and level-3 gives the between-group variance.</p>

<p><em>Hypothetical Example:</em></p>

<p>Suppose I have data for students in classrooms. I want to examine whether certain student level characteristics are important predictors for passing three different pre-tests (math, history, and gym). The pre-tests are constructed such that about half of the students should pass each exam (no floor or ceiling effect). Since some students are better than others, I expect whether or not they pass their history exam to be correlated to their probability of passing their math and gym pre-tests. I also expect that students in the same classroom will perform more similarly than students across classrooms.</p>

<p><strong>Here is my attempt at writing out the model</strong></p>

<p>I use $h$ to index level-1, $i$ to index level-2, and $j$ to index level-3. Recall that level-1 corresponds to within-individual, level-2 corresponds to between-individuals, and level-3 corresponds to between-groups. So I have $h$ measures for the $i^\text{th}$ individual in the $j^\text{th}$ group.</p>

<p>Let 
\begin{align}
\alpha_{1ij} &amp;= 1 \text{ if outcome}_1 = 1 \text{ and 0 otherwise} \\
\alpha_{2ij} &amp;= 1 \text{ if outcome}_2 = 1 \text{ and 0 otherwise} \\
\alpha_{3ij} &amp;= 1 \text{ if outcome}_3 = 1 \text{ and 0 otherwise}
\end{align}</p>

<p>\begin{align}
\text{log}\left( \frac{\pi_{hij}}{1 - \pi_{hij}} \right) &amp;= \alpha_{0hij} + \alpha_{1hij}Z_{ij} + \eta_h \\
Z_{ij} &amp;= \beta_{0ij} + X_{ij}\beta_{ij} + U_{j} + \epsilon_i \\
U_{j} &amp;= \gamma_{0j} + X_{j}\gamma_{j}
 + \rho_j
\end{align}</p>

<p>Please leave suggests about this notation; I am not positive that it is correct.</p>

<p><strong>Trying to specify the model in R</strong></p>

<p>I have been using R 3.0.1, but am open to solutions using other standard software (e.g. SAS, Stata, WinBUGs, etc.). Since I am modeling a binary response I am using the <code>glmer</code> function in the <code>lme4</code> package.</p>

<p>My data is in a long format with one row per outcome per individual per group.</p>

<p>One of my current problems is correctly specifying a 3-level model. Given 5 individual-level measures and one group level measure, I have tried to specify the model as:</p>

<pre><code>&gt; glmer(pi ~ outcome1:(x1 + x2 + x3 + x4 + u5) + 
             outcome2:(x1 + x2 + x3 + x4 + u5) +
             outcome3:(x1 + x2 + x3 + x4 + u5) +
             (1 + x1 + x2 + x3 + x4 | individual) +
             (1 + u5 | group), family=binomial, data=pretest)
</code></pre>

<p>but this often produces warnings and does not converge.</p>

<p><strong>My questions</strong></p>

<ol>
<li><p>Does my approach make sense? (i.e. does it make sense to model a multilevel multivariate model by specifying the multivariate structure in the first level?)</p></li>
<li><p>I have difficult with the proper notation and appreciate suggestions for clarifying my notation.</p></li>
<li><p>Am I using the right tools for this problem? Should I be using other packages or software?</p></li>
</ol>

<p>Thanks in advance for your suggestions and advice.</p>
"
"0.185909149805938","0.178268508203073"," 87487","<p><strong>Short version</strong></p>

<p>Is there a difference <strong>per treatment</strong> given time and this dataset?</p>

<p><strong>Or</strong> if the difference we're trying to demonstrate is important, what's the best method we have for teasing this out?</p>

<p><strong>Long version</strong></p>

<p>Ok, sorry if a bit <em>biology 101</em> but this appears to be an edge case where the data and the model need to line up in the right way in order to draw some conclusions. </p>

<p>Seems like a common issue... Would be nice to demonstrate an intuition rather than repeating this experiment with larger sample sizes. </p>

<p>Let's say I have this graph, showing mean +- std. error:</p>

<p><img src=""http://i.stack.imgur.com/eIKeF.png"" alt=""p1""></p>

<p>Now, it looks like there's a difference here. Can this be justified (avoiding Bayesian approaches)?</p>

<p>The simpleminded man's  approach would be to take Day 4 and apply a <em>t-test</em> (as usual: 2-sided, unpaired, unequal variance), but this doesn't work in this case. It appears the variance is too high as we only had 3x measurements per time-point (err.. mostly my design, p = 0.22).</p>

<p><strong>Edit</strong> On reflection the next obvious approach would be ANOVA on a linear regression. Overlooked this on first draft. This also doesn't seem like the right approach as the usual linear model is impaired from heteroskedasticity (<em>exaggerated variance over time</em>). <strong>End Edit</strong></p>

<p>I'm guessing there's a way to include <strong>all</strong> the data which would fit a simple (1-2 parameter) model of growth over time per predictor variable then compare these models using some formal test. </p>

<p>This method should be justifiable yet accessible to a relatively unsophisticated audience.</p>

<p>I have looked at <code>compareGrowthCurves</code> in <a href=""http://cran.r-project.org/web/packages/statmod/statmod.pdf"" rel=""nofollow"">statmod</a>, read about <a href=""http://www.jstatsoft.org/v33/i07/paper"" rel=""nofollow"">grofit</a> and tried a linear mixed-effects model adapted from <a href=""http://stats.stackexchange.com/questions/61153/nlme-regression-curve-comparison-in-r-anova-p-value"">this question on SE</a>. This latter is closest to the bill, although in my case the measurements are not from the <strong>same subject</strong> over time so I'm not sure mixed-effects/multilevel models are appropriate. </p>

<p>One sensible approach would be to model the rate of growth per time as linear and fixed and have the random effect be <strong>Tx</strong> then <a href=""http://www.statistik.uni-dortmund.de/useR-2008/slides/Scheipl+Greven+Kuechenhoff.pdf"" rel=""nofollow"">test it's significance</a>, although I gather there's <a href=""http://glmm.wikidot.com/faq"" rel=""nofollow"">some debate</a> about the merits of such an approach.</p>

<p>(Also this method specifies a linear model which would not appear to be the best way to model a comparison of growth which in the case of one predictor has not yet hit an upper boundary and in the other appears basically static. I'm guessing there's a generalized mixed-effects model approach to this difficulty which would be more appropriate.)</p>

<p>Now the code:</p>

<pre><code>df1 &lt;- data.frame(Day = rep(rep(0:4, each=3), 2),
              Tx = rep(c(""Control"", ""BHB""), each=15),
              y = c(rep(16e3, 3),
              32e3, 56e3, 6e3,
              36e3, 14e3, 24e3,
              90e3, 22e3, 18e3,
              246e3, 38e3, 82e3,
              rep(16e3, 3),
              16e3, 34e3, 16e3,
              20e3, 20e3, 24e3,
              4e3, 12e3, 16e3,
              20e3, 5e3, 12e3))
### standard error
stdErr &lt;- function(x) sqrt(var(x)) / sqrt(length(x))
library(plyr)
### summarise as mean and standard error to allow for plotting
df2 &lt;- ddply(df1, c(""Day"", ""Tx""), summarise,
             m1 = mean(y),
             se = stdErr(y) )
library(ggplot2)
### plot with position dodge
pd &lt;- position_dodge(.1)
ggplot(df2, aes(x=Day, y=m1, color=Tx)) +
 geom_errorbar(aes(ymin=m1-se, ymax=m1+se), width=.1, position=pd) +
 geom_line(position=pd) +
 geom_point(position=pd, size=3) +
 ylab(""No. cells / ml"")
</code></pre>

<p>Some formal tests:</p>

<pre><code>### t-test day 4
with(df1[df1$Day==4, ], t.test(y ~ Tx))
### anova
anova(lm(y ~ Tx + Day, df1))
### mixed effects model
library(nlme)
f1 &lt;- lme(y ~ Day, random = ~1|Tx, data=df1[df1$Day!=0, ])
library(RLRsim)
exactRLRT(f1)
</code></pre>

<p>this last giving</p>

<pre><code>    simulated finite sample distribution of RLRT.  (p-value based on 10000
    simulated values)

data:  
RLRT = 1.6722, p-value = 0.0465
</code></pre>

<p>By which I conclude that the probability of this data (or something more extreme), <em>given the null hypothesis that there is no influence of <strong>treatment</strong> on <strong>change over time</em></strong> is close to the elusive 0.05. </p>

<p>Again, sorry if this appears a bit basic but I feel a case like this could be used to illustrate the importance of modelling in avoiding further needless experimental repetition. </p>
"
"0.108857251714449","0.104383350095883"," 88796","<p>I am exploring the probability of flight in a seabird (1=flight, 0=no flight) using binomial logistic regression. My predictors are distance to a disturbance (continuous), hour of the day (continuous), site (factor), season (factor), sea state (dichotomous), and group size (dichotomous). I have explored the use of piecewise regression in relation to the distance to a disturbance as this variable spans a large range (out to 74 km) and there is no way that this is affecting flight at the largest distance. </p>

<p>When the model was fit with just reference to distance to a disturbance within the R program 'segmented' it points to a break in the data at 3.9 km. The slope up to this distance is negative and statistically significant while the slope estimate for distances further than 3.9 km is estimated to be 0 and non-significant.</p>

<p>I would like to now sequentially add in additional terms to the model to see if there is any reduction in the deviance when the additional terms are added. Can a term be added just to the section before or after the break? I cannot seem to find any information in the literature regarding this </p>

<p>My questions is can I do this? Or do I need to split the data into two chunks, before and after the breakpoint and explore additional terms this way.</p>

<p>Also the motivation to do this analysis is more to find and identify the breakpoint. Instead of adding in terms after I assess the breakpoint should I explore the breakpoint within a the model including all the terms? Would this find the break in the data in relation to the other terms or does the algorithm completely ignore the other terms in the model when searching for a break in the distance to disturbance variable.</p>

<p>Thanks, </p>
"
"0.220198267701583","0.220746038603917"," 89204","<p>I'm looking for advice on how to analyze complex survey data with multilevel models in R. I've used the <code>survey</code> package to weight for unequal probabilities of selection in one-level models, but this package does not have functions for multilevel modeling. The <code>lme4</code> package is great for multilevel modeling, but there is not a way that I know to include weights at different levels of clustering. <a href=""http://www.statmodel.com/download/asparouhovgmms.pdf"">Asparouhov (2006)</a> sets up the problem:</p>

<blockquote>
  <p>Multilevel models are frequently used to analyze data from cluster sampling designs. Such sampling designs however often use unequal probability of selection at the cluster level and at the individual level. Sampling weights are assigned at one or both levels to reflect these probabilities. If the sampling weights are ignored at either level the parameter estimates can be substantially biased.</p>
</blockquote>

<p>One approach for two-level models is the multilevel pseudo maximum likelihood (MPML) estimator that is implemented in MPLUS (<a href=""http://www.statmodel.com/download/SurveyJSM1.pdf"">Asparouhov et al, ?</a>). <a href=""http://www.biomedcentral.com/1471-2288/9/49"">Carle (2009)</a> reviews major software packages and makes a few recommendations about how to proceed: </p>

<blockquote>
  <p>To properly conduct MLM with complex survey data and design weights, analysts need software that can include weights scaled outside of the program and include the ""new"" scaled weights without automatic program modification. Currently, three of the major MLM software programs allow this: Mplus (5.2), MLwiN (2.02), and GLLAMM. Unfortunately, neither HLM nor SAS can do this.</p>
</blockquote>

<p><a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3630376/"">West and Galecki (2013)</a> give a more updated review, and I'll quote the relevant passage at length:</p>

<blockquote>
  <p>Occasionally, analysts wish to fit LMMs to survey data sets collected from samples with complex designs (see Heeringa et al, 2010, Chapter 12). Complex sample designs are generally characterized by division of the population into strata, multi-stage selection of clusters of individuals from within the strata, and unequal probabilities of selection for both clusters and the ultimate individuals sampled. These unequal probabilities of selection generally lead to the construction of sampling weights for individuals, which ensure unbiased estimation of descriptive parameters when incorporated into an analysis. These weights might be further adjusted for survey nonresponse and calibrated to known population totals. Traditionally, analysts might consider a design-based approach to incorporating these complex sampling features when estimating regression models (Heeringa et al., 2010). More recently, statisticians have started to explore model-based approaches to analyzing these data, using LMMs to incorporate fixed effects of sampling strata and random effects of sampled clusters.</p>
  
  <p>The primary difficulty with the development of model-based approaches to analyzing these data has been choosing appropriate methods for incorporating the sampling weights (see Gelman, 2007 for a summary of the issues). Pfeffermann et al. (1998), Asparouhov and Muthen (2006), and Rabe-Hesketh and Skrondal (2006) have developed theory for estimating multilevel models in a way that incorporates the survey weights, and Rabe-Hesketh and Skrondal (2006), Carle (2009) and Heeringa et al. (2010, Chapter 12) have presented applications using current software procedures, but this continues to be an active area of statistical research. Software procedures capable of fitting LMMs are at various stages of implementing the approaches that have been proposed in the literature thus far for incorporating complex design features, and analysts need to consider this when fitting LMMs to complex sample survey data. Analysts interested in fitting LMMs to data collected from complex sample surveys will be attracted to procedures that are capable of correctly incorporating the survey weights into the estimation procedures (HLM, MLwiN, Mplus, xtmixed, and gllamm), consistent with the present literature in this area.</p>
</blockquote>

<p>This brings me to my question: does anyone have best practice recommendations for fitting LMMs to complex survey data in R?</p>
"
"NaN","NaN"," 90104","<p>Given a linear regression model with all the assumptions checked and validated, I would like to obtain the probability that $Y&gt;y|X=x$. For example for the iris dataset, I would do the following to obtain the probability of $Y&gt;5|X=1,2,3...7$:</p>

<pre><code>plot(Sepal.Length~Petal.Length, data=iris)
lm1&lt;-lm(Sepal.Length~Petal.Length, data=iris)
summary(lm1)
abline(lm1)
predict(lm1, newdata=data.frame(Petal.Length=1:7))
(summary(lm1))$sigma
    pnorm(5, mean = predict(lm1, newdata=data.frame(Petal.Length=1:7)),
        sd = (summary(lm1))$sigma, lower.tail = F)
</code></pre>

<p>Is such an approach correct assuming constant variance?</p>
"
"0.118797740827873","0.130188910980824"," 91903","<p>Is the probability calculated by a logistic regression model (the one that is logit transformed) the fit of cumulative distribution function of successes of original data (ordered by the X variable)?</p>

<p><strong>EDIT:</strong> In other words - how to plot the probability distribution of the original data that you get when you fit a logistic regression model?</p>

<p>The motivation for the question was Jeff Leak's example of regression on the Raven's score in a game and whether they won or not (from Coursera's Data Analysis course). Admittedly, the problem is artificial (see @FrankHarrell's comment below). Here is his data with a mix of his and my code:</p>

<pre><code>download.file(""http://dl.dropbox.com/u/7710864/data/ravensData.rda"", 
              destfile=""ravensData.rda"", method=""internal"")
load(""ravensData.rda"")

plot(ravenWinNum~ravenScore, data=ravensData)
</code></pre>

<p><img src=""http://i.stack.imgur.com/Cr5ka.png"" alt=""enter image description here"">  </p>

<p>It doesn't seem like good material for logistic regression, but let's try anyway:</p>

<pre><code>logRegRavens &lt;- glm(ravenWinNum ~ ravenScore, data=ravensData, family=binomial)
summary(logRegRavens)
# the beta is not significant

# sort table by ravenScore (X)
rav2 = ravensData[order(ravensData$ravenScore), ]

# plot CDF
plot(sort(ravensData$ravenScore), cumsum(rav2$ravenWinNum)/sum(rav2$ravenWinNum), 
         pch=19, col=""blue"", xlab=""Score"", ylab=""Prob Ravens Win"", ylim=c(0,1), 
         xlim=c(-10,50))
# overplot fitted values (Jeff's)
points(ravensData$ravenScore, logRegRavens$fitted, pch=19, col=""red"")
# overplot regression curve
curve(1/(1+exp(-(logRegRavens$coef[1]+logRegRavens$coef[2]*x))), -10, 50, add=T)
</code></pre>

<p>If I understand logistic regression correctly, R does a pretty bad job at finding the right coefficients in this case.  </p>

<p><img src=""http://i.stack.imgur.com/Cb6o8.png"" alt=""enter image description here""></p>

<ul>
<li>blue = original data to be fitted, I believe (CDF)  </li>
<li>red = prediction from the model (fitted data = projection of original data onto regression curve)</li>
</ul>

<p><strong>SOLVED</strong><br>
 - lowess seems to be a good non-parametric estimator of the original data = what is being fitted (thanks @gung). Seeing it allows us to choose the right model, which in this case would be adding squared term to the previous model (@gung)<br>
 - Of course, the problem is pretty artificial and modelling it rather pointless in general (@FrankHarrell)<br>
 - in regular logistic regression it's not CDF, but point probabilities - first pointed out by @FrankHarrell; also my embarrassing inability to calculate CDF pointed out by @gung.</p>
"
"0.151794185179729","0.145555627434896"," 94089","<p>I started to use the function <code>multinom</code> of <code>R</code> package <code>nnet</code> in order to fit several conditional probability distributions with the multinomial logistic model. I need the parameters of the fittings in order to pass them to a Java program, which will compute the probabilities and use them.</p>

<p>My problem is that the probabilities computed with the parameters returned by <code>multinom</code>, following the usual <a href=""http://en.wikipedia.org/wiki/Multinomial_logistic_regression#As_a_set_of_independent_binary_regressions"" rel=""nofollow"">definition</a> of multinomial logistic model, are not the same as those directly computed in <code>R</code>, which are the correct ones. On Stack Overflow I have already asked a <a href=""http://stackoverflow.com/questions/22905807/how-does-the-function-multinom-from-r-package-nnet-compute-the-multinomial-proba"">question</a> about this issue, but I do not still know how the <code>R</code> function <code>multinom</code> computes these probabilities; my guess is that it relies on neural networks, since this function belongs to <code>R</code> package <code>nnet</code>, but I do not have any idea about the details, and an inspection of the code led to nowhere.</p>

<p>Do you know an <code>R</code> package which fits conditional probabilities and returns the corresponding parameters of the model, so that we may easily compute the probabilities in another program? E.g., using the MARS model (<code>R</code> package <code>earth</code>) or Projection Pursuit Regression (<code>R</code> package <code>ppr</code>) is not feasible, since computing the probabilities from the parameters of these models would be a mess. Besides, the function <code>mlogit</code> from the <code>R</code> package with the same name is not applicable as well, since the dataset should be in a certain format (we would also need the predictors corresponding to the alternative, ""non-chosen"" response variable).</p>
"
"0.0678844233302131","0.0650944554904119"," 94257","<p>I'm working with a biased sample of web users.  I'm only able to track responses of users who have navigated my site in a certain way, and I'd like to run an analysis to determine how certain factors (which products they are into, etc) influence how much they buy from my online store.</p>

<p>I've searched around online and found some information on the Heckman Correction, which somehow both won a Nobel Prize and also has extremely little information online, including no R support or youtube videos that I can find.</p>

<p>What are some ways you work with building models that try to correct for biased data?  Are there easy to use software libraries in R?  I have a ton of unlabeled data (products that every user has seen), so it's possible that I could create a naive model for determining probability that a user ends up in my sample if that helps, and I'd be willing to work with more advanced learners like SVM (which I've read can work better for these types of problems) in case linear regressions are a bad choice.</p>
"
"0.159758768875269","0.165958949386154"," 94581","<p>I have a ordinal dependendent variable, easiness, that ranges from 1 (not easy) to 5 (very easy).  Increases in the values of the independent factors are associated with an increased easiness rating.</p>

<p>Two of my independent variables (<code>condA</code> and <code>condB</code>) are categorical, each with 2 levels, and 2 (<code>abilityA</code>, <code>abilityB</code>) are continuous.</p>

<p>I'm using the <a href=""http://cran.r-project.org/web/packages/ordinal/index.html"">ordinal</a> package in R, where it uses what I believe to be</p>

<p>$$\text{logit}(p(Y \leqslant g)) = \ln \frac{p(Y \leqslant g)}{p(Y &gt; g)} = \beta_{0_g} - (\beta_{1} X_{1} + \dots + \beta_{p} X_{p}) \quad(g = 1, \ldots, k-1)$$<br>
(from @caracal's answer <a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">here</a>)</p>

<p>I've been learning this independently and would appreciate any help possible as I'm still struggling with it.  In addition to the tutorials accompanying the ordinal package, I've also found the following to be helpful: </p>

<ul>
<li><a href=""http://stats.stackexchange.com/questions/89474/interpretation-of-ordinal-logistic-regression"">Interpretation of ordinal logistic regression</a></li>
<li><a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">Negative coefficient in ordered logistic regression</a></li>
</ul>

<p>But I'm trying to interpret the results, and put the different resources together and am getting stuck. </p>

<ol>
<li><p>I've read many different explanations, both abstract and applied, but am still having a hard time wrapping my mind around what it means to say: </p>

<blockquote>
  <p>With a 1 unit increase in condB (i.e., changing from one level to the next of the categorical predictor), the predicted odds of observing Y = 5 versus Y = 1 to 4 (as well as the predicted odds of observed Y = 4 versus Y = 1 to 3) change by a factor of exp(beta) which, for diagram, is exp(0.457) = 1.58. </p>
</blockquote>

<p>a. Is this different for the categorical vs. continuous independent variables?<br>
b. Part of my difficulty may be with the cumulative odds idea and those comparisons. ... Is it fair to say that going from condA = absent (reference level) to condA = present is 1.58 times more likely to be rated at a higher level of easiness?  I'm pretty sure that is NOT correct, but I'm not sure how to correctly state it.</p></li>
</ol>

<p>Graphically,<br>
1. Implementing the code in <a href=""http://stats.stackexchange.com/questions/89474/interpretation-of-ordinal-logistic-regression"">this post</a>, I'm confused as to why the resulting 'probability' values are so large.<br>
2. The graph of p (Y = g) in <a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">this post</a> makes the most sense to me ... with an interpretation of the probability of observing a particular category of Y at a particular value of X.  The reason I am trying to get the graph in the first place is to get a better understanding of the results overall.</p>

<p>Here's the output from my model:</p>

<pre><code>m1c2 &lt;- clmm (easiness ~ condA + condB + abilityA + abilityB + (1|content) + (1|ID), 
              data = d, na.action = na.omit)
summary(m1c2)
Cumulative Link Mixed Model fitted with the Laplace approximation

formula: 
easiness ~ illus2 + dx2 + abilEM_obli + valueEM_obli + (1 | content) +  (1 | ID)
data:    d

link  threshold nobs logLik  AIC    niter     max.grad
logit flexible  366  -468.44 956.88 729(3615) 4.36e-04
cond.H 
4.5e+01

Random effects:
 Groups  Name        Variance Std.Dev.
 ID      (Intercept) 2.90     1.70    
 content  (Intercept) 0.24     0.49    
Number of groups:  ID 92,  content 4 

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)    
condA              0.681      0.213    3.20   0.0014 ** 
condB              0.457      0.211    2.17   0.0303 *  
abilityA           1.148      0.255    4.51  6.5e-06 ***
abilityB           0.577      0.247    2.34   0.0195 *  

Threshold coefficients:
    Estimate Std. Error z value
1|2   -3.500      0.438   -7.99
2|3   -1.545      0.378   -4.08
3|4    0.193      0.366    0.53
4|5    2.121      0.385    5.50
</code></pre>
"
"0.203653269990639","0.195283366471236"," 95378","<p>I am doing statistics for the first time in my life and I am not quite sure what to include and how to interpret the results. I am doing a logistic regression in R. Here is what I have so far:</p>

<ol>
<li><p><code>GLM</code> with family = binomial (dependent ~ indep1 + indep2 + ...+ indep7  +0)
If I dont include the 0 I get NA for my last independent variable in the summary output..</p></li>
<li><p><code>Update</code> the model (indep2 has a p-value > 0.05 and is left out)</p></li>
<li><p>I am applying anova</p>

<pre><code>anova(original_model,updated_model, test=""Chisq"")

   Resid.Df  Resid.Dev Df Deviance Pr(&gt;Chi)
1     34067      18078                     
2     34066      18075  1   2.4137   0.1203
</code></pre>

<p>Here I am not sure how to interpret it. What tells me if the simplification of the model is significant? the p-value is with 0.12 bigger than 0.05, does this mean that the simplification is not significant? </p></li>
<li><p>make a cross-table (compare predicted (probability >0.5) - observed)</p>

<pre><code>fit
      FALSE  TRUE
  No  30572    68
  yes  3407    31
</code></pre>

<p>I'd say that 31 values are predicted correctly (yes-true), resp 68 (no-true) but that most values are classified wrong, which means that the model is really bad?</p></li>
<li><p>then I make a wald test for each independent variable for the first independent variable it would look like this:</p>

<pre><code>&gt; wald.test(b = coef(model_updated), Sigma = vcov(model_updated), Terms
&gt; = 1:1)
</code></pre>

<p>here I only look if the p-values are significant and if they are it means that all variables contribute significantly to the predictive ability of the model</p></li>
<li><p>I calculate the odds with their confidence intervals (this is basically exp(estimate)</p>

<pre><code>oddsCI &lt;- exp(cbind(OR = coef(model_updated), confint(model_updated)))
</code></pre>

<p>For all odds smaller than 1 i do 1/odd</p>

<pre><code>Estimate        Odds Ratio      Inverse Odds
-0.000203       0.999801041     1.000198999
 0.000332       1.000326571     odd bigger than 1
-0.000133       0.999846418     1.000153605
-3.48       0.008696665     114.9866056
-4.85       0.029747223     33.61658319
-2.37       0.000438382     2281.113996
-8.16       0.110348634     9.062187402
-2.93       0.062668509     15.95697759
-3.65       0.020156889     49.61083057
-5.45       0.033996464     29.41482359
-4.02       0.004837987     206.6975334
</code></pre>

<p>This O would interpret like that for the ""odd bigger than 1""  the case is over 1 times more likely to occur. (Is is incorrect to say that, or not?) Or for the last row you could say that t for every subtraction of a unit, the odds for the case to appear decreases by a factor of 206.</p></li>
<li><p>Then I look at </p>

<pre><code>with(model_updated, null.deviance - deviance) #deviance
with(model_updated, df.null - df.residsual) #degrees of freedom
 # pvalue
with(Amodel_updated, pchisq(null.deviance - deviance, df.null - df.residual, 
lower.tail = FALSE))
logLik(model_updated)
</code></pre>

<p>But I don't really know what this tells me.</p></li>
<li><p>In a last step I do</p>

<pre><code>stepAIC(model_updated, direction=""both"")
</code></pre>

<p>but also here I don't know how to interpret the outcome. I see that it looks at all interactions between my independent variables but I don't know what it tells me.</p></li>
</ol>

<p>After this, I can make a prediction by using the updated model and by separating it into training data and validation data I suppose?</p>
"
"0.107334697685273","0.102923371199077"," 96999","<p>I'm working on a model that requires me to look for predictors for a rare event (less than 0.5% of the total of my observations). My total sample is a significant part of the total population (50,000 cases). My final objective is to obtain comparable probability values for all the non-events, without the bias of the groups difference in the logistic regression. </p>

<p>I've been reading the info in the following link: </p>

<p><a href=""http://gking.harvard.edu/files/gking/files/0s.pdf"" rel=""nofollow"">http://gking.harvard.edu/files/gking/files/0s.pdf</a></p>

<p>It advises me first to use a sample of my original sample, containing all the events (1) and a random sample of 1-5 times bigger of the non-event (0) sample.</p>

<p>Then it suggests using weights based on the proportion of the sample 1s to 0s. In the section 4.2 of the linked text, he offers a ""easy to implement"" weighted log-likelihood that can be implemented in any logit function.</p>

<p>I wish to implement these weights somehow with R's glm(...,family=binomial(link=""logit"")) or similar function ( the ""weights"" parameter is not for frequency weighting), but I don't really know how to apply this weighting.</p>

<p>Does anybody knows how to make it or any other alternative suggestion?  </p>

<p><strong>Edit1:</strong> As suggested bellow, is Firth's method for bias-correction by penalizing the likelihood in the <code>logistf</code> package a correct approach in this case? I'm not much knowledgeable in statistics, and, while I understand the input and the coefficients/output of the logistic model, what happens in between is still quite a mystery to me, sorry.</p>
"
"0.127000127000191","0.121780575111864"," 99862","<p>Here's my situation.  </p>

<p>I have a multiple linear regression which I've used to come up with a prediction interval to predict a value y for a given (x1,x2,x3,x4,x5,x6).   It reads something like lower: 30, upper:48.  </p>

<p>I also have the same exact thing to predict a value y* at another given (x1*,x2*,x3*,x4*,x5*,x6*).  It reads something like lower:35, upper:51. </p>

<p>I want to answer this question:<br>
What is the probability that the value y* is greater than the value y?</p>

<p>I think it's a basic question, but I'm not sure. 
I could likely come up with this probability if I knew the formula for how the prediction interval is calculated in a multi-variable situation.<br>
Here's what I think should be done, but I wanted to run it by you guys first. </p>

<p>Prediction Intervals are based on a t-distribution with (n-6) degrees of freedom (I have a forced 0 y-int).  So I believe the margin of error calculated is then some constant multiplied by the corresponding value from the t-distribution (t_.05/2 with n-6 degrees of freedom).  The ""some constant"" would be the standard error of this particular estimate. </p>

<p>I then just do a basic 2 sample t-test using the point estimate prediction as the means and these constants as the standard errors with my n-6 degrees of freedom.   Is this accurate? </p>

<p>Is there a better way?</p>

<p>Thanks</p>
"
"0.0480015360737319","0.0460287308949162","105301","<p>Given the outcome variable in a dataframe is a factored/categorical variable, when regressing the dependent variable (DV) onto a set of independent variables (IVs), what is the model predicting? The probability that the DV is the first level of the factor? Or the second?</p>

<p>A related question: I know that given a numerical column of $1$s and $0$s, a logistic regression would model the probability of the higher order variable (i.e., value=$1$), so I have been attempting to recode the factor ""character"" variable into a numerical one. I am coming from a SAS background, so I am entirely to used to <code>if var = ""yes"" then var_num = 1; else var_num=0;</code></p>

<p>That's clearly wrong. What's the most efficient way you have found to recode such variables?</p>
"
"0.1746312383152","0.18978131929287","106259","<p>I use SPSS, but am forced to use R for exact logistic regression.  So I'm brand new to R (and hating it so far) and also new to logistic regression.  I've read the original elrm paper and looked at examples of its use. However, I can't find information on the questions below (after the data description).</p>

<p>The fit of two models of cognitive processing was compared for each subject in each of 3 conditions. My binary dependent variable is whether the difference in model fits was significant or not (my ""Success"" variable below). I have three experimental Conditions: 0, 1, and 2.  0 is my reference group.  My question is:  is there an overall effect of Condition? If so, which conditions differ?  The specific alternative hypothesis is that the proportion/probability of ""success"" should be greater in conditions 0 and 1 than in condition 2.  My data look like</p>

<p><img src=""http://i.stack.imgur.com/OAFtP.gif"" alt=""original data""></p>

<p>...and so on. SPSS actually creates the dummy variables for you on the fly but they are easy enough to create explicitly.</p>

<p><strong>Question 1:</strong>  I have read that to use elrm you have to enter the data such that the response variable represents success/number of trials. And as far as I can tell elrm doesn't create dummy variables automatically.  I've seen examples of tables representing this data structure, but can't find any step-by-step examples of getting raw data into that format, espescially given a one-variable 3-levels situation.  Is there an example out there that I'm missing?  If not, is this what the data should look like? </p>

<p><img src=""http://i.stack.imgur.com/aVxOL.gif"" alt=""reformated data""></p>

<p>I'm not sure how I'd enter the dummy variables into the formula...just as separate variables?</p>

<p><strong>Question 2:</strong>  I can see how I can get the tests of the coefficients of the dummy variables. But I can't figure out how to get a test of the overall effect of the independent variable. I need to evaluate the overall effect of Condition before looking at individual conditions.  Is there a way to get that out of elrm? (I found an example of this done for the aod package which runs regular logistic regression but not exact logistic regression.)</p>

<p><strong>Question 3:</strong>  I can't find a description of what the p-value for individual coeffeicients represents in elrm.  Is this is for the Wald test?</p>
"
"0.0979827252087026","0.112746904200424","107865","<p>I'm investigating environmental effects (wind) on acoustic receiver detection probability for two types of transmitters using a binomial glmer. While my model analysis indicates that there's a significant effect between wind speed and transmitter type, graphical visualisation does not confirm this. If I'm correct, an interaction should demonstrate different regression slopes.</p>

<pre><code>m1 &lt;- glmer(cbind(df$Valid.detections, df$False.Detections) ~ Transmitter.depth + 
               Receiver.depth + Water.temperature + Wind.speed + Transmitter + 
               Distance + Habitat + Replicate + (1 | Day) + (Distance | SUR.ID) + Transmitter:Distance + Transmitter.depth:Habitat + 
               Receiver.depth:Habitat + Wind.speed:Transmitter, data=df, family=binomial(link=logit))
</code></pre>

<p>The model summary is as follows:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: cbind(df$Valid.detections, df$False.Detections) ~ Transmitter.depth +  
    Receiver.depth + Water.temperature + Wind.speed + Transmitter +  
    Distance + Habitat + Replicate + (1 | Day) + (Distance |  
    SUR.ID) + Transmitter:Distance + Transmitter.depth:Habitat +      Receiver.depth:Habitat + Wind.speed:Transmitter
   Data: df

     AIC      BIC   logLik deviance df.resid 
  3941.9   4043.8  -1953.9   3907.9     2943 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-9.4911  0.0000  0.0000  0.5666  1.9143 

Random effects:
 Groups Name        Variance Std.Dev. Corr
 SUR.ID (Intercept)  0.33414 0.5781       
        Distance     0.09469 0.3077   1.00
 Day    (Intercept) 15.96629 3.9958       
Number of obs: 2960, groups:  SUR.ID, 20 Day, 6

Fixed effects:
                                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                      3.20222    2.84984   1.124  0.26116    
Transmitter.depth               -0.35015    0.11794  -2.969  0.00299 ** 
Receiver.depth                  -0.57331    0.51919  -1.104  0.26949    
Water.temperature               -0.26595    0.11861  -2.242  0.02495 *  
Wind.speed                       1.31735    1.50457   0.876  0.38127    
TransmitterPT-04                -0.68854    0.08016  -8.590  &lt; 2e-16 ***
Distance                        -0.39547    0.09228  -4.286 1.82e-05 ***
HabitatFinger                   -0.23746    3.57783  -0.066  0.94708    
Replicate2                      -0.21559    0.08009  -2.692  0.00710 ** 
TransmitterPT-04:Distance       -0.27874    0.08426  -3.308  0.00094 ***
Transmitter.depth:HabitatFinger  0.73965    0.28612   2.585  0.00973 ** 
Receiver.depth:HabitatFinger     3.02083    0.74546   4.052 5.07e-05 ***
Wind.speed:TransmitterPT-04     -0.15540    0.06572  -2.364  0.01806 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Trnsm. Rcvr.d Wtr.tm Wnd.sp TrPT-04 Distnc HbttFn Rplct2 TPT-04: Tr.:HF Rc.:HF
Trnsmttr.dp -0.024                                                                               
Recevr.dpth -0.120 -0.267                                                                        
Watr.tmprtr  0.019 -0.159  0.007                                                                 
Wind.speed   0.130  0.073 -0.974  0.040                                                          
TrnsmtPT-04 -0.015  0.027  0.020 -0.018 -0.024                                                   
Distance     0.022 -0.080  0.151 -0.052 -0.141 -0.164                                            
HabitatFngr -0.813  0.010  0.241 -0.025 -0.253  0.009   0.029                                    
Replicate2  -0.067  0.033  0.377 -0.293 -0.394  0.010   0.085  0.103                             
TrnsPT-04:D -0.006  0.043 -0.007 -0.050 -0.003  0.516  -0.373  0.004  0.006                      
Trnsmtt.:HF  0.017 -0.352  0.021  0.055  0.049  0.026  -0.142  0.031 -0.088  0.025               
Rcvr.dpt:HF  0.103  0.189 -0.830  0.051  0.817 -0.036  -0.143 -0.224 -0.385 -0.003  -0.229       
Wnd.:TPT-04 -0.002  0.026 -0.015  0.003 -0.009  0.176  -0.114 -0.002  0.016  0.306  -0.008  0.014
</code></pre>

<p><img src=""http://i.stack.imgur.com/DFxyQ.png"" alt=""enter image description here""></p>

<p>A side question: I noticed a strong negative correlation between the intercept and a dichotome categorical predictor. I wonder if this causes any problems for my data analysis. All the covariates are centered and scaled for numerical stability during modelling.  </p>
"
"0.083141099321054","0.0797241005179101","109796","<p>I am currently working on a project using a sales system and trying to come up with a way to use the current pipeline of potential sales to predict the amount of product that will be sold in the future. Iâ€™m looking for advice on how to approach this problem and hopefully some resources to teach me what approach to use and why.</p>

<p>The sales system Iâ€™m using has historical data for opportunities (potential sales). Around 50,000 of the opportunities are â€œclosedâ€ meaning that they are either won or lost. I have around 1,000 â€œopenâ€ opportunities that have not yet been won or lost. Some variables that I have on each sale include the product (which is generally homogenous except for the amount), the amount, the salesman, the date, the time it was input into the system, the customer, and other data about the customer.</p>

<p>I understand that if I want to predict a dichotomous variable like win / lose then I should look at a logistic regression. However, Iâ€™m looking for general advice on how to </p>

<ol>
<li>Predict the probability of each individual opportunity closing as won using the data I have (and how to tell if I've done it correctly).</li>
<li>Estimate the total amount of won opportunities for a period.</li>
</ol>

<p>I found a similar question here <a href=""http://stats.stackexchange.com/questions/66276/using-a-logistic-model-on-the-estimates-of-several-other-classification-models"">Using a logistic model on the estimates of several other classification models</a> but Iâ€™m hoping for a response that gives me a better idea of where to start. Iâ€™m comfortable using R or any other statistical software, but ideally I'd like some kind of book or other reference material that is as low-level as possible.</p>
"
"0.108857251714449","0.121780575111864","109851","<p>I am using logistic regression to predict likelihood of an event occurring. Ultimately, these probabilities are put into a production environment, where we focus as much as possible on hitting our ""Yes"" predictions. It is therefore useful for us to have an idea of what definitive ""hits"" or ""non-hits"" might be <em>a priori</em> (before running in production), in addition to other measures we use for informing this determination.</p>

<p>My question is, what would be the proper way to predict a definitive class (1,0) based on  the predicted probability? Specifically, I use R's <code>glmnet</code> package for my modeling. This package arbitrarily picks .5 probability as threshold for a yes or no. I believe that I need to take the results of a proper scoring rule, based on predicted probabilities, to extrapolate  to a definitive class. An example of my modeling process is below:</p>

<pre><code>mods &lt;- c('glmnet', 'scoring')
lapply(mods, require, character.only = T)

# run cross-validated LASSO regression
fit &lt;- cv.glmnet(x = df1[, c(2:100)]), y = df1[, 1], family = 'binomial', 
type.measure = 'auc')

# generate predicted probabilities across new data
df2$prob &lt;- predict(fit, type=""response"", newx = df2[, c(2:100)], s = 'lambda.min')

# calculate Brier score for each record
df2$propscore &lt;- brierscore(df2[,1] ~ df2$prob, data = df2)
</code></pre>

<p>So I now have a series of Brier scores for each prediction, but then how do I use the Brier score to appropriately weight each likelihood being a yes or no?</p>

<p>I understand that there are other methods to make this determination as well, such as Random Forest.</p>
"
"0.152425348755266","0.15944820103582","112481","<p>I am attempting to plot hurdle regression output with an interaction term in R, but I have some trouble doing so with the count portion of the model. </p>

<pre><code>Model &lt;- hurdle(Suicide. ~ Age + gender + bullying*support, dist = ""negbin"", link = ""logit"")
</code></pre>

<p>Since I am unaware of any packages that would allow me to plot the hurdle model in its entirety, I estimated each component separately (binomial logit and negative binomial count), using the <code>MASS</code> package, and I am attempting to plot the results. </p>

<p>So far, I have had the best luck using the <code>visreg</code> package for plotting, but I would like to hear other suggestions. I have been able to reproduce and successfully plot the original logistic output from the hurdle model in <code>MASS</code>, but not the negative binomial count data (i.e., the parameter estimates from <code>MASS</code> are not the same as they are in the hurdle regression output).</p>

<p>I would greatly appreciate any insight regarding how others have plotted hurdle regression results in the past, or how I might be able to reproduce negative binomial coefficients originally obtained from the hurdle model using <code>glm.nb()</code> in <code>MASS</code>. </p>

<p>Here is what I am using to plot my data:</p>

<pre><code>##Logistic 

logistic&lt;-glm(SuicideBinary ~ Age + gender + bullying*support, data = sx, family=""binomial"")

data(""sx"", package = ""MASS"")
##Linear scale
visreg(logistic, ""bullying"", by=""support"", xlab = ""bullying"", ylab = ""Log odds (Suicide yes/no)"")

##Logistic/probability scale
visreg(logistic, ""bullying"", by=""support"", scale = ""response"", xlab = ""bullying"", ylab = ""P(Initial Attempt)"")

##Count model

NegBin&lt;-glm.nb(Suicide. ~ Age + gender + bullying*support, data = sx)

data(""sx"", package = ""MASS"")
##Linear scale
visreg(NegBin, ""bullying"", by=""support"", xlab = ""bullying"", ylab = ""Count model (number of suicide attempts)"")

##Logistic/probability scale
visreg(NegBin, ""bullying"", by=""support"", scale = ""response"", xlab = ""bullying"", ylab = ""P(Subsequent Attempts)"")
</code></pre>
"
"0.117579270250443","0.112746904200424","114184","<p>Specifically, are there any binomial regression models that use a kernel with heavier tails and higher kurtosis than the standard kernels (logistic/probit/cloglog)?</p>

<p>As a function of the linear predictor $\textbf{x}'\mathbf{\hat{\beta}}$, the logistic distribution</p>

<ul>
<li>Underestimates the probability of my data being in the tails of the distribution</li>
<li>Underestimates the kurtosis, or clustering of data, in the middle of the distribution:</li>
</ul>

<p>This can be seen from a diagnostic plot of my fit:</p>

<p><img src=""http://i.stack.imgur.com/ar6OW.png"" alt=""enter image description here""></p>

<ul>
<li>The red line is the logistic CDF, representing a perfect fit</li>
<li>The black line represents the fitted probabilities from my dataset (calculated by binning observations into 0.1 intervals of $\textbf{x}'\mathbf{\hat{\beta}}$, where $\mathbf{\hat{\beta}}$ is obtained from my fit)</li>
<li>The grey bars in the background represent number of observations on which the true probabilities are based upon</li>
<li>The grey areas are where the tail 10% of the data lie (5% each side).</li>
</ul>

<p>Ideally, any solution would use R.</p>

<h2>Edit</h2>

<p>Why am I talking about CDFs? Our GLM equation is:</p>

<p>$$\mathbb{P}(Y = 1) = \mathbb{E}[Y] = g^{-1}(\textbf{x}'\mathbf{\beta})$$</p>

<p>Where $g$ is the link function.</p>

<p>Further, if $g^{-1}$ is a valid probability distribution (i.e. monotonically increasing from 0 to 1, indeed the case with probit, logit, cloglog), then consider a latent (not directly observed) continuous random variable $Y^{*}$ whose distribution (CDF) is given by $g^{-1}$. Then by definition</p>

<p>$$\mathbb{P}(Y^{*} \leq \textbf{x}'\mathbf{\beta}) = g^{-1}(\textbf{x}'\mathbf{\beta})$$</p>

<p>Equating the two equations above, we see the probability of $Y=1$ is exactly equal to the CDF of $Y^{*}$</p>

<p>$$\mathbb{P}(Y = 1) = \mathbb{P}(Y^{*} \leq \textbf{x}'\mathbf{\beta})$$</p>

<p>Hence I talk interchangeably about the expected response $\mathbb{E}[Y]$ and CDF of $Y^{*}$ over linear-predictor ($\textbf{x}'\mathbf{\hat{\beta}}$) space.</p>
"
"0.083141099321054","0.0797241005179101","114221","<p>I plotted normal probability plot in R using <code>qqnorm</code> and <code>qqline</code>. I want some help on:</p>

<ol>
<li>How to estimate ""probability"" that a data has normal distribution? (I read in a paper that a probability of 0.10 is required to assume that a data is normally distributed). </li>
<li>Also, how to calculate correlation coefficient for a normal probability plot in R?</li>
<li>Is the normality test valid for nonlinear regression too? (This might be a silly question, excuse me for that!)</li>
</ol>

<p>Thanks!</p>
"
"0.0979827252087026","0.0939557535003536","116347","<p>I am trying to generate a data frame of fake data for exploratory purposes. Specifically, I am trying to produce data with a binary dependent variable (say, failure/success), and a categorical independent variable called 'picture' with 5 levels (pict1, pict2, etc.). I am following the answer provided <a href=""http://stats.stackexchange.com/questions/49916/simulating-data-for-logistic-regression-with-a-categorical-variable"">here</a>, which allows me to successfully generate the data. However, I need each level of 'picture' to occur the same number of times (i.e. 11 repetitions of each level = 55 total observations per subject). </p>

<p>Here is a reproducible example of what has worked so far (code from user: ocram):</p>

<pre><code>library(dummies)

#------ parameters ------
n &lt;- 1000 
beta0 &lt;- 0.07
betaB &lt;- 0.1
betaC &lt;- -0.15
betaD &lt;- -0.03
betaE &lt;- 0.9
#------------------------

#------ initialisation ------
beta0Hat &lt;- rep(NA, 1000)
betaBHat &lt;- rep(NA, 1000)
betaCHat &lt;- rep(NA, 1000)
betaDHat &lt;- rep(NA, 1000)
betaEHat &lt;- rep(NA, 1000)
#----------------------------

#------ simulations ------
for(i in 1:1000)
{
  #data generation
  x &lt;- sample(x=c(""pict1"",""pict2"", ""pict3"", ""pict4"", ""pict5""), 
              size=n, replace=TRUE, prob=rep(1/5, 5))  #(a)
  linpred &lt;- cbind(1, dummy(x)[, -1]) %*% c(beta0, betaB, betaC, betaD, betaE)  #(b)
  pi &lt;- exp(linpred) / (1 + exp(linpred))  #(c)
  y &lt;- rbinom(n=n, size=1, prob=pi)  #(d)
  data &lt;- data.frame(picture=x, choice=y)

  #fit the logistic model
  mod &lt;- glm(choice ~ picture, family=""binomial"", data=data)

  #save the estimates
  beta0Hat[i] &lt;- mod$coef[1]
      betaBHat[i] &lt;- mod$coef[2]
  betaCHat[i] &lt;- mod$coef[3]
      betaDHat[i] &lt;- mod$coef[4]
  betaEHat[i] &lt;- mod$coef[5]
}
</code></pre>

<p>However, as you can see from the output, each level of the factor 'picture' does not occur the same number of times (i.e. 200 times each). </p>

<pre><code>&gt; summary(data)
picture     choice     
pict1:200   Min.   :0.000  
pict2:207   1st Qu.:0.000  
pict3:217   Median :1.000  
pict4:163   Mean   :0.559  
pict5:213   3rd Qu.:1.000  
            Max.   :1.000 
</code></pre>

<p>Moreover, it is not entirely clear to me how to manipulate the initial beta values as to determine the probability of success/failure for each level of 'picture'. I cannot comment the original question because I do not yet have the necessary reputation points. </p>
"
"0.288114463738593","0.283740153267512","118215","<p>This is my first post on StackExchange, but I have been using it as a resource for quite a while, I will do my best to use the appropriate format and make the appropriate edits. Also, this is a multi-part question. I wasn't sure if I should split the question into several different posts or just one. Since the questions are all from one section in the same text I thought it would be more relevant to post as one question.</p>

<p>I am researching habitat use of a large mammal species for a Master's Thesis. The goal of this project is to provide forest managers (who are most likely not statisticians) with a practical framework to assess the quality of habitat on the lands they manage in regard to this species. This animal is relatively elusive, a habitat specialist, and usually located in remote areas. Relatively few studies have been carried out regarding the distribution of the species, especially seasonally.  Several animals were fitted with GPS collars for a period of one year. One hundred locations (50 summer and 50 winter) were randomly selected from each animal's GPS collar data. In addition, 50 points were randomly generated within each animal's home range to serve as ""available"" or ""pseudo-absence"" locations. The locations from the GPS collars are coded a 1 and the randomly selected available locations are coded as 0.</p>

<p>For each location, several habitat variables were sampled in the field (tree diameters, horizontal cover, coarse woody debris, etc) and several were sampled remotely through GIS (elevation, distance to road, ruggedness, etc). The variables are mostly continuous except for 1 categorical variable that has 7 levels.</p>

<p>My goal is to use regression modelling to build resource selection functions (RSF) to model the relative probability of use of resource units. I would like to build a seasonal (winter and summer) RSF for the population of animals (design type I) as well as each individual animal (design type III).</p>

<p>I am using R to perform the statistical analysis.</p>

<p>The <strong>primary text</strong> I have been using isâ€¦</p>

<ul>
<li>""Hosmer, D. W., Lemeshow, S., &amp; Sturdivant, R. X. 2013. Applied Logistic Regression. Wiley, Chicester"".</li>
</ul>

<p>The majority of the examples in Hosmer et al. use STATA, <strong>I have also been using the following 2 texts for reference with R</strong>.</p>

<ul>
<li>""Crawley, M. J. 2005. Statistics : an introduction using R. J. Wiley,
Chichester, West Sussex, England.""</li>
<li>""Plant, R. E. 2012. Spatial Data Analysis in Ecology and Agriculture 
Using R. CRC Press, London, GBR.""</li>
</ul>

<p>I am currently following the steps in <strong>Chapter 4 of Hosmer et al. for the ""Purposeful Selection of Covariates""</strong> and have a few questions about the process. I have outlined the first few steps in the text below to aid in my questions.</p>

<ol>
<li>Step 1: A univariable analysis of each independent variable (I used a
univariable logistic regression). Any variable whose univariable test
has a p-value of less than 0.25 should be included in the first
multivariable model.</li>
<li>Step 2: Fit a multivariable model containing all covariates
identified for inclusion at step 1 and to assess the importance of
each covariate using the p-value of its Wald statistic. Variables
that do not contribute at traditional levels of significance should
be eliminated and a new model fit. The newer, smaller model should be
compared to the old, larger model using the partial likelihood ratio
test.</li>
<li>Step 3: Compare the values of the estimated coefficients in the
smaller model to their respective values from the large model. Any
variable whose coefficient has changed markedly in magnitude should
be added back into the model as it is important in the sense of
providing a needed adjustment of the effect of the variables that
remain in the model. Cycle through steps 2 and 3 until it appears that all of the important variables are included in the model and those excluded are clinically and/or statistically unimportant. Hosmer et al. use the ""<em>delta-beta-hat-percent</em>""
as a measure of the change in magnitude of the coefficients. They
suggest a significant change as a <em>delta-beta-hat-percent</em> of >20%. Hosmer et al. define the <em>delta-beta-hat-percent</em> as 
$\Delta\hat{\beta}\%=100\frac{\hat{\theta}_{1}-\hat{\beta}_{1}}{\hat{\beta}_{1}}$.
Where $\hat{\theta}_{1}$ is the coefficient from the smaller model and $\hat{\beta}_{1}$ is the coefficient from the larger model.</li>
<li>Step 4: Add each variable not selected in Step 1 to the model
obtained at the end of step 3, one at a time, and check its
significance either by the Wald statistic p-value or the partial
likelihood ratio test if it is a categorical variable with more than
2 levels. This step is vital for identifying variables that, by
themselves, are not significantly related to the outcome but make an
important contribution in the presence of other variables. We refer
to the model at the end of Step 4 as the <em>preliminary main effects
model</em>.</li>
<li>Steps 5-7: I have not progressed to this point so I will leave these
steps out for now, or save them for a different question.</li>
</ol>

<p><strong>My questions:</strong> </p>

<ol>
<li>In step 2, what would be appropriate as a traditional level of
significance, a p-value of &lt;0.05 something larger like &lt;.25?</li>
<li>In step 2 again, I want to make sure the R code I have been using for the partial likelihood test is correct and I want to make sure I am interpreting the results correctly. Here is what I have been doing&hellip;<code>anova(smallmodel,largemodel,test='Chisq')</code> If the p-value is significant (&lt;0.05) I add the variable back to the model, if it is insignificant I proceed with deletion?</li>
<li>In step 3, I have a question regarding the <em>delta-beta-hat-percent</em> and when it is appropriate to add an excluded variable back to the model. For example, I exclude one variable from the model and it changes the $\Delta\hat{\beta}\%$ for a different variable by >20%. However, the variable with the >20% change in $\Delta\hat{\beta}\%$ seems to be insignificant and looks as if it will be excluded from the model in the next few cycles of Steps 2 and 3. How can I make a determination if both variables should be included or excluded from the model? Because I am proceeding by excluding 1 variable at a time by deleting the least significant variables first, I am hesitant to exclude a variable out of order.</li>
<li><p>Finally, I want to make sure the code I am using to calculate $\Delta\hat{\beta}\%$ is correct. I have been using the following code. If there is a package that will do this for me or a more simple way of doing it I am open to suggestions.  </p>

<p><code>100*((smallmodel$coef[2]-largemodel$coef[2])/largemodel$coef[2])</code></p></li>
</ol>
"
"0.205763722938275","0.225493808400849","121120","<p>I am trying to create a logistic regression model with mgcv::gam
with what I think is a simple decision boundary, but the model
I build performs very poorly.  A local regression model built
using locfit::locfit on the same data finds the boundary very easily.
I want to add additional parametric regressors to my real-life model, so
I do not want to switch to a purely local regression.</p>

<p>I want to understand why GAM is having trouble fitting the data,
and whether there was ways of specifying the smooths that
can perform better.</p>

<p>Here's a simplified, reproducible example:</p>

<p>Ground truth is 1 = point lies within the unit circle, 0 if outside</p>

<p>e.g. z = 1 if sqrt(x^2 + y^2) &lt;= 1, 0 otherwise</p>

<p>The observed data is noisy, with both false positives and false negatives</p>

<p>Construct a logistic regression to predict whether a point
is inside the circle or not, based on the point's Cartesian
coordinates.</p>

<p>Local regression can find the boundary well (50% probability contour
is very close to the unit circle), but a logistic GAM consistently 
overestimates the size of the circle for the same probability band.</p>

<pre><code>library(ggplot2)
library(locfit)
library(mgcv)
library(plotrix)

set.seed(0)
radius &lt;- 1 # actual boundary
n &lt;- 10000 # data points
jit &lt;- 0.5 # noise factor

# Simulate random data, add polar coordinates
df &lt;- data.frame(x=runif(n,-3,3), y=runif(n,-3,3))
df$r &lt;- with(df, sqrt(x^2+y^2))
    df$theta &lt;- with(df, atan(y/x))

# Noisy indicator for inside the boundary
df$inside &lt;- with(df, ifelse(r &lt; radius + runif(nrow(df),-jit,jit),1,0))

# Plot data, shows ragged edge
(ggplot(df, aes(x=x, y=y, color=inside)) + geom_point() + coord_fixed() + xlim(-4,4) + ylim(-4,4))
</code></pre>

<p><img src=""http://i.stack.imgur.com/BfzkT.png"" alt=""enter image description here"">    </p>

<pre><code>### Model boundary condition using x,y coordinates

### local regression finds the boundary pretty accurately
m.locfit &lt;- locfit(inside ~ lp(x,y, nn=0.3), data=df, family=""binomial"")
plot(m.locfit, asp=1, xlim=c(-2,-2,2,2))
draw.circle(0,0,1, border=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/fy6z3.png"" alt=""enter image description here"">    </p>

<pre><code>### But GAM fits very poorly, also tried with fx=TRUE but didn't help
m.gam &lt;- gam(inside ~ s(x,y), data=df, family=binomial)
plot(m.gam, trans=plogis, se=FALSE, rug=FALSE)
draw.circle(0,0,1, border=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/ObeIm.png"" alt=""enter image description here"">  </p>

<pre><code>### gam.check doesn't indicate a problem with the model itself
gam.check(m.gam)

Method: UBRE   Optimizer: outer newton
full convergence after 8 iterations.
Gradient range [5.41668e-10,5.41668e-10]
(score -0.815746 &amp; scale 1).
Hessian positive definite, eigenvalue range [0.0002169789,0.0002169789].

Basis dimension (k) checking results. Low p-value (k-index&lt;1) may
indicate that k is too low, especially if edf is close to k'.

           k'    edf k-index p-value
s(x,y) 29.000 13.795   0.973    0.08

#### Try using polar coordinates

### Again, locfit works well
m.locfit2 &lt;- locfit(inside ~ lp(r, nn=0.3), data=df, family=""binomial"")
plot(m.locfit2)
abline(v=1, col=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/9zr73.png"" alt=""enter image description here"">    </p>

<pre><code>### But GAM misses again
m.gam2 &lt;- gam(inside ~ s(r, k=50), data=df, family=binomial)
plot(m.gam2, se=FALSE, rug=FALSE, trans=plogis)
abline(v=1, col=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/55XiQ.png"" alt=""enter image description here"">    </p>

<pre><code>### Can also plot gam on link scale for alternate view
plot(m.gam2, se=FALSE, rug=FALSE)
abline(v=1, col=""red"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/voMP4.png"" alt=""enter image description here"">    </p>

<pre><code>gam.check(m.gam2)

Method: UBRE   Optimizer: outer newton
full convergence after 4 iterations.
Gradient range [-3.29203e-08,-3.29203e-08]
(score -0.8240065 &amp; scale 1).
Hessian positive definite, eigenvalue range [7.290233e-05,7.290233e-05].

Basis dimension (k) checking results. Low p-value (k-index&lt;1) may
indicate that k is too low, especially if edf is close to k'.

         k'    edf k-index p-value
s(r) 49.000 10.537   0.979    0.06
</code></pre>
"
"0.159758768875269","0.165958949386154","122212","<p>I have been developing a logistic regression model based on retrospective data from a national trauma database of head injury in the UK. The key outcome is 30 day mortality (denoted as ""Survive"" measure). Other measures with published evidence of significant effect on outcome in previous studies include:</p>

<pre><code>Year - Year of procedure = 1994-2013
Age - Age of patient = 16.0-101.5
ISS - Injury Severity Score = 0-75
Sex - Gender of patient = Male or Female
inctoCran - Time from head injury to craniotomy in minutes = 0-2880 (After 2880 minutes is defined as a separate diagnosis)
</code></pre>

<p>Using these models, given the dichotomous dependent variable, I have built a logistic regression using lrm.</p>

<p>The method of model variable selection was based on existing clinical literature modelling the same diagnosis. All have been modelled with a linear fit with the exception of ISS which has been modelled traditionally through fractional polynomials. No publication has identified known significant interactions between the above variables.</p>

<p>Following advice from Frank Harrell, I have proceeded with the use of regression splines to model ISS (there are advantages to this approach highlighted in the comments below). The model was thus pre-specified as follows:</p>

<pre><code>rcs.ASDH&lt;-lrm(formula = Survive ~ Age + GCS + rcs(ISS) +
    Year + inctoCran + oth, data = ASDH_Paper1.1, x=TRUE, y=TRUE)
</code></pre>

<p>Results of the model were:</p>

<pre><code>&gt; rcs.ASDH

Logistic Regression Model

lrm(formula = Survive ~ Age + GCS + rcs(ISS) + Year + inctoCran + 
    oth, data = ASDH_Paper1.1, x = TRUE, y = TRUE)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          2135    LR chi2     342.48    R2       0.211    C       0.743    
 0            629    d.f.             8    g        1.195    Dxy     0.486    
 1           1506    Pr(&gt; chi2) &lt;0.0001    gr       3.303    gamma   0.487    
max |deriv| 5e-05                          gp       0.202    tau-a   0.202    
                                           Brier    0.176                     

          Coef     S.E.    Wald Z Pr(&gt;|Z|)
Intercept -62.1040 18.8611 -3.29  0.0010  
Age        -0.0266  0.0030 -8.83  &lt;0.0001 
GCS         0.1423  0.0135 10.56  &lt;0.0001 
ISS        -0.2125  0.0393 -5.40  &lt;0.0001 
ISS'        0.3706  0.1948  1.90  0.0572  
ISS''      -0.9544  0.7409 -1.29  0.1976  
Year        0.0339  0.0094  3.60  0.0003  
inctoCran   0.0003  0.0001  2.78  0.0054  
oth=1       0.3577  0.2009  1.78  0.0750  
</code></pre>

<p>I then used the calibrate function in the rms package in order to assess accuracy of the predictions from the model. The following results were obtained:</p>

<pre><code>plot(calibrate(rcs.ASDH, B=1000), main=""rcs.ASDH"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/HYTsp.png"" alt=""Bootstrap calibration curves penalized for overfitting""></p>

<p>Following completion of the model design, I created the following graph to demonstrate the effect of the Year of incident on survival, basing values of the median in continuous variables and the mode in categorical variables:</p>

<pre><code>ASDH &lt;- Predict(rcs.ASDH, Year=seq(1994,2013,by=1),Age=48.7,ISS=25,inctoCran=356,Other=0,GCS=8,Sex=""Male"",neuroYN=1,neuroFirst=1)
Probabilities &lt;- data.frame(cbind(ASDH$yhat,exp(ASDH$yhat)/(1+exp(ASDH$yhat)),exp(ASDH$lower)/(1+exp(ASDH$lower)),exp(ASDH$upper)/(1+exp(ASDH$upper))))
names(Probabilities) &lt;- c(""yhat"",""p.yhat"",""p.lower"",""p.upper"")
ASDH&lt;-merge(ASDH,Probabilities,by=""yhat"")
plot(ASDH$Year,ASDH$p.yhat,xlab=""Year"",ylab=""Probability of Survival"",main=""30 Day Outcome Following Craniotomy for Acute SDH by Year"", ylim=range(c(ASDH$p.lower,ASDH$p.upper)),pch=19)
arrows(ASDH$Year,ASDH$p.lower,ASDH$Year,ASDH$p.upper,length=0.05,angle=90,code=3)
</code></pre>

<p>The code above resulted in the following output:</p>

<p><img src=""http://i.stack.imgur.com/KGYcz.png"" alt=""Year trend with lower and upper""></p>

<p><strong><em>My remaining questions are the following:</em></strong></p>

<p><strong>1. Spline Interpretation</strong> - How can I calculate the p-value for the splines combined for the overall variable?</p>
"
"0.083141099321054","0.0797241005179101","122580","<p>I'm running a logistic regression with backard selection method. I get coefficients with p-values>.10. Here's an example:</p>

<pre><code>    DF   Estimate   Error   Chi-Square  Pr &gt; ChiSq  Estimate    Exp(Est)
Intercept   1   -30,32       11,48       6,97        0,01            -     
v1  1    0,001       0,00        9,70        0,00        0,10        1,00   
v2  1   -0,001       0,00        2,84        0,09       -0,07        1,00   
v3  1    0,000       0,00        0,12        0,73        0,01        1,00   
v4  1   -0,000       0,00        0,11        0,74       -0,01        1,00   
v5  1   -0,000       0,00        0,74        0,39       -0,03        1,00   
v6  1    0,000       0,00        0,58        0,45        0,02        1,00   
v7  1   -0,005       0,00        3,98        0,05       -0,07        1,00   
v8  1    0,002       0,01        0,04        0,84        0,01        1,00   
v9  1   -0,016       0,05        0,09        0,76       -0,02        0,98   
v10 1    0,014       0,03        0,29        0,59        0,03        1,01   
v11 1    0,102       0,03        14,77       0,00        0,09        1,11   
v12 1    0,009       0,01        1,27        0,26        0,05        1,01   
v13 1   -0,017       0,01        2,39        0,12       -0,05        0,98   
v14 1   -0,005       0,01        0,48        0,49       -0,03        1,00   
</code></pre>

<p>My question is, if the algorithm selects best variables, how is it be possible that keeps the variables that have p-values greater than 0.1? I know that the effect is reflected in the value of the coefficient but the pvalue shows the probability that having that value in that coefficient is only a coincidence, and the coefficient is 0 (considering all the other variables). So why is still keeping those?</p>
"
"0.144730076833889","0.138781845413342","124532","<p>I am estimating a (semi)parametric and a parametric model for a panel data set, and I want to test the functional form by applying the method proposed by <a href=""http://www.sciencedirect.com/science/article/pii/S030440760800016X"" rel=""nofollow"">Henderson et al. (2008, p.267)</a>. In particular, given the two models</p>

<pre><code>y = beta1 X + Z'gamma + u  (parametric)
y = beta2 X + g(z) + e    (semiparametric)
</code></pre>

<p>they use <code>H0</code> to denote the null hypothesis of the linear regression model, against <code>H1</code>: the corresponding alternative is the semiparametric model. The test statistic for testing <code>H0</code> is</p>

<pre><code>I= [beta1* X + Z'gamma* - beta2* X + g(z)]^2
</code></pre>

<p>Under <code>H0</code>. <code>I</code> converges to 0 in probability, whil it converges to a positive constant under <code>H1</code> . Therefore, the statistics <code>I</code> can be used to detect whether <code>H0</code> is true or not. However, given some problems with the asymptotical distribution of <code>I</code>, the authors propose to compute its empirical distribution by resampling <code>n</code> times the residuals <code>u</code>, using them to generate <code>n</code> new <code>y</code> and then re-estimating <code>n</code> times both models. By this way it is possible to obtain <code>n</code> times <code>I*</code> and its empiric distribution which should be approximating the null distribution of <code>I</code>. Therefore it can be used to detect whether <code>H0</code> holds.</p>

<p>My problem is in the interpretation of the results of the test. I reasoned as follows: I plotted the empirical distribution of <code>I*</code> which, in my case, has <code>mean &gt; 0</code>. Therefore what I did was to verify $where$  the <code>I</code> statistics lies. This is what I obtain:</p>

<p><img src=""http://i.stack.imgur.com/TihCM.png"" alt=""Empirical distribution and test""></p>

<p>where I.BB.IVO is my <code>I</code>. Can I say that <code>I</code> belongs to the empirical distribution (since it is not in the rejection zone) with mean <code>&gt; 0</code>, therefore the null hypothesis is rejected?</p>

<p>Is there any other alternative of doing the job?</p>
"
"0.083141099321054","0.0797241005179101","125603","<p>I am new to the world of <strong>Regression</strong> in statistics and I have been doing a research in which I am building an ordinal logistic regression model (ORM). In order to fit my ORM model, I am using the 'orm' function of 'rms' package from R (<a href=""http://cran.r-project.org/web/packages/rms/rms.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/rms/rms.pdf</a>).</p>

<p>Now I am trying to assess the goodness of fit of my model. By reading the R documentation, I can see the following statement in the 'stat' property of the 'orm' object (pg.98):</p>

<p>""(...)Nagelkerke R2 index, the g-index, gr (the g-index on the odds ratio scale),
and <strong>pdm (the mean absolute difference between 0.5 and the predicted probability
that $Y\geq q$  the marginal median)</strong>(...).""</p>

<p>I don't have enough background to understand the short description of the pdm measure. But when I try to do more research on this measure, I am not able to find related material (e.g. I've been finding ""prescription drug misuse""). In summary, my question is:</p>

<p>Would you know if the 'pdm' measure has some synonym which is more widely used? Or can you provide some references where I can study the pdm metric?</p>
"
"0.127000127000191","0.121780575111864","126715","<p>I am doing a logistic regression. The predictor variables are a mixture of categorical and continuous. I ran glm and out of the 80 predictors about 36 came out to be significant based on the p value. The accuracy of the model was also very good.</p>

<ol>
<li><p>I am still stuck with 36 variables but I want to narrow it down further to identify which of the predictors have the greatest impact. I understand that all the 36 predictors are statistically significant but all of these variables do not impact the DV equally. Is their a way to rank these variables based on their influence on the DV? Please feel free to suggest any methods/algorithms you know that does this efficiently.</p></li>
<li><p>Once I narrow down the predictors of my interest, I want to come up with rules based on the variables, much like a decision tree gives. I have tried running <code>rpart</code> and <code>ctree</code> on the 80 variable dataset but the output tree is very small meaning only a few variables appear in the tree, and thus there are very few rules which I can make based on that. I wonder if their is a way to increase the size of my tree to include more variables. Suppose I narrow down to 10-12 predictors, what all modeling techniques can I use to makes rules.</p>

<p>For example, I want something like: when x1 in range (a, b), x2 in range (c, d), ... and so on then the probability of $y(dv) &gt; 0.5$ or the event occurs i.e., $y = 1$ so that the range of values of the predictors can act as rules for determining when the event occurs.</p></li>
</ol>
"
"0.0480015360737319","0.0460287308949162","127385","<p>I'm running a logistic regression in which I'm predicted a binary response from a continuous predictor... I'm interested in determining the exact point in which the predicted probability <code>(exponentiated y-hat / (1 + exponentiated y-hat)</code> becomes significantly different than the average y-hat (or average predicted probability of the discrete event...</p>

<p>I was thinking of some sort of Johnson Neyman technique, but am unsure how to begin this exercise...</p>

<p>Would like to do this in <code>R</code> at the end of the day, but not required for an answer.</p>
"
"0.127000127000191","0.121780575111864","127536","<p>Sampling weights, the inverse probability of a unit's selection into the sample, and other more complex and adjusted weights are very often used in the social sciences. There is statistical software that allows weighting of observations/cases, like the <code>hclust</code> function from the <code>R</code>-package <code>cluster</code>. </p>

<p>In regression analysis, there is an ongoing debate when the usage of observation weights is appropriate (see e.g. Winship/Radbill 1994). I could not find anything concerning observation weights in textbooks about cluster analysis, if weighting is discussed, it is mostly about variable weighting. One exemption is the manual of the <code>R</code>-package <code>WeightedCluster</code>, which discusses observation weighting in more detail. The documentation of the <code>cluster</code> package is not very helpful, as it only shows a trivial example using the weighting option <code>hclust(..., members=""..."")</code> where the number or weight of cases is untouched.</p>

<ol>
<li>Therefore, I am looking for references and recommendations with observation/case weighting in cluster analysis, especially hierarchical cluster analysis. </li>
<li>As I could not find the actual formula for the <code>hclust(..., members=""..."")</code> function : Which parameters changes in the hierarchical cluster algorithm if one uses observation weights? How does that affect the algorithm?</li>
</ol>

<p>In order to get an idea of the difference between clustering with and without case weights, here is an example using weights from survey data and the R-code:
<img src=""http://i.stack.imgur.com/BYiLY.png"" alt=""Reweighting of clustering by using membership""></p>

<pre><code>require(survey)
data(api)
whc &lt;- hclust(dist(apiclus2[, c(""pct.resp"", ""meals"")]), method=""ward.D2"", 
              members=apiclus2$pw)
uwhc &lt;- hclust(dist(apiclus2[, c(""pct.resp"", ""meals"")]), method=""ward.D2"")
opar &lt;- par(mfrow = c(1, 2))
plot(whc,  labels = FALSE, hang = -1, main = ""Weighted survey data"")
plot(uwhc, labels = FALSE, hang = -1, main = ""Unweighted survey data"")
</code></pre>

<h3>References</h3>

<ul>
<li>Studer, M., 2013: WeightedCluster Library Manual. A practical guide to creating typologies of trajectories in the social sciences with R. LIVES Working Papers 24. Lausanne.</li>
<li>Winship, C. &amp; L. Radbill, 1994: Sampling Weights and Regression Analysis. Sociological Methods &amp; Research 23: 230â€“257.</li>
</ul>
"
"0.0480015360737319","0.0460287308949162","129286","<p>I'm following a Quantitive Methods Course, we have been asked to solve 12 questions from an harvard business case concerning tv ratings.</p>

<p><strong>Intro</strong>:
<img src=""http://i.stack.imgur.com/FfYn7.jpg"" alt=""enter image description here""></p>

<p><strong>The questions</strong>:
<em>I've already answered the first 10 but I have problems with the last two, because I know how to tackle them on a paper but I've no idea how to do that on R.</em></p>

<ol start=""11"">
<li><p>Suppose that Warrington has scheduled a fact-based movie without a star for a Monday time slot in March (again, following a show that typically receives ratings of 13.0). Should Warrington accept Harsanyi Electric's offer or accept the fixed fee of $5,000,000?</p></li>
<li><p>Suppose that, prior to accepting or rejecting Harsanyi Electric's offer, Warrington could purchase a regression that would tell with virtual certainty what the Nielsen rating of the proposed movie would be. What is the most that Warrington would be willing to pay for such a regression?</p></li>
</ol>

<p><strong>How i thought of doing the 11th was:</strong>
building a regressione like this:</p>

<pre><code> regression6=lm(formula = rating ~ fact + stars + prevratings + bbs + abn 
    + oct + dec + aprmay + mon + sun)
</code></pre>

<p>Finding the expected value, and the s.d., then somehow building a normal about the rating with the expected value of the prevision, and as s.d. the residual standard error.</p>

<p>Than switching from rating prevision to profit prevision but again I've no idea how to do it on R.
And for last calculating the probability of having a profit higher then $5mil.</p>

<p><strong>For the 12th this time I'm not really sure about which should be the appropriate approach even on paper, decision tree? But is it even possible to create a decision tree on R?</strong></p>

<p>Thanks a lot in advance,
Densetsu</p>
"
"0.127000127000191","0.121780575111864","129298","<p>I would like to get the optimal cutoff of an ROC curve relating to a logistic regression.
I am using the roc from the R package pROC. I am assuming same cost of false negative and false positive using youden's J statistics max(sensitivity+specificity).
I have variable status (binary) and primary variable test (continuous).</p>

<p>roc(status, test, print.thres=T, print.auc=T, plot=T)
Gives me a cutoff of 27.150</p>

<p>I searched on this forum for suggestions and they doesn't seem to give me the right cutoff</p>

<p>I used logistic regression, and I get the parameter value 14.25199 and -0.59877.
Using the parameter values:</p>

<p>roc(status, 14.25199-0.59877*test, print.thres=T, print.auc=T, plot=T)</p>

<p>Gives me a cutoff of -2.005</p>

<p>And another suggestion, is to use the probability instead.</p>

<p>prob=predict(glm(status~test, family=binomial),type=c(""response""))</p>

<p>roc(status, prob, print.thres=T, print.auc=T, plot=T)</p>

<p>Gives me a cutoff of 0.119</p>

<p>As you can see none of the method work. Both method gives the correct AUC but not the cutoff/threshold. The correct method should give me cutoff of 27.150.
What is the correct x form to input to get the correct optimal cutoff/threshold from the command roc(status, x,â€¦.)</p>
"
"0.117579270250443","0.112746904200424","130643","<p>I tried a regression in the form ${\rm logit}(Y) = {\rm coefficient}\times X + 0 + e$, where $Y$ is a binomial variable and $X$ is a factor variable with $n$ levels. I noticed that removing the intercept yields higher $p$ values. I'm wondering how to interpret it though.</p>

<p>Since removing the intercept makes it equal to $0$, I believe that the coefficients returned are relative to a $0$ probability of the event $Y$ and that all $X$ factors are in the $0$ state. But I think this is impossible isn't it?</p>

<p>$X$ are mutually exclusive factors, therefore it's impossible to have a case where no factor is $1$, at least in the presented observations. And it cannot be interpreted like the coefficient is relative to hypothetical cases in which really no one of the factors is present, because we have no data like that.</p>

<p>Regarding $Y$ having a $0$ intercept, wouldn't it mean forcing the probability of the event to $0$ when none of the factors is present? Again, this is an impossible case.</p>

<p>Nonetheless this kind of regression would allow me to retrieve pure probability range of the event Y given a factor by transforming the coefficients in the confidence intervals given as $\exp({\rm coefficient})/(1 + \exp({\rm coefficient}))$, and the $p$ values would test whether this probability is not $50\%$. This could also be a valuable result, since it would give independent probabilities for each factor.</p>

<p>Am I wrong?</p>
"
"0.0480015360737319","0.0460287308949162","138176","<p>I used a logistic regression on a variable indicating whether a person of an address-dataset took part in a survey (1), or not (0). I extracted the probabilities of each person to participate and calculated the inverse-probability (hence the name of the weighting method - inverse propensity score weighting). </p>

<p>What irritates me, is, that my smallest survey-weight is 1.901. I expected the smallest survey weight to at least be below ""1"". </p>

<p>I hope somebody can help me and either find out where i made a mistake, or assure me, that iÂ´m on the right track. Any help is greatly appreciated! Thank you!</p>

<hr>

<hr>

<pre><code>#Calculate logistic regression 
glm2&lt;-glm(indicator ~ var1 + varx,family=binomial,data=sampleframe)

#extract inverse probability of every case  
sampleframe$weight&lt;-glm2$fitted^-1

#combine the survey-weight to the survey-data 
surveydata&lt;-left_join(surveydata,sampleframe, by=""ID"")

#diagnostics:
#summary of the weights for the complete sampleframe    
Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
1.901   2.810   3.247   3.616   3.836  12.070

#summary of the survey-weights of the participants   
Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
1.925   2.686   3.078   3.308   3.502  12.070 

#comparison of mean-weight for participants (1) / non-participants (0)   
indicator weight.mean 
0    3.755967 
1    3.295854
</code></pre>
"
"0.204679237541786","0.196267167994715","140600","<p>I'm running a fixed effects logistic regression in R. The model consists of a binary outcome and two binary predictors, with no interaction term. On the log-odds scale, and as an odds-ratio, the coefficient for one of the predictors (<code>carbf</code> in the mocked-up example below) indicates that the expected probability of Y=1 (""success"") is different between the two levels of the factor (i.e., the effect is significant). </p>

<p>When I use the <code>effects</code> package to get marginal predicted probabilities, the 95% CIs for the two levels of <code>carbf</code> overlap considerably, indicating there is no evidence of a difference in the expected probability of Y=1 between the two factor levels.</p>

<p>When I use the <code>mfx</code> package to get average marginal effects for the coefficients (i.e., for the expected <em>difference</em> in the probability of Y=1 between the two factor levels), I do get a significant difference.</p>

<p><strong>I'm confused as to whether this discrepancy is because:</strong> </p>

<p><strong>1) the output from the model and the <code>mfx</code> package is an expected <em>difference</em> in the probability of Y=1 between factor levels, rather than predicted probabilities for each level.</strong></p>

<p><strong>2) of the way the <code>effects</code> package is calculating the marginal effect.</strong> </p>

<p>In an effort to determine this, I modified the source code from the <code>mfx</code> package to give me average marginal effects for each level of the <code>carbf</code> factor. The 95% CIs for these predictions <em>do not</em> overlap, indicating a significant difference. This makes me wonder why I get such different results using the <code>effects</code> package. Or is it that I'm just confused about the difference between marginal effects for coefficients and for predicted probabilities?</p>

<pre><code>#####################################
# packages
library(effects)
library(mfx)
library(ggplot2)

# data
data(mtcars)
carsdat &lt;- mtcars
carsdat$carb &lt;- ifelse(carsdat$carb %in% 1:3, 0, 1)
facvars &lt;- c(""vs"", ""am"", ""carb"")
carsdat[, paste0(facvars, ""f"")] &lt;- lapply(carsdat[, facvars], factor)

# model
m1 &lt;- glm(vsf ~ amf + carbf, 
    family = binomial(link = ""logit""), 
    data = carsdat)
summary(m1)


#####################################
# effects package
eff &lt;- allEffects(m1)
plot(eff, rescale.axis = FALSE)
eff_df &lt;- data.frame(eff[[""carbf""]])
eff_df 

#   carbf   fit    se  lower upper
# 1     0 0.607 0.469 0.3808 0.795
# 2     1 0.156 0.797 0.0375 0.469


#####################################
# mfx package marginal effects (at mean)
mfx1 &lt;-logitmfx(vsf ~ amf + carbf, data = carsdat, atmean = TRUE, robust = FALSE)
mfx1 

#         dF/dx Std. Err.     z  P&gt;|z|
# amf1    0.217     0.197  1.10 0.2697
# carbf1 -0.450     0.155 -2.91 0.0037

# mfx package marginal effects (averaged)
mfx2 &lt;-logitmfx(vsf ~ amf + carbf, data = carsdat, atmean = FALSE, robust = FALSE)
mfx2

#         dF/dx Std. Err.     z  P&gt;|z|
# amf1    0.177     0.158  1.12 0.2623
# carbf1 -0.436     0.150 -2.90 0.0037


#####################################
# mfx source code
fit &lt;- m1
x1 = model.matrix(fit)  
be = as.matrix(na.omit(coef(fit)))
k1 = length(na.omit(coef(fit)))
fxb = mean(plogis(x1 %*% be)*(1-plogis(x1 %*% be))) 
vcv = vcov(fit)

# data frame for predictions
mfx_pred &lt;- data.frame(mfx = rep(NA, 4), se = rep(NA, 4), 
    row.names = c(""amf0"", ""amf1"", ""carbf0"", ""carbf1""))
disc &lt;- rownames(mfx_pred)

# hard coded prediction estimates and SE  
disx0c &lt;- disx1c &lt;- disx0a &lt;- disx1a &lt;- x1 
disx1a[, ""amf1""] &lt;- max(x1[, ""amf1""]) 
disx0a[, ""amf1""] &lt;- min(x1[, ""amf1""]) 
disx1c[, ""carbf1""] &lt;- max(x1[, ""carbf1""]) 
disx0c[, ""carbf1""] &lt;- min(x1[, ""carbf1""])
mfx_pred[""amf0"", 1] &lt;- mean(plogis(disx0a %*% be))
mfx_pred[""amf1"", 1] &lt;- mean(plogis(disx1a %*% be))
mfx_pred[""carbf0"", 1] &lt;- mean(plogis(disx0c %*% be))
mfx_pred[""carbf1"", 1] &lt;- mean(plogis(disx1c %*% be))
# standard errors
gr0a &lt;- as.numeric(dlogis(disx0a %*% be)) * disx0a
gr1a &lt;- as.numeric(dlogis(disx1a %*% be)) * disx1a
gr0c &lt;- as.numeric(dlogis(disx0c %*% be)) * disx0c
gr1c &lt;- as.numeric(dlogis(disx1c %*% be)) * disx1c
avegr0a &lt;- as.matrix(colMeans(gr0a))
avegr1a &lt;- as.matrix(colMeans(gr1a))
avegr0c &lt;- as.matrix(colMeans(gr0c))
avegr1c &lt;- as.matrix(colMeans(gr1c))
mfx_pred[""amf0"", 2] &lt;- sqrt(t(avegr0a) %*% vcv %*% avegr0a)
mfx_pred[""amf1"", 2] &lt;- sqrt(t(avegr1a) %*% vcv %*% avegr1a)
mfx_pred[""carbf0"", 2] &lt;- sqrt(t(avegr0c) %*% vcv %*% avegr0c)
mfx_pred[""carbf1"", 2] &lt;- sqrt(t(avegr1c) %*% vcv %*% avegr1c)  

mfx_pred$pred &lt;- rownames(mfx_pred)
    mfx_pred$lcl &lt;- mfx_pred$mfx - (mfx_pred$se * 1.96)
mfx_pred$ucl &lt;- mfx_pred$mfx + (mfx_pred$se * 1.96)

#          mfx    se   pred     lcl   ucl
# amf0   0.366 0.101   amf0  0.1682 0.563
# amf1   0.543 0.122   amf1  0.3041 0.782
# carbf0 0.601 0.107 carbf0  0.3916 0.811
# carbf1 0.165 0.105 carbf1 -0.0412 0.372

ggplot(mfx_pred, aes(x = pred, y = mfx)) +
    geom_point() +
    geom_errorbar(aes(ymin = lcl, ymax = ucl)) +
    theme_bw()
</code></pre>
"
"0.144004608221196","0.138086192684749","141379","<p>I'm working with a dataset that has multiple categorical classes that cannot be ranked into any relative or priority order.  To approach this, I'm looking at glmnet's multinomial option. </p>

<p>As I understand it (and please correct me if I'm wrong), for this multinomial model, one class is chosen as the ""pivot"" class, and all other classes are modeled relative to that class.  For example, if I have 4 classes, one class is chosen as the pivot, 3 different logistic regression models are made, one for each of the other 3 classes relative to the pivot.  Probabilities are then scaled relatively in order ensure class probabilities sum to one.</p>

<p>The main question that I have is, which class is chosen as the pivot class?</p>

<p>The next question is why this ""pivot"" process is done rather than a 1-versus-all approach? i.e., 4 models, class 1 versus all others, class 2 versus all others, etc.</p>

<p>Further, in reading this page (<a href=""http://en.wikipedia.org/wiki/Multinomial_logistic_regression"" rel=""nofollow"">http://en.wikipedia.org/wiki/Multinomial_logistic_regression</a>) under the section ""As a set of independent binary regressions"" it discusses the scaling of probabilities to ensure the sum of all probabilities is 1.  But when I use their formulas on the data below, it seems to give an odd result.</p>

<pre><code>Model Prob      Scaled prob
0.72            0.327272727
0.33            0.15
0.15            0.068181818
0.61            0.454545455
</code></pre>

<p>In this case, I chose the last class as the pivot and used it for the scaling formulas.  Notice that the scaled probability for this class is higher than for the first class, despite having a higher Model Probability.</p>

<p>If I use the simpler formula of Scaled Prob = (Model Prob / sum(all Model Probs)), the answer is more intuitive, but perhaps wrong.  Any guidance/explanation there is appreciated.</p>

<p>Finally, any suggestions of other multiclass classification tools is appreciated.</p>
"
"0.159758768875269","0.15319287635645","144348","<p><strong>Note</strong><br>
I've edited the example to be more intuitive and closer to my real data</p>

<p><strong>Intro</strong><br>
I've got data on customers purchases and with it am trying to predict which customers are more likely to make next purchase at some time in the future. Data consist of customers' features like sex, age etc., and their prior purchase behavior like total spendings and number of orders, one row for every customer. The last two columns are the indicator of wether he have made next purchase or not, and number of days till purchase or till today, in case of no purchase.  </p>

<p><strong>Problem</strong><br>
I am building a Cox regression and then want to predict probability of next purchase for individual observations in, say, 30 days from last purchase.</p>

<p>Reproducible example:</p>

<pre><code>library(survival)
library(rms)
library(pec)
library(ggplot2)

data(cost)

# split into train and test sets
set.seed(1)
ind &lt;- sample(1:nrow(cost), 100)
test.set &lt;- cost[ind, ]
train.set &lt;- cost[-ind, ]
</code></pre>

<p>For Cox regression I use <code>cph</code> from <code>rms</code> package, for prediction - <code>predictSurvProb</code> from <code>pec</code> package as suggested in <a href=""http://stats.stackexchange.com/a/36016/72401"">this</a> discussion.</p>

<pre><code># fit Cox model
fit &lt;- cph(Surv(time, status) ~ ., data = train.set, surv = TRUE)

# predict pobability of event in 30 days
test.set$predicted.probs &lt;- 1 - predictSurvProb(fit, newdata = test.set, times = 1000)[, 1]
</code></pre>

<p>Thus, for every customer we have his probability of making a purchase in 1000 units of time. I want to validate prediction against real data.  </p>

<p><strong>Now to the question:</strong> what is the best/valid way to do it?  </p>

<p>Here's what I've tried:<br>
I expect that valid model would predict higher probabilities for customers who made their purchase earlier so correlation between probabilities and number of days to event' would be negative and strong (e.g. for customer who actualy made next purchase in 2 days, probability of buying in 30 days would be very high).</p>

<pre><code>with(test.set, cor(predicted.probs, time))
# [1] -0.5221604
</code></pre>

<p>Also, probability for those who made purchase (status = 1) would be higher than for those who didn't.</p>

<pre><code>with(test.set, by(predicted.probs, status, mean))
# status: 0
# [1] 0.2371247
# --------------
#   status: 1
# [1] 0.4083586
</code></pre>

<p>And a graph to eyeball my assumptions:</p>

<pre><code>qplot(data = test.set, x = time, y = predicted.probs, color = time)
</code></pre>

<p>Am I correct in my reasoning?</p>
"
"0.0480015360737319","0.0460287308949162","151600","<p>Has anyone written a package for R that can do a logistic regression over categorical variables (like <code>glm</code>) but with the constraint, and I do realize this is weird, that <em>all the residuals must be nonnegative?</em>  (In response space, not link space.  In other words, the predicted probability in each cell must come out less than or equal to the observed probability in that cell.) Alternatively, is there a straightforward way to transform a <code>glm</code> problem so that it will come out with nonnegative residuals?</p>

<p>I know I can probably persuade <code>optim</code> to do what I want but if a shortcut exists that sure would be nice.</p>
"
"0.173071999614875","0.165958949386154","151961","<p>*Please note this question is about the Platt probabilistic output and SVM class assignment, not about the code or the package itself. It just happens to be the code where I stumbled on the issue.</p>

<p>In <a href=""http://stats.stackexchange.com/questions/147260/including-class-probabilities-might-skew-a-model-in-caret"">another question</a> I asked about bad models coming from <code>caret</code> and associated <code>kernlab</code> when <code>prob.model=TRUE</code>. I found the answer myself, in both <a href=""http://stackoverflow.com/questions/29766951/different-results-with-caret-when-classprobs-true"">stackoverflow</a> and <a href=""http://r.789695.n4.nabble.com/Inconsistent-results-between-caret-kernlab-versions-td4680500.html"" rel=""nofollow"">from Max Kuhn himself</a>:</p>

<blockquote>
<pre><code>&gt; predict(newSVM, df[43,-1]) [1] O32078 10 Levels: O27479 O31403 O32057 O32059 O32060 O32078 ... O32676
&gt; predict(newSVM, df[43,-1], type = ""probabilities"")
     O27479     O31403     O32057    O32059    O32060     O32078
[1,] 0.08791826 0.05911645 0.2424997 0.1036943 0.06968587 0.1648394
     O32089     O32663     O32668     O32676
[1,] 0.04890477 0.05210836 0.09838892 0.07284396
</code></pre>
  
  <p>Note that, based on the probability model, the class with the largest
  probability is O32057 (p = 0.24) while the basic SVM model predicts
  O32078 (p = 0.16).</p>
  
  <p><strong>Somebody (maybe me) saw this discrepancy and that led to me to follow
  this rule:</strong></p>

<pre><code>if(prob.model = TRUE) use the class with the maximum probability   
  else use the class prediction from ksvm().
</code></pre>
  
  <p>Therefore:</p>

<pre><code>predict(svm.m1, df[43,-1])
 [1] O32057
 10 Levels: O27479 O31403 O32057 O32059 O32060 O32078 ... O32676
</code></pre>
</blockquote>

<p>Isn't that innacurate? <code>kernlab</code> searches for the optimal probability cutoff that minimizes error, that's why the assigned class and the maximum probability don't match: they don't have to.</p>

<p>Check this reproducible example. I excluded two cherrypicked <code>virginica</code> samples.</p>

<pre><code>require(kernlab);require(caret);
#kernel=polynomial; degree=3; scale=0.1; C=0.31
set.seed(101);SVM&lt;-ksvm(Species~., data=iris[-c(135,150),], kernel='polydot',C=.31, kpar=list( scale=.1, degree=3), prob.model=T)
</code></pre>

<p>Here's the resulting model </p>

<pre><code>&gt; SVM
Support Vector Machine object of class ""ksvm"" 

SV type: C-svc  (classification) 
 parameter : cost C = 0.31 

Polynomial kernel function. 
 Hyperparameters : degree =  3  scale =  0.1  offset =  1 

Number of Support Vectors : 58 

Objective Function Value : -1.4591 -0.7955 -10.2392 
Training error : 0.033784 
Probability model included. 
</code></pre>

<p>Now let's check the predicted class probabilities in those two samples</p>

<pre><code>&gt; predict(SVM, iris[c(135,150),-5], type=""probabilities"")
          setosa versicolor virginica
[1,] 0.008286638  0.4414114  0.550302
[2,] 0.013824451  0.3035556  0.682620
</code></pre>

<p>And the class predictions</p>

<pre><code>&gt; predict(SVM, iris[c(135,150),-5])
[1] versicolor virginica 
Levels: setosa versicolor virginica
</code></pre>

<p>Sample 150 was assigned to <code>virginica</code>, with a class probability of around 0.68. Sample 135 was assigned to <code>versicolor</code> with a probability of around 0.44, yet <code>virginica</code> probability nicely sits around 0.55.
Looking at several CV folds, we perceive that kernlab only assigns <code>virginica</code> when its probability is over a given value (way higher than 0.5). That's the cutoff I mentioned, and it happens thanks to the well known bad clustering in <code>iris</code> between <code>virginica</code> and <code>versicolor</code>.</p>

<p>So, am I right on these suppositions and therefore is <code>caret</code> class assignment model (maximum probability) wrong?</p>

<p>EDIT:
I've been experimenting with pairwise probability coupling of Platt scaling (logistic regression fit), isotononic regression and a model I'm working on. A weakness (?) I perceived in Platt's model is the probability isn't bound to be 0.5 when the binary SVM decision output is 0, which is the expected result as the instance would lie exactly on the separating hyperplane.</p>
"
"0.0480015360737319","0.0460287308949162","152091","<p>I am fairly new with logistic regression. I have a binary response. And did this plot. The binary response is:</p>

<p>Y = 0: The student fails</p>

<p>Y = 1: The student succeed</p>

<pre><code>library(ggplot2)
ggplot(data = both, aes(x = age, y = succeed)) + 
  stat_smooth(method = 'glm', family = 'binomial') +
  theme_bw()+xlab(""X"")+ylab(""The student succeed"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/xM9wS.png"" alt=""enter image description here""></p>

<p><strong>What is the y-axis? estimated probability that the student succeed? Or estimated log odds of a student succeed?
Feeling a little confused. Could someone explain what they are I have on the y-axis?</strong></p>
"
"0.0339422116651065","0.0650944554904119","154448","<p>The logistic regression model is:</p>

<p>$$\log\bigg(\frac{p}{1-p}\bigg) = \ldots$$</p>

<p>The most interesting case (for me) is the case that we have $p=1$ and $p=0$. But in this case, the ratio $p/(1-p)$ doesn't exist</p>

<p>For example: In my model, $p$ is the probability that the customer will come back after the first purchase. We observed that 100% of the clients with the income $&gt; 5000$ euros comes back after the first purchase ($p=1$), and 100% of the clients with the income $&lt; 1000$ don't ($p=0$). </p>

<p>When I treat the income as a continuous explanatory variable, there is no problem (income is a significant variable). But it isn't significant when I segment income into intervals $(0,1000)$, $(1000,3000)$, $(3000,5000)$, $(&gt;5000)$. All the categories become non-significant. I think it's because of the $p=1$ that makes $1-p=0$, then the ratio $p/(1-p)$ is degenerate. What should I do in this case?</p>
"
"0.10182663499532","0.113915297108221","155668","<p>I am trying to calculate 'reaction norms' for a fish species. This is essentially the length at which the probability that a fish become mature equals 50% for a particular age class.</p>

<p>I know I have to use a logistic regression model with binomial errors but I can't work out how to calculate this from the summary outputs or plot the regression successfully!</p>

<p>I have a data set that has: 'age' classes in (1,2,3,4,5,6),'Lngth' data in mm and 'Maturity' data (Immature/Mature - 0/1).</p>

<p>I am running a glm as follows</p>

<pre><code>Model&lt;-glm(Maturity~Lgnth, family=binomial(logit)) 
</code></pre>

<p>This however does not take into account the different age classes (I would really like to avoid creating whole new data sets for each age classes as I have multiple year ranges to test). </p>

<p>doing this below doesn't specify the individual age group, or can this be worked out from the summary stats? </p>

<pre><code>Model&lt;-glm(Maturity~Lgnth+age, family=binomial(logit)) 
</code></pre>

<p>And even so, I do not understand how I interpret the summary output to give me a length at which the probability of being mature equals 50%, along with the standard errors of this figure.</p>

<p>I also can't quite get the code right to plot this. Ideally id have one plot with lngth along the x axis, probability along the y and six lines/curves representing each age classes.</p>

<p>I would really appreciate any help any one could provide! I know this can all be achieved but I am really struggling.</p>

<p>Cheers</p>
"
"0.108857251714449","0.104383350095883","155762","<p>I am trying to calculate 'reaction norms' for a fish species. This is essentially the length at which the probability that a fish become mature equals 50% for a particular age class.</p>

<p>I know I have to use a logistic regression model with binomial errors but I can't work out how to calculate this from the summary outputs or plot the regression successfully!</p>

<p>I have a data set that has:
 'age' classes in (1,2,3,4,5,6),'Lngth' data in mm and 'Maturity' data (Immature/Mature - 0/1).</p>

<p>I am running a glm as follows</p>

<pre><code>Model&lt;-glm(Maturity~Lgnth, family=binomial(logit)) 
</code></pre>

<p>This however does not take into account the different age classes (I would really like to avoid creating whole new data sets for each age classes as I have multiple year ranges to test). </p>

<p>And even so, I do not understand how I interpret the summary output to give me a length at which the probability of being mature equals 50%, along with the standard errors of this figure.</p>

<p>I also can't quite get the code right to plot this.
Ideally id have one plot with lngth along the x axis, probability along the y and six lines/curves representing each age classes.</p>

<p>I would really appreciate any help any one could provide! I know this can all be achieved but I am really struggling.</p>

<p>Cheers</p>
"
"0.0554273995473693","0.0531494003452734","156766","<p>I have been having real issues trying to calculate the length at which certain probabilities of maturing are reached. This is NOT the same as the proportion of individuals that are mature as <code>dose.p</code> would calculate.</p>

<p>I know I have to run a logistic regression with binomial errors, something similar to the below code</p>

<pre><code>mylogit &lt;- glm(Maturity ~ Lngth, data = data, family = ""binomial"")
</code></pre>

<p>But from this how can I work out the length at which the probability of maturing equals 50%.
My data frame consists of Maturity (0,1) data and length data. (I have other data but believe this is all I need at this stage (I could be wrong!)
Any help would be greatly appreciated! </p>
"
"0.0678844233302131","0.0650944554904119","156804","<p>I have estimated a mixed-effects logistic regression with glmer
and want to draw a bootstrapped confidence-region for the mean predicted probability for two subgroups of the sample.</p>

<p>I have a $1000 \times 2$ Matrix $X$ containing the bootstrapped mean predicted probabilities for the two groups.
One could now compute the empirical covariance matrix $S$ and draw a
circle around the means using the metric induced by $S^{-1}$,
i.e. drawing a confidence ellipsoid based on normality-assumption.</p>

<p>Are there any widely used alternatives to this approach that do not imply distributional assumptions? </p>

<p>skeletor</p>
"
"0.107334697685273","0.102923371199077","158492","<p>I have fitted a (Cragg's) truncated normal hurdle model over a dataset in which the dependent variable is either zero or positive. The model consists of two parts: a probit which estimates the probability of the value being zero and a truncated regression which is estimated over the subsample of positive values of the dependent variable.</p>

<p>The output of the estimation (using package <code>mhurdle</code> in R) consists of two columns: one gives the probability of y being 0 and the other gives the estimated value for an uncensored observation (let's call this y*).</p>

<p>Now I would like to use these results for prediction, but instead of probabilities and estimated values of the uncensored y I would like to have the estimated values of the dependent variable, including some zeros (or almost zeros). Should I multiply the probability of y NOT being zero by the value of y*? Or should I take all observations for which P(y=0) > 0.5 to be zero and all the others to be equal to y*?</p>

<p>Sorry if the question is trivial but I'm fairly new to statistics and I have not been able to find the answer so far.</p>
"
"0.0678844233302131","0.0650944554904119","159489","<p>When using propensity scores for inverse probability weighting (IPW) the standard errors for the parameters in the regression model may be affected. I have seen several examples of people using different types of standard errors (classical, robust, bootstrap) and am unsure which ones are correct to use and why. Classical weighting would use weights to indicate the precision of individual observation - this is not the case for IPW, where weighting indicates the importance of observations (but not their precision).  </p>

<p>If you want to add references to R packages, that would be appreciated, but I am primarily interested in the methods and why they should or should not be used.</p>
"
"0.146445538135663","0.165958949386154","159647","<p>I've been studying (and applying) SVMs for some time now, mostly through <code>kernlab</code> in <code>R</code>.</p>

<p><code>kernlab</code> allows probabilistic estimation of the outcomes through Platt Scaling, but the same could be achieved with a Pool Adjacent Violators (PAV) isotonic regression (Zadrozny and Elkan, 2002).</p>

<p>I've been wrapping my head over this and came with a (clunky, but it works, or yet I think it does) code to try the PAV algorithm.</p>

<p>I divided the task into three pairwise binary classification task, estimated the probabilities on the training data and coupled the pairwise probabilities to get class probabilities (Wu, Lin, and Weng, 2004).</p>

<p>Predictions were made on the training set. I set the Cost really low <code>C=0.001</code> to try to get some misclassifications. </p>

<p>The Brier Score is defined as:</p>

<p>$$BS=\frac{1}N\sum_{t=1}^N\sum_{i=1}^R(f_{ti}-o_{ti})^2 $$</p>

<p>Where $R$ is the number of classes, $N$ is the number of instances, $f_{ti}$ is the forecast probability of the $t$-th instance belonging to the $i$-th class, and $o_{ti}$ is $1$, if the actual class $y_t$ is equal to $i$ and $0$, if the class $y_t$ is different from $i$.</p>

<pre><code>require(isotone)
require(kernlab)

##PAVA SET/VER
data1   &lt;-  iris[1:100,]        #only setosa and versicolor
MR1 &lt;-  c(rep(0,50),rep(1,100)) #target probabilities
KSVM1   &lt;-  ksvm(Species~., data=data1, type=""C-svc"", kernel=""rbfdot"", C=.001)
PRED1   &lt;-  predict(KSVM1,iris, type=""decision"")    #SVM decision function
PAVA1   &lt;-  gpava(PRED1, MR1)               #generalized pool adjacent violators algorithm 

##PAVA SET/VIR
data2   &lt;-  iris[c(1:50,101:150),]      #only setosa and virginica
MR2 &lt;-  c(rep(0,50),rep(1,50),rep(0,50))    #target probabilities
KSVM2   &lt;-  ksvm(Species~., data=data2, type=""C-svc"", kernel=""rbfdot"", C=.001)
PRED2   &lt;-  predict(KSVM2,iris, type=""decision"")
PAVA2   &lt;-  gpava(PRED2, MR2)

##PAVA VER/VIR
data3   &lt;-  iris[51:150,]   #only versicolor and virginica
MR3 &lt;-  c(rep(0,100),rep(1,50)) #target probabilities
KSVM3   &lt;-  ksvm(Species~., data=data3, type=""C-svc"", kernel=""rbfdot"", C=.001)
PRED3   &lt;-  predict(KSVM3,iris, type=""decision"")
PAVA3   &lt;-  gpava(PRED3, MR3)

#Usual pairwise binary SVM
KSVM    &lt;-  ksvm(Species~.,data=iris, type=""C-svc"", kernel=""rbfdot"", C=.001,prob.model=TRUE)

#probabilities on the training data through Platt scaling and pairwise coupling
PRED    &lt;-  predict(KSVM,iris,type=""probabilities"")

#The usual KSVM response based on the sign of the decision function
RES &lt;-  predict(KSVM,iris)

#pairwise probabilities coupling algorithm on kernlab
PROBS   &lt;-  kernlab::couple(cbind(1-PAVA1$x,1-PAVA2$x,1-PAVA3$x))
colnames(PROBS) &lt;- c(""setosa"",""versicolor"",""virginica"")

#Brier score multiclass definition
BRIER.PAVA  &lt;-  sum(
(cbind(rep(1,50),rep(0,50),rep(0,50))-PROBS[1:50,])^2,
(cbind(rep(0,50),rep(1,50),rep(0,50))-PROBS[51:100,])^2,
(cbind(rep(0,50),rep(0,50),rep(1,50))-PROBS[101:150,])^2)/150

#Brier score multiclass definition
BRIER.PLATT &lt;-  sum(
(cbind(rep(1,50),rep(0,50),rep(0,50))-PRED[1:50,])^2,
(cbind(rep(0,50),rep(1,50),rep(0,50))-PRED[51:100,])^2,
(cbind(rep(0,50),rep(0,50),rep(1,50))-PRED[101:150,])^2)/150

BRIER.PAVA

BRIER.PLATT
</code></pre>

<p>Soon I'll clean up a bit and write a proper wrapper function to do it all, but this result's really worrisome for me.</p>

<pre><code>BRIER.PAVA 
[1] 0.09801759
BRIER.PLATT 
[1] 0.6710232
</code></pre>

<p>The Brier Score I got from the probabilities estimated through PAVA is way better than the one we get on Platt Scaling.</p>

<p>If you check <code>PRED</code> you will see all probabilites fall on the ~0.33 range, while on <code>PROB</code> more extreme values (1 or 0) are expected, which was quite unexpected to me as I'm using a really low <code>C</code>.</p>

<p>References:</p>

<p><a href=""http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf"" rel=""nofollow"">Zadrozny, B., and Elkan, C. ""Transforming classifier scores into accurate multiclass probability estimates."" Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2002.</a></p>

<p><a href=""http://papers.nips.cc/paper/2454-probability-estimates-for-multi-class-classification-by-pairwise-coupling.pdf"" rel=""nofollow"">T.-F. Wu, C.-J. Lin, and Weng, R.C. ""Probability estimates for multi-class classification by pairwise coupling."" The Journal of Machine Learning Research 5 (2004): 975-1005.</a></p>

<p>EDIT:</p>

<p>Also, if you check the AUC of the different probabilities, they are quite high.</p>

<pre><code>requires(caTools)

AUC.PAVA&lt;-caTools::colAUC(PROBS,iris$Species)

AUC.PLATT&lt;-caTools::colAUC(PRED,iris$Species)

colMeans(AUC.PAVA)
colMeans(AUC.PLATT)
</code></pre>

<p>And here's the result</p>

<pre><code>&gt; colMeans(AUC.PAVA)
    setosa versicolor  virginica 
 0.9988667  0.9988667  0.8455333 
&gt; colMeans(AUC.PLATT)
    setosa versicolor  virginica 
 0.8913333  0.8626667  0.9656000 
</code></pre>

<p>Looking at these AUC, I would say Platt Scaling is a really underconfident technique.</p>
"
"0.0554273995473693","0.0797241005179101","162251","<p>I am trying to reproduce the following example of logistic regression with a transformed linear regression:</p>

<pre><code>am.glm &lt;- glm(am ~ hp + wt, data=mtcars, family=binomial)
newdata &lt;- data.frame(hp=120, wt=2.8)
predict(am.glm, newdata, type=""response"") 
##         1 
## 0.6418125
</code></pre>

<p>The equation for the probability of $Y=1$ is the following:
$$
P(Y=1) = {1 \over 1+e^{-(b_0+\sum{(b_iX_i)})}}
$$</p>

<p>So I tried something like this:</p>

<pre><code>am.lm &lt;- lm(am ~ 1/(1+exp(-(hp + wt))),data=mtcars)
predict(am.lm, newdata)
##       1 
## 0.40625
</code></pre>

<p>So this is obviously wrong! (I also tried transforming the given value but nothing worked so far).</p>

<p><strong>My question</strong><br>
How would I have to set up logistic regression with explicitly specifying the formula for the non-linear transformation of the linear model?</p>
"
"0.161121263165147","0.178268508203073","162426","<p>I am trying to create a logistic regression model and a random forest model on the same data to predict probability of default. For the logistic regression model, I have created some dummy variables from categorical variables. Finally, for the input of logistic regression, I have 9 dummy variables and 2 numeric variables (age and level, age takes values from 18 to 60, level from 4 to 10). I want to use same input dataset for the random forest model. When I did so, using ""randomForest"" Package, I get following Variable Importance Plot.</p>

<p><img src=""http://i.stack.imgur.com/qscyb.png"" alt=""enter image description here""></p>

<p>Level seems to be a very good variable both by MSE and Node Purity. Also, level is a very important variable in logistic regression (p value ~ 10^-5). 
However, Age is very important by Node purity, but not by MSE. Also, in logistic regression, age is not a very good variable with p value of 0.026. So I want to understand, Does being numeric increases the node purity importance of a variable by overfitting? Is it not suitable to use numeric and dummy variables together in random forest model? Or is there something I am missing.</p>

<p>I had similar doubts about using numeric and dummy variables in logistic regression, but in logistic regression it did not create any problem. Please help.</p>
"
"0.0480015360737319","0.0460287308949162","163986","<p>I effectively want to model the probability of a player winning his service point (a point in which he is the server) based on the values of explanatory variables (namely court surface and opponent world ranking)</p>

<p>Can this be done using a binary response logistic regression?</p>

<p>Consider the fact that I can view my response variable as number of successes out of a total number of trials (for which I have the data). Will it work considering I have both categorical and numerical explanatory variables?</p>

<p>Any feedback on why this will/won't work or how I can make it work would be hugely appreciated! I am doing the analysis in R, so pointers on functions or packages would also be welcome! </p>
"
"0.144004608221196","0.138086192684749","164120","<p>I am attempting to conduct a logistic regression for a tennis analytics project, endeavoring to predict the probability of a player winning a point in which he is the server. My response variable (service points) is binary in the sense that it can have only two outcomes for each observation - a success (service point win) or a failure (service point loss). </p>

<p>I have an issue with my data: For a given player, I have the point by point data for hundreds of matches. So take my data for R. Nadal as an example:</p>

<p>250 matches, each with about 70 dependent variable observations (service points). So for each match I currently have the two variables: Total_Service_Points_Played <strong>and</strong> Total_Service_Points_Won. </p>

<p>Eg - Match 1: Total_Service_Points_Played: 70 ; Total_Service_Points_Won: 47</p>

<p>So my data isn't in 1's and 0's. Is there a way I can implement a logistic regression with my dependent variable observations in their current form? Is there any simple transformation that comes to mind?</p>

<p>What springs to mind for me is to flesh out my match data into 1's and 0's. So following on from Match 1 above I would have: 47 1's followed by 26 0's . My data doesn't provide information as to what sequence these 1's and 0's arrived in, but since the depdendent variable observations are i.i.d this won't cause an issue? Correct me if I'm wrong please. Another issue posed by this technique would be the massive increase in my data - from 250 observations as a ratio (service point wins/service points played) to 250*70=17500 observations or more.</p>

<p>As a side note, the last thing I'm wondering is about the dispersion of my dependent variable data. Specifically, in the ratio of serve wins to total serve points as above, there exists no values &lt; 0.2 or 20% .... In addition, there exists no value > 0.9 ..... Does this fit the bill for the (link=logit) argument? I know this relates to an S shape curve which is undefined at 0 and 1, but approaches both values.... I might be going off track here but is this something to be concerned about? </p>
"
"0.0960030721474639","0.0920574617898323","164648","<p>I have created a Logistic Regression using the following code:</p>

<pre><code>full.model.f = lm(Ft_45 ~ ., LOG_D)
base.model.f = lm(Ft_45 ~ IP_util_E2pl_m02_flg)
step(base.model.f, scope=list(upper=full.model.f, lower=~1),
     direction=""forward"", trace=FALSE)
</code></pre>

<p>I have then used the output to create a final model:</p>

<pre><code>final.model.f = lm(Ft_45 ~ IP_util_E2pl_m02_flg + IP_util_E2_m02_flg + 
                           AE_NumVisit1_flg + OP_NumVisit1_m01_flg + IP_TotLoS_m02 + 
                           Ft1_45 + IP_util_E1_m05_flg + IP_TotPrNonElecLoS_m02 + 
                           IP_util_E2pl_m03_flg + LTC_coding + OP_NumVisit0105_m03_flg +
                           OP_NumVisit11pl_m03_flg + AE_ArrAmb_m02_flg)
</code></pre>

<p>Then I have predicted the outcomes for a different set of data using the predict function:</p>

<pre><code>log.pred.f.v &lt;- predict(final.model.f, newdata=LOG_V)
</code></pre>

<p>I have been able to use establish a pleasing ROC curve and created a table to establish the sensitivity and specificity which gives me responses I would expect. </p>

<p>However What I am trying to do is establish for each row of data what the probability is of Ft_45 being 1. If I look at the output of log.pred.f.v I get, for example,:</p>

<pre><code>1 -0.171739593    
2 -0.049905948    
3 0.141146419    
4 0.11615669    
5 0.07342591    
6 0.093054334    
7 0.957164383    
8 0.098415639    
.
.
.
104 0.196368229    
105 1.045208447    
106 1.05499112
</code></pre>

<p>As I only have a tentative grasp on what I am doing I am struggling to understand how to interpret the negative and higher that 1 values as I would expect a probability to be between 0 and 1.</p>

<p>So my question is am I just missing a step where I need to transform the output or have I gone completely wrong.
Thank you in advance for any help you are able to offer.</p>
"
"0.0720023041105979","0.0920574617898323","164912","<p>I am modelling invertebrate.biomass ~ habitat.type * calendar.day + habitat.type * calendar.day ^ 2, with a random intercept of transect.id (50 transects were repeated 5 times)</p>

<p>My response is zero-heavy - about 25% are 0s - and the non-zeroes are strongly right-skewed. </p>

<p>I understand a possible way of dealing with this is to construct 2 models - one modelling a binary response in a logistic regression and the other modelling the non-zero response in a (e.g.) Gamma regression. I'm working in R and following the ideas in <a href=""http://seananderson.ca/2014/05/18/gamma-hurdle.html"" rel=""nofollow"">this post</a>.</p>

<p>I want to check the method of combining the results of these 2 models, in order to generate quantitative predictions (ultimately with CI). Am I correct in multiplying the predicted probabilities from the logistic regression with the predicted (non-zero) biomass from the Gamma regression? Thus, the predicted (non-zero) biomass gets down-weighted according to the probability of there actually being an invertebrate present at all. This makes sense in my head, but feels too easy to be true. </p>

<p>See plots below which demonstrate my method in it's current form.
<a href=""http://i.stack.imgur.com/MVmJc.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/MVmJc.png"" alt=""Gamma hurdle model""></a></p>

<p>Assuming I'm right so far, how would I then go about generating a SE / CI for the predictions combining two models? </p>
"
"0.128004096196618","0.138086192684749","165214","<p>I am attempting to conduct a dynamic or time series regression for a tennis analytics project, endeavoring to predict the probability of a player winning a point in which he is the server. 
For a given player, I have the point by point data for hundreds of matches. So take my data for R. Nadal as an example:</p>

<p>Numerous matches, for each I have:</p>

<ul>
<li>No. of service points played </li>
<li>No. of service points Nadal won </li>
<li>(Thus) Nadal's Service point win %</li>
<li>The court surface the match was played on (independent variable)</li>
<li><p>Nadal, and his opponent's world ranking points at time of match </p>

<p>I will be using a model like this:</p></li>
</ul>

<p><em>Nadal serve win % = surface + (Nadal rank points - opponent rank points)</em> </p>

<p>However I would like to include an independent variable that accounts for ""form"" or ""hot hands"" in tennis. So I want to include an independent variable : </p>

<ul>
<li>Xi = Avg. Serve % of Last 5 matches </li>
</ul>

<p>i.e. a moving average of sorts </p>

<p>Is this a good idea? Does anyone have any suggestions how this could be implemented in R specifically? </p>

<p>Lastly, since my dependent variable data is binary, and can be used in a logistic regression in R.... Can I run a dynamic logistic regression/logistic time series for the model I discuss above? </p>

<p>Any advice on how I can account for this form/trend would be massively appreciated</p>
"
"0.128004096196618","0.138086192684749","166987","<p>I've read other similar questions on the site about logistic regression and I've read some articles/book chapters on this, but still I'm a little bit confused about that. I'll try to be as clearer as I can.</p>

<p>I have a medical case-control study, with many variables which could be used as predictors of the binary output variable, thus logistic regression is the best fit.</p>

<p>I have made some code in R, based on a previous question I made, like this:</p>

<pre><code>model&lt;-glm(Case ~ X + Y, data=data,    
family=binomial(logit));
</code></pre>

<p>where Case is the output variable, thus being 0 or 1 if it is a control or a case, respectively; X and Y are the input variables. I then use the output model to compute the area under the curve like this:</p>

<pre><code>aucCP=auc(Case~predict(model), data=data);
</code></pre>

<p>Okay, now the troubles begin. First, I understand that the object ""model"" is the output of the logistic regression model, thus being the log(odds) of the probability that model is Case for each couple of data in X and Y. Am I right?
Then, I know I can express the object model with an equation, being model:</p>

<pre><code>Coefficients:
(Intercept)         X            Y      
  -1.142005    -0.047981     0.020145     
</code></pre>

<p>thus being model=-1.14- 0.05X+ 0.02Y. Right?
Now the biggest problem: could ""model"" be considered as new variable, a combined predictor of X and Y, using which I predict Case?</p>
"
"0.148727319844751","0.178268508203073","168167","<p>My dependent variable is a probability. As such, values lie between 0 and 1. The most common values are 0, 0.5, and 1 each occurring in 20% to 30% of the observations but any value in between is possible and some do occur. </p>

<p><strong>Question 1: Which regression model is best to explain such data?</strong></p>

<ul>
<li><p>Ordinary least squares (OLS, function <code>lm</code> in Râ€™s <code>stats</code> package) is not suitable as it does neither account for the limited interval nor the accumulation at the margins.</p></li>
<li><p>Logit regression (function <code>glm</code> with parameter <code>family=""binomial""</code> in Râ€™s <code>stats</code> package) accounts for the accumulation at 0 and 1 but does not allow intermediate values.</p></li>
<li><p>Ordered logit regression (function <code>polr</code> in Râ€™s <code>MASS</code> package) could be applied when I divide the [0, 1] interval in subintervals. However, I lose the continuous nature of the dependent variable.</p></li>
<li><p>For probit and ordered probit regressions, the same applies as for logit and ordered logit.</p></li>
<li><p>Left- and right-censored tobit regression (function <code>tobit</code> with parameters <code>left=0</code> and <code>right=1</code> in Râ€™s <code>AER</code> package) might be appropriate. However, I found the following quote: â€œSome researchers have considered using censored normal regression techniques such as tobit ([R] tobit) on proportions data that contain zeros or ones. However, this is not an appropriate strategy, as the observed data in this case are not censored: values outside the [0, 1] interval are not feasible for proportions data.â€ (p. 302 in Baum (2008), <a href=""http://www.stata-journal.com/sjpdf.html?articlenum=st0147"" rel=""nofollow"">http://www.stata-journal.com/sjpdf.html?articlenum=st0147</a>). </p></li>
</ul>

<p>Below you find a code example </p>

<pre><code># Load libraries
library(stats, MASS, AER)
# Generate data
set.seed(123)
data &lt;- data.frame(x1 &lt;- runif(60, min = 0, max = 1), x2 &lt;- runif(60, min = 0, max = 1))
data$y  &lt;- -0.7 + data$x1 + 2 * data$x2 + rnorm(60, mean = 0, sd = 0.5)
    data$y  &lt;- ifelse(data$y &lt; 0, 0, data$y)
data$y  &lt;- ifelse(data$y &gt; 0.4 &amp; data$y &lt; 0.6, 0.5, data$y)
data$y  &lt;- ifelse(data$y &gt; 1, 1, data$y)
    data$yCat &lt;- data$y
    data$yCat &lt;- ifelse(data$yCat &gt; 0 &amp; data$yCat &lt; 0.5, 0.25, data$yCat)
    data$yCat &lt;- ifelse(data$yCat &gt; 0.5 &amp; data$yCat &lt; 1, 0.75, data$yCat)
    data$yCat &lt;- as.factor(data$yCat)
    hist(data$y, breaks=101)
# Different regression models
summary(lm(y ~ x1 + x2, data=data)) # OLS
summary(glm(y ~ x1 + x2, data=data, family=""binomial"")) # Logit
summary(polr(yCat ~ x1 + x2, data=data)) # Ordered logit
summary(tobit(y ~ x1 + x2, data=data, left=0, right=1)) # Tobit
</code></pre>

<p>To make matters worse, my data is panel data. I know how to handle individual, time, and mixed effects and random and fixed effects models using plm from Râ€™s plm package and F-test, LM-test, and Hausman test do decide which of these is best. </p>

<p><strong>Question 2: For the dependent variable described above, which panel regression model is best?</strong> </p>

<p>Below your find a code example for the data structure. This extends the prior example.</p>

<pre><code># Load library
library(plm)
# Generate data (builds on prior example)
data$id &lt;- rep( paste( ""F"", 1:15, sep = ""_"" ), each = 4)
    data$time &lt;- rep( 1981:1984, 15 )
pData &lt;- pdata.frame(data, c( ""id"", ""time"" ))
# Panel regression example
summary(plm(y ~ x1 + x2, data=pData, model=""within"", effect=""twoways"")) # Based on OLS
</code></pre>
"
"0.186273320869547","0.18978131929287","168725","<p>This question relates to whether it is a good starting point for a cut point in binary classification with logistic regression to the use the mean of the binary response variable as the initial cut point rather than simply 0.5.</p>

<p>Traditionally when people use logistic regression, people with use 0.5 as the threshold to determine when the model predicts YES/positive versus NO/negative.</p>

<p>People may run into trouble when the model only predicts one ""answer"" when using an imbalanced training set.</p>

<p>One way of dealing with this is to balance the training set via oversampling or under-sampling and keeping the test holdout set with the original balance.</p>

<p>However, I suspect that a good starting point for a cut point appears to be the mean of the binary response variable.  Is this usually true?</p>

<p>I created two models, one on a balanced training set and another on the original imbalanced training set.
<code>print(table(actual=test$y, predicted=test$fit&gt;0.5))</code></p>

<pre><code>       predicted
 actual FALSE TRUE
      0  2359  500
      1    11  130
</code></pre>

<p>With the imbalanced training, I used the mean of the binary response variable:</p>

<pre><code>print(table(actual=test$y, predicted=test$fit&gt;0.0496))

       predicted
 actual FALSE TRUE
      0  2317  542
      1     7  134
</code></pre>

<p>If one just uses 0.5, it looks like the model is a complete failure:</p>

<pre><code>`print(table(actual=test$y, predicted=test$fit&gt;0.5))`

       predicted
 actual FALSE
      0  2848
      1   152
</code></pre>

<p>They both had a KS of 0.76, so it seems like sound advice.</p>

<p>Example R code:</p>

<pre><code>require(ROCR)
require(lattice)
#
x=1:10000/10000;
y=ifelse(runif(10000)-0.7&gt;jitter(x),1,0)
#y=ifelse(rnorm(10000)-0.99&gt;x,1,0)
mean(y)

s=sample(length(x),length(x)*0.7);

df=data.frame(x=x,y=y)


##undersample
train=df[s,]
train=rbind(train[train$y==1,],train[sample(which(train$y==0),sum(train$y==1)),])
    ##oversample
    train=df[s,]
    train=rbind(train[train$y==0,],train[sample(which(train$y==1),sum(train$y==0),replace = T),])
mean(train$y) #now balanced
    threshold=0.5
    test=df[-s,] #unbalanced
    mean(test$y)
#

ex=glm(y~x,train, family = ""binomial"")
summary(ex)
nrow(test)
test$fit=predict(ex,newdata = test,type=""response"")
    message(""threshold="",threshold)
    print(table(actual=test$y, predicted=test$fit&gt;threshold)) 

#+results
pred&lt;-prediction(test$fit,test$y)
perf &lt;- performance(pred,""tpr"",""fpr"")
ks.sc=max(attr(perf,'y.values')[[1]]-attr(perf,'x.values')[[1]])
plot(perf)
print(ks.sc); #ks.score


levelplot(fit~y+x,test,col.regions =  terrain.colors(100)[1:95]) 

#+ imbalanced approach
#############imbalance approach

train=df[s,]
threshold=mean(y)
message(""threshold="",threshold)
ex=glm(y~x,train, family = ""binomial"")
summary(ex)
test$fit=predict(ex,test,type = ""response"")
    summary(test$fit)
print(table(actual=test$y, predicted=test$fit&gt;threshold)) 

print(table(actual=test$y, predicted=test$fit&gt;0.5)) 

pred&lt;-prediction(test$fit,test$y)
perf &lt;- performance(pred,""tpr"",""fpr"")
ks.sc=max(attr(perf,'y.values')[[1]]-attr(perf,'x.values')[[1]])
plot(perf)
print(ks.sc); #ks.score


levelplot(fit~y+x,test,col.regions =  terrain.colors(100)[1:95]) 
</code></pre>

<p>I noticed a similar question asked <a href=""http://stats.stackexchange.com/questions/91305/how-to-choose-the-cutoff-probability-for-a-rare-event-logistic-regression"">How to choose the cutoff probability for a rare event Logistic Regression</a></p>

<p>I like the answer given here which states to maximize the specificity or sensitivity:
<a href=""http://stats.stackexchange.com/questions/25389/obtaining-predicted-values-y-1-or-0-from-a-logistic-regression-model-fit/25398#25398"">Obtaining predicted values (Y=1 or 0) from a logistic regression model fit</a></p>

<p>But I also suspect that the usual starting cut off of 0.5 is bad advice.</p>

<p>Comments?</p>
"
"0.0960030721474639","0.0920574617898323","172850","<p>I have a question about data generation. It seems so simple, but is confusing me.</p>

<p>I am trying to generate data on the probability scale with support of -1, 1. The justification of having this support is that the data is designed to reflect probability of treatment response, which can be positive or negative in direction. I have ""known"" predictors of treatment response. </p>

<p>Currently, I have a regression model with predictors of a treatment response, which I am treating as a logit. This is then converted to probability scale. This is problematic because it makes the support 0,1. </p>

<p>Here is sample code for generation:</p>

<pre><code>X  = rbinom(N*p, 1, .5)
df = data.frame(matrix(X, nrow=N, ncol=p))
df$obs.txt = rep(0:1,N/2)
df$TE = (-2.1*df$X1 + -1.4*df$X2 + -.5*df$X3 + .3*df$X4 + .9*df$X5 + 1.6*df$X6 + 
         1.9*df$X7)
df$pTE = 1/(1+exp(-1*(df$TE)))
</code></pre>

<p>Does anyone have suggestions on how to better generate this data?</p>
"
"0.0678844233302131","0.0650944554904119","172958","<p>I am working on a school enrollment admission project to see how high school students react to scholarship in admission. The purpose is to redesign the scholarship level.</p>

<p>The original policy is 3 levels(0,2000,4000,6000) and used as training data. 
The other attributes are like GPA, ACT/SAT, gender,etc.. Y={enrolled, not enrolled}</p>

<p>What I did is manually expand the levels to (0,1000,2000,...,6000) for this year as testing data. And I used logistic regression and regression tree(LOTUS). </p>

<p>Ideally the probability will increase as the scholarship increases and it will give a sigmoid or S-curve, but not all the plots shown this. I think the reason is there are no data in the training set has the new levels.</p>

<p>I tried conjoint analysis but I don't know what does it mean.</p>

<p>what methods should I use or do I miss something here? </p>
"
"0.0720023041105979","0.0920574617898323","175079","<p>Is it so that:</p>

<ul>
<li>$y_i$ is not a discrete value, but a range with probability density function</li>
<li>Which means for the same predictor(s) value $y_i$ could have different results</li>
<li>In linear regression this distribution can only be normal</li>
<li>In GLM, this distribution can be any distribution from the exponential family</li>
<li>distribution of a single $y_i$ has nothing to do with distribution of all $y(s)$</li>
<li>$\mu_i$ is expected value of $y_i$</li>
<li>In practical use, $\mu_i$ is the predicted value $y_i$, specially if dataset has only one y for given predictor(s)</li>
</ul>

<p>Are above correct? Where am I wrong?</p>

<p>Based on the above I've tried simulating <code>glm</code> with <code>lm</code> in R, and it kinda works:</p>

<pre><code>library(boot)
download.file(""https://dl.dropbox.com/u/7710864/data/ravensData.rda"", 
              destfile=""./ravensData.rda"",method=""curl"")
load(""./ravensData.rda"")
# download manually and loadhere if above fails
# load(""/yourpath/ravensData.rda"")

# calling logit(ravensData$ravenWinNum) results in 
# [1]  Inf  Inf  Inf  Inf  Inf -Inf  Inf  Inf  Inf  Inf -Inf  Inf  Inf  Inf  Inf -Inf
# [17] -Inf -Inf  Inf -Inf
# that's way too much, as inv.logit goes to 1 at 20
# so we'll write our own dummy ""logit"" routine
# this will give us 5 when winNum=1 and -5 when it's zero
win &lt;- ravensData$ravenWinNum*10-5

# now we can do a simple lm
fit &lt;- lm(win~ravensData$ravenScore)

# and get probability of win using inv.logit
fitwin &lt;- inv.logit(fit$fitted.values)
plot(ravensData$ravenScore, fitwin)

# now glm
fitglm &lt;- glm(ravensData$ravenWinNum ~ ravensData$ravenScore, family=""binomial"")
plot(ravensData$ravenScore,fitglm$fitted)
</code></pre>
"
"0.117579270250443","0.112746904200424","175654","<p>I understand that you have to run the resulting regression line through the logistic function to get the predicted probability:</p>

<pre><code>am.glm &lt;- glm(am ~ hp + wt, data=mtcars, family=binomial)
newdata &lt;- data.frame(hp=120, wt=2.8)
p1 &lt;- predict(am.glm, newdata, type=""response"") 
p2 &lt;- 1/(1+exp(-(am.glm$coefficients[1] +
                 am.glm$coefficients[2]*newdata[1,1] + 
                 am.glm$coefficients[3]*newdata[1,2])))
p1 - p2
##            1 
## 1.110223e-16
</code></pre>

<p>Now I want to build two scoring model with the aim in mind to be usable with a hand calculator only: </p>

<ol>
<li>First model: I want to just take the two variables ($hp$, $wt$), multiply them by some factor and add them. The resulting number should be compared to a threshold number which then gives me the decision.</li>
<li>Second model: I want to have certain ranges of the two variables. Depending on the range the variable falls into I am given a number. At the end I simply add all numbers to arrive at my threshold number which again gives me the decision.</li>
</ol>

<p>As an example from the area of credit scoring where these scorecards are used quite heavily (source: <a href=""https://www.eflglobal.com/insights-sbbns-credit-risk-problem-loan-management-workshop/"" rel=""nofollow"">https://www.eflglobal.com/insights-sbbns-credit-risk-problem-loan-management-workshop/</a>):</p>

<p><a href=""http://i.stack.imgur.com/RQnHQ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/RQnHQ.png"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/5IX9K.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5IX9K.png"" alt=""enter image description here""></a></p>

<p><strong>My question</strong><br>
How to go about and esp. how to transform the logistic regression coefficients to be able to build the two (or any of the two) models? </p>

<p>Perhaps you can even demonstrate the steps in R, making use of the above <code>mtcars</code> logistic regression.</p>
"
"0.166282198642108","0.15944820103582","175767","<p>Logistic regression models the relationship between a set of independent variables and the probability that a case is a member of one of the categories of the dependent variable. If the probability is greater than 0.5, the case is classified in the modeled category.  If the probability is less than 0.50, the case is classified in the other category. The problem is that when I run the model with my dataset, the probabilities are far from 0.5, in fact it never gets to that value.</p>

<p>Here is part of My dataset:</p>

<pre><code>  sum_profit   direction   profit_cl1
   10           up          0.00
   0            Not_up     -0.03
  -5            Not_up      0.04
  -5            Not_up     -0.04
</code></pre>

<p>I want to find a relationship between the price of oil and the stock price of a Colombian oil company. So the variable 'sum_profit' is the sum of the change in the stock price in the next ten minutes. The variable 'profit_cl1' shows me the net change in the oil price in the last 10 minutes. </p>

<p>So what I want to know is that if the oil price changes in the last 10 minutes how would I expect the stock price direction to be in the following 10 minutes (Up or Down).</p>

<p>The problem is that my probabilities once I run the logistic regression are far from 0.5 even though the model is significant </p>

<pre><code>    glm.fit=glm(formula = direction ~ profit_cl1, family = binomial, data = datos)

    Deviance Residuals: 
      Min       1Q   Median       3Q      Max  
    -0.6786  -0.6786  -0.6131  -0.6131   1.8783  

    Coefficients:
                    Estimate Std. Error z value Pr(&gt;|z|)    
    (Intercept)     -1.57612    0.01618 -97.394   &lt;2e-16 ***
    profit_cl1       0.22485    0.02288   9.829   &lt;2e-16 ***
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

    (Dispersion parameter for binomial family taken to be 1)

     Null deviance: 48530  on 50309  degrees of freedom
    Residual deviance: 48434  on 50308  degrees of freedom
    AIC: 48438

    Number of Fisher Scoring iterations: 4 
</code></pre>

<p>The code to get the probabilities:</p>

<pre><code>   log.probs=predict(glm.fit, type=""response"")
   mean(log.probs)=0.1873 
</code></pre>

<p>the 0.1873 is very far from 0.5.</p>

<p>Sorry but I did not know where else to look for help! I appreciate any suggestion!</p>
"
"0.0554273995473693","0.0797241005179101","175956","<p>For a (fictional) <strong>multiple logistic regression</strong>, let's consinder a DV 'hired' (0,1) and <strong>three dichotomous IVs</strong> 'college_degree' (0,1), 'affluent' (0,1) and 'recommendated' (0,1) for <em>N</em> = 1,000 participants.</p>

<p>Running a logistic regression and generating predicted probabilities of being hired using the <code>predict</code> function for a <code>glm</code> object works well. For every respondent I have a probability value ranging continuously from 0 to 1.</p>

<p>Since I do have a base distribution of all three IVs and the DV, I want a kind of simulator that predicts the <strong>percentage/proportion</strong> of the DV using each indivduals predicted probability.</p>

<p>Let's say in the sample 20% are hired, 50% have a college degree, 10% are affluent and 35% are recommendated. I want to use the predicted values to see how much would the <strong>proportion of 'hired' goes</strong> up, when I, e.g., <strong>change the proportion of recommendations to 50%</strong>. I guess, I could also use the equation with the coefficients of the logit model, but would need to run it for every individual.</p>

<p>Is there any way to implement this in R (well or Excel, if that is easier)?</p>
"
"0.193202455833491","0.205846742398155","177960","<p>I'm trying to assess the effect of showing more impressions on a user. I want to study if users who saw more ads are more likely to make a purchase onsite. To do so I've created a multilevel model. I grouped users into 10 groups averaging their scores (we score users based on a number of factors). So basically I end up having 10 groups (from 0 to 9), where on group 9 I assume to have the best users, and on group 0 the worst.</p>

<pre><code>  picbucket mcuserid impressions mediacostcpm is_buyer gr.impressions gr.mediacostcpm
1         0 1           1        0.460        0       3.632794        2.767509
2         0 2           2        5.000        0       3.632794        2.767509
3         0 3           1        4.590        0       3.632794        2.767509
4         0 4           1        0.590        0       3.632794        2.767509
5         0 5           1        5.000        0       3.632794        2.767509
6         0 6           1        0.315        0       3.632794        2.767509
</code></pre>

<p>I think a multilevel model could be advantageous here because I'm expecting to see different effects on each group. On the best users I'm expecting an additional impression could have a higher impact, whereas on bad users and additional impression may be worthless. It could also be possible the opposite though. So that users which generally higher score will convert even without the need of serving them more impressions, whereas on mid groups additional impressions tend to change their behaviour. </p>

<p>A good model representation could be:</p>

<p>$y_{i} = \alpha_{j[i]} + X_{i}\beta + \epsilon_{i}$</p>

<p>The second level of the model will then be:</p>

<p>$\alpha_{j} = \mu_{\alpha} + \eta_{j}, \text{ with } \eta_{j} \sim N(0, \sigma_{\alpha}^{2})$</p>

<p>On the first level I want to include as a predictor how many impression a user saw. On the second level I want to include the average impressions a user saw within its group and the average media cost for the impressions we served on that user. I've used the package <code>lme4</code> in R to build my model.</p>

<pre><code>glmer(formula = is_buyer ~ impressions + mediacostcpm + (1 + 
    gr.impressions + gr.mediacostcpm | picbucket), data = new.df, 
    family = binomial())
             coef.est coef.se
(Intercept)  -7.42     0.33  
impressions   0.00     0.02  
mediacostcpm  0.03     0.01  

Error terms:
 Groups    Name            Std.Dev. Corr        
 picbucket (Intercept)     7.86                 
           gr.impressions  2.22     -0.99       
           gr.mediacostcpm 0.57     -0.68  0.60 
 Residual                  1.00                 
---
number of obs: 103146, groups: picbucket, 10
AIC = 2755.4, DIC = 2680.5
deviance = 2708.9 
</code></pre>

<p>This is my first experiment with multilevel modeling so I would like to make sure I don't misunderstand the results of my model. </p>

<p>From what I see here, the <code>impressions</code> predictor on the first level is useless. Its coefficient is zero and its standard deviation is very small. This could be due to the fact I'm including a group average on the second level for the impression count (<code>gr.impressions</code>). So, on any group (<code>picbucket</code>), serving more impressions than the average doesn't tell us much about the likelihood of a cookie to convert.</p>

<p>The average media cost on the first level is however an interesting one. Generally, within a group, if I spend a bit more for every impression I should increase the probability of generating conversions. This is probably due to inventory quality. Better inventory costs more, but also has better changes to be viewable inventory.</p>

<p>The coefficients on the group level instead tell you how much they contribute on explaining the group slope. So in this case the average number of impressions at the group level seems to explain quite a significant part of the group slope. </p>

<p>Interestingly, at the group level <code>gr.impressions</code> seem to be a very useful predictor, but at the within group level its usefulness is limited. The opposite applies to the <code>mediacostcpm</code>.</p>

<p>Am I interpreting these results correctly? How can I tell if the model has a good fit? Please note I've used a binomial regression because the dependent variable, <code>is_buyer</code> can take only 0 or 1 (one being the user made a purchase).</p>
"
"0.173071999614875","0.165958949386154","180447","<p>I have some data on patients presenting to emergency departments after sustaining self-inflicted gunshot injuries, stored in a data frame (""SIGSW,"" which is ~16,000 observations of 47 variables) in R. I want to create a model that helps a physician predict, using several objective covariates, the ""pretest probability"" of the self-shooting being a suicide attempt, or a negligent discharge. The covariates are largely categorical variables, but a few are continuous or binary. My outcome, suicide attempt or not, is coded as a binary/indicator variable, ""SI,"" so I believe a binary logistic regression to be the appropriate tool.  </p>

<p>In order to construct my model, I intended to individually regress SI on each covariate, and use the p-value from the likelihood ratio test for each model to inform which covariates should be considered for the backward model selection. </p>

<p>For each model, SI~SEX, SI~AGE, etc, I receive the following error:</p>

<pre><code>&gt;glm(SI ~ SEX, family = binomial, data=SIGSW)
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: algorithm did not converge
</code></pre>

<p>A little Googling revealed that I perhaps need to increase the number of iterations to allow convergence. I did this with the following:</p>

<pre><code>&gt;glm(SI ~ SEX, family = binomial, data=SIGSW, control = list(maxit = 50))

Call:  glm(formula = SI ~ SEX, family = binomial, data = SIGSW, control = list(maxit = 50))

Coefficients:
(Intercept)          SEX  
 -3.157e+01   -2.249e-13  

Degrees of Freedom: 15986 Total (i.e. Null);  15985 Residual
Null Deviance:      0 
Residual Deviance: 7.1e-12  AIC: 4
Warning message:
glm.fit: fitted probabilities numerically 0 or 1 occurred 
</code></pre>

<p>This warning message, after a little Googling, suggests a ""perfect separation,"" which, as I understand it, means that my predictor is ""too good."" Seeing as how this happens with all of the predictors, I'm somewhat skeptical that they're all ""too good."" Am I doing something wrong? </p>

<p>Edit: In light of the answers, here is a sample of the data (I only selected a few of the variables for space concerns):</p>

<pre><code>   SIGSW.AGENYR_C SIGSW.SEX SIGSW.RACE_C SIGSW.SI
1              19      Male        White        0
2              13      Male        Other        0
3              18      Male   Not Stated        0
4              15      Male        White        0
5              23      Male        White        0
6              11      Male        Black        0
7              16      Male   Not Stated        1
8              21      Male   Not Stated        0
9              14      Male        White        0
10             41      Male        White        0
</code></pre>

<p>And here is the crosstabulation of SEX and SI, showing that SI is coded as an indicator variable, and that there are both men and women with SI, so sex is not a perfect predictor. </p>

<pre><code>  &gt;table(SIGSW$SEX, SIGSW$SI)        
              0     1
  Unknown     1     3
  Male    11729  2121
  Female   1676   457
</code></pre>

<p>Does the small cell size represent a problem?</p>
"
"0.083141099321054","0.0797241005179101","181463","<p>I made a logistic regression in R statistics, but I don't know how to interpret it with 2 categorical variables (the examples I found on the internet and / or stackoverflow were just with one and I have difficulties to imagine it with two). </p>

<p>So imagine I want to see which factors infuence the fact of having a special desease (1: yes, 0: no) and I have:</p>

<pre><code>City: Manhattan, New York
hospital: St. Mary, Avante, Copperfield
bloodshugar: 1, 28, 7 ... , 66 (numeric)
timetoreact: 113, 423, 334, ... (numeric),
</code></pre>

<p>I give it all in a glm-model glm (desease dependent on: <code>City</code>, <code>hospital</code>, <code>City:hospital</code>, ...)</p>

<p>In the output I have the problem that it's all comprised with the factor level of the first letter of the alphabet, so i.e. ""Manhattan"" and ""Avante"" doesn't appear anymore. </p>

<p>There is just written: </p>

<pre><code>NewYork:Bloodshugar: Coeff.: 0.034 
</code></pre>

<p>and I don't know now what it is... Manhattan:Bloodshugar doesn't appear. Is it the difference of the incline from the probability on bloodshugar in Newyork in comparison to Manhattan? Where can I see if the probability to get the desease sinks or inclines with more bloodshugar in New York? When there's written bloodshugar: Coef.: 0.021, is it the bloodshugar ""mean"" of Manhattan and New York or is it just from Manhattan?  </p>

<p>What is the intercept now? Is it the probability to show the desease when cured in the Avantehospital and raised in Manhattan (because it's always the first letter)?</p>

<p>I hope I explained it well, I still can add some more explanations if you'd like to. </p>
"
"0.107334697685273","0.102923371199077","183461","<p>I'm working on a regression that predicts the number of births a female will have based on roughly ~15 variables. Therefore I have a discrete dependent variable. I was doing some research and it seems unfavorable to use a linear model for this estimation, so I then turned to a logit model. If I use the logit model I cannot predicts number of births, instead I can only get a probability of whether they will have a child or not. I was reading about multinomial regressions, but with the number of variables I have this could get extremely cumbersome very quickly, I tested it with only 2 variables, age and church attendance and my output was extremely long. I didn't see any significance levels either. I also ran into problems when I added all my variables,</p>

<pre><code>Error in nnet.default(X, Y, w, mask = mask, size = 0, skip = TRUE, softmax = TRUE,  : too many (1044) weights
</code></pre>

<p>I don't know how to adjust the weights in R, I tried but I kept getting errors.</p>

<p>So what do you think would be the best course of action? The simpler logit model or the multinomial model? My lack of knowledge with the multinom is making we lean toward the former option. </p>
"
"0.136614766661756","0.145555627434896","184712","<p>I am trying to </p>

<p>1) classify a bunch of [0,1] ratios into two groups  Group 0: Ratio = 0, Group 1: Ratio != 0.</p>

<p>2) predict the actual response with multiple predictors in R.</p>

<p>My question would then be:</p>

<p>Q1: Can I use the scaled predicted probability as the predicted response? </p>

<p>Q2: Should I classify the group before the regression before running the regression to solve the warning message? Would the data structure/predicted be affected?</p>

<p>I thought of achieving Goal 1 and Goal 2 separately but I can't seem to find a way to fit a unbalanced [0,1] non-censored data with good prediction.</p>

<hr>

<p>Basically my response is something like this</p>

<pre><code>y&lt;-c(rep(0,100),0.3,0.4,0.8,1.0)
x&lt;-cbind(rnorm(104,20,2),as.factor(c(rep(0,90),rep(1,5),rep(0,8),rep(1,1)))
,as.factor(sample(c(1:3),104,TRUE,prob = c(0.6,0.3,0.1))))

data&lt;-data.frame(cbind(y,x))
</code></pre>

<p>and y is strictly between 0 to 1.</p>

<p>I then fit it with a logistic regression and get the predicted probability:</p>

<pre><code>fit&lt;-glm(y~.,data=data, family = ""binomial"")  
fit.prob&lt;-predict(fit,type=""response"")
</code></pre>

<p>I used the probability to make classification model (Goal 1)</p>

<pre><code>class&lt;-y;class[y==0]=""0"";class[y!=0]=""1""

cutoff&lt;-0.06
fit.pred=rep(0,length(fit.prob)); fit.pred[fit.prob &gt;=cutoff]=1
table(fit.pred,class)
</code></pre>

<p>However, I also want to predict y from new data set, this is probably wrong, but here's what I did</p>

<pre><code>se&lt;-fit.prob&lt;-predict(fit,type=""response"",se=T)$se.fit
scaled.fit&lt;-fit.prob/max(fit.prob)
scale.fit.UL&lt;-scaled.fit+1.96*se
scale.fit.LL&lt;-scaled.fit-1.96*se
</code></pre>

<p>and I used this to be the prediction interval for y. Is there any other way to do it other than this?</p>
"
"0.0480015360737319","0.0460287308949162","185166","<p>I thought I've understood the output of the logistic regression in R (also I learned a lot through stackexchange), but somehow my vizualization tells me something different.
The output of the glm in R is: </p>

<pre><code>Call:
glm(formula = Zustand ~ Temp + Tag + Gehege + Temp:Gehege + Tag:Gehege + 
    Individuum + Gehege:Individuum + Temp:Individuum + Tag:Individuum, 
    family = binomial)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.9257  -0.5462  -0.4408  -0.3106   2.8510  

Coefficients:
                               Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                   -3.124244   1.302784  -2.398 0.016479 *  
Temp                           0.083585   0.068098   1.227 0.219664    
Tag                           -0.005485   0.013584  -0.404 0.686364    
Gehegeneues                    3.646637   1.249182   2.919 0.003509 ** 
IndividuumJoachim             -0.769787   0.927953  -0.830 0.406791    
IndividuumMary                -0.420745   0.797966  -0.527 0.598005    
Temp:Gehegeneues              -0.169516   0.062047  -2.732 0.006294 ** 
Tag:Gehegeneues               -0.057955   0.017154  -3.379 0.000729 ***
Gehegeneues:IndividuumJoachim  0.728588   0.329791   2.209 0.027158 *  
Gehegeneues:IndividuumMary     0.951688   0.396865   2.398 0.016484 *  
Temp:IndividuumJoachim         0.015466   0.049143   0.315 0.752979    
Temp:IndividuumMary           -0.012944   0.044467  -0.291 0.770985    
Tag:IndividuumJoachim         -0.035718   0.019177  -1.863 0.062532 .  
Tag:IndividuumMary             0.016538   0.019597   0.844 0.398724  
</code></pre>

<p>But my vizualization with the visreg-package...</p>

<pre><code>visreg(Ergebnis3, ""Gehege"", by = ""Individuum"", type = ""contrast"", scale = 'response', rug = F, ylim = c(0.0,0.6), main = ""Preening / Moulting"", xlab = ""Enclosure"", ylab = ""Likelihood"")
</code></pre>

<p>...looks like this:</p>

<p><a href=""http://i.stack.imgur.com/YxQc4.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YxQc4.jpg"" alt=""enter image description here""></a></p>

<p>""Individuum = Georg"" and ""Gehege = altes"" is my reference level and I thought the coefficient of ""Gehege = neues"" (3.64) means that the probability of ""Zustand"" in ""Gehege = neues"" is 3.64 times higher than in ""Gehege = altes"" or not? But in the graphic it's lower. Also for changing continuous variables ""Temp"" and ""Tag"" (here it's shown i.e. for 19.5 Â°C) ""Gehege = neues"" keeps to be lower for most of the times. The odds Ratio for ""Gehege = neues"" are also about 38, this high number is a bit weird... </p>

<p>I hope this is enough information to help, I have a real complex question I want to answer.</p>
"
"0.0720023041105979","0.0920574617898323","185495","<p>I am developing some stochastic simulations in which I have four explanatory variables that I named <code>Birth Rate</code>, <code>Inactivation Rate</code>, <code>Deletion Rate</code> and <code>Model</code>. They are all continuous data. I have several responses. One of these responses is a categorical value that can take 5 categories that I just labelled 1 to 5. I called this response an outcome.</p>

<p>I am having lots of trouble figuring out an analysis that I could use to understand the influence of my explanatory variables (all continuous data) on my response (categorical data). For example, I am interested in understanding how the outcome is influenced by <code>Birth Rate</code>, <code>Inactivation Rate</code>, <code>Deletion Rate</code> and <code>Model</code>. </p>

<p>I thought about a multinomial logistic regression, but I am not quite sure whether I can actually apply it to my data.</p>

<hr>

<p><em>Edit</em>: I have followed <a href=""https://www.google.co.uk/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;ved=0ahUKEwj30O-9u8zJAhXIqg4KHWlwDjQQFggsMAI&amp;url=https%3A%2F%2Fstatsthewayilikeit.files.wordpress.com%2F2015%2F05%2Fmultinomial-logistic-regression.docx&amp;usg=AFQjCNHj4IdMOAdjfavqcd-Q8HG66vYoag&amp;sig2=ScEoSVJGu1dDvMyIiAyljQ&amp;cad=rja"" rel=""nofollow"">this Word document</a> to do a multinomial logistic regression. An example of one of my plots after performing the regression is reproduced below (the x-axis is in log10 for the <code>Birth Rate</code>):  </p>

<p><a href=""http://i.stack.imgur.com/jjITQ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/jjITQ.png"" alt=""enter image description here""></a></p>

<p>My code in R:</p>

<pre><code>all.m        &lt;- multinom(code_Final_Info ~ Model+Birth_Rate+Inact_Rate+Del_Rate1+Time, 
                         data=all.mod)
PredProb     &lt;- cbind(preds, predict(all.m, newdata=preds, type='probs', se=TRUE))
PredProbMelt &lt;- melt(PredProb, value.name=""Probability"", 
                     id.vars=c(""Model"",""Birth_Rate"",""Inact_Rate"",""Del_Rate1"",""Time""))

(p &lt;- ggplot(PredProbMelt, aes(x=Del_Rate1, y=Probability, colour=Time)) + 
                           geom_line() +
                           facet_grid(variable ~., scales=""free"") + 
                           theme_bw()) 
</code></pre>

<p>My data.frame looks like:  </p>

<pre><code>&gt; str(all.mod)
'data.frame':   900000 obs. of  6 variables:
 $ Model          : num  0.98 0.98 0.98 0.98 0.98 0.98 0.98 0.98 0.98 0.98 ...
 $ Birth_Rate     : num  -4.05 -4.05 -4.05 -4.05 -4.05 ...
 $ Inact_Rate     : num  -3.14 -3.14 -3.14 -3.14 -3.14 ...
 $ Del_Rate1      : num  -4.26 -4.26 -4.26 -4.26 -4.26 ...
 $ code_Final_Info: Factor w/ 5 levels ""1"",""2"",""3"",""4"",..: 3 3 3 3 3 3 3 3 3 3 ...
 $ Time           : Factor w/ 3 levels ""0T250"",""1T500"",..: 1 1 1 1 1 1 1 1 1 1 ...
</code></pre>
"
"0.19822153725485","0.200634586470368","186393","<p>I built a multivariate regression tree using the <code>party</code> package in R. The depth of the tree (max. number of splits) is 13. For the first 3/4 splits the tree is relatively easy to interpret which is useful in our case. However with an increase in the number of splits interpretation becomes impossible. The idea is to get a measure of variable importance from this tree, similar to the idea of variable importance in random forests. For random forests there is function <code>varimp</code> but for regression trees it does not seem to exist. I'm aware of the <code>caret</code> package but it is built for CART of the <code>rpart</code> package.</p>

<p>Now, I have an idea of how to measure variable importance in CART but i'm a little lost on how to implement it using the <code>party</code>/<code>partykit</code> package. From <em>Ishwaran (2007)</em>:</p>

<blockquote>
  <p>We define the VIMP for a variable x<sub>v</sub> as the difference between prediction error when x<sub>v</sub> is â€œnoised upâ€ versus the prediction error otherwise. To noise up x<sub>v</sub> we adopt the following convention. To assign a terminal value to a case x, drop x down T [which is your tree] and follow its path until either a terminal node is reached or a node with a split depending upon x<sub>v</sub> is reached. In the latter case choose the right or left daughter of the node with equal probability. Now continue down the tree, randomly choosing right and left daughter nodes whenever a split is encountered (whether the split depends upon x<sub>v</sub> or not) until reaching a terminal node. Assign x the node membership of this terminal node.</p>
</blockquote>

<p>However:</p>

<blockquote>
  <p>This type of scenario shows that a non-informative variable can appear informative over a single tree under our noising up process...Moreover, for a single tree, this kind of problem can be resolved by slightly modifying the noising up process. Rather than using random left-right assignments on all nodes beneath x<sub>v</sub>, use random assignments for only those nodes that split on x<sub>v</sub>. This will impact prediction only when x<sub>v</sub> is informative and not affect prediction for non-informative variables</p>
</blockquote>

<p>How do I go about implementing this procedure? It seems that the <code>fitted_node()</code> function from the <code>partykit</code> package should do the trick. <code>fitted_node()</code> takes the following arguments:</p>

<pre><code>fitted_node(node, data, vmatch = 1:ncol(data), obs = 1:nrow(data), perm = NULL)
</code></pre>

<p>where</p>

<blockquote>
  <p><strong>node</strong>:  an object of class partynode<br>
   <strong>data</strong>: a list or data.frame<br>
   <strong>vmatch</strong>: a permutation of the variable numbers in data<br>
   <strong>obs</strong>: a logical or integer vector indicating a subset of the           observations in  data<br>
  <strong>perm</strong>: a vector of integers specifying the variables to be permuted
  prior before splitting (i.e., for computing permutation variable
  importances).  The default NULL doesnâ€™t alter the data.</p>
</blockquote>

<p>I can recursively partition the <code>data</code> using the tree specified in <code>node</code>. However how do I ""noise up"" one of the splitting variables in my tree? It is not clear to me whether i should use the <code>vmatch</code> and/or <code>perm</code> arguments and how i should specify them (for example do <code>perm</code> and <code>vmatch</code> refer to the column number of the covariate or do they refer to the cells in <code>data</code>?)</p>

<h2>References</h2>

<ol>
<li>Ishwaran, H. (2007). Variable importance in binary regression trees
and forests. Electronic Journal of Statistics, 1, 519â€“537.
<a href=""http://doi.org/10.1214/07-EJS039"" rel=""nofollow"">http://doi.org/10.1214/07-EJS039</a></li>
</ol>
"
"0.0678844233302131","0.0650944554904119","187015","<p>First, I have read <a href=""http://stats.stackexchange.com/questions/10985/dispersion-parameter-of-negbin-distribution/"" title=""this post"">this post</a>, <a href=""http://stats.stackexchange.com/questions/10419/what-is-theta-in-a-negative-binomial-regression-fitted-with-r/"" title=""this post"">this post</a> and <a href=""http://stats.stackexchange.com/questions/66586/is-there-a-test-to-determine-whether-glm-overdispersion-is-significant/"" title=""this post"">this post</a>. All have very useful information. I have three other more specific questions.</p>

<p>I have estimated a negative binomial model using the glm.nb function of MASS and discovered the following parameters
Theta: 9.0487, S.E: 0.444</p>

<ol>
<li>Is it correct to assume that dispersion parameter has a standard deviation of 20.38?</li>
<li>Does this value correspond to the Poisson overdispersion that is corrected by the negative binomial model or is my model still overdispersed?</li>
<li>Joseph Hilbe states in his <a href=""http://www.cambridge.org/us/academic/subjects/statistics-probability/statistical-theory-and-methods/modeling-count-data/"" rel=""nofollow"" title=""book"">book</a> that R's glm.nb function employs an inverted relationship of the dispersion parameter, theta. Thus a Poisson model results when theta approaches infinity. Suppose now that my second glm.nb model had estimates of Theta: 19.0487, S.E: 0.444. Would this model be less overdispersed than the first model?</li>
</ol>
"
"0.083141099321054","0.0797241005179101","187658","<p>I ran a logistic regression in R using driving data from about 10,000 people. The model included age, years of driving experience, as well as 4 driving test results. The dependent variable was whether or not they had been involved in a crash recently (yes or no, a categorical variable). </p>

<p>The coefficients of the model are given below:</p>

<pre><code>                    Estimate     Std.Err       z value     Pr(&gt;|z|)    
(Intercept)        -1.450041     0.207144      -7.000      2.56e-12 ***
riding experience  -0.014115     0.003697      -3.818      0.000134 ***
age                -0.034544     0.003608      -9.575       &lt; 2e-16 ***
test 1              0.261485     0.088645       2.950      0.003180 ** 
test 2              0.090102     0.051328       1.755      0.079184 .  
test 3              0.228918     0.073666       3.108      0.001887 ** 
test 4              0.070106     0.063652       1.101      0.270729    
</code></pre>

<p>Firstly, with 10,000 people am I right in thinking that p-values aren't going to be that useful?</p>

<p>I calculated the probabilities of being involved in a crash with a 1 unit increase in each variable by doing <code>exp(variable)</code> to get the odds and then, <code>probability = odds/(1+odds)</code>. It gave me:</p>

<pre><code>(Intercept)        0.1899952          
ridingexp          0.4964712
age                0.4913648
test 1             0.5650012
test 2             0.5225104
test 3             0.5569810   
test 4             0.5175193
</code></pre>

<p>These seem awfully high! It is like saying that an increase in age of 1 year makes you 49% less likely to be involved in a crash? Surely that can't be right.</p>
"
"0.159203084517278","0.152660029954676","188098","<p>I am trying to estimate a model for an event modelled by probability of happening which is a linear function of x (distributed normally) plus an error term, u.</p>

<p>Then I simulate whether the event really happened for each X comparing the probability of it happening against a uniformly distributed random variable.</p>

<p>So, I wrote a little function that simulates this model for a given b0, b1, X (mean and sd.) and error term (mean = 0 and sd.):</p>

<pre><code>SAMPLE_SIZE = 10000

underlying &lt;- function(b0, b1, mean_x, sd_x, sd_u) {
  xs &lt;- rnorm(SAMPLE_SIZE, mean_x, sd_x)
  us &lt;- rnorm(SAMPLE_SIZE, 0.0, sd_u)
  ys &lt;- b0 + (b1 * xs) + us
  ws &lt;- runif(SAMPLE_SIZE) &lt; ys  
  list(ws = ws, ys = ys, xs = xs, us = us)
}
</code></pre>

<p>It neatly returns both the probability of the event taking place in the ys component plus a simulation on the ws component.</p>

<p>I then tested whether I can correctly estimate b0 and b1 using linear regressions. And I got a very weird result.</p>

<p>This is how I simulated the samples and did the regressions:</p>

<pre><code>b1s &lt;- seq(from = 0.0, to = 1.0, length.out = 100)

datasets &lt;- lapply(b1s, FUN = function(x) underlying(0.5, x, 1.0, 0.2, 0.05)) 
regs     &lt;- lapply(datasets,  FUN = function(x) lm(data = x, ws ~ xs))
b0s_hat = sapply(regs, function(x) x$coefficients[[1]])
    b1s_hat = sapply(regs, function(x) x$coefficients[[2]])
</code></pre>

<p>So, for different b1s (and b0 = 0.5) I can plot the estimated b0 and b1 against the real b1:</p>

<pre><code>plot(b1s, b0s_hat)
plot(b1s, b1s_hat)
</code></pre>

<p>And what we get for b1s_hat looks sigmoid-ish like a cumulative distribution function, and b0s_hat looks like a bell curve (like the density function).</p>

<p>I thought I could recover the coefficients using the linear regression. What exactly is smelling weird here?</p>
"
"NaN","NaN","188399","<p>I would like to train a model that has a probability (a success rate between 0 and 1) as outcome.</p>

<p>So the data looks like this:</p>

<pre><code>feature1  feature2   success_rate
0.1       0.3        0.55
0.3       0.6        0.45
</code></pre>

<p>I started using <em>xgboost</em> (gradient boosting machine) with:</p>

<pre><code>""objective"" = ""reg:logistic""
""eval_metric"" = ""auc""
</code></pre>

<p>which means I doing a logistic regression using the Area Under the Curve (AUC) as evaluation function to measure the improvement of the model.</p>

<p>But I understand a logistic regression is usually trained with a categorical target (success or failure), not a probability.
Does this matter? and is this the right approach?</p>
"
"0.291903020319916","0.305943940804936","191810","<h2>Study design</h2>

<p>504 individuals were all sampled 2 times. Once before and once after a celebration.</p>

<p>The goal is to investigate if this event (Celebration) as well as working with animals (sheepdog) have an influence on the probability that an individual gets infected by a parasite. (out of 1008 observations only 22 are found to be infected)</p>

<p><strong>Variables</strong></p>

<ul>
<li>dependent variable = ""T_hydat"" (infected or not) </li>
</ul>

<p>(most predictiv variables are categorical) </p>

<ul>
<li>""Celebration"" (yes/no)</li>
<li>""sex"" (m/f)</li>
<li>""RelAge"" (5 levels)</li>
<li>""SheepDog"" (yes/no)</li>
<li>""Area"" (geographical area = 4 levels)</li>
<li>""InfectionPeriodT_hydat"" (continuous --> Nr Days after deworming"")</li>
<li>""Urbanisation (3 levels)</li>
</ul>

<h2>Question 1:</h2>

<p>1) Should I include Individual-ID (""ID"") as a random Effekt as I sampled each Ind. 2 times? (Pseudoreplication?) </p>

<pre><code>mod_fail &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|ID), family = binomial)

Warnmeldungen:
1: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,  :
  failure to converge in 10000 evaluations
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 1.10808 (tol = 0.001, component 10)
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p>--> unfortunately this model fails to converge (is it a problem that ID = 504 levels with only 2 observations per level?) 
Convergence is achieved with glmmPQL() but after droping some unsignficant preditiv variables the model fails to converge again ? What is the Problem here? Could geeglm() be a solution?</p>

<p>In an other attempt I run the model only with ""Area"" (4 levels) as random effect (my expectation is that Ind. in the same geogr. Area are suffering from the same parasite pressure etc.) and received the follwoing p-Values. </p>

<h2>My model in R:</h2>

<pre><code>mod_converges &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|Area), family = binomial)
</code></pre>

<h2>mod_converges output:</h2>

<p>summary(mod_converges)</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + sex + SheepDog + RelAge + Urbanisation +  
    InfectionPeriodT_hydat + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   203.0    262.0    -89.5    179.0      996 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.461 -0.146 -0.088 -0.060 31.174 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.314    0.561   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)            -6.81086    1.96027   -3.47  0.00051 ***
Celebration1            1.36304    0.57049    2.39  0.01688 *  
sexm                   -0.18064    0.49073   -0.37  0.71279    
SheepDog1               2.02983    0.51232    3.96  7.4e-05 ***
RelAge2                 0.34815    1.18557    0.29  0.76902    
RelAge3                 0.86344    1.05729    0.82  0.41412    
RelAge4                -0.54501    1.43815   -0.38  0.70471    
RelAge5                 0.85741    1.25895    0.68  0.49584    
UrbanisationU           0.17939    0.78669    0.23  0.81962    
UrbanisationV           0.01237    0.59374    0.02  0.98338    
InfectionPeriodT_hydat  0.00324    0.01159    0.28  0.77985    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<h2>glmmPQL-Model output below (added after the first reply by usÎµr11852)</h2>

<p>This model converges with ""Sample_ID"" as a random effect, however, as statet by 
<strong>usÎµr11852</strong> the varaince of the random effect is quiet high  4.095497^2 = 16.8. And the std.error of Area5 is far to high (complete separation).
Can I just remove Datapoints from Area5 to overcome this Problem? </p>

<pre><code>#     T_hydat
#  Area   0   1
#     1 226   4
#     2 203   3
#     4 389  15
#     5 168   0  ## here is the problematic cell

Linear mixed-effects model fit by maximum likelihood
 Data: dat 
  AIC BIC logLik
   NA  NA     NA

Random effects:
 Formula: ~1 | Sample_ID
        (Intercept)  Residual
StdDev:    4.095497 0.1588054

Variance function:
 Structure: fixed weights
 Formula: ~invwt 
Fixed effects: T_hydat ~ Celebration + sex + SheepDog + YoungOld + Urbanisation +      InfectionPeriodT_hydat + Area 
                            Value Std.Error  DF    t-value p-value
(Intercept)            -20.271630     1.888 502 -10.735869  0.0000
Celebration1             5.245428     0.285 502  18.381586  0.0000
sexm                    -0.102451     0.877 495  -0.116865  0.9070
SheepDog1                3.356856     0.879 495   3.817931  0.0002
YoungOldyoung            0.694322     1.050 495   0.661017  0.5089
UrbanisationU            0.660842     1.374 495   0.480990  0.6307
UrbanisationV            0.494653     1.050 495   0.470915  0.6379
InfectionPeriodT_hydat   0.059830     0.007 502   8.587736  0.0000
Area2                   -1.187005     1.273 495  -0.932576  0.3515
Area4                   -0.700612     0.973 495  -0.720133  0.4718
Area5                  -23.436977 28791.059 495  -0.000814  0.9994
 Correlation: 
                       (Intr) Clbrt1 sexm   ShpDg1 YngOld UrbnsU UrbnsV InfPT_ Area2  Area4 
Celebration1           -0.467                                                               
sexm                   -0.355  0.018                                                        
SheepDog1              -0.427  0.079  0.066                                                 
YoungOldyoung          -0.483  0.017  0.134  0.045                                          
UrbanisationU          -0.273  0.005 -0.058  0.317 -0.035                                   
UrbanisationV          -0.393  0.001 -0.138  0.417 -0.087  0.586                            
InfectionPeriodT_hydat -0.517  0.804  0.022  0.088  0.016  0.007  0.003                     
Area2                  -0.044 -0.035 -0.044 -0.268 -0.070 -0.315 -0.232 -0.042              
Area4                  -0.213 -0.116 -0.049 -0.186 -0.023 -0.119  0.031 -0.148  0.561       
Area5                   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000

Standardized Within-Group Residuals:
          Min            Q1           Med            Q3           Max 
-14.208465914  -0.093224405  -0.022551663  -0.004948562  14.733133744 

Number of Observations: 1008
Number of Groups: 504 
</code></pre>

<p><strong>Output from logistf (Firth's penalized-likelihood logistic regression)</strong></p>

<pre><code>logistf(formula = T_hydat ~ Celebration + sex + SheepDog + YoungOld + 
    Urbanisation + InfectionPeriodT_hydat + Area, data = dat, 
    family = binomial)

Model fitted by Penalized ML
Confidence intervals and p-values by Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood 

                               coef   se(coef)  lower 0.95  upper 0.95       Chisq
(Intercept)            -5.252164846 1.52982941 -8.75175093 -2.24379091 12.84909207
Celebration1            1.136833737 0.49697927  0.14999782  2.27716500  5.17197661
sexm                   -0.200450540 0.44458464 -1.09803320  0.77892986  0.17662930
SheepDog1               2.059166246 0.47197694  1.10933774  3.12225212 18.92002321
YoungOldyoung           0.412641416 0.56705186 -0.66182554  1.77541644  0.50507269
UrbanisationU           0.565030324 0.70697218 -0.98974390  1.97489240  0.56236485
UrbanisationV           0.265401035 0.50810444 -0.75429596  1.33772658  0.25619218
InfectionPeriodT_hydat -0.003590666 0.01071497 -0.02530179  0.02075254  0.09198425
Area2                  -0.634761551 0.74958750 -2.27274031  0.90086554  0.66405078
Area4                   0.359032194 0.57158464 -0.76903324  1.63297249  0.37094569
Area5                  -2.456953373 1.44578029 -7.36654837 -0.13140806  4.37267766
                                  p
(Intercept)            3.376430e-04
Celebration1           2.295408e-02
sexm                   6.742861e-01
SheepDog1              1.363144e-05
YoungOldyoung          4.772797e-01
UrbanisationU          4.533090e-01
UrbanisationV          6.127483e-01
InfectionPeriodT_hydat 7.616696e-01
Area2                  4.151335e-01
Area4                  5.424892e-01
Area5                  3.651956e-02

Likelihood ratio test=36.56853 on 10 df, p=6.718946e-05, n=1008
Wald test = 32.34071 on 10 df, p = 0.0003512978
</code></pre>

<p>** glmer Model (Edited 28th Jan 2016)**</p>

<p>Output from glmer2var: Mixed effect model with the 2 most ""important"" variables (""Celebration"" = the factor I am interested in and ""SheepDog"" which was found to have a significant influence on infection when data before and after the celebration were analysed separately.) The few number of positives make it impossible to fit a model with more than two explanatory variables (see commet EdM). </p>

<p>There seems to be a strong effect of ""Celebration"" that probably cancels out the effect of ""SheepDog"" found in previous analysis.</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Sample_ID)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   113.0    132.6    -52.5    105.0     1004 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.5709 -0.0022 -0.0001  0.0000 10.3491 

Random effects:
 Groups    Name        Variance Std.Dev.
 Sample_ID (Intercept) 377.1    19.42   
Number of obs: 1008, groups:  Sample_ID, 504

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -19.896      4.525  -4.397  1.1e-05 ***
Celebration1    7.626      2.932   2.601  0.00929 ** 
SheepDog1       1.885      2.099   0.898  0.36919    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.908       
SheepDog1   -0.297 -0.023
</code></pre>

<h2>Question 2:</h2>

<p>2) Can I use drop1() to get the final model and use the p-Values from summary(mod_converges) for interpretation? Does my output tell me if it makes sense to include the random effect (""Area"") ?</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   190.8    210.4    -91.4    182.8     1004 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.369 -0.135 -0.096 -0.071 17.438 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.359    0.599   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)    -5.912      0.698   -8.47  &lt; 2e-16 ***
Celebration1    1.287      0.512    2.51    0.012 *  
SheepDog1       2.014      0.484    4.16  3.2e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.580       
SheepDog1   -0.504  0.027
</code></pre>

<p>I know there are quite a few questions but I would really appreciate some advice from experienced people. Thanks!</p>
"
"0.1746312383152","0.167454105258415","192319","<p>I have the following dataset:</p>

<ul>
<li>100 indiviuals  (id)</li>
<li>Each individual with 4 measurements at 4 timepoints time1,time2,time3 and
time4</li>
<li>Outcome is a categorical or ordered variable 1,2,3,4.</li>
</ul>

<p>I simulated a data based on probabilities to change from one level to the other (simulation below). Here is an example of the first 6 individuals:</p>

<pre><code>head(df0)
  id time1 time2 time3 time4
1  1     2     2     1     2
2  2     2     2     2     2
3  3     3     1     1     1
4  4     2     2     1     1
5  5     1     1     1     1
6  6     2     1     1     1
</code></pre>

<p>Individual 1 changes from 2 to 2 to 1 and ends to 2. With 4 times and 4 categories there are 4^4 = 256 combinations.</p>

<p>I have the following questions:</p>

<ol>
<li>Is there a standard plot to display this kind of data?
I could use a mosaic plot or plot probability against time (computed from a proportional odds model) but these plots don't consider repeated measurements. Parallele coordinate plot is not appropriate due to low number of levels (overplotting).</li>
<li>Is there a regression method to analize this kind of data (preferably in R)?
Again, the problem is that the measurements are matched.</li>
</ol>

<p>I try to analze the data. I computed the frequency of the various combinations to see which combination has the highest frequency:</p>

<pre><code>library(""sqldf"")
df1 &lt;- sqldf(paste(""SELECT time1, time2,time3,time4, COUNT(*) n""
            ,"" FROM df0""
            ,"" GROUP BY  time1, time2,time3,time4""
            ,"" ORDER BY COUNT(*) DESC""
            )
      )
nrow(df1)
[1] 34
head(df1)
  time1 time2 time3 time4  n
1     2     2     2     2 20
2     1     1     1     1  8
3     2     1     1     1  7
4     3     3     3     3  6
5     2     2     2     1  4
6     2     3     3     3  4
</code></pre>

<p>There are totally 34 combination (of 254 possible combination). the combination with the highest frequency is 2-2-2-2. Interesting would be also to know the inital distribution:</p>

<pre><code>table(df0$time1)
 1  2  3  4
12 51 28  9
</code></pre>

<p>I also tried to plot this kind of informations:</p>

<pre><code>vcol &lt;- c(paste(""indianred"",1:4,sep=""""),paste(""royalblue"",1:4, sep=""""))

plot(1, type=""n"", xlab="""", ylab="""", xlim=c(0,4), ylim=c(1, 4.5), axes=FALSE)
for (i in 1:8)  {
    lines(1:4,df1[i,1:4]+0.02*i
          , lwd=df1[i,5]*2
          , col=vcol[i] )
}
text(0,1:4,1:4)
text(0.5,1:4, labels=table(df0$time1))
axis(1,at=1:4,labels=c(""T1"",""T2"",""T3"",""T4""))
abline(h=1:4, lty = 2)
abline(v=1:4, lty = 2)
</code></pre>

<p><a href=""http://i.stack.imgur.com/nXXkO.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/nXXkO.jpg"" alt=""enter image description here""></a></p>

<p>The width of the lines are proportionally to the counts of the specific combination. The drawback of this plot is that similar informations are splitted. For example, 2-2-2-2- is similar to 2-2-3-2, 2-1-2-2 and so on.</p>

<p>Any idea how to analyze this kind of data. Is there a multivariate methode like principal component, biplot, clusters which reduces a multidemensional dataset to a lower number of dimension?</p>

<p>What follows is the simulated data set:</p>

<pre><code># Number of individuals
n &lt;- 100
# Distribution at time 1
df0 &lt;- data.frame(id = 1:100
              , time1 = sample(rep(1:4, each=25)
                          ,n
                          , prob=rep(c(0.1,0.5,0.3,0.1), each=25)
                          , replace=TRUE)
              )

# Probablities:
p0 &lt;- 0.8     # No change
p1 &lt;- 0.15    # Change to next level
p2 &lt;- 0.04    # Jumps one level, e.g. 2 to 4
p3 &lt;- 0.01    # Jumps two levels, e.g 4 to 1

# Vectors
pp1 &lt;- c(p0,p1,p2,p3)
pp2 &lt;- c(p1,p0,p1,p2)
pp3 &lt;- c(p2,p1,p0,p1)
pp4 &lt;- c(p3,p2,p1,p0)

# Initialize time points
df0$time2 &lt;- NA
df0$time3 &lt;- NA
df0$time4 &lt;- NA

# Compute random levels given change probablities
for (i in 1:nrow(df0) ) {
      df0$time2[i] &lt;- ifelse(df0$time1[i] == 1,sample(c(1:4),1,prob=pp1), df0$time2[i])
      df0$time2[i] &lt;- ifelse(df0$time1[i] == 2,sample(c(1:4),1,prob=pp2), df0$time2[i])
      df0$time2[i] &lt;- ifelse(df0$time1[i] == 3,sample(c(1:4),1,prob=pp3), df0$time2[i])
      df0$time2[i] &lt;- ifelse(df0$time1[i] == 4,sample(c(1:4),1,prob=pp4), df0$time2[i])

      df0$time3[i] &lt;- ifelse(df0$time2[i] == 1,sample(c(1:4),1,prob=pp1), df0$time3[i])
      df0$time3[i] &lt;- ifelse(df0$time2[i] == 2,sample(c(1:4),1,prob=pp2), df0$time3[i])
      df0$time3[i] &lt;- ifelse(df0$time2[i] == 3,sample(c(1:4),1,prob=pp3), df0$time3[i])
      df0$time3[i] &lt;- ifelse(df0$time2[i] == 4,sample(c(1:4),1,prob=pp4), df0$time3[i])

      df0$time4[i] &lt;- ifelse(df0$time3[i] == 1,sample(rep(1:4),1,prob=pp1), df0$time4[i])
      df0$time4[i] &lt;- ifelse(df0$time3[i] == 2,sample(rep(1:4),1,prob=pp2), df0$time4[i])
      df0$time4[i] &lt;- ifelse(df0$time3[i] == 3,sample(rep(1:4),1,prob=pp3), df0$time4[i])
      df0$time4[i] &lt;- ifelse(df0$time3[i] == 4,sample(rep(1:4),1,prob=pp4), df0$time4[i])
}
</code></pre>
"
"0.159203084517278","0.152660029954676","198801","<p>Logistic Regression using R's <code>glm</code> package is giving me the following summary (snap of the few first variables). </p>

<p><strong>My Data Set:</strong></p>

<ul>
<li>Dimensions: 1252 rows and 224 columns (after using model.matrix). The Data  has been standardized.</li>
<li>Response variable is binary.</li>
<li>Trying to predict if an employee will leave the company, based on employee attributes </li>
</ul>

<p><a href=""http://i.stack.imgur.com/Ai42v.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Ai42v.png"" alt=""enter image description here""></a></p>

<p><strong>My Understanding:</strong></p>

<p>The model does not give a good fit because:</p>

<ol>
<li>Residual Deviance > Null Deviance. </li>
<li>p.value = 1 - pchisq(3676.5, 817) turns out to be 0.</li>
<li>The first warning about 0 or 1 fitted probability message suggests that due to some predictor(s) the model might be giving perfect predictions </li>
<li>Second warning on â€˜rank deficiencyâ€™ suggests that there might be predictors that are linearly dependent on one another.</li>
</ol>

<p><strong>My Questions:</strong></p>

<ol>
<li>How can I improve the model? I would like to see Residual Deviance &lt; Null Deviance. I will invest time on dropping the linearly dependent variables in the model, but is there anything I should do first to test the â€˜modelâ€™ itself, before revisiting my data? I am asking this because SVM worked quite well on the same data set. </li>
<li>Why do I have such extreme coefficient values?</li>
<li>Many answers to other posts state that â€˜AICâ€™ is used to compare different</li>
<li>The summary parameters (coefficients , std error and p-values) for many dummy factors obtained via model.matrix, like GSS_SEXM,  is shown as 'NA'. Why is it so? </li>
<li>logistic models. What is meant by â€˜differentâ€™ here? Models trained on different data sets that bear different coefficients, like say different set of attributes? </li>
</ol>
"
"0.0678844233302131","0.0650944554904119","199504","<p>I would like to compare an average probability to a fixed probability value in order to determine if there is a significant difference between the two.</p>

<p>My participants had to detect and point a target that appeared among three distractors. The possibility they answered hazardously therefore is 1 out of 4 (i.e., .25). My dependent variable being binary (Correct answer: 1 ; Incorrect answer: 0) I am using logistic regression for my analyses:</p>

<pre><code>model1 &lt;- glm(Accuracy ~ Task * Masking,
              data = DF,
              family = binomial(link = ""logit""))
</code></pre>

<p>âŸ¶ <a href=""http://i.stack.imgur.com/AyPzG.png"" rel=""nofollow"">Plot of results</a></p>

<p>More precisely, I would like to know if the probability of report in the Reach-Masked condition is significantly different from 0.25.</p>

<p>How do I test this possibility in R?</p>

<p>Thank you for your time. Have a very nice day.</p>
"
"0.118797740827873","0.130188910980824","199978","<p>I've been building a logistic regression model (using the ""glm"" method in caret). The training dataset is extremely imbalanced (99% of the observations in the majority class), so I've been trying to optimize the probability threshold during the resampling process using the train function from the caret package as described in this example of a svm model: <a href=""http://topepo.github.io/caret/custom_models.html#Illustration5"" rel=""nofollow"">Illustrative Example 5: Optimizing probability thresholds for class imbalances.</a></p>

<p>The idea is to get the classification parameters for different values of the probability thershold, like this:</p>

<pre><code>threshold   ROC    Sens   Spec   Dist   ROC SD  Sens SD  Spec SD  Dist SD
 0.0100     0.957  1.000  0.000  1.000  0.0366  0.0000   0.0000   0.0000 
 0.0616     0.957  1.000  0.000  1.000  0.0366  0.0000   0.0000   0.0000 
 0.1132     0.957  1.000  0.000  1.000  0.0366  0.0000   0.0000   0.0000 
 0.1647     0.957  1.000  0.000  1.000  0.0366  0.0000   0.0000   0.0000 
  ...        ...                  ...                      ...      ...
</code></pre>

<p>I noticed that the 'glm' method in caret uses 0.5 as the probability cutoff value as can be seen in the predict function of the model:</p>

<pre><code>code_glm &lt;- getModelInfo(""glm"", regex = FALSE)[[1]]
code_glm$predict
    function(modelFit, newdata, submodels = NULL) {
                    if(!is.data.frame(newdata)) newdata &lt;- as.data.frame(newdata)
                    if(modelFit$problemType == ""Classification"") {
                      probs &lt;-  predict(modelFit, newdata, type = ""response"")
                      out &lt;- ifelse(probs &lt; .5,
                                    modelFit$obsLevel[1],
                                    modelFit$obsLevel[2])
                } else {
                  out &lt;- predict(modelFit, newdata, type = ""response"")
                }
                out
              }
</code></pre>

<p>Any ideas about how to pass a grid of probability cutoff values to the predict function shown above to get the optime cutoff value?</p>

<p>I've been trying to adapt the code from the <a href=""http://topepo.github.io/caret/custom_models.html#Illustration5"" rel=""nofollow"">example shown in the caret website</a>, but I haven't been able to make it work. I think I'm finding difficult to understand how caret uses the model's interfaces... </p>

<p>Any help to make this work would be much appreciated... Thanks in advance.</p>
"
"0.083141099321054","0.0797241005179101","201015","<p>I am currently working on a project where I need to predict a outcome which on average occurs less number of times (for example, let's say the outcome is that a batter reaches the base in a baseball game. Now on average this event occurs with a probability of roughly 0.25-0.39 and seldom ever goes higher than that or lower than that)</p>

<p>Now the questions that I have for this situation:</p>

<ol>
<li><p>Working towards a logistic regression model to predict this, the output of that model on a test set is some kind of a probability which would roughly be in the same range (0.25-0.39). The question is how to come up with a decision threshold probability for such a low probability such that I can predict the class. Is there a specific algorithm, steps, or method that would be give me this?</p></li>
<li><p>Is having a different decision threshold for each observation a possibility?</p></li>
<li><p>Any other algorithm I can use to tackle this problem?</p></li>
</ol>

<p>Apologies if the question is vague or not well formed. Please let me know if I need to provide any additional information. Also, I am using R.</p>
"
"0.0858677581482184","0.102923371199077","202028","<p>I have been provided a sample logistic regression as follows:</p>

<p><code>glm(formula = output ~ X1 + X2 + X3 + X4 + X5 + X1:term + term:X5 - 1, family=""binomial"", data=mydata)</code></p>

<p>I am not too familiar with logistic regression, so I have a few questions about how to properly predict on a new test set using this model:</p>

<p>1) Unlike a regular regression, I cannot simply 'plug-in' the variables and get a meaningful numeric output. Instead, I must first set a threshold probability above which values will be 1 and below which values will be 0. Is this correct?</p>

<p>2) I cannot make use of this sample model or get the same results as the person who provided it until I have the probability threshold that was used for prediction. Is this correct?</p>

<p>3) If I wanted to split the outputs into tiers, would I use the probabilities for that and map them to some other value? How would that process work (feel free to let me know if this is out of scope).</p>

<p>Thanks!</p>
"
"NaN","NaN","202211","<p>I have a logistic regression in R whose goal is to predict the probability of default on some test data. </p>

<p><code>glm(default ~ X1 + X2 + X3 + X4 + X5 + X1:term + term:X5 - 1, family=""binomial"", data=mydata)</code></p>

<p>What I'd like to do is 'bin' this data so that bins 1 to n each have a certain rate of default. How can I bin the logistic regression results in this way? For example, the bins on a sample set of 1000 might look like:</p>

<pre><code>Bin# P(Default) Count
1         4%     400
2         2%     300
3         1%     300
</code></pre>

<p>That is, I set in advance the probabilities I want each bin to have (.04,.02,.01) and then bins are created based around those settings.</p>
"
"0.0678844233302131","0.0650944554904119","204119","<p>There is a package in R called <code>pwr</code>. This is useful to make power analysis when designing the sampling of a project. here are few examples: </p>

<pre><code>library(pwr)
pwr.anova.test(k = 4, f = 0.5, sig.level = 0.05, power = .9)
pwr.2p.test(h = 0.5, sig.level =0.05, power = .9)
pwr.f2.test(u = 4, f2 = .5, sig.level = 0.05, power = .8)
</code></pre>

<p>However, is it possible to run a power analysis for a spline regression (or a generalized additive model (GAM))? I want to know how may organisms I would have to sample to detect an effect of selection, that is a shift in morphology of the beak of birds of only 0.5Â mm, given that my sig.level = 0.05 and that I have 4 species. </p>

<p>Also, Iâ€™m recapturing birds in a population each year since 2003. Is there a power calculation to estimate how many birds should I sample to get a probability of recapture of 25%? Iâ€™m running a recapture model in Bayesian statistics, so there is not a function in the package <code>pwr</code> that can do this. </p>
"
"0.107334697685273","0.102923371199077","204751","<p>Let the dichotomous observation for the $i$ th cluster be $y_{i}=(y_{i1},...,y_{im})'$ and $p_{ij}$ be the probability of win/success, which defined as
$$p_{ij}= Pr(y_{ij}=1|v_{i})=\frac{\exp(z_{ij}'\beta)}{1+\exp(z_{ij}'\beta+v_{i})}$$
where $i=1,...,n; j=1,...,m$ and $z_{ij}=(z_{ij1},...,z_{ijp})$ is a set of independent variables corresponding to $y_{ij}$ and $\beta= (\beta_{1},...,\beta{p})$ is the regression parameters. Here, $v_{i}$ is the random effect for the $i$th cluster having $N(0, \sigma_{v}^2)$.</p>

<p>Now I have tried to generate simulated data from this model using following code in R.</p>

<pre><code>logit &lt;- function(x) log(x/(1-x))
invlogit &lt;- function(x) exp(x)/(1+exp(x))

## simulate cases
simcase&lt;-function(N,p) rbinom(N,1,p)
## simulation scenario
pi_ij &lt;- 0.05
n &lt;- 2; # number of clusters
Nmin &lt;- 2; # min number of subjects per cluster
Nmax &lt;- 10; # max number of subjects per cluster
set.seed(123)
v_i &lt;- rnorm(n,0,1) ; # random effects
logit1 &lt;- logit(pi_ij)+v_i
pind &lt;- invlogit(logit1)
Nsim &lt;- sample(Nmin:Nmax,n,replace=TRUE); # number of cases per cluster

data &lt;- data.frame(ev=do.call(c,mapply(simcase,Nsim,pind,SIMPLIFY=TRUE)),
    id=do.call(c,mapply(function(i,N) rep(i,N),1:n,Nsim)))
    complications$id&lt;-factor(complications$id)
data
</code></pre>

<p>Are these code correct? Can anybody help? 
Thanks in advance.</p>
"
"0.128004096196618","0.138086192684749","206075","<p>I'm relatively new to machine learning (started about 5 months ago), and I'm looking at potentially implementing an ensemble classifier as part of my research. </p>

<p>I have built 3 models that I use to classify whether sales data is going to win or lose. Each model produces the probability of the sale winning or losing, and then I apply thresholds to those to classify them as either a ""Win"", ""Loss"" or ""Borderline Loss"". There are 25 variables, all of which are discrete. </p>

<p>The three models are Naive Bayes, Tree Augmented Naive Bayes (TAN) and Logistic Regression. I am using the bnlearn package for the bayesian classifiers, and a simple glm for the Logistic Regression. All models have high accuracy performances when tested on unseen data:</p>

<p>Naive Bayes Accuracy: 88% </p>

<p>TAN Accuracy: 91%</p>

<p>Logistic Regression Accuracy: 92%</p>

<p>I want to try implementing an ensemble classifier to see if I can get the best possible accuracy across all three models. My question is, how do I go about implementing something like this? I can't find too many examples online, at least not with these models for implementing one. From what I have read, one way to do it is to have a voting system, where if the 2 models predict the sale will win, but 1 predicts with will lose, then it is classified as a win. But what happens in this case if all 3 models had different predictions? I have all my prediction data ready, as in I have all the test data and each models prediction for each sale, my question so is, how would I proceed from here? </p>

<p>If someone knows of any available resources or tutorials that may help, I would greatly appreciate it!</p>
"
"0.0480015360737319","0.0460287308949162","206405","<p>I am working on one of the discrete probability distribution having pmf as</p>

<p>P(x)={p^log(1+x^c)}-{p^log(1+(x+1)^c)}</p>

<pre><code>  0&lt;p&lt;1; c&gt;0; x=0,1,2,.
</code></pre>

<p>It fits well to count data of DMFT in dentistry with p value 0.93.</p>

<p>Decays of teeth: 0, 1, 2, 3 ,4</p>

<p>observed count: 64,17,10,3,4</p>

<p>Expected count: 63,16,9,4,4</p>

<p>Now i need suggestions how add other explanatory variables like sex, age, etc. to use this as nonlinear regression model in R.</p>
"
"0.083141099321054","0.0797241005179101","207427","<p>I am confused with the answer from
<a href=""http://stackoverflow.com/questions/23282048/logistic-regression-defining-reference-level-in-r"">http://stackoverflow.com/questions/23282048/logistic-regression-defining-reference-level-in-r</a></p>

<p>It said if you want to predict the probability of ""Yes"", you set as  <code>relevel(auth$class, ref = ""YES"")</code>. However, in my experiment, if we have a binary response variable with ""0"" and ""1"". We only get the estimation for probability of ""1"" when we set <code>relevel(factor(y),ref=""0"")</code>.</p>

<pre><code>n &lt;- 200
x &lt;- rnorm(n)
sumx &lt;- 5 + 3*x
exp1 &lt;- exp(sumx)/(1+exp(sumx))
y &lt;- rbinom(n,1,exp1) #probability here is for 1
model1 &lt;- glm(y~x,family = ""binomial"")
summary(model1)$coefficients
            Estimate Std. Error  z value     Pr(&gt;|z|)
(Intercept) 5.324099  1.0610921 5.017565 5.233039e-07
x           2.767035  0.7206103 3.839849 1.231100e-04
model2 &lt;- glm(relevel(factor(y),ref=""0"")~x,family = ""binomial"")

summary(model2)$coefficients
            Estimate Std. Error  z value     Pr(&gt;|z|)
(Intercept) 5.324099  1.0610921 5.017565 5.233039e-07
x           2.767035  0.7206103 3.839849 1.231100e-04
</code></pre>

<p>I think if we want to get probability of ""Yes"", we should set <code>relevel(auth$class, ref = ""No"")</code>, am I correct? And what is reference level here means? Actually, what is glm() to predict in default if we use response other than ""0"" and ""1""? </p>
"
"0.141118451592466","0.172223740956793","207608","<p>I'd like to run a probit regression on the ""B1_df"" data frame with 3 categorical outcome variables (rank 1,2 or 3). I cannot use glm because there are 3 outcome variables.  I would like to be able to tie out the results from polr() and mlogit().  I am getting reasonable results from polr() but strange results from mlogit() I believe due to my data frame construction.</p>

<p>Basically I have 3 machine B1, B2 and B3 and each have 5 runs that are ranked 1 to 3 and I am using probit to tell me which machine has the highest probability of returning the highest rank. </p>

<pre><code>First with polr():

require(ggplot2)
require(MASS)
require(mlogit)

machine = c(rep(""B1"",5), rep(""B2"",5),rep(""B3"",5))
rank = c(rep(3,5), rep(2,5),rep(1,5))
#rank = c(c(3,3,3,3,1), rep(2,5),rep(1,5)) # see *** comment below
dat = data.frame(machine = machine, rank = rank)
dat$B1 =  c(rep(1,5), rep(0,5),rep(0,5))
    dat$B2 =  c(rep(0,5), rep(1,5),rep(0,5))
dat$B3 =  c(rep(0,5), rep(0,5),rep(1,5))
B1_df = dat[,1:3]
B1_df
b1=polr(formula = as.factor(rank)~ as.factor(B1), data= B1,  Hess = FALSE, model = TRUE,method = c(""probit""))
b1

   machine rank B1
1       B1    3  1
2       B1    3  1
3       B1    3  1
4       B1    3  1
5       B1    3  1
6       B2    2  0
7       B2    2  0
8       B2    2  0
9       B2    2  0
10      B2    2  0
11      B3    1  0
12      B3    1  0
13      B3    1  0
14      B3    1  0
15      B3    1  0
&gt; b1=polr(formula = as.factor(rank)~ as.factor(B1), data= B1,  Hess = FALSE, model = TRUE,method = c(""probit""))
&gt; b1
Call:
polr(formula = as.factor(rank) ~ as.factor(B1), data = B1, Hess = FALSE, 
    model = TRUE, method = c(""probit""))

Coefficients:
as.factor(B1)1 
      8.599074 

Intercepts:
         1|2          2|3 
0.0002318407 4.4165977032 

Residual Deviance: 13.86319 
AIC: 19.86319 
</code></pre>

<p>Question:  Does the coef of 8.5 indicate that by setting B1 =1 the z-score would increase by 8.5 giving a higher probability of getting a higher rank? I was thinking that it did but then I uncommented this line:</p>

<pre><code>rank = c(c(3,3,3,3,1), rep(2,5),rep(1,5))
</code></pre>

<p>so now B1 does not have all 3's it has 4 3's and one 1. I was expecting the coef, intercepts, deviance to change but they don't.  Please uncomment ** and run. Any idea why no change?</p>

<p>Now I'd like to try to get those same results in mlogit:</p>

<pre><code>B1_df2 = mlogit.data(B1_df, shape = ""wide"", choice =""rank"", id.var= ""B1"") #configure the data frame with mlogit.data
B1_df2
summary(mlogit(rank ~  0 | B1 ,data=B1_df2, Probit=TRUE))  # call mlogit 

&gt; summary(mlogit(rank ~  0 | B1 ,data=B1_df2, Probit=TRUE))  # call mlogit

Call:
mlogit(formula = rank ~ 0 | B1, data = B1_df2, Probit = TRUE, 
    method = ""nr"", print.level = 0)

Frequencies of alternatives:
       1        2        3 
0.533333 0.400000 0.066667 

nr method
16 iterations, 0h:0m:0s 
g'(-H)^-1g = 7.84E-07 
gradient close to zero 

Coefficients :
                 Estimate  Std. Error t-value Pr(&gt;|t|)
2:(intercept) -8.9248e-17  6.3246e-01  0.0000   1.0000
3:(intercept) -1.6669e+01  1.8625e+03 -0.0089   0.9929
2:B1          -1.0986e+00  1.3166e+00 -0.8345   0.4040
3:B1           1.5570e+01  1.8625e+03  0.0084   0.9933

Log-Likelihood: -11.683
McFadden R^2:  0.11726 
Likelihood ratio test : chisq = 3.1037 (p.value = 0.21186)
</code></pre>

<p>You can see the mlogit coefs are nowhere near the 8.5 and there seem to be duplicates.</p>

<p>For mlogit() I am looking at page 22 here for the pure multinomial model:</p>

<p><a href=""https://cran.r-project.org/web/packages/mlogit/mlogit.pdf"" rel=""nofollow"">https://cran.r-project.org/web/packages/mlogit/mlogit.pdf</a></p>

<p>Any idea how to properly set up these models to get them to tie?</p>
"
"0.0678844233302131","0.0650944554904119","207890","<p>I'm able to obtain predicted survival probabilities of cox regression using either <code>survfit.coxph</code> or <code>predictSurvProb</code> from <code>pec</code> package. However, using these approaches I'm unable to predict the probability of new observation which has the time set after the study, or the end time of dataset used to construct the model, e.g. the period of study is from 0 to 1000, and the time for new observation is 1200.</p>

<p>Is this the limitation of cox regression? Otherwise, is there any alternative to calculate the aforesaid probability?</p>
"
"0.0858677581482184","0.102923371199077","208571","<p>I am trying to model my dependent variable (ordinal - three levels) using a set of independent variables (5 ordinal and 10 numeric). I am using <code>lrm</code> function in ""rms"" package of R. I am conducting principle component regression. <code>S1</code>, <code>C5</code>, <code>C2</code>, <code>C3</code>, <code>S7</code> and <code>S4</code> are the selected independent variables using PCA. </p>

<pre><code>          Coef         S.E.   Wald   Z    Pr(&gt;|Z|)
          y&gt;=2      -1.0469 0.6092 -1.72  0.0857  
          y&gt;=3      -8.5826 1.0354 -8.29  &lt;0.0001 
          S1=Simple -2.9091 0.6112 -4.76  &lt;0.0001 
          C5         0.8389 0.1475  5.69  &lt;0.0001 
          C2         1.4904 0.1889  7.89  &lt;0.0001 
          C3         1.2139 0.1908  6.36  &lt;0.0001 
          S7         0.8803 0.2701  3.26  0.0011  
          S4=TN     -1.2460 0.4659 -2.67  0.0075  
</code></pre>

<p>I understand, the output of the ordinal regression model is given by,</p>

<pre><code>ln(Fij/ 1-Fij) = Boj + B1X1 + B2X2 + .....BkXk

where Fi1 is probability that Y=1, 
Fi2 is probability that Y=2, 
Fi3 is probability that Y=3
B0, B1.....Bk - coefficients
X0, X1.....Xk - Independent variables
</code></pre>

<p>My question is, how do we interpret negative coefficients here? Also, does ranking the values of Wald statistics from largest to smallest indicate descending strength of evidence of an association with the dependent variable?</p>
"
"0.0678844233302131","0.0650944554904119","208677","<p>I want to model patient visits. My assumptions are:</p>

<ol>
<li><p>Patients visit the hospital until they stop visiting at all. I don't know if their last visit was the last one.</p></li>
<li><p>Patients visit at certain intervals. These intervals vary across patients, but are roughly the same for a patient. </p></li>
</ol>

<p>I've been searching and reading all day long and my head is about to explode, so I decided to ask what are the ways to approach this. </p>

<p>My best guess is some sort of survival analysis and it looks like survival regression supports recurring events. The problem is that there are multiple ways to do this and I don't know which one to use. </p>

<p>What I'm trying to get out of the model:</p>

<ol>
<li>Probability the patient return at all, given time elapsed from his
last visit.</li>
<li>The number of visits a patient will make over a period T.</li>
</ol>

<p>Can I model this with one regression? Which one?</p>
"
"0.0480015360737319","0.0460287308949162","209374","<p>I'm using the <code>multinom</code> package in R to run a multinomial logistic regression model. My dependent variable has 3 levels and as the output, I'm getting the probability for each of the level.</p>

<p>Currently, I have the VIF, AIC, p-values and confusion matrix in the model.</p>

<p>I have the following questions:</p>

<ol>
<li><p>I want a single output based on the probabilities. How do I decide a ""cut-off"" for deciding the ""best event""?</p></li>
<li><p>Does it make sense to get an ROC curve here? If yes, then how do I get one?</p></li>
<li><p>What are the things I should look at for the validation of the model?</p></li>
</ol>
"
"0.144730076833889","0.152660029954676","209412","<p>I'm trying to build a bivariate copula-based model of income and wealth in Italy and I'm having trouble handling weighted data. I have access to micro data, a survey of about 10,000 households that includes the corresponding sample weights.</p>

<p>When calculating basic statistics (like mean and median) and even when performing linear regressions it is pretty easy to account for weights, besides there are useful packages for that (e. g. survey). But what do I do when I want to fit a parametric model of the distribution to weighted data? Or to estimate its kernel density?</p>

<p>I have a few ideas, but they seem to be pretty crude. For one, I could inflate my sample to the size of the universe. That is, I could multiply all weights by 100 (which would turn them into integers) and then create a vector that repeats each value of income and wealth a given number of times. But that would lead to a very large sample (which I believe still wouldn't be a perfect representation of the population) and will certainly put some extra strain on my computer.</p>

<p>I could also just round the weights off instead of multiplying them by 100, but this would still make the sample noticeably bigger and will inevitably skew the real proportions.</p>

<p>Another approach I came up with would be to normalize the weights (so that they sum up to one) and then randomly sample with repetitions from my initial sample with the corresponding vector of probability weights. R doesn't allow to draw the samples that are larger in size than the one that they are being drawn from. But I think that drawing the sample of the same size as the initial one will lead to some loss of information about the observed proportions. So I could draw the samples of the initial size as described above several times (how would I know how many is though?) and then combine them into one sample. And again, I will have a larger sample with some of the information lost along the way.</p>

<p>So I was wondering if there is a better way to handle weighted data. In some cases I think I could technically introduce the weights into the formula for computing the maximum likelihood for fitting a particular model, although I certainly wouldn't like to code that from the ground up. I will have to fit a lot of models as part of my project, both univariate (e. g. Singh-Mandala) for income and wealth and bivariate for copulas. I don't think the built in functions in any of the copula-related packages that I'm aware of allow one to account for weights. So any advice would help!</p>
"
"0.192339199435604","0.195283366471236","209864","<p>This is a very simple exercise that I'm hoping may help people with limited knowledge in statistical analysis (like myself). 
I am having trouble deciding what statistical analysis I can perform (in R) to determine whether or not my data are closer to one linear model or another. </p>

<p>For example: I have measurements of sodium and chloride in various dilute solutions: </p>

<pre><code>#
Na &lt;- c(1.56, 1.00, 1.60, 3.23, 2.02, 2.81, 2.09, 26.24, 1.59, 0.42)
Cl &lt;- c(1.40, 0.91, 1.22, 2.67, 1.67, 3.01, 2.17, 27.42, 1.45, 0.51)
</code></pre>

<p>For simplicity, this solution is a dilution of either table salt dissolved in water or natural seawater. For each case, Cl/Na will be a specific ratio that reflects the composition of the original solution. We can visualize this by:</p>

<pre><code>plot(Na,Cl)
abline(0,1)    # expected slope for table salt dissolved in water
abline(0,1.16) # expected slope for natural seawater.
</code></pre>

<p>I want to know which model, table salt in water or seawater, is a more statistically accurate fit to the provided data. Linear regression analysis in R gives a line of best fit with a slope of 1.05 (<code>lm(Cl~Na)</code>), right in between the two models.</p>

<p>So, which solution do I more likely have and why? The line of best fit slope is closer to that of table salt dissolved in water, but that does not seem very statistically sound. Thoughts? </p>

<p>Edit: @whuber mentioned that there is one anomaly in the dataset - in reality, the provided data is just a subset of the original data. There are actually hundreds of data points in between the apparent outlier and the rest of the provided data.</p>

<p>Also, here is a <code>log(Na)-log(Cl)</code> summary of the complete dataset:</p>

<pre><code>    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's 
-0.46870 -0.06186  0.02654  0.02218  0.12780  0.47510      183 
</code></pre>

<p>Edit2: As for the ""true nature of my investigation"": The 'solution' in question is likely a mixture of both table salt water and natural seawater. What I'd like to do is find a definitive way (through statistical analysis) to show that I have more of one or the other. I had hoped that my simplified question/dataset would yield an answer from the community, but it seems I was off base. If it helps, a complete dataset is now hosted below:</p>

<p><a href=""http://www.filedropper.com/clna"" rel=""nofollow"">http://www.filedropper.com/clna</a></p>

<p>Looking at the distribution of the complete data shows I have more Cl/Na about 1.00, but this does not seem 'sound enough' to back up an argument. The probability that I have one solution or the other is unknown. I have the raw data and relevant models for Cl to Na to run with.</p>

<p>For clarification, the original question is still the one I'd like to solve.  An alternative question could be: Which solution do I have <em>more</em> of and what analysis did I use to come to that conclusion?</p>
"
"0.107334697685273","0.102923371199077","210646","<p>I am replicating an analysis that models tree mortality data. Data are structured such that forest sites are revisted at some random interval, which is recorded. It is then determined if a tree lived or died over that random interval, generating 0 1 mortality data (if a tree dies, it gets a 1 in the dependent variable). The interval between initial and final observation varies continuously, from 5-15 years. This is relevant, as the more time that passes, the more likely a tree will die. </p>

<p>Here are some pseudo data for R:</p>

<pre><code>mort &lt;- c(0,1,0,0,0,0,0,1,1,1,1,0,1,0,0,0,0,0,0,0,1,0)
interval &lt;- runif(length(mort), 5, 15)
pollution &lt;- rnorm(length(mort), 25,5)
data&lt;- data.frame(mort, interval, pollution)
</code></pre>

<p>I am trying to replicate an analysis which uses a logistic regression model for binary mortality data using the the logit transformation. Authors then model how pollution affects tree mortality rates. In the manuscript the authors write, ""because recensus is not annual, we relate annual mortality probability, <code>pi</code>, of tree <code>i</code> to the observed binomial data on whether that tree lived or died <code>Mi</code> via a Bernoulli likelihood,</p>

<p><a href=""http://i.stack.imgur.com/7i7jA.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7i7jA.png"" alt=""enter image description here""></a></p>

<p>where <code>ti</code> is the time interval between successive censuses.""</p>

<p>My question: How would I implement this using the <code>glm</code> function, or something analagous, in R? Note: I understand modeling this as a hazard function would also be appropriate, but it is not what I am interested in.</p>
"
"0.0960030721474639","0.0920574617898323","211174","<p>We are oversampling the data to use in logistic regression. Aim  is to predict CTR(click probability) which is rare event scenario.
I have predicted the probabilities of click but CTR results are inflated as we over sampled positive class.</p>

<p>model2&lt;-SMOTE(V61 ~ ., z2, perc.over = 600,perc.under=100, learner = 'glm',family=binomial())</p>

<p>Is there any way to undo oversampling results so that I can get exact probabilities ? Based on research so far, one easiest way to divide the output probability by the multiplier we used in over sampling. I dont feel it would be the exact way as I have used synthetic minority over sampling technique(SMOTE) in R.</p>
"
"0.083141099321054","0.0797241005179101","212027","<p>I am using <code>gbm</code> to predict an imbalanced binary outcome, with the intent of obtaining a ranking by class probability estimation that produces a strong class separation on out-of-sample data.  (I am combining this class probability with other predictions, including from logistic regression, in an ensemble model.)</p>

<p>According to <a href=""http://bioconductor.wustl.edu/extra/vignettes/gbm/inst/doc/gbm.pdf"" rel=""nofollow"">this gbm vignette</a> (Ridgeway, 2007), under ""common user options"" for loss functions:</p>

<blockquote>
  <p>This should be easily dictated by the application.  For most
  classification problems either <code>bernoulli</code> or <code>adaboost</code> will be
  appropriate, the former being recommended. (p. 5)</p>
</blockquote>

<p>There's no explanation provided for favoring bernoulli over <a href=""http://stats.stackexchange.com/questions/37497/how-to-use-r-gbm-with-distribution-adaboost"">adaboost</a> nor any mention of the option for <a href=""https://en.wikipedia.org/wiki/Huber_loss"" rel=""nofollow""><code>huberized</code> loss function</a>, although this function may have been added at a later date.</p>

<p>Related question, but broader than mine:  <a href=""http://stats.stackexchange.com/questions/112359/choosing-between-loss-functions-for-binary-classification"">Choosing between loss functions for binary classification</a>.  This answer references <a href=""http://www.eecs.berkeley.edu/~wainwrig/stat241b/bartlettetal.pdf"" rel=""nofollow"">Bartlett (2006)</a> which is a challenging read for me.</p>

<p>Although performance is satisfactory under the bernoulli loss function, I am having a hard time understanding the justification for selecting one over another.  I'm trying all of them, but are there any theoretical justifications that are at least somewhat intuitive?</p>
"
"0.0480015360737319","0.0460287308949162","216119","<p>I am studying how well Kobe Bryant shoots and to do so I have run a logistic regression. The variable shot_made_flag is 0 if missed and 1 if he scored. And I am running the regression against distance from the basket.</p>

<pre><code>  logitshots &lt;- glm(df$shot_made_flag ~ df$shot_distance, family = binomial(link=""logit""))
Call:  glm(formula = df$shot_made_flag ~ df$shot_distance, family = binomial(link = ""logit""))

Coefficients:
 (Intercept)  df$shot_distance  
      0.3681           -0.0441  

Degrees of Freedom: 25696 Total (i.e. Null);  25695 Residual
Null Deviance:      35330 
Residual Deviance: 34290    AIC: 34300
</code></pre>

<p>As you see the coefficient of distance is negative. So what I do next is to compute the probability of scoring if Bryant is 1 meter farther. </p>

<p>To do so I have done it this way, but I get a positive effect, so I am not sure about it. </p>

<pre><code>(exp(coef(logitshots))/(1+exp(coef(logitshots))))
(Intercept) df$shot_distance 
   0.5909933        0.4889768 
</code></pre>

<p>So how would you interpret this? every 1 meter means a 48% more chances of scoring (Lol)? Is this approach the right one? I guess that Kobe scoring from 25 meters is very unlikely (maybe modelling by a quadratic function?)  </p>

<p>I'd really appreciate any interesting insight and help! :)</p>
"
"0.083141099321054","0.0797241005179101","218276","<p>Imagine a data set with approximately 100 variables and 5000 cases. The outcome is a two-level factor. All variables are factors, most of them three levels (yes, no, or indifferent).</p>

<p>After building a simple logistic regression model I'm in doubt about how to reduce the amount of variables used in the final model (and find a proper validation method).</p>

<p>Before building the model I've used chi-squared to examine individual relations between some predictors and the outcome, but this becomes kind of stupid.</p>

<p>Any advice about how to tackle this issue, preferably in an automated way? I've some understanding of basic probability but would use Lasso or Ridge Regression more as a black box now. </p>

<p>As tools I use R and Python.
Thanks in advance!</p>
"
"0.107334697685273","0.102923371199077","218477","<p>I have a database which contains 100k records. It includes 2 continuous and 6 categorical variables. The output is categorical as well and it can take one of 8 different values (e.g. 1, 2, 3...8). My goal is:</p>

<ol>
<li><p>Investigate which of the variables are the most significant ones to determine my output.</p></li>
<li><p>After the analysis, to be able to calculate the probability for any of those outputs to happen if I only know what are the values for (e.g.) two variables? For example, to have some coefficients for every possible categorical value in order to calculate the probability...</p></li>
</ol>

<p>I tried this with the logistic regression but somehow I have big deviation from manually calculated probability (e.g. when I use the number of positive outputs and the total number of the records contained within my database). Anybody has a better idea how I could analyse this? Sth better than logistic regression?
Thanks in advance!</p>
"
"0.0480015360737319","0.0460287308949162","218842","<p>I'm using a random forest in R (randomForest) to predict a binary output (1,0) for a dataset that is heavily unbalanced. In this example let's assume the population has 1% 1's and 99% 0's.</p>

<p>Building the random forest on such unbalanced data is difficult and I get much better results when building it on a 50:50 sample.  When predicting a validation set, I obtain the % of trees that predicted that data point to be a 1.  For example, customer A has a 75% probability of being a 1 (based on the # of trees that predicted 1)</p>

<p>If I want to re-scale these predictions back to the original population ratio of 1:99, is there a good way to do this?</p>

<p>In the past I've used logistic regression, and I can adjust the intercept accordingly to down-scale the predicted probability.</p>

<p>Is there a good way to think about this from the RF point of view?  Can I simply just down-weight the predictions from the 50:50 sample by 50 (50% down to 1%)?</p>

<p>Thanks in advance for any thoughts and help</p>
"
"0.166282198642108","0.15944820103582","219679","<p>I would like to know how to find out the analytical solution of a simple linear regression with fixed intercept = 0:</p>

<p>$$ s = e^{-ht}$$
$$ y = -ln(s)  = h\cdot t$$</p>

<p>Here ist the background: I have three survival probabilities $s$ at 30, 90 and 180 days. Obviously, I have at day = 0 100% survival, so I  include this <em>observation</em>. I know that this is contested (<a href=""http://stats.stackexchange.com/questions/102709/when-forcing-intercept-of-0-in-linear-regression-is-acceptable-advisable"" title=""here"">here</a>) but I think in this special case it makes sense. The data I use for fitting the linear regression:</p>

<pre><code>&gt;     obs
    t    s          y
1   0 1.00 0.00000000
2  30 0.98 0.02020271
3  90 0.90 0.10536052
4 180 0.80 0.22314355
</code></pre>

<p>If I fit with simple regression I get this:</p>

<pre><code>&gt;     (fit1 &lt;- lm(y~t, data=obs))

Call:
lm(formula = y ~ t, data = obs)

Coefficients:
(Intercept)            t  
  -0.008464     0.001275
</code></pre>

<p>This can be obtained analytically if the following function is derived:</p>

<p>$$f(h) = \sum (y_i - ht_i)^2$$</p>

<p>which gives:</p>

<p>$$ \frac{\sum (y_i-\bar{y})\cdot (t_i-\bar{t})}{\sum (t_i-\bar{t})^2}$$</p>

<hr>

<p>UPDATE 1: This is the result of the minimization of 
$$f(h) = \sum (y_i - c - ht_i)^2$$. The correct result (see answers):
$$ \frac{\sum (y_i\cdot t_i)}{\sum t_i^2}$$</p>

<hr>

<p>The analytical results is:</p>

<pre><code>yc &lt;- with(obs,y-mean(y))
tc &lt;- with(obs, t -  mean(t))
sum(yc*tc)/sum(tc^2)
[1] 0.001275204
</code></pre>

<p>The same as coefficient in the fit1. Now, if I fix intercept to intercept=0 I get this:</p>

<pre><code>&gt;     (fit2 &lt;- lm(y~0+t, data=obs))

Call:
lm(formula = y ~ 0 + t, data = obs)

Coefficients:
   t  
0.001214  
</code></pre>

<p>I'm wondering how I can get an analytical solution for this. How I have to consider the fix intercept in the function $f(h)$ above?</p>

<p>Any idea is appreciated.</p>

<p><a href=""http://i.stack.imgur.com/ilwvG.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ilwvG.png"" alt=""enter image description here""></a></p>

<hr>

<p>Here is the way I constructed the data:</p>

<pre><code>set.seed(123)
# Hazard ratio
h &lt;- 0.7
# Number of observation
n &lt;- 50
# Model: exponential
t &lt;- rexp(n,h)
# scale to days
t &lt;- t*365.25
hist(t)
t &lt;- sort(t)
# Put data into a dataframe
df0 &lt;- data.frame(t=t)
head(df0)
# Compute probablities
df0$s &lt;- 1 - c(1:n)/n
head(df0)
# Extract survival probabilities at 30,90 and 180 days
df0$t2 &lt;- ceiling(df0$t/30)*30
# Select survival probablity 30, 90, 180 days
library(sqldf)
obs &lt;- sqldf(""SELECT t2 t, MAX(s) s FROM df0 WHERE t2 IN (30,90,180) GROUP BY t2"")
# Add survival probability=1 at day 0
obs &lt;- rbind(data.frame(t = 0, s = 1), obs)
# s = e^(-ht)  =&gt; y = -ln(s) = h*t
obs$y &lt;- -log(obs$s)
plot(y~t, data=obs)
fit1 &lt;- lm(y~t, data=obs)
abline(fit1,lty=2)
fit2 &lt;- lm(y~0+t, data=obs)
abline(fit2,lty=2, col=""red"")
legend(""topleft"", legend=c(""fit1"",""fit2""), col=c(1,2), lty=c(2,2))
</code></pre>
"
"0.127000127000191","0.121780575111864","219828","<p>I am doing logistic regression in R on a binary dependent variable with only one independent variable. I found the odd ratio as 0.99 for an outcomes. This can be shown in following. Odds ratio is defined as, $ratio_{odds}(H) = \frac{P(X=H)}{1-P(X=H)}$. As given earlier $ratio_{odds} (H) = 0.99$ which implies that $P(X=H) = 0.497$ which is close to 50% probability. This implies that the probability for having a H cases or non H cases 50% under the given condition of independent variable. This does not seem realistic from the data as only ~20% are found as H cases. Please give clarifications and proper explanations of this kind of cases in logistic regression.</p>

<p>I am hereby adding the results of my model output:</p>

<pre><code>M1 &lt;- glm(H~X, data=data, family=binomial())
summary(M1)

Call:
glm(formula = H ~ X, family = binomial(), data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.8563   0.6310   0.6790   0.7039   0.7608  

Coefficients:
                Estimate      Std. Error      z value     Pr(&gt;|z|)    
(Intercept)    1.6416666      0.2290133      7.168      7.59e-13 ***
   X          -0.0014039      0.0009466     -1.483      0.138    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1101.1  on 1070  degrees of freedom
Residual deviance: 1098.9  on 1069  degrees of freedom
  (667 observations deleted due to missingness)
AIC: 1102.9

Number of Fisher Scoring iterations: 4


exp(cbind(OR=coef(M1), confint(M1)))
Waiting for profiling to be done...
                                      OR           2.5 %       97.5 %
(Intercept)                    5.1637680       3.3204509     8.155564
     X                         0.9985971       0.9967357     1.000445
</code></pre>

<p>I have 1738 total dataset, of which H is a dependent binomial variable. There are 19.95% fall in (H=0) category and remaining are in (H=1) category. Further this binomial dependent variable compare with the covariate X whose minimum value is 82.23, mean value is 223.8 and maximum value is 391.6. The 667 missing values correspond to the covariate X i.e 667 data for X is missing in the dataset out of 1738 data.</p>
"
"0.186273320869547","0.18978131929287","220001","<p>I'm trying to model a logistic regression in R between two simple variables:</p>

<ul>
<li>Rating: An independent ordered categorical one, ranging from 1 to 99 (1, 2, 3, 4, 5, 99 in particular, 1 is the best)</li>
<li>Result: A dependent binary variable (0-1, not accepted/accepted)</li>
</ul>

<p>The formula I use is </p>

<pre><code>glm(formula = result_dummy ~ best_rating, family = binomial(link = ""logit""), 
    data = cd[1:10000, ])
</code></pre>

<p>result_dummy is a 0/1 numerical variable (original result column was a factor) and scaled_rating is the rating column after use the R <code>scale</code> function.</p>

<p>My thought here was to find a negative correlation (low rating -> more probability to accept) but the more samples I use the more odd results I find:</p>

<pre><code>10 samples:

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)     0.6484     0.7413   0.875    0.382
scaled_rating  -5.9403     5.8179  -1.021    0.307
</code></pre>

<hr>

<pre><code>100 samples:
Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)   -0.09593    0.27492  -0.349  0.72714   
scaled_rating -5.06251    1.76645  -2.866  0.00416 **
</code></pre>

<hr>

<pre><code>1000 samples:

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -0.03539    0.09335  -0.379    0.705    
scaled_rating -6.81964    0.62003 -10.999   &lt;2e-16 ***
</code></pre>

<hr>

<pre><code>10000 samples:
Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)     0.2489     0.0291   8.553   &lt;2e-16 ***
scaled_rating  -7.2319     0.2004 -36.094   &lt;2e-16 ***
</code></pre>

<hr>

<p>Notes:
I know that after the fit I should check residual plot, normality assumptions, etc. etc. but nonetheless I find really strange this behaviour.</p>

<p>I also have similar results using simply the rating column instead of the scaled one.</p>

<p>Edit:
The <code>rating</code> variable is not really an ordinal one, so as pointed out by @Scortchi maybe it would be better to treat it as a categorical one.
I have surely better results and model stability, obviously the model is a simple one and the residual error would be always high (because some variables as not been included in the model).
Indeed, including the frequency table as requested shows that the rating variable IS NOT sufficient for having a clear separation between the result outcome.</p>

<pre><code>          0      1
  1    2881  42564
  2   13878 129292
  3   36839 179500
  4   43511  97148
  5   37330  47002
  6   31801  21228
  7   19096   6034
  99  10008      3
</code></pre>
"
"0.083141099321054","0.0797241005179101","220364","<p>So, im in a bit of trouble here. I am using R (i'm very new at this), and i'm trying to plot the probability effects of a interaction effect, using the effects package. </p>

<p>This is what the plot shows<a href=""http://i.stack.imgur.com/bBR1O.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bBR1O.jpg"" alt=""enter image description here""></a></p>

<p>However, when looking at the logistic regression model: it shows a b coefficient of B -1.333**, ExpB.27 indicating a negative moderation effect.</p>

<p>My quistion: how do i interpret this plot? and how does this relate to the findings? </p>

<p>Thank you guys in advance</p>

<p>Update:
the code i used is: </p>

<pre><code>data.mod &lt;-glm(outc_bin1~ctr_projsize+ctrfirmage+ctr_avgfirmsize+ctr_unirep+ctr_EPO+ctr_avginv+ctr_funding+ctr_projage+ctr_patent+techdiv+involvement+geolog+tech2+techdiv:involvement+tech2:involvement+geolog:involvement, family=binomial(link = ""logit""), data=data, x=TRUE)

plot(effect(""techdiv:involvement"", data.mod, xlevels=list(involvement=c(1, 2, 3, 4)))
</code></pre>

<p>Regression output:</p>

<p><a href=""http://i.stack.imgur.com/pjQOH.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pjQOH.jpg"" alt=""enter image description here""></a></p>
"
"0.127000127000191","0.121780575111864","221427","<p>I want to understand the interpretation of logistic regression coefficients in terms of an increase in probability of dependent variable being 1. </p>

<p>I tested a logistic regression model in R and got the following coefficients (all statistically significant):</p>

<pre><code>&gt; mud$coefficients
  (Intercept)          var1          var2          var3          var4
-3.557573e+00  1.051031e-01  4.937244e-07 -1.308386e-06  3.937646e-01
</code></pre>

<p>Raising these numbers to the power of e resulted in numbres below. I would interpret them so that a 1 unit increase in var1 would increase the probability of dependent variable being 1 by 11% and 1 000 000 unit decrease in var3 would increase that probability by 1%. </p>

<pre><code>&gt; exp(mud$coefficients)
(Intercept)       var1        var2        var3        var4
0.02850792  1.11082516  1.00000049  0.99999869  1.48255150
</code></pre>

<p>As suggested in a previous a question (<a href=""http://stats.stackexchange.com/a/24422/121763"">http://stats.stackexchange.com/a/24422/121763</a>), below is what I should actually do to find the probabilities.</p>

<pre><code>&gt; exp(mud$coefficients)/(1+exp(mud$coefficients))
(Intercept)       var1        var2        var3        var4 
0.02771774  0.52625162  0.50000012  0.49999967  0.59718862
</code></pre>

<p>So which numbers should I use if I'd like to express the effect of independent variables on the probability of and event occurring? E.g. in case of var1 is it 11% or 53% or something else? </p>
"
"0.152425348755266","0.15944820103582","222479","<p>I'm new in this area, hope my question is understandable.
I need to fit conditional logistic regression model in R and use it for predictions on unseen data (output should be probability).
My datasets are  quite large (over 150k rows) and contains many (~500) noisy features.
I found package called <strong>clogitboost</strong> and tried to use it with relatively small number of boosting iterations (max 30, because with larger values it takes too long to compute and raises an error in the end - perhaps, it's resources limitations) - results are mediocre. I tried to use unconditional approach with regularization - <strong>glmnet</strong> and got better results, however, due nature of data I guess it will be better to use conditional regression with regularization similar to what is used in <strong>glmnet</strong> (tried to remove some features and apply <strong>clogitboost</strong> again and got slightly better results). There is package called <strong>clogitL1</strong> , which seems to do that, I tried to use it and it fits model quickly, but it doesn't provide <strong>predict()</strong> function, Usage described in  paper with attached R code here:
<a href=""https://www.jstatsoft.org/article/view/v058i12"" rel=""nofollow"">https://www.jstatsoft.org/article/view/v058i12</a>, they made some predictions in some way, but I can't understand it. Can I somehow manually predict using  unseen data, just like it's possible with <strong>clogitboost</strong> <strong>predict()</strong> (parameters are Model, X and Strata column) using model that was fitted with <strong>clogitL1</strong>? Note: in description of package <strong>clogitL1</strong> - ""Tools for the fitting and cross validation of <strong><em>exact</em></strong> conditional logistic regression models"" - so I'm not sure about what ""exact"" means here and  if it makes sense to use that package for my purposes. If it's not possible to predict, then, should I manually select features by checking their ""importance"" that can be found in <strong>clogitL1</strong> model? </p>
"
"0.0678844233302131","0.0650944554904119","224438","<p>I've created a multivariate binary logistic regression web app using shiny and R for one of my final projects of the semester. I would like a little critique on it and would love to learn more of just exactly what I'm dealing with. </p>

<p>My model attempts to predict Political Party Affiliation by using the variables of <strong>Age, Sex, Race, Degree of Education, Church Attendance.</strong></p>

<p>I used data collected from the General Social Survey (GSS) and have about 1300 observations in my final data set after everything was cleaned and garbage values taken out. </p>

<p>One problem that I can already see is the distribution of <strong>Race</strong> in my dataset, it's pretty skewed. </p>

<p><a href=""https://rtutorials-portfolio.shinyapps.io/PartyID/"" rel=""nofollow"">My web app can be found here</a></p>

<ul>
<li><p>The probabilities tab shows the expected probability of someone being republican.</p></li>
<li><p>The presentation tab shows a quick presentation on how I went about creating the logistic regression.</p></li>
<li><p>The graphics tab holds all of my graphics of the dataset.</p></li>
<li><p>The model info tab shows the <code>summary()</code> of the glm() and the results of the Hosmer-Lemeshow Test. </p></li>
</ul>

<p>I would love any critique or things you think I could do better with (web design, statistics, coding, etc.)</p>
"
"0.180005760276495","0.184114923579665","226073","<p>I have panel data that is structured like the example below only with more variables. I am using R and my goal is pretty straight forward - 
estimate the effect of the independent variables on my dependent variable.</p>

<pre><code>        country   date        dependent     independent type1  independent type2
        Germany   01/01/2006  70            30                 0.754
        Germany   01/02/2006  72            36                 0.821
        ...  
        Germany   12/31/2016  70            16                 1.214
        Italy     01/01/2006  54            30                 0.213
        Italy     01/02/2006  59            36                 0.343
        ...
</code></pre>

<p>I assume that there are country specific effects that probably also vary across time in the long run which is why I wanted to include fixed effects and then split my panel into 4 time periods and run a regression that is simply like this:</p>

<pre><code>time period 1 lm(dependent ~ independent1+independent2+independent3 ....+country)
time period 2 lm(dependent ~ independent1+independent2+independent3 ....+country)
...
</code></pre>

<p>However when skimming trough some books  I became increasingly unsure (and confused) if this is an adequate approach. Thus my two questions would be:</p>

<ol>
<li><p>Is what I intended to do the (or a) suitable approach to achieve my objective?             </p>

<ol start=""2"">
<li>Can you recommend some other ways to estimate this? I am also very interested in trying some ""creative"" things as long as they are not way over my head.</li>
</ol></li>
</ol>

<p>One more remark.
I indicated type 1 and type 2 for the independent variables. While both vary across time the first one does not vary across countries. I am not sure if that is important on the other hand I feel like I am not sure about anything anymore after looking trough those statistics books.</p>

<p>Thank you.</p>

<p><strong>EDIT:</strong> </p>

<p>What I mean with ""fixed effects change over time"" 
There are (for me) unobservable variables. The effect of these variables is of different magnitude for every country however it will also change to some extend over time. I thought that fixed effects might be able to improve my estimation in that they would capture something like ""the average"" effect of these variables for every country. </p>

<p>It might be difficult to explain without the economic context. I left it out before, because I thought it might be clearer when I express it in general terms but maybe this helps. </p>

<p>I look at the credit default swap (CDS) bond basis which is a spread so simply the difference between the two ""yields"". Now some of this spread I can explain with variables that I am able to proxy for like counter party risk that is involved in CDS etc. However some other parts like ""embedded options in CDS contracts"" I can not observe or proxy for. The impact of these variables will likely be large and also be different for every country. So for example the option value is connected to the default risk of the country thus it will vary over time (as the default probability will vary over time) but especially it will be different for say Greece and Germany.</p>
"
"0.127000127000191","0.121780575111864","230167","<p>I started tinkering with the random forest (RF) model in R recently. I made a long list of coding mistakes on my way to getting a final solution. The output from the mistake that surprised me was when I used the sample operation incorrectly and used the same records for training and validation. The RF algorithm accurately classified all but about 5 of 2000 responders. This shocked me. I later learned how to correctly sample.</p>

<p>The idea that RF could memorize specific records is the focus of my concern. Even though each tree votes on whether the record is responder or non-responder, for example, I think probability has been short-circuited for validation or non-training scoring files by a type of specification bias that ignores maximum likelihood if the validation or scoring record matches a record combination (of fields) that was used in training. This circumstance is how a Relation Database System (RDB) dictionary helps the Oracle or DB2 optimizer (it memorizes performance info to optimize the next exact job/query). The very nature of variability on earth (including statistics) implies that there will be some degree of inconsistencies in observed phenomenon. RF ignores at least some potential variability and the very nature of maximum likelihood probabilities if it correctly classifies almost 100% of records if the validation records are the training records. What happened to the accumulation of (or count of) observed inconsistencies in the data? I think this is a type of specification bias that ignores inherent variability for the validation or non-training scoring data file.</p>

<p>The idea that RF has outperformed Logistic Regression doesn't preempt the issue.</p>

<p>Could we agree that RF is more ""Rule Induction"" than probability?</p>
"
"0.117579270250443","0.112746904200424","233366","<p>I am trying to use <code>lme4::glmer()</code> to fit a binomial GLMM with dependent variable that is not binary, but a continuous variable between zero and one. One can think of this variable as a probability; in fact it <em>is</em> probability as reported by human subjects (in an experiment that I help analyzing). The <code>glmer()</code> yields a model that is clearly off, and very far from the one I get with <code>glm()</code>, so something goes wrong. Why? What can I do? </p>

<hr>

<p><strong>More details</strong></p>

<p>Apparently it is possible to use logistic regression not only for binary DV but also for continuous DV between zero and one. Indeed, when I run </p>

<pre><code>glm(reportedProbability ~ a + b + c, myData, family=""binomial"")
</code></pre>

<p>I get a warning message</p>

<pre class=""lang-none prettyprint-override""><code>Warning message:
In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>but a very reasonable fit (all factors are categorical, so I can easily check whether model predictions are close to the across-subjects-means, and they are). </p>

<p>However, what I actually want to use is</p>

<pre><code>glmer(reportedProbability ~ a + b + c + (1 | subject), myData, family=""binomial"")
</code></pre>

<p>It gives me the identical warning, returns a model, but this model is clearly very much off; the estimates of the fixed effects are very far from the <code>glm()</code> ones and from the across-subject-means. (And I need to include <code>glmerControl(optimizer=""bobyqa"")</code> into the <code>glmer</code> call, otherwise it does not converge at all.)</p>
"
