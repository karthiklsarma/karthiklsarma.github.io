"V1","V2","V3","V4"
"0.0835269069584557","0.0864944897557338","   612","<p>I have tried to reproduce some research (using PCA) from SPSS in R. In my experience, <code>principal()</code> <a href=""http://www.personality-project.org/r/html/principal.html"">function</a> from package <code>psych</code> was the only function that came close (or if my memory serves me right, dead on) to match the output. To match the same results as in SPSS, I had to use parameter <code>principal(..., rotate = ""varimax"")</code>. I have seen papers talk about how they did PCA, but based on the output of SPSS and use of rotation, it sounds more like Factor analysis.</p>

<p>Question: Is PCA, even after rotation (using <a href=""https://en.wikipedia.org/wiki/Varimax_rotation""><code>varimax</code></a>), still PCA? I was under the impression that this might be in fact Factor analysis... In case it's not, what details am I missing?</p>
"
"0.0964485644340824","0.0998752338877845","  5170","<p>I am new to forecasting in R and am trying to automatically fit an ARIMA model to what I believe is a univariate dataset.</p>

<pre><code>&gt; str(p1.z)
'zoo' series from 2009-04-05 to 2010-10-31
  Data: int [1:83] 360 570 540 585 570 690 495 660 510 690 ...
  Index: Class 'Date'  num [1:83] 14339 14346 14353 14360 14367 ...

&gt;  head(p1.z) 
  2009-04-05 2009-04-12 2009-04-19 2009-04-26 2009-05-03 2009-05-10 
         360        570        540        585        570        690
</code></pre>

<p>But when I try to fit the model, I get the error as seen below.</p>

<pre><code>&gt; p1.arima &lt;- auto.arima(p1.z)
Error in nsdiffs(xx) : Non seasonal data
</code></pre>

<p>It is my understanding that the forecast package and the auto.arima function would be able to fit my data seasonal or not.  I am trying to learn time series forecasting and am using a dataset that appears to be ideal for this sort of task .  Also, the function ets() was able to find a model.</p>

<p>Any help you can provide will be greatly appreciated</p>
"
"0.127589457900886","0.0943732253576744","  6329","<p>I've been using the ets() and auto.arima() functions from the <a href=""http://robjhyndman.com/software/forecast/"">forecast package</a> to forecast a large number of univariate time series.  I've been using the following function to choose between the 2 methods, but I was wondering if CrossValidated had any better (or less naive) ideas for automatic forecasting.</p>

<pre><code>auto.ts &lt;- function(x,ic=""aic"") {
    XP=ets(x, ic=ic) 
    AR=auto.arima(x, ic=ic)

    if (get(ic,AR)&lt;get(ic,XP)) {
        model&lt;-AR
    }
    else {
        model&lt;-XP
    }
        model
}
</code></pre>

<p>/edit: What about this function?</p>

<pre><code>auto.ts &lt;- function(x,ic=""aic"",holdout=0) {
    S&lt;-start(x)[1]+(start(x)[2]-1)/frequency(x) #Convert YM vector to decimal year
    E&lt;-end(x)[1]+(end(x)[2]-1)/frequency(x)
    holdout&lt;-holdout/frequency(x) #Convert holdout in months to decimal year
    fitperiod&lt;-window(x,S,E-holdout) #Determine fit window

    if (holdout==0) {
        testperiod&lt;-fitperiod
    }
    else {
        testperiod&lt;-window(x,E-holdout+1/frequency(x),E) #Determine test window
    }

    XP=ets(fitperiod, ic=ic)
    AR=auto.arima(fitperiod, ic=ic)

    if (holdout==0) {
        AR_acc&lt;-accuracy(AR)
        XP_acc&lt;-accuracy(XP)
    }
    else {
        AR_acc&lt;-accuracy(forecast(AR,holdout*frequency(x)),testperiod)
        XP_acc&lt;-accuracy(forecast(XP,holdout*frequency(x)),testperiod)
    }

    if (AR_acc[3]&lt;XP_acc[3]) { #Use MAE
        model&lt;-AR
    }
    else {
        model&lt;-XP
    }
    model
}
</code></pre>

<p>The ""holdout"" is the number of periods you wish to use as an out of sample test.  The function then calculates a fit window and a test window based on this parameter.  Then it runs the auto.arima and ets functions on the fit window, and chooses the one with the lowest MAE in the test window.  If the holdout is equal to 0, it tests the in-sample fit.</p>

<p>Is there a way to automatically update the chosen model with the complete dataset, once it has been selected?</p>
"
"0.0835269069584557","0.0864944897557338","  6330","<p>I have previously used <a href=""http://www.forecastpro.com/"">forecast pro</a> to forecast univariate time series, but am switching my workflow over to R.  The forecast package for R contains a lot of useful functions, but one thing it doesn't do is any kind of data transformation before running auto.arima().  In some cases forecast pro decides to log transform data before doing forecasts, but I haven't yet figured out why.</p>

<p>So my question is: when should I log-transform my time series before trying ARIMA methods on it?</p>

<p>/edit: after reading your answers, I'm going to use something like this, where x is my time series:</p>

<pre><code>library(lmtest)
if ((gqtest(x~1)$p.value &lt; 0.10) {
    x&lt;-log(x)
}
</code></pre>

<p>Does this make sense?</p>
"
"0.118124884643724","0.122321680483099","  6544","<p>I have a dataset that I want to fit a simple linear model to, but I want to include the lag of the dependent variable as one of the regressors. Then I want to predict future values of this time series using forecasts I already have for the independent variables. The catch is: how do I incorporate the lag into my forecast?</p>

<p>Here's an example:</p>

<pre><code>#A function to calculate lags
lagmatrix &lt;- function(x,max.lag){embed(c(rep(NA,max.lag),x),max.lag)}
lag &lt;- function(x,lag) {
 out&lt;-lagmatrix(x,lag+1)[,lag]
 return(out[1:length(out)-1])
}

y&lt;-arima.sim(model=list(ar=c(.9)),n=1000) #Create AR(1) dependant variable
A&lt;-rnorm(1000) #Create independant variables
B&lt;-rnorm(1000)
C&lt;-rnorm(1000)
Error&lt;-rnorm(1000)
y&lt;-y+.5*A+.2*B-.3*C+.1*Error #Add relationship to independant variables 

#Fit linear model
lag1&lt;-lag(y,1)
model&lt;-lm(y~A+B+C+lag1)
summary(model)

#Forecast linear model
A&lt;-rnorm(50) #Assume we know 50 future values of A, B, C
B&lt;-rnorm(50)
C&lt;-rnorm(50)
lag1&lt;-  #################This is where I'm stuck##################

newdata&lt;-as.data.frame(cbind(A,B,C,lag1))
predict.lm(model,newdata=newdata)
</code></pre>
"
"0","0.0499376169438922","  6967","<p>/edit: To clarify: The mtable function from the <a href=""http://cran.r-project.org/web/packages/memisc/index.html"" rel=""nofollow"">memisc</a> package does exactly what I need, but unfortunately does not work with arima models.</p>

<p>Similar to <a href=""http://stats.stackexchange.com/questions/6856/aggregating-results-from-linear-model-runs-r"">this question</a>: I have multiple Arima models, some of which I've also fit with dependent variables. I'd like an easy way to make a table/graph of the coefficients in each model, as well as summary statistics about each model.</p>

<p>Here is some example code:</p>

<pre><code>sim &lt;- arima.sim(list(order = c(1,1,0), ar = 0.7), n = 200)

ar1&lt;-arima(sim,order=c(1,1,0))
ar2&lt;-arima(sim,order=c(2,1,0))
ar3&lt;-arima(sim,order=c(3,1,0))
ar4&lt;-arima(sim,order=c(2,2,1))

#Try mtable
library(memisc)
mtable(""Model 1""=ar1,""Model 2""=ar2,""Model 3""=ar3,""Model 4""=ar4)
#&gt;Error in UseMethod(""getSummary"") : 
#  no applicable method for 'getSummary' applied to an object of class ""Arima""

#Try  apsrtable
library(apsrtable)
apsrtable(""Model 1""=ar1,""Model 2""=ar2,""Model 3""=ar3,""Model 4""=ar4)
#&gt;Error in est/x$se : non-numeric argument to binary operator
</code></pre>
"
"0.0482242822170412","0.0499376169438922","  8868","<p>When doing time series  research in R, I found that <code>arima</code>  provides only the coefficient values and their standard errors of fitted model. However, I also want to get the p-value of the coefficients.</p>

<p>I did not find any function that provides the significance of coef.</p>

<p>So I wish to calculate it by myself, but I don't know the degree of freedom in the t or chisq distribution of the coefficients. So my question is how to get the p-values for the coefficients of fitted arima model in R?</p>
"
"0.0482242822170412","0.0499376169438922","  9512","<p>I have a time series data of 30 years and found that ARIMA(0,1,1) has best model among others. I have used the simulate.Arima (forecast package) function to simulate the series into the future.</p>

<pre><code>library(forecast)

series &lt;- ts(seq(25,55), start=c(1976,1))

arima_s &lt;- Arima(series, c(0,1,1))

simulate(arima_s, nsim=50, future=TRUE)
</code></pre>

<p>Later on, i have found the updated value of first forecasted year (i.e. series[31] &lt;- 65). Now i want to simulate the series with this updated value. I am wondering how to do this in R.</p>
"
"0.118124884643724","0.122321680483099"," 10425","<p>I use the <a href=""http://www.oga-lab.net/RGM2/func.php?rd_id=forecast%3aauto.arima"">auto.arima()</a> function in the <a href=""http://cran.r-project.org/web/packages/forecast/index.html"">forecast</a> package to fit ARMAX models with a variety of covariates. However, I often have a large number of variables to select from and usually end up with a final model that works with a subset of them.  I don't like ad-hoc techniques for variable selection because I am human and subject to bias, but <a href=""http://stats.stackexchange.com/questions/8807/cross-validating-time-series-analysis"">cross-validating time series is hard</a>, so I haven't found a good way to automatically try different subsets of my available variables, and am stuck tuning my models using my own best judgement.</p>

<p>When I fit glm models, I can use the elastic net or the lasso for regularization and variable selection, via the <a href=""http://cran.r-project.org/web/packages/glmnet/index.html"">glmnet</a> package. Is there a existing toolkit in R for using the elastic net on ARMAX models, or am I going to have to roll my own? Is this even a good idea?</p>

<p>edit: Would it make sense to manually calculate the AR and MA terms (say up to AR5 and MA5) and the use glmnet to fit the model?</p>

<p>edit 2: It seems that the <a href=""http://cran.r-project.org/web/packages/FitAR/index.html"">FitAR</a> package gets me part, but not all, of the way there.</p>
"
"0.0835269069584557","0.0864944897557338"," 10697","<p>I'm studying R package dlm. So far it seems very powerful and flexible package, with nice programming interfaces and good documentation.</p>

<p>I've been able to successfully use dlmMLE and dlmModARMA to estimate the parameters of AR(1) process:</p>

<pre><code>u &lt;- arima.sim(list(ar = 0.3), 100)
fit &lt;- dlmMLE(u, parm = c(0.5, sd(u)),
              build = function(x)
                dlmModARMA(ar = x[1], sigma2 = x[2]^2))
fit$par
</code></pre>

<p>Now I'm trying to use similar code to estimate the parameters of simple linear regression model:</p>

<pre><code>r &lt;- rnorm(100)
u &lt;- -1*r + 0.5*rnorm(100)
fit &lt;- dlmMLE(u, parm = c(0, 1),
              build = function(x)
                dlmModReg(x[1]*r, FALSE, dV = x[2]^2))
fit$par
</code></pre>

<p>I expect fit$par to be close to c(-1, 0.5), but I keep getting something like</p>

<pre><code>[1] -0.0002118851  0.4884367070
</code></pre>

<p>The coefficient -1 is not estimated correctly. However, the strange thing is that the variance of the noise is returned correctly.</p>

<p>I understand that max-likelihood estimation might fail given bad initial values, but I observed that the likelihood function returned by dlmLL is very flat in the first coordinate.</p>

<p>So I wonder: can such model be estimated at all using dlm? I believe the model is ""non-singular"", however I'm not sure how the likelihood function is calculated inside the dlm.</p>

<p>Any hint greatly appreciated.</p>
"
"0.0482242822170412","0.0499376169438922"," 10750","<p>I have a time series. I want to model it using ARMA, which will be used for forcasting. </p>

<p>In R I am using <code>arima()</code> function to get the coefficients. But <code>arima()</code> requires order(p,d,q) as input. What is the simplest way in R to arrive at a good value for p and q (with d = 0) so that I don't overfit?</p>
"
"0.136398867894095","0.123589296520629"," 11935","<p>I already posted about exploratory factor analysis to understand the difference with PCA. Now, I carried out an exploratory factor analysis on my data set by using the R's <code>psych::fa</code> function. I have some perplexities about the interpretation of the results listed here below. A is the matrix of my data having 16 rows and 6 columns. </p>

<pre><code>fa(a,nfactors=3,rotate=""varimax"")
In fa, too many factors requested for this number of variables to use SMC for communality estimates, 1s are used instead

Factor Analysis using method =  minres
Call: fac(r = r, nfactors = nfactors, n.obs = n.obs, rotate = rotate, 
    scores = scores, residuals = residuals, SMC = SMC, missing = FALSE, 
    impute = impute, min.err = min.err, max.iter = max.iter, 
    symmetric = symmetric, warnings = warnings, fm = fm, alpha = alpha)
Standardized loadings based upon correlation matrix
     MR1   MR3   MR2   h2    u2
V1 -0.02  0.38  0.06 0.15 0.848
V2  0.14  0.50  0.14 0.29 0.711
V3  0.97  0.06  0.24 1.00 0.005
V4 -0.03 -0.05 -0.47 0.22 0.779
V5  0.67  0.74  0.03 1.00 0.005
V6  0.46  0.39  0.79 1.00 0.005

                MR1  MR3  MR2
SS loadings    1.63 1.10 0.92
Proportion Var 0.27 0.18 0.15
Cumulative Var 0.27 0.45 0.61

Test of the hypothesis that 3 factors are sufficient.

The degrees of freedom for the null model are  15  and the objective function was  2.15 with Chi Square of  26.17
The degrees of freedom for the model are 0  and the objective function was  0.07 

The root mean square of the residuals is  0.03 
The number of observations was  16  with Chi Square =  0.67  with prob &lt;  NA 

Tucker Lewis Index of factoring reliability =  -Inf
Fit based upon off diagonal values = 0.98
Measures of factor score adequacy             
                                                MR1  MR3  MR2
Correlation of scores with factors             1.00 0.99 0.99
Multiple R square of scores with factors       0.99 0.99 0.99
Minimum correlation of possible factor scores  0.99 0.97 0.98
</code></pre>

<p>I cannot understand from chi-square value if I can not reject the null hypothesis of goodness of fit on three factors. I have seen some examples on the web for this function, but I could not find anything similar.</p>

<p>thanks,</p>
"
"0.161868929654819","0.141832209766852"," 12873","<p>I am trying to tackle a problem which deals with the imputation of missing data from a panel data study(Not sure if I am using 'panel data study' correctly - as I learned it today.) I have total death count data for years 2003 to 2009, all the months, male &amp; female, for 8 different districts and for 4 age groups.</p>

<p>The dataframe looks something like this:</p>

<pre><code>         District  Gender Year Month    AgeGroup TotalDeaths
         Northern    Male 2006    11        01-4           0
         Northern    Male 2006    11       05-14           1
         Northern    Male 2006    11         15+          83
         Northern    Male 2006    12           0           3
         Northern    Male 2006    12        01-4           0
         Northern    Male 2006    12       05-14           0
         Northern    Male 2006    12         15+         106
         Southern  Female 2003     1           0           6
         Southern  Female 2003     1        01-4           0
         Southern  Female 2003     1       05-14           3
         Southern  Female 2003     1         15+         136
         Southern  Female 2003     2           0           6
         Southern  Female 2003     2        01-4           0
         Southern  Female 2003     2       05-14           1
         Southern  Female 2003     2         15+         111
         Southern  Female 2003     3           0           2
         Southern  Female 2003     3        01-4           0
         Southern  Female 2003     3       05-14           1
         Southern  Female 2003     3         15+         141
         Southern  Female 2003     4           0           4
</code></pre>

<p>For the 10 months spread over 2007 and 2008 some of the total deaths from all districts were not recorded. I am trying to estimate these missing value through a multiple imputation method. Either using Generalized Linear Models or SARIMA models.</p>

<p>My biggest issue is the use of software and the coding. I asked a question on Stackoverflow, where I want to extract the data into smaller groups such as this:</p>

<pre><code>         District  Gender Year Month    AgeGroup TotalDeaths
         Northern    Male 2003     1        01-4           0
         Northern    Male 2003     2        01-4           1
         Northern    Male 2003     3        01-4           0
         Northern    Male 2003     4        01-4           3
         Northern    Male 2003     5        01-4           4
         Northern    Male 2003     6        01-4           6
         Northern    Male 2003     7        01-4           5
         Northern    Male 2003     8        01-4           0
         Northern    Male 2003     9        01-4           1
         Northern    Male 2003    10        01-4           2
         Northern    Male 2003    11        01-4           0
         Northern    Male 2003    12        01-4           1
         Northern    Male 2004     1        01-4           1
         Northern    Male 2004     2        01-4           0
</code></pre>

<p>Going to</p>

<pre><code>         Northern    Male 2006    11        01-4           0
         Northern    Male 2006    12        01-4           0
</code></pre>

<p>But someone suggested I should rather bring my question here - perhaps ask for a direction? Currently I am unable to enter this data as a proper time-series/panel study into R. My eventual aim is to use this data and the <code>amelia2</code> package with its functions to impute for missing <code>TotalDeaths</code> for certain months in 2007 and 2008, where the data is missing.</p>

<p>Any help, how to do this and perhaps suggestions on how to tackle this problem would be gratefully appreciated.</p>

<p>If this helps, I am trying to follow a similar approach to what Clint Roberts did in his PhD <a href=""http://etd.ohiolink.edu/send-pdf.cgi/Roberts%20Clint.pdf?osu1211910310"">Thesis</a>. </p>

<p><strong>EDIT:</strong></p>

<p>After creating the 'time' and 'group' variable as suggested by @Matt:</p>

<pre><code>&gt; head(dat)
     District Gender Year Month AgeGroup Unnatural Natural Total time                    group
1 Khayelitsha Female 2001     1        0         0       6     6    1     Khayelitsha.Female.0
2 Khayelitsha Female 2001     1     01-4         1       3     4    1  Khayelitsha.Female.01-4
3 Khayelitsha Female 2001     1    05-14         0       0     0    1 Khayelitsha.Female.05-14
4 Khayelitsha Female 2001     1     15up         8      73    81    1  Khayelitsha.Female.15up
5 Khayelitsha Female 2001     2        0         2       9    11    2     Khayelitsha.Female.0
6 Khayelitsha Female 2001     2     01-4         0       2     2    2  Khayelitsha.Female.01-4
</code></pre>

<p>As you notice, there's actually further detail 'Natural' and 'Unnatural'.</p>
"
"0.144672846651124","0.133166978517046"," 13950","<p>As with my previous question, I'm looking at ways to impute missing data in a hierarchical time series data.</p>

<p>With al my other procedures, including the experimentation of imputation packages (<code>Amelia</code>, <code>HoltWinters</code> from <code>Forecast</code> and <code>MICE</code> imputation) I've only been able to use the time series data prior to the missing gap.</p>

<pre><code>     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
2001 220 194 238 190 217 244 242 225 242 259 267 244
2002 212 246 250 236 261 286 265 269 226 267 234 246
2003 202 199 297 272 236 266 235 226 260 183 226 265
2004 211 215 219 213 240 236 273 266 262 244 241 235
2005 212 198 233 251 259 282 305 267 241 264 222 269
2006 182 220 250 287 279 281 286 332 300 272 221 233
2007  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA  NA
2008 193 215 235 242 246 315 326 280 279 239 236 258
2009 246 189 257 241 268 223 260 288 234 260 216 195
</code></pre>

<p>I'm trying to do simple imputation procedure that uses forecasting and backcasting estimates from the time series model. Forecasting using prior data to predict the future and backcasting  using the later data to â€œpredictâ€ the past.</p>

<p>I would then like to combine the forecast and backcast value to use as imputation. After which I will look at the fit etc.</p>

<p>How do I go about this in coding? </p>

<p>For example, I'm able to determine what SARIMA model exist for the first period 2001-end2006. But not the full period (because my basic functions I know from R does not support the NA values.)</p>

<p>This is only for the period 2001-end2006:</p>

<pre><code>ARIMA(2,0,2)(1,0,1)[12] with non-zero mean 

Call: auto.arima(x = ts.datt) 

Coefficients:
         ar1      ar2      ma1     ma2    sar1     sma1  intercept
      1.3610  -0.8258  -1.2407  0.9191  0.8982  -0.7560   244.8374
s.e.  0.0884   0.0960   0.0878  0.1127  0.2190   0.3335     6.1894

sigma^2 estimated as 605.9:  log likelihood = -335.01
AIC = 686.02   AICc = 688.3   BIC = 704.23
</code></pre>

<p>Should I just model the first period, predict by <code>forecast</code>; model then the last period separately and then backcast? How will I do this backcasting (ie. 'predicting' the past)?</p>

<p><strong>EDIT:</strong>
What I'm asking:
1) How do I use the data from years 2008 &amp; 2009 to BACKCAST? I already know how to use 2001-2006 to forecast. </p>

<p>2) How do I determine the SARIMA model for the whole period? (2001-2009) ie. </p>
"
"0.199141363713665","0.217673025739671"," 14742","<p>I am fitting an ARIMA model on a daily time series.
Data are collected daily from 02-01-2010 to 30-07-2011 and are about newspaper sales.
Since a weekly pattern in sales can be found (the daily average amount of copies sold is usually the same from Monday to Friday, then increases on Saturday and Sunday), I am trying to capture this ""seasonality"".
Given the sales data ""data"", I create the time series as follows:</p>

<pre><code>salests&lt;-ts(data,start=c(2010,1),frequency=365)
</code></pre>

<p>and then I use the auto.arima(.) function to select the best ARIMA model via AIC criterion. The result is always a non-seasonal ARIMA model, but if I try some SARIMAs model with the following syntax as example:</p>

<pre><code>sarima1&lt;-arima(salests, order = c(2,1,2), seasonal = list(order = c(1, 0, 1), period = 7))
</code></pre>

<p>I can obtain better results.
Is there anything wrongs in the ts command / arima specification? The weekly pattern is very strong so I would not expect so many difficulties in capturing it. 
Any help would be very useful.
Thank you,
Giulia Deppieri</p>

<p>Update:</p>

<p>I have already changed some arguments. More precisely, the procedure selects ARIMA(4,1,3) as the best model when I set <code>D=7</code>, but AIC and the others good of fit indexes and forecasts as well) do not improve at all. I guess there's some mistakes due to confusion between seasonality and periodicity..?! </p>

<p>Auto.arima call used and output obtained:</p>

<pre><code>modArima&lt;-auto.arima(salests,D=7,max.P = 5, max.Q = 5)



 ARIMA(2,1,2) with drift         : 1e+20
 ARIMA(0,1,0) with drift         : 5265.543
 ARIMA(1,1,0) with drift         : 5182.772
 ARIMA(0,1,1) with drift         : 1e+20
 ARIMA(2,1,0) with drift         : 5137.279
 ARIMA(2,1,1) with drift         : 1e+20
 ARIMA(3,1,1) with drift         : 1e+20
 ARIMA(2,1,0)                    : 5135.382
 ARIMA(1,1,0)                    : 5180.817
 ARIMA(3,1,0)                    : 5117.714
 ARIMA(3,1,1)                    : 1e+20
 ARIMA(4,1,1)                    : 5045.236
 ARIMA(4,1,1) with drift         : 5040.53
 ARIMA(5,1,1) with drift         : 1e+20
 ARIMA(4,1,0) with drift         : 5112.614
 ARIMA(4,1,2) with drift         : 4953.417
 ARIMA(5,1,3) with drift         : 1e+20
 ARIMA(4,1,2)                    : 4960.516
 ARIMA(3,1,2) with drift         : 1e+20
 ARIMA(5,1,2) with drift         : 1e+20
 ARIMA(4,1,3) with drift         : 4868.669
 ARIMA(5,1,4) with drift         : 1e+20
 ARIMA(4,1,3)                    : 4870.92
 ARIMA(3,1,3) with drift         : 1e+20
 ARIMA(4,1,4) with drift         : 4874.095

 Best model: ARIMA(4,1,3) with drift        
</code></pre>

<p>So I assume the arima function should be used as:</p>

<pre><code>bestOrder &lt;- cbind(modArima$arma[1],modArima$arma[5],modArima$arma[2])
sarima1&lt;-arima(salests, order = c(4,1,3))
</code></pre>

<p>with no seasonal component parameters and period specifications.
Data and exploratory analysis show that the same weekly pattern can be approximatively considered for each week, with the only exception of August 2010 (when a consistent increase in sales is registered). Unfortunately I have no expertise in timeseries modeling at all, in fact I am trying this approach in order to find an alternative solution to other parametric e non-parametric models I have tried to fit for these problematic data.
I have also many dependent numeric variables but they have shown low power in explaining the response variable: undoubtedly, the most difficult part to model is the time component. Moreover, the construction of dummy variables to represent months and weekdays turned out not to be a robust solution.  </p>
"
"NaN","NaN"," 18314","<p>I have 4 correlated time series, and I want to predict one of them, from the other 3. There is a clear seasonal effect in the 4 time series, so my first thought was to fit a multivariate ARIMA model, but I can not seem to find an R-function for this.</p>
"
"NaN","NaN"," 19080","<p>I found <code>auto.arima()</code> function in <code>forecast</code> packages.
As far I have understand how it works, It should find the best model for the data, my question is: Is it possible to understand if the variance is not constant during all the period(timeseries) using that function? I mean...
If it fit a model it should means that the variance is homoscedastics because i set <code>stationary=TRUE</code></p>

<p>Let me know, thank you!</p>
"
"0.0964485644340824","0.0749064254158383"," 19549","<p>I have univariate time series data (windspeed at a particular place) measured at 1 hour interval for 5 years. </p>

<p>I used <code>auto.arima()</code> to get the following parameters:</p>

<pre><code>              ar1      ar2     ma1     ma2    intercept
             1.5314  -0.55   -0.1261  0.032    10.1223
     s.e.    0.0105  0.0103   0.011   0.006     0.1211

     sigma^2 estimated as 0.4865 : log likelihood = -83546.65
     AIC = 167105.3   AICc = 167105.3    BIC = 167161    
</code></pre>

<p>I am forecasting using the following equation:</p>

<pre><code>e[t] &lt;- rnorm(1, 0, sqrt(sigma^2))
x[t] &lt;- ar1*x[t-1] + ar2*x[t-2] + e[t] + ma1*e[t-1] + ma2*e[t-2]
</code></pre>

<p>When the result is compared with <code>forecast()</code> function, I get completely different answers. The freq spectrum of <code>forecast()</code> function's output resembles original time-series freq spectrum. While the manual forecast signal looks like noise in freq spectrum.</p>

<p>I can't use <code>forecast()</code> function because the application is in C++. Are the equations correct? What's the right way of forecasting from coefficients?    </p>
"
"0.119349009407333","0.14124491030929"," 20254","<p>With the arima function I found some nice results, however now I have trouble interpreting them for use outside R.
I am currently struggling with the MA terms, here is a short example:</p>

<pre><code>ser=c(1, 14, 3, 9)        #Example series
mod=arima(ser,c(0,0,1))   #From {stats} library
mod

#Series: ser
#ARIMA(0,0,1) with non-zero mean
#
#Coefficients:
#          ma1  intercept
#      -0.9999     7.1000
#s.e.   0.5982     0.8762
#
#sigma^2 estimated as 7.676:  log likelihood = -10.56
#AIC = 27.11   AICc = Inf   BIC = 25.27

mod$resid

#Time Series:
#Start = 1
#End = 4
#Frequency = 1
#[1] -4.3136670  3.1436951 -1.3280435  0.6708065

predict(mod,n.ahead=5)

#$pred
#Time Series:
#Start = 5
#End = 9
#Frequency = 1
#[1] 6.500081 7.100027 7.100027 7.100027 7.100027
#
#$se
#Time Series:
#Start = 5
#End = 9
#Frequency = 1
#[1] 3.034798 3.917908 3.917908 3.917908 3.917908
?arima
</code></pre>

<p>When looking at the specification this formula is presented:
<code>X[t] = a[1]X[t-1] + â€¦ + a[p]X[t-p] + e[t] + b[1]e[t-1] + â€¦ + b[q]e[t-q]</code></p>

<p>Given my choice of AR and MA terms, and considering that I have included a constant this should reduce to:
<code>X[t] =  e[t] + b[1]e[t-1] + constant</code></p>

<p>However this does not hold up when i compare the results from R with manual calculations:
<code>6.500081 != 6.429261 == -0.9999 * 0.6708065 + 7.1000</code></p>

<p>Furthermore I can also not succeed in reproducing the insample errors, assuming i know the first one this should be possible:
<code>-4.3136670 * -0.9999 +7.1000 !=  14 - 3.1436951</code>
<code>3.1436951 * -0.9999 +7.1000 !=   3 + 1.3280435</code>
<code>-1.3280435 * -0.9999 +7.1000 !=  9 - 0.6708065</code></p>

<p>I hope someone can shed some light on this matter so I will actually be able to use the nice results that I have obtained.</p>
"
"0.0964485644340824","0.0749064254158383"," 20929","<p>I want to make forecast on my data by running an arimax model.
The data is like:</p>

<pre><code>Value1, Flag1, Flag2, ................., FlagN    
Value2, Flag1, Flag2, ................., FlagN    
Value3, Flag1, Flag2, ................., FlagN    
Value4, Flag1, Flag2, ................., FlagN    
Value5, Flag1, Flag2, ................., FlagN    
....    
ValueM, Flag1, Flag2, ................., FlagN
</code></pre>

<p>So, when I want to make a new forecast, I will provide flag values to the model, then it can give me the forecast value?</p>

<p>How can I prepare the input data?
What is the proper R function and proper way of calling?</p>
"
"0.0681994339470473","0.0706224551546449"," 21332","<p>I would like to use a set of weather-related historical data to fit a time series (let's say 1970-2000, Fourier terms plus ARIMA terms), but then use the fit on recent data (i.e., the last week/month of data) to forecast the upcoming day/week/month. All of the functions that I've found forecast from the endpoint of the dataset used to fit.</p>

<p>Can someone point me in the right direction? Or let me know it doesn't exist and I have to write it out the long way (i.e. <code>Tomorrow = fit$coef[1]*Yesterday + ...</code>)?</p>
"
"0.0723364233255618","0.0998752338877845"," 25780","<p>I am really new with R and time series. But, I have understood most concept. Part where I am (very) confused is the <code>xreg=</code> argument in <code>arima()</code> from <a href=""http://cran.r-project.org/web/packages/tseries/index.html"" rel=""nofollow"">tseries</a> package and <a href=""http://cran.r-project.org/web/packages/forecast/index.html"" rel=""nofollow"">forecast</a> package. As I have read most of the related threads on this site, I understand that <code>xreg</code> is used for exogenous data. See for example, <a href=""http://stats.stackexchange.com/questions/18375/how-to-fit-an-arimax-model-with-r"">How to fit an ARIMAX-model with R?</a>.</p>

<p>Thus, I conclude that I can fit SARIMAX or ARMAX method using <code>arima()</code>. However, I am currently reading Shumwhay's book and his web <a href=""http://www.stat.pitt.edu/stoffer/tsa2/R_time_series_quick_fix.htm"" rel=""nofollow"">tutorial</a>.</p>

<p>If you check the website (close to the bottom of the page), you can found that he said </p>

<blockquote>
  <p><code>xreg</code> in <code>arima()</code> does not fix ARMAX model.</p>
</blockquote>

<p>It is also explained in the <a href=""http://www.stat.pitt.edu/stoffer/tsa2/Rissues.htm"" rel=""nofollow"">R issues</a> on his website. In addition, he proposed fitting ARMAX model in state space model. So, the answers found on this site and the explanation in the website are completely different.</p>

<p>Can anyone explain me why there is such discrepancy? Which one is correct?</p>

<p>If Shumwhay is correct, is there any function/package that can fit ARMAX or SARIMAX model in R?</p>

<p>Sorry if it turns out I only missed some points, but I hope you can point me to the right direction.</p>
"
"0.0681994339470473","0.0706224551546449"," 26183","<p>I would like to convert an ARIMA model developed in R using the <code>forecast</code> library to Java code. Note that I need to implement only the forecasting part. The fitting can be done in R itself. I am going to look at the <code>predict</code> function and translate it to Java code. I was just wondering if anyone else had been in a similar situation before and managed to successfully use a Java library for the same. </p>

<p>Along similar lines, and perhaps this is a more general question without a concrete answer; What is the best way to deal with situations where in model building can be done in Matlab/R but the prediction/forecasting needs to be done in Java/C++? Increasingly, I have been encountering such a situation over and over again. I guess you have to bite the bullet and write the code yourself and this is not generally as hard as writing the fitting/estimation yourself. Any advice on the topic would be helpful. </p>
"
"0.0482242822170412","0.0499376169438922"," 26668","<p>I wanted to ask whether it was possible to use the auto.arima function to identify subset ARIMA models rather than those of pure lags? I have identified a model in Stata in subset lags that performs well and wanted to cross check this with the <code>auto.arima()</code> function but I can't seem to figure out if subset lags are supported.</p>
"
"0.0681994339470473","0.0706224551546449"," 28286","<p>I need to fit a GLS model, with some known regressors, and where the errors follow an <strong>unknown</strong> ${\rm ARIMA}(1,0,1) \times (1,N,1)$ model. It seems like the main tool out there for such models is the <code>gls</code> function in the <code>nlme</code> package for <code>R</code>. </p>

<p>In <code>gls</code>, one specifies the correct correlation struction using a <code>corStruct</code> object, but I cannot find any <code>corStruct</code> objects for specifying my (really simple) seasonal model. I am new to R, so I don't think I am up for coding a new <code>corStruct</code> for my purposes. Are there any other packages out there for solving this problem? If not, can you point me to some references about how to create custom <code>corStruct</code> objects. Thanks for all your help!</p>
"
"0.0340997169735237","0.0706224551546449"," 28472","<p>A Regression with ARIMA errors is given by the following formula (saw on Hyndman et al, 1998):</p>

<p>$Y_t = b_0 + b_1 X_{1,t} + \dots + b_k X_{k,t} + N_t$</p>

<p>where $N_t$ is modeled as an ARIMA process.</p>

<p>If we have that the model for $N_t$ is ARIMA$(0,0,0)$, then $N_t = e_t$, and $Y_t$ is modeled by an ordinary regression.</p>

<p>Suppose the following data:</p>

<pre><code>a &lt;- structure(c(29305, 9900, 9802, 17743, 49300, 17700, 24100, 11000, 
10625, 23644, 38011, 16404, 14900, 16300, 18700, 11814, 13934, 
12124, 18097, 30026, 3600, 15700, 12300, 14600), .Tsp = c(2010.25, 
2012.16666666667, 12), class = ""ts"")
b &lt;- structure(c(1.108528016, 1.136920872, 1.100239002, 1.057191265, 
1.044200511, 1.102063834, 1.083847756, 1.068585841, 1.084879628, 
1.232979511, 1.168894672, 1.257302058, 1.264967051, 1.234793782, 
1.306452369, 1.252644047, 1.178593218, 1.124432965, 1.132878661, 
1.189926986, 1.17249669, 1.176285957, 1.176552, 1.179178082), .Tsp = 
c(2010.25, 2012.16666666667, 12), class = ""ts"")
</code></pre>

<p>If I model it using <code>auto.arima</code> function, I have:</p>

<pre><code>auto.arima(a, xreg=b)
Series: a 
ARIMA(0,0,0) with zero mean     

Coefficients:
              b
      15639.266
s.e.   1773.186

sigma^2 estimated as 101878176:  log likelihood=-255.33
AIC=514.65   AICc=515.22   BIC=517.01

lm(a~b)

Call:
lm(formula = a ~ b)

Coefficients:
(Intercept)            b  
      48638       -26143  
</code></pre>

<p>Coefficients from the models differ. Shouldn't they be the same? What am I missing?</p>
"
"0.127589457900886","0.113247870429209"," 28566","<p>De novo simulation of data from an experimental design data frame.<br>
With a focus on R (though other language solution would be great).</p>

<p>In designing an experiment or a survey, simulating data and conducting an analysis on this simulated data can provide terrific insight into advantages and weaknesses of the design.</p>

<p>Such an approach can also be essential to the understanding and proper use of statistical tests.</p>

<p>However, this process tends to be somewhat tedious and many are led to skip past this important step in an experiment or survey.</p>

<p>Statistical models and test contain most of the information required to simulate the data (including an assumption or an explicit statement of the distribution).</p>

<p>Given an an analysis model (and its associated assumptions eg. normality and balance), the levels of a factor and a measure of significance (such as p-value), I would like to obtain simulated data (ideally with a generalized function akin to print(), predict(), simulate()).</p>

<p>Is such a generalized simulation framework possible?</p>

<p>If so, is such a framework currently available?</p>

<p>Example, I would like a function, such as:</p>

<pre><code> sim(aov(response~factor1+factor2*factor3),
          p.values=list(factor1=0.05,
                        factor2=0.05,
                        factor3=0.50,
                        factor2:factor3=0.05),
          levels=list(factor1=1:10,
                      factor2=c(""A"", ""B"", ""C""),
                      factor3=c(""A"", ""B"", ""C"")))
</code></pre>

<p>ie, a generalized version of:  </p>

<pre><code>sim.lm&lt;-function(){
library(DoE.base)
design&lt;-fac.design(nlevels=c(10,3,3),
                   factor.names=c(""factor1"", ""factor2"", ""factor3""),
                   replications=3,
                   randomize=F)

response&lt;-with(design, as.numeric(factor1)+
                      as.numeric(factor2)+
                      as.numeric(factor3)+
                      as.numeric(factor2)*as.numeric(factor3)+
                      rnorm(length(factor1)))

simulation&lt;-data.frame(design, response)}
</code></pre>

<p>OR</p>

<pre><code>sim(glm(response~factor1+factor2*factor3, family=poisson),
         p.values=list(factor1=0.05,
                       factor2=0.05,
                       factor3=0.50,
                       factor2:factor3=0.05),
         levels=list(factor1=1:10,
                     factor2=c(""A"", ""B"", ""C""),
                     factor3=c(""A"", ""B"", ""C"")))
</code></pre>

<p>OR</p>

<pre><code>  library(lme4)
  sim(lmer(response~factor1+factor2 + (factor2|factor3)),
           F_value=list(factor1=50,
                        factor2=50),
           levels=list(factor1=1:10,
                       factor2=c(""A"", ""B"", ""C""),
                       factor3=c(""A"", ""B"", ""C"")))
</code></pre>

<p>that would create a complete corresponding data.frame</p>

<p>potential examples of specific functions (please edit at will)<br>
 - arima.sim </p>

<p>function exist to create a data.frame of the factor levels, without the modelled response:<br>
eg. conf.design<br>
<a href=""http://cran.r-project.org/web/views/ExperimentalDesign.html"">http://cran.r-project.org/web/views/ExperimentalDesign.html</a></p>
"
"0.127589457900886","0.132122515500744"," 28737","<p>I have time series as </p>

<pre><code>0.4385487 0.7024281 0.9381081 0.8235792 0.7779642 1.1670665 1.1958634 1.1958634 0.8235792 0.8530141 0.8802216 1.1958634 1.1235897 1.3542734 1.3245534 0.9381081 1.1670665 1.1958634 0.8802216 1.3542734 1.1670665 4.9167998 0.9651803 0.8221709 1.1070461 1.2006974 1.3542734 0.9651803 0.9381081 0.9651803 0.8854192 1.3245534 1.1235897 1.2006974 1.1958634 0.4385487 1.3245534 4.9167998 1.2277843 0.8530141 1.0018480 0.3588158 0.8530141 0.8867365 1.3542734 1.1958634 1.1958634 0.9651803 0.8802216 0.8235792 4.9167998 1.1958634 0.9651803 0.8854192 0.8854192 1.2006974 0.8867365 0.9381081 0.8235792 0.9651803 0.4385487 0.9936722 0.8821301 1.3542734 1.1235897 1.6132899 1.3245534 1.3542734 0.8132233 0.8530141 1.1958634 1.2279813 0.8354292 1.3578511 1.1070461 0.8530141 0.9670581 1.1958634 0.7779642 1.2006974 1.1958634 0.8235792 1.3245534 0.5119648 2.3386331 0.8890464 0.8867365 4.9167998 1.2006974 1.2006974 0.6715839 4.9167998 0.7747481 4.9167998 0.8867365 1.2277843 0.8890464 1.2277843 0.8890464 1.0541099 0.8821301 
</code></pre>

<p>I am using package ""itsmr""-autofit(),""forecast""-auto.arima(),""package""--functions</p>

<ol>
<li><p>Autoregressive model</p>

<pre><code>&gt; ar(t)

Call:
    ar(x = t)

    Order selected 0  sigma^2 estimated as  0.9222 
</code></pre></li>
<li><p>ARMA model</p>

<pre><code>&gt; autofit(t)
    $phi
    [1] 0

    $theta
    [1] 0

    $sigma2
    [1] 0.9130698

    $aicc
    [1] 279.4807

    $se.phi
    [1] 0

    $se.theta
    [1] 0
</code></pre></li>
<li><p>ARIMA model</p>

<pre><code>    &gt; auto.arima(t)
    Series: t 
    ARIMA(0,0,0) with non-zero mean 

    Coefficients:
          intercept
             1.2623
    s.e.     0.0951

    sigma^2 estimated as 0.9131:  log likelihood=-138.72
    AIC=281.44   AICc=281.56   BIC=286.67
</code></pre>

<p>The auto.arima function automatically differences time series: we don't have to worry about transformation.</p>

<pre><code>&gt; auto.arima(AirPassengers)
Series: AirPassengers 
ARIMA(0,1,1)(0,1,0)[12]                    

Coefficients:
          ma1
      -0.3184
s.e.   0.0877

sigma^2 estimated as 137.3:  log likelihood=-508.32
AIC=1020.64   AICc=1020.73   BIC=1026.39`
</code></pre></li>
</ol>

<p>Which model should I select to get p,q values &amp; for forecasting purpose?</p>
"
"0.255178915801771","0.254807708465721"," 29336","<p><strong>Introduction</strong> </p>

<p>I am aiming to forecast the annual growth rates for a number of macroeconomic indicators (denote one by $Y_t$).
One of the tasks is to test the forecasting performance of rival time series models with and without exogenous variables ($X_t$, a $T\times k$ matrix). The list of rival models include:</p>

<ol>
<li>AR(I)MA model (annual growth rates are unlikely to have ""unit Roo"", though the latter is either assumed or tested) $$A(L)Y_t =\mu+ B(L)\varepsilon_t$$</li>
<li>linear regression model with ARMA errors $$Y_t = X_t\beta + \eta_t, \ \ A(L)\eta_t =  B(L)\varepsilon_t $$</li>
<li>lagged dependent variable model (autoregressive model with exogenous variables)
$$A(L)Y_t = X_t\beta + \varepsilon_t$$ </li>
<li>linear regression model
$$Y_t = X_t\beta + \varepsilon_t$$</li>
</ol>

<p>Where $\varepsilon_t$ is assumed to be a strong white noise, zero-mean constant variance i.i.d. process; $A(L)$ and $B(L)$ are autoregressive (of order $p$) and moving average (of order $q$) polynomials with $L$ - a back-shift (lag) operator.</p>

<p>Note that the primary and the only goal is forecasting performance, thus any ""good"" properties of parameter estimates are of the secondary concern. All I need is to test for the most parsimonious, robust to starting conditions forecaster. Decision will be made with one of the <code>accuracy()</code> options, but first I need to obtain the material for the comparison.</p>

<p>Models 1. and 2. are estimated by <code>auto.arima()</code> with default <code>""CSS-ML""</code> estimation method. Models 3. and 4. are estimated by ordinary least squares (<code>lm()</code>). $T$ is about $40$ quarters.</p>

<p><strong>Approaches tried so far</strong></p>

<p>To make the jack-knifed residuals the first approach denoted by ""rolling"" has been implemented. Starting from feasibly large sub-sample of time series data, parameters are estimated and an $h$ ahead forecast is done by the <code>predict()</code> function (EDIT: it is the same suggestion as in the first part Rob's answer to the second question). After that one point is added and the estimation\prediction steps are repeated. </p>

<p>A weak point of such experiments is that the number of time ticks (sample size) used to estimate the parameters is different. While I would like to test the robustness to the starting conditions, keeping the sample size for estimation fixed.</p>

<p>Bearing this in mind I tried to set the several subsequent values (EDIT: for the interval $k+p+q&lt;t_0&lt;t_1&lt;T-h+1$) in $Y_t$ being missing values (NA). In models 2.-4. this also implies dropping the corresponding subsequent rows in data matrix $X_t$. Prediction for 3. and 4. is straightforward (the same <code>predict()</code> with omitted $X_t$ data rows works well). All my concerns are about models 1. and 2.</p>

<p>With just the AR($p$) part the predictions are done sequentially $Y_{t+1|t} = \hat A(L)Y_t$. But with the presence of MA($q$) one could not (?) use the estimated parameters directly. From the Brockwell and Davis ""Introduction to Time Series and Forecasting"" Chapter 3.3 it follows that one needs an innovation algorithm to recursively estimate the $\theta_{n,j}$ from the specific system of equations that involves estimated autoregressive and moving average parameters. EDIT: these $\theta_{n,j}$ parameters are used to make the ARMA prediction, not the originally estimated parameters $\theta_{j}$. However it is remarked in the same chapter that $\theta_{n,j}$ asymptotically approaches $\theta_{j}$ if the process is invertible. It is not evident that 30-40 points is enough for the asymptotic result to be used even if is invertible. </p>

<p>Notes: I don't want to restrict $q$ to zero, since I am not doing so in true out-of-sample forecasting. EDIT: also not that it is not missing value imputation problem, but forecasting experiment, that the trajectory is not supposed to bridge two sub-samples by this imputing the missing values.</p>

<p><strong>Questions</strong></p>

<ol>
<li>Does <code>auto.arima()</code> performs correctly with the presence of missing values inside of the sample? [Already answered by Rob.]</li>
<li>(The actually crucial part of this post) How to correctly forecast (NOT impute) these missed points from the ARMA model when both $p&gt;0$ and $q&gt;0$? (I hope there are the ways already implemented in R language, but I simply is missing something.)</li>
</ol>

<p>EDIT: since the parameters for ARMA parts are estimated correctly could I legally rearrange the arima object to include the estimated parameters and the data only for the first subsample and then use a predict function? </p>

<p>EDIT2: I have tried to modify the estimated <code>mod</code> structure - the resulting forecast from <code>predict.Arima</code> is identical (double precision differences) to the forecast where I use the estimated MA and AR coefficients predicting $Y_{t+1|t}$ directly as $\hat A(L)(Y_t-X_t\hat \beta)+ X_t\hat \beta+\hat B(L)\hat \varepsilon_t $, without <code>KalmanForecast()</code>. This was expected since state space representation is supplied with the same estimated $\theta_j$, not $\theta_{n,j}$. So the only question remains is the difference between $\theta_j$ and $\theta_{n,j}$ significant to influence the point forecasts? I hope the answer is negative.</p>
"
"0.118124884643724","0.122321680483099"," 29424","<p>I'm looking for some forecasting advice when dealing with seasonal time series data that has a large number of observations.  By ""large"" I only mean a few thousand --- I'm used to such sizes in Data Mining being considered pretty small, but it seems that in time series modeling that's pretty unwieldy for many of the tools I've tried.</p>

<p>For example, here's a toy data set that records an observation once per minute, for five days:</p>

<pre><code>set.seed(123)
t &lt;- 1:(5*24*60)
x &lt;- ts(15 + 0.001*t + 10*sin(2*pi*t/(length(t)/5)) + rnorm(length(t)), freq=length(t)/5)
plot(x, type='l')
</code></pre>

<p><img src=""http://i.stack.imgur.com/xVSCN.png"" alt=""time series plot""></p>

<p>(In my real operational data set, the values are observed at irregular intervals, but I've regularized them by doing something like <code>x &lt;- approx(d$t, d$x, xout=1:(5*24*60))</code> first.  Advice on whether that's advisable, or alternative approaches, is welcome too.)</p>

<p>So the seasonality in this data set has a lag of 1,440 observations, which seems to be way outside the range that things like <code>auto.arima()</code> (in the <code>forecast</code> package) will find:</p>

<pre><code>m1 &lt;- auto.arima(x)
plot(forecast(m1))
</code></pre>

<p><img src=""http://i.stack.imgur.com/ccnGc.png"" alt=""prediction plot""></p>

<p>And I'm not quite sure how to interpret the <code>ets()</code> function here, but it doesn't seem to be able to handle this size data, and it didn't seem to pick up on the seasonality:</p>

<pre><code>&gt; m2 &lt;- ets(x, 'MAZ')
&gt; plot(forecast(m2))
Error in forecast.ets(m2) : Forecast horizon out of bounds
&gt; m2$method
[1] ""ETS(M,A,N)""
</code></pre>

<p>Where to go from here?  Any suggestions?  Thanks.</p>
"
"0.199141363713665","0.217673025739671"," 31374","<p>Motivation: I was hired as an intern a few weeks ago to figure out if my company needed to buy new machines six months in advance. Database machines take up to 4 months to install and there is a 2 month grace period.</p>

<p>I signed an NDA, so I don't think I can give any actual data.</p>

<p>The only reliable information I have now, is information on the number of logins and registrations for an education company from 2002 to 2011. I think I can get more recent information on registrations, and people are working on getting login information. We stopped logging login information in 2011 so there will be a gap of no data when I try to forecast :(</p>

<p>The information is collected daily.</p>

<p>I've created a time series forecast of the data using R. I used this tutorial
<a href=""http://a-little-book-of-r-for-time-series.readthedocs.org/en/latest/src/timeseries.html#arima-models"" rel=""nofollow"">http://a-little-book-of-r-for-time-series.readthedocs.org/en/latest/src/timeseries.html#arima-models</a> To make a holt winters exponential model with daily frequency (frequency = 365). I've removed February 29 from the data. Unfortunately the gap in login data means I will have to try a more specific ARIMA right? Will I be able to use arima if there are long gaps in the data? Also, the arima function in R doesn't allow for frequencies greater than 350, and it runs out of memory quickly, so I'd have to use a monthly model (freq = 12). I have tried using fourier but the predictions didn't look right intuitively. Since I want to know what the peak usages are though, I think I might want to be more specific. Is it ok to use a weekly frequency (freq = 52) and just remove Dec 31?</p>

<p>Is daily frequency allowable? Like can I use exponential smoothing with daily frequency even though Sept 7, 2012 might fall on a Sunday, whereas Sept 7, 2011 and 2010 and 2009 might all be weekdays. There is a daily, weekly, and yearly seasonality in demand and number of logins. Eg. 6pm, and Monday, and September are more loaded in general than 4am, and Saturday, and May. There is a yearly seasonality in number of registrations.</p>

<p>I've been having some issues with the login predictions
The problem is that variability increases too much before 6 months have even passed. At the 80% confidence interval. The projection line extends into 2012 and the orange area is the 80% confidence interval. Logging and using additive exponential smoothing gave me much more variability than multiplicative exponential smoothing.</p>

<p>It's not useful to the company to say that ""well you might have 8 jillion logins sometime in the next 6 months and you might have 20% more than you had last year."" How do I reduce the variance in the projection?</p>

<p><a href=""http://img836.imageshack.us/img836/8460/holtwintersloginmultipl.png"" rel=""nofollow"">http://img836.imageshack.us/img836/8460/holtwintersloginmultipl.png</a></p>

<p>Finally, I was thinking that after I got accurate projections, I'd put logins and registrations in a neural network, and I'd put something like average wait time on a few machines as the ouput variable, and I'd forecast peak projected processing power demand in 6 months. There are other variables to consider, like software releases that change cpu demand per user, but I'm hoping the neural network will learn when these happen, or that they are easy to detect and account for. I don't have any good data on average wait time yet, but assuming I find some, is this a good plan?</p>
"
"0.0482242822170412","0.0499376169438922"," 32528","<p>I have fitted ARIMA(5,1,2) model using <code>auto.arima()</code> function in R and by looking order we can say this is not a best model to forecast. If outliers exist in the data series, what is the method to fit a model to such data?</p>
"
"0.0681994339470473","0.0706224551546449"," 32657","<p>I was playing with the <a href=""http://cran.r-project.org/web/packages/TSA/index.html"" rel=""nofollow"">TSA</a> package in R and wanted to test the <code>arimax</code> function to the solution provided in Pankratz's <em>Forecasting with Dynamic Regression Models</em>, chapter 8. The savings rate and the function seems to provide similar results as the ones in the book except for the IO weights which are quite different. I bet there is a transformation that I might be missing.</p>

<p>Any help on understanding why IO coefficients are so different would be appreciated...</p>

<p>the solution states </p>

<pre><code>AO @ t=82,43,89
LS @ t=99
IO @ t=62,55
</code></pre>

<p>with Parameters estimates</p>

<pre><code>C = 6.1635
w82 = 2.3346
w99 = -1.5114
w43 = 1.1378
w62 = 1.4574
w55 = -1.4915
w89 = -1.0702
AR1 = 0.7976
MA2 = -0.3762
</code></pre>

<p>To fit the model in R, I used
(<code>saving</code> is the data)</p>

<pre><code>arimax(saving, order = c(1,0,2), fixed=c(NA,0,NA,NA,NA,NA,NA,NA,NA,NA), io=c(55,62), 
       xreg=data.frame(AO82=1*(seq(saving)==82),
                       AO43=1*(seq(saving)==43),
                       AO89=1*(seq(saving)==89),
                       LS99=1*(seq(saving)&gt;=99)),
       method='ML')
</code></pre>

<p>The savings rate data is (100 points)</p>

<p>4.9
5.2
5.7
5.7
6.2
6.7
6.9
7.1
6.6
7
6.9
6.4
6.6
6.4
7
7.3
6
6.3
4.8
5.3
5.4
4.7
4.9
4.4
5.1
5.3
6
5.9
5.9
5.6
5.3
4.5
4.7
4.6
4.3
5
5.2
6.2
5.8
6.7
5.7
6.1
7.2
6.5
6.1
6.3
6.4
7
7.6
7.2
7.5
7.8
7.2
7.5
5.6
5.7
4.9
5.1
6.2
6
6.1
7.5
7.8
8
8
8.1
7.6
7.1
6.6
5.6
5.9
6.6
6.8
7.8
7.9
8.7
7.7
7.3
6.7
7.5
6.4
9.7
7.5
7.1
6.4
6
5.7
5
4.2
5.1
5.4
5.1
5.3
5
4.8
4.7
5
5.4
4.3
3.5</p>

<p>here it is my output</p>

<pre><code>&gt; arimax(saving, order = c(1,0,2),fixed=c(NA,0,NA,NA,NA,NA,NA,NA,NA,NA),io=c(55,62),xreg=data.frame(AO82=1*(seq(saving)==82),
+ AO43=1*(seq(saving)==43),AO89=1*(seq(saving)==89),LS99=1*(seq(saving)&gt;=99)),method='ML')

Call:
arimax(x = saving, order = c(1, 0, 2), xreg = data.frame(AO82 = 1 * (seq(saving) == 
    82), AO43 = 1 * (seq(saving) == 43), AO89 = 1 * (seq(saving) == 
    89), LS99 = 1 * (seq(saving) &gt;= 99)), fixed = c(NA, 0, NA, NA, NA, NA, 
    NA, NA, NA, NA), method = ""ML"", io = c(55, 62))

Coefficients:
         ar1  ma1     ma2  intercept    AO82    AO43     AO89     LS99    IO-55   IO-62
      0.7918    0  0.3406     6.0628  2.3800  1.1297  -1.0466  -1.4885  -0.5958  0.5517
s.e.  0.0674    0  0.1060     0.3209  0.3969  0.3780   0.3835   0.5150   0.4044  0.3772

sigma^2 estimated as 0.2611:  log likelihood = -75.57,  aic = 169.14
</code></pre>
"
"0.0862662185627507","0.111663906120888"," 32694","<p>I'm using R together with the <code>forecast</code> package to set up a ARIMA model, that will be used to predict a energy related variable. I used <code>auto.arima()</code> to fit different models (according to geographic region), and I need to put the model coefficients in our database, so that the IT folks can automate things. That's exactly the problem: I simply don't know how set up the equations by looking at the model:</p>

<pre><code>ARIMA(1,0,1)(2,0,1)[12] with non-zero mean 

Coefficients:

       ar1     ma1    sar1    sar2     sma1   intercept    prec0    prec1
     0.3561  0.3290  0.6857  0.2855  -0.7079  11333.240   15.5291  28.0817

s.e. 0.2079  0.1845  0.2764  0.2251   0.3887   2211.302    6.2147   6.0906
</code></pre>

<p>I have 2 regressor variables (prec0 and prec1). Given the residuals, the ARIMA vector <code>ARIMA(1,0,1)(2,0,1)[12]</code>, the time series up to period $t$, the number $h$ of forecasting periods and the regressor matrix reg, how can I set a function to return the forecast values? I.e:</p>

<pre><code>do.forecast = function(residuals, ARIMA, timeSeries, h, regMatrix)
{
  p = ARIMA[1]
  q = ARIMA[3]

  ## arima equations here...
}
</code></pre>

<p>Thanks!  </p>

<p>PS: I know this is a possible duplicate of <a href=""http://stats.stackexchange.com/questions/23881/reproducing-arima-model-outside-r"">Reproducing ARIMA model outside R</a>, but my model seems very different, and I really don't know how to start with.</p>
"
"0.144672846651124","0.149812850831677"," 33862","<p>I have some models built with the <code>auto.arima</code> function from the <code>forecast</code> package. I'm modeling a variable called 'natural efluent energy' (ena), which is how much energy you can extract from some Hydrography region. There are 2 regressor variables (rainfall precipitation from period $t$ and $t-1$.)</p>

<p>Each region has it's own model - some series show positive trend, some shows negative trend, and some seems stationary. The problem is that some forecasts 'from <code>auto.arima</code>' are giving values higher/lower than usual (some forecasts give me negative values, which are not possible).</p>

<p>My original call is below:</p>

<pre><code> m1 = auto.arima(serie, xreg = regvars)
</code></pre>

<p>For the data on the link, I changed it to</p>

<pre><code> m1 = auto.arima(serie, xreg = regvars, max.P = 0, max.Q = 0, stationary = TRUE)
</code></pre>

<p>Then I get good forecasts in this case. My question is, what these parameters(<code>max.P</code>, <code>max.Q</code>) actually control, and how they relate to the trend show by my model variable?</p>

<p>Here is a link for the historic data:
<a href=""http://www.datafilehost.com/download-7718b3fc.html"" rel=""nofollow"">http://www.datafilehost.com/download-7718b3fc.html</a></p>

<p>And here a link for the forecast regressors:
<a href=""http://www.datafilehost.com/download-ca44dfa4.html"" rel=""nofollow"">http://www.datafilehost.com/download-ca44dfa4.html</a></p>

<p>And here a link of mean historic values, the forecast must fall between these values:
<a href=""http://www.datafilehost.com/download-e1e265b7.html"" rel=""nofollow"">http://www.datafilehost.com/download-e1e265b7.html</a></p>

<p>My data starts at 2001/Jun, so the serie is:</p>

<pre><code>  y = ts(dframe$ena, freq = 12, start = c(2001, 6))
</code></pre>
"
"0.0482242822170412","0.0499376169438922"," 34139","<p>I am using an ARIMA model to create a model for correlated errors from my regression model. I am using the <code>auto.arima</code> function from the <a href=""http://cran.r-project.org/web/packages/forecast/index.html"" rel=""nofollow"">forecast</a> package in R. I am able to get more data at some frequent interval after the regression model is created, so I get more values for the correlated errors. </p>

<p>My question is how do I update the ARIMA model with a gap in time interval between readings.</p>
"
"0.0681994339470473","0.0706224551546449"," 34226","<p>I'm using a STL decomposition to make forecasts in R (using the <code>forecast</code> package), but I'm not sure how to incorporate my regressors into the model.</p>

<p>I'm using the forecast function:</p>

<pre><code>f = forecast(stl(my.data, s.window = 'periodic'), h = 12, method = 'arima')
</code></pre>

<p>The function above accepts the <code>xreg</code> parameter, but how do I specify the
model regressors and the forecast regressors?</p>

<p>Is there a function that does this or do I have to do it 'by hand'?</p>
"
"0.0482242822170412","0.0499376169438922"," 34690","<p>I wanted to focus on volatility forecasting, so instead of asking R to compute a GARCH where it would compute the errors on the returns, I wanted to model the volatility as an ARMA and add an external regressor using the argument xreg in the arima function.</p>

<p>I have two questions:</p>

<ul>
<li><p>Is it exactly equivalent to compute an ARMA(p,q) on the volatility with external regressors as the squared returns and to compute a GARCH (for the volatility forecast)</p></li>
<li><p>Is it the correct way to do it in R ?</p></li>
</ul>

<p>Tony</p>
"
"0.109362392486473","0.132122515500744"," 38187","<p>I'm modelling a time series data using ARIMA. Now, I'm trying to test for the serial correlation of my model SARIMA(1,1,1) using the durbin watson test.</p>

<p>My problem is that I don't know what linear model I would put on the formula of the <code>dwtest</code> function in R. Here's the usage of the function,</p>

<pre><code>dwtest(formula, order.by = NULL, alternative = c(""greater"", ""two.sided"", ""less""),
       iterations = 15, exact = NULL, tol = 1e-10, data = list())
</code></pre>

<p>Here's my code below,</p>

<p>Data: <a href=""http://iitstat.weebly.com/uploads/7/3/4/0/7340846/chickenprod.rdata"" rel=""nofollow"">http://iitstat.weebly.com/uploads/7/3/4/0/7340846/chickenprod.rdata</a></p>

<p>To download the data just right click the link and click ""Save Link As...""</p>

<pre><code>library(forecast)
library(lmtest)
ChickenProd &lt;- ts(ChickenProd, start = 1980, frequency = 4)
SARIMA111 &lt;- Arima(ChickenProd, seasonal = list(order = c(1,1,1), period = 4))
</code></pre>

<p>The residuals of my model SARIMA111 is obtain by</p>

<pre><code>SARIMA111[[""residuals""]]
</code></pre>

<p>Now, I want to test the serial correlation of it using the Durbin-Watson test, but I don't know what linear model I would use in the <code>formula</code> argument of <code>dwtest</code> function in R. Is it the SARIMA(1,1,1) model? If so, how will I extract the coefficients of the SARIMA(1,1,1) model, and make a linear model formula in R?</p>

<p>Thank you in Advance!</p>
"
"0.102299150920571","0.14124491030929"," 38248","<p>I just want to ask about the <code>Arima</code> function in forecast package. The usage of it is,</p>

<pre><code>Arima(x, order=c(0,0,0), seasonal=list(order=c(0,0,0), period=NA),
    xreg=NULL, include.mean=TRUE, include.drift=FALSE, 
    include.constant, lambda=model$lambda, transform.pars=TRUE, 
    fixed=NULL, init=NULL, method=c(""CSS-ML"",""ML"",""CSS""), n.cond, 
    optim.control=list(), kappa=1e6, model=NULL)
</code></pre>

<p>My data has trend and seasonality, so I applied seasonal differencing using the codes below,</p>

<pre><code>Diff1LogCP &lt;- diff(LogChickenProd, lag = 4, differences = 1)
</code></pre>

<p>Now, I want to have a model SARIMA(1,1,1). Using the <code>Arima</code> function, I'm not sure with the <code>order</code> and <code>seasonal</code> argument. Since my model is SARIMA(1,1,1), so what I did is,</p>

<pre><code>SARIMA111 &lt;- Arima(ChickenProd, seasonal = list(order = c(1,1,1), period = 4))
</code></pre>

<p>Notice that I ignore the <code>order</code> argument before the <code>seasonal</code> argument. This is because I'm using a seasonal ARIMA model, and not the ordinary ARIMA. So I use the <code>seasonal</code> argument only. Is that correct? Or do I need to include the <code>order</code> argument, that makes my new codes be</p>

<pre><code>SARIMA111 &lt;- Arima(ChickenProd, order = c(1,1,1), seasonal = list(order = c(1,1,1), period = 4))
</code></pre>

<p>Or should it be like</p>

<pre><code> SARIMA111 &lt;- Arima(ChickenProd, order = c(1,0,1), seasonal = list(order = c(0,1,0), period = 4))
</code></pre>

<p>Thank you in advance!</p>
"
"0.0482242822170412","0.0499376169438922"," 43370","<p>I have two time-series, <code>x</code> and <code>y</code>. I would like to prewhiten <code>x</code> by fitting an ARMA(p,q) (or in my case ARMA(1,1)) process and then use the coefficients to filter <code>y</code>. This seems like a pretty standard thing to want to do.  However, the <code>stats:::filter</code> function does only MA or AR filtering it looks like.  What is the appropriate way to do this? Also, should one use the <code>arima</code> function in R to do this or are there other ways?</p>
"
"0.136398867894095","0.123589296520629"," 43588","<p>The main <strong>problem</strong> is:  I cannot obtain similar parameter estimates with EViews and R.</p>

<p>For reasons I do not know myself, I need to estimate parameters for certain data using EViews. This is done by picking the NLS (nonlinear least squares) option and using the following formula: <code>indep_var c dep_var ar(1)</code></p>

<p>EViews <a href=""http://forums.eviews.com/viewtopic.php?f=7&amp;t=465"" rel=""nofollow"">claims</a> that they estimate linear AR(1) processes such as:
$$
Y_t = \alpha + \beta X_t + u_t
$$
where $u_t$ errors are defined as: 
$$
u_t = \rho \cdot u_{t-1} + \varepsilon
$$
by using an equivalent equation (with some algebraic substitutions):
$$
Y_t = (1 - \rho) \alpha + \rho Y_{t - 1} + \beta X_t - \rho \beta X_{t - 1} + \varepsilon_t
$$
Furthermore, <a href=""http://forums.eviews.com/viewtopic.php?f=4&amp;t=3768"" rel=""nofollow"">this thread over at the EViews forums</a> suggests that their NLS estimations are generated by the Marquardt algorithm.</p>

<p>Now, the go-to R function to estimate AR(1) processes is <code>arima</code>. However, there are two problems:  </p>

<ol>
<li>the estimates are maximum likelihood estimates; </li>
<li>the intercept estimate is not actually the intercept estimate (according to R.H. Shumway &amp; D.S. Stoffer).</li>
</ol>

<p>Therefore, I turned to the <code>nlsLM</code> function from the minpack.lm package. This function uses the Marquardt algorithm to achieve nonlinear least squares estimates, which should yield the same results as the EViews implementation (or very similar ones, at least).</p>

<p>Now the code. I have a data frame (<code>data</code>) with an independent variable and a dependent variable such as the one generated by the following code:</p>

<pre><code>data &lt;- data.frame(independent = abs(rnorm(48)), dependent = abs(rnorm(48)))
</code></pre>

<p>To estimate parameters in the equation EViews claims to estimate (3<sup>rd</sup> one on this post), I use the following commands:</p>

<pre><code>library(minpack.lm)
result &lt;-
nlsLM(dependentB ~ ((1 - theta1) * theta2) + (theta1 * dependentA) +
                    (theta3 * independentB) - (theta1 * theta3 * independentA),
data = list(dependentB = data$dependent[2:48], dependentA = data$dependent[1:47],
   independentB = data$independent[2:48], independentA = data$independent[1:47]),
start = list(theta1 = -10, theta2 = -10, theta3 = -10)
)
</code></pre>

<p>Unfortunately, the estimates output by <code>nlsLM</code> are not close to the ones output by EViews. Do you have any idea of what might be causing this? Or maybe my code is wrong?</p>

<p>Finally, I would like to say that I personally am an R user - that is exactly why I'm trying to do this in R instead of EViews. I would also love to provide you the data I'm working with but it's impossible since it's confidential data.</p>
"
"NaN","NaN"," 44992","<p>In the <code>arima</code> function in R, what does <code>order(1, 0, 12)</code> mean? What are the values that can be assigned to <code>p</code>, <code>d</code>, <code>q</code>, and what is the process to find those values?</p>
"
"0.0482242822170412","0.0499376169438922"," 45018","<p>I would like to use the stlf forecast function from the R package forecast (http://cran.r-project.org/web/packages/forecast/index.html) and include regressors in the model.</p>

<p>Question 1: This can only be done using """" method=""arima""  """" - right?</p>

<p>Question 2: For the model calibration the parameter ""xreg"" should work but how can I enter the new data for th forecast? ""newxreg"" did not work for me.</p>

<p>Thank you</p>
"
"0.136398867894095","0.14124491030929"," 47185","<p>I am impressed by the R <code>forecast</code> package, as well as e.g. the <code>zoo</code> package for irregular time series and interpolation of missing values.</p>

<p>My application is in the area of call center traffic forecasting, so data on weekends is (nearly) always missing, which can be nicely handled by <code>zoo</code>. Also, some discrete points may be missing, I just use R's <code>NA</code> for that.</p>

<p>The thing is: all the nice magic of the forecast package, such as <code>eta()</code>, <code>auto.arima()</code> etc, seem to expect plain <code>ts</code> objects, i.e. equispaced time series not containing any missing data. I think real world applications for equispaced-only time series are definitely existent, but - to my opinion -  v e r y  limited.</p>

<p>The problem of a few discrete <code>NA</code> values can easily be solved by using any of the offered interpolation functions in <code>zoo</code> as well as by <code>forecast::interp</code>. After that, I run the forecast.  </p>

<p>My questions:  </p>

<ol>
<li>Does anyone suggest a better solution?</li>
<li><p><strong>(my main question)</strong> At least in my application domain, call center traffic forecasting (and as far as I can imagine most other problem domains), time series are not equispaced. At least we have recurring ""business days"" scheme or something. What's the best way to handle that and still use all the cool magic of the forecast package?  </p>

<p>Should I just ""compress"" the time series to fill the weekends, do the forecast, and then ""inflate"" the data again to re-insert NA values in the weekends? (That would be a shame, I think?)  </p>

<p>Are there any plans to make the forecast package fully compatible with irregular time series packages like zoo or its? If yes, when and if no, why not?  </p></li>
</ol>

<p>I'm quite new to forecasting (and statistics in general), so I might overlook something important.</p>
"
"0.107832773203438","0.111663906120888"," 47416","<p>We know that dealing with model involving MA factors is not easy to estimate, since there are past values of errors to be computed recursively. And this recursive estimation requires preliminary (initial) estimates of the parameters. For example, an ARMA(1,2)
$$z_t=\phi z_{t-1}-\theta_1 \varepsilon_{t-1}-\theta_2 \varepsilon_{t-2}+\varepsilon_t$$
To estimate the parameters, we need to compute first the values of $\varepsilon_{t-1}$ and $\varepsilon_{t-2}$, since these are not available yet. And they are computed using
$$\varepsilon_t=z_t-\phi z_{t-1}+\theta_1 \varepsilon_{t-1}+\theta_2 \varepsilon_{t-2}$$
Procedures for obtaining preliminary estimates of the parameters is available in Box and Jenkins, Time Series: Forecasting and Control. And this estimation is already available in much statistical software. My question is, ""Is there a function for obtaining a preliminary estimate of the parameters in R?""</p>

<p>I need this to obtain a preliminary estimate for my Space-Time ARIMA. Another question is, ""How does <code>arima</code> function of R compute preliminary estimates of the parameters when there are MA factors involved?""</p>

<p>Thanks in advance!</p>
"
"0.109362392486473","0.132122515500744"," 47419","<p>I am trying to fit a time series using the function auto.arima and I face some strange results.</p>

<p>As a first try, I use the command</p>

<pre><code>auto.arima(data,d=0,D=1,max.p=2,max.q=2,max.P=2,max.Q=2,max.order=8, xreg=xreg_past,trace=TRUE,ic=""aic"")
</code></pre>

<p>The model I get is an ARIMA(2,0,2)(0,1,1)[12] with an AIC equal to -300.14.</p>

<p>But since I know that this command will make use of the stepwise selection algorithm, I want to make a try with the tests of all possible models using the option stepwise=FALSE. </p>

<p>I thus try the command</p>

<pre><code>auto.arima(data,d=0,D=1,max.p=2,max.q=2,max.P=2,max.Q=2,max.order=8, xreg=xreg_past,stepwise=FALSE,trace=TRUE,ic=""aic"")
</code></pre>

<p>And now, the model I get is an ARIMA(0,0,2)(2,1,0)[12] with an AIC equal to -293.14. Since my second attempt takes all the models into account, this result is strange as the previous model had a lower AIC. Furthermore, If I take a look in the trace of the last function call, I see that the ARIMA(2,0,2)(0,1,1)[12] model has now an AIC of -245.13 which explains why it has been rejected. Why did the AIC value change ?</p>

<p>At least, if I use the simple command </p>

<pre><code>arima(data, order=c(2,0,2), seasonal= list(order=c(2,1,2), period=12), xreg=xreg_past)
</code></pre>

<p>I get an AIC value of -319.15, which is better that the two models provided before.</p>

<p>I think I am missing something important but I am not able to see what. Can somebody help me ?</p>

<p>Thanks in advance,</p>

<p>Regards,</p>

<p>Ludo</p>
"
"0.0681994339470473","0.0706224551546449"," 52035","<p>I have a weekly time series representing costs for a cohort. I want to tell whether an intervention on the cohort (we can assume it happened in a single week) has decreased costs for the cohort. I happen to know that the trend over this period for the population from which this cohort was taken was -120 per week per week.</p>

<p>My initial thought was simply to do a linear regression <code>lm(Costs~Weeks,offset=-120*Weeks)</code> but (obviously) the significance is not only a function of the effect of the intervention but also how far back I look (if I look back to $-\infty$ it will of course appear non-significant).</p>

<p>I looked at this website: <a href=""http://www.r-bloggers.com/time-series-intervention-analysis-wih-r-and-sas/"" rel=""nofollow"">http://www.r-bloggers.com/time-series-intervention-analysis-wih-r-and-sas/</a> and tried to replicate the R code with my data, but when I enter the arimax() command, I got the error message </p>

<pre><code>Error in stats:::arima(x=x,order=order,seasonal=seasonal,fixed=par[1:narma], : wrong length for 'fixed'
</code></pre>

<p>Now, I'm not sure what to do. Can anyone give me some guidance?</p>
"
"0","0.0499376169438922"," 53051","<p>I was working on ARIMA in R and I am trying not to use library <code>forecast</code> as much as possible. I have a code for finding the best ARIMA model, but it is showing some warning messages.</p>

<p>Here is my function:</p>

<pre><code>best.aic&lt;-1e8
n&lt;-length(x.ts)
for(p in 0:maxord[1]) for(d in 0:maxord[2]) for(q in 0:maxord[3])
{   
        fit&lt;-arima(x.ts,order=c(p,d,q))
    fit.aic&lt;--2*fit$loglik+(log(n)+1)*length(fit$coef)
    if(fit.aic&lt;best.aic)
    {
        best.aic&lt;-fit.aic
        best.fit&lt;-fit
        best.model&lt;-c(p,d,q)
    }
}
list(best.aic,best.fit,best.model)
</code></pre>

<p>It gives the same warning 4 times:</p>

<pre><code>In arima(x.ts, order = c(p, d, q)) :
possible convergence problem: optim gave code=1
</code></pre>

<p>Also, I will appreciate any help concerning simulation of ARIMA by hand, not by <code>arima.sim</code> function. Thank you.</p>
"
"0.0835269069584557","0.0576629931704892"," 55462","<p>I am using <code>KFAS</code> package for <code>R</code>.</p>

<p>You can run</p>

<pre><code>install.packages(""KFAS"")
library(KFAS)
?regSSM
</code></pre>

<p>to see how this package allows to build a state space representation of linear regression models and many others.</p>

<p>Now let we have the following state space system:</p>

<p>$S_{t}=\alpha+(1+k_{t})L_{t}+v_{t}$</p>

<p>$k_{t}=\phi k_{t-1}+(1-\phi)\bar{k}+w_{t}$</p>

<p>being $\bar{k}$ a constant, a.k.a. the unconditional mean of the unobservable AR(1) process.</p>

<p>Anyone can tell me how may I set this state space representation in <code>KFAS</code> through <code>regSSM</code> or any other <code>KFAS</code> package's function (like <code>arimaSSM</code>)?</p>
"
"0.107832773203438","0.0893311248967107"," 55899","<p>I have some data which i am trying to work on. I am pretty new to R though, but i love R. First, I fit an <code>arima</code> model to this data and used the <code>detectIO</code> function in R to detect a single influential outlier (IO). I then incorporated this IO into my model and then developed an <code>arimax</code> model, now with the IO. 
I first used this model: </p>

<pre class=""lang-r prettyprint-override""><code>Model2 = arimax(mydata, order=c(0,2,1),
                seasonal=list(order=c(0,0,1), period=12), io=c(48))
</code></pre>

<p>Then later, used the model below after seeing a similar code in a book:  </p>

<pre class=""lang-r prettyprint-override""><code>Model4 = arima(mydata, order = c(0, 2, 1),
               seasonal=list(order = c(0, 0, 1), period = 12), 
               xtransf=data.frame(I48=1*(seq(mydata)==48), I48=1*(seq(mydata)==48)), 
               transfer=list(c(0,0), c(1,0)))
</code></pre>

<p>I then run the <code>predict</code> function on these two models and I keep getting this error:</p>

<pre class=""lang-r prettyprint-override""><code>&gt; predict(Model4, n.ahead=24)

Error in array(x, c(length(x), 1L), if (!is.null(names(x))) list(names(x),  :
  attempt to set an attribute on NULL
</code></pre>

<p>I don't know what I am doing wrong. I don't know if I am using the right functions and codes? Does there really exist a <code>predict</code> function on <code>arimax</code>?</p>

<p>I would also appreciate if I get any help on how to set-up the <code>xtransf</code> with some real data values.</p>
"
"0.145401681725576","0.165624338327385"," 56374","<p>as I am stepping into forecasting with ARIMA models, I am trying to understand how I can improve a forecast based on ARIMA fit with seasonality and drift. </p>

<p>My data is the following time series ( over 3 years, with clear trend upwards and visible seasonality, which seems to be not supported by autocorrelation at lags 12, 24, 36??). </p>

<pre><code>    &gt; bal2sum3years.ts
             Jan     Feb     Mar     Apr     May     Jun     Jul     Aug          
    2010 2540346 2139440 2218652 2176167 2287778 1861061 2000102 2560729 
    2011 3119573 2704986 2594432 2362869 2509506 2434504 2680088 2689888 
    2012 3619060 3204588 2800260 2973428 2737696 2744716 3043868 2867416 
             Sep     Oct     Nov     Dec
    2010 2232261 2394644 2468479 2816287
    2011 2480940 2699780 2760268 3206372
    2012 2951516 3119176 3032960 3738256
</code></pre>

<p>The model that was suggested by <code>auto.arima(bal2sum3years.ts)</code> gave me the following model:</p>

<pre><code>    Series: bal2sum3years.ts 
    ARIMA(0,0,0)(0,1,0)[12] with drift         

    Coefficients:
              drift
          31725.567
    s.e.   2651.693

    sigma^2 estimated as 2.43e+10:  log likelihood=-321.02
    AIC=646.04   AICc=646.61   BIC=648.39
</code></pre>

<p>However, the <code>acf(bal2sum3years.ts,max.lag=35)</code> does not show acf coefficients higher than 0.3. The seasonality of the data is, however, pretty obvious - spike at the beginning of every year. This is what the series looks like on the graph:
<img src=""http://i.stack.imgur.com/kQi5N.png"" alt=""Original Time Series""></p>

<p>The forecast using <code>fit=Arima(bal2sum3years.ts,seasonal=list(order=c(0,1,0),period=12),include.drift=TRUE)</code> , called by function <code>forecast(fit)</code>, results in the next 12months's means being equal to the last 12 months of the data plus constant. This can be seen by calling <code>plot(forecast(fit))</code>, </p>

<p><img src=""http://i.stack.imgur.com/GJqcG.png"" alt=""Actual and Forecasted Data""></p>

<p>I have also checked the residuals, which are not autocorrelated but have positive mean ( non zero). </p>

<p>The fit does not model the original time series precisely, in my opinion ( blue the original time series, red is the <code>fitted(fit)</code>:</p>

<p><img src=""http://i.stack.imgur.com/ux3i7.png"" alt=""Original vs fit""></p>

<p>The guestion is, is the model incorrect? Am I missing something? How can I improve the model? It seems that the model literally takes the last 12 months and adds a constant to achieve the next 12 months. </p>

<p>I am a relative beginner in time series forecasting models and statistics. </p>

<p>Thank you very much for your answers!</p>
"
"0.0482242822170412","0.0499376169438922"," 56428","<p>I have a time series data set.  I can decompose it and get the trend but I would like to put confidence ranges around the trend (past) not the forecast-ed component.  The decompose function also doesn't handle N/As very well so is there another way to define the trend with data that has N/As.  I have been trying to use Holt-Winters and ARIMA but neither seem to be able to do this.</p>
"
"0.0835269069584557","0.0864944897557338"," 57119","<p>I'm dealing with a time series data and I'm trying to construct a time series model for this particular dataset.  I'm new to R and tried using the the <code>auto.arima</code> function under the forecast package:</p>

<pre><code>fit &lt;- auto.arima(tsdata, xreg=cbind(CSS2$Month,CSS2$DayID,CSS2$Year), 
                  stepwise=FALSE, approximation=FALSE)
summary(fit)
resid(fit)
acf(resid(fit))
</code></pre>

<p>However, I noticed some problems in the ACF plot and the PACF plot from the resulting model.  Both of those appear to show some trends or seasonality.
The data that I'm dealing with do have some strong seasonal patterns (on a weekly basis and on a monthly basis), and I thought that this would be captured using <code>auto.arima</code>.  I do have the most recent version of the forecast package.
I should mention that I have daily data for 3 years, so there are a total of 1095 observations.</p>

<p>Any suggestions would be much appreciated, thank you!</p>

<p>ACF Plot PACF Plot</p>
"
"0.0482242822170412","0.0499376169438922"," 57123","<p>I need to estimate parameters of an AR model which is in the form of AR(1,11) it means that coefficients of AR orders from order 2 until order 10 are zero. How can I estimate these two parameters in <code>R</code> since <code>arima</code> function only accepts p as the order of the AR component. Note that this model has a different structure than Seasonal AR. </p>

<p>Thanks</p>
"
"0.119349009407333","0.14124491030929"," 58407","<p>I think this is a basic question, but maybe I am confusing the concepts.</p>

<p>Suppose I fit an ARIMA model to a time series using, for example, the function auto.arima() in the R forecast package. The model assumes constant variance. How do I obtain that variance? Is it the variance of the residuals?</p>

<p>If I use the model for forecasting, I know that it gives me the conditional mean. I'd like to know the (constant) variance as well.</p>

<p>Thank you.</p>

<p>Bruno</p>

<hr>

<h2>Update 1:</h2>

<p>I added some code below. The variance given by <code>sigma2</code> isn't close to the one calculated from the fitted values. I'm still wondering if <code>sigma2</code> is the right option. See figure below for time series plot.</p>

<pre><code>demand.train &lt;- c(10.06286, 9.56286, 10.51914, 12.39571, 14.72857, 15.89429, 15.89429, 17.06143, 
              17.72857, 16.56286, 14.23000, 15.39571, 13.06286, 15.39571, 15.39571, 16.56286,
              16.21765, 15.93449, 14.74856, 14.46465, 15.38132)
timePoints.train &lt;- c(""Q12006"", ""Q22006"", ""Q32006"", ""Q12007"", ""Q22007"", ""Q32007"", ""Q12008"", ""Q22008"",
                      ""Q32008"", ""Q12009"", ""Q22009"", ""Q32009"", ""Q12010"", ""Q22010"", ""Q32010"", ""Q12011"",
                      ""Q22011"", ""Q32011"", ""Q12012"", ""Q22012"", ""Q32012"")

plot(1:length(timePoints.train), demand.train, type=""o"", xaxt=""n"", ylim=c(0, max(demand.train) + 2), 
     ylab=""Demand"", xlab=""Quadrimestre"")

title(main=""Time Series Demand of Product C"", font.main=4)
axis(1, at=1:length(timePoints.train), labels=timePoints.train)
box()

### ARIMA Fit
library(forecast)

# Time series
demandts.freq &lt;- 3
demandts.train &lt;- ts(demand.train, frequency=demandts.freq, start=c(2006, 1))

# Model fitting
demandts.train.arima &lt;- auto.arima(demandts.train, max.p=10, max.q=10, max.P=10, max.Q=10, max.order=10)
print(demandts.train.arima)
summary(demandts.train.arima)
demandts.train.arima.fit &lt;- fitted(demandts.train.arima)

# Forecast ARIMA (conditional means)
demandts.arima.forecast &lt;- forecast(demandts.train.arima, h = 3, level=95)
print(demandts.arima.forecast)

# Constant variance from ARIMA
demandts.arima.var &lt;- demandts.train.arima$sigma2
print(demandts.arima.var)

# Variance from fitted values
print(var(demandts.train.arima.fit))
</code></pre>

<p><img src=""http://i.stack.imgur.com/E5gv0.png"" alt=""Time Series Plot""></p>
"
"0.180841058313905","0.199750467775569"," 58657","<p>I'm using a daily time series of sales data that contains about 2 years of daily data points. Based on some of the online-tutorials / examples I tried to identify the seasonality in the data. It seems that there is a weekly, monthly and probably a yearly periodicity / seasonality.</p>

<p>For example, there are paydays, particularly on 1st payday of the month effect that lasts for few days during the week. There are also some specific Holiday effects, clearly identifiable by noting the observations.</p>

<p>Equipped with some of these observations, I tried the following:</p>

<ol>
<li><p>ARIMA (with <code>Arima</code> and <code>auto.arima</code> from R-forecast package), using regressor (and other default values needed in the function).  The regressor I created is basically a matrix of 0/1 values:</p>

<ul>
<li>11 month (n-1) variables</li>
<li>12 holiday variables</li>
<li>Could not figure out the payday part...since it's little more complicated effect than I thought. The payday effect works differently, depending on the weekday of the 1st of month.</li>
</ul>

<p>I used 7 (i.e., weekly frequency) to model the time series. I tried the test - forecasting 7 days at a time. The results are reasonable: average accuracy for a forecast of 11 weeks comes to weekly avg RMSE to 5%.</p></li>
<li><p>TBATS model (from R-forecast package) - using multiple seasonality (7, 30.4375, 365.25) and obviously no regressor. The accuracy is surprisingly better than the ARIMA model at weekly avg RMSE 3.5% .</p>

<p>In this case, the model without ARMA errors perform slightly better. Now If I apply the coefficients for just the Holiday Effects from the ARIMA model described in #1, to the results of the TBATS model the weekly avg RMSE improves to 2.95%</p></li>
</ol>

<p>Now without having much background or knowledge on the underlying theories of these models, I'm in a dilemma whether this TBATS approach is even a valid one. Even though it's improving the RMSE significantly in the 11 weeks test, I'm wondering whether it can sustain this accuracy in the future. Or even if applying Holiday effects from ARIMA to the TBATS result is justifiable. Any thoughts from any / all the contributors will be highly appreciated. </p>

<p><a href=""https://s3.amazonaws.com/CKI-FILE-SHARE/TS+Test+Data.txt"">Link for Test Data</a></p>

<p>Note: Do ""Save Link As"", to download the file.</p>
"
"0.144672846651124","0.149812850831677"," 59058","<p>I'm trying to forecast a seasonal time series based on its historical values, and also two more time series (that are seasonal themselves.)  </p>

<p>I'm trying to use an <strong>auto.arima</strong>, and I'm going to input the other two time series (the exogeneous regressors) as a contatenated list of dummy variables, in auto.arima's <strong>xreg</strong> parameter.</p>

<p>I am having difficulty how to use the forecast function after this point.  I've written up the following code, but I don't understand what I should put in the <strong>xreg</strong> and <strong>newxreg</strong> parameters of the forecast function.</p>

<pre><code>tempfit&lt;-auto.arima(dnew, xreg=dExt)
plot(forecast(tempfit, xreg=dnew1,newxreg=dExt1))
</code></pre>

<p>Also, my data points for these three series were all values per day that had a seven day seasonality. In order to let auto.arima calculate the (p,q,d) for seasonality, I converted them to time series with a frequency of 7. Now, after forecasting is done, the plot shows one unit for every seven days.  How can I covert this back to one unit per day?</p>

<p>Further, do you happen to know how we can input a set of external regressors to an ETS model?</p>

<p>I would greatly appreciate your inputs!</p>

<p>Thank you.</p>

<p><strong>EDIT</strong>:</p>

<p>I just saw the following page from Dr. Hyndman:
<a href=""http://stats.stackexchange.com/questions/34493/time-series-modeling-with-dynamic-regressors-in-sas-vs-in-r"">Time series modeling with dynamic regressors in SAS vs. in R</a></p>

<p>Is it safe to assume that I don't need to enter a newxreg parameter for my forecast?</p>

<p>Also, I really want to know if it's statistically correct to use the two external regressors in xreg, but then also use a number of dummy variables in xreg that will represent the seasonality of these two variables.  </p>
"
"0.160500112851682","0.166202435506784"," 60648","<p>I am trying to forecast electricity consumption in GWh for 2 years ahead (from June 2013 ahead), using R (the forecast package). For that purpose, I tried regression with ARIMA errors. I fitted the model using the <code>auto.arima</code> function, and I used the following variables in the <code>xreg</code> argument in the <code>forecast.Arima</code> function: </p>

<p>- Heating and Cooling Degree Days,<br>
- Dummies for all 12 months and<br>
- Moving holidays dummies (Easter and Ramadan)  </p>

<p>I have several questions regarding the model:</p>

<p>1) Is it correct to use all 12 dummies for monthly seasonality, since when I tried to include 11, the function returned error. The <code>Auto.arima</code> function returned the model ARIMA(0,1,2)</p>

<p>2)The model returned the following coefficients (I won't specify all of them as there are too many coefficients):</p>

<pre><code>ma1      ma2     HDD     CDD   January  February  March     April
-0.52 -0.16      0.27    0.12  525.84   475.13    472.57    399.01
</code></pre>

<p>I am trying to determine the influence of the temperature component over electricity load. In percentages, (interpreting the coefficients just as with the usual regression) the temperature components (<code>HDD</code>+<code>CDD</code>) account for 11,3% of the electricity consumption. Isn't this too little, considering the fact that the electricity consumption is mostly influenced by the weather component? On the other hand, taking look at the dummies' coefficients, it turns out that the seasonality accounts for the greater part of the load. Why is this? Is the model completely incorrect?</p>

<p>I tried linear regression, and the temperature component accounts for 20%, but it is still a low percentage. Why is this?</p>

<p>3) I am obviously making some mistakes in the use of <code>forecast.Arima</code> or the plot function parameters since when I plot the forecasts, I get a picture of the original time series which is continued (merged) with the forecasts for the whole time series period (from 2004 until 2015). I don't know how to explain this better, I tried to paste the picture, but it seems I cannot paste pictures here.</p>
"
"0.0681994339470473","0.0706224551546449"," 60751","<p>I need to estimate specific lag ARMA model. Here is an example.</p>

<pre><code>ts1 &lt;- arima.sim(list(order = c(0, 0, 5), ma = c(0, 0.7, 0, 0, 0.1)), n = 1000)
</code></pre>

<p>The above model is $X_t = Z_t + 0.7 Z_{t-2} +0.1 Z_{t-5}$, where $\{ Z_t\}$ is white noise.</p>

<p>Is it possible to estimate this model in R? I tried <code>arima</code> function and I haven't found the option to specify the specific lag.</p>

<p>If it is not possible in R, is there any specific reason?</p>
"
"0.127589457900886","0.132122515500744"," 60804","<p>The MARSS package in R offers function for dynamic factor analysis. In this package, the dynamic factor model is written as a special form of state space model and they assume the common trends follow AR(1) process. As I am not very familiar with those two methods, I come with two questions:</p>

<p>Is the Dynamic Factor Analysis a special form of State Space Model? What is the difference between those two methods? </p>

<p>In addition, the Dynamic Factor Analysis does not necessary assume the common trends as AR(1) process. Is there any package that allows the the common trends as seasonal ARIMA (or some other) process?</p>
"
"0.0835269069584557","0.0864944897557338"," 62237","<p>I am working on a data set. After using some model identification techniques, I came out with an ARIMA(0,2,1) model. </p>

<p>I used the <code>detectIO</code> function in the package <code>TSA</code> in R to detect an <em>innovative</em> outlier (IO) at the 48th observation of my original data set. </p>

<p>How do I incorporate this outlier into my model so I can use it for forecasting purposes? I don't want to use the ARIMAX model since I might not be able to make any predictions from that in R. Are there any other ways I could do this?  </p>

<p>Here are my values in order:</p>

<pre><code>VALUE &lt;- scan()
  4.6  4.5  4.4  4.5  4.4  4.6  4.7  4.6  4.7  4.7  4.7  5.0  5.0  4.9  5.1  5.0  5.4
  5.6  5.8  6.1  6.1  6.5  6.8  7.3  7.8  8.3  8.7  9.0  9.4  9.5  9.5  9.6  9.8 10.0
  9.9  9.9  9.8  9.8  9.9  9.9  9.6  9.4  9.5  9.5  9.5  9.5  9.8  9.3  9.1  9.0  8.9
  9.0  9.0  9.1  9.0  9.0  9.0  8.9  8.6  8.5  8.3  8.3  8.2  8.1  8.2  8.2  8.2  8.1
  7.8  7.9  7.8  7.8
</code></pre>

<p>That is actually my data. They are unemployment rates over a period of 6 years. There are 72 observations then . Each value is to at most one decimal place</p>
"
"0.0482242822170412","0.0499376169438922"," 63448","<p>I am trying to fit an ARIMA model with one exogenous variable using in R the function <code>auto.arima</code> with <code>xreg</code> option. </p>

<p>I am having some doubts because none of my data is stationary and they are both seasonal.  </p>

<p>Do I have to apply log and  differentiate my series before applying <code>auto.arima</code>?<br>
How do I treat the seasonality in the original time series and in the exogenous series?
How do I interpret the results of <code>auto.arima</code> function?</p>
"
"0.144672846651124","0.149812850831677"," 63681","<p>I have been adamantly searching the web to learn how to successfully implement a dynamic regression time series in the forecast package for R. The time series data that I am using is weekly data (frequency=52) of incoming call volume and prediction variables are mailers sent out every now and then. They are a significant predictor of the data for the week that they hit, the following week, and the week after that. I have created lagged variables and use these three as the predictors. </p>

<p>My main concern is that the arima model is not taking into account the time series frequency. When I tell it to recognize the ts with a frequency of 52 it has an error. 
I have looked at the <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">fortrain function</a> but do not understand it. I also have looked at the tbats suggested but found that those will not work with prediction variables. </p>

<p>The Zoo function recognizes 52 frequency but it is not advised to use with the <a href=""http://stackoverflow.com/questions/16050684/using-the-combination-forecastauto-arima"">forecast package</a>.</p>

<p>Here is the basic code. The problem is that the time series calwater[,5] is not recognized as such. It is imputed as a simple vector as an integer...</p>

<pre><code>#this works without taking into acount the ts
fit2 &lt;- auto.arima(calwater[6:96,5], xreg=calwater[6:96,6:8], d=0)
fccal &lt;- forecast(fit2, xreg=calwater[97:106,6:8], h=10)
fccal
plot(fccal, main=""Forecast Cal Water"", ylab=""Calls"")

#to form a ts object
calincall&lt;-ts(calwater[1:106,5],start=c(2011,23),frequency=52)

#once the ts is added to the model this dispalys
#Error in `[.default`(calincall, 2:100, 1) : incorrect number of dimensions
</code></pre>

<p>Maybe the error is because there is just a little over two years of data. </p>

<pre><code>#Time Series: Start = c(2011, 23), End = c(2013, 24),Frequency = 52 
</code></pre>

<p>I would be very grateful for any guidance in for this particular issue. I am using the forecast package and prefer to continue within the package but I am open to suggestions. </p>
"
"0.204598301841142","0.200096956271494"," 64711","<p>I have a time series I am trying to forecast, for which I have used the seasonal ARIMA(0,0,0)(0,1,0)[12] model (=fit2). It is different from what R suggested with auto.arima (R calculated ARIMA(0,1,1)(0,1,0)[12] would be a better fit, I named it fit1). However, in the last 12 months of my time series my model (fit2) seems to be a better fit when adjusted (it was chronically biased, I have added the residual mean and the new fit seems to sit more snugly around the original time series. Here is the example of the last 12 months and MAPE for 12 most recent months for both fits:</p>

<p><img src=""http://i.stack.imgur.com/kkUOb.png"" alt=""fit1, fit2 and original data""></p>

<p>The time series looks like this:</p>

<p><img src=""http://i.stack.imgur.com/twNkT.png"" alt=""original time series""></p>

<p>So far so good. I have performed residual analysis for both models, and here is the confusion. </p>

<p>The acf(resid(fit1)) looks great, very white-noisey:</p>

<p><img src=""http://i.stack.imgur.com/gyIv3.png"" alt=""acf of fit1""></p>

<p>However, Ljung-Box test doesn't look good for , for instance, 20 lags: </p>

<pre><code>    Box.test(resid(fit1),type=""Ljung"",lag=20,fitdf=1)
</code></pre>

<p>I get the following results:</p>

<pre><code>    X-squared = 26.8511, df = 19, p-value = 0.1082
</code></pre>

<p>To my understanding, this is the confirmation that the residuals are not independent ( p-value is too big to stay with the Independence Hypothesis). </p>

<p>However, for lag 1 everything is great:</p>

<pre><code>    Box.test(resid(fit1),type=""Ljung"",lag=1,fitdf=1)
</code></pre>

<p>gives me the result: </p>

<pre><code>    X-squared = 0.3512, df = 0, p-value &lt; 2.2e-16
</code></pre>

<p>Either I am not understanding the test, or it is slightly contradicting to what I see on the acf plot. The autocorrelation is laughably low. </p>

<p>Then I checked fit2. The autocorrelation function looks like this:</p>

<p><img src=""http://i.stack.imgur.com/JZ7Sc.png"" alt=""acf fit2""></p>

<p>Despite such obvious autocorrelation at several first lags, the Ljung-Box test gave me much better results at 20 lags, than fit1:</p>

<pre><code>    Box.test(resid(fit2),type=""Ljung"",lag=20,fitdf=0)
</code></pre>

<p>results in :</p>

<pre><code>    X-squared = 147.4062, df = 20, p-value &lt; 2.2e-16
</code></pre>

<p>whereas just checking autocorrelation at lag1, also gives me the confirmation of the null-hypothesis! </p>

<pre><code>    Box.test(resid(arima2.fit),type=""Ljung"",lag=1,fitdf=0)
    X-squared = 30.8958, df = 1, p-value = 2.723e-08 
</code></pre>

<p>Am I understanding the test correctly? The p-value should be preferrably smaller than 0.05 in order to confirm the null hypothesis of residuals independence. Which fit is better to use for forecasting, fit1 or fit2? </p>

<p>Additional info: residuals of fit1 display normal distribution, those of fit2 do not.  </p>
"
"0.127589457900886","0.113247870429209"," 65064","<p>I'm using R forecast package with a daily time series data, that has complex i.e. Multiple seasonality (weekly, Yearly, monthly). The fit/forecast process also needs to take into account certain day specific effects.</p>

<p>I plan to:</p>

<ul>
<li>Use auto.arima function </li>
<li>Set TS frequency to 7, to take care of the weekly seasonality</li>
<li>Use Fourier terms for 'xreg' parameter to take care of monthly and yearly seasonality</li>
<li><p>Use a regressor matrix for other effects e.g. Holidays.</p>

<ol>
<li><p>Can someone help in providing a concrete example on how to use the fourier function, which can take both monthly and yearly seasonality into account?</p></li>
<li><p>Would like to see an example of how to combine a regressor matrix with the fourier result, so that it can be assigned to 'xreg' parameter or together?</p></li>
</ol></li>
</ul>

<p>On both these questions, I have only found possibilities of the above mentioned by Dr.Hyndman, but concrete examples can really be useful to the community as well.</p>

<p>Thanks.</p>
"
"0.107832773203438","0.111663906120888"," 65585","<p>I have a daily weather data set, which has, unsurprisingly, very strong seasonal effect.</p>

<p><img src=""http://i.stack.imgur.com/B5Zpo.jpg"" alt=""enter image description here""></p>

<p>I adapted an ARIMA model to this data set using the function auto.arima from forecast package.
To my surprise the function does not apply any seasonal operations- seasonal differencing, seasonal ar or ma components. Here is the model it estimated:</p>

<pre><code>library(forecast)
data&lt;-ts(data,frequency=365)
auto.arima(Berlin)

Series: data
ARIMA(3,0,1) with non-zero mean 

Coefficients:
         ar1      ar2     ar3      ma1  intercept
      1.7722  -0.9166  0.1412  -0.8487   283.0378
s.e.  0.0260   0.0326  0.0177   0.0214     1.7990

sigma^2 estimated as 5.56:  log likelihood=-8313.74
AIC=16639.49   AICc=16639.51   BIC=16676.7
</code></pre>

<p>And also the forecasts using this model are not really satisfying. Here is the plot of the forecast:
<img src=""http://i.stack.imgur.com/IkpIq.jpg"" alt=""enter image description here""></p>

<p>Can anyone give me a hint what is wrong here?</p>
"
"0.136398867894095","0.105933682731967"," 66369","<p>I've found two definitions in the literature for the autocorrelation time of a weakly stationary time series:</p>

<p>$$
\tau_a = 1+2\sum_{k=1}^\infty \rho_k \quad \text{versus} \quad \tau_b = 1+2\sum_{k=1}^\infty \left|\rho_k\right|
$$</p>

<p>where $\rho_k = \frac{\text{Cov}[X_t,X_{t+h}]}{\text{Var}[X_t]}$ is the autocorrelation at lag $k$.  </p>

<p>One application of the autocorrelation time is to find the ""effective sample size"": if you have $n$ observations of a time series, and you know its autocorrelation time $\tau$, then you can pretend that you have</p>

<p>$$
n_\text{eff} = \frac{n}{\tau}
$$</p>

<p>independent samples instead of $n$ correlated ones for the purposes of finding the mean.  Estimating $\tau$ from data is non-trivial, but there are a few ways of doing it (see <a href=""http://arxiv.org/abs/1011.0175"">Thompson 2010</a>).</p>

<p>The definition without absolute values, $\tau_a$, seems more common in the literature; but it admits the possibility of $\tau_a&lt;1$.  Using R and the ""coda"" package:</p>

<pre><code>require(coda)
ts.uncorr &lt;- arima.sim(model=list(),n=10000)         # white noise 
ts.corr &lt;- arima.sim(model=list(ar=-0.5),n=10000)    # AR(1)
effectiveSize(ts.uncorr)                             # Sanity check
    # result should be close to 10000
effectiveSize(ts.corr)
    # result is in the neighborhood of 30000... ???
</code></pre>

<p>The ""effectiveSize"" function in ""coda"" uses a definition of the autocorrelation time equivalent to $\tau_a$, above.  There are some other R packages out there that compute effective sample size or autocorrelation time, and all the ones I've tried give results consistent with this:  that an AR(1) process with a negative AR coefficient has <em>more</em> effective samples than the correlated time series.  This seems strange.  </p>

<p>Obviously, this can never happen in the $\tau_b$ definition of autocorrelation time.</p>

<p>What is the correct definition of autocorrelation time?  Is there something wrong with my understanding of effective sample sizes?  The $n_\text{eff} &gt; n$ result shown above seems like it must be wrong... what's going on?</p>
"
"0.0964485644340824","0.0998752338877845"," 66927","<p>I need to take the output parameters from an ARIMA model fitted in R from the following set (1,0,1), (0,1,0), (1,1,0), (0,1,1), (1,1,1) of models and implement the prediction function in C. I DO NOT HAVE THE OPTION of calling predict or any other R package for that step. </p>

<p>Obviously, I can eventually track down all the source code in predict and figure it out. But I was hoping there is somewhere that will walk me through a simple example of how to map the various output parameters of Arima() with X, Y, a, b, E, t (no upper and lower case thetas and B^t's) since every paper loves to include those already. </p>

<p>I think this request is slightly duplicative except in previous versions the question was retired without an answer or a link.</p>

<p>UPDATE: So, first, I HIGHLY second all recommendations for <a href=""http://otexts.com/fpp/"" rel=""nofollow"">Forecasting: Principles and Practice by Hyndman&amp;Athanasopoulos</a>. </p>

<p>I think what I've been missing is that ""d"" isn't a model parameter -- it changes what is being modeled. So while I'm not all the way to where I want to be, I'm starting to be able to write predictive equations based on R output. I will update with my eventual findings if nobody else posts something better. </p>
"
"0.152498570332605","0.142124949417345"," 68131","<p>Just asking if someone knows why the prediction intervals are quite different when one uses a time series analytic method of estimation <em>versus</em> when one simulates such time series. </p>

<p>For example, I used the forecast package's <code>auto.arima</code> function to get the best fit to my data, say it was an ARIMA(1,1,1), and then, on the one hand, I simulated such process doing around 10 thousand simulations and then calculating the 95% percentile with ""quantile"" function, and on the other hand, I used R's <code>forecast</code> package to do it. So I realized that these different approaches gave prediction intervals with different width (actually, those related with simulation approach are closer than those obtained with forecast package). The way I simulated such time series process is simulating the parameters as random variables distributed normally with mean equal to its estimated value and standard deviation equal to its related standard error. The ""white noise"" variables related with the Moving Average (MA) part of the process were simulated as normally distributed with mean zero and variance equal to the variance of the residuals.</p>

<p>Thanks in advance for your help.</p>
"
"0.107832773203438","0.111663906120888"," 68309","<p>I am using R to develop an ARIMA model to evaluate the influence of several seasonal covariates (e.g., meteorological data) upon the incidence of a seasonal disease. I have weekly data available and have set the period equal to 52 weeks. Using the <code>auto.arima</code> function the disease of interest has a form of <code>ARIMA(0,1,2)(0,0,1)</code>.</p>

<p>Using the <code>TSA</code> package, I can then evaluate the influence of each of the covariates:</p>

<pre><code>covariates &lt;- data.frame(covariate1, covariate2, covariate3)
model &lt;- arima(disease, order=c(0,1,2), seasonal=list(order=c(0,0,1), period=52), xreg=covariates)
</code></pre>

<p>However, I am worried that this approach is identifying spurious associations between the covariates, each of which has a seasonal component...  Is it more appropriate to decompose the covariate data and subtract the seasonal component before fitting the ARIMA model?</p>

<pre><code>covariate1_components &lt;- decompose(covariate1)
covariate1_adjust &lt;- covariate1 - covariate1_components$seasonal
[...]
covariates_adjust &lt;- data.frame(covariate1_adjust, covariate2_adjust, covariate3_adjust)
model2 &lt;- arima(disease, order=c(0,1,2), seasonal=list(order=c(0,0,1), period=52), xreg=covariates_adjust)
</code></pre>

<p>Any thoughts on which of the two approaches would be preferable for evaluating seasonal covariates?</p>
"
"0.0556846046389705","0.0864944897557338"," 68387","<p>I asked a question earlier in the forum on auto arima click here <a href=""http://stats.stackexchange.com/questions/68261/performance-evaluation-of-auto-arima-in-r-and-ucm-on-one-dataset"">Performance evaluation of auto.arima in R and UCM on one dataset</a>. The auto.arima provided strange forecast, upon further looking at the code I did not find anything wrong in my R code see code below. This seems a very straightforward problem. If auto.arima does not fit a simple straightforward dataset, I would be very cautious in using this function to fit more complicated datasets. I would encourage using other tools/ functions and verify forecast. </p>

<pre><code>plot(eggs)

## Hold out 10 data points - 1984 thru 1993
egg_price = ts(eggs,start = 1900, end = 1983)

## Fit arima model

fit &lt;- auto.arima(egg_price)
fcast &lt;- forecast(fit,h=10)
plot(fcast)
</code></pre>
"
"0.0835269069584557","0.0864944897557338"," 69405","<p>I am fitting a model using the <code>auto.arima</code> function in package <code>forecast</code>. I get a model that is AR(1), for example. I then extract residuals from this model. How does this generate the same number of residuals as the original vector? If this is an AR(1) model then the number of residuals should be 1 less than the dimensionality of the original time series. What am I missing?</p>

<p>Example:</p>

<pre><code>require(forecast)
arprocess = as.numeric(arima.sim(model = list(ar=.5), n=100))
#auto.arima(arprocess, d=0, D=0, ic=""bic"", stationary=T)
#  Series: arprocess 
#  ARIMA(1,0,0) with zero mean     

#  Coefficients:
#          ar1
#       0.5198
# s.e.  0.0867

# sigma^2 estimated as 1.403:  log likelihood=-158.99
# AIC=321.97   AICc=322.1   BIC=327.18
r = resid(auto.arima(arprocess, d=0, D=0, ic=""bic"", stationary=T))
&gt; length(r)
  [1] 100
</code></pre>

<p>Update: Digging into the code of auto.arima, I see that it uses Arima which in turn uses <code>stats:::arima</code>. Therefore the question is really how does <code>stats:::arima</code> compute residuals for the very first observation?</p>
"
"0.167550260223602","0.186849453316002"," 76761","<p><strong>Problem:</strong> I would like to extract the BIC and AICc from an arima() object in R.</p>

<p><strong>Background:</strong> The arima() function produces an output of results, which includes the estimated coefficients, standard errors, AIC, BIC, and AICc. Let's run some sample code to see what this looks like:</p>

<pre><code># Load the sunspots dataset
data(sunspots)
# Build an ARIMA(2,0,2) model and store as an object
model &lt;- arima(x=sunspots, order=c(2,0,2), method=""ML"")
# Show a summary of the model
model 
</code></pre>

<p>The output of results for the model appears like this:</p>

<pre><code>Series: sunspots 
ARIMA(2,0,2) with non-zero mean 

Coefficients:
         ar1     ar2      ma1      ma2  intercept
      0.9822  0.0004  -0.3997  -0.1135    51.2652
s.e.  0.1221  0.1196   0.1206   0.0574     8.1441

sigma^2 estimated as 247.9:  log likelihood=-11775.69
AIC=23563.39   AICc=23563.42   BIC=23599.05
</code></pre>

<p>On the bottom line, we can see values for AIC, BIC, and AICc. (Note: this is the output shown by arima() when the forecast package has been loaded, i.e. library(forecast))</p>

<p>Accessing the AIC value is quite easy. One can simply type:</p>

<pre><code>&gt; model$aic
[1] 23563.39
</code></pre>

<p>Access to the AIC value in this manner is made possible due to the fact that it's stored as one of the model's attributes. The following code and output will make this clear:</p>

<pre><code>&gt; attributes(model)
$names
 [1] ""coef""      ""sigma2""    ""var.coef""  ""mask""      ""loglik""   
 [6] ""aic""       ""arma""      ""residuals"" ""call""      ""series""   
[11] ""code""      ""n.cond""    ""model""    

$class
[1] ""Arima""
</code></pre>

<p>Notice, however, that bic and aicc are not model attributes, so the following code is no use to us:</p>

<pre><code>&gt; model$bic
NULL
&gt; model$aicc
NULL
</code></pre>

<p>The BIC and AICc values are, indeed, calculated by the arima() function, but the object that it returns does not give us direct access to their values. This is inconvenient and I've come across others who've raised the issue. Unfortunately, I've not found a solution to the problem.</p>

<p>Can anyone out there help? Which method can I use to access the BIC and AICc from the Arima class of object.</p>

<p><strong>Note:</strong> I've suggested an answer below, but would like to hear improvements and suggestions.</p>

<p>Edit (Version details as requested):</p>

<pre><code>&gt; R.Version()
$platform
[1] ""i686-pc-linux-gnu""

$arch
[1] ""i686""

$os
[1] ""linux-gnu""

$system
[1] ""i686, linux-gnu""

$status
[1] """"

$major
[1] ""3""

$minor
[1] ""0.2""

$year
[1] ""2013""

$month
[1] ""09""

$day
[1] ""25""

$`svn rev`
[1] ""63987""

$language
[1] ""R""

$version.string
[1] ""R version 3.0.2 (2013-09-25)""

$nickname
[1] ""Frisbee Sailing""
</code></pre>
"
"0.0482242822170412","0.0499376169438922"," 81414","<p>After fitting my time series with an ARIMA model, I want to test outliers in the residuals' series. Are there any functions in R that could do this test and furtherly test whether the outlier is additive or innovational, seasonal or just one pulse?</p>
"
"0.0340997169735237","0.0706224551546449"," 83433","<p>I would like to ask how the long-term (multiple step ahead) prediction intervals are calculated by function <code>predict.Arima</code> in R. I am particularly interested in ARIMA models, SARIMA models and in ARIMA models with external regressors (include argument xreg => regression with ARIMA errors) </p>
"
"0.210204772808868","0.217673025739671"," 83868","<p>There are two simple questions at the end, but I think it is also useful to share the background that motivated them. It comes from <a href=""http://stats.stackexchange.com/questions/31073/flat-ets-forecast-of-clearly-increasing-time-series"">this question</a> on an unexpected forecast from the fully automatic methodology behind the forecast::ets function in R. The code, plot and forecast are given below for convenience:</p>

<pre><code>library(forecast);options(scipen=999)
usage &lt;- ts(scan('http://cl.ly/102L0j3o1p2m0m3p0t2o/usage'), frequency = 24)

plot(f1&lt;-forecast(m1&lt;-HoltWinters(usage), h = 168))
plot(f2&lt;-forecast(m2&lt;-ets(usage), h = 168));AIC(m2) #replication OK
</code></pre>

<p><img src=""http://i.stack.imgur.com/CJ4N3.png"" alt=""enter image description here""></p>

<pre><code>plot(f3&lt;-forecast(m3&lt;-ets(usage,additive.only=TRUE), h = 168))
plot(f4&lt;-forecast(m4&lt;-ets(usage,additive.only=TRUE,damped=FALSE), h = 168))
</code></pre>

<p>Just by looking at the plot of the data it appears to me immediately that ARIMA(0,2,2) or 
ETS(AAN) will be among the best non-seasonal models (and their point forecasts will not differ much). Following the usual advice that the AIC of only a small set of potentially useful models should be compared, I can see no reason to consider multiplicative models here, nor can I see how a damped forecast will be useful for me. With this information in, the ""best"" ets model m4 and its associated forecast f4 is what was expected, but the 
process is not a fully automatic one.</p>

<p>With many time series, which I would not have time or desire to look at, I would hardly have a better option than to blindly use ets(data). The ets documentation page assures me that <em>""The methodology is fully automatic. The only required argument for ets is the time series. The model is chosen automatically if not specified""</em>. The above example is not the first one to show that the methodology that works is at best semi-automatic and the fully automatic one may fail to work as expected even in simpler cases. </p>

<p>One way to rectify the problem is to consider n $\ge$ 1 models with suitably low values of AIC as equally supported by the data. I agree with <a href=""http://stats.stackexchange.com/questions/31073/flat-ets-forecast-of-clearly-increasing-time-series"">RJH</a> (<em>""Although the model may not give the best AIC, it may give forecasts that are more useful to you.""</em>) that the most useful forecasts need not be those from the model with the smallest value of the AIC. But then we have the question of when the AIC is useful for selecting a model that forecasts best. It can be deduced from, for example, <a href=""http://stats.stackexchange.com/questions/78949/when-is-it-appropriate-to-select-models-by-minimising-the-aic"">quotes in this question</a> that if the AICs of all models considered differ by no more than some small number c, then the AIC loses its power to distinguish between these models and <a href=""http://stats.stackexchange.com/questions/81552/what-do-i-do-when-values-of-aic-are-low-but-approximately-equal"">other factors/ideas</a> must be considered.  But how small is c?</p>

<p>Those working with Akaike (Y Sakamoto and M Ishiguro and G. Kitagawa) in the book entitled ""Akaike Information Criterion statistics"" (in the section entitled ""Some remarks on the use of the AIC"") mentioned c=1. The number c=2 is often mentioned (e.g. a quote from Brian Ripley can be added to those two linked to above). The number c=4 was mentioned by Chris Chatfield in one of his books. I have not seen anything explicit on the values of c greater than four, but this probably depends on the variability of data, sampling error of the deviance and related factors. </p>

<p>In the above example, AIC(m3,m4) suggests that the AIC of the model with a more useful forecast is greater than the AIC of the other model by ""only"" 78348.75-78337.22=11.52. Are there any formulae, guidelines or rules of thumb for useful values of c given data? Have values of c greater than four been mentioned in the literature? </p>
"
"0.119349009407333","0.14124491030929"," 90304","<p>I perform some time series fitting with the help of the forecast and urca packages. I have a question regarding the correspondance between results coming from statistical test such as KPSS, ADF or ERS, and the results arising from automatical selection procedures such as auto.arima from the forecast package. </p>

<p>Suppose we are in a situation where the outcome of the KPSS test is that the series does not need to be differentiated. I then apply the ADF test, which shows that the series is stationnary up to a trend (at+b).</p>

<p>In that situation, I could force the auto.arima function to consider only ARIMA models that include a linear trend, but I can also explore all the models (including or not the trend) and let auto.arima choose the best one. And there are cases where the outcome is in contradiction with the test result, e.g. the chosen model does not include a trend while the test advises to include one.</p>

<p>What is the best practice in such a case ? Should the test result be prevalent to the selection of the ARIMA model, or is it best to consider all possible models and choose the best one ?</p>

<p>Thanks.</p>
"
"0.118124884643724","0.081547786988733"," 91675","<p>I have been looking for a function that can make recursive window out-of-sample forecasts, but seems there is none. So I'm thinking about about making a function that can be used for recursive window forecasting in an ARIMA model. However I know little about programming, so I'm seeking for help.</p>

<p>What I want to do is use the function <code>forecast.Arima</code> (<strong>forecast</strong> package) to predict future values in a expanding window. Suppose 20 years is the initial window, and I expand the window by 1 year on each iteration until it is of size  30 years. More specifically, use 20 years data to predict one value, use 21 years data to predict the next value, etc.</p>
"
"0.118124884643724","0.101934733735916"," 95709","<p>I am fitting a regression with ARMA errors using the base R function <code>arima()</code> and the <code>Arima()</code> function from the <code>forecast</code> package.  </p>

<p>The estimated coefficients from both are identical. My problem comes from using <code>arima.errors()</code> on these two models, and using <code>tsdisplay()</code> to view these structural residuals (that is, the residuals straight from the regression, before any ARMA model is fit on them). These ARMA errors (and their corresponding ACFs, PACFs) are different between the two, and I don't know why. Even more curious is that the final residuals from both are in fact the same, which would make me think the structural residuals would have to be the same. I have put a MWE below.</p>

<pre><code>library('forecast')
data(usconsumption, package='fpp')

fit1 = arima(usconsumption[ ,1], xreg=usconsumption[ ,2], order=c(2,0,0))
tsdisplay(arima.errors(fit1), main=""ARIMA errors, arima function"") # not the same as the other


fit2 = Arima(usconsumption[,1], xreg=usconsumption[,2], order=c(2,0,0))
dev.new()
tsdisplay(arima.errors(fit2), main=""ARIMA errors, Arima function"") # not the same as the other


View(cbind(resid(fit1), resid(fit2))) # final residuals are the same
</code></pre>

<p>Note this example is from <a href=""https://www.otexts.org/fpp/9/1"" rel=""nofollow"">https://www.otexts.org/fpp/9/1</a></p>
"
"0.0835269069584557","0.0864944897557338"," 97417","<p>I'm trying to understand whether it is possible to fit a threshold AR model with a GARCH effect using R. So far, I only found the functions <code>garchFit</code> (which also allows to ""merge"" ARIMA and GARCH models) and <code>tar</code>. In my book (by Tsay) I found the code to fit such models on RATS, but I'm asked to use R. If that is impossible, I'd ask to suggest a solution to bypass this problem. 
I'm studying the continuously compounded log-returns of S&amp;P500 index, and I successfully (at least from the tests!) fitted a GARCH model already. This because the series showed heteroskedasticity in the variance. But the plot also shows some leverage effect, so I thought that a TAR (better than a TGARCH) could explain this phenomenon.<br>
Is it reasonable to try merging those two models? Should I leave the GARCH?
Thanks.</p>
"
"0.192897128868165","0.199750467775569","104558","<p>I am really new to R and to time series. My field of studies is in the field of Networks and Telecommunication, but my summer internship is about trying to find a statistical model for some sets of data.</p>

<p>The data consists of what is called ""10-minutes-points"", recorded over a year and which represent power consuption of a source substation. It means I have 6 * 24 * 365 = 52 560 points of data to process, one set for each source substation.</p>

<p>It's been about a week I'm trying to found information about ARIMA models. <a href=""https://www.otexts.org/fpp/8/9"" rel=""nofollow"">This website</a> and the report of my predecessor quite helped me getting in the subject, by I still encountered many problems.</p>

<p>I found one might be due to the large size of the data set <a href=""http://stats.stackexchange.com/questions/27313/how-would-you-fit-arima-model-with-lots-of-autocorrelations"">as explained here</a>, the second one to the existence of exogenous data as <a href=""http://stats.stackexchange.com/questions/25780/what-is-the-purpose-of-and-how-to-use-the-xreg-argument-when-fitting-arima-model"">mentioned there</a>.</p>

<p>My predecessor found the ARIMA model to be effective for short term predictions (up to 20-ish hours), and the SARIMAX for mid-term predictions (around a dozen days). I guess it is cause exogeneous data doesn't affect as much the core data on such short periods of time.</p>

<p>I found <a href=""http://stats.stackexchange.com/questions/18375/how-to-fit-an-arimax-model-with-r"">this thread</a> to be very interesting but I'm not sure I understand everything.</p>

<p>In a first time I would like to know if my understanding of the general method to evaluate a model is correct :</p>

<ol>
<li><p>first you plot your data and try to look for any trend/seasonality (the data I have showed to have a daily seasonality and a yearly one)</p></li>
<li><p>you use log in order to reduce the trend, and maybe differentiate to eliminate the seasonality (so I should use something like : <code>diff(data.ts, 144)</code> in my case to get rid of the daily seasonality (6*24 points a day) ?)</p></li>
<li><p>plot the acf/pcf of the differentiated time series and try to estimate a model from there</p></li>
<li><p>try to fit the model to my data with <code>fit &lt;- Arima(data, order=c(p,d,q), seasonal=c(P,D,Q))</code> but I don't where the seasonality (144) would appear in this function ?</p></li>
<li><p>study the residuals of fit to see if the model is correct (looking at the acf/pacf)</p></li>
<li><p>use fitted or forecast (I don't know which one is better) to predict future values</p></li>
</ol>

<p>Thing is, since the data set is huge, I always get significant spikes at many lags in the acf/pacf and I don't feel I can judge if a model is correct or not.</p>

<p>Here is an example :</p>

<p><code>data = scan(""auch.txt"", skip=1)
plot.ts(data)</code></p>

<p><img src=""http://i.stack.imgur.com/weVCX.png"" alt=""Data""></p>

<p><code>data.ts = ts(log(data)
data.diff = diff(data.ts, 144)
plot.ts(data.diff)</code></p>

<p><img src=""http://i.stack.imgur.com/Ck7mu.png"" alt=""Datadiff""></p>

<p>Which seems somehow stationary to me. I then proceed to look at the acf/pacf, and had to differentiate once more because it wasn't stationary in fact :</p>

<p><code>tsdisplay(data.diff, lag.max=150)
tsdisplay(diff(data.diff), lag.max=150)</code></p>

<p><img src=""http://i.stack.imgur.com/dgqG6.png"" alt=""Tsdisplay"">
<img src=""http://i.stack.imgur.com/P7Kj7.png"" alt=""Tsdisplaydiff""></p>

<p>And I really don't know how to handle these results, so I hoped I could find some help here, because I came across the website a lot during my researchs.</p>

<p>Thanks in advance, and I apologies for any grammatical mistakes or vocabulary error ; English is not my native language.</p>

<p><strong>Edit :</strong> does anyone know why my pictures won't appear ?</p>

<p><strong>Edit bis :</strong> nvm in fact it might be me, because imgur is blocked on my work computer</p>
"
"0.159941849898134","0.165624338327385","105273","<p>I have a dataset depicting weekly revenue over time for a computer company. The plot for the data looks like this:</p>

<p><img src=""http://i.stack.imgur.com/DDoy8.png"" alt=""enter image description here""></p>

<p>I decomposed the data into its additive components using the <code>decompose</code> function in R and plotted the various components:</p>

<p><img src=""http://i.stack.imgur.com/4HbER.png"" alt=""enter image description here""></p>

<p>Next I tried removing the seasonal component using the following code:</p>

<pre><code>&gt; RevenueDec &lt;- decompose(Revenue)
&gt; RevenueSeasonallyAdjusted &lt;- Revenue  - RevenueDec$seasonal
</code></pre>

<p>However, I still get a seasonal component when I decompose 'RevenueSeasonallyAdjusted':</p>

<p><img src=""http://i.stack.imgur.com/CC9V9.png"" alt=""enter image description here""></p>

<p>The y-axis has very small values but the seasonality exists nonetheless.</p>

<p>Could you help me out here.</p>

<p>EDIT: In the next step I tried using the <code>auto.arima</code> function on my seasonally adjusted data to get a forecast and got a plot like this:</p>

<p><img src=""http://i.stack.imgur.com/1BPbL.png"" alt=""enter image description here""></p>

<p>Is this the correct approach to use, or should I try something different?</p>
"
"NaN","NaN","105475","<p>Is there a way to estimate an ARMA equation using the <code>lm()</code> function in R without using <code>arima()</code>?</p>
"
"0.107832773203438","0.111663906120888","106038","<p>I know that this is probably a question that's been asked plenty of times, but i haven't seen an answer that's both accurate and simple. How do you estimate the appropriate forecast model for a time series by visual inspection of the ACF and PACF plots? Which one, ACF or PACF, tells the AR or the MA (or do they both?) Which part of the graphs tell you the seasonal and non seasonal part for a seasonal ARIMA?</p>

<p>Take for instance these functions:</p>

<p><img src=""https://i.imgur.com/E64Sd7p.png"" alt=""enter image description here""></p>

<p>They show the ACF and PCF of a log transformed series that's been differenced twice, one simple difference and one seasonal.</p>

<p>How would you caracterize it? What model best fits it?</p>

<p>Thanks in advance!</p>

<p><strong>EDIT:</strong> Added raw data</p>

<p>Original data: <a href=""http://pastebin.com/KRJnXzXp"">here</a></p>

<p>Log transformed data: <a href=""http://pastebin.com/JR3bkctv"">here</a></p>

<p><strong>EDIT:</strong> Corrected ACF and PACF functions (previous ones were overdifferentiated)</p>
"
"NaN","NaN","108111","<p>I am studying intervention analysis in time series with the Cryer and Chan <a href=""http://homepage.stat.uiowa.edu/~kchan/TSA.htm"" rel=""nofollow"">book</a> and am looking at trying to understand how to code the step response interventions. One question I had is how to differentiate between these two models:</p>

<p><img src=""http://i.stack.imgur.com/t8MHe.jpg"" alt=""enter image description here""></p>

<p>It appears the only difference is in (c) the value of $\delta$ is constrained to be 1. How can this constraint be added to the arimax function? I believe that (b) uses the coding</p>

<p><code>transfer = list(c(1,0))</code></p>

<p>Is there a way to constrain $\delta$ =1?</p>
"
"NaN","NaN","108309","<p>I am trying to forecast data using ARIMA. I have read articles which say that I should first take the log of the series, if my series follows an exponential trend. </p>

<p>My question is: Is there a mathematical approach to find what kind of trend (linear or exponential) my data is following? </p>

<p>Im working with MATLAB and R. So any builtin functions would be really helpful. </p>
"
"0.167550260223602","0.160156674270859","108374","<p>I have a monthly time series with an intervention and I would like to quantify the effect of this intervention on the outcome. I realize the series is rather short and the effect is not yet concluded.</p>

<p><strong>The Data</strong></p>

<pre><code>  cds&lt;- structure(c(2580L, 2263L, 3679L, 3461L, 3645L, 3716L, 3955L, 
    3362L, 2637L, 2524L, 2084L, 2031L, 2256L, 2401L, 3253L, 2881L, 
    2555L, 2585L, 3015L, 2608L, 3676L, 5763L, 4626L, 3848L, 4523L, 
    4186L, 4070L, 4000L, 3498L), .Dim = c(29L, 1L), .Dimnames = list(
        NULL, ""CD""), .Tsp = c(2012, 2014.33333333333, 12), class = ""ts"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/lNOEk.jpg"" alt=""enter image description here""></p>

<p><strong>The methodology</strong></p>

<p>1) The pre-intervention series (up until October 2013) was used with the <code>auto.arima</code> function. The model suggested was ARIMA(1,0,0) with non-zero mean. The ACF plot looked good.</p>

<pre><code>pre&lt;-window(cds,start = c(2012,01), end=c(2013,09))

mod.pre&lt;-auto.arima(log(pre))

Coefficients:
         ar1  intercept
      0.5821     7.9652
s.e.  0.1763     0.0810

sigma^2 estimated as 0.02709:  log likelihood=7.89
AIC=-9.77   AICc=-8.36   BIC=-6.64
</code></pre>

<p>2) Given the plot of the full series, the pulse response was chosen below, with T = Oct 2013,</p>

<p><img src=""http://i.stack.imgur.com/YU3nB.jpg"" alt=""enter image description here""></p>

<p>which according to cryer and chan can be fit as follows with the arimax function:</p>

<pre><code>   mod.arimax&lt;-arimax(log(cds), order=c(1,0,0), seasonal=list(order=c(0,0,0),frequency=12), include.mean = TRUE,       
            xtransf=data.frame(Oct13=1*(seq(cds)==22)),
            transfer=list(c(1,1))
          )

    mod.arimax


Series: log(cds) 
ARIMA(1,0,0) with non-zero mean 

Coefficients:
         ar1  intercept  Oct13-AR1  Oct13-MA0  Oct13-MA1
      0.7619     8.0345    -0.4429     0.4261     0.3567
s.e.  0.1206     0.1090     0.3993     0.1340     0.1557

sigma^2 estimated as 0.02289:  log likelihood=12.71
AIC=-15.42   AICc=-11.61   BIC=-7.22
</code></pre>

<p>The residuals from this appeared OK:</p>

<p><img src=""http://i.stack.imgur.com/wvdXD.jpg"" alt=""enter image description here""></p>

<p>The plot of fitted and actuals:</p>

<pre><code>plot(fitted(mod.arimax),col=""red"", type=""b"")
lines(window(log(cds),start=c(2012,02)),type=""b"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/kJ1pj.jpg"" alt=""enter image description here""></p>

<p><strong>The Questions</strong></p>

<p>1) Is this methodology correct for intervention analysis?</p>

<p>2) Can I look at estimate/SE for the components of the transfer function and say that the effect of the intervention was significant?</p>

<p>3) How can one visualize the transfer function effect (plot it?)</p>

<p>4) Is there a way to estimate how much the intervention increased the output after 'x' months? I guess for this (and maybe #3) I am asking how to work with an equation of the model - if this were simple linear regression with dummy variables (for example) I could run scenarios with and without the intervention and measure the impact - but I am just unsure how to work this this type of model.</p>

<p><strong>ADD</strong></p>

<p>Per request, here are the residuals from the two parametrizations.</p>

<p>First from the fit:</p>

<pre><code>fit &lt;- arimax(log(cds), order = c(1,0,0), 
              xtransf = data.frame(Oct13a = 1*(seq_along(cds)==22), Oct13b = 1*(seq_along(cds)==22)),
              transfer = list(c(0,0), c(1,0)))

plot(resid(fit), type=""b"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/sqMZN.jpg"" alt=""enter image description here""></p>

<p>Then, from this fit</p>

<pre><code>mod.arimax&lt;-arimax(log(cds), order=c(1,0,0), seasonal=list(order=c(0,0,0),frequency=12), include.mean = TRUE,       
                   xtransf=data.frame(Oct13=1*(seq(cds)==22)),
                   transfer=list(c(1,1))
)

mod.arimax
plot(resid(mod.arimax), type=""b"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/DjAyu.jpg"" alt=""enter image description here""></p>
"
"0.181865157192126","0.176556137886612","108495","<p>I am trying to calculate the average of hourly data of three sensors but the hourly timestamps of all three sensors are different. How is it possible to measure the average of hourly data of all three sensors at a regular interval of hours say- 01:00,02:00,03:00 and so on.</p>

<p>And also is it possible to start an extensible time series with a specific time which is before the first timestamp of the given dataset so that the extra timestamps have NA values and i can use na.rm=TRUE while calculating mean.:</p>

<pre><code>  library(xts)
 library(forecast)
 data1$V1&lt;-as.POSIXct(paste(data1$V1, data1$V2), format=""%Y-%m-%d %H:%M:%S"") 
     sensid&lt;-as.factor(data1$V4)

 #For Temp

 for(i in 1:length(levels(sensid)))
  {
   l[[i]]=data.frame(data1$V6[data1$V4==i])
   l[[i]]&lt;-xts(l[[i]],order.by=data1$V1[data1$V4==i])
   if(length(which (is.na(l[[i]])))&lt;length(l[[i]]))
     {      
      wAvgt[[i]]&lt;-apply.weekly(l[[i]], function(x) sapply(x,mean,na.rm=TRUE)) #Weekly   Average of Temp
      dAvgt[[i]]&lt;- apply.daily(l[[i]], function(x) sapply(x,mean,na.rm=TRUE)) #Daily Average of Temp
      hrAvgt[[i]] &lt;- period.apply(l[[i]], endpoints(l[[i]], on = ""hours"", 1), function(x) sapply(x,mean,na.rm=TRUE)) #Hourly Average of Temp

     #For forecasting future values
     aritw[[i]]&lt;-auto.arima(wAvgt[[i]]) #Weekly Forecast ARIMA model using ARIMA 
     foretw[[i]]&lt;-forecast.Arima(aritw[[i]],h=5) #Forecast of Weekly Average of Temp

     arith[[i]]&lt;-auto.arima(hrAvgt[[i]]) #Hourly Forecast ARIMA model using ARIMA 
     foreth[[i]]&lt;-forecast.Arima(arith[[i]],h=50)#Forecast of Hourly Average of Temp

     aritd[[i]]&lt;-auto.arima(dAvgt[[i]]) #Daily Forecast ARIMA model using ARIMA 
     foretd[[i]]&lt;-forecast.Arima(aritd[[i]],h=10)#Forecast of Daily Average of Temp
    }
else
    {
     next
    }
 }
</code></pre>

<p>I have 27 sensors data. so i m trying to caclulate hourly,weekly and daily average of each sensors but each sensor provides 4 critical parameters like - blood pressure, temperature, Pulse and WBC. The above loop provided is only for temperature of 27 sensors. I have a bed connected to some 3 sensors and each sensor has 4 critical parameters to measure. I m concerned only with temperature for now. I need to calculate hourly average of temp for that bed only. So, initially i measure hourly average of any 3 given sensors each and then i intend to take the hourly average between these 3 sensors.</p>

<p>The problem i m facing here is each hourly average has different number of rows and different timestamps. How to normalise all the timestamps?</p>

<p>If the starting timestamp of sensor 1 is 2004-02-28 00:59:16, sensor 2 is 2004-02-28 01:08:23, sensor 3 is 2004-02-28 01:19:34, it is not possible to normalise them by any methods i know of yet.</p>

<p>Please help me find the hourly average of three sensors.</p>
"
"0.0681994339470473","0.0706224551546449","109594","<p>I can't seem to find much info on the following: I have a dataset <em>D</em> at time <em>t</em> which I use to fit an ARIMA model. I forecast the value of the time series at time <em>t</em> + 1. Now, when I'm in <em>t</em> + 1,  I would like to predict the value of my time series at <em>t</em> + 2 using data up until time <em>t</em> + 1. However, I don't want to refit the whole model. Is there a function in R which does this? I would have thought this is a common thing to do with time series analysis. </p>
"
"0.210468022470956","0.207048346845277","109835","<p>While working on a big data set made of 10-minutes-points of information - i.e. <code>144</code> points per day, <code>1008</code> per week and <code>52560</code> per year - I encountered a few problem in R. The information concerns electricity load on a source substation during the year.</p>

<h3>Multiple seasonality :</h3>

<p>The data set clearly shows multiple seasonalities, which are daily, weekly and yearly. From <a href=""http://stats.stackexchange.com/questions/47729/two-seasonal-periods-in-arima-using-r"">there</a> I understood that R doesn't handle multiple seasonality within the ARIMA modeling functions.  I would really like to work with ARIMA models though, because my previous work is based on ARIMA models and I know approximatively how to translate a model into an equation.  </p>

<h3>Long seasonality :</h3>

<p>Each of the seasonalities is of high value, with the shortest one being the daily seasonality at 144. Unfortunately from the SARIMA general equation which is<br>
$\phi(B)\Phi(B^s)W_t = \theta(B)\Theta(B^s)Z_t$<br>
I guessed that the maximum lag for a given model <code>SARIMA(p,d,q)(P,D,Q)144</code> is<br>
$max((p+P*144), (q+Q*144))$</p>

<p>I would really like to try and fit models with values of P and/or Q greater than 1, but R doesn't allow me since the <code>maximum supported lag = 350</code>. To do so I found <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">this link</a> which is really interesting and led to new functions in the forecast package by M. Hyndman, called <code>fourier</code> and <code>fourierf</code> which you can find <a href=""http://www.inside-r.org/packages/cran/forecast/docs/fourier"" rel=""nofollow"">here</a>. But since I am not a specialist in forecasting nor in statistics, I have some difficulties understanding how I can make this work.  </p>

<hr>

<p>The thing is I thing this whole fourier regressors package could help me a lot. From what I understood I could use it to simulate the long-seasonality of my data set, maybe use it to simulate multiple seasonality, and even more it could allow me to introduce exogenous variables - which are the <code>temperature</code> and (<code>public holiday + sundays</code>).<br>
I also tried doing some regression following <a href=""http://robjhyndman.com/hyndsight/forecasting-weekly-data/"" rel=""nofollow"">this example</a> but I couldn't make it work because :</p>

<pre><code>Error in forecast.Arima(bestfit, xreg = fourierf(gas, K = 12, h = 1008)) : 
Number of regressors does not match fitted model
</code></pre>

<p>I really hope somebody can help me get a better understanding of these functions. Thanks.</p>

<p><strong>Edit :</strong> So I tried my best with the fourier example given <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">here</a> but couldn't figure out how it handles the fitting. Here is the code (I copy-pasted M. Hyndman one and adapted to my data set - unsuccessfully) :</p>

<pre><code>n &lt;- 50000
m &lt;- 144
y &lt;- read.table(""auch.txt"", skip=1)
fourier &lt;- function(t,terms,period)
{
  n &lt;- length(t)
  X &lt;- matrix(,nrow=n,ncol=2*terms)
  for(i in 1:terms)
  {
    X[,2*i-1] &lt;- sin(2*pi*i*t/period)
    X[,2*i] &lt;- cos(2*pi*i*t/period)
  }
  colnames(X) &lt;- paste(c(""S"",""C""),rep(1:terms,rep(2,terms)),sep="""")
  return(X)
}

library(forecast)
fit &lt;- Arima(y[1:n,1], order=c(2,1,5), seasonal=c(1,2,8), xreg=cbind(fourier(1:n,4,m),fourier(1:n,4,1008)))
plot(forecast(fit, h=14*m, xreg=cbind(fourier(n+1:(14*m),4,m), fourier(n+1:(14*m),4,1008))))
</code></pre>

<p>So I wanted to ""force"" the model to be a <code>SARIMA(2,1,5)(1,2,8)[144]</code> but when I type <code>arimod</code>this is the result of the Arima fitting :</p>

<pre><code>&gt; fit  
Series: y[1:n, 1] , 
ARIMA(2,1,5)                  

sigma^2 estimated as 696895:  log likelihood=-407290.2  
AIC=814628.3   AICc=814628.3   BIC=814840
</code></pre>

<p>It doesn't even take into consideration the seasonal part of the model, and I don't know much about the range the AIC values can take, but it seems way too high to be a good fitting model right there. I think it all comes down to my misunderstanding of the use of Fourier terms as regressors, but I can't figure out why.</p>

<p><strong>Edit 2 :</strong> Also I can't seem to be able to add another exogenous variable to the Arima function. I need to use <code>temperature</code> - probably as a lead - to fit the <code>SARIMAX</code> model, but as soon as I write this :</p>

<pre><code>fit &lt;- Arima(y[1:n,1], order=c(2,1,5), seasonal=c(1,2,8), xreg=cbind(fourier(1:n,4,m),fourier(1:n,4,1008), tmp[1:n]))
plot(forecast(fit, h=14*m, xreg=cbind(fourier(n+1:(14*m),4,m),fourier(n+1:(14*m),4,1008), tmp[n+1:(14*m)])))
</code></pre>

<p>Nothing is plotted besides the initial data set. There is no forecast while without <code>tmp</code> as an <code>xreg</code> I still get some results.</p>
"
"0.177014545523257","0.194760075661811","110611","<p>Note that this is a simplified example:</p>

<p>I have some time series that I made stationary by differencing twice. Then I ran <code>arima</code> on it, and set d = 0 to prevent additional differencing (I'm aware that <code>auto.arima</code> could detect the order of integration, but I'm hard-coding this myself for other reasons). Now I want to use <code>fitted</code> data from my <code>arima</code> object to determine what the <em>non-stationary</em> fit would look like.</p>

<p>For example:</p>

<pre><code>library('forecast')
# simulate ARIMA(1,2,0) time series:
rawData &lt;- arima.sim(n = 20, list(order = c(1,2,0), ar = 0.7)) 
# use diff function to make the series stationary:
stationaryData &lt;- diff(diff(rawData))

# fit ARIMA on them appropriately
rawDataFit &lt;- arima(rawData, c(1,2,0)) # include.mean = FALSE by default
stationaryDataFit &lt;- arima(stationaryData, c(1,0,0), include.mean = FALSE) # stationaryData is already twice differenced

# notice that there is very small variance between the AR(1) coefficients:
coef(rawDataFit)
coef(stationaryDataFit)
</code></pre>

<p>In this particular instance, my AR(1) coefficeints are 0.5511049 and 0.5511048. I also forced my ARIMA to exclude an intercept, so these ARIMA objects so be similar.</p>

<pre><code># plot of rawData and the fitted values
plot(rawData, type = ""l"")
lines(fitted(rawDataFit), col = ""slategrey"")
</code></pre>

<p>Here's an example of what that plot could look like:
<img src=""http://i.stack.imgur.com/5R0cM.png"" alt=""Here&#39;s an example of what that plot could look like""></p>

<p>I want to recreate the above plot, <em>without</em> using the rawDataFit object</p>

<pre><code># using the diffinv function, I can easily replicate the rawData:
recoveredRawData &lt;- diffinv(stationaryData, differences = 2, xi = rawData[1:2])

# Now I also want to ""recover"" the non-stationary data from the fitted AR(1) object:
recoveredFit &lt;- diffinv(fitted(stationaryDataFit), differences = 2, xi = c(0,0))

# plot of rawData and the fitted values
plot(recoveredRawData, type = ""l"")    
lines(recoveredFit, col = ""slategrey"")
</code></pre>

<p>Here's the attempt to recreate the above plot, using the results from my stationaryDataFit:</p>

<p><img src=""http://i.stack.imgur.com/xZuKH.png"" alt=""Here&#39;s the attempt to recreate the above plot, using the results from my stationaryDataFit""></p>

<p>The shape looks correct, but the values are clearly off. I am <em>not</em> expecting to recover exactly the same results from both methods of fitting, but I still expect them to be reasonably close. </p>

<p>I strongly suspect the problem is with my choice of xi in the <code>diffinv</code> function, since that's really the only place I'm making any assumptions. But I'm having trouble reconciling the issue.</p>

<p>To integrate the data, <code>diffinv</code> requires the first observations of the integrated data. This is how I can convert the stationaryData back to the rawData, by passing the first two values of rawData to the <code>diffinv</code> xi argument. But I'm unsure what to use as the starting values to integrate <code>fitted(stationaryDataFit)</code>. The first two values of the (integrated) rawData are 0, so that's what I'm trying for now...</p>

<p>Any ideas?</p>

<p><strong>EDIT:</strong> Is this a legitimate work-around? Take the residuals from my stationaryDataFit object, and just subtract those from my rawData? For example:</p>

<pre><code># prefix 2 zeros, so the vectors are the same length (due to secord-differencing):
recoveredFit &lt;- rawData - c(rep(0, 2), stationaryDataFit$residuals)
</code></pre>

<p>My concern is about whether I need to transform my residuals from the stationaryDataFit somehow? In fact, the residuals from both fits are extremely close (within several decimals).</p>

<p>Thank you!</p>
"
"0.0984374038697697","0.122321680483099","110798","<p>I have a problem with the <code>forecast</code> function for ARIMA models in R. It calls <code>predict</code> that calls <code>KalmanForecast</code>. Ok...here's the deal.</p>

<p>the mean one-step forecast of the Arima object produced by this call</p>

<pre><code>forecast(Arima, h=1)$mean[[1]]
</code></pre>

<p>is often significantly different from the result of a manual forecast by conditional expected value (best linear predictor). </p>

<p>For example a non seasonal Arima(1,1,1) without drift has of course the structure</p>

<pre><code>y[t] = y[t-1] + AR1*(y[t-1] - y[t-2]) + MA1*epsilon[t-1] + epsilon[t]
</code></pre>

<p>so the one-step prediction is very straightforward </p>

<pre><code>y[t] = y[t-1] + AR1*(y[t-1] - y[t-2]) + MA1*epsilon[t-1]
</code></pre>

<p>but this result is always different from the result of the forecast function call.
Is it due to approximation errors in the Kalman recursion?</p>

<p>Try yourself with this code, it only needs the <code>forecast</code> package</p>

<pre><code>  x = arima.sim(n = 1000, list(ar = c(0.8897, -0.4858), ma = c(-0.2279, 0.2488)),sd=sqrt(0.1796))
  t = length(x) + 1
 Arimafit = Arima(x = x, order = c(1,1,1), seasonal = list(order = c(0,0,0), period =     1), include.mean = FALSE,include.drift = FALSE)
 manualforecast = x[t-1] + coef(Arimafit)[[""ar1""]]*(x[t-1] - x[t-2]) + coef(Arimafit)    [[""ma1""]]*Arimafit$residuals[t-1]
     autoforecast = forecast(Arimafit, h = 1)$mean[[1]]
</code></pre>

<p><code>autoforecast</code> is always different from <code>manualforecast</code>, sometimes significantly.</p>
"
"0.144672846651124","0.133166978517046","114675","<p>I really want to understand how the math is working here. I am trying to get the standard error of the fitted values for a time series regression model. In the non-time series regression, I know I can take the transpose of the data multiplied by the variance - covariance matrix of the model coefficients and then multiply by the data values again to get the standard errors of the fitted values.</p>

<p>But I'm not sure how to do this when I am including an autoregressive term.</p>

<pre><code>require(forecast)
require(tserieS)
</code></pre>

<p>Response variable</p>

<pre><code>Sablects &lt;- rnorm(10)
</code></pre>

<p>Covariates</p>

<pre><code>my.xreg &lt;- cbind(rnorm(10),rbinom(10,1,0.5))
</code></pre>

<p>In my actual data, values are normalized so I set the intercept equal to zero here.</p>

<pre><code>m4&lt;-arima(Sablects, order=c(2,0,0),fixed=c(0,NA,0,NA,NA),xreg=my.xreg) 
</code></pre>

<p>The predict function will give me standard errors on my in-sample prediction (the fitted values of my model).</p>

<pre><code>my.se &lt;- predict(m4, newxreg = my.xreg, n.ahead = 10)$se         

my.se
</code></pre>

<p>Now to compare the output of my.se, I want to do this mathematically but I don't know what to use for the values of the ar2 term. I use 1's as a placeholder to demonstrate that my output does not equal the values from <code>my.se</code> above</p>

<pre><code>C &lt;- cbind(rep(1, nrow(my.xreg)), my.xreg[, 1], my.xreg[, 2])

C
</code></pre>

<p>I think this value should equal the first value in my.se, but is not producing the same value as my.se</p>

<pre><code>sqrt(t(C[1, ]) %*% vcov(m4) %*% C[1, ])
</code></pre>

<p>Also, I'm not so great with matrix multiplication but here is my work around for getting all of the se values.</p>

<pre><code>se.output &lt;- matrix(nrow=nrow(C))
</code></pre>

<p>Specify that the max number of i is equal to number of rows of <code>C</code>.</p>

<pre><code>  for(i in 1:nrow(C)){

    # Loop through your multiplication for each row (i) of `C`. For each iteration, save the new data into the new row of se.output

    se.output[i] &lt;- sqrt(t(C[i, ]) %*% vcov(m4) %*% C[i, ])  
    }

se.output
</code></pre>
"
"0.0681994339470473","0.0706224551546449","115271","<p>I have been given data to forecast however it has a negative figure within the data which then, when doing a log transformation to make the series stationary, the ARIMA script i have written won't work.</p>

<pre><code>datan&lt;-c(144627.7451,575166.2487,854245.7137,1230639.153,1160052.421,479928.7072,-261427.4238,1181746.229,168251.621,556741.5149,1840484.518,1704679.404,1878380.278,1865288.502,1849340.253,1965974.112,2093192.242,1912399.391,2633179.421,2134618.008,2070856.492,1238565.331)

freqdata&lt;-4
startdata&lt;-c(9,2)
horiz&lt;-4
datats&lt;-ts(datan,frequency=freqdata,start=startdata)
force.log&lt;-""log""
datadates&lt;-as.character(c(""9q2"",""9q3"",""9q4"",""10q1"",""10q2"",""10q3"",""10q4"",""11q1"",""11q2"",""11q3"",""11q4"",""12q1"",""12q2"",""12q3"",""12q4"",""13q1"",""13q2"",""13q3"",""13q4"",""14q1"",""14q2"",""14q3""))
dataMAT&lt;-matrix(0,ncol=freqdata,nrow=(length(datats)+freqdata),byrow=TRUE)
for (i in 1:freqdata)
  {dataMAT[,i]&lt;-c(rep(0,length=i-1),lag(datats,k=-i+1),rep(0,length=freqdata-i+1))}
dataind&lt;-dataMAT[c(-1:(-freqdata+1),-(length(dataMAT[,1])-freqdata+1):-(length(dataMAT[,1]))),]
dataind2&lt;-data.frame(dataind)
lm1&lt;-lm(X1~.,data=dataind2)
lm2&lt;-lm(X1~X2+dataind2[,length(dataind2[1,])],data=dataind2)
library(lmtest)
library(car)
bptest1&lt;-bptest(lm1)
bptest2&lt;-bptest(lm2)
gqtest1&lt;-gqtest(lm1)
ncvtest1&lt;-ncvTest(lm1)
ncvtest2&lt;-ncvTest(lm2)
if(force.log==""level"") 
  {aslog&lt;-""n""}else
    {{if(force.log==""log"")
       {aslog&lt;-""y""}else
         {if(bptest1$p.value&lt;0.1|bptest2$p.value&lt;0.1|gqtest1$p.value&lt;0.1|ncvtest1$p&lt;0.1|ncvtest2$p&lt;0.1)
           {aslog&lt;-""y""}else
              {aslog&lt;-""n""}}}}
if(aslog==""y"")
  {dataa&lt;-log(datats)}else
    {dataa&lt;-datats}
startLa&lt;-startdata[1]+trunc((1/freqdata)*(length(dataa)-horiz))
startLb&lt;-1+((1/freqdata)*(length(dataa)-horiz)-trunc((1/freqdata)*(length(dataa)-horiz)))*freqdata
startL&lt;-c(startLa,startLb)
K&lt;-ts(rep(dataa,length=length(dataa)-horiz),frequency=freqdata,start=startdata)
L&lt;-ts(dataa[-1:-(length(dataa)-horiz)],frequency=freqdata,start=startL)
library(strucchange)
efp1rc&lt;-efp(lm1,data=dataind2,type=""Rec-CUSUM"")
efp2rc&lt;-efp(lm2,data=dataind2,type=""Rec-CUSUM"")
efp1rm&lt;-efp(lm1,data=dataind2,type=""Rec-MOSUM"")
efp2rm&lt;-efp(lm2,data=dataind2,type=""Rec-MOSUM"")
plot(efp2rc)
lines(efp1rc$process,col =""darkblue"")
plot(efp2rm)
lines(efp1rm$process,col=""darkblue"")
gefp2&lt;-gefp(lm2,data=dataind2)
plot(gefp2)
plot(dataa)
pacf(dataa)
sctest(efp2rc)
cat(""log series,y/n?:"",aslog)
</code></pre>

<p>then i want to run arima to get the forecasts</p>

<pre><code>library(tseries)
library(forecast)
max.sdiff&lt;-3
arima.force.seasonality&lt;-""n""
kpssW&lt;-kpss.test(dataa,null=""Level"")
ppW&lt;-tryCatch({ppW&lt;-pp.test(dataa,alternative=""stationary"")},error=function(ppW){ppW&lt;-list(error=""TRUE"",p.value=0.99)})
adfW&lt;-adf.test(dataa,alternative=""stationary"",k=trunc((length(dataa)-1)^(1/3)))
if(kpssW$p.value&lt;0.05|ppW$p.value&gt;0.05|adfW$p.value&gt;0.05)
  {ndiffsW=1}else
    {ndiffsW=0}
aaW&lt;-auto.arima(dataa,max.D=max.sdiff,d=ndiffsW,seasonal=TRUE,allowdrift=FALSE,stepwise=FALSE,trace=TRUE,seasonal.test=""ch"")
orderWA&lt;-c(aaW$arma[1],aaW$arma[6],aaW$arma[2])
orderWS&lt;-c(aaW$arma[3],aaW$arma[7],aaW$arma[4])
if(sum(aaW$arma[1:2])==0)
  {orderWA[1]&lt;-1}else
    {NULL}
if(arima.force.seasonality==""y"")
  {if(sum(aaW$arma[3:4])==0)
    {orderWS[1]&lt;-1}else
      {NULL}}else
        {NULL}
Arimab&lt;-Arima(dataa,order=orderWA,seasonal=list(order=orderWS),method=""ML"")
fArimab&lt;-forecast(Arimab,h=8,simulate=TRUE,fan=TRUE)
if(aslog==""y"")
  {fArimabF&lt;-exp(fArimab$mean[1:horiz])}else
    {fArimabF&lt;-fArimab$mean[1:horiz]}
plot(fArimab,main=""ARIMA Forecast"",sub=""blue=fitted,red=actual"") 
lines(dataa,col=""red"",lwd=2) #changes colour and size of dataa
lines(ts(append(fitted(Arimab),fArimab$mean[1]),frequency=freqdata,start=startdata),col=""blue"",lwd=2)
if(aslog==""y"")
  {Arimab2f&lt;-exp(fArimab$mean[1:horiz])}else
    {Arimab2f&lt;-fArimab$mean[1:horiz]} 
start(fArimab$mean)-&gt;startARIMA
ArimaALTf&lt;-ts(prettyNum(Arimab2f,big.interval=3L,big.mark="",""),frequency=freqdata,start=startARIMA)
View(ArimaALTf,title=""ARIMA2 final forecast"") #brings up table of the forecasts
summary(Arimab)
</code></pre>

<p>If anyone can help me figure out how to forecast this data with the negative i will be really grateful!!</p>
"
"0.137248713299344","0.142124949417345","115506","<p>Forecasting airline passengers seasonal time series using auto arima</p>

<p>Hi, I am trying to model some airline data in an attempt to provide an accurate monthly forecast for June-December this year using monthly data from January 2003 onwards.  The data is taken from: <a href=""http://www.transtats.bts.gov/Data_Elements.aspx?Data=1"" rel=""nofollow"">http://www.transtats.bts.gov/Data_Elements.aspx?Data=1</a></p>

<p>Here is the time series plot and ACF</p>

<p><a href=""http://imgur.com/EGh40pR"" rel=""nofollow""><img src=""http://i.imgur.com/EGh40pR.jpg"" title=""Hosted by imgur.com""/></a> </p>

<p><a href=""http://imgur.com/BJy78dn"" rel=""nofollow""><img src=""http://i.imgur.com/BJy78dn.jpg"" title=""Hosted by imgur.com""/></a></p>

<p>I have used auto.arima to develop two models and checked that they correspond to the autocorrelation functions.  Basically I am having trouble deciding whether to use:</p>

<ol>
<li>The following seasonal ARIMA model</li>
</ol>

<p><a href=""http://imgur.com/0k2Q8I4"" rel=""nofollow""><img src=""http://i.imgur.com/0k2Q8I4.jpg"" title=""Hosted by imgur.com""/></a></p>

<ol start=""2"">
<li><p>The following non-seasonal ARIMA model of $N_t$ after I first decomposed the model into a trend, seasonal component and random component $X_t = T_t +S_t +N_t $ using a 12-point moving average (basically did the same thing as the <code>decompose()</code> function manually)</p>

<p><a href=""http://imgur.com/r4TkpxX"" rel=""nofollow""><img src=""http://i.imgur.com/r4TkpxX.jpg"" title=""Hosted by imgur.com""/></a></p></li>
</ol>

<p>I have analysed the important properties of both models such as ensuring residuals are close to a white noise process and so on but am unsure which of the above 2 models is most suitable for forecasting purposes and why?</p>

<p>Also I am unsure how to compute forecast for the trend component vector if I use the classical decomposition model $X_t = T_t + S_t +N_t$.  Is it even possible to create forecasts using this type of model?</p>

<p>Edit:
Here is the output of <code>dput(IAP)</code> (the raw data without trend or seasonal component removed)</p>

<blockquote>
  <p>dput(IAP)
  structure(c(9726436L, 8283372L, 9538653L, 8309305L, 8801873L, 
  10347900L, 11705206L, 11799672L, 9454647L, 9608358L, 9481886L, 
  10512547L, 10252443L, 9310317L, 10976440L, 10802022L, 10971254L, 
  12159514L, 13502913L, 13203566L, 10570682L, 10772177L, 10174320L, 
  11244427L, 11387275L, 9945067L, 12479643L, 11521174L, 12164600L, 
  13140061L, 14421209L, 13703334L, 11325800L, 11107586L, 10580099L, 
  11812574L, 11724098L, 10167275L, 12707241L, 12619137L, 12610793L, 
  13690835L, 14912621L, 14171796L, 12010922L, 11517228L, 11222687L, 
  12385958L, 12072442L, 10590281L, 13246293L, 12795517L, 12978086L, 
  14170877L, 15470687L, 15120200L, 12321953L, 12381689L, 12004268L, 
  13098697L, 12767516L, 11648482L, 14194753L, 12961165L, 13602014L, 
  14413771L, 15449821L, 15327739L, 11731364L, 11921490L, 11256163L, 
  12463351L, 12075267L, 10412676L, 12508793L, 12629805L, 11806548L, 
  13199636L, 14953615L, 14844821L, 11659775L, 11905529L, 11093714L, 
  12659154L, 12393439L, 10694165L, 13279320L, 12398700L, 13380664L, 
  14406776L, 16026852L, 15317926L, 12599149L, 12874707L, 11651314L, 
  12915663L, 12668763L, 10944610L, 13473705L, 13537152L, 13935132L, 
  14814672L, 16623674L, 15753387L, 13220884L, 13185627L, 12144742L, 
  13546071L, 13206682L, 11732944L, 14387677L, 13995377L, 14291285L, 
  15582335L, 16969590L, 16621336L, 13791714L, 13397785L, 12762536L, 
  14096567L, 13766673L, 12023339L, 15177069L, 14278932L, 15306328L, 
  16232176L, 17645538L, 17517022L, 14239561L, 14209627L, 13133257L, 
  15083929L, 14589637L, 12385546L, 15486317L, 14857685L, 15615732L
  ), .Tsp = c(2003, 2014.33333333333, 12), class = ""ts"")</p>
</blockquote>

<p>Here is the output of <code>dput(IAP.res)</code> (the random component from the decomposition)</p>

<blockquote>
  <p>dput(IAP.res)
  structure(c(NA, NA, NA, NA, NA, NA, -669127.347569446, -168943.285069446, 
  225871.456597222, 271337.106597223, 711896.11076389, 284583.435763889, 
  165401.360763887, 622993.194097221, -268299.21423611, -9406.73506944434, 
  -233904.910069446, -147124.755902779, -260973.055902776, -163628.243402778, 
  -43056.7100694457, 121365.814930555, 205106.485763889, -107464.272569445, 
  247575.569097221, 279399.444097225, 309270.160763888, -166333.068402778, 
  129823.798263889, 22571.1190972265, -113455.59756944, -384199.160069444, 
  62061.8315972222, -155858.226736111, 13600.0274305546, -87564.1475694429, 
  71845.7357638887, 8145.86076388881, 47627.494097226, 442212.72326389, 
  73639.5065972234, 60882.5774305568, -135204.389236112, -437744.576736112, 
  203832.581597222, -264145.435069444, 179945.61076389, 15812.1024305553, 
  -49648.0975694434, -61460.8059027772, 89656.3690972241, 118205.931597224, 
  -84196.4517361106, 4197.78576389072, -134118.722569442, -87234.4517361117, 
  -126555.418402776, -57714.9350694417, 293250.152430556, 59462.6857638892, 
  10340.8190972245, 416646.652430557, 526459.702430556, -135041.068402776, 
  239767.631597222, 67034.9940972247, -221066.180902774, 207611.839930556, 
  -424486.00173611, -94779.3517361115, 89796.4857638886, 130285.644097223, 
  104776.152430555, 16099.8607638888, -317097.047569448, 335867.264930556, 
  -796342.285069446, -446777.464236111, -93681.7225694442, 242962.798263888, 
  -143380.293402778, 135423.439930556, 28934.7357638923, 186390.185763891, 
  116969.777430558, -113617.264236109, -39733.9225694438, -471572.526736109, 
  130389.423263891, 80446.7857638926, 298895.444097222, 38486.7982638846, 
  143712.123263886, 419260.898263889, -113385.347569445, -181233.730902779, 
  -178686.680902779, -412733.597569445, -380106.797569444, 172783.973263888, 
  220863.173263891, 11443.2440972247, 392297.319097224, -62825.8267361117, 
  176278.664930556, 139372.439930556, -174159.88923611, -111755.439236109, 
  -206233.264236111, -197431.097569445, -55065.5892361099, 48314.3065972236, 
  -6745.32673610683, 193492.494097225, 155009.569097224, 241747.214930556, 
  209670.99826389, -173438.47673611, -101510.63923611, -128948.689236113, 
  -222773.597569443, -498474.472569441, 146856.619097224, -275463.026736109, 
  386273.214930557, 213400.994097223, 171865.11076389, 464391.381597217, 
  1489.99826388643, -9918.39340277936, -362009.847569447, NA, NA, 
  NA, NA, NA, NA), .Tsp = c(2003, 2014.33333333333, 12), class = ""ts"")</p>
</blockquote>
"
"0.0835269069584557","0.0576629931704892","115710","<p>I have been using the forecast package in R to make forecasts based on an ARIMA model and have noticed a difference in the output of the forecast and simulate functions when calculating confidence intervals.</p>

<p>For example the 95% quantile calculated by the forecast function is about 0.5% higher than that based on 10000 applications of the simulate() function.  Also the mean of the simulated values and the point forecasts provided by the forecast functions are slightly different.</p>

<p>Which one of the functions will do the job better?  Or are the differences too small to worry about?  (The only reason I decided to try simulate was so that a distribution could be fitted to the simulated data).</p>

<p>Edit 1: Example</p>

<pre><code>library(forecast)

#Fit arima model to data
dm1 = arima(DAP, order = c(1,1,0), method = ""ML"", seasonal = list(order = c(0,1,1)))   

#Simulate 10000 times
n.mnths = 7
    n.sim = 10000
    domesticsimulator = function(i){
      simulate(dm1, nsim = n.mnths)
    }

sim.d &lt;- sapply(1:n.sim, function(x)domesticsimulator(x))
distr.d.mat&lt;-t(sim.d); distr.d.mat
distr.d&lt;-data.frame(Jun = distr.d.mat[,1],Jul = distr.d.mat[,2], Aug = distr.d.mat[,3], Sep = distr.d.mat[,4], Oct = distr.d.mat[,5], Nov = distr.d.mat[,6], Dec = distr.d.mat[,7]); distr.d

#Compare to forecast
forecast(dm1)
</code></pre>

<p>Edit 2: Data</p>

<blockquote>
  <p>dput(DAP)
  structure(c(43032450L, 41166780L, 49992700L, 47033260L, 49152352L, 
  52209516L, 55810773L, 53920973L, 44213408L, 49944935L, 47059495L, 
  49757124L, 43815481L, 45306644L, 54147227L, 53253194L, 53030873L, 
  56959142L, 59614287L, 57380873L, 47671785L, 54167489L, 51782564L, 
  52640057L, 47977657L, 47074882L, 58838975L, 54908859L, 57323876L, 
  59724061L, 62396446L, 59110633L, 50600325L, 53738093L, 52766404L, 
  52801276L, 48886043L, 47348142L, 58286011L, 55828555L, 57145193L, 
  59297121L, 60838606L, 58303233L, 49949551L, 55088986L, 53852209L, 
  53538970L, 50022168L, 47766421L, 59244232L, 57398267L, 59285571L, 
  61493934L, 63457403L, 62660179L, 52310402L, 57208618L, 55047116L, 
  53291139L, 50245100L, 50118363L, 59213077L, 55611053L, 58047400L, 
  59559171L, 61401480L, 58966473L, 47680101L, 52956023L, 47658141L, 
  50253800L, 44825056L, 43680328L, 53534891L, 52247781L, 52951246L, 
  55898027L, 59468957L, 56568180L, 48235025L, 52279405L, 48584832L, 
  49793527L, 45501620L, 42440614L, 54424077L, 52498074L, 53842422L, 
  56689853L, 59142493L, 57370748L, 50304708L, 54826050L, 51420519L, 
  51076415L, 46305000L, 43657818L, 55649428L, 52858479L, 55982234L, 
  57778699L, 60310568L, 57403835L, 50982170L, 54124363L, 51660083L, 
  51534990L, 47080840L, 46405385L, 56200391L, 53691570L, 55749349L, 
  57903293L, 59688267L, 58646304L, 50134504L, 53779646L, 51844482L, 
  51165451L, 47814031L, 45736763L, 56564538L, 53226735L, 56557964L, 
  57986530L, 59306473L, 58110953L, 50761250L, 54682312L, 50538227L, 
  54329096L, 47941907L, 45486064L, 57729464L, 54821717L, 57145762L
  ), .Tsp = c(2003, 2014.33333333333, 12), class = ""ts"")</p>
</blockquote>
"
"0.0723364233255618","0.0998752338877845","115712","<p>I am running ARIMA model in R and I used auto.arima(X) function to decide appropriate model.After using this function I found that the order of my model is ARIMA(2,1,0).
The problem is I run the same ARIMA(2,1,0) model using arima(X,order=c(2,1,0)) and I got AIC as AIC=832.16. but for same model by using auto.arima(X) as AIC=805.29. I dont know why for the same model AIC is different. Please hep me to over come this problem.
Thank you in advance.</p>
"
"0.0681994339470473","0.0706224551546449","118637","<p>I use: <code>fit = auto.arima(Y, xreg=X)</code> in R to get ARIMA(1,0,0), result as follows:  </p>

<pre><code>ar1: 0.3793;    intercept: 9132.46;    X: 22.0469
</code></pre>

<p>Then:  </p>

<ol>
<li>I build the function: <code>(Y(t) - 9132.46) = 0.3793*(Y(t-1) - 9132.46) + 22.0469*X(t)</code> in Excel to calculate fitted value and predicted value manually.</li>
<li>In R, I use <code>fitted(fit)</code> to get fitted value, <code>forecast()</code> to get predicted value.</li>
</ol>

<p>But I found the results of 1) and 2) are different, is there anything wrong with the function I built? </p>
"
"0.0984374038697697","0.122321680483099","120008","<p>I am interested in fitting an ARIMAX model using R.
As known, ARIMAX can be understood as a composition of ARIMA models and regression models with exogenous (independent) variables. I have a time series $Y_i$, and want to estimate the ARIMA and nonlinear coefficients. The nonlinear model is the following:</p>

<p>$y_i=Î²_0+Î²_1t_i+Î²_2d+Î²_3 sin(2Ï€t_i/Î²_4 )+Î²_5 (-1^{t_i})+Îµ_i$,  nonlinear 
regression with an exogenous variable.
Where 
$t_i$ =1, 2â€¦, 60
and</p>

<p>d = dummy variable with 20 0's and 40 number 1's</p>

<pre><code>d=c(rep(0,20),rep(1,40))
</code></pre>

<p>And an ARIMA model (1,1,1) for $Y_i$. Therefore, I want to estimate simultaneously the $Î²_i$ and the ARIMA coefficients in order to avoid the confusion between the exogenous coefficients and ARIMA coefficients.  I know that $arima()$ can deal with this formulation but, how do the nonlinear model can be set within function function?. It seems that the <em>xreg</em> term only deals with linear parameters.</p>
"
"0.0835269069584557","0.0576629931704892","120415","<p>I'm running a PCA using the R function <code>prcomp</code>. This is the function:</p>

<pre><code>d2.pca &lt;- prcomp(sel.d2, center=TRUE, scale.=TRUE)
</code></pre>

<p>So variables are scaled an centered. (This always has to be done, right?)</p>

<p>This is my original loadings matrix:</p>

<pre><code>                    PC1    PC2    PC3    PC4
var1              0.551 -0.246  0.576 -0.551
var2             -0.545 -0.233  0.736  0.328
var3             -0.427 -0.704 -0.333 -0.460
var4             -0.467  0.625  0.126 -0.613
</code></pre>

<p>When I apply variamx rotation:</p>

<pre><code>varimax(d2.pca$rotation)
</code></pre>

<p>The output is this one:</p>

<pre><code>$loadings

Loadings:
                 PC1 PC2 PC3 PC4
var1              1             
var2                      1     
var3                 -1         
var4                         -1 

                PC1  PC2  PC3  PC4
SS loadings    1.00 1.00 1.00 1.00
Proportion Var 0.25 0.25 0.25 0.25
Cumulative Var 0.25 0.50 0.75 1.00

$rotmat
       [,1]  [,2]   [,3]   [,4]
[1,]  0.551 0.427 -0.545  0.466
[2,] -0.246 0.704 -0.232 -0.625
[3,]  0.576 0.333  0.736 -0.125
[4,] -0.551 0.461  0.328  0.613
</code></pre>

<p>This looks very strange to me, how should I interpret the loadings (<code>1</code> and <code>-1</code> values) matrix after varimax rotation? Any help or advise will be appreciated, I'm probably missing something...</p>

<p>Note: KMO was 0.6 for the correlation matrix. Just in case, here it is the correlation matrix:</p>

<pre><code>         var1        var2        var3        var4
var1    1.000      -0.680      -0.491      -0.771
var2   -0.680       1.000       0.697       0.550
var3   -0.491       0.697       1.000       0.166
var4   -0.771       0.550       0.166       1.000 
</code></pre>
"
"0.136398867894095","0.14124491030929","120806","<p>I'm using R(3.1.1), and ARIMA models for forecasting. 
I would like to know <strong>what should be the ""frequency"" parameter, which is assigned in the <code>ts()</code> function</strong>, if im using time series data which is:</p>

<ol>
<li>separated by <strong>minutes</strong> and is spread over 180 days (1440 minutes/day) </li>
<li>separated by <strong>seconds</strong> and is spread over 180 days (86,400 seconds/day).</li>
</ol>

<p>If I recall right the definition, a ""frequency"" in ts in R, is the number of observations per ""season"". </p>

<h2>Question part 1:</h2>

<p>then, what is the ""season"" in my case?</p>

<p>If the season is ""day"", then is the ""frequency"" for minutes = 1440 and 86,400 for seconds?</p>

<h2>Question part 2:</h2>

<p><strong>Could the ""frequency""  also depend on what I am trying to achieve/forecast?</strong>
for example, in my case, I'd like to have a very short-term forecast. 
One-step ahead of 10minutes each time. 
<strong>Would it then be possible to consider the season as an hour instead of a day?</strong>
In that case frequency= 60 for minutes, frequency = 3600 for seconds?</p>

<p>I've tried for example to use frequency = 60 for the minute data and got better results compared to frequency = 1440 (used <code>fourier</code> see link below by Hyndman)
<a href=""http://robjhyndman.com/hyndsight/forecasting-weekly-data/"">http://robjhyndman.com/hyndsight/forecasting-weekly-data/</a> </p>

<p>(The comparison was made by using MAPE for the measure of forecast accuracy)</p>

<p><strong>In case the results are complete arbitrary, and the frequency cannot be changed. 
What would be actually the interpretation of using freq = 60 on my data?</strong> </p>

<p>I also think it's worth mentioning that my data contains seasonality at every hour and every two hours (by observing the raw data and the Autocorrelation function)</p>

<p>Thanks!</p>
"
"0.0723364233255618","0.0998752338877845","121298","<p>It seems that the <code>auto.arima</code> function in the ""forecast"" package in R only considers full ARIMA models. By ""full"" I mean that if an AR lag $k$ is included, AR lag $j$ will also be included for $k&gt;1$, $0&lt;j&lt;k$ (and the same with MA in place of AR). This was claimed <a href=""http://stats.stackexchange.com/questions/26668/subset-models-in-auto-arima-function-in-forecast-package"">here</a> by the author of the <code>auto.arima</code> function himself. </p>

<p>I am interested in non-full (restricted) ARIMA models, e.g. an AR(2) model where the first AR lag is restricted to zero: $x_t=\varphi_2 x_{t-2}+\varepsilon_t$.</p>

<p><strong>Question 1:</strong> Is there a good <strong>theoretical</strong> reason for not considering the non-full ARIMA models?</p>

<p><strong>Question 2:</strong> Is there a good <strong>practical</strong> reason for not considering the non-full ARIMA models? (Besides the argument of high computational burden if all sub-models within given maximum AR and MA orders are to be estimated.)</p>
"
"0.0723364233255618","0.0998752338877845","121745","<p>I have trouble understanding the output of function <code>arima()</code> in <code>R</code>. </p>

<p>Reading the help file and other sources has not helped much...</p>

<p>Consider an AR(1) model as an example. As I understand it,</p>

<pre><code>m1=arima(x, order=c(1,0,0), include.mean=TRUE)
</code></pre>

<p>estimates a model of the form</p>

<p>$$(x_t-\mu_x)=\varphi_0+\varphi_1(x_{t-1}-\mu_x)+\varepsilon_t$$</p>

<p>where $\mu_x$ is the mean of $x$. Since the population mean is not known, its sample counterpart $\frac{1}{T}\sum_{t=1}^{T} x_t$ is used.</p>

<p>However, I am <strong>unable to reconstruct</strong> $x$ from the fitted values and the residuals of the model, thus I must have misunderstood something. Here is what I do step-by-step:</p>

<pre><code>T=1000; set.seed(1); x=rnorm(T, sd=10)         # generate a random variable x
m1=arima(x, order=c(1,0,0), include.mean=TRUE) # fit an AR(1) model for x
</code></pre>

<p>Obtain the $\varphi_0+\varphi_1(x_{t-1}-\mu_x)$:</p>

<pre><code>fitted=(x-mean(x))*m1$coef[1] + 1*m1$coef[2]   # define fitted values from the AR(1) model
fitted=c(NA,fitted[-T])        # adjust so that fitted[t] corresponds to x[t] for any t
</code></pre>

<p>Obtain the $\varepsilon_t$:</p>

<pre><code>resids=resid(m1)               # residuals from the AR(1) model
resids=c(NA,resids[-1])        # adjust so that resids[t] corresponds to x[t] for any t
</code></pre>

<p>Obtain the $(x_t-\mu_x)$:</p>

<pre><code>true=x-mean(x)
</code></pre>

<p>Plot the $(x_t-\mu_x)$ in black and the $\varphi_0+\varphi_1(x_{t-1}-\mu_x)+\varepsilon_t$ in red:</p>

<pre><code>plot( true[1:20], type=""l"" )
lines( (fitted+resids)[1:20], col=""red"" )
</code></pre>

<p>The two are <strong>not equal!</strong> (Although close, but likely more than purely due to an approximation error.)</p>

<p><strong>Edit:</strong> Perhaps I should use $\frac{\varphi_0}{1-\varphi_1}$ instead of $\mu_x$, i.e. <code>m1$coef[2]/(1-m1$coef[1])</code> instead of <code>mean(x)</code>? I have tried that, too, and it did not help.</p>

<p><strong>Have I misinterpreted the model definition</strong> (see how I define <code>true</code>, <code>fitted</code> and <code>resids</code>)<strong>?</strong></p>

<hr>

<p>By the way, I have no problem with the case of no constant term: <code>arima(..., include.mean=FALSE)</code>:</p>

<pre><code>T=1000; set.seed(1); x=rnorm(T, sd=10)
m1=arima(x, order=c(1,0,0), include.mean=FALSE)
fitted=x[-T]*m1$coef[1]
resids=resid(m1)[-1]
plot(x[(1+1):(20+1)],type=""l"")
lines((fitted+resids)[1:20],col=""red"")
x[(1+1):(20+1)]==(fitted+resids)[1:20]
</code></pre>
"
"NaN","NaN","121749","<p>I am wondering about the exact definition of ARIMA model in function <code>arima</code> in <code>R</code> when exogenous regressors are included.</p>

<p>I understand that <code>arima(y, order=c(p,0,q), xreg=x)</code> is equivalent to estimating the following equation (where $\mu_y$ and $\mu_x$ stand for the means of $y$ and $x$, respectively):</p>

<p><strong>(1)</strong> $(y_t-\mu_y)=\varphi_0+\phi_1(y_{t-1}-\mu_y)+...+\varphi_p(y_{t-p}-\mu_y)+\varepsilon_t+\theta_1\varepsilon_{t-1}+...+\theta_q\varepsilon_{t-q}+\beta_1x_t$</p>

<p>Or is it</p>

<p><strong>(2)</strong> $(y_t-\mu_y)=\varphi_0+\phi_1(y_{t-1}-\mu_y)+...+\varphi_p(y_{t-p}-\mu_y)+\varepsilon_t+\theta_1\varepsilon_{t-1}+...+\theta_q\varepsilon_{t-q}+\beta_1(x_t-\mu_x)$</p>

<p>(only the last term differs between <strong>(1)</strong> and <strong>(2)</strong>)?</p>

<p>Or perhaps I got both of them wrong?</p>

<p><strong>Edit:</strong> I now realize that including both {$\mu_x$ and $\mu_y$} and $\varphi_0$ in <strong>(2)</strong> was superfluous.</p>
"
"0.0681994339470473","0.0706224551546449","121882","<p>I am facing a strange issue with auto.arima. On a dataset named data, I run the following code</p>

<pre><code>auto.arima(data,d=0,D=1,xreg=1:length(data),max.p=3,max.q=3,max.order=10,
       seasonal=TRUE,stepwise=FALSE,approximation=FALSE,ic=(""aic""),parallel=TRUE)
</code></pre>

<p>The outcome is </p>

<pre><code>   Series: data 
   ARIMA(1,0,3)(2,1,2)[12]                    

 Coefficients:
        ar1      ma1      ma2     ma3    sar1     sar2     sma1    sma2  1:length(data)
        0.6939  -0.6417  -0.2391  0.4350  0.6197  -0.5055  -1.3211  0.6027   5e-04
  s.e.  0.1349   0.1654   0.1169  0.1502  0.3040   0.1762   0.3907  0.4269   3e-04

 sigma^2 estimated as 0.001792:  log likelihood=143.69
 AIC=-267.38   AICc=-264.79   BIC=-241.73
</code></pre>

<p>Then I try to fit this model using the Arima function:</p>

<pre><code> Arima(data,order=c(1,0,3),seasonal=list(order=c(2,1,2),period=12),
       xreg=1:length(data),method=""ML"")
</code></pre>

<p>but I get the following error message:</p>

<pre><code> Error in optim(init[mask], armafn, method = optim.method, hessian = TRUE,  : 
   non-finite finite-difference value [1]
</code></pre>

<p>Does someone understand why this appears ? Thanks.</p>
"
"0.264272806544809","0.247178593041257","122270","<p>This is a long post but it is <em>not</em> conceptually difficult. Please bear with me.</p>

<p>I am trying to model the <strong>seasonality</strong> of production volume of an agricultural commodity. I do not care about the structure of the model (as long as it produces sensible output) nor about explaining seasonality; my goal is to adjust the series for seasonality before proceeding to forecasting. </p>

<p>I know the seasonality is due to supply (weather is a key determinant) as well as demand (higher demand around Christmas and Easter, etc.). </p>

<p>My <strong>data is weekly</strong>, and I am following the approach by prof. Hyndman of modelling the time series as an <strong>ARMA process with Fourier terms</strong> to account for seasonality (see <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">here</a>). I also include Christmas and Easter dummies together with Fourier terms. </p>

<p>You might think I do not need a Christmas dummy since Christmas is always the 25th of December and thus Fourier terms would account for it, unlike for Easter which jumps back and forth in March-April from year to year. However, Christmas sometimes occur in week 51 and sometimes in week 52 (at least in my calendar), and Fourier terms will not account for that; hence a Christmas dummy is actually relevant.</p>

<p>Actually, I would like to include up to four dummies for Christmas (from two weeks before Christmas to one week after Christmas) as the seasonal effect is not concentrated on just the Christmas week but also spread out a little in time. The same is with Easter.</p>

<p>Thus I end up with $4+4=8$ dummies, up to 25 pairs of Fourier terms and an unknown ARMA order, which could be as high as (5,5), perhaps. This gives me an <strong>awfully large pool of models to choose from</strong>. Thus I have trouble replicating the approach used in the source above because of <strong>limited computational resources</strong>. Recall that in the original source there were no dummies, so the model pool was $2^{4+4}=256$ times smaller than mine and it was possible to try out a reasonably large proportion of all possible models, and then choose one of them by AIC. </p>

<p>Even worse, I would like to repeat my exercise multiple times (100 times) using a rolling window on the data I have. Thus the <strong>computational requirements</strong> for my exercise <strong>become exorbitant</strong>. I am looking for a solution, and hope someone could help me out.</p>

<p>Here are a few ideas I have:</p>

<ol>
<li><p>Drop the ARMA terms all together. </p>

<ul>
<li>This will make the estimates of coefficients on Fourier terms and dummies biased and inconsistent, thus the seasonal component (the Fourier terms times their coefficients plus dummies times their coefficients) might be rubbish. However, if ARMA patterns are pretty weak, maybe the bias and inconsistency of the coefficient estimates will not be too bad?</li>
</ul></li>
<li><p>Estimate only some models from the pool of all possible models. E.g. select the ARMA order by <code>auto.arima()</code> function in <code>forecast</code> package in <code>R</code>: <code>auto.arima()</code> does not consider all possible ARMA orders but only non-restricted ones (e.g. an AR(2) model with the AR(1) coefficient restricted to zero is not considered) and takes some other shortcuts. Also, I could force all dummies to be present without questioning whether all of them are really relevant. </p>

<ul>
<li>This indeed reduces the model pool considerably (by a factor of $2^{4+4}=256$ only due to dummies), but it comes at a cost of likely inclusion of irrelevant variables. Not sure how bad that could be. </li>
</ul></li>
<li><p>Use a different estimation method: conditional sum of squares (CSS) instead of maximum likelihood (ML) when possible, something else? </p>

<ul>
<li>Unfortunately, CSS in place of ML does not seem to reduce the computational time enough in practice (I tested a few examples).</li>
</ul></li>
<li><p>Do the estimation in two (or more) steps: select the ARMA order independently of the number of Fourier terms and inclusion/exclusion of dummies, then select the number of Fourier terms, then select the number of dummies, or something similar. </p>

<ul>
<li>This would reduce the model pool to the following: the number of all possible ARMA models (e.g. $2^{5+5}$) plus 26 (0 pairs of Fourier terms, 1 pair of Fourier terms, ..., 25 pairs of Fourier terms) plus $2^{4+4}$ (all combinations of dummies). This particular division into stages certainly does not make sense, I admit. But perhaps there exists a smart way to divide model selection into steps? I guess we need mutual orthogonality of regressors to be able to consider their inclusion/exclusion from the model one by one, or something similar. I need some insight here...</li>
</ul></li>
<li><p>Do something along the lines of Cochrane-Orcutt correction: <strong>(1)</strong> estimate a model without any ARMA terms (thus a simple OLS regression of production volume on Fourier terms and dummies); <strong>(2)</strong> examine the ARMA patterns in residuals and determine the relevant ARMA order; <strong>(3)</strong> estimate the corresponding ARMA model with Fourier terms and dummies as exogenous regressors. <br>(I know Cochrane-Orcutt does not consider ARMA(p,q) but just AR(1), but I thought maybe I could generalize it? Not sure, perhaps I am making a mistake here?)</p>

<ul>
<li>This would reduce the model pool to be estimated by a factor of $2^{5+5}$ if we would otherwise consider ARMA(5,5) and all sub-models nested in it, which is very nice. However, Cochrane-Orcutt correction is not as good as getting the model order correct in the first place. (I lack a theoretical argument there, but I suspect you do not always end up with the correct ARMA order if you use Cochrane-Orcutt or something similar.)</li>
</ul></li>
</ol>

<p>I would appreaciate any comments on the ideas above and, even more, your own ideas on how to save computational time.</p>

<p><strong>Thank you!</strong></p>

<p>P.S. Here is the data (2007 week 1 to 2012 week 52): <br><code>6809  8281  8553  8179  8257  7426  8098  8584  8942  9049  9449 10099 10544  6351  8667  9395  9062  8602 10500  8723  9919  9120 10068  9937  9998  9379  9432  8751 10250  7690  6857  8199  8299  9204 11755  9861 10193 10452  9769 10289 10399 10181 10419 10324 11347 11692 12571 13049 13712 15381 13180  3704  5833  9909  8979  8883  9599  9034  9617  9710  9878 10872 12246  7122  8071  9656  9157  9729  8814  8704 10724  9294 10176 10394  9633 10872 10353  9400  9692  9244  8921  7698  8191  8494  8681  9420  8947  9287 10043  9953 10136 10235 10027 10137 11187 10383 11321 12023 11824 13752 13907 14658 14634  5353  5603 11607  9512  9537  9775  8558  9227  9146  9673 10225 10794 10646 10545 12375  7742  9269 11134  9588 10890 10061 10244 11275  9428 10301 11039 10779 10580  9215  9538  9717  9966 10678 10821 10898 11458 11090 12080 10904 12990 12325 12257 12246 12351 12408 11732 12702 14181 14963 15027 16657 17766  8265  7552 11067 10808 10891 10967 10826 10929 10891  9293 10902 11640 11921 13320  7751  9929 11632 11073 10955 11785 10533 10629 11535 12513 11780 12639 12280 12245 11817 11275 10242 10625  9884 10182 11048 12120 12697 13132 12597 13516 13536 13279 13935 13539 14335 13748 13988 14497 15255 15346 17626 17716 10713  7752  9309 11133 11853 11570 10320 10817 10982 10585 10941 12801 12165 11899 10941 12729 13763  8130  9888 11889 13036 10413 12609 10992 13201 11833 13159 12880 12160 13277 12758 10897 11197 12169 12639 13788 13819 14804 14613 14992 16079 16648 16376 16207 15906 15301 16194 17327 18890 18710 19953 22369 16499  9721 11785 12528 13994 12990 13226 12883 13613 14382 15829 15525 16395 17175 18334  9286 15530 16510 15370 13019 16526 14425 16542 15328 16993 17607 16734 15826 16555 16064 14412 14355 15238 15064 15317 15533 16542 16474 17354 18359 18459 17312 17499 18298 17301 17795 17867 18453 19372 19394 21897 23674 23219  7356</code></p>
"
"0.0835269069584557","0.0864944897557338","122803","<p>I would like to conduct a forecast based on a multiple time series ARIMA-model with multiple exogeneous variables. Since I am not that skillfull with regards to neither statistics nor R I want to keep is as simple as possible (Trend forecast for 3 months is sufficient).</p>

<p>I have 1 dependent time series and 3-5 predictor time series, all monthly data, no gaps, same time ""horizon"".</p>

<p>I encountered the auto.arima function and asked myself if this would be a suitable solution for my problem. I have different commodity prices and prices of products made from them. All raw-data are non-stationary but via first-order differencing they all become stationary data. ADF, KPSS indicate this. (This means that I have tested for integration, right?).</p>

<p>My question now is: How do I apply this with the auto.arima function AND is ARIMA the right approach anyways? Some ppl already adviced me to use VAR, but is it possible with ARIMA too?</p>

<p>The following table is my data. Actually the data-set goes up til 105 observations, but the first 50 will do. Trend as well as seasonality are obviously of interest here.</p>

<p><img src=""http://i.stack.imgur.com/8lnMa.png"" alt=""enter image description here""></p>

<p>Thanks for any advices and help!
Georg</p>
"
"0.21130821751815","0.255284953170072","123576","<p>I am trying to test the effect on the heat flux between indoors and outdoors before and after removing insulation.</p>

<p>Briefly, I have 26 sensors on a wall, measuring heat flow between indoors and outdoors over a number of days. The wall was part of a real world experimental setup so that the insulation on the wall was removed halfway through the experiment. Â What I care about is to have a measure of the effect of the removal of the insulation (I am not interested in any form of forecasting). Â I am exploring the use of a SARIMA/ARIMAX models with one regressor because, aside from the removal of the insulation, the heat flow between indoors and outdoors was affected by daily cyclical and random environmental effects (heating on or off, daily temperature changes, wind, etc).  Here I will present that data and analysis of one sensor.  My data has been collected hourly, and I have transformed the variable â€˜insulatedâ€™ â€˜not insulatedâ€™ as a factor of 0s and 1s as indicator.</p>

<pre><code>heat.flux = c(8.677048,6.558642,5.920314,5.583614,5.373176,5.253928,4.938272,7.358305,9.743266,10.46577,11.06201,10.90067,11.49691,13.15236,12.10017,10.60606,10.45875,10.03788,9.588945,9.287318,8.578844,8.024691,10.26936,11.8757,10.20623,8.634961,8.305275,8.101852,8.12991,7.947531,7.814254,10.40264,13.08221,14.3729,14.94809,15.08838,15.20763,15.75477,14.57632,12.79461,11.97391,10.97082,10.33249,9.701178,9.715208,9.083895,10.63412,12.07912,9.736251,7.638889,6.453423,5.983446,5.499439,5.099607,4.70679,6.972503,9.259259,9.981762,10.24832,10.17116,10.27637,10.27637,9.546857,7.568743,7.168911,6.867284,6.705948,6.916386,8.319304,8.424523,11.41274,13.52413,11.70034,9.532828,8.957632,9.07688,9.694164,9.301347,9.048822,12.28255,14.95511,15.22868,15.24972,15.12346,15.08838,15.17256,13.68547,12.18434,12.1633,12.13524,11.81257,11.58109,11.44781,11.27946,13.87486,15.92312,14.07828,11.90376,10.46577,9.518799,8.978676,8.803311,8.684063,11.65123,14.39394,15.69865,16.61756,16.828,16.83502,16.16863,14.23962,12.19837,12.09315,11.5881,11.20932,10.50786,10.59203,10.64815,13.51712,15.71268,13.92396,12.10718,12.2615,11.65123,11.05499,10.31846,9.834456,12.9349,15.41807,15.78283,15.8179,16.11953,15.95118,15.63552,13.1243,11.22334,10.21324,8.705107,7.526655,6.15881,5.30303,5.597643,8.599888,11.17424,9.631033,8.038721,7.638889,7.203984,7.161897,6.76908,6.888328,9.518799,12.40881,13.21549,14.28872,14.43603,14.8078,14.81481,13.60129,12.59119,11.86167,11.91779,11.73541,12.04405,11.51796,11.74242,13.7486,15.85999,14.84989,12.63328,10.68322,9.343434,8.592873,8.333333,8.445567,10.97783,13.82576,15.12346,16.58249,17.61364,18.30808,19.10774,17.97138,16.62458,15.867,16.07744,15.63552,16.0073,15.42508,15.01122,17.10157,18.94641,22.44669,18.94641,16.01431,14.55527,13.88889,12.77357,11.66526,12.46493,15.41807,16.75786,17.27694,17.03143,16.84905,16.828,16.02834,16.35802,16.04237,15.03928,14.00112,14.1344,13.86785,13.99411,15.30584,18.20286,19.49355,16.16162,14.05022,12.05107,12.27553,13.01207,12.5491,13.72054,16.91218,18.62374,18.79209,20.80527,19.50758,20.18799,20.63692,18.49747,17.25589,17.38215,18.40629,18.60269,19.12177,18.66582,21.09989,24.45286,26.71156,23.54798,20.01964,17.98541,14.83586,14.31678,15.15152,15.30584,17.95735,19.71801,20.30724,20.19501,20.2862,20.1459,20.10382,18.20988,16.54742,15.22868,13.96605,12.71044,11.61616,10.71829,12.12121,14.77273,14.04321,12.44388,10.94978,10.2413,9.708193,9.638047,9.322391,11.27245,14.24663,14.77273,14.75168,14.92705,15.47419,15.48822,14.73765,13.68547,12.65432,12.35269,12.34568,12.32464,12.7385,12.84371,14.16947,17.34007,17.09456,15.0954,13.40488,11.70735,10.8165,10.64815,12.01599,13.55219,16.7298,17.45932,17.61364,19.58474,20.02666,19.79517,19.38833,17.32604,16.11953,15.62851,15.01122,14.70258,14.5693,14.35887,16.28086,18.69388,18.92536,16.56846,15.97222,13.34877,12.81566,12.04405,13.23653,14.1835,16.75786,17.55752,17.98541,18.85522,18.8482,19.02357,18.96044,17.31201,15.42508,14.38692,13.57323,12.36672,12.03002,11.41274,13.15236,15.88103,14.66049,12.8858,11.67228,11.03395,9.399551,8.375421,8.073793,10.6271,13.57323,13.61532,14.31678,14.73765,15.08838,15.62149,16.6807,15.28479,14.07127,13.14534,12.61223,12.57015,12.02301,12.17031,14.33782,18.83418,20.45455,18.67985,18.40629,16.51235,14.45006,14.61841,15.20763,15.57941,18.06958,19.88636,20.51066,21.633,23.24635,24.28451,24.70539,24.19332,22.81145,21.97671,21.58389,21.3945,21.21212,20.89646,21.1069,23.86364)

insulation = c(0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1)
</code></pre>

<p>First off, the time series plot of the heat flux is this (the red line is when the insulation is removed):</p>

<p><img src=""http://i.stack.imgur.com/SYSQj.jpg"" alt=""Time series plot of heat flux""></p>

<p>Than this are the ACF and PACF plots of the same data:</p>

<p><img src=""http://i.stack.imgur.com/7keT7.jpg"" alt=""ACF and PACF of the data""></p>

<p>For my data, an <code>stl()</code> decomposition, run as <code>stl(ts(heat.flux, frequency = 24), 'period')</code></p>

<p>shows a strong â€˜seasonalâ€™ (i.e daily) component and a trend in the series. Â </p>

<p><img src=""http://i.stack.imgur.com/CUsta.jpg"" alt=""STL of the data""></p>

<p>Firs off I am trying to determine the best parameters for a SARIMA or ARIMAX model so that I can get an estimation of the effect removing the insulation. Despite the fact I can produce the ACF and PACF plots there is no way I can figure out the proper orders, so I load the library <code>forecast</code> and I run:</p>

<pre><code>library(forecast)
auto.arima(ts(heat.flux, frequency = 24), xreg = insulation, max.p = 10, max.q = 10, max.P = 10, max.Q = 10, stationary = F)Â 
</code></pre>

<p>The reason why I do not specify a stationary model is because of the trend I see with <code>stl()</code> and because I assume an effect of removing the insulation.</p>

<p>from <code>auto.arima()</code> I get:</p>

<pre><code>Series: ts(heat.flux, frequency = 24) 
ARIMA(2,0,2)(1,0,1)[24] with non-zero mean 

Coefficients:
         ar1      ar2      ma1      ma2    sar1     sma1  intercept  carp.hour$interv
      1.9414  -0.9495  -0.7423  -0.1793  0.9717  -0.6009    11.2449            4.3338
s.e.  0.0231   0.0221   0.0570   0.0544  0.0104   0.0544     2.1075            0.5548

sigma^2 estimated as 0.4484:  log likelihood=-411.28
AIC=840.55   AICc=841.03   BIC=876.11
</code></pre>

<p>If I try to use the <code>TSA</code> package and use <code>arimax()</code> with those orders I get basically the same stuff:</p>

<pre><code>library(TSA)
arimax(ts(heat.flux, frequency = 24), xreg = insulation, order = c(2,0,2), seasonal = list(order = c(1,0,1), frequency = 24))
Series: x 
ARIMA(2,0,2)(1,0,1)[24] with non-zero mean 

Coefficients:
         ar1      ar2      ma1      ma2    sar1     sma1  intercept    xreg
      1.9414  -0.9495  -0.7423  -0.1793  0.9717  -0.6009    11.2449  4.3338
s.e.  0.0231   0.0221   0.0570   0.0544  0.0104   0.0544     2.1075  0.5548

sigma^2 estimated as 0.4484:  log likelihood=-411.28
AIC=838.55   AICc=839.03   BIC=874.11
</code></pre>

<p>And all is apparently well (Irrespective of the function I choose I get an estimate of the effect of the removal of the insulation and a se with it with is what I want). Â Unfortunately, when I test the fit of this model with the function <code>sarima()</code> from the <code>astsa</code>package I get significant Ljung-Box p-values for all my sensors and for all the lags:</p>

<pre><code>library(astsa)
sarima(ts(heat.flux, frequency = 24), p = 2, d = 0, q = 2, P =1, D = 0, Q = 1, S = 24, xreg = insulation)
$fit

Call:
stats::arima(x = xdata, order = c(p, d, q), seasonal = list(order = c(P, D, 
Q), period = S), xreg = xreg, optim.control = list(trace = trc, REPORT = 1, 
reltol = tol))

Coefficients:
         ar1      ar2      ma1      ma2    sar1     sma1  intercept    xreg
      1.9414  -0.9495  -0.7423  -0.1793  0.9717  -0.6009    11.2449  4.3338
s.e.  0.0231   0.0221   0.0570   0.0544  0.0104   0.0544     2.1075  0.5548

sigma^2 estimated as 0.4484:  log likelihood = -411.28,  aic = 840.55
</code></pre>

<p>but the plot that comes with is shows that at every single lag the Ljung-Box statistics is significant:</p>

<p><img src=""http://i.stack.imgur.com/ZMGKN.jpg"" alt=""SARIMA""></p>

<p>What is going on?  To sum it up:</p>

<ol>
<li>which of these models is the most correct to estimate the effect of insulation?</li>
<li>why are the Ljung-Box p-values all significant?  I would have though that the ARIMA/ARIMAX/SARIMA would have sorted that issue</li>
<li>If the orders calculated by <code>auto.arima()</code> are the problem, how could I find them in a different way (which is computationally feasible and does not take days).</li>
</ol>

<p>Finally, two notes.  I also have collected variables such as internal and external temperatures, windspeed, etc, but I would have though that integrating these in the model would be superfluous given the fact it is already an ARIMA model to start with.  Second, I am not at all wedded to this kind of analysis, but I am aware that a straightforward linear model would not be acceptable given the autocorrelation between the data points.</p>
"
"0.133750094043068","0.152352232547885","123723","<p>Can anyone tell me the formula behind the <code>forecast</code> function in R? Preferably in the form easily understood by mathematicians (e.g  x_t, Î¸ etc)</p>

<p>Here is my code in case it helps</p>

<pre><code>suppressMessages(library(lmtest))
suppressMessages(library(car))
suppressMessages(library(tseries))
suppressMessages(library(forecast))
suppressMessages(library(TTR))
suppressMessages(library(geoR))
suppressMessages(library(MASS))
suppressMessages(library(gtools))
#-------------------------------------------------------------------------------
Model &lt;- ""choosing ARIMA""
Series.title &lt;- ""EMEA GAM&lt;250K""
#-------------------------------------------------------------------------------
Input.data &lt;- matrix(c(""08Q1"",""08Q2"",""08Q3"",""08Q4"",""09Q1"",""09Q2"",""09Q3"",""09Q4"",""10Q1"",""10Q2"",""10Q3"",""10Q4"",""11Q1"",""11Q2"",""11Q3"",""11Q4"",""12Q1"",""12Q2"",""12Q3"",""12Q4"",""13Q1"",""13Q2"",""13Q3"",""13Q4"",""14Q1"",""14Q2"",""14Q3"",5403.675741,6773.504993,7231.117289,7835.55156,5236.709983,5526.619467,6555.781711,11464.72728,7210.068674,7501.610403,8670.903486,10872.93518,8209.022658,8153.393088,10196.44775,13244.50201,8356.732878,10188.44157,10601.32205,12617.82102,11786.52641,10044.98676,11006.0051,15101.9456,10992.27282,11421.18922,10731.31198),ncol=2,byrow=FALSE)

#-------------------------------------------------------------------------------
# The frequency of the data. 1/4 for QUARTERLY, 1/12 for MONTHLY

Frequency &lt;- 1/4

#-------------------------------------------------------------------------------
# How many quarters/months to forecast

Forecast.horizon &lt;- 4

#-------------------------------------------------------------------------------
# The first date in the series. Use c(8, 1) to denote 2008 q1

Start.date &lt;- c(8, 1)

#-------------------------------------------------------------------------------
# The dates of the forecasts

Forecast.dates &lt;- c(""14Q4"", ""15Q1"", ""15Q2"", ""15Q3"")

#-------------------------------------------------------------------------------
# Selects the data column from Input.data

Data.col &lt;- as.numeric(Input.data[, 2])

#-------------------------------------------------------------------------------
# Turns the Data.col into a time-series

Data.col.ts &lt;- ts(Data.col, deltat=Frequency, start = Start.date)

#-------------------------------------------------------------------------------
# A character vector of the dates from Input.data

Dates.col &lt;- as.character(Input.data[,1])

#------- Transform ------------------------------------------------------------------------
# Starts the testing to see if the data should be logged

transform.method &lt;- round(BoxCox.lambda(Data.col.ts, method = ""loglik""), 5)

log.values &lt;- seq(0, 0.24999, by = 0.00001)
sqrt.values &lt;- seq(0.25, 0.74999, by = 0.00001)

which.transform.log &lt;- transform.method %in% log.values
which.transform.sqrt &lt;- transform.method %in% sqrt.values

if (which.transform.log == ""TRUE""){
  as.log &lt;- ""log""
  Data.new &lt;- log(Data.col.ts)
} else {
  if (which.transform.sqrt == ""TRUE""){
    as.log &lt;- ""sqrt""
    Data.new &lt;- sqrt(Data.col.ts)
  } else {
    as.log &lt;- ""no""
    Data.new &lt;- Data.col.ts
  }
}

#----- Find best ARIMA model ---------------------------------------------------

a &lt;- permutations(n = 3, r = 6, v = c(0:2), repeats.allowed = TRUE)
a &lt;- a[ifelse((a[, 1] + a[, 4] &gt; 2 | a[, 2] + a[, 5] &gt; 2 | a[, 3] + a[, 6] &gt; 2),
              FALSE, TRUE), ]

Arimafit &lt;- matrix(0,
                   ncol  = length(Data.new),
                   nrow  = length(a[, 1]),
                   byrow = TRUE)

totb &lt;- matrix(0, ncol = 1, nrow = length(a[, 1]))
arimaerror &lt;- matrix(0, ncol = length(Data.new), nrow = 1)

for (i in 1:length(a[, 1])){
  ArimaData.new &lt;- try(Arima(Data.new,
                             order    = a[i, c(1:3)],
                             seasonal = list(order = a[i, c(4:6)]),
                             method   = ""ML""),
                       silent = TRUE)

  if (is(ArimaData.new, ""try-error"")){
    ArimaData.new &lt;- arimaerror
  } else {
    ArimaData.new &lt;- ArimaData.new
  }

  arimafitted &lt;- try(fitted(ArimaData.new), silent = TRUE)

  if (is(arimafitted, ""try-error"")){
    fitarima &lt;- arimaerror
  } else {
    fitarima &lt;- arimafitted
  }

  if (as.log == ""log""){
    Arimafit[i, ] &lt;- c(exp(fitarima))
    Datanew &lt;- c(exp(Data.new))
  } else {
    if (as.log == ""sqrt""){
      Arimafit[i, ] &lt;- c((fitarima)^2)
      Datanew &lt;- c((Data.new)^2)
    } else {
      Arimafit[i, ] &lt;- c(fitarima)
      Datanew &lt;- c(Data.new)
    }
  }

  data &lt;- c(Datanew)

  arima.fits &lt;- c(Arimafit[i, ])

  fullres &lt;- data - arima.fits

  v &lt;- acf(fullres, plot = FALSE)

  w &lt;- pacf(fullres, plot = FALSE)

  if (v$acf[2]&gt;0.4|v$acf[2]&lt;(-0.4)|v$acf[3]&gt;0.4|v$acf[3]&lt;(-0.4)|v$acf[4]&gt;0.4|v$acf[4]&lt;(-0.4)|v$acf[5]&gt;0.4|v$acf[5]&lt;(-0.4)|v$acf[6]&gt;0.4|v$acf[6]&lt;(-0.4)|v$acf[7]&gt;0.4|v$acf[7]&lt;(-0.4)|w$acf[1]&gt;0.4|w$acf[1]&lt;(-0.4)|w$acf[2]&gt;0.4|w$acf[2]&lt;(-0.4)|w$acf[3]&gt;0.4|w$acf[3]&lt;(-0.4)|w$acf[4]&gt;0.4|w$acf[4]&lt;(-0.4)|w$acf[5]&gt;0.4|w$acf[5]&lt;(-0.4)|w$acf[6]&gt;0.4|w$acf[6]&lt;(-0.4)){
    totb[i] &lt;- ""n""
  } else {
    totb[i] &lt;- sum(abs(w$acf[1:4]))
  }

  j &lt;- match(min(totb), totb)

  order.arima &lt;- a[j, c(1:3)]

  order.seasonal.arima &lt;- a[j, c(4:6)]
}

#----- ARIMA -------------------------------------------------------------------
# Fits an ARIMA model with the orders set
Arima.Data.new &lt;- Arima(Data.new,
                        order    = order.arima,
                        seasonal = list(order=order.seasonal.arima),
                        method   = ""ML"")

#-------------------------------------------------------------------------------
# Forecasts from the ARIMA model

suppressWarnings(forecast.Data.new &lt;- forecast(Arima.Data.new,
                                               h        = ifelse(frequency(Arima.Data.new) &gt; 1, 2 * frequency(Arima.Data.new), 10),
                                               simulate = TRUE,
                                               fan      = TRUE))
</code></pre>
"
"0.177014545523257","0.194760075661811","124388","<p>I have a code which tests each possible order of ARIMA and selects the best model by choosing the one with the absolute minimum sum of lags from the PACF graph. The code then proceeds to add weight to recent errors and runs an optimization on the parameters to get the minimum mean absolute error.</p>

<p>The code runs fine and gives excellent results (e.g 0.2% MAPE etc) however once the parameters have been optimized the ACF and PACf graphs show lags outside the threshold.</p>

<p>I would like to add into my code a loop which does the following:</p>

<p>if any of the first 4 lags of the ACF or PACF graphs of the residuals found from the optimized ARIMA model are outside the threshold (2/sqrt(n)) then the optimization is re-run but doesn't allow those parameters to be selected/those parameters are skipped in the optimization process.</p>

<p>Here is my code:</p>

<pre><code>suppressMessages(library(lmtest))
suppressMessages(library(car))
suppressMessages(library(tseries))
suppressMessages(library(forecast))
suppressMessages(library(TTR))
suppressMessages(library(geoR))
suppressMessages(library(MASS))
suppressMessages(gtools))
#-------------------------------------------------------------------------------
Data.col&lt;-c(5403.676,6773.505, 7231.117, 7835.552, 5236.710, 5526.619, 6555.782,11464.727, 7210.069, 7501.610, 8670.903,10872.935, 8209.023, 8153.393,10196.448,13244.502, 8356.733,10188.442,10601.322,12617.821, 11786.526,10044.987,11006.005,15101.946,10992.273,11421.189,10731.312)
#-------------------------------------------------------------------------------
# Turns the Data.col into a time-series

Data.col.ts &lt;- ts(Data.col, deltat=(1/4), start = c(8,1))

#-------------------------------------------------------------------------------
# Starts the testing to see if the data should be logged

trans&lt;- BoxCox.lambda(Data.col, method = ""loglik"")
categ&lt;-as.character( c(cut(trans,c(0,0.25,0.75,Inf),right=FALSE)) )
Data.new&lt;-switch(categ,
                 ""1""=log(Data.col.ts),
                 ""2""=sqrt(Data.col.ts),
                 ""3""=Data.col.ts
)

#----- Weighting ---------------------------------------------------------------
fweight &lt;- function(x){
  PatX &lt;- 0.5+x 
  return(PatX)
}

#Split the integral to several intervals, and pick the weights accordingly

integvals &lt;- rep(0, length.out = length(Data.new))
for (i in 1:length(Data.new)){
  integi &lt;- integrate(fweight, lower = (i-1)/length(Data.new), upper= i/length(Data.new))
  integvals[i] &lt;- 2*integi$value
}

#----- Find best ARIMA model ---------------------------------------------------

a &lt;- permutations(n = 3, r = 6, v = c(0:2), repeats.allowed = TRUE)
a &lt;- a[ifelse((a[, 1] + a[, 4] &gt; 2 | a[, 2] + a[, 5] &gt; 2 | a[, 3] + a[, 6] &gt; 2),
              FALSE, TRUE), ]

Arimafit &lt;- matrix(0,
                   ncol  = length(Data.new),
                   nrow  = length(a[, 1]),
                   byrow = TRUE)

totb &lt;- matrix(0, ncol = 1, nrow = length(a[, 1]))
arimaerror &lt;- matrix(0, ncol = length(Data.new), nrow = 1)

for (i in 1:length(a[, 1])){
  ArimaData.new &lt;- try(Arima(Data.new,
                             order    = a[i, c(1:3)],
                             seasonal = list(order = a[i, c(4:6)]),
                             method   = ""ML""),
                       silent = TRUE)

  if (is(ArimaData.new, ""try-error"")){
    ArimaData.new &lt;- arimaerror
  } else {
    ArimaData.new &lt;- ArimaData.new
  }

  arimafitted &lt;- try(fitted(ArimaData.new), silent = TRUE)

  if (is(arimafitted, ""try-error"")){
    fitarima &lt;- arimaerror
  } else {
    fitarima &lt;- arimafitted
  }

  if (categ==""1""){
    Arimafit[i, ] &lt;- c(exp(fitarima))
    Datanew &lt;- c(exp(Data.new))
  } else {
    if (categ==""2""){
      Arimafit[i, ] &lt;- c((fitarima)^2)
      Datanew &lt;- c((Data.new)^2)
    } else {
      Arimafit[i, ] &lt;- c(fitarima)
      Datanew &lt;- c(Data.new)
    }
  }

  data &lt;- c(Datanew)

  arima.fits &lt;- c(Arimafit[i, ])

  fullres &lt;- data - arima.fits

  v &lt;- acf(fullres, plot = FALSE)

  w &lt;- pacf(fullres, plot = FALSE)

  if (v$acf[2]&gt;(2/sqrt(length(Data.col)))|v$acf[2]&lt;(-(2/sqrt(length(Data.col))))|v$acf[3]&gt;(2/sqrt(length(Data.col)))|v$acf[3]&lt;(-(2/sqrt(length(Data.col))))|v$acf[4]&gt;(2/sqrt(length(Data.col)))|v$acf[4]&lt;(-(2/sqrt(length(Data.col))))|v$acf[5]&gt;(2/sqrt(length(Data.col)))|v$acf[5]&lt;(-(2/sqrt(length(Data.col))))|v$acf[6]&gt;(2/sqrt(length(Data.col)))|v$acf[6]&lt;(-(2/sqrt(length(Data.col))))|v$acf[7]&gt;(2/sqrt(length(Data.col)))|v$acf[7]&lt;(-(2/sqrt(length(Data.col))))|w$acf[1]&gt;(2/sqrt(length(Data.col)))|w$acf[1]&lt;(-(2/sqrt(length(Data.col))))|w$acf[2]&gt;(2/sqrt(length(Data.col)))|w$acf[2]&lt;(-(2/sqrt(length(Data.col))))|w$acf[3]&gt;(2/sqrt(length(Data.col)))|w$acf[3]&lt;(-(2/sqrt(length(Data.col))))|w$acf[4]&gt;(2/sqrt(length(Data.col)))|w$acf[4]&lt;(-(2/sqrt(length(Data.col))))|w$acf[5]&gt;(2/sqrt(length(Data.col)))|w$acf[5]&lt;(-(2/sqrt(length(Data.col))))|w$acf[6]&gt;(2/sqrt(length(Data.col)))|w$acf[6]&lt;(-(2/sqrt(length(Data.col))))){
    totb[i] &lt;- ""n""
  } else {
    totb[i] &lt;- sum(abs(w$acf[1:4]))
  }

  j &lt;- match(min(totb), totb)

  order.arima &lt;- a[j, c(1:3)]

  order.seasonal.arima &lt;- a[j, c(4:6)]
}

#----- ARIMA -------------------------------------------------------------------
# Fits an ARIMA model with the orders set
stAW &lt;- Arima(Data.new, order= order.arima, seasonal=list(order=order.seasonal.arima), method=""ML"")
parSW &lt;- stAW$coef
    WMAEOPT &lt;- function(parSW)
    {
      ArimaW &lt;- Arima(Data.new, order = order.arima, seasonal=list(order=order.seasonal.arima), 
                      include.drift=FALSE, method = ""ML"", fixed = c(parSW))
      errAR &lt;- c(abs(resid(ArimaW)))
      WMAE &lt;- t(errAR) %*% integvals 
      return(WMAE)
    }
    OPTWMAE &lt;- optim(parSW, WMAEOPT, method=""SANN"", set.seed(2), control = list(fnscale= 1, maxit = 5000))
    # Alternatively, set  method=""Nelder-Mead"" or method=""L-BFGS-B"" 
    parS3 &lt;- OPTWMAE$par
Arima.Data.new &lt;- Arima(Data.new, order = order.arima, seasonal=list(order=order.seasonal.arima), 
                        include.drift=FALSE, method = ""ML"", fixed = c(parS3))
</code></pre>

<p>Before the parameters are optimized it gives a graph like this:
<img src=""http://i.stack.imgur.com/cjY1x.png"" alt=""enter image description here""></p>

<p>After the parameters are optimized it gives a graph like this:
<img src=""http://i.stack.imgur.com/6HKXl.png"" alt=""enter image description here""></p>

<p>I want to stop this happening in the second picture. Is this possible to do using <code>optim</code>?</p>
"
"0.136398867894095","0.14124491030929","126072","<p>I want to understand how forecast from STL function in R works. So, I am not giving any reproducible code here.</p>

<p>Below is the procedure that I worked on time series</p>

<ol>
<li><p>I used STL decomposition on my time series.</p></li>
<li><p>Checked residuals component from step 1 for white noise using Box.test</p></li>
<li>Found that residuals are not white-noise. So, used ARIMA model to fit a forecasting model.
Now, my task is to compute forecast values that consist of a. Seasonal and Trend component from step 1 above b. Residuals component from ARIMA model - from step 3 above.</li>
</ol>

<p>If I use</p>

<pre><code>forecast(stl(..)), 
</code></pre>

<p>it gives me</p>

<pre><code> Point Forecast     Lo 80    Hi 80    Lo 95    Hi 95 
</code></pre>

<p>However, I am interested in only seasonal and trend parts of forecast. How can I get seasonal trend components?</p>

<p>What components does constitute forecast(stl(..))</p>

<p>Please advise.</p>
"
"0.0482242822170412","0.0499376169438922","128709","<p>I wonder how the Arima() function in R computes the AIC. Applying the standard formula AIC= 2*k - 2 LN(L) (with k number of parameters and L maximized value of likelihood) doesn't reproduce the displayed result. Does the estimated variance counts as additional parameter?</p>

<p>Thank you for helping,</p>

<p>laterstat </p>

<p>PS: Here is an example:</p>

<pre><code>arima(x = example, order = c(2, 0, 2), method = ""ML"")

Coefficients:
          ar1      ar2     ma1     ma2  intercept
      -0.7508  -0.0367  1.3156  0.5433    -0.0183
s.e.   0.2591   0.1772  0.2424  0.1958     0.0244

sigma^2 estimated as 0.03748:  log likelihood = 35.67,  aic = -59.33

&gt; 5*2-2*35.67
</code></pre>

<p>[1] -61.34</p>
"
"0.0681994339470473","0.0706224551546449","128730","<p>If i understand correctly, the ARIMA function produces an estimate for the mean of the process instead of the intercept. It is possible to transform the mean into the intercept: mean= 1-Sum(AR-Coefficients). Is it also possible to transform the standard error of the mean into standard error of the intercept? </p>

<p>As an example:</p>

<pre><code>arima(x = example, order = c(2, 0, 2), method = ""ML"")

Coefficients:
          ar1      ar2     ma1     ma2  intercept
      -0.7508  -0.0367  1.3156  0.5433    -0.0183
s.e.   0.2591   0.1772  0.2424  0.1958     0.0244

sigma^2 estimated as 0.03748:  log likelihood = 35.67,  aic = -59.33

# mean:
&gt; 1+0.7508+0.0367 
[1] 1.7875
</code></pre>

<p>Thanks a lot</p>

<p>laterstat     </p>
"
"NaN","NaN","129358","<p>Suppose you fit a time series with the <code>ets</code> function from the <code>forecast</code> package in <code>R</code>:</p>

<pre><code>library(forecast)
fit &lt;- ets(USAccDeaths, model=""MAM"")
summary(fit)

# ETS(M,A,M) 
# 
# Call:
#   ets(y = USAccDeaths, model = ""MAM"") 
# 
# Smoothing parameters:
#   alpha = 0.4761 
# beta  = 0.0326 
# gamma = 1e-04 
# ...more output...
</code></pre>

<p>Is it possible to generate confidence intervals for the parameters (e.g. <code>alpha</code>, <code>beta</code>, <code>gamma</code>)?  If methods exist, are they restricted to certain taxonomies?  That is, can this method be extended to ARIMA models?</p>
"
"0.139211511597426","0.158573231218845","132845","<p>I have a problem in interpreting what the <code>arima</code> function in R is doing.  I have the following code:</p>

<pre><code>x &lt;- 1000*.8^(0:100)
arima(x, order = c(1,0,0), include.mean = F)
</code></pre>

<p>The resulting coefficient is ""0.9988"".  But I would think the coefficient should be exactly ""0.8"", since <code>x[t] = 0.8 * x[t-1]</code>.</p>

<p>I must be missing something that R is doing in processing the data.</p>

<p>Any help would be appreciated.</p>

<p><strong>New Info I:</strong>
If I change the function to </p>

<pre><code>arima(x, order = c(1,0,0), include.mean = F, method=""CSS"")
</code></pre>

<p>then it solves for the correct coefficient of 0.8.</p>

<p>My problem that I am generally finding that the function <code>arima</code> with <code>order = c(1,0,0)</code> is often producing different results to:</p>

<pre><code>lm(x[-1] ~ I(x[-length(x)]))
</code></pre>

<p>for all sorts of different time series that I am reviewing. </p>

<p><strong>New Info II:</strong>
Some of the comments and answers below are concerned that there is no random fluctuation in my data.  I did this to make the problem as simple as possible, but even if you add in random fluctuation, <code>arima</code> still produces the same wrong results in the default case.  To add insult to injury, <code>arima</code> will sometimes get the right answer if you change <code>method = ""CSS""</code>. This suggest that there is perhaps a computational issue with <code>arima</code> and not my misunderstanding the statistical model. Here is an extended set of two examples that highlight the problem (I ignore the intercept difference between <code>lm</code> and <code>arima</code> as these two items are not the same thing, but the coefficients should be.</p>

<pre><code>set.seed(1)
# Example 2
x &lt;- rep(1000,200)
for (i in 2:200) x[i]=x[i-1]*.8 + runif(1)*100 
plot(x,type=""l"")
arima(x, order = c(1,0,0))  #Incorrect answer:  coefficient = 0.9944
arima(x, order = c(1,0,0), method=""CSS"") # correct answer: coefficient = 0.7915
lm(x[-1] ~ I(x[-length(x)])) # correct answer: coefficient = 0.7914

# Example 3
x &lt;- rep(0,200)
for (i in 2:200) x[i]=x[i-1]*.8 + runif(1)*.2 
plot(x,type=""l"")
arima(x, order = c(1,0,0))  #Incorrect answer:  coefficient = 0.8836
arima(x, order = c(1,0,0), method=""CSS"") # correct answer: coefficient = 0.8158
lm(x[-1] ~ I(x[-length(x)])) # correct answer: coefficient = 0.8158
</code></pre>
"
"NaN","NaN","133504","<p>For data at a weekly level, I'm giving frequency as 52 for the auto.arima function. What happens when a year has 53 weeks? How does it affect the forecast?</p>
"
"0.241299953435539","0.201820476096712","135565","<p>I have a few questions about turing a univariate time series into a multivariate time series and optimizing the predictors. Here is the univariate data:</p>

<pre><code>index
22
26
34
33
40
39
39
45
50
58
64
78
51
60
80
80
93
100
96
108
111
119
140
164
103
112
154
135
156
170
146
156
166
176
193
204
</code></pre>

<p>My first step here was to of course create a ts object in R and visualize the data:</p>

<pre><code>tsData &lt;- ts(data = dummyData, start = c(2012,1), end = c(2014,12), frequency = 12)

     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
2012  22  26  34  33  40  39  39  45  50  58  64  78
2013  51  60  80  80  93 100  96 108 111 119 140 164
2014 103 112 154 135 156 170 146 156 166 176 193 204

plot(tsData)
</code></pre>

<p>I interpreted this plot as a deterministic time series with a trend and perhaps a bit of seasonality</p>

<p><img src=""http://i.stack.imgur.com/YD0vW.png"" alt=""enter image description here"">
Examining the acf and pacf plot confirms the trend component of the time series</p>

<p><img src=""http://i.stack.imgur.com/c8eu3.png"" alt=""enter image description here""></p>

<p>My first question has to do with creating trend &amp; seasonal variables for the time series using the decompose() function in R which yields the following plots:</p>

<p><img src=""http://i.stack.imgur.com/9xXYx.png"" alt=""enter image description here""></p>

<p>I understand that the decompose() function in R has created a list of vectors for the trend, seasonal and random components of the original time series but what am I suppose to do with them? Should I cbind() them to my univariate data and model:</p>

<pre><code>lm(index ~ trend + seasonal + random)

         index     trend    seasonal       random
Jan 2012    22        NA -23.8940972           NA
Feb 2012    26        NA -19.4357639           NA
Mar 2012    34        NA   6.8350694           NA
Apr 2012    33        NA  -7.5399306           NA
May 2012    40        NA   4.3142361           NA
Jun 2012    39        NA   9.5017361           NA
Jul 2012    39  45.20833  -6.3524306   0.14409722
Aug 2012    45  47.83333  -0.8315972  -2.00173611
Sep 2012    50  51.16667  -1.1232639  -0.04340278
Oct 2012    58  55.04167   2.2517361   0.70659722
Nov 2012    64  59.20833  11.2100694  -6.41840278
Dec 2012    78  63.95833  25.0642361 -11.02256944
Jan 2013    51  68.87500 -23.8940972   6.01909722
Feb 2013    60  73.87500 -19.4357639   5.56076389
Mar 2013    80  79.04167   6.8350694  -5.87673611
Apr 2013    80  84.12500  -7.5399306   3.41493056
May 2013    93  89.83333   4.3142361  -1.14756944
Jun 2013   100  96.58333   9.5017361  -6.08506944
Jul 2013    96 102.33333  -6.3524306   0.01909722
Aug 2013   108 106.66667  -0.8315972   2.16493056
Sep 2013   111 111.91667  -1.1232639   0.20659722
Oct 2013   119 117.29167   2.2517361  -0.54340278
Nov 2013   140 122.20833  11.2100694   6.58159722
Dec 2013   164 127.75000  25.0642361  11.18576389
Jan 2014   103 132.75000 -23.8940972  -5.85590278
Feb 2014   112 136.83333 -19.4357639  -5.39756944
Mar 2014   154 141.12500   6.8350694   6.03993056
Apr 2014   135 145.79167  -7.5399306  -3.25173611
May 2014   156 150.37500   4.3142361   1.31076389
Jun 2014   170 154.25000   9.5017361   6.24826389
Jul 2014   146        NA  -6.3524306           NA
Aug 2014   156        NA  -0.8315972           NA
Sep 2014   166        NA  -1.1232639           NA
Oct 2014   176        NA   2.2517361           NA
Nov 2014   193        NA  11.2100694           NA
Dec 2014   204        NA  25.0642361           NA
</code></pre>

<p>When I use the auto.arima function in the forecast package is this all happening under the hood? It seems to me that the auto.arima() selected a MA(1) term and a differencing term to deal with the trend? Is my interpretation correct? What is drift?</p>

<pre><code>plot(forecast(auto.arima(tsData, stepwise=FALSE)))

Forecast method: ARIMA(0,0,1)(0,1,0)[12] with drift        

Model Information:
Series: tsData 
ARIMA(0,0,1)(0,1,0)[12] with drift         

Coefficients:
         ma1   drift
      0.9622  4.5780
s.e.  0.4698  0.4352

sigma^2 estimated as 176.6:  log likelihood=-44.52
AIC=95.05   AICc=96.25   BIC=98.58

Error measures:
                    ME     RMSE      MAE        MPE     MAPE       MASE
Training set 0.2459764 7.673429 4.967187 -0.7272714 4.661455 0.08876581
                   ACF1
Training set -0.0791942
</code></pre>

<p><img src=""http://i.stack.imgur.com/3TWL6.png"" alt=""enter image description here""></p>

<p>What happens if I'm interested in expanding the model to include other time series variables such as spend_1 and spend_2? do I need to create trend and seasonal and random variables for each of these spend variables or do I just plug them into the auto.arima as external variables:</p>

<pre><code>auto.ariam(tsData, xreg=spendData, stepwise= FALSE)

spend_1 spend_2
0   0
0   0
0   0
0   0
0   0
0   209
0   0
0   0
0   239
0   0
0   553
0   216
0   0
0   161
0   449
107 0
53  0
120 81
242 0
100 80
482 0
708 81
54  240
688 0
80  0
254 108
183 84
104 191
183 84
243 167
0   108
0   0
0   191
0   191
0   167
0   0
</code></pre>

<p>Once I build this multivariate time series model how do I interpret the coefficients for spend_1 and spend_2? How do to optimize them in order to maximize the index variable where the model was something like:</p>

<pre><code>lm(index ~ spend_1 + spend_2 + trend + seasonal + random)
</code></pre>

<p>Thanks all for the advice please let me know if I can clarify anything further.</p>
"
"0.153132662757169","0.172988979511468","138108","<p>Using the attached data that has been recently updated I am not able to obtain a statistically significant forecast. The data is extremely seasonal. The data is stored here for easy replication: </p>

<p><a href=""http://ge.tt/1uihVfA2/v/0?c"" rel=""nofollow"">http://ge.tt/1uihVfA2/v/0?c</a></p>

<pre><code># 1. Make a R timeseries out of the rawdata: specify frequency &amp; startdate
gIIP &lt;- ts(Data, frequency=12, start=c(2003,11))
print(gIIP)
plot.ts(gIIP, type=""l"", col=""blue"", ylab=""MTD Ships"", lwd=2,
        main=""Full data"")
grid()
</code></pre>

<p>Using the auto.arima function I don't need to factor a Box-Cox because the auto.arima factors that into selecting the best model. </p>

<p>Upon ""selecting the best model"" I  The best model suggested was Arima(order = c(0, 0, 1),
      seasonal = list(order = c(1, 0, 1), period = 12) with non-zero mean </p>

<pre><code># 5. Perform estimation
library(forecast)
library(zoo)
library(stats)
auto.arima(gIIP, d=NA, D=NA, max.p=12, max.q=12,
           max.P=2, max.Q=2, max.order=12, max.d=2, max.D=2,
           start.p=2, start.q=2, start.P=1, start.Q=1,
           stationary=FALSE, seasonal=TRUE,
           ic=c(""aicc"",""aic"", ""bic""), stepwise=FALSE, trace=TRUE,
           approximation=FALSE | frequency(gIIP)&gt;12), xreg=NULL,
           test=c(""kpss"",""adf"",""pp""), seasonal.test=c(""ocsb"",""ch""),
           allowdrift=TRUE, lambda=TRUE, parallel=FALSE, num.cores=4
</code></pre>

<p>)</p>

<p>then proceed to conduct accuracy diagnostics but unable to obtain any output.</p>

<pre><code>#Check standard error etc of ""fitted"" ARIMA
pos.arima &lt;- function(gIIP, order = c(0, 0, 1),
      seasonal = list(order = c(1, 0, 1), period = 12),
      xreg = NULL, include.drift=TRUE, 
      transform.pars = TRUE,
      fixed = NULL, init = NULL,
      method = c(""CSS-ML"", ""ML"", ""CSS""), 
      optim.method = ""BFGS"",
      optim.control = list(), kappa = 1e6)

acf(pos.arima) 
pacf(pos.arima)
</code></pre>

<p>The following step to conduct an ex ante (out of sample forecast) but also unable to obtain a statistically significant forecast---forecast with lowest standard error rate. I tested this by removing the last 5 observations to test the model. </p>

<pre><code># 7. Forecast Out-Of-Sample ---this used to work
fit &lt;- Arima(gIIP, order = c(0, 0, 1), seasonal = list(order = c(1, 0, 1), period = 12),
             xreg = TRUE, include.mean = TRUE, transform.pars = TRUE, 
             fixed = NULL, init = NULL, method = c(""CSS-ML"", ""ML"", ""CSS""), 
             optim.method = ""BFGS"", optim.control = list(), kappa = 1e6)
plot(forecast(fit,h=9))
print(forecast(fit,h=9))
</code></pre>

<p>Used to obtain output here. Can you please help me diagnose why there ARIMA model is not working like it once did for me? Thank you for your time.  </p>
"
"NaN","NaN","140780","<p>i'm referring to <a href=""https://www.otexts.org/fpp/8/7"" rel=""nofollow"">https://www.otexts.org/fpp/8/7</a></p>

<p>what is the difference between arima() and Arima() function in r?</p>

<p>when to use these functions?</p>
"
"0.0482242822170412","0","140791","<p>I ran PCA in R using the principal() function in the ""psych"" package.</p>

<p>Suppose my dataset is called ""data"" (118 rows and 8 columns).
The 8 variables are answers from a questionnaire, with a scale that ranges from 1 to 7.</p>

<p>From the PCA, 2 principal components are extracted. Next, I ran again the principal function in order to apply the varimax orthogonal rotation.</p>

<pre><code>pc_rotated &lt;- principal(data, nfactors=2, rotate=""varimax"", scores=TRUE)
</code></pre>

<p>The scores could be extracted by:</p>

<pre><code>scores &lt;- pc_rotated$scores
</code></pre>

<p>My question is: how do I destandardize the scores I get? </p>

<p>I need the two components in the original scale (1 to 7).</p>

<p>Edit: Further question, how this function ( principal() ) calculates the loadings? I calculate the eigenvectors of the correlation matrix but these are different from the loadings proposed by the function.</p>
"
"0.0482242822170412","0.0499376169438922","143049","<p>I have heating power data from one year (8670 observations). I also have regressors for day length and temperature (8670 observations also). </p>

<p>I would like to add seasonality with 24h (1 day)  168h (1 week) periods to an ARMA model. Is there an effective way to construct a this kind of seasonal matrix (with day length and temperature data appended to; so a $8670\times(167+2)$ matrix). The application where I would use this is the <code>auto.arima</code> functions xreg argument.</p>
"
"0.0862662185627507","0.111663906120888","143358","<p>I'm working on a forecast for the following data:</p>

<pre><code>data &lt;-
c(1932, 4807, 6907, 8650, 10259, 11374, 8809, 6745, 7429, 
8041, 9740, 10971, 11953, 9227, 7401, 8355, 9681, 10438, 
11092, 11543, 9181, 7428, 8358, 10049, 10938, 12280, 
13063, 10022, 8125, 8763, 9330, 9919, 11309, 12169, 11063, 
10112, 10621, 11506, 12425, 12929, 13025, 10938, 9437, 
9910, 11104, 11985, 13024, 13962, 11900, 9576, 9590, 
10740, 11689, 13084, 13829, 11975, 10224, 10493, 11899, 
12697, 13959, 14415, 11650, 9477, 11166, 12327, 13238, 
13801, 13493, 11118, 9073, 9954, 11077, 12509, 12985, 
13380, 11454, 9265, 10053, 11443, 12132, 13733, 13850, 
11560, 9401, 9921, 11401, 12622, 14224, 14289, 12097, 
9623, 10630, 11572, 12816, 14180, 14125, 11667, 9328, 
9936, 11159, 12536, 13953, 13840, 11430, 9313, 9926, 
11557, 12428, 13802, 13041, 9927, 7448, 9143, 10872, 
12331, 14370, 14496, 13237, 11176, 11936, 12661, 14442, 
15005, 15359, 12871, 10505, 11231, 12078, 13307, 14027, 
14368, 12057, 9965, 10121, 11414, 13375, 14525, 14686, 
12243, 9833, 10722, 11778, 13143, 14844, 14856, 12745, 
9134, 7856, 9429, 11539, 13241, 14324, 12102, 10136, 
11107, 12028, 13999, 15130, 15488, 13379, 11028, 11708, 
13280, 14665, 15362, 15600, 12950, 10716, 10988, 12350, 
14163, 15264, 15724, 13374, 11764, 12711, 13239, 14849, 
15455, 15914, 13541, 10570, 9376, 10132, 11725, 12328, 
13105, 11022, 9710, 10659, 12068, 12890, 14242, 14294, 
11847, 9776, 10681, 12413, 13571, 14344, 14500, 12234, 
9961, 10699, 11626, 13135, 14387, 15282, 13028, 11211, 
11992, 13524, 15131, 15741, 15357, 12489, 9985, 10786, 
11492, 13851, 14509, 14751, 12327, 10023, 11315, 12363, 
13487, 14944, 15006, 12290, 9867, 11540, 12179, 14094, 
14941, 15006, 13585, 10769, 11408, 12634, 14073, 15361, 
15236, 13151, 9580, 8934, 10128, 12475, 13890, 14740, 
12617, 10358, 11648, 12418, 14094, 15127, 15775, 13647, 
11281, 11773, 13407, 15441, 15601, 15951, 13865, 11447, 
12422, 13725, 15766, 16389, 16868, 15221, 12503, 12780, 
14525, 16479, 17032, 17403, 14553, 12484, 13204, 13792, 
14896, 15673, 16332, 14196, 11749, 12977, 13886, 14931, 
15955, 16037, 14082, 11271, 12512, 13942, 16362, 17456, 
17446, 15509, 13069, 13524, 14918, 16161, 17524, 18138, 
14604, 12993, 13763, 14945, 16686, 17717, 17947, 15744, 
13388, 13177, 14588, 16075, 16705, 17074, 14415, 12766, 
13372, 14033, 14300, 12508, 11502, 9391, 7689, 9613, 
12291, 14448, 15075, 15670, 13929, 10989, 11875, 13409, 
15203, 15654, 16150, 13387, 10931, 11492, 12479, 13674, 
14519, 14241, 11685, 9486, 9990, 11440, 12415, 13505, 
12103, 10311, 8267, 7510, 8595, 10620, 11664, 3182, 6241, 
9365, 10965, 12372, 9958, 8088, 9290, 10665, 12132, 12827, 
13040, 10692, 8882, 9538, 10027, 12086, 13276, 13107, 
10680, 9136, 10744, 11733, 13334, 14654, 14830, 12189, 
9613, 11399, 12837, 13661, 15007, 15579, 12268, 9703, 
10627, 12077, 13287, 14459, 14825, 11958, 10049, 11512, 
12770, 13869, 14873, 15233, 12056, 9654, 10386, 11465, 
13354, 14601, 15161, 12324, 9782, 10791, 12502, 14111, 
14914, 15250, 12366, 10333, 11638, 12449, 13518, 14637, 
14756, 12011, 9878, 10976, 12464, 13674, 14979, 15312, 
12106, 10127, 11666, 12843, 13910, 15024, 15333, 12308, 
9992, 11278, 13364, 14966, 15231, 15507, 13744, 11417, 
12232, 14414, 15245, 15988, 15168, 11905, 9165, 10536, 
12570, 14106, 15204, 15509, 12821, 10321, 11282, 13133, 
14174, 15099, 14750, 12817, 10384, 11368, 12994, 14591, 
16154, 15904, 12784, 10737, 11865, 13809, 14721, 15202, 
15322, 12722, 10741, 11991, 13546, 14716, 15817, 15879, 
12679, 10390, 11524, 13140, 14426, 15613, 16212, 13088, 
10720, 11730, 13776, 14477, 15758, 15922, 13119, 9220, 
8372, 10239, 12397, 14740, 15550, 13306, 10833, 11892, 
13630, 15186, 16154, 16678, 12898, 10485, 11313, 13705, 
15572, 16086, 16305, 14129, 11066, 12251, 13830, 15345, 
16550, 16518, 13700, 10890, 12301, 14163, 15890, 16985, 
17544, 15337, 12633, 13383, 12813, 12051, 13149, 13636, 
10914, 9617, 10619, 12224, 13954, 15325, 15473, 12418, 
9730, 11214, 12572, 14565, 15287, 15721, 12519, 10689, 
11662, 13139, 14902, 16374, 16392, 13895, 11777, 12948, 
14326, 15625, 16745, 16980, 13946, 11181, 12665, 13678, 
15269, 16279, 16634, 14399, 11142, 11900, 13800, 14783, 
16626, 16861, 13917, 11228, 12531, 14206, 15773, 16344, 
16930, 13945, 11110, 12427, 14085, 15627, 16854, 17106, 
14677, 10410, 8550, 10626, 13366, 15337, 16460, 13619, 
11630, 12582, 13926, 15297, 16715, 17036, 14063, 11368, 
12246, 14111, 15525, 16900, 17272, 14254, 11961, 13155, 
14579, 16260, 17187, 17919, 15493, 13162, 13771, 15231, 
15836, 16880, 16976, 14728, 12106, 13030, 13848, 15344, 
16475, 17122, 13601, 10921, 12043, 14114, 15846, 16190, 
17125, 13769, 10768, 12336, 13849, 16138, 17507, 18050, 
15492, 12905, 12847, 14181, 15967, 16704, 17762, 14882, 
12591, 13807, 14959, 16933, 17369, 17453, 14351, 11582, 
13102, 14328, 16185, 16321, 16843, 13773, 11053, 12199, 
14147, 14470, 12598, 11916, 9185, 7903, 9742, 12691, 
15153, 15945, 16254, 13630, 11437, 12235, 14040, 15161, 
15995, 16291, 12944, 10947, 12055, 13444, 14852, 16029, 
16361, 13658, 10885, 11604, 13030, 13959, 14291, 14786, 
12002, 9014, 7610, 7426, 9602, 11077, 12544, 11334, 5710, 
9874, 11949, 10321, 8945, 10152, 11821, 13434, 15187, 
15269, 12661, 10699, 12040, 13154, 14149, 15472, 16569, 
13008, 10521, 11674, 13272, 14025, 15803, 16791, 13615, 
11043, 12448, 13929, 15158, 16610, 17520, 13900, 11095, 
11735, 13652, 14939, 16001, 16265, 13371, 11198, 11583, 
13377, 15361, 16420, 16765, 13800, 10866, 12026, 13908, 
14902, 16044, 16807, 13694, 11475, 13009, 14453, 16231, 
17093, 17411, 14433, 12242, 13035, 14304, 16309, 17026, 
16811, 13986, 11812, 13216, 14397, 16026, 17780, 17463, 
14717, 12029, 13046, 14820, 16626, 17564, 17802, 14134, 
13158, 15356, 16573, 16887, 17494, 17326, 13525, 11517, 
12410, 13817, 14933, 16399, 17019, 14008, 11808, 12599, 
14639, 16339, 17521, 17820, 14444, 11530, 13352, 14997, 
16038, 17631, 17614, 15601, 15176, 16930, 17979, 18772, 
19728, 19452, 16272, 14006, 15510, 17299, 17774, 18345, 
19080, 16486, 14242, 15465, 16973, 17971, 19068, 19075, 
15606, 13315, 14784, 16505, 17910, 18586, 18315, 15659, 
13621, 14673, 16037, 17467, 17972, 17676, 15452, 11850, 
10959, 13641, 15217, 16813, 17641, 15404, 13102, 14391, 
15764, 17326, 17715, 17947, 15272, 13078, 13962, 15372, 
18292, 18569, 16427, 13374, 14725, 15957, 17425, 18530, 
19251, 17094, 13711, 15275, 16663, 18254, 19023, 19787, 
16636, 14398, 15392, 16302, 15844, 14301, 14559, 11739, 
10080, 11690, 14352, 16702, 17810, 17898, 15159, 12527, 
14250, 15788, 17012, 18219, 17743, 15183, 12633, 14033, 
15528, 16984, 18041, 18388, 15248, 12831, 14289, 16143, 
17340, 18863, 18597, 15984, 13697, 14653, 16143, 17262, 
17805, 18565, 16147, 14734, 16548, 17410, 18044, 18705, 
18462, 15706, 13242, 14977, 16168, 17683, 18224, 18454, 
15784, 14003, 16605, 18013, 19361, 19204, 18970, 16655, 
12928, 11502, 13233, 15211, 16883, 17454, 15043, 12953, 
14515, 15846, 17501, 18922, 18903, 16175, 13492, 14150, 
15710, 18297, 18872, 19490, 15921, 13935, 14943, 16457, 
18425, 19975, 20440, 17716, 15059, 16086, 17290, 18477, 
19896, 20115, 17580, 15001, 15640, 17915, 18951, 20029, 
20221, 16653, 15063, 15726, 16849, 18121, 18843, 19112, 
16516, 13960, 15255, 16910, 18895, 20091, 20663, 17698, 
15441, 16775, 18158, 19897, 20424, 20111, 17784, 15044, 
16869, 17773, 19783, 21255, 20632, 18081, 15891, 17180, 
18143, 20197, 20926, 20639, 18407, 16313, 16998, 17860, 
19177, 19618, 19919, 17662, 16033, 17439, 18741, 18108, 
16641, 16319, 13221, 11160, 12783, 14876, 16831, 18379, 
18858, 16191, 14632, 16089, 16828, 18169, 19512, 18828, 
17364, 15516, 17065, 18245, 18684, 19472, 19235, 16885, 
14854, 14526, 12921, 12675, 14884, 15284, 13492, 11457, 
5938, 9694, 9429, 9142, 10648, 13235, 15610, 16868, 17364, 
16043, 14497, 15329, 16839, 17548, 18818, 19320, 15884, 
13834, 14748, 15784, 16729, 18274, 19138, 17413, 15394, 
16596, 17853, 18934, 20310, 20165, 18870, 16562, 16823, 
18051, 18816, 20410, 21211, 18551, 16274, 17289, 18317, 
20259, 19993, 19831, 18166, 16517, 17114, 17763, 19011, 
20541, 19974, 18105, 16130, 17422, 18472, 20213, 20721, 
20803, 19250, 16246, 16582, 18410, 19559, 20821, 20412, 
18576, 16272, 16917, 19027, 19917, 20418, 21188, 18382, 
16842, 17911, 19126, 20471, 21120, 20756, 18190, 15873, 
16924, 18468, 19579, 20877, 20726, 18525, 16110, 17480, 
19313, 20323, 20661, 20541, 18284, 16124, 17312, 18361, 
19170, 19945, 20548, 17605, 15973, 17488, 17444, 19086, 
19775, 19827, 17269, 14616, 15690, 16469, 18626, 19288, 
20111, 17769, 15738, 17060, 18885, 20010, 21371, 21541, 
18682, 15971, 16714, 18659, 19934, 21499, 22118, 18952, 
16025, 18120, 18897, 20630, 20286, 21077, 17710, 14857, 
16050, 17877, 19928, 21299, 21202, 18858, 14339, 13172, 
15521, 17434, 19823, 20679, 18288, 16798, 18673, 20628, 
21462, 22720, 22241, 20064, 17327, 18720, 19896, 19710, 
21185, 21916, 19661, 17134, 18027, 19449, 20912, 21234, 
21950, 19495, 17023, 18473, 19080, 20875, 21031, 21492, 
20091, 17511, 18834, 19126, 19922, 21215, 19017, 15506, 
12854, 14605, 16279, 18129, 20043, 21248, 18518, 15467, 
16586, 18277, 18915, 20597, 21244, 19024, 16294, 17234, 
18786, 20960, 21345, 22068, 19774, 17491, 18279, 19809, 
20757, 21618, 22131, 20214, 17581, 18321, 19590, 21486, 
22492, 23194, 20020, 16819, 17892, 18948, 20921, 21696, 
22549, 19559, 16404, 17301, 18659, 20430, 22300, 22569, 
19630, 16800, 17898, 19584, 21190, 21926, 22359, 20157, 
15823, 14136, 15930, 18341, 21044, 21204, 18994, 16973, 
18171, 19378, 20794, 22442, 22144, 19874, 17859, 18703, 
19082, 20781, 21860, 21536, 20172, 18429, 19221, 19824, 
21326, 22504, 23381, 21733, 19231, 20312, 21994, 22609, 
23317, 23074, 22005, 19209, 20734, 22513, 23017, 23698, 
24385, 22512, 19471, 20061, 21235, 22351, 22532, 22869, 
20409, 17908, 18722, 19894, 20960, 21999, 22125, 20797, 
19091, 19910, 20463, 22106, 22737, 22827, 21695, 19498, 
20180, 21204, 22272, 22803, 22808, 20979, 18952, 20365, 
20875, 22944, 23022, 22786, 21284, 19302, 20394, 21144, 
22633, 23511, 23355, 21979, 19988, 20143, 21966, 22574, 
19974, 19410, 15641, 13265, 14880, 16838, 19262, 19941, 
20479, 18929, 17760, 18078, 19055, 20553, 21732, 21671, 
19218, 18485, 18864, 20278, 21120, 21747, 21087, 17982, 
15115, 16518, 16282, 15032, 15658, 14966, 12172, 10336, 
12669, 14238, 14031, 12441, 13313, 11047, 10158, 12438, 
14255, 16434, 17873, 18481, 16360, 14479, 15595, 17392, 
18878, 19999, 19958, 16748, 13852, 14931, 16410, 18097, 
19654, 19480, 16387, 14515, 15205, 16854, 18544, 19510, 
20382, 17838, 14878, 15041, 16661, 19008, 20265, 20947, 
18048, 16472, 16434, 18250, 19571, 21148, 20117, 17788, 
14321, 14996, 15779, 17789, 18804, 18934, 17488, 15095, 
15859, 16691, 18369, 20012, 21073, 18029, 15582, 17247, 
18608, 19783, 20322, 20908, 18221, 15919, 17107, 18404, 
19262, 21741, 21514, 19798, 17410, 17973, 18469, 17910, 
14901)
</code></pre>

<p>The <code>ts.plot(data)</code> gives:<img src=""http://i.stack.imgur.com/E6WU0.jpg"" alt=""enter image description here""></p>

<p>With this data, I'm looking to forecast the values for the next year. This data is victim to both weekly and yearly seasonality. Due to this, I first attempted to use <code>tbats</code> from the <code>forecast</code> package but received an improper forecast that mirrors that found at <a href=""http://www.github.com/robjhyndman/forecast/issues/87"" rel=""nofollow"">http://www.github.com/robjhyndman/forecast/issues/87</a></p>

<p>Instead, I used the following code:</p>

<pre><code>n&lt;-length(data)
bestfit &lt;- list(aicc=Inf)
bestk &lt;- 0
for(i in 1:20)
{
fit &lt;- auto.arima(data, xreg = fourier(1:n,i,m1) + fourier(1:n,i,m2), max.p=10, max.q=10, max.d=2, stepwise=FALSE, ic=""aicc"", allowdrift=TRUE)
if(fit$aicc &lt; bestfit$aicc)
{
    bestfit &lt;- fit
    bestk &lt;- i
}
}

k &lt;- bestk

bestfit &lt;- auto.arima(data, xreg = fourier(1:n,k,m1) + fourier(1:n,k,m2), max.p=10, max.q=10, max.d=2, stepwise=FALSE, ic=""aicc"", allowdrift=TRUE)
accuracy(bestfit)
fc &lt;- forecast(bestfit, xreg = fourier((n+1):(n+365),k,m1) + fourier((n+1):(n+365),k,m2), level = c(50,80,90), bootstrap = TRUE)
plot(fc)
</code></pre>

<p>This code is searching for the best ARIMA model through the use of Fourier terms in <code>xreg</code> to capture both seasonality components. This Fourier function is defined (per <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/longseasonality/</a>) as:</p>

<pre><code>fourier &lt;- function(t,terms,period)
{
  n &lt;- length(t)
  X &lt;- matrix(,nrow=n,ncol=2*terms)
  for(i in 1:terms)
  {
    X[,2*i-1] &lt;- sin(2*pi*i*t/period)
    X[,2*i] &lt;- cos(2*pi*i*t/period)
  }
  colnames(X) &lt;- paste(c(""S"",""C""),rep(1:terms,rep(2,terms)),sep="""")
  return(X)
}
</code></pre>

<p>This forecasting gives me the following plot:<img src=""http://i.stack.imgur.com/2IsSD.jpg"" alt=""enter image description here""></p>

<p>In looking at this forecast, it seems by my naked eye to be off. Just by observation it appears that my forecast is not properly catching the small, but visible, increasing trend. Instead of being ""centered"" around the extended trendline, it appears that the forecast is ""centered"" around the mean of the entire dataset.</p>

<p>First off, am I doing something that is just blatantly wrong? (my mind is a little fuzzy this morning)</p>

<p>If my forecast is correct, how is it that it falls so much below the extended trendline?</p>

<p>Lastly, are there any other suggestions which might be beneficial to my forecasting?</p>
"
"0.0681994339470473","0.0706224551546449","143636","<p>I have daily visitors data for the last 10 years. I want to do some basic tests like which is the busiest day, which is the busiest month, busiest week etc. I used <code>auto.arima</code> function with argument <code>xreg</code> to find out the coefficients of all the days of the week, week of the month. This is the output I got:</p>

<pre><code>&gt; summary(arima1)
Series: dailysea 
ARIMA(1,1,2)                    

Coefficients:
          ar1      ma1      ma2         Sun        Mon         Tue        Wed         Thu
      -0.1250  -0.4506  -0.3712  -1466.6853  -3623.175  -3895.0555  -3722.146  -3327.4288
s.e.   0.1207   0.1117   0.0891    325.7253    386.738    379.8793    379.883    386.7512
            Fri
      -2146.910
s.e.    325.736

sigma^2 estimated as 7776468:  log likelihood=-6808.5
AIC=13637   AICc=13637.31   BIC=13682.92

Training set error measures:
                   ME     RMSE      MAE  MPE MAPE      MASE         ACF1
Training set 59.63838 2784.809 1952.625 -Inf  Inf 0.8353728 -0.001839015
</code></pre>

<p>Can I use these coefficients to conclude that Saturday is the busiest followed by Sunday, Friday etc.? Also I have infinite MAPE which is not making sense to me.</p>
"
"0.119349009407333","0.14124491030929","144158","<p>I am trying to do time series analysis and am new to this field. I have daily count of an event from 2006-2009 and I want to fit a time series model to it. Here is the progress that I have made:</p>

<pre><code>timeSeriesObj = ts(x,start=c(2006,1,1),frequency=365.25)
plot.ts(timeSeriesObj)
</code></pre>

<p>The resulting plot I get is:</p>

<p><img src=""http://i.stack.imgur.com/q2Gf5.jpg"" alt=""Time Series Plot""></p>

<p>In order to verify whether there is seasonality and trend in the data or not, I follow the steps mentioned in this <a href=""http://stats.stackexchange.com/questions/57705/identify-seasonality-in-time-series-data"">post</a> :</p>

<pre><code>ets(x)
fit &lt;- tbats(x)
seasonal &lt;- !is.null(fit$seasonal)
seasonal
</code></pre>

<p>and in Rob J Hyndman's <a href=""http://robjhyndman.com/hyndsight/detecting-seasonality/"" rel=""nofollow"">blog</a>:</p>

<pre><code>library(fma)
fit1 &lt;- ets(x)
fit2 &lt;- ets(x,model=""ANN"")

deviance &lt;- 2*c(logLik(fit1) - logLik(fit2))
df &lt;- attributes(logLik(fit1))$df - attributes(logLik(fit2))$df 
#P value
1-pchisq(deviance,df)
</code></pre>

<p>Both cases indicate that there is no seasonality.</p>

<p>When I plot the ACF &amp; PACF of the series, here is what I get:</p>

<p><img src=""http://i.stack.imgur.com/mgBav.jpg"" alt=""ACF"">
<img src=""http://i.stack.imgur.com/p4DYo.jpg"" alt=""PACF""></p>

<p>My questions are:</p>

<ol>
<li><p>Is this the way to handle daily time series data? This <a href=""http://www.r-bloggers.com/forecasting-with-daily-data/"" rel=""nofollow"">page</a> suggests that I should be looking at both weekly and annual patterns but the approach is not clear to me.</p></li>
<li><p>I do not know how to proceed once I have the ACF and PACF plots.</p></li>
<li><p>Can I simply use the auto.arima function?</p>

<p>fit &lt;- arima(myts, order=c(p, d, q)</p></li>
</ol>

<p>*****Updated Auto.Arima results******</p>

<p>When i change the frequency of the data to 7 according to Rob Hyndman's comments <a href=""http://stats.stackexchange.com/questions/14742/auto-arima-with-daily-data-how-to-capture-seasonality-periodicity"">here</a>, auto.arima selects a seasonal ARIMA model and outputs:</p>

<pre><code>Series: timeSeriesObj 
ARIMA(1,1,2)(1,0,1)[7]                    

Coefficients:
       ar1      ma1     ma2    sar1     sma1
      0.89  -1.7877  0.7892  0.9870  -0.9278
s.e.   NaN      NaN     NaN  0.0061   0.0162

sigma^2 estimated as 21.72:  log likelihood=-4319.23
AIC=8650.46   AICc=8650.52   BIC=8682.18 
</code></pre>

<p>******Updated Seasonality Check******</p>

<p>When I test seasonality with frequency 7, it outputs True but with seasonality 365.25, it outputs false. <strong>Is this enough to conclude a lack of yearly seasonality?</strong></p>

<pre><code>timeSeriesObj = ts(x,start=c(2006,1,1),frequency=7)
fit &lt;- tbats(timeSeriesObj)
seasonal &lt;- !is.null(fit$seasonal)
seasonal
</code></pre>

<p>returns:</p>

<pre><code>True
</code></pre>

<p>while </p>

<pre><code>timeSeriesObj = ts(x,start=c(2006,1,1),frequency=365.25)
fit &lt;- tbats(timeSeriesObj)
seasonal &lt;- !is.null(fit$seasonal)
seasonal
</code></pre>

<p>returns:</p>

<pre><code>False
</code></pre>
"
"0.385971512171474","0.375461185287668","144745","<p>I have 17 years (1995 to 2011) of death certificate data related to suicide deaths for a state in the U.S. There is a lot of mythology out there about suicides and the months/seasons, much of it contradictory, and of the literature I've reviewed, I do not get a clear sense of methods used or confidence in results.</p>

<p>So I've set out to see if I can determine whether suicides are more or less likely to occur in any given month within my data set. All of my analyses are done in R.</p>

<p>The total number of suicides in the data is 13,909.</p>

<p>If you look at the year with the fewest suicides, they occur on 309/365 days (85%). If you look at the year with the most suicides, they occur on 339/365 days (93%).</p>

<p>So there are a fair number of days each year without suicides. However, when aggregated across all 17 years, there are suicides on every day of the year, including February 29 (although only 5 when the average is 38).</p>

<p><img src=""http://i.stack.imgur.com/VMQYa.jpg"" alt=""enter image description here""></p>

<p>Simply adding up the number of suicides on each day of the year doesn't indicate a clear seasonality (to my eye).</p>

<p>Aggregated at the monthly level, average suicides per month range from:</p>

<p>(m=65, sd=7.4, to m=72, sd=11.1)</p>

<p>My first approach was to aggregate the data set by month for all years and do a chi-square test after computing the expected probabilities for the null hypothesis, that there was no systematic variance in suicide counts by month. I computed the probabilities for each month taking into account the number of days (and adjusting February for leap years).</p>

<p>The chi-square results indicated no significant variation by month:</p>

<pre><code># So does the sample match  expected values?
chisq.test(monthDat$suicideCounts, p=monthlyProb)
# Yes, X-squared = 12.7048, df = 11, p-value = 0.3131
</code></pre>

<p>The image below indicates total counts per month. The horizontal red lines are positioned at the expected values for February, 30 day months, and 31 day months respectively. Consistent with the chi-square test, no month is outside the 95% confidence interval for expected counts.
<img src=""http://i.stack.imgur.com/XRCzM.jpg"" alt=""enter image description here""></p>

<p>I thought I was done until I started to investigate time series data. As I imagine many people do, I started with the non-parametric seasonal decomposition method using the <code>stl</code> function in the stats package. </p>

<p>To create the time series data, I started with the aggregated monthly data:</p>

<pre><code>suicideByMonthTs &lt;- ts(suicideByMonth$monthlySuicideCount, start=c(1995, 1), end=c(2011, 12), frequency=12) 

# Plot the monthly suicide count, note the trend, but seasonality?
plot(suicideByMonthTs, xlab=""Year"",
  ylab=""Annual  monthly  suicides"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/xSWJm.jpg"" alt=""enter image description here""></p>

<pre><code>     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
1995  62  47  55  74  71  70  67  69  61  76  68  68
1996  64  69  68  53  72  73  62  63  64  72  55  61
1997  71  61  64  63  60  64  67  50  48  49  59  72
1998  67  54  72  69  78  45  59  53  48  65  64  44
1999  69  64  65  58  73  83  70  73  58  75  71  58
2000  60  54  67  59  54  69  62  60  58  61  68  56
2001  67  60  54  57  51  61  67  63  55  70  54  55
2002  65  68  65  72  79  72  64  70  59  66  63  66
2003  69  50  59  67  73  77  64  66  71  68  59  69
2004  68  61  66  62  69  84  73  62  71  64  59  70
2005  67  53  76  65  77  68  65  60  68  71  60  79
2006  65  54  65  68  69  68  81  64  69  71  67  67
2007  77  63  61  78  73  69  92  68  72  61  65  77
2008  67  73  81  73  66  63  96  71  75  74  81  63
2009  80  68  76  65  82  69  74  88  80  86  78  76
2010  80  77  82  80  77  70  81  89  91  82  71  73
2011  93  64  87  75 101  89  87  78 106  84  64  71
</code></pre>

<p>And then performed the <code>stl()</code> decomposition</p>

<pre><code># Seasonal decomposition
suicideByMonthFit &lt;- stl(suicideByMonthTs, s.window=""periodic"")
plot(suicideByMonthFit)
</code></pre>

<p><img src=""http://i.stack.imgur.com/cS5pE.jpg"" alt=""enter image description here""></p>

<p>At this point I became concerned because it appears to me that there is both a seasonal component and a trend. After much internet research I decided to follow the instructions of Rob Hyndman and George AthanaÂ­sopouÂ­los as laid out in their on-line text ""Forecasting: principles and practice"", specifically to apply a seasonal ARIMA model.</p>

<p>I used <code>adf.test()</code> and <code>kpss.test()</code> to assess for <em>stationarity</em> and got conflicting results. They both rejected the null hypothesis (noting that they test the opposite hypothesis).</p>

<pre><code>adfResults &lt;- adf.test(suicideByMonthTs, alternative = ""stationary"") # The p &lt; .05 value 
adfResults

    Augmented Dickey-Fuller Test

data:  suicideByMonthTs
Dickey-Fuller = -4.5033, Lag order = 5, p-value = 0.01
alternative hypothesis: stationary

kpssResults &lt;- kpss.test(suicideByMonthTs)
kpssResults

    KPSS Test for Level Stationarity

data:  suicideByMonthTs
KPSS Level = 2.9954, Truncation lag parameter = 3, p-value = 0.01
</code></pre>

<p>I then used the algorithm in the book to see if I could determine the amount of differencing that needed to be done for both the trend and season. I ended  with 
nd = 1, ns = 0.</p>

<p>I then ran <code>auto.arima</code>, which chose a model that had both a trend and a seasonal component along with a ""drift"" type constant.</p>

<pre><code># Extract the best model, it takes time as I've turned off the shortcuts (results differ with it on)
bestFit &lt;- auto.arima(suicideByMonthTs, stepwise=FALSE, approximation=FALSE)
plot(theForecast &lt;- forecast(bestFit, h=12))
theForecast
</code></pre>

<p><img src=""http://i.stack.imgur.com/qTUi9.jpg"" alt=""enter image description here""></p>

<pre><code>&gt; summary(bestFit)
Series: suicideByMonthFromMonthTs 
ARIMA(0,1,1)(1,0,1)[12] with drift         

Coefficients:
          ma1    sar1     sma1   drift
      -0.9299  0.8930  -0.7728  0.0921
s.e.   0.0278  0.1123   0.1621  0.0700

sigma^2 estimated as 64.95:  log likelihood=-709.55
AIC=1429.1   AICc=1429.4   BIC=1445.67

Training set error measures:
                    ME    RMSE     MAE       MPE     MAPE     MASE       ACF1
Training set 0.2753657 8.01942 6.32144 -1.045278 9.512259 0.707026 0.03813434
</code></pre>

<p>Finally, I looked at the residuals from the fit and if I understand this correctly, since all values are within the threshold limits, they are behaving like white noise and thus the model is fairly reasonable. I ran a <em>portmanteau test</em> as described in the text, which had a p value well above 0.05, but I'm not sure that I have the parameters correct.</p>

<pre><code>Acf(residuals(bestFit))
</code></pre>

<p><img src=""http://i.stack.imgur.com/gso3q.jpg"" alt=""enter image description here""></p>

<pre><code>Box.test(residuals(bestFit), lag=12, fitdf=4, type=""Ljung"")

    Box-Ljung test

data:  residuals(bestFit)
X-squared = 7.5201, df = 8, p-value = 0.4817
</code></pre>

<p>Having gone back and read the chapter on arima modeling again, I realize now that <code>auto.arima</code> did choose to model trend and season. And I'm also realizing that forecasting is not specifically the analysis I should probably be doing. I want to know if a specific month (or more generally time of year) should be flagged as a high risk month. It seems that the tools in the forecasting literature are highly pertinent, but perhaps not the best for my question. Any and all input is much appreciated.</p>

<p>I'm posting a link to a csv file that contains the daily counts. The file looks like this:</p>

<pre><code>head(suicideByDay)

        date year month day_of_month t count
1 1995-01-01 1995    01           01 1     2
2 1995-01-03 1995    01           03 2     1
3 1995-01-04 1995    01           04 3     3
4 1995-01-05 1995    01           05 4     2
5 1995-01-06 1995    01           06 5     3
6 1995-01-07 1995    01           07 6     2
</code></pre>

<p><a href=""https://dl.dropboxusercontent.com/u/1252082/daily_suicide_counts.csv"" rel=""nofollow"">daily_suicide_data.csv</a></p>

<p>Count is the number of suicides that happened on that day. ""t"" is a numeric sequence from 1 to the total number of days in the table (5533).</p>

<p>I've taken note of comments below and thought about two things related to modeling suicide and seasons. First, with respect to my question, months are simply proxies for marking change of season, I am not interested in wether or not a particular month is different from others (that of course is an interesting question, but it's not what I set out to investigate). Hence, I think it makes sense to <strong>equalize</strong> the months by simply using the first 28 days of all months. When you do this, you get a slightly worse fit, which I am interpreting as more evidence towards a lack of seasonality. In the output below, the first fit is a reproduction from an answer below using months with their true number of days, followed by a data set <strong>suicideByShortMonth</strong> in which suicide counts were computed from the first 28 days of all months. I'm interested in what people think about wether or not this adjustment is a good idea, not necessary, or harmful?</p>

<pre><code>&gt; summary(seasonFit)

Call:
glm(formula = count ~ t + days_in_month + cos(2 * pi * t/12) + 
    sin(2 * pi * t/12), family = ""poisson"", data = suicideByMonth)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.4782  -0.7095  -0.0544   0.6471   3.2236  

Coefficients:
                     Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)         2.8662459  0.3382020   8.475  &lt; 2e-16 ***
t                   0.0013711  0.0001444   9.493  &lt; 2e-16 ***
days_in_month       0.0397990  0.0110877   3.589 0.000331 ***
cos(2 * pi * t/12) -0.0299170  0.0120295  -2.487 0.012884 *  
sin(2 * pi * t/12)  0.0026999  0.0123930   0.218 0.827541    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 302.67  on 203  degrees of freedom
Residual deviance: 190.37  on 199  degrees of freedom
AIC: 1434.9

Number of Fisher Scoring iterations: 4

&gt; summary(shortSeasonFit)

Call:
glm(formula = shortMonthCount ~ t + cos(2 * pi * t/12) + sin(2 * 
    pi * t/12), family = ""poisson"", data = suicideByShortMonth)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.2414  -0.7588  -0.0710   0.7170   3.3074  

Coefficients:
                     Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)         4.0022084  0.0182211 219.647   &lt;2e-16 ***
t                   0.0013738  0.0001501   9.153   &lt;2e-16 ***
cos(2 * pi * t/12) -0.0281767  0.0124693  -2.260   0.0238 *  
sin(2 * pi * t/12)  0.0143912  0.0124712   1.154   0.2485    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 295.41  on 203  degrees of freedom
Residual deviance: 205.30  on 200  degrees of freedom
AIC: 1432

Number of Fisher Scoring iterations: 4
</code></pre>

<p>The second thing I've looked into more is the issue of using month as a proxy for season. Perhaps a better indicator of season is the number of daylight hours an area receives. This data comes from a northern state that has substantial variation in daylight. Below is a graph of the daylight from the year 2002. </p>

<p><img src=""http://i.stack.imgur.com/yvVXl.jpg"" alt=""enter image description here""></p>

<p>When I use this data rather than month of the year, the effect is still significant, but the effect is very, very small. The residual deviance is much larger than the models above. If daylight hours is a better model for seasons, and the fit is not as good, is this more evidence of very small seasonal effect? </p>

<pre><code>&gt; summary(daylightFit)

Call:
glm(formula = aggregatedDailyCount ~ t + daylightMinutes, family = ""poisson"", 
    data = aggregatedDailyNoLeap)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.0003  -0.6684  -0.0407   0.5930   3.8269  

Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)      3.545e+00  4.759e-02  74.493   &lt;2e-16 ***
t               -5.230e-05  8.216e-05  -0.637   0.5244    
daylightMinutes  1.418e-04  5.720e-05   2.479   0.0132 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 380.22  on 364  degrees of freedom
Residual deviance: 373.01  on 362  degrees of freedom
AIC: 2375

Number of Fisher Scoring iterations: 4
</code></pre>

<p>I'm posting the daylight hours in case anyone wants to play around with this. Note, this is not a leap year, so if you want to put in the minutes for the leap years, either extrapolate or retrieve the data.</p>

<p><a href=""https://dl.dropboxusercontent.com/u/1252082/state.daylight.2002.csv"" rel=""nofollow"">state.daylight.2002.csv</a></p>

<p>[<strong>Edit</strong> to add plot from deleted answer (hopefully rnso doesn't mind me moving the plot in the deleted answer up here to the question. svannoy, if you don't want this added after all, you can revert it)]</p>

<p><img src=""http://i.stack.imgur.com/WiuvE.png"" alt=""enter image description here""></p>
"
"0.0835269069584557","0.0864944897557338","147279","<p>I am trying to do time series analysis in R. 
I have data time series data set like this. </p>

<pre><code>    Month       Year    Value 
    December    2013    5300
    January     2014    289329.8
    February    2014    596518
    March       2014    328457
    April       2014    459600
    May         2014    391356
    June        2014    406288
    July        2014    644339
    August      2014    251238
    September   2014    386466.5
    October     2014    459792
    November    2014    641724
    December    2014    399831
    January     2015    210759
    February    2015    121690
    March       2015    280070
    April       2015    41336
</code></pre>

<p>Googling I found I can use auto.arima function to forecast the result. 
I managed to write R code to do forecast using auto.arima function </p>

<pre><code>    data &lt;- c(5300,289329.8,596518,328457,459600,391356,406288,644339,251238,386466.5,459792,641724,399831,210759,121690,280070,41336)
    data.ts &lt;- ts(data, start=c(2013, 12), end=c(2015, 4), frequency=12) 
    plot(data.ts)
    fit &lt;- auto.arima(data.ts)
    forec &lt;- forecast(fit)
    plot(forec)
</code></pre>

<p>Problem is my forecast result always remain same. </p>

<p><img src=""http://i.stack.imgur.com/SuJ6a.png"" alt=""enter image description here""></p>

<p>Could  any tell me what is going wrong. or help me to correct my forecast result. Thanks</p>
"
"0.107832773203438","0.111663906120888","147840","<p>I'm a beginner at Econometrics, and I'm trying to learn the main econometric techniques in <code>R</code>. My doubt is on how to normalize the cointegration matrix to ensure that the coefficient of the cointegration relations is 1 from the output of the <code>ca.jo</code> function for Johansen procedure.</p>

<p>I simulated a multivariate process with 4 series, which 2 are cointegration relations and 2 are common trend:</p>

<pre><code>sigma&lt;-diag(0.5,4)
u&lt;-mvrnorm(n=250,mu=c(0,0,0,0),sigma)
ut1&lt;-arima.sim(n=250,list(order=c(1,0,0),ar=c(0.3)),innov=u[,1])
ut2&lt;-arima.sim(n=250,list(order=c(1,0,0),ar=c(0.5)),innov=u[,2])
y2&lt;-cumsum(u[,3]) ##trend (random walk)
y3&lt;-cumsum(u[,4]) ##trend (random walk)
y1&lt;-0.2*y2+ut1  ##cointegration relation
y4&lt;-0.8*y3+ut2  ##cointegration relation

{ts.plot(y1,y2,y3,y4,col=c(1:4),main=""Simulazione di un sistema con 2 relazioni di cointegrazione"")
legend(""topleft"",legend=c(""y1-cointegrata1"",""y2-common_trend1"",""y3-common_trend2"",""cointegrata2""),pch=15,col=c(1:4))}
</code></pre>

<p>Then for testing the presence of <code>0.2</code> and <code>0.8</code> of my relations in the cointegration matrix, I use <code>ca.jo</code>, this is the output of interest:</p>

<pre><code>xx&lt;-ca.jo(cbind(y1,y2,y3,y4),type=""eigen"",ecdet=""none"",K=2,spec=""transitory"")

xx@Vorg
           y1.l1       y2.l1       y3.l1        y4.l1
y1.l1  1.4948016  0.36787213 -0.02461994 -0.040606696
y2.l1 -0.2951314 -0.07362746  0.05016916  0.362012720
y3.l1 -0.1873146  1.07028673  0.59184042  0.021477244
y4.l1  0.2217581 -1.40606285 -0.05629814  0.009461934
</code></pre>

<p>As I understand, now I have to normalize the parameters for the cointegration relations that are <code>y1</code> and <code>y4</code>, I do in this way:</p>

<pre><code>X&lt;-xx@Vorg
beta.y1 &lt;- X[, 1]/X[1, 1]
beta.y1
     y1.l1      y2.l1      y3.l1      y4.l1 
 1.0000000 -0.1974385 -0.1253107  0.1483528
</code></pre>

<p>As I expected for <code>y2.l1</code> there's a value really close to <code>-0.2</code>...reasoning in the same way for <code>y4</code> I expected to find ""my"" <code>-0.8</code>:</p>

<pre><code>beta.y4 &lt;- X[, 4]/X[4, 4]
beta.y4
    y1.l1     y2.l1     y3.l1     y4.l1 
-4.291585 38.259906  2.269858  1.000000 
</code></pre>

<p>Instead there isn't -0.8 -- I find it is like this:</p>

<pre><code>beta.y4 &lt;- X[, 2]/X[4, 2]
beta.y4
      y1.l1       y2.l1       y3.l1       y4.l1 
-0.26163278  0.05236427 -0.76119409  1.00000000
</code></pre>

<p><code>-0.76</code> for <code>y3.l1</code>. I cannot understand why for the first relation, I divided the first column of the matrix for the element <code>[1,1]</code>, while for the second relation I have to divided the second column (that is the common trend) and not the fourth.</p>
"
"0.0681994339470473","0.0706224551546449","148371","<p>I'm trying to fit a sarima model on the univariate data with 180 points (periodicity=12). I use the auto.arima function in R. After fitting a model to the data, the only problem is the violation of the normality assumption. Then, I refit models after transforming the data but the residuals are still non-normal. For transformation of the data, I use both BoxCox.lambda (in forecast package) and boxcoxnc (in AID package) functions. Can anybody help me to fix this problem?</p>

<pre><code>ser=c(1.887090e+04, -6.023007e+00,  1.193635e-02, -1.455856e-05,  1.064251e-08, -4.953592e-12,  1.517229e-15, -3.090332e-19,
4.137144e-23, -3.491891e-27,  1.682794e-31, -3.527046e-36,  1.904962e+04, -7.394189e+00,  1.600849e-02, -2.077511e-05,
1.585519e-08,-7.587987e-12,    2.363570e-15, -4.859251e-19,  6.534816e-23, -5.525202e-27,  2.663420e-31, -5.580438e-36,
2.009098e+04, -1.061082e+01,  2.319182e-02, -2.917768e-05,  2.171827e-08, -1.019917e-11,  3.133564e-15, -6.379905e-19,
8.520995e-23, -7.168462e-27,  3.442102e-31, -7.188143e-36,  2.067028e+04, -8.034999e+00,  1.761326e-02, -2.240562e-05,
1.680919e-08, -7.961614e-12,  2.469832e-15, -5.081494e-19,  6.861040e-23, -5.835236e-27,  2.831898e-31, -5.974519e-36,
2.233604e+04, -1.033148e+01,  2.287039e-02, -2.952031e-05,  2.255568e-08, -1.086351e-11,  3.419260e-15, -7.123005e-19,
9.720229e-23, -8.341734e-27,  4.079166e-31, -8.660882e-36,  2.392045e+04, -8.246481e+00,  1.585412e-02, -2.056180e-05,
1.636424e-08, -8.253437e-12,  2.710813e-15, -5.858824e-19,  8.245204e-23, -7.258003e-27,  3.624039e-31, -7.827743e-36,
2.636514e+04, -9.886355e+00,  1.951992e-02, -2.504930e-05,  1.963158e-08, -9.789139e-12,  3.190186e-15, -6.856046e-19,
9.606813e-23, -8.427664e-27,  4.196799e-31, -9.046539e-36,  2.866210e+04, -8.866902e+00,  1.734494e-02, -2.387617e-05,
1.957175e-08, -9.993900e-12,  3.300201e-15, -7.152619e-19,  1.008517e-22, -8.892694e-27,  4.448060e-31, -9.626143e-36,
3.002254e+04, -1.007403e+01,  2.151203e-02, -2.984675e-05,  2.427803e-08, -1.226036e-11,  3.997630e-15, -8.550747e-19,
1.190499e-22, -1.037815e-26,  5.140218e-31, -1.103334e-35,  2.929311e+04, -1.123255e+01,  2.282206e-02, -2.968240e-05,
2.323868e-08, -1.146069e-11,  3.677709e-15, -7.777557e-19,  1.073806e-22, -9.301478e-27,  4.584147e-31, -9.800725e-36,
3.306894e+04, -1.396117e+01,  2.326777e-02, -2.724425e-05,  2.023428e-08, -9.690231e-12,  3.055811e-15, -6.392630e-19,
8.763020e-23, -7.552202e-27,  3.707622e-31, -7.901994e-36,  3.491666e+04, -1.315883e+01,  2.554492e-02, -3.194439e-05,
2.437661e-08, -1.184053e-11,  3.762542e-15, -7.896499e-19,  1.082565e-22, -9.310722e-27,  4.554895e-31, -9.664092e-36,
3.775600e+04, -2.101521e+01,  4.695457e-02, -6.000206e-05,  4.510264e-08, -2.134088e-11,  6.600784e-15, -1.352465e-18,
1.817468e-22, -1.538166e-26,  7.429410e-31, -1.560507e-35,  3.699341e+04, -1.019327e+01,  1.761360e-02, -2.428662e-05,
2.084200e-08, -1.112473e-11,  3.796505e-15, -8.415154e-19,  1.204392e-22, -1.072641e-26,  5.402195e-31, -1.174885e-35,
4.009280e+04, -1.887174e+01,  3.441926e-02, -4.161190e-05,  3.152055e-08, -1.535050e-11,  4.911316e-15, -1.040003e-18,
1.440215e-22, -1.251900e-26,  6.190925e-31, -1.327693e-35)

require(""forecast"")
fit=auto.arima(ser,d = 0,D = 1,max.p = 6, max.q = 6,max.P = 6, max.Q = 6, max.order = 25,start.p=1, start.q=1, start.P=1, start.Q=1,stationary = FALSE,
seasonal=TRUE,stepwise=TRUE,trace=TRUE,approximation=FALSE,allowdrift=FALSE,ic=""aicc"")
</code></pre>
"
"0.0835269069584557","0.0864944897557338","148820","<p>I am working on an alogorithm in R to automatize a monthly forecast calculation. I am using, among others, the forecast(method='arima') function from the forecast package to calculate forecast. It is working very well. But for some times series some forecast are quite strange.</p>

<p>Please find below the code i'm using:</p>

<p><code>train_ts&lt;- ts(values, frequency=12)
fit1 &lt;- stl(train_ts, s.window=""periodic"",t.window=24, )
arima &lt;- forecast(fit1,h=forecasthorizon,method ='arima')</code></p>

<p><code>values &lt;- c(27, 27, 7, 24, 39, 40, 24, 45, 36, 37, 31, 47, 16, 24, 6, 21, 35, 36, 21, 40, 32, 33, 27, 42, 14, 21, 5,   19, 31, 32, 19, 36, 29, 29, 24, 42, 15, 24, 21)</code></p>

<p>Here, on the graph, you will see the historical data (black), the fitted value (green) and the forecast(blue). The forecast is not in lines with the fitted value.</p>

<p><img src=""http://i.stack.imgur.com/5530d.png"" alt=""enter image description here"">
As you can see the Forecast is not in line with the history,
My question is ""does a setup for Arima to bound the forecast in line with the history exist"" ?</p>
"
"0.0723364233255618","0.0998752338877845","148990","<p>I've been using <a href=""http://cran.r-project.org/web/packages/dlm/index.html"" rel=""nofollow"">DLM</a> package for modeling my timeseries in state-space format, and then use Kalman Filter to get better 2 step-ahead forecasts. </p>

<p>Even though I've read the <a href=""http://cran.r-project.org/web/packages/dlm/dlm.pdf"" rel=""nofollow"">vignette</a> and parts of <a href=""http://rads.stackoverflow.com/amzn/click/0387772375"" rel=""nofollow"">their book</a>, I'm still struggling when it comes to modeling my ARIMA process in state-space format, specifically as the dlm package understands it. My plan is to use <strong>auto.arima()</strong> to get the best arima model, then represent it as a dlm.</p>

<p>I've seen examples like <a href=""http://stats.stackexchange.com/q/66156/42176"">this</a>.</p>

<pre><code># Estimation of a state space representation of Arima(1,1,1) model and forecast:
level0 &lt;- data.s.g[1]
slope0 &lt;- mean(diff(data.s.g))
buildGap &lt;- function(u) {
trend &lt;- dlmModPoly(dV = 1e-7, dW = exp(u[1 : 2]),
                  m0 = c(level0, slope0),
                  C0 = 2 * diag(2))
gap &lt;- dlmModARMA(ar = ARtransPars(u[4]),ma=u[5], sigma2 = exp(u[3]))
return(trend + gap)}
</code></pre>

<p>and the dlmModARMA part seems clear. What is baffling me is, what to do if I want to get for example ARIMA(1,3,1); how would I represent that 3 differencing part?</p>
"
"0.118124884643724","0.122321680483099","149377","<p>I have an interrupted time series consisting of 200 observations that I've cleaved into two subseries: a sub-series of 150 pre-interrupted observations, and a sub-series of 50 post-interrupted observations. So the overall structure looks like this:</p>

<p>series = pre-interrupted + post-interrupted</p>

<p>I've fitted the pre-interrupted series with an ARIMA(1, 0, 0) model, and everything looks good. When I run the model in R I get good residual plots and something like this:</p>

<pre><code>&gt; arima(pre-interrupted, 1, 0, 0)

## Coefficients:
##           ar1   xmean
##         0.792  51.472
## s.e.    0.082   2.418
</code></pre>

<p>I created an indicator variable, which to my understanding functions like a dummy variable. It looks like this:</p>

<pre><code>indvar &lt;- c(rep(1, 150), rep(0, 50))
</code></pre>

<p>And then I run the arima command with that included:</p>

<pre><code>&gt; arima(series, c(1, 0, 0), xreg = indicator)
</code></pre>

<p>And get something like this:</p>

<pre><code>## Coefficients:
##           ar1   intercept      indvar
##         0.761      59.721      -8.178
## s.e.    0.082       2.996       1.842
</code></pre>

<p>And I'm not exactly sure how to interpret this. Specifically:</p>

<p>Why did the xmean change to intercept? </p>

<p>What is the role of indvar in the context of the output?</p>
"
"0.255330762834431","0.264402272926146","151657","<p>I am running X-13 SEATS on r for monthly data in six years of observations and I think I got a (sufficiently) reasonable fit for the ARIMA model, but the output also shows me that my original series does not have significant seasonality, as it follows:</p>

<pre><code> Call:
seas(x = data_r[, 1], transform.function = ""log"", regression.aictest = NULL, 
    outlier = NULL, arima.model = ""(0 1 1)(1 1 0)"")

Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
AR-Seasonal-12     -0.6194     0.1110  -5.581 2.39e-08 ***
MA-Nonseasonal-01   0.6220     0.1093   5.690 1.27e-08 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

SEATS adj.  ARIMA: (0 1 1)(1 1 0)  Obs.: 60  Transform: log
AICc: 773.4, BIC: 778.4  QS (no seasonality in final):    0  
Box-Ljung (no autocorr.): 20.04   Shapiro (normality): 0.9754
    &gt; 
                qs p-val
    qsori        0     1
    qsorievadj   0     1
    qsrsd        0     1
    qssadj       0     1
    qssadjevadj  0     1
    qsirr        0     1
    qsirrevadj   0     1
</code></pre>

<p>(Still, there is also the fact that the irregular component seems to dominate the SI ratio for some specific months in some years. So maybe there is some dummy variable in the pre-adjustment that I am missing (right?)) </p>

<p>But when I run a regression on Stata for yearly and monthly dummies on the original series -- assuming the seasonality is deterministic --, I cannot reject with an F test that they are all equal to zero. What does this show me? That my ARIMA fit is not correct?</p>

<p>Also, if someone could point me out the difference in interpretation that you should have when running a regression on seasonal dummies and deseasonalizing data with a X-13 SEATS, it would be also very helpful. Maybe that is what I am missing here.</p>

<p>Edit: is it by any chance a common practice, in some particular situations (when you are deseasonalizing a set of series), still deseasonalize a given series even if that series does not show significant seasonality?</p>

<p>Edit2: Adding the results of the automatic adjustment:</p>

<pre><code>Coefficients:
                   Estimate Std. Error z value Pr(&gt;|z|)    
Constant            59.1761    38.0551   1.555  0.11994    
Easter[15]        -903.6151   341.1891  -2.648  0.00809 ** 
MA-Nonseasonal-01    0.4974     0.1138   4.370 1.24e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

SEATS adj.  ARIMA: (0 1 1)  Obs.: 60  Transform: none
AICc: 925.6, BIC: 933.2  QS (no seasonality in final):    0  
Box-Ljung (no autocorr.):  21.9   Shapiro (normality): 0.9498 *

            qs p-val
qsori        0     1
qsorievadj   0     1
qsrsd        0     1
qssadj       0     1
qssadjevadj  0     1
qsirr        0     1
qsirrevadj   0     1 
</code></pre>

<p>I also, I get the following error for the monthplot function with the automatic adjustment: </p>

<pre><code>Error in `[.default`(x$data, , ""seasonal"") : subscript out of bounds
</code></pre>

<p>Following this result from the automatic adjustment, the use of the dummy for easter, with the original specification, does not change that much the first output:</p>

<pre><code>Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
Easter[15]        -0.08307    0.02690  -3.088  0.00202 ** 
AR-Seasonal-12    -0.63353    0.10816  -5.858  4.7e-09 ***
MA-Nonseasonal-01  0.50391    0.12075   4.173  3.0e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

SEATS adj.  ARIMA: (0 1 1)(1 1 0)  Obs.: 60  Transform: log
AICc: 767.9, BIC: 774.3  QS (no seasonality in final):    0  
Box-Ljung (no autocorr.): 29.37   Shapiro (normality): 0.9721  
            qs p-val
qsori        0     1
qsorievadj   0     1
qsrsd        0     1
qssadj       0     1
qssadjevadj  0     1
qsirr        0     1
qsirrevadj   0     1
</code></pre>

<p>Most recent observation: Now I Think I am fairly sure that there is no significant seasonality in this series, but I would be thankful if someone could show me other problems that I might not be considering. Still, I would like a possible canonical/scholarly answer on why I can reject the null hypothesis for the whole set of seasonal dummies being zero (though I had a small result for the F test with my data, ~4, but I still reject the null) and still get a reasonable ARIMA fit with which I cannot reject no seasonality in my original data. Does that have something to do with the difference of the adjustment with ARIMA models and deterministic seasonality? An intuitive answer on this difference would be of some help.</p>
"
"0.0964485644340824","0.0749064254158383","152012","<p>This might fit better here than on stackoverflow, I guess.</p>

<p>I was <a href=""http://stackoverflow.com/questions/30139874/r-dynamic-linear-regression-with-dynlm-package-how-to-predict"">trying to build a dynamic regression model with the dynlm</a> package, but it did not work out. After reading <a href=""http://robjhyndman.com/hyndsight/arimax/"" rel=""nofollow"">this</a> by Hyndman, I now switched to an ARMAX model:</p>

<pre><code>y_t = a_1*x1_t + a_2*x2_t + ... + a_k*xk_t + n_t
</code></pre>

<p>where the error term follows an ARMA model</p>

<pre><code>n_t ~ ARMA(p,q)
</code></pre>

<p>So far I am using the function <code>auto.arima(y, xreg=cbind(x1, ..., xk))</code> from the <code>forecast</code>package, which is doing the job!</p>

<p>As a benchmark I am running a pure multiple regression with <code>lm()</code>, where I make use of the <code>step()</code> function to kick out non relevant variables (about 100 variables, from which 96 are dummies) to optimize the model according to <code>AIC</code>.</p>

<p>The in-sample forecasting for both models is more or less equal. As the ARMAX model always includes <strong>all</strong> independent variables <code>(x1, ..., xk)</code>, I am pretty sure that, if I could apply the <code>step()</code> function on it, I would achieve a further improvement here.</p>

<p>The problem is that the <code>step()</code> function does not work on <code>auto.arima()</code>?!</p>

<p>Do you have any suggestions how I could still do this? Or would I need a totally new approach?</p>

<p>(I have not provide a reproducible example, as this is a rather general question of which methods/functions/packages to use. If the question is not clear enough, please tell me and I will try to provide one)</p>
"
"0.107832773203438","0.111663906120888","152976","<p>My goal is to forecast sensor measurements (e.g. temperature, humidity) in a lightweight (and real-time) fashion.</p>

<p>To this end I use ARIMA forecasting as implemented in R, where I retrain the model for every new forecast. Ideally I'd like to retrain the model as little as possible to save computing power, but this is where I get confused. Let's say we use the hypothetical ARIMA(2,0,2) model as follows:</p>

<p>$$
y_t = 0.3 y\prime_{t-1} + 0.4 y\prime_{t-2} + 0.4 e_{t-1} + 0.5 e_{t-2} + c + e_t
$$</p>

<p>Now say I train some model based on measurements from $$t_0 \text{ to } t_{250}$$ If at this point I want to forecast for t = 251, we use the measurements for t-1=250 and t-2=249 in the above equation. Now I want to forecast for t=252. If I use the same approach (in this case use t-1=251 and t-2=250), is this essentially the same as L-step forecasting? And if setting the n.ahead parameter of the R <a href=""https://stat.ethz.ch/R-manual/R-patched/library/stats/html/predict.arima.html"" rel=""nofollow"">predict</a> function, does this actually work in this fashion (using previously predicted values)?</p>

<p>If I were to fit a model, and then use it for say, 2000 forecasts. Would this essentially be L-step forecasting with L=2000? </p>
"
"0.192897128868165","0.187266063539596","153404","<p>I am fairly new to R so my data manipulation experience isn't as strong as it is with other software packages. I have been primarily using the high level functions that others have written. The forecast package written by Hyndman is fantastic and I have also enjoyed reading his book.</p>

<p>What I am struggling with is how to create ARIMA fitted values that are greater than one step forward. The current function will painlessly create fitted values for a one time period projection on historical data. You can use the accuracy function to determine RMSE, MAPE, MASE to test the accuracy of your modelling forecasts. However, I would like to variably test how well the forecast performs for different projections into the future. </p>

<p>For example, I am currently working with weekly time series data and the current goal is to forecast one month into the future. The fitted values and accuracy function show how well my forecast would have performed if I were only projecting 1 week into the future instead of 4. Is there a quick and efficient way to select how many periods into the future we would like to estimate the accuracy for historical data?</p>

<p>Forgive me if I have missed a fundamental aspect of forecasting. I know that I can simply aggregate my weekly time series data into monthly data and then use my ARIMA model to forecast monthly instead of weekly periods. However, since I have multiple years of data, I am largely concerned with seasonality and it would be nice to see how the data ramps up in each week of the month (also would be nice to account for holidays that may affect weekly results). I also realize that I can simply forecast monthly data and then use cubic spline interpolation.</p>

<p>Thank you in advance!</p>
"
"0.107832773203438","0.111663906120888","154104","<p>I am trying to estimate parameter $d$ for ARFIMA model using different methods: Hurst, ML, fdSperio, fdGPH and the function <code>arfima</code> which selects the best fit automatically. The results shown are too large in difference, and I wonder why. </p>

<p>I attached an image of my coding:</p>

<p><img src=""http://i.stack.imgur.com/ZKz6B.png"" alt=""""></p>

<p>Especially parameter $d$ from <code>arfima</code>, the value is too extreme. Other values are having big differences as well.</p>

<p>Also, this is brief explanation on what I am doing:</p>

<ol>
<li>I am fitting ARFIMA model with my data: monthly Brent Oil Price.</li>
<li>Before that, I will need to estimate the values of $d$, and I am doing that with different methods as mentioned above.</li>
<li>After the values of $d$ are obtained, I will use <code>diffseries</code> to difference the series manually.</li>
<li>Then I will fit those differenced series into ARIMA model to compare their AICs, to find the best model.</li>
</ol>

<p>Am I going to the right way? Please correct me if I am wrong. </p>
"
"0.0482242822170412","0.0499376169438922","154639","<p>I present here two examples one with transformed data and the other without any transformation. In the transformed data case, the upper interval gets enormous large, whereas not in the untransformed case. (Function in forecast package in R)</p>

<pre><code>    dmnd=c(8.6,9.8,11.2,12.4,13.5,15.7,18.6,21.1,22.3,23.6,24.6,26.3,28.3,29.6,33.3,36.4,40.5,44.9,48.4,52.6,56.8,60.5,67.2,73.4,77.8,85.6,94.8,105.5,114.0,118.5,128.3,126.9,132.6,141.2,150.0,160.8,174.6,190.0,198.1,194.1,210.4,230.3,242.4,246.4,255.5)
    #with transformation
    fit &lt;- Arima(dmnd[11:45], order=c(1,2,0), lambda=-0.25)
    prg=forecast(fit,h=16,level=c(95),fan=FALSE,lambda=-0.25)
    prg



Point Forecast     Lo 95      Hi 95
36       262.8665 241.34010   286.8475
37       271.4097 231.18466   320.7765
38       279.9162 216.75679   367.8499
39       288.9224 201.26616   429.9376
40       298.2242 184.91488   513.2452
41       307.9328 168.56079   626.1724
42       318.0273 152.59291   782.3793
43       328.5442 137.35984  1003.7047
44       339.4971 123.06285  1326.8115
45       350.9112 109.82630  1815.8213
46       362.8087  97.70297  2589.1252
47       375.2149  86.69722  3879.8347
48       388.1558  76.77777  6185.0627
49       401.6596  67.89002 10677.4213
50       415.7557  59.96484 20513.6022
51       430.4756  52.92545 45881.0816

#without transformation
fit &lt;- Arima(dmnd[11:45], order=c(1,2,0))
prg=forecast(fit,h=16,level=c(95),fan=FALSE)
prg

 36       263.5024 252.64559 274.3592
 37       271.7410 249.52972 293.9523
 38       279.9288 243.87448 315.9832
 39       288.1275 236.23983 340.0153
 40       296.3239 226.81344 365.8344
 41       304.5208 215.76687 393.2747
 42       312.7176 203.22448 422.2107
 43       320.9144 189.28707 452.5417
 44       329.1112 174.03721 484.1851
 45       337.3079 157.54444 517.0715
 46       345.5047 139.86827 551.1412
 47       353.7015 121.06045 586.3426
 48       361.8983 101.16647 622.6302
 49       370.0951  80.22673 659.9635
 50       378.2919  58.27746 698.3063
 51       386.4887  35.35135 737.6260
</code></pre>
"
"0.170498584867618","0.200096956271494","154641","<p>This question is similar to the following <a href=""http://stats.stackexchange.com/questions/32634/difference-time-series-before-arima-or-within-arima"">question</a> in the sense I am currently doing the differencing and mean removal of the time series outside the <code>Arima</code> function in R. And I do not know how to do these steps within <code>Arima</code> function in R. The reason is that I am trying to perform the following procedure (data <code>dowj_ts</code> can be found at the bottom): </p>

<pre><code>dowj_ts_d1 &lt;- diff(dowj_ts) # differencing at lag 1 (1-B)
drift &lt;- mean(diff(dowj_ts))
dowj_ts_d1_demeaned &lt;- dowj_ts_d1 - mean(dowj_ts_d1) # mean removal
# Maximum Likelihood AR(1) for the mean-corrected differences X_t
fit &lt;- Arima(dowj_ts_d1_demeaned, order=c(1,0,0),include.mean=F, transform.pars = T)
</code></pre>

<p>Note that the <code>drift</code> is actually <code>0.1336364</code>. And <code>summary(fit)</code> gives the table below:</p>

<pre><code>Series: dowj_ts_d1_demeaned 
ARIMA(1,0,0) with zero mean     

Coefficients:
         ar1
      0.4471
s.e.  0.1051

sigma^2 estimated as 0.1455:  log likelihood=-35.16
AIC=74.32   AICc=74.48   BIC=79.01

Training set error measures:
                       ME     RMSE       MAE       MPE     MAPE      MASE
Training set -0.004721362 0.381457 0.2982851 -9.337089 209.6878 0.8477813
                    ACF1
Training set -0.04852626
</code></pre>

<p>Ultimately, I want to predict 2-step ahead forecast of <strong>the original series</strong>, and this starts to become ugly: </p>

<pre><code> tail(c(dowj_ts[1], dowj_ts[1] + cumsum(c(dowj_ts_d1_demeaned,forecast.Arima(fit,h=2)$mean) + drift)),2)
</code></pre>

<p>And currently these are all done outside the <code>Arima</code> function from the <code>forecast</code> package. I know I can do differencing within Arima like this: </p>

<pre><code> Arima(dowj_ts, order=c(1,1,0),include.drift=T,transform.pars = F)
</code></pre>

<p>This gives:</p>

<pre><code>Series: dowj_ts 
ARIMA(1,1,0) with drift         

Coefficients:
         ar1   drift
      0.4478  0.1204
s.e.  0.1059  0.0786

sigma^2 estimated as 0.1474:  log likelihood=-34.69
AIC=75.38   AICc=75.71   BIC=82.41
</code></pre>

<p>But the drift term computed by R is different from the <code>drift = 0.1336364</code> that I computed manually.</p>

<p>So <strong>my question is: how can I differenced the series and then remove the mean of the differenced series within the Arima function ?</strong></p>

<p><strong>Second question:</strong> Why is the drift term estimated by <code>Arima</code> different from the drift term I computed ? In fact, what does the <strong>mathematical model</strong> look like when <code>include.drift = T</code> ? This really confuses me. </p>

<p>Data can be found below: </p>

<pre><code>structure(c(110.94, 110.69, 110.43, 110.56, 110.75, 110.84, 110.46, 
110.56, 110.46, 110.05, 109.6, 109.31, 109.31, 109.25, 109.02, 
108.54, 108.77, 109.02, 109.44, 109.38, 109.53, 109.89, 110.56, 
110.56, 110.72, 111.23, 111.48, 111.58, 111.9, 112.19, 112.06, 
111.96, 111.68, 111.36, 111.42, 112, 112.22, 112.7, 113.15, 114.36, 
114.65, 115.06, 115.86, 116.4, 116.44, 116.88, 118.07, 118.51, 
119.28, 119.79, 119.7, 119.28, 119.66, 120.14, 120.97, 121.13, 
121.55, 121.96, 122.26, 123.79, 124.11, 124.14, 123.37, 123.02, 
122.86, 123.02, 123.11, 123.05, 123.05, 122.83, 123.18, 122.67, 
122.73, 122.86, 122.67, 122.09, 122, 121.23), .Tsp = c(1, 78, 
1), class = ""ts"")
</code></pre>
"
"NaN","NaN","155661","<p>Is there any function in R such as the <code>auto.arima</code> for the ARIMA processes that finds the most suitable GARCH order for a given data?</p>
"
"0","0.0499376169438922","158493","<p>I've been using the R forecast package's <em>auto.arima()</em> function to fit an ARIMA model to my time series data. I want to see how good of a fit the ARIMA model is to my original data. I hope to plot my original time series and the ARIMA simulation on the same plot and see how well they match up. How can I do this?</p>

<p>Thanks!  </p>
"
"0.0835269069584557","0.0576629931704892","158701","<p>I use the svm function (for regression) to make forecast like I would with for exemple the arima function:<br>
<code>fit&lt;-auto.arima(ts)</code><br>
<code>prediction&lt;-forecast(fit,h=20)</code><br>
which returns different attributes : </p>

<blockquote>
  <ol>
  <li><code>prediction$mean</code> which is the actual prediction  </li>
  <li><code>prediction$lower</code> and <code>prediction$upper</code> which are the   <strong>boundaries of the confidence intervals</strong> on each points of the   <code>prediction$mean</code>.  </li>
  </ol>
</blockquote>

<p>I would like the <code>svm</code> function (from <em>e1071</em> package) to return a more detailed answer than just the value (like the <code>forecast()</code> would).<br>
 But I guess it is not implemented in the function yet.
Is there another function to do it ? Or should I use <strong>bootstrap</strong> methods to try to estimate those boundaries? And if I should use this are they pre-implemented version of them instead of using sample over a for loop which is very time-consuming ?</p>
"
"0.0984374038697697","0.122321680483099","159626","<p>I have built an ARIMA model in R with the ""forecast"" package's <code>auto.arima()</code> function. I want to simulate the ARIMA model with the same starting values as the original time series. For example, if my original time series has values from time 0 to time 2000, I want the ARIMA model to simulate the same time period (0 to 2000). If the ARIMA model is ARIMA(5,0,1), I want the values from time 0 to time 4 to be retained in the ARIMA model and the model simulates from that point on using those first 5 points. </p>

<p>I appreciate your help!  </p>
"
"0.127589457900886","0.132122515500744","160612","<p>I have manually discovered that the best model for my time series is the next one (AIC = 244.9):</p>

<p><img src=""http://i.stack.imgur.com/EaH3R.jpg"" alt=""enter image description here""></p>

<p>But auto.arima function tells me that the best model is (0,1,0) with AIC = 247.93:</p>

<p><img src=""http://i.stack.imgur.com/oKgyN.jpg"" alt=""enter image description here""></p>

<p>If I look at the trace I see: </p>

<p><img src=""http://i.stack.imgur.com/C03Vi.jpg"" alt=""enter image description here""></p>

<p>Why can't auto.arima calculate AIC values for the majority of the models and returns Inf? </p>
"
"0.0681994339470473","0.0706224551546449","163774","<p>I have a relatively simple problem, but yet taking some time to solve it. I am using the <code>arimax()</code> function from the <code>TSA</code> package. <em>(Note: not <code>arima()</code> from the <code>stats</code> package.)</em> This is the model: </p>

<pre><code>out &lt;- arimax(sub_s_t_series, order=c(2,0,1), xreg=sub_r_t_series, method=c(""ML""))
</code></pre>

<p>and these are my coefficients:</p>

<pre><code>Call
arimax(x = sub_s_t_series, order = c(2, 0, 1), xreg = sub_r_t_series, method = c(""ML""))

Coefficients:
         ar1      ar2      ma1  intercept     xreg
      1.4825  -0.6613  -0.8516  52745.107  -1.0132
s.e.  0.0295   0.0294   0.0064     40.828   0.0012

sigma^2 estimated as 0.08929:  log likelihood = -105.98,  aic = 221.97
</code></pre>

<p>All I am trying to do is to interpret the results. According to my understanding and the help given in the TSA package, the above ARIMAX(2,0,1) model is represented as follows:
$$
{\rm sub\_s\_t\_series\_hat[k]} = {\rm intercept} + xreg\times {\rm sub\_r\_t\_series[k]} + 
\frac{a_{t[k]}+ma1*a_{t[k-1]}}{a_{t[k]}-ar1*a_{t[k-1]}-ar2*a_{t[k-2]}}  \tag{1} 
$$
where $a_t$ are the residuals. When I use e_t = fitted(out)-sub_s_t_series_hat to measure the error / residuals myself, e_t matches exactly to the values obtained by <code>out[[""residuals""]]</code>.</p>

<p>But when I use (1) as follows: e_t_hat = sub_s_t_series_hat - sub_s_t_series, 
e_t_hat does not match with <code>out[[""residuals""]]</code>, in fact the results deviate by a magnitude of almost 4.</p>

<p><strong>My questions is:</strong> did an ARIMAX(2,0,1) fit would result in (1) or am I missing something?</p>
"
"0.0723364233255618","0.0998752338877845","163878","<p>I want to estimate an ARIMA model on my timeseries, then represent it in state space format, mainly because it will be more responsive to change in pattern.
I used <code>auto.arima</code> from <code>forecast</code> package to find the best ARIMA, and then, if it didn't need differencing, tried to estimate the same ARMA using <code>dlm</code>package. </p>

<pre><code>fit.arima &lt;- auto.arima(training_data_list, seasonal = FALSE)
Series: training_data_list 
ARIMA(1,0,1) with zero mean     

Coefficients:
         ar1      ma1
      0.8247  -0.4913
s.e.  0.0395   0.0618

sigma^2 estimated as 489.7:  log likelihood=-3161.25
AIC=6328.5   AICc=6328.53   BIC=6342.15
</code></pre>

<p>Now the same thing using <code>dlm</code> package:</p>

<pre><code>test.model.arma &lt;- function(u){
  arma  &lt;- dlmModARMA(ar = ARtransPars(u[1]), ma = u[2], sigma2 = exp(u[3]))   # ma = c(u[4]),
  return(arma)
}
init &lt;-c(rep(0,3))
outMLE2 &lt;- dlmMLE(training_data_list, init, test.model.arma,method = ""Nelder-Mead"")
# outMLE2 &lt;- dlmMLE(training_data_list, init, test.model.arma)
if(outMLE2$convergence != 0) {print(""MLE did NOT converge!"")}

dlmModel4 &lt;- test.model.arma(outMLE2$par)
</code></pre>

<p>Now the loglikelihood from the later one is $2523.158$, while the <code>auto.arima</code> gives $3161.25$. the coefficients for $ar$ and $ma$ are very close, so I think it might be unlikely to be the reason.</p>

<pre><code>dput(training_data_list)
structure(c(0.92, 3.76, 2.64, 2.72, -4.48, 4.68, 6.2, 4.16, 22.32, 
28.96, 5.72, 3.44, 29.56, 37.28, 38.16, 31.28, 32.04, 21.32, 
2.88, 7.08000000000001, 52.32, 9.80000000000001, 8.56, 2.24000000000001, 
29.8, 49.2, 23.88, 42.32, -0.08, 3.76, -0.359999999999999, 2.72, 
8.52, 26.68, 7.2, -18.84, -13.68, -3.03999999999999, 10.72, -14.56, 
-16.44, 44.28, -17.84, -8.72000000000003, 3.04000000000002, 30.32, 
-21.12, -13.92, -3.68000000000001, -17.2, -16.44, -7.75999999999999, 
39.8, -1.80000000000001, 25.88, 9.31999999999999, -0.08, 1.76, 
2.64, -5.28, -4.48, 3.68, 1.2, -10.84, -21.68, -1.03999999999999, 
-6.28, 22.44, 62.56, 27.28, 1.16000000000003, -8.72000000000003, 
23.04, -11.68, -40.12, -33.92, -43.68, -26.2, -58.44, 5.24000000000001, 
-33.2, -18.8, -48.12, -57.68, -0.08, 3.76, -2.36, -7.28, -1.48, 
-1.32, 2.2, -6.84, 14.32, 23.96, -6.28, -11.56, -13.44, 14.28, 
-38.84, -31.72, 4.04000000000002, 4.31999999999999, 24.88, -3.91999999999999, 
-19.68, 30.8, 13.56, 4.24000000000001, 22.8, 36.2, 44.88, -17.68, 
-0.08, -1.24, -4.36, -1.28, -5.48, 7.68, 1.2, 14.16, -1.68000000000001, 
9.96000000000001, -3.28, 8.44, 59.56, 33.28, 23.16, 72.28, 48.04, 
36.32, 41.88, 7.08000000000001, 13.32, 20.8, 19.56, -0.759999999999991, 
-4.19999999999999, 31.2, 40.88, -10.68, -0.08, 6.76, 2.64, 2.72, 
7.52, 1.68, 1.2, -8.84, -7.68000000000001, 11.96, 31.72, -5.56, 
38.56, 27.28, -15.84, 60.28, 21.04, -22.68, 33.88, 55.08, 41.32, 
24.8, 6.56, 20.24, 60.8, 36.2, -9.12, 42.32, -0.08, 2.76, 2.64, 
1.72, -1.48, 2.68, 8.2, -18.84, -24.68, -4.03999999999999, 2.72, 
-31.56, -9.44, -11.72, -48.84, -5.72000000000003, 2.04000000000002, 
-19.68, -2.12, 27.08, 7.31999999999999, -22.2, 42.56, 11.24, 
9.80000000000001, 17.2, 25.88, 55.32, -0.08, -3.24, -4.36, 1.72, 
-6.48, 0.68, -0.799999999999997, 6.16, -13.68, -14.04, -2.28, 
12.44, -24.44, -15.72, 13.16, 1.27999999999997, -16.96, -40.68, 
-76.12, -50.92, -49.68, -58.2, -42.44, -36.76, -8.19999999999999, 
-43.8, -71.12, -62.68, -0.08, -3.24, -2.36, -1.28, 8.52, 4.68, 
-0.799999999999997, 18.16, 16.32, -18.04, 14.72, -18.56, 6.56, 
14.28, 53.16, 41.28, 35.04, -40.68, -12.12, -5.91999999999999, 
22.32, 11.8, -16.44, -6.75999999999999, -36.2, 3.19999999999999, 
-8.12, -12.68, -0.08, 3.76, 2.64, -3.28, -0.48, 13.68, 11.2, 
10.16, 9.31999999999999, -15.04, -6.28, -22.56, 14.56, -5.72000000000003, 
53.16, 36.28, 45.04, -12.68, -0.120000000000005, 21.08, -24.68, 
-10.2, -17.44, 3.24000000000001, 8.80000000000001, -11.8, -26.12, 
-27.68, -0.08, 2.76, 0.640000000000001, 2.72, -4.48, -2.32, -4.8, 
18.16, 7.31999999999999, 12.96, 45.72, 37.44, -22.44, 9.27999999999997, 
36.16, 51.28, 7.04000000000002, 54.32, 5.88, -1.91999999999999, 
25.32, 27.8, 15.56, 22.24, 18.8, -38.8, 31.88, 31.32, -0.08, 
6.76, -0.359999999999999, -0.279999999999999, 1.52, 1.68, -3.8, 
18.16, -8.68000000000001, 7.96000000000001, -7.28, 30.44, 12.56, 
51.28, 9.16000000000003, 16.28, -14.96, 50.32, -18.12, -32.92, 
10.32, -8.19999999999999, -5.44, 22.24, 23.8, 33.2, -12.12, 51.32, 
-0.08, -4.24, -2.36, -0.279999999999999, -2.48, -4.32, -4.8, 
0.159999999999997, -17.68, 7.96000000000001, 9.72, 9.44, 1.56, 
-15.72, 4.16000000000003, 6.27999999999997, -28.96, -38.68, -18.12, 
-36.92, -34.68, -25.2, -23.44, -43.76, -59.2, -37.8, -54.12, 
-61.68, -0.08, 0.76, 2.64, 9.72, -1.48, 2.68, 21.2, 13.16, -1.68000000000001, 
10.96, 20.72, 22.44, 23.56, 18.28, -3.83999999999997, -16.72, 
-19.96, 46.32, 47.88, 25.08, 0.319999999999993, 21.8, 14.56, 
32.24, 1.80000000000001, -5.80000000000001, -12.12, 1.31999999999999, 
-0.08, 5.76, -4.36, 4.72, -3.48, -5.32, 4.2, 4.16, -12.68, -14.04, 
-11.28, 0.439999999999998, -13.44, 25.28, 58.16, 29.28, 32.04, 
38.32, 14.88, 32.08, 39.32, 47.8, 16.56, 17.24, -25.2, 13.2, 
27.88, -6.68000000000001, -0.08, -6.24, -2.36, -6.28, -3.48, 
3.68, -1.8, -9.84, 11.32, 4.96000000000001, -26.28, -32.56, -38.44, 
-49.72, -62.84, -40.72, -20.96, -13.68, -52.12, -24.92, -49.68, 
-25.2, -19.44, -82.76, -36.2, -51.8, -60.12, -65.68, 0.92, -3.24, 
-1.36, -0.279999999999999, -19.48, -34.32, -22.8, 1.16, 19.32, 
-2.03999999999999, 27.72, 4.44, 12.56, -53.72, -35.84, -60.72, 
24.04, -15.68, 27.88, 11.08, 12.32, 33.8, 5.56, 3.24000000000001, 
24.8, -21.8, 18.88, -15.68, -0.08, -3.24, -1.36, -0.279999999999999, 
8.52, -5.32, 7.2, -12.84, 9.31999999999999, -0.039999999999992, 
-19.28, 13.44, -4.44, -5.72000000000003, -75.84, -82.72, -39.96, 
-60.68, 17.88, 23.08, 11.32, 38.8, 2.56, -6.75999999999999, 26.8, 
36.2, 27.88, 48.32, -0.08, -2.24, 1.64, 2.72, 2.52, -5.32, 0.200000000000003, 
-10.84, -4.68000000000001, -16.04, -15.28, -9.56, -27.44, -32.72, 
-63.84, -32.72, -49.96, -37.68, 38.88, 27.08, 7.31999999999999, 
5.80000000000001, 5.56, 10.24, -13.2, -20.8, 39.88, 36.32, -0.08, 
-3.24, 1.64, -9.28, -3.48, 1.68, -13.8, -2.84, -12.68, -15.04, 
8.72, 16.44, 7.56, 1.27999999999997, -23.84, -4.72000000000003, 
21.04, -22.68, -47.12, -16.92, -29.68, -32.2, -25.44, -40.76, 
-72.2, -50.8, -54.12, -99.68, -0.08, -4.24, 0.640000000000001, 
11.72, 4.52, -10.32, 1.2, 8.16, 15.32, -5.03999999999999, -14.28, 
-12.56, -34.44, -1.72000000000003, -11.84, -56.72, -25.96, -5.68000000000001, 
10.88, -4.91999999999999, 4.31999999999999, -40.2, 46.56, 20.24, 
5.80000000000001, 21.2, -1.12, 38.32, -0.08, -2.24, 2.64, -1.28, 
4.52, -1.32, -3.8, 1.16, 13.32, -0.039999999999992, 3.72, 2.44, 
19.56, 4.27999999999997, -41.84, -21.72, -29.96, 15.32, 39.88, 
-22.92, 28.32, -11.2, 67.56, 23.24, 25.8, -9.80000000000001, 
54.88, 18.32, -0.08, -1.24, 2.64, -1.28, 10.52, -4.32, -7.8, 
2.16, 16.32, -9.03999999999999, -0.280000000000001, 8.44, -47.44, 
-35.72, 51.16, 12.28, 15.04, 27.32, 19.88, 38.08, 50.32, 35.8, 
-16.44, 25.24, -1.19999999999999, 15.2, 25.88, 21.32, -0.08, 
-2.24, 1.64, 0.720000000000001, -0.48, 2.68, 6.2, -5.84, 13.32, 
11.96, -33.28, -32.56, -39.44, -50.72, 42.16, -38.72, -12.96, 
49.32, -4.12, -21.92, -13.68, -11.2, 2.56, 25.24, 16.8, 34.2, 
25.88, 56.32, -0.08, -2.24, -1.36, -6.28, 6.52, -4.32, -13.8, 
-12.84, -26.68, -15.04, -30.28, -0.560000000000002, 2.56, -28.72, 
58.16, 52.28, -51.96, -30.68, -36.12, -1.91999999999999, -56.68, 
-22.2, -26.44, -21.76, -27.2, -12.8, -58.12, -13.68), .Tsp = c(1, 
25.9642857142857, 28), class = ""ts"")
</code></pre>
"
"0.127589457900886","0.132122515500744","165004","<p>I'm trying to find the best fit line for this data below but no matter what I try, the fit line seems to never be able to account for the lower values as shown below.</p>

<p>The x-values are just dates from 1/1/2014 to 7/20/2015 (566 values), but I don't know how to give you guys the y-values. I have it in my Environment but I don't know how to give you that without copying and pasting from the Console output.</p>

<p><a href=""http://i.stack.imgur.com/KA2LC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/KA2LC.png"" alt=""Data with sinusodial fit""></a></p>

<p>This is the code that I'm using to get that fit line:</p>

<pre><code>wb.loglik=function(theta,y,x,null=NA)
{
  a=theta[1]
  b=theta[2]
  c=theta[3]
  d=theta[4]

  if(!is.na(null))
  {
    d=null
  }

  s2=theta[5]
  n=length(y)
  return((-n/2)*log(s2)-1/(2*s2)*sum((y-(a+b*cos(2*pi*((x-c)/d))))^2))
}
result=optim(par=c(mean(wbbcf),sd(wbbcf),1,365.25,var(wbbcf)/2),
fn=wb.loglik,x=Time,y=wbbcf,control=list(fnscale=-1))
theta=result$par
theta
value=result$value
value
</code></pre>

<p>This is the code to get the plot above:</p>

<pre><code>plot(date,wbbcf,xlim=c(as.Date(""2014-01-01""),as.Date(""2015-07-
    20"")),ylim=range(c(-3.2,0)),xlab=""Date (1/1/2014 to
    7/20/2015)"",ylab=""Total Net with Storage (bcf)"",main=""Total Burn with
    Model"")
par(new=T)
curve(-0.9740582-0.7857229*cos(2*pi*(x-5.9582996)/385.1581090),1,566,
    ylim=range(c(-3.2,0)),col=""blue"",xlab="""",ylab="""",xaxt='n',yaxt='n')
</code></pre>

<p>What else can I do to generate a better fit line for this data? Also, if there's a good way to predict future data, I would appreciate help with that as well.</p>

<p>Sorry in advance, if I'm not giving enough information. Please feel free to ask for any information you need and I will promptly edit the post.</p>

<p>EDIT: I have added the work I did with ARIMA below.</p>

<p>I inputted the following code and got the following results:</p>

<pre><code>forecast::auto.arima(wbbcf)
fit.arima = arima(wbbcf,order=c(0,1,2))
pred.arima = predict(fit.arima,n.ahead=500)
plot(wbbcf,xlim=c(1,800),ylim=range(c(-3.2,1)))
lines(pred.arima$pred,col=""red"")
lines(pred.arima$pred+1.96*pred.arima$se,col=""blue"",lty=3)
lines(pred.arima$pred-1.96*pred.arima$se,col=""green"",lty=3)
</code></pre>

<p><a href=""http://i.stack.imgur.com/W8Hqt.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/W8Hqt.png"" alt=""ARIMA predictions""></a></p>

<p>Here's the <code>auto.arima()</code> output:</p>

<pre><code>Series: wbbcf 
ARIMA(0,1,2)                    

Coefficients:
          ma1      ma2
      -0.3023  -0.3188
s.e.   0.0397   0.0396

sigma^2 estimated as 0.05788:  log likelihood=3.02
AIC=-0.04   AICc=0.01   BIC=12.98
</code></pre>

<p>Is something wrong with what I'm doing with ARIMA?</p>
"
"NaN","NaN","165182","<p>According to <a href=""https://stat.ethz.ch/R-manual/R-devel/library/stats/html/arima.html"" rel=""nofollow"">the manual</a> the <code>arima</code> function in R doesn't transform parameters when optimizing using <code>CSS</code>. Is there a particular (statistical) reason for this?  </p>
"
"0.0681994339470473","0.0706224551546449","166413","<p>I am trying to fit my model using garchFit function in the fGarch package, however I keep on getting this error:</p>

<pre><code>In arima(.series$x, order = c(u, 0, v), include.mean = include.mean) :
possible convergence problem: optim gave code = 1
</code></pre>

<p>Here is my code:</p>

<pre><code>library(quantmod)
library(fGarch)
library(PerformanceAnalytics)
getSymbols(""SPY"")
spyRets=Return.calculate(Cl(SPY),method=c('log')) #log returns
garchFit(~arma(5,5)+garch(1,1),data=tail(spyRets,500)) #gives error
</code></pre>

<p>After looking around I found this answer <a href=""http://stats.stackexchange.com/questions/53051/simulate-arima-by-hand"">Simulate ARIMA by hand</a>
which tells me to add in <code>control=list(maxit = large number)</code> as my solution, however that still does not work:</p>

<pre><code>garchFit(~arma(5,5)+garch(1,1),data=tail(spyRets,500),control=list(maxit = 10000)) #still gives error
</code></pre>

<p>I have two questions. First of all I would like help to make this error disappear? Secondly, if I am trying to find the best fit, does it really matter that optim converges?</p>
"
"0.0681994339470473","0.0706224551546449","167432","<p>I'm using <code>auto.arima</code> function to analyze my data. And here's what I get:</p>

<pre><code>Series: data 
ARIMA(2,1,2) with drift         

Coefficients:
         ar1      ar2      ma1     ma2   drift
      1.6679  -0.8005  -1.2424  0.3125  0.4225
s.e.  0.1007   0.1107   0.1396  0.1413  0.1895

sigma^2 estimated as 17.34:  log likelihood=-438.37
AIC=888.73   AICc=889.3   BIC=906.99
</code></pre>

<p>I'm trying to perform a heteroscedastic test to my residual model using bptest from <code>lmtest</code> package with this code:</p>

<pre><code>&gt; bp&lt;-bptest(lm(residuals(model)~1))
&gt; bp

    studentized Breusch-Pagan test

data:  lm(residuals(model) ~ 1)
BP = 1.231e-30, df = 0, p-value &lt; 2.2e-16
</code></pre>

<p>Am I doing <code>bptest</code> right? When I analyze another data with this code I always get the same <code>df</code> and <code>p-value</code>. </p>

<p><strong>edit:</strong>
here is the data :</p>

<pre><code>    World_Oil_Prices
[1-10] [11-20] [21-30] [31-40] [41-50] [51-60] [61-70] [71-80] [81-90] [91-100] [101-110] [111-120] [121-130] [131-140] [141-150] [151-156]
17.79   22.25   18.73   12.72   16.12   27.49   25.95   18.69   28.28   28.59   37.63   48.75   59.67   53.53   75.91   131.22
17.69   23.51   20.12   12.49   16.24   23.45   27.24   18.52   27.53   29.68   35.54   46.00   54.17   57.22   81.27   121.87
19.46   23.29   19.16   13.80   18.75   27.23   25.02   19.15   24.79   26.88   37.93   43.67   56.63   50.14   90.54   96.85
20.78   20.54   17.24   13.26   20.21   29.62   25.66   19.98   27.89   29.01   42.08   52.55   66.85   54.46   89.76   69.16
19.12   19.42   15.07   11.88   22.37   28.16   27.55   23.64   30.77   29.12   41.65   52.24   63.49   57.78   85.53   46.03
18.56   17.98   14.18   10.41   22.19   29.41   26.97   25.43   32.88   29.95   46.87   58.74   62.26   63.25   93.51   38.60
19.56   19.47   13.24   11.32   24.22   32.08   24.80   25.69   30.36   31.40   42.23   58.20   68.08   66.75   99.32   
20.19   18.02   13.39   10.75   25.01   31.40   25.81   24.49   25.49   31.32   39.09   53.32   66.45   68.29   111.03  
22.14   18.45   13.97   12.86   25.21   32.33   25.03   25.75   26.06   33.67   42.76   49.41   56.38   73.69   123.35  
23.43   18.79   12.48   15.73   27.15   25.28   20.73   26.78   27.91   33.71   44.35   51.66   53.58   67.10   128.33  
</code></pre>

<p>And this is the R code:</p>

<pre><code>library(forecast)
model&lt;-auto.arima(data)
model
library(lmtest)
bp&lt;-bptest(lm(residuals(model)~1))
bp
</code></pre>
"
"0.0835269069584557","0.0864944897557338","168055","<p>I have a line of code in R:</p>

<pre><code>garchFit(substitute(formula~arma(p,q)+garch(1,1),  
list(p=auto.arima(data[i:(i+T-1)])$arma[1],q=auto.arima(data[i:(i+T-1)])$arma[2])),  
data = data[i:(i+T-1)],trace=F)  
</code></pre>

<p>which is fitted repeatedly in a <code>for</code> loop for different <code>i</code> but unfortunately produces an error at when <code>i</code> reaches 3613.</p>

<p>The code is designed to fit ""best-fit"" ARIMA model to the data ANF GARCH(1,1) as there doesn't appear to be automatic ""best fit"" GARCH function in R unlike <code>auto.arima</code>.</p>

<p>The error is: </p>

<pre><code>Error in arima(.series$x, order = c(u, 0, v), include.mean = include.mean) : non-stationary AR part from CSS.
</code></pre>

<p>In addition warning is produced: </p>

<pre><code>In sqrt(diag(fit$cvar)) : NaNs produced.
</code></pre>

<p>Any help on how to solve this issue much appreciated.</p>
"
"0.204882269086533","0.223327812241777","168655","<p>I have got monthly data from 1993 to 2015 and would like to do forecasting on these data. I used tsoutliers package to detect the outliers, but I do not know how do I continue to forecast with my set of data .</p>

<p>This is my code:</p>

<pre><code>product.outlier&lt;-tso(product,types=c(""AO"",""LS"",""TC""))
plot(product.outlier)
</code></pre>

<p>This is my output from tsoutliers package</p>

<pre><code>ARIMA(0,1,0)(0,0,1)[12]                    

Coefficients:
        sma1    LS46    LS51    LS61    TC133   LS181   AO183   AO184   LS185   TC186    TC193    TC200
      0.1700  0.4316  0.6166  0.5793  -0.5127  0.5422  0.5138  0.9264  3.0762  0.5688  -0.4775  -0.4386
s.e.  0.0768  0.1109  0.1105  0.1106   0.1021  0.1120  0.1119  0.1567  0.1918  0.1037   0.1033   0.1040
       LS207    AO237    TC248    AO260    AO266
      0.4228  -0.3815  -0.4082  -0.4830  -0.5183
s.e.  0.1129   0.0782   0.1030   0.0801   0.0805

sigma^2 estimated as 0.01258:  log likelihood=205.91
AIC=-375.83   AICc=-373.08   BIC=-311.19

 Outliers:
    type ind    time coefhat  tstat
1    LS  46 1996:10  0.4316  3.891
2    LS  51 1997:03  0.6166  5.579
3    LS  61 1998:01  0.5793  5.236
4    TC 133 2004:01 -0.5127 -5.019
5    LS 181 2008:01  0.5422  4.841 
6    AO 183 2008:03  0.5138  4.592
7    AO 184 2008:04  0.9264  5.911
8    LS 185 2008:05  3.0762 16.038
9    TC 186 2008:06  0.5688  5.483
10   TC 193 2009:01 -0.4775 -4.624
11   TC 200 2009:08 -0.4386 -4.217
12   LS 207 2010:03  0.4228  3.746
13   AO 237 2012:09 -0.3815 -4.877
14   TC 248 2013:08 -0.4082 -3.965
15   AO 260 2014:08 -0.4830 -6.027
16   AO 266 2015:02 -0.5183 -6.442
</code></pre>

<p><a href=""http://i.stack.imgur.com/qKI4N.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qKI4N.jpg"" alt=""This is my plot""></a></p>

<p>I have these warning messages as well.</p>

<pre><code>Warning messages:
1: In locate.outliers.iloop(resid = resid, pars = pars, cval = cval,  :
  stopped when â€˜maxitâ€™ was reached
2: In locate.outliers.iloop(resid = resid, pars = pars, cval = cval,  :
  stopped when â€˜maxitâ€™ was reached
3: In locate.outliers.oloop(y = y, fit = fit, types = types, cval = cval,  :
  stopped when â€˜maxitâ€™ was reached
4: In arima(x, order = c(1, d, 0), xreg = xreg) :
  possible convergence problem: optim gave code = 1
5: In auto.arima(x = c(5.77, 5.79, 5.79, 5.79, 5.79, 5.79, 5.78, 5.78,  :
  Unable to fit final model using maximum likelihood. AIC value approximated
</code></pre>

<p><strong>Doubts:</strong></p>

<ol>
<li>If I am not wrong, tsoutliers package will remove the outliers it detect and through the use of the dataset with outliers removed, it
will give us the best arima model suited for the data set, is it
correct?</li>
<li>The adjust series data set is being shifted down by a lot due to remove of the level shift,etc. Doesn't this mean that if the forecasting is done on the adjusted series, the output of the forecast will be very inaccurate, since the more recent data are already more than 12, while adjusted data shift it to around 7-8.</li>
<li>What does warning message 4 and 5 means? Does it mean it cannot do auto.arima using the adjusted series?</li>
<li>What does the [12] in ARIMA(0,1,0)(0,0,1)[12] mean? Is it just my frequency/periodicity of my dataset, which I set it to monthly? And does this also means that my data series is seasonal as well? </li>
<li>How do I detect seasonality in my data set? As from the visualisation of the time series plot, I cant see any obvious trend, and if I use the decompose function, it will assume that there is a seasonal trend? So do I just believe what the tsoutliers tell me, where there is seasonal trend, since there is MA of order 1?</li>
<li>How do I continue to do my forecasting with this data after identifying these outliers?</li>
<li><strong>How to incorporate these outliers to other forecasting models - Exponential Smoothing, ARIMA, Strutural Model, Random Walk, theta? I am sure I cannot remove the outliers since there are level shift, and if I only take adjusted series data, the values will be too small, so what do I do?</strong></li>
</ol>

<p><strong>Do I need to add these outliers as regressor in the auto.arima for forecasting? How does this work then?</strong></p>
"
"0.119349009407333","0.14124491030929","169299","<p>I use auto.arima function in R to fit a TS model to a annual data composed of electricity demand. The series is transformed w.r.t Box-Cox lambda due to the prevailing heteroscedasticity and then it is twiced differenced to eliminate the trend in the data. The first ACF/PACF plot (w/o transformation) suggest that an ARIMA model should be fitted to the model; whereas the second ACF/PACF (with transformation) plot suggests that an AR model should be fitted. However both of them depend on the same data. 
In both of the case, the auto.arima function selects the best model as ARIMA(1,2,1) which can be expected according to the first plot but not according to the second plot due to the one spike in PACF.     </p>

<p><a href=""http://i.stack.imgur.com/qoYJH.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qoYJH.jpg"" alt=""untransformed""></a>
<a href=""http://i.stack.imgur.com/FOzNv.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FOzNv.jpg"" alt=""transformed""></a></p>

<pre><code>dmnd=c(8.6,9.8,11.2,12.4,13.5,15.7,18.6,21.1,22.3,23.6,24.6,26.3,28.3,29.6,33.3,36.4,40.5,44.9,48.4,52.6,56.8,60.5,67.2,73.4,77.8,85.6,94.8,105.5,
</code></pre>

<p>114.0,118.5,128.3,126.9,132.6,141.2,150.0,160.8,174.6,190.0,198.1,194.1,210.4,230.3,242.4,246.4,257.2)</p>

<pre><code>x=ts(dmnd,frequency=1)

sdx=diff(x,differences = 2)
x1&lt;-acf(sdx,length(sdx),ylab=""Sample ACF"",main ="""")
x2&lt;-pacf(sdx,length(sdx),ylab=""Sample PACF"",main ="""")

library(FitAR)

#transformation
fit=arima(x,order=c(0,2,0))
BoxCox(fit, interval = c(-1, 1), type = ""BoxCox"")

library(forecast)
tx=BoxCox(x, -0.049)

sdx=diff(tx,differences = 2)
x1&lt;-acf(sdx,length(sdx),ylab=""Sample ACF"",main ="""")
x2&lt;-pacf(sdx,length(sdx),ylab=""Sample PACF"",main ="""")


fit=auto.arima(x,d = 2,D = 0,start.p=0, start.q=0, max.p=5, max.q=5,stationary=FALSE,seasonal=FALSE,stepwise=TRUE,trace=TRUE,approximation=FALSE,allowdrift=TRUE,ic=""aicc"",lambda=-0.049)

par(mfrow=c(1,2)) 
x1&lt;-acf(fit$residuals,length(fit$residuals),ylab=""Sample ACF"",main ="""")
x2&lt;-pacf(fit$residuals,length(fit$residuals),ylab=""Sample PACF"",main ="""")

Box.test(fit$residuals, lag = length(fit$residuals)/5, type = c(""Ljung-Box""), fitdf = length(fit$ coef))

shapiro.test(fit$residuals)

library(TSA)
x.standard=rstandard.Arima(fit)
qqnorm(x.standard,main ="""")
qqline(x.standard)
</code></pre>

<p><strong><em>Results of ARIMA(3,1,0) Model</em></strong> </p>

<p><a href=""http://i.stack.imgur.com/MmgeP.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/MmgeP.jpg"" alt=""Summary""></a></p>

<p><a href=""http://i.stack.imgur.com/ojFY1.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ojFY1.jpg"" alt=""sample ACF/PACf""></a>
<a href=""http://i.stack.imgur.com/Gh94D.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Gh94D.jpg"" alt=""Normality""></a></p>

<pre><code>Shapiro-Wilk normality test

data:  fit$residuals
W = 0.8557, p-value = 5.153e-05

Box-Ljung test

data:  fit$residuals
X-squared = 14.2044, df = 6, p-value = 0.02743
</code></pre>

<p><strong><em>Diagnostics of Residuals ARIMA(1,1,1) transformed</em></strong>
<a href=""http://i.stack.imgur.com/lQGXi.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/lQGXi.jpg"" alt=""ACF/PACF""></a>
<a href=""http://i.stack.imgur.com/twQf2.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/twQf2.jpg"" alt=""Normal""></a></p>

<p><strong><em>auto.arima result for the series without the observations ""32, 40, 41""</em></strong>
<a href=""http://i.stack.imgur.com/9XI2O.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9XI2O.jpg"" alt=""without some observation""></a></p>
"
"0.201109158996187","0.229079684602171","169468","<p>I have monthly time series data, and would like to do forecasting with detection of outliers .</p>

<p><strong>This is the sample of my data set:</strong></p>

<pre><code>       Jan   Feb   Mar   Apr   May   Jun   Jul   Aug   Sep   Oct   Nov   Dec
2006  7.55  7.63  7.62  7.50  7.47  7.53  7.55  7.47  7.65  7.72  7.78  7.81
2007  7.71  7.67  7.85  7.82  7.91  7.91  8.00  7.82  7.90  7.93  7.99  7.93
2008  8.46  8.48  9.03  9.43 11.58 12.19 12.23 11.98 12.26 12.31 12.13 11.99
2009 11.51 11.75 11.87 11.91 11.87 11.69 11.66 11.23 11.37 11.71 11.88 11.93
2010 11.99 11.84 12.33 12.55 12.58 12.67 12.57 12.35 12.30 12.67 12.71 12.63
2011 12.60 12.41 12.68 12.48 12.50 12.30 12.39 12.16 12.38 12.36 12.52 12.63
</code></pre>

<p>I have referred to <a href=""http://stats.stackexchange.com/questions/140163/timeseries-analysis-procedure-and-methods-using-r?lq=1"">Timeseries analysis procedure and methods using R</a>, to do a series of different model of forecasting, however it does not seems to be accurate. In additional, I am not sure how to incorporate the tsoutliers into it as well.</p>

<p>I have got another post regarding my enquiry of tsoutliers and arima modelling and procedure over <a href=""http://stats.stackexchange.com/questions/168655/how-to-interpret-and-do-forecasting-using-tsoutliers-package-and-auto-arima/168869#168869"">here</a> as well.</p>

<p>So these are my code currently, which is similar to link no.1.</p>

<p><strong>Code:</strong></p>

<pre><code>product&lt;-ts(product, start=c(1993,1),frequency=12)

#Modelling product Retail Price

#Training set
product.mod&lt;-window(product,end=c(2012,12))
#Test set
product.test&lt;-window(product,start=c(2013,1))
#Range of time of test set
period&lt;-(end(product.test)[1]-start(product.test)[1])*12 + #No of month * no. of yr
(end(product.test)[2]-start(product.test)[2]+1) #No of months
#Model using different method
#arima, expo smooth, theta, random walk, structural time series
models&lt;-list(
#arima
product.arima&lt;-forecast(auto.arima(product.mod),h=period),
#exp smoothing
product.ets&lt;-forecast(ets(product.mod),h=period),
#theta
product.tht&lt;-thetaf(product.mod,h=period),
#random walk
product.rwf&lt;-rwf(product.mod,h=period),
#Structts
product.struc&lt;-forecast(StructTS(product.mod),h=period)
)

##Compare the training set forecast with test set
par(mfrow=c(2, 3))
for (f in models){
    plot(f)
    lines(product.test,col='red')
}

##To see its accuracy on its Test set, 
#as training set would be ""accurate"" in the first place
acc.test&lt;-lapply(models, function(f){
    accuracy(f, product.test)[2,]
})
acc.test &lt;- Reduce(rbind, acc.test)
row.names(acc.test)&lt;-c(""arima"",""expsmooth"",""theta"",""randomwalk"",""struc"")
acc.test &lt;- acc.test[order(acc.test[,'MASE']),]

##Look at training set to see if there are overfitting of the forecasting
##on training set
acc.train&lt;-lapply(models, function(f){
    accuracy(f, product.test)[1,]
})
acc.train &lt;- Reduce(rbind, acc.train)
row.names(acc.train)&lt;-c(""arima"",""expsmooth"",""theta"",""randomwalk"",""struc"")
acc.train &lt;- acc.train[order(acc.train[,'MASE']),]

 ##Note that we look at MAE, MAPE or MASE value. The lower the better the fit.
</code></pre>

<p>This is the plot of my different forecasting, which doesn't seem very reliable/accurate, through the comparison of the red""test set"", and blue""forecasted"" set.
<strong>Plot of different forecast</strong>
<a href=""http://i.stack.imgur.com/WZSNq.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/WZSNq.jpg"" alt=""Different forecast""></a></p>

<p><strong>Different accuracy of the respective models of test and training set</strong></p>

<pre><code>Test set
                    ME      RMSE       MAE        MPE     MAPE      MASE      ACF1 Theil's U
theta      -0.07408833 0.2277015 0.1881167 -0.6037191 1.460549 0.2944165 0.1956893 0.8322151
expsmooth  -0.12237967 0.2681452 0.2268248 -0.9823104 1.765287 0.3549976 0.3432275 0.9847223
randomwalk  0.11965517 0.2916008 0.2362069  0.8823040 1.807434 0.3696813 0.4529428 1.0626775
arima      -0.32556886 0.3943527 0.3255689 -2.5326397 2.532640 0.5095394 0.2076844 1.4452932
struc      -0.39735804 0.4573140 0.3973580 -3.0794740 3.079474 0.6218948 0.3841505 1.6767075

Training set
                     ME      RMSE       MAE         MPE     MAPE      MASE    ACF1 Theil's U
theta      2.934494e-02 0.2101747 0.1046614  0.30793753 1.143115 0.1638029  0.2191889194        NA
randomwalk 2.953975e-02 0.2106058 0.1050209  0.31049479 1.146559 0.1643655  0.2190857676        NA
expsmooth  1.277048e-02 0.2037005 0.1078265  0.14375355 1.176651 0.1687565 -0.0007393747        NA
arima      4.001011e-05 0.2006623 0.1079862 -0.03405395 1.192417 0.1690063 -0.0091275716        NA
struc      5.011615e-03 1.0068396 0.5520857  0.18206018 5.989414 0.8640550  0.1499843508        NA
</code></pre>

<p>From the models accuracy, we can see that the most accurate model would be theta model.
I am not sure why is the forecast very inaccurate, and I think that one of the reasons would be that, I did not treat the ""outliers"" in my data set, resulting in a bad forecast for all model.</p>

<p><strong>This is my outliers plot</strong></p>

<p><strong>Outliers Plot</strong>
<a href=""http://i.stack.imgur.com/bZDQv.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bZDQv.jpg"" alt=""Outliers""></a></p>

<p><strong>tsoutliers output</strong></p>

<pre><code>ARIMA(0,1,0)(0,0,1)[12]                    

Coefficients:
        sma1    LS46    LS51    LS61    TC133   LS181   AO183   AO184   LS185   TC186    TC193    TC200
      0.1700  0.4316  0.6166  0.5793  -0.5127  0.5422  0.5138  0.9264  3.0762  0.5688  -0.4775  -0.4386
s.e.  0.0768  0.1109  0.1105  0.1106   0.1021  0.1120  0.1119  0.1567  0.1918  0.1037   0.1033   0.1040
       LS207    AO237    TC248    AO260    AO266
      0.4228  -0.3815  -0.4082  -0.4830  -0.5183
s.e.  0.1129   0.0782   0.1030   0.0801   0.0805

sigma^2 estimated as 0.01258:  log likelihood=205.91
AIC=-375.83   AICc=-373.08   BIC=-311.19

 Outliers:
    type ind    time coefhat  tstat
1    LS  46 1996:10  0.4316  3.891
2    LS  51 1997:03  0.6166  5.579
3    LS  61 1998:01  0.5793  5.236
4    TC 133 2004:01 -0.5127 -5.019
5    LS 181 2008:01  0.5422  4.841 
6    AO 183 2008:03  0.5138  4.592
7    AO 184 2008:04  0.9264  5.911
8    LS 185 2008:05  3.0762 16.038
9    TC 186 2008:06  0.5688  5.483
10   TC 193 2009:01 -0.4775 -4.624
11   TC 200 2009:08 -0.4386 -4.217
12   LS 207 2010:03  0.4228  3.746
13   AO 237 2012:09 -0.3815 -4.877
14   TC 248 2013:08 -0.4082 -3.965
15   AO 260 2014:08 -0.4830 -6.027
16   AO 266 2015:02 -0.5183 -6.442
</code></pre>

<p>I would like to know how can I further ""analyse""/forecast my data, with these relevant data set and detection of outliers, etc.
Please do help me in treatment of my outliers as well to do my forecasting as well . </p>

<p>Lastly, I would like to know how to combined the different model forecasting together, as from what @forecaster had mentioned in link no.1, combining the different model will most likely result in a better forecasting/prediction.</p>

<p><strong>EDITED</strong></p>

<p>I would like to incorporate the outliers in other models are well.</p>

<p>I have tried some codes, eg. </p>

<pre><code>forecast.ets( res$fit ,h=period,xreg=newxreg)
    Error in if (object$components[1] == ""A"" &amp; is.element(object$components[2], : argument is of length zero

forecast.StructTS(res$fit,h=period,xreg=newxreg)
Error in predict.Arima(object, n.ahead = h) : 'xreg' and 'newxreg' have different numbers of columns
</code></pre>

<p>There are some errors produced, and I am unsure about the correct code to incorporate the outliers as regressors.
Furthermore, how do I work with thetaf or rwf, as there are no forecast.theta or forecast.rwf?</p>
"
"0.0862662185627507","0.066998343672533","169564","<p>The <code>arimax</code> function in the <code>TSA</code> package is to my knowledge the only <code>R</code> package that will fit a transfer function for intervention models. It lacks a <a href=""http://stats.stackexchange.com/questions/34106/forecasting-with-arimax-model-including-xtransf"">predict function</a> though which is sometimes needed.</p>

<p>Is the following a work-around for this issue, leveraging the excellent <code>forecast</code> package? Will the predictive intervals be correct? In my example, the std errors are ""close"" for the components.</p>

<ol>
<li>Use the forecast package arima function to determine the pre-intervention noise series and add any outlier adjustment.</li>
<li>Fit the same model in <code>arimax</code> but add the transfer function</li>
<li>Take the fitted values for the transfer function (coefficients from <code>arimax</code>) and add them as xreg in <code>arima</code>. </li>
<li>Forecast with <code>arima</code></li>
</ol>

<blockquote>
<pre><code>library(TSA)
library(forecast)
data(airmiles)
air.m1&lt;-arimax(log(airmiles),order=c(0,0,1),
              xtransf=data.frame(I911=1*(seq(airmiles)==69)),
              transfer=list(c(1,0))
              )
</code></pre>
  
  <p>air.m1</p>
</blockquote>

<p>Output:</p>

<pre><code>Coefficients:
  ma1  intercept  I911-AR1  I911-MA0
0.5197    17.5172    0.5521   -0.4937
s.e.  0.0798     0.0165    0.2273    0.1103

sigma^2 estimated as 0.01223:  log likelihood=88.33
AIC=-168.65   AICc=-168.09   BIC=-155.02
</code></pre>

<p>This is the filter, extended out 5 more periods that the data</p>

<pre><code>tf&lt;-filter(1*(seq(1:(length(airmiles)+5))==69),filter=0.5521330,method='recursive',side=1)*(-0.4936508)
forecast.arima&lt;-Arima(log(airmiles),order=c(0,0,1),xreg=tf[1:(length(tf)-5)])
forecast.arima
</code></pre>

<p>Output:</p>

<pre><code>Coefficients:
         ma1  intercept  tf[1:(length(tf) - 5)]
      0.5197    17.5173                  1.0000
s.e.  0.0792     0.0159                  0.2183

sigma^2 estimated as 0.01223:  log likelihood=88.33
AIC=-168.65   AICc=-168.28   BIC=-157.74
</code></pre>

<p>Then to Predict</p>

<pre><code>predict(forecast.arima,n.ahead = 5, newxreg=tf[114:length(tf)])
</code></pre>
"
"0.118124884643724","0.101934733735916","169934","<p>I recently noticed that <a href=""http://stackoverflow.com/questions/32349235/cannot-reproduce-varimax-rotation-from-psych-order-of-factors-is-changed""><code>psych::principal</code> <em>reorders</em> principal components on (automatic) rotation</a>, according to their Eigenvalues (from highest to lowest).
(Recall that rotation matrix-multiplies the <em>loadings</em>, and so the order of their squared column sums (fka.  the Eigenvalues) can change, too).</p>

<p>Here's an example: </p>

<pre><code>library(qmethod)
library(psych)
data(""lipset"")  # this dataset is used because it causes a re-ordering of components
Lipset &lt;- cor(x = lipset[[1]], method = ""pearson"")  # must calculate cor matrix first

  # calculate unrotated loadings:
principal.unrotated &lt;- principal(r = Lipset, nfactors = 4, rotate = ""none"")$loa  
  # calculate varimax rotated loadings:
principal.varimax   &lt;- principal(r = Lipset, nfactors = 4, rotate = ""varimax"")$loa  
  # manually calculate varimax rotmat on unrotated loadings:
rot.mat.varimax     &lt;- varimax(x = principal.unrotated)$rotmat  

  # should manually reproduce the varimax rotation:
repr.varimax &lt;- unclass(principal.unrotated) %*% rot.mat.varimax  
repr.varimax
#&gt;             [,1]        [,2]        [,3]        [,4]
#&gt; US1 -0.229427530  0.15096123  0.81283285  0.06465534
#&gt; US2  0.002334831 -0.11383236  0.89114173 -0.06389700
#&gt; US3 -0.009194167  0.79325633  0.03603989  0.18951141
#&gt; US4  0.255174168  0.76681887  0.26319367 -0.08631099
#&gt; JP5  0.003227787 -0.87396713  0.21605277  0.05615580
#&gt; CA6  0.922371369  0.08409883 -0.01191349 -0.08215986
#&gt; UK7  0.823285358  0.07913797 -0.17255592  0.03003107
#&gt; US8 -0.447664930  0.02677531  0.37686206 -0.60826777
#&gt; FR9 -0.158333934  0.06509079  0.11083125  0.87837316
  # notice how eigenvalue order is out of whack:
apply(X = repr.varimax, MARGIN = 2, FUN = function(x) sum(x^2))  
#&gt; [1] 1.871892 2.035122 1.756305 1.203962

unclass(principal.varimax)  # notice how cols 1 and 2 have changed
#&gt;             PC2          PC1         PC3         PC4
#&gt; US1  0.15096123 -0.229427530  0.81283285  0.06465534
#&gt; US2 -0.11383236  0.002334831  0.89114173 -0.06389700
#&gt; US3  0.79325633 -0.009194167  0.03603989  0.18951141
#&gt; US4  0.76681887  0.255174168  0.26319367 -0.08631099
#&gt; JP5 -0.87396713  0.003227787  0.21605277  0.05615580
#&gt; CA6  0.08409883  0.922371369 -0.01191349 -0.08215986
#&gt; UK7  0.07913797  0.823285358 -0.17255592  0.03003107
#&gt; US8  0.02677531 -0.447664930  0.37686206 -0.60826777
#&gt; FR9  0.06509079 -0.158333934  0.11083125  0.87837316
  # eigenvalue order is fine again:
apply(X = principal.varimax, MARGIN = 2, FUN = function(x) sum(x^2))  
#&gt;      PC2      PC1      PC3      PC4 
#&gt; 2.035122 1.871892 1.756305 1.203962
</code></pre>

<p>I get how and why this works.</p>

<p>My question is simply: <strong>is there a reason â€“ other than convention or convenience â€“ why this reordering would be necessary?</strong></p>

<p>(This causes a <a href=""https://github.com/aiorazabala/qmethod/issues/263"" rel=""nofollow"">bunch of problems for my use case in another function</a>, and I'd like to avoid if that is statistically sound).</p>

<p>The way I see it, <em>rotated</em> principal components are no longer principal components anyway, so you might as well leave them in any order they come in.</p>
"
"0.109362392486473","0.132122515500744","173042","<p>I am new to R and forecasting. I have access to weekly data (104 weeks) for a certain SKU, its value and volume sales and a few promo variables.</p>

<p>Promo 1 and Promo 2 are continuous variables (unfortunately Promo 1 is 0 here for this SKU) while Promo 3 and Promo 4 are categorical variables.</p>

<p>I tried forecasting the volume sales for this SKU for the next 72 weeks. I included dummy variables using <code>seasonaldummy</code> function</p>

<pre><code>actual_vol = ts(data$Volume , frequency =52)
    dummy = seasonaldummy(actual_vol)
    xreg = cbind(data$Promo1 , data$Promo2 , data$Promo3 , data$Promo4 , dummy)

fit = auto.arima(actual_vol , xreg = xreg)
</code></pre>

<p>I am trying to forecast sales for the next 72 weeks by keeping my promo variables as 0 (basically baseline sales). I used <code>seasonaldummyf</code> and promo variables as 0 for forecast.</p>

<p>The plot looks something like this
<a href=""http://i.stack.imgur.com/tdsuO.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/tdsuO.png"" alt=""Forecast - Arima""></a></p>

<p>As you can see the forecast looks exactly the same as the previous data (same as using <code>snaive</code>) and it seems promo had no effect at all on volume sales.</p>

<p>Kindly let me know if the method is correct and if not how can I improve it.</p>

<p><a href=""https://drive.google.com/file/d/0B6sOv1da0JMeVHl1SlRMZmJDODQ/view?usp=sharing"" rel=""nofollow"">https://drive.google.com/file/d/0B6sOv1da0JMeVHl1SlRMZmJDODQ/view?usp=sharing</a></p>
"
"NaN","NaN","173505","<p>I have a basic question with the <code>auto.arima</code> function in the ""forecast"" package in R. I create a model using the following simple commands:</p>

<pre><code>data_ts&lt;-ts(data$Value, frequency=24)
fc&lt;-auto.arima(data_ts)
plot(forecast(fc, level=c(80), h=30*24)) 
</code></pre>

<p>The result looks always like this</p>

<p><a href=""http://i.stack.imgur.com/o5HzI.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/o5HzI.jpg"" alt=""enter image description here""></a></p>

<p>After some periods it converges to the mean value. Is this a ""normal"" behaviour with ARIMA models or is it a sign that some information or parameter is missing? </p>

<p>BTW: On weekends I have different patterns. Is this considered automatically by <code>auto.arima</code> or do I have to create a dummy variable and use <code>xreg</code>?</p>

<p>Thanks!</p>
"
"0.107832773203438","0.111663906120888","174514","<p>I used two different methods to forecast a time series data.</p>

<ol>
<li>The first one used is HoltWinters with Beta and Gamma as FALSE, since I don't see any trend or seasonality in the plot.</li>
</ol>

<p>Below is the result from Box.test</p>

<pre><code>Box.test(fore.holt.stat$residuals, type=""Ljung-Box"", lag=10)

Box-Ljung test
data:  fore.holt.stat$residuals
X-squared = 10.691, df = 10, p-value = 0.3821
</code></pre>

<p>The p-value is 0.3821</p>

<ol start=""2"">
<li><p>I used auto.arima on the data and below is the result</p>

<pre><code>Box.test(fore.arima$residuals, type=""Ljung-Box"", lag=10)

Box-Ljung test
data:  fore.arima$residuals
X-squared = 14.724, df = 10, p-value = 0.1425
</code></pre></li>
</ol>

<p>The p-value is 0.14</p>

<pre><code>Question 1 :
</code></pre>

<p>Can I say that the first model is better since I have a higher p-vale?</p>

<p>Below are few other observations :</p>

<p>Model 1:</p>

<pre><code>accuracy(fore.holt.stat)
               ME     RMSE      MAE       MPE     MAPE     MASE       ACF1
Training set 424.9864 10275.55 7930.602 0.8782302 9.251837 0.766108 0.02142331
</code></pre>

<p>Model 2:</p>

<pre><code>accuracy(fore.arima)
               ME     RMSE     MAE        MPE     MAPE      MASE      ACF1
Training set 284.5242 7243.413 5371.42 -0.1874984 6.036736 0.5183941 0.0100049


Question 2 :
</code></pre>

<p>Which of the model is correct based on the <code>accuracy</code> function output?</p>

<p>In both the models, the p-value is high, but the mean of errors is not close to zero.</p>
"
"0.118124884643724","0.122321680483099","174655","<p>I used two different methods to forecast a time series data.</p>

<p>The first one used is HoltWinters with Beta and Gamma as FALSE, since I don't see any trend or seasonality in the plot.
Below is the result from <code>Box.test</code></p>

<pre><code>Box.test(fore.holt.stat$residuals, type=""Ljung-Box"", lag=10)

Box-Ljung test
data:  fore.holt.stat$residuals
X-squared = 10.691, df = 10, p-value = 0.3821
</code></pre>

<p>The p-value is 0.3821</p>

<p>I used auto.arima on the data and below is the result</p>

<pre><code>Box.test(fore.arima$residuals, type=""Ljung-Box"", lag=10)

Box-Ljung test
data:  fore.arima$residuals
X-squared = 14.724, df = 10, p-value = 0.1425
</code></pre>

<p>The p-value is 0.14</p>

<pre><code>Question 1:
</code></pre>

<p>Can I say that the first model is better since I have a higher p-value?</p>

<p>Below are few other observations :</p>

<p>Model 1:</p>

<pre><code>accuracy(fore.holt.stat)
           ME     RMSE      MAE       MPE     MAPE     MASE       ACF1
Training set 424.9864 10275.55 7930.602 0.8782302 9.251837 0.766108 0.02142331
</code></pre>

<p>Model 2:</p>

<pre><code> accuracy(fore.arima)
           ME     RMSE     MAE        MPE     MAPE      MASE      ACF1
 Training set 284.5242 7243.413 5371.42 -0.1874984 6.036736 0.5183941 0.0100049


Question 2:
</code></pre>

<p>Which of the model is correct based on the <code>accuracy()</code> function output?</p>

<p>In both the models, the p-value is high, but the mean of errors is not close to zero.</p>
"
"0.0964485644340824","0.0998752338877845","174687","<p>I have around 10000 time series and I want to train ARIMA model using 8000 of them.</p>

<p>I wanted to use auto.arima function <a href=""http://www.inside-r.org/packages/cran/forecast/docs/auto.arima"" rel=""nofollow"">http://www.inside-r.org/packages/cran/forecast/docs/auto.arima</a>
however I am unable to find best ARIMA model for many time series. </p>

<p>Here is the code, I can always use x as my time series.  but how to train it using more time series and find best model?</p>

<pre><code>y=auto.arima(x)
plot(forecast(y,h=30))
</code></pre>

<p>Sample 
time series 1</p>

<pre><code>0.0003748,0.0003929,0.0003653,0.0003557,0.0004463,0.000349,0.0003099,0.0003395,0.0003157,0.0002871,0.0002604,0.0002422,0.0001917,0.0002117,0.0002689
</code></pre>

<p>time series 2</p>

<pre><code>0.0003977,0.0003481,0.0002413,0.0002069,0.0002127,0.0002108,0.0002003,0.0002174,0.0002098,0.0002069,0.0001955,0.0001926,0.0002108,0.0002146,0.0002079
</code></pre>
"
"0.12859808591211","0.149812850831677","174692","<p>I have around 10000 time series showing one particular metric over 5 hours. </p>

<p>I used <a href=""http://www.inside-r.org/packages/cran/forecast/docs/auto.arima"" rel=""nofollow"">auto.arima function</a> </p>

<p>In my previous question, people suggested that I have to use auto.arima for each time series, hold off some of data points and test the prediction with my hold off points.</p>

<p>I am holding off 20% of data points (if you see sample out of 40 I will hold off 8) and then let auto.arima predict. Then I can compare generated 8 values with actual 8 values.
But is there a formal way to test accuracy in ARIMA model? Is my approach correcT?</p>

<p>Is there a prebuilt function to test the accuracy of Arima.</p>

<p>Here is the code, I can always use x as my time series.</p>

<pre><code>y=auto.arima(x)
plot(forecast(y,h=30))
</code></pre>

<p>Sample 
time series 1</p>

<pre><code>0.0003748,0.0003929,0.0003653,0.0003557,0.0004463,0.000349,0.0003099,0.0003395,0.0003157,0.0002871,0.0002604,0.0002422,0.0001917,0.0002117,0.0002689
</code></pre>

<p>time series 2</p>

<pre><code>0.0003977,0.0003481,0.0002413,0.0002069,0.0002127,0.0002108,0.0002003,0.0002174,0.0002098,0.0002069,0.0001955,0.0001926,0.0002108,0.0002146,0.0002079
</code></pre>

<p>Both have 40 points. I can hold off 20% of them (8) and compare after auto.arima predicts.   But is there a simpler way I can test accuracy?</p>
"
"0.0482242822170412","0.0499376169438922","174841","<p>I'm in need of some help in using R (bit of a newbie).<br>
If I use the following code in R</p>

<pre><code>arima(adj.close,order=c(2,2,3))
</code></pre>

<p>it will fit an ARIMA(2,2,3) model to <code>adj.close</code>. </p>

<p>My <strong>question</strong> is, do I difference the data before fitting the model or after?</p>

<p>I used function <code>ndiffs</code> to get the number of first differences I should take, and the return was <code>2</code>.</p>
"
"0.127589457900886","0.113247870429209","175996","<p>I have been studying a few simple statistical models for (univariate) time series. From my understanding,</p>

<ul>
<li><p>ARIMA and its siblings are used to model the <em>mean</em> of a time series. Rather than a static measure like <code>mean()</code>, the result is a series estimating the mean.</p></li>
<li><p>ARCH and its brothers are used to model the <em>volatility</em> of a time series. Rather than the usual <code>sd()</code>, the result is a series estimating the variance. </p></li>
</ul>

<h2>Question</h2>

<p>What would be a credible model for the correlation of two time series?</p>

<h2>Notes</h2>

<p>While mean models explore the idea of regressing lagged values of the time series, volatility models (eg. ARCH model) explore the idea of regressing lagged residuals where residuals are the difference of a mean model to its original time series.</p>

<p>In its general sense and for a variety of reasons, ARIMA and ARCH are <em>superior</em> models than rolling windows with <code>mean()</code> (popularly known as moving averages outside statistics world) and <code>sd()</code>.</p>

<p>However, there is no such a thing for the <em>correlation</em> of two time series X and Y to my knowledge.</p>

<p>The closest thing would be rolling a sad, straight window with <code>cor()</code>, Pearson's coefficient function in R, and work around the resulting series.</p>

<h2>A poor solution</h2>

<p>Trying to replicate Pearson's correlation model,</p>

<pre><code>p_(X,Y) = cov(X,Y) / (sd(X) sd(Y))
        = E((X-mean(X))(Y-mean(Y))) / (sd(X) sd(Y)),
</code></pre>

<p>to the time series world, I had the above without the intended success.</p>

<pre><code>library('forecast')
library('fGarch')

X &lt;- 1:200 + rnorm(200, sd=10)
Y &lt;- 50 + (1:200)/100 + rnorm(200, sd=5)

plot(1:200, X, t='l', main=""What would be a resulting ts correlation of X and Y?"")
lines(1:200, Y, t='l', col='blue')

# Mimic Pearson correlation, cov(X,Y)/(sd(X)*sd(Y)).
Xm &lt;- as.vector(X) - as.vector(fitted(Arima(X, order=c(2,0,1))))
Ym &lt;- as.vector(Y) - as.vector(fitted(Arima(Y, order=c(2,0,1))))

Xv &lt;- garchFit(formula=~arma(2,1) + garch(2,1), data=X)@sigma.t
Yv &lt;- garchFit(formula=~arma(2,1) + garch(2,1), data=Y)@sigma.t

correlation &lt;- Xm * Ym / (Xv * Yv)    # this can be forecast

plot(correlation, t='l', col='blue', ylim=c(-2, 2), main='Correlation models')
abline(h=c(-1, 1))
abline(h=cor(X, Y), col='red', lwd=5)

# Correlation rolling window of size 10.
df &lt;- data.frame(X, Y)
crw &lt;- rep(NA, 10)
for (i in 11:nrow(df))
  crw &lt;- c(crw, cor(df[(i-10):i, 1], df[(i-10):i, 2]))

lines(crw, col='darkgreen', lwd=5)

legend('topright',
  c('pearson mimic', 'static cor()', 'rolling cor() like moving averages'),
  col=c('blue', 'red', 'darkgreen'), lwd=c(1, 5, 5))
</code></pre>
"
"0.237721744707918","0.264402272926146","176129","<p>I've been working on some various time series forecasts and I've begun to notice a trend (pardon the pun) in my analyses. For about 5-7 datasets that I've worked with so far, it would be helpful to allow for multiple seasonal periods along with an option for holiday dummies. I've tried various methods and usually stick with <code>tbats</code> since <code>auto.arima()</code> with regressors has been giving me issues. By this point, it's probably obvious I'm working in R.</p>

<p>Before I get too far, let me give some sample data. Hopefully the following link works: <a href=""https://gist.github.com/JaredRayWolf/c8cb601dd26ec72a64d0"" rel=""nofollow"">https://gist.github.com/JaredRayWolf/c8cb601dd26ec72a64d0</a>.</p>

<p>This data yields the following time series plot:
<a href=""http://i.stack.imgur.com/FYS1x.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FYS1x.jpg"" alt=""Time Series Plot""></a>
The large dips are around Christmas and New Years, however there are also smaller dips around Thanksgiving. In the code below, I name this dataset <code>traindata</code>.</p>

<p>Now, <code>ets</code> and ""plain"" <code>auto.arima</code> don't look so hot in the long run since they are limited to only one seasonal period (I choose weekly). However for my test set that I held out they performed fairly well for the month's worth of data (with the exception of Labor Day weekend). This being said, forecasting out for a year would be ideal.</p>

<p>I next tried <code>tbats</code> with weekly and yearly seasonal periods. That results in the following forecast:
<a href=""http://i.stack.imgur.com/kcXmd.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/kcXmd.jpg"" alt=""TBATS Forecast""></a></p>

<p>Now this looks pretty good. From the naked eye it looks great at taking into account the weekly and yearly seasonal periods as well as Christmas and New Years effects (since they obviously fall on the same dates each year). It would be best if I could include the holidays (and the days around them) as dummy variables. Hence my attempts at <code>auto.arima</code> with <code>xreg</code> regressors.</p>

<p>For ARIMA with regressors, I've followed Dr. Hyndman's suggestions for the fourier function (given here: <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/longseasonality/</a>) as well as his selection of the number of fourier terms (given here: <a href=""http://robjhyndman.com/hyndsight/forecasting-weekly-data/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/forecasting-weekly-data/</a>)</p>

<p>My code is as follows:</p>

<pre><code>fourier &lt;- function(t,terms,period)
{
  n &lt;- length(t)
  X &lt;- matrix(,nrow=n,ncol=2*terms)
  for(i in 1:terms)
  {
    X[,2*i-1] &lt;- sin(2*pi*i*t/period)
    X[,2*i] &lt;- cos(2*pi*i*t/period)
  }
  colnames(X) &lt;- paste(paste(c(""S"",""C""),rep(1:terms,rep(2,terms)),sep=""""),period,sep=""_"")
  return(X)
}

fcdaysout&lt;-365
m1&lt;-7
m2&lt;-30.4375
m3&lt;-365.25

hol&lt;-cbind(traindata$CPY_HOL, traindata$DAY_BEFORE_CPY_HOL, traindata$DAY_AFTER_CPY_HOL)
hol&lt;-as.matrix(hol)

n &lt;- nrow(traindata)
bestfit &lt;- list(aicc=Inf)
bestk &lt;- 0

for(i in 1:m1)
{
    fake_xreg = cbind(fourier(1:n,i,m1), fourier(1:n,i,m3), hol)
    fit &lt;- auto.arima(traindata$ACTIVE_LOADS, xreg = fake_xreg, max.p=10, max.q=10, max.d=2, stepwise=FALSE, ic=""aicc"", allowdrift=TRUE)
    	if(fit$aicc &lt; bestfit$aicc)
    {
        bestfit &lt;- fit
        bestk &lt;- i
    }
    else
    {
    }
}

k &lt;- bestk
k
##k&lt;-3

xreg&lt;-cbind(fourier(1:n,k,m1), fourier(1:n,k,m3), hol)
xreg&lt;-as.matrix(xreg)

aacov_fit &lt;- auto.arima(traindata$ACTIVE_LOADS, xreg = xreg, max.p=10, max.q=10, max.d=2, stepwise=FALSE, ic=""aic"", allowdrift=TRUE)
summary(aacov_fit)
</code></pre>

<p>Where my issues come in is inside the for loop to determine the <code>k</code>, the number of fourier terms, that minimizes AIC. In all of my attempts at ARIMA with regressors, it always produces an error when <code>k&gt;3</code> (or <code>i&gt;3</code> if we're talking about inside my loop). The error being <code>Error in solve.default(res$hessian * n.used, A) : system is computationally singular: reciprocal condition number = 1.39139e-34</code>. Simply setting <code>k=3</code> gives some decent results for my test set but for the next year it doesn't appear to adequately catch the steep drops around the end of the year and is much smoother than imagined as evidenced in this forecast:<a href=""http://i.stack.imgur.com/rj30h.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/rj30h.jpg"" alt=""AutoArima with Covariates (k=3)""></a></p>

<p>I assume this general smoothness is due to the small number of fourier pairs. Is there an oversight in my code in that I'm just royally screwing up the procedure provided by Dr. Hyndman? Or is there a theoretical issue that I'm unknowingly running into by trying to find more than 3 pairs of fourier terms for the multiple seasons I'm attempting to account for? Is there a better way to include the multiple seasonalities and dummy variables?</p>

<p>Any help in getting these covariates into the arima model with an appropriate number of fourier terms would be appreciated. If not, I'd at least like to know whether or not what I'm attempting is possible in general with larger number of fourier pairs.</p>
"
"0.0681994339470473","0.0706224551546449","176560","<p>I have a data for ATM transaction on a daily basis and this data represent a seasonal variation in weekend and holidays.</p>

<p>data structure like this</p>

<pre><code>trans_date  tot_amount  Weekend Holiday_flag
01/10/2013  164800  0   0
02/10/2013  205900  0   1
03/10/2013  215600  0   0
04/10/2013  228600  0   0
05/10/2013  410200  1   0
</code></pre>

<p>I used <code>arima()</code> function in R to forecast the next one month data but I am not getting better forecast.</p>

<p>I am confusing to capture seasonal variation in my data.</p>

<p>I have to select the ARIMA order from ACF and PACF plot but I have some confusion to capture seasonal order from this graph.</p>

<p>So please advise me how can I select the right ARIMA model for my data</p>
"
"0.0482242822170412","0.0499376169438922","177117","<p>I use auto.arima function to model the below provided time series data. At the end of the analysis, the best model is given as ARIMA(1,2,1). The log- likelihood=93.69 is positive which is unusual. It is clear for me that the log-likehood is not as same as the probability. But how can this originate from the analysis? Does it depend on the data? Or just due to sign convention? </p>

<pre><code>  srs=c(8.6,9.8,11.2,12.4,13.5,15.7,18.6,21.1,22.3,23.6,24.6,26.3,28.3,29.6,33.3,36.4,40.5,44.9,48.4,52.6,56.8,60.5,67.2,73.4,77.8,85.6,94.8,105.5,
114.0,118.5,128.3,126.9,132.6,141.2,150.0,160.8,174.6,190.0,198.1,194.1,210.4,230.3,242.4,246.4,257.2)

x=ts(srs,frequency=1)

fit=auto.arima(x,d = 2,D = 0,start.p=0, start.q=0, max.p=5, max.q=5,stationary=FALSE,seasonal=FALSE,stepwise=TRUE,trace=TRUE,approximation=FALSE,allowdrift=TRUE,ic=""aicc"",lambda=-0.049)

library(forecast)
tx=BoxCox(x, -0.049)
</code></pre>

<h1>result of transformation</h1>

<pre><code> tx=(2.042209,2.159383,2.278396,2.368590,2.443563,2.575967,2.723460,2.832405, 2.879977, 2.928574, 2.964082,3.021106, 3.083437, 3.121522, 3.221002, 3.295802, 3.385064, 3.470876, 3.533058, 3.601728, 3.664871, 3.716566,3.802248, 3.873901, 3.921001, 3.998007, 4.079888, 4.165227, 4.226782, 4.257449, 4.320209, 4.311558, 4.346176,4.395557, 4.442923, 4.497221, 4.561284, 4.626783, 4.659033, 4.643283, 4.705451, 4.774832, 4.814009, 4.826510,4.859228)
</code></pre>

<h1>result of density function given observation</h1>

<pre><code>dnorm(tx ,mean(tx), sd(tx),log=TRUE)
Time Series:
Start = 1 
End = 45 
Frequency = 1 
[1] -2.7168796 -2.4462962 -2.1917838 -2.0125389 -1.8724951 -1.6450191 -1.4214597 -1.2765217 -1.2186144
[10] -1.1628381 -1.1242424 -1.0660746 -1.0078703 -0.9750711 -0.8992890 -0.8517306 -0.8055615 -0.7720362
[19] -0.7543945 -0.7414067 -0.7354801 -0.7349191 -0.7424969 -0.7569828 -0.7705473 -0.7996330 -0.8399630
[28] -0.8923108 -0.9366056 -0.9607175 -1.0143004 -1.0065755 -1.0381350 -1.0861521 -1.1355218 -1.1961059
[37] -1.2730668 -1.3578864 -1.4019282 -1.3802323 -1.4679575 -1.5724597 -1.6345414 -1.6548185 -1.7089570
</code></pre>
"
"NaN","NaN","177613","<p>The output of the <code>auto.arima</code> function gives log-likelihood as 93.69. However I calculate it as 99.40152 by using the residuals. What causes the difference? </p>

<pre><code>resid=c( 0.0009133036, -0.0024778952,  0.0016247257, -0.0262748348,-0.0245270803,  0.0416803278,  0.0283238192,-0.0200986496, -0.0637956188,-0.0304700821, -0.0389081231, -0.0066333658, -0.0079335582, -0.0327841692, 0.0422120965, -0.0107629969,  0.0141159479,  0.0033650571, -0.0195425507, -0.0016719690, -0.0092509480,-0.0173254345,  0.0233666990, -0.0063053729, -0.0249087321,  0.0175185423,  0.0090894124,  0.0094325355,-0.0170129762, -0.0368547384,  0.0118255009, -0.0728012098,  0.0069336386,  0.0052472716, -0.0028184078, 0.0052579484,  0.0117586931,  0.0079340426, -0.0270301760,  -0.0590942951,  0.0448756767,  0.0175497969,-0.0178870348, -0.0310982909,  0.0033326883)


L=length(resid)*log(1)-length(resid)*log(sd(resid)*sqrt(2*pi))-(sum(resid^2)/(2*var(resid)))
</code></pre>

<p>Any df for std and var of residuals is not considered.  </p>
"
"0.0835269069584557","0.0864944897557338","178014","<p>As the title states, I want to generate a time series that follows an AR(1) proces and thus has a certain overall level of autocorrelation.</p>

<p>I'm using the <code>arima.sim</code> function (which is implemented as standard in R).</p>

<p>I thought that for example the following command:</p>

<pre><code>arima.sim(model=list(Ar=-0.5),n=400)
</code></pre>

<p>would generate a time series of length 400 and an autocorrelation of -0.5.
However, I've noticed that the values you can give to the <code>Ar</code> parameter are not limited to [-1; 1]. For example, you could input <code>10 000</code>.</p>

<p>Can anyone explain to me what the <code>Ar</code> parameter actually represents? Because it apparently is not a correlation coefficient...</p>

<p>After reading on the internet it seems to me that there's not a lot of information there for people who want to simulate time series data using a model as opposed to people who want to fit data to a model...</p>
"
"0.181865157192126","0.188326547079053","180246","<p>I have two questions related to transfer functions.  The first is a general question regarding how to compute values of $Y_t$ from a rational transfer polynomial function of the form popularized by Box and Jenkins.  The second is related to the arimax() function in the TSA package in R. 
It is my understanding that the rational transfer function can be expressed as (ignoring any noise terms):</p>

<p>$Y_t = \frac{\omega(B)^sB^d}{\delta(B)^r}X_t$</p>

<p>Lets first make it easy and assume r = 1, s = 0 and d = 0.  Then the equation should simplify to:</p>

<p>$Y_t = \frac{\omega_0}{(1-\delta_1B^1)}X_t$ (equation 1)</p>

<p>My first intuition would be to multiply both sides by ${(1-\delta_1 B^1)}$ then re-arrange to solve for $Y_t$:</p>

<p>$Y_t = \omega_oX_t+\delta_1 Y_{t-1}$ (equation 2)</p>

<p>However, I've seen <a href=""https://support.sas.com/documentation/cdl/en/etsug/60372/HTML/default/viewer.htm#etsug_arima_sect014.htm"" rel=""nofollow"">here</a> and <a href=""http://www.stat.pitt.edu/stoffer/tsa3/tsa3.pdf"" rel=""nofollow"">here</a> (pg. 286) that you can express the transfer function as so:</p>

<p>$Y_t = \frac{\omega_0}{(1-\delta_1B^1)}X_t = \omega_o(1+\delta_1 B^1)X_t$ </p>

<p>This would yield a different equation for $Y_t$:</p>

<p>$Y_t = \omega_oX_t+\delta_1\omega_o X_{t-1}$ (equation 3)</p>

<p>So is equation 2 the correct way to calculate $Y_t$ or is it equation 3? If $d \neq 0$, say r = 0, s = 1 and d = 1, could you re-write the equation 1 as:</p>

<p>$Y_t = \frac{\omega_0}{(1-\delta_1B^1)}X_{t-1}$</p>

<p>and then solve for $Y_t$ by one of the equations above?</p>

<p>Finally, for a related R question.  Assuming one of the two ways above is correct for solving for $Y_t$, I would like to check the fitted values that are calculated using the arimax() function in the TSA package.  I will use the Box and Jenkins sales and lead data, and use their transfer function.  The arimax() function is taken from <a href=""https://stackoverflow.com/questions/25224155/transfer-function-models-arimax-in-tsa"">this post</a> and gives correct coefficients:</p>

<pre><code> &gt;library(astsa)
 &gt;library(TSA)

 &gt;d.lead &lt;- diff(BJsales.lead) #first difference
 &gt;d.sale &lt;- diff(BJsales) #first difference
 &gt;center.d.lead &lt;- d.lead - mean(d.lead) # mean centered, first difference
 &gt;center.d.sale &lt;- d.sale - mean(d.sale) # mean centered, first difference

 &gt;mod &lt;- arimax(center.d.sale,
               order=c(0,0,1),
               include.mean=TRUE,
               fixed=c(NA,NA,NA,0,0,0,NA),
               xtransf=center.d.lead,
               transfer=list(c(1,3)),
               method=""ML"")

 &gt;mod
 Coefficients:
           ma1  intercept  T1-AR1  T1-MA0  T1-MA1  T1-MA2  T1-MA3
       -0.4500    -0.0055  0.7253       0       0       0  4.7008
 s.e.   0.0772     0.0107  0.0048       0       0       0  0.0613

&gt;fitted(mod)[1:5]
[1]         NA         NA         NA  0.1264714  1.3631528
</code></pre>

<p>Now, I would like to calculate by hand the fitted values of mod.  Assuming equation 1 above is correct $Y_t$ (center.d.sale) equals (again ignoring noise):</p>

<p>$Y_t = \omega_3X_{t-3}+\delta_1 Y_{t-1}$</p>

<p>If $\omega_3 = 4.7008$ and $\delta_1 = 0.7253$ then the first calculated value ($Y_4$) should be:</p>

<pre><code> &gt;4.7008*center.d.lead[1] + 0.7253*center.d.sale[3]
 -0.4922764
</code></pre>

<p>This is clearly not the same as the fitted value:</p>

<pre><code> &gt;fitted(mod)[4]
 0.1264714
</code></pre>

<p>Even including the intercept, the values aren't the same. Do you know how arimax() calculates the fitted values, or if my equation is wrong, the proper way to calculate the fitted values from the transfer function coefficients?</p>
"
"0.0835269069584557","0.0864944897557338","180820","<p>I am using the auto.arima function in R.  I'm using this to forecast daily sales and am loading a number of covariates (mostly holiday/seasonal dummy variables) with Xreg.  </p>

<p>Question (I apologize if this question is too fundamental, however, my experience is with regressions on cross sectional data in fixed time periods and I have little experience with dynamic regressions): Using auto.arima, do I still need to exclude a dummy variable from my set of covariates (ie, 6 dummy variables for day of the week instead of 7)?  Is the excluded dummy still caught in the output for the intercept variable?</p>

<p>Thanks.</p>
"
"0.159941849898134","0.165624338327385","184880","<p>So I wanted to generate $500$ data points from an $ARMA(1,1)$ distribution in R, use the first $400$ as my training data and use the training data and the <code>predict</code> function to both see if I could obtain the correct model via AIC and then plot my prediction. I wanted to generate a whole bunch of different $ARMA$ models to use for the AIC which is why I basically built a grid, however I get a bunch of warnings and errors with my current method, despite the fact it seems to work. Note it is the estimation and not the actual prediction that raises the problems. Can I use a grid like so to produce AICs for various model types?</p>

<pre><code>####### ARMA(1,1) ############
# Causal stationary - root of the function phi(z) &gt; 1
theta1&lt;- 0.5
phi1&lt;- 0.6

arma11&lt;- arima.sim(n = 500,list(ar = c(phi1), ma = c(theta1)), sd = sqrt(1))
tr.data&lt;-arma11[1:400]
AIC&lt;-c()
for (i in 0:3){
  for (j in 0:3){
    aic.ij&lt;- arima(tr.data,order = c(i,0,j))$aic
    AIC&lt;- c(AIC,aic.ij) 
  }
}
AIC&lt;- matrix(AIC, ncol=4, byrow =T)
colnames(AIC)&lt;- c(""q=0"",""q=1"",""q=2"",""q=3"")
rownames(AIC)&lt;- c(""p=0"",""p=1"",""p=2"",""p=3"")
AIC
index&lt;-which(AIC==min(AIC),arr.ind=T)
index
prd&lt;-predict(arima(tr.data, order = c(index[1]-1,0,index[2]-1)),n.ahead =100)
vld.data&lt;-arma11[401:500]

# plot the training data with the next 100 predictions and the 95% confidence intervals
plot.ts(arma11, xlim=c(0,500),ylim=c(floor(min(arma11)),ceiling(max(arma11)))) 
lines(prd$pred, col='blue')
lines(prd$pred-(1.96*prd$se), col='red')
lines(prd$pred+(1.96*prd$se), col='red')
</code></pre>

<p>For this particular $ARMA$ I am error free but receive various size mismatch errors with other models under the same method, I receive this warning though. </p>

<pre><code>Warning message:
In arima(tr.data, order = c(i, 0, j)): possible convergence problem: optim gave code = 1Warning message:
In arima(tr.data, order = c(i, 0, j)): possible convergence problem: optim gave code = 1         q=0      q=1      q=2      q=3
p=0 1608.907 1296.137 1207.432 1187.231
p=1 1244.182 1178.820 1180.004 1181.993
p=2 1190.716 1180.072 1182.002 1183.776
p=3 1179.731 1181.688 1183.767 1181.305
    row col
p=1   2   2
</code></pre>
"
"0.118124884643724","0.122321680483099","185058","<p>I have to forecast sales for stores. So for that I am using ARIMA model.Here first we need to create times series object using ts function which takes frequency parameter.As far as I know we use 1=annual, 4=quarterly, 12=monthly but don't know sure what will be frequency for daily observations. I tried using 1,7,365 and number of observation as values for frequency parameter but with these I am not able to get proper plots and forecast.My second question is how to deal with 0 values for specific observation as they are producing errors as follows:</p>

<pre><code>Error in na.fail.default(as.ts(x)) : missing values in object for acf() and pacf() 
</code></pre>

<p>and</p>

<pre><code>Error in OCSBtest(x, m) : The OCSB regression model cannot be estimatedauto.arima() functions.
</code></pre>

<p>Here is the data:
<a href=""https://drive.google.com/file/d/0B-KJYBgmb044QlNUS3FhVFhUbE0/view?usp=sharing"" rel=""nofollow"">https://drive.google.com/file/d/0B-KJYBgmb044QlNUS3FhVFhUbE0/view?usp=sharing</a></p>

<p>Below is my code:</p>

<pre><code> data&lt;-read.csv(""Book5.csv"")
   View(data)

   mydata&lt;- ts(data[,2], start=1, end=181, frequency = 7)
   View(mydata)
   plot(mydata, xlab=""Day"", ylab = ""Sales"")

   plot(diff(mydata),xlab=""Day"",ylab=""Differenced Sales"")
   plot(log10(mydata),ylab=""Log(Sales)"")
   plot(diff(log10(mydata)),ylab=""Differenced Log (Sales)"")

   par(mfrow = c(1,2))
   acf(ts(diff(log10(mydata))),main=""ACF Sales"")
   pacf(ts(diff(log10(mydata))),main=""PACF Sales"")

   require(forecast)
   ARIMAfit &lt;- auto.arima(log10(mydata), approximation=FALSE,trace=FALSE)
   summary(ARIMAfit)

   pred &lt;- predict(ARIMAfit, n.ahead= 31)
   pred
   class(pred$pred)
       10^(pred$pred)

   # Write CSV in R
   write.csv(10^(pred$pred), file = ""MyData.csv"")

   plot(mydata,type=""l"",xlim=c(1,52),ylim=c(1,6000),xlab = ""Day"",ylab =   ""Sales"")
   lines(10^(pred$pred),col=""blue"")
       lines(10^(pred$pred+2*pred$se),col=""orange"")
       lines(10^(pred$pred-2*pred$se),col=""orange"")
</code></pre>
"
"0.156728917205384","0.199750467775569","186164","<p>I am new to time series and am trying to fit some time series data.</p>

<p>I understand the general concept of ARIMA model. However, as I read more textbooks and articles from Rob Hyndman, I realized I could put some regressors using the <code>xreg</code> argument for the functions <code>auto.arima</code> or <code>arima</code> in R to get an ARMAX model. Therefore, I wonder if it is still necessary to include seasonality in <code>ts(...,frequency)</code> as everything can be specified as dummy variable within the <code>xreg</code> matrix and a more complicated seasonality structure (e.g. monthly seasonality) can be specified. </p>

<p>In addition, what would be a good way to check the accuracy of the forecast? I am fitting multiple time series data with a hierarchical structure. Using <code>auto.arima</code>, I am able to select the best model and validate the model by looking at the residuals (check whether they are white noise). However, is there a way to even improve on the model if the prediction is still far from the actual data?</p>

<p>To sum up, </p>

<ol>
<li>Is the <code>frequency</code> argument in <code>ts</code> function really necessary?  Can I just specify everything in the <code>xreg</code> matrix?</li>
<li>What would be a normal routine to improve on model after selecting the appropriate ARIMA model with the lowest AIC?</li>
</ol>

<p>Updates (Dec 17):</p>

<p>I am now able to fit an ARIMA model with SARIMA error by specifying <code>xreg</code> argument and <code>seasonal=F</code>. One issue that I have with that is, my <code>xreg</code> matrix is not invertible (I assumed) and its not due to the presence of intercept term. Thus <code>auto.arima()</code> only fit a <code>c(0,0,0)</code> model.</p>

<p>I then tried using <code>Arima()</code> to manually select model and it outputted the following error</p>

<pre><code>Error in optim(init[mask], armafn, method = optim.method, hessian = TRUE,  : 
non-finite value supplied by optim
</code></pre>

<p>I check the <code>xreg</code> matrix and it turns out column 48 (Day) and column 52 (2015) is causing the issue. Could you check if there's something wrong with my <a href=""https://drive.google.com/file/d/0B-b9YsAB5mpnam1oN0hYcFRwLXM/view?usp=sharing"" rel=""nofollow"">matrix structure</a> ? </p>

<p>If you think this additional updates should be asked in stack overflow or additional question, I will move it.</p>
"
"0.107832773203438","0.111663906120888","186265","<p>I am currently working on some research and we are trying to do some Time-Series prediction using neural networks. To get started, I was using the paper published by G. Peter Zhang (<a href=""http://cs.uni-muenster.de/Professoren/Lippe/diplomarbeiten/html/eisenbach/Untersuchte%20Artikel/Zhan03.pdf"" rel=""nofollow"">Time Series forcasting using a hybrid ARIMA and NN model</a>) since I am no expert in either R or statistics, I could really do with some help. </p>

<p>I got R and the neuralnet lib setup and then took the Lynx dataset, then created a data-frame with the data long with the lags to set as input. My data now looks something like this (this is only for t, t-1, and t-2 lags) </p>

<pre><code>     x     x1    x2
1   269    NA    NA
2   321   269    NA
3   585   321    269
</code></pre>

<p>Now I want to train a NN with input x1 and x2 and get output at x.</p>

<p>I do the training with the following code </p>

<pre><code>nn &lt;- neuralnet(x~x1+x2, data=dat, hidden = 2, linear.output = T) # I am using t-1 ... t-4 so using hidden layer of 2
</code></pre>

<p>This does train the model, but the error is really high, and when I use it to do any computation the results of the second layer neuron is alway 1. I was discussing with some freinds and they said that its because I am maybe using the wrong activation function. I looked in the help for the act.fct and tried with both <code>logistic</code> and <code>tanh</code> but the results remain the same. </p>

<p>I have been stuck on this for a few days now, so could really use some help. May I am doing something wrong? Or missing something? </p>

<p>Thanks</p>
"
"0.109362392486473","0.132122515500744","186505","<p>I am working on project to forecast sales of stores to learn forecasting. Until now I have successfully used simple <code>auto.arima</code> function for forecasting. But to make these forecast more accurate I can make use of covariates. I have defined covariates like holidays, promotion which affect on sales of store using <code>xreg</code> argument with the help of this post:
<a href=""http://stats.stackexchange.com/questions/41070/how-to-setup-xreg-argument-in-auto-arima-in-r"">How to setup xreg argument in auto.arima() in R?</a></p>

<p>But my code fails at line:</p>

<pre><code>ARIMAfit &lt;- auto.arima(saledata, xreg=covariates)
</code></pre>

<p>and gives error saying:</p>

<pre><code>Error in model.frame.default(formula = x ~ xreg, drop.unused.levels = TRUE) : 
  variable lengths differ (found for 'xreg')
In addition: Warning message:
In !is.na(x) &amp; !is.na(rowSums(xreg)) :
  longer object length is not a multiple of shorter object length
</code></pre>

<p>Below is link to my Dataset:
<a href=""https://drive.google.com/file/d/0B-KJYBgmb044blZGSWhHNEoxaHM/view?usp=sharing"" rel=""nofollow"">https://drive.google.com/file/d/0B-KJYBgmb044blZGSWhHNEoxaHM/view?usp=sharing</a></p>

<p>This is my code:</p>

<pre><code>data = read.csv(""xdata.csv"")[1:96,]
View(data)

saledata &lt;- ts(data[1:96,4],start=1, end=96,frequency =7 )
View(saledata)

saledata[saledata == 0] &lt;- 1
View(saledata)

covariates = cbind(DayOfWeek=model.matrix(~as.factor(data$DayOfWeek)),
                 Customers=data$Customers,
             Open=data$Open,
                 Promo=data$Promo,
             SchoolHoliday=data$SchoolHoliday)
View(head(covariates))


# Remove intercept
covariates &lt;- covariates[,-1]
View(covariates)

require(forecast)
ARIMAfit &lt;- auto.arima(saledata, xreg=covariates)//HERE IS ERROR LINE
summary(ARIMAfit)
</code></pre>

<p>Also tell me how I can forecast for the next 48 days. I know how to forecast using simple <code>auto.arima</code> using the argument <code>n.ahead</code> but I don't know how to do it when the argument <code>xreg</code> is used.</p>
"
"0.159941849898134","0.165624338327385","186725","<p>Short version: How would one be able to quantify an intervention effect in time-series analysis when the intervention decreases seasonal amplitude variation but doesn't directly effect the median?</p>

<p><a href=""https://www.dropbox.com/s/hb3g7j17igeqnoc/dat.csv?dl=0"" rel=""nofollow"">Here</a> is a link to my raw data.</p>

<p>I have a complex time-series of daily incidence numbers for a population over 7 years, totaling 2557 observations. There is a strong weekly and yearly seasonality (high incidence in winter months and low incidence in summer months). There is a baseline negative trend which is orders of magnitude smaller than the seasonality. An intervention was introduced at time = 1700. This intervention should theoretically not cause a level shift. My aim is to detect whether the intervention increases the baseline negative trend.</p>

<p>I have attempted to fit a dynamic linear regression with ARIMA errors in R using <code>auto.arima()</code> in the <code>forecast</code> package. I modeled the weekly season using a dummy variable for each weekday and the weekend. I modeled the monthly seasonality with harmonics using <code>fourier()</code> function in the <code>forecast</code> package. An the intervention effect was coded in by specifying the time index and post-intervention times as independent variables using the methods described in <a href=""http://isites.harvard.edu/fs/docs/icb.topic79832.files/L06_Program_Evaluation_2/Segmented_Regression.Wagner.2002.pdf"" rel=""nofollow"">Segmented regression analysis of interrupted time series studies in medication use research</a>. With these variables specified <code>auto.arima()</code> suggests an ARMA(7,7) process. The coefficients for baseline trend and post-intervention trend are however non-significant.  </p>

<p>I am concerned that by using fourier terms to model away the seasonality I am artificially removing any intervention effect, as visual analysis of the time series indicates that the intervention is specifically decreasing incidence during the winter months and therefore reducing the yearly seasonal variability. </p>
"
"0.0681994339470473","0.0706224551546449","187250","<p>I want to simulate a time series in R, following an ARMA(1,0) model in the form $Y_t = Y_{t-1} + \epsilon_t$, shocking it at time 20.
In a few words, I therefore have to input $\epsilon_{20} = 30$ (the shock magnitude).</p>

<p>Now, I am using the <code>arima.sim</code> function as it is the one I'm familiar with for simulating a time series, but I am not sure on how to implement a shock into it.</p>

<p>Let's start with a standard simulation, based on 250 observations:</p>

<pre><code>shocksim &lt;- arima.sim(n=250, list(ar = c(0.5)))
</code></pre>

<p>How can I input the shock in such a simulation?</p>
"
"0.159941849898134","0.165624338327385","187870","<p>I'm working on a sales forecasting package which should be easy to use for the end user. Given a time series with historical sales data I would like to automatically select one of the three forecasts: Auto.Arima, ETS and STLF. 
The idea is to split historical data into 80% train set and 20% test (holdout) set. Then run Auto.Arima, ETS, STLF and choose the one that has best MAPE on the test set. </p>

<p>Now comes the part that is not entirely clear to me. Once I figured out that e.g. ETS gives me the best result should I now </p>

<ol>
<li>Retrain ETS on the entire set of historical data and generate
forecast using this new model? My reservation here is that after I
run ETS again it may even change the class of the algorithm as well
as the fit parameters which will render the MAPE I got on the test
set irrelevant.  </li>
<li>Just generate the forecast using the model that was trained on the
80% train set? My problem with this approach is that we are ignoring
the last 20% of data which is probably the most important
information for the forecast.</li>
<li>The third idea is to use the same model fit parameters that we got
after training the model on the 80% train set. But then use the
entire set of data for        forecasting. This seems like a
reasonable approach but I cannot figure out how to do it for ETS and
STL (For Arima we can do it by supplying the original fit as the model
parameter of the arima function)</li>
</ol>

<p>Could you please let me know what is the right way to approach this problem?</p>
"
"0.232830634260615","0.250375931986108","188595","<p>I have already read</p>

<p><a href=""http://stats.stackexchange.com/questions/126525/time-series-forecast-convert-differenced-forecast-back-to-before-difference-lev"">Time Series Forecast: Convert differenced forecast back to before difference level</a></p>

<p>and</p>

<p><a href=""http://stats.stackexchange.com/questions/130448/how-to-undifference-a-time-series-variable"">How to &quot;undifference&quot; a time series variable</a></p>

<p>None of these unfortunately gives any clear answer how to convert forecast done in ARIMA using differenced method(diff()) to reach at stationary series.</p>

<p>code sample.</p>

<pre><code>## read data and start from 1 jan 2014
dat&lt;-read.csv(""rev forecast 2014-23 dec 2015.csv"")
val.ts &lt;- ts(dat$Actual,start=c(2014,1,1),freq=365)
##Check how we can get stationary series
plot((diff(val.ts)))
plot(diff(diff(val.ts)))
plot(log(val.ts))
plot(log(diff(val.ts)))
plot(sqrt(val.ts))
plot(sqrt(diff(val.ts)))
##I found that double differencing. i.e.diff(diff(val.ts)) gives stationary series.

#I ran below code to get value of 3 parameters for ARIMA from auto.arima
ARIMAfit &lt;- auto.arima(diff(diff(val.ts)), approximation=FALSE,trace=FALSE, xreg=diff(diff(xreg)))
#Finally ran ARIMA
fit &lt;- Arima(diff(diff(val.ts)),order=c(5,0,2),xreg = diff(diff(xreg)))

#plot original to see fit
plot(diff(diff(val.ts)),col=""orange"")
#plot fitted
lines(fitted(fit),col=""blue"")
</code></pre>

<p>This gives me a perfect fit time series. However, how do i reconvert fitted values into their original metric from the current form it is now in? i mean from double differencing into actual number? For log i know we can do 10^fitted(fit) for square root there is similar solution, however what to do for differencing, that too double differencing?</p>

<p>Any help on this please in R? After days of rigorous exercise, i am stuck at this point.</p>

<p>Edit: Let me paste images from 3 iterations i ran to test if differencing has any impact on model fit of auto.arima function and found that it does. so auto.arima can't handle non stationary series and it requires some effort on part of analyst to convert the series to stationary.</p>

<p>Firstly, auto.arima without any differencing. Orange color is actual value, blue is fitted.</p>

<pre><code>ARIMAfit &lt;- auto.arima(val.ts, approximation=FALSE,trace=FALSE, xreg=xreg)
plot(val.ts,col=""orange"")
lines(fitted(ARIMAfit),col=""blue"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/VWVHK.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/VWVHK.png"" alt=""enter image description here""></a></p>

<p>secondly, i tried differencing</p>

<pre><code>ARIMAfit &lt;- auto.arima(diff(val.ts), approximation=FALSE,trace=FALSE, xreg=diff(xreg))
plot(diff(val.ts),col=""orange"")
lines(fitted(ARIMAfit),col=""blue"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/sTnxQ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/sTnxQ.png"" alt=""enter image description here""></a> </p>

<p>thirdly, i did differencing 2 times.</p>

<pre><code>ARIMAfit &lt;- auto.arima(diff(diff(val.ts)), approximation=FALSE,trace=FALSE, 
xreg=diff(diff(xreg)))
plot(diff(diff(val.ts)),col=""orange"")
lines(fitted(ARIMAfit),col=""blue"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/1x8ex.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1x8ex.png"" alt=""enter image description here""></a></p>

<p>A visual inspection can suggest that 3rd graph is more accurate out of all. This i am aware of. The challenge is how to reconvert this fitted value which is in the form of double differenced form into the actual metric!</p>

<p>Edit2: Why it is not so simple. Let me explain by below example.</p>

<p>Actual data with single difference and double difference.
<a href=""http://i.stack.imgur.com/hJSOF.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/hJSOF.png"" alt=""enter image description here""></a></p>

<p>Lets go back to actual data by using differences and first value of prior series.</p>

<p><a href=""http://i.stack.imgur.com/IW6js.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/IW6js.png"" alt=""enter image description here""></a></p>

<p>If i use diff(diff(val.ts)) in auto.arima as input data, i get below fitted values. However i do not have first value of first order difference of fitted value and neither i have first data point in fitted value in original metric format! This is where i am struck!</p>

<p><a href=""http://i.stack.imgur.com/llFtr.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/llFtr.png"" alt=""enter image description here""></a></p>

<p>What if i use Richard Hardy's advice and use data from actual series as reference. This gives me negative numbers. Can you imagine negative sales? And to clarify my original numbers do not have ANY negative number and it does not have any returns or cancellation data!</p>

<p><a href=""http://i.stack.imgur.com/IEKrJ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/IEKrJ.png"" alt=""enter image description here""></a></p>
"
"0.136398867894095","0.123589296520629","189050","<p>I have a time series of number of visitors of an website for two years. I have to do the forecast of the number of visitors for the next semestre. To do so, I used three forecast models: ARIMA, auto_ARIMA and Holt Winters.</p>

<p>My problem is how to choose the most approprite model based on these accuracy function outputs:</p>

<p>accuracy function output for arima model:</p>

<p><a href=""http://i.stack.imgur.com/5TLKH.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5TLKH.png"" alt=""enter image description here""></a></p>

<p>accuracy function output for auto-arima model:</p>

<p><a href=""http://i.stack.imgur.com/yFcPs.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/yFcPs.png"" alt=""enter image description here""></a></p>

<p>accuracy function output for Holt Winters model:</p>

<p><a href=""http://i.stack.imgur.com/QdffX.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/QdffX.png"" alt=""enter image description here""></a></p>

<p>Here's the graph of the initial time series:</p>

<p><a href=""http://i.stack.imgur.com/xtJ1J.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xtJ1J.png"" alt=""enter image description here""></a></p>
"
"0.159941849898134","0.165624338327385","189983","<p>I have daily data from last 2 years.</p>

<p>I want to do ARIMAX and the regressor component being autoregressive distributed lag of the same variable. Since it has impact, along with dummy variables to account for seasonality in the <code>xreg</code> paratemer in <code>auto.arima</code> function.</p>

<p>The challenge i am facing is predicting my predictor for future. For example, i used daily data for 2 year for model building. For forecasting into future, i also need values of lag variable, which i do not know. If i use 2 lags of daily data in the model, then in order to predict for future i will also need value of those lag variables as well. So to predict $Value$ at time $t$ i will need $Value$ at $t-1$ and $t-2$ which i have from past records. However, if i want to find value at $t+5$ then i will need to find $t+3$ and $t+4$. Not sure how to proceed in this direction. As stated earlier, i am using <code>auto.arima</code> function from <code>forecast</code> package in <code>R</code> . </p>

<p>My ultimate goal is to predict for next 365 days. What i assume to be a solution is that i predict for $t+1$ as it will require $t$ and $t-1$ as lag component which i already have. once done i can use this predicted $t+1$ component to predict for $t+2$ as i will know value of $t+1$ from previous iteration and $t$ from original values. Is it the right approach?</p>
"
"0.0482242822170412","0.0499376169438922","190476","<p>In R, if one includes external regressors in the function <code>arima</code>, are the regressor lags considered (Box-Jenkins or something similar)?</p>

<p>The following suggests that they are:
<a href=""http://stats.stackexchange.com/questions/121749/definition-of-arima-with-exogenous-regressors-in-r"">Definition of ARIMA with exogenous regressors in R</a>.</p>

<p>The following, as well as many other sources suggest that they are not:
<a href=""http://stats.stackexchange.com/questions/25780/what-is-the-purpose-of-and-how-to-use-the-xreg-argument-when-fitting-arima-model"">What is the purpose of and how to use the xreg argument when fitting ARIMA model in R?</a></p>

<p>Rob Hyndman's <a href=""http://robjhyndman.com/hyndsight/arimax/"" rel=""nofollow"">explanation</a> is ambiguous to me.</p>
"
"0.167053813916911","0.172988979511468","192739","<p>I have been working with the forecast package in R a lot, recently. And my question might seem trivial (or not, maybe I'm missing something), but for the life of me I can't seem to find a way to fit an Arima model with exogenous variables (<code>xreg</code> argument) that has been computed by the <code>auto.arima</code> function to previously unseen test data.</p>

<p>So, I'm basically trying to do the following:</p>

<pre><code>library(forecast)
fit &lt;- auto.arima(trainingdata, xreg = trainingvariables)
</code></pre>

<p>...and then I would like to ""apply"" the model to new test data, for which I also have new exogenous variables available. I can see the following methods:</p>

<pre><code>fitted(fit)
</code></pre>

<p>That returns one-step in-sample forecasts, so, in effect, that's exactly what I want. Except that it's in-sample. However, I would like to calculate <strong>one-step out-of-sample forecasts</strong> (with <strong>new exogenous variables</strong> that I have available). Another method:</p>

<pre><code>forecast(fit, xreg = newvariables, h = ...)
</code></pre>

<p>That works for exactly one step, but then seems to merely forecast the trainingdata stored in the model fit. But I don't think I can use new testdata here? (So, I can't use this method for testing one-step prediction accuracy.) One more idea:</p>

<pre><code>fit2 &lt;- Arima(testdata, model = fit)
</code></pre>

<p>According to the manual, if the <code>model</code> parameter is used, ""this same model is
fitted to [testdata] without re-estimating any parameters"". Great, but I don't think I can supply any new exogenous variables, can I?</p>

<p>I really think, I must be missing something simple. Any help would be much appreciated.</p>
"
"0","0.0499376169438922","193178","<p>Please consider the following code (in R)</p>

<pre><code>library(forecast)
tt&lt;-structure(c(1494.5, 1367.57, 1357.57, 1222.23, 1124.02, 1011.64, 
4575.64, 3201.87, 3050.04, 2173.38, 1967.88, 1838.55, 1666.05, 
1656.05, 1524.96, 835.96, 775.36, 592.36, 494.15, 4058.15, 2624.36, 
2448.47, 1598.47, 1398.47, 1264.14, 1165.88, 1053.67, 941.36, 
821.36, 471.36, 373.15, 259.91, 3808.91, 2262.26, 1940.39, 1011.39, 
800.81, 790.81), index = structure(c(16563L, 16565L, 16570L, 
16572L, 16577L, 16579L, 16584L, 16585L, 16586L, 16587L, 16588L, 
16589L, 16590L, 16592L, 16593L, 16599L, 16606L, 16607L, 16608L, 
16612L, 16613L, 16614L, 16617L, 16618L, 16619L, 16620L, 16621L, 
16628L, 16633L, 16635L, 16638L, 16642L, 16647L, 16648L, 16649L, 
16650L, 16651L, 16654L), class = ""Date""), class = ""zoo"")

tt2&lt;-as.ts(tt)
tt2&lt;-na.locf(tt2) #I replace the NA with the previous non-NA value
mm&lt;-auto.arima(tt2)

plot(forecast(mm, h=60))
</code></pre>

<p>The results of the auto.arima function is puzzling...
There is a clear seasonality in the data (this is the balance of an account: every month a salary is cashed in and there is a spike in the value of the series, followed by a decrease until the next salary is received). I would like to forecast a couple of cycles, but the auto.arima forecast is nothing like I expect.
Does anybody have any suggestions (also outside the auto.arima)?
Any suggestion is welcome.</p>
"
"0.0482242822170412","0.0499376169438922","193945","<p>I want to use ARIMA for forecasting website visits after some fixed amount of seconds, say 100 seconds.</p>

<p>I have csv file available that contain two columns; one is the time and another is the visits, like below</p>

<pre><code>time,visitors

0,0.23171857

100,0.255455594

200,0.38544406

300,0.302929642

400,0.292756632

500,0.339100346
</code></pre>

<p>What is the frequency I should use in function <code>ts</code>?</p>

<p>I want to use one-day data for forecasting for the second day.</p>
"
"0.0835269069584557","0.0864944897557338","194130","<p>I want to get the cross-correlation of two time series <code>x</code> and <code>y</code> in R. </p>

<p>I have calculated an ARIMA model, and I can get the <code>mod1$residuals</code> from signal <code>x</code>. These residuals almost have no autocorrelation, so that's great. </p>

<pre><code>xts &lt;- ts(x,start=1,frequency=12) #convert to a time series
library(fpp)  #load forecasting package
mod1 &lt;- auto.arima(xts)
</code></pre>

<p>I now did the same procedure on signal <code>y</code>. </p>

<p>My question is: is this correct? Or should I somehow deduct the <code>mod1</code> (based on <code>x</code>) from <code>y</code> to de-trend it? </p>

<pre><code>ccf(mod1$residuals, mod2$residuals)
</code></pre>

<p>Secondly, I am confused about the order of operations. Should I prewhiten the data before calculating the model? </p>

<p>I found this code: </p>

<pre><code>prewhiten(x, y, x.model = ar.res,ylab=""CCF"", ...)
</code></pre>

<p>Should I estimate the <code>mod1</code> first and then supply it to the function <code>prewhiten</code>? And are <code>x</code> and <code>y</code> the two time series? Many thanks!</p>
"
"0.118124884643724","0.122321680483099","194468","<p>I am using function <code>prewhiten</code> from ""TSA"" package in R. I get an error about <code>NA</code> values, but I don't understand it, because I don't have <code>NA</code> values in my data. Here is the error message:</p>

<pre><code>whitedata &lt;- prewhiten(xhr, ypred, mod1)

Error in na.omit.ts(as.ts(x)) : all times contain an NA
</code></pre>

<p>It works fine for some data files, but not for others. When I print <code>xhr</code> and <code>ypred</code> I don't see any <code>NA</code> values. </p>

<p>Both are time series: </p>

<pre><code>xhr &lt;- ts(data$hr_z,start=1,frequency=10) #convert to a time series
ypred &lt;- ts(data$pred_z,start=1,frequency=10) #convert to a time series
</code></pre>

<p>Strangely, if I run it with a different model (one built on <code>ypred</code>), it runs just fine. The model I am using is: </p>

<pre><code>ARIMA(2,1,2)                    

Coefficients:
         ar1      ar2      ma1     ma2
      1.4835  -0.7641  -0.9574  0.4021
s.e.  0.1136   0.0826   0.1365  0.0910

sigma^2 estimated as 0.02589:  log likelihood=79.98
AIC=-149.96   AICc=-149.65   BIC=-133.55
</code></pre>
"
"0.186771841909407","0.180513721521448","195443","<p>I am looking at two time series, from 01/01/2000 to the present: <br></p>

<ul>
<li>The <a href=""https://research.stlouisfed.org/fred2/series/NAPMNOI/"" rel=""nofollow"" title=""ISM Manufacturing: New Orders Index"">ISM Manufacturing: New Orders Index</a>, only available seasonally adjusted</li>
<li>The manufacturing industry unemployment rate, only available unadjusted (<a href=""https://research.stlouisfed.org/fred2/series/LNU04032232"" rel=""nofollow"">https://research.stlouisfed.org/fred2/series/LNU04032232</a>)</li>
</ul>

<p>I was <em>hoping</em> to construct a multivariate ts model, and use the <strong>New Orders Index</strong> to forecast the <strong>manufacturing industry unemployment rate</strong>. However, am I correct in assuming it is not 'ideal' to use seasonally adjusted data to predict another time series? Because doesn't SA cause (ideally) all the seasonal time series structure to be removed from the data?</p>

<h3>EDIT:</h3>

<p>Sorry, it just now hit me to link to the data I was using by putting it on Google Drive. It's in .csv files, for easy viewing with any program.</p>

<ul>
<li>Manufacturing new orders index data, in <strong>OrdersIndex.csv</strong><br><a href=""https://drive.google.com/file/d/0B2Y54SySHrVwZXczR1N4LXZMdXc/view?usp=sharing"" rel=""nofollow"">https://drive.google.com/file/d/0B2Y54SySHrVwZXczR1N4LXZMdXc/view?usp=sharing</a></li>
<li>Manufacturing industry unemployment rate, in <strong>Unem.csv</strong>
<br><a href=""https://drive.google.com/file/d/0B2Y54SySHrVweFVpRjJFanAwQmc/view?usp=sharing"" rel=""nofollow"">https://drive.google.com/file/d/0B2Y54SySHrVweFVpRjJFanAwQmc/view?usp=sharing</a></li>
</ul>

<p>Below is the New Orders Index time series, with the dashed line indicating the mean of 54.61. It looks fairly stationary to me; a decent spike in 2008, but definitely reverts to the mean.</p>

<pre><code>&gt; plot.ts(OrdersIndex[,2])
&gt; mean(OrdersIndex[,2])
[1] 54.60829
&gt; abline(h=c(54.61), lty=2)
&gt; 
</code></pre>

<p><a href=""http://i.stack.imgur.com/C61sm.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/C61sm.png"" alt=""New Orders Index""></a></p>

<p>The ACF and PACF of the series are below. ACF displays dampened sine-wave behavior, PACF has a sharp cut-off after lag 1. This suggests an AR(1) model, as the ACF's slow dying off (at lags > 1) is due to the auto correlation at lag 1.</p>

<pre><code>&gt; Acf(OrdersIndex[,2], plot=T)   #the Acf() function is part of 'forecast' package
&gt; Acf(OrdersIndex[,2], plot=T, type=c('partial'))
&gt;
</code></pre>

<p><a href=""http://i.stack.imgur.com/Dg2Es.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Dg2Es.png"" alt=""ACF plot""></a>
<a href=""http://i.stack.imgur.com/0PqBR.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/0PqBR.png"" alt=""PACF plot""></a></p>

<p>After running an arima(1,0,0) model with a mean, the ACF and PACF of the residuals do not show significant spikes at any lags.</p>

<pre><code>&gt; OrdersIndex100 &lt;- arima(OrdersIndex[,2], order=c(1,0,0))
&gt; OrdersIndex100

Call:
arima(x = OrdersIndex[, 2], order = c(1, 0, 0))

Coefficients:
         ar1  intercept
      0.8738    54.6979
s.e.  0.0341     1.9399

sigma^2 estimated as 12.39:  log likelihood = -517.44,  aic = 1040.88
&gt;
</code></pre>

<p>Running an Ljung-Box test on the residuals indicates there is not any time series structure left in the data.</p>

<pre><code>&gt; LBQPlot(OrdersIndex100$residuals, k=1)   # LBQPlot() is part of 'FitAR' package
&gt;
</code></pre>

<p><a href=""http://i.stack.imgur.com/xXQKc.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xXQKc.png"" alt=""Ljung-Box Test""></a></p>

<h3>Conclusion</h3>

<p>The conclusion I arrive at is that the seasonally adjusting done to the data by the ISM (Institute of Supply Management) effectively removed all the seasonality from the data. So, this SA data would be less useful in modeling than non-SA data (this is assuming that I would be using this data series as the Input, and the unemployment data series as the Output). Is this a valid conclusion? You all see any glaring problems with my analysis?</p>
"
"0.0482242822170412","0.0499376169438922","198887","<p>I am trying to model a time series that contains a sequence of zeros. I tried fitting an ARIMA model using <code>auto.arima</code> function from the forecast package in R but the MAPE is reported as infinity (probably due to division by zero). Moreover, the <code>auto.arima</code> fits an ARIMA(0,1,0) model over the data. </p>

<p>Can you suggest any types of models that may be appropriate for such data?</p>
"
"0.0835269069584557","0.0864944897557338","201669","<p>I have read that auto.arima choses the model with the best AIC.</p>

<p>I am looking to create a model that best neutralises the autocorrelation, as it will be used for prewhitening.</p>

<p>Can I use auto.arima for that, or should I use a different function/is this at all possible in R? I am looking to find the best model that removes the acf.</p>
"
"0.0681994339470473","0.0706224551546449","202326","<p>I have a huge dataset with over 4000 companies and I have estimated a liquidity measure for these each 4000 companies. But liquidity is highly persistent and exihibits auto-correlation. In order to mitigate this autocorrelation problem each of the liquidity measure estimated for the company has to be trasformed by AR(2) process i.e. residuals of autoregressive model are used instead of actual values.
But when I estiamate the AR(2) with following code in r</p>

<pre><code>AR&lt;- data.frame(dfAR1, apply(dfAR1, 2, function(x) arima(x, order = c(2,0,0),optim.method=""Nelder-Mead"")$res))
</code></pre>

<p>I receive warning </p>

<pre><code> arima(x, order = c(2, 0, 0), optim.method = ""Nelder-Mead"") :
  possible convergence problem: optim gave code = 10
</code></pre>

<p>When I looked up in the manual it says: 
""10
indicates degeneracy of the Nelderâ€“Mead simplex""
I don't understand how bad this warning is for my estimations and how can I fix it.
I would really appreciate your help in this regard.</p>
"
"NaN","NaN","203114","<p>I am trying to fit mortality data to best ARIMA model. What is the difference between function forecast.lca and auto.arima when choosing a model to fit. forecast.lca also includes random walk, but is there any other difference?</p>
"
"0.0681994339470473","0.0706224551546449","203142","<p>my professor says that if we see that there are NA values for the AR or MA terms(either the estimated values or the estimated se) in the R output for the arima models fitted using the arima() function, it is suggesting that our model is over fit. Is he right?    </p>

<p>I figured out that if I allow the function for a slightly higher number of iterations allowed, we can get get rid of those NAs in estimates and be able to obtain the a model.    </p>

<p>So it appears to me that it can be just a simple problem with the optimization rather than blame the model to be too complex and overfit.  </p>
"
"0.144672846651124","0.149812850831677","203806","<p>Let $\left\{X_t\right\}$ be a stochastic process formed by concatenating iid draws from an AR(1) process, where each draw is a vector of length 10. In other words, $\left\{X_1, X_2, \ldots, X_{10}\right\}$ are realizations of an AR(1) process; $\left\{X_{11}, X_{12}, \ldots, X_{20}\right\}$ are drawn from the same process, but are independent from the first 10 observations; et cetera.</p>

<p>What will the ACF of $X$ -- call it $\rho\left(l\right)$ -- look like?  I was expecting $\rho\left(l\right)$ to be zero for lags of length $l \geq 10$ since, by assumption, each block of 10 observations is independent from all other blocks.</p>

<p>However, when I simulate data, I get this:</p>

<pre><code>simulate_ar1 &lt;- function(n, burn_in=NA) {
    return(as.vector(arima.sim(list(ar=0.9), n, n.start=burn_in)))
}

simulate_sequence_of_independent_ar1 &lt;- function(k, n, burn_in=NA) {
    return(c(replicate(k, simulate_ar1(n, burn_in), simplify=FALSE), recursive=TRUE))
}

set.seed(987)
x &lt;- simulate_sequence_of_independent_ar1(1000, 10)
png(""concatenated_ar1.png"")
acf(x, lag.max=100)  # Significant autocorrelations beyond lag 10 -- why?
dev.off()
</code></pre>

<p><a href=""http://i.stack.imgur.com/r1luW.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/r1luW.png"" alt=""sample autocorrelation function for x""></a></p>

<p>Why are there autocorrelations so far from zero after lag 10?</p>

<p>My initial guess was that the burn-in in arima.sim was too short, but I get a similar pattern when I explicitly set e.g. burn_in=500.</p>

<p>What am I missing?</p>

<hr>

<p><strong>Edit</strong>: Maybe the focus on concatenating AR(1)s is a distraction -- an even simpler example is this:</p>

<pre><code>set.seed(9123)
n_obs &lt;- 10000
x &lt;- arima.sim(model=list(ar=0.9), n_obs, n.start=500)
png(""ar1.png"")
acf(x, lag.max=100)
dev.off()
</code></pre>

<p><a href=""http://i.stack.imgur.com/GA8sD.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/GA8sD.png"" alt=""acf of plain vanilla ar1""></a></p>

<p>I'm surprised by the big blocks of significantly nonzero autocorrelations at such long lags (where the true ACF $\rho(l) = 0.9^l$ is essentially zero).  Should I be?</p>

<hr>

<p><strong>Another Edit</strong>: maybe all that's going on here is that $\hat{\rho}$, the estimated ACF, is itself extremely autocorrelated.  For example, here's the joint distribution of $\left(\hat{\rho}(60), \hat{\rho}(61)\right)$, whose true values are essentially zero:</p>

<pre><code>## Look at joint sampling distribution of (acf(60), acf(61)) estimated from AR(1)
get_estimated_acf &lt;- function(lags, n_obs=10000) {
    stopifnot(all(lags &gt;= 1) &amp;&amp; all(lags &lt;= 100))
    x &lt;- arima.sim(model=list(ar=0.9), n_obs, n.start=500)
    return(acf(x, lag.max=100, plot=FALSE)$acf[lags + 1])
}
lags &lt;- c(60, 61)
acf_replications &lt;- t(replicate(1000, get_estimated_acf(lags)))
colnames(acf_replications) &lt;- sprintf(""acf_%s"", lags)
colMeans(acf_replications)  # Essentially zero
plot(acf_replications)
abline(h=0, v=0, lty=2)
</code></pre>

<p><a href=""http://i.stack.imgur.com/iIvCJ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/iIvCJ.png"" alt=""sampling distribution of estimated acf""></a></p>
"
"0.0482242822170412","0.0499376169438922","204440","<p>I'm using the <code>auto.arima</code> function in R's <code>forecast</code> package to build an ARIMA model with external regressors. I have a non-seasonal monthly stationary time-series dataset as shown below:</p>

<pre><code>&gt; dim(tsdata)
[1] 95  4
&gt; head(tsdata)
                    y         x1         x2          x3
2007-02-01  0.0532113 -0.7547812 -1.1156320  1.15193457
2007-03-01 -0.4461565  0.5104070  1.2489777 -1.19172591
2007-04-01 -1.4087036  2.0866994  0.2835917  0.15941672
2007-05-01 -0.4960451 -1.9455242 -2.6847517 -0.06603252
2007-06-01  0.8025322 -2.9295067 -0.6049654  0.34332637
2007-07-01 -0.8053754 -0.2385492 -1.7850528 -1.29843072
</code></pre>

<p>I can use <code>auto.arima(tsdata[,1], xreg=tsdata[,2:4])</code> to fit a model with <code>x1</code>, <code>x2</code>, and <code>x3</code> as regressors. My question is, is there a way to model the interaction between external regressions?</p>
"
"0.118124884643724","0.122321680483099","205897","<p>I modeled a univariate time series in R using the <code>Arima</code> command. One can obtain fitted values for the original series using this command by applying the function <code>fitted</code> to the model. However, I noticed that the fitted data has the same dimension as the original data. Hence, a fitted value for the first value in the time series was computed even though there is no past data. I checked wheter it is the mean of the series or the intercept of the model but that isn't the solution. What are possible approaches to get a fitted value here?</p>

<p>The code below is a reproducible example using the <code>Lynx</code> data set.</p>

<pre><code>&gt; library(xts)
&gt; library(forecast)
&gt; 
&gt; data(""lynx"")
&gt; 
&gt; Y      &lt;- as.xts(log10(lynx))
&gt; model  &lt;- Arima(Y, order= c(1, 0, 0))  #Fit AR(1) model
&gt; fit    &lt;- fitted(model)
&gt; length(fit) == length(Y)  
[1] TRUE
</code></pre>
"
"0.174320385782113","0.167619884269916","206188","<p>I'm trying to forecast data that has an hourly and weekly pattern.  The model I made using predictors created using seasonaldummy does a nice job of picking up the hourly weekly pattern, but it takes a long time to train the model.  I tried to create a similar forecast using fourier function, but it doesn't seem to be picking up the hourly pattern as well.  Am I setting up fourier correctly to try to achieve the effect I've gotten with seasonaldummy?  Should the frequency in ts be something other than 168?  My data is hourly.  I've provided some sample data below.  </p>

<p>My end goal is to combine the predictors for the hourly weekly pattern with other predictors, that's why I'm not just using tbats. I've provided examples below of how I'm trying to combine dummy variables for the hourly weekly pattern with other predictors.  </p>

<p>Code:</p>

<pre><code>##BoxCox

TTTlambda &lt;- BoxCox.lambda(tsData)

##Partitioning Time Series
EndTrain&lt;-1344
ValStart&lt;-EndTrain+1
ValEnd&lt;-ValStart+336

tsTrain &lt;-tsData[1:EndTrain]
tsValidation&lt;-tsData[ValStart:ValEnd]
tsTest &lt;- tsData[TestStart:TestEnd]

##Predictors
xregTrain&lt;-dfPredictors[1:EndTrain,]
xregVal&lt;-dfPredictors[ValStart:ValEnd,]
xregTest&lt;-dfPredictors[TestStart:TestEnd,]

##Seasonal Dummies
x=ts(tsData,freq=168) 
dummies=seasonaldummy(x)
xreg2Train&lt;-dummies[1:EndTrain,]
xreg2Val&lt;-dummies[ValStart:ValEnd,]
xreg2Test&lt;-dummies[TestStart:TestEnd,]


##Fourier Terms

tsTTT&lt;-ts(tsData, freq=168)

bestfit &lt;- list(aicc=Inf)
for(i in 1:25)
{
  fit &lt;- auto.arima(tsTTT, xreg=fourier(tsTTT, K=i), seasonal=FALSE)
  if(fit$aicc &lt; bestfit$aicc)
    bestfit &lt;- fit
  else break;
}

bestfit$coef ## K=2

xreg3&lt;-fourier(tsTTT,2)

xreg3Train&lt;-xreg3[1:EndTrain,]
xreg3Val&lt;-xreg3[ValStart:ValEnd,]
xreg3Test&lt;-xreg3[TestStart:TestEnd,]


##hourly weekly
Arima.fit_D &lt;- auto.arima(tsTrain, lambda = TTTlambda, xreg=xreg2Train, stepwise=FALSE, approximation = FALSE, seasonal = FALSE )

Arima.fit_D_P &lt;- auto.arima(tsTrain, lambda = TTTlambda, xreg=cbind(xreg2Train,xregTrain$Predictor), stepwise=FALSE, approximation = FALSE, seasonal = FALSE )

##Fourier hourly weekly
Arima.fit_F &lt;- auto.arima(tsTrain, lambda = TTTlambda, xreg=xreg3Train, stepwise=FALSE, approximation = FALSE, seasonal = FALSE )

Arima.fit_F_P &lt;- auto.arima(tsTrain, lambda = TTTlambda, xreg=cbind(xreg3Train,xregTrain$Predictor), stepwise=FALSE, approximation = FALSE, seasonal = FALSE )

##Forecast Model

Acast_D&lt;-forecast(Arima.fit_D,xreg=xreg2Val, h=336)

Acast_D_P&lt;-forecast(Arima.fit_D_P,xreg=cbind(xreg2Val,xregVal$Predictor), h=336)

Acast_F&lt;-forecast(Arima.fit_F,xreg=xreg3Val, h=336)

Acast_F_P&lt;-forecast(Arima.fit_F_P,xreg=cbind(xreg3Val,xregVal$Predictor), h=336)
</code></pre>

<p>Data:</p>

<pre><code>dput(tsData[1:1681])
</code></pre>

<p>c(11, 14, 17, 5, 5, 5.5, 8, NA, 5.5, 6.5, 8.5, 4, 5, 9, 10, 11, 
7, 6, 7, 7, 5, 6, 9, 9, 6.5, 9, 3.5, 2, 15, 2.5, 17, 5, 5.5, 
7, 6, 3.5, 6, 9.5, 5, 7, 4, 5, 4, 9.5, 3.5, 5, 4, 4, 9, 4.5, 
6, 10, NA, 9.5, 15, 9, 5.5, 7.5, 12, 17.5, 19, 7, 14, 17, 3.5, 
6, 15, 11, 10.5, 11, 13, 9.5, 9, 7, 4, 6, 15, 5, 18, 5, 6, 19, 
19, 6, 7, 7.5, 7.5, 7, 6.5, 9, 10, 5.5, 5, 7.5, 5, 4, 10, 7, 
5, 12, 6, NA, 4, 2, 5, 7.5, 11, 13, 7, 8, 7.5, 5.5, 7.5, 15, 
7, 4.5, 9, 3, 4, 6, 17.5, 11, 7, 6, 7, 4.5, 4, 4, 5, 10, 14, 
7, 7, 4, 7.5, 11, 6, 11, 7.5, 15, 23.5, 8, 12, 5, 9, 10, 4, 9, 
6, 8.5, 7.5, 6, 5, 8, 6, 5.5, 8, 11, 10.5, 4, 6, 7, 10, 11.5, 
11.5, 3, 4, 16, 3, 2, 2, 8, 4.5, 7, 4, 8, 11, 6.5, 7.5, 17, 6, 
6.5, 9, 12, 17, 10, 5, 5, 9, 3, 8.5, 11, 4.5, 7, 16, 11, 14, 
6.5, 15, 8.5, 7, 6.5, 11, 2, 2, 13.5, 4, 2, 16, 11.5, 3.5, 9, 
16.5, 2.5, 4.5, 8.5, 5, 6, 7.5, 9.5, NA, 9.5, 8, 2.5, 4, 12, 
13, 10, 4, 6, 16, 16, 13, 8, 12, 19, 19, 5.5, 8, 6.5, NA, NA, 
NA, 15, 12, NA, 6, 11, 8, 4, 2, 3, 4, 10, 7, 5, 4.5, 4, 5, 11.5, 
12, 10.5, 4.5, 3, 4, 7, 15.5, 9.5, NA, 9.5, 12, 13.5, 10, 10, 
13, 6, 8.5, 15, 16.5, 9.5, 14, 9, 9.5, 11, 15, 14, 5.5, 6, 14, 
16, 9.5, 23, NA, 19, 12, 5, 11, 16, 8, 11, 9, 13, 6, 7, 3, 5.5, 
7.5, 19, 6.5, 5.5, 4.5, 7, 8, 7, 10, 11, 13, NA, 12, 1.5, 7, 
7, 12, 8, 6, 9, 15, 9, 3, 5, 11, 11, 8, 6, 3, 7.5, 4, 7, 7.5, 
NA, NA, NA, NA, 6.5, 2, 16.5, 7.5, 8, 8, 5, 2, 7, 4, 6.5, 4.5, 
10, 6, 4.5, 6.5, 9, 2, 6, 3.5, NA, 5, 7, 3.5, 4, 4.5, 13, 19, 
8.5, 10, 8, 13, 10, 10, 6, 13.5, 12, 11, 5.5, 6, 3.5, 9, 8, NA, 
6, 5, 8.5, 3, 12, 10, 9.5, 7, 24, 7, 9, 11.5, 5, 7, 11, 6, 5.5, 
3, 4.5, 4, 5, 5, 3, 4.5, 6, 10, 5, 4, 4, 9.5, 5, 7, 6, 3, 13, 
5.5, 5, 7.5, 3, 5, 6.5, 5, 5.5, 6, 4, 3, 5, NA, 5, 5, 6, 7, 8, 
5, 5.5, 9, 6, 8.5, 9.5, 8, 9, 6, 12, 5, 7, 5, 3.5, 4, 7.5, 7, 
5, 4, 4, NA, 7, 5.5, 6, 8.5, 6.5, 9, 3, 2, 8, 15, 6, 4, 10, 7, 
13, 14, 9.5, 9, 18, 6, 5, 4, 6, 4, 11.5, 17.5, 7, 8, 10, 4, 7, 
5, 9, 6, 5, 4, 8, 4, 2, 1.5, 3.5, 6, 5.5, 5, 4, 8, 10.5, 4, 11, 
9.5, 5, 6, 11, 21, 9.5, 11, 13.5, 7.5, 13, 10, 7, 9.5, 6, 10, 
5.5, 6.5, 12, 10, 10, 6.5, 2, 8, NA, 10, 5, 4, 4.5, 5, 7.5, 12, 
22, 5, 8.5, 2.5, 3, 10.5, 4, 7, 13, 4, 3, 5, 6.5, 3, 9, 9.5, 
16, NA, 4, 12, 4.5, 7, 5.5, 8, 14, 3, 8, 12, 14, 7, 8, 6, 8.5, 
6, 6.5, 15.5, 13, 3.5, 12, 7, 6, NA, 3, 5.5, 8.5, 9, 12, 13, 
8, 6.5, 8, 3, 5, 16.5, 2, 7, 6, 2, 5, 6.5, 3, 3, 7, 2, NA, 13, 
7, 16, 13, 12.5, 12, 7, 13, 11, 21.5, 16, 20, 3, 4, 5, 7, 11, 
7, 9, 11, 7, 13, 4, 14, 5, 12, 6, 7, 9, 12, 7, 12.5, 6.5, 16, 
5, 12, 9, 9.5, 9, 7, 9.5, 3, 13, 8, 7, 7, 7, 9, 6, 6, 11, 15, 
9, 6, 19, 10.5, 4, 6, 14.5, 9, 17, 14, 4, 16, 5, 6.5, 10, 9, 
17, 11.5, 3, 5, 9, 8, 16, 10, NA, 7, 5, 12.5, 12, 11, 3, 3, 3.5, 
14, 12, 7, 4, NA, 6, NA, 6, 10, 8, 10, 2, NA, 4, 5.5, 14, 4, 
4.5, 8.5, 13, 21, 10, 11.5, 18, 5, 3, 2, 6, 11, 3, 7.5, 6, 3, 
5, 9, 7.5, 7.5, 5, 9, 17, 3, 9.5, 5.5, 9.5, 15, 14.5, 10, 9, 
13.5, 12, 12, 3, 11, 6, 4, 8, 17.5, 7.5, 7.5, NA, 7, 4, 6, 6, 
6, 6, 6, 5, 8.5, 6, 6, 5, 6, 7, 5, 5, 5, 5, 7, 6, 8, 14, 6.5, 
9.5, 5, 18.5, 5, 8, 10, 16, 12, 13, 7, 6, 13, 9, 18, 17, 8, 7, 
3, 8, 2, 9, 11, 5, 2, 5.5, 6.5, 7, 10, 2, 3, 2, 3, 5, 4, 5, 6, 
3, 5, 3.5, 5, 4, 9, NA, 10.5, 16, NA, 11, 8.5, 13, 4, 12.5, 12, 
13, 18.5, 21, 5, 9, 4.5, 3, 3, 4, 3, 4, 4, 2, 8, 4.5, 4, 5, 9, 
5, 4.5, 4, 7.5, 6, 7, 22, 5, 8, 5, 7, 4, 8, 5.5, 3, 8, 7, 6, 
7.5, 6, 15, 13.5, 10, 7, 2.5, 7.5, 9, 9.5, 8, 19, 8, 8, 10, 6, 
9, 5, 4.5, 9, 3.5, 4, 3.5, 8, 5, 3.5, 8.5, 9, 12.5, 7, 8, 10.5, 
10, 1.5, 5, 10, 9, 2, 5, 8, 11, 3, 4.5, 2, 8.5, 4, 8, 2, 3, 4, 
5.5, 2, 4, 6, 4.5, 6, 6.5, 0, 2, 3.5, 10, 7, 14, 14, 12.5, 3, 
7, 8, 3, 7, 12, 12.5, 2, 2.5, 3, 9, 10.5, 8, 6, 6.5, 8.5, 5, 
10.5, 9, 3.5, 7, 5, 8, 5, 5, 5.5, 4, 9, 8, 5.5, 5, 6, 10.5, 4, 
9, 6, 5, 11, 10.5, 10.5, 4, 11.5, 11, 6, 2, 9, 5, 9, 5, 5.5, 
7, 4, 10, 5, 3, 9, 9, 19.5, 13, 6, 15, 7, 10, 8, 10.5, 8, 16, 
7, 10.5, 8.5, 10.5, 8, 8, 7, 5, 5, 6, 6, 5, 4, 9, 6.5, 4, 7, 
7, 5, 4, 7, 6, 3, 6, 8.5, 8.5, 4, 5.5, 7, 8, 5, 6, 3, 9, 12, 
6, 7.5, 4, 3, 5.5, 2, 5.5, 7, NA, 8.5, 2, 5, 8, 8, 4, 3, 6, 4, 
4.5, 5, 3, 7.5, 9, 13, 8, 10, 12, 6.5, 3, 3.5, 8.5, 9, NA, 12, 
8, 9, 4, 6, 8, 8, 9.5, 8, 6, 5, 4, 10.5, 6.5, 4, 3.5, 5, 7, 7, 
5, 9, 6, NA, 6, 6, 5, 10, 7, 9, 9, 5, 4, 5, 4, 6, 8, 5, 3, 2.5, 
2.5, 13, 4.5, 2.5, 2, 3, 9.5, 3, 5.5, 6, 10, 9, 10, 13, 14.5, 
9, 7, 6, 5, 4, 4, 4, 5, 6.5, 11, 13.5, 11, 12, 3, 3, 14, 11, 
6, 8, 5.5, 9, 8, 8, 7, 7, 5.5, 3.5, 10.5, 6, 5.5, 8, 8, 15, 6.5, 
8, 9.5, 6.5, 5, 7, 6, 4, 14.5, 4, 2.5, 5, 8, 18, 13, 10, 6, 7, 
18, 4.5, 7, 6.5, 5, 17, 7, 3, 5.5, 4, 6.5, 5.5, 6, 8, NA, 9.5, 
14, 9, 11, 8, 7, 17, 7, 8, 8, 9, 2, 2, 4, 3, 8, 4, 9, 6, 9, 11, 
13, 7.5, 8.5, 6, 6, 10, 17.5, 18.5, 14, 8.5, 4, 5, 6, 3, 2, 4, 
4, 12, 11, 5, 2.5, 8, 6, 10, 5, 8, 8, 10.5, 14, 7, 16, 15, 6, 
4.5, 10, 19, 3, 3, 4.5, 6.5, 4, 7.5, 8, 6, 20, 6, 7, 13, 13, 
4, 10, 6, 5, 4.5, 6, 10, 6, 4, 8.5, 7.5, 3, 3.5, 3, 2, 2, 20.5, 
6, 18, 5.5, 7.5, 5, 3.5, 8, 6, 6.5, 3, 4, 8, 5, 15.5, 4, 5, 8, 
5, 3, 4, 5, 3, 3, 3, 6, 4, 12, 8, 10, 12, 5.5, 9.5, NA, 5, 4.5, 
7, 16, 7, 4.5, 5, 5, 10, 6, 19, 8, 15, 7, 19.5, 10, 7.5, 9, 9, 
7, 8, 3, 6, 5.5, 6, 7, 8, 14, 8, 13, 5.5, 3.5, 5, 9, 4.5, 4, 
4, 3, 7.5, 4, 5, 6.5, 9, 4, NA, 12, 5.5, 6, 12.5, 6.5, 6.5, 5, 
11, 4.5, 8, 2, 4, 5, 5, 3, 2.5, 6, 7, 4, 17, 4, 3, 5, 6, 2, 8, 
8.5, 6.5, 4, 10, 12.5, 11, 6.5, 9, 12.5, 5.5, 5, 7.5, 16, 11.5, 
4, 5.5, 3.5, 4, 3, 6, 4, NA, 5, 6, 7, 3, 4.5, 7, 5.5, 4, 7, 11, 
7, 3, 3, 4, 3.5, 9, 4.5, 8, 5, 6, 8, 5, 5.5, 8, 5, 9, 8, 8, 6.5, 
6, 10, 7, 7, 9, 12, 8, 13, 6.5, 6, 4, 5.5, 6, 3, 7, 8, 15, 10, 
8, 3, NA, 5, 7, 7, 6, 9, 19, 13, 7, 7.5, 11, 8.5, 4, 7.5, 6, 
13.5, 17, 9, 5, 6.5, 6, 4, 5, NA, 3, 6, 10.5, 6, 14, 6, 9.5, 
6, 10, 11, 10, 3, 7, 9, 16.5, 5.5, 12.5, 8, 5, 10, 6, 1, 5, 6.5, 
10, 8.5, 5, NA, 9.5, 13, 10, 10, 20, 7, 8, 5, 3, 3, 4.5, 3.5, 
2, 5, 11, 3, 7.5, NA, 5.5, NA, 6, 6, 11, 12, 7, 5, 15, 11, 6, 
17.5, 13.5, 16, 16.5, 5, 4, 3, 5.5, 3, 8, 11, 8, 12, 14, NA, 
10, 6, 4, 5, 8, 10, 12.5, 6, 3, 6, 5, 8, 6, 11, 12.5, 7, 6, 9.5, 
2, 8.5, 9.5, 8, 8, 2, 7.5, NA, 6, 2.5, 4, 5, 5, 6, 9, 4, 7, 6, 
2, 4.5, 3, 4, 4, 5, 4, 3, 7.5, 8.5, NA, 12, 9, 11, 9, 3, 2.5, 
7, 4, 4, 7, 8.5, 12.5, 3.5, 6.5, 10, 6, 8, 7, 13, 13.5, 12, 13, 
8, NA, 8, 9, 15, NA, 4, 3.5, 2, 7, 8, 7.5, 9.5, 1.5, 5, 4, 8, 
11, 5, 12, 4, 3, 11, 8, 7.5, 5.5, 13, 11, NA, 12, 7, 8, 6, 13, 
8, 5, 4, 7, 8, 2, 3, 4, 4, 5, 5.5, 5.5, 6, 5, 6, 14, 12, 6, 11.5, 
13, 5, 5, 8, 9, 2, 5, 6, 10, 4.5, 4, 7, 7.5, 7, 4, 10, 6.5, 6, 
10)</p>

<pre><code>dput(dfPredictors$Predictor[1:1681])
</code></pre>

<p>c(2, 6, 3, 5, 3, 2, 2, NA, 2, 6, 12, 11, 9, 10, 13, 9, 11, 7, 
12, 8, 6, 4, 10, 6, 2, 7, 2, 1, 3, 2, 1, 3, 8, 7, 7, 8, 13, 13, 
13, 11, 12, 4, 12, 18, 12, 7, 5, 4, 6, 4, 3, 3, NA, 4, 2, 8, 
8, 8, 7, 3, 5, 3, 7, 8, 7, 7, 11, 8, 10, 3, 10, 6, 5, 5, 3, 1, 
2, 1, 1, 3, 4, 8, 8, 5, 9, 12, 12, 11, 8, 5, 9, 10, 7, 8, 4, 
6, 4, 1, 3, 1, 3, NA, 2, 1, 4, 10, 7, 13, 6, 9, 6, 16, 12, 11, 
10, 12, 9, 7, 7, 7, 6, 2, 3, 1, 1, 2, 2, 3, 11, 10, 9, 8, 9, 
13, 6, 6, 10, 9, 11, 10, 8, 7, 6, 4, 2, 3, 5, 3, 2, 4, 4, 4, 
8, 5, 12, 8, 7, 12, 9, 12, 12, 12, 13, 12, 9, 8, 9, 10, 4, 7, 
4, 2, 2, 4, 1, 7, 6, 6, 8, 11, 11, 5, 7, 6, 9, 12, 15, 9, 11, 
5, 10, 5, 4, 4, 2, 3, 3, 2, 5, 4, 7, 8, 6, 6, 5, 12, 10, 8, 10, 
10, 4, 13, 12, 6, 8, 6, 3, 1, 4, 2, NA, 4, 3, 2, 6, 5, 8, 10, 
4, 13, 2, 13, 8, 11, 13, 8, 9, 10, 9, 5, 1, NA, 1, 1, 2, NA, 
1, 7, 6, 10, 7, 8, 12, 12, 9, 5, 6, 8, 13, 13, 13, 8, 8, 1, 5, 
7, 6, 2, NA, 2, 1, 2, 7, 9, 12, 12, 10, 10, 10, 6, 8, 2, 8, 3, 
4, 5, 6, 2, 2, 1, 4, 1, NA, 3, 1, 3, 8, 8, 11, 11, 12, 5, 7, 
14, 9, 10, 14, 11, 8, 6, 8, 7, 5, 4, 3, 4, 9, NA, 2, 4, 5, 8, 
2, 12, 8, 15, 12, 8, 9, 12, 9, 9, 12, 7, 7, 8, 7, 5, 4, NA, 1, 
NA, NA, 4, 9, 8, 8, 8, 12, 13, 7, 11, 8, 14, 12, 13, 15, 8, 6, 
4, 4, 5, 2, NA, 2, 5, 4, 5, 6, 15, 11, 10, 16, 10, 5, 5, 10, 
13, 10, 9, 8, 7, 5, 4, 5, 6, NA, 2, 5, 4, 1, 6, 5, 8, 4, 3, 10, 
11, 8, 12, 10, 10, 10, 12, 10, 10, 7, 5, 7, 3, 4, 3, 3, 3, 3, 
8, 4, 8, 10, 5, 10, 10, 10, 11, 10, 11, 7, 10, 7, 6, 7, 7, 3, 
3, NA, 3, 6, 5, 3, 3, 5, 6, 6, 13, 14, 14, 7, 13, 9, 10, 4, 9, 
10, 8, 3, 6, 10, 5, 2, 1, NA, 3, 4, 4, 12, 12, 11, 12, 11, 13, 
10, 9, 11, 11, 14, 10, 13, 10, 7, 11, 1, 3, 1, 4, 1, 2, 2, 3, 
9, 6, 9, 9, 8, 9, 7, 12, 17, 13, 9, 10, 8, 8, 10, 2, 3, 3, 6, 
2, 2, 1, 6, 8, 7, 9, 5, 11, 8, 8, 12, 13, 14, 10, 7, 5, 11, 11, 
8, 5, 7, 3, 2, 3, 5, NA, 1, 3, 3, 4, 9, 12, 12, 3, 5, 12, 10, 
9, 14, 15, 12, 7, 8, 7, 3, 4, 1, 5, 6, 4, NA, 5, 9, 6, 7, 8, 
15, 13, 9, 12, 9, 7, 7, 6, 7, 8, 8, 6, 4, 5, 4, 1, 5, 1, NA, 
5, 4, 12, 7, 20, 12, 14, 10, 11, 11, 12, 6, 6, 11, 5, 6, 7, 4, 
7, 5, 1, 2, NA, 2, 7, 16, 9, 4, 12, 14, 12, 9, 8, 12, 7, 6, 11, 
9, 15, 9, 4, 4, 3, 3, 2, 5, 2, 1, 6, 8, 3, 12, 11, 14, 9, 6, 
3, 12, 11, 10, 14, 10, 10, 12, 2, 3, 3, 5, 3, 2, 3, 3, 5, 9, 
5, 10, 14, 9, 14, 11, 9, 12, 9, 15, 13, 12, 15, 11, 4, 7, 3, 
3, 3, 2, 5, 5, 11, 4, 2, NA, 3, 6, 10, 8, 5, 9, 9, 10, 11, 8, 
9, 8, NA, 3, NA, 1, 1, 4, 3, 3, NA, 4, 8, 3, 9, 6, 12, 9, 7, 
11, 6, 6, 12, 5, 4, 11, 7, 1, 2, 3, 2, 4, 8, 2, 6, 5, 9, 3, 7, 
8, 8, 8, 14, 10, 12, 5, 12, 9, 13, 7, 3, 5, 3, 4, 2, 4, 2, NA, 
5, 5, 9, 8, 7, 11, 9, 5, 6, 10, 13, 10, 9, 16, 11, 7, 5, 6, 2, 
5, 3, 5, 2, 2, 6, 5, 11, 7, 13, 6, 10, 7, 9, 7, 8, 9, 12, 7, 
7, 5, 3, 5, 3, 3, 5, 3, 1, 2, 10, 11, 8, 1, 9, 10, 14, 12, 7, 
11, 11, 10, 7, 5, 9, 8, 6, NA, 4, 2, NA, 3, 4, 4, 6, 10, 9, 6, 
8, 9, 10, 9, 14, 12, 8, 11, 16, 16, 13, 8, 7, 4, 4, 1, 3, 4, 
2, 2, 5, 9, 9, 3, 8, 12, 6, 11, 10, 6, 8, 15, 12, 12, 7, 6, 7, 
3, 2, 1, 3, 2, 2, 5, 7, 11, 8, 3, 4, 5, 5, 9, 6, 10, 9, 7, 17, 
12, 3, 8, 6, 4, 4, 4, 3, 4, 2, 1, 4, 7, 12, 5, 4, 8, 7, 15, 8, 
6, 6, 10, 5, 10, 7, 3, 4, 4, 1, 5, 3, 4, 5, 2, 1, 8, 6, 8, 9, 
7, 13, 11, 11, 6, 9, 9, 9, 9, 8, 1, 8, 1, 3, 2, 2, 1, 3, 2, 3, 
8, 9, 10, 12, 6, 9, 12, 7, 8, 8, 14, 10, 10, 8, 10, 4, 4, 4, 
4, 3, 2, 4, 2, 3, 16, 3, 9, 8, 15, 9, 11, 10, 12, 12, 7, 15, 
13, 8, 9, 3, 8, 1, 2, 2, 3, 3, 3, 10, 7, 5, 10, 4, 8, 8, 10, 
8, 9, 9, 6, 7, 7, 4, 6, 8, 4, 5, 5, 1, 1, 1, 5, 7, 3, 3, 12, 
8, 7, 10, 7, 12, 11, 7, 6, 14, 13, 9, 14, 3, 4, 6, 3, 2, 3, NA, 
2, 3, 7, 6, 9, 7, 5, 9, 8, 10, 7, 7, 6, 11, 11, 9, 7, 5, 2, 3, 
2, 2, 5, NA, 2, 5, 6, 7, 8, 14, 11, 6, 9, 10, 10, 9, 8, 16, 9, 
6, 6, 3, 2, 5, 1, 1, NA, 3, 3, 2, 5, 10, 9, 10, 13, 11, 8, 17, 
13, 5, 11, 10, 11, 6, 9, 6, 4, 5, 6, 5, 2, 4, 1, 2, 5, 9, 12, 
10, 10, 18, 9, 13, 16, 8, 13, 5, 16, 11, 8, 10, 3, 6, 3, 1, 2, 
2, 6, 2, 12, 9, 5, 8, 10, 11, 4, 10, 9, 9, 9, 19, 11, 8, 9, 4, 
4, 4, 7, 5, 2, 2, 1, 2, 6, 10, 11, 12, 9, 9, 12, 9, 8, 14, 8, 
5, 3, 5, 7, 6, 3, 4, 6, 3, 3, NA, 2, 2, 7, 12, 11, 14, 7, 10, 
10, 13, 12, 5, 8, 8, 9, 10, 4, 9, 4, 5, 3, 1, 4, 2, 1, 7, 9, 
10, 10, 11, 9, 8, 6, 3, 8, 10, 9, 9, 10, 8, 11, 4, 1, 1, 3, 1, 
1, 3, 6, 2, 3, 10, 4, 9, 6, 10, 11, 6, 7, 8, 6, 11, 8, 4, 8, 
5, 5, 1, 7, 1, 3, 3, 3, 6, 6, 8, 8, 9, 7, 6, 10, 9, 10, 6, 13, 
10, 20, 7, 9, 2, 6, 3, 2, 1, 1, 2, 1, 3, 13, 7, 6, 7, 11, 7, 
9, 7, 9, 10, 9, 10, 6, 11, 7, 5, 5, 5, 4, 6, 4, NA, 4, 4, 11, 
11, 9, 8, 9, 9, 12, 8, 11, 12, 3, 9, 12, 10, 8, 7, 3, 5, 2, 2, 
3, 2, 3, 4, 8, 13, 6, 8, 7, 4, 7, 13, 10, 9, 11, 10, 10, 5, 12, 
4, 3, 7, NA, 2, 2, 1, 6, 2, 8, 8, 9, 6, 11, 7, 7, 9, 11, 9, 6, 
8, 11, 10, 6, 5, 3, 5, 5, 1, 1, 2, 2, 5, 7, 13, 11, 7, 17, 10, 
4, 10, 9, 10, 6, 5, 8, 4, 6, 6, 3, 5, NA, 5, 1, 3, 1, 5, 3, 8, 
11, 6, 8, 9, 7, 11, 5, 12, 10, 10, 8, 9, 8, 7, 3, 2, 4, 5, 1, 
3, 1, 2, 9, 10, 4, 7, 10, 11, 6, 11, 8, 8, 7, 10, 9, 9, 11, 9, 
2, 3, 4, 3, NA, 2, 5, 5, 8, 13, 7, 14, 11, 4, 7, 10, 15, 10, 
15, 8, 6, 9, 5, 4, 2, 1, 3, NA, 1, 1, 2, 3, 13, 6, 8, 6, 12, 
10, 13, 5, 14, 11, 14, 14, 10, 10, 5, 3, 4, 2, 4, 2, 3, 2, 2, 
NA, 10, 9, 6, 14, 10, 7, 7, 12, 3, 13, 12, 12, 14, 8, 11, 3, 
6, NA, 2, NA, 3, 2, 5, 5, 11, 3, 5, 7, 7, 12, 8, 5, 11, 5, 2, 
8, 9, 6, 7, 3, 3, 1, 1, NA, 1, 3, 1, 3, 5, 10, 10, 7, 3, 5, 8, 
11, 9, 11, 7, 9, 9, 10, 5, 10, 4, 3, 3, 1, 2, NA, 3, 4, 6, 6, 
7, 11, 9, 11, 9, 6, 6, 8, 7, 9, 12, 12, 9, 5, 4, 3, NA, 1, 2, 
1, 2, 2, 2, 6, 9, 8, 8, 8, 12, 8, 13, 7, 12, 7, 8, 12, 8, 5, 
5, 1, NA, 4, 3, 1, NA, 5, 6, 10, 7, 15, 12, 12, 6, 11, 12, 12, 
10, 16, 10, 8, 11, 8, 5, 4, 2, 1, 2, NA, 5, 2, 8, 9, 7, 9, 14, 
7, 13, 3, 6, 9, 13, 10, 8, 6, 4, 6, 4, 5, 1, 3, 3, 2, 1, 4, 4, 
9, 11, 4, 8, 9, 11, 8, 8, 9, 19, 9, 7, 7, 11, 6, 8)</p>
"
"0.0964485644340824","0.0749064254158383","206447","<p>I have a good understanding of ARIMA models but I've always found significant spikes in ACFs and PACFs that gave me the appropriate AR and MA parameters. </p>

<p>Now I'm dealing with a series that is more like an ARIMA(0,1,0) which I think is the same as a random walk? This random walk concept is a little new to me but I sort of understand it. My question now is how do I even create a model for this series?</p>

<p>I've seen some methods in the <a href=""https://cran.r-project.org/web/packages/forecast/forecast.pdf"" rel=""nofollow"">{forecast}</a> package that might be the ones I'm looking for but I want to understand how they are different.</p>

<p>The functions that I am confused about are <code>naive()</code> and <code>rwf()</code>. It seems like both try to address the same random walk problem. But also, what happens if I just build an <code>Arima()</code> model with parameters 0,1,0? how is that different?</p>
"
"0.118124884643724","0.122321680483099","206701","<p>I have some very noisy data that seems like it might have a frequency to it.  I'm trying to build a model with the data, like the example code below.  So I've been experimenting with fourier series predictors.  So far I've just been guessing at the frequency, like frequency = 168 in the example.  I was wondering if anyone could suggest a better way to detect frequency in noisy data.  For example, it's entirely possible that my data actually has a frequency of 6 or 18.  </p>

<p>(Also I know I'm using frequency incorrectly here, but that's because in the ts function they use frequency like we would normally use period.  I hope that's not too confusing.)</p>

<p>Code:    </p>

<pre><code>##Partitioning Time Series
Train&lt;-336


TrainSeries &lt;-xData[1:Train]



##Fourier Terms

tsF&lt;-ts(xData, freq=168)


xregF&lt;-fourier(tsF,2)

xregFTrain&lt;-xregF[1:Train,]

##Model
Model1 &lt;- auto.arima(TrainSeries, xreg=xregFTrain, seasonal = FALSE )
</code></pre>

<p>Data:</p>

<pre><code>dput(xData)
</code></pre>

<p>c(11, 14, 17, 5, 5, 5.5, 8, NA, 5.5, 6.5, 8.5, 4, 5, 9, 10, 11, 
7, 6, 7, 7, 5, 6, 9, 9, 6.5, 9, 3.5, 2, 15, 2.5, 17, 5, 5.5, 
7, 6, 3.5, 6, 9.5, 5, 7, 4, 5, 4, 9.5, 3.5, 5, 4, 4, 9, 4.5, 
6, 10, NA, 9.5, 15, 9, 5.5, 7.5, 12, 17.5, 19, 7, 14, 17, 3.5, 
6, 15, 11, 10.5, 11, 13, 9.5, 9, 7, 4, 6, 15, 5, 18, 5, 6, 19, 
19, 6, 7, 7.5, 7.5, 7, 6.5, 9, 10, 5.5, 5, 7.5, 5, 4, 10, 7, 
5, 12, 6, NA, 4, 2, 5, 7.5, 11, 13, 7, 8, 7.5, 5.5, 7.5, 15, 
7, 4.5, 9, 3, 4, 6, 17.5, 11, 7, 6, 7, 4.5, 4, 4, 5, 10, 14, 
7, 7, 4, 7.5, 11, 6, 11, 7.5, 15, 23.5, 8, 12, 5, 9, 10, 4, 9, 
6, 8.5, 7.5, 6, 5, 8, 6, 5.5, 8, 11, 10.5, 4, 6, 7, 10, 11.5, 
11.5, 3, 4, 16, 3, 2, 2, 8, 4.5, 7, 4, 8, 11, 6.5, 7.5, 17, 6, 
6.5, 9, 12, 17, 10, 5, 5, 9, 3, 8.5, 11, 4.5, 7, 16, 11, 14, 
6.5, 15, 8.5, 7, 6.5, 11, 2, 2, 13.5, 4, 2, 16, 11.5, 3.5, 9, 
16.5, 2.5, 4.5, 8.5, 5, 6, 7.5, 9.5, NA, 9.5, 8, 2.5, 4, 12, 
13, 10, 4, 6, 16, 16, 13, 8, 12, 19, 19, 5.5, 8, 6.5, NA, NA, 
NA, 15, 12, NA, 6, 11, 8, 4, 2, 3, 4, 10, 7, 5, 4.5, 4, 5, 11.5, 
12, 10.5, 4.5, 3, 4, 7, 15.5, 9.5, NA, 9.5, 12, 13.5, 10, 10, 
13, 6, 8.5, 15, 16.5, 9.5, 14, 9, 9.5, 11, 15, 14, 5.5, 6, 14, 
16, 9.5, 23, NA, 19, 12, 5, 11, 16, 8, 11, 9, 13, 6, 7, 3, 5.5, 
7.5, 19, 6.5, 5.5, 4.5, 7, 8, 7, 10, 11, 13, NA, 12, 1.5, 7, 
7, 12, 8, 6, 9, 15, 9, 3, 5, 11, 11, 8, 6, 3, 7.5)    </p>
"
"0.0681994339470473","0.0706224551546449","207473","<p>I am new to R and statistics. I have a problem related to the prediction:
I am not able to plot the real value together with the predicted value. 
PROBLEM: I want to feed first 16 values into the ARIMA and then I want ARIMA should predict the next 3 values. I used both forecast and predict function but not sure which one is good for my case (please tell me).
After prediction, I only can plot the green line which is the prediction values but not able to add the real values in the same line.</p>

<p>Bonus: How to get the MAPE error measure when I use predict function in R.</p>

<p>Thanks..</p>

<p>SAMPLE CODE:</p>

<pre><code>x=file$Cost
    k&lt;-auto.arima(x[1:16]) 
    m=forecast(k,h=3) ## I tried both of them
    m=predict(k, n.ahead = 3)
    j=(m$fitted)
a=j[17:19]
b=x[17:19]
plot(a, col=""green"",type=""l"") # predicted
lines(b) # real
summary(m)
</code></pre>

<p>SAMPLE DATA:</p>

<pre><code>Timestamp   Cost
2010-09-21T00:00:00+00:00   5
2010-09-21T00:01:00+00:00   6
2010-09-21T00:02:00+00:00   6
2010-09-21T00:03:00+00:00   6
2010-09-21T00:04:00+00:00   6
2010-09-21T00:05:00+00:00   6
2010-09-21T00:06:00+00:00   6
2010-09-21T00:07:00+00:00   5
2010-09-21T00:08:00+00:00   6
2010-09-21T00:09:00+00:00   5
2010-09-21T01:10:00+00:00   5
2010-09-21T01:11:00+00:00   6
2010-09-21T01:12:00+00:00   6 
2010-09-21T01:13:00+00:00   6
2010-09-21T01:14:00+00:00   6
2010-09-21T01:15:00+00:00   6
2010-09-21T01:16:00+00:00   6
2010-09-21T01:17:00+00:00   5
2010-09-21T01:18:00+00:00   6
</code></pre>
"
"NaN","NaN","207987","<p>I have fit an ARIMA model to a time series with function <code>auto.arima</code> from ""forecast"" package in R. I wanted to check prediction intervals for robustness by changing the ARIMA terms. </p>

<p>Here is my R code:</p>

<pre><code>library(""forecast"", lib.loc=""~/R/win-library/3.2"")
library(""tseries"", lib.loc=""~/R/win-library/3.2"")

price = c(256, 223, 190, 170 ,140, 123, 133, 133, 125, 120, 125, 140, 166, 186, 206, 206, 206, 206, 206, 206,
       229, 263, 273, 273 ,273 ,273 ,258, 239, 233, 226, 226, 226, 249, 249, 249, 249, 249, 269, 279, 279,
       279, 279, 299, 316, 316, 316, 316, 316, 316, 316, 299, 299, 299 ,319, 319, 339 ,339, 356 ,356, 356,
       343, 343, 333 ,343 ,442 ,599, 599, 599, 599, 549, 516, 336, 336, 336, 309, 309 ,319, 565, 665, 832,
       832, 698, 632, 532, 499, 526, 526, 526, 526, 499, 466, 333 ,233, 233, 216, 200, 200, 200, 226, 239,
       279, 316, 333 ,366 ,366 ,366, 366 ,366 ,333 ,349 ,349, 349 ,359 ,359, 442 ,459 ,449 ,449, 449, 449,
       449, 449 ,449 ,459, 459 ,459, 459, 459, 446, 446, 446, 446, 459, 459, 439, 439, 439, 439, 482, 482,
       482, 482 ,516,516, 532, 532, 532 ,532 ,532 ,549, 599, 632 ,632 ,632, 632, 599 ,565 ,532, 482, 482,
       482, 482, 499 ,475 ,449, 416)

ts.plot(price)

auto.arima(price)

arima.fit&lt;-Arima(price, c(2,1,4), include.drift=TRUE)
plot(forecast.Arima(arima.fit, 60), ylim=c(-300,1300))

arima.fit&lt;-Arima(price, c(2,1,3), include.drift=TRUE)
plot(forecast.Arima(arima.fit, 60), ylim=c(-300,1300))
</code></pre>

<p>What I saw surprised me quite a bit:</p>

<p><a href=""http://i.stack.imgur.com/SHPAE.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/SHPAE.jpg"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/9pNVK.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9pNVK.jpg"" alt=""enter image description here""></a></p>

<p>Why do the prediction intervals widen in the MA(3) case and hardly so in the MA(4) case? </p>
"
"0.173875122255988","0.180052638465683","209426","<p>After reading <a href=""http://multithreaded.stitchfix.com/blog/2016/04/21/forget-arima/"" rel=""nofollow"">this blog post</a> about Bayesian structural time series models, I wanted to look at implementing this in the context of a problem I'd previously used ARIMA for.</p>

<p>I have some data with some known (but noisy) seasonal components - there are definitely an annual, monthly and weekly components to this, and also some effects due to special days (such as federal or religious holidays).</p>

<p>I have used the <code>bsts</code> package to implement this and as far as I can tell I haven't done anything wrong, although the components and prediction simply don't look as I'd expect. It isn't clear to me if my implementation is wrong, incomplete or has some other problem.</p>

<p>The full time series looks like this: </p>

<p><a href=""http://i.stack.imgur.com/Oy1ci.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Oy1ci.png"" alt=""Full data""></a></p>

<p>I can train the model on some subset of the data, and the model generally looks good in terms of the fit (plot is below). The code I am using to do this is here:</p>

<pre><code>library(bsts)

predict_length = 90
training_cut_date &lt;- '2015-05-01'
test_cut_date &lt;- as.Date(training_cut_date) + predict_length

df = read.csv('input.tsv', sep ='\t')

df$date &lt;- as.Date(as.character(df$date),format=""%Y-%m-%d"")
df_train = df[df$date &lt; training_cut_date,]

yts &lt;- xts(log10(df_train$count), order.by=df_train$date)

ss &lt;- AddLocalLinearTrend(list(), yts)
ss &lt;- AddSeasonal(ss, yts, nseasons = 7)
ss &lt;- AddSeasonal(ss, yts, nseasons = 12)
ss &lt;- AddNamedHolidays(ss, named.holidays = NamedHolidays(), yts)

model &lt;- bsts(yts, state.specification = ss, niter = 500, seed=2016)
</code></pre>

<p>The model looks reasonable:</p>

<p><a href=""http://i.stack.imgur.com/kinlf.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/kinlf.png"" alt=""Model Plot""></a></p>

<p>But if I plot the prediction then firstly the trend is completely wrong, and secondly the uncertainty grows VERY quickly - to the point where I can't show the uncertainty band on the same plot as the predictions without making the y axis on a log-scale.  The code for this part is here:</p>

<pre><code>burn &lt;- SuggestBurn(0.1, model)
pred &lt;- predict(model, horizon = predict_length, burn = burn, quantiles = c(.025, .975))
</code></pre>

<p>The pure prediction looks like this:</p>

<p><a href=""http://i.stack.imgur.com/PQl7t.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/PQl7t.png"" alt=""pure prediction""></a></p>

<p>And then when scaled back to the initial distribution (with the dotted line showing the transition from training to prediction, the problems are obvious:</p>

<p><a href=""http://i.stack.imgur.com/YolqM.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YolqM.png"" alt=""full distro""></a></p>

<p>I have tried adding more seasonal trends, removing seasonal trends, adding an AR term, changing the AddLocalLinearModel to AddGeneralizedLocalLinearTrend and several other things concerning tweaking the model, but nothing has resolved the issues and made the predictions more meaningful. In some cases the direction changes, so rather than dropping to 0 the prediction just continues to increase as a function of time. I definitely don't understand why the model is breaking down in this way. Any suggestions would be very welcome.  </p>
"
"NaN","NaN","209730","<p>Could someone please explain the differences between the 3 fitting methods, method = c(""CSS-ML"", ""ML"", ""CSS""), in Arima?  If I run the code below I get an error message, but if I specify method=""ML"" in Arima it runs fine.  So I was curious what the difference was between the 3 fitting methods.</p>

<p>Code with error:</p>

<pre><code>library(""fpp"")

tsTrain &lt;- window(hsales,end=1989.99)

pvar&lt;-1:10
dvar&lt;-1:2
qvar&lt;-1:7

OrderGrid&lt;-expand.grid(pvar,dvar,qvar)

n &lt;- function(a,b,c) {Arima(tsTrain, order=c(a,b,c))}
ModFit &lt;- do.call(Vectorize(n, SIMPLIFY=FALSE), unname(OrderGrid))
</code></pre>

<p>Fixed Code:</p>

<pre><code>library(""fpp"")

tsTrain &lt;- window(hsales,end=1989.99)

pvar&lt;-1:10
dvar&lt;-1:2
qvar&lt;-1:7

OrderGrid&lt;-expand.grid(pvar,dvar,qvar)

n &lt;- function(a,b,c) {Arima(tsTrain, order=c(a,b,c),method=""ML"")}
ModFit &lt;- do.call(Vectorize(n, SIMPLIFY=FALSE), unname(OrderGrid))
</code></pre>
"
"0.0482242822170412","0.0499376169438922","210117","<p>I have the code below which trains ARIMA models for a range of order combinations. I'm getting the error below in the step training the ARIMA models.  The code worked just fine with the <code>hsales</code> time-series provided for Hyndman's text book in the ""fpp"" package in R. If anyone can point out the issue or suggest how to solve it, I would be grateful.</p>

<p>Code:</p>

<pre><code>library(""forecast"")
library(""tseries"")
library(""sqldf"")
library(""manipulate"")
library(""dplyr"")
library(""xts"")

tsTrain &lt;- tsTrain
tsTest &lt;- tsValidation

pvar&lt;-1:17
dvar&lt;-1:2
qvar&lt;-1:17

##Creating All Combingations

OrderGrid&lt;-expand.grid(pvar,dvar,qvar)

##Vectorize Suggestion

n &lt;- function(a,b,c) {Arima(tsTrain, order=c(a,b,c),method=""ML"")}
mod_fit &lt;- do.call(Vectorize(n, SIMPLIFY=FALSE), unname(OrderGrid))
</code></pre>

<p>Error:</p>

<pre><code>Error in optim(init[mask], armafn, method = optim.method, hessian = TRUE,  : 
  non-finite finite-difference value [3] 
</code></pre>

<p>Data:</p>

<pre><code>c(11, 14, 17, 5, 5, 5.5, 8, NA, 5.5, 6.5, 8.5, 4, 5, 9, 10, 11, 
7, 6, 7, 7, 5, 6, 9, 9, 6.5, 9, 3.5, 2, 15, 2.5, 17, 5, 5.5, 
7, 6, 3.5, 6, 9.5, 5, 7, 4, 5, 4, 9.5, 3.5, 5, 4, 4, 9, 4.5, 
6, 10, NA, 9.5, 15, 9, 5.5, 7.5, 12, 17.5, 19, 7, 14, 17, 3.5, 
6, 15, 11, 10.5, 11, 13, 9.5, 9, 7, 4, 6, 15, 5, 18, 5, 6, 19, 
19, 6, 7, 7.5, 7.5, 7, 6.5, 9, 10, 5.5, 5, 7.5, 5, 4, 10, 7, 
5, 12, 6, NA, 4, 2, 5, 7.5, 11, 13, 7, 8, 7.5, 5.5, 7.5, 15, 
7, 4.5, 9, 3, 4, 6, 17.5, 11, 7, 6, 7, 4.5, 4, 4, 5, 10, 14, 
7, 7, 4, 7.5, 11, 6, 11, 7.5, 15, 23.5, 8, 12, 5, 9, 10, 4, 9, 
6, 8.5, 7.5, 6, 5, 8, 6, 5.5, 8, 11, 10.5, 4, 6, 7, 10, 11.5, 
11.5, 3, 4, 16, 3, 2, 2, 8, 4.5, 7, 4, 8, 11, 6.5, 7.5, 17, 6, 
6.5, 9, 12, 17, 10, 5, 5, 9, 3, 8.5, 11, 4.5, 7, 16, 11, 14, 
6.5, 15, 8.5, 7, 6.5, 11, 2, 2, 13.5, 4, 2, 16, 11.5, 3.5, 9, 
16.5, 2.5, 4.5, 8.5, 5, 6, 7.5, 9.5, NA, 9.5, 8, 2.5, 4, 12, 
13, 10, 4, 6, 16, 16, 13, 8, 12, 19, 19, 5.5, 8, 6.5, NA, NA, 
NA, 15, 12, NA, 6, 11, 8, 4, 2, 3, 4, 10, 7, 5, 4.5, 4, 5, 11.5, 
12, 10.5, 4.5, 3, 4, 7, 15.5, 9.5, NA, 9.5, 12, 13.5, 10, 10, 
13, 6, 8.5, 15, 16.5, 9.5, 14, 9, 9.5, 11, 15, 14, 5.5, 6, 14, 
16, 9.5, 23, NA, 19, 12, 5, 11, 16, 8, 11, 9, 13, 6, 7, 3, 5.5, 
7.5, 19, 6.5, 5.5, 4.5, 7, 8, 7, 10, 11, 13, NA, 12, 1.5, 7, 
7, 12, 8, 6, 9, 15, 9, 3, 5, 11, 11, 8, 6, 3, 7.5, 4, 7, 7.5, 
NA, NA, NA, NA, 6.5, 2, 16.5, 7.5, 8, 8, 5, 2, 7, 4, 6.5, 4.5, 
10, 6, 4.5, 6.5, 9, 2, 6, 3.5, NA, 5, 7, 3.5, 4, 4.5, 13, 19, 
8.5, 10, 8, 13, 10, 10, 6, 13.5, 12, 11, 5.5, 6, 3.5, 9, 8, NA, 
6, 5, 8.5, 3, 12, 10, 9.5, 7, 24, 7, 9, 11.5, 5, 7, 11, 6, 5.5, 
3, 4.5, 4, 5, 5, 3, 4.5, 6, 10, 5, 4, 4, 9.5, 5, 7, 6, 3, 13, 
5.5, 5, 7.5, 3, 5, 6.5, 5, 5.5, 6, 4, 3, 5, NA, 5, 5, 6, 7, 8, 
5, 5.5, 9, 6, 8.5, 9.5, 8, 9, 6, 12, 5, 7, 5, 3.5, 4, 7.5, 7, 
5, 4, 4, NA, 7, 5.5, 6, 8.5, 6.5, 9, 3, 2, 8, 15, 6, 4, 10, 7, 
13, 14, 9.5, 9, 18, 6, 5, 4, 6, 4, 11.5, 17.5, 7, 8, 10, 4, 7, 
5, 9, 6, 5, 4, 8, 4, 2, 1.5, 3.5, 6, 5.5, 5, 4, 8, 10.5, 4, 11, 
9.5, 5, 6, 11, 21, 9.5, 11, 13.5, 7.5, 13, 10, 7, 9.5, 6, 10, 
5.5, 6.5, 12, 10, 10, 6.5, 2, 8, NA, 10, 5, 4, 4.5, 5, 7.5, 12, 
22, 5, 8.5)
</code></pre>
"
"0.0340997169735237","0.0353112275773224","210228","<p>I'm working with a time series of 59 elements. I'm wondering why the R function, <code>Arima</code>, throws an <code>NaN</code> for the standard errors of some parameters. I'm trying to model the residuals of a log-quadratic stational model:</p>

<pre><code>arimaModel = Arima(log(myTS),order=c(1,0,1),seasonal=list(order=c(0,0,1)),xreg=myModelParameters, method=""ML"")
</code></pre>

<p>\begin{array}{|c|c|}
\hline Parameter &amp; Estimate &amp; Std. Error &amp; T\ Value &amp; P(&gt;|t|) \\\hline
ar1    &amp;    -5.8991e-01   &amp;       NA  &amp;    NA  &amp;      NA \\\hline   
ma1     &amp;   1.0000e+00 &amp; 4.4617e-02 &amp; 22.413 &amp; &lt; 2.2e-16 *** \\\hline
sma1    &amp;   3.0305e-01   &amp;       NA  &amp;    NA   &amp;     NA \\\hline   
intercept &amp; 8.2949e+00 &amp; 3.4403e-02 &amp; 241.108 &amp; &lt; 2.2e-16 *** \\\hline
t        &amp;  2.1970e-02 &amp; 1.4813e-03 &amp; 14.831 &amp; &lt; 2.2e-16 *** \\\hline
t^2       &amp; -4.4673e-05   &amp;       NA  &amp;    NA  &amp;      NA   \\\hline 
Q1      &amp;  -5.9911e-01 &amp; 2.1024e-02 &amp; -28.497 &amp; &lt; 2.2e-16 *** \\\hline
Q2      &amp;  -4.2457e-01 &amp; 2.9678e-02 &amp; -14.306 &amp; &lt; 2.2e-16 *** \\\hline
Q3      &amp;  -3.1433e-01 &amp; 2.1135e-02 &amp; -14.872 &amp; &lt; 2.2e-16 *** \\\hline
\end{array}</p>

<p>What could I do with this? Why does this happen? I had switched the method between of the function with CSS, CSS-ML, but it's always the same...</p>
"
"0.0964485644340824","0.0749064254158383","211213","<p>I'm fitting a
$$ARIMA(p,d,q)\times (P,D,Q)_{12}$$
model. The first loop fits p and q. The second loop fits P and Q. Here, d and D are both assumed to be 0 since I'm looking at the interaction between two cycles.</p>

<pre><code>&lt;&lt;arima,echo=FALSE, fig.show='hold', tidy=TRUE&gt;&gt;=
aic_table &lt;- function(data,P,Q,xreg=NULL){
  table &lt;- matrix(NA,(P+1),(Q+1))
  for(p in 0:P) {
    for(q in 0:Q) {
      table[p+1,q+1] &lt;- arima(data,order=c(p,0,q),xreg=xreg, method=""ML"")$aic
    }
  }
  dimnames(table) &lt;- list(paste(""AR"",0:P, sep=""""),paste(""MA"",0:Q,sep=""""))
  table
}
u_aic_table &lt;- aic_table(cars_hp$cycle,5,4,xreg=steel_hp$cycle)
@

#a and b are found from the previous step
&lt;&lt;sarima,echo=FALSE, fig.show='hold', tidy=TRUE&gt;&gt;=
aic_table2 &lt;- function(data,P,Q,xreg=NULL){
  table &lt;- matrix(NA,(P+1),(Q+1))
  for(p in 0:P) {
    for(q in 0:Q) {
      table[p+1,q+1] &lt;- arima(data,order=c(5,0,3),xreg=xreg, seasonal = list(order = c(p,0,q), Period=12))$aic
    }
  }
  dimnames(table) &lt;- list(paste(""AR"",0:P, sep=""""),paste(""MA"",0:Q,sep=""""))
  table
}
u_aic_table2 &lt;- aic_table2(cars_hp$cycle,5,4,xreg=steel_hp$cycle)
</code></pre>

<p>The thing is when I try this, I can get ACF values under the ARIMA just fine. It's when I run the second loop that I get issues. Is there a workaround for this? Is it because I'm using the cyclic decompositions of each time series? I want to understand how my cars time series relates to the steel time series.</p>

<pre><code>     Error in optim(init[mask], armafn, method = optim.method, hessian = TRUE,  : 
  non-finite finite-difference value [1] 
</code></pre>
"
"NaN","NaN","211670","<p>I want to make predictions use ARIMA in forecast package. I find that basically the prediction is just a lag of the actuals. Is there any way that I can better fit the model or any other approach available? (I find the ARIMA parameters through function <code>auto.arima</code> in ""forecast"" package in R.)</p>

<pre><code>Model and Plot:
        fit &lt;- arima(dataf$actuals, xreg=dataf$regressor,order=c(0,1,0))
        labDates &lt;- seq(as.Date(""2016-01-01"", format = ""%Y-%m-%d""),as.Date(""2016-01-16"", format = ""%Y-%m-%d""),by = ""day"")
        plot(labDates, dataf$actuals,col=""red"",type='l')
        lines(labDates,fitted(fit),col=""blue"")
        legend('topleft',c(""Actual Number"",""Predicted Number""),pch=c(20,20,20),col=c(""red"",""blue""))

Data:
    actuals&lt;-c(26952, 38178, 36377, 45718, 40393, 43111, 39947, 40853, 38792, 41816, 46091, 41866, 35701, 52738,73834, 82813) 
    actuals_next&lt;-c(38178, 36377, 45718, 40393, 43111, 39947, 40853, 38792, 41816, 46091, 41866, 35701, 52738, 73834,82813,NA)  
    regressor&lt;-c(519020, 671049 ,501083 ,288259 ,290899 ,260817, 276166, 274859 ,279405, 286689, 234050,95562,15138,  16401,  27145,  53717)    
    dataf&lt;-as.data.frame(cbind(actuals, actuals_next, regressor))
</code></pre>
"
"0.181865157192126","0.200096956271494","213192","<p>I have half-hourly electricity data of several homes for a duration of one month. Also, I have ambient temperature at same sampling rate. Now, I need to make half-hourly forecasts using historical electricity data and forecasted temperature for coming day.</p>

<p>Currently, I am facing a problem with seasonality, i.e., in some homes I observe (via visual inspection) day-wise seasonality and in some other homes no pattern is found. <a href=""http://stats.stackexchange.com/a/212797/60072"">Stephan</a> suggested to check for weekly seasonality as well, but visually I do not find any. So, I thought to try models with different forced seasonalities to find the prediction accuracy. I can think of two options:</p>

<ol>
<li>Model_1 with daily seasonality (frequency = 48 observations)</li>
<li>Model_2 with weekly and daily seasonality (48, 7*48)</li>
</ol>

<p>Keeping the above approach in mind, I am facing the following issue:</p>

<ol>
<li>For Model_1, I can use <code>auto.arima()</code> with <code>xreg</code> option to specify an extra <code>temperature</code> predictor. However, this only works for a single seasonality.</li>
<li>For Model_2, I tried to use the <code>tbats()</code> forecasting function, which models multiple seasonalities, but does not allow extra predictor variables. </li>
</ol>

<p>Is there a forecasting function which allows <em>both</em> multiple predictors and multiple seasonalities?</p>

<p>Here is the electricity data of one month. </p>

<pre><code>    data &lt;- structure(c(1642.8, 1467.1, 165.57, 1630.99, 1618.65, 1629.29, 
        1598.93, 1839.9, 1604.52, 1606.73, 1473.82, 1669.17, 1698.9, 
        2111.21, 2056.41, 3671.29, 2808.01, 1336.15, 794.11, 1212.15, 
        377.36, 888.54, 174.58, 218.54, 420.76, 389.58, 397.77, 395.31, 
        359.11, 364.8, 376.13, 389.37, 929.5, 1702.38, 519.65, 2452.28, 
        1354.45, 1842.96, 725.41, 661.11, 528.44, 733.4, 429.51, 310.47, 
        279.72, 407.83, 1791.1, 1754.53, 1536.73, 1608.37, 1432.23, 1401.72, 
        1582.14, 1558.75, 1536.24, 1745.59, 1375.61, 1556.71, 1671.12, 
        1206.77, 1391.84, 876.23, 1617.3, 1638.99, 1833.61, 1591.42, 
        1455, 183.87, 177.55, 184.36, 332.99, 352.95, 425.1, 945.67, 
        342.3, 348.45, 227.18, 382.15, 268.91, 335.88, 326.94, 233.23, 
        169.71, 179.51, 195.3, 207.23, 1681.9, 1493.32, 941.52, 980.36, 
        924.31, 379.02, 1229.89, 1590.21, 1250.92, 1149.24, 1124.04, 
        993.78, 883.98, 860.69, 934.17, 969.31, 1049.55, 1104.94, 904.3, 
        1220.23, 1183.9, 891.26, 825.33, 787.77, 1060.93, 1029.1, 982.25, 
        193.13, 182.65, 181.75, 167.68, 165.89, 291.02, 300.2, 418.4, 
        297.66, 231.89, 305.66, 701.18, 338.5, 337.24, 332.11, 332.66, 
        187.8, 179.03, 130.22, 177.73, 172.24, 173.45, 334.23, 810.53, 
        359.41, 330.23, 333.29, 568.85, 2462.46, 1660.32, 1156.13, 1136.2, 
        1189.07, 832.73, 181.91, 185.93, 1076.77, 672.78, 1376.71, 1020.17, 
        382.18, 1160.74, 791.36, 1569.4, 817.78, 850.71, 747.21, 826.12, 
        1306.46, 506.23, 140.05, 132.6, 304.19, 308.14, 406.13, 290.73, 
        188.9, 165.59, 174.56, 145.97, 151.83, 142.29, 443.58, 799.9, 
        279.36, 223.88, 221.03, 291.26, 374.97, 431.36, 598.98, 625.82, 
        1052.02, 2036.83, 1230.03, 1429.81, 1099.34, 1646.03, 1668.56, 
        1631.79, 1604.04, 2849.49, 2998.63, 2476.96, 1601.04, 1216.42, 
        2004.2, 1868.51, 1961.91, 1813.35, 1500.22, 1276.94, 1369.29, 
        632.43, 238.15, 488.76, 467.6, 330.27, 144.67, 153.36, 924.12, 
        1348.18, 799.01, 524.3, 420.5, 264.34, 283.86, 198.95, 206.52, 
        217.33, 356.16, 207.83, 197.6, 194.03, 193.01, 249.85, 271.22, 
        244.25, 442.5, 660.49, 245.66, 356.13, 443.32, 336.15, 849.74, 
        1709.21, 1542.09, 1315.69, 2628.6, 2261.8, 1576.42, 1776.48, 
        1239.64, 1401.12, 1106.17, 1378.55, 1315.57, 1141, 1642.63, 2484.13, 
        1968.94, 3059.42, 1317.32, 905.05, 484.42, 486.86, 96.79, 183.45, 
        173.94, 342, 255.96, 320.54, 106.19, 147.88, 150.77, 176.17, 
        344.75, 371.73, 309.46, 237.86, 187.32, 202.61, 292.5, 248.28, 
        259.3, 283.67, 365.09, 230.47, 326.15, 350.92, 335.42, 419.39, 
        345.31, 1093.22, 1392.87, 1298.11, 919.16, 1654.53, 1045.99, 
        558.42, 437.29, 857.9, 758.34, 1220.04, 1390.62, 956.74, 909.93, 
        584.67, 409.87, 387.3, 387.93, 1276.28, 871.06, 413.8, 313.2, 
        199.5, 330.21, 210.9, 358.28, 352.13, 233.62, 259.18, 123.57, 
        255.58, 411.78, 427.65, 318.95, 298.5, 283.23, 279.85, 200.45, 
        205.97, 254.24, 307.98, 1090.53, 289.71, 215.14, 286.63, 328.55, 
        288.96, 1281.19, 1354.8, 1302.05, 1254.46, 261.95, 270.09, 243.28, 
        696.61, 314.27, 241.73, 245.68, 157.74, 222.55, 294.36, 185.46, 
        203.49, 182.14, 246.69, 178.26, 397.5, 330.2, 212.02, 248.72, 
        265.48, 249.37, 130.59, 248.97, 279.94, 319.07, 358.5, 278.98, 
        251.92, 304.66, 455.05, 365.95, 340.93, 287.51, 264.82, 260.18, 
        34.35, 35.11, 184.09, 247.18, 160.9, 139.27, 284.96, 296.31, 
        252.3, 342.65, 353.03, 380.52, 346.19, 350.06, 218.52, 133.94, 
        173.7, 128.26, 167.8, 112.77, 147.8, 129, 170.54, 89.88, 243.08, 
        97.61, 190.31, 193.94, 268.17, 233.5, 205.27, 92.29, 167.43, 
        168.34, 151.99, 193.84, 379.1, 318.69, 327.28, 487.39, 414.01, 
        336.06, 278.02, 168.05, 155.6, 236.4, 264.94, 296.05, 326.46, 
        357.43, 356.31, 340.29, 319.81, 312.79, 341.53, 317.36, 309.62, 
        440.6, 285.5, 282.06, 288.99, 334.48, 196.54, 144.24, 218.55, 
        173.64, 242.29, 251.78, 186.81, 184.36, 141.62, 208.91, 157.53, 
        154.03, 139.44, 137.66, 256.75, 1202.05, 177.36, 177.93, 72.83, 
        252.9, 231.35, 1090.39, 442.91, 363.12, 248.96, 478.75, 249.64, 
        297.29, 227.28, 365.82, 879.7, 488.93, 184.79, 138.13, 151.77, 
        123.18, 175.76, 251.84, 208.06, 126.68, 246.3, 307.34, 319.79, 
        324.3, 379.6, 309.53, 253.17, 221.91, 228.42, 150.24, 148.59, 
        118.79, 86.89, 140.51, 200.43, 212.15, 276.14, 441.81, 125.77, 
        152.42, 329.28, 269.21, 177.35, 1106.29, 128.92, 96.35, 63.53, 
        520.62, 940.25, 1014.34, 314.99, 390.2, 330.1, 377.04, 341.35, 
        342.79, 241.79, 249.9, 391.92, 292.68, 105.02, 179.99, 118.53, 
        154.17, 90.53, 206.7, 345.33, 244.75, 291.68, 820.57, 1777.84, 
        1805.83, 1753.73, 1416.7, 279.2, 262.82, 1345.88, 467.98, 1136.66, 
        170.02, 159.96, 1478.8, 1414.12, 1347.93, 1505.59, 1341.69, 445.53, 
        277.59, 1609.61, 1476.45, 244.08, 192.57, 213.55, 439.02, 112.86, 
        128.54, 376.09, 251.15, 116.27, 254.82, 302.56, 304.6, 198.5, 
        240.05, 219.35, 70.3, 190.96, 211.95, 328.53, 714.62, 3176.56, 
        2604.09, 191.65, 145.25, 93.93, 83.6, 81.13, 146.12, 331.75, 
        250.24, 1144.53, 1616.47, 1008.7, 316.65, 311.46, 1152.99, 1504.86, 
        1543.21, 1081.54, 1428.07, 1358.23, 1349.75, 190.23, 2398.92, 
        2196.11, 1466.94, 2249.77, 2150.2, 2542.25, 618.03, 453.22, 880.99, 
        1497.86, 440.96, 161.85, 324.88, 434.11, 316.33, 444.66, 359.14, 
        277.41, 1237.28, 761.41, 183.53, 309.44, 213.48, 121.64, 346.7, 
        149.86, 2060.39, 1102.13, 347.97, 600.24, 912.6, 590.77, 1805.76, 
        1673.93, 1573.91, 505.74, 446.76, 1033.41, 1668.68, 1293.9, 383.81, 
        1419.99, 1349.4, 711.55, 218.63, 182, 401.93, 1876, 1486.34, 
        1543.11, 2313.8, 478.57, 615.19, 542.68, 971.98, 531.11, 766.21, 
        489.76, 344.47, 319.86, 321.26, 311.41, 288.67, 310.67, 305.15, 
        419.33, 422.84, 950.08, 2188.88, 3454.92, 1989.54, 590.33, 327.05, 
        354.78, 578.41, 1583.29, 2016.66, 1481.03, 293.21, 1864.84, 399.65, 
        366.76, 357.7, 2074.97, 1626.86, 1133, 1624.61, 1506.93, 628.4, 
        1405.68, 217.8, 1223.11, 1356.97, 1171.72, 1182.86, 1642.11, 
        2289.02, 814.39, 595.76, 542.78, 1596.41, 884.97, 235.25, 1540.68, 
        781.95, 115.71, 1204.98, 718.66, 452.09, 305.58, 444.67, 356.76, 
        182.54, 674.47, 153.8, 862.25, 1322.88, 323.33, 1659.64, 496.72, 
        304.74, 246.6, 327.12, 239.31, 246.72, 225.72, 234.7, 324.07, 
        304.27, 171.86, 97.64, 242.69, 295, 324.53, 513.81, 1100.65, 
        1151.77, 231.56, 189.88, 786.3, 1164.87, 676.09, 882.82, 1496.3, 
        1027.91, 872.92, 809.1, 840.31, 1302.18, 2055.87, 677.74, 934.66, 
        263.91, 186.68, 248.5, 214.62, 371.54, 298, 294.52, 304.86, 1295.77, 
        942.5, 305, 265.78, 255.89, 255.63, 151.54, 108.16, 116.81, 100.19, 
        224.75, 84.11, 1143.89, 262.15, 784.21, 1728.29, 1506.79, 434.94, 
        374.29, 265.43, 560.74, 1651.49, 1063.07, 1054.69, 1298.4, 1261.59, 
        1132.75, 692.82, 660.57, 198.25, 97.81, 1258.67, 833.64, 796.35, 
        868.76, 999.86, 2240.02, 885.72, 1317.52, 1267.18, 167.93, 133.22, 
        364.44, 267.17, 406.13, 412.52, 1036.04, 779.34, 655.43, 1901.2, 
        270.18, 266.31, 284.21, 288.66, 135.38, 176.11, 154.86, 160.21, 
        146.28, 163.72, 139.75, 278.12, 253.51, 319.62, 396.39, 1662.69, 
        1577.09, 1059.71, 241.64, 407.54, 290.49, 846.17, 1325.31, 1418.23, 
        1432.5, 1412.6, 1015.87, 1619.88, 1426.58, 1333.32, 1963.35, 
        1638.11, 1081.89, 285.56, 1084.58, 2038.77, 1022.39, 1145.92, 
        513.87, 107.7, 176.19, 143.77, 374.3, 373.99, 221.76, 148.3, 
        331.01, 2323.12, 1502.09, 347.17, 296.7, 306.06, 313.03, 221.69, 
        295.35, 301.95, 250.92, 231.54, 140.6, 717.63, 863.5, 402.15, 
        1337.78, 1575.44, 1738.49, 1675.57, 1617.66, 1365.58, 242.8, 
        286.29, 712.34, 1559.59, 1600.34, 3447.88, 3432.89, 3337.23, 
        1472.21, 1323.76, 1265.31, 1221.65, 1312.63, 2016.09, 2972.13, 
        1451.67, 735.67, 130.13, 379.87, 162.21, 226.48, 417.18, 357.51, 
        346.37, 200.89, 190.15, 276.05, 942.74, 1471.99, 1047.53, 1240.06, 
        742.62, 169.34, 144.28, 220.58, 165.23, 344.8, 227.47, 254.64, 
        742.37, 1809.01, 436.11, 1692.87, 1697.74, 1474.91, 1635.68, 
        1664.2, 1489.85, 1316.4, 1364.07, 1510.02, 1497.89, 1709.17, 
        2846.71, 2736.08, 1015.43, 1017.08, 1195.21, 751.18, 523.89, 
        199.69, 2148.71, 1151.39, 1182.84, 788.91, 259.31, 146.29, 141.09, 
        299.38, 349.53, 404.08, 449.9, 391.77, 251.89, 222.25, 281.04, 
        565.4, 371.24, 219.75, 324.45, 227, 157.67, 212.48, 201.69, 140.84, 
        220.67, 187.64, 399.79, 157.92, 275.49, 326.99, 1340.51, 1578.68, 
        1599.41, 1667.74, 1129.07, 1312.55, 1393.09, 1368.69, 1130.85, 
        968.71, 1130.2, 1223.1, 1124.5, 1077.09, 1052.42, 1255.31, 918.21, 
        1263.93, 706.21, 3080.29, 1620.18, 1122.48, 750.06, 262.89, 110.02, 
        236.83, 413, 227.53, 355, 277.51, 258.03, 368.44, 656.68, 1808.17, 
        1493.41, 1272.21, 330.67, 1674.18, 719.45, 1089.68, 784.01, 275.73, 
        312.69, 345.11, 761.8, 1020.11, 259.16, 345.98, 270.23, 580.15, 
        1303.68, 1659.7, 1732.94, 204.9, 373.58, 373.36, 1381.52, 1437.74, 
        1262.78, 1264.6, 1184.54, 1175.12, 857.09, 1428.34, 841.92, 232.47, 
        223.22, 473.24, 382.7, 189.84, 1737.48, 1689.34, 378.48, 872.56, 
        180.12, 363.03, 301.38, 412.57, 401.17, 387.35, 417.63, 300.61, 
        376.82, 284.31, 232.31, 269.96, 188.41, 203.79, 134.88, 193.66, 
        57.86, 89.71, 167.66, 60.84, 197.03, 703.66, 1638.7, 1467.03, 
        347.22, 1397.23, 1511.92, 1362.25, 1397.18, 1106.19, 826.5, 1033.04, 
        1039.46, 584.98, 706.35, 548.07, 373.39, 681.6, 1231.28, 288.94, 
        649.36, 79.28, 209.23, 290.75, 304.12, 132.57, 91.2, 355.37, 
        197.45, 343.17, 339.2, 284.65, 229, 234.73, 322.36, 323.43, 295.1, 
        197.03, 308.17, 223.88, 235.08, 225.16, 172.83, 236.26, 135.17, 
        394.89, 479.2, 315.34, 280.9, 282.36, 204.78, 367.24, 1712.29, 
        1521, 1686.09, 960.19, 1019.12, 1062.77, 851.88, 1369.98, 689.2, 
        580.5, 751.74, 547.67, 556.64, 493.85, 404.15, 428.07, 716.2, 
        1442.05, 1045.81, 1497.07, 567.59, 155.07, 537.72, 446.03, 282.57, 
        642.88, 409.37, 338.91, 173.89, 358.63, 195.42, 209.95, 186.44, 
        152.34, 105.51, 132.26, 82.14, 122.88, 149.38, 211.42, 350.17, 
        429.72, 336.4, 982.21, 1436.25, 1726.87, 1830.42, 1282.05, 1293.4, 
        1121.01, 946.2, 707.1, 154.53, 767.56, 607.78, 448, 288.23, 270.32, 
        223.93, 166.22, 262.45, 223.74, 159.31, 210.44, 257.94, 183.99, 
        151.38, 206.11, 193.43, 388.95, 577.98, 304.64, 285.13, 256.59, 
        420.26, 289.34, 356.02, 358.08, 325.22, 275.95, 164.46, 213.23, 
        142.99, 221.66, 270.61, 206.56, 213.68, 254.33, 250.15, 267.99, 
        403.95, 671.2, 1574.11, 396.34, 477.88, 631.08, 618.25, 1366.88, 
        298.19, 287.05, 290.38, 332.44, 235.9, 229.79, 831.6, 1320.86, 
        477.31, 944.84, 547.33, 411.21, 705.43, 873.49, 572.81, 585.36, 
        1229.69, 701.01, 653.49, 74.81, 162.47, 179.54, 330.27, 544.51, 
        332.41, 296.66, 130.66, 1055.61, 556.79, 265.43, 383.44, 398.22, 
        362.66, 223.99, 130.35, 193.67, 217.68, 273.3, 247.84, 161.66, 
        320.08, 322.52, 274.61, 811.44, 353.85, 323.41, 383.61, 389.5
        ), .Dim = c(1248L, 1L), .Dimnames = list(NULL, ""power""))
</code></pre>

<p>Temperature predictor values are:</p>

<pre><code>temp&lt;-structure(c(31, 31, 31, 31, 30, 29, 28, 27.5, 27, 26, 26, 26, 
26, 26, 26, 28, 29, 29, 30, 31, 32, 33, 33, 34, 34, 35, 36, 36, 
36.5, 37, 38, 38, 39, 39, 39, 38, 37, 36, 36, 35, 34, 33, 33, 
33, 32.5, 32, 32, 31, 30, 26, 24, 24, 24, 24, 24, 24, 25, 25, 
25, 25, 26, 26, 26, 27, 28.3, 29.7, 31, 32, 33, 33, 34, 34, 34, 
35, 35, 35, 36, 37, 38, 39, 39, 39, 39, 38.5, 38, 37, 36, 35, 
34, 34, 33, 32.5, 32, 30, 30, 30, 28, 28, 27, 25, 25, 24, 24, 
24, 24, 24, 25, 25, 25, 25, 25, 27, 28, 30, 31, 33, 34, 37, 37, 
38, 38, 39, 39.3, 39.7, 40, 40, 40, 40, 41, 40, 40, 38, 38, 36, 
35, 34, 33, 31, 31, 30, 31, 30, 30, 29, 28, 28, 28, 27, 27, 28, 
27, 24, 24, 24, 23, 23, 23, 24, 24, 26, 27, 28, 29.7, 31.3, 33, 
34, 35.3, 36.7, 38, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 
39, 38, 37, 36, 34, 33, 32, 32, 30, 28.5, 27, 27, 27, 27, 27, 
26, 26, 26, 25.3, 24.7, 24, 24, 24, 24, 23, 23, 24.5, 26, 28, 
29.7, 31.3, 33, 34.5, 36, 37, 38, 39, 40, 40, 40.3, 40.7, 41, 
41, 41, 41, 41, 41, 41, 39.5, 38, 37, 37, 35, 34, 34, 33, 33, 
33, 32, 31, 31, 30, 30, 30, 29, 29, 28.5, 28, 28, 28, 29, 29, 
28.5, 28, 29, 29, 30, 31, 31, 32.5, 34, 35, 37, 39.5, 42, 42, 
42, 42, 42, 43, 44, 45, 45, 44, 43, 42, 42, 41, 39, 36.5, 34, 
34, 33, 33, 33, 33, 33, 33, 32, 32, 32, 32, 31.5, 31, 30, 30, 
30, 29, 29, 28, 27, 28, 29, 30, 31, 32.5, 34, 34, 35, 37, 40, 
41, 41, 42, 42, 42.5, 43, 43, 43, 42, 42, 43, 42, 42, 41, 40, 
39, 37.5, 36, 35, 35, 34, 34, 33, 32, 32, 32, 32, 31, 31, 31, 
31, 31, 28, 28, 28, 28, 28, 28.5, 29, 30, 30.5, 31, 32, 33.5, 
35, 36, 37, 38, 39, 38, 39, 40, 41, 42, 42, 43, 42, 42, 42, 41, 
41, 40, 39, 38, 37, 36, 35, 34, 34, 33, 32, 32, 31, 30.5, 30, 
30, 30, 30, 29, 29, 29, 28, 27, 27, 27, 27, 28.5, 30, 31.5, 33, 
34, 35, 36, 37, 38, 38, 39, 40, 41, 42, 42, 42, 42, 42, 42, 42, 
42, 42, 42, 42, 40, 40, 39, 37, 37, 36.5, 36, 35, 34, 34, 33, 
33, 32, 32, 32, 32, 32, 32, 31, 31, 30, 30, 30, 30, 31, 31, 31, 
32, 33.5, 35, 35.5, 36, 37, 37, 38, 39, 40, 41, 41, 42, 42, 42, 
43, 43, 42, 41, 41, 40, 38, 36, 35, 34, 34, 35, 36, 36, 35, 35, 
33, 33, 32, 31, 31, 30, 30, 29, 29, 29, 29, 29, 31, 31.5, 32, 
31, 30, 31, 32, 33, 34, 34, 35, 36, 36, 37, 37, 38, 38, 38, 39, 
39, 39, 39, 39, 39, 39, 38, 37, 37, 37, 37, 35, 33, 31, 31, 31, 
30, 30, 30, 30, 30, 30, 30, 30, 29, 29, 28, 28, 27, 27, 26, 29, 
29, 30, 31, 32, 32, 33, 33, 33, 34, 34, 35, 35, 37, 37, 37, 37, 
37, 38, 38, 38, 38, 37, 37, 36, 35, 34.5, 34, 34, 33, 33, 32, 
32, 32, 32, 32, 31, 31, 31, 31, 31, 31, 31, 30, 30, 29.5, 29, 
29, 30, 30, 31, 32, 32, 33, 33.5, 34, 34, 34, 30.7, 27.3, 24, 
24, 24, 24.7, 25.3, 26, 27, 28, 29, 29, 29, 28, 28, 27, 27, 26, 
25, 25, 25, 24.5, 24, 24, 24, 24, 24, 24, 24, 24, 23, 23, 23, 
23, 23, 23, 24, 24, 24, 25, 25, 26, 28, 29, 30, 31, 31, 32, 33, 
34, 35, 35, 36, 37, 37, 37, 37, 37, 37, 37, 35, 34, 33, 33, 33, 
32, 31.7, 31.3, 31, 31, 31, 31, 30, 30, 30, 30, 28, 28, 27, 27, 
26, 26, 25, 25, 25, 25, 25, 26, 27, 28, 29.7, 31.3, 33, 34, 34, 
35, 36, 37, 36, 36, 35, 35, 34, 32, 32, 33, 31.3, 29.7, 28, 28, 
29, 29, 29, 29, 28, 27.5, 27, 26.5, 26, 26, 26, 26, 26, 26, 25, 
25, 25, 25, 25, 24, 24, 23, 23, 23, 24.5, 26, 27, 28, 29, 30, 
31, 32, 32, 32, 33, 34, 35, 36, 36, 36, 36, 36, 36, 36, 37, 37, 
37, 36, 36, 34, 34, 33, 32, 32, 31, 31, 30, 28, 28, 28, 28, 27, 
27, 27, 27, 27, 27, 27, 27, 26.5, 26, 26, 26, 26, 27, 28.8, 30.5, 
32.2, 34, 34, 34, 35, 36, 37, 38, 38, 38, 39, 40, 39, 39, 39, 
40, 40, 40, 39, 39, 38, 37, 35, 35, 34, 34, 34, 33, 32, 32, 32, 
32, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 32, 33, 34, 
35, 37, 37, 37.3, 37.7, 38, 39, 40, 43, 43, 43, 44, 44, 43, 43, 
43, 43, 43, 43, 42, 41, 40, 39, 38, 37, 36, 35, 35, 34, 34, 34, 
34, 34, 33, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 33, 33, 34, 
35, 36, 37, 38, 38, 40, 40.5, 41, 42, 42, 42, 42, 41, 40, 40, 
39, 39, 37, 32, 27, 26, 26, 27, 28, 27, 27, 27, 26, 25, 25, 25, 
25, 25, 25, 25, 25, 25, 25, 25, 24, 23, 23, 22.5, 22, 22, 23, 
24, 25, 26.3, 27.7, 29, 30, 32, 33, 34, 36, 37, 37, 39, 40, 40.5, 
41, 42, 42, 42, 43, 43, 42, 41.5, 41, 39, 38, 35, 35, 35, 34, 
33, 33, 33, 33, 32, 32, 31, 31, 30, 30, 30, 29, 29, 28, 28, 28, 
28.5, 29, 29, 29, 30, 32, 32, 34, 35, 37, 38, 39, 40, 41, 42, 
42, 43, 44, 44, 44, 45, 45, 45, 44, 43, 43, 42, 40, 38.5, 37, 
35, 35, 34, 34, 33, 34, 33, 33, 33, 33, 32, 32, 31, 32, 31, 30, 
29, 29, 29, 29, 30, 30, 31, 32.7, 34.3, 36, 37, 38, 39, 40, 41, 
41, 42, 42, 43, 44, 44, 44, 44, 44, 43.5, 43, 43, 42, 42, 40, 
38, 38, 37, 36, 36, 36, 36, 35, 35, 34, 34, 34, 34, 32, 32, 31.5, 
31, 31, 30, 30, 30, 30, 31, 33, 34, 35, 37, 38, 39, 40, 40, 42, 
42, 43, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 43, 42, 40, 40, 
40, 39, 38, 38, 38, 37, 37, 36, 36, 35.5, 35, 34.5, 34, 33, 33, 
32, 32, 31, 30, 30, 30, 31, 32, 33.5, 35, 36.5, 38, 38, 39, 40, 
41, 41, 41, 42, 42, 42, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 
42, 40.5, 39, 38, 38, 37, 37, 37, 36, 36, 36, 36, 36, 36, 36, 
35, 35, 34, 34, 34, 32, 32, 31, 31, 33, 34, 35, 35, 36, 38, 39, 
40, 41, 42, 42.5, 43, 43, 44, 44, 45, 45, 45, 45, 45, 45, 45, 
44, 43.5, 43, 43, 42, 41, 40, 40, 39, 39, 38, 38, 37, 36, 36, 
35, 34, 34, 34, 32, 32, 32, 32, 32, 32, 32, 34, 35, 35, 37, 38, 
38, 39, 39, 40, 41, 42, 43, 44, 44, 44, 45, 45, 46, 46, 46, 45, 
45, 45, 44, 42, 40, 38, 37, 37, 37, 37, 37, 36, 35), .Dim = c(1248L, 
1L), .Dimnames = list(NULL, ""temperature""))
</code></pre>

<p>Forecasted temperature values are:</p>

<pre><code>forecast_temp &lt;- structure(c(35, 34, 34, 33, 33, 30, 29.7, 29.3, 29, 28, 28, 27, 
27.3, 27.7, 28, 29, 30.5, 32, 33.5, 35, 36, 36, 37, 39, 39.5, 
40, 40, 41, 42, 42, 43, 43, 43, 44, 44, 44, 43, 42, 40, 39, 39, 
38, 37, 37, 36, 35, 35, 34), .Dim = c(48L, 1L), .Dimnames = list(
    NULL, ""temperature""))
</code></pre>

<h3><em>UPDATE-1</em></h3>

<p>From @Stephan's suggestion, I followed the approach mentioned by Prof. Rob at <a href=""http://robjhyndman.com/hyndsight/dailydata/"" rel=""nofollow"">link1</a>, <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">link2</a> as</p>

<pre><code>library(forecast)
tsob &lt;- ts(train_power,frequency = 48) #training electriciy data at daily frequency
tsob_weekly &lt;- fourier(ts(train_power,frequency = 7*48),K=3) #training data at weekly frequency
tempob &lt;- ts(train_temperature,frequency = 48) #Temperature, another predictor variable
fit &lt;- auto.arima(tsob,xreg=cbind(tsob_weekly,tempob),seasonal=FALSE)
tempob_forecast &lt;- ts(test_temperature,frequency = 48) #forecasted temperauture values
forecast_val &lt;- forecast(fit,xreg=tempob_forecast,h=48*5) #forecast for coming 5 days
</code></pre>

<p>As evident from the code, I have used Fourier transformation suggested at above links to show weekly seasonal affect. Up to <code>auto.arima()</code>, it works properly but at forecast function an error is thrown as:</p>

<pre><code>Error in forecast.Arima(fit, xreg = tempob_forecast, h = 48 * 5) : 
  Number of regressors does not match fitted model
</code></pre>

<p>The error is clear, i.e., I do not provide the forecasted regressor values of <code>tsob_weekly</code>, which I don't have. How should I handle this issue? Prof. Rob has used the same value of regressor for both the <code>auto.arima</code> and <code>forecast</code> functions.</p>
"
"NaN","NaN","214969","<p>I refer to this question <a href=""http://stats.stackexchange.com/questions/87726/how-to-get-the-true-mean-forecast-using-the-arima-package-with-a-box-cox-transfo"">How to get the true mean forecast using the Arima package with a Box-Cox transformation</a></p>

<p>Could anyone please tell me why is the variance of the Box-Cox transformed data given by this formula:</p>

<pre><code>fvar &lt;- ((BoxCox(fc$upper,fit$lambda)BoxCox(fc$lower,fit$lambda))/qnorm(0.975)/2)^2
</code></pre>

<p>Any hints or references?
I understand what the BoxCox() function does, and also qnorm(), but where is this formula from? Thanks!</p>
"
"0.167053813916911","0.172988979511468","219792","<p>My objective it to manually compute one-step ahead forecast using the estimated coefficientes given by the <code>arima</code> function in R.</p>

<p>I will consider the specific model ARIMA(0,0,0)(0,1,3) with weekly seasonality (<code>period = 7</code>). The equation for this model is:</p>

<p>$$ x_{t} = x_{t-7} + \Theta_{1}e_{t-7} + \Theta_{2}e_{t-14} + \Theta_{3}e_{t-21} + e_{t} $$</p>

<p>I will start by computing the one-step ahead forecast using the <code>predict</code> function and then compare it's value with the result given from the above equation. So first I will have to compute <code>theta</code> vector and the residuals vector <code>e_t</code>.</p>

<p>My data consists of daily observations for 35 days.</p>

<pre><code>data &lt;- c(2570,4530,3990,4480,5880,3380,1340,4180,4600,4170,1980,5170,2900,940,7430,6330,7310,9210,8460,3080,1020,4400,2980,5090,7230,3670,2440,1980,2090,3380,2410,3630,3930,2450,1590)
</code></pre>

<p>I start by fitting the model:</p>

<pre><code>fit &lt;- arima(data, order=c(0,0,0), seasonal=list(order=c(0,1,3), period=7), method=""ML"")
</code></pre>

<p>Then I recover the estimated <code>theta</code> coefficients and the last 3 observed residuals. Note that the seasonality period is 7, so the last 3 residuals regarding this seasonality are as stated:</p>

<ul>
<li>Last residual is given by position <code>35 - 7 + 1 = 29</code></li>
<li>Before last residual is given by position <code>35 - 14 + 1 = 22</code></li>
<li>Before before last residual is given by position <code>35 - 28 + 1 = 15</code></li>
</ul>

<p>So that's the reason I have the funny indexes in line two of the following code:</p>

<pre><code>theta &lt;- as.vector(fit$coef)
e_t &lt;- fit$residuals[c(29,22,15)]
</code></pre>

<p>Finnaly, I also fetch the last observation (given seasonality period 7)</p>

<pre><code>z_t &lt;- data[29]
</code></pre>

<p>And when I compute the above formula:</p>

<pre><code>sum(e_t * theta) + z_t)
</code></pre>

<p>I get the value of <code>4613.141</code> which is different from </p>

<pre><code>predict(fit)$pred[1]
</code></pre>

<p>which returns the value <code>4671.607</code>.</p>

<p>Can you please explain where is my error? I've tried this procedure with several different samples and sample sizes and I never get the same forecast as the R function.</p>
"
"0.0482242822170412","0.0499376169438922","220239","<p>I have 5 years of daily price data of an asset for which I have fitted an ARMA model. I want to generate 10000 simulations for next 6 months using the last available value for the asset as starting value. But I noticed that in <code>arima.sim</code> function there is no option to put a starting value as input or even the time period for which I need the simulations.</p>
"
"0.0278423023194852","0.0864944897557338","220680","<p>I am trying to fit a time series with function <code>auto.arima</code> in the ""forecast"" package in R that is choosing the best model automatically. Since I am using it for my thesis, I have to show the trace of stepwise selection algorithm manually (without adding line ""trace"" in <code>auto.arima</code> code) with my web apps here <a href=""https://nugraha92.shinyapps.io/Analisis/"" rel=""nofollow"">https://nugraha92.shinyapps.io/Analisis/</a></p>

<p>But, when I manually checked it, I found that <code>auto.arima</code> did not choose minimum AIC values.</p>

<p>Then, I input the <code>trace=TRUE</code> line and the result shows that the AIC/BIC value is not the same between the ""trace ARIMA list"" and ""choosen ARIMA model"". For example, in the image it is shown that the best model is ARIMA(3,1,3), BIC score at the list is -252.4451 while at the equation below is -250.89.</p>

<p>Why and which one is the correct one? Did I make some mistakes?</p>

<p><a href=""http://i.stack.imgur.com/KWSPh.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/KWSPh.png"" alt=""enter image description here""></a></p>
"
"0.0340997169735237","0.0706224551546449","220830","<p>I need to forecast using <code>HoltWinters</code> with regression parameters using R. But I found there is not any option of <code>xreg</code> in <code>HoltWinters</code> function in R. I thought to use <code>auto.arima</code> with <code>xreg</code> option but my <code>HoltWinters</code> is performing better than <code>auto.arima</code> without any regression parameters.</p>

<p>Can you please suggest me how to incorporate <code>xreg</code> in <code>HoltWinters</code> function in R?</p>
"
"0.0340997169735237","0.0706224551546449","220973","<p><a href=""http://i.stack.imgur.com/27CVA.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/27CVA.png"" alt=""enter image description here""></a>I'm using the R function <code>auto.arima</code> to fit an arima model for a time series, 
the result is an ARIMA(2,1,1). After that I apply the <code>forecast</code> function to predict some futur values. My question is Should I do the transformation (""un-differentiate"" the predicted values) or is it done by <code>forecast</code> automatically ? 
edit : here is what i get when i execute the code : </p>

<pre><code>arimaf = auto.arima(timeseries)
pred = forecast(arimaf, h = 10)
plot(pred, main = ""PREDICTION USING ARIMA(2,1,1)"")
</code></pre>
"
"0.0681994339470473","0.0706224551546449","221475","<p>I fitted a times series using an ARIMA(6,1,0), and tried to analyze the residuals, I wrote a code that gave me same four plots as in the lm R function, the one I'm interested in the last one where I plot residuals against the fitted values, here is the plot<a href=""http://i.stack.imgur.com/PRNVy.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/PRNVy.png"" alt=""enter image description here""></a>
there is a clear heteroskedasticity in here right ? Should I use GARCH to get a better fit ?</p>

<p>edit here is how the data plot looks like : <a href=""http://i.stack.imgur.com/hgxv6.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/hgxv6.png"" alt=""enter image description here""></a></p>
"
"0.128884815556617","0.17350306379343","221881","<p>This question is sort of a follow up of <a href=""http://stats.stackexchange.com/questions/140163/timeseries-analysis-procedure-and-methods-using-r"">this</a> great thread.</p>

<p>I have a Time Series Analysis project in which I have to create a model to predict new values given historical univariate data using R. My plan is to run several models, assess their accuracies and choose the best one to use in production.</p>

<p>I must admit that I'm a beginner in TS analysis, so I'm not sure about the procedure that I must take. The main question is: Should I do a seasonal adjust in the data before fitting it into models?</p>

<p>I've done some simulation myself and the results are quite interesting:</p>

<h2>Raw data (without adjusting)</h2>

<pre><code>library(seasonal)
library(forecast)
data(""AirPassengers"")

training &lt;- window(AirPassengers, end = c(1959, 12))
test &lt;- window(AirPassengers, start = c(1960, 1))

models &lt;- list(
  mod_arima = auto.arima(training, ic='aicc', stepwise=FALSE),
  mod_etrainingp = ets(training, ic='aicc', restrict=FALSE),
  mod_neural = nnetar(training, p=12, size=25),
  mod_tbats = tbats(training, ic='aicc', seasonal.periods=12),
  mod_bats = bats(training, ic='aicc', seasonal.periods=12),
  mod_stl = stlm(training, s.window=12, ic='aicc', robust=TRUE, method='ets'),
  mod_sts = StructTS(training)
)

forecasts &lt;- lapply(models, forecast, 12)

acc &lt;- lapply(forecasts, function(f){
  accuracy(f, test)[2,,drop=FALSE]
})
acc &lt;- Reduce(rbind, acc)
row.names(acc) &lt;- names(forecasts)
(acc &lt;- round(acc, 2))
                   ME   RMSE    MAE   MPE  MAPE MASE ACF1 Theil's U
mod_arima      -15.72  22.96  17.91 -3.68  4.05 0.59 0.08      0.51
mod_etrainingp   4.99  19.01  14.40  0.75  3.03 0.47 0.27      0.41
mod_neural      12.97  26.08  23.38  2.55  4.94 0.77 0.16      0.55
mod_tbats      -15.49  25.67  18.20 -3.71  4.14 0.60 0.17      0.58
mod_bats         0.69  23.12  16.48 -0.35  3.45 0.54 0.38      0.50
mod_stl         31.35  57.93  39.95  5.31  7.43 1.31 0.65      1.01
mod_sts        177.41 199.41 177.41 36.02 36.02 5.83 0.77      3.76
</code></pre>

<h2>Adjusted data</h2>

<pre><code>adj &lt;- seas(AirPassengers)
adj &lt;- final(adj)

adj_training &lt;- window(adj, end = c(1959, 12))
adj_test &lt;- window(adj, start = c(1960, 1))

models_adj &lt;- list(
  mod_arima = auto.arima(adj_training, ic='aicc', stepwise=FALSE),
  mod_etrainingp = ets(adj_training, ic='aicc', restrict=FALSE),
  mod_neural = nnetar(adj_training, p=12, size=25),
  mod_tbats = tbats(adj_training, ic='aicc', seasonal.periods=12),
  mod_bats = bats(adj_training, ic='aicc', seasonal.periods=12),
  mod_stl = stlm(adj_training, s.window=12, ic='aicc', robust=TRUE, method='ets'),
  mod_sts = StructTS(adj_training)
)

forecasts_adj &lt;- lapply(models_adj, forecast, 12)

acc_adj &lt;- lapply(forecasts_adj, function(f) accuracy(f, adj_test)[2,,drop=FALSE])
acc_adj &lt;- Reduce(rbind, acc_adj)
row.names(acc_adj) &lt;- names(forecasts_adj)
(acc_adj &lt;- round(acc_adj, 2))
                   ME  RMSE   MAE   MPE MAPE MASE ACF1 Theil's U
mod_arima        6.02 10.05  9.49  1.22 1.99 0.31 0.64      1.26
mod_etrainingp  -2.76  7.12  4.43 -0.62 0.96 0.15 0.40      0.90
mod_neural     -30.84 33.27 30.84 -6.47 6.47 1.02 0.56      4.11
mod_tbats      -15.38 16.60 15.38 -3.26 3.26 0.51 0.05      2.09
mod_bats        -7.13 10.41  7.31 -1.55 1.58 0.24 0.43      1.34
mod_stl         -1.77  7.01  4.67 -0.41 1.01 0.15 0.39      0.89
mod_sts         -9.59 11.05  9.59 -2.04 2.04 0.32 0.11      1.38
</code></pre>

<p>Now, a comparison table to highlight the differences:</p>

<pre><code>abs(acc_adj) &lt; abs(acc)
                  ME  RMSE   MAE   MPE  MAPE  MASE  ACF1 Theil's U
mod_arima       TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE     FALSE
mod_etrainingp  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE     FALSE
mod_neural     FALSE FALSE FALSE FALSE FALSE FALSE FALSE     FALSE
mod_tbats       TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE     FALSE
mod_bats       FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE     FALSE
mod_stl         TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE      TRUE
mod_sts         TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE      TRUE
as.data.frame(rowMeans(abs(acc_adj) &lt; abs(acc)))
               rowMeans(abs(acc_adj) &lt; abs(acc))
mod_arima                                  0.750
mod_etrainingp                             0.750
mod_neural                                 0.000
mod_tbats                                  0.875
mod_bats                                   0.500
mod_stl                                    1.000
mod_sts                                    1.000
</code></pre>

<p>The only model that has better accuracy in raw data is <code>mod_neural</code>, created with neural networks.</p>

<p>So, what should I learn from this experiment? Does it make sense to use X13ARIMA-SEATS to adjust a series before fitting models? </p>
"
"0.173875122255988","0.180052638465683","222914","<p>I have a time series object <code>calc_visit_ts</code>. <strong>I want to apply the best fit time series model based on the MAPE value for each model.</strong> The issue I face is that the MAPE value HOLT-WINTER multiplicative model cannot be calculated in the same way as the other models(as it gives me a different MAPE value when compared to <code>summary(visit_model_Hw_M)</code>).</p>

<pre><code>#### AUTO-ARIMA
visit_model_Arima &lt;- auto.arima(calc_visit_ts)
# summary(visit_model_Arima)

#### HOLT-WINTER ADDITIVE
visit_model_Hw_A &lt;- hw(calc_visit_ts,h=monthly_prediction,seasonal = ""additive"")
# summary(visit_model_Hw_A)

#### HOLT-WINTER MULTIPLICATIVE
visit_model_Hw_M &lt;- hw(calc_visit_ts,h=monthly_prediction,seasonal = ""multiplicative"")
# summary(visit_model_Hw_M)


#### Calculating MAPE on models for best suit
model_Mape&lt;- c( MAPE_model(visit_model_Arima)
                ,MAPE_model(visit_model_Hw_A))
                #,MAPE_model(visit_model_Hw_M))  this is not accurate

model_Mape=na.omit(model_Mape)
token&lt;-which(min(model_Mape)==model_Mape)

if(length(token)&gt;0)
{
  if(token==1)
    {visit_model&lt;-visit_model_Arima
  }else if(token==2)
    {visit_model&lt;-visit_model_Hw_A
  }else if(token==3)
    {visit_model&lt;-visit_model_Hw_M
  }else 
  {
    ##EXCEPTION HANDLING  
  }
}

summary(visit_model)
</code></pre>

<p>And here is the <strong>function I use to perform MAPE calculation</strong> on the models - </p>

<pre><code>MAPE_model &lt;- function(visit_model) {
 #CHECK FOR ZERO CONDIITION  if(visit_model$x!=0)
 mape = mean(abs(visit_model$residuals)/visit_model$x)
 return(mape)
}
</code></pre>

<p><strong>Data</strong> for time series -</p>

<pre><code>calc_visit_ts
     Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec
2012          35  53  65  60  64  49  63  55  59  66
2013  62  54  77  67  84  62  82  65  59  67  60  67
2014  73  75  55  76  93  96  89  76  88  65  83  82
2015  76  72  75  94  91  83  72  73  80  83  81  81
2016  97  91  90  80 101  98  

dput(calc_visit_ts)
structure(c(35, 53, 65, 60, 64, 49, 63, 55, 59, 66, 62, 54, 77, 
67, 84, 62, 82, 65, 59, 67, 60, 67, 73, 75, 55, 76, 93, 96, 89, 
76, 88, 65, 83, 82, 76, 72, 75, 94, 91, 83, 72, 73, 80, 83, 81, 
81, 97, 91, 90, 80, 101, 98), .Tsp = c(2012.16666666667, 2016.41666666667, 
12), class = ""ts"")
</code></pre>

<p>To show exactly what I mean -</p>

<p><strong>Holt-Winter Additive Plot</strong></p>

<p><a href=""http://i.stack.imgur.com/i0c0K.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/i0c0K.jpg"" alt=""Holt-Winter Additive Plot""></a></p>

<p><strong>Holt-Winter Multiplicative Plot</strong>
<a href=""http://i.stack.imgur.com/rgvFt.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/rgvFt.jpg"" alt=""Holt-Winter Multiplicative Plot""></a></p>

<p>The <strong>issue</strong> is <code>summary(visit_model_Hw_M)</code> gives <code>MAPE = 9.075097</code>
whereas, <code>MAPE_model(visit_model_Hw_M)</code> gives <code>0.001273087</code> because the multiplicative model fits the curve(data points) therefore using <code>visit_model_Hw_M$residuals</code> isn't an appropriate way to calculate the MAPE(as the function tries to fit the curve).</p>

<p>Is there a way I can fetch the MAPE value for HOLT-WINTER multiplicative from the summary itself? OR a way to correctly estimate the MAPE value for the HOLT-WINTER multiplicative model?</p>
"
"0.109362392486473","0.132122515500744","223379","<p>I'm fitting an <code>arima</code>(1,0,0) model using the <code>forecast</code> package in R on the <code>usconsumption</code> dataset. However, when I mimic the same fit using <code>lm</code>, I get different coefficients. My understanding is that they should be the same (in fact, they give the same coefficients if I model an <code>arima</code>(0,0,0) and <code>lm</code> with only the external regressor, which is related to this post: <a href=""http://stats.stackexchange.com/questions/28472/regression-with-arima0-0-0-errors-different-from-linear-regression"">Regression with ARIMA(0,0,0) errors different from linear regression</a>). </p>

<p>Is this because <code>arima</code> and <code>lm</code> use different techniques to calculate coefficients? If so, can someone explain the difference?  </p>

<p>Below is my code.</p>

<pre><code>&gt; library(forecast)
&gt; library(fpp)
&gt; 
&gt; #load data
&gt; data(""usconsumption"")
&gt; 
&gt; #create equivalent data frame from time-series
&gt; lagpad &lt;- function(x, k=1) {
+   c(rep(NA, k), x)[1 : length(x)] 
+ }
&gt; usconsumpdf &lt;- as.data.frame(usconsumption)
&gt; usconsumpdf$consumptionLag1 &lt;- lagpad(usconsumpdf$consumption)
&gt; 
&gt; #create arima model
&gt; arima(usconsumption[,1], xreg=usconsumption[,2], order=c(1,0,0))

Call:
arima(x = usconsumption[, 1], order = c(1, 0, 0), xreg = usconsumption[, 2])

Coefficients:
         ar1  intercept  usconsumption[, 2]
      0.2139     0.5867              0.2292
s.e.  0.0928     0.0755              0.0605

sigma^2 estimated as 0.3776:  log likelihood = -152.87,  aic = 313.74
&gt; 
&gt; #create lm model
&gt; lm(consumption~consumptionLag1+income, data=usconsumpdf)

Call:
lm(formula = consumption ~ consumptionLag1 + income, data = usconsumpdf)

Coefficients:
    (Intercept)  consumptionLag1           income  
         0.3779           0.2456           0.2614  
</code></pre>
"
"0.118124884643724","0.101934733735916","223457","<p>Suppose I have a training dataset, I use <code>auto.arima</code> (from ""forecast"" package in R) to fit the training data. As a result I get the lag and integration orders $(p, d, q)$ and the corresponding coefficients $\psi_i$ and $\theta_i$.</p>

<pre><code>ytrain = c(0.435477843, 0.435394762, 0.195528995, 1.451623315, 1.740084831 2.379904714, 1.092366508, 0.001031411, 0.592164090, 0.670323418)

fit &lt;- auto.arima(ytrain)
</code></pre>

<p>Now I have new data </p>

<pre><code>ytest = c(-0.1349199  0.9001208 -0.5171740 -0.9958452  0.4125953 -0.3320575  0.1633313  0.2890109 -0.4284824  0.7902680)
</code></pre>

<p>I want to fit this new data by using the model from training data (using the same $(p, d, q)$ and also the same corresponding coefficients). I.e. I want to use the model I have from <code>ytrain</code> to make prediction based on <code>ytest</code>. As a result I can know if there are any points in the new data looking like anomaly points (compared to the training data)</p>

<p>I have searched long time and haven't find a R function to implement it. I know I can compute this by hand, e.g. for ARMA(1,2):</p>

<p>$\hat{Y}_n =  \hat{\mu} + \hat{\psi}_1 Y_{n-1} - \hat{\theta}_{1} \epsilon_{n-1} - \hat{\theta}_2 \epsilon_{n-2} $</p>

<p>But if I do this, I am not sure how to start to get $\epsilon_1 = Y_1 - \hat{Y}_1$ and $\epsilon_2 = Y_2 - \hat{Y}_2$ to start since I don't have $\hat{Y}_1$ and $\hat{Y}_2$. </p>

<ul>
<li>Could anyone suggest an R function for doing this? Or if not,  </li>
<li>Could anyone help me with this question if there is no R function doing this?</li>
</ul>
"
"0.118124884643724","0.101934733735916","223872","<p>Data consisting of 30 values is stored in a time series <code>time</code>.<br>
After applying ARIMA modelling on <code>time</code>, I used <code>forecast</code> function to predict future values:</p>

<pre><code>model = arima(time, order = c(3,2,1))
prediction = forecast.Arima(model,h=10)
prediction step is not working and showing error 
Error in ts(x) : object is not a matrix
</code></pre>

<p>As you see above, I am getting an error message. But if I do</p>

<pre><code>model = arima(time[1:25], order = c(3,2,1))
prediction = forecast.Arima(model,h=10)
</code></pre>

<p>it works. Why is it so?</p>

<p>When I used the <code>predict</code> function </p>

<pre><code>model = arima(time, order = c(3,2,1))
prediction=predict(model,n.ahead=10)
</code></pre>

<p>it also works.</p>

<p><strong>Which</strong> function would be better to use, <code>predict</code> or <code>forecast</code>, for ARIMA models in R, and <strong>why</strong>?</p>
"
"0.0964485644340824","0.0998752338877845","224078","<p>I have three related questions on the package CausalImpact in R. The package can be found <a href=""https://github.com/google/CausalImpact"" rel=""nofollow"">here</a> and a reproducible example is below.</p>

<ol>
<li>Do I basically understand correctly, that the model makes ""1-step ahead""
predictions? I assume it works like a simple lm model that makes
lots of regressions for t+1 with predictors values from t-1 and then looks for the most contributory predictors?</li>
<li>When talking about ""coefficients"", does that mean they are (Pearson)
correlation coefficients?</li>
<li>The function <code>plot(impact$model$bsts.model,""coefficients"")</code> produces
a plot with inclusion probabilities ranging from 0 to 1. Is there
any way to access the actual values in a table? I found
that <code>colMeans(impact$model$bsts.model$coefficients)</code> provides some
values but I'd like to have a confirmation for this.</li>
<li>In my code, I changed <code>bsts.model &lt;- bsts(y ~ x1, ss, niter = 1000)</code> to <code>bsts.model &lt;- bsts(y ~ x1 + x2 + x3 + x4 + x5, ss, niter = 1000)</code> in order to get values for different variables that I defined before. I did not change the cbind functions for that, as I weren't sure if that was necessary. Now I wonder what to do when I do have a table with -let's say- 200 predictors? Do I have to enter x1 to x200 after <code>bsts(y ~</code> to find the best predictors?</li>
</ol>

<p>R code below:</p>

<pre><code>install.packages(""devtools"")
library(devtools)
devtools::install_github(""google/CausalImpact"")
#Download the tar from the git and then install the package in RStudio.

library(CausalImpact)

set.seed(1)
x1 &lt;- 100 + arima.sim(model = list(ar = 0.999), n = 100)
x2 &lt;- 50 + arima.sim(model = list(ar = 0.899), n = 100)
x3 &lt;- 80 + arima.sim(model = list(ar = 0.799), n = 100)
x4 &lt;- 1.25 * x1 + rnorm(100)
x5 &lt;- 101 + arima.sim(model = list(ar = 0.999), n = 100)
y &lt;- 1.2 * x1 + rnorm(100)
y[71:100] &lt;- y[71:100] + 10
data &lt;- cbind(y, x1)

dim(data)
head(data)
data
matplot(data, type = ""l"")

time.points &lt;- seq.Date(as.Date(""2014-01-01""), by = 1, length.out = 100)
data &lt;- zoo(cbind(y, x1), time.points)
head(data)

pre.period &lt;- as.Date(c(""2014-01-01"", ""2014-03-11""))
post.period &lt;- as.Date(c(""2014-03-12"", ""2014-04-10""))

impact &lt;- CausalImpact(data, pre.period, post.period)
plot(impact)

summary(impact)
summary(impact, ""report"")
impact$summary

post.period &lt;- c(71, 100)
post.period.response &lt;- y[post.period[1] : post.period[2]]
y[post.period[1] : post.period[2]] &lt;- NA

ss &lt;- AddLocalLevel(list(), y)

bsts.model &lt;- bsts(y ~ x1 + x2 + x3 + x4 + x5, ss, niter = 1000)

impact &lt;- CausalImpact(bsts.model = bsts.model,post.period.response =     post.period.response)

plot(impact)
summary(impact)
summary(impact, ""report"")

plot(impact$model$bsts.model,""coefficients"")
plot(impact$model$bsts.model, ""coef"", inc = .1)
plot(impact$model$bsts.model, ""coef"", inc = .05)

colMeans(impact$model$bsts.model$coefficients)
</code></pre>
"
"0.0482242822170412","0.0499376169438922","224176","<p>If my series requires a log-transformation to stabilize variability, do I apply the <code>sarima</code> function to the log-transformed series or the original series? Does the same apply to the <code>auto.arima</code> function?</p>
"
"0.0964485644340824","0.0998752338877845","225297","<p>Any given ARIMA(p,d,q) model $y^*_t=\sum_{i=1}^pa_iy^*_{t-i}+e_t+\sum_{i=1}^qb_ie_{t-i}$,where $ y^*_t=\Delta_dy_t$ - the difference of d$^{th}$ order, can be re-written as a dynamic linear model in state space (<a href=""http://www-stat.wharton.upenn.edu/~stine/stat910/lectures/14_state_space.pdf"" rel=""nofollow"">1</a>, <a href=""http://radhakrishna.typepad.com/TimeSeries_Analysis_State_Space_Methods.pdf"" rel=""nofollow"">2</a>) in the following way: 
$$\begin{cases} y_t=ZX_t,\\
X_{t+1}=TX_t+Re_t.
\end{cases}
$$
Such representation is used for fitting model to the observed time series and forecasting future values.
What I'm trying to understand is the way function SSMArima from KFAS estimates the covariance matrix of the nondiffuse part of the initial state vector P1. The particularly important to the answer is this piece of code:</p>

<pre><code>nd &lt;- which(diag(P1inf) == 0)
    mnd &lt;- length(nd)
    temp &lt;- try(solve(a = diag(mnd^2) - matrix(kronecker(T[nd, 
        nd], T[nd, nd]), mnd^2, mnd^2), b = c(R[nd, , drop = FALSE] %*% 
        Q %*% t(R[nd, , drop = FALSE]))), TRUE)
    if (class(temp) == ""try-error"") {
        stop(""ARIMA part is numerically too close to non-stationarity."")
    }
    else P1[nd, nd] &lt;- temp
</code></pre>

<p>The main part is </p>

<pre><code>solve(a = diag(mnd^2) - matrix(kronecker(T[nd, 
    nd], T[nd, nd]), mnd^2, mnd^2), b = c(R[nd, , drop = FALSE] %*% 
    Q %*% t(R[nd, , drop = FALSE])))
</code></pre>

<p>In its simplest case when $d = 0, Q = 1$ the covariance matrix P1 is obtained after solving the equation:
$$
(I_{r^2} - T\otimes T)x = vec(RR^T)
$$
and putting the result into square matrix (devectorize it $devec(x)$).
<br>
The closest thing I came to it (considering $\mathbb{E}X_tX_{t}^T = \mathbb{E}X_{t + 1}X_{t + 1}^T = \Sigma $) is solving
$$
\Sigma = T\Sigma T^T + RR^T
$$
But how are these two approaches equivalent and can we really think that $\mathbb{E}X_tX_{t}^T = \mathbb{E}X_{t + 1}X_{t + 1}^T$? There is nothing about it in the specification of the package.</p>
"
"0.0482242822170412","0.0499376169438922","226323","<p>The estimates by function <code>Arima</code> from the ""forecast"" package in R are as follows:</p>

<pre><code>ARIMA(1,0,0) with non-zero mean   

Coefficients:

             ar1   intercept  SEASONAL.. 1MO_LIBOR... GDP_GOODS... CORP...                 

          0.3950   0.0464    -0.0783    -0.0220       1.8730       0.0679 

    s.e.  0.1463   0.0068     0.0083     0.0115       0.8527       0.0323 
</code></pre>

<p>Meanwhile, the EViews estimates are</p>

<p><a href=""http://i.stack.imgur.com/3IgIT.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/3IgIT.png"" alt=""enter image description here""></a></p>

<p>Why do the standard error differ in EViews vs. R even though the coefficients coincide?</p>

<p>By the way, could any body help me to calculate the $p$-values of the coefficients? Is this function correct?</p>

<pre><code>P_Value &lt;- function(fit)
{
  if(inherits(fit,""lm"")){
    res=summary(fit)$coef[-1,4]
  }else
  {
    vars=colnames(fit$xreg)
    n=nobs(fit)
    df=nobs(fit)-length(fit$coef)
    res=(1-pt(abs(fit$coef)/sqrt(diag(fit$var.coef)*n/df),df))*2
  }
  return (res[vars])
}
</code></pre>
"
"0.116321345380461","0.165624338327385","229721","<p>I have 4 years electrical load data. I split the data into 3 years (75%) training data, 1 year for testing (25%). Also I have the temperature data for each day during the previous period. (The link to the dataset: <a href=""https://drive.google.com/open?id=0B08HdcWBksWcTUxqc1ByOW1UVEU"" rel=""nofollow"">here</a>.) </p>

<p>I want to make use of the temperature data to enhance the forecasting using argument <code>xreg</code> in <code>arima</code> function. </p>

<p>Here is my code:</p>

<pre><code>mydata1&lt;-read.csv(""1st pape/kaggle_data.csv"");
mydata&lt;-ts(mydata1[,2],start = c(2004),frequency = 365)

#split the data into trainData and test data
trainData = window(mydata, end=c(2007))
testData = window(mydata, start=c(2007))
temp&lt;-ts(mydata1[,3],start = c(2004),frequency = 365)

#split the temperature into trainData and test data
trainReg = window(temp, end=c(2007))
testReg = window(temp, start=c(2007))
</code></pre>

<p>Apply ARIMA model without using <code>xreg</code>:</p>

<pre><code>mod_arima &lt;- auto.arima(trainData, ic='aicc', stepwise=FALSE)
summary(mod_arima)
Series: trainData 
ARIMA(1,0,3) with non-zero mean 

Coefficients:
         ar1      ma1      ma2      ma3  intercept
      0.9642  -0.2098  -0.2157  -0.1693  24008.122
s.e.  0.0110   0.0322   0.0330   0.0325   1018.007

sigma^2 estimated as 9318421:  log likelihood=-10347.38
AIC=20706.75   AICc=20706.83   BIC=20736.75

Training set error measures:
                   ME     RMSE      MAE       MPE     MAPE      MASE
Training set 6.102332 3045.638 2293.946 -1.519484 9.625694 0.5151126
                    ACF1
Training set 0.004483007

plot(forecast(mod_arima)); lines(testData , col=""red"", start= c(2007,1,1)); 
legend(""topleft"", lty=1,col=c(4,2),legend=c(""forecasted data"",""real data""))

y &lt;- msts(trainData, c(7,365)) # multiseasonal ts
x &lt;- msts(trainReg, c(7,365)) # multiseasonal ts

fit &lt;- auto.arima(y, xreg=(fourier(y, K=c(3,30))))
fit_f &lt;- forecast(fit, xreg= fourier(y, K=c(3,30), 365), 365)
plot(fit_f)
</code></pre>

<p>the red line is the actual data, while the blue is the foretasted data. The left plot is appeared before using fourier function, while the right after using it. </p>

<p><a href=""http://i.stack.imgur.com/QxvKC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/QxvKC.png"" alt=""enter image description here""></a></p>

<p>Apply ARIMA model using <code>xreg</code>:</p>

<pre><code>mod_arima2 &lt;- auto.arima(trainData ,xreg = trainReg, ic='aicc', stepwise=FALSE)
summary(mod_arima2)
Series: trainData 
ARIMA(1,0,3) with non-zero mean 

Coefficients:
         ar1      ma1      ma2      ma3  intercept  trainReg
      0.9709  -0.2403  -0.2108  -0.1609  29984.188  -88.3976
s.e.  0.0094   0.0320   0.0330   0.0321   1468.108   13.1966

sigma^2 estimated as 8955023:  log likelihood=-10325.13
AIC=20664.26   AICc=20664.36   BIC=20699.26

Training set error measures:
                   ME     RMSE      MAE       MPE     MAPE      MASE
Training set 6.030471 2984.292 2267.803 -1.464553 9.529988 0.5092422
                    ACF1
Training set 0.005526977

plot(forecast(mod_arima2,xreg = testReg)); lines(testData , col=""red"", start= c(2007,1,1)); 
legend(""topleft"", lty=1,col=c(4,2), legend=c(""forecasted data"",""real data""))

l = (fourier(y, K=c(3,30)))
z = cbind(l,x)
fit2 &lt;- auto.arima(y, xreg=z)
fit_f2 &lt;- forecast(fit, xreg= z, 365)
plot(fit_f2)
</code></pre>

<p><a href=""http://i.stack.imgur.com/TgJE5.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/TgJE5.png"" alt=""enter image description here""></a>
<strong>Questions</strong>:</p>

<ol>
<li>Did I use <code>xreg</code> correctly?</li>
<li>If yes, why is the summary the same without using <code>xreg</code>?</li>
<li>Why are the forecasts far away from the real data?</li>
</ol>
"
"0.144672846651124","0.149812850831677","229894","<p>I have monthly data. I am trying make this data stationary by taking normal and seasonal differences. When I am taking seasonal difference, I choose period that gives the lowest Chi-Square result in Ljung-Box test. I have recently notice that other periods with higher Chi-Square results can give better results according to <code>mape, rmse</code> and <code>mae</code>. To be clear, I want to share two steps of my work. First, I take first difference and apply <code>Ljung-Box</code> test with <code>lag=20</code>. If it is higher than the chi-square value in the table, I try seasonal difference. I use a for loop to try different periods and collect test results. And select the one with the smallest Chi-Square result. And in further candidate models, I take period as the period I found. Is it a good logic or not? I also try to choose the one that is closest to Chi-Square table value. But it doesn't give the best model again. It changes according to data. What should I try?  I'am using <code>Forecast::Arima</code> function in R.</p>
"
"0.180438741779263","0.186849453316002","229948","<p>There are likely more than one serious misunderstandings in this question, but it is not meant to get the computations right, but rather to motivate the learning of time series with some focus in mind.</p>

<p>In trying to understand the application of time series, it seems as though de-trending the data makes predicting future values implausible. For instance, the <code>gtemp</code> time series from the <code>astsa</code> package looks like this:</p>

<p><a href=""http://i.stack.imgur.com/Ev6gt.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Ev6gt.png"" alt=""enter image description here""></a></p>

<p>The trend upward in the past decades needs to be factored in when plotting predicted future values.</p>

<p>However, to evaluate the time series fluctuations the data need to be converted into a stationary time series. If I model it as an ARIMA process with differencing (I guess this is carried out because of the middle <code>1</code> in <code>order = c(-, 1, -)</code>) as in:</p>

<pre><code>require(tseries); require(astsa)
fit = arima(gtemp, order = c(4, 1, 1))
</code></pre>

<p>and then try to predict future values ($50$ years), I miss the upward trend component:</p>

<pre><code>pred = predict(fit, n.ahead = 50)
ts.plot(gtemp, pred$pred, lty = c(1,3), col=c(5,2))
</code></pre>

<p><a href=""http://i.stack.imgur.com/Qrx9F.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Qrx9F.png"" alt=""enter image description here""></a></p>

<p>Without necessarily touching on the actual optimization of the particular ARIMA parameters,  <strong>how can I recover the upward trend in the predicted part of the plot?</strong></p>

<p>I suspect there is an OLS ""hidden"" somewhere, which would account for this non-stationarity?</p>

<p>I have come across the concept of <code>drift</code>, which can be incorporated into the <code>Arima()</code> function of the <code>forecast</code> package, rendering a plausible plot:</p>

<pre><code>par(mfrow = c(1,2))
fit1 = Arima(gtemp, order = c(4,1,1), 
             include.drift = T)
future = forecast(fit1, h = 50)
plot(future)
fit2 = Arima(gtemp, order = c(4,1,1), 
             include.drift = F)
future2 = forecast(fit2, h = 50)
plot(future2)
</code></pre>

<p><a href=""http://i.stack.imgur.com/nHRwj.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/nHRwj.png"" alt=""enter image description here""></a></p>

<p>which is more opaque as to its computational process. I am aiming at some sort of understanding of how the trend is incorporated into the plot calculations. Is one of the problems that there no <code>drift</code> in <code>arima()</code> (lower case)?</p>

<hr>

<p>In comparison, using the dataset <code>AirPassengers</code>, the predicted number of passengers beyond the endpoint of the dataset is plotted accounting for this upward trend:</p>

<p><a href=""http://i.stack.imgur.com/Pzf3c.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Pzf3c.png"" alt=""enter image description here""></a></p>

<p>The <a href=""http://www.analyticsvidhya.com/blog/2015/12/complete-tutorial-time-series-modeling/"" rel=""nofollow"">code</a> is:</p>

<pre><code>fit = arima(log(AirPassengers), c(0, 1, 1), seasonal = list(order = c(0, 1, 1), period = 12))
pred &lt;- predict(fit, n.ahead = 10*12)
ts.plot(AirPassengers,exp(pred$pred), log = ""y"", lty = c(1,3))
</code></pre>

<p>rendering a plot that makes sense.</p>
"
"0.0681994339470473","0.0706224551546449","230277","<p>I am doing factor analysis on dichotomous variables. I first calculated the tetrachoric correlations using tetrachoric() in the psych package. </p>

<p>I then used the tetrachoricresult$rho to be my correlation matrix in the factor analysis.</p>

<p>On using 
    fa(tetrachoricresult$rho, factors=2, rotation='varimax'), </p>

<p>I obtained the loadings and a plot.
But on using (factanal(tetrachoricresult$rho,.....same as above), I got the following error-</p>

<p>Error in solve.default(cv) : 
  system is computationally singular: reciprocal condition number = 4.05483e-18</p>

<p>Why is there a difference int he two methods? Which function do I use?</p>
"
"0.137248713299344","0.126333288370973","232320","<p>I have a raster stack of 19 layers called ""raster_bio"". The code to do PCA analysis is:  </p>

<pre><code>vv     &lt;- getValues(raster_bio)
my.prc &lt;- prcomp(na.omit(vv), center=TRUE, scale=TRUE)

# Then I selected first 5 PCs and did varimax rotation.
varima &lt;- varimax(my.prc$rotation[,1:5])
</code></pre>

<p>Then I want to use <code>varima</code>, the 5 rotated loadings, to get spatial PCs:</p>

<pre><code>pprc &lt;- predict(raster_bio, varima)
</code></pre>

<p>The predict function would not work since ""varima"" here is not a model. So I tried something else based on the page mentioned in the reply.</p>

<pre><code>&gt; varima
$loadings

Loadings:
      PC1    PC2    PC3    PC4    PC5   
bio1          0.350 -0.275              
bio2                 0.194         0.658
bio3         -0.134 -0.297         0.573
bio4          0.240  0.426              
bio5          0.505                     
bio6          0.141 -0.418              
bio7          0.221  0.430              
bio8  -0.436  0.394 -0.149        -0.187
bio9   0.369  0.163 -0.189         0.275
bio10         0.502                     
bio11         0.145 -0.415              
bio12  0.180                0.361       
bio13                       0.537       
bio14  0.405                            
bio15 -0.291         0.114         0.322
bio16                       0.520       
bio17  0.369                            
bio18 -0.199                0.523       
bio19  0.443                            

                 PC1   PC2   PC3   PC4   PC5
SS loadings    1.000 1.000 1.000 1.000 1.000
Proportion Var 0.053 0.053 0.053 0.053 0.053
Cumulative Var 0.053 0.105 0.158 0.211 0.263

$rotmat
           [,1]         [,2]        [,3]        [,4]       [,5]
[1,]  0.6108976 -0.003008026 -0.62556569  0.45759088 -0.1614720
[2,]  0.2354179  0.818121480 -0.01836594 -0.43651780 -0.2904661
[3,]  0.6128850 -0.321653016  0.63329819 -0.07390564 -0.3382051
[4,] -0.2685337  0.369912143  0.36408301  0.74586160 -0.3196696
[5,]  0.3516307  0.300620258  0.27332622  0.19568143  0.8203565

 &gt; pca_rotated

Principal Components Analysis
Call: psych::principal(r = na.omit(vv), nfactors = 5, rotate = ""varimax"", 
scores = TRUE)
Standardized loadings (pattern matrix) based upon correlation matrix
    RC1   RC4   RC2   RC3   RC5   h2     u2 com
bio1   0.47  0.01  0.80  0.37 -0.02 1.00 0.0034 2.1
bio2  -0.80 -0.22 -0.22 -0.13  0.46 0.97 0.0327 2.0
bio3  -0.30  0.35 -0.28  0.77  0.29 0.97 0.0332 2.4
bio4  -0.31 -0.45  0.19 -0.80  0.10 1.00 0.0024 2.1
bio5   0.16 -0.35  0.88 -0.26  0.07 1.00 0.0045 1.6
bio6   0.49  0.23  0.50  0.66 -0.08 1.00 0.0025 3.1
bio7  -0.35 -0.45  0.15 -0.80  0.12 1.00 0.0013 2.2
bio8  -0.46 -0.12  0.82 -0.02 -0.21 0.95 0.0531 1.8
bio9   0.77  0.03  0.35  0.43  0.21 0.94 0.0571 2.3
bio10  0.25 -0.29  0.90 -0.17  0.05 0.99 0.0050 1.5
bio11  0.49  0.23  0.50  0.67 -0.06 1.00 0.0022 3.1
bio12  0.63  0.71 -0.12  0.27 -0.01 0.99 0.0111 2.3
bio13  0.37  0.88 -0.14  0.21  0.06 0.98 0.0172 1.5
bio14  0.95  0.21  0.12  0.08  0.05 0.96 0.0355 1.2
bio15 -0.93 -0.09 -0.04 -0.14  0.18 0.93 0.0695 1.1
bio16  0.33  0.89 -0.16  0.26  0.02 0.99 0.0073 1.5
bio17  0.94  0.27  0.08  0.13  0.01 0.97 0.0268 1.2
bio18 -0.04  0.91 -0.20  0.33 -0.11 0.99 0.0127 1.4
bio19  0.97  0.18  0.04  0.08  0.07 0.99 0.0140 1.1

                   RC1  RC4  RC2  RC3  RC5
SS loadings           6.77 3.96 3.85 3.54 0.48
Proportion Var        0.36 0.21 0.20 0.19 0.03
Cumulative Var        0.36 0.57 0.77 0.95 0.98
Proportion Explained  0.36 0.21 0.21 0.19 0.03
Cumulative Proportion 0.36 0.58 0.78 0.97 1.00

Mean item complexity =  1.9
Test of the hypothesis that 5 components are sufficient.

The root mean square of the residuals (RMSR) is  0.01 
 with the empirical chi square  32.94  with prob &lt;  1 

Fit based upon off diagonal values = 1&gt; 
</code></pre>

<p>The objective for me to do the rotation is to see simplified relationship of PCs and bioclimatic variables. In the first part of my code, I did the varimax rotation on eigenvalues, and the correlation matrix seem to do what I want. But by using ""psych::principal"", the Standardized loadings (pattern matrix) based upon correlation matrix does not show simplified relationships. I'm confused about the difference to these. </p>

<p>And based on the youtube video:<a href=""https://www.youtube.com/watch?v=oZ2nfIPdvjY"" rel=""nofollow"">https://www.youtube.com/watch?v=oZ2nfIPdvjY</a>
The varimax rotation was done without scaling the loadings. </p>

<p>So my questions is:
1. how can I get simplified relationship of PCs and bioclimatic variables using rotation correctly?
2. how to get a raster surface of PC other than matrix?</p>

<p>I used to do that in Arcgis which does not give me much control over the process. I appreciate any comment. Thanks a lot!</p>
"
"0.107832773203438","0.111663906120888","232590","<p>I have a number of groups with monthly data from 2010 to 2016. It's over 80 groups. I succesfully ran an ARIMA model with the montly data but with the sales data summed up (without groups). </p>

<p>Now I'd like to compare the performance with a per group model that runs an ARIMA model for each group and maybe later consider another type of grouping (geographical location, clustering, etc.)</p>

<p>I ran my original model with the following code:</p>

<pre><code>        Datos &lt;- read.csv(""C:/Users/borja.sanz/Desktop/Borja/Forecasting/V`enter code here`entas/Datos para Forecast.csv"")
        options(scipen=999)
        library(lubridate)
        Datos$Fecha = dmy(Datos$Fecha)

        #Declare time series
        tsDatos&lt;-ts(Datos$VentaLocal,start = c(2010,1),frequency = 12)
        plot(tsDatos)
        library(forecast)
        library(dplyr)

        #AutoArima Model
        m_aa = auto.arima(tsDatos)
        f_aa = forecast(m_aa, h=36)
        plot(f_aa)

#Create the forecasts along with the lower and upper bound
    forecast_df = data.frame(prediction=f_aa$mean,
                             abajo=f_aa$lower[,2],
                             arriba=f_aa$upper[,2],
                             date=last_date + seq(1/12, 3, by=1/12))
    forecast_df
</code></pre>

<p>This is how my data looks like:</p>

<pre><code>       Group    Year    Month   Date    Sales
1   2010    1   1/01/2010   134536.625
1   2010    2   1/02/2010   117506.625
1   2010    3   1/03/2010   132153.75
1   2010    4   1/04/2010   129723.125
1   2010    5   1/05/2010   135834.5
1   2010    6   1/06/2010   130115.375
1   2010    7   1/07/2010   144716
1   2010    8   1/08/2010   137195
1   2010    9   1/09/2010   137522.875
1   2010    10  1/10/2010   187063
1   2010    11  1/11/2010   162002.75
1   2010    12  1/12/2010   262297.375
1   2011    1   1/01/2011   177291.25
1   2011    2   1/02/2011   154816
1   2011    3   1/03/2011   171231.125
1   2011    4   1/04/2011   217717
1   2011    5   1/05/2011   178767.75
1   2011    6   1/06/2011   180817.75
1   2011    7   1/07/2011   216927.125
1   2011    8   1/08/2011   204509.125
1   2011    9   1/09/2011   199449.5
1   2011    10  1/10/2011   243812.125
1   2011    11  1/11/2011   232135.875
1   2011    12  1/12/2011   330854.75
1   2012    1   1/01/2012   217123.875
1   2012    2   1/02/2012   200558
1   2012    3   1/03/2012   215689.5
1   2012    4   1/04/2012   245500.25
1   2012    5   1/05/2012   219687.25
1   2012    6   1/06/2012   243345.625
1   2012    7   1/07/2012   249042
1   2012    8   1/08/2012   198443.75
1   2012    9   1/09/2012   209157.375
1   2012    10  1/10/2012   234089
1   2012    11  1/11/2012   237531
1   2012    12  1/12/2012   365301.25
1   2013    1   1/01/2013   211129.375
1   2013    2   1/02/2013   185249.625
1   2013    3   1/03/2013   256565.625
1   2013    4   1/04/2013   183549.5
1   2013    5   1/05/2013   189698.25
1   2013    6   1/06/2013   207955.625
1   2013    7   1/07/2013   230764.125
1   2013    8   1/08/2013   212551.625
1   2013    9   1/09/2013   201329.5
1   2013    10  1/10/2013   242745.125
1   2013    11  1/11/2013   261893.375
1   2013    12  1/12/2013   418313.25
1   2014    1   1/01/2014   205532.75
1   2014    2   1/02/2014   170487.75
1   2014    3   1/03/2014   196077
1   2014    4   1/04/2014   221760.875
1   2014    5   1/05/2014   198185
1   2014    6   1/06/2014   204919.25
1   2014    7   1/07/2014   218972.75
1   2014    8   1/08/2014   221439.875
1   2014    9   1/09/2014   195888.375
1   2014    10  1/10/2014   234595.75
1   2014    11  1/11/2014   259712.875
1   2014    12  1/12/2014   355691.875
1   2015    1   1/01/2015   205156.25
1   2015    2   1/02/2015   185358.875
1   2015    3   1/03/2015   218555.75
1   2015    4   1/04/2015   204233.625
1   2015    5   1/05/2015   212160.625
1   2015    6   1/06/2015   207217.25
1   2015    7   1/07/2015   225723.75
1   2015    8   1/08/2015   205902.625
1   2015    9   1/09/2015   196940.625
1   2015    10  1/10/2015   250916
1   2015    11  1/11/2015   236835.125
1   2015    12  1/12/2015   358327.625
2   2010    1   1/01/2010   227175.875
2   2010    2   1/02/2010   205042
2   2010    3   1/03/2010   239206.375
2   2010    4   1/04/2010   212059.875
2   2010    5   1/05/2010   232789
2   2010    6   1/06/2010   247876.125
2   2010    7   1/07/2010   278557
2   2010    8   1/08/2010   270410.125
2   2010    9   1/09/2010   251060.375
2   2010    10  1/10/2010   302738.625
2   2010    11  1/11/2010   266869.75
2   2010    12  1/12/2010   272978.75
2   2011    1   1/01/2011   238614.5
2   2011    2   1/02/2011   224240.375
2   2011    3   1/03/2011   245457.375
2   2011    4   1/04/2011   238583.5
2   2011    5   1/05/2011   252392.75
2   2011    6   1/06/2011   256749.5
2   2011    7   1/07/2011   264736.125
2   2011    8   1/08/2011   256414
2   2011    9   1/09/2011   242335.125
2   2011    10  1/10/2011   305224.75
2   2011    11  1/11/2011   289199.875
2   2011    12  1/12/2011   281807.75
2   2012    1   1/01/2012   244886.125
2   2012    2   1/02/2012   232062.375
2   2012    3   1/03/2012   264991.75
2   2012    4   1/04/2012   232750.5
2   2012    5   1/05/2012   248498.375
2   2012    6   1/06/2012   264290.875
2   2012    7   1/07/2012   272689.75
2   2012    8   1/08/2012   260441.25
2   2012    9   1/09/2012   251852.375
2   2012    10  1/10/2012   305929.625
2   2012    11  1/11/2012   276711.625
2   2012    12  1/12/2012   278672.875
2   2013    1   1/01/2013   242613.875
2   2013    2   1/02/2013   227575.75
2   2013    3   1/03/2013   250318.875
2   2013    4   1/04/2013   250150.375
2   2013    5   1/05/2013   258467.25
2   2013    6   1/06/2013   261359.25
2   2013    7   1/07/2013   279113.75
2   2013    8   1/08/2013   258699
2   2013    9   1/09/2013   244841.375
2   2013    10  1/10/2013   308197.25
2   2013    11  1/11/2013   284195.5
2   2013    12  1/12/2013   287718.75
2   2014    1   1/01/2014   239510.375
2   2014    2   1/02/2014   216338.125
2   2014    3   1/03/2014   245626.75
2   2014    4   1/04/2014   230619.875
2   2014    5   1/05/2014   251758.875
2   2014    6   1/06/2014   254946.75
2   2014    7   1/07/2014   276268.75
2   2014    8   1/08/2014   266151.75
2   2014    9   1/09/2014   245859.375
2   2014    10  1/10/2014   317797.5
2   2014    11  1/11/2014   283786.625
2   2014    12  1/12/2014   289767.875
2   2015    1   1/01/2015   244008
2   2015    2   1/02/2015   228638
2   2015    3   1/03/2015   260056
2   2015    4   1/04/2015   232560.875
2   2015    5   1/05/2015   252642.125
2   2015    6   1/06/2015   249018.5
2   2015    7   1/07/2015   278113.125
2   2015    8   1/08/2015   255851
2   2015    9   1/09/2015   263046.625
2   2015    10  1/10/2015   344240.75
2   2015    11  1/11/2015   295486.125
2   2015    12  1/12/2015   293499.375
</code></pre>

<p>I only included two groups in the sample. I would like to use a function like one of the apply (tapply, lapply, sapply, etc.) that can run an AUTO.ARIMA model per group. Then I would like to obtain the forecast for each group for x number of months and also if I could visualize the model coefficients.</p>
"
"0.0835269069584557","0.0576629931704892","235408","<p>How to change the iteration number in principle function?</p>

<p>My data has 580000 rows and cannot be separated in PCA analysis.</p>

<pre><code>    &gt; pca_rotated &lt;- psych::principal(na.omit(vv1), rotate=""varimax"", nfactors=5, scores=F)
    There were 32 warnings (use warnings() to see them)
    &gt; warnings()
    Warning messages:
    1: In cor.smooth(r) : Matrix was not positive definite, smoothing was done
    2: In pchisq(chi.sq.statistic, df, ncp = lam) :
  pnchisq(x=8.99011e+06, ..): not converged in 1000000 iter.
    3: In pchisq(chi.sq.statistic, df, ncp = lam) :
  pnchisq(x=8.99011e+06, ..): not converged in 1000000 iter.
    4: In pchisq(chi.sq.statistic, df, ncp = lam) :
  pnchisq(x=8.99011e+06, ..): not converged in 1000000 iter.
</code></pre>
"
