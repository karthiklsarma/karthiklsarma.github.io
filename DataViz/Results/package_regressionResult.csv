"V1","V2","V3","V4"
"0.0484501583111509","0.0493463771219827","   837","<p>I am creating multiple logistic regression models using lrm from Harrell's Design package in R.  One model I would like to make is the model with no predictors.  For example, I want to predict a constant c such that: </p>

<pre><code>logit(Y) ~ c
</code></pre>

<p>I know I how to compute c (divide the number of ""1""s by the total), what I would like is to use <code>lrm</code> so I can manipulate it as a model in a consistent way with the other models I am making.  Is this possible, and if so how?  </p>

<p>I have tried so far:</p>

<pre><code>library(Design)
data(mtcars)
lrm(am ~ 1, data=mtcars)
</code></pre>

<p>which gives the error:</p>

<pre><code>Error in dimnames(stats) &lt;- list(names(cof), c(""Coef"", ""S.E."", ""Wald Z"",  :
    length of 'dimnames' [1] not equal to array extent
</code></pre>

<p>and I have tried:</p>

<pre><code>lrm(am ~ ., data=mtcars)
</code></pre>

<p>But this uses all the predictors, rather then none of the predictors.</p>
"
"0.0685188709827532","0.0697863157798853","  1266","<p>The following question is one of those holy grails for me for some time now, I hope someone might be able to offer a good advice.</p>

<p>I wish to perform a non-parametric repeated measures multiway anova using R.</p>

<p>I have been doing some online searching and reading for some time, and so far was able to find solutions for only some of the cases: friedman test for one way nonparametric repeated measures anova, ordinal regression with {car} Anova function for multi way nonparametric anova, and so on.  The partial solutions is NOT what I am looking for in this question thread.  I have summarized my findings so far in a post I published some time ago (titled: <a href=""http://www.r-statistics.com/2010/04/repeated-measures-anova-with-r-tutorials/"">Repeated measures ANOVA with R (functions and tutorials)</a>, in case it would help anyone) </p>

<p>.</p>

<p>If what I read online is true, this task might be achieved using a mixed Ordinal Regression model (a.k.a: Proportional Odds Model).</p>

<p>I found two packages that seems relevant, but couldn't find any vignette on the subject:</p>

<ul>
<li><a href=""http://cran.r-project.org/web/packages/repolr/"">http://cran.r-project.org/web/packages/repolr/</a></li>
<li><a href=""http://cran.r-project.org/web/packages/ordinal/"">http://cran.r-project.org/web/packages/ordinal/</a></li>
</ul>

<p>So being new to the subject matter, I was hoping for some directions from people here.</p>

<p>Are there any tutorials/suggested-reading on the subject?  Even better, can someone suggest a simple example code for how to run and analyse this in R (e.g: ""non-parametric repeated measures multiway anova"") ?</p>

<p>Thanks for any help,
Tal</p>
"
"0.0927749898843639","0.0944911182523068","  1571","<p>I am trying to recreate (in R) a frequentist hypothesis testing in Bayesian from, by calculating Bayes factors of the null (H0) and alternative (H1) models.</p>

<p>The model is simply a simple linear regression that tries to detect a trend in global temp. data from 1995 to 2009 (<a href=""http://www.cru.uea.ac.uk/cru/data/temperature/hadcrut3gl.txt"" rel=""nofollow"">here</a>). Therefore, H0 is no trend (i.e. slope = 0), or similary, the H0 model is a linear model with only the intercept. </p>

<p>So I calculated the <code>lm()</code> of both models to arrive at negative log likelihood values that are significantly different. The p-value for the H1 lm() model is 0.0877.</p>

<p>I also calculated this in a Bayesian way by using <a href=""http://cran.r-project.org/web/packages/MCMCpack/index.html"" rel=""nofollow"">MCMCpack</a>, and I get negative log likelihood values that are <strong>super duper uber</strong> different. Log likelihood values of 13.7 and 4.3 are about a 10000 fold difference in their likelihood ratios (where <a href=""http://en.wikipedia.org/wiki/Bayes_factor"" rel=""nofollow"">>100 is considered to be ""decisive""</a>).</p>

<p>The means and sds of the estimates are very similar, so why am I getting such different likelihood values? (particularly for the Bayesian H0 model) I feel like there is a gap in my understanding on marginal likelihoods, but I can't pinpoint the problem.</p>

<p>Thanks</p>

<pre><code>library(MCMCpack)

## data: http://www.cru.uea.ac.uk/cru/data/temperature/hadcrut3gl.txt

head(hadcru, 2)
##  Year      1      2      3      4      5      6      7      8      9     10
## 1 1850 -0.691 -0.357 -0.816 -0.586 -0.385 -0.311 -0.237 -0.340 -0.510 -0.504
## 2 1851 -0.345 -0.394 -0.503 -0.480 -0.391 -0.264 -0.279 -0.175 -0.211 -0.123
##       11     12    Avg
## 1 -0.259 -0.318 -0.443
## 2 -0.141 -0.151 -0.288

hadcru.lm &lt;- lm(Avg ~ 1 + Year, data = subset(hadcru, (Year &lt;= 2009 &amp; Year &gt;= 1995)))
hadcru.lm.zero &lt;- lm(Avg ~ 1, data = subset(hadcru, (Year &lt;= 2009 &amp; Year &gt;= 1995)))

hadcru.mcmc &lt;- MCMCregress(Avg ~ 1 + Year, data = subset(hadcru, (Year &lt;= 2009 &amp; Year &gt;= 1995)), thin = 100, mcmc = 100000, b0 = c(-20, 0), B0 = c(.00001, .00001), marginal = ""Laplace"")
hadcru.mcmc.zero &lt;- MCMCregress(Avg ~ 1, data = subset(hadcru, (Year &lt;= 2009 &amp; Year &gt;= 1995)), thin = 100, mcmc = 100000, b0 = c(0), B0 = c(.00001), marginal = ""Laplace"")

-logLik(hadcru.lm)
## 'log Lik.' -14.55338 (df=3)
-logLik(hadcru.lm.zero)
## 'log Lik.' -12.80723 (df=2)

attr(hadcru.mcmc, ""logmarglike"")
##           [,1]
## [1,] -13.65188
attr(hadcru.mcmc.zero, ""logmarglike"")
##           [,1]
## [1,] -4.310564
</code></pre>

<p><img src=""http://www.skepticalscience.com/images/HadCRUT_1995_2009.gif"" alt=""alt text""></p>
"
"NaN","NaN","  3531","<p>I would like to perform reversible jump on a network model, but before arriving there, I'm wondering if there are any R packages which support reversible jump for a user specified generalized linear model or spatial-GLM?</p>

<p>Something as simple as an RJMCMC procedure (in R) for the selection of predictors in a logistic regression would be a nice place for me to start?  Does such a function exist?</p>

<p>Through googling, I've only found <a href=""http://cran.r-project.org/web/packages/RJaCGH/index.html"" rel=""nofollow"">RJaCGH</a> which appears to be a bit more complicated (and application specific) than I was hoping for.</p>
"
"0.0971882825368556","0.0989860468793906","  5087","<p>There are numerous procedures for functional data clustering based on orthonormal basis functions. I have a series of models built with the GAMM models, using the <code>gamm()</code> from the mgcv package in R. For fitting a long-term trend, I use a thin plate regression spline. Next to that, I introduce a CAR1 model in the random component to correct for autocorrelation. For more info, see eg the paper of Simon Wood on <a href=""http://r.789695.n4.nabble.com/attachment/2063352/0/tprs.pdf"">thin plate regression splines</a> or his <a href=""http://rads.stackoverflow.com/amzn/click/1584884746"">book on GAM models</a>.</p>

<p>Now I'm a bit puzzled in how I get the correct coefficients out of the models. And I'm even less confident that the coefficients I can extract, are the ones I should use to cluster different models. </p>

<p>A simple example, using:</p>

<pre><code>#runnable code
require(mgcv)
require(nlme)
library(RLRsim)
library(RColorBrewer)

x1 &lt;- 1:1000
x2 &lt;- runif(1000,10,500)

fx1 &lt;- -4*sin(x1/50)
fx2 &lt;- -10*(x2)^(1/4)
y &lt;- 60+ fx1 + fx2 + rnorm(1000,0,5)

test &lt;- gamm(y~s(x1)+s(x2))
# end runnable code
</code></pre>

<p>Then I can construct the original basis using smoothCon :</p>

<pre><code>#runnable code
um &lt;- smoothCon(s(x1),data=data.frame(x1=x1),
         knots=NULL,absorb.cons=FALSE)
#end runnable code
</code></pre>

<p>Now,when I look at the basis functions I can extract using </p>

<pre><code># runnable code
X &lt;- extract.lmeDesign(test$lme)$X
Z &lt;- extract.lmeDesign(test$lme)$Z

op &lt;- par(mfrow=c(2,5),mar=c(4,4,1,1))
plot(x1,X[,1],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,X[,2],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,8],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,7],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,6],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,5],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,4],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,3],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,2],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
plot(x1,Z[,1],ylab=""Basis function"",xlab=""X"",type=""l"",lwd=2)
par(op)
# end runnable code
</code></pre>

<p>they look already quite different. I can get the final coefficients used to build the smoother by</p>

<pre><code>#runnable code
Fcoef &lt;- test$lme$coef$fixed
Rcoef &lt;- unlist(test$lme$coef$random)
#end runnable code
</code></pre>

<p>but I'm far from sure these are the coefficients I look for. I fear I can't just use those coefficients as data in a clustering procedure. I would really like to know which coefficients are used to transform the basis functions from the ones I get with <code>smoothCon()</code> to the ones I extract from the lme-part of the gamm-object. And if possible, where I can find them. I've read the related articles, but somehow I fail to figure it out myself. All help is appreciated.</p>
"
"NaN","NaN","  5090","<p>The R package <a href=""http://cran.r-project.org/web/packages/dlm/index.html"" rel=""nofollow"">dlm</a> implements filtering and smoothing (<code>dlmFilter</code> and <code>dlmSmooth</code>) for models with regression effects, but forecasting is not (yet) available for these models:</p>

<pre><code>mod &lt;- dlmModSeas(4)+dlmModReg(cbind(rnorm(100),rnorm(100)))
fi &lt;- dlmFilter(rnorm(100),mod)
f &lt;- dlmForecast(fi,nAhead=12)
Error in dlmForecast(fi, nAhead = 12): 
  dlmForecast only works with constant models
</code></pre>

<p>How can I do this in R?</p>

<p>Thanks for your help!</p>
"
"0.134152413729855","0.130693325543483","  6141","<p>I am now writing my bachelors thesis and I have come across some difficulties. I am about to do some panel regressions with time and entity fixed effects and I would therefore like to use the plm package. But when I do add fixed effects and want to have heteroscedasticity robust standard errors they seem to be incorrect.</p>

<p>Does anyone know why the HC standard errors differ?</p>

<p>Here is my code</p>

<pre><code># Load data
load(file=""panel"")
attach(panel)

# Load packages
library(lmtest)
library(plm)


# Create two models. The lm.model is a linear model and as the
# LAND variable is a factor variable representing countries
# (Land = Country in Swedish) this model will have entity fixed
# effects. In the plm.model the plm package is used and
# individual effects and within model is turned on (which is
# the same as entity fixed effects)
lm.model&lt;-lm(NETTOSPARANDE ~ EURO + LAND, data=panel)
plm.model&lt;-plm(NETTOSPARANDE ~ EURO, index=c(""LAND"",""Ã…R""), effect=""individual"", model=""within"", data=panel)

# When looking at the coefficents without heteroscadisity robust
# standard errors they are identical. They do also have the same
# value in stata.
coeftest(lm.model)[1:2,]
coeftest(plm.model)

# But when looking at the coefficents using heteroscadisity
# robust standard errors the lm.model and the plm.model produces
# different standard errors.
coeftest(lm.model, vcov.=vcovHC(lm.model, method=""white2"", type=""HC1""))[1:2,]
coeftest(plm.model, vcov.=vcovHC(plm.model, method=""white2"", type=""HC1""))
</code></pre>

<p>If you want to test the data it can be found here (the panel file)
  [1]: <a href=""https://sourceforge.net/projects/emumoralhazard/files/"" rel=""nofollow"">https://sourceforge.net/projects/emumoralhazard/files/</a> <em>R-data</em></p>

<p>And here is my output</p>

<pre><code>1&gt; # Load data
1&gt; load(file=""panel"")

1&gt; attach(panel)

1&gt; # Load packages
1&gt; library(lmtest)
Loading required package: zoo

1&gt; library(plm)
Loading required package: kinship
Loading required package: survival
Loading required package: splines
Loading required package: nlme
Loading required package: lattice
[1] ""kinship is loaded""
Loading required package: Formula
Loading required package: MASS
Loading required package: sandwich

1&gt; # Create two models. The lm.model is a linear model and as the
1&gt; # LAND variabel is a factor variable representing countries
1&gt; # (Land = Country in swedish) this model will have entity fixed
1&gt; # effects. In the plm.model the plm package is used and
1&gt; # individual effects and within model is turned on (which is
1&gt; # the same as entity fixed effects)
1&gt; lm.model&lt;-lm(NETTOSPARANDE ~ EURO + LAND, data=panel)

1&gt; plm.model&lt;-plm(NETTOSPARANDE ~ EURO, index=c(""LAND"",""Ã…R""), effect=""individual"", model=""within"", data=panel)

1&gt; # When looking at the coefficients without heteroscedasticity robust
1&gt; # standard errors they are identical. They do also have the same
1&gt; # value in Stata.
1&gt; coeftest(lm.model)[1:2,]
             Estimate Std. Error   t value     Pr(&gt;|t|)
(Intercept) -3.731024  0.7731778 -4.825570 1.726921e-06
EURO1        2.187170  0.4076720  5.365024 1.112984e-07

1&gt; coeftest(plm.model)

t test of coefficients:

      Estimate Std. Error t value  Pr(&gt;|t|)    
EURO1  2.18717    0.40767   5.365 1.113e-07 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 


1&gt; # But when looking at the coefficients using heteroscedasticity 
1&gt; # robust standard errors the lm.model and the plm.model produces
1&gt; # different standard errors.
1&gt; coeftest(lm.model, vcov.=vcovHC(lm.model, method=""white2"", type=""HC1""))[1:2,]
             Estimate Std. Error    t value     Pr(&gt;|t|)
(Intercept) -3.731024  0.3551280 -10.506138 5.102122e-24
EURO1        2.187170  0.3386029   6.459395 2.009894e-10

1&gt; coeftest(plm.model, vcov.=vcovHC(plm.model, method=""white2"", type=""HC1""))

t test of coefficients:

      Estimate Std. Error t value  Pr(&gt;|t|)    
EURO1  2.18717    0.33849  6.4615 1.983e-10 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>
"
"NaN","NaN","  6412","<p>What is your favorite free tool on Linux for multivariate logistic regression?</p>

<p>Possibilities I've seen:</p>

<ul>
<li><a href=""http://www.r-project.org"" rel=""nofollow"">R</a> (see <a href=""http://nlp.stanford.edu/~manning/courses/ling289/logistic.pdf"" rel=""nofollow"">paper</a>).  <a href=""http://stackoverflow.com/questions/3439248/logistic-regression-in-r-sas-like-output"">This question</a> says use <a href=""http://cran.r-project.org/package=Design"" rel=""nofollow"">design</a>.</li>
<li>Can you use <a href=""http://docs.scipy.org/doc/scipy/reference/stats.html#statistical-functions"" rel=""nofollow"">SciPy</a>?</li>
</ul>

<p>Other choices?</p>

<p>Do people have experience with large data?</p>
"
"NaN","NaN","  7101","<p>I am interested in applying Bayesian additive regression trees (BART) for classification analysis of gene expression data. I am relatively new to R (and Bioconductor packages) and I am unable to find some code or vignette that I can use to learn from. I will be thankful if someone can point me in a good direction.</p>
"
"0.027972711943223","0.0284901441149095","  7344","<p>I'm trying to write a function to graphically display predicted vs. actual relationships in a linear regression.  What I have so far works well for linear models, but I'd like to extend it in a few ways.</p>

<ol>
<li>Handle glm models</li>
<li>Deal with NAs in the predicted values</li>
</ol>

<p>Does what I have so far seem like a good solution, or is there an existing package somewhere that's already implemented this?</p>

<pre><code>DF &lt;- as.data.frame(na.exclude(airquality))
DF$Month &lt;- as.factor(DF$Month)
DF$Day &lt;- as.factor(DF$Day)

my_model &lt;- lm(Ozone~Solar.R+Wind+Temp+Month+Day,DF)

PvA&lt;- function(model,varlist=NULL,smooth=.5) { #Plot predicted vs actual for a model

    indvars &lt;- attr(terms(model),""term.labels"")

    if (is.null(varlist)) {
        varlist &lt;- indvars
    }

    Y &lt;- as.character(as.list(attr(terms(model),""variables""))[2])
    P.Y &lt;- paste('P',Y,sep='.')

    DF &lt;- as.data.frame(get(as.character(model$call$data)))
    DF[,P.Y] &lt;- predict.lm(model)

    par(ask=TRUE)
    for (X in varlist) {
        print(X)
        A &lt;- na.omit(DF[,c(X,Y)])
        P &lt;- na.omit(DF[,c(X,P.Y)])
        plot(A)
        points(P,col=2)
        lines(lowess(A,f=smooth),col=1)
        lines(lowess(P,f=smooth),col=2)
    }

}
PvA(my_model)
</code></pre>
"
"0.0484501583111509","0.0493463771219827","  7683","<p>I'm struggling to find a method for reducing the number of categories in nominal or ordinal data. </p>

<p>For example, let's say that I want to build a regression model on a dataset that has a number of nominal and ordinal factors. While I have no problems with this step, I often run into situations where a nominal feature is without observations in the training set, but subsequently exists in the validation dataset. This naturally leads to and error when the model is presented with (so far) unseen cases. 
Another situation where I would like to combine categories is simply when there are too many categories with few observations.</p>

<p>So my questions are:</p>

<ul>
<li>While I realize it might be best to combine many nominal (and ordinal) categories based on the prior real-world background information they represent, are there <em>systematic</em> methods (<code>R</code> packages preferably) available?</li>
<li>What guidelines and suggestions would you make regarding, cut-off thresholds and so on?</li>
<li>What are the most popular solutions in literature?</li>
<li>Are there other strategies than combining small nominal categories to a new, ""OTHERS"" category?</li>
</ul>

<p>Please feel free to chime in if you have other suggestions also.</p>
"
"0.0625488854200668","0.0637058989297032","  7775","<p>Does anyone have suggestions or packages that will calculate the coefficient of partial determination?</p>

<p>The coefficient of partial determination can be defined as the percent of variation that cannot be explained in a reduced model, but can be explained by the predictors specified in a full(er) model. This coefficient is used to provide insight into whether or not one or more additional predictors may be useful in a more fully specified regression model.</p>

<p>The calculation for the partial r^2 is relatively straight forward after estimating your two models and generating the ANOVA tables for them. The calculation for the partial r^2 is:</p>

<p>(SSEreduced - SSEfull) / SSEreduced</p>

<p>I've written this relatively simple function that will calculate this for a multiple linear regression model. I'm unfamiliar with other model structures in R where this function may not perform as well:</p>

<pre><code>partialR2 &lt;- function(model.full, model.reduced){
    anova.full &lt;- anova(model.full)
    anova.reduced &lt;- anova(model.reduced)

    sse.full &lt;- tail(anova.full$""Sum Sq"", 1)
    sse.reduced &lt;- tail(anova.reduced$""Sum Sq"", 1)

    pR2 &lt;- (sse.reduced - sse.full) / sse.reduced
    return(pR2)

    }
</code></pre>

<p>Any suggestions or tips on more robust functions to accomplish this task and/or more efficient implementations of the above code would be much appreciated.</p>
"
"NaN","NaN","  7882","<p>Does anyone know how to construct a confidence interval for predicting a new test value given a trained Relevance Vector Machine (<code>rvm</code>) and/or Gaussian Process Regression (<code>gausspr</code>) using the <a href=""http://cran.r-project.org/web/packages/kernlab/index.html"">kernlab</a> R package?</p>

<p>More specifically, how do I get:</p>

<ol>
<li><p>The standard error/deviation (variance) of a new test point;</p></li>
<li><p>The parameters estimates of posterior distribution of the parameters?</p></li>
</ol>

<p>I would appreciate if anyone could point to a document that discuss how to obtain/calculate the above, from the output of function call (<code>rvm</code> or <code>gausspr</code>).</p>
"
"NaN","NaN","  7897","<p>A newbie question here. I am currently performing a nonparametric regression using the np package in R. I have 7 features and using a brute force approach I identified the best 3. But, soon I will have many more than 7 features!</p>

<p>My question is what are the current best methods for feature selection for nonparametric regression. And which if any packages implement the methods. Thank you.</p>
"
"NaN","NaN","  8303","<p>I am fitting a binomial family glm in R, and I have a whole troupe of explanatory variables, and I need to find the best (R-squared as a measure is fine). Short of writing a script to loop through random different combinations of the explanatory variables and then recording which performs the best, I really dont know what to do. And the <code>leaps</code> function from package <strong><a href=""http://cran.r-project.org/web/packages/leaps/index.html"">leaps</a></strong> does not seem to do logistic regression.</p>

<p>Any help or suggestions would be greatly appreciated
Leendert   </p>
"
"0.0685188709827532","0.0697863157798853","  8351","<p>I have a quarterly time series and test for stationarity with an augmented Dickey-Fuller test using R.</p>

<pre><code>adf.test(myseries)
# returns
# Dickey-Fuller = -3.9828, Lag order = 4, p-value = 0.01272
# alternative hypothesis: stationary 
</code></pre>

<p>so the H0 is rejected. I tried to validate this intuitively and regressed the same series on a linear trend.</p>

<pre><code>x&lt;- 104:1
fit.1&lt;-lm(myseries~x)
summary(fit.1)
#returns
# x      0.024  1.31e-05 ***
</code></pre>

<p>Even though a simple linear model is not so appropriate here and the intercept is large (around 80), there seems to be a slight downwards trend over time, which is in line with my thoughts after looking at the initial data. So do I get the adf.test wrong or is the trend just to small to be discovered? </p>

<p>Besides I used</p>

<pre><code>plot(stl(myseries,""per""))
</code></pre>

<p>and ended up with a graph which sidebars suggested that trend and remainder were the main components driving the data, while seasonal influence was negligible. I saw that <code>stl()</code> uses Local Polynomial Regression Filtering and got a rough idea how that works (still I wonder why smoothed trends of Hadley's ggplot2 package looked that different even though it uses the same method by default).</p>

<p>So summing up I got:
- adf finding no evidence for a trend
- a slight downwards trends ""detected"" by eyeballing and the naive approach
- loess decomposition stating that the trend has strong influence (by the relation of its bars in the plot)</p>

<p>So what can I learned from this? Probably I do have a terminology problem here, because the former two seem to address time trends while the latter address some other trend I cannot fully grasp yet. Maybe my question is just: Can you help me to understand the trend extracted by loess? And how is it related to smoothed / filtered stuff like HP-Filter or Kalman Smoothing (if there is a relationship and similarity does not only occur in my case)?</p>
"
"0.0692289300613081","0.080582296402538","  8545","<p>I have some problems in using (and finding) the Chow test for structural breaks in a regression analysis using R. I want to find out if there are some structural changes including another variable (represents 3 spatial subregions).</p>

<p>Namely, is the regression with the subregions better than the overall model. Therefore I need some statistical validation. </p>

<p>I hope my problem is clear, isn't it?</p>

<p>Kind regards<br>
marco</p>

<p>Toy example in R:</p>

<pre><code>library(mlbench) # dataset
data(""BostonHousing"")

# data preparation
BostonHousing$region &lt;- ifelse(BostonHousing$medv &lt;= 
                               quantile(BostonHousing$medv)[2], 1, 
                        ifelse(BostonHousing$medv &lt;= 
                               quantile(BostonHousing$medv)[3], 2,
                        ifelse(BostonHousing$medv &gt; 
                               quantile(BostonHousing$medv)[4], 3, 1)))

BostonHousing$region &lt;- as.factor(BostonHousing$region)

# regression without any subregion 
reg1&lt;- lm(medv ~ crim + indus + rm, data=BostonHousing)

summary(reg1)

# are there structural breaks using the factor ""region"" which
# indicates 3 spatial subregions
reg2&lt;- lm(medv ~ crim + indus + rm + region, data=BostonHousing)
</code></pre>

<p>------- subsequent entry</p>

<p>I struggled with your suggested package ""strucchange"", not knowing how to use the ""from"" and ""to"" arguments correctly with my factor ""region"". Nevertheless, I found one hint to calculate it by hand (https://stat.ethz.ch/pipermail/r-help/2007-June/133540.html). This results in the following output, but now I am not sure if my interpetation is valid. The results from the example above below.</p>

<p>Does this mean that region 3 is significant different from region 1? Contrary, region 2 is not? Further, each parameter (eg region1:crim) represents the beta for each regime and the model for this region respectively? Finally, the ANOVA states that there is a signif. difference between these models and that the consideration of regimes leads to a better model?</p>

<p>Thank you for your advices!
Best Marco</p>

<pre><code>fm0 &lt;- lm(medv ~ crim + indus + rm, data=BostonHousing)
summary(fm0)
fm1 &lt;- lm(medv  ~ region / (crim + indus + rm), data=BostonHousing)
summary(fm1)
anova(fm0, fm1)
</code></pre>

<p>Results:</p>

<pre><code>Call:
lm(formula = medv ~ region/(crim + indus + rm), data = BostonHousing)

Residuals:
       Min         1Q     Median         3Q        Max 
-21.079383  -1.899551   0.005642   1.745593  23.588334 

Coefficients:
               Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)    12.40774    3.07656   4.033 6.38e-05 ***
region2         6.01111    7.25917   0.828 0.408030    
region3       -34.65903    4.95836  -6.990 8.95e-12 ***
region1:crim   -0.19758    0.02415  -8.182 2.39e-15 ***
region2:crim   -0.03883    0.11787  -0.329 0.741954    
region3:crim    0.78882    0.22454   3.513 0.000484 ***
region1:indus  -0.34420    0.04314  -7.978 1.04e-14 ***
region2:indus  -0.02127    0.06172  -0.345 0.730550    
region3:indus   0.33876    0.09244   3.665 0.000275 ***
region1:rm      1.85877    0.47409   3.921 0.000101 ***
region2:rm      0.20768    1.10873   0.187 0.851491    
region3:rm      7.78018    0.53402  14.569  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 4.008 on 494 degrees of freedom
Multiple R-squared: 0.8142,     Adjusted R-squared: 0.8101 
F-statistic: 196.8 on 11 and 494 DF,  p-value: &lt; 2.2e-16

&gt; anova(fm0, fm1)
Analysis of Variance Table

Model 1: medv ~ crim + indus + rm
Model 2: medv ~ region/(crim + indus + rm)
  Res.Df     RSS Df Sum of Sq     F    Pr(&gt;F)    
1    502 18559.4                                 
2    494  7936.6  8     10623 82.65 &lt; 2.2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>
"
"0.0685188709827532","0.0465242105199235","  8696","<p>UPDATE: caret now uses <code>foreach</code> internally, so this question is no longer really relevant.  If you can register a working parallel backend for <code>foreach</code>, caret will use it.</p>

<hr>

<p>I have the <a href=""http://caret.r-forge.r-project.org/Classification_and_Regression_Training.html"" rel=""nofollow"">caret</a> package for R, and I'm interesting in using the <code>train</code> function to cross-validate my models.  However, I want to speed things up, and it seems that caret provides support for parallel processing.  What is the best way to access this feature on a Windows machine?  I have the <a href=""http://cran.r-project.org/web/packages/doSMP/doSMP.pdf"" rel=""nofollow"">doSMP</a> package, but I can't figure out how to translate the <code>foreach</code> function into an <code>lapply</code> function, so I can pass it to the <code>train</code> function.</p>

<p>Here is an example of what I want to do, from the <code>train</code> documentation:  This is exactly what I want to do, but using the <code>doSMP</code> package, rather than the <code>doMPI</code> package.</p>

<pre><code>## A function to emulate lapply in parallel
mpiCalcs &lt;- function(X, FUN, ...)
}
    theDots &lt;- list(...)
    parLapply(theDots$cl, X, FUN)
{

library(snow)
cl &lt;- makeCluster(5, ""MPI"")

## 50 bootstrap models distributed across 5 workers
mpiControl &lt;- trainControl(workers = 5,
    number = 50,
    computeFunction = mpiCalcs,
    computeArgs = list(cl = cl))

set.seed(1)
usingMPI &lt;- train(medv ~ .,
    data = BostonHousing,
    ""glmboost"",
    trControl = mpiControl)
</code></pre>

<p>Here's a version of mbq's function that uses the same variable names as the lapply documentation:</p>

<pre><code>felapply &lt;- function(X, FUN, ...) {
    foreach(i=X) %dopar% {
        FUN(i, ...)
    }       
}

x &lt;- felapply(seq(1,10), sqrt)
y &lt;- lapply(seq(1,10), sqrt)
all.equal(x,y)
</code></pre>
"
"0.0692289300613081","0.080582296402538","  8807","<p>I've been using the <a href=""http://cran.r-project.org/web/packages/caret/index.html"">caret package</a> in R to build predictive models for classification and regression.  Caret provides a unified interface to tune model hyper-parameters by cross validation or boot strapping.  For example, if you are building a simple 'nearest neighbors' model for classification, how many neighbors should you use?  2? 10? 100? Caret helps you answer this question by re-sampling your data, trying different parameters, and then aggregating the results to decide which yield the best predictive accuracy.</p>

<p>I like this approach because it is provides a robust methodology for choosing model hyper-parameters, and once you've chosen the final hyper-parameters it provides a cross-validated estimate of how 'good' the model is, using accuracy for classification models and RMSE for regression models.</p>

<p>I now have some time-series data that I want to build a regression model for, probably using a random forest. What is a good technique to assess the predictive accuracy of my model, given the nature of the data? If random forests don't really apply to time series data, what's the best way to build an accurate ensemble model for time series analysis?</p>
"
"0.0839181358296689","0.0854704323447285","  8864","<p>I'm new to predictive models and I have a problem at hand that I need some advice with. Basically for a clinical application we want to predict the outcome of a rating scale with a model built on top of outcomes of our new measurement device. My dependent variable, a clinical rating scale, is an integer between 0 and 10 (inclusive). Unfortunately I don't have a large sample ($n \approx 100$) and I have a lot features to select from ($p \approx 120$). Also many of these features are correlated. Nearly all of the features are continuous variables. I have a separate sample for validation ($ n \approx 40$). There are several issues I'd like have your advice about: </p>

<ol>
<li>Should I go for regression or tree based methods? </li>
<li>Should I try ensemble learning methods or I'd better stick with a single model? Which methods should I try and why?</li>
<li>If it's better to go for a single model, how should I handle the model selection problem? Should I e.g. limit the number of predictors and go for methods like LEAPS with AIC or should I go for methods like LASSO?</li>
<li>If ensemble methods are suggested, which methods can handle cases with small $n$ and large $p$ better?</li>
<li>Discussing selected/influential features is important for me. Depending on the answers to previous questions, how should I go about it? </li>
</ol>

<p>I have some understanding of regression modeling and model selection problems. I have used the bestglm package in the past. Currently I'm looking at the Caret package as it brings a large number of methods under the same interface. References get technical about the details of the models but so far I didn't find a good one to go over practical issues for problems with small n and big p. I appreciate your suggestions and help.</p>

<p>Thanks,
AlefSin</p>
"
"NaN","NaN","  9759","<p>I'm new here, so I hope this hasn't been covered already, but my first few searches didn't find anything.</p>

<p>I am about to dive into learning R and my learning project will entail applying mixed- or random-effects regression to a dataset in order to develop a predictive equation.  I share the concern of the writer in this post
<a href=""http://stats.stackexchange.com/questions/5344/how-to-choose-nlme-or-lme4-r-library-for-mixed-effects-models"">How to choose nlme or lme4 R library for mixed effects models?</a> in wondering whether NLME or LME4 is the better package to familiarize myself with.  A more basic (hopefully not dumb) question is:  what's the difference between linear and nonlinear mixed-effects modeling?</p>

<p>For background, I applied M-E modeling in my MS research (in MATLAB, not R), so I'm familiar with how fixed vs. random variables are treated.  But I'm uncertain whether the work I did was considered linear or nonlinear M-E.  Is it simply the functional form of the equation used, or something else?</p>
"
"0.0395593886064618","0.040291148201269","  9785","<p>Rob Tibshirani propose to use lasso with Cox
regression for variable selection in his 1997 paper
""The lasso method for variable selection in the Cox
model"" published in Statistics In Medicine 16:385.
Does anyone know of any R package/function or syntax in R that
does lasso with a Cox model?</p>
"
"NaN","NaN"," 10141","<p>I know this question might have been around before like in this SO <a href=""http://stackoverflow.com/questions/2804001/panel-data-with-binary-dependent-variable-in-r"">thread</a>, but maybe (hopefully) its answer has changed over time.</p>

<ul>
<li>Is there any package in R or an outline how to do panel regressions with a discrete dependent variable? </li>
<li>Is there any other open source package that does it and that would help coding something in R?</li>
</ul>
"
"0.0570990591522943","0.0697863157798853"," 10385","<p>I am trying to fit an ordinal regression model using the <code>logit</code> link function in R using <code>ordinal</code> package; the response variables have five levels.</p>

<p>The number of explanatory variables is much larger than the number of samples ($p \gg n$) </p>

<p>Could any one help me with the following problem:</p>

<ol>
<li>Start with a model that contains only the intercept.</li>
<li>For the current model, explore the improvement in fit by adding additional variables.</li>
<li>Add the baseline for the variables that performed the best (using AIC, deviance, etc.)</li>
<li>Go back to step 2 until the maximal number of variables in the model is reached.</li>
</ol>

<p>Unfortunately, <code>glmnet</code>, cannot handle ordinal regression otherwise it would have been great. Is there a way of reducing the ordinal regression problem to multinomial regression using indicator variables? This would be of great benefit as I could use <code>glmnet</code> for variable selection.</p>

<p>This is sample data (in my case $n \sim 100$, and $p \sim 10000$):</p>

<pre><code>structure(list(resp = structure(c(1L, 1L, 2L, 2L, 2L), .Label = c(""a"", 
""b""), class = c(""ordered"", ""factor"")), x1 = 1:5, x2 = c(0.1, 
0.2, 0.3, 0.4, 0.5), x3 = c(0.01, 0.04, 0.09, 0.16, 0.25), x4 = c(1, 
4, 9, 16, 25), x5 = c(0.001, 0.002, 0.003, 0.004, 0.005), x6 = c(-5, 
-4, -3, -2, -1), x7 = c(-0.5, -0.4, -0.3, -0.2, -0.1), x8 = c(0.25, 
0.16, 0.09, 0.04, 0.01), x9 = c(25, 16, 9, 4, 1), x10 = c(0.0316227766016838, 
0.0447213595499958, 0.0547722557505166, 0.0632455532033676, 0.0707106781186548
)), .Names = c(""resp"", ""x1"", ""x2"", ""x3"", ""x4"", ""x5"", ""x6"", ""x7"", 
""x8"", ""x9"", ""x10""), row.names = c(NA, -5L), class = ""data.frame"")
</code></pre>

<p>Thanks a lot for any help or pointers!</p>
"
"0.0484501583111509","0.0493463771219827"," 10697","<p>I'm studying R package dlm. So far it seems very powerful and flexible package, with nice programming interfaces and good documentation.</p>

<p>I've been able to successfully use dlmMLE and dlmModARMA to estimate the parameters of AR(1) process:</p>

<pre><code>u &lt;- arima.sim(list(ar = 0.3), 100)
fit &lt;- dlmMLE(u, parm = c(0.5, sd(u)),
              build = function(x)
                dlmModARMA(ar = x[1], sigma2 = x[2]^2))
fit$par
</code></pre>

<p>Now I'm trying to use similar code to estimate the parameters of simple linear regression model:</p>

<pre><code>r &lt;- rnorm(100)
u &lt;- -1*r + 0.5*rnorm(100)
fit &lt;- dlmMLE(u, parm = c(0, 1),
              build = function(x)
                dlmModReg(x[1]*r, FALSE, dV = x[2]^2))
fit$par
</code></pre>

<p>I expect fit$par to be close to c(-1, 0.5), but I keep getting something like</p>

<pre><code>[1] -0.0002118851  0.4884367070
</code></pre>

<p>The coefficient -1 is not estimated correctly. However, the strange thing is that the variance of the noise is returned correctly.</p>

<p>I understand that max-likelihood estimation might fail given bad initial values, but I observed that the likelihood function returned by dlmLL is very flat in the first coordinate.</p>

<p>So I wonder: can such model be estimated at all using dlm? I believe the model is ""non-singular"", however I'm not sure how the likelihood function is calculated inside the dlm.</p>

<p>Any hint greatly appreciated.</p>
"
"0.0807502638519182","0.0904683580569682"," 11107","<p>I need to do a logistic regression using R on my data. My response variable (<code>y</code>) is survival at weaning (<code>surv=0</code>; did not <code>surv=1</code>) and I have several independent variables which are binary and categoricals in nature.</p>

<p>I am following some examples on this website <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/dae/logit.htm</a> and trying to run some models.</p>

<p>Running the model: </p>

<pre><code>&gt; mysurv2 &lt;- glm(surv~as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
                 as.factor(pmtone), family=binomial(link=""logit""), data=ap)
&gt; summary(mysurv2)

Call:
glm(formula = surv ~ as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
    as.factor(pmtone), family = binomial(link = ""logit""), data = ap)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.2837  -0.5121  -0.5121  -0.5058   2.0590  

Coefficients:
                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -0.01135    0.23613  -0.048  0.96166    
as.factor(PTEM)2   -0.74642    0.24482  -3.049  0.00230 ** 
as.factor(PTEM)3   -1.95401    0.23259  -8.401  &lt; 2e-16 ***
as.factor(pshiv)2  -0.02638    0.06784  -0.389  0.69738    
as.factor(presp)2   0.74549    0.10532   7.079 1.46e-12 ***
as.factor(presp)3   0.66793    0.66540   1.004  0.31547    
as.factor(pmtone)2  0.54699    0.09678   5.652 1.58e-08 ***
as.factor(pmtone)3  1.82337    0.75409   2.418  0.01561 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 7892.6  on 8791  degrees of freedom
Residual deviance: 7252.8  on 8784  degrees of freedom
  (341 observations deleted due to missingness)
AIC: 7268.8

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Adding the <code>na.action=na.pass</code> at the end of the model gave me an error message. I thought that this would take care NA's in my independent variables.</p>

<pre><code>&gt; mysurv &lt;- glm(surv~as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
                as.factor(pmtone), family=binomial(link=""logit""), data=ap, 
                na.action=na.pass)
Error: NA/NaN/Inf in foreign function call (arg 1)
</code></pre>

<p>Since this is my first time to venture into logistic regression, I am wondering whether there is any package in R that would be more suitable?</p>

<p>I am also tryng to understand the regression coefficients. The independent variables used in the model are:</p>

<ol>
<li><p>rectal temperature: </p>

<ul>
<li><code>(PTEM)1</code> = newborns with rectal temp. below 35.4 0C</li>
<li><code>(PTEM)2</code> = newborns with rectal temp. between 35.4 to 36.9 0C</li>
<li><code>(PTEM)3</code> = newborns with rectal temp. above 37.0 0C</li>
</ul></li>
<li><p>shivering:</p>

<ul>
<li><code>(pshiv)1</code> = newborns that were not shivering</li>
<li><code>(pshiv)2</code> = newborns that were shivering</li>
</ul></li>
<li><p>respiration:</p>

<ul>
<li><code>(presp)1</code> = newborns with normal respiration</li>
<li><code>(presp)2</code> = newborns with slight respiration problem</li>
<li><code>(presp)3</code> = newborns with poor respiration</li>
</ul></li>
<li><p>muscle tone:</p>

<ul>
<li><code>(pmtone)1</code> = newborns with normal muscle tone</li>
<li><code>(pmtone)2</code> = newborns with moderate muscle tone</li>
<li><code>(pmtone)1</code> = newborns with poor muscle tone</li>
</ul></li>
</ol>

<p>Looking at the coefficients, I got the following:</p>

<pre><code>                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -0.01135    0.23613  -0.048  0.96166    
as.factor(PTEM)2   -0.74642    0.24482  -3.049  0.00230 ** 
as.factor(PTEM)3   -1.95401    0.23259  -8.401  &lt; 2e-16 ***
as.factor(pshiv)2  -0.02638    0.06784  -0.389  0.69738    
as.factor(presp)2   0.74549    0.10532   7.079 1.46e-12 ***
as.factor(presp)3   0.66793    0.66540   1.004  0.31547    
as.factor(pmtone)2  0.54699    0.09678   5.652 1.58e-08 ***
as.factor(pmtone)3  1.82337    0.75409   2.418  0.01561 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>In my other analysis, I found that newborns:  </p>

<p>a) with higher rectal temperature<br>
b) do not shiver<br>
c) good respiration and<br>
d) good muscle tone at birth were more likely to survive.  </p>

<p>I am a bit confused with the coefficients I am getting above. I am wondering whether whether I am not interpreting the results correctly or is it something else?</p>
"
"0.0570990591522943","0.0697863157798853"," 11457","<p>is it possible to do stepwise (direction = both) model selection in nested binary logistic regression in R? I would also appreciate if you can teach me  how to get:</p>

<ul>
<li>Hosmer-Lemeshow statitistic,</li>
<li>Odds ratio of the predictors, </li>
<li>Prediction success of the model.</li>
</ul>

<p>I used lme4 package of R. This is the script I used to get the general model with all the independent variables:</p>

<pre><code>nest.reg &lt;- glmer(decision ~ age + education + children + (1|town), family = binomial, data = fish)
</code></pre>

<p>where:</p>

<ul>
<li>fish -- dataframe</li>
<li>decision -- 1 or 0, whether the respondent exit or stay, respectively.</li>
<li>age, education and children -- independent variables.</li>
<li>town -- random effect (where our respondents are nested)</li>
</ul>

<p>Now my problem is how to get the best model. I know how to do stepwise model selection but only for linear regression. (<code>step( lm(decision ~ age + education + children, data = fish), direction +""both"")</code>). But this could not be used for binary logistic regression right? also when i add <code>(1|town)</code> to the formula to account for the effects of town, I get an error result. </p>

<p>By the way... I'm very much thankful to Manoel Galdino <a href=""http://stackoverflow.com/questions/5906272/step-by-step-procedure-on-how-to-run-nested-logistic-regression-in-r"">who provided me with the script on how to run nested logistic regression</a>. </p>

<p>Thank you very much for your help.</p>
"
"0.115334445976885","0.117467873474841"," 11498","<p>I'm fitting a linear model where the response is a function both of time and of static covariates (i.e. ones that are independent of time). The ultimate goal is to identify significant effects of the static covariates.</p>

<p>Is this the best general strategy for selecting variables (in R, using the <code>nlme</code> package)? Anything I can do better?</p>

<ol>
<li>Break the data up by groups and plot it against time. For continuous covariates, bin it and plot the data in each bin against time. Use the group-specific trends to make an initial guess at what time terms to include-- time, time^n, sin(2*pi*time)+cos(2*pi*time), log(time), exp(time), etc.</li>
<li>Add one term at a time, comparing each model to its predecessor, never adding a higher order in the absence of lower order terms. Sin and cos are never added separately. <strong><em>Is it acceptable to pass over a term that significantly improves the fit of the model if there is no physical interpretation of that term?</em></strong>.</li>
<li>With the full dataset, use forward selection to add static variables to the model and then relevant interaction terms with each other and with the time terms. <strong><em>I've seen some strong criticism of stepwise regression, but doesn't forward selection ignore significant higher order terms if the lower order terms they depend on are not significant? And I've noticed that it's hard to pick a starting model for backward elimination that isn't saturated, or singular, or fails to converge. How do you decide between variable selection algorithms?</em></strong></li>
<li>Add random effects to the model. <strong><em>Is this as simple as doing the variable selection using <code>lm()</code> and then putting the final formula into <code>lme()</code> and specifying the random effects? Or should I include random effects from the very start?</em></strong>. Compare the fits of models using a random intercept only, a random interaction with the linear time term, and random interaction with each successive time term. </li>
<li>Plot a semivariogram to see if an autoregressive error term is needed. <strong><em>What should a semivariogram look like if the answer is 'no'? A horizontal line? How straight, how horizontal? Does including autoregression in the model again require checking potential variables and interactions to make sure they're still relevant?</em></strong></li>
<li>Plot the residuals to see if the variance changes as a function of fitted value, time, or any of the other terms. If it does, weigh the variances appropriately (for <code>lme()</code>, use the <code>weights</code> argument to specify a <code>varFunc()</code>) and compare to the unweighted model to see if this improves the fit. <strong><em>Is this the right sequence in which to do this step, or should it be done before autocorrelation?</em></strong>.</li>
<li>Do <code>summary()</code> of the fitted model to identify significant coefficients for numeric covariates. Do <code>Anova()</code> of the fitted model to identify significant effects for qualitative covariates.</li>
</ol>
"
"0.0839181358296689","0.0854704323447285"," 12223","<p>I am trying to figure out how to control the smoothing parameters in an mgcv:gam model.</p>

<p>I have a binomial variable I am trying to model as primarily a function of x and y coordinates on a fixed grid, plus some other variables with more minor influences.  In the past I have constructed a reasonably good local regression model using package locfit and just the (x,y) values.  </p>

<p>However, I want to try incorporating the other variables into the model, and it looked like generalized additive models (GAM) were a good possibility.  After looking at packages gam and mgcv, both of which have a GAM function, I opted for the latter since a number of comments in mailing list threads seem to recommend it.  One downside is that it doesn't seem to support a local regression smoother like loess or locfit.</p>

<p>To start, I just wanted to try to replicate approximately the locfit model, using just (x,y) coordinates.  I tried with both regular and tensor product smooths:</p>

<pre><code>my.gam.te &lt;- gam(z ~ te(x, y), family=binomial(logit), data=my.data, scale = -1)

my.gam.s  &lt;- gam(z ~  s(x, y), family=binomial(logit), data=my.data, scale = -1)
</code></pre>

<p>However, plotting the predictions from the model, they are much much more smoothed compared to the locfit model.  So I've been trying to tune the model to not oversmooth as much.  I've tried adjusting the parameters sp and k, but it's not clear to me how they affect the smoothing.  In locfit, the nn parameter controls the span of the neighborhood used, with smaller values allowing for less smoothing and more ""wiggling"", which helps to capture some areas on the grid where the probability of the binomial outcomes changes rapidly.  How would I go about setting up the gam model to enable it to behave similarly?</p>
"
"0.0484501583111509","0.0493463771219827"," 12519","<p>This is a supervised learning problem. Ideally would like to work in R due to having an easy way to pre-process the input data, but could work around that as well.</p>

<p>For each sample, input consists of tens of thousands of features. These are genomics data and will likely need to be reduced to a manageable amount, somehow, before being used to train the classifier.</p>

<p>Supervisory signal consists of 4 dependent continuous values, representing relative composition of the sample.</p>

<p>e.g. continuous between 0 and 1, all 4 summing to 1 for each sample:</p>

<pre><code>Sub012  0.5940594   0.26732673  0.07920792  0.059405941
Sub013  0.5102041   0.34693878  0.08163265  0.061224490
Sub014  0.6521739   0.20652174  0.07608696  0.065217391
</code></pre>

<p>Wanted: a regression function capable of predicting the relative composition of a sample in terms of those same 4 dependent continuous values.</p>

<p>The constraints on the supervisory signal are what is causing me pause: the dependence of the variables, being constrained between 0-1 and summing to 1. I was hoping someone might have attempted something similar and could point me in the right direction - packages or approaches which may work or definitely won't work - all thoughts welcomed.</p>

<p>Thank you.</p>
"
"0.027972711943223","0.0284901441149095"," 12743","<p>I would like to know how to estimate a population average model of a hierarchical logistic regression using <code>R</code> package <code>geepack</code>.</p>

<p>The <code>Stata</code> code is: </p>

<pre><code>xtlogit dep ind1 ind2 ind3, i(ind4) pa
</code></pre>

<p>I would like to reproduce this in <code>R</code> using <code>geepack</code> or any other method.</p>
"
"0.0625488854200668","0.0637058989297032"," 13091","<p>I have this model:</p>

<pre><code>model &lt;- zelig(dv~(product*intervention), model = ""negbin"", data = data)
</code></pre>

<p>intervention has <strong>two levels</strong>: neutral(=0), treatment(=1)<br />
product has <strong>two levels</strong>: product1(=0), product2(=1)</p>

<p>I build f_all to just have one factor with 4 groups for comparison analysis.</p>

<p>Thus I have <strong>4 groups</strong> in f_all<br />
1. product1-neutral<br />
2. product1-treatment<br />
3. product2-neutral<br />
4. product2-treament<br /></p>

<p><strong>My interaction hypothesis is that treatment only works for product2.</strong></p>

<p>Zelig gives me my predicted significant interaction. <br /></p>

<p>Yet, I need planned contrasts to test my specific hypothesis: c(-1,1,0,0) and c(0,0,1,-1)</p>

<p>I researched and found a description of doing this with multcomp on this page: <a href=""http://stats.stackexchange.com/questions/12993/how-to-setup-and-interpret-anova-contrasts-with-the-car-package-in-r"">post comparisons</a></p>

<p>The regression output shows my predicted interaction</p>

<pre><code>(Intercept)  1.34223    0.08024  16.728   &lt;2e-16 ***
product      0.08747    0.08025   1.090   0.2757
intervention 0.07437    0.07731   0.962   0.3361
interaction  0.45645    0.22263   2.050   0.0403 * 
</code></pre>

<p>However, it said multcomp and the glht function is for linear models, but I am using a negbin model.</p>

<p><strong>3 Questions regarding this problem:</strong><br />
1. Can I do planned comparisons on my negbin model using multcomp?<br />
2. If not what appropriate method is there to do this for my negbin model?<br />
3. Based on R using treatment contrasts per default could I just interpret the interaction coefficient as the contrast comparing product2-neutral versus product2-treatment? Can I then interpret the intervention coefficient as contrast comparing product1-neutral versus product1-treament?</p>
"
"0.0323001055407673","0.0328975847479884"," 13319","<p>Are there any packages that supports weighted average semi-parametric regressions in R? An example of such a regression is in the links below.</p>

<p>I see that there is package <a href=""http://www.google.com/url?sa=t&amp;source=web&amp;cd=1&amp;ved=0CBoQFjAA&amp;url=http://eprints.lse.ac.uk/24504/1/dp599.pdf&amp;ei=FOYoTqo0zdOAB8qn8Z8L&amp;usg=AFQjCNGx4pn6837FopcKifg8qwK4d2Setg"" rel=""nofollow"">GAM</a> in R for Generalized Additive Models. However, it is not clear to me whether this procedure is the same as weighted average semi-parametric regression. There is also a package called <a href=""http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-nonparametric-regression.pdf"" rel=""nofollow"">SPM</a></p>

<p>Some background on semi-parametric methods as used by Connor is <a href=""http://www.google.com/url?sa=t&amp;source=web&amp;cd=1&amp;ved=0CBoQFjAA&amp;url=http://eprints.lse.ac.uk/24504/1/dp599.pdf&amp;ei=FOYoTqo0zdOAB8qn8Z8L&amp;usg=AFQjCNGx4pn6837FopcKifg8qwK4d2Setg"" rel=""nofollow"">here</a>. This paper refers extensively to improvements in the methodology vs. a prior paper by Connor <a href=""http://sticerd.lse.ac.uk/dps/em/Em506.pdf"" rel=""nofollow"">here</a></p>

<p>Also, attached is a paper that discusses <a href=""http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-nonparametric-regression.pdf"" rel=""nofollow"">non-parametric regressions in R</a></p>

<p>Thank you</p>
"
"0.0500391083360534","0.0637058989297032"," 13836","<p>Some research has shown that in linear regression applications the Mahalanobis distance approach can be used to perform regressions that lower the influence of outliers. The idea is that in the regression every observation is given a weight as an inverse of the Mahalanobis distance. </p>

<p>I see that there is a package <a href=""http://cran.r-project.org/web/packages/RLMM/vignettes/RLMM.pdf"" rel=""nofollow"">RLMM</a> for applying Mahalanobis distance in a classification setting. However, I do not see a regression technique that allows one to apply this as a robust regression technique. </p>

<p>My assumption is that I can use the <code>lm()</code> function and specify weights as the inverse of the output of Mahalanobis distance function. Since it seems the Mahalanobis distance function is <a href=""http://en.wikipedia.org/wiki/Generalized_least_squares"" rel=""nofollow"">equivalent</a> to using GLS then can I simply use the <code>gls()</code> function?</p>
"
"NaN","NaN"," 14701","<p>I run into a problem where I would like to build a GEE in R with cubic regression splines (or any other spline type) for a longitudinal data set and an urgent need for grouping and multiple autocorrelation structures. However, I did not find any package capable allowing for that. </p>

<p>If anyone could give a suggestion for a suitable package, I would really appreciate that.</p>
"
"0.131327836050277","0.13375710524478"," 15160","<p>I have a large dataset with patients and I'm studying a rare outcome (~ 2%) and death is a competing risk (mean age ~69 years). I've used the R ""cmprsk"" package for my statistics and it seems that competing risks and the Cox regression are performing similarly although the competing risk analysis is more conservative giving hazard ratios closer to 1.</p>

<p>I've been suggested to do a Poisson regression on the data but the results don't make any sense and I would be really grateful to get some input on the benefits of doing this kind of analysis on survival data. I've created this simulation for creating a dataset with similar risk factors:</p>

<pre><code>library(""cmprsk"")
# The time for the study
accrual_time &lt;- 10
followup_time &lt;- 1

base_risk &lt;- list(""event"" = .015, ""cmprsk"" = .1)

risk_factors &lt;- list(list(""frequency""=.1, 
                ""event"" = base_risk$event*.5, 
                ""cmprsk"" = base_risk$cmprsk*2),
        list(""frequency""=.05, 
                ""event"" = base_risk$event*1, 
                ""cmprsk"" = base_risk$cmprsk*1),
        list(""frequency""=.05, 
                ""event"" = base_risk$event*-.5, 
                ""cmprsk"" = base_risk$cmprsk*0))

# Number of subjects
n &lt;- 5000

# Create base time, sequential inclusion
time_in_study &lt;- rep(c(1:n)/n*accrual_time + followup_time, 1)

set.seed(100)

# Create empty sets
x &lt;- matrix(0, ncol=length(risk_factors), nrow=n)
time_2_event &lt;- rep(0, n)
time_2_comprsk &lt;- rep(0, n)

# Create each studied observation and outcome
for(i in 1:n){
    # Set base risk
    event_risk &lt;- base_risk$event 
    comp_risk &lt;- base_risk$cmprsk

    for(j in 1:length(risk_factors)){
        x[i, j] &lt;- rbinom(1, 1, risk_factors[[j]]$frequency)[1]

        # If there is a risk factor defined
        if (x[i, j] &gt; 0){
            event_risk &lt;- event_risk +
                    risk_factors[[j]]$event
            comp_risk &lt;- comp_risk + 
                    risk_factors[[j]]$cmprsk
        }
    }

    # Time 2 event/risk is 1/rate meaning that higher number -&gt; shorter time
    time_2_event[i] &lt;- rexp(1, rate=event_risk)[1]
    time_2_comprsk[i] &lt;- rexp(1, rate=comp_risk)[1]
}

cn &lt;- c()
for(i in 1:length(risk_factors)){
    ev_rsk &lt;- risk_factors[[i]]$event/base_risk$event+1
    cmp_rsk &lt;- risk_factors[[i]]$cmprsk/base_risk$cmprsk+1
    name &lt;- paste(""Risk factor no: "", i, ""\n * ev="", ev_rsk, "" cr="", cmp_rsk, "" *"", sep="""")
    cn &lt;- c(cn, name)
}
colnames(x) &lt;- cn

# Select the event that happens first: study ends, evenent occurs, a competing event occurs
time &lt;- apply(cbind(time_in_study, time_2_event, time_2_comprsk), 1, min)

# Outcome identifiers
event &lt;- (time_2_event == time) + 0
comprsk &lt;- (time_2_comprsk == time) + 0
cens &lt;- event+2*(event==0 &amp; comprsk==1)

out.cox_ev &lt;- coxph(Surv(time, event)~x)
summary(out.cox_ev)

out.crr_ev &lt;- crr(time, cens, x, failcode=1)
summary(out.crr_ev)

out.cox_cmprsk &lt;- coxph(Surv(time, comprsk)~x)
summary(out.cox_cmprsk)

out.crr_cmprsk &lt;- crr(time, cens, x, failcode=2)
summary(out.crr_cmprsk)
</code></pre>

<p>The output makes sense but when I do a:</p>

<pre><code>out.glm_pr &lt;- glm(event ~ x, family=""poisson"")
summary(out.glm_pr)
</code></pre>

<p>It gives estimates of:</p>

<ul>
<li>RF 1 ~ .14 </li>
<li>RF 2 ~ .41 </li>
<li>RF 3 ~ -.23</li>
</ul>

<p>My questions: </p>

<ul>
<li>Is the glm() code correct or should I somehow transform my data?</li>
<li>Does the Poisson output make any sense and how should if so interpret it?</li>
<li>What are the benefits/pitfalls in using Poisson regression for survival data?</li>
</ul>

<p>Thanks!</p>

<hr>

<h2>UPDATE</h2>

<p>After adding exp(out.glm_pr$coefficients) the results are almost identical to the competing risk regression, here's a forest plot that compares the three:</p>

<p><img src=""http://i.stack.imgur.com/14Zt0.png"" alt=""A forestplot comparing the different methods - Poisson: 1.152  1.509  0.794, CRR: 1.151 1.524 0.812, Cox PH: 1.897 1.931 0.798""></p>

<p>The x-axis is perhaps not entirely valid (should be ""incident rate ratios"" for the Poisson regression) but why are the outcomes for CRR &amp; poisson almost identical?</p>

<p>As for testing over-dispersion I've found these two methods:</p>

<pre><code>&gt; library(qcc)
&gt; qcc.overdispersion.test(event)

Overdispersion test Obs.Var/Theor.Var Statistic p-value
       poisson data         0.9391878      4695 0.99902
&gt; 
&gt; library(pscl)
&gt; out.glm_nb &lt;- glm.nb(event ~ x)
Warning messages:
1: In theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace = control$trace &gt;  :
  iteration limit reached
2: In theta.ml(Y, mu, sum(w), w, limit = control$maxit, trace = control$trace &gt;  :
  iteration limit reached
&gt; odTest(out.glm_nb)
Likelihood ratio test of H0: Poisson, as restricted NB model:
n.b., the distribution of the test-statistic under H0 is non-standard
e.g., see help(odTest) for details/references

Critical value of test statistic at the alpha= 0.05 level: 2.7055 
Chi-Square Test Statistic =  -0.0139 p-value = 0.5 
</code></pre>

<p>I conclude that there isn't any evidence of over-dispersion or are there other methods better suited for testing over-dispersion in this kind of survival data?</p>

<p>The quasipoisson analysis gives similar values:</p>

<pre><code>&gt; out.glm_quasi_pr &lt;- glm(event ~ x, family=quasipoisson(link=""log""))
&gt; round(exp(out.glm_quasi_pr$coefficients), 3)
(Intercept)       xRF 1       xRF 2       xRF 3 
      0.059       1.152       1.509       0.794 
</code></pre>
"
"0.0484501583111509","0.0493463771219827"," 15469","<p>Recently we discussed on SO how to update a standard linear regression summary with NeweyWest standard errors. I used <code>coeftest</code>from the <code>sandwich</code> package. It was told to use unclass to update my already existing summary like this: </p>

<pre><code>library(sandwich)
library(lmtest)
temp.lm &lt;- lm(runif(100) ~ rnorm(100))
temp.summ &lt;- summary(temp.lm)
temp.summ$coefficients &lt;- unclass(coeftest(temp.lm, vcov. = NeweyWest)
</code></pre>

<p>Now I wonder whether the joint parameters shown in the summary aren't affected at all when using a NeweyWest VC matrix? I mean with this code they are not affected obviously â€“Â but is this correct? Note this is not a syntax but a stats question :) Stuff like</p>

<pre><code>Residual standard error: 1.177 on 83 degrees of freedom  
Multiple R-squared: 0.7265, Adjusted R-squared:  0.71 
F-statistic:  44.1 on 5 and 83 DF,  p-value: &lt; 2.2e-16 
</code></pre>

<p>remains the same. Are there any cases that need adjustment as well?</p>
"
"0.0745938985152613","0.0854704323447285"," 15623","<p>I am completely new to R, just downloaded and installed it today. I am familiar with SAS and Stata; I am using R because I have found out that in survey regression analysis, R is capable of using data that have stratum with one PSU. However, I cannot figure out how to write the code at all.</p>

<p>Here is what I have done so far: read a Stata dataset and save the .RData file. I have also put in the MASS, pscl, and survey (for svyglm) packages.</p>

<p>Here's what I need to do:
1) I am using survey data, so I have a ""weight"" variable, a ""strata"" variable, and a ""PSU"" variable. I need to incorporate those; I know how to use svyset in Stata, but no idea in R.
2) I have stratum with singleton PSUs. I need to use an option called survey.lonely.psu I believe, and I have no idea where to even begin with that. This is the reason why I am using R instead of Stata as I do not want to collapse stratum or delete observations.
3) The types of regression models I have to run: survey negative binomial, survey zero-inflated negative binomial (need to also determine the predictors of zeros), survey logistic, and survey OLS regression.
4)I also really can't make much sense in R of how to write the model in R code. In Stata, I can simply write the model as:</p>

<p>svy: nbreg dependent_var independent_var1 independent_var2 independent_var3</p>

<p>I can't figure out how to do that at all in R.</p>

<p>Any and all help will be greatly appreciated.</p>
"
"0.0559454238864459","0.056980288229819"," 16346","<p>(Please note the cross-post at <a href=""http://stackoverflow.com/questions/7626347/difference-between-lp-or-simply-in-rs-locfit"">http://stackoverflow.com/questions/7626347/difference-between-lp-or-simply-in-rs-locfit</a>)</p>

<p>I am not sure I see the difference between different examples for local logistic regression in the documentation of the gold standard locfit package for R: <a href=""http://cran.r-project.org/web/packages/locfit/locfit.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/locfit/locfit.pdf</a></p>

<p>I get starkingly different results with</p>

<pre><code>fit2&lt;-scb(closed_rule ~ lp(bl),deg=1,xlim=c(0,1),ev=lfgrid(100), family='binomial',alpha=cbind(0,0.3),kern=""parm"")
</code></pre>

<p>from</p>

<pre><code>fit2&lt;-scb(closed_rule ~ bl,deg=1,xlim=c(0,1),ev=lfgrid(100), family='binomial',alpha=cbind(0,0.3),kern=""parm"")
</code></pre>

<p>.</p>

<p>What is the nature of the difference? Maybe that can help me phrase which I wanted. I had in mind an index linear in bl within a logistic link function predicting the probability of closed_rule. The documentation of lp says that it fits a local polynomial -- which is great, but I thought that would happen even if I leave it out. And in any case, the documentation has examples for ""local logistic regression"" either way...</p>
"
"0.0559454238864459","0.056980288229819"," 16837","<p>The randomForest package in R software includes outlier function for the detection of outliers. This function uses proximity matrix or randomForest object for the outlier detection. The manual says that the type of the randomForest object can not be regression? Why is it so that this function can be used for classification models only? Is there a reasonable way to detect outliers in regression models? </p>
"
"0.0807502638519182","0.0986927542439653"," 17480","<p>I've created a few Cox regression models and I would like to see how well these models perform and I thought that perhaps a ROC-curve or a c-statistic might be useful similar to this articles use:</p>

<p><a href=""http://onlinelibrary.wiley.com/doi/10.1002/bjs.6930/abstract"" rel=""nofollow"">J. N. Armitage och J. H. van der Meulen, â€Identifying coâ€morbidity in surgical patients using administrative data with the Royal College of Surgeons Charlson Scoreâ€, British Journal of Surgery, vol. 97, num. 5, ss. 772-781, Maj 2010.</a></p>

<p>Armitage used logistic regression but I wonder if it's possible to use a model from the survival package, the <a href=""http://cran.r-project.org/web/packages/survivalROC/index.html"" rel=""nofollow"">survivalROC</a> gives a hint of this being possible but I can't figure out how to get that to work with a regular Cox regression. </p>

<p>I would be grateful if someone would show me how to do a ROC-analysis on this example:</p>

<pre><code>library(survival)
data(veteran)

attach(veteran)
surv &lt;- Surv(time, status)
fit &lt;- coxph(surv ~ trt + age + prior, data=veteran)
summary(fit)
</code></pre>

<p>If possible I would appreciate both the raw c-statics output and a nice graph</p>

<p>Thanks!</p>

<h2>Update</h2>

<p>Thank you very much for answers. @Dwin: I would just like to be sure that I've understood it right before selecting your answer. </p>

<p>The calculation as I understand it according to DWin's suggestion:</p>

<pre><code>library(survival)
library(rms)
data(veteran)

fit.cph &lt;- cph(surv ~ trt + age + prior, data=veteran, x=TRUE, y=TRUE, surv=TRUE)

# Summary fails!?
#summary(fit.cph)

# Get the Dxy
v &lt;- validate(fit.cph, dxy=TRUE, B=100)
# Is this the correct value?
Dxy = v[rownames(v)==""Dxy"", colnames(v)==""index.corrected""]

# The c-statistic according to the Dxy=2(c-0.5)
Dxy/2+0.5
</code></pre>

<p>I'm unfamiliar with the validate function and bootstrapping but after looking at prof. Frank Harrel's answer <a href=""http://r.789695.n4.nabble.com/Interpreting-the-example-given-by-Prof-Frank-Harrell-in-Design-validate-cph-tt3316820.html#a3324516"" rel=""nofollow"">here on R-help</a> I figured that it's probably the way to get the Dxy. The help for validate states:</p>

<blockquote>
  <p>... Somers' Dxy rank correlation to be computed at each resample (this
  takes a bit longer than the likelihood based statistics). The values
  corresponting to the row Dxy are equal to 2 * (C - 0.5) where C is the
  C-index or concordance probability.</p>
</blockquote>

<p>I guess I'm mostly confused by the columns. I figured that the corrected value is the one I should use but I haven't really understood the validate output:</p>

<pre><code>      index.orig training    test optimism index.corrected   n
Dxy      -0.0137  -0.0715 -0.0071  -0.0644          0.0507 100
R2        0.0079   0.0278  0.0037   0.0242         -0.0162 100
Slope     1.0000   1.0000  0.2939   0.7061          0.2939 100
...
</code></pre>

<p>In the <a href=""http://r.789695.n4.nabble.com/Interpreting-the-example-given-by-Prof-Frank-Harrell-in-Design-validate-cph-tt3316820.html#a3324516"" rel=""nofollow"">R-help question</a> I've understood that I should have ""surv=TRUE"" in the cph if I have strata but I'm uncertain on what the purpose of the ""u=60"" parameter in the validate function is. I would be grateful if you could help me understand these and check that I haven't made any mistakes.</p>
"
"0.100857047225074","0.102722675451665"," 17552","<p>I have a gene expression data-set with log2-transformed expression values (no NAs) for 495 genes for 59 samples for which values of a continuous response variable (r) are also known (no NAs). I want to use leave-one-out cross validation to test if r of the test sample can be predicted from the sample's gene expression.</p>

<p>For this, I intend to use the <a href=""http://cran.r-project.org/web/packages/samr/index.html"" rel=""nofollow"">samr</a> R package for Significance Analysis of Microarrays to identify significant genes associated with r in the training set of samples. Then, I want to generate a linear model using the significant genes as variables, which will then be used to predict r of the test sample. I have tried the following code to begin with, but when I generate the model and examine it, I see many NAs in the model summary, which makes me suspect that I am doing something wrong.</p>

<p>Can someone tell me what I might be doing wrong?</p>

<p>Secondly, I will appreciate any comment on the use of nperms (in SAM) with a value of 100. Is it too low for an expression data-set for 495 genes. </p>

<pre><code># rVals with the r values is read as a vector from a row of a table for phenotypic data read from a tab-delimited file with sample-names as column names and phenotype features as row-names
# geneVals is the log2-transformed gene expression data-set read as a matrix from a tab-delimited file with sample-names as column names and gene-names as row-names

# Perform SAM with FDR of 5% and obtain list of significant genes

sam &lt;- SAM(x=geneVals, y=rVals, resp.type=c(""Quantitative""),
testStatistic=c(""standard""), regression.method=c(""standard""), logged2=TRUE, 
fdr.output=0.05, eigengene.number=1, knn.neighbors=10, nperms=100, 
genenames=as.vector(rownames(geneVals)))

sigGenes &lt;- rbind(sam$siggenes.table$genes.up, sam$siggenes.table$genes.lo)

# Generate linear model
toModel &lt;- data.frame(t(rbind(rVals, geneVals)), check.names=FALSE)
myModel &lt;- lm(toModel[c('r', sigGenes[,c(""Gene ID"")])])

# Examine model
summary(myModel)

...output...

Call:
lm(formula = toModel[c(""rVals"", sigGenes[, c(""Gene ID"")])])

Residuals:
ALL 59 residuals are 0: no residual degrees of freedom!

Coefficients: (58 not defined because of singularities)
           Estimate Std. Error t value Pr(&gt;|t|)
(Intercept)   -18.29363         NA      NA       NA
`let-7e`       -1.70545         NA      NA       NA
`miR-125a-5p`   2.43177         NA      NA       NA
`miR-151-5p`    2.67439         NA      NA       NA
...
</code></pre>
"
"NaN","NaN"," 17672","<p>I am trying to implement a simple quantile regression in <code>R</code> using <code>JAGS</code>:</p>

<pre><code>n &lt;- 200
x &lt;- runif(n=n,min=0,max=10)
y &lt;- 1 + 2*x + rnorm(n,sd=.6*x)
p &lt;- 0.95
jdf &lt;- list(y=y,x=x,p=p)
params &lt;- c(""alpha"",""beta"",""tau"")
mcmcmodel &lt;- jags.model(file=""qreg.jag"",data=jdf,n.chains=3)
update(mcmcmodel,2000)
mcsamples &lt;- coda.samples(mcmcmodel,params,n.iter=10000,thin=10)
</code></pre>

<p>with <strong>qreg.jag</strong> as follows:</p>

<pre><code>model{
 for(i in 1:length(y)){
   mu[i] &lt;- alpha + beta*x[i]
   cmu[i] &lt;- (step(mu[i])/(1-p) + step(-mu[i])/p)*mu[i]/2
   y[i] ~ ddexp(cmu[i],2*tau*p*(1-p))
 }

 #priors for regression
 alpha ~ dnorm(0,1E-6)
 beta ~ dnorm(0,1E-6)#dunif(0,100)

 lsigma ~ dunif(-5,15)
 sigma &lt;- exp(lsigma)
 tau &lt;- pow(sigma,-2)
}
</code></pre>

<p>I have compared my results with <code>bayesQR</code> package: the results do not match and what I have implemented gives unreasonable estimates.</p>

<p>Can anybody help?</p>
"
"0.0395593886064618","0.0201455741006345"," 17815","<p>I was wondering if it is possible to use the caret package with non numerical data.
I know, for example, if I want to use a simple linear regression <code>lm</code> I could have a factor variable for classification.
However, caret blows up if I attempt this. I'm also following the step outlined here <a href=""http://www.r-project.org/conferences/useR-2010/slides/Kuhn.pdf"" rel=""nofollow"">The caret Package: A Unifed Interface for Predictive
Models</a></p>

<p>for illustration I'm attempting to run <code>stepDurationlm &lt;- train (x= trainDescr, y=trainClass, method=""lm"")</code></p>

<p>on</p>

<pre><code>str(trainDescr)
'data.frame':   589235 obs. of  2 variables:
 $ Anon.Student.Id    : Factor w/ 574 levels &quot;02i5jCrfQK&quot;,&quot;02ZjVTxC34&quot;,..: 7 7 7 7 7 7 7 7 7 7 ...
 $ Step.Duration..sec.: num  5 5 5 4 5 5 5 5 4 4 ...
</code></pre>

<p>I get</p>

<pre><code>Error in train.default(x = trainDescr, y = trainClass, method = ""lm"") : 
All predictors must be numeric for this model. Use the formula interface: train(formula, data)
</code></pre>

<p>alternatively, could anyone explain how to have a test set for model performance in R? That's what's motivating me to get caret working.</p>
"
"0.0969003166223018","0.0986927542439653"," 18045","<p>The data simulated below has a maximum value of 4 and is interestingly skewed. The maximum of 4 is a limitation imposed by the instrument used and the data is semi-discrete, i.e., there are a reasonably large number of numbers it could be between -4 and 4. Because of the shape of the data, I thought about transforming it so it would approximate a gamma distribution:  </p>

<p><em>Edit to update for comments:</em><br>
It is limited to this range in this instance because it is a signal detection measure (d prime <a href=""http://en.wikipedia.org/wiki/D%27"" rel=""nofollow"">http://en.wikipedia.org/wiki/D%27</a>) and the accuracy we have for this particular measure limits us to +-4. It is skewed like this because one population does not very often get false positives and will generally get more hits while the other populations often do get false positives and less hits.</p>

<pre><code>set.seed(69)
g1&lt;-rnorm(700,0,1); g2&lt;-rnorm(100,-0.5,1.5); g3&lt;-rnorm(100,-1,2.5)
gt&lt;-data.frame(score=c(g1, g2, g3), fac1=factor(rep(c(""a"", ""b"", ""c""), c(700, 100, 100))), fac2=ordered(rep(c(0,1,2), c(3,13,4))))
gt$score&lt;-with(gt, ifelse(fac2 == 0, score, score-rnorm(1, 0.5, 2)))
gt$score&lt;-with(gt, ifelse(fac2 == 2, score-rnorm(1, 0.5, 2), score))
gt$score&lt;-round(with(gt, ifelse(score&gt;0, score*-1, score)), 1)+4
gt$score&lt;-with(gt, ifelse(score &lt; -4, -4, score))
gt$cov1&lt;-with(gt, score + rnorm(900, sd=40))/40
hist(gt$score)
gt$score2&lt;-with(gt, 4-score+0.0000001) #Gamma distribution can't have 0s (and is positive skewed???)
hist(gt$score2)

glm1&lt;-glm(score2~cov1+fac1*fac2, family=""Gamma"", data=gt)
</code></pre>

<p>This is quite new territory for me.<br>
1. Is this a reasonable thing to do?<br>
2. Are there other distributions I might try and compare (exponential perhaps)?</p>

<p><em>Update:</em><br>
After some comments below, I investigated beta regression using the <em>betareg</em> package in R. It gave me skewed residuals:  </p>

<pre><code>gt$scorer&lt;-with(gt, (score--4)/(4--4))
gt$scorer&lt;-with(gt, (scorer*(length(scorer)-1)+0.5)/length(scorer))
b1 &lt;- betareg(scorer ~ cov1 + fac1 * fac2, data=gt)
plot(density(resid(b1))) #Strange residuals, even straight lm looks better
</code></pre>

<p>So I had a look at a quasibinomial regression and it gave me smaller and better looking residuals:</p>

<pre><code>glm2 &lt;- glm(scorer~cov1 + fac1 * fac2, data=gt, family=""quasibinomial"")
plot(density(resid(g1))) #Better residuals
</code></pre>

<p>Are the residuals good enough to go on in this case?<br>
Or is the fact that d', while based upon T/F, is not a binary variable, a serious issue?  </p>

<p><em>Edit 3: d' clarification</em> 
The below is an example of my d' scores, with the rough distributional qualities and similar raw scores for hits and false positives.  </p>

<pre><code>hitrate&lt;-sample(0:16, 100, replace=T, prob=c(rep(0.02,11), 0.025, 0.05, 0.1, 0.2, 0.3, 0.2))/16
hitrate&lt;-ifelse(hitrate==1, 31/32,hitrate); hitrate&lt;-ifelse(hitrate==0, 1/32,hitrate)
farate&lt;-sample(0:32,100, replace=T, prob=c(0.7,0.1,0.05,0.05,0.05,0.02,rep(0.001, 27)))/32
farate&lt;-ifelse(farate==0, 1/64,farate); farate&lt;-ifelse(farate==1, 63/64,farate)

dprime&lt;-round(qnorm(hitrate) - qnorm(farate),1)
plot(density(dprime))
</code></pre>
"
"NaN","NaN"," 18387","<p>Could someone please explain what the difference is between the two, and perhaps avoid the worst statistical jargon?</p>

<p>I am currently using the <code>dlm</code> package to model dynamic regressions as can be seen on p.122-5 in <a href=""http://rads.stackoverflow.com/amzn/click/0387772375"" rel=""nofollow"">Dynamic Linear Models with R</a>, and I don't really grasp the theoretical difference between using <code>dlmFilter</code> and <code>dlmSmooth</code>.</p>

<p>More specifically, why is the <code>dlmFilter</code> example on p. 123 not considered dynamic, while the <code>dlmSmooth</code> example on p.124-5 is? And also, why are the two results fairly different?</p>
"
"0.0500391083360534","0.0637058989297032"," 18470","<p>Basically all I want to do is predict a scalar response using some curves.
I've got as far as doing a regression (using fRegress from the fda package) but have no idea how to apply the results to a NEW set of curves (for prediction).</p>

<p>I have N=536 curves, and 536 scalar responses. Here's what I've done so far:</p>

<ul>
<li>I've created a basis for the curves.</li>
<li>I've created a fdPar object to introduce a penalty</li>
<li>I've created the fd object using smooth.basis to smooth the curves with the chosen penalty on the specified basis.</li>
<li>I've ran a regression using fRegress(), regressing the curves on the scalar response.</li>
</ul>

<p>Now, all I'd like to do, is use that regression to produce predictions for a new set of data that I have. I can't seem to find an easy way to do this.</p>

<p>Cheers</p>
"
"0.145350474933453","0.142556200574617"," 18576","<p><strong>Update</strong>: Sorry for another update but I've found some possible solutions with fractional polynomials and the competing risk-package that I need some help with.</p>

<hr>

<h2>The problem</h2>

<p>I can't find an easy way to do a time dependent coefficient analysis is in R. I want to be able to take my variables coefficient and do it into a time dependent coefficient (not variable) and then plot the variation against time:</p>

<p>$\beta_{my\_variable}=\beta_0+\beta_1*t+\beta_2*t^2...$</p>

<h2>Possible solutions</h2>

<h3>1) Splitting the dataset</h3>

<p>I've looked at <a href=""http://anson.ucdavis.edu/~johnson/st222/lab8/lab8.htm"">this</a> example (Se part 2 of the lab session) but the creation of a separate dataset seems complicated, computationally costly and not very intuitive...</p>

<h3>2) Reduced Rank models - The coxvc package</h3>

<p>The <a href=""https://www.msbi.nl/dnn/Research/SurvivalAnalysis/Coxmodelswithtimevaryingeffects.aspx"">coxvc package</a> provides an elegant way of dealing with the problem - here's a <a href=""https://openaccess.leidenuniv.nl/bitstream/handle/1887/4918/Appendix.pdf?sequence=5"">manual</a>. The problem is that the author is no longer developing the package (last version is since 05/23/2007), after some e-mail conversation I've gotten the package to work but one run took 5 hours on my dataset (140 000 entries) and gives extreme estimates at the end of the period. You can find a slightly updated <a href=""http://pastebin.com/uiPy0ueF"">package here</a> - I've mostly just updated the plot function. </p>

<p>It might be just a question of tweaking but since the software doesn't easily provide confidence intervals and the process is so time consuming I'm looking right now at other solutions. </p>

<h3>3) The timereg package</h3>

<p>The impressive <a href=""http://cran.r-project.org/web/packages/timereg/index.html"">timereg package</a> also addresses the problem but I'm not certain of how to use it and it doesn't give me a smooth plot.</p>

<h3>4) Fractional Polynomial Time (FPT) model</h3>

<p>I found Anika Buchholz' excellent dissertation on <a href=""http://deposit.ddb.de/cgi-bin/dokserv?idn=1008218782&amp;dok_var=d1&amp;dok_ext=pdf&amp;filename=1008218782.pdf"">""Assessment of timeâ€“varying longâ€“term effects of therapies and prognostic factors""</a> that does an excellent job covering different models. She concludes that <a href=""http://onlinelibrary.wiley.com/doi/10.1002/bimj.200610328/abstract"">Sauerbrei et al's proposed FPT</a> seems to be the most appropriate for time-dependent coefficients:</p>

<blockquote>
  <p>FPT is very good at detecting time-varying effects, while the Reduced Rank approach results in far too complex models, as it does not include selection of time-varying effects.</p>
</blockquote>

<p>The research seems very complete but it's slightly out of reach for me. I'm also a little wondering since she happens to work with Sauerbrei. It seems sound though and I  guess the analysis could be done with the <a href=""http://cran.r-project.org/web/packages/mfp/index.html"">mfp package</a> but I'm not sure how.</p>

<h3>5) The cmprsk package</h3>

<p>I've been thinking of doing my competing risk analysis but the calculations have been to time-consuming so I switched to the regular cox regression. The <a href=""http://www.inside-r.org/packages/cran/cmprsk/docs/crr"">crr</a> has thoug an option for time dependent covariates:</p>

<pre><code>....
cov2        matrix of covariates that will be multiplied 
            by functions of time; if used, often these 
            covariates would also appear in cov1 to give 
            a prop hazards effect plus a time interaction
....
</code></pre>

<p>There is the quadratic example but I'm don't quite follow where the time actually appears and I'm not sure of how to display it. I've also looked at the test.R file but the example there is basically the same...</p>

<h2>My example code</h2>

<p>Here's an example that I use to test the different possibilities</p>

<pre><code>library(""survival"")
library(""timereg"")
data(sTRACE)

# Basic cox regression    
surv &lt;- with(sTRACE, Surv(time/365,status==9))
fit1 &lt;- coxph(surv~age+sex+diabetes+chf+vf, data=sTRACE)
check &lt;- cox.zph(fit1)
print(check)
plot(check, resid=F)
# vf seems to be the most time varying

######################################
# Do the analysis with the code from #
# the example that I've found        #
######################################

# Split the dataset according to the splitSurv() from prof. Wesley O. Johnson
# http://anson.ucdavis.edu/~johnson/st222/lab8/splitSurv.ssc
new_split_dataset = splitSuv(sTRACE$time/365, sTRACE$status==9, sTRACE[, grep(""(age|sex|diabetes|chf|vf)"", names(sTRACE))])

surv2 &lt;- with(new_split_dataset, Surv(start, stop, event))
fit2 &lt;- coxph(surv2~age+sex+diabetes+chf+I(pspline(stop)*vf), data=new_split_dataset)
print(fit2)

######################################
# Do the analysis by just straifying #
######################################
fit3 &lt;- coxph(surv~age+sex+diabetes+chf+strata(vf), data=sTRACE)
print(fit3)

# High computational cost!
# The price for 259 events
sum((sTRACE$status==9)*1)
# ~240 times larger dataset!
NROW(new_split_dataset)/NROW(sTRACE)

########################################
# Do the analysis with the coxvc and   #
# the timecox from the timereg library #
########################################
Ft_1 &lt;- cbind(rep(1,nrow(sTRACE)),bs(sTRACE$time/365,df=3))
fit_coxvc1 &lt;- coxvc(surv~vf+sex, Ft_1, rank=2, data=sTRACE)

fit_coxvc2 &lt;- coxvc(surv~vf+sex, Ft_1, rank=1, data=sTRACE)

Ft_3 &lt;- cbind(rep(1,nrow(sTRACE)),bs(sTRACE$time/365,df=5))
fit_coxvc3 &lt;- coxvc(surv~vf+sex, Ft_3, rank=2, data=sTRACE)

layout(matrix(1:3, ncol=1))
my_plotcoxvc &lt;- function(fit, fun=""effects""){
    plotcoxvc(fit,fun=fun,xlab='time in years', ylim=c(-1,1), legend_x=.010)
    abline(0,0, lty=2, col=rgb(.5,.5,.5,.5))
    title(paste(""B-spline ="", NCOL(fit$Ftime)-1, ""df and rank ="", fit$rank))
}
my_plotcoxvc(fit_coxvc1)
my_plotcoxvc(fit_coxvc2)
my_plotcoxvc(fit_coxvc3)

# Next group
my_plotcoxvc(fit_coxvc1)

fit_timecox1&lt;-timecox(surv~sex + vf, data=sTRACE)
plot(fit_timecox1, xlab=""time in years"", specific.comps=c(2,3))
</code></pre>

<p>The code results in these graphs: Comparison of <a href=""http://i.stack.imgur.com/e2aSr.png"">different settings for coxvc</a> and  of the 
<a href=""http://i.stack.imgur.com/zXz1H.png"">coxvc and the timecox</a> plots. I guess the results are ok but I don't think I'll be able to explain the timecox graph - it seems to complex...</p>

<h2>My (current) questions</h2>

<ul>
<li>How do I do the FPT analysis in R?</li>
<li>How do I use the time covariate in cmprsk?</li>
<li>How do I plot the result (preferably with confidence intervals)?</li>
</ul>
"
"0.195848157151738","0.199470906975978"," 18709","<p>I want to fit mixed model using lme4, nlme, baysian regression package or any available. </p>

<p><em><strong>Mixed model in Asreml- R  coding conventions</em></strong></p>

<p>before going into specifics, we might want to have details on asreml-R conventions, for those who are unfamiliar with ASREML codes.</p>

<pre><code>y = XÏ„ + Zu + e ........................(1) ; 
</code></pre>

<p>the usual mixed model with, y denotes the n Ã— 1 vector of observations,where Ï„ is the pÃ—1 vector of ï¬xed eï¬€ects, X is an nÃ—p design matrix of full column rank which associates observations with the appropriate combination of ï¬xed eï¬€ects, u is the q Ã— 1 vector of random eï¬€ects, Z is the n Ã— q design matrix which associates observations with the appropriate combination of random eï¬€ects, and e is the n Ã— 1 vector of residual errors.The model (1) is called a linear mixed model or linear mixed eï¬€ects model. It is assumed </p>

<p><img src=""http://i.stack.imgur.com/gxdur.jpg"" alt=""enter image description here""></p>

<p>where the matrices G and R are functions of parameters Î³ and Ï†, respectively.</p>

<p>The parameter Î¸ is a variance parameter which we will refer to as the scale parameter.</p>

<p>In mixed eï¬€ects models with more than one residual variance, arising for example in the
analysis of data with more than one section or variate, the parameter Î¸ is
ï¬xed to one. In mixed eï¬€ects models with a single residual variance then Î¸ is equal to
the residual variance (Ïƒ2). In this case R must be correlation matrix. Further details on the models are provided in the <a href=""http://www.vsni.co.uk/downloads/asreml/release2/doc/asreml-R.pdf"">Asreml manual (link)</a>. </p>

<p>Variance structures for the errors: R structure and Variance structures for the random eï¬€ects: G structures can be specified.</p>

<p><img src=""http://i.stack.imgur.com/or4Gj.jpg"" alt=""enter image description here""><img src=""http://i.stack.imgur.com/oXTgc.jpg"" alt=""enter image description here""></p>

<p>variance modelling in asreml() it is important to understand the formation of variance structures via direct products. The usual least squares assumption (and the default in asreml()) is that these are independently and identically distributed (IID). However, if the data was from a field experiment laid out in a rectangular array of r rows by c columns, say, we could arrange the residuals e as a matrix and potentially consider that they were autocorrelated within rows and columns.Writing the residuals as a vector in field order, that is, by sorting the residuals rows
within columns (plots within blocks) the variance of the residuals might then be</p>

<p><img src=""http://i.stack.imgur.com/SPE5b.jpg"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/IcikW.jpg"" alt=""enter image description here""> are correlation matrices for the row model (order r, autocorrelation parameter Â½r) and column model (order c, autocorrelation parameter Â½c)
respectively. More specifically, a two-dimensional separable autoregressive spatial structure
(AR1 x Â­ AR1) is sometimes assumed for the common errors in a field trial analysis.</p>

<p><em><strong>The example data:</em></strong></p>

<p>nin89 is from asreml-R library, where different varities were grown in replications / blocks in rectangular field. To control additional variability in row or column direction each plot is referenced as Row and Column variables (row column design). Thus this row column design with blocking. Yield is measured variable. </p>

<p><strong>Example models</strong> </p>

<p>I need something equivalent to the asreml-R codes:</p>

<p>The simple model syntax will look like the follows:</p>

<pre><code> rcb.asr &lt;- asreml(yield âˆ¼ Variety, random = âˆ¼ Replicate, data = nin89)  
 .....model 0
</code></pre>

<p>The linear model is specified in the fixed (required), random (optional) and rcov (error
component) arguments as formula objects.The default is a simple error term and does not need to be formally specified for error term as in the model 0. </p>

<p>here the variety is fixed effect and random is replicates (blocks). Beside random and fixed terms we can specify error term. Which is default in this model 0. The residual or error component of the model is specified in a formula object through the rcov argument, see the following models 1:4. </p>

<p>The following model1 is more complex in which both G (random) and R (error) structure are specified.</p>

<p><strong>Model 1:</strong> </p>

<pre><code>data(nin89)


 # Model 1: RCB analysis with G and R structure
     rcb.asr &lt;- asreml(yield ~ Variety, random = ~ idv(Replicate), 
      rcov = ~ idv(units), data = nin89)
</code></pre>

<p>This model is equivalent to above model 0, and introduces the use of G and R variance model. Here the option random and rcov specifies random and rcov formulae to explicitly specify the G and R structures. where idv() is the special model function in asreml() that identifies the variance model. The expression idv(units) explicitly sets the variance matrix for e to a scaled identity.</p>

<p><em><strong># Model 2: two-dimensional spatial model with correlation in one direction</em></strong></p>

<pre><code>  sp.asr &lt;- asreml(yield ~ Variety, rcov = ~ Column:ar1(Row), data = nin89)
</code></pre>

<p>experimental units of nin89 are indexed by Column and Row. So we expect random variation in two direction - row and column direction in this case. where ar1() is a special function specifying a first order autoregressive variance model for Row. This call specifies a two-dimensional spatial structure for error but with spatial correlation in the row direction only.The variance model for Column is identity (id()) but does not need to be formally
specified as this is the default.</p>

<p><em><strong># model 3: two-dimensional spatial model, error structure in both direction</em></strong></p>

<pre><code> sp.asr &lt;- asreml(yield ~ Variety, rcov = ~ ar1(Column):ar1(Row),  
 data = nin89)
sp.asr &lt;- asreml(yield ~ Variety, random = ~ units, 
 rcov = ~ ar1(Column):ar1(Row), data = nin89)
</code></pre>

<p>similar to above model  2, however the correlation is two direction - autoregressive one. </p>

<p>I am not sure how much of these models are possible with open source R packages. Even if solution of any one of these models will be of great help. <strong><em>Even if the bouty of +50 can stimulate to develop such package will be of great help !</em></strong></p>

<p><em><strong>See MAYSaseen has provided output from each model and data  (as answer)  for comparision.</em></strong> </p>

<p><em><strong>Edits: 
The following is suggestion I received in mixed model discussion forum:</em></strong>
"" You might look at the regress and spatialCovariance packages of David Clifford.  The former allows fitting of (Gaussian) mixed models where you can specify the structure of the covariance matrix very flexibly (for example, I have used it for pedigree data).  The spatialCovariance package uses regress to provide more elaborate models than AR1xAR1, but may be applicable.  You may have to correspond with the author about applying it to your exact problem."" </p>
"
"0.0745938985152613","0.0854704323447285"," 18880","<p>Recently I have opened a question here to understand the output of a GARCH model.
My goal is to understand if the series I'm checking is heteroscedastic or not.</p>

<p>I'm using the <code>garch()</code> function from the <a href=""http://cran.r-project.org/web/packages/tseries/index.html"" rel=""nofollow"">tseries</a> package.</p>

<p>First I built a linear regression like this:</p>

<pre><code>mod &lt;- lm(a ~ b)
</code></pre>

<p>Then I need to check if the residuals of this linear regression present heteroscedasticity.</p>

<p>I did:</p>

<pre><code>g &lt;- garch(resid(mod), order(c(1,1)))
</code></pre>

<p>and then </p>

<pre><code>summary(g)
</code></pre>

<p>I get the follow output:</p>

<pre><code>&gt; summary(g) 

Call: 
garch(x = lm(A ~ B)$resi, order = order(c(1, 1))) 

Model: 
GARCH(1,2) 

Residuals: 
    Min      1Q  Median      3Q    Max 
-4.2058 -1.0262  0.1404  1.1069  3.6553 

Coefficient(s): 
    Estimate  Std. Error  t value Pr(&gt;|t|)    
a0 3.361e-04  9.352e-05    3.594 0.000326 *** 
a1 3.045e-01  4.486e-02    6.787 1.14e-11 *** 
a2 1.209e-06  8.855e-02    0.000 0.999989    
b1 4.938e-01  1.060e-01    4.660 3.17e-06 *** 
--- 
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Diagnostic Tests: 
    Jarque Bera Test 

data:  Residuals 
X-squared = 18.84, df = 2, p-value = 8.108e-05 


    Box-Ljung test 

data:  Squared.Residuals 
X-squared = 49.7251, df = 1, p-value = 1.769e-12 
</code></pre>

<p>Thanks to the user who answered my question, I now understand that the <code>ao</code> is the intercept and the other <code>a1</code> and <code>b1</code> are the coefficients I need to check to understand if this time series is heteroscedastic or not.</p>

<p>The problem (doubt) is that now I also see <code>a2</code> in the regression table: What does it stand for?</p>

<p>Is it correct to say that if all coefficients have a $p$-value above 0.05 (...)</p>
"
"NaN","NaN"," 19082","<p>I have a logistic regression model with several categorical explanatory variables and one interaction term (between two binary variables, named A and B). I know how to calculate the odds ratios for the different levels of A and B (for A=1, e.g., I need to add the coeff for A and coeff for A*B, then exponentiate), but how do I get a confidence interval for this OR? I need to do this in R please, this is the only statistical package I have access to.</p>
"
"0.027972711943223","0.0284901441149095"," 20157","<p>I try to use the <code>gbm.fit()</code> function for a boosted regression tree model implemented in the R package <a href=""http://cran.r-project.org/web/packages/gbm/index.html"" rel=""nofollow"">gbm</a>. To investigate e.g., the bootstrapped prediction error and all other functionalities I want to use the <code>errorest()</code> from the <a href=""http://cran.r-project.org/web/packages/ipred/index.html"" rel=""nofollow"">ipred</a> package. I think <code>errorest()</code> does not accept the <code>gbm</code> output. Is there a workaround? </p>

<p>Sorry, for the missing example. Please, see below</p>

<pre><code>library(ipred)
library(gbm)
data(BostonHousing)
test &lt;- gbm(medv ~ ., distribution = ""gaussian"",  data=BostonHousing)
</code></pre>

<p>I am not sure how to use the result in <code>errorest()</code>. Can someone give me a helping hand? Thanks!</p>
"
"0.128187069873015","0.130558241966773"," 20452","<p>My primary question is how to interpret the output (coefficients, F, P) when conducting a Type I (sequential) ANOVA? </p>

<p>My specific research problem is a bit more complex, so I will break my example into parts. First, if I am interested in the effect of spider density (X1) on say plant growth (Y1) and I planted seedlings in enclosures and manipulated spider density, then I can analyze the data with a simple ANOVA or linear regression. Then it wouldn't matter if I used Type I, II, or III Sum of Squares (SS) for my ANOVA. In my case, I have 4 replicates of 5 density levels, so I can use density as a factor or as a continuous variable. In this case, I prefer to interpret it as a continuous independent (predictor) variable. In R I might run the following:</p>

<pre><code>lm1 &lt;- lm(y1 ~ density, data = Ena)
summary(lm1)
anova(lm1)
</code></pre>

<p>Running the anova function will make sense for comparison later hopefully, so please ignore the oddness of it here. The output is:</p>

<pre><code>Response: y1
          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
density    1 0.48357 0.48357  3.4279 0.08058 .
Residuals 18 2.53920 0.14107 
</code></pre>

<p>Now, let's say I suspect that the starting level of inorganic nitrogen in the soil, which I couldn't control, may have also significantly affected the plant growth. I'm not particularly interested in this effect but would like to potentially account for the variation it causes. Really, my primary interest is in the effects of spider density (hypothesis: increased spider density causes increased plant growth - presumably through reduction of herbivorous insects but I'm only testing the effect not the mechanism). I could add the effect of inorganic N to my analysis. </p>

<p>For the sake of my question, let's pretend that I test the interaction density*inorganicN and it's non-significant so I remove it from the analysis and run the following main effects:</p>

<pre><code>&gt; lm2 &lt;- lm(y1 ~ density + inorganicN, data = Ena)
&gt; anova(lm2)
Analysis of Variance Table

Response: y1
           Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
density     1 0.48357 0.48357  3.4113 0.08223 .
inorganicN  1 0.12936 0.12936  0.9126 0.35282  
Residuals  17 2.40983 0.14175 
</code></pre>

<p>Now, it makes a difference whether I use Type I or Type II SS (I know some people object to the terms Type I &amp; II etc. but given the popularity of SAS it's easy short-hand). R anova{stats} uses Type I by default. I can calculate the type II SS, F, and P for density by reversing the order of my main effects or I can use Dr. John Fox's ""car"" package (companion to applied regression). I prefer the latter method since it is easier for more complex problems.</p>

<pre><code>library(car)
Anova(lm2)
            Sum Sq Df F value  Pr(&gt;F)  
density    0.58425  1  4.1216 0.05829 .
inorganicN 0.12936  1  0.9126 0.35282  
Residuals  2.40983 17  
</code></pre>

<p>My understanding is that type II hypotheses would be, ""There is no linear effect of x1 on y1 given the effect of (holding constant?) x2"" and the same for x2 given x1. I guess this is where I get confused. <strong>What is the hypothesis being tested by the ANOVA using the type I (sequential) method above compared to the hypothesis using the type II method?</strong></p>

<p>In reality, my data is a bit more complex because I measured numerous metrics of plant growth as well as nutrient dynamics and litter decomposition. My actual analysis is something like:</p>

<pre><code>Y &lt;- cbind(y1 + y2 + y3 + y4 + y5)
# Type II
mlm1 &lt;- lm(Y ~ density + nitrate + Npred, data = Ena)
Manova(mlm1)

Type II MANOVA Tests: Pillai test statistic
        Df test stat approx F num Df den Df  Pr(&gt;F)    
density  1   0.34397        1      5     12 0.34269    
nitrate  1   0.99994    40337      5     12 &lt; 2e-16 ***
Npred    1   0.65582        5      5     12 0.01445 * 


# Type I
maov1 &lt;- manova(Y ~ density + nitrate + Npred, data = Ena)
summary(maov1)

          Df  Pillai approx F num Df den Df  Pr(&gt;F)    
density    1 0.99950     4762      5     12 &lt; 2e-16 ***
nitrate    1 0.99995    46248      5     12 &lt; 2e-16 ***
Npred      1 0.65582        5      5     12 0.01445 *  
Residuals 16                                           
</code></pre>
"
"0.0884574820723792","0.0900937462695559"," 20613","<p>Hello statistical gurus and R programming wizards,</p>

<p>I am interested in modeling animal captures as a function of environmental conditions and day of the year. As part of another study, I have counts of captures on ~160 days over three years. On each of these days I have temperature, rainfall, windspeed, relative humidity, etc. Because the data was collected repeatedly from the same 5 plots, I use plot as a random effect.</p>

<p>My understanding is that nlme can easily account for temporal autocorrelation in the residuals but doesn't handle non-Gaussian link functions like lme4 (which can't handle the autocorrelation?). Currently, I think it might work to use the nlme package in R on log(count). So my solution right now would be to run something like:</p>

<pre><code>m1 &lt;- lme(lcount ~ AirT + I(AirT^2) + RainAmt24 + I(RainAmt24^2) + RHpct + windspeed + 
      sin(2*pi/360*DOY) + cos(2*pi/360*DOY), random = ~1|plot, correlation =
      corARMA(p = 1, q = 1, form = ~DOY|plot), data = Data)
</code></pre>

<p>where DOY = Day of the Year. There may be more interactions in the final model, but this is my general idea. I could also potentially try to model the variance structure further with something like </p>

<pre><code>weights = v1Pow
</code></pre>

<p>I'm not sure if there is a better way to do with with a Poisson mixed model regression or anything? I just found mathematical discussion in Chapter 4 of ""Regression Models for Time Series Analysis"" by Kedem and Fokianos. It was a bit beyond me at the moment, especially in application (coding it in R). I also saw a MCMC solution in Zuur et al. Mixed Effects Models book (Chp 23) in the BUGS language (using winBUGS or JAG). Is that my best option? Is there an easy MCMC package in R that would handle this? I'm not really familiar with GAMM or GEE techniques but would be willing to explore these possibilities if people thought they'd provide better insight. <strong>My main objective is to create a model to predict animal captures given environmental conditions. Secondarily, I would like to explain what the animals a responding to in terms of their activity.</strong></p>

<p>Any thoughts on the best way to proceed (philosophically), how to code this in R, or in BUGS would be appreciated. I'm fairly new to R and BUGS (winBUGS) but am learning. This is also the first time I've ever tried to address temporal autocorrelation.</p>

<p>Thanks,
Dan</p>
"
"0.0395593886064618","0.040291148201269"," 20890","<p>Sorry to ask a stupid question, but I have problems using the package segmented.</p>

<p>My linear regression is very simple, between offer and demand:</p>

<blockquote>
  <p>linearModel &lt;- lm(demand~offer)</p>
</blockquote>

<p>And so should be my model using ""segmented"":</p>

<blockquote>
  <p>piecewiseModel &lt;- segmented(lm(demand~offer), seg.Z = ~ offer, psi = NA)</p>
</blockquote>

<p>But I have an error message and I really cannot find a reason for this:</p>

<pre><code>Error in seg.lm.fit(y, XREG, Z, PSI, weights, offs, opz) : 
  (Some) estimated psi out of its range
</code></pre>

<p>Here are my data:</p>

<pre><code>demand  offer
1155    39.3
362 23.5
357 22.4
111 6.1
703 35.9
494 35.5
410 23.2
63  9.1
616 27.5
468 28.6
973 41.3
235 16.9
180 18.2
69  9
305 28.6
106 12.7
155 11.8
422 27.9
44  21.6
1008    45.9
225 11.4
321 16.6
1001    40.7
531 22.4
143 17.4
251 14.3
216 14.6
57  6.6
146 10.6
226 14.3
169 3.4
32  5.1
75  4.1
102 4.1
4   1.7
68  7.5
102 7.8
462 22.6
295 8.6
196 7.7
50  7.8
739 34.7
287 15.6
226 18.5
706 35
127 16.5
85  11.3
234 7.7
153 14.8
4   2
373 12.4
54  9.2
81  11.8
18  3.9
</code></pre>
"
"0.0559454238864459","0.056980288229819"," 20939","<p>I have a regression with a harmonic effect of day of the year, which interacts with other variables. I am not sure how to interpret the coefficients. My model is:</p>

<pre><code>m1 &lt;- lme(lcount ~ AirT + sin(2*pi/360*DOY) + cos(2*pi/360*DOY) + 
          AirT*sin(2*pi/360*DOY) + AirT*cos(2*pi/360*DOY) + RainAmt + RainAmt*AirT,
          random = ~1|plot))
</code></pre>

<p>I get significant interaction effects of air temperature with the linearized harmonic day of the year (DOY) function. My response variable is the log of animal counts on each day. I want to describe how the effect of air temperature on animal observations changes depending on the day of the year.</p>

<p>Does anyone have suggestions on how I can interpret my beta values and/or how I can visualize the effect? I am using R but am not that skilled. The package I used for analyzing my data is nlme.</p>

<p>EDIT: <strong>My primary goals are (1) to describe the response of animals to environmental variables and (2) to predict future activity periods (i.e. when and under what conditions should a research bother trying to catch these animals).</strong> So if there is a better way to model this data, I would be interested in hearing it (such as cubic splines - see comments below).</p>
"
"0.0395593886064618","0.040291148201269"," 21058","<p>I have some time series data where the measured variable is discrete positive integers (counts). I want to test if there is an upward trend over time (or not). The independent variable (x) is in the range 0-500 and the dependent variable (y) is in the the range 0-8.</p>

<p>I thought that I answer this by fitting a regression of the form <code>y =  floor(a*x + b)</code> using ordinary least squares (OLS).</p>

<p>How would I go about doing this using R (or Python)? Is there an existing package for it, or am I better off writing my own algorithm?</p>

<p>PS: I know this is not the ideal technique, but I need to do a relatively simple analysis that I can actually understand - my background is biology not maths. I know I am violating assumptions about error in measured variable, and independence of measurements over time.</p>
"
"NaN","NaN"," 21506","<p>There are at least three R packages providing some functions to perform a Bayesian selection variable in linear Gaussian regression model: LearnBayes, mombf and BMA.</p>

<p>I would be glad to know some opinions about which one is the best.</p>
"
"0.0395593886064618","0.0201455741006345"," 21715","<p>There are several packages that can apply the Durbin-Watson test for serial correlation. However, I do not see a package that supports the calculation in the case that once has a GLS weighted regression.</p>

<p>For example, CRAN package <a href=""http://cran.r-project.org/web/packages/lmtest/index.html"" rel=""nofollow"">lmtest</a> notes in their <a href=""http://cran.r-project.org/web/packages/lmtest/NEWS"" rel=""nofollow"">changelog</a> that they explicitly do not support weighted regressions (yet). Before the recent release, lmtest would not throw an error when passing weighted regressions. </p>

<p>My concern is that the other dwtest packages may also not be explicitly dealing with this scenario.</p>
"
"NaN","NaN"," 22346","<p>How can one obtain standardized (fixed effect) regression weights from a multilevel regression?</p>

<p>And, as an ""add-on"": What is the easiest way to obtain these standardized weights from a <code>mer</code>-object (from the <code>lmer</code> function of the <code>lme4</code>package in <code>R</code>)?</p>
"
"0.0685188709827532","0.0697863157798853"," 22392","<p>I am learning logistic regression modeling using the book ""Applied Logistic Regression"" by Hosmer.</p>

<p>In chpaters, he suggested using Fractional Polynomials for fitting continuous variable which does not seems to be related to logit in linear fashion. I tried the <code>mfp</code> package and can give exactly the same verbose as the book. </p>

<p>But I don't know how to write the transformed variable based on the output of fractional polynomials. The book only shows example of the transformed variable when $J=2$ with $p_1=0$ and $p_2=-0.5$ (page 101) and when $J=2$ with $p_1=2$ and $p_2=2$ (page 101), But what about the others? Currently my case is $J=2$ with $p_1=-1$ and $p_2=-1$.</p>

<p>I know little about fractional polynomials and the book seems not giving sufficient hits on this part. Can anyone refer me to some place which I can know how to write the polynomial? Thanks.</p>
"
"0.10466430427046","0.106600358177805"," 23042","<p>Can someone explain my Cox model to me in plain English? </p>

<p>I fitted the following Cox regression model to <strong>all</strong> of my data using the <code>cph</code> function. My data are saved in an object called <code>Data</code>. The variables <code>w</code>, <code>x</code>, and <code>y</code> are continuous; <code>z</code> is a factor of two levels. Time is measured in months. Some of my patients are missing data for variable <code>z</code> (<em>NB</em>: I have duly noted Dr. Harrell's suggestion, below, that I impute these values so as to avoid biasing my model, and will do so in the future).</p>

<pre><code>&gt; fit &lt;- cph(formula = Surv(time, event) ~ w + x + y + z, data = Data, x = T, y = T, surv = T, time.inc = 12)

Cox Proportional Hazards Model
Frequencies of Missing Values Due to Each Variable
Surv(time, event)    w    x    y    z 
                0    0    0    0   14 

                Model Tests          Discrimination 
                                            Indexes        
Obs       152   LR chi2      8.33    R2       0.054    
Events     64   d.f.            4    g        0.437    
Center 0.7261   Pr(&gt; chi2) 0.0803    gr       1.548    
                Score chi2   8.07                      
                Pr(&gt; chi2) 0.0891                      

                   Coef    S.E.   Wald Z   Pr(&gt;|Z|)
         w      -0.0133  0.0503    -0.26     0.7914  
         x      -0.0388  0.0351    -1.11     0.2679  
         y      -0.0363  0.0491    -0.74     0.4600  
         z=1     0.3208  0.2540     1.26     0.2067
</code></pre>

<p>I also tried to test the assumption of proportional hazards by using the <code>cox.zph</code> command, below, but do not know how to interpret its results. Putting <code>plot()</code> around the command gives an error message.</p>

<pre><code> cox.zph(fit, transform=""km"", global=TRUE)
            rho chisq      p
 w      -0.1125 1.312 0.2520
 x       0.0402 0.179 0.6725
 y       0.2349 4.527 0.0334
 z=1     0.0906 0.512 0.4742
 GLOBAL      NA 5.558 0.2347
</code></pre>

<hr>

<h3>First Problem</h3>

<ul>
<li>Can someone explain the results of the above output to me in plain English? I have a medical background and no formal training in statistics.</li>
</ul>

<h3>Second Problem</h3>

<ul>
<li><p>As suggested by Dr. Harrell, I would like to internally validate my model by performing 100 iterations of 10-fold cross-validation using the <code>rms</code> package (from what I understand, this would entail building <code>100 * 10 = 1000</code> different models and then asking them to predict the survival times of patients that they had never seen).</p>

<p>I tried using the <code>validate</code> function, as shown.</p>

<pre><code>&gt; v1 &lt;- validate(fit, method=""crossvalidation"", B = 10, dxy=T)
&gt; v1
      index.orig training    test optimism index.corrected  n
Dxy      -0.2542  -0.2578 -0.1356  -0.1223         -0.1320 10
R2        0.0543   0.0565  0.1372  -0.0806          0.1350 10
Slope     1.0000   1.0000  0.9107   0.0893          0.9107 10
D         0.0122   0.0128  0.0404  -0.0276          0.0397 10
U        -0.0033  -0.0038  0.0873  -0.0911          0.0878 10
Q         0.0155   0.0166 -0.0470   0.0636         -0.0481 10
g         0.4369   0.4424  0.6754  -0.2331          0.6700 10
</code></pre>

<p>How do you perform the 100x resampling? I think my above code only performs the cross-validation once.</p></li>
<li><p>I then wanted to know how good my model was at prediction. I tried the following:</p>

<pre><code>&gt; c_index &lt;- abs(v1[1,5])/2 + 0.5
&gt; c_index
[1] 0.565984
</code></pre>

<p>Does this mean that my model is only very slightly better than flipping a coin?</p></li>
</ul>

<h3>Third Problem</h3>

<p>Dr. Harrell points out that I have assumed linearity for the covariate effects, and that the number of events in my sample is just barely large enough to fit a reliable model if all covariate effects happen to be linear.</p>

<ul>
<li>Does this mean that I should include some sort of interaction term in my model? If so, any advice as to what to put?</li>
</ul>
"
"0.0740088392978143","0.0753778361444409"," 23110","<p>I have a data set that's 200k rows X 50 columns.  I'm trying to use a <code>knn</code> model on it but there is huge variance in performance depending on which variables are used (i.e., <code>rsqd</code> ranges from .01 (using all variables) to .98 (using only 5 variables)).</p>

<p>This kind of compounds my problem as now I need to determine <code>k</code> <em>and</em> which variables to use.</p>

<p>Is there a package in R that helps with selecting variables for a <code>knn</code> model, while tuning <code>k</code>?  I've looked at <code>rfe()</code> in <code>caret</code> but it seems to only be built for linear regression, <code>randomforest</code>, naive bayes, etc but no <code>knn</code>.  </p>

<p>As an aside, I've tried manually building a loop to use the caret train function like this:</p>

<pre><code>for(i in 2:50){
knnFit &lt;- train(x[,i],y,...) ## trains model using single variable
}
</code></pre>

<p>My problem is that <code>knnFit$results</code> prints all of the results and <code>knnFit$bestTune</code> only prints the final parameter of <code>k</code>.  </p>

<pre><code>&gt; data1 &lt;- data.frame(col1=runif(20), col2=runif(20), col3=runif(20), col4=runif(20), col5=runif(20))
&gt; bootControl &lt;- trainControl(number = 1)
&gt; knnGrid &lt;- expand.grid(.k=c(2:5))
&gt; set.seed(2)
&gt; knnFit1 &lt;- train(data1[,-c(1)], data1[,1]
+ , method = ""knn"", trControl = bootControl, verbose = FALSE,
+ tuneGrid = knnGrid )
&gt; knnFit1 
20 samples
 4 predictors

No pre-processing
Resampling: Bootstrap (1 reps) 

Summary of sample sizes: 20 

Resampling results across tuning parameters:

  k  RMSE   Rsquared
  2  0.485  0.124   
  3  0.54   0.369   
  4  0.52   0.241   
  5  0.528  0.232   

RMSE was used to select the optimal model using  the smallest value.
The final value used for the model was k = 2. 

&gt; knnFit1$results
      k      RMSE  Rsquared RMSESD RsquaredSD
    1 2 0.4845428 0.1241031     NA         NA
    2 3 0.5401009 0.3690569     NA         NA
    3 4 0.5197262 0.2410814     NA         NA
    4 5 0.5277939 0.2317607     NA         NA

&gt; knnFit1$bestTune
      .k
    1  2
</code></pre>

<p>I need some way to print the RMSE/rsqd/other metric for the best single performing model (i.e., just ""R-Squared: .91"").</p>

<p>Any suggestions?</p>
"
"0.027972711943223","0.0284901441149095"," 23346","<p>I am looking for a good and modern Kernel Regression package in R, which has the following features:</p>

<ol>
<li>It has cross-validation </li>
<li>It can automatically choose the ""optimal"" bandwidth</li>
<li>It doesn't have random effect - i.e. if I run the function at different times on the same data-set, the results should be exactly the same...</li>
</ol>

<p>I am trying ""np"", but I am seeing:</p>

<pre><code>Multistart 1 of 1 |
Multistart 1 of 1 |
...
</code></pre>

<p>It looks like in order to do the optimization, it's doing multiple-random-start optimization ... Am I right?</p>

<p>Could you please give me some pointers?</p>

<p>I did some google search but there are so many packages that do this... I just wanted to find the best/modern one to use...</p>
"
"0.10833784750436","0.110341853688094"," 23795","<p>I am using a relevance vector machine as implemented in the kernlab-package in R, trained on a dataset with 360 continuous variables (features) and 60 examples (also continuous, so it's a relevance vector regression).</p>

<p>I have several datasets with equivalent dimensions from different subjects. Now it works fine for most of the subjects, but with one particular dataset, I get this strange results:</p>

<p>When using leave-one-out cross validation (so I train the RVM and try to subsequently predict one observation that was left out of the training), most of the predicted values are just around the mean of the example-values.
So I really don't get good predictions, but just a slightly different value than the mean.</p>

<p>It seems like the SVM is not working at all;
When I plot the fitted values against the actual values, I see the same pattern; predictions around the mean. So the RVM is not even able to predict the values it was trained on (for the other datasets I get correlations of around .9 between fitted and actual values).</p>

<p>It seems like, that I can at least improve the fitting (so that the RVM is at least able to predict the values it was trained on) by transforming the dependent variable (the example-values), for example by taking the square root of the dependent variable.</p>

<p>so this is the output for the untransformed dependent variable:</p>

<p>Relevance Vector Machine object of class ""rvm"" 
Problem type: regression </p>

<pre><code>Linear (vanilla) kernel function. 

Number of Relevance Vectors : 5 
Variance :  1407.006
Training error : 1383.534902093 
</code></pre>

<p>this, if I first transform the dependent variable by taking the square root:</p>

<p>Relevance Vector Machine object of class ""rvm"" 
Problem type: regression </p>

<pre><code>Linear (vanilla) kernel function. 

Number of Relevance Vectors : 55 
Variance :  1.711355
Training error : 0.89601609 
</code></pre>

<p>How is it, that the RVM-results change so dramatically, just by transforming the dependent variable? And what is going wrong, when an SVM just predicts values around the mean of the dependent variable (even for the values and observations it was trained on)?</p>
"
"0.0807502638519182","0.074019565682974"," 24072","<p>I am running the following unit root test (Dickey-Fuller) on a time series using the <code>ur.df()</code> function in the <code>urca</code> package.</p>

<p>The command is:</p>

<pre><code>summary(ur.df(d.Aus, type = ""drift"", 6))
</code></pre>

<p>The output is:</p>

<pre><code>############################################### 
# Augmented Dickey-Fuller Test Unit Root Test # 
############################################### 

Test regression drift 


Call:
lm(formula = z.diff ~ z.lag.1 + 1 + z.diff.lag)

Residuals:
      Min        1Q    Median        3Q       Max 
-0.266372 -0.036882 -0.002716  0.036644  0.230738 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)  0.001114   0.003238   0.344  0.73089   
z.lag.1     -0.010656   0.006080  -1.753  0.08031 . 
z.diff.lag1  0.071471   0.044908   1.592  0.11214   
z.diff.lag2  0.086806   0.044714   1.941  0.05279 . 
z.diff.lag3  0.029537   0.044781   0.660  0.50983   
z.diff.lag4  0.056348   0.044792   1.258  0.20899   
z.diff.lag5  0.119487   0.044949   2.658  0.00811 **
z.diff.lag6 -0.082519   0.045237  -1.824  0.06874 . 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Residual standard error: 0.06636 on 491 degrees of freedom
Multiple R-squared: 0.04211,    Adjusted R-squared: 0.02845 
F-statistic: 3.083 on 7 and 491 DF,  p-value: 0.003445 


Value of test-statistic is: -1.7525 1.6091 

Critical values for test statistics: 
      1pct  5pct 10pct
tau2 -3.43 -2.86 -2.57
phi1  6.43  4.59  3.78
</code></pre>

<ol>
<li><p>What do the significance codes (Signif. codes) mean? I noticed that some of them where written against: z.lag.1, z.diff.lag.2, z.diff.lag.3 (the ""."" significance code) and z.diff.lag.5 (the ""**"" significance code).</p></li>
<li><p>The output gives me two (2) values of test statistic: -1.7525 and 1.6091. I know that the ADF test statistic is the first one (i.e. -1.7525). What is the second one then?</p></li>
<li><p>Finally, in order to test the hypothesis for unit root at the 95% significance level, I need to compare my ADF test statistic (i.e. -1.7525) to a critical value, which I normally get from a table. The output here seems to give me the critical values through. However, the question is: which critical value between ""tau2"" and ""phi1"" should I use.</p></li>
</ol>

<p>Thank you for your response.</p>
"
"0.0484501583111509","0.0493463771219827"," 24098","<p>I'm trying to perform a knn regression on a sales time series using the kknn package. I can't see how you can restrict the neighbours to past sales only though.</p>

<p>e.g.</p>

<pre><code>Product, Price, Sale Date
A,       Â£100,  2012-01-04
A,       Â£100,  2012-01-05
A,       Â£200,  2012-01-06
</code></pre>

<p>If I want to predict for the second row, it should only consider the first row as a neighbour -- not the third.</p>

<p>Is there a way to do this or am I going to have to write something custom?</p>

<p>If it's going to have to be custom, what's the most efficient way of doing this in R? My dataset is 200,000 rows so I imagine it could be slow going using a loop.</p>

<p>Any help you can offer is appreciated. Thanks</p>
"
"0.0395593886064618","0.040291148201269"," 24193","<p>I am performing a returns analysis. The idea is to regress a time-series of returns on the returns of various asset classes. The beta coefficients must be constrained such that sum of the coefficients is 1 and no coefficient is less than 0 or greater than 1. These beta coefficients can then be interpreted as explaining what % of returns are explained by exposure to the various asset classes.</p>

<p>Are there any packages in R that let me setup the above regression and benefit from the attendant reporting on model fit statistics? Or do I need to do some homework on setting up constrained least squares optimization in R (please provide any references to recommended R packages)?</p>
"
"0.0969003166223018","0.0904683580569682"," 24365","<p>I am using the mlogit package in R to run a multinomial logistic regression on pooled discrete choice data collected using two different questionnaire formats. I want to test whether the format had a significant effect on choices. When I run the basic model I get a result. But when I run the same model with a dummy variable indicating which format the respondents saw, I get an error: ""Error in solve.default(H, g[!fixed]) : Lapack routine dgesv: system is exactly singular""</p>

<p>I was able to replicate the error using Train's Electricity dataset in the mlogit package, setting a dummy based on whether the respondent ID was odd or even:</p>

<pre><code>library(mlogit)
data(""Electricity"", package = ""mlogit"")
Electr &lt;- mlogit.data(Electricity, id = ""id"", choice = ""choice"", 
                      varying = 3:26, shape = ""wide"", sep = """")
Electr$odd.dummy &lt;- ifelse(Electr$id %% 2 == 0, 0, 1) # As example, set dummy if ID is odd
summary(mlogit(choice ~ pf + cl + loc + wk + tod + seas | 0, data=Electr)) # Basic model
summary(mlogit(choice ~ pf + cl + loc + wk + tod + seas + odd.dummy | 0, data=Electr)) # Basic + dummy
summary(mlogit(choice ~ odd.dummy | 0, data=Electr)) # Only dummy
</code></pre>

<p>As with my data, the first model runs, but the second two are singular.</p>

<p>I understand that a result will be singular if there is perfect colinearity between variables, but I don't see how this is the case here.  Respondents were randomly assigned to one format or the other, and the underlying experimental design was the same in both formats, so there shouldn't be any colinearity between the dummy and the other variables.</p>

<p>I would be grateful if someone could explain why adding the dummy leads to a singular result, and even more grateful if they could suggest a solution to avoid it.</p>
"
"0.100857047225074","0.102722675451665"," 24442","<p>I'm running into troubles fitting a polytomous logistic regression model using grouped data. The data are of the form (dput at bottom):</p>

<pre><code>&gt; head(alligator)
    lake  sex  size    food count
1 Hancock male small    fish     7
2 Hancock male small  invert     1
3 Hancock male small reptile     0
4 Hancock male small    bird     0
5 Hancock male small   other     5
6 Hancock male large    fish     4
</code></pre>

<p>And I've tried to fit the model with <code>vglm()</code> from package VGAM:</p>

<pre><code>&gt; result &lt;- vglm(food~lake+size+sex, data=alligator, fam=multinomial, weights=count)
Error in if (max(abs(ycounts - round(ycounts))) &gt; smallno) warning(""converting 'ycounts' to integer in @loglikelihood"") : 
  missing value where TRUE/FALSE needed
In addition: Warning messages:
1: In checkwz(wz, M = M, trace = trace, wzepsilon = control$wzepsilon) :
  96 elements replaced by 1.819e-12
</code></pre>

<p>It was also suggested to look at <code>mlogit()</code> from package <code>globaltest</code> (on Bioconductor), but it does not appear to support grouped data. It obviously doesn't support the <code>weights</code> parameter, but I can't find where the equivalent parameter is documented:</p>

<pre><code>source(""http://bioconductor.org/biocLite.R"")
biocLite(""globaltest"")

result &lt;- mlogit(food~lake+size+sex, weights=count, data=alligator)
Error in mlogit(food ~ lake + size + sex, weights = count, data = alligator) : 
  unused argument(s) (weights = count)
</code></pre>

<p>If anyone could put me down the right path, I'd appreciate it!</p>

<pre><code>&gt; dput(alligator)
structure(list(lake = structure(c(2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 3L, 3L, 
3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 3L, 
3L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 4L, 
4L, 4L, 4L, 4L, 4L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), .Label = c(""George"", ""Hancock"", 
""Oklawaha"", ""Trafford""), class = ""factor""), sex = structure(c(2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L), .Label = c(""female"", 
""male""), class = ""factor""), size = structure(c(2L, 2L, 2L, 2L, 
2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 
2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 
1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 
2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 
1L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 1L, 1L, 1L, 1L), .Label = c(""large"", 
""small""), class = ""factor""), food = structure(c(2L, 3L, 5L, 1L, 
4L, 2L, 3L, 5L, 1L, 4L, 2L, 3L, 5L, 1L, 4L, 2L, 3L, 5L, 1L, 4L, 
2L, 3L, 5L, 1L, 4L, 2L, 3L, 5L, 1L, 4L, 2L, 3L, 5L, 1L, 4L, 2L, 
3L, 5L, 1L, 4L, 2L, 3L, 5L, 1L, 4L, 2L, 3L, 5L, 1L, 4L, 2L, 3L, 
5L, 1L, 4L, 2L, 3L, 5L, 1L, 4L, 2L, 3L, 5L, 1L, 4L, 2L, 3L, 5L, 
1L, 4L, 2L, 3L, 5L, 1L, 4L, 2L, 3L, 5L, 1L, 4L), .Label = c(""bird"", 
""fish"", ""invert"", ""other"", ""reptile""), class = ""factor""), count = c(7L, 
1L, 0L, 0L, 5L, 4L, 0L, 0L, 1L, 2L, 16L, 3L, 2L, 2L, 3L, 3L, 
0L, 1L, 2L, 3L, 2L, 2L, 0L, 0L, 1L, 13L, 7L, 6L, 0L, 0L, 3L, 
9L, 1L, 0L, 2L, 0L, 1L, 0L, 1L, 0L, 3L, 7L, 1L, 0L, 1L, 8L, 6L, 
6L, 3L, 5L, 2L, 4L, 1L, 1L, 4L, 0L, 1L, 0L, 0L, 0L, 13L, 10L, 
0L, 2L, 2L, 9L, 0L, 0L, 1L, 2L, 3L, 9L, 1L, 0L, 1L, 8L, 1L, 0L, 
0L, 1L)), .Names = c(""lake"", ""sex"", ""size"", ""food"", ""count""), class = ""data.frame"", row.names = c(NA, 
-80L))
</code></pre>
"
"0.0969003166223018","0.0986927542439653"," 24857","<p>Much like with regression, handling binary dependent variables in SEM requires special considerations. In particular, some of these are noted on Dave Garson's <a href=""http://faculty.chass.ncsu.edu/garson/PA765/structur.htm"" rel=""nofollow"">Structural Equation Modeling</a> and include:</p>

<blockquote>
  <ol>
  <li><p>Polychoric correlation. LISREL/PRELIS uses polyserial, tetrachoric, and polychoric correlations to create the input correlation matrix,
  combined with ADF estimation (see below), for variables which cannot
  be assumed to have a bivariate normal distribution.</p>
  
  <ul>
  <li>Sample size issue. ADF [Asymptotically distribution-free] estimation in turn requires a very large sample size. Yuan and Bentler (1994)
  found satisfactory estimates only with a sample size of at least 2,000
  and preferably 5,000. Violating this requirement may introduce
  problems greater than treating ordinal data as interval and using ML
  estimation. This is also a reason cited for preferring the Bayesian
  estimation approach to ordinal data taken by Amos since Bayesian
  estimation can handle smaller samples than ML or ADF.</li>
  </ul></li>
  </ol>
</blockquote>

<p>I'm currently trying to use the package <a href=""http://cran.r-project.org/web/packages/sem/index.html"" rel=""nofollow"">sem</a> in R to test my model, and the author of the model suggests using polychoric correlations on <a href=""http://r.789695.n4.nabble.com/Link-functions-in-SEM-td859182.html"" rel=""nofollow"">R-help</a>. The problems are: </p>

<ol>
<li>I don't know what estimation method is being used with these correlations (i.e., ADF or ML). </li>
<li>My sample size is small (N = 173). </li>
<li>I'm not familiar with how to interpret polychoric associations (in the case that it is appropriate for me to use them). All the other variables in my model are continuous in nature. </li>
</ol>

<p>Any help and/or links would be greatly appreciated. I'm also considering using other software like OpenMX, but I'm still reading about how it handles binary data. Help with what other software I might want to use would also be appreciated.</p>
"
"0.0791187772129236","0.080582296402538"," 24948","<p>I have a reasonable understanding of why multicollinearity is a problem is regression models, along the <a href=""http://stats.stackexchange.com/questions/1149/is-there-an-intuitive-explanation-why-multicollinearity-is-a-problem-in-linear-r"">lines</a> of this excellent post.</p>

<p>To summarise my understanding, for a regression model of $y = \alpha + \beta_1x + \beta_2z$ (where $x$ and $z$ are correlated), beta coefficient estimates (as well as being unstable) are difficult to interpret, as a situation where you might increase $z$ without increasing $x$ is unlikely to occur, and not supported by the data.</p>

<p>I understand multicollinearity is less harmful to purely <em>predictive</em> as opposed to explanatory or descriptive models.</p>

<p>I'm interested in another interpretation:</p>

<p><em>If I decided to increase $z$, and let $x$ vary as it pleases in reaction, what would I see happen to $y$, accounting for the fact that $x$ is likely to move with $z$, and also have it's own effect?</em></p>

<p>In other words, accepting the causal interpretation that $x$ and $z$ both cause $y$, and are themselves correlated to some extent (.7 say), how would all three variables move if $z$ is (linearly) increased by some amount?</p>

<p>I've tried to model this sort of thing before, fitting $y = \alpha + \beta_1x + \beta_2z$ (model 1), and $x = \alpha + \beta_1z$ (model 2). Hypothetical increased $z$ values are produced, and resulting $x$ values are predicted with model 2. The hypothetical $x$ and $z$ values are used to predict $y$ using model 1. However this feels very unsatisfactory, complicated simulations are required to capture uncertainty (I used <code>sim</code> in <code>arm</code>). Additionally, my gut tells me that apart from being painfully inelegant, it's a bad idea for other reasons I can't put my finger on.</p>

<ul>
<li>Is such an 'observational'/conditional-when-I-feel-like-it interpretation possible?</li>
<li>Does anyone know of a better method for this interpretation?</li>
<li>Can anyone recommend a paper or <code>R</code> package along these lines?</li>
<li>Is the above multi-model mess at-all valid?</li>
</ul>

<p>I'm aware that a model along the lines of $y = \alpha + \beta_1z$ would yield a similar answer to the two-stage mess above, but would lose information in $x$.</p>

<p>I understand that these ideas are similar to structural equation modelling, but apart from having scant knowledge of SEM, I'm yet to find an <code>R</code> package which allows flexibly extending these models with different link functions for proportional odds models, etc.</p>
"
"0.0161500527703836","0.0164487923739942"," 25282","<p>I want to make a nested logistic regression in R with the package <a href=""http://cran.r-project.org/web/packages/mlogit/index.html"" rel=""nofollow"">mlogit</a>.</p>

<p>I would like to test how producer's decision to enter organisations (14 organisations) or not is affected by different factors.</p>

<p>Producer can be in specific organisation and other according to year. I was advised to use year as separate variables in columns. This is OK. But for my different organizations will it be the same? So if my producer appear in my database several times (6 times) in row, isn't it redundant? </p>

<p>Have you an Idea of the struture of database that I can adopt?</p>

<p>Here is how my data looks like in R:</p>

<pre><code>   Year   Organisation   Member AGE.Member ...
1  2005 Organisation 1 Member 1         37
2  2005 Organisation 1 Member 2         32
3  2005 Organisation 3 Member 3         32
4  2005 Organisation 4 Member 4         35
5  2005 Organisation 2 Member 5         33
6  2005 Organisation 3 Member 6         33

'data.frame':   18 obs. of  4 variables:
 $ ANNEE       : int  2005 2005 2005 2005 2005 2005 2005 2005 2005 2006 2006 2006...
     $ Organisation: Factor w/ 4 levels ""Organisation 1"",..: 1 1 3 4 2 3 2 3 3 2 ...
 $ Member      : Factor w/ 9 levels &quot;Member 1&quot;,&quot;Member 2&quot;,..: 1 2 3 4 5 6 7 8 9 1 ...
     $ AGE.Member  : int  37 32 32 35 33 33 32 32 33 37 ...
</code></pre>
"
"NaN","NaN"," 25284","<p>This is almost dumb and a little embarassing, but I can't figure out how to (or even whether it is possible) to use the <a href=""http://cran.r-project.org/web/packages/plm/index.html"" rel=""nofollow"">plm</a> package in R to run a regression including fixed-effects that do not correspond to the individual observation unit. For instance, I have observations on firms' outcomes, but I need to include sector-specific dummy variables. Of course I would have to ""demean"" these out (because there are so many of them). Is that even possible?</p>
"
"0.0745938985152613","0.0854704323447285"," 25538","<p>I am looking into time series data compression at the moment.</p>

<p>The idea is to fit a curve on a time series of n points so that the maximum deviation of any of the points is not greater than a given threshold. In other words, none of the values that the curve takes at the points where the time series is defined should be ""further away"" than a certain threshold from the actual values.</p>

<p>Till now I have found out how to do nonlinear regression using the least squares estimation method in R (<code>nls</code> function) and other languages, but I haven't found any packages that implement nonlinear regression with the L-infinity norm.</p>

<p>I have found papers on <a href=""http://www.jstor.org/discover/10.2307/2006101?uid=3737864&amp;uid=2&amp;uid=4&amp;sid=21100693651721"">""Non-linear curve fitting in the $L_1$ and $L_{\infty}$ norms""</a>, by Shrager and Hill and <a href=""http://www.dtic.mil/dtic/tr/fulltext/u2/a080454.pdf"">""A linear  programming algorithm  for curve fitting in the $L_{\infty}$ norm""</a>, by Armstrong and Sklar.</p>

<p>I could try to implement this in R for instance, but I first looking to see if this hasn't already been done and that I could maybe reuse it.</p>

<p>I have found a solution that I don't believe to be ""very scientific"": I use nonlinear least squares regression to find the starting values of the parameters which I subsequently use as starting points in the R <code>optim</code> function that minimizes the maximum deviation of the curve from the actual points.</p>

<p>The idea is to be able to find out if this type of curve-fitting is possible on a given time series sequence and to determine the parameters that allow it.</p>
"
"0.0740088392978143","0.0753778361444409"," 25702","<p>I have spent much time looking for a special package that could run the Pesaran(2007) unit root test (which assumes cross-sectional dependence unlike most others) and I have found none. So, I decided to do it manually; however, I don't know where I'm going wrong, because my results are very different from Microsoft Excel's results (in which it is done very easily).</p>

<p>My data frame is made up of 22 countries with 506 observations of daily price indices. Following is the model to run using the Pesaran(2007) unit root test:</p>

<p>(i) With an intercept only</p>

<p>$$\Delta Y_{i,t} = a_i + b_iY_{i,t-1} + c_i\overline{Y}_{t-1} + d_i\Delta\overline{Y}_{t-1}+ e_i\Delta\overline{Y}_{t-2}+ f_i\Delta\overline{Y}_{i,t-1}+ g_i\Delta\overline{Y}_{i,t-2} + \varepsilon_{i,t}$$</p>

<p>where $\overline{Y}$ is the cross-section average of the observations across countries at each time $t$ and $b$ is the coefficient of interest to us because it will allow us to compute the ADF test statistic and then determine whether the process is stationary or not.</p>

<p>I constructed each of these variables in the following way:</p>

<p>$\Delta Y_t$</p>

<pre><code>dif.yt = diff(yt) 
## yt is the object containing all the observations for a specific country 
## (e.g. Australia)
</code></pre>

<p>$Y_{t-1}$</p>

<pre><code>yt.lag.1 = lag(yt, -1)
</code></pre>

<p>$\overline{Y}_{t-1}$</p>

<pre><code>ybar.lag.1 = lag(c(rowMeans(x)), -1) 
## x is the object containing my entire data frame
</code></pre>

<p>$\Delta \overline{Y}_{t-1}$</p>

<pre><code>dif.ybar.lag.1 = diff(ybar.lag.1)
</code></pre>

<p>$\Delta \overline{Y}_{t-2}$</p>

<pre><code>dif.ybar.lag.2 = diff(lag(c(rowMeans(x)), -2))
</code></pre>

<p>$\Delta Y_{t-1}$</p>

<pre><code>dif.yt.lag.1 = diff(yt.lag.1)
</code></pre>

<p>$\Delta Y_{t-2}$</p>

<pre><code>dif.yt.lag.2 = diff(lag(yt, -2)
</code></pre>

<p>After constructing each variable individually, I then run the linear regression</p>

<pre><code>reg = lm(dif.yt ~ yt.lag.1[-1] + ybar.lag.1[-1] + dif.ybar.lag.1 + 
                  dif.ybar.lag.2 + dif.yt.lag.1 + dif.yt.lag.2)
summary(reg)
</code></pre>

<p>It is obvious that the explanatory variables in my regression equation differ in length, so I'd like to know whether there is a way in R to make all the variables of equal length (perhaps with a function).</p>

<p>Also, I'd like to know whether the procedure I used was correct and if there are more optimal ways.</p>
"
"0.0395593886064618","0.040291148201269"," 25817","<p>Is it possible to calculate AIC or BIC values for lasso regression models and other regularized models where parameters are only partially entering the equation.  How does one determine the degrees of freedom?</p>

<p>I'm using R to fit lasso regression models with the <code>glmnet()</code> function from the <code>glmnet</code> package, and I'd like to know how to calculate AIC and BIC values for a model.  In this way I might compare the values with models fit without regularization.  Is this possible to do?</p>
"
"0.0927749898843639","0.0944911182523068"," 25912","<p>I want to estimate a multivariate variance function in R.  That is, I want to allow the variance (as well as the mean) to vary according to some set of independent variables.  </p>

<p>In this particular case, I want to estimate the effects of a set of typical demographic covariates (age, race, education) on the variance of logged wages.  </p>

<p>What is a good way to implement this in R?  Is there a package that simplifies this?</p>

<p>It may be that this is only a search away - but having searched on the R help pages, Google, Rseek, and StackOverflow, I can't find anything relevant under ""variance function"" or similar. </p>

<p>Any suggestions gratefully received. </p>

<hr>

<p>Thanks for your responses -- I will try to clarify my question.</p>

<p>I am working in a maximum likelihood framework.  I can code this by hand from the log-likelihood, but the real data set has a <em>lot</em> of variables and ""optim"" is very slow, so I would like to find a package in R that makes this more computationally efficient.</p>

<p>I start with the log-likelihood for a basic OLS regression:
$$
\text{ln }L = \sum (-\frac{1}{2} (\text{ln }\sigma^2 - \frac{(y - xB)^2}{\sigma^2}))
$$
Then I relax the assumption of constant variance (homoskedasticity) and redefine the variance as:
$$
\sigma^2 = exp(Z*\gamma)
$$
where $Z$ is the matrix of variables affecting $\sigma^2$.  (Exponentiate so that you don't end up with $\sigma^2$ less than zero.)  When I substitute the reparameterization of $\sigma^2$ into the original log-likelihood and code the new log-likelihood function in R, I get this: </p>

<pre><code>ll.normal.vary &lt;- function (par, X, Y, Z) {
  beta  &lt;-par[1:ncol(X)]
  gamma &lt;- par[(ncol(X)+1):(ncol(X)+ncol(Z))]   
  -1/2* sum((Z %*% gamma) + ((Y - X %*% beta)^2)/exp(Z %*% gamma))
}
</code></pre>

<p>Then I optimize:</p>

<pre><code>v.optim1 &lt;- optim (par = start1, fn=ll.normal.vary, X=x.mat, Y=y.vec, Z=z.mat, 
                   method = ""BFGS"", hessian = F, control = list(fnscale = -1))
v.optim1$par
v.optim1$value
</code></pre>

<p>Here are some sample data if you want to test it:</p>

<pre><code>var1   &lt;- c(0,0,0,1,1,0)
var2   &lt;- c(.28, .07, -.05, .38, .08, -.1)
var3   &lt;- c(-.11, -.17, -.17, -.05, .1, -.01)
x.mat  &lt;- cbind(var1, var2, var3)
y.vec  &lt;- c(.46, .77, .49, .59, .60, .44)
z.mat  &lt;- cbind(var1, var2) 
start1 &lt;- rep(0.1, ncol(x.mat)+ncol(z.mat))
</code></pre>

<p>Thanks again for any tips.</p>
"
"0.0395593886064618","0.040291148201269"," 25926","<p>I have a large data set (400k rows X 60 columns) that I'm trying to use to build a knn model.  I'm using the <code>caret</code> package version of <code>knn</code> and the <code>forward.search</code> method from the <code>FSelector</code> package to eliminate variables via cross-validation.  My problem is that once I use more than 20k lines of data I get a message about there being too many ties.</p>

<p>Currently I'm only checking k-values between 1-19 (and only odd #'s as they supposedly shrink risk of ties) and only using variables with > 2 levels.  </p>

<p>Are there any other tweaks to using big chunks of data into a <code>knn</code>?</p>

<p>EDIT: This is regression problem, not a classification problem.</p>
"
"0.104897669787086","0.106838040430911"," 25988","<p>An assumption of the ordinal logistic regression is the proportional odds assumption. Using R and the 2 packages mentioned I have 2 ways to check that but I have questions in each one.</p>

<p>1) Using the rms package</p>

<p>Given the next commands</p>

<pre><code>library(rms)
ddist &lt;- datadist(Ki67,Cyclin_E)
options(datadist='ddist')
f &lt;- lrm(grade ~Ki67+Cyclin_E);f
sf &lt;- function(y)
c('Y&gt;=1'=qlogis(mean(y &gt;= 1)),'Y&gt;=2'=qlogis(mean(y &gt;= 2)),'Y&gt;=3'=qlogis(mean(y &gt;= 3)))
s &lt;- summary(grade ~Ki67+Cyclin_E, fun=sf)
plot(s,which=1:3,pch=1:3,xlab='logit',main='',xlim=c(-2.5,2.5))
</code></pre>

<p>I have</p>

<pre><code>lrm(formula = grade ~ Ki67 + Cyclin_E)

Frequencies of Missing Values Due to Each Variable
   grade     Ki67 Cyclin_E 
       0        0        3 


                     Model Likelihood     Discrimination    Rank Discrim.    
                        Ratio Test            Indexes          Indexes       

Obs            42    LR chi2     11.38    R2       0.268    C       0.728    
 1             11    d.f.            2    g        1.279    Dxy     0.456    
 2             15    Pr(&gt; chi2) 0.0034    gr       3.592    gamma   0.458    
 3             16                         gp       0.192    tau-a   0.308    
max |deriv| 1e-07                         Brier    0.166                     


         Coef    S.E.   Wald Z Pr(&gt;|Z|)
y&gt;=2     -0.1895 0.8427 -0.22  0.8221  
y&gt;=3     -2.0690 0.9109 -2.27  0.0231  
Ki67      0.0971 0.0330  2.94  0.0033  
Cyclin_E -0.0076 0.0227 -0.33  0.7387 
</code></pre>

<p>The <code>s</code> table gives: (unfortunately I don't know how to upload a graph made in R)</p>

<pre><code>grade    N=45

+--------+-------+--+----+---------+----------+
|        |       |N |Y&gt;=1|Y&gt;=2     |Y&gt;=3      |
+--------+-------+--+----+---------+----------+
|Ki67    |[ 2, 9)|12|Inf |0.6931472|-1.0986123|
|        |[ 9,16)|12|Inf |0.3364722|-2.3978953|
|        |[16,24)|10|Inf |2.1972246| 0.0000000|
|        |[24,44]|11|Inf |2.3025851| 1.5040774|
+--------+-------+--+----+---------+----------+
|Cyclin_E|[ 3,16)|15|Inf |1.0116009|-0.1335314|
|        |[16,22)| 7|Inf |1.7917595|-0.9162907|
|        |[22,33)|10|Inf |1.3862944|-0.8472979|
|        |[33,80]|10|Inf |0.4054651|-0.4054651|
|        |Missing| 3|Inf |      Inf| 0.6931472|
+--------+-------+--+----+---------+----------+
|Overall |       |45|Inf |1.1284653|-0.4054651|
+--------+-------+--+----+---------+----------+
</code></pre>

<p>Where for the Ki67 I see that 3 out of the 4 differences  <code>logit(P[Y&gt; = 2])-logit(P[Y&gt; = 3])</code> are close to 2. Only the last one is quite lower (around 0.8). But here Ki67 is continuous and not categorical so I don't know if the results of the table are correct and there isn't any p-value to decide. By the way I run the above in SPSS and I didn't reject the assumption.</p>

<p>2) Using the VGAM package</p>

<p>Here using the next commands I have the model under the assumption of proportional odds</p>

<pre><code>library(VGAM)
fit1 &lt;- vglm(grade ~Ki67+Cyclin_E,family=cumulative(parallel=T))
summary(fit1)
</code></pre>

<p>And the results</p>

<pre><code>Coefficients:
                Estimate Std. Error  z value
(Intercept):1  0.1894723   0.820442  0.23094
(Intercept):2  2.0690395   0.886732  2.33333
Ki67          -0.0970972   0.032423 -2.99467
Cyclin_E       0.0075887   0.021521  0.35261

Number of linear predictors:  2 

Names of linear predictors: logit(P[Y&lt; = 1]), logit(P[Y&lt; = 2])

Dispersion Parameter for cumulative family:   1

Residual deviance: 79.86801 on 80 degrees of freedom

Log-likelihood: -39.93401 on 80 degrees of freedom

Number of iterations: 5 
</code></pre>

<p>While using the next commands I have the model without the assumption of proportional odds</p>

<pre><code>fit2 &lt;- vglm(grade ~Ki67+Cyclin_E,family=cumulative(parallel=F))
</code></pre>

<p>where unfortunately i receice the next message </p>

<blockquote>
  <p>Warning message: In vglm.fitter(x = x, y = y, w = w, offset = offset,
  Xm2 = Xm2,  :   convergence not obtained in 30 iterations</p>
</blockquote>

<p>However if I type <code>summary(fit2)</code> I get results but again I don't know if they are correct. My intention was to use the next commands and get the answer but know I doubt if this is correct (by the way if I do it I get <code>p-value=0.6</code>. </p>

<pre><code>pchisq(deviance(fit1)-deviance(fit2),
df=df.residual(fit1)-df.residual(fit2),lower.tail=FALSE)
</code></pre>

<p>So, regarding the methods mentioned above does anyone knows whether the results I get are valid or in the case of the VGAM package is there any way to increase the number of itterations?Is there any other way to check it? </p>
"
"NaN","NaN"," 26084","<p>If i do a modelLookup('glmnet') it says TRUE for probModel (and in fact, I'd expect it to be usable as a model to predict probabilities in a binary outcome prediction problem as glmnet has a 'binomial' family argument).</p>

<p>However, following the instructions from the caret package I say:</p>

<pre><code>trainControl = trainControl(classProbs=TRUE)

modelFit = train(X, y, method='glmnet', trControl=trainControl)
</code></pre>

<p>and I get:</p>

<pre><code>cannot compute class probabilities for regression
</code></pre>

<p>Am I doing something wrong?</p>
"
"0.0323001055407673","0.0493463771219827"," 26178","<p>I want to create a classification table regarding an ordinal response variable with three levels but I don't know how to do it. Searching on the site I fell on the question posted by Brandon Bertelsen that covers only the case of the binary logistic regression (link at the end of the post).Does anyone knows how I can create such that table in my case?</p>

<p>I don't know if it is important but I used the <code>rms</code> package to run the olr and using the <code>predict(fit,type=""fitted.ind"")</code> command I get the next table with probability for each case</p>

<pre><code>      grade=1    grade=2   grade=3
1  0.08042197 0.28380601 0.6357720
2  0.08086877 0.28475584 0.6343754
3  0.41472656 0.40802584 0.1772476
4  0.39680650 0.41484517 0.1883483
5  0.25402385 0.43644283 0.3095333
6  0.13539881 0.37098177 0.4936194
7  0.12591996 0.35959459 0.5144855
8  0.50489952 0.36489760 0.1302029
9          NA         NA        NA
10 0.34757283 0.42969971 0.2227275
11 0.24690054 0.43539812 0.3177013
12 0.17325212 0.40529586 0.4214520
13 0.45795712 0.38900855 0.1530343
14 0.03594015 0.16033637 0.8037235
15         NA         NA        NA
16 0.50188652 0.36653955 0.1315739
17 0.48710163 0.37441720 0.1384812
18 0.38094725 0.42028884 0.1987639
19 0.04134659 0.17894428 0.7797091
20 0.12844729 0.36275605 0.5087967
21 0.23991274 0.43410413 0.3259831
22 0.20506362 0.42316514 0.3717712
23 0.45457929 0.39061326 0.1548075
24         NA         NA        NA
25 0.31269786 0.43606610 0.2512360
26 0.20905830 0.42483513 0.3661066
27 0.05240710 0.21353381 0.7340591
28 0.26569967 0.43759072 0.2967096
29 0.21258621 0.42621415 0.3611996
30 0.11407246 0.34347156 0.5424560
31 0.34656138 0.42993750 0.2235011
32 0.01813256 0.08978609 0.8920813
33 0.44034224 0.39716470 0.1624931
34 0.12213714 0.35468488 0.5231780
35 0.40888783 0.41032190 0.1807903
36 0.33901842 0.43161582 0.2293658
37 0.13275554 0.36793345 0.4993110
38 0.32091057 0.43492411 0.2441653
39 0.45984161 0.38810515 0.1520532
40 0.55550665 0.33564053 0.1088528
41 0.02812293 0.13122652 0.8406505
42 0.46250424 0.38681892 0.1506768
43 0.07352751 0.26852580 0.6579467
44 0.04330967 0.18541327 0.7712771
45 0.45457929 0.39061326 0.1548075
</code></pre>

<p><a href=""http://stats.stackexchange.com/questions/4832/logistic-regression-classification-tables-a-la-spss-in-r"">Logistic Regression: Classification Tables a la SPSS in R</a></p>
"
"0.0897122608032513","0.0913717355809759"," 26180","<p>I run an ordinal regression model and I wanted to check the proportional odds assumption. In order to do that I used the <code>VGAM</code> package and I run olr twice, the first under the assumption and the second without the assumption. Below is the code and the results</p>

<pre><code>&gt; fit1 &lt;- vglm(stage ~Ki67+Cyclin_E,family=cumulative(parallel=T))
&gt; summary(fit1)

Call:
vglm(formula = stage ~ Ki67 + Cyclin_E, family = cumulative(parallel = T))

Pearson Residuals:
                     Min       1Q  Median      3Q    Max
logit(P[Y&lt; = 1]) -3.1177 -0.43593 0.37246 0.53111 1.4927
logit(P[Y&lt; = 2]) -3.8479  0.14119 0.18785 0.28679 1.9903

Coefficients:
                Estimate Std. Error  z value
(Intercept):1  2.2414705   1.091225  2.05409
(Intercept):2  3.2164214   1.178916  2.72829
Ki67          -0.1157273   0.039889 -2.90124
Cyclin_E       0.0085266   0.028626  0.29786

Number of linear predictors:  2 

Names of linear predictors: logit(P[Y&lt; = 1]), logit(P[Y&lt; = 2])

Dispersion Parameter for cumulative family:   1

Residual deviance: 50.82946 on 62 degrees of freedom

Log-likelihood: -25.41473 on 62 degrees of freedom

Number of iterations: 5 


&gt; fit2 &lt;- vglm(grade ~Ki67+Cyclin_E,family=cumulative(parallel=F),maxit=50)
&gt; summary(fit2)

Call:
vglm(formula = grade ~ Ki67 + Cyclin_E, family = cumulative(parallel = F), 
    maxit = 50)

Pearson Residuals:
                     Min       1Q   Median      3Q    Max
logit(P[Y&lt; = 1]) -1.1870 -0.65271 -0.23199 0.44910 3.4798
logit(P[Y&lt; = 2]) -2.6235 -0.70599  0.27305 0.72691 2.8544

Coefficients:
               Estimate Std. Error   z value
(Intercept):1 -0.059702   0.928078 -0.064328
(Intercept):2  2.687277   1.050909  2.557097
Ki67:1        -0.100832   0.047754 -2.111483
Ki67:2        -0.101817   0.036567 -2.784377
Cyclin_E:1     0.018768   0.022708  0.826474
Cyclin_E:2    -0.015416   0.022927 -0.672390

Number of linear predictors:  2 

Names of linear predictors: logit(P[Y&lt; = 1]), logit(P[Y&lt; = 2])

Dispersion Parameter for cumulative family:   1

Residual deviance: 78.93318 on 78 degrees of freedom

Log-likelihood: -39.46659 on 78 degrees of freedom

Number of iterations: 34 
</code></pre>

<p>In order to check if the difference of the two models is significant I run the next command</p>

<pre><code>pchisq(deviance(fit2)-deviance(fit1),df=df.residual(fit2)-df.residual(fit1),lower.tail=FALSE)
[1] 0.03072927
</code></pre>

<p>As you see the result is that the 2 models differ and so the proportional odds assumption isn't true. But if you see the coefficients about the Ki67 (Cyclin is not significantly important so i guess i can skip it) they are almost the same. In that case what should I do? I believe that I could stick with the model under the po assumption but I'd like to know what others think</p>
"
"NaN","NaN"," 26236","<p>Is it possible to use <code>zelig</code> for beta regression (as in the <code>betareg</code> package in R)? I could not find any documentation so I'm guessing it is not implemented. Is there anything that precludes its use for beta reg?</p>
"
"0.10833784750436","0.102985730108887"," 26500","<p>Hello after struggling with using R for the last couple of days I was hoping someone could help me with a statistical analysis I am completing for an environmental science honours project. Using R statistics is not something we have been taught and I am worried that I may have bitten of more then I can chew, however my whole project is based around the <strong>hierarchical partitioning method and the exhaustive search multiple regression analysis method.</strong></p>

<p>The <a href=""http://cran.r-project.org/web/packages/hier.part/index.html"" rel=""nofollow"">hier.part</a> package was installed along with <a href=""http://cran.r-project.org/web/packages/gtools/index.html"" rel=""nofollow"">gtools</a>.</p>

<p>I have converted my dataset to a .csv file with seven independent variables and one dependant variable with around 400 replicates (my intention is to do this analysis on eight datasets in total with different amounts of replicates and another dependant variable, but I am starting with this one). The dependant variable is GPP, the independent variables are, NDVI, Temperature, Precipitation, Solar Radiation, Nutrient Availability and Soil Available Water Capacity.</p>

<p>Secondly I imported the .csv file into R using the script</p>

<pre><code>GPPANDDRIVER &lt;- read.table(""C:\\etc, header=T, sep="","")
</code></pre>

<p>This works fine and I can edit the table using </p>

<pre><code>edit(GPPANDDRIVER)
</code></pre>

<p>After looking at the <code>hier.part</code> package documentation available <a href=""http://cran.r-project.org/web/packages/hier.part/hier.part.pdf"" rel=""nofollow"">here</a> it seems like I need to define Y which in the script below is the dependent variable and define <code>scan</code> which is the independent variables (mentioned before).</p>

<pre><code>hier.part(y, xcan, family = ""gaussian"", gof = ""RMSPE"", barplot = TRUE)
</code></pre>

<p>I was defining the dependant <code>y</code> vector as </p>

<pre><code>y &lt;- as.vector(GPPANDDRIVER[""GPP""])
</code></pre>

<p>This also works fine and I have my y vector. However I am not sure how to load independent variables onto the xcan dataframe part of the script. I have tried typing in two scripts but they have not worked.</p>

<pre><code>xcan &lt;- as.vector(GPPANDDRIVER[-GPP])
## AND
xcan &lt;- data.frame(GPPANDDRIVER[-GPP])
</code></pre>

<p>If anyone could help me find the right script for representing my independant variables as xcan that would be greatly appreciated. Also once defined if I entered in the hier.part script mentioned above would R then show me results of the analysis after processing? I will be moving onto to the regression analysis after this if anyone can shed some light on this first problem.</p>

<pre><code>*information on hier.part arguments.*

**Arguments**

y a vector containing the dependent variables

xcan a dataframe containing the n independent variables

family family argument of glm

gof Goodness-of-fit measure. Currently ""RMSPE"", Root-mean-square â€™predictionâ€™

error, ""logLik"", Log-Likelihood or ""Rsqu"", R-squared

print.vars if FALSE, the function returns a vector of goodness-of-fit measures. If TRUE, a data frame is returned with first column listing variable combinations and the
second column listing goodness-of-fit measures.
</code></pre>
"
"0.027972711943223","0.0284901441149095"," 26528","<p>I want to use Lasso or ridge regression for a model with more than 50,000 variables. I want do so using software package in R. How can I estimate the shrinkage parameter ($\lambda$)? </p>

<p>Edits:</p>

<p>Here is the point I got up to:</p>

<pre><code>set.seed (123)
Y &lt;- runif (1000)
Xv &lt;- sample(c(1,0), size= 1000*1000,  replace = T)
X &lt;- matrix(Xv, nrow = 1000, ncol = 1000)

mydf &lt;- data.frame(Y, X)

require(MASS)
lm.ridge(Y ~ ., mydf)

plot(lm.ridge(Y ~ ., mydf,
              lambda = seq(0,0.1,0.001)))
</code></pre>

<p><img src=""http://i.stack.imgur.com/9xPJ6.jpg"" alt=""enter image description here""></p>

<p>My question is: How do I know which $\lambda$ is best for my model?</p>
"
"0.0791187772129236","0.080582296402538"," 26558","<p>Suppose you are analyzing a huge data set at the tune of billions of observations per day, where each observation has a couple thousand sparse and possibly redundant numerical and categorial variables. Let's say there is one regression problem, one unbalanced binary classification problem, and one task of ""find out which predictors are most important."" My thought for how to approach the problem is:</p>

<p>Fit some predictive model on progressively larger and larger (random) sub-samples of the data until:</p>

<ol>
<li><p>Fitting and cross-validating the model becomes computationally difficult (e.g., unreasonbly slow on my laptop, R runs out of memory, etc.), OR</p></li>
<li><p>The training and test RMSE or precision/recall values stabilize.</p></li>
</ol>

<p>If the training and test errors did not stabilize (1.), use a simpler model and/or implement multicore or multinode versions of the model and restart from the beginning. </p>

<p>If the training and test errors stabilized (2.):</p>

<ul>
<li><p>If $N_{subset} \ll N$ (i.e., I can still run algorithms on $X_{subset}$ as it's not too large yet), try to improve performance by expanding the feature space or using a more complex model and restarting from the beginning.</p></li>
<li><p>If $N_{subset}$ is 'large' and running further analyses is costly, analyze variable importance and end.</p></li>
</ul>

<p>I plan to use packages like <code>biglm</code>, <code>speedglm</code>, <code>multicore</code>, and <code>ff</code> in R initially, and later use more complicated algorithms and/or multinode (on EC2) as necessary.</p>

<p>Does this sound like a reasonable approach, and if so, do you have any specific advice or suggestions? If not, what would you try instead for a data set of this size?</p>
"
"0.0740088392978143","0.0753778361444409"," 26568","<p>I would like to understand how to generate <em>prediction intervals</em> for logistic regression estimates. </p>

<p>I was advised to follow the procedures in Collett's <em>Modelling Binary Data</em>, 2nd Ed p.98-99. After implementing this procedure and comparing it to R's <code>predict.glm</code>, I actually think this book is showing the procedure for computing <em>confidence intervals</em>, not prediction intervals.</p>

<p>Implementation of the procedure from Collett, with a comparison to <code>predict.glm</code>, is shown below.</p>

<p>I would like to know: how do I go from here to producing a prediction interval instead of a confidence interval?</p>

<pre><code>#Derived from Collett 'Modelling Binary Data' 2nd Edition p.98-99
#Need reproducible ""random"" numbers.
seed &lt;- 67

num.students &lt;- 1000
which.student &lt;- 1

#Generate data frame with made-up data from students:
set.seed(seed) #reset seed
v1 &lt;- rbinom(num.students,1,0.7)
v2 &lt;- rnorm(length(v1),0.7,0.3)
v3 &lt;- rpois(length(v1),1)

#Create df representing students
students &lt;- data.frame(
    intercept = rep(1,length(v1)),
    outcome = v1,
    score1 = v2,
    score2 = v3
)
print(head(students))

predict.and.append &lt;- function(input){
    #Create a vanilla logistic model as a function of score1 and score2
    data.model &lt;- glm(outcome ~ score1 + score2, data=input, family=binomial)

    #Calculate predictions and SE.fit with the R package's internal method
    # These are in logits.
    predictions &lt;- as.data.frame(predict(data.model, se.fit=TRUE, type='link'))

    predictions$actual &lt;- input$outcome
    predictions$lower &lt;- plogis(predictions$fit - 1.96 * predictions$se.fit)
    predictions$prediction &lt;- plogis(predictions$fit)
    predictions$upper &lt;- plogis(predictions$fit + 1.96 * predictions$se.fit)


    return (list(data.model, predictions))
}

output &lt;- predict.and.append(students)

data.model &lt;- output[[1]]

#summary(data.model)

#Export vcov matrix 
model.vcov &lt;- vcov(data.model)

# Now our goal is to reproduce 'predictions' and the se.fit manually using the vcov matrix
this.student.predictors &lt;- as.matrix(students[which.student,c(1,3,4)])

#Prediction:
this.student.prediction &lt;- sum(this.student.predictors * coef(data.model))
square.student &lt;- t(this.student.predictors) %*% this.student.predictors
se.student &lt;- sqrt(sum(model.vcov * square.student))

manual.prediction &lt;- data.frame(lower = plogis(this.student.prediction - 1.96*se.student), 
    prediction = plogis(this.student.prediction), 
    upper = plogis(this.student.prediction + 1.96*se.student))

print(""Data preview:"")
print(head(students))
print(paste(""Point estimate of the outcome probability for student"", which.student,""(2.5%, point prediction, 97.5%) by Collett's procedure:""))
manual.prediction
print(paste(""Point estimate of the outcome probability for student"", which.student,""(2.5%, point prediction, 97.5%) by R's predict.glm:""))    
print(output[[2]][which.student,c('lower','prediction','upper')])
</code></pre>
"
"0.0791187772129236","0.080582296402538"," 26614","<p>Continuing on from this <a href=""http://stats.stackexchange.com/questions/26329/what-is-a-unit-information-prior"">question</a> and this <a href=""http://stats.stackexchange.com/questions/26339/the-unit-information-prior-and-its-bic-approximation"">question</a> re BIC and its approximation to the Bayes factor with a unit information prior <a href=""http://www.jstor.org/discover/10.2307/2291327?uid=3737536&amp;uid=2&amp;uid=4&amp;sid=47698890289477"" rel=""nofollow"">(Kass &amp; Wasserman, 1995)</a>, I'm trying to quantify this relationship as a stepping stone into Bayesian stats. So far, my calculation of the BIC approximation of the Bayes factor (based upon my impression of <a href=""http://www.jstor.org/discover/10.2307/2291327?uid=3737536&amp;uid=2&amp;uid=4&amp;sid=47698890289477"" rel=""nofollow"">Wagenmakers 2007</a>) is linearly related to my Bayes factor that is calculated from my interpretation of the unit information prior using the <a href=""http://www.r-inla.org/home"" rel=""nofollow"">INLA</a> package in R. Good start! However, my BIC Bayes factor is ~ 3 times smaller than the Bayes factor calculated with INLA and I'm not sure why. </p>

<p>The prior I've used in the ""<strong>inla</strong>"" function is N(0, 1/(variance * n)) and this seems to me the likely place where I'm out. I'm not sure how I got the multiply by n in the formula, but it appears to work... roughly. Kass and Wasserman have N(0, variance / n) which when converted to precision would be N(0, n / variance), but this gives me a less good relationship. </p>

<p>Help based on other Bayesian packages is also welcome.</p>

<p><em>EDIT</em></p>

<pre><code>*Deleted code, see below answer instead*
</code></pre>

<p><em>EDIT</em></p>

<p>So I'm pretty sure I've figured out the one sample case. I would still appreciate help for the two sample case and the regression case (which I'll start working on now).</p>
"
"NaN","NaN"," 27297","<p>I'm using the <code>logistf</code> package in R to perform Firth logistic regression on an unbalanced dataset. I have a logistf object:</p>

<pre><code>fit = logistf(a~b)
</code></pre>

<p>Is there a <code>predict()</code> function like on that's used in the <code>lm</code> class to predict probabilities for future data points? Or do I have to manually input the estimated parameters from the Firth regression.</p>
"
"0.0839181358296689","0.0664770029347888"," 27351","<p>I want to compare three models, one linear-regression-model, one regression-tree-model (from <code>rpart</code>) and one MARS-model (from <code>mda</code> package).</p>

<p>I want to compare the models using a <em>leave one out cross validation</em> using the mean square error and MAPE. I have the following implementation in R:</p>

<pre><code>library(data.table)
library(rpart)
library(mda)

#Load Sample-Data
data(trees)

#The following models should be compared:
# lm(Volume~Girth+Height, data=trees)
# rpart(Volume~Girth+Height, data=trees)
# mars(trees[,-3], trees[3])

LOOCV&lt;-function(modelCall) {
  unlist(sapply(seq(1,nrow(trees)), function(i) {         
    training=trees[-i,]
    test=trees[i,]

    fit=eval(modelCall)
    testValue = predict(fit, test[1:2])

    test[3]-testValue
  }))
}

LOOCV_MSE&lt;-function(modelCall) {
   sum(LOOCV(modelCall)^2)/nrow(trees)
}

LOOCV_RMSE&lt;-function(modelCall) {
   sqrt(LOOCV_MSE(modelCall))
}

LOOCV_MAPE&lt;-function(modelCall) {
  sum(abs(LOOCV(modelCall)/sapply(seq(1, nrow(trees)), function(i) {trees[i,3]})))/nrow(trees)*100                                    
}


cat(""Cross-Validation Metrics:\n"")
cat(""-------------------------\n"")
cat(""LOOCV MSE for LM:"", LOOCV_MSE(quote(lm(Volume~Girth+Height, data=training))),""\n"")
cat(""LOOCV MSE for CART:"", LOOCV_MSE(quote(rpart(Volume~Girth+Height, data=training))),""\n"")
cat(""LOOCV MSE for MARS:"", LOOCV_MSE(quote(mars(training[,-3], training[3]))),""\n"")
cat(""\n"")

cat(""LOOCV RMSE for LM:"", LOOCV_RMSE(quote(lm(Volume~Girth+Height, data=training))),""\n"")
cat(""LOOCV RMSE for CART:"", LOOCV_RMSE(quote(rpart(Volume~Girth+Height, data=training))),""\n"")
cat(""LOOCV RMSE for MARS:"", LOOCV_RMSE(quote(mars(training[,-3], training[3]))),""\n"")
cat(""\n"")

cat(""LOOCV MAPE for LM:"", LOOCV_MAPE(quote(lm(Volume~Girth+Height, data=training))),""\n"")
cat(""LOOCV MAPE for CART:"", LOOCV_MAPE(quote(rpart(Volume~Girth+Height, data=training))),""\n"")
cat(""LOOCV MAPE for MARS:"", LOOCV_MAPE(quote(mars(training[,-3], training[3]))),""\n"")
</code></pre>

<p>Outputs:</p>

<pre><code>Cross-Validation Metrics:
-------------------------
LOOCV MSE for LM: 18.15783 
LOOCV MSE for CART: 69.83769 
LOOCV MSE for MARS: 13.72282 

LOOCV RMSE for LM: 4.2612 
LOOCV RMSE for CART: 8.356895 
LOOCV RMSE for MARS: 3.704432 

LOOCV MAPE for LM: 14.6114 
LOOCV MAPE for CART: 23.51401 
LOOCV MAPE for MARS: 10.00316 
</code></pre>

<p>Does this implementation make sense? When whould using MSE on the errors make sense? When would I use MAPE/SMAPE instead? I already read ""<a href=""http://stats.stackexchange.com/questions/13478/metric-to-compare-models"">Metric to compare models?</a>"" and the conclusion there was <em>it depends</em>, can someone explain this further. On what does it depend?</p>

<p>My data is not a time series, it is more like the <code>tree</code> example data. </p>
"
"0.0484501583111509","0.0493463771219827"," 27621","<p>I used R <a href=""http://cran.r-project.org/web/packages/mvpart/index.html"" rel=""nofollow"">mvpart</a> package to create a multivariate regression tree. This is part of the output:</p>

<pre><code>          CP               nsplit         rel error    xerror       xstd
1       0.02717093      0         1.0000000 1.0005358 0.03481409
2       0.01302184      2         0.9456581 0.9521266 0.03306820

Node number 1: 3479 observations,    complexity param=0.02717093
Means=12.94,0.5749,9.375,0.72,1.611,0.973,2.153,0.6209,3.307,3.702,2.422,0.3837,1.499,     Summed MSE=1305.19 
left son=2 (992 obs) right son=3 (2487 obs)
Primary splits:
  Dag           splits as  RRLLRRR, improve=0.02478172, (0 missing)
  Hoofdberoep    splits as  RLRLLRRLLLLRLL, improve=0.02313676, (0 missing)
  Ploegenstelsel splits as  RRLRRRL, improve=0.02191660, (0 missing)
  Werksituatie   splits as  LRLR, improve=0.02179270, (0 missing)
  Werkuren       splits as  RLRRRR, improve=0.02130351, (0 missing)
</code></pre>

<p>How do I have to interpret the different 'improve values' (0.02478172,0.02313676,...) and how are they related to the complexity parameter (0.02717093)?</p>
"
"0.0395593886064618","0.040291148201269"," 28438","<p>I'd like to do rank prediction (something very similar with regression) based on historical data, is there any package I can use in R ? 
Here's my problems:</p>

<p>I have a historical data of sports games, and all the rank of each team and some statistic of these teams, I'd like to use these data to predict these teams' next year rank.</p>

<p>Thanks</p>
"
"0.115334445976885","0.110557998564556"," 28492","<p>For fun, I tried to replicate the results of <a href=""http://rpproxy.iii.com:9797/MuseSessionID=248c435aa056d82d70d390e949c628fb/MuseHost=rfs.oxfordjournals.org/MusePath/content/22/1/435.abstract"" rel=""nofollow"">Petersen (2009)</a> who deals with the correct estimation of standard errors in finance panel data sets. </p>

<p>In a nutshell, he estimates the following standard regression for a panel data set:</p>

<p>$$
Y_{it} = X_{it} \beta + \epsilon_{it}
$$ </p>

<p>where $\epsilon_{it} = \gamma_i + \eta_{it}$ and $x_{it} = \mu_{i} + \nu_{it}$. Hence, both the residual and the independent variable have a firm-specific component. Petersen goes on to show that this results in biased standard errors when applying the standard OLS. For example, he shows in table 1 of his paper that if both the residual volatility and the variable volatility are driven by 50% by a firm-specific component, the true standard errors are nearly twice as large as the ones given by OLS.</p>

<p>He shows that in a MCS and I reproduced those results in R, as you can see from the code below. Naturally, I asked myself how I would compute the correct standard errors in R and the package of choice seemed to be <code>plm</code>. However, I just don't get the correct results out of it and I don't know what I miss.</p>

<p>Here is my code:</p>

<pre><code>library(plm)
runMCS &lt;- function(runs, nrN, nrT, fracFirmX, fracFirmEps, sd_X, sd_eps, beta) {

  betas    &lt;- numeric(runs)
  se_betas &lt;- numeric(runs)
  panel_betas    &lt;- numeric(runs)
  se_panel_betas &lt;- numeric(runs)

  for (i in 1:runs) {

    #Model epsilon, X, and Y
    eps &lt;- rep(rnorm(nrN, 
                     mean=0, 
                     sd = sd_eps * sqrt(fracFirmEps)), 
               each=nrT) + 
                 rnorm(nrN * nrT, mean=0, sd = sd_eps * sqrt(1-fracFirmEps))
    X   &lt;- rep(rnorm(nrN, 
                     mean=0, 
                     sd = sd_X   * sqrt(fracFirmX)),   
               each=nrT) + 
                 rnorm(nrN * nrT, mean=0, sd = sd_X   * sqrt(1-fracFirmX))
    Y   &lt;- beta * X + eps

    #Compute regression (OLS)
    reg &lt;- summary(lm(Y ~ X))

    #Save results
    betas[i]    &lt;- reg$coef[2, 1]
    se_betas[i] &lt;- reg$coef[2, 2]

    #Try plm
    df &lt;- data.frame(Firm = rep(1:nrN, each=nrT),
                     Time = rep(1:nrT, times=nrN),
                     Y = Y,
                     X = X)
    preg &lt;- summary(plm(Y ~ X, data=df, index=c(""Firm"", ""Time""), effect=""individual"", model=""within"")) #within is fixed effects
    panel_betas[i]    &lt;- preg$coef[1, 1]
    se_panel_betas[i] &lt;- preg$coef[1, 2]
  }

  return(c(avg_beta = mean(betas), 
           true_se = sd(betas), 
           avg_se = mean(se_betas), 
           avg_clustered = mean(panel_betas),
           se_clustered = mean(se_panel_betas)))

}
MCS_50_50 &lt;- runMCS(50, 500, 10, 0.5, 0.5, 1, 2, 1)
MCS_50_50
     avg_beta       true_se        avg_se avg_clustered  se_clustered 
   1.00503955    0.06020203    0.02825567    1.00433092    0.02985546
</code></pre>

<p>Note that I only run the simulation 50 times here because the plm function slows it down considerably. So basically, it makes virtually no difference if I call <code>lm</code> or <code>plm</code>. I'm pretty confident that I set the <code>index</code> and <code>model</code> option correct after reading the vignette of the package. However, I must miss something here! Interestingly, the package also has the <code>fixef</code> function and if I call that on one run, I get something like  this:</p>

<pre><code>summary(fixef(plm(Y ~ X, data=df, index=c(""Firm"", ""Time""), effect=""individual"", model=""within"")))
1      13.60377     0.44112    30.8391 &lt; 2.2e-16 ***
2    -830.74707     0.44136 -1882.2236 &lt; 2.2e-16 ***
3    -326.96042     0.44137  -740.7840 &lt; 2.2e-16 ***
4     169.16463     0.44246   382.3287 &lt; 2.2e-16 ***
...
</code></pre>

<p>I'm not quite sure how to interpret those results, but here, I get considerably larger standard errors for each firm separately. If I would average those, I would end up with something above 0.44 which is considerably closer to the true standard errors, but still not right.</p>

<p>So, again a very long question from me, sorry for that ;-) Note that I did check answers before and I found this interesting <a href=""http://stats.stackexchange.com/questions/10017/standard-error-clustering-in-r-either-manually-or-in-plm"">link</a>. The white paper that is referred to in the answer is interestingly the same person that implemented the solution on Petersen's <a href=""http://www.kellogg.northwestern.edu/faculty/petersen/htm/papers/se/se_programming.htm"" rel=""nofollow"">webpage</a>. So I'm pretty sure that I could get the correct standard errors by implementing Mahmood Arai's solution. But I'm looking for an already implemented and therefore safe option and I just wonder why that plm function does not work.</p>
"
"0.0839181358296689","0.0854704323447285"," 28732","<p>I am using the randomForest package in R (R version 2.13.1, randomForest version 4.6-2) for regression and noticed a significant bias in my results: the prediction error is dependent on the value of the response variable. High values are under-predicted and low values are over-predicted. At first I suspected this was a consequence of my data but the following simple example suggests that this is inherent to the random forest algorithm:</p>

<pre><code>n = 1000; 
x1 = rnorm(n, mean = 0, sd = 1)
response = x1
predictors = data.frame(x1=x1) 
rf = randomForest(x=predictors, y=response)
error = response-predict(rf, predictors)
plot(x1, error)
</code></pre>

<p>I suspect the bias is dependent on the distribution of the response, for example, if <code>x1</code> is uniformly-distributed, there is no bias; if <code>x1</code> is exponentially distributed, the bias is one-sided. Essentially, the values of the response at the tails of a normal distribution are outliers. It is no surprise that a model would have difficulty predicting outliers. In the case of randomForest, a response value of extreme magnitude from the tail of a distribution is less likely to end up in a terminal leaf and its effect will be washed out in the ensemble average.</p>

<p>Note that I tried to capture this effect in a previous example, ""RandomForest in R linear regression tails mtry"". This was a bad example. If the bias in the above example is truly inherent to the algorithm, it follows that a bias correction could be formulated given the response distribution one is trying to predict, resulting in more accurate predictions.  </p>

<p>Are tree-based methods, such as random forest, subject to response distribution bias? If so, is this previously known to the statistics community and how is it usually corrected (e.g. a second model that uses the residuals of the biased model as input)?</p>

<p>Correction of a response-dependent bias is difficult because, by nature, the response is not known. Unfortunately, the estimate/predicted response does not often share the same relationship to the bias.</p>
"
"0.027972711943223","0.0284901441149095"," 29044","<p>R and Statistics newbie here.</p>

<p>Ok, I have a logistic regression and have used the predict function to develop a probability curve based on my estimates. </p>

<pre><code>## LOGIT MODEL:
library(car)
mod1 = glm(factor(won) ~ as.numeric(bid), data=mydat, family=binomial(link=""logit""))

## PROBABILITY CURVE:
all.x &lt;- expand.grid(won=unique(won), bid=unique(bid))
y.hat.new &lt;- predict(mod1, newdata=all.x, type=""response"")
plot(bid&lt;-000:1000,predict(mod1,newdata=data.frame(bid&lt;-c(000:1000)),type=""response""), lwd=5, col=""blue"", type=""l"")
</code></pre>

<p>This is great but I'm curious about plotting the confidence intervals for the probabilities. I've tried plot.ci() but had no luck. Can anyone point me to some ways to get this done, preferably with the car package or base R.</p>

<p>Thanks.</p>
"
"NaN","NaN"," 29357","<p>I am searching tutorial for support vector regression in R. I found <a href=""http://www.jstatsoft.org/v15/i09/paper"">this</a> and <a href=""http://cran.r-project.org/web/packages/e1071/e1071.pdf"">manual for e1071 package</a>. But there is few explanation how to set parameters, like choose kernels, choose regression, not classification. Any material is appreciated.</p>
"
"0.0969003166223018","0.0986927542439653"," 29653","<p>The likelihood ratio (a.k.a. deviance) $G^2$ statistic and lack-of-fit (or goodness-of-fit) test is fairly straightforward to obtain for a logistic regression model (fit using the <code>glm(..., family = binomial)</code> function) in R. However, it can be easy to have some cell counts end up low enough that the test is unreliable. One way to verify the reliability of the likelihood ratio test for lack of fit is to compare its test statistic and <em>P</em>-value to those of Pearson's chi square (or $\chi^2$) lack-of-fit test.</p>

<p>Neither the <code>glm</code> object nor its <code>summary()</code> method report the test statistic for Pearson's chi square test for lack of fit. In my search, the only thing I came up with is the <code>chisq.test()</code> function (in the <code>stats</code> package): its documentation says ""<code>chisq.test</code> performs chi-squared contingency table tests and goodness-of-fit tests."" However, the documentation is sparse on how to perform such tests:</p>

<blockquote>
  <p>If <code>x</code> is a matrix with one row or column, or if <code>x</code> is a vector and <code>y</code> is not given, then a <em>goodness-of-fit</em> test is performed (<code>x</code> is treated as a one-dimensional contingency table). The entries of <code>x</code> must be non-negative integers. In this case, the hypothesis tested is whether the population probabilities equal those in <code>p</code>, or are all equal if <code>p</code> is not given.</p>
</blockquote>

<p>I'd imagine that you could use the <code>y</code> component of the <code>glm</code> object for the <code>x</code> argument of <code>chisq.test</code>. However, you can't use the <code>fitted.values</code> component of the <code>glm</code> object for the <code>p</code> argument of <code>chisq.test</code>, because you'll get an error: ""<code>probabilities must sum to 1.</code>""</p>

<p>How can I (in R) at least calculate the Pearson $\chi^2$ test statistic for lack of fit without having to run through the steps manually?</p>
"
"0.0634361479695551","0.0753778361444409"," 30005","<p>I am using PCA on foreign exchange return series to find a market ""beta"". I am using 10 years of daily data with a 2-year half life weighting in the PCA using the package FactoMineR's PCA function. I extract the first principal component return series (so the product of the first eigenvector and the returns matrix) and I want to regress that against each foreign exchange return vector to find the residuals, that is, to find each currency's returns independently of the market beta. </p>

<p>Should I use the same 2 year half-life weighting in the regressions? Will this ""double up"" the weighting somehow? Conversely if I don't weight the regression, will I implicitly be putting too much weight on PC1 returns that are less ""relevant""?</p>

<p>For what it's worth market participants tend psychologically to put a higher weight on recent than long past currency behaviour. </p>

<p>Thanks for the help.</p>
"
"0.08882529023711","0.0904683580569682"," 30061","<p>What approaches exist to observe the time lag between two variables?</p>

<p>I need to analyze the relationship between blood pressure and some other factor, such as exercise. The data set I am drawing from has around 1800 individuals, with an average of 100 entries a piece. It is generally known that there is a strong relationship between exercise level and blood pressure. However, if a person increases their steps to 8000+ a day, how long will it take for their blood pressure to drop? I am new to this type of analysis, and this is a challenge I have been thinking about for weeks. </p>

<p>I don't know if anyone wants to comment on possible approaches to this challenge or any issues surrounding it.</p>

<p>Some issues I have been dealing with:</p>

<ol>
<li><p>Is it better to treat this as a times series analysis or longitudinal data analysis?</p>

<p>My understanding is that time series usually focuses on one variable with no missing data and is observed at consistent intervals, where as longitudinal is over a longer period and has inconsistent time intervals, dropouts, and missing data.</p>

<p>The data I have seems to fit the longitudinal description more, but it also seems like time series could be used if I averaged the values by week so there would be no missing entries. I'm not sure about the pros and cons of each approach.</p></li>
<li><p>Should I be fitting a causal model, or would some other method like regression be more helpful?</p>

<p>I've been looking at various possible causal models, for example Marginal Structural Models (MSM) or Structural Nested Models (SNM), but there seem to be very little information on their application. I did find one R package that applied inverse probability weights and then used Cox proportional hazards regression model on a survival object (MSM), but that seemed to be focus on weighting for confounding and right censoring. Its result was a correlation coefficient, which I don't think helps me.</p>

<p>So I'm not sure if fitting a causal model is what I want, because that seems to be more focused on the making intellectually satisfying assumptions about relationships within the data and then determining the degree of causality, rather than providing information about time lag.</p>

<p>If anyone knows about MSM, SNM, their use in R, or how they might relate to this problem, that would be awesome to hear.</p></li>
<li><p>What about survival analysis or SEM?</p>

<p>I haven't explored these options very in-depth yet but they sound potentially relevant.</p></li>
</ol>

<p>I've kind of stalled, so any hints about what direction I might want to go would be really appreciated. </p>

<p>Thanks in advance.</p>
"
"NaN","NaN"," 30243","<p>I've recently embarked on fitting regression mixed models in the Bayesian framework, using a MCMC algorithm (function MCMCglmm in R actually).</p>

<p>I believe I have understood how to diagnose convergence of the estimation process (trace, geweke plot, autocorrelation, posterior distribution...).</p>

<p>One of the thing that strikes me in the Bayesian framework is that much effort seems to devoted to do those diagnostics, whereas very little appears to be done in terms of checking the residuals of the fitted model. For instance in MCMCglmm the residual.mcmc() function does exist but is actually not yet implemented (ie.returns: ""residuals not yet implemented for MCMCglmm objects""; same story for predict.mcmc()). It seems to be lacking from other packages too, and more generally is little discussed in the literature I've found (apart from DIC which is quite heavily discussed too).</p>

<p>Could anyone point me to some useful references, and ideally R code I could play with or modify? </p>

<p>Many thanks.</p>
"
"0.0884574820723792","0.0810843716426003"," 30451","<p>I'm trying to build a model that would describe some process of payment and distribution of payments in time. I believe that time of payment has <a href=""http://en.wikipedia.org/wiki/L%C3%A9vy_distribution"" rel=""nofollow"">Levy distribution</a> with probability density function:</p>

<p>$ f(x,c)=\sqrt{\frac{c}{2\pi}}~~\frac{e^{ -\frac{c}{2x}}} {x^{3/2}} $</p>

<p>This distribution depends on parameter <em>c</em> which actually defines the shape of distribution. My task is to build model that explains dependency of this parameter on some explaining variables. I'm trying linear dependency $c = \sum_i \beta_i x_i$ </p>

<p>This is example of generalized linear model and is implemented in the <a href=""http://cran.r-project.org/web/packages/VGAM/index.html"" rel=""nofollow"">VGAM package</a> in R. Problem is that in a sample for building this model I have data only from some period at the beginning and this period is different for different groups of cases. And because of that I can not just to run the model in VGAM package on this data as the result will be incorrect significantly exaggerating probability of early payments.</p>

<p>One possible solution I can think about is to change the likelihood function from which parameter is estimated. If we have information only from time up to <em>t</em> and as cumulative distribution function of Levy distribution is:</p>

<p>$ F(x,c)=\textrm{erfc}\left(\sqrt{c/2x}\right) $</p>

<p>the density of distribution up to time <em>t</em>  is  $ f_1(x,c,t)= \frac{f(x,c)}{F(t,c)} $ (where $f(x,c), F(t,c)$ defined as above). This new density functions can be used in estimating regression parameters with maximum likelyhood method. But can it be done in R using methods from VGAM package or usual <strong>glm</strong> function or some other packages? Or there are some better approaches to my problem? I'm interested in implementation in R.</p>

<p>Thank you in advance for any help!</p>
"
"NaN","NaN"," 31299","<p>I have a dependent variable made up of 3 categories and 14 binary predictor variables.</p>

<p>I have tried using <code>mlogit</code> and <code>nnet/multinom</code> packages in R. </p>

<p><strong>Is there a better approach than multinomial logistic regression for this particular scenario?</strong></p>
"
"0.0740088392978143","0.0646095738380922"," 31494","<p>I'm very new to all this, and I am testing different ways to perform a two-way type III ANOVA on my data.</p>

<ul>
<li>I have tried <code>anova()</code> from the <code>stats</code> package, after fitting a linear regression with <code>lm()</code>;</li>
<li>I have tried <code>Anova()</code> from the <code>car</code> package, using the same linear regression (and this gives me the same result as <code>anova()</code> when I use <code>type=""II""</code> - I thought <code>anova()</code> used type I SS by default?).</li>
<li>And I am now trying to use <code>ezANOVA()</code> from the <code>ez</code> package.</li>
</ul>

<p>With this last one, I can't understand what the <code>wid=.()</code> argument is (even reading the help), and as it is not optional, I can't leave it blank. What I am trying to use is as follows, with its result:</p>

<pre><code>&gt; attach(data)
&gt; library(""ez"")
&gt; ezANOVA(data=data, dv=.(AG.DW), wid=.(), within=.(Genotype, Treatment), type=3)
Warning: Converting """" to factor for ANOVA.
Error in sort.list(y) : 'x' must be atomic for 'sort.list'
Have you called 'sort' on a list?
</code></pre>

<p>Is this the right script? What is <code>wid</code> and what should I fill it with?</p>

<p>Concerning my data, the columns <code>Genotype</code> and <code>Treatment</code> are my two factors, and I want to see if there is an interaction when looking at the aboveground dry weight of my plants (column <code>AG.DW</code>). My data is balanced.</p>

<p>I am sorry if information is missing or inaccurate here: this is my first contribution here, and I am only discovering statistics at the moment (and I can't see how to join my data file).</p>
"
"NaN","NaN"," 32239","<p>As described in Merlo et al (<a href=""http://www.ncbi.nlm.nih.gov/pubmed/16537344"" rel=""nofollow"">J Epidem Comm Health 2006</a>), the 95% credible interval for MOR is calculated using MCMC. MOR is defined as $\exp(\sqrt{2\sigma^2}\times 0.675)$, where $\sigma$ is the level-2 variance of the random intercept $u$ from a null model of a hierarchical logistic regression.  </p>

<p>Does anyone have an idea of how to write a program for an Markov chain Monte Carlo to calculate the standard error of the  median odds ratio (MOR) using <a href=""http://cran.r-project.org/web/packages/rjags/index.html"" rel=""nofollow"">rjags</a>?<br>
My dependent variable is outcome(alive/dead) and the clustering (level2)variable is Hospital. There are 140 hospitals and would like to see variations in outcome between hospitals. Other risk factors will be included later as independent level1 variables.</p>
"
"0.0570990591522943","0.0697863157798853"," 32313","<p>I have a linear regression model that is used to forecast the 'afluent natural energy' (ANE) of some region.</p>

<p>The predictors for this model are:</p>

<ul>
<li>the previous month ANE (<code>ANE0</code>)</li>
<li>the previous month rain volume (<code>PREC0</code>)</li>
<li>the current month forecast for rain volume (<code>PREC1</code>)</li>
</ul>

<p>We have 7 years of historical data for all of these variables, for each month. The current model just runs a OLS linear regression. I feel there's a lot of improvements to be done, but i'm not a time series specialist.</p>

<p>The first thing I notice is that the predictors are highly correlated (multicollinearity).
I'm not certain of the impacts of multicollinearity on prediction confidence.</p>

<p>I decided to try a time series approach, so I ran a ACF and PACF on the historic data:
The ACF shows a sine wave pattern, and the PACF has a spike at 1 and 2. So I tried both <code>ARIMA (2, 0, 0)</code> and <code>ARIMA(2,0,1)</code> to predict 20 periods ahead.</p>

<p>The ARIMA(2,0,1) shows good results, but I'm not certain as to how to compare it to the linear regression model.</p>

<p>What's the best way to test the performance of these model?  I'm using R as analysis tool (together with the <code>forecast</code> package). </p>
"
"NaN","NaN"," 32641","<p>Is there an R package for constrained regression, that has both a formula interface and a <code>predict</code> method?  Two existing options are <a href=""http://rgm2.lab.nig.ac.jp/RGM2/func.php?rd_id=mgcv%3apcls"" rel=""nofollow"">pcls</a> from <a href=""http://cran.r-project.org/web/packages/mgcv/index.html"" rel=""nofollow"">mgcv</a> and <a href=""http://rgm2.lab.nig.ac.jp/RGM2/func.php?rd_id=nnls%3annls"" rel=""nofollow"">nnls</a> from <a href=""http://cran.r-project.org/web/packages/nnls/index.html"" rel=""nofollow"">nnls</a>, but neither one appears to have a formula interface or a predict method.</p>
"
"0.0484501583111509","0.0493463771219827"," 32657","<p>I was playing with the <a href=""http://cran.r-project.org/web/packages/TSA/index.html"" rel=""nofollow"">TSA</a> package in R and wanted to test the <code>arimax</code> function to the solution provided in Pankratz's <em>Forecasting with Dynamic Regression Models</em>, chapter 8. The savings rate and the function seems to provide similar results as the ones in the book except for the IO weights which are quite different. I bet there is a transformation that I might be missing.</p>

<p>Any help on understanding why IO coefficients are so different would be appreciated...</p>

<p>the solution states </p>

<pre><code>AO @ t=82,43,89
LS @ t=99
IO @ t=62,55
</code></pre>

<p>with Parameters estimates</p>

<pre><code>C = 6.1635
w82 = 2.3346
w99 = -1.5114
w43 = 1.1378
w62 = 1.4574
w55 = -1.4915
w89 = -1.0702
AR1 = 0.7976
MA2 = -0.3762
</code></pre>

<p>To fit the model in R, I used
(<code>saving</code> is the data)</p>

<pre><code>arimax(saving, order = c(1,0,2), fixed=c(NA,0,NA,NA,NA,NA,NA,NA,NA,NA), io=c(55,62), 
       xreg=data.frame(AO82=1*(seq(saving)==82),
                       AO43=1*(seq(saving)==43),
                       AO89=1*(seq(saving)==89),
                       LS99=1*(seq(saving)&gt;=99)),
       method='ML')
</code></pre>

<p>The savings rate data is (100 points)</p>

<p>4.9
5.2
5.7
5.7
6.2
6.7
6.9
7.1
6.6
7
6.9
6.4
6.6
6.4
7
7.3
6
6.3
4.8
5.3
5.4
4.7
4.9
4.4
5.1
5.3
6
5.9
5.9
5.6
5.3
4.5
4.7
4.6
4.3
5
5.2
6.2
5.8
6.7
5.7
6.1
7.2
6.5
6.1
6.3
6.4
7
7.6
7.2
7.5
7.8
7.2
7.5
5.6
5.7
4.9
5.1
6.2
6
6.1
7.5
7.8
8
8
8.1
7.6
7.1
6.6
5.6
5.9
6.6
6.8
7.8
7.9
8.7
7.7
7.3
6.7
7.5
6.4
9.7
7.5
7.1
6.4
6
5.7
5
4.2
5.1
5.4
5.1
5.3
5
4.8
4.7
5
5.4
4.3
3.5</p>

<p>here it is my output</p>

<pre><code>&gt; arimax(saving, order = c(1,0,2),fixed=c(NA,0,NA,NA,NA,NA,NA,NA,NA,NA),io=c(55,62),xreg=data.frame(AO82=1*(seq(saving)==82),
+ AO43=1*(seq(saving)==43),AO89=1*(seq(saving)==89),LS99=1*(seq(saving)&gt;=99)),method='ML')

Call:
arimax(x = saving, order = c(1, 0, 2), xreg = data.frame(AO82 = 1 * (seq(saving) == 
    82), AO43 = 1 * (seq(saving) == 43), AO89 = 1 * (seq(saving) == 
    89), LS99 = 1 * (seq(saving) &gt;= 99)), fixed = c(NA, 0, NA, NA, NA, NA, 
    NA, NA, NA, NA), method = ""ML"", io = c(55, 62))

Coefficients:
         ar1  ma1     ma2  intercept    AO82    AO43     AO89     LS99    IO-55   IO-62
      0.7918    0  0.3406     6.0628  2.3800  1.1297  -1.0466  -1.4885  -0.5958  0.5517
s.e.  0.0674    0  0.1060     0.3209  0.3969  0.3780   0.3835   0.5150   0.4044  0.3772

sigma^2 estimated as 0.2611:  log likelihood = -75.57,  aic = 169.14
</code></pre>
"
"0.0745938985152613","0.0854704323447285"," 32735","<p>As a financial institution, we often run into analysis of time series data. A lot of times we end up doing regression using time series variables. As this happens, we often encounter residuals with time series structure that violates basic assumption of independent errors in OLS regression.  Recently we are building another model in which I believe we have regression with autocorrelated errors.The residuals from linear model have <code>lm(object)</code> has clearly a AR(1) structure, as evident from ACF and PACF.  I took two different approaches, the first one was obviously to fit the model using Generalized least squares <code>gls()</code> in R. My expectation was that the residuals from gls(object) would be a white noise (independent errors).  But the residuals from <code>gls(object)</code> still have the same ARIMA structure as in the ordinary regression. Unfortunately there is something wrong in what I am doing that I could not figure out.  Hence I decided to manually adjust the regression coefficients from the linear model (OLS estimates). Surprisingly that seems to be working when I plotted the residuals from adjusted regression (the residuals are white noise). I really want to use <code>gls()</code> in <code>nlme</code> package so that coding will be lot simpler and easier. What would be the approach I should take here? Am I supposed to use REML? or is my expectation of non-correlated residuals (white noise) from gls() object wrong?</p>

<pre><code>gls.bk_ai &lt;- gls(PRNP_BK_actINV ~ PRM_BK_INV_ENDING + NPRM_BK_INV_ENDING, 
                 correlation=corARMA(p=1), method='ML',  data  = fit.cap01A)

gls2.bk_ai  &lt;- update(gls.bk_ai, correlation = corARMA(p=2))

gls3.bk_ai &lt;- update(gls.bk_ai, correlation = corARMA(p=3))

gls0.bk_ai &lt;- update(gls.bk_ai, correlation = NULL)

anova(gls.bk_ai, gls2.bk_ai, gls3.bk_ai, gls0.bk_ai)  
     ## looking at the AIC value, gls model with AR(1) will be the best bet

acf2(residuals(gls.bk_ai)) # residuals are not white noise
</code></pre>

<p>Is there something wrong with what I am doing???????</p>
"
"0.0971882825368556","0.106600358177805"," 33463","<p>I've got a dataset for Temperature &amp; KwH and I'm currently performing the regression below. (further regression based on coeffs is performed within PHP)</p>

<pre><code># Some kind of List structure..
UsageDataFrame  &lt;- data.frame(Energy, Temperatures);

# lm is used to fit linear models. It can be used to carry out regression,
# single stratum analysis of variance and analysis of covariance (although
# aov may provide a more convenient interface for these).
LinearModel     &lt;- lm(Energy ~ 1+Temperatures+I(Temperatures^2), data = UsageDataFrame)

# coefficients
Coefficients    &lt;- coefficients(LinearModel)

system('clear');

cat(""--- Coefficients ---\n"");
print(Coefficients);
cat('\n\n');
</code></pre>

<p>The issue comes with our data, we can't ensure there isn't random communication failures or just random errors. This can leave us with values like</p>

<pre><code>Temperatures &lt;- c(16,15,13,18,20,17,20);
Energy &lt;- c(4,3,3,4,0,60,4)

Temperatures &lt;- c(17,17,14,17,21,16,19);
Energy &lt;- c(4,3,3,4,0,0,4)
</code></pre>

<p>Now as humans we can clearly see that the 60 for Kwh is a mistake based on the temperature, however we have over 2,000 systems each with multiple meters and each in different locations all over the country.. and with different levels of normal Energy usage.</p>

<p>A normal dataset would be 48 values for both Temperatures &amp; Energy per day, per meter. In a full year its likely we could have around 0-500 bad points per meter out of 17520 points.</p>

<p>I've read other posts about the <code>tawny</code> package however I've not really seen any examples which would me to pass a <code>data.frame</code> and it process them through cross analysis.</p>

<p>I understand not much can be done, however big massive values surely could be stripped based on the temperature? And the number of times it occurs..  </p>

<p>Since R is maths based I see no reason to move this into any other language.</p>

<p>Please note: I'm a Software Developer and have never used R before.</p>

<p>-- Edit --</p>

<p>Okay here's a real world example, seems this meter is a good example. You can see the Zeros are building up then a massive value is inserted. ""23, 65, 22, 24"" being examples of this. This happens when its in comms failure and it holds the data value and continues to add it up on the device. </p>

<p>(Just to say the comms failures are out of my hands nor can I change the software)</p>

<p>However because Zero is a valid value im wanting to remove any massive numbers against the temperatures or Zeros where its clear they are an Error.</p>

<p>The thought of detecting this and averaging the data back isn't a fix for this either, however it was discussed but since this meter data is every 30mins and comms failures can happen for days.</p>

<p>Most systems are using more Energy then this so its perhaps a bad example from a removing Zero's point of view.</p>

<p>Energy: <a href=""http://pastebin.com/gBa8y5sM"" rel=""nofollow"">http://pastebin.com/gBa8y5sM</a>
Temperatures: <a href=""http://pastie.org/4371735"" rel=""nofollow"">http://pastie.org/4371735</a></p>

<p>(Pastebin seems to have gone down for me after posting such a big file)</p>
"
"0.027972711943223","0"," 33964","<p>I have some repeated measures with 196 individuals and some of my regressions have a plateau. So I want to characterize this with an eventual breakpoint for each ID.</p>

<p>Example: subset of my personal data</p>

<pre><code> ID     time   y
7G009   0       9
7G009   108,33  13
7G009   185,69  16
7G009   309,22  20
7G009   515,08  21
7G051   0       10
7G051   108,33  14
7G051   185,69  19
7G051   309,22  23
7G051   515,08  25
8S027   0       8
8S027   108,33  13
8S027   185,69  17
8S027   309,22  22
8S027   515,08  23
</code></pre>

<p>I have tested with the <code>strucchange</code> package (<code>breakpoint()</code>), or with the <code>segmented</code> package (<code>segemented()</code>). I have also tried with the <code>siZer</code> package (<code>piecewise.linear()</code>); itâ€™s OK with one ID, but I am stuck when I want to deal with all my IDs.</p>

<p>My first attempt was with using the code in the review named <a href=""http://ftp.csie.ntu.edu.tw/pub/R/CRAN/doc/Rnews/Rnews_2008-1.pdf#page=20"" rel=""nofollow"">segmented: An R Package to Fit Regression Models with Broken-Line Relationships</a> by Vito M. R. Muggeo. Cf page 23: </p>

<pre><code>&gt; data(""plant"")
&gt; attach(plant)
&gt; X&lt;-model.matrix(~0+group)*time
&gt; time.KV&lt;-X[,1]
&gt; time.KW&lt;-X[,2]
&gt; time.WC&lt;-X[,3]
</code></pre>

<p>But I am stuck when I try to use my 196 different explanatory variables.  Can anybody can tell me how deal with the segmented package or offer any other solution?  </p>
"
"0.0395593886064618","0.040291148201269"," 34088","<p>Could you recommend an R package for estimating a (frequentist) multilevel Weibull regression model? </p>

<p>I need to model random intercepts, random slopes, as well as a cross-classified structure. </p>

<p><strong>UPDATE:</strong>
It seems like there is currently no ""easy"" solution for that. I decided to leave it for now by estimating a multilevel discrete hazard model with <code>glmer</code> and a multilevel Cox PH model with <code>coxme</code> (proposed by EddieMcGoldrick). With regards to the latter, I have still to figure out if implementing a cross-classified structure is possible.</p>
"
"0.027972711943223","0.0284901441149095"," 34139","<p>I am using an ARIMA model to create a model for correlated errors from my regression model. I am using the <code>auto.arima</code> function from the <a href=""http://cran.r-project.org/web/packages/forecast/index.html"" rel=""nofollow"">forecast</a> package in R. I am able to get more data at some frequent interval after the regression model is created, so I get more values for the correlated errors. </p>

<p>My question is how do I update the ARIMA model with a gap in time interval between readings.</p>
"
"0.027972711943223","0.0284901441149095"," 34363","<p>I'm using the <a href=""http://cran.r-project.org/web/packages/randomForest/index.html"">randomForest</a> package in R and using the iris data, the random forest generated is a classification but when I use a dataset with around 700 features (the features are each pixel in a 28x28 pixel image) and the label column is named <code>label</code>, the <code>randomForest</code> generated is regression. I'm using the following line:</p>

<pre><code>rf &lt;- randomForest(label ~ ., data=train)
</code></pre>

<p>How come regression is used instead of classification? The data is read in through <code>read.csv()</code>.</p>
"
"0.0791187772129236","0.0705095093522208"," 34445","<p>I'd be really grateful for recommendations of a robust package for fitting discrete choice models to a large amount ($n$ in the millions and $p$ in 2000 range) of data. I want a smoothed model that can deal with multi-colinear dependent variables and matrix inversion issues sensibly - like <a href=""http://cran.r-project.org/web/packages/glmnet/index.html"" rel=""nofollow"">glmnet</a>. I'm happy to bootstrap samples, which may be the only way to deal with big data in R.</p>

<p>I've tried using the <a href=""http://cran.r-project.org/web/packages/mlogit/index.html"" rel=""nofollow"">mlogit</a> package and it falls apart with more than a few hundred predictors, producing errors to do with matrix inversion.</p>

<p>My alternative is to use the <a href=""http://cran.r-project.org/web/packages/glmnet/index.html"" rel=""nofollow"">glmnet</a> package for binary regression and then use transforms to approximate the discrete choice model using something called Begg and Gray's approximation.  </p>

<p>This data is not multinomial, in the traditional sense. It is discrete-choice, that is the classes themselves change from observation to observation -- possibly also the number of classes. Each of the classes has a set of predictors which are measured on the same scale and are class specific -- cf. <a href=""http://en.wikipedia.org/wiki/Discrete_choice"" rel=""nofollow"">Discrete Choice Models</a>. I wrote to the maintainer of <code>glmnet</code>, Trevor Hastie,  who says there is no mapping to discrete choice models in their package.</p>

<p>Another name for discrete-choice is conditional logit, with the correct parameterization. I found the package <a href=""http://cran.r-project.org/web/packages/pglm/index.html"" rel=""nofollow"">pglm</a>, but it is also lacking in robustness. There's reference, <a href=""http://stats.stackexchange.com/questions/10141/"">Discrete choice panel models in R</a> to <a href=""http://cran.r-project.org/web/packages/lme4/index.html"" rel=""nofollow"">lme4</a> also, but I have found no examples of the conditional logit with it.</p>
"
"0.0484501583111509","0.0493463771219827"," 34796","<p>I'm using the svydesign package in R to run survey weighted logit regressions as follows:</p>

<pre><code>sdobj &lt;- svydesign(id = ~0, weights = ~chweight, strata = ~strata, data = svdat)

model1 &lt;- svyglm(formula=formula1,design=sdobj,family = quasibinomial)
</code></pre>

<p>However, the documentation states a caveat about regressions without specifying finite population corrections (FPC):</p>

<pre><code>    If fpc is not specified then sampling is assumed to be
with replacement at the top level and only the first stage of
 cluster is used in computing variances.
</code></pre>

<p>Unfortunately, I do not have sufficient information to specify my populations at each level (of which I sampling very little).  Any information on how to specify survey weights without FPC information would be very helpful. </p>
"
"0.0625488854200668","0.0637058989297032"," 34859","<p>I would like to find predictors for a continuous dependent variable out of a set of 30 independent variables. I am using Lasso regression as implemented in the <a href=""http://cran.r-project.org/web/packages/glmnet/index.html"">glmnet</a> package in R. Here is some dummy code:</p>

<pre><code># generate a dummy dataset with 30 predictors (10 useful &amp; 20 useless) 
y=rnorm(100)
x1=matrix(rnorm(100*20),100,20)
x2=matrix(y+rnorm(100*10),100,10)
x=cbind(x1,x2)

# use crossvalidation to find the best lambda
library(glmnet)
cv &lt;- cv.glmnet(x,y,alpha=1,nfolds=10)
l &lt;- cv$lambda.min
alpha=1

# fit the model
fits &lt;- glmnet( x, y, family=""gaussian"", alpha=alpha, nlambda=100)
res &lt;- predict(fits, s=l, type=""coefficients"")
res 
</code></pre>

<p>My questions is how to interpret the output:</p>

<ul>
<li><p>Is it correct to say that in the final output all predictors that show a coefficient different from zero are related to the dependent variable? </p></li>
<li><p>Would that be a sufficient report in the context of a journal publication? Or is it expected to provide test-statistics for the significance of the coefficients? (The context is human genetics)</p></li>
<li><p>Is it reasonable to calculate p-values or other test-statistic to claim significance? How would that be possible? Is a procedure implemented in R? </p></li>
<li><p>Would a simple regression plot (data points plotted with a linear fit) for every predictor be a suitable way to visualize this data?</p></li>
<li><p>Maybe someone can provide some easy examples of published articles showing the use of Lasso in the context of some real data &amp; how to report this in a journal?</p></li>
</ul>
"
"NaN","NaN"," 35071","<p>Fitting a logistic regression using <a href=""http://cran.r-project.org/web/packages/lme4/index.html"">lme4</a> ends with </p>

<pre><code>Error in mer_finalize(ans) : Downdated X'X is not positive definite. 
</code></pre>

<p>A likely cause of this error is apparently rank deficiency. What is rank deficiency, and how should I address it?</p>
"
"0.0927749898843639","0.0944911182523068"," 35173","<p>I am currently conducting a meta-analysis in which I need to use a mixed treatment comparison method.  As I understand it, this method works in the following way:</p>

<p>Say you have a group of studies that make the following set of treatment comparisons:</p>

<ul>
<li>Intervention 1</li>
<li>Intervention 2</li>
<li>Intervention 3</li>
<li>Control</li>
</ul>

<p>You are interested in all possible comparisons between these treatments.  So, not only are you interested in intervention 1 versus control, intervention 2 versus control, and intervention 3 versus control, but also intervention 1 versus intervention 2, intervention 1 versus intervention 3, etc.  The problem occurs in that not all of the studies in your meta analysis include each intervention type.  So, while study 1 may have tested intervention 1, intervention 2, and a control group, study 2 tested intervention 2 and intervention 3 versus a control group.  And so on.  Mixed treatment comparisons (<a href=""http://www.bmj.com/content/331/7521/897?view=long&amp;pmid=16223826"" rel=""nofollow"">Caldwell, Ades, &amp; Higgins, 2005</a>; <a href=""http://www.ncbi.nlm.nih.gov/pubmed/15449338"" rel=""nofollow"">Lu &amp; Ades, 2004</a>; <a href=""http://www.dovepress.com/multiple-treatment-comparison-meta-analyses-a-step-forward-into-comple-peer-reviewed-article-CLEP"" rel=""nofollow"">Mills et al., 2011</a>) arose as a way of using the indirect information from your sample of studies to estimate the magnitude of the missing comparisons.</p>

<p>For my study, I am interested in how several different moderators affect the magnitude of the various treatment comparisons.  I stumbled across a paper (<a href=""http://www.ncbi.nlm.nih.gov/pubmed/16900557"" rel=""nofollow"">Nixon, Bansback, &amp; Brennan, 2007</a>) that combines the mixed treatment comparison method with meta-regression.  My problem is finding a good software implementation for this method (preferably an implementation in R, since I'm most familiar with R).  As far as I can tell, the <a href=""http://www.metafor-project.org/"" rel=""nofollow"">metafor</a> package isn't able to handle mixed treatment comparisons.  Does anybody know whether there's a package out there that's able to handle both mixed treatment comparisons and meta-regression?</p>
"
"0.118842882298127","0.127411797859406"," 35489","<p>I have real daily market data which I'm looking at to create a model for forecasting. The model that I created (below) used autoregressive terms within a linear regression.</p>

<p>I was sharing this with a colleague and he said ""autoregressive variables are correlated with the other variables in multiple linear setting which creates multicollinarity problem, creating unreliable result.""</p>

<p>So I'm turning to the group for help. Here is the data and the analysis that I performed in R.</p>

<pre><code>#Read in Data
MarketData = read.table('http://sharp-waterfall-3397.herokuapp.com/MarketCategories6.txt', header=TRUE,na.strings = ""NA"", sep="","")
MarketData$Month &lt;- as.factor(MarketData$Month)
MarketData$Weekday &lt;- as.factor(MarketData$Weekday)

str(MarketData)
</code></pre>

<p>I'm using the <a href=""http://cran.r-project.org/web/packages/PERregress/index.html"" rel=""nofollow"">PERregress</a> library to help with the autoregression using the <code>back()</code> function and to help with the residual diagnostics:</p>

<pre><code>library(PERregress)
descStat(MarketData)
</code></pre>

<p>Subsetting the data for model building and prediction purposes:</p>

<pre><code>Total = MarketData
MarketData = MarketData[1:268,]
attach(MarketData)
</code></pre>

<p>Here is a regression with everything that I can think of. Note you can have higher autoregressive terms but this will start to mask events since R will ignore the first several rows. Also just an FYI for some reason the residual analysis is breaking which I liked to look for points with undue leverage.</p>

<pre><code>#Market1Category1 Regression for the markets with everything that I can think of it
Market1Category1Output=lm(Market1Category1 ~ Trend+Month2+Month3+Month4+
                          Month5+Month6+Month7+Month8+Month9+Monday+
                          Tuesday+Wednesday+Thursday+Friday+Saturday+
                          Holiday1+Holiday2+Holiday3+Holiday4+
                          Event1+Event2+Event3+Event4+Event5+Event6+Event7+
                          Event8+Event9+Event10+Event11+Event12+Event13+
                          Event14+Event15+Event16+Event17+Event18+Event19+
                          Event20+Event21+Event22+Event23+Event24+Event25+
                          Event26+Event27+Event28+
                          back(Market1Category1)+back(Market1Category1, 2))
summary(Market1Category1Output)
acf(Market1Category1Output$residuals)
residualPlots(Market1Category1Output)
</code></pre>

<p>Here is the final equation. I'd like to say that I reduced the variables using partial f-test but I couldn't find an easy way to do this so if you know a function please let me know. Basically I looked at the change in adjusted $R^2$.</p>

<pre><code>#Final regression equation 
Market1Category1Output=lm(Market1Category1 ~ Month5+Month6+Month7+
                          Tuesday+Wednesday+Thursday+Friday+Saturday+
                          Holiday1+Holiday3+Event2+Event7+Event10+
                          Event13+Event16+Event25+Event28+
                          back(Market1Category1)+back(Market1Category1, 6))
summary(Market1Category1Output)
acf(Market1Category1Output$residuals)
residualPlots(Market1Category1Output)
</code></pre>

<p>Here is a plot of the actuals in green vs the predictions in blue but there's a problem:</p>

<pre><code>plot(Time, Market1Category1, col='green')
points(Time, predict(Market1Category1Output, MarketData), col='blue', pch=20)
</code></pre>

<p>The issue is that predict will use the data values instead of it's predicted values for the autoregressive terms. In order to make it use predicted terms I created this loop. If you know a better way let me know.</p>

<pre><code>dataSet2 &lt;- Total
dataSet2[8:length(dataSet2$Time),""Market1Category1""] &lt;- NA
    for (i in (1:(length(dataSet2$Time)-7))) {
  dataSet2[6+i+1,""Market1Category1""] &lt;- 1
  dataSet2[6+i+1,""Market1Category1""] &lt;- predict(Market1Category1Output, 
                                                dataSet2[0:6+i+1,])[6+1] 
}
</code></pre>

<p>Here is the plot again with the results in blue using the predicted results for the autoregressive terms (with the exception of the first 7 since the model needs those to <code>predict</code>):</p>

<pre><code>plot(Total$Time, Total$Market1Category1, col='green')
points(dataSet2$Time, dataSet2$Market1Category1, col='blue', pch=20)
</code></pre>

<p>So here are my questions in order of importance:</p>

<ol>
<li>Does using autoregressive and linear terms violate any fundamental assumptions?</li>
<li>What issues can this cause and what analysis/steps should I do take to avoid these problems?</li>
<li>Is there a better approach to modeling this timeseries?</li>
<li>Is there a more efficient approach?</li>
<li>Given the residuals what steps would you take?</li>
</ol>

<p>Finally two questions which is just causing me more work than possibly necessary:</p>

<ol>
<li>As you can see instead of using the factors for weekday and month I'm using separate conditional variables. I'm doing this because if I use the factor and a level turns out to be insignificant (e.g., Monday for days of the week). I can't remove it. Perhaps there's a way?</li>
<li>Is there a quick way to run a partial F-statistic to understand whether removing a variable makes sense?</li>
</ol>
"
"0.027972711943223","0.0284901441149095"," 36064","<p>I am looking at some simple regression models using both R and the <code>statsmodels</code> package of Python. I've found that, when computing the coefficient of determination, <code>statmodels</code> uses the following formula for $R^2$:
$$
R^2 = 1 - \frac{SSR}{TSS}~~~~~~(\text{centered})
$$
where $SSR$ is the sum of squared residuals, and $TSS$ is the total sum of squares of the model. (""Centered"" means that the mean has been removed from the series.) However, the same calculation in R yields a different result for $R^2$. The reason is that R seems to be calculating $R^2$ as:
$$
R^2 = 1 - \frac{SSR}{TSS}~~~(\text{uncentered})
$$
So, what gives? Presumably there's some reason to prefer one over the other in certain situations. I haven't been able to find any information online about the cases where one of the above formulae should be preferred. </p>

<p>Can someone please explain why one is better than the other?</p>
"
"0.027972711943223","0.0284901441149095"," 36311","<p>I've started using <a href=""http://cran.r-project.org/web/packages/lmPerm/index.html"" rel=""nofollow"">lmPerm</a> in order to perform regressions in R. 
The equation I want to fit has the form: </p>

<pre><code>out3 &lt;- lmp(outcome ~  bin1 + bin2 + cont1 + cont2, perm=""Exact"")
</code></pre>

<p>Where ""outcome"" is a non-normally distributed continuous variable, and <code>bin*</code> and <code>cont</code> are binary and continuous regressors (similarly, they are non-normally distributed). Each variable has a length of approx. 110 cases. </p>

<p>Here are my questions: </p>

<ol>
<li><p>This code works fine, but every time it runs in R, different p-values appear for each regressor. Which p-value should be reported in my results? I've tried repeating the test several times (in a loop) and getting an estimate from that, but I'm not sure it works... </p></li>
<li><p>If some of the predictors are changed and (then) several models / hypothesis are tested, should Bonferroni corrections be used in the same way they are applied for ordinary regressions? Is <code>lmp</code> somehow robust to multiple testing procedures? </p></li>
</ol>
"
"0.0484501583111509","0.0328975847479884"," 37383","<p>I am using the <a href=""http://cran.r-project.org/web/packages/np/index.html"" rel=""nofollow"">np</a> package in R with the <code>npregiv</code> command.  The program is in beta, and I cannot call ordered(var) on one of my instruments (a bug in the program I am pretty sure, the help file says this is allowed).  The variable in question is the number of a particular type of institutions in a district, and all districts are on the [0,13] interval.  If I do not call <code>ordered()</code>, what are the consequences?  In a parametric setting it is a question of whether the increments of the ordinal variable are linear or not - I am not sure how to interpret this in the non parametric regression setting.</p>

<p>Otherwise I could call <code>factor()</code> and convert the variable to an indicator of whether or not there exists an institution in a particular district, but then I am throwing away information.</p>
"
"0.0791187772129236","0.080582296402538"," 37411","<p>Suppose I'm building a logistic regression classifier that predicts whether someone is married or single. (1 = married, 0 = single) I want to choose a point on the precision-recall curve that gives me at least 75% precision, so I want to choose thresholds $t_1$ and $t_2$, so that:</p>

<ul>
<li>If the output of my classifier is greater than $t_1$, I output ""married"".</li>
<li>If the output is below $t_2$, I output ""single"".</li>
<li>If the output is in between, I output ""I don't know"".</li>
</ul>

<p>A couple questions:</p>

<ol>
<li>I think under the standard definition of precision, precision will be measuring the precision of the married class alone (i.e., precision = # times I correctly predict married / total # times I predict married). However, what I really want to do is measure the overall precision (i.e., the total # times I correctly predict married or single / total # times I predict married or single). Is this an okay thing to do? If not, what should I be doing?</li>
<li>Is there a way to calculate this ""overall"" precision/recall curve in R (e.g., using the ROCR package or some other library)? I'm currently using the ROCR package, but it seems to only give me the single-class-at-a-time precision/recall.</li>
</ol>
"
"NaN","NaN"," 37830","<p>I've been experimenting with the <code>rfe</code> function in the <code>caret</code> package to do logistic regression with feature selection. I used the <code>lmFuncs</code> functions with the following <code>rfeContol</code> :</p>

<p><code>ctrl &lt;- rfeControl(functions = lmFuncs,
                     method = 'cv',
                     rerank=TRUE,
                     saveDetails=TRUE,
                     verbose = TRUE,
                     returnResamp = ""all"",
                     number=100)</code></p>

<p>Below is the structure of the <code>rfe</code> call:</p>

<p><code>fit.rfe=rfe(df.preds,df.depend, metric='RMSE',sizes=c(5,10,15,20), rfeControl=ctrl)</code></p>

<p><code>df.preds</code> is a data frame of inputs to the model. <code>df.depend</code> is a vector of 1 or 0 corresponding to each row in <code>df.preds</code> to indicate response.</p>

<p>The resulting model accessed in from the <code>fit</code> object in the <code>rfe</code> object is of class <code>lm</code> and produces predicted values of less than zero and greater than 1 when I use the following code with the <code>predict</code> function:</p>

<p><code>predict(fit.rfe$fit,df,type='response')</code></p>

<p>Given I'm expecting this to be a logistic, all predicted values should greater than zero and less than one. </p>

<p>Any help will be appreciated.</p>
"
"0.0395593886064618","0.040291148201269"," 37973","<p>I am fitting a simple linear regression model with 4 predictors:</p>

<p><code>lm(Outcome ~ Predictor1 + Predictor2 + Predictor3 + Predictor4, data=dat.s)</code></p>

<p>I'm finding that the model predictions are consistently off as shown in this graph:
<img src=""http://i.stack.imgur.com/CNLJz.png"" alt=""scatterplot of predictions and residuals""></p>

<p>The model clearly overestimates the low values and underestimates the high values, but the miss-estimation is very linear -- it seems like the model should be able to just adjust the slope and fit the data better. Why is that not happening? In case it helps, here are scatterplots of the the Outcome against each of the four Predictors:
<img src=""http://i.stack.imgur.com/uc55e.png"" alt=""enter image description here""></p>

<p>Using the <code>car</code> package <code>outlierTest</code> function did not identify any outliers.</p>
"
"0.0927749898843639","0.0944911182523068"," 38491","<p>If we have a spatial autoregressive process, we can estimate a model to control for the autoregression with a spatial lag,
$$y=\rho W y + X\beta + \epsilon$$
Where $\rho$ is the strength of the spatial correlation, and $W$ is a matrix of spatial weights. The <code>spdep</code> package for R contains the <code>lagsarlm</code> command which is designed to estimate precisely this model. The package contains methods for creating the weights. But there seems to be some discrepancy between the model fit between <code>lagsarlm()</code> and <code>lm()</code> fitted to what should be a similar model.</p>

<p>As an example, consider the example given with <code>?lagsarlm</code> in R. </p>

<pre><code>library(spdep)
data(oldcol)
COL.lag &lt;- lagsarlm(CRIME ~ INC + HOVAL, data=COL.OLD,
                nb2listw(COL.nb, style=""W""), method=""eigen"", quiet=TRUE)
summary(COL.lag)
Residuals:
      Min        1Q    Median        3Q       Max 
-37.68585  -5.35636   0.05421   6.02013  23.20555 

Type: lag 
Coefficients: (asymptotic standard errors) 
             Estimate Std. Error z value  Pr(&gt;|z|)
(Intercept) 45.079251   7.177347  6.2808 3.369e-10
INC         -1.031616   0.305143 -3.3808 0.0007229
HOVAL       -0.265926   0.088499 -3.0049 0.0026570

Rho: 0.43102, LR test value: 9.9736, p-value: 0.001588
Asymptotic standard error: 0.11768
    z-value: 3.6626, p-value: 0.00024962
Wald statistic: 13.415, p-value: 0.00024962
</code></pre>

<p>We can estimate what (I think) should be the same model by computing the actual spatial lag variable,</p>

<pre><code>crime.lag &lt;- lag.listw(nb2listw(COL.nb, style=""W""), COL.OLD$CRIME)
linearlag &lt;- lm(CRIME ~ crime.lag + INC + HOVAL, data=COL.OLD)
Residuals:
    Min      1Q  Median      3Q     Max 
-38.644  -6.103   0.266   6.563  21.610 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 38.18099    9.21531   4.143 0.000149 ***
crime.lag    0.55733    0.15029   3.709 0.000570 ***
INC         -0.86584    0.35541  -2.436 0.018864 *  
HOVAL       -0.26358    0.09136  -2.885 0.005986 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

Residual standard error: 10.12 on 45 degrees of freedom
Multiple R-squared: 0.6572, Adjusted R-squared: 0.6343 
F-statistic: 28.75 on 3 and 45 DF,  p-value: 1.543e-10 
</code></pre>

<p>The two models, which I think should be identical, are in fact significantly different from each other in every parameter and in model fit (with the <code>linearlag</code> model providing significantly lower AIC). Are there reasons why this should be? Why should I just not use the second model and abandon the special methods?</p>
"
"0.027972711943223","0.0284901441149095"," 38541","<p>I used the functions from this <a href=""http://www.math.mcmaster.ca/peter/s4f03/s4f03_0607/rochl.html"" rel=""nofollow"">link</a> for creating ROC curve for logistic regression model. Since the object produced by <code>glmer</code> in <code>lme4</code> package is a S4 object (as far as I know) and the function from the link cannot handle it.</p>

<p>I wonder if there are similar functions for creating ROC curve for multi-level logistic regression model in R.</p>
"
"0.0395593886064618","0.040291148201269"," 38552","<p>I have worked on clinical trials in FDA submissions for many years. I use SAS almost exclusively. Recently I discussed a consulting job I had to bootstrap a Deming regression. Bootstrapping is much more easily done in R I think and several experts have suggested ways that I can do this in R. Since this is going into a resubmission to the FDA for my client I am concerned as to whether software validation might become an issue. I hear that R is gaining acceptance with the FDA and experts tell me that traceability which is an attractive feature with SAS can be dealt with easily in R as well.</p>

<p>My client did the original Deming regression with a package that I am not familiar with and validation was not an issue with the original submission. I want to help without adding any new headaches for them.</p>

<p>Can the R experts here give me some guidance? If it will help I can mention some of the programming options that were suggested?</p>
"
"0.027972711943223","0.0284901441149095"," 38563","<p>I'm not doing a pure QAR (quantile auto regression) but I do have a lagged dependent variable (AR(1)) as a predictor. I'm using the <code>quantreg</code> package in <code>R</code> to do it. I have two very closely related questions:</p>

<p>(1) I use <code>quantreg</code> in <code>R</code> to do my QRs. Do I need to do anything fancy to be fine by putting an AR(1) term on the RHS?  </p>

<p>(2) Is it fine to use AR(1) term as a regressor to deal with autocorrelation in quantile regression as you can with OLS? Or is QR different in this regards?</p>
"
"0.100857047225074","0.102722675451665"," 40499","<p>When using the <code>step.plr()</code> function in the <a href=""http://cran.r-project.org/web/packages/stepPlr/index.html"" rel=""nofollow"">stepPlr</a> package, if my predictors are factors, do I need to encode my predictors as dummy variables manually before passing it to the function? I do know that I can specify ""level"", but how  the ""level"" parameter works is confusing to me. 
My understanding is that I need to tell <code>step.plr()</code> explicitly which factors should be encoded as dummy variables and thus leaving one factor out intentionally. </p>

<p>Let's consider a simple example. Suppose I have 1 categorical predicator with 4 levels and binary response. Normally, if I use <code>glm()</code> to fit a logistic regression model, <code>glm()</code> would automatically convert the categorical predicator into 3 dummy variables. Now in <code>stepPlr()</code>, do I specify the ""level"" parameter for that predictor with 4 levels or 3 levels? The ""Help"" section is vague, and says: </p>

<blockquote>
  <p>If the j-th column of x is discrete, level[[ j ]] is the set of levels for the categorical factor.</p>
</blockquote>

<p>Does it mean I should tell <code>step.plr()</code> about all 4 levels, or I should make an intelligent decision myself and tell <code>step.plr()</code> to use only 3 levels? </p>

<p>==============UPDATE (16 Oct 2012)=============</p>

<p>The following example will demonstrate what is the problem with <code>step.plr()</code>'s automatic dummy variable encoding. It is a slight modification of the code in the function's help section. 
     set.seed(100)</p>

<pre><code>n &lt;- 100
p &lt;- 3
z &lt;- matrix(sample(seq(3),n*p,replace=TRUE),nrow=n)
x &lt;- data.frame(x1=factor(z[ ,1]),x2=factor(z[ ,2]),
                x3=factor(sample(seq(3), n, replace=TRUE, prob=c(0.2, 0.5, 0.3))),
                x4=factor(sample(seq(3), n, replace=TRUE, prob=c(0.1, 0.3, 0.6))))
y &lt;- sample(c(0,1),n,replace=TRUE)
fit &lt;- step.plr(x,y, cp=""aic"")
summary(fit)
</code></pre>

<p>And here's an excerpt of the result:</p>

<pre><code>Call:
plr(x = ix0, y = y, weights = weights, offset.subset = offset.subset, 
    offset.coefficients = offset.coefficients, lambda = lambda, 
    cp = cp)

Coefficients:
      Estimate Std.Error z value Pr(&gt;|z|)
Intercept  0.91386   5.04780   0.181    0.856
x4.1       1.33787   4.61089   0.290    0.772
x4.2      -1.70462   4.91240  -0.347    0.729
x4.3       0.36675   3.18857   0.115    0.908
x3.1:x4.1  7.04901  14.35112   0.491    0.623
x3.1:x4.2 -5.50973  15.53674  -0.355    0.723
x3.1:x4.3 -0.50012   7.95651  -0.063    0.950
</code></pre>

<p>You can see that all levels, that is, (1,2,3), are used to fit the model. But normally you only need two dummy variables to encode a predictor with 3 levels.
On the other hand, if you use <code>glm()</code>: </p>

<pre><code>glm(y~.^2, data=x, family=binomial)
</code></pre>

<p>you will get the correct dummy variable encoding.</p>
"
"0.128674474938826","0.131054662928584"," 40670","<p>I am familiar with linear regression models but the random section of linear mixed models just melts my mind. I did find an excellent guide that could have helped me but the languageR package is not compatible with newer versions of lme4 so I've been unable to implement it in my work.</p>

<p>For me the fixed effects are very understandable (below lactation and a higher yr2 value both contribute to a higher weight but the lactation effect is more consistent which results in a higher t-value).</p>

<p>The first problem is to understand what I am actually putting in. To a certain extent I understand that <code>(1|P$grupp)</code> means that the mixed model add to the base line (intercept) while <code>(P$grupp|P$lweek)</code> mean that belong to a group is expected to affect the average weight increase (or decrease) while <code>P$lweek</code> adds to the baseline value. But why does all tutorials seem to favor write ups like <code>(1+P$fgrupp|P$lweek)</code> rather than <code>(P$grupp|P$lweek)</code>?</p>

<p>Now on to the actual output (see below for full output). I've used the following models (sorry for the Swenglish but the sample is the weight of cows <code>P$vikt</code> is the weight at certain time points and <code>P$lweek</code> is the time since a calf was born, <code>P$fgrupp</code> is a factor telling if the cow belongs to feed group 1,2 or 3):</p>

<pre><code>Formula: P$vikt ~ P$lweek + P$laktation + P$yr + (1 | P$fgrupp)            #$
Formula: P$vikt ~ P$lweek + P$laktation + P$yr + (1 + P$fgrupp | P$lweek)
</code></pre>

<p>Where I understand it as the first one being rather useless (essentially it tells us that the average weight of the cows isn't affected of which feed group it belongs too). This is reflected by fgrupp having variance 0 in the first formula below. The second is more interesting as the <code>P$fgrupp|P$lweek</code> as I understand it should show if different feed groups affect the weight increase of cows as function of the time. But I am really not competent enough to understand the input. I understand that variance somehow mean that belonging to group2 or 3 explain some of the variation in the growth curves but I really don't understand how to interpret this.</p>

<pre><code>Random effects:
 Groups   Name        Variance Std.Dev. Corr        
 P$lweek  (Intercept)   13.068  3.6149                                     #$
          P$fgrupp2     77.230  8.7881  1.000                              #$
          P$fgrupp3     81.188  9.0104  1.000 1.000                        #$
 Residual             4031.831 63.4967              
Number of obs: 1048, groups: P$lweek, 84
</code></pre>

<p><strong>Full output</strong></p>

<pre><code>#First model#
Linear mixed model fit by REML 
Formula: P$vikt ~ P$lweek + P$laktation + P$yr + (1 | P$fgrupp)            #$
   AIC   BIC logLik deviance REMLdev
 11703 11732  -5845    11698   11691
Random effects:
 Groups   Name        Variance Std.Dev.
 P$fgrupp (Intercept)    0.0    0.000                                      #$
 Residual             4139.9   64.342  
Number of obs: 1048, groups: P$fgrupp, 3                                   #$

Fixed effects:
            Estimate Std. Error t value
(Intercept)  509.593      4.683  108.82
P$lweek        1.028      0.105    9.79                                    #$
P$laktation   22.789      1.454   15.67                                    #$
P$yr2         35.294      4.093    8.62                                    #$

Correlation of Fixed Effects:
            (Intr) P$lwek P$lktt                                           #$
P$lweek     -0.560                                                         #$
P$laktation -0.636  0.030                                                  #$
P$yr2       -0.240 -0.034 -0.141                                           #$


#Second model#

Linear mixed model fit by REML 
Formula: P$vikt ~ P$lweek + P$laktation + P$yr + (1 + P$fgrupp | P$lweek)  #$
       AIC   BIC logLik deviance REMLdev
     11707 11761  -5842    11693   11685
    Random effects:
     Groups   Name        Variance Std.Dev. Corr        
     P$lweek  (Intercept)   13.068  3.6149                                     #$
              P$fgrupp2     77.230  8.7881  1.000                              #$
              P$fgrupp3     81.188  9.0104  1.000 1.000                        #$
     Residual             4031.831 63.4967              
    Number of obs: 1048, groups: P$lweek, 84                                   #$

Fixed effects:
            Estimate Std. Error t value
(Intercept) 508.2291     5.1770   98.17
P$lweek       1.0662     0.1192    8.94                                    #$
P$laktation  22.6525     1.4459   15.67                                    #$
P$yr2        35.6343     4.0848    8.72                                    #$

Correlation of Fixed Effects:
            (Intr) P$lwek P$lktt                                           #$$
P$lweek     -0.627                                                         #$
P$laktation -0.570  0.025                                                  #$
P$yr2       -0.224 -0.018 -0.136                                           #$
</code></pre>
"
"0.0484501583111509","0.0328975847479884"," 40739","<p>The <code>bild</code> package appears to be an excellent package for serial binary responses.  But it is for discrete time.  I would like to specify a smooth function of time for the odds ratio connection of the current response Y with binary responses measured at earlier times, or at least a first-order Markov version of this.  I believe this is called alternating logistic regression.  Does anyone know of an R package that handles continuous time, i.e., measurement times can be at any follow-up time?  I don't need random effects in the model. </p>
"
"0.027972711943223","0.0284901441149095"," 40745","<p>I am using earth package for the following data.</p>

<pre><code>x &lt;- c(127, 128, 255, 256, 511, 512, 600, 700, 800, 900, 1000, 1023, 1100,
       1200, 1300, 1400, 1500, 1600, 2047, 2048, 2100, 2200, 2300, 2400, 2500,
       2600, 2700, 2800, 3000, 3100, 3200, 3300, 3500, 4063, 4064, 4100, 4200,
       5200, 5400)

y &lt;- c(0.59, 0.61, 0.59, 1.55, 1.33, 3.50, 1.00, 1.22, 2.50, 3.00, 3.79,
       3.98, 4.33, 4.45, 4.59, 4.72, 4.82, 4.90, 4.96, 7.92, 5.01, 5.01,
       4.94, 5.05, 5.04, 5.03, 5.06, 5.10, 5.04, 5.06, 7.77, 5.07, 5.08,
       5.08, 5.12, 5.12, 5.08, 5.17, 5.18) 
</code></pre>

<p>After building the model, </p>

<pre><code>model&lt;-earth(y~x)
</code></pre>

<p>I get following regression model. </p>

<pre><code>summary(model)
Call: earth(x=x, y=y)
coefficients
(Intercept)  5.225822553
h(1400-x)   -0.003820087
</code></pre>

<p>Is there any possibility that I can increase somehow the number of knots or regression splines?</p>
"
"0.0395593886064618","0.040291148201269"," 41540","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/41697/prediction-results-for-two-response-variable-from-random-forest"">Prediction results for two response variable from random forest</a>  </p>
</blockquote>



<p>I use <a href=""http://cran.r-project.org/web/packages/randomForest/index.html"" rel=""nofollow"">randomForest</a> R package to do regression <code>rf = randomForest(A + B ~ C + D, data, ..)</code>. I want to know what are prediction values for A and B, and use <code>predict(rf)</code>. The output is an aggregate value instead of two values for A and B. Do you have any suggestion to implement a <code>predict</code> function to generate two predictions? </p>
"
"0.0484501583111509","0.0493463771219827"," 41697","<p>Would any R expert explain the predic function in the randomforest package to me?</p>

<p>I want to get two prediction results for numberic response variable A and B seperately from following regression</p>

<pre><code>result &lt;-randomforest(A + B ~ C + D + E, data = dataset)
predict(result)
</code></pre>

<p>I can get one prediction result. But prediction is neither A nor B. I can get the prediction results for both A and B from Mvpart and party package. </p>

<p>Thanks in advance!</p>
"
"0.0740088392978143","0.0538413115317435"," 41917","<p>Analyzing educational datasets we have samples of children from samples of class in samples of schools - we have sampling weights, so I use the survey package e.g. to do a linear model. But this kind of design also requires looking at the mixed effects. But this isnâ€™t possible using the survey package. I can do this in nlme â€“ but then I don't know how to account for the weighting. I guess I could use the sample weights as predictors in nlme regressions but I donâ€™t think that is correct.</p>

<p>It seems that this kind of design (in fact any stratified survey sample which includes nested levels) needs analysing from both perspectives â€“ (survey weights and mixed effects) at once â€“ but the packages of choice for each of these perspectives, survey and nlme, each donâ€™t seem to have slots for the other perspective.</p>

<p>Can someone put me on the right track, or suggest another package which does both at the same time?</p>
"
"0.027972711943223","0"," 43551","<p>I am running a multinomial logistic regression using the <code>mlogit</code> package and <code>mlogit</code> function in R.  Now I need to check for outliers for the model.</p>

<p>Is there any approach or function in R for testing outliers in an <code>mlogit</code> model?</p>
"
"0.0559454238864459","0.056980288229819"," 43747","<p>Using the method in <a href=""http://stackoverflow.com/questions/1395147/best-way-to-plot-interaction-effects-from-a-linear-model"">this post</a>, I have made a plot to visualize the interaction between two predictor variables using the effects package in <a href=""/questions/tagged/r"" class=""post-tag"" title=""show questions tagged 'r'"" rel=""tag"">r</a>, but I'm not really sure what I am looking at. </p>

<p>Tide heights and rain averages are continuous. 8 bins were the maximum the function would allow me to use. The following is the call to <code>effect</code> producing this plot:</p>

<pre><code> R &gt; plot(effect(term=""rain.avg2:tide.avg"",mod=bkrain9.lm,default.levels=8),
          main="""", xlab=""Precipitation - 24hr average (cm)"",
          ylab=expression(""TCB Concentration - CFU*100m""*L^-1),multiline=TRUE)
</code></pre>

<p><strong>Plot updated.</strong> Could somebody explain the purpose of this plotting feature in context to predictor interactions?</p>

<p><img src=""http://i.stack.imgur.com/neZql.jpg"" alt=""P""></p>

<p>Background:
For a class project, I have created a linear regression model to evaluate the effects of the interaction between two predictor variables (tide height and precipitation) on bacteria concentrations.</p>

<p>Thermo-tolerant coliform bacteria concentrations were sampled at 5 sites in a day, where a sample time was recorded at sampling completion. I took an average of these for roughly 20 days, and calculated an associated 24hr average of precipitation before sampling completion, and a 50 minute (the sampling duration) average of tide height before sampling completion.</p>
"
"NaN","NaN"," 44584","<p>I have been playing around with a seemingly unrelated regression (SUR) estimation. However, for dynamic SUR models it is known that -- analogous to the ARIMA case -- an OLS/GLS estimate is biased. For example <a href=""http://www.sciencedirect.com/science/article/pii/030440769401670U"" rel=""nofollow"">this article</a> provides a correction. So here's my question: Is there a <em>R</em> package or some other implementation that provides this correction? Thanks in advance!</p>
"
"0.0791187772129236","0.080582296402538"," 45449","<p>I have a large set of predictors (more than 43,000) for predicting a dependent variable which can take 2 values (0 or 1). The number of observations is more than 45,000. Most of the predictors are unigrams, bigrams and trigrams of words, so there is high degree of collinearity among them. There is a lot of sparsity in my dataset as well. I am using the logistic regression from the glmnet package, which works for the kind of dataset I have. My problem is how can I report p-value significance of the predictors. I do get the beta coefficient, but is there a way to claim that the beta coefficients are statistically significant?</p>

<p>Here is my code:</p>

<pre><code>library('glmnet')
data &lt;- read.csv('datafile.csv', header=T)
mat = as.matrix(data)
X = mat[,1:ncol(mat)-1] 
y = mat[,ncol(mat)]
fit &lt;- cv.glmnet(X,y, family=""binomial"")
</code></pre>

<p>Another question is:
I am using the default alpha=1, lasso penalty which causes the additional problem that if two predictors are collinear the lasso will pick one of them at random and assign zero beta weight to the other. I also tried with ridge penalty (alpha=0) which assigns similar coefficients to highly correlated variables rather than selecting one of them. However, the model with lasso penalty gives me a much lower deviance than the one with ridge penalty. Is there any other way that I can report both predictors which are highly collinear?</p>
"
"0.0395593886064618","0.040291148201269"," 45723","<p>I am a student in biology, currently finishing my master in Behavior, evolution and conservation. I have been in the Swiss national park for my field work and I have some trouble to analyze my data.</p>

<p>So basically what I have is a lot of factors and what I need to do is a multinomial logistic regression :</p>

<p>Factors :</p>

<ul>
<li>Behaviour (4 states - Moving, Feeding, Resting, Runing)</li>
<li>Age (Of the individual)</li>
<li>Temperature</li>
<li>Valley (where the chamois was, 2 different choices)</li>
<li>Year (of the observation)</li>
<li>Month (of the observation)</li>
<li>Kid (if it has a kid or not)</li>
<li>Individual (The number written on the tag he had on his ear)</li>
</ul>

<p>What I want to do is to check what factors influence the Behavior.</p>

<p>I tried several things with R but can't use the <code>mlogit</code> package which is the one I have been told to use.</p>

<p>I have also tried in JMP, now the problem I have here is that I just clicked on ""Fit Y in function of X"" and selected my response variable and my factors and bam, I had results, but to be honest this seems very simple and I was wondering if I am not missing something somewhere.</p>

<p>Edit : Here is what R returns when I try the <code>mlogit</code> function :</p>

<pre><code>mlogit(Behavior~Age+Temp+Valley+Individual+Year+Month, Merge)
Error in `row.names&lt;-.data.frame`(`*tmp*`, value = value) : 
  invalid 'row.names' length
</code></pre>

<p>Can anyone here that can help me either with R or with JMP?</p>
"
"0.0740088392978143","0.0753778361444409"," 45882","<p>I have got a question regarding ordered choice regressions in <strong>R</strong>. </p>

<p>I have several demographic variables with which I want to explain the ordered choice of individuals within a survey in an <strong>ordered choice</strong> (<strong>probit</strong> or <strong>logit</strong>, this is not important) framework. Standard ordered choice estimations of course just give me aggregate parameter estimates. For my task it would however be useful to estimate or extract ""hypothetical"" individual-level parameter estimates (betas) for a certain independent variable and each individual in the survey. </p>

<p>I have experimented with hierarchical Bayes algorithms provided by the <strong>bayesm</strong> and <strong>ChoiceModelR</strong>. Correct me if I am wrong but I think these techniques also demand that individuals appear several times within a survey and are confronted with different choice situations, so that one can estimate the influence of certain attributes on the individuals choices.</p>

<p>My data however don't have any panel structure. I was also experimenting with Bayesian inference in example by the <strong>MCMCoprobit</strong> function in the <strong>MCMCpack</strong> package, but this function just simulates betas. I can't however, as far as I know, attribute them to certain individuals in the survey, which would be good. I would be very glad if somebody could give me a hint, sometimes already a catchword is helpful to google the correct solution!</p>
"
"0.115512844298478","0.1241856590838"," 46096","<p>I'm working in R, using glm.nb (of the MASS package) to model count data with a negative binomial regression model.  I'd like to compare the relative importance of each of my predictor variables regarding their impact on the response variable (note: the predictors each have quite different scales - sometimes by orders of magnitude).  Unfortunately, the output from R gives me results as unstandardized (<em>b</em>) coefficients (""estimates"").  I'm hoping someone can give me a hint as to how to go about getting standardized (<em>beta</em>) coefficients from the NB regression model... or another 'better' way to determine the relative importance of each of my predictors on my response variable.</p>

<p>I've investigated several potential ways like: </p>

<ol>
<li>using the R package 'relimpo' (as suggested in a comment to <a href=""http://stats.stackexchange.com/a/7118"">http://stats.stackexchange.com/a/7118</a>), but it does not work on a NB regression model, thus completely changing the assumptions I should be accounting for and making the outcomes very different; </li>
<li>mean-centering and scaling my data, which changes the interpretation and makes it so that I can't use NB model due to response variables now having negative values; </li>
<li>scaling-only, so that I can still run a NB model... which I <em>thought</em> would only affect the scale of the coefficients without changing their direction (viz., <a href=""http://stats.stackexchange.com/a/29784"">http://stats.stackexchange.com/a/29784</a> ) - but I do get some positive coefficients that flip to neg. and vice-verse... which seems strange to me and makes me wonder whether I'm making a mistake.</li>
</ol>

<p>I've benefited from looking at <a href=""http://stats.stackexchange.com/q/29781"">When should you center your data &amp; when should you standardize?</a> (and the suggested links from comments on the question such as <a href=""http://andrewgelman.com/2009/07/when_to_standar/"" rel=""nofollow"">http://andrewgelman.com/2009/07/when_to_standar/</a> and <a href=""http://stats.stackexchange.com/q/7112"">When and how to use standardized explanatory variables in linear regression</a> and <a href=""http://stats.stackexchange.com/q/19216"">Variables are often adjusted (e.g. standardised) before making a model - when is this a good idea, and when is it a bad one?</a>).  </p>

<p>Bottom line: I have not yet found a way to use a NB model in R (which I have statistically confirmed is more appropriate than lm, glm, or poisson for modeling my data) and still get at the relative importance - or at least to the standardized beta coefficients - for my predictors...</p>

<p>The R scripts is something like this:</p>

<pre><code>library(""MASS"")
nb = glm.nb(responseCountVar ~ predictor1 + predictor2 + 
  predictor3, data=myData, control=glm.control(maxit=125))
summary(nb)

scaled_nb = glm.nb(scale(responseCountVar, center = FALSE) ~ scale(predictor1, center = FALSE) + scale(predictor2, center = FALSE) + 
  scale(predictor3, center = FALSE), data=myData, control=glm.control(maxit=125))
summary(scaled_nb)
</code></pre>
"
"0.0740088392978143","0.0753778361444409"," 46312","<p>I'm working in R, using glm.nb (of the MASS package) to model count data with a negative binomial regression model.  I'd like to get the standardized (<em>beta</em>) coefficients from the model, but am given the unstandardized (<em>b</em> ""Estimate"") coefficients.</p>

<p>The R documentation does not seem to show of a way to retrieve the standardized beta weights easily for a negative bionomial regression model.</p>

<p>The R script is something like:</p>

<pre><code>library(""MASS"")
nb = glm.nb(responseCountVar ~ predictor1 + predictor2 + 
    predictor3 + predictor4 + predictor5 + predictor6 + 
    predictor7 + predictor8 + predictor9 + predictor10 + 
    predictor11 + predictor12 + predictor13 + predictor14 + 
    predictor15 + predictor16 + predictor17 + predictor18 + 
    predictor19 + predictor20 + predictor21,
    data=myData, control=glm.control(maxit=125))
summary(nb)
</code></pre>

<p>and the output of the above is:</p>

<pre><code>Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-5.1462  -1.0080  -0.4247   0.2277   3.4336  

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -3.059e+00  3.782e-01  -8.088 6.05e-16 ***
predictor1    -2.447e+00  4.930e-01  -4.965 6.88e-07 ***
predictor2    -1.004e+00  1.313e-01  -7.650 2.00e-14 ***
predictor3     1.158e+00  1.440e-01   8.047 8.46e-16 ***
predictor4     1.334e+00  7.034e-02  18.970  &lt; 2e-16 ***
predictor5     9.862e-01  2.006e-01   4.915 8.87e-07 ***
predictor6     1.166e+00  2.378e+00   0.490  0.62392    
predictor7    -1.057e-01  1.494e-01  -0.707  0.47936    
predictor8     4.051e-01  7.318e-02   5.536 3.10e-08 ***
predictor9    -3.320e-01  1.132e-01  -2.933  0.00336 ** 
predictor10    3.761e-01  1.561e-01   2.409  0.01600 *  
predictor11    8.660e-02  4.332e-02   1.999  0.04557 *  
predictor12   -1.583e-01  2.044e-01  -0.774  0.43872    
predictor13    6.404e-02  3.972e-03  16.122  &lt; 2e-16 ***
predictor14    4.264e-03  2.297e-04  18.563  &lt; 2e-16 ***
predictor15    3.279e-03  5.697e-04   5.755 8.68e-09 ***
predictor16    3.487e-03  3.447e-03   1.012  0.31177    
predictor17    1.534e-04  1.647e-04   0.931  0.35182    
predictor18   -7.606e-05  9.021e-05  -0.843  0.39917    
predictor19    2.536e-04  1.733e-05  14.633  &lt; 2e-16 ***
predictor20    2.997e-02  4.977e-03   6.021 1.73e-09 ***
predictor21    2.756e+01  3.508e+00   7.856 3.98e-15 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for Negative Binomial(0.9232) family taken to be 1)

    Null deviance: 5631.1  on 1835  degrees of freedom
Residual deviance: 2120.7  on 1814  degrees of freedom

                                AIC: 19268    
Number of Fisher Scoring iterations: 1    
                              Theta: 0.9232 
                          Std. Err.: 0.0282 
                 2 x log-likelihood: -19221.9910
</code></pre>

<p><strong>My question is</strong>:  Is there a way to get the beta weights, or do I need to try to convert my unstandardized b coefficients to standardized beta coefficients (if so, how would I do that)?</p>
"
"0.121930224537282","0.1241856590838"," 46322","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/28936/how-to-compute-significant-interaction-estimates-when-main-effect-is-not-signifi"">How to compute significant interaction estimates when main effect is not significant?</a>  </p>
</blockquote>



<p>I am an applied linguist and I am modelling responses to a vocabulary test taken by second language learners of English; the aim is to test theoretical hypotheses regarding the relationship between the nature of the word and the likelihood of the learnersâ€™ knowing that word. The models I am using to explore the role of explanatory variables are the random item Item Response Theory model LLTM+e (see de Boeck et al. 2011; de Boeck 2008). These are created using the lmer function in the LME4 package in R and treat item and person responses as random. The estimates shown below are effectively like those from a binary logistic regression model, indicating the log-odds of a correct response on a word, given certain properties. Covariates relate to both item and person characteristics. </p>

<p>The nature of my concern is actually a basic regression issue. I have found a significant interaction between ability grouping of the test taker (GRP; with two levels High and Low) and the length of the word in letters (LEN_L). As far as I can see the estimate of the fixed effects for the first model shows that (a) the lower level learners have an overall lower probability of giving a correct answer (b) that LEN_L does not provide a significant explanation across the pattern of responses for the whole test-taker population, and (c) a significant interaction between GRP and LEN_L indicates that the lower ability learners are less likely to give a correct answer for a longer word. This is in keeping with theory. </p>

<p>However, when I model the data without including the main effect for LEN_L, I am not seeing a significant effect for either high or low groups as shown in Model 2. LEN_L does not show as significant if modelled without interaction with grp low (not shown). I feel that I am missing something obvious, but I cannot quite grasp what is happening. And my references have run dry on this particular issue and I am thinking myself around in circles about it.</p>

<p>(NB: in my full model I have many other significant covariates, but this pattern holds true.) 
My query basically regards whether I should use Model 1, including the non-significant main effect, or whether my findings from Model 2 indicate that the finding is a little unstable. Any advice would be much appreciated! (I can post more details if necessary.)
Karen</p>

<p>Model 1 Fixed effects:</p>

<pre><code>              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)    0.25244    0.83194   0.303  0.76156    
grp low       -2.09126    0.26406  -7.920 2.38e-15 ***
LEN_L          0.02093    0.13062   0.160  0.87272    
grp low:LEN_L -0.09185    0.03146  -2.919  0.00351 **
</code></pre>

<p>Model 2 Fixed effects:</p>

<pre><code>               Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)     0.25243    0.83194   0.303    0.762    
grp low        -2.09126    0.26406  -7.920 2.38e-15 ***
grp high:LEN_L  0.02093    0.13062   0.160    0.873    
grp low:LEN_L  -0.07092    0.13202  -0.537    0.591  
</code></pre>

<blockquote>
  <p>De Boeck, P., Bakker, M., Zwitser, R., Nivard, M., Hofman, A.,
  Tuerlinckx, F. and Partchev, I. (2011) <a href=""http://www.jstatsoft.org/v39/i12/"" rel=""nofollow"">The Estimation of Item
  Response Models with the 'lmer' Function from the lme4 Package in
  R</a>. Journal of Statistical Software (39:12) pp 1-28</p>
</blockquote>
"
"0.027972711943223","0.0284901441149095"," 46340","<p>I have a series of samples of varying length, and the number of bugs created in those time samples. Reading the literature, this is often modeled as a Poisson process. If you write it like:</p>

<p>$$P(k,t)=\frac{e^{-\lambda t}(\lambda t)^k}{k!}$$</p>

<p>I know $k$ and $t$ and would like to regress to find $\lambda$.</p>

<p>Is there an R package to do this? The standard <code>glm</code> works for Poisson regression, but I can't find anything which allows for samples having non-unit times.</p>
"
"0.0419590679148345","0.0427352161723642"," 46434","<p>The <code>summary.rq</code> function from the <a href=""http://cran.r-project.org/web/packages/quantreg/quantreg.pdf"">quantreg vignette</a> provides a multitude of choices for standard error estimates of quantile regression coefficients. What are the special scenarios where each of these becomes optimal/desirable?</p>

<ul>
<li><p>""rank"" which produces confidence intervals for the estimated parameters by inverting a rank test as described in Koenker (1994). The default option assumes that the errors are iid, while the option iid = FALSE implements the proposal of Koenker Machado (1999). See the documentation for rq.fit.br for additional arguments.</p></li>
<li><p>""iid"" which presumes that the errors are iid and computes an estimate of the asymptotic covariance matrix as in KB(1978).</p></li>
<li><p>""nid"" which presumes local (in tau) linearity (in x) of the the conditional quantile functions and computes a Huber sandwich estimate using a local estimate of the sparsity.</p></li>
<li><p>""ker"" which uses a kernel estimate of the sandwich as proposed by Powell(1990).</p></li>
<li><p>""boot"" which implements one of several possible bootstrapping alternatives for estimating standard errors.</p></li>
</ul>

<p>I have read at least 20 empirical papers where this is applied either in the time-series or the cross-sectional dimension and haven't seen a mention of standard error choice. </p>
"
"0.0740088392978143","0.0646095738380922"," 46568","<p>Given the following model which relates the full year home sales to the unemployment rate (observed or estimated) I get a projected increase of 14% for 2013 over 2012... last year the same approach over projected by 6% (the 2012 projection was for 41,992 and the actual is coming in about 39,535) So I think the model is over projecting but I'm at a dead end thinking of a good (valid) way to modify it? BTW the projection)s) is identical to what a simple straight line regression in a spreadsheet yields.</p>

<p>I want to also thank the fine people at Stack overflow who got me this far in my 1st foray into R <a href=""http://stackoverflow.com/questions/14032768/csv-input-to-r-forecast-with-dates-via-r-studio#14032768"">http://stackoverflow.com/questions/14032768/csv-input-to-r-forecast-with-dates-via-r-studio#14032768</a></p>

<p>Pointers appreciated.</p>

<pre><code> # load the base data as presented in the question
 Workbook1 &lt;- structure(list(Year = structure(1:10, .Label = c(""31-Dec-04"", 
""31-Dec-05"", ""31-Dec-06"", ""31-Dec-07"", ""31-Dec-08"", ""31-Dec-09"", 
""31-Dec-10"", ""31-Dec-11"", ""31-Dec-12"", ""31-Dec-13""), class = ""factor""), 
total = c(51439L, 59674L, 58664L, 55698L, 42235L, 37918L, 
36234L, 36965L, 39535L, NA), UnemplRt = c(5.7, 4.7, 3.8, 
3.7, 4.3, 8.5, 10.9, 10, 8.3, 7.1)), .Names = c(""Year"", ""total"", 
""UnemplRt""), class = ""data.frame"", row.names = c(NA, -10L))

# Make a time series out of the value
dependent &lt;- ts(Workbook1[1:9,]$total, start=c(2004), frequency=1)

# load forecast package
require(forecast)
# load independent variables in variables.
unemployment &lt;- ts(Workbook1[1:9,]$UnemplRt, start=c(2004), frequency=1)
    unemployment_future &lt;- ts(Workbook1[10:10,]$UnemplRt, start=c(2004), frequency=1)

# make a model that fits the history
fit2 &lt;- auto.arima(dependent, xreg=unemployment)

# generate a forecast with the already known unemployment rate for 2013.
fcast2 &lt;- forecast(fit2,xreg=unemployment_future)
fcast2
     Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95
2013       45168.02 38848.92 51487.12 35503.79 54832.25
</code></pre>

<p>This yield exactly the same result a a simple spreadsheet liniear regression.
 And I belive the suggested increase in home sales is too high, last year the projection turned out to be about 6% too high. So I'm trying to torture the numbers in some statically valid method to come up with a somewhat lower number for 2013</p>
"
"NaN","NaN"," 46661","<p>I'm using the R package <a href=""http://cran.r-project.org/web/packages/ltm/index.html"" rel=""nofollow"">ltm</a> to create a 2-parameter logistic regression.</p>

<p>The input matrix is sparse - many users have taken a small subset of the items in the item bank.</p>

<p>For some of my data sets i'm running into this error:</p>

<pre><code>Error in if (any(ind &lt;- pr == 0)) pr[ind] &lt;- sqrt(.Machine$double.eps) : 
  missing value where TRUE/FALSE needed
</code></pre>

<p>Not sure what the issue is.  Doesn't repro on most of my data sets.  Any thoughts?</p>
"
"0.0927749898843639","0.0944911182523068"," 46821","<p>I am producing a script for creating bootstrap samples from the <code>cats</code> dataset (from the <code>-MASS-</code> package). </p>

<p>Following the Davidson and Hinkley textbook [1] I ran a simple linear regression and adopted a fundamental non-parametric procedure for bootstrapping from iid observations, namely <strong>pairs resampling</strong>.</p>

<p>The original sample is in the form:</p>

<pre><code>Bwt   Hwt

2.0   7.0
2.1   7.2

...

1.9    6.8
</code></pre>

<p>Through an univariate linear model we want to explain cats hearth weight through their brain weight. </p>

<p>The code is:</p>

<pre><code>library(MASS)
library(boot)


##################
#   CATS MODEL   #
##################

cats.lm &lt;- glm(Hwt ~ Bwt, data=cats)
cats.diag &lt;- glm.diag.plots(cats.lm, ret=T)


#######################
#   CASE resampling   #
#######################

cats.fit &lt;- function(data) coef(glm(data$Hwt ~ data$Bwt)) 
statistic.coef &lt;- function(data, i) cats.fit(data[i,]) 

bootl &lt;- boot(data=cats, statistic=statistic.coef, R=999)
</code></pre>

<p>Suppose now that there exists a clustering variable <code>cluster = 1, 2,..., 24</code> (for instance, each cat belongs to a given litter). For simplicity, suppose that data are balanced: we have 6 observations for each cluster. Hence, each of the 24 litters is made up of 6 cats (i.e. <code>n_cluster = 6</code> and <code>n = 144</code>).</p>

<p>It is possible to create a fake <code>cluster</code> variable through:</p>

<pre><code>q &lt;- rep(1:24, times=6)
cluster &lt;- sample(q)
c.data &lt;- cbind(cats, cluster)
</code></pre>

<p>I have two related questions:</p>

<p>How to simulate samples in accordance with the (clustered) dataset strucure? That is, <strong>how to resample at the cluster level?</strong> I would like to sample the clusters with replacement and to set the observations within each selected cluster as in the original dataset (i.e. sampling with replacenment the clusters and without replacement the observations within each cluster). </p>

<p>This is the strategy proposed by Davidson (p. 100). 
Suppose we draw <code>B = 100</code> samples. Each of them should be composed by 24 possibly recurrent clusters (e.g. <code>cluster = 3, 3, 1, 4, 12, 11, 12, 5, 6, 8, 17, 19, 10, 9, 7, 7, 16, 18, 24, 23, 11, 15, 20, 1</code>), and each cluster should contain the same 6 observations of the original dataset. How to do that in <code>R</code>? (either with or without the <code>-boot-</code> package.) Do you have alternative suggestions for proceeding?</p>

<p>The second question concerns the initial regression model. Suppose I adopt a <strong>fixed-effects model</strong>, with cluster-level intercepts. <strong>Does it change the resampling procedure</strong> adopted? </p>

<p>[1] Davidson, A. C., Hinkley, D. V. (1997). <em>Bootstrap methods and their applications</em>. Cambridge University press.</p>
"
"0.0884574820723792","0.0810843716426003"," 46978","<p>I am fitting a <em>Fixed-Effects</em> model, with intercepts at <code>cluster</code> level.</p>

<p>One of the most direct ways is probably to use the <code>-plm-</code> package. Another well-known possibility is to apply OLS (i.e. to adopt <code>-lm-</code>) to the <em>demeaned data</em>, where the means are taken at the clustering level.</p>

<p>This second approach is usually referred to as the <strong>within transformation</strong>. It is quite convenient from a computational standpoint, because we are still controlling unobserved heterogeneity at clustering level, but we do not need to estimate all the time-fixed intercepts.</p>

<p>I have tried both of these approaches, and I came to a strange result. In practice, the coefficient of the regressor of interest, <code>x</code>, is the same in both cases. However, its standard error (and actually all the other relevant quantities of the regression: R squared, F test, etc.) is different.</p>

<p>Please, notice that I have carefully read both the <em>R documentation</em> about <code>-plm-</code> and the <a href=""http://www.google.it/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;cad=rja&amp;ved=0CD4QFjAB&amp;url=http://www.jstatsoft.org/v27/i02/paper&amp;ei=7f3mUP_0DYrXtAaD7oDADw&amp;usg=AFQjCNFu_xrsnFYsC8j8DDh9mRQnoyQ6jg&amp;bvm=bv.1355534169,d.bGE"" rel=""nofollow"">related paper of the authors</a>, where it is stated that the package apply the <em>within transformation</em> and then apply OLS, as I did...</p>

<p>The R script is:</p>

<pre><code># set seed, load packages, create fake sample

set.seed(999)
library(plyr)
library(plm)

dat &lt;- expand.grid(id=factor(1:3), cluster=factor(1:6))
dat &lt;- cbind(dat, x=runif(18), y=runif(18, 2, 5))


############################
#   FE model using -plm-   #
############################

# model fit  
fe.1 &lt;- plm(y ~ x, data=dat, index=""cluster"", model=""within"")

# estimated coefficient and standard error of x
b.1 &lt;- summary(fe.1)$coefficients[,1]
    se.1 &lt;- summary(fe.1)$coefficients[,2]


######################################
#   OLS on within-transformed data   #
######################################

# augmenting data frame with cluster-mean centered variables 
dat.2 &lt;- ddply(dat, .(cluster), transform, dem_x=x-mean(x), dem_y=y-mean(y))

# model fit
fe.2 &lt;- lm(dem_y ~ dem_x - 1, data=dat.2)

# estimated coefficient and standard error of x
b.2 &lt;- summary(fe.2)$coefficients[1,1]
    se.2 &lt;- summary(fe.2)$coefficients[1,2]


#########################
#   models comparison   #
#########################

b.1; b.2
se.1; se.2

summary(fe.1)
summary(fe.2)
</code></pre>

<p>Notice that in the second model it is necessary to manually eliminate the intercept from the model. </p>
"
"0.0685188709827532","0.0697863157798853"," 47302","<p>I am currently working on time series modeling, especially on stationarity tests. For this purpose, I am extensively using Pfaff's book ""<a href=""http://www.springer.com/statistics/statistical+theory+and+methods/book/978-0-387-75966-1"" rel=""nofollow"">Analysis of integrated and cointegrated time series with R</a>"" and I have some questions :</p>

<ol>
<li><p>On page 63, there is a nice ordinogram (Figure 3.3) explaining how all the ADF tests are related, and what should be the underlying decision tree. First of all, one needs to estimate the ADF equation with a linear trend and test for $\pi=0$ (this statistic is called <code>tau3</code> in the associated package <a href=""http://cran.r-project.org/web/packages/urca/index.html"" rel=""nofollow"">urca</a>). If we reject the null hypothesis, then there is no unit root. If we cannot reject, we test for $\beta_2=0$ given $\pi=0$ (this statistic is called <code>phi3</code> in <code>urca</code>). If we reject, then Pfaff writes ""test again for a unit root using a standardized normal"" with no further explanation.</p>

<p>Does anyone understand what he is talking about? Does this ""normal test"" appear somewhere in the urca implementation?</p></li>
<li><p>Suppose the test <code>tau3</code> for $\pi=0$ is rejected. Then the conclusion should be that there is no unit root but a trend in the series (the series is trend stationary). I have at disposal  the underlying linear regression result given by <code>ur.df()</code> from the package <code>urca</code>. Is it correct to conclude that there is actually no trend when the p-value of the t-statistic for the trend coefficient is significant? </p></li>
</ol>

<p>Thanks in advance for your help.</p>
"
"0.220285106552881","0.220798616890548"," 48040","<p>I'm trying to test the significance of the ""component"" effect in a multivariate regression model. I'm not sure what is the right way. Using R, I have tried a way with <code>lm()</code> and another way with <code>gls()</code>, and they don't yield compatible results. </p>

<p><strong>Please note that this is not a question about which methodology is the right one to use to analyze my data. By the way these are simulated data. My question is about the understanding in mathematical terms of the R procedures I use.</strong></p>

<p>The dataset:</p>

<pre><code>&gt; str(dat)
'data.frame':   31 obs. of  5 variables:
 $ group: Factor w/ 5 levels ""1"",""5"",""2"",""3"",..: 1 1 1 1 1 1 1 1 3 3 ...
     $ id   : Factor w/ 8 levels ""1"",""2"",""3"",""4"",..: 1 2 3 4 5 6 7 8 1 3 ...
 $ x    : num  2.5 3 3 4 1.2 3.8 3.9 4 2.5 2.9 ...
     $ y    : num  2.6 3.8 3.9 3.8 1.6 5.2 1.3 3.6 4 3.2 ...
 $ z    : num  3.1 3.6 4.9 3.8 2.1 6 2.1 2.9 4.2 2.9 ...
&gt; head(dat,10)
   group id   x   y   z
1      1  1 2.5 2.6 3.1
2      1  2 3.0 3.8 3.6
3      1  3 3.0 3.9 4.9
4      1  4 4.0 3.8 3.8
5      1  5 1.2 1.6 2.1
6      1  6 3.8 5.2 6.0
7      1  7 3.9 1.3 2.1
8      1  8 4.0 3.6 2.9
9      2  1 2.5 4.0 4.2
10     2  3 2.9 3.2 2.9
</code></pre>

<p>I convert this dataset into ""long format"" for graphics (and later for <code>gls()</code>):</p>

<pre><code>dat$subject &lt;- dat$group : dat$id
    dat.long &lt;- reshape(dat, direction=""long"", varying=list(3:5), 
    	idvar=""subject"", v.names=""value"", timevar=""component"", times=c(""x"",""y"",""z""))
    dat.long$component &lt;- factor(dat.long$component)

xyplot(value ~ component | group, data=dat.long, 
    pch=16, 
    strip = strip.custom(strip.names=TRUE,var.name=""group"" ), layout=c(5,1))
</code></pre>

<p><img src=""http://i.stack.imgur.com/14KtA.png"" alt=""enter image description here""></p>

<p>Each individual of each group has $3$ repeated measures $x$,$y$,$z$ (I should join the points in the graphic to see the repeated measures).</p>

<p>I want to fit a MANOVA model using group as factor and $(x,y,z)$ is the multivariate response:
$$
\begin{pmatrix} x_{ij} \\ y_{ij} \\ z_{ij} \end{pmatrix} 
\sim {\cal N}_3\left( 
\begin{matrix} \mu_{i1} \\ \mu_{i2} \\ \mu_{i3} \end{matrix}, 
\Sigma \right), \quad i=1,\ldots,5
$$
(of course we could use the default R parameterization $\mu_{ik}=\mu_{1k} + \alpha_{ik}$ by considering <code>group1</code>as the ""intercept"" for each response but I prefer ""my"" parameterization).</p>

<p>This model is fitted as follows using <code>lm()</code>:</p>

<pre><code>###  multivariate least-squares fitting  ###
mfit &lt;- lm( cbind(x,y,z)~group, data=dat )
</code></pre>

<p>I think the model can also  be fitted with <code>gls()</code> as follows (but with a different fitting procedure) :</p>

<pre><code>### generalized least-squares fitting  ###
library(nlme)
gfit &lt;- gls(value ~ group*component, data=dat.long, correlation=corSymm(form= ~ 1 | subject))
</code></pre>

<p>Recall that <code>subject = group:id</code> is the identifier of the individuals. The <code>correlation=corSymm(form= ~ 1 | subject)</code> argument means that the responses $x$, $y$, $z$ for each individual are correlated. Here <code>corSymm</code> means a general, ""unrestricted"",  covariance structure (termed as ""unstructured"" in SAS language).</p>

<p>To check that <code>mfit</code> and <code>gfit</code> are equivalent, we can check for instance that we can deduce the estimated parameters of <code>mfit</code> from the estimated parameters of <code>gfit</code>and vice-versa (so the ""mean"" parameters have exactly the same fitted values):</p>

<pre><code>&gt; coef(mfit)
                  x          y          z
(Intercept)  3.1750  3.2250000  3.5625000
group5      -0.9500 -0.4750000  0.1125000
group2      -1.0750 -0.5678571 -0.2339286
group3      -0.7875 -0.1000000  0.1875000
group4      -0.3750  0.4000000 -0.0125000
&gt; coef(gfit)
      (Intercept)            group5            group2            group3 
        3.1750000        -0.9500000        -1.0750000        -0.7875000 
           group4        componenty        componentz group5:componenty 
       -0.3750000         0.0500000         0.3875000         0.4750000 
group2:componenty group3:componenty group4:componenty group5:componentz 
        0.5071429         0.6875000         0.7750000         1.0625000 
group2:componentz group3:componentz group4:componentz 
        0.8410714         0.9750000         0.3625000 
</code></pre>

<p>Now I want to test the ""component effect"". Rigorously speaking, writing the model as 
$$
\begin{pmatrix} x_{ij} \\ y_{ij} \\ z_{ij} \end{pmatrix} 
\sim {\cal N}_3\left( 
\begin{matrix} \mu_{i1} \\ \mu_{i2} \\ \mu_{i3} \end{matrix}, 
\Sigma \right),
$$
I want to test the hypothesis $\boxed{H_0\colon \{\mu_{1i}=\mu_{2i}=\mu_{3i} \quad \forall i=1,2,3,4,5 \}}$.</p>

<p>Below are my attempts, one attempt with <code>gfit</code> and two attempts with <code>mfit()</code>:</p>

<pre><code>###########################################
## testing significance of the component ##
###########################################

&gt; ### with gfit  ###
&gt; anova(gfit)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 504.5226  &lt;.0001
group               4   0.7797  0.5418
component           2  11.7073  &lt;.0001
group:component     8   0.7978  0.6063
&gt; 
&gt; ### with mfit ###
&gt; library(car)
&gt; 
&gt; # first attempt : 
&gt; idata &lt;- data.frame(component=c(""x"",""y"",""z""))
&gt; ( av.ok &lt;- Anova(mfit, idata=idata, idesign=~component, type=""III"") )

Type III Repeated Measures MANOVA Tests: Pillai test statistic
                Df test stat approx F num Df den Df    Pr(&gt;F)    
(Intercept)      1   0.84396  140.625      1     26 5.449e-12 ***
group            4   0.10369    0.752      4     26    0.5658    
component        1   0.04913    0.646      2     25    0.5328    
group:component  4   0.22360    0.818      8     52    0.5901    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
&gt; 
&gt; # second attempt :
&gt; linearHypothesis(mfit, ""(Intercept) = 0"", idata=idata, idesign=~component, iterms=""component"")

 Response transformation matrix:
  component1 component2
x          1          0
y          0          1
z         -1         -1

Sum of squares and products for the hypothesis:
           component1 component2
component1    1.20125    1.04625
component2    1.04625    0.91125

Sum of squares and products for error:
           component1 component2
component1   31.46179   14.67696
component2   14.67696   21.42304

Multivariate Tests: 
                 Df test stat  approx F num Df den Df  Pr(&gt;F)
Pillai            1 0.0491253 0.6457903      2     25 0.53277
Wilks             1 0.9508747 0.6457903      2     25 0.53277
Hotelling-Lawley  1 0.0516632 0.6457903      2     25 0.53277
Roy               1 0.0516632 0.6457903      2     25 0.53277
</code></pre>

<p>With <code>anova(gfit)</code> the component is significant, but not with my two attempts using <code>mfit</code> and the <code>car</code> package. </p>

<p>I know that <code>gls()</code> use a different fitting method than <code>lm()</code> but this is surely not the cause of the difference. </p>

<p>So my questions are :</p>

<ul>
<li>did I do something wrong ?</li>
<li>which method tests my $H_0$ hypothesis ?</li>
<li>what is the $H_0$ hypothesis of the other methods ?</li>
</ul>

<p>And I have an auxiliary question: how to get $\hat\Sigma$ with <code>mfit</code> and <code>gfit</code> ?</p>

<h2>Update 1</h2>

<p>Below is a reproducible example which simulates the dataset. 
Now I think I understand : both ANOVA methods are correct (the first one with <code>anova(gfit)</code> and the second one with <code>Anova(mfit, ...)</code>, <strong>and they yield very close results when using the type II sum of squares in <code>Anova(mfit, ...)</code></strong>.  For the above example: </p>

<pre><code>&gt; Anova(mfit, idata=idata, idesign=~component, type=""II"")

Type II Repeated Measures MANOVA Tests: Pillai test statistic
                Df test stat approx F num Df den Df    Pr(&gt;F)    
(Intercept)      1   0.94691   463.72      1     26 &lt; 2.2e-16 ***
group            4   0.10369     0.75      4     26 0.5657691    
component        1   0.50344    12.67      2     25 0.0001584 ***
group:component  4   0.22360     0.82      8     52 0.5900848    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>is very close to </p>

<pre><code>&gt; anova(gfit)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 504.5226  &lt;.0001
group               4   0.7797  0.5418
component           2  11.7073  &lt;.0001
group:component     8   0.7978  0.6063
</code></pre>

<p>Below is the reproducible code with the data sampler (I simulate uncorrelated repeated measures but it suffices to include a covariance matrix in the <code>rmvnorm()</code> function to simulate correlated repeated measures) :</p>

<pre><code>library(mvtnorm)
library(nlme)
library(car)

# set data parameters 
I &lt;- 5 # number of groups
J &lt;- 16 # number of individuals per group
dat &lt;- data.frame(
    group = gl(I,J),
    id = gl(J,1,I*J),
    x=NA, 
    y=NA, 
    z=NA
)
Mu &lt;- c(1:I) # group means of components (assuming E(x)=E(y)=E(z) in each group)

# simulates data: 
for(i in 1:I){
    which.group.i &lt;- which(dat$group==i)
    dat[which.group.i,c(""x"",""y"",""z"")] &lt;- round(rmvnorm(n=J, mean=rep(Mu[i],3)), 1)
}

dat$subject &lt;- droplevels( dat$group : dat$id )
    dat.long &lt;- reshape(dat, direction=""long"", varying=list(3:5), 
    	idvar=""subject"", v.names=""value"", timevar=""component"", times=c(""x"",""y"",""z""))
    dat.long$component &lt;- factor(dat.long$component)

# multivariate least-squares fitting 
mfit &lt;- lm( cbind(x,y,z)~group, data=dat )

# gls fitting
dat.long$order.xyz &lt;- as.numeric(dat.long$component)
gfit &lt;- gls(value ~ group*component , data=dat.long, correlation=corSymm(form=  ~ order.xyz | subject)) 

# compares ANOVA : 
anova(gfit)
idata &lt;- data.frame(component=c(""x"",""y"",""z""))
Anova(mfit, idata=idata, idesign=~component, type=""II"")
Anova(mfit, idata=idata, idesign=~component, type=""III"")
</code></pre>

<p>So now I wonder which type of sum of squares is the more appropriate one for my real study... but this is another question</p>

<h2>Update 2</h2>

<p>About my question <em>""how to get $\hat\Sigma$""</em>, here is the answer for <code>gls()</code>:</p>

<pre><code>&gt; getVarCov(gfit)
Marginal variance covariance matrix
        [,1]    [,2]    [,3]
[1,] 0.92909 0.47739 0.24628
[2,] 0.47739 0.92909 0.53369
[3,] 0.24628 0.53369 0.92909
  Standard Deviations: 0.96389 0.96389 0.96389 
</code></pre>

<p>That shows that <strong><code>mfit</code>and <code>gfit</code> were not equivalent models</strong>: <code>gfit</code>assumes the same variance for the three components.</p>

<p>In order to fit a fully unrestricted covariance matrix for the repeated measures, we have to type:</p>

<pre><code>gfit2 &lt;- gls(value ~ group*component , data=dat.long, 
    correlation=corSymm(form=  ~ 1 | subject), 
    weights=varIdent(form = ~1 | component))

&gt; summary(gfit2)
Generalized least squares fit by REML
  Model: value ~ group * component 
  Data: dat.long 
       AIC      BIC    logLik
  264.0077 313.4986 -111.0038

Correlation Structure: General
 Formula: ~1 | subject 
 Parameter estimate(s):
 Correlation: 
  1     2    
2 0.529      
3 0.300 0.616
Variance function:
 Structure: Different standard deviations per stratum
 Formula: ~1 | component 
 Parameter estimates:
       x        y        z 
1.000000 1.253534 1.169335 

....

Residual standard error: 0.8523997 
</code></pre>

<p>But yet I don't understand the extracted covariance matrix given by <code>getVarCov()</code> (but this is not important since we get this matrix with <code>summary(gfit2)</code>): </p>

<pre><code>   &gt; getVarCov(gfit2)
    Error in t(S * sqrt(vars)) : 
      dims [product 9] do not match the length of object [0]
    &gt; getVarCov(gfit2, individual=""1:1"")
    Marginal variance covariance matrix
            [,1]    [,2]    [,3]
    [1,] 0.72659 0.48164 0.25500
    [2,] 0.48164 1.14170 0.65562
    [3,] 0.25500 0.65562 0.99349
      Standard Deviations: 0.8524 1.0685 0.99674 
    &gt; getVarCov(gfit2, individual=""1:2"")
    Marginal variance covariance matrix
            [,1]    [,2]    [,3]
    [1,] 1.14170 0.56319 0.27337
    [2,] 0.56319 0.99349 0.52302
    [3,] 0.27337 0.52302 0.72659
      Standard Deviations: 1.0685 0.99674 0.8524 
</code></pre>

<p>Unfortunately, the <code>anova(gfit2)</code> table is not as close to <code>Anova(mfit, ..., type=""II"")</code> as <code>anova(gfit)</code>:</p>

<pre><code>&gt; anova(gfit2)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 498.1744  &lt;.0001
group               4   1.0514  0.3864
component           2  13.1801  &lt;.0001
group:component     8   0.8310  0.5780

&gt; Anova(mfit, idata=idata, idesign=~component, type=""II"")

Type II Repeated Measures MANOVA Tests: Pillai test statistic
                Df test stat approx F num Df den Df    Pr(&gt;F)    
(Intercept)      1   0.94691   463.72      1     26 &lt; 2.2e-16 ***
group            4   0.10369     0.75      4     26 0.5657691    
component        1   0.50344    12.67      2     25 0.0001584 ***
group:component  4   0.22360     0.82      8     52 0.5900848    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

&gt; anova(gfit)
Denom. DF: 78 
                numDF  F-value p-value
(Intercept)         1 504.5226  &lt;.0001
group               4   0.7797  0.5418
component           2  11.7073  &lt;.0001
group:component     8   0.7978  0.6063
</code></pre>
"
"0.108550066801774","0.110557998564556"," 48381","<p>I'm trying to estimate power in a logistic regression with a continuous exposure in a cohort study (ie, the ratio of the sampling probabilities is 1). I have population cumulative incidence (probability) and population exposure variability and exposure mean and an expected odds ratio. I also have a total sample size.</p>

<p>I'm using R and it seems like <code>Hmisc::bpower</code> is only for logistic regression with binary exposure and I can't seem to find any packages that estimate binomial power with continuous exposure.</p>

<p>I've attempted the following simulation but it's quite slow given my total sample size and I'm not sure if it's right:</p>

<pre><code>p &lt;- vector()
betahat &lt;- vector()
for(i in 1:1000){
n &lt;- 40000  #total sample size
intercept = log(0.008662265)  #where exp(intercept) = P(D=1)
beta &lt;- log(1.4) #where exp(beta)=OR corresponding to a one unit change in xtest
xtest &lt;- rnorm(n,1.2,.31)  #xtest is vector length 40,000 with mean 1.2 and sd .31
linpred &lt;- intercept + xtest*beta #linear predictor
prob &lt;- exp(linpred)/(1 + exp(linpred)) #link function
runis &lt;- runif(n,0,1) #generate a vector length n from a uniform distribution 0,1
ytest &lt;- ifelse(runis &lt; prob,1,0)  #if a random value from a uniform distribution 0,1 is less than prob, then the outcome is 1.  otherwise the outcome is 0
coefs &lt;- coef(summary(glm(ytest~xtest, family=""binomial"")))  #run a logistic regression
p[i] &lt;- coefs[2,4] #store the p value
betahat[i] &lt;- coefs[2,1] #store the unexponentiated betahat
}

mean(p &lt; .05)
#power

exp(mean(betahat))
#sanity check, should equal 1.4--it does
</code></pre>

<p>Is there anything wrong with this approach?</p>

<p>One concern of mine is that the cumulative incidence (ie, probability of event over the given time period) comes from a population that did not have 0 exposure.  In fact, it's reasonable to assume that the value i'm using for an intercept is actually from a population that has an exposure variability similar to mine. In that case, how would I estimate the unexposed probability given an odds ratio (and other information that I would find in say, a published paper) to use in my power calculation?</p>
"
"0.122486986448998","0.130693325543483"," 48415","<p>I come to you today because I face a huge problem that I cannot explain.</p>

<p>I have run a multinomial logistic regression (using the mlogit package) on behavioral data. I prepare the data by doing</p>

<pre><code>    mlogit &lt;- mlogit.data(Merge, choice = ""Choice"", shape = ""long"", alt.var = ""Comp"", 
                          drop.index = TRUE)
</code></pre>

<p>on my Merge data.</p>

<p>which gives me the following:</p>

<pre><code>                Date     Time ActivityX ActivityY Temp Behavior Valley Age Month Year kid Individual Choice
    1.F   01/05/2012 00:00:00        80        58   10        F  Fuorn   8     5 2012   Y         26   TRUE
    1.R   01/05/2012 00:00:00        80        58   10        F  Fuorn   8     5 2012   Y         26  FALSE
    1.M   01/05/2012 00:00:00        80        58   10        F  Fuorn   8     5 2012   Y         26  FALSE
    1.RUN 01/05/2012 00:00:00        80        58   10        F  Fuorn   8     5 2012   Y         26  FALSE
    2.F   01/05/2012 00:05:00        90        76   10        F  Fuorn   8     5 2012   Y         26   TRUE
    2.R   01/05/2012 00:05:00        90        76   10        F  Fuorn   8     5 2012   Y         26  FALSE
    2.M   01/05/2012 00:05:00        90        76   10        F  Fuorn   8     5 2012   Y         26  FALSE
    2.RUN 01/05/2012 00:05:00        90        76   10        F  Fuorn   8     5 2012   Y         26  FALSE
    3.F   01/05/2012 00:10:00        51        47   10        M  Fuorn   8     5 2012   Y         26  FALSE
    3.R   01/05/2012 00:10:00        51        47   10        M  Fuorn   8     5 2012   Y         26  FALSE
    3.M   01/05/2012 00:10:00        51        47   10        M  Fuorn   8     5 2012   Y         26   TRUE
    3.RUN 01/05/2012 00:10:00        51        47   10        M  Fuorn   8     5 2012   Y         26  FALSE
    4.F   01/05/2012 00:15:00         0         0   10        R  Fuorn   8     5 2012   Y         26  FALSE
    4.R   01/05/2012 00:15:00         0         0   10        R  Fuorn   8     5 2012   Y         26   TRUE
    4.M   01/05/2012 00:15:00         0         0   10        R  Fuorn   8     5 2012   Y         26  FALSE
    4.RUN 01/05/2012 00:15:00         0         0   10        R  Fuorn   8     5 2012   Y         26  FALSE
    5.F   01/05/2012 00:20:00         0         0    9        R  Fuorn   8     5 2012   Y         26  FALSE
    5.R   01/05/2012 00:20:00         0         0    9        R  Fuorn   8     5 2012   Y         26   TRUE
    5.M   01/05/2012 00:20:00         0         0    9        R  Fuorn   8     5 2012   Y         26  FALSE
    5.RUN 01/05/2012 00:20:00         0         0    9        R  Fuorn   8     5 2012   Y         26  FALSE
</code></pre>

<p>then I ran my regression :</p>

<pre><code>m1 &lt;- mlogit(Choice ~ 1 |Temp + Valley + Age + kid + Month , mlogit)
</code></pre>

<p>and it gave me significant results :</p>

<pre><code>                          Estimate  Std. Error  t-value  Pr(&gt;|t|)    
    M:(intercept)      -4.2153e-01  5.7533e-02  -7.3268 2.358e-13 ***
    R:(intercept)       6.2325e-01  3.4958e-02  17.8284 &lt; 2.2e-16 ***
    RUN:(intercept)    -1.2275e+01  4.0526e-01 -30.2895 &lt; 2.2e-16 ***
    M:Temp              1.5371e-02  9.8680e-04  15.5764 &lt; 2.2e-16 ***
    R:Temp             -3.9871e-02  6.7926e-04 -58.6975 &lt; 2.2e-16 ***
    RUN:Temp           -4.4532e-02  6.8696e-03  -6.4825 9.023e-11 ***
    M:ValleyTrupchun   -3.6154e-01  1.6362e-02 -22.0968 &lt; 2.2e-16 ***
    R:ValleyTrupchun   -4.0186e-02  9.7968e-03  -4.1020 4.096e-05 ***
    RUN:ValleyTrupchun  1.2895e+00  8.5357e-02  15.1066 &lt; 2.2e-16 ***
    M:Age              -1.1026e-02  2.6902e-03  -4.0985 4.158e-05 ***
    R:Age               1.9465e-02  1.6479e-03  11.8119 &lt; 2.2e-16 ***
    RUN:Age             5.5473e-02  1.6661e-02   3.3294 0.0008703 ***
    M:kidY              6.0686e-02  2.2638e-02   2.6807 0.0073460 ** 
    R:kidY             -4.1638e-01  1.2391e-02 -33.6024 &lt; 2.2e-16 ***
    RUN:kidY            6.2311e-01  1.0410e-01   5.9854 2.158e-09 ***
    M:Month            -2.0466e-01  8.4448e-03 -24.2346 &lt; 2.2e-16 ***
    R:Month             2.4148e-02  5.2317e-03   4.6157 3.917e-06 ***
    RUN:Month           9.8715e-01  5.6209e-02  17.5622 &lt; 2.2e-16 ***
</code></pre>

<p>those results were in line with what I expected to find in literature so I was quite happy.</p>

<p>My next step was to plot my results and here is when I have some trouble.</p>

<p>First of all when I plot my original data and compare it with the result of my regression I find some huge differences. For example, when I plot the %of time spend in a behavior (M for moving, F for feeding, R for resting and Run for running, in my regression F is the reference) in function of age, I find that the older an individual gets, the more they will rest and the more they will move, but the estimates I got from my regression shows that they should rest more (when they get older) but move less. So to summarize, my graph on the original data shows the opposite as what I got from the regression.</p>

<p>I don't know if it is normal, in the sense that I don't know if I can compare my original data to the result of my regression in a way that my regression shows the probability from switching to a behavior from an other each time my variable grows of one unit.</p>

<p>So I wanted to use the <code>predict()</code> function but I don't know how to do that. I was hoping to get some help here.</p>
"
"0.08882529023711","0.0904683580569682"," 48512","<p>I have a regression in which I try to understand how much variance of the metric dependent variable each of the regressors explains. I use the package R <a href=""http://cran.r-project.org/web/packages/relaimpo/index.html"" rel=""nofollow"">relaimpo</a> (GrÃ¶mping, 2006) for that purpose, that allocates $R^2$ shares to each regressor using the LMG metric. The hypothesis is that the regressors differ drastically in their $R^2$ contribution. </p>

<p>If I had a cross-sectional sample of $N=6000$, things would be dandy. Unfortunately, I only have $N=2000$, but 3 measurement points (weeks 0, 6, 12). Both the dependent variable and regressors are time-varying, that is, they are assessed at each measurement point. </p>

<p>Currently I run the analyses separately for each measurement point and find large differences between relative importance estimates between regressors (0% to 25%), and I find that the ranking of regressors and the explained variance is surprisingly stable over time (at least from a qualitative perspective, plotting the explained variance of each of the 14 regressors for each measurement point). </p>

<p>For a scientific paper, there are two reasons why I want to do this in one regression instead of 3. (1) 3 different analyses take up a lot of space in the paper and obfuscate the main message a bit (regressors differ drastically in their relative importance). (2) $N=2000$ isn't all that much when disentangling the relative contributions of 14 correlated regressors (the CIs are rather large). </p>

<p>Therefore, I wondered whether there are ways to ""pool"" all subjects into one regression that would not lead to an outcry of anybody with some statistical background (""you severely violated the assumption of statistical independence!!""). In the best case I would simply have one regression that covers all 3 time points.</p>

<p>The method has to be a regression (ie., not using NLE or LME packages), because that is what the <code>relaimpo</code> package uses as baseline model to then calculate unique $R^2$ contributions of regressors. </p>

<p>What are my options?</p>
"
"0.0395593886064618","0.0201455741006345"," 48651","<p>I am looking for a Least Angle Regression (LAR) packages in R or MATLAB which can be used for <strong>classification</strong> problems.</p>

<p>The only package that I currently know which fits this description is <a href=""http://cran.r-project.org/web/packages/glmpath/index.html"" rel=""nofollow"">glmpath</a>. The issue with this package is that it is a little old and somewhat limited in its scope (I am forced to rely on logistic regression for classification problems model).</p>

<p>I am wondering if anyone knows of other packages that allow me to run LAR on different types of classification models, such as Support Vector Machines (see <a href=""http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.62.391&amp;rep=rep1&amp;type=pdf"" rel=""nofollow"">The Entire Regularization Path for the Support Vector Machine</a>). </p>

<p>The ideal package would allow me to run LAR-type algorithms for different types of classification models and also provide a function that can produce the full regularization path. </p>
"
"0.027972711943223","0.0284901441149095"," 48777","<p>I need to automate the transformation on some linear regression models. There is only one predictor in this case. Sometimes i get a good model with the original variables, sometimes i need to log the predictor, and in some cases log both sides.</p>

<p>I'm using R, so what kind of tests/packages can i use to automate this? I'm using Pearson correlation now, but i'm not sure if it makes sense.</p>

<p>thanks!</p>

<p>PS: This may look a duplicate question, but i couldn't find yet the methodology to apply.</p>
"
"0.0484501583111509","0.0493463771219827"," 48811","<p>For count data that I have collected, I use Poisson regression to build models. I do this using the <code>glm</code> function in R, where I use <code>family = ""poisson""</code>. To evaluate possible models (I have several predictors) I use the AIC. So far so good. Now I want to perform cross-validation. I already succeeded in doing this using the <code>cv.glm</code> function from the <code>boot</code> package. From <a href=""http://stat.ethz.ch/R-manual/R-patched/library/boot/html/cv.glm.html"">the documentation</a> of <code>cv.glm</code> I see that e.g. for binomial data you need to use a specific cost function to get a meaningful prediction error. However, I have no idea yet what cost function is appropriate for <code>family = poisson</code>, and an extensive Google search did not yield any specific results. My question is anybody has some light to shed on which cost function is appropriate for <code>cv.glm</code> in case of poisson glm's.</p>
"
"0.0395593886064618","0.040291148201269"," 48854","<p>I have linear regression code written in R and I have to do the same thing in Java. I used <a href=""http://commons.apache.org/math/apidocs/overview-summary.html"" rel=""nofollow"">Apache Commons math</a> library for this. I used the same data in R code and in Java code, but I got different intercept value. I could not figure out what stupid thing I have done in the code.</p>

<p><strong>R Code:</strong></p>

<pre><code>test_trait &lt;- c( -0.48812477 , 0.33458213, -0.52754476, -0.79863471, -0.68544309, -0.12970239,  0.02355622, -0.31890850,0.34725819 , 0.08108851)
geno_A &lt;- c(1, 0, 1, 2, 0, 0, 1, 0, 1, 0)
geno_B &lt;- c(0, 0, 0, 1, 1, 0, 0, 0, 0, 0) 
fit &lt;- lm(test_trait ~ geno_A*geno_B)
fit
</code></pre>

<p><strong>R Output</strong>:</p>

<pre><code>Call:
lm(formula = test_trait ~ geno_A * geno_B)

Coefficients:
  (Intercept)         geno_A         geno_B  geno_A:geno_B  
    -0.008235      -0.152979      -0.677208       0.096383 
</code></pre>

<p><strong>Java Code (includes EDIT1):</strong></p>

<pre><code>package linearregression;
import org.apache.commons.math3.stat.regression.SimpleRegression;
public class LinearRegression {
    public static void main(String[] args) {

        double[][] x = {{1,0},
                        {0,0},
                        {1,0},
                        {2,1},
                        {0,1},
                        {0,0},
                        {1,0},
                        {0,0},
                        {1,0},
                        {0,0}
        };

        double[]y = { -0.48812477,
                       0.33458213,
                      -0.52754476,
                      -0.79863471,
                      -0.68544309,
                      -0.12970239,
                       0.02355622,
                      -0.31890850,
                       0.34725819,
                       0.08108851
        };
        SimpleRegression regression = new SimpleRegression(true);
        regression.addObservations(x,y);

        System.out.println(""Intercept: \t\t""+regression.getIntercept());
// EDIT 1 -----------------------------------------------------------
System.out.println(""InterceptStdErr: \t""+regression.getInterceptStdErr());
System.out.println(""MeanSquareError: \t""+regression.getMeanSquareError());
System.out.println(""N: \t\t\t""+regression.getN());
System.out.println(""R: \t\t\t""+regression.getR());
System.out.println(""RSquare: \t\t""+regression.getRSquare());
System.out.println(""RegressionSumSquares: \t""+regression.getRegressionSumSquares());
System.out.println(""Significance: \t\t""+regression.getSignificance());
System.out.println(""Slope: \t\t\t""+regression.getSlope());
System.out.println(""SlopeConfidenceInterval: ""+regression.getSlopeConfidenceInterval());
System.out.println(""SlopeStdErr: \t\t""+regression.getSlopeStdErr());
System.out.println(""SumOfCrossProducts: \t""+regression.getSumOfCrossProducts());
System.out.println(""SumSquaredErrors: \t""+regression.getSumSquaredErrors());
System.out.println(""XSumSquares: \t\t""+regression.getXSumSquares());
// EDIT1 ends here --------------------------------------------------

    }
}
</code></pre>

<p><strong>Java Output:</strong></p>

<pre><code>Intercept:      -0.08732359363636362
</code></pre>

<p><strong>Java Output of EDIT1:</strong></p>

<pre><code>Intercept:      -0.08732359363636362
InterceptStdErr:    0.17268454347538026
MeanSquareError:    0.16400973355415271
N:          10
R:          -0.3660108396736771
RSquare:        0.13396393475863017
RegressionSumSquares:   0.20296050132281976
Significance:       0.2982630977579106
Slope:          -0.21477287227272726
SlopeConfidenceInterval: 0.4452137360615129
SlopeStdErr:        0.193067188937234
SumOfCrossProducts:     -0.945000638
SumSquaredErrors:   1.3120778684332217
XSumSquares:        4.4
</code></pre>

<p>I will greatly appreciate your help. Thanks !</p>
"
"0.0791187772129236","0.080582296402538"," 48863","<p>Consider the following scenario: I have an auto dealership, and I want to decide whether to buy a used car. If it's a good buy, I would make a substantial amount of money of it. If it's a bad buy, I would lose a substantial amount of money. If it's neither (""nop"") , I would lose a small amount of money.</p>

<p>Naturally, I start with training and testing sets, and then use a simple 2-class classifier, that aims to predict whether it's a good buy or a bad buy ( I used <a href=""http://cran.r-project.org/web/packages/gbm/index.html"" rel=""nofollow"">gbm</a> ,whose latest version, by the way, now includes also multinomial regression!). </p>

<p>Once I have my classifier, I can look at my testing set, and inspect the cars that the classifier told me to buy. with these cars, I would calculate my expected future earnings: </p>

<pre><code>   number of good buys X profit - number of bad buys x loss - number of 'nop' buys x smaller loss.
</code></pre>

<p>So, if you think about it, I actually shouldn't be trying to classify. I should try and find an area in my features space in which the given criterion is maximized. However, this sound like a combinatorial problem. </p>

<p>So (finally), my question is: what algorithm / method is there that I can use to maximize my cost function?</p>

<p>Thanks a lot!</p>
"
"0.0791187772129236","0.080582296402538"," 48922","<p>I am trying to estimate a selection model of the form:</p>

<p>$Z_i = 1[\alpha_0 + \alpha_1X_{1,i} + \alpha_2X_{2,i} + \delta_i$ > 0]</p>

<p>$Y_i = \beta_0 + \beta_1X_{1,i} + Z_i + \epsilon_i$</p>

<p>where $1[]$ denotes the indicator function.</p>

<p>The purpose of the model is to calculate the indirect effect of $X_1$ on $Y$ through $Z$, as well as the the direct effect.</p>

<p>My first question is how to go about estimating this type of model, and how this estimation can be achieved in R. As far as I see it I have a few possible approaches:</p>

<p>(1) Use a standard Heckman selection model, using OLS for both the reduced form and structural equations, using ivreg() in R. This will obviously ignore the constraint that $Z$ is bounded between 0 and 1.</p>

<p>(2) Estimate the first stage with a probit model (i.e. $\delta_i \sim N(0,1)$), and the second stage using standard OLS. I understand that I could do this via manual 2SLS, but as far as I am aware the standard errors will be incorrect? Am I right in that this model is feasible, and if so, can you direct me to a method of achieving this in R?</p>

<p>(3) Build a switching regression model (tobit-5) using the selection() function from sampleSelection package in R. I believe this model will estimate two equations for  $Y$, one for where $Z_i=0$ and one where $Z_i=1$, and with a unique intercept and coefficients for each of the regressors in the outcome equations.</p>

<p>The question then is how to get an estimate of the indirect effect of $X_1$ for each of these methods.</p>

<ul>
<li><p>If I use (1) or (2) then I imagine it might be possible to calculate the average marginal effect of $Z$ on $Y$, and the average marginal effect of $X_1$ on $Z$, then approximate the indirect effect by multiplying the two values?</p></li>
<li><p>If (3) then could I take the fitted value under the estimated model for $Y$ where $Z=0$, and compare the mean to the mean of the fitted values under the estimated model for $Y$ where $Z=1$? This would then give me an estimate of the marginal effect of $Z$? Then use the same method as above and multiple this effect by the marginal effect of $X_1$ on $Z$?</p></li>
</ul>

<p>Many thanks in advance!</p>
"
"0.0884574820723792","0.0900937462695559"," 49497","<p>I have a dataset I'm working on that has some co-variate shift between the training set and the test set.  I'm trying to build a predictive model to predict an outcome, using the training set.  So far my best model is a random forest.</p>

<p>How can I deal with the shifted distributions in the training vs. test set?  I've come across 2 possible solutions that I've been able to implement myself:</p>

<ol>
<li>Remove the shifted variables.  This is sub-optimal, but helps prevent my model from over fitting the training set.</li>
<li>Use a logistic regression to predict whether a given observation is from the test set (after balancing the classes), predict ""test set probabilities"" for the training set, and then boostrap sample the training set, using the probabilities for sampling.  Then fit the final model on the new training set.</li>
</ol>

<p>Both 1 and 2 are pretty easy to implement, but neither one satisfies me, as #1 omits variables that might be relevant, and #2 uses a logistic regression, when my final model is tree-based.  Furthermore, #2 takes a few paragraphs of custom code, and I worry that my implementation may not be correct.</p>

<p>What are the standard methods for dealing with covariate shift?  Are there any packages in R (or another language) that implement these methods?</p>

<p>/edit: It seems like ""kernel mean matching"" is another approach I could take.  I've found lots of academic papers on the subject, but no one seems to have published any code.  I'm going to try to implement this on my own, and will post the code as an answer to this question when I do.</p>
"
"NaN","NaN"," 49513","<p>I have a learning problem from $X$ to $Y$ where:</p>

<ul>
<li>$X$ = $n$ input numeric vectors of $m$ dimensions </li>
<li>$Y$ = $n$ output numeric vectors of $k$ dimensions</li>
</ul>

<p>In other words:</p>

<p>&nbsp; &nbsp; &nbsp; <img src=""http://i.stack.imgur.com/rgcru.png"" alt=""enter image description here""></p>

<p>I am hoping to collect a <strong>list</strong> of <strong>R</strong> packages or <strong>Python</strong> libraries for <strong>multiple-output</strong> problems  for classification and regression.</p>

<p>For example, do any of the learning methods in <a href=""http://cran.r-project.org/web/packages/caret/index.html"" rel=""nofollow"">caret</a> support this functionality? What packages in general are available for this problem?</p>
"
"0.100857047225074","0.102722675451665"," 49607","<h1>Background</h1>

<h2>Introduction</h2>

<p>I have a data set consisting of data collected from a questionnaire that I wish to validate. I have chosen to use confirmatory factor analysis to analyse this data set.</p>

<h2>Instrument</h2>

<p>The instrument consists of 11 subscales. There is a total of 68 items in the 11 subscales. Each item is scored on an integer scale between 1 to 4.</p>

<h2>Confirmatory factor analysis (CFA) setup</h2>

<p>I use the <code>sem</code> package to conduct the CFA. My code is as below:</p>

<pre><code>cov.mat &lt;- as.matrix(read.table(""http://dl.dropbox.com/u/1445171/cov.mat.csv"", sep = "","", header = TRUE))
rownames(cov.mat) &lt;- colnames(cov.mat)

model &lt;- cfa(file = ""http://dl.dropbox.com/u/1445171/cfa.model.txt"", reference.indicators = FALSE)
cfa.output &lt;- sem(model, cov.mat, N = 900, maxiter = 80000, optimizer = optimizerOptim)

Warning message:
In eval(expr, envir, enclos) : Negative parameter variances.
Model may be underidentified.
</code></pre>

<p>Straight off you might notice a few anomalies, let me explain.</p>

<ul>
<li>Why is the optimizer chosen to be <code>optimizerOptim</code>? </li>
</ul>

<p>ANS: I originally stuck with the default <code>optimizerSem</code> but no matter how many iterations I run, either I run out of memory first (8GB RAM setup) or it would report <code>no convergence</code> Things ""seemed"" a little better when I switched to <code>optimizerOptim</code> where by it would conclude successfully but throws up the error that the model is underidentified. Upon closer inspection, I realise that the output shows <code>convergence</code> as <code>TRUE</code> but <code>iterations</code> is <code>NA</code> so I am not sure what is exactly happening.</p>

<ul>
<li>The <code>maxiter</code> is too high.</li>
</ul>

<p>ANS: If I set it to a lower value, it refuses to converge, although as mentioned above, I doubt real convergence actually occurred.</p>

<h1>Problem</h1>

<p>So by now I guess that the model is really underidentified so I looked for resources to resolve this problem and found:</p>

<ul>
<li><a href=""http://davidakenny.net/cm/identify_formal.htm"" rel=""nofollow"">http://davidakenny.net/cm/identify_formal.htm</a></li>
<li><a href=""http://faculty.ucr.edu/~hanneman/soc203b/lectures/identify.html"" rel=""nofollow"">http://faculty.ucr.edu/~hanneman/soc203b/lectures/identify.html</a></li>
</ul>

<p>I followed the 2nd link quite closely and applied the t-rule:</p>

<ul>
<li>I have 68 observed variables, providing me with 68 variances and 2278 covariances between variables = <strong>2346 data points</strong>.</li>
<li>I also have 68 regression coefficients, 68 error variances of variables, 11 factor variances and 55 factor covariances to estimate making it a total of 191 parameters.</li>
<li>Since I will be fixing the variances of the 11 latent factors to 1 for scaling, I would remove them from the parameters to estimate making it a total of <strong>180 parameters to estimate</strong>.
<ul>
<li>My degrees of freedom is therefore 2346 - 180 = 2166, making it an over identified model by the t-rule.</li>
</ul></li>
</ul>

<h1>Questions</h1>

<ol>
<li>Is the low variance of some of my items a possible cause for the underidentification? I asked a previous question on items with zero variance which led me to think about items which are very close to zero. Should they be removed too? <a href=""http://stats.stackexchange.com/questions/49359/confirmatory-factor-analysis-using-sem-what-do-we-do-with-items-with-zero-varia"">Confirmatory factor analysis using SEM: What do we do with items with zero variance?</a></li>
<li>After reading much, I surmise that the underidentification might be a case of empirical underidentification. Is there a systematic way of diagnosing what kind of underidentification it is? And what are my options to proceed with my analysis?</li>
</ol>

<p>I have more questions but let's take it at these 2 for now. Thanks for any help!</p>
"
"0.027972711943223","0.0284901441149095"," 49714","<p>I have recently begun to read about bayesian statistics and I am playing around with the R2WinBUGS package. I'm trying to fit a logistic regression to the spam data (that can be found on the webpage of the elements of statistical learning) using R and WinBUGS. My approach was to first divide the data into 80% training and 20% testing sets. I can fit the model using the 80% set but I dont know how to write WinBugs code to predict on new observations (say the 20% test set) and I wonder if this approach to study model/classification precisicion make sense in a Bayesian Approach?</p>
"
"NaN","NaN"," 49835","<p>The R plotting package ggplot2 has an awesome function called <a href=""http://docs.ggplot2.org/0.9.2.1/stat_smooth.html"">stat_smooth</a> for plotting a regression line (or curve) with the associated confidence band.</p>

<p>However I am having a hard time figuring out exactly how this confidence band is generated, for every time of regression line (or ""method""). How can I find this information?</p>
"
"0.140248859508703","0.137552660599408"," 50086","<p>Assume for example a trivariate Gaussian model:
$$
{\boldsymbol Y}_1, \ldots, {\boldsymbol Y}_n \sim_{\text{iid}} {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right) \quad (*)
$$
with ${\boldsymbol \mu} = (\mu_1,\mu_2,\mu_3)'$. </p>

<p>The Bayesian conjugate theory of this model is well known. 
This model is the most simple case of a  multivariate linear regression model. 
And more generally, there is a well known Bayesian conjugate theory of multivariate linear regression, which is  the extension to the case when  the multivariate mean  ${\boldsymbol \mu}= {\boldsymbol \mu}(x_i)$ is allowed to depend on the covariates $x_i$ of individual $i \in \{1, \ldots, n \}$, with linear constraints about the multivariate means ${\boldsymbol \mu}(x_i)$. See for instance <a href=""http://books.google.be/books?id=GL8VS9i_B2AC&amp;dq=bayesian%20econometrics%20bayesm&amp;hl=fr&amp;source=gbs_navlinks_s"" rel=""nofollow"">Rossi &amp; al's book</a> accompanied by the crantastic <a href=""http://cran.r-project.org/web/packages/bayesm/index.html"" rel=""nofollow""><code>bayesm</code></a> package for <code>R</code>. 
We know in addition that the Jeffreys prior is a limit form of the conjugate prior  distributions.</p>

<p>Let us come back to the simple multivariate Gaussian model $(*)$. Instead of generalizing this model to the case of linearly dependent multivariate means ${\boldsymbol \mu}(x_i)$ depending on individuals $i=1,\ldots,n$, we can consider a more restrictive model by assuming linear constraints about the components $\mu_1$, $\mu_2$, $\mu_3$ of the multivariate mean ${\boldsymbol \mu}$. </p>

<h3>Example</h3>

<p>Consider some concentrations $x_{i,t}$ of $4$ blood samples $i=1,2,3,4$ measured at $3$ timepoints $t=t_1,t_2,t_3$. 
Assume that the samples are independent and that the series of the three measurements 
$(x_{i,t_1},x_{i,t_2},x_{i,t_3}) \sim {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right)$ for each sample $i$ with a mean 
${\boldsymbol \mu} = (\mu_1,\mu_2,\mu_3)'$ whose three components are 
linearly related to the timepoints: $\mu_j = \alpha + \beta t_j$.</p>

<p>This example falls into the context of <em>Multivariate linear regression with a within-design structure</em>. 
See for instance <a href=""http://wweb.uta.edu/management/Dr.Casper/Fall10/BSAD6314/Coursematerial/O%27Brien%20&amp;%20Kaiser%201985%20-%20MANOVA%20-%20RM%20-%20Psy%20Bull%2085.pdf"" rel=""nofollow"">O'Brien &amp; Kaiser 1985</a> and <a href=""http://www.stat.ualberta.ca/~wiens/stat575/misc%20resources/Appendix-Multivariate-Linear-Models.pdf"" rel=""nofollow"">Fox &amp; Weisberg's appendix</a></p>

<p>So my example is a simple example of this situation because there are only some predictors (the timepoints) for the components of the mean, but there are no predictors for individuals. This example could be written as follows:
$$
(**) \left\{\begin{matrix} 
{\boldsymbol Y}_1, \ldots, {\boldsymbol Y}_n \sim_{\text{iid}} {\cal N}_3\left({\boldsymbol \mu}, \Sigma\right) \\ 
{\boldsymbol \mu} = X {\boldsymbol \beta}
\end{matrix}\right.
$$
with ${\boldsymbol \beta}=(\alpha, \beta)'$ and $X=\begin{pmatrix} 1 &amp; t_1 \\ 1 &amp; t_2 \\ 1 &amp; t_3 \end{pmatrix}$ is the matrix of covariates for the components of the multivariate mean ${\boldsymbol \mu}$. The second line of $(**)$ could be termed as the <em>within design</em>, or the <em>repeated measures design</em>, or the <em>structural design</em> (I would appreciate if a specialist had some comments about this vocabulary).</p>

<p>I think such a model can be fitted as a generalized least-squares model, as follows in  <code>R</code> :</p>

<pre><code>gls(response ~ ""between covariates"" , data=dat, 
  correlation=corSymm(form=  ~ ""within covariates"" | individual ))
</code></pre>

<p>(after stacking the data in long format).</p>

<p><strong>My first question</strong> is Bayesian: what about the Bayesian analysis of model $(**)$ and more generally the Bayesian analysis of the multivariate linear regression model with a structural design ? 
Is there a conjugate family ? What about the Jeffreys prior ? Is there an appropriate R package to perform this Bayesian analysis ?</p>

<p><strong>My second question</strong> is not Bayesian: I have recently discovered some possibilities of John Fox's great <code>car</code> package to analyse 
such models with ordinary least squares theory (the <code>Anova()</code> function with the <code>idesign</code> argument --- see <a href=""http://www.stat.ualberta.ca/~wiens/stat575/misc%20resources/Appendix-Multivariate-Linear-Models.pdf"" rel=""nofollow"">Fox &amp; Weisberg's appendix</a>). Perhaps I'm wrong, but I am under the impression that this package only allows to get the MANOVA table (sum of squares analysis) with an orthogonal matrix $X$, and I'd like to get (exact) confidence intervals about the within-design parameters for an arbitrary matrix $X$, as well as prediction intervals. Is there a way to do so with <code>R</code> using ordinary least squares ?  </p>
"
"0.0395593886064618","0.040291148201269"," 51006","<p><strong>Context</strong>: Hierarchical regression with some missing data.</p>

<p><strong>Question</strong>: How do I use full information maximum likelihood (FIML) estimation to address missing data in R?  Is there a package you would recommend, and what are typical steps?  Online resources and examples would be very helpful too.</p>

<p><strong>P.S.</strong>: I'm a social scientist who recently started using R.  Multiple imputation is an option, but I really like how elegantly programs like Mplus handles missing data using FIML.  Unfortunately Mplus doesn't seem to compare models in the context of hierarchical regression at the moment (please let me know if you know a way to do that!).  I wondered whether there is anything similar in R?  Many thanks!</p>
"
"0.0740088392978143","0.0753778361444409"," 51152","<p>I've been trying to use the fastbw function from the rms package in R to perform logistic regression with backward selection, with p-values as exclusion criterion (I am well aware of the arguments against using p-values for this as opposed to e.g. AIC). However, the results are not in agreement with what I would get if I perform the backward selection manually, as fastbw often drops more factors in comparison. The results also seem to depend on the number of factors considered, even with the option </p>

<pre><code>type=""individual"".
</code></pre>

<p>I created some simple example data in order to prove my point, which give the following result:</p>

<pre><code>&gt; fastbw(lrm(y~x1+x2+x3+x4),rule=""p"",type=""individual"")

 Deleted Chi-Sq d.f. P      Residual d.f. P      AIC  
 x3      0.37   1    0.5412 0.37     1    0.5412 -1.63
 x1      1.82   1    0.1771 2.20     2    0.3336 -1.80
 x4      2.58   1    0.1082 4.78     3    0.1889 -1.22
 x2      3.56   1    0.0591 8.34     4    0.0799  0.34

[...]

Factors in Final Model

None
</code></pre>

<p>I.e., x2 is dropped as the last of the factors considered, resulting in a model without factors. However, if I consider x2 only, I get the following result. </p>

<pre><code>&gt; fastbw(lrm(y~x2),rule=""p"",type=""individual"")

No Factors Deleted

Factors in Final Model

[1] x2
</code></pre>

<p>The same is true if I do the backward selection manually, as x2 considered separately has a p-value of 0.045. What might cause this behavior? Since x2 is the last remaining variable in the backward selection, the results shouldn't depend on associations with other model covariates.</p>
"
"NaN","NaN"," 51176","<p>Do machine learning algorithms like Boosted Regression Trees (in the R package (gbm)) follow the same statistical assumptions of not including correlated predictor variables in GLM? </p>

<p>i.e. If I have two correlated predictrs (rsq=.7) should I be including both into my BRT model? </p>

<p>Any input or thoughts on this question would be greatly appreciated. </p>

<p>Cheers, </p>
"
"0.0395593886064618","0.040291148201269"," 51417","<p>Today I tried to estimate models using both <code>plm</code> and <code>pgmm</code> functions in the <a href=""http://cran.r-project.org/web/packages/plm/index.html"" rel=""nofollow"">plm</a> R package, with an interaction between <code>X1</code> and <code>lag(X2, 1)</code>. And I notice two issues.</p>

<p>Let $Y=b_1  X_1 + b_2  X_2 + b_3  X_1  X_2 + e$ be our model.</p>

<ol>
<li><p>When using <code>plm</code>, I got different results when I coded the interaction term with <code>I(X1 * lag(X2, 1))</code> and when I just saved this multiplication <code>X1 * lag(X2, 1)</code> in a different variable of the dataset and then used it in the regression.</p></li>
<li><p>With <code>pgmm</code> it is not even possible to run a formula which contains <code>I(X1 * lag(X2, 1))</code>. How can I pass such interaction?</p></li>
</ol>
"
"0.0884574820723792","0.0720749970156447"," 51464","<p>Background: For a project, I am fitting a conditional logit model where I have 5 control cases for every realized case. To do that I use the <code>clogit()</code> function in the package <code>survival</code>. I wanted to graph interactions with the <code>effects</code> package by John Fox et al. It turns out that this package can't handle <code>clogit</code> objects (output of <code>clogit()</code>). </p>

<p>As I believed I remembered that conditional logit were a special case of GLM, I thought the clever/lazy way to get my interaction plots would be to refit the model using a fixed effects glm and then use <code>effect()</code>.
The documentation of <code>clogit</code> seemed to confirm my intuition:</p>

<blockquote>
  <p>It turns out that the logliklihood for a conditional logistic regresson model = loglik from a Cox model with a particular data structure. [...]
  When a well tested Cox model routine is available many packages use this â€˜trickâ€™ rather than writing a new software routine from scratch, and this is what the clogit routine does. </p>
  
  <p>In detail, a stratified Cox model with each case/control group assigned to its own stratum, time set to a constant, status of 1=case 0=control, and using the exact partial likelihood has the same likelihood formula as a conditional logistic regression. The clogit routine creates the necessary dummy variable of times (all 1) and the strata, then calls coxph.</p>
</blockquote>

<p>Based on this description, it seems that I should be able to reproduce the stratification achieved through <code>strata()</code> by using a random intercept for each case/control group with <code>1|group</code> in <code>lmer()</code>. However, when I try, the results of <code>clogit</code> and <code>lmer</code> differ. One thing is that I probably have the wrong likelihood function. I don't really know how to specify this in <code>lmer</code> but more important, I am wondering what else I am missing. </p>

<p>I wonder whether I am completely wrong or somewhat on the right track but missing some pieces? What I would like is to understand what are the difference in terms of how the model is fitted between a conditional logit and a regular one (I understand that might be quite a long answer, so a book reference would be a great start). The my usual references for regression (Gelman and Hill, 2007; Mills 2011) are somewhat silent on the subject.</p>
"
"0.0395593886064618","0.040291148201269"," 52155","<p>Here's my context for this question: From what I can tell, we cannot run an ordinary least squares regression in R when using weighted data and the <code>survey</code> package. Here, we have to use <code>svyglm()</code>, which instead runs a generalized linear model (which may be the same thing? I am fuzzy here in terms of what is different).</p>

<p>In OLS and through the <code>lm()</code> function, it calculates an R-squared value, the interpretation of which I do understand. However, <code>svyglm()</code> does not seem to calculate this and instead gives me a Deviance, which my brief trip around the internet tells me is a goodness-of-fit measure that is interpreted differently than an R-squared.</p>

<p>So I guess I essentially have two questions on which I was hoping to get some direction:</p>

<ol>
<li>Why can we not run OLS in the <code>survey</code> package, while it seems that this is possible to do with weighted data in Stata?</li>
<li>What is the difference in interpretation between the deviance of a generalized linear model and an r-squared value?</li>
</ol>

<p>Thanks.</p>
"
"0.139863559716115","0.142450720574547"," 52252","<p>I have a regression model that looks like this: $$Y = \beta_0+\beta_1X_1 + \beta_2X_2 + \beta_3X_3 +\beta_{12}X_1X_2+\beta_{13}X_1X_3+\beta_{123}X_1X_2X_3$$</p>

<p>...or in R notation: <code>y ~ x1 + x2 + x3 + x1:x2 + x1:x3 + x1:x2:x3</code></p>

<p>Let's say $X_1$ and $X_2$ are categorical variables and $X_3$ is numeric. The complication is that $X_1$ has three levels $X_{1a}, X_{1b}, X_{1c}$ and instead of standard contrasts, I need to test: </p>

<ul>
<li>Whether the intercept for level $X_{1a}$ significantly differs from the average intercept for levels $X_{1b}$ and $X_{1c}$.</li>
<li>Whether the response of $X_2$ is significantly different between level $X_{1a}$ and the average of levels $X_{1b}$ and $X_{1c}$.</li>
<li>Whether the slope of $X_3$ is significantly different between level $X_{1a}$ and the average of levels $X_{1b}$ and $X_{1c}$.</li>
</ul>

<p>Based on <a href=""http://stats.stackexchange.com/questions/32188/how-to-interpret-these-custom-contrasts"">this post</a> it seems like the matrix I want is...</p>

<pre><code> 2
-1
-1
</code></pre>

<p>So I do <code>contrasts(mydata$x1)&lt;-t(ginv(cbind(2,-1,-1)))</code>. The estimate of $\beta_1$ changes, but so do the others. I can reproduce the new estimate of $beta_1$ by subtracting the predicted values of the $X_1b$ and $X_1c$ group means (when $X_3=0$ and $X_2$ is at its reference level) from twice the value of $X_1a$ at those levels. But I can't trust that I specified my contrast matrix correctly unless I can also similarly derive the other coefficients.</p>

<p>Does anybody have any advice for how to wrap my head around the relationship between cell means and contrasts? Thanks. Is there a standard name for this type of contrast?</p>

<hr>

<p>Aha! According to the <a href=""http://www.ats.ucla.edu/stat/r/library/contrast_coding.htm"">link posted in Glen_b's answer</a>, the bottom line is, you can convert ANY comparison of group means you want into an R-style contrast attribute as follows:</p>

<ol>
<li>Make a square matrix. The rows represent the levels of your factor and the columns represent contrasts. Except the first one, which tells the model what the intercept should represent.</li>
<li>If you want your intercept to be the grand mean, fill the first column with all of the same non-zero value, doesn't matter what. If you want the intercept to be one of the level means, put a number in that row and fill the rest with zeros. If you want the intercept to be a mean of several levels, put numbers in those rows and zeros in the rest. If you want it to be a weighted mean, use different numbers, otherwise use the same number. <em>You can even put in negative values in the intercept column and that probably means something too, but it completely changes the other contrasts, so I have no idea what that's for</em></li>
<li>Fill in the rest of the columns with positive and negative values indicating what levels you want compared to what others. I forget why summing to zero is important, but adjust the values so that the columns do sum to zero.</li>
<li>Transpose the matrix using the <code>t()</code> function.</li>
<li>Use <code>ginv()</code> from the <code>MASS</code> package or <code>solve()</code> to get the inverse of the transposed matrix.</li>
<li>Drop the first column, e.g. <code>mycontrast&lt;-mycontrast[,-1]</code>. You now have a p x p-1 matrix, but the information you put in for your intercept was encoded in the matrix as a whole during step 5.</li>
<li>If you want labels in the summary output more pleasant to read than <code>lm()</code> et al.'s default output, name your matrix's columns accordingly. The intercept will always automatically get named <code>(Intercept)</code> however.</li>
<li>Make your matrix the new contrast for the factor in question, e.g. <code>contrasts(mydata$myfactor)&lt;-mymatrix</code></li>
<li>Run <code>lm()</code> (and probably many other functions that use formulas) as normal in standard R without having to load <code>glht</code>, <code>doBy</code>, or <code>contrasts</code>.</li>
</ol>

<p>Glen_b, thank you, and thank you UCLA Statistical Consulting Group. My applied stats prof spent several days handwaving on this topic, and I was still left with no idea how to actually write my own contrast matrix. And now, an hour of reading and playing with R, and I finally think I get it. Guess I should have applied to UCLA instead. Or University of StackExchange.</p>
"
"NaN","NaN"," 52783","<p>I am trying to replicate a Weibull model for political leaders' survival in R. The model uses the Weibull distribution with the hazard rate <code>h(t)=lambda*pt^(p-1)</code>.</p>

<p>The term p is dependent on a variable W. So in <code>Stata</code> it can be done simply with adding <code>anc(W)</code> to the regression. </p>

<p>But the <code>weibreg</code> in the <a href=""http://cran.r-project.org/web/packages/eha/index.html"" rel=""nofollow""><code>eha</code></a> package only allows one to set values to ""shape"". Is there anyway to do this with <code>weibreg</code>? Is there another package/function I should be considering using for this purpose?</p>
"
"0.0395593886064618","0.040291148201269"," 54668","<p>I want to know if it is possible for a library in R to evaluate the association of independent variables and create a formula? I am trying to come up with a model to predict power consumption of a machine, using some hardware counters and performance attributes. When I use linear regression, I have no problem since I could represent my formula like <code>power~lm(a1+a2+a3+a4)</code>, but for the non-linear case, I am not sure what would be the formula or which model should I choose. 
I would want to have a way to do this:</p>

<pre><code>power ~ &lt;some-non-linear-reg-pkg&gt;(a1+a2+a3+non-linear(a4))
</code></pre>

<p>I reviewed some packages for non-linear regression such as <code>nls</code> and <code>gnm</code>, and they expect a formula to be provided by the user. I am however able to identify which variables have linear associations and which are non-linear (by performing correlation tests), the problem is building a formula out of them. </p>
"
"NaN","NaN"," 54824","<p>Curious whether anyone knows a package, or has written an implementation themselves, for conducting instrumental variables regressions using LIML in R. All of the R packages I have seen for IV regressions seem to resort to 2SLS/GMM as opposed to LIML, which may have more desirable finite sample qualities compared to 2SLS <a href=""http://press.princeton.edu/titles/6946.html"" rel=""nofollow"">(Hayashi 2000)</a></p>

<p>For additional context, stata's ivregress command includes options to use LIML estimation, and hoping someone has already implemented something similar in R so I don't have to write it myself.</p>
"
"0.0685188709827532","0.0697863157798853"," 54962","<p>I want to learn the statistics package <a href=""http://gretl.sourceforge.net/gretl_and_R.html"" rel=""nofollow"">gretl</a>.</p>

<p>My first attempt to do so is to calculate a linear regression model of a set of data:</p>

<p>$$y_i = \alpha + \beta x_i + u_i$$</p>

<p>First I want to create a crossplot of the data and then calculate the variance of the residuals $s^2_u$ and also those of $\alpha$ and $\beta$.</p>

<p>My questions are:</p>

<p>How to get the crossplot, I do not see any option in the menu which says crossplot? Is this possible in gretl?</p>

<p>When I click Summary statistics I think I get the $s^2$ of the data, however is it also possible to get the the variance of the residuals $s^2_u$ and also those of $\alpha$ and $\beta$?</p>

<p>Is it also possible to calculate in <code>gretl</code> a model without $\alpha$, such as:
$$y_i = \beta x_i + u_i$$</p>

<p>I appreciate your answers!</p>

<p><strong>UPDATE</strong></p>

<p>My gretl window:</p>

<p><img src=""http://i.stack.imgur.com/OSRkv.png"" alt=""enter image description here""></p>
"
"0.027972711943223","0.0284901441149095"," 55240","<p>I'm working on a data set modeling road kills (0 = random point, 1 = road kill) as a function of a number of habitat variables.  Following Hosmer and Lemeshow, I've examined each continuous predictor variable for linearity, and a couple appear nonlinear.  I'd like to try a fractional polynomial transformation for each, also following Hosmer and Lemeshow, and have looked at the R package mfp, but I'm having trouble coming up with (and understanding) the R code that will correctly transform the variable.  Can anyone suggest R code that would help me accomplish the concepts on p. 101 - 102 of Hosmer and Lemeshow's Applied Logistic Regression (2000).  Thanks!</p>
"
"0.0930988128231456","0.102722675451665"," 55393","<p>I have a PDF (Probability Density Function) generated from a vector of 1,000,000 empirical values. This empirical PDF is heavily skewed to the right.</p>

<p>In this form, I can't make accurate predictions using a linear regression.</p>

<p>To fix this, is there some method to find the function F(x) to transform (i.e. ""squash"") the values in the vector into a standard normal distribution, so I can feed said transformed vector into a linear regression?</p>

<p>Of course, this would also involve finding the inverse of F(x) that transforms (i.e. ""de-squashes"") any predictions back into the original empirical PDF.</p>

<p><strong>What I have tried</strong></p>

<p>So far, I have managed to generate the density function from the empirical data:</p>

<p><img src=""http://i.stack.imgur.com/HIBUP.png"" alt=""enter image description here""></p>

<p>Here is the R code:</p>

<pre><code>par(mfrow=c(2,1))

install.packages(""bootstrap"")
library(bootstrap)
data(stamp)
nobs &lt;- dim(stamp)[1]
hist(stamp$Thickness,col=""grey"",breaks=100,freq=F)
	dens &lt;- density(stamp$Thickness)
lines(dens,col=""blue"",lwd=3)

plot(density(stamp$Thickness),col=""black"",lwd=3, main=""Simulation to choose density plot"")
	for(i in 1:10)
	{
		newThick &lt;- rnorm(nobs,mean=stamp$Thickness,sd=dens$bw*1.5)
		lines(density(newThick,bw=dens$bw),col=""grey"",lwd=3)
}

# If I wanted to do a linear regression to predict stamp thickness,
# what is the function F(x) to ""squash"" (i.e. transform) the ""stamp""
# vector into a normal distribution, and the corresponding inverse 
# function Finv(x) to ""desquash"" (i.e. untransform) any predictions back 
# into the original prediction?
</code></pre>

<p><strong>Update 1</strong></p>

<p>@Andre Silva sugggested that:</p>

<blockquote>
  <p>What need to have normal distribution are the residuals (predicted
  versus observed) derived from your (multiple) linear regression model.</p>
</blockquote>

<p>According to <a href=""http://www.stat.yale.edu/Courses/1997-98/101/linmult.htm"" rel=""nofollow"">post on Multiple Linear Regression</a>:</p>

<blockquote>
  <p>After fitting the regression line, it is important to investigate the
  residuals to determine whether or not they appear to fit the
  assumption of a normal distribution. A normal quantile plot of the
  standardized residuals y -  is shown to the left. Despite two large
  values which may be outliers in the data, the residuals do not seem to
  deviate from a random sample from a normal distribution in any
  systematic manner.</p>
</blockquote>

<p><img src=""http://i.stack.imgur.com/3ybm0.gif"" alt=""enter image description here""></p>

<p><strong>Update 2</strong></p>

<p>See <a href=""http://stats.stackexchange.com/questions/11351/left-skewed-vs-symmetric-distribution-observed/11352#11352"">Left skewed vs. symmetric distribution observed</a> for R code that illustrates that the only relevant concern is if the residuals are normally distributed.</p>
"
"0.0484501583111509","0.0328975847479884"," 55462","<p>I am using <code>KFAS</code> package for <code>R</code>.</p>

<p>You can run</p>

<pre><code>install.packages(""KFAS"")
library(KFAS)
?regSSM
</code></pre>

<p>to see how this package allows to build a state space representation of linear regression models and many others.</p>

<p>Now let we have the following state space system:</p>

<p>$S_{t}=\alpha+(1+k_{t})L_{t}+v_{t}$</p>

<p>$k_{t}=\phi k_{t-1}+(1-\phi)\bar{k}+w_{t}$</p>

<p>being $\bar{k}$ a constant, a.k.a. the unconditional mean of the unobservable AR(1) process.</p>

<p>Anyone can tell me how may I set this state space representation in <code>KFAS</code> through <code>regSSM</code> or any other <code>KFAS</code> package's function (like <code>arimaSSM</code>)?</p>
"
"0.0484501583111509","0.0328975847479884"," 56092","<p>I am very new to R. I am learning machine learning right now.
Very sorry, if this question appears to be very basic. 
I am trying to find a good feature selection package in R. 
I went through Boruta package. It is a good package but I read that it is only useful for classification.</p>

<p>I want to do implement feature selection in R for regression tasks. I went through the caret package documentation but for my level, it is very difficult to understand.</p>

<p>Can any one please point me to a good tutorial or list any good packages or most frequently used packages in R for feature selection.</p>

<p>Any help would be appreciated.
Thanks in advance.</p>
"
"0.0559454238864459","0.0427352161723642"," 56608","<p>I'm doing some clinical database research and in an effort to lessen the burden on our statistical staff, I started to look for different software solutions to get the analyses I need, and that's where BIDS (business intelligence development studio).  BIDS allows queries to be run against a SQL Server and store the results of those queries in a table or view.  That table or view is then consumed by BIDS and logistic and linear regressions as while as CART analyses can be done on them.<br>
BIDS Version</p>

<p>The x axis that is cut off on the lift chart is 'overall population %'
<img src=""http://i.stack.imgur.com/LbpXf.jpg"" alt=""enter image description here""> </p>

<p>A mining accuracy chart of the CART
<img src=""http://i.stack.imgur.com/Vm5Te.jpg"" alt=""enter image description here""></p>

<p>I'm not quite sure how this works in other stats packages, but in BIDS you define a certain percentage of your data set to train the model and the rest of the set is compared against that model and the lift chart shows the improvement in identifying the outcome you desire vs. a guess.  I'm only vaguely familiar with CART analyses in the first place, and don't know the first thing about R, but this same analysis was done in R with very similar results.  The red portion of what looks like a health meter in a video game corresponds nearly identical to the analysis done in R.  However, there are no p values in the BIDS version.  Consider the node Max Total Poly Pharm >=7 and &lt; 13.
BIDS shows me (not present in the picture)
value         cases    probability
not present   2133     91.89</p>

<p>Is there any way to ascertain a p value from that.  And does R use any of it's cases as training data for the model?</p>
"
"0.0484501583111509","0.0493463771219827"," 56871","<p>I have a dataset from a bank with demographic data and one variable telling if the customer is a good customer or not (binary variable). I would like to do prediction on if the customer is good or not based on this demographic data.</p>

<p>I managed to do it with a logistic regression, but would like now to compare the result (classification rate) with neural networks. </p>

<p>I found 2 functions from different packages doing that:
- nnet()
- neuralnet()
But those functions seem to be conceived for numerical dependent variables.</p>

<p>Thus my question: is there a possibility to use these functions for a categorical numerical variable (by estimating a posteriori probabilities for instance) or is there another function doing that?</p>

<p>Thanks a lot!</p>

<p>Robin</p>
"
"0.0559454238864459","0.056980288229819"," 57180","<p>I am using a binomial regression model for presence/absence, with 20 independent variables to test. The data has x and y coordinates and I would like to understand how can I take into account the spatial autocorrelation.</p>

<p>I already studied the correlation between the variables and run the same model for 1000 different samples (I have a big dataset that allows me to do this) to understand the distribution of each parameter and check for variables that might be introducing problems in my model. </p>

<p><code>glm_model &lt;- glm(PA ~ Var1 + Var2 + Var3 + Var4 + Var5,family=binomial(link=logit)</code>)</p>

<p>However I believe I also need to account for spatial autocorrelation. I saw that there is a package that might help me (<code>spdep</code>), however I am not sure I completely understand if I can use my model or not. My question is what are my options ?</p>
"
"0.0484501583111509","0.0493463771219827"," 57448","<p>I am running a model (logistic regression) with 20 independent variables in R. </p>

<p>Before running the model I calculated the correlation between all the variables and finally selected my variables by also checking ""visually"" the histograms of each variable in the case of presence and again in the case of absence. In situations where I don't see any obvious distribution associated to both presence &amp; absence, I discard the variable.</p>

<p>I would like to make ""official"" calculations for the level of relation between Presence/Absence and each variable (how much each variable contributes to the Presence/Absence), for example with <code>Cramer's V index</code>, but the available function I find is from the package <code>vcd</code> and has some limitations: 
doesn't give the <code>Cramer's V</code> (as well as the Phi-Coefficient Contingency Coeff.) for each independent variable, and it doesn't run for one independent variable.</p>

<p>I might be missing some other obvious way to do this. Any help is appreciated.</p>
"
"0.027972711943223","0.0284901441149095"," 57551","<p>Was wondering if anyone knows of an R package to estimate the Cauchy-M estimator of regression (see for example the end of this <a href=""http://www.brnt.eu/phd/node13.html#SECTION00712200000000000000"" rel=""nofollow"">section</a>, but with simultaneous estimation of the scale parameter as in section 2 of (1)). </p>

<blockquote>
  <p>(1) Mizera, I. MÃ¼ller, C. H. (2002).  Breakdown points of Cauchy
  regression-scale estimators, Statistics &amp; Probability Letters, Volume
  57, Issue 1, Pages 79-89.</p>
</blockquote>
"
"0.0740088392978143","0.0753778361444409"," 57811","<p>I'm trying to do LASSO in R with the package glmpath. However, I'm not sure if I am using the accompanying prediction function <em>predict.glmpath()</em> correctly. Suppose I fit some regularized binomial regression model like so:</p>

<pre><code>fit &lt;- glmpath(x = data$x, y=data$y, family=binomial)
</code></pre>

<p>Then I can use predict.glmpath() to estimate the value of the response variable $y$ at $x$ for varying values of $\lambda$ through</p>

<pre><code>pred &lt;- predict.glmpath(fit, newx = x, mode=""lambda"", s=seq(0,10,1),type=""response"")
</code></pre>

<p>However, in the help file it can be seen that there is also an option <em>newy</em>. How should one interpret the result when calling <em>predict.glmpath()</em> with <em>newy = some.y</em>? </p>

<p><strong>[Edit]</strong> An additional question came to mind:</p>

<p>The option <em>type</em> can have the following values, according to the help file:</p>

<pre><code>                      description in help file

""response""            the estimated responses are returned
""loglik""              the log-likelihoods are returned
""coefficients""        the coefficients are returned. The coefficients for the initial input variables are returned (rather than the standardized coefficients)
""link""(default)       the linear predictors are returned
</code></pre>

<p>However, to which linear predictors and coefficients are they referring to? Surely not those of the original model?</p>
"
"0.0484501583111509","0.0328975847479884"," 57826","<p>I have a dataset that includes variables about customer income levels.  The income was collected in binned fashion (<code>Which range describes your income? 0-25k, 25k-50k,...</code>).  My question is how best use this for modeling using <code>glmnet</code> and <code>gbm</code> packages in <code>R</code>.</p>

<p>I have looked at the <code>grouped</code> packaged in <code>R</code> but it seems to do everything (coursening and regression) for you. Is there a package that converts binned data back to continuous data for use in with other algos?</p>

<p>EDIT: The current method I'm using is to convert them to the mid-point of the range (<code>0-25k -&gt; 12500</code>), then using an <code>ifelse()</code> stmt to code a few variables to convey the fact that there is a relationship between the levels.</p>

<pre><code>incOver25k &lt;- ifelse(df1$income &gt;= 25000,1,0)
    incOver50k &lt;- ifelse(df1$income &gt;= 50000,1,0)
</code></pre>

<p>Then use these flags instead of using <code>model.matrix()</code>.  </p>

<p>Was curious if there were any better ideas.</p>
"
"0.027972711943223","0.0284901441149095"," 57912","<p>I'm trying to implement a regression model with both fixed and random effects. The package I use is the <code>lme4</code>.</p>

<p>I want to find the relationship between the continuous variables <code>X1</code> and <code>X2</code>, using the categorical variable <code>F1</code> as a fixed effect.</p>

<p>When I try to implement the model <code>model &lt;- lmer(X1 ~ F1 + (1|X2))</code>  I get the following error:</p>

<blockquote>
  <p><code>Number of levels of a grouping factor for the random effects must be less than the number of observations</code></p>
</blockquote>

<p>My understanding for the error is that the <code>X2</code> varies less than the <code>X1</code> (which has some similar values). Why is this a problem?</p>
"
"0.0791187772129236","0.080582296402538"," 58315","<p>I want to find the most important predictors for a binomial dependent variable out of a set of more than 43,000 independent variables (These form the columns of my input dataset). The number of observations is more than 45,000 (these form the rows of my input dataset). Most of the independent variables are unigrams, bigrams and trigrams of words, so there is high degree of collinearity among them. There is a lot of sparsity in my dataset as well. I am using the logistic regression from the glmnet package, which works for the kind of dataset I have. Here is some code:</p>

<pre><code>library('glmnet')
data &lt;- read.csv('datafile.csv', header=T)
mat = as.matrix(data)
X = mat[,1:ncol(mat)-1] 
y = mat[,ncol(mat)]
fit &lt;- cv.glmnet(X,y, family=""binomial"", type.measure = ""class"")
betacoeff = as.matrix(fit$glmnet.fit$beta[,ncol(fit$glmnet.fit$beta)])
</code></pre>

<p>betacoeff returns the betas for all the independent variables. I am thinking of showing the predictors corresponding to the top 50 betas as the most important predictors. 
My questions are:</p>

<ol>
<li><p>glmnet picks one good predictor out of a bunch of highly correlated good predictors. So I am not sure how much I can rely on the betas returned by the above model run.</p></li>
<li><p>Should I manually sample the data (say 10 times) and each time run the above model, get the list of predictors with the top betas and then find those which are present in all 10 repetitions? Is there any standard way of doing this? What is the standard way of sampling in this case?</p></li>
<li><p>My other question is about cvm (cross validation error) returned by the above model. Since I use type.measure = ""class"", cvm gives the misclassification error for different values of lambda. How do I report the misclassification error for the entire model? Is it the cvm corresponding to lambda.min?</p></li>
</ol>
"
"0.0839181358296689","0.0854704323447285"," 58448","<p>I'm stuck with a regression modeling problem. I have panel data where the dependent variable is a probability. Below is an excerpt from my data. The complete panel covers more countries and years, however it is unbalanced. What I can observe is the number of events and the number of trials. The event probability was derived from those values (estimation of this probability should be quite good, given the large number of trials). All independent variables are county-year specific.</p>

<pre><code>     country  year  event_prob  events trials    x    x_lag2 ... more variables
  1   Cyprus  2008  0.03902140  11342  290661   4.60   4.13  ...
  2   Cyprus  2009  0.04586650  13482  293940   4.60   4.48  ...
  3   Cyprus  2010  0.05188398  15206  293077   4.60   4.60  ...
  4   Cyprus  2011  0.06433411  18505  287639   5.79   4.60  ...
  5  Estonia  2008  0.07872978  21686  275449   6.02   4.11  ...
  6  Estonia  2009  0.09516270  33599  353069  13.18   4.91  ...
  7  Estonia  2010  0.08645905  36180  418464   7.95   6.03  ...
  8  Estonia  2011  0.07731997  31590  408562   5.53  13.18  ...
  ...
165  USA  2011  0.06100000  9192822  150702000   2.73  3.27  ...
</code></pre>

<p>My goal is to use regression analysis to find out which variables are significant for the event probability. In R-terminology, I'm looking for a model of the form <code>event_prob ~ x + x_lag2 + ...</code> .</p>

<p>The problem is as follows: <code>event_prob</code> has to be between 0 and 1, hence using <code>event_prob ~ x + x_lag2 + ...</code> might not be the best idea. So I was thinking of using the logit transform of <code>event_prob</code> such that <code>logit(event_prob)</code> ranges from $-\infty$ to $\infty$. The first idea was to use the R's <code>plm</code> package, i.e. <code>plm(logit(event_prob)~x+x_lag2,data,index=c(""country"",""year""),model=""random"")</code> or <code>model=""within""</code> (see below). Is that a reasonable approach or am I violating some essential assumptions?</p>

<p>I was also thinking of using panel generalized linear models from the package <code>pglm</code> (with the logit link function), however since I don't know the outcome of the binary events (only the total number of events and trials) is known, I got stuck there. Maybe someone can help me how to proceed here.</p>

<p>Since I have panel data, I'd like to compute both fixed-effects models and random-effects model and then apply the Hausman (1978) test to decide which model is more appropriate.</p>

<p>Do my first attempts at modeling make sense? I'm really not sure how to correctly address this problem. I hope the description of my problem is detailed enough. If not, I'm happy to provide more details</p>

<p>In terms of software, I'd prefer R. SAS and SPSS are also ok since my university has licences for them. I just don't have much experience with them.</p>
"
"0.027972711943223","0.0284901441149095"," 58504","<p>I am running a negative binomial regression of clinic counts in each county in the entire country (~3k counties).  I'd like to at least partially account for the non-independence of neighboring counties by bootstrapping the confidence intervals in a ""clustered"" fashion--e.g. draw an entire state's (50 states total) worth of data at once.  This has become <a href=""http://www.mitpressjournals.org/doi/abs/10.1162/rest.90.3.414#.Vg7SjnUVhBc"" rel=""nofollow"">standard practice</a>, for better or for worse, in the econometric literature.</p>

<p>I could write the code to do this myself, but the <code>boot</code> package seems like it should have the ability to do this somehow, and in general I prefer tested, general solutions to one-off hacks.  Is there a way to coerce the <code>boot</code> package to do a clustered bootstrap?</p>

<p>I tried the <code>strata</code> argument, but that randomizes <em>within</em> strata rather than randomizing which cluster gets taken, as the following code confirms:</p>

<pre><code>dat &lt;- data.frame( cluster=rep(letters[1:5],each=10), x=runif(5*10), stringsAsFactors=TRUE )
boot.stat &lt;- function(dat,idx) {
    print(dat[idx,]$cluster)
    	print(table(dat[idx,]$cluster))
    mean(dat[idx,]$x)
    }
    boot( 
    	data=dat, 
    	statistic=boot.stat, 
    	strata=dat$cluster, 
    stype=""i"", 
    R=5 
)
</code></pre>
"
"0.0634361479695551","0.0646095738380922"," 58538","<p>I'm looking for breakpoints in species abundance as a function of spatial distance, using the <code>segmented</code> package for R. 'segmented' appears to return a breakpoint no matter what; I don't understand whether it returns an estimate of the signifcance of the breakpoint (or whether the segmented linear model is better than an unsegmented model). </p>

<p>For instance (R code): </p>

<pre><code>require(segmented)
set.seed(1)
x &lt;- 1:100
y &lt;- rnorm(100) + x # No real breakpoint
y2 &lt;- c(50-x[1:50], x[51:100-50]) + rnorm(100) # Clear breakpoint at 50
plot(x,y)
points(x, y2)
</code></pre>

<p><img src=""http://i.stack.imgur.com/DRysM.png"" alt=""enter image description here""></p>

<pre><code># Segmented model for the unsegmented data
testM &lt;- lm(y ~ x)
testMs &lt;- segmented(testM, seg.Z = ~x, psi=90)
summary(testMs)
</code></pre>

<p>Despite the fact that there is clearly no breakpoint in the data, <code>summary</code> reports that <code>t value for the gap-variable(s) V:  0</code>, and the standard error is fairly small (3.28).</p>

<pre><code>testM2 &lt;- lm(y2 ~ x)
testM2s &lt;- segmented(testM2, seg.Z=~x, psi=50)
summary(testM2s)
</code></pre>

<p>Here the guess is correct (49.78), the standard error is even smaller (0.2), and the t value is the same. How do I interpret this result? </p>

<p><strong>Note:</strong> I have no particular attachment to the <code>segmented</code> package - I just want to test the hypothesis that a segmented regression models my data better than a single-domain regression. But the other breakpoint-analysis packages I've looked at seem to require that points on the domain be evenly-spaced (e.g. timeseries) and there is a single dependent value per independent value. These assumptions are not met for my spatial data.</p>
"
"0.0395593886064618","0.040291148201269"," 58772","<p>In testing the parallel regression assumption in ordinal logistic regression I find there are several approaches. I've used both the graphical approach (as detailed in HarrellÂ´s book) and the approach detailed using the  <a href=""http://cran.r-project.org/web/packages/ordinal/vignettes/clm_tutorial.pdf"" rel=""nofollow"">ordinal package</a> in R. </p>

<p>However I would also like to run the Brant test (from Stata) for both the individual variables and also for the total model. I've looked around but cannot find it implemented in R. </p>

<p><strong>Is there an implementation of the Brant test in R?</strong></p>
"
"0.0559454238864459","0.056980288229819"," 59311","<p>I'm working on a behavoural scorecard modelling exercise, and many of the decisions taken to date have been based on the experience of a consulting credit analyst (whose experience software-wise is SAS) as I am primarily in BI. So far I have:</p>

<ul>
<li>a linux pc with 32gb of ram and an i7 processor</li>
<li>an observation window  </li>
<li>~90 potential characteristics </li>
<li>a binary outcome</li>
</ul>

<p>In <strong>R</strong>, I have</p>

<ol>
<li>loaded the dataset (225k obs of 88 vars, 1 outcome)</li>
<li>split the dataset up based on the recommendations/examples in the package <strong>caret</strong> i.e. predictors and outcomes split up (150k obs in training sample)</li>
<li>removed any variables showing a high degree of correlation (caret::findCorrelation)</li>
<li>cut all continuous variables into categorical intervals </li>
<li>reduced the number of variables based on near zero variance, missing values, and low information value (IV) (150k obs of 48 vars)</li>
<li>tried bestglm::bestglm, caret::train (with glm and glmnet), FWDselect::selection, FWDselect::qselection but eventually had to interrupt each of these due to not completing after 4 hours of 100% CPU usage</li>
<li>used FactoMineR:MCA to perform a multiple correspondence analysis (on predictors only)</li>
</ol>

<p>What I would like to do is have a selection of logistic regression models for say 4, 8, 12, and 16 variables that are the most predictive models at each point.  I'm not sure if I'm going in the correct direction here with MCA as I've mainly been simply trying to find something that works in a timely fashion for reducing my variables further or going directly to variable selection steps.</p>

<p>I would appreciate any advice on how to do any of step 6 better, whether 7 makes sense and what step 8 should be. </p>

<p>Thanks,</p>

<p>Steph</p>

<p>PS Design decisions up to 6&amp;7 can't be revised so please, no telling me off for them! </p>
"
"0.0884574820723792","0.0900937462695559"," 59530","<p>With a colleague, we are working on a dataset containing ~5000 continuous variables for 120 individuals belonging to 8 classes.</p>

<p>We want to <strong>estimate the relative importance of each variable</strong> to explain the classes.
We have used a random forest approach with some success.
Now, we could like to go deeper by considering the fact that <strong>the 8 classes we fit are unequally distant from each other</strong>.
In fact, in our case <strong>we can <em>a priori</em> generate a distance matrix (<em>i.e.</em> cost matrix) for all possible pairs of classes</strong>.</p>

<p>My (very limited) understanding of random forest is that, for regression problems,
the error $E$ is computed by the mean square difference between the OOB sample and the prediction for the same sample:</p>

<p>$E = n^{-1}\sum\limits_{i=1}^n{{(y_i-\hat{y}_i)}^2}$</p>

<p>Where $y_i$ is the predicted value and $\hat{y}_i$ the real value of an out-of bag-sample $i$.</p>

<p>Ultimately, the calculation of the variable importance depends on how the error is computed (right?).</p>

<p>In our case, I would like to use a modified loss function, for instance:</p>

<p>$E = n^{-1}\sum\limits_{i=1}^n{M_{y_i,\hat{y}_i}}$</p>

<p>Where $M$ is predefined a distance matrix; so $M_{a,b}$ is the distance between class $a$ and $b$.
In this way the misclassification error would be more important if $y_i$ and $\hat{y}_i$ represent distant classes and, ultimately, <strong>the variable importance should be more relevant</strong>.</p>

<p>My questions are:</p>

<ol>
<li>Does this approach make sense to you, or am I missing something?</li>
<li>Can you think of any study that has used something similar.</li>
<li>We have so far used the <code>randomForest</code> package in R. It does not seem possible to use it in combination with an a priori distance matrix between classes. Do you know if this is already implemented somewhere?</li>
</ol>

<p><strong>EDIT</strong></p>

<p>I believe this is a very frequent problem in my field, biology, because we deal with classes for which relations can be represented and quantified by trees (dendrograms), often because of there lineage.</p>

<p>After some research, it appears that my question is about using a <strong>cost-sensitive</strong> version of random forest. In this respect, it is very similar to <a href=""http://stats.stackexchange.com/questions/46963/how-to-control-the-cost-of-misclassification-in-random-forests"">this question</a>.
I specificity want to use a <strong>cost matrix</strong> rather than a cost vector though.
It there any ontological reason why it is not possible or is it simply not implemented?</p>
"
"NaN","NaN"," 59691","<p>I have a question about the <code>rugarch</code> package. </p>

<p>My sample size is 43 and I have a problem to model a garch whose mean equation includes an exogenous model; otherwise my mean equation is linear regression, that is, $y_i=a+bx_i+e_i$ that $i=1, \ldots, 43$ and $e_i$ follow a $\text{garch}(1,1)$. But when I run it simultaneously R says <code>you need at least 100 points</code>. I don't know what to do.</p>
"
"0.0559454238864459","0.056980288229819"," 59741","<p>say I have a sensor that measures temperature, pressure ++, and want to use this data to predict some quantity ""A"". If I use multivariate regression, I can simply implement a model of the form A=a0+a1x1+a2x2+..., and whenever I have new measurements I can use the model to make predictions.</p>

<p>If I on the other hand make a predictive model using random forests, I'm not really sure how to use it. I've used the caret package to split my data into training and test sets, and do automatic feature selection using random forest and cross-validation. I get good predictions on the test set, but have no idea how to implement these trees to use in say a digital signal processor. In R I just use the predict() function, but this is obviously not available outside of R.</p>

<p>This is probably a stupid questing, but it's the best I can do.</p>

<p>Any suggestions are welcome.</p>
"
"0.027972711943223","0"," 59990","<p>I have tried the <code>clm</code> and <code>clmm</code> functions in the <code>ordinal</code> package and the <code>lrm</code> function in the <code>rms</code> package, but none of them has a good performance. The best accuracy rate is around 70%.</p>

<p>Is there any other package implementing ordinal regression in R? Or I should consider replacing the features in the model in order to improve the accuracy?</p>
"
"0.0419590679148345","0.056980288229819"," 60087","<p>Is it viable to do several binary logistic regressions instead of doing a multinomial regression? From this question: <a href=""http://stats.stackexchange.com/questions/52104/multinomial-logistic-regression-vs-binary-logistic-regression"">Multinomial logistic regression vs binary logistic regression</a> I see that the multinomial regression might have lower standard errors. </p>

<p>However, the package I would like to utilize has not been generalized to multinomial regression (<code>ncvreg</code>: <a href=""http://cran.r-project.org/web/packages/ncvreg/ncvreg.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/ncvreg/ncvreg.pdf</a>) and so I was wondering if I could simply do several binary logistic regressions instead.</p>
"
"0.0884574820723792","0.0900937462695559"," 60274","<p>I have a dataset that I'm trying to classify into 2 groups, A and B, using a random forest model. I know the true grouping and I'm trying to see how well I can model it using the other available variables. I've tried 2 different approaches that I thought would be equivalent, but which are actually giving me quite different results:</p>

<ol>
<li>Reading in the grouping as a (non-numeric) factor in R, growing a classification forest, and taking the proportion of trees that vote for group A as my prediction.</li>
<li>Constructing an indicator variable for membership of group A, growing a regression forest, and taking the ensemble prediction as usual.</li>
</ol>

<p>The split between the 2 groups is roughly 90-10 A vs. B. I'm growing 240 trees from ~200k observations of the same variables. I've left most of the settings at the defaults for the R randomForest package, but to keep the processing time down to a manageable level I've increased the node size to 200. The results are as follows:</p>

<ol>
<li>In the vast majority of cases, all 240 trees vote for A. The average predicted chance of any one observation being in A is about 99.9%. Worse still, not a single member of group B gets a majority of votes for group B!</li>
<li>I get a wide range of predictions, with the mean prediction lying close to the observed mean of ~90%.</li>
</ol>

<p>How can two apparently similar methods give such different results?</p>

<p>As for how I ended up trying this - I was initially trying to classify my dataset into a larger number of groups, of which B was one, but I noticed that B was being classified almost 100% incorrectly. The other groups are all much better behaved, even though most of them make up a far smaller proportion of my data.</p>
"
"0.0395593886064618","0.040291148201269"," 60434","<p>I am using <a href=""http://lavaan.ugent.be/"" rel=""nofollow"">R lavaan package</a> to estimate a structural equation model. Let's say the model consists of 1 endogenous manifest variable with 1 latent and 2 manifest explanatory variables: </p>

<pre><code>group = {0,1}
attitude1 = latent,scale
age = respondent's age
</code></pre>

<p>The desired lavaan model is then (doesn't work): </p>

<pre><code>model &lt;- '
attitude1 =~ att1 + att2 + att3
outcome ~ age*group + attitude1*group'
</code></pre>

<p>My goal is, in the lines of what can be done in linear regression, to <strong>establish main and interaction effects between each variable and group. Can this be done?</strong> </p>
"
"0.08882529023711","0.0986927542439653"," 60648","<p>I am trying to forecast electricity consumption in GWh for 2 years ahead (from June 2013 ahead), using R (the forecast package). For that purpose, I tried regression with ARIMA errors. I fitted the model using the <code>auto.arima</code> function, and I used the following variables in the <code>xreg</code> argument in the <code>forecast.Arima</code> function: </p>

<p>- Heating and Cooling Degree Days,<br>
- Dummies for all 12 months and<br>
- Moving holidays dummies (Easter and Ramadan)  </p>

<p>I have several questions regarding the model:</p>

<p>1) Is it correct to use all 12 dummies for monthly seasonality, since when I tried to include 11, the function returned error. The <code>Auto.arima</code> function returned the model ARIMA(0,1,2)</p>

<p>2)The model returned the following coefficients (I won't specify all of them as there are too many coefficients):</p>

<pre><code>ma1      ma2     HDD     CDD   January  February  March     April
-0.52 -0.16      0.27    0.12  525.84   475.13    472.57    399.01
</code></pre>

<p>I am trying to determine the influence of the temperature component over electricity load. In percentages, (interpreting the coefficients just as with the usual regression) the temperature components (<code>HDD</code>+<code>CDD</code>) account for 11,3% of the electricity consumption. Isn't this too little, considering the fact that the electricity consumption is mostly influenced by the weather component? On the other hand, taking look at the dummies' coefficients, it turns out that the seasonality accounts for the greater part of the load. Why is this? Is the model completely incorrect?</p>

<p>I tried linear regression, and the temperature component accounts for 20%, but it is still a low percentage. Why is this?</p>

<p>3) I am obviously making some mistakes in the use of <code>forecast.Arima</code> or the plot function parameters since when I plot the forecasts, I get a picture of the original time series which is continued (merged) with the forecasts for the whole time series period (from 2004 until 2015). I don't know how to explain this better, I tried to paste the picture, but it seems I cannot paste pictures here.</p>
"
"0.108550066801774","0.103648123654272"," 61138","<p>I am trying to calculate the marginal effects of a multinomial logistic regression. To do this I use the <code>mlogit</code> package and the <code>effects()</code> function.</p>

<p>Here is how the procedure works (source : <code>effects()</code> function of <code>mlogit</code> package) :</p>

<pre><code>data(""Fishing"", package = ""mlogit"")
Fish &lt;- mlogit.data(Fishing, varying = c(2:9), shape = ""wide"", choice = ""mode"")
m &lt;- mlogit(mode ~ price | income | catch, data = Fish)
# compute a data.frame containing the mean value of the covariates in the sample
z &lt;- with(Fish, data.frame(price = tapply(price, index(m)$alt, mean), 
	catch = tapply(catch, index(m)$alt, mean), 
income = mean(income)))
# compute the marginal effects (the second one is an elasticity
effects(m, covariate = ""income"", data = z)
effects(m, covariate = ""price"", type = ""rr"", data = z)
effects(m, covariate = ""catch"", type = ""ar"", data = z)
</code></pre>

<p>I have no problem with first step (<code>mlogit.data()</code> function). I think my problem is in the specification of the multinomial regression.</p>

<p>My regression (for example with three variables) is on the form: <code>Y ~ 0 | X1 + X2 + X3</code>. When I try to estimate the marginal effects for a model with 2 variables, there is no problem, however for 3 variables R console returns me the following error: ""Error in if (rhs% in% c (1, 3)) {: argument is of length zero "" (translation from error in R console in french).</p>

<p>To understand what is my problem I tried to perform a multinomial regression of similar shape on the dataset ""Fishing"", i.e.,: <code>mode ~ 0 | income + price + catch</code> (even if this form has no ""economic"" sense.) Again the R console returns me the same error for 3 variables but manages to estimate these effects for a model with two variables.</p>

<p>This leads me to think that my problem really comes from the specification of my multinomial regression.  Do you know how I could find a solution to my problem? Or could you suggest another logit multinomial regression form ?</p>

<p>Thank you for your help :)</p>
"
"0.0570990591522943","0.0697863157798853"," 61480","<p>I am trying to estimate a GAM regression model using the implementation of <code>gam</code> from the <code>mgcv</code> package. I have a working Gaussian model for the dispersion and a log link for the linear predictors but I receive the error </p>

<pre><code>&gt;""Error in eval(expr, envir, enclos) : cannot find valid starting values: please specify some"". 
</code></pre>

<p><strong>Edit 1</strong> - The exact syntax is </p>

<pre><code>splineWAR &lt;- gam(WAR ~ s(zAge, bs=""cr"") + s(zAdjProd, bs=""cr"") + s(zSOPct, bs=""cr"") + s(zBBPct, bs=""cr""), family=gaussian(link=""log""), data = mydata,  start=c(0, 0, 0, 0, 0))
</code></pre>

<p>I have read the relevant threads <a href=""http://stackoverflow.com/questions/13567169/glm-function-in-r-with-log-link-not-working"">here</a> and <a href=""http://stackoverflow.com/questions/8212063/r-glm-starting-values-not-accepted-log-link"">here</a> but have unable to apply the steps suggested to a multiple regression. For instance, when I try and set start values for the 5 variables in my regression (1 dependent and 4 independent) by adding the <code>start=c(n1, n2, n3, n4, n5)</code> argument (where the <code>n</code>'s are the mean of the relevant variable), I receive the same error even though I am seemingly copying the syntax exactly from the first link. Can anyone make a suggestion as to what I should try next? Thanks. </p>

<p><strong>Edit 2</strong> The code in the <code>gam.fit</code> function that runs right before the error is - </p>

<pre><code>if (!(validmu(mu) &amp;&amp; valideta(eta))) 

stop(""Can't find valid starting values: please specify some"")
</code></pre>
"
"0.100857047225074","0.102722675451665"," 61711","<p>In this <a href=""http://stats.stackexchange.com/questions/61547/help-me-fit-this-non-linear-multiple-regression-that-has-defied-all-previous-eff"">thread</a>, I laid out a problem involving fitting a model that attempts to use minor league baseball statistics to predict success at the major league level (explained in full in the thread). After doing further research outside of the thread, I have come to the conclusion that a zero-inflated negative binomial model is likely the best fit given that I believe there are two processes generating the data. The first process determines whether a player will make the majors and once the player reaches the majors, a second process governs their success (as measured by WAR - also explained in the linked thread). </p>

<p>I ran the model and the ZINB model appears to be a reasonable fit given the following diagnostic plot of fitted values vs residuals (broken into two plots to make it easier to inspect visually).</p>

<p><img src=""http://i.imgur.com/GO28KA3.jpg"" alt=""image3"">
<img src=""http://i.imgur.com/6J1QGOj.jpg"" alt=""image1"">
<img src=""http://i.imgur.com/Qe3vmGx.jpg"" alt=""image2""></p>

<p><strong>EDIT 2: Here is a plot of the Pearson residuals.</strong></p>

<p><img src=""http://i.imgur.com/eyRBF7C.jpg/"" alt=""image3""></p>

<p><strong>EDIT 3: Here is a plot of the Pearson residuals vs the fitted values.</strong> </p>

<p><img src=""http://i.imgur.com/nNsgFPl.jpg"" alt=""image4""></p>

<p>Although this is a big improvement over my previous models, there is clearly a bias for the model to underestimate a player's career WAR i.e., a majority of the residuals are greater than zero. <strong>EDIT: It turns out that the plot is misleading due to the high number of overlapping residuals. In reality, only 10% of the residuals are greater than zero.</strong> I am guessing that this may be improved by either a) a better specification of the functional form of the covariates or b) using a different yet similar model e.g., ZIP. Given that this type of regression goes well beyond what I have worked with before, I would appreciate any suggestions on further diagnostics I can use to both test this model and compare it to others and how to improve the functional form of the covariates given that the R function I have used, zeroinfl (from the pscl package), does not appear to be that flexible. Thank you!</p>
"
"0","0.0284901441149095"," 61845","<p>I am trying to run a Cox regression on a sample 2,000,000 row dataset as follows using only R. This is a direct translation of a PHREG in SAS. The sample is representative of the structure of the original dataset.</p>

<pre><code>##
library(survival)

### Replace 100000 by 2,000,000

test &lt;- data.frame(start=runif(100000,1,100), stop=runif(100000,101,300), censor=round(runif(100000,0,1)), testfactor=round(runif(100000,1,11)))

test$testfactorf &lt;- as.factor(test$testfactor)
summ &lt;- coxph(Surv(start,stop,censor) ~ relevel(testfactorf, 2), test)

# summary(summ)
##

user  system elapsed 
9.400   0.090   9.481 
</code></pre>

<p>The main challenge is in the compute time for the original dataset (2m rows). As far as I understand, in SAS this could take up to 1 day, ... but at least it finishes.</p>

<ul>
<li><p>Running the example with only 100,000 observations take only 9 seconds. Thereafter the time increases almost quadratically for every 100,000 increment in the number of observations.</p></li>
<li><p>I have not found any means to parallelize the operation (e.g., we can leverage a 48-core machine if this was possible)</p></li>
<li><p>Neither <code>biglm</code> nor any package from Revolution Analytics is available for Cox regression, and so I cannot leverage those.</p></li>
</ul>

<p><strong>Is there a means to represent this in terms of a logistic regression (for which there are packages in Revolution) or if there are any other alternatives to this problem?</strong> I know that they are fundamentally different, but it's the closest I can assume as a possibility given the circumstances. </p>
"
"0.0979044918012804","0.0997155044021832"," 61869","<p>I am trying to replicate a path analysis SEM model using Lavaan in R, and was very confused about the results that it gave regarding the model fit statistics. </p>

<p><strong>The code is as follows:</strong> </p>

<pre><code>#Import Package
library(lavaan)

#Input Correlation Matrix
sigma &lt;- matrix(c(1.00, -0.03,  0.39, -0.05, -0.08,
                 -0.03,  1.00,  0.07, -0.23, -0.16,
                  0.39,  0.07,  1.00, -0.13, -0.29,
                 -0.05, -0.23, -0.13,  1.00,  0.34,
                 -0.08, -0.16 ,-0.29,  0.34,  1.00), nr=5, byrow=TRUE)
rownames(sigma) &lt;-c(""Exercise"", ""Hardiness"", ""Fitness"", ""Stress"", ""Illness"")
colnames(sigma) &lt;-c(""Exercise"", ""Hardiness"", ""Fitness"", ""Stress"", ""Illness"")

#Create Covariance Matrix
sdevs &lt;-c(66.5, 3.8, 18.4, 6.7, 624.8)
covmax &lt;- cor2cov(sigma, sdevs)
as.matrix(covmax)

#Specify Model 
mymodel&lt;-'Illness ~ Exercise + Fitness
Illness ~ Hardiness + Stress
Fitness ~ Exercise + Hardiness 
Stress ~ Exercise + Hardiness + Fitness 
Exercise ~~ Exercise 
Hardiness ~~ Hardiness 
Exercise ~~ Hardiness'

#Fit the model with the covariance matrix
N = 363
fit.path &lt;-sem(mymodel,sample.cov=covmax, sample.nobs=N, fixed.x=FALSE)

#Summary of the model fit
summary(fit.path, fit.measures = TRUE)
</code></pre>

<p><strong>And the output I get is as follows:</strong> </p>

<pre><code> lavaan (0.5-12) converged normally after  93 iterations

 Number of observations                         37300

 Estimator                                         ML
 Minimum Function Test Statistic                0.000
 Degrees of freedom                                 0
 P-value (Chi-square)                           1.000

 Model test baseline model:

 Minimum Function Test Statistic            16594.387
 Degrees of freedom                                10
 P-value                                        0.000

 Full model versus baseline model:

 Comparative Fit Index (CFI)                    1.000
 Tucker-Lewis Index (TLI)                       1.000

 Loglikelihood and Information Criteria:

 Loglikelihood user model (H0)             -882379.005
 Loglikelihood unrestricted model (H1)     -882379.005

 Number of free parameters                         15
 Akaike (AIC)                              1764788.009
 Bayesian (BIC)                            1764915.910
 Sample-size adjusted Bayesian (BIC)       1764868.240

 Root Mean Square Error of Approximation:

 RMSEA                                          0.000
 90 Percent Confidence Interval          0.000  0.000
 P-value RMSEA &lt;= 0.05                          1.000

 Standardized Root Mean Square Residual:

 SRMR                                           0.000

 Parameter estimates:

 Information                                 Expected
 Standard Errors                             Standard

                Estimate  Std.err  Z-value  P(&gt;|z|)
 Regressions:
 Illness ~
 Exercise          0.318    0.048    6.640    0.000
 Fitness          -8.835    0.174  -50.737    0.000
 Hardiness       -12.146    0.793  -15.321    0.000
 Stress           27.125    0.451   60.079    0.000
 Fitness ~
 Exercise          0.109    0.001   82.602    0.000
 Hardiness         0.396    0.023   17.211    0.000
 Stress ~
 Exercise         -0.001    0.001   -2.614    0.009
 Hardiness        -0.393    0.009  -44.332    0.000
 Fitness          -0.040    0.002  -19.953    0.000

 Covariances:
 Exercise ~~
 Hardiness        -7.581    1.309   -5.791    0.000

 Variances:
 Exercise       4422.131   32.381
 Hardiness        14.440    0.106
 Illness       318744.406 2334.012
 Fitness         284.796    2.085
 Stress           41.921    0.307
</code></pre>

<p><strong>These are my questions:</strong>  </p>

<ul>
<li>Why does the chi-squared say that there are no degrees of freedom? </li>
<li>Why are the p-values exactly 1? Why is the CFI and TLI exactly 1? </li>
<li><p>Why is the RMSEA 0?</p></li>
<li><p>What would I need to do to simulate a more realistic model that doesn't appear artificially ""perfect""? </p></li>
<li>Does it have to do with the model specification? </li>
</ul>
"
"0.0634361479695551","0.0753778361444409"," 61914","<p>Consider the following (in R):</p>

<pre><code>library(MASS)
plot(stack.loss~Air.Flow,data=stackloss)
regression &lt;- rlm(stack.loss~Air.Flow,data=stackloss)
abline(regression)
</code></pre>

<p>For each of the points I would like to to test against the null hypothesis that it really is located on the line (rather than where it really is) - in essence quantifying the degree to which these are outliers with respect to the regression model.</p>

<p>Possibly using hopeless search term combinations, I have been unable to identify appropriate methodology.  Would it be valid to use for each residual a one-sample t-test (ar equivalent) against the residual population to establish such a measure?</p>

<p>Edit: Further reading seems to indicate that ""Tolerance Interval"" is what I might be looking for - the ""tolerance"" R package provides calculations of that.
I am however puzzled by the apparent contradictory nature of such intervals for regressions as defined by different publications. In the <a href=""http://www.jstatsoft.org/v36/i05/"" rel=""nofollow"">R package case</a> the tolerance bands seem to track the regression line parallely (see Fig. 12 and below example), while alternative sources such as <a href=""http://www.sciencedirect.com/science/article/pii/S0169743907000457"" rel=""nofollow"">this</a> operate with bands that are reminiscent of confidence intervals in shape (see Fig. 4). The latter seems more intuitive, but other's opinions would be appreciated.</p>

<p>For each point in a sample I am actually looking for ""which x/95% tolerance band is running through this point"" ...</p>

<p>Here's how usage of ""tolerance"" might look like (adding to above code) for 95%/95% 2-sided nonparametric regression tolerance bounds:</p>

<pre><code>library(tolerance)
tol.bounds &lt;- npregtol.int(x=stackloss$Air.Flow,        
    y=stackloss$stack.loss, y.hat=regression$fit, side=2, alpha=0.05, P=0.95, 
    method=""WILKS"")
lines(tol.bounds$x,y=tol.bounds$""2-sided.lower"",col=""red"")
lines(tol.bounds$x,y=tol.bounds$""2-sided.upper"",col=""red"")
</code></pre>
"
"0.112084934384975","0.120873444603807"," 62070","<p>Let's say you have a response variable and an independent variable. Your data is measured across several levels of a categorical independent variable. One approach in analysing these data would be to use linear regression to estimate a slope at each level of the categorical independent variable. This is the approach I've used here, using <code>sleepstudy</code> dataset from the <code>R</code> <code>lme4</code> package (I've stored the betas from each model in <code>lmBetas</code>):</p>

<pre><code>library(lme4); library(plyr); library(ggplot2)
lmBetas &lt;- daply(sleepstudy, .(Subject), function(x) coef(lm(Reaction ~ Days, data=x))[""Days""])
</code></pre>

<p>Another approach in analysing these data would be to use a mixed effects model to estimate slopes for each level of the categorical independent variable, which in this case is <code>Subject</code>. This is the approach I've taken here (I've stored the betas from the model in <code>lmerBetas</code>):</p>

<pre><code>lmerBetas &lt;- coef(lmer(Reaction ~ Days + (Days | Subject), data=sleepstudy))$Subject[,""Days""]
</code></pre>

<p>I have learned that a single mixed effects model, as implemented through the <code>lmer</code> function in R, is more accurate at estimating slopes than a multiple linear regression model applied to multilevel data. This can be demonstrated with this plot of betas from the above models. </p>

<pre><code>betas &lt;- data.frame(method.betas = c(lmerBetas, lmBetas))
betas$method &lt;- c(rep(""lmer"", 18), rep(""lm"", 18))

ggplot(betas, aes(method.betas)) + 
  geom_histogram() +
  facet_grid(method ~ .)
</code></pre>

<p>The top histogram shows betas estimated using linear regression, and the bottom histogram shows betas estimated using mixed effects. You can see betas estimated using linear regression are more widely spread than those estimated through the mixed effects model.</p>

<p><img src=""http://i.stack.imgur.com/YtB22.jpg"" alt=""enter image description here""></p>

<p>So finally, my questions:</p>

<ol>
<li><p>Is a mixed effects model's higher accuracy in betas estimation connected with the fact that it models intercepts and slopes for each level of the categorical independent variable under a joint probability distribution?</p></li>
<li><p>Generally speaking, why is a mixed effects model more accurate in its betas estimation?</p></li>
</ol>
"
"0.0570990591522943","0.0697863157798853"," 62307","<p>I used regular logistic regression on my dataset and got a few significant hits. However, since the data is 1:1 case-control matched data I decided to try using conditional logistic regression (<code>clogit()</code> in the <code>survival</code> package of <code>R</code>). </p>

<p>However, none of the results are significant any more. Is this normal? I was hoping to gain power by switching to conditional logistic regression. </p>

<pre><code> m&lt;-clogit(PHENO==2 ~ x + as.factor(COVAR[,1]) + strata(COVAR[,2]) )
</code></pre>

<p>This is a GWAS (genome wide association study). <code>PHENO</code> is either 1 (control) or 2 (case). <code>x</code> is number of copies of the minor allele (0, 1, or 2) in the SNP of interest, <code>COVAR[,1]</code> is a race covariate and <code>COVAR[,2]</code> is the matching covariate. </p>

<p>This test is performed for each SNP (~300,000) and the resulting p-values are adjusted for FDR. </p>

<p>Perhaps the matching of the cases and controls was not very good?</p>

<p>Matching: case and control pairs had: 1) same sex. 2) BMI > threshold. 3) same self-reported ethnicity. 4) lived within 50 miles of procurement site. </p>

<p>Perhaps I should use GLMM?</p>
"
"0.0884574820723792","0.0900937462695559"," 62427","<p>I am working to investigate association between environmental pollution and daily hospital admission due to various causes.   This outcome data has excess zeros on days when there are no admissions for specific causes.  I would like to adjust for temperature and humidity using smooth terms.  Usually Poisson generalized additive models with smoothing parameters for time trend and meteorological variables are used to model such associations.   However due to excessive zeros I was advised to consider zero inflated models.  How can I implement this in R and address non parametric associations of temperature, humidity and time trend? </p>

<ol>
<li>What is the criterion to use zero inflated models for a count data?</li>
<li>How do I run zero inflated model and how do I check the fit?</li>
</ol>

<p>The following is a sample GAM model in R using mgcv package:</p>

<pre><code>Log {E (hospital admission)} = Î±+Î²(pollutant)+s(time)+s(temperature) + s(Relative Humidity) + DOW + flu
Where: 
E (admission) = expected count of cause specific admissions on day t
Î²= regression coefficient of the pollutant
pollutant = air pollutant (PM10, ozone, NO2) level at time t
s = smooth function using natural or penalized spline
dow =  vector of regression coefficient associated with indicator variables for day of the week (DOW).
Flu= weekly influenza count
Time, temperature and relative humidity are covariates 
</code></pre>

<p>Thanks</p>
"
"0.027972711943223","0.0284901441149095"," 62483","<p>I've got a conditional logistic regression setup using <code>clogit</code> in <code>R</code> like this: </p>

<pre><code>m&lt;-clogit(PHENO==2 ~ x + as.factor(COVAR[,1]) + strata(COVAR[,2]) )
</code></pre>

<p>I wanted to try doing GLMM analysis in <code>R</code>. I'm a little confused on the syntax for the <code>lme4</code> package in <code>R</code>. <code>COVAR[,2]</code> is the matching variable in my data. Can someone explain to me the difference between some of these statements:</p>

<pre><code>m &lt;-  lmer(PHENO==2 ~ x + as.factor(COVAR[,1]) + (x|COVAR[,2]) )
m &lt;-  lmer(PHENO==2 ~ x + as.factor(COVAR[,1]) + (1|COVAR[,2]) )
</code></pre>

<p>Which one is more appropriate? </p>
"
"0.115334445976885","0.117467873474841"," 62852","<p>I have implemented a Gibbs Sampler for the <strong>Bayesian Elastic Net</strong> (BEN) according to this paper on <a href=""http://www.stat.ufl.edu/~casella/Papers/BL-Final.pdf"" rel=""nofollow"">Penalized Regression by Kyung et al.</a><br>
In this paper, they execute a simulation study that has been used in other papers on Penalized Regression (LASSO, Bridge, Ridge) to compare the performance of the proposed models.</p>

<p>Here are details of the simulation taken from the above mentioned paper:  </p>

<blockquote>
  <p>We simulate data from the true model
  $$
y=X\beta+\sigma\epsilon \quad\epsilon_i\,{\raise.17ex\hbox{$\scriptstyle\sim$}}\,\text{iid}\,N(0,1)
$$
  We simulate data sets with $n=20$ to fit models and $n=200$ to compare prediction errors of proposed models with eight predictors. We let $\beta=(3,1.5,0,0,2,0,0,0)$ and $\sigma=3$. The pairwise correlation between $x_i$ and $x_j$ was set to be $corr(i,j)=0.5^{|i-j|}$.<br>
  Later on they say, that for the prediction error, they calculate the average mean squared error based on 50 replications. By average they mean the median in this case.</p>
</blockquote>

<p>To simulate this data and calculate the MSE I've used following code in R: </p>

<pre><code># Number of observations
n.train &lt;- 20
n.test  &lt;- 200
# Error variance
sigma &lt;- 3
# Pairwise correlation of X
cor &lt;- 0.5
# Number of predictors
p &lt;- 8
# Create training and test data set (package QRM and mvtnorm required)
Z &lt;- equicorr(p, rho=cor)
X.train &lt;- rmvnorm(n.train,sigma=Z)
X.test  &lt;- rmvnorm(n.test,sigma=Z)
# Create error 
error.train &lt;- rnorm(n.train,mean=0,sd=1)
error.test  &lt;- rnorm(n.test,mean=0,sd=1)
# Create beta
beta.true &lt;- c(3,1.5,0,0,2,0,0,0)
# Create both responses
Y.train &lt;- X.train %*% beta.true + sigma*error.train
Y.test  &lt;- X.test %*% beta.true + sigma*error.test

# Fit the training data set with the BEN Gibbs Sampler
beta.ben &lt;- BEN(X.train,Y.train, iter=11000, burn = 1000)
# Calculate the predicted response
Y.pred   &lt;- X.test %*% beta.ben
# Calculate the mean squared error (MSE)
MSE      &lt;- sum((Y.train - Y.pred)^2)/n.train
</code></pre>

<p>My problem is that my results are not even close to comparable to the ones in the paper which makes me doubt my simulation study ""setup"".<br>
As one of the authors of the paper has uploaded the Gibbs Sampler code and I could check if I did something wrong, I know that the problem doesn't lie there.</p>

<p>So my questions are:</p>

<ol>
<li>Does anybody have experience with this kind of simulation study and can check if I did something wrong?</li>
<li>Is the MSE I calculate the same as the one used in the paper? In researching on this topic I found many different ways to calculate the MSE and it was also sometimes used but actually the mean squared prediction error was meant. For example the Wikipedia article on MSE alone lists three variations.</li>
</ol>

<p>I don't need help with coding, rather more information on how this simulation is typically excecuted so I can figure out what I'm doing wrong.</p>
"
"0.0395593886064618","0.040291148201269"," 62853","<p>I am running a regression analyis in r:</p>

<pre><code>fit &lt;- lm(Cost ~ Slope + YardDist, data = test)
</code></pre>

<p>I want to test the two independent variables for multicollinearity. I tested it with <code>vif()</code> (from the car package) and <code>kappa()</code>.</p>

<pre><code>&gt; vif(fit)
   Slope YardDist 
1.000121 1.000121 
&gt; kappa(fit)
[1] 11631.87
</code></pre>

<p>VIF tells me there is no multicollinearity and kappa tells me there is very high multicollinearity. 
What is the difference between both and which one is 'right'?</p>
"
"NaN","NaN"," 62919","<p>@Dmitrij Celov posted an answer to the question <a href=""http://stats.stackexchange.com/questions/8303/how-to-do-logistic-regression-subset-selection"">How to do logistic regression subset selection</a> saying that <a href=""http://www.jstor.org/discover/10.2307/2531779?uid=3739256&amp;uid=2&amp;uid=4&amp;sid=21102415674961"" rel=""nofollow"">this paper</a> was a good reference. I was wondering if anyone knows if any of these authors ever created an R package based on it.</p>
"
"0.118678165819385","0.114158253236929"," 63222","<p>How do I get p-values using the <code>multinom</code> function of <code>nnet</code> package in <code>R</code>?</p>

<p>I have a dataset which consists of â€œPathology scoresâ€ (Absent, Mild, Severe) as outcome variable, and two main effects: Age (two factors: twenty / thirty days) and Treatment Group (four factors: infected without ATB; infected + ATB1; infected + ATB2; infected + ATB3).</p>

<p>First I tried to fit an ordinal regression model, which seems more appropriate given the characteristics of my dependent variable (ordinal). However, the assumption of odds proportionality was severely violated (graphically), which prompted me to use a multinomial model instead, using the <code>nnet</code> package.  </p>

<p>First I chose the outcome level that I need to use as baseline category: </p>

<pre><code>Data$Path &lt;- relevel(Data$Path, ref = ""Absent"")
</code></pre>

<p>Then, I needed to set baseline categories for the independent variables:</p>

<pre><code>Data$Age &lt;- relevel(Data$Age, ref = ""Twenty"")
Data$Treat &lt;- relevel(Data$Treat, ref=""infected without ATB"") 
</code></pre>

<p>The model:</p>

<pre><code>test &lt;- multinom(Path ~ Treat + Age, data = Data) 
# weights:  18 (10 variable) 
initial value 128.537638 
iter 10 value 80.623608 
final  value 80.619911 
converged
</code></pre>

<p>The output:</p>

<pre><code>Coefficients:
         (Intercept)   infected+ATB1   infected+ATB2   infected+ATB3    AgeThirty
Moderate   -2.238106   -1.1738540      -1.709608       -1.599301        2.684677
Severe     -1.544361   -0.8696531      -2.991307       -1.506709        1.810771

Std. Errors:
         (Intercept)    infected+ATB1   infected+ATB2   infected+ATB3    AgeThirty
Moderate   0.7880046    0.8430368       0.7731359       0.7718480        0.8150993
Severe     0.6110903    0.7574311       1.1486203       0.7504781        0.6607360

Residual Deviance: 161.2398
AIC: 181.2398
</code></pre>

<p>For a while, I could not find a way to get the $p$-values for the model and estimates when using <code>nnet:multinom</code>. Yesterday I came across a post where the author put forward a similar issue regarding estimation of $p$-values for coefficients (<a href=""http://stats.stackexchange.com/questions/9715/how-to-set-up-and-estimate-a-multinomial-logit-model-in-r"">How to set up and estimate a multinomial logit model in R?</a>). There, one blogger suggested that getting $p$-values from the <code>summary</code> result of <code>multinom</code> is pretty easy, by first getting the $t$values as follows: </p>

<pre><code>pt(abs(summary1$coefficients / summary1$standard.errors), df=nrow(Data)-10, lower=FALSE) 

         (Intercept)   infected+ATB1   infected+ATB2   infected+ATB3    AgeThirty
Moderate 0.002670340   0.08325396      0.014506395     0.02025858       0.0006587898
Severe   0.006433581   0.12665278      0.005216581     0.02352202       0.0035612114
</code></pre>

<p>According to Peter Dalgard, ""There's at least a factor of 2 missing for a two-tailed $p$-value. It is usually a mistake to use the $t$-distribution for what is really a $z$-statistic; for aggregated data, it can be a very bad mistake.""
According to Brian Ripley, ""it is also a mistake to use Wald tests for <code>multinom</code> fits, since they suffer from the same (potentially severe) problems as binomial fits. 
Use profile-likelihood confidence intervals (for which the package does provide software), or if you must test, likelihood-ratio tests (ditto).""</p>

<p>I just need to be able to derive reliable $p$-values.</p>
"
"0.128674474938826","0.131054662928584"," 63233","<h2>Background</h2>

<p>In a paper from Epstein (1991): <a href=""http://journals.ametsoc.org/doi/pdf/10.1175/1520-0442%281991%29004%3C0365%3AOODCVF%3E2.0.CO%3B2"" rel=""nofollow"">On obtaining daily climatological values from monthly means</a>, the formulation and an algorithm for calculating Fourier interpolation for periodical and even-spaced values are given.</p>

<p>In the paper, the goal is to <strong>obtain daily values from monthly means</strong> by interpolation.</p>

<p>In short, it is assumed that unknown daily values can be represented by the sum of harmonic components:
$$
y(t) = a_{0} + \sum_{j}\left[a_{j}\,\cos(2\pi jt/12)+b_{j}\,\sin(2\pi jt/12)\right]
$$
In the paper $t$ (time) is expressed in months.</p>

<p>After some derviation, it is shown that the terms can be calculated by:
$$
\begin{align}
a_{0} &amp;= \sum_{T}Y_{T}/12 \\
a_{j} &amp;= \left[ (\pi j/12)/\sin(\pi j/12)\right] \times \sum_{T}\left[Y_{T}\,\cos(2\pi jT/12)/6 \right]~~~~~~~j=1,\ldots, 5 \\
b_{j} &amp;= \left[ (\pi j/12)/\sin(\pi j/12)\right] \times \sum_{T}\left[Y_{T}\,\sin(2\pi jT/12)/6 \right]~~~~~~~j=1,\ldots, 5 \\
a_{6} &amp;= \left[ (\pi j/12)/\sin(\pi j/12)\right]\times \sum_{T}\left[Y_{T}\cos(\pi T)/12\right] \\
b_{6} &amp;= 0
\end{align}
$$
Where $Y_{T}$ denote the monthly means and $T$ the month.</p>

<p><a href=""http://journals.ametsoc.org/doi/pdf/10.1175/1520-0493%281995%29123%3C2251%3ATIODSU%3E2.0.CO%3B2"" rel=""nofollow"">Harzallah (1995)</a> summarizes this aproach as follows: ""The interpolation is carried out by adding zeros to the spectral coefficients of data and by performing an inverse Fourier transform to the resulting extended coefficients. The method is equivalent to applying a rectangular filter to Fourier coefficients.""</p>

<hr>

<h2>Questions</h2>

<p>My goal is to use the above methodology for interpolation of <strong>weekly means to obtain daily data</strong> (see <a href=""http://stats.stackexchange.com/questions/59418/interpolation-of-influenza-data-that-conserves-weekly-mean/63135#63135"">my previous question</a>). In summary, I have 835 weekly means of count data (see the example dataset at the bottom of the question). There are quite a few things that I don't understand before I can apply the approach outlined above:</p>

<ol>
<li>How would the formulas have to be changed for my situation (weekly instead of monthly values)?</li>
<li>How could the time $t$ be expressed? I assumed $t/835$ (or $t/n$ with $n$ data points in general), is that correct?</li>
<li>Why does the author calculate 7 terms (i.e. $0\leq j \leq 6$)? How many terms would I have to consider?</li>
<li>I understand that the question can probably be solved by using a <a href=""http://stats.stackexchange.com/questions/60500/how-to-find-a-good-fit-for-semi-sinusoidal-model-in-r?lq=1"">regression approach</a> and using the predictions for interpolation (thanks to Nick). Still, some things are unclear to me: How many terms of harmonics should be included in the regression? And what period should I take? How can the regression be done to ensure that the weekly means are preserved (as I don't want an exact harmonic fit to the data)?</li>
</ol>

<p>Using the <a href=""http://stats.stackexchange.com/questions/60500/how-to-find-a-good-fit-for-semi-sinusoidal-model-in-r?lq=1"">regression approach</a> (which is also explained in <a href=""http://www.stata-journal.com/sjpdf.html?articlenum=st0116"" rel=""nofollow"">this paper</a>), I managed to get an exact harmonic fit to the data (the $j$ in my example would run through $1, \ldots, 417$, so I fitted 417 terms). <strong>How can this approach be modified -$~$if possible$~$- to achieve the conservation of the weekly means?</strong> Maybe by applying correction factors to each regression term?</p>

<p>The plot of the exact harmonic fit is:</p>

<p><img src=""http://i.stack.imgur.com/7XuxU.png"" alt=""Exact harmonic fit""></p>

<p><strong>EDIT</strong></p>

<p>Using the <a href=""http://cran.r-project.org/web/packages/signal/"" rel=""nofollow"">signal package</a> and the <code>interp1</code> function, here's what I've managed to do using the example data set from below (many thanks to @noumenal). I use <code>q=7</code> as we have weekly data:</p>

<pre><code># Set up the time scale

daily.ts &lt;- seq(from=as.Date(""1995-01-01""), to=as.Date(""2010-12-31""), by=""day"")

# Set up data frame 

ts.frame &lt;- data.frame(daily.ts=daily.ts, wdayno=as.POSIXlt(daily.ts)$wday,
                       yearday = 1:5844,
                       no.influ.cases=NA)

# Add the data from the example dataset called ""my.dat""

ts.frame$no.influ.cases[ts.frame$wdayno==3] &lt;- my.dat$case

# Interpolation

case.interp1 &lt;- interp1(x=ts.frame$yearday[!is.na(ts.frame$no.influ.case)],y=(ts.frame$no.influ.cases[!is.na(ts.frame$no.influ.case)]),xi=ts.frame$yearday, method = c(""cubic""))

# Plot subset for better interpretation
par(bg=""white"", cex=1.2, las=1)
plot((ts.frame$no.influ.cases)~ts.frame$yearday, pch=20,
     col=grey(0.4),
     cex=1, las=1,xlim=c(0,400), xlab=""Day"", ylab=""Influenza cases"")
lines(case.interp1, col=""steelblue"", lwd=1)
</code></pre>

<p><img src=""http://i.stack.imgur.com/R1FE8.png"" alt=""Cubicinterpo""></p>

<p>There are two issues here:</p>

<ol>
<li>The curve seem to fit ""too good"": it goes through every point </li>
<li>The weekly means are not conserved</li>
</ol>

<p><strong>Example dataset</strong></p>

<pre><code>structure(list(date = structure(c(9134, 9141, 9148, 9155, 9162, 
9169, 9176, 9183, 9190, 9197, 9204, 9211, 9218, 9225, 9232, 9239, 
9246, 9253, 9260, 9267, 9274, 9281, 9288, 9295, 9302, 9309, 9316, 
9323, 9330, 9337, 9344, 9351, 9358, 9365, 9372, 9379, 9386, 9393, 
9400, 9407, 9414, 9421, 9428, 9435, 9442, 9449, 9456, 9463, 9470, 
9477, 9484, 9491, 9498, 9505, 9512, 9519, 9526, 9533, 9540, 9547, 
9554, 9561, 9568, 9575, 9582, 9589, 9596, 9603, 9610, 9617, 9624, 
9631, 9638, 9645, 9652, 9659, 9666, 9673, 9680, 9687, 9694, 9701, 
9708, 9715, 9722, 9729, 9736, 9743, 9750, 9757, 9764, 9771, 9778, 
9785, 9792, 9799, 9806, 9813, 9820, 9827, 9834, 9841, 9848, 9855, 
9862, 9869, 9876, 9883, 9890, 9897, 9904, 9911, 9918, 9925, 9932, 
9939, 9946, 9953, 9960, 9967, 9974, 9981, 9988, 9995, 10002, 
10009, 10016, 10023, 10030, 10037, 10044, 10051, 10058, 10065, 
10072, 10079, 10086, 10093, 10100, 10107, 10114, 10121, 10128, 
10135, 10142, 10149, 10156, 10163, 10170, 10177, 10184, 10191, 
10198, 10205, 10212, 10219, 10226, 10233, 10240, 10247, 10254, 
10261, 10268, 10275, 10282, 10289, 10296, 10303, 10310, 10317, 
10324, 10331, 10338, 10345, 10352, 10359, 10366, 10373, 10380, 
10387, 10394, 10401, 10408, 10415, 10422, 10429, 10436, 10443, 
10450, 10457, 10464, 10471, 10478, 10485, 10492, 10499, 10506, 
10513, 10520, 10527, 10534, 10541, 10548, 10555, 10562, 10569, 
10576, 10583, 10590, 10597, 10604, 10611, 10618, 10625, 10632, 
10639, 10646, 10653, 10660, 10667, 10674, 10681, 10688, 10695, 
10702, 10709, 10716, 10723, 10730, 10737, 10744, 10751, 10758, 
10765, 10772, 10779, 10786, 10793, 10800, 10807, 10814, 10821, 
10828, 10835, 10842, 10849, 10856, 10863, 10870, 10877, 10884, 
10891, 10898, 10905, 10912, 10919, 10926, 10933, 10940, 10947, 
10954, 10961, 10968, 10975, 10982, 10989, 10996, 11003, 11010, 
11017, 11024, 11031, 11038, 11045, 11052, 11059, 11066, 11073, 
11080, 11087, 11094, 11101, 11108, 11115, 11122, 11129, 11136, 
11143, 11150, 11157, 11164, 11171, 11178, 11185, 11192, 11199, 
11206, 11213, 11220, 11227, 11234, 11241, 11248, 11255, 11262, 
11269, 11276, 11283, 11290, 11297, 11304, 11311, 11318, 11325, 
11332, 11339, 11346, 11353, 11360, 11367, 11374, 11381, 11388, 
11395, 11402, 11409, 11416, 11423, 11430, 11437, 11444, 11451, 
11458, 11465, 11472, 11479, 11486, 11493, 11500, 11507, 11514, 
11521, 11528, 11535, 11542, 11549, 11556, 11563, 11570, 11577, 
11584, 11591, 11598, 11605, 11612, 11619, 11626, 11633, 11640, 
11647, 11654, 11661, 11668, 11675, 11682, 11689, 11696, 11703, 
11710, 11717, 11724, 11731, 11738, 11745, 11752, 11759, 11766, 
11773, 11780, 11787, 11794, 11801, 11808, 11815, 11822, 11829, 
11836, 11843, 11850, 11857, 11864, 11871, 11878, 11885, 11892, 
11899, 11906, 11913, 11920, 11927, 11934, 11941, 11948, 11955, 
11962, 11969, 11976, 11983, 11990, 11997, 12004, 12011, 12018, 
12025, 12032, 12039, 12046, 12053, 12060, 12067, 12074, 12081, 
12088, 12095, 12102, 12109, 12116, 12123, 12130, 12137, 12144, 
12151, 12158, 12165, 12172, 12179, 12186, 12193, 12200, 12207, 
12214, 12221, 12228, 12235, 12242, 12249, 12256, 12263, 12270, 
12277, 12284, 12291, 12298, 12305, 12312, 12319, 12326, 12333, 
12340, 12347, 12354, 12361, 12368, 12375, 12382, 12389, 12396, 
12403, 12410, 12417, 12424, 12431, 12438, 12445, 12452, 12459, 
12466, 12473, 12480, 12487, 12494, 12501, 12508, 12515, 12522, 
12529, 12536, 12543, 12550, 12557, 12564, 12571, 12578, 12585, 
12592, 12599, 12606, 12613, 12620, 12627, 12634, 12641, 12648, 
12655, 12662, 12669, 12676, 12683, 12690, 12697, 12704, 12711, 
12718, 12725, 12732, 12739, 12746, 12753, 12760, 12767, 12774, 
12781, 12788, 12795, 12802, 12809, 12816, 12823, 12830, 12837, 
12844, 12851, 12858, 12865, 12872, 12879, 12886, 12893, 12900, 
12907, 12914, 12921, 12928, 12935, 12942, 12949, 12956, 12963, 
12970, 12977, 12984, 12991, 12998, 13005, 13012, 13019, 13026, 
13033, 13040, 13047, 13054, 13061, 13068, 13075, 13082, 13089, 
13096, 13103, 13110, 13117, 13124, 13131, 13138, 13145, 13152, 
13159, 13166, 13173, 13180, 13187, 13194, 13201, 13208, 13215, 
13222, 13229, 13236, 13243, 13250, 13257, 13264, 13271, 13278, 
13285, 13292, 13299, 13306, 13313, 13320, 13327, 13334, 13341, 
13348, 13355, 13362, 13369, 13376, 13383, 13390, 13397, 13404, 
13411, 13418, 13425, 13432, 13439, 13446, 13453, 13460, 13467, 
13474, 13481, 13488, 13495, 13502, 13509, 13516, 13523, 13530, 
13537, 13544, 13551, 13558, 13565, 13572, 13579, 13586, 13593, 
13600, 13607, 13614, 13621, 13628, 13635, 13642, 13649, 13656, 
13663, 13670, 13677, 13684, 13691, 13698, 13705, 13712, 13719, 
13726, 13733, 13740, 13747, 13754, 13761, 13768, 13775, 13782, 
13789, 13796, 13803, 13810, 13817, 13824, 13831, 13838, 13845, 
13852, 13859, 13866, 13873, 13880, 13887, 13894, 13901, 13908, 
13915, 13922, 13929, 13936, 13943, 13950, 13957, 13964, 13971, 
13978, 13985, 13992, 13999, 14006, 14013, 14020, 14027, 14034, 
14041, 14048, 14055, 14062, 14069, 14076, 14083, 14090, 14097, 
14104, 14111, 14118, 14125, 14132, 14139, 14146, 14153, 14160, 
14167, 14174, 14181, 14188, 14195, 14202, 14209, 14216, 14223, 
14230, 14237, 14244, 14251, 14258, 14265, 14272, 14279, 14286, 
14293, 14300, 14307, 14314, 14321, 14328, 14335, 14342, 14349, 
14356, 14363, 14370, 14377, 14384, 14391, 14398, 14405, 14412, 
14419, 14426, 14433, 14440, 14447, 14454, 14461, 14468, 14475, 
14482, 14489, 14496, 14503, 14510, 14517, 14524, 14531, 14538, 
14545, 14552, 14559, 14566, 14573, 14580, 14587, 14594, 14601, 
14608, 14615, 14622, 14629, 14636, 14643, 14650, 14657, 14664, 
14671, 14678, 14685, 14692, 14699, 14706, 14713, 14720, 14727, 
14734, 14741, 14748, 14755, 14762, 14769, 14776, 14783, 14790, 
14797, 14804, 14811, 14818, 14825, 14832, 14839, 14846, 14853, 
14860, 14867, 14874, 14881, 14888, 14895, 14902, 14909, 14916, 
14923, 14930, 14937, 14944, 14951, 14958, 14965, 14972), class = ""Date""), 
    cases = c(168L, 199L, 214L, 230L, 267L, 373L, 387L, 443L, 
    579L, 821L, 1229L, 1014L, 831L, 648L, 257L, 203L, 137L, 78L, 
    82L, 69L, 45L, 51L, 45L, 63L, 55L, 54L, 52L, 27L, 24L, 12L, 
    10L, 22L, 42L, 32L, 52L, 82L, 95L, 91L, 104L, 143L, 114L, 
    100L, 83L, 113L, 145L, 175L, 222L, 258L, 384L, 755L, 976L, 
    879L, 846L, 1004L, 801L, 799L, 680L, 530L, 410L, 302L, 288L, 
    234L, 269L, 245L, 240L, 176L, 188L, 128L, 96L, 59L, 63L, 
    44L, 52L, 39L, 50L, 36L, 40L, 48L, 32L, 39L, 28L, 29L, 16L, 
    20L, 25L, 25L, 48L, 57L, 76L, 117L, 107L, 91L, 90L, 83L, 
    76L, 86L, 104L, 101L, 116L, 120L, 185L, 290L, 537L, 485L, 
    561L, 1142L, 1213L, 1235L, 1085L, 1052L, 987L, 918L, 746L, 
    620L, 396L, 280L, 214L, 148L, 148L, 94L, 107L, 69L, 55L, 
    69L, 47L, 43L, 49L, 30L, 42L, 51L, 41L, 39L, 40L, 38L, 22L, 
    37L, 26L, 40L, 56L, 54L, 74L, 99L, 114L, 114L, 120L, 114L, 
    123L, 131L, 170L, 147L, 163L, 163L, 160L, 158L, 163L, 124L, 
    115L, 176L, 171L, 214L, 320L, 507L, 902L, 1190L, 1272L, 1282L, 
    1146L, 896L, 597L, 434L, 216L, 141L, 101L, 86L, 65L, 55L, 
    35L, 49L, 29L, 55L, 53L, 57L, 34L, 43L, 42L, 13L, 17L, 20L, 
    27L, 36L, 47L, 64L, 77L, 82L, 82L, 95L, 107L, 96L, 106L, 
    93L, 114L, 102L, 116L, 128L, 123L, 212L, 203L, 165L, 267L, 
    550L, 761L, 998L, 1308L, 1613L, 1704L, 1669L, 1296L, 975L, 
    600L, 337L, 259L, 145L, 91L, 70L, 79L, 63L, 58L, 51L, 53L, 
    39L, 49L, 33L, 47L, 56L, 32L, 43L, 47L, 19L, 32L, 18L, 34L, 
    39L, 63L, 57L, 55L, 69L, 76L, 103L, 99L, 108L, 131L, 113L, 
    106L, 122L, 138L, 136L, 175L, 207L, 324L, 499L, 985L, 1674L, 
    1753L, 1419L, 1105L, 821L, 466L, 274L, 180L, 143L, 82L, 101L, 
    72L, 55L, 71L, 50L, 33L, 26L, 25L, 27L, 21L, 24L, 24L, 20L, 
    18L, 18L, 25L, 23L, 13L, 10L, 16L, 9L, 12L, 16L, 25L, 31L, 
    36L, 40L, 36L, 47L, 32L, 46L, 75L, 63L, 49L, 90L, 83L, 101L, 
    78L, 79L, 98L, 131L, 83L, 122L, 179L, 334L, 544L, 656L, 718L, 
    570L, 323L, 220L, 194L, 125L, 95L, 77L, 46L, 42L, 29L, 35L, 
    21L, 29L, 16L, 14L, 19L, 15L, 19L, 18L, 21L, 10L, 14L, 7L, 
    7L, 5L, 9L, 14L, 11L, 18L, 22L, 39L, 36L, 46L, 44L, 37L, 
    30L, 39L, 37L, 45L, 71L, 59L, 57L, 80L, 68L, 88L, 72L, 74L, 
    208L, 357L, 621L, 839L, 964L, 835L, 735L, 651L, 400L, 292L, 
    198L, 85L, 64L, 41L, 40L, 23L, 18L, 14L, 22L, 9L, 19L, 8L, 
    14L, 12L, 15L, 14L, 4L, 6L, 7L, 7L, 8L, 13L, 10L, 19L, 17L, 
    20L, 22L, 40L, 37L, 45L, 34L, 26L, 35L, 67L, 49L, 77L, 82L, 
    80L, 104L, 88L, 49L, 73L, 113L, 142L, 152L, 206L, 293L, 513L, 
    657L, 919L, 930L, 793L, 603L, 323L, 202L, 112L, 55L, 31L, 
    27L, 15L, 15L, 6L, 13L, 21L, 10L, 11L, 9L, 8L, 11L, 7L, 5L, 
    1L, 4L, 7L, 2L, 6L, 12L, 14L, 21L, 29L, 32L, 26L, 22L, 44L, 
    39L, 47L, 44L, 93L, 145L, 289L, 456L, 685L, 548L, 687L, 773L, 
    575L, 355L, 248L, 179L, 129L, 122L, 103L, 72L, 72L, 36L, 
    26L, 31L, 12L, 14L, 14L, 14L, 7L, 8L, 2L, 7L, 8L, 9L, 26L, 
    10L, 13L, 13L, 5L, 5L, 3L, 6L, 1L, 10L, 6L, 7L, 17L, 12L, 
    21L, 32L, 29L, 18L, 22L, 24L, 38L, 52L, 53L, 73L, 49L, 52L, 
    70L, 77L, 95L, 135L, 163L, 303L, 473L, 823L, 1126L, 1052L, 
    794L, 459L, 314L, 252L, 111L, 55L, 35L, 14L, 30L, 21L, 16L, 
    9L, 11L, 6L, 6L, 8L, 9L, 9L, 10L, 15L, 15L, 11L, 6L, 3L, 
    8L, 4L, 7L, 7L, 13L, 10L, 23L, 24L, 36L, 25L, 34L, 37L, 46L, 
    39L, 37L, 55L, 65L, 54L, 60L, 82L, 55L, 53L, 61L, 52L, 75L, 
    92L, 121L, 170L, 199L, 231L, 259L, 331L, 357L, 262L, 154L, 
    77L, 34L, 41L, 21L, 17L, 16L, 7L, 15L, 11L, 7L, 5L, 6L, 13L, 
    7L, 6L, 8L, 7L, 1L, 11L, 9L, 3L, 9L, 9L, 8L, 15L, 19L, 16L, 
    10L, 12L, 26L, 35L, 35L, 41L, 34L, 30L, 36L, 43L, 23L, 55L, 
    107L, 141L, 217L, 381L, 736L, 782L, 663L, 398L, 182L, 137L, 
    79L, 28L, 26L, 16L, 14L, 8L, 4L, 4L, 6L, 6L, 11L, 4L, 5L, 
    7L, 7L, 6L, 8L, 2L, 3L, 3L, 1L, 1L, 3L, 3L, 2L, 8L, 8L, 11L, 
    10L, 11L, 8L, 24L, 25L, 25L, 33L, 36L, 51L, 61L, 74L, 92L, 
    89L, 123L, 402L, 602L, 524L, 494L, 406L, 344L, 329L, 225L, 
    136L, 136L, 84L, 55L, 55L, 42L, 19L, 28L, 8L, 7L, 2L, 7L, 
    6L, 4L, 3L, 5L, 3L, 3L, 0L, 1L, 2L, 3L, 2L, 1L, 2L, 2L, 9L, 
    4L, 9L, 10L, 18L, 15L, 13L, 12L, 10L, 19L, 15L, 22L, 23L, 
    34L, 43L, 53L, 47L, 57L, 328L, 552L, 787L, 736L, 578L, 374L, 
    228L, 161L, 121L, 96L, 58L, 50L, 37L, 14L, 9L, 6L, 15L, 12L, 
    9L, 1L, 6L, 4L, 7L, 7L, 3L, 6L, 9L, 15L, 22L, 28L, 34L, 62L, 
    54L, 75L, 65L, 58L, 57L, 60L, 37L, 47L, 60L, 89L, 90L, 193L, 
    364L, 553L, 543L, 676L, 550L, 403L, 252L, 140L, 125L, 99L, 
    63L, 63L, 76L, 85L, 68L, 67L, 38L, 25L, 24L, 11L, 9L, 9L, 
    4L, 8L, 4L, 6L, 5L, 2L, 6L, 4L, 4L, 1L, 5L, 4L, 1L, 2L, 2L, 
    2L, 2L, 3L, 4L, 4L, 7L, 5L, 2L, 10L, 11L, 17L, 11L, 16L, 
    15L, 11L, 12L, 21L, 20L, 25L, 46L, 51L, 90L, 123L)), .Names = c(""date"", 
""cases""), row.names = c(NA, -835L), class = ""data.frame"")
</code></pre>
"
"NaN","NaN"," 63350","<p>I have some data that is bounded between 0 and 1. I have used the <code>betareg</code> package in R to fit a regression model with the bounded data as the dependent variable. My question is: how do I interpret the coefficients from the regression?</p>
"
"0.0197796943032309","0.040291148201269"," 63652","<p>I've been looking into the boot package in R and while I have found a number of good primers on how to use it, I have yet to find anything that describes exactly what is happening ""behind the scenes"". For instance, in this <a href=""http://www.ats.ucla.edu/stat/r/dae/zinbreg.htm"">example</a>, the guide shows how to use standard regression coefficients as a starting point for a bootstrap regression but doesn't explain what the bootstrap procedure is actually doing to derive the bootstrap regression coefficients. It appears there is some sort of iterative process that is happening but I can't seem to figure out exactly what is going on.</p>
"
"0.0839181358296689","0.0759737176397586"," 63681","<p>I have been adamantly searching the web to learn how to successfully implement a dynamic regression time series in the forecast package for R. The time series data that I am using is weekly data (frequency=52) of incoming call volume and prediction variables are mailers sent out every now and then. They are a significant predictor of the data for the week that they hit, the following week, and the week after that. I have created lagged variables and use these three as the predictors. </p>

<p>My main concern is that the arima model is not taking into account the time series frequency. When I tell it to recognize the ts with a frequency of 52 it has an error. 
I have looked at the <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">fortrain function</a> but do not understand it. I also have looked at the tbats suggested but found that those will not work with prediction variables. </p>

<p>The Zoo function recognizes 52 frequency but it is not advised to use with the <a href=""http://stackoverflow.com/questions/16050684/using-the-combination-forecastauto-arima"">forecast package</a>.</p>

<p>Here is the basic code. The problem is that the time series calwater[,5] is not recognized as such. It is imputed as a simple vector as an integer...</p>

<pre><code>#this works without taking into acount the ts
fit2 &lt;- auto.arima(calwater[6:96,5], xreg=calwater[6:96,6:8], d=0)
fccal &lt;- forecast(fit2, xreg=calwater[97:106,6:8], h=10)
fccal
plot(fccal, main=""Forecast Cal Water"", ylab=""Calls"")

#to form a ts object
calincall&lt;-ts(calwater[1:106,5],start=c(2011,23),frequency=52)

#once the ts is added to the model this dispalys
#Error in `[.default`(calincall, 2:100, 1) : incorrect number of dimensions
</code></pre>

<p>Maybe the error is because there is just a little over two years of data. </p>

<pre><code>#Time Series: Start = c(2011, 23), End = c(2013, 24),Frequency = 52 
</code></pre>

<p>I would be very grateful for any guidance in for this particular issue. I am using the forecast package and prefer to continue within the package but I am open to suggestions. </p>
"
"NaN","NaN"," 63698","<p>I have been checking out which diagnostics to use for a GEE analysis. It seem that influence measures are appropriate (Preisser, 1996). Does anyone know of a package that can be used in R to examine influence measures in GEE? The ""stats"" package does not work: it returns an error when I run</p>

<pre><code>influence.measures(model) 
</code></pre>

<p>on a gee model, but works fine with an ordinary linear regression. </p>
"
"0.0484501583111509","0.0493463771219827"," 63883","<p>Is there a method to find the right distance function in non-parametric regression?
I use some time series to learn forecasting. Series are nonlinear and non-gaussian.
I can get the right dimension and delay. I can find the right bandwidth with the hdrcde library.
I have no problem with kernel functions.<br>
My problem is with distances. I use Euclidian, Cosine and Correlation weighting functions.
These are the kernel which give good results, but one time, Euclidian is good, then after adding some data, one to 5-7 generally, Euclidian give nothing, even with very little change in statistics.
So, my question is if there is a method to choose the right distance. I would like to have advices on that point and on articles that will help solve this problem. What package in R could eventually help?</p>

<p>Thank you. </p>
"
"0.027972711943223","0.0284901441149095"," 64130","<p>I want to include sample weights to my quantile regression model, but I'm not sure how to do this.  </p>

<p>I've already define my weight, which are replicated weights already given in survey dataset (computed in survey package): </p>

<pre><code>w&lt;-svrepdesign(variables=data[,1:10],repweights=data[,11:30],type=""BRR"", 
  combined.weights=TRUE, weights=r.weights, rho=0.5,dbname="""")
</code></pre>

<p>and my rq model is:</p>

<pre><code>rq(y~x,tau=c(.1,.2,.3,.4,.5,.6,.7,.8,.9),data=my.data))
</code></pre>

<p>I tried to use <code>withReplicates</code> function, but with no success. Any suggestions? </p>
"
"0.10833784750436","0.110341853688094"," 64535","<p>My ecological question is: ""What are the trends in percent coral cover by island and depth across the state of Hawaii from 1999 to 2012?""  </p>

<p>I am trying to analyze this hierarchical data set using R with 10 transects at each depth, 2 depths per site, and site nested in island.</p>

<p>Data structure:</p>

<pre><code>Fixed effects:
 Island: Hawaii, Maui, Molokai, Kahoolawe, Oahu, Kauai.
 DepthCat: S = Shallow, D = Deep.
 WYear: 0-13. It was suggested that I use this factor as a covariate for years.

Random effects:
 Site: 34 sites across the 6 islands with 2 depths per site.
 Transect: 10 permanent transects at each depth.
 Year: 1999 â€“ 2012 (14 years)

Dependent variable: PercentCover
</code></pre>

<p>Currently, I am using the <code>lmer</code> function in the <code>lmerTest</code> package and this is the model that I've constructed.</p>

<pre><code>fit1 &lt;- lmer(PercentCover ~ WYear*Island*DepthCat +
             (1+WYear|Island/Site/DepthCat/Transect) + (1|Year), data=Benthic)
</code></pre>

<p>Unfortunately, the data are spotty (i.e., missing data in multiple years for a number of sites) so the model returns <code>[1] ""Asymptotic covariance matrix A is not positive!""</code>, even using arcsin transformed data. I can still run the summary statistics to get results, but I don't feel comfortable with the error message. Perhaps I have not structured the model correctly in terms of organizing the nested factors, but the number of observations for each of the levels in the summary stats seems correct. I tried different and simpler iterations of the model such as:</p>

<pre><code> fit1 &lt;- lmer(PercentCover ~ WYear + Island + DepthCat + (1+WYear|Transect/Site) + 
              (1|Year), data=Benthic)
</code></pre>

<p>which works, but doesn't give me the interaction information and returns a larger AIC suggesting that the model does not fit the data as well.</p>

<p>To deal with all of the missing data, I tried another approach by using the regression slope of percent cover over time as the dependent variable for each site X depth combination.</p>

<pre><code>Data structure:

Fixed effects:
 Island: Hawaii, Maui, Molokai, Kahoolawe, Oahu, Kauai.
 DepthCat: S = Shallow, D = Deep.

Random effects:
 Site: 34 sites across the 6 islands with 2 depths per site.
 Transect: 10 permanent transects at each depth.

Dependent variable: Trend
</code></pre>

<p>I used the following model, but the summary results did not make much sense, even after transforming the data.</p>

<pre><code> fit1&lt;-lmer(Trend ~ Island*DepthCat + (1| Island/Site/DepthCat/Transect), data=Benthic)
</code></pre>

<p>Any suggestions on improving my analytical approach would be appreciated.</p>
"
"0.0559454238864459","0.056980288229819"," 65356","<p>I am interested to know, how to get the coefficient from VGAM package (generalized Poisson regression), because, when I want to get the coefficient, is just can to get the estimate with @ to get the estimate. An other coefficient can to get all of the part from coefficient with ""$""...</p>

<p>My program like this:</p>

<pre><code>library(""VGAM"")  
fit &lt;- vglm(y ~ x1, genpoisson,trace = TRUE)  
summary(fit)  
hasilgp &lt;- summary(fit)  
koefgp &lt;- hasilgp@coefficients  
</code></pre>

<p>And only the estimate can to attach...
Any one help me...</p>
"
"0.0395593886064618","0.040291148201269"," 65375","<p>I have done an ordinal regression using <code>lrm</code> from the <code>rms</code> package.</p>

<pre><code>load(url(""http://www.produnis.de/R/NEXT-Q0-OR.RData""))
require(rms)
mydata &lt;- NEXT.or
fit &lt;- lrm(QuitJob~QD,data=mydata);fit
</code></pre>

<p>I would like to predict the rank in ""QuitJob"" (1=won't quit ... 5=will quit very soon) by giving a value for ""QD"" (how much work to do). For example, if ""QD"" is 78, what rank of ""QuitJob"" is likely?</p>

<p>If I type in </p>

<pre><code>predict.lrm(fit, data.frame(QD=78))
</code></pre>

<p>I get</p>

<pre><code>        1 
0.3191933 
</code></pre>

<p>Do I understand it right, that the probability of being in rank 1 is 31,92% ?!
If so, how can I get the probabilities for ranks 2-5 ?</p>
"
"0.027972711943223","0.0284901441149095"," 66239","<p>Is there any way to calculate variable importance in R for SVM regression and averaged neural networks?</p>

<p>I've been using <strong>caret</strong> package, that has <strong>varImp</strong> function in it</p>

<pre><code>&gt; m &lt;- best.tune(svm, train.x = descr[rownames(tr[[i]]),2:ncol(descr)], 
train.y = tr[[i]][,1],  data = df, cost = 2^(seq(0,10,5))), 
tunecontrol = tune.control(sampling = ""cross""))
&gt; varImp(m)
 Error in UseMethod(""varImp"") : 
 no applicable method for 'varImp' applied to an object of class ""svm""
</code></pre>

<p><a href=""https://stat.ethz.ch/pipermail/r-help/2010-October/257627.html"" rel=""nofollow"">According to the developer</a>, this approach wasn't realized for SVM method </p>

<p>However, <strong>rminer</strong>  package suggests such function as <em>Importance</em>. Though, it throws an error:</p>

<pre><code>VariableImportance = Importance(svmFit, data=descr[rownames(tr[[i]]), 2:ncol(descr)],    
                                method=""1D-SA"")
Error in Importance(svmFit, data = descr[rownames(tr[[i]]), 2:ncol(descr)],  : 
duplicate 'switch' defaults: 'lm == func...' and 'NULL'
</code></pre>
"
"0.027972711943223","0.0284901441149095"," 66279","<p>Has anyone written a package in <code>R</code> to calculate diagnostic plots after <code>clogit</code>, conditional logistic regression? e.g. leverage. Or a related question, how do you stratify using <code>glm</code> (perhaps I can stratify using <code>glm</code> and <code>family =binomial</code>, and then use diagnostic packages for <code>glm</code>?)</p>
"
"0.0559454238864459","0.0427352161723642"," 66419","<p>Is it a good or a bad practice to use R packages from CRAN for research? I'm talking about the common packages like: simple models for regression, estimation, econometrics.<br>
Most of them use function that can be written easily on your own.</p>

<p>My yes arguments:</p>

<ul>
<li>Time to focus on the main part of the research</li>
<li>The community is a good quality control</li>
<li>R offers a lot of modern methods</li>
<li>Some models are too complicated to write them on your own</li>
</ul>

<p>My con arguments:</p>

<ul>
<li>Not every package of R has been created in an academic environment</li>
<li>There could be bugs that influence the outcome and I do not know it</li>
<li>Most models can be written in a short time period from scratch</li>
</ul>

<p>How can we use open source without risking failures in the outcome? Are there certain quality indicators for packages in general and for R?</p>
"
"NaN","NaN"," 67157","<p>Is there a free alternative available for the Stata procedure <a href=""http://www.stata.com/manuals13/mimiimputeintreg.pdf"" rel=""nofollow"">mi impute intreg</a> (Impute using interval regression)? For example as an R package. I have not found any yet.</p>
"
"0.0625488854200668","0.0637058989297032"," 67209","<p>I'm a novice attempting to predict automobile sales using a combination of previous sales (seasonal AR model), macroeconomic indicators such as CPI, consumer sentiment index etc. and more significantly, I'm using weekly data from google search trends for the automobile and other variables from the category (automobile) in google trends. (code developed using Choi and Varian's 2009 paper). The model essentially looks at search trend indices for three weeks before the month in question</p>

<p>In addition to running spike and slab regression to determine attribute importance, I also ran a gradient boosted machine (R package GBM). as of now, the model seems to be performing fairly satisfactorily, with the Mean Average Error = 5.4 (monthly sales are in the range of 10000), however there are clear seasonal trends in the difference between predicted variables and actual sales that seem to coincide with sales promotions and other promotional events. My question, therefore is:</p>

<p>How do I account for these seasonal trends while using GBM? or for a linear model?</p>
"
"0.0395593886064618","0.040291148201269"," 67257","<p>I have difficulties fitting a joint model in <code>R</code>. My data consists of two responses <code>X</code> &amp; <code>Y</code> and one predictor variable <code>Z</code>. Now I want to model both <code>X</code> and <code>Y</code> in function of <code>Z</code> (just linear regression: $E(X|Z)=Z\alpha$ and $E(Y|Z)=Z\beta$, both outcomes are normally distributed) but while doing so I also want to estimate the variance covariance matrix since it is the correlation between <code>X</code> and <code>Y</code> that I am interested in.</p>

<p>I already looked into a couple of functions (<code>lm</code>, <code>mcer</code>, <code>lme</code>) but it doesn't seem to do the trick. Is there something that I am overlooking in a certain package or a new suggestions to try?</p>
"
"0.0559454238864459","0.056980288229819"," 67312","<p>I am trying to analyse a dataset with at minimum 50 explanatory variables coded as 0 and 1 for presence/absence and a binary response variable (case/control). The goal is to see how the variables can predict the separation between case and control.</p>

<p>As there are more variables than observations I applied a partial least square discriminant analysis (PLS-DA) using the package mixOmics in R. However, when I want to test the significance of the analysis with PLSDA.test (package RVAideMemoire) I get a lot of warnings :</p>

<pre><code>1: In pls(X, ind.mat, ncomp = ncomp, mode = ""regression"",  ... :
 Zero- or near-zero variance predictors. 
 Reset predictors matrix to not near-zero variance predictors.
 See $nzv for problematic predictors.
</code></pre>

<p>I guess the problem with near-zero variance results from the 0/1 coding of the predictor variables. I tried to convert the variables to factors, but this doesn't help. Is there a different analysis more suitable? How can I deal with presence/ absence variables as predictors?</p>
"
"NaN","NaN"," 67363","<p>I am running a Bayesian regression model by WinGUBS via R2WinBUGS package in R. Everything looks fine except for one parameter:</p>

<pre><code>nlssim$summary[37,][c(1,2,8,9)]
    mean           sd         Rhat        n.eff 
3.054326e-05 9.523965e-06 1.000000e+00 1.000000e+00 
</code></pre>

<p>The number of effective size is 1! It seems to indicate that the autocorrelation between samples are extremely large, but the trace plot looks all fine. And I then try the effectiveSize function in code package and the result is 4590.</p>

<p>I am curious how WinBUGS calculate the n.eff statistic, and if n.eff=1 is symptom of some of my mistakes?</p>

<p>I run 3 chains in parallel, and each chain has 60000 iterations. n.burnin=200, n.thin=30</p>

<p>Thank you very much in advance!</p>
"
"0.0740088392978143","0.0753778361444409"," 67385","<p>I am trying to model <strong>count data in R that is apparently underdispersed</strong> (Dispersion Parameter ~ .40). This is probably why a <code>glm</code> with <code>family = poisson</code> or a negative binomial (<code>glm.nb</code>) model are not significant. When I look at the descriptives of my data, I don't have the typical skew of count data and the residuals in my two experimental conditions are homogeneous, too. </p>

<p>So my questions are:</p>

<ol>
<li><p><strong>Do I even have to use special regression analyses for my count data, if my count data doesn't really behave like count data?</strong> I face non-normality sometimes (usually due to the kurtosis), but I used the percentile bootstrap method for comparing trimmed means (Wilcox, 2012) to account for non-normality. Can methods for count data be substituted by any robust method suggested by Wilcox and realized in the WRS package?</p></li>
<li><p><strong>If I have to use regression analyses for count data, how do I account for the under-dispersion?</strong> The Poisson and the negative binomial distribution assume a higher dispersion, so that shouldn't be appropriate, right? I was thinking about applying the <strong>quasi-Poisson</strong> distribution, but that's usually recommended for over-dispersion. I read about <strong>beta-binomial</strong> models which seem to be able to account for over- as well as underdispersion are availabe in the <code>VGAM</code> package of R. The authors however seem to recommend a <strong>tilded Poisson distribution</strong>, but I can't find it in the package. </p></li>
</ol>

<p><strong>Can anyone recommend a procedure for underdispersed data and maybe provide some example R code for it?</strong></p>
"
"0.0395593886064618","0.040291148201269"," 67473","<p>I'm using <a href=""http://cran.r-project.org/web/packages/kernlab/index.html"" rel=""nofollow"">kernlab package</a></p>

<p>Here are two examples:<br/>
First:</p>

<pre><code>library(kernlab)
x &lt;- runif(1020, 1, 5000)
y &lt;- sqrt(x)
model.vanilla &lt;- rvm(x, y, kernel='vanilladot')
</code></pre>

<p>Got error:</p>

<pre><code>Error in chol.default(crossprod(Kr)/var + diag(1/thetatmp)) :
the leading minor of order 2 is not positive definite
</code></pre>

<p>Second:</p>

<pre><code>library(kernlab)
x &lt;- runif(1020, 1, 5000)
y &lt;- sqrt(x)
model.rbf &lt;- rvm(x[1:1000], y[1:1000], kernel='rbfdot')
print(model.rbf)
py.rbf &lt;- predict(model.rbf, x[1001:1020])
print(paste(""MSE: "", sum((py.rbf - y[1001:1020]) ^ 2) / length(py.rbf)))
</code></pre>

<p>OK:</p>

<pre><code>Using automatic sigma estimation (sigest) for RBF or laplace kernel 
Relevance Vector Machine object of class ""rvm"" 
Problem type: regression 

Gaussian Radial Basis kernel function. 
 Hyperparameter : sigma =  5.44268665122008e-06 

Number of Relevance Vectors : 247 
Variance :  4.368e-06
Training error : 3.418e-06 
[1] ""MSE:  4.921706631013e-05""
</code></pre>

<p>Why doesn't using linear kernel work here? <code>polydot</code> (polynomial kernel function) doesn't work either.</p>

<p>Can this be fixed?</p>
"
"0.027972711943223","0.0284901441149095"," 67484","<p>I've got a set of input-output vector pairs, and I want to find a function that approximates the output vectors from the input vectors. Specifically, I want a <em>matrix</em> by which to multiply an input vector such that I get a good approximation for the output.</p>

<p>I guess <em>linear</em> regression is not what I'm looking for, or else I wouldn't know what to do with the linear function. So what sort of regression do I need to apply here? Literature tips and / or Java / Python / R packages are very welcome!</p>
"
"0.0395593886064618","0.040291148201269"," 67662","<p>I am a research scholar and monitoring phenological events of timber line at a Himalayan region from past 4 years. During data analysis I found a research paper ""Estimation and comparison of flowering curve"" similar to my work.</p>

<p>In this paper the <code>R</code> package <code>bbmle</code> was used and five parameters $(\beta_{0},\ldots,\beta_{4})$ which describe: (i) the height; (ii) the peak date; (iii) the range; (iv) the symmetry; and (v) the peakedness of the regression curve were calculated. I also read the appendix table and followed the code to estimate these parameter but as a newbie I failed to do the calculations.</p>

<p>The full appendix table is here:
<img src=""http://i.stack.imgur.com/wOkDn.jpg"" alt=""Appendix table""></p>

<p>I am also posting the <code>startvals</code> for the year 2007 as mentioned in the appendix. Look at the values in figure below for year 2007.
<img src=""http://i.stack.imgur.com/ZPtKH.jpg"" alt=""Start values for year 2007""></p>

<p>My main problem is how to calculate <code>startvals</code> for example data of year 2007 as shown in figure where $(\beta_{0},\ldots,\beta_{4})$ describe: </p>

<ol>
<li>the height</li>
<li>the peak date</li>
<li>the range</li>
<li>the symmetry</li>
<li>the peakedness</li>
</ol>

<p>of the regression curve.</p>
"
"NaN","NaN"," 67680","<p>I am using a <a href=""http://en.wikipedia.org/wiki/Relevance_vector_machine"" rel=""nofollow"">relevance vector machine</a> in R, <a href=""http://rss.acs.unt.edu/Rdoc/library/kernlab/html/rvm.html"" rel=""nofollow"">rvm()</a>, to solve a regression problem. I need to know the variance of the fitted values for each identified RVs. Does anyone know how to program to compute the prediction variances with R's <a href=""http://cran.r-project.org/web/packages/kernlab/index.html"" rel=""nofollow"">Kernlab package</a>?</p>
"
"0.183476730565529","0.182623576931816"," 67873","<p><strong>TLDR</strong>: How can I perform inference for the between group differences in a possibly logistic growth with time in the presence of outliers, unequal measurement times and frequency, bounded measurements and possible random effects on individual and per study level?</p>

<p>I am attempting to analyse a dataset where measurements for individuals were made at different time points. Measurements start low at time 0 and follow (very roughly) a logistic growth pattern with time. I am trying to establish if there are differences between two groups of individuals. The analysis is complicated by the following factors:</p>

<ul>
<li>The effect of time is non-linear, so either a non-linear logistic regression (biologically plausible, but not particularly well fitting) or a non-parametric regression seem appropriate</li>
<li>There are massive outliers, so regression using the sum of squared residuals seems off the table. Quantile regression seems appropriate.</li>
<li>Random effects may be appropriate on a per individual and per study level. Mixed effects models seems appropriate.</li>
<li>Measurement times, number of available measurements and end of monitoring differ between individuals. Survival analysis techniques seem appropriate. Possibly also applying weights equal to 1 / number of observations for individual.</li>
<li>Measurements are bounded below at 0 and while there is no obvious boundary above, arbitrarily high measurements seem biologically implausible. However, quite a few individuals have some measurements of zero (partly due to the measurement accuracy of the device).</li>
<li>A few models I tried so far failed to fit, usually with an unhelpful error related to the numerical procedure. This leads me to believe that I will need a reasonably robust method able to deal with this somewhat ugly dataset.</li>
<li>Finally, I want to produce inference of the form ""group 1 has faster growth than group 2"" or ""group 1 has a higher asymptotic level than group 2"".</li>
</ul>

<p>What I have tried so far (all in R) - I was aware that most of the below are not particularly appropriate for the dataset, but I wanted to see which models could actually be fitted without numerical errors:</p>

<ul>
<li>Non-parametric regression using crs in the crs package. Nicely produces a curve reasonably close to logistic growth for most of the time period with some strange behavious toward the end of the monitoring period (where there are fewer observations). Using individuals as fixed effects reveals some outliers. Using the variable of interest as fixed effects shows some difference. However, I am not sure if there is any way to assess fits and do inference on a model this complex.</li>
<li>Non-linear mixed effects regression using nlme in package nlme and SSlogis. Gradually building up the model with update() works reasonably well. Getting too complex with the fixed effects or the random effects leads to convergence failure. Since there is no reason to believe that residuals are normally distributed, it seems a bad idea to pursue this further. Edit: I have recently become aware that it is possible to specify autocorrelated residuals in nlme. However, at the moment it seems I cannot even get fixed weights to work. Advice on the correct syntax is welcome.</li>
<li>Non-linear mixed effects regression using nlmer in package LME4 and a custom likelihood for the logistic growth model. Works fairly well, but standard errors on the fixed effects get massive, probably due to the outliers. I also have the slight suspicion that some of the models fail to fit without error, as I sometimes get tiny random effects (about 10^10 smaller than with slightly simpler models). Since there is no reason to believe that residuals are normally distributed, it seems a bad idea to pursue this further.</li>
<li>Non-linear quantile regression using nlrq in package quantreg and SSlogis. Fits reliably and quickly, but percentile lines intersect. This means that an area containing 90% of the data is not fully contained in an area containing 95% of the data.</li>
<li>Non-parametric quantile regression using the LMS method with package VGAM. Even trivial models failed with obscure errors using this dataset. I believe the number of zeros in the dataset and / or the large range of the data while also getting close to zero may be an issue.</li>
<li>To complete this list, I should probably also mention the lqmm package for Linear Quantile Mixed Models, which I have not used yet. While the package cannot use non-linear models as far as I know, transforming the time variable may produce something reasonably close.</li>
</ul>

<p>I would appreciate feedback if these or any other method might be used to produce reasonably robust inference in this scenario. Maybe regression is not needed at all and another, possibly simpler method is sufficient. I'd be happy to provide an example dataset, if required, but think this question might also be of interest beyond the current dataset.</p>
"
"0.027972711943223","0.0284901441149095"," 68181","<p>I am trying to implement <em>all-possible regressions</em> in order to select the best predictors of stock returns from an exhaustive list of potential economic/fundamental variables.</p>

<p>My response variable <em>y</em> (i.e. stock returns) is a panel of 3000 securities (cross-section), each having 384 observations (time-series).</p>

<p>Would anyone please suggest the best way to handle this procedure in R, in the context of panel data? I came across the package <code>leaps</code>, but it addresses the case of <em>y</em> as a response <strong>vector</strong> rather than a response <strong>matrix</strong>.</p>

<p>Thank you very much,</p>
"
"0.0395593886064618","0.040291148201269"," 68183","<p>I was asked to do some regression analysis on anonymised data, which I don't have yet. It will have a few categorical inputs and ~100 boolean outputs, something like this:</p>

<pre><code>Category   Type         Size       Blood pressure     Y1   Y2    Y3  ....   Y99  
-------------------------------------------------------------------------------
cat        persian      small      high               1    0     0   ....   1
cat        persian      big        low                0    1     1   ....   0
dog        wolfhound    big        normal             1    1     0   ....   0
...        ...          ...        ...                      ......   
duck       scoter       small      above normal       0    1     1   ....   1  
</code></pre>

<p>(I just made up the meanings here). The number of samples in the real data set will be about a million, without missing values. The task is to predict the output, eg <code>P(Y86 = 1 | {cat, siamese, big, low})</code> and generally understand the relationship between inputs and outputs. </p>

<p>My questions: </p>

<ol>
<li>which approaches are worth trying, what are the pros/cons?</li>
<li>which <code>R</code> packages can be of help?</li>
</ol>
"
"0.0484501583111509","0.0493463771219827"," 68351","<p>I have a question regarding to the concept of robust standard errors. What I found about that topic is, that one can estimate the robust standard error for regression coefficients to eliminate problems with heteroscedasticity (when one wants to interpret a model). I want to know if there is a way not only to determine robust standard errors of coefficients but also of the standard error of the overall regression (residual standard error). When its possible, how can I calculate such a value in general?</p>

<p>Because I'm using R its also interesting for me if there is a <code>R-function</code> for this problem (I only know the <code>sandwich-package</code> for the normal robust SE of the coefficients).</p>

<p>Thanks.</p>
"
"0.0791187772129236","0.080582296402538"," 68553","<p>I am trying to run a logistic regression analysis in R using the speedglm package. 
The data is CNVs (a type of genetic variant), whether that CNV occurred in a case or control and wether genes in a pathway is hit by the CNV or not (Pathway.hit), and how many genes were hit by the CNV that were not in the pathway (Pathway.out).
I run two models with and without the Pathway.hit covariate and compare to see if a pathway is preferentially hit by cases vs controls.  </p>

<p>the models and comparison of said are as follows:</p>

<pre><code>fit1 = speedglm(status~size+Pathway.out, data=cnvs, family=binomial('logit'))
fit2 = speedglm(status~size+Pathway.out+Pathway.hit, data=cnvs,family=binomial('logit'))
P.anova = 1-pchisq(abs(fit1$deviance - fit2$deviance), abs(fit1$df - fit2$df))
</code></pre>

<p>It seems to work okay for most data I throw at it, but in a few cases I get the error:</p>

<pre><code>Error in solve.default(XTX, XTz, tol = tol.solve) : 
  system is computationally singular: reciprocal condition number = 1.87978e-16
</code></pre>

<p>After some googling around I think I found what's causing the problem:</p>

<pre><code>by(cnvs$Pathway.hit, cnvs$status, summary)
cnvs$status: 1 (controls)
        0     1 
    13333     0 
    ------------------------------------ 
    cnvs$status: 2 (cases)
    0     1 
10258     2 
</code></pre>

<p>So here there no observations in controls and only 2 in cases. </p>

<p>If I use with normal glm method however, then it does not throw an error (but that of course doesn't necessarily mean the results will be meaningful). The reason I am using the speedglm package is that I have approximately 16,000 of these analyses to run, and using the base glm function for all 16,000 takes about 20 hours, where as I think speedglm can reduce it down to 8 or so.</p>

<p>So my question is, should I ignore those analyses which throw an error and list the results as NA as there were too few observations, or when speed glm fails should I retry with normal glm? In the above example there are 2 observations of the covariate in cases, but 0 in controls. Might this not be interesting? Would the analysis also fail if there were 0 in controls and 20 in cases - that would certainly be interesting would it not?</p>

<p>Thanks for the help in advance,
Cheers,
Davy</p>
"
"0.111890847772892","0.113960576459638"," 68786","<p>I measured a binary response for each subject in 5 different conditions. For each subject and condition, I replicated the experiment 36 times. I thus have 36 binary values per condition per subject.</p>

<p>I am trying to build a model for those data. I suppose a logistic regression is what I'm looking for, and I am working with the <code>lmer</code> package. My aim is to check whether the conditions significantly influence the observed values, so I would have two models:</p>

<pre><code>lmH1&lt;-lmer(value~condition, (random effects), data=dataset, family=binomial)
</code></pre>

<p>and</p>

<pre><code>lmH0&lt;-lmer(value~1, (random effects), data=dataset, family=binomial) 
</code></pre>

<p>By looking at the output from <code>anova(lmH0, lmH1)</code>, I would be able to determine the significance of the effect of my condition.</p>

<p>I am just not sure what to specify as random effect; the models I defined so far are:</p>

<pre><code>lmH1 &lt;- lmer( value ~ condition + ( 1 | subject ), data = dataSet, family = binomial )
</code></pre>

<p>and </p>

<pre><code>lmH2 &lt;- lmer( value ~ condition + ( 1 | subject/condition ), data = dataSet, family = binomial )
</code></pre>

<p>However I am not sure about how lmer handles the replicates, so I don't know whether I should include those replicates in my random effects or not. I could modify the proposed models so that the grouping defined by the random effects refers to a specific binary values instead of a group of binaries values. My new models would then be</p>

<pre><code>lmH1a &lt;- lmer( value ~ condition + ( 1 | subject/(condition:replicate) ), data = dataSet, family = binomial )
</code></pre>

<p>and</p>

<pre><code>lmH2a &lt;- lmer( value ~ condition + ( 1 | subject/condition/replicate ), data = dataSet, family = binomial )
</code></pre>

<p>With those models R returns the warning message <code>Number of levels of a grouping factor for the random effects is equal to n, the number of observations</code>. But the model is still computed.</p>

<p>All 4 models return very similar values for the fixed effects and for the random effects that they have in common (e.g. the subject random effects are very similar for all 4 models and the condition within subject random effects are very similar for <code>lmH2</code> and <code>lmH2a</code>).</p>

<p>How can I check which random effect structure is the most appropriate for my design and collected data?</p>
"
"0.0740088392978143","0.0753778361444409"," 69906","<p>Is there a way/method/approach to decompose a time series data using regression splines:</p>

<ol>
<li>Seasonal time series into trend+seasonal+random component ?</li>
<li>A non seasonal time series into trend+random component ?</li>
</ol>

<p>I'm familiar with STL, Census and classical decomposition in R. All these techniques require time series data with seasonal component. We cannot extract trend if the time series is non seasonal (i.e., Frequency = 1). </p>

<p>I recently came across this interesting article which is data driven in the recent 2013 ISF. Any insights on methods like these that are data driven decomposition using regression splines and that can be readily programmed in software packages such R would be greatly helpful.</p>

<p>Thanks so much</p>

<p><strong>Detrending time series with cycle and seasonal components</strong>
<em>Tatyana Krivobokova and Francisco Rosales</em>
In this work we discuss a nonparametric and completely data-driven approach to the decomposition of time series into a trend (cycle), seasonal and random components. Two former are modeled with penalized splines, while the latter is assumed to follow an ARMA structure. Empirical Bayesian approach allows to estimate both smoothing parameters and the orders of the ARMA process simultaneously resulting in an efficient, fast and data-driven decomposition procedure. The practical relevance of the approach is illustrated by real-data examples. The work is the extension of Kauermann, G., Krivobokova, T., Semmler, W. (2011) Filtering time series with penalized splines. <em>Studies in Nonlinear Dynamics &amp; Econometrics.</em></p>
"
"0.0500391083360534","0.0637058989297032"," 69949","<p>To use the scores of the extracted components/factors in a further regression analysis, like mixed effects model regression as predictors to an outcome variable or DV. Would be there any discrepancies in the results coming out of the regression analysis when using scores of the following scenarios (<code>psych</code> package in R):<br>
- <code>none</code> non-rotated principal components<br>
- <code>varimax</code> orthogonally rotated principal components<br>
- <code>promax</code> obliquely rotated principal components<br>
- <code>promax</code> obliquely rotated factors using <code>ml</code> (maximum likelihood) extraction<br>
- <code>promax</code> obliquely rotated factors using <code>pa</code> (principal axes) extraction? </p>

<p>Would it be invalid to use any of the above scores in a further regression analysis? any known issues in this field? or previous similar experiences?    </p>
"
"0.119908024219818","0.122126052614799"," 69957","<p>Here's my issue of the day:</p>

<p>At the moment I'm teaching myself Econometrics and making use of logistic regression. I have some SAS code and I want to be sure I understand it well first before trying to convert it to R. (I don't have and I don't know SAS). In this code, I want to model the probability for one person to be an 'unemployed employee'. By this I mean ""age"" between 15 and 64, and ""tact"" = ""jobless"". I want to try to predict this outcome with the following variables: sex, age and idnat (nationality number). (Other things being equal).</p>

<p>SAS code :</p>

<pre><code>/* Unemployment rate : number of unemployment amongst the workforce */
proc logistic data=census;
class sex(ref=""Man"") age idnat(ref=""spanish"") / param=glm;
class tact (ref=first);
model tact = sex age idnat / link=logit;
where 15&lt;=age&lt;=64 and tact in (""Employee"" ""Jobless"");
weight weight;
format age ageC. tact $activity. idnat $nat_dom. inat $nationalty. sex $M_W.;

lsmeans sex / obsmargins ilink;
lsmeans idnat / obsmargins ilink;
lsmeans age / obsmargins ilink;
run;
</code></pre>

<p>This is a sample of what the database should looks like :</p>

<pre><code>      idnat     sex     age  tact      
 [1,] ""english"" ""Woman"" ""42"" ""Employee""
 [2,] ""french""  ""Woman"" ""31"" ""Jobless"" 
 [3,] ""spanish"" ""Woman"" ""19"" ""Employee""
 [4,] ""english"" ""Man""   ""45"" ""Jobless"" 
 [5,] ""english"" ""Man""   ""34"" ""Employee""
 [6,] ""spanish"" ""Woman"" ""25"" ""Employee""
 [7,] ""spanish"" ""Man""   ""39"" ""Jobless"" 
 [8,] ""spanish"" ""Woman"" ""44"" ""Jobless"" 
 [9,] ""spanish"" ""Man""   ""29"" ""Employee""
[10,] ""spanish"" ""Man""   ""62"" ""Retired"" 
[11,] ""spanish"" ""Man""   ""64"" ""Retired"" 
[12,] ""english"" ""Woman"" ""53"" ""Jobless"" 
[13,] ""english"" ""Man""   ""43"" ""Jobless"" 
[14,] ""french""  ""Man""   ""61"" ""Retired"" 
[15,] ""french""  ""Man""   ""50"" ""Employee""
</code></pre>

<p>This is the kind of result I wish to get :</p>

<pre><code>Variable    Modality    Value   ChiSq   Indicator
Sex         Women       56.6%   0.00001 -8.9%
            Men         65.5%       
Nationality 
            1:Spanish   62.6%       
            2:French    51.2%   0.00001 -11.4%
            3:English   48.0%   0.00001 -14.6%
Age 
            &lt;25yo       33.1%   0.00001 -44.9%
        Ref:26&lt;x&lt;54yo   78.0%       
            55yo=&lt;      48.7%   0.00001 -29.3%
</code></pre>

<p>Indicator is P(category)-P(ref)
(I interpret the above as follows: other things being equal, women have -8.9% chance of being employed vs men and those aged less than 25 have a -44.9% chance of being employed than those aged between 26 and 54).</p>

<p>So if I understand well, the best approach would be to use a binary logistic regression (link=logit). This uses references ""male vs female""(sex), ""employee vs jobless""(from 'tact' variable)... I presume 'tact' is automatically converted to a binary (0-1) variable by SAS.</p>

<p>Here is my 1st attempt in R. I haven't check it yet (need my own PC) :</p>

<pre><code>### before using glm function 
### change all predictors to factors and relevel reference
recens$sex &lt;- relevel(factor(recens$sex), ref = ""Man"")
recens$idnat &lt;- relevel(factor(recens$idnat), ref = ""spanish"")  
recens$tact &lt;- relevel(factor(recens$tact), ref = ""Employee"")
recens$ageC &lt;- relevel(factor(recens$ageC), ref = ""Ref : De 26 a 54 ans"")

### Calculations of the probabilities with function glm, 
### formatted variables, and conditions with subset restriction to ""from 15yo to 64""
### and ""employee"" and ""jobless"" only.
glm1 &lt;- glm(activite ~ sex + ageC + idnat, data=recens, weights = weight, 
            subset= recens$age[(15&lt;= recens$age | recens$age &lt;= 64)] 
            &amp; recens$tact %in% c(""Employee"",""Jobless""), 
            family=quasibinomial(""logit""))
</code></pre>

<p>My questions :</p>

<p>For the moment, it seems there are many functions to carry out a logistic regression in R like glm which seems to fit.</p>

<p>However after visiting many forums it seems a lot of people recommend not trying to exactly reproduce SAS PROC LOGISTIC, particularly the function LSMEANS. Dr Franck Harrel, (author of package:rms) for one.</p>

<p>That said, I guess my big issue is LSMEANS and its options Obsmargins and ILINK. Even after reading over its description repeatedly I can hardly understand how it works.</p>

<p>So far, what I understand of Obsmargin is that it respects the structure of the total population of the database (i.e. calculations are done with proportions of the total population). ILINK appears to be used to obtain the predicted probability value (jobless rate, employment rate) for each of the predictors (e.g. female then male) rather than the value found by the (exponential) model?</p>

<p>In short, how could this be done through R, with lrm from rms or lsmeans?</p>

<p>I'm really lost in all of this. If someone could explain it to me better and tell me if I'm on the right track it would make my day.</p>

<p>Thank you for your help and sorry for all the mistakes my English is a bit rusty.</p>

<p>Binh</p>
"
"0.0484501583111509","0.0493463771219827"," 70174","<p>I have three variables, a factor (<code>c</code>) as the dependent variable and two ordinal independent variables (<code>a, b</code>). Each variable has five categories (<code>1,2,3,4,5</code>). Thus, I fitted a multinomial logistic regression (<code>testus</code>, see below) with the <code>car</code> package. Now I would like to get the predicted probabilities. </p>

<pre><code>&gt; a &lt;- sample(5,100,TRUE)
&gt; b &lt;- sample(5,100,TRUE)
&gt; c &lt;- sample(5,100,TRUE)
&gt; required(car)
&gt; a &lt;- as.ordered(a)
&gt; b &lt;- as.ordered(b)
&gt; c &lt;- as.factor(c)
&gt; testus        &lt;- multinom(c~a+b)
&gt; predictors    &lt;- expand.grid(b=c(""1"",""2"",""3"",""4"",""5""), a=c(""1"",""2"",""3"",""4"",""5""))
&gt; p.fit         &lt;- predict(testus, predictors, type='probs')
&gt; probabilities &lt;- data.frame(predictors,p.fit)
</code></pre>

<p>Now I got the predicted probabilities for a under b and c. </p>

<pre><code>&gt; head(probabilities)
  b a         X1         X2         X3         X4        X5
1 1 1 0.10609054 0.22599152 0.20107167 0.21953158 0.2473147
2 2 1 0.20886614 0.27207108 0.08613633 0.18276394 0.2501625
3 3 1 0.17041268 0.24995975 0.16234240 0.13111518 0.2861700
4 4 1 0.23704078 0.21179521 0.08493274 0.03135092 0.4348804
5 5 1 0.09494071 0.09659144 0.24162612 0.21812449 0.3487172
6 1 2 0.14059489 0.17793438 0.29272452 0.26104833 0.1276979`
</code></pre>

<p>The first two columns show the categories of the independent variables <code>a</code> and <code>b</code>. The next five columns show the conditional probabilities (e.g., $P(c=1|b=1~\&amp;~a=1) = 0.10609$. But now I would like to know only the predicted probabilities for <code>c</code> under <code>a</code> or the predicted probabilities for <code>c</code> under <code>b</code>. Is this possible?</p>
"
"0.0791187772129236","0.0705095093522208"," 70249","<p>I would like to use GLM and Elastic Net to select those relevant features + build a linear regression model (i.e., both prediction and understanding, so it would be better to be left with relatively few parameters). The output is continuous. It's $20000$ genes per $50$ cases. I've been reading about the <code>glmnet</code> package, but I'm not 100% sure about the steps to follow:</p>

<ol>
<li><p>Perform CV to choose lambda:<br>
<code>cv &lt;- cv.glmnet(x,y,alpha=0.5)</code><br>
<strong>(Q1)</strong> given the input data, would you choose a different alpha value?<br>
<strong>(Q2)</strong> do I need to do something else before build the model?</p></li>
<li><p>Fit the model:<br>
<code>model=glmnet(x,y,type.gaussian=""covariance"",lambda=cv$lambda.min)</code><br>
<strong>(Q3)</strong> anything better than ""covariance""?<br>
<strong>(Q4)</strong> If lambda was chosen by CV, why does this step need <code>nlambda=</code>?<br>
<strong>(Q5)</strong> is it better to use <code>lambda.min</code> or <code>lambda.1se</code>?</p></li>
<li><p>Obtain the coefficients, to see which parameters have fallen out ("".""):<br>
<code>predict(model, type=""coefficients"")</code></p>

<p>In the help page there are many <code>predict</code> methods (e.g., <code>predict.fishnet</code>, <code>predict.glmnet</code>, <code>predict.lognet</code>, etc). But any ""plain"" predict as I saw on an example.<br>
<strong>(Q6)</strong> Should I use <code>predict</code> or <code>predict.glmnet</code> or other?</p></li>
</ol>

<p>Despite what I've read about regularization methods, I'm quite new in R and in these statistical packages, so it's difficult to be sure if I'm adapting my problem to the code. Any suggestions will be welcomed.</p>

<p><strong>UPDATE</strong><br>
<a href=""http://www.jstatsoft.org/v28/i05/paper"">Based on</a> ""As previously noted, an object of class train contains an element called <code>finalModel</code>, which is the fitted model with the tuning parameter values selected by resampling. This object can be used in the traditional way to generate predictions for new samples, using that model's
predict function.""  </p>

<p>Using <code>caret</code> to tune both alpha and lambda:    </p>

<pre><code>  trc = trainControl(method=cv, number=10)  
  fitM = train(x, y, trControl = trC, method=""glmnet"")  
</code></pre>

<p>Does <code>fitM</code> replace previous step 2? If so, how to specify the glmnet options (<code>type.gaussian=""naive"",lambda=cv$lambda.min/1se</code>) now?<br>
And the following <code>predict</code> step, can I replace <code>model</code> to <code>fitM</code>?</p>

<p>If I do  </p>

<pre><code>  trc = trainControl(method=cv, number=10)  
  fitM = train(x, y, trControl = trC, method=""glmnet"")  
  predict(fitM$finalModel, type=""coefficients"")
</code></pre>

<p>does it make sense at all or am I incorrectly mixing both package vocabulary?</p>
"
"0.0484501583111509","0.0493463771219827"," 70598","<p>I am estimating an instrumental variables linear regression that has a large number of indicator (factor) variables.  I don't particularly care about the coefficient estimates on those indicator variables.  In Stata's ivreg2 package there is a ""partial"" option that applies the Frisch-Waugh-Lovell theorem to orthogonalize the dependent and exogenous variables to the indicator variables.  After this transformation the indicator variables are not estimated because they do not affect the coefficients on the variables I am interested in.</p>

<p>My question is, is there something like this in R?  It doesn't have to be part of an IV regression package but I am looking to orthogonalize one set of variables to another set of variables.  This seems like something that would have already been implemented.  Thanks.</p>
"
"0.0685188709827532","0.0697863157798853"," 71292","<p>I need to compare the goodness of fit of several averaged logistic regression models by calculating the deviance explained. I'm using the <code>MuMIn</code> package in R to average many logistic regression models into a single averaged model. I ultimately want to compare the explanatory power of several averaged models, in part by using the deviance explained.</p>

<p>My questions are:</p>

<ol>
<li><p>Does deviance explained apply to averaged models as a strong measure of the goodness of fit?</p></li>
<li><p>How does one calculate the deviance explained (calculated as the null deviance less the residual deviance as a proportion of the null deviance) from the averaged model output from the <code>model.avg()</code> command in <code>MuMIn</code>? </p>

<p>Examining the structure of the averaged model object indicates that the null and residual deviances are calculated on each individual model that contributes to the averaged model, but I'm not sure how to extract them and then calculate the overall deviance explained by the averaged model.</p></li>
</ol>
"
"0.0740088392978143","0.0753778361444409"," 71727","<p>In addition to <a href=""http://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_varclus_sect004.htm"">PROC VARCLUS</a>, <a href=""http://cran.r-project.org/web/packages/randomForest/index.html"">randomForest</a>, <a href=""http://cran.r-project.org/web/packages/glmnet/index.html"">glmnet</a>, and assessing multicollinearity among potential predictor variables (without regards to the outcome of interest), I am seeking other methods of variable selection in lieu of using stepwise methods for building more parsimonious binary logistic regression models (containing 8 to 12 variables to predict outcomes such as loan payment/default or current/late payment history) from a wide array of potential predictor variables (500+ variables, 200k+ records). </p>

<p>Below I have included an R script using <a href=""http://cran.r-project.org/web/packages/FSelector/index.html"">FSelector</a> to select the 8 highest ""ranked"" variables:</p>

<pre><code>library(FSelector)
fit &lt;- information.gain(outcome ~ ., dataset)
fit2 &lt;- cutoff.k(fit,8)
reducedmodel &lt;- as.simple.formula(fit2,""outcome"")
print(reducedmodel)
</code></pre>

<p>I have two questions regarding this script and the <code>FSelector</code> algorithm in general:   </p>

<ol>
<li><p>Is the <code>information.gain</code> criteria in the above script synonymous with <a href=""http://en.wikipedia.org/wiki/Kullback-Leibler_divergence"">Kullback-Leibler divergence</a>?
If so, can someone explain this in more layman terms than Wikipedia as I am relatively new to this area of statistics and would like to start off with the right idea of this concept as I may likely use this approach a great deal in the future?</p></li>
<li><p>Is this a valid approach, if there is such a thing as a valid approach, to select a desired number of variables for a binary logistic regression model (e.g., selecting the 8 highest ""ranked"" variables for use in a parsimonious model)? If not, can you provide an alternative approach to do so?</p></li>
</ol>

<p>Any insight or references regarding this topic and/or these questions will be greatly appreciated!</p>
"
"0.0791187772129236","0.080582296402538"," 71924","<p><em><a href=""http://stackoverflow.com/q/19186966/1414455"">Cross-posted from SO.</a></em></p>

<p>I am trying to replicate the results of <code>bgtest</code> from the <a href=""http://cran.r-project.org/web/packages/lmtest/lmtest.pdf"" rel=""nofollow""><code>lmtest</code></a> R package.</p>

<p>I am using the following dataset:</p>

<pre><code>           rs   month   r20
1    2.365042  1952m3  4.33
2    2.317500  1952m4  4.23
3    2.350833  1952m5  4.36
4    2.451833  1952m6  4.57
5    2.466167  1952m7  4.36
6    2.468417  1952m8  4.11
7    2.485583  1952m9  4.20
8    2.415125 1952m10  4.19
9    2.389875 1952m11  4.15
10   2.418167 1952m12  4.22
11   2.396042  1953m1  4.13
12   2.401042  1953m2  4.10
13   2.400833  1953m3  4.04
14   2.383500  1953m4  3.94
15   2.366708  1953m5  3.95
16   2.365625  1953m6  4.02
17   2.348583  1953m7  3.98
18   2.334375  1953m8  3.94
19   2.133542  1953m9  3.78
20   2.097375 1953m10  3.80
21   2.097708 1953m11  3.78
22   2.130583 1953m12  3.83
23   2.096000  1954m1  3.79
24   2.064042  1954m2  3.79
25   2.115083  1954m3  3.76
26   2.047333  1954m4  3.71
27   1.713875  1954m5  3.65
28   1.606167  1954m6  3.61
29   1.561667  1954m7  3.35
30   1.613292  1954m8  3.36
31   1.621083  1954m9  3.35
32   1.587667 1954m10  3.35
33   1.637792 1954m11  3.38
34   1.865917 1954m12  3.51
35   2.356417  1955m1  3.64
36   3.810000  1955m2  3.85
37   3.797000  1955m3  3.83
38   3.906000  1955m4  4.15
39   3.937000  1955m5  4.21
40   3.969000  1955m6  4.33
41   3.971000  1955m7  4.47
42   4.005000  1955m8  4.84
43   4.072000  1955m9  4.68
44   4.071000 1955m10  4.50
45   4.104000 1955m11  4.64
46   4.072000 1955m12  4.70
47   4.071000  1956m1  4.84
48   5.218000  1956m2  4.87
49   5.165000  1956m3  5.02
50   5.008000  1956m4  4.85
51   4.955000  1956m5  5.12
52   5.136000  1956m6  5.25
53   4.977000  1956m7  5.27
54   4.027000  1956m8  5.20
55   5.091000  1956m9  5.35
56   4.991000 1956m10  5.33
57   5.020000 1956m11  5.50
58   4.858000 1956m12  5.29
59   4.553000  1957m1  4.91
60   4.148000  1957m2  4.93
61   4.099000  1957m3  5.08
62   3.914000  1957m4  5.11
63   3.921000  1957m5  5.43
64   3.854000  1957m6  5.55
65   3.845000  1957m7  5.60
66   4.121000  1957m8  5.75
67   6.605000  1957m9  5.98
68   6.603000 1957m10  5.84
69   6.459000 1957m11  5.89
70   6.375000 1957m12  5.81
71   6.127000  1958m1  5.66
72   6.014000  1958m2  5.65
73   5.523000  1958m3  5.64
74   5.179000  1958m4  5.45
75   4.816000  1958m5  5.46
76   4.294000  1958m6  5.45
77   4.159000  1958m7  5.46
78   3.760000  1958m8  5.49
79   3.625000  1958m9  5.36
80   3.584000 1958m10  5.35
81   3.305000 1958m11  5.36
82   3.152000 1958m12  5.36
83   3.107000  1959m1  5.20
84   3.276000  1959m2  5.20
85   3.287000  1959m3  5.24
86   3.283000  1959m4  5.22
87   3.382000  1959m5  5.28
88   3.452000  1959m6  5.18
89   3.484000  1959m7  5.13
90   3.488000  1959m8  5.21
91   3.472000  1959m9  5.33
92   3.386000 1959m10  5.06
93   3.400000 1959m11  5.04
94   3.687000 1959m12  5.21
95   4.538000  1960m1  5.34
96   4.554000  1960m2  5.43
97   4.621000  1960m3  5.53
98   4.652000  1960m4  5.59
99   4.556000  1960m5  5.62
100  5.681000  1960m6  5.92
101  5.546000  1960m7  5.97
102  5.588000  1960m8  5.95
103  5.565000  1960m9  5.97
104  5.090000 1960m10  5.97
105  4.639000 1960m11  5.97
106  4.349000 1960m12  6.01
107  4.165000  1961m1  6.01
108  4.399000  1961m2  6.04
109  4.485000  1961m3  6.05
110  4.407000  1961m4  6.01
111  4.436000  1961m5  6.08
112  4.537000  1961m6  6.33
113  6.688000  1961m7  6.52
114  6.700000  1961m8  6.63
115  6.552000  1961m9  6.65
116  5.727000 1961m10  6.33
117  5.389000 1961m11  6.34
118  5.403000 1961m12  6.41
119  5.242000  1962m1  6.35
120  5.531000  1962m2  6.26
121  4.405000  1962m3  6.25
122  4.052000  1962m4  6.24
123  3.816000  1962m5  6.25
124  3.921000  1962m6  6.24
125  3.887000  1962m7  5.98
126  3.752000  1962m8  5.77
127  3.635000  1962m9  5.27
128  3.858000 1962m10  5.37
129  3.689000 1962m11  5.42
130  3.717000 1962m12  5.36
131  3.491000  1963m1  5.54
132  3.426000  1963m2  5.74
133  3.756000  1963m3  5.69
134  3.709000  1963m4  5.50
135  3.635000  1963m5  5.31
136  3.702000  1963m6  5.28
137  3.761000  1963m7  5.20
138  3.723000  1963m8  5.22
139  3.674000  1963m9  5.21
140  3.745000 1963m10  5.26
141  3.739000 1963m11  5.51
142  3.721000 1963m12  5.63
143  3.758000  1964m1  5.64
144  4.307000  1964m2  5.85
145  4.302000  1964m3  5.76
146  4.302000  1964m4  5.93
147  4.384000  1964m5  5.90
148  4.464000  1964m6  5.97
149  4.654000  1964m7  6.02
150  4.656000  1964m8  6.00
151  4.703000  1964m9  6.00
152  4.698000 1964m10  6.06
153  6.630000 1964m11  6.23
154  6.627000 1964m12  6.41
155  6.543000  1965m1  6.41
156  6.442000  1965m2  6.43
157  6.549000  1965m3  6.53
158  6.375000  1965m4  6.61
159  6.364000  1965m5  6.76
160  5.542000  1965m6  6.78
161  5.630000  1965m7  6.80
162  5.559000  1965m8  6.65
163  5.559000  1965m9  6.35
164  5.440000 1965m10  6.37
165  5.395000 1965m11  6.40
166  5.521000 1965m12  6.59
167  5.483000  1966m1  6.52
168  5.620000  1966m2  6.61
169  5.604000  1966m3  6.77
170  5.638000  1966m4  6.78
171  5.659000  1966m5  6.82
172  5.728000  1966m6  7.03
173  6.679000  1966m7  7.29
174  6.726000  1966m8  7.41
175  6.747000  1966m9  7.29
176  6.513000 1966m10  6.96
177  6.738000 1966m11  6.97
178  6.527000 1966m12  6.78
179  6.080000  1967m1  6.58
180  6.035000  1967m2  6.49
181  5.495000  1967m3  6.50
182  5.412000  1967m4  6.46
183  5.248000  1967m5  6.65
184  5.275000  1967m6  6.86
185  5.345000  1967m7  6.92
186  5.291000  1967m8  6.90
187  5.475000  1967m9  6.98
188  5.726000 1967m10  7.00
189  7.553000 1967m11  7.22
190  7.484000 1967m12  7.20
191  7.520000  1968m1  7.28
192  7.374000  1968m2  7.28
193  7.108000  1968m3  7.29
194  7.080000  1968m4  7.34
195  7.241000  1968m5  7.50
196  7.242000  1968m6  7.87
197  7.059000  1968m7  7.63
198  6.945000  1968m8  7.63
199  6.577000  1968m9  7.64
200  6.493000 1968m10  7.70
201  6.789000 1968m11  7.93
202  6.777000 1968m12  8.17
203  6.728000  1969m1  8.47
204  7.711000  1969m2  8.61
205  7.782000  1969m3  8.81
206  7.798000  1969m4  8.90
207  7.850000  1969m5  9.46
208  7.880000  1969m6  9.31
209  7.830000  1969m7  9.19
210  7.790000  1969m8  9.49
211  7.811000  1969m9  9.21
212  7.743000 1969m10  8.95
213  7.738000 1969m11  9.29
214  7.650000 1969m12  9.04
215  7.550000  1970m1  9.03
216  7.600000  1970m2  8.79
217  7.270000  1970m3  8.75
218  6.940000  1970m4  8.94
219  6.190000  1970m5  9.40
220  6.870000  1970m6  9.58
221  6.850000  1970m7  9.33
222  6.820000  1970m8  9.19
223  6.820000  1970m9  9.28
224  6.810000 1970m10  9.15
225  6.810000 1970m11  9.51
226  6.820000 1970m12  9.62
227  6.790000  1971m1  9.51
228  6.750000  1971m2  9.35
229  6.660000  1971m3  9.07
230  5.920000  1971m4  9.07
231  5.650000  1971m5  9.03
232  5.590000  1971m6  9.08
233  5.570000  1971m7  9.22
234  5.750000  1971m8  8.96
235  4.830000  1971m9  8.50
236  4.630000 1971m10  8.51
237  4.480000 1971m11  7.79
238  4.360000 1971m12  8.10
239  4.360000  1972m1  7.93
240  4.370000  1972m2  7.90
241  4.340000  1972m3  8.16
242  4.300000  1972m4  8.26
243  4.270000  1972m5  8.60
244  5.210000  1972m6  9.32
245  5.600000  1972m7  9.23
246  5.790000  1972m8  9.36
247  6.440000  1972m9  9.54
248  6.740000 1972m10  9.46
249  6.880000 1972m11  9.45
250  7.760000 1972m12  9.62
251  8.210000  1973m1  9.56
252  8.080000  1973m2  9.65
253  8.070000  1973m3 10.01
254  7.670000  1973m4  9.93
255  7.330000  1973m5 10.02
256  7.060000  1973m6 10.15
257  8.270000  1973m7 10.60
258 10.910000  1973m8 11.30
259 10.970000  1973m9 11.55
260 10.770000 1973m10 11.28
261 11.730000 1973m11 12.00
262 12.460000 1973m12 12.50
263 12.090000  1974m1 12.89
264 11.920000  1974m2 13.50
265 11.950000  1974m3 13.68
266 11.520000  1974m4 14.21
267 11.360000  1974m5 13.80
268 11.230000  1974m6 14.38
269 11.200000  1974m7 14.88
270 11.240000  1974m8 15.29
271 11.060000  1974m9 14.95
272 10.930000 1974m10 15.68
273 10.980000 1974m11 16.75
274 10.990000 1974m12 17.18
275 10.590000  1975m1 16.02
276  9.880000  1975m2 14.58
277  9.500000  1975m3 13.43
278  9.260000  1975m4 13.89
279  9.470000  1975m5 14.53
280  9.430000  1975m6 14.41
281  9.710000  1975m7 13.93
282 10.430000  1975m8 13.87
283 10.360000  1975m9 13.79
284 11.420000 1975m10 14.66
285 11.100000 1975m11 14.81
286 10.820000 1975m12 14.79
287  9.990000  1976m1 13.79
288  8.760000  1976m2 13.46
289  8.460000  1976m3 13.88
290  9.060000  1976m4 13.77
291 10.440000  1976m5 13.59
292 10.960000  1976m6 14.09
293 10.870000  1976m7 14.16
294 10.880000  1976m8 14.33
295 12.050000  1976m9 14.79
296 14.000000 1976m10 16.03
297 14.140000 1976m11 15.79
298 13.780000 1976m12 15.48
299 12.730000  1977m1 14.48
300 11.020000  1977m2 13.93
301  9.920000  1977m3 13.25
302  8.240000  1977m4 13.05
303  7.400000  1977m5 12.69
304  7.450000  1977m6 13.26
305  7.430000  1977m7 13.62
306  6.540000  1977m8 13.12
307  5.680000  1977m9 11.88
308  4.530000 1977m10 10.98
309  4.960000 1977m11 11.28
310  6.370000 1977m12 11.16
311  5.810000  1978m1 11.06
312  5.960000  1978m2 11.75
313  5.930000  1978m3 11.72
314  6.730000  1978m4 12.39
315  8.400000  1978m5 12.72
316  9.170000  1978m6 12.79
317  9.220000  1978m7 12.72
318  8.900000  1978m8 12.55
319  8.980000  1978m9 12.64
320  9.860000 1978m10 12.91
321 11.510000 1978m11 13.16
322 11.570000 1978m12 13.22
323 11.860000  1979m1 13.68
324 12.630000  1979m2 13.94
325 11.350000  1979m3 12.35
326 11.320000  1979m4 11.68
327 11.350000  1979m5 11.94
328 12.570000  1979m6 12.69
329 13.320000  1979m7 12.25
330 13.320000  1979m8 12.30
331 13.380000  1979m9 12.60
332 13.380000 1979m10 13.16
333 15.330000 1979m11 14.54
334 15.900000 1979m12 14.72
335 15.790000  1980m1 14.17
336 16.140000  1980m2 14.45
337 16.180000  1980m3 14.70
338 16.170000  1980m4 14.27
339 16.090000  1980m5 14.01
340 15.800000  1980m6 13.78
341 14.550000  1980m7 13.07
342 14.860000  1980m8 13.58
343 14.400000  1980m9 13.38
344 14.290000 1980m10 13.12
345 13.950000 1980m11 13.22
346 13.070000 1980m12 13.67
347 12.820000  1981m1 13.96
348 12.090000  1981m2 13.89
349 11.530000  1981m3 13.68
350 11.330000  1981m4 13.64
351 11.350000  1981m5 14.31
352 12.090000  1981m6 14.57
353 13.150000  1981m7 15.14
354 13.420000  1981m8 15.09
355 13.960000  1981m9 15.59
356 15.550000 1981m10 15.95
357 14.080000 1981m11 15.44
358 14.510000 1981m12 15.65
359 14.160000  1982m1 15.58
360 13.300000  1982m2 14.74
361 12.480000  1982m3 13.72
362 12.890000  1982m4 13.96
363 12.530000  1982m5 13.69
364 12.230000  1982m6 13.56
365 11.280000  1982m7 13.20
366 10.080000  1982m8 12.23
367  9.910000  1982m9 11.40
368  8.910000 1982m10 10.50
369  9.220000 1982m11 10.64
370  9.960000 1982m12 11.34
371 10.590000  1983m1 11.60
372 10.740000  1983m2 11.50
373 10.470000  1983m3 10.97
374  9.840000  1983m4 10.56
375  9.700000  1983m5 10.65
376  9.470000  1983m6 10.39
377  9.370000  1983m7 10.95
378  9.340000  1983m8 11.07
379  9.160000  1983m9 10.67
380  8.840000 1983m10 10.61
381  8.840000 1983m11 10.29
382  8.870000 1983m12 10.35
383  8.870000  1984m1 10.28
384  8.850000  1984m2 10.42
385  8.430000  1984m3 10.23
386  8.380000  1984m4 10.40
387  8.820000  1984m5 10.93
388  8.860000  1984m6 11.15
389 10.970000  1984m7 11.67
390 10.210000  1984m8 10.98
391 10.020000  1984m9 10.78
392  9.850000 1984m10 10.69
393  9.230000 1984m11 10.32
394  9.100000 1984m12 10.46
395 10.550000  1985m1 10.96
396 12.690000  1985m2 11.06
397 12.930000  1985m3 10.90
398 11.930000  1985m4 10.68
399 11.940000  1985m5 10.88
400 11.890000  1985m6 10.70
401 11.390000  1985m7 10.44
402 10.960000  1985m8 10.37
403 11.060000  1985m9 10.39
404 11.050000 1985m10 10.22
405 11.110000 1985m11 10.37
406 11.150000 1985m12 10.45
407 11.980000  1986m1 10.80
408 12.020000  1986m2 10.40
409 11.060000  1986m3  9.39
410  9.990000  1986m4  8.76
411  9.700000  1986m5  9.00
412  9.320000  1986m6  9.23
413  9.450000  1986m7  9.37
414  9.390000  1986m8  9.41
415  9.610000  1986m9  9.97
416 10.250000 1986m10 10.62
417 10.630000 1986m11 10.80
418 10.660000 1986m12 10.69
419 10.520000  1987m1 10.09
420 10.290000  1987m2  9.83
421  9.350000  1987m3  9.16
422  9.430000  1987m4  9.12
423  8.460000  1987m5  8.82
424  8.540000  1987m6  8.90
425  8.840000  1987m7  9.23
426  9.790000  1987m8  9.20
427  9.690000  1987m9  9.98
428  9.450000 1987m10  9.88
429  8.430000 1987m11  9.20
430  8.190000 1987m12  9.57
431  8.370000  1988m1  9.57
432  8.790000  1988m2  9.38
433  8.270000  1988m3  9.12
434  7.740000  1988m4  9.12
435  7.540000  1988m5  9.27
436  8.880000  1988m6  9.32
437 10.050000  1988m7  9.51
438 11.130000  1988m8  9.47
439 11.530000  1988m9  9.60
440 11.540000 1988m10  9.23
441 12.070000 1988m11  9.30
442 12.540000 1988m12  9.46
443 12.450000  1989m1  9.35
444 12.390000  1989m2  9.15
445 12.410000  1989m3  9.26
446 12.470000  1989m4  9.52
447 12.540000  1989m5  9.52
448 13.590000  1989m6  9.88
449 13.290000  1989m7  9.53
450 13.320000  1989m8  9.37
451 13.440000  1989m9  9.62
452 14.460000 1989m10  9.81
453 14.450000 1989m11  9.99
454 14.500000 1989m12  9.96
455 14.500000  1990m1 10.28
456 14.450000  1990m2 10.72
457 14.570000  1990m3 11.46
458 14.590000  1990m4 11.77
459 14.500000  1990m5 11.49
460 14.380000  1990m6 11.01
461 14.320000  1990m7 11.03
462 14.310000  1990m8 11.41
463 14.260000  1990m9 11.32
464 13.370000 1990m10 11.12
465 12.920000 1990m11 10.94
466 12.960000 1990m12 10.40
467 13.000000  1991m1 10.22
468 12.390000  1991m2  9.89
469 11.640000  1991m3 10.06
470 11.250000  1991m4  9.99
471 10.840000  1991m5 10.15
472 10.720000  1991m6 10.34
473 10.520000  1991m7 10.10
474 10.200000  1991m8  9.89
475  9.660000  1991m9  9.54
476  9.860000 1991m10  9.62
477  9.980000 1991m11  9.68
478 10.100000 1991m12  9.56
479  9.970000  1992m1  9.34
480  9.800000  1992m2  9.21
481 10.100000  1992m3  9.54
482  9.970000  1992m4  9.33
483  9.430000  1992m5  8.99
484  9.420000  1992m6  9.02
485  9.430000  1992m7  8.90
486  9.650000  1992m8  9.13
487  9.160000  1992m9  9.12
488  7.470000 1992m10  9.24
489  6.490000 1992m11  8.84
490  6.390000 1992m12  8.84
491  6.050000  1993m1  8.92
492  5.370000  1993m2  8.63
493  5.380000  1993m3  8.33
494  5.330000  1993m4  8.39
495  5.300000  1993m5  8.60
496  5.190000  1993m6  8.39
497  5.130000  1993m7  7.96
498  5.060000  1993m8  7.39
499  5.170000  1993m9  7.18
500  5.150000 1993m10  7.09
501  4.950000 1993m11  7.06
502  4.870000 1993m12  6.46
503  4.890000  1994m1  6.41
504  4.760000  1994m2  6.83
505  4.830000  1994m3  7.47
506  4.880000  1994m4  7.83
507  4.810000  1994m5  8.24
508  4.880000  1994m6  8.55
509  5.090000  1994m7  8.41
510  5.340000  1994m8  8.52
511  5.390000  1994m9  8.72
512  5.440000 1994m10  8.63
513  5.630000 1994m11  8.53
514  5.870000 1994m12  8.44
515  5.930000  1995m1  8.61
516  6.160000  1995m2  8.52
517  6.090000  1995m3  8.50
518  6.300000  1995m4  8.39
519  6.200000  1995m5  8.18
520  6.370000  1995m6  8.16
521  6.620000  1995m7  8.36
522  6.590000  1995m8  8.24
523  6.520000  1995m9  8.09
524  6.530000 1995m10  8.34
525  6.380000 1995m11  8.01
526  6.220000 1995m12  7.94
</code></pre>

<p>which is saved as <code>ukrates.csv</code>.</p>

<p>Here is the code to attempt to reproduce the <code>bgtest</code> module.</p>

<pre><code>rm(list = ls())

library(zoo)
library(lmtest)
library(dynlm)

# read in the data
dfUK = read.csv('./data/ukrates.csv', header = TRUE)
summary(dfUK)

# run the time series regression
zooUK = zoo(dfUK[, c('rs', 'r20')], order.by = as.yearmon(dfUK$month, 
                                                              '%Ym%m'))
    zooUKAug = merge(zooUK, 
                     'drs' = diff(zooUK$rs, 1), 
                 'ldr20' = lag(diff(zooUK$r20, 1), -1))
lmUK2 = dynlm(drs ~ ldr20, data = zooUKAug)

# Breusch-Godfrey regression
zooUKBG = merge(zooUKAug, 'resid' = resid(lmUK2))
lmBG = dynlm(as.formula(paste('resid',  
                              '~', 
                              attr(lmUK2$terms, 'term.labels'),
                              ' + L(resid, 1)')),
             data = zooUKBG) 

# BG test using lmtest package
bgtest(lmUK2, order = 1, type = 'Chisq') # 14.5614

# attempt to recreate BG-test 
length(lmBG$residuals)*
      sum(lmBG$fitted^2)/sum(lmBG$residuals^2)
</code></pre>

<p>This is based on the following code for computing the chi-squared statistic directly from the <code>bgtest</code> function code:</p>

<pre><code>&gt; bgtest
function (formula, order = 1, order.by = NULL, type = c(""Chisq"", 
    ""F""), data = list(), fill = 0) 
{
    dname &lt;- paste(deparse(substitute(formula)))
    if (!inherits(formula, ""formula"")) {
        X &lt;- if (is.matrix(formula$x)) 
                formula$x
        else model.matrix(terms(formula), model.frame(formula))
        y &lt;- if (is.vector(formula$y)) 
                formula$y
        else model.response(model.frame(formula))
    }
    else {
        mf &lt;- model.frame(formula, data = data)
        y &lt;- model.response(mf)
        X &lt;- model.matrix(formula, data = data)
    }
    if (!is.null(order.by)) {
        if (inherits(order.by, ""formula"")) {
            z &lt;- model.matrix(order.by, data = data)
            z &lt;- as.vector(z[, ncol(z)])
        }
        else {
            z &lt;- order.by
        }
        X &lt;- as.matrix(X[order(z), ])
        y &lt;- y[order(z)]
    }
    n &lt;- nrow(X)
    k &lt;- ncol(X)
    order &lt;- 1:order
    m &lt;- length(order)
    resi &lt;- lm.fit(X, y)$residuals
        Z &lt;- sapply(order, function(x) c(rep(fill, length.out = x), 
            resi[1:(n - x)]))
        if (any(na &lt;- !complete.cases(Z))) {
            X &lt;- X[!na, , drop = FALSE]
            Z &lt;- Z[!na, , drop = FALSE]
            y &lt;- y[!na]
            resi &lt;- resi[!na]
            n &lt;- nrow(X)
        }
        auxfit &lt;- lm.fit(cbind(X, Z), resi)
        cf &lt;- auxfit$coefficients
    vc &lt;- chol2inv(auxfit$qr$qr) * sum(auxfit$residuals^2)/auxfit$df.residual
    names(cf) &lt;- colnames(vc) &lt;- rownames(vc) &lt;- c(colnames(X), 
        paste(""lag(resid)"", order, sep = ""_""))
    switch(match.arg(type), Chisq = {
        bg &lt;- n * sum(auxfit$fitted^2)/sum(resi^2)
            p.val &lt;- pchisq(bg, m, lower.tail = FALSE)
            df &lt;- m
            names(df) &lt;- ""df""
        }, F = {
            uresi &lt;- auxfit$residuals
        bg &lt;- ((sum(resi^2) - sum(uresi^2))/m)/(sum(uresi^2)/(n - 
            k - m))
        df &lt;- c(m, n - k - m)
        names(df) &lt;- c(""df1"", ""df2"")
        p.val &lt;- pf(bg, df1 = df[1], df2 = df[2], lower.tail = FALSE)
    })
    names(bg) &lt;- ""LM test""
    RVAL &lt;- list(statistic = bg, parameter = df, method = paste(""Breusch-Godfrey test for serial correlation of order up to"", 
        max(order)), p.value = p.val, data.name = dname, coefficients = cf, 
        vcov = vc)
    class(RVAL) &lt;- c(""bgtest"", ""htest"")
    return(RVAL)
}
&lt;environment: namespace:lmtest&gt;
</code></pre>

<p>I am wondering why I am getting the different results.</p>
"
"0.0740088392978143","0.0646095738380922"," 72339","<p>After some frantic googling I do believe the answer is yes, but more so I am frustrated that the relation between the two parameters seems to be nowhere described explicitely so I do it here. (I hope this isn't against the rules of stackexchange.)</p>

<p><a href=""http://digitalcommons.unl.edu/cgi/viewcontent.cgi?article=1141&amp;context=usdeptcommercepub&amp;sei-redir=1&amp;referer=http%3A%2F%2Fscholar.google.nl%2Fscholar_url%3Fhl%3Dnl%26q%3Dhttp%3A%2F%2Fdigitalcommons.unl.edu%2Fcgi%2Fviewcontent.cgi%253Farticle%253D1141%2526context%253Dusdeptcommercepub%26sa%3DX%26scisig%3DAAGBfm3vz9gDbxRveIafikl02v0aeUyu0w%26oi%3Dscholarr%26ei%3DDj9VUqWlL6LG0QXe1oHwAg%26ved%3D0CDAQgAMoADAA#search=%22http%3A%2F%2Fdigitalcommons.unl.edu%2Fcgi%2Fviewcontent.cgi%3Farticle%3D1141%26context%3Dusdeptcommercepub%22"" rel=""nofollow"">This very nice article</a> states: we will denote the random variable Y having a negative binomial distribution as Y ~ NB($\mu, \kappa$) with a parameterization such that E(Y) = $\mu$, var(Y) = $\mu + \kappa \mu^2$.</p>

<p>I take this latter equation as the definition of $\kappa$.</p>

<p><a href=""http://books.google.nl/books?id=Ohks0xwvyT4C&amp;pg=PA196&amp;lpg=PA196&amp;dq=kappa+parameter+negative+binomial+proc+glimmix&amp;source=bl&amp;ots=PYKpaGQ8VN&amp;sig=5sNEB-7H7ZocErTKhi35ORKd2lA&amp;hl=nl&amp;sa=X&amp;ei=lEBVUqCnNcTJ0QXppYGoAg&amp;ved=0CDYQ6AEwAA#v=onepage&amp;q=kappa%20parameter%20negative%20binomial%20proc%20glimmix&amp;f=false"" rel=""nofollow"">Apparently</a> this kappa is implemented in SAS.</p>

<p>Now turning to R, the function <code>glm.nb</code> in the <code>MASS</code> package contains a parameter $\mu$ which is obviously the same $\mu$ as above and a parameter $\theta$. The question is how $\theta$ and $\kappa$ are related. The documentation for <code>glm.nb</code> only refers to it as an ""additional parameter"". The answers to <a href=""http://stats.stackexchange.com/questions/10419/what-is-theta-in-a-negative-binomial-regression-fitted-with-r"">this</a> and <a href=""http://stats.stackexchange.com/questions/10457/interpreting-negative-binomial-regression-output-in-r?rq=1"">this</a> stackexchange questions directly imply that $\theta = 1/\kappa$, but <a href=""http://stats.stackexchange.com/questions/30360/what-is-the-distribution-of-theta-in-a-negative-binomial-model-glm-nb-with-r?rq=1"">this</a> question [EDIT: since removed] seems to suggest that $\theta = \kappa$. </p>

<p>The <a href=""http://stat.ethz.ch/R-manual/R-patched/library/stats/html/NegBinomial.html"" rel=""nofollow"">help page for negative binomial in R</a> is nice and introduces a parameter called <code>size</code> that equals $1/\kappa$. Fitting <code>glm.nb</code> on random data generated by <code>rnbinom</code> for various choices of $\mu$ and <code>size</code> seems to support the thesis that $\theta = 1/\kappa$ (i.e. that $\theta$ = <code>size</code>) but also that for large values of size the estimation is poor.</p>

<p>Summarizing: I do believe that $\theta = 1/\kappa$ but it would be nice if there were an easily googlable place on the internet stating this explicitly. Maybe one of the answers to this questions can serve as such a place? </p>
"
"NaN","NaN"," 72968","<p>I am working on vector auto-regression (VARs) and impulse response function (IRFs) estimation based on panel data with 33 individuals over 77 quarters.  How should this type of situation be analyzed?  What algorithm's exist for this purpose?  I would prefer to conduct these analyses in R, so if anyone is familiar with R code or a package designed for this purpose that they could suggest, that would be especially helpful.  </p>
"
"0.0559454238864459","0.056980288229819"," 73950","<p>My question is similar to this link <a href=""http://stats.stackexchange.com/questions/12425/creating-a-certainty-score-from-the-votes-in-random-forests"">Creating a &quot;certainty score&quot; from the votes in random forests?</a></p>

<p>I am trying to build a random forest for a binary response (1 &amp; 0). Let's say we have 10,000 different records and I am building 500 trees. Is there a way to score the records in terms of the certainty / confidence / likelihood of being categorized in category 1 (for example)? The link above suggests using the number of votes among all 500 trees, but this way can only give me up to 500 different scores, how can I differentiate further for these 10,000 records? (Like regression, the scores can be easily obtained). </p>

<p>One solution is to average the score of each tree in the forest. the tree is the probability of 1s in the final node. Anyone know how to produce that average in R? I couldnt find this in the randomForest package. I think if I write my own codes to do that it , the run time may not be as fast as a built-in function. </p>
"
"0","0.0284901441149095"," 74304","<p>What goodness of fit tests are usually used for quantile regression? Ideally I need something similar to F-test in linear regression, but something like AIC in logistic regression will suite as well. I use quantreg R package, but found only some Khmaladze test in there. To be fair I hardly understand what is does.</p>
"
"0.10466430427046","0.106600358177805"," 76490","<h2>Background</h2>

<p>A laboratory wants to evaluate whether a certain form of <a href=""http://en.wikipedia.org/wiki/Polyacrylamide_gel_electrophoresis"" rel=""nofollow"">gel electrophoresis</a> is suited as a classification method for the quality of a certain substance. Several gels were loaded, each with a clean sample of the substance and with a sample that contains impurities. In addition, a molecular marker was also loaded which serves as a reference. The following picture illustrates the setup (the picture doesn't show the actual experiment, I have taken it from Wikipedia for illustration):</p>

<p><img src=""http://i.stack.imgur.com/vC53q.png"" alt=""Example of a gel electrophoresis""></p>

<p>Two parameters were measured for each gel and each lane:</p>

<ol>
<li>The <strong>molecular weight</strong> (that is how ""high up"" a compound wandered during the electrophoresis)</li>
<li>The <strong>relative quantity.</strong> The total quantity of each lane is normalized to 1 and the density of each band is measured which results in the relative quantity of each band.</li>
</ol>

<p>A scatterplot of the relative quantity vs. molecular weight is then produced which could look something like this (it's artificial data):</p>

<p><img src=""http://i.stack.imgur.com/ndzh9.png"" alt=""Example scatterplot""></p>

<p>This graphic can be read as follows: Both the ""good"" (blue points) and ""impure"" (red points) substance exhibit two bands, one at around a molecular weight of 120 and one at around 165. The bands of the ""impure"" substance at a molecular weight around 120 are considerably less dense than the ""good"" substance and can be well distinguished.</p>

<hr>

<h2>Goal</h2>

<p>The goal is to determine two boxed (see graphic below) which determine a ""good"" substance. These boxes will then be used for classification of the substance in the future into ""good"" and ""impure"". If a substance exhibits lanes that fall within the boxes it is classified as ""good"" and else as ""impure"".</p>

<p>These decision-rules should be <em>simple</em> to apply for someone in the laboratory. That's why it should be boxes instead of curved decision boundaries.</p>

<p>False-negatives (i.e. classify a sample as ""impure"" when it's really ""good"") are considered worse than false-positives. That is, an emphasis should be placed on the <a href=""http://en.wikipedia.org/wiki/Sensitivity_and_specificity#Sensitivity"" rel=""nofollow"">sensitivity</a>, rather than on the <a href=""http://en.wikipedia.org/wiki/Sensitivity_and_specificity#Specificity"" rel=""nofollow"">specificity</a>.</p>

<p><img src=""http://i.stack.imgur.com/vhzEW.png"" alt=""Example decision boxed""></p>

<hr>

<h2>Question</h2>

<p>I'm am no expert in machine learning. I know, however, that there are quite a few machine learning algorithms/techniques that could be helpful: $k$-nearest neighbors (e.g. <code>knn</code> in <code>R</code>), classification trees (e.g. <code>rpart</code> or <code>ctree</code>), support vector machines (<code>ksvm</code>), logistic regression, boosting and bagging methods and many more.</p>

<p>One problem of many of those algorithms is that they don't provide a simple ruleset or linear boundaries. In addition, the <strong>sample size</strong> is around <strong>70.</strong></p>

<p>My questions are:</p>

<ul>
<li>Has anyone an idea of how to proceed here?</li>
<li>Does it make sense to split the dataset into training- and test-set?</li>
<li>What proportion of the data should the training set be (I thought around a 60/40-split).</li>
<li>What, in general, is the workflow for such an analysis? Something like: Splitting dataset -> fit algorithm on the training set -> predict outcome for the test set?</li>
<li>How to avoid overfitting (i.e. boxes that are too small)?</li>
<li>What is a good statistic to assess the predictive performance in this case? AUC? Accurary? Positive predictive value? <a href=""http://en.wikipedia.org/wiki/Matthews_correlation_coefficient"" rel=""nofollow"">Matthews correlation coefficient</a>?</li>
</ul>

<p>Assume that I'm familiar with <code>R</code> and the <code>caret</code> package. Thank you very much for you time and help.</p>

<hr>

<h2>Example data</h2>

<p>Here is an example dataset.</p>

<pre><code>structure(list(mol.wt = c(125.145401455869, 118.210252208676, 
165.048583787746, 126.003687476776, 170.149347112565, 127.761533014759, 
155.523172614798, 120.094514977175, 161.234986765321, 168.471542655269, 
156.522990530521, 154.377948321209, 165.365756398877, 167.965538771316, 
116.132241687833, 115.143539160903, 156.696830822196, 162.578494491556, 
136.830624758899, 123.886594633942, 124.247484227948, 126.257226352824, 
160.684010454816, 166.618872115047, 126.599387146887, 165.690375912529, 
159.786861142652, 114.520735974329, 125.753594471656, 157.551537154148, 
157.320636890647, 171.5759136115, 158.580005438661, 125.647463565197, 
130.404710783509, 127.128218318572, 162.144126888907, 161.804616951055, 
167.917268243627, 168.582197247178), rel.qtd = c(57.68339235957, 
54.0514508510085, 25.0703901938793, 37.6933881305906, 36.6853653723001, 
53.6650555524679, 52.268438087776, 52.8621831466857, 43.1242291166037, 
46.6771236380788, 38.0328239221277, 40.0454611708371, 44.6406366176158, 
40.8238699987682, 51.9464749018547, 54.0302533272953, 37.9792331383524, 
48.3853988095525, 38.2093977349102, 42.2636098418388, 42.9876895407144, 
40.8018728193786, 40.1097096927465, 38.7432550253867, 39.2633283608111, 
43.4673723102812, 53.3740718733815, 49.1067921475768, 52.3002598744634, 
44.9847844953241, 44.3014423068017, 44.0191971364465, 47.0805245356855, 
55.0124134796556, 57.9938440244052, 62.8314454977068, 45.8093815891894, 
43.2300677500964, 39.4801550161538, 51.6253515591173), quality = structure(c(2L, 
2L, 2L, 1L, 2L, 2L, 1L, 2L, 2L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 1L), .Label = c(""bad"", ""good""), class = ""factor"")), .Names = c(""mol.wt"", 
""rel.qtd"", ""quality""), row.names = c(10L, 14L, 47L, 16L, 57L, 
54L, 45L, 12L, 43L, 67L, 25L, 21L, 1L, 55L, 20L, 22L, 37L, 15L, 
8L, 38L, 46L, 64L, 51L, 65L, 52L, 61L, 63L, 32L, 50L, 27L, 19L, 
69L, 23L, 42L, 6L, 48L, 11L, 13L, 5L, 71L), class = ""data.frame"")
</code></pre>
"
"0.027972711943223","0.0284901441149095"," 76625","<p>I'm looking for a way to run a repeated-measures multiple regression in R, which would take care of sphericity - either by applying some corrections (such as Huynh-Feldt), or by avoiding the problem in some other way.</p>

<p>I have 2 factorial repeated measure variables: 3- and 2-level (<code>roi_ant</code>, <code>roi_lat</code>), and a quantitative between-subject variable (<code>pred</code>), and one dependent quantitative variable (<code>mv</code>). I want to test for a full model including all possible interactions between the two within-subject and the between-subject variables (i.e., <code>mv ~ pred * roi_ant * roi_lat</code>). I am most interested in the slope of <code>pred</code> - whether it is different from 0, and whether it changes depending on <code>roi_ant</code> and <code>roi_lat</code>.</p>

<p>I have tried to do repeated-measures MANCOVA (that would remove the sphericity assumption) using car::Anova package, but if I got <a href=""http://r.789695.n4.nabble.com/car-Anova-Can-it-be-used-for-ANCOVA-with-repeated-measures-factors-td4637324.html"" rel=""nofollow"">this</a> discussion right, <code>car::Anova()</code> is not able to handle such a design.</p>

<p><code>ezANOVA()</code> performs ANCOVA with applying corrections for sphericity, but reports only the repeated-measure variables (after removing the influence of the covariate), and does not report anything on the covariate <code>pred</code>.</p>

<p>Mixed linear models (<code>lme4</code>) should handle this design well, but since <code>mcmsamp</code> is still not implemented, it is difficult to get p-values out of them.</p>

<p>Do you have any other suggestions? </p>
"
"0.0484501583111509","0.0328975847479884"," 76627","<p>I am using the data <code>veteran</code> from  <code>R</code> package <code>survival</code>. </p>

<p>How can I  diagnose the normality assumption about <code>time</code>? Should I need to perform a linear regression to measure the dependency of <code>time</code> on <code>age</code> and <code>karno</code>? </p>

<p>Are the following commands all to answer the question? or Do I need to add more information?</p>

<pre><code>   #diagnoses normality assumption about 'time'.
   shapiro.test(veteran$time)
   qqnorm(veteran$time)
   qqline(veteran$time)

   #perform a linear regression
   with(veteran,lm(time~age+karno))
   with(veteran,cor.test(time,age))
   with(veteran,cor.test(time,karno))
</code></pre>

<h3>EDIT :</h3>

<p>I want to diagnose the normality assumption. How can I do this for the above data?
And for diagnosing the normality assumption, I chose <code>R</code> software.
So my question involves both diagnosie the normality assumption and R syntax.</p>
"
"0.0395593886064618","0.040291148201269"," 76850","<p>I am trying to do a multinomial logistic regression on some data that I generated. I am using R and the package mlogit. My data looks like the following:</p>

<pre><code>Class X1 X2  X3
V +0.0655197 +0.6418541 +1.8110291
V-0.6713268 -0.0262458 -0.3602958
V +0.2357610 -0.3602958 -0.6943458
M +0.3900129 +0.5583416 -1.7800082
M +0.5714871 -0.2767833 +0.5583416
M +1.0732807 -1.7800082 -0.3602958
S +0.9553640 -0.3602958 +0.6418541
S +0.1139899 +0.1356030 +0.3889280
S +0.4717283 -0.2852090 -1.1229880
</code></pre>

<p>My model is</p>

<pre><code>Class = B1*X1 + B2*X2 + B3*X3
</code></pre>

<p>So far, I have:</p>

<pre><code>library(mlogit);
allData &lt;- read.table(""Features/AllFeatures.dat"", header=TRUE);
allData$Class&lt;-as.factor(allData$Class);
mlData&lt;-mlogit.data(allData, choice=""Class"");
myData&lt;- mlogit(Class~1|X1 + X2 + x3, data = mlData);
print(summary(myData));
</code></pre>

<p>Which gives me:</p>

<pre><code>Coefficients :
                           Estimate Std. Error t-value  Pr(&gt;|t|)    
S:(intercept)                -0.6832392  0.0951834 -7.1781 7.068e-13 ***
V:(intercept)                -0.6696254  0.0943282 -7.0989 1.258e-12 ***
S:X1                         -0.1362492  0.1134039 -1.2015    0.2296    
V:X1                         -0.0052649  0.1128722 -0.0466    0.9628    
S:X2                         -0.0198451  0.0973608 -0.2038    0.8385    
V:X2                          0.0183261  0.0974789  0.1880    0.8509    
S:X3                          0.1728694  0.1110473  1.5567    0.1195    
V:X3                          0.0230260  0.1101147  0.2091    0.8344
</code></pre>

<p>However in the following: <a href=""http://cran.r-project.org/web/packages/mlogit/vignettes/Exercises.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/mlogit/vignettes/Exercises.pdf</a></p>

<p>The author gets:</p>

<pre><code>Coefficients :
Estimate Std. Error t-value Pr(&gt;|t|)
ic -0.00623187 0.00035277 -17.665 &lt; 2.2e-16 ***
oc -0.00458008 0.00032216 -14.217 &lt; 2.2e-16 ***
</code></pre>

<p>How can I change my function call to get the same? I want the actual coefficients for the model, not for the comparisons.</p>

<p>Also: What is the difference between 'wide' and 'long'? What form is my data in?</p>

<p>Also: Do you have some mathematical references for this type of multinomial logistic regression aimed at engineers?</p>

<p>Thanks!</p>
"
"0.0484501583111509","0.0493463771219827"," 76856","<p>I've got three variables, a factored (c) and two ordinal independent (a,b). Every variable has five categories (1,2,3,4,5). Thus, I fitted a <a href=""http://stat.ethz.ch/R-manual/R-devel/library/nnet/html/multinom.html"" rel=""nofollow"">multinomial logit regression</a> (testus, see below) with the car package. Now I would like to predict the probabilities. </p>

<pre><code>&gt; a&lt;-sample(5,100,TRUE)
&gt; b&lt;-sample(5,100,TRUE)
&gt; c&lt;-sample(5,100,TRUE)
&gt; required(car)
&gt; a&lt;-as.ordered(a)
&gt; b&lt;-as.ordered(b)
&gt; c&lt;-as.factor(c)
&gt; testus&lt;-multinom(c~a+b)
&gt; predictors &lt;-
&gt; expand.grid(b=c(""1"",""2"",""3"",""4"",""5""),a=c(""1"",""2"",""3"",""4"",""5""))
&gt; p.fit &lt;- predict(testus, predictors, type='probs')
&gt; probabilities&lt;-data.frame(predictors,p.fit)
</code></pre>

<p>Now I got the predicted probabilities for a under b and c. </p>

<pre><code>`head(probabilities)
  b a         X1         X2         X3         X4        X5
1 1 1 0.10609054 0.22599152 0.20107167 0.21953158 0.2473147
2 2 1 0.20886614 0.27207108 0.08613633 0.18276394 0.2501625
3 3 1 0.17041268 0.24995975 0.16234240 0.13111518 0.2861700
4 4 1 0.23704078 0.21179521 0.08493274 0.03135092 0.4348804
5 5 1 0.09494071 0.09659144 0.24162612 0.21812449 0.3487172
6 1 2 0.14059489 0.17793438 0.29272452 0.26104833 0.1276979`
</code></pre>

<p>The first two cols shows the categories of the independent variables a and b. the next five columns show the conditional probabilities (p.e. P(c=1|b==1&amp;&amp;a==1)=0,10609). But now I would like to know only the predicted probabilities for c under a or the predicted probabilities for c under b. Is this possible?</p>
"
"0.0484501583111509","0.0493463771219827"," 76925","<p>Is it possible to penalize coefficients toward a number other than zero in a ridge regression in R?</p>

<p>For example, let's say I have dependent variable Y and independent variables X1,X2,X3, and X4. Because of the multicollinear nature of the ivs, ridge regression is appropriate. But say I'm fairly certain that the coefficient of X1 is near 5, X2 is near 1, X3 is near -1, and X4 is near -5. </p>

<p>Is there a ridge package and method in R where I can implement penalties on the coefficients of the ivs that penalize them toward those numbers instead of 0? I'd love to see an example in R with my example data, if possible. Thank you.</p>
"
"0.0395593886064618","0.040291148201269"," 76997","<p>What is the apropriate statistic to measure the goodness-of-fit in Boosted Regression Tree (or Gradient Boosting Regression) with continuous response?
How can I calculate the coefficient of determination (RÂ²) in the train and test data? 
If I calculate the RÂ² as bellow, How can I calculate the intercept-only model?</p>

<p>RÂ² = 1âˆ’L1/L0, where L1 and L0 are the log likelihoods of the model under consideration and an
intercept-only model, respectively (see <a href=""http://www.stata-journal.com/sjpdf.html?articlenum=st0087"" rel=""nofollow"">http://www.stata-journal.com/sjpdf.html?articlenum=st0087</a>).</p>

<p>I'm using the package ""dismo"" in R, so if any one have a solution in R it will be great.</p>

<p>Example with binary data just to show the procedure:</p>

<pre><code>library(dismo)

data(Anguilla_train)

angaus.tc5.lr005 &lt;- gbm.step(data=Anguilla_train, gbm.x = 3:13, gbm.y = 2, family = ""bernoulli"", tree.complexity = 5, learning.rate = 0.005, bag.fraction = 0.5 , keep.fold.models = TRUE, keep.fold.vector = TRUE, keep.fold.fit = TRUE)
</code></pre>

<p>Thank you in advance!</p>
"
"0.08882529023711","0.0986927542439653"," 77057","<p>I have a data set consisting of six independent environmental variables (all binomial: present / absent) and one dependent variable (binomial: disease present / absent). 
In order to determine the combination of factors that have the highest probability of leading to disease, I first need to conduct an Expert Opinion poll where I will have several experts rank all possible combinations of variables according to their probability of leading to the occurrence of disease. Then, I will obtain regression parameters for each variable using a conjoint analysis approach where each expert conforms a level (hierarchical design), the six environmental variables are independent variables, and the rankings are the dependent variable. There being six factor variables, there exist a total of 64 possible orthogonal combinations. 
I reduced this overwhelming number of possible combinations (while retaining orthogonality) using the <code>AlgDesign</code> package of R. Here is the code followed only by relevant pieces of output:</p>

<pre><code>levels.design = c(2,2,2,2,2,2)
full.design &lt;- gen.factorial(levels.design)

   X1 X2 X3 X4 X5 X6
1  -1 -1 -1 -1 -1 -1
2   1 -1 -1 -1 -1 -1
3  -1  1 -1 -1 -1 -1
   .................
63 -1  1  1  1  1  1
64  1  1  1  1  1  1

set.seed(69)
fractional &lt;- optFederov(~., data=full.design, approximate=FALSE, criterion=""D"")
fractional
</code></pre>

<p>The result is a subset of 12 combinations to be included in the conjoint analysis:</p>

<pre><code>$design
    X1 X2 X3 X4 X5 X6
4   1  1 -1 -1 -1 -1
5  -1 -1  1 -1 -1 -1
    ................
57 -1 -1 -1  1  1  1 
</code></pre>

<p>From what I understand, doing a regression analysis on all 64 combinations should lead to the same regression parameters as those obtained if I use only the reduced set (i.e. 12 combinations from the fractional factorial design).  </p>

<p>Questions: </p>

<ol>
<li>Do the code and the resulting output make sense?</li>
<li>Could anyone point me to a good and simple reference on how this fractional design works? I am afraid I might be doing things wrong by selecting a subset that produces different results from those obtained if a full factorial design was employed.</li>
</ol>
"
"0.0740088392978143","0.0753778361444409"," 77245","<p>To better ask my question, I have provided some of the outputs from both a 16 variable model (<code>fit</code>) and a 17 variable model (<code>fit2</code>) below (all predictor variables in these models are continuous, where the only difference between these models is that <code>fit</code> does not contain variable 17 (var17)):</p>

<pre><code>fit                    Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
 Obs        102849    LR chi2   13602.84    R2       0.173    C       0.703    
  0          69833    d.f.            17    g        1.150    Dxy     0.407    
  1          33016    Pr(&gt; chi2) &lt;0.0001    gr       3.160    gamma   0.416    
 max |deriv| 3e-05                          gp       0.180    tau-a   0.177    
                                            Brier    0.190       


fit2                 Model Likelihood       Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
 Obs        102849    LR chi2   13639.70    R2       0.174    C       0.703    
  0          69833    d.f.            18    g        1.154    Dxy     0.407    
  1          33016    Pr(&gt; chi2) &lt;0.0001    gr       3.170    gamma   0.412    
 max |deriv| 3e-05                          gp       0.180    tau-a   0.177    
                                            Brier    0.190          
</code></pre>

<p>I used Frank Harrell's <code>rms</code> package to build these <code>lrm</code> models. As you can see, these models do not appear to vary much, if at all, across <em>Discrimination Indexes</em> and <em>Rank Discrim. Indexes</em>; however, using <code>lrtest(fit,fit2)</code>, I was provided with the following results:</p>

<pre><code> L.R. Chisq         d.f.            P 
3.685374e+01     1.000000e+00    1.273315e-09 
</code></pre>

<p>As such, we would reject the null hypothesis of this likelihood ratio test; however, I would assume this is likely due to the large sample size (<em>n</em> = 102849) as these models appear to perform in a similar fashion. Furthermore, I am interested in finding a better way of formally comparing nested binary logistic regression models when <em>n</em> is large.</p>

<p>I greatly appreciate any feedback, R scripts, or documentation that can steer me in the right direction in terms of comparing these types of nested models! Thanks!</p>
"
"0.0969003166223018","0.0986927542439653"," 77546","<p>I am trying to fit a multivariate linear regression model with approximately 60 predictor variables and 30 observations, so I am using the <strong>glmnet</strong> package for regularized regression because p>n.</p>

<p>I have been going through documentation and other questions but I still can't interpret the results, here's a sample code (with 20 predictors and 10 observations to simplify):</p>

<p>I create a matrix x with num rows = num observations and num cols = num predictors and a vector y which represents the response variable</p>

<pre><code>&gt; x=matrix(rnorm(10*20),10,20)
&gt; y=rnorm(10)
</code></pre>

<p>I fit a glmnet model leaving alpha as default (= 1 for lasso penalty)</p>

<pre><code>&gt; fit1=glmnet(x,y)
&gt; print(fit1)
</code></pre>

<p>I understand I get different predictions with decreasing values of lambda (i.e. penalty)</p>

<pre><code>Call:  glmnet(x = x, y = y) 

        Df    %Dev   Lambda
  [1,]  0 0.00000 0.890700
  [2,]  1 0.06159 0.850200
  [3,]  1 0.11770 0.811500
  [4,]  1 0.16880 0.774600
   .
   .
   .
  [96,] 10 0.99740 0.010730
  [97,] 10 0.99760 0.010240
  [98,] 10 0.99780 0.009775
  [99,] 10 0.99800 0.009331
 [100,] 10 0.99820 0.008907
</code></pre>

<p>Now I predict my Beta values choosing, for example, the smallest lambda value given from <code>glmnet</code></p>

<pre><code>&gt; predict(fit1,type=""coef"", s = 0.008907)

21 x 1 sparse Matrix of class ""dgCMatrix""
                  1
(Intercept) -0.08872364
V1           0.23734885
V2          -0.35472137
V3          -0.08088463
V4           .         
V5           .         
V6           .         
V7           0.31127123
V8           .         
V9           .         
V10          .         
V11          0.10636867
V12          .         
V13         -0.20328200
V14         -0.77717745
V15          .         
V16         -0.25924281
V17          .         
V18          .         
V19         -0.57989929
V20         -0.22522859
</code></pre>

<p>If instead I choose lambda with </p>

<pre><code>cv &lt;- cv.glmnet(x,y)
model=glmnet(x,y,lambda=cv$lambda.min)
</code></pre>

<p>All of the variables would be (.).</p>

<p>Doubts and questions:</p>

<ol>
<li>I am not sure about how to choose lambda.</li>
<li>Should I use the non (.) variables to fit another model? In my case I would like to keep as much variables as possible.</li>
<li>How do I know the p-value, i.e. which variables significantly predict the response?</li>
</ol>

<p>I apologize for my poor statistical knowledge! And thank you for any help.</p>
"
"0.0570990591522943","0.0697863157798853"," 78022","<p>I am performing quantile regressions in R using the package quantreg. My dataset includes 12,328 observations ranging from 0.12 to 330. The timepoints for my data are not exactly continuous; all data fall into one of a few dozen bins ranging from 73 to 397.</p>

<p>When I performed a linear regression on this data using the lm() function, I was able to do this with polynomials up to 4:</p>

<pre><code>lm(Y~poly(X,3,raw=TRUE),data=mydata)
</code></pre>

<p>However, with the package quantreg and the rq() command, I cannot use any polynomials. A simple regression works just fine:</p>

<pre><code>rq(Y~X,data=mydata,tau=.15)
</code></pre>

<p>But as soon as I get into polynomials, no dice. When I enter this:</p>

<pre><code>rq(Y~poly(X,2,raw=TRUE),data=mydata,tau=.15)
</code></pre>

<p>I get the following error message:</p>

<pre><code>Error in rq.fit.br(x, y, tau = tau, ...) : Singular design matrix
</code></pre>

<p>I am very new to all of this. I've read up on singular matrices as much as I can and I think there might be two reasons for this: (1) I only have one variable on each axis, or (2) my data are binned/the Y variable isn't truly continuous.</p>

<p>Can anyone tell me why I'm getting this error?</p>

<p>Thank you!</p>

<p>PS - This is how the graph looks:</p>

<p><img src=""http://i.stack.imgur.com/htUdA.png"" alt=""enter image description here""></p>
"
"0.115512844298478","0.1241856590838"," 78563","<p><strong>Short version:</strong> I'm looking for an R package that can build decision trees whereas each leaf in the decision tree is a full Linear Regression model. AFAIK, the library <code>rpart</code> creates decision trees where the dependent variable is constant in each leaf. Is there another library (or a <code>rpart</code> setting I'm not aware of) that can build such trees?</p>

<p><strong>Long version:</strong> I'm looking for an algorithm that builds a decision tree based on a training data set. Each decision in the tree splits the training data set into two parts, according to a condition on one of the independent variables. The root of the tree contains the full data set, and each item in the data set is contained in exactly one leaf node.</p>

<p>The algorithm goes like this:</p>

<ol>
<li>Begin with the full dataset, which is the root node of the tree. Pick this node and call it $N$.</li>
<li>Create a Linear Regression model on the data in $N$.</li>
<li>If $R^2$ of $N$'s linear model is higher than some threshold $\theta_{R^2}$, then we're done with $N$, so mark $N$ as a leaf and jump to step 5.</li>
<li>Try $n$ random decisions, and pick the one that yields the best $R^2$ in the subnodes:
<ul>
<li>Pick a random independent variable $v_i$, as well as a random threshold $\theta_i$.</li>
<li>The decision $v_i \leq \theta_i$ splits the data set of $N$ into two new nodes, $\hat{N}$ and $\tilde{N}$.</li>
<li>Create Linear Regression models on both $\hat{N}$ and $\tilde{N}$, and calculate their $R^2$ (call them $\hat{r}$ and $\tilde{r}$).</li>
<li>From all those $n$ tuples $(v_i, \theta_i, \hat{r}, \tilde{r})$, select the one with the maximal $min(\hat{r}, \tilde{r})$. This yields a new decision in the tree, and $N$ has two new subnodes $\hat{N}$ and $\tilde{N}$.</li>
</ul></li>
<li>We have finished processing $N$. Pick a new node $N$ which has not yet been processed and go back to step 2. If all nodes have been processed, the algorithm ends.</li>
</ol>

<p>This will recursively build a decision tree which splits the data into smaller parts, and calculates a Linear Model on each of those parts.</p>

<p>Step 3 is the exit condition, which prevents the algorithm from overfitting. Of course, there are other possible exit conditions:</p>

<ul>
<li>Exit if $N$'s depth in the tree is above $\theta_{depth}$</li>
<li>Exit if the data set in $N$ is smaller than $\theta_{data set}$</li>
</ul>

<p><strong>Is there such an algorithm in an R package?</strong></p>
"
"0.112084934384975","0.120873444603807"," 78663","<p>Is any one here familiar with an R package called Zelig?</p>

<p>I have a data frame like this:</p>

<pre><code>IQ   AGE
80   50
100  18
90   25
</code></pre>

<p>etc.</p>

<p>What I need to do is build a model of IQ given AGE, I am running these commands:
<code>z.out &lt;- zelig(IQ~AGE,data=df,model=""ls"")</code>
this runs the what-if given age 110, what would be the IQ
<code>x.out &lt;- setx(z.out, AGE=110)</code>
This is a simulation model where given the age 110, after running 1 million runs of simulation, what would be the IQ with 95% confidence interval.
<code>s.out &lt;- sim(z.out,x.out, num=1000000, level=95)</code></p>

<p>I have a hard time understanding from what pool of data the <code>sim()</code> function draws the numbers. I read though the docs, but they are written for Ph.D. students, if not more advanced readers. I have asked the Zelig creators this question multiple times but they are directing me to the docs which I read multiple times, with no luck. However, one of the  person that works with Zelig sent me this email:</p>

<blockquote>
  <p>Suppose that you fit 
  $$\text{IQ} = a + \text{Age} * b + e$$
  Then you get a table of regression coefficients where a=50, b=2, and their standard errors are something like $\text{s.e.}(a)=\sqrt{10}$ and $\text{s.e.}(b)=1$. These are all hypothetical examples. 
  In maximum likelihood estimation, this regression output is another way of saying that $a$ and $b$ are distributed bivariate normal with means $[50,2]$ and there's a variance-covariance matrix that looks something like this (all numbers are made up):
  $$\begin{array}{cc}
10 &amp; cov(a,b) \\
cov(a,b) &amp; 1 \\
\end{array}$$
  So, the variance of $a$ is 10, the variance of $b$ is 1, and their covariance is $cov(a,b)$. It won't be shown in your regression table, but Zelig remembers it for you. Let's pretend it's 3.
  This variance-covariance matrix is the inverse of the Hessian I mentioned earlier. Don't worry about it. For this example, you need only remember that $\text{mean}(a,b) = [50,2]$ and $cov(a,b)=\begin{array}{cc}10&amp;3\\3&amp;1\end{array}$. For purposes of plain text email, I'm representing matrices with columns separated by commas and rows by semicolons.
  In addition, suppose that the error term $e$ is distributed with mean 0 and s.d.=1.
  Now, one way to predict what IQ you might get for somebody aged 88, based on this regression table, is exactly what you would expect: you simply calculate 50 + 88 * 2 = 226. This is your point estimate. The 95% confidence interval around this point estimate is a function of the standard errors of the coefficient estimates of $a=50$ and $b=2$, and the exact formula for that is in any econometrics textbook.
  Simulation makes it unnecessary to dig up that textbook. Instead, for 1000 rounds, <code>sim()</code> will come up with 1000 different pairs of $(a,b)$ estimates drawn from the bivariate normal with mean=[50,2] and cov=$\begin{array}{cc}10&amp;3\\3&amp;1\end{array}$. One such pair might be $(47,1.5)$; another might be $(52,3)$; yet another might be $(10,5)$. 
  Whatever they are, <code>sim()</code> plugs them into the formula and gives you 1000 different estimates for the IQ. Their average is your point estimate. If you stack them from lowest to highest, the ends of the 95% confidence interval are the top 25th value and the bottom 25th. That's it. That's all that <code>sim()</code> does.</p>
</blockquote>

<p>Given the above explanation, can anybody tell me in lay terms, what numbers <code>sim()</code> is picking? How are those numbers in pool generated? I would greatly appreciate if anyone brings some light into this.</p>
"
"0.0484501583111509","0.0493463771219827"," 78747","<p>I am working on neural networks for a regression problem in R using packages like <code>nnet</code>, <code>caret</code> etc. I have split my data into train, validation and test. My doubt is does the <code>train()</code> function in <code>caret</code> package for R takes care for validation set also.</p>

<p>From what I understand, After training the <code>nnet</code> model, you need to keep checking with validation data set, to avoid overfitting or overlearning i.e restricting the number of iterations. Then we have to tune for the decay parameters and size of hidden layers and finally apply it on the test data set.</p>

<p>Is there anything wrong with the understanding? FYI. Here is the code that I am implementing</p>

<pre><code>Y=read.csv(file=""./dolcan.csv"",header=T)
ratio=as.integer(0.5*nrow(Y))
ratio1=as.integer(0.75*nrow(Y))
traindata=Y[(1:ratio),c(2:ncol(Y))]
valdata=Y[(ratio:ratio1),c(2:ncol(Y))]
testdata=Y[((ratio1+1):nrow(Y)),c(2:ncol(Y))]

## Train the network and tuning the number of nodes and decay
maxout= max(traindata[,1]) # to scale the output
mygrid &lt;- expand.grid(.decay=c(0.5, 0.1), .size=c(3,4,5))
nnetfit &lt;- train(dolcan/maxout ~ ., data=traindata, method=""nnet"", maxit=1000, tuneGrid=mygrid, trace=F)
nnetfit
</code></pre>
"
"NaN","NaN"," 78762","<p>I have an outcome which is truncated from both left and right sides. I would like to know which regression method may account this kind of analysis. Here is an example:</p>

<pre><code>set.seed(123)
z&lt;-rnorm(300)
y&lt;-z[z&gt;=-0.8 &amp; z&lt;=1.2] #truncated
x1&lt;-sample(1:3, length(y), T)
x2&lt;-rbinom(length(y), 1, 0.3)
</code></pre>

<p>What's the best regression method for modeling <code>y~x1+x2</code>?  The R package <code>truncreg</code> seems just have arguments for either left or right truncation rather than both sides like this example.</p>
"
"0.0625488854200668","0.0637058989297032"," 79107","<p>I'm trying my hand at resampling techniques with a dataset I have, and I think either I'm missing a conceptual point with bootstrapping, or I'm doing something incorrectly in <code>R</code>. Basically, I'm trying to use it in a correlation/regression framework, and I'm able to get the original coefficients, the bootstrap bias, and the bootstrap coefficients but I can't find a way to have <code>R</code> easily display the bootstrap model $R^2$ (when I'm working with several predictors), the Pearson $r$, or the $p$-values for individual regression coefficients. (I'm using the <code>Boot</code> function in the <code>car</code> package).</p>

<p>A secondary question...the more general function <code>boot</code> in the <code>boot</code> package requires defining a function to use as an argument. The function must include an argument for the original data set, and a second argument which is a set of indices, frequencies, or weights for the bootstrap sample. I'm a little confused by this. What conceptually are these indices I am specifying, and how do I specify them syntactically within my function?</p>
"
"0.0484501583111509","0.0328975847479884"," 80888","<p>I am trying to understand how to get the coefficient of a multiple linear regression. </p>

<p>The formula is:</p>

<p>$b = (X'X)^{-1}(X')Y$</p>

<p>I try to calculate $b$ without package and with the <code>lm</code> package inside R. </p>

<p>Doing so, I got different results. </p>

<p>I want to know why. Did I made a mistake? Or does the <code>lm</code> package calculate differently because of the intercept?</p>

<pre><code>&gt; y &lt;-  c(1,2,3,4,5)
&gt; x1 &lt;- c(1,2,3,4,5)
&gt; x2 &lt;- c(1,4,5,7,9)
&gt; Y &lt;- as.matrix(y)
&gt; X &lt;- as.matrix(cbind(x1,x2))
&gt; beta = solve(t(X) %*% X) %*% (t(X) %*% Y) ; beta
            [,1]
x1  1.000000e+00
x2 -1.421085e-14
&gt; model &lt;- lm(y~x1+x2) ; model$coefficients
 (Intercept)           x1           x2 
1.191616e-15 1.000000e+00 1.192934e-15 
</code></pre>

<p><img src=""http://i.stack.imgur.com/KHD2q.png"" alt=""3d""></p>

<h1>Update</h1>

<p>As Alex and the other told me, it was a question of roundoff error. Therefore, I decided to take another data from the book ""Essential Statistics for business and economics"" by Anderson and all. In this case, the coefficients are the same in both <code>lm</code> function and in my own matrix.</p>

<pre><code>&gt; y &lt;- c(9.3, 4.8, 8.9, 6.5, 4.2, 6.2, 7.4, 6, 7.6, 6.1)
&gt; x0 &lt;- c(1,1,1,1,1,1,1,1,1,1) 
&gt; x1 &lt;-  c(100,50,100,100,50,80,75,65,90,90)
&gt; x2 &lt;- c(4,3,4,2,2,2,3,4,3,2)
&gt; Y &lt;- as.matrix(y)
&gt; X &lt;- as.matrix(cbind(x0,x1,x2))

&gt; beta = solve(t(X) %*% X) %*% (t(X) %*% Y);beta
         [,1]
x0 -0.8687015
x1  0.0611346
x2  0.9234254
&gt; model &lt;- lm(y~+x1+x2) ; model$coefficients
(Intercept)          x1          x2 
 -0.8687015   0.0611346   0.9234254 
</code></pre>
"
"0.0969003166223018","0.0986927542439653"," 81612","<p>Recently I was trying to do logistic regression using the <code>rms::lrm()</code> function. But I had some trouble understanding the model objects from the function. Here is the example from the package:</p>

<pre><code>#dataset
n            &lt;- 1000    # define sample size
set.seed(17)            # so can reproduce the results
treat        &lt;- factor(sample(c('a','b','c'), n,TRUE))
num.diseases &lt;- sample(0:4, n,TRUE)
age          &lt;- rnorm(n, 50, 10)
cholesterol  &lt;- rnorm(n, 200, 25)
weight       &lt;- rnorm(n, 150, 20)
sex          &lt;- factor(sample(c('female','male'), n,TRUE))
L            &lt;- .1*(num.diseases-2) + .045*(age-50) +
                (log(cholesterol - 10)-5.2)*(-2*(treat=='a') +
                3.5*(treat=='b')+2*(treat=='c'))
y            &lt;- ifelse(runif(n) &lt; plogis(L), 1, 0)
#fit model
g            &lt;- lrm(y ~ treat*rcs(age))

&gt; g

Logistic Regression Model

lrm(formula = y ~ treat * rcs(age))

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          1000    LR chi2      76.77    R2       0.099    C       0.656    
 0            478    d.f.            14    g        0.665    Dxy     0.312    
 1            522    Pr(&gt; chi2) &lt;0.0001    gr       1.945    gamma   0.314    
max |deriv| 3e-06                          gp       0.156    tau-a   0.156    
                                           Brier    0.231    
&gt; anova(g)
                Wald Statistics          Response: y 

 Factor                                     Chi-Square d.f. P     
 treat  (Factor+Higher Order Factors)        5.62      10   0.8462
  All Interactions                           1.30       8   0.9956
 age  (Factor+Higher Order Factors)         65.99      12   &lt;.0001
  All Interactions                           1.30       8   0.9956
  Nonlinear (Factor+Higher Order Factors)    2.23       9   0.9872
 treat * age  (Factor+Higher Order Factors)  1.30       8   0.9956
  Nonlinear                                  0.99       6   0.9858
  Nonlinear Interaction : f(A,B) vs. AB      0.99       6   0.9858
 TOTAL NONLINEAR                             2.23       9   0.9872
 TOTAL NONLINEAR + INTERACTION               2.57      11   0.9953
 TOTAL                                      69.06      14   &lt;.0001
</code></pre>

<p><strong>Here are my questions:</strong><br>
For the object <code>g</code>,  </p>

<ul>
<li>What does the <code>max |deriv| 3e-06</code> mean?  </li>
<li>What do the Discrimination and Rand Discrim. Indexes suggest?  </li>
</ul>

<p>For the <code>anova(g)</code> object,  </p>

<ul>
<li>What's the <code>Factor +Higher Order Factors</code> for the treat?  </li>
<li>Why there are two <code>all interactions</code>? How to explain the nonlinear parts?</li>
</ul>
"
"0.0484501583111509","0.0493463771219827"," 82139","<p>I want to predict if a customer is interested in a new product  and I use the randomForest package for that.</p>

<p>Target variable : factor (Yes or No) <em>so I use the randomForest for classification</em> :</p>

<pre><code>randomForest(x=train,y=labels_train,xtest=test, ytest=labels_test,  ntree=100)
</code></pre>

<p>Variables : </p>

<ul>
<li>the city (A,B,C)</li>
<li>Gender : Male / Female</li>
<li>Age classes : 18-25, 25-59 ,60 and +</li>
</ul>

<p><strong>Problem :</strong> only 40% of the residents of the city A are interested (same proportion for both gender and all age classes).</p>

<p>In this case, ALL the tree of the forest assign ""Not interested"" for the residents of this city.</p>

<p>According to the forest these residents have a probability of 0% to be interested by the product.</p>

<p><strong>Proposition :</strong> Using regression instead of classification.
In this way, each tree will declare 40%, and the vote is replace by an average wich give a probability
of 40%</p>

<p>Could you confirm it's correct to do that when the goal is not really to classify but to have a probability of interest,
in order to only contact people who have a probability higher than 30% for example.</p>
"
"0.0500391083360534","0.0637058989297032"," 82236","<p>I am using <code>earth package</code><a href=""http://cran.r-project.org/web/packages/earth/index.html"" rel=""nofollow"">earth: Multivariate Adaptive Regression Spline Models</a> regression to get a constant piecewise approximation of my data. I want to plot a band of confidence around it.  Does this make sense to estimate a confidence interval of the smoothed function? If yes how can I do this?</p>

<p>I know that confidence intervals cannot be calculated directly, since it is a non parametric regression but I want to have a coherent result like (plot a band of confidence around my smoothed function) what I can get using <code>lm</code> or <code>loess</code> smoothing.</p>

<p><strong>EDIT</strong> </p>

<p>I add some code to clarify my idea, Here what I would do if I am using linear regression or <code>loess</code>.</p>

<pre><code>set.seed(1)
x &lt;- rnorm(15)
dat &lt;- data.frame(x = x, y = c(x + rnorm(15)))
</code></pre>

<h3>Using lm</h3>

<pre><code>mod &lt;- lm(y ~ x,data=dat)
predfit &lt;- predict(mod,se=TRUE,interval=""confidence"")$fit
</code></pre>

<h3>Using loess</h3>

<pre><code>level=0.95
mod &lt;- loess(y~x,data=dat)
pred &lt;- predict(mod, se = TRUE)
y = pred$fit
    ci &lt;- pred$se.fit * qt(level / 2 + .5, pred$df)
data.frame(ymin = y - ci,
           ymax = y + ci)
</code></pre>

<p>My question how to get <code>ymin</code> and <code>ymax</code> if I use <code>earth</code> :</p>

<pre><code>library(earth)
mod = earth(y~x,data=dat)
</code></pre>
"
"0.0395593886064618","0.0201455741006345"," 82307","<p>I was working through the lab on ridge regression and LASSO in ISLR and I came across a strange behavior in the <code>cv.glmnet</code> function. When I followed the lab as written I got the following </p>

<pre><code>set.seed(1)
train &lt;- sample(1:nrow(x), nrow(x)/2)
test &lt;- (-train)
y.test &lt;- y[test]
set.seed(1)
cv.out &lt;- cv.glmnet(x[train,], y[train], lambda=grid, alpha=0)
plot(cv.out)
bestlam &lt;- cv.out$lambda.min
bestlam
[1] 231.013
</code></pre>

<p>For my own benefit I tried it using a different seed (<code>8675309</code>) and got back a different result. Any combination of setting the seeds resulted in different answers. I am assuming this has to do with how the 10-folds are changed with the different seeds, however the different <code>lambda.min</code> can vary so much I am concerned the package might not be stable. Am I missing something?</p>
"
"0.0740088392978143","0.0646095738380922"," 82509","<p>I have time series data on fish catches from 1950-2011. </p>

<p>I wish to implement a regression model with varying coefficients. I'm aware that cox models etc. exist and implementation via the <code>survival</code> package in R. My data is not survival data, it's just several variables with fish catches and year.</p>

<p>Is there a way in R to implement such models? I've yet to come across this but I don't think it's unreasonable to want to model such data without it being survival data.</p>

<p>I want to model <code>inlandfao</code> from <code>marinefao</code>. </p>

<p>Here is my data and some plots:</p>

<pre><code>fishdata &lt;- read.csv(""http://dl.dropbox.com/s/4w0utkqdhqribl4/fishdata.csv"", header=T)

require(reshape2)
require(ggplot2)
theme_set(theme_bw())
require(scales)

df2 &lt;- data.frame(cbind(year,totalmarinefao, totalinlandfao))
df2
dd &lt;- melt(df2, id.vars = ""year"")
dd
pp &lt;- ggplot(dd, aes(year, value, colour=variable)) + geom_point() + geom_line(size=1)
pp_final &lt;- pp +  xlab(""Year"") + ylab(""Catches (Tons)"") + ggtitle(""Time Series of Variables (1950-2011)"") 
pp_final
pp_final2 &lt;- pp_final +  scale_colour_discrete(name = ""Variable - Catches (FAO)"", breaks = c(""totalmarinefao"", ""totalinlandfao""),
                                               labels=c(""Marine"", ""Inland"")) + 
  scale_shape_discrete(name = ""Variable (FAO)"", breaks = c(""totalmarinefao"", ""totalinlandfao""), labels=c(""Marine"", ""Inland"")) + 
  scale_x_continuous(breaks=seq(1950,2011,10)) + scale_y_continuous(labels=comma)

pp_final2
pp_3 &lt;- pp_final2 + theme(axis.text.x  = element_text(vjust=1, size=16)) + theme(axis.title.x = element_text(size=20))
pp_4 &lt;- pp_3 + theme(axis.text.y = element_text(vjust=0, size=16)) + theme(axis.title.y = element_text(size=20, vjust=0.2))
pp_5 &lt;- pp_4 + theme(plot.title = element_text(lineheight=.8, face=""bold"", size=20))
pp_5

qplot(marinefao, inlandfao, data=fishdata, main=""Scatterplot of the Marine &amp; \n Inland 
fish Catches (Tons)"", xlab=""Marine Catches"", ylab=""Inland Catches"") + 
scale_x_continuous(labels = comma) + scale_y_continuous(labels = comma)
</code></pre>

<p>From these plots, a linear model isn't appropriate. I have fitted GAMs etc. to these data. </p>

<p>Let me more if you require details.</p>
"
"0.0740088392978143","0.0753778361444409"," 82698","<p>I've run a simulation study in order to determine type I error rate of a statistic.My simulation design includes threes factors as sample size (4 levels), test length or number of items (3 levels) and estimator (3 levels). The statistic is developed to measure person fit with test data in educational testing situation.I've replicated the analysis in each cell (i.e. the design is fully-crossed) 100 times.</p>

<p>Now, I have the results and type I error rates range from 0.005 to 0.105 (i.e. across the whole analysis). I want to analyze how factors affect type I error rate using something similar to ANOVA. I tried Beta Regression in R using <code>betareg</code> package but I received this error message:</p>

<blockquote>
  <p>invalid dependent variable, all observations must be in (0, 1)</p>
</blockquote>

<p>Any idea on how to determine the effect of design factors on type I error rate?</p>
"
"0.0559454238864459","0.056980288229819"," 82981","<p>I am exploring hierarchical logistic regression, using glmer from the lme4 package. To my understanding, one of the first steps in multilevel modeling is to estimate the degree of clustering of level-1 units within level-2 units, given by the intraclass correlation (to ""justify"" the additional cost of estimating parameters to account for the clustering). When I run a fully unconditional model with glmer</p>

<pre><code>fitMLnull &lt;- glmer(outcome ~ 1 + (1|level2.ID), family=binomial)
</code></pre>

<p>the glmer fit gives me the variance in the intercept for outcome at level-2, but the residual level-1 variance in outcome is nowhere to be found. I read somewhere (can't track it down now) that the residual level-1 variance is <em>not</em> estimated in HGLM (or at least in glmer). Is this true? If so, is there an alternative way to approximate the degree of clustering in the data? If not, how can I access this value to calculate the ICC?</p>
"
"0.0625488854200668","0.0637058989297032"," 83260","<p>I used the clogit function (from the survival package) to run a conditional logistic regression in R with a big dataset of 1:M matched pairs with n=300368964 and number of events= 39995.</p>

<pre><code>model &lt;- clogit(Alliance ~ OVB + CVC + BVB + strata(Strata), method=""exact"")    
</code></pre>

<p>I received following results:</p>

<pre><code>                 coef  exp(coef)   se(coef)       z Pr(&gt;|z|)    
OVB        -0.0498174  0.9514031  0.0166275  -2.996  0.00273 ** 
BVB         0.0277405  1.0281289  0.0304956   0.910  0.36300    
CVC         1.1709851  3.2251683  0.1089709  10.746  &lt; 2e-16 ***
EarlyStage -1.3215824  0.2667129  0.0205851 -64.201  &lt; 2e-16 ***
AvgVCSize   0.0087976  1.0088364  0.0002035  43.224  &lt; 2e-16 ***
NumberVC    0.0643579  1.0664740  0.0034502  18.653  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Rsquare= 0   (max possible= 0.001 )
Likelihood ratio test= 6511  on 6 df,   p=0
Wald test            = 6471  on 6 df,   p=0
Score (logrank) test = 6801  on 6 df,   p=0
</code></pre>

<p>Since Rsquare equals 0 and the test ratios seems very high, I tried to plot the results to check whether the model fits. But I wasn't able to plot it properly.</p>

<p>I would online many papers which use the ratio Prob > chi2 = 0 from Stata as test ratio to proof the model fit. </p>

<p>How could I calculate this ratio in R? Are there any other ways I could check the model fit of my clogit results?</p>

<p>I would appreciate any help.</p>

<p>Thanks you very much in advance.</p>
"
"0.027972711943223","0.0284901441149095"," 83364","<p>I have been reading a number of papers where researchers have created risk scores based on logistic regression models. Often they refer to ""<a href=""http://www.ncbi.nlm.nih.gov/pubmed/15122742"" rel=""nofollow"">Sullivan's method</a>"" but I have no access to this paper and the explanations provided are far from clear. I noticed that Dr. Harrell's excellent RMS package provides a nomogram function which is in a way similar to creating a risk score (albeit with a very pretty graphical output).</p>

<p>It seems after tinkering around with it that the way it works is by dividing the beta coefficients by the smallest beta coefficient and then multiplying a constant to create points for categorical variables. However I cannot for the life of me figure out what is going on with continuous variables. I've spent hours searching google without much luck, and I would appreciate if someone could shed some light on this for me. Thanks!</p>
"
"0.0634361479695551","0.0753778361444409"," 83400","<p>I have response variable count data that should be treated as quasipoisson or something similar.  This data also contains outliers which are important to the dataset.  I cannot find an r package that will let me do robust regression with quasipoisson-type distribution.  I could, however, collect weights for each model using robust regression and then try to use those weights in glm.  Note those weights are always > 0.  My problem is that I cannot find a clear definition of how weights are used in glm with family=quasipoisson.  It sounds similar but I can't figure out if actually does what I want it to do (e.g. downweight outlier response variables to decrease their impact).</p>

<p>The response data are numbers of moths caught daily at pheromone traps, and the predictor variables are weather data.  All variables are diffed to handle temporal autocorrelation.  The outliers are high numbers of migratory moths in response to cold front passages.</p>

<p>I just found this reference which explains that low weights are interpreted as representing observations with a high variance, which does make sense in my case.  But does that mean the effect is to reduce the impact of that observation on the regression? <a href=""http://r.789695.n4.nabble.com/weights-in-glm-PR-8720-td910336.html"" rel=""nofollow"">http://r.789695.n4.nabble.com/weights-in-glm-PR-8720-td910336.html</a></p>
"
"0.0484501583111509","0.0493463771219827"," 83446","<p>Suppose you want to find clusters based on a set of variables $Y$, and that you want to estimate the effects of some variables $X$ on membership in those clusters. Here is how I am doing it now.</p>

<p>Step 1: Perform model-based clustering on the variables $Y$ (using the <code>mclust</code> package for this).</p>

<p>Step 2: Optimize a multinomial regression model with cluster membership as the outcome variable.</p>

<p>It seems like there must be a better way in which the models are estimated simultaneously. Anyone know a good tool in R for this and, even better, a good set of references for (a) the statistical model that the package implements, and (b) how to use the package?</p>

<p>Thanks</p>
"
"0.122486986448998","0.136633931250005"," 83469","<p>I have a problem I've been going over and over for months to find the right statistical analysis method. I'm planning to execute the analysis in R, so any mention of appropriate packages is also appreciated. I appreciate any advice anyone can offer. Here's an analogy to my data.</p>

<p>Consider that I have 5 colleges. My hypothesis is that one particular college has awarded more degrees than the other colleges. So I want to test whether the number of degrees awarded by the different colleges differs significantly. My data includes 3 data sets as follows:</p>

<p>1)The total number of degrees earned by students from each college, since the opening of each respective college. This is not a random sample, but the actual count. All colleges have different opening dates, so they differ in age. I do have the age of each college if it turns out to be relevant. This data includes second, third, and fourth degrees earned by individual students in the count. So the data includes two columns (college name, and number of degrees awarded), and 5 rows (one for each college).</p>

<p>2)The number of degrees per student per school. I have the list of about 5000 students in the total data set, and the number of degrees each student obtained at each school. Some students attended more than one of the schools, and many earned degrees from more than one school. Every degree ever awarded by these five colleges is represented in this data set as well. This data set does not include students who attended, but never earned a degree at these five colleges. This data set has six columns (one for student ID, and five for college name), and about 5000 rows (one for each student). The totals for each college column are equal to the values in the first data set.</p>

<p>3)The total number of students attending each school over the 30 year period, including those who did not earn a degree. The total number of students, of course, varies at each school. So five rows (one for each college) and two columns (college name, and total students attended in the history of the college).  </p>

<p>So I have count data with unequal sample sizes. My categorical predictor needs to be the college, as that's what I'm interested in addressing. </p>

<p>I've considered different types of regression models, but I'm not sure if these would be most appropriate. For a regression model I guess my response would simply be the number of degrees for each school. Or maybe the proportion of degrees with respect to the total students per school. Would this be sufficient to correct for the unequal sample sizes? </p>

<p>Any thoughts on which regression model would be most appropriate? Binomial doesn't work, because I need to take into account students with more than one degree, so it's not a binomial response. I'm not sure what type of distribution should be assumed, as picking one to test against seems very arbitrary to me. </p>

<p>Would some type of contingency table be more appropriate? </p>

<p>Thanks in advance for any suggestions you can offer.</p>
"
"0.126175703579688","0.128509669926956"," 83493","<p>I have a data set on which I'm trying to do regression, and failing.</p>

<p>The situation:</p>

<ul>
<li>Thousands of battle robot operators are fighting battles among each other using battle robots.</li>
<li>Some battle robots are strong and powerful, and others are weak; the strong ones win more often and deal more damage.</li>
<li>Robot operators vary in skill, with the more skilled ones winning more often, and delivering more damage</li>
<li>We have some summary information about the outcomes of their battles, but not all of the details.</li>
<li>We know what battle robots they used in their battles, and how many times (including how many of those battles they won), and we know the total damage they dealt (of two kinds, damageA and damageB) in total</li>
<li>Some robots are better at inflicting damageA, while others damageB</li>
<li>For unknown battle robot operators based only on what robots they have used in battles (and how many times), we would like to estimate how much damage of each kind they would achieve, and what % of battles they have most likely won</li>
</ul>

<p>For example:</p>

<ul>
<li>John has used Robot A for 4 battles, and Robot B for 2 battles, and has dealt 240 units worth of DamageA</li>
<li>James has used Robot A for 1 battle, and Robot B for 10 battles, and has dealt 1010 units worth of DamageA</li>
</ul>

<p>I can therefore estimate that Robot A probably deals 10 units of Damage A per battle, while Robot B deals 100 units of Damage A per battle, and thus if asked to estimate Damage A dealt by Matthew who has only played each of the two robots for 2 battles each, will estimate at 220 == (10*2 + 100*2).</p>

<p>Unfortunately, the real data are not as clean and straightforward, probably because:</p>

<ul>
<li>There is a significant variance due to robot operator skill, e.g., a good operator could deal 20 units of damage with Robot A while a bad one only 5 units. </li>
<li>There is some random variance due to opponents drawn in case of a small sample (e.g. somebody draws a strong opponent and loses despite having a better robot than the opponent), although eventually it would even out</li>
<li>There may be some minor selection bias in that the best robot operators manage to pick the best robots to take into battle more often</li>
</ul>

<p>The real data set is available here (630k entries of known battle operator results):</p>

<p><a href=""http://goo.gl/YAJp4O"" rel=""nofollow"">http://goo.gl/YAJp4O</a></p>

<p>The data set is organized as follows, with one robot operator entry per row:</p>

<ul>
<li>Column 1 with no label - operator ID</li>
<li>battles - total battles this operator has participated in</li>
<li>victories - total battles this operator has won</li>
<li>defeats - total battles this operator has lost</li>
<li>damageA - total Damage A points inflicted</li>
<li>damageB - total Damage B points inflicted</li>
<li>130 pairs of columns as follows:
<ul>
<li>battles_[robotID] - battles using robot [robotID]</li>
<li>victories_[robotID] - victories attained using robot [robotID]</li>
</ul></li>
</ul>

<p>What I've done so far:</p>

<ul>
<li>Tried a couple of linear models using R <code>biglm</code> package which build a formula such as <code>damageA ~ 0 + battles_1501 + battles_4201 + ...</code> to try to get fitted ""expected"" values for each of the robots.</li>
<li>Same, but removing the forced origin intercept by not including <code>0 +</code> in the formula</li>
<li>Same, but also included the <code>victories_[robotID]</code> in the independent variables</li>
<li>Same as before, but only selecting those robot operators whose victory numbers are close to their defeat numbers</li>
<li>A linear regression model for <code>damageA ~ 0 + battles_1501 + battles_non_1501</code> where <code>battles_non_1501</code> are all the battles in robots other than robot model 1501. Then repeated for all the other robot types.</li>
</ul>

<p>I did sanity checks by looking at the predicted damageA and damageB values, as well as comparing the victories/battles ratio with the actual victories/battles ratio that we can actually precisely calculate for each of the robots.</p>

<p>In all cases while the results weren't completely off, they were sufficiently off to see that the model isn't quite working. For example, some robots achieved negative damage numbers which shouldn't really happen as you cannot do negative damage in a battle. </p>

<p>In case where I also used the known <code>victories_[robotID]</code> values in the formula, many of the <code>battle_[robotID]</code> coefficients ended up being somewhat large negative numbers, so I tried estimating for the ""average"" operator by <code>battle_[robotID] + victories_[robotID] / 2</code> but that also didn't give reasonable results.</p>

<p>I'm somewhat out of ideas now.</p>
"
"NaN","NaN"," 83522","<p>I just fitted a boosted regression coxph model:</p>

<pre><code>cox=gbm(Surv(periods, event) ~ grade + fico_range_low + revol_util + dti, data=notes)
</code></pre>

<p>However, I want to obtain the survival curve from the model similar to the <code>survfit()</code> function in the <code>survival</code> package.  Does anyone know how to obtain using the model from the <code>gbm</code> package?</p>
"
"0.0969003166223018","0.0986927542439653"," 83826","<p>I estimated a robust linear model in <code>R</code> with MM weights using the <code>rlm()</code> in the MASS package. `R`` does not provide an $R^2$ value for the model, but I would like to have one if it is a meaningful quantity. I am also interested to know if there is any meaning in having an $R^2$ value that weighs the total and residual variance in the same way that observations were weighted in the robust regression. My general thinking is that, if, for the purposes of the regression, we are essentially with the weights giving some of the estimates less influence because they are outliers in some way, then maybe for the purpose of calculating $r^2$ we should also give those same estimates less influence? </p>

<p>I wrote two simple functions for the $R^2$ and the weighted $R^2$, they are below. I also included the results of running these functions for my model which is called HI9.  EDIT: I found web page of Adelle Coster of UNSW that gives a formula for <code>R2</code> that includes the weights vector in calculating the calculation of both <code>SSe</code> and <code>SSt</code> just as I did, and asked her for a more formal reference: <a href=""http://web.maths.unsw.edu.au/~adelle/Garvan/Assays/GoodnessOfFit.html"">http://web.maths.unsw.edu.au/~adelle/Garvan/Assays/GoodnessOfFit.html</a> (still looking for help from Cross Validated on how to interpret this weighted $r^2$.)</p>

<pre><code>#I used this function to calculate a basic r-squared from the robust linear model
r2 &lt;- function(x){  
+ SSe &lt;- sum((x$resid)^2);  
+ observed &lt;- x$resid+x$fitted;  
+ SSt &lt;- sum((observed-mean(observed))^2);  
+ value &lt;- 1-SSe/SSt;  
+ return(value);  
+ }  
r2(HI9)  
[1] 0.2061147

#I used this function to calculate a weighted r-squared from the robust linear model
&gt; r2ww &lt;- function(x){
+ SSe &lt;- sum((x$w*x$resid)^2); #the residual sum of squares is weighted
+ observed &lt;- x$resid+x$fitted;
+ SSt &lt;- sum((x$w*(observed-mean(observed)))^2); #the total sum of squares is weighted      
+ value &lt;- 1-SSe/SSt;
+ return(value);
+ }
 &gt; r2ww(HI9)
[1] 0.7716264
</code></pre>

<p>Thanks to anyone who spends time answering this. Please accept my apologies if there is already some very good reference on this which I missed, or if my code above is hard to read (I am not a code guy).</p>
"
"0.027972711943223","0.0284901441149095"," 83852","<p>I have a dataset for vehicles and trying to predict what will fail. Here is my data set</p>

<pre><code>vId, MileageSincePartLastReplaced,  AgeOfPart, TypeOfVehicle, Failure

x,100000,200days,Truck,Alt Belt

y,200000,600days,PCar,Transmission Belt

z,140000,230days,Van,Fan Belt
</code></pre>

<p>Failure is outcome variable with 20 different types of failure categories. What I am looking for is given mileage driven and age of the part which one is likely to fail?</p>

<p>I am unable to figure out which model/models I should be looking for? I was looking into multinomial regression &amp; ordered logistic regression  but was not sure. Any help around how to go about this and which package I could use?  </p>

<p>Note: I asked same question in Stack Overflow and was suggested to move to this forum.</p>
"
"0.0395593886064618","0.040291148201269"," 84243","<p>I have made this linear regression model:</p>

<pre><code>mtcars_lm &lt;- lm(mpg ~ drat + hp, mtcars)
</code></pre>

<p>Using the effects package, I can predict values of <code>mpg</code> for every value of <code>hp</code> between 70 and 150, plus get a confidence interval for each value of <code>mpg</code>:</p>

<pre><code>library(effects)
as.data.frame(effect(""hp"", mtcars_lm, xlevels = list(hp=seq(70, 150, 10))))

   hp      fit        se    lower    upper
1  70 24.06201 0.9066002 22.20781 25.91622
2  80 23.54415 0.8355302 21.83530 25.25300
3  90 23.02628 0.7691351 21.45322 24.59934
4 100 22.50841 0.7087300 21.05890 23.95793
5 110 21.99055 0.6559716 20.64894 23.33216
6 120 21.47268 0.6128381 20.21929 22.72608
7 130 20.95481 0.5814753 19.76556 22.14407
8 140 20.43695 0.5638509 19.28374 21.59015
9 150 19.91908 0.5612604 18.77118 21.06699
</code></pre>

<p>And I can plot the result:</p>

<pre><code>plot(effect(""hp"", mtcars_lm, xlevels = list(hp=seq(70, 150, 10))))
</code></pre>

<p><img src=""http://i.stack.imgur.com/KEmDi.jpg"" alt=""enter image description here""></p>

<p>My question is: what is the maths behind the calculation of the standard error and confidence interval for every value of <code>mpg</code>?</p>
"
"0.027972711943223","0.0284901441149095"," 84334","<p>I have a dataset for vehicles and trying to predict what will fail. Here is my data set</p>

<pre><code>vId, MileageSincePartLastReplaced,  AgeOfPart, TypeOfVehicle, Failure

x,100000,200days,Truck,Alt Belt

y,200000,600days,PCar,Transmission Belt

z,140000,230days,Van,Fan Belt
</code></pre>

<p>Failure is outcome variable with 20 different types of failure categories. What I am looking for is given mileage driven and age of the part which one is likely to fail?</p>

<p>I am unable to figure out which model/models I should be looking for? I was looking into multinomial regression but was not sure. Any help around how to go about this and which package I could use?  </p>
"
"0.0740088392978143","0.0753778361444409"," 85555","<p>I would like to run a lagged random effects regression.</p>

<p>The data is from an experiment in which participants were assigned to groups of five and participated in an interactive game for 20 rounds.</p>

<p>Participants could exchange something during the experiment, which is the dependent variable.</p>

<p>Now I would like to predict/explain, how much participant received from other participants based on the behaviour of previous rounds.</p>

<p>Since the data is clustered on three levels: subject, group and time (rounds), I am a little bit lost how to correctly formulate the model.</p>

<p>I am currently using the lme4 package in R. 
I transformed the dependent variable to a 0/1 (nothing received/something received) variable, due to high skewness, so I would need to specify a multilevel logistic model.</p>

<p>So far, I specified and ran the following models:</p>

<pre><code>glmer(DV ~ predictors* + (1 + round * subject | group), family = binomial)
</code></pre>

<p>and:</p>

<pre><code>glmer(DV ~ predictors* + (1 + round * group | subject), family = binomial)
</code></pre>

<p>*predictors are on subject-level.</p>

<p>I get similar (although not the same) estimates for both models, however in model1, z-values are much higher (and therefore p-values much lower).</p>

<p>Can someone help me on that?</p>

<p>What I want to know is; Can previous behaviour (that is behaviour from round x-1 etc.) predict how much a participant received in round x.
But control/acknowledge that participants are clustered in groups and that behaviour is correlated over time (rounds).</p>
"
"0.0395593886064618","0.040291148201269"," 85586","<p>No regular here will be unaware of the perils of using stepwise and similar automatic methods for variable selection in regression analysis. But preferred alternatives, such as the lasso or elasticnet, have there own difficulties.</p>

<p>I can't find anywhere in the archive here a discussion of the methods provided by the <strong>subselect</strong> package in R, which I've just come across - the package has, so far as I can see, existed for a decade and more, and presumably has proved useful.</p>

<p>In addition to a variation on the leaps procedure, <strong>subselect</strong> offers three algorithms (which it calls anneal, genetic and improve) for variable selection for different kinds of analysis. </p>

<p>Have these procedures (or any of them) proved to be of value in variable selection? </p>
"
"0.111890847772892","0.106838040430911"," 86273","<p>I'm trying to calculate the log-likelihood for a generalized nonlinear least squares regression for the function $f(x)=\frac{\beta_1}{(1+\frac x\beta_2)^{\beta_3}}$ optimized by the <code>gnls</code> function in the R package <code>nlme</code>, using the variance covariance matrix generated by distances on a a phylogenetic tree assuming Brownian motion (<code>corBrownian(phy=tree)</code> from the <code>ape</code> package). The following reproducible R code fits the gnls model using x,y data and a random tree with 9 taxa:</p>

<pre><code>require(ape)
require(nlme)
require(expm)
tree &lt;- rtree(9)
x &lt;- c(0,14.51,32.9,44.41,86.18,136.28,178.21,262.3,521.94)
y &lt;- c(100,93.69,82.09,62.24,32.71,48.4,35.98,15.73,9.71)
data &lt;- data.frame(x,y,row.names=tree$tip.label)
model &lt;- y~beta1/((1+(x/beta2))^beta3)
f=function(beta,x) beta[1]/((1+(x/beta[2]))^beta[3])
start &lt;- c(beta1=103.651004,beta2=119.55067,beta3=1.370105)
correlation &lt;- corBrownian(phy=tree)
fit &lt;- gnls(model=model,data=data,start=start,correlation=correlation)
logLik(fit) 
</code></pre>

<p>I would like to calculate the log-likelihood ""by hand"" (in R, but without use of the <code>logLik</code> function) based on the estimated parameters obtained from <code>gnls</code> so it matches the output from <code>logLik(fit)</code>. NOTE: I am not trying to estimate parameters; I just want to calculate log-likelihood of the parameters estimated by the <code>gnls</code> function (although if someone has a reproducible example of how to estimate parameters without <code>gnls</code>, I would be very interested in seeing it!). </p>

<p>I'm not really sure how to go about doing this in R. The linear algebra notation described in Mixed-Effects Models in S and S-Plus (Pinheiro and Bates) is very much over my head and none of my attempts have matched <code>logLik(fit)</code>. Here are the details described by Pinheiro and Bates:</p>

<p>The log-likelihood for the generalized nonlinear least squares model  $y_i=f_i(\phi_i,v_i)+\epsilon_i$ where $\phi_i=A_i\beta$ is calculated as follows:</p>

<p>$l(\beta,\sigma^2,\delta|y)=-\frac 12 \Bigl\{ N\log(2\pi\sigma^2)+\sum\limits_{i=1}^M{\Bigl[\frac{||y_i^*-f_i^*(\beta)||^2}{\sigma^2}+\log|\Lambda_i|\Bigl]\Bigl\}}$</p>

<p>where $N$ is the number of observations, and $f_i^*(\beta)=f_i^*(\phi_i,v_i)$.</p>

<p>$\Lambda_i$ is positive-definite, $y_i^*=\Lambda_i^{-T/2}y_i$ and $f_i^*(\phi_i,v_i)=\Lambda_i^{-T/2}f_i(\phi_i,v_i)$</p>

<p>For fixed $\beta$ and $\lambda$, the ML estimator of $\sigma^2$ is </p>

<p>$\hat\sigma(\beta,\lambda)=\sum\limits_{i=1}^M||y_i^*-f_i^*(\beta)||^2 / N$</p>

<p>and the profiled log-likelihood is</p>

<p>$l(\beta,\lambda|y)=-\frac12\Bigl\{N[\log(2\pi/N)+1]+\log\Bigl(\sum\limits_{i=1}^M||y_i^*-f_i^*(\beta)||^2\Bigl)+\sum\limits_{i=1}^M\log|\Lambda_i|\Bigl\}$</p>

<p>which is used with a Gauss-Seidel algorithm to find the ML estimates of $\beta$ and $\lambda$. A less biased estimate of $\sigma^2$ is used:</p>

<p>$\sigma^2=\sum\limits_{i=1}^M\Bigl|\Bigl|\hat\Lambda_i^{-T/2}[y_i-f_i(\hat\beta)]\Bigl|\Bigl|^2/(N-p)$</p>

<p>where $p$ represents the length of $\beta$.</p>

<p>I have compiled a list of specific questions that I am facing:</p>

<ol>
<li>What is $\Lambda_i$? Is it the distance matrix produced by <code>big_lambda &lt;- vcv.phylo(tree)</code> in <code>ape</code>, or does it need to be somehow transformed or parameterized by $\lambda$, or something else entirely?</li>
<li>Would $\sigma^2$ be <code>fit$sigma^2</code>, or the equation for the less biased estimate (the last equation in this post)?</li>
<li>Is it necessary to use $\lambda$ to calculate log-likelihood, or is that just an intermediate step for parameter estimation? Also, how is $\lambda$ used? Is it a single value or a vector, and is it multiplied by all of $\Lambda_i$ or just off-diagonal elements, etc.?</li>
<li>What is $||y-f(\beta)||$? Would that be <code>norm(y-f(fit$coefficients,x),""F"")</code> in the package <code>Matrix</code>? If so, I'm confused about how to calculate the sum $\sum\limits_{i=1}^M||y_i^*-f_i^*(\beta)||^2$, because <code>norm()</code> returns a single value, not a vector.</li>
<li>How does one calculate $\log|\Lambda_i|$? Is it <code>log(diag(abs(big_lambda)))</code> where <code>big_lambda</code> is $\Lambda_i$, or is it <code>logm(abs(big_lambda))</code> from the package <code>expm</code>? If it is <code>logm()</code>, how does one take the sum of a matrix (or is it implied that it is just the diagonal elements)?</li>
<li>Just to confirm, is $\Lambda_i^{-T/2}$ calculated like this: <code>t(solve(sqrtm(big_lambda)))</code>?</li>
<li>How are $y_i^*$ and $f_i^*(\beta)$ calculated? Is it either of the following:</li>
</ol>

<p><code>y_star &lt;- t(solve(sqrtm(big_lambda))) %*% y</code></p>

<p>and</p>

<p><code>f_star &lt;- t(solve(sqrtm(big_lambda))) %*% f(fit$coefficients,x)</code></p>

<p>or would it be</p>

<p><code>y_star &lt;- t(solve(sqrtm(big_lambda))) * y</code></p>

<p>and</p>

<p><code>f_star &lt;- t(solve(sqrtm(big_lambda))) * f(fit$coefficients,x)</code> ?</p>

<p>If all of these questions are answered, in theory, I think the log-likelihood should be calculable to match the output from <code>logLik(fit)</code>. Any help on any of these questions would be greatly appreciated. If anything needs clarification, please let me know. Thanks!</p>

<p><strong>UPDATE</strong>: I have been experimenting with various possibilities for the calculation of the log-likelihood, and here is the best I have come up with so far. <code>logLik_calc</code> is consistently about 1 to 3 off from the value returned by <code>logLik(fit)</code>. Either I'm close to the actual solution, or this is purely by coincidence. Any thoughts?</p>

<pre><code>  C &lt;- vcv.phylo(tree) # variance-covariance matrix
  tC &lt;- t(solve(sqrtm(C))) # C^(-T/2)
  log_C &lt;- log(diag(abs(C))) # log|C|
  N &lt;- length(y)
  y_star &lt;- tC%*%y 
  f_star &lt;- tC%*%f(fit$coefficients,x)
  dif &lt;- y_star-f_star  
  sigma_squared &lt;-  sum(abs(y_star-f_star)^2)/N
  # using fit$sigma^2 also produces a slightly different answer than logLik(fit)
  logLik_calc &lt;- -((N*log(2*pi*(sigma_squared)))+
       sum(((abs(dif)^2)/(sigma_squared))+log_C))/2
</code></pre>
"
"NaN","NaN"," 86432","<p>I'm using 'betareg' package in R to perform beta regression. predict() function with se.fit=T is supposed to return standard errors along with the prediction but it doesn't. Is there any other way I can get the standard error outputs?</p>

<p>I'm open to using other packages that can perform beta regression too.</p>
"
"0.131203648943494","0.133630620956212"," 87278","<p>I don't know if a similar problem has been asked before so if it has been, please provide me a link to the related/duplicate questions. I am sorry if I seem to be asking too much. But I really like to learn this stuff and this seems to be a good place to start asking.</p>

<p>I have been teaching myself statistics through self-study and I found Logan's <a href=""http://as.wiley.com/WileyCDA/WileyTitle/productCd-1405190086.html"" rel=""nofollow""><strong>Biostatistical Design and Analysis Using R</strong></a> very helpful in that it shows how the actual computations are done (in R) and how the results are interpreted. I particularly like the part about multiple regression (Chapter 9). I use R since it is the most accessible (and free) software that I can get my hands into. </p>

<p>Right now, I am trying to learn multivariate multiple regression. But unfortunately, I can't find a good resource. My specific problem is finding the best linear model for each response variable for the following morphometric data of a plant species (that some of my high school biology students are investigating), where <code>Leaves</code>, <code>CorL</code>, <code>CorD</code>, <code>FilL</code>, <code>AntL</code>, <code>AntW</code>, <code>StaL</code>, <code>StiW</code>, and <code>HeiP</code> are the response variables and <code>pH</code>, <code>OM</code>, <code>P</code>, <code>K</code> (nutrient variables), <code>Elev</code>, <code>SoilTemp</code>, and <code>AirTemp</code> (environment variables) are the independent variables.</p>

<p>I don't know if it is okay to proceed as in the case of only one dependent variable, but I went through the steps of Example 9B of Logan anyhow.</p>

<h3>lily.csv</h3>

<pre><code>Leaves,CorL,CorD,FilL,AntL,AntW,StaL,StiW,HeiP,Elev,pH,OM,P,K,SoilTemp,AirTemp
55,213.4,114.6,170.3,10.6,2.35,210,6.7,0.93,1431,6.37,1,3,170,29,26
44,192.15,95.25,160.6,7.1,2.25,176.4,6.55,0.79,1471,6.02,1,0,180,25,23
38,156.75,95.5,155.2,5.65,1.8,170.9,4.4,0.78,1471,6.02,1,0,180,25,23
29,191.8,88.35,155.2,10,2.5,178.25,5.9,0.75,1464,5.99,1,3,150,25,22
36,200.85,99.4,161.9,6.5,1.55,187.4,6.15,0.8,1464,5.99,1,3,150,25,22
43,210.2,74,147,7,1,170,5,0.8,1464,5.99,1,3,150,25,22
34,183.2,97.3,149.5,6.9,1.85,168.8,5.45,0.71,1464,5.99,1,3,150,25,22
52,233.3,107.7,179.6,9.2,3.05,210,6.45,0.82,1464,5.99,1,3,150,25,22
43,205.7,108.8,164.6,9.4,2,190.9,5.15,0.66,1464,5.99,1,3,150,25,22
28,203.15,119.35,160.6,8.9,2.3,180,6.85,0.77,1503,5.98,3,2,240,29.5,25.5
45,188.85,100.5,150.6,6.4,2.3,174.85,7.7,0.84,1503,5.98,3,2,240,29.5,25.5
49,205.2,126.85,150.8,10.1,2.8,177.5,9,0.84,1487,6.09,4,4,180,26,25
35,187.7,102.35,142.1,5.55,1.85,175.35,5.75,0.56,1485,6.17,3.5,1,220,24,23
23,181.05,94.6,136.6,6.9,1.8,169.3,5.8,0.59,1485,6.17,3.5,1,220,24,23
31,172.5,63.7,113.6,5.2,1.5,151.2,4.7,0.57,1482,6.29,5,2,280,24,23
34,190.5,93.1,151.9,5.65,1.85,172.5,5.25,0.68,1482,6.29,5,2,280,24,23
41,185.85,85.2,148.6,5.9,1.05,169.6,5.9,0.62,1472,6.48,0.5,3,170,25.22,22.89
29,195,159.2,159.3,15,4,185,6.3,0.59,1472,6.48,0.5,3,170,25.22,22.89
31,115.6,108.6,165.8,8.5,3,200.5,7.5,0.83,1454,5.53,5,14,350,25.22,22.89
27,176.65,93.1,128.65,6.65,2.85,180.5,6.65,0.53,1454,5.53,5,14,350,25.22,22.89
33,210,119,148,7,3,193,6,0.62,1454,5.53,5,14,350,25.22,22.89
42,200,93,166,18.3,4.55,177,8,1.12,1454,5.53,5,14,350,25.22,22.89
42,205,101.4,166.8,9,2.5,190,8.2,1.12,1454,5.53,5,14,350,25.22,22.89
25,192.9,94.15,147.8,6.45,2.3,167.65,7.15,0.61,1445,5.59,4,7,260,25.22,22.89
36,187.95,65.05,150.2,6.55,2.7,177.5,6.55,0.52,1445,5.59,4,7,260,25.22,22.89
32,110.4,11.6,168.15,7.6,2,197.95,7.85,0.73,1481,6.29,1.5,1,80,25.22,22.89
29,185,80,143,9,2,179,7.5,0.69,1481,6.29,1.5,1,80,25.22,22.89
29,179.8,70.6,134.8,11.15,3.2,165.65,5.3,0.6,1481,6.29,1.5,1,80,25.22,22.89
</code></pre>

<p>Firstly, I tried to investigate for possible collinearity among the variables.</p>

<pre><code>library(car)
lily = read.csv(""lily.csv"",header=T)
scatterplotMatrix(lily,diag=""boxplot"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/Ytsnd.png"" alt=""enter image description here""></p>

<p><code>FilL</code>, <code>AntL</code>, <code>AntW</code>, and <code>HeiP</code> seem to be non-normal so I made <code>log10</code> transformations. This <em>seems</em> to work fine. (And it is fine for you to educate me at this point if I am doing it wrong. I'd appreciate it very much.)</p>

<pre><code>scatterplotMatrix(~Elev + pH + OM + P + K + SoilTemp + AirTemp +
AirTemp + Leaves + CorL + CorD + log10(FilL) + log10(log10(log10(AntL)+0.1)+0.1) + 
log10(AntW) + StaL + StiW + log10(HeiP),data=lily,diag=""boxplot"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/t9nmI.png"" alt=""enter image description here""></p>

<p>I check for multicolinearity among the independent variables.</p>

<pre><code>&gt; cor(lily[,10:16])
                Elev          pH          OM           P           K
Elev      1.00000000  0.48252995 -0.06601928 -0.56726786 -0.28159580
pH        0.48252995  1.00000000 -0.58587694 -0.81673123 -0.70434283
OM       -0.06601928 -0.58587694  1.00000000  0.65931857  0.86478172
P        -0.56726786 -0.81673123  0.65931857  1.00000000  0.79782480
K        -0.28159580 -0.70434283  0.86478172  0.79782480  1.00000000
SoilTemp  0.14558365  0.01543524 -0.10436250 -0.05023853 -0.01041523
AirTemp   0.26450883  0.15711849  0.16862694 -0.09735977  0.11655030
            SoilTemp     AirTemp
Elev      0.14558365  0.26450883
pH        0.01543524  0.15711849
OM       -0.10436250  0.16862694
P        -0.05023853 -0.09735977
K        -0.01041523  0.11655030
SoilTemp  1.00000000  0.83202496
AirTemp   0.83202496  1.00000000
</code></pre>

<p>Among the independent variables, pairs <code>P</code> and <code>pH</code>, <code>K</code> and <code>pH</code>, <code>P</code> and <code>K</code>, <code>OM</code> and <code>K</code>, and <code>SoilTemp</code> and <code>AirTemp</code> have strong collinearity. </p>

<p>I also checked for collinearity among the dependent variables although I don't have an idea if this is a alright.</p>

<pre><code>&gt; cor(lily[,1:9])
            Leaves        CorL       CorD      FilL      AntL        AntW
Leaves 1.000000000  0.44495257 0.17903019 0.5222644 0.1495016 0.004680606
CorL   0.444952572  1.00000000 0.51084625 0.1319070 0.2101801 0.097530007
CorD   0.179030187  0.51084625 1.00000000 0.2368117 0.3297344 0.376806953
FilL   0.522264352  0.13190704 0.23681171 1.0000000 0.3932006 0.284738542
AntL   0.149501570  0.21018008 0.32973443 0.3932006 1.0000000 0.796401542
AntW   0.004680606  0.09753001 0.37680695 0.2847385 0.7964015 1.000000000
StaL   0.416083096  0.06574503 0.23272070 0.7762797 0.2701401 0.318744025
StiW   0.194927129 -0.05594094 0.08322138 0.3752195 0.3755628 0.445964273
HeiP   0.577737137  0.17603412 0.13911530 0.6348948 0.4583508 0.254173681
             StaL        StiW      HeiP
Leaves 0.41608310  0.19492713 0.5777371
CorL   0.06574503 -0.05594094 0.1760341
CorD   0.23272070  0.08322138 0.1391153
FilL   0.77627970  0.37521953 0.6348948
AntL   0.27014013  0.37556279 0.4583508
AntW   0.31874403  0.44596427 0.2541737
StaL   1.00000000  0.38306631 0.3794643
StiW   0.38306631  1.00000000 0.5039679
HeiP   0.37946433  0.50396793 1.0000000
</code></pre>

<p>From here, I can check for variance inflation and their inverses and possibly investigate interactions but I am really not sure now how to proceed or if it is alright at all to do these things in the multivariate case. And it seems to be a long way still to assessing the best multivariate model. In the case of the one dependent variable case, I can use the <code>MuMIn</code> package to automate the determination of the best fit but it doesn't work in the multiple response  case.</p>

<p>How do I proceed from this point? I will also appreciate it very much if you can point me to a good book or online material (preferably with applications in R).</p>
"
"0.131203648943494","0.133630620956212"," 87359","<p><strong>Background</strong></p>

<p>I have a large dataset that contains three binary outcomes for individuals belonging to groups. I am interested in jointly modeling these binary outcomes because I have reason to believe they are positively correlated with one another. Most of my data is at the individual level, however I also have some group-level information.</p>

<p>Because of the structure of my data, I am treating this as a 3-level logistic regression. The first level defines the multivariate structure through dummy variables that indicate for each outcome. Therefore level-1 accounts for the within-individual measurements. Level-2 provides the between-individuals variances and level-3 gives the between-group variance.</p>

<p><em>Hypothetical Example:</em></p>

<p>Suppose I have data for students in classrooms. I want to examine whether certain student level characteristics are important predictors for passing three different pre-tests (math, history, and gym). The pre-tests are constructed such that about half of the students should pass each exam (no floor or ceiling effect). Since some students are better than others, I expect whether or not they pass their history exam to be correlated to their probability of passing their math and gym pre-tests. I also expect that students in the same classroom will perform more similarly than students across classrooms.</p>

<p><strong>Here is my attempt at writing out the model</strong></p>

<p>I use $h$ to index level-1, $i$ to index level-2, and $j$ to index level-3. Recall that level-1 corresponds to within-individual, level-2 corresponds to between-individuals, and level-3 corresponds to between-groups. So I have $h$ measures for the $i^\text{th}$ individual in the $j^\text{th}$ group.</p>

<p>Let 
\begin{align}
\alpha_{1ij} &amp;= 1 \text{ if outcome}_1 = 1 \text{ and 0 otherwise} \\
\alpha_{2ij} &amp;= 1 \text{ if outcome}_2 = 1 \text{ and 0 otherwise} \\
\alpha_{3ij} &amp;= 1 \text{ if outcome}_3 = 1 \text{ and 0 otherwise}
\end{align}</p>

<p>\begin{align}
\text{log}\left( \frac{\pi_{hij}}{1 - \pi_{hij}} \right) &amp;= \alpha_{0hij} + \alpha_{1hij}Z_{ij} + \eta_h \\
Z_{ij} &amp;= \beta_{0ij} + X_{ij}\beta_{ij} + U_{j} + \epsilon_i \\
U_{j} &amp;= \gamma_{0j} + X_{j}\gamma_{j}
 + \rho_j
\end{align}</p>

<p>Please leave suggests about this notation; I am not positive that it is correct.</p>

<p><strong>Trying to specify the model in R</strong></p>

<p>I have been using R 3.0.1, but am open to solutions using other standard software (e.g. SAS, Stata, WinBUGs, etc.). Since I am modeling a binary response I am using the <code>glmer</code> function in the <code>lme4</code> package.</p>

<p>My data is in a long format with one row per outcome per individual per group.</p>

<p>One of my current problems is correctly specifying a 3-level model. Given 5 individual-level measures and one group level measure, I have tried to specify the model as:</p>

<pre><code>&gt; glmer(pi ~ outcome1:(x1 + x2 + x3 + x4 + u5) + 
             outcome2:(x1 + x2 + x3 + x4 + u5) +
             outcome3:(x1 + x2 + x3 + x4 + u5) +
             (1 + x1 + x2 + x3 + x4 | individual) +
             (1 + u5 | group), family=binomial, data=pretest)
</code></pre>

<p>but this often produces warnings and does not converge.</p>

<p><strong>My questions</strong></p>

<ol>
<li><p>Does my approach make sense? (i.e. does it make sense to model a multilevel multivariate model by specifying the multivariate structure in the first level?)</p></li>
<li><p>I have difficult with the proper notation and appreciate suggestions for clarifying my notation.</p></li>
<li><p>Am I using the right tools for this problem? Should I be using other packages or software?</p></li>
</ol>

<p>Thanks in advance for your suggestions and advice.</p>
"
"0.115334445976885","0.117467873474841"," 87487","<p><strong>Short version</strong></p>

<p>Is there a difference <strong>per treatment</strong> given time and this dataset?</p>

<p><strong>Or</strong> if the difference we're trying to demonstrate is important, what's the best method we have for teasing this out?</p>

<p><strong>Long version</strong></p>

<p>Ok, sorry if a bit <em>biology 101</em> but this appears to be an edge case where the data and the model need to line up in the right way in order to draw some conclusions. </p>

<p>Seems like a common issue... Would be nice to demonstrate an intuition rather than repeating this experiment with larger sample sizes. </p>

<p>Let's say I have this graph, showing mean +- std. error:</p>

<p><img src=""http://i.stack.imgur.com/eIKeF.png"" alt=""p1""></p>

<p>Now, it looks like there's a difference here. Can this be justified (avoiding Bayesian approaches)?</p>

<p>The simpleminded man's  approach would be to take Day 4 and apply a <em>t-test</em> (as usual: 2-sided, unpaired, unequal variance), but this doesn't work in this case. It appears the variance is too high as we only had 3x measurements per time-point (err.. mostly my design, p = 0.22).</p>

<p><strong>Edit</strong> On reflection the next obvious approach would be ANOVA on a linear regression. Overlooked this on first draft. This also doesn't seem like the right approach as the usual linear model is impaired from heteroskedasticity (<em>exaggerated variance over time</em>). <strong>End Edit</strong></p>

<p>I'm guessing there's a way to include <strong>all</strong> the data which would fit a simple (1-2 parameter) model of growth over time per predictor variable then compare these models using some formal test. </p>

<p>This method should be justifiable yet accessible to a relatively unsophisticated audience.</p>

<p>I have looked at <code>compareGrowthCurves</code> in <a href=""http://cran.r-project.org/web/packages/statmod/statmod.pdf"" rel=""nofollow"">statmod</a>, read about <a href=""http://www.jstatsoft.org/v33/i07/paper"" rel=""nofollow"">grofit</a> and tried a linear mixed-effects model adapted from <a href=""http://stats.stackexchange.com/questions/61153/nlme-regression-curve-comparison-in-r-anova-p-value"">this question on SE</a>. This latter is closest to the bill, although in my case the measurements are not from the <strong>same subject</strong> over time so I'm not sure mixed-effects/multilevel models are appropriate. </p>

<p>One sensible approach would be to model the rate of growth per time as linear and fixed and have the random effect be <strong>Tx</strong> then <a href=""http://www.statistik.uni-dortmund.de/useR-2008/slides/Scheipl+Greven+Kuechenhoff.pdf"" rel=""nofollow"">test it's significance</a>, although I gather there's <a href=""http://glmm.wikidot.com/faq"" rel=""nofollow"">some debate</a> about the merits of such an approach.</p>

<p>(Also this method specifies a linear model which would not appear to be the best way to model a comparison of growth which in the case of one predictor has not yet hit an upper boundary and in the other appears basically static. I'm guessing there's a generalized mixed-effects model approach to this difficulty which would be more appropriate.)</p>

<p>Now the code:</p>

<pre><code>df1 &lt;- data.frame(Day = rep(rep(0:4, each=3), 2),
              Tx = rep(c(""Control"", ""BHB""), each=15),
              y = c(rep(16e3, 3),
              32e3, 56e3, 6e3,
              36e3, 14e3, 24e3,
              90e3, 22e3, 18e3,
              246e3, 38e3, 82e3,
              rep(16e3, 3),
              16e3, 34e3, 16e3,
              20e3, 20e3, 24e3,
              4e3, 12e3, 16e3,
              20e3, 5e3, 12e3))
### standard error
stdErr &lt;- function(x) sqrt(var(x)) / sqrt(length(x))
library(plyr)
### summarise as mean and standard error to allow for plotting
df2 &lt;- ddply(df1, c(""Day"", ""Tx""), summarise,
             m1 = mean(y),
             se = stdErr(y) )
library(ggplot2)
### plot with position dodge
pd &lt;- position_dodge(.1)
ggplot(df2, aes(x=Day, y=m1, color=Tx)) +
 geom_errorbar(aes(ymin=m1-se, ymax=m1+se), width=.1, position=pd) +
 geom_line(position=pd) +
 geom_point(position=pd, size=3) +
 ylab(""No. cells / ml"")
</code></pre>

<p>Some formal tests:</p>

<pre><code>### t-test day 4
with(df1[df1$Day==4, ], t.test(y ~ Tx))
### anova
anova(lm(y ~ Tx + Day, df1))
### mixed effects model
library(nlme)
f1 &lt;- lme(y ~ Day, random = ~1|Tx, data=df1[df1$Day!=0, ])
library(RLRsim)
exactRLRT(f1)
</code></pre>

<p>this last giving</p>

<pre><code>    simulated finite sample distribution of RLRT.  (p-value based on 10000
    simulated values)

data:  
RLRT = 1.6722, p-value = 0.0465
</code></pre>

<p>By which I conclude that the probability of this data (or something more extreme), <em>given the null hypothesis that there is no influence of <strong>treatment</strong> on <strong>change over time</em></strong> is close to the elusive 0.05. </p>

<p>Again, sorry if this appears a bit basic but I feel a case like this could be used to illustrate the importance of modelling in avoiding further needless experimental repetition. </p>
"
"0.0969003166223018","0.0986927542439653"," 87650","<p>I ran into (what I think is) an inconsistency when running a random-intercept model (using the <em>lmer</em> function in the <em>lme4</em> package in R).</p>

<p>Here is what I do: I first run a model with a set of covariates; then I run the same model re-scaling (linearly transforming) one of the regressors. To my knowledge, this should change <strong>only the coefficient</strong> of the variable that is linearly transformed. And indeed, this is what happens when I run this ""experiment"" with a simple linear regression model and with a logistic model.</p>

<p>This code replicates the ""normal"" behaviour:</p>

<pre><code># Create three random independent variables
x1 &lt;- rnorm(20)
x2 &lt;- rnorm(20)
x3 &lt;- as.factor(sample(0:2, 20, replace = TRUE))
# Their random coefficients
coef1 &lt;- runif(1, -1, 1)
coef2 &lt;- runif(1, -1, 1)
# Create a continuous dependent variable and a binomial one
y1 &lt;- coef1 * x1 + coef2 * x2 + runif(20)
y2 &lt;- y1
y2[which(y1 &gt; quantile(y1, 0.5))] &lt;- 1
y2[which(y1 &lt;= quantile(y1, 0.5))] &lt;- 0
# Finally, a linear transformation of x1
x1.trans &lt;- x1*3
</code></pre>

<p>So, let us run an OLS model:</p>

<pre><code>lm &lt;- lm(y1 ~ x1 + x2 + x3)
summary(lm)
# OLS model with one variable linearly transformed
lm.bis &lt;- lm(y1 ~ x1.trans + x2 + x3)
summary(lm.bis)
</code></pre>

<p>The coefficients of <em>x1</em> and <em>x1.trans</em> are different, <strong>but the R-square of the two models is the same</strong>:</p>

<pre><code>summary(lm)$r.sq == summary(lm.bis)$r.sq
</code></pre>

<p>The same with a logistic model:</p>

<pre><code>logm &lt;- glm(y2 ~ x1 + x2, family=""binomial"")
summary(logm)
logm.bis &lt;- glm(y2 ~ x1.trans + x2, family=""binomial"")
summary(logm.bis)
</code></pre>

<p>Even in this case, <strong>the log-likelihood of the two models is the same</strong>:</p>

<pre><code>logLik(logm) == logLik(logm.bis)
</code></pre>

<p>So far, so good. However, when I do the same with a hierarchical model, <strong>the log-likelihood (and consequently the AIC and BIC) of the two models are different</strong>, although the coefficient of the transformed variable remains significant with the same z value and the other coefficients are the same.</p>

<pre><code># Multilevel model
mm &lt;- lmer(y1 ~ x1 + x2 + (1 | x3))
summary(mm)
mm.bis &lt;- lmer(y1 ~ x1.trans + x2 + (1 | x3))
summary(mm.bis)
logLik(mm) == logLik(mm.bis) ### FALSE! ###
</code></pre>

<p>Why? Also the ""REML criterion at convergence"" is obviously different. I don't understand this result. This is probably due to my moderate knowledge of the math of hierarchical models. I'd be very happy if some of you could show me what's the trick here.</p>

<p>Since we then use AIC and BIC to compare models, I am puzzled by the fact that a simple transformation that shouldn't change anything makes one model better (or worse) than another.</p>
"
"0.0559454238864459","0.056980288229819"," 87956","<p>I have a repeated-measures experiment where the dependent variable is a percentage, and I have multiple factors as independent variables. I'd like to use <code>glmer</code> from the R package <code>lme4</code> to treat it as a logistic regression problem (by specifying <code>family=binomial</code>) since it seems to accommodate this setup directly.</p>

<p>My data looks like this:</p>

<pre><code> &gt; head(data.xvsy)
   foldnum      featureset noisered pooldur dpoolmode       auc
 1       0         mfcc-ms      nr0       1      mean 0.6760438
 2       1         mfcc-ms      nr0       1      mean 0.6739482
 3       0    melspec-maxp    nr075       1       max 0.8141421
 4       1    melspec-maxp    nr075       1       max 0.7822994
 5       0 chrmpeak-tpor1d    nr075       1       max 0.6547476
 6       1 chrmpeak-tpor1d    nr075       1       max 0.6699825
</code></pre>

<p>and here's the R command that I was hoping would be appropriate:</p>

<pre><code> glmer(auc~1+featureset*noisered*pooldur*dpoolmode+(1|foldnum), data.xvsy, family=binomial)
</code></pre>

<p>The problem with this is that the command complains about my dependent variable not being integers:</p>

<pre><code>In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>and the analysis of this (pilot) data gives weird answers as a result.</p>

<p>I understand why the <code>binomial</code> family expects integers (yes-no counts), but it seems it should be OK to regress percentage data directly. How to do this?</p>
"
"0.027972711943223","0.0284901441149095"," 88388","<p>I would like to use cross-validation to test how predictive my mixed-effect logistic regression model is (model run with glmer). Is there an easy way to do this using a package in R? I've only seen cross validation functions in R for use with linear models.</p>
"
"0.027972711943223","0.0284901441149095"," 89130","<p>Consider this logistic regression:</p>

<pre><code>mtcars$vs &lt;- as.factor(mtcars$vs)
log_reg_mtcars &lt;- glm(am ~ vs*wt +vs*mpg, family = ""binomial"", mtcars)
</code></pre>

<p>I tried using the effects package to extract some coefficients from the model:</p>

<pre><code>as.data.frame(effect(""vs"", log_reg_mtcars)) 

NOTE: vs is not a high-order term in the model
  vs       fit       se       lower     upper
1  0 0.3957869 2.916877 0.002150238 0.9950031
2  1 0.0336822 2.181009 0.000484832 0.7146704
</code></pre>

<p>Could anyone explain why the coefficients/standard error for <code>vs</code> given by this code are different. Or in other words, what is the effects package doing?</p>

<pre><code>as.data.frame(summary(log_reg_mtcars)$coef)[2,1:2]

     Estimate Std. Error
vs1 -40.70047   40.53197
</code></pre>
"
"0.148017678595629","0.134603278829359"," 89204","<p>I'm looking for advice on how to analyze complex survey data with multilevel models in R. I've used the <code>survey</code> package to weight for unequal probabilities of selection in one-level models, but this package does not have functions for multilevel modeling. The <code>lme4</code> package is great for multilevel modeling, but there is not a way that I know to include weights at different levels of clustering. <a href=""http://www.statmodel.com/download/asparouhovgmms.pdf"">Asparouhov (2006)</a> sets up the problem:</p>

<blockquote>
  <p>Multilevel models are frequently used to analyze data from cluster sampling designs. Such sampling designs however often use unequal probability of selection at the cluster level and at the individual level. Sampling weights are assigned at one or both levels to reflect these probabilities. If the sampling weights are ignored at either level the parameter estimates can be substantially biased.</p>
</blockquote>

<p>One approach for two-level models is the multilevel pseudo maximum likelihood (MPML) estimator that is implemented in MPLUS (<a href=""http://www.statmodel.com/download/SurveyJSM1.pdf"">Asparouhov et al, ?</a>). <a href=""http://www.biomedcentral.com/1471-2288/9/49"">Carle (2009)</a> reviews major software packages and makes a few recommendations about how to proceed: </p>

<blockquote>
  <p>To properly conduct MLM with complex survey data and design weights, analysts need software that can include weights scaled outside of the program and include the ""new"" scaled weights without automatic program modification. Currently, three of the major MLM software programs allow this: Mplus (5.2), MLwiN (2.02), and GLLAMM. Unfortunately, neither HLM nor SAS can do this.</p>
</blockquote>

<p><a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3630376/"">West and Galecki (2013)</a> give a more updated review, and I'll quote the relevant passage at length:</p>

<blockquote>
  <p>Occasionally, analysts wish to fit LMMs to survey data sets collected from samples with complex designs (see Heeringa et al, 2010, Chapter 12). Complex sample designs are generally characterized by division of the population into strata, multi-stage selection of clusters of individuals from within the strata, and unequal probabilities of selection for both clusters and the ultimate individuals sampled. These unequal probabilities of selection generally lead to the construction of sampling weights for individuals, which ensure unbiased estimation of descriptive parameters when incorporated into an analysis. These weights might be further adjusted for survey nonresponse and calibrated to known population totals. Traditionally, analysts might consider a design-based approach to incorporating these complex sampling features when estimating regression models (Heeringa et al., 2010). More recently, statisticians have started to explore model-based approaches to analyzing these data, using LMMs to incorporate fixed effects of sampling strata and random effects of sampled clusters.</p>
  
  <p>The primary difficulty with the development of model-based approaches to analyzing these data has been choosing appropriate methods for incorporating the sampling weights (see Gelman, 2007 for a summary of the issues). Pfeffermann et al. (1998), Asparouhov and Muthen (2006), and Rabe-Hesketh and Skrondal (2006) have developed theory for estimating multilevel models in a way that incorporates the survey weights, and Rabe-Hesketh and Skrondal (2006), Carle (2009) and Heeringa et al. (2010, Chapter 12) have presented applications using current software procedures, but this continues to be an active area of statistical research. Software procedures capable of fitting LMMs are at various stages of implementing the approaches that have been proposed in the literature thus far for incorporating complex design features, and analysts need to consider this when fitting LMMs to complex sample survey data. Analysts interested in fitting LMMs to data collected from complex sample surveys will be attracted to procedures that are capable of correctly incorporating the survey weights into the estimation procedures (HLM, MLwiN, Mplus, xtmixed, and gllamm), consistent with the present literature in this area.</p>
</blockquote>

<p>This brings me to my question: does anyone have best practice recommendations for fitting LMMs to complex survey data in R?</p>
"
"NaN","NaN"," 89749","<p>I was wondering if someone knows a R-package or function library for the topic of shrinkage for regression with ARMA errors.</p>

<p>Please let me know if you came across something related.</p>

<p>Thank you!</p>

<p>Regards,<br>
Patricia Tencaliec</p>
"
"0.0843408998948762","0.0859010165930062"," 89930","<p>I am attempting to construct a contrast matrix that I can run in R, using the limma bioconductor package, but I am not sure that I have coded the contrast matrix correctly. A previous <a href=""https://stats.stackexchange.com/questions/64249/creating-contrast-matrix-for-linear-regression-in-r?newreg=add2674ca9d04b7eb85fad255b45b7f5"">post</a> and the limma guide were helpful, but my two factorial design is more complicated than what is illustrated there.</p>

<p>The first factor is the treatment, with two levels (control=c and stress=s), and the second factor is the genotype, with five levels (g1, g2, g3, g4, g5). Each genotype/treatment consists of 3-biological replicates (30xsamples total). My dataset has already been normalized and log2 transformed. It consists of 1208 proteins (based upon spectral counting for those that care) that measures protein abundance differences in the five genotypes and two treatments. The dataset is complete, meaning each sample/condition has a datapoint.</p>

<h2>Subset of the data:</h2>

<pre><code>proteinID   g1.s1   g1.s2   g1.s3   g1.c1   g1.c2   g1.c3   g2.s1   g2.s2   g2.s3   g2.c1   g2.c2   g2.c3   g3.s1   g3.s2   g3.s3   g3.c1   g3.c2   g3.c3   g4.s1   g4.s2   g4.s3   g4.c1   g4.c2   g4.c3   g5.s1   g5.s2   g5.s3   g5.c1   g5.c2   g5.c3
prot1   -9.70583694 -9.940059478    -9.764489183    -9.691937821    -9.547306096    -9.668928704    -9.821333234    -10.00376839    -9.843380585    -10.0789111 -9.958506961    -9.791583706    -10.04996359    -10.10279896    -10.0689715 -9.989303332    -10.05414639    -10.00619809    -9.907032795    -10.09700113    -10.00902876    -10.05603575    -10.26218387    -10.15527373    -9.88009858 -9.748974338    -9.730010667    -9.899956956    -9.773955101    -9.957684691
prot2   -9.810354967    -9.844319231    -9.896748977    -9.777040294    -9.821308434    -9.906798728    -9.832236541    -9.876359355    -9.935535795    -10.05991278    -9.831098077    -9.789738587    -10.08470861    -10.18515166    -10.10371621    -10.01971224    -9.977142493    -10.09055782    -9.739831978    -9.586647999    -9.949407778    -9.800183583    -9.83900565 -9.943521592    -9.99229056 -9.744850134    -9.794814509    -9.98542989 -9.766324886    -9.95430439
prot3   -11.70842601    -11.72521838    -11.90389475    -11.98273998    -11.915401  -11.88620205    -11.91603643    -11.96029519    -12.14926486    -12.23846499    -12.26650985    -11.84300821    -12.64562082    -12.41471031    -12.66462278    -12.577619  -12.90001898    -12.31577711    -11.66323243    -11.50283992    -11.4844068 -11.60402491    -11.95270942    -11.68245512    -12.32380181    -12.24294758    -12.23990879    -12.21563403    -12.33730369    -12.437377
prot4   -10.88942769    -11.16906693    -11.13942576    -11.31332257    -11.04718433    -11.11811122    -11.17687812    -11.12503828    -10.9724186 -11.16837945    -11.19642214    -10.96468249    -11.3975887 -11.28808753    -11.32778647    -11.34124725    -11.30972182    -11.29564372    -10.74370929    -10.92223539    -10.97733154    -11.40528844    -11.1238659 -11.15938598    -11.24937805    -10.8691392 -11.12478375    -10.75566728    -10.99485703    -11.09493115
prot5   -10.0102959 -9.936796529    -9.964629149    -9.842835973    -9.791578592    -9.773380518    -9.72290866 -9.715837804    -9.79028651 -9.951486129    -9.636225505    -9.820715987    -10.41899204    -10.25269382    -10.26949484    -10.02644184    -10.13120897    -10.20756299    -9.752087376    -9.687001368    -10.07111473    -9.815279198    -9.995624174    -9.993526894    -9.722360141    -9.551502595    -9.551929198    -9.724500546    -9.502769792    -9.65324573
prot6   -10.34051005    -10.27571947    -10.14968761    -10.17419023    -10.47812301    -10.11019796    -10.40447672    -10.15885481    -10.22900798    -10.26612428    -10.21920493    -10.17186677    -10.66125689    -10.95438025    -10.63751536    -10.65825783    -10.60857688    -10.78516027    -10.33890785    -10.49726978    -10.47100414    -10.64742463    -10.78932619    -10.5318634 -10.26494688    -9.975182247    -10.24870036    -10.2356165 -10.26689552    -10.13061368
prot7   -10.24930429    -10.37307132    -10.03573128    -10.29985129    -9.991216794    -10.05854902    -10.1958704 -10.30549818    -10.2078462 -10.28795766    -10.23314344    -10.23897922    -9.997472306    -10.27461285    -10.20805608    -10.06261332    -10.24876706    -10.12643737    -9.906088449    -10.07316322    -10.23545822    -10.30970717    -10.40745591    -10.36432166    -10.22423532    -10.25703553    -10.44925268    -9.902554721    -9.891163766    -10.0695915
prot8   -10.98782595    -10.84184533    -10.76496107    -10.68290092    -10.55763113    -10.91736394    -10.87505278    -10.76474268    -10.58319007    -10.87547281    -10.71948079    -10.95011831    -10.99753277    -11.061728  -10.8852958 -10.86371208    -10.96638746    -11.24112703    -10.46809937    -10.78446288    -10.71240489    -10.80931259    -10.6598091 -10.54801115    -10.70612733    -10.7339808 -10.8184854 -10.53370359    -10.47323989    -10.62675183
prot9   -8.83857166 -8.736344638    -8.743339515    -8.8152675  -8.743086044    -8.719612156    -8.898093257    -8.902781886    -9.071574958    -8.945970659    -8.862394746    -8.825061244    -8.82313363 -9.161452294    -8.905846232    -8.940119002    -9.024995852    -8.943721201    -8.768488159    -8.802155458    -8.721187011    -8.84850416 -8.931513624    -8.86743278 -8.856904592    -8.675257846    -8.900833162    -8.676117406    -8.758661701    -8.925717389
prot10  -10.65297508    -10.74532307    -10.65940071    -10.36671791    -10.50431649    -10.54915637    -11.07154003    -10.79884265    -10.97164196    -11.1201714 -11.14821342    -10.9254445 -10.92875918    -10.90806369    -10.77581175    -11.2324716 -11.31360896    -11.01070959    -11.04450945    -10.89694291    -10.76865867    -10.92983387    -11.07365287    -11.43888216    -11.14948441    -10.69611194    -10.85827316    -10.64470128    -10.79046792    -10.86048168
</code></pre>

<h2>Code that I am attempting to utilize:</h2>

<pre><code>proteins.mat &lt;- as.matrix(proteins.df)
treat = c(""g1.s"",""g1.c"",""g2.s"",""g2.c"",""g3.s"",""g3.c"",""g4.s"",""g4.c"",""g5.s"",""g5.c"")
factors = gl(10,3,labels=treat)
design &lt;- model.matrix(~0+factors)
colnames(design) &lt;- treat
</code></pre>

<h2>Here is the design for my model:</h2>

<pre><code>&gt; design
   g1.s g1.c g2.s g2.c g3.s g3.c g4.s g4.c g5.s g5.c
1     1    0    0    0    0    0    0    0    0    0
2     1    0    0    0    0    0    0    0    0    0
3     1    0    0    0    0    0    0    0    0    0
4     0    1    0    0    0    0    0    0    0    0
5     0    1    0    0    0    0    0    0    0    0
6     0    1    0    0    0    0    0    0    0    0
7     0    0    1    0    0    0    0    0    0    0
8     0    0    1    0    0    0    0    0    0    0
9     0    0    1    0    0    0    0    0    0    0
10    0    0    0    1    0    0    0    0    0    0
11    0    0    0    1    0    0    0    0    0    0
12    0    0    0    1    0    0    0    0    0    0
13    0    0    0    0    1    0    0    0    0    0
14    0    0    0    0    1    0    0    0    0    0
15    0    0    0    0    1    0    0    0    0    0
16    0    0    0    0    0    1    0    0    0    0
17    0    0    0    0    0    1    0    0    0    0
18    0    0    0    0    0    1    0    0    0    0
19    0    0    0    0    0    0    1    0    0    0
20    0    0    0    0    0    0    1    0    0    0
21    0    0    0    0    0    0    1    0    0    0
22    0    0    0    0    0    0    0    1    0    0
23    0    0    0    0    0    0    0    1    0    0
24    0    0    0    0    0    0    0    1    0    0
25    0    0    0    0    0    0    0    0    1    0
26    0    0    0    0    0    0    0    0    1    0
27    0    0    0    0    0    0    0    0    1    0
28    0    0    0    0    0    0    0    0    0    1
29    0    0    0    0    0    0    0    0    0    1
30    0    0    0    0    0    0    0    0    0    1
attr(,""assign"")
[1] 1 1 1 1 1 1 1 1 1 1
attr(,""contrasts"")
attr(,""contrasts"")$factors
[1] ""contr.treatment""
</code></pre>

<h2>My contrast model. I want to test for interaction, differences between genotypes, and to see if specific genotypes respond differently to the treatment from one another:</h2>

<pre><code>cmtx &lt;- makeContrasts(
  GenotypevsTreatment=(g1.s-g1.c)-(g2.s-g2.c)-(g3.s-g3.c)-(g4.s-g4.c)-(g5.s-g5.c),
  genotype=(g1.s+g1.c)-(g2.s+g2.c)-(g3.s+g3.c)-(g4.s+g4.c)-(g5.s+g5.c),
  Treatment=(g1.s+g2.s+g3.s+g4.s+g5.s)-(g1.c+g2.c+g3.c+g4.c+g5.c),
  levels=design)
</code></pre>

<h2>What my contrast model looks like, but I don't think this is correct:</h2>

<pre><code>&gt; cmtx
      Contrasts
Levels GenotypevsTreatment Genotype Treatment
  g1.s                   1        1         1
  g1.c                  -1        1        -1
  g2.s                  -1       -1         1
  g2.c                   1       -1        -1
  g3.s                  -1       -1         1
  g3.c                   1       -1        -1
  g4.s                  -1       -1         1
  g4.c                   1       -1        -1
  g5.s                  -1       -1         1
  g5.c                   1       -1        -1
</code></pre>

<h2>Fitting the linear model by empirical bayes statistics for differential expression:</h2>

<pre><code>fit &lt;- eBayes(contrasts.fit(lmFit(proteins.mat, design), cmtx))
topTable(fit, adjust.method=""BH"")
</code></pre>

<h2>The below topTable proteins are the same as the subset of data from above:</h2>

<pre><code>&gt; topTable(fit, adjust.method=""BH"")
       GenotypevsTreatment Genotype    Treatment    AveExpr        F      P.Value    adj.P.Val
prot1        -0.40786338 60.30918  0.073054723  -9.918822 17308.55 1.124646e-39 1.232079e-36
prot2        -0.09255219 59.60864  0.061701713  -9.897968 15801.43 3.304533e-39 1.232079e-36
prot3        -0.23880357 73.48557  0.536672827 -12.090016 15650.65 3.701463e-39 1.232079e-36
prot4        -0.11834000 66.76931  0.305471823 -11.122034 15522.46 4.079731e-39 1.232079e-36
prot5        -0.15210172 59.21509 -0.183849274  -9.876144 14734.51 7.556112e-39 1.423908e-36
prot6        -0.15761118 62.87467  0.155340561 -10.389362 14565.87 8.658504e-39 1.423908e-36
prot7        -0.03886438 61.15652 -0.166795475 -10.182834 14551.88 8.757515e-39 1.423908e-36
prot8        -0.10425341 64.63523 -0.186904167 -10.780359 14461.18 9.429854e-39 1.423908e-36
prot9        -0.03426380 53.48057  0.007403722  -8.854471 13713.49 1.767090e-38 2.021378e-36
prot10       -0.75250251 66.62646  0.327497120 -10.894506 13480.51 2.164184e-38 2.021378e-36
</code></pre>

<p>Aside from thinking that I didnâ€™t do this correctly, the result for Genotype looks incorrect to me. Any input would be much appreciated.</p>
"
"NaN","NaN"," 90402","<p>I'd like to know what technique or techniques are used by regression packages, (in particular the <code>lm</code> function of R) to minimize the sum of squares. </p>

<p>Does it use gradient descent?</p>

<p>Thanking in anticipation.</p>
"
"0.0692289300613081","0.0705095093522208"," 90904","<p>I'm new to using CART trees, but have been asked to do so for a project I'm working on. I've had success running the scripts (from both RPART and PARTY packages) but I can't seem to get exactly what I'm looking for. I'm working with spectral data (Red, NIR, NDVI...) for 80 trees in four categories (Mesic-control, Mesic-fertilized, Xeric-control and Xeric-fertilized). There are significant differences in the mean values for spectral bands among the four categories and I'd like to use those differences to develop an algorithm for assigning category to unknown trees. </p>

<p>Here's a dummy tree I made using the RPART package:
<img src=""http://i.stack.imgur.com/7XUQ3.jpg"" alt=""RPART tree""></p>

<pre><code>fit &lt;- rpart(Category ~ red.top + NIR.top + R.NIR.top, method=""anova"", data=CCA)
plot(fit, uniform=T,main=""Classification Tree for Kyphosis"")
text(fit, use.n=TRUE, all=TRUE, cex=.8)
</code></pre>

<p>And here's another tree I made with PARTY:
<img src=""http://i.stack.imgur.com/ourPe.jpg"" alt=""PARTY tree""></p>

<pre><code>library(party)
fit &lt;- ctree(Category ~ red.top + NIR.top + R.NIR.top, data=CCA)
plot(fit, main=""Conditional Inference Tree for Kyphosis"")
gtree &lt;- ctree(Category ~ ., data = CART)
plot(gtree)
</code></pre>

<p>Both look fine, except they don't really do what I want. The RPART one looks good, but I can't figure out how to determine the category identity of the trees in each 'leaf' and the PARTY one is what I want, except the tree is way simplified compared to the regression tree in the first example. My ultimate goal is to essentially combine the two and create a larger regression tree that uses more of the 'rules' from the data and gets me to output 'leaves' with categorical information and some predictive power. I'm not really too hung up on whether I use regression or categorization--as long as it has utilitarian value.</p>

<p>So, I guess what I'm really looking for is better scripts for either package that give me a more detailed tree with visual output (bar graphs on the leaves) or a way to determine the identity of the groups created by the RPART tree.</p>
"
"0.104897669787086","0.106838040430911"," 91243","<p>I have a large data frame in the following form (I apologize for this formatting):</p>

<pre><code>Site    Season  T          SC    pH    Chl   DO.S   DO      BGA  Tur    fDOM    Flow    Rainfall    Solar      Rain
300N    Winter  14.05   1692.77 7.93    NA  82.26   8.42    NA  9.25    NA      NA      0.00          219.18     no
</code></pre>

<p>If you can't understand the formatting, there are 12 numerical factors, and 3 categorical factors (<code>Site</code>, <code>Season</code>, <code>Rain</code> [yes/no]). Each row represents the average daily values that I have calculated from 15-minute time series. I have spent a good amount of time doing data exploration (linear regression analysis, looking at time series plots for patterns), but haven't found a method that works for me yet. I have also worked with <code>corrplot</code>, correlation matrices, and covariance functions in an arduous way, where I subset each categorical combination and found <code>corrplot</code>s for each (I have also tried it with <code>ddply</code>, but the resulting format is not in the correlation matrix format that is easy to plot). I have also attempted PCA on the data to little avail.</p>

<p>My question is first and foremost, does anyone have an idea for data visualization of this kind of dataset? The main question I am after is, ""What are the factors that influence <code>DO</code> (dissolved oxygen)?"". How does this change by location (<code>Site</code>), <code>Season</code>, and with the influence of <code>Rain</code>. I would really like a quick method for shooting out correlation matrices (or heat maps; I have tried both) for each categorical subset. I tried this with <code>ggplot</code> and <code>facet_wrap</code>, but it wasn't happening for me. I also tried <code>ggpairs</code> from the GGally package, but honestly didn't spend too much time with that method.</p>

<p>I was starting to get into the idea of star graphs (on polar coordinates), which can be used to visualize repeating periodicity in time series, but am running out of time and decided to seek the advisement of Stack Overflow. I really appreciate any advice or thoughts on visualizing this data that come to your mind. I feel like some combination of <code>ddply</code> and graphing is what I need, but I haven't gotten there yet.
Thank you for your time.</p>

<p>EDIT:
<code>dput</code> of the data frame in question:</p>

<pre><code>structure(list(Site = structure(c(2L, 2L, 2L, 2L, 2L, 2L), .Label = c(""2100S"", 
""300N"", ""3300S"", ""800S"", ""Burnham"", ""Center""), class = ""factor""), 
    Season = structure(c(4L, 4L, 4L, 4L, 2L, 2L), .Label = c(""Fall"", 
    ""Spring"", ""Summer"", ""Winter""), class = ""factor""), T = c(14.05, 
    14.18, 14.5, 14.58, 14.07, 11.91), SC = c(1692.77, 1671.31, 
    1680.71, 1661.79, 1549.56, 1039.63), pH = c(7.93, 7.92, 7.96, 
    7.95, 7.93, 7.79), Chl = c(NA_real_, NA_real_, NA_real_, 
    NA_real_, NA_real_, NA_real_), DO.S = c(82.26, 78.79, 82.05, 
    80.92, 74.33, 73.96), DO = c(8.42, 8.04, 8.31, 8.18, 7.61, 
    7.97), BGA = c(NA_real_, NA_real_, NA_real_, NA_real_, NA_real_, 
    NA_real_), Tur = c(9.25, 9.77, 9.41, 10.6, 40.38, 50.25), 
    fDOM = c(NA_real_, NA_real_, NA_real_, NA_real_, NA_real_, 
    NA_real_), Flow = c(NA, 178.08, 178.53, 188.13, 306.15, 382.22
    ), Rainfall = c(0, 0, 0, 0, 0.01, 0.81), Solar = c(219.18, 
    228.33, 244.3, 247.69, 105.15, 220.73), Rain = structure(c(1L, 
    1L, 1L, 1L, 2L, 2L), .Label = c(""no"", ""yes""), class = ""factor"")), .Names = c(""Site"", 
""Season"", ""T"", ""SC"", ""pH"", ""Chl"", ""DO.S"", ""DO"", ""BGA"", ""Tur"", 
""fDOM"", ""Flow"", ""Rainfall"", ""Solar"", ""Rain""), row.names = c(NA, 
6L), class = ""data.frame"")
</code></pre>
"
"0.027972711943223","0.0284901441149095"," 91479","<p>I'm using R to compute robust multiple linear regression.
I use the command <code>rlm</code> from the package MASS.</p>

<p>As psi function I use <code>psi.huber</code> or <code>psi.bisquare</code>.</p>

<p>Is there a way to get an estimator of the goodness of fit of the model? Maybe something comparable to the Adjusted R-squared, for the parametric multiple linear regression?</p>

<p>Moreover, am I right saying that this kind of robust regression doesn't solve the problem of non-normality of the dependent or independent variables?</p>
"
"0.0839181358296689","0.0854704323447285"," 91745","<p>Please, i need to do a network meta-analysis and metaregression or a multivariate meta analysis and metaregression. I have multi arm randomized controlled studies.
I tried the metafor package</p>

<h1>Dummy database with controlled multiarm treatment studies</h1>

<pre><code>Study = c(1,2,3,1:2,4:10,1,2,7)
Group1 = as.factor(c(rep(1,3),rep(2,9),rep(3,3)))
numberInt= c(rep(30,5),rep(28,7),rep(40,3))
InterventionMean1  = c(rnorm(3, mean = 350, sd = 2), rnorm(9, mean = 540, sd = 2),rnorm(3, mean = 860, sd = 2))
InterventionSD  = rnorm(15, mean = 80, sd = 15)
numberCont= c(rep(29,4), 28,rep(30,7),rep(42,3))
ControlMean  = rnorm(15, mean = 230, sd = 2)
ControlSD  = rnorm(15, mean = 55, sd = 9)
Dose = c(40,50,40,100,100,100,100,100,100,100,100,100,200,200,200)
x=as.factor(c(1,2,1,rep(1,5),rep(2,3),rep(1,4)))
data = data.frame(cbind(Study,Group, numberInt,InterventionMean1,InterventionSD,
             numberCont, ControlMean, ControlSD,Dose,x))
</code></pre>

<h1>x and Dose are covariates</h1>

<h1>Loading libraries</h1>

<pre><code>library(metafor)
library(Matrix)
</code></pre>

<h1>Effect size : mean difference</h1>

<p>dataEscalc = escalc(measure = ""MD"", m1i = InterventionMean1, sd1i = InterventionSD, n1i = numberInt,
                    m2i = ControlMean, sd2i = ControlSD, n2i = numberCont,
                    data=data)</p>

<h1>Number of patients per studies</h1>

<pre><code>dataEscalc$Ni &lt;- unlist(lapply(split(data, data$Study), function(x) rep(sum(x$numberInt) + x$numberCont[1], each=nrow(x))))
</code></pre>

<h1>Variance-covariance matrix</h1>

<pre><code>    calc.v &lt;- function(x) {
    v &lt;- matrix(vi/x$numberCont[1] + outer(x$yi, x$yi, ""*"")/(2*x$Ni[1]), nrow=nrow(x), ncol=nrow(x))
      diag(v) &lt;- x$vi
          v
          }
    V &lt;- lapply(split(dataEscalc, dataEscalc$Study), calc.v)
V &lt;- as.matrix(bdiag(V))
V
</code></pre>

<h1>Conducting multivariate meta anlaysis</h1>

<p>names(dataEscalc)</p>

<pre><code>res &lt;- rma.mv(yi, V, mods = ~ factor(Group1)  - 1,
              random = ~ factor(Group1) | Study,
              data=dataEscalc, method=""REML"")
</code></pre>

<blockquote>
  <p>I get this error message</p>
  
  <p>Error in rma.mv(yi, V, mods = ~factor(Group1) - 1, random =
  ~factor(Group1) |  : 
                    Error during optimization.</p>
</blockquote>

<h1>Conducting multivariate metaregression</h1>

<pre><code>res &lt;- rma.mv(yi, V, mods = ~ factor(Group1) + Group1:I(Dose) + Group1:I(x) - 1,
              random = ~ factor(Group1) | Study,
              data=dataEscalc, method=""REML"")
</code></pre>

<blockquote>
  <p>I get this error message</p>
  
  <p>Error in rma.mv(yi, V, mods = ~factor(Group1) + Group1:I(Dose) +
  Group1:I(x) -  : 
                    Model matrix not of full rank. Cannot fit model.</p>
</blockquote>

<p>I followed the instructions from 
<a href=""http://www.metafor-project.org/doku.php/analyses%3agleser2009"" rel=""nofollow"">http://www.metafor-project.org/doku.php/analyses:gleser2009</a>
http://www.metafor-project.org/doku.php/analyses:vanhouwelingen2002</p>

<p>Thanks in advance</p>
"
"NaN","NaN"," 91803","<p>I am using the ""metafor"" package to do a multivariate meta-regression in ""R"". </p>

<p>I have 6 predictors and I am able to run the full model (all the predictors simultaneously in the model) just fine.</p>

<p>However, I would like to do a backward deletion meta-regression. I cannot figure out how to do it, and I am wondering if this is a possibility. </p>

<p>Thank you.</p>
"
"0.104897669787086","0.113960576459638"," 92150","<p>Actually, I thought I had understood what one can show a with partial dependence plot, but using a very simple hypothetical example, I got rather puzzled. In the following chunk of code I generate three independent variables (<em>a</em>, <em>b</em>, <em>c</em>) and one dependent variable (<em>y</em>) with <em>c</em> showing a close linear relationship with <em>y</em>, while <em>a</em> and <em>b</em> are uncorrelated with <em>y</em>. I make a regression analysis with a boosted regression tree using the R package <code>gbm</code>:</p>

<pre><code>a &lt;- runif(100, 1, 100)
b &lt;- runif(100, 1, 100)
c &lt;- 1:100 + rnorm(100, mean = 0, sd = 5)
y &lt;- 1:100 + rnorm(100, mean = 0, sd = 5)
par(mfrow = c(2,2))
plot(y ~ a); plot(y ~ b); plot(y ~ c)
Data &lt;- data.frame(matrix(c(y, a, b, c), ncol = 4))
names(Data) &lt;- c(""y"", ""a"", ""b"", ""c"")
library(gbm)
gbm.gaus &lt;- gbm(y ~ a + b + c, data = Data, distribution = ""gaussian"")
par(mfrow = c(2,2))
plot(gbm.gaus, i.var = 1)
plot(gbm.gaus, i.var = 2)
plot(gbm.gaus, i.var = 3)
</code></pre>

<p>Not surprisingly, for variables <em>a</em> and <em>b</em> the partial dependence plots yield horizontal lines around the mean of <em>a</em>. What me puzzles is the plot for variable <em>c</em>. I get horizontal lines for the ranges <em>c</em> &lt; 40 and <em>c</em> > 60 and the y-axis is restricted to values close to the mean of <em>y</em>. Since <em>a</em> and <em>b</em> are completely unrelated to <em>y</em> (and thus there variable importance in the model is 0), I expected that <em>c</em> would show partial dependence along its entire range instead of that sigmoid shape for a very restricted range of its values. I tried to find information in Friedman (2001) ""Greedy function approximation: a gradient boosting machine"" and in Hastie et al. (2011) ""Elements of Statistical Learning"", but my mathematical skills are too low to understand all the equations and formulae therein. Thus my question: What determines the shape of the partial dependence plot for variable <em>c</em>? (Please explain in words comprehensible to a non-mathematician!)     </p>

<p>ADDED on 17th April 2014:</p>

<p>While waiting for a response, I used the same example data for an analysis with R-package <code>randomForest</code>. The partial dependence plots of randomForest resemble much more to what I expected from the gbm plots: the partial dependence of explanatory variables <em>a</em> and <em>b</em> vary randomly and closely around 50, while explanatory variable <em>c</em> shows partial dependence over its entire range (and over almost the entire range of <em>y</em>). What could be the reasons for these different shapes of the partial dependence plots in <code>gbm</code> and <code>randomForest</code>?</p>

<p><img src=""http://i.stack.imgur.com/PrlC1.jpg"" alt=""partial plots of gbm and randomForest""></p>

<p>Here the modified code that compares the plots:</p>

<pre><code>a &lt;- runif(100, 1, 100)
b &lt;- runif(100, 1, 100)
c &lt;- 1:100 + rnorm(100, mean = 0, sd = 5)
y &lt;- 1:100 + rnorm(100, mean = 0, sd = 5)
par(mfrow = c(2,2))
plot(y ~ a); plot(y ~ b); plot(y ~ c)
Data &lt;- data.frame(matrix(c(y, a, b, c), ncol = 4))
names(Data) &lt;- c(""y"", ""a"", ""b"", ""c"")

library(gbm)
gbm.gaus &lt;- gbm(y ~ a + b + c, data = Data, distribution = ""gaussian"")

library(randomForest)
rf.model &lt;- randomForest(y ~ a + b + c, data = Data)

x11(height = 8, width = 5)
par(mfrow = c(3,2))
par(oma = c(1,1,4,1))
plot(gbm.gaus, i.var = 1)
partialPlot(rf.model, Data[,2:4], x.var = ""a"")
plot(gbm.gaus, i.var = 2)
partialPlot(rf.model, Data[,2:4], x.var = ""b"")
plot(gbm.gaus, i.var = 3)
partialPlot(rf.model, Data[,2:4], x.var = ""c"")
title(main = ""Boosted regression tree"", outer = TRUE, adj = 0.15)
title(main = ""Random forest"", outer = TRUE, adj = 0.85)
</code></pre>
"
"0.134152413729855","0.130693325543483"," 92892","<p>I'm working on a meta-analysis of prevalence data. The aim is to get estimates of prevalence at the country level. The main issue is that the disease is highly correlated with age, and the sample ages of included studies are highly heterogeneous. Only median age is available for most studies, so I can't use SMR-like tricks. I figured I could use meta-regression to solve this, including age as a fixed-effect and introducing study-level and country-level random-effects.</p>

<p>The idea (that I took from <a href=""http://www.thelancet.com/journals/lancet/article/PIIS0140-6736%2813%2961249-0/abstract"" rel=""nofollow"">Fowkes et al</a>) was to use this model to make country-specific predictions of prevalence for each 5-year age group from 15 to 60 (using the median age of the group), and to apply these predictions to the actual population size of each of those groups in the selected country, in order to obtain total infected population and to calculate age-adjusted prevalence in the 15-60 population from that.</p>

<p>I tried several ways to do this using R with packages <code>meta</code> and <code>mgcv</code>. I got some satisfying results, but I'm not that confident with my results and would appreciate some feedback.</p>

<p>First is some simulated data, then the description of my different approaches:</p>

<pre><code>data&lt;-data.frame(id_study=c(""UK1"",""UK2"",""UK3"",""FRA1"",""FRA2"",""BEL1"",""GER1"",""GER2"",""GER3""),
                 country=c(""UK"",""UK"",""UK"",""FRANCE"",""FRANCE"",""BELGIUM"",""GERMANY"",""GERMANY"",""GERMANY""),
                 n_events=c(91,49,18,10,50,6,9,10,22),
                 n_total=c(3041,580,252,480,887,256,400,206,300),
                 study_median_age=c(25,50,58,30,42,26,27,28,36))
</code></pre>

<p><strong>Standard random-effect meta-analysis</strong> with package <code>meta</code>.</p>

<p>I used <code>metaprop()</code> to get a first estimate of the prevalence in each country without taking age into account, and to obtain weights. As expected, heterogeneity was very high, so I used weights from the random-effects model.</p>

<pre><code> meta &lt;- metaprop(event=n_events,n=n_total,byvar=country,sm=""PLOGIT"",method.tau=""REML"",data=data)
 summary(meta)
 data$weight&lt;-meta$w.random
</code></pre>

<p>I used meta to get a first estimate of the prevalence without taking age into account, and to obtain weights. As expected, heterogeneity was very high, so I used weights from the random-effects model.</p>

<p><strong>Generalized additive model</strong> to include age with package <code>mgcv</code>.</p>

<p>The <code>gam()</code> model parameters (k and sp) were chosen using BIC and GCV number (not shown here).</p>

<pre><code> model &lt;- gam( cbind(n_events,n_total-n_events) ~ s(study_median_age,bs=""cr"",k=4,sp=2) + s(country,bs=""re""), weights=weight, data=data, family=""binomial""(link=logit), method=""REML"")
 plot(model,pages=1,residuals=T, all.terms=T, shade=T)
</code></pre>

<p>Predictions for each age group were obtained from this model as explained earlier. CI were obtained directly using <code>predict.gam()</code>, that uses the  Bayesian posterior covariance matrix of the parameters. For exemple considering UK:</p>

<pre><code> newdat&lt;-data.frame(country=""UK"",study_median_age=seq(17,57,5))
 link&lt;-predict(model,newdat,type=""link"",se.fit=T)$fit
 linkse&lt;-predict(model,newdat,type=""link"",se.fit=T)$se
 newdat$prev&lt;-model$family$linkinv(link)
 newdat$CIinf&lt;-model$family$linkinv(link-1.96*linkse)
 newdat$CIsup&lt;-model$family$linkinv(link+1.96*linkse)
 plot(newdat$prev~newdat$study_median_age, type=""l"",ylim=c(0,.12))
 lines(newdat$CIinf~newdat$study_median_age, lty=2)
 lines(newdat$CIsup~newdat$study_median_age, lty=2)
</code></pre>

<p>The results were satisfying, representing the augmentation of the prevalence with advanced age, with coherent confidence intervals. I obtained a total prevalence for the country using the country population structure (not shown, I hope it is clear enough).</p>

<p>However, I figured I needed to include study-level random-effects since there was a high heterogeneity (even though I did not calculate heterogeneity after the meta-regression).</p>

<p><strong>Introducing study-level random-effect</strong> with package <code>gamm4</code>.</p>

<p>Since <code>mgcv</code> models can't handle that much random-effect parameters, I had to switch to <code>gamm4</code>.</p>

<pre><code> model2 &lt;- gamm4(cbind(n_events,n_total-n_events) ~ s(study_median_age,bs=""cr"",k=4) + s(country,bs=""re""), random=~(1|id_study), data=data, weights=weight, family=""binomial""(link=logit))
 plot(model2$gam,pages=1,residuals=T, all.terms=T, shade=T)

 link&lt;-predict(model2$gam,newdat,type=""link"",se.fit=T)$fit
 linkse&lt;-predict(model2$gam,newdat,type=""link"",se.fit=T)$se
 newdat$prev2&lt;-model$family$linkinv(link)
 newdat$CIinf2&lt;-model$family$linkinv(link-1.96*linkse)
 newdat$CIsup2&lt;-model$family$linkinv(link+1.96*linkse)
 plot(newdat$prev2~newdat$study_median_age, type=""l"",col=""red"",ylim=c(0,0.11))
 lines(newdat$CIinf2~newdat$study_median_age, lty=2,col=""red"")
 lines(newdat$CIsup2~newdat$study_median_age, lty=2,col=""red"")
 lines(newdat$prev~newdat$study_median_age, type=""l"",ylim=c(0,.12))
 lines(newdat$CIinf~newdat$study_median_age, lty=2)
 lines(newdat$CIsup~newdat$study_median_age, lty=2)
</code></pre>

<p>Since the study-level random effect was in the mer part of the fit, I didn't have to handle it. </p>

<p>As you can see, I obtain rather different results, with a much smoother relation between age and prevalence, and quite different confidence intervals. It is even more different in the full-data analysis, where the CI are much wider in the model including study-level RE, to the point it is sometimes almost uninformative (prevalence between 0 and 15%, but if it is the way it is...). Moreover, the study-level RE model seems to be more stable when outliers are excluded.</p>

<p><strong>So, my questions are:</strong></p>

<ul>
<li>Did I properly extract the weights from the metaprop() function and used them further?</li>
<li>Did I properly built my <code>gam()</code> and <code>gamm4()</code> models? I read a lot about this, but I'm not used to this king of models.</li>
<li>Which of these models should I use?</li>
</ul>

<p>I would really appreciate some help, since neither my teachers nor my colleagues could. It was a really harsh to conduct the systematic review, and very frustrating to struggle with the analysis... Thank you in advance!</p>
"
"0.027972711943223","0.0284901441149095"," 93202","<p>I  am using a  decision tree and random forest for a classification problem.
The output is  binary {0,1} and some of the input variables are categorical while the others are continuous. </p>

<p>I would like to know if it is possible to extract odds ratio from one of these models. Odds ratio similar to the ones that are commonly obtained using logistic regression.</p>

<p>I am using R with the package <code>rpart</code> and <code>randomforest</code> so the best answer would be a theoretical explanation and an R code. </p>

<p>Thanks!</p>
"
"0.0484501583111509","0.0493463771219827"," 93438","<p>I've been wondering something for a while. If you run a simple regression model in R and then perform a step-wise selection (it doesn't have to be the way I typed the code below), how do you extract some of the ""relevant"" information of the model in R suchs as Odds Ratios and/or confidence intervals? </p>

<p>The general model would look the following:</p>

<pre><code>glm&lt;-glm(y~x+z+n+p, family=binomial(link=""logit))
step(glm, direction=""backward"", test=""F"")
</code></pre>

<p>Interestingly, in the newest version of R, I can't save an object as the step function. </p>

<pre><code>stepmodel&lt;-step(glm, direction...)
</code></pre>

<p>Only runs the stepwise selection model. 
So performing <code>step$anova</code>or <code>conf(step)</code>- or even better <code>exp(conf(step))</code>etc. doesn't seem possible....</p>

<p>Any ideas on this? Also, would any of you know of a package, where a stepwise variable selection of a cox-regression model is possible? </p>

<p>Thanks a mill for your thoughts and help. </p>

<p>Cheers, </p>

<p>Oliver</p>
"
"0.027972711943223","0.0284901441149095"," 93708","<p>Suppose I have a data frame such that:</p>

<pre><code>set.seed(2014)
df&lt;-data.frame(y=rbinom(100,1,0.3),futime=as.integer(rnorm(100,100,10)),
     age=rnorm(100,50,5),sex=rbinom(100,1,0.5))
df[df&gt;100]&lt;-100
</code></pre>

<p>where <code>y</code> is the outcomes and <code>futime&lt;100</code> are right censored of follow up time. I would fit a logistic regression model to predict the outcomes as if they are not censored. I saw there are some options such as <code>tobit</code> model, <code>survival</code> model and <code>censReg</code> package, but seems not very similar situation. Would somebody know the best way for this using <code>age</code> and <code>sex</code> as predictors? </p>
"
"0.0395593886064618","0.040291148201269"," 93815","<p>I have some experiences with time series modelling, in the form of simple ARIMA models and so on. Now I have some data that exhibits volatility clustering, and I would like to try to start with fitting a GARCH (1,1) model on the data. </p>

<p>I have a data series and a number of variables I think influence it. So in basic regression terms, it looks like: </p>

<p>$$
y_t = \alpha + \beta_1 x_{t1} + \beta_2 x_{t2} + \epsilon_t .
$$</p>

<p>But I am at a complete loss at how to implement this into a GARCH (1,1) - model? I've looked at the <code>rugarch</code>-package and the <code>fGarch</code>-package in <code>R</code>, but I haven't been able to do anything meaningful besides the examples one can find on the internet. </p>
"
"0.0484501583111509","0.0493463771219827"," 94009","<p>I've built a random forest model (regression model) using randomForest package in R, and I calculate the correlation between the predicted values and the actual ones in order to know how the trained model is going to perform, which is very high in my case, so I was wondering in such case how this correlation is build, I mean is it leave one out correlation or any type of cross validation correlation or just random and can't represent the real performance of the model when tested on unseen new cases???? the following is a snapshot of my script for calculating the correlation where x is the data (observations) and y is the numeric values I want the model to learn/predict (in the testing cases):</p>

<pre><code>mytr_all = randomForest(x, y, ntree = 500,corr.bias=TRUE)
cor(mytr_all$y,mytr_all$predicted)
</code></pre>
"
"0.0484501583111509","0.0493463771219827"," 94062","<p>I am trying to NOT use packages for the estimation of models in order to have a deeper understanding of how things work. Currently, I am trying to estimate a VAR(1) (vector autoregression of first order) and my question is about how to construct the matrix of dependent and independent variables.</p>

<p>Theoretically, my model is:</p>

<p><img src=""http://latex.codecogs.com/gif.latex?%5Cbinom%7Bx_t%7D%7By_t%7D%20%3D%20%5Cbinom%7Bc_1%7D%7Bc_2%7D%20&plus;%20%5Cbegin%7Bpmatrix%7D%20a_%7B11%7D%20%26%20a_%7B12%7D%5C%5C%20a_%7B21%7D%20%26%20a_%7B22%7D%20%5Cend%7Bpmatrix%7D%5Cbinom%7Bx_%7Bt-1%7D%7D%7Bx_%7Bt-2%7D%7D%20&plus;%20%5Cbinom%7Bu_%7B1%2Ct%7D%7D%7Bu_%7B2%2Ct%7D%7D"" alt=""model""></p>

<p>As for the matrix of dependent variables, it should be easy enough. I just need to stack <code>xt</code> and <code>zt</code>. However, I don't know how to construct the matrix of independent variables. I read that VAR is very similar to SUR (Seemingly Unrelated Regressions) so I think that the matrix of independent variables should be a block diagonal matrix, but what should it look like? Especially if I want to add a constant and a time trend to the model?</p>

<p>Thank you.</p>
"
"0.0884574820723792","0.0810843716426003"," 94089","<p>I started to use the function <code>multinom</code> of <code>R</code> package <code>nnet</code> in order to fit several conditional probability distributions with the multinomial logistic model. I need the parameters of the fittings in order to pass them to a Java program, which will compute the probabilities and use them.</p>

<p>My problem is that the probabilities computed with the parameters returned by <code>multinom</code>, following the usual <a href=""http://en.wikipedia.org/wiki/Multinomial_logistic_regression#As_a_set_of_independent_binary_regressions"" rel=""nofollow"">definition</a> of multinomial logistic model, are not the same as those directly computed in <code>R</code>, which are the correct ones. On Stack Overflow I have already asked a <a href=""http://stackoverflow.com/questions/22905807/how-does-the-function-multinom-from-r-package-nnet-compute-the-multinomial-proba"">question</a> about this issue, but I do not still know how the <code>R</code> function <code>multinom</code> computes these probabilities; my guess is that it relies on neural networks, since this function belongs to <code>R</code> package <code>nnet</code>, but I do not have any idea about the details, and an inspection of the code led to nowhere.</p>

<p>Do you know an <code>R</code> package which fits conditional probabilities and returns the corresponding parameters of the model, so that we may easily compute the probabilities in another program? E.g., using the MARS model (<code>R</code> package <code>earth</code>) or Projection Pursuit Regression (<code>R</code> package <code>ppr</code>) is not feasible, since computing the probabilities from the parameters of these models would be a mess. Besides, the function <code>mlogit</code> from the <code>R</code> package with the same name is not applicable as well, since the dataset should be in a certain format (we would also need the predictors corresponding to the alternative, ""non-chosen"" response variable).</p>
"
"0.0685188709827532","0.0697863157798853"," 94489","<p>I have a pretty generic question which I am guessing could be relevant to many social scientists who deal with panel data sets. What are the best practices for making graphs about interaction effects.</p>

<p>There is a variety of articles that talk about packages like ggplot, ggplot2, sjPlot, effects and so on but to my knowledge none of them really takes into account multiple years and multiple individuals.</p>

<p>In my field of management what I see most often is the complete disregard for year and individual effects so that an interaction for instance gets fitted by one x-variable on the x-axis, a simple high versus low of the other interacting variable in the graph and the y-variable on the y-axis of course. A difference in slope then shows how the interaction results in different impact on the y-variable. </p>

<p>This seems pretty simplistic as it (if I understand correctly) pretty much erases everything idiosyncratic about panel data, so basically it disregards the fundamental reasons why panels are interesting for statistical inference. So hence my question to the broader community: How do you plot results (and interactions) from panel data.</p>

<p>I'd appreciate both general and coding suggestions :)</p>

<p>Some extra questions:</p>

<ol>
<li>When you have a bunch of control variables, do you chose artificial values for all of them? What about time and individual variables?</li>
<li>Is there a way of plotting the difference in residuals between the regression without interaction effects and the ones with interactions (similar to marginal effects) that clarifies how the interacting variables affect the dependent (response) variable y?</li>
<li>While uncommon in management, can we draw 3 dimensional figures to show interactions?</li>
</ol>

<p>Looking forward to your feedback and suggestions</p>
"
"0.100857047225074","0.102722675451665"," 94581","<p>I have a ordinal dependendent variable, easiness, that ranges from 1 (not easy) to 5 (very easy).  Increases in the values of the independent factors are associated with an increased easiness rating.</p>

<p>Two of my independent variables (<code>condA</code> and <code>condB</code>) are categorical, each with 2 levels, and 2 (<code>abilityA</code>, <code>abilityB</code>) are continuous.</p>

<p>I'm using the <a href=""http://cran.r-project.org/web/packages/ordinal/index.html"">ordinal</a> package in R, where it uses what I believe to be</p>

<p>$$\text{logit}(p(Y \leqslant g)) = \ln \frac{p(Y \leqslant g)}{p(Y &gt; g)} = \beta_{0_g} - (\beta_{1} X_{1} + \dots + \beta_{p} X_{p}) \quad(g = 1, \ldots, k-1)$$<br>
(from @caracal's answer <a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">here</a>)</p>

<p>I've been learning this independently and would appreciate any help possible as I'm still struggling with it.  In addition to the tutorials accompanying the ordinal package, I've also found the following to be helpful: </p>

<ul>
<li><a href=""http://stats.stackexchange.com/questions/89474/interpretation-of-ordinal-logistic-regression"">Interpretation of ordinal logistic regression</a></li>
<li><a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">Negative coefficient in ordered logistic regression</a></li>
</ul>

<p>But I'm trying to interpret the results, and put the different resources together and am getting stuck. </p>

<ol>
<li><p>I've read many different explanations, both abstract and applied, but am still having a hard time wrapping my mind around what it means to say: </p>

<blockquote>
  <p>With a 1 unit increase in condB (i.e., changing from one level to the next of the categorical predictor), the predicted odds of observing Y = 5 versus Y = 1 to 4 (as well as the predicted odds of observed Y = 4 versus Y = 1 to 3) change by a factor of exp(beta) which, for diagram, is exp(0.457) = 1.58. </p>
</blockquote>

<p>a. Is this different for the categorical vs. continuous independent variables?<br>
b. Part of my difficulty may be with the cumulative odds idea and those comparisons. ... Is it fair to say that going from condA = absent (reference level) to condA = present is 1.58 times more likely to be rated at a higher level of easiness?  I'm pretty sure that is NOT correct, but I'm not sure how to correctly state it.</p></li>
</ol>

<p>Graphically,<br>
1. Implementing the code in <a href=""http://stats.stackexchange.com/questions/89474/interpretation-of-ordinal-logistic-regression"">this post</a>, I'm confused as to why the resulting 'probability' values are so large.<br>
2. The graph of p (Y = g) in <a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">this post</a> makes the most sense to me ... with an interpretation of the probability of observing a particular category of Y at a particular value of X.  The reason I am trying to get the graph in the first place is to get a better understanding of the results overall.</p>

<p>Here's the output from my model:</p>

<pre><code>m1c2 &lt;- clmm (easiness ~ condA + condB + abilityA + abilityB + (1|content) + (1|ID), 
              data = d, na.action = na.omit)
summary(m1c2)
Cumulative Link Mixed Model fitted with the Laplace approximation

formula: 
easiness ~ illus2 + dx2 + abilEM_obli + valueEM_obli + (1 | content) +  (1 | ID)
data:    d

link  threshold nobs logLik  AIC    niter     max.grad
logit flexible  366  -468.44 956.88 729(3615) 4.36e-04
cond.H 
4.5e+01

Random effects:
 Groups  Name        Variance Std.Dev.
 ID      (Intercept) 2.90     1.70    
 content  (Intercept) 0.24     0.49    
Number of groups:  ID 92,  content 4 

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)    
condA              0.681      0.213    3.20   0.0014 ** 
condB              0.457      0.211    2.17   0.0303 *  
abilityA           1.148      0.255    4.51  6.5e-06 ***
abilityB           0.577      0.247    2.34   0.0195 *  

Threshold coefficients:
    Estimate Std. Error z value
1|2   -3.500      0.438   -7.99
2|3   -1.545      0.378   -4.08
3|4    0.193      0.366    0.53
4|5    2.121      0.385    5.50
</code></pre>
"
"0.027972711943223","0.0284901441149095"," 94818","<p>I have a dataset of 306967 rows and 23 columns, and I am building a regression model for predicting a factor based upon 6 other iid variables.
Doing this I encountered an issue of large no of levels present in one of the iid variables, so to sort out it I used <code>combine.levels</code> in <code>Hmisc</code> package.
The syntax of the general formula is <code>combine.levels(var,minlev=0.05)</code><br>
<em>Note 0.05 is the default value</em>.<br>
I used 0.001 as it reduces my levels from 19881 to just 121 and my model ran well.</p>

<p>Is it advisable to use 0.001 as value for it? Is there any technique to judge it and select its value?</p>
"
"0.0484501583111509","0.0493463771219827"," 95001","<p>I have a regression model and I want to determine the relative importance of the predictors. I used the package <code>relaimpo</code> in r.</p>

<p>The package says that 98.84% of the variance is explained by the model</p>

<pre><code>Response variable: C 
Total response variance: 115.4857 
Analysis based on 161187 observations 

10 Regressors: 
I(VPT^(-0.66)) TPA S SD TPA:VPT S:SD TPA:SD TPA:S SD:VPT S:VPT 
Proportion of variance explained by model: 98.84%
</code></pre>

<p>I did not normalize the metrics.</p>

<pre><code>Metrics are not normalized (rela=FALSE). 
</code></pre>

<p>These are my relative importance metrics:</p>

<pre><code>Relative importance metrics: 

                        lmg
I(VPT^(-0.66)) 0.2647988336
TPA            0.0776291924
S              0.0816760485
SD             0.2661375234
TPA:VPT        0.0779008229
S:SD           0.0036628215
TPA:SD         0.0002298444
TPA:S          0.0002684624
SD:VPT         0.0065747570
S:VPT          0.0114800491
</code></pre>

<p>They sum up <code>0.79</code>. Shouldn't they sum up to <code>0.9884</code>? If so, why don't they?</p>
"
"NaN","NaN"," 95203","<p>I need to perform stepwise binary logistic regression (The horror! The horror!) on 1.5 million observations.  This takes far too long in SAS, so I'm wondering if I can use R to process it in a multicore environment.  Apparently package gmulti (<a href=""http://www.jstatsoft.org/v34/i12/paper"" rel=""nofollow"">http://www.jstatsoft.org/v34/i12/paper</a>) will do the trick, but it's not clear to me if it will do that outside of its genetic algorithm.  That still might work for me, but I don't have a large number of variables (about 30) so it's not necessary.  As long as the results of the brute force and ga approach could be assured to be similar, then I might try it.  However, I see others have had problems getting the parallel feature to run: <a href=""https://stat.ethz.ch/pipermail/r-help/2013-April/351820.html"" rel=""nofollow"">https://stat.ethz.ch/pipermail/r-help/2013-April/351820.html</a>.  Any other suggestions 
on how to parallelize logistic regression in R?  A web search turned up a couple of papers, but not much that seemed specific to R.  And please spare me a lecture about stepwise regression-I'm very well aware of the pitfalls.  I'm replicating someone else's analysis.  I'm using a Windows 64 bit system.</p>
"
"0.027972711943223","0"," 95425","<p>I am trying to calculate ePCP and Brier scores in R for a mixed-effect binary logistic regression. It cannot seem to find any packages that work for mixed models. I have tried the packages OOmisc and ModelGood, and spent much time searching the web with no success.</p>

<p>I am using the lme4 package to fit the regression.</p>

<p>I would be very grateful for any tips.</p>
"
"0.0685188709827532","0.0697863157798853"," 95709","<p>I am fitting a regression with ARMA errors using the base R function <code>arima()</code> and the <code>Arima()</code> function from the <code>forecast</code> package.  </p>

<p>The estimated coefficients from both are identical. My problem comes from using <code>arima.errors()</code> on these two models, and using <code>tsdisplay()</code> to view these structural residuals (that is, the residuals straight from the regression, before any ARMA model is fit on them). These ARMA errors (and their corresponding ACFs, PACFs) are different between the two, and I don't know why. Even more curious is that the final residuals from both are in fact the same, which would make me think the structural residuals would have to be the same. I have put a MWE below.</p>

<pre><code>library('forecast')
data(usconsumption, package='fpp')

fit1 = arima(usconsumption[ ,1], xreg=usconsumption[ ,2], order=c(2,0,0))
tsdisplay(arima.errors(fit1), main=""ARIMA errors, arima function"") # not the same as the other


fit2 = Arima(usconsumption[,1], xreg=usconsumption[,2], order=c(2,0,0))
dev.new()
tsdisplay(arima.errors(fit2), main=""ARIMA errors, Arima function"") # not the same as the other


View(cbind(resid(fit1), resid(fit2))) # final residuals are the same
</code></pre>

<p>Note this example is from <a href=""https://www.otexts.org/fpp/9/1"" rel=""nofollow"">https://www.otexts.org/fpp/9/1</a></p>
"
"0.128319700089426","0.130693325543483"," 95994","<p>I`d like to extract the parameters of a two-component mixture distribution of noncentral student t distributions which first has to be fitted to a one-dimensional sample.</p>

<p>My question is closely related to this thread, but as pointed out I want to use Student t components for the mixture:
<a href=""http://stats.stackexchange.com/questions/10062/which-r-package-to-use-to-calculate-component-parameters-for-a-mixture-model?newreg=fe1454a4702e4532a03bd2c705fe3b02"">Which R package to use to calculate component parameters for a mixture model</a></p>

<p>There are many packages for R that are capable of handling mixture distributions in one way or another. Some in the context of a Bayesian framework requiring kernels. Some in a regression framework. Some in a nonparametric framework. ...</p>

<p>In general the ""mixdist""-package seems to come closest to my wish. This package fits parametric mixture distributions to a sample of data. Unfortunately it doesn`t support the student t distribution.</p>

<p>I have also tried to manually set up a likelihood function as described here:
<a href=""http://stackoverflow.com/questions/6485597/r-how-to-fit-a-large-dataset-with-a-combination-of-distributions"">http://stackoverflow.com/questions/6485597/r-how-to-fit-a-large-dataset-with-a-combination-of-distributions</a>
But my result is far from perfect.</p>

<p>The ""gamlss.mx""-package might be helping, but originally it seems to be set up for another context, i.e. regression. I tried to regress my data on a constant and then extract the parameters for the estimated mixture error distribution. Is this a valid approach? </p>

<p>But with this approach the estimated parameters seem to be not directly accessable individually by some command (such as fit1$sigma). And more importantly there seem to be serious estimation problems even in pretty simple and nonambiguous cases.
E.g. in example 2 (see syntax below) I simulated a mixture which looks like this:</p>

<p><img src=""http://i.stack.imgur.com/MG7AA.jpg"" alt=""kernel density estimate of the mixture""></p>

<p>When trying to fit a two-component student t mixture to these data either I get this error message (the deeper meaning of which I don't understand):</p>

<p><img src=""http://i.stack.imgur.com/UPvg4.jpg"" alt=""enter image description here""></p>

<p>or I get wrong results (convergenve is reached only after approximately two hours as can be seen from the output):</p>

<p><img src=""http://i.stack.imgur.com/HjlfW.jpg"" alt=""enter image description here""></p>

<p>The means could be estimated well, but both the variance and the degrees of freedom are estimated badly. In the TF2 implementation of the student t, the sigma parameter denotes the standard deviation. Its estimate is NEGATIVE for the first component! And for the second component the degrees of freedom estimate is also NEGATIVE. Probably one should not use these results in practice :(</p>

<p>By the way: Is there a way to restrict these degree-of-freedom coefficient estimates to be natural numbers? </p>

<p>The following syntax is my gamlss.mx-setup so far:</p>

<pre><code>library(gamlss.dist)
library(gamlss.mx)
library(MASS)

# example 1 (real data):
data(geyser)
plot(density(geyser$waiting) )
fit1 &lt;- gamlssMX( waiting~1,data=geyser,family=""TF2"",K=2 )
fit1
# works fine

# example 2 (simulated data):
N &lt;- 100000
components &lt;- sample(1:2,prob=c(0.6,0.4),size=N,replace=TRUE)
mus &lt;- c(3,-6)    # denotes the mean of component 1 and 2, respectively
sds &lt;- c(1,9)     # ... the standard deviations
nus &lt;- c(25,3)    # ... the degrees of freedom
mixsim &lt;-data.frame(rTF2( N,mu=mus[components],sigma=sds[components],nu=nus[components] ))
colnames(mixsim) &lt;- ""MCsim""
plot(density(mixsim$MCsim) , xlim=c(-50,50))
fit2 &lt;- gamlssMX(MCsim~1,data=mixsim,family=""TF2"",K=2)
fit2
# error message or strange results (this also happens when using a sample of S&amp;P500 returns)
</code></pre>

<p>I would be very grateful for any advice!
I've read through many related manuals and vignettes so far but I`m still lost.</p>

<p>Thanks a lot in advance!!
Jo</p>
"
"0.167836271659338","0.170940864689457"," 96010","<p>I am struggling to fit alternative count models into my data. I guess my problem is just too many zeros.</p>

<p>This is my data</p>

<pre><code>&gt; summary(smpl)
    response        predict1          predict2        
 Min.   :0.000   Min.   :   1.00   Min.   :    22005  
 1st Qu.:0.000   1st Qu.:   3.00   1st Qu.:  4669705  
 Median :0.000   Median :   8.00   Median : 12540318  
 Mean   :0.017   Mean   :  23.27   Mean   : 20382574  
 3rd Qu.:0.000   3rd Qu.:  20.00   3rd Qu.: 25468156  
 Max.   :3.000   Max.   :1584.00   Max.   :145348049

&gt; table(smpl$response)
  0   1   2   3 
987  10   2   1 
</code></pre>

<p>I tried three regressions: basic Poisson, negative binomial and zero-inflated but the only formula returning coefficients without warnings is the Poisson:</p>

<pre><code>&gt; summary(glm(response ~ ., data = smpl, family = poisson))

Call:
glm(formula = response ~ ., family = poisson, data = smpl)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.3871  -0.2214  -0.1722  -0.1148   4.7861  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -3.472e+00  3.521e-01  -9.862  &lt; 2e-16 ***
predict1     3.229e-03  7.271e-04   4.442 8.93e-06 ***
predict2    -6.258e-08  3.060e-08  -2.045   0.0409 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for poisson family taken to be 1)

    Null deviance: 150.67  on 999  degrees of freedom
Residual deviance: 135.84  on 997  degrees of freedom
AIC: 170.06

Number of Fisher Scoring iterations: 8
</code></pre>

<p>The negative binomial returns a warnings on both the convergence and the alternation limit</p>

<pre><code>summary(glm.nb(response ~ ., data = smpl))

Call:
glm.nb(formula = response ~ ., data = smpl, init.theta = 0.04901296596, 
    link = log)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.28844  -0.17677  -0.14542  -0.09808   2.38314  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -3.899e+00  4.587e-01  -8.499  &lt; 2e-16 ***
predict1     1.226e-02  2.144e-03   5.720 1.06e-08 ***
predict2    -5.982e-08  3.407e-08  -1.756   0.0791 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for Negative Binomial(0.049) family taken to be 1)

    Null deviance: 69.927  on 999  degrees of freedom
Residual deviance: 55.940  on 997  degrees of freedom
AIC: 152.37

Number of Fisher Scoring iterations: 1


              Theta:  0.0490 
          Std. Err.:  0.0251 
Warning while fitting theta: alternation limit reached 

 2 x log-likelihood:  -144.3700 
Warning messages:
1: glm.fit: algorithm did not converge 
2: In glm.nb(response ~ ., data = smpl) : alternation limit reached
</code></pre>

<p>and the zero-inflated (from the <code>pscl</code> package) doesn't return anything at all</p>

<pre><code>&gt; summary(zeroinfl(response ~ ., data = smpl, dist = ""negbin""))

Call:
zeroinfl(formula = response ~ ., data = smpl, dist = ""negbin"")

Pearson residuals:
     Min       1Q   Median       3Q      Max 
-0.45252 -0.08817 -0.05515 -0.04210 19.56118 

Count model coefficients (negbin with log link):
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -1.477e+00         NA      NA       NA
predict1     2.678e-03         NA      NA       NA
predict2    -1.160e-07         NA      NA       NA
Log(theta)  -1.241e+00         NA      NA       NA

Zero-inflation model coefficients (binomial with logit link):
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  4.869e+00         NA      NA       NA
predict1    -1.329e-01         NA      NA       NA
predict2    -1.346e-07         NA      NA       NA
Error in if (getOption(""show.signif.stars"") &amp; any(rbind(x$coefficients$count,  : 
  missing value where TRUE/FALSE needed
</code></pre>

<p>Then my questions are: </p>

<ol>
<li>Is there anything I can do in terms of ""formula tweaking"" with the negative binomial (to avoid the warnings) and with the zero-inflated (to get the coefficients)?</li>
<li>Looking only at the results above (thus including problems with convergence and alternation limit) should I select the negative binomial model since it seems, looking at the AIC, to fit better than the Poisson in my data?</li>
</ol>
"
"0.0456792473218354","0.0697863157798853"," 96510","<p>The function <code>resettest</code> in the <code>R</code> package <code>lmtest</code> runs the Ramsey RESET test.  A student today brought to my attention that this function behaves oddly when it is handed a regression which displays perfect fit:</p>

<pre><code>library(lmtest)

x &lt;- seq(1,100)
y &lt;- x

reg1 &lt;- lm(y~x)
summary(reg1)

# Should match but don't
summary(lm(y~x+I(x^2)))
resettest(reg1,power=2,type=""regressor"")

set.seed(12344321)
x &lt;- rnorm(1000)
y &lt;- x

reg2 &lt;- lm(y~x)
summary(reg2)

# Should match but don't
summary(lm(y~x+I(x^2)))
resettest(reg2,power=2,type=""regressor"")

set.seed(12344321)
x &lt;- seq(1,100)
y &lt;- x + 0.001*rnorm(100)

reg3 &lt;- lm(y~x)
summary(reg3)

# Should match and do, even though almost the same as reg1 example
summary(lm(y~x+I(x^2)))
resettest(reg3,power=2,type=""regressor"")
</code></pre>

<p>In this code sample, the p-value for the x^2 term should match the p-value for the <code>resttest</code> in each of the three pairs in the code.  What actually happens is that they fail to match in examples reg1 and reg2---two cases where the regression fits perfectly.  In example reg3---which is almost identical to example reg1, differing only by the addition of a tiny bit of noise---they do match.</p>

<p>This does not have a lot of practical significance since real world regressions basically never fit perfectly, but it is disconcerting if you are just playing around with the function to convince yourself that you understand what it does.</p>

<p>Does anyone understand why this seemingly undesirable behavior occurs or if I am doing something wrong in my use of <code>resttest</code>?  Maybe the test statistic ends up being calculated as (roundoff error)/(roundoff error)?</p>
"
"0.0625488854200668","0.0637058989297032"," 96999","<p>I'm working on a model that requires me to look for predictors for a rare event (less than 0.5% of the total of my observations). My total sample is a significant part of the total population (50,000 cases). My final objective is to obtain comparable probability values for all the non-events, without the bias of the groups difference in the logistic regression. </p>

<p>I've been reading the info in the following link: </p>

<p><a href=""http://gking.harvard.edu/files/gking/files/0s.pdf"" rel=""nofollow"">http://gking.harvard.edu/files/gking/files/0s.pdf</a></p>

<p>It advises me first to use a sample of my original sample, containing all the events (1) and a random sample of 1-5 times bigger of the non-event (0) sample.</p>

<p>Then it suggests using weights based on the proportion of the sample 1s to 0s. In the section 4.2 of the linked text, he offers a ""easy to implement"" weighted log-likelihood that can be implemented in any logit function.</p>

<p>I wish to implement these weights somehow with R's glm(...,family=binomial(link=""logit"")) or similar function ( the ""weights"" parameter is not for frequency weighting), but I don't really know how to apply this weighting.</p>

<p>Does anybody knows how to make it or any other alternative suggestion?  </p>

<p><strong>Edit1:</strong> As suggested bellow, is Firth's method for bias-correction by penalizing the likelihood in the <code>logistf</code> package a correct approach in this case? I'm not much knowledgeable in statistics, and, while I understand the input and the coefficients/output of the logistic model, what happens in between is still quite a mystery to me, sorry.</p>
"
"0.0685188709827532","0.0697863157798853"," 97811","<p>I am using leave-one-out cross-validation to evaluate a linear regression model. In subsequent analysis, I need three specific values for each observation: observed value, predicted value, prediction standard error. Prediction standard error values can be retrieved from function <code>predict.lm</code> setting argument <code>se.fit = TRUE</code>. The following code (adapted from <a href=""http://www.analyticbridge.com/profiles/blogs/cross-validation-in-r-a-do-it-yourself-and-a-black-box-approach"" rel=""nofollow"">here</a>) can be used to do what I currently need:</p>

<pre><code>library(faraway)
gala[1:3, ]
c1 &lt;- c(1:30)
gala2 &lt;- cbind(gala, c1)
gala2[1:3, ]
obs  &lt;- numeric(30)
pred &lt;- numeric(30)
se   &lt;- numeric(30)
for (i in 1:30) {
     model1  &lt;- lm(Species ~ Endemics + Area + Elevation,
                   subset = (c1 != i), data = gala2)
     specpr  &lt;- predict(model1, gala2[i, ], se.fit = TRUE)
     obs[i]  &lt;- gala2[i, 1]
     pred[i] &lt;- specpr$fit
     se[i]   &lt;- specpr$se.fit
}
res &lt;- data.frame(obs, pred, se)
head(res)
  obs       pred       se
1  58  70.185063 5.524249
2  31  72.942732 6.509655
3   3  -8.303608 7.055163
4  25  20.948932 6.998093
5   2 -15.953141 7.403062
6  18  27.274440 6.220029
</code></pre>

<p>I searched through the documentation of some of the packages that offer functions for cross-validation, but did not find any that saves prediction standard errors. Is there any package that already offers such functionality?</p>
"
"NaN","NaN"," 97839","<p>I'm very new to <strong>R</strong> and PLS-regression. I would like to know, based on your experience, which <strong>R</strong> packages for PLS-regression are most highly recommended. My area of application is chemistry.</p>
"
"0.027972711943223","0.0284901441149095"," 99272","<p>I'm using a regression package that uses <code>gam()</code> from the <code>mgcv</code> package. Is it possible to include an L1 penalty with the <code>gam</code> function? The documentation indicates that an L2 penalty can be used using the <em>H</em> parameter, but I do not understand how, or if it can be used for the lasso as well. Splines are not in the model; the only reason why I'm not using <code>glmnet</code> is because the function I use has GAM implemented. Example code:</p>

<pre><code>formula.eq = Y ~ X
model.out = gam(formula.eq, binomial(link = ""logit""), gamma = 1)
</code></pre>

<p>Where <code>Y</code> is a vector of zeros and ones, and <code>X</code> includes > 400 features, so I need to perform some shrinkage method.
Thanks.</p>
"
"0.0625488854200668","0.0509647191437626"," 99334","<p>I need a package for missing data imputation in R. But since I am dealing with big data, the number of missing data entries can also be high. The packages which impute using mean or median are of course working fast, but more complicated packages which impute using regression or PCA take too long for a high number of missing values. I tried <code>missMDA</code> and <code>missForest</code>, but as I said, they look like taking forever. There is a package named <code>FastImputation</code>, but I could not figure out how to use it when I have no patterns from some training data. Any suggestions of packages which would impute fast?</p>
"
"0.027972711943223","0.0284901441149095"," 99981","<p>I am doing bivariate kernel regression using the sm.regression function:<br>
<a href=""http://cran.r-project.org/web/packages/sm/sm.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/sm/sm.pdf</a></p>

<p>There is an option to compare the nonparametric estimate with linear model. When requested, it returns p-value that the nonparametric estimate is not different from the linear (null hypothesis).</p>

<p>Does anyone know what test is used to calculate the p-value? I can't find it anywhere.</p>

<p>The answer might be in:
Bowman, A.W. (2006). Comparing nonparametric surfaces. Statistical Modelling, 6, 279-299.</p>

<p>Thank you!</p>
"
"0.121930224537282","0.1176495717636","100101","<p>I am currently working at work on a project that attempts to predict an environmental change variable. I am personally not a huge fan of the project, but I still want to do the best job possible. Anyhow, let me first describe the properties of the data, and then state my question. </p>

<p>The environmental variable we are trying to model is continuous, and ranges from 0 to 25 (it can have values such as 0.1345 or 1.2335 or 5.674). The environmental variable also has a significant mass at zero (around 30% of the data are zero values), and most of the data is in the range between > 0 and &lt; 1. To complicate things further, around 3% of the data have extreme values of greater than 2. In my opinion, the distribution of the data resembles a Tweedie distribution. </p>

<p>We have around 5 million observations in the dataset. We are going to predict the environmental variable using a set of eight explanatory variables.</p>

<p>We have been modeling the prediction of the environmental for a week, and the predictive power of our results has been meager. Here are the different modeling approaches I have used:</p>

<ol>
<li><p>GLM model with a tweedie distribution (glm function in r). This model approach overestimate the environmental change when the variables has low values. Most values between 0 - 1 are significantly overestimated, and the model does not predict high values very well either. The GLM approach produces a very bad fit for our data.</p></li>
<li><p>Generalized Additive Model with a tweedie distribution (bam function from mgcv package in r). This approaches fits the data slightly better than the model above, but still significantly overestimates values in the low range.</p></li>
<li><p>Regression tree model (rpart model in r). The regression tree model most accurately predicts values in the lower range of the distribution, but fails to predict zero values, and performs also poorly for values greater than 2. </p></li>
<li><p>Boosted regression model (dismo package in r using gbm.step). The model significantly overestimate values in the lower range of the distribution. For example, if values are 0.23 it predicts values to be 1.67 and so forth. I believe the gbm.step model to be inadequate for our data, since the family of distribution in the package only models the bernoulli (=binomial), poisson, laplace or gaussian family. None of which accurately describe our distribution.</p></li>
</ol>

<p>Since our dataset is very large, I split the data 50/50 into a test and training dataset, and evaluated model fit on a variety of test statistics for the test data set. </p>

<p>Knowing the structure of our data, can anyone think of our modeling approaches that would possibly produce better predictive results? I am still new to machine learning techniques, and I hope that someone might know of other techniques that could be employed.   </p>

<p>My strongest program language is R, but I can also do this analysis in Python or Stata. </p>
"
"0.0634361479695551","0.0646095738380922","100841","<p>Say I have below example data, where rows are observations and columns are variables, and NAs stand for missing values.</p>

<pre><code> 1  2 NA  4  5 6 14 5  2
 6 13  7  1 11 4 NA 9  6
15 12  3 12 NA 8  3 7 12
 8  1 NA  7  8 9  4 6  1
</code></pre>

<p>I want to impute the missing values by regression (I know I can impute by means, but I need to see how regression performs). There is a CRAN package named 'Amelia' for imputation by regression, but it gives an error for above data saying that #observations is smaller than #variables. 'mi' package also gives an error. I can code myself, but I do not want to reinvent the wheel since I am sure there is already a package for that which would work faster than the one I write (Speed is important since I will run this imputation for thousands of variables and hundreds of observations with lots of missing values). So, does anybody know about a package which would impute the values above by regression? Thanks.</p>
"
"0.0484501583111509","0.0493463771219827","101003","<p>I have a data set of around 5000 features. For that data I first used Chi Square test for feature selection; after that, I got around 1500 variables which showed significance relationship with the response variable. </p>

<p>Now I need to fit logistic regression on that. I am using glmulti package for R (glmulti package provides efficient subset selection for vlm) but it can use only 30 features at a time, else its performance goes down as the number of rows in my dataset is around 20000.</p>

<p>Is there any other approach or techniques to solve the above problems? If I go by the above method it will take too much time to fit the model.</p>
"
"NaN","NaN","101126","<p>I was wondering how can I best fit nonlinear regression model to this data, using R package. How can I check if model is good fitted since Rsquared value is not returned in most functions for nonlinear models?</p>

<p>Thanks in advance for answers
<img src=""http://ramza.tarchomin.pl/rozklad.jpeg"" alt=""data""></p>
"
"0.156772655213643","0.150548447899124","101187","<p>I know that this has been discussed before, but those discussions did not really answer my questions. I know how the ADF test works, but I am having trouble interpreting the output for the three options using the <code>ur.df</code> function in R (package: <code>urca</code>). Could someone walk me through the interpretations?
More specifically, what are <code>tau1</code>, <code>tau2</code>, <code>phi1</code>, <code>phi2</code>, and <code>phi3</code>? </p>

<pre><code>summary(ur.df(tcm.ts, type=""none"",selectlags=""BIC""))
summary(ur.df(tcm.ts, type=""drift"",selectlags=""BIC""))
summary(ur.df(tcm.ts, type=""trend"",selectlags=""BIC""))

&gt; summary(ur.df(tcm.ts, type=""none"",selectlags=""BIC""))

############################################### 
# Augmented Dickey-Fuller Test Unit Root Test # 
############################################### 

Test regression none 


Call:
lm(formula = z.diff ~ z.lag.1 - 1 + z.diff.lag)

Residuals:
    Min      1Q  Median      3Q     Max 
-95.199 -23.380  -6.608  26.885  86.560 

Coefficients:
           Estimate Std. Error t value Pr(&gt;|t|)   
z.lag.1     0.04398    0.01205   3.650  0.00183 **
z.diff.lag -0.03722    0.24417  -0.152  0.88053   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 45 on 18 degrees of freedom
Multiple R-squared:  0.7091,    Adjusted R-squared:  0.6768 
F-statistic: 21.94 on 2 and 18 DF,  p-value: 1.492e-05


Value of test-statistic is: 3.6495 

Critical values for test statistics: 
      1pct  5pct 10pct
tau1 -2.66 -1.95  -1.6

&gt; summary(ur.df(tcm.ts, type=""drift"",selectlags=""BIC""))

############################################### 
# Augmented Dickey-Fuller Test Unit Root Test # 
############################################### 

Test regression drift 


Call:
lm(formula = z.diff ~ z.lag.1 + 1 + z.diff.lag)

Residuals:
    Min      1Q  Median      3Q     Max 
-69.366 -24.625  -3.018  34.165  82.227 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept) -75.21181   45.89715  -1.639   0.1196  
z.lag.1       0.09756    0.03467   2.814   0.0119 *
z.diff.lag   -0.20396    0.25469  -0.801   0.4343  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 43.03 on 17 degrees of freedom
Multiple R-squared:  0.3596,    Adjusted R-squared:  0.2843 
F-statistic: 4.773 on 2 and 17 DF,  p-value: 0.02264


Value of test-statistic is: 2.814 8.6258 

Critical values for test statistics: 
      1pct  5pct 10pct
tau2 -3.75 -3.00 -2.63
phi1  7.88  5.18  4.12

&gt; summary(ur.df(tcm.ts, type=""trend"",selectlags=""BIC""))

############################################### 
# Augmented Dickey-Fuller Test Unit Root Test # 
############################################### 

Test regression trend 


Call:
lm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag)

Residuals:
   Min     1Q Median     3Q    Max 
-86.46 -14.84   5.56  20.87  70.29 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)  91.4039    92.1407   0.992   0.3360  
z.lag.1      -0.1127     0.1082  -1.042   0.3129  
tt           13.0810     6.4318   2.034   0.0589 .
z.diff.lag   -0.1287     0.2369  -0.543   0.5946  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 39.54 on 16 degrees of freedom
Multiple R-squared:  0.4911,    Adjusted R-squared:  0.3957 
F-statistic: 5.148 on 3 and 16 DF,  p-value: 0.01109


Value of test-statistic is: -1.042 8.1902 6.7579 

Critical values for test statistics: 
      1pct  5pct 10pct
tau3 -4.38 -3.60 -3.24
phi2  8.21  5.68  4.67
phi3 10.61  7.24  5.91
</code></pre>
"
"0.0484501583111509","0.0493463771219827","102667","<p>I've created a regression model on my data using random forests in R. The output is quite large, I'm wondering if there's any way to reduce this to only the necessary pieces to make a prediction?</p>

<p>The training data set contains 20 variables and ~45,000 rows, which is also large. My code is listed below.</p>

<pre><code>data &lt;- readRDS(""data.Rds"")

require(""data.table"")
require(""doParallel"")
require(""randomForest"")

train &lt;- data[ which(set == ""train"")]
test &lt;- data[ which(set == ""test"")]
rm(data)

x &lt;- data.table(train[, 2:21, with=FALSE])
y &lt;- as.vector(as.matrix(train[, 23, with=FALSE]))

cl &lt;- makeCluster(detectCores())
registerDoParallel(cl, cores=4)
time &lt;- system.time({rf.fit &lt;- foreach(ntree=rep(500, 6),
                               .combine=combine,
                               .multicombine=TRUE,
                               .packages=""randomForest"") %dopar% 
                   {randomForest(x, y, ntree=ntree)}})
stopCluster(cl)

saveRDS(rf.fit, ""rf.fit.Rds"")
</code></pre>

<p>The output of this is ~230 MB. Once I have the model, is it possible to reduce the size to make it easier to work with? My goals with this are to identify the important variables, and make a prediction on new data. </p>
"
"0.0843408998948762","0.0859010165930062","102892","<p>My situation:</p>

<ul>
<li>small sample size: 116</li>
<li>binary outcome variable</li>
<li>long list of explanatory variables: 44</li>
<li>explanatory variables did not come from the top of my head; their choice was based on the literature.</li>
</ul>

<p>Statistical test chosen: logistic regression</p>

<p>I need to find the variables that best explain variations in the outcome variable (I am not interested in making predictions).</p>

<p>The problem: This question is a follow-up on the 2 questions listed below. From them, I got that performing automated stepwise regression has its downsides. Anyway, it seems that my sample size would be too small for that. It seems that my sample is also too small to enter all variables at once (using the SPSS 'Enter' method). This leaves me with my issue unresolved: how can I select a subset of variables from my original long list in order to perform multivariate logistic regression analysis?</p>

<p>UPDATE1: I am not an statistician, so I would appreciate if jargons can be reduced to the minimum. I am working with SPSS and am not familiar with other packages, so options that could be run with that software would be highly preferable.</p>

<p>UPDATE2: It seems that SPSS does not support LASSO for logistic regression. So following one of your suggestions, I am now struggling with R. I have passed through the basics, and managed to run a univariate logistic regression routine successfully using the glm code. But as I tried glmnet with the same dataset, I am receiving an error message. How could I fix it? Below is the code I used, followed by the error message:</p>

<pre><code>data1 &lt;- read.table(""C:\\\data1.csv"",header=TRUE,sep="";"",na.string=99:9999)

y &lt;- data1[,1]

x &lt;- data1[,2:45]

glmnet(x,y,family=""binomial"",alpha=1)  

**in lognet(x, is.sparse, ix, jx, y, weights, offset, alpha, nobs,  : 
(list) object cannot be coerced to type 'double'**
</code></pre>

<p>UPDATE3: I got another error message, now related to missing values. My question concerning that matter is <a href=""http://stats.stackexchange.com/questions/104194/how-to-handle-with-missing-values-in-order-to-prepare-data-for-feature-selection"">here</a>. </p>

<ul>
<li><a href=""http://stats.stackexchange.com/questions/88482/can-univariate-linear-regression-be-used-to-identify-useful-variables-for-a-subs"">Can univariate linear regression be used to identify useful variables for a subsequent multiple logistic regression?</a></li>
<li><a href=""http://stats.stackexchange.com/questions/20836/algorithms-for-automatic-model-selection/20856#20856"">Algorithms for automatic model selection</a></li>
</ul>
"
"0.0395593886064618","0.040291148201269","103063","<p>I have some questions about penalized Bayesian quantile regression with LASSO and adaptive LASSO penalty: </p>

<ol>
<li>Would you give me a detailed outline for the formula, especially as used in Bayesian inference? </li>
<li>What package in R is easy to use and suitable for this topic?</li>
<li>Would you give me R code for running inference Bayesian quantile regression with LASSO penalty?</li>
</ol>
"
"0.0791187772129236","0.080582296402538","103280","<p>I'm attempting a project where I need to statistically rank available cars based on several variables such as cost, mpg, seating, milage, etc.. I wish to rank these cars in order decide which car would be the best choice (highest ""worth"") to buy (or best several cars if I was informing multiple people of the best cars to get). As the list of available cars changes from day to day, I will also need to re-run the code every day to allow the rankings to give me the best decision for this new day. </p>

<p>What statistical methods should I use to go about this ranking system? I plan on determining which factors I find most important so the variables used will be subjective in choice. I thought about trying MDS or clustering but I didn't know if that would be relevant since I'm already subjectively determining what variables are to be used. I don't see how regression can be used since I can't get a handle on the ""worth"" of previous cars as that is what I'm trying to rank by. Also, I will be attempting this in R so any helpful packages/functions would be great to know as well.</p>

<p>Any help with how to go about this ranking scheme would be helpful as I'm at a loss.</p>

<p>Thanks so much</p>
"
"0.0559454238864459","0.056980288229819","103598","<p>I'm wondering how to explain the <code>logSigma</code> and its p.value in a censored regression model:</p>

<pre><code>require(cenReg)
data( ""Affairs"", package = ""AER"" )
estResult &lt;- censReg( affairs ~ age + yearsmarried + religiousness +
                           occupation + rating, data = Affairs )
summary(estResult)

Call:
censReg(formula = affairs ~ age + yearsmarried + religiousness + 
    occupation + rating, data = Affairs)

Observations:
         Total  Left-censored     Uncensored Right-censored 
           601            451            150              0 

Coefficients:
              Estimate Std. error t value  Pr(&gt; t)    
(Intercept)    8.17420    2.74145   2.982  0.00287 ** 
age           -0.17933    0.07909  -2.267  0.02337 *  
yearsmarried   0.55414    0.13452   4.119 3.80e-05 ***
religiousness -1.68622    0.40375  -4.176 2.96e-05 ***
occupation     0.32605    0.25442   1.282  0.20001    
rating        -2.28497    0.40783  -5.603 2.11e-08 ***
logSigma       2.10986    0.06710  31.444  &lt; 2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Newton-Raphson maximisation, 7 iterations
Return code 1: gradient close to zero
Log-likelihood: -705.6 on 7 Df
</code></pre>

<p>My question is how to explain the logSigma and its significant p.value in the above model.</p>
"
"0.0484501583111509","0.0328975847479884","103638","<p>I'm using R and the package boot.</p>

<p>my boot function returns the coefficients and the r.square and sqrt of the r.square </p>

<pre><code> rsq2 &lt;- function(formula, data, indices) {
     d &lt;- data[indices,] # allows boot to select sample 
     fit &lt;- lm(formula, data=d)
     return(c( coef(fit),summary(fit)$r.square,sqrt(summary(fit)$r.square)))
 }

boot(data=mtcars,rsq2,1000,formula=""mpg~wt"")

#ORDINARY NONPARAMETRIC BOOTSTRAP

Call:
boot(data = mtcars, statistic = rsq2, R = 1000, formula = ""mpg~wt"")


Bootstrap Statistics :
      original        bias    std. error
t1* 37.2851262  0.1170681005  2.32470420
t2* -5.3444716 -0.0557873180  0.70523904
t3*  0.7528328  0.0019940314  0.05758118
t4*  0.8676594  0.0004980607  0.03362469
</code></pre>

<p>I determine a pvalue using the original value and the std. error determined by the boot program</p>

<pre><code>pvalue of the regression coefficient
2*pnorm(-abs(-5.3444716/0.70523904))
[1] 3.502706e-14
pvalue of the CC
 2*pnorm(-abs(0.8676594/0.03362469))
[1] 7.947938e-147
</code></pre>

<p>Why are these not the same value?</p>

<p>In a simple univariable the linear regression of the p-value of the coefficient and Pearson's CC are the same value?</p>
"
"0.0559454238864459","0.056980288229819","103884","<p>I would like to find if there is a significant difference between two ROC-curves. I've found the roc.test in the pROC package. However, I cannot seem to find any information on how this test is actually performed. Can anyone explain it to me or refer to a webpage that has information on this?</p>

<p>From the <a href=""http://www.inside-r.org/packages/cran/pROC/docs/roc.test"" rel=""nofollow"">documentation</a> for <code>roc.test()</code>, I've found that the used test is DeLong's test (based on the article 'Comparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approach'). However, I don't quite understand the article. </p>

<p>My ROC-curves have been found by comparing two logistic regressions, where one has a subset of attributes of the other seeing as I want to examine whether these attributes are useful. The probabilities are found using 5-fold cross-validation.</p>
"
"0.0927749898843639","0.0944911182523068","104194","<p>My situation:</p>

<ul>
<li>small sample size: 116 </li>
<li>binary outcome variable</li>
<li>long list of explanatory variables: 44</li>
<li>explanatory variables did not come from the top of my head; their choice was based on the literature.</li>
<li>most cases in the sample and most variables have missing values.</li>
</ul>

<p>Approach to feature selection chosen: LASSO</p>

<p>R's glmnet package won't let me run the glmnet routine, apparently due to the existence of missing values in my data set. There seems to be various methods for handling missing data, so I would like to know:</p>

<ul>
<li>Does LASSO impose any restriction in terms of the method of imputation that I can use?</li>
<li>What would be the best bet for imputation method? Ideally, I need a method that I could run on SPSS (preferably) or R.</li>
</ul>

<p>UPDATE1: It became clear from some of the answers below that I have do deal with more basic issues before considering imputation methods. I would like to add here new questions regarding that. On the the answer suggesting the coding as constant value and the creation of a new variable in order to deal with 'not applicable' values and the usage of group lasso:</p>

<ul>
<li>Would you say that if I use group LASSO, I would be able to use the approach suggested to continuous predictors also to categorical predictors? If so, I assume it would be equivalent to creating a new category - I am wary that this may introduce bias.</li>
<li>Does anyone know if R's glmnet package supports group LASSO? If not, would anyone suggest another one that does that in combination with logistic regression? Several options mentioning group LASSO can be found in CRAN repository, any suggestions of the most appropriate for my case? Maybe SGL?</li>
</ul>

<p>This is a follow-up on a previous question of mine (<a href=""http://stats.stackexchange.com/questions/102892/how-to-select-a-subset-of-variables-from-my-original-long-list-in-order-to-perfo"">How to select a subset of variables from my original long list in order to perform logistic regression analysis?</a>).</p>

<p>OBS: I am not a statistician.</p>
"
"0.0625488854200668","0.0637058989297032","104306","<p>One common thing to do when doing Principal Component Analysis (PCA) is to plot two loadings against each other to investigate the relationships between the variables. In the paper accompanying the <a href=""http://www.jstatsoft.org/v18/i02"">PLS R package</a> for doing Principal Component Regression and PLS regression there is a different plot, called the <em>correlation loadings plot</em> (see figure 7 and page 15 in the paper). The <em>correlation loading</em>, as it is explained, is the correlation between the scores (from the PCA or PLS) and the actual observed data. </p>

<p>It seems to me that loadings and correlation loadings are pretty similar, except that they are scaled a bit differently. A reproducible example in R, with the built in data set mtcars is as follows:</p>

<pre><code>data(mtcars)
pca &lt;- prcomp(mtcars, center=TRUE, scale=TRUE)

#loading plot
plot(pca$rotation[,1], pca$rotation[,2],
     xlim=c(-1,1), ylim=c(-1,1),
     main='Loadings for PC1 vs. PC2')

#correlation loading plot
correlationloadings &lt;- cor(mtcars, pca$x)
plot(correlationloadings[,1], correlationloadings[,2],
     xlim=c(-1,1), ylim=c(-1,1),
     main='Correlation Loadings for PC1 vs. PC2')
</code></pre>

<p><img src=""http://i.stack.imgur.com/GJA4c.png"" alt=""loadingplot"">
<img src=""http://i.stack.imgur.com/y32sb.png"" alt=""correlationloadinsplot""></p>

<p>What is the difference in interpretation of these plots? And which plot (if any) is best to use in practice?</p>
"
"0.0484501583111509","0.0493463771219827","104722","<p>I'm calculating a penalized least squares regression (PLS). Two influence variables are connected to a latent variable. This latent variable has an influence (beta coefficient) of 0.6 on the response (or latent variable of the response).</p>

<p>Both influence variables are correlated (outer loading) with 0.6 and 0.8 to the latent variable. </p>

<p>Does anyone know how to (back)-calculate a beta coefficient for each influence variable and not just the latent variable connected to the influence variables?</p>

<p>I'm using a mix of the smartPLS programming tool and R.</p>

<p>EDIT:
<a href=""http://stackoverflow.com/questions/24446300/sempls-package-backcalculating-influence"">http://stackoverflow.com/questions/24446300/sempls-package-backcalculating-influence</a></p>
"
"0.0884574820723792","0.0900937462695559","104889","<p>I am working on cross-validation of prediction of my data with 200 subjects and 1000 variables. I am interested ridge regression as number of variables (I want to use) is greater than number of sample. So I want to use shrinkage estimators.  The following is made up example data:</p>

<pre><code> #random population of 200 subjects with 1000 variables 
    M &lt;- matrix(rep(0,200*100),200,1000)
    for (i in 1:200) {
    set.seed(i)
      M[i,] &lt;- ifelse(runif(1000)&lt;0.5,-1,1)
    }
    rownames(M) &lt;- 1:200

    #random yvars 
    set.seed(1234)
    u &lt;- rnorm(1000)
    g &lt;- as.vector(crossprod(t(M),u))
    h2 &lt;- 0.5 
    set.seed(234)
    y &lt;- g + rnorm(200,mean=0,sd=sqrt((1-h2)/h2*var(g)))

    myd &lt;- data.frame(y=y, M)
myd[1:10,1:10]

y X1 X2 X3 X4 X5 X6 X7 X8 X9
1   -7.443403 -1 -1  1  1 -1  1  1  1  1
2  -63.731438 -1  1  1 -1  1  1 -1  1 -1
3  -48.705165 -1  1 -1 -1  1  1 -1 -1  1
4   15.883502  1 -1 -1 -1  1 -1  1  1  1
5   19.087484 -1  1  1 -1 -1  1  1  1  1
6   44.066119  1  1 -1 -1  1  1  1  1  1
7  -26.871182  1 -1 -1 -1 -1  1 -1  1 -1
8  -63.120595 -1 -1  1  1 -1  1 -1  1  1
9   48.330940 -1 -1 -1 -1 -1 -1 -1 -1  1
10 -18.433047  1 -1 -1  1 -1 -1 -1 -1  1
</code></pre>

<p>I would like to do following for cross validation - </p>

<p>(1) split data into two halts - use first half as training and second half as test </p>

<p>(2) K-fold cross validation (say 10 fold or suggestion on any other appropriate fold for my case are welcome)  </p>

<p>I can simply sample the data into two (gaining and test) and use them: </p>

<pre><code># using holdout (50% of the data) cross validation 
training.id &lt;- sample(1:nrow(myd), round(nrow(myd)/2,0), replace = FALSE)
test.id &lt;- setdiff(1:nrow(myd), training.id)

 myd_train &lt;- myd[training.id,]
 myd_test  &lt;- myd[test.id,]   
</code></pre>

<p>I am using <code>lm.ridge</code> from <code>MASS</code> R package. </p>

<pre><code>library(MASS)
out.ridge=lm.ridge(y~., data=myd_train, lambda=seq(0, 100,0.001))
plot(out.ridge)
select(out.ridge)

lam=0.001
abline(v=lam)

out.ridge1 =lm.ridge(y~., data=myd_train, lambda=lam)
hist(out.ridge1$coef)
    out.ridge1$ym
hist(out.ridge1$xm)
</code></pre>

<p>I have two questions - </p>

<p>(1) How can I predict the test set and calculate accuracy (as correlation of predicted vs actual)?</p>

<p>(2) How can I perform K-fold validation? say 10-fold?</p>
"
"0.0484501583111509","0.0493463771219827","105006","<p>Is there any way to perform bivariate regression using pairwise deletion of missing values in R? na.action options in lm() do not offer such a possibility â€“ the default na.action is na.omit, which is equivalent to listwise deletion. I already tried estimating the covariance matrix using pairwise deletion and then use the function mat.regress (package psych) with the pairwise covariance matrix. However, mat.regress is a function to compute multiple regression (not bivariate). Thank  you.</p>
"
"0.0395593886064618","0.040291148201269","105183","<p>Pardon my ignorance, for I have very basic training in statistics, but have never taken a regression course. </p>

<p>I ran an experiment where participants are in one of two conditions (between subjects). They categorized an object that had values for rotation and spacing, both ranging from 1 to 100. Each participant saw 40 of those values twice. There were a lot of participants. </p>

<p>What I want to know is for what values of rotation (or spacing) were the participants as a whole closest to chance (.5, is it or is it not in the category) in each condition, and whether those values differ significantly from one another. </p>

<p>What is this process called, and what are some recommended packages or methods in SAS, R, SPSS, Mathematica, or Stata (or really anything) I could use to go about figuring it out. </p>

<p>Any help rephrasing the question is also very welcome. </p>
"
"NaN","NaN","105406","<p>I'm trying to run K-fold cross-validation on a multiple regression model that was generated via the <code>step</code> function in R. However, the call to <code>cv.glm</code> returns <code>NaN</code>.</p>

<p>Here is a simplified script that replicates the problem:</p>

<pre><code># this works just fine
library(boot)
data(mammals, package=""MASS"")
mammals.glm &lt;- glm(log(brain) ~ log(body), data = mammals)
cv.glm(mammals, mammals.glm, K=5)$delta

# but when I use the step function...
mammals.simple &lt;- lm((""brain ~ 1""), data=mammals)
mammals.glm &lt;- step(mammals.simple, direction=""forward"", test=""F"", scope=""brain ~ body"")
# it returns NaN
cv.glm(mammals, mammals.glm, K=5)$delta
</code></pre>

<p>Is this due to some fundamental misunderstanding, where it doesn't make sense to cross-validate a stepwise-generated model?</p>

<p>I'm using R v3.0.1 (but 3.1.0 has the same problem).</p>
"
"0.0395593886064618","0.040291148201269","105424","<p>I'm trying to do a linear regression in R, however I have the added constraint the coefficients from the linear regression need to sum to a user given value between 0 and 1. (I understand forcing the coefficients like this will make the fit rather poor, but it's a needed constraint)</p>

<p>I've been though the documentation for lm() and the glmc package, as well as some similar questions, but none seem to tackle how to get the coefficients to sum to a specific value.</p>

<p>For data T, N, and target sum for coefficients p:</p>

<pre><code>Regression &lt;- function(T, N, p) {    
     fit &lt;- lm(T[,1] ~ N)
     coef &lt;- coef(fit)
...
}
</code></pre>

<p>Ideally I want sum(coef[-1]) = p (ignoring the intercept)</p>

<p>Sorry I can't provide more code, but I don't think I can do what I would like with lm().</p>

<p>Does anyone know of a way to do this, or of a package that will let me do this? </p>
"
"0.0419590679148345","0.056980288229819","105480","<p>I have a survey question where the respondent can check one choice or two choice maximum. Question looks like this:</p>

<blockquote>
  <p>What is the more important characteristic when you buy chocolate?</p>
  
  <ul>
  <li>Sweetness (characteristic 1) </li>
  <li>Packaging (characteristic 2) </li>
  <li>Price (characteristic 3) </li>
  <li>... (characteristic 4)  </li>
  <li>... (characteristic 5) </li>
  <li>... (characteristic 6)</li>
  </ul>
</blockquote>

<p>In my data, I have 6 column (1 for each characteristic) with the value 1 if the respondent checked the ad-hoc column and 0 otherwise.</p>

<p>I would like to perform a regression where the outcome is the question and the predictors the socio demographic criteria (some of them are numerical such as the age and other categorical such as the gender).</p>

<p>Now I wonder how I should do. Should I do a simple logistic regression 6 times with same predictors but each time different outcome?</p>

<blockquote>
  <ul>
  <li>glm(caracteristic1 ~ age+gender)</li>
  <li>glm(caracteristic2 ~ age+gender)</li>
  <li>glm(caracteristic3 ~ age+gender)</li>
  <li>glm(caracteristic4 ~ age+gender)</li>
  <li>glm(caracteristic5 ~ age+gender)</li>
  <li>glm(caracteristic6 ~ age+gender)</li>
  </ul>
</blockquote>

<p>Or is it possible to use another method such as the  multinomial logistic regression. If yes which R packages do you recommend me to use?</p>
"
"0.0484501583111509","0.0328975847479884","106165","<p>I have a data set derived from administrative registers covering the population of a small European country, containing a large number of defined groups (15 000+). For each of these groups I have multiple observations (different years), and two explanatory variables that characterize the exposure of each group to a policy intervention.</p>

<p>In STATA I would estimate this using the regression command, with aweight=group_size and using absorb(group_identifier). I've done this before, and it is extremely fast. How do I do this in R?  In R, my problem is that I cannot find a package that allows me to both weight by group size AND include fixed effects.</p>

<p>Using the standard lm function I could include the group-size weight, but would have to estimate the group-effects explicitly (simply adding the groups identifier as a factor variable). This is extremely time consuming (I cancelled it after it had run for several minutes).</p>

<p>Alternatively, I could turn to fixed-effects packages - such as the lfe package. These allow for fixed effects, but I have not found any such package that also accepts weighting by group size.</p>

<p>Any suggestions?</p>
"
"0.112084934384975","0.120873444603807","106259","<p>I use SPSS, but am forced to use R for exact logistic regression.  So I'm brand new to R (and hating it so far) and also new to logistic regression.  I've read the original elrm paper and looked at examples of its use. However, I can't find information on the questions below (after the data description).</p>

<p>The fit of two models of cognitive processing was compared for each subject in each of 3 conditions. My binary dependent variable is whether the difference in model fits was significant or not (my ""Success"" variable below). I have three experimental Conditions: 0, 1, and 2.  0 is my reference group.  My question is:  is there an overall effect of Condition? If so, which conditions differ?  The specific alternative hypothesis is that the proportion/probability of ""success"" should be greater in conditions 0 and 1 than in condition 2.  My data look like</p>

<p><img src=""http://i.stack.imgur.com/OAFtP.gif"" alt=""original data""></p>

<p>...and so on. SPSS actually creates the dummy variables for you on the fly but they are easy enough to create explicitly.</p>

<p><strong>Question 1:</strong>  I have read that to use elrm you have to enter the data such that the response variable represents success/number of trials. And as far as I can tell elrm doesn't create dummy variables automatically.  I've seen examples of tables representing this data structure, but can't find any step-by-step examples of getting raw data into that format, espescially given a one-variable 3-levels situation.  Is there an example out there that I'm missing?  If not, is this what the data should look like? </p>

<p><img src=""http://i.stack.imgur.com/aVxOL.gif"" alt=""reformated data""></p>

<p>I'm not sure how I'd enter the dummy variables into the formula...just as separate variables?</p>

<p><strong>Question 2:</strong>  I can see how I can get the tests of the coefficients of the dummy variables. But I can't figure out how to get a test of the overall effect of the independent variable. I need to evaluate the overall effect of Condition before looking at individual conditions.  Is there a way to get that out of elrm? (I found an example of this done for the aod package which runs regular logistic regression but not exact logistic regression.)</p>

<p><strong>Question 3:</strong>  I can't find a description of what the p-value for individual coeffeicients represents in elrm.  Is this is for the Wald test?</p>
"
"0.0197796943032309","0.0201455741006345","107584","<p>I adjust the partial least squares regression for one categorical factor (2 levels â€“ <code>be</code> or <code>nottobe</code>) with with the <code>pls</code> package in R. I try to use <code>round()</code> function in the predict values for take the decision if the result are the first or second level in my factor. Does this approach sound correct?</p>

<pre><code>require(pls) 

#Artificial data  

T&lt;-as.factor(sort(rep(c(""be"", ""nottobe""), 100))) 

y1 &lt;- c(rnorm(100,1,0.1),rnorm(100,1,0.1)) 
y2 &lt;- c(rnorm(100,10,0.3),rnorm(100,10,0.6)) 
y3 &lt;- c(rnorm(100,10,2.3),rnorm(100,11,2.6)) 
y4 &lt;- c(rnorm(100,5,0.5),rnorm(100,7,0.5)) 
y5 &lt;- c(rnorm(100,0,0.1),rnorm(100,0,0.1)) 

#Create the data frame 
avaliacao &lt;- as.numeric(T) 
espectro &lt;- cbind(y1,y2,y3,y4,y5) 
dados &lt;- data.frame(avaliacao = I(as.matrix(avaliacao)), bands = I(as.matrix(espectro))) 

#PLS regression
taumato &lt;- plsr(avaliacao ~ bands, ncomp = 5, validation = ""LOO"", data=dados) 
summary(taumato) 

#Components analysis 
plot(taumato, plottype = ""scores"", comps = 1:5) 


#Cross validation 
taumato.cv &lt;- crossval(taumato, segments = 10) 
plot(MSEP(taumato.cv), legendpos = ""topright"") 
summary(taumato.cv, what = ""validation"") 
plot(taumato, xlab =""mediÃ§Ã£o"", ylab=""prediÃ§Ã£o"", ncomp = 3, asp = 1, main="" "", line = TRUE) 


#Predition for 3 components 
T&lt;-as.factor(sort(rep(c(""be"", ""nottobe""), 50))) 

y1 &lt;- c(rnorm(100,1,0.1),rnorm(100,1,0.1)) 
y2 &lt;- c(rnorm(100,10,0.3),rnorm(100,10,0.6)) 
y3 &lt;- c(rnorm(100,10,2.3),rnorm(100,11,2.6)) 
y4 &lt;- c(rnorm(100,5,0.5),rnorm(100,7,0.5)) 
y5 &lt;- c(rnorm(100,0,0.1),rnorm(100,0,0.1)) 

espectro2 &lt;- cbind(y1,y2,y3,y4,y5) 
new.dados &lt;- data.frame(bands = I(as.matrix(espectro2))) 
round(predict(taumato, ncomp = 3, newdata = new.dados))##
</code></pre>
"
"0.0323001055407673","0.0328975847479884","108035","<p>I am trying to use the R package ""urca"". It has functions <code>cajorls</code> and <code>cajools</code>, which do OLS regression of restricted and unrestricted VECM respectively. Both functions take in the VECM estimated using Johansen procedure and return the regression parameters and cointegrating vectors.</p>

<p>I do not understand, why is there a need to do OLS regression of VECM? Using Johansen test on the data (by calling <code>ca.jo</code>) will give me the cointegrating vectors. What is the difference between cointegrating vectors of VECM and OLS regression of VECM?</p>
"
"0.0484501583111509","0.0493463771219827","108069","<p>I have the same problem as the following post, but I have more samples and the index of the outlier is known. </p>

<p><a href=""http://stats.stackexchange.com/questions/73304/outlier-treatment-in-vector-autoregression-var-model?newreg=31eda9f1618047dcba346fdcb8014dcb"">Outlier treatment in Vector Autoregression (VAR) Model</a></p>

<p>I tried deleting the outliers; it works. I also want to try including dummy variables for the outliers (the dummy take the value of 1 on the outliers, and zero otherwise) and compare the two treatments. One reason I want to do this is that a special event at t1 may also impact the response variable y2 at t2. However, its impact on t2 is not significant enough, thus y2 is not an outlier.</p>

<p><code>Question</code>: The VAR in R package <code>vars</code> returns ""NAs in dummy variable"". I guess is because the dummy variable comprises a lot of zeros. I don't know how to fix it. Thanks in advance.</p>
"
"0.0927749898843639","0.0944911182523068","108088","<p>I have some elementary problems understanding the consequences of using/adding a lagged  dependent variable in my  predictive model.  Iâ€™m trying to predict values $Y_{i,t+\tau}$  for  $\tau=1-3$ with:</p>

<p>$Y_{i,t+1}=a+bY_{i,t}+cX_{i,t}+e_{i,t+1}$</p>

<p>$Y_{i,t+2}=a+bY_{i,t}+cX_{i,t}+e_{i,t+2}$</p>

<p>$Y_{i,t+3}=a+bY_{i,t}+cX_{i,t}+e_{i,t+3}$ </p>

<p>I already performed a pooled regression where you basically ignore individual firm effects and time-effects and treat every subject equally. As I am trying to forecast different levels (in USD) and my data appears to be extremely tailed as it covers a few extremely large subjects (with extremely high values) but also many small subjects the predictions of the model perform rather poor as the intercept $a$ that is equal for all subject seems largely responsible for this. A fixed model however with individual intercepts is not valid with Lagged dependent variables as the LDV is correlated with the within errors. To account for the heavy tailed errors I already estimated the pooled model with the rlm package (robust lm) that produced slightly better results but overall they appear still very unsatisfactory.</p>

<p>I further read that adding LDVâ€™s results in biased and inconsistent estimators as there is severe correlation between the predictor variables and the model errors and that regular procedures for autocorrelation are not valid anymore. One solution I came across is the use of Instrumental Variables with an Anderson-Hsiao Estimator (i.e using a lag -2 that is not correlated with the error term (with non-autocorrelation assumed but how can you assume no autocorrelation if you incorporate a lag?) Another one is the Arellano Bond GMM estimator, however applying GMM you have to set up moment conditions and I have no idea how to do that and I donâ€™t know exactly how this methods work. What I care about is to obtain an unbiased estimator with valid coefficients and not about standard errors as I donâ€™t do inference. Are there any other strategies to cope with LDVâ€™s I am currently unaware of and what is the best/ideal/easiest way to deal with such matter? Do you best take care of some issues while you ignore others (e.g. autocorrelation)? Iâ€™m a little bit lost here.</p>
"
"0.0559454238864459","0.056980288229819","108143","<p>I'm currently struggling with the latent change model that I try to realize in R with the lavaan package. R can't estimate all standard errors, which makes me believe that the model is not fully identified.
I did not have identification problems with the latent state model, so I guess the critical part may be due to the change parameter. </p>

<pre><code>      Latent_change_prepost &lt;- '
                 # first order latent variable definition
                      T1_43_1 =~ 1*A43_1_1 
                               + 1*A43_1_2 
                               + 1*A43_1_3 
                               + 1*A43_1_4 
                               + 1*A43_1_5 

                      T1_43_2 =~ 1*A43_2_1 
                               + 1*A43_2_2 
                               + 1*A43_2_3 
                               + 1*A43_2_4 
                               + 1*A43_2_5 

                      T1_43_7 =~ 1*A43_7_1 
                               + 1*A43_7_2 
                               + 1*A43_7_3 
                               + 1*A43_7_4 
                               + 1*A43_7_5 

                      T1_43_3 =~ 1*A43_3_1 
                               + 1*A43_3_2 
                               + 1*A43_3_3 
                               + 1*A43_3_4 

                      T1_43_4 =~ 1*A43_4_1 
                               + 1*A43_4_2 
                               + 1*A43_4_3 
                               + 1*A43_4_4 

                      T1_43_5 =~ 1*A43_5_1 
                               + 1*A43_5_2 
                               + 1*A43_5_3 


                  # second order latent variable definition
                      T1 =~ 1*T1_43_1
                          + 1*T1_43_2
                          + 1*T1_43_7

                      T2 =~ 1*T1_43_3
                          + 1*T1_43_4
                          + 1*T1_43_5

                  # regressions
                      change =~ 1*T2 #(no regression, need to introduce ""change"")
                      T2 ~ 1*T1

                  # (co)variances
                      change ~~ T1
                      T2 ~~ 0*T2

                  # intercepts
                      T1 ~ NA*1
                      T2 ~ NA*1
                      change ~ NA*1
                      T1_43_1 ~ NA*1
                      T1_43_2 ~ NA*1
                      T1_43_7 ~ NA*1
                      T1_43_3 ~ NA*1
                      T1_43_5 ~ NA*1
                    '

  fit2_latent_change &lt;- sem(
                            model = Latent_change_prepost
                          , data = diss_prepost_w_interv_kovar
                          , missing = ""fiml""
                          , meanstructure = TRUE
                          )
</code></pre>

<p>I get estimates of most parameters, but no std. errors. Also if I alternate the model slightly I might get std. errors but they are astronomically high.</p>

<p>Any ideas and help would be much appreciated!</p>
"
"0.0559454238864459","0.056980288229819","108256","<p>I wish to test my time series data for volatility clustering, i.e. conditional heteroskedasticity.</p>

<p>So far, I have used the ACF test on the squared and absolute returns of my data, as well as the Ljung-Box test on the squared data (i.e. McLeod.Li.test).</p>

<p>In a recent paper (<a href=""http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1771862"" rel=""nofollow"">http://papers.ssrn.com/sol3/papers.cfm?abstract_id=1771862</a>, the test is reported on page 8) co-authored by a well-known researcher, they have employed the White test (<a href=""http://ideas.repec.org/a/ecm/emetrp/v48y1980i4p817-38.html"" rel=""nofollow"">http://ideas.repec.org/a/ecm/emetrp/v48y1980i4p817-38.html</a>) to directly test for heteroskedasticity.</p>

<p>I have tried the same approach, however was unable to do so.
From my understanding, the White test needs residual variance (usually from a linear regression model) as an input.</p>

<p>Now my question is: How did the researchers perform the White test? I do not understand which inputs they used for their White test.</p>

<p>While searching for solutions, I have found the sandwich package which uses the vcovHC and vcovHAC functions to estimate a heteroskedasticity-consistent covariance matrix, however the input is also a fitted linear regression model..</p>
"
"NaN","NaN","108403","<p>Is there a package in R that can estimate panel data with cross-sectional <a href=""https://en.wikipedia.org/wiki/Seemingly_unrelated_regressions"" rel=""nofollow"">seemingly unrelated regression</a> generalized least squares weights (like in EViews)? </p>
"
"0.027972711943223","0.0284901441149095","108466","<p>Different implementation software are available for <strong>lasso</strong>. I know a lot discussed about bayesian approach vs frequentist approach in different forums. My question is very specific to lasso - <strong><em>What are differences or advantages of baysian lasso vs regular lasso</em></strong>? </p>

<p>Here are two example of implementation in the package: </p>

<pre><code># just example data
set.seed(1233)
X &lt;- scale(matrix(rnorm(30),ncol=3))[,]
set.seed(12333)
Y &lt;- matrix(rnorm(10, X%*%matrix(c(-0.2,0.5,1.5),ncol=1), sd=0.8),ncol=1)

require(monomvn) 
## Lasso regression
reg.las &lt;- regress(X, Y, method=""lasso"")

## Bayesian Lasso regression
reg.blas &lt;- blasso(X, Y)
</code></pre>

<p>So when should I go for one or other methods ? Or they are same ? </p>
"
"0.0484501583111509","0.0493463771219827","108529","<p>I am trying to compare OLR, ridge and lasso in my situation. I could calculate SE for OLR and lasso but not for ridge. The following is <code>Prostrate</code> data from <code>lasso2</code> package. </p>

<pre><code>require(lasso2)
require(MASS)
data(Prostate)

fit.lm = lm(lpsa~.,data=Prostate)
summary(fit.lm)$coefficients
               Estimate  Std. Error    t value     Pr(&gt;|t|)
(Intercept)  0.669399027 1.296381277  0.5163597 6.068984e-01
lcavol       0.587022881 0.087920374  6.6767560 2.110634e-09
lweight      0.454460641 0.170012071  2.6731081 8.956206e-03
age         -0.019637208 0.011172743 -1.7575995 8.229321e-02
lbph         0.107054351 0.058449332  1.8315753 7.039819e-02
svi          0.766155885 0.244309492  3.1360054 2.328823e-03
lcp         -0.105473570 0.091013484 -1.1588785 2.496408e-01
gleason      0.045135964 0.157464467  0.2866422 7.750601e-01
pgg45        0.004525324 0.004421185  1.0235545 3.088513e-01

fit.rd = lm.ridge(lpsa~.,data=Prostate, lamda = 6.0012)
#summary(fit.rd)$coefficients, doesnot provide SE 

lfit = l1ce(lpsa~.,data=Prostate,bound=(1:500)/500)
summary(lfit[[10]])$coefficients
                 Value  Std. Error   Z score  Pr(&gt;|Z|)
(Intercept) 2.43614448 2.130515543 1.1434530 0.2528505
lcavol      0.03129045 0.125288320 0.2497475 0.8027826
lweight     0.00000000 0.274549270 0.0000000 1.0000000
age         0.00000000 0.018287840 0.0000000 1.0000000
lbph        0.00000000 0.095587974 0.0000000 1.0000000
svi         0.00000000 0.390936045 0.0000000 1.0000000
lcp         0.00000000 0.149824868 0.0000000 1.0000000
gleason     0.00000000 0.260274039 0.0000000 1.0000000
pgg45       0.00000000 0.007285054 0.0000000 1.0000000
</code></pre>

<p>I have a couple of questions: </p>

<p>(1) How can we calculate Std. Error in case of ridge regression ?</p>

<p>(2) Is it valid to compare <strong>Std. Error</strong> for deciding which (<code>ridge</code>, <code>lasso</code> or <code>OLS</code>) method to use ? Or there are other methods ? If so how can I get them ? </p>
"
"0.0791187772129236","0.080582296402538","108745","<p>My situation:</p>

<ul>
<li>small sample size: 116</li>
<li>binary outcome variable</li>
<li>long list of explanatory variables: 44</li>
<li>explanatory variables did not come from the top of my head; their choice was based on the literature.</li>
<li>most cases in the sample and most variables have missing values.</li>
</ul>

<p>Approach to feature selection chosen: LASSO</p>

<p>R's glmnet package won't let me run the glmnet routine, apparently due to the existence of missing values in my data set. It became clear from some of the answers to my <a href=""http://stats.stackexchange.com/questions/104194/how-to-handle-with-missing-values-in-order-to-prepare-data-for-feature-selection"">previous question</a> that I have do deal with more basic issues before considering imputation methods. I would like to add here new questions regarding that. On the the answer suggesting the coding as constant value and the creation of a new variable in order to deal with 'not applicable' values in continuous variables and the usage of group lasso:</p>

<ul>
<li><p>Would you say that if I use group LASSO, I would be able to use the approach suggested to continuous predictors also to categorical predictors? If so, I assume it would be equivalent to creating a new category - I am wary that this may introduce bias. If that is not advisable, what would be in the case of categorical variables?</p></li>
<li><p>Does anyone know if R's glmnet package supports group LASSO? If not, would anyone suggest another one that does that in combination with logistic regression? Several options mentioning group LASSO can be found in CRAN repository, any suggestions of the most appropriate for my case? Maybe SGL?</p></li>
</ul>

<p>OBS: I am not a statistician.</p>
"
"0.0484501583111509","0.0493463771219827","109077","<p>I am trying to check the assumptions of a two-way ANCOVA. 
So in my model I have</p>

<ul>
<li>two factors (F1, F2)</li>
<li>one dummy coded two level covariate (C)</li>
<li>one dependent variable (D)</li>
</ul>

<p>In order to check the assumption of homogeneity of regression slopes I tried
to perform an ANOVA with type 3 sums for the model D ~ F1*F2*C to see whether any
interactions with the covariate might be significant.
Using the Anova function from the car package this corresponds to</p>

<pre><code>modd&lt;-aov(D~F1*F2*C)
Anova(modd,type=3)
</code></pre>

<p>However, I encounter the following Error message:</p>

<pre><code>Error in Anova.III.lm(mod, error, singular.ok = singular.ok, ...) : 
 there are aliased coefficients in the model
</code></pre>

<p>My question is, whether it makes sense for the homogeneity test to force R to compute the
ANOVA anyway by supplying the singular.ok=T option or what else I should do in order to 
check the assumption.</p>
"
"0.0559454238864459","0.056980288229819","109261","<p>Do you know of an approach/package that facilitates mixed model regression of ordinal dependent variables on multiply imputed datasets in R?</p>

<p>Ideally, the function takes:</p>

<p>a list of multiply imputed datasets</p>

<p>a list of target variables (dependent variables, one or more)</p>

<p>a list of factors (independent variables, one or more)</p>

<p>a list of dummy coded conditions (for analysis of multifactor IVs)</p>

<p>and returns a table similar to the result of other regressions in r</p>

<p>After extensive searching, I had to create such a function using CLMM in the ordinal package. If you can't answer the first question perhaps you can advise me wrt code adaptations, statistical appropriateness of my approach, efficiency (it takes a LONG time with many imputed datasets), etc</p>

<p>here's some data that mirrors mine</p>

<pre><code>numimpdatasets = 3; N = 170; datalist = list()
for (datasetnum in 1:numimpdatasets){
    dvone = sample(1:5, N, replace=T)   
    dvtwo = sample(1:5, N, replace=T)
    teacher = c('Tom','Dick','Harry')[sample(1:3, N, replace=T)]
    studentclass = c('Class1','Class2','Class3','Class4','Class5',
                     'Class6')[sample(1:6, N, replace=T)]
    aptitude = runif(N, -3.5, 3.5)
    randord = sample(1:3, N, replace=T)
    conddummycode1 = c('a_cond1', 'b_cond2', 'c_cond3')[randord]
    conddummycode2 = c('c_cond1', 'b_cond2', 'a_cond3')[randord]
    datalist[[datasetnum]] = data.frame(cbind(dvone, dvtwo,teacher,
            studentclass, aptitude, conddummycode1, conddummycode2))
    }
dvs = colnames(datalist[[1]])[1:2]
conditions = colnames(datalist[[1]])[6:7]
ivs = c(""+as.numeric(aptitude)"",""+(1|teacher/studentclass)"")
</code></pre>
"
"0.0484501583111509","0.0493463771219827","109673","<p>I have built a cox model in R using the coxph function in the survival package, and now I need to replicate the model in SQL for scoring.  From my understanding, the model has the form described on the bottom of page 2 of this document, <a href=""http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-cox-regression.pdf"" rel=""nofollow"">http://cran.r-project.org/doc/contrib/Fox-Companion/appendix-cox-regression.pdf</a>, which gives it semi-parametric flexibility. Since there is an unspecified alpha term, I cannot just take the coefficients and use the model like a typical linear model or generalized linear model (and exponentiate). There are ways to estimate this alpha term, and I believe this added term to the hazard is needed to specify the complete model.  If this is the case, how do I get my hands on this alpha term?</p>
"
"0.128187069873015","0.124341182825498","109835","<p>While working on a big data set made of 10-minutes-points of information - i.e. <code>144</code> points per day, <code>1008</code> per week and <code>52560</code> per year - I encountered a few problem in R. The information concerns electricity load on a source substation during the year.</p>

<h3>Multiple seasonality :</h3>

<p>The data set clearly shows multiple seasonalities, which are daily, weekly and yearly. From <a href=""http://stats.stackexchange.com/questions/47729/two-seasonal-periods-in-arima-using-r"">there</a> I understood that R doesn't handle multiple seasonality within the ARIMA modeling functions.  I would really like to work with ARIMA models though, because my previous work is based on ARIMA models and I know approximatively how to translate a model into an equation.  </p>

<h3>Long seasonality :</h3>

<p>Each of the seasonalities is of high value, with the shortest one being the daily seasonality at 144. Unfortunately from the SARIMA general equation which is<br>
$\phi(B)\Phi(B^s)W_t = \theta(B)\Theta(B^s)Z_t$<br>
I guessed that the maximum lag for a given model <code>SARIMA(p,d,q)(P,D,Q)144</code> is<br>
$max((p+P*144), (q+Q*144))$</p>

<p>I would really like to try and fit models with values of P and/or Q greater than 1, but R doesn't allow me since the <code>maximum supported lag = 350</code>. To do so I found <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">this link</a> which is really interesting and led to new functions in the forecast package by M. Hyndman, called <code>fourier</code> and <code>fourierf</code> which you can find <a href=""http://www.inside-r.org/packages/cran/forecast/docs/fourier"" rel=""nofollow"">here</a>. But since I am not a specialist in forecasting nor in statistics, I have some difficulties understanding how I can make this work.  </p>

<hr>

<p>The thing is I thing this whole fourier regressors package could help me a lot. From what I understood I could use it to simulate the long-seasonality of my data set, maybe use it to simulate multiple seasonality, and even more it could allow me to introduce exogenous variables - which are the <code>temperature</code> and (<code>public holiday + sundays</code>).<br>
I also tried doing some regression following <a href=""http://robjhyndman.com/hyndsight/forecasting-weekly-data/"" rel=""nofollow"">this example</a> but I couldn't make it work because :</p>

<pre><code>Error in forecast.Arima(bestfit, xreg = fourierf(gas, K = 12, h = 1008)) : 
Number of regressors does not match fitted model
</code></pre>

<p>I really hope somebody can help me get a better understanding of these functions. Thanks.</p>

<p><strong>Edit :</strong> So I tried my best with the fourier example given <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">here</a> but couldn't figure out how it handles the fitting. Here is the code (I copy-pasted M. Hyndman one and adapted to my data set - unsuccessfully) :</p>

<pre><code>n &lt;- 50000
m &lt;- 144
y &lt;- read.table(""auch.txt"", skip=1)
fourier &lt;- function(t,terms,period)
{
  n &lt;- length(t)
  X &lt;- matrix(,nrow=n,ncol=2*terms)
  for(i in 1:terms)
  {
    X[,2*i-1] &lt;- sin(2*pi*i*t/period)
    X[,2*i] &lt;- cos(2*pi*i*t/period)
  }
  colnames(X) &lt;- paste(c(""S"",""C""),rep(1:terms,rep(2,terms)),sep="""")
  return(X)
}

library(forecast)
fit &lt;- Arima(y[1:n,1], order=c(2,1,5), seasonal=c(1,2,8), xreg=cbind(fourier(1:n,4,m),fourier(1:n,4,1008)))
plot(forecast(fit, h=14*m, xreg=cbind(fourier(n+1:(14*m),4,m), fourier(n+1:(14*m),4,1008))))
</code></pre>

<p>So I wanted to ""force"" the model to be a <code>SARIMA(2,1,5)(1,2,8)[144]</code> but when I type <code>arimod</code>this is the result of the Arima fitting :</p>

<pre><code>&gt; fit  
Series: y[1:n, 1] , 
ARIMA(2,1,5)                  

sigma^2 estimated as 696895:  log likelihood=-407290.2  
AIC=814628.3   AICc=814628.3   BIC=814840
</code></pre>

<p>It doesn't even take into consideration the seasonal part of the model, and I don't know much about the range the AIC values can take, but it seems way too high to be a good fitting model right there. I think it all comes down to my misunderstanding of the use of Fourier terms as regressors, but I can't figure out why.</p>

<p><strong>Edit 2 :</strong> Also I can't seem to be able to add another exogenous variable to the Arima function. I need to use <code>temperature</code> - probably as a lead - to fit the <code>SARIMAX</code> model, but as soon as I write this :</p>

<pre><code>fit &lt;- Arima(y[1:n,1], order=c(2,1,5), seasonal=c(1,2,8), xreg=cbind(fourier(1:n,4,m),fourier(1:n,4,1008), tmp[1:n]))
plot(forecast(fit, h=14*m, xreg=cbind(fourier(n+1:(14*m),4,m),fourier(n+1:(14*m),4,1008), tmp[n+1:(14*m)])))
</code></pre>

<p>Nothing is plotted besides the initial data set. There is no forecast while without <code>tmp</code> as an <code>xreg</code> I still get some results.</p>
"
"0.0634361479695551","0.0753778361444409","109851","<p>I am using logistic regression to predict likelihood of an event occurring. Ultimately, these probabilities are put into a production environment, where we focus as much as possible on hitting our ""Yes"" predictions. It is therefore useful for us to have an idea of what definitive ""hits"" or ""non-hits"" might be <em>a priori</em> (before running in production), in addition to other measures we use for informing this determination.</p>

<p>My question is, what would be the proper way to predict a definitive class (1,0) based on  the predicted probability? Specifically, I use R's <code>glmnet</code> package for my modeling. This package arbitrarily picks .5 probability as threshold for a yes or no. I believe that I need to take the results of a proper scoring rule, based on predicted probabilities, to extrapolate  to a definitive class. An example of my modeling process is below:</p>

<pre><code>mods &lt;- c('glmnet', 'scoring')
lapply(mods, require, character.only = T)

# run cross-validated LASSO regression
fit &lt;- cv.glmnet(x = df1[, c(2:100)]), y = df1[, 1], family = 'binomial', 
type.measure = 'auc')

# generate predicted probabilities across new data
df2$prob &lt;- predict(fit, type=""response"", newx = df2[, c(2:100)], s = 'lambda.min')

# calculate Brier score for each record
df2$propscore &lt;- brierscore(df2[,1] ~ df2$prob, data = df2)
</code></pre>

<p>So I now have a series of Brier scores for each prediction, but then how do I use the Brier score to appropriately weight each likelihood being a yes or no?</p>

<p>I understand that there are other methods to make this determination as well, such as Random Forest.</p>
"
"0.121930224537282","0.1241856590838","110033","<p>I am running a post-hoc analysis on the data collected during an experiment in which 15 unique stimuli were presented to participants. Having run a least squares regression using the lm() function in R I have found significant results for a subset of the data including 90 observations from 6 participants with two continuous variables and their interaction.</p>

<p>Taking advice from an article by Judd, Westfall &amp; Kenny (2012) I attempted to use a combination of the lmer() function found in the lme4 package in combination with a Kenward-Roger approximation through the KRmodcomp() function in the pbkrtest package (see the appendix in the article) in order to control for random effects:</p>

<pre><code>lmer(Prediction_Difference_Scale~Diff_AWD_LRTI_End_Scale*Diff_AWD_BD_End_Scale + (1|Unique_ID) + (Diff_AWD_LRTI_End_Scale*Diff_AWD_BD_End_Scale|Block),data=Data)
</code></pre>

<p>The first variable after the DV is the fixed effect, the second variable in parentheses indicates that the intercept is random with respect the unique stimuli (Unique_ID) and the third variable in parentheses indicates that both the intercept and the Condition slopes are random with respect to participant (Block) and that a covariance between the effects should be estimated. </p>

<p>When running the lmer() function I get the following error message:</p>

<pre><code>Error in checkNlevels(reTrms$flist, n = n, control) : 
  number of levels of each grouping factor must be &lt; number of observations
</code></pre>

<p>This is obviously because the number of observations equal the number of unique stimuli.</p>

<p>The function works when excluding the (1|Unique_ID) random  effect, which if I understand correctly is the same as carrying out a 'by stimulus' analysis. However, the authors warn against this by stating: ""Conceptually, a significant by-participant result suggests that experimental results would be likely to replicate for a new set of participants, but only using the same sample of stimuli. A significant by-stimulus result, on the other hand, suggests that experimental results would be likely to replicate for a new set of stimuli, but only using the same sample of participants. However, it is a fallacy to assume that the conjunction of these two results implies that a result would be likely to replicate with simultaneously new samples of both participants and stimuli.""</p>

<p>I would like to control for the random effects of both stimuli and participants, but I am unsure how to proceed?</p>

<p>The article can be accessed here: <a href=""http://jakewestfall.org/publications/JWK.pdf"" rel=""nofollow"">http://jakewestfall.org/publications/JWK.pdf</a></p>

<hr>

<p>To clarify the question regarding the 15 unique stimuli, this is 15 unique stimuli per participant, meaning the sample of 90 observations consists of 6 participants. The stimuli for all of the 90 observations are unique however.</p>

<p>I suppose what my question boils down to is whether there is even a need to include the (1|Unique_ID) 'variable' in the function formula as there is no error dependence between any of the stimuli?</p>
"
"NaN","NaN","110773","<p>I have fitted a zero-inflated beta regression model to my data in R, using the gamlss package. </p>

<p>However, I am unsure of how to assess the fit of the model to my data, i.e. finding a coefficient of determination. </p>

<p>Does anyone know if this can be easily calculated from a zero-inflated beta regression model?</p>

<p>Thanks in advance</p>
"
"0.0484501583111509","0.0493463771219827","110932","<p>I am working on some <strong>non-parametric bayesian based predictive analysis</strong> using <strong>R</strong>. I have a set of data which denotes various parameters of an online transaction. Based on these parameters I want to develop a model which will provide predictions for future online transactions.</p>

<p>The training data consist of records in this format:</p>

<pre><code>transaction_id (numeric)| duration (integer)| amount | is_holiday (boolean) | status(1 or 0)
                        |                   |        |                      |
                        |                   |        |                      | 
</code></pre>

<p>The problem that I am facing is that I do not know how to proceed ahead. I am do know know what are the steps that I need to follow. I looked up and found that there are few packages in R like <code>DPpackage</code> which have some functions for non-parametric bayesian modeling but there is no concrete example available about how to use it in order to perform various steps of training and testing.</p>

<p>It would be helpful for me if someone could provide me some guidance as in which process will be better for such kind of predictive/regression analysis and how to proceed ahead, like what steps should I perform to get the training and testing done.</p>

<p>Thanks in advance!  </p>
"
"0.0796117338651413","0.0720749970156447","111287","<p>For a simulation study I have to generate data according to a trivariate mediation model (Baron &amp; Kennys causal step approach). Assuming $X$, $M$, and $Y$ are continuous variables of sample size $n$, the following regression equations constitute the data generating process: </p>

<p>$M = \alpha + aX + \epsilon$</p>

<p>$Y = \alpha + cX + bM + \epsilon$</p>

<p>Additionally, $X$ is not normally distributed, but is sampled from a distribution with skewness. I chose log-normal distribution in my current R implementation (see below). Sample size is also highly variable, starting at $n = 15$ and going up to $n = 15.000$.    </p>

<p>Here is my admittedly really primitive and inefficient code: </p>

<p>Sample from <code>rlnorm()</code> until a random sample with desired skewness is found (for clarification: <code>tol</code> refers to a tolerance parameter set to $.01$). </p>

<pre><code>library(moments)
s &lt;- 1 # desired skewness
tol &lt;- .01
n &lt;- 20
a &lt;- b &lt;- c &lt;- .59
skewness &lt;- sqrt(log(-1+2^(1/3)/(2+s^2+sqrt(4*s^2+s^4))^(1/3)+(2+s^2+sqrt(4*s^2+s^4))^(1/3)/2^(1/3)))

repeat{X &lt;- rlnorm(n=n, meanlog=0, sdlog=skewness)
    if (skewness(X) &gt; (skewness - tol) &amp;&amp; skewness(X) &lt; (skewness + tol)) {break}
} 
</code></pre>

<p>Then calculate $M$ and $Y$ from $X$ in a similar fashion and according to the regression equations above (where $a$, $b$ and $c$ are the correlations between the variables):</p>

<pre><code>repeat{
    M &lt;- a * X + rnorm(n)
    if (cor(X, M) &gt; (a - tol) &amp;&amp; cor(X, M) &lt; (a + tol)) {break}
}

repeat{
    Y &lt;- c * X + b * M + rnorm(n)
    if (cor(Y, X) &gt; (c - tol) &amp;&amp; cor(Y, X) &lt; (c + tol) &amp;&amp;
        cor(Y, M) &gt; (b - tol) &amp;&amp; cor(Y, M) &lt; (b + tol)) {break}
}
</code></pre>

<p>Again, this code is woefully ineffective and is only applicable for very low $n$, as the computation time is pretty low for small samples. Once higher sample sizes are reached (about $n=200$), it stops working altogether.  </p>

<p>So here are my actual questions:</p>

<ul>
<li>Is there a way to generate intercorrelated data ($X$, $M$ and $Y$) when $X$ isn't normally distributed and therefore <code>mvrnorm</code> from the <code>MASS</code> package can't be used? Also the correlation between the variables has to be either an exact value, or within a certain range ($\pm$ tolerance). </li>
<li>Assuming it can be done theoretically (which I am quite sure of), how can it be implemented in <code>R</code>?           </li>
</ul>
"
"NaN","NaN","111324","<p>How do I find the adjusted $R^2$ (or $r^2$) from Lasso and Ridge regression?</p>

<p>I used the glmnet package.  For instance if I have this code so far....</p>

<pre><code>###LASSO 
library(glmnet)
attach(mtcars)
y=mtcars$mpg
    model=model.matrix(mpg~ . data=mtcars)
    lasso.reg=cv.glmnet(model, y, type.measure='mse', alpha=0)
    names(lasso.reg)
    mse=lasso.reg$cvm[lasso.reg$lambda == lasso.reg$lambda.min]
rmse = sqrt(mse)
</code></pre>

<p>Can someone show me the code that will give me the $R^2$ and the Adjusted $R^2$.  Sorry I'm missing something obvious.</p>
"
"0.108550066801774","0.117467873474841","111383","<p>I have been reading several CV posts on binary logistic regression but I am still confused for my current situation.</p>

<p>I am attempting to fit a binary logistic regression to a series of continuous and categorical variables in order to predict the mortality or the survival of animals (<code>qual_status</code>). Please see the <code>str</code> below:</p>

<pre><code>&gt; str(logit)
'data.frame':   136 obs. of  9 variables:
 $ id         : Factor w/ 135 levels ""01001"",""01002"",..: 26 27 28 29 30 31 32 33 34 35 ...
 $ gear       : Factor w/ 2 levels ""j"",""sc"": 2 1 1 2 1 2 1 2 2 1 ...
 $ depth      : num  146 163 179 190 194 172 172 175 240 214 ...
 $ length     : num  37 35 42 38 37 41 37 52 38 37 ...
 $ condition  : Factor w/ 4 levels ""1"",""2"",""3"",""4"": 1 1 4 1 4 2 2 1 2 1 ...
 $ in_water   : num  80 45 114 110 60 121 56 140 93 68 ...
 $ in_air     : num  60 136 128 136 165 118 220 90 177 240 ...
 $ delta_temp : num  8.5 8.4 8.3 8.5 8.5 8.6 8.6 8.7 8.7 8.7 ...
 $ qual_status: Factor w/ 2 levels ""0"",""1"": 1 1 2 1 2 1 2 1 1 1 ...
</code></pre>

<p>I have no issues fitting an the following additive binary logistic regression with the <code>glm</code> function:</p>

<p><code>glm(qual_status ~ gear + depth + length + condition + in_water + in_air + delta_temp, data = logit, family = binomial)</code></p>

<p>...but I am also interested at how these predictor variables interact with one another and possibly influence survival. However, when I attempt the following interactive binary logistic regression:</p>

<p><code>glm(qual_status ~ gear * depth * length * condition * in_water * in_air * delta_temp, data = logit, family = binomial)</code></p>

<p>I receive a warning message <code>""glm.fit: fitted probabilities numerically 0 or 1 occurred""</code>, along with missing coefficients due to singularities (NA or &lt;2e-16 <em>*</em>) when I use <code>summary</code>:</p>

<pre><code>Call:
glm(formula = qual_status ~ gear * depth * length * condition * 
    in_water * in_air * delta_temp, family = binomial, data = logit)

Deviance Residuals: 
  [1]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
 [36]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
 [71]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
[106]  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0

Coefficients: (122 not defined because of singularities)
                                                            Estimate Std. Error    z value Pr(&gt;|z|)    
(Intercept)                                                1.419e+30  5.400e+22   26274077   &lt;2e-16 ***
gearsc                                                    -1.419e+30  5.400e+22  -26274077   &lt;2e-16 ***
depth                                                      1.396e+28  4.040e+20   34539471   &lt;2e-16 ***
length                                                     6.807e+28  1.836e+21   37079584   &lt;2e-16 ***
condition2                                                -3.229e+30  8.559e+22  -37727993   &lt;2e-16 ***
condition3                                                 1.747e+31  4.636e+23   37671986   &lt;2e-16 ***
condition4                                                 9.007e+31  2.388e+24   37724167   &lt;2e-16 ***
in_water                                                  -4.540e+28  1.263e+21  -35935748   &lt;2e-16 ***
in_air                                                    -4.429e+28  1.182e+21  -37470809   &lt;2e-16 ***
delta_temp                                                -1.778e+28  3.237e+21   -5492850   &lt;2e-16 ***
gearsc:depth                                              -1.396e+28  4.040e+20  -34539471   &lt;2e-16 ***
gearsc:length                                             -6.807e+28  1.836e+21  -37079584   &lt;2e-16 ***
depth:length                                              -9.293e+26  2.450e+19  -37930778   &lt;2e-16 ***
gearsc:condition2                                          1.348e+30  3.567e+22   37809001   &lt;2e-16 ***
gearsc:condition3                                          2.816e+30  7.495e+22   37575317   &lt;2e-16 ***
gearsc:condition4                                                 NA         NA         NA       NA    
</code></pre>

<p>Fitting only the continuous variables to a binary logistic regression doesn't yield any warnings or singularities but the addition of the ordinal predictor variables causes issues. Along with avoiding these warnings, is there a function/package that can handle dummy variables (I believe that is what I am looking for) in logistic regressions in <code>R</code>?</p>
"
"0.0685188709827532","0.0697863157798853","111457","<p>I ran two logistic regression models, one with a dataset including outliers and one without outliers, with multiple predictors.</p>

<p>I checked each model's fit with the le Cessie â€“ van Houwelingen â€“ Copas â€“ Hosmer unweighted sum of squares test for global goodness of fit from the rms package in R (following advice <a href=""http://www.r-bloggers.com/veterinary-epidemiologic-research-glm-evaluating-logistic-regression-models-part-3/"" rel=""nofollow"">here</a>).</p>

<pre><code>model1 &lt;- lrm(y ~ a + b + c + d, data1, method = ""lrm.fit"", model = TRUE, x = TRUE, y = TRUE, linear.predictors = TRUE, se.fit = FALSE)
residuals(model1, type = ""gof"")
</code></pre>

<p>For the model with outliers the p value was close to 0, indicating a lack of fit. For the model without outliers p value was 0.52, indicating that my model was not incorrect.</p>

<p>I then ran 10-fold cross validation for both models with DAAG package and was surprised to get identical (poor) accuracy results for both = 0.56</p>

<pre><code>cv10&lt;-CVbinary(model1,nfolds=10)
</code></pre>

<p>I thought that the model created using the dataset without outliers, having a much better fit, will give me higher accuracy. Am I missing something here? I will be grateful for your help.</p>
"
"0.0395593886064618","0.040291148201269","111541","<p>Could anyone provide me the details of how to determine the lag order of the  distributed lags for an $\text{ADL}(p,q)$ model in Matlab or another statistical package (and very much preferably in combination with the autoregression lags)? </p>

<p>Full working examples with model selection criteria ($\text{AIC}$ and $\text{BIC}$) seem to be available on the Matlab website for $\text{VAR}$ models, $\text{ARMA}$ models etc. but I can't find one for the $\text{ADL}(p,q)$ model. I would not have a clue how to rewrite those models to $\text{ADL}(p,q)$ myself but I have a vague feeling that such a thing would be possible.</p>

<p>In the end I want to automate this analysis by first checking the lag orders $p$, and $q$ and then using these numbers automatically to create the regressions out of this. So basically I'm looking for a fully working example. (I want to skip part of adding and deleting regressors by hand as much as possible to get a quick idea of the distributed lags of several assets).</p>
"
"0.0839181358296689","0.0854704323447285","111549","<p>I am trying to fit a NB GLMM with a gemoetric distribution. I have come across very little information on this form of regression. And would like some pointers/reasurance.</p>

<p>some literature is available for glm method here: <a href=""http://cran.r-project.org/web/packages/pscl/vignettes/countreg.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/pscl/vignettes/countreg.pdf</a></p>

<p>but I cannot find anything on a mixed model to use with this.</p>

<p>main points:</p>

<p>Is this code correct to run such a model?
Is this the best package for this analysis?
Does model selection and validation follow that for regular Negative binomial models?</p>

<p>sample data</p>

<pre><code>DF&lt;- structure(list(code = structure(c(1L, 1L, 6L, 6L, 7L, 9L, 10L, 
10L, 10L, 10L, 10L, 11L, 11L, 12L, 13L, 14L, 14L, 16L, 16L, 17L, 
17L, 23L, 24L, 26L, 27L, 27L, 27L, 28L, 28L, 29L, 30L, 30L, 31L, 
32L, 34L, 35L, 8L, 8L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 25L, 
25L, 25L, 25L, 15L, 33L, 33L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 3L, 
19L, 19L, 18L, 5L, 5L, 4L, 4L, 22L, 21L, 21L, 20L, 20L), .Label = c(""15010212"", 
""15010213"", ""15010215"", ""15010216"", ""15010220"", ""15010222"", ""15010245"", 
""15010269"", ""15010274"", ""15010284"", ""15010285"", ""15010287"", ""15010290"", 
""15010291"", ""15010292"", ""15010294"", ""15010299"", ""15020313"", ""15020314"", 
""15020315"", ""15020316"", ""15020317"", ""15020326"", ""15020345"", ""15020348"", 
""15020384"", ""15020395"", ""15020396"", ""15020397"", ""15030312"", ""15030317"", 
""15030349"", ""15030392"", ""15030394"", ""15030395""), class = ""factor""), 
flow = c(15.97766667, 14.226, 17.15724762, 14.7465, 39.579, 
23.355, 110.2926923, 71.95709524, 50.283, 66.66754955, 38.9218, 
72.73666667, 32.37466667, 50.34905172, 27.98471429, 49.244, 
109.1759778, 77.71733333, 37.446875, 101.23875, 67.78534615, 
21.359, 36.54257143, 34.13961111, 64.35253333, 80.98554545, 
68.0554, 61.50857143, 48.983, 63.81072727, 26.105, 46.783, 
23.0605, 33.61557143, 46.31042857, 62.37061905, 12.565, 42.31983721, 
15.3982, 14.49625, 16.40853846, 17.84350847, 14.625375, 13.10714286, 
13.35466667, 12.94033333, 13.54236364, 14.10023711, 12.5747807, 
23.77425, 25.626, 15.23888523, 74.62485714, 170.1547778, 
91.292, 71.422, 42.50887568, 53.89983761, 141.7211667, 50.67125, 
48.098, 66.83644444, 76.564875, 80.63189189, 136.0573243, 
136.3484, 86.68688889, 34.82169565, 70.00415385, 64.67233333, 
81.72766667, 57.74522034), success = c(0L, 1L, 0L, 1L, 1L, 
1L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 
1L, 1L, 1L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 
1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 
0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 
0L, 1L, 1L, 0L, 1L, 0L, 1L), length = c(595, 595, 582, 582, 
565, 537, 585, 585, 585, 585, 585, 595, 595, 607, 625, 627, 
627, 607, 607, 644, 644, 620, 560, 567, 615, 615, 615, 595, 
595, 546, 580, 580, 594, 605, 610, 640, 575, 575, 632, 632, 
632, 632, 632, 632, 632, 632, 632, 632, 632, 525, 585, 585, 
624, 624, 624, 624, 624, 624, 624, 608, 635, 635, 655, 670, 
670, 584, 584, 707, 680, 680, 740, 740), attempt = structure(c(1L, 
2L, 1L, 2L, 1L, 1L, 1L, 2L, 3L, 4L, 5L, 1L, 2L, 1L, 1L, 1L, 
2L, 1L, 2L, 1L, 2L, 1L, 1L, 1L, 1L, 2L, 3L, 1L, 2L, 1L, 1L, 
2L, 1L, 1L, 1L, 1L, 1L, 2L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 
9L, 10L, 11L, 1L, 1L, 2L, 1L, 2L, 3L, 4L, 5L, 6L, 7L, 1L, 
1L, 2L, 1L, 1L, 2L, 1L, 2L, 1L, 1L, 2L, 1L, 2L), .Label = c(""1"", 
""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9"", ""10"", ""11""), class = ""factor"")), .Names =         c(""code"", 
""flow"", ""success"", ""length"", ""attempt""), row.names = c(NA, -72L
), class = ""data.frame"")
</code></pre>

<p>model is as follows. setting theta = 1 should determine a geometric distribution.</p>

<pre><code>library(MASS)
M1&lt;-glmmPQL(success ~ length + flow + attempt,
          random = ~ 1|code,
          family = negative.binomial(theta = 1),
          data = DF)

summary(M1)
</code></pre>

<p>Ultimately I am trying predict success (0 = fail, 1 = success). However this is measured for many different individuals (code), essentially a repeated measure and hence should be included as a random effect. each individual may only have one success but can have multiple attempts. Predictors of success come in the form of ""length"" of the individual, ""attempt"" number... a factor of the number of the attempt, and ""flow"" which is river flow at time of the attempt and so a continuous variable.</p>

<p>Thanks in advance</p>
"
"0.0625488854200668","0.0637058989297032","111902","<p>I am conducting a two-sample test (1-way ANOVA with 2 treatments), and the goal is to estimate the ratio of cell means assuming that the data are lognormal. A simple approach is to log the response and fit a model </p>

<p>$\log Y = b_0 + b_1 * X$</p>

<p>and then estimate the ratio as</p>

<p>$R = e^{b_1}$</p>

<p>However, that gives the ratio of geometric cell means rather than arithmetic cell means. </p>

<p>I assumed that if I fit a ""proper"" lognormal model using either <code>gamlss</code> in R or <code>PROC GLIMMIX</code> in SAS, I will get the ratio of arithmetic means, but for some reason both procedures generate the same slope as the $\log Y$ regression.</p>

<p>This is odd because when I use this approach with Poisson or Negative Binomial regression, I do get the ratio of arithmetic means. What am I missing?</p>

<hr>

<p>P.S.</p>

<p>I think I identified the source of confusion, but I don't have an explanation for it. A lognormal setup with the identity link function is:</p>

<p>$\log Y_1 \sim N(b_0, \sigma^2)$</p>

<p>$\log Y_2 \sim N(b_0 + b_1, \sigma^2)$</p>

<p>which implies </p>

<p>$\frac{E[Y_2]}{E[Y_1]} = \frac{e^{b_0 + b_1 +\sigma^2/2}}{e^{b_0 + \sigma^2/2}} = e^{b_1}$</p>

<p>To me, it means that $e^{b_1}$ should have a point estimate equal to the ratio of arithmetic means for the original response.</p>

<p>On the other hand,</p>

<p>$E[\log Y_1] = b_0$</p>

<p>$E[\log Y_2] =  b_0 + b_1$</p>

<p>$b_0$ is estimated as arithmetic mean of $\log Y_1$, $b_0 + b_1$ is estimated as arithmetic mean of $\log Y_2$. Hence, $e^{b_1}$ should have
a point estimate equal to the ratio of geometric means for the original response, and it does, given the output from those two packages. Where did I make a mistake?</p>
"
"0.0740088392978143","0.0753778361444409","112247","<p>I'm trying to use the Match() function from the Matching package in R to do a propensity score analysis.</p>

<p>My outcome of interest is a binary variable (0/1).  My treatment is also a binary variable (0/1).  In addition, I have a number of other variables that I want to control for in this analysis.</p>

<p>First, I fit a logistic regression to define a propensity score for the treatment:</p>

<pre><code>glm1 = glm(Treatment ~ variable1 + variable2 + variable3 + ..., 
           data=dataset, family=""binomial"")
</code></pre>

<p>Then, I used the Match function to estimate the average treatment effect on the treated:</p>

<pre><code>rr1 = Match(Y = Outcome, Tr = Treatment, X = glm1$fitted)
</code></pre>

<p>Finally, I called for a summary:</p>

<pre><code>summary(rr1)
</code></pre>

<p>My question is how to interpret the output.  I get:</p>

<pre><code>Estimate... -0.349,
AI SE... 0.124,
T-stat... -2.827,
p.val... 0.005
</code></pre>

<p>What does this mean?  In particular, what is Estimate?  The documentation says it's ""The estimated average causal effect.""  But what are the units?  Can I interpret this to mean that the treatment reduced the outcome by a relative 35%?  Or by an absolute 0.35?  Or do I need to exponentiate?</p>

<p>Any help on the interpretation would be much appreciated!</p>
"
"0.0625488854200668","0.0509647191437626","112348","<p>I am running a logistic regression model in R using multiply imputed data created using Amelia II, which I am then analyzing using Zelig. I would like to be able to report some measures of goodness-of-fit (e.g. likelihood ratio, pseudo R-squared, Hosmer-Lemeshow), however none are provided in the default Zelig output and I haven't been able to figure out a way to extract any from the <code>zelig()</code> object. </p>

<p>Do measures of goodness-of-fit need to be calculated differently when using multiply imputed datasets? Are there any R packages that are able to do this? I have looked into several packages that provide measures of goodness-of-fit, such as pscl, however they only work on glm objects, not MI objects created when using Amelia and Zelig.</p>

<p>Thanks in advance for your help!</p>
"
"0.111890847772892","0.113960576459638","112380","<p>I have a problem to build and to explain the linear multiple regression.</p>

<p>I have a data set called <code>Cars93</code> with 26 variables (numeric and not numeric) and 93 observations. This data set you can find in the <code>MASS</code> R package. I want to build a linear regression model for predicting the price of a car. Then I have to do a variable selection (forward and backward stepwise) using AIC and BIC in R.  My knowledge in R is too little thatÂ´s why I have some problems. I really hope you can help me! </p>

<p>1) The data set has some missing values </p>

<p>I just solved this problem like this:</p>

<pre><code> Cars93 [! Complete.cases (Cars93)] 
 Cars93new &lt;- na.omit (Cars93) 
 Cars93 = Cars93new 
</code></pre>

<p>I think some informations are going lost. Is there another solution to eliminate the missing values? </p>

<p>2) Some variables from the dataset are not numeric
I tried to convert these values into numerical values like this:</p>

<pre><code>Cars93 $ airbags = factor (Cars93 $ airbags, labels = c (2,1,0)) 
Cars93 $ airbags 
  [1] 0 2 1 2 1 1 1 1 1 1 2 0 1 2 0 1 2 2 1 0 1 1 1 1 0 2 0 0 0 1 1 1 1 0 1 1 2 2 
[39] 0 0 0 0 1 1 2 2 2 0 0 1 1 2 1 0 0 1 1 1 1 0 1 1 0 0 0 2 0 2 1 1 0 0 1 0 1 1 
[77] 1 0 0 0 1 2 
Levels: 2 1 0 
</code></pre>

<p>I did the same with other not numeric variables.</p>

<p>Afterwards I tried to build a linear model regression with all variables:</p>

<pre><code>Modell=lm(Price~Horsepower+EngineSize+MPG.city+MPG.highway+Rev.per.mile+Man.trans.avail+Fuel.tank.capacity+Passengers+Length+Wheelbase+Width+Turn.circle+Weight+Rear.seat.room+Luggage.room+Origin+AirBags+Type+Cylinders+Weight+PRM)
summary(Modell)
</code></pre>

<p>But the output does make any sense:</p>

<pre><code>Call:
lm(formula = Price ~ Horsepower + EngineSize + MPG.city + MPG.highway + 
    Rev.per.mile + Man.trans.avail + Fuel.tank.capacity + Passengers + 
    Length + Wheelbase + Width + Turn.circle + Weight + Rear.seat.room + 
    Luggage.room + Origin + AirBags + Type + Cylinders + Weight + 
    RPM)

Residuals:
    Min      1Q  Median      3Q     Max 
-9.4893 -2.3664 -0.0062  2.1180 18.1112 

Coefficients:
                    Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)        81.335018  37.993697   2.141 0.036826 *  
Horsepower          0.123535   0.049355   2.503 0.015372 *  
EngineSize         -0.615828   3.047223  -0.202 0.840602    
MPG.city           -0.392888   0.470385  -0.835 0.407259    
MPG.highway         0.013646   0.428978   0.032 0.974740    
Rev.per.mile        0.001498   0.002511   0.597 0.553206    
Man.trans.availYes -1.600967   2.480497  -0.645 0.521387    
Fuel.tank.capacity  0.462731   0.572169   0.809 0.422219    
Passengers          0.615593   1.823089   0.338 0.736925    
Length              0.074875   0.130511   0.574 0.568547    
Wheelbase           0.740146   0.343760   2.153 0.035796 *  
Width              -1.745792   0.571082  -3.057 0.003473 ** 
Turn.circle        -0.695287   0.415708  -1.673 0.100203    
Weight             -0.004068   0.006255  -0.650 0.518171    
Rear.seat.room      0.101150   0.420050   0.241 0.810619    
Luggage.room        0.176183   0.367199   0.480 0.633306    
Originnon-USA       1.881047   1.762845   1.067 0.290696    
AirBagsDriver only -3.294049   1.888346  -1.744 0.086777 .  
AirBagsNone        -8.535307   2.289737  -3.728 0.000464 ***
TypeLarge          -1.692122   3.999146  -0.423 0.673887    
TypeMidsize         2.684947   2.639047   1.017 0.313504    
TypeSmall           1.913341   2.896592   0.661 0.511710    
TypeSporty          4.686129   3.268426   1.434 0.157407    
Cylinders4         -3.126727   4.554852  -0.686 0.495360    
Cylinders5         -4.732933   7.498898  -0.631 0.530605    
Cylinders6          0.224795   5.695793   0.039 0.968664    
Cylinders8          4.020677   7.255406   0.554 0.581755    
RPM                -0.002778   0.002450  -1.134 0.261805    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 5.009 on 54 degrees of freedom
  (11 observations deleted due to missingness)
Multiple R-squared:  0.8313,    Adjusted R-squared:  0.747 
F-statistic: 9.859 on 27 and 54 DF,  p-value: 1.014e-12
</code></pre>

<p>I have 4 times ""Cylinders"" and ""Type"" and twice ""AirBag"" in the summary. I dont know why... And only 4 variables are significant in the model. Can somebody tell me, where is the mistake in my model?</p>

<p>I also would like to know how to test other assumptions in R for multiple linear model.</p>
"
"0.108550066801774","0.0898283738337021","112481","<p>I am attempting to plot hurdle regression output with an interaction term in R, but I have some trouble doing so with the count portion of the model. </p>

<pre><code>Model &lt;- hurdle(Suicide. ~ Age + gender + bullying*support, dist = ""negbin"", link = ""logit"")
</code></pre>

<p>Since I am unaware of any packages that would allow me to plot the hurdle model in its entirety, I estimated each component separately (binomial logit and negative binomial count), using the <code>MASS</code> package, and I am attempting to plot the results. </p>

<p>So far, I have had the best luck using the <code>visreg</code> package for plotting, but I would like to hear other suggestions. I have been able to reproduce and successfully plot the original logistic output from the hurdle model in <code>MASS</code>, but not the negative binomial count data (i.e., the parameter estimates from <code>MASS</code> are not the same as they are in the hurdle regression output).</p>

<p>I would greatly appreciate any insight regarding how others have plotted hurdle regression results in the past, or how I might be able to reproduce negative binomial coefficients originally obtained from the hurdle model using <code>glm.nb()</code> in <code>MASS</code>. </p>

<p>Here is what I am using to plot my data:</p>

<pre><code>##Logistic 

logistic&lt;-glm(SuicideBinary ~ Age + gender + bullying*support, data = sx, family=""binomial"")

data(""sx"", package = ""MASS"")
##Linear scale
visreg(logistic, ""bullying"", by=""support"", xlab = ""bullying"", ylab = ""Log odds (Suicide yes/no)"")

##Logistic/probability scale
visreg(logistic, ""bullying"", by=""support"", scale = ""response"", xlab = ""bullying"", ylab = ""P(Initial Attempt)"")

##Count model

NegBin&lt;-glm.nb(Suicide. ~ Age + gender + bullying*support, data = sx)

data(""sx"", package = ""MASS"")
##Linear scale
visreg(NegBin, ""bullying"", by=""support"", xlab = ""bullying"", ylab = ""Count model (number of suicide attempts)"")

##Logistic/probability scale
visreg(NegBin, ""bullying"", by=""support"", scale = ""response"", xlab = ""bullying"", ylab = ""P(Subsequent Attempts)"")
</code></pre>
"
"0.0559454238864459","0.056980288229819","112541","<p>I'm having trouble interpreting the results from the Spread-Level Plot function in R (car package). The documentation says:</p>

<blockquote>
  <p>PowerTransformation<br>
  spread-stabilizing power transformation, calculated as 1 - slope of the line fit to the plot.</p>
</blockquote>

<p>This is not explicit enough for me. Should this transformation be applied to every variable in the regression?</p>

<p>For example, assume I have an lm object given by:</p>

<pre><code>myFit &lt;- lm(y ~ x1 + x2)
</code></pre>

<p>Then I use Spread-Level Plot:</p>

<pre><code>slp(myFit)
</code></pre>

<p>If the 'suggested power transformation' is 0.5, then does that imply a homoscedastic model could be fit using one of the following?</p>

<pre><code>refitA &lt;- lm(sqrt(y) ~ sqrt(x1) + sqrt(x2))
refitB &lt;- lm(sqrt(y) ~ x1 + x2)
refitC &lt;- lm(sqrt(y) ~ sqrt(x1 + x2))
</code></pre>

<p>If I understand correct, refitA would be the suggested model to approximate homoscedasticity. On the other hand, if I <em>only</em> want to transform the LHS, I would use the <code>powerTransform</code> function (also from car package). i.e., an ""estimated transform parameter"" of 0.5 from the powerTransform function would imply that refitB is homoscedastic.</p>

<p>Is this correct?</p>

<p>Thanks!</p>
"
"0.0197796943032309","0.040291148201269","112660","<p>Say I have search data like this</p>

<p><code>AvgCost   QualityScore   SearchShare</code></p>

<p><code>3.12      6               0.6364</code></p>

<p>Where AvgCost is a continuous numerical variable, qualityScore is a categorical variable with values 1-10, and SearchShare is a percentage...I am wondering how to tease out the effect Avgcost and QualityScore have as well as whether they are too related to be both included. I'm also wondering what kinds of regressions besides simple linear regressions can be run on data sets where I have these sorts of variables i.e. categorical and numerical explanatory variables and a percentage response variable. </p>

<p>I have done some obvious things, like check <code>cor()</code> between qualityScore and AvgCost as well as do a simple linear regression. I looked at the <code>relaimpo</code> package, not sure if that's what I need. </p>

<p>Anyway, thanks very much for any help!</p>
"
"0.101115324337403","0.102985730108887","112760","<p>I have used <code>mlogit</code> package and I am trying to summarize the results I have from my model.  I have a question regarding the reference value and will get to that in a moment.</p>

<pre><code>redata.full &lt;- mlogit(no.C~ 1| WR+age+age2+BP+noC.1yr, data=redata, reflevel=""0"", na.action=na.fail)

no.C = number of offspring    
WR = risk
age+age2 = the non-linear relationship that as an individual ages their production decreases
BP = browsing pressure
noC.1yr = number of offspring produced the year before
</code></pre>

<p>I recognize that my data is ordinal in nature, but Im following other people's methods who have done this and used the reference based approach rather than ordinal logistic regression.  However, I am still shakey on justification other than citing the other person and saying ""he did it too!""  If anyone has a suggestion I would appreciate it.</p>

<p>My results for this model are: </p>

<pre><code>Call:
mlogit(formula = no.C ~ 1 | WR + age + age2 + BP + noC.1yr, data = redata, 
    na.action = na.fail, reflevel = ""0"", method = ""nr"", print.level = 0)

Frequencies of alternatives:
       0        1        2 
0.233766 0.675325 0.090909 

nr method
5 iterations, 0h:0m:0s 
g'(-H)^-1g = 2.16E-07 
gradient close to zero 

Coefficients :
               Estimate Std. Error t-value Pr(&gt;|t|)  
1:(intercept) -0.281226   1.225763 -0.2294  0.81854  
2:(intercept) -0.605312   1.997179 -0.3031  0.76183  
1:WR           0.847273   0.518854  1.6330  0.10248  
2:WR           1.347976   0.689916  1.9538  0.05072 .
1:age          0.314075   0.275486  1.1401  0.25425  
2:age         -0.422368   0.395240 -1.0686  0.28523  
1:age2        -0.018998   0.014446 -1.3151  0.18847  
2:age2         0.022572   0.018949  1.1912  0.23359  
1:BP          -0.143720   0.173585 -0.8280  0.40770  
2:BP          -0.074553   0.331108 -0.2252  0.82185  
1:noC.1yr      0.574304   0.377821  1.5200  0.12850  
2:noC.1yr      1.251673   0.626033  1.9994  0.04557 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Log-Likelihood: -116.6
McFadden R^2:  0.079844 
Likelihood ratio test : chisq = 20.236 (p.value = 0.0271)

exp(cbind(OddsRatio = coef(redata.full), ci))
              OddsRatio      2.5 %    97.5 %
1:(intercept) 0.7548580 0.06831155  8.341351
2:(intercept) 0.5459038 0.01089217 27.360107
1:WR          2.3332750 0.84394900  6.450831
2:WR          3.8496270 0.99577472 14.882511
1:age         1.3689929 0.79782462  2.349065
2:age         0.6554925 0.30209181  1.422317
1:age2        0.9811815 0.95379086  1.009359
2:age2        1.0228284 0.98553735  1.061530
1:BP          0.8661299 0.61634947  1.217136
2:BP          0.9281585 0.48504538  1.776078
1:noC.1yr     1.7758933 0.84686698  3.724076
2:noC.1yr     3.4961862 1.02497823 11.925441
</code></pre>

<p>I would like confirmation of my interpretations:
The model is better than a null - obtained from the likelihood ratio test.</p>

<p>Question: How do I test how well the model is actually working (i.e., goodness of fit)?  Hosmer-Lemshow test? Ive read warnings about using the McFaddin's Pseudo R where they really aren't applicable to multinomial regressions.  Ive found a HL test with <code>ResourceSelection</code> library and it says my model is NOT doing well at all.  Now what?</p>

<p>Interpretation:
WR and noC.1yr are the only variables that are coming out as slightly significant.  But this is only between the reference value of 0 and production of 2 calves.  It is not significantly different between 0 or 1 for these variables.  </p>

<p>Question: Ive been trying to find somewhere in the vignette what the t-value is - it is just a t-test?  How would I refer to the estimate as being significant?  ""The estimated odds for 2-offspring being produced versus 0 were 3.85 (95% CI = 1.0-14.88) which was significant (t= 1.99, P=0.05)""</p>

<p>Referring to my statement regarding setting the reference value.  When I run this exact same model using my other options of 0 or 1 offspring - I get completely different results of which variables are significant.  If I use 2 as the reference value then Age+WR+noC.yr are significant.  If I use 1, then Age only is sig.  So, which one to use?  I have read you want to pick one that is most relevant to your hypothesis, but in this case I could motivate any of the 3 levels.  </p>
"
"0.0745938985152613","0.0759737176397586","112801","<p>I am seemingly blindly following this <a href=""http://www.cfc.umt.edu/grizzlybearrecovery/pdfs/Schwartz%20et%20al.%202006e.pdf"" rel=""nofollow"">publication</a> that has done work very similar to what I need to accomplish (page 18-21).  My analysis is a multinomial logistic regression where I have 3 possible outcomes 0, 1, or 2 offspring produced.  In the publication, they have recommended a Hosmer-Lemshow and a Persons test for goodness-of-fit.  I have only figured out how to do the Hosmer-Lemshow test and my results are not so good (i.e. P-val is 0.00002).  I have no idea how to do the Pearsons test (suggestions are appreciated).</p>

<p>The paper I am following, of course their tests are ""good"" for model fit (page 21).  But they then go onto suggest that Somers D, the Goodman-Kruskala gamma and the Kendall's tau-a all indicate that their models are a good fit.  But the paper does not report any of the values for these indices or how they calculated them. </p>

<p>I have just found a package <code>ryouready</code> that runs all of these tests.  However, I have been having difficulties finding any help explaining what the values mean, let alone knowing if I have input my variables correctly.</p>

<p>My response variable is number of offspring, most of my explanatory variables are continuous like age or risk.  Do I need to calculate the mean of each explanatory variable within each response variable (get the mean risk for 1 offspring, mean risk for 2 offspring etc...)and then compare those? It also seems that these tests are for 2x2 tables.  If I am just looking at risk, my table would be a 1x3.  However, my complete model will have 4 variables (age, risk, bp, and #offspring year before).  </p>

<p>As you can likely tell, I am in the dark here on where to start. I would appreciate suggested readings, pdf lectures or videos of lectures would even be better!    </p>

<p>EDIT/UPDATE:
I have run the tests over my counts - I have 2 time periods (before/after) and then the count of offspring in each class (0,1,2).  I do not know how to interpret the values - what is ""good"".  What should I be looking for?  Any source that explains these values would be nice to see.  </p>

<pre><code>Kendall's (and Stuart's) Tau statistics
    Tau-b: 0.143
    Tau-c: 0.130
Somers' d:
    Columns dependent: 0.151 
    Rows dependent: 0.136 
    Symmetric: 0.143 
Goodman-Kruskal Gamma: 0.312 
Warning message:
In formatC(x, digits, format = ""f"") : class of 'x' was discarded
</code></pre>
"
"0.0395593886064618","0.0201455741006345","112858","<p>I am currently trying to fit a survival analysis model which has the following survival function:</p>

<p>$S(t) = \lambda_i e^{-\lambda_i t}$</p>

<p>but with </p>

<p>$\lambda_i = e^{\beta_0 +\beta_1 log(1+X_i)}$</p>

<p>where $X_i$ represents each unique observation. </p>

<p>I am trying to use the default survival package but am not sure where I can program in the regression formula for the hazard rate. I believe I might be misunderstanding what the survival analysis package does but would anyone be able to tell me if this can be even done in the R package? Thanks!</p>
"
"0.0740088392978143","0.0646095738380922","112997","<p>I am new to modelling percentage data, and I would be greatfull for some advice. I have proportion data (0,1] on a percentage of money sent by Player B to Player A. Participants received an amount of money, and could decide what percentage they will send back. I have two categorical predictors (1<sup>st</sup> with 3 factors, 2<sup>nd</sup> with 2); one continuous predictor; and one nesting factor (class). Since the data are bound between 0 and 1, I figured out that the best option would be Beta regression. I tried to use <code>hglm</code> package which fitted well, however, since the data are one-inflated (many people chose to send back the full amount), I am looking for other options.</p>

<p>As most appropriate seem to be <code>gamlss</code> package, which can use BEOI (Beta One Inflated) distribution. I used this code:</p>

<pre><code>m1 &lt;- gamlss(percent~cat1+cat2+continous, random(class), family=BEOI, data=dat, 
             mixture=""gq"", K=1)
</code></pre>

<p>From what I understand from package help files, this should be the simplest option. However, it produces very different results from the <code>hglm</code>command. Especially the standard errors are higher than beta coefficients, leading to non-significant results. I tried to specify other other functions in the model (e.g., <code>K, sigma.formula, nu.formula, mixture</code> etc.), but these are beyond my understanding, and I am not really sure what I did there.</p>

<p>I would very much appreciate any suggestions regarding either how to better specify the model, or simple explanations of <code>gamlss</code> function.</p>
"
"0.0839181358296689","0.0854704323447285","113756","<p>I'd like to test the <em>anova rbf kernel</em> included in the <strong>kernlab</strong> package in <strong>caret</strong>. Following excelent tutorial (<a href=""https://topepo.github.io/caret/custom_models.html"" rel=""nofollow"">https://topepo.github.io/caret/custom_models.html</a>) I've come up with the following code:</p>

<pre><code>SVManova &lt;- list(type = ""Regression"", library = ""kernlab"", loop = NULL)
prmanova &lt;- data.frame(parameter = c(""C"", ""sigma"", ""degree"", ""eps""),
                     class = rep(""numeric"", 4),
                     label = c(""Cost"", ""Sigma"", ""Degree"", ""Eps""))
SVManova$parameters &lt;- prmanova
    svmGridanova &lt;- function(x, y, len = NULL) {
    library(kernlab)
    sigmas &lt;- sigest(as.matrix(x), na.action = na.omit, scaled = TRUE, frac = 1)
    expand.grid(sigma = mean(sigmas[-2]), epsilon = 0.000001,
                C = 2 ^(-5:len), degree = 1:2) # len = tuneLength in train
    }
    SVManova$grid &lt;- svmGridanova
svmFitanova &lt;- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
  ksvm(x = as.matrix(x), y = y,
       kernel = ""anovadot"",
       kpar = list(sigma = param$sigma, degree = param$degree),
       C = param$C, epsilon = param$epsilon,
       prob.model = classProbs,
       ...) #default type = ""eps-svr""
}
SVManova$fit &lt;- svmFitanova
    svmPredanova &lt;- function(modelFit, newdata, preProc = NULL, submodels = NULL)
      predict(modelFit, newdata)
    SVManova$predict &lt;- svmPredanova
svmProb &lt;- function(modelFit, newdata, preProc = NULL, submodels = NULL)
  predict(modelFit, newdata, type=""probabilities"")
SVManova$prob &lt;- svmProb
    svmSortanova &lt;- function(x) x[order(x$C), ]
SVManova$sort &lt;- svmSortanova
</code></pre>

<p>I then asked for the model to train some dataset:</p>

<pre><code>set.seed(100) #use the same seed to train different models
svrFitanova &lt;- train(R ~ .,
                data = trainSet,
                method = SVManova,
                preProc = c(""center"", ""scale""),
                trControl = ctrl, tuneLength = 20,
                allowParallel = TRUE) #By default, RMSE and R2 are computed for regression (in all cases, selects the tunning and cross-val model with best value) , metric = ""ROC""
#Print the results
svrFitanova
</code></pre>

<p>But I get the following error:</p>

<pre><code>Error in train.default(x, y, weights = w, ...) : 
  The tuning parameter grid should have columns C, sigma, degree, eps
</code></pre>

<p>I don't see why this error occurs.... tune grid has four columns as requested... Any ideas? Thanks</p>
"
"0.0484501583111509","0.0493463771219827","114195","<p>I have a dataset with a lot of missing values and mix of continues and categorical variables.  I want to use something like group lasso to do features selection. Probably the output is binary 0,1 and so grouped lasso logistic regression seems to be the more sensible choice. </p>

<p>My problem is the very large number of missing values. Deleting non complete rows is not an option.</p>

<p>Is there any R implementation that can be used similarly to the lasso and that can handle missing values and categorical variables at the same time?   </p>

<p>A possible solution has been proposed <a href=""http://stats.stackexchange.com/questions/110461/lasso-or-other-regularized-regression-with-censored-missing-data"">here</a> but it does not refer to any R package.</p>
"
"0.0796117338651413","0.0900937462695559","114211","<p>Out of curiosity, I want to understand how to model this problem. I've been hearing people suggest the use of linear regression but <strong>I am not sure how to encode this problem</strong> (included my attempt below) in R as I am a complete beginner in this area. </p>

<p>I have a task that can be done any number of times (each individual instance is a task instance). Everytime the task completes 1%, I recorded the time elapsed since the task's start time. Therefore, for each task, I will have 100 points (100 1% increments) at which I recorded the time elapsed. </p>

<p>Given that I have this data for many instances, is it possible to predict the finish time for this task when a new task instance is given? </p>

<pre><code>      TaskID Percent TimeElapsed
   1:      1       0   0.2035333
   2:      1       1   0.2062833
   3:      1       2   0.2137167
   4:      1       3   0.2180833
   5:      1       4   0.2490833
  ---                           
3127:     31      96   4.9391667
3128:     31      97   4.9970500
3129:     31      98   5.5644500
3130:     31      99   5.6532667
3131:     31     100   5.8359833
</code></pre>

<p>A quick look at the task behavior (below) tells me there is a bit of a variance in how the task behaves so its hinting that the output should not just be a time prediction but rather a time prediction with some confidence? </p>

<p>In addition, I'm thinking just using the information about the current progress of the task might not be sufficient - the task may have slowed down in some its previous progress points so the finsh time would be affected. Therefore, this information should somehow be encoded into the model?</p>

<p><img src=""http://i.stack.imgur.com/eiEKh.png"" alt=""enter image description here""></p>

<p>I am particularly interested in how to do this using R. I included my initial attempt at using linear regression here but the result does not look good to me. Any suggestions on how to improve this or use some other methods? </p>

<p>I have given the output of dput (on a data table: <code>install.packages(""data.table"")</code>) on <a href=""http://pastebin.com/zX5GdKP2"" rel=""nofollow"">pastebin</a>. If you want a data.frame instead, please see this <a href=""http://pastebin.com/JwVhTCTU"" rel=""nofollow"">paste</a> instead.</p>

<p><strong>EDIT:</strong> Attempt at using linear regression</p>

<p>The thick black line is the median at every point. The thick red line is the regression line fit to the median line. </p>

<p><img src=""http://i.stack.imgur.com/mZFZd.png"" alt=""enter image description here""></p>
"
"0.0395593886064618","0.040291148201269","114218","<p>After running a gradient boosted model with <code>n</code> data points using multinomial regression where the response variable (a factor, as required by the gbm function) has <code>k</code> levels with R package gbm, I see that the predictions are output as as a vector of length <code>n*k</code>. Predicted responses are from:</p>

<pre><code>probs.var.multinom &lt;- predict.gbm(gbm.model.multinom, test.data, best.iter.gbm, 
                                  type=""response"")
</code></pre>

<p>Note that this is different from the output of a logistic (distribution = ""bernoulli"") model, where the results are a vector the same length as the number of cases.</p>

<p>How should this be interpreted? Specifically, how can I link the response vector back to the input data set to evaluate the classification?</p>
"
"0.0559454238864459","0.056980288229819","114399","<p>I have a theoretical growth function that can be perturbed by events, and I'd like to estimate the growth parameters as well as the perturbation, and the rate of falloff after that perturbation.</p>

<p>I'm thinking of using a logistic function to model the effect of the event and the falloff of that effect (if any).</p>

<p>To ground this, $x$ is time, and $t$ is the time the event occurs. Before time $t$, or if the event never occurs, we have a simple linear regression. After the event occurs, I model the contribution of the event with magnitude controlled by $\beta_2$ and rate of falloff by $\beta_3$.</p>

<p>$y_i=\left\{x_{i}&lt;t:\beta_{0}+\beta_1x_i+\epsilon_i,x_i&gt;t:\beta_0+\beta_1x_i+2\beta_2\frac{1}{\left(1+e^{\beta_3\left(x_i-t\right)}\right)}+\epsilon_i\right\}$</p>

<p>(<em>edited to add the error term</em>)</p>

<p>Here's a <a href=""https://www.desmos.com/calculator/nzmusqqosq"" rel=""nofollow"">Desmos graph</a> if it helps.</p>

<p>I'm really not sure how to estimate parameters for this model in any of the stats packages I'm familiar with in R. Do I need to turn to Bayesian methods?</p>
"
"0.0897122608032513","0.106600358177805","114468","<p>I am using the metafor package in R. I have fit a random effects model with a continuous predictor as follows </p>

<pre><code>SIZE=rma(yi=Ds,sei=SE,data=VPPOOLed,mods=~SIZE)
</code></pre>

<p>Which yields the output:</p>

<pre><code>R^2 (amount of heterogeneity accounted for):            63.62%
Test of Moderators (coefficient(s) 2): 
QM(df = 1) = 9.3255, p-val = 0.0023

Model Results:

                 se    zval    pval   ci.lb   ci.ub    
intrcpt  0.3266  0.1030  3.1721  0.0015  0.1248  0.5285  **
SIZE     0.0481  0.0157  3.0538  0.0023  0.0172  0.0790  **
</code></pre>

<p>Below I have plotted the regression.The effect sizes are plotted proportionally to the inverse of the standard error. I realize that this is a subjective statement, but the R2 (63% variance explained) value seems a lot larger than is reflected by the modest relationship shown in the plot (even taking weights into account).</p>

<p><img src=""http://i.stack.imgur.com/3JNmM.jpg"" alt=""enter image description here""></p>

<p>To show you what I mean, If I then do the same regression with the lm function (specifying study weights in the same way):</p>

<pre><code>lmod=lm(Ds~SIZE,weights=1/SE,data=VPPOOLed)
</code></pre>

<p>Then the R2 drops to 28% variance explained. This seems closer to the way things are (or at least, my impression of what kind of R2 should correspond to the plot). </p>

<p>I realize, after having read this article (including the meta-regression section): (<a href=""http://www.metafor-project.org/doku.php/tips:rma_vs_lm_and_lme"">http://www.metafor-project.org/doku.php/tips:rma_vs_lm_and_lme</a>), that differences in the way the lm and rma functions apply weights can influence the model coefficients. However, it is still unclear to me why the R2 values are so much larger in the case of meta-regression. Why does a model that looks to have a modest fit account for over half the heterogeneity in effects?</p>

<p>Is the larger R2 value because the variance is partitioned differently in the meta analytic case? (sampling variability v other sources) Specifically, does the R2 reflect the percent of heterogeneity accounted for <em>within the portion that cant be attributed to sampling variability</em>?. Perhaps there is a difference between ""variance"" in a non-meta-analytic regression and ""heterogeneity"" in a meta-analytic regression that I am not appreciating.</p>

<p>I'm afraid subjective statements like ""It doesn't seem right"" are all I have to go on here. Any help with interpreting R2 in the meta-regression case would be much appreciated. </p>
"
"0.027972711943223","0.0284901441149095","114692","<p>I would like to solve a linear regression (in R) with weights $w$ and a constraint.</p>

<p>In other words, I would like to find $x$ that minimizes the sum of squares
$$\sum_i w_i(b_i-Ax_i)^2$$</p>

<p>On top of that I have an external vector $d$, which I would like to use in a constraint, such that $d \cdot x \le 5$.</p>

<p>Is this something that would be possible to do in R with <code>solve.QP</code> or perhaps some other R script?</p>

<p><strong>Edit</strong>: I am adding a bounty for a solution that doesn't require any other custom software except the cran packages. While rstan works perfectly unfortunately I am unable to install it on my production servers due to old versions of some libraries.</p>
"
"0.10833784750436","0.102985730108887","115647","<p>I have the following problem: I have a set of English words which I want to translate to Dutch. Of each words I mined a set of possible translations. For example, for the word ""Eighteen"" I obtained only one possible Dutch translation: ""Achttien"", which is correct. However, for other words I obtained multiple translations. For the word ""Good""  I have the translations ""Goed"", ""Braaf"" and ""Eerlijk"", which are technically correct translations but by far the best and most commonly used translation is only the word ""Goed"".</p>

<p>For a training set of English words I manually defined the optimal (correct) translation. Using this set I want to train some model to optimally pick for each English word the optimal Dutch word using some predictors. For example, I assume words that are more frequently used are probably better translations than others, and I assume that words that are noted first in a list of translations are probably better translations than others (e.g., in a dictionary, the first translation is usually the best).</p>

<p>So, my data looks something like this:</p>

<pre><code>English     Dutch       Frequency   Order   Correct
---------------------------------------------------
Eighteen    Achttien    800         1       TRUE
Good        Goed        900         1       TRUE
Good        Braaf       500         2       FALSE
Good        Eerlijk     600         3       FALSE
old         bejaard     300         1       FALSE
old         oud         900         2       TRUE
</code></pre>

<p>I want to predict the classification in the column <code>Correct</code>. At first I thought a logistic regression could do this, but that does not take into account that each row is not independent. e.g., for each unique value of the column <code>English</code> only one is correct and all others are false. Thus, some other classification method is required.</p>

<p>I was hoping you could point me in the right direction as to what method (or even better, an <code>R</code> package) would be suitable to tackle this problem. I guess this problem occurs more often in Machine Learning but I have no experience in that field.</p>
"
"0.0685188709827532","0.0697863157798853","115841","<p>I am a medical intern trying to understand Cox regression modelling using R.</p>

<p>I am using the pbc data of the survival package with the following code:</p>

<pre><code>library(survival)
data(pbc)
s &lt;- Surv(pbc$time, pbc$status==2)
cfit.age &lt;- coxph(s ~ age, data=pbc)
summary(cfit.age)
</code></pre>

<p>The summary result is:</p>

<pre><code>Call:
coxph(formula = s ~ age, data = pbc)

  n= 418, number of events= 161 

        coef exp(coef) se(coef)     z Pr(&gt;|z|)    
age 0.039185  1.039963 0.007847 4.994 5.92e-07 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

    exp(coef) exp(-coef) lower .95 upper .95
age      1.04     0.9616     1.024     1.056

Concordance= 0.616  (se = 0.025 )
Rsquare= 0.058   (max possible= 0.985 )
Likelihood ratio test= 25.19  on 1 df,   p=5.205e-07
Wald test            = 24.94  on 1 df,   p=5.922e-07
Score (logrank) test = 25.3  on 1 df,   p=4.918e-07
</code></pre>

<p>As I understand it, the HR per year is 1.04. But I am not sure about:</p>

<ol>
<li><p>How can I calculate how much the HR is per 5, 10, or 20 years?</p></li>
<li><p>How can I modify the data frame to get the HR for age per 10 years
    by default?</p></li>
</ol>

<p>I am looking Forward to your replies!</p>
"
"0.0839181358296689","0.0854704323447285","116174","<p>I would like to set up a series of tests on the difference in survival between two very unequal sized groups.<br>
Generally either log-rank (using the R survdiff function) or a cox regression (R coxph) with stratified patient variables works well.  However, in some cases one group is small and the event relatively low incidence, which makes the expected number of events very small.  In those circumstances, it does not seem sensible to use the p-value generated by the log-rank test, since this is based on a chi-squared test, which is inappropriate for small numbers of expected events (surprisingly R does not give a warning message for this).  Taking an admittedly fairly extreme example to illustrate:</p>

<pre><code>survdiff(formula = survobject ~ (Fixation == i), data = TKRGroup)
n=637763, 424 observations deleted due to missingness.

                         N Observed Expected (O-E)^2/E (O-E)^2/V
Fixation == i=FALSE 637725    11174 1.12e+04  5.52e-04      11.9
Fixation == i=TRUE      38        3 5.17e-01  1.19e+01      11.9

Chisq= 11.9  on 1 degrees of freedom, p= 0.000555 
</code></pre>

<p>Cox regression gives a higher p-value of 0.0023, though it still looks a rather on the low side for these values of observed and expected events.</p>

<pre><code>coxph(formula = survobject ~ (Fixation == i), data = TKRGroup)

                  coef exp(coef) se(coef)    z      p
Fixation == iTRUE 1.76       5.8    0.577 3.05 0.0023
</code></pre>

<p>Further summary information gives</p>

<pre><code>Likelihood ratio test= 5.58  on 1 df,   p=0.01813
Wald test            = 9.27  on 1 df,   p=0.002325
Score (logrank) test = 11.92  on 1 df,   p=0.0005543
</code></pre>

<p>At this point, I could do with some expert advice on which, if any, of these p-values to use, or whether there is some alternative approach available (preferably available within an R package!)  Given the size of the groups, I rather naively attempted to get some idea of a sensible p-value by applying a Poisson exact test to the observed and expected figures; Values of observed / expected of 3 / 0.517 would give a cumulative Poisson P(X â‰¥ 3) = 0.0157.  That seems a much more reasonable figure, though I am not sure I could defend it.</p>
"
"0.027972711943223","0.0284901441149095","116175","<p>I try to get into in Cox Regression and read the example chapter from <a href=""http://www.clinicalpredictionmodels.org/"" rel=""nofollow"">Steyerberg's book</a>.</p>

<p>Afterwards I tried to plot log relative hazards against continuous variables using the rms package:</p>

<pre><code>d  &lt;- read.spss('~/R-Test/SMARTst.sav',use.value.labels=F, to.data.frame=T)
dd &lt;- datadist(d)
options(datadist=""dd"")
fit &lt;- cph(Surv(TEVENT,EVENT) ~  rcs(IMT,4), data=d)
plot(Predict(fit), lty=2, lwd=2)
</code></pre>

<p>What are knots? Well, I see what happens when I Change 4 to 3 or numbers > 4 ... but I do not know what I do and why? ;-) Maybe that will be explained in earlier chapters I currrently do not have.</p>

<p>Can someone explain it too me or recommand free websites/articles explaining this in an easy fashin (i am not a statistician).</p>

<p>My final goal is to learn how to create adequate cox models for exploratory medical analyses.</p>
"
"0.0969003166223018","0.0904683580569682","116272","<p>I have read similar posts to this but my problem is not resolved by the answers given. I want to do a v simple linear regression to see if bite incidence is related to district, zone (vacc or control) and year. As you can see in the output one of the districts RORYA is given NA coefficients, and I get the message ""Coefficients: (1 not defined because of singularities)"". I have read up on this and it seems its to do with co-linearity of factors. One solution given is to add -1 to the call, which removes the intercept but does not solve my problem as RORYA district still has NAs in the summary output.</p>

<p>Another solution I have tried is changing the order of the explanatory variables in the call. This does change things...Rorya district suddenly has coefficients but the Zone variable becomes NA'd. Neither of which is good as I would like a coefficent for all the explanatory variables.</p>

<p>I was wondering whether anyone might know why this is happening and whether there is a solution to this problem so that all the variables can have coefficients?</p>

<p>Thanks in advance.</p>

<p>A Reproducible example:</p>

<pre><code>df &lt;- structure(list(DISTRICT = structure(c(1L, 6L, 5L, 3L, 2L, 4L, 
1L, 6L, 5L, 3L, 2L, 4L, 1L, 6L, 5L, 3L, 2L, 4L, 1L, 6L, 5L, 3L, 
2L, 4L), .Label = c(""BUNDA"", ""MASWA"", ""MUSOMA"", ""RORYA"", ""SERENGETI"", 
""TARIME""), class = ""factor""), zone = structure(c(2L, 2L, 2L, 
1L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 2L, 2L, 1L, 1L, 1L, 2L, 
2L, 2L, 1L, 1L, 1L), .Label = c(""c"", ""v""), class = ""factor""), 
year = structure(c(1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 
2L, 2L, 3L, 3L, 3L, 3L, 3L, 3L, 4L, 4L, 4L, 4L, 4L, 4L), .Label = c(""2010"", 
""2011"", ""2012"", ""2013""), class = ""factor""), bites = c(7.461327937, 
NA, NA, NA, 35.16164185, 26.39109338, 57.89990479, 1.47191729, 
3.608371422, 51.36718605, NA, 16.21167165, 46.85713945, 15.89670673, 
5.212092054, 259.8137381, 30.80276062, 20.73585909, 10.44585911, 
9.420270656, 7.617673001, 307.4586643, 27.31565565, 30.16124958
), deaths = c(0, NA, NA, NA, 0, 1.508062479, 0.298453117, 
0, 0, 0, NA, 2.262093719, 0.298453117, 0.294383458, 0, 2.233355915, 
0.581184163, 1.131046859, 0.298453117, 0.588766916, 1.202790474, 
2.977807887, 0, 1.885078099)), .Names = c(""DISTRICT"", ""zone"", 
""year"", ""bites"", ""deaths""), row.names = c(NA, -24L), class = ""data.frame"")
</code></pre>

<p>Code:</p>

<pre><code>summary(df )
names(df)
attach(df)
is.numeric(year)
df$year  &lt;- as.factor(as.character(df$year))
is.factor(df$year)

model1 &lt;- lm(bites ~   zone + DISTRICT-1 +year, data = df)
summary(model1)

&gt; sessionInfo()
R version 3.1.0 (2014-04-10)
Platform: x86_64-apple-darwin13.1.0 (64-bit)

locale:
[1] en_GB.UTF-8/en_GB.UTF-8/en_GB.UTF-8/C/en_GB.UTF-8/en_GB.UTF-8

attached base packages:
[1] grid      stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] ggplot2_1.0.0

loaded via a namespace (and not attached):
[1] colorspace_1.2-4 digest_0.6.4     gtable_0.1.2     MASS_7.3-34      munsell_0.4.2   plyr_1.8.1       proto_0.3-10     Rcpp_0.11.2     
[9] reshape2_1.4     scales_0.2.4     stringr_0.6.2    tools_3.1.0  
</code></pre>
"
"0.0395593886064618","0.040291148201269","116494","<p>I am doing regression with a data with Y as target variable and 16 feature variables. I had two date feature variables which where as factor. I converted them to date format as shown below:</p>

<pre><code>X2 &lt;-  as.Date(X2, format=""%m/%d/%Y""))
</code></pre>

<p>I had a lot of missing data in my training as well as validation data set. I was suggested to try out imputation. R had a lot of packages like AMELIA and Mice.I started trying with Mice package but I am getting the below error.  </p>

<pre><code>Error in FUN(newX[, i], ...) : 'x' must be numeric
In addition: Warning message:
In FUN(newX[, i], ...) : NAs introduced by coercion 
</code></pre>

<p>Can anyone help me with this error.</p>

<p>Thanks</p>
"
"0.0791187772129236","0.080582296402538","116659","<p>I have a collection of continuous data from the literature, including the mean, the standard deviation and the number of observations for both experimental and control groups, as well some environmental variables. A meta-analysis coupled with a meta-regression could be done. However, some studies had several experimental sites, i.e. each line is not a single study, but a single site taken from a study: a random site effect could thus be nested in studies. The following meta mixed-model can be described.</p>

<ol>
<li>effect-size as response</li>
<li>environmental variables as fixed effects,</li>
<li>sites nested in studies as random effects and</li>
<li>each observation is weighted on the sampling variance.</li>
</ol>

<p>I tried two approaches with R, that returned somewhat different results.</p>

<p>As first step, let's create a dummy data frame.</p>

<pre><code>set.seed(40)
Study = factor(c(1,1,1,1,1,2,2,3,3,3,4,5,5,5,5))
Site = factor(c(1,2,3,4,5,1,2,1,2,3,1,1,2,3,4))
n_case = length(Study)
experimental_mean = rnorm(n_case,5,2)
experimental_sd = abs(rnorm(n_case,1,0.1))
experimental_n = round(runif(n_case, 2, 10))
control_mean = experimental_mean+runif(n_case,0,5)
control_sd = abs(rnorm(n_case,1,0.1))
control_n = round(runif(n_case, 2, 10))
A = experimental_mean/control_mean * runif(n_case, 0.5, 0.8)
B = rnorm(n_case,-1,1)
C = factor(letters[round(runif(n_case, 1, 3))])

data_table = data.frame(Study, Site, 
                        experimental_mean, experimental_sd, experimental_n,
                        control_mean, control_sd, control_n,
                        A, B, C)
</code></pre>

<p>Then, compute the effect size and the sampling variance.</p>

<pre><code>library(metafor)
meta_table = escalc(measure='ROM', data=data_table,
                  m1i=experimental_mean, m2i=control_mean,
                  sd1i=experimental_sd, sd2i=control_sd,
                  n1i=experimental_n, n2i=control_n)
</code></pre>

<p>The rma.mv function from the metafor package could be used to run the meta mixed-model.</p>

<pre><code>res = rma.mv(yi, vi, data=meta_table, method=""REML"", level=95,
              mods = ~ A+B+C, random=~1|Study/Site)
summary(res)
</code></pre>

<p>Another option is to run a mixed-model on the effect-size.</p>

<pre><code>library(nlme)
mixed_meta_model = lme(data = meta_table,
                       fixed = yi ~ A+B+C, # yi is the effect size
                       random = ~1|Study/Site,
                       weights = varIdent(~vi)) # vi is the sampling variance
summary(mixed_meta_model)
</code></pre>

<p>Three questions:</p>

<ol>
<li>Is the global approach valid?</li>
<li>Which approach would you suggest?</li>
<li>Is my R code correct? - with special attention to the weights argument <strong>varIdent(~vi)</strong> in the lme function.</li>
</ol>

<p>Many thanks,</p>

<p>S.-Ã‰. Parent
Laval University, Canada</p>
"
"0.0484501583111509","0.0493463771219827","116681","<p>I am working on a large linear regression with a volume metric as my dependent. Right now I am multiplying the model.matrix by the respective coefficients to get to the relative volume contribution by variable.</p>

<pre><code>decomp =  t(apply(model.matrix(fit$terms, data = Data[Data$RetailRead == ""Retail"",]), 1, function(x) {x*fit$coef})) 
</code></pre>

<p>However, this leads to some of the volume being meaningless due to a factor variables within the model. The factor variables need to be added to the base category in order to get to the total overall volume for that variable. </p>

<pre><code>asgn = attr(model.matrix(fit$terms, data = Data), ""assign"") #find indexes which need to be summed

merged =data.frame(t(apply(decomp, 1, function(x) {tapply(x, asgn, sum)})))#sum each appropriate collumn
colnames(merged) = c(""Intercept"", attr(terms(fit), ""term.labels"")) #label collumns
</code></pre>

<p>The code I have written to summarize the volume driven per variable is clunky and inefficient - there are several further steps then above to allow each column of the variable decomposition to be able to stand alone. I am surprised that there is little literature on doing this within R and I wonder if any one know of a package to better perform this task.</p>
"
"0.0559454238864459","0.0427352161723642","116850","<p>I have a dataset consisting of repeated measures data of graded toxicity scores (0-4) in a large number of patients being treated with a anti-cancer drug. We would like to identify predictors for developing dose limiting toxicity (e.g. score 3 or 4).</p>

<p>Initially the dataset was analyzed using a cumulative link mixed model to identify such predictors (clmm in R package ordinal).</p>

<p>However, there is also informative dropout occuring in the dataset (MNAR), e.g. the risk of dropping out is related to the severity of toxicity. As this therefore may bias parameter estimates, it seems a joint model that includes a survival model for dropout would be best. </p>

<p>My questions are:</p>

<ol>
<li><p>Is a joint model for dropout and outcome the only suitable option to account for dropout?  Or would there be other ways as well ? The only ugly alternative would be to analyze only the first part of the dataset, but I would rather like to use ALL the available data.</p></li>
<li><p>Is there an R package available that would readily allow such an analysis. E.g. longitudinal mixed-effect modelling of ordered categorical data, together with a dropout survival model. Or, alternatively, a mixed effect model logistic regression model with dropout (e.g. in this case we would reduce the dataset to having a dose limiting toxicity yes/no) ? The JM and joinR packages appear to be based on continuous data for the repeated measures.</p></li>
</ol>
"
"0.0395593886064618","0.040291148201269","116852","<p>I need help in understanding the <code>pmodel.response</code> function from the R package <code>plm</code>. So far I have interpreted this as a way to get predicted values from a panel data regression.</p>

<p>In the code below I run a least squares dummy variables regression using the standard <code>lm</code>-function and a fixed effects model using <code>plm</code> and then try to compare predictions and model response.</p>

<pre><code>library(plm)

data(Grunfeld)
Grunfeld &lt;- pdata.frame(Grunfeld, index = c(""firm"", ""year""))

grun.lm &lt;- lm(inv ~ value + capital + factor(firm), data=Grunfeld)
grun.fe &lt;- plm(inv ~ value + capital, data=Grunfeld, effect=""individual"",
               model=""within"")

Grunfeld$predict.lm &lt;- predict(grun.lm)
Grunfeld$predict.plm &lt;- pmodel.response(grun.fe)
</code></pre>

<p>Now, if I take a look at the outcome:</p>

<pre><code>&gt; head(Grunfeld)
       firm year   inv  value capital predict.lm predict.plm
1-1935    1 1935 317.6 3078.5     2.8   269.5876     -290.42
1-1936    1 1936 391.8 4661.7    52.6   459.3769     -216.22
1-1937    1 1937 410.6 5387.1   156.9   571.6005     -197.42
1-1938    1 1938 257.7 2792.2   209.2   302.0566     -350.32
1-1939    1 1939 330.8 4313.2   203.4   467.7566     -277.22
1-1940    1 1940 461.2 4643.9   207.2   505.3528     -146.82
</code></pre>

<p>It seems like the output of <code>pmodel.response</code> hardly has anything to do with predicted values. So, what does this function actually do? How to interpret the values in column <code>Grundfeld$predict.plm</code>? This does not get clear for me from the documentation.</p>

<p>Thanks for any help!</p>
"
"0.0685188709827532","0.0697863157798853","117052","<p>I have been trying to replicate the results of the Stata option <code>robust</code> in R. I have used the <code>rlm</code> command form the MASS package and also the command <code>lmrob</code> from the package ""robustbase"". In both cases the results are quite different from the ""robust"" option in Stata. Can anybody please suggest something in this context?</p>

<p>Here are the results I obtained when I ran the robust option in Stata:</p>

<pre><code>. reg yb7 buildsqb7 no_bed no_bath rain_harv swim_pl pr_terrace, robust

Linear regression                                      Number of obs =    4451
                                                       F(  6,  4444) =  101.12
                                                       Prob &gt; F      =  0.0000
                                                       R-squared     =  0.3682
                                                       Root MSE      =   .5721

------------------------------------------------------------------------------
             |               Robust
         yb7 |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
   buildsqb7 |   .0046285   .0026486     1.75   0.081    -.0005639     .009821
      no_bed |   .3633841   .0684804     5.31   0.000     .2291284    .4976398
     no_bath |   .0832654   .0706737     1.18   0.239    -.0552904    .2218211
   rain_harv |   .3337906   .0395113     8.45   0.000     .2563289    .4112524
     swim_pl |   .1627587   .0601765     2.70   0.007     .0447829    .2807346
  pr_terrace |   .0032754   .0178881     0.18   0.855    -.0317941    .0383449
       _cons |   13.68136   .0827174   165.40   0.000     13.51919    13.84353
</code></pre>

<p>And this is what I obtained in R with the lmrob option:</p>

<pre><code>&gt; modelb7&lt;-lmrob(yb7~Buildsqb7+No_Bed+Rain_Harv+Swim_Pl+Gym+Pr_Terrace, data&lt;-bang7)
&gt; summary(modelb7)

Call:
lmrob(formula = yb7 ~ Buildsqb7 + No_Bed + Rain_Harv + Swim_Pl + Gym + Pr_Terrace, 
    data = data &lt;- bang7)
 \--&gt; method = ""MM""
Residuals:
      Min        1Q    Median        3Q       Max 
-51.03802  -0.12240   0.02088   0.18199   8.96699 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 12.648261   0.055078 229.641   &lt;2e-16 ***
Buildsqb7    0.060857   0.002050  29.693   &lt;2e-16 ***
No_Bed       0.005629   0.019797   0.284   0.7762    
Rain_Harv    0.230816   0.018290  12.620   &lt;2e-16 ***
Swim_Pl      0.065199   0.028121   2.319   0.0205 *  
Gym          0.023024   0.014655   1.571   0.1162    
Pr_Terrace   0.015045   0.013951   1.078   0.2809    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Robust residual standard error: 0.1678 
Multiple R-squared:  0.8062,    Adjusted R-squared:  0.8059 
</code></pre>
"
"0.0559454238864459","0.056980288229819","117340","<p>My situation:</p>

<ul>
<li>small sample size: 116</li>
<li>binary outcome variable</li>
<li>long list of explanatory variables: 50</li>
<li>explanatory variables did not come from the top of my head; their choice was based on the literature.</li>
</ul>

<p>Following a suggestion to a previous <a href=""http://stats.stackexchange.com/questions/102892/how-to-select-a-subset-of-variables-from-my-original-long-list-in-order-to-perfo"">question</a> of mine, I have run LASSO (using R's glmnet package) in order to select the subset of exaplanatory variables that best explain variations in my binary outcome variable.</p>

<p>I have calculated lambda.min through cross-validation (cv.glmnet command) and got the correspondent coefficients for my explanatory variables. For 6 of my total 50 explanatory variables, the coefficients were non-zero. Are those coefficients comparable, i.e. can I say that the variables with the highest ones are the most important? If they are not comparable, can I run logistic regression (using glm) with those 6 variables and then compare them in terms of coefficients and p-values?</p>
"
"0.142998663041709","0.140442251384781","117664","<p>In order to run Lasso and elastic net multiple regressions on my company's SAS server (which doesn't support R), I've been working on a coordinate descent macro for performing least squares regressions (as described in the 2010 paper <a href=""http://www.jstatsoft.org/v33/i01/paper"" rel=""nofollow"">""Regularization Paths for Generalized Linear Models via Coordinate Descent""</a> by Jerome Friedman, Trevor Hastie, and Rob Tibshirani). </p>

<p>Ideally, I would like the coefficient estimates from my SAS algorithm to match the outcomes from the  <a href=""http://cran.r-project.org/web/packages/glmnet/index.html"" rel=""nofollow"">glmnet package</a> in R written by Friedman, et al. which also implements coordinate descent for least squares regression. </p>

<p>I've decided to test the algorithm on the Fitness data from SAS documentation, with Oxygen as the response variable: </p>

<pre><code>data fitness;
  input Age Weight Oxygen RunTime RestPulse RunPulse MaxPulse @@;
  datalines;
   44 89.47 44.609 11.37 62 178 182   40 75.07 45.313 10.07 62 185 185
   44 85.84 54.297  8.65 45 156 168   42 68.15 59.571  8.17 40 166 172
   38 89.02 49.874  9.22 55 178 180   47 77.45 44.811 11.63 58 176 176
   40 75.98 45.681 11.95 70 176 180   43 81.19 49.091 10.85 64 162 170
   44 81.42 39.442 13.08 63 174 176   38 81.87 60.055  8.63 48 170 186
   44 73.03 50.541 10.13 45 168 168   45 87.66 37.388 14.03 56 186 192
   45 66.45 44.754 11.12 51 176 176   47 79.15 47.273 10.60 47 162 164
   54 83.12 51.855 10.33 50 166 170   49 81.42 49.156  8.95 44 180 185
   51 69.63 40.836 10.95 57 168 172   51 77.91 46.672 10.00 48 162 168
   48 91.63 46.774 10.25 48 162 164   49 73.37 50.388 10.08 67 168 168
   57 73.37 39.407 12.63 58 174 176   54 79.38 46.080 11.17 62 156 165
   52 76.32 45.441  9.63 48 164 166   50 70.87 54.625  8.92 48 146 155
   51 67.25 45.118 11.08 48 172 172   54 91.63 39.203 12.88 44 168 172
   51 73.71 45.790 10.47 59 186 188   57 59.08 50.545  9.93 49 148 155
   49 76.32 48.673  9.40 56 186 188   48 61.24 47.920 11.50 52 170 176
   52 82.78 47.467 10.50 53 170 172
   ;
run;
</code></pre>

<p>Here's my first attempt at writing the code for a simple OLS model. (I realize running a data set inside a macro loop is bad form &amp; slows down execution times - this is just a first pass at the problem.)</p>

<p>For the example here I'm fitting a model for a single value of lambda and alpha in an elastic net model. I'm achieving the closest match to glmnet output when I standardize the six predictor variables using proc standard. Initial values for the coefficients are fit via proc reg. Output coefficient values are then converted back to the original unstandardized scale (scroll to bottom of the code below). </p>

<pre><code>            /* Calculate mean and stnd dev values for standardizing fitness variables. */
            proc means data=fitness mean std;
               var Oxygen Age Weight RunTime RestPulse RunPulse MaxPulse;
               output out=fitness_mean_std;
            run;

            data fitness_mean_std (drop=_TYPE_ _FREQ_);
            set fitness_mean_std;
               if _STAT_ in ('MEAN','STD');
            run;

            %let t=7;
            data _null_;
            set fitness_mean_std;
               if _STAT_='MEAN' then do;
                 array mean[1:&amp;t] Oxygen Age Weight RunTime RunPulse RestPulse MaxPulse;
                 do m = 1 to &amp;t;
                    call symputx(cats('mean',m),mean[m],'g');
                 end;
               end;
               else if _STAT_='STD' then do;
                 array std[1:&amp;t] Oxygen Age Weight RunTime RunPulse RestPulse MaxPulse;
                 do s = 1 to &amp;t;
                     call symputx(cats('std',s),mean[s],'g');
                 end;
               end;
            run;

            /* Create input dataset for coordinate descent macro. */
            proc standard data=fitness mean=0 std=1 out=fitness_stnd;
               var Age Weight RunTime RunPulse RestPulse MaxPulse;
            run;

            proc reg data=fitness_stnd outest=params_stnd;
               model Oxygen = Age Weight RunTime RunPulse RestPulse MaxPulse;
            run;
            quit;

            %let t=6;
            data _null_;
            set params_stnd;
               array x[0:&amp;t] Intercept Age Weight RunTime RunPulse RestPulse MaxPulse;
               do _n_ = 0 to &amp;t;
                  call symputx(cats('p',_n_),x[_n_],'g');
               end;
            run;
            %put &amp;p0 &amp;p1 &amp;p2 &amp;p3 &amp;p4 &amp;p5 &amp;p6;

            %macro assignvar(k);
            data fitness_array (drop=Oxygen Age Weight RunTime RunPulse RestPulse MaxPulse);
            set fitness_stnd;
               y=Oxygen;
               array a[6] Age Weight RunTime RunPulse RestPulse MaxPulse;
               array x[6];
               %do i=1 %to 6;
                 x[&amp;i]=a[&amp;i];
               %end;
            run;
            %mend;
            %assignvar(6)    

            /* Coordinate descent macro. */
            %macro test(dataset=, numvars=, numiter=, lambda=, alpha=);
               %do i=1 %to &amp;numiter;
                 %do j=1 %to &amp;numvars;
                    data &amp;dataset (keep=y x1-x&amp;numvars);
                    set &amp;dataset end=end_data;
                       array x[&amp;numvars] x1-x&amp;numvars;
                       %let gamma = %sysevalf(&amp;lambda*&amp;alpha);

                       /* Calculate partial residuals for fitting coefficients.*/
                       yhat_&amp;j = &amp;p0 - &amp;&amp;p&amp;j*x[&amp;j];
                       %do k=1 %to &amp;numvars;
                            yhat_&amp;j = yhat_&amp;j + &amp;&amp;p&amp;k*x[&amp;k];
                       %end;
                       if _n_=1 then z_&amp;j = x&amp;j*(y - yhat_&amp;j);                                           else z_&amp;j = x&amp;j*(y - yhat_&amp;j) + z_&amp;j; end;
                       if end_data then do;
                          z_avg_&amp;j = z_&amp;j/_n_;
                          if (z_avg_&amp;j &gt; 0 and &amp;gamma &lt; abs(z_avg_&amp;j)) then do;
                             p&amp;j = (z_avg_&amp;j - &amp;gamma)/(1 + &amp;lambda - &amp;gamma);
                             call symputx(""p&amp;j"", p&amp;j, 'g');
                          end;  
                          else if (z_avg_&amp;j &lt; 0 and &amp;gamma &lt; abs(z_avg_&amp;j)) then do;
                             p&amp;j = (z_avg_&amp;j + &amp;gamma)/(1 + &amp;lambda - &amp;gamma);
                             call symputx(""p&amp;j"", p&amp;j, 'g');
                          end;
                          else if &amp;gamma &gt;= abs(z_avg_&amp;j) then do;
                             p&amp;j = 0;
                             call symputx(""p&amp;j"", p&amp;j, 'g');
                          end;
                       end;
                       retain z_&amp;j;
                    run;
                 %end;
               %end;
               %put _user_;
            %mend;
            %test(dataset=fitness_array, numvars=6, numiter=50, lambda=.1, alpha=.5)            

            /* Return regression coefficients in original scale. */
                %let p0_unstand = %sysevalf(&amp;p0-(&amp;p1*&amp;mean2/&amp;std2)-(&amp;p2*&amp;mean3/&amp;std3)-(&amp;p3*&amp;mean4/&amp;std4)-(&amp;p4*&amp;mean5/&amp;std5)-(&amp;p5*&amp;mean6/&amp;std6)-(&amp;p6*&amp;mean7/&amp;std7));
                %let p1_unstand = %sysevalf(&amp;p1/&amp;std2);
                %let p2_unstand = %sysevalf(&amp;p2/&amp;std3);
                %let p3_unstand = %sysevalf(&amp;p3/&amp;std4);
                %let p4_unstand = %sysevalf(&amp;p4/&amp;std5);
                %let p5_unstand = %sysevalf(&amp;p5/&amp;std6);
                %let p6_unstand = %sysevalf(&amp;p6/&amp;std7);

                %put 
                p0_unstand = &amp;p0_unstand 
                p1_unstand = &amp;p1_unstand 
                p2_unstand = &amp;p2_unstand 
                p3_unstand = &amp;p3_unstand 
                p4_unstand = &amp;p4_unstand 
                p5_unstand = &amp;p5_unstand 
                p6_unstand = &amp;p6_unstand;
                /*
                p0_unstand = 107.068671308076 
                p1_unstand = -0.24178321947146 
                p2_unstand = -0.05008520720235
                p3_unstand = -2.47736090772018 
                p4_unstand = -0.16124847253703 
                p5_unstand = -0.03822018686055
                p6_unstand = 0.06524084784678
                */
</code></pre>

<p>The corresponding commands in R:</p>

<pre><code>&gt; elasticnet_fit = glmnet(x, y, family=""gaussian"", lambda=.1, alpha=.5); 
&gt; coef(elasticnet_fit);   
7 x 1 sparse Matrix of class ""dgCMatrix""
                      s0
(Intercept) 105.10824556
x1           -0.22996264
x2           -0.05775625
x3           -2.64766834
x4           -0.01998125
x5           -0.26222028
x6            0.18003526
</code></pre>

<p>My question: The coefficients output from the SAS macro doesn't match the output from glmnet, although the values are close. Is there a flaw in my code or should I not be too concerned? Thanks!</p>
"
"0.0740088392978143","0.0753778361444409","117816","<p>I tried gradient boosting models using both <code>gbm</code> in R and <code>sklearn</code> in Python. However, neither of them can provide the coefficients of the model. For <code>gbm</code> in R, it seems one can get the tree structure, but I can't find a way to get the coefficients. For <code>sklearn</code> in Python, I can't even see the tree structure, not to mention the coefficients. Can anyone give me some help? </p>

<p>After searching online for couple of hours, I still can't find the answer. I can find similar questions since 2009, but no answers. Like the followings:</p>

<ul>
<li><p><a href=""http://r.789695.n4.nabble.com/GBM-package-Extract-coefficients-td963379.html"" rel=""nofollow"">GBM package: Extract coefficients </a> (r-help thread)</p></li>
<li><p><a href=""https://stackoverflow.com/questions/24478868/"">Implementing Gradient Boosted Regression Trees in production - Mathemtically describing the learnt model</a> (SO thread)</p></li>
</ul>

<p>This make me wonder if R and Python are mainly used by academic people, and thus majority of the users don't care about how to use them in industry. For example, if you want to implement the results in some real-time platform which doesn't run Python, what would you do?</p>
"
"0.0559454238864459","0.056980288229819","117867","<p>I have a basic linear regression model I fitted to a time series. Unfortunately I have to account for autocorrelation and heteroskedasicity in the model and I have done so with the NeweyWest function from the sandwich package in R while analyzing the coefficients. </p>

<p>Now I would like to create prediction intervals using the predict() function (or any other function) while utilizing the NeweyWest matrix/SEs.</p>

<p>As this is the first quesiton I post on here and my experinece in R is very limited here is some information:</p>

<pre><code>LMModel = lm(Return~Sentiment, data=Time Series)
</code></pre>

<p><strong>This is the function I used for my coefficient testing:</strong></p>

<pre><code>coeftest(LMModel , vcov=NeweyWest(LMModel , lag=27, ar.method=""ols""))
</code></pre>

<p><strong>I would like thsi function to use NeweyWest in some way:</strong></p>

<pre><code>predict(LMModel, newdata, interval = ""prediction"", level = 0.95) 
</code></pre>

<p>Thanks a lot in advance!</p>
"
"0.0419590679148345","0.056980288229819","117910","<p>According to Wikipedia (source of all truth and knowledge...),
<a href=""http://en.wikipedia.org/wiki/Generalized_least_squares#Properties"" rel=""nofollow"">http://en.wikipedia.org/wiki/Generalized_least_squares#Properties</a></p>

<p>a weighted least square regression is equivalent to a standard least square regression, if the variables have been previously ""decorrelated"" using a Cholesky decomposition.</p>

<p>I made up then a very simple example with the function pgls from the package CAPER to test it, where the correlation arises from a phylogeny tree:</p>

<pre><code>tree.mod:
((A:0.2,(B:0.1,C:0.1):0.1):0.1,((E:0.1,F:0.1):0.1,D:0.2):0.1);
</code></pre>

<p>The two approaches are compared here:</p>

<pre><code>library(caper)

## Data
species = c(""A"",""B"",""C"",""D"",""E"",""F"")
gene = c(0.1,0.2,0.3,0.5,0.6,0.7)
pheno = c( 0,0,0,1,1,1)
data=data.frame(species,gene,pheno)

## Phylogeny
tree = read.tree( ""small/tree_small.mod"" )

## GLS regression
cat(""\n     ===&gt; GLS\n"")
cdata   = comparative.data( phy = tree, data = data, names.col = ""species"" )
res = pgls( pheno~gene, cdata )
print(summary(res))

## Cholesky
cat(""\n     ===&gt; Cholesky\n"")
corr = vcv( tree )
cholesky = chol( corr )
invCho = solve( cholesky )
data.gene =  invCho %*% as.vector( data$gene )
data.pheno =  invCho %*% as.vector( data$pheno )
res=lm( data.pheno ~ data.gene )
print(summary(res))
</code></pre>

<p>and yield the outputs:</p>

<pre><code>====&gt; GLS
Coefficients:
        Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept) -0.13000    0.27261 -0.4769  0.65834  
gene         1.63333    0.59489  2.7456  0.05161 .


=====&gt;Cholesky
Coefficients:
        Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)  0.02214    0.28551   0.078    0.942  
data.gene    1.29188    0.35006   3.690    0.021 *
</code></pre>

<p>as you can see the results are different...</p>

<p>Does anyone have a clue why?</p>
"
"0.027972711943223","0.0284901441149095","117980","<p>I have some data that I'm fitting a multiple regression to, with the twist that the error distribution is t (with user-defined degrees of freedom) instead of Gaussian. I've been coding up my own function to do this, but recently I saw some posts that the <code>rlm</code> function in the MASS package has this ability already. Is this true? I have the accompanying book* and it mentions a number of M-estimators with Huber's as the default. I don't see how any of them correspond to the t distribution.</p>

<p>(Background: the t is because I'm interested in simulating the distribution of the response, not just estimating its conditional mean. From looking at the residuals, it appears that a suitably scaled t distribution with 4-5 df is a reasonable fit, and certainly much better than the Gaussian. It's only tangentially related to robustness considerations.)</p>

<p><br></p>

<p>* <em>Modern Applied Statistics with S</em> 4th Ed, Venables &amp; Ripley (2002)</p>
"
"0.0419590679148345","0.056980288229819","118034","<p>I would like to build a logistic regression model in which I will be looking for predictor variables having a significant effect on the breeding success of a raptor bird. </p>

<p>The predictors in the dataset are highly correlated, which led me to consider logistic ridge regression. Furthermore, I investigated different breeding grounds in which one or multiple birds have been breeding. Since this makes the data clustered, I would need to add the breeding ground as a random effect in the model.</p>

<p>Thus, I would need a 'mixed logistic ridge regression' approach if I am getting things right here. This paper suggests this approach too for another problem:</p>

<p><a href=""http://www.ncbi.nlm.nih.gov/pubmed/22049265"" rel=""nofollow"">http://www.ncbi.nlm.nih.gov/pubmed/22049265</a></p>

<p>Are there any people aware of the availability of an R package or something related having implemented a mixed logistic ridge regression approach as the paper and myself just described? I did not succeed in finding one.</p>

<p>Thank you.</p>
"
"NaN","NaN","118102","<p>Has any body encountered a problem finding numerical values of b-coefficients while developing partial least squares regression model from spectroscopic data using <code>pls</code> package in R? If so, how could I get those numbers so as I could develop a b-coefficient plot using the numbers?</p>
"
"0.187691011929479","0.195318587567468","118215","<p>This is my first post on StackExchange, but I have been using it as a resource for quite a while, I will do my best to use the appropriate format and make the appropriate edits. Also, this is a multi-part question. I wasn't sure if I should split the question into several different posts or just one. Since the questions are all from one section in the same text I thought it would be more relevant to post as one question.</p>

<p>I am researching habitat use of a large mammal species for a Master's Thesis. The goal of this project is to provide forest managers (who are most likely not statisticians) with a practical framework to assess the quality of habitat on the lands they manage in regard to this species. This animal is relatively elusive, a habitat specialist, and usually located in remote areas. Relatively few studies have been carried out regarding the distribution of the species, especially seasonally.  Several animals were fitted with GPS collars for a period of one year. One hundred locations (50 summer and 50 winter) were randomly selected from each animal's GPS collar data. In addition, 50 points were randomly generated within each animal's home range to serve as ""available"" or ""pseudo-absence"" locations. The locations from the GPS collars are coded a 1 and the randomly selected available locations are coded as 0.</p>

<p>For each location, several habitat variables were sampled in the field (tree diameters, horizontal cover, coarse woody debris, etc) and several were sampled remotely through GIS (elevation, distance to road, ruggedness, etc). The variables are mostly continuous except for 1 categorical variable that has 7 levels.</p>

<p>My goal is to use regression modelling to build resource selection functions (RSF) to model the relative probability of use of resource units. I would like to build a seasonal (winter and summer) RSF for the population of animals (design type I) as well as each individual animal (design type III).</p>

<p>I am using R to perform the statistical analysis.</p>

<p>The <strong>primary text</strong> I have been using isâ€¦</p>

<ul>
<li>""Hosmer, D. W., Lemeshow, S., &amp; Sturdivant, R. X. 2013. Applied Logistic Regression. Wiley, Chicester"".</li>
</ul>

<p>The majority of the examples in Hosmer et al. use STATA, <strong>I have also been using the following 2 texts for reference with R</strong>.</p>

<ul>
<li>""Crawley, M. J. 2005. Statistics : an introduction using R. J. Wiley,
Chichester, West Sussex, England.""</li>
<li>""Plant, R. E. 2012. Spatial Data Analysis in Ecology and Agriculture 
Using R. CRC Press, London, GBR.""</li>
</ul>

<p>I am currently following the steps in <strong>Chapter 4 of Hosmer et al. for the ""Purposeful Selection of Covariates""</strong> and have a few questions about the process. I have outlined the first few steps in the text below to aid in my questions.</p>

<ol>
<li>Step 1: A univariable analysis of each independent variable (I used a
univariable logistic regression). Any variable whose univariable test
has a p-value of less than 0.25 should be included in the first
multivariable model.</li>
<li>Step 2: Fit a multivariable model containing all covariates
identified for inclusion at step 1 and to assess the importance of
each covariate using the p-value of its Wald statistic. Variables
that do not contribute at traditional levels of significance should
be eliminated and a new model fit. The newer, smaller model should be
compared to the old, larger model using the partial likelihood ratio
test.</li>
<li>Step 3: Compare the values of the estimated coefficients in the
smaller model to their respective values from the large model. Any
variable whose coefficient has changed markedly in magnitude should
be added back into the model as it is important in the sense of
providing a needed adjustment of the effect of the variables that
remain in the model. Cycle through steps 2 and 3 until it appears that all of the important variables are included in the model and those excluded are clinically and/or statistically unimportant. Hosmer et al. use the ""<em>delta-beta-hat-percent</em>""
as a measure of the change in magnitude of the coefficients. They
suggest a significant change as a <em>delta-beta-hat-percent</em> of >20%. Hosmer et al. define the <em>delta-beta-hat-percent</em> as 
$\Delta\hat{\beta}\%=100\frac{\hat{\theta}_{1}-\hat{\beta}_{1}}{\hat{\beta}_{1}}$.
Where $\hat{\theta}_{1}$ is the coefficient from the smaller model and $\hat{\beta}_{1}$ is the coefficient from the larger model.</li>
<li>Step 4: Add each variable not selected in Step 1 to the model
obtained at the end of step 3, one at a time, and check its
significance either by the Wald statistic p-value or the partial
likelihood ratio test if it is a categorical variable with more than
2 levels. This step is vital for identifying variables that, by
themselves, are not significantly related to the outcome but make an
important contribution in the presence of other variables. We refer
to the model at the end of Step 4 as the <em>preliminary main effects
model</em>.</li>
<li>Steps 5-7: I have not progressed to this point so I will leave these
steps out for now, or save them for a different question.</li>
</ol>

<p><strong>My questions:</strong> </p>

<ol>
<li>In step 2, what would be appropriate as a traditional level of
significance, a p-value of &lt;0.05 something larger like &lt;.25?</li>
<li>In step 2 again, I want to make sure the R code I have been using for the partial likelihood test is correct and I want to make sure I am interpreting the results correctly. Here is what I have been doing&hellip;<code>anova(smallmodel,largemodel,test='Chisq')</code> If the p-value is significant (&lt;0.05) I add the variable back to the model, if it is insignificant I proceed with deletion?</li>
<li>In step 3, I have a question regarding the <em>delta-beta-hat-percent</em> and when it is appropriate to add an excluded variable back to the model. For example, I exclude one variable from the model and it changes the $\Delta\hat{\beta}\%$ for a different variable by >20%. However, the variable with the >20% change in $\Delta\hat{\beta}\%$ seems to be insignificant and looks as if it will be excluded from the model in the next few cycles of Steps 2 and 3. How can I make a determination if both variables should be included or excluded from the model? Because I am proceeding by excluding 1 variable at a time by deleting the least significant variables first, I am hesitant to exclude a variable out of order.</li>
<li><p>Finally, I want to make sure the code I am using to calculate $\Delta\hat{\beta}\%$ is correct. I have been using the following code. If there is a package that will do this for me or a more simple way of doing it I am open to suggestions.  </p>

<p><code>100*((smallmodel$coef[2]-largemodel$coef[2])/largemodel$coef[2])</code></p></li>
</ol>
"
"0.0484501583111509","0.0493463771219827","118396","<p>I'm trying to fit a quantile regression model for rigth censoring data and I'm using R with the package quantreg and its function  crq. I'm trying the Portnoy method that it's suposed to estimate the full range of tau (quantiles) , but the results only contains 85 taus, ending in tau=0.4 and giving me estimated values of that lasts taus that are very far away from the real 0.4 quantile and more around 0.95 quantile.</p>

<p>I read in the quantreg doc that Portnoy and Peng-Huang may be unable to estimate upper conditional quantiles if censoring is heavy in the upper in the upper tail, but this doesn't seem to be my case. In fact the general distribution summary of time is:</p>

<pre><code>Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
1.0   223.0   447.0   482.5   714.0  1251.0 
</code></pre>

<p>the quantiles vary obviously with levels of VAR1 and VAR2 but the shape of the distribution is almost the same.I can produce Kaplan-Mier estimation without problem. </p>

<p>Here is my syntax</p>

<pre><code>qreg1&lt;-crq(Surv(TIME,EVENT,type=""right"")~VAR1+VAR2,
       data=DATA_TRAIN,method = ""Portnoy"")
</code></pre>

<p>What am I doing wrong?</p>

<p>Thank you in advance,</p>
"
"0.0419590679148345","0.056980288229819","118559","<p>I have a regression problem scenario. Basically, I want to model a certain biological problem with regression models and at the end my model should be interpretable.</p>

<p>I need to have a sparse model. So I'm trying Lasso and Elastic Nets. But the performance of these methods in my data set is not good. </p>

<p>I'm thinking about using a random forest for the regression. <br></p>

<ul>
<li>Would someone give me the name of the most useful R package for random forest for regression? <br></li>
<li>Also, is the random forest solution for regression sparse and interpretable? </li>
</ul>
"
"0.0909113138154747","0.106838040430911","118621","<p>I am trying to perform a relatively simple multiple regression with a single breakpoint using the <em>segmented</em> package in R. My question is how to handle the interaction between the predictor variables as follows.</p>

<p>My model is: $$Runoff = \beta_0+\beta_1(A+\beta_2P)$$ where there is a breakpoint at some value of $(A+\beta_2P)$. I expect $\beta_0$ and $\beta_1$ to be different on each side of the breakpoint, and $\beta_2$ to be the same (in fact I'll just say <em>a priori</em> that $\beta_2$ should not vary across the breakpoint, but this would be interesting to test)</p>

<p>My question is how to implement this using <em>segmented</em> (or a similar breakpoint linear regression model). Options are:<br>
1. Run some loop through values of $\beta_2$ and create an intermediate variable $\tau = (A+\beta_2P)$, then it's a simple application of <em>segmented</em>. My concern is that I'd like to get a fitted value of $\beta_2$ from the data and this seems like a crude way of doing it.<br>
2. Some clever way of using <em>segmented</em> on the fully explicit model that will estimate $\beta_0, \beta_1,$ and $\beta_2$<br>
any help appreciated</p>

<p><strong>ADDENDUM:</strong></p>

<p>To describe what I'm trying to do. This is for a watershed runoff generation project. My hypothesis is that runoff amount is a function of both precipitation and soil water table position, and that there is a very strong threshold response in the latter. Whenever it rains, the water table goes up, but if it does not reach the threshold no runoff is generated. If enough rain falls to raise the water table above the threshold, then runoff is generated. Thus, runoff is a function of $(Ant + Pcp)$ with a notable breakpoint. </p>

<p>So, in the model as formulated in my original question $\beta_0$ and $\beta_1$ are the slope and intercept of the runoff response, and $\beta_2$ is a coefficient that converts rainfall depth to water table rise, which I assume is something like the porosity of the soil.  </p>

<p>I'm interested in (in order of importance): (1) is this model better than just a linear relationship between runoff and precipitation?; (2) can the model estimate where the water table threshold is?; (3) Can the model estimate the ""porosity"" coefficient (this would be a nice check against physical reality and I think also improve the physical fidelity of the estimated threshold position).  </p>

<p>Below is a graph using a hand-picked value of $\beta_2$, which let me do a piece wise linear regression. Solid line is the piece wise, dashed is a linear regression between runoff and precip alone. I'd like to be able to estimate the parameters of the model ($\beta_0, \beta_1, \beta_2$, and the breakpoint position) from the data. Ultimately, I have three replicates of three watershed types, so I'm interested in comparisons there as well.</p>

<p>Thanks for the help,</p>

<p><img src=""http://i.stack.imgur.com/L5JuQ.jpg"" alt=""enter image description here""></p>
"
"0.0395593886064618","0.040291148201269","119946","<p>I have some time series data where I'm modelling temperature as a function of various predictors. On physical grounds, I can expect that</p>

<p>$$\frac{dT}{dt} \propto T_a - T$$</p>

<p>where $T_a$ is the ambient temperature (which can vary over time, but whose values are known). I thus fit models of the form</p>

<p>$$\Delta T(t) \sim \alpha + \beta \left[ T_a(t) -T(t) \right] + \gamma X(t)$$</p>

<p>with $X$ being the other covariates, and $\alpha$, $\beta$ and $\gamma$ are the regression parameters. I can fit these easily enough in R:</p>

<pre><code>lm(diff(T) ~ I(Ta - T) + x, data=df)
</code></pre>

<p>and I can get predictions for the change in $T$. However, what I really want are predictions for $T$ itself. At the moment I'm calculating these via a loop, where I plug $\hat{T}(t)$ into the regression equation to obtain $\hat{\Delta T}(t+1)$.</p>

<p>Is there any R package, probably time series-related, that will do these calculations automatically?</p>

<p>Also, if there are any issues with this approach, I'd be happy to know about them.</p>
"
"0.0570990591522943","0.0697863157798853","120201","<p>Being aware of <a href=""http://www.ssicentral.com/lisrel/techdocs/HowLargeCanaStandardizedCoefficientbe.pdf"" rel=""nofollow"">that article</a>, I am curious about the question how big standardized coefficients can get. I had a discussion with my professor about that issue and she was arguing standardized coefficients (beta) in multiple linear regressions can not become greater than |1|. I have also heard that predictors with standardized coefficients greater than 1 should not be be included/appear in multiple linear regression. When I recently estimated a multiple linear regression in R using lm(), I estimated the standardized coefficients with lm.beta() function from the package 'lm.beta'. In the results I could observe a standardized coefficient greater than one. Right now I am just not sure about what is the truth.</p>

<p>Can standardized coefficients become greater than |1|?
If yes, what does that mean and should they be excluded from the model?
If yes, why?</p>

<p>I would be very thankful, if somebody could make this issue clear for me. </p>

<p>Thanks in advance!!</p>
"
"0.0419590679148345","0.056980288229819","120749","<p>I'm working on a biological question, with species data derived from an external database, which has multiple response and predictor variables. As a result, I want to do multivariate regression across a phylogeny to empirically test if my response variables are significantly different in respect to my predictors.</p>

<p>Please refer to source [2] and it's citations for your own investigation of this process.</p>

<p>I know how to do multiple regression via pGLS, but the R package [1] only mentions predictors and response. Furthermore, another source [2] discusses how multivariate regression though pGLS in R can be done, but requires one to transform the data under a Brownian motion model. (Edit: It seems that [2] is a solution...so I'm looking the process).</p>

<p>Sources:</p>

<ol>
<li><p>The vignette for the pGLS package (<a href=""http://cran.r-project.org/web/packages/pGLS/pGLS.pdf"" rel=""nofollow"">pdf</a>)</p></li>
<li><p>D.C. Adams. 2014. A Method for Assessing Phylogenetic Least Squares Models for Shape and Other High-Dimensional Multivariate Data. Evolution. 68:9 2675-2688. doi: 10.1111/evo.12463</p></li>
<li><p>Revell, L. J. (2010), Phylogenetic signal and linear regression on species data. Methods in Ecology and Evolution, 1: 319â€“329. doi: 10.1111/j.2041-210X.2010.00044.x</p></li>
</ol>
"
"0.0395593886064618","0.040291148201269","121131","<p>I'm new to R and I've been searching for a while for a function which can reduce the number of explanatory variables in my lda function (linear discriminant analysis).</p>

<p>Basically, I've loaded the dataset and ran the lda function on my binomial dependent variable explained by 30 independent variables. (received a warning that the independent variables are collinear).</p>

<p>My professor has shown us stepwise feature selection (leaps package, regsubsets function) in a regression framework, but these codes aren't compatible for LDA/QDA.</p>

<p>Thanks in advance,
Marvin</p>
"
"0.0395593886064618","0.040291148201269","121159","<p>I have a regression problem where Iâ€™m attempting to train a data set with 70 predictors, but only 35 observations with glmnet in the caret package. Iâ€™m trying to determine the best resampling method.  It seems like 35 observations is too few to use K-Fold Cross Validation. Would bootstrapping be a better method? What we would be the ideal number of iterations for either method? My biggest concern is overfitting. Is there some general guideline to handle these type of data sets?</p>

<p>Are there better models for these type data of sets?  Random Forests or Boosting come to mind.</p>
"
"0.027972711943223","0.0284901441149095","121480","<p>I'm following the Caret package tutorial for constructing customized functions for a <a href=""http://topepo.github.io/caret/featureselection.html"" rel=""nofollow"">recursive feature elimination</a>. I can reproduce the provided example which is a random forest regression. However, when I modify the code to deal with classification, I receive an odd error:</p>

<pre><code>library(caret)
library(mlbench)
library(Hmisc)
library(randomForest)

n &lt;- 100
p &lt;- 40
sigma &lt;- 1
set.seed(1)
sim &lt;- mlbench.friedman1(n, sd = sigma)
colnames(sim$x) &lt;- c(paste(""real"", 1:5, sep = """"),
                         paste(""bogus"", 1:5, sep = """"))
    bogus &lt;- matrix(rnorm(n * p), nrow = n)
    colnames(bogus) &lt;- paste(""bogus"", 5+(1:ncol(bogus)), sep = """")
    x &lt;- cbind(sim$x, bogus)
y &lt;- sim$y
#customizing tutorial example for binary outcome

y[y &lt;= 12] &lt;- 0    
y[y&gt; 12] &lt;- 1

y &lt;- factor(y)


normalization &lt;- preProcess(x)
x &lt;- predict(normalization, x)
x &lt;- as.data.frame(x)
subsets &lt;- c(1:5, 10, 15, 20, 25)
rfRFE &lt;-  list(summary = defaultSummary,
                      fit = function(x, y, first, last, ...){
             library(randomForest)
             randomForest(x, y, importance = first, ...)
             },
           pred = function(object, x)  predict(object, x),
           rank = function(object, x, y) {
             vimp &lt;- varImp(object)
             vimp &lt;- vimp[order(vimp$Overall,decreasing = TRUE),,drop = FALSE]
                 vimp$var &lt;- rownames(vimp)
             vimp
             },
           selectSize = pickSizeBest,
           selectVar = pickVars)

ctrl &lt;- rfeControl(functions = lmFuncs,
                   method = ""repeatedcv"",
                   repeats = 5,
                   verbose = FALSE)
ctrl$functions &lt;- rfRFE
    ctrl$returnResamp &lt;- ""all""
set.seed(10)
rfProfile &lt;- rfe(x, y, sizes = subsets, rfeControl = ctrl)
rfProfile
</code></pre>

<p>The error is:</p>

<pre><code>Error in {: task 1 failed - ""argument 1 is not a vector""
</code></pre>

<p>My question is how should one go about defining <code>rfRFE</code> for random forest models with binary response variables?</p>
"
"0.0843408998948762","0.0944911182523068","122039","<p><strong>SOLVED</strong>: an elastic net model, as any other logistic regression model, will not generate more coefficients than input variables. Check Zach's answer to understand how from an (apparent) low number of inputs, more coefficients can be generated. The cause of this question was a code bug, as the users pointed out.</p>

<p>This is a simple question. I've fitted a model with 1334 variables using elastic net to perform feature selection and regularization. I'm now trying to interpret the obtained coefficients in order to find correlations between the input variables and the output. The only problem is that instead of the (expected) 1335 coefficients (intercept+1334), extracting the coefficients through <code>coef(model,s=""lambda.min"")</code> yields around 1390 coefficients. This seems highly counterintuitive and stops me from mapping a single coefficient to a single input variable, so I suppose I'm not understanding some of the insides of the elastic net. Any idea would be very helpful. Thanks in advance.</p>

<p>PS: just in case someone wonders so, I've not included interaction terms nor any synthetic variable, just the original 1334 ones.</p>

<p>PS2: elastic net references:</p>

<ul>
<li>Mathematical paper: <a href=""http://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&amp;%20Hastie.pdf"" rel=""nofollow"">http://web.stanford.edu/~hastie/Papers/B67.2%20(2005)%20301-320%20Zou%20&amp;%20Hastie.pdf</a></li>
<li>R package tutorial: <a href=""http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html"" rel=""nofollow"">http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html</a></li>
</ul>

<p>PS3: about the code used to fit the model:</p>

<p>it is a 250 line script, so unless you specifically need it, I think it'd only clutter the question. Basically, the algorithm takes as an input a data frame of 1393 colums, where the last one is the target variable and the first 1392 are the input variables. So, after separating those into two matrices, input and output, the actual model fitting is done in this call:</p>

<p><code>cv.glmnet(x=input_matrix,y=output_matrix,family=""binomial"",type.measure=""auc"")</code></p>

<p>If you need to, I can actually generate a reproducible file with the data I use and the whole script. </p>
"
"NaN","NaN","122165","<p>I have a basic question about quantile regression (I'm new to it):
Why doesn't it seem possible to do a quantile regression with a specified family (e.g. gamma) and link function (e.g. log), as in a glm? Or if it is possible, is there an R package that can do this? I've looked over the internet but no such thing seems to exist and I can't figure out why.</p>
"
"0.0971882825368556","0.106600358177805","122212","<p>I have been developing a logistic regression model based on retrospective data from a national trauma database of head injury in the UK. The key outcome is 30 day mortality (denoted as ""Survive"" measure). Other measures with published evidence of significant effect on outcome in previous studies include:</p>

<pre><code>Year - Year of procedure = 1994-2013
Age - Age of patient = 16.0-101.5
ISS - Injury Severity Score = 0-75
Sex - Gender of patient = Male or Female
inctoCran - Time from head injury to craniotomy in minutes = 0-2880 (After 2880 minutes is defined as a separate diagnosis)
</code></pre>

<p>Using these models, given the dichotomous dependent variable, I have built a logistic regression using lrm.</p>

<p>The method of model variable selection was based on existing clinical literature modelling the same diagnosis. All have been modelled with a linear fit with the exception of ISS which has been modelled traditionally through fractional polynomials. No publication has identified known significant interactions between the above variables.</p>

<p>Following advice from Frank Harrell, I have proceeded with the use of regression splines to model ISS (there are advantages to this approach highlighted in the comments below). The model was thus pre-specified as follows:</p>

<pre><code>rcs.ASDH&lt;-lrm(formula = Survive ~ Age + GCS + rcs(ISS) +
    Year + inctoCran + oth, data = ASDH_Paper1.1, x=TRUE, y=TRUE)
</code></pre>

<p>Results of the model were:</p>

<pre><code>&gt; rcs.ASDH

Logistic Regression Model

lrm(formula = Survive ~ Age + GCS + rcs(ISS) + Year + inctoCran + 
    oth, data = ASDH_Paper1.1, x = TRUE, y = TRUE)

                      Model Likelihood     Discrimination    Rank Discrim.    
                         Ratio Test            Indexes          Indexes       
Obs          2135    LR chi2     342.48    R2       0.211    C       0.743    
 0            629    d.f.             8    g        1.195    Dxy     0.486    
 1           1506    Pr(&gt; chi2) &lt;0.0001    gr       3.303    gamma   0.487    
max |deriv| 5e-05                          gp       0.202    tau-a   0.202    
                                           Brier    0.176                     

          Coef     S.E.    Wald Z Pr(&gt;|Z|)
Intercept -62.1040 18.8611 -3.29  0.0010  
Age        -0.0266  0.0030 -8.83  &lt;0.0001 
GCS         0.1423  0.0135 10.56  &lt;0.0001 
ISS        -0.2125  0.0393 -5.40  &lt;0.0001 
ISS'        0.3706  0.1948  1.90  0.0572  
ISS''      -0.9544  0.7409 -1.29  0.1976  
Year        0.0339  0.0094  3.60  0.0003  
inctoCran   0.0003  0.0001  2.78  0.0054  
oth=1       0.3577  0.2009  1.78  0.0750  
</code></pre>

<p>I then used the calibrate function in the rms package in order to assess accuracy of the predictions from the model. The following results were obtained:</p>

<pre><code>plot(calibrate(rcs.ASDH, B=1000), main=""rcs.ASDH"")
</code></pre>

<p><img src=""http://i.stack.imgur.com/HYTsp.png"" alt=""Bootstrap calibration curves penalized for overfitting""></p>

<p>Following completion of the model design, I created the following graph to demonstrate the effect of the Year of incident on survival, basing values of the median in continuous variables and the mode in categorical variables:</p>

<pre><code>ASDH &lt;- Predict(rcs.ASDH, Year=seq(1994,2013,by=1),Age=48.7,ISS=25,inctoCran=356,Other=0,GCS=8,Sex=""Male"",neuroYN=1,neuroFirst=1)
Probabilities &lt;- data.frame(cbind(ASDH$yhat,exp(ASDH$yhat)/(1+exp(ASDH$yhat)),exp(ASDH$lower)/(1+exp(ASDH$lower)),exp(ASDH$upper)/(1+exp(ASDH$upper))))
names(Probabilities) &lt;- c(""yhat"",""p.yhat"",""p.lower"",""p.upper"")
ASDH&lt;-merge(ASDH,Probabilities,by=""yhat"")
plot(ASDH$Year,ASDH$p.yhat,xlab=""Year"",ylab=""Probability of Survival"",main=""30 Day Outcome Following Craniotomy for Acute SDH by Year"", ylim=range(c(ASDH$p.lower,ASDH$p.upper)),pch=19)
arrows(ASDH$Year,ASDH$p.lower,ASDH$Year,ASDH$p.upper,length=0.05,angle=90,code=3)
</code></pre>

<p>The code above resulted in the following output:</p>

<p><img src=""http://i.stack.imgur.com/KGYcz.png"" alt=""Year trend with lower and upper""></p>

<p><strong><em>My remaining questions are the following:</em></strong></p>

<p><strong>1. Spline Interpretation</strong> - How can I calculate the p-value for the splines combined for the overall variable?</p>
"
"0.165552678563412","0.163931279423618","122270","<p>This is a long post but it is <em>not</em> conceptually difficult. Please bear with me.</p>

<p>I am trying to model the <strong>seasonality</strong> of production volume of an agricultural commodity. I do not care about the structure of the model (as long as it produces sensible output) nor about explaining seasonality; my goal is to adjust the series for seasonality before proceeding to forecasting. </p>

<p>I know the seasonality is due to supply (weather is a key determinant) as well as demand (higher demand around Christmas and Easter, etc.). </p>

<p>My <strong>data is weekly</strong>, and I am following the approach by prof. Hyndman of modelling the time series as an <strong>ARMA process with Fourier terms</strong> to account for seasonality (see <a href=""http://robjhyndman.com/hyndsight/longseasonality/"" rel=""nofollow"">here</a>). I also include Christmas and Easter dummies together with Fourier terms. </p>

<p>You might think I do not need a Christmas dummy since Christmas is always the 25th of December and thus Fourier terms would account for it, unlike for Easter which jumps back and forth in March-April from year to year. However, Christmas sometimes occur in week 51 and sometimes in week 52 (at least in my calendar), and Fourier terms will not account for that; hence a Christmas dummy is actually relevant.</p>

<p>Actually, I would like to include up to four dummies for Christmas (from two weeks before Christmas to one week after Christmas) as the seasonal effect is not concentrated on just the Christmas week but also spread out a little in time. The same is with Easter.</p>

<p>Thus I end up with $4+4=8$ dummies, up to 25 pairs of Fourier terms and an unknown ARMA order, which could be as high as (5,5), perhaps. This gives me an <strong>awfully large pool of models to choose from</strong>. Thus I have trouble replicating the approach used in the source above because of <strong>limited computational resources</strong>. Recall that in the original source there were no dummies, so the model pool was $2^{4+4}=256$ times smaller than mine and it was possible to try out a reasonably large proportion of all possible models, and then choose one of them by AIC. </p>

<p>Even worse, I would like to repeat my exercise multiple times (100 times) using a rolling window on the data I have. Thus the <strong>computational requirements</strong> for my exercise <strong>become exorbitant</strong>. I am looking for a solution, and hope someone could help me out.</p>

<p>Here are a few ideas I have:</p>

<ol>
<li><p>Drop the ARMA terms all together. </p>

<ul>
<li>This will make the estimates of coefficients on Fourier terms and dummies biased and inconsistent, thus the seasonal component (the Fourier terms times their coefficients plus dummies times their coefficients) might be rubbish. However, if ARMA patterns are pretty weak, maybe the bias and inconsistency of the coefficient estimates will not be too bad?</li>
</ul></li>
<li><p>Estimate only some models from the pool of all possible models. E.g. select the ARMA order by <code>auto.arima()</code> function in <code>forecast</code> package in <code>R</code>: <code>auto.arima()</code> does not consider all possible ARMA orders but only non-restricted ones (e.g. an AR(2) model with the AR(1) coefficient restricted to zero is not considered) and takes some other shortcuts. Also, I could force all dummies to be present without questioning whether all of them are really relevant. </p>

<ul>
<li>This indeed reduces the model pool considerably (by a factor of $2^{4+4}=256$ only due to dummies), but it comes at a cost of likely inclusion of irrelevant variables. Not sure how bad that could be. </li>
</ul></li>
<li><p>Use a different estimation method: conditional sum of squares (CSS) instead of maximum likelihood (ML) when possible, something else? </p>

<ul>
<li>Unfortunately, CSS in place of ML does not seem to reduce the computational time enough in practice (I tested a few examples).</li>
</ul></li>
<li><p>Do the estimation in two (or more) steps: select the ARMA order independently of the number of Fourier terms and inclusion/exclusion of dummies, then select the number of Fourier terms, then select the number of dummies, or something similar. </p>

<ul>
<li>This would reduce the model pool to the following: the number of all possible ARMA models (e.g. $2^{5+5}$) plus 26 (0 pairs of Fourier terms, 1 pair of Fourier terms, ..., 25 pairs of Fourier terms) plus $2^{4+4}$ (all combinations of dummies). This particular division into stages certainly does not make sense, I admit. But perhaps there exists a smart way to divide model selection into steps? I guess we need mutual orthogonality of regressors to be able to consider their inclusion/exclusion from the model one by one, or something similar. I need some insight here...</li>
</ul></li>
<li><p>Do something along the lines of Cochrane-Orcutt correction: <strong>(1)</strong> estimate a model without any ARMA terms (thus a simple OLS regression of production volume on Fourier terms and dummies); <strong>(2)</strong> examine the ARMA patterns in residuals and determine the relevant ARMA order; <strong>(3)</strong> estimate the corresponding ARMA model with Fourier terms and dummies as exogenous regressors. <br>(I know Cochrane-Orcutt does not consider ARMA(p,q) but just AR(1), but I thought maybe I could generalize it? Not sure, perhaps I am making a mistake here?)</p>

<ul>
<li>This would reduce the model pool to be estimated by a factor of $2^{5+5}$ if we would otherwise consider ARMA(5,5) and all sub-models nested in it, which is very nice. However, Cochrane-Orcutt correction is not as good as getting the model order correct in the first place. (I lack a theoretical argument there, but I suspect you do not always end up with the correct ARMA order if you use Cochrane-Orcutt or something similar.)</li>
</ul></li>
</ol>

<p>I would appreaciate any comments on the ideas above and, even more, your own ideas on how to save computational time.</p>

<p><strong>Thank you!</strong></p>

<p>P.S. Here is the data (2007 week 1 to 2012 week 52): <br><code>6809  8281  8553  8179  8257  7426  8098  8584  8942  9049  9449 10099 10544  6351  8667  9395  9062  8602 10500  8723  9919  9120 10068  9937  9998  9379  9432  8751 10250  7690  6857  8199  8299  9204 11755  9861 10193 10452  9769 10289 10399 10181 10419 10324 11347 11692 12571 13049 13712 15381 13180  3704  5833  9909  8979  8883  9599  9034  9617  9710  9878 10872 12246  7122  8071  9656  9157  9729  8814  8704 10724  9294 10176 10394  9633 10872 10353  9400  9692  9244  8921  7698  8191  8494  8681  9420  8947  9287 10043  9953 10136 10235 10027 10137 11187 10383 11321 12023 11824 13752 13907 14658 14634  5353  5603 11607  9512  9537  9775  8558  9227  9146  9673 10225 10794 10646 10545 12375  7742  9269 11134  9588 10890 10061 10244 11275  9428 10301 11039 10779 10580  9215  9538  9717  9966 10678 10821 10898 11458 11090 12080 10904 12990 12325 12257 12246 12351 12408 11732 12702 14181 14963 15027 16657 17766  8265  7552 11067 10808 10891 10967 10826 10929 10891  9293 10902 11640 11921 13320  7751  9929 11632 11073 10955 11785 10533 10629 11535 12513 11780 12639 12280 12245 11817 11275 10242 10625  9884 10182 11048 12120 12697 13132 12597 13516 13536 13279 13935 13539 14335 13748 13988 14497 15255 15346 17626 17716 10713  7752  9309 11133 11853 11570 10320 10817 10982 10585 10941 12801 12165 11899 10941 12729 13763  8130  9888 11889 13036 10413 12609 10992 13201 11833 13159 12880 12160 13277 12758 10897 11197 12169 12639 13788 13819 14804 14613 14992 16079 16648 16376 16207 15906 15301 16194 17327 18890 18710 19953 22369 16499  9721 11785 12528 13994 12990 13226 12883 13613 14382 15829 15525 16395 17175 18334  9286 15530 16510 15370 13019 16526 14425 16542 15328 16993 17607 16734 15826 16555 16064 14412 14355 15238 15064 15317 15533 16542 16474 17354 18359 18459 17312 17499 18298 17301 17795 17867 18453 19372 19394 21897 23674 23219  7356</code></p>
"
"0.0197796943032309","0.040291148201269","122387","<p>I am using the segmented package to run a piecewise linear regression.  My regression model is called fmod, and I use: </p>

<pre><code>fmod$psi[1]
</code></pre>

<p>to extract the first breakpoint from the summary. To view the slopes of each piece of the regression, I use:</p>

<pre><code>slope(fmod)
</code></pre>

<p>and to view the intercepts of each piece, I use:</p>

<pre><code>intercept(fmod)
</code></pre>

<p>However, I am unable to extract a single slope estimate or intercept estimate from the output. Does anyone know how to do this?</p>
"
"0.0197796943032309","0.040291148201269","122935","<p>I have a design involving 1 between-subjects categorical factor, 1 between-subjects continuous factor and 2 within-subjects categorical factors.</p>

<p>This is theoretically a case of multiple regression, but as in this question <a href=""http://stats.stackexchange.com/questions/48455/anova-or-regression-1-continuous-factor-1-categorical-factor-with-continuous"">here</a>, I was able to use <code>lm()</code> in <code>R</code> to specify my model (actually, using the <code>aov.car()</code> function in the <code>afex</code> package).</p>

<p>This means that my output was in the form of an ANOVA table, with F and p-values. Can I report my analysis as though it was an ANOVA, or should I find a way to force the output of regression coefficients and report the analysis as though it were a multiple regression? I know that I shouldn't use an ANCOVA in this case, because my continuous predictor variable is certainly not a nuisance variable!</p>
"
"0.0930988128231456","0.0948209311861521","123012","<p>I'm using <code>glmboost</code> in the mboost package to fit a boosted regression using linear models as the base learner. There are 13200 observations and about 75 variables, and I want to get a measure of the importance of each variable.</p>

<p>At the moment, I'm exploring the following two options:</p>

<ol>
<li><p>The boosted model object returned by <code>glmboost</code> includes information on the selection probabilities of the variables, ie how frequently they are selected by the boosting algorithm.</p></li>
<li><p>I can use <code>stabsel</code>, from the package of the same name, to identify the important variables. This uses a resampling approach to perturb the data, and the output is the frequency with which each variable appears in the resampled models (if I understand correctly).</p></li>
</ol>

<p>The problem is that these two methods are giving radically different results. This is the output from 1:</p>

<pre><code>&gt; summary(php.glmb)

         Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = p_hp ~ ., data = hdata.php.trn)

....

Selection frequencies:
degc238   etc48    etc7   per45   bar25   bar43    etc8   etc60   bar33   bar59
   0.28    0.17    0.07    0.06    0.05    0.05    0.05    0.05    0.04    0.03
  per15   per60   degc1   etc65   bar67 degc209    per5    per23   etc70 
   0.03    0.03    0.02    0.02    0.01    0.01    0.01     0.01    0.01 
</code></pre>

<p>And this is the output from 2:</p>

<pre><code>&gt; stabsel(php.glmb, cutoff=0.75, q=10)
        Stability Selection with unimodality assumption

Selected base-learners:
degc1  per5 per15 per45 per60  etc8 etc60 etc65 etc70 
   20    43    44    52    54    60    72    74    76 

Selection probabilities:
(Intercept)       bar25       bar26       bar28       bar29       bar32       bar42 
       0.00        0.00        0.00        0.00        0.00        0.00        0.00 
      bar43       bar45       bar46       bar49       bar50       bar59       bar60 
       0.00        0.00        0.00        0.00        0.00        0.00        0.00 
      bar62       bar63       bar66       degc2       degc3      degc16     degc109 
       0.00        0.00        0.00        0.00        0.00        0.00        0.00 
    degc111     degc147     degc154     degc155     degc158     degc181     degc183 
       0.00        0.00        0.00        0.00        0.00        0.00        0.00 
    degc204     degc205     degc206     degc209     degc229     degc231     degc238 
       0.00        0.00        0.00        0.00        0.00        0.00        0.00 
    degc255     degc256     degc257     degc260       per21       per24       per34 
       0.00        0.00        0.00        0.00        0.00        0.00        0.00 
      per40       per42       per43       per58       per61       per63        etc5 
       0.00        0.00        0.00        0.00        0.00        0.00        0.00 
       etc6        etc7       etc11       etc15       etc26       etc30       etc32 
       0.00        0.00        0.00        0.00        0.00        0.00        0.00 
      etc33       etc41       etc45       etc47       etc48       etc59       etc64 
       0.00        0.00        0.00        0.00        0.00        0.00        0.00 
      etc69       bar67       bar33       per23       per60       degc1        per5 
       0.00        0.15        0.18        0.71        0.96        1.00        1.00 
      per15       per45        etc8       etc60       etc65       etc70 
       1.00        1.00        1.00        1.00        1.00        1.00 
</code></pre>

<p>So the important variables are almost completely disjoint: method 1 says <code>degc238</code>, <code>etc48</code>, <code>etc7</code> and <code>per45</code> are the most important (have the highest selection probabilities), while method 2 says <code>etc70</code>, <code>etc65</code>, <code>etc60</code>, <code>etc8</code>, <code>per60</code> and so on. </p>

<p>What can be the reason for this? I should also mention that there's a significant amount of collinearity in this dataset; several predictors have univariate correlations of 90%+ with the response and with each other. Could this be impacting the result?</p>
"
"0.100857047225074","0.102722675451665","123498","<p>I would like to generate a confidence interval for predicted vs actual rates.</p>

<p>I am auditing my group of anaesthetists (aka anesthesiologists) to see how we compare on a number of potentially preventable complications (eg post-operative nausea, severe pain, hypothermia).</p>

<p>I have 20000 surgical operation records and I can make a GLM to make a ""case-mix adjusted risk"" (using age, gender, type of surgery, duration of surgery as risk factors) and thus <a href=""http://www.r-tutor.com/elementary-statistics/logistic-regression/estimated-logistic-regression-equation"" rel=""nofollow"">generate a risk</a> for each patient.</p>

<p>I can then aggregate the risk and actual per clinician I can generate an actual and predicted rate for each complication. I can make a confidence interval for my actual - but it seems a bit simplistic to just test to see if the confidence interval on the actual rate includes the rate generated from summing the glm-predicted risks.</p>

<p><a href=""http://stats.stackexchange.com/questions/7344/how-to-graphically-compare-predicted-and-actual-values-from-multivariate-regress"">This question</a> has some pointers to a package but I am hoping for some more specific suggestions.</p>

<p>To clarify what I have already (using ""requirement for pain protocol"" as an example):</p>

<pre><code># make model (dependent variable has values 1/0)
model.pp = glm(
pain_protocol1 ~
age + log_age + age2 + inv_age
+ op_time + log_op_time + op_time2
+ gender
+ category
+ thimble,
family = ""binomial"",
data=d4)
# calculate predicted PACU time and then difference between predicted and actual:
d4$pred_pp = predict(model.pp, newdata=d4, type=""response"", na.action=""na.pass"")

d4$extra_pp = d4$pain_protocol1 - d4$pred_pp
# aggregate deviation from predicted rate
ppr_pa &lt;- aggregate(extra_pp ~ adult_anaesthetist, data=d4, FUN=mean)
barplot(ppr_pa$extra_pp, name=ppr_pa$adult_anaesthetist,las=2) 
</code></pre>

<p>So I can make this plot for my colleagues, showing the variation we have in how much pain our patients experience in the ""post anaesthesia care unit"". These variations are great enough so that they definitely represent a material difference in patient experience, and most of the difference will also be unlikely to be variation due to chance (ie ""statistically significant""). However, as I examine smaller subgroups and other complications that are less frequent it would be good to be able to calculate confidence intervals.</p>

<p><img src=""http://i.stack.imgur.com/FFe8W.png"" alt=""bar plot illustrating difference between predicted and actual rates of &quot;needing pain protocol&quot;""></p>

<p>Note that each clinician has a different number of cases, and each clinician is given a bird code-name for anonymity.</p>
"
"NaN","NaN","124373","<p>I'm using the earth package (using caret train function) MARS spline implementation in order to perform non - linear regression modeling. I would like to obtain a measure of prediction uncertainty (not only the expected value). Is there any way to obtain it? Thanks in advance for any help.</p>
"
"0.0745938985152613","0.0854704323447285","124512","<p>I have been developing a logistic regression model based on retrospective data from a national trauma database of head injury in the UK. The key outcome is 30 day mortality (denoted as ""Survive"" measure). Other measures across the whole database with published evidence of significant effect on outcome in previous studies include:</p>

<pre><code>Year - Year of procedure = 1994-2013
Age - Age of patient = 16.0-101.5
ISS - Injury Severity Score = 1-75
GCS - Glasgow Coma Scale = 3-15
Sex - Gender of patient = Male or Female
inctoCran - Time from head injury to craniotomy in minutes = 0-2880 (After 2880 minutes is defined as a separate diagnosis)
</code></pre>

<p>There are two additional variables where the data is only available from 2004:</p>

<pre><code>Pupil reactivity (1 is brisk, 2 is sluggish and 3 is unreactive)
Pupil size (continuous variable in mm - 1-10)
</code></pre>

<p>Based on literature these are significant predictors of outcome. However, out of a series of 2140, I am missing 936. Secondly, the measure is not missing at random, having only been collected in recent years.</p>

<p>My questions are the following in order to address the year range 1994-2013:</p>

<p>1) My data is heavily skewed to later years; how can I adapt the logistic regression to reduce the effect of this when assessing the effect of the year of procedure on outcome?</p>

<p>2) Can I exclude pupil reactivity since it was not collected before 2004 in performing this analysis even if it is a strong predictor?</p>

<p>3) If I should include pupil reactivity, can a multivariate regression be built with the variables above with which to perform imputation to create data for 1994-2003 given 43% of the data is missing?</p>

<p>4) If not possible, could imputation be performed based on data since 2009 where ~15% is missing?</p>

<p>I perform all statistical analyses exclusively with R and would be grateful if you could add known packages/formulae to execute your suggestions.</p>
"
"0.0197796943032309","0.040291148201269","124658","<p>There is an R package called BayesTree which can fit regression trees in Bayesian environment. However, this way only simple regression is possible. I would like to use regression trees as a part of a bigger hierarchical model instead of the simple GLM formulas (CARTs are proven to give better results in certain applications, see e.g. <a href=""http://dx.plos.org/10.1371/journal.pone.0023903"" rel=""nofollow"">Hu et al 2011</a>).</p>

<p>Is it possible (and how) to fit regression trees as a part of the model in WinBUGS/OpenBUGS/JAGS? Are there any such packages for these pieces of software?</p>
"
"0.027972711943223","0.0284901441149095","125211","<p>I am new to machine learning/statistical modelling.</p>

<p>I am trying to run a classification on a highly sparse dataset with 100 features, most of which are categorical (TRUE/FALSE) with the remaining values missing. To handle missing values, I filled the missing spots with the text 'Nothing', thereby creating a new level.</p>

<p>Next, I am trying to run a logistic regression using a penalty (glmnet package). When I check the coefficients, I see dummy variables corresponding to 'Nothing' having the higher coefficients.</p>

<p>How should I remove these coefficients? What would be a better approach to this?</p>

<p>Or should I just use trees? Please suggest the best way forward.</p>

<p>Thanks!</p>
"
"0.142731332931499","0.145371541135707","125453","<p>I have used the â€˜polrâ€™ function in the MASS package to run an ordinal logistic regression for an ordinal categorical response variable with 15 continuous explanatory variables.</p>

<p>I have used the code (shown below) to check that my model meets the proportional odds assumption following advice provided in <a href=""http://www.ats.ucla.edu/stat/r/dae/ologit.htm"">UCLA's guide</a>. However, Iâ€™m a little worried about the output implying that not only are the coefficients across various cutpoints similar, but they are exactly the same (see graphic below). </p>

<pre><code>FGV1b &lt;- data.frame(FG1_val_cat=factor(FGV1b[,""FG1_val_cat""]), 
                    scale(FGV1[,c(""X"",""Y"",""Slope"",""Ele"",""Aspect"",""Prox_to_for_FG"", 
                          ""Prox_to_for_mL"", ""Prox_to_nat_border"", ""Prox_to_village"", 
                          ""Prox_to_roads"", ""Prox_to_rivers"", ""Prox_to_waterFG"", 
                          ""Prox_to_watermL"", ""Prox_to_core"", ""Prox_to_NR"", ""PCA1"", 
                          ""PCA2"", ""PCA3"")]))
b     &lt;- polr(FG1_val_cat ~ X + Y + Slope + Ele + Aspect + Prox_to_for_FG + 
                            Prox_to_for_mL + Prox_to_nat_border + Prox_to_village + 
                            Prox_to_roads + Prox_to_rivers + Prox_to_waterFG + 
                            Prox_to_watermL + Prox_to_core + Prox_to_NR, 
              data=FGV1b, Hess=TRUE)
</code></pre>

<p>View a summary of the model:</p>

<pre><code>summary(b)
(ctableb &lt;- coef(summary(b)))
q        &lt;- pnorm(abs(ctableb[, ""t value""]), lower.tail=FALSE) * 2
(ctableb &lt;- cbind(ctableb, ""p value""=q))
</code></pre>

<p>And now we can look at the confidence intervals for the parameter estimates:</p>

<pre><code>(cib &lt;- confint(b)) 
confint.default(b)
</code></pre>

<p>But these results are still quite hard to interpret, so let's convert the coefficients into odds ratios</p>

<pre><code>exp(cbind(OR=coef(b), cib))
</code></pre>

<p>Checking the assumption. So the following code will estimate the values to be graphed. First it shows us the logit transformations of the probabilities of being greater than or equal to each value of the target variable</p>

<pre><code>FG1_val_cat &lt;- as.numeric(FG1_val_cat)
sf &lt;- function(y) {
  c('VC&gt;=1' = qlogis(mean(FG1_val_cat &gt;= 1)),
    'VC&gt;=2' = qlogis(mean(FG1_val_cat &gt;= 2)),
    'VC&gt;=3' = qlogis(mean(FG1_val_cat &gt;= 3)),
    'VC&gt;=4' = qlogis(mean(FG1_val_cat &gt;= 4)),
    'VC&gt;=5' = qlogis(mean(FG1_val_cat &gt;= 5)),
    'VC&gt;=6' = qlogis(mean(FG1_val_cat &gt;= 6)),
    'VC&gt;=7' = qlogis(mean(FG1_val_cat &gt;= 7)),
    'VC&gt;=8' = qlogis(mean(FG1_val_cat &gt;= 8)))
}
(t &lt;- with(FGV1b, summary(as.numeric(FG1_val_cat) ~ X + Y + Slope + Ele + Aspect + 
                             Prox_to_for_FG + Prox_to_for_mL + Prox_to_nat_border + 
                             Prox_to_village + Prox_to_roads + Prox_to_rivers + 
                             Prox_to_waterFG + Prox_to_watermL + Prox_to_core + 
                             Prox_to_NR, fun=sf)))
</code></pre>

<p>The table above displays the (linear) predicted values we would get if we regressed our dependent variable on our predictor variables one at a time, without the parallel slopes assumption. So now, we can run a series of binary logistic regressions with varying cutpoints on the dependent variable to check the equality of coefficients across cutpoints</p>

<pre><code>par(mfrow=c(1,1))
plot(t, which=1:8, pch=1:8, xlab='logit', main=' ', xlim=range(s[,7:8]))
</code></pre>

<p><img src=""http://i.stack.imgur.com/4Uicq.jpg"" alt=""polr assumption check""></p>

<p>Apologies that I am no statistics expert and perhaps I am missing something obvious here. However, I have spent a long time trying to figure out if there is a problem in how I tested the model assumption and also trying to figure out other ways to run the same kind of model. </p>

<p>For example, I read in many help mailing lists that others use the vglm function (in the VGAM package) and the lrm function (in the rms package) (for example see here:  <a href=""http://stats.stackexchange.com/questions/25988/proportional-odds-assumption-in-ordinal-logistic-regression-in-r-with-the-packag"">Proportional odds assumption in ordinal logistic regression in R with the packages VGAM and rms</a>). I have tried to run the same models but am continuously coming up against warnings and errors.</p>

<p>For example, when I try to fit the vglm model with the â€˜parallel=FALSEâ€™ argument (as the previous link mentions is important for testing the proportional odds assumption), I encounter the following error:</p>

<blockquote>
  <p>Error in lm.fit(X.vlm, y = z.vlm, ...) : NA/NaN/Inf in 'y'<br>
  In addition: Warning message:<br>
  In Deviance.categorical.data.vgam(mu = mu, y = y, w = w, residuals = residuals,  :
    fitted values close to 0 or 1</p>
</blockquote>

<p>I would like to ask please if there is anyone who might understand and be able to explain to me why the graph I produced above looks as it does. If indeed it means that something isnâ€™t right, could you please help me find a way to test the proportional odds assumption when just using the polr function. Or if that is just not possible, then I will resort to trying to use the vglm function, but would then need some help to explain why I keep getting the error given above.</p>

<p>NOTE: As a background, there are 1000 datapoints here, which are actually location points across a study area. I am looking to see if there are any relationships between the categorical response variable and these 15 explanatory variables. All of those 15 explanatory variables are spatial characteristics (for example, elevation, x-y coordinates, proximity to forest etc.). The 1000 datapoints were randomly allocated using a GIS, but I took a stratified sampling approach. I made sure that 125 points were randomly chosen within each of the 8 different categorical response levels. I hope this information is also helpful.</p>
"
"0.0395593886064618","0.040291148201269","125455","<p>I want to meta-analyze the interaction effect of a 2x2 ANOVA.</p>

<p>(I am <em>not</em> talking about an interaction in the meta-regression, as in <a href=""http://stats.stackexchange.com/questions/71404/main-effects-and-interaction-in-multivariate-meta-analysis-network-meta-analysi"">this question</a> but about an interaction as the focal effect that should be meta-analytically summarized).</p>

<p><strong>What is the best way to code the interaction effect size for a subsequent meta-analysis?</strong>
(preferably in the <code>metafor</code> package)</p>
"
"0.0625488854200668","0.0637058989297032","125465","<p>I'm searching for a built-in function in R for calculating the required sample size (given certain power, alpha,..) of a mixed ANOVA (2 between, 1 within variable, 2x2x2 design). Does this function exist? I didn't find it in the following packages:</p>

<p>â€¢pwr is the oldest power-analysis library; some introductory info can be found on Quick-R</p>

<p>â€¢PoweR: Computation of power and level tables for hypothesis tests</p>

<p>â€¢Power2Stage: Power and Sample size distribution of 2-stage BE studies via simulations</p>

<p>â€¢powerAnalysis: Power analysis in experimental design</p>

<p>â€¢powerGWASinteraction: Power Calculations for Interactions for GWAS</p>

<p>â€¢powerMediation: Power/Sample size calculation for mediation analysis, simple linear 
regression, logistic regression, or longitudinal study</p>

<p>â€¢powerpkg: Power analyses for the affected sib pair and the TDT design</p>

<p>â€¢powerSurvEpi: Power and sample size calculation for survival analysis of epidemiological studies</p>

<p>â€¢PowerTOST: Power and Sample size based on two one-sided t-tests (TOST) for (bio)equivalence studies</p>

<p>â€¢longpower: Power and sample size for linear model of longitudinal data</p>

<p>Thanks!</p>
"
"0.0484501583111509","0.0493463771219827","125603","<p>I am new to the world of <strong>Regression</strong> in statistics and I have been doing a research in which I am building an ordinal logistic regression model (ORM). In order to fit my ORM model, I am using the 'orm' function of 'rms' package from R (<a href=""http://cran.r-project.org/web/packages/rms/rms.pdf"" rel=""nofollow"">http://cran.r-project.org/web/packages/rms/rms.pdf</a>).</p>

<p>Now I am trying to assess the goodness of fit of my model. By reading the R documentation, I can see the following statement in the 'stat' property of the 'orm' object (pg.98):</p>

<p>""(...)Nagelkerke R2 index, the g-index, gr (the g-index on the odds ratio scale),
and <strong>pdm (the mean absolute difference between 0.5 and the predicted probability
that $Y\geq q$  the marginal median)</strong>(...).""</p>

<p>I don't have enough background to understand the short description of the pdm measure. But when I try to do more research on this measure, I am not able to find related material (e.g. I've been finding ""prescription drug misuse""). In summary, my question is:</p>

<p>Would you know if the 'pdm' measure has some synonym which is more widely used? Or can you provide some references where I can study the pdm metric?</p>
"
"NaN","NaN","125646","<p>How can I fit reduced-rank regression with continuous response in R?</p>

<p>I found the package <code>VGAM</code> but it only fits for discrete distributions...</p>
"
"0.0395593886064618","0.040291148201269","125728","<p>In linear regression model, the <code>predict</code> in <code>R</code> is able to calculate the <code>confidence band</code> and <code>prediction band</code>. But for the general predictive models, what are the normal approaches to calculate <code>confidence band</code> and <code>prediction band</code>? I am asking this is because I am experimenting different models for a given data set. The users would like to get <code>confidence band</code> and <code>prediction band</code> as what <code>predict.lm</code> can give. However, not all those model packages provide these two bands-related information. I may have to write my algorithm to calculate them.</p>
"
"0.0559454238864459","0.056980288229819","125764","<p>I'm trying to understand what factors contribute to the a certain outcome which is a ordered factor variable. In order to just understand which factor is statistically more significant than the others, I would like to build a model and given that my output variable is an ordered factorial variable - I thought I should go for Ordinal Logistic Regression. Given the tradeoff between interpretability and flexibility of models, in my case since I'm only making inferences and not predictions, should I rather go for a easier model to handle like Generalized Linear Models? It kind of boils down to me choosing the <code>polr</code> package in R versus the <code>glm</code> one.</p>
"
"0.027972711943223","0.0284901441149095","125987","<p>I'm trying to estimate $p$ in tweedie regression, but I got the following message: </p>

<blockquote>
  <p>glm.fit: algorithm did not converge</p>
</blockquote>

<p>I'm using public data from ""GLMs for insurance data"" book  by Piet de Jong, and Gillian Z. Heller. Here is my code:</p>

<pre><code>install.packages(""sas7bdat"") # A package to read SAS data set
library(sas7bdat)

mydata &lt;- read.sas7bdat(""http://www.businessandeconomics.mq.edu.au/our_departments/Applied_Finance_and_Actuarial_Studies/acst_docs/glms_for_insurance_data/data/claims_sas_miner.sas7bdat"")
View(mydata) # Viewing the data


library(tweedie)

out=tweedie.profile(mydata$CLM_AMT~1,p.vec=seq(1.1,1.9,length=9),
                    method=""interpolation"",do.ci=TRUE,do.smooth=TRUE,do.plot=TRUE) # Estimating p
</code></pre>

<p>Any idea?</p>
"
"0.027972711943223","0.0284901441149095","126126","<p>I am quite new in R and am on a stage of running a regression model there.
The approach we have chosen is linear regression with dummy variables.
As far as my knowledge and experience go when using dummies one should choose a null interval - class(group) within a variable that will receive 0 points. This should help tackle the multicollinearity.</p>

<p>My question is - how do we set (if we can) the null interval?
I reviewed the ""dummies"" package but did not see an option there.</p>

<p>Thanks,
N</p>
"
"0.0570990591522943","0.0697863157798853","126179","<p>I've been using R to analyze my data (as shown in example below) and <code>lm.beta</code> from the <code>QuantPsyc</code> package to get the standardized regression coefficients. </p>

<p>My understanding is that the absolute value of the standardized regression coefficients should reflect its importance as a predictor. I was also under the impression (and the intuition) that the variable with the largest absolute value should be the most significant independent predictor and should have the <em>lowest</em> p-value. However, I'm not finding that in my data.</p>

<p>For example (taken from my data), I have a multiple regression with dependent variable <code>y</code> and 7 independent variables <code>x1:x7</code>. </p>

<pre><code>    Call:
lm(formula = y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7)
</code></pre>

<p>For 3 of the variables, the beta values and the p-values make sense to me (the greater the magnitude of beta, the lower p-value), but for 4 of them this is not the case. I'll show only the p-values and betas for those 4 to keep this short.</p>

<pre><code>    x1          x2          x3          x7
p   0.006635    0.00004683  0.000152    0.022427
ÃŸ   0.15707977  0.24149287  0.27171665  0.16583391 
</code></pre>

<p>As you can see, <code>x2</code> has a lower p-value than <code>x3</code>, but <code>x3</code> has a larger value for beta. Similarly, <code>x7</code> has a larger beta value than <code>x1</code>, but is less significant. </p>

<p>I've searched for an explanation but have found conflicting information. Is that because there's no straightforward answer to this question? Am I doing something wrong? </p>
"
"0.0927749898843639","0.0944911182523068","126990","<p>I would like to conduct a meta-analysis in the context where I have studies available that measure a continuos variable at multiple time points (0, 1, 2, 3, 4, 5). Time 0 represents the baseline where values are at 100%. Right afterwards there is an intervention and the effect of the intervention is measured over time (114% represents a 14% change relative to baseline). Also I have given two different groups that received different interventions.</p>

<p>Please consider the following dummy data set:</p>

<pre><code>library(ggplot2)
library(metafor)
library(dplyr)
n &lt;- 10
a &lt;- c(rnorm(n,100,0), rnorm(n, 110,2), rnorm(n,130,2), rnorm(n,135,2), rnorm(n,130,2), rnorm(n,125,2))
b &lt;- c(rnorm(n,100,0), rnorm(n,107,2), rnorm(n,122,2), rnorm(n,128,2), rnorm(n,122,2), rnorm(n,125,2))
sd &lt;- rnorm(n,10,1)
my_dat &lt;- data.frame(mean=c(a, b), sd=rep(sd,12), time=rep(c(rep(0,n), rep(1,n), rep(2,n), rep(3,n), rep(4,n), rep(5,n)),2), group=c(rep(""A"", 60), rep(""B"",60)), n=rep(n,120))
my_dat$study &lt;- 1:10
p &lt;- ggplot(aes(y=mean, x=time, colour=group), data=my_dat)
p + geom_jitter() + geom_smooth() + ylab(""% relative to baseline"") + xlab(""time"") 
</code></pre>

<p><img src=""http://i.stack.imgur.com/Lx6TY.png"" alt=""raw data example""></p>

<p>I would like to :</p>

<p>1) investigate the main effect of time (as well as post-hoc tests) for each group individually using the metafor package.</p>

<p>2) investigate the main effect of group (as well as post-hoc tests) for each point in time using the metafor package.</p>

<p>3) investigate group-time interactions.</p>

<p>Thus I rearrange the data and calculate hegdes g relative to baseline t0:</p>

<pre><code>t0_dat &lt;- summarise(group_by(my_dat[my_dat$time==0,], study, group), t0_mean=mean(mean), t0_sd=mean(sd))
my_dat &lt;- merge(my_dat, t0_dat, by=c(""study"", ""group""), all.x=T)
my_dat &lt;- escalc(m1i=mean, m2i=t0_mean, sd1i=sd, sd2i=t0_sd, n1i=n, n2i=n, measure=""SMD"", data=my_dat, append=T)
p &lt;- ggplot(aes(y=yi, x=time, xmin=yi-vi, xmax=yi+vi, colour=group), data=my_dat)
p + geom_point() + geom_smooth() + ylab(""hedges g"") + xlab(""time"") + xlim(c(0,5)) + ylim(c(0,5))
</code></pre>

<p><img src=""http://i.stack.imgur.com/Gf11j.png"" alt=""enter image description here""></p>

<p>Finally I can run the meta-analysis:</p>

<pre><code>m1 &lt;- rma(yi,vi, data=my_dat, mods=~time*group)
summary(m1)
</code></pre>

<p>This indicates a sig. effect of time, a sig. effect of group but no interaction:
Model Results:</p>

<pre><code>         estimate      se     zval    pval    ci.lb    ci.ub     
intrcpt        1.4500  0.2159   6.7158  &lt;.0001   1.0269   1.8732  ***
time           0.3294  0.0667   4.9363  &lt;.0001   0.1986   0.4602  ***
groupB        -0.6808  0.2994  -2.2738  0.0230  -1.2676  -0.0940    *
time:groupB    0.0802  0.0932   0.8609  0.3893  -0.1024   0.2628     

---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>Is this an valid approach?
Would it be appropriate to instead of converting to effect size (hedges g) to use the percentage values (as extracted from the papers) and log-transform them as suggested <a href=""http://stats.stackexchange.com/questions/34057/estimating-percentages-as-the-dependent-variable-in-regression"">in this question</a>, <a href=""http://stackoverflow.com/questions/9958722/r-variable-selection-for-multiple-regression-w-percentage-dependent-variable"">in this question</a> or in the comments below?
Hints to papers that conducted comparable analysis are more then welcome!</p>
"
"0.0740088392978143","0.0753778361444409","127536","<p>Sampling weights, the inverse probability of a unit's selection into the sample, and other more complex and adjusted weights are very often used in the social sciences. There is statistical software that allows weighting of observations/cases, like the <code>hclust</code> function from the <code>R</code>-package <code>cluster</code>. </p>

<p>In regression analysis, there is an ongoing debate when the usage of observation weights is appropriate (see e.g. Winship/Radbill 1994). I could not find anything concerning observation weights in textbooks about cluster analysis, if weighting is discussed, it is mostly about variable weighting. One exemption is the manual of the <code>R</code>-package <code>WeightedCluster</code>, which discusses observation weighting in more detail. The documentation of the <code>cluster</code> package is not very helpful, as it only shows a trivial example using the weighting option <code>hclust(..., members=""..."")</code> where the number or weight of cases is untouched.</p>

<ol>
<li>Therefore, I am looking for references and recommendations with observation/case weighting in cluster analysis, especially hierarchical cluster analysis. </li>
<li>As I could not find the actual formula for the <code>hclust(..., members=""..."")</code> function : Which parameters changes in the hierarchical cluster algorithm if one uses observation weights? How does that affect the algorithm?</li>
</ol>

<p>In order to get an idea of the difference between clustering with and without case weights, here is an example using weights from survey data and the R-code:
<img src=""http://i.stack.imgur.com/BYiLY.png"" alt=""Reweighting of clustering by using membership""></p>

<pre><code>require(survey)
data(api)
whc &lt;- hclust(dist(apiclus2[, c(""pct.resp"", ""meals"")]), method=""ward.D2"", 
              members=apiclus2$pw)
uwhc &lt;- hclust(dist(apiclus2[, c(""pct.resp"", ""meals"")]), method=""ward.D2"")
opar &lt;- par(mfrow = c(1, 2))
plot(whc,  labels = FALSE, hang = -1, main = ""Weighted survey data"")
plot(uwhc, labels = FALSE, hang = -1, main = ""Unweighted survey data"")
</code></pre>

<h3>References</h3>

<ul>
<li>Studer, M., 2013: WeightedCluster Library Manual. A practical guide to creating typologies of trajectories in the social sciences with R. LIVES Working Papers 24. Lausanne.</li>
<li>Winship, C. &amp; L. Radbill, 1994: Sampling Weights and Regression Analysis. Sociological Methods &amp; Research 23: 230â€“257.</li>
</ul>
"
"0.0559454238864459","0.056980288229819","129260","<p>I'm currently estimating a survival model (accelerated failure time model) with a log-logistic distribution in R using the survival package and the survreg function. I want to simulate expected survival times in line with King et al (2001), but I am unsure of the link function needed to calculate the expected survival time for the log-logistic distribution from the survreg regression output. I have added a minimal working example below.</p>

<pre><code>library(survival)
data(kidney)

survreg(formula = Surv(time, status) ~ 
                  age + 
                  cluster(id), 
                  data = kidney, 
                  dist = ""loglogistic"", 
                  robust = TRUE)

               Value Std. Err (Naive SE)     z        p
(Intercept)  4.38127   0.6783     0.5338  6.46 1.05e-10
age         -0.00298   0.0135     0.0114 -0.22 8.26e-01
Log(scale)  -0.23009   0.0732     0.1038 -3.14 1.67e-03

Scale= 0.794 

Log logistic distribution
Loglik(model)= -342   Loglik(intercept only)= -342
    Chisq= 0.07 on 1 degrees of freedom, p= 0.79 
(Loglikelihood assumes independent observations)
Number of Newton-Raphson Iterations: 3 
n= 76 
</code></pre>

<p>I simply want to know how I can calculate expected survival time from the estimated parameters from the survreg output.</p>

<p>King, G., Tomz, M., &amp; Wittenberg, J. (2000). Making the most of statistical analyses: Improving interpretation and presentation. American journal of political science, 347-361.</p>
"
"0.0740088392978143","0.0753778361444409","129298","<p>I would like to get the optimal cutoff of an ROC curve relating to a logistic regression.
I am using the roc from the R package pROC. I am assuming same cost of false negative and false positive using youden's J statistics max(sensitivity+specificity).
I have variable status (binary) and primary variable test (continuous).</p>

<p>roc(status, test, print.thres=T, print.auc=T, plot=T)
Gives me a cutoff of 27.150</p>

<p>I searched on this forum for suggestions and they doesn't seem to give me the right cutoff</p>

<p>I used logistic regression, and I get the parameter value 14.25199 and -0.59877.
Using the parameter values:</p>

<p>roc(status, 14.25199-0.59877*test, print.thres=T, print.auc=T, plot=T)</p>

<p>Gives me a cutoff of -2.005</p>

<p>And another suggestion, is to use the probability instead.</p>

<p>prob=predict(glm(status~test, family=binomial),type=c(""response""))</p>

<p>roc(status, prob, print.thres=T, print.auc=T, plot=T)</p>

<p>Gives me a cutoff of 0.119</p>

<p>As you can see none of the method work. Both method gives the correct AUC but not the cutoff/threshold. The correct method should give me cutoff of 27.150.
What is the correct x form to input to get the correct optimal cutoff/threshold from the command roc(status, x,â€¦.)</p>
"
"0.0395593886064618","0.040291148201269","130414","<p>How to perform a logistic regression and/or SVM on sparse data in R?  I have $ 10^6 $ observations, $ 10^4 $ TRUE/FALSE features, and the data is sparse, i.e.</p>

<pre><code>1,0,0,0,0,0,1,0,0,...
0,0,0,1,0,0,0,0,0,...
0,0,0,0,0,0,1,0,1,...
...
</code></pre>

<p>The sparseness is about 99%.</p>

<p>The package should be able to estimate feature significances and/or perform feature selection of some kind.</p>
"
"0.165552678563412","0.173298781104968","131312","<p>I have some R code (which I did not write) and which performs some state space analysis on some time-series. The data itself is shown as dots (scatter plot) and the Kalman filtered and smoothed state is the solid line.</p>

<p><img src=""http://i.stack.imgur.com/41jaI.png"" alt=""Plot""></p>

<p>My question is regarding the confidence intervals shown in this plot. I calculate <em>my own</em> confidence intervals using the standard method (my C# code is below)</p>

<pre><code>public static double ConfidenceInterval(
    IEnumerable&lt;double&gt; samples, double interval)
{
    Contract.Requires(interval &gt; 0 &amp;&amp; interval &lt; 1.0);

    double theta = (interval + 1.0) / 2;
    int sampleSize = samples.Count();
    double alpha = 1.0 - interval;
    double mean = samples.Mean();
    double sd = samples.StandardDeviation();

    var student = new StudentT(0, 1, samples.Count() - 1);
    double T = student.InverseCumulativeDistribution(theta);
    return T * (sd / Math.Sqrt(samples.Count()));
}
</code></pre>

<p>Now this will return a single interval (and it does it correctly) which I will add/subtract from each point on the series I have applied the calculation to to give me my confidence interval. But this is a constant and the R implementation seems to change over the time-series.</p>

<p>My question is why is <strong>the confidence interval changing for the R implementation? Should I be implementing my confidence levels/intervals differently?</strong></p>

<p>Thanks for your time.</p>

<hr>

<p>For reference the R code that produces this plot is below:</p>

<pre><code>install.packages('KFAS')
require(KFAS)

# Example of local level model for Nile series
modelNile&lt;-SSModel(Nile~SSMtrend(1,Q=list(matrix(NA))),H=matrix(NA))
modelNile
modelNile&lt;-fitSSM(inits=c(log(var(Nile)),log(var(Nile))),model=modelNile,
method='BFGS',control=list(REPORT=1,trace=1))$model

# Can use different optimisation: 
# should be one of â€œNelder-Meadâ€, â€œBFGSâ€, â€œCGâ€, â€œL-BFGS-Bâ€, â€œSANNâ€, â€œBrentâ€
modelNile&lt;-SSModel(Nile~SSMtrend(1,Q=list(matrix(NA))),H=matrix(NA))
modelNile
modelNile&lt;-fitSSM(inits=c(log(var(Nile)),log(var(Nile))),model=modelNile,
method='L-BFGS-B',control=list(REPORT=1,trace=1))$model

# Filtering and state smoothing
out&lt;-KFS(modelNile,filtering='state',smoothing='state')
out$model$H
out$model$Q
out

# Confidence and prediction intervals for the expected value and the observations.
# Note that predict uses original model object, not the output from KFS.
conf&lt;-predict(modelNile,interval='confidence')
pred&lt;-predict(modelNile,interval='prediction')
ts.plot(cbind(Nile,pred,conf[,-1]),col=c(1:2,3,3,4,4),
ylab='Predicted Annual flow', main='River Nile')
KFAS 13

# Missing observations, using same parameter estimates
y&lt;-Nile
y[c(21:40,61:80)]&lt;-NA
modelNile&lt;-SSModel(y~SSMtrend(1,Q=list(modelNile$Q)),H=modelNile$H)
out&lt;-KFS(modelNile,filtering='mean',smoothing='mean')

# Filtered and smoothed states
plot.ts(cbind(y,fitted(out,filtered=TRUE),fitted(out)), plot.type='single',
col=1:3, ylab='Predicted Annual flow', main='River Nile')

# Example of multivariate local level model with only one state
# Two series of average global temperature deviations for years 1880-1987
# See Shumway and Stoffer (2006), p. 327 for details
data(GlobalTemp)
model&lt;-SSModel(GlobalTemp~SSMtrend(1,Q=NA,type='common'),H=matrix(NA,2,2))

# Estimating the variance parameters
inits&lt;-chol(cov(GlobalTemp))[c(1,4,3)]
inits[1:2]&lt;-log(inits[1:2])
fit&lt;-fitSSM(inits=c(0.5*log(.1),inits),model=model,method='BFGS')
out&lt;-KFS(fit$model)
    ts.plot(cbind(model$y,coef(out)),col=1:3)
legend('bottomright',legend=c(colnames(GlobalTemp), 'Smoothed signal'), col=1:3, lty=1)

# Seatbelts data
## Not run:
model&lt;-SSModel(log(drivers)~SSMtrend(1,Q=list(NA))+
SSMseasonal(period=12,sea.type='trigonometric',Q=NA)+
log(PetrolPrice)+law,data=Seatbelts,H=NA)

# As trigonometric seasonal contains several disturbances which are all
# identically distributed, default behaviour of fitSSM is not enough,
# as we have constrained Q. We can either provide our own
# model updating function with fitSSM, or just use optim directly:
# option 1:
ownupdatefn&lt;-function(pars,model,...){
model$H[]&lt;-exp(pars[1])
    diag(model$Q[,,1])&lt;-exp(c(pars[2],rep(pars[3],11)))
model #for option 2, replace this with -logLik(model) and call optim directly
}
14 KFAS
fit&lt;-fitSSM(inits=log(c(var(log(Seatbelts[,'drivers'])),0.001,0.0001)),
model=model,updatefn=ownupdatefn,method='BFGS')
out&lt;-KFS(fit$model,smoothing=c('state','mean'))
    out
    ts.plot(cbind(out$model$y,fitted(out)),lty=1:2,col=1:2,
    main='Observations and smoothed signal with and without seasonal component')
    lines(signal(out,states=c(""regression"",""trend""))$signal,col=4,lty=1)
legend('bottomleft',
legend=c('Observations', 'Smoothed signal','Smoothed level'),
col=c(1,2,4), lty=c(1,2,1))

# Multivariate model with constant seasonal pattern,
# using the the seat belt law dummy only for the front seat passangers,
# and restricting the rank of the level component by using custom component
# note the small inconvinience in regression component,
# you must remove the intercept from the additional regression parts manually
model&lt;-SSModel(log(cbind(front,rear))~ -1 + log(PetrolPrice) + log(kms)
+ SSMregression(~-1+law,data=Seatbelts,index=1)
+ SSMcustom(Z=diag(2),T=diag(2),R=matrix(1,2,1),
Q=matrix(1),P1inf=diag(2))
+ SSMseasonal(period=12,sea.type='trigonometric'),
data=Seatbelts,H=matrix(NA,2,2))
likfn&lt;-function(pars,model,estimate=TRUE){
model$H[,,1]&lt;-exp(0.5*pars[1:2])
    model$H[1,2,1]&lt;-model$H[2,1,1]&lt;-tanh(pars[3])*prod(sqrt(exp(0.5*pars[1:2])))
    model$R[28:29]&lt;-exp(pars[4:5])
if(estimate) return(-logLik(model))
model
}
fit&lt;-optim(f=likfn,p=c(-7,-7,1,-1,-3),method='BFGS',model=model)
model&lt;-likfn(fit$p,model,estimate=FALSE)
    model$R[28:29,,1]%*%t(model$R[28:29,,1])
    model$H
out&lt;-KFS(model)
out
ts.plot(cbind(signal(out,states=c('custom','regression'))$signal,model$y),col=1:4)

# For confidence or prediction intervals, use predict on the original model
pred &lt;- predict(model,states=c('custom','regression'),interval='prediction')
ts.plot(pred$front,pred$rear,model$y,col=c(1,2,2,3,4,4,5,6),lty=c(1,2,2,1,2,2,1,1))

## End(Not run)
## Not run:
# Poisson model
model&lt;-SSModel(VanKilled~law+SSMtrend(1,Q=list(matrix(NA)))+
SSMseasonal(period=12,sea.type='dummy',Q=NA),
KFAS 15
data=Seatbelts, distribution='poisson')

# Estimate variance parameters
fit&lt;-fitSSM(inits=c(-4,-7,2), model=model,method='BFGS')
model&lt;-fit$model

# use approximating model, gives posterior mode of the signal and the linear predictor
out_nosim&lt;-KFS(model,smoothing=c('signal','mean'),nsim=0)

# State smoothing via importance sampling
out_sim&lt;-KFS(model,smoothing=c('signal','mean'),nsim=1000)
out_nosim
out_sim

## End(Not run)
# Example of generalized linear modelling with KFS
# Same example as in ?glm
counts &lt;- c(18,17,15,20,10,20,25,13,12)
outcome &lt;- gl(3,1,9)
treatment &lt;- gl(3,3)
print(d.AD &lt;- data.frame(treatment, outcome, counts))
glm.D93 &lt;- glm(counts ~ outcome + treatment, family = poisson())
model&lt;-SSModel(counts ~ outcome + treatment, data=d.AD,
distribution = 'poisson')
out&lt;-KFS(model)
coef(out,start=1,end=1)
coef(glm.D93)
summary(glm.D93)$cov.s
    out$V[,,1]
outnosim&lt;-KFS(model,smoothing=c('state','signal','mean'))
set.seed(1)
outsim&lt;-KFS(model,smoothing=c('state','signal','mean'),nsim=1000)

## linear
# GLM
glm.D93$linear.predictor

# approximate model, this is the posterior mode of p(theta|y)
c(outnosim$thetahat)

# importance sampling on theta, gives E(theta|y)
c(outsim$thetahat)

## predictions on response scale
16 KFAS

# GLM
fitted(glm.D93)

# approximate model with backtransform, equals GLM
c(fitted(outnosim))

# importance sampling on exp(theta)
fitted(outsim)

# prediction variances on link scale
# GLM
as.numeric(predict(glm.D93,type='link',se.fit=TRUE)$se.fit^2)

# approx, equals to GLM results
c(outnosim$V_theta)
    # importance sampling on theta
    c(outsim$V_theta)
# prediction variances on response scale
# GLM
as.numeric(predict(glm.D93,type='response',se.fit=TRUE)$se.fit^2)
    # approx, equals to GLM results
    c(outnosim$V_mu)
# importance sampling on theta
c(outsim$V_mu)
    ## Not run:
    data(sexratio)
    model&lt;-SSModel(Male~SSMtrend(1,Q=list(NA)),u=sexratio[,'Total'],data=sexratio,
    distribution='binomial')
    fit&lt;-fitSSM(model,inits=-15,method='BFGS',control=list(trace=1,REPORT=1))
    fit$model$Q #1.107652e-06

# Computing confidence intervals in response scale
# Uses importance sampling on response scale (4000 samples with antithetics)
pred&lt;-predict(fit$model,type='response',interval='conf',nsim=1000)
    ts.plot(cbind(model$y/model$u,pred),col=c(1,2,3,3),lty=c(1,1,2,2))

# Now with sex ratio instead of the probabilities:
imp&lt;-importanceSSM(fit$model,nsim=1000,antithetics=TRUE)
    sexratio.smooth&lt;-numeric(length(model$y))
sexratio.ci&lt;-matrix(0,length(model$y),2)
    w&lt;-imp$w/sum(imp$w)
    for(i in 1:length(model$y)){
sexr&lt;-exp(imp$sample[i,1,])
sexratio.smooth[i]&lt;-sum(sexr*w)
oo&lt;-order(sexr)
sexratio.ci[i,]&lt;-c(sexr[oo][which.min(abs(cumsum(w[oo]) - 0.05))],
+ sexr[oo][which.min(abs(cumsum(w[oo]) - 0.95))])
}

# Same by direct transformation:
out&lt;-KFS(fit$model,smoothing='signal',nsim=1000)
    KFS 17
    sexratio.smooth2 &lt;- exp(out$thetahat)
sexratio.ci2&lt;-exp(c(out$thetahat)
    + qnorm(0.025) * sqrt(drop(out$V_theta))%o%c(1, -1))
ts.plot(cbind(sexratio.smooth,sexratio.ci,sexratio.smooth2,sexratio.ci2),
col=c(1,1,1,2,2,2),lty=c(1,2,2,1,2,2))

## End(Not run)
# Example of Cubic spline smoothing
## Not run:
require(MASS)
data(mcycle)
model&lt;-SSModel(accel~-1+SSMcustom(Z=matrix(c(1,0),1,2),
T=array(diag(2),c(2,2,nrow(mcycle))),
Q=array(0,c(2,2,nrow(mcycle))),
P1inf=diag(2),P1=diag(0,2)),data=mcycle)
model$T[1,2,]&lt;-c(diff(mcycle$times),1)
model$Q[1,1,]&lt;-c(diff(mcycle$times),1)^3/3
model$Q[1,2,]&lt;-model$Q[2,1,]&lt;-c(diff(mcycle$times),1)^2/2
    model$Q[2,2,]&lt;-c(diff(mcycle$times),1)
    updatefn&lt;-function(pars,model,...){
    model$H[]&lt;-exp(pars[1])
    model$Q[]&lt;-model$Q[]*exp(pars[2])
    model
    }
    fit&lt;-fitSSM(model,inits=c(4,4),updatefn=updatefn,method=""BFGS"")
    pred&lt;-predict(fit$model,interval=""conf"",level=0.95)
plot(x=mcycle$times,y=mcycle$accel,pch=19)
lines(x=mcycle$times,y=pred[,1])
    lines(x=mcycle$times,y=pred[,2],lty=2)
lines(x=mcycle$times,y=pred[,3],lty=2)
## End(Not run)
</code></pre>

<p>The time-series data is:</p>

<pre><code>Time, 2.4, 2.6, 3.2, 3.6, 4, 6.2, 6.6, 6.8, 7.8, 8.2, 8.8, 8.8, 9.6, 10, 10.2, 10.6, 11, 11.4, 13.2, 13.6, 13.8, 14.6, 14.6, 14.6, 14.6, 14.6, 14.6, 14.8, 15.4, 15.4, 15.4, 15.4, 15.6, 15.6, 15.8, 15.8, 16, 16, 16.2, 16.2, 16.2, 16.4, 16.4, 16.6, 16.8, 16.8, 16.8, 17.6, 17.6, 17.6, 17.6, 17.8, 17.8, 18.6, 18.6, 19.2, 19.4, 19.4, 19.6, 20.2, 20.4, 21.2, 21.4, 21.8, 22, 23.2, 23.4, 24, 24.2, 24.2, 24.6, 25, 25, 25.4, 25.4, 25.6, 26, 26.2, 26.2, 26.4, 27, 27.2, 27.2, 27.2, 27.6, 28.2, 28.4, 28.4, 28.6, 29.4, 30.2, 31, 31.2, 32, 32, 32.8, 33.4, 33.8, 34.4, 34.8, 35.2, 35.2, 35.4, 35.6, 35.6, 36.2, 36.2, 38, 38, 39.2, 39.4, 40, 40.4, 41.6, 41.6, 42.4, 42.8, 42.8, 43, 44, 44.4, 45, 46.6, 47.8, 47.8, 48.8, 50.6, 52, 53.2, 55, 55, 55.4, 57.6                                                                                                
mcycle, 0, -1.3, -2.7, 0, -2.7, -2.7, -2.7, -1.3, -2.7, -2.7, -1.3, -2.7, -2.7, -2.7, -5.4, -2.7, -5.4, 0, -2.7, -2.7, 0, -13.3, -5.4, -5.4, -9.3, -16, -22.8, -2.7, -22.8, -32.1, -53.5, -54.9, -40.2, -21.5, -21.5, -50.8, -42.9, -26.8, -21.5, -50.8, -61.7, -5.4, -80.4, -59, -71, -91.1, -77.7, -37.5, -85.6, -123.1, -101.9, -99.1, -104.4, -112.5, -50.8, -123.1, -85.6, -72.3, -127.2, -123.1, -117.9, -134, -101.9, -108.4, -123.1, -123.1, -128.5, -112.5, -95.1, -81.8, -53.5, -64.4, -57.6, -72.3, -44.3, -26.8, -5.4, -107.1, -21.5, -65.6, -16, -45.6, -24.2, 9.5, 4, 12, -21.5, 37.5, 46.9, -17.4, 36.2, 75, 8.1, 54.9, 48.2, 46.9, 16, 45.6, 1.3, 75, -16, -54.9, 69.6, 34.8, 32.1, -37.5, 22.8, 46.9, 10.7, 5.4, -1.3, -21.5, -13.3, 30.8, -10.7, 29.4, 0, -10.7, 14.7, -1.3, 0, 10.7, 10.7, -26.8, -14.7, -13.3, 0, 10.7, -14.7, -2.7, 10.7, -2.7, 10.7
</code></pre>
"
"0.0692289300613081","0.080582296402538","131401","<p>I am running a pooled OLS regression using the plm package in R. Though, my question is more about basic statistics, so I try posting it here first ;)</p>

<p>Since my regression results yield heteroskedastic residuals I would like to try using heteroskedasticity robust standard errors. As a result from <code>coeftest(mod, vcov.=vcovHC(mod, type=""HC0""))</code> I get a table containing estimates, standard errors, t-values and p-values for each independent variable, which basically are my ""robust"" regression results.</p>

<p>For discussing the importance of different variables I would like to plot the share of variance explained by each independent variable, so I need the respective sum of squares. However, using function <code>aov()</code>, I don't know how to tell R to use robust standard errors.</p>

<p>Now my question is: How do I get the ANOVA table/sum of squares that refers to robust standard errors? Is it possible to calculate it based on the ANOVA table from regression with normal standard errors?</p>

<p>Edit:</p>

<p>In other words and disregarding my R-issues:</p>

<p>If R$^2$ is not affected by using robust standard errors, will also the respective contributions to explained variance by the different explanatory variables be unchanged?</p>

<p>Edit:</p>

<p>In R, does <code>aov(mod)</code> actually give a correct ANOVA table for a panelmodel (plm)?</p>
"
"0.027972711943223","0.0284901441149095","132774","<p>I trying to conduct linear discriminant analysis using the lda package and I keep getting a warning message saying that the variables are collinear.</p>

<p>I want to pinpoint and remove the redundant variables. What is the best method for doing this in R?</p>

<p>I've read about solutions such as stepwise selection which can be used to do this but this doesn't work with discriminant analysis.</p>

<p>I tried lasso regression but this shrank my 66 variables down to just 12 - the optimal set and it's hard to identify the order in which it's done this as I would prefer to keep a larger number. </p>
"
"0.0740088392978143","0.0753778361444409","133248","<p>Iâ€™m analyzing student performance data. In my dataset, each row corresponds to a student and each column contains several performance metrics (continuous) and the student type (categorical, 4 types). The student type was computed in another analysis, using Expectation-Maximization, based on how students were graded over time. My sample is small, with 50 students.</p>

<p>I want to understand what characterizes each student type, regarding the performance features I have. I want to understand things like â€œthe more grade they have the more likely is to belong to a particular clusterâ€ and so on, if they are present at all in my data.</p>

<p>My first question is:
1) I believe that what I need is Multinomial Logistic Regression. Am I right or is there a better way to achieve this?</p>

<p>If yes, Iâ€™ve been exploring Multinomial Logistic Regression in R, using the multinom of the nnet package, but I need help with the following:</p>

<p>2) Understanding if the model is any good. So far I have the percentage of correctly classified instances, but I know this is not a very good goodness of fit measure. </p>

<p>3) How to assess how good each individual predictor is. I know how to look for the exponentiated B, but I donâ€™t know how to assess its significance. I read that using the t-distribution to compute the p-value here is usually a mistake. I found a <a href=""http://stats.stackexchange.com/questions/63222/getting-p-values-for-multinorm-in-r-nnet-package"">similar post here</a>, but a clear answer was not provided.</p>

<p>Thank you in advance for any answer, suggestion or comment.</p>
"
"0.0625488854200668","0.0637058989297032","133387","<p>I'm trying to create a prediction model for estimation of continuous variable based on about 35 Independent variables.My data set has circa 27k observartions.
Here is the summary of the the targeted continuous variable:</p>

<pre><code>              Frequency Percent
(0,5]              2706  10.053
(5,10]             5226  19.415
(10,25]            4397  16.335
(25,100]           7142  26.533
(100,1e+03]        6465  24.018
(1e+03,1e+05]       981   3.645
Total             26917 100.000
</code></pre>

<p>I tried (by using R) Random Forest (RandomForest package),Linear regression, Conditional Inference Trees (ctree function in party package) but all of them have results that have a significant overestimation.
Here are the results of the prediction where I counted number of observations by thier distance from the actual values:
Any idea how can i balance the results?</p>

<p><img src=""http://i.stack.imgur.com/y70OM.png"" alt=""enter image description here""></p>

<p>Here are some views on the data:
The target variable is LTV for a user, I would like to predict LTV value after 180 days  based on users behavior of the first 7 days.
Here Is a summary fot the target variavle:</p>

<pre><code>  vars     n   mean     sd median trimmed   mad  min      max    range skew kurtosis   se
1    1 26917 178.35 622.29  33.49   66.63 39.28 0.03 22103.73 22103.71 14.1   325.08 3.79
</code></pre>

<p>UPDATE:
Here are the distributions of the targeted variable (first)and the prediction (secound)results:
<img src=""http://i.stack.imgur.com/b3MBs.png"" alt=""targeted variable"">
<img src=""http://i.stack.imgur.com/3N7d1.png"" alt=""prediction results based on the linear regression model that was the best""></p>
"
"0.115512844298478","0.1241856590838","133571","<p>I know there are already lots of questions around this topic (especially <a href=""http://stats.stackexchange.com/questions/16390/when-to-use-generalized-estimating-equations-vs-mixed-effects-models/16415#16415"">this one</a> and <a href=""http://stats.stackexchange.com/questions/24689/interpreting-coefficients-of-ordinal-logistic-regression-when-there-is-clusterin/29701#29701"">this one</a>) but I haven't really seen anything that directly helps me (It will be obvious I'm not a great statistician, but I'll do my best to explain). </p>

<p>I am running an ordinal regression in R (<code>clm</code> and <code>clmm</code>). My response variable is a rating between 0 and 4. I have two types of explanatory variables: individual and scenario variables [let's say <code>IVs</code> and <code>SVs</code>]. </p>

<p>Six different scenario variables (all dummies with at most 4 different values) represent potential collaboration scenarios that get rated by the respondent (between 0 and 4) creating the response variable. (Research design is a conjoint analysis; there are a total of 192 different scenarios possible)</p>

<p>On top of that I have a variety of individual characteristics about the respondent (age, gender, work experience, networking skills, ...) all derived from a survey.</p>

<p>Every respondent rates between 3 and 16 different scenarios (average 8.1); every scenario is rated by at least 8 respondents. Every respondent and every scenario have a unique identifier (called <code>IVid</code> and <code>SVid</code>). So they are non nested within each other.</p>

<p>Thus the basic regression looks like this:</p>

<pre><code>clm.base &lt;- clm(rating ~ SVs + IVs, data = dt) 
</code></pre>

<p>The hypothesis I am trying to test is that there are specific individual characteristics, that will influence the rating of the scenarios, independent of the actual content of the scenarios. Basically, some people are more or less favourable to all types of collaboration scenarios. </p>

<p>Now a reviewer of my paper asks me to include individual fixed effects (which in management [my field] basically means dummies for each individual). My assumption originally was that this would result in all individual variables being dropped. This is exactly what happens when I use another model (package <code>lfe</code>)</p>

<pre><code>felm.complete &lt;- felm(rating ~ SVs + IVs | SVid + IVid | 0 | IVid, data = dt) 
</code></pre>

<p>In this regression basically all my variables are perfectly collinear as expected.
However, when I approximate this in the ordinal package, there is no perfect collinearity. I presume this is related that <code>clmm</code> adds so-called 'random effects'. The regression takes a couple of minutes to run but eventually returns results</p>

<pre><code>clmm.complete &lt;- clmm(rating ~ SVs + IVs + (1|SVid) + (1|IVid), data = dt)
</code></pre>

<p>Now, the results here are pretty useless:</p>

<ul>
<li>All but one of my IVs are insignificant</li>
</ul>

<p>I am trying to understand what exactly happens when adding the <code>(1|IVid)</code> term in the <code>clmm</code> model. If it basically adds something like an individual dummy than the fact almost everything is now insignificant is no surprise. The coefficients of the <code>IVid</code> dummies would capture the effect I am looking for (some people rate all scenarios higher or lower, regardless of scenario content) most accurately.</p>

<p>Now I wonder whether this interpretation is correct or whether the results I got from running the simple <code>clm</code> regression are just not reliable? </p>

<p>Concretely, I'd like to find out:</p>

<ul>
<li>What happens when adding a random effect to <code>clmm</code></li>
<li>A laymen explanation of how the Laplace approximation works</li>
<li>How to group errors around individuals when running <code>clm</code></li>
<li>Is it possible to extract the coefficients of these random effects <code>(1|id)</code> for as far as there is such a thing?</li>
</ul>
"
"0.0791187772129236","0.080582296402538","134961","<p>I have 295 observations of two variables, of which here are a few:   </p>

<pre><code> Date             Close price  
   1/04/14 0:00     478.72   
    2/04/14 0:00    437.51   
    3/04/14 0:00    447.08  
    4/04/14 0:00    448.88  
    5/04/14 0:00    464.83  
    6/04/14 0:00    460.70  
    7/04/14 0:00    446.22  
    8/04/14 0:00    450.46   
    9/04/14 0:00    440.20  
    10/04/14 0:00   360.84  
    11/04/14 0:00   420.06  
    12/04/14 0:00   420.66  
    13/04/14 0:00   414.95  
    14/04/14 0:00   457.63  
    15/04/14 0:00   520.12  
    16/04/14 0:00   529.16    
</code></pre>

<p>The first variable is the date plus time stamp ""0:00"" and the second variable is the price at that date. My whole data set spans 1/4/2014 till 20/1/2015 with daily observations.</p>

<p>I want to decompose this data into two components, trend and errors. I know stl is mainly for seasonal data, but I use the following code, setting the value of s.window to be large to negate the seasonality component.</p>

<pre><code>btc.ts&lt;-ts(btc.csv$Close.Price,frequency=12)
decomp&lt;-stl(btc.ts,s.window=10000)
plot(decomp)
</code></pre>

<p>There are a few issues here, the frequency of my data is daily, and frequency = 12 is not correct, but my hopes are that I just want to extract the trend component of the stl and use the zoo package:</p>

<pre><code>trend&lt;-decomp$time.series[,""trend""]
td&lt;-seq(as.Date(""2014/4/1""),as.Date(""2015/1/15""),""days"")
trendz&lt;-zoo(x=trend,order.by=td)
</code></pre>

<p>The reason for this is that the trend component found in stl looks the best out of all my approaches, as shown below:</p>

<p><img src=""http://i.stack.imgur.com/oDN2F.png"" alt=""enter image description here""></p>

<p>Now, what I want to do is fit a polynomial to model the trend, the issue I am having here is that I cannot use 'td' in the following command (natural splines):</p>

<p><code>trendz.fit&lt;-lm(trendz~ns(td+I(td^2)+I(td^3)))</code></p>

<p>as td is a 'date' object. I just want to have something like : trend (t) = a + bt + c(t^2) + d(t^3)..</p>

<p>What can I do to achieve this? How does one usually treat regressions with one of the variables being time/date?</p>

<p>Any suggestions are appreciated, thanks</p>
"
"0.0484501583111509","0.0493463771219827","135011","<p>I'm having trouble finding a time series technique to deal with a data set I am working on. It contains multiple subjects and multiple variables, not all of which will likely be part of the time series. It looks something like this:</p>

<pre><code>Subject  Date      T1  T2  V1  V2  V3
A        1/1/2012  1   5   9   13  17
A        2/1/2012  2   6   10  14  18
...
B        1/1/2012  3   7   11  15  19
B        2/1/2012  4   8   12  16  20
...
</code></pre>

<p>Where T1, T2 are likely time series, and V1, V2, and V3 are likely not. I'm sure that this distinction is probably unnecessary, since techniques like Box-Jenkins should detect autoregression in any variable.</p>

<p>Ultimately, I want to be able to do forecasting on other subjects that were probably not used to build this model.</p>

<p>If you know of any R package(s) that can take this on, please let me know. Some example code would also be greatly appreciated. Thank you for any insight you can provide.</p>

<p>Edit: I am looking into dynamic linear regression using the <code>dynlm</code> package, but am having trouble coding it to include the dates and subjects.</p>
"
"0.155821572025607","0.153744422666267","135043","<p>I have three questions concerning accelerated failure time models (AFT), one statistical, one regarding how to implement these models in R, and one related to finding out information about what R is doing. In short my questions are;</p>

<p>1) What is the relationship between the Gumbel and Weibull distributions?</p>

<p>2) How can I use (1) to simulate a AFT model using Gumbel errors and fit this model in R?</p>

<p>3) Where can I find formulae regarding exactly what distribution specification R is using when fitting a Weibull distribution, and exactly what model is being fitted?</p>

<p>I am having difficulties implementing 2), which may be due to my mis-understanding of 1), but which I can't seem to resolve due to 3). Question (3) is self-explanatory but (2) and (3) require more detail;</p>

<p>1) It seems a standard result that if $U\sim Gumbel(\alpha,\beta)$ then $V:=\exp(U)\sim Weibull(\lambda,\sigma)$ where $\alpha=\log(\sigma)$ and $\beta=1/\lambda$. However using the definition of the Gumbel and Weibull distributions commonly used (for example Wikipedia), when I do the derivation I can only get the transformation $V':=1/\exp(U)$ to give this result but where $\alpha=-\log(\sigma)=\log(1/\sigma)$. Thus can anyone confirm or not any knowledge of this relationship, or perhaps suggest where I have gone wrong (for brevity in the first instance I do not supply the detail)?</p>

<p>2) My approach is to use</p>

<p>$Y_{i}:=\log\left(\frac{1}{T_{i}}\right)=\beta_{0} + \beta_{1}x_{i} + e_{i},\hspace{20pt}i=1,...,N$,</p>

<p>as a data-generating mechanism for the logarithm of the time to event where $e_{i}\sim Gumbel(\alpha,\beta)$, where $i$ indexes subjects, $x_{i}$ is a scalar covariate, and the $e_{i}$ are all independent. I choose $\alpha=-\beta*c$ where $c$ is Euler's constant in order to ensure $E[e_{i}]=\alpha+c\beta=0$. This gives</p>

<p>$Y_{i}\sim Gumbel(\beta_{0} + \beta_{1}x_{i}+\alpha,\beta)$,</p>

<p>and using (1)</p>

<p>$T_{i}\sim Weibull(1/\beta,\exp[-(\beta_{0} + \beta_{1}x_{i}+\alpha)])$</p>

<p>The code at the end of this post is a minimal working example of this approach, where I censor subjects if $T_{i}$ is greater than the median of the $N$ theoretical medians of $\{T_{1},...,T_{N}\}$, and create an event if not. This gives $50-60\%$ of subjects being censored, the balance having events, and I interpret this to be right-censoring (say the end of a study).</p>

<p>I then use the survreg package in R to try to fit an AFT to $Y_{i}$ using the ""dist=weibull"" option. Using $\beta_{0}=-10$ and $\beta_{1}=0$ gives the following output</p>

<p><img src=""http://i.stack.imgur.com/4sQlb.png"" alt=""enter image description here""></p>

<p>which gives the intercept being positive when it should be negative. Things get worse when using $\beta_{0}=-10$ and $\beta_{1}=2$ which gives the following output</p>

<p><img src=""http://i.stack.imgur.com/i7o3f.png"" alt=""enter image description here""></p>

<p>which is obviously wrong. Thus I would like to know what model I am actually fitting when using the survreg package.</p>

<p>The code below is a minimal working example (apart from some code to produce plots which can be helpful).</p>

<pre><code># minimal working example
set.seed(123)
require(survival)
#params of the gumbel(alpha_gum,beta_gum) distribution so that E[X]=0
beta_gum = 1/5 #
alpha_gum = -(beta_gum*(-digamma(1)))

#calc the mean of the errors using Eulers constant as the negative of the diagamma function
mu_e = alpha_gum + (beta_gum*(-digamma(1)))#should be 0   

# regression parameters
intercept = -10;
beta1 =0;
#beta1 =2;

#number of subjects
N=1000;

# vector of uniform random numbers
U = runif(N)

#vector for gumbel distributed errors
e = matrix(,nrow=N,ncol=1)


# log of time to event, time to event, mean LTTE
logTTE = matrix(,nrow=N,ncol=1)
Xbeta_LTTE= matrix(,nrow=N,ncol=1)
TTE = matrix(,nrow=N,ncol=1)
TTE2 = matrix(,nrow=N,ncol=1)

#censoring variable
censor = matrix(,nrow=N,ncol=1)

#simulate covariate from a normal distribution
covariate1 = rnorm(N,6,4)

for (i in 1:N)
{
  # calculate the Gumbel RV from the inverse CDF of the Gumbel
  e[i,1] = alpha_gum + (-beta_gum*log(-log(U[i])))

  #generate the mean log TTE  
  Xbeta_LTTE[i,1] = intercept + (beta1*covariate1[i])

  #add the errors
  logTTE[i,1] = Xbeta_LTTE[i,1] + e[i,1]  

  #transform to raw time variable - this is a Weibull dist
  #TTE_i ~ Weibull[1/beta_gum , exp(-[logTTE_i+alpha_gum])
  TTE[i,1] = 1/exp(logTTE[i,1])      
}

#calc the median the TTE given TTE ~ Weibull[1/beta_gum , exp(-[X_i^t*beta+alpha_gum])
lambda_array = exp(-(Xbeta_LTTE + alpha_gum + (beta_gum*(-digamma(1)))))
kappa = 1/beta_gum
median_TTE_array = (lambda_array)*(log(2)^(1/kappa))
median_TTE = median(median_TTE_array)

# calculate the censoring variable
for (i in 1:N)
{
  #censoring: subjects with a TTE &gt;median_TTE will be right-censored
  #i.e. study ends at T=median_TTE say
  if (TTE[i,1]&gt;median_TTE)
  {
    censor[i,1]=1 
    TTE2[i,1]=median_TTE
  }
  else
  {
    censor[i,1]=0    
    TTE2[i,1]=TTE[i,1]
  }  
}

#calculate the percentage of censored subjects and do a plot
pc_censored = sum(censor)/N

#fit AFT model
datframe_surv = data.frame(covariate1)
attach(datframe_surv)

m.surv = Surv(TTE2,censor,type=""right"")
m.surv.fit = survreg(m.surv~covariate1,dist=""weibull"",scale=1)
sum = summary(m.surv.fit)
print(sum)



###################  plots ########################


#histogram of the errors - gumbel dist
h1 = hist(e, breaks=50, plot=FALSE) 

#histogram of the mean log TTE - gumbel dist
h2 = hist(logTTE, breaks=50, plot=FALSE) 

#histogram of the fixed means
h3 = hist(Xbeta_LTTE, breaks=50, plot=FALSE) 

#histogram of the TTE - weibul dist
h4 = hist(TTE, breaks=50, plot=FALSE) 

#calc the mean of the log TTE given logTTE ~ Gumbel(X_i^t*beta+alpha_gum,beta_gum)
median_logTTE_array = Xbeta_LTTE + alpha_gum - (beta_gum*(log(log(2))))
median_logTTE = median(median_logTTE_array)



#calc the means
ylim_h1 = c(min(h1$density),max(h1$density) )
xlim_h1 = c(mu_e,mu_e )

ylim_h2 = c(min(h3$density),max(h3$density) )
xlim_h2 = c(median_logTTE,median_logTTE )

ylim_h3 = c(min(h3$density),max(h3$density) )
xlim_h3 = c(mean(Xbeta_LTTE),mean(Xbeta_LTTE) )


ylim_h4 = c(min(h4$density),max(h4$density) )
xlim_h4 = c(median_TTE,median_TTE )


#dev.off()
par(mfrow=c(2,2))

plot(h1$mids,h1$density,col='red',main=""errors - gumbel dist"",xlab=""errors (log time)"")
lines(xlim_h1,ylim_h1)

plot(h3$mids,h3$density,col='red',main=""mean log TTE (X*beta) - fixed"",xlab=""mean log TTE (log time)"")
lines(xlim_h3,ylim_h3)

plot(h2$mids,h2$density,col='red',main=""log TTE - gumbel dist"",xlab=""log TTE (log time)"")
lines(xlim_h2,ylim_h2)


plot(h4$mids,h4$density,col='red',main=""TTE - Weibull dist"",xlab=""TTE (time)"")
lines(xlim_h4,ylim_h4)
</code></pre>
"
"0.0395593886064618","0.040291148201269","135292","<p>How do you fit a multilevel regression model in R where you want to include a group-level regression? I think that the answer is using STAN, but I've been trying to use lmer() from the lme4 package.</p>

<p>Say you have data on radon levels in houses within counties. You have a house-level predictor ""Dust"" and a county-level measure of ""Uranium"" and ""Plutonium.""</p>

<p>County level: $ \alpha_j \sim N\big (\gamma_0 + \gamma_1 plut_j + \gamma_2 uran_j, \sigma^2_{alpha}\big)$</p>

<p>House level: $radon_i = \alpha_{j[i]} + \beta dust_i + \epsilon_i$</p>

<p>How would you fit a model at the county level, like the following:
county ~ uranium + plutonium
and then incorporate the new county coefficients in the first model?</p>
"
"0.0843408998948762","0.0944911182523068","135967","<p>I am a beginner in R. I am doing logistic regression using around 80 independent variables using <code>glm</code> function in R. The dependent variable is <code>churn</code> which says whether a customer churned or not. I want to know how to identify the right combination of variables to get a good predictive logistic regression model in R.  I also want to know how to identify the same for making good decision tree in R ( I am using the <code>ctree</code> function from the <code>party</code> package).
So far, I had used <code>drop1</code> function  and  <code>anova(LogMdl, test=""Chisq"")</code> where <code>LogMdl</code> is my logistic regression model to drop unwanted variables in the predictive model.  But maximum accuracy I was able to achieve was only 60%. </p>

<p>Also I am not sure if I am using the <code>drop1</code> and <code>anova</code> functions correctly. I dropped the variables with lowest AIC using <code>drop1</code> function.  Using <code>anova</code> function, I dropped variables with p value > 0.05</p>

<p>Kindly help me how to identify the right set of variables for both logistic regression and decision tree models to increase my model's predictive accuracy to close to 90% or more than that if possible.   </p>

<pre><code>library(party)
setwd(""D:/CIS/Project work"")
CellData &lt;- read.csv(""Cell2Cell_SPSS_Data - Orig.csv"")
trainData &lt;- subset(CellData,calibrat==""1"")
testData &lt;- subset(CellData,calibrat==""0"") # validation or test data set
LogMdl = glm(formula=churn ~ revenue  + mou    + recchrge+ directas+ 
               overage + roam    + changem +
               changer  +dropvce + blckvce + unansvce+ 
               custcare+ threeway+ mourec  +
               outcalls +incalls + peakvce + opeakvce+ 
               dropblk + callfwdv+ callwait+
               months  + uniqsubs+ actvsubs+  phones  + models  +
               eqpdays  +customer+ age1    + age2    + 
               children+ credita + creditaa+
               creditb  +creditc + creditde+ creditgy+ creditz + 
               prizmrur+ prizmub +
               prizmtwn +refurb  + webcap  + truck   + 
               rv      + occprof + occcler +
               occcrft  +occstud + occhmkr + occret  + 
               occself + ownrent + marryun +
               marryyes +marryno + mailord + mailres + 
               mailflag+ travel  + pcown   +
               creditcd +retcalls+ retaccpt+ newcelly+ newcelln+ 
               refer   + incmiss +
               income   +mcycle  + creditad+ setprcm + setprc  + retcall, 
               data=trainData, family=binomial(link=""logit""),
               control = list(maxit = 50))
ProbMdl = predict(LogMdl, testData, type = ""response"")
testData$churndep = rep(0,31047)  # replacing all churndep with zero
testData$churndep[ProbMdl&gt;0.5] = 1   # converting records with prob &gt; 0.5 as churned
table(testData$churndep,testData$churn)  # comparing predicted and actual churn
mean(testData$churndep!=testData$churn)    # prints the error %
</code></pre>

<p>Link for documentation of variables: <a href=""https://drive.google.com/file/d/0B9y78DHd3U-DZS05VndFV3A4Ylk/"" rel=""nofollow"">https://drive.google.com/file/d/0B9y78DHd3U-DZS05VndFV3A4Ylk/</a></p>

<p>Link for Dataset (.csv file) : 
<a href=""https://drive.google.com/file/d/0B9y78DHd3U-DYm9FOV9zYW15bHM/"" rel=""nofollow"">https://drive.google.com/file/d/0B9y78DHd3U-DYm9FOV9zYW15bHM/</a></p>

<p>I could not produce the output of <code>dput</code> since the data size is more than 5 MB. So I have zipped the file and placed in the above link. </p>

<p>Description of important variables:
* <code>churn</code> is the variable that says whether a customer churned or not.....
* <code>churndep</code> is the variable that needs to be predicted in the test data (validation data) and has to be compared with the <code>churn</code> variable which is already populated with actual churn.
For both churn and churndep, value of 1 means churned and 0 means not churned.</p>
"
"0.101115324337403","0.110341853688094","136012","<p>I want to do the following:</p>

<p>1) OLS regression (no penalization term) to get beta coefficients $b_{j}^{*}$; $j$ stands for the variables used to regress. I do this by </p>

<pre><code>lm.model = lm(y~ 0 + x)
betas    = coefficients(lm.model)
</code></pre>

<p>2) Lasso regression with a penalization term, the selection criteria shall be the Bayesian Information Criteria (BIC), given by</p>

<p>$$\lambda _{j} = \frac{\log (T)}{T|b_{j}^{*}|}$$</p>

<p>where $j$ stands for the variable/regressor number, $T$ for the number of observations, and $b_{j}^{*}$ for the initial betas obtained in step 1). I want to have regression results for this specific $\lambda_j$ value, which is different for each regressor used. Hence if there are three variables, there will be three different values $\lambda_j$. </p>

<p>The OLS-Lasso optimization problem is then given by</p>

<p>$$\underset{b\epsilon \mathbb{R}^{n} }{min} = \left \{ \sum_{t=1}^{T}(y_{t}-b^{\top} X_{t}  )^{2} + T\sum_{j=1}^{m} ( \lambda_{t}|b_{j}| )\right \}$$</p>

<p>How can I do this in R with either the lars or glmnet package? I cannot find a way to specify lambda and I am not 100% sure if I get the correct results if I run </p>

<pre><code>lars.model &lt;- lars(x,y,type = ""lasso"", intercept = FALSE)
predict.lars(lars.model, type=""coefficients"", mode=""lambda"")
</code></pre>

<p>I appreciate any help here.</p>

<hr>

<p><strong>Update:</strong></p>

<p>I have used the following code now:</p>

<pre><code>fits.cv = cv.glmnet(x,y,type=""mse"",penalty.factor = pnlty)
lmin    = as.numeric(fits.cv[9]) #lambda.min
fits    = glmnet(x,y, alpha=1, intercept=FALSE, penalty.factor = pnlty)
coef    = coef(fits, s = lmin)
</code></pre>

<p>In line 1 I use cross validation with my specified penalty factor ($\lambda _{j} = \frac{\log (T)}{T|b_{j}^{*}|}$), which is different for each regressor. 
Line 2 selects the ""lambda.min"" of fits.cv, which is the lambda that gives minimum mean cross-validation error.
Line 3 performs a lasso fit (<code>alpha=1</code>) on the data. Again I used the penalty factor $\lambda$.
Line 4 extracts the coefficients from fits which belong to the ""optimal"" $\lambda$ chosen in line 2.</p>

<p>Now I have the beta coefficients for the regressors which depict the optimal solution of the minimization problem </p>

<p>$$\underset{b\epsilon \mathbb{R}^{n} }{min} = \left \{ \sum_{t=1}^{T}(y_{t}-b^{\top} X_{t}  )^{2} + T\sum_{j=1}^{m} ( \lambda_{t}|b_{j}| )\right \}$$</p>

<p>with a penalty factor $\lambda _{j} = \frac{\log (T)}{T|b_{j}^{*}|}$. The optimal set of coefficients is most likely a subset of the regressors which I initially used, this is a consequence of the Lasso method which shrinks down the number of used regressors.</p>

<p>Is my understanding and the code correct? </p>
"
"0.0395593886064618","0.040291148201269","136563","<p>I am using the <code>nnls()</code> function from the <code>nnls package</code> in R to do a linear regression for regressors $x_i$ and observations $y$.
The function delivers beta coefficients $\beta_i\geq{0}, \forall i$. However, is it possible to apply the constraints only to <strong>some</strong> regressors so that</p>

<p>$$\beta_k \geq 0 \quad k \in \{1...,10\}, k\neq i \\ 
\beta_i \in \mathbb{R} \quad i \in \{1...,10\}$$</p>

<p>given that I have 10 regressor variables?</p>

<p><code>nnls()</code> offers the possibility to enforce some coefficients to be negative and others to be positive. I only want the positive constraint for some of them, the other ones can be either positive or negative.</p>
"
"0.0685188709827532","0.0697863157798853","136763","<p><img src=""http://i.stack.imgur.com/enCPt.png"" alt=""Data is a subset of my original DF. Snip of it is attached""></p>

<p>I use R, Party package in order to fit prediction model (""classifier"") for </p>

<p>""Converted.clicks"" as response variable.</p>

<p>The rest of vars are used as explaining variables in the model.</p>

<p>Here is the relevant part of my code:</p>

<pre><code>table(DF$Converted.clicks)

""0"" = 31456              
""1"" =  39  
""2"" =  6


Formula&lt;-Converted.clicks ~ Day.of.week 
                          + Device
                          + Keyword 
                          + Quality.score
                          + Network..with.search.partners. 
                          + Ad.group
                          + Match.type

ct&lt;-ctree(Formula,data=DF) 

####################################### 
</code></pre>

<h3>Issue:</h3>

<p>The Converted.clicks variable is highly imbalanced.The majority of the observations has </p>

<p>class ""zero"". So after ctree function is applied,all the predictions are ""zero"",there are </p>

<p>no classes ""1"" and ""2"" predicted.</p>

<h3>My questions are:</h3>

<ol>
<li><p>Is the classifier Decision Tree model is appropriate model to predict </p>

<p>as.factor(DF$Converted.clicks)?</p></li>
<li><p>If so, how can I balance the response var (i.e.to give the chance the two rest classes</p>

<p>""1"" and ""2"" to be predicted?) - if I need to use weights, I need an        </p>

<p>example,please.</p></li>
<li><p>Is there any other appropriate model to predict # of Converted.clicks? I understand </p>

<p>that Regression Decision Tree is only for continuous response variable, but in my case   </p>

<p>I have an integer response var, please advise.</p></li>
</ol>
"
"0.0791187772129236","0.080582296402538","137209","<p>When I run regression analysis I find it important to run some model diagnostics, such as detection of outliers, influential observations, multi-collinearity (much like these examples <a href=""http://www.statmethods.net/stats/rdiagnostics.html"" rel=""nofollow"">http://www.statmethods.net/stats/rdiagnostics.html</a>).</p>

<p>Example of Diagnostics I use:</p>

<pre><code>#Assessing the Assumption of Independence, using Durbin Watson Test
dwt(lmModel)

#Controlling for Multicollinearity
vif(lmModel)
1/vif(lmModel)
mean(vif(lmModel))
</code></pre>

<p>I have a sample with a lot of missing data across most variables. Thus, I need to use multiple imputations. </p>

<p>However, model diagnostics seems to be impossible to explore when using multiple imputations. So far, I have used the mice package and since I am still a novice at R my multiple imputation script basically looks like this:</p>

<pre><code>#Imputes 5 datasets    
imp &lt;- mice(myData, m=5)    

#Runs regression analysis on each imputed dataset    
fit &lt;- with(imp, lm(A ~ B + C))   

#Pools the results
pooled &lt;- pool(fit)
summary(pooled)
</code></pre>

<p>Is there some way to use the diagnostic test on the pooled data? or do I have to use diagnostic tests on each imputed dataset (before being pooled)? or is there some other smart way of solving this issue?</p>

<p>Thanks for your time</p>
"
"0.027972711943223","0.0284901441149095","137280","<p>While performing betaregression using betareg R package I noticed that the terms in my model are often significant, even with very small sample sizes. I tried the same model using glm with binomial family and logit link function, and I get very similar effect sizes but non-significant terms.</p>

<p>Can someone explain me how should I interpret this? Do the two models test significance in different ways? </p>

<p>NOTE: In my case the response variable is a proportion, so, although extremely unlikely, it could even take values 0 and 1.</p>

<pre><code>library(betareg)

Y=c(0.5283019, 0.4845361, 0.4974874, 0.6884735, 0.5967742, 0.6835443, 0.4152047, 0.4949495,
  0.6478873, 0.7695853, 0.4764398, 0.5780591, 0.5689655)
X=c(0.3616452, -0.4931525,  0.7890441,  0.7890441, -0.9205514,  0.7890441, -0.9205514,
 -0.9205514,  1.2164429,  1.2164429, -1.3479503, -1.3479503,  0.7890441)

summary(glm(Y~X, family=binomial('logit')))
summary(betareg(Y~X))
</code></pre>
"
"0.0685188709827532","0.0697863157798853","137424","<p>I am fitting a binomial logistic regression in R using glm. By chance, I have found out that if I change the order of my predictor variables, glm fails to estimate the model. The message I get is  <em>unexpected result from lpSolveAPI for primal test</em>. </p>

<p>I am using the safeBinaryRegression package, so I am confident there are no separation issues between my outcome and predictor variables. However, I am not so confident that there are no quasi-separation issues among my predictor variables. Am I correct that if this is the case, then I might be running into multicolinearity, and this is the source of glm not being able to fit the model? </p>

<p>If so, my question is for advice on how to approach the issue. Should I look for the predictor variables highly correlated and omit one of them? Is there any convenient way of doing so for 11 categorical predictors? </p>

<p>What I see right now: </p>

<pre><code>lModel &lt;- glm(mob_change ~ education + gender + start_age + income + dist_change + lu_change + dou_change + marriage + student2work + wh_change,
              data = regression_data, 
              family = binomial())
# Fine, and I can inspect the model. No predictor has std. error &gt; 1.05

# Now if I move the last variable (or any of the last three, for what I've tested) to
# be the first in predictor... 
lModel.3 &lt;- glm(mob_change ~ wh_change + gender + education + start_age + income + dist_change + lu_change + dou_change + marriage + student2work,
            data = regression_data, 
            family = binomial())

Error in separator(X, Y, purpose = ""find"") : 
  unexpected result from lpSolveAPI for primal test
</code></pre>
"
"0.0570990591522943","0.0697863157798853","137498","<p>I implemented both those tests with R, using the lmtest package.  Both tests directionally say the same thing (I think) with a very similar p-value of very close to 0.  But, are those tests saying that the underlying regression model's residuals are adequately linear.  Or are they saying just the opposite.  I know that the tests have slightly different nuances.  The Harvey-Collier test indicates whether the residuals are linear.  Meanwhile, the Rainbow test indicates whether the linear fit of the model is adequate even if some underlying relationships are not linear.  Any insight, on the interpretation of those results is greatly appreciated.   </p>

<p>I am posting the results of the tests below:</p>

<p>In R with lmtest package.</p>

<blockquote>
  <p>harvtest(Regression, order.by = NULL)</p>
</blockquote>

<pre><code>    Harvey-Collier test
</code></pre>

<p>data:  Regression
HC = 4.3826, df = 119, p-value = 2.543e-05</p>

<blockquote>
  <p>raintest(Regression, fraction = 0.5, order.by = NULL, center = NULL)</p>
</blockquote>

<pre><code>    Rainbow test
</code></pre>

<p>data:  Regression
Rain = 1.7475, df1 = 62, df2 = 58, p-value = 0.01664</p>
"
"0.0484501583111509","0.0493463771219827","137572","<p>The <code>plm::pcdtest</code> function implements Pesaran, (2004) General Diagnostic Tests for Cross Section Dependence in Panels. For example a test for a panel model works as such:</p>

<pre><code>data(Grunfeld, package = ""plm"")
## test on heterogeneous model (separate time series regressions)
pcdtest(inv ~ value + capital, data=Grunfeld,
    index = c(""firm"",""year""))
</code></pre>

<p>But if I want to test the cross-section dependance of only one variable in the panel for example <code>inv</code> in the example above, how can I do it in R?</p>
"
"0.0559454238864459","0.056980288229819","137650","<p>Linear Regression has the below set of Assumptions,</p>

<ol>
<li>The Y-Values (or the errors, ""e"") are independent!</li>
<li>The Y-Values can be expressed as a linear function of the X variable.</li>
<li>Variation of observations around the regression line (the residual SE) is constant (homoscedasticity).</li>
<li>For given value of X, Y values (or the error) are Normally distributed.</li>
</ol>

<p>Is there any empirical test instead of visual test in R that can be used to validate the Assumptions in 1, 2 and 4?</p>

<p>I can only find for Assumption 3,
Empirical Test: ncvTest() from CAR package
Value to be observed: p-value
Pass criteria: > 0.05</p>

<p>This is to make an automated script that can assist to choose the best model for a set non linear data using linear regression.</p>

<p>Do assist to point to any books or website if this has been discussed previously as my search has been futile. I find many approaches are visual based then empirical.</p>
"
"NaN","NaN","137695","<p>I created an orthogonal design in R using AlgDesign package. I got 14 profiles and some output: D =0.2519353; A = 5.462121; Ge = 0.748; Dea = 0.714.</p>

<p>How to interpret D , A , Ge and Dea? Also, In regression like we have r square, which value should we be looking for D-Efficiency and how much it should be in order for this design to be usable in a choice based conjoint/Discrete choice model experiment as alternatives? Any ideas?</p>
"
"0.0395593886064618","0.040291148201269","137901","<p>I am using the package MatchIt in R to perform propensity score matching. I have chosen to use nearest neighbor matching with a caliper of 0.2 and since in my case i have more cases than controls i have to use the replacement=TRUE option, so that a control can be used more than once.</p>

<p>The graphical histogram check is satisfying and the stand.mean differences are all small with a max of 0.03 (btw any other suggestions for testing the matching?)
I want to use the matched dataset to check the treatment effect after all the matching(perform logistic regression with mortality as outcome and treatment as explanatory variable now) and i am wondering if i should take into consideration the weights that were resulted from the matching. Since i used the replacement option not all observations have a weight of 1 anymore. Shall i use this somehow or can i just perform an unweighted final logistic regression on the matched data to estimate the effect of treatment.</p>
"
"0.12523984671879","0.127556501821839","138230","<p>I want to perform propensity score matching of observational data of an Intensive Care Unit in order to find out wheather hydroxyethyl starch is better or worse than colloids in terms of renal replacement therapy (RRT), Akute Kidney Injury (AKI) and mortality. </p>

<p>I use the MatchIt package in R (King et al. 2007 - <a href=""http://gking.harvard.edu/matchit"" rel=""nofollow"">http://gking.harvard.edu/matchit</a>). This package is quite well documented. But there are some things that I dont understand.
First I matched on sociodemographic covariates (as this seems standard protocol with matching): Gender, weight, height and age.
Nearest neighbor matching seems to have worked:</p>

<p>NN matching</p>

<pre><code>m.out.nn

Call: 
matchit(formula = treat1 ~ Geschlecht + Gewicht.kg + Groesse.cm + 
    Alter.bei.ITS.Aufn + BMI, data = hes.vs.kristall.clean, method = ""nearest"")

Sample sizes:
          Control Treated
All           359    3944
Matched       359     359
Unmatched       0    3585
Discarded       0       0

Treatment status treat1 is HES = yes , Colloids otherwise btw. I did a numerical balance check and balance actually WORSENED after matching. Overall as well as some of the covariates drastically:
Call:
matchit(formula = treat1 ~ Geschlecht + Gewicht.kg + Groesse.cm + 
    Alter.bei.ITS.Aufn + BMI, data = hes.vs.kristall.clean, method = ""nearest"")

Summary of balance for all data:
                   Means Treated Means Control SD Control Mean Diff eQQ Med eQQ Mean  eQQ Max
distance                  0.9168        0.9145     0.0131    0.0022  0.0018   0.0023   0.0388
Geschlecht                0.0041        0.0000     0.0000    0.0041  0.0000   0.0056   1.0000
Geschlechtm               0.6463        0.6100     0.4884    0.0363  0.0000   0.0362   1.0000
Geschlechtw               0.3496        0.3900     0.4884   -0.0403  0.0000   0.0390   1.0000
Gewicht.kg               79.1349       77.8930    18.2092    1.2419  2.0000   1.8462  30.0000
Groesse.cm              169.9184      169.9861    11.9693   -0.0677  0.0000   0.7604  30.0000
Alter.bei.ITS.Aufn       64.5950       63.4808    14.4918    1.1142  0.8000   1.2916   6.2000
BMI                      28.4858       27.8005    15.1559    0.6853  0.7080   2.1550 347.8520


Summary of balance for matched data:
                   Means Treated Means Control SD Control Mean Diff eQQ Med eQQ Mean  eQQ Max
distance                  0.9357        0.9145     0.0131    0.0212  0.0172   0.0212   0.0687
Geschlecht                0.0446        0.0000     0.0000    0.0446  0.0000   0.0446   1.0000
Geschlechtm               0.9053        0.6100     0.4884    0.2953  0.0000   0.2953   1.0000
Geschlechtw               0.0501        0.3900     0.4884   -0.3398  0.0000   0.3398   1.0000
Gewicht.kg               98.1744       77.8930    18.2092   20.2813 17.0000  20.2813  62.0000
Groesse.cm              164.7103      169.9861    11.9693   -5.2758  2.0000   5.4540  77.0000
Alter.bei.ITS.Aufn       72.5702       63.4808    14.4918    9.0894  7.0000   9.0894  26.5000
BMI                      44.1753       27.8005    15.1559   16.3748  6.7500  16.3748 258.9020

Percent Balance Improvement:
                   Mean Diff.   eQQ Med  eQQ Mean   eQQ Max
distance            -852.5593 -839.8420 -808.2029  -77.0723
Geschlecht          -998.6072    0.0000 -700.0000    0.0000
Geschlechtm         -714.0668    0.0000 -715.3846    0.0000
Geschlechtw         -742.6908    0.0000 -771.4286    0.0000
Gewicht.kg         -1533.1522 -750.0000 -998.5214 -106.6667
Groesse.cm         -7691.0845      -Inf -617.2161 -156.6667
Alter.bei.ITS.Aufn  -715.7611 -775.0000 -603.7093 -327.4194
BMI                -2289.4307 -853.3898 -659.8482   25.5712

Sample sizes:
          Control Treated
All           359    3944
Matched       359     359
Unmatched       0    3585
Discarded       0       0
</code></pre>

<p>How can this be possible?</p>

<p>I also did genetic matching (Sekhon 2011 - <a href=""http://sekhon.berkeley.edu/matching/"" rel=""nofollow"">http://sekhon.berkeley.edu/matching/</a>). This is a fancy algorithm that automatically optimizes covariate balance. There covariate balance has indeed improved (as it should have):</p>

<pre><code>Genetic matching
load(file=""m.out.genetic.RData"")
Numerical Balance Check 
summary(m.out.genetic)

Call:
matchit(formula = treat1 ~ Geschlecht + Gewicht.kg + Groesse.cm + 
    Alter.bei.ITS.Aufn, data = hes.vs.kristall.clean, method = ""genetic"")

Summary of balance for all data:
                   Means Treated Means Control SD Control Mean Diff eQQ Med eQQ Mean eQQ Max
distance                  0.9167        0.9147     0.0126    0.0021  0.0019   0.0022  0.0374
Geschlecht                0.0041        0.0000     0.0000    0.0041  0.0000   0.0056  1.0000
Geschlechtm               0.6463        0.6100     0.4884    0.0363  0.0000   0.0362  1.0000
Geschlechtw               0.3496        0.3900     0.4884   -0.0403  0.0000   0.0390  1.0000
Gewicht.kg               79.1349       77.8930    18.2092    1.2419  2.0000   1.8462 30.0000
Groesse.cm              169.9184      169.9861    11.9693   -0.0677  0.0000   0.7604 30.0000
Alter.bei.ITS.Aufn       64.5950       63.4808    14.4918    1.1142  0.8000   1.2916  6.2000


Summary of balance for matched data:
                   Means Treated Means Control SD Control Mean Diff eQQ Med eQQ Mean eQQ Max
distance                  0.9167        0.9164     0.0105    0.0003  0.0018   0.0021  0.0374
Geschlecht                0.0041        0.0000     0.0000    0.0041  0.0000   0.0056  1.0000
Geschlechtm               0.6463        0.6481     0.4782   -0.0018  0.0000   0.0364  1.0000
Geschlechtw               0.3496        0.3519     0.4782   -0.0023  0.0000   0.0392  1.0000
Gewicht.kg               79.1349       79.0556    15.9832    0.0793  2.0000   1.7801 30.0000
Groesse.cm              169.9184      170.0479    10.6992   -0.1296  0.0000   0.7703 30.0000
Alter.bei.ITS.Aufn       64.5950       64.7378    13.3160   -0.1428  0.8000   1.2440  6.2000

Percent Balance Improvement:
                   Mean Diff. eQQ Med eQQ Mean eQQ Max
distance              83.7418  3.8423   2.7923       0
Geschlecht             0.0000  0.0000  -0.5602       0
Geschlechtm           95.1066  0.0000  -0.5602       0
Geschlechtw           94.3414  0.0000  -0.5602       0
Gewicht.kg            93.6115  0.0000   3.5817       0
Groesse.cm           -91.3359  0.0000  -1.2969       0
Alter.bei.ITS.Aufn    87.1817  0.0000   3.6903       0

Sample sizes:
          Control Treated
All           359    3944
Matched       357    3944
Unmatched       2       0
Discarded       0       0
</code></pre>

<p>I also checked balance graphically and it did improve (despite being good pre-matching).</p>

<p>Now my questions are:</p>

<ol>
<li><p>Can I use the nearest neighbor matched data? How could I change this so that balance does improve? What kind of distance metric does Nearest neighbor matching use (by default) (Euclidean ?). Because with Euclidean the non-Boolean covariates (Gender) could be made more important than they are.</p></li>
<li><p>How can I perform analysis after matching? - How can I get the Average Treatment Effect (ATE) and the Average Treatment Effect on the Treated (ATET) in terms of HES for AKI, RRT and mortaility and does that make sense for these response variables (AKI, RRT and mortaility)? Or should I get the odds ratio for Akute Kidney Injury, renal replacement therapy and mortaility from the matched observational data? How do I get these values?
I know that MatchIt recommends using Zelig to get these values but that didn't seem to work with my data. 
Can I use logistic regression with the matched data to get the odds ratio of HES vs. Cristalloids of AKI, RRT and mortality ?</p></li>
</ol>
"
"0.0971882825368556","0.106600358177805","138506","<p>I have created regression models using robust regression - in particular, LTS and MM-estimators (using the R package robustbase).  I am now looking to creation prediction intervals.</p>

<p>The standard formula for prediction intervals for linear regression is:
$$
\hat{y_0} \pm t_{\alpha/2, n-p} \sqrt{\hat{\sigma^2}(1+x_0'(X'X)^{-1}x_0)}
$$
(see Montgomery and Peck, Introduction to Linear Regression Analysis, 1992)</p>

<p>For robust regression, obviously, the term $x_0'(X'X)^{-1}x_0$ cannot be used.  The Hat Matrix is different due to the weights.  It would seem to me that we can instead use the Hat Matrix modified with the weights:</p>

<p>$$
x_0'(X'WX)^{-1}x_0
$$</p>

<p>(see page 44 of the PhD thesis by Christopher Assaid at Virigina Tech)(<a href=""http://scholar.lib.vt.edu/theses/available/etd-3649212139711101/unrestricted/Ch6.PDF"" rel=""nofollow"">http://scholar.lib.vt.edu/theses/available/etd-3649212139711101/unrestricted/Ch6.PDF</a>)</p>

<p>We can approximate $\hat{\sigma^2}$ from the data as
$$
\hat{\sigma^2} = \frac{1}{df}\sum e_i^2
$$
where $df$ are the number of degrees of freedom.</p>

<p>I have three questions on this formulation for prediction intervals:</p>

<p>Is my formula correct?  Is the simple adjustment by factoring in the Weight Matrix enough to adapt the OLS formula for prediction intervals to robust regression.</p>

<p>If it is correct, does it apply to <em>all</em> types of robust regression, or just a subset?</p>

<p>Is it correct to estimate the variance as above?  If so, it would seem to me that  robust regression will always have a larger variance, and thus larger prediction intervals, than OLS.  The reason is is that OLS, by definition, is set up to minimize the residual sum of squares.  Robust regression, on the other hand, by definition of down-weighting potential outliers, even though it may give an overall better fit, will see a larger net residual sum of squares because of the contribution of the squared residuals from the outlier points.  Consequently, since the length of the interval is $\sqrt{\hat{\sigma}^2(1+\delta)}$ (where granted $\delta$ is not necessarily small, but the interval is always $\sigma$ plus something), if $\hat{\sigma}_{RR} &gt; \hat{\sigma}_{OLS}$, in general, the length of the interval for RR will be larger.  It seems counter-intuitive to me that if data is fit with both OLS and robust regression and prediction intervals are made, those from OLS will be by definition narrower and may even be contained within the robust ones.  It seems to thus minimize the power of robust regression.</p>

<p>Any answers to these questions or other suggestion/advice on creating prediction intervals for LTS and MM regression would be appreciated.</p>
"
"0.0927749898843639","0.0944911182523068","138908","<p>I am looking at the effects role has an opportunities to collaborate between groups in a social network. At a basic level the data are modeled as:</p>

<pre><code>relRatio~role
</code></pre>

<p>With relative ratio being the percentage of teammates who are part of the subject's normal group. The data I have come from multiple time slices over the years, with some of the subjects being polled two or more times. Not every subject has multiple entries, nor does every subject with multiple entries have the same number of entries. From some advice I received it was suggested that I test the differences between groups using a random effect ANOVA model, which would be modeled (in R) as</p>

<pre><code>relRatio~role+Error(subjectId)
</code></pre>

<p>After trying to read up more on random effects ANOVA, I started to get the impression that linear mixed effects models (with the lmer) package are preferred over random effects ANOVA, although I have yet to see a clear distinction between the two. This leads to my first question: Which approach is best for modeling my data?</p>

<p>If it involves using the random effects ANOVA, I would greatly appreciate it if someone could recommend a resource for the process of interpreting the results.</p>

<p>My second question is, if the better approach is to use the mixed effects models, which I have tried, why do I get striping in my residuals?</p>

<p><img src=""http://i.stack.imgur.com/vqWjn.png"" alt=""enter image description here""></p>

<p>One guide suggested that I am dealing with categorical data, which requires the use of logistic regression. However, the dependent variable for my data is continuous, and the IV is categorical, which I thought LMM are supposed to handle. This leads to my second question - does my residual plot indicate something is wrong with the way I have modeled my data?</p>
"
"0.0484501583111509","0.0493463771219827","138990","<p>Currently, I am doing the simulation with Autoregressive conditional Poisson (ACP). To test whether my parameter estimator is better than MLE (by comparing the biasness), first, I have to generate ACP process. The process is given as:</p>

<pre><code>X_t|F_(t-1)=Pois(lambda_t) and lambda_t=gamma+alpha * X_(t-1)+beta * lambda_(t-1)
</code></pre>

<p>The ACP package in R, just applicable for real data set (eg: Zeger 1988) and used for regression case. For me, i have to generate my own data set using ACP model. Could someone help me to generate autoregressive conditional Poisson (ACP) data using R software?</p>
"
"NaN","NaN","139756","<p>I'm working on a classification problem with continous and categorical predictors with Random Forests (RF). I'm particularly interested on RF as we avoid the specification of the functional form.</p>

<p>However when it comes to the partial dependance for categorical variables, I'm not sure how to interpret that. For instance, the partial dependence (with the command <code>partialPlot</code> in the <code>R</code> package <code>randomForest</code>) for a binary predictor would give two values, one for each category. My question is: how exactely do you interpret those values? The documentation of <code>partialPlot</code> is quite cryptic in this respect.</p>

<p>My confusion arises, I guess, because I'm used with usual logistic regression where with a dummy coding system you in general obtain the log-odds of the variable of interest against the baseline category. But with RF things are different... Any help is appreciable!</p>
"
"0.119276044494086","0.115408263553092","140600","<p>I'm running a fixed effects logistic regression in R. The model consists of a binary outcome and two binary predictors, with no interaction term. On the log-odds scale, and as an odds-ratio, the coefficient for one of the predictors (<code>carbf</code> in the mocked-up example below) indicates that the expected probability of Y=1 (""success"") is different between the two levels of the factor (i.e., the effect is significant). </p>

<p>When I use the <code>effects</code> package to get marginal predicted probabilities, the 95% CIs for the two levels of <code>carbf</code> overlap considerably, indicating there is no evidence of a difference in the expected probability of Y=1 between the two factor levels.</p>

<p>When I use the <code>mfx</code> package to get average marginal effects for the coefficients (i.e., for the expected <em>difference</em> in the probability of Y=1 between the two factor levels), I do get a significant difference.</p>

<p><strong>I'm confused as to whether this discrepancy is because:</strong> </p>

<p><strong>1) the output from the model and the <code>mfx</code> package is an expected <em>difference</em> in the probability of Y=1 between factor levels, rather than predicted probabilities for each level.</strong></p>

<p><strong>2) of the way the <code>effects</code> package is calculating the marginal effect.</strong> </p>

<p>In an effort to determine this, I modified the source code from the <code>mfx</code> package to give me average marginal effects for each level of the <code>carbf</code> factor. The 95% CIs for these predictions <em>do not</em> overlap, indicating a significant difference. This makes me wonder why I get such different results using the <code>effects</code> package. Or is it that I'm just confused about the difference between marginal effects for coefficients and for predicted probabilities?</p>

<pre><code>#####################################
# packages
library(effects)
library(mfx)
library(ggplot2)

# data
data(mtcars)
carsdat &lt;- mtcars
carsdat$carb &lt;- ifelse(carsdat$carb %in% 1:3, 0, 1)
facvars &lt;- c(""vs"", ""am"", ""carb"")
carsdat[, paste0(facvars, ""f"")] &lt;- lapply(carsdat[, facvars], factor)

# model
m1 &lt;- glm(vsf ~ amf + carbf, 
    family = binomial(link = ""logit""), 
    data = carsdat)
summary(m1)


#####################################
# effects package
eff &lt;- allEffects(m1)
plot(eff, rescale.axis = FALSE)
eff_df &lt;- data.frame(eff[[""carbf""]])
eff_df 

#   carbf   fit    se  lower upper
# 1     0 0.607 0.469 0.3808 0.795
# 2     1 0.156 0.797 0.0375 0.469


#####################################
# mfx package marginal effects (at mean)
mfx1 &lt;-logitmfx(vsf ~ amf + carbf, data = carsdat, atmean = TRUE, robust = FALSE)
mfx1 

#         dF/dx Std. Err.     z  P&gt;|z|
# amf1    0.217     0.197  1.10 0.2697
# carbf1 -0.450     0.155 -2.91 0.0037

# mfx package marginal effects (averaged)
mfx2 &lt;-logitmfx(vsf ~ amf + carbf, data = carsdat, atmean = FALSE, robust = FALSE)
mfx2

#         dF/dx Std. Err.     z  P&gt;|z|
# amf1    0.177     0.158  1.12 0.2623
# carbf1 -0.436     0.150 -2.90 0.0037


#####################################
# mfx source code
fit &lt;- m1
x1 = model.matrix(fit)  
be = as.matrix(na.omit(coef(fit)))
k1 = length(na.omit(coef(fit)))
fxb = mean(plogis(x1 %*% be)*(1-plogis(x1 %*% be))) 
vcv = vcov(fit)

# data frame for predictions
mfx_pred &lt;- data.frame(mfx = rep(NA, 4), se = rep(NA, 4), 
    row.names = c(""amf0"", ""amf1"", ""carbf0"", ""carbf1""))
disc &lt;- rownames(mfx_pred)

# hard coded prediction estimates and SE  
disx0c &lt;- disx1c &lt;- disx0a &lt;- disx1a &lt;- x1 
disx1a[, ""amf1""] &lt;- max(x1[, ""amf1""]) 
disx0a[, ""amf1""] &lt;- min(x1[, ""amf1""]) 
disx1c[, ""carbf1""] &lt;- max(x1[, ""carbf1""]) 
disx0c[, ""carbf1""] &lt;- min(x1[, ""carbf1""])
mfx_pred[""amf0"", 1] &lt;- mean(plogis(disx0a %*% be))
mfx_pred[""amf1"", 1] &lt;- mean(plogis(disx1a %*% be))
mfx_pred[""carbf0"", 1] &lt;- mean(plogis(disx0c %*% be))
mfx_pred[""carbf1"", 1] &lt;- mean(plogis(disx1c %*% be))
# standard errors
gr0a &lt;- as.numeric(dlogis(disx0a %*% be)) * disx0a
gr1a &lt;- as.numeric(dlogis(disx1a %*% be)) * disx1a
gr0c &lt;- as.numeric(dlogis(disx0c %*% be)) * disx0c
gr1c &lt;- as.numeric(dlogis(disx1c %*% be)) * disx1c
avegr0a &lt;- as.matrix(colMeans(gr0a))
avegr1a &lt;- as.matrix(colMeans(gr1a))
avegr0c &lt;- as.matrix(colMeans(gr0c))
avegr1c &lt;- as.matrix(colMeans(gr1c))
mfx_pred[""amf0"", 2] &lt;- sqrt(t(avegr0a) %*% vcv %*% avegr0a)
mfx_pred[""amf1"", 2] &lt;- sqrt(t(avegr1a) %*% vcv %*% avegr1a)
mfx_pred[""carbf0"", 2] &lt;- sqrt(t(avegr0c) %*% vcv %*% avegr0c)
mfx_pred[""carbf1"", 2] &lt;- sqrt(t(avegr1c) %*% vcv %*% avegr1c)  

mfx_pred$pred &lt;- rownames(mfx_pred)
    mfx_pred$lcl &lt;- mfx_pred$mfx - (mfx_pred$se * 1.96)
mfx_pred$ucl &lt;- mfx_pred$mfx + (mfx_pred$se * 1.96)

#          mfx    se   pred     lcl   ucl
# amf0   0.366 0.101   amf0  0.1682 0.563
# amf1   0.543 0.122   amf1  0.3041 0.782
# carbf0 0.601 0.107 carbf0  0.3916 0.811
# carbf1 0.165 0.105 carbf1 -0.0412 0.372

ggplot(mfx_pred, aes(x = pred, y = mfx)) +
    geom_point() +
    geom_errorbar(aes(ymin = lcl, ymax = ucl)) +
    theme_bw()
</code></pre>
"
"0.0927749898843639","0.0944911182523068","140622","<p>In the past I've run separate multiple regression models for <strong>many correlated independent variables</strong> and <strong>one dependent variable</strong>. For this I've been using the R package multtest (<a href=""http://www.bioconductor.org/packages/release/bioc/html/multtest.html"" rel=""nofollow"">http://www.bioconductor.org/packages/release/bioc/html/multtest.html</a>). This allowed me to compute adjusted p-values that took the correlation structure of my matrix of independent variables into account.</p>

<p>Now, I want to do the same thing but for <strong>several dependent variables</strong> as well. Put differently, I have a matrix Y with my dependent variables and a matrix X with my independent variables. From this I want to estimate <strong>X*Y regression models</strong>. Importantly, I want the <strong>adjusted p-values</strong> to account for the correlation structure of both the independent and the dependent variables. I'm looking forward to your suggestions. I've suggested to the authors of multtest before to extend their library to accomodate this case but this hasn't happened yet.</p>

<p><strong>Example</strong> (added):
Let's say I have gene expression data from 10 different tissues. Now I want to know if gene expression is correlated with a 100 different SNPs. This means I'm effectively testing 100*10 = 1000 hypothesis. However, all these hypothesis are not independent of each other. The SNPs might be correlated to each other due to linkage disequilibrium and gene expression might also be correlated accross different tissues, depending on their similarity. Therefore a Bonferroni correction of my p-values for this 1000 statistical tests would be too conservative. I'm looking for a way to <em>derive adjusted p-values that accounts for the above described dependencies within both the independent and the dependent variables</em>. </p>
"
"0.0685188709827532","0.0697863157798853","140929","<p>I wish to carry out logistic regression analysis using Firth's method, as implemented in R logistf package, to analyse SNP case/control data, for rare variants, whilst controlling for stratification using PCA eigenvectors as covariates. I wish to obtain p-values for each SNP (additive model).</p>

<p>Previously I have performed logistic regression analysis using PLINK:</p>

<pre><code>plink  --bfile snpdata --logistic --ci 0.95 --covar plink2_pca.eigenvec --covar-number 1-2 --out snpout
</code></pre>

<p>I would like to perform similar analysis, but wish to handle quasi-complete separation of the rare variants in my data sets.</p>

<p>I have followed a SNP analysis example provided with logistf and been able to obtain P values:</p>

<p>A very small sample of the snpdata (cases: case 1, control 0; SNP additive allele counts for minor allele: 0, 1, 2):</p>

<pre><code>           PC01         PC02 case exm226_A exm401_A exm4584_A exm146_A
1  -0.003092320 -0.002737810    1            0       0       0       0
2   0.015637300  0.008232330    1            0       0       0       0
3   0.006746730  0.008704400    1            0       1       0       1
4   0.001438270  0.000875751    0            0       0       0       0
5  -0.004161490  0.011407500    0            0       0       2       0

for(i in 1:ncol(snpdata)) snpdata[,i] &lt;-as.factor(snpdata[,i])
snpdata &lt;- snpdata[sapply(snpdata,function(x) length(levels(x))&gt;=2)] 
fitsnp &lt;- logistf(data=snpdata, formula=case~1, pl=FALSE)
add1(fitsnp)
</code></pre>

<p>But I am not clear on how to pass the eigenvectors in as covariates, or whether I can used the eigenvector values as is, or need to convert to these as factors?   </p>

<pre><code>fitsnp &lt;- logistf(data=snpdata,formula=case~(1+PC01+PC02), pl=FALSE)
</code></pre>

<p>I'm not sure if I am on the right track here and can't find a sufficiently similar example on-line to follow.</p>

<p>I would appreciate any assistance, or explanation if I am going completely wrong here.</p>

<p>Thanks in advance.</p>
"
"0.0395593886064618","0.040291148201269","141243","<p>I am trying to replicate what the function <code>dfbetas()</code> does in <strong><em>R</em></strong>.</p>

<p><code>dfbeta()</code> is not an issue... Here is a set of vectors:</p>

<pre><code>x &lt;- c(0.512, 0.166, -0.142, -0.614, 12.72)
y &lt;- c(0.545, -0.02, -0.137, -0.751, 1.344)
</code></pre>

<p>If I fit two regression models as follows:</p>

<pre><code>fit1 &lt;- lm(y ~ x)
fit2 &lt;- lm(y[-5] ~ x[-5])
</code></pre>

<p>I see that eliminating the last point results in a very different slope (blue line - steeper):</p>

<p><img src=""http://i.stack.imgur.com/4ypID.jpg"" alt=""enter image description here""></p>

<p>This is reflected in the change in slopes:</p>

<pre><code>fit1$coeff[2] - fit2$coeff[2]
-0.9754245
</code></pre>

<p>which coincides with the <code>dfbeta(fit1)</code> for the fifth value:</p>

<pre><code>   (Intercept)            x
1  0.182291949 -0.011780253
2  0.020129324 -0.001482465
3 -0.006317008  0.000513419
4 -0.207849024  0.019182219
5 -0.032139356 -0.975424544
</code></pre>

<p>Now if I want to standardize this change in slope (obtain <strong><em>dfbetas</em></strong>) and I resort to: </p>

<blockquote>
  <p>Williams, D. A. (1987) Generalized linear model diagnostics using the
  deviance and single case deletions. Applied Statistics 36, 181â€“191</p>
</blockquote>

<p>which I think may be one of the references in the R documentation under the package <strong>{stats}</strong>. There the formula for <strong><em>dfbetas</em></strong> is:</p>

<p>$\large \mathrm{dfbetas} (i, \mathrm{fit}) = \Large {(\hat{b} - \hat{b}_{-i})\over \mathrm{SE}\, \hat{b}_{-i}}$</p>

<p>This could be easily calculated in R:</p>

<pre><code>(fit1$coef[2] - fit2$coef[2])/summary(fit2)$coef[4]
</code></pre>

<p>yielding: <code>-6.79799</code> </p>

<p>The question is why I am not getting the fifth value for the slope in:</p>

<pre><code>dfbetas(fit1)

  (Intercept)            x
1  1.06199661  -0.39123009
2  0.06925319  -0.02907481
3 -0.02165967   0.01003539
4 -1.24491242   0.65495527
5 -0.54223793 -93.81415653!
</code></pre>

<p>What is the right equation to go from <strong><em>dfbeta</em></strong> to <strong><em>dfbetas</em></strong>?</p>
"
"0.0634361479695551","0.0753778361444409","141422","<p>I'm trying to estimate the value of a real estate upon its characteristics. To do so, I'm using the <code>Hedonic Model</code> and I'm doing the regression using <code>lmRob (R package: robust)</code>.</p>

<p>There is a problem with the model and I don't know how to solve it.</p>

<p>If the real estate for which I want to evaluate the price, has more characteristics than the real estates used for the model creation, the evaluation wouldn't be complete.</p>

<p><strong>Example:</strong></p>

<p>Let's say that I want to evaluate a house which has 4 rooms, 190 mÂ², an inside pool, 3 balconies and a sauna.</p>

<p>None of the other house from the neighborhood (houses which will be used for the regression) have an inside pool, the rest of the characteristics are similar.</p>

<p>This means that the house with the inside pool must have a bigger price than the other, but the regression will not take into account that the house has a inside pool.</p>

<p>My question is: how can I find the coefficient of the inside pool, in order to use it for the evaluation.</p>

<p>Thank you! </p>
"
"0.0559454238864459","0.056980288229819","141583","<p>I am working on a few (both simple and multivariable) regression analyses, and I have cases where the residuals are non-normal, to varying degrees. As I've understood, the Gauss-Markov theorem states that normality of residuals is not necessary for the coefficient point estimates to be correct, i.e., I can trust that the regression summary tells me the BLUE coefficient estimates. However, the standard errors may be biased, and thus, the corresponding t- and p-values may not be correct.</p>

<p>A previous question (<a href=""http://stats.stackexchange.com/questions/83012/how-to-obtain-p-values-of-coefficients-from-bootstrap-regression"">How to obtain p-values of coefficients from bootstrap regression?</a>) asked if it was possible to calculate p-values from bootstrapped coefficients and their CIs. However, if I bootstrap the coefficients and use the bootstrapped mean and the bootstrapped SE to re-calculate t- and p-values, would that be a sound approach?</p>

<p>Here is the code I use, in R:</p>

<pre><code># create linear model
mod &lt;- lm(Y~X,data=dataset); summary(mod)

# create function to return coefficient
mod.bootstrap &lt;- function(data, indices) {    
d &lt;- data[indices, ]
mod &lt;- lm(Y~X, data=d)  
return(coef(mod))
}

# set seed
set.seed(1234)

# begin bootstrap using the boot package
mod.boot &lt;- boot(data=dataset, statistic=mod.bootstrap, R=2000)

# now here is how I re-calculate t- and p-values
bootmean &lt;- mean(mod.boot$t[,2])
booter &lt;- sd(boot.t[,2])
tval &lt;- bootmean/booter
p &lt;- 2*pt(-abs(tval),df=mod$df.residual)
</code></pre>

<p>The new t- and p-values come out fairly close to the LM summary, albeit a bit more conservative, as expected. Is this something I could report? I am very new at this, so I can't really tell if my logic is valid or not.</p>

<p>Edit: Clarified a bit.</p>
"
"0.0395593886064618","0.040291148201269","141719","<p>I am using the package <code>caret</code> and GBM method for my predictions.</p>

<pre><code>fitControl &lt;- trainControl(## 10-fold CV
        method = ""repeatedcv"",
        number = 10,
        ## repeated ten times
        repeats = 10)

gbmGrid &lt;-  expand.grid(interaction.depth = c(1, 5, 9),
                        n.trees = (1:30)*50,
                        shrinkage = 0.1)

gbmFit &lt;- train(target ~ ., data = traindf,
                method = ""gbm"",
                trControl = fitControl,
                verbose = FALSE,
                ## Now specify the exact models 
                ## to evaludate:
                tuneGrid = gbmGrid,
                metric = ""ROC"")
</code></pre>

<p>There is one concept that I misunderstand. User guides of <code>caret</code> say that ""<strong>By default, the train function chooses the model with the largest performance value (or smallest, for mean squared error in regression models).</strong>"" So, when I run <code>ggplot(gbmFit)</code> I get this graphic:</p>

<p><img src=""http://i.stack.imgur.com/mETe8.png"" alt=""enter image description here""></p>

<p>When I type <code>gbmFit</code> in the console, I see that ""<strong>The final values used for the model were n.trees = 300, interaction.depth = 9 and shrinkage = 0.1</strong>""  How can I manually change these settings in order to make my predictions with different number of boosting iterations and trees?:</p>

<pre><code>predictions_gbm &lt;- predict(gbmFit, newdata = testdf, type = ""raw"")
</code></pre>
"
"0.0791187772129236","0.080582296402538","141771","<p>I've been using <code>nls()</code> to fit a custom model to my data, but I don't like how the model is fitting and I would like to use an approach that minimizes residuals in both x and y axes.  </p>

<p>I've done a lot of searching, and have found solutions for fitting <strong>linear</strong> models: </p>

<ul>
<li>via the <a href=""http://cran.r-project.org/web/packages/deming/index.html"" rel=""nofollow""><code>deming</code> package</a>, </li>
<li>various stackoverflow posts:
<ul>
<li><a href=""http://stackoverflow.com/questions/6889809/"">total-least-square-method-using-r</a>,</li>
<li><a href=""http://stackoverflow.com/questions/6872928/"">how-to-calculate-total-least-squares-in-r-orthogonal-regression</a>, </li>
</ul></li>
<li>and this very nice CrossValidated post: <a href=""http://stats.stackexchange.com/questions/13152/how-to-perform-orthogonal-regression-total-least-squares-via-pca"">How to perform orthogonal regression (total least squares) via PCA?</a>.  </li>
<li>I've also found a MATLAB solution: <a href=""http://stats.stackexchange.com/questions/110772/total-least-squares-curve-fit-problem"">Total least squares curve fit problem</a>, </li>
</ul>

<p>but these fit a second order polynomial and not a custom, user-defined model.</p>

<p>What I would like is something similar to <code>nls()</code> that does the x and y residual minimization.  This would allow me to enter my custom model.  Is anyone aware of any solution in R?</p>

<p>Here's an example, but please note that I'm seeking suggestions on a general solution for nonlinear total least squares regression, and not something specific to this dataset (this is just an example data from <a href=""http://stackoverflow.com/questions/21815745/modifying-a-curve-to-prevent-singular-gradient-matrix-at-initial-parameter-estim/21819042#21819042"">here</a>):</p>

<pre><code>df &lt;- structure(list(x = c(3, 4, 5, 6, 7, 8, 9, 10, 11), y = c(1.0385, 
1.0195, 1.0176, 1.01, 1.009, 1.0079, 1.0068, 1.0099, 1.0038)), .Names = c(""x"", 
""y""), row.names = c(NA, -9L), class = ""data.frame"")

(nlsfit &lt;- nls(y ~ a^b^x, data = df, start = c(a=0.9, b=0.6)))

library(ggplot2)
ggplot(df, aes(x=x, y=y)) + 
    geom_point() + 
    geom_smooth(method=""nls"", formula = y ~ a^b^x, se=F, start = list(a=0.9, b=0.6))
</code></pre>

<p>Does anyone have any suggestion for how I might proceed? </p>
"
"0.115334445976885","0.117467873474841","141844","<p>I tried to plot the results of an ordered logistic regression analysis by calculating the probabilities of endorsing every answer category of the dependent variable (6-point Likert scale, ranging from ""1"" to ""6""). However, I've received strange probabilities when I calculated the probabilities based on this formula: $\rm{Pr}(y_i \le k|X_i) = \rm{logit}^{-1}(X_i\beta)$.</p>

<p>Below you see how exactly I tried to calculate the probabilities and plot the results of the ordered logistic regression model (<code>m2</code>) that I fitted using the <code>polr</code> function (<code>MASS</code> package). The probabilities (<code>probLALR</code>) that I calculated and used to plot an ""expected mean score"" are puzzling as the expcected mean score in the plot increases along the RIV.st continuum while the coefficient for <code>RIV.st</code> is negative (-0.1636). I would have expected that the expected mean score decreases due to the negative main effect of <code>RIV.st</code> and the irrelevance of the interaction terms for the low admiration and low rivalry condition (LALR) of the current 2 by 2 design (first factor = <code>f.adm</code>; second factor = <code>f.riv</code>; dummy coding 0 and 1).</p>

<p>Any idea of how to make sense of the found pattern? Is this the right way to calculate the probabilities? The way I used the intercepts in the formula to calculate the probabilities might be problematic (cf., <a href=""https://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression"">Negative coefficient in ordered logistic regression</a>).</p>

<pre><code>m2 &lt;- polr(short.f ~ 1 + f.adm*f.riv + f.adm*RIV.st + f.riv*RIV.st, data=sampleNS)

# f.adm  = dummy (first factor of 2 by 2 design);
# f.riv  = dummy (second factor of 2 by 2 design);
# RIV.st = continuous predictor (standardized)
summary(m2)
Coefficients:
                Value Std. Error t value
f.adm1         1.0203    0.14959  6.8203
f.riv1        -0.8611    0.14535 -5.9240
RIV.st        -0.1636    0.09398 -1.7403
f.adm1:f.riv1 -1.2793    0.20759 -6.1625
f.adm1:RIV.st  0.0390    0.10584  0.3685
f.riv1:RIV.st  0.6989    0.10759  6.4953

Intercepts:
    Value    Std. Error t value 
1|2  -2.6563   0.1389   -19.1278
2|3  -1.2139   0.1136   -10.6898
3|4  -0.3598   0.1069    -3.3660
4|5   0.9861   0.1121     8.7967
5|6   3.1997   0.1720    18.6008
</code></pre>

<p>Here you see how I tried to calculate the probabilities (<code>probLALR</code>) for 1 of the 4 conditions of the 2 by 2 design:</p>

<pre><code>inv.logit  &lt;- function(x){ return(exp(x)/(1+exp(x))) }
Pred       &lt;- seq(-3, 3, by=0.01)
b = c(-2.6563,-1.2139,-0.3598,0.9861,3.1997) # intercepts of model m2
a = c(1.0203,-0.8611,-0.1636,-1.2793,0.0390,0.6989) # coefficients of m2
probLALR   &lt;- data.frame(matrix(NA,601,5))
for (k in 1:5){ 
    probLALR[,k] &lt;- inv.logit(b[k] + a[1]*0 + a[2]*0 + 
                               a[3]*Pred  + a[4]*0*0 + 
                               a[5]*Pred*0 + a[6]*Pred*0)
}

plot(Pred,probLALR[,1],type=""l"",ylim=c(0,1))   # p(1)
lines(Pred,probLALR[,2],col=""red"")             # p(1 or 2)
lines(Pred,probLALR[,3],col=""green"")           # P(1 or 2 or 3)
lines(Pred,probLALR[,4],col=""orange"")          # P(1 or 2 or 3 or 4)
lines(Pred,probLALR[,5],col=""orange"")          # P(1 or 2 or 3 or 4 or 5)

# option response functions:

orc = matrix(NA,601,6)
orc[,6] = 1-probLALR[,5]        # prob of 6
orc[,5]= probLALR[,5]-probLALR[,4]  # prob of 5
orc[,4]= probLALR[,4]-probLALR[,3]  # prob of 4
orc[,3]= probLALR[,3]-probLALR[,2]  # prob of 3
orc[,2]= probLALR[,2]-probLALR[,1]  # prob of 2
orc[,1]= probLALR[,1]           # prob of 1


plot(Pred,orc[,1],type=""l"",ylim=c(0,1))   # p(1)
lines(Pred,orc[,2],col=""red"")             # p(2)
lines(Pred,orc[,3],col=""green"")           # P(3)
lines(Pred,orc[,4],col=""orange"")          # P(4)
lines(Pred,orc[,5],col=""purple"")          # P(5)
lines(Pred,orc[,6],col=""purple"")          # P(6)

# mean score

mean = orc[,1]*1+orc[,2]*2+orc[,3]*3+orc[,4]*4+orc[,5]*5+orc[,6]*6
plot(Pred,mean,type=""l"",xlab=""RIV.st"",ylab=""expected mean score"",ylim=c(1,6))  
</code></pre>
"
"0.027972711943223","0.0284901441149095","142312","<p>I'm having an interesting dilemma with the <code>neuralnet</code> and <code>nnet</code> packages in <code>R</code>.  I recently tried a series of feed-forward neural networks giving each the same data sets and every single time, no matter how I tweak the algorithms, hidden layers, neuron sizes, maximum iterations or error thresholds, both functions keep converging their predictions to approximately the mean of whatever they are training on.</p>

<p>A linear regression does way better for each series in terms of fit, and both of these packages seem to do a better job fitting random data from the <code>rnorm</code> function than real data.  In regards to the mathematics of the problem, what could be causing this and how should I resolve?  I have sample code below and can paste a sample dataset below if requested.  Thanks!</p>

<pre><code>model6 &lt;- neuralnet(
    target ~ 1 + majorholiday + mon + sat + sun + thu + tue + wed + tickets + l1_target + l7_target, data = data_nn
    ,algorithm = ""rprop+"", hidden = c(8), stepmax = 500000
    ,err.fct = ""sse"", threshold = 0.01, lifesign = ""full"", lifesign.step = 100
    , linear.output= T)
</code></pre>

<p><strong>EDIT</strong></p>

<p>A user requested I paste some data.  Here is one set below and I just tried the same code again prior to uploading and the same thing happens, converges to the mean of <code>target</code> at about 17.45</p>

<pre><code>    row.names   target  majorholiday    mon sat sun thu tue wed backtickets l1_target   l7_target
1   8   18.976573088    0   0   0   0   0   0   0   13806   18.114001584    36.521334684
2   9   20.701716096    0   1   0   0   0   0   0   15308   18.976573088    35.477867979
3   10  25.014573616    0   0   1   0   0   0   0   13439   20.701716096    28.173601042
4   11  15.706877377    1   0   0   0   0   0   0   11283   25.014573616    27.602288128
5   12  19.633596721    0   0   0   0   1   0   0   12272   15.706877377    13.801144064
6   13  20.049395337    0   0   0   0   0   1   0   9528    19.633596721    32.777717152
7   14  21.720178282    0   0   0   1   0   0   0   13747   20.049395337    18.114001584
8   15  23.390961226    0   0   0   0   0   0   0   15277   21.720178282    18.976573088
9   16  16.707829447    0   1   0   0   0   0   0   16058   23.390961226    20.701716096
10  17  15.872437975    0   0   1   0   0   0   0   14218   16.707829447    25.014573616
11  18  23.295531996    1   0   0   0   0   0   0   11249   15.872437975    15.706877377
12  19  22.363710716    0   0   0   0   1   0   0   13993   23.295531996    19.633596721
13  20  24.227353276    0   0   0   0   0   1   0   13402   22.363710716    20.049395337
14  21  20.500068156    0   0   0   1   0   0   0   14244   24.227353276    21.720178282
15  22  26.090995836    0   0   0   0   0   0   0   14502   20.500068156    23.390961226
16  23  18.636425597    0   1   0   0   0   0   0   16296   26.090995836    16.707829447
17  24  15.840961757    0   0   1   0   0   0   0   13694   18.636425597    15.872437975
18  25  20.650050308    1   0   0   0   0   0   0   10774   15.840961757    23.295531996
19  26  13.467424114    0   0   0   0   1   0   0   12348   20.650050308    22.363710716
20  27  19.752222033    0   0   0   0   0   1   0   12936   13.467424114    24.227353276
21  28  27.832676502    0   0   0   1   0   0   0   14342   19.752222033    20.500068156
22  29  18.854393759    0   0   0   0   0   0   0   14390   27.832676502    26.090995836
23  30  10.773939291    0   1   0   0   0   0   0   16724   18.854393759    18.636425597
24  31  12.569595839    0   0   1   0   0   0   0   14091   10.773939291    15.840961757
25  32  28.153882107    1   0   0   0   0   0   0   11250   12.569595839    20.650050308
26  33  24.400031160    0   0   0   0   1   0   0   12803   28.153882107    13.467424114
27  34  21.584642949    0   0   0   0   0   1   0   13318   24.400031160    19.752222033
28  35  27.215419370    0   0   0   1   0   0   0   14193   21.584642949    27.832676502
29  36  21.584642949    0   0   0   0   0   0   0   14312   27.215419370    18.854393759
30  37  15.015403791    0   1   0   0   0   0   0   16445   21.584642949    10.773939291
31  38  26.276956633    0   0   1   0   0   0   0   13753   15.015403791    12.569595839
32  39  15.139500902    1   0   0   0   0   0   0   11619   26.276956633    28.153882107
33  40  12.467824272    0   0   0   0   1   0   0   14006   15.139500902    24.400031160
34  41  21.373413039    0   0   0   0   0   1   0   14098   12.467824272    21.584642949
35  42  8.015029889 0   0   0   1   0   0   0   14462   21.373413039    27.215419370
36  43  16.030059779    0   0   0   0   0   0   0   15367   8.015029889 21.584642949
37  44  19.592295285    0   1   0   0   0   0   0   17868   16.030059779    15.015403791
38  45  18.701736409    0   0   1   0   0   0   0   15052   19.592295285    26.276956633
39  46  16.002499062    1   0   0   0   0   0   0   10035   18.701736409    15.139500902
40  47  16.943822536    0   0   0   0   1   0   0   13708   16.002499062    12.467824272
41  48  11.295881691    0   0   0   0   0   1   0   13463   16.943822536    21.373413039
42  49  19.767792959    0   0   0   1   0   0   0   13998   11.295881691    8.015029889
43  50  19.767792959    0   0   0   0   0   0   0   14745   19.767792959    16.030059779
44  51  16.943822536    0   1   0   0   0   0   0   16156   19.767792959    19.592295285
45  52  14.119852113    0   0   1   0   0   0   0   13552   16.943822536    18.701736409
46  53  22.869570079    1   0   0   0   0   0   0   11554   14.119852113    16.002499062
47  54  10.481886286    0   0   0   0   1   0   0   13437   22.869570079    16.943822536
48  55  19.057975066    0   0   0   0   0   1   0   14076   10.481886286    11.295881691
49  56  20.010873819    0   0   0   1   0   0   0   14567   19.057975066    19.767792959
50  57  9.528987533 0   0   0   0   0   0   0   14277   20.010873819    19.767792959
51  58  21.916671326    0   1   0   0   0   0   0   16545   9.528987533 16.943822536
52  59  11.000000000    1   0   0   0   0   0   1   15599   21.916671326    14.119852113
53  60  17.000000000    0   0   0   0   1   0   1   17463   11.000000000    22.869570079
54  61  10.000000000    0   0   0   0   0   1   1   17935   17.000000000    10.481886286
55  62  20.000000000    0   0   0   1   0   0   1   18357   10.000000000    19.057975066
56  63  19.000000000    0   0   0   0   0   0   1   19246   20.000000000    20.010873819
57  64  17.000000000    0   1   0   0   0   0   1   21234   19.000000000    9.528987533
58  65  11.000000000    0   0   1   0   0   0   1   18493   17.000000000    21.916671326
59  66  9.000000000 1   0   0   0   0   0   1   15315   11.000000000    11.000000000
60  67  22.000000000    0   0   0   0   1   0   1   17841   9.000000000 17.000000000
61  68  9.000000000 0   0   0   0   0   1   1   18312   22.000000000    10.000000000
62  69  11.000000000    0   0   0   1   0   0   1   17880   9.000000000 20.000000000
63  70  5.000000000 0   0   0   0   0   0   1   19371   11.000000000    19.000000000
64  71  15.000000000    0   1   0   0   0   0   1   21696   5.000000000 17.000000000
65  72  12.000000000    0   0   1   0   0   0   1   18829   15.000000000    11.000000000
66  73  10.000000000    1   0   0   0   0   0   1   14749   12.000000000    9.000000000
67  74  15.000000000    0   0   0   0   1   0   1   17928   10.000000000    22.000000000
68  75  7.000000000 0   0   0   0   0   1   1   18254   15.000000000    9.000000000
</code></pre>
"
"0.0395593886064618","0.040291148201269","142316","<p>As a programmer I have used the spdep package successfully for spatial filtering. But would appreciate it if someone could offer a description (preferably with supporting references) of how this concept works. Let's suppose I have a standard linear regression model with an integer response variable and 3 or more predictors comprising real values:</p>

<ol>
<li><p>According to <a href=""http://books.google.de/books/about/Spatial_Statistics_and_Geostatistics.html?id=xSPMCHuchEgC&amp;redir_esc=y"" rel=""nofollow"">Chun and Griffith</a>, a spatial filter would be constructed from missing predictors which are spatially correlated, and which will help to model the autocorrelation of the all observations. How is the autocorrelation among multivariate predictors taken into account?</p></li>
<li><p>How does this approach compare to the spatial Durbin model?</p></li>
<li><p>Does this concept have any relationship with the <a href=""http://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem"" rel=""nofollow"">Perronâ€“Frobenius theorem</a> from linear algebra?</p></li>
</ol>
"
"0.027972711943223","0.0284901441149095","142372","<p>Iâ€™m trying to find a feature selection package in <code>R</code> that can be used for regression. Most of the packages implement their methods for classification using a factor or class for the response variable. In particular Iâ€™m interested if thereâ€™s something using random forest for that purpose. Also if someone knows some good paper in this field would be welcome.</p>
"
"0.027972711943223","0.0284901441149095","142580","<p>I have calculated simple slopes for a multiple regression using simpleSlope from package pequod(). Is there a way to easily compute an effect size for a single simple slope analysis from the output?</p>

<p>Example:</p>

<pre><code>library(pequod)

dat &lt;- data.frame(""DV""=rnorm(20,0,1), ""PRED""=rnorm(20,0,1),
                  ""MOD1""=rnorm(20,0,1), ""MOD2""=rnorm(20,0,1))

fit &lt;- lmres(DV~PRED*MOD1*MOD2, dat)

slopes &lt;- simpleSlope(fit, pred=""PRED"", mod1=""MOD1"", mod2=""MOD2"")
</code></pre>
"
"0.0685188709827532","0.0697863157798853","142594","<p>I'm executing a few test runs of a lasso regression with the glmnet package in R using the diabetes dataset (<a href=""http://www4.stat.ncsu.edu/~boos/var.select/diabetes.tab.txt"" rel=""nofollow"">http://www4.stat.ncsu.edu/~boos/var.select/diabetes.tab.txt</a>). Iâ€™m choosing a single lambda value = 0.1 (not a sequence of lambda values) and alpha=1:</p>

<pre><code>&gt; set.seed(1);
  elasticnet_fit &lt;- glmnet(data_matrix, y, family=""gaussian"", lambda=.1, alpha=1); 
  coef(elasticnet_fit);
11 x 1 sparse Matrix of class ""dgCMatrix""
                       s0
(Intercept) -299.82915923
x1            -0.02102016
x2             5.63508548
x3             1.10288671
x4            -0.73514322
x5             0.42465094
x6            -0.03356997
x7             5.39641052
x8            59.76829212
x9             0.27508521
x10          -22.34877815
</code></pre>

<p>I noticed the glmnet documentation reads as follows when selecting lambda values:</p>

<blockquote>
  <p>WARNING: use with care. Do not supply a single value for lambda (for
  predictions after CV use predict() instead). Supply instead a
  decreasing sequence of lambda values. glmnet relies on its warms
  starts for speed, and its often faster to fit a whole path than
  compute a single fit.</p>
</blockquote>

<p>Are the coefficient estimates invalid when supplying only a single lambda value rather than a sequence? (Cross-posting from Stack Overflow.)</p>
"
"0.0884574820723792","0.0900937462695559","143165","<p>I am attempting to build a model for the following dataset:</p>

<p>Level 1 Observations (Product-Level): 89000<br>
Level 2 Observations (""BU_SBU"" Department-Level): 135</p>

<p>Unfortunately I cannot share a sample of my data, since it is confidential.</p>

<p>The dependent variable in the model is a percentage (Delivery Reliability, 0-100%). Fixed effects include roughly 20 variables at level 1 and 5 variables at level 2. The only random effects are the intercepts at level 2. Having run the regression, I have a number of questions regarding the violation of model assumptions which I cannot answer myself:</p>

<ol>
<li><p>Constant variance of residuals: The graphic shows that there appears to be an upper- and lower-bound of the residuals. My guess is that this is due to the limitation of the dependent variable. But do the upper- and lower-bounds shown in the graphic actually indicate a violation of model assumptions? I have also run a GLMER model with a <code>binomial(logit)</code>-link but this did not resolve the issue. The diagnostic plots look almost identical in all three cases.</p></li>
<li><p>Distribution of residuals: Is there a way to compute confidence intervals for residual QQ-plots of LMER models? And is it possible to compute heteroskedasticity-robust standard errors via the lme4-package?</p></li>
<li><p>Normal distribution of level-2 intercepts: The level-2 intercepts do not appear to be Normally distributed. Is this an issue and if so, how can I resolve it?</p></li>
</ol>

<p>I would greatly appreciate, if someone could help me at least with some of these questions. I am currently stuck and was not able to find any resources that provide answers. I am also grateful for recommendations to helpful literature.  </p>

<p><img src=""http://i.stack.imgur.com/WK1NL.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/ZU4x5.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/GUgmq.png"" alt=""enter image description here""></p>

<p><a href=""https://www.dropbox.com/sh/dji9f95kg88i8kg/AACjQCTA4lW86aYJNa8CHaUha?dl=0"" rel=""nofollow"">Dropbox to Diagnostic Plots</a></p>
"
"NaN","NaN","143328","<p>I am developing a logistic regression model where perfect variable separation occurs. I want to calculate a cutoff from this data. Interestingly, the length of the slot <code>cutoffs</code> of <code>pred.obj</code> is only 5, as well as the slots <code>fp</code>, <code>tp</code>, <code>tn</code>, <code>fn</code>, <code>n.pos.pred</code> and <code>n.neg.pred</code>. I expect it to have the same length as the observations. </p>

<p>Has anybody an explanation for this? (And knows how to solve it?) </p>

<p>MWE:</p>

<pre><code> library(ROCR) # package for prediction/performance functions
 y &lt;- c(0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0)
 x &lt;- c(-5, 5, 3, -2, 4, 3, -8, 2, 5, 3, -5, -3, -2)
 model &lt;- glm(as.factor(y) ~ x, family = ""binomial"")
 preds &lt;- predict(model, type = ""response"")
 (pred.obj &lt;- prediction(preds, y))
 perf &lt;- performance(pred.obj, ""acc"")
 (cutoff &lt;- perf@x.values[[1]][which.max(perf@y.values[[1]])])
</code></pre>
"
"0.0685188709827532","0.0697863157798853","143570","<p>I have a dataset of retail products which contains weekly sales for 12 different items in a single category. For each item, I have three dummy variables representing different types of advertising (FrontCover,BackCover,Inside) that could be run for that item that week. The data is weekly, and seasonal for a year so I have the frequency set to 52.</p>

<p>I have a two part question:</p>

<p><strong>1. How can I convert the advertising coefficients to % lifts when the data is seasonal?</strong></p>

<p>What I can do now is subset the data for a single item, and run multiple regression using tslm() from the R forecast package and read the coefficients to determine the lift for that item. However with tslm() I also have 51 other seasonal coefficients. How can I state this as a %?</p>

<pre><code>    x
(Intercept) 601.7857143
data.subset$Ad.Front	249.4285714
    data.subset$Ad.Inside    243.4285714
data.subset$Ad.Back 78
season2 92.5
season3 113.2142857
season4 -31.71428571
season5 189.7142857
season6 -25.21428571
season7 124.5
season8 77.21428571
season9 71.71428571
season10    -25.5
season11    -161.2142857
season12    47.21428571
season13    -13
season14    -47.5
season15    9.214285714
season16    -33.5
season17    -76.5
season18    54.71428571
season19    52.71428571
season20    -90.78571429
season21    -27.28571429
season22    -124.2857143
season23    -101.2857143
season24    -23.71428571
season25    38.71428571
season26    -225.2142857
season27    -47.78571429
season28    -46
season29    27
season30    43.28571429
season31    1498.5
season32    791.7142857
season33    666.7857143
season34    1913.5
season35    1657
season36    119
season37    -205.7857143
season38    -420.2142857
season39    -152.7857143
season40    -360.2142857
season41    -123.7857143
season42    77.21428571
season43    -40.78571429
season44    -10.78571429
season45    -48.78571429
season46    73.21428571
season47    81.21428571
season48    26.21428571
season49    -1.785714286
season50    25.21428571
season51    -105.2142857
season52    -161.2142857
</code></pre>

<p><strong>2. My second question is how do I extrapolate this for the entire category?</strong>
If I have the above information for 12 items, how can I look at the coefficients collectively? That is, I want to say ""Front page advertising has x% lift for Category A"".</p>
"
"0.10833784750436","0.110341853688094","144348","<p><strong>Note</strong><br>
I've edited the example to be more intuitive and closer to my real data</p>

<p><strong>Intro</strong><br>
I've got data on customers purchases and with it am trying to predict which customers are more likely to make next purchase at some time in the future. Data consist of customers' features like sex, age etc., and their prior purchase behavior like total spendings and number of orders, one row for every customer. The last two columns are the indicator of wether he have made next purchase or not, and number of days till purchase or till today, in case of no purchase.  </p>

<p><strong>Problem</strong><br>
I am building a Cox regression and then want to predict probability of next purchase for individual observations in, say, 30 days from last purchase.</p>

<p>Reproducible example:</p>

<pre><code>library(survival)
library(rms)
library(pec)
library(ggplot2)

data(cost)

# split into train and test sets
set.seed(1)
ind &lt;- sample(1:nrow(cost), 100)
test.set &lt;- cost[ind, ]
train.set &lt;- cost[-ind, ]
</code></pre>

<p>For Cox regression I use <code>cph</code> from <code>rms</code> package, for prediction - <code>predictSurvProb</code> from <code>pec</code> package as suggested in <a href=""http://stats.stackexchange.com/a/36016/72401"">this</a> discussion.</p>

<pre><code># fit Cox model
fit &lt;- cph(Surv(time, status) ~ ., data = train.set, surv = TRUE)

# predict pobability of event in 30 days
test.set$predicted.probs &lt;- 1 - predictSurvProb(fit, newdata = test.set, times = 1000)[, 1]
</code></pre>

<p>Thus, for every customer we have his probability of making a purchase in 1000 units of time. I want to validate prediction against real data.  </p>

<p><strong>Now to the question:</strong> what is the best/valid way to do it?  </p>

<p>Here's what I've tried:<br>
I expect that valid model would predict higher probabilities for customers who made their purchase earlier so correlation between probabilities and number of days to event' would be negative and strong (e.g. for customer who actualy made next purchase in 2 days, probability of buying in 30 days would be very high).</p>

<pre><code>with(test.set, cor(predicted.probs, time))
# [1] -0.5221604
</code></pre>

<p>Also, probability for those who made purchase (status = 1) would be higher than for those who didn't.</p>

<pre><code>with(test.set, by(predicted.probs, status, mean))
# status: 0
# [1] 0.2371247
# --------------
#   status: 1
# [1] 0.4083586
</code></pre>

<p>And a graph to eyeball my assumptions:</p>

<pre><code>qplot(data = test.set, x = time, y = predicted.probs, color = time)
</code></pre>

<p>Am I correct in my reasoning?</p>
"
"0.10466430427046","0.106600358177805","145684","<p>My problem (question at the end) is to calculate confidence interval (CI) (NOT prediction interval) of the response of a nonlinear model.</p>

<p>I am working with R but this question is not R-specific.</p>

<p>I want to model some data after the following equation (model):</p>

<p>Y ~ a * X^b/b</p>

<p>First, I estimate the parameters a and b through nonlinear regression (using R's ""nls()""), which yields estimates and error on the corresponding estimate.</p>

<pre><code> Nonlinear regression model
 model: Y ~ (A * X^B/B)
  data: data.frame(X = X, Y = Y)
    A      B 
  7.4154 0.6041 
   residual sum-of-squares: 88983
</code></pre>

<p>Then I calculate 95% CI for a and b (using confint(nlm &lt;- nls(Y ~  A * X^B/B, start=list(A=1,B=1)))</p>

<pre><code> &gt; confint(nlm)
 Waiting for profiling to be done...
         2.5%     97.5%
 A 1.21719414 11.549562
 B 0.08583486  1.482389
</code></pre>

<p>In order to calculate 95%CI for Y, given some fixed, certain value of X, my first idea was to propagate uncertainties on a and b to Y through the model equation. This yields some value for 95% CI of Y, given X.</p>

<p>I then came accross the ""propagate"" package that proposes to calculate 95%CI of Y, given X, ""based on asymptotic normality"" (citation from ""<a href=""http://127.0.0.1:22638/library/propagate/html/predictNLS.html"" rel=""nofollow"">http://127.0.0.1:22638/library/propagate/html/predictNLS.html</a>""). However this method yields a VERY different 95%CI. </p>

<p><strong>My question is: Why aren't these two CI equal ?</strong></p>

<p>A worked example (with some random equation that just crossed my mind):</p>

<p>Values needed for error propagation : A, CI(A), B, CI(B), X, CI(X) :</p>

<p>Parameters (A &amp; B)' estimates and 95%CI were calculated from 
     confint(nlm &lt;- nls(Y ~  A * X^B/B, start=list(A=1,B=1))</p>

<p>X was then fixed at 30 for the sake of the argument, and considered error-free.</p>

<pre><code>                A         B  X
 value   7.415380 0.6041404 30
 95% CI  5.166184 0.6982769  0
</code></pre>

<p>The general formula for uncertainties propagation (works for sd, se, ci95%) is :</p>

<p>Y=f(Ai | i = 1 to n)
=> delta(Y) = sqrt( sum( ( dY/dAi * delta(Ai) )^2 ) )</p>

<p>The equation being        Y =  A * X^B/B </p>

<p>Partial derivatives are then: </p>

<pre><code> dF/dA  =  X^B/B
 dF/dB  =  A * (X^B * log(X))/B - A * X^B/B^2
 dF/dX  =  A * (X^(B - 1) * B)/B
</code></pre>

<p>Then</p>

<pre><code> dF/dA = 12.9196498927581
 dF/dB = 167.269472901412
 dF/dX = 1.92930443474376
</code></pre>

<p>This yields</p>

<pre><code> Y = 95.8041099173585 +- 134.526084150286
</code></pre>

<p>However, when using the predictNLS() function from ""propagate"" R package:</p>

<pre><code> predictNLS(nlm, newdata=data.frame(X=30), interval = ""confidence"")$summary

 Propagating predictor value #1 ...
   Prop.Mean.1 Prop.Mean.2 Prop.sd.1 Prop.sd.2 Prop.2.5%
      95.80411    102.8339  20.89399  24.86949  51.89104
   Prop.97.5% Sim.Mean   Sim.sd Sim.Median  Sim.MAD  Sim.2.5%
     153.7767  93.5643 1712.894   97.85209 21.98703 -117.3541
   Sim.97.5%
    210.3916
</code></pre>

<p>Which yields</p>

<pre><code>    Y = 95.80411 +- (153.7767-51.89104)/2
 =&gt; Y = 95.80411 +- 50.94283
</code></pre>

<p>Obviously I must have missed / misunderstood some essential information about CI of response variable, because I believe the person who coded the predictNLS() function must be way more knowledgeable than me about it.</p>

<p>Thanks in advance for your explanations.</p>
"
"0.137891567933076","0.145643816250884","145849","<p>Iâ€™ve got a question concerning the R package <em>strucchange</em> that I use for testing and dating structural breaks in my PhD thesis.  To be specific, I use the generalized fluctuation test framework with CUSUM/MOSUM and in particular Moving Estimates (<strong>ME</strong>) tests for my analysis. Thus, the following description focuses on the ME test, but in principle is more general to all fluctuation tests.</p>

<p><strong>The problem:</strong> I am testing time series data for structural breaks with the ME test that draws on the function <strong>efp</strong> provided by strucchange. Given the nature of time series data, I want to tackle potential heteroskedasticity and autocorrelation in the data. Strucchange provides some functionality with respect to calculating <em>heteroskedasticity</em> (<strong>HC</strong>) and <em>autocorrelation</em> (<strong>HAC</strong>) consistent covariance matrices,  e.g., the approaches suggested by Newey-West (1987) or Andrews (1991). </p>

<p>However, this functionality in strucchange is limited to the function <strong>gefp</strong> that calculates Generalized Empirical M-Fluctuation Processes that as far as I know does not allow to perform estimates-based tests such as the ME test. Thus, I cannot use <strong>efp</strong> to estimate ME tests (or other tests that are available in this function) using HAC covariance matrices. </p>

<p><strong>The question:</strong> Does anybody know how I could make use of the <strong>efp</strong> function in <em>strucchange</em> for testing and dating structural changes but use HAC covariance matrices to take heteroskedasticity and autocorrelation into account? Maybe there is some way to use the sandwich package for this?</p>

<p><strong>Many thanks for any help!</strong></p>

<p>Here is a minimal working example to show the problem</p>

<pre><code>library(foreign)
library(strucchange)

data(""Nile"")

#using the function efp to perform a moving estimates test
#assuming sperical disturbances
ocus.nile &lt;- efp(Nile ~ 1, type = ""ME"")
sctest(ocus.nile)

#applying the vcov function with the kernHAC option to take heteroskedasticity and autocorrelation does not work, i.e., the option is not used and the result is the same
sctest(ocus.nile, vcov=kernHAC)

#using the function gefp to perform a generalized M-fluctuation process however works with vcov
#assuming spherical disturbances
ocus.nile2 &lt;- gefp(Nile ~ 1, fit = lm)
sctest(ocus.nile2)

#controlling for heteroskedasticity and autocorrelation using an appropriate covariance matrix changes the result, i.e. works
ocus.nile2 &lt;- gefp(Nile ~ 1, fit = lm, vcov= kernHAC)
sctest(ocus.nile2)
</code></pre>

<p><strong>Some background</strong></p>

<p>Though probably not necessary, here is some more in-depth background about the problem for the interested reader (and the archive). The formulas are taken from Zeileis et al., 2005, â€Monitoring structural change in dynamic econometric modelsâ€. </p>

<p>The ME test is used to detect structural breaks in the standard linear regression model over time. What it does it in essence partitioning the data and rather than estimating the regression based on the whole sample, it sequentially moves â€œthroughâ€ time in a fixed-width windows containing only a sub-sample of the observations and in each window it estimates the model. These estimates are used to the computation of empirical fluctuation processes that capture fluctuations in regression coefficients and residuals over time. Significant fluctuations of the coefficients are signs of a structural break in the regression. The test statistic of the Moving estimates test is</p>

<p><img src=""http://i.stack.imgur.com/qQ0FY.png"" alt=""Moving estimates test statistic""></p>

<p>where <em>n</em> is the number of observations, <em>h</em> is the bandwith (how many percent of the total number of observations are used for the window), <em>nh</em> is thus the size of the window, Q_(n)=X_(n)^T X_(N)/n, i=[k+t(n-k)], and sigma^2 is an estimate of the variance. The way I understand the above statistic is that it compares the difference between the sub-sample estimate of beta with the whole sample (the window) estimate and how this difference develops over time. A zero difference would indicate a sub-sample estimate that perfectly equals the whole-sample estimate, which would indicate perfect stability of the coefficient. In my understanding, the efp function in strucchange calculates sigma^2 based on the standard OLS residuals u^ i.e., sigma^2=(1/n-k)âˆ‘_(i=1)^n u_i^2 . Thus, in the presence of heteroskedasticity or autocorrelation, the OLS assumption of spherical disturbances will be violated. Thus, ideally, sigma^2 should be estimated based on a HAC covariance matrix to avoid wrong inference.</p>

<p>The question that comes to my mind is whether there is a way to use the ME test based on a HAC estimate. If not, it seems to me that it is limited to spherical disturbances of residuals, which seems to be violated in most applications.</p>
"
"NaN","NaN","145870","<p>I need some assistance with a nonlinear adjust. I am trying to make a mathematical model that describes the rate of silicic acid escaping from an underwater sediment. For theoretical reasons, the equation resulting from the nonlinear regression must be exponential and have the form $Y = A + Be^{CX}$, in which $A$, $B$ and $C$ are constants, $X$ is an independent predictor (time) and $Y$ is a dependent variable (concentration). I lack but the basic understanding of statistical software, so I have just tried to make it on Excel. The problem is that there I can only adjust my data to equations like $Y = Ae^{Bx}$, that are without a constant. I know for sure that R with a specific package will work just fine for this problem, but I cannot find any tutorials to orientate me. I guess that there is a really easy answer for this almost trivial question, but for now it is eluding me. Thank you very much for your time and help!</p>
"
"0.027972711943223","0.0284901441149095","146075","<p>I have a dataframe with one continuous response variable and hundreds of predictor variables (hundreds of additional columns in my dataframe). I'd like to run a regression for the single Response variable against all of the predictor variables, not univariately but all at once - what technique is best suited to do this, is the high dimsionality an issues..? I had been looking at Support Vector Regression using svm from the e1071 package, however my predictions come out to be a static number - I have many 0's in some of the columns..?</p>

<p>P.</p>
"
"0.115334445976885","0.117467873474841","146421","<p>I am using rlm robust linear regression of MASS package on modified iris data set as follows:</p>

<pre><code>&gt; myiris = iris
&gt; myiris$Species = as.numeric(myiris$Species)
&gt; head(myiris)
  Sepal.Length Sepal.Width Petal.Length Petal.Width Species
1          5.1         3.5          1.4         0.2       1
2          4.9         3.0          1.4         0.2       1
3          4.7         3.2          1.3         0.2       1
4          4.6         3.1          1.5         0.2       1
5          5.0         3.6          1.4         0.2       1
6          5.4         3.9          1.7         0.4       1

&gt; library(MASS)
&gt; rmod = rlm(Species~., data=myiris)
&gt; rmod
Call:
rlm(formula = Species ~ ., data = myiris)
Converged in 6 iterations

Coefficients:
 (Intercept) Sepal.Length  Sepal.Width Petal.Length  Petal.Width 
  1.14943807  -0.11067690  -0.02603537   0.21581357   0.63793686 

Degrees of freedom: 150 total; 145 residual
Scale estimate: 0.191 
&gt; 
&gt; sumrmod = summary(rmod)
&gt; sumrmod

Call: rlm(formula = Species ~ ., data = myiris)
Residuals:
     Min       1Q   Median       3Q      Max 
-0.59732 -0.15769  0.01089  0.10955  0.56317 

Coefficients:
             Value   Std. Error t value
(Intercept)   1.1494  0.2056     5.5906
Sepal.Length -0.1107  0.0579    -1.9128
Sepal.Width  -0.0260  0.0599    -0.4346
Petal.Length  0.2158  0.0571     3.7821
Petal.Width   0.6379  0.0948     6.7287

Residual standard error: 0.1913 on 145 degrees of freedom
</code></pre>

<p>This does not give p.values so I calculated them as follows (using pt function of base R):</p>

<pre><code>&gt; dd = data.frame(sumrmod$coefficients)                             #$
&gt; dd$p.value =  pt(dd$t.value, sumrmod$df[2])                       #$
&gt; dd
                   Value Std..Error    t.value    p.value
(Intercept)   1.14943807 0.20560264  5.5905804 0.99999995
Sepal.Length -0.11067690 0.05786107 -1.9128044 0.02887227
Sepal.Width  -0.02603537 0.05991073 -0.4345693 0.33226054
Petal.Length  0.21581357 0.05706173  3.7821068 0.99988663
Petal.Width   0.63793686 0.09480869  6.7286751 1.00000000
</code></pre>

<p>However, these are not correct since ordinary lm function and other regression functions show that Petal.Length and Petal.Width are highly significant in this regression:</p>

<pre><code>&gt; summary(lm(Species~., data=myiris))

Call:
lm(formula = Species ~ ., data = myiris)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.59215 -0.15368  0.01268  0.11089  0.55077 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)   1.18650    0.20484   5.792 4.15e-08 ***
Sepal.Length -0.11191    0.05765  -1.941   0.0542 .  
Sepal.Width  -0.04008    0.05969  -0.671   0.5030    
Petal.Length  0.22865    0.05685   4.022 9.26e-05 ***
Petal.Width   0.60925    0.09446   6.450 1.56e-09 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 0.2191 on 145 degrees of freedom
Multiple R-squared:  0.9304,    Adjusted R-squared:  0.9285 
F-statistic: 484.5 on 4 and 145 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>Where is the error? Am I not using correct method to calculate p.value here?</p>

<p>Edit: As suggested (further) by @Glen_b in the comments: </p>

<pre><code>&gt; dd$p.value =  2*pt(abs(dd$t.value), sumrmod$df[2], lower.tail=FALSE)      #$
&gt; dd
               Value Std..Error    t.value      p.value
(Intercept)   1.14943807 0.20560264  5.5905804 1.089792e-07
Sepal.Length -0.11067690 0.05786107 -1.9128044 5.774455e-02
Sepal.Width  -0.02603537 0.05991073 -0.4345693 6.645211e-01
Petal.Length  0.21581357 0.05706173  3.7821068 2.267410e-04
Petal.Width   0.63793686 0.09480869  6.7286751 3.691993e-10
</code></pre>

<p>These seem to be correct (finally).</p>
"
"0.0625488854200668","0.0637058989297032","146622","<p>I am fairly new to using prebuilt machine learning packages in R. I am looking at the following problem.</p>

<p>I have a long feature set, very small training set, and a large test set. The goal is to perform regression and estimate the target column.</p>

<p>I am using feature extraction algorithms to reduce feature set. And using this feature set to train a model using caret package.</p>

<p>After training, I want to do the following:</p>

<p>1) Predict target column from the test set only for entries which can be predicted by 'high confidence'.</p>

<p>2) Then use the newly created values and perform co-training (I could also re-run feature extraction).</p>

<p>3) Iterate till it covers the entire training set.</p>

<p>What I am unable to find is a way to predict entries with 'high confidence'. Is there a way to simply specify confidence levels in caret.predict? The documentation mentioned using the attribute 'type=raw/prob', but that apparently works only if the trained model is for classification.</p>

<p>Thanks, any pointers will be appreciated! Also if you know any general techniques to handle such a problem, that would be helpful.</p>
"
"0.0796117338651413","0.0900937462695559","146853","<p><strong>What I am doing so far:</strong></p>

<p>I am doing a constraint linear regression with R's <code>quadprog</code> package, function <code>solve.QP()</code>. The regression does not have an intercept $\alpha$, therefore the objective function can be stated by $$min_{b} (Y-Xb)^\top(Y-Xb)$$ which is the squared residuals. </p>

<p>Quadprog optimizes the function $$min_{b}\Big(\frac{1}{2}b^\top Db-d^\top b\Big)$$ Therefore I have to transform the first function into the second one. The end result is $$\frac{1}{2}b^\top X^\top Xb-Y^\top Xb$$ where $X^\top X =:D$ and $Y^\top X=:d$.</p>

<p>There are two risk factors in this example (hence Y is a vector of the dependent variable and X is a 2-dim matrix of the independent variables), whereas the first one is restricted to be greater or equal to -10 and the second one greater or equal to zero. The code for this is the following:</p>

<pre><code>require(""quadprog"")

Dmat = t(X) %*% X
Amat = t(diag(2))
bvec = c(-10,0)
dvec = t(Y) %*% X

solve.QP(Dmat = Dmat, dvec = dvec, Amat = Amat, bvec = bvec, meq = 0, factorized = F)
</code></pre>

<hr>

<p><strong>What I want to add:</strong></p>

<p>I want to add penalties to the regression in order to replicate a Lasso regression. Therefore, the initial objective function has to be expanded by the penalty term $\lambda |b|$ $$min_{b} (Y-Xb)^\top(Y-Xb)+\lambda |b|$$ 
In order to bring it into the form usable by the algorithm, I form it into $$\frac{1}{2}b^\top X^\top Xb-(Y^\top X-\frac{1}{2}\lambda)|b|$$
The penalty term $\lambda$ is a vector with two entries $\lambda=(80.56,5.65)$. However, when I now run the algorithm, the objective function gets negative and $b_1$ will be $b_1 = -10$, which is the most negative piossible value allowed by the constraints. $b_2$ will be $b_2=0$. </p>

<p>These results are not equal to results I get with the <code>glmnet</code> package which allows me to perform a Lasso regression with the same penalties. Those results have been checked and are correct. Hence, I do not know why the quadprog algorithm delivers different results. Any hints? Is the objective function wrong? Did I specify any input parameter for <code>quadprog</code> incorrectly? </p>
"
"NaN","NaN","146959","<p>So the <a href=""http://stats.stackexchange.com/questions/143394/help-constructing-a-simple-regression-model-with-a-breakpoint"">saga</a> continues...</p>

<p>So I am trying to fit the model
$$Runoff = \begin{cases}
\beta_0 + \beta_1Pcp &amp; \text{if $(Ant + Pcp) &lt; Thold$;}\\
\beta_2 + \beta_3Pcp &amp; \text{if $(Ant + Pcp) \geq Thold$;}
\end{cases}$$
Where $Pcp$ and $Ant$ are observed variables, and $Thold$ and the other parameters are estimated. I do not think I can do this with the <em>segmented</em> package, as the breakpoint is not in the predictor variables, but I think I should be able to do it fairly simply as a non-linear least-squares regression? I am not so good with the nls syntax, so any help would be appreciated</p>
"
"0.0839181358296689","0.0854704323447285","147119","<p>I have a plot of residual values of a linear model in function of the fitted values where the heteroscedasticity is very clear. However I'm not sure how I should proceed now because as far as I understand this heteroscedasticity makes my linear model invalid. (Is that right?)</p>

<ol>
<li><p>Use robust linear fitting using the <code>rlm()</code> function of the <code>MASS</code> package because it's apparently robust to heteroscedasticity. </p></li>
<li><p>As the standard errors of my coefficients are wrong because of the heteroscedasticity, I can just adjust the standard errors to be robust to the heteroscedasticity? Using the method posted on Stack Overflow here: <a href=""http://stackoverflow.com/questions/4385436/regression-with-heteroskedasticity-corrected-standard-errors"">Regression with Heteroskedasticity Corrected Standard Errors</a></p></li>
</ol>

<p>Which would be the best method to use to deal with my problem? If I use solution 2 is my predicting capability of my model completely useless?</p>

<p>The Breusch-Pagan test confirmed that the variance is not constant.</p>

<p>My residuals in function of the fitted values looks like this:  </p>

<p><a href=""http://i.stack.imgur.com/OtlGC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/OtlGC.png"" alt=""http://i.gyazo.com/9407a829a168492b31dfa3d1dd33a21d.png""></a> </p>

<p>(larger version)</p>

<p><img src=""http://i.gyazo.com/9407a829a168492b31dfa3d1dd33a21d.png"" width=""600""></p>
"
"0.0740088392978143","0.0753778361444409","147170","<p>I have a plot of residual values of a linear model in function of the fitted values where the heteroscedasticity is very clear. However I'm not sure how I should proceed now because as far as I understand this heteroscedasticity makes my linear model invalid right?</p>

<p>So I have been doing some reading about this subjet and I found two suggestions in other stackoverflow threads.</p>

<p>1) Use robut linear fitting using the rlm() function of the MASS package because it's apparently robust to heteroscedasticity. </p>

<p>2) As the standard errors of my coefficients are wrong because of the heteroscedasticity, I can just adjust the standard errors to be robust to the heteroskedasticity? Using the method posted <a href=""http://stackoverflow.com/questions/4385436/regression-with-heteroskedasticity-corrected-standard-errors"">here</a></p>

<p>Which would be the best method to use to deal with my problem?
If I use solution 2 is my predicting capability of my model completely useless?</p>

<p>My residuals in function of the fitted values looks like this <a href=""http://i.gyazo.com/9407a829a168492b31dfa3d1dd33a21d.png"" rel=""nofollow"">http://i.gyazo.com/9407a829a168492b31dfa3d1dd33a21d.png</a> and the Breusch-Pagan test confirmed that the variance is not constant.</p>
"
"0.027972711943223","0.0284901441149095","147600","<p>I am looking for a teaching example of a multivariate (not bivariate) implementation of Metropolis-Hastings for MCMC in R. I know several packages implement the algorithm more generally, but the code is difficult to follow and typically includes all sorts of other things besides this particular example. How do I implement Metropolis-Hastings for Bayesian multivariate regression like in this <a href=""https://theoreticalecology.wordpress.com/2010/09/17/metropolis-hastings-mcmc-in-r/"" rel=""nofollow"">bivariate</a> example?</p>
"
"0.0625488854200668","0.0637058989297032","147958","<p>As a result of linear regression, we can have its residual and see its plot to check whether it shows normal distributed or not as follows :</p>

<pre><code>library(car)
m1 &lt;- lm(mpg~disp+hp+wt+drat, data=mtcars)  #Create a linear model
resid(m1) #List of residuals
plot(density(resid(m1))) #A density plot
qqnorm(resid(m1)) # A quantile normal plot - good for checking normality
qqline(resid(m1))
</code></pre>

<p>We can have std for each parameters, however, it seems I could not find the source to create linear regression residuals for each parameters.</p>

<p>residualPlots function (in car package) is fine, but it would be nice if I could have its distribution or boxplot as well.</p>

<p>Let me know how I can plot each parameters in R.</p>
"
"0.0559454238864459","0.056980288229819","148529","<p>I am estimating a Weighted Spatial Simultaneous Autoregression Model (<code>spdep::spautolm</code> --> <a href=""http://cran.r-project.org/web/packages/spdep/spdep.pdf"" rel=""nofollow"">Link</a>) in R and I would like to do some residual analysis.</p>

<p>Unfortunately functions such as <code>hatvalues</code>,<code>cooks.distance</code> or <code>plot.lm</code> do not work for <code>spautolm</code> objects. Yet, I would like to calculate leverages and cook distances for my model (see also <a href=""https://stackoverflow.com/questions/29747519/how-to-do-residual-regression-deletion-diagnostics-on-spautolm-sarlm-objects/29756024#29756024"">my post on stackoverflow</a>).</p>

<p>My Model looks like this:</p>

<p>$Y = X^T\beta + \lambda W(Y âˆ’ X^T\beta)+ \epsilon$ with $\epsilon\sim^{iid} N(0,\sigma^2)$</p>

<p>Thus:</p>

<p>$Var[Y]=\Sigma_{SAR} = \sigmaÂ²(I-\lambda W)^{-1}V(I-\lambda W)^{-1}$ with $V=diag[1/n_i]$</p>

<p>$\rho$ is my spatial autoregression parameter and $W$ the matrix that represents spatial dependence.</p>

<p>Obviously, a simple calculation of the hatmatrix $H$ via $H=X(X^TX)^{-1}X^T$ is not valid here due to the weighting and the modeled spatial autocorrelation.</p>

<p><strong>Any ideas how to calculate the leverages and cook's distances by hand in R for this model?</strong></p>
"
"0.0484501583111509","0.0493463771219827","149004","<p>I'm trying to fit a Bayesian hierarchical poisson regression. To do so, I'm using MCMChpoisson function from MCMCpack in R. Based on this package, the model is:</p>

<p>$$Y_i \sim Poisson(\lambda_i)$$
$$\phi(\lambda_i) = X_i\beta + W_i \beta_i + \epsilon_i$$
$$\epsilon_i \sim N(0, \sigma^2 I_{k_i})$$
$$ \dots $$</p>

<p>In the model above, $\phi$ is the link function.</p>

<p>I skipped the rest of the model as only the parts of above are related to my question. My question is why they consider an measurement error ($\epsilon_i$) in the systematic component whereas in GLM we have a function of the mean; in other words, sampling from poisson will itself generate a measurement error. </p>

<p>Also, I think the extra $\epsilon_i$ term above causes me to get strange results. Does anyone know any other function/package in R to fit a model very similar to the model above with no measurement error in the systematic component.</p>

<p>Thanks very much for your help,</p>
"
"0.0884574820723792","0.0900937462695559","149012","<p><a href=""http://en.wikipedia.org/wiki/Discrete_choice#F._Logit_with_variables_that_vary_over_alternatives_.28also_called_conditional_logit.29"" rel=""nofollow"">Conditional logistic regression</a> is a <a href=""http://en.wikipedia.org/wiki/Fixed_effects_model"" rel=""nofollow"">fixed effects model</a>. If you're modeling the dependent variable $y$, a glm fixed effect model doesn't actually model $y$. Instead, the glm fixed effect models measure $y-mean(y)$ for a particular group. I think that this is <em>not</em> the case for a conditional logistic regression. The coefficients of the regression can be interpreted in the space of $y$. Is that correct?</p>

<p>My particular situation:
I am running a conditional logit with <a href=""https://stat.ethz.ch/R-manual/R-devel/library/survival/html/clogit.html"" rel=""nofollow"">clogit</a> in R, from the <code>survival</code> package. Are the coefficients returned to be interpreted in the space of $y$, or in the space of something like $y-mean(y)$? </p>

<p>Normally the difference isn't very relevant; one would interpret the coefficient roughly the same either way. However, in my case one of the independent variables is fitted as a spline. Specifically, it is a restricted cubic spline, as calculated from <code>rcspline.eval</code> in the <a href=""http://cran.r-project.org/web/packages/Hmisc/Hmisc.pdf"" rel=""nofollow"">Hmisc</a> package. <code>clogit</code> produces a coefficient for each knot of the spline, and in order to interpret the overall effect of the variable one needs to reconstruct the spline from the coefficients (using <code>rcspline.restate</code>). I want to make sure that I should be looking at the shape of this spline in the range of $y$ (which in my case is 0-100) or in the range of something like $y-mean(y)$ (in this case, $mean(y)$ is the same for all groups: 50). If it is the case that the space is shifted this will be particularly weird for a spline, because presumably the knots should also be shifted somehow.</p>
"
"0.0559454238864459","0.0427352161723642","149140","<p>Currently I am working on a large data set with well over 200 variables (238 to be exact) and 290 observations for each variable (in theory). This data set is missing quite a lot of values, with variables ranging from 0-100% 'missingness'. I will eventually be performing logistical regression on this data, so of my 238 columns I will at most only be using ten or so.</p>

<p>However as almost all of my columns are missing some data, I am turning to multiple imputation to fill in the blanks (using the MICE package).</p>

<p>My question is; given that I have a large amount of variation in the missing data, at what percentage missing should I start to exclude variables from the mice() function? </p>

<p>Can mice function well with variables that are missing 50% of their values? What about 60%, 70%, 80%, 90%?</p>
"
"0.0484501583111509","0.0493463771219827","149502","<p>I want to study the internal Italian migration using the network analysis. </p>

<p>My nodes are the Italian cities, the edges are people who move from a city to another. I built my edge list in SPSS. I have three columns (source node, target node and weight). </p>

<p>Other variables in my database are relative to the node, some of those are numerical (i.e. city population, GDP pro capita) others categorical (i.e. if in the city there is a university, quality of life). </p>

<p>Other variables are relative to the actors who moves from a node (city) to another: gender, age, and so on. </p>

<p>I want to study the relevance of these variables to structure the internal Italian migration network. In other words I would like to construct a regression model (may be logit) where the dependent variable is my (valued) network and the independent variables are the attributes of nodes and edges. The aim is to understand which of those attributes explains the structure of the network. </p>

<p>How can I do it? Do I need to implement random graph models? p2 or p *? Is there a simple tutorial for R (sna, ergm or other packages)? </p>
"
"0.0839181358296689","0.0854704323447285","149627","<p>I want to make a piecewise linear regression in R. I have a large dataset with 3 segments where I want the first and third segment to be without slope, i.e. parallel to x-axis and I also want the regression to be continuous. I have a small example dataset and example code below. I'm rather new to R and have tried to write the model formula without success. Can anyone help with the formula and how to extract intercept and slope for the different segments? Three specific questions are inserted as comments in the code. I have looked at package segmented but are unable to understand how to constrain segment 1 and 3 to be parallel to x-axis.</p>

<pre><code>#Example data
y &lt;- c(4.5,4.3,2.57,4.40,4.52,1.39,4.15,3.55,2.49,4.27,4.42,4.10,2.21,2.90,1.42,1.50,1.45,1.7,4.6,3.8,1.9)  
x &lt;- c(320,419,650,340,400,800,300,570,720,480,425,460,675,600,850,920,975,1022,450,520,780)  
plot(x, y, col=""black"",pch=16)

#model 1 not continuous, Q1: how to get that?
fit1&lt;-lm(y ~ I(x&lt;500)+I((x&gt;=500&amp;x&lt;800)*x) + I(x&gt;=800)) 
summary(fit1) #intercepts and slopes extracted from summary(fit1)
lines(c(min(x),500),c(round(summary(fit1)[[4]][1]+summary(fit1)[[4]][2],2),round(summary(fit1)[[4]][1]+summary(fit1)[[4]][2],2)),col=""red"")
lines(c(800,max(x)),c(round(summary(fit1)[[4]][1]+summary(fit1)[[4]][4],2),round(summary(fit1)[[4]][1]+summary(fit1)[[4]][4],2)),col=""red"")
lines(c(500,800),c((summary(fit1)[[4]][1])+(500*(summary(fit1)[[4]][3])),
               (summary(fit1)[[4]][1])+(800*(summary(fit1)[[4]][3]))),col=""red"")

#model 2 continuous but first and third segment not parallell to x-axis, Q2: how to get that?
fit2&lt;-lm(y ~ x + I(pmax(x-500,0)) + I(pmax(x-800,0)))                                                  
summary(fit2) # Q3: how to get intercept and slope from summary(fit2) for model2?
mg &lt;- seq(min(x),max(x),length=400)
mpv &lt;- predict(fit2, newdata=data.frame(x = mg,t = (mg - 800)*as.numeric(mg &gt; 800)))
lines(mg, mpv,col=""blue"")
</code></pre>
"
"0.0395593886064618","0.040291148201269","149908","<p>The function <code>powerTranform</code> from the ""car"" package in R mentions the following code for Box-Cox transformation for multiple regression: </p>

<pre><code>summary(p1 &lt;- powerTransform(cycles ~ len + amp + load, Wool))
# fit linear model with transformed response:
coef(p1, round=TRUE)
summary(m1 &lt;- lm(bcPower(cycles, p1$roundlam) ~ len + amp + load, Wool))
</code></pre>

<p>Is it sensible to apply Box-Cox method to just the dependent variable (and not the whole formula) and proceed with the regression: </p>

<pre><code>library(fifer)
cycles = boxcoxR(cycles)
summary(m1 &lt;- lm(cycles ~ len + amp + load, Wool))
</code></pre>

<p>I suspect this method is not right but I am not sure.</p>
"
"0.027972711943223","0.0284901441149095","151121","<p>I am trying to run <code>netlogit</code> on a network of about 60,000 nodes, and I would like to know if the SNA package's functions are designed to support such large operations. I know, for instance, that with RSiena we are not supposed to go further than a few hundred nodes for the algorithms to converge in a reasonable time. But is this the case with SNA's regression models as well?</p>

<p>My initial experience is negative: When I run the commands on my network, the R process expands in memory until it fills both the live memory and the virtual memory and then it crashes (even with one repetition). Is this a matter throwing more resource at R, and is there a good way to calculate how much? Or are the current MRQAP algorithms simply not adapted to such networks? Any resources that can help determine the requirements?</p>
"
"0.027972711943223","0.0284901441149095","151600","<p>Has anyone written a package for R that can do a logistic regression over categorical variables (like <code>glm</code>) but with the constraint, and I do realize this is weird, that <em>all the residuals must be nonnegative?</em>  (In response space, not link space.  In other words, the predicted probability in each cell must come out less than or equal to the observed probability in that cell.) Alternatively, is there a straightforward way to transform a <code>glm</code> problem so that it will come out with nonnegative residuals?</p>

<p>I know I can probably persuade <code>optim</code> to do what I want but if a shortcut exists that sure would be nice.</p>
"
"0.0685188709827532","0.0697863157798853","151915","<p>I've performed a logistic regression with L-BFGS on R and noticed that if I changed the initialization, the model retuned was different.</p>

<p>Here is my dataset (390 obs. of 14 variables, Y is the target variable) :</p>

<pre><code>GEST    DILATE    EFFACE    CONSIS    CONTR    MEMBRAN    AGE    STRAT    GRAVID    PARIT    DIAB    TRANSF    GEMEL    Y
31           3       100         3        1         2     26         3         1        0       2         2       1     1
28           8         0         3        1         2     25         3         1        0       2         1       2     1
31           3       100         3        2         2     28         3         2        0       2         1       1     1
...
</code></pre>

<p>This dataset is found here: <a href=""http://tutoriels-data-mining.blogspot.fr/2008/04/rgression-logistique-binaire.html"" rel=""nofollow"">http://tutoriels-data-mining.blogspot.fr/2008/04/rgression-logistique-binaire.html</a> in ""DonnÃ©es : prematures.xls"". Y is a column I created with the column ""PREMATURE"", Y=IF(PREMATURE=""positif"";1;0)</p>

<p>I've used the optimx package like here <a href=""http://stats.stackexchange.com/questions/17436/logistic-regression-with-lbfgs-solver"">Logistic regression with LBFGS solver</a>, here is the code: </p>

<pre><code>install.packages(""optimx"")
  library(optimx)

vY = as.matrix(premature['PREMATURE'])
# Recoding the response variable
vY = ifelse(vY == ""positif"", 1, 0)

mX = as.matrix(premature[c('GEST', 'DILATE', 'EFFACE', 'CONSIS', 'CONTR', 
                           'MEMBRAN', 'AGE', 'STRAT', 'GRAVID', 'PARIT', 
                           'DIAB', 'TRANSF', 'GEMEL')])

#add an intercept to the predictor variables
mX = cbind(rep(1, nrow(mX)), mX)

#the number of variables and observations
iK = ncol(mX)
iN = nrow(mX)

#define the logistic transformation
logit = function(mX, vBeta) {
  return(exp(mX %*% vBeta)/(1+ exp(mX %*% vBeta)) )
}

# stable parametrisation of the log-likelihood function
logLikelihoodLogitStable = function(vBeta, mX, vY) {
  return(-sum(
    vY*(mX %*% vBeta - log(1+exp(mX %*% vBeta)))
    + (1-vY)*(-log(1 + exp(mX %*% vBeta)))
  )  # sum
  )  # return 
}

# score function
likelihoodScore = function(vBeta, mX, vY) {
  return(t(mX) %*% (logit(mX, vBeta) - vY) )
}

# initial set of parameters (arbitrary starting parameters)
vBeta0 = c(10, -0.1, -0.3, 0.001, 0.01, 0.01, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01)

optimLogitLBFGS = optimx(vBeta0, logLikelihoodLogitStable,
                         method = 'L-BFGS-B', gr = likelihoodScore, 
                         mX = mX, vY = vY, hessian=TRUE)
</code></pre>

<p>I get this :</p>

<pre><code> optimLogitLBFGS
                p1         p2       p3         p4         p5         p6
L-BFGS-B 9.720242 -0.1652943 0.525449 0.01681583 0.02781123 -0.3921004
                 p7          p8         p9       p10        p11        p12
L-BFGS-B -1.694412 -0.03461208 0.02759248 0.1993573 -0.6718275 0.02537887
                 p13      p14   value fevals gevals niter convcode  kkt1  kkt2
L-BFGS-B -0.8374338 0.625044 187.581    121    121    NA        1 FALSE FALSE
          xtimes
L-BFGS-B  0.044
</code></pre>

<p>But if I change </p>

<pre><code>vBeta0 = c(10, -0.1, -0.3, 0.001, 0.01, 0.01, 0.001, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01)
</code></pre>

<p>in</p>

<pre><code>vBeta0 = rep(0.1, iK)
</code></pre>

<p>I get a different result :</p>

<pre><code>optimLogitLBFGS
                 p1             p2             p3              p4               p5
L-BFGS-B 0.372672689046 0.206785276091 0.398104550108 0.0175008380158 -0.0460042719084
                 p6             p7               p8            p9            p10
L-BFGS-B 0.139760396213 -1.43192069477 -0.0207666651106 -1.1396642657 0.212186387416
                 p11             p12             p13            p14         value
L-BFGS-B -0.583698421298 0.0576485672766 -0.802789658686 0.993103617257 185.472518798
         fevals gevals niter convcode  kkt1  kkt2 xtimes
L-BFGS-B    121    121    NA        1 FALSE FALSE   0.05
</code></pre>

<p>How can I choose the initial parameters to get the best model?</p>
"
"0.100857047225074","0.102722675451665","151961","<p>*Please note this question is about the Platt probabilistic output and SVM class assignment, not about the code or the package itself. It just happens to be the code where I stumbled on the issue.</p>

<p>In <a href=""http://stats.stackexchange.com/questions/147260/including-class-probabilities-might-skew-a-model-in-caret"">another question</a> I asked about bad models coming from <code>caret</code> and associated <code>kernlab</code> when <code>prob.model=TRUE</code>. I found the answer myself, in both <a href=""http://stackoverflow.com/questions/29766951/different-results-with-caret-when-classprobs-true"">stackoverflow</a> and <a href=""http://r.789695.n4.nabble.com/Inconsistent-results-between-caret-kernlab-versions-td4680500.html"" rel=""nofollow"">from Max Kuhn himself</a>:</p>

<blockquote>
<pre><code>&gt; predict(newSVM, df[43,-1]) [1] O32078 10 Levels: O27479 O31403 O32057 O32059 O32060 O32078 ... O32676
&gt; predict(newSVM, df[43,-1], type = ""probabilities"")
     O27479     O31403     O32057    O32059    O32060     O32078
[1,] 0.08791826 0.05911645 0.2424997 0.1036943 0.06968587 0.1648394
     O32089     O32663     O32668     O32676
[1,] 0.04890477 0.05210836 0.09838892 0.07284396
</code></pre>
  
  <p>Note that, based on the probability model, the class with the largest
  probability is O32057 (p = 0.24) while the basic SVM model predicts
  O32078 (p = 0.16).</p>
  
  <p><strong>Somebody (maybe me) saw this discrepancy and that led to me to follow
  this rule:</strong></p>

<pre><code>if(prob.model = TRUE) use the class with the maximum probability   
  else use the class prediction from ksvm().
</code></pre>
  
  <p>Therefore:</p>

<pre><code>predict(svm.m1, df[43,-1])
 [1] O32057
 10 Levels: O27479 O31403 O32057 O32059 O32060 O32078 ... O32676
</code></pre>
</blockquote>

<p>Isn't that innacurate? <code>kernlab</code> searches for the optimal probability cutoff that minimizes error, that's why the assigned class and the maximum probability don't match: they don't have to.</p>

<p>Check this reproducible example. I excluded two cherrypicked <code>virginica</code> samples.</p>

<pre><code>require(kernlab);require(caret);
#kernel=polynomial; degree=3; scale=0.1; C=0.31
set.seed(101);SVM&lt;-ksvm(Species~., data=iris[-c(135,150),], kernel='polydot',C=.31, kpar=list( scale=.1, degree=3), prob.model=T)
</code></pre>

<p>Here's the resulting model </p>

<pre><code>&gt; SVM
Support Vector Machine object of class ""ksvm"" 

SV type: C-svc  (classification) 
 parameter : cost C = 0.31 

Polynomial kernel function. 
 Hyperparameters : degree =  3  scale =  0.1  offset =  1 

Number of Support Vectors : 58 

Objective Function Value : -1.4591 -0.7955 -10.2392 
Training error : 0.033784 
Probability model included. 
</code></pre>

<p>Now let's check the predicted class probabilities in those two samples</p>

<pre><code>&gt; predict(SVM, iris[c(135,150),-5], type=""probabilities"")
          setosa versicolor virginica
[1,] 0.008286638  0.4414114  0.550302
[2,] 0.013824451  0.3035556  0.682620
</code></pre>

<p>And the class predictions</p>

<pre><code>&gt; predict(SVM, iris[c(135,150),-5])
[1] versicolor virginica 
Levels: setosa versicolor virginica
</code></pre>

<p>Sample 150 was assigned to <code>virginica</code>, with a class probability of around 0.68. Sample 135 was assigned to <code>versicolor</code> with a probability of around 0.44, yet <code>virginica</code> probability nicely sits around 0.55.
Looking at several CV folds, we perceive that kernlab only assigns <code>virginica</code> when its probability is over a given value (way higher than 0.5). That's the cutoff I mentioned, and it happens thanks to the well known bad clustering in <code>iris</code> between <code>virginica</code> and <code>versicolor</code>.</p>

<p>So, am I right on these suppositions and therefore is <code>caret</code> class assignment model (maximum probability) wrong?</p>

<p>EDIT:
I've been experimenting with pairwise probability coupling of Platt scaling (logistic regression fit), isotononic regression and a model I'm working on. A weakness (?) I perceived in Platt's model is the probability isn't bound to be 0.5 when the binary SVM decision output is 0, which is the expected result as the instance would lie exactly on the separating hyperplane.</p>
"
"0.0559454238864459","0.056980288229819","152012","<p>This might fit better here than on stackoverflow, I guess.</p>

<p>I was <a href=""http://stackoverflow.com/questions/30139874/r-dynamic-linear-regression-with-dynlm-package-how-to-predict"">trying to build a dynamic regression model with the dynlm</a> package, but it did not work out. After reading <a href=""http://robjhyndman.com/hyndsight/arimax/"" rel=""nofollow"">this</a> by Hyndman, I now switched to an ARMAX model:</p>

<pre><code>y_t = a_1*x1_t + a_2*x2_t + ... + a_k*xk_t + n_t
</code></pre>

<p>where the error term follows an ARMA model</p>

<pre><code>n_t ~ ARMA(p,q)
</code></pre>

<p>So far I am using the function <code>auto.arima(y, xreg=cbind(x1, ..., xk))</code> from the <code>forecast</code>package, which is doing the job!</p>

<p>As a benchmark I am running a pure multiple regression with <code>lm()</code>, where I make use of the <code>step()</code> function to kick out non relevant variables (about 100 variables, from which 96 are dummies) to optimize the model according to <code>AIC</code>.</p>

<p>The in-sample forecasting for both models is more or less equal. As the ARMAX model always includes <strong>all</strong> independent variables <code>(x1, ..., xk)</code>, I am pretty sure that, if I could apply the <code>step()</code> function on it, I would achieve a further improvement here.</p>

<p>The problem is that the <code>step()</code> function does not work on <code>auto.arima()</code>?!</p>

<p>Do you have any suggestions how I could still do this? Or would I need a totally new approach?</p>

<p>(I have not provide a reproducible example, as this is a rather general question of which methods/functions/packages to use. If the question is not clear enough, please tell me and I will try to provide one)</p>
"
"0.0625488854200668","0.0637058989297032","152158","<p>I have a linear model containing a few continuous variables and four categorical variables, each represented by 12, 3, 4, and 5 dummy variables respectively. When using model selection criteria such as PRESS, Mallows's Cp, and BIC using the leaps package, the best model returned for each criterion contains only some of the dummy variables for each categorical variable. It is my understanding that this is not good practice, and either all or none of the dummies must be included in the model. Is there a way to have leaps treat the dummy variables for each categorical variable as one variable?</p>

<p>Also, could this method be extended to use with the glmnet package? I'm having the same issue with lasso and ridge regression.</p>

<p>EDIT: Is there a way to specify an lm object with a subset of independent variables to be treated as one?</p>
"
"0.0625488854200668","0.0509647191437626","152394","<p>I am using R's flexsurvreg function (in the flexsurv package) to fit a AFT model to my data. </p>

<p>This is the line of code that fits the model to the data:</p>

<pre><code>TestModel &lt;- flexsurvreg(Surv(time,death) ~ param1 + param2 + param3 + param4 + param5 + param6 + param7 + param8 + param9 + param10 + param11 + param12 + param13, data = DataTest, dist = ""weibull"")  
</code></pre>

<p>Once the model fits, this is a summary of the results:</p>

<pre><code>Estimates: 
        data mean     est        L95%       U95%       se         exp(est)   L95%       U95%     
shape      NA         9.99e-01         NA         NA         NA         NA         NA         NA
scale      NA         2.20e+02         NA         NA         NA         NA         NA         NA
param1     1.32e-01   2.51e-01         NA         NA         NA   1.29e+00         NA         NA
param2     1.61e-01  -1.54e-02         NA         NA         NA   9.85e-01         NA         NA
param3     1.89e-01  -4.68e-02         NA         NA         NA   9.54e-01         NA         NA
param4     1.76e-01  -2.25e-02         NA         NA         NA   9.78e-01         NA         NA
param5     1.87e-01  -5.35e-02         NA         NA         NA   9.48e-01         NA         NA
param6     7.56e-01  -2.74e-01         NA         NA         NA   7.60e-01         NA         NA
param7     2.28e-01   3.23e-02         NA         NA         NA   1.03e+00         NA         NA
param8     1.58e-01  -1.69e-02         NA         NA         NA   9.83e-01         NA         NA
param9     4.32e-01  -1.89e-02         NA         NA         NA   9.81e-01         NA         NA
param10    1.30e+02  -1.01e-03         NA         NA         NA   9.99e-01         NA         NA
param11    2.26e+01  -4.08e-03         NA         NA         NA   9.96e-01         NA         NA
param12    5.54e+02  -2.84e-04         NA         NA         NA   1.00e+00         NA         NA
param13    9.57e+01  -4.69e-03         NA         NA         NA   9.95e-01         NA         NA

N = 40320,  Events: 32154,  Censored: 8166
Total time at risk: 2584693
Log-likelihood = -171611.5, df = 15
AIC = 343253.1
</code></pre>

<p>I want to measure how the covariates affect the survival time. The estimates provide an understanding of this. Also, as I read <a href=""http://stats.stackexchange.com/questions/6026/how-do-i-interpret-expb-in-cox-regression"">here</a>, $exp(est)$ provides an estimate of how the hazard changes with change in 1 unit of a covariate by keeping the other covariates fixed. Is there a way I can calculate p-values for these covariates?</p>

<p>I have fitted a Weibull distribution to my dataset.</p>
"
"0.0484501583111509","0.0493463771219827","152526","<p>I would like to estimate a model of the following form:</p>

<p>$$
y = \sigma G y + \beta X + \delta G^* X + \epsilon
$$</p>

<p>where $G$ and $G^*$ are quadratic adjacency matrices, $y$ is a vector of a dependent variable subject to peer effects and $X$ is a matrix of controls/exogenous characteristics. $G$ is a binary matrix with entries equal to 1 if two individuals are peers of each other; as such, $G$ is also symmetric with a 0-diagonal. $G^*$ is the row-normalized version of $G$ (in fact, it's a stacked matrix with the row-normalized $G$ along the diagonal an 0 in the off-diagonals because $X$ is a matrix).</p>

<p>Econometricians will immediately recognize the similarity to spatial simultaneous autoregressive lag models, whose general form is</p>

<p>$$
y = \rho W y + \beta X + \epsilon
$$</p>

<p>(with $W = G$). Because of the high similarity, I tried Roger Bivand's <a href=""http://cran.r-project.org/web/packages/spdep/index.html"" rel=""nofollow""><code>spdep</code></a> package for <code>R</code>. The regression command is <code>lagsarlm()</code>, but when I am not mistaken, it only estimates </p>

<p>$$
y = \sigma G y + \beta X + \epsilon
$$</p>

<p>using the following command:</p>

<pre><code>lagsarlm(y ~ x1 + x2, data=reg_data, listw=G, method=""eigen"",
    quiet=FALSE, zero.policy = FALSE, tol.solve=1e-14)
</code></pre>

<p>which works and also returns results, but obviously an entire term is missing. How can I incorporate the $ \delta G^* X$ term? Are there packages that serve my needs better?</p>
"
"0.0625488854200668","0.0637058989297032","152868","<p>I have an administrative database with hospital readmissions (binomial: yes/no) and a couple of predictors. I've fitted a multilevel model with the function <code>glmer</code> from the package <code>lme4</code> to estimate the effect of these predictors on readmissions. 
The model has two levels: <code>hospital</code> and <code>patient</code>.
When I calculate the predicted probabilities (the chance of a readmission), and afterwards calculate the readmission ratio's for each hospital (by dividing the observed readmissions by the predicted readmissions), all my ratio's are around 1 which can't be correct.</p>

<p>Before I've calculated the predicted probabilities with a normal logistic regression, which gives more plausible ratio's (from 0,64 to 1,5)</p>

<p>This is my code to calculate predicted probabilities for the multilevel model:</p>

<pre><code>database$predprob &lt;- fitted(model1)
</code></pre>

<p>I've also tried this one, but it gives exactly the same predicted probabilities:</p>

<pre><code>database$predprob &lt;- predict(model1, newdata = database, type = ""response"", na.action = na.omit)
</code></pre>

<p>Does anybody know how to calculate predicted probabilities for a multilevel analysis? I suppose there must be another way to calculate it as my calculated ratios (observed/predicted) are all around 1.</p>
"
"0.0559454238864459","0.056980288229819","153199","<p>I've used tslm() under the R-package fpp to analyse two time series, which seem similar:</p>

<pre><code>library(fpp)
a&lt;-ts(c(1,10,2,3,4,5,6,7,8,9,10,11,12,2,21,4,6,8,10,11,12,13,14,18), start = c(1959, 1), frequency=12)
b&lt;-ts(c(1,10,2,3,4,5,6,7,8,9,10,11,12,2,21,4,6,8,10,11,12,13,14,18), start = c(1959, 3), frequency=12)
</code></pre>

<p>However, the results of a simple time series regression </p>

<pre><code>summary(tslm(a~trend+season))
</code></pre>

<p>and </p>

<pre><code>summary(tslm(b~trend+season))
</code></pre>

<p>look different - why isn't this the same? How can the trend be the same, but the other results be different? I'd understand shifted seasonal results, but these are really different.</p>

<p>Idea: the function tslm() expects full years, thus for time series b months jan and feb '59 as well as the rest of 1960 are filled with values computed from the given data. But is that idea true?</p>
"
"0.027972711943223","0.0284901441149095","153332","<p>I'm running some proportional hazards regression models using the <code>survival</code> package in <code>r</code> and would like to know how I can access the overall p-value for the likelihood ratio test to include it in an <code>rmarkdown</code> document. I can run the regression models and print the outputs with appropriate p-values in the r console window, but I can't find the argument for p-values in the <code>coxph</code> class of objects.</p>

<p>Here's an example model:</p>

<pre><code>library(KMsurv)
library(survival)

data(burn); attach(burn)
my.surv &lt;- Surv(T1, D1)
coxph.fit &lt;- coxph(my.surv ~ Z1 + as.factor(Z11), method = ""breslow"")
coxph.fit
detach(burn)
</code></pre>

<p>The overall p-value = 0.006.</p>
"
"NaN","NaN","153480","<p>I need to implement a SAR model with no covariates. To be more specific, the regression I have to estimate is y=bWy+e where: </p>

<ul>
<li>y is the dependent variable;</li>
<li>b is the coefficient parameter to be estimated;</li>
<li>W is the adjacency matrix;</li>
<li>e is the error.</li>
</ul>

<p>My idea was to use the lagsarlm function of the spdep package. But I've gone through spdep documentation and it seems that this function works only adding covariates: i.e. y=bWy+cX+e, and I don't know how to erase the X term.</p>

<p>Note: For those who are acquainted with network analysis literature and not with spatial econometrics, in a way this is a method to estimate a parameter for bonachich centrality.</p>
"
"0.0419590679148345","0.0427352161723642","153527","<p>I am looking at the <code>wine</code> dataset from the R <code>FactoMineR</code>package to find out whether the categorical variable soil type affects the feature set. Basically there are four distinct soil types and 29 different features and there are 20 observations</p>

<pre>
obs&nbsp;soil&nbsp;f1&nbsp;f2&nbsp;...&nbsp;f29
  1&nbsp;   1&nbsp;11&nbsp;87&nbsp;...&nbsp;20
  .&nbsp;   .&nbsp; .&nbsp; .&nbsp;...&nbsp; .
  7&nbsp;   2&nbsp;13&nbsp;10&nbsp;...&nbsp;10
  .&nbsp;   .&nbsp; .&nbsp; .&nbsp;...&nbsp; .
 15&nbsp;   3&nbsp;77&nbsp;53&nbsp;...&nbsp;54
  .&nbsp;   .&nbsp; .&nbsp; .&nbsp;...&nbsp; .
 20&nbsp;   4&nbsp;88&nbsp;81&nbsp;...&nbsp;21
</pre> 

<p>Given that I cannot use ANOVA or logistic regressions what other ways are there to figure out if the soil type affects the features? I am considering using box plots for visual inspection, but 29 box plots seems like a bit much. Are there any succinct plot types that could give me this kind of information?</p>
"
"0.0796117338651413","0.0810843716426003","153547","<p>I have a very large data set with repeated measurements of same blood value (co) (1 to 7 measurements per patient). Each measurement is coupled with time which is the time interval between surgical operation and blood level measurement. </p>

<p>My aim is to show that this blood value correlates positively with time.</p>

<p>Blood level measurements are highly skewed to <em>right</em> and hence I am using a log-transformation and linear mixed effect regression model (lmer in lme4 package).</p>

<p>I have constructed a null model: </p>

<pre><code>fit1&lt;-(lmer(lgco~(1|id),data=ASR))
</code></pre>

<p>Model 2 includes time as independent variable:</p>

<pre><code>fit2&lt;-(lmer(lgco~time+(1|id),data=ASR))
</code></pre>

<p>Id is the patient number in th dataset.</p>

<p>By using the anova() function I see that fit2 is significantly better than fit1:</p>

<pre><code>&gt; anova(fit1,fit2)
refitting model(s) with ML (instead of REML)

Data: ASR
Models:
fit1: lgco ~ (1 | id)
fit2: lgco ~ time + (1 | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit1  3 342.77 357.50 -168.39   336.77                             
fit2  4 320.64 340.27 -156.32   312.64 24.135      1  8.983e-07 ***
</code></pre>

<p>However I have other data which suggests that the correlation between time and blood value might even more profound, for example quadratic. This would be Model 3.</p>

<p>I tried the following: first I took the square root of the blood value and after that I made the transformation using log.</p>

<pre><code>fit3&lt;-(lmer(lgsqrtco~time+(1|id),data=ASR))
</code></pre>

<p>My question is that can I compare models 2 and 3 in anyway now after the dependent variable has two different transformations in these models. In fit1 and fit2 the transformation is identical, only the independent is added. I assume that with different dependent variable transformation the use of anova() is not allowed: </p>

<pre><code>anova(fit2,fit3)
refitting model(s) with ML (instead of REML)
Data: ASR
Models:
fit2: lgco ~ time + (1 | id)
fit3: lgsqrtco ~ time + (1 | id)
     Df      AIC      BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit2  4   320.64   340.27 -156.32   312.64                             
fit3  4 -1065.66 -1046.03  536.83 -1073.66 1386.3      0  &lt; 2.2e-16 ***
</code></pre>
"
"0.0559454238864459","0.0427352161723642","153707","<p>I have a time-series of historical volatility observations. I want to use an EGARCH model because I believe it is a better representation of the behaviour of these volatilities. Can I estimate an EGARCH model using the observed volatilities without using the underlying returns? I'm using R and I think in the input the program expects returns instead of volatilities.</p>

<p>To be more specific, I'm trying to using the same methodology described in a paper about idiosyncratic volatility. To estimate it, the author run the following regression:</p>

<p>\begin{equation}
r_t-rf_t=Î±_t+b1_t (rm_t-rf_t)+b2_t*SMB_t+b3_t*HML_t+Îµ_t.    equation(1)
\end{equation}</p>

<p>\begin{equation}
 Îµ_t\thicksim N(0,\sigma_t^2)
\end{equation}</p>

<p>\begin{equation}
 lnâ¡(\sigma_t^2)=w+\sum_{i=1}\beta_i lnâ¡(\sigma_{t-i}^2)+
\sum_{i=1}c_i\Biggl[\theta \biggl( \frac{Îµ_{t-i}}{\sigma_{t-i}}\biggl)+\gamma\Biggl[ \left|\frac{Îµ_{t-i}}{\sigma_{t-i}}\right|-\sqrt{\frac{2}{\pi}} \Biggl] 
    \Biggr]
\end{equation}</p>

<p>(I didn't know how to put the sign over the summation; anyway it is from  i=1 to p and i=1 to q.)</p>

<p>Idiosyncratic volatility is defined as the standard error of the residuals of the regression in equation(1).</p>

<p>I want to build an EGARCH to have a conditional idiosyncratic volatility. TO do this, I run the regression 1 and took the standard error of the residuals; this is the historical idiosyncratic volatility. Then, when I give this historical idiosyncratic volatilities as input to my program ( I use R and the package rugarch). 
Does it make sense this procedure or should I do something else? The problem is that in all the application that I view of EGARCH, the inputs are the returns but in my case, if I give returns as input, then I would have an EGARCH for the normal volatility and not the idiosyncratic, which is the one in which I am interested.</p>
"
"0.0927749898843639","0.0859010165930062","153761","<p>I don't have a lot of experience working with time series data. Now I have a 3 year, monthly data for several entities (you can think about them as different stores), that I would like to do some analysis, e.g. regression. I am not sure if there are trend and seasonality effects on these series.
Using the package <code>Forecast</code> in <code>R</code>, and applying the function <code>stl</code>, I decomposed the series and plotted them. I have attached some of the resulting plots (for different stores):</p>

<p><img src=""http://i.stack.imgur.com/np21E.jpg"" alt=""trend 2""></p>

<p><img src=""http://i.stack.imgur.com/ISKHJ.jpg"" alt=""trend 3""></p>

<p><img src=""http://i.stack.imgur.com/Rsxhw.jpg"" alt=""trend 4""></p>

<p>When I print the result of the fitted <code>stl</code> function, I get something like this: </p>

<pre><code> Call:
 stl(x = dlr1, s.window = ""period"")

 Components
            seasonal      trend   remainder
 Jan 2010 -0.05643233 -0.2151193 -0.02526416
 Feb 2010 -0.14799311 -0.2193160  0.13137861
 Mar 2010  0.10125889 -0.2235127  0.13747509
 Apr 2010 -0.29720645 -0.2266819 -0.47611165
 May 2010 -0.22746429 -0.2298511  0.28988090
 Jun 2010  0.12403035 -0.2320100  0.05470502
 Jul 2010 -0.10418880 -0.2341688  0.15684340
 Aug 2010  0.14560622 -0.2358294 -0.25225647
 Sep 2010  0.16699531 -0.2374901 -0.35570221
 Oct 2010 -0.21709617 -0.2402783  0.13772671
 Nov 2010  0.20363225 -0.2430665  0.19027750
 Dec 2010  0.30885826 -0.2444804 -0.11424289
 Jan 2011 -0.05643233 -0.2458944  0.16533482
 Feb 2011 -0.14799311 -0.2329029  0.09169095
 Mar 2011  0.10125889 -0.2199115 -0.33798701
 Apr 2011 -0.29720645 -0.2111018  0.58659147
 May 2011 -0.22746429 -0.2022921 -0.56724011
 Jun 2011  0.12403035 -0.2105492 -0.38534202
 Jul 2011 -0.10418880 -0.2188064  0.45324407
 Aug 2011  0.14560622 -0.2282670  0.15884344
 Sep 2011  0.16699531 -0.2377275  0.07834284
 Oct 2011 -0.21709617 -0.2372438  0.38658989
 Nov 2011  0.20363225 -0.2367601 -0.38545840
 Dec 2011  0.30885826 -0.2406277 -0.02293191
 Jan 2012 -0.05643233 -0.2444953 -0.14810135
 Feb 2012 -0.14799311 -0.2603740 -0.23092833
 Mar 2012  0.10125889 -0.2762527  0.19282561
 Apr 2012 -0.29720645 -0.2778357 -0.11752792
 May 2012 -0.22746429 -0.2794186  0.27095247
 Jun 2012  0.12403035 -0.2747109  0.32488093
 Jul 2012 -0.10418880 -0.2700031 -0.61519386
 Aug 2012  0.14560622 -0.2649596  0.08891071
 Sep 2012  0.16699531 -0.2599160  0.27346079
 Oct 2012 -0.21709617 -0.2550556 -0.52784826
 Nov 2012  0.20363225 -0.2501951  0.19201780
 Dec 2012  0.30885826 -0.2451385  0.13415736
</code></pre>

<p>Now based on these results, I am not sure how to decide whether there is a strong/weak trend and seasonality, and whether I should remove the trend and seasonality effects in order to build my regression model?
In another words I would like to know how to interpret the plots and the resulting trend and seasonality numbers, e.g. what does it mean in Dec 2012 when it says ""seasonal"" = 0.30885826, etc.?</p>

<p>Thanks</p>
"
"0.101115324337403","0.102985730108887","153802","<p>I have a large data set with repeated measurements of same blood value (co) (2 to 7 measurements per patient). Each measurement is coupled with time which is the time interval between surgical operation and blood level measurement.</p>

<p>My aim is to show that this blood value correlates positively with time.</p>

<p>Blood level measurements are highly skewed to right and hence I am using a log-transformation and linear mixed effect regression model (lmer in lme4 package).</p>

<p>At first I assumed random intercepts among patients. I constructed a null model and model with time as independent.</p>

<pre><code>fit0&lt;-(lmer(lgco~(1|id),data=mebhr))
fit1&lt;-(lmer(lgco~time+(1|id),data=mebhr))
anova(fit0,fit1)
refitting model(s) with ML (instead of REML)
Data: mebhr
Models:
fit0: lgco ~ (1 | id)
fit1: lgco ~ time + (1 | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)    
fit0  3 200.44 213.16 -97.219   194.44                             
fit1  4 189.62 206.59 -90.811   181.62 12.815      1  0.0003438 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Ok, so I have an empty model and model with independent variable.
<img src=""http://i.stack.imgur.com/SFIFL.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/8RbgF.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/phYJ1.png"" alt=""enter image description here"">
<img src=""http://i.stack.imgur.com/G4HNH.png"" alt=""enter image description here""></p>

<p>Adding covariate time in my model improves it significantly and also the graphical explanation is clear.</p>

<p>Fixed slopes, however, are not reasonable in my data, so I should use random slopes.</p>

<pre><code>fit0&lt;-(lmer(lgco~(time|id),data=mebhr))
fit1&lt;-(lmer(lgco~time+(time|id),data=mebhr))
anova(fit0,fit1)
refitting model(s) with ML (instead of REML)
Data: mebhr
Models:
fit0: lgco ~ (time| id)
fit1: lgco ~ time + (time | id)
     Df    AIC    BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)   
fit0  5 190.15 211.36 -90.076   180.15                            
fit1  6 182.06 207.51 -85.029   170.06 10.094      1   0.001487 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>At this point I dont understand my model equations. Graphical outputs for fit0 and fit1 are as follows:
<img src=""http://i.stack.imgur.com/E4w5D.png"" alt=""enter image description here""></p>

<p><img src=""http://i.stack.imgur.com/XyeTJ.png"" alt=""enter image description here""></p>

<p>For the fit1 the model equation is:
<img src=""http://i.stack.imgur.com/Z6WLa.png"" alt=""enter image description here""></p>

<p>Why the lines in fit0 have non-zero slopes? What are they and what is the equation in that case? Also I dont understand how should I clarify the change in model fit? In the case of only random intercepts I can state that ""adding fixed factor <em>beta1</em> to model improves it significantly"". What would be the equal statement in the case of random slopes?</p>
"
"0.0484501583111509","0.0328975847479884","154416","<p>I have climate data for 240 predictors and precipitation flux (as the target variable) for 3000+ days. I want to use <strong>Gaussian kernel regression</strong> to predict the precipitation flux for the next 2000+ days.</p>

<p>I have gone through some of the available packages in both <code>R</code> and <code>MatLab</code>. </p>

<p><code>R</code> has the <code>np</code> package which provides the <code>npreg()</code> to perform kernel regression. However, the documentation for this package does not tell me how I can use the model derived to predict new data.</p>

<p>Similarly, MatLab has the codes provided by <strong>Yi Cao</strong> (<a href=""http://www.mathworks.com/matlabcentral/fileexchange/19279-multivariant-kernel-regression-and-smoothing"" rel=""nofollow"">ksrmv.m</a>) and <strong>Youngmok Yun</strong> (<a href=""http://youngmok.com/gaussian-kernel-regression-for-multidimensional-feature-with-matlab-code/"" rel=""nofollow"">gaussian_kern_reg.m</a>). Here as well, I am unable to understand how to use the model obtained to predict the new data.</p>

<p>I would be very grateful if someone can explain to me how to perform prediction using any one of these methods.</p>
"
"0.104897669787086","0.113960576459638","154588","<p>I'm trying to build a covariance-based structural equation model (SEM) using both reflective and formative specifications of latent variables. I use the <code>sem</code> function in the <code>lavaan</code> package for estimation (R version 3.1.3, lavaan version 0.5-18). But estimates turn always out to be zero which is unreasonable.</p>

<p>The lavaan model syntax uses <code>=~</code> for reflective specification of latent variables, <code>&lt;~</code> for formative specification of latent variables, and <code>~</code> for regressions (<a href=""http://www.inside-r.org/packages/cran/lavaan/docs/model.syntax"" rel=""nofollow"">http://www.inside-r.org/packages/cran/lavaan/docs/model.syntax</a>). Here is a simple working example with only reflective specifications (it is a simplified version of the example provided at <a href=""http://lavaan.ugent.be/tutorial/sem.html"" rel=""nofollow"">http://lavaan.ugent.be/tutorial/sem.html</a> and by <code>example(sem)</code>)</p>

<pre><code>library(lavaan)
model &lt;- ' 
# latent variable definitions
ind60 =~ x1 + x2 
dem60 =~ y1 + y2
# regressions
dem60 ~ ind60
'
summary(sem(model, data=PoliticalDemocracy))
</code></pre>

<p>Now assume that based on prior theory I would know that dem60 is a formative construct composed of y1 and y2. Thus I change the specification from <code>=~</code> to <code>&lt;~</code> and obtain the following code</p>

<pre><code>library(lavaan)
model &lt;- ' 
# latent variable definitions
ind60 =~ x1 + x2 
dem60 &lt;~ y1 + y2
# regressions
dem60 ~ ind60
'
summary(sem(model, data=PoliticalDemocracy))
</code></pre>

<p>The estimates for both y1 and y2 turn out to be zero. Analogously, the regression effect of ind60 on dem60 turns out to be zero. What do I need to change to get a meaningful result?</p>

<p>Several websites and blogs suggested the following modifications:
(1) Fix one parameter in the formative construct, i.e. <code>dem60 &lt;~ 1*y1 + y2</code>.
(2) Allow for covariance of the manifest indicators, i.e. <code>y1 ~~ y2</code>. 
(3) Fix the variance of the formative construct, i.e. <code>dem60 ~~ 1</code>.
(4) Free the variance of the formative construct, i.e. <code>dem60 ~~ NA*dem60</code>. 
None of these are working. Again: What do I need to change to get a meaningful result?</p>
"
"0.0692289300613081","0.080582296402538","154621","<p>I using the regression method called <code>MARS</code>, in <code>R</code> is it called <code>earth</code> and is located in the package <code>earth</code>, in order to find the best regression model for my datat.</p>

<p>I know that this method is suitable for large data-sets, can handle <code>NA</code> and also decides which variables will be used and which not into the regression.</p>

<p><strong>What I'm doing</strong></p>

<p>After the regression is estimated, I detect the <code>outliers</code> using <code>boxplot</code>  and then I eliminate from the data the observations which are <code>extreme values</code> and compute the model again.</p>

<p>I do this until maximum of <code>grsq</code> and <code>rsq</code> are found.</p>

<p><strong>CODE</strong></p>

<pre><code>model &lt;- earth(log(price) ~ ., data = data, weights = weights)
max_grsq &lt;- round(model$grsq, digits = 4)
    max_rsq &lt;- round(model$rsq, digits = 4)
min_diff &lt;- abs(max_grsq - max_rsq)

while(!done) {
  residuals_abs &lt;- abs(model$residuals)
      boxplot &lt;- boxplot(residuals_abs, plot=F)
      indexes_to_remove &lt;- c(which((residuals_abs &gt; boxplot$stats[4]) == T), which((residuals_abs &lt; boxplot$stats[2]) == T))

  if (length(indexes_to_remove) &gt; 0) {
    data &lt;- data[-indexes_to_remove, ]
    distances &lt;- distances[-indexes_to_remove]
    weights &lt;- (1/distances)/(sum(1/distances))
  }

  tempModel &lt;- earth(log(price) ~ ., data = data, weights = weights)
  temp_grsq &lt;- round(tempModel$grsq, digits = 4)
      temp_rsq &lt;- round(tempModel$rsq, digits = 4)
  temp_diff &lt;- abs(temp_grsq - temp_rsq)

  if ((temp_grsq &gt; max_grsq &amp;&amp; temp_rsq &gt;= max_rsq) || (temp_grsq &gt;= max_grsq &amp;&amp; temp_rsq &gt; max_rsq)) {
    model &lt;- tempModel
    max_grsq &lt;- temp_grsq
    max_rsq &lt;- temp_rsq
    min_diff &lt;- temp_diff
  } else {
    done = T
  }
 }
</code></pre>

<p><strong>QUESTION</strong></p>

<p>I'm not a statistician so I don't know any better way for removing the outliers. </p>

<ul>
<li>is my approach correct?</li>
<li>should I use another approach?</li>
<li>I know that there are bad outliers and good outliers (leverage points), how can I remove only the bad outliers?</li>
<li>I'm using the <code>semi-log form</code> of the regression. because of the use of <code>dummy variables</code> I can't use the <code>log-log form</code>. Is there any other approach for data transformation? or should I standardize the data? <code>x &lt;- (x - x_min)/(x_max - x_min)</code></li>
</ul>

<p>Does anyone has some hints?</p>
"
"0.027972711943223","0.0284901441149095","155495","<p>Could someone explain to me what is criterion for interpretation of Breusch-Pagan test?</p>

<p>I have applied ncvTest test from the package car in R on a simple linear regression with one predictor variable e.g. lm(weight~size). I have the following result:</p>

<p>Chisquare = 7.182687    Df = 1     p = 0.007361039 </p>

<p>I see in other questions that p = 0.073459 implies heteroscedasticity
while p = 0.6283239 and p-value = 0.858 imply homoscedascity. By looking at these samples I would assume that my result set is heteroscedasticit, but I would like to know is p value only criterion and is there some boundary value for yes/no decision (i.e. some p value between 0.007 and 0.6).</p>

<p>Does Chisquare value matters?</p>
"
"0.0559454238864459","0.056980288229819","156034","<p>I am dealing with a heteroscedastic censored dataset. I tried to use the survival analysis package in R to estimate a linear model for it. So before doing that, I conducted a simulation study, where I generate a sample for x:
$x  \sim Unif[-2,2]$
and y:
$y \sim 1+0.3x+0.6(1-0.3x)\epsilon $ where $\epsilon\sim N(0,1)$.
y is censored as $y=y$ if $y&lt;x$; $y=x$ otherwise.</p>

<p>Then I use the survreg function in R to estimate a linear model for it.</p>

<p><code>x&lt;-runif(10000,-2,2)</code></p>

<p><code>y&lt;-1+0.3*x+rnorm(length(x))*0.6*(1-0.3*x)</code></p>

<p><code>y[y&gt;=x]=x[y&gt;=x]</code></p>

<p><code>fit&lt;-survreg(Surv(y, y&lt;x, type='right')~x, dist='gaussian')</code></p>

<p>The result is really poor, the estimate is around(intercept = 0.5, slope = 0.6) and it is very stable no matter what initial points I gave.</p>

<p>The result does not get significant improved even if I feed the true weight to it:</p>

<p><code>fit&lt;-survreg(Surv(y, y&lt;x, type='right')~x, dist='gaussian', 'weights=1/(1-0.3x)^2')</code></p>

<p>But the estimation is great when no heteroscedasticity is presented:</p>

<p><code>x&lt;-runif(10000,-2,2)</code></p>

<p><code>y&lt;-1+0.3*x+rnorm(length(x))*0.6</code></p>

<p><code>y[y&gt;=x]=x[y&gt;=x]</code></p>

<p><code>fit&lt;-survreg(Surv(y, y&lt;x, type='right')~x, dist='gaussian')</code></p>

<p>I also tried quantile regression for censored data with tau=0.5. (basically LAD regression which is proposed by J.Powell 1984). The result is still poor under this parameter setting.</p>

<p>Is there any good way to get a consistent estimation for heteroscedastic censored data?</p>

<p>Any suggestion on package, software or papers are welcome.</p>

<p>Thanks a lot.</p>
"
"0.0685188709827532","0.0697863157798853","156037","<p>The R Vars package has a Vector Auto Regression function called var. The arguments include (among other things) ""p"" defined as the ""Integer for the lag order"" and ""lag.max,"" which is defined as ""Integer, determines the highest lag order for lag length selection according to the choosen (sic) ic."" See cran.r-project.org/web/packages/vars/vars.pdf.</p>

<p>My questions are:</p>

<p>What is the definition of the ""lag order?""</p>

<p>I thought that ""lag.max"" meant the highest number of lags to consider, but the package documentation defines ""lag.max"" as the highest lag order. So, if the ""lag.max"" is the highest ""lag order"" then, clearly the ""lag order"" argument is not asking for the maximum ""lag order"" because that would be redundant. So, what is the argument for ""lag order"" asking for? ... the minimum lag order? ... the actual order of the lags, i.e., an order such as t-1, t-3, t-2 instead of t-1, t-2, t-3?</p>

<p>The definition of ""lag.max"" uses the term ""lag length."" What is that? I would have thought that it would also be the maximum number of lags to consider, but clearly from the context, that is not the case. So, what are the definitions of ""lag.max"" and ""lag length?""</p>

<p>Statistics would not be hard if statisticians would learn to define their terms!</p>
"
"0.0395593886064618","0.040291148201269","156610","<p>Whenever I run a logistic regression, I need to set the threshold so that it groups probabilities higher than the threshold to my positive group:</p>

<pre><code>table(test$Noshow, logpred&gt;0.5)
</code></pre>

<p>What I sometimes do is to optimize my confusion matrix so that I get a good Sensitivity and Specificity values. Is this a good way to do it?</p>

<p>On the other hand, when I run a randomForest model using the ""randomForest"" package, I can set the cutoffs such as this:</p>

<pre><code>k &lt;- 0.5
rfcutoff &lt;- c(k, 1-k)
rftree &lt;- randomForest(rffrmla_all, data = train1, mtry = 2, keep.forest = TRUE, importance = TRUE, ntree = 500, cutoff = rfcutoff)
</code></pre>

<p>Again I optimize my confusion matrix to get a good cutoff or base the cutoff on a naive baseline proportion. What is a good way to do this? </p>
"
"0.0839181358296689","0.0854704323447285","156661","<p><img src=""http://i.stack.imgur.com/6gO09.gif"" alt=""enter image description here""></p>

<p>I get a similar scatter plot (as above) showing the relation between two different quantitative variables. It is also fan-shaped.</p>

<p>I am trying to fit a linear model for this relation. I think I should apply some kind of transformation to the variables in order to unify the ascent variance in the relation before fitting a linear regression model, but I can't find the way to do it. Or maybe, there is a better model to use in these cases, I can't either find it.</p>

<p>I have tried <code>rlm</code>, but when plotting the residuals vs the predictor they are very skewed. I have tried to use the weights retrieved by the model to transform the output:</p>

<pre><code>fit &lt;- rlm(y ~ x, data=df)
plot(fit$resid ~ df$x) # Heteroskedastic &amp; Skewed

df$y &lt;- df$y * fit$w 
    plot(df$y ~ df$x) # Wrong
</code></pre>

<p>But it is obviously wrong.</p>

<p>I have also tried to apply a SD ratio calculated from all the y of each x and other similar erratic approaches.</p>

<p>Is there any typical way of fitting a model for this kind of relation (fan-shaped) or a typical model (and R package) to use in this cases?</p>

<p>Is there any typical transformation that could be applied to the variables in order to reduce the variance?</p>

<p><strong>Sorry, I duplicated the post: Follow <a href=""http://stats.stackexchange.com/questions/156654/fit-regression-model-from-a-fan-shaped-relation-in-r"">Fit regression model from a fan-shaped relation, in R</a>, instead.</strong></p>
"
"0.0685188709827532","0.0697863157798853","157003","<p>I am using <code>glm (target, formula = target~.,family=binomial)</code> to predict binary outcome.</p>

<p>I have 9 grouped predictors.
I convert them into factors so that I can test them in the regression as dummies.</p>

<p>Additionally, I have initially set the reference group <code>relevel=var_name(var_name, ref = ""ref_group_name"")</code></p>

<p>I run the regression and the result is decent at first glance (good Gini, just a few dummies with high significance).</p>

<p>What I would like to do further is as follows:</p>

<p>1) check Mallows' Cp ( one thing to be checked for overfitting of the model that I know is that Mallows' Cp should be close to the number of dummies entering the model)</p>

<p>2) interactively add/remove dummies in the model</p>

<p>3) check correlation between dummies rather than between the whole variables</p>

<p>For the <strong>first</strong> question I am not sure how to test Mallows' Cp (summary of the <code>glm</code> result does not show it)?</p>

<p>For the <strong>second</strong> question I found package called <code>dummies(dummy.data.frame function)</code> that can easily convert factor variables into dummies so that I can interactively add or remove them (I found that potentially I can do that with package <code>leaps(update function)</code>)
What I am missing is when I create the dummies data frame with <code>dummy.data.frame</code> how can I select which is the <code>reference group</code> when I put the resulting data.frame into the glm function?</p>

<p>For the <strong>third</strong> question: does it now make sense to run Pearson/Spearman correlation on the dummies data.frame (note: initially variables are both numeric and categorical before they were grouped)</p>
"
"0.0685188709827532","0.0697863157798853","157186","<p>Let's say you're trying to fit a model to a dataset that includes categorical variables, group (A or B) and treatment (1, 2, 3 or 4).</p>

<p>In R, your model formula would be <strong>DV ~ group * treatment</strong> (DV stands for dependent variable) and your model output will look like this:</p>

<pre><code>(intercept)                 [...]
groupB                      [...]
treatment2                  [...]
treatment3                  [...]
treatment4                  [...]
groupB:treatment2           [...]
groupB:treatment3           [...]
groupB:treatment4           [...]
</code></pre>

<p>My question is how to interpret this kind of output. Below is what I believe is right for the interpretation of the main effects, and what puzzles me about the interaction parameters.</p>

<pre><code>(intercept)
</code></pre>

<p>This the reference value, i.e. for treatment 1 in group A.</p>

<pre><code>groupB
</code></pre>

<p>This is the difference between group A and group B for treatment 1 only.</p>

<pre><code>treatment2
</code></pre>

<p>This is the difference between treatment 1 and treatment 2, within group A only. It indeed still refers to the intercept value.
Same logic for the two following estimates (""treatment3"" and ""treatment4"").</p>

<pre><code>groupB:treatment2
</code></pre>

<p>Here is where I get puzzled. Is this testing if <strong>the difference between treatment1 and treatment2 is the same in groupB compared to groupA</strong>, or is it testing if if <strong>the difference between groupA and groupB is the same for treatment1 compared to treatment2</strong>.</p>

<p>I thought this question would be very basic, but I went through several R books with no luck and found inconsistent answers on here (see <a href=""http://stats.stackexchange.com/questions/87412/how-to-interpret-2-way-and-3-way-interaction-in-lmer"">http://stats.stackexchange.com/questions/87412/how-to-interpret-2-way-and-3-way-interaction-in-lmer</a> for support for the first idea and <a href=""http://stats.stackexchange.com/questions/33709/interpreting-the-regression-output-from-a-mixed-model-when-interactions-between"">http://stats.stackexchange.com/questions/33709/interpreting-the-regression-output-from-a-mixed-model-when-interactions-between</a> for the other way).</p>

<p>If that matters, I'm working with the glmer function of the lme4 package.</p>

<p>Thanks!</p>
"
"0.027972711943223","0.0284901441149095","157755","<p>I am using <code>partykit</code> R package to perform a trees of poisson regression. I would like to have the estimated (<code>predict</code> with <code>type = ""response""</code>) value at each. A reproducible example follows: </p>

<pre><code>set.seed(1)
d &lt;- data.frame(
 x1 = runif(500),
 x2 = runif(500),
 exposure = runif(500, 1, 10)
)
d$claims &lt;- rpois(500, lambda = exp(d$x1 &gt; 0.5 &amp; d$x2 &gt; 0.5) * d$exposure)
m &lt;- glmtree(claims ~ offset(log(exposure)) | x1 + x2,
  data = d, family = poisson)
</code></pre>

<p>I would like to plot the <code>predict(..., type = ""response"")</code> at the end of each leaf.</p>
"
"0.0685188709827532","0.0581552631499044","157777","<p>I am using the boot function in R to get standard errors for several statistics (I am doing a oaxaca blinder decomposition). My data (EU-SILC) has sample weights (PB040) for every observation. My understanding is that I have to use those weights for every OLS regression or other package I run to correct for sampling errors of the survey.</p>

<p>The thing is now that the boot package in R also can use sample weights.</p>

<p>Should I use the same sample weights for the call of the boot function and for the calculations ""inside"" the boot function?</p>

<p>My reason for this question is that since the boot function does a weighted draw from the original sample the function ""inside"" the boot-function should allready have a weighted sample. So using sample weights inside the boot function would be redundant or lead to a bias.</p>

<p>This is my first posting to cross validated and I am pretty new to bootstraping and econometrics so I hope I asked the question concise enough.</p>
"
"0.108550066801774","0.103648123654272","157851","<p>In the <code>lmer</code> function within <code>lme4</code> in <code>R</code> there is a call for constructing a model matrix of random effects, $Z$, as explained <a href=""http://cran.r-project.org/web/packages/lme4/vignettes/lmer.pdf"" rel=""nofollow"">here</a>, pages 7 - 9.</p>

<p>Calculating $Z$ entails KhatriRao and/or Kronecker products of two matrices, $J_i$ and $X_i$.  </p>

<p>The matrix $J_i$ is a mouthful: ""Indicator matrix of grouping factor indices"", but it seems to be a sparse matrix with dummy coding to select which unit (for example, subjects in repetitive measurements) corresponding to higher hierarchical levels are ""on"" for any observation. The $X_i$ matrix seems to act as a selector of measurements in the lower hierarchical level, so that the combination of both ""selectors"" would yield a matrix, $Z_i$ of the form illustrated in the paper via the following example:</p>

<pre><code>(f&lt;-gl(3,2))

[1] 1 1 2 2 3 3
Levels: 1 2 3

(Ji&lt;-t(as(f,Class=""sparseMatrix"")))

6 x 3 sparse Matrix of class ""dgCMatrix""
     1 2 3
[1,] 1 . .
[2,] 1 . .
[3,] . 1 .
[4,] . 1 .
[5,] . . 1
[6,] . . 1

(Xi&lt;-cbind(1,rep.int(c(-1,1),3L)))
     [,1] [,2]
[1,]    1   -1
[2,]    1    1
[3,]    1   -1
[4,]    1    1
[5,]    1   -1
[6,]    1    1
</code></pre>

<p>Transposing each of these matrices, and performing a Khatri-Rao multiplication:</p>

<p>$\left[\begin{smallmatrix}1 &amp; 1 &amp;. &amp;. &amp;. &amp;.\\.&amp;.&amp;1&amp;1&amp;.&amp;.\\.&amp;.&amp;.&amp;.&amp;1&amp;1 \end{smallmatrix}\right]\ast \left[\begin{smallmatrix}\,\,\,\,1 &amp; 1 &amp;\,\,\,\,1 &amp;1 &amp;\,\,\,\,1 &amp;1\\-1&amp;1&amp;-1&amp;1&amp;-1&amp;1 \end{smallmatrix}\right]=
\left[\begin{smallmatrix}\,\,1 &amp; 1 &amp;.&amp;.&amp;.&amp;.\\\,\,\,\,-1 &amp;1&amp;.&amp;.&amp;.&amp;.\\ .&amp;.&amp;\,\,\,\,\,1 &amp;1&amp;.&amp;.\\.&amp;.&amp;\,\,-1&amp;1&amp;.&amp;.\\.&amp;.&amp;.&amp;.&amp;\,\,\,1&amp;1\\.&amp;.&amp;.&amp;.&amp;-1&amp;1 \end{smallmatrix}\right]$</p>

<p>But $Z_i$ is the transpose of it:</p>

<pre><code>(Zi&lt;-t(KhatriRao(t(Ji),t(Xi))))

6 x 6 sparse Matrix of class ""dgCMatrix""

[1,] 1 -1 .  . .  .
[2,] 1  1 .  . .  .
[3,] .  . 1 -1 .  .
[4,] .  . 1  1 .  .
[5,] .  . .  . 1 -1
[6,] .  . .  . 1  1
</code></pre>

<p>It turns out that the authors make use of the database <code>sleepstudy</code> in <code>lme4</code>, but don't really elaborate on the design matrices as they apply to this particular study. So I'm trying to understand how the made up code in the paper reproduced above would translate into the more meaningful <code>sleepstudy</code> example.</p>

<p>For visual simplicity I have reduced the data set to just three subjects - ""309"", ""330"" and ""371"":</p>

<pre><code>require(lme4)
sleepstudy &lt;- sleepstudy[sleepstudy$Subject %in% c(309, 330, 371), ]
rownames(sleepstudy) &lt;- NULL
</code></pre>

<p>Each individual would exhibit a very different intercept and slope should a simple OLS regression be considered individually, suggesting the need for a mixed-effect model with the higher hierarchy or unit level corresponding to the subjects:</p>

<pre><code>    par(bg = 'peachpuff')
    plot(1,type=""n"", xlim=c(0, 12), ylim=c(200, 360),
             xlab='Days', ylab='Reaction')
    for (i in sleepstudy$Subject){
                fit&lt;-lm(Reaction ~ Days, sleepstudy[sleepstudy$Subject==i,])
            lines(predict(fit), col=i, lwd=3)
            text(x=11, y=predict(fit, data.frame(Days=9)), cex=0.6,labels=i)
        }
</code></pre>

<p><img src=""http://i.stack.imgur.com/opwVvm.png"" alt=""enter image description here""></p>

<p>The mixed-effect regression call is:</p>

<pre><code>fm1&lt;-lmer(Reaction~Days+(Days|Subject), sleepstudy)
</code></pre>

<p>And the matrix extracted from the function yields the following:</p>

<pre><code>parsedFormula&lt;-lFormula(formula= Reaction~Days+(Days|Subject),data= sleepstudy)
parsedFormula$reTrms

$Ztlist
    $Ztlist$`Days | Subject`
6 x 12 sparse Matrix of class ""dgCMatrix""

309 1 1 1 1 1 1 1 1 1 1 . . . . . . . . . . . . . . . . . . . .
309 0 1 2 3 4 5 6 7 8 9 . . . . . . . . . . . . . . . . . . . .
330 . . . . . . . . . . 1 1 1 1 1 1 1 1 1 1 . . . . . . . . . .
330 . . . . . . . . . . 0 1 2 3 4 5 6 7 8 9 . . . . . . . . . .
371 . . . . . . . . . . . . . . . . . . . . 1 1 1 1 1 1 1 1 1 1
371 . . . . . . . . . . . . . . . . . . . . 0 1 2 3 4 5 6 7 8 9
</code></pre>

<p>This seems right, but if it is, what is linear algebra behind it? I understand the rows of <code>1</code>'s being the selection of individuals like. For instance, subject <code>309</code> is on for the baseline + nine observations, so it gets four <code>1</code>'s and so forth. The second part is clearly the actual measurement: <code>0</code> for baseline, <code>1</code> for the first day of sleep deprivation, etc.</p>

<p><strong>But what are the actual</strong> $J_i$ <strong>and</strong> $X_i$ <strong>matrices and the corresponding</strong> $Z_i= (J_i^{T}âˆ—X_i^{T})^âŠ¤$ <strong>or</strong> $Z_i= (J_i^{T}\otimes X_i^{T})^âŠ¤$, <strong>whichever is pertinent?</strong></p>

<p>Here is a possibility,</p>

<p>$\left[\begin{smallmatrix}1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp;1&amp;1&amp;.  &amp;. &amp;. &amp;. &amp;.&amp; . &amp;. &amp;. &amp;. &amp;. &amp;. &amp;.&amp;. &amp; . &amp;. &amp;. &amp;. &amp;. &amp;. &amp;.\\
.&amp;.&amp;.&amp;.&amp;. &amp; . &amp;. &amp;. &amp;. &amp;. &amp;1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp;1&amp;1&amp;.&amp; . &amp;. &amp;. &amp;. &amp;. &amp;. &amp;.&amp;.&amp;.\\&amp;. &amp; . &amp;. &amp;. &amp;. &amp;. &amp;. &amp;.&amp;. &amp; . &amp;. &amp;. &amp;. &amp;. &amp;.&amp;.&amp;.&amp;.&amp;.&amp;1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp; 1 &amp;1&amp;1\end{smallmatrix}\right]\ast \left[\begin{smallmatrix}1 &amp; 1 &amp; 1 &amp; 1&amp;1&amp;1&amp;1 &amp; 1 &amp; 1 &amp; 1\\0&amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9 \end{smallmatrix}\right]=$</p>

<p>$\left[\begin{smallmatrix}1 &amp; 1 &amp; 1 &amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.\\0 &amp; 1 &amp; 2 &amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.\\.&amp;.&amp;.&amp;.&amp;&amp;.&amp;.&amp;.&amp;.&amp;.&amp;1 &amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.\\ &amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;0 &amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.\\.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1&amp;1\\&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;.&amp;0&amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9 \end{smallmatrix}\right] $</p>

<p>The problem is that it is not the transposed as the <code>lmer</code> function seems to call for, and still is unclear what the rules are to create $X_i$.</p>
"
"0.101115324337403","0.102985730108887","158366","<p>I'm trying to fit a multiple regression model with pairwise deletion in the context of missing data.  <code>lm()</code> uses listwise deletion, which I'd prefer not to use in my case.  I'd also prefer not to use multiple imputation or FIML.  How can I do multiple regression with pairwise deletion in R?</p>

<p>I have tried the <code>mat.regress()</code> function of the <code>psych</code> package, which fits regression models to correlation/covariance matrices (which can be obtained from pairwise deletion), but the regression model does not appear to include an intercept parameter.</p>

<p>Here's what I've tried (small example):</p>

<pre><code>set.seed(33333)
y &lt;- rnorm(1000)
x1 &lt;- y*2 + rnorm(1000, sd=.2)
x2 &lt;- y*5 + rnorm(1000, sd=.5)

y[sample(1:1000, 10)] &lt;- NA
x1[sample(1:1000, 10)] &lt;- NA
x2[sample(1:1000, 10)] &lt;- NA

mydata &lt;- data.frame(y, x1, x2)
covMatrix &lt;- cov(mydata, use=""pairwise.complete.obs"")

#Listwise Deletion
listwiseDeletion &lt;- lm(y ~ x1 + x2, data=mydata)
observations &lt;- length(listwiseDeletion$na.action) #30 rows deleted due to listwise deletion

coef(listwiseDeletion)
(Intercept)          x1          x2 
0.001995527 0.245372245 0.100001989

#Pairwise Deletion --- but missing intercept
pairwiseDeletion &lt;- mat.regress(y=""y"", x=c(""x1"",""x2""), data=covMatrix, n.obs=observations)
pairwiseDeletion$beta
       y
x1 0.1861277
x2 0.1251995

#Pairwise Deletion --- tried to add intercept, but received error when fitting model
mydata$intercept &lt;- 0
covMatrixWithIntercept &lt;- cov(mydata, use=""pairwise.complete.obs"")

pairwiseDeletionWithIntercept &lt;- mat.regress(y=""y"", x=c(""intercept"",""x1"",""x2""), data=covMatrixWithIntercept, n.obs=observations)
Something is seriously wrong the correlation matrix.
In smc, smcs were set to 1.0
Warning messages:
1: In cov2cor(C) :
  diag(.) had 0 or NA entries; non-finite result is doubtful
2: In cor.smooth(R) :
  I am sorry, there is something seriously wrong with the correlation matrix,
cor.smooth failed to  smooth it because some of the eigen values are NA.  
Are you sure you specified the data correctly?
</code></pre>

<p>So, how can I obtain an intercept parameter using <code>mat.regress</code>, or how can I obtain parameter estimates from pairwise deletion using another method or package in R?  I've seen <a href=""https://stats.stackexchange.com/questions/107597/is-there-a-way-to-use-the-covariance-matrix-to-find-coefficients-for-multiple-re"">matrix calculations</a> to do this, but, ideally, there'd be a package that also outputs regression diagnostics, fit stats, etc.  Also, preferably, the method would be able to fit interaction terms.</p>
"
"0.0625488854200668","0.0637058989297032","158492","<p>I have fitted a (Cragg's) truncated normal hurdle model over a dataset in which the dependent variable is either zero or positive. The model consists of two parts: a probit which estimates the probability of the value being zero and a truncated regression which is estimated over the subsample of positive values of the dependent variable.</p>

<p>The output of the estimation (using package <code>mhurdle</code> in R) consists of two columns: one gives the probability of y being 0 and the other gives the estimated value for an uncensored observation (let's call this y*).</p>

<p>Now I would like to use these results for prediction, but instead of probabilities and estimated values of the uncensored y I would like to have the estimated values of the dependent variable, including some zeros (or almost zeros). Should I multiply the probability of y NOT being zero by the value of y*? Or should I take all observations for which P(y=0) > 0.5 to be zero and all the others to be equal to y*?</p>

<p>Sorry if the question is trivial but I'm fairly new to statistics and I have not been able to find the answer so far.</p>
"
"0.0484501583111509","0.0493463771219827","158701","<p>I use the svm function (for regression) to make forecast like I would with for exemple the arima function:<br>
<code>fit&lt;-auto.arima(ts)</code><br>
<code>prediction&lt;-forecast(fit,h=20)</code><br>
which returns different attributes : </p>

<blockquote>
  <ol>
  <li><code>prediction$mean</code> which is the actual prediction  </li>
  <li><code>prediction$lower</code> and <code>prediction$upper</code> which are the   <strong>boundaries of the confidence intervals</strong> on each points of the   <code>prediction$mean</code>.  </li>
  </ol>
</blockquote>

<p>I would like the <code>svm</code> function (from <em>e1071</em> package) to return a more detailed answer than just the value (like the <code>forecast()</code> would).<br>
 But I guess it is not implemented in the function yet.
Is there another function to do it ? Or should I use <strong>bootstrap</strong> methods to try to estimate those boundaries? And if I should use this are they pre-implemented version of them instead of using sample over a for loop which is very time-consuming ?</p>
"
"0.027972711943223","0.0284901441149095","159053","<p>I'm using the CausalImpact to evaluate the effect of a programme. My covariates are seasonal and I wonder whether I need to deseasonalise/ detrend the regressors before using the R package?</p>

<p>Hal Varian did this in the below analysis: </p>

<p><a href=""http://people.ischool.berkeley.edu/~hal/Papers/2013/pred-present-talk.pdf"" rel=""nofollow"">http://people.ischool.berkeley.edu/~hal/Papers/2013/pred-present-talk.pdf</a></p>

<p>slide 24:</p>

<ul>
<li>Deseasonalize predictors using R command stl</li>
<li>Detrend predictors using simple linear regression</li>
<li>Let bsts choose predictors</li>
</ul>
"
"0.0484501583111509","0.0493463771219827","159261","<p>(<strong>EDIT</strong>: the question has been modified just a little bit to be more specific) </p>

<p>I want to fit a multivariate polynomial regression that accounts for measurement errors (an Error-in-Variables model). </p>

<p>As an example, my input data is like: </p>

<pre><code>y      sd_y      x       sd_x       z       sd_z

9.55   0.26     6.74     0.71      0.25     0.02
8.31   0.19     5.93     0.33      -0.40    0.05
...    ...      ...      ...        ...     ...   
</code></pre>

<p>where sd_y, sd_x, sd_z are the standard deviations of each variable, and </p>

<pre><code> wx &lt;- 1/(sd_x)**2 ; wy &lt;- 1/(sd_y)**2 ; wz &lt;- 1/(sd_z)**2 
</code></pre>

<p>would be the weights for each variable.</p>

<p>If I use a standard regression model (where predictors are supposed to have been measured exactly or without error) my function or fit, in R, would be:</p>

<pre><code>p &lt;- lm(y~polym(x, z, degree = 2, raw=TRUE))
</code></pre>

<p>Is there a method/package in R that allows to deal with ""error-in-variables models""? If so, how I would write my fit <em>p</em> when using the supposed package?</p>
"
"NaN","NaN","159299","<p>I would like to compute R-squared change for the interaction/moderation term in a multiple regression model, along with the corresponding F- and p-values. Previously, I have worked with the modprobe macro by A. Hayes, which can produce this for SPSS. As I am transitioning to R now, I am trying to find a function/package or a custom-made script in R that does this. In case it helps, my current interaction model looks like this:</p>

<pre><code>m1 &lt;- lm(all_ART~Neuroticism*Agreeableness+Attentional.Control, 
         data=stp2_sub2, na.action=na.omit)
</code></pre>

<p>Any pointers on how to compute these values (i.e., $R^2$(interaction), F-value(interaction) and p-value(interaction)) for the interaction term in R would be much appreciated!</p>
"
"0.0395593886064618","0.040291148201269","159301","<p>I have a classification problem I am attempting to model using logistic regression (via the <code>glm</code> package in R): </p>

<pre><code>cols &lt;- c(""x"", ""z"", ""a"", ""b"", ""c"") 
formula = paste0(""x ~ "", paste(cols, collapse = ""+""))
formula = as.formula(formula)
</code></pre>

<p>I have a bunch of explanatory variables at the moment. How advisable is it to model this relationship using <code>gbm</code>, see the relative inference strength of each variable, and then remove seemingly meaningless variables before <code>glm</code> regression?</p>

<p>I ask just because I have done this in the past, with <code>glm</code> and <code>gbm</code> giving seemingly contradictory signals on different explanatory variables. </p>
"
"NaN","NaN","159337","<p>I am unfamiliar with the implementation used in the <code>R</code> package GVLMA. What are some basic tests of heteroscedasticity in linear regression models and how or where are they implemented?</p>
"
"0.150377719523045","0.162168743285201","159355","<p>I performed regression with robust variances (after Stata 12.1 lnskew transformation). A question of overfitting has been raised.</p>

<p>To summarise what I did: </p>

<ol>
<li>Comparison of disease B (disgrp=2) versus disease C (disgrp=3)
patients with 45-54 dependent observations (FibrosisP, continuous variable) in each disease group. Each patient had observations taken from Regions P, Q and R and Walls X, Y and Z (i.e. 9 observations per patient). </li>
<li>lnskew0 transformation (natural log transformation with zero skew of
resulting distribution) of FibrosisP to give lfibr. </li>
<li>Simple and then multiple regression with clustered robust variances/standard errors (clustered by patient and using independent categorical variables Disease, Wall and Region).</li>
</ol>

<p>Note that Disease A is excluded from this analysis (and is not provided in data set below).</p>

<p>$$ multipleregression: lfibr \sim Disease + Wall + Region $$</p>

<p>I have copied the original Stata v12.1 log file below, which will hopefully tell the whole story. </p>

<pre><code>. gen disgrp=.
(153 missing values generated)

. replace disgrp=1 if disease==""A""
(54 real changes made)

. replace disgrp=2 if disease==""B""
(54 real changes made)

. replace disgrp=3 if disease==""C""
(45 real changes made)

. lnskew0 lfibr= fibrosisp

       Transform |         k     [95% Conf. Interval]       Skewness
-----------------+--------------------------------------------------
  ln(fibrosis-k) |   .0116473      (not calculated)        -2.77e-08

    . gen region1=.
(153 missing values generated)

. replace region1=1 if region==""P""
(51 real changes made)

. replace region1=2 if region==""Q""
(51 real changes made)

. replace region1=3 if region==""R""
(51 real changes made)

. gen wall1=.
(153 missing values generated)

. replace wall1=1 if wall==""X""
(51 real changes made)

. replace wall1=2 if wall==""Y""
(51 real changes made)

. replace wall1=3 if wall==""Z""
(51 real changes made)

. **Comparing C with B**

. xi:regress lfibr disgrp if disgrp &gt; 1  , cluster(pat)

Linear regression                                      Number of obs =      99
                                                       F(  1,    10) =   20.51
                                                       Prob &gt; F      =  0.0011
                                                       R-squared     =  0.3884
                                                       Root MSE      =   .5833

                             (Std. Err. adjusted for 11 clusters in patientid)
------------------------------------------------------------------------------
             |               Robust
       lfibr |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      disgrp |   .9241372    .204061     4.53   0.001     .4694609    1.378813
       _cons |  -4.667246   .4602243   -10.14   0.000    -5.692689   -3.641802
------------------------------------------------------------------------------

. xi:regress lfibr region1 if disgrp &gt; 1  , cluster(pat)

Linear regression                                      Number of obs =      99
                                                       F(  1,    10) =    3.17
                                                       Prob &gt; F      =  0.1055
                                                       R-squared     =  0.0068
                                                       Root MSE      =  .74333

                             (Std. Err. adjusted for 11 clusters in patientid)
------------------------------------------------------------------------------
             |               Robust
       lfibr |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
     region1 |   .0748154   .0420368     1.78   0.105    -.0188483    .1684792
       _cons |   -2.54854    .177876   -14.33   0.000    -2.944872   -2.152207
------------------------------------------------------------------------------

. xi:regress lfibr i.region1 if disgrp &gt; 1  , cluster(pat)
i.region1         _Iregion1_1-3       (naturally coded; _Iregion1_1 omitted)

Linear regression                                      Number of obs =      99
                                                       F(  2,    10) =    5.59
                                                       Prob &gt; F      =  0.0235
                                                       R-squared     =  0.0612
                                                       Root MSE      =  .72645

                             (Std. Err. adjusted for 11 clusters in patientid)
------------------------------------------------------------------------------
             |               Robust
       lfibr |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
 _Iregion1_2 |   .4400513   .1398977     3.15   0.010     .1283397    .7517628
 _Iregion1_3 |   .1496308   .0845103     1.77   0.107    -.0386698    .3379314
       _cons |   -2.59547   .1905071   -13.62   0.000    -3.019946   -2.170993
------------------------------------------------------------------------------

. xi:regress lfibr i.wall1 if disgrp &gt; 1  , cluster(pat)
i.wall1           _Iwall1_1-3         (naturally coded; _Iwall1_1 omitted)

Linear regression                                      Number of obs =      99
                                                       F(  2,    10) =    6.17
                                                       Prob &gt; F      =  0.0180
                                                       R-squared     =  0.0630
                                                       Root MSE      =  .72575

                             (Std. Err. adjusted for 11 clusters in patientid)
------------------------------------------------------------------------------
             |               Robust
       lfibr |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
   _Iwall1_2 |   .3285724   .1654396     1.99   0.075    -.0400499    .6971948
   _Iwall1_3 |   .4356131   .1305289     3.34   0.008     .1447766    .7264496
       _cons |  -2.653637   .1780801   -14.90   0.000    -3.050425    -2.25685
------------------------------------------------------------------------------

. xi3:regress lfibr disgrp*i.region1*i.wall1 if disgrp &gt; 1 , cluster(pat)
i.region1         _Iregion1_1-3       (naturally coded; _Iregion1_1 omitted)
i.wall1           _Iwall1_1-3         (naturally coded; _Iwall1_1 omitted)

Linear regression                                      Number of obs =      99
                                                       F(  8,    10) =       .
                                                       Prob &gt; F      =       .
                                                       R-squared     =  0.5401
                                                       Root MSE      =  .55355

                             (Std. Err. adjusted for 11 clusters in patientid)
------------------------------------------------------------------------------
             |               Robust
       lfibr |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      disgrp |   .5419458   .4154947     1.30   0.221    -.3838342    1.467726
 _Iregion1_2 |  -.2963738   1.409317    -0.21   0.838    -3.436529    2.843781
 _Iregion1_3 |  -.2791626   1.335259    -0.21   0.839    -3.254304    2.695979
   _Iwall1_2 |  -1.039268     .97762    -1.06   0.313    -3.217541    1.139005
   _Iwall1_3 |  -.9227228   1.131622    -0.82   0.434    -3.444133    1.598687
    _IdiXre2 |    .305921   .5859783     0.52   0.613    -.9997201    1.611562
    _IdiXre3 |   .2146185   .5228838     0.41   0.690    -.9504393    1.379676
    _IdiXwa2 |   .5887627   .4158743     1.42   0.187    -.3378629    1.515388
    _IdiXwa3 |   .5677226   .5322211     1.07   0.311      -.61814    1.753585
   _Ire2Xwa2 |   .9560212   1.372943     0.70   0.502    -2.103087    4.015129
   _Ire2Xwa3 |   1.876106   1.632401     1.15   0.277    -1.761111    5.513323
   _Ire3Xwa2 |   .1403149   1.711091     0.08   0.936    -3.672233    3.952863
   _Ire3Xwa3 |   .5961959   1.627029     0.37   0.722     -3.02905    4.221442
_IdiXre2Xwa2 |  -.4387073   .5346165    -0.82   0.431    -1.629907    .7524925
_IdiXre2Xwa3 |  -.7328102   .7126107    -1.03   0.328    -2.320606    .8549855
_IdiXre3Xwa2 |  -.1024311   .6268405    -0.16   0.873    -1.499119    1.294257
_IdiXre3Xwa3 |  -.3174033   .6961228    -0.46   0.658    -1.868461    1.233655
       _cons |  -4.217918   .9792797    -4.31   0.002     -6.39989   -2.035947
------------------------------------------------------------------------------

. xi3:regress lfibr disgrp i.region1 i.wall1 if disgrp &gt; 1 , cluster(pat)
i.region1         _Iregion1_1-3       (naturally coded; _Iregion1_1 omitted)
i.wall1           _Iwall1_1-3         (naturally coded; _Iwall1_1 omitted)

Linear regression                                      Number of obs =      99
                                                       F(  5,    10) =   10.60
                                                       Prob &gt; F      =  0.0010
                                                       R-squared     =  0.5127
                                                       Root MSE      =  .53177

                             (Std. Err. adjusted for 11 clusters in patientid)
------------------------------------------------------------------------------
             |               Robust
       lfibr |      Coef.   Std. Err.      t    P&gt;|t|     [95% Conf. Interval]
-------------+----------------------------------------------------------------
      disgrp |   .9241372   .2084032     4.43   0.001     .4597858    1.388488
 _Iregion1_2 |   .4400513   .1421362     3.10   0.011      .123352    .7567505
 _Iregion1_3 |   .1496308   .0858625     1.74   0.112    -.0416828    .3409444
   _Iwall1_2 |   .3285724   .1680868     1.95   0.079    -.0459482    .7030931
   _Iwall1_3 |   .4356131   .1326175     3.28   0.008     .1401229    .7311033
       _cons |  -5.118535   .4532473   -11.29   0.000    -6.128433   -4.108637
------------------------------------------------------------------------------
</code></pre>

<p>csv data:</p>

<pre><code>""row"",""PatientID"",""Disease"",""Wall"",""Region"",""FibrosisP""
""1"",1,""C"",""X"",""P"",0.11574464021797
""2"",1,""C"",""X"",""Q"",0.06409239204845
""3"",1,""C"",""X"",""R"",0.05589004594181
""4"",2,""C"",""X"",""P"",0.08452786770152
""5"",2,""C"",""X"",""Q"",0.19765474370344
""6"",2,""C"",""X"",""R"",0.29491566808792
""7"",3,""C"",""X"",""P"",0.13849556170319
""8"",3,""C"",""X"",""Q"",0.21529108879539
""9"",3,""C"",""X"",""R"",0.23260346696877
""10"",4,""C"",""X"",""P"",0.03242538798989
""11"",4,""C"",""X"",""Q"",0.18213249953927
""12"",4,""C"",""X"",""R"",0.0464009382069
""13"",17,""C"",""X"",""P"",0.12925196186539
""14"",17,""C"",""X"",""Q"",0.16685146683109
""15"",17,""C"",""X"",""R"",0.16298253982187
""16"",5,""B"",""X"",""P"",0.06082167946576
""17"",5,""B"",""X"",""Q"",0.06179248715729
""18"",5,""B"",""X"",""R"",0.04635879285168
""19"",6,""B"",""X"",""P"",0.0512284261286
""20"",6,""B"",""X"",""Q"",0.05560175796177
""21"",6,""B"",""X"",""R"",0.05038057719884
""22"",7,""B"",""X"",""P"",0.03485909775192
""23"",7,""B"",""X"",""Q"",0.07526805988175
""24"",7,""B"",""X"",""R"",0.03989544438546
""25"",8,""B"",""X"",""P"",0.05069990522336
""26"",8,""B"",""X"",""Q"",0.11638788902232
""27"",8,""B"",""X"",""R"",0.23086071670409
""28"",9,""B"",""X"",""P"",0.12712370092246
""29"",9,""B"",""X"",""Q"",0.05070659692429
""30"",9,""B"",""X"",""R"",0.06183074530974
""31"",10,""B"",""X"",""P"",0.04509566111129
""32"",10,""B"",""X"",""Q"",0.09050081347533
""33"",10,""B"",""X"",""R"",0.05178363738579
""52"",1,""C"",""Y"",""P"",0.14421181658066
""53"",1,""C"",""Y"",""Q"",0.1299066509205
""54"",1,""C"",""Y"",""R"",0.14904819595697
""55"",2,""C"",""Y"",""P"",0.08801608368174
""56"",2,""C"",""Y"",""Q"",0.24864891863453
""57"",2,""C"",""Y"",""R"",0.15962998919524
""58"",3,""C"",""Y"",""P"",0.4272296674396
""59"",3,""C"",""Y"",""Q"",0.2593375589095
""60"",3,""C"",""Y"",""R"",0.26700346966879
""61"",4,""C"",""Y"",""P"",0.14002780500134
""62"",4,""C"",""Y"",""Q"",0.28346720806288
""63"",4,""C"",""Y"",""R"",0.19312813953225
""64"",17,""C"",""Y"",""P"",0.17668051188556
""65"",17,""C"",""Y"",""Q"",0.18609876357474
""66"",17,""C"",""Y"",""R"",0.26587590290484
""67"",5,""B"",""Y"",""P"",0.05356234036154
""68"",5,""B"",""Y"",""Q"",0.04731210983269
""69"",5,""B"",""Y"",""R"",0.04877515848359
""70"",6,""B"",""Y"",""P"",0.06240572241178
""71"",6,""B"",""Y"",""Q"",0.13301297541279
""72"",6,""B"",""Y"",""R"",0.17973855854636
""73"",7,""B"",""Y"",""P"",0.06463245380331
""74"",7,""B"",""Y"",""Q"",0.10244742460486
""75"",7,""B"",""Y"",""R"",0.0599854720435
""76"",8,""B"",""Y"",""P"",0.05824947941558
""77"",8,""B"",""Y"",""Q"",0.11926213239492
""78"",8,""B"",""Y"",""R"",0.04685947691071
""79"",9,""B"",""Y"",""P"",0.06752011460398
""80"",9,""B"",""Y"",""Q"",0.09542812038592
""81"",9,""B"",""Y"",""R"",0.08668150350578
""82"",10,""B"",""Y"",""P"",0.06486814661182
""83"",10,""B"",""Y"",""Q"",0.05854476138367
""84"",10,""B"",""Y"",""R"",0.04438863783229
""103"",1,""C"",""Z"",""P"",0.05133333746688
""104"",1,""C"",""Z"",""Q"",0.14821006659988
""105"",1,""C"",""Z"",""R"",0.08174176027544
""106"",2,""C"",""Z"",""P"",0.23884995419341
""107"",2,""C"",""Z"",""Q"",0.2099355433643
""108"",2,""C"",""Z"",""R"",0.13176723596276
""109"",3,""C"",""Z"",""P"",0.46479557484677
""110"",3,""C"",""Z"",""Q"",0.33304596595977
""111"",3,""C"",""Z"",""R"",0.29770388592371
""112"",4,""C"",""Z"",""P"",0.15308213537672
""113"",4,""C"",""Z"",""Q"",0.28081619128875
""114"",4,""C"",""Z"",""R"",0.24592983188039
""115"",17,""C"",""Z"",""P"",0.21312809357862
""116"",17,""C"",""Z"",""Q"",0.23336174725733
""117"",17,""C"",""Z"",""R"",0.22714195157817
""118"",5,""B"",""Z"",""P"",0.06818263568709
""119"",5,""B"",""Z"",""Q"",0.07257093444773
""120"",5,""B"",""Z"",""R"",0.08201262934886
""121"",6,""B"",""Z"",""P"",0.0644884733419
""122"",6,""B"",""Z"",""Q"",0.11937946452025
""123"",6,""B"",""Z"",""R"",0.07081608918845
""124"",7,""B"",""Z"",""P"",0.06720225949377
""125"",7,""B"",""Z"",""Q"",0.12509595330262
""126"",7,""B"",""Z"",""R"",0.06657357031905
""127"",8,""B"",""Z"",""P"",0.05878644062606
""128"",8,""B"",""Z"",""Q"",0.26638352132337
""129"",8,""B"",""Z"",""R"",0.06789933388591
""130"",9,""B"",""Z"",""P"",0.0908078338911
""131"",9,""B"",""Z"",""Q"",0.17670466924957
""132"",9,""B"",""Z"",""R"",0.10642489420997
""133"",10,""B"",""Z"",""P"",0.05107976253608
""134"",10,""B"",""Z"",""Q"",0.07242867177979
""135"",10,""B"",""Z"",""R"",0.05074329491013
</code></pre>

<p>My interpretation is that we have 99 observations for 3 categorical variables (2-3 categories each) in the regression analysis; Root MSE = 0.53177.</p>

<p>Is overfitting a valid concern here? If so, is there any way to address it?</p>

<p>A general answer would be helpful. Moreover, I'm trying to move to R (rather than Stata), so advice on replicating the analysis and/or addressing overfitting with R would be gratefully received.</p>

<p>ADDENDUM1:
I've partially worked out how to replicate analysis in R. Haven't yet replicated lnskew0, though <a href=""https://rpubs.com/chrisbrunsdon/skewness"" rel=""nofollow"">https://rpubs.com/chrisbrunsdon/skewness</a> looks similar.</p>

<pre><code>p2.df &lt;- read.table(""data_above.csv"", header=TRUE, sep="","")

library(foreign)
library(sandwich)
library(lmtest)
library(DAAG)

options(digits = 8)  # for more exact comparison with Stata's output

#create ln FibrosisP (need to replicate lnskew0 from Stata - manual k entered here from Stata calculation)

p2.df$FibrosisPln &lt;- log(p2.df$FibrosisP-0.0116473)

p1.df &lt;- p2.df[c(""PatientID"", ""DiseaseG"", ""WallG"", ""RegionG"", ""FibrosisPln"")]

p1.df$DWR &lt;- paste(p1.df$DiseaseG, p1.df$WallG, p1.df$RegionG)

p.df &lt;- pdata.frame(p1.df, index = c(""PatientID"", ""DWR""), drop.index = F, row.names = T)

# tools_reg.R from http://www.existencia.org/pro/?p=134
source(""tools_reg.R"")
mod &lt;- lm(FibrosisPln~factor(DiseaseG)+factor(WallG)+factor(RegionG),data=p.df)
get.coef.clust(mod, p.df$PatientID) # identical to Stata output
</code></pre>

<p>ADDENDUM2:
I have performed 10-fold cross-validation with CVlm from R package DAAG. However, I am uncertain how to interpret the results. Does the presence of overlapping lines in the plot for all 10 folds suggest no overfitting is present?</p>

<pre><code>library(foreign)
library(sandwich)
library(lmtest)
library(DAAG)
CVlm(df=p.df, form.lm=mod, m=10, plotit = c(""Observed"",""Residual""), main=""Small symbols show cross-validation predicted values"", legend.pos=""topleft"", printit=TRUE)
</code></pre>

<p><img src=""http://i.stack.imgur.com/HqTzi.png"" alt=""CVlm plot""></p>

<pre><code>t test of coefficients:

                  Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)        -3.2703     0.1441  -22.70  &lt; 2e-16 ***
factor(DiseaseG)C   0.9241     0.2084    4.43  2.5e-05 ***
factor(WallG)Y      0.3286     0.1681    1.95   0.0536 .  
factor(WallG)Z      0.4356     0.1326    3.28   0.0014 ** 
factor(RegionG)Q    0.4401     0.1421    3.10   0.0026 ** 
factor(RegionG)R    0.1496     0.0859    1.74   0.0847 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>ADDENDUM3:
After a lot of searching, the only explanation I could find online (which I presume is accurate) is given at <a href=""http://rstatistics.net/regression-modelling/"" rel=""nofollow"">http://rstatistics.net/regression-modelling/</a></p>

<blockquote>
  <p>""The fitted lines of different colors are parallel and on-top of each other. Indicating a stable model direction and less influence of outliers.""</p>
</blockquote>
"
"0.0395593886064618","0.040291148201269","159489","<p>When using propensity scores for inverse probability weighting (IPW) the standard errors for the parameters in the regression model may be affected. I have seen several examples of people using different types of standard errors (classical, robust, bootstrap) and am unsure which ones are correct to use and why. Classical weighting would use weights to indicate the precision of individual observation - this is not the case for IPW, where weighting indicates the importance of observations (but not their precision).  </p>

<p>If you want to add references to R packages, that would be appreciated, but I am primarily interested in the methods and why they should or should not be used.</p>
"
"0.139967124009992","0.148039131365948","159745","<p>I want to test a regression model with neuroticism as focal predictor, agreeableness as moderator and RT variability as dependent measure (covariates: attentional control and mean RT). Previously, I have used the modprobe macro in SPSS by Andrew Hayes for this (see: <a href=""http://link.springer.com/article/10.3758%2FBRM.41.3.924"" rel=""nofollow"">http://link.springer.com/article/10.3758%2FBRM.41.3.924</a>). I am in the process of transitioning to R, however, and would like to learn how to run a similar routine there. I have set up my regression model as follows:</p>

<pre><code>m3&lt;-lm(data=stp2_sub2, all_SD~Neuroticism*Agreeableness+Attentional.Control+all_RT, na.action=na.omit) # full interaction model
m33&lt;-lm(data=stp2_sub2, all_SD~Neuroticism+Agreeableness+Attentional.Control+all_RT, na.action=na.omit) # reduced model
</code></pre>

<p>I know that I can obtain F-change and p-change, using:</p>

<pre><code>anova(m3, m33) # provides F-change and p-change
</code></pre>

<p>What I still donâ€™t know yet is how to obtain the R squared change value, which gives me the effect size of the interaction effect. I have already posted a similar question at one of the sister websites (<a href=""http://stats.stackexchange.com/questions/159299/computing-r-squared-change-f-and-p-values-for-the-interaction-moderation-te/159304?noredirect=1#"">http://stats.stackexchange.com/questions/159299/computing-r-squared-change-f-and-p-values-for-the-interaction-moderation-te/159304?noredirect=1#</a>), although it seems that my question was slightly off-topic there. The users there have been very helpful (especially @gung), but I still have some remaining questions. Hence me posting here.</p>

<p>Basically, the recommendations have been so far to compute (a) semi-partial r-squared or (b) partial eta squared. For (a) semi-partial r-squared a custom-written function already exists (see: <a href=""http://stats.stackexchange.com/questions/71816/calculating-effect-size-for-variables-in-a-multiple-regression-in-r/136615#136615"">http://stats.stackexchange.com/questions/71816/calculating-effect-size-for-variables-in-a-multiple-regression-in-r/136615#136615</a>). Unfortunately I am a bit at a loss as to how to exactly adapt it to my own case, particularly the following bit: y = 4 + .5*x1 - .3*x2 + rnorm(10, mean=0, sd=1). Moreover, even if I did succeed at adapting y to my own needs, I would still need to know how to compute the r change value. The function for semi-partial r-squared yields a single r value (i.e. it doesnâ€™t provide a change value, yet). In my case, would I need to, in a first step, run this function for (1) the full model (here: m3) as well as for (2) the reduced model (here: m33), thereby providing me with two semi-partial r-squared values (full vs. reduced)? In a second step, would I then subtract semi-partial r-squared (reduced) from semi-partial r-squared (full), with the outcome being the r-change value that I need?</p>

<p>As for b, I have been told to use the following Â« formula Â» ( SSE(reduced)-SSE(full) ) / SSE(reduced) to compute partial eta squared for the interaction effect. I have found code for computing the sum of squared errors (see: <a href=""http://stats.stackexchange.com/questions/49924/how-does-anova-lm-in-r-calculates-sum-sq"">http://stats.stackexchange.com/questions/49924/how-does-anova-lm-in-r-calculates-sum-sq</a>) and have applied this to my case as follows:</p>

<pre><code>SSEfull&lt;-sum(m3$residuals^2) # sum of squared errors/residual sum of squares for the full regression model
SSEred&lt;-sum(m33$residuals^2) # sum of squared errors/residual sum of squares for the reduced regression model

pes&lt;-(SSEred-SSEfull)/SSEred # computing partial eta-squared
</code></pre>

<p>I would like to know whether I have correctly implemented the code to compute partial eta squared for the interaction effect. Moreover, given that I need the r squared change value, I was wondering how I might be able to convert the partial eta squared value to the r squared change value that I need. That said, as mentioned above, I just need to know how to compute the r squared change value for my interaction model. Ultimately, I donâ€™t mind whether I do this following suggestions (a) or (b) (but note that I still have some questions in this regard) or use a completely different way. Indeed, perhaps thereâ€™s already a function/package in R that calculates the r change value (I havenâ€™t found one yet)? I feel like being very close to finding what I need, but just need a final few pointers. Apologies for the long-winded way of writing this simple question, but I thought it could be of use to show what thought (even if not yet conclusive) has already gone into this. Any guidance on how to best go about this would be much appreciated.</p>
"
"0.0559454238864459","0.0427352161723642","160096","<p>I am going through the LAB section Â§6.6 on Ridge Regression/Lasso in the book <a href=""http://www-bcf.usc.edu/~gareth/ISL/index.html"" rel=""nofollow"">'An Introduction to Statistical Learning with Applications in R'</a> by James, Witten, Hastie, Tibshirani (2013).</p>

<p>More specifically, I am trying to do apply the scikit-learn <code>Ridge</code> model to the 'Hitters' dataset from the R package 'ISLR'. I have created the same set of features as shown in the R code. However, I cannot get close to the results from the <code>glmnet()</code> model. I have selected one L2 tuning parameter to compare. ('alpha' argument in scikit-learn).</p>

<p><strong>Python:</strong><BR></p>

<pre><code>regr = Ridge(alpha=11498)
regr.fit(X, y)
</code></pre>

<p><a href=""http://nbviewer.ipython.org/github/JWarmenhoven/ISL-python/blob/master/Notebooks/Chapter%206.ipynb"" rel=""nofollow"">http://nbviewer.ipython.org/github/JWarmenhoven/ISL-python/blob/master/Notebooks/Chapter%206.ipynb</a></p>

<p><strong>R:</strong></p>

<p>Note that the argument <code>alpha=0</code> in <code>glmnet()</code> means that a L2 penalty should be applied (Ridge regression). The documentation warns not to enter a single value for <code>lambda</code>, but the result is the same as in ISL, where a vector is used.</p>

<pre><code>ridge.mod &lt;- glmnet(x,y,alpha=0,lambda=11498)
</code></pre>

<p>What causes the differences?</p>

<p><B>Edit:</B><BR>
When using <code>penalized()</code> from the penalized package in R, the coefficients are the same as with scikit-learn.</p>

<pre><code>ridge.mod2 &lt;- penalized(y,x,lambda2=11498)
</code></pre>

<p>Maybe the question could then also be: 'What is the difference between <code>glmnet()</code> and <code>penalized()</code> when doing Ridge regression?</p>

<p><B>New python wrapper for actual Fortran code used in <em>R</em> package glmnet</B><BR>
<a href=""https://github.com/civisanalytics/python-glmnet"" rel=""nofollow"">https://github.com/civisanalytics/python-glmnet</a></p>
"
"NaN","NaN","160331","<p>I'd like to use negative binomial regression with caret. However in the <a href=""http://rpackages.ianhowson.com/rforge/caret/man/models.html"" rel=""nofollow"">list of supported models</a> I can't find it.</p>

<p>I tried to use:</p>

<pre><code>train(data=dataset,trControl=trainControl(method = ""none""), method=""glm.nb"", family=binomial()) 
</code></pre>

<p>But this gives me an error too.</p>

<p>So how can I used negative binomial regression with caret?</p>
"
"0.027972711943223","0","160495","<p>I working with R on a classification problem. My outcome variable is binary with two levels 1 and 2. 
First of all I tried the logistic regression, which of all methods has the best performance, altough still poor. </p>

<p>I tried nnet package, random forest, the fuzzy package frbs and decision trees. </p>

<p>The nnet function gives me only one class - in this case 2.</p>

<p>I had some hope with frbs package. See my code below:</p>

<pre><code>obj &lt;- frbs.learn(train,method.type=""FRBCS.CHI"",control=list(num.labels=3,type.mf=""GAUSSIAN""))
summary(obj)
#test set without def 
pred&lt;-predict(obj,newdata=test[,1:8])
</code></pre>

<p>But the predictions are wrong, the class 1 is completely missclassified</p>

<pre><code>#percentage error
tdef&lt;-test$def
err = 100*sum(pred!=tdef)/ nrow(pred)
print(err)
[1] 16.93038
</code></pre>

<p>I'm wondering what I could improve to classify the output variable. Is something wrong with my data? 
Are the parameters not right? </p>

<p>Can someone please verifiy?  I'm at the end of my knowledge...</p>

<p>You can find the (normalized) data here:
<a href=""https://drive.google.com/open?id=1xrCXTLqKvGiGeo2X0Y1DvoSKvzbYFnyccLimceDIbZg"" rel=""nofollow"">https://drive.google.com/open?id=1xrCXTLqKvGiGeo2X0Y1DvoSKvzbYFnyccLimceDIbZg</a></p>
"
"0.108550066801774","0.103648123654272","160638","<h1>General question</h1>

<p>When I perform a logistic regression using lrm and specify weights for the observations, I get the following warning message:</p>

<blockquote>
  <p>Warning message:
  In lrm(Tag ~ DLL, weights = W, data = tagdata, x = TRUE, y = TRUE) :
    currently weights are ignored in model validation and bootstrapping lrm fits</p>
</blockquote>

<p>My interpretation is that everything that the rms package will tell me regarding goodness-of-fit, notably using the residuals.lrm tool, is wrong. Is this correct?</p>

<h1>Specific example</h1>

<p>To be more specific, I have working example. All the code and output can be found in this <a href=""https://github.com/jwimberley/crossvalidated-posts/tree/master/lrm_gof"" rel=""nofollow"">GitHub repository</a>. I have two CSV tables of data, <a href=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/toystudy.csv"" rel=""nofollow"">toystudy.csv</a> and <a href=""https://github.com/jwimberley/crossvalidated-posts/raw/master/lrm_gof/realstudy.csv"" rel=""nofollow"">realstudy.csv</a>. There are three columns in each:</p>

<ol>
<li>The binomial response $y$ (0 or 1) [called Tag in code]</li>
<li>The predictor $x$ [called DLL in code]</li>
<li>The weight for the observation [called W in code]</li>
</ol>

<p>The former is simulated data, where all the weights are unity and where a logistic regression $log(\pi) = \theta_0 + \theta_1 x$ should fit the data perfectly. The latter is real data from my analysis, where the validity of this simple model is in question. The real data has weighted observations. (Some of the weights are negative, but there is a well-defined reason for this). The analysis code in contained completely in <a href=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/regressionTest.R"" rel=""nofollow"">regressionTest.R</a>; the meat of the code is</p>

<pre><code>library(rms)
fit &lt;- lrm(Tag ~ DLL, weights = W, data = tagdata, x=TRUE, y=TRUE)
residuals(fit,""gof"")
</code></pre>

<p>Here are the results for the two tables of data.</p>

<h3>Case 1: Toy data</h3>

<p>The goodness-of-fit claimed by lrm (which is something called the le Cessie-van Houwelingen-Copas-Hosmer test, I understand?) is very good:</p>

<p><img src=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/toy/residuals.png"" alt=""enter image description here""></p>

<p>This is confirmed by grouping the data into 20 quantiles of the predictor and overlaying the predicted success rate over the average actual success rate:</p>

<p><img src=""http://i.stack.imgur.com/hOEFs.png"" alt=""enter image description here""></p>

<h3>Case 2: Real data</h3>

<p>In this case, the goodness-of-fit reported by lrm is horrendous:</p>

<p><img src=""https://raw.githubusercontent.com/jwimberley/crossvalidated-posts/master/lrm_gof/real/residuals.png"" alt=""enter image description here""></p>

<p>However, I don't think it should be that bad. Again grouping the data into quantiles, and taking into account the weights when computing the average values in each bin:</p>

<p><img src=""http://i.stack.imgur.com/mgzhc.png"" alt=""enter image description here""></p>

<p>Comparing the prediction to the observed values and their standard errors, I don't think this is that bad (the error bars here depend on how the standard error on a weighted mean is computed, so they might not be 100% right, but should at least be close). On the other hand, if I produce the same plot while ignoring the weights:</p>

<p><img src=""http://i.stack.imgur.com/dId9F.png"" alt=""enter image description here""></p>

<p>I can definitely imagine this fit being as poor as the goodness-of-fit test says.</p>

<h2>Conclusion</h2>

<p>So, is residuals.rm simply ignoring the weights when it calculates its goodness-of-fit statistic? And if so, is there any R package that will do this correctly?</p>
"
"0.027972711943223","0.0284901441149095","160665","<p>I am trying to run a cointegration test with a dummy variable using <code>ca.jo</code> function in <code>urca</code> package. </p>

<pre><code>johcoint=ca.jo(Ydata[10:60,1:5],type=""trace"",ecdet=c(""const""),K=2,spec=""transitory"",dumvar=dumvar)
</code></pre>

<p><code>dumvar</code> is the binary variable that takes 1 and 0 only. The first two observations are 1 and the rest are 0s as you can see in the sample given below. 
When I run the code, I get </p>

<pre><code>Error in solve.default(M11) : 
      Lapack routine dgesv: system is exactly singular: U[1,1] = 0
</code></pre>

<p>I think this is something to do with the invertability of the input matrix, and this occurs only when I include <code>dumvar</code> in the regression. The above error message disappears if I change the 3rd observation to '1' in <code>dumvar</code>.</p>

<p>Below is the sample data just for info:</p>

<pre><code>   A         B       C       D         E       dumvar
1  2.255446 1.688807 1.506579 1.880152 9.575868      1
2  2.230118 1.578281 1.546805 1.905426 9.545534      1
3  2.255446 1.688807 1.506579 1.880152 9.575868      0
4  2.230118 1.578281 1.546805 1.905426 9.545534      0
5  2.255446 1.688807 1.506579 1.880152 9.575868      0
6  2.230118 1.578281 1.546805 1.905426 9.545534      0
7  2.255446 1.688807 1.506579 1.880152 9.575868      0
8  2.230118 1.578281 1.546805 1.905426 9.545534      0
9  2.255446 1.688807 1.506579 1.880152 9.575868      0
10 2.230118 1.578281 1.546805 1.905426 9.545534      0
11 2.255446 1.688807 1.506579 1.880152 9.575868      0
12 2.230118 1.578281 1.546805 1.905426 9.545534      0
13 2.255446 1.688807 1.506579 1.880152 9.575868      0
14 2.230118 1.578281 1.546805 1.905426 9.545534      0
15 2.255446 1.688807 1.506579 1.880152 9.575868      0
16 2.230118 1.578281 1.546805 1.905426 9.545534      0
17 2.255446 1.688807 1.506579 1.880152 9.575868      0
18 2.230118 1.578281 1.546805 1.905426 9.545534      0
19 2.255446 1.688807 1.506579 1.880152 9.575868      0
20 2.230118 1.578281 1.546805 1.905426 9.545534      0
21 2.255446 1.688807 1.506579 1.880152 9.575868      0
22 2.230118 1.578281 1.546805 1.905426 9.545534      0
23 2.255446 1.688807 1.506579 1.880152 9.575868      0
24 2.230118 1.578281 1.546805 1.905426 9.545534      0
25 2.255446 1.688807 1.506579 1.880152 9.575868      0
26 2.230118 1.578281 1.546805 1.905426 9.545534      0
27 2.255446 1.688807 1.506579 1.880152 9.575868      0
28 2.230118 1.578281 1.546805 1.905426 9.545534      0
29 2.255446 1.688807 1.506579 1.880152 9.575868      0
30 2.230118 1.578281 1.546805 1.905426 9.545534      0
31 2.255446 1.688807 1.506579 1.880152 9.575868      0
32 2.230118 1.578281 1.546805 1.905426 9.545534      0
33 2.255446 1.688807 1.506579 1.880152 9.575868      0
34 2.230118 1.578281 1.546805 1.905426 9.545534      0
35 2.255446 1.688807 1.506579 1.880152 9.575868      0
36 2.230118 1.578281 1.546805 1.905426 9.545534      0
37 2.255446 1.688807 1.506579 1.880152 9.575868      0
38 2.230118 1.578281 1.546805 1.905426 9.545534      0
39 2.255446 1.688807 1.506579 1.880152 9.575868      0
40 2.230118 1.578281 1.546805 1.905426 9.545534      0
41 2.255446 1.688807 1.506579 1.880152 9.575868      0
42 2.230118 1.578281 1.546805 1.905426 9.545534      0
43 2.255446 1.688807 1.506579 1.880152 9.575868      0
44 2.230118 1.578281 1.546805 1.905426 9.545534      0
45 2.255446 1.688807 1.506579 1.880152 9.575868      0
46 2.230118 1.578281 1.546805 1.905426 9.545534      0
47 2.255446 1.688807 1.506579 1.880152 9.575868      0
48 2.230118 1.578281 1.546805 1.905426 9.545534      0
49 2.255446 1.688807 1.506579 1.880152 9.575868      0
50 2.230118 1.578281 1.546805 1.905426 9.545534      0
</code></pre>

<p>Thank you!</p>
"
"0.101115324337403","0.110341853688094","160675","<p>I'm interested in determining both the slope regression coefficient and plotting regression lines for autocorrelated time-series datasets of rainfall.  Specifically, I'd like to identify the best approach in R that would allow me to visualize the regression line on the original (undifferenced) time-series plot when I need to difference the data to remove stationarity (i.e, when d>0 in an arima model).</p>

<p>As a start, I'm exploring the use of auto.arima (from the forecast package) and sarima (from the astsa package) which can output regression coefficients in the presence of autocorrelation.</p>

<p>For example:</p>

<ol>
<li><p>Using auto.arima. The 'drift' of -5.009 represents the slope (see <a href=""http://robjhyndman.com/hyndsight/arima-trends/"" rel=""nofollow"">http://robjhyndman.com/hyndsight/arima-trends/</a>) </p>

<pre><code>&gt; min.ar &lt;- auto.arima(dec.yr.mmin$min_prcp)
&gt; summary(min.ar)

Series: dec.yr.mmin$min_prcp 
ARIMA(1,1,0) with drift         

Coefficients:
          ar1    drift
      -0.5138  -5.0089
s.e.   0.2465   5.7986

sigma^2 estimated as 949:  log likelihood=-57.82
AIC=121.64   AICc=124.31   BIC=123.34

Training set error measures:
                     ME     RMSE      MAE       MPE     MAPE      MASE       ACF1
Training set -0.9479987 28.52129 23.83494 -2.484233 16.12547 0.7957998 -0.2617352
</code></pre></li>
<li><p>Using sarima to fit the model and output the slope</p>

<pre><code>  &gt; fit.min &lt;- sarima(dec.yr.mmin$min_prcp, 1,1,0,                       reg=dec.yr.mmin$decade)
initial  value 3.542448 
  iter   2 value 3.488927
  iter   3 value 3.386967
  iter   4 value 3.383464
  iter   5 value 3.382408
  iter   6 value 3.382051
  iter   7 value 3.382024
  iter   8 value 3.382020
  iter   9 value 3.381925
  iter   9 value 3.381925
  iter   9 value 3.381925
  final  value 3.381925 
  converged
  initial  value 3.400729 
  iter   2 value 3.399523
  iter   3 value 3.399490
  iter   4 value 3.399488
  iter   4 value 3.399488
  iter   4 value 3.399488
  final  value 3.399488 
  converged
</code></pre></li>
</ol>

<p>3.Output coefficients</p>

<pre><code>      &gt; fit.min$fit$coef
             ar1       xreg 
      -0.5137696 -0.5009045 
</code></pre>

<ol start=""4"">
<li><p>For comparison, this is the output from an OLS regression which may give an incorrect slope due to autocorrelation.</p>

<pre><code>  &gt; m3 &lt;- lm(dec.yr.mmin$min_prcp ~ dec.yr.mmin$decade)
  &gt; summary(m3)

  Call:
  lm(formula = dec.yr.mmin$min_prcp ~ dec.yr.mmin$decade)

  Residuals:
      Min      1Q  Median      3Q     Max 
  -45.504  -8.048   1.892  13.650  38.357 

  Coefficients:
                      Estimate Std. Error t value Pr(&gt;|t|)   
  (Intercept)        1014.1570   319.9461   3.170  0.00807 **
  dec.yr.mmin$decade   -0.4222     0.1580  -2.672  0.02032 * 
  ---
  Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

  Residual standard error: 23.83 on 12 degrees of freedom
  Multiple R-squared:  0.3731,  Adjusted R-squared:  0.3209 
  F-statistic: 7.142 on 1 and 12 DF,  p-value: 0.02032
</code></pre>

<p>`</p></li>
</ol>

<p>The output from sarima identifies the slope coefficient and also intercept when d=0. When differencing is required (e.g., ARIMA (1,1,0) as output above), sarima only outputs the slope. </p>

<p>My question: when d = 1 or more, what approaches in R would allow me to add/visualize the regression line onto the original undifferenced time-series plot. Is it possible to derive the fitted values of the regression line, or derive intercept values from sarima/auto.arima or other package?</p>

<p>Many thanks in advance for your suggestions.</p>
"
"0.0484501583111509","0.0493463771219827","160696","<p>As a pet project, I have been learning some data analysis and machine learning skills (mainly text analytics) with the Analytics Edge course on edX. I decided to put some of my new skills at use analysing a dataset from UCI Machine Learning: <a href=""https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection"" rel=""nofollow"">https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection</a></p>

<p>I did some analysis already (can be seen at <a href=""https://github.com/Khaltar/Portfolio/blob/master/R/Machine%20Learning/SMS.R"" rel=""nofollow"">https://github.com/Khaltar/Portfolio/blob/master/R/Machine%20Learning/SMS.R</a>) and computed a LogRegression Model, a RF Model and a CART Model. The Random Forest Model seems to be getting the best results regarding AUC and accuracy but I'm still not happy with it.</p>

<p>A friend of mine suggested using a bagging approach joining the three models and using some kind of ""voting"" system to classify predictions and achieve better results but I am completely at a loss on how to do that.  My doubt is how to actually implement a bootstrapping model in R using RF to raise accuracy of the model. I tried using bagRboostR package (sample code in my sms.R file in github) but I can't figure out how to use it or if there is a simpler solution to implement it.</p>

<p>Thanks in advance</p>
"
"0.0740088392978143","0.0753778361444409","160721","<p>I am trying to replicate the <a href=""http://www.pnas.org/content/110/15/5802.full"" rel=""nofollow"">Kosinski, Stillwell, &amp; Graepel (2013) study</a> about predicting private traits and attributes from Facebook like data for study purposes. First I have admit, however, that I am quite a newbie to data science and building prediction models in R.</p>

<p>My Data:</p>

<ol>
<li>A sparse matrix (dim 237 x 43232) that contains 237 users and if they liked one of the 43232 fb-pages or not (indicated by 1 for liked and 0 for not-liked). The row-names contain user.ids.</li>
<li>A data.frame with columns containing the user.ids, age, gender, and several survey scores, for this example lets take the SOP2 score (optimism-pessimism score).</li>
</ol>

<p>My goal is to predict the SOP2 score with the user likes. Kosinski et al. describe their model building in <a href=""http://www.pnas.org/content/110/15/5802/F1.expansion.html"" rel=""nofollow"">this graphic</a>. So far I have done a SVD using the irlab R package:</p>

<pre><code>Comps.likes &lt;- irlba(Likes.matrix, nu = 100, nv = 100)
</code></pre>

<p>And this is where I am stuck .. how to go on from here?</p>

<p>From what I assume I should get to something like this:</p>

<pre><code>fit &lt;- lm(SOP2 ~ Comps.likes$d, data = someDataFrame)
</code></pre>

<p>or an equivalent using the caret package.</p>

<p>What I am trying to figure out:</p>

<ol>
<li>The step missing is how to get from the Comps.likes (SVD step) to
the regression formula with a coefficient for each Facebook like to
predict SOP2.</li>
<li>The step to actually predict SOP2 from a user vector
with the likes per user.id.</li>
</ol>

<p>Any help or hints to further resources?</p>
"
"0.0559454238864459","0.056980288229819","161121","<p>I have an R question. I'm wondering why there is a difference in p-values in the original regression analysis using lm versus in the k-fold cross-validation using the DAAG package.</p>

<p>So, first I run the regression.</p>

<pre><code>Model = lm(ExampleData$DependentVariable ~ ExampleData$IV1  + 
           ExampleData$IV2  + ExampleData$IV3  + ExampleData$IV4)
</code></pre>

<p>This gives me the p-values for the predictors.</p>

<pre><code>Coefficients:
                Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)     -55.6644    23.4690  -2.372  0.01958 * 
ExampleData$IV1   1.2118     0.6277   1.931  0.05631 .
ExampleData$IV2   6.2636     2.0563   3.046  0.00295 **
ExampleData$IV3   2.1531     0.7490   2.875  0.00492 **
ExampleData$IV4  -5.4468     1.8859  -2.888  0.00473 **
</code></pre>

<p>Then, I go to cross-validate the model using cv.lm in the DAAG package.</p>

<pre><code>cv.lm(df=ExampleData, Model_forCV, m=5)
</code></pre>

<p>This gives me the cross-validation results along with the p-values for the predictors.</p>

<pre><code>Response: DependentVariable
           Df Sum Sq Mean Sq F value  Pr(&gt;F)    
IV1         1  26755   26755    3.23 0.07541 .  
IV2         1 104332  104332   12.58 0.00059 ***
IV3         1  36119   36119    4.36 0.03938 *  
IV4         1  69167   69167    8.34 0.00473 ** 
Residuals 102 845806    8292   
</code></pre>

<p>Why are the p-values different?</p>

<p>Thank you! </p>
"
"NaN","NaN","161439","<p>I'm in the process of evaluating some behavioral data I've collected and cannot find a package to obtain relative factor importance for the following linear model:
<code>
cbind(accuracy, rt) ~ A + B + C + D
</code></p>

<p>In an earlier, parallel â€“ but univariate â€“ experiment I was able to use the <code>relaimpo</code> R package, which worked fine. But it is unable, I believe, to support multivariate analysis. I'm aware of Tonidandel &amp; LeBreton's <a href=""http://relativeimportance.davidson.edu/multivariateregression.html"" rel=""nofollow"">online tool/downloadable R code</a>, but was hoping their might be an alternative package-based solution. Am I in luck, or is their code the best bet out there right now?</p>
"
"0.0197796943032309","0.040291148201269","161614","<p>I want to solve the first exercice of the Multiple Regression Chapter of R. Hyndman's online book on Time Series Forecasting (see <a href=""https://www.otexts.org/fpp/5/8"" rel=""nofollow"">https://www.otexts.org/fpp/5/8</a>). I use <code>R</code> with <code>fpp</code> package as wanted in the exercise.</p>

<p>I am blocked in the following question:
c. Use R to fit a regression model to the logarithms of these sales data with a linear trend, seasonal dummies and a â€œsurfing festivalâ€ dummy variable.</p>

<p>Indeed, I don't know how to make the function <code>tslm</code> work with my dummy vector for the surfing festival. Here is my code.</p>

<pre><code>library(fpp)
log_fancy = log(fancy)
dummy_fest_mat = matrix(0, nrow=84, ncol=1)
for(h in 1:84)
    if(h%%12 == 3)   #this loop builds a vector of length 84 with
        dummy_fest_mat[h,1] = 1   #1 corresponding to each month March
dummy_fest_mat[3,1] = 0 #festival started one year later

dummy_fest = ts(dummy_fest_mat, freq = 12, start=c(1987,1))
fit = tslm(log_fancy ~ trend + season + dummy_fest)
</code></pre>

<p>When I do <code>summary(fit)</code>, I see that the regression coefficients have been well calculated, but when I continue with <code>forecast(fit)</code>
I get the following error : </p>

<pre><code>Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels) : 
  variables have not equal length (found for 'factor(dummy_fest)')
In addition: Warning message:
'newdata' had 50 rows but variables found have 84 rows 
</code></pre>

<p>But what is even stranger is that when I do <code>forecast(fit, h=84)</code>, it works!!
I don't know what is happening here, can someone explain me?</p>
"
"0.101115324337403","0.110341853688094","162426","<p>I am trying to create a logistic regression model and a random forest model on the same data to predict probability of default. For the logistic regression model, I have created some dummy variables from categorical variables. Finally, for the input of logistic regression, I have 9 dummy variables and 2 numeric variables (age and level, age takes values from 18 to 60, level from 4 to 10). I want to use same input dataset for the random forest model. When I did so, using ""randomForest"" Package, I get following Variable Importance Plot.</p>

<p><img src=""http://i.stack.imgur.com/qscyb.png"" alt=""enter image description here""></p>

<p>Level seems to be a very good variable both by MSE and Node Purity. Also, level is a very important variable in logistic regression (p value ~ 10^-5). 
However, Age is very important by Node purity, but not by MSE. Also, in logistic regression, age is not a very good variable with p value of 0.026. So I want to understand, Does being numeric increases the node purity importance of a variable by overfitting? Is it not suitable to use numeric and dummy variables together in random forest model? Or is there something I am missing.</p>

<p>I had similar doubts about using numeric and dummy variables in logistic regression, but in logistic regression it did not create any problem. Please help.</p>
"
"0.100857047225074","0.0948209311861521","162463","<p>I am doing some data analysis on a fairly large health data set of patients with diagnoses and the respective procedures received for each event. I was asked to run a multinomial logistic regression on my data.</p>

<p>The dataset has around 4,000 columns of attributes, of which around 3,000 are unique diagnoses. The diagnosis variables take on the value of 1 if the patient had that diagnosis and 0 if he or she did not.  The remaining approximately 1,000 variables pertain to unique procedures, which also take on the value of 1 if the patient has received it, and 0 if he or she did not.</p>

<p>The dataset contains information on approximately 30,000 patients. I, admittedly naively, ran a the multinom function in the multinom package in R on all 4,000 variables, with the dependent variable being the very last procedure the patient has received (marked as ""Final procedure"" in the matrix), but R isn't able to complete the computation. </p>

<p>I would like some overall advice in perhaps a different package I could use for running regressions on large data sets (cannot use bigmemory however because this is on windows) or even perhaps reformatting my data. </p>

<p>Initially, my data set had around 50 columns, because the maximum number of diagnoses and procedures a patient had was 25 diagnoses and 25 procedures, so each column was marked as ""Diagnosis X"" and ""Procedure x,"" with the corresponding element being the actual diagnosis/procedure identifier. For all the patients who did I have all 25 diagnoses/procedures (so most of them), the values in the data frame would just be NA. Now I am wondering if I could perhaps resort to using this data frame instead and have a nicer, smaller matrix to work with? The only real reason I reformatted my data set into the much larger matrix was because my grad student asked me to do so, but maybe this isn't the way to go.</p>
"
"0.0570990591522943","0.0697863157798853","162831","<p>I've been playing around with the package <code>strucchange</code> (and to some extent <code>segmented</code>) in R. I'm trying to determine whether there are changes in slope in a linear regression and more importantly, how many breakpoints. A toy dataset:</p>

<pre><code>x &lt;- c(0, 5, 10, 15, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80)
y &lt;- c(-84.16, -86.67, -87.74, -86.07, -89.15, -91.90, -93.64, -95.92,
  -95.96, -99.19, -100.73, -107.29, -106.10, -107.29)
</code></pre>

<p>First problem: if I use the breakpoints function:</p>

<pre><code>breakpoints(y ~ x, data = data.frame(x, y))
</code></pre>

<p>I get the following error:</p>

<pre><code>Error in breakpoints.formula(y ~ x, data = data.frame(x, y)) : 
minimum segment size must be greater than the number of regressors
</code></pre>

<p>I think this arises because the default h parameter in the breakpoints command is 0.15 and 14 (the number of observations I have) * 0.15 = 2.1 which, rounded down, is not greater than 2 (the ""number of regressors"": incidentally, I would have thought the number of regressors would be 1 given my formula but I've learned from other working examples of <code>y ~ x</code> that nreg = 2 in these cases. I guess the intercept counts as a regressor?). </p>

<p>If I set h to 3 or some fraction such that 14 * h >= 3, the command works.</p>

<pre><code>breakpoints(y ~ x, data = data.frame(x, y), h = 3)
</code></pre>

<p>Two breakpoints are returned. But the result is sensitive to h. Such that if I use:</p>

<pre><code>breakpoints(y ~ x, data = data.frame(x, y), h = 4)
</code></pre>

<p>I get a different solution. In the latter case, a single optimal break is found because the minimum number of observations before a break can be called is higher. Is there a way to somehow determine whether one solution has more support than the other? In other words, how best to optimize not the position of breakpoints, but the number of breakpoints (perhaps across values of h)? I think the Fstats command might be the key but I'm having a lot of trouble understanding the help for this command...</p>
"
"0.0559454238864459","0.056980288229819","162867","<p>I'm currently building zero-inflated Poisson &amp; negative binomial predictive models using the zeroinfl() function from the pscl package in R.</p>

<p>Incorporating penalized regressions into my model to account for shrinkage and variable selection is a priority. In addition I'd like to use penalization to avoid convergence issues due to perfect/quasi separation in my data (better than manually removing variables).</p>

<p><strong>Question</strong>: Realizing that zero-inflated models $\neq$ hurdle models, for purposes of variable selection will my models be <strong>seriously biased</strong> if I first run separate run lasso (or elastic net) Poisson and logistic regressions with glmnet to select variables for the zeroinfl()?</p>
"
"0.0839181358296689","0.0854704323447285","163705","<p>I've got a dataset which clearly shows a trend. However, I want to assess wether this trend is deterministic or stochastic. If I understood it right, I would need to use differences if the trend is stochastic and I could just detrend it if the trend is deterministic.</p>

<p>Hence, I used the <code>kpss.test()</code> from the <code>tseries</code> package in R with the null hypothesis that my data is trend-stationary which gave me a p-value of 0.01, i.e. I have evidence that it is not trend-stationary and the trend is stochastic, right?</p>

<p>On the other hand, I made a regression to identify the (possibly present) deterministic trend and made an additional <code>kpss.test()</code> on the residuals with the null of level-stationarity. This gave me a p-value of 0.1, i.e. I can't reject the null of my data being stationary after detrending, right?</p>

<p>Am I missing something or is there a possible further test I should use?</p>

<p>Here is my data:</p>

<pre><code>y &lt;- ts(c(12.9860268, 12.5362944, 10.9379455, 10.7029227, 9.6421311, 
  8.168712, 7.0846535, 6.7134053, 6.5685634, 5.6701865, 4.2352191, 
  4.3919294, 3.1960928, 2.8841746, 2.1974112, 0.5650275, -0.5647561, 
  -1.7419743, -2.9294583, -4.456346, -4.9608364, -5.3176373, -7.8000258, 
  -8.4957238, -10.1346795, -10.9322896, -11.491641, -12.1036813, 
  -13.022572, -14.5290742), start = 1982)
</code></pre>

<p>The regression and its fitted values and residuals are given by</p>

<pre><code>m &lt;- lm(y ~ time(y))
f &lt;- ts(fitted(m), start = 1982)
r &lt;- ts(residuals(m), start = 1982)
</code></pre>

<p>The KPSS test results are</p>

<pre><code>library(""tseries"")
kpss.test(y, null = ""Trend"")
##         KPSS Test for Trend Stationarity
## 
## data:  y
## KPSS Trend = 0.30727, Truncation lag parameter = 1, p-value = 0.01    
kpss.test(r, null = ""Level"")
##         KPSS Test for Level Stationarity
## 
## data:  r
## KPSS Level = 0.30727, Truncation lag parameter = 1, p-value = 0.1
</code></pre>

<p>And a plot of the data and the residuals from the regression:</p>

<pre><code>plot(y, type = ""o"", main = ""data"", xlab = ""years"", ylab = """")
lines(f, col = 2)
plot(r, type = ""o"", main = ""residuals"", xlab = ""years"", ylab = """")
</code></pre>

<p><a href=""http://i.stack.imgur.com/DvOKe.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/DvOKe.jpg"" alt=""enter image description here""></a></p>
"
"0.0625488854200668","0.0637058989297032","163819","<p>I am running a multinomial logistic regression model (with 3 possible outcomes) in R. I am trying to find the best way to assess the predictive power/accuracy of the model, and the best thing I've come up with is using a ROC curve.</p>

<p>For multi-class ROC analysis, I know that there is the one vs. one comparison or the one vs. all comparison. For the one vs. one comparison, would I need three separate ROC curves for each possible combination of outcome comparisons? If so, do I need to make a third model for comparing the two outcomes that were initially being compared to the baseline outcome?</p>

<p>For one vs. all comparison is the threshold for a r ""random model"" now 33% instead of 50%?</p>

<p>And finally, is there a better way to go about doing this/visualizing it?</p>

<p>EDIT: I know the pROC package has a multi class.roc function, but I don't totally get what it does.</p>
"
"0.027972711943223","0.0284901441149095","163986","<p>I effectively want to model the probability of a player winning his service point (a point in which he is the server) based on the values of explanatory variables (namely court surface and opponent world ranking)</p>

<p>Can this be done using a binary response logistic regression?</p>

<p>Consider the fact that I can view my response variable as number of successes out of a total number of trials (for which I have the data). Will it work considering I have both categorical and numerical explanatory variables?</p>

<p>Any feedback on why this will/won't work or how I can make it work would be hugely appreciated! I am doing the analysis in R, so pointers on functions or packages would also be welcome! </p>
"
"0.0484501583111509","0.0493463771219827","164973","<p>I am trying to fit a survival analysis in <code>R</code> with non-recurrent events and time-varying coefficients. The baseline distribution is exponential or Weibull and the frailty distribution is gamma distributed. I have roughly 900.000 rows. </p>

<p>So far I have tried the <code>parfm</code> and <code>frailtypack</code>. Though, neither has worked â€“ they just keep running and never return. The calls for <code>parfm</code> and <code>frailtypack</code> are similar to respectively: </p>

<pre><code>frailtyPenal(Surv(stop-start,event)~.-start-stop-event-year+cluster(temp$year), 
             data= regressionData, hazard=""Weibull"", RandDist=â€Gammaâ€)

parfm(Surv(stop-start,event)~.-start-stop-event-year,cluster=â€yearâ€, 
      regressionData, dist=""exponential"", frailty = ""gamma"")
</code></pre>

<p>Where <code>event</code> is zero-one coded. My guess so far is to use the <code>lme4</code> package with the function <code>glmer</code> where the family is Poisson, the respond are zero-one coded, the offset is the difference in time and random effect is an intercept for the year factor. I.e. something like:</p>

<pre><code>glmer(event ~.-start-stop-event-year+(1|year), family = Poisson(), offset=stop-start)
</code></pre>

<p>I know that this will yield Gaussian distributed random effects and not Gamma. Further, I am not sure that I get the model I want. My goal is to have exponential distributed conditional waiting times and hence I chose the Poisson distribution. Question is whether this is correct? Any suggestions on other packages that will do the job?</p>
"
"0.0395593886064618","0.040291148201269","165056","<p>My data is in a numeric matrix of RNA-seq data from Illumina 2000 platform (with proper alignment and other preprocessing done), where columns represent subjects, and rows represent raw expression counts of genes. My goal is to use the normalized matrix for further regression etc. analyses (with other tools than edgeR).
I wrote a function to do this:</p>

<pre><code>##getNormalized matrix
##input: numeric matrix
##output: numeric matrix with normalized counts
##requires edgeR package
getNormalizedMatrix &lt;- function(M){
  require(edgeR)
  norm.factors &lt;- calcNormFactors(M, method = ""TMM"")
  return(equalizeLibSizes(DGEList(ah, norm.factors = norm.factors))$pseudo.counts)
}
</code></pre>

<p>Is this the way I am supposed to do the TMM-normalization?</p>
"
"0.0625488854200668","0.0637058989297032","166485","<p>I created for the following data set a multiple regression. Now I would like to forecast the next 20 data points.</p>

<pre><code>&gt; dput(datSel)
structure(list(oenb_dependent = c(1.0227039, -5.0683144, 0.6657713, 
3.3161374, -2.1586704, -0.7833623, -0.2203209, 2.416144, -1.7625406, 
-0.1565037, -7.9803936, 9.4594715, -4.8104584, 8.4827107, -6.1895262, 
1.4288595, 1.4896459, -0.4198522, -5.1583964, 5.2502294, 1.0567102, 
-1.0923342, -1.5852298, 0.6061936, -0.3752335, 2.5008664, -1.3999729, 
2.2802166, -2.1468756, -1.4890328, -0.79254376, 3.21804705, -0.94407886, 
-0.27802316, -0.20753079, -1.12610048, 2.0883735, -0.7424854, 
0.44203729, -1.48905938, 1.39644424, -3.8917377, 11.25665848, 
-9.22884035, 3.26856762, -0.00179541, -2.39664325, 4.00455574, 
-5.60891295, 4.6556348, -4.40536951, 6.64234497, -7.34787319, 
7.56303006, -8.23083674, 4.43247855, 1.31090412), carReg = c(0.73435946, 
0.24001161, 16.90532537, -14.60281976, 6.47603166, -8.35815849, 
3.55576685, 7.10705794, -4.6955223, 10.9623709, 5.5801857, -6.4499936, 
-9.46196502, 9.36289122, -8.52630424, 5.45070994, -4.5346405, 
-2.26716538, 2.56870398, 0.013737, 5.7750101, -27.1060826, 1.08977179, 
4.94934712, 17.55391859, -13.91160577, 10.38981128, -11.81349246, 
-0.0831467, 2.79748237, 1.84865463, -1.98736934, -6.24191695, 
13.33602659, -3.86527871, 0.78720993, 4.73360651, -4.1674034, 
9.37426802, -5.90660464, -0.4915792, -5.84811629, 9.67648643, 
-6.96872719, -7.6535767, 0.24847595, 0.18685263, -2.28766949, 
1.1544631, -3.87636933, -2.4731545, 4.33876671, 1.08836339, 5.64525271, 
1.90743854, -3.94709355, -0.84611324), cpi = c(1.16, -3.26, 0.22, 
-3.51, 0.84, -2.81, -0.34, -4.57, -0.12, -3.95, -1.37, -2.73, 
0.35, -5.38, -4.43, -3.08, 0.74, -3.03, -1.09, -2, 0.35, -1.52, 
1.28, 0.2, -0.25, -4.55, -2.49, -4.24, -0.31, -2.96, -2.24, -0.46, 
-0.06, -2.67, -1.27, -1.4, -0.7, -0.96, -2.18, -2.53, -0.52, 
-1.74, -2.18, -1.4, -0.34, -0.09, -1.65, -1.15, -0.17, -2.01, 
-1.38, -1.24, 0.09, -2.44, -1.92, -2.61, -0.34), primConstTot = c(-0.33334, 
-0.93333, -0.16667, -0.33333, -0.16667, -0.86666, -0.3, -0.4, 
-0.26667, -1.56667, -0.73333, 0.1, -0.23333, -0.26667, -1.5774, 
-0.19284, 0.38568, -2.42423, -0.93663, 0.08265, -0.63361, 0.0551, 
-0.49587, 2.39668, -1.70798, -3.36085, -2.56196, 0.16529, 0, 
-1.84572, -1.3774, -0.49586, -1.70798, -1.90081, -0.55096, -0.77134, 
-0.16529, -0.30303, -0.17066, -0.23853, -0.64401, -1.52657, -1.57426, 
-0.28623, -0.54861, -1.07336, -0.71558, 0.02385, -0.38164, -1.09721, 
0, 0.14311, -0.38164, -1.02566, -0.42934, -0.35779, -0.4532), 
    resProp.Dwell = c(0.8, -4, -3.2, 2.7, -1.6, -1, -2.4, -0.4, 
    -0.8, 1, -12.1, 0.2, -5.2, 3.7, -2.7, -1.7, 1.5, 0.7, -7.9, 
    0.3, 0.3, 1.4, -3.3, -1, -1.6, 1.5, 0.5, 1.5, -1, -2.2, -3.5, 
    0.5, 0.5, -0.9, -0.4, -3.4, 0.9, 0.1, -0.2, -2.8, -0.8, -6.2, 
    11.3, -4.6, 1, 1.1, -1.7, 4.1, -5, 2.3, -2.3, 4.6, -6.3, 
    6.3, -6.9, 0, 2.4), cbre.office.primeYield = c(0, 0, 0.15, 
    0.15, 0.2, 0.2, 0.2, 0.25, 0.25, 0.25, 0.25, 0.2, 0.15, 0.1, 
    0.05, 0.15, 0.3, 0.35, 0.4, 0.3, 0.2, 0, -0.15, -0.85, -1, 
    -0.85, -0.75, -0.1, 0, 0, 0, 0.05, 0.05, 0.05, 0.05, 0, 0, 
    0, 0.2, 0.2, 0.2, 0.2, 0, 0, 0, 0, 0.25, 0.25, 0.25, 0.25, 
    0, 0, 0, 0, 0, 0, 0), cbre.retail.capitalValue = c(-1882.35294, 
    230.76923, -230.76923, -226.41509, -670.78117, -436.13707, 
    -222.22223, 0, -205.91233, -202.16847, 0, -393.5065, -403.91909, 
    -186.30647, -539.81107, -748.11463, -764.70588, -311.47541, 
    -301.42782, -627.09677, -480, 720, 782.6087, 645.96273, 251.42857, 
    1386.66667, -533.33334, -533.33333, -533.33333, 0, 0, -1024.56141, 
    -192.10526, 0, -730, 0, 0, 0, 0, 0, -834.28571, 0, -1450.93168, 
    0, 0, 0, -700.78261, 0, 0, 0, 0, 0, 0, 0, -1452, 0, 0)), .Names = c(""oenb_dependent"", 
""carReg"", ""cpi"", ""primConstTot"", ""resProp.Dwell"", ""cbre.office.primeYield"", 
""cbre.retail.capitalValue""), row.names = c(NA, -57L), class = ""data.frame"")
&gt; 
&gt; fit &lt;- lm(oenb_dependent ~ carReg + cpi + primConstTot + 
+             resProp.Dwell + cbre.office.primeYield + cbre.retail.capitalValue , data = datSel)
&gt; summary(fit) # show results

Call:
lm(formula = oenb_dependent ~ carReg + cpi + primConstTot + resProp.Dwell + 
    cbre.office.primeYield + cbre.retail.capitalValue, data = datSel)

Residuals:
   Min     1Q Median     3Q    Max 
-5.166 -1.447 -0.162  1.448  7.903 

Coefficients:
                          Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)               0.831630   0.492297    1.69    0.097 .  
carReg                    0.085208   0.039600    2.15    0.036 *  
cpi                      -0.349192   0.212044   -1.65    0.106    
primConstTot              0.752772   0.383810    1.96    0.055 .  
resProp.Dwell             0.994356   0.086812   11.45  1.4e-15 ***
cbre.office.primeYield    1.274734   1.212782    1.05    0.298    
cbre.retail.capitalValue  0.000528   0.000643    0.82    0.416    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 2.24 on 50 degrees of freedom
Multiple R-squared:  0.754, Adjusted R-squared:  0.725 
F-statistic: 25.6 on 6 and 50 DF,  p-value: 1.2e-13
</code></pre>

<p>I tried the following:</p>

<pre><code>vals.multipleRegr &lt;- forecast(fit, h = 20)
Error: could not find function ""forecast""
</code></pre>

<p>However, this does not work as the function forecast cannot be found. I am using the following packages in my code, <code>library(bootstrap)</code>, <code>library(DAAG)</code> and <code>library(relaimpo)</code>. </p>

<p>Any suggestion how to forecasting using multiple regression?</p>

<p>I appreciate your replies!</p>
"
"0.101115324337403","0.110341853688094","166584","<p>I am conducting a regression in order to predict a tennis player's service point win % i.e. the percentage of points he wins when he is the server.
Model 1 If my DV data lies in the range 0.3-0.9, does it make sense to use a logistic regression? If using logistic I would endeavor to build a model with serve win % as my DV and my IV's as:</p>

<p>+average serve win % of last n matches (maybe n=5 or 10) to account for form </p>

<p>+surface </p>

<p>+player ranking </p>

<p>+opposition ranking</p>

<p>..... Would this be a good model to use? Preliminary logistic regressions just involving serve win % regressed on surface + player ranking + opponent ranking ... are showing some strange results so im losing faith in logistic for this data.</p>

<p>An alternative I'm considering is to use raw variables in a linear regression type model with interactions.... Along the lines of Aiken &amp; West 1991
My dependent variable will be number of service points won in match, and my independent variables will be:</p>

<p>+no. service points played in match +the surface the match played on </p>

<p>+the player's ranking points +the opponents ranking points</p>

<p>+an interaction between player and opponent ranking points </p>

<p>+an interaction between surface and no. points played </p>

<p>+average service points won in last n matches</p>

<p>+average % of service points won in last m matches</p>

<p>Do either of these models stand out as smart or appropriate ways to model this data? For context, for each player I have between 100-350 matches worth of data. I would love to hear what you guys think, or if you have any other suggestions on how to predict serve win % using the stated variables I would really appreciate it. I'm conducting this analysis in R so any code/package suggestions would also be great </p>
"
"0.0843408998948762","0.0944911182523068","166779","<p>Iâ€™ve seen some papers that present the idea of training classifiers such as logistic regression that are really meant to optimize a custom cost model (such as by maximizing profit given expect revenues for predictions depending on whether they are false positives, true negatives, true positives, or true negatives) not by optimizing the typical log-loss function and then looking for the optimal decision cut-off threshold, but by using different loss functions that weight differently the costs of each classification type or of each misclassification type (although I've seen that different authors propose different functions), and these seem to provide better results when evaluating them based on the customly-defined cost function.</p>

<p>I was wondering if there are any implementations of such methods in R. Particularly, I'd like to try fitting a logistic regression treating the cost of misclassifying as false positive to be a multiple of the cost of misclassifying as false negative. I found a package that does just this for decision trees (although in that case it's based on the class proportions on the leaves rather than something like log-loss) and I see that there are some options for observation-specific weights in logistic regression, but not for error type weights.</p>
"
"0.0395593886064618","0.040291148201269","166884","<p>I am using RMS package of R to validate cox regression model with bootstrap. Please see the R code below. I am predicting 1, 2, 3, 4, 5 years survival. Is the optimism corrected discrimination index Dxy a single index for the model, regardless of which year I am predicting, or does it have a different value for predicting each year survival? When â€œiâ€ is specified to be 1, 2, 3, 4, 5 (years), the optimism corrected Dxy changes. Is this because of the change in the year? Or is it just due to the different random samples generated in each run? </p>

<p>f = cph(s1 ~ 
          GRADE_f_imp + 
          METAST_f_imp +
          largests_2b_imp +
          lymphnod_f_imp +
          LNPOS_f_imp +
          genderma_f_imp,
        data=d_OS, x=T, y=T, surv=T, time.inc=i*12)
validate (f, B=200, u=i*12)</p>
"
"0.0740088392978143","0.0753778361444409","166953","<p><strong>Issue</strong>: Cannot forecast sales accurately using quantile regression in R. I am using rq function from ""quantreg"" package which is giving me warning ""Result might have Non unique solutions""</p>

<p><strong>Aim</strong>: I am trying to forecast hourly sales of a store using quantile regression. </p>

<p>Below are the columns in my source table for forecasting.</p>

<ul>
<li><em>transaction_date</em> : sales date (input)</li>
<li><em>hr1 to hr24</em> : column with hourly sales info. (24 columns) (input)</li>
<li><em>totala</em> : total of 24 column hr1 to hr24 (not using currently)</li>
<li><em>location, department, sales_type</em>: forecasting will be done for each location, sales_type and department. (used to select data)</li>
<li><em>f1 to f24 :</em> columns I want to forecast for each hour (24 columns) (output)</li>
</ul>

<p>Packages Used: forecast, quantreg, Metrics</p>

<p><strong>Code</strong>: 
I have extracted date features from transaction_date eg. weekend, week of month and also holidays (1 if it is holiday 0 for regular days).</p>

<pre><code>attach(train_data) 
Y &lt;- cbind(hr) 
X &lt;- cbind(transation_date, Years, Months, Days, WeekDay, WeekofYear, Weekend, WeekofMonth, holidays) 

quantreg.all &lt;- rq(Y ~ X, tau = seq(0.05, 0.95, by = 0.05))
prediction_train &lt;- data.frame(predict(quantreg.all))
</code></pre>

<p>I have 19 models in prediction_train for each tau from 0.05 to 0.95, I select best model based on rmse value and than forecast using that tau.</p>

<pre><code>rmse(actual, predicted)
</code></pre>

<p>transaction_date is Date type, quantreg.all is rqs class and rest are numeric.</p>

<p><strong>Note:</strong> Stores are not open 24 hours, hence many hour columns will be 0 (time when store was close). Currently for most of such hours rq is predicting 0 or some negative values.</p>

<p>Weather  does not have major impact on sales.</p>
"
"0.0791187772129236","0.080582296402538","166986","<p>I have this simple example, where I try to get to the same factor loading as <code>nlme::gls</code></p>

<p>Prepare data</p>

<pre><code>#number of individuals
NN=2
#time
TT=100
set.seed(1234)
#predictors
x=matrix(rnorm(NN*TT,0,0.01),ncol=NN)
#correlated residuals
eps_ucor=matrix(rnorm(NN*TT,0,0.01),ncol=NN)
eps_cor=eps_ucor%*%chol(matrix(c(1,0.8,0.8,1),NN,NN))
#true factor loading
beta=1
#response
y=beta*x+eps_cor
#prepare data.frame; coerce by rows, i.e. y=c(y[1,1],y[1,2],y[2,1],y[2,2],...)
data=data.frame(t=sort(rep(1:TT,NN)),n=rep(1:NN,TT),y=c(t(y)),x=c(t(x)))
</code></pre>

<p>First, let's calculate the factor loading ""manually"" (I will follow <a href=""https://en.wikipedia.org/wiki/Generalized_least_squares"" rel=""nofollow"">https://en.wikipedia.org/wiki/Generalized_least_squares</a>)</p>

<pre><code>#covariance of residuals
cov_eps=cov(eps_cor)
#this is the matrix \Omega in the wikipedia article
#note that I assume that there is no correlation in time!
omega=diag(TT)%x%cov_eps
#calculate inverse
omega_inv=diag(TT)%x%solve(cov_eps)
</code></pre>

<p>We can now calculate the factor loading</p>

<pre><code>beta_gls_1=(t(data$x)%*%omega_inv%*%data$x)^(-1)*t(data$x)%*%omega_inv%*%data$y
beta_gls_1
#         [,1]
#[1,] 1.017732
</code></pre>

<p>Finally, let's calculate the same with <code>nlme::gls</code>
We have to prepare a vector with normalized variances</p>

<pre><code>var_vec=diag(cov_eps)
var_vec=var_vec/var_vec[1]
names(var_vec)=1:2
</code></pre>

<p>Now let's perform the regression</p>

<pre><code>res_gls=gls(model=y~x+0,
    data=data,
    correlation=corSymm(value=cov2cor(cov_eps)[2,1],form=~n|t,fixed=TRUE),
    weights=varIdent(value=numeric(0),form=~1|n,fixed=var_vec[-1]),method=""ML"")
</code></pre>

<p>The gls factor loading is</p>

<pre><code>res_gls$coef
#       x 
#1.015082
</code></pre>

<p>As you can see, there is a difference between <code>beta_gls_1</code> and <code>res_gls$coef</code>. I know, the difference is rather small - but from my experience it usually pays off to get to the bottom of such discrepancies.
My guess is that I have misunderstood how to correctly initialize the fixed correlation and variance structure in <code>gls</code>.
Any pointers to understand the difference highly appreciated!</p>

<p>BTW:</p>

<pre><code>R version 3.0.2 (2013-09-25)
Platform: x86_64-pc-linux-gnu (64-bit)

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C               LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8     LC_MONETARY=en_US.UTF-8   
 [6] LC_MESSAGES=en_US.UTF-8    LC_PAPER=en_US.UTF-8       LC_NAME=C                  LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] nlme_3.1-113

loaded via a namespace (and not attached):
[1] grid_3.0.2      lattice_0.20-24 tools_3.0.2 
</code></pre>
"
"0.0740088392978143","0.0753778361444409","167159","<p>I'm currently analyzing a dataset about quality of fodder. The data for e.g. crude protein content are given in %, I don't have absolute values. Because of the data structure (I don't give you the details here), I opted for the beta-regression from R's betareg package.
I want to know the influence of various factors (and their interactions) on the crude protein content. Some factors have two levels (e.g. fertilization: with/without), others have three or more (e.g. site: site1, site2, site3 or the harvest time of the year where my samples come from (cut: spring, summer, autumn).
I have specified a model that works quite well on the data I have, residuals look well-behaved, predictors and model parameters are good, etc.
Now I would love to have some more information than the summary(mymodel) gives. I would like to test not only if other factor levels are different from my reference level but if they are significantly different from each other. In my example that would be: On which of my sites is the crude protein level in the fodder larger than on others and are those differences all significant? 
I would think about doing a post-hoc test similar to e.g. the Tukey I could do if I had done a simple ANOVA. I googled, searched a lot online and asked people, but I can't find anything that works with betareg-objects.</p>

<p>So my question is: How do I do a post-hoc test for parameters of betareg-models in R? Or is this a bad idea? If so, why? Or is there no method yet? Please help me, thank you a lot!</p>

<p>I'm not a statistics/math pro, I learned almost everything I know by the modelling books by Alain Zuur and this site, so please be gentle.</p>

<p>My model (in the simplified version of my dataset) is specified like this:</p>

<pre><code>library(betareg)
prot&lt;-protein/100
mymodel&lt;-betareg(prot~ sward * Fert + site + cut + repetition | site +cut, data=mydata, link=""loglog"")

str(mydata)
 'data.frame':   848 obs. of  58 variables:
  $ protein     num  16.4 16.5 13.7 13.5 15 ...
  $ cut          : Factor w/ 3 levels ""autumn"",""spring"",..: 1 1 2 2 3 3 1 2 2 3 ...
  $ loc          : Factor w/ 3 levels ""G"",""O"",""S"": 1 1 1 1 1 1 1 1 1 1 ...
  $ repetition   : Factor w/ 4 levels ""I"",""II"",""III"",..: 1 1 1 1 1 1 1 1 1 1 ...
  $ Fert         : Factor w/ 2 levels ""fertilized"",""non-fertilized"": 1 1 1 1 1 1 1 1 1 1 ...
  $ sward        : Factor w/ 2 levels ""diverse"",""species-poor"": 1 1 1 1 1 1 1 1 1 1 ...
</code></pre>

<p>a reproducible but very simplified example code would be (note that my actual model is much larger!):</p>

<pre><code>site&lt;-c(""site1"",""site2"",""site3"",""site1"",""site2"",""site3"",""site1"",""site2"",""site3"",""site2"")
prot&lt;-c(0.1642038, 0.1650442, 0.1369376, 0.1350139, 0.1502178, 0.1515794, 0.1354457, 0.1301206, 0.1311298, 0.1308463)
fert&lt;-c(""with"",""without"",""with"",""without"",""with"",""without"",""with"",""without"",""with"",""without"")
cut&lt;-c(""autumn"",""autumn"",""spring"", ""spring"", ""summer"", ""summer"", ""autumn"", ""spring"", ""spring"", ""summer"")

mymodel&lt;-betareg(prot~ fert + site + cut | site, link=""loglog"")
</code></pre>
"
"0.0625488854200668","0.0637058989297032","167389","<p>I have a database with more than 500 samples with 22 quantitative features each and I would like to predict a categorical variable (0 or 1).
I am trying to fit a logistic regression model and a neural network in R using glm and the neural net package.</p>

<p>By selecting different features I noted that I get different results in terms of ability of my two models to predict the test set (another 500+ samples database) a +-10% in accuracy depending on how many and what features are used (using the same test set).
Essentially the models seems to get worse using more than 12 features on average but of course I cannot try all the combinations and I'd rather not use random features as a blind guess. Also the neural network seems a little bit picky on features and does not always converge (however this is a minor issue).</p>

<p>What tests could I run in R to select the most ""explicative"" features?</p>

<p>(By the way, is ""explicative"" the right terminology?)</p>
"
"0.0971882825368556","0.0989860468793906","168167","<p>My dependent variable is a probability. As such, values lie between 0 and 1. The most common values are 0, 0.5, and 1 each occurring in 20% to 30% of the observations but any value in between is possible and some do occur. </p>

<p><strong>Question 1: Which regression model is best to explain such data?</strong></p>

<ul>
<li><p>Ordinary least squares (OLS, function <code>lm</code> in Râ€™s <code>stats</code> package) is not suitable as it does neither account for the limited interval nor the accumulation at the margins.</p></li>
<li><p>Logit regression (function <code>glm</code> with parameter <code>family=""binomial""</code> in Râ€™s <code>stats</code> package) accounts for the accumulation at 0 and 1 but does not allow intermediate values.</p></li>
<li><p>Ordered logit regression (function <code>polr</code> in Râ€™s <code>MASS</code> package) could be applied when I divide the [0, 1] interval in subintervals. However, I lose the continuous nature of the dependent variable.</p></li>
<li><p>For probit and ordered probit regressions, the same applies as for logit and ordered logit.</p></li>
<li><p>Left- and right-censored tobit regression (function <code>tobit</code> with parameters <code>left=0</code> and <code>right=1</code> in Râ€™s <code>AER</code> package) might be appropriate. However, I found the following quote: â€œSome researchers have considered using censored normal regression techniques such as tobit ([R] tobit) on proportions data that contain zeros or ones. However, this is not an appropriate strategy, as the observed data in this case are not censored: values outside the [0, 1] interval are not feasible for proportions data.â€ (p. 302 in Baum (2008), <a href=""http://www.stata-journal.com/sjpdf.html?articlenum=st0147"" rel=""nofollow"">http://www.stata-journal.com/sjpdf.html?articlenum=st0147</a>). </p></li>
</ul>

<p>Below you find a code example </p>

<pre><code># Load libraries
library(stats, MASS, AER)
# Generate data
set.seed(123)
data &lt;- data.frame(x1 &lt;- runif(60, min = 0, max = 1), x2 &lt;- runif(60, min = 0, max = 1))
data$y  &lt;- -0.7 + data$x1 + 2 * data$x2 + rnorm(60, mean = 0, sd = 0.5)
    data$y  &lt;- ifelse(data$y &lt; 0, 0, data$y)
data$y  &lt;- ifelse(data$y &gt; 0.4 &amp; data$y &lt; 0.6, 0.5, data$y)
data$y  &lt;- ifelse(data$y &gt; 1, 1, data$y)
    data$yCat &lt;- data$y
    data$yCat &lt;- ifelse(data$yCat &gt; 0 &amp; data$yCat &lt; 0.5, 0.25, data$yCat)
    data$yCat &lt;- ifelse(data$yCat &gt; 0.5 &amp; data$yCat &lt; 1, 0.75, data$yCat)
    data$yCat &lt;- as.factor(data$yCat)
    hist(data$y, breaks=101)
# Different regression models
summary(lm(y ~ x1 + x2, data=data)) # OLS
summary(glm(y ~ x1 + x2, data=data, family=""binomial"")) # Logit
summary(polr(yCat ~ x1 + x2, data=data)) # Ordered logit
summary(tobit(y ~ x1 + x2, data=data, left=0, right=1)) # Tobit
</code></pre>

<p>To make matters worse, my data is panel data. I know how to handle individual, time, and mixed effects and random and fixed effects models using plm from Râ€™s plm package and F-test, LM-test, and Hausman test do decide which of these is best. </p>

<p><strong>Question 2: For the dependent variable described above, which panel regression model is best?</strong> </p>

<p>Below your find a code example for the data structure. This extends the prior example.</p>

<pre><code># Load library
library(plm)
# Generate data (builds on prior example)
data$id &lt;- rep( paste( ""F"", 1:15, sep = ""_"" ), each = 4)
    data$time &lt;- rep( 1981:1984, 15 )
pData &lt;- pdata.frame(data, c( ""id"", ""time"" ))
# Panel regression example
summary(plm(y ~ x1 + x2, data=pData, model=""within"", effect=""twoways"")) # Based on OLS
</code></pre>
"
"0.0625488854200668","0.0637058989297032","168308","<p>I am currently performing a meta-analysis on bank relationship and firm performance with 27 different studies; almost all of them report different cases.</p>

<p>I calculated the partial effect size (I have different dependent and independent variable), its variance and confidence interval using the formula provided by Aloe &amp; Thompson, ""The synthesis of partial effect size"".</p>

<p>Due to the studies-specifics I have now to face two issues:
- Studies have different sample size and different specification, so heteroskedasticity is likely to arise;
- Studies present several cases, so some of the observation in my meta-analysis are not independent;</p>

<p>I thought to face the second issue by using what in econometrics would be called an ""study-fixed effect regression"" that, if I got correctly, in the metafor package should be:</p>

<pre><code>  res &lt;- rma(Yi, Vi, mods = ~ I(study))
  res
</code></pre>

<p>That is, a Mixed-Effects Model. I would also like to compare these results with the one of a multilevel\hierarchical model (Gelman &amp; Hill, 2006).</p>

<p>Is the <code>ram.mv</code> package good for that or I have to manually write the function following Gelman &amp; Hill? If it is good, how can I can calculate my Variance-Covariance Matrix considering the fact that I have different independent variable? Last, how can I account for heteroskedasticity?</p>

<p>Thank you for your time</p>
"
"0.0685188709827532","0.0697863157798853","168552","<p>I'm wondering if there is a good procedure people are using for gradient descent that is pretty well validated--something like a package for R or Python, or generic code many people adapt. After taking Andrew Ng's machine learning course in Coursera I was able to implement gradient descent in Octave, but I'm hoping to work with it in R or Python, with which I am more familiar. I'm also hoping for something more standardized than code I would write itself, to give it more credibility with my place of employment or other researchers.</p>

<p>Most of what I've found on this site is questions about writing code for gradient descent, like this post (<a href=""http://stats.stackexchange.com/questions/115425/multiplicative-gradient-descent"">Multiplicative gradient descent?</a>
) and this one (<a href=""http://stats.stackexchange.com/questions/142257/procedure-for-gradient-descent"">procedure for gradient descent</a>). And I've found examples of code from other sites, like this <a href=""http://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/"" rel=""nofollow"">http://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/</a>.</p>

<p>So I think there is no gradient descent package for R. But I was wondering if people had any thoughts for a good way to start implementing it in R or Python that doesn't rely on trust in the individual coder (e.g. me). Is there a blog post or tutorial with good generic code that can easily be used to standardize different gradient descent implementations?</p>
"
"0.0197796943032309","0.040291148201269","168714","<p>I would like to make a special kind of hybrid tree model in R, similar to the <code>mob</code> models in the <code>party</code> and <code>partykit</code> packages. But instead of learning splits from the data, I want to specify the splits in advance based on expert opinion, so that I obtain a more nicely structured and interpretable model. Can any existing tools do this in a slick way?</p>

<p>To fit the model, you partition data into $n$ sets according to the value of some factor $X$ with $n$ levels. You then fit some classical regression model, say Cox regression, to each subset. To predict on a new observation, you first determine which set your new observation belongs to, then feed it to the appropriate regression model.</p>

<p>I suppose I could rig this up with S3 without too much trouble, but is there a better way?</p>
"
"0.027972711943223","0","168717","<p>this is my first post.  I have an irregular time series that exhibits large shifts in both mean and in the direction of the trend.  It looks something like this (though this is far cleaner than reality):</p>

<pre><code>set.seed(1)
x = sort(rep(seq(as.POSIXct('2015/01/01'),as.POSIXct('2015/01/10'),by='day'),100) + runif(1000,0,80000))
y = c(seq(300),seq(90,70,-.2),seq(20,50,.1),seq(238.5,90,-.5)) + runif(1000,50,80)
plot(x,y)
</code></pre>

<p>The task is to:
 1. accurately partition/segment the data
 2. extract the change point indices
 3. fit regression separately for each segment</p>

<p>I have tried several routes, including:
a) hierarchical clustering based on dissimilarity matrix
b) function cpt.mean/var/meanvar from package changepoint (does not seem to work well)
c) function 'breakpoints' from package strucchange (slow and often inaccurate)
d) various types of kmeans (inappropriate, I know)</p>

<p>I have also explored some other packages, such as TSclust, urca, and DTW, but these seem better suited to clustering <em>sets</em> of time-series, not individual values within a time series.</p>

<p>Can someone point me in the correct direction? Perhaps I am not considering the appropriate data model?</p>

<p><strong>UPDATE</strong>
Thank you all for your very considered responses.  I went back to the strucchange package, and after some fiddling, have gotten it to work quite well.  I had not initially appreciated the h 'minimal segment size' argument.
Finished product:
<a href=""http://i.stack.imgur.com/qBgeY.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/qBgeY.jpg"" alt=""enter image description here""></a></p>
"
"0.0791187772129236","0.080582296402538","168765","<p>I have a formative construct in a structural equation model (SEM) which I would like to estimate with the function <code>sem</code> in the <code>lavaan</code> package in <code>R</code>. Currently, the model is underidentified. I know about four different approaches for identifying the model</p>

<ol>
<li>Adding two reflective indicators</li>
<li>Adding two reflective constructs</li>
<li>Adding one reflective indicator and one reflective construct</li>
<li>Fixing the variance of the disturbance term to zero</li>
</ol>

<p>These modelling approaches are described in section 5.3.3 of <a href=""http://www.researchgate.net/profile/Adamantios_Diamantopoulos/publication/222652088_Advancing_formative_measurement_models/links/09e4151406c288b6b3000000.pdf"" rel=""nofollow"">http://www.researchgate.net/profile/Adamantios_Diamantopoulos/publication/222652088_Advancing_formative_measurement_models/links/09e4151406c288b6b3000000.pdf</a></p>

<p>In my case, I do not have additional reflective indicators or constructs. Thus, I would like to fix the variance of the disturbance term to zero (approach 4). I know about the downsides of this approach as described on page 13 of the paper linked above. Nevertheless, if I want to do it, how do I specify this in the <code>lavaan</code> syntax?</p>

<p>Here is a code example. It returns a note that the model is underidentified. How do I get this working? </p>

<pre><code>library(lavaan)
model &lt;- ' 
    # latent variable definitions
    ind60 =~ x1 + x2 + x3
    dem60 &lt;~ 1*y1 + y2
    # regression
    dem60 ~ ind60
    # variance
    ind60 ~~ 1*ind60
    '
summary(fit &lt;- sem(model, data=PoliticalDemocracy))
</code></pre>

<p>From <a href=""http://stats.stackexchange.com/questions/154588/how-to-use-formative-indicators-in-covariance-based-sem-with-lavaan"">How to use formative indicators in covariance-based SEM with lavaan?</a> I know that it would work if I would invert the direction of causality (<code>ind60 ~ sem60</code> instead of <code>sem60 ~ ind60</code>) or specify <code>sem60</code> as reflective construct but neither of these appraoches would fit the theoretical basis.</p>
"
"0.0634361479695551","0.0753778361444409","168857","<p>I have a fairly large dataset of the following form, and I want to run a linear regression returning coefficients for each factor:</p>

<pre><code>Case    Variable1   Variable2   Result
1       Factor1     FactorA     50
2       Factor2     FactorA     60
3       Factor1     FactorB     55
4       Factor2     FactorB     65
...     ...         ...         ...
</code></pre>

<p>Running a linear regression using <code>lm()</code> on this would be very straightforward, but the size of the dataset seems to be too large.  I have about 1,000,000 cases, with about 10,000 factors in each variable.</p>

<p><code>lm()</code> (or other standard linear regression methods) translates this to an extremely wide matrix where each factor is a Boolean variable, correct?  So ~ 20,000 wide x 1,000,000 tall?  Running <code>lm()</code> on just a 25,000 case sample still takes several minutes and over a gb of memory.</p>

<p>My initial thought was to attack this regression problem using package <code>biglm</code>, but for it to behave properly, I believe <code>biglm</code> requires every factor to be present in every ""chunk"" of data it digests.  This would not occur in my data; some of the factors are only present a few times.  This is called <em>rank deficiency</em>, I believe. (However, an answer at <a href=""http://stackoverflow.com/questions/10502882/r-biglm-with-categorical-variables"">this StackOverflow question</a> indicates there might be a workaround?)</p>

<p>So my question: is there a better way to structure my data to run this regression?  Is there a better package or approach I should be using to run this analysis?</p>
"
"0.0559454238864459","0.056980288229819","168893","<p>I am performing a penalized B-spline regression on a simple time series of count data in R using the mgcv package. When I calculate a pointwise confidence band from the standard error of the fit based on the estimated degrees of freedom, it turns out to be slightly <em>wider</em> than the simultaneous confidence band produced by posterior simulation of the fitted GAM (as per:  <a href=""http://stats.stackexchange.com/questions/33327/confidence-interval-for-gam-model"">Confidence interval for GAM model</a>). As per Wood (2006), I'm using the Bayesian posterior covariance matrix from mgcv.</p>

<p>Some difference may be attributable to using the t distribution for calculating the pointwise band and assuming a multivariate normal for the posterior simulation, but I had expected the latter to reflect more uncertainty about the mean response due to the multiple comparisons issue. Am I correct in assuming that approximate equivalence of the simultaneous and pointwise confidence bands is a special case, and if so, are there specific conditions required to obtain this result?</p>

<p>Wood, S.N. (2006) Generalized Additive Models: An Introduction with R. Chapman and Hall/CRC.</p>
"
"0.0791187772129236","0.0705095093522208","169008","<p>I am conducting a retrospective study where I have a cohort of cases who underwent the same surgical procedure. The primary outcome of the study is the recurrence incidence rate during a follow up period of up to seven years. The risk of recurrence is known to be highest during the first year and then decrease over time.</p>

<p>I am investigating how a specific event during follow-up influence the risk of recurrence. I have identified 237 cases with the specific event (P) (group A), and matched this group 1:3 (based on other known risk factors) with cases without the specific event (group B).</p>

<p>Overall recurrence rate:</p>

<pre><code>Group A: 43/237 = 18.1%
Group B: 78/711 = 11.0%
</code></pre>

<p>Thus, P seems to effect the recurrence rate. However, in group A, 19 of the recurrences actually happened prior to P, and thus these 19 recurrences can't be contributed to the effect of P.</p>

<p>Therefore, I fitted an Extended Cox PH regression using the survival-package in R as follows:</p>

<pre><code>data &lt;- read.csv2(file=""Dataset.csv"", header=T, sep="";"", dec="","")
sdata &lt;- tmerge(data, data, id=1:nrow(data),death = event(ftime, Recurrence), P = tdc(Ptime))
ftime = total days of follow up, Recurrence = 0/1, P = the specific event (0/1), and Ptime = days from start to P (NA if P=0).

Call: coxph(formula = Surv(tstart, tstop, death) ~ P, data = sdata)

        coef exp(coef) se(coef)    z     p
P -0.552 0.576 0.23 -2.4 0.017

Likelihood ratio test=6.35 on 1 df, p=0.0118 n= 1165, number of events= 121
</code></pre>

<p>This model reports that those who have experienced P are less likely to have a recurrence. However, this must be due to P occurring after a median of ~ 1 year after start, and the model thus simply reports the reduced risk of a recurrence, if you have not failed until then.</p>

<p>Is it possible to fit a model that take this into account?</p>
"
"0.0395593886064618","0.040291148201269","169188","<p>During my analysis that I perform with a collaborator we get consistently different ouputs in cox regression between R and SPSS. Betas and p-values differ minutely, usually somewhere in the third or fourth decimal, though sometimes more. This occurs both for factors and continuous variables, so it does not appear to be a rounding issue in covariates. </p>

<p>In essence the entire database is prepared in SPSS, which includes calculation of follow-up. The file is then loaded into R via get.spps in HMISC. 
The follow-up does not consist of whole numbers (for days), could it be that one of the software packages rounds follow-up to whole numbers? Or are the methods calculation different? We use SPSS version 22, and 3.12 with coxph() from the survival package. </p>

<p>Did anyone encounter something similar? </p>
"
"0.0323001055407673","0.0493463771219827","169334","<p>I'm trying to use the <code>circular</code> package in R to perform regression of a circular response variable and linear predictor, and I do not understand the coefficient value I'm getting. I've spent considerable time searching in vain for an explanation that I can understand, so I'm hoping somebody here may be able to help.</p>

<p>Here's an example:</p>

<pre><code>library(circular)

# simulate data
x &lt;- 1:100
set.seed(123)
y &lt;- circular(seq(0, pi, pi/99) + rnorm(100, 0, .1))

# fit model
m &lt;- lm.circular(y, x, type=""c-l"", init=0)

&gt; coef(m)
[1] 0.02234385
</code></pre>

<p>I don't understand this coefficient of 0.02 -- I would expect the slope of the regression line to be very close to pi/100, as it is in garden variety linear regression:</p>

<pre><code>&gt; coef(lm(y~x))[2]
         x
0.03198437
</code></pre>

<p>Does the circular regression coefficient not represent the change in response angle per unit change in the predictor variable? Perhaps the coefficient needs to be transformed via some link function to be interpretable in radians? Or am I thinking about this all wrong? Thanks for any help you can offer.</p>
"
"0.0930988128231456","0.102722675451665","171193","<p>I recently employed multiple quantile regression in my area of research and found some interesting quantile differences across the distribution of Y, but I don't quite understand what they all really mean.  Unlike the traditional methods such as dividing the sample into multiple groups where I have access to the groups' data on various variables which then allows me to make sense of, for example, why the correlation between X and Y is 0 for group 1 and .7 for group 2, I feel like I have no idea where those quantile regression estimates come from, especially when there are more than 2 predictors in the QR model. Another way of putting this is I don't know which specific data points contribute heavily to a given quantile regression estimate and so this makes it very difficult for me to understand what the quantile differences really mean.  </p>

<p>Based on my understanding of QR, it uses all the data points in the full sample but weights the data points that are farther from a quantile of interest less heavily than the data points that are closer to that same quantile of interest, is this correct? If so, as a follow up, can I divide my full sample into 10 groups, e.g., 10th quantile, 20th quantile, 30th quantile group, and then examine how the 10 groups differ on various variables of interest in order to make sense of the 10 quantile regression estimates that I got? I know the subgroups approach is not ideal, which is why I used QR, but if you think this is a terrible idea, please let me know why. And if you know of any other methods  that allow me to have a more fine-grained understanding of my results, please help.  I conducted QR using the <code>quantreg</code> package in R.  </p>
"
"0.101115324337403","0.102985730108887","171325","<p>I am trying to fit a logistic regression model in R to classify a y variable as either 0 or 1. I have a dataset of around 2000 observations and decided to split it in half (training and testing).</p>

<p>After having decided which variables to include in my model, I subset the data and fitted the logistic regression as follows:</p>

<pre><code>clf &lt;- glm(y~.,data=df,family='binomial')
summary(clf)
</code></pre>

<p>Then, I tested the classifier on the testing set (1000 observations) and got 0.75 accuracy score.</p>

<pre><code>results &lt;- ifelse(predict(model,testdf,type='response') &gt; 0.5,1,0)
error &lt;- mean(r_results != results)
print(1-error) #prints out 0.74984
</code></pre>

<p>After this step, I decided to crossvalidate using the boot package</p>

<pre><code>library(boot)

# K-fold CV
error_cv = NULL

# Cost function for binary variable (as suggested by the R documentation)
cost &lt;- function(r, pi = 0) mean(abs(r-pi) &gt; 0.5)


for(i in 1:10)
{
    error_cv[i] &lt;- cv.glm(df,clf,cost,K=10)$delta[1]
}

error_cv
</code></pre>

<p>now, here is where I encounter a problem:</p>

<p>K-fold cross validation as I understand it, does the following (quote from Wikipedia):
""In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k âˆ’ 1 subsamples are used as training data. The cross-validation process is then repeated k times (the folds), with each of the k subsamples used exactly once as the validation data.""</p>

<p>However, how come that cv.glm() gets as argument my already fitted model? I don't understand what it is doing. Furthermore, if the data argument is equal to the training set, I get error rates of arount 0.2 whereas if I set data=testdf I get error rates of around 0.4. Since the two sets, df and testdf, have been splitted randomly, I cannot explain this large difference and I cannot explain why cv.glm() does not (apparently) do the fit and test process it is supposed to do.</p>

<p>What am I missing?</p>
"
"0.0484501583111509","0.0493463771219827","171448","<p>Doing ridge regression in R I have discovered</p>

<ul>
<li><p><code>linearRidge</code> in the <code>ridge</code> package - which fits a model, reports coefficients and p values but nothing to measure the overall goodness of fit</p></li>
<li><p><code>lm.ridge</code> in the <code>MASS</code> package - which reports coefficients and GCV but no p values for parameters</p></li>
</ul>

<p>How can I get all of these things (goodness of fit, coefficients and p values) from the same ridge regression?  I'm new to R so not familiar with facilities that may be available e.g. for computing $r^2$ from the data and fitted coefficients.</p>
"
"0.0625488854200668","0.0509647191437626","171473","<p>I'm currently working on analysing data from a simulation. The simulation result are statistical twins, which only vary in one point, the usage of an application to find a fueling station, that is cheap. In advance I have to say that my statistic skills are a little rusty.</p>

<p>I want to measure the effect of such an application on the fuel price. To do that me and my advisor at the university decided to use a linear regression. I came up with the following formula:
$$
Y_{â‚¬â„L}=Î²_0+Î²_1 X_{App}+Î²_2 X_{Oilprice} +Î²_3 X_{hour}+Î²_4 X_{day}+Î²_5 X_{fueltype}+ Î²_6 X_{highwaystation}+Î²_7 X_{brand}+ Z_{Home}+ Z_{Work}+ Z_{Station}+Ïµ
$$
The $Z_{work}$, as well as the others, represent a geometry within germany, that is identified by something call  RegionalschlÃ¼ssel, which is a unique number like 03255. The reason why I want to include this kind of information is, that this way I can observe the effects on the fuelprice that might be caused by local price differences. (I really hope I understood that right)</p>

<p>My current linear regression in R looks like
<code>lm1 &lt;- lm(price ~ app + oilprice + hour + day + fuel_type + brand + factor(start_rs) + factor(end_rs) + factor(station), data=queried_data)</code></p>

<p>The problem with this is, that my machine runs out of memory due to the very high amount of observations (9.759.911).</p>

<pre><code>Error: cannot allocate vector of size 85.5 Gb
In addition: Warning messages:
1: In model.matrix.default(mt, mf, contrasts) :
  Reached total allocation of 16005Mb: see help(memory.size)
2: In model.matrix.default(mt, mf, contrasts) :
  Reached total allocation of 16005Mb: see help(memory.size)
3: In model.matrix.default(mt, mf, contrasts) :
  Reached total allocation of 16005Mb: see help(memory.size)
4: In model.matrix.default(mt, mf, contrasts) :
  Reached total allocation of 16005Mb: see help(memory.size)
</code></pre>

<p>I have already done some reseach on other packages like</p>

<ul>
<li>lfe</li>
<li>lme4</li>
</ul>

<p>but as I said in the beginning my skillset is not high enough to comprehend them, nor is my english.</p>

<p>It would really help me if you could point me in the right directions.</p>
"
"NaN","NaN","171569","<p>I'm doing survival analysis with the ""riskregression"" package in R. 
What I want to do is to obtain the Fine&amp;Gray CIF with confidence intervals for my individual outcome(s) with death as a competing risk. 
Further investigated covariate influence on survival - similar to as one would in a cox regression model.  </p>

<p>I'm pretty new as far as R go, but I have some Ph.D. courses on it. 
My problem is mostly coding related.
Before it get any further into my problem is there anyone who can offer som help?
It would be much much appreciated!</p>

<p>Best</p>
"
"0.0791187772129236","0.0705095093522208","171745","<p>[Moved from stack overflow)
While I have done a good amount of reading regarding the usage of PSM, I am still struggling a bit to see if it can be used in my application. </p>

<p>I am trying to analyze the impact of an advertising campaign on in-store sales lift. While I know Propensity Scores are typically used in non-randomized studies, this study will be randomized (some stores will randomly see the advertising campaign, some will not). I have sales level data by store (such as pre-sales volume etc), and I want to conduct a sub-group analysis, to make sure I am comparing stores that behave similarly (start out at same level of sales volume, received roughly the same amount of spend, etc).<br>
Can anyone advise the best way to go about this?</p>

<p>I was initially thinking about using propensity scores to formulate the matching, but I know that seems counter-intuitive in this randomized setting.  However, in what way should I go about making a matched control group based on those variables? Would it still use the MatchIt package- but does that strictly use some variation of Propensity Scores? </p>

<p>In addition, after a matched set, would a simple multiple regression be the best means of finding the treatment effect? </p>

<p>Any thoughts/help greatly appreciated, thanks!</p>
"
"0.0484501583111509","0.0493463771219827","171763","<p>I am working on a paper about sexual coherence in women. Sexual coherence is defined as the relationship between subjective (SA) and genital sexual (GA) arousal. There is research that shows that this coherence can be higher or lower, depending on other factors, like age or arousability...</p>

<p>Both measures (SA and GA) have been measured continuously over a period of 5 minutes. I divided these 5-minutes into 15-second intervalls and calculated the mean for both arousal measures for each section.</p>

<p>Additionally, I have 2 questionnaire scores (P1, P2) that might influence SA, GA or (most importantly) the relationship between SA and GA</p>

<p>I use the package nlme in R and my data is transformed into long format.</p>

<p>My first question: Is it, in your opinion, possible to assess sexual coherence between GA and SA with a regression analysis, in which SA is the outcome and GA is the predictor?</p>

<p>My second question: If I want to investigate the impact of P1 and P2 on sexual concordance (the association between GA and SA), is it feasible to add the questionnaires to the above mentioned regression?
The model would look something like this: Coherence.model &lt;-nlme ( SA ~ GA + P1 + P2 + (GAP1) + (GAP2) + (P1*P2)) My idea is that you can assess the direct influence of GA, P1 and P2 on SA and (if the interaction terms (GA*P1) is significant) you can say that, e.g., P1 is a moderator of the relationship between GA and SA.</p>

<p>What do you think? Or do you have another idea, who to work with an ""coherence measure"" as outcome variable?</p>

<p>Best, Julia</p>

<p>Please excuse that I did not get into detail regarding syntax or programming. But I hope that this is not necessary at this moment.</p>
"
"0.0342594354913766","0.0465242105199235","171785","<p>I am running multiple linear regression with categorical variables and I need confidence interval 95% for standardized regression coefficient. I searched around and found 2 methods:</p>

<ol>
<li><p>Using the <code>QuantPsyc</code> package, with the function <code>lm.beta</code>. However, using <code>lm.beta</code> I can only get the standardized coefficients whereas I need with their 95% CI too. Is there a way?</p></li>
<li><p>To extract standardized regression coefficient, first standardize all the variables involved, and then run it in linear regression then you'll get estimates for standardized coefficients.</p></li>
</ol>

<p>So here is my model:</p>

<pre><code>model1 &lt;- lm(Life_Satisfaction ~ Subjective + Age + Sex + CountryCat11 + 
                                 CountryCat12 + CountryCat13 + CountryCat14 + 
                                 CountryCat15 + CountryCat16 + CountryCat17 + 
                                 CountryCat18 + CountryCat19 + CountryCat20 + 
                                 CountryCat23 + CountryCat25 + CountryCat28 + 
                                 CountryCat29 + CountryCat30 + Education_ISCED1 + 
                                 Education_ISCED2 + Education_ISCED3 + 
                                 Education_ISCED4 + Education_ISCED5 + 
                                 Education_ISCED6 + Education_stillinschool + 
                                 Education_None + Education_other, data=lifesat)

lm.beta (model1)
</code></pre>

<p>I ran that, but I cannot get the 95% CI.</p>

<p>So I tried the scale method:</p>

<pre><code>model2 &lt;- lm(scale(Life_Satisfaction) ~ scale(Subjective) + scale(Age) + 
                                        scale(Sex) + scale(CountryCat11) + 
                                        scale(CountryCat12) + scale(CountryCat13) + 
                                        scale(CountryCat14) + scale(CountryCat15) + 
                                        scale(CountryCat16) + scale(CountryCat17) + 
                                        scale(CountryCat18) + scale(CountryCat19) + 
                                        scale(CountryCat20) + scale(CountryCat23) + 
                                        scale(CountryCat25) + scale(CountryCat28) + 
                                        scale(CountryCat29) + scale(CountryCat30) + 
                                    scale(Education_ISCED1) + scale(Education_ISCED2) + 
                                    scale(Education_ISCED3) + scale(Education_ISCED4) + 
                                    scale(Education_ISCED5) + scale(Education_ISCED6) + 
                               scale(Education_stillinschool) + scale(Education_None) + 
                                        scale(Education_other), data=lifesat)

summary(model2)
</code></pre>

<p>I ran that, and I got the standardized regression and 95% CI but it was different from the standardized regression results I got from SPSS? Did I do it wrong?</p>
"
"0.027972711943223","0.0284901441149095","171914","<p>Is there any package provides tree structure manipulation? E.g. create a tree, get child / parent, add child / siblings /, traverse, etc.</p>

<p>I googled and find several packages such as 'tree', however they are for classification / regression</p>
"
"0.0559454238864459","0.056980288229819","172690","<p>I'm running some basic regressions which can be specified compactly as the <code>formula</code>: <code>y~treatment*dummy</code>.</p>

<p>Say there are several ($m$) treatments, $T_1,\ldots,T_m$ (with $T_1$ being the reference/control); the dummy is also multifaceted (categorical), taking $n$ values $D_1,\ldots,D_n$</p>

<p>Then (suppressing the observation index and error) the above formula specification basically returns the specified formula as</p>

<p>$$y=T^TBD$$</p>

<p>Where $T$ is the $m\times 1$ vector $[1, T_2, \ldots, T_m]$ of treatment indicators (excluding the reference treatment), $B$ is the $m\times n$ matrix of coefficients $\{\beta_{i,j}\}_{i=1,j=1}^{m\quad n}$, and $D$ is the $n\times 1$ vector $[1, D_2,\ldots, D_n]$ of dummy indicators (excluding the reference category).</p>

<p>This is all well and good, but the resulting coefficients in $B$ don't really have any clean interpretation, especially for my application. In particular, I'm looking for significant treatment effects--consider trying to answer the following: was Treatment 5 significantly better among individuals in category 3?</p>

<p>In the above specification, we'd be examining $\mathbb{E}[y|T_5,D_3]-\mathbb{E}[y|T_1,D_3]=\beta_{5,0}+\beta_{5,3}$, so we <em>could</em> add the coefficients we get out and use, e.g., a Wald test to determine significance.</p>

<p>However, consider the equivalent specification (I'm 100% sure someone besides has written it this way before since it took me all of 20 minutes to come up with):</p>

<p>$$y = \delta_0 + \sum_{j=2}^n\beta_j D_j + \sum_{i=2}^m \sum_{j=1}^n \gamma_{i,j}T_iD_j$$</p>

<p>Now the treatment effect is $\mathbb{E}[y|T_5,D_3]-\mathbb{E}[y|T_1,D_3]=\gamma_{5,3}$.</p>

<p>So this latter formulation has the convenient property that we can read our treatment effects right off our regression summary (especially including standard errors); its major drawback is that there's no way to supply this as an R <code>formula</code> parsimoniously, or at least I can't see a way to.</p>

<p>Does anyone have any experience with some secret <code>formula</code> or package to deal with this (I imagine exceedingly common) specification?</p>
"
"0.0938928011704452","0.102985730108887","173335","<p>I have two regression models (see output below). I want to put the outputs of model1 and model2 side by side, and compare them.</p>

<p>In Stata I would run </p>

<blockquote>
  <p>reg colgpa athlete
  outreg2  colgpa athlete using comparison.tex, append
  reg colgpa athlete sat
  outreg2  colgpa athlete sat using comparison.tex, append</p>
</blockquote>

<p>and then open <em>comparison.tex</em> How do I do it in R using knitr?</p>

<p>I am looking for a command that would output summary(model1) and summary(model2) in the same console.</p>

<hr>

<p>Some background information:</p>

<p>model1:</p>

<pre><code>&gt; summary(model1)

Call:
lm(formula = colgpa ~ athlete, data = gpa2)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.66603 -0.43603  0.00397  0.46397  1.61851 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  2.66603    0.01045 255.212  &lt; 2e-16 ***
athlete     -0.28453    0.04824  -5.898 3.97e-09 ***
---
Signif. codes:  
0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.656 on 4135 degrees of freedom
Multiple R-squared:  0.008343,  Adjusted R-squared:  0.008104 
F-statistic: 34.79 on 1 and 4135 DF,  p-value: 3.966e-09
</code></pre>

<p>model2:</p>

<pre><code>&gt; summary(model2)

Call:
lm(formula = colgpa ~ athlete + sat, data = gpa2)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.84611 -0.38276  0.03056  0.42472  1.76647 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  6.801e-01  7.134e-02   9.533   &lt;2e-16 ***
athlete     -5.061e-02  4.499e-02  -1.125    0.261    
sat          1.917e-03  6.823e-05  28.092   &lt;2e-16 ***
---
Signif. codes:  
0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.6012 on 4134 degrees of freedom
Multiple R-squared:  0.1673,    Adjusted R-squared:  0.1669 
F-statistic: 415.3 on 2 and 4134 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>I can of course type</p>

<blockquote>
  <p>summary(model1)</p>
  
  <p>summary(model2)</p>
</blockquote>

<p>And let the reader eyeball back and forth. But I want them side by side. This, I can do using <code>stargazer(model1, model2, title=""Regression Results"", align=TRUE)</code> but then I cannot connect it to knitr so that when I knit HTML it produces the output correctly. My goal is namely to go from the Rmd file to a published report directly.</p>

<p>So, to summarize, I am looking for a command that would output summary(model1) and summary(model2) in the same console. This would solve muy problem (although perhaps in an ugly way). If you know a more elegant solution -- perhaps to go from knitr to latex or another approach enterily, my ears are huge. </p>

<p><code>stargazer(model1, model2, title=""Regression Results"", align=TRUE)</code> outputs</p>

<pre><code>% Table created by stargazer v.5.2 by Marek Hlavac, Harvard University. E-mail: hlavac at fas.harvard.edu
% Date and time: Sun, Sep 20, 2015 - 14:56:02
% Requires LaTeX packages: dcolumn 
\begin{table}[!htbp] \centering 
      \caption{Regression Results} 
      \label{} 
    \begin{tabular}{@{\extracolsep{5pt}}lD{.}{.}{-3} D{.}{.}{-3} } 
    \\[-1.8ex]\hline 
    \hline \\[-1.8ex] 
     &amp; \multicolumn{2}{c}{\textit{Dependent variable:}} \\ 
    \cline{2-3} 
    \\[-1.8ex] &amp; \multicolumn{2}{c}{colgpa} \\ 
    \\[-1.8ex] &amp; \multicolumn{1}{c}{(1)} &amp; \multicolumn{1}{c}{(2)}\\ 
    \hline \\[-1.8ex] 
     athlete &amp; -0.285^{***} &amp; -0.051 \\ 
      &amp; (0.048) &amp; (0.045) \\ 
      &amp; &amp; \\ 
     sat &amp;  &amp; 0.002^{***} \\ 
      &amp;  &amp; (0.0001) \\ 
      &amp; &amp; \\ 
     Constant &amp; 2.666^{***} &amp; 0.680^{***} \\ 
      &amp; (0.010) &amp; (0.071) \\ 
      &amp; &amp; \\ 
    \hline \\[-1.8ex] 
    Observations &amp; \multicolumn{1}{c}{4,137} &amp; \multicolumn{1}{c}{4,137} \\ 
    R$^{2}$ &amp; \multicolumn{1}{c}{0.008} &amp; \multicolumn{1}{c}{0.167} \\ 
    Adjusted R$^{2}$ &amp; \multicolumn{1}{c}{0.008} &amp; \multicolumn{1}{c}{0.167} \\ 
    Residual Std. Error &amp; \multicolumn{1}{c}{0.656 (df = 4135)} &amp; \multicolumn{1}{c}{0.601 (df = 4134)} \\ 
    F Statistic &amp; \multicolumn{1}{c}{34.790$^{***}$ (df = 1; 4135)} &amp; \multicolumn{1}{c}{415.289$^{***}$ (df = 2; 4134)} \\ 
    \hline 
    \hline \\[-1.8ex] 
    \textit{Note:}  &amp; \multicolumn{2}{r}{$^{*}$p$&lt;$0.1; $^{**}$p$&lt;$0.05; $^{***}$p$&lt;$0.01} \\ 
    \end{tabular} 
    \end{table} 
</code></pre>

<p>ïœ€</p>
"
"0.0570990591522943","0.0697863157798853","173394","<p>I'm working on an analysis of a team-based dataset, where teams of 3 compete against each other under specific, identical tasks. The hierarchy therefore is person :: team :: task. Or, put another way, 6 people :: 2 teams :: 1 task. I am using <code>lme4</code> in R for linear regression (to predict a continuous variable).</p>

<p>For the hierarchy, I'm writing in the regression equation <code>(1|team)</code> and <code>(1|task)</code> for random effects (with a bunch of other fixed effect variables). However, in our dataset, each set of teams within any task is coded as ""left"" or ""right"" team (ie., 0 or 1, across each task). </p>

<p>When setting up the regression equation in <code>lme4</code>, do I need to recode team_id as a unique value taking into account task_id? E.g., I could concatenate to get unique team values, so task_1_team_0, task_1_team_1, task_2_team_0, task_2_team_1, etc.? Or will the <code>lme4</code> package take care of this for me, and I can keep the 2nd level values as 0 and 1?</p>

<p>I want to make sure that it is not putting half the dataset into one team and half into another team, because they are all coded the same, and then applying the task level on top of ""two"" teams.</p>
"
"0.0625488854200668","0.0637058989297032","173410","<p>I am working with a lasso regression with the glmnet package. I read these threads: <a href=""http://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia"">When conducting multiple regression, when should you center your predictor variables &amp; when should you standardize them?</a>, <a href=""http://stats.stackexchange.com/questions/19523/need-for-centering-and-standardizing-data-in-regression"">Need for centering and standardizing data in regression</a> and <a href=""http://stats.stackexchange.com/questions/86434/is-standardisation-before-lasso-really-necessary"">Is standardisation before Lasso really necessary?</a>.</p>

<p>Based on the responses I decided that I need to standardize my data before using it. I do have some questions however:</p>

<ul>
<li>Do I need to standardize the predictors and the responses or only the predictors?</li>
<li>I am using the function scale(myData, center = TRUE, scale = TRUE) for building the model, but I am wondering what do I do when I want to do predictions with a test data set. I think I should also standardize and center the test data, but how to I do that? Substracting the mean from the initial (training) dataset and the dividing it by the standard deviation of the initial dataset? </li>
<li>When I get a result do I need to ""backscale"" it (using the original mean and standard deviation) or do I already get the ""final"" result? </li>
</ul>
"
"0.0634361479695551","0.0538413115317435","173629","<p>When applying the ""urca"" package function <code>ur.df</code>, like </p>

<pre><code>summary(ur.df(data$col1, type = c(""none""), lags = 12, selectlags = c(""AIC"")))
</code></pre>

<p>I get following result:</p>

<pre><code>############################################### 
# Augmented Dickey-Fuller Test Unit Root Test # 
############################################### 

Test regression trend 


Call:
lm(formula = z.diff ~ z.lag.1 + 1 + tt + z.diff.lag)

Residuals:
      Min        1Q    Median        3Q       Max 
-12928366  -2888728   1284718   4218373   7179531 

Coefficients:
                 Estimate    Std. Error  t value  Pr(&gt;|t|)   
(Intercept)  5.391984e+07  1.638362e+07  3.29108 0.0043123 **
z.lag.1     -2.438154e+00  7.557134e-01 -3.22629 0.0049588 **
tt           6.579260e+05  2.730453e+05  2.40959 0.0275861 * 
z.diff.lag1  1.712004e+00  6.595980e-01  2.59553 0.0188537 * 
z.diff.lag2  1.402824e+00  6.379412e-01  2.19899 0.0420083 * 
z.diff.lag3  1.321555e+00  5.294537e-01  2.49607 0.0231329 * 
z.diff.lag4  1.099430e+00  4.720412e-01  2.32910 0.0324428 * 
z.diff.lag5  8.132753e-01  4.181477e-01  1.94495 0.0685140 . 
z.diff.lag6  1.797331e-01  3.654326e-01  0.49184 0.6291254   
z.diff.lag7  5.890640e-01  2.939590e-01  2.00390 0.0612825 . 
z.diff.lag8  3.919041e-01  2.794371e-01  1.40248 0.1787705   
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 6708593 on 17 degrees of freedom
Multiple R-squared:  0.7237276, Adjusted R-squared:  0.5613144 
F-statistic: 4.253547 on 10 and 17 DF,  p-value: 0.003348755


Value of test-statistic is: -3.2263 3.9622 5.2635 

Critical values for test statistics: 
      1pct  5pct 10pct
tau3 -4.15 -3.50 -3.18
phi2  7.02  5.13  4.31
phi3  9.31  6.73  5.61
</code></pre>

<p>Now the question:</p>

<ol>
<li>I do understand that ""-3.2263"" is the critical value (t-value)</li>
<li><strong>There is a unit root</strong> with trend since -3.2263 > -3.18 (tau3@10pct)
This means the time-series is <strong>non-stationary</strong> at a 10% significance level.</li>
<li>But, what is the meaning of ""p-value: 0.003348755""? Should I list this value in a table summarizing my unit root test results or rather mark the 0.1 significance level (*10%)?</li>
</ol>

<p>The <a href=""http://www.inside-r.org/packages/cran/urca/docs/ur.df"" rel=""nofollow"">documentation</a> says that critical values are based on Hamilton (1994) and Dickey and Fuller (1981)"". </p>
"
"0.0323001055407673","0.0493463771219827","173828","<p>I have a data set shown below, the 1st, 2nd,3rd column are dependent variable(dv), and 2 independent variables (iv1 &amp; iv2) respectively, I expected the regression coefficient of the ""iv1"" shows a positive value, as there is a positive correlation between dv and iv1. However, The result shows a negative regression coefficient for iv1 (beta_iv1 = -0.55), I am wondering why this happened, I appreciate if anyone can help.</p>

<p>dv    iv1     iv2</p>

<p>1     0.00    7.70<br>
1     2.90    0.00<br>
1     0.00    7.70<br>
1     2.90    0.50<br>
1     0.00    9.50<br>
1     1.50    7.70<br>
1     5.70    0.50<br>
1     7.10    2.30<br>
1     5.70    4.10<br>
1     0.00    4.10<br>
1     4.30    4.10<br>
1     0.00    10.00<br>
1     0.00    4.10<br>
1     2.90    0.50<br>
1     0.00    9.50<br>
0     0.00    9.50<br>
1     0.00    5.90<br>
1     0.00    4.10<br>
1     1.50    5.90<br>
1     5.70    2.30<br>
1     1.50    0.00<br>
0     0.00    10.00<br>
1     5.70    0.00<br>
1     5.70    0.50<br>
1     4.30    2.30<br>
0     0.00    10.00<br>
1     2.90    5.90<br>
1     0.00    5.90<br>
1     0.00    5.90<br>
1     2.90    2.30<br>
1     1.50    2.30<br>
1     2.90    0.50<br>
1     5.70    4.10<br>
1     1.50    0.00<br>
1     0.00    7.70  </p>

<p>I run this using R with package ""logistf"" which overcomes separation problem of logistic regression. The code I run this data set is as below:</p>

<blockquote>
  <p>library(logistf);<br>
      tempT=read.table(fileS);<br>
     fit&lt;-logistf(dv ~ iv1+iv2, data=tempT);</p>
</blockquote>

<p>and the result shows below:  </p>

<pre><code>           coef  se(coef) lower 0.95  upper 0.95     Chisq      p
</code></pre>

<blockquote>
  <p>(Intercept)  9.0086382 5.1741382   1.650380 61.61244068 7.6897111 
  0.005553652<br>
  tempT[, 2]  -0.5509122 0.6567110  -6.013208  1.55280975 0.5490404 0.458710039<br>
  tempT[, 3]  -0.9051062 0.5597601  -6.317335 -0.06328166 4.8315401 0.027943657</p>
</blockquote>

<p>Likelihood ratio test=7.213821 on 2 df, p=0.02713555, n=35</p>
"
"0.0843408998948762","0.0859010165930062","173996","<p>I'm using R (package lmer) to run linear mixed model My study looks at allergy levels of skin patches from patients and readings (repeated 5 times) are measured over 4 time points.</p>

<p>I need to determine if the allergy level for skin patch changes over time
(e.g., if allergy level from skin patch 1 for patient 1 at time 0 is different from allergy level for skin patch 1 for patient 1 at time 1 etc.) I do not want to see the difference between skin patch 1 and skin patch 2. Using package lmer:  </p>

<pre><code>model &lt;- lmer(allergy_level ~ time +(time|patient/patch))
</code></pre>

<p><strong>Results from this model indicate that time is not significant - the average patient allergy level for individual skin patches does not change over time</strong> (see below for output). However, <strong>I need to be able to tell if there is a significant difference for individual patches for individual patients over time</strong>.</p>

<p>If I run individual regression models for each skin patch for each patient, this will result in a large number of models as I have There are 16 skin patches per patient. (10 patients in total) 5 readings are taken at each of the 4 time points. I thought linear mixed models would be an appropriate method to answer my question (I need to be able to tell if there is a significant difference for individual patches for individual patients over time). </p>

<p>Output:</p>

<pre><code>Random effects:
 Groups   Name        Variance Std.Dev. Corr             
 ID:patch (Intercept) 17.4109  4.1726                    
          time1        2.7109  1.6465   -0.30            
          time2        3.0082  1.7344   -0.26  0.60      
          time3        5.7643  2.4009   -0.35  0.15  0.54
 patch    (Intercept) 19.1576  4.3769                    
          time1        0.2103  0.4586   -0.56            
          time2        0.4372  0.6612   -0.94  0.48      
          time3        0.5895  0.7678   -0.48  0.96  0.49
 Residual              4.9467  2.2241                    
Number of obs: 2956, groups:  ID:patch, 149; patch, 16

Fixed effects:
            Estimate Std. Error t value
(Intercept)  6.44763    1.15028   5.605
time1       -0.01907    0.21237  -0.090
time2       -0.03172    0.24759  -0.128
time3       -0.01124    0.29940  -0.038

model1: AllergyLevel ~ 1 + (1 + time | patch/ID)
model2: AllergyLevel ~ time + (1 + time | patch/ID)
         Df   AIC   BIC  logLik deviance  Chisq Chi Df Pr(&gt;Chisq)
model11 22 14281 14413 -7118.5    14237                         
model12 25 14287 14437 -7118.4    14237 0.0208      3     0.9992
</code></pre>

<p>I have extracted the random coefficients from model 1:</p>

<pre><code>ranef(model1)

`ID:patch`
      (Intercept)       time1        time2        time3
1:11    5.9845070  0.34088535  0.431998708  1.590906238
1:12    5.1236456 -0.03178611 -0.149784278 -0.116150278
1:13    6.3746877 -0.76853294 -0.550037715  0.842518786
   :
   :
</code></pre>
"
"0.0484501583111509","0.0493463771219827","174110","<p>I'm growing a regression tree with the <code>rpart</code> function in R (package of the same name). I would like to be able to choose myself the number of nodes (not the depth of the tree, but the actual number of nodes), either by growing or pruning afterwards.</p>

<p>The problem is that in the cptable the number of nodes jumps and skips some numbers (from 1 to 5 here). I would like, for instance a tree giving 3, 4 or 5 distinct prediction values. </p>

<pre><code>set.seed(1)
df=data.frame(x=rnorm(100), y=rnorm(100))
tree=rpart(data=df, y~x)
tree$cptable
####           CP nsplit rel error   xerror      xstd
#### 1 0.03357572      0 1.0000000 1.013942 0.1337594
#### 2 0.02899422      1 0.9664243 1.187690 0.1620186
#### 3 0.01488440      5 0.8504474 1.188779 0.1632158
</code></pre>

<p>And I can't find the control parameter to set this, if it is even possible (growing or pruning).
Can someone help here please?</p>
"
"0.0843408998948762","0.0944911182523068","174252","<p>I have built some ""regular"" and robust regression models, using the standard lm function as well as rlm and lmrob.  While I know that there is some discussion about using stepwise regression, I have used the stepAIC function to prune my variable set.  After I've gotten a reduced set of variables using stepAIC, I've then run some robust regressions.</p>

<p>The <strong>cvTools</strong> package allows me to use cross validation to compare the performance of my various models.</p>

<p>I obviously would like to run lasso regression (ideally using the <strong>glmnet</strong> package) on my dataset.</p>

<p>My question is whether or not there is an already built package/functionality that will allow me to use cross validiation to compare the lasso regression model with the other models.</p>

<p>If there is not, then my initial thought had been to go back to first principles and manually code K-fold cross validation for lasso regression.  However, I am now wondering if this is theoretically a good idea.  Each time I run a fold in my manual CV, I would run cv.glmnet on the training set.  Each training set would most likely result in a different lambda.min and lambda.1se.  </p>

<p>My question is:  is it technically proper CV to determine the overall CV error by averaging the error on each fold given that the lambda chosen for each fold will be producing a different lasso result?</p>

<p>Here is some sample code that I have used to create leave-one-out CV on the dataset to evaluate the lasso regression.  I have computed my cross validation error  on each fold using lambda.1se and labmda.min that arise for that fold.</p>

<pre><code>lassocv&lt;-function() {

len&lt;-length(drx$DR)

errmin&lt;-0
err1se&lt;-0
print(len)

for (i in 1:len) {
    gmer&lt;-data.matrix(drx[-i,])
    yxer&lt;-yx[-i]
    lfit&lt;-cv.glmnet(gmer, yxer)
    newr&lt;-data.matrix(drx[i,])
    pmin&lt;-predict(lfit,newx=newr,s=lfit$lambda.min)[[1]]
	    p1se&lt;-predict(lfit,newx=newr,s=lfit$lambda.1se)[[1]]
    errmin&lt;-errmin+abs(pmin-yx[i])
    err1se&lt;-err1se+abs(p1se-yx[i])
}
print(errmin/len)
print(err1se/len)

}
</code></pre>

<p>However, I get different CV results.  The two results that are returned for my dataset are 21.94867 and 23.74074.</p>
"
"0.0484501583111509","0.0493463771219827","174396","<p>Hi I'm doing some logistic regression, currently using <code>glmnet</code> package in R.</p>

<p><code>glmnet</code> provides a few measure metrics for cross validation. For classification, we can use type.measure = 'auc' (area under ROC curve) or type.measure = 'class' (misclassification rate).</p>

<p>I'm working with some insurance data with a low rate of positive examples (y = 0 for 95% data, and = 1 for 5% of them. OR 95% people did't buy insurance). </p>

<pre><code>library(ISLR)
data(Caravan)
y = dta$Purchase
x = as.matrix( dta[ , -which(colnames(dta)=='Purchase') ] )
</code></pre>

<p>And I want to predict who are likely to buy insurance.</p>

<p>I think I need to use F1 score or some custom metrics. For example, if I can make 200 by selling an insurance, and the cost to contact one person is 20, then I want to maximize </p>

<p>metric = 200 * N(true positive) - 20 * N(predicted positive)</p>

<p>Is there a way I can do this with glmnet or something else more suitable? Or is <code>AUC</code> a similar metric as F1 score? Any help or discussion's appreciated. Thanks</p>
"
"0.0625488854200668","0.0637058989297032","174462","<p>I am using the glmnet package in R to predict credit default. I have a 50 x variables which I have used in the first model. </p>

<pre><code>fit1=cv.glmnet(x[1:test,1:50], y[1:test], type.measure=""class"")
</code></pre>

<p>I have also generated interaction action terms covering all possible two-way interactions between x variables (i.e. x1*x2, x1*x3... xn*xn). This adds approx 2000 variables to the data set. Some of these interactions I know to be significant above and beyond the impact of of their linear effects. I then used both the 50 original variable, plus the 2000 interactions to fit my second model.  </p>

<pre><code> fit2=cv.glmnet(x[1:test,], y[1:test], type.measure=""class"")
</code></pre>

<p>I figured fit2 would be atleast as good as fit1 in out of sample testing. But strangely, the the best out-of-sample classification rate from fit1 (tested across all values of lambda) beats the equivalent from fit2. </p>

<p>I would've though fit2 to perform at least as well and probably better than fit1 (given that the lasso algorithm should push out all non-important interactions, which is likely to be most of them). I could manually add in only the interaction terms I suspect to be important, but then I face the dilemma about where to draw the line and lose the feature selection capability, which is one of the most useful aspects of lasso regression. My questions:</p>

<ol>
<li>Is there a logical explanation as to why fit2 performs better out of
sample than fit2?</li>
<li>How can I improve fit1 by including interaction variables that I am
confident are significant, without reducing predictive performance?</li>
</ol>
"
"0.0685188709827532","0.0697863157798853","174518","<p>consider the following data set:</p>

<pre><code>a &lt;- c(1, 2, 3, 1, 4, 1968, 2, 1)
b &lt;- c(2, 1, 2, 4, 3, 1984, 2, 0)
c &lt;- c(3, 3, 4, 2, 1, 1945, 1, 0)
d &lt;- c(4, 1, 4, 3, 2, 1975, 3, 1)
df &lt;- data.frame(rbind(a,b,c,d))
names(df) &lt;- c(""ID"", ""OptionW"", ""OptionX"", ""OptionY"", ""OptionZ"", ""yearofBirth"", ""education"", ""sex"")


ID OptionW OptionX OptionY OptionZ yearofBirth education sex
1       2       3       1       4        1968         2   1
2       1       2       4       3        1984         2   0
3       3       4       2       1        1945         1   0
4       1       4       3       2        1975         3   1
</code></pre>

<p>Two hundred people where asked to rank Options W to Z from 1 to 4 in their effectiveness to lower crime rates in their community. Their age, highest academic degree and sex are annotated as well.
I want to find out:</p>

<ul>
<li>which options are preferred by the majority of citizens?</li>
<li>are there significant differences in what men or women, old or young, well or less well educated citizens believe?</li>
<li>how likely is the ranking order going to change if the person is older/younger, has had more or less formal education and is male or female? </li>
</ul>

<p>I read that a multinomial logistic regression might be the way to go, but I find it hard to adapt the examples I find to my data set. Often they allow for only one option to be chosen, making each choice (W, X Y Z) a level of one variable (Options). But in my case I have several variables (OptionW, OptionX, OptionY, OptionZ) where the ranking placement appears to be the level (1,2,3,..10). Or am I looking at it the wrong way?</p>

<p>Which function from what package would be suitable? And are there other methods available apart from mlr?</p>

<p>I use R mostly for spatial analysis and am not very fluent in statistics. Hopefully you can help me here.</p>
"
"0.0796117338651413","0.0900937462695559","174557","<p>I'm currently running a ridge regression in R using the <code>glmnet</code> package, however, I recently ran into a new problem and was hoping for some help in interpreting my results. My data can be found here: <a href=""https://www.dropbox.com/sh/hpxu3t0vqkrzfgf/AAB6F-yMYMfuI5E__gfDuW6sa?dl=0"" rel=""nofollow"">https://www.dropbox.com/sh/hpxu3t0vqkrzfgf/AAB6F-yMYMfuI5E__gfDuW6sa?dl=0</a></p>

<p>My data consists of a 26531x428 observation matrix <code>x</code> and a 26531x1 response vector <code>y</code>. I am attempting to determine the optimal value of <code>lambda.min</code>, and when I run the code</p>

<p><code>&gt; lambda=cv.glmnet(x=x,y=y,weights=weights,alpha=0,nfolds=10,standardize=FALSE)</code></p>

<p>I get</p>

<p><code>$lambda.min
[1] 2.123479
$lambda.1se
[1] 619.0054</code></p>

<p>which are results I would expect. However, I would like to add a slight tweak to this regression. I have prior knowledge of each of my 428 coefficients, and instead of shrinking each coefficient towards 0, as is the default with ridge regression, I would like to shrink each coefficient towards a specific value other than 0. After reaching out to Dr. Trevor Hastie, one of the creators of <code>glmnet</code>, he told me that this could be achieved by running the same code after substituting <code>y</code> with <code>y2</code>, where <code>y2 = y - x%*%d</code> and <code>d</code> is a 428x1 vector of coefficient priors. He said to then add <code>d</code> to my new coefficients, which would give me my prior-informed coefficients. After rerunning the code</p>

<p><code>&gt; lambda=cv.glmnet(x=x,y=y2,weights=weights,alpha=0,nfolds=10,standardize=FALSE)</code></p>

<p>I unfortunately get</p>

<p><code>$lambda.min
[1] 220.3026
$lambda.1se
[1] 220.3026</code></p>

<p>The results of <code>plot(lambda)</code> look like this
<a href=""http://i.stack.imgur.com/ivP0b.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ivP0b.png"" alt=""lambda plot""></a></p>

<p>Does anyone know why <code>glmnet</code> can't find a suitable <code>lambda.min</code>? Could it be because my vector of priors contains estimates that are too far off? Any help would be greatly appreciated!</p>
"
"0.027972711943223","0.0284901441149095","174793","<p>I am trying to do a multivariate probit regression in R. I have 20 dependent variables and 150 indept variables variables. I am trying to test which variables to include in the regression. I know there are several packages to do it but not in the multivariate way: randomForest,VSURF, mixOmics. Any help? some R codes to share. Thank you, Maurice</p>
"
"0.0839181358296689","0.0854704323447285","174861","<p>Here is <a href=""https://onlinecourses.science.psu.edu/stat501/sites/onlinecourses.science.psu.edu.stat501/files/data/iqsize.txt"" rel=""nofollow"">sample data</a>:</p>

<pre><code>    brainIQ &lt;- 
  read.table (file= ""https://onlinecourses.science.psu.edu/stat501/sites/onlinecourses.science.psu.edu.stat501/files/data/iqsize.txt"",
 head = TRUE)
</code></pre>

<p>I am trying to fit multiple linear regression.</p>

<pre><code>mylm &lt;- lm(PIQ ~  Brain + Height + Weight, data = brainIQ)
anova(mylm)
</code></pre>

<p>Default function anova in R provides sequential sum of squares (type I) sum of square. </p>

<pre><code>Analysis of Variance Table

Response: PIQ
          Df  Sum Sq Mean Sq F value  Pr(&gt;F)  
Brain      1  2697.1 2697.09  6.8835 0.01293 *
Height     1  2875.6 2875.65  7.3392 0.01049 *
Weight     1     0.0    0.00  0.0000 0.99775  
Residuals 34 13321.8  391.82                  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>I belief, thus the SS are Brain, Height | Brain, Weight | (Brain, Weight) and residuals respectively.</p>

<p>Using package car we can also get type II sum of square. </p>

<pre><code>library(car)
Anova(mylm, type=""II"")
Anova Table (Type II tests)

Response: PIQ
           Sum Sq Df F value    Pr(&gt;F)    
Brain      5239.2  1 13.3716 0.0008556 ***
Height     1934.7  1  4.9378 0.0330338 *  
Weight        0.0  1  0.0000 0.9977495    
Residuals 13321.8 34                      
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>Here sum of squares are like: Brian | (Height, Weight), Height | (Brain, Weight), Weight | (Brain, Height).</p>

<p>Which look pretty like Mintab output:</p>

<p><a href=""http://i.stack.imgur.com/0iXgH.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/0iXgH.png"" alt=""enter image description here""></a></p>

<p>My question is how can I calculate the regression row in the above table in R ? </p>
"
"0.027972711943223","0.0284901441149095","174989","<p>Please bear with me, I am very new to R.</p>

<p>My question is regarding the use of the <code>improveProb</code> function in the <code>Hmisc</code> package. I have two logistic models, the only difference being that the second model contains my novel marker of interest. I am trying to calculate NRI and IDI.</p>

<p>I have the PredRisks for both models - PredRisk1 and PredRisk2, and my outcome is disease 0/1. How do I define this in R in order to run</p>

<p><code>improveProb(x1, x2, y)</code>?</p>

<hr>

<p>The data are the same for both models. We are looking at ways to validate our findings. We have performed k-fold cross-validation (MSE=0.08) and bootstrapping with optimism (AUC original = 0.826 After correction =0.791) to check for overfitting. Is this appropriate? The LRT was significant for both logistic regression models, but I need to check this. Also, the AIC for model 2 is lower than model 1. Thanks again for your expert knowledge :)</p>
"
"0.027972711943223","0.0284901441149095","175312","<p>I'm using the model ReLogit from the package Zelig in R. ReLogit is a logistic regression for rare events data.
After having estimated the model on the training set, I want to calculate the AUC (Area under the ROC Curve) of this model in the test set. 
How can I do this with this package?</p>
"
"0.0395593886064618","0.040291148201269","175325","<p>I am trying to determine which of several independent variables {A,B,C} best predicts a subject's response R, which can be one of 3 choices, {high, medium, low}.</p>

<p>As the dependent variable is multinomial i.e. not binomial, a binomial logistic regression is not suitable. However, using the lme4 package to fit a glm with <code>family = 'binomial'</code> to the data runs without error or warning, and correctly identifies that independent variable B is the best predictor.</p>

<p>Is this a lucky coincidence, or is the call to <code>glm()</code> automatically fitting multiple binomial regressions to the data (one for each binary combination), or is it something else entirely? Any comments are welcome.</p>
"
"NaN","NaN","175418","<p>I ran the mixed effects model using the lme4 package as follows:
lmer9&lt;- lmer(y ~ station + (1|station:tow) + (1|tow), data=my.data)
 where station is the fixed factor and tow a random one...</p>

<p>However I am interested in obtaining the ANOVA for the regression model. 
I ran anova(lmer9) but it gives the ANOVA (F stat ) only for the fixed factor and doesn't give details about the random effects term and the interaction term.</p>

<p>Please suggest ways in which I could obtain a complete ANOVA...</p>

<p>Thanks,
Praniti</p>
"
"0.108550066801774","0.110557998564556","175597","<p>I am using the <code>quantreg</code> package in R to develop quantile estimates at different taus, then using <code>anova</code> to test whether the Beta Estimates at different quantiles are equal ($H_0$) or not ($H_1$). Thus </p>

<pre><code>library(quantreg)
data(Mammals) # sample data in quantreg
</code></pre>

<p>for taus 0.1, 0.25, 0.5, 0.75 and 0.9</p>

<pre><code>fit1 &lt;- rq(weight ~ speed + hoppers + specials, tau = .1, data = Mammals)
fit2 &lt;- rq(weight ~ speed + hoppers + specials, tau = .25, data = Mammals)
fit3 &lt;- rq(weight ~ speed + hoppers + specials, tau = .5, data = Mammals)
fit4 &lt;- rq(weight ~ speed + hoppers + specials, tau = .75, data = Mammals)
fit5 &lt;- rq(weight ~ speed + hoppers + specials, tau = .9, data = Mammals)

anova(fit1, fit2, fit3, fit4, fit5, test=""Wald"", joint=FALSE)
</code></pre>

<p>Results in </p>

<pre><code>Quantile Regression Analysis of Deviance Table

Model: weight ~ speed + hoppers + specials
Tests of Equality of Distinct Slopes: tau in {  0.1 0.25 0.5 0.75 0.9  }

             Df Resid Df F value  Pr(&gt;F)  
speed         4      531  1.0952 0.35810  
hoppersTRUE   4      531  2.5898 0.03599 *
specialsTRUE  4      531  1.3774 0.24046  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<p>However changing the order of the models to say;</p>

<pre><code>anova(fit3, fit1, fit2, fit4, fit5, test=""Wald"", joint=FALSE)
</code></pre>

<p>Produces the <strong>exact same result!</strong></p>

<h2>My question is basically, what gives?</h2>

<p><strong>(1)</strong> Is <code>anova</code> truly comparing all the models to one another (ie all estimates from different taus, ${_nC_r} = {_5C_2} = 10$ <strong>separate</strong> comparisons) </p>

<p><strong>OR</strong> </p>

<p><strong>(2)</strong> Is <code>anova</code> selecting the model with the lowest tau and comparing the remaining models to that?</p>

<p>I've extracted (and annotated) the relevant segments of the of <code>anova</code> function called in the <code>quantreg</code> environment bellow.</p>

<pre><code>getAnywhere(anova.rqlist)
sum.fit1 &lt;- summary(fit1, covariance=TRUE); sum.fit2 &lt;- summary(fit2, covariance=TRUE); 
sum.fit3 &lt;- summary(fit3, covariance=TRUE); sum.fit4 &lt;- summary(fit4, covariance=TRUE); 
sum.fit5 &lt;- summary(fit5, covariance=TRUE)
objects &lt;- list(); objects[[1]] &lt;- sum.fit1; objects[[2]] &lt;- sum.fit2 
objects[[3]] &lt;- sum.fit3; objects[[4]] &lt;- sum.fit4; objects[[5]] &lt;- sum.fit5
taus &lt;- c(0.1, 0.25, 0.5, 0.75, 0.9)
m &lt;- length(taus)
n &lt;- length(fit1$y)
    Omega &lt;- outer(taus, taus, pmin) - outer(taus, taus) ##!!!HERE!!!###
    J &lt;- objects[[1]]$J 
# From help file on summary.rq: J is Unscaled Outer product of gradient matrix returned if cov=TRUE and se != ""iid"". The Huber sandwich is cov = tau (1-tau) Hinv %*% J %*% Hinv. 
p &lt;- dim(J)[1]
H &lt;- array(unlist(lapply(objects, function(x) x$Hinv)), c(p, p, m))
# From help file on summary.rq: Hinv : inverse of the estimated Hessian matrix returned if cov=TRUE and se %in% c(""nid"",""ker"") , note that for se = ""boot"" there is no way to split the estimated covariance matrix into its sandwich constituent parts.    
H &lt;- matrix(aperm(H, c(1, 3, 2)), p * m, p) %*% t(chol(J))
W &lt;- (H %*% t(H)) * (kronecker(Omega, outer(rep(1, p), rep(1, p)))) ##!!!HERE!!!###
coef &lt;- unlist(lapply(objects, function(x) coef(x)[, 1]))
Tn &lt;- pvalue &lt;- rep(0, p - 1)
ndf &lt;- m - 1
ddf &lt;- n * m - (m - 1)
for (i in 2:p) {
  E &lt;- matrix(0, 1, p)
  E[1, i] &lt;- 1
  D &lt;- kronecker(diff(diag(m)), E)
  Tn[i - 1] &lt;- t(D %*% coef) %*% solve(D %*% W %*% 
                                         t(D), D %*% coef)/ndf
  pvalue[i - 1] &lt;- 1 - pf(Tn[i - 1], ndf, ddf)
}
pvalue
</code></pre>

<p>The reason i care is that if explanation <strong>(1)</strong> is being implemented then all the estimates are truly being compared, while if explanation <strong>(2)</strong> is being implemented, then technically the models are only being compared to minimum tau and <strong>NOT</strong> to one another. </p>

<p><strong>Note:</strong> The lines that define <code>Omega</code> and <code>W</code> suggest to me that the latter interpretation <strong>(2)</strong> is being implemented, but I'm not sure.</p>
"
"0.027972711943223","0.0284901441149095","175617","<p>I was interested in knowing if anyone is using the custom made function of BRT by Elith et al. (2008) in Journal of Animal Ecology <a href=""http://onlinelibrary.wiley.com/doi/10.1111/j.1365-2656.2008.01390.x/pdf"" rel=""nofollow"">""A working guide to boosted regression trees""</a> and knows what does <code>tolerance = fixed</code> or <code>tolerance = auto</code> does in the following function: </p>

<p>This function is found in the <code>dismo</code> package in R. </p>

<p><a href=""https://cran.r-project.org/web/packages/dismo/dismo.pdf"" rel=""nofollow"">https://cran.r-project.org/web/packages/dismo/dismo.pdf</a></p>

<pre><code>?dismo::gbm.step

mdl &lt;- gbm.step(data=df1,
                gbm.x = 4:7,
                gbm.y = 16,
                family = ""gaussian"", 
                tree.complexity = 1,
                learning.rate = 0.1,
                bag.fraction = 0.5,
                tolerance.method = ""fixed"",
                tolerance = 0.1)
</code></pre>
"
"0.0559454238864459","0.056980288229819","176147","<p>Im trying to estimate the linear curve (y~x) where I know intercept must be normally distributed around -100, and slope always positive and normally distributed around 2 (blue continous line in plots below).</p>

<p>The example.data.1 below is ""clean"" and the linear regression (red dashed line) is ok. The resulting red dashed line is what I want.</p>

<p>But example.data.2 has many measurement errors so the red dashed line becomes unrealistic. The resulting line should be parallel with the blue line, but lower.</p>

<p>How can I assign a strong prior similar to the blue line in the plots, so that I get a posterior reasonably similar to the blue line?</p>

<pre><code>example.data.1 &lt;- structure(list(x = c(1.36, 2.22, 2.53, 3.09, 3.44, 3.25, 3.15, 
                                       3.21, 3.57, 3.63, 3.51, 2.85, 2.56, 2.25, 1.61, 1.35, 1, 1.6, 
                                       1.92, 1.9, 2.3, 2.61, 3.9, 3.74, 3.77, 3.77, 3.49, 3.37, 3.35, 
                                       2.79, 2.31, 1.88, 1.5, 1.18, 1.83, 2.32, 3.06, 3.37, 3.77, 3.82, 
                                       3.75, 3.72, 3.53, 3.35, 3.67, 3.18, 3.11, 2.43, 1.9, 1.39, 1.17, 
                                       1.48, 2.05, 2.62, 3.08, 3.65, 3.92, 4.08, 4.1, 3.47, 3.84, 3.45, 
                                       2.87, 2.83, 2.49, 1.87, 2.06, 2.49, 1.78, 2.33, 2.95, 3.73, 3.64, 
                                       3.62, 4.1, 3.85, 4.06, 3.67, 3.3, 2.86, 2.46, 2.32, 2.08, 1.64, 
                                       1.96), y = c(-101.04, -99.42, -98.33, -96.88, -95.22, -91.89, 
                                                    -91.63, -90.19, -92.98, -95.58, -95.69, -96.32, -96.94, -98.25, 
                                                    -100.11, -100.81, -101.87, -99.72, -99.94, -100.87, -100.38, 
                                                    -98.64, -93.38, -92.98, -93.39, -93.76, -93.25, -93.12, -94.46, 
                                                    -96.45, -97.46, -99.75, -100.09, -101.62, -101.1, -97.8, -96.33, 
                                                    -96.21, -94.37, -93.18, -93.32, -93.73, -94.13, -94.4, -94.63, 
                                                    -94.83, -96.29, -98.11, -100.2, -100.82, -101.56, -101.35, -100.61, 
                                                    -98.65, -97.37, -95.36, -95.45, -95.33, -95.63, -95.26, -97.08, 
                                                    -97.1, -97.14, -96.9, -98.17, -99.47, -100.17, -100.58, -100.55, 
                                                    -99.94, -99.02, -97.3, -96.25, -95.44, -95.69, -95.21, -95.87, 
                                                    -95.87, -97.71, -96.91, -97.62, -97.94, -98.9, -99.79, -99.88
                                       )), .Names = c(""x"", ""y""), row.names = c(NA, -85L), class = ""data.frame"")

example.data.2 &lt;- structure(list(x = c(3.11, 3.46, 3.42, 3.34, 3.3, 2.45, 4, 4.2, 
                                       4.08, 3.57, 1.97, 1.83, 1.07, 0.68, 0.54, 0.47, 0.63, 3.19, 3.52, 
                                       3.49, 3.47, 3.36, 2.76, 3.42, 3.17, 3.54, 2.56, 1.06, 1.09, 0.84, 
                                       0.64, 0.61, 0.74, 0.49, 3.49, 3.56, 3.46, 3.25, 3.72, 3.57, 3.58, 
                                       2.62, 1.99, 1.85, 1.04, 1.06, 0.62, 0.49, 0.48, 0.68, 0.5, 3.63, 
                                       3.71, 3.75, 3.67, 3.78, 3.52, 3.04, 2.26, 1, 1.17, 1.01, 0.92, 
                                       0.65, 0.54, 0.36, 0.38, 0.3, 3.08, 3.79, 3.9, 3.5, 3.4, 2.57, 
                                       3.03, 1.93, 2.02, 1.5, 0.67, 0.63, 0.72, 0.6, 0.67, 0.63, 0.53
), y = c(-105.28, -104.1, -104.81, -104.34, -104.37, -105.31, 
         -103.59, -103.32, -102.66, -103.57, -103.73, -104.47, -97.69, 
         -92.56, -95.9, -95.72, -107.6, -104.39, -105.12, -104.18, -104.46, 
         -102.19, -103.59, -103.38, -103.48, -102.84, -96.52, -88.54, 
         -90.36, -93.7, -85.21, -89.68, -99.47, -91.92, -104.58, -103.91, 
         -104.47, -104.49, -104.41, -104.41, -102.6, -98.65, -87.98, -89.23, 
         -86.34, -94.21, -91.57, -84.62, -84.14, -95.33, -102.14, -104.18, 
         -103.8, -102.47, -101.75, -101.73, -102.84, -97.49, -92.67, -91.72, 
         -80.45, -80.97, -84.94, -80.2, -81.05, -77.84, -82.72, -91.75, 
         -105.19, -104.66, -104.36, -104.31, -103.57, -102.68, -98.4, 
         -89.48, -85.92, -84.59, -84.49, -81.13, -83.28, -83.12, -85.62, 
         -85.89, -90.07)), .Names = c(""x"", ""y""), row.names = c(NA, -85L
         ), class = ""data.frame"")


lm.1 &lt;- coefficients(lm(example.data.1$y~example.data.1$x))
lm.2 &lt;- coefficients(lm(example.data.2$y~example.data.2$x))

library(ggplot2)
p &lt;- ggplot(example.data.1, aes(x=x, y=y))
p &lt;- p + geom_point()
p &lt;- p + geom_abline(intercept=(-100), slope=2, color=""blue"")
p &lt;- p + geom_abline(intercept=lm.1[1], slope=lm.1[2], color=""red"", linetype=""dashed"")
p &lt;- p + xlim(0, 10)
p &lt;- p + ylim(-110, -50)
p 

p &lt;- ggplot(example.data.2, aes(x=x, y=y))
p &lt;- p + geom_point()
p &lt;- p + geom_abline(intercept=(-100), slope=2, color=""blue"")
p &lt;- p + geom_abline(intercept=lm.2[1], slope=lm.2[2], color=""red"", linetype=""dashed"")
p &lt;- p + xlim(0, 10)
p &lt;- p + ylim(-110, -50)
p
</code></pre>

<p>I need to do this for tens of thousands of data-groups like each example above, so a fast algorithm is important. I have tried to use Stan, Jags, and arm-package, but don't understand how to tell those functions what I want.</p>

<p>My limited knowledge about statistics lead me to think that a bayesian approach is best, but I could be wrong.</p>
"
"0.0500391083360534","0.0637058989297032","176918","<p>In modeling claim count data in an insurance environment, I began with Poisson but then noticed overdispersion. A Quasi-Poisson better modeled the greater mean-variance relationship than the basic Poisson, but I noticed that the coefficients were identical in both Poisson and Quasi-Poisson models. </p>

<p>If this isn't an error, why is this happening? What is the benefit of using Quasi-Poisson over Poisson?</p>

<p><strong>Things to note:</strong></p>

<ul>
<li>The underlying losses are on an excess basis, which (I believe) prevented the Tweedie from working - but it was the first distribution I tried. I also examined NB, ZIP, ZINB, and Hurdle models, but still found the Quasi-Poisson provided the best fit. </li>
<li>I tested for overdispersion via dispersiontest in the AER
package. My dispersion parameter was approximately 8.4, with p-value
at the 10^-16 magnitude.  </li>
<li>I am using glm() with family = poisson or quasipoisson and a log link
for code. </li>
<li>When running the Poisson code, I come out
with warnings of ""In dpois(y, mu, log = TRUE) : non-integer x = ..."".</li>
</ul>

<p><strong>Helpful SE Threads per Ben's guidance:</strong></p>

<ol>
<li><a href=""http://stats.stackexchange.com/questions/11182/when-to-use-an-offset-in-a-poisson-regression"">Basic Math of Offsets in Poisson regression</a></li>
<li><a href=""http://stats.stackexchange.com/questions/167964/poisson-glm-with-non-count-data-rate-data?lq=1"">Impact of Offsets on Coefficients</a></li>
<li><a href=""http://stats.stackexchange.com/questions/175349/in-a-poisson-model-what-is-the-difference-between-using-time-as-a-covariate-or?"">Difference between using Exposure as Covariate vs Offset</a></li>
</ol>
"
"0.0625488854200668","0.0637058989297032","177516","<p>I aim to perform a Cox regression. My data set contains roughly 10 variables which I intend to include, for a total of 5000 patients, yielding 900 events. I want to present how a certain variable relates to hazard by using restricted cubic splines. However I cannot do that with the complete case data set due to missing data; I tried it, though, and it returned very implausible results. Contrary to all previous studies. So i imputed data with the mice package. And then analyzed one of these data sets by using the rms package functions cph() and plot.Predict(). However, displaying the results from one of the complete data sets is not acceptable, as most statisticians would concur.</p>

<p>How would you resolve this?</p>

<pre><code># Data to set out
library(mice)
library(survival)
# Example data set
data(lung)

# impute 5 data sets
imputation   &lt;- mice(lung, m=5, maxit=10, seed=500)

# Fit a Cox proportional hazards mdoel
fit &lt;- with(imputation, coxph(Surv(time, status) ~ pspline(meal.cal) + pat.karno))

# How on earth to proceed to create plot showing how the variable modelled as a spline relates to hazard?
</code></pre>
"
"0.0927749898843639","0.0944911182523068","177654","<p>When running an ordered logistic regression using the <code>polr</code> function of the <code>MASS</code>package (DV is low, medium, high) and have a look at the summary I get Î²s for every IV and the intercepts for low|medium and medium|high.</p>

<p>The <code>predict</code>function for assessing the probabilities (<code>type='p'</code>) or the classes (<code>type='class'</code>) also works just fine.</p>

<p>However I want to calculate the probabilities myself in order to use them with different data sets.</p>

<p>If I use the following code for a <em>logistic model with a binary (!) dependent variable</em>, I can exactly replicate the <code>predict</code> - outcome:</p>

<p><code>log_pred &lt;- (logit_model$coefficients[1] + logit_model$coefficients[2]*IV_1 + logit_model$coefficients[3]*IV_2)</code></p>

<ul>
<li><code>logit_model</code> is my <code>glm</code>-object</li>
<li><code>logit_model$zeta[1]</code> is the first intercept</li>
<li><code>logit_model$zeta[2]</code> is the second intercept</li>
<li><code>logit_model$coefficients[1]</code> is the Î² of IV_1</li>
<li><code>logit_model$coefficients[2]</code> is the Î² of IV_2</li>
</ul>

<p>the only thing I have to do now, to get the predicted probabilities is:</p>

<p><code>log_pred_probs &lt;- exp(log_pred)/(1+exp(log_pred))</code></p>

<p>If I understand all the posts on ordered logistic regression I read correctly, the only thing I have to change with a <code>polr</code> object with the 3 ""groups"" of low, medium, and high would be to:</p>

<ul>
<li>run the <code>log-pred</code>part for each group using their own intercepts, let's call them <code>log_pred1</code> and <code>log_pred2</code></li>
<li>and to, then, run the following code (similar to the logistic model above):
<code>log_pred_probs1 &lt;- exp(log_pred1)/(1+exp(log_pred1)+exp(log_pred2))</code> for ""low""
<code>log_pred_probs2 &lt;- exp(log_pred2)/(1+exp(log_pred1)+exp(log_pred2))</code> for ""medium""
<code>log_pred_probs3 &lt;- 1/(1+exp(log_pred1)+exp(log_pred2))</code> for ""high""</li>
</ul>

<p>I think there are at least two problems ('cause this doesn't work at all):</p>

<ol>
<li>I need the Î²-coefficients for every level of the dependent variable, and <code>summary(polr-object)</code>does only show the Î²s for the first group (so does <code>$coefficients</code>)</li>
<li>and I am not sure about the computation of the predicted probabilities for group 3, ""high"".</li>
</ol>

<p>So these are the questions in short: <strong>How do I assess the Î²-coefficients for every level of the DV in a <code>polr</code>object?</strong></p>

<p>And</p>

<p><strong>How do I compute the predicted probabilities for every level of the DV myself?</strong></p>
"
"0.134152413729855","0.136633931250005","177960","<p>I'm trying to assess the effect of showing more impressions on a user. I want to study if users who saw more ads are more likely to make a purchase onsite. To do so I've created a multilevel model. I grouped users into 10 groups averaging their scores (we score users based on a number of factors). So basically I end up having 10 groups (from 0 to 9), where on group 9 I assume to have the best users, and on group 0 the worst.</p>

<pre><code>  picbucket mcuserid impressions mediacostcpm is_buyer gr.impressions gr.mediacostcpm
1         0 1           1        0.460        0       3.632794        2.767509
2         0 2           2        5.000        0       3.632794        2.767509
3         0 3           1        4.590        0       3.632794        2.767509
4         0 4           1        0.590        0       3.632794        2.767509
5         0 5           1        5.000        0       3.632794        2.767509
6         0 6           1        0.315        0       3.632794        2.767509
</code></pre>

<p>I think a multilevel model could be advantageous here because I'm expecting to see different effects on each group. On the best users I'm expecting an additional impression could have a higher impact, whereas on bad users and additional impression may be worthless. It could also be possible the opposite though. So that users which generally higher score will convert even without the need of serving them more impressions, whereas on mid groups additional impressions tend to change their behaviour. </p>

<p>A good model representation could be:</p>

<p>$y_{i} = \alpha_{j[i]} + X_{i}\beta + \epsilon_{i}$</p>

<p>The second level of the model will then be:</p>

<p>$\alpha_{j} = \mu_{\alpha} + \eta_{j}, \text{ with } \eta_{j} \sim N(0, \sigma_{\alpha}^{2})$</p>

<p>On the first level I want to include as a predictor how many impression a user saw. On the second level I want to include the average impressions a user saw within its group and the average media cost for the impressions we served on that user. I've used the package <code>lme4</code> in R to build my model.</p>

<pre><code>glmer(formula = is_buyer ~ impressions + mediacostcpm + (1 + 
    gr.impressions + gr.mediacostcpm | picbucket), data = new.df, 
    family = binomial())
             coef.est coef.se
(Intercept)  -7.42     0.33  
impressions   0.00     0.02  
mediacostcpm  0.03     0.01  

Error terms:
 Groups    Name            Std.Dev. Corr        
 picbucket (Intercept)     7.86                 
           gr.impressions  2.22     -0.99       
           gr.mediacostcpm 0.57     -0.68  0.60 
 Residual                  1.00                 
---
number of obs: 103146, groups: picbucket, 10
AIC = 2755.4, DIC = 2680.5
deviance = 2708.9 
</code></pre>

<p>This is my first experiment with multilevel modeling so I would like to make sure I don't misunderstand the results of my model. </p>

<p>From what I see here, the <code>impressions</code> predictor on the first level is useless. Its coefficient is zero and its standard deviation is very small. This could be due to the fact I'm including a group average on the second level for the impression count (<code>gr.impressions</code>). So, on any group (<code>picbucket</code>), serving more impressions than the average doesn't tell us much about the likelihood of a cookie to convert.</p>

<p>The average media cost on the first level is however an interesting one. Generally, within a group, if I spend a bit more for every impression I should increase the probability of generating conversions. This is probably due to inventory quality. Better inventory costs more, but also has better changes to be viewable inventory.</p>

<p>The coefficients on the group level instead tell you how much they contribute on explaining the group slope. So in this case the average number of impressions at the group level seems to explain quite a significant part of the group slope. </p>

<p>Interestingly, at the group level <code>gr.impressions</code> seem to be a very useful predictor, but at the within group level its usefulness is limited. The opposite applies to the <code>mediacostcpm</code>.</p>

<p>Am I interpreting these results correctly? How can I tell if the model has a good fit? Please note I've used a binomial regression because the dependent variable, <code>is_buyer</code> can take only 0 or 1 (one being the user made a purchase).</p>
"
"0.0740088392978143","0.0646095738380922","178160","<p>I have a run a linear mixed effects model in R to model clinical data. However, this model is heteroscedastic (as there excess zeros in the response variable).</p>

<p>I have tried transforming the data (log transform) and (sqrt). Still, neither transformation resolves the issue (see residual versus fitted value plot). I have not used Cox proportional hazards model as the data is not time-to-event data, the data measures force and there are a large number of observations have a reading of zero. I cannot exclude these readings as they are valid.</p>

<p>I have found an R package that runs Tobit regression (AER). Nevertheless, this will not accommodate the random effects in the model.
I cannot find any R packages that run Weibull mixed effects models (or gamma mixed effects models)... </p>

<p>Does anyone know if there is a package to run these type of models? (or can they suggest any alternative approach). </p>

<p>Many thanks</p>

<p>Etn</p>
"
"0.0740088392978143","0.0753778361444409","178727","<p>I am looking for a way to perform weighted total least squares in R. I know one can use PCA for this as described nicely in the following post. 
<a href=""http://stats.stackexchange.com/questions/13152/how-to-perform-orthogonal-regression-total-least-squares-via-pca"">How to perform orthogonal regression (total least squares) via PCA?</a></p>

<p>However, I need a weighted version of total least squares, i.e. I want to account for measurement error in my data, where the error $(\Delta x_i, \Delta y_i)$ can be different for each data point $(x_i, y_i)$. Any suggestions? Specifically, I want to do the following:</p>

<pre><code>x = rnorm(10,0,2)
x.se = rnorm(10,0,0.7)
y = 20*x
y.se = rnorm(10,0,1)
r &lt;- prcomp( ~ x + y )
slope &lt;- r$rotation[2,1] / r$rotation[1,1]
intercept &lt;- r$center[2] - slope*r$center[1]
</code></pre>

<p>However, here I am not accounting for the varying measurement errors in $x$ and $y$. Is there any R package which I can use to account for the <code>x.se</code> and <code>y.se</code> vectors?</p>
"
"0.0685188709827532","0.0581552631499044","178944","<p><strong>Context</strong></p>

<p>I'm attempting to understand how R's coxph() accepts and handles repeated entries for subjects (or patient/customer if you prefer). Some call this Long format, others call it 'repeated measures'.</p>

<p>See for example the data set that includes the ID column in the Answers section at:</p>

<p><a href=""http://stats.stackexchange.com/questions/143340/best-packages-for-cox-models-with-time-varying-covariates/162621?noredirect=1#comment338858_162621"">Best packages for Cox models with time varying covariates</a></p>

<p><em>Also assume covariates are time-varying throughout and there is exactly one censor (i.e. event) variable, which is binary.</em></p>

<p><strong>Questions</strong></p>

<p>1) In the above link's answer, if ID is not given as a parameter in the call to coxph() should the results be the same as including cluster(ID) as a parameter in coxph()? </p>

<p>I attempted to search for documentation, but the following doesn't seem to clearly address (1):
<a href=""https://stat.ethz.ch/pipermail/r-help//2013-July/357466.html"" rel=""nofollow"">https://stat.ethz.ch/pipermail/r-help//2013-July/357466.html</a></p>

<p>2) If the answer to (1) is 'no', then (mathematically) why? It seems cluster() in coxph() seeks correlations between subjects as per subsection 'cluster' on pg. 20 at</p>

<p><a href=""https://cran.r-project.org/web/packages/survival/survival.pdf"" rel=""nofollow"">https://cran.r-project.org/web/packages/survival/survival.pdf</a></p>

<p>3) Vague question: how does coxph() with repeated measures compare to R's frailtypack regression methods? </p>

<p><strong>Addenda</strong></p>

<p>The following hints at using cluster(ID):</p>

<p><a href=""http://stats.stackexchange.com/questions/10051/is-there-a-repeated-measures-aware-version-of-the-logrank-test"">Is there a repeated measures aware version of the logrank test?</a></p>

<p>as does:</p>

<p><a href=""https://stat.ethz.ch/pipermail/r-help//2013-July/357466.html"" rel=""nofollow"">https://stat.ethz.ch/pipermail/r-help//2013-July/357466.html</a></p>

<blockquote>
  <p>GEE approach: add ""+ cluster(subject)"" to the model statement in coxph
  Mixed models approach: Add "" + (1|subject)"" to the model statment in coxme.</p>
</blockquote>

<p>Thanks in advance!</p>
"
"0.0791187772129236","0.080582296402538","178974","<p>I have a dataset with 100 predictor variables (95 continuous, and 5 categorical) and 1 target variable (continuous). After plotting the density plots of the continuous predictor variables, they are all normally distributed. </p>

<p>My goal is to build a linear regression model, as a start, and use the 100 predictor variables to predict the target variable. </p>

<ol>
<li>How do I know if I need to ""normalize"" my data (the predictor variables that are continuous)?</li>
<li>If I determine that I need to normalize my predictor variables, do I also need to normalize my target variable? </li>
<li>How do I determine which method of normalization is appropriate? Is this a local (per variable) decision or global (one normalization approach for all variables)? </li>
</ol>

<p>I am using R, if there are any packages that can help, please let me know. </p>

<p>I am not sure if I should make the decision to normalize values before or after the regression model is built. For example, I could forgo data normalization, build the model, and cross-validate it, and if I don't like the results, repeat the process by tinkering with data normalization. To me, this approach would seem like fudging the process until I get a reasonable result. </p>
"
"0.0930988128231456","0.0948209311861521","179250","<p>I am conducting a multifactorial analyisis involving categorical variables by using R. The response is â€œyesâ€ or â€œnoâ€ (Iâ€™m therefore using binary logistic regression) and the predictors have 2 up to 3 levels. The sample size is quite small (n = 230).</p>

<p>I tried before both the glm and rsm packages, but one predictor (B2) had an extremely high standard error; it was a separation problem since one of its level has zero observation. I have been then suggested to rely on penalized logistic regression and in particular the brglm package.</p>

<p>Here as follows, you can see the new output (brglm) for the model containing all the variables:</p>

<pre><code>Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)  1.64053    0.78655   2.086   0.0370 * 
B1x         -0.07434    0.71502  -0.104   0.9172   
B1y         -0.20966    0.59622  -0.352   0.7251   
B2x          0.09004    0.57168   0.158   0.8748   
B2y         -2.68270    1.67192  -1.605   0.1086   
B3x         -0.53347    0.33882  -1.574   0.1154   
B4x         -0.59495    0.30320  -1.962   0.0497 (*) 
B5x         -0.79962    0.47249  -1.692   0.0906 . 
B5y         -1.42761    0.59344  -2.406   0.0161 (*) 
B6x          1.36948    0.49224   2.782   0.0054 (**)
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 275.58  on 229  degrees of freedom
Residual deviance: 270.93  on 220  degrees of freedom
Penalized deviance: 253.9314 
AIC:  290.93 
</code></pre>

<p>As you can see, the problem regarding the huge standard error has been finally solved but there are still several issues:</p>

<p>1) In the description of the bglrm package, they say:</p>

<p>â€œIt is not advised to use methods associated with model comparison (add1, drop1,anova, etc.) on objects of class ""brglm"". Model comparison when estimation is performed using the modified scores or the penalized likelihood is an on-going research topic and will be implemented as soon as it is concludedâ€</p>

<p><strong>How can I safely remove non significant predictors?</strong> <strong>Have I only to rely on the p-value in the above output?</strong> 
That means that I have to remove from a further model predictors from B1 to B4, is that correct? (that was also case when I used a non penalized logistic regression model (such as lrm), but then I could use drop1).</p>

<p>2) <strong>How can I detect multicollinearity issues in brglm?</strong> </p>

<p>By using a non penalized likelihood model, I know that there is multicollinearity between B5 and B6, but how can I prove it now?</p>

<p>3) <strong>How can I calculate in brglm indexes such as C and Nagelgerkeâ€™s R</strong>? </p>

<p>4) In the description of the package, it is also written: â€œThe use of Akaike's information criterion (AIC) for model selection when method = ""brglm.fit"" is controversial.â€  </p>

<p>Is it safe or not to publish AIC in a paper?</p>

<p>Thank you very much in advance!</p>
"
"0.027972711943223","0.0284901441149095","179541","<p>As the complexity parameter is calculated? What is the meaning of it?</p>

<p>From what I read, the cp is a value at which the tree makes divisions in the nodes until the reduction in the relative error is less than a certain value.</p>

<p>There are places I read that say the CP affects only the growth of the tree and others say that interferes with pruning too. For min appears that it interferes only in growth but not sure.</p>

<p>I am using rpart () package to create trees, in the case of the classification tree exists missclassification rate to evaluate the ratings, but in the case of regression is not anything to evaluate the predictions beyond the MSE?</p>
"
"0.0395593886064618","0.040291148201269","179803","<p>Can Theil-Sen be defined for multiple linear regression? If so, is there an implementation in R for it?</p>

<p>I simply want a formula <code>a~b+c</code>, but the package <code>mblm</code> fails with the (misleading) error message: <code>stop(""Only linear models are accepted"")</code>. Of course, this is a linear model: but they mean that Theil-Sen only works for two-dimensional linear models. </p>
"
"0.0484501583111509","0.0493463771219827","179891","<p>I have a data frame that is consisted of 20 observations and 35 variables.</p>

<p>I want to prepare the data for partial least square regression PLS in R.</p>

<p>Many authors suggest:</p>

<p>1)Check whether the variables are normally distributed or not </p>

<p>2)log-transform variables that are not normally distributed</p>

<p>3)center data </p>

<p>4)scale data (standardize data)</p>

<p>I checked the normal distribution of the variables using Shapiro-Wilks test and then I log transformed the variables that are not normally distributed.</p>

<p>My questions are: 1) should I standardize log transformed data or the original dataset?
2) Is there any R package that pre-process data for pls?</p>
"
"0.027972711943223","0.0284901441149095","179898","<p>I have a clinical dataset (1400 cases) and I applied 4 data mining techniques (ANN, Decision Tree, SVM, Logistic Regression) to predict the binary outcome (Yes, No).</p>

<p>Now, I want to improve prediction accuracy through ensemble methods.<br>
What are the criteria to choose which model can be combined with another model?
And how can that be done in R? Can I use the ""caret"" package?</p>
"
"0.0791187772129236","0.080582296402538","180178","<p>Using the package <code>mfp</code> in R and would like to plot the fit with 95% confidence intervals.</p>

<p>This seems straightforward with an <code>lm</code> object (<a href=""http://stackoverflow.com/questions/15180008/how-to-calculate-the-95-confidence-interval-for-the-slope-in-a-linear-regressio"">as in this question</a>, <a href=""http://stats.stackexchange.com/questions/135707/plotting-a-polynomial-regression-with-its-confidence-interval-of-95-in-r"">and this question</a>) and a <code>glm</code> object as shown in  <a href=""http://stackoverflow.com/questions/20620277/get-95-confidence-interval-with-glm-in-r"">this question</a>.  </p>

<p>However, the <code>mfp</code> is not recognized/structured as either <code>lm</code> or <code>glm</code>as the function <code>mfp</code> generates an <code>mfp</code> object.  <code>mfp</code> objects do use <code>predict.glm</code>, from which I can get the <code>se.fit</code>. </p>

<p><em>All that to say, that I want to confirm I'm using the appropriate formula to calculate CI around the fit as shown below:</em></p>

<p><a href=""http://i.stack.imgur.com/rQ6I3.gif"" rel=""nofollow""><img src=""http://i.stack.imgur.com/rQ6I3.gif"" alt=""enter image description here""></a></p>

<p><a href=""http://i.stack.imgur.com/RYzou.gif"" rel=""nofollow""><img src=""http://i.stack.imgur.com/RYzou.gif"" alt=""enter image description here""></a></p>

<pre><code>somedat &lt;- data.frame(x=1:20, y=pexp(1:20, rate=1/3))
library(mfp)
mymfp &lt;- mfp(formula = y~fp(x), data = somedat)
mypred &lt;- predict(object = mymfp, type = 'response', se.fit = T)

plot(somedat$x, somedat$y, ylim=c(0,1.2))
lines(predict(mymfp,  type = 'response')) #fit
lines(mypred$fit-mypred$se.fit*1.96, col='orange') #lower
lines(mypred$fit+mypred$se.fit*1.96, col='orange') #upper
</code></pre>

<p><a href=""http://i.stack.imgur.com/xhpHG.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/xhpHG.png"" alt=""enter image description here""></a></p>

<p>Is the above an appropriate equation/solution for a 95% confidence interval around the fractional polynomial fit?</p>
"
"0.0685188709827532","0.0697863157798853","180191","<p>We can apply the Hosmer-Lemeshow goodness of fit to logistic regression modelling and to test if an underlying assumption is not applicable.</p>

<p>This <a href=""https://www.youtube.com/watch?v=MYW8gA1EQCQ"" rel=""nofollow"">link</a> shows a video of the application to a standard <code>glm()</code> model</p>

<p>This <a href=""http://stats.stackexchange.com/questions/132652/how-to-determine-which-distribution-fits-my-data-best-r"">detailed question</a>, outlines various simulation-based tests one can run to assess underlying distributions.</p>

<p><strong>But I want to apply the Hosmer-Lemeshow goodness of fit to survival analysis with assumed underlying data distributions</strong>.</p>

<p>Much literature points one towards a cox proportional hazards model, but from what I understand, a cox ph model does not assume an underlying distribution of data.
Therefore lets take some random data from the <code>survreg()</code> function of the <code>survival</code> package</p>

<pre><code>library(survival)

data(ovarian)

head(ovarian)

s &lt;- Surv(ovarian$futime, ovarian$fustat)
sWei &lt;- survreg(s ~ age,dist='weibull',data=ovarian)
</code></pre>

<p>How can we applying a H+L G.O.F statistic test?
I had hoped to follow this <a href=""http://thestatsgeek.com/2014/02/16/the-hosmer-lemeshow-goodness-of-fit-test-for-logistic-regression/"" rel=""nofollow"">link</a>, however the <code>survreg()</code> does not allow a <code>fitted()</code> function. Thus this does not work</p>

<pre><code>library(ResourceSelection)
hl &lt;- hoslem.test(sWei$y, fitted(sWei), g=10))
</code></pre>
"
"0.0559454238864459","0.056980288229819","180217","<p>I'm using time series data containing both trend and seasonality. I also have 2 endogenous predictor variables that I would like to include in my model.</p>

<p>In R I've used the forecast package to develop a dynamic regression model with use of <code>auto.arima()</code> and the <code>xreg</code> argument from the <code>forecast package</code>. I understand this procedure takes a regression and then attempts to fit the residuals with an ARMA Model.</p>

<p>I've also developed what seems to be an appropriate model using the forecasting Module in SPSS by specifying a Seasonal ARIMA model and including my covariates. However, one of the coefficients on one of my endogeneous predictors has a negative sign which makes no sense intuitively. </p>

<p>I've read Dr. Hyndman's article <a href=""http://robjhyndman.com/hyndsight/arimax/"" rel=""nofollow"">The ARIMAX model muddle</a> and found it to be extremely insightful and useful. However, I have not been able to find any documentation on what type of statistical procedure SPSS uses to fit an ARIMA model with covariates, so I'm not sure how I should interpret the coefficients or how concerned I should be with a flipped sign. Any help clarifying the modelling procedure used by SPSS would be tremendously appreciated. </p>
"
"0.0791187772129236","0.0705095093522208","180285","<p>I'm pretty new to the concepts of stationarity/cointegration. I am using the ""urca"" package in ""Rstudio"" to run my tests.</p>

<p>I have been trying to run cointegration tests, but the frustrating thing is that I haven't been able to find two series that are non-stationary, even when I try using examples cited by cointegration tutorials. My $p$-value is always too big such that I have to reject the null straight away. However, if I look at the $t$-values and compare them to the critical values, they seem to suggest otherwise. </p>

<p>Should I then ignore the $p$-value in the ADF test?
Here are my test results. My two price series are <code>XLE US Equity</code> and <code>CO1 Comdty</code> (Brent 1st futures) from 01/01/2010 - today (5/11/2015).</p>

<p>Any help/elaboration will be very much appreciated, thank you!</p>

<pre><code>&gt; testXLE&lt;-ur.df(XLE,type=""drift"",selectlags=""AIC"")
&gt; summary(testXLE)

############################################### 
# Augmented Dickey-Fuller Test Unit Root Test # 
############################################### 

Test regression drift 


Call:
lm(formula = z.diff ~ z.lag.1 + 1 + z.diff.lag)

Residuals:
     Min       1Q   Median       3Q      Max 
-10.3948  -2.5809   0.6846   2.7908  10.1940 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)  6.58864    3.43524   1.918   0.0596 .
z.lag.1     -0.08584    0.04533  -1.894   0.0628 .
z.diff.lag   0.05529    0.12544   0.441   0.6609  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 4.162 on 64 degrees of freedom
Multiple R-squared:  0.05337,   Adjusted R-squared:  0.02379 
F-statistic: 1.804 on 2 and 64 DF,  p-value: 0.1729


Value of test-statistic is: -1.8936 1.8395 

Critical values for test statistics: 
      1pct  5pct 10pct
tau2 -3.51 -2.89 -2.58
phi1  6.70  4.71  3.86
</code></pre>

<p>My interpretation of the results:</p>

<blockquote>
  <ul>
  <li>according to p-value (0.1729>0.05) do not reject null; series is stationary   </li>
  <li>t-value = (-1.8936>-2.89) --> do not reject null hypothesis; series is not stationary  </li>
  <li>t-value = (1.8395&lt;4.71) --> do not reject a0=0 --> there is no drift</li>
  </ul>
</blockquote>

<p>Conclusion: The series is non-stationary: Random Walk with no drift.</p>
"
"NaN","NaN","180337","<p>I always report odds ratios when using logistic regression for predictions. 
I wanted know is it meaningful to report odds ratios when modeling with gradient boosting approach? 
I am using gbm package in R to make the predictions.</p>

<p>Thanks!</p>
"
"0.100857047225074","0.102722675451665","180521","<p>I have a time series that includes some rare extreme values. We are talking about daily data, in total 1461 observations and 11 extreme values. I adjusted those 11 values with a multiple regression. Now I am using the <code>tbats()</code> on the original time series and the adjusted one. </p>

<pre><code>accuracy(original)
&gt;                   ME    RMSE      MAE MPE MAPE      MASE          ACF1
&gt;Training set 10.23539 4202.19 2921.593 NaN  Inf 0.6777689 -0.0003493096
accuracy(adjusted)
&gt;                   ME    RMSE      MAE MPE MAPE      MASE          ACF1
&gt;Training set 43.35625 3803.618 2787.39 NaN  Inf 0.6827622 -0.004749092

#original AIC
&gt;35101.43
#adjusted AIC
&gt;34798.24
</code></pre>

<p>How can I see if the model improves due to the adjustment or not? Since I reduced those 11 extreme values, I can't just compare MAE, RMSE or AIC. MASE is the only measure that should work?</p>

<p>I could divide MAE, RMSE and AIC by the mean of the respective time series.</p>

<pre><code># original
0.4962245 # MAE/mean(original)
0.7137304 # RMSE/mean(original)
5.96188 # AIC/mean(original)

# adjusted
0.4862567 # MAE/mean(adjusted)
0.6635364 # RMSE/mean(adjusted)
6.07051 # AIC/mean(adjusted)
</code></pre>

<p>Is that a legitimate way to compare the results?</p>

<p>Here are the <code>pacf</code>-diagrams of both models:</p>

<p><strong>original</strong>:</p>

<p><a href=""http://i.stack.imgur.com/nFARp.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/nFARp.png"" alt=""original""></a></p>

<p><strong>adjusted</strong>:</p>

<p><a href=""http://i.stack.imgur.com/YXIGF.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YXIGF.png"" alt=""adjusted""></a></p>

<p><strong>Update:</strong></p>

<p>I just realized that when i use the <code>accuracy()</code> function of the <code>forecast</code> package with a <code>tbats()</code> based on a <code>msts()</code> object the resulting MASE is using an in-sample naive forecast for scaling. I guess that is not optimal? It should be better to use an in-sample naive seasonal forecast with the longest season of the <code>msts()</code> object.</p>

<pre><code>MASE(original) # scaled with a in-sample naive seasonal forecast (365)
&gt; 0.6339

MASE(adjusted) # scaled with a in-sample naive seasonal forecast (365)
&gt; 0.6287
</code></pre>
"
"NaN","NaN","180580","<p>To understand my logistic regression fit and identify non linear effects, I plan to estimate the conditional density and then calculate the log odds comparing to log odds from logistic regression. To me  this is the equivalent of scatter plot of single  independent variable vs dependent and prediction. </p>

<p>A) Does this seem like the right approach? </p>

<p>B) I am using R, and I am surprised that there is no package already doing this? </p>
"
"0.10466430427046","0.0913717355809759","180854","<p>I am trying to test for cointegration between two series that based on qualitative reasoning, should be cointegrated. They are the prices of XLE ETF (<code>XLE US equity</code>) and 1st futures of Brent (<code>CO1 Comdty</code>). However, the results that I arrive at using two different methods both show that there exists no cointegration between the two series - not sure if my execution or the interpretation of the data is wrong? </p>

<p>(Both XLE and Brent 1st Futures have been tested for non-stationarity using ADF test from ""urca"" package)</p>

<p><strong>1st test - Engle Granger 2-step test:</strong><br>
In doing this, I referenced <em>Using R to Test Pairs of Securities for Cointegration</em> by Paul Teetor</p>

<p><strong>(1)</strong> Conducting Spread</p>

<pre><code>&gt; M&lt;-lm(XLE~Brent+0,data=XLE.Brent)
&gt; beta&lt;-coef(M)[1]
&gt; spread&lt;-XLE.Brent$XLE-beta*XLE.Brent$Brent
&gt; 
&gt; summary(M)

Call:
lm(formula = XLE ~ Brent + 0, data = XLE.Brent)

Residuals:
   Min      1Q  Median      3Q     Max 
-20.363  -9.543  -2.909  13.294  36.269 

Coefficients:
     Estimate Std. Error t value Pr(&gt;|t|)    
&gt;Brent  0.74962    0.02004    37.4   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 16.37 on 68 degrees of freedom
Multiple R-squared:  0.9536,    Adjusted R-squared:  0.953 
F-statistic:  1399 on 1 and 68 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p><strong>(2)</strong> Testing the stationarity of the spread using ADF test (from package ""urca""):</p>

<pre><code>&gt; spread.ADF&lt;-ur.df(spread,type=""none"",selectlags=""AIC"")
&gt; summary(spread.ADF)

############################################### 
# Augmented Dickey-Fuller Test Unit Root Test #  
</code></pre>

#########################################

<pre><code>Test regression none 


Call:
lm(formula = z.diff ~ z.lag.1 - 1 + z.diff.lag)

Residuals:
   Min      1Q  Median      3Q     Max 
 -6.1449 -2.2523  0.5559  2.9194  8.4567 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)
z.lag.1    -0.0003928  0.0266919  -0.015    0.988
z.diff.lag  0.1207084  0.1278700   0.944    0.349

Residual standard error: 3.443 on 65 degrees of freedom
Multiple R-squared:  0.01395,   Adjusted R-squared:  -0.01639 
F-statistic: 0.4596 on 2 and 65 DF,  p-value: 0.6335


Value of test-statistic is: -0.0147 

Critical values for test statistics: 
    1pct  5pct 10pct
tau1 -2.6 -1.95 -1.61
</code></pre>

<p>My interpretation: since $t$-value = <code>-0.0147</code> is bigger than <code>-1.61</code>, do not reject null. Spread is not stationary. Hence no cointegration between XLE and Brent.</p>

<p><strong>Second Test: Johansen Test</strong></p>

<pre><code>&gt; XLE.brent.coint&lt;-ca.jo(data.frame(XLE,Brent),type=""trace"",ecdet=""trend"",K=2,spec=""longrun"")
&gt; summary(XLE.brent.coint)
&gt;
&gt;###################### 
&gt;# Johansen-Procedure # 
&gt;###################### 
&gt;
&gt;Test type: trace statistic , with linear trend in cointegration 
&gt;
&gt;Eigenvalues (lambda):
&gt;[1] 8.179514e-02 6.025284e-02 2.775558e-17
&gt;
&gt;Values of teststatistic and critical values of test:
&gt;
&gt;        test 10pct  5pct  1pct
&gt;r &lt;= 1 | 4.16 10.49 12.25 16.26
&gt;r = 0  | 9.88 22.76 25.32 30.45
&gt;
&gt;Eigenvectors, normalised to first column:
&gt;(These are the cointegration relations)
&gt;
&gt;          XLE.l2   Brent.l2   trend.l2
&gt;XLE.l2   1.000000  1.0000000  1.0000000
&gt;Brent.l2 1.467806 -0.4346323  0.1610563
&gt;trend.l2 1.896366 -0.4903454 -0.8891875
&gt;
&gt;Weights W:
&gt;(This is the loading matrix)
&gt;
&gt;            XLE.l2    Brent.l2      trend.l2
&gt;XLE.d   -0.01629102 -0.13534537 -4.695795e-17
&gt;Brent.d -0.03819241 -0.03886418  5.127543e-17
</code></pre>

<p>My interpretation: Since $t$-value for <code>r=0: 9.88&lt;22.76</code>, do not reject null. Hence <code>r=0</code>, there exists no cointegration between <strong>XLE and Brent</strong>.</p>

<p>Additionally, I have carried out cointegration tests (both methods) on <strong>US 10 year and 2 year yields</strong>, and the results on both tell me that the series are not co-integrated, which does not make sense intuitively. Something must be wrong with the way I'm doing the tests!</p>

<p><a href=""http://i.stack.imgur.com/eApO4.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/eApO4.jpg"" alt=""enter image description here""></a>
<a href=""http://i.stack.imgur.com/sf2HV.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/sf2HV.jpg"" alt=""enter image description here""></a></p>
"
"0.027972711943223","0","180908","<p>I'm fitting an elastic net regression model.  I've attempted this via both the glmnet and caret packages.  I'm tuning for both alpha and lambda, so I'm leaning toward caret, as it offers a cleaner way to do this.  The problem I'm having is that I can't figure out how to tell caret to choose lambda.1se over lambda.min when tuning for lambda.  Is this possible or do I have to stick with glmnet if I want to use lambda.1se?</p>
"
"0.0791187772129236","0.080582296402538","180992","<p>I'm building a machine learning (random forest) regression model to predict flow in a river, using rainfall, relative humidity, air temperature and certain other climatic variables. Since flow on a particular day (<code>flow_t</code>) is highly correlated with flow on previous day (<code>flow_t_1</code>), I want to include lagged flow in the model formulation.</p>

<p>In case I build the model this way:</p>

<pre><code>require(randomForest)
flow.rf=randomForest(flow_t~flow_t_1+temp+humidity..........)
</code></pre>

<p>How can I use the above model for predictions? 
Since the input dataset for prediction will not have the flow variable, I cannot include its lagged version in the prediction call. I know that the <code>dynlm</code> package can be used to perform 'autoregressive distributed lag modeling' to include lagged dependent variables, but how can this be done for machine learning models? Or even for other statistical modeling techniques, like GLMs and GAMs?</p>
"
"0.100857047225074","0.0948209311861521","181665","<p>My question relates to calculating the uncertainty associated with the estimation of slopes in a varying intercept, varying slope hierarchical model.</p>

<p>I would like to calculate the effect of a treatment in different districts. If I ran a simple linear regression in which there was no pooling at district level, my model would look something like:</p>

<pre><code>    fm1 &lt;- lm(response ~ treatment*district)
</code></pre>

<p>I could then find the effect of treatment in district j by summing the coefficient for treatment with that of the interaction term treatment:districtj, and the standard error for that estimate could be found with:</p>

<pre><code>    sqrt(vcov(fm1)[2,2] + vcov(fm1)[4,4] + 2*vcov(fm1)[2,4])
</code></pre>

<p>I get stuck when moving to a multilevel model with random slopes along the lines of:</p>

<pre><code>    fm2 &lt;- lmer(response ~ treatment + (1 + treatment | district))
</code></pre>

<p>I can again find the effect of treatment in district j easily by summing the coefficient for treatment with the random slope estimate for district j. I would like to account for the uncertainty of both coefficients and I'm not sure how to do that. I can use the arm package to extract the standard errors associated with the slope estimates for each district and I know the standard error of the treatment estimate but I don't know how to estimate their correlation.  </p>

<p>It seems like this should be an easy question to answer but I haven't been able to find it. Hoping someone can point me in the right direction.</p>
"
"NaN","NaN","181843","<p>I'm a beginner in econometrics and have to interpret the graph I get after running an nonparametric regression using the package ""mgcv"" on RStudio (R). We have to use the function ""gam()"" for it. To begin with and for simplicity, we should regress on only one variable (price of cigarettes).</p>

<p>The formula was: </p>

<pre><code>model &lt;- gam(packs~ s(price)).
</code></pre>

<p>After plotting it I get this graph:<a href=""http://i.stack.imgur.com/Hcey2.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Hcey2.png"" alt=""enter image description here""></a></p>

<p>How should I interpret this graph?</p>
"
"0.0419590679148345","0.056980288229819","181862","<p>I'm trying to build robust linear regression model (lmrob from robustbase) using several (&lt; 15) features. I know that traditional stepwise algorithms aren't the best alternative since they are biased. Could I use this algorithm:</p>

<p>1) Add a variable to model. (If it is not significant on whole dataset should I remove it from the model?)</p>

<p>2) Run bootstrap to calculate list of RMSEs of different train and test sets for current model.</p>

<p>3) Compare medians of current and previous model using non-parametric test. (What is the best? Should I use corrections for multiple comparisons such as Holm method?)</p>

<p>I think the best way to select features is to run a LASSO regression but I don't know if robust LASSO regression exists... Is there a package in R for it?</p>

<p>Sorry, if questions are stupid - I'm newbie in statistics and data science</p>
"
"0.0884574820723792","0.0900937462695559","181980","<p>I would like to use a kernel matrix generated with a custom kernel function to fit a PLS-DA model (I am thinking of caret's PLS-DA at the moment), with only one binary response variable in the Y block. Before beginning, I am centering the kernel matrix on feature space with </p>

<p><a href=""http://i.stack.imgur.com/E7Hhd.gif"" rel=""nofollow""><img src=""http://i.stack.imgur.com/E7Hhd.gif"" alt=""enter image description here""></a></p>

<p>A few remarks:</p>

<ol>
<li>I see that caret's <code>plsda</code> function relies on the <code>pls</code> package functions <code>mvr</code> and <code>plsr</code>. When fitting a PLS-DA model, the method used to fit the model defaults to <code>kernelpls</code>, which is the version described on algorithm 1 on <em>Dayal, B. S. and MacGregor, J. F. (1997) Improved PLS algorithms. Journal of Chemometrics, 11, 73-85.</em> In this paper, they propose to compute a kernel matrix directly as <a href=""http://i.stack.imgur.com/Ajfv3.gif"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Ajfv3.gif"" alt=""enter image description here""></a> as part of the algorithm, and they rely directly on X as well during other steps. Therefore, it seems to me that using this method would mean to calculate a kernel matrix again over my kernel matrix.</li>
<li>I've seen three different methods in the literature that involve kernels and PLS. The first one is Dayal and MacGregor's kernel algorithm, the second one is <a href=""http://www.jmlr.org/papers/volume2/rosipal01a/rosipal01a.pdf"" rel=""nofollow"">K-PLS</a> (<em>Rosipal, Roman, and Leonard J. Trejo.""Kernel partial least squares regression in reproducing kernel hilbert space."" The Journal of Machine Learning Research 2 (2002): 97-123.</em>) and the third one is <a href=""http://homepages.rpi.edu/~bennek/papers/KB-ME-PLS.pdf"" rel=""nofollow"">DK-PLS</a> (direct kernel PLS). My understanding is that K-PLS is just a modification of the NIPALS algorithm (oscorespls fitting method in the <code>pls</code> package) to use a kernel matrix, and therefore I suspect that this might be the one I should be using. DK-PLS seems to use a kernel matrix as input as well.</li>
</ol>

<p>In short, I guess my question can be summarized as: Which method should I use to fit a PLS-DA model for a binary response, with a custom kernel matrix as input data? Any insights would be appreciated!</p>
"
"0.0484501583111509","0.0493463771219827","182071","<p>I'm analyzing presence/absence data using Rmark occupancy analysis (i.e. Rmark is the package which runs Program MARK in R). This is for a BACI (before-after-control-impact) design and I want to answer the question of ""is the amount of change between before and after phases significantly different for the impact site as compared with the control site?"".  I'm able to do this using regression for the count data, but am not sure how to calculate lsmeans to use in the BACI contrast using the occupancy results. </p>

<p>Here is an example: </p>

<p>Analysis of the count data using zero-inflated poisson regression: 
(ZP = zone/phase combination: zones are control/impact, phases are before/after). Note: extra script from this answer was needed to run lsmeans on the zerofinl object (<a href=""http://stats.stackexchange.com/questions/181333/zero-inflated-poisson-regression-how-can-i-calculate-contrasts-for-baci-before/181502?noredirect=1#comment345516_181502"">see this answer for more details</a>).</p>

<pre><code>summary(m2 &lt;- zeroinfl(Opossum ~ ZP|ZP, data = bact))
result.lsmo.SP2 &lt;- lsmeans::lsmeans(m2, ~ZP)
contrast(result.lsmo.SP2, list(bact=c(1, -1, -1, 1)))
confint(contrast(result.lsmo.SP2, list(bact=c(1, -1, -1, 1))))
</code></pre>

<p>Analysis of the presence/absence data using occupancy analysis in Rmark: </p>

<pre><code>BEAR.models=mark(BEAR, model=""Occupancy"", group=c(""Zone"", ""Phase""), model.parameters=list(p=list(formula=~WeekDay+DetectionDist+Trail),Psi=list(formula=~Zone+Phase)), invisible=FALSE)
</code></pre>

<p>Any ideas of how I can calculate lsmeans from the occupancy results (which includes occupancy estimate (psi), standard error, and upper/lower 95% confidence interval limits)? Thanks!</p>
"
"0.0395593886064618","0.040291148201269","182079","<p>I am running a meta-analysis using <strong>metafor</strong> R package. I am comparing studies on a continuous variable, that can be synthetized by the mean. Such studies can be grouped in three blocks. I used a meta regression, e.g. </p>

<pre><code>dat &lt;- escalc(measure=""MN"", mi=mean, sdi=sd, ni=num, data=dbtemp)
res &lt;- rma(yi, vi, mods =~ factor(group), data=dat)
</code></pre>

<p>I know that I can know whether group is significant or not. But how can I assess whether each level within group is significant from the other. I.e., how I can perform multiple comparisons (post hoc analysis) within a meta - analysis context?</p>
"
"NaN","NaN","182467","<p>I'm running a logistic regression (presence/absence response) in R, using glmer (lme4 package). Ben Bolker'sÂ <a href=""http://glmm.wikidot.com/faq"" rel=""nofollow"">overdisp_fun</a> (see link) tells me my model is overdispersed, so I decided to include an individual-level random effect. This is not solving my problem, as I get convergence issues and overdispersion is not reduced. Could anyone recommend an alternative?</p>

<p>Thanks!! </p>
"
"0.0559454238864459","0.056980288229819","182595","<p>I'm trying to use sparse linear model for my data,input x(29*50),output y(29*1). In R, the package of <strong><em>glmnet</em></strong> can be used. </p>

<p>Firstly, cv.glmnet() choose lambda and coefficients(at min error), here with leave-one-out cv method,and then plot it. </p>

<pre><code>cv.fit = cv.glmnet(x,y,family=""gaussian"",nfolds=29)

plot(cv.fit)
</code></pre>

<p><a href=""http://i.stack.imgur.com/PEEeb.png"" rel=""nofollow"">the plot of mse aganist log(lambda) in cv model</a></p>

<p>Next, print the coefficients</p>

<pre><code>coef(cv.fit,s=""lambda.min"")
</code></pre>

<blockquote>
  <p>51 x 1 sparse Matrix of class ""dgCMatrix""               </p>

<pre><code>              1
</code></pre>
  
  <p>(Intercept)   267.7241</p>
  
  <p>cluster_0  .<br>
  cluster_1     .<br>
  cluster_2     .<br>
  cluster_3     .<br>
  cluster_4     .<br>
  ...</p>
  
  <p>cluster_47    .<br>
  cluster_48    .<br>
  cluster_49    .  </p>
</blockquote>

<p>Finally, to measure the model's ability for prediction, accuracy is calculated(defined as 1 minus average absolute error divided by numeric range of y)</p>

<pre><code>py &lt;- predict(cv.fit,newx=x,s=""lambda.min"")
py
</code></pre>

<blockquote>
  <p>V1     267.7241</p>
  
  <p>V2     267.7241</p>
  
  <p>...</p>
  
  <p>v29    267.7241  </p>
</blockquote>

<pre><code>ave_abs_error &lt;- mean(abs(py-y))
n_range &lt;- max(y)-min(y)
acc &lt;- 1-ave_abs_error/n_range
acc
</code></pre>

<blockquote>
  <blockquote>
    <p>0.918365</p>
  </blockquote>
</blockquote>

<p>Although the acc(0.918365) is very high, there is a serious problem. As seen from the plot above, the lambda.min is very large(73.03439),and all coefficients  are zero(only with intercept value 267.7241), all predicted py are the same as intercept.
That's really weird! </p>

<p>I searched lots of threads in forum, here<a href=""http://stats.stackexchange.com/questions/72251/an-example-lasso-regression-using-glmnet-for-binary-outcome"">http://stats.stackexchange.com/questions/72251/an-example-lasso-regression-using-glmnet-for-binary-outcome</a> explains that there is no local min for too few observations and all coefficients were shrunk to zero with the shrinkage penalties.</p>

<p>Does anybody has other interpretations?</p>

<p>Thanks in advance!</p>
"
"0.0685188709827532","0.0697863157798853","182656","<p>I have data in longitudinal or clustered format (please see the example below). My response variable is dichotomous. I want to examine which factors explains why a subject in the dataset gets Y=1. In the example below I show only one predictor â€“ X.</p>

<p>Since I have a dichotomous response variable, I am thinking of logistic regression. However, the longitudinal format violates the distributional assumption of ML-theory. <strong>So my question is which logistic model would be appropriate here?</strong> And if possible, which R-package would be relevant (if not covered by the standard stats)? </p>

<p>All subjects are countries and observed (let's say) from 1990-1994. A country can get more than 1 Y per year, from different Z's.  I have been thinking of logistic panel models. Although I am not sure which specific model would be appropriate (assuming that panel models are more appropriate group of models). Perhaps random effects as each observation is not of the same nature (a country can get Y=1 from different groups, see variable Z). The Z variable is not a part of analysis though. Grateful for all suggestions!</p>

<pre><code>COUNTRY      YEAR  Y           X        Z

    A        1990  0           0        K
    A        1991  1           0        K
    A        1992  0           0        K
    A        1993  1           0        L
    B        1994  0           1        L
    B        1990  0           1        L
    B        1991  0           1        L
    B        1992  1           1        L
    C        1990  1           0        K
    C        1991  1           0        K
    C        1992  0           0        L
    C        1993  0           1        K
    C        1994  0           1        L
    D        1990  0           1        L
    D        1991  0           0        K
    D        1992  0           0        K
    D        1993  0           1        K
    D        1994  0           1        K
</code></pre>
"
"0.0559454238864459","0.0427352161723642","182689","<p>My name is Ashley. I'm working on the analyses for my dissertation which involves a meta-analysis of 4 predictors, 1 mediator, and one outcome. So far, I've calculated the meta-analytic correlation matrix between predictors and outcome(s) and harmonic mean of N to run analyses on the model level. 
Out of two major studies published in my area of the social sciences (DeChurch &amp; Mesmer-Magnus, 2010; Joseph et al., 2015), this is all of the information provided for running meta-regression with more than one predictor. No programs are specified and no other matrices are indicated. </p>

<p>I'm having trouble identifying how to run the multivariate metaregression analysis. From what I've found so far, mvmeta package in R is the closest that I've come to identifying a program/package that will produce the estimates that I need. However, I would have to calculate corrected rho for each study/predictor-outcome relationship individually. Also, there is no place to indicate harmonic mean of N. And because of this, I'm skeptical in using this package. </p>

<p>Is anyone aware of another R package, SPSS macro or other statistical software program/package that can handle this type of analysis? Or is mvmeta the best bet?</p>
"
"0.027972711943223","0.0284901441149095","183052","<p>I am building a regression tree in R, using the 'tree' package. Up until now, I only have worked with classification trees, so, I have relied on the misclassification error rate to judge how good my classification tree is at fitting the data. However, for my regression tree, I am only given ""residual mean deviance"". So, for example, for one of my trees I get: </p>

<p>Residual mean deviance:  2.687 = 1247 / 464 </p>

<p>Can someone please explain what this means?</p>
"
"0.0500391083360534","0.0637058989297032","183153","<p>I'm working on lasso as an alternative to step-wise forward/backward regression using the <code>lars</code> package in R. I normalized my variables, calculated the <code>lars</code> and did a CV for figuring out the most suitable fraction s. After that I used that s-value for predicting my model using <code>predict.lars</code> and identify those variables to use and to neglect in my regression equation.
No I got stucked in the further evaluation of my work. What needs to be done next to evaluate the fitting of my model. Is there a kind of p-value and R-square to use as parameter for valuation?
I would like to use the regression equation for interpolation my Y using the variables to a map and need to evaluate the regression.</p>

<p>I know that there are many topics covering my question at a certain point but after browsing the website and literature (Elements of Statistical Learning, An introduction to statistical learning, regression shrinkage and selection via lasso, a significance test for the lasso) I came not up with a specific result. </p>
"
"0.0559454238864459","0.056980288229819","183206","<p>Simply put, I'd like to know how the plm package in R calculates the residuals of a random-effect regression.</p>

<p>I ask this because i'm getting some ""weird"" outputs. Let-me reproduce them here using the Grunfeld data for four firms, like Gujarati in his Basic Econometrics do:</p>

<pre><code>require(plm)
require(foreign)

Grunfeld&lt;-read.dta(""Data.dta"")
Grunfeld&lt;-pdata.frame(Grunfeld,index = c(""id"",""t""))

grun.re &lt;- plm(Y~X2+X3,data=Grunfeld,model=""random"",index=""id"")

#Means by id
X2M&lt;-tapply(Grunfeld$X2,Grunfeld$id,FUN = mean)
X3M&lt;-tapply(Grunfeld$X3,Grunfeld$id,FUN = mean)
YM&lt;-tapply(Grunfeld$Y,Grunfeld$id,FUN = mean)

#Random Effect: Fit the model and the calculate residuals ""by hand""
fit.re&lt;-grun.re$coefficients[1]+grun.re$coefficients[2]*Grunfeld$X2+grun.re$coefficients[3]*Grunfeld$X3
    calcResid.re&lt;-(Grunfeld$Y-fit.re)

#Random Effect:
head(cbind(grun.re$residuals,Grunfeld[,11:13],calcResid.re))

  grun.re$residuals   alphaRE       eRE        uRE calcResid.re
1         99.395803 -169.9282 116.23154  -53.69666    -53.69666
2         18.023715 -169.9282  34.85946 -135.06874   -135.06874
3        -39.256625 -169.9282 -22.42089 -192.34909   -192.34908
4         -2.857048 -169.9282  13.97869 -155.94951   -155.94951
5        -28.334107 -169.9282 -11.49837 -181.42656   -181.42656
6          6.475226 -169.9282  23.31096 -146.61723   -146.61723
</code></pre>

<p>In this table, uRE is the overall residual of the regression provided by Stata (which is identical to Gretl's) and calcResid.re is the manually calculated residuals from the fitted model. So, Stata, Gretl and I did the same. But what plm package do?</p>

<p>We can se that calcResid.re and uRE are equals. But the residuals provided by the plm estimation (grun.re$residuals) completely differs.</p>

<p>Here is a link to the dataset and results: <a href=""https://www.dropbox.com/s/v6uex5ziee8xroj/Data.dta?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/v6uex5ziee8xroj/Data.dta?dl=0</a></p>
"
"NaN","NaN","183534","<p>I am using function <code>neuralnet</code> in the package <code>neuralnet</code> to build the neural network, and I see the error:</p>

<p><code>algorithm did not converge in 1 of 1 repetition(s) within the stepmax</code></p>

<p>The neural network has 20 inputs and 1 output. The problem is, with the same data and same set of inputs, I ran linear regression or random forest without any problem. So what should I look to for debugging my problem?</p>
"
"0.0839181358296689","0.0854704323447285","183603","<p>Please apologize for this potentially ""stupid"" question. But I am currently attempting to test a mediation in for a multilevel dataset. Unfortunately, the residuals of the regressions do not follow a Normal distribution and they do not have a constant variance. Ideally, I would thus use bootstrapping to obtain confidence intervals. However, the Mediation package in R does not provide this function for multilevel datasets. Instead, it calculates Quasi-Bayesian intervals in this case.</p>

<p>My question is:
<strong>Can I use Quasi-Bayesian Confidence intervals, if I am aware the residuals do not follow a Normal distribution and that they are heteroskedastic? If not, which package/functions could I use instead?</strong></p>

<p>Here is the code I have used. Unfortunately I cannot share a sample of my data, since it is confidential.</p>

<pre><code>&gt; med.fit &lt;- lmer(OTIF ~  LDLV + COLT + Slack2 + (1 | BU), data = Data_P5)
&gt; out.fit &lt;- lmer(EBIT ~ OTIF + Slack2 + LDLV + COLT  + (1 | BU), data = Data_P5)
&gt; med.out &lt;- mediate(med.fit, out.fit, treat = ""Slack2"", mediator = ""OTIF"", sims = 100)
&gt; summary(med.out)

Causal Mediation Analysis 
Quasi-Bayesian Confidence Intervals
Mediator Groups: BU 
Outcome Groups: BU 

Output Based on Overall Averages Across Groups 

            Estimate 95% CI Lower 95% CI Upper p-value
ACME            5.12e-03     1.49e-03     1.06e-02    0.00
ADE            -5.64e-03    -1.77e-02     8.46e-03    0.50
Total Effect   -5.24e-04    -1.38e-02     1.41e-02    0.86
Prop. Mediated -3.21e-01    -1.34e+01     6.01e+00    0.86

Sample Size Used: 167 


Simulations: 100 
</code></pre>

<p>Since bootstrapping is not available for multilevel models I get an error message:</p>

<pre><code>&gt; med.fit &lt;- lmer(OTIF ~  LDLV + COLT + Slack2 + (1 | BU), data = Data_P5)
&gt; out.fit &lt;- lmer(EBIT ~ OTIF + Slack2 + LDLV + COLT  + (1 | BU), data = Data_P5)
&gt; med.out &lt;- mediate(med.fit, out.fit, treat = ""Slack2"", mediator = ""OTIF"", sims = 100, 
+ boot = TRUE)
Error in mediate(med.fit, out.fit, treat = ""Slack2"", mediator = ""OTIF"",  : 
'boot' must be 'FALSE' for models used
</code></pre>
"
"0.0559454238864459","0.056980288229819","183846","<p>I created a SEM model in R (lavaan package), but one of myÂ dependent variables is continuous, while the other is binary.</p>

<p>The model isÂ as follows:</p>

<pre><code>modelx &lt;- '
a =~ a1Â + a2Â + a3

bÂ =~ b1Â + b2 +Â b3

c =~Â c1 + c2 + c3

x ~ a + b + c + zÂ +Â w

y ~ a + b + cÂ + zÂ +Â w

'

sem(modelx, data=mydata, estimator=""DWLS"")
</code></pre>

<p>zÂ and wÂ are covariates. x is a scale (0-12), however y is a binary variable (0;1).Â </p>

<p>Thus, the question is, how can I implement a linear regression for predicting x and a logistic regression for y in a single SEM model? I assume that lavaan doesn't automatically account for the different dependent variables. If it does, I would like to see a reference for that. All ideas are welcome.</p>

<p>Edit: Using the latent variable factor scores from the measurement model for a, b, c in a glm (binomial reg for y and linear for x) and lavaan, the results are more closely aligned for x than for y. Does it mean that lavaan ignores/doesn't do good with the dichotomous variable in this particular case, or my question from the start is moot or unnecessary?</p>
"
"0.0395593886064618","0.040291148201269","183976","<p>I created a SEM model in R (<code>lavaan</code> package), but one of my dependent variables is continuous, while the other is binary.</p>

<p>The model is as follows:</p>

<pre><code>modelx &lt;- '
a =~ a1 + a2 + a3
b =~ b1 + b2 + b3
c =~ c1 + c2 + c3
x ~ a + b + c + z + w

y ~ a + b + c + z + w
'
sem(modelx, data=mydata, estimator=""DWLS"")
</code></pre>

<p><code>z</code> and <code>w</code> are covariates. <code>x</code> is a scale (0-12), however <code>y</code> is a binary variable (0;1). </p>

<p>Thus, the question is, how can I implement a linear regression for predicting x and a logistic regression for y in a single SEM model? I assume that lavaan doesn't automatically account for the different dependent variables. If it does, I would like to see a reference for that.</p>
"
"NaN","NaN","184327","<p>The covTest package in R gives significance values for a LASSO regression. How should the results be interpreted? I get negative predictor numbers and NAs for the p-values. What do these mean? More specifically, it might be assumed that the predictor number is an index for the external regressors, but how can this be negative?</p>

<p>Here is an example:</p>

<pre><code> significance_pars &lt;- covTest(lars_output,x,y, sigma.est = ""full"")
</code></pre>

<p>When calling significance_pars$results, the output is:</p>

<pre><code> Predictor_Number Drop_in_covariance P-value
          118          8830.0759  0.0000
          105          1557.1987  0.0000
          104           755.5005  0.0000
          119           833.6093  0.0000
           46           183.8750  0.0000
           45             3.3674  0.0345
          103            56.8601  0.0000
           60           214.5187  0.0000
           44             0.4365  0.6463
          113           248.4377  0.0000
          120           220.6486  0.0000
          124             1.2863  0.2763
         -118                 NA      NA
</code></pre>
"
"0.0197796943032309","0.040291148201269","184391","<p>I am attempting to perform a piecewise/segmented logistic regression on survey data using  <a href=""http://www.asdfree.com/2015/11/statistically-significant-trends-with.html"" rel=""nofollow"">this tutorial</a> as my basis. I have data for the period 2006 to 2013, however 2012 is missing.</p>

<p>The analysis proceeds as expected until the point in step 8 where I add the segmented variable with one breakpoint (the final line of code in the example below).</p>

<pre><code>library(segmented)
df &lt;- data.frame(yr=c(2006:2011,2013),
             mean= c(0.11290830, 0.12814364, 0.11149552, 0.12071058, 0.11776731, 0.10363014, 0.09888132),
             wgt = c(602.2272, 546.2958, 594.1818, 756.0167, 579.1533, 481.9694, 654.3281))
o &lt;- lm( log( mean ) ~ yr , weights = wgt , data = df )
os &lt;- segmented( o , ~yr)
</code></pre>

<p>At this point I get the error message:</p>

<blockquote>
  <p>""Error in segmented.lm(o, ~yr) : only 1 datum in an interval: breakpoint(s) at the boundary or too close each other""</p>
</blockquote>

<p>From my reading, in particular <a href=""http://r.789695.n4.nabble.com/Estimating-and-predicting-using-quot-segmented-quot-Package-td4682541.html"" rel=""nofollow"">here</a>, this is because the breakpoint falls at 2007, thus leaving 2006 on it's own and unable to have a slope calculated for it. I understand that this is likely because I have so few data points.</p>

<p>Does anyone have any tips for getting around this or another package / technique that would be more appropriate? The second link suggests using additional dummy data but I'm a bit wary of this approach.</p>
"
"NaN","NaN","184420","<p>I am trying to do local polynomial regression in R. The model I wish to fit has a single explanatory variable and a single response, it should have a bandwidth of 5, and a polynomial degree of 3. </p>

<p>I have looked through the reference manuals for packages: <code>np</code>, <code>kernsmooth</code>, and <code>locpol</code>. None of them seem to allow for this setup? Can someone suggest how to do this?</p>
"
"0.0625488854200668","0.0637058989297032","184713","<p>I am fairly new to R. I have attempted to read up on time series analysis and have already finished </p>

<ol>
<li>Shumway and Stoffer's <a href=""http://www.stat.pitt.edu/stoffer/tsa3/"" rel=""nofollow"">Time series analysis and its applications 3rd Edition</a>,</li>
<li>Hyndman's excellent <a href=""https://www.otexts.org/fpp"" rel=""nofollow"">Forecasting: principles and practice</a></li>
<li>Avril Coghlan's <a href=""http://a-little-book-of-r-for-time-series.readthedocs.org/en/latest/src/timeseries.html"" rel=""nofollow"">Using R for Time Series Analysis</a></li>
<li>A. Ian McLeod et al <a href=""http://www.stats.uwo.ca/faculty/aim/tsar/tsar.pdf"" rel=""nofollow"">Time Series Analysis with R</a></li>
<li>Dr. Marcel Dettling's <a href=""https://stat.ethz.ch/education/semesters/ss2013/atsa/ATSA-Scriptum-SS2013_130218.pdf"" rel=""nofollow"">Applied Time Series Analysis</a></li>
</ol>

<p>Edit: I'm not sure how to handle this but I found a usefull resource outside of Cross Validated. I wanted to include it here in case anyone stumbles upon this question. </p>

<p><a href=""http://isites.harvard.edu/fs/docs/icb.topic79832.files/L06_Program_Evaluation_2/Segmented_Regression.Wagner.2002.pdf"" rel=""nofollow"">Segmented regression analysis of interrupted time series studies in medication use research</a></p>

<p>I have a univariate time series of the number of items consumed (count data) measured daily for 7 years. An intervention was applied to the study population at roughly the middle of the time series. This intervention is not expected to produce an immediate effect and the timing of the onset of effect is essentially unknowable.</p>

<p>Using Hyndman's <code>forecast</code> package I have fitted an ARIMA model to the pre-intervention data using <code>auto.arima()</code>. But I am unsure of how to use this fit to answer whether there has been a statistically significant change in trend and quantify the amount.</p>

<pre><code># for simplification I will aggregate to monthly counts
# I can later generalize any teachings the community supplies
count &lt;- c(2464, 2683, 2426, 2258, 1950, 1548, 1108,  991, 1616, 1809, 1688, 2168, 2226, 2379, 2211, 1925, 1998, 1740, 1305,  924, 1487, 1792, 1485, 1701, 1962, 2896, 2862, 2051, 1776, 1358, 1110,  939, 1446, 1550, 1809, 2370, 2401, 2641, 2301, 1902, 2056, 1798, 1198,  994, 1507, 1604, 1761, 2080, 2069, 2279, 2290, 1758, 1850, 1598, 1032,  916, 1428, 1708, 2067, 2626, 2194, 2046, 1905, 1712, 1672, 1473, 1052,  874, 1358, 1694, 1875, 2220, 2141, 2129, 1920, 1595, 1445, 1308, 1039,  828, 1724, 2045, 1715, 1840)
# for explanatory purposes
# month &lt;- rep(month.name, 7)
# year &lt;- 1999:2005
ts &lt;- ts(count, start(1999, 1))
train_month &lt;- window(ts, start=c(1999,1), end = c(2001,1))
require(forecast)
arima_train &lt;- auto.arima(train_month)
fit_month &lt;- Arima(train_month, order = c(2,0,0), seasonal = c(1,1,0), lambda = 0)
plot(forecast(fit_month, 36)); lines(ts, col=""red"")
</code></pre>

<p>Are there any resources specifically dealing with interrupted time series analysis in R? I have found <a href=""http://epoc.cochrane.org/sites/epoc.cochrane.org/files/uploads/21%20Interrupted%20time%20series%20analyses%202013%2008%2012_1.pdf"" rel=""nofollow"">this</a> dealing with ITS in SPSS but I have not been able to translate this to R. </p>
"
"0.0625488854200668","0.0637058989297032","184795","<p>I have a number (48) bivariate relationships (N = 10 for each) where I want to fit a linear model and estimate the confidence interval (CI) using bootstrapping. </p>

<p>What I want to present, is the slope and CI for this regression. However, instead of picking one CI, I'd rather present the distribution of the slope estimates so the reader can judge for himself. What I thought about doing, is to present the histogram of the bootstrapped slope estimates along with the information about how many % of the estimates where > 0.</p>

<p>Is that a valid and/or good way to present the data? And is it valid to say that if 97.7 % of the slope estimates are > 0, the slope is significant with alpha = 0.023?</p>

<p>here is an example of what I mean</p>

<p><a href=""http://i.stack.imgur.com/33AxH.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/33AxH.png"" alt=""scatterplots""></a></p>

<p><a href=""http://i.stack.imgur.com/FMaFT.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FMaFT.png"" alt=""bootstrapped slope estimates, 10000 draws""></a></p>

<p>another way to formulate the question would be: prior to going this way, I calculated bootstrapped CI with the <code>boot.ci</code> function in in the <code>boot</code> package in <code>R</code>. However, the CI seem to be wider that what is suggested by the histograms. How exactly are bootstrapped CI calculated and is it wrong to assume that it should span 95% of the bootstrapped slope estimates? </p>
"
"0.027972711943223","0.0284901441149095","185166","<p>I thought I've understood the output of the logistic regression in R (also I learned a lot through stackexchange), but somehow my vizualization tells me something different.
The output of the glm in R is: </p>

<pre><code>Call:
glm(formula = Zustand ~ Temp + Tag + Gehege + Temp:Gehege + Tag:Gehege + 
    Individuum + Gehege:Individuum + Temp:Individuum + Tag:Individuum, 
    family = binomial)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.9257  -0.5462  -0.4408  -0.3106   2.8510  

Coefficients:
                               Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                   -3.124244   1.302784  -2.398 0.016479 *  
Temp                           0.083585   0.068098   1.227 0.219664    
Tag                           -0.005485   0.013584  -0.404 0.686364    
Gehegeneues                    3.646637   1.249182   2.919 0.003509 ** 
IndividuumJoachim             -0.769787   0.927953  -0.830 0.406791    
IndividuumMary                -0.420745   0.797966  -0.527 0.598005    
Temp:Gehegeneues              -0.169516   0.062047  -2.732 0.006294 ** 
Tag:Gehegeneues               -0.057955   0.017154  -3.379 0.000729 ***
Gehegeneues:IndividuumJoachim  0.728588   0.329791   2.209 0.027158 *  
Gehegeneues:IndividuumMary     0.951688   0.396865   2.398 0.016484 *  
Temp:IndividuumJoachim         0.015466   0.049143   0.315 0.752979    
Temp:IndividuumMary           -0.012944   0.044467  -0.291 0.770985    
Tag:IndividuumJoachim         -0.035718   0.019177  -1.863 0.062532 .  
Tag:IndividuumMary             0.016538   0.019597   0.844 0.398724  
</code></pre>

<p>But my vizualization with the visreg-package...</p>

<pre><code>visreg(Ergebnis3, ""Gehege"", by = ""Individuum"", type = ""contrast"", scale = 'response', rug = F, ylim = c(0.0,0.6), main = ""Preening / Moulting"", xlab = ""Enclosure"", ylab = ""Likelihood"")
</code></pre>

<p>...looks like this:</p>

<p><a href=""http://i.stack.imgur.com/YxQc4.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YxQc4.jpg"" alt=""enter image description here""></a></p>

<p>""Individuum = Georg"" and ""Gehege = altes"" is my reference level and I thought the coefficient of ""Gehege = neues"" (3.64) means that the probability of ""Zustand"" in ""Gehege = neues"" is 3.64 times higher than in ""Gehege = altes"" or not? But in the graphic it's lower. Also for changing continuous variables ""Temp"" and ""Tag"" (here it's shown i.e. for 19.5 Â°C) ""Gehege = neues"" keeps to be lower for most of the times. The odds Ratio for ""Gehege = neues"" are also about 38, this high number is a bit weird... </p>

<p>I hope this is enough information to help, I have a real complex question I want to answer.</p>
"
"0.0884574820723792","0.0900937462695559","185449","<p>I've implemented a comparison between the performance of 80%-forecast intervals is in the forecast package - see 1st part of the code below providing a number of hits
This number states, how many times the forcast interval was right for the left-out data entries. Btw regarding variable names: the German ""preis"" means ""price"" and ""absatz"" means ""sales"", i.e.
""preise"" means ""prices"" and ""absaetze"" is the plural for ""sales"".</p>

<p>So, I compared the formula-based prediction interval to what I think bootstrapping is - see 2nd part of the code. But the number of hits in the 2nd case by no means resembles the 80% of the first case.
The following actions did not help to reproduce the 80% : using less data in the given data frame, using median formulas for bootstrapping instead of the upper/lower computation in the loop,
more samples resampling in the resampling.</p>

<p>I cannot imagine the bootstrapping approach performing so bad - what did I do wrong?  </p>

<pre><code>#given

# data frame

preis&lt;-c(1:100)
absatz&lt;-(-2*preis)+1000+rnorm(100)


jeansData&lt;-data.frame(absaetze=absatz,preise=preis)

#### implementation ###


#leave-one-out cross-validation for formula, i.e. with the borders         given     above  
###### (1ST PART) ########

numberOfHits&lt;-0

for(i in (1:100)){

preisCandidateToBeChecked&lt;-preis[i]
absatzCandidateToBeChecked&lt;-absatz[i]

absatzWithoutCandidate&lt;-absatz[-i]
preisWithoutCandidate&lt;-preis[-i]

jeansData&lt;-data.frame            (absaetze=absatzWithoutCandidate,preise=preisWithoutCandidate)
fit&lt;-lm((absaetze~preise), data=jeansData)

#check, if in interval and count as hit, if value is in interval

if(absatzCandidateToBeChecked &lt;= (forecast(fit,     newdata=preisCandidateToBeChecked)$upper[1]) &amp; (absatzCandidateToBeChecked &gt;= (forecast(fit, newdata=preisCandidateToBeChecked)$lower[1])) )
{numberOfHits&lt;-numberOfHits+1}

}

#execute code until here and inspect numberOfHits; the hit rate pretty much resembles the 80% assumed

#then execute the rest

#leave-one-ot cross-validation for bootstrapping (not using the bootstrap function)  ###### (2ND PART) ########


numberOfHits&lt;-0

for(i in (1:100)){

preisCandidateToBeChecked&lt;-preis[i]
absatzCandidateToBeChecked&lt;-absatz[i]

absatzWithoutCandidate&lt;-absatz[-i]
preisWithoutCandidate&lt;-preis[-i]

jeansData&lt;-data.frame(absaetze=absatzWithoutCandidate,preise=preisWithoutCandidate)

#ten or hundred or thousand regressions by bootstrapping

allPredictions&lt;-c()

for(j in (1:10)){

fit&lt;-lm((absaetze~preise), data=jeansData[sample(nrow(jeansData),10,replace=TRUE),])

allPredictions&lt;-c(allPredictions,forecast(fit,     newdata=preisCandidateToBeChecked)$mean)

}

#build and name bootstrapped forecast interval from regressions

upper&lt;-sort(allPredictions)[9]
lower&lt;-sort(allPredictions)[2]

if((absatzCandidateToBeChecked &lt;= upper) &amp; (absatzCandidateToBeChecked &gt;= lower) )
{numberOfHits&lt;-numberOfHits+1}

} #inspect numberOfHitsAgain - it's around 40%. What is foul here?!
</code></pre>
"
"0.0500391083360534","0.0637058989297032","185800","<p>I try to find a model using logistic regression. More precisely, what I did so far, is using stepwise regression and subset selection (although I know, it is often a bad idea) to find the ""best"" model. Clearly, depending on the information criteria I used, I got different results. </p>

<p>Now, I found an interesting example on page 250 in the book <a href=""http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf"" rel=""nofollow"">""An Introduction to Statistical Learning""</a>. They chose among the models of different sizes using cross-validation, that is they make predictions for each model and compute the test errors. Eventually, the compute the cross validation error and choose the model corresponding to the minimal average cross-validation error. </p>

<p>However, the function <code>regsubsets</code> of the R package ""leaps"" is only working for linear models. How can I implement this for logistic regression or glm models in general? </p>

<p>My idea was, to just estimate the models within a cross-validation using the <code>step</code> function of the ""stats"" package and then kind of take the average number of features (which is determined by minimum AIC, for example). Is this a legitimate approach?</p>
"
"0.0625488854200668","0.0637058989297032","186240","<p>Hi I am trying a mediation analysis (using library(""mediation"") in R)</p>

<p>My model has 3 predictors and one mediator (n=455), but I am only interested in predictor 1. There is some collinerarity between predictor 1 and 2 - 0.383444 (Pearson). No collinerarity between predictor 3 and the others. The Mediator is correlated with IV1 and slightly with IV2. Predictors, Mediator and dependent variable are all continuous.</p>

<pre><code>lm(DV ~ IV1 + IV2 + IV3 , data = data)
</code></pre>

<p>Only IV2 is significant, R2 = 0.050</p>

<pre><code>lm(DV ~ Mediator + IV1 + IV2 + IV3 , data = data)
</code></pre>

<p>Mediator and IV2 is significant, R2 = 0.056</p>

<p>I have a much bigger dataset with n = 1200, but unfortunately I don't have Mediator information available for them. If I do a linear regression to predict DV with this dataset, IV1 and IV2 are both highly significant, the standardized beta meaningful.</p>

<ol>
<li><p>With this information can I investigate the mediating effect of the mediator on IV1 with my small dataset with 455 subjects (using the mediate()-Function of the ""mediation""-package in R) , even though the dataset itself is too small to show a significant effect of IV1 on the DV?</p></li>
<li><p>Also, I was wondering whether my mediator might mediate IV2-effect. The correlation between IV1 and the mediator is higher than between IV2 and the mediator though. </p></li>
</ol>

<p>I am thankful for any ideas.</p>
"
"0.118842882298127","0.121041207966436","186393","<p>I built a multivariate regression tree using the <code>party</code> package in R. The depth of the tree (max. number of splits) is 13. For the first 3/4 splits the tree is relatively easy to interpret which is useful in our case. However with an increase in the number of splits interpretation becomes impossible. The idea is to get a measure of variable importance from this tree, similar to the idea of variable importance in random forests. For random forests there is function <code>varimp</code> but for regression trees it does not seem to exist. I'm aware of the <code>caret</code> package but it is built for CART of the <code>rpart</code> package.</p>

<p>Now, I have an idea of how to measure variable importance in CART but i'm a little lost on how to implement it using the <code>party</code>/<code>partykit</code> package. From <em>Ishwaran (2007)</em>:</p>

<blockquote>
  <p>We define the VIMP for a variable x<sub>v</sub> as the difference between prediction error when x<sub>v</sub> is â€œnoised upâ€ versus the prediction error otherwise. To noise up x<sub>v</sub> we adopt the following convention. To assign a terminal value to a case x, drop x down T [which is your tree] and follow its path until either a terminal node is reached or a node with a split depending upon x<sub>v</sub> is reached. In the latter case choose the right or left daughter of the node with equal probability. Now continue down the tree, randomly choosing right and left daughter nodes whenever a split is encountered (whether the split depends upon x<sub>v</sub> or not) until reaching a terminal node. Assign x the node membership of this terminal node.</p>
</blockquote>

<p>However:</p>

<blockquote>
  <p>This type of scenario shows that a non-informative variable can appear informative over a single tree under our noising up process...Moreover, for a single tree, this kind of problem can be resolved by slightly modifying the noising up process. Rather than using random left-right assignments on all nodes beneath x<sub>v</sub>, use random assignments for only those nodes that split on x<sub>v</sub>. This will impact prediction only when x<sub>v</sub> is informative and not affect prediction for non-informative variables</p>
</blockquote>

<p>How do I go about implementing this procedure? It seems that the <code>fitted_node()</code> function from the <code>partykit</code> package should do the trick. <code>fitted_node()</code> takes the following arguments:</p>

<pre><code>fitted_node(node, data, vmatch = 1:ncol(data), obs = 1:nrow(data), perm = NULL)
</code></pre>

<p>where</p>

<blockquote>
  <p><strong>node</strong>:  an object of class partynode<br>
   <strong>data</strong>: a list or data.frame<br>
   <strong>vmatch</strong>: a permutation of the variable numbers in data<br>
   <strong>obs</strong>: a logical or integer vector indicating a subset of the           observations in  data<br>
  <strong>perm</strong>: a vector of integers specifying the variables to be permuted
  prior before splitting (i.e., for computing permutation variable
  importances).  The default NULL doesnâ€™t alter the data.</p>
</blockquote>

<p>I can recursively partition the <code>data</code> using the tree specified in <code>node</code>. However how do I ""noise up"" one of the splitting variables in my tree? It is not clear to me whether i should use the <code>vmatch</code> and/or <code>perm</code> arguments and how i should specify them (for example do <code>perm</code> and <code>vmatch</code> refer to the column number of the covariate or do they refer to the cells in <code>data</code>?)</p>

<h2>References</h2>

<ol>
<li>Ishwaran, H. (2007). Variable importance in binary regression trees
and forests. Electronic Journal of Statistics, 1, 519â€“537.
<a href=""http://doi.org/10.1214/07-EJS039"" rel=""nofollow"">http://doi.org/10.1214/07-EJS039</a></li>
</ol>
"
"NaN","NaN","186560","<p>I am fitting a multinomial logistic regression using the glmnet package in R:</p>

<pre><code>library(glmnet)
data(MultinomialExample)
cvfit=cv.glmnet(x, y, family=""multinomial"", type.multinomial = ""grouped"")
plot(cvfit)
</code></pre>

<p><a href=""http://i.stack.imgur.com/gF135.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/gF135.png"" alt=""rplot""></a></p>

<p>What is ""Multinomial Deviance"" and how does it relate to ""<a href=""https://www.kaggle.com/wiki/MultiClassLogLoss"" rel=""nofollow"">Multinomial Logloss</a>""?</p>
"
"0.100857047225074","0.102722675451665","186725","<p>Short version: How would one be able to quantify an intervention effect in time-series analysis when the intervention decreases seasonal amplitude variation but doesn't directly effect the median?</p>

<p><a href=""https://www.dropbox.com/s/hb3g7j17igeqnoc/dat.csv?dl=0"" rel=""nofollow"">Here</a> is a link to my raw data.</p>

<p>I have a complex time-series of daily incidence numbers for a population over 7 years, totaling 2557 observations. There is a strong weekly and yearly seasonality (high incidence in winter months and low incidence in summer months). There is a baseline negative trend which is orders of magnitude smaller than the seasonality. An intervention was introduced at time = 1700. This intervention should theoretically not cause a level shift. My aim is to detect whether the intervention increases the baseline negative trend.</p>

<p>I have attempted to fit a dynamic linear regression with ARIMA errors in R using <code>auto.arima()</code> in the <code>forecast</code> package. I modeled the weekly season using a dummy variable for each weekday and the weekend. I modeled the monthly seasonality with harmonics using <code>fourier()</code> function in the <code>forecast</code> package. An the intervention effect was coded in by specifying the time index and post-intervention times as independent variables using the methods described in <a href=""http://isites.harvard.edu/fs/docs/icb.topic79832.files/L06_Program_Evaluation_2/Segmented_Regression.Wagner.2002.pdf"" rel=""nofollow"">Segmented regression analysis of interrupted time series studies in medication use research</a>. With these variables specified <code>auto.arima()</code> suggests an ARMA(7,7) process. The coefficients for baseline trend and post-intervention trend are however non-significant.  </p>

<p>I am concerned that by using fourier terms to model away the seasonality I am artificially removing any intervention effect, as visual analysis of the time series indicates that the intervention is specifically decreasing incidence during the winter months and therefore reducing the yearly seasonal variability. </p>
"
"0.10466430427046","0.0913717355809759","186728","<p>I am using the great <code>{caret}</code> package to run a lot of models, however I would like to analyse the model as one usually does having run that model in its own right, i.e. not within caret.</p>

<p>I am using the mboost package, starting with the <code>glmboost</code> function. If you run this model there are then functions within the mboost package that can be applied directly to the output of that function. however, these same functions do not work on the output of <code>train</code> from caret.
<code>train</code> is essentially the wrapper function which allows you to optimise the parameters for the chosen model, glmboost in my case.</p>

<p>Here is some dummy code if anybody wants to play with it. Its a boosted tree regression model, first using the <code>glmboost</code> function directly from the mboost package, then the same thing through the caret package (with some extra parameters to optimise over):</p>

<pre><code>## ============================================================== ##
##  Create a simple model using glmboost that runs through caret  ##
## ============================================================== ##

## install as necessary!
library(mboost)
library(caret)
## Use multicore if you can!
library(doMC)
registerDoMC(4)

## ============= ##
##  Create data  ##
## ============= ##

## Let's say we are predicting a numeric value, based on the predictors
## 70 observations of 10 variables, assuming they are chronologically order (a time-series)

set.seed(666)                                                # the devil's seed
myData &lt;- as.data.frame(matrix(rnorm(70*15, 2, .4), 70, 10)) #10 columns of random numbers
names(myData) &lt;- c(""to.predict"", paste0(""var_"", seq(1, 9)))
# Have a ganders
str(myData)                             

## Create model output using the mboost package directly
glm_mboost &lt;- glmboost(to.predict ~ .,  # predict against all variables
                       myData,          # supply our data
                       control = boost_control(mstop = 200)
                       )

## This is what I'd like to do with the output from the caret package!
plot(glm_mboost)
cvr &lt;- cvrisk(glm_mboost)
plot(cvr)

## ========================================== ##
##  Set parameters for train() - using caret  ##
## ========================================== ##

## glmboost takes 'mstop' and 'prune' as inputs
myGrid &lt;- expand.grid(mstop = seq(20, 250, 50),
                      prune = ""AIC""    #this isn't actually required by the mboost package!
                      )
myControl &lt;- trainControl(method = ""timeslice"", # take consequetive portions of the time-series
                          fixedWindow = TRUE, # If this is TRUE, we get the error
                          horizon = 1,
                          initialWindow = 20) # ~1 months of trading days
## fixedWindow = TRUE  --&gt; 

## =============== ##
##  Run the model  ##
## =============== ##

glm_caret &lt;- train(to.predict ~ ., data = myData,
                method = ""glmboost"",
                #metric = ""MyGauss"",
                trControl = myControl,
                tuneGrid = myGrid
                ##verbose = FALSE)
                )

## Maybe this will give you some idea about how to extract it
str(glm_caret)

## This is the best I can do, but the first plot doesn't come out right
x &lt;- glm_caret$finalModel
plot(x)
cvr1 &lt;- cvrisk(x)
plot(cvr1)
</code></pre>

<p>An idea I have is to simply use the optimal output given by caret to run the <code>glmboost</code> function once, with the provided parameters, but as I am going through many models, I'd rather save the computing time!</p>
"
"0.0969003166223018","0.0986927542439653","186845","<p>I created some data using the following code:</p>

<pre><code>set.seed(1221)
x &lt;- runif(500)
y &lt;- runif(500,0,2)
z &lt;- rep(0,500)
z[-0.8*x + y - 0.75 &gt; 0] &lt;- 1
plot(x,y,col=as.factor(z))
</code></pre>

<p>This produces the following plot</p>

<p><a href=""http://i.stack.imgur.com/ycWdr.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ycWdr.png"" alt=""enter image description here""></a></p>

<p>The data is linearly separable. Then, I applied the glm function to create a logistic regression model.</p>

<pre><code>df &lt;- data.frame(class = z, x = x, y = y)
model &lt;- glm(z ~ x + y, family = binomial, data = df)
</code></pre>

<p>This produces the following output:</p>

<pre><code>summary(model)
Call:
glm(formula = z ~ x + y, family = binomial, data = df)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-8.127e-04  -2.000e-08  -2.000e-08   2.000e-08   7.699e-04  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)    -1062      52666   -0.02    0.984
x              -1163      57197   -0.02    0.984
y               1433      70408    0.02    0.984

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 6.8274e+02  on 499  degrees of freedom
Residual deviance: 1.3345e-06  on 497  degrees of freedom
AIC: 6

Number of Fisher Scoring iterations: 25
</code></pre>

<p>The result surprised me, first because the parameter estimates are huge, and second because I was expecting such estimates to be close to the original decision boundary function, i.e. <code>-0.8x + y - 0.75 = 0</code>.</p>

<p>I then used the <a href=""http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html"" rel=""nofollow"">glmnet</a> package to see if I could solve this issue. This package creates a penalised logistic regression model in order to deal with the large values in the parameter estimates. The code I used is the following:</p>

<pre><code>library(glmnet)
cvfit &lt;- cv.glmnet(as.matrix(df[,-1]), as.factor(df$class), family =   ""binomial"", type.measure = ""class"")
plot(cvfit)
</code></pre>

<p><a href=""http://i.stack.imgur.com/vH4AV.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/vH4AV.png"" alt=""enter image description here""></a></p>

<p>And the coefficients for the optimal penalty strength are:</p>

<pre><code>coef(cvfit, s = ""lambda.min"")
3 x 1 sparse Matrix of class ""dgCMatrix""
                    1
(Intercept) -84.01446
x           -91.40983
y           113.18736
</code></pre>

<p>Such coefficients are smaller than the ones obtained with the <code>glm</code> function. Still they are not the same as the decision boundary function. </p>

<p>Does anybody know why this is happening? Any help is greatly appreciated.</p>
"
"0.0927749898843639","0.0944911182523068","187105","<p>When you have two factors <code>A</code> (levels: A1, A2) and <code>B</code> (levels: B1, B2) with their interaction, R {metafor} package allows you to use a very useful alternative parameterisation for mixed-effects meta-regression, that shows the effect for each of the subgroups of the interaction:</p>

<pre><code>rma(yi, vi, mods= ~ factor (A) : factor (B) - 1, data=dat)
A1:B1 ...
A1:B2 ...
A2:B1 ...
A2:B2 ...
</code></pre>

<p>With <code>â€¦</code> showing the model results.<br>
However, in my case there is also a significant continuos variable <code>C</code>. I would like to obtain the effect for each of the subgroups of the interaction as above, but controlling for the effect of the continuous variable. Is that possible? The following parameterisation does not seem to work:</p>

<pre><code>rma(yi, vi, mods= ~ factor (A) : factor (B) - 1 + C, data=dat)
</code></pre>

<p>Thanks</p>

<p>EDIT including a real output. For the first model with two factors:</p>

<pre><code>rma(yi, vi, mods=~ Myc : newN -1, data=dat)
Model Results:

                  estimate      se    tval    pval    ci.lb   ci.ub     
MycAM:newNNlow      0.0100  0.0561  0.1781  0.8591  -0.1016  0.1216     
MycECM:newNNlow     0.2753  0.0461  5.9666  &lt;.0001   0.1835  0.3672  ***
MycAM:newNNhigh     0.2439  0.0592  4.1226  &lt;.0001   0.1261  0.3616  ***
MycECM:newNNhigh    0.3058  0.0453  6.7474  &lt;.0001   0.2156  0.3960  ***
</code></pre>

<p>For the second model with two factors and one variable:</p>

<pre><code>rma(yi, vi, mods=~ Myc : newN -1 + co2conc, data=dat)

Model Results:

                  estimate      se     tval    pval    ci.lb   ci.ub   
co2conc             0.0007  0.0003   2.0947  0.0394   0.0000  0.0014  *
MycAM:newNNlow     -0.4355  0.2220  -1.9618  0.0533  -0.8774  0.0064  .
MycECM:newNNlow    -0.2058  0.2319  -0.8872  0.3777  -0.6675  0.2560   
MycAM:newNNhigh    -0.2469  0.2353  -1.0491  0.2974  -0.7154  0.2216   
MycECM:newNNhigh   -0.1516  0.2180  -0.6954  0.4889  -0.5855  0.2824 
</code></pre>

<p>The first model with two factors shows that the effect is significant and positive (0.24-0.30) in all subgroups except one. However, when including one variable in the second model, the effects are now negative for the 4 subgroups of the interaction, which does not make any sense with this example.</p>
"
"0.0323001055407673","0.0328975847479884","187221","<p>I'm looking for a way to shrink a proportional odds model with LASSO for ordinal regression. </p>

<p>As far as I know, the ""penalized"" package has no options for ordinal regression. I've found ""glmnetcr"" package and ""glmnet"" package but they are both only suitable for fitting continuation ratio models instead of proportional odds. </p>

<p>Is there another package available to use LASSO for ordinal regression? Or will it be better to use ridge regression in the ""lrm"" funtion from the ""rms"" package?</p>

<p>Thanks for your help,<br>
Esmee </p>
"
"NaN","NaN","187443","<p>I wanna run sample selection on my data. My response is binary and also, I have some continuous and some binary variables. I was wondering how can I use ""sampleSelection"" package? 
It seems that it does not support logistic regression as well as factor variables.
Any thought?</p>

<p>PS. My goal is modeling my data using binomial logistic regression.</p>
"
"0.0395593886064618","0.040291148201269","187679","<p>I'd like to test two classifiers at the same time, that is logistic regression and classification trees. To find a classification threshold, which for example maximises F1-score, I split my data set into train, validation and test set. Because this is all pretty new to me, I wrote my own loop to understand the procedure behind it. I was wondering, how would you find manually (without just using cross-validation from the caret package) the optimal cp-value? Is the optimal cp the average of all the cp picked for each fold? But if so, what do I need the validation set for? In logistic regression, this is more clear to me since I need it to find the threshold which maximises my F1 score. I appreciate your help!</p>
"
"0.0625488854200668","0.0637058989297032","187841","<p>I am using R's <code>ordinal</code> package to run a mixed regression model with an ordinal dependent variable. The data I am working with looks like this:</p>

<pre><code>      x y z
1  S153 A 2
2   S11 A 2
3   S40 A 2
4  S112 A 1
5  S150 A 2
6   S40 A 2
7   S40 A 2
8  S150 A 2
9   S40 A 2
10  S39 A 2
11 S150 A 2
12  S53 A 2
13 S150 A 2
14 S150 A 2
15  S23 A 2
16  S36 A 1
17  S79 A 2
18 S150 A 2
19  S70 A 2
20 S133 A 1
21  S40 A 2
22 S150 A 2
23  S48 A 2
24  S53 A 2
25 S150 A 2
26  S12 A 2
27 S150 A 1
28  S80 B 2
29 S147 B 3
30  S92 C 2
31   S2 D 2
32  S37 D 2
33  S14 D 2
34  S56 D 2
35  S14 D 2

structure(list(x = structure(c(8L, 1L, 14L, 2L, 7L, 14L, 14L,  7L, 14L, 
13L, 7L, 16L, 7L, 7L, 10L, 11L, 19L, 7L, 18L, 4L, 14L,  7L, 15L, 16L, 7L, 3L, 
7L, 20L, 6L, 21L, 9L, 12L, 5L, 17L, 5L), .Label = c(""S11"",  ""S112"", ""S12"", 
""S133"", ""S14"", ""S147"", ""S150"", ""S153"", ""S2"", ""S23"",  ""S36"", ""S37"", ""S39"", 
""S40"", ""S48"", ""S53"", ""S56"", ""S70"", ""S79"", ""S80"", ""S92""), class = ""factor""), y = 
structure(c(1L, 1L, 1L,  1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L,  1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 3L, 4L, 4L, 4L, 4L, 4L ), 
.Label = c(""A"", ""B"", ""C"", ""D""), class = ""factor""), z = c(2L,  2L, 2L, 1L, 2L, 
2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 1L, 2L,  2L, 2L, 1L, 2L, 2L, 2L, 2L, 
2L, 2L, 1L, 2L, 3L, 2L, 2L, 2L, 2L,  2L, 2L)), .Names = c(""x"", ""y"", ""z""), 
class = ""data.frame"", row.names = c(NA, -35L))
</code></pre>

<p>Variable 'z' is my response variable (ordinal factor). Variable 'y' is my predictor and I want to include 'x' as random effects. To do this, I am using <code>clmm</code> as follows:</p>

<pre><code>m1 &lt;- clmm(factor(z, ordered=T) ~ y + (1|x) , data=df)
</code></pre>

<p>However, this results in the following warning message:</p>

<pre><code>Warning message:
(1) Hessian is numerically singular: parameters are not uniquely determined 
In addition: Absolute convergence criterion was met, but relative criterion was not met
</code></pre>

<p>I have tried running this with clm excluding the random effects and I keep getting  the same warning. </p>

<p>Here is the table of the predictor and response variabes:</p>

<pre><code>table(df$z,df$y)

     A  B  C  D
  1  4  0  0  0
  2 23  1  1  5
  3  0  1  0  0
</code></pre>

<p>I am not sure if this is a problem of complete separation or not. Why am I getting this warning and how can I deal with it?</p>
"
"0.0884574820723792","0.0900937462695559","188011","<p>I'm doing machine learning in R. I would like to know how we can create a model object that we can pass to ""predict"" function along with new data so that we obtain predicted values. 
To elaborate, I'm trying to write a new machine learning algorithm in R. Till now I have only used predict function but don't know how to create ""model"" objects to pass to predict function. If we're doing a linear regression, calling lm would create ""lm"" object. If we're doing naiveBayes classifier, and call it from e1071 package, it would create naiveBayes classifier object, which we will pass to predict function. Now, if I'm writing a new algorithm, how do I create an object of that algorithm? And how exactly predict function will process that? What class variables/methods that ""model"" object should have so that it can be processed by ""predict"" function available in R? I know this is a bit open ended question, but I couldn't find any proper documentation. A basic/prototype example in terms of code would be highly appreciated. Though I've been using R, I'm not familiar to classes/objects concept in R. Thank you very much.</p>
"
"0.0419590679148345","0.056980288229819","188129","<p>I'm trying to compute a rolling annualized alpha for a given portfolio using the R package PerformanceAnalytics.</p>

<p>Problem is when using <code>RollingRegression()</code> on daily returns, I get a rolling daily alpha. I'd like that alpha to be annualized the same way it's done in <code>RollingPerformance()</code>.</p>

<p>I thought of annualizing my returns before performing my regression so I'd get an annualized alpha.</p>

<p>So would you agree saying that:</p>

<ol>
<li>performing a rolling regression on daily returns</li>
<li>get the daily alpha and annualize it</li>
</ol>

<p>is equivalent to:</p>

<ol>
<li>perform a rolling regression on annualized returns</li>
<li>get the annualized alpha</li>
</ol>

<p>Thanks</p>

<p>Here's the code of the second solution:</p>

<pre><code>library(PerformanceAnalytics)

dates &lt;- seq.Date(as.Date(""2006-12-31""),by=""day"",length.out = 1000)

set.seed(3543)
ra &lt;- xts(rnorm(1000,0,0.001),order.by = dates)
rb &lt;- xts(rnorm(1000,0,0.001),order.by = dates)

ra_ann &lt;- (1+ra)^250 - 1
rb_ann &lt;- (1+rb)^250 - 1

chart.RollingRegression(ra_ann,rb_ann,width = 250, attribute = ""Alpha"", na.pad = FALSE)

chart.RollingPerformance(ra,width = 250)#, FUN=""Return.annualized"")
</code></pre>

<p>Which gives me that output:
<a href=""http://i.stack.imgur.com/1RNGU.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1RNGU.png"" alt=""enter image description here""></a></p>
"
"0.0395593886064618","0.040291148201269","189005","<p>I am trying to simulate from observed data that I have fit to a zero-inflated poisson regression model. I fit the data in R using zeroinfl() from the package pscl, but I am having trouble figuring out how to derive the ZIP distribution from the coefficient estimates. I know how to derive the predicted counts from these coefficient estimates (more information here: <a href=""http://www.ats.ucla.edu/stat/stata/faq/predict_zip.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/stata/faq/predict_zip.htm</a>), but can anyone help me understand how to find/derive estimates for my distribution parameters (i.e. lambda for the Poisson distribution, p for the Bernoulli distribution) that I can then sample from? Thanks!</p>
"
"0.0969003166223018","0.0904683580569682","189903","<p>I am conducting logistic regression analysis: The data includes 107 observations, dependent variable is a binary one, there is about 5 covariates which are both continuous, binary and multi-categorical variables. I want to use some cut_off points to predict the outcome.</p>

<p>So basicly, I select one cut_off point (based on the requirement that the sensitivity >70% and specificity > 70%). Then I devide my data into train set (85% data points) and test set (15% data points).</p>

<p>I fit the model with 5 covariates on the train set, and use the model to predict the outcome on the test set. I use the glm() function to fit the model, and glm.predict() function to predict on test sets. </p>

<p>Since there is missing data, I create 40 imputed data sets using MICE package in R. The procedure above is repeated over 40 imputed data sets. For each data set, I obtain the mis-classification errors.</p>

<p>So, to get the overall mis-classification errors, I averaged over 40 mis-classification error rates.</p>

<p>My question is: </p>

<p>How to assess the variability of this overall mis-classification errors?</p>

<p>As I am thinking that we can not use the usual formula to calculate the variance for this number, as the mis-classification errors over different imputed data sets might be correlated to each other.</p>

<p>Does any one have a suggestion or reference to do this?
(I am using R).
Thank you for any inputs.</p>
"
"0.0625488854200668","0.0637058989297032","190389","<p>I built a conditional logistic regression with the function clogit (package survival) in R and in which I included one categorical independent variable (habitat type) with 15 levels. I noted that the sign of parameter estimates changed between models that were built for each level of the categorical independent variable and a model that included the categorical variable (thus, all levels). Contrary to the model including the categorical variable, the results of models for each level of the categorical variable made sense from a biological standpoint. Does sign changes signify a multicollinearity issue? However, in my case, the values of VIFs for each level of the categorical variable were &lt; 3. Should I group some levels of my categorical variable because I noted the levels that were significant, were often those with few observations ?</p>
"
"0.027972711943223","0.0284901441149095","190899","<p>I installed the R package AER, from which you get the data <code>PSID1982</code>. Then I define this model:</p>

<pre><code>data(""PSID1982"")
set.seed(15606)
selectedobs = sample.int(nrow(PSID1982),size = 400, replace = FALSE)
attach(PSID1982);
experiencesq = experience^2
dgender = 1*(gender==""male"")
dmarried = 1*(married==""yes"")
dunion = 1*(union==""yes"")
dindustry = 1*(industry==""yes"")

outreg = lm(log(wage) ~ experience + experiencesq + education + dgender + 
                        dmarried + dunion + dindustry)
summary(outreg);
</code></pre>

<p>And now I need to find out if the impact of the amount of work <code>experience</code> on the <code>log(wage)</code> is different for women compared to men, and what do I expect it to be intuitively?</p>

<p>I want to divide the set of data into 2 subsets being men and women and perform a linear regression each dataset, so I do this. but then I get this error message and I don't know how to fix this.... </p>

<pre><code>outreg = lm(wage~ experience+experiencesq+education+dmarried+dunion+dindustry, 
            subset(PSID1982,gender==""male""))
Error in model.frame.default(formula = wage ~ experience + experiencesq +  : 
  variable lengths differ (found for 'experiencesq')
</code></pre>

<p>Thanks in advance!</p>
"
"0.0625488854200668","0.0637058989297032","191506","<p>My dependent variable has 4 categories, but when I run the multinomial logistic regression using the package <code>nnet</code> with function <code>multinom</code> the results only show 3 categories. </p>

<p>I've tried changing the category numbers from 0,2,3,4 to 1,2,3,4, and also tried using names instead of numbers for the categories but it still wont show all 4 categories in the results. </p>

<p>Also, when I changed the categories to names instead of numbers, the resulting p values for each category drastically changed. Why is this? 
The p values were acquired using these commands</p>

<pre><code>z &lt;- summary(siglm)$coefficients/summary(siglm)$standard.errors
p &lt;- (1 - pnorm(abs(z), 0, 1)) * 2
p
</code></pre>
"
"0.0685188709827532","0.0581552631499044","191611","<p>I'm using the set.seed() function in R to achieve reproducability of my results. I compare different regression methods (e.g. RandomForest, SVM, GAM) by their MSE derived from a cross-validation procedure. To my surprise, I realized that results differ whether I place 'set.seed(123)' at the beginning of my code (and then running the whole script) or whether I place 'set.seed(123)' just before calling each method in the script.</p>

<p>To illustrate pls follow my example below (although the answer by 'Sean Easter' and the example given by 'Cliff AB' below should explain as well): </p>

<pre><code>data(iris)
iris
myf&lt;- Sepal.Length ~ 
 Sepal.Width+
 Petal.Length+
 Petal.Width+
 Species

# required packages
library(sperrorest)
library(randomForest)
library(rpart)

##### Regression Tree
set.seed(123)
ctrl &lt;- rpart.control(cp = 0.001)
fit_rpart &lt;- rpart(myf, data = iris, control = ctrl)

#5-repeated 10-fold CV
mypred.rpart &lt;- function(object, newdata) predict(object, newdata)
eval_ns_rpart &lt;- sperrorest(data = iris, formula = myf, model.fun= 
                            rpart, model.args = list(control = ctrl),
                            pred.fun = mypred.rpart, smp.fun = 
                            partition.cv, smp.args = 
                            list(repetition=1:5, nfold=10))
summary(eval_ns_rpart$error)

##### Random Forest
#set.seed(123) # REMOVE HASH IN 2ND RUN!!!! 
fit_rf &lt;- randomForest(myf, data = iris, ntree=1000)

#5-repeated 10-fold CV
mypred.rf &lt;- function(object, newdata) predict(object, newdata)
eval_ns_rf &lt;- sperrorest(data = iris, formula = myf,
                         model.fun = randomForest,
                         pred.fun = mypred.rf,
                         smp.fun = partition.cv, smp.args= list(repetition=1:5, nfold=10))
summary(eval_ns_rf$error)

#### SUMMARIES Mean Squared Errors(MSE)
tr_MSE_rpart&lt;-(summary(eval_ns_rpart$error)[3,1]) # MSE training error
# 0.08548725

t_MSE_rpart&lt;-(summary(eval_ns_rpart$error)[10,1]) # MSE test error
# 0.1445583

tr_MSE_RF&lt;-(summary(eval_ns_rf$error)[3,1]) # MSE training error
# 0.07241344 # 2nd run: 0.07266605

t_MSE_RF&lt;-(summary(eval_ns_rf$error)[10,1]) # MSE test error
# 0.1403778  # 2nd run: 0.1358957
</code></pre>
"
"0.0395593886064618","0.040291148201269","191712","<p>I am using KFAS to fit a dynamic logistic model of the form;</p>

<p>$\hat{y} = \bf \beta_t x + \epsilon$ </p>

<p>$\beta_t = \beta_{t-1} + \eta$</p>

<p>So the regression parameters change over time, and act as latent variables to be estimated by the filter.</p>

<p>Can state space models of this form generally accept situations where we have multiple observations per time period? I believe they can, but I can't figure out how to specify this in KFAS (or any other R package for that matter).</p>

<p>I've tried the below code, but KFAS thinks that this means there are 22 time periods - there are actually only ten.</p>

<pre><code>library(KFAS)
y = c(1,0,0,0,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1,1,0,1)
i = seq.Date(from = as.Date(""2014-01-01""), as.Date(""2014-01-10""), length.out = 22)
x = rnorm(n = 22, mean = 1, sd = 2)

a =   model = SSModel(y ~ 
                    SSMregression(~x),
                  distribution = ""binomial"")

fit = fitSSM(a, inits = c(0,0))
</code></pre>
"
"0.10833784750436","0.110341853688094","191916","<p>I have taken plenty of time to try and help myself, but I keep reaching dead ends. </p>

<p>I have a dataset consisting of body measurements collected from a bird species, and the sex of each bird (known by molecular means). I built a logistic regression model (using the AIC information criterion) to assess which measurements explain better the sex of the birds. My ultimate goal is to have an equation which could be used by others under field conditions to predict reliably the sex of the birds by taking as few body measurements as possible. </p>

<p>My final model includes four independent variables, namely ""Culmen"", ""Head-bill"", ""Tarsus length"", and ""Wing length"" (all continuous). I wish my model was a little more parsimonious, but all the variables seem to be important according to AIC criterion. Because the model produced should be used as prediction tool, I decided validate it using a leave-one-out cross validation approach. In my learning process, I first tried to complete the analyses (cross-validation and plotting) by including only one explanatory variable, namely ""Culmen"". </p>

<p>The output of the cross validation (package ""boot"" in R) yields two values (deltas), which are the cross-validated prediction errors where the first number is the raw leave-one-out, or lieu cross-validation result, and the second one is a bias-corrected version of it. </p>

<pre><code>model.full &lt;- glm(Sex ~ Culmen, data = my.data, family = binomial)
summary(model.full.1)

cv.glm(my.data, model.full, K=114)

$call
cv.glm(data = my.data, glmfit = model.full, K = 114)

$K
[1] 114

$delta
[1] 0.05941851 0.05937288
</code></pre>

<p>Q1. Could anyone expalin what do these two values represent and how to interpret them?    </p>

<p>Following is the code as presented by Dr. Markus MÃ¼ller (Calimo) in a similar, albeit not identical, post (<a href=""http://stackoverflow.com/questions/20346568/feature-selection-cross-validation-but-how-to-make-roc-curves-in-r"">http://stackoverflow.com/questions/20346568/feature-selection-cross-validation-but-how-to-make-roc-curves-in-r</a>) which I tried to tweak to meet my data:</p>

<pre><code>library(pROC)
data(my.data)
k &lt;- 114    # Number of observations or rows in dataset
n &lt;- dim(my.data)[1]
indices &lt;- sample(rep(1:k, ceiling(n/k))[1:n])

all.response &lt;- all.predictor &lt;- aucs &lt;- c()
for (i in 1:k) {
test = my.data[indices==i,]
learn = my.data[indices!=i,]
model &lt;- glm(Sex ~ Culmen, data = learn, family=binomial)
model.pred &lt;- predict(model, newdata=test)
aucs &lt;- c(aucs, roc(test$Sex, model.pred)$auc)
all.response &lt;- c(all.response, test$outcome)
all.predictor &lt;- c(all.predictor, model.pred)
}

Error in roc.default(test$Sex, model.pred) : No case observation.

roc(all.response, all.predictor)

Error in roc.default(all.response, all.predictor) : No valid data provided.

mean(aucs)
</code></pre>

<p>Q2. What's the reason for the first error message? I guess the second error is associated with the first one, and that it will be solved once I find a solution to the first one.</p>

<p>I will appreciate very much any help!!</p>

<p>Luciano </p>
"
"0.0625488854200668","0.0509647191437626","192507","<p>BACKGROUND:</p>

<p>I have some animal behaviour data. The time allocated by a group of animals 
to different behaviours per minute was recorded repeatedly until
the end of the experiment. Therefore, I have 4 response variables:</p>

<p>y1 = proportion of time allocated to behaviour 1
y2 = proportion of time allocated to behaviour 2
y3 = proportion of time allocated to behaviour 3
y4 = proportion of time allocated to behaviour 4
NB: The sum of all the y variables is equal to 1. </p>

<p>I also have a number of candidate predictor variables. I wish to establish which 
of these candidate predictor variables have a significant effect on the proportion
of time allocated to the different behaviours.</p>

<p>PROBLEM:</p>

<p>I believe this may be modeled using Dirichlet multivariable regression 
(used for modeling data representing components as a percentage of a total, DirichletReg package in R). However,
there is a issue of temporal correlation ie. the y variables are not independent. For example,
if the proportion of y1 at time_1 is high, then the proportion of y1 at time_2 is also
likely to be high. </p>

<p>QUESTION:</p>

<p>Is it possible incorporate temporal autocorrelation into Dirichlet multivariable regression? If so, which R package
would be suitable? Are there any other possible approaches to this problem? Many thanks in advance.</p>
"
"0.0930988128231456","0.0948209311861521","192785","<p><strong>Objective</strong></p>

<p>I have a crossed and implicitly nested design and am trying to validate the correct â€˜maximalâ€™ model (including all linear and pairwise interactions of the variables) for use in <code>lmer()</code>.  I intend to use this as the starting point for some kind of backward stepwise regression, possibly making use of the function <code>mixed()</code> in the  <code>{afex}</code> package.</p>

<p><strong>Experimental design</strong></p>

<p>This a linguistics study.  We have 20 <code>Subjects</code>, each speaking 180 utterances, amounting to 3600 observations in total. Each utterance is initiated via prompting, and an associated Response Time is measured. Log Response Time is the dependent variable. </p>

<p><em>Conditions &amp; Blocks</em></p>

<p>The Response Time for the utterances is affected by 3 <code>Conditions</code> (coded 1 to 3). Each <code>Condition</code> is implemented by prompting the <code>Subject</code> to recite 1 of 4 <code>Blocks</code> of utterances (coded 1 to 12).</p>

<p><em>Words &amp; Tones</em></p>

<p>Each <code>Block</code> brings about its associated <code>Condition</code> via 15-utterance repetition of 3 carefully chosen <code>Words</code>.  There are a total of 12 <code>Words</code> used in the experiment (coded 1 to 12). The <code>Words</code> within each <code>Block</code> can also be categorized by <code>Tone</code> (coded 1 to 2).  There are 6 <code>Words</code> per <code>Tone</code>.  </p>

<p><em>Summary</em></p>

<p>Each of the 20 <code>Subjects</code> utter all 12 <code>Blocks</code> of 15 utterances each.  In doing so, they repeatedly utter all 12 <code>Words</code> (15 utterances per <code>Word</code>), and thereby use both <code>Tones</code> (90 utterances per <code>Tone</code>).</p>

<p>I would like to consider <code>Block</code>, <code>Word</code>, and <code>Subject</code> as random effects, and <code>Condition</code> and <code>Tone</code> as fixed.</p>

<p><strong>Proposed Model</strong></p>

<p>I think the model can be written in the following wayâ€¦</p>

<p><code>RT_log ~ Condition*Tone + (Condition*Tone|Subject) + (Condition|Word) + (Tone|Block)</code></p>

<p><strong>Questions</strong></p>

<p><strong>1.</strong> Is this the 'maximal' model (with linear plus pairwise interactions) appropriate for my experimental design?_ </p>

<p><strong>2.</strong> There is correlation between <code>Block</code> and <code>Condition</code> (there are only 4 possible blocks - out of the total 12 - for each <code>Condition</code>).  There is, similarly, correlation between <code>Word</code> and <code>Tone</code>.  Is it 'okay' to leave this correlation in the model? I don't see a good way of removing it.</p>

<p><strong>3.</strong> How will lme4 handle implicit nesting: I.e., the blocks, which are implicitly nested in the 3 conditions (i.e., only 4 blocks are applicable to each of the 3 conditions, even though the blocks are coded from 1 to 12), and the words, which are implicitly nested within the 2 tones (only 6 words are applicable to each tone, even though words are coded from 1 to 12)?</p>

<p><strong>4.</strong> Some <code>Blocks</code> utilize <code>Words</code> of only a single <code>Tone</code>, whereas other <code>Blocks</code> utilize words of both <code>Tones</code>.  Will that cause problems for the <code>(Tone|Block)</code> term in the model? It will only make sense for certain values of Block.</p>

<p><strong>5.</strong> It has been suggested by some that we might need a ""Subject:Word"" grouping (random effect).  Why might we need this grouping?</p>
"
"NaN","NaN","193419","<p>What is the implication if I don't fix a logistic regression that has complete or quasi separation? can I still read the marginal effects or are they not going to be valid? </p>

<p>My exercise is actually to just find out which independent variables are most predictive of Y. </p>

<p>I read some responses to complete/quasi-separation and I tried using logistf package for R but got this error message ""NA/NaN/Inf in foreign function call logistf"". why does this arises? </p>
"
"0.0419590679148345","0.056980288229819","194140","<p>I've been using stepAIC to narrow down my logistic regression model.  However, I get the following warning when I run my model:</p>

<p>glm.fit: fitted probabilities numerically 0 or 1 occurred</p>

<p>I know this means I have complete or quasi-complete separation in my data.  On examination of my data, I see the quasi-complete separation and think that it's meaningful.  Reading online, I see recommendations to use a Firth penalized regression (logistf) or exact logistic regression (elrm); but neither of these will work with stepAIC.  I've also tried bayesglm but I still get the same warning. </p>

<p>How should I select a model when my data has complete separation?  How would I do this in R?  Is my mistake in my stats or in my understanding of using the packages in R?  Any help would be much appreciated!</p>
"
"0.0559454238864459","0.056980288229819","194373","<p>First, I am new to analyzing public opinion polls and the r package ""Survey"".  I would like some advice.  I am running a regression model with weights from a Pew survey, however, I noticed that a significant portion of my data is missing because of the covariates.  As a robustness check, I would like to impute my data set.  I would like some advice on the best way to handle this in r.</p>

<p>I am most familiar with using the ""mice"" package in r to handle missing data.  I don't believe ""survey"" can accommodate mice.  Should I separate each imputed dataset and then perform a regression analysis (using ""survey"") for each dataset?  Is this the most efficient method?  Finally, how do you pool the estimates?  You don't average the estimates from all the imputed data sets, do you? </p>
"
"0.0484501583111509","0.0493463771219827","194582","<p>I'm attempting <strong>one-to-one exact</strong> matching in R. That is, I have 60,000 treatment observations and 200,000 control observations but I only want 60,000 control observations and I want those to be matched exactly to the treatment set on a set of variables; normally this would reduce N greatly but it will not in this case. The <strong>one-to-one</strong> aspect of this can be done with nearest-neighbor matching in R's MatchIt package but that method isn't <strong>exact</strong>; it seems to rely on logistic regression propensity scores. MatchIt can also do <strong>exact</strong> matching but then it's not <strong>one-to-one</strong>. Does MatchIt, or another package, implement <strong>exact one-to-one</strong> matching? I realize I can probably create the solution myself with a lot of code but this seems like something that might already exist.</p>
"
"0.115334445976885","0.117467873474841","194597","<p>I am doing a meta-regression with metafor package in R. The mixed-effect model for proportion is used to assess the linearity between study performed year and medication prevalence. Here below is my script in R:</p>

<pre><code>model_A &lt;- rma.glmm(xi=A, ni=Sample, measure=""PLO"", mods=~year)
print(model_A)
</code></pre>

<p>And results I got from R are:</p>

<pre><code>Mixed-Effects Model (k = 32; tau^2 estimator: ML)

tau^2 (estimated amount of residual heterogeneity):     1.6349
tau (square root of estimated tau^2 value):             1.2786
I^2 (residual heterogeneity / unaccounted variability): 99.40%
H^2 (unaccounted variability / sampling variability):   168.00

Tests for Residual Heterogeneity: 
Wld(df = 30) = 2221.4535, p-val &lt; .0001
LRT(df = 30) = 3187.7073, p-val &lt; .0001

Test of Moderators (coefficient(s) 2): 
QM(df = 1) = 22.7322, p-val &lt; .0001

Model Results:

          estimate        se     zval    pval      ci.lb      ci.ub
intrcpt  -554.8145  116.4605  -4.7640  &lt;.0001  -783.0728  -326.5561  ***
year        0.2767    0.0580   4.7678  &lt;.0001     0.1630     0.3905  ***

---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>Followed by this model, I would also like to perform a scatterplot in R. So my script is:</p>

<pre><code>wi &lt;- 0.5/sqrt(dat$vi)
preds &lt;- predict(model_A, transf = transf.ilogit, addx=TRUE)
plot(year, transf.ilogit(dat$yi), cex=wi)
lines(year, preds$pred)
</code></pre>

<p>The plot I got is: 
<a href=""http://i.stack.imgur.com/7Ej3P.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/7Ej3P.png"" alt=""enter image description here""></a></p>

<p>Apparently, it doesn't seem right!. So my questions are:</p>

<ol>
<li><p>Did I use the right model with <code>rma.glmm</code>?</p></li>
<li><p>How could I weight individual study (<code>cex=wi</code>?)? How to calculate standard error for individual study?</p></li>
<li><p>How could I fit a right estimated line in scatterplot?</p></li>
</ol>

<p>Many thanks.</p>

<p>Updates:</p>

<p>Followed by Wolfgang's suggestions, I managed to rescale the bubble and get predicted line fitted (the model remains the same):</p>

<p><a href=""http://i.stack.imgur.com/u8N0t.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/u8N0t.png"" alt=""enter image description here""></a></p>

<p>Obviously, the line wasn't straight! Should I change model into polynomial regression? Or is that normal with this graph?</p>

<p>I tried polynomial model like:</p>

<blockquote>
  <p>model1&lt;-rma.glmm(xi=A, ni=Sample, measure=""PLO"", mods=~year+I(year^2))</p>
</blockquote>

<p>The error came with ""Error in print(model1) : 
  error in evaluating the argument 'x' in selecting a method for function 'print': Error: object 'model1' not found""</p>

<p>And I tried another model:</p>

<blockquote>
  <p>model2: model2&lt;-rma.glmm(xi=A, ni=Sample, measure=""PLO"", mods=~year+year^2)</p>
</blockquote>

<p>I got exactly the same result as original model, which has only the year as covariate fitted. I am not sure where the problem is....</p>

<p>Many thanks!</p>

<p>Min</p>
"
"0.027972711943223","0.0284901441149095","194847","<p>I am building a regression model where I want to score/optimize/train 'over-predictions' to be twice costly as under predictions.  I am attempting to do this in R and hopefully with caret package.   </p>

<p>I don't even really know where to start or if this is feasible?  The models I would want to test this out on would be the <code>gbm</code> and <code>rf</code> method within caret.</p>

<p>I am not sure how to go about doing this.  I don't have an specific example handy, but if anyone can provide an example using the <code>Boston</code> dataset in the <code>MASS</code> package that would be helpful.  It's a lot easier for me to pick things up once I have a somewhat working example. (plus a simple statistical explanation of key things I should keep in mind, I would appreciate it)</p>

<p>I have tried to search this extensively on Internet, but I can't find a good practical example to get me started.   </p>

<pre><code>library(caret)
library(MASS)
rf1 = train(medv ~.-chas, method='rf', data=Boston)
# or any other example..
</code></pre>
"
"0.0484501583111509","0.0493463771219827","195021","<p>I am trying to test out a hand-rolled cross-validation procedure for ridge regression. </p>

<p>I've run the <code>glmnet</code> package which gives me an MSE bottoming out around log(lambda)=0.5. My hand-rolled one gives log(lambda) = 5. </p>

<p>I think I must've screwed up in the code somewhere, but I don't see it.</p>

<p>Using <code>glmnet</code>:</p>

<pre><code>require(glmnet)
X = scale(NIR)
L = 500 # lambda.max
V = 5 # number of folds
lSeq = seq(1, L, by=1)
K = 5 #folds

cvres = cv.glmnet(X, y, nfolds = K)
plot(cvres)
</code></pre>

<p>Hand-rolled:</p>

<pre><code>require(caret)
folds = createFolds( y = y, k=K)
l = seq(10,1000,10)

# For each of the folds folds[v]
# For each of the ridge parameter values l[i]

ridge_mse = vector()
for(i in 1:length(l)){
  beta = matrix(nrow=ncol(X), ncol=length(folds))
  fold_error = vector()
  for(v in 1:length(folds)){ #for each fold
    X_nv = X[-folds[[v]],]
    beta[,v] = solve(crossprod(X_nv) + l[i] * diag(nrow=ncol(X_nv))) %*% crossprod(X,y)
    beta_int = rbind(mean(y[-folds[[v]]]), beta) # with an intercept
    fold_error[v] = mean(  cbind(1, X[folds[[v]],]) %*% beta_int[,v] - y[folds[[v]]])^2
  }
  ridge_mse[i] = mean(fold_error) # mse for each ridge parameter value
}
plot(log(l),ridge_mse, type=""l"")
</code></pre>

<p>These are the plots of the MSE estimates (glmnet on the left). </p>

<p><a href=""http://i.stack.imgur.com/FEupE.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FEupE.png"" alt=""enter image description here""></a></p>
"
"0.0969003166223018","0.0986927542439653","195120","<p>I recently ran a beta regression model in R using the <code>betareg</code> package. I am modeling a continuous dependent variable (a fraction out of 1) that is bound between 0 and 1, as a function of a continuous variable that only takes on positive values. Model code, and code to generate residual vs. fitted is here:</p>

<pre><code>fit &lt;- betareg(y ~ x, data=d)
plot(residuals(fit) ~ fitted(fit))
</code></pre>

<p>The residual vs. fitted plot looks like this:
<a href=""http://i.stack.imgur.com/5hVHy.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5hVHy.png"" alt=""enter image description here""></a></p>

<p>So like... what is going on here. Is this normal for beta regression, or have I mis-specified my model somehow?</p>

<p>Histogram of dependent variable, <code>y</code>:
<a href=""http://i.stack.imgur.com/bB2wK.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bB2wK.png"" alt=""enter image description here""></a></p>

<p>Histogram of independent variable, <code>x</code>:
<a href=""http://i.stack.imgur.com/8T7gq.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/8T7gq.png"" alt=""enter image description here""></a></p>

<p>output from <code>summary(fit)</code></p>

<pre><code>Call:
betareg(formula = relEM ~ mat, data = d1)

Standardized weighted residuals 2:
    Min      1Q  Median      3Q     Max 
-2.0716 -0.3940 -0.1730  0.4468  2.0633 

Coefficients (mean model with logit link):
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.863468   0.062820  13.745   &lt;2e-16 ***
mat         -0.053734   0.005667  -9.482   &lt;2e-16 ***

Phi coefficients (precision model with identity link):
      Estimate Std. Error z value Pr(&gt;|z|)    
(phi) 0.261182   0.005735   45.54   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 

Type of estimator: ML (maximum likelihood)
Log-likelihood: 1.156e+04 on 3 Df
Pseudo R-squared: 0.03853
Number of iterations: 14 (BFGS) + 1 (Fisher scoring) 
</code></pre>
"
"0.122082923688585","0.130558241966773","195359","<p>I have a set of complex survey data with sampling weights. I am using the <code>svyglm()</code> function from the <code>survey</code> package in R to describe the relationship between 2 variables in a GLM. I am using the quasipoisson family because both variables are over-dispersed. </p>

<p>The GLM output is as follows:</p>

<pre><code>hlsereg &lt;- svyglm(formula = HLSEPALLACRESFIX ~ HLSE_ACRE, sbdiv, family = quasipoisson)

Survey design:
svydesign(id = ~1, weights = ~spwgtdividedby3, data = sportsbind)

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  5.489465   0.414979  13.228   &lt;2e-16 ***
HLSE_ACRE   -0.002744   0.001118  -2.454   0.0144 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for quasipoisson family taken to be 2.601914e+15)

Number of Fisher Scoring iterations: 12
</code></pre>

<p>I have used the <code>predict()</code> and <code>lines()</code> function to plot this model output:</p>

<pre><code>acreaxis &lt;- seq(0,2000,.1)
hlse = predict(hlsereg, list(HLSE_ACRE = acreaxis))
    plot(jitter(sportsbind$HLSE_ACRE,  amount = 2.5), jitter(sportsbind$HLSEPALLACRESFIX),pch = 16,  xlab = ""Acres"", ylab = ""Price per person per acre"",  xlim = c(0, 350), ylim = c(0,35), col=alpha(""red"",.35), font = 2, font.lab = 2)
    lines(acreaxis, hlse, lwd=4, col = ""red"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/3EUZ6.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/3EUZ6.png"" alt=""enter image description here""></a></p>

<p>This plots a line given by the regression output of an intercept at 5.5 and a very slow negative slope of -.003, but I'm uncertain if this is a correct representation of the line.</p>

<p>I have found others using the <code>predict(..., type = ""response"")</code> option, which is shown in various plots of quasipoisson models, including the one found by @Glen_b at <a href=""http://stats.stackexchange.com/a/177926/45582"">this question</a> and for <a href=""http://stats.stackexchange.com/questions/38201/problems-plotting-glm-data-of-binomial-proportional-data?rq=1"">binomial GLMs here</a>. The <code>predict.glm()</code> help page notes for the <code>type</code> argument that: ""The default is on the scale of the linear predictors; the alternative ""response"" is on the scale of the response variable."" I just don't understand what that means.  The ""response"" type yields a very different prediction line, which is curved and at a much higher value (note the scale of the y-axis, with an intercept at ~250):</p>

<pre><code>hlse = predict(hlsereg, list(HLSE_ACRE = acreaxis), type = ""response"")
plot(jitter(sportsbind$HLSE_ACRE,  amount = 2.5), jitter(sportsbind$HLSEPALLACRESFIX),pch = 16,  xlab = ""Acres"", ylab = ""Price per person per acre"",  xlim = c(0, 350), ylim = c(0,400), col=alpha(""red""),     font = 2, font.lab = 2)
lines(acreaxis, hlse, lwd=4, col = ""black"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/jnY9T.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/jnY9T.png"" alt=""enter image description here""></a></p>

<p>I have also tried to run a GLM using the negative binomial distribution, but despite inputting the quasipoisson coefficient values for starting values, the model can't find valid coefficients (I have purged all zeros from the data):</p>

<pre><code> hlsereg.nb &lt;- glm.nb(HLSEPALLACRESFIX~HLSE_ACRE,data = model.frame(sbdiv.scaledweights), start = c(5.45, -.003))
Error: no valid set of coefficients has been found: please supply starting values
In addition: Warning message:
glm.fit: fitted rates numerically 0 occurred 
</code></pre>

<p>My questions:</p>

<p>1) What is the most appropriate illustration of the GLM output from a quasipoisson family?<br>
2) If the negative binomial is more appropriate to describe this relationship, why can't it find a coefficient? If I figure out how to get it to find a coefficient, how would I visualize that output?</p>
"
"0.0419590679148345","0.056980288229819","195478","<p>Resampling is usually used to find the best tuning parameters for a model. However, for some models, such as linear regression model, there is no tuning parameters. In this case, what can we get from resampling on them?</p>

<p>In particular, in R caret package, you can train a linear regression model by using cross validation control function. In this case, how is the coefficient estimated? On the whole training sample? If so, what extra information can we get from applying CV on linear regression models?</p>

<p>Thank you.</p>
"
"0.0559454238864459","0.056980288229819","196586","<p>I would like to do a gene x environment interaction analysis in a matched (1-1) case control samples. I referred all related previous publications and in most of the papers authors used either STATA or SAS. I got few references for performing conditional logisitic regression in R, for example using survival (clogit) package. But I couldn't find any reference for adding interaction terms in conditional logistic models in R. Can someone help me with references for interaction analysis using conditional logistic regression in R?</p>
"
"0.0625488854200668","0.0637058989297032","196703","<p>For all my forecast models (<code>arima</code> with Fourier, <code>tbats</code>, <code>ets</code> and <code>stlf</code> from the ""forecast"" package in R) I use the following:</p>

<pre><code>model &lt;- auto.arima(x, xreg=fourierf(x, K=y, h=52)) 
</code></pre>

<p>or </p>

<pre><code>model &lt;- tbats(x) 
</code></pre>

<p>or </p>

<pre><code>model &lt;- ets(x)
</code></pre>

<p>or </p>

<pre><code>model &lt;- stlf(x)
</code></pre>

<p>then</p>

<pre><code>forecast(model, h=52)
</code></pre>

<p><code>h=52</code> as that takes my data to the end of this quarter.</p>

<p>I also used multiple regression separately. </p>

<p>I then decided to use ARIMA with the dummy variables I'd used in my regression model:</p>

<pre><code>model &lt;- auto.arima(x, xreg=dummy)
</code></pre>

<p>then</p>

<pre><code>forecast(model, xreg=dummy, h=52)
</code></pre>

<p>However it doesnt matter if I use <code>h=52</code> or <code>h=1</code> or leave it blank, the forecast automatically seems to forecast the total length of my data set i.e. if I had 404 values it forecasted 404 values forward.</p>

<p>Just wondering why this happens and can I restrict the forecast to less?</p>
"
"NaN","NaN","196734","<p>I have a panel data set with binary dependent variable of 20,000 observations and 11 independent variables.  I ran a logistic regression with fixed effects and the model returns maximum log likelihood value of <code>-7417.845</code> and AIC equals <code>Inf</code>. I am not sure why here the AIC value goes to infinity?</p>

<p>I am using R package ""<a href=""https://cran.r-project.org/web/packages/glmmML/glmmML.pdf"" rel=""nofollow"">glmmML</a>"".</p>
"
"0.0634361479695551","0.0753778361444409","196829","<p>I have a dataset with 15 binary covariates and a continuous response variable bounded between 0 and 1. The binary variables represent correct or incorrect answers on a short test and the response variable is a measure of the same test takers performance on a related but more advanced and reliable test. I would like to select the best variables and weights to predict the score on the more advanced test. What would be the best way of doing this?</p>

<p>PS. I'm not a statistician but a computer scientist with only basic statistics and machine learning in my portfolio.</p>

<p>(Side note: One idea I had was to use some kind of logistic L1 or L2 regularized regression, however, glmnet does not seem to accept non-binary response variables when fitting a logistic model, which I guess is reasonable for normal use. The built-in glm function does accept a (0,1)-bounded response but does not perform regularization. If this approach seems reasonable, any tips on suitable packages or would I have to implement it myself? Other ideas I had was using ""normal"" regularized regression, or perhaps Principal Component Regression, however, I have tried both these and they give very different results and neither perform very well.)</p>
"
"0.0971882825368556","0.106600358177805","196901","<p>I'm trying to figure out how to find the marginal effect of an interaction term from a restricted cubic spline in a non-linear model.  The post <a href=""http://stats.stackexchange.com/questions/134526/nonlinear-effect-in-an-interaction-term"">Nonlinear effect in an interaction term</a> is a good start on modeling the nonlinear effects and how to get plots, but does not address finding the marginal effect.  </p>

<p>The package <a href=""http://maartenbuis.nl/software/postrcspline.html"" rel=""nofollow"">postrcspline</a> in <code>STATA</code> has a function <a href=""http://repec.org/bocode/m/mfxrcspline.html"" rel=""nofollow"">mfxrcspline</a> which ""displays the marginal effect of a restricted cubic spline,""
 which is exactly what I am after. (See Figure 1 below)  </p>

<p>R does not seem to offer this feature as conveniently ,so I'm trying to figure out how to get these same results.</p>

<p>As I understand it, suppose I have a multi-variable regression with restricted cubic splines and an interaction:</p>

<p>$$y = \beta_{0} + \beta_{1}x1 + \beta_{2} \mathcal{f}(x2) + \beta_{3} \mathcal{f}(x2) \cdot x1 + \epsilon$$</p>

<p>where $\mathcal{f}(x2)$ is a spline of the time-series (year)</p>

<p>The marginal effect of $\frac{\partial y}{\partial x1}$ is:</p>

<p>$$\frac{\partial y}{\partial x1} = \beta_{1} + \beta_{3} \mathcal{f}(x2)$$</p>

<p>where $\beta_{3}$ is the coefficient on the spline and $ \mathcal{f}(x2)$ is a design matrix for each year in the regression that causes the slope to change for each $y$.  </p>

<p>To say in words, I would like to find the marginal effect of $y$ for each year $x2$ in the spline given $\beta_{3}$.  </p>

<p>In other words, it shows for each value of the spline variable how much the expected value of your explained variable changes for a unit change in the spline variable. It is the first derivative of the curve.</p>

<p>This appears to be simple matrix multiplication to plot the marginal effect, but I'm not sure how to statistically do this.  </p>

<p>Here is a plot to illustrate what I'm after:</p>

<p><strong>Figure 1:</strong> The left plot shows the results of the regression using a restricted cubic spline and the right provides the marginal effect--note the changes on the y-axis.
<a href=""http://i.stack.imgur.com/uqcX4.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/uqcX4.png"" alt=""Figure 1""></a></p>

<hr>

<p>Here is an R example to demonstrate the nonlinear effect from the regression (left plot in Figure 1):</p>

<pre><code>library(rms)
set.seed(5)
# Fit a complex model and approximate it with a simple one
x1 &lt;- runif(200)
x2 &lt;- runif(200)
y &lt;- x1 + x2 + rnorm(200)
f &lt;- ols(y ~ x1 + rcs(x2,4)  + rcs(x2,4)*x1)
ddist &lt;- datadist(x1,x2)
options(datadist='ddist')
plot(Predict(f))
</code></pre>

<p><a href=""http://i.stack.imgur.com/DAuXS.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/DAuXS.png"" alt=""enter image description here""></a></p>
"
"0.0839181358296689","0.0854704323447285","197001","<p>I am trying to follow the procedure offered by <a href=""http://www.jstor.org/stable/2082979?seq=1#page_scan_tab_contents"" rel=""nofollow"">Beck and Katz 1995</a> in a way that I also have a TSCS data with $T=100$ (time dimension) and $N=12$ (unit dimension). My data is not balanced, which means that for some time periods, not all units have observations. </p>

<p>I am using R, and I found a <code>pcse</code> package that does what I need. It calculates panel corrected standard errors which accounts for contemporaneous correlation of errors across units and unit level heteroskedasity of errors. However, the steps I have to take to calculate panel robust standard errors for this type of regression start with the need to correct for serial correlation of errors, if I understand it well. Particularly, that is what is recommended in <code>pcse</code> package documentation:</p>

<p><a href=""http://i.stack.imgur.com/GNowz.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/GNowz.png"" alt=""enter image description here""></a></p>

<p>So, I am lost trying to understand what I need to do. My options how I see them:</p>

<ol>
<li>Run simple OLS regression on my pooled panel data. </li>
<li><p>Test for serial correlation of error term using Durbinâ€“Watson test and examining ACF/PACF. In most cases, I will have AR(1) in errors. </p>

<ul>
<li>Either compute clustered standard errors - it should account for the fact that errors should be clustered on the unit variable. After this step, I would get robust standard errors, but I cannot use it in pcse estimation - I don't need the VCV of errors as an input for the <code>pcse</code> function, but the OLS <code>lm</code> object itself.</li>
<li>Or use Cochraneâ€“Orcutt transformation first, and then use transformed  model as an input for pcse estimation. I started doing it, but realized that after CO transformation, error term became serially independent, but had the kurtosis of 20 (normality assumption fails).</li>
</ul></li>
</ol>

<p>So, my options are not so suitable. How do you think I should approach this situation?</p>
"
"0.027972711943223","0.0284901441149095","198007","<p>Could someone clarify the purpose of the <em>drop in dispersion test</em> (<code>drop.test</code>) in the <a href=""https://cran.r-project.org/web/packages/Rfit/index.html"" rel=""nofollow"">Rfit package</a> in R? I think that it shows the difference in explanatory power between the full and reduced models in linear regression. I don't understand where 'dispersion' comes into it.</p>

<p>The output for the drop in dispersion test of my models is:  </p>

<pre><code>F-statistic = 1.90451, p-value = 0.14911... 
</code></pre>

<p>How would I interpret this other than not being significant?</p>

<p>NB. to clarify, I do not need help with the code, just the interpretation of the test output results! </p>
"
"0.0692289300613081","0.0705095093522208","198281","<p>I'm not sure this is the right Stack Exchange Community for the question: in case I'm wrong, please let me know. I would like to try using GP for regression: here is an example of data I need to fit</p>

<p><a href=""http://i.stack.imgur.com/2iuiY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/2iuiY.png"" alt=""enter image description here""></a></p>

<p>Note that, even if the data look quite regular, they are experimental data, thus I need to perform a regression, not an interpolation. I'd really not write my own R code to perform GP regression. I've seen there are tens of different packages, and I'm not an expert on the topic, so I don't know how to choose. Any suggestions? Or, any suggestions on how to choose? My requirements are:</p>

<ol>
<li>easy to use (I'm not an expert in GPR)</li>
<li>should scale to at least 4 predictors, one quantitative response </li>
<li>(nice to have) it should run in a reasonable time (minutes?) on a modern workstation with moderately sized samples (let's say, about half a thousand training points).</li>
</ol>

<p>PS if you cannot recommend a package over another, even knowing which package you usually use may help me.</p>

<p>PS2 you may reasonably object that the set of data seems simple enough that a smoothing spline may suffice. That's what I thought too. Turns out <code>smooth.spline</code> from package <code>stats</code> doesn't agree: after complaining that it set the smoothing parameter to a very large value, I get the following plot: </p>

<p><a href=""http://i.stack.imgur.com/HSeXB.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/HSeXB.png"" alt=""enter image description here""></a></p>

<p>the ""tail"" of the spline doesn't look great. Also, I think smoothing splines wouldn't extend so easily to the multivariate case.</p>
"
"0.101115324337403","0.110341853688094","198315","<p>I am currently getting slightly confused with how a rolling forecast should be setup in R, as in how the data should be organised in order to <em>train</em> and <em>test</em> my model. I feel there is a large gap in my understanding somewhere.</p>

<p>I have seen many examples of forecasting methods, but not many on time-series (using lagged variables) that go into detail, i.e. perform everything manually using <code>predict()</code>. Instead it normally just points to a built in R package. <strong>My question has to do with the training of the model - the alignment of the data for the training.</strong></p>

<p>I know that the regression equation (assuming I am performing a linear regression) looks like this:</p>

<p>$$ y_t = \beta_1 y_{t-1} + \beta_2 y_{t-2} + \beta_3 x_{t-1} + \beta_4 x_{t-2} + \varepsilon_t $$ </p>

<p>So I have my outcome variable, $y$, being explained by two of its own lagged values, plus two lagged values of a second variable, $x$. Each variable has its own coefficient, all of which my model is estimating.</p>

<p>Let's say I have a <em>data.table</em> (or data.frame), where each row consists of the data aligned according to the equation above. Each row is one day, and represents the given equation. I have five columns, and for this example say 200 rows/days.</p>

<blockquote>
  <p>Day  |  $y_t$  |  $y_{t-1}$  |  $y_{t-2}$  |  $x_{t-1}$  |  $x_{t-2}$</p>
  
  <p>001  | <em>Values</em> --></p>
  
  <p>002  | <em>Values</em> --></p>
  
  <p>003  | <em>Values</em> --></p>
</blockquote>

<p>I use the above equation as the formula to fit my model to obtain estimates for the four coefficients (neglecting $\varepsilon$) using a fixed 40-day time frame.</p>

<pre><code>model &lt;- lm(y ~ y_1 + y_2 + x_1, x_2,            ## my regression formula
            data = input_data[1:40])             ## my data.table 
</code></pre>

<p>Now I want to make a prediction using the fitted <code>model</code>.
I do this by using <code>predict()</code> in <strong>R</strong> as follows:
I take the next (41st) row of data, minus the outcome variable</p>

<pre><code>my_pred &lt;- predict(model, newdata = input_data[41][, outcome := NULL])
</code></pre>

<p>And then calculate my error:</p>

<pre><code>my_error &lt;- input_data[41, outcome] - my_pred
</code></pre>

<p>I then shift everything forward one row, so still a 40-day frame <code>input_data[2:41]</code> updating the coefficients for all variables and predicting the following outcome variable, <code>input_data[42]</code>. This is yielding terrible results for my model, with overall accuracies not much better than a naÃ¯ve forecast, i.e. random guessing.</p>

<p>Should I realign the data for the training segment, so that each row rather represents the data I had on that day? This would mean adding one more column, $x_t$.</p>

<p>Any other suggestions or comments?</p>

<p>Thanks.</p>
"
"0.0969003166223018","0.0986927542439653","198801","<p>Logistic Regression using R's <code>glm</code> package is giving me the following summary (snap of the few first variables). </p>

<p><strong>My Data Set:</strong></p>

<ul>
<li>Dimensions: 1252 rows and 224 columns (after using model.matrix). The Data  has been standardized.</li>
<li>Response variable is binary.</li>
<li>Trying to predict if an employee will leave the company, based on employee attributes </li>
</ul>

<p><a href=""http://i.stack.imgur.com/Ai42v.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Ai42v.png"" alt=""enter image description here""></a></p>

<p><strong>My Understanding:</strong></p>

<p>The model does not give a good fit because:</p>

<ol>
<li>Residual Deviance > Null Deviance. </li>
<li>p.value = 1 - pchisq(3676.5, 817) turns out to be 0.</li>
<li>The first warning about 0 or 1 fitted probability message suggests that due to some predictor(s) the model might be giving perfect predictions </li>
<li>Second warning on â€˜rank deficiencyâ€™ suggests that there might be predictors that are linearly dependent on one another.</li>
</ol>

<p><strong>My Questions:</strong></p>

<ol>
<li>How can I improve the model? I would like to see Residual Deviance &lt; Null Deviance. I will invest time on dropping the linearly dependent variables in the model, but is there anything I should do first to test the â€˜modelâ€™ itself, before revisiting my data? I am asking this because SVM worked quite well on the same data set. </li>
<li>Why do I have such extreme coefficient values?</li>
<li>Many answers to other posts state that â€˜AICâ€™ is used to compare different</li>
<li>The summary parameters (coefficients , std error and p-values) for many dummy factors obtained via model.matrix, like GSS_SEXM,  is shown as 'NA'. Why is it so? </li>
<li>logistic models. What is meant by â€˜differentâ€™ here? Models trained on different data sets that bear different coefficients, like say different set of attributes? </li>
</ol>
"
"0.027972711943223","0.0284901441149095","199375","<p>I'm struggling to find a regularized regression model with a percentage as response variable. I've already seen the glmnet package in R and it seems that there is not way to handle a proportion as response variable on this kind of models.</p>

<p>How can I do a lasso regularization for a glm with Beta distributed response in R?</p>
"
"0.0791187772129236","0.080582296402538","199688","<p>I am looking for R code or package to fit some monotonic regression function to 2D (resp. 3D) data, with additional constraints that are ""fixed values"". I found the SCAM package for fitting monotonic splines. However, I do not know how to add my ""fixed values"" additional constraints.</p>

<p>The covariates are $\{(x_1^{(i)},x_2^{(i)})\}_{i=1}^n \in \mathbb{R}^2$ (resp. $(x_1^{(i)},x_2^{(i)},x_3^{(i)})\}_{i=1}^n \in \mathbb{R}^3$). </p>

<p>The responses are $\{y^{(i)}\}_{i=1}^n \in \mathbb{R}$. </p>

<p>I also have:</p>

<ul>
<li>one data point $X^{min}=(x_1^{min},x_2^{min})$ (resp. $(x_1^{min},x_2^{min},x_3^{min})$) with response $y^{min}$, </li>
<li>one data point $X^{max}=(x_1^{max},x_2^{max})$ (resp. $(x_1^{max},x_2^{max},x_3^{max})$)
with response $y^{max}$.</li>
</ul>

<p>As the notations suggest, I know that these two data points represent the (unique) min and the (unique) max of the data set, in the sense of the response $y$.</p>

<p>I want to fit a smooth function $f$ to these data, so that:</p>

<ul>
<li>$f$ is monotonic,</li>
<li>$f$ has its unique min at $X^{min}$ with value $y^{min}$ and its unique max at $X^{max}$ with value $y^{max}$,</li>
<li>$grad f (X^{min}) = grad f (X^{max}) = 0$</li>
</ul>

<p>In other words, I want to fix the values of the smoothing function and of its derivatives at two points.</p>

<p>Thank you for your help!</p>
"
"0.0625488854200668","0.0637058989297032","199912","<p>I have a large dataset with 4000 variables and 15000 observations. I am looking to build a predictive model using logistic regression. I believe that the glmnet package (using elastic net) is the best tool to use with such a large set of variables. Every variable of the 4000 is a moving average. I have split the dataset into two - training and testing.</p>

<p>When I run the code with glmnet I find something unusual happening. As I increase the number of variables for glmnet to select the model probabilities get more and more extreme which causes the misclassification rate to converge to 0%. I realise something is wrong but I cannot figure what it is.</p>

<p>Here is the code I have used:</p>

<pre><code>x &lt;- as.matrix(training[1:4000])
newx &lt;- as.matrix(testing[1:4000])

model &lt;- cv.glmnet(x, y, alpha = 0.5, family = 'binomial')

predict(model, type = ""coefficients"",s = model$lambda.min)
predict(model, newx, type= ""response"",s = model$lambda.min)
</code></pre>

<p>Is this overfitting?
I also read that categorical variables need to worked around with glmnet - none of the 4000 are categorical but they are grouped by external categorical vars.</p>

<p>I'm desperate for some help!</p>
"
"0.0839181358296689","0.0759737176397586","199978","<p>I've been building a logistic regression model (using the ""glm"" method in caret). The training dataset is extremely imbalanced (99% of the observations in the majority class), so I've been trying to optimize the probability threshold during the resampling process using the train function from the caret package as described in this example of a svm model: <a href=""http://topepo.github.io/caret/custom_models.html#Illustration5"" rel=""nofollow"">Illustrative Example 5: Optimizing probability thresholds for class imbalances.</a></p>

<p>The idea is to get the classification parameters for different values of the probability thershold, like this:</p>

<pre><code>threshold   ROC    Sens   Spec   Dist   ROC SD  Sens SD  Spec SD  Dist SD
 0.0100     0.957  1.000  0.000  1.000  0.0366  0.0000   0.0000   0.0000 
 0.0616     0.957  1.000  0.000  1.000  0.0366  0.0000   0.0000   0.0000 
 0.1132     0.957  1.000  0.000  1.000  0.0366  0.0000   0.0000   0.0000 
 0.1647     0.957  1.000  0.000  1.000  0.0366  0.0000   0.0000   0.0000 
  ...        ...                  ...                      ...      ...
</code></pre>

<p>I noticed that the 'glm' method in caret uses 0.5 as the probability cutoff value as can be seen in the predict function of the model:</p>

<pre><code>code_glm &lt;- getModelInfo(""glm"", regex = FALSE)[[1]]
code_glm$predict
    function(modelFit, newdata, submodels = NULL) {
                    if(!is.data.frame(newdata)) newdata &lt;- as.data.frame(newdata)
                    if(modelFit$problemType == ""Classification"") {
                      probs &lt;-  predict(modelFit, newdata, type = ""response"")
                      out &lt;- ifelse(probs &lt; .5,
                                    modelFit$obsLevel[1],
                                    modelFit$obsLevel[2])
                } else {
                  out &lt;- predict(modelFit, newdata, type = ""response"")
                }
                out
              }
</code></pre>

<p>Any ideas about how to pass a grid of probability cutoff values to the predict function shown above to get the optime cutoff value?</p>

<p>I've been trying to adapt the code from the <a href=""http://topepo.github.io/caret/custom_models.html#Illustration5"" rel=""nofollow"">example shown in the caret website</a>, but I haven't been able to make it work. I think I'm finding difficult to understand how caret uses the model's interfaces... </p>

<p>Any help to make this work would be much appreciated... Thanks in advance.</p>
"
"0.0419590679148345","0.056980288229819","200182","<p>I am wondering how I can present the results of nonparametric regression. I performed the nonparametric tests using R, and R package 'np'.</p>

<p>The commands used for this are</p>

<blockquote>
  <p>freq &lt;- npreg(Respno ~ Colony + Localden + Agg.prop, regtype = ""ll"",bwmethod = ""cv.aic"",gradients = TRUE, data = resp)</p>
  
  <p>summary(freq2)</p>
  
  <p>npsigtest(freq2)</p>
</blockquote>

<p>Using the last command, 'npsigtest', I get results like this</p>

<blockquote>
  <p>npsigtest(freq)</p>
  
  <p>Kernel Regression Significance Test
  Type I Test with IID Bootstrap (399 replications, Pivot = TRUE, joint = FALSE)</p>
  
  <p>Explanatory variables tested for significance:</p>
  
  <p>Colony (1), Localden (2), Agg.prop (3)</p>
  
  <p>Colony Localden Agg.prop</p>
  
  <p>Bandwidth(s): 21.88052 5956578 0.3183519</p>
  
  <p>Individual Significance Tests</p>
  
  <p>P Value: </p>
  
  <p>Colony 0.0025063 ** </p>
  
  <p>Localden &lt; 2.22e-16 *** </p>
  
  <p>Agg.prop 0.0802005 .</p>
</blockquote>

<p>How do I present this data in a scientific paper? For the simple linear regression results, I included the n, df, t and P. </p>

<p>Thank you, any advice would be greatly appreciated!!</p>
"
"0.0884574820723792","0.0900937462695559","200304","<p>I am trying to understand if there is a way to approximate what portion of the variance explained is being contributed by each independent variable in a random forest model. Just for illustration, I am borrowing the following model from the Stanford StatLearning class notes. This builds a random forest model for predicting median housing prices in Boston using the dataset provided with the <code>MASS</code> package.</p>

<pre><code>require(randomForest)
require(MASS)
set.seed(101)
dim(Boston)
train=sample(1:nrow(Boston),300)
</code></pre>

<p>Fitting the model (just using a simple model here without any validation just for illustration)</p>

<pre><code>rf.boston=randomForest(medv~.,data=Boston,subset=train)
rf.boston
</code></pre>

<p>I get the following output</p>

<pre><code>Call:
 randomForest(formula = medv ~ ., data = Boston, subset = train) 
               Type of random forest: regression
                     Number of trees: 500
No. of variables tried at each split: 4

          Mean of squared residuals: 12.34243
                    % Var explained: 85.09
</code></pre>

<p>Now <code>R</code> tells me that this model explains 85.09% variance in median housing prices. Additionally, I can run the <code>importance</code> command to figure out what variables turned out to be ""significant"" in my model.</p>

<pre><code>importance(rf.boston)

        IncNodePurity
crim        1487.1777
zn           142.0280
indus        965.7756
chas         234.6918
nox         1741.9305
rm          7435.3378
age          655.6031
dis         1357.3411
rad          316.3278
tax          794.0953
ptratio     1858.7183
black        455.5382
lstat       6947.9121
</code></pre>

<p>Is there a way to use these two pieces of information (or using some other approach) to tell us what percentage of <code>85.09</code> was explained by <code>crim</code>, <code>zn</code> and so on. </p>

<p>My goal here is to show this as a 100% stacked bar graph ordered by variable importance illustrating major drivers of the dependent variable (median housing prices in this example). Overall, I want to see if we can get outputs akin to shapely value regression  as shown <a href=""http://www.predictiveanalyticsworld.com/sanfrancisco/2013/pdf/Day2_1550_Reno_Tuason_Rayner.pdf"" rel=""nofollow"">here</a> (esp slide 21) using random forests.</p>
"
"0.0625488854200668","0.0637058989297032","200477","<p>I am conducting multiple imputation by chained equations in R using the MICE package, followed by a logistic regression on the imputed dataset.</p>

<p>I need to compute a 95% confidence interval about the predictions for use in creating a plotâ€”that is, the grey shading in the image at this link.</p>

<p><a href=""http://imgur.com/guLEyTQ"" rel=""nofollow"">http://imgur.com/guLEyTQ</a></p>

<hr>

<p>I followed the approach described in the answer to this question...</p>

<p><a href=""http://stats.stackexchange.com/questions/66946/how-are-the-standard-errors-computed-for-the-fitted-values-from-a-logistic-regre"">How are the standard errors computed for the fitted values from a logistic regression?</a></p>

<p>...which uses the following lines of code to yield the std.er of prediction for any specific value of the predictor:</p>

<pre><code>o &lt;- glm(y ~ x, data = dat)
C &lt;- c(1, 1.5)
std.er &lt;- sqrt(t(C) %*% vcov(o) %*% C)
</code></pre>

<p>But of course <strong>I need to adapt this code to the fact that I am using a model resulting from multiple imputation</strong>.  In that context, I am not sure <strong><em>which</em></strong> variance-covariance matrix (corresponding to â€œvcov(o)â€ in the above example) I should be using in my equation to produce the ""std.er"".</p>

<hr>

<p>Based on the documentation for MICE I see three candidate matrices:</p>

<ul>
<li><p>ubar - The average of the variance-covariance matrix of the complete data estimates.</p></li>
<li><p>b - The between imputation variance-covariance matrix.</p></li>
<li><p>t - The total variance-covariance matrix.</p></li>
</ul>

<p><a href=""http://www.inside-r.org/packages/cran/mice/docs/is.mipo"" rel=""nofollow"">http://www.inside-r.org/packages/cran/mice/docs/is.mipo</a></p>

<p>Based on trying all three, the b matrix seems patently wrong, but both the t and the ubar matrices seem plausible.  Can anybody confirm which one is appropriate?</p>

<p>Thank you.</p>
"
"0.0897122608032513","0.0989860468793906","200703","<p>I'm using matched pairs logistic regression (1-1 matched case-control; Hosmer and Lemeshow 2000) to model differences between vegetation selected at nest sites vs. paired random sites. To do this, I created a data frame that contained the difference in vegetation measurements between nest and random sites (so nest minus random) and used R to fit a logistic regression model, using a vector of all 1's as the 'Response' and a no-intercept model.</p>

<p>Here's the data frame (I only include 1 of the covariates, grass density, for the example):</p>

<pre><code>nest&lt;-structure(list(VerGR = c(1.380952381, 1.952380953, 2.666666667, 
-3.809523809, 2.428571428, 2.142857143, 0.142857143, 2.095238095, 
1.952380952, 3.333333334, 3.190476191, -2.857142857, 2.857142858, 
-1.666666667, 0.523809524, 4.761904762, 0.571428571, 2.238095238, 
-2.809523809, 0.857142857, 1.523809524, -2.476190476, -0.428571428, 
-5.190476191, 4.142857143, 2.857142858, -2.476190476, 4.095238096, 
1.428571428, 1.714285714, -2.80952381, 3.142857143, 2.809523809, 
7.238095238, 2.523809523, 2.333333333, -0.095238096, -0.095238096, 
-0.142857143, 4.047619048, 4.761904759, -1.285714285, -1.190476191, 
2.523809524, -2.095238095, -2, 4.761904761, 8.952380952, 1.095238096, 
5.666666666, -0.714285714, 0, 2.809523809, -0.238095239, 3.666666667, 
0.904761905, -4.952380952, -3.666666667, 2, -0.619047619, 4.523809524, 
1.523809524, 4.619047619, 6.142857143, 3.19047619, -2.190476191, 
-1.666666667, 2.714285714, -1.285714286, 2.857142857, 2.761904762, 
2.809523809, -7.142857139, -5.952380949, -1.19047619, 1.523809524, 
-0.38095238, 5.571428571, 5.238095239, 2.047619048, 7.857142857, 
0.61904761, 2.523809524, -1.190476191), Response = c(1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 
1L, 1L)), .Names = c(""VerGR"", ""Response""), class = ""data.frame"", row.names = c(NA, 
-84L))
</code></pre>

<p>And the no-intercept logistic regression models I am running:</p>

<pre><code>grass.mod &lt;- glm(Response ~ VerGR - 1, data=nest, family=""binomial"")
grass2.mod &lt;- glm(Response ~ VerGR + I(VerGR^2) - 1, data=nest, family=""binomial"")
</code></pre>

<p>For the most part the models run fine, and give the same parameter estimates as models implemented using the 'clogit' function from the survival R package. The data set for the clogit models is slightly different, with Responses = 1 (nest) or = 0 (random point), and includes a column called 'PairID' to indicate nest-random pairs. Here's what the clogit models look like:</p>

<pre><code>library(survival)
grass.mod.clog &lt;- clogit(Response ~ VerGR + strata(PairID), data=full)
grass2.mod.clog &lt;- clogit(Response ~ VerGR + I(VerGR^2) + strata(PairID), data=full)
</code></pre>

<p>But when I run the glm's, I get these 2 warnings if using a quadratic term:</p>

<pre><code>Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
</code></pre>

<p>I'm able to satisfy the first warning if I use more iterations in the glm formula, but I'm not sure what is happening with the second warning. I would be glad to use the 'clogit' function (which works with quadratic terms), but I'm unsure how to create prediction plots to visually display the data when going that route. Any suggestions?</p>

<p>Thanks,
Jay</p>
"
"0.027972711943223","0.0284901441149095","201440","<p>I have two time series and I want to check the relationship between them. I would like to use vector autoregression (VAR) model to do this. </p>

<p>I'd like to specify the model so that both variables will be explained by the lagged values of both itself and the other variable.
Moreover, (and here I encountered the actual trouble,) one of the time series variables, x, will need to depend on the <strong>same period</strong> value of the other variable y. </p>

<p>In R, can I somehow use the package ""vars"" to do this?
I tried to look at the documentation but I could only figure out how to include lagged variables in the model.</p>

<p>Or is there some other (easy) way to do this? Preferably in R.</p>
"
"0.0484501583111509","0.0493463771219827","201893","<p>I'm using the function <code>randomForest</code> in R's <code>randomForest</code> package to do a regression. However, when I'm trying to include an interaction term in the following codes:</p>

<pre><code>library(MASS)
library(randomForest)
Boston_f &lt;- within(Boston, factor(rad))
mdl &lt;- randomForest(lstat ~ rad * . , data = Boston_f)
</code></pre>

<p>The result <code>mdl$term</code> does include interaction, but if I peek into the trees that <code>mdl</code> is using, </p>

<pre><code>getTree(mdl, 1, T)
</code></pre>

<p>I cannot find any split variable using interaction term.</p>

<p>Does anyone know how to include interaction term using <code>randomForest</code> or other function?</p>
"
"0.0570990591522943","0.0697863157798853","202181","<p>I'm getting some odd coefficients when I apply <code>lm</code> to dates that have been processed and rounded using the <code>lubridate</code> package.  MWE:  </p>

<pre><code>library(ggplot2)
library(lubridate)
library(dplyr)

lakers$month &lt;- ymd(lakers$date) %&gt;% round_date(unit = 'month')
items_by_month &lt;- lakers %&gt;% group_by(month) %&gt;% summarize(count = n()) %&gt;%
    mutate(count = count / 1000)

ggplot(data = items_by_month, aes(x = month, y = count)) + 
    geom_line() +
    stat_smooth(method = 'lm', data = items_by_month)

model &lt;- lm(data = items_by_month, count ~ month)
summary(model)
time &lt;- max(items_by_month$month) - min(items_by_month$month)
coef(model)['month'] * as.numeric(time)
</code></pre>

<p>The plot indicates that <code>ggplot</code>, at least, understands what's going on with the regression model.<br>
<a href=""http://i.stack.imgur.com/HYTks.png""><img src=""http://i.stack.imgur.com/HYTks.png"" alt=""Plot with monthly totals and regression line""></a></p>

<p>But in <code>summary(model)</code> the coefficient on <code>month</code> is on the order of 10^-7, which is about 5 orders of magnitude too small:  the plot shows an increase of about 2.5 between the first and last dates, but the last line shows an increase of about 2.5 * 10^-5.  </p>

<p>Note that I've divided the <code>count</code> column by 10^3, in order to get values that are easier to read (and closer to my actual use case).  But that shouldn't effect either the plot or <code>lm</code>.  Also, I know there are more sophisticated techniques than linear regression for analyzing time series data; but I'm just looking at gross trends over time, not factor out seasonal patterns, etc.  </p>
"
"0.0740088392978143","0.0753778361444409","202264","<p>I am doing a comparison between mlogit in R and statsmodels in python and have had trouble getting them to produce the same result. I'm wondering if the difference is a result of libraries or I am specifying something incorrectly. Any help would be appreciated.</p>

<p>I am using the ""TravelMode"" dataset to test the two.
In R:
</p>

<pre><code>&gt; library(""mlogit"")
&gt; library(""AER"")
&gt; data(""TravelMode"", package=""AER"")
&gt; write.csv(TravelMode, ""travelmode.csv"")
&gt; TM &lt;- mlogit.data(TravelMode, choice = ""choice"", shape = ""long"", 
                    chid.var = ""individual"", alt.var = ""mode"", drop.index = TRUE)
&gt; TMlogit = mlogit(mFormula(choice ~ vcost), TM)
&gt; summary(TMlogit)
Call:
mlogit(formula = mFormula(choice ~ vcost), data = TM, method = ""nr"", 
    print.level = 0)

Frequencies of alternatives:
    air   train     bus     car 
0.27619 0.30000 0.14286 0.28095 

nr method
4 iterations, 0h:0m:0s 
g'(-H)^-1g = 0.000482 #'
successive function values within tolerance limits 

Coefficients :
                    Estimate Std. Error t-value  Pr(&gt;|t|)    
train:(intercept) -0.3885180  0.2622157 -1.4817 0.1384272    
bus:(intercept)   -1.3712065  0.3599380 -3.8096 0.0001392 ***
car:(intercept)   -0.8711172  0.3979705 -2.1889 0.0286042 *  
vcost             -0.0138883  0.0055318 -2.5106 0.0120514 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Log-Likelihood: -280.54
McFadden R^2:  0.011351 
Likelihood ratio test : chisq = 6.4418 (p.value = 0.011147)
</code></pre>

<p>In statsmodels:
</p>

<pre><code>&gt; import pandas as pd
&gt; import statsmodels.formula.api as smf
&gt; TM = pd.read_csv('travelmode.csv')
&gt; TM = pd.concat([TM, pd.get_dummies(TM['mode'])], axis=1)
&gt; TMlogit = smf.mnlogit('choice ~ train + bus + car + vcost -1', TM)
&gt; TMlogit_fit = TMlogit.fit()
Optimization terminated successfully.
         Current function value: 0.550273
         Iterations 6
&gt; TMlogit_fit.summary()
&lt;class 'statsmodels.iolib.summary.Summary'&gt;
""""""
                          MNLogit Regression Results                          
==============================================================================
Dep. Variable:                      y   No. Observations:                  840
Model:                        MNLogit   Df Residuals:                      836
Method:                           MLE   Df Model:                            3
Date:                Thu, 17 Mar 2016   Pseudo R-squ.:                 0.02145
Time:                        15:04:48   Log-Likelihood:                -462.23
converged:                       True   LL-Null:                       -472.36
                                        LLR p-value:                 0.0001497
=================================================================================
y=choice[yes]       coef    std err          z      P&gt;|z|      [95.0% Conf. Int.]
---------------------------------------------------------------------------------
train            -0.3249      0.172     -1.891      0.059        -0.662     0.012
bus              -1.4468      0.205     -7.070      0.000        -1.848    -1.046
car              -0.7247      0.157     -4.603      0.000        -1.033    -0.416
vcost            -0.0105      0.002     -6.282      0.000        -0.014    -0.007
=================================================================================
""""""
</code></pre>

<p>I would think the values of the coefficients would be closer to each other when comparing between the two models. Any help would be appreciated.</p>
"
"0.111890847772892","0.113960576459638","202973","<p>Suppose I have a data set of <code>N</code> observations <code>(n = 1...N)</code> for out-of-sample estimation and values of ($y_n$). I have also <code>I</code> statistical models <code>(i= 1...I)</code> which every model has its own estimate on each data point ($\hat{y}^i_n$).</p>

<p>In addition I have a model selection method $\phi$ which would pick a model's estimate among the model set as its own according to its assessment on previous performance of the models ($\hat{y}^\phi_n = \min_i\{\hat{f_i}(y_n), i \in I\} $).</p>

<p>My claim should be ""model selection's performance is better than all models it picks estimates from"". I am trying to find a proper method to describe the statistical power of the model selection method, compared to individual models in the model set.</p>

<p>All individual models follow different assumptions, distributions and dependence structure. Some are iid, some have heteroskedasticity. Actually, there is no restriction on models except it should yield an estimate.</p>

<p>Some The models are employed on time series but what they do is asset pricing on different assets and contracts. But for a broaded audience I will make the following analogy.</p>

<p>Suppose you have a machine that predicts the scores on basketball matches. It does not only predict the final score, it also predicts a distribution of the scores throughout the time. It also predicts which player will score when.</p>

<p>Suppose you have many machines of this sort and all have different predictions. All of them had been right on some occasion (That is what statistics is after all right? No model is perfect.). </p>

<p>I am trying to figure out which machine is better at predicting what and when, using the previous performance of the machines. I can say stuff like 'oh machine A was good at predicting scores occured in the last 10 mins, but for the last 2 months model B became better'. </p>

<p>It turns out my estimates using the machines are better than any machine could do it alone in the long run. I checked for several error terms starting with MAPE and MSE. But I want to show that it is not a coincidence but a statistically significant fact. I have a fair sample size (~100k) over a good enough time period (5 years).</p>

<p>I fiddled with some thoughts about proportion of $\phi$ selecting the model with the lowest error and some logistic regression on that according to the criteria it uses to pick the models. But I lack the comprehensive knowledge on this domain of statistics.</p>

<p>ps. R package suggestions are also appreciated.</p>
"
"0.0634361479695551","0.0538413115317435","203359","<p>Consider the following heteroscedastic model:
$$y_i = f(x_i, \beta) + g(x_i, \theta)\varepsilon_i, i = 1, \ldots, n, \tag{1}$$
where $f(\cdot, \beta)$ is the regression function and $g(\cdot, \theta)$
is the variance function. For simplicity, assume the errors $\{\varepsilon_i\}$ are i.i.d. with mean $0$ and variance $\sigma^2$.</p>

<p>Regarding model $(1)$, I understand (but I am not quite sure) that the <code>gls</code> function in <code>nlme</code> package can be used (at least when $f$ is linear) to implement the iteratively reweighted least squares algorithm (Carroll, Ruppert, <em>Transformation and Weighting in Regression</em>, pp. 69). When I read the manual of <code>nlme</code>, it looks to me that <code>gls</code> function restricts the forms of $g$ to a very small class of functional forms. For example, given observations $\{(y_i, x_{i1}, x_{i2}): i = 1, \ldots, n\}$, is it
possible to use <code>gls</code> to fit the following special case of $(1)$:
$$y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \sqrt{\theta_0 + \theta_1 x_{i1}^2 + \theta_2 x_{i2}^2}\varepsilon_i, i = 1, \ldots, n$$
, where $\theta_0 &gt; 0, \theta_1 \geq 0, \theta_2 \geq 0$? If yes, how should I specify my own square-root variance functional form in <code>gls</code>? If
no, are there any other available R packages to implement IRLS algorithm?</p>
"
"0.0419590679148345","0.056980288229819","203417","<p>Since my original question was to R-code-specific I'm trying to rewrite it:</p>

<p>I want to make a regression where my dependent variable <code>y</code> should follow a log-normal-distribution influenced by the explanatory variable <code>x</code> where the mean and variance changes across the observation.</p>

<p>Since log-normal doesn't belong to the exponential-family I can't try a glm.
Then I found gamlss which should exactly do the trick.</p>

<p>I was looking for some paper explaining the theory a little deeper - especially in my case of log-normal and where all the parameters of the distribution are  functions of the explanatory variables.</p>

<p>First of all I would like to know if there is a formula like the one below for an ordinary linear regression to calculate <code>y</code> after fitting:</p>

<p><a href=""http://i.stack.imgur.com/5Xqte.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5Xqte.png"" alt=""enter image description here""></a></p>

<p>But my biggest problem is, that I have no idea on how to handle that the moments of <code>y</code> change across the observation of <code>x</code>.
So let's say I have the following:</p>

<p><a href=""http://i.stack.imgur.com/dv61k.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/dv61k.png"" alt=""enter image description here""></a></p>

<p>I'm trying to achieve the regression via R and its <code>gamlss</code>-package.</p>

<p>There I start with <code>gamlss(y~x,familiy=LOGNO())</code> and then one has the possibility the make <code>sigma</code> depending on <code>x</code> via <code>sigma.formula=~</code> but no option for <code>mu</code>. So is it even possible in general?</p>

<p>Here is my code:</p>

<pre><code>library(gamlss)

y&lt;-c(1495418, 1684470, 1997120, 1901727, 2070008, 2213829, 2364602, 2333710, 2491570, 2540110, 2620947, 2761075, 2943475, 2854544)
x&lt;-c(3932300, 4119100, 4354400, 4483752, 4585303, 4803234, 4989701, 5177605, 5380031, 5494672, 5606376, 5783627, 6015992, 6171564)

fm&lt;-gamlss(y~x,familiy=LOGNO())
summary(fm)
fitted(fm)
residuals(fm)
y-fitted(fm) #How come this aren't the residuals?

fitted(fm,""mu"")
fitted(fm,""sigma"")
</code></pre>
"
"0.115512844298478","0.1241856590838","203454","<p>Although I have visited this site several times, this is the first time I make a question, so be kind if it is not in a appropriate form.</p>

<p>My problem is part statistical and part R. I am trying to build a Cox PH model in order to make prediction of unemployment. I have a big dataset, N=32538 with covariates p=37 . I split this sample in 3 parts according to Hastie &amp; Tibshirani, train =50%, test=25% and validation = 25%. So, I now have a training set of N=16270 cases. I would like to reduce the number of predictors, but from what I know and have read, it is not wise to do any kind of stepwise elimination. Therefore, I am trying to perform a penalized cox regression, especially with LASSO, using the R package 'penalized'. </p>

<pre><code>library( penalized )
</code></pre>

<p>However, it seems that it cannot run...I am not sure why, but I suppose that either my laptop is not very powerful, or that the respective functions are not very efficient for such a bog dataset. </p>

<pre><code>optL1( Surv ( time, status ) ~ . , minlambda=5, fold=3, data=mydata )
optL1( Surv ( time, status ) ~ . , minlambda=5,maxlamda=15, fold=3, data=mydata )
</code></pre>

<p>As you can see, I specify minlambda in the first case and both min and max lambda in the second. If I leave it unspecified, it just crushes my whole OS. Now, my pc runs veeeeeeery slow, and after 3 hours ( the most I left it running ), although it seems still running, nothing at all was produced. Those familiar with this function, know that while it is running, it produces in the console ""what is going on"". That is , for every lambda that it checks, it shows it in the consore along with the according cvl( log-lik ). </p>

<p>In some cases, but not always,  it produces the well-known irritating message of memory error....</p>

<pre><code> Error: cannot allocate vector of size xxx Mb
</code></pre>

<p>My details :</p>

<pre><code>session(info)
R version 3.2.4 Revised  ( 2016-03-16 r70336)
Platform: x86_64-w64-mingw32/x64 ( 64 bit)
Running under: Windows &gt;= 8 x64 ( build 9200)
</code></pre>

<p>For now, I tried to run the function in subsets of the full dataset, and I ""managed"""" to make it run until  N=8000( the half sample).</p>

<p><strong>Question</strong> 1:</p>

<p>Do you now if I am doing something wrong in running the specific function, or it is an unsolved problem and I have to find another way to proceed ?  </p>

<p><strong>Question 2</strong></p>

<p>Do you know if there are any other packages in R, that can accommodate more efficient the penalized cox regression, and also be capable of making predictions ?</p>

<p>Many thanks!!
Giannis</p>

<p><strong>EDIT</strong></p>

<p>actually, as you can see, I used the classic formula for regression. Meaning, I used the 'dot' in order to include all the predictors in the model. Moreover, as you maybe have guessed, I have many categorical predictors. Do you think that it is better to add the variables all by name in the model, and specify the factors with :</p>

<pre><code>factor(var1) 
</code></pre>

<p>????</p>

<p>Because by reading the vignette( penalized) , I realized that they use only continuous predictors, leaving out of the model the categorical ones! </p>
"
"0.142731332931499","0.145371541135707","203785","<p>I'm using the <code>tgp</code> package in R for fully Bayesian Gaussian Process Regression, and it's great! I'm currently performing regression for experimental data coming from turbomachinery testing, and I'm using the <code>bgp</code> function. This function uses a GP prior with either a <code>linear</code> mean or a <code>constant</code> mean (respectively, option <code>meanfn=""linear""</code> or <code>meanfn=""constant""</code>, which is the default). Note that <code>tgp</code> allows the use of treed Gaussian priors, but for now I'm staying simple, so I'm using the <code>bgp</code> function which doesn't use regression trees, just ordinary Gaussian Processes.</p>

<p>I would like my posterior predictive mean to go to zero away from the training set data, for physical reasons. How can I impose that? I was thinking to set the prior over $\beta_0$ to a Normal distribution centered at 0 and with an extremely small variance, but I'm not sure how to do that. From <code>help(btgp)</code></p>

<pre><code>bprior Linear (beta) prior, default is ""bflat""; alternates include ""b0"" hierarchical Normal
prior, ""bmle"" empirical Bayes Normal prior, ""b0not"" Bayesian treed LMstyle
prior from Chipman et al. (same as ""b0"" but without tau2), ""bmzt"" a independent
Normal prior (mean zero) with inverse-gamma variance (tau2), and
""bmznot"" is the same as ""bmznot"" without tau2. The default ""bflat"" gives
an â€œimproperâ€ prior which can perform badly when the signal-to-noise ratio is
low. In these cases the â€œproperâ€ hierarchical specification ""b0"" or independent
""bmzt"" or ""bmznot"" priors may perform better
</code></pre>

<p>Default is the improper prior <code>""bflat""</code>, which is not what I want. If I use the <code>""b0""</code> hierarchical Normal prior, I guess I cannot set the mean and the variance because they should become additional hyperparameters to be determined in the Bayesian paradigm. Thus, I may go for <code>""bmzt""</code>, the independent Normal prior with zero mean. However, with this prior I cannot set the variance, which is again an hyperparameter. Basically, I want my prior mean function to be zero, so that away from the data, also the posterior predictive mean will be zero. Is there a way to achieve that?</p>

<p>EDIT: nobody wants to have a try? :) As my actual case is quite complicated, I wrote a small test case which illustrates the main problem, with the help of the <code>tgp</code> package author. NOTE: unless you have an optimized version of R, you may want to set <code>BTE = c(1000,10000,2)</code> in the call to <code>bgp</code>, or you may have to wait for a very long time to get an answer.</p>

<pre><code># clear the workspace
rm(list=ls())
gc()
graphics.off()

# set seed for reproducibility
set.seed(825)

# load required packages
library(tgp)
library(ggplot2)

# simulated data
x &lt;- seq(-1,1,len=100)
eps &lt;- rnorm(n=100,mean=0,sd=0.5)
y &lt;- -5*x^2+eps
ymean &lt;- mean(y)

# prediction points
xpred &lt;- seq(-20,20,len=100)

# fit GP
GPModel &lt;- bgp(X=x,Z=y,XX=xpred,meanfn = ""constant"", bprior=""bmzt"", 
BTE = c(2000,52000,2), tau2.p=c(1,10000), tau2.lam=""fixed"")    
ypred &lt;- GPModel$ZZ.mean 

# plots
ymean_vector &lt;- rep(ymean,100)
df &lt;- data.frame(x,y,xpred,ypred,ymean_vector)
p &lt;- ggplot(data=df)
p &lt;- p + geom_point(aes(x=x,y=y)) + 
    geom_line(aes(x=xpred,y=ypred),col=""blue"") +
    geom_line(aes(x=xpred,y=ymean_vector),col=""red"") +
    geom_line(aes(x=xpred,y=GPModel$ZZ.q1), col=""green"") + 
        geom_line(aes(x=xpred,y=GPModel$ZZ.q2), col=""green"")
p
</code></pre>

<p>The resulting plot is</p>

<p><a href=""http://i.stack.imgur.com/VjePX.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/VjePX.png"" alt=""enter image description here""></a></p>

<p>The mean response is the red line: the blue line is the GP posterior predictive mean, and the green lines give the 90% credible interval.Thus, outside the training data range, the data mean is indeed included in the 90% credible interval, but I would like the predictive mean to converge to it...I think that if I could find a way to set the standard deviation of the prior for $\beta_0$ to some  extremely small value, I would achieve what I want, but I don't know how to do it.</p>

<p>EDIT2: I can use either a multiplicative (separable) squared exponential kernel
or an additive squared exponential kernel.</p>

<pre><code>sep_Gaussian_Kernel &lt;- function(x,y,sigma,l) {
    prod(sigma*exp(-0.5*(abs(x-y)/l)^2))
}    

add_Gaussian_Kernel &lt;- function(x,y,sigma,l) {
    sum(sigma*exp(-0.5*(abs(x-y)/l)^2))/length(x)
} 
</code></pre>
"
"0.0938928011704452","0.0956296065296812","203816","<p>I am trying to duplicate the results from <code>sklearn</code> logistic regression library using <code>glmnet</code> package in R.</p>

<p>From the <code>sklearn</code> logistic regression <a href=""http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"" rel=""nofollow"">documentation</a>, it is trying to minimize the cost function under l2 penalty
$$\min_{w,c} \frac12 w^Tw + C\sum_{i=1}^N \log(\exp(-y_i(X_i^Tw+c)) + 1)$$</p>

<p>From the <a href=""https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet_beta.html#log"" rel=""nofollow"">vignettes</a> of <code>glmnet</code>, its implementation minimizes a slightly different cost function
$$\min_{\beta, \beta_0} -\left[\frac1N \sum_{i=1}^N y_i(\beta_0+x_i^T\beta)-\log(1+e^{(\beta_0+x_i^T\beta)})\right] + \lambda[(\alpha-1)||\beta||_2^2/2+\alpha||\beta||_1]$$</p>

<p>With some tweak in the second equation, and by setting $\alpha=0$, $$\lambda\min_{\beta, \beta_0} \frac1{N\lambda} \sum_{i=1}^N \left[-y_i(\beta_0+x_i^T\beta)+\log(1+e^{(\beta_0+x_i^T\beta)})\right] + ||\beta||_2^2/2$$</p>

<p>which differs from <code>sklearn</code> cost function only by a factor of $\lambda$ if set $\frac1{N\lambda}=C$, so I was expecting the same coefficient estimation from the two packages. But they are different. I am using the dataset from UCLA idre <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">tutorial</a>, predicting <code>admit</code> based on <code>gre</code>, <code>gpa</code> and <code>rank</code>. There are 400 observations, so with $C=1$, $\lambda = 0.0025$.</p>

<pre><code>#python sklearn
df = pd.read_csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
y, X = dmatrices('admit ~ gre + gpa + C(rank)', df, return_type = 'dataframe')
X.head()
&gt;  Intercept  C(rank)[T.2]  C(rank)[T.3]  C(rank)[T.4]  gre   gpa
0          1             0             1             0  380  3.61
1          1             0             1             0  660  3.67
2          1             0             0             0  800  4.00
3          1             0             0             1  640  3.19
4          1             0             0             1  520  2.93

model = LogisticRegression(fit_intercept = False, C = 1)
mdl = model.fit(X, y)
model.coef_
&gt; array([[-1.35417783, -0.71628751, -1.26038726, -1.49762706,  0.00169198,
     0.13992661]]) 
# corresponding to predictors [Intercept, rank_2, rank_3, rank_4, gre, gpa]


&gt; # R glmnet
&gt; df = fread(""http://www.ats.ucla.edu/stat/data/binary.csv"")
&gt; X = as.matrix(model.matrix(admit~gre+gpa+as.factor(rank), data=df))[,2:6]
&gt; y = df[, admit]
&gt; mylogit &lt;- glmnet(X, y, family = ""binomial"", alpha = 0)
&gt; coef(mylogit, s = 0.0025)
6 x 1 sparse Matrix of class ""dgCMatrix""
                    1
(Intercept)      -3.984226893
gre               0.002216795
gpa               0.772048342
as.factor(rank)2 -0.530731081
as.factor(rank)3 -1.164306231
as.factor(rank)4 -1.354160642
</code></pre>

<p>The <code>R</code> output is somehow close to logistic regression without regularization, as can be seen <a href=""http://stats.stackexchange.com/questions/203740/logistic-regression-scikit-learn-vs-statsmodels"">here</a>. Am I missing something or doing something obviously wrong?</p>

<p>Update: I also tried to use <code>LiblineaR</code> package in <code>R</code> to conduct the same process, and yet got another different set of estimates (<code>liblinear</code> is also the solver in <code>sklearn</code>):</p>

<pre><code>&gt; fit = LiblineaR(X, y, type = 0, cost = 1)
&gt; print(fit)
$TypeDetail
    [1] ""L2-regularized logistic regression primal (L2R_LR)""
    $Type
[1] 0
$W
            gre          gpa as.factor(rank)2 as.factor(rank)3 as.factor(rank)4         Bias
[1,] 0.00113215 7.321421e-06     5.354841e-07     1.353818e-06      9.59564e-07 2.395513e-06
</code></pre>

<p>Update 2: turning off standardization in <code>glmnet</code> gives:</p>

<pre><code>&gt; mylogit &lt;- glmnet(X, y, family = ""binomial"", alpha = 0, standardize = F)
&gt; coef(mylogit, s = 0.0025)
6 x 1 sparse Matrix of class ""dgCMatrix""
                     1
(Intercept)      -2.8180677693
gre               0.0034434192
gpa               0.0001882333
as.factor(rank)2  0.0001268816
as.factor(rank)3 -0.0002259491
as.factor(rank)4 -0.0002028832
</code></pre>
"
"0.0484501583111509","0.0493463771219827","204119","<p>There is a package in R called <code>pwr</code>. This is useful to make power analysis when designing the sampling of a project. here are few examples: </p>

<pre><code>library(pwr)
pwr.anova.test(k = 4, f = 0.5, sig.level = 0.05, power = .9)
pwr.2p.test(h = 0.5, sig.level =0.05, power = .9)
pwr.f2.test(u = 4, f2 = .5, sig.level = 0.05, power = .8)
</code></pre>

<p>However, is it possible to run a power analysis for a spline regression (or a generalized additive model (GAM))? I want to know how may organisms I would have to sample to detect an effect of selection, that is a shift in morphology of the beak of birds of only 0.5Â mm, given that my sig.level = 0.05 and that I have 4 species. </p>

<p>Also, Iâ€™m recapturing birds in a population each year since 2003. Is there a power calculation to estimate how many birds should I sample to get a probability of recapture of 25%? Iâ€™m running a recapture model in Bayesian statistics, so there is not a function in the package <code>pwr</code> that can do this. </p>
"
"0.0796117338651413","0.0900937462695559","204145","<p><strong>Background and Problem</strong></p>

<p>I have a question concerning a meta-analysis combining effects from between- and within-subject designs using log-odds ratios (OR) as the metric of interest. I am familiar with conducting meta-analyses and will be undertaking my calculations in R (using the <code>metafor</code> and <code>lme4</code> packages). To provide greater context, the studies in question ask research subjects to make a binary decision with respect to a personal preference across one of two conditions. In some cases, each participant is assigned to a single condition (making only a single binary response); in others, each subject takes part in both conditions (making two binary responses). For now, presume I have the raw data in all cases. The issue I face is how best to calculate an OR that is comparable across design and whether I should take the correlation between conditions into account for the within-subject designs.</p>

<p><strong>My Current Approach</strong></p>

<p>I presently use logistic regression to estimate the OR for between-subject designs. The slope represents the OR and the sampling variance can be calculated by squaring the SE of the slope coefficient. Using this approach produces estimates comparable to equations reported in common texts such The Handbook of Research Synthesis and Meta-Analysis, 2nd Edition (p. 243). I then extend this approach to use a multilevel logistic regression model including a random intercept by subject to estimate the OR for within-subject designs while account for the dependency between conditions. The OR and sampling variance are otherwise calculated in the same fashion. </p>

<p><strong>My Questions:</strong></p>

<p>With this in mind, I would like to ask:</p>

<ol>
<li>Is it reasonable to meta-analytically aggregate OR calculated using standard and multilevel logistic regression?</li>
<li>Would it be better to use standard logistic regression for both designs (ignoring the correlation between conditions for the within-subject designs)?</li>
</ol>
"
"0.0559454238864459","0.056980288229819","204158","<p>I would like to regress several explanatory variables (called X) on a dependent variable called Y where Y is strictly positive and continuous. It is left-censored at 0. Y represents how much a household is willing to pay to protect the environment in a specific area.  It is cross-section data.</p>

<p>I asked R to run a Tobit I model using <code>censReg</code> package or <code>AER</code> package:</p>

<pre><code>tobit &lt;- tobit(Y ~ X, left=0, data = mydata)
</code></pre>

<p>or</p>

<pre><code>tobit2 &lt;- censReg(Y ~ X, left=0, data = mydata)
</code></pre>

<p>and then</p>

<pre><code>plot(fitted(tobit), residuals(tobit))
</code></pre>

<p>When observing the residuals' plot, I see the distribution is not normal  whereas it's necessary that the errors from this regression be normally distributed and homoskedastic.</p>

<p>Does anyone know how I can test these errors for normality and heteroskedastcity in R with the specific censored data issue? Using <code>bptest()</code> from the <code>lmtest</code> package - the Breuch Pagan test- does not work. I know that in some programs, like Stata, it's pretty straightforward to test residuals, but I have no idea how in R.</p>

<p>Thank you! </p>

<p>Mareen</p>
"
"0.0395593886064618","0.040291148201269","204440","<p>I'm using the <code>auto.arima</code> function in R's <code>forecast</code> package to build an ARIMA model with external regressors. I have a non-seasonal monthly stationary time-series dataset as shown below:</p>

<pre><code>&gt; dim(tsdata)
[1] 95  4
&gt; head(tsdata)
                    y         x1         x2          x3
2007-02-01  0.0532113 -0.7547812 -1.1156320  1.15193457
2007-03-01 -0.4461565  0.5104070  1.2489777 -1.19172591
2007-04-01 -1.4087036  2.0866994  0.2835917  0.15941672
2007-05-01 -0.4960451 -1.9455242 -2.6847517 -0.06603252
2007-06-01  0.8025322 -2.9295067 -0.6049654  0.34332637
2007-07-01 -0.8053754 -0.2385492 -1.7850528 -1.29843072
</code></pre>

<p>I can use <code>auto.arima(tsdata[,1], xreg=tsdata[,2:4])</code> to fit a model with <code>x1</code>, <code>x2</code>, and <code>x3</code> as regressors. My question is, is there a way to model the interaction between external regressions?</p>
"
"0.0484501583111509","0.0493463771219827","204807","<p>My DV consists of a variable ranging between 0 and 1 (it's a percentage to be more precise) and I am using the <code>gbm</code> package to predict it. 
Given the nature of my DV I am assuming that using a Laplace distribution, in order to minimize the absolute error, might be better than using a guassian distribution, which minimizes the squared error. However, I am not sure my assumption is correct and I would like to know whether in theory my assumption is correct. </p>

<p>Additionally, on a later stage I might be interested in defining a cut-off point, <em>x</em>, for binary classification. If I aim to do that, which of the following would be the best alternative (I'm open for additional suggestions of course):</p>

<ul>
<li>spliting the data beforehand into ""high"" and ""low"" values (e.g.
<code>ifelse(DV &gt; x, 1, 0)</code>) and treating it as a
binary variable using <strong>adaboost</strong> or <strong>bernouli</strong> distributions </li>
<li>staying with
the original data and using a <strong>quantile regression</strong> with <em>x</em> as a cut-off
point</li>
<li>staying with the original data and using a <strong>gaussian</strong>/<strong>laplace</strong>
distribution and defining <em>x</em> a posteriori.</li>
</ul>

<p>Thanks a lot in advance!</p>
"
"0.0634361479695551","0.0538413115317435","204839","<p>Further to <a href=""http://stats.stackexchange.com/questions/200460/multiple-imputation-for-predictive-analysis-using-mice-package-in-r"">my prior question</a> on multivariable adjustment in regression models, using covariates which are available only for some cases, I have researched in some detail the main methods for limited dependent variables, including Heckman correction or tobit models. However, I fear that they do not apply to my issue, which has more to do with <strong>limited independent variables</strong>.</p>

<p>In particular, I am giving below an example of the dataset and the possible analysis in R (disregard the overfitting, it's just to make an example, my actual dataset has at least 10,000 cases):</p>

<pre><code>dep &lt;- c(8, 9, 21, -3, 4, 6, 9, 10, 8, 9, 11, 39, 91, 51, 38, 28, 21)
cov1 &lt;- c(68, 58, 42, 19, 39, 49, 29, 38, 25, 22, 19, 36, 39,90, 105, 73, 25)
cov2 &lt;- c(0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0)
cov3 &lt;- c(0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1)
cov4 &lt;- c(NA, NA, NA, NA, NA, NA, 56, 33, 45, 44, 56, 49, 36, 39, 40, 41, 59)
cov5 &lt;- c(NA, NA, NA, NA, NA, NA, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0)
mydata &lt;- data.frame(cbind(dep, cov1, cov2, cov3, cov4, cov5)) 
mydata

reg1 &lt;- lm(dep ~ cov1 + cov2, data = mydata, na.action = na.omit)
anova(reg1)
summary(reg1)

reg2 &lt;- lm(dep ~ cov1 + cov2 + cov3 + cov4 + cov5, data = mydata, na.action = na.omit)
anova(reg2)
summary(reg2)
</code></pre>

<p>What should I do to best adjust for covariates cov1, cov2, cov3, cov4 and cov5, having dep as dependent variable, given that cov4 and cov5 are available only for patients with cov3 = 1? </p>

<p>Should I discard all cases with cov3 = 0, or should I conduct two separate analyses and then pool the regression coefficients according to their standard error? Or is there any other more reasonable approach?</p>

<p>Unfortunately I did not find anything meaningful searching Google, Google Scholar, or PubMed:</p>

<p><a href=""https://www.google.it/search?q=limited+independent+variable&amp;nirf=limited+dependent+variable"" rel=""nofollow"">https://www.google.it/search?q=limited+independent+variable&amp;nirf=limited+dependent+variable</a></p>

<p><a href=""https://scholar.google.it/scholar?hl=en&amp;q=limited+independent+variable"" rel=""nofollow"">https://scholar.google.it/scholar?hl=en&amp;q=limited+independent+variable</a></p>

<p><a href=""http://www.ncbi.nlm.nih.gov/pubmed/?term=limited+independent+variable"" rel=""nofollow"">http://www.ncbi.nlm.nih.gov/pubmed/?term=limited+independent+variable</a>*</p>

<p>To further clarify what is at stake, this is my real problem: I want to create a clinical prediction score (to predict prognosis and future quality of life) for patients undergoing myocardial perfusion imaging (a non-invasive cardiac test used in subjects with or at risk for coronary artery disease). The imaging test follows immediately an exercise stress test in fit patients, and a pharmacologic stress test in those who are not fit. The latter test is worse than the former, and does not provide several important prognostic features (eg maximum heart rate, or workload), so I must include exercise test variables in the multivariable model. But if I do so, I lose more than 1000 patients who only underwent a pharmacologic stress test.</p>
"
"0.0323001055407673","0.0493463771219827","205395","<p>I'm conducting regression analysis on sleeping time data. The data is survey data and the answer possibilities are of type ""less than 4 h"", ""5 h"", ""6 h"", etc. so they can be thought to be interval censored answers of individuals' real sleep times (E.g. if a person sleeps 6.8 hours a night, s/he might answer ""7 h"". Then if someone answered ""7 h"" we know that the real sleep length is somewhere between 6.5 and 7.5 hours). The data contains repeated measurements. Every individual has answered the same question on 4 different studies, which are a couple of years apart from each other. This introduces a grouping factor, frailty, in survival regression terms.</p>

<p>I have searched without result an R package, a Python module or something else that can model survival regression with Weibull distribution, interval censoring and frailty. Does someone know one?</p>
"
"0.0685188709827532","0.0697863157798853","205453","<p>I don't have a working example, because this question is more conceptual.  Let's say I'm running a linear regression using the <code>plm</code> package in R on the relationship between graduating from college and getting lunch-subsidies as a child.  </p>

<p>I have panel data that includes observations from individuals that may be in the same family, so I add family-level fixed effects to allow for arbitrary correlations within the family.  </p>

<p>If I were to add race covariates, however, R would omit them because of multicollinearity in the family level.  </p>

<p>How would I test the hypothesis, then, that blacks have differential effects than whites, but still resolving the issues solved by fixed effects?  </p>

<p><strong>To answer the questions below:</strong></p>

<p>Let's say I have the following regression, attempting to test for the hypothesis that different races and sexes are helped differentially from a free lunch program insofar as it relates to graduating college:</p>

<pre><code>plm(graduate_college ~ free_lunch + black + male + hispanic + data=data, index=c(""mother_id""), model=""within"")
</code></pre>

<p>Mother_ID just tracks siblings from the same mother.  Now, if I want to test the hypothesis that blacks have different effects than whites as it relates to the effect of free lunch on graduating college, how would I test this?  My guess is to remove fixed effects, add clustered standard errors at the mother_id level and add an interaction term for black*free_lunch?</p>
"
"0.0395593886064618","0.040291148201269","205604","<p>I'm running a regression in R's <code>plm</code> package similar to this post <a href=""http://stackoverflow.com/questions/33155638/clustered-standard-errors-in-r-using-plm-with-fixed-effects"">Clustered standard errors in R using plm (with fixed effects)</a>. I.e. panel data with fixed effects and the within-model from <code>plm</code>.</p>

<p>My Question is the following: I'm trying to figure out how to cluster my standard errors according to a different variable than the variable called state from the dataset <code>Cigar</code>, which is seemingly automatically used by the <code>cluster = 'group'</code> option in <code>vcovHC</code>. Specifically, if I e.g. have a variable called <code>id</code>, how can I tell <code>vcovHC</code> to use it as my cluster?</p>

<p>A very related question is the process of how <code>vcocHC</code> is selecting the variable for clustering, is it always just the first column in the dataset?</p>
"
"0.0559454238864459","0.056980288229819","205664","<p>Nice and simple. I've spent two hours googling, reading cross validated, and several r blogs to attempt to find a simple method of outputting the representative tree in R.</p>

<p>I was attempting to demonstrate to a coworker that random forest was producing better results (better in accuracy and more reproducible) than his linear regression on the same data set. However he ultimately said it didn't matter because he'd given up trying to explain it to his superior. His boss wants to use the linear regression model because he can in-turn explain it to his superiors. While essentially they have to trust the output of the random forest.</p>

<p>I recall that it's possible to display a tree producted by a CART model, and in my googling I found you can also simply call plot() on the output of ctree from the cforest package. However I can't seem to find a way to plot the output of randomforest (or cforest) in the same fashion.</p>

<p>Is there a way to do this? Or alternatively is there a known way to extract the tree from the forest to plot using the existing tools?</p>
"
"0.148347707274232","0.156128199279917","206042","<p>I implement <code>n</code> permutations into a regression analysis, to test the model for stability. Thus I obtain <code>n</code> odds ratios (ORs) and <code>n</code> associated 95% CI intervals. </p>

<p>Each permutation represents a matched-pair study. We pair similar <code>case</code>'s with <code>control</code>'s and then run a conditional logistic regression to obtain a measure of association between the outcome of interest and exposure variable (treatment status).</p>

<p>Taking the following example I have implemented into a <code>R</code> script.
In short what I have done is:</p>

<ol>
<li>Take a portion of the a given population</li>
<li>We assign at dummy variable to the population (1/0) to indicate treatment status</li>
<li>based on a set of parameters we pair those with treatment status==1 to equivalent treatment status==0</li>
<li>we define an outcome of interest that we wish to measure if treatment had an effect on the outcome</li>
<li>We conduct a logistic regression to determine the ORs associated with treatment status</li>
<li>we repeat this n time, each time obtaining an ORs and associated 95% confidence interval</li>
</ol>

<p><strong>But what I am not sure, is how I can report on the spread of my data. I generate a different odds ratio and 95% CI for each permutation.</strong></p>

<p>Taking the following hypothetical example, we run a simulation 100 times. It only takes a minute to simulate.</p>

<p>We take an worked exampled from the <a href=""https://cran.r-project.org/web/packages/Matching/Matching.pdf"" rel=""nofollow"">Matching package</a> in R. </p>

<pre><code>set.seed(123)    
# preamble, prepare the data for the simulation
    #1.
    library(Matching)
    library(survival)
    #2.
    require(doParallel)
    cl&lt;-makeCluster(2)
    registerDoParallel(cl)
    #3.
    clusterEvalQ(cl,library(Matching))
    clusterEvalQ(cl,library(survival))

    m &lt;- 100


    Result = foreach(i=1:m,.combine=cbind) %do%{

      # attach the data
      data(lalonde)

      # we want to assess if treatment is associated with greater odds for the outcome of interest
      # lets create our hypothetical outcome of interest
      lalonde$success &lt;- with(lalonde, ifelse(re78 &gt; 8125, 1, 0))

      # lets take a portion of the original population, say only 395
      n &lt;- sample(1:445,420, replace = F)
      n &lt;- sort(n, decreasing = F)
      lalonde &lt;- lalonde[n,]
      head(lalonde$age)

      # taking from the example from GenMatch (in Matching package)
      #The covariates we want to match on
      # but we only include some of the original variables (we come back to the others later)
      X = cbind(lalonde$age, lalonde$educ, lalonde$black, lalonde$hisp, 
                lalonde$married, lalonde$nodegr)

      #The covariates we want to obtain balance on
      BalanceMat &lt;- X

      # creat our matrix
      genout &lt;- GenMatch(Tr=lalonde$treat, X=X, BalanceMatrix=BalanceMat, 
                         pop.size=16, max.generations=10, wait.generations=1)


      # match our collisions on a 1-1 basis
      mout &lt;- Match(Y=NULL, Tr=lalonde$treat, X=X, Weight.matrix=genout, ties = F, replace = F)
      summary(mout)

      # here we create our case and control populations
      treat &lt;- lalonde[mout$index.treat,]
          control &lt;- lalonde[mout$index.control,]

      # and we want to apply a unique identifier for each pair
      # we call this command during the regression
      treat$Pair_ID &lt;- c(1:length(treat$age))
      control$Pair_ID &lt;- treat$Pair_ID 

      # finally we combine the data
      matched &lt;- rbind(treat, control)

      # now we run a conditional logitic regression on the paired data to determine the Odds Ratio associated with treatment
      # we account for the difference in pairs by the strata() command
      # we account for some of the original matching parameters that we removed from the matching process
      model_1 &lt;- clogit(success ~ treat + strata(Pair_ID) + re74, matched, method = 'efron')
      summary(model_1)

      OR_M1 &lt;- exp(model_1$coefficients[1])
      CI_U1 &lt;- exp(confint(model_1))[1,2]
      CI_L1 &lt;- exp(confint(model_1))[1,1]

      Result &lt;- rbind(OR_M1, CI_U1, CI_L1)

    }
</code></pre>

<p>To summarise the script:</p>

<ul>
<li>we take 420 people from the original population (of 445)</li>
<li>we define the outcome of interest is. That is if the person had <code>re78 &gt; 8125</code> yes or no</li>
<li>for each treat==1, we find an equivalent treat==0 based on age, educ,  black, hisp, married, nodegr. We only want exact 1-1 matching</li>
<li>we assign an unique indicator variable for each pair 1,2,3.....x</li>
<li>We then develop a regression model to determine the OR for our outcome of interest (<code>re78 &gt; 8125</code>) associated with the treatment status (=1 relative to =0). </li>
<li>we save the ORs and 95%CI</li>
</ul>

<p>We can then plot the ORs and shade the 95%CI</p>

<pre><code>plot(Result[1,], ylim=c(0,2.5))
polygon(c(1:m,m:1), c(Result[3,],Result[2,]),col=adjustcolor(""grey"", alpha=0.4), border = NA)
</code></pre>

<p><strong>But how can I summarise the several ORs I obtained, the spread of it and/or an associated confidence level?</strong></p>

<p><strong>EDIT</strong>
Am I able to assess my study as if it was a meta-analysis. If so, one could implement the solution proposed by @Bernd Weiss <a href=""http://stats.stackexchange.com/questions/9483/how-to-calculate-confidence-intervals-for-pooled-odd-ratios-in-meta-analysis?rq=1"">here</a>?</p>

<p>For this we need to obtain the natural log of the ORs and the std. err.?</p>

<p>We update the last part of the command to:</p>

<pre><code>.......    
model_1 &lt;- clogit(success ~ treat + strata(Pair_ID) + re74, matched, method = 'efron')
      summary(model_1)

  OR_M1 &lt;- exp(model_1$coefficients[1])

  l_OR_T2 &lt;- model_1$coefficients[1]
  s_e &lt;- coef(summary(model_1))[1,3]

  CI_U1 &lt;- exp(confint(model_1))[1,2]
  CI_L1 &lt;- exp(confint(model_1))[1,1]

  Result &lt;- rbind(OR_M1, l_OR_T2, s_e, CI_U1, CI_L1)
</code></pre>

<p>Using we can then call upon the <code>metagen()</code>, command</p>

<pre><code>library(meta)
or.fem &lt;- metagen(as.numeric(Result[2,]), as.numeric(Result[3,]), sm = ""OR"")
</code></pre>

<p>Where <code>as.numeric(Result[2,])</code> is the log(OR) and <code>as.numeric(Result[3,])</code> is the std. err. Thus we obtain a 95% CI ...... But have we introduced a bias in the CI by the imputations. We see our 95% range is significant (greater than 1), however for each permutation, we only get a lower 95% CI > 1 </p>

<pre><code>sum(as.numeric(Result[5,])&gt;1.00)
</code></pre>

<p>times. Therefore I think the large <code>n</code> and thus <code>degrees of freedom</code> in the meta-analysis are giving us a significant result </p>
"
"NaN","NaN","206055","<p>I have been reading a lot on semi-supervised learning lastly. It appears that this type of learning is most common for classification tasks, but I have noticed that it also exists for regression problems (contineous dependent variable). </p>

<p>I was wondering what the options are in R related to semi-supervised regression and whether there would be any package that is preferred?</p>
"
"0.0692289300613081","0.080582296402538","206058","<p>Using R, I can only find tools for performing L1 and/or L2 regularized linear regression (lars, glmnet) and tools for constrained linear regression (quadprog , or lsei {limSolve} , where the inequality and equality constraints can be only given in the form Ax = b , Gx &lt;= h).</p>

<p>It seems inutitive for me that the possibility of combining both should be required very oft when solving specific regression problems, but so far I havent been able to do it. </p>

<p>Instead of providing information on my specific set of constraints and algebraic system, IÂ´d be interested to know if this is a problem I can actually solve using the above mentioned packages? Are there any packages at all in R built for both parameter regularization and specific parameter penalties? </p>

<p>Update: For better understanding: I am not trying to combine different regularization methods (like in elastic net), nor trying to combine different parameter constraints. My goal is to combine regularization with specific coefficient constraints, so for example:</p>

<p>Find the most sparse solution (penalizing absolute values through LASSO) of a linear regression y = bx which satisfy the coefficient constraints bA &lt; h for some given matrix A and threshold h. </p>

<p>min($\parallel$ $\beta$x-y$\parallel_2^2$ + $\lambda$ $\mid$ $\beta$ $\mid_1$ ), s.t   $\beta$ A $\leqslant$ h</p>
"
"0.0843408998948762","0.0944911182523068","206075","<p>I'm relatively new to machine learning (started about 5 months ago), and I'm looking at potentially implementing an ensemble classifier as part of my research. </p>

<p>I have built 3 models that I use to classify whether sales data is going to win or lose. Each model produces the probability of the sale winning or losing, and then I apply thresholds to those to classify them as either a ""Win"", ""Loss"" or ""Borderline Loss"". There are 25 variables, all of which are discrete. </p>

<p>The three models are Naive Bayes, Tree Augmented Naive Bayes (TAN) and Logistic Regression. I am using the bnlearn package for the bayesian classifiers, and a simple glm for the Logistic Regression. All models have high accuracy performances when tested on unseen data:</p>

<p>Naive Bayes Accuracy: 88% </p>

<p>TAN Accuracy: 91%</p>

<p>Logistic Regression Accuracy: 92%</p>

<p>I want to try implementing an ensemble classifier to see if I can get the best possible accuracy across all three models. My question is, how do I go about implementing something like this? I can't find too many examples online, at least not with these models for implementing one. From what I have read, one way to do it is to have a voting system, where if the 2 models predict the sale will win, but 1 predicts with will lose, then it is classified as a win. But what happens in this case if all 3 models had different predictions? I have all my prediction data ready, as in I have all the test data and each models prediction for each sale, my question so is, how would I proceed from here? </p>

<p>If someone knows of any available resources or tutorials that may help, I would greatly appreciate it!</p>
"
"0.0395593886064618","0.040291148201269","207148","<p>I have a OLS model looks like this:</p>

<p><img src=""https://chart.googleapis.com/chart?cht=tx&amp;chl=%5CDelta%20y_t%3D%5Cbeta_0%2B%5Cbeta_1%5CDelta%20x_t%2B%5Cepsilon_t"" alt=""formula""></p>

<p><a href=""http://i.stack.imgur.com/5FlOO.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5FlOO.png"" alt=""enter image description here""></a></p>

<p>However, the residuals have auto-correlation like this:</p>

<p><a href=""http://i.stack.imgur.com/tcCNn.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/tcCNn.png"" alt=""enter image description here""></a></p>

<p>It doesn't seem a strong autocorrelation, and the model passes the Engle-Granger cointegration test (package egcm). However, the model fails Durbinâ€“Watson test. Adding an autoregressive term like AR(1) will totally solve this problem, because ARMAX explicitly models the errors. but I am a bit worried about the possibility of spurious regression.</p>

<p>Are my OLS model residuals good enough to conclude cointegration? If not, does ARMAX help in my case?</p>

<p>Thanks in advance.</p>
"
"0.0559454238864459","0.056980288229819","207177","<p>I am fitting a logistic regression model for the likelihood of patients suffering morbidity after surgery. The most commonly used prediction tool at the moment is POSSUM (Physiological and Operative Severity Score for the enUmeration of Mortality and Morbidity), which I would like to compare my model against.</p>

<p>In terms of discrimination, I have the Area Under the ROC curves calculated for both and would like to compare the two. </p>

<p>It seems in Stata that the command to use is <code>roccomp</code>. This produces a chi2 statistic and a p-value.</p>

<p>The R equivalent seems to require the <code>pROC</code> package and the function to use is <code>roc.test()</code>. However this function returns a z-statistic and p-value.</p>

<p>Looking at the documentation, both seem to be implementations of DeLong et al's methods of comparing AUROCs[1], but I cannot for the life of me understand why one gives a chi2 and the other a z-statistic. Are the tests equivalent?</p>

<p><em>Reference</em>:
1. Elisabeth R. DeLong, David M. DeLong and Daniel L. Clarke-Pearson (1988) â€œComparing the areas under two or more correlated receiver operating characteristic curves: a nonparametric approachâ€. Biometrics 44, 837--845.</p>

<p><strong>EDIT</strong>: Does this have anything to do with the explanation: <a href=""http://stats.stackexchange.com/questions/173415/at-what-level-is-a-chi2-test-mathematically-identical-to-a-z-test-of-propo/173483#173483"">At What Level is a $\chi^2$ test Mathematically Identical to a $z$-test of Proportions?</a> ?</p>
"
"0.108550066801774","0.110557998564556","207608","<p>I'd like to run a probit regression on the ""B1_df"" data frame with 3 categorical outcome variables (rank 1,2 or 3). I cannot use glm because there are 3 outcome variables.  I would like to be able to tie out the results from polr() and mlogit().  I am getting reasonable results from polr() but strange results from mlogit() I believe due to my data frame construction.</p>

<p>Basically I have 3 machine B1, B2 and B3 and each have 5 runs that are ranked 1 to 3 and I am using probit to tell me which machine has the highest probability of returning the highest rank. </p>

<pre><code>First with polr():

require(ggplot2)
require(MASS)
require(mlogit)

machine = c(rep(""B1"",5), rep(""B2"",5),rep(""B3"",5))
rank = c(rep(3,5), rep(2,5),rep(1,5))
#rank = c(c(3,3,3,3,1), rep(2,5),rep(1,5)) # see *** comment below
dat = data.frame(machine = machine, rank = rank)
dat$B1 =  c(rep(1,5), rep(0,5),rep(0,5))
    dat$B2 =  c(rep(0,5), rep(1,5),rep(0,5))
dat$B3 =  c(rep(0,5), rep(0,5),rep(1,5))
B1_df = dat[,1:3]
B1_df
b1=polr(formula = as.factor(rank)~ as.factor(B1), data= B1,  Hess = FALSE, model = TRUE,method = c(""probit""))
b1

   machine rank B1
1       B1    3  1
2       B1    3  1
3       B1    3  1
4       B1    3  1
5       B1    3  1
6       B2    2  0
7       B2    2  0
8       B2    2  0
9       B2    2  0
10      B2    2  0
11      B3    1  0
12      B3    1  0
13      B3    1  0
14      B3    1  0
15      B3    1  0
&gt; b1=polr(formula = as.factor(rank)~ as.factor(B1), data= B1,  Hess = FALSE, model = TRUE,method = c(""probit""))
&gt; b1
Call:
polr(formula = as.factor(rank) ~ as.factor(B1), data = B1, Hess = FALSE, 
    model = TRUE, method = c(""probit""))

Coefficients:
as.factor(B1)1 
      8.599074 

Intercepts:
         1|2          2|3 
0.0002318407 4.4165977032 

Residual Deviance: 13.86319 
AIC: 19.86319 
</code></pre>

<p>Question:  Does the coef of 8.5 indicate that by setting B1 =1 the z-score would increase by 8.5 giving a higher probability of getting a higher rank? I was thinking that it did but then I uncommented this line:</p>

<pre><code>rank = c(c(3,3,3,3,1), rep(2,5),rep(1,5))
</code></pre>

<p>so now B1 does not have all 3's it has 4 3's and one 1. I was expecting the coef, intercepts, deviance to change but they don't.  Please uncomment ** and run. Any idea why no change?</p>

<p>Now I'd like to try to get those same results in mlogit:</p>

<pre><code>B1_df2 = mlogit.data(B1_df, shape = ""wide"", choice =""rank"", id.var= ""B1"") #configure the data frame with mlogit.data
B1_df2
summary(mlogit(rank ~  0 | B1 ,data=B1_df2, Probit=TRUE))  # call mlogit 

&gt; summary(mlogit(rank ~  0 | B1 ,data=B1_df2, Probit=TRUE))  # call mlogit

Call:
mlogit(formula = rank ~ 0 | B1, data = B1_df2, Probit = TRUE, 
    method = ""nr"", print.level = 0)

Frequencies of alternatives:
       1        2        3 
0.533333 0.400000 0.066667 

nr method
16 iterations, 0h:0m:0s 
g'(-H)^-1g = 7.84E-07 
gradient close to zero 

Coefficients :
                 Estimate  Std. Error t-value Pr(&gt;|t|)
2:(intercept) -8.9248e-17  6.3246e-01  0.0000   1.0000
3:(intercept) -1.6669e+01  1.8625e+03 -0.0089   0.9929
2:B1          -1.0986e+00  1.3166e+00 -0.8345   0.4040
3:B1           1.5570e+01  1.8625e+03  0.0084   0.9933

Log-Likelihood: -11.683
McFadden R^2:  0.11726 
Likelihood ratio test : chisq = 3.1037 (p.value = 0.21186)
</code></pre>

<p>You can see the mlogit coefs are nowhere near the 8.5 and there seem to be duplicates.</p>

<p>For mlogit() I am looking at page 22 here for the pure multinomial model:</p>

<p><a href=""https://cran.r-project.org/web/packages/mlogit/mlogit.pdf"" rel=""nofollow"">https://cran.r-project.org/web/packages/mlogit/mlogit.pdf</a></p>

<p>Any idea how to properly set up these models to get them to tie?</p>
"
"0.0395593886064618","0.040291148201269","207890","<p>I'm able to obtain predicted survival probabilities of cox regression using either <code>survfit.coxph</code> or <code>predictSurvProb</code> from <code>pec</code> package. However, using these approaches I'm unable to predict the probability of new observation which has the time set after the study, or the end time of dataset used to construct the model, e.g. the period of study is from 0 to 1000, and the time for new observation is 1200.</p>

<p>Is this the limitation of cox regression? Otherwise, is there any alternative to calculate the aforesaid probability?</p>
"
"0.0791187772129236","0.080582296402538","208078","<p>If you run the following code, you will have a data frame <code>real.dat</code> which has 1063 samples for 20531 genes. There are 2 extra columns named <code>time</code> and <code>event</code> where <code>time</code> is the survival time and <code>event</code> is <code>death</code> in case of <code>1</code> and <code>0</code> in case of <code>censored</code>.</p>

<pre><code>lung.dat &lt;- read.table(""genomicMatrix_lung"")
lung.clin.dat &lt;- read.delim(""clinical_data_lung"")

# For clinical data, get only rows which do not have NA in column ""X_EVENT""
lung.no.na.dat &lt;- lung.clin.dat[!is.na(lung.clin.dat$X_EVENT), ]

# Getting the transpose of main lung cancer data
ge &lt;- t(lung.dat)
# Getting a vector of all the id's in the clinical data frame without any 'NA' values
keep &lt;- lung.no.na.dat$sampleID

# getting only the samples(persons) for which we have a value rather than 'NA' values
real.dat &lt;- ge[ge[, 1] %in% keep, ]

# adding the 2 columns from clinical data to gene expression data
keep_again &lt;- real.dat[, 1]
temp_df &lt;- lung.no.na.dat[lung.no.na.dat$sampleID %in% keep_again, ]

# naming the columns into our gene expression data
col_names &lt;- ge[1, ]
colnames(real.dat) &lt;- col_names

dd &lt;- temp_df[, c('X_TIME_TO_EVENT', 'X_EVENT')]
real.dat &lt;- cbind(real.dat, dd)

# renaming the 2 new added columns
colnames(real.dat)[colnames(real.dat) == 'X_TIME_TO_EVENT'] &lt;- 'time'
colnames(real.dat)[colnames(real.dat) == 'X_EVENT'] &lt;- 'event'
</code></pre>

<p>I want to get the univariate Cox regression p-value for each gene in the above data frame. Now, when I try to run the <code>coxph</code> function from the <code>survival</code> package even for one gene, it shows the following error -</p>

<p><code>&gt; coxph(Surv(time, event) ~ HIF3A, real.dat)
Error in fitter(X, Y, strats, offset, init, control, weights = weights,  : 
  NA/NaN/Inf in foreign function call (arg 6)
In addition: Warning message:
In fitter(X, Y, strats, offset, init, control, weights = weights,  :
  Ran out of iterations and did not converge</code></p>

<p>What am I doing wrong here?</p>

<p>You can download the data from <a href=""https://drive.google.com/open?id=0B2p9dpw7AL_zMTNXR1JwRW1KRmM"" rel=""nofollow"">here</a>.</p>
"
"0.0927749898843639","0.0944911182523068","208090","<p>I have a dataset with multiple dependent variables, which are counts of about 53 different categories of debris found on beaches. I also have a variety of independent variables, some of which I am interested in as fixed effects, but others of which are probably best off as random effects. They are a mix of categorical and continuous variables, e.g. State, county, distance from north to south, number of people present, etc. </p>

<p>A very tiny sample data set is as follows, though there are additional factors such as distance to road, date of survey, number of people on beach, and multiple transects per site. </p>

<pre><code>Counts&lt;- as.data.frame(matrix (rpois(100,1), ncol=5))
colnames(Counts)&lt;-c(""Glass"", ""HardPlastic"", ""SoftPlastic"", ""PlasticBag"", ""Fragments"")
State&lt;-rep(c(""CA"",""OR"",""WA""), each=6)
Counts$State&lt;-c(State,""CA"",""OR"")
    County&lt;-rep((1:9), each=2)
    Counts$County&lt;-c(County, 1,4)
Counts$Distance&lt;-c(10, 15, 13, 19, 18, 23, 38, 40, 49, 44, 47, 45, 52, 53, 55, 59, 51, 53, 14, 33)
    Year&lt;-rep(c(""2010"",""2011"",""2012""), times=7)
    Counts$Year&lt;-Year[1:20]
</code></pre>

<p>I would like to know whether the data vary by state, whether they change over time, and ultimately, whether the variability is higher within a site or between sites. </p>

<p>I think the best way to look at the data would be through a multinomial logistic regression model. I have been working in R, so I have looked at nnet (multinom) and VGAM (vglm), but it appears that neither of these support random effects. It could also be useful to have a smooth on some of the geographic data, so I've had a look at mgcv, but I can't find whether that package would support multiple dependent variables. </p>

<p>I have read that MCMCglmm will handle random effects, but I must admit I am a bit daunted by the complexity of how to set up the model structure, especially with respect to the priors. </p>

<p>My specific questions therefore are:</p>

<ol>
<li>can mgcv handle multiple DVs? </li>
<li>Is there another package I have overlooked?</li>
</ol>

<p>UPDATE:</p>

<p>I have found the following site: <a href=""http://search.r-project.org/library/mgcv/html/mvn.html"" rel=""nofollow"">http://search.r-project.org/library/mgcv/html/mvn.html</a>, which describes using the mvn family in mgcv as a way to run multivariate normal additive models. The model structure then for my sample data set would appear to look something like this:</p>

<pre><code>b&lt;-gam(list(Glass~s(State)+s(County),SoftPlastic~s(State)+s(County),
  PlasticBag~s(State)+s(County),family=mvn(d=3),data=Counts))
</code></pre>

<p>If this is correct, however, the model will get enormously long and complex as additional categories are added, and furthermore, I don't believe my data are normally distributed. </p>

<p>In other words, I'm still very much looking for an answer to this question!</p>
"
"0.0625488854200668","0.0637058989297032","208571","<p>I am trying to model my dependent variable (ordinal - three levels) using a set of independent variables (5 ordinal and 10 numeric). I am using <code>lrm</code> function in ""rms"" package of R. I am conducting principle component regression. <code>S1</code>, <code>C5</code>, <code>C2</code>, <code>C3</code>, <code>S7</code> and <code>S4</code> are the selected independent variables using PCA. </p>

<pre><code>          Coef         S.E.   Wald   Z    Pr(&gt;|Z|)
          y&gt;=2      -1.0469 0.6092 -1.72  0.0857  
          y&gt;=3      -8.5826 1.0354 -8.29  &lt;0.0001 
          S1=Simple -2.9091 0.6112 -4.76  &lt;0.0001 
          C5         0.8389 0.1475  5.69  &lt;0.0001 
          C2         1.4904 0.1889  7.89  &lt;0.0001 
          C3         1.2139 0.1908  6.36  &lt;0.0001 
          S7         0.8803 0.2701  3.26  0.0011  
          S4=TN     -1.2460 0.4659 -2.67  0.0075  
</code></pre>

<p>I understand, the output of the ordinal regression model is given by,</p>

<pre><code>ln(Fij/ 1-Fij) = Boj + B1X1 + B2X2 + .....BkXk

where Fi1 is probability that Y=1, 
Fi2 is probability that Y=2, 
Fi3 is probability that Y=3
B0, B1.....Bk - coefficients
X0, X1.....Xk - Independent variables
</code></pre>

<p>My question is, how do we interpret negative coefficients here? Also, does ranking the values of Wald statistics from largest to smallest indicate descending strength of evidence of an association with the dependent variable?</p>
"
"0.0625488854200668","0.0637058989297032","208765","<p>I know its possible to extract r-squared values to quantify the 'goodness-of-fit' of regressions in R, with something to the effect of:</p>

<pre><code>fit &lt;- lm(y ~ x1 + x2 + x3, data=mydata)  # Not actual data
r-sq &lt;- summary(fit)$r.squared # or $adj.r.squared
</code></pre>

<p>I've recently been using the <code>cumSeg</code> package for step-function regressions, but it doesn't appear to offer this functionality, though it does provide residuals as a vector.</p>

<p>Is there some way to extract an r-squared (or adj. r squared) that I don't know about? Or can it be calculated 'de novo' with something that <code>cumSeg</code> does actually provide?</p>

<p><strong>EDIT</strong>
This is the output of <code>summary()</code> for my stepfunction created via <code>cumSeg</code>. Perhaps someone more mathematically versed with stepfunctions knows if the  nomenclature for an r-squared (or whatever the equivalent is) is just different and the data I'm looking for is actually there (or if it is even a legitimate question to ask for an R-squared for stepfunctions?! I'm assuming it should be calculable from any fitted model really.</p>

<pre><code>&gt; summary(stepfunc)
              Length Class  Mode   
coefficients   3     -none- numeric
residuals     16     -none- numeric
effects       16     -none- numeric
rank           1     -none- numeric
fitted.values 16     -none- numeric
assign         0     -none- NULL   
qr             5     qr     list   
df.residual    1     -none- numeric
epsilon        1     -none- numeric
it             1     -none- numeric
psi            1     -none- numeric
beta.c         1     -none- numeric
gamma.c        1     -none- numeric 
V             16     -none- numeric
y             16     -none- numeric
id.group      16     -none- numeric
est.means      2     -none- numeric
n.psi          1     -none- numeric
</code></pre>
"
"0.0395593886064618","0.040291148201269","209030","<p>I fitted a Cox PH model in R with the survival package and the <code>coxph</code> function.
I get the beta estimates from this model.
How can I use these coefficients to manually predict on new data, like the predict function does.</p>

<p>In a linear regression this is just the matrix multiplication <code>X %*% beta</code> if $X$ is the data and $beta$ is the vector of coefficients.</p>

<p>How is this in the Cox model? I also see that predict has several options for types of predictions.</p>

<p>here is a minimal example:</p>

<pre><code>library(survival)
data(""ovarian"")
m &lt;- coxph(formula = Surv(futime, fustat) ~., data=ovarian)
</code></pre>

<p>these two give different results:</p>

<pre><code>head(as.matrix(ovarian[, -c(1:2)]) %*% m$coefficients)

      [,1]
1 10.102002
2 10.371810
3  9.706097
4  6.820160
5  7.357138
6  7.627324

head(predict(m, ovarian))
          1           2           3           4           5           6 
 2.66935119  2.93915962  2.27344680 -0.61249088 -0.07551308  0.19467374 
</code></pre>
"
"0.0484501583111509","0.0493463771219827","209055","<p>I am interested in panel data analysis with more than 20 variables in R using the package ""plm"". Right now, I am looking at adjusted R-square for the set of variables that best explain my dependent variable. My panel has data for around 65 days from 50 countries.</p>

<ol>
<li>Is there any other test statistic apart from adjusted R-square to judge the feature selection of the model?</li>
<li>Is it also possible to do panel data regression using LASSO methodology as I came to know that it can handle a large number of variables?</li>
</ol>
"
"0.027972711943223","0.0284901441149095","209374","<p>I'm using the <code>multinom</code> package in R to run a multinomial logistic regression model. My dependent variable has 3 levels and as the output, I'm getting the probability for each of the level.</p>

<p>Currently, I have the VIF, AIC, p-values and confusion matrix in the model.</p>

<p>I have the following questions:</p>

<ol>
<li><p>I want a single output based on the probabilities. How do I decide a ""cut-off"" for deciding the ""best event""?</p></li>
<li><p>Does it make sense to get an ROC curve here? If yes, then how do I get one?</p></li>
<li><p>What are the things I should look at for the validation of the model?</p></li>
</ol>
"
"0.0969003166223018","0.0986927542439653","209412","<p>I'm trying to build a bivariate copula-based model of income and wealth in Italy and I'm having trouble handling weighted data. I have access to micro data, a survey of about 10,000 households that includes the corresponding sample weights.</p>

<p>When calculating basic statistics (like mean and median) and even when performing linear regressions it is pretty easy to account for weights, besides there are useful packages for that (e. g. survey). But what do I do when I want to fit a parametric model of the distribution to weighted data? Or to estimate its kernel density?</p>

<p>I have a few ideas, but they seem to be pretty crude. For one, I could inflate my sample to the size of the universe. That is, I could multiply all weights by 100 (which would turn them into integers) and then create a vector that repeats each value of income and wealth a given number of times. But that would lead to a very large sample (which I believe still wouldn't be a perfect representation of the population) and will certainly put some extra strain on my computer.</p>

<p>I could also just round the weights off instead of multiplying them by 100, but this would still make the sample noticeably bigger and will inevitably skew the real proportions.</p>

<p>Another approach I came up with would be to normalize the weights (so that they sum up to one) and then randomly sample with repetitions from my initial sample with the corresponding vector of probability weights. R doesn't allow to draw the samples that are larger in size than the one that they are being drawn from. But I think that drawing the sample of the same size as the initial one will lead to some loss of information about the observed proportions. So I could draw the samples of the initial size as described above several times (how would I know how many is though?) and then combine them into one sample. And again, I will have a larger sample with some of the information lost along the way.</p>

<p>So I was wondering if there is a better way to handle weighted data. In some cases I think I could technically introduce the weights into the formula for computing the maximum likelihood for fitting a particular model, although I certainly wouldn't like to code that from the ground up. I will have to fit a lot of models as part of my project, both univariate (e. g. Singh-Mandala) for income and wealth and bivariate for copulas. I don't think the built in functions in any of the copula-related packages that I'm aware of allow one to account for weights. So any advice would help!</p>
"
"0.0625488854200668","0.0637058989297032","209421","<p>If I have theoretical reasons to suppose the data might be fit with an unusual equation such as the following:</p>

<p>$$Y_i = (\beta_0 + \beta_1x_{1i} +  \beta_2x_{2i} + \epsilon_i)^{\beta_3}$$</p>

<p>Can I use Ordinary Least Squares Multiple Linear Regression after a transformation to estimate parameters $\beta{_0,_1,_2,_3}$? If yes, what transformation?</p>

<p>If not, is there some specialized package in R (and brief reading) that might help me compare the fit and residuals from this model against a more typical MLR model?</p>

<p>Thanks.</p>

<p>Example Code:</p>

<pre><code>## while I can run ""nls,"" I cannot get $\epsilon$ inside parentheses nor
## can I have four BETAs

var1 &lt;- rnorm(50, 100, 1)
var2 &lt;- rnorm(50, 120, 2)
var3 &lt;- rnorm(50, 500, 5)

## make a model without $\beta_1$ and $\beta_2$ and with $\epsilon_i$ on outside
nls(var3 ~ (a + var1 + var2)^b, start = list(a = 0.12345, b = 0.54321))

Nonlinear regression model
  model: var3 ~ (a + var1 + var2)^b
  data: parent.frame()
   a        b 
 475.5234   0.9497 
 residual sum-of-squares: 1365

Number of iterations to convergence: 6 
Achieved convergence tolerance: 8.332e-08

## FAILS with exponent on left-hand side and $\epsilon$ inside parentheses
nls(var3^(1/b) ~ (a + var1 + var2), start = list(a = 0.12345, b = 0.54321))
Error in eval(expr, envir, enclos) : object 'b' not found

## FAILS with all BETAs
nls(var3 ~ (a + b*var1 + c*var2)^d, start = list(a = 4, b = 1, c = 1, d = 1))
Error in numericDeriv(form[[3L]], names(ind), env) : 
Missing value or an infinity produced when evaluating the model
</code></pre>
"
"0.0745938985152613","0.0664770029347888","209510","<p>I want to regress a data set that contains a lot of zero's (~55%) and is determined by a typical 2-stage decision process generating the zeros:</p>

<ol>
<li>Consumer decides to apply for a bank loan or not (0-1)</li>
<li>Bank grants application or not (0-Inf; on a continuous scale)</li>
</ol>

<p>Based on the research I conducted so far I am pretty certain that the double hurdle model proposed by Cragg (or a version of it) is a promising approach for my regression. 
(Research includes e.g., Frees, E. W. Regression Modeling with Actuarial and Financial Applications 2011; Cragg, J. G. Some Statistical Models for Limited Dependent Variables with Application to the Demand for Durable Goods Econometrica, 1971; Smith, On Dependency in Double-Hurdle Models, 1998 (available at: <a href=""https://core.ac.uk/download/files/454/12162575.pdf"" rel=""nofollow"">https://core.ac.uk/download/files/454/12162575.pdf</a>); Vignette of the mhurdle package in R: <a href=""https://cran.r-project.org/web/packages/mhurdle/vignettes/mhurdle.pdf"" rel=""nofollow"">https://cran.r-project.org/web/packages/mhurdle/vignettes/mhurdle.pdf</a>)</p>

<p>With the mhurdle package in R this would suggest a regression as follows:</p>

<pre><code>hurd &lt;- function (x,y) {
         mhurdle(DV ~ IV1 + IV2 | IV2 + IV3 | 0,
                 data = xx, dist = x, corr = y)
}
# with x being ""n"" for normal, ""ln"" for log-normal, ""tn"" for truncated-normal"" and y being ""i"" for independent or ""d"" for dependent
</code></pre>

<p>I am though still struggeling to determine the following two things with required certainty - maybe someone is able to help me with this:</p>

<ul>
<li>Double Hurdle models (also the ones proposed by Cragg) are reguarly referenced in the context of count DV - are those models appropriate to handle a continuous DV as well? Are there any specific modifications required? (the mhurdle package e.g., suggests that it is possible)</li>
<li>Zero-inflation models (that are though only applicable to count data) can generally handle two sources of zeros. Are double hurdle models also capable of handling zeros arising from both hurdles? (the vignette of the mhurdle package implies that the Cragg DH model with a ""normal"" distribution accounts for that)</li>
</ul>

<p>Thanks a lot,
Jan</p>
"
"NaN","NaN","209617","<p>I have noticed in the CRAN documentation for the <code>survival</code> package that survival time tie-handling is discussed extensively for Cox-PH regression (allowing for Efron, Breslow, or Exact methods), but not for Aalen's Additive Model.  How would the below function call handle a tie in <code>DURATION</code>?  </p>

<p><code>aareg(formula = Surv(DURATION, OBSERVED) ~ regressor1 + regressor2 -1, data=df, nmin = 1)</code></p>
"
"NaN","NaN","210651","<p>I would like to use the OOB cases from a random forest fit to estimate the mean squared prediction error so I don't have to cross-validate. I am using the randomForest package in R. It is clear from the documentation that OOB error is reported for classification, but I can't figure out how to get OOB MSPE for regression. Am I missing it or is it truly not reported, which seems odd?</p>
"
"0.0839181358296689","0.0854704323447285","212355","<p>I am using beta regression to model fulfillment ratio of a store (orders delivered/orders placed), which is between 0 and 1. I am using betareg package in RStudio to run this analysis. I have 24 predictors (many of these correlated with each other) with which i am trying to build the model. For certain variables i am getting this error:</p>

<pre><code>    Error in chol.default(K) : 
      the leading minor of order 18 is not positive definite
    In addition: Warning message:
    In sqrt(wpp) : NaNs produced
    Error in chol.default(K) : 
      the leading minor of order 18 is not positive definite
    In addition: Warning messages:
1: In betareg.fit(X, Y, Z, weights, offset, link, link.phi, type, control) :
  failed to invert the information matrix: iteration stopped prematurely
2: In sqrt(wpp) : NaNs produced
</code></pre>

<p>For a few other variables i am getting this error:</p>

<pre><code>Warning message:
In betareg.fit(X, Y, Z, weights, offset, link, link.phi, type, control) :
  optimization failed to converge
</code></pre>

<p>I do not what these errors mean and how to fix them. There are no missing data points in the dataset. Will be glad if someone could help me out here. Thanks.</p>

<hr>

<p>Thank you for letting me know my info is insufficient. Approximately i have 3,900 observations.</p>

<p>I am posting the summary of my dataset since i cannot post the actual data. Let me know if this provides insight into the data i am dealing with:</p>

<pre><code>   chain_id       timediff        ratio_store     fulfilled_store 
 0      :3781   Min.   :  0.000   Min.   :0.0000   Min.   :   0.0  
 11     :  37   1st Qu.:  0.000   1st Qu.:0.4138   1st Qu.:   8.0  
 3      :  25   Median :  7.771   Median :0.7349   Median :  39.0  
 13     :  18   Mean   : 31.179   Mean   :0.6331   Mean   : 134.7  
 2      :   9   3rd Qu.: 47.771   3rd Qu.:0.8889   3rd Qu.:  97.0  
 12     :   8   Max.   :153.771   Max.   :1.0000   Max.   :1294.0  
 (Other):  19                                                      
 pricecount_monthly store_pricecount  ratio_monthly    fulfilled_monthly
 Min.   :   0       Min.   :    2.0   Min.   :0.0000   Min.   :  0.00   
 1st Qu.:  16       1st Qu.:  199.0   1st Qu.:0.4000   1st Qu.:  2.00   
 Median :  79       Median :  479.0   Median :0.7727   Median : 11.00   
 Mean   : 193       Mean   :  829.7   Mean   :0.6347   Mean   : 36.53   
 3rd Qu.: 223       3rd Qu.: 1095.0   3rd Qu.:0.9130   3rd Qu.: 37.00   
 Max.   :3654       Max.   :13561.0   Max.   :1.0000   Max.   :379.00   

 pricecount_sp     allpricecount_product monthlypricecount_product
 Min.   :  1.000   Min.   :    1.0       Min.   :   0.0           
 1st Qu.:  1.000   1st Qu.:   26.0       1st Qu.:   4.0           
 Median :  2.000   Median :   80.0       Median :  15.0           
 Mean   :  4.027   Mean   :  533.2       Mean   : 115.9           
 3rd Qu.:  5.000   3rd Qu.:  375.0       3rd Qu.:  86.0           
 Max.   :116.000   Max.   :28907.0       Max.   :1412.0           

 ratio_product    total_fulfilledproduct monthlyratio_product
 Min.   :0.0000   Min.   :   0.00        Min.   :0.0000      
 1st Qu.:0.4167   1st Qu.:   2.00        1st Qu.:0.4000      
 Median :0.6667   Median :   6.00        Median :0.7692      
 Mean   :0.6130   Mean   :  70.67        Mean   :0.6442      
 3rd Qu.:0.8621   3rd Qu.:  40.00        3rd Qu.:1.0000      
 Max.   :1.0000   Max.   :1245.00        Max.   :1.0000      

 monthly_fulfilledproduct monthlyratio_sp  monthpricecount_sp
 Min.   :  0.00           Min.   :0.0000   Min.   : 0.000    
 1st Qu.:  1.00           1st Qu.:0.0000   1st Qu.: 0.000    
 Median :  2.00           Median :1.0000   Median : 1.000    
 Mean   : 23.55           Mean   :0.6174   Mean   : 1.374    
 3rd Qu.:  9.00           3rd Qu.:1.0000   3rd Qu.: 2.000    
 Max.   :414.00           Max.   :1.0000   Max.   :32.000    

 monthlystorecount_product storecount_product productcount_store
 Min.   :  0.00            Min.   :  1.0      Min.   :   1.0    
 1st Qu.:  3.00            1st Qu.: 13.0      1st Qu.: 111.0    
 Median :  8.00            Median : 37.0      Median : 213.0    
 Mean   : 45.92            Mean   :115.8      Mean   : 324.6    
 3rd Qu.: 37.00            3rd Qu.:131.0      3rd Qu.: 436.0    
 Max.   :330.00            Max.   :870.0      Max.   :2084.0    

 monthlyproductcount_store leafordersplaced_sp  leafratio_sp   
 Min.   :   0.00           Min.   :  1.00      Min.   :0.0000  
 1st Qu.:  12.00           1st Qu.:  3.00      1st Qu.:0.3571  
 Median :  50.00           Median : 11.00      Median :0.7692  
 Mean   :  96.38           Mean   : 46.99      Mean   :0.6345  
 3rd Qu.: 125.00           3rd Qu.: 39.00      3rd Qu.:0.9524  
 Max.   :1148.00           Max.   :620.00      Max.   :1.0000  

 leafmonthlycount_sp leafmonthlyfulfilled_sp averageprice_sp   
 Min.   :  1.00      Min.   :  0.00          Min.   :       5  
 1st Qu.:  2.00      1st Qu.:  1.00          1st Qu.:    1119  
 Median :  4.00      Median :  2.00          Median :    2253  
 Mean   : 14.89      Mean   : 12.31          Mean   :   10187  
 3rd Qu.: 13.00      3rd Qu.:  9.00          3rd Qu.:    8141  
 Max.   :268.00      Max.   :211.00          Max.   :14280251  

   dependent        
 Min.   :0.0001282  
 1st Qu.:0.0001282  
 Median :0.9998718  
 Mean   :0.6173614  
 3rd Qu.:0.9998718  
 Max.   :0.9998718  
</code></pre>
"
"0.0395593886064618","0.040291148201269","212375","<p>I work most of my time with categorical data (predictors and outcome), I usually do a trees in SPSS to make groups and rank which groups are more predominant to buy / not buy.</p>

<p>But now I'm into R, and I'm finding limitations trying to use other modelling techniques as they require numerical data (SVM, NN, Logistic Regression)... I'm using the package ""caret"" to do the models but I usually get no results.</p>

<p>How do you treat categorical variables to fit in those models?</p>

<p>Most of the data is multinomial... should I create n-1 predictors for a n-level predictor? Does R have a function to do this automatically?</p>
"
"0.0969003166223018","0.0986927542439653","212611","<p>I have ran these two Logistic Regression models (below) on some small data and I am able to interpret the output - significance and direction - of the regressors, but I do not know for sure how to interpret all the data which is supposed to tell me everything related to <strong>effect (size) etc</strong>. I did select my predictors properly by adding one each time and checking whether the model was still significant (which yielded the same result as an automatic stepAIC from the MASS package) and I also did some diagnostic checks (outliertest, VIF-score).</p>

<p>What (I think) I got from the models is:</p>

<ul>
<li><strong>R2</strong>: model1 only explains 4.8% of all variation and model2 6.6%, so no predictive power?</li>
<li><strong>C</strong>: model1 does not have acceptable discrimination, neither does model2 (&lt;0.7)</li>
</ul>

<p>Is there other <strong>important information that I am ignorant of</strong>? It seems that these models do <strong>not have much 'power'</strong> (according to <strong>R2</strong> and <strong>C</strong>), but how are they then <strong>still significant</strong> (there is also very significant behaviour (***) for regressors)?</p>

<p>*PS: Sorry if am missing obvious things - I do not have that strong of a statistical background. I am also finding it a hard time searching for all the parameters and metrics since they are often denoted by a one letter name (e.g. C, g) - which is not easy to search for if you do not know what you are looking for... So that's why I came to CrossValidated!</p>

<p>I have found <a href=""http://stats.stackexchange.com/questions/104485/logistic-regression-evaluation-metrics"">this question</a>, but it does not really have an answer since it's maybe way too vague? If someone else has a reading suggestion for my problem, that's also welcomed!*</p>

<h2>First model: Agentivity ~ Period + Genre</h2>

<pre><code>(from lrm)      
                     Model Likelihood      Discrimination    Rank Discrim.    
                       Ratio Test            Indexes           Indexes       
Obs          700    LR chi2      25.55    R2       0.048    C       0.602    
 strong      403    d.f.             4    g        0.440    Dxy     0.204    
 weak        297    Pr(&gt; chi2) &lt;0.0001    gr       1.553    gamma   0.240    
max |deriv| 3e-14                         gp       0.105    tau-a   0.100    
                                          Brier    0.236                     

(from glm)
Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.3171  -0.9825  -0.8094   1.1882   1.6090  

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 954.29  on 699  degrees of freedom
Residual deviance: 928.74  on 695  degrees of freedom
AIC: 938.74

Number of Fisher Scoring iterations: 4
</code></pre>

<h2>Second model: Type ~ Period</h2>

<pre><code>(from lrm)
                      Model Likelihood      Discrimination    Rank Discrim.    
                         Ratio Test           Indexes           Indexes       
Obs           872    LR chi2      36.70    R2       0.066    C       0.637    
 mediopassive 701    d.f.             2    g        0.552    Dxy     0.275    
 passive      171    Pr(&gt; chi2) &lt;0.0001    gr       1.736    gamma   0.401    
max |deriv| 9e-10                          gp       0.087    tau-a   0.087    
                                           Brier    0.151                     

(for glm)
Deviance Residuals: 
 Min       1Q   Median       3Q      Max  
-0.8645  -0.6109  -0.4960  -0.4960   2.0767  

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 863.19  on 871  degrees of freedom
Residual deviance: 826.49  on 869  degrees of freedom
AIC: 832.49

Number of Fisher Scoring iterations: 4
</code></pre>
"
"0.0625488854200668","0.0637058989297032","212652","<p>Say that I have 2 dependent variables, one continuous and one count:</p>

<ul>
<li>amount of adrenaline <strong>(continuous)</strong></li>
<li>number of remembered digits in a task <strong>(count)</strong></li>
</ul>

<p>and I want to check if these two variables differ between various groups, controlling by age and other covariates.</p>

<p>If I were doing two univariate regressions I would choose a poisson distribution for my count DV. In R:</p>

<pre><code>lm(adrenaline ~ groups + covariates)
glm(digits ~ groups + covariates, family=""poisson"")
</code></pre>

<p>But since I suppose that both variables are interacting and correlated, I would use a multivariate regression. In R:</p>

<pre><code>lm(cbind(adrenaline, digits) ~ groups + covariates)
</code></pre>

<p>Here I am conceptually stuck: it seems to me both wrong to use normal distribution of errors when they might not be, and to model both variables independently, as they are NOT independent.</p>

<p>So I see two choices: either embrace complexity and use some exotic (and <a href=""https://cran.r-project.org/web/packages/sabreR/index.html"" rel=""nofollow"">unmantained</a> ) package as <a href=""http://sabre.lancs.ac.uk/sabreR_coursebook5.pdf"" rel=""nofollow"">sabreR</a>. Or simplify the problem in a way that I cannot yet see.</p>

<p>As it seems to me a pretty common scenario, I really hope that I'm misunderstanding something and there's an easy way out.</p>

<p>I would like to know how to proceed with such an analysis. Examples in R would be welcome! :)</p>

<p>Thank you!</p>
"
"0.0484501583111509","0.0493463771219827","213011","<p>In the <code>car</code> package, we have the function <code>powerTransform</code> which transforms variables in a regression equation to make the residuals in the transformed equation as normal as possible. I am confused about what this transformation is and further in the following example:</p>

<pre><code># Box Cox Method, univariate
summary(p1 &lt;- powerTransform(cycles ~ len + amp + load, Wool))

# fit linear model with transformed response:
coef(p1, round=TRUE)
summary(m1 &lt;- lm(bcPower(cycles, p1$roundlam) ~ len + amp + load, Wool))
</code></pre>

<p>What I am confused about is what exactly the model <code>p1</code> is. Is it simply the linear model without a transformation, then it finds the optimal parameter, we then use that to specify <code>m1</code>? So what is the regression equation for <code>p1</code>, <code>m1</code>??</p>
"
"0.0484501583111509","0.0493463771219827","213231","<p>I can perform a vector autoregression using the ""vars"" package in R.</p>

<pre><code>library(vars)
data(Canada)
VAR(Canada, p = 2, type = ""none"")
</code></pre>

<p>But as I understand it, vector autoregression will only work if the factors used are cointegrated.</p>

<p>Is there an R package for vector autoregression that only selects cointegrated factors, or penalises non-cointegrated factors before the VAR estimation?
I searched <a href=""http://rseek.org/"" rel=""nofollow"">http://rseek.org/</a> but did not find this.</p>
"
"0.027972711943223","0.0284901441149095","213233","<p>The following plot is generated using the <code>scatterplot</code> function from <code>R</code>'s <code>car</code> package. </p>

<p>The red ""nonparametric-regression smooth"" generated by <code>loessLine</code> drops way below any values in the data set around 2270 and on the end. What am I to make of this?</p>

<p><a href=""http://i.stack.imgur.com/YkB2D.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/YkB2D.png"" alt=""Scatterplot with unexpected low loess regression values""></a></p>

<p>R code generating the plot:</p>

<pre><code>df &lt;- read.table(header=T, text=""X     Y
2040    5.5
2186    4.5
2232    4.6
2238    4.6
2238    4.6
2262    4.3
2272    4
2272    4.4
2272    4.4
2281    4.8
2281    5
2289    4.4
2295    3.6
2318    4.3
2353    4.5"")

require('car')

scatterplot(Y~X,data=df,ylim=c(0,8))
</code></pre>
"
"0.0685188709827532","0.0697863157798853","213253","<p>In general, my question is how to estimate some prediction intervals in the case of penalized linear models (in particular, I think about the glmnet R package). I understood that the introduction of a penalization in the objective function generates a shrinkage effect, which is a bias on the estimated coefficients. 
I understand that in this case the calculation of the uncertainties is troublesome</p>

<p><a href=""https://air.unimi.it/retrieve/handle/2434/153099/133417/phd_unimi_R07738.pdf"" rel=""nofollow"">https://air.unimi.it/retrieve/handle/2434/153099/133417/phd_unimi_R07738.pdf</a></p>

<p>(see sections 3.2 and 3.3 the quoted papers)</p>

<p>Two bootstrap methods (random x vs fixed x) are discussed in the context of standard linear models here</p>

<p><a href=""http://stats.stackexchange.com/questions/64813/two-ways-of-using-bootstrap-to-estimate-the-confidence-interval-of-coefficients?rq=1"">Two ways of using bootstrap to estimate the confidence interval of coefficients in regression</a></p>

<p>but again the focus is on the beta coefficients.
However, I am not interested in the estimate of the confidence intervals on the beta coefficients, but only on the predicted values.
For instance, consider the following R code</p>

<pre><code>library(glmnet)


# Generate data
set.seed(19875)  # Set seed for reproducibility
n &lt;- 1000  # Number of observations
p &lt;- 5000  # Number of predictors included in model
real_p &lt;- 15  # Number of true predictors
x &lt;- matrix(rnorm(n*p), nrow=n, ncol=p)
y &lt;- apply(x[,1:real_p], 1, sum) + rnorm(n)

# Split data into train (2/3) and test (1/3) sets
train_rows &lt;- sample(1:n, .66*n)
x.train &lt;- x[train_rows, ]
x.test &lt;- x[-train_rows, ]

y.train &lt;- y[train_rows]
y.test &lt;- y[-train_rows]



fit.elnet &lt;- glmnet(x.train, y.train, family=""gaussian"", alpha=.5)

yhat &lt;- predict(fit.elnet, s=fit.elnet$lambda, newx=x.test)
</code></pre>

<p>Does anybody know how to calculate a meaningful confidence interval for yhat?</p>

<p>Thanks!</p>
"
"0.0570990591522943","0.0697863157798853","213446","<p>In a sample regression like this, $r=b_1f_1+b_2f_2$, where $f_1$ and $f_2$ are financial risk factors, I want to see if one of the factors say $f_1$ drives out the other $f_2$, described in John Cochrane's Asset Pricing as a 'Horce Race Regression'. Specifically, in the presence of $f_1$, is $b_2=0.$ John Cochrane describes a Wald Test to do this. 
For references, the description from his book Asset Pricing,</p>

<blockquote>
  <p>We want to know, given factors $f_1$, do we need $f_2$ to price assets-ie, is $b_2=0$. 
  First and foremost obviously, we have an asymptotic covariance matrix for [$b_1,b_2$],so we can form a t-test(if $b_2$ is scalar) or $\chi^2$ for $b_2=0$ by forming the statistic, 
  $\hat{b_2^{'}}Var(\hat{b_2^{'}}) \backsim \chi_{\#b_{2}}^{2}$ where $\#b_2$ is the number of elements in the $b_2$ vector. This is a Wald Test. [Asset Pricing 13.3, page 259]   </p>
</blockquote>

<ol>
<li>Is there an R package or any other statistical package which performs such a test automatically? </li>
<li>At a cursory reading, it seems to me that a simple linear regression where I add factors one by one should suffice to conclude if the factor in question is relevant or now. What's special about 'Horse Races' as described by Cochrane?   </li>
</ol>

<p>Edit: Manela and Moreira JFE 2015 (<em>forthcoming</em>) has a Table that presents the results of a 'Horse Race'. 
<a href=""http://i.stack.imgur.com/3mB7U.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/3mB7U.jpg"" alt=""enter image description here""></a></p>
"
"0.0685188709827532","0.0697863157798853","213804","<p>I am running some linear regressions in R. I am dealing with a linear dependent and linear as well as categorical independent variables using <a href=""https://stat.ethz.ch/R-manual/R-devel/library/stats/html/lm.html"" rel=""nofollow"">lm</a>. So far, I have looked at the output that <code>summary(model)</code> gives me. </p>

<p>Other studies instead run <a href=""http://www.inside-r.org/packages/cran/car/docs/Anova"" rel=""nofollow"">Anova()</a> from the <a href=""https://cran.r-project.org/web/packages/car/index.html"" rel=""nofollow"">car</a> package on their linear model, which returns a similar table. The docs for <code>Anova()</code> state that it</p>

<blockquote>
  <p>Calculates type-II or type-III analysis-of-variance tables for model objects. </p>
</blockquote>

<p>I am under the impression that this <code>Anova()</code> returns an F instead of the t-statistic but is ~ equivalent in what its tell me. (sample output below). So I was wondering</p>

<ul>
<li><p>Are standard R <code>summary(lm)</code> and car <code>Anova(lm)</code> indeed doing pretty much the same calculations here? If not, what is the difference?</p></li>
<li><p>They both report the same p-value, however the F-statistic at the bottom of the standard output is different from the <code>Anova()</code> one. Why is that?</p></li>
<li><p>What are applications where one would choose one over the other?</p></li>
</ul>

<p>Any help is much appreciated!</p>

<p>Sample output:</p>

<p>Standard R</p>

<pre><code>summary(linreg)
...
         Estimate    t value    Pr(&gt;|t|)
Age      -18.016     -3.917     0.000107
Gender   -45.4912    -4.916     1.35e-06
---
Residual standard error: 85.81 on 359 degrees of freedom
F-statistic: 16.71 on 2 and 359 DF, p-value: 1.147e-07
</code></pre>

<p>Anova() output</p>

<pre><code>Anova(linreg)

Anova Table (Type II tests)

           Sum Sq    F value   Pr (&gt;F)
Age        112997    15.345    0.0001072
Gender     1777936   24.164    1.348e-06
</code></pre>
"
"0.027972711943223","0.0284901441149095","213910","<p>I'm curious as to how BoxTidwell works in R. The page for the package itself seems to lack descriptions. I have a logistic regression with many numerical and categorical predictors. Every time I use BoxTidwell(y ~ x1+x2...) I get</p>

<blockquote>
  <p>Error in boxTidwell.default(y, X1, X2, max.iter = max.iter, tol = tol,  : 
    the variables to be transformed must have only positive values</p>
</blockquote>

<p>This occurs even when I removed all the negative predictors. Does this mean that I should not take any categorical variables in the test? and because I do have negative predictors how would I incorporate them?</p>

<p>Also, should I specify something like 'family= binomial' in the command as I do in glm?</p>
"
"NaN","NaN","214017","<p>I have done a multi-level meta analysis with R, using <a href=""https://cran.r-project.org/web/packages/metafor/metafor.pdf"" rel=""nofollow"">metafor package</a>. My questions relates to funnel plot asymmetry. </p>

<p>(1) If a regression test suggested funnel asymmetry (i.e. suggested a publication bias is present), Does this mean that my meta-analysis model is bad? and my results of meta-analysis are bad?</p>

<p>(2) What are other possible reasons, besides publication bias, that could lead to asymmetry in the funnel plots? </p>
"
"0.0395593886064618","0.040291148201269","214022","<p>I was trying out the Subselect R package to see how it worked and if it would be useful for a logistic regression problem I'm working on.  <a href=""https://cran.r-project.org/web/packages/subselect/vignettes/subselect.pdf"" rel=""nofollow"">Link</a> to the package.</p>

<p>I decided I would follow Example 4 on page 28 to see if I could perform the Anneal function on the glm I previously fit to my data.  I used the helper function, glmHmat, to extract the required matrices in the same way sa done in example 4.</p>

<pre><code>Fullmodel&lt;-glm(G,family=binomial,data)
Hmat &lt;- glmHmat(Fullmodel)
</code></pre>

<p>I then tried the anneal function and got the following error.</p>

<pre><code>test&lt;-anneal(Hmat$mat,1,10,H=Hmat$H,r=1,nsol=10,criterion = ""Wald"")
</code></pre>

<p>Yet, I got this error.</p>

<pre><code>Error in validmat(mat, p, tolval, tolsym, allowsingular = FALSE, algorithm) : 

 The covariance/total matrix supplied is not symmetric.
  Symmetric entries differ by up to 1.02445483207703e-07.
</code></pre>

<p>So, I thought I would test if this were true:</p>

<pre><code>isSymmetric(Hmat$mat,tol=1e-09)
[1] TRUE
isSymmetric(Hmat$H,tol=1e-09)
[1] TRUE
</code></pre>

<p>So I can't make heads or tails of this error message.  Any ideas?</p>
"
"0.0395593886064618","0.040291148201269","214175","<p>I'm new to xgboost package and here is the <a href=""https://github.com/dmlc/xgboost/blob/master/doc/parameter.md"" rel=""nofollow"">doc</a> on the parameters of this library for your reference.</p>

<p>My question is, logistic regression <strong>doesn't do</strong> binary splitting and build a tree unlike decision trees. If so, why max.depth and eta (learning rate) has been used in the <a href=""http://stats.stackexchange.com/questions/171043/how-to-tune-hyperparameters-of-xgboost-trees"">example</a> where the objective parameter is binary:logistic. (and the answer is accepted) </p>

<p>Isn't it wrong combination? or am I missing anything?</p>

<pre><code># xgboost fitting with arbitrary parameters
xgb_params_1 = list(
  objective = ""binary:logistic"",                                               # binary classification
  eta = 0.01,                                                                  # learning rate
  max.depth = 3,                                                               # max tree depth
  eval_metric = ""auc""                                                          # evaluation/loss metric
)
</code></pre>
"
"NaN","NaN","214512","<p>I'm trying to run a logistic regression with a L2-Penalty on a dataset I have. For the regression coefficient I also want to have the p-values of the signifance or at least the standard errors.
My plan was to do it with Python but unfortunately none of the package fulfilled my needs. </p>

<p>As I need a solution for this problem really quick, I thought to use R in this case. Unfortunately I don't have the time to read all about the package R offers, so I just liked to ask if there is any way to do the above mentioned in R?</p>
"
"0.027972711943223","0.0284901441149095","214790","<p>My model is logistic regression. Is there a way to tune the parameter lambda of lasso or ridge based on cross-validated log-loss and brier(eg. proper scores?) in any R packages? </p>

<p>I'm using glmnet right now and the only measure available seems to be deviance, mean absolute error, misclassification error(is this based on 0.5 cut off?) and auc which are not proper scores and are therefore less desirable. </p>

<p>On a related note, is there a score like squared loss but penalize the deviation from one outcome more severely?</p>
"
"0.0791187772129236","0.080582296402538","215256","<p>I wanted to do something equivalent to a PCA on a mixed data set containing categorical variables and continuous numerical predictor variables which are normally distributed but measured in very different units. The aims are (a) to explore/describe the variable relationships, and (b) hopefully to reduce the dimensions of the data set for predictive modelling. </p>

<p>Based on this <a href=""http://stats.stackexchange.com/questions/5774/can-principal-component-analysis-be-applied-to-datasets-containing-a-mix-of-cont/5777#5777"">cross validated post</a> I have been using (and liking!) the Factor Analysis of Mixed Data function FAMD() of the FactoMineR package in R.  </p>

<p>But I can't work out if this analysis can be treated the same way as a PCA. Two specific questions:</p>

<ol>
<li>I understand the <a href=""http://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia"">need to scale</a> (ie subtract mean and divide by sd) such numerical variables in a PCA to stop some variables unhelpfully dominating the components. But is this necessary in factor analysis of mixed data as done by the FactoMineR package? Running with both scaled and unscaled as supplementary variables seems to show no difference. 

<ol start=""2"">
<li>Can the principle component dimensions of a mixed data analysis be extracted (from a training set) and applied to a test set, as we might with a true PCA? E.g. if I extract the coordinates of each individual in the training set of Dimension1, then run a regression using the original variables predicting Dimension1, and use the coefficients as the weights of each variable to make a new composite 'Dimension 1 variable' which can be applied to the test set variables - would that be valid?</li>
</ol></li>
</ol>
"
"0.0559454238864459","0.056980288229819","216005","<p>I am going to try to give as much information as possible. 
I have a data base <a href=""http://i.stack.imgur.com/rgrDc.png"" rel=""nofollow""><code>describe(Df)</code></a></p>

<p>So all binary variables except one with 1,2,3. 
Therefore I wanted to do a probit model out of it with y113 being endogenous. 
However, I want to use the lavaan package for SEM because I want to add variables and links to this model later. </p>

<p>So I created a model: </p>

<pre><code>&gt;Model2 &lt;- 'y113~x14+x15+x16'
From which I get from &gt;sem(Model2,data=shortD,ordered=c(""y113"",""x14"",""x15"",""x16""),estimator = ""WLSMV"")
: 

lavaan (0.5-20) converged normally after  19 iterations

                                  Used       Total
  Number of observations   .............   1671 ......... 1672

  Estimator  .................................. .   DWLS ..... Robust

  Minimum Function Test Statistic.. 0.000   .......    0.000

  Degrees of freedom   ..  ..................0      ..............     0

  Minimum Function Value  ............. 0.0000000000000

 Scaling correction factor           ................................   NA

and
Regressions:

                   Estimate   Std.Err   Z-value   P(&gt;|z|)

  y113 ~   

    x14               0.101    0.050    2.009    0.045
    x15               0.047    0.095    0.497    0.619
    x16              -0.381    0.076   -5.038    0.000

Intercepts:

                   Estimate  Std.Err  Z-value  P(&gt;|z|)
    y113              0.000                           

Thresholds:

                   Estimate  Std.Err  Z-value  P(&gt;|z|)
    y113|t1           0.923    0.089   10.372    0.000

Variances:

                   Estimate  Std.Err  Z-value  P(&gt;|z|)
    y113              1.000                           

Scales y*:

                   Estimate  Std.Err  Z-value  P(&gt;|z|)
    y113              1.000                           
</code></pre>

<p>So this table gives values that don't look too real, however I this is my first attempt at SEM so I am not comfortable with it.So I wanted to know how well fitted my model is : fitMeasures(sem)</p>

<pre><code>                     npar                          fmin 
                    4.000                         0.000 
                    chisq                            df 
                    0.000                         0.000 
                   pvalue                  chisq.scaled 
                       NA                         0.000 
                df.scaled                 pvalue.scaled 
                    0.000                            NA 
     chisq.scaling.factor                baseline.chisq 
                       NA                        29.660 
              baseline.df               baseline.pvalue 
                    3.000                         0.000 
    baseline.chisq.scaled            baseline.df.scaled 
                   29.487                         3.000 
   baseline.pvalue.scaled baseline.chisq.scaling.factor 
                    0.000                         1.007 
                      cfi                           tli 
                    1.000                         1.000 
                     nnfi                           rfi 
                    1.000                         1.000 
                      nfi                          pnfi 
                    1.000                         0.000 
                      ifi                           rni 
                    1.000                         1.000 
               cfi.scaled                    tli.scaled 
                    1.000                         1.000 
              nnfi.scaled                    rfi.scaled 
                    1.000                         1.000 
               nfi.scaled                    ifi.scaled 
                    1.000                         1.000 
               rni.scaled                         rmsea 
                    1.000                         0.000 
           rmsea.ci.lower                rmsea.ci.upper 
                    0.000                         0.000 
             rmsea.pvalue                  rmsea.scaled 
                    1.000                         0.000 
    rmsea.ci.lower.scaled         rmsea.ci.upper.scaled 
                    0.000                         0.000 
      rmsea.pvalue.scaled                          wrmr 
                    1.000                         0.000 
                    cn_05                         cn_01 
                    1.000                         1.000 
                      gfi                          agfi 
                    1.000                         1.000 
                     pgfi                           mfi 
                    0.000                         1.000
</code></pre>

<p>Still did not expect to see fmin = 0 or chisq = 0 or pvalue = NA, or anything like those. </p>

<p>I expect to have made mistakes in my programming. 
Do these results make sense ?</p>

<p>(I wish to help as many people that I can with my question (and your answers) so please help me improve it if needed :) )</p>

<p>If you need anymore information just let me know !</p>
"
"0.0625488854200668","0.0637058989297032","217365","<p>If we have a dataset like x=(3,4,2,1,4,...,5), we have classic methods (method of moments, maximum likelihood method, etc) to fit a distribution.</p>

<p>However, in certain real life cases, we can have uncertain about our dataset: </p>

<ul>
<li>if they are mesurements, there can be incertainty due to diverse instruments, </li>
<li>if they are survey resutls, people can be very sure about one choice, and give an estimation, between x and y.</li>
<li>sometimes, the data comes from a retreatment: when estimating a fortune of a person who lived in 1900, we have to take into account the inflation. etc.</li>
</ul>

<p>So, let's then consider that along with the initial dataset, we can also give the standard deviation of estimation of each dataset: sdx=(1,0.4,0.3,1,0.7,...,0.8). How can we take this information when fitting a distribution? There are some well established methods (derivated from method of moments, or maximum likelihood method, etc.), any R packages ?</p>

<p>When doing a regression, I think that this problem can also be encountered. It seems that weighted regression could partillay deal with this.</p>
"
"0.0559454238864459","0.056980288229819","217643","<p>I'm trying to find out if my numeric predictors have a linear relation to the logit of my logistic regression. I tried to use the lrm fit in the rms package where I have used 3 knot cubic spline on all numeric predictors like this:</p>

<pre><code>&gt; fit &lt;- lrm(y ~ rcs(x1,3)+rcs(x2,3)+.....)
</code></pre>

<p>There after I used anova on lrm fit. The main question is how do I use the result in anova(fit)? </p>

<p>My understanding is that the wald statistics are just the associated coefficients squared and dived by it's se. But what about the statistics for nonlinear terms here? are they the wald statistics for the coefficients for the squared predictors? </p>

<p>If none of the statistics are significant, can I conclude that there are no quadratic effect from my predictors?</p>
"
"0.0395593886064618","0.040291148201269","217972","<p>Hoping for some help related to a survival analysis using R and the <code>survival</code> package. I've been relying heavily on a series of blog posts done by Dayne Batten, particularly this portion:
[<a href=""http://daynebatten.com/2015/12/survival-analysis-customer-churn-time-varying-covariates/]"" rel=""nofollow"">http://daynebatten.com/2015/12/survival-analysis-customer-churn-time-varying-covariates/]</a></p>

<p>I've collected and merged the data as instructed using the <code>tmerge</code> function. My model relies on a cumulative time-dependent covariate, which strays away from the example provided. So my first question is does a cumulative covariate affect the validity of the Cox Regression? Here is my code at the moment:</p>

<pre><code>fit &lt;- coxph( Surv(tstart, tstop, had_event) ~ review_event, data = newdatatestcum)
</code></pre>

<p>My second question pertains to the lack of an ID being assigned within this model. For each customer ID within this data I have up to a few hundred lines of events with my covariate. I don't see how this regression could possibly be accounting for that.</p>
"
"0.0625488854200668","0.0637058989297032","218486","<p>How can we do weighted ridge regression in R?</p>

<p>In MASS package in R, I can do weighted linear regression by passing a weight parameter to <code>lm</code>. It can be seen that the model with weights is different from the one without weights.</p>

<pre><code># with weights
&gt; model = lm( y ~ X - 1, weights = w)
&gt; model$coeff
X(Intercept) XSepal.Length  XSepal.Width XPetal.Length  XPetal.Width
-2.1135159    -0.1890203     1.8198141    -1.1771699     2.2840825 

# without weights
&gt; model = lm( y ~ X - 1)
&gt; model$coeff
X(Intercept) XSepal.Length  XSepal.Width XPetal.Length  XPetal.Width 
-5.23869771    0.09802533    2.16742901   -1.07331102    2.40425352 
</code></pre>

<p>However, when I try to replicate the same with <code>lm.ridge</code>, model generated with and without weights are same.</p>

<pre><code>  # with weights
  &gt; model = lm.ridge(y ~ X - 1, lambda=lmd, weights = w)
  &gt; model
  X(Intercept) XSepal.Length  XSepal.Width XPetal.Length  XPetal.Width 
  -5.17253104    0.08770593    2.15946954   -1.06284572    2.38714738

  # without weights
  &gt; model = lm.ridge(y ~ X - 1, lambda=lmd, weights = w)
  &gt; model
  X(Intercept) XSepal.Length  XSepal.Width XPetal.Length  XPetal.Width 
  -5.17253104    0.08770593    2.15946954   -1.06284572    2.38714738
</code></pre>

<p>Edit 1: 
In linear model, I can calculate stderr of coefficients as follows:</p>

<pre><code>rss = sum( residuals( model, type=""pearson"")^2 )
dispersion = rss / model$df.residual
stderr = sqrt( diag(vcov(model)) ) / sqrt(dispersion)
</code></pre>
"
"0.0559454238864459","0.056980288229819","218671","<p>I tried looking this question up on google and didn't find material that answered my question. But my questions are:</p>

<p>(1)  Is there a method to determine how long it takes a leading indicator to affect a variable ? So if we are looking at the affects of oil production on sales, when oil drops how long does it take to affect sales.</p>

<p>Could I use survival analysis for this? <a href=""http://stats.stackexchange.com/questions/30061/identifying-the-time-lag-between-cause-and-effect?rq=1"">This</a> seems related but in a biological context </p>

<p>(2) Can we measure the degree to which oil production affects sales? If oil production drops by 10% it affects sales by 17%.</p>

<p>(3) What's the best way to determine the most important leading indicator? Univariate regression and compare models?</p>

<p>(4) Is there a package in R that could be used for this?</p>
"
"0.0484501583111509","0.0493463771219827","218676","<p>I'm running a maximum likelihood of a logit regression, but the estimated parameters value and the loglikelihood value are depending on the value of the algorithm's start. For example, if my start is a vector of 0's I get one value, ifI change that I get another. Any way to avoid that?</p>

<p>I'm running the optimization with the maxLik package. (Sorry for the function is huge)</p>

<pre><code>    g = function(b){
    bb = inv.logit(b[1] + b[37+1]*dnorde + b[37+2]*dsulde + b[37+3]*dsul + b[37+4]*dc + b[37+5]*BAM + b[37+6]*BRS + b[37+7]*BES + b[37+8]*BPA + b[37+9]*BSE + b[2]*RDPC + b[3]*GINI + b[4]*PEA18M + b[5]*P_AGRO + b[6]*T_ANALF18M 
             + b[7]*DENSIDADE - exp(b[8])*p.br - exp(b[9])*p.bs - exp(b[10])*p.bc - exp(b[11])*p.bi - exp(b[12])*p.bh)

      caixa = inv.logit(b[1] + b[37+1]*dnorde + b[37+2]*dsulde + b[37+3]*dsul + b[37+4]*dc + b[37+5]*BAM + b[37+6]*BRS + b[37+7]*BES + b[37+8]*BPA + b[37+9]*BSE + b[2]*RDPC + b[3]*GINI + b[4]*PEA18M + b[5]*P_AGRO + b[6]*T_ANALF18M 
                + b[7]*DENSIDADE - exp(b[13])*p.br- exp(b[14])*p.bs - exp(b[15])*p.bb - exp(b[16])*p.bi - exp(b[17])*p.bh)

      bradesco = inv.logit(b[1] + b[37+1]*dnorde + b[37+2]*dsulde + b[37+3]*dsul + b[37+4]*dc + b[37+5]*BAM + b[37+6]*BRS + b[37+7]*BES + b[37+8]*BPA + b[37+9]*BSE + b[2]*RDPC + b[3]*GINI + b[4]*PEA18M + b[5]*P_AGRO + b[6]*T_ANALF18M 
                   + b[7]*DENSIDADE - exp(b[18])*p.bb- exp(b[19])*p.bs - exp(b[20])*p.bc - exp(b[21])*p.bi - exp(b[22])*p.bh)

      hsbc = inv.logit(b[1] + b[37+1]*dnorde + b[37+2]*dsulde + b[37+3]*dsul + b[37+4]*dc + b[37+5]*BAM + b[37+6]*BRS + b[37+7]*BES + b[37+8]*BPA + b[37+9]*BSE + b[2]*RDPC + b[3]*GINI + b[4]*PEA18M + b[5]*P_AGRO + b[6]*T_ANALF18M 
               + b[7]*DENSIDADE- exp(b[23])*p.br- exp(b[24])*p.bs - exp(b[25])*p.bc - exp(b[26])*p.bi - exp(b[27])*p.bb)

               itau = inv.logit(b[1] + b[37+1]*dnorde + b[37+2]*dsulde + b[37+3]*dsul + b[37+4]*dc + b[37+5]*BAM + b[37+6]*BRS + b[37+7]*BES + b[37+8]*BPA + b[37+9]*BSE + b[2]*RDPC + b[3]*GINI + b[4]*PEA18M + b[5]*P_AGRO + b[6]*T_ANALF18M 
               + b[7]*DENSIDADE - exp(b[28])*p.br - exp(b[29])*p.bs - exp(b[30])*p.bc - exp(b[31])*p.bb - exp(b[32])*p.bh)

      santander = inv.logit(b[1] + b[37+1]*dnorde + b[37+2]*dsulde + b[37+3]*dsul + b[37+4]*dc + b[37+5]*BAM + b[37+6]*BRS + b[37+7]*BES + b[37+8]*BPA + b[37+9]*BSE + b[2]*RDPC + b[3]*GINI + b[4]*PEA18M + b[5]*P_AGRO + b[6]*T_ANALF18M 
                    + b[7]*DENSIDADE - exp(b[33])*p.br - exp(b[34])*p.bb - exp(b[35])*p.bc - exp(b[36])*p.bi - exp(b[37])*p.bh)

      l = sum(BB*(log(bb))
      +(1-BB)*(log(1 - bb))
      + CAIXA*(log(caixa))
      +(1-CAIXA)*(log(1 - caixa ))

      + BRADESCO*(log(bradesco))
      +(1-BRADESCO)*(log(1 - bradesco))

      + HSBC*(log(hsbc))
      +(1-HSBC)*(log(1 - hsbc))

      + ITAU*(log(itau))
      +(1-ITAU)*(log(1 - itau))

      + SANTANDER*(log(santander))
      +(1-SANTANDER)*(log(1 - santander))

      )
      return(l)

    }


    ll = maxLik(logLik = g, start = rep(0,46), method = ""BFGS"", control = list(tol = 1e-20, iterlim = 1000))
</code></pre>

<p>Output:
Maximum Likelihood estimation
BFGS maximization, 179 iterations
Return code 0: successful convergence 
Log-Likelihood: -11065.77 (46 free parameter(s))
Estimate(s): -4.278447 0.002979005 4.625648 8.724918e-05 -0.04196911 -0.01600133 -6.890406e-06 -1.14766 -0.6503709 -1.265141 -1.051562 -1.08185 -0.9048582 -0.5394084 -0.5875544 -0.30975 -0.1041046 -1.87203 -1.706692 -0.4717443 -0.8142002 -0.3034415 -2.413816 0.1269036 -1.353817 -1.455872 1.122101 -0.7207807 0.2147704 -0.1741227 -0.9657993 0.5873628 -1.392237 0.4828495 -0.3115696 -2.190175 -0.5272591 0.8638808 1.085701 0.4827111 0.6055928 1.302957 0.377728 -1.119531 -0.4588106 -0.6437521 </p>
"
"0.027972711943223","0.0284901441149095","218765","<p>I have been using the <code>rlm</code> (MM estimator) command form the MASS package in <code>R</code>. Unfortunately the pattern of fit.model followed the 1st predictor pattern. Actually this syntax uses everytime when I want to use robust regression. I applied the r syntax for this data was not working well. Any body can give some explanation? Here are my syntax's:</p>

<pre><code>&gt;library(MASS)
&gt;mod.rlm &lt;- rlm(ODM~PC1+PC2, data=mydata, method=""MM"")
&gt;fit.ODM5 &lt;-predict(mod.rlm)
&gt;plot(fit.ODM5,actual.ODM)
</code></pre>

<p><a href=""http://i.stack.imgur.com/jgVBK.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/jgVBK.png"" alt=""enter image description here""></a></p>

<p>and here is the data <strong><a href=""https://www.dropbox.com/s/0phmv6fzyy2i7f5/Rdata.xlsx?dl=0"" rel=""nofollow""><code>Rdata.xlsx</code></a></strong></p>

<p>Many thanks for advances</p>
"
"0.0927749898843639","0.0944911182523068","219304","<p>I am examining social interaction data in individuals within two groups. Each social encounter has been coded to one of 4 categories, and these encounters are nested within individual, whom are nested within groups. The number of social encounters per individual is variable and my groups are unequal sample sizes. </p>

<p>I want to examine whether the proportion of social encounters in different categories significantly differ as a function of group. I previously examined a different DV in this data that was continuous, not categorical, and used a multilevel model in R (nlme package) to do so (data nested within individuals within groups). I have done some looking online and as far as I can tell, R should be able to run a multilevel model with categorical dependent variable as well. (i.e., <a href=""http://www.upa.pdx.edu/IOA/newsom/mlrclass/ho_binary.pdf"" rel=""nofollow"">http://www.upa.pdx.edu/IOA/newsom/mlrclass/ho_binary.pdf</a>). However, I am not sure how to implement this and I think that the naming online is inconsistent (some sources referring to this analysis as MLM with categorical variable, others calling it a multinomial logistic regression). </p>

<p>Is it possible to modify my current R script for continuous DV so that it analyzes for a categorical DV instead? Or do I need a different script? Thank you in advance for any help.</p>
"
"0.0484501583111509","0.0493463771219827","219453","<p>The Hosmer-Lemeshow test is a statistical test for goodness of fit for logistic regression models. According to <code>?hoslem.test</code>, it deals only with binary logistic regression. However, I wonder if this test can be used to a ordered logit model which has more than 2 levels for the dependent variable.</p>

<p>The ordered logit model is also known as the proportional odds model, or a cumulative logit model. And I use the ""Ordinal"" package. Thanks a lot.</p>
"
"0.0484501583111509","0.0493463771219827","220364","<p>So, im in a bit of trouble here. I am using R (i'm very new at this), and i'm trying to plot the probability effects of a interaction effect, using the effects package. </p>

<p>This is what the plot shows<a href=""http://i.stack.imgur.com/bBR1O.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/bBR1O.jpg"" alt=""enter image description here""></a></p>

<p>However, when looking at the logistic regression model: it shows a b coefficient of B -1.333**, ExpB.27 indicating a negative moderation effect.</p>

<p>My quistion: how do i interpret this plot? and how does this relate to the findings? </p>

<p>Thank you guys in advance</p>

<p>Update:
the code i used is: </p>

<pre><code>data.mod &lt;-glm(outc_bin1~ctr_projsize+ctrfirmage+ctr_avgfirmsize+ctr_unirep+ctr_EPO+ctr_avginv+ctr_funding+ctr_projage+ctr_patent+techdiv+involvement+geolog+tech2+techdiv:involvement+tech2:involvement+geolog:involvement, family=binomial(link = ""logit""), data=data, x=TRUE)

plot(effect(""techdiv:involvement"", data.mod, xlevels=list(involvement=c(1, 2, 3, 4)))
</code></pre>

<p>Regression output:</p>

<p><a href=""http://i.stack.imgur.com/pjQOH.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pjQOH.jpg"" alt=""enter image description here""></a></p>
"
"0.027972711943223","0.0284901441149095","220801","<p>I'm looking at this example for <code>LASSO</code> regression in <code>R</code>: <a href=""http://machinelearningmastery.com/penalized-regression-in-r/"" rel=""nofollow"">http://machinelearningmastery.com/penalized-regression-in-r/</a>.</p>

<p>It mentions ""step size"" and ""best_step"".  And, the documentation <a href=""http://www.inside-r.org/packages/cran/lars/docs/predict.lars"" rel=""nofollow"">here</a> also mentions this idea of a ""step"".  </p>

<p>I have never heard of a step size being applied to LASSO regression.  Can someone please help me understand where this is coming from?</p>
"
"0.0897122608032513","0.0913717355809759","220813","<p>I have troubles finding methodology to test a model in R. My question may be long and contain many details, but I will be glad if somebody can help me and give some hints how to tackle it.  </p>

<p>I have multilevel (nested) panel data, where one observation is a frequency of flights (or ratio of use of bio fuel from 0 to 1) of airline <code>i</code> on route <code>j</code> for time period <code>t</code> corresponding to one month.
 I suspect that I need to introduce airline fixed effect since frequency of flights may depend on which airline it is (Lufthansa or Singapore Airlines for example). 
Then I also need to do route fixed effect, since frequency of flights between Frankfurt-New York and Frankfurt-Abidjan will be also specific to each route. And I also need to do time fixed effect, since frequency of flight in the following month most likely to depend on the previous month, since airlines when they schedule flights in advance they choose particular aircraft and competitors behavior. </p>

<p>Below I provided the illustration of data set that I have</p>

<pre><code>A &lt;- data.frame(route=as.factor(rep('Paris-London', 138)), 
                 airline = as.factor(c(rep('Luft', 12), rep('DL', 8), rep('Sin', 72), rep('Turkish', 46))),
                 Date = c(seq(as.Date('2010-01-01'), as.Date('2011-01-01'), by = 'month'), 
                          seq(as.Date('2010-01-01'), as.Date('2010-07-01'), by = 'month'),
                          seq(as.Date('2010-01-01'), as.Date('2015-12-01'), by = 'month'),
                          seq(as.Date('2012-03-01'), as.Date('2015-12-01'), by = 'month')), 
                 ratio_use_bio_fuel= runif(138),
                distance_km=c(rep(600, 138)),
                revenue=sample(10000:25000, 138),
                frequency=sample(100:250, 138, replace=TRUE)
)


B&lt;- data.frame(route=as.factor(rep('Amsterdam-Brussels', 108)), 
               airline = as.factor(c(rep('EEE', 36), rep('FFF', 36), rep('GGG', 36))),
               Date = c(seq(as.Date('2013-01-01'), as.Date('2015-12-01'), by = 'month'), 
                        seq(as.Date('2013-01-01'), as.Date('2015-12-01'), by = 'month'),
                        seq(as.Date('2013-01-01'), as.Date('2015-12-01'), by = 'month')),
               ratio_use_bio_fuel= runif(108),
               distance_km=c(rep(1000, 108)),            
                revenue = sample(10000:25000, 108),
               frequency=sample(50:100, 108, replace=TRUE)



)


C &lt;- data.frame(route=as.factor(rep('Tokyo-Bangkok', 131)), 
                airline = as.factor(c(rep('Chin', 46), rep('Thai', 13), rep('Jap', 72))),
                Date = c(seq(as.Date('2012-03-01'), as.Date('2015-12-01'), by = 'month'),
                         seq(as.Date('2010-01-01'), as.Date('2011-01-01'), by = 'month'), 
                         seq(as.Date('2010-01-01'), as.Date('2015-12-01'), by = 'month')), 
                ratio_use_bio_fuel= runif(131),
                distance_km=c(rep(2500, 131)),
                revenue=sample(10000:25000, 131),
                frequency=sample(32:78, 131, replace=TRUE)

)

d&lt;-rbind(A,B,C)
</code></pre>

<p>Some of the airlines do not have observations for particular time period. In some cases they exited the route and do not provide flights any more or there was a merger. Therefore, I have unbalanced panel data. Does it cause any issues if I run the regression? </p>

<p>I know there are different packages in R such as plm and lme. </p>

<p>But do not know how to do logistic fixed effect regression where my dependent variable is ratio of usage of bio fuel (continuous of 0 to 1) conditional on airline and route. </p>
"
"NaN","NaN","221499","<p>When I run a univariate cox regression test using the <code>survival</code> package in <code>R</code>, assuming that my categorical variable-factor has two or more options the reference group becomes the one with the first listed entry on levels. I'd like to know what happens in the following cases:</p>

<ol>
<li>More than one categorical variable is entered into the model (multiple or multivariate) and some don't have the same amount of options-categories</li>
<li>A scale variable is used like age.</li>
</ol>
"
"0.0740088392978143","0.0753778361444409","221525","<p>I am using the <code>svyglm</code> function in the <code>survey</code> package in <code>R</code> to fit logistic regression models to a stratified, cluster survey. I want to calculate confidence intervals for my regression coefficients. The default method for <code>confint.svyglm</code> says that it creates Wald confidence intervals by adding and subtracting a multiple of the standard error. But the confidence interval this produces is not consistent with the p-value from the model - confidence intervals that do not overlap 0 still have p-values greater than .05.</p>

<p>I tried to replicate the p-value and confidence interval calculations by hand. It appears the p-value is calculated using a t-test, with the df of the t distribution taken from the residual degrees of freedom from the model. So far so good. But the confidence interval provided by <code>confint.svyglm</code> is just coefficient +/- 1.96*standard.error. This seems wrong - for a 95% confidence interval, I think the multiplier for the standard error should be the .975 quantile of a t-distribution with the appropriate degrees of freedom (in my case 10), which can be somewhat different from 1.96 (the .975 quantile of a z-distribution). True? Has anyone else had this problem? I am relatively new to working with survey data. Is there a reason to always use the z-quantile instead of the t-quantile for complex surveys specifically, or is this just a bug in the package?</p>
"
"0.0323001055407673","0.0493463771219827","221681","<p>I have a <code>quantile regression model</code> that I fit with the <code>rq()</code> function in the <code>quantreg</code> package in R. </p>

<p>However, since my sample size if fairly small (n = 36) compared to  the number of X variables (= 8), I need to estimate power for the various regression coefficients. How can I do that?</p>

<p>It would be great to be pointed to a function in R that can do that for a <code>quantreg model</code>, but a general explanation of how to calculate power in such a model would be fine as well (I can then code that in R myself). </p>

<p>How can I determine the power for a given <code>quantile regression model</code> with a given dataset?</p>

<p>thanks, Steve</p>
"
"NaN","NaN","221844","<p>I want to add random effects terms to a generalized poisson regression model. I know this will involve approximation of the loglikelihood (laplace or Gaussian quadratures). Please, i need some more insight on the mathematics and is there any package in R that can facilitate easy implementation?</p>
"
"0.0625488854200668","0.0637058989297032","222011","<p>I have a generated a trend stationary dataset by the following codes:</p>

<pre><code>#Generate a trend stationary series 
n = 1000  #sample size
alpha = 0.5 #intercept
beta = 1 #slope
gamma = 0.05 #trend multiplier
myerror = rnorm(n) #error term
x = rnorm(n) #independent variables
mytrend = seq(1:n) #trend component
ytrendstat = alpha + beta*x +gamma*mytrend + myerror #definition of regression
myreg = lm(ytrendstat~mytrend) #regression of ytrendstat on trend
plot(ytrendstat, type='l')
</code></pre>

<p>Next, I want to modify this data to include one or two structural breaks</p>

<pre><code>#Structural breaks
install.packages(""strucchange"")
library(strucchange)

y&lt;-alpha+beta*x
for(t in 1:n) {
  jump&lt;-ifelse(runif(mytrend[t])&lt;500,1,0);

  y[t+1]&lt;-y[t]+myerror[t+1]+gamma*jump
}
plot(y, type='l')
</code></pre>

<p>Any suggestions on how to create structural breaks in the trend stationary dataset in the most efficient way? I want to do a Chow test first for testing structural break/s in the dataset. </p>
"
"0.0685188709827532","0.0581552631499044","222426","<p>I would like to analyze a Randomized Response variable as the final response variable in a Structural Equation Model (SEM), with <code>R</code>. However, I found no example about this. To the best of my knowledge <code>R</code> packages enable users to fit multivariate logistic regressions only. 
However, I have seen that Randomized Response can be analyzed with SEM in Mplus (Hox, J., &amp; Lensvelt-Mulders, G. (2004). <a href=""http://www.tandfonline.com/doi/abs/10.1207/s15328007sem1104_6?journalCode=hsem20"" rel=""nofollow"">Randomized response analysis in Mplus</a>. Structural equation modeling, 11(4), 615-620.). </p>

<p>Given that <code>R</code> has good packages for latent variable analysis, like <code>lavaan</code>, could you tell me how could I model a randomized response with SEM in <code>R</code>?</p>
"
"0.08882529023711","0.0904683580569682","222479","<p>I'm new in this area, hope my question is understandable.
I need to fit conditional logistic regression model in R and use it for predictions on unseen data (output should be probability).
My datasets are  quite large (over 150k rows) and contains many (~500) noisy features.
I found package called <strong>clogitboost</strong> and tried to use it with relatively small number of boosting iterations (max 30, because with larger values it takes too long to compute and raises an error in the end - perhaps, it's resources limitations) - results are mediocre. I tried to use unconditional approach with regularization - <strong>glmnet</strong> and got better results, however, due nature of data I guess it will be better to use conditional regression with regularization similar to what is used in <strong>glmnet</strong> (tried to remove some features and apply <strong>clogitboost</strong> again and got slightly better results). There is package called <strong>clogitL1</strong> , which seems to do that, I tried to use it and it fits model quickly, but it doesn't provide <strong>predict()</strong> function, Usage described in  paper with attached R code here:
<a href=""https://www.jstatsoft.org/article/view/v058i12"" rel=""nofollow"">https://www.jstatsoft.org/article/view/v058i12</a>, they made some predictions in some way, but I can't understand it. Can I somehow manually predict using  unseen data, just like it's possible with <strong>clogitboost</strong> <strong>predict()</strong> (parameters are Model, X and Strata column) using model that was fitted with <strong>clogitL1</strong>? Note: in description of package <strong>clogitL1</strong> - ""Tools for the fitting and cross validation of <strong><em>exact</em></strong> conditional logistic regression models"" - so I'm not sure about what ""exact"" means here and  if it makes sense to use that package for my purposes. If it's not possible to predict, then, should I manually select features by checking their ""importance"" that can be found in <strong>clogitL1</strong> model? </p>
"
"0.0685188709827532","0.0697863157798853","222544","<p>I used the package for random forest. It is not clear to me how to use the results. 
In  logistic regression you can have an equation as an output, in standard tree some rules. If you receive a new dataset you can apply the equation on the new data and predict an outcome (like default/no default). Or saying the customers with characteristics a and characteristics b will have a default, so you can predict the outcome before it happens. That is the scoring tecnique.</p>

<p>Is it possible to use random forest in a similar situation, or how would you use the results of a RF? </p>

<p>my python code:</p>

<pre><code>from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score

#creating a test and train dataset

from sklearn.cross_validation import train_test_split

train, test = train_test_split(df, test_size = 0.3)

clf = RandomForestClassifier(max_depth = 30, min_samples_split=2, n_estimators = 200, random_state = 1)

#training the model
clf.fit(train[columns], train[""churn""])

#testing the model
predictions = clf.predict(test[columns])

print(predictions)

print(roc_auc_score(predictions, test[""churn""]))
</code></pre>
"
"0.0395593886064618","0.040291148201269","223006","<p>I have a lot of categorical variables that I am using in an XGB model. I use the package <em>dummies</em> to make dummy variables and it creates dummies for all levels. There is no baseline. I know that in regression models this is a big problem, but have no clue if this is the case in XGB. 
Should I manually define a baseline by removing one dummy variable from each of the previous categorical variables or should I retain all dummy variables (w/o any baseline)?</p>
"
"0.0740088392978143","0.0753778361444409","223379","<p>I'm fitting an <code>arima</code>(1,0,0) model using the <code>forecast</code> package in R on the <code>usconsumption</code> dataset. However, when I mimic the same fit using <code>lm</code>, I get different coefficients. My understanding is that they should be the same (in fact, they give the same coefficients if I model an <code>arima</code>(0,0,0) and <code>lm</code> with only the external regressor, which is related to this post: <a href=""http://stats.stackexchange.com/questions/28472/regression-with-arima0-0-0-errors-different-from-linear-regression"">Regression with ARIMA(0,0,0) errors different from linear regression</a>). </p>

<p>Is this because <code>arima</code> and <code>lm</code> use different techniques to calculate coefficients? If so, can someone explain the difference?  </p>

<p>Below is my code.</p>

<pre><code>&gt; library(forecast)
&gt; library(fpp)
&gt; 
&gt; #load data
&gt; data(""usconsumption"")
&gt; 
&gt; #create equivalent data frame from time-series
&gt; lagpad &lt;- function(x, k=1) {
+   c(rep(NA, k), x)[1 : length(x)] 
+ }
&gt; usconsumpdf &lt;- as.data.frame(usconsumption)
&gt; usconsumpdf$consumptionLag1 &lt;- lagpad(usconsumpdf$consumption)
&gt; 
&gt; #create arima model
&gt; arima(usconsumption[,1], xreg=usconsumption[,2], order=c(1,0,0))

Call:
arima(x = usconsumption[, 1], order = c(1, 0, 0), xreg = usconsumption[, 2])

Coefficients:
         ar1  intercept  usconsumption[, 2]
      0.2139     0.5867              0.2292
s.e.  0.0928     0.0755              0.0605

sigma^2 estimated as 0.3776:  log likelihood = -152.87,  aic = 313.74
&gt; 
&gt; #create lm model
&gt; lm(consumption~consumptionLag1+income, data=usconsumpdf)

Call:
lm(formula = consumption ~ consumptionLag1 + income, data = usconsumpdf)

Coefficients:
    (Intercept)  consumptionLag1           income  
         0.3779           0.2456           0.2614  
</code></pre>
"
"0.08882529023711","0.0986927542439653","223582","<p>I am trying to tie the odds ratio from a 2x2 cross classification table to the intercepts of a logistic regression on those 2 variables. I have a cross classification table that produces 2 odds ratios and the results of a logistic regression of PLACE3 ~ VIOL should produce intecepts should match the odds ratio of the contingency table. i.e. Odds ratio = exp(intercepts)  BUT the POLR package is not producing the correct intercepts.</p>

<p>Here is the data.  In the logistic regression PLACE3 is the outcome and VIOl is the independent variable.   You can see the PLACE3 vs. VIOL contingency table below and the logistic regression of PLACE3 ~ VIOL.  The odds ratios in the contingency table 1.79 and 3.1 are correct but the polr function seems off. Any thoughts on why  exp(summary(m)$zeta) does not produce 1.79 and 3.1?</p>

<p>For reference this is from Lemeshow's Applied Logisitic Regression book page 274.</p>

<pre><code>library(data.table)
aps &lt;- fread('http://www.umass.edu/statdata/statdata/data/aps.dat')
colnames(aps) = c(""ID"",""PLACE"",""PLACE3"",""AGE"",""RACE"",""GENDER"",""NEURO"",""EMOT"",""DANGER"",""ELOPE"",""LOS"",""BEHAV"",""CUSTD"",
                    ""VIOL"")
head(aps)
</code></pre>

<p>Here is  a cross classification table of PLACE3 vs. VIOl variables</p>

<pre><code>table(aps$PLACE3,aps$VIOL) 
      0   1
  0  80 179
  1  26 104
  2  15 104
</code></pre>

<p>using PLACE3 = 0 as the reference the 2 odds ratios from the contingency table are </p>

<pre><code>(104*80)/(179*26)  #1.79
(104*80)/(179*15)  #3.10
</code></pre>

<p>These odds ratios should be the same as exponentiating the slope coefficients  from 
a logistic model  PLACE3 ~ VIOL which is below</p>

<pre><code>aps$constant = rep(1,dim(aps)[1])
m &lt;- polr(as.factor(PLACE3) ~ constant + as.factor(VIOL), data = aps, Hess=TRUE,model=TRUE,method = c(""logistic""))
summary(m)

&gt; summary(m)
Call:
polr(formula = as.factor(PLACE3) ~ constant + as.factor(VIOL), 
    data = aps, Hess = TRUE, model = TRUE, method = c(""logistic""))

Coefficients:
                  Value Std. Error t value
as.factor(VIOL)1 0.8454     0.2112   4.003

Intercepts:
    Value  Std. Error t value
0|1 0.6869 0.1884     3.6464 
1|2 1.8608 0.2032     9.1557 

Residual Deviance: 1031.75 
AIC: 1037.75 
</code></pre>

<p>But you can see the exponentiation of the zeta vector is not 1.79 and 3.10</p>

<pre><code>exp(summary(m)$zeta)

&gt; exp(summary(m)$zeta)
     0|1      1|2 
1.987495 6.429049 
</code></pre>
"
"0.0559454238864459","0.056980288229819","223683","<p>I have a good working knowledge of how the loess model works but am curious as to how weights work in conjunction with the model. Obviously, this method weights locally, but many statistical packages (I use R) have an optional weights parameter. How exactly do these weights work and what are suggestions for using them?</p>

<p>I am attempting to use sample size as a weight (someone has asked &amp; answered a question on this here if interested: <a href=""http://stats.stackexchange.com/questions/212518/weight-values-by-sample-size-in-a-loess-regression-in-r"">Weight values by sample size in a LOESS regression in R</a>). But I would also like to use a categorical variable (3 levels) as a weight as well. Is it possible to do both? How would this effect the model? Any suggestions for the use of this (or particulars to use in R)?</p>
"
"0.0395593886064618","0.040291148201269","223917","<p>I have multiple time series of air passenger demand with specific classification data. Data looks like this (some rows may lack some data):</p>

<pre><code>Origin  Destination Time  {more classifiers}  Apr. 1st   May 2nd   Jun. 20th {more dates} 
------------------------------------------------------------------------------------------
Madrid  London      early ...                       10        15          20  ...  
{more rows}
London  Rio         late  ...                       12        10          15  ...   
</code></pre>

<p>The task: The target is to predict values for the next month/day(s). We made an approach with regression trees (ctree, R package partykit) but this results in quite inaccurate results. That is why we want to give time series analysis a try.</p>

<p>My Question: Which algorithm/method would you suggest for this problem? I think it should be a mixture between time series analysis and classification. Any hints/suggestions are highly appreciated. Thanks!</p>
"
"0.0559454238864459","0.0427352161723642","224078","<p>I have three related questions on the package CausalImpact in R. The package can be found <a href=""https://github.com/google/CausalImpact"" rel=""nofollow"">here</a> and a reproducible example is below.</p>

<ol>
<li>Do I basically understand correctly, that the model makes ""1-step ahead""
predictions? I assume it works like a simple lm model that makes
lots of regressions for t+1 with predictors values from t-1 and then looks for the most contributory predictors?</li>
<li>When talking about ""coefficients"", does that mean they are (Pearson)
correlation coefficients?</li>
<li>The function <code>plot(impact$model$bsts.model,""coefficients"")</code> produces
a plot with inclusion probabilities ranging from 0 to 1. Is there
any way to access the actual values in a table? I found
that <code>colMeans(impact$model$bsts.model$coefficients)</code> provides some
values but I'd like to have a confirmation for this.</li>
<li>In my code, I changed <code>bsts.model &lt;- bsts(y ~ x1, ss, niter = 1000)</code> to <code>bsts.model &lt;- bsts(y ~ x1 + x2 + x3 + x4 + x5, ss, niter = 1000)</code> in order to get values for different variables that I defined before. I did not change the cbind functions for that, as I weren't sure if that was necessary. Now I wonder what to do when I do have a table with -let's say- 200 predictors? Do I have to enter x1 to x200 after <code>bsts(y ~</code> to find the best predictors?</li>
</ol>

<p>R code below:</p>

<pre><code>install.packages(""devtools"")
library(devtools)
devtools::install_github(""google/CausalImpact"")
#Download the tar from the git and then install the package in RStudio.

library(CausalImpact)

set.seed(1)
x1 &lt;- 100 + arima.sim(model = list(ar = 0.999), n = 100)
x2 &lt;- 50 + arima.sim(model = list(ar = 0.899), n = 100)
x3 &lt;- 80 + arima.sim(model = list(ar = 0.799), n = 100)
x4 &lt;- 1.25 * x1 + rnorm(100)
x5 &lt;- 101 + arima.sim(model = list(ar = 0.999), n = 100)
y &lt;- 1.2 * x1 + rnorm(100)
y[71:100] &lt;- y[71:100] + 10
data &lt;- cbind(y, x1)

dim(data)
head(data)
data
matplot(data, type = ""l"")

time.points &lt;- seq.Date(as.Date(""2014-01-01""), by = 1, length.out = 100)
data &lt;- zoo(cbind(y, x1), time.points)
head(data)

pre.period &lt;- as.Date(c(""2014-01-01"", ""2014-03-11""))
post.period &lt;- as.Date(c(""2014-03-12"", ""2014-04-10""))

impact &lt;- CausalImpact(data, pre.period, post.period)
plot(impact)

summary(impact)
summary(impact, ""report"")
impact$summary

post.period &lt;- c(71, 100)
post.period.response &lt;- y[post.period[1] : post.period[2]]
y[post.period[1] : post.period[2]] &lt;- NA

ss &lt;- AddLocalLevel(list(), y)

bsts.model &lt;- bsts(y ~ x1 + x2 + x3 + x4 + x5, ss, niter = 1000)

impact &lt;- CausalImpact(bsts.model = bsts.model,post.period.response =     post.period.response)

plot(impact)
summary(impact)
summary(impact, ""report"")

plot(impact$model$bsts.model,""coefficients"")
plot(impact$model$bsts.model, ""coef"", inc = .1)
plot(impact$model$bsts.model, ""coef"", inc = .05)

colMeans(impact$model$bsts.model$coefficients)
</code></pre>
"
"NaN","NaN","224265","<p>I'm fitting  a Bayesian logistic regression to model the effect of two covariates on a Randomized Response, with the 'rr' package.
I would like to compare two nested models by using the Bayes factor. Unfortunately, the 'rr'package does not estimate it by default. Is there any way to obtain it from the output of the 'rrreg.bayes' function?</p>

<p>For anybody who would like to help me, here is an example of application. If you were able to explain me how to compute the Bayes factor from the output of the example provided, I will be able to do the same on my data.</p>

<p><a href=""http://www.inside-r.org/node/333540"" rel=""nofollow"">http://www.inside-r.org/node/333540</a></p>

<p>Kind regards,</p>

<p>Jacopo Cerri </p>
"
"0.0625488854200668","0.0637058989297032","224539","<p>I have a dataset with about 30 potential predictors and 115 observations. I'm looking into building a prediction model with the data using logistic regression.</p>

<p>From what I have read - the typical rule of thumb to split the dataset is an 80/20 split, where 80 percent of the dataset will be used for training the model and the remaining 20 percent will be used for validation/testing. </p>

<p>Using the Confusion Matrix from the Caret package in R, the accuracy of the model is 91%, No information Rate of .55, and a significant p value comparing the Accuracy to NIR (P&lt;.0001).</p>

<p>I'm wary to trust these results, since only 92 observations were used for training and 23 observations for testing.</p>

<p>Is the sample size enough to create a generalizable prediction model? If not, how do I determine the required sample size for model training and testing?</p>
"
"NaN","NaN","225908","<p>What is a goodness of fit measure for quantile regression? I'm not too familiar with the theory but in R the AIC and log-likelihood are given in <code>quantreg</code> package. So I can use that to do variable selection.</p>

<p>But if I want to compare two fitting methods, eg whether <code>quantreg</code> does better than <code>quantregForest</code>, what shall I do? Maybe trimmed residual mean square?</p>
"
"0.0197796943032309","0.040291148201269","226306","<p>Is there an R package or Stata command that implements panel smooth transition regression as detailed in this 2005 paper: <a href=""https://www.researchgate.net/profile/Dick_Van_Dijk/publication/23697021_Panel_Smooth_Transition_Regression_Models/links/02e7e520e99ad69c59000000.pdf"" rel=""nofollow"">Panel Smooth Transition Regression Models - AndrÃ©s GonzÃ¡lez, Timo TerÃ¤svirta and Dick van Dijk</a>?</p>

<p>There already is <a href=""http://stats.stackexchange.com/questions/14110/r-package-for-smooth-transition-regression-models"">a question related to smooth transition regression models in time series</a>. But the time series I want to use are rather short (30 years or less) and I would like to pool data for 40 or more countries. I'm looking for a multi-variate estimator that would allow the use of panel data.</p>
"
"0.0484501583111509","0.0493463771219827","226514","<p>I have data frame with <strong>44,353 entries with 17 variables (4 categorical + 13 continuous).</strong> Out of all variables only 1 categorical variable (with 52 factors) has NAs</p>

<blockquote>
  <p>No of factors in the categorical variables are 1601, 6, 52 and 15</p>
</blockquote>

<p>When I use <code>missforest</code> package it throws error that it cannot handle categorical predictors with more that 53 categories.</p>

<p>Please suggest an imputation method in R for best accuracy. Also since the variable to be imputed is categorical I would prefer to avoid methods that use regression techniques to impute values.</p>
"
"0.027972711943223","0.0284901441149095","226849","<p>How can I find the function 'petest' in R? </p>

<p>A little background, I want to compare two different #regressions, 
y=x1+x2
logy=logx1+logx2</p>

<p>It is clear to me that I cannot use the BIC or AIC values, because the outcome variable is different. I searched in verbeek 2008 (A guide to modern Econometrics) and he suggests the PE test to compare linear and loglinear models.</p>

<p>I searched on this website <a href=""http://artax.karlin.mff.cuni.cz/r-help/library/lmtest/html/petest.html"" rel=""nofollow"">http://artax.karlin.mff.cuni.cz/r-help/library/lmtest/html/petest.html</a> and there I could find that the PE test exist in R, however, I cannot find it. I have the package lmtest, and 'petest' is supposed to be there, but when I try to use it, it says: Error: could not find function ""petest"".</p>

<p>Also, I have looked for more explanation, like examples, or videos that show how to apply this PE model, but I did not find any. Any help on this topic is highly appreciated!</p>
"
"0.0740088392978143","0.0753778361444409","227013","<p>I'm new to propensity score matching (PSM). So, my questions can be bit trivial.</p>

<p>1) Suppose I've 3 treatment levels and want to check the effectiveness of the treatment levels. Treatment levels are taking drug on time, not taking drug on time and not taking drug at regular interval. For this I need to do multinomial logistic regression. </p>

<p>But in PSM we do case-control study. So, how we are going to define which will be the case and which will be the control? Will it be the case that we will use one group as control for each time and other 2 groups as case?</p>

<p>2) Can anyone please tell me which package to use for multilevel group in R. I checked <a href=""http://stats.stackexchange.com/questions/81434/compare-regressions-among-more-than-2-groups"">this</a> link. But this link is old. I also checked <a href=""https://cran.r-project.org/web/packages/twang/"" rel=""nofollow"">this</a> package which seems to do multilevel. But is there any other option for packages?</p>
"
"0.0969003166223018","0.0904683580569682","228238","<p>I'm having a strange problem running a meta-regression using the function <code>rma.mv()</code> in the 'metafor' package in R.</p>

<p>Since some of my data are from multiple-endpoint studies, I have calculated the variance-covariance matrix so that correlations between outcomes are taken into account. I'm also using random effects at study and treatment level. As far as I'm aware, I have now covered all issues with regard to dependent effect sizes.</p>

<p>The model looks like this:</p>

<pre><code>cov_mod &lt;- rma.mv(Hedges_g, cov, mods = ~ days, random = ~ treatment | study, data = rev)
</code></pre>

<p>When running the code, it gives this error message:</p>

<pre><code>Error in rma.mv(Hedges_g, cov9, mods = ~days, random = ~1 | treatment/study,  : 
  Error during optimization.
In addition: Warning message:
In rma.mv(Hedges_g, cov9, mods = ~days, random = ~1 | treatment/study,  :
  V appears to be not positive definite.
</code></pre>

<p>I have discovered that the problem lies with one particular study (9 effect sizes in total, coming from 3 treatment groups that were each tested at 3 moments in time). When I remove this study from the data set, the code runs without problem.</p>

<p>Thus, apparently this particular study causes the matrix to be 'not positive definite'. I have read that this likely means that ""at least one of [the] variables can be expressed as a linear combination of the others"" (<a href=""http://stats.stackexchange.com/questions/30465/what-does-a-non-positive-definite-covariance-matrix-tell-me-about-my-data"">source</a>).</p>

<p>However, here comes the strange thing: I have replaced all values in the variance-covariance matrix relating to this particular study with random numbers between 0-1 (maintaining the symmetry), and the error message remains unchanged. I am puzzled, because the matrix can no longer be linearly predictable if it contains random numbers.</p>

<p>What could be the issue?</p>
"
"0.0419590679148345","0.0427352161723642","228257","<p>I have performed mixed effect Cox hazard regressions, and <a href=""http://stats.stackexchange.com/questions/228229/reconstructing-slopes-in-mixed-effect-models"">reconstructed the slopes</a> to get group specific slopes (e.g. sex-specific responses to the explanatory variable). I aim to test whether the slopes differ from one another (e.g. do males and females respond differently to the explanatory variable?). To do this I will use Z-tests (<a href=""http://stats.stackexchange.com/questions/55501/test-a-significant-difference-between-two-slope-values"">here</a> and <a href=""http://stats.stackexchange.com/questions/13112/what-is-the-correct-way-to-test-for-significant-differences-between-coefficients"">here</a>) where</p>

<p>$$Z=  \frac{\beta_1-\beta_2}{\sqrt{{SE_{\beta_1}}^{2}+{SE_{\beta_2}}^2}}$$</p>

<p>However, I have performed my models in R using the coxme package which gives the following output, from which I reconstruct the sex- and group-specific slopes with the included function.</p>

<pre><code>...
Fixed coefficients
                        coef exp(coef)   se(coef)      z    p
SexM             0.091305017 1.0956031 0.09085235   1.00 0.31
GroupG2         -0.036313825 0.9643376 0.08889039  -0.41 0.68
NE              -0.192009224 0.8252993 0.01317388 -14.57 0.00
SexM:GroupG2     0.009757875 1.0098056 0.12750426   0.08 0.94
SexM:NE         -0.212264676 0.8087506 0.02008058 -10.57 0.00
GroupG2:NE      -0.006933708 0.9930903 0.01814987  -0.38 0.70
SexM:GroupG2:NE  0.044999019 1.0460268 0.02756553   1.63 0.10
...


coxSlopeFunc = function(model, nfixed = 1){
    if(nfixed ==1){
        # Slope for Females + G1
        FG1 = model$coefficients[3]
	# Slope for Males + G1
	MG1 = model$coefficients[3] + model$coefficients[5]

        # Slope for Females + G2
        FG2 = model$coefficients[3] + model$coefficients[6]
        # Slope for Males + G2
        MG2 = model$coefficients[3] + model$coefficients[5] + model$coefficients[6] + model$coefficients[7]

        # Sex differences in slope
        SG1 = FG1 - MG1
        SG2 = FG2 - MG2

    matrix(c(FG1,MG1,FG2,MG2,SG1,SG2), ncol = 1, byrow = T)}
}
round(coxSlopeFunc(coxdum),3)

&gt; round(coxSlopeFunc(coxdum),3)
       [,1]
[1,] -0.192
[2,] -0.404
[3,] -0.199
[4,] -0.366
[5,]  0.212
[6,]  0.167
</code></pre>

<p>However, I am unsure how to calculate the SE of the slope for each - should I just sum the standard errors of the components? </p>

<p>$$\frac{(-0.192009224 - (-0.192009224 + -0.212264676))}{\sqrt{0.01317388^2 + (0.01317388 +  0.02008058)^2}}$$</p>
"
"0.0843408998948762","0.0944911182523068","228641","<p>Consider the following dataset I want to use as the independent variables to conduct linear regression on:</p>

<pre><code>set.seed(42)
sa = runif(10)
sb = runif(10)
sc = sb+sa
sd = sb-sa
df = data.frame(sa,sb,sc,sd)
</code></pre>

<p>Now I want to perform tests for multicollinearity. I'm aware of the <code>ppcor</code> package, which calculates the partial correlation between the variables. In this case:</p>

<pre><code>&gt; pcor(df)
$estimate
            [,1]        [,2]       [,3]       [,4]
[1,]  1.00000000  0.06649968 -0.7325597  0.7706902
[2,]  0.06649968  1.00000000 -0.6304810 -0.6870502
[3,] -0.73255975 -0.63048097  1.0000000  0.1308260
[4,]  0.77069021 -0.68705016  0.1308260  1.0000000
</code></pre>

<p>As far as I know, there is no way of telling that <code>sc</code> and <code>sd</code> are linear combinations of <code>sa</code> and <code>sb</code>, just by looking at the estimates (or the other outputs of <code>pcor</code>, for that matter).</p>

<p>The only method that comes to my mind, is applying linear regression on each of the independent variables like so:</p>

<pre><code>summary(lm(sc~sa+sb,df))

Coefficients:
             Estimate Std. Error   t value Pr(&gt;|t|)    
(Intercept) 1.404e-16  3.915e-17 3.587e+00   0.0089 ** 
sa          1.000e+00  4.716e-17 2.121e+16   &lt;2e-16 ***
sb          1.000e+00  4.135e-17 2.418e+16   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 3.672e-17 on 7 degrees of freedom
Multiple R-squared:      1, Adjusted R-squared:      1 

summary(lm(sd~sa+sb,df))

Coefficients:
             Estimate Std. Error   t value Pr(&gt;|t|)    
(Intercept) 1.404e-16  3.915e-17 3.587e+00   0.0089 ** 
sa          1.000e+00  4.716e-17 2.121e+16   &lt;2e-16 ***
sb          1.000e+00  4.135e-17 2.418e+16   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 3.672e-17 on 7 degrees of freedom
Multiple R-squared:      1, Adjusted R-squared:      1 
</code></pre>

<p>I'm wondering two things: </p>

<ol>
<li><p>Is my approach with linear regression reasonable? I think the downside is, that it can only detect linear correlation. But non-linear correlation shouldn't be a problem with linear regression, right?</p></li>
<li><p>Is there an R function/package that automatically checks for multiple correlation?</p></li>
</ol>
"
"0.0419590679148345","0.0427352161723642","228679","<p>I understand that ""glmnet"" package has alpha and lambda regularization parameters which can be optimized by ""caret"" package's train function. Optimal lambda value and lambda values of trained model are in the image. </p>

<p><strong>Can some one please help me understand what these lambda values mean, do they mean while minimizing the criterion function of multinomial regression, the aforementioned lambda values represent values at each iteration?</strong></p>

<pre><code>library(caret)
library(nnet)
ctrl &lt;- trainControl(method = ""repeatedcv"", number = 10, savePredictions = TRUE)
model_train_glmnet &lt;- train(Class2 ~ ZCR + Energy + EntropyE + SpectralC + SpectralS + SpectralE + SpectralF + SpectralR + MFCC1 + MFCC2 + MFCC3 + MFCC4 + MFCC5 + MFCC6 + MFCC7 + MFCC8 + MFCC9 + MFCC10 + MFCC11 + MFCC12 + MFCC13, data = training, method=""glmnet"", trControl = ctrl, tuneLength = 5)

print(model_train_glmnet$finalModel$lambdaOpt)
[1] 0.007676627
&gt; 
&gt; print(model_train_glmnet$finalModel$lambda)
[1] 3.838314e-01 3.497328e-01 3.186635e-01 2.903543e-01 2.645601e-01
[6] 2.410573e-01 2.196424e-01 2.001300e-01 1.823510e-01 1.661514e-01
[11] 1.513910e-01 1.379418e-01 1.256875e-01 1.145217e-01 1.043479e-01
[16] 9.507796e-02 8.663150e-02 7.893539e-02 7.192299e-02 6.553355e-02
[21] 5.971173e-02 5.440710e-02 4.957373e-02 4.516973e-02 4.115698e-02
[26] 3.750071e-02 3.416925e-02 3.113375e-02 2.836791e-02 2.584778e-02
[31] 2.355154e-02 2.145928e-02 1.955290e-02 1.781587e-02 1.623316e-02
[36] 1.479105e-02 1.347706e-02 1.227979e-02 1.118889e-02 1.019490e-02
[41] 9.289211e-03 8.463983e-03 7.712066e-03 7.026948e-03 6.402693e-03
[46] 5.833895e-03 5.315628e-03 4.843402e-03 4.413128e-03 4.021078e-03
[51] 3.663856e-03 3.338369e-03 3.041798e-03 2.771573e-03 2.525354e-03
[56] 2.301009e-03 2.096593e-03 1.910338e-03 1.740629e-03 1.585996e-03
[61] 1.445100e-03 1.316722e-03 1.199748e-03 1.093165e-03 9.960517e-04
[66] 9.075652e-04 8.269396e-04 7.534766e-04 6.865398e-04 6.255495e-04
[71] 5.699774e-04 5.193422e-04 4.732052e-04 4.311670e-04 3.928633e-04
[76] 3.579624e-04 3.261620e-04 2.971867e-04 2.707854e-04 2.467296e-04
[81] 2.248108e-04 2.048393e-04 1.866419e-04 1.700611e-04 1.549534e-04
[86] 1.411878e-04 1.286450e-04 1.172166e-04 1.068034e-04 9.731524e-05
[91] 8.867002e-05 8.079282e-05 7.361541e-05 6.707562e-05 6.111681e-05
[96] 5.568736e-05 5.074025e-05 4.623262e-05 4.212544e-05 3.838314e-05
</code></pre>
"
"0.0740088392978143","0.0753778361444409","228781","<p>I recently estimated some OLS regressions with daily returns scaled by 100 as dependent variable (thus in percentage points). As I learned (and empirically confirmed), this scaling only scales coefficients by 100, but has no impact on statistical significance.</p>

<p>I used the same, scaled returns to estimate a simple EGARCH(1,1) model and found that the scaling directly impacts the coefficient for the unconditional variance <code>omega</code> and the significance of all other coefficients. The results are thus vastly different when using scaled data, especially when it comes to the interpretation of significance.</p>

<p>I used the SP500 data from the ""rugarch"" package in R and the <code>ugarchfit</code> function to produce following example:</p>

<p>Here with normal returns:</p>

<pre><code>        Estimate  Std. Error    t value Pr(&gt;|t|)  
mu      0.000670    0.000211     3.17900 0.001478               
ar1    -0.679036    0.017029   -39.87517 0.000000               
ma1     0.701977    0.016065    43.69611 0.000000               
omega  -0.269569    0.005428   -49.65921 0.000000               
alpha1 -0.197466    0.025594    -7.71537 0.000000               
alpha2  0.129236    0.005627    22.96744 0.000000               
beta1   0.970782    0.000080 12106.51240 0.000000               
gamma1 -0.009223    0.068496    -0.13465 0.892888               
gamma2  0.124195    0.055994     2.21802 0.026553               
shape   4.670759    0.848486     5.50481 0.000000                           
LogLikelihood : 3204.702 
</code></pre>

<p>And here with returns scaled by 100:</p>

<pre><code>        Estimate  Std. Error    t value Pr(&gt;|t|)  
mu      0.067050    0.021114   3.175639 0.001495  
ar1    -0.679038    0.016626 -40.840967 0.000000  
ma1     0.701978    0.016644  42.176242 0.000000  
omega  -0.000460    0.006190  -0.074301 0.940771  
alpha1 -0.197462    0.060240  -3.277926 0.001046  
alpha2  0.129237    0.061149   2.113468 0.034561  
beta1   0.970786    0.003998 242.815325 0.000000  
gamma1 -0.009222    0.073792  -0.124977 0.900542  
gamma2  0.124189    0.075641   1.641818 0.100628  
shape   4.670628    0.881931   5.295909 0.000000  

LogLikelihood : -1400.468 
</code></pre>

<p>Look especially at <code>omega</code> and the significance of <code>gamma2</code>!  </p>

<p>Does anybody know why this is the case?</p>
"
"0.027972711943223","0.0284901441149095","228878","<p>gling with the interpretation of the coefficients of a zero-inflation model and I find no clear answer in the net. Maybe someone can help me and other people in the same situation.</p>

<p>After fitting cancer incidences through a Poisson regression with zero-inflation (zeroinfl package in R), in the logistic component, the coefficient estimate for the age variable is -3.6.</p>

<p>Does that mean that for each additional year of age, the odds of having zero cancer incidences increases by 3.6, or vice versa?</p>

<p>Many thanks, Gion</p>
"
"0.0791187772129236","0.080582296402538","228985","<p>I have, as the title suggests, two heavily skewed, overdispersed histograms. The data ranges from 0 minutes to 85334 minutes. 90% of the data is below 15 minutes, and takes the form of a positive-skewed exponential/power distribution. Then, there's just a huge tail. There are two groups with similar data structuresâ€”one for <strong>Conversation A</strong> and <strong>Conversations B</strong>. </p>

<p>I'm solid enough with basic statistics to know that comparing the means, STD, p-values, etc. is pretty useless, but I'm not good enough to know <em>how</em> I can compare these two, or what metrics I can compare with one another to see if being in <strong>A</strong> or <strong>B</strong> has any significant effect on the data. I've done some research, and it looks like <em>negative binomial regression</em> fittings will suit my purposes best.</p>

<p>I'm using the <code>MASS</code> package in R, w/ the calls
<code>glm.nb(conversation$A_times ~ 1)</code>:</p>

<pre><code>Coefficients:
(Intercept)
    5.624

Degrees of Freedom: 1674 (i.e. Null); 1674 Residual
Null Deviance:      1850
Residual Deviance:  1850    AIC: 17130
</code></pre>

<p>and <code>glm.nb(conversation$B_times ~ 1)</code>:</p>

<pre><code>Coefficients:
(Intercept)
    4.768

Degrees of Freedom: 1072 (i.e. Null); 1072 Residual
Null Deviance:      1234
Residual Deviance:  1234    AIC: 12390
</code></pre>

<p>Now, I imagine that the goal here is to compare two coefficients (or sets thereof) for significant differences, but I'm not actually sure what to do with this info. What are some directions I can take to learn more and really figure out what I'm doing? </p>
"
"0.109095464059674","0.1241856590838","229477","<p><br>I am struggling to interpret the results of a binomial logistic regression I did.<br> The experiment has 4 conditions, in each condition all participants receive different version of treatment. <br>DVs (1 per condition)=DE01,DE02,DE03,DE04, <br>all binary (1 - participants take a spec. decision, 0 - don't)
<br>Predictors: FTFinal (continuous, a freedom threat scale)
<br>SRFinal (continuous, situational reactance scale)
<br>TRFinal (continuous, trait reactance scale)
<br>SVO_Type(binary, egoists=1, altruists=0)
<br>After running these binomial (logit) models,<br><br> <code>model_soc_inf&lt;- glm(mydata$DE01~FTFinal+SRFinal+TRFinal+SVO_Type,
                    family=binomial(link='logit'),data=mydata)
model_soc_inf1&lt;- glm(mydata$DE01~FTFinal+SRFinal+TRFinal,
                     family=binomial(link='logit'),data=mydata)
summary(model_soc_inf)
model_pers_inf &lt;- glm(mydata$DE02~FTFinal+SRFinal+TRFinal+SVO_Type,
                      family=binomial(link='logit'),data=mydata)
model_pers_inf1 &lt;- glm(mydata$DE02~FTFinal+SRFinal+TRFinal,
                       family=binomial(link='logit'),data=mydata)
model_pers_inf2 &lt;- glm(mydata$DE02~SRFinal+TRFinal+SVO_Type,
                       family=binomial(link='logit'),data=mydata)
summary(model_pers_inf)
model_soc_uninf&lt;-glm(mydata$DE03~FTFinal+SRFinal+TRFinal+SVO_Type,
                     family=binomial(link='logit'),data=mydata)
model_soc_uninf1&lt;-glm(mydata$DE03~FTFinal+SRFinal+TRFinal,
                      family=binomial(link='logit'),data=mydata)
summary(model_soc_uninf)
model_pers_uninf&lt;-glm(mydata$DE04~FTFinal+SRFinal+TRFinal+SVO_Type,
                      family=binomial(link='logit'),data=mydata)
model_pers_uninf1&lt;-glm(mydata$DE04~FTFinal+SRFinal+TRFinal,
                       family=binomial(link='logit'),data=mydata)
summary(model_pers_uninf)</code><br><br>I ended up with the following<a href=""http://i.stack.imgur.com/4JKLa.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/4JKLa.png"" alt=""enter image description here""></a>. Initially I tested 2 models per condition, when condition 2 (DE02 as a DV) got my attention. In model(3)There are two variables, which are significant predictors of DE02 (taking a decision or not) - FTFinal and SVO Type. In context, the values for model (3) would mean that all else equal, being an Egoist (SVO_Type 1) decreases the (log)likelihood of taking a decision in comparison to being an altruist. Also, higher scores on FTFinal(freedom threat) increase the likelihood of taking the decision. So far so good. Removing SVO_Type from the regression (model 4) made the FTFinal coefficient non-significant. Removing FTFinal from the model does not change the significance of SVO_Type.</p>

<p>So I figured:ok, mediaiton, perhaps, or moderation. I tried first to look for mediation in both in R and SPSS. The moderation attempt was in vain: entering an interaction term SVO_Type:FTFinal makes all variables in model(3) non-significant.Here's the code for that:<code>model1&lt;-glm(DE02~FTFinal,family=binomial(link='logit'),data=mydata)
summary(model1)
model2&lt;-glm(DE02~SVO_Type,family=binomial(link='logit'),data=mydata)
summary(model2)
model3&lt;-glm(DE02~FTFinal+SVO_Type,family=binomial(link='logit'),data=mydata)
summary(model3)
interaction&lt;-glm(DE02~SVO_Type+FTFinal+SVO_Type:FTFinal, family =binomial(
  link = ""logit""),data = mydata)</code> <br>As for mediation, I followed  <a href=""http://www.nrhpsych.com/mediation/logmed.html"" rel=""nofollow"">this</a> mediation procedure for logistic regression, but found no mediation. </p>

<p>To sum up:
There is some relationship between SVO_Type and FTFinal, but I have no clue what.
Predicting DE02 from SVO_Type only is not significant.
Predicting DE02 from FTFinal is not significant
Putitng those two in the regression makes them both significant predictors.
Including an interaction between these both in any model, predicting DE02 model makes all variables in the model insignificant.<br>
So I am at a total loss: As far as I know, to test moderation, you need an interaction term. This term is between a categorical var (SVO_Type) and the continuous one(FTFinal), perhaps that goes wrong? And to test mediation outside SPSS, I tried the ""mediate"" package in R, only to discover that there is a ""treatment"" argument in the main funciton, which is to be the treatment variable (exp Vs cntrl). I don't have such, all ppns are subjected to different versions of the same treatment. 
I apologize for <a href=""http://www.filedropper.com/mydata"" rel=""nofollow"">this external way of uploading the dataset</a>, it is way too complicated to reproduce here (I am a noob).
Any help would be greatly appreciated. I have no clue what the relationship between SVO_Final and FTFinal is.</p>
"
"NaN","NaN","229542","<p>Im doing a multiple imputation of a dataset using R's MICE package. </p>

<pre><code>imp &lt;- mice(nhanes, m=5, print = FALSE, seed = 55152)
</code></pre>

<p>I figured out that to pool regression coefficients you really only need to get the mean of the 5 regression coefficients for the 5 datasets. </p>

<p>But now i need to pool means, confidence intervals and standard deviation using Rubin's rules. </p>

<p>How do i do that? </p>

<p>/Kind regards</p>
"
"0.0927749898843639","0.0944911182523068","229884","<p>I have a cancer classification problem (type A vs type B) on radiological images from which i have generated 756 texture-based predictive features (wavelet transform followed by texture analysis, i.e., features described by Haralick, Amasadun etc) and 8 semantic features based on subjective assessment by expert radiologist. This is entirely for research and publication to show that these predictive features may be useful in this particular problem. I do not intend to deploy the model for practitioners. </p>

<p>I have 107 cases. 60% cases are type A and 40% type B (in keeping with their natural proportions in population). I have done several iterations of model development with varying results. One particular method is giving me an 80% 80% classification accuracy but I am suspicious that my method is not going to stand critical analysis. I am going to outline my method and a few alternatives. I will be grateful if someone can pick if it is flawed. I have used R for this:</p>

<p>Step 1: Split into 71 training and 36 test cases.<br>
Step 2: remove correlated features from training dataset (766 -> 240) using findcorrelation function in R (caret package)<br>
Step 3: rank training data features using Gini index (Corelearn package)<br>
Step 4: Train multivariate logistic regression models on top 10 ranked features using subsets of sizes 3 , 4, 5 ,and 6 in all possible combination (<sup>10</sup>C<sub>3</sub>=252, <sup>10</sup>C<sub>4</sub>=504, <sup>10</sup>C<sub>5</sub>=630). So <strong>total 1386 multivariate logistic regression models were trained</strong> using 10-fold cross-validation and tested on test dataset.<br>
Step 5: Of these I selected a model which gave the best combination of training and test dataset accuracy, i.e., 3 feature model with 80% 80% accuracy.<p></p>

<p>Somehow running 1300 permutations seems quite dodgy to me and seems to have introduced some false discovery. Just want to confirm if this is a valid ML technique or whether I should skip step 4 and only train on top 5 ranked features without running and permutations.</p>

<p>Thanks. <p> PS I experiemented a bit with naive bayes and random forests but get rubbish test set accuracy so dropped them</p>

<p>====================</p>

<h1>UPDATE</h1>

<p>Following discussion with SO members, i have changed the model drastically and thus moved more recent questions regarding model optimisation into a new post <a href=""http://stats.stackexchange.com/questions/232829/lasso-regularised-classification-highly-variable-choice-of-lambda-min-on-repeate"">LASSO regularised classification highly variable choice of lambda.min on repeated cv</a></p>
"
"0.0843408998948762","0.0944911182523068","230201","<p>I'm using the glmnet package in R to do ridge regression. When I have a full set of dummy variables (if you took a horizontal sum of all these dummy variables you would get the constant), ridge regression with lambda = 0 is NOT dropping any of the dummy variables. In contrast, OLS gives the expected result by dropping at least 1 of the dummies to prevent perfect multi-collinearity. I'd like to know why the discrepancy exists. </p>

<pre><code> library(glmnet)
 set.seed(1)
make_dummies_out_of_factors&lt;- function(your_df, names_of_factor_variables) {
  indices&lt;- which(names(your_df) %in% names_of_factor_variables) #Finds columns corresponding to factor variables
  model_matrices_list&lt;- lapply(indices, function(x) {
    model.matrix(~your_df[,x] - 1, your_df)
  })
  #create a model matrix for each factor variable, and stores each one as a list
  model_matrices_together&lt;- do.call(cbind, model_matrices_list)
  #Column bind all model matrices which are stored as lists
  final&lt;- cbind(your_df, model_matrices_together)
  #Column bind all the model matrices to the original data
  final&lt;- final[,-indices]
  #Get rid of the original factor variables

  names(final)&lt;- gsub(""your_df.*\\]"", ""dummy_"", names(final))
  #Give appropriate names to the dummies

  return(final)
}
test_df&lt;- data.frame(numeric1 = rnorm(1000), numeric2 = rnorm(1000), 
                     state = rep(letters[1:4], 250), year = rep(c(""yr1"", ""yr2""), 500)) #This data frame has 2 factor variables
test_df&lt;- make_dummies_out_of_factors(test_df, names_of_factor_variables = c(""state"", ""year""))

linear_alldum&lt;- lm(test_df$numeric2 ~ test_df$numeric1 + test_df$dummy_yr1 + test_df$dummy_yr2 + test_df$dummy_a + 
                     test_df$dummy_b + test_df$dummy_c + test_df$dummy_d)


X_test&lt;- as.matrix(test_df[,-1]) #Remove dependent variable out of X matrix
y_test&lt;- test_df[,1] #This is the dependent variable

ridge_alldum&lt;- glmnet(x = X_test, y = y_test, lambda = seq(200, 0, by = -1), alpha = 0)


comparison = data.frame(as.matrix(coef(ridge_alldum))[,201], coefficients(linear_alldum))
names(comparison)[1]&lt;- ""coefficients_ridge_l0""
names(comparison)[2]&lt;- ""coefficients_linear_reg""
#Note that coefficients aren't identical, and that ridge regression doesn't drop coefficients. 

prediction_linear&lt;- predict(linear_alldum)
prediction_ridge&lt;- predict(ridge_alldum, newx = X_test, s = 0)
predictions&lt;- data.frame(prediction_linear, prediction_ridge = prediction_ridge)
names(predictions)[2]&lt;- ""prediction_ridge""

#Note that the predictions using linear regression and ridge regression aren't the same. 

sapply(predictions, mean) #Means of predictions using linear and ridge.
sapply(predictions, sd) #SDs of predictions using linear and ridge. 
</code></pre>
"
"0.0395593886064618","0.040291148201269","230456","<p>I have a large n (>1,000,000) dataset with a small number of features to estimate (regression) random forest and have been looking to implement Rborist (in R). I'd like to parallelize my work, but am not finding much guidance on how that would be done. I have 16 processors to use on the machine where it's running. When I use doParallel with the randomForest package, for example, the command:</p>

<blockquote>
  <p>rf &lt;- foreach(ntree=rep(32, 16), .combine=combine, .packages='randomForest') %dopar% randomForest(x, y, nodesize = 25, ntree=ntree)</p>
</blockquote>

<p>It launches 16 R processes, and works slowly as randomForest does, but works.</p>

<p>The analogous command for Rborist:</p>

<blockquote>
  <p>rb &lt;- foreach(ntree=rep(32, 16), .combine=combine, .packages='Rborist') %dopar% Rborist(x, y, minNode = 25, ntree=ntree)</p>
</blockquote>

<p>Throws the error:</p>

<blockquote>
  <p>error calling combine function:
  
  Warning message:
  In mclapply(argsList, FUN, mc.preschedule = preschedule, mc.set.seed = set.seed,  :
    all scheduled cores encountered errors in user code</p>
</blockquote>

<p>Does anyone know how to parallelize with Rborist? It does not appear to be happening under the hood as it's only using 1 cpu when I run:</p>

<blockquote>
  <p>rb &lt;- Rborist(x, y, minNode = 25, ntree = 512)</p>
</blockquote>
"
"NaN","NaN","230521","<p>I have the typical linear regression model:</p>

<p>$$y_i = x_i^T\beta + e_i,$$
where $e_i\sim N(0,\sigma^2)$, iid. However, in my case, <em>some</em> (not all of them, only around 1/3 of them) response variables $y_i$ are right-censored. As far as I understand, I cannot apply LASSO (R package <code>lars</code>) directly. What alternative methods (I would be interested in a LASSO-type method) are there for selecting variables with right-censored outcomes? If they are implemented in R, much better.</p>
"
"0.0839181358296689","0.0854704323447285","230532","<p>Trying to get the Bayes Factor for a correlation between two variables in my data, I tried three different functions. All implement the Jeffreysâ€“Zellnerâ€“Siow (JZS) prior, but I get quite different results with the three approaches. Two questions:</p>

<ol>
<li><p>Is this suspicious, or is it reasonable that they produce different values, as the implementations are slightly different?</p></li>
<li><p>Is there a consensus on the best measure to use?  </p></li>
</ol>

<p>My data:</p>

<pre><code>a=rnorm(100,1,2)
b=rnorm(100,.8,1.5)
myData &lt;- data.frame(a=a, b=b)
</code></pre>

<p>I try the <code>jzs_corbf</code> function, described and implemented <a href=""http://www.ncbi.nlm.nih.gov/pubmed/22798023"" rel=""nofollow"">here</a> (<a href=""http://dsquintana.com/post/98962697485/how-to-calculate-a-bayes-factor-for-correlations"" rel=""nofollow"">shorter version</a>)</p>

<pre><code>cor.resu.a_b &lt;- cor.test(myData$a, myData$b, method=c(""pearson""))
cor.resu.a_b$estimate
n = 100
r = cor.resu.a_b$estimate
jzs_corbf(r,n)
[1] 0.08206358
</code></pre>

<p>I also tried the convenience function from the <code>BayesFactor</code> package:</p>

<pre><code>require(BayesFactor)
regressionBF(b ~ a, data = myData, progress=FALSE)

Bayes factor analysis
--------------
[1] a : 0.2181081 Â±0%

Against denominator:
  Intercept only 
---
Bayes factor type: BFlinearModel, JZS
</code></pre>

<p>And I also tried the a function described <a href=""http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4891395/"" rel=""nofollow"">recently</a> (<a href=""https://osf.io/9d4ip/"" rel=""nofollow"">code</a>)</p>

<pre><code>bf10JeffreysIntegrate(n=100, r=r)

      cor 
0.1297927
</code></pre>

<p>While in this case the differences are only numerical, in my real data I get quite big differences that make it more difficult to decide on an interpretation. </p>

<p><a href=""http://stats.stackexchange.com/questions/184950/calculating-bayes-factor-from-a-correlation-coefficient"">Related</a></p>
"
"0.0740088392978143","0.0753778361444409","230581","<p>My problem is the following, my data has a lot of branch off points and the tree grows very rapidly. The end result is not readable, the end nodes are overlapped and even conversion to rules is more or less useless. 
I am using the rpart package. </p>

<pre><code>#Scoring model
d = sort(sample(nrow(Memmbers),nrow(Memmbers)* .6))
#select training sample
train&lt;-Memmbers[d, ]
test&lt;-Memmbers[-d, ]


s&lt;-glm(verifikation ~ . - userId,data = Memmbers,family = binomial())
summary(s)

library(ROCR)

#score test data set 
test$score &lt;- predict(s,type='response',test)
pred&lt;-prediction(test$score,test$verifikation)
perf&lt;- performance(pred,""tpr"",""fpr"")
plot(perf)

max(attr(perf,'y.values')[[1]]-attr(perf,'x.values')[[1]])

#get results of terms in regression 
g&lt;-predict(s,type='terms',test)
#function to pick top 3 reasons
#works by sorting coefficient terms in equation
# and selecting top 3 in sort for each loan scored 
ftopk&lt;- function(x,top=3){
  res=names(x)[order(x, decreasing = TRUE)][1:top]
  paste(res,collapse="";"",sep="""")
}
# Application of the function using the top 3 rows
topk=apply(g,1,ftopk,top=3)
#add reason list to scored tets sample
test&lt;-cbind(test, topk)

library(rpart)
library(rattle)

fit1 &lt;- rpart(verifikation ~ . - userId, data = train)
fancyRpartPlot(fit1);
test$t&lt;-predict(fit1,type='class',test)

################## PLot tree with priors 
#score test data 
test$score1 &lt;- predict(fit2,type = 'prob',test)
pred5&lt;-prediction(test$score1[,2],test$verifikation)
perf5&lt;- performance(pred5,""tpr"",""fpr"")

#90-10 priors with smaller complexity parameter to allow more complex trees
fit2 &lt;- rpart(verifikation ~ . - userId , data = train,method = ""class"",parms = list(prior=c(.9,.1)),cp=.0002)
plot(fit2);text(fit2,pos=2,cex=0.1,col=""blue"");

#compare complexity
printcp(fit1)
printcp(fit2)
plotcp(fit2)

#convert trees to rules 
amess&lt;-asRules(fit2)
t.b&lt;-rpart.rules.table(fit2)
library(rattle)
library(rpart.plot)
library(RColorBrewer)

fancyRpartPlot(fit2)
</code></pre>

<p>And here is the output of fancyRpartPlot(fit2) </p>

<p><a href=""http://i.stack.imgur.com/otsP3.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/otsP3.png"" alt=""enter image description here""></a></p>

<p>My goal is to extract some useful rules from the entire process to implement in a score card. </p>
"
"0.108550066801774","0.117467873474841","230709","<p>I'm working on a dataset with around 7000 points, a binary response variable and (at minimum) five predictors, two of which are binary, one is a number, and two are factors. One of those factors (""speaker"") is the participant id, with 26 levels once participants with categorical behaviour are excluded. The other (""vowel"") is a variable with six levels with represents the main condition I'm interested in. I (mostly) have a minimum of 20 tests per participant-condition combination. What I'm specifically trying to investigate is whether the effect of the different conditions differs for different participants. For that reason, I've tried to fit a binomial regression with an interaction between speaker and vowel. Here's the call:</p>

<pre><code>glm_vowel_interactions&lt;-glm(formula = perceptually.rhotic ~ vowel * speaker + function_word + modified_clip_start + prepausal, family = ""binomial"", data =data_excluding_rare_vowels_and_categorical_speakers)
</code></pre>

<p>This appears to work. However, it produces some alarmingly large (> 15 and &lt; -15) coefficient estimates. I ran it with the safeBinaryRegession package, and sure enough, it reports that separation is occurring with 33 points - 30 specific vowel/speaker (participant) combinations (of the 162 total combinations) and two specific values of ""speaker"" (which I think represent separation in the combination of those participants and the reference level for ""vowel""):</p>

<pre><code>Error in glm(formula = perceptually.rhotic ~ vowel * speaker + function_word +  : 
  The following terms are causing separation among the sample points: speaker4, speaker7, vowelNURSE:speaker19, vowellettER:speaker21, vowelNURSE:speaker21, vowelNORTH~FORCE:speaker22, vowelNEAR:speaker24, vowelNURSE:speaker27, vowellettER:speaker28, vowelSTART:speaker28, vowellettER:speaker4, vowelNEAR:speaker4, vowelNORTH~FORCE:speaker4, vowelNURSE:speaker4, vowelSTART:speaker4, vowelNEAR:speaker6, vowelNORTH~FORCE:speaker6, vowelSTART:speaker6, vowellettER:speaker7, vowelNEAR:speaker7, vowelNORTH~FORCE:speaker7, vowelNURSE:speaker7, vowelSTART:speaker7, vowelSTART:speakerb3, vowelNEAR:speakerb5, vowelNURSE:speakerb5, vowelSTART:speakerb5, vowelNEAR:speakerb6, vowelNEAR:speakerb7, vowelSTART:speakerb7, vowelNEAR:speakerb8, vowelNURSE:speakerb8, vowelSTART:speakerb8
</code></pre>

<p>Looking at my data I find that many of these are indeed categorical cells, with participants producing 100% or 0% TRUE of the response variable in that condition. As there are participants who overall produce mostly TRUE (up to 96%) or mostly FALSE (as low as 5%), it's unsurprising that there should be some categorical cells and doesn't necessarily imply that there's anything drastically unusual going on in these cells.</p>

<p>So, how can I investigate the differing effect of this condition (""vowel"") on the response variable for the different participants, while taking into account my other independent variables? I assume I can't interpret the results of the regression that did converge without safeBinaryRegression, as the coefficients and p-values are likely to be inflated? I came across <a href=""http://www.carlislerainey.com/papers/separation.pdf"" rel=""nofollow"">this paper</a> which suggests adding a prior to limit the largest possible coefficient estimates, so that instances with separation aren't infinite - would this be a sensible approach here? If so, is it implemented in R? What packages are recommended? Alternatively, should I be using some other (non regression?) method to investigate this question?</p>

<p>Thanks very much for your help!</p>

<p>Full disclosure: I've cross-posted this on the Maths stackexchange, but my experience is that here is more active!</p>
"
"0.0197796943032309","0.040291148201269","230760","<p>I need to do a multinomial logistic regression with a nominal variable, and I've heard about the Gmulti package in r ,and how it provides automatic selection methods models, However all the examples that i found are only applied on binaire logistic regression, so I wonder if it's even working on a multinomial logistic regression and in case is true, is the Gmulti take in consideration the problem of multicolinearity between the independents variables. Please help me thank you </p>
"
"0.027972711943223","0.0284901441149095","230776","<p>I'am trying to do a multinomial logistic regression to explain the political orientations in Tunisia but I'am really confused about using the vglm() function from package VGAM or the multinom() function from package nnet or the mlogit function... can someone tell me please the difference and wich one is the best ?
Thank you very much</p>
"
"0.122082923688585","0.124341182825498","231066","<p>*EDIT: I ran test again with data set provided and realized that the cause of problem is definitely rank deficiency, because estimated values of parameters in nonlinear regression showed non existing p values and there was no way to create confidence intervals with this data. </p>

<h2>Thank you all for reading and help! This question is closed.</h2>

<p>I researched seed germination. I took 75 seed replicates and put them in  different ecological parameters (like temperature) and took data about sprouts in different time intervals. </p>

<p>Reading statistical science papers about this topic, I found that I should analyze my data in a time-to-event model (dose response curve), where I can use log-logistic regression or nonlinear regression (Ritz et al., 2013 -<a href=""http://dx.doi.org/10.1016/j.eja.2012.10.003"" rel=""nofollow"">http://dx.doi.org/10.1016/j.eja.2012.10.003</a>). </p>

<p>Two models (nonlinear and log-logistic) lead to quantitatively very similar fitted germination curves, i.e., similar parameter estimates, but qualitatively different statements about the precision of estimates. Nonlinear regression model yields an overly precise estimate of the proportion of seeds that germinated during the experiment, so the precision reported by the nonlinear regression is too high.</p>

<p>Similarly, the 95% confidence intervals of the fitted curves also demonstrate the dramatic difference in precision of the two models: Accurate prediction of germination percentages is not warranted by the data unless very low percentages are of interest.</p>

<p>Because of that I choose log-logistic regression as a model. First few data sets; treatments analyzed in R using analysis of Dose-Response Curves (drc package) went smooth, and I was able to plot and get final graph. Such data, which was successfully analyzed, contained treatments where max seeds germination was for example 50% of total seed number.</p>

<p>Example:</p>

<p><a href=""http://i.stack.imgur.com/yUqOu.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/yUqOu.jpg"" alt=""Example of successful data analysis""></a></p>

<p>The problems arose when I entered the log-logistic model with treatment where all the seeds germinated in a short amount of time (meaning the treatment for this set of seeds is most adequate for their successful sprouting). For example, 100% of seeds germinated in only 5 days, so there are only two or three time intervals and a large number of sprouted seeds. The R program here reported  convergence error:</p>

<pre><code>Error in optim(startVec, opfct, hessian = TRUE, method = optMethod, control = list(maxit = maxIt,  : 
non-finite value supplied by optim
Error in drmOpt(opfct, opdfct1, startVecSc, optMethod, constrained, warnVal,  : 
Convergence failed 
</code></pre>

<p>Since I'm still a student in biology I have a very basic knowledge in statistics, so I tried to solve the problem with literature. </p>

<p>At first I thought that convergence failed because of perfect or complete separation, but through longer research it seems that the problem lies in rank deficiency. </p>

<p>When I analyzed the same data with nonlinear regression I've managed to fit curve and plot a graph without a problem.  </p>

<p>So, is there a way to make log-logistic model work even though I have obviously small data in cases of 100% germination? Should I switch to nonlinear regression  even though the reported precision would be too high. </p>
"
"0.0559454238864459","0.0427352161723642","231974","<p>Many Gaussian process packages are available in R. For example there is $\textbf{BACCO}$ that offers some calibration techniques, $\textbf{mlegp}$ and $\textbf{tgp}$ focusing on treed models and parameter estimation and $\textbf{GPML}$ for Gaussian process classification and regression. </p>

<p>The problem with these packages is that the choice of correlation function is restricted. Only some choices are provided for building the correlation function (Gaussian, Matern, etc...).</p>

<p>Does anyone have specific experience on how to can insert my own correlation function and just use the optimization routines available in these packages which are specifically tailored for GP's. Or is there a package which allows me to do that ?</p>
"
"0.027972711943223","0.0284901441149095","232161","<p>I am using randomForest package in R and getting these results</p>

<pre><code>enter Random Forest on Global Rating and Predicted Rating               
  |      Out-of-bag   |             
    Tree    MSE %Var(y) 
    100 0.4703  54.4    
    200 0.4758  55.04   
    300 0.4755  55.01   
    400 0.4732  54.75   
    500 0.472   54.6    
    600 0.4714  54.53   
    700 0.4695  54.31   
    800 0.4692  54.28   
    900 0.4681  54.16   
    1000    0.4669  54.02   

Call:               
 randomForest(formula = Rating ~ Prating + grating, data = mynewdata,      ntree = 1000, importance = T, do.trace = 100)                
               Type of random forest: regression                
                     Number of trees: 1000              
No. of variables tried at each split: 1             

          Mean of squared residuals: 0.4669436              
                    % Var explained: 45.98              
</code></pre>

<blockquote>
  <p>How to validate the model generated with these values is good or bad?
  Is %var explained similar to R^2 and why the %var(explained) is diff.
  from out of bag % var(target).</p>
</blockquote>
"
"0.0740088392978143","0.0753778361444409","232450","<p>We are developping a software that run hierarchical linear model in R with the lme4 package. The model we are trying to fit is of the following shape:</p>

<pre><code>&gt; data
ID | Dummy_1 | Dummy_2 | Dummy_3 | Rating 
1  | 0       | 1       | 1       | 14
1  | 1       | 0       | 1       | 15
1  | 0       | 1       | 0       | 11
2  | 1       | 0       | 1       | 15
2  | 1       | 0       | 0       | 12

x = lmer(formula = Rating ~ Dummy_1 + Dummy_2 + Dummy_3 + (1 + Dummy_1 + Dummy_2 + Dummy_3 | ID), data = data)
</code></pre>

<p>It is important to note that this is the shape that the data will take, however, Rating can have very different range depending on the data te user provide.</p>

<p>The example above illustrate a limit case we are trying to deal with which occurs when the dummy variables perfectly predict the independant variable.</p>

<p>Here we can see that for example with <code>intercept = 10</code>, <code>B1 = 2</code>, <code>B2 = 1</code> and <code>B3 = 3</code> we perfectly predict the Rating variable. It implies first that the <code>ID</code> is useless and that we are in a case of <code>complete separation</code>.</p>

<p><strong>Question:</strong> How do you deal with (quasi-)complete separation when the independant variable is not binomial but continuous as it is the case here ? I could only find explanations for logistic regression. Please, ignore the fact that I use a linear regression for discrete-ordinal data and that I treat them as continuous :)</p>

<p>So that you know the warnings I get from R are the following:</p>

<pre><code>1: In optwrap(optimizer, devfun, getStart(start, rho$lower,  ... :
  convergence code 1 from bobyqa: bobyqa -- maximum number of function evaluations exceeded
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  ... :
  unable to evaluate scaled gradient
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  ... :
  Model failed to converge: degenerate  Hessian with 4 negative eigenvalues
</code></pre>
"
"0.0484501583111509","0.0493463771219827","232682","<p>I have a <em>balanced panel</em> dataset in which I have observations for N countries across T quarters. I use a <em>fixed effect model</em> to explore the relationship between my variables. Since all countries have the same number of observations, they are all given the same ""weight"" in the regression. I would like to apply a weigthing by GDP such that observations of big countries (in terms of wealth) have more impact on my estimation. My first idea was to compute weights for all countries and rescale the dependent variable. However, I think this method will only affect the (country-specific) intercepts and won't have much impact on the coefficients. I also don't think that controlling for GDP by including it in the regression is the solution. </p>

<p>I am working with the R plm package to estimate my models. I know that the lm function has a weight parameter but couldn't find an equivalent in the plm package. </p>

<p>Any idea how I should proceed?</p>
"
"0.126175703579688","0.128509669926956","232829","<p>I am using CT scans to classify lung cancer into one of two types (Adenocarcinoma vs. Squamous carcinoma; we can abbreviate them A &amp; B). I am applying LASSO penalized logistic regression to a data set containing 756 CT-derived texture features and 12 radiologist identified categorical (yes/no) features. The latter are based on literature for relevance whereas the former are computer generated with no prior proof of being useful. I have 107 cases so my final dataframe (df) is 107 x 768 dimensional:</p>

<p>i)Texture features (mathematical quantities n=756) are continuous variables scaled and centered. Their names are stored in list <code>â€˜texVarsâ€™</code></p>

<p>ii)Semantic features(Qualitative features subjective assessed by experienced radiologist, n=12). These are usually categorical binary inputs of yes / no type. Their names are stored in list <code>â€˜semVarsâ€™</code>.</p>

<p>Following comments from community on my original (very different) model <a href=""http://stats.stackexchange.com/questions/229884/is-my-high-dimensional-data-logistic-regression-workflow-correct"">Is my high dimensional data logistic regression workflow correct?</a>, I performed my LR development in three steps:</p>

<p>1)Feature selection: I used principle components analysis to reduce texture feature-space from 756 to 30. I kept 4 most relevant (from literature) semantic features. This gave me 34 final features. I used the following command:</p>

<pre><code>trans = preProcess(df[,texVars], method=c(""BoxCox"", ""center"",   ""scale"", ""pca""),thresh=.95)  # only column-names matching â€˜texVarsâ€™ are included.
neodf2 &lt;- predict(trans,df[,texVars]).
neodf.sem &lt;- neodf2[,c(""Tumour"",""AirBronchogram"", ""Cavity"", ""GroundglassComponent"",""Shape"")]  # this DF is 107 x 4 dimensional, containing only 4 semantic features (most relevant from prior knowledge).
neodf.tex &lt;- neodf2[,c(""Tumour"",setdiff(names(neodf2),names(neodf.sem)))] # this only has the 30 PCA vectors (labelled PC1 â€“ PC30).
</code></pre>

<p>2) Model development (LASSO) and penalty term tuning (10fold cross-validation) using cv.glmnet command  Deviance was used as determinant of model quality. Using this method, I developed a model incorporating only semantic features, a second model incorporating only texture features, and a third model incorporating both semantic and texture features. Here are the commands:</p>

<pre><code>#Converting to model.matrix for glmnet 
xall &lt;- model.matrix(Tumour~.,neodf2)[,-1]
xtex &lt;- model.matrix(Tumour~.,neodf.tex)[,-1]
xsem &lt;- model.matrix(Tumour~.,neodf.sem)[,-1]
y &lt;- neodf$Tumour
require(glmnet)
grid &lt;- 10^seq(10,-2,length=100)

lasso.all &lt;- cv.glmnet(xall,y,alpha=1,lambda=grid, family=""binomial"", type.measure=""deviance"") 
lasso.tex &lt;- cv.glmnet(xtex,y,alpha=1,lambda=grid, family=""binomial"", type.measure=""deviance"")
lasso.sem &lt;- cv.glmnet(xsem,y,alpha=1,lambda=grid, family=""binomial"", type.measure=""deviance"")
</code></pre>

<p>3) Testing model classification accuracy on entire dataset. The following is the backbone of  bootstrap to generate 95% confidence intervals of predictive accuracy:</p>

<pre><code>pred &lt;- predict(lasso.all, newx = xall, s = ""lambda.min"", ""class"")
tabl &lt;- table(pred,y)
sum(diag(prop.table(tabl)))
</code></pre>

<p>4) As an alternative means to assess model performance than classification accuracy, I used ROC area under curve on entire dataset and compared AUROC curves from different models using DeLong's method (pROC package)</p>

<p>The results are interesting</p>

<pre><code> =================================================
</code></pre>

<p>LR MODEL BASED ON SEMANTIC FEATURES ALONE:
    lasso.sem$lambda.min
     0.01</p>

<p>Plot cv lambda vs. binomial devance <a href=""http://i.stack.imgur.com/6Modw.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/6Modw.png"" alt=""cv lambda vs binomia deviance""></a></p>

<pre><code>             Feature          Odds Ratio
1                (Intercept)  0.1292604
2      AirBronchogramPresent  0.1145378
3              CavityPresent 35.4350358
4 GroundglassComponentAbsent  4.3657928
5                 ShapeOvoid  2.4752881

AUC: .84


=================================================    
</code></pre>

<p>LR MODEL BASED ON TEXTURE FEATURES ALONE:</p>

<pre><code>lasso.tex$lambda.min
1e+10   
</code></pre>

<p>Plot  cv lambda vs binomial deviance (texture alone). Note how the 95% CI's are all overlapping! <a href=""http://i.stack.imgur.com/1Mk7M.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1Mk7M.png"" alt="" cv lambda vs binomial deviance ""></a></p>

<pre><code>   Feature OddsRatio
1 (Intercept) 0.6461538



============================================================
</code></pre>

<p>LR MODEL BASED ON TEXTURE + SEMANTIC FEATURES:</p>

<pre><code>lasso.all$lambda.min
0.05 
</code></pre>

<p>Plot  cv lambda vs binomial deviance <a href=""http://i.stack.imgur.com/p1AHX.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/p1AHX.png"" alt=""cv lambda vs binomial deviance""></a></p>

<pre><code>                          Feature   OddsRatio
1                (Intercept)        0.3136489
2                       PC23        0.9404430
3                       PC27        0.8564001
4      AirBronchogramPresent        0.2691959
5              CavityPresent        6.7422427
6 GroundglassComponentAbsent        2.0514275
7                 ShapeOvoid        1.5974378

 AUC : .88
</code></pre>

<p>Plot showing loglambda vs coefficients. The dashed vertical line shows the cross-validated optimum lambda:<a href=""http://i.stack.imgur.com/d6FaO.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/d6FaO.jpg"" alt=""enter image description here""></a></p>

<p>Having rejected the texture only model, which only contains intercept, i was left with two models - semantic and combined texture+semantic. I created ROC curves for both and compared them using DeLong's method:</p>

<pre><code>pred.sem&lt;- predict(lasso.sem, newx = xsem, s = ""lambda.min"")
pred.all&lt;- predict(lasso.all, newx = xall, s = ""lambda.min"")

roc.sem&lt;- roc(y,as.numeric(pred.sem), ci=TRUE, boot.n=1000, ci.alpha=0.95,stratified=FALSE)    

roc.all&lt;- roc(y,as.numeric(pred.all), ci=TRUE, boot.n=1000, ci.alpha=0.95,stratified=FALSE)
</code></pre>

<p>Outputs of ROC analysis are:</p>

<pre><code>data:  roc.sem and roc.all
Z = -2.1212, p-value = 0.0339
alternative hypothesis: true difference in AUC is not equal to 0
sample estimates:
AUC of roc1 AUC of roc2 
  0.8369963   0.8809524 
</code></pre>

<p>Showing that combined model ROC curve is significantly better despite the modest improvement in AUC (83% vs 88%).
Questions are:</p>

<p>a) is my methodology airtight from a publication point of view now? Apologies in advance for any gross errors in my presentation of this problem.</p>

<p>b) what is the formal inference that texture model is intercept only.</p>

<p>c) if texture model is useless, how do its variables become useful once added to semantic features and yield a higher overall accuracy in the combined result? perhaps that means the effect of texture features alone is too small to be detected in this small dataset but becomes apparent when combined with a stronger predictor (i.e., semantic features). </p>

<p>Any further comments are welcome.</p>
"
"0.0791187772129236","0.080582296402538","233007","<p>I'm currently reading the book <code>An R Companion to applied regression</code> and have started the section on effects plots which is a good method for seeing the effects of
independent variables on dependent variables.</p>

<p>The book explains the steps as follows</p>

<ul>
<li>Identify high order terms of a model (which seems to be when factors are multiplied by numeric vectors to produce interactions ~ the interactions are the high order terms)</li>
<li>It then seems that that all the other independent values are held constant while the value we are interested in seeing the effect for is varied. The constant seems to be mean</li>
</ul>

<p>I have three questions in relation to the plots</p>

<p>1) Although it is explained in the book, I'm having trouble getting my head around how the mean captured for factor variables when holding them constant</p>

<p>2) How is the Y value of the plot interpreted? Is it just a case of when the main line varies along the X-AXIS (independent variables), the Y Axis is the effect on the value to be predicted? If this is the case, how do we determine a large effect? I would assume a researcher would go back to the literature in whatever area they are studying to review previous researcher papers but effects seem to be rarely reported</p>

<p>3) How are interaction terms effects interpreted from the plot?</p>

<p>In essence, I'm trying to get a conceptual understanding to the plots. I assume that a plot with compact 95% confidence intervals and a steep slope is ideal scenario for the plot. </p>

<p>Taken from the <code>effects</code> package as an example</p>

<pre><code># Examples
summary(TitanicSurvival)
titanic &lt;- glm(survived ~ (passengerClass + sex + age)^2,
data=TitanicSurvival, family=binomial)
titanic.all &lt;- allEffects(titanic, typical=median,
given.values=c(passengerClass2nd=1/3, passengerClass3rd=1/3, sexmale=0.5))
plot(titanic.all, ticks=list(at=c(.01, .05, seq(.1, .9, by=.2), .95, .99)), ask=FALSE)

plot(effect(""passengerClass*sex*age"", titanic, xlevels=list(age=0:65)), ticks=list(at=c(.001, .005, .01, .05, seq(.1, .9, by=.2), .95, .99, .995)))
</code></pre>

<p>Thank you for your help</p>
"
"0.0625488854200668","0.0637058989297032","233178","<p>I have a database with 1200 observations and 14 variables and I'am trying to do a classification tree for my dependent nominal variable who hase 4 modality</p>

<pre><code>    &gt; table(testarbre2$Q99)

  Autres       Nahdha Ne pas voter Nidaa Tounes 
     248          351          303          298 
</code></pre>

<p>at firt i tried to do a multinom logistic regression but i got the mojority of my predictor variables non significant. it seems that Even with 1200 people I was trying to fit a model for which I don't have sufficient data. 
so i tried to do a classification tree using the package rpart from R 
but the problem is that the error is so high about 65% and more, and the missclassification is about 70% 
this is the code R that i used </p>

<pre><code>   #preparation of the data
   set.seed(26)
   train=sample(1:nrow(testarbre2),nrow(testarbre2)*7/10)
   test=-train
   training_data=testarbre2[train,]
   testing_data=testarbre2[test,]
   testing_vote=vote[test]

   #fitting the model
   library(rpart)
   library(rpart.plot)
   Tree &lt;- rpart(Q99~.,data=training_data)
   rpart.plot(Tree)
   printcp(Tree)
   plotcp(Tree)

    #Construction of the complete tree
  Tree &lt;-rpart(Q99~.,data=training_data,control=rpart.control(minsplit=50,cp=0))

     #Prune the tree
    treeOptimal &lt;- prune(Tree,cp=Tree$cptable[which.min(Tree$cptable[,4]),1])
    rpart.plot(treeOptimal)

   #Prediction
   a=predict(ptitanicOptimal,testing_data2,type = ""class"")
   mc=table(a,testing_vote2)
</code></pre>

<p>I don't know if i missed a step or i used a wrong approach in the construction of my classification tree or the database is causing the problem</p>

<p>Please someone help me to understand what's wrong with my model</p>
"
"0.111890847772892","0.113960576459638","234076","<p>I am looking to do time series forecasting with multiple variables. For example a data frame (df) of 4 different time series might look like this, where each column is its own time series: </p>

<pre><code>    X1 X2 X3 X4
1   4 13  2 81
2  24 91 86 58
3  21 97 39  1    
4   1 56 79 55
5  63  6 91 79
6  66 96 95 81 
</code></pre>

<p>Let's say X1 is 'cost' and the other variables are things like temperature, volume, and #_of_people.</p>

<p>I would like to forecast 'cost' using the other 3 variables. I imagine using something like <strong>Vector Autoregressive Models (VAR) for Multivariate Time Series</strong> can be used to see how each variable impacts the other in each separate time series (one for each variable). </p>

<p>For example, using the <strong>vars</strong> package in r we can run forecasts against the 4 time series and plot the results: </p>

<pre><code>var.2c &lt;- VAR(df, p = 2, type = ""const"")
var.2c.prd &lt;- predict(var.2c, n.ahead = 8, ci = 0.95)
fanchart(var.2c.prd)
</code></pre>

<p><a href=""http://i.stack.imgur.com/lq4VD.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/lq4VD.png"" alt=""enter image description here""></a></p>

<p>As I understand it, 4 separate regression models were built, one for each variable, where all the other variables were considered for each one. In other words, 'cost' was forecasted, taking into consideration not just the 'cost' trends, but also the impact the other 3 variables (X2, X3, and X3) had on 'cost.'</p>

<p>My question is, say I wanted to take a date in the future, on the forecast of cost, and see what happens to that forecasted value when temperature is increased (X2). I am assuming I can just take the coefficients in the 'cost' regression model that was used to forecast 'cost' using VAR, and use them as you would normally. For example, if the coefficient says the 'cost' will increase by 5 dollars for every one unit increase in temperature (X2), then I could take the forecasted value at the date of interest and add the $5, to say that is what would happen to the forecasted value if X2 were to change. </p>

<p>Are my intuitions correct here or am I missing something? Are there better ways to run 'what-if' analyses on forecasted multivariate time series?</p>
"
"0.0323001055407673","0.0493463771219827","234084","<p>I am new to multinomial logit regression, though I have done work with simple logisitc regression. I am attempting to build a model looking like the following:</p>

<pre><code>log(Pi/1-Pi) ~ X1 + X2 + X3
</code></pre>

<p>Where my dependent variable I am attempting to model for is in 4 levels: -2, 0, 3, or 7. I am running the model using the <code>nnet</code> package in <code>R</code>. While my model works, it doesn't show any prediction values of -2 or 3. I would expect this for the -2 because very little of the data resulted in -2 but a fair amount did show a 3 as the result. The model yielded a misclassification error of 34%. </p>

<p><strong>Would the reason for a binary response when I hoped for a 4-way response be something within my R code or is there an understanding of multinomial regression I am not understanding?</strong> I know this question is very general as I would like to understand not only the math behind what is happening but also I am working in R properly. Below is my R code.</p>

<pre><code>Drive$Points.2 &lt;- factor(Drive$Points)

library(nnet)
model.100 &lt;- multinom(Points.2 ~ X1 + X2 + X3, data = Drive)
summary(model.100)

cm &lt;- table(predict(model.100, Drive), Drive$Points.2)
cm
</code></pre>
"
"0.0395593886064618","0.040291148201269","234220","<p>I am new to time series analysis. I am trying to use the R package dlm for time-varying regression with multiple regressors. I got the basic dlmModReg to work, so I can get the coefficients for all the regressors as a function of time. My question is, what's the correct way to estimate the relative importance of each regressor at each time point (or within some window)? Thank you!</p>
"
"0.027972711943223","0.0284901441149095","234446","<p>I have a time series $Y_t$ that is stationary, and several explanatory variables $X$ .. $Z$ (stationary as well). </p>

<p>Is there an R package (or Python one) that can automatically fit all the possible predictive regressions of the form</p>

<p>$Y_t$ = a + b * L(X) + c * L(Y) + d * L(Z) + epsilon</p>

<p>where indicates the vector of lagged X valuesm ie L(X) = ($X_{t-1}$, $X_{t-2}$, ..). The package should tell me automatically (for a given max lag of course) which specification has the best in-sample prediction accuracy (by eventually dropping some of the predictive variables)?</p>

<p>Thanks!</p>
"
"0.0791187772129236","0.080582296402538","234537","<p>I've split my data set into a training and test set. I've performed a principal component analysis on the training set and have used the first 3 principal components to generate a logistic regression model for my response.</p>

<p>I now want to use this model to make predictions for my test data set and check if this is true. </p>

<p>I've been trying to use the predict function but obviously the model uses the principal components of the training set as the predictors whereas my test set just has all the original predictors so obviously they're not compatible.</p>

<p><strong>How do I go about 'projecting' my test data onto the principal components I've already generated so I can use my model to make predictions?</strong></p>

<p>Ideally I'd like to do this without using any external packages (it's for university). I am working in R.</p>
"
"0.027972711943223","0.0284901441149095","234672","<p>It's well known that there are <a href=""http://rstatistics.net/variable-importance-of-predictors-that-contribute-most-significantly-to-a-response-variable-in-r/"" rel=""nofollow"">a lot of feature selection libraries out there</a> for R. What are the most commonly used (and statistically sound) packages to use when selecting features for use in a logistic regression?</p>
"
"0.0927749898843639","0.0944911182523068","234690","<p>I am new to survival analysis. Below is my data with very unbalanced sample size (treat group has 2 samples with 1 event, 1 censored and control group has 700+ samples). I use Cox regression in 'survival' package in R and results show 3 different tests (likelihood ratio test, log rank test and Wald test). </p>

<pre><code>sample   trt    censor time
A7       TRT     0 1.0219178
BH       TRT     1 0.6136986
SB        C      0 0.7095890
SD        C      0 1.1972603
SE        C      0 3.6191781
..       ..     ..  ..
A1        C      0 4.0082192
</code></pre>

<p>My code:</p>

<pre><code>coxph(Surv(time,censor)~trt, data=dataAll)
</code></pre>

<p>Result:</p>

<pre><code>&gt; coxfit
Call:
coxph(formula = Surv(time, censor) ~ trt, data = dataAll)

  n= 772, number of events= 100 

                 coef exp(coef) se(coef)      z Pr(&gt;|z|)    
trtC -3.80047   0.02236  1.04854 -3.625 0.000289 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

             exp(coef) exp(-coef) lower .95 upper .95
trtC   0.02236      44.72  0.002864    0.1746

Concordance= 0.513  (se = 0.002 )
Rsquare= 0.007   (max possible= 0.73 )
Likelihood ratio test= 5.55  on 1 df,   p=0.01845
Wald test            = 13.14  on 1 df,   p=0.0002895
Score (logrank) test = 38.85  on 1 df,   p=4.579e-10
</code></pre>

<p>My questions are: </p>

<ol>
<li>There are 3 tests giving different p values, and they look quite different with the likelihood ratio test the most conservative. Do they all test for the significant of the Cox coefficient? Which one should I choose?</li>
<li>Give the fact that the treatment group has so few samples, could the p value trustable?</li>
<li>Is it appropriate to apply Cox regression to unbalanced sample? If no, is there any alternative methods?</li>
</ol>

<p>Thanks a lot!</p>

<p>J</p>
"
"0.0484501583111509","0.0493463771219827","234741","<p>I am looking for the equivalent of constrained nonlinear regression (CNLR command) in SPSS in R.</p>

<p>The constraints on the parameter needs to be a <em>function</em> rather than simply being that the parameters need to lie in certain region (thus having simple upper or lower bounds)</p>

<p>I have looked at the <em>nloptr</em> and <em>optim</em> packages but it appears not to be what I am looking for.</p>

<p>My SPSS syntax:</p>

<pre><code>MODEL PROGRAM a=0.21771052038064 b=0.69276310944875 c=1.3015054535771 
d=-0.95674643053895  .
COMPUTE PRED_ = a + b * X + c * X ** 2 + d * X ** 3 .
CONSTRAINED FUNCTION.
COMPUTE CONSTR1_= sin((a) + (b*(-1)) + 
            (c*(-1)**2) + (d*(-1)**3)  ) *
            cos((a) + (b*(-1)) + (c*(-1)**2) + (d*(-1)**3)  ) *
            ((b) + (2*c*(-1)) + (3*d*(-1)**2)  ) .
COMPUTE CONSTR2_= sin((a) + (b*(1)) + 
            (c*(1)**2) + (d*(1)**3)  ) *
            cos((a) + (b*(1)) + (c*(1)**2) + (d*(1)**3)  ) *
            ((b) + (2*c*(1)) + (3*d*(1)**2)  ) .

_set Printback off.
CNLR G
  /OUTFILE='TempFile'
  /PRED PRED_
  /BOUNDS CONSTR1_ &gt; 0;CONSTR2_ &gt; 0
  /SAVE PRED
  /CRITERIA STEPLIMIT 2 ISTEP 1E+20 .
</code></pre>

<p>a, b, c and d being my starting values, X my data points.</p>
"
"0.0625488854200668","0.0637058989297032","234763","<p>I have a fairly large dataset ($\approx 3 \bar{M}$ observations for a dozen candidate predictors) and I would like to perform a logistic regression on that dataset.
I have a problem of separation in that dataset so usual model can't converge. That's why I am using Firth penalization (logistf package for R) to have my model to adjust.</p>

<p>I would like to select the best subset of variables for my final model but I can't find the proper way to do that. I know that stepwise selection is out of question and I usually would use L1 or L2 penalized regression so that some coefficients are reduced to 0.</p>

<p>My problem is : the function I am using to adjust my model doesn't handle extra penalization so no Elasticnet-Firth regression.</p>

<p>Is there, apart from stepwise selection, another way to select my variables ?</p>

<p>Thanks in advance !</p>
"
"0.0625488854200668","0.0509647191437626","234846","<p>I'm trying to perform non-proportional survival analysis with competing risk in R.</p>

<p>Without the competing risk, to perform non-proportional analysis, i used timeSplitter in package Greg to split the dataset and then create time-dependent dummy variables:</p>

<p><code>spl_female_oadlt &lt;- timeSplitter(female_oadlt, 
                               by = 1,
                               time_var = ""CensorAge"",
                               event_var = ""Censor"")
spl_female_oadlt$SEI1_Time &lt;- (spl_female_oadlt$SEICat_End == 1)*spl_female_oadlt$Start_time
spl_female_oadlt$SEI2_Time &lt;- (spl_female_oadlt$SEICat_End == 2)*spl_female_oadlt$Start_time
spl_female_oadlt$SEI4_Time &lt;- (spl_female_oadlt$SEICat_End == 4)*spl_female_oadlt$Start_time
</code></p>

<p>and then use coxph to perform Cox regression:</p>

<pre><code>NonProp_20yrSEIcat &lt;- coxph(Surv(Start_time, Stop_time, CensorCOD_Cancer == 1) ~ factor(SEICat_End) + Age + cluster(id) + 
                          SEI1_Time + SEI2_Time + SEI4_Time,
                        ties = ""efron"",
                        data = spl_female_20yr_fu)
</code></pre>

<p>Without non-proportional analysis, to perform competing risk, I used coxph but censored for both alive and people died from other causes. CensorCOD_Cancer has 3 values (0 = Alive, 1 = Died from cancer, 2 = Died from other causes):</p>

<pre><code>CmpRsk_20yrSEIcat &lt;- coxph(Surv(CensorAge, CensorCOD_Cancer == 1) ~ factor(SEICat_End) + Age + cluster(id) + 
                          SEI1_Time + SEI2_Time + SEI4_Time,
                        ties = ""efron"",
                        data = spl_female_20yr_fu)
</code></pre>

<p>Anyone has any idea on how to do competing risk in non-proportional hazard models in R? And am I doing competing risk correctly?</p>

<p>Thanks.</p>
"
"0.027972711943223","0.0284901441149095","234911","<p>The <a href=""https://cran.r-project.org/web/packages/randomForest/randomForest.pdf"" rel=""nofollow"">default</a>/recommended value of mtry is <strong>P/3</strong> for regression task while it is <strong>SquareRoot(P)</strong> for a classification task. (<em>where P is number of variables)</em></p>

<p>As per my understanding the fundamental idea behind RF is using smaller subset of variables in Random Forest is to create weak &amp; diverse classifiers and aggregating them into one stronger classifier.</p>

<p>But I could not find any specific documentation on why a regression task has default of P/3 instead of <strong>SquareRoot(P)</strong>.</p>

<p><strong>Is it something specific to Random Forest?If so what is it?</strong></p>

<p>OR</p>

<p><strong>Is it more fundamental like difference between performance/construction of regression/classification trees which dictates RF.</strong></p>

<p>I'm not looking for a complete simplified answer, just point me to right literature links :)</p>

<p>Thanks!</p>
"
