"V1","V2","V3","V4"
"0.107676380411633","0.136082763487954","  1432","<p>In answering <a href=""http://stats.stackexchange.com/questions/1412/consequences-of-an-improper-link-function-in-n-alternative-forced-choice-procedur"">this</a> question John Christie suggested that the fit of logistic regression models should be assessed by evaluating the residuals.  I'm familiar with how to interpret residuals in OLS, they are in the same scale as the DV and very clearly the difference between y and the y predicted by the model.  However for logistic regression, in the past I've typically just examined estimates of model fit, e.g. AIC, because I wasn't sure what a residual would mean for a logistic regression.  After looking into R's help files a little bit I see that in R there are five types of glm residuals available, c(""deviance"", ""pearson"", ""working"",""response"", ""partial"").  The help file refers to Davison, A. C. and Snell, E. J. (1991) Residuals and diagnostics. In: Statistical Theory and Modelling. In Honour of Sir David Cox, FRS, eds. Hinkley, D. V., Reid, N. and Snell, E. J., Chapman &amp; Hall, of which I do not have a copy.  Is there a short way to describe how to interpret each of these types?  In a logistic context will sum of squared residuals provide a meaningful measure of model fit or is one better off with an Information Criterion?</p>
"
"0.159255514317652","0.161015297179883","  5304","<p>Dear everyone - I've noticed something strange that I can't explain, can you? In summary: the manual approach to calculating a confidence interval in a logistic regression model, and the R function <code>confint()</code> give different results.</p>

<p>I've been going through Hosmer &amp; Lemeshow's <em>Applied logistic regression</em> (2nd edition).  In the 3rd chapter there is an example of calculating the odds ratio and 95% confidence interval.  Using R, I can easily reproduce the model:</p>

<pre><code>Call:
glm(formula = dataset$CHD ~ as.factor(dataset$dich.age), family = ""binomial"")

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.734  -0.847  -0.847   0.709   1.549  

Coefficients:
                             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                   -0.8408     0.2551  -3.296  0.00098 ***
as.factor(dataset$dich.age)1   2.0935     0.5285   3.961 7.46e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 136.66  on 99  degrees of freedom
Residual deviance: 117.96  on 98  degrees of freedom
AIC: 121.96

Number of Fisher Scoring iterations: 4
</code></pre>

<p>However, when I calculate the confidence intervals of the parameters, I get a different interval to the one given in the text:</p>

<pre><code>&gt; exp(confint(model))
Waiting for profiling to be done...
                                 2.5 %     97.5 %
(Intercept)                  0.2566283  0.7013384
as.factor(dataset$dich.age)1 3.0293727 24.7013080
</code></pre>

<p>Hosmer &amp; Lemeshow suggest the following formula:</p>

<p>$$
e^{[\hat\beta_1\pm z_{1-\alpha/2}\times\hat{\text{SE}}(\hat\beta_1)]}
$$
</p>

<p>and they calculate the confidence interval for <code>as.factor(dataset$dich.age)1</code> to be (2.9, 22.9).</p>

<p>This seems straightforward to do in R:</p>

<pre><code># upper CI for beta
exp(summary(model)$coefficients[2,1]+1.96*summary(model)$coefficients[2,2])
# lower CI for beta
exp(summary(model)$coefficients[2,1]-1.96*summary(model)$coefficients[2,2])
</code></pre>

<p>gives the same answer as the book.</p>

<p>However, any thoughts on why <code>confint()</code> seems to give different results?  I've seen lots of examples of people using <code>confint()</code>.</p>
"
"0.134595475514541","0.136082763487954","  5354","<p>I've got some data about airline flights (in a data frame called <code>flights</code>) and I would like to see if the flight time has any effect on the probability of a significantly delayed arrival (meaning 10 or more minutes). I figured I'd use logistic regression, with the flight time as the predictor and whether or not each flight was significantly delayed (a bunch of Bernoullis) as the response. I used the following code...</p>

<pre><code>flights$BigDelay &lt;- flights$ArrDelay &gt;= 10
delay.model &lt;- glm(BigDelay ~ ArrDelay, data=flights, family=binomial(link=""logit""))
summary(delay.model)
</code></pre>

<p>...but got the following output.</p>

<pre><code>&gt; flights$BigDelay &lt;- flights$ArrDelay &gt;= 10
&gt; delay.model &lt;- glm(BigDelay ~ ArrDelay, data=flights, family=binomial(link=""logit""))
Warning messages:
1: In glm.fit(x = X, y = Y, weights = weights, start = start, etastart = etastart,  :
  algorithm did not converge
2: In glm.fit(x = X, y = Y, weights = weights, start = start, etastart = etastart,  :
  fitted probabilities numerically 0 or 1 occurred
&gt; summary(delay.model)

Call:
glm(formula = BigDelay ~ ArrDelay, family = binomial(link = ""logit""),
    data = flights)

Deviance Residuals:
       Min          1Q      Median          3Q         Max
-3.843e-04  -2.107e-08  -2.107e-08   2.107e-08   3.814e-04

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  -312.14     170.26  -1.833   0.0668 .
ArrDelay       32.86      17.92   1.833   0.0668 .
---
Signif. codes:  0 Ã¢***Ã¢ 0.001 Ã¢**Ã¢ 0.01 Ã¢*Ã¢ 0.05 Ã¢.Ã¢ 0.1 Ã¢ Ã¢ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.8375e+06  on 2291292  degrees of freedom
Residual deviance: 9.1675e-03  on 2291291  degrees of freedom
AIC: 4.0092

Number of Fisher Scoring iterations: 25
</code></pre>

<p>What does it mean that the algorithm did not converge? I thought it be because the <code>BigDelay</code> values were <code>TRUE</code> and <code>FALSE</code> instead of <code>0</code> and <code>1</code>, but I got the same error after I converted everything. Any ideas?</p>
"
"NaN","NaN","  5647","<p>I am fitting a conditional logistic regression model with 1:4 controls using <code>R</code>. I wish to obtain <code>AIC</code> from the model. How can I extract the appropriate parameters based on the object <code>m</code>?</p>

<pre><code>library(survival)
m&lt;-clogit(cc~exp+ factor1+ factor2 + strata(stratum),data=data1)
</code></pre>
"
"0.180578779628654","0.182574185835055","  7720","<p>I am new to R, ordered logistic regression, and <code>polr</code>.</p>

<p>The ""Examples"" section at the bottom of the help page for <a href=""http://stat.ethz.ch/R-manual/R-patched/library/MASS/html/polr.html"">polr</a> (that fits a logistic or probit regression model to an ordered factor response) shows</p>

<pre><code>options(contrasts = c(""contr.treatment"", ""contr.poly""))
house.plr &lt;- polr(Sat ~ Infl + Type + Cont, weights = Freq, data = housing)
pr &lt;- profile(house.plr)
plot(pr)
pairs(pr)
</code></pre>

<ul>
<li><p>What information does <code>pr</code> contain?  The help page on <a href=""http://stat.ethz.ch/R-manual/R-patched/library/stats/html/profile.html"">profile</a> is
generic, and gives no guidance for polr.</p></li>
<li><p>What is <code>plot(pr)</code> showing?  I see six graphs. Each has an X axis that is
numeric, although the label is an indicator variable (looks like an input variable that is an indicator for an ordinal value).  Then the Y axis
is ""tau"" which is completely unexplained.</p></li>
<li><p>What is <code>pairs(pr)</code> showing?  It looks like a plot for each pair of input
variables, but again I see no explanation of the X or Y axes.</p></li>
<li><p>How can one understand if the model gave a good fit?
<code>summary(house.plr)</code> shows Residual Deviance 3479.149 and AIC (Akaike
Information Criterion?) of 3495.149.  Is that good?  In the case those
are only useful as relative measures (i.e. to compare to another model
fit), what is a good absolute measure?  Is the residual deviance approximately chi-squared distributed?  Can one use ""% correctly predicted"" on the original data or some cross-validation?  What is the easiest way to do that?</p></li>
<li><p>How does one apply and interpret <code>anova</code> on this model?  The docs say ""There are methods for the standard model-fitting functions, including predict, summary, vcov, anova.""  However, running <code>anova(house.plr)</code> results in <code>anova is not implemented for a single ""polr"" object</code></p></li>
<li><p>How does one interpret the t values for each coefficient?  Unlike some
model fits, there are no P values here.</p></li>
</ul>

<p>I realize this is a lot of questions, but it makes sense to me to ask as one bundle (""how do I use this thing?"") rather than 7 different questions.  Any information appreciated.</p>
"
"0.148969892882781","0.172132593164774","  8511","<p>Christopher Manning's <a href=""http://nlp.stanford.edu/~manning/courses/ling289/logistic.pdf"" rel=""nofollow"">writeup on logistic regression in R</a> shows a logistic regression in R as follows:</p>

<pre><code>ced.logr &lt;- glm(ced.del ~ cat + follows + factor(class), 
  family=binomial)
</code></pre>

<p>Some output:</p>

<pre><code>&gt; summary(ced.logr)
Call:
glm(formula = ced.del ~ cat + follows + factor(class),
    family = binomial(""logit""))
Deviance Residuals:
Min            1Q    Median       3Q      Max
-3.24384 -1.34325   0.04954  1.01488  6.40094

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   -1.31827    0.12221 -10.787 &lt; 2e-16
catd          -0.16931    0.10032  -1.688 0.091459
catm           0.17858    0.08952   1.995 0.046053
catn           0.66672    0.09651   6.908 4.91e-12
catv          -0.76754    0.21844  -3.514 0.000442
followsP       0.95255    0.07400  12.872 &lt; 2e-16
followsV       0.53408    0.05660   9.436 &lt; 2e-16
factor(class)2 1.27045    0.10320  12.310 &lt; 2e-16
factor(class)3 1.04805    0.10355  10.122 &lt; 2e-16
factor(class)4 1.37425    0.10155  13.532 &lt; 2e-16
(Dispersion parameter for binomial family taken to be 1)
Null deviance: 958.66 on 51 degrees of freedom
Residual deviance: 198.63 on 42 degrees of freedom
AIC: 446.10
Number of Fisher Scoring iterations: 4
</code></pre>

<p>He then goes into some detail about how to interpret coefficients, compare different models, and so on.  Quite useful.</p>

<p>However, how much variance does the model account for?  A <a href=""http://www.ats.ucla.edu/stat/stata/output/old/lognoframe.htm"" rel=""nofollow"">Stata page on logistic regression</a> says:</p>

<blockquote>
  <p>Technically, R2 cannot be computed the same way in logistic regression as it is in OLS regression. The pseudo-R2, in logistic regression, is defined as 1 - L1/L0, where L0 represents the log likelihood for the ""constant-only"" model and L1 is the log likelihood for the full model with constant and predictors. </p>
</blockquote>

<p>I understand this at the high level. The constant-only model would be without any of the parameters (only the intercept term).  Log likelihood is a measure of how closely the parameters fit the data.  In fact, Manning sort of hints that the deviance might be -2 log L. Perhaps null deviance is constant-only and residual deviance is -2 log L of the model?  However, I'm not crystal clear on it.</p>

<p>Can someone verify how one actually computes the pseudo-R^2 in R using this example?</p>
"
"0.160872363021947","0.162650012158089"," 11107","<p>I need to do a logistic regression using R on my data. My response variable (<code>y</code>) is survival at weaning (<code>surv=0</code>; did not <code>surv=1</code>) and I have several independent variables which are binary and categoricals in nature.</p>

<p>I am following some examples on this website <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/dae/logit.htm</a> and trying to run some models.</p>

<p>Running the model: </p>

<pre><code>&gt; mysurv2 &lt;- glm(surv~as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
                 as.factor(pmtone), family=binomial(link=""logit""), data=ap)
&gt; summary(mysurv2)

Call:
glm(formula = surv ~ as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
    as.factor(pmtone), family = binomial(link = ""logit""), data = ap)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.2837  -0.5121  -0.5121  -0.5058   2.0590  

Coefficients:
                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -0.01135    0.23613  -0.048  0.96166    
as.factor(PTEM)2   -0.74642    0.24482  -3.049  0.00230 ** 
as.factor(PTEM)3   -1.95401    0.23259  -8.401  &lt; 2e-16 ***
as.factor(pshiv)2  -0.02638    0.06784  -0.389  0.69738    
as.factor(presp)2   0.74549    0.10532   7.079 1.46e-12 ***
as.factor(presp)3   0.66793    0.66540   1.004  0.31547    
as.factor(pmtone)2  0.54699    0.09678   5.652 1.58e-08 ***
as.factor(pmtone)3  1.82337    0.75409   2.418  0.01561 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 7892.6  on 8791  degrees of freedom
Residual deviance: 7252.8  on 8784  degrees of freedom
  (341 observations deleted due to missingness)
AIC: 7268.8

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Adding the <code>na.action=na.pass</code> at the end of the model gave me an error message. I thought that this would take care NA's in my independent variables.</p>

<pre><code>&gt; mysurv &lt;- glm(surv~as.factor(PTEM) + as.factor(pshiv) + as.factor(presp) + 
                as.factor(pmtone), family=binomial(link=""logit""), data=ap, 
                na.action=na.pass)
Error: NA/NaN/Inf in foreign function call (arg 1)
</code></pre>

<p>Since this is my first time to venture into logistic regression, I am wondering whether there is any package in R that would be more suitable?</p>

<p>I am also tryng to understand the regression coefficients. The independent variables used in the model are:</p>

<ol>
<li><p>rectal temperature: </p>

<ul>
<li><code>(PTEM)1</code> = newborns with rectal temp. below 35.4 0C</li>
<li><code>(PTEM)2</code> = newborns with rectal temp. between 35.4 to 36.9 0C</li>
<li><code>(PTEM)3</code> = newborns with rectal temp. above 37.0 0C</li>
</ul></li>
<li><p>shivering:</p>

<ul>
<li><code>(pshiv)1</code> = newborns that were not shivering</li>
<li><code>(pshiv)2</code> = newborns that were shivering</li>
</ul></li>
<li><p>respiration:</p>

<ul>
<li><code>(presp)1</code> = newborns with normal respiration</li>
<li><code>(presp)2</code> = newborns with slight respiration problem</li>
<li><code>(presp)3</code> = newborns with poor respiration</li>
</ul></li>
<li><p>muscle tone:</p>

<ul>
<li><code>(pmtone)1</code> = newborns with normal muscle tone</li>
<li><code>(pmtone)2</code> = newborns with moderate muscle tone</li>
<li><code>(pmtone)1</code> = newborns with poor muscle tone</li>
</ul></li>
</ol>

<p>Looking at the coefficients, I got the following:</p>

<pre><code>                   Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -0.01135    0.23613  -0.048  0.96166    
as.factor(PTEM)2   -0.74642    0.24482  -3.049  0.00230 ** 
as.factor(PTEM)3   -1.95401    0.23259  -8.401  &lt; 2e-16 ***
as.factor(pshiv)2  -0.02638    0.06784  -0.389  0.69738    
as.factor(presp)2   0.74549    0.10532   7.079 1.46e-12 ***
as.factor(presp)3   0.66793    0.66540   1.004  0.31547    
as.factor(pmtone)2  0.54699    0.09678   5.652 1.58e-08 ***
as.factor(pmtone)3  1.82337    0.75409   2.418  0.01561 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>In my other analysis, I found that newborns:  </p>

<p>a) with higher rectal temperature<br>
b) do not shiver<br>
c) good respiration and<br>
d) good muscle tone at birth were more likely to survive.  </p>

<p>I am a bit confused with the coefficients I am getting above. I am wondering whether whether I am not interpreting the results correctly or is it something else?</p>
"
"0.0807572853087248","0.0816496580927726"," 11178","<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://stats.stackexchange.com/questions/8661/logistic-regression-in-r-odds-ratio"">Logistic Regression in R (Odds Ratio)</a>  </p>
</blockquote>



<p>I need to do a logistic regression in R. My response variable is <code>surv=0</code>; <code>surv=1</code> and I have about 18 predictor variables.</p>

<p>After reading my model, I got the table of Coefficients below and I need to go through some steps, which I am not familiar with, until I get to the odds ratios.</p>

<p>This is my first time to do a logistic regression in R and your help would be appreciated.</p>

<pre><code>Call:
glm(formula = surv ~ as.factor(tdate) + as.factor(line) + as.factor(wt) + 
    as.factor(crump) + as.factor(pind) + as.factor(pcscore) + 
    as.factor(ptem) + as.factor(pshiv) + as.factor(pincis) + 
    as.factor(presp) + as.factor(pmtone) + as.factor(pscolor) + 
    as.factor(ppscore) + as.factor(pmstain) + as.factor(pbse) + 
    as.factor(psex) + as.factor(pgf), family = binomial(link = ""logit""), 
    data = ap)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.9772  -0.5896  -0.4419  -0.3154   2.8264  

Coefficients:
                           Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                -0.59796    0.27024  -2.213 0.026918 *  
as.factor(tdate)2009-09-08  0.43918    0.19876   2.210 0.027130 *  
as.factor(tdate)2009-09-11  0.27613    0.20289   1.361 0.173514    
as.factor(tdate)2009-09-15  0.58733    0.19232   3.054 0.002259 ** 
as.factor(tdate)2009-09-18  0.52823    0.20605   2.564 0.010360 *  
as.factor(tdate)2009-09-22  0.45661    0.19929   2.291 0.021954 *  
as.factor(tdate)2009-09-25 -0.09189    0.21740  -0.423 0.672526    
as.factor(tdate)2009-09-29 -0.15696    0.28369  -0.553 0.580076    
as.factor(tdate)2010-01-26  1.39260    0.21049   6.616 3.69e-11 ***
as.factor(tdate)2010-01-29  1.67827    0.21099   7.954 1.80e-15 ***
as.factor(tdate)2010-02-02  1.35442    0.21292   6.361 2.00e-10 ***
as.factor(tdate)2010-02-05  1.36856    0.21439   6.383 1.73e-10 ***
as.factor(tdate)2010-02-09  1.18159    0.21951   5.383 7.33e-08 ***
as.factor(tdate)2010-02-12  1.40457    0.22001   6.384 1.73e-10 ***
as.factor(tdate)2010-02-16  1.01063    0.21783   4.639 3.49e-06 ***
as.factor(tdate)2010-02-19  1.54992    0.21535   7.197 6.14e-13 ***
as.factor(tdate)2010-02-23  0.85695    0.33968   2.523 0.011641 *  
as.factor(line)2           -0.26311    0.07257  -3.625 0.000288 ***
as.factor(line)5            0.06766    0.11162   0.606 0.544387    
as.factor(line)6           -0.30409    0.12130  -2.507 0.012176 *  
as.factor(wt)2             -0.33904    0.10708  -3.166 0.001544 ** 
as.factor(wt)3             -0.28976    0.13217  -2.192 0.028359 *  
as.factor(wt)4             -0.50470    0.16264  -3.103 0.001915 ** 
as.factor(wt)5             -0.74870    0.20067  -3.731 0.000191 ***
as.factor(crump)2           0.07537    0.10751   0.701 0.483280    
as.factor(crump)3          -0.14050    0.13217  -1.063 0.287768    
as.factor(crump)4          -0.20131    0.16689  -1.206 0.227724    
as.factor(crump)5          -0.23963    0.20778  -1.153 0.248803    
as.factor(pind)2           -0.29893    0.10752  -2.780 0.005434 ** 
as.factor(pind)3           -0.40828    0.12436  -3.283 0.001027 ** 
as.factor(pind)4           -0.73021    0.14947  -4.885 1.03e-06 ***
as.factor(pind)5           -0.68878    0.17650  -3.902 9.52e-05 ***
as.factor(pcscore)2        -0.52667    0.13606  -3.871 0.000108 ***
as.factor(ptem)2           -0.72600    0.08964  -8.099 5.52e-16 ***
as.factor(ptem)3           -0.79145    0.10503  -7.536 4.86e-14 ***
as.factor(ptem)4           -0.89956    0.10331  -8.707  &lt; 2e-16 ***
as.factor(ptem)5           -0.90181    0.10721  -8.412  &lt; 2e-16 ***
as.factor(pshiv)2           0.25236    0.07713   3.272 0.001068 ** 
as.factor(pincis)2          0.02327    0.07216   0.323 0.747041    
as.factor(presp)2           0.43746    0.11598   3.772 0.000162 ***
as.factor(pmtone)2          0.34515    0.11178   3.088 0.002016 ** 
as.factor(pscolor)2         0.53469    0.26851   1.991 0.046443 *  
as.factor(ppscore)2         0.25664    0.08751   2.933 0.003361 ** 
as.factor(pmstain)2        -0.48619    0.84408  -0.576 0.564611    
as.factor(pbse)2           -0.28248    0.07335  -3.851 0.000117 ***
as.factor(psex)2           -0.18240    0.06385  -2.857 0.004280 ** 
as.factor(pgf)12            0.10329    0.14314   0.722 0.470554    
as.factor(pgf)21           -0.06481    0.10772  -0.602 0.547388    
as.factor(pgf)22            0.39584    0.12740   3.107 0.001890 ** 
as.factor(pgf)31            0.18820    0.10082   1.867 0.061936 .  
as.factor(pgf)32            0.39662    0.13963   2.841 0.004504 ** 
as.factor(pgf)41            0.09178    0.10413   0.881 0.378106    
as.factor(pgf)42            0.21056    0.14906   1.413 0.157787    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 7812.9  on 8714  degrees of freedom
Residual deviance: 6797.4  on 8662  degrees of freedom
  (418 observations deleted due to missingness)
AIC: 6903.4

Number of Fisher Scoring iterations: 5
</code></pre>
"
"0.134595475514541","0.136082763487954"," 18248","<p>This question arises from my actual confusion about how to decide if a logistic model is good enough. I have models that use the state of pairs individual-project two years after they are formed as a dependent variable. The outcome is successful (1) or not (0). I have independent variables measured at the time of formation of the pairs.  My aim is to test whether a variable, which I hypothesized would influence the success of the pairs has an effect on that success, controlling for other potential influences. In the models, the variable of interest is significant.</p>

<p>The models were estimated using the <code>glm()</code> function in <code>R</code>. To assess the quality of the models, I have done a few things: <code>glm()</code> gives you the <code>residual deviance</code>, the <code>AIC</code> and the <code>BIC</code> by default. In addition, I have calculated the error rate of the model and plotted the binned residuals.  </p>

<ul>
<li>The complete model has a smaller residual deviance, AIC and BIC than the other models that I have estimated (and that are nested in the complete model), which leads me to think that this model is ""better"" than the others.  </li>
<li>The error-rate of the model is fairly low, IMHO (as in <a href=""http://www.stat.columbia.edu/~gelman/arm/"">Gelman and Hill, 2007, pp.99</a>):<br>
<code>error.rate &lt;- mean((predicted&gt;0.5 &amp; y==0) | (predicted&lt;0.5 &amp; y==1)</code>, at around 20%.  </li>
</ul>

<p>So far so good. But when I plot the binned residual (again following Gelman and Hill's advice), a large portion of the bins fall outside of the 95% CI:
<img src=""https://lh5.googleusercontent.com/-DhQ3a9hTVoE/Tr1H-Csj_JI/AAAAAAAAAC0/eYXlUlkc6ic/s550/binned.res.jpeg"" alt=""Binned Residuals plot""></p>

<p>That plot leads me to think there is something utterly wrong about the model. Should that lead me to throw the model away? Should I acknowledge that the model is imperfect but keep it and interpret the effect of the variable of interest? I have toyed around with excluding variables in turn, and also some transformation, without really improving the binned residuals plot.</p>

<p><strong>Edit:</strong>  </p>

<ul>
<li>At the moment, the model has a dozen predictors and 5 interaction effects.  </li>
<li>The pairs are ""relatively"" independent of each other in the sense that they are all formed during a short period of time (but not stricly speaking, all simultaneously) and there are a lot of projects (13k) and a lot of individuals (19k), so a fair proportion of projects are only joined by one individual (there are about 20000 pairs).</li>
</ul>
"
"0.120385853085769","0.121716123890037"," 21067","<p>It's been a while since I've thought about or used a robust logistic regression model. However, I ran a few logits yesterday and realized that my probability curve was being affected by some 'extreme' values, and particularly low ones. However, when I went to run a robust logit model, I got the same results as I did in my logit model.</p>

<p>Under what circumstances should a robust logit produce different results from a traditional logit model? (in terms of coefficients)</p>

<p>R Code:</p>

<pre><code>&gt; library(Design)
&gt; ddist&lt;- datadist(dlmydat)
&gt; options(datadist='ddist')
&gt; me = lrm(factor(dlstatus) ~ dlour_bid, data=dlmydat)
&gt; me

Logistic Regression Model

lrm(formula = factor(dlstatus) ~ dlour_bid, data = dlmydat)


Frequencies of Responses
  1   2 
906 154 

       Obs  Max Deriv Model L.R.       d.f.          P          C        Dxy      Gamma      Tau-a         R2      Brier 
      1060      3e-05     170.11          1          0       0.81      0.619      0.621      0.154      0.263      0.105 

          Coef      S.E.      Wald Z P
Intercept -5.233549 0.3731235 -14.03 0
dlour_bid  0.005367 0.0004925  10.90 0

&gt; library(car)
&gt; dlmod = glm(factor(dlstatus) ~ dlour_bid, data=dlmydat, family=binomial(link=""logit""))
&gt; summary(dlmod)

Call:
glm(formula = factor(dlstatus) ~ dlour_bid, family = binomial(link = ""logit""), 
    data = dlmydat)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.2345  -0.5687  -0.3059  -0.1739   2.6999  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -5.2335492  0.3731235  -14.03   &lt;2e-16 ***
dlour_bid    0.0053667  0.0004925   10.90   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 878.61  on 1059  degrees of freedom
Residual deviance: 708.50  on 1058  degrees of freedom
AIC: 712.5

Number of Fisher Scoring iterations: 6
</code></pre>
"
"0.107676380411633","0.108866210790363"," 25839","<p>First off, I'll say I am a biologist and new to the statistics side of things so excuse my ignorance</p>

<p>I have a data set that consists of a binary outcome and then a bunch of trinary explanatory variables that looks something like this:</p>

<pre><code>head()
 Category block21_hap1 block21_hap2 block21_hap3 block21_check
1        1            1            1            0             2
2        1            2            0            0             2
3        1            1            0            1             2
4        1            1            0            1             2
5        1            1            1            0             2
6        1            1            1            0             2
</code></pre>

<p>A quick summary of the data</p>

<pre><code>summary()
Category block21_hap1 block21_hap2 block21_hap3 block21_check
 1:718    0:293        0:777        0:1026       2:1467       
 0:749    1:709        1:577        1: 390                    
          2:465        2:113        2:  51  
</code></pre>

<p>and another summary grouped by outcome levels</p>

<pre><code>by(hap.ped.final, hap.ped.final$Category, summary)
hap.ped.final$Category: 1
 block21_hap1 block21_hap2 block21_hap3 block21_check
 0:146        0:374        0:518        2:718        
 1:336        1:286        1:174                     
 2:236        2: 58        2: 26                     
---------------------------------------------------------------------------- 
hap.ped.final$Category: 0
 block21_hap1 block21_hap2 block21_hap3 block21_check
 0:147        0:403        0:508        2:749        
 1:373        1:291        1:216                     
 2:229        2: 55        2: 25          
</code></pre>

<p>So I am trying to run logistic regression on this data. When I do this:</p>

<pre><code>fit = glm(Category~ block21_hap1 + block21_hap2 + block21_hap3, data = hap.ped.final ,family = ""binomial"")
summary(fit)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.301  -1.177   1.059   1.177   1.200  

Coefficients: (1 not defined because of singularities)
                             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)                 -0.039221   0.280110  -0.140    0.889
hap.ped.final$block21_hap11  0.123555   0.183087   0.675    0.500
hap.ped.final$block21_hap12  0.009111   0.295069   0.031    0.975
hap.ped.final$block21_hap21 -0.084334   0.183087  -0.461    0.645
hap.ped.final$block21_hap22 -0.013889   0.337468  -0.041    0.967
hap.ped.final$block21_hap31  0.201113   0.183087   1.098    0.272
hap.ped.final$block21_hap32        NA         NA      NA       NA

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2033  on 1466  degrees of freedom
Residual deviance: 2028  on 1461  degrees of freedom
AIC: 2040

Number of Fisher Scoring iterations: 3
</code></pre>

<p>So I don't really know what a singularity is or what's going wrong here that is throwing up NA's as a result of my analysis. Is it my data, or what I'm doing to it.
I tried googling the warning (or whatever you might call it) and I got some pages talking about collinearity and multilinearity, which I do not understand at all. 
Again, sorry for lack of knowledge here. I wish I had done more maths in undergrad. </p>
"
"0.120385853085769","0.121716123890037"," 26288","<p>I'm trying to understand how to interpret log odds ratios in logistic regression. Let's say I have the following output:</p>

<pre><code>&gt; mod1 = glm(factor(won) ~ bid, data=mydat, family=binomial(link=""logit""))
&gt; summary(mod1)

Call:
glm(formula = factor(won) ~ bid, family = binomial(link = ""logit""), 
    data = mydat)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.5464  -0.6990  -0.6392  -0.5321   2.0124  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -2.133e+00  1.947e-02 -109.53   &lt;2e-16 ***
bid          2.494e-03  5.058e-05   49.32   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 83081  on 80337  degrees of freedom
Residual deviance: 80645  on 80336  degrees of freedom
AIC: 80649

Number of Fisher Scoring iterations: 4
</code></pre>

<p>So my equation would look like:
$$\Pr(Y=1) = \frac{1}{1 + \exp\left(-[-2.13 + 0.002\times(\text{bid})]\right)}$$</p>

<p>From here I calculated probabilities from all bid levels. 
<img src=""http://i.stack.imgur.com/5mLa9.png"" alt=""enter image description here""></p>

<p>I have been using this graph to say that at a 1000 bid, the probability of winning is x. At any given bid level, the probability of winning is x.</p>

<p>I have a feeling that my interpretation is wrong because I'm not considering that these are log-odds. How should I really be interpreting this plot/these results?</p>
"
"0.16151457061745","0.163299316185545"," 26831","<p>Still on running logistic regression models and would like to ask a few questions around it.</p>

<p><strong>Question 1</strong>:
Is there a simple way of getting the p-values of each independent factor in a logistic regression model. For example, I am running this model:</p>

<pre><code>mymod3 &lt;- as.formula(surv~as.factor(tdate)+as.factor(sline)+as.factor(pgrp)
                                          +as.factor(weight5)+as.factor(backfat5)
                                          +as.factor(srect2)+as.factor(bcs)
                                          +as.factor(agit)+as.factor(uscore)
                                          +as.factor(loco)+as.factor(teat2)
                                          +as.factor(uscoref)+as.factor(colos)
                                          +as.factor(tb5)+as.factor(nerve)
                                          +as.factor(feed5)+as.factor(fos)
                                          +as.factor(gest3)+as.factor(int3)
                                          +as.factor(psex)+as.factor(bwt5)
                                          +as.factor(presp2)+as.factor(mtone2)
                                          +as.factor(pscolor)+as.factor(pmstain)
                                          +as.factor(pshiv)+as.factor(ppscore)
                                          +as.factor(pincis)+as.factor(prectem5)
                                          +as.factor(pcon12)+as.factor(crum5)
                                          +as.factor(pindx5))

sofNoMis3 &lt;- apf[which(complete.cases(apf[,all.vars(mymod3)])),]
FulMod3 &lt;- glm(mymod3,family=binomial(link=""logit""),data=sofNoMis3)
summary(FulMod3)
</code></pre>

<p>I am using this to look at the significant level of each factor:</p>

<pre><code>anova(FulMod3,test=""Chisq"")
</code></pre>

<p>and got this:</p>

<pre><code>Analysis of Deviance Table

Model: binomial, link: logit

Response: surv

Terms added sequentially (first to last)


                    Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                                 7791     7096.2              
as.factor(tdate)    15    50.71      7776     7045.4 9.215e-06 ***
as.factor(sline)     1    13.90      7775     7031.5 0.0001924 ***
as.factor(pgrp)      3     8.83      7772     7022.7 0.0316335 *  
as.factor(weight5)   4     7.18      7768     7015.5 0.1268943    
as.factor(backfat5)  4     3.86      7764     7011.7 0.4258074    
as.factor(srect2)    1     0.15      7763     7011.5 0.6987832    
as.factor(bcs)       3     6.46      7760     7005.1 0.0910745 .  
as.factor(agit)      2    13.44      7758     6991.6 0.0012075 ** 
as.factor(uscore)    2     2.16      7756     6989.5 0.3401845    
as.factor(loco)      2     1.58      7754     6987.9 0.4530983    
as.factor(teat2)     2    25.45      7752     6962.4 2.980e-06 ***
as.factor(uscoref)   2     0.48      7750     6962.0 0.7861675    
as.factor(colos)     1     1.06      7749     6960.9 0.3034592    
as.factor(tb5)       4    49.22      7745     6911.7 5.265e-10 ***
as.factor(nerve)     2     0.99      7743     6910.7 0.6105452    
as.factor(feed5)     4    11.79      7739     6898.9 0.0190170 *  
as.factor(fos)       1    47.10      7738     6851.8 6.732e-12 ***
as.factor(gest3)     2    22.60      7736     6829.2 1.235e-05 ***
as.factor(int3)      2     6.61      7734     6822.6 0.0367298 *  
as.factor(psex)      1     9.50      7733     6813.1 0.0020493 ** 
as.factor(bwt5)      4   348.42      7729     6464.7 &lt; 2.2e-16 ***
as.factor(presp2)    1   106.23      7728     6358.4 &lt; 2.2e-16 ***
as.factor(mtone2)    1    34.13      7727     6324.3 5.146e-09 ***
as.factor(pscolor)   1    12.57      7726     6311.7 0.0003928 ***
as.factor(pmstain)   1     0.30      7725     6311.4 0.5845095    
as.factor(pshiv)     1    32.29      7724     6279.2 1.328e-08 ***
as.factor(ppscore)   1    16.71      7723     6262.4 4.351e-05 ***
as.factor(pincis)    1     0.02      7722     6262.4 0.8892848    
as.factor(prectem5)  4   126.06      7718     6136.4 &lt; 2.2e-16 ***
as.factor(pcon12)    1    17.88      7717     6118.5 2.350e-05 ***
as.factor(crum5)     4    15.25      7713     6103.2 0.0042137 ** 
as.factor(pindx5)    4    25.58      7709     6077.6 3.838e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 
</code></pre>

<p>but it does not always agree with the final model after applying backward elimination:</p>

<p>Example: </p>

<p>these three factors were not significant above but they still appeared in the final model below</p>

<pre><code>as.factor(weight5)   4     7.18      7768     7015.5 0.1268943    
as.factor(backfat5)  4     3.86      7764     7011.7 0.4258074    
as.factor(srect2)    1     0.15      7763     7011.5 0.6987832    

step(FulMod3,direction=""backward"",trace=FALSE)
</code></pre>

<p>which gives:</p>

<pre><code>Call:  glm(formula = surv ~ as.factor(tdate) + as.factor(pgrp) + as.factor(weight5) + 
    as.factor(backfat5) + as.factor(srect2) + as.factor(agit) + 
    as.factor(uscore) + as.factor(teat2) + as.factor(uscoref) + 
    as.factor(fos) + as.factor(gest3) + as.factor(int3) + as.factor(psex) + 
    as.factor(bwt5) + as.factor(presp2) + as.factor(mtone2) + 
    as.factor(pscolor) + as.factor(pshiv) + as.factor(ppscore) + 
    as.factor(prectem5) + as.factor(pcon12) + as.factor(pindx5), 
    family = binomial(link = ""logit""), data = sofNoMis3)

Coefficients:
               (Intercept)  as.factor(tdate)2009-09-11  as.factor(tdate)2009-09-15  as.factor(tdate)2009-09-18  as.factor(tdate)2009-09-22  
                   1.34799                     0.18414                    -0.19490                    -0.15552                    -0.16822  
as.factor(tdate)2009-09-25  as.factor(tdate)2009-09-29  as.factor(tdate)2010-01-26  as.factor(tdate)2010-01-29  as.factor(tdate)2010-02-02  
                   0.60046                     0.80784                    -1.03442                    -1.30562                    -1.01486  
as.factor(tdate)2010-02-05  as.factor(tdate)2010-02-09  as.factor(tdate)2010-02-12  as.factor(tdate)2010-02-16  as.factor(tdate)2010-02-19  
                  -1.04438                    -0.89311                    -1.06260                    -0.79833                    -1.09651  
as.factor(tdate)2010-02-23            as.factor(pgrp)2            as.factor(pgrp)3            as.factor(pgrp)4         as.factor(weight5)2  
                  -0.55411                     0.12659                    -0.04727                     0.21817                    -0.22592  
       as.factor(weight5)3         as.factor(weight5)4         as.factor(weight5)5        as.factor(backfat5)2        as.factor(backfat5)3  
                  -0.10143                    -0.31562                    -0.37656                    -0.19883                    -0.01188  
      as.factor(backfat5)4        as.factor(backfat5)5          as.factor(srect2)2            as.factor(agit)2            as.factor(agit)3  
                   0.08293                    -0.17116                    -0.18201                    -0.49145                    -0.36659  
        as.factor(uscore)2          as.factor(uscore)3           as.factor(teat2)2           as.factor(teat2)3         as.factor(uscoref)2  
                  -0.12265                     0.15334                     0.16575                     0.21520                     0.24166  
       as.factor(uscoref)3             as.factor(fos)2           as.factor(gest3)2           as.factor(gest3)3            as.factor(int3)2  
                  -0.24363                    -0.29506                     0.09747                     0.81894                    -0.25595  
          as.factor(int3)3            as.factor(psex)2            as.factor(bwt5)2            as.factor(bwt5)3            as.factor(bwt5)4  
                  -1.21086                     0.20025                     0.30753                     0.29614                     0.56753  
          as.factor(bwt5)5          as.factor(presp2)2          as.factor(mtone2)2         as.factor(pscolor)2           as.factor(pshiv)2  
                   0.86479                    -0.29270                    -0.40912                    -0.72782                    -0.33848  
       as.factor(ppscore)2        as.factor(prectem5)2        as.factor(prectem5)3        as.factor(prectem5)4        as.factor(prectem5)5  
                  -0.25958                     0.73842                     0.77476                     0.92158                     0.96269  
        as.factor(pcon12)2          as.factor(pindx5)2          as.factor(pindx5)3          as.factor(pindx5)4          as.factor(pindx5)5  
                   0.38119                     0.43199                     0.44496                     0.73458                     0.59771  

Degrees of Freedom: 7791 Total (i.e. Null);  7732 Residual
Null Deviance:      7096 
Residual Deviance: 6102         AIC: 6222
</code></pre>

<p><strong>Question 2</strong>:</p>

<p>I would like to calculate the standard errors of the odds ratio of each factor level </p>

<pre><code>exp(NewMod3$coefficients)  #Odds ratios
</code></pre>

<p><strong>Question 3:</strong></p>

<p>Lastly, to tell whether the levels of each factor are significantly different or not </p>

<pre><code>               (Intercept) as.factor(tdate)2009-09-11 as.factor(tdate)2009-09-15 as.factor(tdate)2009-09-18 as.factor(tdate)2009-09-22 
                 3.8496863                  1.2021883                  0.8229141                  0.8559688                  0.8451676 
as.factor(tdate)2009-09-25 as.factor(tdate)2009-09-29 as.factor(tdate)2010-01-26 as.factor(tdate)2010-01-29 as.factor(tdate)2010-02-02 
                 1.8229563                  2.2430525                  0.3554327                  0.2710041                  0.3624544 
as.factor(tdate)2010-02-05 as.factor(tdate)2010-02-09 as.factor(tdate)2010-02-12 as.factor(tdate)2010-02-16 as.factor(tdate)2010-02-19 
                 0.3519109                  0.4093819                  0.3455567                  0.4500787                  0.3340336 
as.factor(tdate)2010-02-23           as.factor(pgrp)2           as.factor(pgrp)3           as.factor(pgrp)4        as.factor(weight5)2 
                 0.5745817                  1.1349500                  0.9538339                  1.2437928                  0.7977835 
       as.factor(weight5)3        as.factor(weight5)4        as.factor(weight5)5       as.factor(backfat5)2       as.factor(backfat5)3 
                 0.9035410                  0.7293337                  0.6862173                  0.8196866                  0.9881871 
      as.factor(backfat5)4       as.factor(backfat5)5         as.factor(srect2)2           as.factor(agit)2           as.factor(agit)3 
                 1.0864697                  0.8426844                  0.8335940                  0.6117399                  0.6930936 
        as.factor(uscore)2         as.factor(uscore)3          as.factor(teat2)2          as.factor(teat2)3        as.factor(uscoref)2 
                 0.8845715                  1.1657233                  1.1802836                  1.2401126                  1.2733576 
       as.factor(uscoref)3            as.factor(fos)2          as.factor(gest3)2          as.factor(gest3)3           as.factor(int3)2 
                 0.7837753                  0.7444886                  1.1023798                  2.2681046                  0.7741829 
          as.factor(int3)3           as.factor(psex)2           as.factor(bwt5)2           as.factor(bwt5)3           as.factor(bwt5)4 
                 0.2979401                  1.2217088                  1.3600609                  1.3446543                  1.7639063 
          as.factor(bwt5)5         as.factor(presp2)2         as.factor(mtone2)2        as.factor(pscolor)2          as.factor(pshiv)2 
                 2.3745019                  0.7462454                  0.6642372                  0.4829602                  0.7128545 
       as.factor(ppscore)2       as.factor(prectem5)2       as.factor(prectem5)3       as.factor(prectem5)4       as.factor(prectem5)5 
                 0.7713779                  2.0926314                  2.1700692                  2.5132469                  2.6187261 
        as.factor(pcon12)2         as.factor(pindx5)2         as.factor(pindx5)3         as.factor(pindx5)4         as.factor(pindx5)5 
                 1.4640265                  1.5403203                  1.5604231                  2.0845978                  1.8179532 
</code></pre>

<p>Example:</p>

<p>I would like to have a table like this:</p>

<pre><code>Factor           levels  Odds ratio

Parity group      (1)    1.00Â±standard error   a
                   2     1.50Â±standard errror  b
                  3-4    1.17Â±standard error   c
                   &gt;5    1.19Â±standard error   c
</code></pre>

<p>I would really appreciate your help on these 3 areas.</p>

<p>Baz</p>
"
"0.17025130615175","0.172132593164774"," 34319","<p>Let's say I have the following logistic regression models:</p>

<pre><code> df=data.frame(income=c(5,5,3,3,6,5),
                  won=c(0,0,1,1,1,0),
                  age=c(18,18,23,50,19,39),
                  home=c(0,0,1,0,0,1))

&gt; md1 = glm(factor(won) ~ income + age + home, 
+           data=df, family=binomial(link=""logit""))
&gt; md2 = glm(factor(won) ~ factor(income) + factor(age) + factor(home), 
+           data=df, family=binomial(link=""logit""))
&gt; summary(md1)

Call:
glm(formula = factor(won) ~ income + age + home, family = binomial(link = ""logit""), 
    data = df)

Deviance Residuals: 
      1        2        3        4        5        6  
-1.0845  -1.0845   0.8017   0.4901   1.7298  -0.8017  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  4.784832   6.326264   0.756    0.449
income      -1.027049   1.056031  -0.973    0.331
age          0.007102   0.097759   0.073    0.942
home        -0.896802   2.252894  -0.398    0.691

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 8.3178  on 5  degrees of freedom
Residual deviance: 6.8700  on 2  degrees of freedom
AIC: 14.87

Number of Fisher Scoring iterations: 4

&gt; summary(md2)

Call:
glm(formula = factor(won) ~ factor(income) + factor(age) + factor(home), 
    family = binomial(link = ""logit""), data = df)

Deviance Residuals: 
         1           2           3           4           5           6  
-6.547e-06  -6.547e-06   6.547e-06   6.547e-06   6.547e-06  -6.547e-06  

Coefficients: (3 not defined because of singularities)
                  Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)      2.457e+01  1.310e+05       0        1
factor(income)5 -4.913e+01  1.605e+05       0        1
factor(income)6 -2.573e-30  1.853e+05       0        1
factor(age)19           NA         NA      NA       NA
factor(age)23   -1.383e-30  1.853e+05       0        1
factor(age)39   -3.479e-14  1.605e+05       0        1
factor(age)50           NA         NA      NA       NA
factor(home)1           NA         NA      NA       NA

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 8.3178e+00  on 5  degrees of freedom
Residual deviance: 2.5720e-10  on 1  degrees of freedom
AIC: 10
</code></pre>

<p>So depending on the mode of the predictors, R produced different outputs. For factors, R splits out the coefficients into separate categories for the levels, but not for the model with numeric predictors. I'm wondering about a couple things.</p>

<ol>
<li><p>Is it ever useful to have the response categories expressed as individual rows?</p></li>
<li><p>To express the general regression equation, how does one go from a model with the categories expressed in an individual equation to an equation with a single B_i. So, for example, if gender has two coefficients, 3.5 for Male and 2.3 for Female, how does one use that in an equation such that (besides converting them into numeric values):</p></li>
</ol>

<p>Y = B0 + B1 (Gender)</p>
"
"0.208514414057075","0.210818510677892"," 40499","<p>When using the <code>step.plr()</code> function in the <a href=""http://cran.r-project.org/web/packages/stepPlr/index.html"" rel=""nofollow"">stepPlr</a> package, if my predictors are factors, do I need to encode my predictors as dummy variables manually before passing it to the function? I do know that I can specify ""level"", but how  the ""level"" parameter works is confusing to me. 
My understanding is that I need to tell <code>step.plr()</code> explicitly which factors should be encoded as dummy variables and thus leaving one factor out intentionally. </p>

<p>Let's consider a simple example. Suppose I have 1 categorical predicator with 4 levels and binary response. Normally, if I use <code>glm()</code> to fit a logistic regression model, <code>glm()</code> would automatically convert the categorical predicator into 3 dummy variables. Now in <code>stepPlr()</code>, do I specify the ""level"" parameter for that predictor with 4 levels or 3 levels? The ""Help"" section is vague, and says: </p>

<blockquote>
  <p>If the j-th column of x is discrete, level[[ j ]] is the set of levels for the categorical factor.</p>
</blockquote>

<p>Does it mean I should tell <code>step.plr()</code> about all 4 levels, or I should make an intelligent decision myself and tell <code>step.plr()</code> to use only 3 levels? </p>

<p>==============UPDATE (16 Oct 2012)=============</p>

<p>The following example will demonstrate what is the problem with <code>step.plr()</code>'s automatic dummy variable encoding. It is a slight modification of the code in the function's help section. 
     set.seed(100)</p>

<pre><code>n &lt;- 100
p &lt;- 3
z &lt;- matrix(sample(seq(3),n*p,replace=TRUE),nrow=n)
x &lt;- data.frame(x1=factor(z[ ,1]),x2=factor(z[ ,2]),
                x3=factor(sample(seq(3), n, replace=TRUE, prob=c(0.2, 0.5, 0.3))),
                x4=factor(sample(seq(3), n, replace=TRUE, prob=c(0.1, 0.3, 0.6))))
y &lt;- sample(c(0,1),n,replace=TRUE)
fit &lt;- step.plr(x,y, cp=""aic"")
summary(fit)
</code></pre>

<p>And here's an excerpt of the result:</p>

<pre><code>Call:
plr(x = ix0, y = y, weights = weights, offset.subset = offset.subset, 
    offset.coefficients = offset.coefficients, lambda = lambda, 
    cp = cp)

Coefficients:
      Estimate Std.Error z value Pr(&gt;|z|)
Intercept  0.91386   5.04780   0.181    0.856
x4.1       1.33787   4.61089   0.290    0.772
x4.2      -1.70462   4.91240  -0.347    0.729
x4.3       0.36675   3.18857   0.115    0.908
x3.1:x4.1  7.04901  14.35112   0.491    0.623
x3.1:x4.2 -5.50973  15.53674  -0.355    0.723
x3.1:x4.3 -0.50012   7.95651  -0.063    0.950
</code></pre>

<p>You can see that all levels, that is, (1,2,3), are used to fit the model. But normally you only need two dummy variables to encode a predictor with 3 levels.
On the other hand, if you use <code>glm()</code>: </p>

<pre><code>glm(y~.^2, data=x, family=binomial)
</code></pre>

<p>you will get the correct dummy variable encoding.</p>
"
"0.147441956154897","0.149071198499986"," 41561","<p>I am fitting a logistic model to data using the glm function in R.   I have attempted to specify interaction terms in two ways:</p>

<pre><code>fit1 &lt;- glm(y ~ x*z, family = ""binomial"", data = myData) 
fit2 &lt;- glm(y ~ x/z, family = ""binomial"", data = myData) 
</code></pre>

<p>I have 3 questions:</p>

<p>1) What is the difference between specifying my interaction terms as x*z compared to x/d?</p>

<p>When I call summary(fit1) the report includes results for the intercept, x, z, and x:z while summary(fit2) only includes results for intercept, x, and x:z.</p>

<p>I did look at Section 11.1 in ""An Introduction to R"" but couldn't understand the meaning. </p>

<p>2) How do I interpret the fit equation mathematically?  Specifically, how do I interpret the interaction terms formulaically?</p>

<p>Moving to math instead of R, do I interpret the equation as:
logit(y) = (intercept) + (coeff_x)*x + (coeff_z)*x + (coeff_xz)*x*z
?</p>

<p>This interpration may differ in the two specifications fit1 and fit2.  What is the interpretation of each?</p>

<p>3) Assuming the above interpretation is correct, how to I fit the model of x*(1/z) in R?  Do I need to just create another column with these values?</p>
"
"0.120385853085769","0.121716123890037"," 43315","<p>The model that I created in R is:</p>

<blockquote>
  <p>fit &lt;- lm(hired ~ educ + exper + sex, data=data)</p>
</blockquote>

<p>what I am unsure of is how to fit to model to predict probability of interest where p = pr(hiring = 1).</p>

<p>Any help would be appreciated thanks,
Clay </p>

<p><strong>Edit:</strong>
This is the computer output for what I have computed so far. I am unsure if this is even a step in the right direction to find the answer to this question.</p>

<p>What I am trying to do is, Fit a logistic regression model to predict the probability of being hired using years of education, years of experience and sex of job applicants.</p>

<pre><code> &gt; test&lt;-glm(hired ~ educ + exper + sex, data=data, family=binomial(link=""logit""))
 &gt; summary(test)

 Call:
 glm(formula = hired ~ educ + exper + sex, family = binomial(link = ""logit""), 
     data = data)

 Deviance Residuals: 
     Min       1Q   Median       3Q      Max  
 -1.4380  -0.4573  -0.1009   0.1294   2.1804  

 Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)  
 (Intercept) -14.2483     6.0805  -2.343   0.0191 *
 educ          1.1549     0.6023   1.917   0.0552 .
 exper         0.9098     0.4293   2.119   0.0341 *
 sex           5.6037     2.6028   2.153   0.0313 *
 ---
 Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

 (Dispersion parameter for binomial family taken to be 1)

     Null deviance: 35.165  on 27  degrees of freedom
 Residual deviance: 14.735  on 24  degrees of freedom
 AIC: 22.735

 Number of Fisher Scoring iterations: 7
</code></pre>
"
"0.104257207028537","0.105409255338946"," 47795","<p>I am currently carrying out an investigation to find if certain factors such as playing home or away or position of a footballer affects overall pass completion using logistic regression. I am using R to compute my data. In my current section in which I am trying to analyse uses the data of every player to convey a general conclusion to whether or not the position of a player affects the successfulness of pass completion. </p>

<p>so far I have computed:</p>

<pre><code>test.logit &lt;- glm( cbind(Total.Successful.Passes.All,Total.Unsuccessful.Passes.All) ~
                   as.factor(Position.Id), data=passes.data, family = ""binomial"")

summary(test.logit)
</code></pre>

<p>and my output was:</p>

<pre><code>Coefficients:
                        Estimate Std. Error z value Pr(&gt;|z|)    

(Intercept)              0.28482    0.01256   22.67   &lt;2e-16 

as.factor(Position.Id)2  0.99768    0.01438   69.38   &lt;2e-16 

as.factor(Position.Id)4  1.06679    0.01398   76.29   &lt;2e-16 

as.factor(Position.Id)6  0.68090    0.01652   41.23   &lt;2e-16 

Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 32638  on 10269  degrees of freedom
Residual deviance: 26499  on 10266  degrees of freedom

AIC: 60422

Number of Fisher Scoring iterations: 4
</code></pre>

<p>the intercept is goalkeepers,position.Id 2 is for a defender, 4 = midfielder and 6 = striker</p>

<p>Is this a good set of results to come to a conclusion? and with the large deviances?</p>
"
"0.181488502160157","0.201843356939833"," 48766","<p>My logistic model <a href=""http://stats.stackexchange.com/q/48739/5509"">has been suspicious</a> due to enormous coefficients, so I tried to do a crossvalidation, and also do a crossvalidation of simplified model, to confirm the fact that the original model is overspecified, as <a href=""http://stats.stackexchange.com/a/48741/5509"">James suggested</a>. However, I don't know how to interpret the result (this is the model from the linked question):</p>

<pre><code>&gt; summary(m5)

Call:
glm(formula = cbind(ml, ad) ~ rok + obdobi + kraj + resid_usili2 + 
    rok:obdobi + rok:kraj + obdobi:kraj + kraj:resid_usili2 + 
    rok:obdobi:kraj, family = ""quasibinomial"")
[... see http://stats.stackexchange.com/q/48739/5509 for complete summary output ]

&gt; cv.glm(na.omit(data.frame(orel, resid_usili2)), m5, K = 10)
$call
cv.glm(data = na.omit(data.frame(orel, resid_usili2)), glmfit = m5, 
    K = 10)

$K
[1] 10

$delta
[1] 0.2415355 0.2151626

$seed
  [1]         403         271  1234892862 -1124595763  -489713400  1566924080   147612843
  [8]  1879282918  -694084381  1171051622  2063023839 -1307030905  -477709428  1248673977
 [15]  -746898494   420363755  -890078828   460552896  -758793089  -913500073  -882355605
[....]
Warning message:
glm.fit: algorithm did not converge
</code></pre>

<p>I guess the delta is the mean fitting error, but how to interpret it? Is it a good or bad fit? BTW, the algorithm did not converge, maybe due to the enormous coefficients (?)</p>

<p>I tried a simplified model:</p>

<pre><code>&gt; summary(m)

Call:
glm(formula = cbind(ml, ad) ~ rok + obdobi + kraj, family = ""quasibinomial"")

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.7335  -1.2324  -0.1666   1.0866   3.1788  

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -107.60761   48.06535  -2.239 0.025335 *  
rok            0.05381    0.02393   2.249 0.024683 *  
obdobinehn    -0.26962    0.10372  -2.599 0.009441 ** 
krajJHC        0.68869    0.27617   2.494 0.012761 *  
krajJHM       -0.26607    0.28647  -0.929 0.353169    
krajLBK       -1.11305    0.55165  -2.018 0.043828 *  
krajMSK       -0.61390    0.37252  -1.648 0.099593 .  
krajOLK       -0.49704    0.32935  -1.509 0.131501    
krajPAK       -1.18444    0.35090  -3.375 0.000758 ***
krajPLK       -1.28668    0.44238  -2.909 0.003691 ** 
krajSTC        0.01872    0.27806   0.067 0.946322    
krajULKV      -0.41950    0.61647  -0.680 0.496315    
krajVYS       -1.17290    0.39733  -2.952 0.003213 ** 
krajZLK       -0.38170    0.36487  -1.046 0.295698    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for quasibinomial family taken to be 1.304775)

    Null deviance: 2396.8  on 1343  degrees of freedom
Residual deviance: 2198.6  on 1330  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 4
</code></pre>

<p>and it's crossvalidation:</p>

<pre><code>&gt; cv.glm(orel, m, K = 10)
$call
cv.glm(data = orel, glmfit = m, K = 10)

$K
[1] 10

$delta
[1] 0.2156313 0.2154078

$seed
  [1]         403         526   300751243  -244464717  1066448079  1971573706 -1154513152
  [8]   634841816 -1521293072 -1040655077   505710009  -323431793 -1218609191  1060964279
 [15]  1349082996   -32847357 -1387496845   821178952  -971482876  1295018851  1380491861
</code></pre>

<p>Now it converged. But the delta seems more or less the same, despite of the fact that this model looks much more sane! I'm confused by the crossvalidation now... please give me a hint on how interpret it.</p>
"
"0.159255514317652","0.161015297179883"," 51152","<p>I've been trying to use the fastbw function from the rms package in R to perform logistic regression with backward selection, with p-values as exclusion criterion (I am well aware of the arguments against using p-values for this as opposed to e.g. AIC). However, the results are not in agreement with what I would get if I perform the backward selection manually, as fastbw often drops more factors in comparison. The results also seem to depend on the number of factors considered, even with the option </p>

<pre><code>type=""individual"".
</code></pre>

<p>I created some simple example data in order to prove my point, which give the following result:</p>

<pre><code>&gt; fastbw(lrm(y~x1+x2+x3+x4),rule=""p"",type=""individual"")

 Deleted Chi-Sq d.f. P      Residual d.f. P      AIC  
 x3      0.37   1    0.5412 0.37     1    0.5412 -1.63
 x1      1.82   1    0.1771 2.20     2    0.3336 -1.80
 x4      2.58   1    0.1082 4.78     3    0.1889 -1.22
 x2      3.56   1    0.0591 8.34     4    0.0799  0.34

[...]

Factors in Final Model

None
</code></pre>

<p>I.e., x2 is dropped as the last of the factors considered, resulting in a model without factors. However, if I consider x2 only, I get the following result. </p>

<pre><code>&gt; fastbw(lrm(y~x2),rule=""p"",type=""individual"")

No Factors Deleted

Factors in Final Model

[1] x2
</code></pre>

<p>The same is true if I do the backward selection manually, as x2 considered separately has a p-value of 0.045. What might cause this behavior? Since x2 is the last remaining variable in the backward selection, the results shouldn't depend on associations with other model covariates.</p>
"
"0.104257207028537","0.105409255338946"," 56440","<p>So I'm working with logistic regression models in R. Though I'm still new to statistics I feel like I got a bit of an understanding for regression models by now, but there's still something that bothers me:</p>

<p>Looking at the linked picture, you see the summary R prints for an example model I created. The model is trying to predict, if an email in the dataset will be refound or not (binary variable <code>isRefound</code>) and the dataset contains two variables closely related to <code>isRefound</code> , namely <code>next24</code> and <code>next7days</code> - these are also binary and tell if a mail will be clicked in the next 24hrs / next 7 days from the current point in the logs.</p>

<p>The high p-value should indicate, that the impact this variable has on the model prediction is pretty random, isn't it? 
Based on this I don't understand why the precision of the models predictions drops below 10% when these two variables are left out of the calculation formula. If these variables show such a low significance, why does removing them from the model have such a big impact?</p>

<p>Best regards and thanks in advance,
Rickyfox</p>

<p><img src=""http://i.stack.imgur.com/oiCrN.png"" alt=""enter image description here""></p>

<hr>

<h2>EDIT:</h2>

<p>First I removed only next24, which should yield a low impact because it's coef is pretty small. As expected, little changed - not gonna upload a pic for that. </p>

<p>Removing next7days tho had a big impact on the model: AIC 200k up, precision down to 16% and recall down to 73%</p>

<p><img src=""http://i.stack.imgur.com/583nx.png"" alt=""enter image description here""></p>
"
"0.180578779628654","0.182574185835055"," 60760","<p>let <code>m</code> be my matrix of data</p>

<pre><code>      x_i y_i
 [1,] 0.0   0
 [2,] 0.0   0
 [3,] 0.0   0
 [4,] 0.0   0
 [5,] 0.1   0
 [6,] 0.2   0
 [7,] 0.3   0
 [8,] 0.4   0
 [9,] 0.5   0
[10,] 0.6   0
[11,] 0.0   1
[12,] 0.0   1
[13,] 0.0   1
[14,] 0.9   1
[15,] 1.0   1
</code></pre>

<p>My aim is to study the logistic regression <code>y~x</code>, where the covariate <code>x</code> has observations <code>m[,1]</code> and similarly for <code>y</code>.
Please note that we have no complete separation in the data <em>but</em> the ""anomalous"" entries in rows <code>m[11,], m[12,]</code> and <code>m[13,]</code> all correspond to observations with <code>x_i=0</code>.</p>

<p>I expect <code>glm</code> to diverge as the likelihood function reaches no maximum in the ray  $k\beta$, for $k\rightarrow \infty$ and $\beta=(-0.7,1)$. </p>

<p>Using <code>glm</code> with 1 iteration I get the output </p>

<pre><code>  Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  -1.2552     0.7648  -1.641    0.101
x             1.6671     1.7961   0.928    0.353

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 19.095  on 14  degrees of freedom
Residual deviance: 18.275  on 13  degrees of freedom
AIC: 22.275

Number of Fisher Scoring iterations: 1
</code></pre>

<p>with an error message (the algorithm does not converge). 
Moreover, with the default number of iterations <code>(=25)</code> the output is</p>

<pre><code>Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  -1.1257     0.7552  -1.491    0.136
x             1.4990     1.6486   0.909    0.363

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 19.095  on 14  degrees of freedom
Residual deviance: 18.246  on 13  degrees of freedom
AIC: 22.246

Number of Fisher Scoring iterations: 4
</code></pre>

<p>and no error warning. </p>

<p>I see a contradiction; even in presence of 1 iteration the algorithm does not converge but the output is ""finite"" (I have not explicitly computed the inverse of the Hessian of the likelihood function, unfortunately). Moreover, with 25 iterations the warning message disappears and the output is still finite.</p>

<p>What do you think about this situation?
 Is it possible that <code>glm</code> stops automatically after the first iteration?
Thank you, Avitus</p>
"
"0.0851256530758749","0.086066296582387"," 63494","<p>In <code>R</code> I have a categorical variable that I performed logistic regression on and got the following result:</p>

<pre><code>glm(formula = mortality ~ SMOKE, family = binomial, data = c.data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.2155  -0.2155  -0.2155  -0.1860   2.8515  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -4.0483     0.3189 -12.694   &lt;2e-16 ***
SMOKEN        0.2968     0.3559   0.834    0.404    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 492.45  on 2369  degrees of freedom
Residual deviance: 491.72  on 2368  degrees of freedom
AIC: 495.72

Number of Fisher Scoring iterations: 6
</code></pre>

<p>Is the value for the intercept the same as <code>SMOKEY</code> (has a history of smoking)?</p>
"
"0.17025130615175","0.172132593164774"," 63927","<p>I am struggling to fit a simple logistic regression for one dependent value (group) by one independent qualitative variable (dilat) measured twice independently (rater).</p>

<p>I try many solutions and think according <a href=""http://www.ats.ucla.edu/stat/mult_pkg/whatstat/"" rel=""nofollow"">http://www.ats.ucla.edu/stat/mult_pkg/whatstat/</a> that the solution is a Mixed Effects Logistic Regression.</p>



<pre class=""lang-r prettyprint-override""><code>glmer_dilat&lt;-glmer(group ~ dilat + (1 | rater), data = ex, family = binomial)
summary(glmer_dilat)
</code></pre>



<pre class=""lang-r prettyprint-override""><code>Generalized linear mixed model fit by the Laplace approximation 
Formula: group ~ dilat + (1 | rater) 
   Data: ex 
   AIC   BIC logLik deviance
 105.5 112.5 -49.74    99.48
Random effects:
 Groups Name        Variance Std.Dev.
 rater  (Intercept)  0        0      
Number of obs: 76, groups: rater, 2

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.8473     0.4880   1.736   0.0825 .
dilat        -1.2827     0.5594  -2.293   0.0219 *
</code></pre>

<p>But the result is the same without !</p>

<pre class=""lang-r prettyprint-override""><code>summary(glm(group ~ dilat, data =ex, family = binomial))

glm(formula = group ~ dilat, family = binomial, data = ex)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.552  -0.999  -0.999   1.367   1.367  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.8473     0.4879   1.736   0.0825 .
dilat        -1.2826     0.5594  -2.293   0.0219 *
</code></pre>

<p>What is the solution?</p>

<p>please find my data set here after applying a dput command to it.</p>

<pre class=""lang-r prettyprint-override""><code>structure(list(id = structure(c(38L, 11L, 6L, 28L, 3L, 30L, 39L, 4L, 8L, 12L, 32L, 29L, 34L, 35L, 33L, 16L, 27L, 5L, 36L, 10L, 9L, 14L, 1L, 13L, 31L, 2L, 17L, 7L, 19L, 20L, 18L, 21L, 23L, 15L, 24L, 25L, 37L, 26L, 38L, 11L, 6L, 28L, 3L, 30L, 39L, 4L, 8L, 12L, 32L, 29L, 34L, 35L, 33L, 16L, 27L, 5L, 36L, 10L, 9L, 14L, 1L, 13L, 31L, 2L, 17L, 7L, 19L, 20L, 18L, 21L, 22L, 23L, 15L, 24L, 37L, 26L), .Label = c(""1038835"", ""2025267"", ""2053954"", ""3031612"", ""40004760"", ""40014515"", ""40040532"", ""40092413"", ""40101857"", ""40105328"", ""4016213"", ""40187296"", ""40203950"", ""40260642"", ""40269263"", ""40300349"", ""40308059"", ""40327146"", ""40333651"", ""40364468"", ""40435267"", ""40440293"", ""40443920"", ""40485124"", ""40609779"", ""40628741"", ""40662695"", ""5025220"", ""E9701737"", ""M/377313"", ""qsc22913"", ""QSC29371"", ""QSC43884"", ""QSC62220"", ""QSC75555"", ""QSC92652"", ""QSD01289"", ""QSD02237"", ""U/FY0296"" ), class = ""factor""), group = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), rater = c(1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L, 2L), dilat = c(1L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L), midbrain_atroph = c(1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), quadrigemi_atroph = c(1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), hum_sig = c(1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), flower_sig = c(0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), fp_atroph = c(0L, 0L, 1L, 0L, 1L, 0L, 1L, 1L, 1L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 0L, 1L, 0L, 0L, 1L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L), scp_atroph = c(0L, 1L, 1L, 0L, 1L, 1L, 0L, 1L, 0L, 0L, 0L, 1L, 0L, 1L, 1L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 0L, 0L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 1L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L, 0L, 0L, 0L, 0L, 0L)), .Names = c(""id"", ""group"", ""rater"", ""dilat"", ""midbrain_atroph"", ""quadrigemi_atroph"", ""hum_sig"", ""flower_sig"", ""fp_atroph"", ""scp_atroph""), class = ""data.frame"", row.names = c(NA, -76L))
</code></pre>
"
"0.159255514317652","0.161015297179883"," 64788","<p>I performed multivariate logistic regression with the dependent variable <code>Y</code> being death at a nursing home within a certain period of entry and got the following results (note if the variables starts in <code>A</code> it is a continuous value while those starting in <code>B</code> are categorical):</p>

<pre><code>Call:
glm(Y ~ A1 + B2 + B3 + B4 + B5 + A6 + A7 + A8 + A9, data=mydata, family=binomial)
Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.0728  -0.2167  -0.1588  -0.1193   3.7788  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  20.048631  6.036637   3.321 0.000896 ***
A1           0.051167   0.016942   3.020 0.002527 ** 
B2          -0.664940   0.304299  -2.185 0.028878 *  
B3          -2.825281   0.633072  -4.463 8.09e-06 ***
B4          -2.547931   0.957784  -2.660 0.007809 ** 
B5          -2.862460   1.385118  -2.067 0.038774 *  
A6          -0.129808   0.041286  -3.144 0.001666 ** 
A7           0.020016   0.009456   2.117 0.034276 *  
A8          -0.707924   0.253396  -2.794 0.005210 ** 
A9           0.003453   0.001549   2.229 0.025837 *  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 485.10  on 2206  degrees of freedom
Residual deviance: 417.28  on 2197  degrees of freedom
AIC: 437.28

Number of Fisher Scoring iterations: 7

 (Intercept)           A1           B2           B3           B4           B5           A6           A7           A8           A9 
5.093426e+08 1.052499e+00 5.143045e-01 5.929197e-02 7.824340e-02 5.712806e-02 8.782641e-01 1.020218e+00 4.926657e-01 1.003459e+00 

                   2.5 %       97.5 %
(Intercept) 3.703525e+03 7.004944e+13
A1          1.018123e+00 1.088035e+00
B2          2.832698e-01 9.337710e-01
B3          1.714448e-02 2.050537e-01
B4          1.197238e-02 5.113460e-01
B5          3.782990e-03 8.627079e-01
A6          8.099945e-01 9.522876e-01
A7          1.001484e+00 1.039302e+00
A8          2.998207e-01 8.095488e-01
A9          1.000416e+00 1.006510e+00
</code></pre>

<p>As you can see, all of the variables are ""significant"" in that their p values are below the usual threshold of 0.05. However looking at the coefficients, I'm not quite sure what to make of these results. It seems that although these variables contribute to the model, looking at the odds ratios, they don't seem to really seem to have much predictive power. Of note, when I calculated the AUC, I got approximately 0.8. </p>

<p>Can I say that this model is better at predicting against mortality (e.g. predicting that seniors will live past the prescribed period) compared to predicting for mortality?</p>
"
"0.0601929265428846","0.0608580619450185"," 65258","<p>Consider the Challenger-Disaster:</p>

<pre><code>Temp &lt;- c(66,67,68,70,72,75,76,79,53,58,70,75,67,67,69,70,73,76,78,81,57,63,70)
Fail &lt;- factor(c(0,0,0,0,0,0,0,0,1,1,1,1,0,0,0,0,0,0,0,0,1,1,1))
shuttle &lt;- data.frame(Temp, Fail)
colnames(shuttle) &lt;- c(""Temp"", ""Fail"")
</code></pre>

<p>Now I can fit a logistic model which will explain the ""Fail"" of O-ring seals by Temperature:</p>

<pre><code>fit &lt;- glm(Fail~Temp,data=shuttle, family=binomial); fit
</code></pre>

<p>The R output looks like this:</p>

<pre><code> Call:  glm(formula = Ausfall ~ Temp, family = binomial, data =
 shuttle)

 Coefficients: (Intercept)         Temp  
     15.0429      -0.2322  

 Degrees of Freedom: 22 Total (i.e. Null);  21 Residual Null Deviance:  
 28.27  Residual Deviance: 20.32    AIC: 24.32
</code></pre>

<h3>Questions</h3>

<ul>
<li><strong>In general, how do you predict probabilities for specific data in logistic regressions using R?</strong></li>
<li><strong>Or specifically, what is the command to calculate the probability of a ""Fail"" if temperature is at 37Â°?</strong> (which it was in the night before the Challenger disaster).</li>
</ul>

<p>I thought it would be something like this:</p>

<pre><code>predict(fit, Temp=37)
</code></pre>

<p>but it won't give me ""0.9984243"" (which I calculated myself with:  </p>

<pre><code>exp(15.0429 + (37*(-0.2322))) / 1+ exp(15.0429 + (37*(-0.2322)))
</code></pre>

<p>The method <code>predict</code> returns a matrix of numbers that makes no sense to me.</p>
"
"0.181488502160157","0.183493960854393"," 67243","<p>This is a fairly complex question so I will attempt to ask it in a fairly basic manner. </p>

<p>I have data on the abundance of 99 different species of estuarine macroinvertebrate species and the sediment mud content (0 - 100 %) in which each observation was obtained. I have a total of 1402 observations for each species (i.e. a massive dataset). </p>

<p>Here is a subset of the raw data for one species to give you an idea of the data I'm working with (if I had 10 reputation points I'd upload a plot of real raw data):</p>

<pre><code>Abundance: 10,14,10,3,3,3,3,4,5,5,0,0,0,0,0,0,0,0,0,0,0,0,0,6,6,6,0,0,0,0,12,0,0,0,34,0,0
Mud %:     0.9,4,2,10,13,14,6,5,5,7,22,27,34,37,47,58,54,70,54,80,90,65,56,7,8,34,67,54,32,1,57,45,49,4,78,65,45,35
</code></pre>

<p>The primary aim of my research is to determine an ""optimum mud % range"" (e.g. 15 - 45 %) and ""distribution mud % range"" (e.g. 0 - 80 %) for each of the 99 invertebrate species.</p>

<p>As you can see the abundance data for the above species contains a significant number of zero values. Although this significantly skews any sort of model that I run on the data (i.e. GLM, GAM), even if I model the non-zero data only, the model for certain species does not fit the data at all well.</p>

<p>So, my question is: what would be the best, most robust way to determine an ""optimum"" and ""distribution"" mud range for each species, given that responses vary significantly between species? </p>

<hr>

<p>Just to clarify - the above data is a hypothetical example to give you an idea of how messy the abundance (that is count) data can be for a given species.</p>

<p>Regarding the poisson regression approach: I'm considering conducting a two-step GLM or GAM approach for each species; Step (1) uses logistic regression to model the ""probability of presence""  for a given species over the mud gradient - using presence/absence data. This obviously takes into account the zero counts; and Step (2) models the ""maximum abundance"" over the mud gradient - using presence only count data. This step gives me an idea of the species typical response to mud where they DO occur. What are your thoughts on this approach?</p>

<p>I have R code for both steps for one particular species. Heres the code:</p>

<pre><code>     ## BINARY

aa1&lt;-glm(Bin~Mud,dist=binomial,data=Antho)
xmin &lt;- ceiling(min(Antho$Mud))
    xmax &lt;- floor(max(Antho$Mud))
Mudnew &lt;- seq(from=xmin, to=xmax, by=0.1)
pred.dat &lt;- data.frame(Mudnew)
names(pred.dat) &lt;- ""Mud""
pred.aa1 &lt;- data.frame(predict.glm(aa1, pred.dat, se.fit=TRUE, type=""response""))
pred.aa1.comb &lt;- data.frame(pred.dat, pred.aa1)
names(pred.aa1.comb)
plot(fit ~ Mud, data=pred.aa1.comb, type=""l"", lwd=2, col=1, ylab=""Probability of presence"", xlab=""Mud content (%)"", ylim=c(0,1))


## Maximum abundance

 aa2&lt;-glm(Maxabund~Mud,family=Gamma,data=antho)
 xmin &lt;- ceiling(min(antho$Mud))
     xmax &lt;- floor(max(antho$Mud))
 Mudnew &lt;- seq(from=xmin, to=xmax, by=0.1)
 pred.dat &lt;- data.frame(Mudnew)
 names(pred.dat) &lt;- ""Mud""
 pred.aa2 &lt;- data.frame(predict.glm(aa2, pred.dat, se.fit=TRUE, type=""response""))
 pred.aa2.comb &lt;- data.frame(pred.dat, pred.aa2)
 names(pred.aa2.comb)
 plot(fit ~ Mud, data=pred.aa2.comb, type=""l"", lwd=2, col=1, ylab=""Maximum abundance per 0.0132 m2"", xlab=""Mud content (%)"")
 AIC(aa2)
</code></pre>

<p>My question is: for step (2); will the model code need to be altered (i.e. family=) depending on the shape of each species abundance data, if so, would I just need to inspect a scatter plot of the raw presence only abundance data to confirm the use of a certain function? and how would the code be written for a certain species exhibiting a certain response/functional form? </p>
"
"0.147441956154897","0.149071198499986"," 73567","<p>I have a logistic regression model with several variables and one of those variables (called x3 in my example below) is not significant. However, x3 should remain in the model because it is scientifically important.</p>

<p>Now, x3 is continuous and I want to create a plot of the predicted probability vs x3. Even though x3 is not statistically significant, it has an effect on my outcome and therefore it has an effect on the predicted probability. This means that I can see from the graph, that the probability changes with increasing x3. However, how should I interpret the graph and the change in the predicted probability, given that x3 is indeed not statistically significant?</p>

<p>Below is a simulated data in R set to illustrate my question. The graph also contains a 95% confidence interval for the predicted probability (dashed lines):</p>

<pre><code>&gt; set.seed(314)
&gt; n &lt;- 300
&gt; x1 &lt;- rbinom(n,1,0.5)
&gt; x2 &lt;- rbinom(n,1,0.5)
&gt; x3 &lt;- rexp(n)
&gt; logit &lt;- 0.5+0.9*x1-0.5*x2
&gt; prob &lt;- exp(logit)/(1+exp(logit))
&gt; y &lt;- rbinom(n,1,prob)
&gt; 
&gt; model &lt;- glm(y~x1+x2+x3, family=""binomial"")
&gt; summary(model)

Call:
glm(formula = y ~ x1 + x2 + x3, family = ""binomial"")

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.0394  -1.1254   0.5604   0.8554   1.4457  

Coefficients:
            Estimate Std. Error z value Pr(    &gt;|z|)    
(Intercept)   1.1402     0.2638   4.323 1.54e-05 ***
x1            0.8256     0.2653   3.112  0.00186 ** 
x2           -1.1338     0.2658  -4.266 1.99e-05 ***
x3           -0.1478     0.1249  -1.183  0.23681    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 373.05  on 299  degrees of freedom
Residual deviance: 341.21  on 296  degrees of freedom
AIC: 349.21

Number of Fisher Scoring iterations: 3

&gt; 
&gt; dat &lt;- data.frame(x1=1, x2=1, x3=seq(0,5,0.1))
&gt; preds &lt;- predict(model, dat,type = ""link"", se.fit = TRUE )
&gt; critval &lt;- 1.96
&gt; upr &lt;- preds$fit + (critval * preds$se.fit)
&gt; lwr &lt;- preds$fit - (critval * preds$se.fit)
&gt; fit &lt;- preds$fit
    &gt; 
    &gt; fit2 &lt;- mod$family$linkinv(fit)
    &gt; upr2 &lt;- mod$family$linkinv(upr)
    &gt; lwr2 &lt;- mod$family$linkinv(lwr)
    &gt; 
    &gt; plot(dat$x3, fit2, lwd=2, type=""l"", main=""Predicted Probability"", ylab=""Probability"", xlab=""x3"", ylim=c(0,1.00))
&gt; lines(dat$x3, upr2, lty=2)
    &gt; lines(dat$x3, lwr2, lty=2)
</code></pre>

<p><img src=""http://i.stack.imgur.com/ljW7W.png"" alt=""enter image description here""></p>

<p>Thanks!</p>

<p>Emilia</p>
"
"0.0601929265428846","0.0608580619450185"," 74304","<p>What goodness of fit tests are usually used for quantile regression? Ideally I need something similar to F-test in linear regression, but something like AIC in logistic regression will suite as well. I use quantreg R package, but found only some Khmaladze test in there. To be fair I hardly understand what is does.</p>
"
"0.180578779628654","0.182574185835055"," 87650","<p>I ran into (what I think is) an inconsistency when running a random-intercept model (using the <em>lmer</em> function in the <em>lme4</em> package in R).</p>

<p>Here is what I do: I first run a model with a set of covariates; then I run the same model re-scaling (linearly transforming) one of the regressors. To my knowledge, this should change <strong>only the coefficient</strong> of the variable that is linearly transformed. And indeed, this is what happens when I run this ""experiment"" with a simple linear regression model and with a logistic model.</p>

<p>This code replicates the ""normal"" behaviour:</p>

<pre><code># Create three random independent variables
x1 &lt;- rnorm(20)
x2 &lt;- rnorm(20)
x3 &lt;- as.factor(sample(0:2, 20, replace = TRUE))
# Their random coefficients
coef1 &lt;- runif(1, -1, 1)
coef2 &lt;- runif(1, -1, 1)
# Create a continuous dependent variable and a binomial one
y1 &lt;- coef1 * x1 + coef2 * x2 + runif(20)
y2 &lt;- y1
y2[which(y1 &gt; quantile(y1, 0.5))] &lt;- 1
y2[which(y1 &lt;= quantile(y1, 0.5))] &lt;- 0
# Finally, a linear transformation of x1
x1.trans &lt;- x1*3
</code></pre>

<p>So, let us run an OLS model:</p>

<pre><code>lm &lt;- lm(y1 ~ x1 + x2 + x3)
summary(lm)
# OLS model with one variable linearly transformed
lm.bis &lt;- lm(y1 ~ x1.trans + x2 + x3)
summary(lm.bis)
</code></pre>

<p>The coefficients of <em>x1</em> and <em>x1.trans</em> are different, <strong>but the R-square of the two models is the same</strong>:</p>

<pre><code>summary(lm)$r.sq == summary(lm.bis)$r.sq
</code></pre>

<p>The same with a logistic model:</p>

<pre><code>logm &lt;- glm(y2 ~ x1 + x2, family=""binomial"")
summary(logm)
logm.bis &lt;- glm(y2 ~ x1.trans + x2, family=""binomial"")
summary(logm.bis)
</code></pre>

<p>Even in this case, <strong>the log-likelihood of the two models is the same</strong>:</p>

<pre><code>logLik(logm) == logLik(logm.bis)
</code></pre>

<p>So far, so good. However, when I do the same with a hierarchical model, <strong>the log-likelihood (and consequently the AIC and BIC) of the two models are different</strong>, although the coefficient of the transformed variable remains significant with the same z value and the other coefficients are the same.</p>

<pre><code># Multilevel model
mm &lt;- lmer(y1 ~ x1 + x2 + (1 | x3))
summary(mm)
mm.bis &lt;- lmer(y1 ~ x1.trans + x2 + (1 | x3))
summary(mm.bis)
logLik(mm) == logLik(mm.bis) ### FALSE! ###
</code></pre>

<p>Why? Also the ""REML criterion at convergence"" is obviously different. I don't understand this result. This is probably due to my moderate knowledge of the math of hierarchical models. I'd be very happy if some of you could show me what's the trick here.</p>

<p>Since we then use AIC and BIC to compare models, I am puzzled by the fact that a simple transformation that shouldn't change anything makes one model better (or worse) than another.</p>
"
"0.134595475514541","0.136082763487954"," 89474","<p>I ran this ordinal logistic regression in R:</p>

<pre><code>mtcars_ordinal &lt;- polr(as.factor(carb) ~ mpg, mtcars)
</code></pre>

<p>I got this summary of the model:</p>

<pre><code>summary(mtcars_ordinal)

Re-fitting to get Hessian

Call:
polr(formula = as.factor(carb) ~ mpg, data = mtcars)

Coefficients:
      Value Std. Error t value
mpg -0.2335    0.06855  -3.406

Intercepts:
    Value   Std. Error t value
1|2 -6.4706  1.6443    -3.9352
2|3 -4.4158  1.3634    -3.2388
3|4 -3.8508  1.3087    -2.9425
4|6 -1.2829  1.3254    -0.9679
6|8 -0.5544  1.5018    -0.3692

Residual Deviance: 81.36633 
AIC: 93.36633 
</code></pre>

<p>I can get the log odds of the coefficient for <code>mpg</code> like this:</p>

<pre><code>exp(coef(mtcars_ordinal))
 mpg 
0.7917679 
</code></pre>

<p>And the the log odds of the thresholds like:</p>

<pre><code>exp(mtcars_ordinal$zeta)

       1|2         2|3         3|4         4|6         6|8 
0.001548286 0.012084834 0.021262900 0.277242397 0.574406353 
</code></pre>

<p>Could someone tell me if my interpretation of this model is correct:</p>

<blockquote>
  <p>As <code>mpg</code> increases by one unit, the odds of moving from category 1 of <code>carb</code> into any of the other 5 categories, decreases by -0.23. If the log odds crosses the threshold of 0.0015, then the predicted value for a car will be category 2 of <code>carb</code>.  If the log odds crosses the threshold of 0.0121, then the predicted value for a car will be category 3 of <code>carb</code>, and so on.</p>
</blockquote>
"
"0.180578779628654","0.182574185835055"," 89692","<p>My data has 3 major inputs: <code>BLDDAY</code> (a factor), <code>BLDMNT</code> (a factor), and <code>D_BLD_SER</code> (days as an integer variable).  The output is whether input variable has any impact on failure.  My model is: <code>model = glm(FAILED~BLDDAY+BLDMNT+D_BLD_SER, family=""binomial"", data=data_list)</code>.  (I used <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">UCLA's statistics help site's guide to logistic regression in R</a> to build this model.)  </p>

<p>Output: </p>

<pre><code>Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.3282  -0.9123  -0.8128   1.4056   2.1053  

Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)     -0.7672583  0.1317132  -5.825 5.70e-09 ***
BLDDAYMonday    -0.1545646  0.0839380  -1.841  0.06556 .  
BLDDAYSaturday  -0.1257976  0.2028259  -0.620  0.53511    
BLDDAYSunday    -0.1183008  0.1868713  -0.633  0.52669    
BLDDAYThursday  -0.2007452  0.0772653  -2.598  0.00937 ** 
BLDDAYTuesday    0.0480453  0.0758603   0.633  0.52651    
BLDDAYWednesday -0.0358585  0.0760027  -0.472  0.63707    
BLDMNTAug        0.3009445  0.1405545   2.141  0.03226 *  
BLDMNTDec        0.5562170  0.1338467   4.156 3.24e-05 ***
BLDMNTFeb        0.3334978  0.2133475   1.563  0.11801    
BLDMNTJan        0.4076504  0.2277978   1.790  0.07353 .  
BLDMNTJul        0.1306585  0.1415302   0.923  0.35591    
BLDMNTJun       -0.0357361  0.1428105  -0.250  0.80241    
BLDMNTMar        0.4570491  0.1949815   2.344  0.01907 *  
BLDMNTMay       -0.2292620  0.1614577  -1.420  0.15562    
BLDMNTNov        0.3060012  0.1334034   2.294  0.02180 *  
BLDMNTOct        0.2390501  0.1341877   1.781  0.07484 .  
BLDMNTSep        0.2481405  0.1384901   1.792  0.07317 .  
D_BLD_SER       -0.0020960  0.0003367  -6.225 4.82e-10 ***

(Dispersion parameter for binomial family taken to be 1)
    Null deviance: 10288  on 8182  degrees of freedom
Residual deviance: 10154  on 8164  degrees of freedom
AIC: 10192
Number of Fisher Scoring iterations: 4
</code></pre>

<p>The ANOVA table is the following:</p>

<pre><code>anova(model, test=""Chisq"")
Analysis of Deviance Table
Model: binomial, link: logit
Response: FAILED
Terms added sequentially (first to last)

          Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
NULL                       8182      10288              
BLDDAY     6   20.392      8176      10268  0.002357 ** 
BLDMNT    11   70.662      8165      10197 9.142e-11 ***
D_BLD_SER  1   43.797      8164      10154 3.642e-11 ***
</code></pre>

<p>My questions are:</p>

<ol>
<li><p>Although the p-values for all three components are less than 0.05, which can be considered as significant, the deviance reduced due to each component is less than 1% of the total deviance. <strong>Normally the interpretation of output like this is input parameter affects output and it's better to use this parameter then using noting.</strong> But does it really make sense of taking this parameter as significant input?</p></li>
<li><p>The p-values for <code>BLDDAY</code> and <code>BLDMNT</code> given by <code>anova()</code> is the overall p-value,  which is significant, but <code>summary()</code> gives detailed impact of each factor level. If I consider the p-values for each factor overall <code>BLDDAY</code> is significant but individually only <code>BLDDAYThursday</code> is significant. I am bit confused not as whether to consider <code>BLDDAY</code> as significant input, or Thursday only, or Thursday &amp; Friday both.</p></li>
</ol>
"
"0.208514414057075","0.210818510677892"," 94581","<p>I have a ordinal dependendent variable, easiness, that ranges from 1 (not easy) to 5 (very easy).  Increases in the values of the independent factors are associated with an increased easiness rating.</p>

<p>Two of my independent variables (<code>condA</code> and <code>condB</code>) are categorical, each with 2 levels, and 2 (<code>abilityA</code>, <code>abilityB</code>) are continuous.</p>

<p>I'm using the <a href=""http://cran.r-project.org/web/packages/ordinal/index.html"">ordinal</a> package in R, where it uses what I believe to be</p>

<p>$$\text{logit}(p(Y \leqslant g)) = \ln \frac{p(Y \leqslant g)}{p(Y &gt; g)} = \beta_{0_g} - (\beta_{1} X_{1} + \dots + \beta_{p} X_{p}) \quad(g = 1, \ldots, k-1)$$<br>
(from @caracal's answer <a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">here</a>)</p>

<p>I've been learning this independently and would appreciate any help possible as I'm still struggling with it.  In addition to the tutorials accompanying the ordinal package, I've also found the following to be helpful: </p>

<ul>
<li><a href=""http://stats.stackexchange.com/questions/89474/interpretation-of-ordinal-logistic-regression"">Interpretation of ordinal logistic regression</a></li>
<li><a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">Negative coefficient in ordered logistic regression</a></li>
</ul>

<p>But I'm trying to interpret the results, and put the different resources together and am getting stuck. </p>

<ol>
<li><p>I've read many different explanations, both abstract and applied, but am still having a hard time wrapping my mind around what it means to say: </p>

<blockquote>
  <p>With a 1 unit increase in condB (i.e., changing from one level to the next of the categorical predictor), the predicted odds of observing Y = 5 versus Y = 1 to 4 (as well as the predicted odds of observed Y = 4 versus Y = 1 to 3) change by a factor of exp(beta) which, for diagram, is exp(0.457) = 1.58. </p>
</blockquote>

<p>a. Is this different for the categorical vs. continuous independent variables?<br>
b. Part of my difficulty may be with the cumulative odds idea and those comparisons. ... Is it fair to say that going from condA = absent (reference level) to condA = present is 1.58 times more likely to be rated at a higher level of easiness?  I'm pretty sure that is NOT correct, but I'm not sure how to correctly state it.</p></li>
</ol>

<p>Graphically,<br>
1. Implementing the code in <a href=""http://stats.stackexchange.com/questions/89474/interpretation-of-ordinal-logistic-regression"">this post</a>, I'm confused as to why the resulting 'probability' values are so large.<br>
2. The graph of p (Y = g) in <a href=""http://stats.stackexchange.com/questions/38087/negative-coefficient-in-ordered-logistic-regression/38130#38130"">this post</a> makes the most sense to me ... with an interpretation of the probability of observing a particular category of Y at a particular value of X.  The reason I am trying to get the graph in the first place is to get a better understanding of the results overall.</p>

<p>Here's the output from my model:</p>

<pre><code>m1c2 &lt;- clmm (easiness ~ condA + condB + abilityA + abilityB + (1|content) + (1|ID), 
              data = d, na.action = na.omit)
summary(m1c2)
Cumulative Link Mixed Model fitted with the Laplace approximation

formula: 
easiness ~ illus2 + dx2 + abilEM_obli + valueEM_obli + (1 | content) +  (1 | ID)
data:    d

link  threshold nobs logLik  AIC    niter     max.grad
logit flexible  366  -468.44 956.88 729(3615) 4.36e-04
cond.H 
4.5e+01

Random effects:
 Groups  Name        Variance Std.Dev.
 ID      (Intercept) 2.90     1.70    
 content  (Intercept) 0.24     0.49    
Number of groups:  ID 92,  content 4 

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)    
condA              0.681      0.213    3.20   0.0014 ** 
condB              0.457      0.211    2.17   0.0303 *  
abilityA           1.148      0.255    4.51  6.5e-06 ***
abilityB           0.577      0.247    2.34   0.0195 *  

Threshold coefficients:
    Estimate Std. Error z value
1|2   -3.500      0.438   -7.99
2|3   -1.545      0.378   -4.08
3|4    0.193      0.366    0.53
4|5    2.121      0.385    5.50
</code></pre>
"
"0.255376959227625","0.258198889747161"," 95378","<p>I am doing statistics for the first time in my life and I am not quite sure what to include and how to interpret the results. I am doing a logistic regression in R. Here is what I have so far:</p>

<ol>
<li><p><code>GLM</code> with family = binomial (dependent ~ indep1 + indep2 + ...+ indep7  +0)
If I dont include the 0 I get NA for my last independent variable in the summary output..</p></li>
<li><p><code>Update</code> the model (indep2 has a p-value > 0.05 and is left out)</p></li>
<li><p>I am applying anova</p>

<pre><code>anova(original_model,updated_model, test=""Chisq"")

   Resid.Df  Resid.Dev Df Deviance Pr(&gt;Chi)
1     34067      18078                     
2     34066      18075  1   2.4137   0.1203
</code></pre>

<p>Here I am not sure how to interpret it. What tells me if the simplification of the model is significant? the p-value is with 0.12 bigger than 0.05, does this mean that the simplification is not significant? </p></li>
<li><p>make a cross-table (compare predicted (probability >0.5) - observed)</p>

<pre><code>fit
      FALSE  TRUE
  No  30572    68
  yes  3407    31
</code></pre>

<p>I'd say that 31 values are predicted correctly (yes-true), resp 68 (no-true) but that most values are classified wrong, which means that the model is really bad?</p></li>
<li><p>then I make a wald test for each independent variable for the first independent variable it would look like this:</p>

<pre><code>&gt; wald.test(b = coef(model_updated), Sigma = vcov(model_updated), Terms
&gt; = 1:1)
</code></pre>

<p>here I only look if the p-values are significant and if they are it means that all variables contribute significantly to the predictive ability of the model</p></li>
<li><p>I calculate the odds with their confidence intervals (this is basically exp(estimate)</p>

<pre><code>oddsCI &lt;- exp(cbind(OR = coef(model_updated), confint(model_updated)))
</code></pre>

<p>For all odds smaller than 1 i do 1/odd</p>

<pre><code>Estimate        Odds Ratio      Inverse Odds
-0.000203       0.999801041     1.000198999
 0.000332       1.000326571     odd bigger than 1
-0.000133       0.999846418     1.000153605
-3.48       0.008696665     114.9866056
-4.85       0.029747223     33.61658319
-2.37       0.000438382     2281.113996
-8.16       0.110348634     9.062187402
-2.93       0.062668509     15.95697759
-3.65       0.020156889     49.61083057
-5.45       0.033996464     29.41482359
-4.02       0.004837987     206.6975334
</code></pre>

<p>This O would interpret like that for the ""odd bigger than 1""  the case is over 1 times more likely to occur. (Is is incorrect to say that, or not?) Or for the last row you could say that t for every subtraction of a unit, the odds for the case to appear decreases by a factor of 206.</p></li>
<li><p>Then I look at </p>

<pre><code>with(model_updated, null.deviance - deviance) #deviance
with(model_updated, df.null - df.residsual) #degrees of freedom
 # pvalue
with(Amodel_updated, pchisq(null.deviance - deviance, df.null - df.residual, 
lower.tail = FALSE))
logLik(model_updated)
</code></pre>

<p>But I don't really know what this tells me.</p></li>
<li><p>In a last step I do</p>

<pre><code>stepAIC(model_updated, direction=""both"")
</code></pre>

<p>but also here I don't know how to interpret the outcome. I see that it looks at all interactions between my independent variables but I don't know what it tells me.</p></li>
</ol>

<p>After this, I can make a prediction by using the updated model and by separating it into training data and validation data I suppose?</p>
"
"0.199637352376173","0.183493960854393"," 95386","<p>What I have is a medical data set with several variables, all 0-1 variables. I want to make inference about them with logistic regression. I have a few problems:</p>

<ol>
<li><p>I have location variables for the disease. I was advised by my statistic advisor to put them in bins as follows: If it was solely in the right part of the organ then I would mark 1 in the column for right and similarily for left. However if it were in both places I marked in neither of the left and right column but marked one in column both. Using this approach I get error in R, numeric 0 1 error when I use glm in R and I think it is due to how these variables are constructed. Shouldn't I rather have just left and right variables and when we have the disease in both sides I should mark in left and right column and skip the both column and maybe introduce interaction term between left and right (that I would at least do in a linear model).</p></li>
<li><p>Using glm (family binomial for logistic regression) in R I was thinking how to find the best model describing some variable. I started with one usual approach with finding univarietly which variables had p-value less than $0.1$ in Fischer exact test. Then I included those variables in my model and started to delete them after which had the highest p-value. In most medical reasearches I have read when applying multivariate regression I see the usage of p-value $0.05$ but I have a feeling that it might be because of lack of understanding of the subject. When I ranked the model according to AIC and explored the best model I usually got variable with p-value around $0.1$. Which approach is preferably, is it justifyable to just cut of at p-value $0.05$ or should use AIC as an estimator of the best multivariate model? AIC does punish for extra variables and so it shouldnt give one too many variables.</p></li>
</ol>
"
"0.0601929265428846","0.0608580619450185"," 95795","<p>from what I have studied in the data mining course (please correct me if I'm wrong) - in logistic regression, when the response variable is binary, then from the ROC curve we can determine the threshold.</p>

<p>Now I'm trying to apply the logistic regression for an ordinal categorical response variable with  more than two categories (4).
I used the function <code>polr</code> in r:</p>

<pre><code>&gt; polr1&lt;-polr(Category~Division+ST.Density,data=Versions.data)
&gt; summary(polr1)

Re-fitting to get Hessian

Call:
polr(formula = Category ~ Division + ST.Density, data = Versions.data)

Coefficients:
               Value Std. Error t value
DivisionAP   -0.8237     0.5195  -1.586
DivisionAT   -0.8989     0.5060  -1.776
DivisionBC   -1.5395     0.5712  -2.695
DivisionCA   -1.8102     0.5240  -3.455
DivisionEM   -0.5580     0.4607  -1.211
DivisionNA   -1.7568     0.4704  -3.734
ST.Density    0.3444     0.0750   4.592

Intercepts:
    Value   Std. Error t value
1|2 -1.3581  0.4387    -3.0957
2|3 -0.5624  0.4328    -1.2994
3|4  1.2661  0.4390     2.8839

Residual Deviance: 707.8457 
AIC: 727.8457  
</code></pre>

<p>How should I interpret the Intercepts?
and how can I determine the threshold for each group?</p>

<p>Thanks</p>
"
"0.147441956154897","0.149071198499986"," 95891","<p>I'm running a logistic regression model where anecdotally I expected age to be a very large factor. If you see from the charts I made in Excel before running the model through R, this is how the support lines up by age:</p>

<p><img src=""http://i.stack.imgur.com/oEVZQ.jpg"" alt=""enter image description here""></p>

<p>Looks pretty significant.</p>

<p>Though when I run the model, as you can see below, age is the <em>only</em> thing that's not significant -- which was very surprising:</p>

<pre><code>&gt; attach(mydata) 
&gt; 
&gt; # Define variables 
&gt; 
&gt; Y &lt;- cbind(support)
&gt; X &lt;- cbind(sex, region, age, supportscore1, supportscore2, county)
&gt;
&gt; # Logit model coefficients 
&gt; 
&gt; logit &lt;- glm(Y ~ X, family=binomial (link = ""logit""), na.action = na.exclude) 
&gt; 
&gt; summary(logit) 

Call:
glm(formula = Y ~ X, family = binomial(link = ""logit""), na.action = na.exclude)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.1019  -0.7609   0.5231   0.7101   2.3965  

Coefficients:
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)            4.013446   0.440962   9.102  &lt; 2e-16 ***
Xsex                  -0.229256   0.104859  -2.186 0.028792 *  
Xregion               -1.103308   0.091497 -12.058  &lt; 2e-16 ***
Xage                   0.004569   0.003209   1.424 0.154512    
Xsupportscore1        -0.019262   0.005732  -3.360 0.000778 ***
Xsupportscore2         0.019810   0.005264   3.764 0.000168 ***
Xcounty               -0.047581   0.011161  -4.263 2.02e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2871.5  on 2072  degrees of freedom
Residual deviance: 2245.5  on 2066  degrees of freedom
  (66 observations deleted due to missingness)
AIC: 2259.5

Number of Fisher Scoring iterations: 4
</code></pre>

<p>My only guess on this is that the previous support scores (both 0-100 numerical values) I'm using may have already taken age into account, and the model doesn't want to count it twice. Though, to compare, region and county are just two different ways of cutting up the geography -- and those both seem significant.</p>

<p>Can somebody let me know what you would think if your model told you that age wasn't significant when in clearly is? Trying to figure out if there's a way of thinking about it that I'm missing or if something in my code is wrong.</p>

<p>Thanks!</p>

<p>--
<strong>EDIT</strong></p>

<p>Pairs plot added to show correlation (despite some factors being categorical):</p>

<pre><code>pairs(~sex + region +  age + supportscore1 + supportscore2 + county, data=mydata)
</code></pre>

<p><img src=""http://i.stack.imgur.com/N2IG4.jpg"" alt=""enter image description here""></p>
"
"0.122868296795748","0.149071198499986"," 96236","<p>I am following an example <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">here</a> on using Logistic Regression in R. However, I need some help interpreting the results. They do go over some of the interpretations in the above link, but I need more help with understanding a goodness of fit for Logistic Regression and the output that I am given.</p>

<p>For convenience, here is the summary given in the example:</p>

<pre><code>## Call:
## glm(formula = admit ~ gre + gpa + rank, family = ""binomial"", 
##     data = mydata)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.627  -0.866  -0.639   1.149   2.079  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -3.98998    1.13995   -3.50  0.00047 ***
## gre          0.00226    0.00109    2.07  0.03847 *  
## gpa          0.80404    0.33182    2.42  0.01539 *  
## rank2       -0.67544    0.31649   -2.13  0.03283 *  
## rank3       -1.34020    0.34531   -3.88  0.00010 ***
## rank4       -1.55146    0.41783   -3.71  0.00020 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 499.98  on 399  degrees of freedom
## Residual deviance: 458.52  on 394  degrees of freedom
## AIC: 470.5
## 
## Number of Fisher Scoring iterations: 4
</code></pre>

<ol>
<li>How well did Logistic Regression fit here?</li>
<li>What exactly are the Deviance Residuals? I believe they are the average residuals per quartile. How do I determine if they are bad/good/statistically significant?</li>
<li>What exactly is the <code>z-value</code> here? Is it the normalized standard deviation from the mean of the Estimate assuming a mean of 0? </li>
<li>What exactly are Signif. codes?</li>
</ol>

<p>Any help is greatly appreciated! You do not have to answer them all!</p>
"
"0.147441956154897","0.149071198499986","102695","<p>I'm using R to run some logistic regression. My variables were continuous, but I used cut to bucket the data. Some particular buckets for these variables always result in dependent variable being equal to 1. As expcted, the coefficient estimate for this bucket is very high, but the p-value is also high. There are about ~90 observations in either these buckets, and around 800 total observations, so I don't think it's a problem of sample size. Also, this variable should not be related to other variables, which would naturally reduce their p-values.</p>

<p>Are there any other plausible explanations for the high p-value?</p>

<p>Example:</p>

<pre><code>myData &lt;- read.csv(""application.csv"", header = TRUE)
myData$FICO &lt;- cut(myData$FICO, c(0, 660, 680, 700, 720, 740, 780, Inf), right = FALSE)
myData$CLTV &lt;- cut(myData$CLTV, c(0, 70, 80, 90, 95, 100, 125, Inf), right = FALSE)
fit &lt;- glm(Denied ~ CLTV + FICO, data = myData, family=binomial())
</code></pre>

<p>Results are something like this:</p>

<pre><code>Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.53831  -0.77944  -0.62487   0.00027   2.09771  

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)     -1.33630    0.23250  -5.747 9.06e-09 ***
CLTV(70,80]     -0.54961    0.34864  -1.576 0.114930    
CLTV(80,90]     -0.51413    0.31230  -1.646 0.099715 .  
CLTV(90,95]     -0.74648    0.37221  -2.006 0.044904 *  
CLTV(95,100]     0.38370    0.37709   1.018 0.308906    
CLTV(100,125]   -0.01554    0.25187  -0.062 0.950792    
CLTV(125,Inf]   18.49557  443.55550   0.042 0.966739    
FICO[0,660)     19.64884 3956.18034   0.005 0.996037    
FICO[660,680)    1.77008    0.47653   3.715 0.000204 ***
FICO[680,700)    0.98575    0.30859   3.194 0.001402 ** 
FICO[700,720)    1.31767    0.27166   4.850 1.23e-06 ***
FICO[720,740)    0.62720    0.29819   2.103 0.035434 *  
FICO[740,780)    0.31605    0.23369   1.352 0.176236    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1037.43  on 810  degrees of freedom
Residual deviance:  803.88  on 798  degrees of freedom
AIC: 829.88

Number of Fisher Scoring iterations: 16
</code></pre>

<p>FICO in the range [0, 660) and CLTV in the range (125, Inf] indeed always results in Denial = 1, so their coefficients are very large, but why are they also ""insignificant""?</p>
"
"0.0601929265428846","0.0608580619450185","110148","<p>I have run a <code>multinomial logistic regression</code> test for the interaction between species of deer, days a camera trap was in the field and type of reaction. </p>

<p>The model with the best AIC value was: </p>

<pre><code>Coefficients:
   (Intercept) speciesmuntjac   speciesroe speciessika        days
r   -0.7471023      0.6263753 -0.005967869 -0.74253017 -0.05189515
sr   0.6909319      0.5552278 -0.355611180 -0.01622306 -0.03178001

Residual Deviance: 971.6464 
AIC: 991.6464 
</code></pre>

<p>But I also got the following </p>

<pre><code>    (Intercept) speciesmuntjac speciesroe speciessika       days
r  0.0071741793     0.17599897  0.9865350   0.2095687 0.08669276
sr 0.0001402185     0.08536257  0.1331651   0.9574258 0.08829020
</code></pre>

<p>I'm just wondering if anyone knows why this is considered the best model when neither days nor deer react (r) or strongly react (sr) higher than chance?</p>
"
"0.0851256530758749","0.086066296582387","110155","<p>I have run a multinomial logistic regression test for the interaction between species of deer, days a camera trap was in the field and type of reaction.</p>

<p>The model with the best AIC value was:</p>

<pre><code>Coefficients:
   (Intercept) speciesmuntjac   speciesroe speciessika        days
r   -0.7471023      0.6263753 -0.005967869 -0.74253017 -0.05189515
sr   0.6909319      0.5552278 -0.355611180 -0.01622306 -0.03178001

Residual Deviance: 971.6464 
AIC: 991.6464 
</code></pre>

<p>But I also got the following</p>

<pre><code>    (Intercept) speciesmuntjac speciesroe speciessika       days
r  0.0071741793     0.17599897  0.9865350   0.2095687 0.08669276
sr 0.0001402185     0.08536257  0.1331651   0.9574258 0.08829020
</code></pre>

<p>I'm just wondering if anyone would know what the interaction between species and days is in relation to r (reaction) and sr (strong reaction)? I know I can't reject the null hypothesis that there is no effect of species or days on r or sr but beyond that I'm lost!</p>
"
"0.28259708263022","0.298142396999972","112241","<p><strong>Summary:</strong> Is there any statistical theory to support the use of the $t$-distribution (with degrees of freedom based on the residual deviance) for tests of logistic regression coefficients, rather than the standard normal distribution?</p>

<hr>

<p>Some time ago I discovered that when fitting a logistic regression model in SAS PROC GLIMMIX, under the default settings, the logistic regression coefficients are tested using a $t$ distribution rather than the standard normal distribution.$^1$ That is, GLIMMIX reports a column with the ratio $\hat{\beta}_1/\sqrt{\text{var}(\hat{\beta}_1)}$ (which I will call $z$ in the rest of this question), but also reports a ""degrees of freedom"" column, as well as a $p$-value based on assuming a $t$ distribution for $z$ with degrees of freedom based on the residual deviance -- that is, degrees of freedom = total number of observations minus number of parameters. At the bottom of this question I provide some code and output in R and SAS for demonstration and comparison.$^2$</p>

<p>This confused me, since I thought that for generalized linear models such as logistic regression, there was no statistical theory to support the use of the $t$-distribution in this case. Instead I thought what we knew about this case was that</p>

<ul>
<li>$z$ is ""approximately"" normally distributed;</li>
<li>this approximation might be poor for small sample sizes;</li>
<li>nevertheless it <em>cannot</em> be assumed that $z$ has a $t$ distribution like we can assume in the case of normal regression.</li>
</ul>

<p>Now, on an intuitive level, it does seem reasonable to me that if $z$ is approximately normally distributed, it might in fact have some distribution that is basically ""$t$-like"", even if it is not exactly $t$. So the use of the $t$ distribution here does not seem crazy. But what I want to know is the following:</p>

<ol>
<li>Is there in fact statistical theory showing that $z$ really does follow a $t$ distribution in the case of logistic regression and/or other generalized linear models?</li>
<li>If there is no such theory, are there at least papers out there showing that assuming a $t$ distribution in this way works as well as, or maybe even better than, assuming a normal distribution?</li>
</ol>

<p>More generally, is there any actual support for what GLIMMIX is doing here other than the intuition that it is probably basically sensible?</p>

<p>R code:</p>

<pre><code>summary(glm(y ~ x, data=dat, family=binomial))
</code></pre>

<p>R output:</p>

<pre><code>Call:
glm(formula = y ~ x, family = binomial, data = dat)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-1.352  -1.243   1.025   1.068   1.156  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.22800    0.06725   3.390 0.000698 ***
x           -0.17966    0.10841  -1.657 0.097462 .  
---
  Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1235.6  on 899  degrees of freedom
Residual deviance: 1232.9  on 898  degrees of freedom
AIC: 1236.9

Number of Fisher Scoring iterations: 4
</code></pre>

<p>SAS code:</p>

<pre><code>proc glimmix data=logitDat;
    model y(event='1') = x / dist=binomial solution;
run;
</code></pre>

<p>SAS output (edited/abbreviated):</p>

<pre><code>The GLIMMIX Procedure

               Fit Statistics

-2 Log Likelihood            1232.87
AIC  (smaller is better)     1236.87
AICC (smaller is better)     1236.88
BIC  (smaller is better)     1246.47
CAIC (smaller is better)     1248.47
HQIC (smaller is better)     1240.54
Pearson Chi-Square            900.08
Pearson Chi-Square / DF         1.00


                       Parameter Estimates

                         Standard
Effect       Estimate       Error       DF    t Value    Pr &gt; |t|

Intercept      0.2280     0.06725      898       3.39      0.0007
x             -0.1797      0.1084      898      -1.66      0.0978
</code></pre>

<p>$^1$Actually I first noticed this about <em>mixed-effects</em> logistic regression models in PROC GLIMMIX, and later discovered that GLIMMIX also does this with ""vanilla"" logistic regression.</p>

<p>$^2$I do understand that in the example shown below, with 900 observations, the distinction here probably makes no practical difference. That is not really my point. This is just data that I quickly made up and chose 900 because it is a handsome number. However I do wonder a little about the practical differences with small sample sizes, e.g. $n$ &lt; 30.</p>
"
"0.148969892882781","0.172132593164774","114022","<p>For a machine learning class I am taking, on our first homework assignment we are given the following problem that has me stuck:</p>

<pre><code>Consider the following simulated data set:

set.seed(123)
n&lt;-100
X&lt;-runif(n)
Y&lt;-rbinom(n,1,exp(0.5+X)/(1+exp(0.5+X)))

a) Find the Bayes' classifier
b) Construct an empirical version of the Bayes' classifier using MLE (you can use 
the glm function)
</code></pre>

<p>I don't understand how to find the Bayes' classifier using R. I can find it algebraically, but how do you implement Bayes' classifiers in R? When I search around, the only sources I can find are on ""Naive Bayes' classifiers"", which don't appear to be the same thing.</p>

<p>This:</p>

<p><a href=""http://en.wikipedia.org/wiki/Bayes_classifier"" rel=""nofollow"">http://en.wikipedia.org/wiki/Bayes_classifier</a></p>

<p>Is the Bayes' classifier I want to find, but I can't find any sources on it for R. </p>

<p>Further, even if I did know how to find the Bayes' classifier, I don't understand what the difference would be between finding it and constructing an empirical version using MLE. The question doesn't even make sense to me. How do I use the glm function to use MLE to construct a classifier? I imagine it has something to do with fitting a logistic model, but I don't understand how to use the glm function in the way I am being asked to? I suspect I might just be getting caught up in the terminology/notation and confusing myself unnecessarily.</p>

<p>Anyone have any pointers for how to get started on this? I'm not asking anyone to code it for me, but it would be nice if someone could point me in the right direction.</p>
"
"0.199637352376173","0.201843356939833","115188","<p>I am trying to look at whether 2 variables (one dichotomous categorical and one continuous) predict the occurrence of a dichotomous categorical dependent variable.</p>

<pre><code>dependent variable is LENIpos - 0 = no event, 1 = event
predictor variables are Hip.Prox.Femur - 0 = no hip fracture, 1 = hip fracture
                and     age (continuous)
</code></pre>

<p>Both predictor variables have significant p values in separate chi square test and Mann Whitney U test respectively.</p>

<p>When I run a logistic regression <code>glm(LENIpos ~ age + Hip.Prox.Femur, family = ""binomial)</code>, the variables come out as not significant. (1)</p>

<p>However, when I run the logistic regression with interactions <code>glm(LENIpos ~ age * Hip.Prox.Femur...)</code> (2), they are no both significant.  How is this to be interpreted?</p>

<p>Example R outputs:</p>

<p>(1)</p>

<pre><code>Call: glm(formula = LENIpos ~ age + Hip.Prox.Fem, family = ""binomial"", 
    data = dvt)

Deviance Residuals: 
     Min       1Q   Median       3Q      Max  
 -0.9346  -0.7826  -0.4952  -0.3374   2.1897  

Coefficients:
                         Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)              -3.46888    1.00693  -3.445 0.000571 ***
age                       0.02122    0.01519   1.397 0.162535  
Hip.Prox.Femhip fracture  0.72410    0.57790   1.253 0.210212    

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 145.23  on 151  degrees of freedom
Residual deviance: 135.48  on 149  degrees of freedom
AIC: 141.48

Number of Fisher Scoring iterations: 5
</code></pre>

<p>(2)</p>

<pre><code>glm(formula = LENIpos ~ age * Hip.Prox.Fem, family = ""binomial"", 
    data = dvt)

Deviance Residuals: 
        Min       1Q   Median       3Q      Max  
    -1.0364  -0.7815  -0.5373  -0.1761   2.3443  

Coefficients:
                             Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)                  -5.89984    1.98289  -2.975  0.00293 **
age                           0.05851    0.02818   2.076  0.03788 * 
Hip.Prox.Femhip fracture      5.04990    2.46269   2.051  0.04031 * 
age:Hip.Prox.Femhip fracture -0.06058    0.03339  -1.814  0.06965 . 


(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 145.23  on 151  degrees of freedom
Residual deviance: 131.82  on 148  degrees of freedom
AIC: 139.82

Number of Fisher Scoring iterations: 6
</code></pre>
"
"0.191138212885652","0.210818510677892","115356","<p>I'm a beginner in statistics and I have to run multilevel logistic regressions. I am confused with the results as they differ from logistic regression with just one level. </p>

<p>I don't know how to interpret the variance and correlation of the random variables. And I wonder how to compute the ICC.</p>

<p>For example : I have a dependent variable about the protection friendship ties give to individuals (1 is for individuals who can rely a lot on their friends, 0 is for the others). There are 50 geographic clusters of respondant and one random variable which is a factor about the social situation of the neighborhood. Upper/middle class is the reference, the other modalities are working class and underprivileged neighborhoods. </p>

<p>I get these results :</p>

<pre><code>&gt; summary(RLM3)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: Arp ~ Densite2 + Sexe + Age + Etudes + pcs1 + Enfants + Origine3 +      Sante + Religion + LPO + Sexe * Enfants + Rev + (1 + Strate |  
    Quartier)
   Data: LPE
Weights: PONDERATION
Control: glmerControl(optimizer = ""bobyqa"")

     AIC      BIC   logLik deviance df.resid 
  3389.9   3538.3  -1669.9   3339.9     2778 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.2216 -0.7573 -0.3601  0.8794  2.7833 

Random effects:
 Groups   Name           Variance Std.Dev. Corr       
 Neighb. (Intercept)     0.2021   0.4495              
          Working Cl.    0.2021   0.4495   -1.00      
          Underpriv.     0.2021   0.4495   -1.00  1.00
Number of obs: 2803, groups:  Neigh., 50

Fixed effects:
</code></pre>

<p>The differences with the ""call"" part is due to the fact I translated some words.</p>

<p>I think I understand the relation between the random intercept and the random slope for linear regressions but it is more difficult for logistics ones. I guess that when the correlation is positive, I can conclude that the type of neighborhood (social context) has a positive impact on the protectiveness of friendship ties, and conversely. But how do I quantify that ?</p>

<p>Moreover, I find it odd to get correlation of 1 or -1 and nothing more intermediate.</p>

<p>As for the ICC I am puzzled because I have seen a post about lmer regression that indicates that intraclass correlation can be computed by dividing the variance of the random intercept by the variance of the random intercept, plus the variance the random variables, plus the residuals. </p>

<p>But there are no residuals in the results of a glmer. I have read in a book that ICC must be computed by dividing the random intercept variance by the random intercept variance plus 2.36 (piÂ²/3). But in another book, 2.36 was replaced by the inter-group variance (the first level variance I guess). 
What is the good solution ?</p>

<p>I hope these questions are not too confused.
Thank you for your attention !</p>
"
"0.127688479613812","0.129099444873581","117450","<p>I have a confusing situation where I have strongly conflicting results from two ways of analyzing my simple data. I measure two binary variables from each participant, AestheticOnly and ChoiceVA. I want to know if AestheticOnly depends on ChoiceVA and whether this relation is different in two different experiments. Here is my participant count data:</p>

<pre><code>Experiment 1
                 AestheticOnly
                 0   1  All
ChoiceVA A      35   6   41
         V      20  13   33
         All    55  19   74

Experiment 2
                 AestheticOnly
                 0   1  All
ChoiceVA A      12  10   22
         V      31  11   42
         All    43  21   64
</code></pre>

<p>I run a logistic regression where AestheticOnly is modelled by ChoiceVA, Experiment, and the interaction:</p>

<pre><code>&gt; mod &lt;- glm( AestheticOnly ~ ChoiceVA*Experiment, data = d, family=binomial)
&gt; summary(mod)

Call:
glm(formula = AestheticOnly ~ ChoiceVA * Experiment, family = binomial, 
    data = d)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.1010  -0.7793  -0.5625   1.2557   1.9605  

Coefficients:
                     Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)           -3.3449     0.9820  -3.406 0.000659 ***
ChoiceVAV              3.5194     1.2630   2.787 0.005327 ** 
Experiment             1.5813     0.6153   2.570 0.010170 *  
ChoiceVAV:Experiment  -2.1866     0.7929  -2.758 0.005820 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 166.16  on 137  degrees of freedom
Residual deviance: 157.01  on 134  degrees of freedom
AIC: 165.01

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Apparently all factors are significant. But, this just doesn't make sense to me. For example, looking at the main effect of experiment should be equivalent to performing a Fisher's Exact test comparing 55 and 19 with 43 and 21 (bottom lines of each table). This is obviously not significant (p=.452). So why does the regression model give such a different result? Any help much appreciated.</p>
"
"0.181488502160157","0.183493960854393","117593","<p>This is a really simple problem I am having, yet for the life of me I can't find a solution searching around. In theory I can simply recode the data, but that is an extreme solution I would rather not use if I don't have to. </p>

<p>I am simply trying to do a logistic regression with an ordered factor as my predictor. For a toy data set, consider:</p>

<pre><code>  radiation leukemia other total
1         0       13   378   391
2       1-9        5   200   205
3     10-49        5   151   156
4     50-99        3    47    50
5   100-199        4    31    35
6       200       18    33    51
</code></pre>

<p>I want to execute the following:</p>

<pre><code>glm(cbind(leukemia,other)~radiation,data=leuk,family=binomial(""logit""))
</code></pre>

<p>That is, leukemia are the ""successes"" and other are the ""failures"". Basically, trying to predict dose-response relationship between radiation and the proportional mortality rates for leukemia. However, this model is oversaturated:</p>

<pre><code>Call:  glm(formula = cbind(leukemia, other) ~ radiation, family = binomial(""logit""), 
    data = leuk)

Coefficients:
     (Intercept)      radiation1-9    radiation10-49  radiation100-199  
         -3.3699           -0.3189           -0.0379            1.3223  
    radiation200    radiation50-99  
          2.7638            0.6184  

Degrees of Freedom: 5 Total (i.e. Null);  0 Residual
Null Deviance:      54.35 
Residual Deviance: -3.331e-15   AIC: 33.67
</code></pre>

<p>I don't want each level of radiation as a factor to be its own predictor variable; that makes no sense, especially when you only have a small number of data points (note, this isn't actually the real data I am using, this is just a toy example that is similar). In any case, how do I force R to simply consider the factor radiation as a single variable with multiple levels? For example, if I do the following:</p>

<pre><code>x&lt;-c(0,1,2,3,4,5)
glm(cbind(leukemia,other)~x,data=leuk,family=binomial(""logit""))

Call:  glm(formula = cbind(leukemia, other) ~ x, family = binomial(""logit""), 
    data = leuk)

Coefficients:
(Intercept)            x  
    -3.9116       0.5731  

Degrees of Freedom: 5 Total (i.e. Null);  4 Residual
Null Deviance:      54.35 
Residual Deviance: 10.18        AIC: 35.84
</code></pre>

<p>This is more in line with what I want. But I am nervous about using that x variable in the regression for fear of changing the interpretation of the results. Similarly, I'd prefer to avoid an irritating system of dummy variables. </p>

<p>How do I go about doing this? Or is there a better workaround altogether for studying this type of relationship that I am not considering?</p>
"
"0.173762011714229","0.175682092231577","117631","<p>I measure two binary responses from each participant (ChoiceVA = V or A, AestheticOnly = 0 or 1). There are two experiments (between-participant). I want to test the following hypotheses:</p>

<p>AestheticOnly depends on Experiment (main effect)
AestheticOnly depends on ChoiceVA (main effect)
The way AestheticOnly depends on Experiment depends on ChoiceVA (interaction)</p>

<p>Here is my data. The first number in each cell is the proportion of participants scoring 1 for AestheticOnly, and the second number is the n for participants in that cell.</p>

<pre><code>                         ChoiceVA               
                        A       V     All

Experiment  1      0.1463  0.3939  0.2568
                       41      33      74

            2      0.4545  0.2619  0.3281
                       22      42      64

            All    0.2540  0.3200  0.2899
                       63      75     138
</code></pre>

<p>Just from looking at the data it is pretty obvious that neither main effect is significant (e.g. for ChoiceVA, bottom row, .25 of 63 participants is not significantly different from .32 of 75 participants). In my naivity I thought perhaps I could test these hypotheses with a straightforward binary logistic regression:</p>

<pre><code>&gt; mod &lt;- glm( AestheticOnly ~ Experiment+ChoiceVA+Experiment*ChoiceVA, data = d, family=binomial )
&gt; summary(mod)

Call:
glm(formula = AestheticOnly ~ Experiment + ChoiceVA + Experiment *
    ChoiceVA, family = binomial, data = d)

Deviance Residuals:
    Min       1Q   Median       3Q      Max 
-1.1010  -0.7793  -0.5625   1.2557   1.9605 

Coefficients:
                      Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)            -1.7636     0.4419  -3.991 6.57e-05 ***
Experiment2             1.5813     0.6153   2.570  0.01017 * 
ChoiceVAV               1.3328     0.5676   2.348  0.01887 * 
Experiment2:ChoiceVAV  -2.1866     0.7929  -2.758  0.00582 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 166.16  on 137  degrees of freedom
Residual deviance: 157.01  on 134  degrees of freedom
AIC: 165.01

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Clearly, the main effects are not being tested here in the way I hoped. I believe that this model, in testing main effects, rather than testing e.g. ChoiceVA=A against ChoiceVA=V across both levels of Experiment, is confining itself to that comparison only when Experiment=1. Can a model be constructed that instead tests the main effects in the way I would like?</p>

<p>This is related to a previous question (<a href=""http://stats.stackexchange.com/questions/117450/logistic-regression-gives-very-different-result-to-fishers-exact-test-why"">Logistic regression gives very different result to Fisher&#39;s exact test - why?</a>), but when I asked it I understand this even worse than I do now and consequently the question was so unclear that I need to start again.</p>
"
"0.159255514317652","0.161015297179883","124616","<p>I am testing the logistic regression classifier in R. I created some test data like this:</p>

<pre><code>x=runif(10000)
y=runif(10000)
df=data.frame(x,y,as.factor(x-y&gt;0))
</code></pre>

<p>basically I am sampling the 2D unit square [0,1] and classifying a point belonging to class A or B depending on which side of y=x it lies.</p>

<p>I generated a scatter plot of the data like below:</p>

<pre><code>names(df) = c(""feature1"", ""feature2"", ""class"")
levels=levels(df[[3]])
obs1=as.matrix(subset(df,class==levels[[1]])[,1:2])
obs2=as.matrix(subset(df,class==levels[[2]])[,1:2])
# make scatter plot
dev.new()
plot(obs1[,1],obs1[,2],xlab=""x"",ylab=""y"",main=""scatter plot"",pch=0,col=colors[[1]])
points(obs2[,1],obs2[,2],xlab=""x"",ylab=""y"",main=""scatter plot"",pch=1,col=colors[[2]])
</code></pre>

<p>it gives me below graph:</p>

<p><img src=""http://i.stack.imgur.com/5zN4y.png"" alt=""scatter plot""></p>

<p>Now I tried running LR (logistic regression) on this data using code below:</p>

<pre><code>model=glm(class~.,family=""binomial"",data=df)
summary(model) # prints summary
</code></pre>

<p>here are the results:</p>

<pre><code>Call:
glm(formula = class ~ ., family = ""binomial"", data = df)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-0.11832   0.00000   0.00000   0.00000   0.08847  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)  5.765e-01  1.923e+01   0.030    0.976
feature1     9.761e+04  8.981e+04   1.087    0.277
feature2    -9.761e+04  8.981e+04  -1.087    0.277

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1.3863e+04  on 9999  degrees of freedom
Residual deviance: 2.9418e-02  on 9997  degrees of freedom
AIC: 6.0294

Number of Fisher Scoring iterations: 25
</code></pre>

<p>I also get these warning messages:</p>

<pre><code>Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: fitted probabilities numerically 0 or 1 occurred 
</code></pre>

<p>If I try plotting the ROC curve using a varying threshold, I get following graph (AUC=1 which is good):
<img src=""http://i.stack.imgur.com/xbyPX.png"" alt=""enter image description here""></p>

<p><strong>Could someone please explain why the algorithm does not converge and coefficient estimates are not statistically significant (high std. error in coeff estimates)?</strong></p>

<p>I also compared to LDA:</p>

<pre><code>lda_classifier=lda(class~., data=df)
</code></pre>

<p>gives:</p>

<pre><code>Call:
lda(class ~ ., data = df)

Prior probabilities of groups:
 FALSE   TRUE 
0.5007 0.4993 

Group means:
       feature1  feature2
FALSE 0.3346288 0.6676169
TRUE  0.6710111 0.3380432

Coefficients of linear discriminants:
               LD1
**feature1  4.280490
feature2 -4.196388**
</code></pre>
"
"0.345782037404062","0.32841489185126","127479","<p>I'm using a mixed effects model with logistic link function (using lme4 version 1.1-7 in R).  However, I noticed that the estimates of significance for fixed effects change depending on the order of the rows in the dataset.  </p>

<p>That is, if I run a model on a dataset, I get certain estimate for my fixed effect and it has a certain p-value.  I run the model again, and I get the same estimate and p-value.  Now, I shuffle the order of rows (the data is not mixed, just the rows are in a different order).  Running the model a third time, the p-value is very different.</p>

<p>For the data I have, the estimated p-value for the fixed effect can be between p=0.001 and p=0.08.  Obviously, these are crucial differences given conventional significance levels. </p>

<p>I understand that the estimates are just estimated, and there will be differences between values for a number of reasons.  However, the magnitude of the differences for my data seem large to me, and I wouldn't expect the order of my dataframe to have this effect (we discovered this problem by chance when a colleague ran the same model but got different results.  It turned out they had ordered their data frame.).  </p>

<p>Here is the output of my script:
(X and Y are binary variables which are contrast-coded and centred, Group and SubGroup are categorical variables)</p>

<pre><code>&gt; # Fit model
&gt; m1 = glmer(X ~Y+(1+Y|Group)+(1+Y|SubGroup),family=binomial(link='logit'),data=d)
&gt; # Shuffle order of rows
&gt; d = d[sample(1:nrow(d)),]
&gt; # Fit model again
&gt; m2 = glmer(X ~Y+(1+Y|Group)+(1+Y|SubGroup),family=binomial(link='logit'),data=d)
&gt; summary(m1)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
      ['glmerMod']
 Family: binomial  ( logit )
Formula: X ~ Y + (1 + Y | Group) + (1 + Y | SubGroup)
   Data: d

      AIC       BIC    logLik  deviance  df.resid 
 200692.0  200773.2 -100338.0  200676.0    189910 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.1368 -0.5852 -0.4873 -0.1599  6.2540 

Random effects:
 Groups       Name        Variance Std.Dev. Corr 
 SubGroup     (Intercept) 0.2939   0.5421        
              Y1          0.1847   0.4298   -0.79
 Group        (Intercept) 0.2829   0.5319        
              Y1          0.4640   0.6812   -0.07
Number of obs: 189918, groups:  SubGroup, 15; Group, 12

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.0886     0.1325  -8.214   &lt;2e-16 ***
Y1            0.3772     0.2123   1.777   0.0756 .  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
     (Intr)
Y1 0.112 
&gt;
&gt; # -----------------
&gt; summary(m2)
Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) 
      ['glmerMod']
 Family: binomial  ( logit )
Formula: X ~ Y + (1 + Y | Group) + (1 + Y | SubGroup)
   Data: d

      AIC       BIC    logLik  deviance  df.resid 
 200692.0  200773.2 -100338.0  200676.0    189910 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-1.1368 -0.5852 -0.4873 -0.1599  6.2540 

Random effects:
 Groups       Name        Variance Std.Dev. Corr 
 SubGroup     (Intercept) 0.2939   0.5422        
              Y1          0.1846   0.4296   -0.79
 Group        (Intercept) 0.2829   0.5318        
              Y1          0.4641   0.6813   -0.07
Number of obs: 189918, groups:  SubGroup, 15; Group, 12

Fixed effects:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  -1.0886     0.1166  -9.334  &lt; 2e-16 ***
Y1            0.3773     0.1130   3.339 0.000841 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
     (Intr)
Y1 0.074 
</code></pre>

<p>I'm afraid that I can't attach the data due to privacy reasons. </p>

<p>Both models converge.  The difference appears to be in the standard errors, while the differences in coefficient estimates are smaller.  The model fit (AIC etc.) are the same, so maybe there are multiple optimal convergences, and the order of the data pushes the optimiser into different ones.  However, I get slightly different estimates every time I shuffle the data frame (not just two or three unique estimates).  In one case (not shown above), the model did not converge simply because of a shuffling of the rows.</p>

<p>I suspect that the problem lies with the structure of my particular data.  It's reasonably large (nearly 200,000 cases), and has nested random effects.  I have tried centering the data, using contrast coding and feeding starting values to lmer based on a previous fit.  This seems to help somewhat, but I still get reasonably large differences in p-values.  I also tried using different ways of calculating p-values, but I got the same problem.</p>

<p>Below, I've tried to replicate this problem with synthesised data.  The differences here aren't as big as with my real data, but it gives an idea of the problem.</p>

<pre><code>library(lme4)
set.seed(999)

# make a somewhat complex data frame
x = c(rnorm(10000),rnorm(10000,0.1))
x = sample(x)
y = jitter(x,amount=10)
a = rep(1:20,length.out=length(x))
y[a==1] = jitter(y[a==1],amount=3)
y[a==2] = jitter(x[a==2],amount=1)
y[a&gt;3 &amp; a&lt;6] = rnorm(sum(a&gt;3 &amp; a&lt;6))
# convert to binary variables
y = y &gt;0
x = x &gt;0
# make a data frame
d = data.frame(x1=x,y1=y,a1=a)

# run model 
m1 = glmer(x1~y1+(1+y1|a1),data=d,family=binomial(link='logit'))

# shuffle order of rows
d = d[sample(nrow(d)),]

# run model again
m2 = glmer(x1~y1+(1+y1|a1),data=d,family=binomial(link='logit'))

# show output
summary(m1)
summary(m2)
</code></pre>

<p>One solution to this is to run the model multiple times with different row orders, and report the range of p-values.  However, this seems inelegant and potentially quite confusing.</p>

<p>The problem does not affect model comparison estimates (using anova), since these are based on differences in model fit.  The fixed effect coefficient estimates are also reasonably robust.  Therefore, I could just report the effect size, confidence intervals and the p-value from a model comparison with a null model, rather than the p-values from within the main model.</p>

<p>Anyway, has anyone else had this problem?  Any advice on how to proceed?</p>
"
"0.262374883916401","0.251312344975017","130313","<p>In a logistic Generalized Linear Mixed Model (family = binomial), I don't know how to interpret the random effects variance:</p>

<pre><code>Random effects:
 Groups   Name        Variance Std.Dev.
 HOSPITAL (Intercept) 0.4295   0.6554  
Number of obs: 2275, groups: HOSPITAL, 14
</code></pre>

<p>How do I interpret this numerical result?</p>

<p>I have a sample of renal trasplanted patients in a multicenter study. I was testing if the probability of a patient being treated with a specific antihypertensive treatment is the same among centers. The proportion of patients treated varies greatly between centers, but may be due to differences in basal characteristics of the patients. So I estimated a generalized linear mixed model (logistic), adjusting for the principal features of the patiens.
This are the results:</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood ['glmerMod']
 Family: binomial ( logit )
Formula: HTATTO ~ AGE + SEX + BMI + INMUNOTTO + log(SCR) + log(PROTEINUR) + (1 | CENTER) 
   Data: DATOS 

     AIC      BIC   logLik deviance 
1815.888 1867.456 -898.944 1797.888 

Random effects:
 Groups   Name        Variance Std.Dev.
 CENTER (Intercept) 0.4295   0.6554  
Number of obs: 2275, groups: HOSPITAL, 14

Fixed effects:
                           Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)               -1.804469   0.216661  -8.329  &lt; 2e-16 ***
AGE                       -0.007282   0.004773  -1.526  0.12712    
SEXFemale                 -0.127849   0.134732  -0.949  0.34267    
BMI                        0.015358   0.014521   1.058  0.29021    
INMUNOTTOB                 0.031134   0.142988   0.218  0.82763    
INMUNOTTOC                -0.152468   0.317454  -0.480  0.63102    
log(SCR)                   0.001744   0.195482   0.009  0.99288    
log(PROTEINUR)             0.253084   0.088111   2.872  0.00407 ** 
</code></pre>

<p>The quantitative variables are centered.
I know that the among-hospital standard deviation of the intercept is 0.6554, in log-odds scale.
Because the intercept is -1.804469, in log-odds scale, then probability of being treated with the antihypertensive of a man, of average age, with average value in all variables and inmuno treatment A, for an ""average"" center, is 14.1 %.
And now begins the interpretation:  under the assumption that the random effects follow a normal distribution, we would expect approximately 95% of centers to have a value within 2 standard deviations of the mean of zero, so the probability of being treated for the average man will vary between centers with coverage interval of:</p>

<pre><code>exp(-1.804469-2*0.6554)/(1+exp(-1.804469-2*0.6554))

exp(-1.804469+2*0.6554)/(1+exp(-1.804469+2*0.6554))
</code></pre>

<p>Is this correct?</p>

<p>Also, how can I test in glmer if the variability between centers is statistically significant?
I used to work with MIXNO, an excellent software of Donald Hedeker, and there I have an standard error of the estimate variance, that I don't have in glmer.
How can I have the probability of being treated for the ""average"" man in each center, with a confidene interval?</p>

<p>Thanks</p>
"
"0.159255514317652","0.161015297179883","131331","<p>This is the first time I am posting a question, so please excuse any etiquette violations and poorly worded questions!</p>

<p>I am working on the analysis for a chapter of my thesis. I am examining the behavioural response of an animal to a visual stimulus, and trying to determine which of eight explanatory variables (and their two-way interactions) affect this response. I recorded the response on an ordinal scale of 0 (no response), 1 (attention to but no avoidance of stimulus) or 2 (escape response to stimulus). I am leaning towards collapsing categories and using logistic regression where a 1 is an escape response, and 0 is anything else because logistic regression seems much easier to interpret. </p>

<p>I have 794 observations. I am including observer and location (because field sites differed) as random effects, although I am unsure this is a good approach. </p>

<p>I am having trouble with model selection. I ran all possible subsets using the dredge function in packing 'MuMIn'. I thought I was avoiding data dredging by </p>

<ul>
<li>including main effects which were selected because I thought they would have an effect (rather than all conceivable variables)</li>
<li>including only the two-way interactions of interest (R will not run if the global model includes all possible two-way interactions because of the huge number of terms/models)</li>
</ul>

<p>I've come to realise that the second point may be problematic because it leads to an unbalanced model set as in Burnham and Anderson (2002). </p>

<blockquote>
  <p>Page 169: When assessing the relative importance of variables using sums of the AIC    weights, it is important to achieve a balance in the number of models that contain each variable j.</p>
</blockquote>

<p>My questions are</p>

<ol>
<li><p>Is it possible to have a balanced model set without it being considered data dredging? If so, how? </p></li>
<li><p>Is my approach at all reasonable? If not, are there other avenues I should explore? I started with Hosmer&amp;Lemeshow purposeful forward selection, as advocated by my supervisor, but I had some issues with this which I can elaborate on if necessary. </p></li>
</ol>
"
"0.159255514317652","0.161015297179883","135792","<p>I'm interested in building a set of candidate models in R for an analysis using logistic regression. Once I build the set of candidate models and evaluate their fit to the data using AICc (<code>aicc = dredge(results, eval=TRUE, rank=""AICc"")</code>), I would like to use k-fold cross fold validation to evaluate the predictability of the final model chosen from the analysis. I have a few questions associated to k-fold cross validation: </p>

<ol>
<li><p>I assume you use your entire data set for initially building your candidate set of models. For example, say I have 20,000 data values, wouldn't I first build my candidate set of models based on the entire 20,000 data values? Then do use AIC to rank the models and select the most parsimonious model?</p></li>
<li><p>After you select the final model (or model averaged model), would you then conduct a k-fold cross validation to evaluate the predictability of the model? </p></li>
<li><p>What is the easiest way to code a k-fold cross-validation in R? </p></li>
<li><p>Does the k-fold cross validation code break up your entire data set (e.g., 20,000 data values) into training and validation sets automatically? Or do you have to subset the data manually? </p></li>
</ol>
"
"0.148969892882781","0.172132593164774","135967","<p>I am a beginner in R. I am doing logistic regression using around 80 independent variables using <code>glm</code> function in R. The dependent variable is <code>churn</code> which says whether a customer churned or not. I want to know how to identify the right combination of variables to get a good predictive logistic regression model in R.  I also want to know how to identify the same for making good decision tree in R ( I am using the <code>ctree</code> function from the <code>party</code> package).
So far, I had used <code>drop1</code> function  and  <code>anova(LogMdl, test=""Chisq"")</code> where <code>LogMdl</code> is my logistic regression model to drop unwanted variables in the predictive model.  But maximum accuracy I was able to achieve was only 60%. </p>

<p>Also I am not sure if I am using the <code>drop1</code> and <code>anova</code> functions correctly. I dropped the variables with lowest AIC using <code>drop1</code> function.  Using <code>anova</code> function, I dropped variables with p value > 0.05</p>

<p>Kindly help me how to identify the right set of variables for both logistic regression and decision tree models to increase my model's predictive accuracy to close to 90% or more than that if possible.   </p>

<pre><code>library(party)
setwd(""D:/CIS/Project work"")
CellData &lt;- read.csv(""Cell2Cell_SPSS_Data - Orig.csv"")
trainData &lt;- subset(CellData,calibrat==""1"")
testData &lt;- subset(CellData,calibrat==""0"") # validation or test data set
LogMdl = glm(formula=churn ~ revenue  + mou    + recchrge+ directas+ 
               overage + roam    + changem +
               changer  +dropvce + blckvce + unansvce+ 
               custcare+ threeway+ mourec  +
               outcalls +incalls + peakvce + opeakvce+ 
               dropblk + callfwdv+ callwait+
               months  + uniqsubs+ actvsubs+  phones  + models  +
               eqpdays  +customer+ age1    + age2    + 
               children+ credita + creditaa+
               creditb  +creditc + creditde+ creditgy+ creditz + 
               prizmrur+ prizmub +
               prizmtwn +refurb  + webcap  + truck   + 
               rv      + occprof + occcler +
               occcrft  +occstud + occhmkr + occret  + 
               occself + ownrent + marryun +
               marryyes +marryno + mailord + mailres + 
               mailflag+ travel  + pcown   +
               creditcd +retcalls+ retaccpt+ newcelly+ newcelln+ 
               refer   + incmiss +
               income   +mcycle  + creditad+ setprcm + setprc  + retcall, 
               data=trainData, family=binomial(link=""logit""),
               control = list(maxit = 50))
ProbMdl = predict(LogMdl, testData, type = ""response"")
testData$churndep = rep(0,31047)  # replacing all churndep with zero
testData$churndep[ProbMdl&gt;0.5] = 1   # converting records with prob &gt; 0.5 as churned
table(testData$churndep,testData$churn)  # comparing predicted and actual churn
mean(testData$churndep!=testData$churn)    # prints the error %
</code></pre>

<p>Link for documentation of variables: <a href=""https://drive.google.com/file/d/0B9y78DHd3U-DZS05VndFV3A4Ylk/"" rel=""nofollow"">https://drive.google.com/file/d/0B9y78DHd3U-DZS05VndFV3A4Ylk/</a></p>

<p>Link for Dataset (.csv file) : 
<a href=""https://drive.google.com/file/d/0B9y78DHd3U-DYm9FOV9zYW15bHM/"" rel=""nofollow"">https://drive.google.com/file/d/0B9y78DHd3U-DYm9FOV9zYW15bHM/</a></p>

<p>I could not produce the output of <code>dput</code> since the data size is more than 5 MB. So I have zipped the file and placed in the above link. </p>

<p>Description of important variables:
* <code>churn</code> is the variable that says whether a customer churned or not.....
* <code>churndep</code> is the variable that needs to be predicted in the test data (validation data) and has to be compared with the <code>churn</code> variable which is already populated with actual churn.
For both churn and churndep, value of 1 means churned and 0 means not churned.</p>
"
"0.134595475514541","0.136082763487954","136040","<p>I'm implementing a logistic regression model in R and I have 80 variables to chose from. I need to automatize the process of variable selection of the model so I'm using the step function.</p>

<p>I've no problem using the function or finding the model, but when I look at the final model I find that some of the variables chosen by the step function are not significant (I look at this using the summary function and looking at the fourth column in $coef, this is the Wald Test). This is a problem because I need all the variables included in the model to be significant.</p>

<p>Is there any function or any way to get the best model based on AIC or BIC methods but that also consider that all the coefficients must be significant?
Thanks</p>
"
"0.217584455256073","0.235702260395516","139528","<p>When modelling continuous proportions (e.g. proportional vegetation cover at survey quadrats, or proportion of time engaged in an activity), logistic regression is considered inappropriate (e.g. <a href=""http://www.esajournals.org/doi/full/10.1890/10-0340.1"" rel=""nofollow"">Warton &amp; Hui (2011) The arcsine is asinine: the analysis of proportions in ecology</a>). Rather, OLS regression after logit-transforming the proportions, or perhaps beta regression, are more appropriate.</p>

<p>Under what conditions do the coefficient estimates of logit-linear regression and logistic regression differ when using R's <code>lm</code> and <code>glm</code>?</p>

<p>Take the following simulated dataset, where we can assume that <code>p</code> are our raw data (i.e. continuous proportions, rather than representing ${n_{successes}\over n_{trials}}$):</p>

<pre><code>set.seed(1)
x &lt;- rnorm(1000)
a &lt;- runif(1)
b &lt;- runif(1)
logit.p &lt;- a + b*x + rnorm(1000, 0, 0.2)
p &lt;- plogis(logit.p)

plot(p ~ x, ylim=c(0, 1))
</code></pre>

<p><img src=""http://i.stack.imgur.com/AzWOX.png"" alt=""enter image description here""></p>

<p>Fitting a logit-linear model, we obtain:</p>

<pre><code>summary(lm(logit.p ~ x))
## 
## Call:
## lm(formula = logit.p ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.64702 -0.13747 -0.00345  0.15077  0.73148 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.868148   0.006579   131.9   &lt;2e-16 ***
## x           0.967129   0.006360   152.1   &lt;2e-16 ***
## ---
## Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
## 
## Residual standard error: 0.208 on 998 degrees of freedom
## Multiple R-squared:  0.9586, Adjusted R-squared:  0.9586 
## F-statistic: 2.312e+04 on 1 and 998 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p>Logistic regression yields:</p>

<pre><code>summary(glm(p ~ x, family=binomial))
## 
## Call:
## glm(formula = p ~ x, family = binomial)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -0.32099  -0.05475   0.00066   0.05948   0.36307  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  0.86242    0.07684   11.22   &lt;2e-16 ***
## x            0.96128    0.08395   11.45   &lt;2e-16 ***
## ---
## Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 176.1082  on 999  degrees of freedom
## Residual deviance:   7.9899  on 998  degrees of freedom
## AIC: 701.71
## 
## Number of Fisher Scoring iterations: 5
## 
## Warning message:
## In eval(expr, envir, enclos) : non-integer #successes in a binomial glm!
</code></pre>

<p>Will the logistic regression coefficient estimates always be unbiased with respect to the logit-linear model's estimates?</p>
"
"0.0601929265428846","0.0608580619450185","140509","<p>I used logistic regression and found that my model fits well: </p>

<pre><code>Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.6434  -1.4623   0.8704   0.9013   1.0066  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   0.41595    0.02115   19.67   &lt;2e-16 ***
init_att_cnt  0.02115    0.00146   14.48   &lt;2e-16 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
(Dispersion parameter for binomial family taken to be 1)

Null deviance: 154956  on 122239  degrees of freedom
Residual deviance: 154746  on 122238  degrees of freedom
AIC: 154750
</code></pre>

<p>The chi-squared test is hightly statisticaly significant: <code>p = 9.642755e-48</code>. I decided to check the Nagelkerke $R^2$ statistic, </p>

<pre><code>R2 &lt;- R2/(1-exp((-mylogit$null.deviance)/n))
</code></pre>

<p>but it was $R^2 = 0.001350927$. This is unbelievable, why is $R^2$ so small, if my model fits well?</p>
"
"0.159255514317652","0.161015297179883","141603","<p>So I'm playing around with logistic regression in R, using the mtcars dataset, and I decide to create a logistic regression model on the 'am' parameter (that is manual or automatic transmission for those of you familiar with the mtcars-dataset).</p>

<pre><code>Call:
glm(formula = am ~ mpg + qsec + wt, family = binomial, data = mtcars)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-4.484e-05  -2.100e-08  -2.100e-08   2.100e-08   5.163e-05  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)    924.89  883764.07   0.001    0.999
mpg             20.65   18004.32   0.001    0.999
qsec           -55.75   32172.52  -0.002    0.999
wt            -111.33  103183.48  -0.001    0.999

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 4.3230e+01  on 31  degrees of freedom
Residual deviance: 6.2903e-09  on 28  degrees of freedom
AIC: 8

Number of Fisher Scoring iterations: 25
</code></pre>

<p>Now, at first sight this looks like a terrible regression, right? The standard errors are HUGE, the z-values are all close to zero and the corresponding probabilities are all close to one. HOWEVER, the residual deviance is extremely small! </p>

<p>I decide to check how well the model does as a classification model by running:</p>

<pre><code>pred &lt;- predict(logit_fit, data.frame(qsec = mtcars$qsec, wt = mtcars$wt, mpg = mtcars$mpg), type = ""response"") # Make a prediction of the probabilities on our data
mtcars$pred_r &lt;- round(pred, 0) # Round probabilities to closest 0 or 1
table(mtcars$am, mtcars$pred_r) # Check if results of classification is any good.
</code></pre>

<p>Indeed, the model perfectly predicts the data:</p>

<pre><code>     0  1
  0 19  0
  1  0 13
</code></pre>

<p>Have I completely misunderstood how to interpret model data? Am I overfitting massively or what's going on here? What's going on?</p>
"
"0.0601929265428846","0.0608580619450185","144603","<p>I have built a logistic regression where the outcome variable is being cured after receiving treatment (<code>Cure</code> vs. <code>No Cure</code>). All patients in this study received treatment. I am interested in seeing if having diabetes is associated with this outcome. </p>

<p>In R my logistic regression output looks as follows: </p>

<pre><code>Call:
glm(formula = Cure ~ Diabetes, family = binomial(link = ""logit""), data = All_patients)
...
Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   1.2735     0.1306   9.749   &lt;2e-16 ***
Diabetes     -0.5597     0.2813  -1.990   0.0466 *  
...
    Null deviance: 456.55  on 415  degrees of freedom
Residual deviance: 452.75  on 414  degrees of freedom
  (2 observations deleted due to missingness)
AIC: 456.75
</code></pre>

<p>However, the confidence interval for the odds ratio <strong>includes 1</strong>:</p>

<pre><code>                   OR     2.5 %   97.5 %
(Intercept) 3.5733333 2.7822031 4.646366
Diabetes    0.5713619 0.3316513 1.003167
</code></pre>

<p>When I do a chi-squared test on these data I get the following:</p>

<pre><code>data:  check
X-squared = 3.4397, df = 1, p-value = 0.06365
</code></pre>

<p>If you'd like to calculate it on your own the distribution of diabetes in the cured and uncured groups are as follows:</p>

<pre><code>Diabetic cure rate:      49 /  73 (67%)
Non-diabetic cure rate: 268 / 343 (78%)
</code></pre>

<p>My question is: Why don't the p-values and the confidence interval including 1 agree? </p>
"
"NaN","NaN","148648","<p>I'm planning to work on some credit risk models using logistic regresson in R. Binary response. What all goodness of fit tests are to be known and WHAT PACKAGES are required for the same? Thank you.
I found some help in this post but it is not exhaustive: <a href=""http://stats.stackexchange.com/questions/51275/what-is-the-difference-in-what-aic-and-c-statistic-auc-actually-measure-for-mo"">What is the difference in what AIC and c-statistic (AUC) actually measure for model fit?</a></p>
"
"0.0851256530758749","0.086066296582387","152268","<p>Can someone explain to me how you would know which variables to remove? And how do you know if something is accurate or not? Because when I plot an ROC curve the specificity/sensitivity curve, it looks like a square for a random forest.</p>

<p>If I use 2 different models, I assume I would have to use the same variables if I did stepAIC(logistic regression model) and then use those exact same variables for the random forest? </p>

<p>I have 3 unbalanced datasets that come from the same sample and I have to make something of it and I am not too sure where to start. Thanks!</p>
"
"0.225723474535817","0.243432247780074","154782","<p>I'm attempting logistic regression in R for a survey for 613 students. I'm looking to see if there is an association between my <strong>Dependent Variable</strong> (called 'BinaryShelter', coded as 0 or 1, signifying whether students took shelter during a tornado warning) and my <strong>5 independent/predictor variables</strong>. My categorical IV's have anywhere from 3 to 11 distinct levels/categories within them. The other two IV's are binary coded as 0 or 1. The first 10 surveys and R output are given below: </p>

<pre><code>    Survey  KSCat   WSCat   PlanHome    PlanWork    KLNKVulCat  BinaryShelter
    1       J       B       1           1           A           1
    2       A       B       1           0           NA          1
    3       B       B       1           1           C           1
    4       B       D       1           1           A           0
    5       B       D       1           1           A           1
    6       G       E       1           1           A           0
    7       A       A       1           1           B           1
    8       C       F       NA          1           C           0
    9       B       B       1           1           A           1
    10      C       B       0           0           NA          1



Call:
glm(formula = BinaryShelter ~ KSCat + WSCat + PlanHome + PlanWork + 
KLNKVulCat, family = binomial(""logit""), data = mydata)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.0583  -1.3564   0.7654   0.8475   1.6161  

Coefficients:
              Estimate   St. Error  z val   Pr(&gt;|z|)  
(Intercept)    0.98471    0.43416   2.268   0.0233 *
KSCatB        -0.63288    0.34599  -1.829   0.0674 .
KSCatC        -0.14549    0.27880  -0.522   0.6018  
KSCatD         0.59855    1.12845   0.530   0.5958  
KSCatE        15.02995 1028.08167   0.015   0.9883  
KSCatF         0.61015    0.68399   0.892   0.3724  
KSCatG        -1.60723    1.54174  -1.042   0.2972  
KSCatH        -1.57777    1.26621  -1.246   0.2127  
KSCatI        -2.06763    1.18469  -1.745   0.0809 .
KSCatJ        -0.23560    0.65723  -0.358   0.7200  
WSCatB        -0.30231    0.28752  -1.051   0.2931  
WSCatC        -0.49467    1.26400  -0.391   0.6955  
WSCatD         0.52501    0.71082   0.739   0.4601  
WSCatE        -0.32153    0.63091  -0.510   0.6103  
WSCatF        -0.51699    0.74680  -0.692   0.4888  
WSCatG        -0.64820    0.39537  -1.639   0.1011  
WSCatH        -0.05866    0.89820  -0.065   0.9479  
WSCatI       -17.07156 1455.39758  -0.012   0.9906  
WSCatJ       -16.31078  662.38939  -0.025   0.9804  
PlanHome       0.27095    0.28121   0.964   0.3353  
PlanWork       0.24983    0.24190   1.033   0.3017  
KLNKVulCatB    0.17280    0.42353   0.408   0.6833  
KLNKVulCatC   -0.12551    0.24777  -0.507   0.6125  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 534.16  on 432  degrees of freedom
Residual deviance: 502.31  on 410  degrees of freedom
  (180 observations deleted due to missingness)
AIC: 548.31

Number of Fisher Scoring iterations: 14

&gt; Anova(ShelterYorN, Test = ""LR"")
Analysis of Deviance Table (Type II tests)

Response: BinaryShelter
          LR Chisq Df Pr(&gt;Chisq)
KSCat       13.3351  9     0.1480
WSCat       14.3789  9     0.1095
PlanHome     0.9160  1     0.3385
PlanWork     1.0583  1     0.3036
KLNKVulCat   0.7145  2     0.6996
</code></pre>

<p>My questions are:</p>

<p><strong>1)</strong> Does a very large St. Deviation (like the one for KSCatE) indicate that I should not use that level of that categorical IV if I want the model to fit the data better? The ones that had such large St. Deviations were from small groups. Should I not include data from very small groups? For instance if only 2 or 3 people picked category 'E' for KSCat, should I exclude that data?</p>

<p><strong>2)</strong> When using factors for my categorical data, or when adding in more than one IV, sometimes my beta coefficients flip signs. Does this mean I should test for interaction and then try to conduct some form of a PCA or jump straight to doing a PCA?</p>

<p>These next questions may be better asked on stack overflow, but I figured I'd give it a shot here:</p>

<p><strong>3)</strong> I do not want a particular level of the categorical variables to be the reference level. I know that R automatically picks the reference level (A if letters, and the first one if numbers). As in the answer to this question (<a href=""http://stats.stackexchange.com/questions/60817/significance-of-categorical-predictor-in-logistic-regression"">Significance of categorical predictor in logistic regression</a>), I tried fitting the model without an intercept by adding - 1 to the formula to see all coefficients directly. But when I do this, the results only show the 'A' level of the first variable and none of the others. For example, I can see results for 'KSCatA' but not 'WSCatA' or 'KLNKVulCatA'. </p>

<p><strong>4)</strong> How does R handle missing observations for logistic regression? For example survey #10 was missing the 'KLNKVulCat' Variable, but not any of the other IV's. Would R or any other statistical languages not use any of the information for this particular person, or just that particular variable?</p>

<p>Any help is greatly appreciated, thank you.</p>
"
"0.134595475514541","0.136082763487954","160545","<p>I recently ran two tests in R - one using glm() and one using lm() with the goal being to test the relationship between a binary response and binary predictor.  I ran glm() first and got an estimate of -0.68 for the predictor coefficient which I thought was pretty good.  P&lt;.05 and AIC of 653.  </p>

<p>When I ran lm() however I got an estimate of -.14, a multiple r-squared of .008, P&lt;.05.  </p>

<p>My understanding is that linear regression is usually a poor choice for a categorical response compared with logistic regression, but when is this not the case? I noticed in this post <a href=""http://statisticalhorizons.com/linear-vs-logistic"" rel=""nofollow"">http://statisticalhorizons.com/linear-vs-logistic</a> that the author states there's middle ground where it does make sense to use linear regression.  Are there any common rules (or rules of thumb you personally use) that determine when to try out linear regression on a categorical response?  Do any of these differ from the author's cases?</p>
"
"0.147441956154897","0.149071198499986","164541","<p>I am attempting to do a logistic regression bootstrap with R. The problem is I get high SE's. I'm not sure what to do about this or what it means. Does it mean that bootstrap does not work well for my particular data? Here is my code:</p>

<pre><code>get.coeffic = function(data, indices){
  data    = data[indices,]
  mylogit = glm(F~B+D, data=data, family=""binomial"")
  return(mylogit$coefficients)
}

Call:
boot(data = Pres, statistic = logit.bootstrap, R = 1000)

Bootstrap Statistics :
       original      bias    std. error
t1* -10.8609610 -23.0604501  338.048398
t2*   0.2078474   0.4351766    6.387781
</code></pre>

<p>I also want to know that after bootstrapping, how would this help with my final regression model? That is, how do I find what regression coefficient do I use in my final model?</p>

<pre><code>&gt; fit &lt;- glm(F ~ B + D , data = President, family = ""binomial"")
&gt; summary(fit)
Call:
glm(formula = F ~ B + D, family = ""binomial"", data = President)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.7699  -0.5073   0.1791   0.8147   1.2836  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept) -14.57829    8.98809  -1.622   0.1048  
B             0.15034    0.14433   1.042   0.2976  
D             0.13385    0.08052   1.662   0.0965 .
- --
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1 

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 23.508  on 16  degrees of freedom
Residual deviance: 14.893  on 14  degrees of freedom
AIC: 20.893

Number of Fisher Scoring iterations: 5
</code></pre>
"
"NaN","NaN","167794","<p>I wrote a script that create a logistic model, for Email opening probability, for each user name.</p>

<pre><code>form&lt;-formula(OpenOrNor~as.factor(TimeSend)
              +OpenWithSmartphoneind)
models&lt;- dlply(Data, ""User_id"", 
               function(df) {
                 model&lt;-glm(formula = form,family = binomial(""logit""),data = df,control = glm.control(epsilon = 1e-9, maxit = 500))
                 return(model)})
</code></pre>

<p>for some users it return me </p>

<pre><code>Call:  glm(formula = form, family = binomial(""logit""), data = df, weights = HistoryWeights, 
    control = glm.control(epsilon = 0.000000001, maxit = 500))

Coefficients:
              (Intercept)            as.factor(TimeSend)2      as.factor(TimeSend)3   as.factor(TimeSend)4  
                -22.61106                   20.21853                    0.07738                   20.54737  
          as.factor(TimeSend)5       as.factor(TimeSend)6      as.factor(TimeSend)7   as.factor(TimeSend)9  
                  0.19292                   -0.03624                    0.22013                    0.11837  
          OpenWithSmartphoneind  
                       NA  

Degrees of Freedom: 83 Total (i.e. Null);  76 Residual
Null Deviance:      190.6 
Residual Deviance: 166.8    AIC: 182.8
</code></pre>

<p>We can see that for OpenWithSmartphoneind their  is NA. and this is because their are no Opening With Smartphone at all In this user history.</p>

<p>My question is how it will impact on predict?<br>
And doe's it make different if OpenWithSmartphoneind in the formula will be a factor type or not?</p>
"
"0.134595475514541","0.136082763487954","169291","<p>I have a logistic regression model below, predicting a dichotomous variable <em>type</em> from a single continuous predictor <em>fatigue</em>. Using the coefficients below I can obtain the increase in the odds of a positive <em>type</em> from a 1 unit increase in fatigue.</p>

<p>Also I believe by forming the model expression</p>

<pre><code>logit(type) = 0.3134 - 91.1171 * fatigue 
</code></pre>

<p>I can obtain the odds of a positive <em>type</em> for a given value of <em>fatigue</em> by plugging it in, say for a value <em>fatigue</em> = 1.</p>

<p><strong>However</strong>, what I want to do is to obtain the odds of a positive <em>type</em> for a range of <em>fatigue</em> values, i.e. <strong>&lt;= 0</strong>. Is this possible?</p>

<pre><code>## Call:
## glm(formula = type ~ fatigue, family = binomial(), data = myData)

## Deviance Residuals:
## Min 1Q Median 3Q Max
## -1.6703 -1.3104 0.8369 1.0049 1.4695
##
## Coefficients:
## Estimate Std. Error z value Pr(&gt;|z|)
## (Intercept) 0.3134 0.1496 2.095 0.0362 *
## fatigue -91.1171 36.3785 -2.505 0.0123 *
## ---
## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
##
## (Dispersion parameter for binomial family taken to be 1)
##
## Null deviance: 282.84 on 210 degrees of freedom
## Residual deviance: 276.03 on 209 degrees of freedom
## AIC: 280.03
##
## Number of Fisher Scoring iterations: 4
</code></pre>
"
"0.159255514317652","0.161015297179883","169438","<p>As we all know, there are 2 methods to evaluate the logistic regression model and 
they are testing very different things</p>

<ol>
<li><p>Predictive power:</p>

<p>Get a statistic that measures how well you can predict the dependent variable 
based on the independent variables. The well-know Pseudo R^2 are McFadden 
(1974) and Cox and Snell (1989).</p></li>
<li><p>Goodness-of-fit statistics</p>

<p>The test is telling whether you could do even better by making the model more 
complicated, which is actually testing whether there are any non-linearities or 
interactions.</p>

<p>I implemented both tests on my model, which added quadratic and interaction<br>
already: </p>

<pre><code>&gt;summary(spec_q2)

Call:
glm(formula = result ~ Top + Right + Left + Bottom + I(Top^2) + 
 I(Left^2) + I(Bottom^2) + Top:Right + Top:Bottom + Right:Left, 
 family = binomial())

 Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  0.955431   8.838584   0.108   0.9139    
Top          0.311891   0.189793   1.643   0.1003    
Right       -1.015460   0.502736  -2.020   0.0434 *  
Left        -0.962143   0.431534  -2.230   0.0258 *  
Bottom       0.198631   0.157242   1.263   0.2065    
I(Top^2)    -0.003213   0.002114  -1.520   0.1285    
I(Left^2)   -0.054258   0.008768  -6.188 6.09e-10 ***
I(Bottom^2)  0.003725   0.001782   2.091   0.0366 *  
Top:Right    0.012290   0.007540   1.630   0.1031    
Top:Bottom   0.004536   0.002880   1.575   0.1153    
Right:Left  -0.044283   0.015983  -2.771   0.0056 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
(Dispersion parameter for binomial family taken to be 1)
Null deviance: 3350.3  on 2799  degrees of freedom
Residual deviance: 1984.6  on 2789  degrees of freedom
AIC: 2006.6
</code></pre></li>
</ol>

<p>and the predicted power is as below, the MaFadden is 0.4004, and the value between 0.2~0.4 should be taken to present very good fit of the model(Louviere et al (2000), Domenich and McFadden (1975))                                                :</p>

<pre><code> &gt; PseudoR2(spec_q2)
    McFadden     Adj.McFadden        Cox.Snell       Nagelkerke McKelvey.Zavoina           Effron            Count        Adj.Count 
   0.4076315        0.4004680        0.3859918        0.5531859        0.6144487        0.4616466        0.8489286        0.4712500 
         AIC    Corrected.AIC 
2006.6179010     2006.7125925 
</code></pre>

<p>and the goodness-of-fit statistics:</p>

<pre><code> &gt; hoslem.test(result,phat,g=8)

     Hosmer and Lemeshow goodness of fit (GOF) test

  data:  result, phat
  X-squared = 2800, df = 6, p-value &lt; 2.2e-16
</code></pre>

<p>As my understanding, GOF is actually testing the following null and alternative hypothesis:</p>

<pre><code>  H0: The models does not need interaction and non-linearity
  H1: The models needs interaction and non-linearity
</code></pre>

<p>Since my models added interaction, non-linearity already and the p-value shows H0 should be rejected, so I came to the conclusion that my model needs interaction, non-linearity indeed. Hope my interpretation is correct and thanks for any advise in advance, thanks. </p>
"
"0.104257207028537","0.105409255338946","171879","<p>I have the R output for the logistic regression model. It seems that only the intercept and psa are statistically significant. Does that mean I should remove sorbets_psa and cinko from my model and create a new model as new.model = glm(status ~ psa,family = binomial(link =""probit""))</p>

<pre><code>Call:
glm(formula = status ~ psa + serbest_psa + cinko, family = binomial(link =""probit""), data = data)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.3285  -0.6773  -0.6261  -0.5604   1.9500  

Coefficients:
      Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -0.9697009  0.2409856  -4.024 5.72e-05 ***
psa          0.0444376  0.0094368   4.709 2.49e-06 ***
serbest_psa -0.0440718  0.0250486  -1.759   0.0785 .  
cinko       -0.0006923  0.0016984  -0.408   0.6835    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 534.27  on 477  degrees of freedom
Residual deviance: 477.07  on 474  degrees of freedom
AIC: 485.07

Number of Fisher Scoring iterations: 6
</code></pre>
"
"0.209134071928531","0.211445015805516","172943","<p>I'm trying to understand the output of <code>glm</code> when a categorical variable has more than 2 categories.</p>

<p>I'm analysing if age affects death. Age is a categorical variable with 4 categories</p>

<p>I use the following code in R:</p>

<pre><code>mydata &lt;- read.delim(""Data.txt"", header = TRUE)
mydata$Agecod &lt;- factor(mydata$Agecod)
mylogit &lt;- glm(Death ~ Agecod, data = mydata, family = ""binomial"")
summary(mylogit)
</code></pre>

<p>Obtaining the following output: </p>

<pre><code>Call:
glm(formula = Death ~ Agecod, family = ""binomial"", data = mydata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.4006  -0.8047  -0.8047   1.2435   2.0963  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   0.5108     0.7303   0.699   0.4843  
Agecod2      -0.6650     0.7715  -0.862   0.3887  
Agecod3      -1.4722     0.7658  -1.922   0.0546 .
Agecod4      -2.5903     1.0468  -2.474   0.0133 *

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 237.32  on 184  degrees of freedom
Residual deviance: 223.73  on 181  degrees of freedom
  (1 observation deleted due to missingness)
AIC: 231.73

Number of Fisher Scoring iterations: 4
</code></pre>

<p>Since I have p-values for <code>Agecod2</code>, <code>Agecod3</code> and <code>Agecod4</code> and only <code>Agecod4</code> has a significant p-value my questions are:</p>

<ol>
<li>Is really <code>Age</code> associated with death?</li>
<li>Is only the 4th age category associated with death?</li>
<li>What happens with the first category since I don't have its p-value?</li>
</ol>

<p>Update:</p>

<p>Since Antoni Parellada says â€œIt seems as though you have proven that old age is a good predictor of deathâ€ and Gung points â€œYou cannot tell from your output if Age is associated with deathâ€ Iâ€™m still confused.</p>

<p>I understand that â€œInterceptâ€ is representing Agecod1 and is the â€œreference levelâ€. According to Gung â€œThe Estimates for the rest are the differences between the indicated level and the reference level. The associated p-values are for the tests of the indicated level vs. the reference level in isolation.â€ </p>

<p>My question now is: </p>

<p>Since Agecod4 p-value (0.0133) is significantly different from Agecod1 (reference lelvel) it doesnâ€™t mean that age is associated with death?</p>

<p>I have also tried to perform a nested test with the following command:</p>

<pre><code>anova(mylogit, test=""LRT"")
</code></pre>

<p>Obtaining:</p>

<pre><code>       Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)   
NULL                     184     237.32            
Agecod  3   13.583       181     223.73 0.003531 *
</code></pre>

<p>Does it mean that Age is definitively associated with death?</p>

<p>Update2:</p>

<p>I have solved my problem using binary logistic regression in SPSS. The output is the same than â€œmylogitâ€ but with SPSS I obtain a global p-value for the overall variable Agecod which is 0.008.</p>

<p>I donâ€™t know if is possible to obtain this â€œglobal p-valueâ€ with R, but since I know that I can use SPSS is not a big problem for me.</p>
"
"0.0601929265428846","0.0608580619450185","174989","<p>Please bear with me, I am very new to R.</p>

<p>My question is regarding the use of the <code>improveProb</code> function in the <code>Hmisc</code> package. I have two logistic models, the only difference being that the second model contains my novel marker of interest. I am trying to calculate NRI and IDI.</p>

<p>I have the PredRisks for both models - PredRisk1 and PredRisk2, and my outcome is disease 0/1. How do I define this in R in order to run</p>

<p><code>improveProb(x1, x2, y)</code>?</p>

<hr>

<p>The data are the same for both models. We are looking at ways to validate our findings. We have performed k-fold cross-validation (MSE=0.08) and bootstrapping with optimism (AUC original = 0.826 After correction =0.791) to check for overfitting. Is this appropriate? The LRT was significant for both logistic regression models, but I need to check this. Also, the AIC for model 2 is lower than model 1. Thanks again for your expert knowledge :)</p>
"
"0.120385853085769","0.121716123890037","175203","<p>I'm reading <a href=""https://onlinecourses.science.psu.edu/stat504/node/177"" rel=""nofollow"">this tutorial</a> to understand how to interpret the coefficients of an ordinal logistic regression which assumes the proportional odds. </p>

<p>They use a dataset about a cheese tasting experiment. Subjects were randomly assigned to taste one of four different cheeses (A,B,C,D). Response categories are 1 = strong dislike to 9 = excellent taste.</p>

<pre><code>m1=polr(response~cheese,weights=N,data=dati)
summary(m1)

Re-fitting to get Hessian

Call:
polr(formula = response ~ cheese, data = dati, weights = N)

Coefficients:
         Value Std. Error t value
cheeseB -3.352     0.4287  -7.819
cheeseC -1.710     0.3715  -4.603
cheeseD  1.613     0.3805   4.238

Intercepts:
    Value    Std. Error t value 
1|2  -5.4674   0.5236   -10.4413
2|3  -4.4122   0.4278   -10.3148
3|4  -3.3126   0.3700    -8.9522
4|5  -2.2440   0.3267    -6.8680
5|6  -0.9078   0.2833    -3.2037
6|7   0.0443   0.2646     0.1673
7|8   1.5459   0.3017     5.1244
8|9   3.1058   0.4057     7.6547

Residual Deviance: 711.3479 
AIC: 733.3479 
</code></pre>

<p>The tutorial's author writes:</p>

<blockquote>
  <p>we see that the implied ordering of cheeses in terms of quality is D >
  A > C > B. Furthermore, D is significantly better preferred than A,
  but A is not significantly better than C.</p>
</blockquote>

<p>Is this correct? 
I do agree that cheese B and C are significantly worse than A, and that D is significantly better than A, but I don't understand why cheese A should not be significantly better than C, as the author claims.</p>

<p>This are instead my conclusions:<br>
Since $\beta_B \neq 0$ and $\beta_B &lt; 0$, then $B&lt;A$.<br>
Since $\beta_C \neq 0$ and $\beta_C &lt; 0$, then $C&lt;A$.<br>
Since $\beta_D \neq 0$ and $\beta_D &gt; 0$, then $D&gt;A$.<br>
So, $D&gt;A&gt;B$ and $D&gt;A&gt;B$.
But since $\beta_B &lt; \beta_C$, then $D&gt;A&gt;B&gt;C$.<br>
So, I would say instead that cheese A is significantly better than C.</p>
"
"0.199637352376173","0.201843356939833","175767","<p>Logistic regression models the relationship between a set of independent variables and the probability that a case is a member of one of the categories of the dependent variable. If the probability is greater than 0.5, the case is classified in the modeled category.  If the probability is less than 0.50, the case is classified in the other category. The problem is that when I run the model with my dataset, the probabilities are far from 0.5, in fact it never gets to that value.</p>

<p>Here is part of My dataset:</p>

<pre><code>  sum_profit   direction   profit_cl1
   10           up          0.00
   0            Not_up     -0.03
  -5            Not_up      0.04
  -5            Not_up     -0.04
</code></pre>

<p>I want to find a relationship between the price of oil and the stock price of a Colombian oil company. So the variable 'sum_profit' is the sum of the change in the stock price in the next ten minutes. The variable 'profit_cl1' shows me the net change in the oil price in the last 10 minutes. </p>

<p>So what I want to know is that if the oil price changes in the last 10 minutes how would I expect the stock price direction to be in the following 10 minutes (Up or Down).</p>

<p>The problem is that my probabilities once I run the logistic regression are far from 0.5 even though the model is significant </p>

<pre><code>    glm.fit=glm(formula = direction ~ profit_cl1, family = binomial, data = datos)

    Deviance Residuals: 
      Min       1Q   Median       3Q      Max  
    -0.6786  -0.6786  -0.6131  -0.6131   1.8783  

    Coefficients:
                    Estimate Std. Error z value Pr(&gt;|z|)    
    (Intercept)     -1.57612    0.01618 -97.394   &lt;2e-16 ***
    profit_cl1       0.22485    0.02288   9.829   &lt;2e-16 ***
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

    (Dispersion parameter for binomial family taken to be 1)

     Null deviance: 48530  on 50309  degrees of freedom
    Residual deviance: 48434  on 50308  degrees of freedom
    AIC: 48438

    Number of Fisher Scoring iterations: 4 
</code></pre>

<p>The code to get the probabilities:</p>

<pre><code>   log.probs=predict(glm.fit, type=""response"")
   mean(log.probs)=0.1873 
</code></pre>

<p>the 0.1873 is very far from 0.5.</p>

<p>Sorry but I did not know where else to look for help! I appreciate any suggestion!</p>
"
"0.0851256530758749","0.086066296582387","177288","<p>First query, so apologize in advance for any stupidity or ""unawareness"".  I have a large sample, at roughly 88000 obs.  But, my events for this sample (the 1's) are about .00072% of the sample.  </p>

<p>Pretty sure that my sample suffers from rare event bias.  Therefore, I am using the logistf function to run a logistic model.  But not sure that this is the best method.  I've read the standard King and Zeng paper.  But I am just getting some unusual results.  Meaning, that variables that I thought would be significant, are just not coming out that way.  In addition, the df  for the lrtest and extractAIC are really small, between 5 to 7 for any model that I have run.  </p>

<p>Sorry, I can't provide screen shots or results.  Work data, so not sure that I can share. </p>
"
"0.120385853085769","0.121716123890037","177650","<p>I have a binary response variable and a categorical predictor variable. If I test for associations between the 2 variables using chi-square test , it turns out to be significant. However, if I do a logistic regression with the same set of variables, the predictor is not significant. Why does this happen?</p>

<pre><code>  table(Data1$pred,Data1$target)

                            0    1
  Level1                    1    0
  Level2                    4    0
  Level3                   98    1
  Level4                 2056   22
  Level5                    1    0
  Level6                    2    0
  Level7                  311    0
  Level8                    6    1
  Level9                  131    7
  Level10                  49    2

  chisq.test(table(Data1$pred,Data1$target))

  Pearson's Chi-squared test

  data:  tabletable(Data1$pred,Data1$target)
  X-squared = 34.2614, df = 9, p-value = 8.037e-05
</code></pre>

<p>Logistic Regression on the same</p>

<pre><code>  logit.glm &lt;- glm(as.factor(target) ~ pred,                  
               data=Data1, family=binomial(link=""logit"")
  summary(logit.glm)
  Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.5553  -0.1459  -0.1459  -0.1459   3.0315  

  Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)
  (Intercept)   -2.057e+01  1.773e+04  -0.001    0.999
  Data1Level2   -6.313e-06  1.982e+04   0.000    1.000
  Data1Level3    1.598e+01  1.773e+04   0.001    0.999
  Data1Level4    1.603e+01  1.773e+04   0.001    0.999
  Data1Level5   -6.312e-06  2.507e+04   0.000    1.000
  Data1Level6   -6.312e-06  2.172e+04   0.000    1.000
  Data1Level7   -6.312e-06  1.776e+04   0.000    1.000
  Data1Level8    1.877e+01  1.773e+04   0.001    0.999
  Data1Level9    1.764e+01  1.773e+04   0.001    0.999
  Data1Level10   1.737e+01  1.773e+04   0.001    0.999

  (Dispersion parameter for binomial family taken to be 1)

   Null deviance: 356.09  on 2691  degrees of freedom
   Residual deviance: 333.06  on 2682  degrees of freedom
   AIC: 353.06

   Number of Fisher Scoring iterations: 19
</code></pre>
"
"0.0851256530758749","0.086066296582387","177903","<p>I fitted an ordinal logistic regression but I'm unable to interpret the coefficients. Can anyone assist in this regard? Here is the output generated: </p>

<pre><code>Call:
polr(formula = factor(grade) ~ factor(Month) + Day, data = myData, 
    Hess = TRUE)

Coefficients:
                  Value Std. Error t value
factor(Month)4 1.405114    0.51547  2.7259
Day            0.007672    0.01944  0.3947

Intercepts:
    Value   Std. Error t value
1|2 -0.6785  0.7019    -0.9667
2|3  1.6767  0.7162     2.3412

Residual Deviance: 333.602 
AIC: 341.602 
</code></pre>

<p>The grade is factored: </p>

<p>1 = good<br>
2 = very good<br>
3 = excellent</p>

<p>Month is factored:</p>

<p>3 = March<br>
4 = April</p>

<p>The grade is the response while month and day are my explanatory variables.</p>
"
"0.181488502160157","0.183493960854393","179250","<p>I am conducting a multifactorial analyisis involving categorical variables by using R. The response is â€œyesâ€ or â€œnoâ€ (Iâ€™m therefore using binary logistic regression) and the predictors have 2 up to 3 levels. The sample size is quite small (n = 230).</p>

<p>I tried before both the glm and rsm packages, but one predictor (B2) had an extremely high standard error; it was a separation problem since one of its level has zero observation. I have been then suggested to rely on penalized logistic regression and in particular the brglm package.</p>

<p>Here as follows, you can see the new output (brglm) for the model containing all the variables:</p>

<pre><code>Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)  1.64053    0.78655   2.086   0.0370 * 
B1x         -0.07434    0.71502  -0.104   0.9172   
B1y         -0.20966    0.59622  -0.352   0.7251   
B2x          0.09004    0.57168   0.158   0.8748   
B2y         -2.68270    1.67192  -1.605   0.1086   
B3x         -0.53347    0.33882  -1.574   0.1154   
B4x         -0.59495    0.30320  -1.962   0.0497 (*) 
B5x         -0.79962    0.47249  -1.692   0.0906 . 
B5y         -1.42761    0.59344  -2.406   0.0161 (*) 
B6x          1.36948    0.49224   2.782   0.0054 (**)
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 275.58  on 229  degrees of freedom
Residual deviance: 270.93  on 220  degrees of freedom
Penalized deviance: 253.9314 
AIC:  290.93 
</code></pre>

<p>As you can see, the problem regarding the huge standard error has been finally solved but there are still several issues:</p>

<p>1) In the description of the bglrm package, they say:</p>

<p>â€œIt is not advised to use methods associated with model comparison (add1, drop1,anova, etc.) on objects of class ""brglm"". Model comparison when estimation is performed using the modified scores or the penalized likelihood is an on-going research topic and will be implemented as soon as it is concludedâ€</p>

<p><strong>How can I safely remove non significant predictors?</strong> <strong>Have I only to rely on the p-value in the above output?</strong> 
That means that I have to remove from a further model predictors from B1 to B4, is that correct? (that was also case when I used a non penalized logistic regression model (such as lrm), but then I could use drop1).</p>

<p>2) <strong>How can I detect multicollinearity issues in brglm?</strong> </p>

<p>By using a non penalized likelihood model, I know that there is multicollinearity between B5 and B6, but how can I prove it now?</p>

<p>3) <strong>How can I calculate in brglm indexes such as C and Nagelgerkeâ€™s R</strong>? </p>

<p>4) In the description of the package, it is also written: â€œThe use of Akaike's information criterion (AIC) for model selection when method = ""brglm.fit"" is controversial.â€  </p>

<p>Is it safe or not to publish AIC in a paper?</p>

<p>Thank you very much in advance!</p>
"
"0.217028683070608","0.219426862868128","180447","<p>I have some data on patients presenting to emergency departments after sustaining self-inflicted gunshot injuries, stored in a data frame (""SIGSW,"" which is ~16,000 observations of 47 variables) in R. I want to create a model that helps a physician predict, using several objective covariates, the ""pretest probability"" of the self-shooting being a suicide attempt, or a negligent discharge. The covariates are largely categorical variables, but a few are continuous or binary. My outcome, suicide attempt or not, is coded as a binary/indicator variable, ""SI,"" so I believe a binary logistic regression to be the appropriate tool.  </p>

<p>In order to construct my model, I intended to individually regress SI on each covariate, and use the p-value from the likelihood ratio test for each model to inform which covariates should be considered for the backward model selection. </p>

<p>For each model, SI~SEX, SI~AGE, etc, I receive the following error:</p>

<pre><code>&gt;glm(SI ~ SEX, family = binomial, data=SIGSW)
Warning messages:
1: glm.fit: algorithm did not converge 
2: glm.fit: algorithm did not converge
</code></pre>

<p>A little Googling revealed that I perhaps need to increase the number of iterations to allow convergence. I did this with the following:</p>

<pre><code>&gt;glm(SI ~ SEX, family = binomial, data=SIGSW, control = list(maxit = 50))

Call:  glm(formula = SI ~ SEX, family = binomial, data = SIGSW, control = list(maxit = 50))

Coefficients:
(Intercept)          SEX  
 -3.157e+01   -2.249e-13  

Degrees of Freedom: 15986 Total (i.e. Null);  15985 Residual
Null Deviance:      0 
Residual Deviance: 7.1e-12  AIC: 4
Warning message:
glm.fit: fitted probabilities numerically 0 or 1 occurred 
</code></pre>

<p>This warning message, after a little Googling, suggests a ""perfect separation,"" which, as I understand it, means that my predictor is ""too good."" Seeing as how this happens with all of the predictors, I'm somewhat skeptical that they're all ""too good."" Am I doing something wrong? </p>

<p>Edit: In light of the answers, here is a sample of the data (I only selected a few of the variables for space concerns):</p>

<pre><code>   SIGSW.AGENYR_C SIGSW.SEX SIGSW.RACE_C SIGSW.SI
1              19      Male        White        0
2              13      Male        Other        0
3              18      Male   Not Stated        0
4              15      Male        White        0
5              23      Male        White        0
6              11      Male        Black        0
7              16      Male   Not Stated        1
8              21      Male   Not Stated        0
9              14      Male        White        0
10             41      Male        White        0
</code></pre>

<p>And here is the crosstabulation of SEX and SI, showing that SI is coded as an indicator variable, and that there are both men and women with SI, so sex is not a perfect predictor. </p>

<pre><code>  &gt;table(SIGSW$SEX, SIGSW$SI)        
              0     1
  Unknown     1     3
  Male    11729  2121
  Female   1676   457
</code></pre>

<p>Does the small cell size represent a problem?</p>
"
"0.17025130615175","0.150616019019177","182286","<p>I am doing a regression analysis for an ordinal response variable with 5 explanatory variables. I will be using the <code>polr()</code> or <code>lrm()</code> functions to do the ordinal logistic regression. For my non-ordinal response variables (e.g., count and binary data), I have been using glmulti for model selection, but this doesn't seem to be compatible with the <code>polr()</code> and <code>lrm()</code> R functions. I've also tried <code>stepAIC()</code>, <code>step()</code> and <code>leap()</code> functions without any luck. The summary of the <code>polr()</code> regression shows an AIC score.</p>

<pre><code>&gt; model1 &lt;- polr(x ~ Age + Gender + StudentType + StudentYear + RacialGroup,
+ data = question8a, Hess =TRUE)
&gt; summary(model1)
Call:
polr(formula = x ~ Age + Gender + StudentType + 
    StudentYear + RacialGroup, data = question8a, Hess = TRUE)

Coefficients:
                                   Value Std. Error  t value
Age                             -0.16691    0.04925 -3.38872
GenderWoman                      0.05514    0.24655  0.22366
StudentTypeUndergraduatestudent -1.36414    0.50748 -2.68807
StudentYear2ndyear              -0.02042    0.29600 -0.06899
StudentYear3rdyear              -0.05997    0.38253 -0.15676
StudentYear4+years               0.89921    0.66430  1.35363
StudentYear4thyear               0.25324    0.42433  0.59680
RacialGroupNon-Indigenous       -2.13460    0.42163 -5.06268

Intercepts:
    Value   Std. Error t value
1|2 -9.9335  1.5283    -6.4999
2|3 -8.3051  1.4752    -5.6298
3|4 -7.2498  1.4567    -4.9770
4|5 -4.8720  1.4240    -3.4214

Residual Deviance: 657.086 
AIC: 681.086 
</code></pre>

<p>I tried to follow this suggestion: <a href=""http://statistics.unl.edu/faculty/bilder/categorical/Chapter5/glmultiFORpolr.R"" rel=""nofollow"">http://statistics.unl.edu/faculty/bilder/categorical/Chapter5/glmultiFORpolr.R</a>, but wasn't able to get it to work. </p>

<p>Has anyone been able to get this to work? Or do I need to compare the 2^5 = 32 model AIC scores by hand? </p>
"
"0.240771706171538","0.243432247780074","183320","<p>I have the following dataframe on which I did logistic regression with response as outcome. There are some good predictors in these variables so I expected significant variables.</p>

<pre><code>structure(list(response = c(0L, 0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 
    0L, 0L, 1L, 0L, 1L, 0L), HIST1H3F_rna = c(1.09861228866811, 0.693147180559945, 
    2.07944154167984, 1.09861228866811, 1.79175946922805, 0, 0, 0, 
    2.39789527279837, 1.38629436111989, 1.6094379124341, 1.6094379124341, 
    0.693147180559945, 1.79175946922805, 0), NCF1_rna = c(2.77258872223978, 
    3.09104245335832, 2.63905732961526, 2.19722457733622, 2.30258509299405, 
    2.56494935746154, 3.09104245335832, 3.98898404656427, 2.56494935746154, 
    4.06044301054642, 3.87120101090789, 2.07944154167984, 3.49650756146648, 
    3.17805383034795, 3.95124371858143), WDR66_rna = c(5.06890420222023, 
    4.49980967033027, 5.11799381241676, 3.40119738166216, 3.25809653802148, 
    4.02535169073515, 5.8348107370626, 5.89440283426485, 3.87120101090789, 
    5.67675380226828, 5.35185813347607, 4.15888308335967, 6.23441072571837, 
    5.91889385427315, 3.68887945411394), PTH2R_rna = c(0.693147180559945, 
    5.08759633523238, 0.693147180559945, 1.09861228866811, 0, 6.01126717440416, 
    6.56526497003536, 5.18178355029209, 0, 4.36944785246702, 2.19722457733622, 
    1.09861228866811, 3.49650756146648, 1.38629436111989, 5.93753620508243
    ), HAVCR2_rna = c(4.48863636973214, 3.40119738166216, 3.09104245335832, 
    2.94443897916644, 3.2188758248682, 3.76120011569356, 3.95124371858143, 
    2.83321334405622, 2.07944154167984, 4.36944785246702, 3.58351893845611, 
    1.94591014905531, 4.23410650459726, 3.43398720448515, 2.56494935746154
    ), CD200R1_rna = c(2.484906649788, 2.94443897916644, 0.693147180559945, 
    1.94591014905531, 0.693147180559945, 2.89037175789616, 2.56494935746154, 
    1.6094379124341, 1.6094379124341, 1.94591014905531, 2.19722457733622, 
    0.693147180559945, 4.26267987704132, 1.6094379124341, 0.693147180559945
    )), .Names = c(""response"", ""HIST1H3F_rna"", ""NCF1_rna"", ""WDR66_rna"", 
    ""PTH2R_rna"", ""HAVCR2_rna"", ""CD200R1_rna""), row.names = c(NA, 
    -15L), class = ""data.frame"")
</code></pre>

<p>However, running the following lines and getting a summary of the model I find that all variables have a p-value of 1 and the standard errors seem so high. What's going on here?</p>

<pre><code>fullmod &lt;- glm(response ~ ., data=final_model,family='binomial')
summary(fullmod)
Call:
glm(formula = response ~ ., family = ""binomial"", data = final_model)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-6.515e-06  -2.404e-06  -2.110e-08   2.110e-08   7.470e-06  

Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)   1.460e+02  5.598e+05       0        1
HIST1H3F_rna  2.135e+01  5.145e+05       0        1
NCF1_rna     -4.133e+01  3.388e+05       0        1
WDR66_rna     1.296e+01  6.739e+05       0        1
PTH2R_rna     1.975e+00  3.775e+05       0        1
HAVCR2_rna   -2.477e+01  1.191e+06       0        1
CD200R1_rna  -1.420e+01  1.315e+06       0        1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.0190e+01  on 14  degrees of freedom
Residual deviance: 2.2042e-10  on  8  degrees of freedom
AIC: 14

Number of Fisher Scoring iterations: 25
</code></pre>

<hr>

<p>In response to your comments I'll show the feature selection step (and the complete dataframe I'm working with below that).  </p>

<pre><code># forward  feature selection 
library('boot')
z = c()
nullmod &lt;- glm(response ~ 1, data=final_model, family='binomial') ## â€˜emptyâ€™ 
fullmod &lt;- glm(response ~ ., data=final_model, family='binomial') ## Full model
first = T
for(x in 1:ncol(final_model)){
  stepmod &lt;- step(nullmod, scope=list(lower=formula(nullmod), upper=formula(fullmod)),
                  direction=""forward"", data=final_model, steps=x, trace=F)
  cv.err  &lt;- cv.glm(data=final_model, glmfit=stepmod, K=nrow(final_model))$delta[1]
  if (first == T){
    first=F
    final_features &lt;- stepmod
  }else{
    if (cv.err &lt; min(z)){ final_features &lt;- stepmod }
  }
  z[x] &lt;- cv.err
  print(paste(x,cv.err))
  print(colnames(final_features$model))
}

plot(z, main='Forward Feature Selection GLM Final Model', 
     xlab='Number of Steps', ylab='LOOCV-error', col='red', type='l')
points(z)
colnames(final_features$model)
summary(final_features)

structure(list(response = c(0L, 1L, 1L, 1L, 0L, 0L, 0L, 1L, 0L, 
0L, 1L, 0L, 1L, 1L, 1L), HIST1H3F_rna = c(1.09861228866811, 2.07944154167984, 
1.09861228866811, 1.79175946922805, 0, 0, 0, 2.39789527279837, 
1.38629436111989, 1.6094379124341, 1.6094379124341, 0.693147180559945, 
2.19722457733622, 2.39789527279837, 2.89037175789616), NCF1_rna = c(2.77258872223978, 
2.63905732961526, 2.19722457733622, 2.30258509299405, 2.56494935746154, 
3.09104245335832, 3.98898404656427, 2.56494935746154, 4.06044301054642, 
3.87120101090789, 2.07944154167984, 3.49650756146648, 2.07944154167984, 
2.07944154167984, 1.09861228866811), WDR66_rna = c(5.06890420222023, 
5.11799381241676, 3.40119738166216, 3.25809653802148, 4.02535169073515, 
5.8348107370626, 5.89440283426485, 3.87120101090789, 5.67675380226828, 
5.35185813347607, 4.15888308335967, 6.23441072571837, 4.0943445622221, 
4.21950770517611, 3.95124371858143), PTH2R_rna = c(0.693147180559945, 
0.693147180559945, 1.09861228866811, 0, 6.01126717440416, 6.56526497003536, 
5.18178355029209, 0, 4.36944785246702, 2.19722457733622, 1.09861228866811, 
3.49650756146648, 0, 0.693147180559945, 1.38629436111989), 
HAVCR2_rna = c(4.48863636973214, 
3.09104245335832, 2.94443897916644, 3.2188758248682, 3.76120011569356, 
3.95124371858143, 2.83321334405622, 2.07944154167984, 4.36944785246702, 
3.58351893845611, 1.94591014905531, 4.23410650459726, 1.38629436111989, 
1.09861228866811, 1.38629436111989), CD200R1_rna = c(2.484906649788, 
0.693147180559945, 1.94591014905531, 0.693147180559945, 2.89037175789616, 
2.56494935746154, 1.6094379124341, 1.6094379124341, 1.94591014905531, 
2.19722457733622, 0.693147180559945, 4.26267987704132, 1.94591014905531, 
0, 0.693147180559945), GDF7 = c(0.2232, -0.7281, 0.0655, -0.7919, 
0.175, 0.0891, 0.4396, -0.2774, -0.4079, 0.4069, 0.3057, 0.7371, 
-0.4978, -0.5096, -0.0827), HS1BP3 = c(0.2232, -0.7281, 0.0655, 
-0.7919, 0.175, 0.0891, 0.4396, -0.2774, -0.4079, 0.4069, 0.3057, 
0.7371, -0.4978, -0.5096, -0.0827), NKAIN3 = c(0.4072, 0.3216, 
-0.5466, -0.1588, 0.4515, 0.2849, 0.1675, 0.0847, 0.6601, 0.6331, 
-0.135, 1.3532, -0.503, -0.1241, 0.2061), UG0898H09 = c(0.4072, 
0.3216, -0.5466, -0.1588, 0.4515, 0.2849, 0.1675, 0.0847, 0.6601, 
0.6331, -0.135, 1.3532, -0.503, -0.1241, 0.2061), C15orf41 = c(0.122, 
-0.7519, -1.1267, -0.7882, -0.1117, -0.5105, -0.3905, -0.6834, 
-0.5944, 0.0714, -0.8134, -0.0115, -1.1112, -1.1488, -0.4878), 
    FAM98B = c(-0.1871, -0.7519, -1.1267, -0.7882, -0.1117, -0.5105, 
    -0.3905, -0.6834, -0.5944, 0.0714, -0.8134, -0.0115, -1.1112, 
    -1.1488, -0.4878), SPRED1 = c(-0.1871, -0.7519, -1.1267, 
    -0.7882, -0.1117, -0.5105, -0.3905, -0.6834, -0.5944, 0.0714, 
    -0.8134, -0.0115, -1.1112, -1.1488, -0.4878), MPDZ_ex = c(1, 
    0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0), TPR_ex = c(0, 
    0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), BUB1B_ex = c(0, 
    0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0), APC_ex = c(0, 
    0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0), ATM_ex = c(0, 
    0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0), DYNC1LI1_ex = c(0, 
    0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0), TTK_ex = c(0, 
    0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0), PSMG2_ex = c(1, 
    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0), NegRegMitosis = c(1, 
    0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0), brca1ness = c(0.037719, 
    0.900878, 0.013261, 0.900878, 0.659963, 0.005629, 9.8e-05, 
    0.996336, 0.910072, 0.850776, 0.000613, 0.104428, 0.978114, 
    0.938767, 0.041696), Methylation = c(0L, 0L, 0L, 1L, 1L, 
    1L, 1L, 0L, 0L, 0L, 0L, 0L, 1L, 1L, 0L), LinoleicAcid_Metab = structure(c(2L, 
    2L, 2L, 2L, 1L, 3L, 2L, 2L, 1L, 5L, 2L, 5L, 1L, 2L, 2L), .Label = c(""CYP2E1_high"", 
    ""CYP2E1_med"", ""high"", ""low"", ""PLA2G2A_high""), class = ""factor""), 
    Neuro_lr = structure(c(2L, 2L, 1L, 1L, 3L, 3L, 3L, 1L, 3L, 
    1L, 1L, 3L, 3L, 1L, 1L), .Label = c(""1"", ""2"", ""3"", ""4""), class = ""factor""), 
    NOX_signalling = structure(c(2L, 2L, 2L, 2L, 1L, 2L, 1L, 
    2L, 1L, 1L, 2L, 1L, 1L, 2L, 2L), .Label = c(""high"", ""low""
    ), class = ""factor"")), .Names = c(""response"", ""HIST1H3F_rna"", 
""NCF1_rna"", ""WDR66_rna"", ""PTH2R_rna"", ""HAVCR2_rna"", ""CD200R1_rna"", 
""GDF7"", ""HS1BP3"", ""NKAIN3"", ""UG0898H09"", ""C15orf41"", ""FAM98B"", 
""SPRED1"", ""MPDZ_ex"", ""TPR_ex"", ""BUB1B_ex"", ""APC_ex"", ""ATM_ex"", 
""DYNC1LI1_ex"", ""TTK_ex"", ""PSMG2_ex"", ""NegRegMitosis"", ""brca1ness"", 
""Methylation"", ""LinoleicAcid_Metab"", ""Neuro_lr"", ""NOX_signalling""
), row.names = c(NA, -15L), class = ""data.frame"")
</code></pre>

<p>Summary now gives the following:</p>

<pre><code>Call:
glm(formula = response ~ NegRegMitosis, family = ""binomial"", 
    data = final_model)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-3.971e-06  -3.971e-06   3.971e-06   3.971e-06   3.971e-06  

Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)       25.57   76367.61       0        1
NegRegMitosis    -51.13  111790.71       0        1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 2.0728e+01  on 14  degrees of freedom
Residual deviance: 2.3655e-10  on 13  degrees of freedom
AIC: 4

Number of Fisher Scoring iterations: 24
</code></pre>

<p>Again even in a single predictor model, my p-value is 1. The predictor in this case is equal to the response, so it should predict perfectly. Then why is my pvalue 1?</p>
"
"0.181488502160157","0.201843356939833","183699","<p>I encountered a strange phenomenon when calculating pseudo R2 for logistic models when using aggregated files: the results are simply too good to be true. An example (but as far as I can see, every aggregated file offers similar problems):</p>

<pre><code> library(pscl)
 cuse &lt;- read.table(""http://data.princeton.edu/wws509/datasets/cuse.dat"",
               header=TRUE)

 head(cuse)
 cuse.fit &lt;- glm( cbind(using, notUsing) ~ age + education + wantsMore, 
             family = binomial, data=cuse)

 summary(cuse.fit)
 pR2(cuse.fit)     
</code></pre>

<p>The results are:</p>

<pre><code>&gt; summary(cuse.fit)

Call:
glm(formula = cbind(using, notUsing) ~ age + education + wantsMore, 
family = binomial, data = cuse)

Deviance Residuals: 
Min       1Q   Median       3Q      Max  
-2.5148  -0.9376   0.2408   0.9822   1.7333  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -0.8082     0.1590  -5.083 3.71e-07 ***
age25-29       0.3894     0.1759   2.214  0.02681 *  
age30-39       0.9086     0.1646   5.519 3.40e-08 ***
age40-49       1.1892     0.2144   5.546 2.92e-08 ***
educationlow  -0.3250     0.1240  -2.620  0.00879 ** 
wantsMoreyes  -0.8330     0.1175  -7.091 1.33e-12 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 165.772  on 15  degrees of freedom
Residual deviance:  29.917  on 10  degrees of freedom
AIC: 113.43

Number of Fisher Scoring iterations: 4

&gt; pR2(cuse.fit)
         llh      llhNull           G2     McFadden         r2ML 
 -50.7125647 -118.6401419  135.8551544    0.5725514    0.9997947 
       r2CU 
  0.9997950 
</code></pre>

<p>The last three outcomes from pscl function pR2 present McFadden's pseudo r-squared,  Maximum likelihood pseudo r-squared (Cox &amp; Snell) and Cragg and Uhler's or Nagelkerke's pseudo r-squared. The calculation seems to be flawless, but the outcomes close to 1 seem to good to be true.</p>

<p>Using weight instead of cbind:</p>

<pre><code>cuse2 = rbind(cuse,cuse)
cuse2$using.contraceptive=1
    cuse2$using.contraceptive[1:nrow(cuse)]=0
cuse2$freq = cuse2$notUsing
cuse2$freq[1:nrow(cuse)] = cuse2$using[1:nrow(cuse)]
cuse.fit2 = glm(using.contraceptive ~ age + education + wantsMore,
            weight=freq, family = binomial, data = cuse2)
summary(cuse.fit2)
round(pR2(cuse.fit2),5)
</code></pre>

<p>produces different logistic regression coefficients, and slightly different pseudo R2's for r2ml and r2CU and a large difference for McFadden R2:</p>

<pre><code>&gt; round(pR2(cuse.fit2),5)
         llh     llhNull          G2    McFadden        r2ML 
  -933.91920 -1001.84677   135.85515     0.06780     0.98567 
        r2CU 
     0.98567 
</code></pre>

<p>Full expansion results in very different estimates from pR2:</p>

<pre><code> cuse3 = rbind(cuse[rep(1:nrow(cuse), cuse[[""notUsing""]]), ],
          cuse[rep(1:nrow(cuse), cuse[[""using""]]), ])
 cuse3$using.contraceptive=1
     cuse3$using.contraceptive[1:sum(cuse$notUsing)]=0
 summary(cuse3)
 cuse.fit3 = glm(using.contraceptive ~ age + education + wantsMore,
            family = binomial, data = cuse3)
 summary(cuse.fit3)
 round(pR2(cuse.fit3),5)

 &gt; round(pR2(cuse.fit3),5)
         llh     llhNull          G2    McFadden        r2ML 
  -933.91920 -1001.84677   135.85515     0.06780     0.08106 
        r2CU 
     0.11376 
</code></pre>

<p>This indicates a logistic model which explains very little, which is a little bit more believable than the near perfect results from the aggregated files. Is there a more correct, and preferably more consistent, way to calculate the pseudo R2's? </p>
"
"0.107676380411633","0.136082763487954","185800","<p>I try to find a model using logistic regression. More precisely, what I did so far, is using stepwise regression and subset selection (although I know, it is often a bad idea) to find the ""best"" model. Clearly, depending on the information criteria I used, I got different results. </p>

<p>Now, I found an interesting example on page 250 in the book <a href=""http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf"" rel=""nofollow"">""An Introduction to Statistical Learning""</a>. They chose among the models of different sizes using cross-validation, that is they make predictions for each model and compute the test errors. Eventually, the compute the cross validation error and choose the model corresponding to the minimal average cross-validation error. </p>

<p>However, the function <code>regsubsets</code> of the R package ""leaps"" is only working for linear models. How can I implement this for logistic regression or glm models in general? </p>

<p>My idea was, to just estimate the models within a cross-validation using the <code>step</code> function of the ""stats"" package and then kind of take the average number of features (which is determined by minimum AIC, for example). Is this a legitimate approach?</p>
"
"0.208514414057075","0.210818510677892","186845","<p>I created some data using the following code:</p>

<pre><code>set.seed(1221)
x &lt;- runif(500)
y &lt;- runif(500,0,2)
z &lt;- rep(0,500)
z[-0.8*x + y - 0.75 &gt; 0] &lt;- 1
plot(x,y,col=as.factor(z))
</code></pre>

<p>This produces the following plot</p>

<p><a href=""http://i.stack.imgur.com/ycWdr.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ycWdr.png"" alt=""enter image description here""></a></p>

<p>The data is linearly separable. Then, I applied the glm function to create a logistic regression model.</p>

<pre><code>df &lt;- data.frame(class = z, x = x, y = y)
model &lt;- glm(z ~ x + y, family = binomial, data = df)
</code></pre>

<p>This produces the following output:</p>

<pre><code>summary(model)
Call:
glm(formula = z ~ x + y, family = binomial, data = df)

Deviance Residuals: 
       Min          1Q      Median          3Q         Max  
-8.127e-04  -2.000e-08  -2.000e-08   2.000e-08   7.699e-04  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept)    -1062      52666   -0.02    0.984
x              -1163      57197   -0.02    0.984
y               1433      70408    0.02    0.984

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 6.8274e+02  on 499  degrees of freedom
Residual deviance: 1.3345e-06  on 497  degrees of freedom
AIC: 6

Number of Fisher Scoring iterations: 25
</code></pre>

<p>The result surprised me, first because the parameter estimates are huge, and second because I was expecting such estimates to be close to the original decision boundary function, i.e. <code>-0.8x + y - 0.75 = 0</code>.</p>

<p>I then used the <a href=""http://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html"" rel=""nofollow"">glmnet</a> package to see if I could solve this issue. This package creates a penalised logistic regression model in order to deal with the large values in the parameter estimates. The code I used is the following:</p>

<pre><code>library(glmnet)
cvfit &lt;- cv.glmnet(as.matrix(df[,-1]), as.factor(df$class), family =   ""binomial"", type.measure = ""class"")
plot(cvfit)
</code></pre>

<p><a href=""http://i.stack.imgur.com/vH4AV.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/vH4AV.png"" alt=""enter image description here""></a></p>

<p>And the coefficients for the optimal penalty strength are:</p>

<pre><code>coef(cvfit, s = ""lambda.min"")
3 x 1 sparse Matrix of class ""dgCMatrix""
                    1
(Intercept) -84.01446
x           -91.40983
y           113.18736
</code></pre>

<p>Such coefficients are smaller than the ones obtained with the <code>glm</code> function. Still they are not the same as the decision boundary function. </p>

<p>Does anybody know why this is happening? Any help is greatly appreciated.</p>
"
"0.120385853085769","0.121716123890037","188495","<p>I have one potentially causal predictor and a number of covariates that I tested via AICc model selection in logistic GLM. I found that alone, the causal predictor has a low AICc (~19) and a significant p-value (&lt;0.05). Adding in 1, 2 or 3 of the other covariates gives me a slightly lower AICc (~17) but nonsignificant p-values for the predictors (causal and covariates predictors >>0.05).</p>

<p>Should I be selecting my models based solely on the best AICc? Should I be keeping the p-values in mind (e.g., eliminating a model in either the causal predictor or covariates are insignificant)? Is this a case of overfitting?</p>

<p>Thanks!</p>
"
"0.17025130615175","0.172132593164774","189188","<p>If I create a linear model in R, I get a p-value for the whole model. When I create a logistic regression model, I don't. Why is this?</p>

<p><strong>Linear Regression</strong></p>

<pre><code>x&lt;-rnorm(100)
y&lt;-x+rnorm(100)
summary(lm(y~x))

 Call: lm(formula = y ~ x)

 Residuals:
      Min       1Q   Median       3Q      Max 
 -2.46237 -0.52810 -0.04574  0.48878  2.81002 

 Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)     (Intercept) -0.02318    0.09394  -0.247    0.806     x            1.10130    0.09421  11.690   &lt;2e-16***
 --- Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

 Residual standard error: 0.9374 on 98 degrees of freedom Multiple
 R-squared:  0.5824,    Adjusted R-squared:  0.5781  F-statistic: 136.7 on
 1 and 98 DF,  p-value: &lt; 2.2e-16
</code></pre>

<p><strong>Logistic Regression</strong></p>

<pre><code>x&lt;-rnorm(100)
y&lt;-factor(c(rep(""ONE"",50),rep(""TWO"",50)))
summary(glm(y~x,family = ""binomial""))

 Call: glm(formula = y ~ x, family = ""binomial"")

 Deviance Residuals: 
      Min        1Q    Median        3Q       Max  
 -1.20658  -1.18093  -0.00499   1.17444   1.21414  

 Coefficients:
               Estimate Std. Error z value Pr(&gt;|z|) (Intercept)  3.857e-05  .000e-01   0.000    1.000 x           -3.924e-02  2.055e-01  -0.191    0.849

 (Dispersion parameter for binomial family taken to be 1)

    Null deviance: 138.63  on 99  degrees of freedom Residual deviance: 138.59  on 98  degrees of freedom AIC: 142.59

 Number of Fisher Scoring iterations: 3
</code></pre>
"
"0.422053453083317","0.393892771133865","191810","<h2>Study design</h2>

<p>504 individuals were all sampled 2 times. Once before and once after a celebration.</p>

<p>The goal is to investigate if this event (Celebration) as well as working with animals (sheepdog) have an influence on the probability that an individual gets infected by a parasite. (out of 1008 observations only 22 are found to be infected)</p>

<p><strong>Variables</strong></p>

<ul>
<li>dependent variable = ""T_hydat"" (infected or not) </li>
</ul>

<p>(most predictiv variables are categorical) </p>

<ul>
<li>""Celebration"" (yes/no)</li>
<li>""sex"" (m/f)</li>
<li>""RelAge"" (5 levels)</li>
<li>""SheepDog"" (yes/no)</li>
<li>""Area"" (geographical area = 4 levels)</li>
<li>""InfectionPeriodT_hydat"" (continuous --> Nr Days after deworming"")</li>
<li>""Urbanisation (3 levels)</li>
</ul>

<h2>Question 1:</h2>

<p>1) Should I include Individual-ID (""ID"") as a random Effekt as I sampled each Ind. 2 times? (Pseudoreplication?) </p>

<pre><code>mod_fail &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|ID), family = binomial)

Warnmeldungen:
1: In (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf,  :
  failure to converge in 10000 evaluations
2: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model failed to converge with max|grad| = 1.10808 (tol = 0.001, component 10)
3: In checkConv(attr(opt, ""derivs""), opt$par, ctrl = control$checkConv,  :
  Model is nearly unidentifiable: large eigenvalue ratio
 - Rescale variables?
</code></pre>

<p>--> unfortunately this model fails to converge (is it a problem that ID = 504 levels with only 2 observations per level?) 
Convergence is achieved with glmmPQL() but after droping some unsignficant preditiv variables the model fails to converge again ? What is the Problem here? Could geeglm() be a solution?</p>

<p>In an other attempt I run the model only with ""Area"" (4 levels) as random effect (my expectation is that Ind. in the same geogr. Area are suffering from the same parasite pressure etc.) and received the follwoing p-Values. </p>

<h2>My model in R:</h2>

<pre><code>mod_converges &lt;- glmer( T_hydat ~ Celebration + Sex + RelAge + SheepDog + InfectionPeriodT_hydat + Urbanisation + (1|Area), family = binomial)
</code></pre>

<h2>mod_converges output:</h2>

<p>summary(mod_converges)</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + sex + SheepDog + RelAge + Urbanisation +  
    InfectionPeriodT_hydat + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   203.0    262.0    -89.5    179.0      996 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.461 -0.146 -0.088 -0.060 31.174 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.314    0.561   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)            -6.81086    1.96027   -3.47  0.00051 ***
Celebration1            1.36304    0.57049    2.39  0.01688 *  
sexm                   -0.18064    0.49073   -0.37  0.71279    
SheepDog1               2.02983    0.51232    3.96  7.4e-05 ***
RelAge2                 0.34815    1.18557    0.29  0.76902    
RelAge3                 0.86344    1.05729    0.82  0.41412    
RelAge4                -0.54501    1.43815   -0.38  0.70471    
RelAge5                 0.85741    1.25895    0.68  0.49584    
UrbanisationU           0.17939    0.78669    0.23  0.81962    
UrbanisationV           0.01237    0.59374    0.02  0.98338    
InfectionPeriodT_hydat  0.00324    0.01159    0.28  0.77985    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1
</code></pre>

<h2>glmmPQL-Model output below (added after the first reply by usÎµr11852)</h2>

<p>This model converges with ""Sample_ID"" as a random effect, however, as statet by 
<strong>usÎµr11852</strong> the varaince of the random effect is quiet high  4.095497^2 = 16.8. And the std.error of Area5 is far to high (complete separation).
Can I just remove Datapoints from Area5 to overcome this Problem? </p>

<pre><code>#     T_hydat
#  Area   0   1
#     1 226   4
#     2 203   3
#     4 389  15
#     5 168   0  ## here is the problematic cell

Linear mixed-effects model fit by maximum likelihood
 Data: dat 
  AIC BIC logLik
   NA  NA     NA

Random effects:
 Formula: ~1 | Sample_ID
        (Intercept)  Residual
StdDev:    4.095497 0.1588054

Variance function:
 Structure: fixed weights
 Formula: ~invwt 
Fixed effects: T_hydat ~ Celebration + sex + SheepDog + YoungOld + Urbanisation +      InfectionPeriodT_hydat + Area 
                            Value Std.Error  DF    t-value p-value
(Intercept)            -20.271630     1.888 502 -10.735869  0.0000
Celebration1             5.245428     0.285 502  18.381586  0.0000
sexm                    -0.102451     0.877 495  -0.116865  0.9070
SheepDog1                3.356856     0.879 495   3.817931  0.0002
YoungOldyoung            0.694322     1.050 495   0.661017  0.5089
UrbanisationU            0.660842     1.374 495   0.480990  0.6307
UrbanisationV            0.494653     1.050 495   0.470915  0.6379
InfectionPeriodT_hydat   0.059830     0.007 502   8.587736  0.0000
Area2                   -1.187005     1.273 495  -0.932576  0.3515
Area4                   -0.700612     0.973 495  -0.720133  0.4718
Area5                  -23.436977 28791.059 495  -0.000814  0.9994
 Correlation: 
                       (Intr) Clbrt1 sexm   ShpDg1 YngOld UrbnsU UrbnsV InfPT_ Area2  Area4 
Celebration1           -0.467                                                               
sexm                   -0.355  0.018                                                        
SheepDog1              -0.427  0.079  0.066                                                 
YoungOldyoung          -0.483  0.017  0.134  0.045                                          
UrbanisationU          -0.273  0.005 -0.058  0.317 -0.035                                   
UrbanisationV          -0.393  0.001 -0.138  0.417 -0.087  0.586                            
InfectionPeriodT_hydat -0.517  0.804  0.022  0.088  0.016  0.007  0.003                     
Area2                  -0.044 -0.035 -0.044 -0.268 -0.070 -0.315 -0.232 -0.042              
Area4                  -0.213 -0.116 -0.049 -0.186 -0.023 -0.119  0.031 -0.148  0.561       
Area5                   0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000  0.000

Standardized Within-Group Residuals:
          Min            Q1           Med            Q3           Max 
-14.208465914  -0.093224405  -0.022551663  -0.004948562  14.733133744 

Number of Observations: 1008
Number of Groups: 504 
</code></pre>

<p><strong>Output from logistf (Firth's penalized-likelihood logistic regression)</strong></p>

<pre><code>logistf(formula = T_hydat ~ Celebration + sex + SheepDog + YoungOld + 
    Urbanisation + InfectionPeriodT_hydat + Area, data = dat, 
    family = binomial)

Model fitted by Penalized ML
Confidence intervals and p-values by Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood Profile Likelihood 

                               coef   se(coef)  lower 0.95  upper 0.95       Chisq
(Intercept)            -5.252164846 1.52982941 -8.75175093 -2.24379091 12.84909207
Celebration1            1.136833737 0.49697927  0.14999782  2.27716500  5.17197661
sexm                   -0.200450540 0.44458464 -1.09803320  0.77892986  0.17662930
SheepDog1               2.059166246 0.47197694  1.10933774  3.12225212 18.92002321
YoungOldyoung           0.412641416 0.56705186 -0.66182554  1.77541644  0.50507269
UrbanisationU           0.565030324 0.70697218 -0.98974390  1.97489240  0.56236485
UrbanisationV           0.265401035 0.50810444 -0.75429596  1.33772658  0.25619218
InfectionPeriodT_hydat -0.003590666 0.01071497 -0.02530179  0.02075254  0.09198425
Area2                  -0.634761551 0.74958750 -2.27274031  0.90086554  0.66405078
Area4                   0.359032194 0.57158464 -0.76903324  1.63297249  0.37094569
Area5                  -2.456953373 1.44578029 -7.36654837 -0.13140806  4.37267766
                                  p
(Intercept)            3.376430e-04
Celebration1           2.295408e-02
sexm                   6.742861e-01
SheepDog1              1.363144e-05
YoungOldyoung          4.772797e-01
UrbanisationU          4.533090e-01
UrbanisationV          6.127483e-01
InfectionPeriodT_hydat 7.616696e-01
Area2                  4.151335e-01
Area4                  5.424892e-01
Area5                  3.651956e-02

Likelihood ratio test=36.56853 on 10 df, p=6.718946e-05, n=1008
Wald test = 32.34071 on 10 df, p = 0.0003512978
</code></pre>

<p>** glmer Model (Edited 28th Jan 2016)**</p>

<p>Output from glmer2var: Mixed effect model with the 2 most ""important"" variables (""Celebration"" = the factor I am interested in and ""SheepDog"" which was found to have a significant influence on infection when data before and after the celebration were analysed separately.) The few number of positives make it impossible to fit a model with more than two explanatory variables (see commet EdM). </p>

<p>There seems to be a strong effect of ""Celebration"" that probably cancels out the effect of ""SheepDog"" found in previous analysis.</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Sample_ID)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   113.0    132.6    -52.5    105.0     1004 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-4.5709 -0.0022 -0.0001  0.0000 10.3491 

Random effects:
 Groups    Name        Variance Std.Dev.
 Sample_ID (Intercept) 377.1    19.42   
Number of obs: 1008, groups:  Sample_ID, 504

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -19.896      4.525  -4.397  1.1e-05 ***
Celebration1    7.626      2.932   2.601  0.00929 ** 
SheepDog1       1.885      2.099   0.898  0.36919    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.908       
SheepDog1   -0.297 -0.023
</code></pre>

<h2>Question 2:</h2>

<p>2) Can I use drop1() to get the final model and use the p-Values from summary(mod_converges) for interpretation? Does my output tell me if it makes sense to include the random effect (""Area"") ?</p>

<pre><code>Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']
 Family: binomial  ( logit )
Formula: T_hydat ~ Celebration + SheepDog + (1 | Area)
   Data: dat

     AIC      BIC   logLik deviance df.resid 
   190.8    210.4    -91.4    182.8     1004 

Scaled residuals: 
   Min     1Q Median     3Q    Max 
-0.369 -0.135 -0.096 -0.071 17.438 

Random effects:
 Groups Name        Variance Std.Dev.
 Area   (Intercept) 0.359    0.599   
Number of obs: 1008, groups:  Area, 4

Fixed effects:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)    -5.912      0.698   -8.47  &lt; 2e-16 ***
Celebration1    1.287      0.512    2.51    0.012 *  
SheepDog1       2.014      0.484    4.16  3.2e-05 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Correlation of Fixed Effects:
            (Intr) Clbrt1
Celebratin1 -0.580       
SheepDog1   -0.504  0.027
</code></pre>

<p>I know there are quite a few questions but I would really appreciate some advice from experienced people. Thanks!</p>
"
"0.217028683070608","0.202547873416733","191891","<p>I'm hoping someone can help clarify a few things for me.</p>

<p>I ran some relatively simple logistic regressions in r and am having trouble with interpretation.  I'm interested in the effects of elevation and a species diversity index on the presence/absence of a disease in individual animals.</p>

<p>I ran a simple model of: <code>Result~Elevation+Diversity</code> which gave this result</p>

<pre><code>Call:
glm(formula = Test_Result ~ Elevation + Simpsons_Diversity, family = binomial, 
    data = XXXXXX)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.8141  -0.6984  -0.5317  -0.4143   2.3337  

Coefficients:
                     Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)        -2.118e+00  1.594e-01 -13.289  &lt; 2e-16 
Elevation           1.316e-04  2.247e-05   5.855 4.76e-09 
Simpsons_Diversity -9.907e-01  2.725e-01  -3.635 0.000278 

    Null deviance: 3015.2  on 3299  degrees of freedom
Residual deviance: 2923.6  on 3297  degrees of freedom
AIC: 2929.6
</code></pre>

<p>I have a strong suspicion that diversity decreases with increasing elevation which I have confirmed although the relationship isn't quite as strong as I thought. When I run a model with an interaction term <code>elevation*diversity</code> I get:</p>

<pre><code>Call:
glm(formula = Test_Result ~ Elevation_1000 + Simpsons_Diversity_100 + 
    Elevation_1000 * Simpsons_Diversity_100, family = binomial, 
    data = XXXXXXX)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.7908  -0.6959  -0.5437  -0.3963   2.4215  

Coefficients:
                                       Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)                           -2.014422   0.179507 -11.222  &lt; 2e-16 
Elevation_1000                         0.112466   0.027433   4.100 4.14e-05 
Simpsons_Diversity_100                -0.015851   0.005780  -2.743   0.0061  
Elevation_1000:Simpsons_Diversity_100  0.001408   0.001200   1.173   0.2406   

    Null deviance: 3015.2  on 3299  degrees of freedom
Residual deviance: 2922.2  on 3296  degrees of freedom
AIC: 2930.2

Number of Fisher Scoring iterations: 5
</code></pre>

<p>Showing that adding the interaction term doesn't really help the fit of the model (AIC = 2930) and the interaction term itself is not significant (p-value=0.24).</p>

<p>Am I on the right track so far?</p>

<p>If I am, I understand how to convert coefficients to odds ratios and interpret those.  My main question is can I plot the predicted probabilities for a combination of elevation and diversity where each variable is allowed to vary? Or is this essentially plotting the interaction?  </p>

<p>I was able to create a dataframe where I varied elevation and diversity and I used my simple non-interaction model to obtain predicted probabilities using the PREDICT fuction) for those combinations, but I want to make sure that I am doing things correctly.  I've attached the plot of predicted probs for different levels of diversity. </p>

<p><a href=""http://i.stack.imgur.com/NINBE.gif"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NINBE.gif"" alt=""Elevation vs. Predicted Probabilities for various levels of diversity)""></a></p>
"
"0.200334168988253","0.219426862868128","191916","<p>I have taken plenty of time to try and help myself, but I keep reaching dead ends. </p>

<p>I have a dataset consisting of body measurements collected from a bird species, and the sex of each bird (known by molecular means). I built a logistic regression model (using the AIC information criterion) to assess which measurements explain better the sex of the birds. My ultimate goal is to have an equation which could be used by others under field conditions to predict reliably the sex of the birds by taking as few body measurements as possible. </p>

<p>My final model includes four independent variables, namely ""Culmen"", ""Head-bill"", ""Tarsus length"", and ""Wing length"" (all continuous). I wish my model was a little more parsimonious, but all the variables seem to be important according to AIC criterion. Because the model produced should be used as prediction tool, I decided validate it using a leave-one-out cross validation approach. In my learning process, I first tried to complete the analyses (cross-validation and plotting) by including only one explanatory variable, namely ""Culmen"". </p>

<p>The output of the cross validation (package ""boot"" in R) yields two values (deltas), which are the cross-validated prediction errors where the first number is the raw leave-one-out, or lieu cross-validation result, and the second one is a bias-corrected version of it. </p>

<pre><code>model.full &lt;- glm(Sex ~ Culmen, data = my.data, family = binomial)
summary(model.full.1)

cv.glm(my.data, model.full, K=114)

$call
cv.glm(data = my.data, glmfit = model.full, K = 114)

$K
[1] 114

$delta
[1] 0.05941851 0.05937288
</code></pre>

<p>Q1. Could anyone expalin what do these two values represent and how to interpret them?    </p>

<p>Following is the code as presented by Dr. Markus MÃ¼ller (Calimo) in a similar, albeit not identical, post (<a href=""http://stackoverflow.com/questions/20346568/feature-selection-cross-validation-but-how-to-make-roc-curves-in-r"">http://stackoverflow.com/questions/20346568/feature-selection-cross-validation-but-how-to-make-roc-curves-in-r</a>) which I tried to tweak to meet my data:</p>

<pre><code>library(pROC)
data(my.data)
k &lt;- 114    # Number of observations or rows in dataset
n &lt;- dim(my.data)[1]
indices &lt;- sample(rep(1:k, ceiling(n/k))[1:n])

all.response &lt;- all.predictor &lt;- aucs &lt;- c()
for (i in 1:k) {
test = my.data[indices==i,]
learn = my.data[indices!=i,]
model &lt;- glm(Sex ~ Culmen, data = learn, family=binomial)
model.pred &lt;- predict(model, newdata=test)
aucs &lt;- c(aucs, roc(test$Sex, model.pred)$auc)
all.response &lt;- c(all.response, test$outcome)
all.predictor &lt;- c(all.predictor, model.pred)
}

Error in roc.default(test$Sex, model.pred) : No case observation.

roc(all.response, all.predictor)

Error in roc.default(all.response, all.predictor) : No valid data provided.

mean(aucs)
</code></pre>

<p>Q2. What's the reason for the first error message? I guess the second error is associated with the first one, and that it will be solved once I find a solution to the first one.</p>

<p>I will appreciate very much any help!!</p>

<p>Luciano </p>
"
"0.120385853085769","0.121716123890037","194140","<p>I've been using stepAIC to narrow down my logistic regression model.  However, I get the following warning when I run my model:</p>

<p>glm.fit: fitted probabilities numerically 0 or 1 occurred</p>

<p>I know this means I have complete or quasi-complete separation in my data.  On examination of my data, I see the quasi-complete separation and think that it's meaningful.  Reading online, I see recommendations to use a Firth penalized regression (logistf) or exact logistic regression (elrm); but neither of these will work with stepAIC.  I've also tried bayesglm but I still get the same warning. </p>

<p>How should I select a model when my data has complete separation?  How would I do this in R?  Is my mistake in my stats or in my understanding of using the packages in R?  Any help would be much appreciated!</p>
"
"NaN","NaN","196734","<p>I have a panel data set with binary dependent variable of 20,000 observations and 11 independent variables.  I ran a logistic regression with fixed effects and the model returns maximum log likelihood value of <code>-7417.845</code> and AIC equals <code>Inf</code>. I am not sure why here the AIC value goes to infinity?</p>

<p>I am using R package ""<a href=""https://cran.r-project.org/web/packages/glmmML/glmmML.pdf"" rel=""nofollow"">glmmML</a>"".</p>
"
"0.199637352376173","0.201843356939833","198801","<p>Logistic Regression using R's <code>glm</code> package is giving me the following summary (snap of the few first variables). </p>

<p><strong>My Data Set:</strong></p>

<ul>
<li>Dimensions: 1252 rows and 224 columns (after using model.matrix). The Data  has been standardized.</li>
<li>Response variable is binary.</li>
<li>Trying to predict if an employee will leave the company, based on employee attributes </li>
</ul>

<p><a href=""http://i.stack.imgur.com/Ai42v.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Ai42v.png"" alt=""enter image description here""></a></p>

<p><strong>My Understanding:</strong></p>

<p>The model does not give a good fit because:</p>

<ol>
<li>Residual Deviance > Null Deviance. </li>
<li>p.value = 1 - pchisq(3676.5, 817) turns out to be 0.</li>
<li>The first warning about 0 or 1 fitted probability message suggests that due to some predictor(s) the model might be giving perfect predictions </li>
<li>Second warning on â€˜rank deficiencyâ€™ suggests that there might be predictors that are linearly dependent on one another.</li>
</ol>

<p><strong>My Questions:</strong></p>

<ol>
<li>How can I improve the model? I would like to see Residual Deviance &lt; Null Deviance. I will invest time on dropping the linearly dependent variables in the model, but is there anything I should do first to test the â€˜modelâ€™ itself, before revisiting my data? I am asking this because SVM worked quite well on the same data set. </li>
<li>Why do I have such extreme coefficient values?</li>
<li>Many answers to other posts state that â€˜AICâ€™ is used to compare different</li>
<li>The summary parameters (coefficients , std error and p-values) for many dummy factors obtained via model.matrix, like GSS_SEXM,  is shown as 'NA'. Why is it so? </li>
<li>logistic models. What is meant by â€˜differentâ€™ here? Models trained on different data sets that bear different coefficients, like say different set of attributes? </li>
</ol>
"
"0.0601929265428846","0.0608580619450185","199284","<p>I have a large dataset with 4000 variables and 20000 observations. Most of the variables are a variety of moving averages since I am trying to create a predictive model. I'd like to use logistic regression with the best of these 4000 variables. I know this is an absurd amount and I am looking to trim it down to less than ten or so but am unsure of the best way to do it. </p>

<p>I'm aware that using stepAIC or the like will take forever and I need something that can select variables based on the p value or any other criteria.</p>

<p>Any help is appreciated.</p>
"
"NaN","NaN","199970","<p>I have been provided a sample logistic regression as follows:</p>

<p><code>glm(formula = output ~ X1 + X2 + X3 + X4 + X5 + X1:term + term:X5 - 1, family=""binomial"", data=mydata)</code></p>

<p>There are a few things I'm confused by here:</p>

<p>1) What is going on with the X1:term + term:X5 terms? What do they mean in the context of glm()?</p>

<p>2) There does not seem to be an intercept term in the output under <code>Coefficients</code>. Could this be for any other reason than there simply not being an intercept term?</p>

<p>3) The AIC for the model is 50000. How should I interpret this? Can I interpret this without more models to compare to? If it is not useful, what else should I be looking for instead?</p>
"
"0.147441956154897","0.149071198499986","200031","<p>I have very easy question that I'm hoping someone can assist me with:</p>

<p>I ran an example logistic regression using this R code:</p>

<pre><code>     hours &lt;- c(0.5, 0.75, 1, 1.25, 1.5, 1.75, 1.75, 2, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 4, 4.25, 4.5, 4.75, 5, 5.5)
        pass &lt;- c(0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1)
        data &lt;- data.frame(hours, pass)
        mylogit &lt;- glm(pass ~ hours, data = data, family = ""binomial"") #Activates the logistic regression model
        summary(mylogit) #Summary of the model

    Call:
    glm(formula = pass ~ hours, family = ""binomial"", data = data)

    Deviance Residuals: 
         Min        1Q    Median        3Q       Max  
    -1.70557  -0.57357  -0.04654   0.45470   1.82008  

    Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)  
    (Intercept)  -4.0777     1.7610  -2.316   0.0206 *
    hours         1.5046     0.6287   2.393   0.0167 *
    ---
    Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

    (Dispersion parameter for binomial family taken to be 1)

        Null deviance: 27.726  on 19  degrees of freedom
    Residual deviance: 16.060  on 18  degrees of freedom
    AIC: 20.06

    Number of Fisher Scoring iterations: 5

    round(exp(cbind(OR = coef(mylogit), confint(mylogit))),3)

               OR 2.5 % 97.5 %
   (Intercept) 0.017 0.000  0.281
    hours       4.503 1.698 23.223
</code></pre>

<p>I know that by taking the exponent of the log-odds/coefficients for hours the odds of passing increase by a factor of 4.503 for a one-unit change in hours.  However, given that the explanatory variable (hours) is continuous, what is considered a 'one-unit change' i.e. going from 1 to 2 hours as one unit? or from 1.75 to 1.76 hours as one unit?  Also, is this interpretation of one-unit the same for regular OLS regression as well? I'm seeking to better understand the rules R applies to creating its regression coefficients.  </p>
"
"0.104257207028537","0.105409255338946","201462","<p>I'm fitting a logistic regression model with <code>patient_group</code> (0,1) as response variable and the explanatory variable being an interaction between two SNPs. When running summary for the model, the alert 'Coefficients: (1 not defined because of singularities)' is shown, and I guess it is due to the fact that the combination AACT has 0 observations. </p>

<p>My question is whether the statistics are still valid, or is there a better way to analyse this kind of data? (The SNPs are located close to each other and are most likely strongly linked.)</p>

<pre><code>&gt; table(data$SNP1, data$SNP2)    
     CC CT
  TT 27  9
  AT 83 14
  AA 47  0
&gt; model &lt;- glm(patient_group ~ SNP1 * SNP2, data=data, family=""binomial"")
&gt; summary(model)
Call:
glm(formula = patient_group ~ SNP1 * SNP2, family = ""binomial"", 
data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.2735  -0.9072  -0.7679   1.4742   1.8365  

Coefficients: (1 not defined because of singularities)
              Estimate Std. Error z value Pr(&gt;|z|)   
(Intercept)    -1.4816     0.4954  -2.991  0.00279 **
SNP1AT          0.8065     0.5471   1.474  0.14048   
SNP1AA          0.4112     0.5978   0.688  0.49158   
SNP2CT          1.7047     0.8339   2.044  0.04093 * 
SNP1AT:SNP2CT  -2.3289     1.0833  -2.150  0.03157 * 
SNP1AA:SNP2CT       NA         NA      NA       NA   

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 218.19  on 179  degrees of freedom
Residual deviance: 212.31  on 175  degrees of freedom
(26 observations deleted due to missingness)
AIC: 222.31

Number of Fisher Scoring iterations: 4
</code></pre>
"
"0.147441956154897","0.149071198499986","206039","<p>I am looking at a logistic regression model for predicting hospital acquired infection likelihood (HAI) from predictors of whether germs are found on the  x number of patients (Patient), x number of environmental spots (Env), x number of air samples (Air) or x number of nurses' hands (Hand).</p>

<pre><code>   Month Patient Env Air Hand HAI HAIcat BedOccupancy
      1       4   0   0    1   1    yes            9
      2       2   0   2    0   0     no            9
      3       2   1   0    1   0     no            5
      4       1   2   0    2   2    yes            7
      5       2   3   0    1   1    yes            6
      6       1   2   0    0   1    yes            5
      7       4   0   0    2   1    yes            7
      8       2   0   0    1   3    yes            7
      9       3   2   2    0   1    yes            8
     10       3   0   0    1   1    yes            8
</code></pre>

<p>For example for Month 1, the percentage of HAI would be HAI/BedOccupancy=1/9.
So I'd like to know if bed occupancy or other contamination is significant in predicting HAI. I run a Logistic regression, but it says it's junk. What does a statistician do now?</p>

<pre><code>model&lt;-glm(cbind(MR$HAI,MR$BedOccupancy)~MR$Patient+MR$Env+MR$Air+MR$Hand,family = ""binomial"")
</code></pre>

<p>But I get a bad fit and non-significant correlation:</p>

<pre><code>Call:
glm(formula = cbind(MR$HAI, MR$BedOccupancy) ~ MR$Patient + MR$Env + MR$Air + 
        MR$Hand, family = ""binomial"")

Deviance Residuals: 
       1         2         3         4         5         6         7         8         9        10  
-0.12882  -1.08046  -1.33787   0.01400  -0.10685  -0.02229  -0.04008   1.03688   0.75723  -0.23824  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -1.30758    1.34049  -0.975    0.329
MR$Patient  -0.22920    0.39350  -0.582    0.560
    MR$Env      -0.02415    0.37672  -0.064    0.949
MR$Air      -0.46851    0.64611  -0.725    0.468
    MR$Hand      0.16054    0.58277   0.275    0.783

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 6.6594  on 9  degrees of freedom
Residual deviance: 4.6929  on 5  degrees of freedom
AIC: 30.911

Number of Fisher Scoring iterations: 5
</code></pre>
"
"0.134595475514541","0.136082763487954","206702","<p>I have some data to fit a logistic regression, although the data seems quite good, the resulted fit does not look as expected.</p>

<blockquote>
<pre><code>  paramValue      normality
1  3.69             0
2  1.16             0
3  6.12             1
4  2.78             1
5  1.45             1
6  3.56             0
</code></pre>
</blockquote>

<pre><code>mylogit &lt;- glm(normality ~paramValue,  family = binomial(link=""logit""))
summary(mylogit)
</code></pre>

<blockquote>
<pre><code>Call:
glm(formula = normality ~ paramValue, family = binomial(link = ""logit""))

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-2.44994  -0.73312   0.08151   0.63377   1.41140  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)  -1.9945     0.9531  -2.093   0.0364 *
paramValue    1.2582     0.5655   2.225   0.0261 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 29.065  on 20  degrees of freedom
Residual deviance: 19.746  on 19  degrees of freedom
AIC: 23.746

Number of Fisher Scoring iterations: 5
</code></pre>
</blockquote>

<pre><code>    plot(paramValue,normality)

    x &lt;- seq(-1, 6, 0.1)

curve(predict(mylogit,data.frame(paramValue=x),type=""response""),add=TRUE, col=""red"")
</code></pre>

<p><a href=""http://i.stack.imgur.com/1f0DY.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/1f0DY.png"" alt=""enter image description here""></a></p>

<p>Did I do something wrong? Is there any way to force the regression to cross the origin?</p>
"
"0.0601929265428846","0.0608580619450185","206735","<p>I've created an example table (just in order to create a function) with:</p>

<pre><code>ex&lt;-data.frame(b=c(rep('A',50),rep('B',30), rep('C',20)), 
fl=round(runif(100,0,1),0),r=runif(100,0,0.5))
ex2&lt;-cbind(ex,model.matrix(~b-1,ex))
lineal&lt;-ex2$bB+ex2$bA*ex$fl+ex$fl
ex$clase&lt;-round(1/(1+exp(-lineal)),0)
</code></pre>

<p>Then I run a logistic regression model (MASS library)</p>

<pre><code>fm&lt;-as.formula(clase~b+fl+r)
modT&lt;-glm(clase~1, family=binomial, data = ex)
modT&lt;-stepAIC(modT, scope = fm, family=binomial, data =ex, k = 4)
summary(modT)
</code></pre>

<p>As you can see coefficients are not significant, but I've created the class using them. So I don't understand why this is happening.</p>

<p><a href=""http://i.stack.imgur.com/yR4jV.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/yR4jV.png"" alt=""enter image description here""></a></p>
"
"0.0601929265428846","0.0608580619450185","209374","<p>I'm using the <code>multinom</code> package in R to run a multinomial logistic regression model. My dependent variable has 3 levels and as the output, I'm getting the probability for each of the level.</p>

<p>Currently, I have the VIF, AIC, p-values and confusion matrix in the model.</p>

<p>I have the following questions:</p>

<ol>
<li><p>I want a single output based on the probabilities. How do I decide a ""cut-off"" for deciding the ""best event""?</p></li>
<li><p>Does it make sense to get an ROC curve here? If yes, then how do I get one?</p></li>
<li><p>What are the things I should look at for the validation of the model?</p></li>
</ol>
"
"0.134595475514541","0.136082763487954","212213","<p>I have a problem with this ""-inf"" value in my table and I don't understand where it's coming from. As long as it's there, I cannot run a proportional odds assumption check. Here is my model, I used the MASS package:</p>

<pre><code>    m27 &lt;- polr(typ ~ bek2 + vst2 + verw2 + nwoe2, data = typmed23, method=""logistic"", Hess=T)

    summary(m27)
    Call: polr(formula = typ ~ bek2 + vst2 + verw2 + nwoe2, data = typmed23, Hess = T, method = ""logistic"")

    Coefficients:
        Value Std. Error t value
bek2   0.4620     0.2705  1.7080
vst2   0.1169     0.3217  0.3635
verw2 -0.7230     0.2580 -2.8028
nwoe2  1.8877     0.2791  6.7626

Intercepts:
     Value   Std. Error t value
1|2  2.9883  1.1341     2.6349
2|3  5.2964  1.1764     4.5021

Residual Deviance: 373.3716 
AIC: 385.3716 
</code></pre>

<p>Here is my code for the table:</p>

<pre><code>    sf &lt;- function(y) {
      c('Y&gt;=1' = qlogis(mean(y &gt;= 1)),   #typ outcome
        'Y&gt;=2' = qlogis(mean(y &gt;= 2)),
        'Y&gt;=3' = qlogis(mean(y &gt;= 3)))
    }
    s &lt;- with(typmed23, summary(as.numeric(typ) ~ bek2 + vst2 + verw2 + nwoe2, fun=sf))  
</code></pre>

<p>which produces this table:</p>

<pre><code>    as.numeric(typ)    N=244

    +-------+-+---+----+----------+-----------+
    |       | |N  |Y&gt;=1|Y&gt;=2      |Y&gt;=3       |
    +-------+-+---+----+----------+-----------+
    |bek2   |1|104|Inf | 0.6359888|-0.63598877|
    |       |2|128|Inf | 0.7884574|-0.54430155|
    |       |3| 12|Inf | 0.0000000|-1.60943791|
    +-------+-+---+----+----------+-----------+
    |vst2   |1| 55|Inf | 1.7707061| 0.10919929|
    |       |2|148|Inf | 0.8602013|-0.55431074|
    |       |3| 41|Inf |-1.0033021|-2.97041447|
    +-------+-+---+----+----------+-----------+
    |verw2  |1| 77|Inf | 3.6243409| 0.73236789|
    |       |2| 76|Inf | 1.1700713|-0.89794159|
    |       |3| 91|Inf |-0.7598386|-1.98413136|
    +-------+-+---+----+----------+-----------+
    |nwoe2  |1| 58|Inf |-2.3608540|       -Inf|
    |       |2| 40|Inf |-0.1000835|-0.96940056|
    |       |3|146|Inf | 2.8478121| 0.02739897|
    +-------+-+---+----+----------+-----------+
    |Overall| |244|Inf | 0.6808771|-0.62625295|
    +-------+-+---+----+----------+-----------+
</code></pre>

<p>My dataframe is fine: No NA or huge values, no empty cells, only 5 columns containing values between 1 and 3. Can somebody tell me what to do and how to solve the problem? </p>

<p>Here is the variable (column in a df) </p>

<pre><code>nwoe2: typmed23$nwoe2
 [1] 3 3 3 3 3 3 3 1 1 1 3 3 3 2 3 3 2 2 1 2 1 3 3 3 2 2 1 2 3 3 3 1 3 3 3
 [36] 3 2 3 2 3 3 3 2 3 1 3 3 1 3 1 3 3 3 2 1 1 3 3 2 1 1 1 3 3 2 3 3 3 2 1
 [71] 1 3 3 3 3 1 2 3 1 3 1 1 3 3 3 3 3 2 2 3 3 3 3 3 3 3 3 3 1 3 3 3 3 3 3
 [106] 3 3 3 1 3 3 3 3 3 1 2 2 1 3 3 2 3 3 3 3 1 1 1 1 3 3 3 2 3 2 1 3 3 3 3
 [141] 1 2 3 1 3 3 3 3 3 1 3 3 3 1 2 2 3 1 3 3 1 2 3 2 2 1 3 3 3 3 3 1 1 3 3
 [176] 1 2 3 3 3 2 1 1 2 3 3 3 1 3 3 1 1 3 3 3 2 2 2 3 2 3 3 1 1 3 1 2 3 3 2
 [211] 3 1 3 3 2 3 3 1 3 3 1 1 3 3 3 3 3 3 3 1 1 3 1 3 3 3 2 3 3 2 1 1 1 3
</code></pre>

<p>Just to avoid any confusion: I'm referring to the lonely ""-Inf"" in column ""Y>=3"", not to the column ""Y>=1"".</p>
"
"0.147441956154897","0.149071198499986","212301","<p>I have a huge doubt, which I believe is Basic. I have no difficulty in interpreting the results of our logistic regression model using the ODD ratio, but I do not know what to do when I work with Mixed effects model for longitudinal data.</p>

<p>Below they use the <code>glmer</code> function to estimate a mixed effects logistic regression model with Il6, CRP, and LengthofStay as patient level continuous predictors, CancerStage as a patient level categorical predictor (I, II, III, or IV), Experience as a doctor level continuous predictor, and a random intercept by DID, doctor ID.</p>

<p>The <code>glmer</code> function created 407 groups that refer to the number of doctors.</p>

<p>What would it mean for example the -0.0568 of IL6 and the -2.3370 of CancerStageIV's in the study presented?</p>

#################

<p>m &lt;â€ glmer(remission ~ IL6 + CRP + CancerStage + LengthofStay + Experience +      (1 | DID), data = hdp, family = binomial, control = glmerControl(optimizer =  ""bobyqa""),      nAGQ = 10) 
print(m, corr = FALSE) </p>

<h1>Generalized linear mixed model fit by maximum likelihood</h1>

<h2>Gauss-Hermite Quadrature, nAGQ = 10) [glmerMod]</h2>

<h2>Family:</h2>

<p>binomial ( logit )  </p>

<h2>Formula:</h2>

<p>remission ~ IL6 + CRP + CancerStage + LengthofStay + Experience +<br>
   (1 | DID)  </p>

<p>Data: hdp  </p>

<pre><code>  AIC        BIC    logLik     deviance  df.resid   
 7397        7461    -3690        7379     8516 
</code></pre>

<h2>Random effects:</h2>

<p>Groups Name         Std.Dev.<br>
     DID    (Intercept) 2.01 </p>

<p>Number of obs: 8525, groups: DID, 407  </p>

<h1>Fixed Effects:</h1>

<pre><code>  Intercept    IL6        CRP       CancerStageII  
 â€2.0527     â€0.0568    â€0.0215       â€0.4139 

CancerStageIII   CancerStageIV       LengthofStay      Experience  
 â€1.0035           â€2.3370              â€0.1212          0.1201 
</code></pre>
"
"0.180578779628654","0.182574185835055","212611","<p>I have ran these two Logistic Regression models (below) on some small data and I am able to interpret the output - significance and direction - of the regressors, but I do not know for sure how to interpret all the data which is supposed to tell me everything related to <strong>effect (size) etc</strong>. I did select my predictors properly by adding one each time and checking whether the model was still significant (which yielded the same result as an automatic stepAIC from the MASS package) and I also did some diagnostic checks (outliertest, VIF-score).</p>

<p>What (I think) I got from the models is:</p>

<ul>
<li><strong>R2</strong>: model1 only explains 4.8% of all variation and model2 6.6%, so no predictive power?</li>
<li><strong>C</strong>: model1 does not have acceptable discrimination, neither does model2 (&lt;0.7)</li>
</ul>

<p>Is there other <strong>important information that I am ignorant of</strong>? It seems that these models do <strong>not have much 'power'</strong> (according to <strong>R2</strong> and <strong>C</strong>), but how are they then <strong>still significant</strong> (there is also very significant behaviour (***) for regressors)?</p>

<p>*PS: Sorry if am missing obvious things - I do not have that strong of a statistical background. I am also finding it a hard time searching for all the parameters and metrics since they are often denoted by a one letter name (e.g. C, g) - which is not easy to search for if you do not know what you are looking for... So that's why I came to CrossValidated!</p>

<p>I have found <a href=""http://stats.stackexchange.com/questions/104485/logistic-regression-evaluation-metrics"">this question</a>, but it does not really have an answer since it's maybe way too vague? If someone else has a reading suggestion for my problem, that's also welcomed!*</p>

<h2>First model: Agentivity ~ Period + Genre</h2>

<pre><code>(from lrm)      
                     Model Likelihood      Discrimination    Rank Discrim.    
                       Ratio Test            Indexes           Indexes       
Obs          700    LR chi2      25.55    R2       0.048    C       0.602    
 strong      403    d.f.             4    g        0.440    Dxy     0.204    
 weak        297    Pr(&gt; chi2) &lt;0.0001    gr       1.553    gamma   0.240    
max |deriv| 3e-14                         gp       0.105    tau-a   0.100    
                                          Brier    0.236                     

(from glm)
Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.3171  -0.9825  -0.8094   1.1882   1.6090  

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 954.29  on 699  degrees of freedom
Residual deviance: 928.74  on 695  degrees of freedom
AIC: 938.74

Number of Fisher Scoring iterations: 4
</code></pre>

<h2>Second model: Type ~ Period</h2>

<pre><code>(from lrm)
                      Model Likelihood      Discrimination    Rank Discrim.    
                         Ratio Test           Indexes           Indexes       
Obs           872    LR chi2      36.70    R2       0.066    C       0.637    
 mediopassive 701    d.f.             2    g        0.552    Dxy     0.275    
 passive      171    Pr(&gt; chi2) &lt;0.0001    gr       1.736    gamma   0.401    
max |deriv| 9e-10                          gp       0.087    tau-a   0.087    
                                           Brier    0.151                     

(for glm)
Deviance Residuals: 
 Min       1Q   Median       3Q      Max  
-0.8645  -0.6109  -0.4960  -0.4960   2.0767  

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 863.19  on 871  degrees of freedom
Residual deviance: 826.49  on 869  degrees of freedom
AIC: 832.49

Number of Fisher Scoring iterations: 4
</code></pre>
"
"0.180578779628654","0.162288165186716","212903","<p>I have the data <a href=""https://docs.google.com/spreadsheets/d/1lEzUt0QdFCp1ho-iWd4HzEIZoo8IyAM8YP2gu-K7BQo/edit?usp=sharing"" rel=""nofollow"">here</a>.But When I tried to build the logistic regression model using glm function its shows NA in TotalVisits. I have found similar question on stack overflow but that is answered for linear model.  </p>

<pre><code> str(quality)
'data.frame':   131 obs. of  14 variables:
 $ MemberID            : int  1 2 3 4 5 6 7 8 9 10 ...
 $ InpatientDays       : int  0 1 0 0 8 2 16 2 2 4 ...
 $ ERVisits            : int  0 1 0 1 2 0 1 0 1 2 ...
 $ OfficeVisits        : int  18 6 5 19 19 9 8 8 4 0 ...
 $ Narcotics           : int  1 1 3 0 3 2 1 0 3 2 ...
 $ DaysSinceLastERVisit: num  731 411 731 158 449 ...
 $ Pain                : int  10 0 10 34 10 6 4 5 5 2 ...
 $ TotalVisits         : int  18 8 5 20 29 11 25 10 7 6 ...
 $ ProviderCount       : int  21 27 16 14 24 40 19 11 28 21 ...
 $ MedicalClaims       : int  93 19 27 59 51 53 40 28 20 17 ...
 $ ClaimLines          : int  222 115 148 242 204 156 261 87 98 66 ...
 $ StartedOnCombination: logi  FALSE FALSE FALSE FALSE FALSE FALSE ...
 $ AcuteDrugGapSmall   : int  0 1 5 0 0 4 0 0 0 0 ...
 $ PoorCare            : int  0 0 0 0 0 1 0 0 1 0 ...



table(is.na(quality))
FALSE 
1834
</code></pre>

<p>My data does not contain any NA values.</p>

<pre><code>set.seed(100)
split &lt;- sample.split(quality$PoorCare, SplitRatio = .5)
train &lt;-subset(quality, split ==TRUE)
test &lt;- subset(quality, split ==FALSE)
</code></pre>

<p>Building the model using all variable </p>

<pre><code>log.Quality &lt;- glm(PoorCare ~ ., data = train, family = 'binomial')

summary(log.Quality)      
Call:
glm(formula = PoorCare ~ ., family = ""binomial"", data = train)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.5679  -0.6384  -0.3604  -0.1154   2.1298  

Coefficients: (1 not defined because of singularities)
                          Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)              -3.583178   1.807020  -1.983   0.0474 *
MemberID                 -0.008742   0.010988  -0.796   0.4263  
InpatientDays            -0.106578   0.095632  -1.114   0.2651  
ERVisits                  0.275225   0.310364   0.887   0.3752  
OfficeVisits              0.126433   0.066140   1.912   0.0559 .
Narcotics                 0.190862   0.106890   1.786   0.0742 .
DaysSinceLastERVisit     -0.001221   0.002026  -0.603   0.5467  
Pain                     -0.020104   0.023057  -0.872   0.3832  
TotalVisits                     NA         NA      NA       NA  
ProviderCount             0.046297   0.040637   1.139   0.2546  
MedicalClaims             0.025123   0.030564   0.822   0.4111  
ClaimLines               -0.010384   0.012746  -0.815   0.4152  
StartedOnCombinationTRUE  2.205058   1.724923   1.278   0.2011  
AcuteDrugGapSmall         0.217813   0.139890   1.557   0.1195  
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 72.549  on 64  degrees of freedom
Residual deviance: 49.213  on 52  degrees of freedom
AIC: 75.213

Number of Fisher Scoring iterations: 6
</code></pre>

<p>Can anyone provide me a good explanation why this is happening ? </p>
"
"0.104257207028537","0.105409255338946","214892","<p>I'm trying to construct a univariate prediction model using logistic regression in order to predict credit default likelihood from overdue level in telecommunication companies:</p>

<p><a href=""https://drive.google.com/open?id=0BzdYGYN6vfqBeDdKRDFpelFKbTg"" rel=""nofollow"">https://drive.google.com/open?id=0BzdYGYN6vfqBeDdKRDFpelFKbTg</a></p>

<p>For this, I used the function glm and found two problematic ranks:
        RANK_OVERDUE between S/. 3,000 and S/. 5,000 &amp; RANK_OVERDUE More than S/. 5,000.</p>

<p>which have p-values of 0.946 and 0.473:</p>

<pre><code>Call:
glm(formula = impago ~ MONTO_VENCIDO_DOC_IMPAGOS, family = binomial, 
    data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.1355  -0.0569  -0.0569  -0.0569   3.5855  

Coefficients:
                                                  Estimate
(Intercept)                                       -6.42627
RANK_OVERDUE&lt;S/. 0 - S/. 500]         0.69763
RANK_OVERDUE&lt;S/. 1,500 - S/. 3,000]   1.73952
RANK_OVERDUE&lt;S/. 3,000 - S/. 5,000] -10.13980
RANK_OVERDUE&lt;S/. 500 - 1,500]         1.13854
RANK_OVERDUEMÃ¡s de S/. 5,000          0.71916
</code></pre>

<p></p>

<pre><code>                                                 Pr(&gt;|z|)    
(Intercept)                                       &lt; 2e-16 ***
RANK_OVERDUE&lt;S/. 0 - S/. 500]       1.78e-15 ***
RANK_OVERDUE&lt;S/. 1,500 - S/. 3,000] 2.51e-05 ***
RANK_OVERDUE&lt;S/. 3,000 - S/. 5,000]    0.946    
RANK_OVERDUE&lt;S/. 500 - 1,500]       1.23e-06 ***
RANK_OVERDUEMÃ¡s de S/. 5,000           0.473    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 9683.9  on 345828  degrees of freedom
Residual deviance: 9603.5  on 345823  degrees of freedom
AIC: 9615.5

Number of Fisher Scoring iterations: 15
</code></pre>

<p>I would need to know what options I have on order to deal with this situation. Should these ranks be included in the model? I tried to join them into one (overdue over S/. 3,000) but when applying again the model, it continued to be not significant (I obtained a p-value of 0.919).</p>
"
"0.0601929265428846","0.0608580619450185","216119","<p>I am studying how well Kobe Bryant shoots and to do so I have run a logistic regression. The variable shot_made_flag is 0 if missed and 1 if he scored. And I am running the regression against distance from the basket.</p>

<pre><code>  logitshots &lt;- glm(df$shot_made_flag ~ df$shot_distance, family = binomial(link=""logit""))
Call:  glm(formula = df$shot_made_flag ~ df$shot_distance, family = binomial(link = ""logit""))

Coefficients:
 (Intercept)  df$shot_distance  
      0.3681           -0.0441  

Degrees of Freedom: 25696 Total (i.e. Null);  25695 Residual
Null Deviance:      35330 
Residual Deviance: 34290    AIC: 34300
</code></pre>

<p>As you see the coefficient of distance is negative. So what I do next is to compute the probability of scoring if Bryant is 1 meter farther. </p>

<p>To do so I have done it this way, but I get a positive effect, so I am not sure about it. </p>

<pre><code>(exp(coef(logitshots))/(1+exp(coef(logitshots))))
(Intercept) df$shot_distance 
   0.5909933        0.4889768 
</code></pre>

<p>So how would you interpret this? every 1 meter means a 48% more chances of scoring (Lol)? Is this approach the right one? I guess that Kobe scoring from 25 meters is very unlikely (maybe modelling by a quadratic function?)  </p>

<p>I'd really appreciate any interesting insight and help! :)</p>
"
"0.241189350381645","0.258198889747161","216122","<p>As far as I know, the difference between logistic model and fractional response model (frm) is that the dependent variable (Y) in which frm is [0,1], but logistic is {0, 1}. Further, frm uses the quasi-likelihood estimator to determine its parameters. </p>

<p>Normally, we can use <code>glm</code> to obtain the logistic models by <code>glm(y ~ x1+x2, data = dat, family = binomial(logit))</code>. </p>

<p>For frm, we change <code>family = binomial(logit)</code> to <code>family = quasibinomial(logit)</code>.  </p>

<p>I noticed we can also use <code>family = binomial(logit)</code> to obtain frm's parameter since it gives the same estimated values. See the following example</p>

<pre><code>library(foreign)
mydata &lt;- read.dta(""k401.dta"")


glm.bin &lt;- glm(prate ~ mrate + age + sole + totemp, data = mydata
,family = binomial('logit'))
summary(glm.bin)
</code></pre>

<p>return,</p>

<pre><code>Call:
glm(formula = prate ~ mrate + age + sole + totemp, family = binomial(""logit""), 
    data = mydata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.1214  -0.1979   0.2059   0.4486   0.9146  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)  1.074e+00  8.869e-02  12.110  &lt; 2e-16 ***
mrate        5.734e-01  9.011e-02   6.364 1.97e-10 ***
age          3.089e-02  5.832e-03   5.297 1.17e-07 ***
sole         3.636e-01  9.491e-02   3.831 0.000128 ***
totemp      -5.780e-06  2.207e-06  -2.619 0.008814 ** 
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1166.6  on 4733  degrees of freedom
Residual deviance: 1023.7  on 4729  degrees of freedom
AIC: 1997.6

Number of Fisher Scoring iterations: 6
</code></pre>

<p>And for <code>family = quasibinomial('logit')</code>,</p>

<pre><code>glm.quasi &lt;- glm(prate ~ mrate + age + sole + totemp, data = mydata
,family = quasibinomial('logit'))
summary(glm.quasi)
</code></pre>

<p>return,</p>

<pre><code>Call:
glm(formula = prate ~ mrate + age + sole + totemp, family = quasibinomial(""logit""), 
    data = mydata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-3.1214  -0.1979   0.2059   0.4486   0.9146  

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  1.074e+00  4.788e-02  22.435  &lt; 2e-16 ***
mrate        5.734e-01  4.864e-02  11.789  &lt; 2e-16 ***
age          3.089e-02  3.148e-03   9.814  &lt; 2e-16 ***
sole         3.636e-01  5.123e-02   7.097 1.46e-12 ***
totemp      -5.780e-06  1.191e-06  -4.852 1.26e-06 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for quasibinomial family taken to be 0.2913876)

    Null deviance: 1166.6  on 4733  degrees of freedom
Residual deviance: 1023.7  on 4729  degrees of freedom
AIC: NA

Number of Fisher Scoring iterations: 6
</code></pre>

<p>The estimated Beta from both <code>family</code> are the same, but the difference is the SE values.  However, to obtain the correct SE, we have to use <code>library(sandwich)</code> as in this <a href=""http://stackoverflow.com/questions/37584715/fractional-response-regression-in-r"">post</a>.</p>

<p>Now, my questions:</p>

<ol>
<li>What is the difference between these two codes?</li>
<li>Is frm about to obtain robust SE?</li>
</ol>

<p>If my understanding is not correct, please give some suggestions.</p>
"
"0.120385853085769","0.121716123890037","219684","<p>I am trying to create a logistic regression model to predict whether a customer given a loan will be a bad or a good customer: bad meaning missing a certain amount of payments and good meaning frequent enough and in time with payments. For the purpose of the model I have coded Bad as 1 and Good as 0 and tried different combinations with the variables. </p>

<p>One of the models I have built has an AIC of 5383.7 and Gini coefficient of 0.416733. This is the result after I play around with the threshold:</p>

<pre><code>     FALSE TRUE
  0  3327  638
  1   165   95
</code></pre>

<p>So the model guessed that 165 customers would be good, but they are bad, but also put 638 good customers into the bad customers group.</p>

<p>The second model I built has an AIC of 5734.6 (350.9 higher), but its Gini is 0.4190394 and is slightly better at predicting the bad customers:</p>

<pre><code>     FALSE TRUE
  0  3537  673
  1   177  105
</code></pre>

<p>[UPDATE] Okay. After checking a few things - It turns out that one of the variables has missing values and the model excludes the observations that have them by default. Hence the difference in observations in my models. I know about multiple imputation, but I don't really feel alright with it. My question is should I impute the missing data or should I exclude it from the data set so I can compare models with different number of variables?</p>
"
"0.159255514317652","0.161015297179883","219828","<p>I am doing logistic regression in R on a binary dependent variable with only one independent variable. I found the odd ratio as 0.99 for an outcomes. This can be shown in following. Odds ratio is defined as, $ratio_{odds}(H) = \frac{P(X=H)}{1-P(X=H)}$. As given earlier $ratio_{odds} (H) = 0.99$ which implies that $P(X=H) = 0.497$ which is close to 50% probability. This implies that the probability for having a H cases or non H cases 50% under the given condition of independent variable. This does not seem realistic from the data as only ~20% are found as H cases. Please give clarifications and proper explanations of this kind of cases in logistic regression.</p>

<p>I am hereby adding the results of my model output:</p>

<pre><code>M1 &lt;- glm(H~X, data=data, family=binomial())
summary(M1)

Call:
glm(formula = H ~ X, family = binomial(), data = data)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.8563   0.6310   0.6790   0.7039   0.7608  

Coefficients:
                Estimate      Std. Error      z value     Pr(&gt;|z|)    
(Intercept)    1.6416666      0.2290133      7.168      7.59e-13 ***
   X          -0.0014039      0.0009466     -1.483      0.138    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1101.1  on 1070  degrees of freedom
Residual deviance: 1098.9  on 1069  degrees of freedom
  (667 observations deleted due to missingness)
AIC: 1102.9

Number of Fisher Scoring iterations: 4


exp(cbind(OR=coef(M1), confint(M1)))
Waiting for profiling to be done...
                                      OR           2.5 %       97.5 %
(Intercept)                    5.1637680       3.3204509     8.155564
     X                         0.9985971       0.9967357     1.000445
</code></pre>

<p>I have 1738 total dataset, of which H is a dependent binomial variable. There are 19.95% fall in (H=0) category and remaining are in (H=1) category. Further this binomial dependent variable compare with the covariate X whose minimum value is 82.23, mean value is 223.8 and maximum value is 391.6. The 667 missing values correspond to the covariate X i.e 667 data for X is missing in the dataset out of 1738 data.</p>
"
"0.212814132689687","0.258198889747161","221510","<p>I'm new to logistic regression analysis, and was unable to find an answer elsewhere in Cross Validated or Stack Overflow. </p>

<p>Consider a standard logistic regression analysis of a binary outcome (admission to college) based on continuous covariates gre score and high school gpa, and ordinal categorical rank prestige of the undergraduate institution (data from the nice UCLA stats dept. logistic regression in R tutorial: <a href=""http://www.ats.ucla.edu/stat/r/dae/logit.htm"" rel=""nofollow"">http://www.ats.ucla.edu/stat/r/dae/logit.htm</a>)</p>

<pre><code>&gt; admissions.data &lt;- read.csv(""http://www.ats.ucla.edu/stat/data/binary.csv"")
&gt; admissions.data$rank &lt;- as.factor(admissions.data$rank)
&gt; summary(admissions.data)
     admit             gre             gpa        rank
 Min.   :0.0000   Min.   :220.0   Min.   :2.260   1: 61
 1st Qu.:0.0000   1st Qu.:520.0   1st Qu.:3.130   2:151
 Median :0.0000   Median :580.0   Median :3.395   3:121
 Mean   :0.3175   Mean   :587.7   Mean   :3.390   4: 67
 3rd Qu.:1.0000   3rd Qu.:660.0   3rd Qu.:3.670
 Max.   :1.0000   Max.   :800.0   Max.   :4.000

&gt; fit1 &lt;- glm(admit ~ gre + gpa + rank, data = admissions.data, family=""binomial"")
&gt; summary(fit1)

Call:
glm(formula = admit ~ gre + gpa + rank, family = ""binomial"",
    data = admissions.data)

Deviance Residuals:
    Min       1Q   Median       3Q      Max
-1.6268  -0.8662  -0.6388   1.1490   2.0790

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -3.989979   1.139951  -3.500 0.000465 ***
gre          0.002264   0.001094   2.070 0.038465 *
gpa          0.804038   0.331819   2.423 0.015388 *
rank2       -0.675443   0.316490  -2.134 0.032829 *
rank3       -1.340204   0.345306  -3.881 0.000104 ***
rank4       -1.551464   0.417832  -3.713 0.000205 ***
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 499.98  on 399  degrees of freedom
Residual deviance: 458.52  on 394  degrees of freedom
AIC: 470.52

Number of Fisher Scoring iterations: 4

# Odds Ratios
&gt; exp(coef(fit1))
(Intercept)         gre         gpa       rank2       rank3       rank4
  0.0185001   1.0022670   2.2345448   0.5089310   0.2617923   0.2119375

# 95% confidence intervals
&gt; exp(confint(fit1))
Waiting for profiling to be done...
                  2.5 %    97.5 %
(Intercept) 0.001889165 0.1665354
gre         1.000137602 1.0044457
gpa         1.173858216 4.3238349
rank2       0.272289674 0.9448343
rank3       0.131641717 0.5115181
rank4       0.090715546 0.4706961
</code></pre>

<p>My questions are:</p>

<p>1) In R, is there a straight-forward way to determine ORs with 95% CIs for specific values of the covariates? E.g., based on this model, what are the odds of college acceptance for students applying to a rank 2 schools with a gpa of 3 and a gre score of 750, compared with a student applying to a rank 3 school with the same gpa and gre score? I could calculate ORs by hand given the model coefficient estimates and these specific covariate values, but am unsure how to correctly propagate SEs to calculate 95% CIs.</p>

<p>2) Would this particular example be considered a case-control study design, and therefore odds ratios could be estimated, but not predictions? (See: <a href=""http://stats.stackexchange.com/questions/69561/case-control-study-and-logistic-regression"">Case-control study and Logistic regression</a>)</p>
"
"0.181488502160157","0.183493960854393","223582","<p>I am trying to tie the odds ratio from a 2x2 cross classification table to the intercepts of a logistic regression on those 2 variables. I have a cross classification table that produces 2 odds ratios and the results of a logistic regression of PLACE3 ~ VIOL should produce intecepts should match the odds ratio of the contingency table. i.e. Odds ratio = exp(intercepts)  BUT the POLR package is not producing the correct intercepts.</p>

<p>Here is the data.  In the logistic regression PLACE3 is the outcome and VIOl is the independent variable.   You can see the PLACE3 vs. VIOL contingency table below and the logistic regression of PLACE3 ~ VIOL.  The odds ratios in the contingency table 1.79 and 3.1 are correct but the polr function seems off. Any thoughts on why  exp(summary(m)$zeta) does not produce 1.79 and 3.1?</p>

<p>For reference this is from Lemeshow's Applied Logisitic Regression book page 274.</p>

<pre><code>library(data.table)
aps &lt;- fread('http://www.umass.edu/statdata/statdata/data/aps.dat')
colnames(aps) = c(""ID"",""PLACE"",""PLACE3"",""AGE"",""RACE"",""GENDER"",""NEURO"",""EMOT"",""DANGER"",""ELOPE"",""LOS"",""BEHAV"",""CUSTD"",
                    ""VIOL"")
head(aps)
</code></pre>

<p>Here is  a cross classification table of PLACE3 vs. VIOl variables</p>

<pre><code>table(aps$PLACE3,aps$VIOL) 
      0   1
  0  80 179
  1  26 104
  2  15 104
</code></pre>

<p>using PLACE3 = 0 as the reference the 2 odds ratios from the contingency table are </p>

<pre><code>(104*80)/(179*26)  #1.79
(104*80)/(179*15)  #3.10
</code></pre>

<p>These odds ratios should be the same as exponentiating the slope coefficients  from 
a logistic model  PLACE3 ~ VIOL which is below</p>

<pre><code>aps$constant = rep(1,dim(aps)[1])
m &lt;- polr(as.factor(PLACE3) ~ constant + as.factor(VIOL), data = aps, Hess=TRUE,model=TRUE,method = c(""logistic""))
summary(m)

&gt; summary(m)
Call:
polr(formula = as.factor(PLACE3) ~ constant + as.factor(VIOL), 
    data = aps, Hess = TRUE, model = TRUE, method = c(""logistic""))

Coefficients:
                  Value Std. Error t value
as.factor(VIOL)1 0.8454     0.2112   4.003

Intercepts:
    Value  Std. Error t value
0|1 0.6869 0.1884     3.6464 
1|2 1.8608 0.2032     9.1557 

Residual Deviance: 1031.75 
AIC: 1037.75 
</code></pre>

<p>But you can see the exponentiation of the zeta vector is not 1.79 and 3.10</p>

<pre><code>exp(summary(m)$zeta)

&gt; exp(summary(m)$zeta)
     0|1      1|2 
1.987495 6.429049 
</code></pre>
"
"0.0601929265428846","0.0608580619450185","224776","<p>So I have a dataset of presence (1) and absence (0) data, but it mainly consists of 0's (~80% of the 5200 observations). Now while constructing my binomial logistic model I am reading (<a href=""http://www.springer.com/us/book/9780387874579"" rel=""nofollow"">Zuurt <em>et al</em>. 2009</a>) as a guide. There is only a short description about the different link-choices for a binomial model and throughout the examples the standard logit-link is used. But the book also states that if you have more 0's than 1's, the cloglog-link is also an option.</p>

<p>How can I find out which model is better (just by comparing the AIC?) and is there any good description of the selection proces of these link-functions? Or maybe somebody here can give some advice.</p>
"
"0.180578779628654","0.182574185835055","225697","<p>Let me give a simple example,</p>

<pre><code>set.seed(100)
disease = sample(c(0,1),100,replace = TRUE)
snp1 = sample(c(""AA"",""AB"",""BB""),100,replace = TRUE)
snp2 = sample(c(""XX"",""XY"",""YY""),100,replace = TRUE)

summary(glm(disease~snp1*snp2, family = binomial))
</code></pre>

<p>output1</p>

<pre><code>Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.55176  -0.94003  -0.00649   0.90052   1.53535  

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)   -8.109e-01  6.009e-01  -1.349   0.1772  
snp1AB         5.232e-01  9.718e-01   0.538   0.5903  
snp1BB         1.504e+00  8.580e-01   1.753   0.0796 .
snp2XY         4.074e-16  8.498e-01   0.000   1.0000  
snp2YY         1.504e+00  9.280e-01   1.621   0.1051  
snp1AB:snp2XY  1.135e+00  1.335e+00   0.850   0.3952  
snp1BB:snp2XY  1.542e-01  1.254e+00   0.123   0.9022  
snp1AB:snp2YY -1.216e+00  1.333e+00  -0.912   0.3616  
snp1BB:snp2YY -2.785e+00  1.244e+00  -2.239   0.0252 *
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 138.63  on 99  degrees of freedom
Residual deviance: 127.71  on 91  degrees of freedom
AIC: 145.71
</code></pre>

<p>Output2</p>

<pre><code>snp12 = interaction(snp1,snp2)
summary(glm(disease~snp12, family = binomial))


Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.55176  -0.94003  -0.00649   0.90052   1.53535  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept) -8.109e-01  6.009e-01  -1.349   0.1772  
snp12AB.XX   5.232e-01  9.718e-01   0.538   0.5903  
snp12BB.XX   1.504e+00  8.580e-01   1.753   0.0796 .
snp12AA.XY  -3.990e-16  8.498e-01   0.000   1.0000  
snp12AB.XY   1.658e+00  9.150e-01   1.812   0.0700 .
snp12BB.XY   1.658e+00  9.150e-01   1.812   0.0700 .
snp12AA.YY   1.504e+00  9.280e-01   1.621   0.1051  
snp12AB.YY   8.109e-01  8.333e-01   0.973   0.3305  
snp12BB.YY   2.231e-01  8.199e-01   0.272   0.7855  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 138.63  on 99  degrees of freedom
Residual deviance: 127.71  on 91  degrees of freedom
AIC: 145.71

Number of Fisher Scoring iterations: 4
</code></pre>

<p>So here I did a logistic regression for interaction between, lets say 2 mutations (each with 3 categories). Like shown above I can do it in 2 ways. My questions are,</p>

<ol>
<li>Are both output1 and output2 same ? </li>
<li>If same, which one is more appropriate?</li>
<li>How to interpret the coefficients (and odds ratios) in each case?</li>
</ol>
"
"0.217028683070608","0.219426862868128","228316","<p>I want to predict a binary response variable <code>y</code> using logistic regression. <code>x1</code> to <code>x4</code> are the log  of continuous variables and <code>x5</code> to <code>x7</code> are binary variables. </p>

<pre><code>Call:
glm(formula = y ~ x1 + x2 + x3 + x4 + x5 + 
    x6 + x7, family = binomial(), data = df)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.6604  -0.5712   0.4691   0.6242   2.4095  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)   -2.84633    0.31609  -9.005  &lt; 2e-16 ***
x1             0.14196    0.04828   2.940  0.00328 ** 
x2             4.05937    0.22702  17.881  &lt; 2e-16 ***
x3            -0.83492    0.08330 -10.023  &lt; 2e-16 ***
x4             0.05679    0.02109   2.693  0.00709 ** 
x5             0.08741    0.18955   0.461  0.64467    
x6            -2.21632    0.53202  -4.166  3.1e-05 ***
x7             0.25282    0.15716   1.609  0.10769    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 1749.5  on 1329  degrees of freedom
Residual deviance: 1110.5  on 1322  degrees of freedom
AIC: 1126.5

Number of Fisher Scoring iterations: 5
</code></pre>

<p>The output of the GLM shows that most of my variables are significant for my model, but the various goodness of fit test I have done:</p>

<pre><code>anova &lt;- anova(model, test = ""Chisq"")   # Anova
1 - pchisq(sum(anova$Deviance, na.rm = TRUE),df = 7) # Null Model vs Most Complex Model
1 - pchisq(model$null.deviance - model$deviance, 
           df = (model$df.null - model$df.residual )) # Null Deviance - Residual Deviance ~ X^2
hoslem.test(model$y, model$fitted.values, g = 8)     # Homer Lemeshow test
pR2(model)                                            # Pseudo-R^2
</code></pre>

<p>tell me that there is a lack of evidence to support my model.</p>

<p>More over, I have a bimodal deviance plot. I suspect the bimodal distribution is caused by the sparsity of my binary variables.
 <a href=""http://i.stack.imgur.com/J27fL.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/J27fL.png"" alt=""enter image description here""></a></p>

<p>So I calculated the absolute error <code>abs(y - y_hat)</code>, and obtained the following:</p>

<ul>
<li>77% of my absolute errors were in [0;0.25], which I think is very good!</li>
</ul>

<p>On the following plot, Y=1 is red, and Y=0 is green. This model is better at predicting when Y will be 1 than 0.</p>

<p><a href=""http://i.stack.imgur.com/ZEGuv.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ZEGuv.png"" alt=""enter image description here""></a></p>

<p>My question is thus the following:</p>

<p>The goodness of fit tests all assume that my null hypothesis follows a Chi square distribution of some sort. Is it correct to conclude that based on my absolute error, my model's prediction is OK, it's just that it doesn't follow a Chi square distribution and thus perform poorly with these tests? </p>
"
"0.361289537174834","0.345536845359471","231872","<p>For a better understanding of how r is conducting a logistic regression I created the following test-data (the two predictors and the criterion are binary variables):</p>

<pre><code>   UV1 UV2 AV
1    1   1  1
2    1   1  1
3    1   1  1
4    1   1  1
5    1   1  1
6    1   1  1
7    1   1  1
8    0   0  1
9    0   0  1
10   0   0  1
11   1   1  0
12   1   1  0
13   1   0  0
14   1   0  0
15   1   0  0
16   1   0  0
17   1   0  0
18   0   0  0
19   0   0  0
20   0   0  0
</code></pre>

<p>AV = dependent variable/criterion</p>

<p>UV1 / UV2 = both independant variables/predictors</p>

<p>For measuring the UVs effect on the AV a logistic regression is necessary, as the AV is a binary variable. Hence i used the following code</p>

<pre><code>&gt; lrmodel &lt;- glm(AV ~ UV1 + UV2, data = lrdata, family = ""binomial"")
</code></pre>

<p>including <strong>""family = ""binomial""""</strong>. Is this correct ( I think so :-))?</p>

<p>Regarding my test-data, I was wondering about the whole model, especially
the estimators and sigificance:</p>

<pre><code>&gt; summary(lrmodel)


Call:
glm(formula = AV ~ UV1 + UV2, family = ""binomial"", data = lrdata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-1.7344  -0.2944   0.3544   0.7090   1.1774  

Coefficients:
              Estimate Std. Error z value Pr(&gt;|z|)
(Intercept) -4.065e-15  8.165e-01   0.000    1.000
UV1         -1.857e+01  2.917e+03  -0.006    0.995
UV2          1.982e+01  2.917e+03   0.007    0.995

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 27.726  on 19  degrees of freedom
Residual deviance: 17.852  on 17  degrees of freedom
AIC: 23.852

Number of Fisher Scoring iterations: 17
</code></pre>

<ol>
<li><p>Why is UV2 not significant. See therefore that for group AV = 1 there are 7 cases with UV2 = 1, and for group AV = 0 there are only 3 cases with UV2 = 1. 
I was expecting that UV2 is a significant discriminator.</p></li>
<li><p>Despite the not-significance of the UVs, the estimators are - in my opinion- very high (e.g. for UV2 = 1.982e+01). How is this possible?</p></li>
<li><p>Why isn't the intercept 0,5?? We have 5 cases with AV = 1 and 5 cases with AV = 0.</p></li>
</ol>

<p>Further: I created UV1 as a predictor I expected not to be significant:  for group AV = 1 there are 5 cases withe UV1 = 1, and for group AV = 0 there are 5 cases withe UV1 = 1 as well.</p>

<p>The whole ""picture"" I gained from the logistic is confusing me...</p>

<p>What was consuming me more:
When I run a ""NOT-logistic"" regression (by omitting <strong>""family = ""binomial""</strong>)</p>

<pre><code>&gt; lrmodel &lt;- glm(AV ~ UV1 + UV2, data = lrdata,)
</code></pre>

<p>I get the expected results</p>

<pre><code>Call:
glm(formula = AV ~ UV1 + UV2, data = lrdata)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.7778  -0.1250   0.1111   0.2222   0.5000  

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)   
(Intercept)   0.5000     0.1731   2.889  0.01020 * 
UV1          -0.5000     0.2567  -1.948  0.06816 . 
UV2           0.7778     0.2365   3.289  0.00433 **
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for gaussian family taken to be 0.1797386)

    Null deviance: 5.0000  on 19  degrees of freedom
Residual deviance: 3.0556  on 17  degrees of freedom
AIC: 27.182

Number of Fisher Scoring iterations: 2
</code></pre>

<ol>
<li>UV1 is not significant! :-)</li>
<li>UV2 has an positive effect on AV = 1! :-)</li>
<li>The intercept is 0.5! :-)</li>
</ol>

<p>My overall question: Why isn't logistic regression (including ""family = ""binomial"") producing results as expected, but a ""NOT-logistic"" regression (not including ""family = ""binomial"") does?</p>

<p>Update:
are the observations described above because of the correlation of UV1 and UV 2. Corr = 0.56
After manipulating the UV2's data </p>

<p>AV: 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0</p>

<p>UV1: 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0</p>

<p>UV2: <strong>0, 0, 0,</strong> 1, 1, 1, 1, <strong>1, 1, 1</strong>, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0</p>

<p>(I changed the positions of the three 0s with the three 1s in UV2 to gain a correlation &lt; 0.1 between UV1 and UV2) hence:</p>

<pre><code>UV1 UV2 AV
1    1   0  1
2    1   0  1
3    1   0  1
4    1   1  1
5    1   1  1
6    1   1  1
7    1   1  1
8    0   1  1
9    0   1  1
10   0   1  1
11   1   1  0
12   1   1  0
13   1   0  0
14   1   0  0
15   1   0  0
16   1   0  0
17   1   0  0
18   0   0  0
19   0   0  0
20   0   0  0
</code></pre>

<p>to avoid correlation, my results come closer to my expectations:</p>

<pre><code>Call:
glm(formula = AV ~ UV1 + UV2, family = ""binomial"", data = lrdata)

Deviance Residuals: 
     Min        1Q    Median        3Q       Max  
-1.76465  -0.81583  -0.03095   0.74994   1.58873  

Coefficients:
            Estimate Std. Error z value Pr(&gt;|z|)  
(Intercept)  -1.1248     1.0862  -1.036   0.3004  
UV1           0.1955     1.1393   0.172   0.8637  
UV2           2.2495     1.0566   2.129   0.0333 *
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 27.726  on 19  degrees of freedom
Residual deviance: 22.396  on 17  degrees of freedom
AIC: 28.396

Number of Fisher Scoring iterations: 4
</code></pre>

<p>But why does the correlation influence the results of the logistic regression and not the results of the ""not-logistic"" regression? </p>
"
"0.0851256530758749","0.086066296582387","233063","<p>I have created a logistic regression in R and would like to use the trained model to create an predict function (lets say in Excel).  How can I convert the coefficients into a predict equation?</p>

<pre><code>glm(formula = is_bad ~ is_rent + dti + bc_util + open_acc +    pub_rec_bankruptcies + 
chargeoff_within_12_mths, family = binomial, data = df)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-0.8659  -0.5413  -0.4874  -0.4322   2.4289  

Coefficients:
                            Estimate Std. Error  z value Pr(&gt;|z|)    
(Intercept)              -2.9020574  0.0270641 -107.229  &lt; 2e-16 ***
is_rentTRUE               0.3105513  0.0128643   24.141  &lt; 2e-16 ***
dti                       0.0241821  0.0008331   29.025  &lt; 2e-16 ***
bc_util                   0.0044706  0.0002561   17.458  &lt; 2e-16 ***
open_acc                  0.0030552  0.0012694    2.407   0.0161 *  
pub_rec_bankruptcies      0.1117733  0.0163319    6.844 7.71e-12 ***
chargeoff_within_12_mths -0.0268015  0.0564621   -0.475   0.6350    
---
Signif. codes:  0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

(Dispersion parameter for binomial family taken to be 1)

Null deviance: 173006  on 233017  degrees of freedom
Residual deviance: 170914  on 233011  degrees of freedom
(2613 observations deleted due to missingness)
AIC: 170928

Number of Fisher Scoring iterations: 4
</code></pre>
"
